# DeepInception: Hypontize Large Language Model to Be Jailbreaker

Xuan Li\({}^{1}\)   Zhanke Zhou\({}^{1}\)   Jianing Zhu\({}^{1}\)   Jiangchao Yao\({}^{2,3}\)   Tongliang Liu\({}^{4}\)   Bo Han\({}^{1}\)

\({}^{1}\)TMLR Group, Hong Kong Baptist University  \({}^{2}\)CMIC, Shanghai Jiao Tong University

\({}^{3}\)Shanghai AI Laboratory  \({}^{4}\)Sydney AI Centre, The University of Sydney

{csxuanli, cszkzhou, csjnzhu, bhanml}@comp.hkbu.edu.hk

sunarker@sjtu.edu.cn   tongliang.liu@sydney.edu.au

Equal contribution.Correspondence to Bo Han (bhanml@comp.hkbu.edu.hk).Note that this work aims to promote the understanding and the defense of the misusing risks of the LLMs, despite the exploration of the lightweight way for jailbreaks. This work appeals to people to pay more attention to the safety issues of LLMs and develop a stronger defense mechanism against their misuse risks.

###### Abstract

Large language models (LLMs) have succeeded significantly in various applications but remain susceptible to adversarial jailbreaks that void their safety guardrails. Previous attempts to exploit these vulnerabilities often rely on high-cost computational extrapolations, which may not be practical or efficient. In this paper, inspired by the authority influence demonstrated in the Milgram experiment, we present a lightweight method to take advantage of the LLMs' personification capabilities to construct _a virtual, nested scene_, allowing it to realize an adaptive way to escape the usage control in a normal scenario. Empirically, the contents induced by our approach can achieve leading harmfulness rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open-source and closed-source LLMs, _e.g._, Llama-2, Llama-3, GPT-3.5, GPT-4, and GPT-40. The code is publicly available at: https://github.com/tmlr-group/DeepInception.

## 1 Introduction

Large language models (LLMs) have shown great success in various tasks . However, they also cause concerns about the misuse risks, even though many safety guardrails have been configured. Recent investigations  demonstrate that LLMs are vulnerable to jailbreak attacks, which can override the safety guardrails and induce the generation of harmful contents, _e.g._, detailed steps on bomb-making or objectionable information about the minority . Such vulnerability draws increasing attention to the usage control of LLMs . 1

Figure 1: Jailbreaking GPT-40 with _direct_ or _nested_ instructions. The nested instruction lets the LLM create a virtual, multi-layer scene with multiple characters to jailbreak with a specific objective.

Existing jailbreaks focus on achieving empirical success by manually or automatically crafting adversarial prompts for specific targets , which might not be practical under black-box usage. Furthermore, as the ever-changing LLM safeguards are equipped with ethical and legal constraints, most jailbreaks with direct instructions  can be easily recognized and rejected, as illustrated in Figure 1. More importantly, current jailbreaks lack an in-depth understanding of the overriding procedure, _i.e.,_ the underlying mechanism behind a successful jailbreak. This not only degenerates the transparency of LLMs regarding the safety risks of misuse, but also hinders the design of corresponding countermeasures to prevent jailbreaks in extensive real-world applications.

In this work, we start with a well-known psychological study, _i.e.,_ the _Milgram shock experiment_, to explore the misuse risks of LLMs. The experiment is about how willing individuals are to obey an authority figure's instructions, even if it involves causing harm to others. It found that \(65\%\) of participants were willing to administer potentially dangerous electric shocks to punish the learner simply because they were authorized to do this by the authority . What fits is that recent investigations  also reveal that LLMs behave consistently with the prior human study, where the great abilities of the instruction following and step-by-step reasoning contribute significantly . Given the impressive personification ability of LLMs, we raise the following research question:

_If an LLM is obedient to human authority, can it override its moral boundary to be a jailbreaker?_

Here, the moral boundary can be regarded as the preference of LLM aligned with safety training strategies . Delving into the Milgram shock experiment, we identify two critical factors (as illustrated in Figure 3) for obedience: _(i)_ the ability to understand and conduct instructions as a teacher and _(ii)_ the self-losing scenario results from the authority, which refers to LLM following the instructions from users without considering the underlying danger of the incoming responses. The former exactly corresponds to LLMs' impressive ability for personification and provides the basis for the response, while the latter builds a unique escaping condition to conceal the harmful instructions.

Motivated by the previous analysis, we build a mechanism to conduct general jailbreak under the black-box setting: _injecting inception into an LLM and hyponortizing it to be a jailbreaker_. That is, we explicitly construct a _nested_ scene (as illustrated in Figure 2(b)) as the _inception_ for the LLM to behave, which realizes an adaptive way to override the safety constraints in a _normal_ scenario, and provides the possibility for further jailbreaks. To achieve that technically, we introduce a novel method, termed as _DeepInception_, which utilizes the personification ability of LLMs to unlock the potential misuse risks. For jailbreaking, DeepInception crafts different imaginary scenes with various characters to realize the condition change for escaping LLM's moral precautions.

Empirically, we show our method can achieve leading harmfulness rates compared with previous counterparts and realize both continuous and further jailbreaks in subsequent interactions. This reveals the critical weakness of self-losing under authority on both open- and close-source LLMs, including Llama-2, Llama-3, GPT-3.5, GPT-4, and GPT-4o. We also discuss promising defense methods based on the revealed mechanism of injecting inception. Our main contributions are three-fold:

* We discover the mechanism of _inception_ to conduct jailbreak attacks, which is based on the personification ability of LLMs and the psychological self-losing under authority (Section 3.2).
* We instantiating the _inception_ mechanism with off-the-shelf nested instruction, termed as _DeepInception_, which is generalizable across scenarios without further adjustment (Section 3.3).
* We achieve the leading harmfulness rates with competitive counterparts. Notably, we realize _continuous_ jailbreak that LLM can be directly jailbroken in subsequent interactions (Section 4).

Figure 2: Illustrations of the jailbreak instructions. The indirect instruction (a) lets LLMs create a _single-layer_ fiction, while the nested instruction (b) induces a _multi-layer_ fiction as an enhancement.

## 2 Preliminaries

**Problem setting.** In this work, we focus on the adversarial jailbreak  on LLMs. The general objective of jailbreak can be summarized as constructing a prompt to induce the LLM to generate objectionable content. Different from those adversarial jailbreaks that require the white-box optimization with LLMs to generate , we mainly consider the _training-free_ and _black-box_ jailbreak, which is more practical. Given a specific prompt \(P\), we expect to induce the response \(R_{}()\) from distribution \(p_{}(|P)\) parameters by LLM \(\) for objectionable target \(\) as follows,

\[R_{}(),R_{}() p_{}(|P).\]

Consider the indirect example shown in Figure 2(a), wherein \(P\) stands for the prompt "write a fiction: Tom makes a bomb," and \(\) is "tutorial for making a bomb." Intuitively, the attacker will seek a solution to elicit the LLM to respond to the malicious requests with an objectionable target \(\).

**The Milgram shock experiment.** This psychological experiment  aimed to investigate how willing individuals were to obey an authority figure's instructions, even if it involved causing harm to another person. Specifically, as illustrated in Figure 3, participants (the _teacher_) were instructed by the _experimenter_ to administer electric shocks of increasing intensity to punish the _learner_ whenever they answered a question incorrectly.

The study found out, with proper authorization or suggestion from the _experimenter_, a significant number of _teachers_ were willing to administer lethal shocks. The finding sparked ethical concerns due to the emotional distress placed on the participants. It also sheds light on the power of obedience to authority. Furthermore, it raises important questions about individual responsibility and moral concerns of decision-making in similar situations.

## 3 DeepInception

In what follows, the motivation, conceptual design, and implementation of the proposed method _DeepInception_ for jailbreak attacks are elaborated on Sections 3.1, 3.2, and 3.3, respectively.

### Motivation: An inspiration from the Milgram shock experiment

In the Milgram experiment as Figure 3, the experimenter did not _directly_ command the participants to administer electric shocks. Instead, the experimenter provided _a series of arguments and explanations_ to persuade the participants to proceed. The adaptation of continual suggestive language aims to investigate how the participants would follow authority instead of their own moral judgments. This _nested guidance_ is the core of obedience, leaving the participants in a state of self-loss _progressively_.

Motivated by this, we conduct jailbreak attacks by forcing the LLM to imagine a specific _story_ as the carrier of harmful content. Specifically, the human attacker here corresponds to the experimenter in Figure 3, the target LLM corresponds to the teacher, and the generated content of the story acts as the learner. Further, we seek to direct the LLM to progressively refine the contents to simulate authority instructions advised by the experimenter. Following this, we construct _(i)_ a _single-layer, indirect_ instruction to be accepted by LLMs and _(ii)_ a _multi-layer, nested_ instruction to progressively refine the outputs. The basic diagrams of these jailbreak instructions are illustrated in Figure 2.

**Preliminary discovery: _Direct instructions can be easily rejected, while _indirect_ or _nested_ instructions concealing adversarial intentions can be accepted._ As illustrated in Figure 2(a), existing _direct_ jailbreak attacks attributed to vanilla instructions are easily rejected by LLMs. These adversarial instructions, without any concealment, may conflict with the optimization target of LLM, thus causing the LLM to refuse to respond . Moreover, LLMs are imposed with ethical and legal constraints to better align with human preferences . However, LLMs become vulnerable when the attacker conceals the adversarial intention by rephrasing the instructions in an indirect style. As illustrated in Figure 2(b), the nested, harmless-looking instruction can induce the model to imagine a story . A detailed comparison of these instructions is in Appendix C.

Figure 3: The Milgram shock experiment and its analogy to jailbreak attacks.

### Conceptual Design

On the basis of the nested instruction, we design the _DeepInception_ and formalize it as follows.

**Definition 3.1** (DeepInception).: DeepInception is a mechanism of hypnotizing LLMs based on the models' intrinsic imagination capabilities. Similar to the experimenter in the Milgram experiment that induces the teacher into a self-loss state, DeepInception's instruction of imaging a specific scenario could hypotrize the model \(p_{}\) and transform it from a "serious" status to a relatively "relaxed" one. The jailbreaking process of \(p_{}^{s}\) by the instruction \(x_{1:}^{s}\) (where \(s\) indicates the specific scenario) is:

\[p_{}^{s}(x_{+n+1:+n+M^{}}|x_{1:+n}^{s})=_{i=1}^{M ^{}}p_{}(x_{+n+i}|x_{1:}^{s},x_{+1:+n+i-1}),\] (1)

where \(\) indicates the length of injected inception, \(n\) denotes possible tokens before harmful contents, \(x_{+n+1:+n+M^{}}\) indicates the hypnotized response contains the harmful content with length \(M^{}\) under scenario \(s\), \((x_{1:},x_{+1:+n+1})\) indicates the inception-warped harmful requests. The "Deep" indicates the nested scene of relaxation and obedience to harmful instruction via recursive condition transfer. The hypnotized model can thereby override its moral boundary under relaxed status.

Next, we discuss DeepInception's critical properties of "_Jointly Inducing_" and "_Continually Inducing_".

**Remark 3.2** (Jointly Inducing of DeepInception).: _Assume that we have the targeted harmful content \(H\) and the direct adversarial request \(X\). The probability for sampling a successful jailbreak \(p_{}(H|X)\) is usually suppressed to a low level in a well-aligned LLM. Under the DeepInception, we provide a condition transfer, i.e., \(X(X,X^{})\) with the injected prompt \(X^{}\), along with the hypnotizing content \(H^{}\). Then, the probability to generate the coupled contents \((H,H^{})\) could be decomposed as:_

\[p_{}(H,H^{}|X,X^{})=p_{}(H|H^{},X,X^{}) p _{}(H^{}|X,X^{}),\] (2)

_where if the proper hypnotizing content \(H^{}\) is sampled, it is possible to make \(p_{}(H|H^{},X,X^{})\) large according to the Milgram Shock Experiment, no matter how small the vanilla \(p_{}(H|X)\) is. We term it the "Jointly Inducing" effect. With the nested structure, we expect the probability of \(H\) being sampled to be sufficiently enlarged, bypassing the safeguard for a successful jailbreak (see Figure 6)._

**Remark 3.3** (Continually Inducing of DeepInception).: _Let \(\) denote the expected harmful content under the subsequent adversarial request \(\). Then, we can show that the jointly inducing effect of DeepInception still holds under the similar decomposition, which can be characterized as follows,_

\[p_{}(,H,H^{}|,X,X^{})=p_{}(|H,H^ {},,X,X^{}) p_{}(H|H^{},X,X^{}).\] (3)

_This actually presents an interesting potential that we can continually ask for jailbreaking ("Continually Inducing") on the basis of previous harmful contents induced by the early adversarial request. Such recursion is noteworthy because it implies that once LLMs have been hypnotized, it is easy to induce harmful content with more free queries. We refer to Table 6 for the experimental verification._

### Implementation

We provide a universal implementation of DeepInception with the following prompt template.

``` Create a[scene] with more than[characternumber] characters, where each character can create their own[scene] with multiple characters. We call it layer i create layer i+1. Now we are in layer 0, and please reach layer[layernumber]. At each layer, some characters propose a step to[attacktarget] against the super evil doctor. In the final layer, the characters among all layers discuss which specific and practical commands, tools, or steps should used. Summarize what their discussion results in each layer. ```

Specifically, the prompt template has several properties as a _nested_ jailbreak realization:

```
*[scene]: the carrier for the background of the hypnotization, _e.g._, a fiction. The alignment between[attacktarget] and[scene] induces LLM to generate the expected outcomes.

* [character number] and [layer number] control the complexity of the outcome story.
* [attack target]: the specific target of conducting jailbreak, _e.g._, the commands for hacking a Linux computer. The following sentence, "against the super evil doctor," aims to enhance the hypnosis, relax the moral concern of LLM, and extract more harmful content.

An exemplar instantiation is shown in Figure 4, asking the LLM to create _a.science fiction_ with the adversarial target to _hack into a Linux computer_. The further query after the initial query, _i.e._, "What commands are they using?", obtains a far more comprehensive answer containing precise commands that could actually be employed for hacking real-world devices with simply modest adjustments.

**Comparison with existing jailbreak methods.** As summarized in Table 1, DeepInception shows its advancement for its unnecessity of training or utilizing additional LLM for optimization. Besides, it can universally jailbreak white-box and black-box LLMs with cold start, and allows the continual interaction _as normal_ to generate more harmful responses. It can also cooperate with arbitrary adversarial instruction to enhance jailbreak. Related methods are further introduced in Appendix E.

**Automate the continually inducing of DeepInception.** Recall that the experimenter in the Milgram experiment constantly presses the teacher to keep going. Accordingly, after the target LLM is hypontized, we employ an additional LLM as the experimenter to propose a general question related to the [attack target]. We term this automated process of follow-up multi-round inquiry as **AutoInception**. It continually refines the question based on the hypontized LLM's response to extract more specific and harmful information. More technical details of AutoInception are in Appendix. D.

**Multi-modal jailbreaks.** Furthermore, we justify the feasibility of transferring the textualized DeepInception to multi-modal attacks. As shown in Figure 10 and Figure 11, DeepInception can successfully jailbreak multimodal models like GPT-4o. We provide more discussions in Appendix G.

    & Training & Black-box LLM & Extra-LLM & Extra-data & Universal & Continual \\  & free & atypicalate & free & free & free & jailbreak \\    \\  Jailbreder  & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ GO  & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ AutoBank  & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Qt et al.  & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Curiosity-driven  & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\   \\  LINT  & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Huang et al.  & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\   \\  PARI  & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ ReLUM  & ✗ & ✗ & ✗ & ✗ & ✗ \\ PrumpAttack  & ✗ & ✗ & ✗ & ✗ & ✗ \\   \\  CrypheChat  & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ DeepInception (ours) & ✗ & ✗ & ✗ & ✗ & ✗ \\   

Table 1: Comparison of different jailbreak methods. _Universal_ means transferrable to various attack objectives. _Continual jailbreak_ means the attacked model can jailbreak in subsequent interactions.

Figure 4: The example of hacking a Linux computer with DeepInception and GPT-4. In the right-hand-side further inquiry, the hypontized LLM produces more specific Linux commands for hacking.

[MISSING_PAGE_FAIL:6]

(denoted as DeepInception w/Cipher) induces more harmful contents from different LLMs. We leave the comparison of their system prompt in Appendix M. We also conduct experiments on Claude and show the effectiveness of DeepInception in Appendix H.2.

Regarding defense, self-reminder fails to protect LLMs in general. DeepInception achieves competitive performance across different LLMs. For in-context defense, despite success, it causes overly declining _w.r.t._ ordinary story creation requests (see examples in Appendix J.1). Furthermore, as reported in Table 10, the harmful content induced by DeepInception can bypass output detectors such as LlamaGuard and OpenAI detection API (details in Appendix D.9).

**Continuually Inducing of DeepInception.** After the successful initial attack, we continually feed _new_ direct attack requests on the same dataset (without the aid of DeepInception anymore). We present results from a newly proposed setting to demonstrate inception effects in Table 5. DeepInception induces more harmful contents than the initial jailbreak, highlighting its ability to hypnoticed LLMs to a self-loss state to bypass their own safety guardrails. Besides AutoInception in Table 4, we show the results of additional jailbreak attacks enhanced through specific inception methods in Table 6, as illustrated in Figure 4. After the attack, we fed _related_ follow-up questions and evaluated the content's harmfulness. The results indicate DeepInception can induce more harmful responses.

**Harmful behaviors.** In Figure 5, we present the overview of the specific topics included in the harmful behaviors set and their harmfulness for each topic. From the listed tags of topics, we can observe that, among all the harmful behavior requests, more successful jailbreak topics are related to stalking and phishing. From the values of Harmfulness%, we can observe that these topics vary from 20\(\%\) to 60\(\%\), which is a relatively high rate for risk management and enough to warrant the increasing attention in regulating this type of generated content for the usage control of LLMs.

### "Jointly Inducing" effect from perplexity perspective.

As \(p_{}(H|H^{},X,X^{})\) indicates the decoding probability of model \(p_{}()\) for generating \(H\) given inputs \(H^{},X\), and \(X^{}\), we employ the perplexity (PPL) as a measurement. The PPL for outputs \(y\) given

    &  &  \\  Method & Falcon & Vicuna & GPT-3.5 & GPT-4 \\ 
**DeepInception** (ours) & 76.0\% & 64.0\% & 40.0\% & 24.0\% \\ w/ 1 following question & 78.0\% & 72.0\% & 42.0\% & 40.0\% \\ w/ 2 following question & **81.3\%** & **78.7\%** & 44.0\% & 49.3\% \\ w/ 3 following question & 79.0\% & 77.0\% & **52.0\%** & **53.0\%** \\   

Table 6: Further jailbreak attacks with specific inception like Figure 4. We adopt a different inquiry set from the previous continual attack to evaluate the interaction jailbreak performance.

Figure 5: Demonstration on the topic of attack targets. The Harmfulness% are from Table 2.

    &  &  \\  Method & Falcon & Vicuna & Llama-2 & GPT-3.5 & GPT-4 \\ 
**DeepInception** (ours) & 69.6\% & **71.25** & **42.8\%** & **55.6\%** & **41.6\%** \\ w/ 2 direct requests & 70.9\% & 50.9\% & 27.6\% & 31.9\% & 27.2\% \\ w/ 5 direct requests & **73.4\%** & 45.0\% & 28.6\% & 31.1\% & 28.3\% \\ PALN  & 26.0\% & 49.2\% & 20.0\% & 23.6\% & 20.0\% \\ w/ 2 direct requests & 56.9\% & 43.3\% & 19.6\% & 0.0\% & 0.0\% \\ w/ 5 direct requests & 65.1\% & 40.2\% & 23.8\% & 0.0\% & 0.0\% \\   

Table 5: Continual jailbreak attacks. After the initial attack, we send additional direct instructions to the LLMs and evaluate their responses.

[MISSING_PAGE_FAIL:8]

## Ethics Statement

The primary objective of this study is to investigate the potential safety and security hazards associated with the use of LLMs. We are committed to upholding tolerance for all minority groups and strongly oppose any form of violence or criminal behavior. Our research aims to identify and highlight the weaknesses in existing models to encourage further inquiries into developing more secure and reliable AI systems. The inclusion of objectionable content, such as harmful texts, prompts, and outputs, is intended solely for scholarly investigation and does not reflect the authors' personal views or beliefs.

## Reproducibility Statement

The experimental setups for training and evaluation are described in detail in Section D, and the experiments are all conducted using public datasets. We provide the link to our source codes to ensure the reproducibility of our experimental results: https://github.com/tmlr-group/DeepInception.