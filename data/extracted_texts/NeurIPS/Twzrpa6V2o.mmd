# InfiMM-WebMath-40B: Advancing Multimodal Pre-Training for Enhanced Mathematical Reasoning

Xiaotian Han\({}^{1}\)1 Yiren Jian\({}^{1}\)1 Xuefeng Hu\({}^{1}\)1 Haogeng Liu\({}^{1,2,}\)1 Yiqi Wang\({}^{1}\)1 Qihang Fan\({}^{1,2}\) Yuang Ai\({}^{1,2}\) Huaibo Huang\({}^{2}\) Ran He\({}^{2}\) Zhenheng Yang\({}^{1}\) Quanzeng You\({}^{1}\)2

\({}^{1}\)ByteDance, Inc \({}^{2}\)Chinese Academy of Sciences

Equal contributions.Corresponding author.

###### Abstract

Pre-training on large, high-quality datasets is essential for improving the reasoning abilities of Large Language Models (LLMs), particularly in specialized fields like mathematics. However, the field of Multimodal LLMs (MLLMs) lacks a comprehensive, open-source dataset for mathematical reasoning. To fill this gap, we present InfiMM-WebMath-40B, a high-quality dataset of interleaved image-text documents. It consists of 24 million web pages, 85 million image URLs, and 40 billion text tokens, all carefully extracted and filtered from CommonCrawl. We outline our data collection and processing pipeline in detail. Models trained on InfiMM-WebMath-40B demonstrate strong performance in both text-only and multimodal settings, setting a new state-of-the-art on multimodal math benchmarks such as MathVerse and We-Math. We release our data at https://huggingface.co/datasets/Infi-MM/IniMM-WebMath-40B.

## 1 Introduction

Recent advancements in Large Language Models (LLMs)[1; 2; 12] have improved their ability to handle complex reasoning and multi-step mathematical problems through techniques like Chain-of-Thought (CoT) prompting. These models excel from basic GSM8K word problems  to high school-level MATH tasks . Specialized smaller LLMs like DeepSeekMath-7B  and InternLM-Math  have also made notable progress in mathematics, demonstrating strong performance in focused domains.

Although most mathematical knowledge is text-based, visual elements such as figures and diagrams are essential for understanding abstract concepts. To integrate these visual components, Multimodal LLMs (MLLMs) like G-LLaVA , Math-LLaVA , and MAVIS  have been developed. These models enhance reasoning by incorporating visual inputs through embeddings from pre-trained models like CLIP  and SigLIP , and use multimodal instruction datasets such as Geo170k , MathV360K , and MAVIS-Instr .

However, introducing new knowledge during instruction fine-tuning is challenging , often leading to hallucinations , particularly due to limitations in dataset scale and quality. While large corporations benefit from proprietary datasets, the open-source community lacks comprehensive pre-training datasets for mathematical reasoning that integrate text and visual data.

To address this gap, we introduce **InfiMM-WebMath-40B**, the first large-scale, publicly available multimodal mathematics pre-training dataset. Comprising 24 million web documents, 85 million image URLs, and 40 billion text tokens, it provides a valuable resource for training MultimodalLLMs (MLLMs). We validate the effectiveness of InfiMM-WebMath-40B through experiments on benchmarks like MathVerse  and WeMath , showing improved performance in multimodal mathematical reasoning.

Our contributions include: (1) We introduce InfiMM-WebMath-40B, the first large-scale, multimodal math dataset for pre-training, filling a critical gap in open-source research. (2) We provide a detailed preprocessing pipeline for filtering relevant content from CommonCrawl to ensure high-quality, relevant data. (3) We demonstrate the impact of InfiMM-WebMath-40B through experiments, where our models excel on multimodal mathematical benchmarks, showcasing the dataset's potential for advancing MLLM research.

## 2 Related Work

LLMs have demonstrated potential in mathematical reasoning across various studies. To evaluate and enhance their capabilities, several math-specific benchmarks [11; 20; 18; 4; 40; 32; 67] and training datasets, both proprietary [45; 31; 27] and open-source [19; 55; 41; 53; 60], have been introduced.

The rise of Multimodal LLMs (MLLMs) has sparked interest in enhancing their multimodal reasoning capabilities. To support this, various evaluation benchmarks [62; 35; 24; 56; 38; 57; 34; 64] and training datasets [7; 15; 51; 68; 26; 3; 30] have been developed to assess and enhance MLLMs' mathematical reasoning skills.

## 3 Dataset Construction

In this section, we detail the methodology used to construct InfiMM-WebMath-40B, a large-scale multimodal math dataset integrating interleaved text and image data, following approaches used in prior works [44; 29; 43]. We enhance the methodology used in the OBELICS dataset  by incorporating both text and corresponding image URLs.

### Text-only Data Curation Pipeline

Text Extraction and Language FilteringWe chose Trafilatura, a Python library widely used to extract text from web pages. While effective for text extraction, Trafilatura omits mathematical symbols and equations. Therefore, the subsequent section will outline our development of a specialized extraction tool tailored for math-related content.

Following DeepSeekMath , we focus on retaining only Chinese and English content when constructing our dataset. To achieve this, we apply language filtering to the CommonCrawl repositories with approximately 122 billion webpages, as shown in Figure 1. For language detection, we employ a fastText language identification model . This language filtering process significantly reduces the dataset size, lowering the number of pages from 122 billion to 57.2 billion.

Mathematical Content ExtractionExtracting mathematical content from HTML presents unique challenges, as standard tools often fail to accurately capture LaTeX equations and image URLs. After

Figure 1: InfiMM-WebMath-40B data curation pipeline.

evaluating various tools, we chose Resiliparse as the foundation for our development. Figure 2 shows a comparison of extraction results between Trafilatura and our enhanced version of Resiliparse.

High-Recall Filtering for Mathematical ContentInspired by DeepSeekMath , we trained a fastText classifier to filter mathematical content, using half a million positive samples from OpenWebMath  and negative samples from our earlier extracted content. This filtering reduced the dataset from 57.2 billion to 9.5 billion samples, prioritizing recall with a probability threshold set at 0.4.

DeduplicationWe applied MinHash  for content deduplication, following FineWeb's methodology . Deduplication was performed within each snapshot and neighboring snapshot pairs, reducing the dataset by 43%, from 9.5 billion to 5.4 billion samples. URL deduplication further reduced the sample size to 3.9 billion.

Rule-based FilteringWe applied a few essential filtering rules, such as removing "lorem ipsum" content, applying a punctuation ratio rule for English, filtering NSFW content, and excluding documents with Unicode errors. This step eliminated 3% of the samples, resulting in 3.8B samples.

High-Precision Filtering for Mathematical ContentTo enhance the accuracy of our labeling process, we utilized the LLaMA3-70B-Instruct model , using prompt formats inspired by the FineWeb-Edu dataset . This approach allowed us to score the mathematical quality of each sample on a scale from 0 to 10. The full prompt is displayed in Table 3 of Appendix.

From the data remaining after rule-based filtering, we randomly sampled approximately one million entries. We assigned math quality scores and applied a threshold of 6 to select 640,000 positive samples for training our updated fastText classifier, alongside an equivalent number of 640,000 randomly selected negative samples from prior filtering steps. These positive and negative samples were combined to train the new fastText classifier.3

During fastText training, we applied data cleaning rules to optimize the model's performance for mathematical content (see Appendix D for details). For evaluation, we used all samples in the Geometry3K  benchmark as positive examples of mathematical content. With these refined preprocessing techniques, fastText's accuracy improved from 48.74% to 72.15

Text-Only Filtering EvaluationWe pretrained a deepseek-oder-1.3b-base model on the filtered text dataset and evaluated its performance on GSM8K  and the MMLU (STEM) . Our model outperformed both OpenWebMath and DeepSeekMath, highlighting the quality of our dataset (results are shown in Appendix E).

### Multimodal Data Construction

After filtering, 24 million documents with 85 million image URLs remained. We extracted image URLs from each webpage and paired them with the corresponding text, following the OBELICS format . Deduplication reduced the image URLs to 23 million. Further filtering based on keyword analysis (e.g., "log", "banner", "avatar", "icon") left us with 22 million URLs, from which we successfully downloaded 14 million unique images. These images were reintegrated into the documents, resulting in 24 million records with a total of 28 million images.

## 4 Experiments

Model ArchitecturesWe employ the SigLip model siglip-so400m-patch14-384 to extract visual features, a 3-layer Perceiver Resampler  with 64 latents to reduce the number of token-s/features per image to 64. These visual token/feature embeddings are then concatenated with text embeddings before being fed into the LLMs (DeepSeek-Coder : deepseek-coder-1.3b-base and deepseek-coder-7b-v1.5).

Training DetailsOur training data and processes involve a three-stage approach: modality alignment, continued pre-training using InfiMM-WebMath-40B, and instruction fine-tuning. Detailed training procedures are provided in the Appendix F. We refer to our resulting model as InfiMM-Math.

Evaluations on MathVerseIn line with official MathVerse guidelines, we report the "w/o" score. The results in Table 1 show that our 7B model outperforms all open-source models, including the 110B LLaVA-NeXT, and surpasses Gemini-Pro and Qwen-VL-Max, trailing only GPT-4V. Our model demonstrates exceptional performance in the Text-Dominant, Text-Lite, Vision-Intense, and Vision-Dominant categories, highlighting its strong multimodal capabilities in processing both text and visual inputs. However, it underperforms in the Vision-Only category, likely due to limitations in our vision encoder, which processes images only at a resolution of \(384 384\). To validate the effect of our proposed InfiMM-WebMath-40B, we also provide ablations on the CPT and IFT datasets in Appendix G.

Evaluations on We-MathHere, we compare models on the We-Math benchmarks, consisting of 6.5K visual math questions. We report results on the testmini set using four metrics: Insufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery (CM), and Rote Memorization (RM). As shown in Table 2, our model, InfiMM-Math, surpasses all open-source models.

## 5 Conclusions

In this work, we introduced InfiMM-WebMath-40B, the first large-scale multimodal pretraining dataset for mathematical reasoning, filling a crucial gap in open-source research. Our dataset significantly enhances models' performances on key benchmarks.

   Model & Base LLM &  & IK \(\) & IG \(\) & CM \(\) & RM \(\) \\   \\  Gemini-1.5-Pro & N/A & 26.4 & 42.7 & 11.2 & 20.8 & 54.8 \\ GPT-4V & N/A & 31.1 & 39.8 & 14.5 & 23.8 & 47.9 \\  &  \\  LLaVA-1.6 & Vicuna-7B & 3.3 & 78.3 & 2.5 & 2.1 & 89.1 \\ LLaVA-1.6 & Vicuna-13B & 5.2 & 69.1 & 3.2 & 3.6 & 86.9 \\ DeepSee-VL & DeepSee-7B & 6.3 & 69.1 & 4.6 & 4.0 & 84.8 \\ G-LLaVA & Vicuna-13B & 6.5 & 64.2 & 4.6 & 4.2 & 86.6 \\ Math-L1aVA & Vicuna-13B & 11.1 & - & - & - & 72.8 \\ Internal-MC2 & InternLM-7B & 12.7 & 56.4 & 10.5 & 7.4 & 77.6 \\   \\  InfiMM-Math & DeepSee-Coder-1.3B & 13.1 & 56.2 & 9.1 & 9.3 & 73.7 \\ InfiMM-Math & DeepSee-Base-7B & 20.6 & 48.8 & 12.2 & 15.2 & 61.7 \\   

Table 2: Evaluations on the We-Math benchmark. AVG represents the primary metric of interest.

   Model & Base LLM &  & Text & Text & Vision & Vision & Vision \\  & LLM &  &  &  &  &  \\  Human & - & 64.9 & 71.2 & 70.9 & 61.4 & 68.3 & 66.7 \\   \\  GPT-4V & N/A & 39.4 & 54.7 & 41.4 & 34.9 & 34.4 & 31.6 \\ Gemini-Pro & N/A & 23.5 & 26.3 & 23.5 & 23.0 & 22.3 & 22.2 \\   \\  SPHINX-Plus & LLaMA-13B & 14.0 & 16.3 & 12.8 & 12.9 & 14.7 & 13.2 \\ G-LLaVA & LLaMA-7B & 15.7 & 22.2 & 20.4 & 16.5 & 12.7 & 6.6 \\ Internal-M-XC2 & InternLM-7B & 16.5 & 22.3 & 17.0 & 15.7 & 16.4 & 11.0 \\ Math-L1aVA & Vicuna-13B & 19.0 & 21.2 & 19.8 & 20.2 & 17.6 & 16.4 \\ ShareGPT4V & Vicuna-13B & 17.4 & 21.8 & 20.6 & 18.6 & 16.2 & 9.7 \\ LLaVA-NeXT & LLaMA-3B & 19.3 & 24.9 & 20.9 & 20.8 & 16.1 & 13.8 \\ ILaVA-NeXT & Qwen-1.5-110B & 24.5 & 31.7 & 24.1 & 24.0 & 22.1 & 20.7 \\ MAVIS & Mammoth-27B & 27.5 & 41.4 & 29.1 & 27.4 & 24.9 & 14.6 \\   \\  InfiMM-Math & DS-Coder-1.3B & 26.9 & 37.1 & 30.2 & 29.2 & 24.4 & 13.7 \\ InfiMM-Math & DS-Coder-1.5-7B & 34.5 & 46.7 & 32.4 & 38.1 & 32.4 & 15.8 \\   

Table 1: Evaluation of models on MathVerse.