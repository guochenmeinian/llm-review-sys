# Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature

Tong Zhou

Northeastern University, Boston

zhou.tong1@northeastern.edu

&Xuandong Zhao

UC Berkeley

xuandongzhao@berkeley.edu

&Xiaolin Xu

Northeastern University, Boston

x.xu@northeastern.edu

&Shaolei Ren

UC Riverside

shaolei@ucr.edu

###### Abstract

Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially leading to the wrongful attribution of blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability. Code is available at https://github.com/Tongzhou0101/Bileve-official.

## 1 Introduction

Watermarks have been envisioned as a promising method to differentiate content generated by large language models (LLMs) from human . It involves injecting statistical signals into the token sampling process utilizing a secret watermark key. Subsequently, the one who knows the key can verify the content's origin by assessing the presence of the predefined signal through a statistical test. Current watermarking schemes primarily focus on user-side concerns, striving to achieve robustness against watermark removal attacks (i.e., perturb the generated text to remove the watermark), thereby combatting academic dishonesty and other deceptive practices.

However, a critical vulnerability remains unaddressed in watermark design: _spoofing attacks directed towards model owners_. In these attacks, malicious actors attempt to falsely attribute content generated by humans or other models to the targeted model, with the aim of evading accountability1 or damaging the model's reputation. A few recent works have identified two kinds of spoofing attacks targeting the LLM watermark by exploiting either its symmetric characteristic  or learnability , as detailed in Sec. 2.3. Furthermore, we propose a new spoofing attack, named semantic manipulation,which enables attackers to alter the sentiment of generated content with minimal token modifications, as described in Sec. 3.2. It assumes the most constrained capabilities of attackers, where they only have access to the victim model's detector. Due to the robustness of LLM watermarks against perturbations, this attack can manipulate the originally helpful content into something harmful or offensive without compromising the detectability of the watermark, thus successfully achieving spoofing attacks.

Given the serious consequences of spoofing attacks, it is highly in demand to answer the question: _How to avoid an LLM being wrongly blamed?_ To solve this problem, we aim to design a watermark for LLM, which focuses more on the model owners' side instead of only watermarking on the users' side. To reliably identify the provenance of machine-generated content while being able to defend against the above spoofing attacks, a signature should have the following properties:

* **Robust:** The signature remains capable of tracing the source of machine-generated text even when subjected to certain perturbations, ensuring it is not overly fragile or easily rendered ineffective.
* **Unforgeable:** The signature is inherently resistant to being learned given the components utilized in its detection.
* **Tamper-evident:** It should be able to check the integrity of the generated content, showing reliable tampering evidence to safeguard the interests of model owners.
* **Transparent:** It is detectable without needing access to generation secrets or relying on a black-box API, allowing independent, reliable verification.

Despite the critical importance, achieving all desired properties in a single LLM watermark remains challenging, as even state-of-the-art (SOTA) designs cannot meet them all (see Tab. 1). Indeed, designing such a watermarking scheme involves a fundamental trade-off between defending against removal attacks and spoofing attacks. Specifically, being robust to removal attacks requires that the watermark's detectability remains unaffected by certain perturbations, while anti-spoofing demands sensitivity to perturbations to verify text integrity, distinguishing harmful content from genuine model output and tampered content.

To overcome the above challenges, we propose Bileve, a novel sampling strategy by embedding a bi-level signature into generated tokens. At the coarse-grained level, we utilize statistical signals across the entire text to detect the presence of the watermark, ensuring robustness against perturbations. Concurrently, at the fine-grained level, we integrate content-dependent signature bits into each token to uphold content integrity, which leverages a digital signature scheme to ensure unforgeability, as the secret key required for watermark embedding will be securely held by model owners. This scheme enables transparent detection by allowing verification with a public key instead of embedding secrets, so independent parties can authenticate without proprietary details or a black-box API, ensuring reliable detection. And the tampering evidence will show when these two level detection results are not consistent.

Our contributions are threefold: 1) We uncover an advanced spoofing attack that exploits the robustness of SOTA watermarking schemes; 2) We introduce Bileve, the first watermarking scheme to simultaneously ensure robustness and unforgeability by embedding a bi-level signature through a novel rank-based sampling strategy; 3) Bileve is capable of distinguishing five distinct scenarios during the detection phase, effectively defeating spoofing attacks and serving as a promising tool to regulate LLM safety mechanism.

## 2 Background and Related Works

### Language Model Basics

Let \(\) denote a language model with a vocabulary \(\) containing \(K:=||\) tokens. To generate the next token \(w_{t}\), \(\) will take prior tokens \(w_{1:t-1}\) as the input and output a vector of logits \(l^{(l)}\)

  
**Methods** & **Robust** & **Unforgeable** & **Tamper-evident** & **Transparent** \\  Kirchenbauer _et al._ & ✓ & ✗ & ✗ & ✗ \\ Zhao _et al._ & ✓ & ✗ & ✗ & ✗ \\ Kuditipud _et al._ & ✓ & ✗ & ✗ & ✗ \\ Liu _et al._ & ✓ & ✗ & ✗ & ✗ \\ Fairoze _et al._ & ✗ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of different watermarking methods based on four desired properties.

which is transformed into a probability distribution \(^{(t)}=(p_{1}^{(t)},...,p_{K}^{(t)})\) via the softmax function. Then the sampling strategy is applied to determine how the model selects \(_{t}\) based on \(^{(t)}\). One common sampling strategy is multinomial sampling, where \(\) randomly selects the next token from \(\) according to the probabilities \(p_{k}^{(t)}\) assigned to each token. This process is repeated iteratively to generate a sequence of tokens.

### LLM Watermarks

Watermarks for model-generated texts are used to identify the provenance of the text, ensuring accountability in cases where generated content needs to be traced back to a specific LLM. The existing watermark schemes rely on the specialized decoding algorithm to embed statistical signals into generated contents, then enabling watermark detection via computing p-value [1; 11; 23; 4; 13]. For instance, for generating the next token, one approach dynamically partitions the vocabulary into green and red lists based on its previous few tokens and a watermark key , then increasing the logits of green tokens to enhance their chance of being selected. During detection, the watermark detection key is used to count the number of green tokens in the text, with the calculated z-statistic indicating the existence of the watermark. Moreover, Zhao _et al_.  simplify the scheme proposed in  by fixing the green-red list for each token, demonstrating that their watermark is twice as robust to edit as . Furthermore, unlike modifying logits, a distortion-free watermark is proposed to preserve the original text distribution . It leverages robust sequence alignment to align watermarked text to a watermark key sequence in the sampling phase, e.g., using exponential minimum sampling.

However, these watermarking schemes only enable detection by individuals possessing the key, which doesn't facilitate transparent regulation. On one hand, making the key public is susceptible to attacks . On the other hand, maintaining detection privately (e.g., via APIs) compromises reliability, as it functions as a black box, allowing the model owner to manipulate detection results.

### Spoofing Attacks

Spoofing attacks can fall into three categories based on the capability of attackers, and each of them exploits different vulnerabilities in SOTA watermarks, as summarized in Tab. 2. First, due to the watermark embedding and detection process sharing the same secret key (i.e., symmetric schemes), the semi-honest detector knowing the secret key can embed the watermark to any content. Such a vulnerability has been discussed in [6; 15], where they design asymmetric watermarking schemes so that detection does not rely on the key used for embedding. Specifically,  proposes embedding watermarks using digital signature algorithms, ensuring that only model owners possess the secret key for watermark embedding while providing detectors with access to the public key for detection. However, it is easily broken once the message tokens are perturbed. Besides, it suggests using error-correcting encoding to improve robustness, which unfortunately would increase the risk of spoofing attacks and should not be adopted. Moreover,  employs two distinct neural networks for watermark embedding and detection, leveraging an asymmetric scheme for public detection.

However,  is based on  and thus can be learned as studied in , which is the second kind of spoofing attacks. In particular,  demonstrated that, by querying the victim model and collecting its watermarked samples, attackers can fine-tune an adversary model utilizing a sampling-based watermark distillation technique to learn the watermark (detailed in Appendix A). The fine-tuned adversary model can respond to any malicious requests, with the response containing the watermark of the victim model. Recently, an independent work also proposed attacks by exploiting watermark robustness . Beyond techniques such as randomly inserting toxic tokens or modifying tokens to alter sentence accuracy, our attack leverages a reward model to guide targeted semantic manipulation. More importantly, while they suggest compromising watermark robustness to mitigate spoofing attacks, our work demonstrates how to achieve effective mitigation without sacrificing robustness.

  
**Methods** & **Exploited Vulnerabilities** & **Attackers’ Capabilities** \\ 
[6; 15] & Symmetry & Know the secret key for embedding watermarks \\
[8; 10] & Learnability & Get access to the victim model and query it multiple times \\ Our attack & Robustness & Only get access to the victim models’ detector \\   

Table 2: Three categories of spoofing attacks (ranked by attackers’ capabilities from high to low).

Potential Attack: Semantic Manipulation

### Threat Model

**Attackers' Objective.** Given text generated by the victim LLM, attackers seek to alter the semantic meaning of the text with minimal changes, transitioning it from something helpful or neutral to harmful or offensive. Owing to the robustness of the existing watermarks, the watermark detector can still identify the presence of the watermark in the modified content. Consequently, the altered content erroneously attributes its origin to the victim model, potentially damaging the model's reputation.

**Attackers' Capability.** Contrary to existing spoofing attacks, our approach assumes the strictest attacker capabilities, where the adversary only gains access to the watermark detector, as outlined in Tab. 2. These attackers lack knowledge of the secret key and are not required to query the victim LLM multiple times to acquire watermarked samples for training other adversary models. They may utilize public language models to enhance the efficiency of their attacks.

### Attack Method

We identify the dilemma of being robust and unforgeable. In particular, being robust indicates that the watermark should be preserved after perturbation, thus we can exploit this characteristic to design a novel spoofing attack. Specifically, attackers can query the victim model with harmless prompts, and then use basic word replacement techniques to change its semantic meaning to be toxic or harmful. _Due to the robustness properties of LLM watermarks, the detectability will not be compromised if the portion of word replacement is low._ Consequently, a detector cannot discern whether the content originated from the victim model or was manipulated by malicious actors. This highlights the limitation of current watermarks for auditing LLMs.

By exploiting the above observation, we propose a novel spoofing attack. Let \(^{}\) denote the original response of victim models, and \(^{}\) represent its manipulated version. The goal is to generate \(^{}\) that maximizes the change in sentiment while minimizing the Levenshtein distance between the original and manipulated responses. The problem can be formulated as follows:

\[_{^{}} R=R(^{})-R(^{}),\ \ \ \ (^{},^{}) T\] (1)

Here, \( R\) represents the sentiment change, defined as the difference between the reward scores (denoted by \(R()\)) of the original and manipulated responses obtained by the reward model.2 A lower score of the reward model indicates less alignment with human feedback, such as a toxic response. The Levenshtein distance, denoted by \((t_{1},t_{2})\), measures the minimum number of word edits required to transform text \(t_{1}\) into text \(t_{2}\). \(T\) is the length of \(^{}\) and \(\) is the word edit budget. A trade-off exists in choosing \(\), wherein a larger value affords greater flexibility in manipulating the semantic meaning of \(^{}\), while a smaller value better preserves the detectability of the watermarks. To strike a balance, we opt for a larger \(\) to maximize semantic alteration and introduce a tuning factor \((0,1)\) to adjust \(\) in case the detectability is broken. Furthermore, instead of manually replacing the words in \(^{}\), attackers can simply leverage a powerful and accessible LLM (denoted as \(Q\)) to execute such attacks efficiently. To enhance the generation quality while meeting the constraint, attackers can apply in-context learning by providing a few task demonstrations. We summarize the algorithm with more details of task demonstrations in Appendix B.

## 4 Proposed Defense

With the above attack, in conjunction with other existing spoofing attacks, we can recognize the vulnerability of current watermark schemes. This underscores the importance of designing secure schemes to defend against spoofing attacks and achieve all properties listed in Tab. 1, ensuring reliable identification of text provenance.

### Single-level Signature

To enable secure and reliable text attribution, we first examine the vulnerabilities exploited by attackers in conducting spoofing attacks, including symmetry, learnability, and robustness, as summarized in Tab. 2. In particular, previous methods embed the statistical signal into the generated texts [11; 23; 13; 1], so as to identify the existence of a watermark during detection. Such statistical signal is consistent for every text, thus enabling the adversary model to learn the watermark rule can forge it. Therefore, to defend against spoofing attacks, we seek the opposite characteristics, i.e., _asymmetric_, _unlearnable_, and _perturbation-sensitive_, in the solution.

We envision the digital signature as a promising solution as in , where the scheme is defined as \(=(,,,)\), where:

* \((,)\) outputs a public key pair \((,)\), where \(\) is public while \(\) is held by the owner of model \(\).
* \(_{}()\) uses \(\) to sign the digest of the message \(:=_{1:m}\) via the hash function \(H\) (e.g., MD5) and obtain the signature \(=()\).
* \(()\) embeds signature into subsequent tokens following \(_{m}\), and outputs \(\) incorporating the message-signature pair \((,)\).
* \(_{}()\{, \}\) extracts \(= H()\) from \(\) and verifies it using the public key \(\). If verification succeeds, it outputs \(\); otherwise, it outputs \(\).

Unlike the digital signature methods, which typically attach signatures as metadata [20; 19], \(\) assigns the first few tokens as the message and uses the following tokens to carry the signatures. Specifically, the key idea of \(\) is to embed signature bits into tokens, ensuring that the block hashes to the corresponding signature bit (e.g., employing rejection sampling until the hash \(h\) result matches the next signature bit). This method keeps the message-signature pair self-contained within the generated text, enabling verification solely based on the generated content. Such a scheme satisfies the above characteristics since a digital signature uses the secret key for embedding and the public key for verification, ensuring asymmetry. The signature is content-dependent, so the signature for different generations is also different. Also, it is dependent on the secret key, which cannot be inferred by attackers, making it impossible to learn. Its ability to check integrity is proved in cryptography, where even a single modification will cause verification failure.

However, two problems arise in this scheme: 1) digital signature is too fragile, which hinders its applicability to the real world for attributing the text. In particular, even a single token insertion or deletion would lead to a verification failure, and the trace of the target LLM will easily disappear. 2) In cases where token replacement occurs and the replaced token hashes to the same signature bit as the original token, the signature remains unaffected. However, such replacements undermine the text's integrity without detection, which is referred to as the "signature preservation attack".

Figure 1: **Overview of \(\).****(a) Embedding:** The first \(m\) tokens from \(\) form the message, which is signed using a secret key. Candidate tokens are selected via a rank-based strategy employing a Weighted Rank Addition (WRA) score, with a coarse-grained signal embedded. It then embeds the fine-grained signature by choosing the first candidate matching the designated signature bit. **(b) Detection:** We first extract the message-signature pair to conduct an integrity check using the public key. A statistical test is performed if necessary.

```
0: Language model \(\), secret key \(\), message length \(m\), random key sequence \(\)
1: Apply cyclic shift to \(\)
2:for\(t=1,,m\)do
3: Apply \(\) to prior tokens and sample \(w_{t}\) with \(_{t}\) involved (Eq. 2)
4:endfor
5: Apply a hash function on \(w_{1:m}\) to get the digest of message
6: Use \(\) to sign the digest to obtain the signature and convert it into a bit string \(B\)
7:for\(t=m+1,,m+b+1\)do
8: Apply \(\) to prior tokens to get a score vector \(WRA^{(t)}\) over \(\)
9:\(\{w_{t,1},,w_{t,K}\}\)\(\) Sorted tokens based on their logits in descending order
10:for\(k=1\) to \(K\)do
11:if\(h(w_{t,k})=B_{t-m}\)then
12:\(w_{t} w_{t,k}\); break
13:endfor
14:endfor ```

**Algorithm 1** Rank-based Sampling Strategy in Bileve

### Bi-level Signature (Bileve)

We introduce Bileve, a bi-level signature scheme that improves upon the SLS in terms of detectability and security. At the fine-grained level, Bileve embeds the message-signature pair to verify content integrity, while the coarse-grained level incorporates a robust signal to boost detectability. Following [1; 13], we design the signal as a random watermark key sequence \((^{K})\). We propose a ranking-based sampling strategy to embed \(\) into generated tokens, where the objective is to let the randomness affect the sampling outcome but the selected token is also expected to have a large probability of preserving the generation quality.

**Generation.** We propose a weighted rank addition (\(WRA\)) score for each token in \(\) to rank the candidate tokens instead of ranking them based on probability like conventional methods . In particular, given a probability vector \(p\) of \(w_{t}\) and a pre-defined random sequence \(\) (both of dimension \(K\)), \(WRA\) is calculated by (we omit \(t\) for simplicity):

\[WRA_{k}=(P_{k})+(_{k}), k[1,K]\] (2)

where \((p_{k})\) and \((_{k})\) are the rank scores for \(k\)-th token based on \(p\) and \(\), respectively, determined by their order when values are sorted in ascending order (e.g., if \(p_{k}\) is the smallest one in \(p\), then \((p_{k})\) is 0). Besides, by adjusting the hyperparameter \(\) (where \(<1\)), we enhance the impact of higher probabilities while still allowing for randomness to affect the outcome. During generation, we rank token candidates by favoring larger \(WRA\) ( in Fig. 1). When sampling tokens carrying signature bits, we incorporate an additional signature bit matching step by selecting the first candidate token that, through the hash function \(h\), maps to the predetermined signature bit ( in Fig. 1).

Besides, we enhance the diversity of generation by using the shift-generate algorithm  (detained in Appendix C). This involves pre-generating \(n\)\(\) sequences and iteratively decoding tokens using sequences \(=(^{d},\ ^{d+1},\...,\ ^{n},\ ^{b},\...,\ ^{d-1})\), where \(d[0,n)\) shifts with each new response generation. Such a shifting strategy ensures that \(\) can generate diverse tokens even if their prefix tokens are the same, and iterative decoding ensures that generated tokens \(w\) align well with \(\). The rank-based sampling strategy with shift-generate is summarized in Alg. 1. Thus, although a signature preservation attack may maintain alignment with the signature, it is less likely to simultaneously align well with \(\) sequences, thereby effectively mitigating such attacks.

**Statistical Test.** Following , we define the alignment cost as

\[d(w,)\,:=_{t=1}^{T}(1-_{t,w_{t}})\] (3)

If the text \(w\) generated by \(\), \(_{t,w_{t}}\) will be large due to Eq. 2, then \(d\) will be smaller compared to human-generated text or text from other models. Thus, we test \(w_{t}\) with random \(^{}\) for \(N\) times, and got p-value as \((1+_{i=1}^{N}1\{d(w,^{}) d(w,)\})\) for the null hypothesis that \(w\) is not generated by \(\). Hence, a small p-value (e.g., <0.01 when \(N\)=100) indicates \(w\) is high likely from \(\). For checking the signature preservation attack, we run a _local alignment_, i.e., splitting \(w\) into several segments, if the p-value for a certain segment is larger than the rest, then it indicates the token replacement happens in that segment with their associated signature bits unchanged. When signature validation fails, we run a _global alignment_ test, with Eq. 2 enhanced by Levenshtein distance to be robust against insertion and deletion, as detailed in Appendix D.

**Detection.** With detectors getting access to \(\), \(h\), and \(\), they will apply two primary methods during detection: extracting the message-signature pair for integrity verification using the public key \(\), and conducting statistical tests. The verification process is described as follows: **Step 1**: Check the signature at a fine-grained level ( 1 in Fig. 1). If the signature is valid and model owners raise no doubts, verification is completed, and the text attribution is assigned to the target LLM (_Case 1_). **Step 2**: If the signature is valid but the model owner identifies suspicious content (e.g., potentially offensive material not in line with their model's safety mechanisms), they can conduct a local alignment test ( 2 in Fig. 1). Abnormal results suggest signature replacement (_Case 2_), while normal results suggest that there is a high chance that the safety mechanisms of target LLM require improvement (_Case 3_). **Step 3**: If the signature is invalid, examine the coarse-grained signal through a global alignment test ( 2 in Fig. 1). A small p-value serves as tampering evidence that the content originates from the targeted LLM but has been altered (_Case 4_). Otherwise, it suggests the text originates from a source other than the targeted LLM (_Case 5_). Overall, Bileve can differentiate 5 cases with the bi-level signature, reliably tracing the text provenance with mitigating spoofing attacks.

## 5 Experiments

In this section, we evaluate our approach from multiple perspectives, including detectability, generation quality, and security. Specifically, given that our method is asymmetric and unlearnable due to its cryptographic design, our focus is solely on assessing its efficacy in defending against spoofing attacks that exploit robustness, i.e., semantic manipulation. Additionally, we demonstrate the effectiveness of the bi-level signature in tackling the challenges encountered by the single-level signature, i.e., fragility and signature preservation attacks.

### Experimental Setup

**Datasets and Models.** We conduct experiments using two publicly available LLMs: OPT-1.3B  and LLaMA-7B . Our evaluation employs two datasets: 1) OpenGen  for text completion task, consisting of 3K two-sentence samples from WikiText-103 , with the first sentence as the prompt and the second as the human completion; 2) LFQA  for long-form question answering task, consisting of 3K question-answer pairs, where we use questions as prompt and answers as human-written answers in experiments.

**Evaluation.** To measure detectability, we use metrics, including the True Positive Rate (TPR), False Positive Rate (FPR), and F-1 score. We use LLaMA-13B as the oracle language model to compute perplexity (PPL) for evaluating the generation quality, which is defined as the exponentiated average negative log-likelihood of a sequence.

**Schemes.** To assess the effectiveness of Bileve, we conduct a comparative analysis with two state-of-the-art schemes. The first scheme, Unigram , stands out for its robustness against removal attacks. The second scheme, as proposed in , employs cryptographic techniques to defeat spoofing attacks, denoted as the SLS in this work.

**Settings.** For Unigram, we set watermark strength to 2.0 and a green list ratio to 0.5, where the threshold of z-score for detection is 6.0 and set FPR as 0.01 during detection. The nucleus sampling  is employed to introduce randomness for Unigram and SLS. Also, for SLS, we generate 300 tokens with the first 44 tokens as the message and the rest 256 tokens as the signature bit (the signature length for Bileve is 256-bit). This also applies to Bileve, except we use rank-based sampling with the \(\) set to 0.001. We set \(n\) for shift-generate to 300 and \(N\)=100 for detection. All experiments are conducted on NVIDIA A100 GPUs.

### Detectability

As demonstrated in Tab. 3, we evaluate the detectability of each scheme under two scenarios: no edits to the generated text, and editing involving 10% of the tokens (through random deletion, addition, and replacement). In the unedited scenario, both SLS and Bileve surpass Unigram in FPR and F1 scores. This superiority is due to the use of digital signatures in SLS and Bileve, which ensure integrity by making the signature \(\) content-dependent on \(\) and signed by sk. This setup prevents texts not produced by the target LLM from passing verification with pk.

Furthermore, Bileve excels when 10% of tokens are edited, maintaining a high F1 score (0.999) and achieving an FPR of 0. This contrasts sharply with SLS, whose F1 score becomes inapplicable due to both TPR and FPR dropping to 0, illustrating the fragility of the SLS scheme. In contrast, Bileve can leverage coarse-grained level signal to test global alignment with \(\). The resulting p-value < 0.01 indicates the source of perturbed text is from the target LLM. The failure of verification caused by disrupted message-signature pair along with the small p-value serve as the tampering evidence for texts from target LLM. Furthermore, alignment cost analysis in Bileve (Fig. 2) shows machine-generated texts aligning with the key sequence \(\) incur lower costs than human-written texts, aiding in provenance tracing and distinguishing _Case 5_.

### Generation Quality

We measure the perplexity of texts generated by various schemes, with results of OPT-1.3B shown in Fig. 3 and LLaMA shown in Appendix E. The perplexity of Unigram is close to that of human text, which serves as our baseline. In contrast, the perplexities of SLS and Bileve are relatively higher. This increase is attributed to the need for embedding digital signature bits into tokens precisely. Such embedding may lead to the selection of tokens that, while matching the signature bits, are not the optimal choice, thus increasing perplexity. Notably, Bileve uses rank-based sampling with shift-generate instead of SLS's nucleus sampling, achieving a 23.08% perplexity reduction on OpenGen using OPT-1.3B, as tokens with higher \(W\,RA\) scores better preserve textual coherence.

While our method exhibits higher perplexity than Unigram, human evaluation reveals no noticeable degradation in generation quality, with examples available in Appendix F. This discrepancy may result from Unigram's lower perplexity due to repetitive text generation, as recent studies indicate that model perplexity often favors repetition . To further assess quality, we conduct zero-shot evaluations using GPT-4 Turbo, following the approach in , where higher scores represent better quality. On the question-answering task with OPT-1.3B, Bileve and Unigram achieve scores of 16\(\)6.52 and 16\(\)9.62, respectively.

    &  &  &  \\   & & TPR \(\) & FPR \(\) & F1 \(\) & TPR \(\) & FPR \(\) & F1 \(\) \\   & Unigram & 1.000 & 0.010 & 0.995 & 1.000 & 0.010 & 0.995 \\  & SLS & 1.000 & 0.000 & 1.000 & 1.000 & 0.000 & 1.000 \\  & Bileve & 1.000 & 0.000 & 1.000 & 1.000 & 0.000 & 1.000 \\   & Unigram & 0.992 & 0.010 & 0.991 & 0.997 & 0.010 & 0.994 \\  & SLS & 0.000 & 0.000 & / & 0.000 & 0.000 & / \\   & Bileve & 0.998 & 0.000 & 0.999 & 0.999 & 0.000 & 0.999 \\   

Table 3: The detectability of different schemes with OPT-1.3B.

### Security

**Against Signature Preservation Attack** The signature preservation attack occurs only when attackers replace tokens in a way that satisfies Line 11 in Alg. 1. This is challenging, as attackers have to find tokens also maintain contextual coherence at the same time. We demonstrate that, although rare, when attackers meet these conditions, Bileve can detect such attacks through local alignment testing. We split \(\) into 5 segments and perform the signature preservation attack on the third one as a case study. The local alignment test returns a p-value for each segment, as shown in Fig. 4. Using the p-values of the rest segments as baselines, segment 3 has an abnormally high p-value, indicating misalignment with key sequence \(\). Moreover, we further explore the alignment cost and show the best 10 alignment scores among 300 shifts, with the lowest cost as the deciding factor. The results in Fig. 4 show the best alignment cost of segment 3 after the attack has increased from -2.5 to -1.3, further uncovering the misalignment caused by signature preservation attacks.

**Against Semantic Manipulation** Here, we conduct a semantic manipulation attack on Unigram, SLS, and Bileve. Note that even a single successful attack could jeopardize the model's reputation. As shown in Tab. 4, due to the robustness of Unigram, after replacing a few tokens to change the tone from neutral to offensive, the z-score remains high enough (>6) to show the watermark, demonstrating its inefficacy in differentiating such spoof attacks. In the case of SLS, semantic manipulation perturbs embedded signature bits, leading to verification failure. If the message tokens are perturbed, the verification fails as well due to the unmatched results (see Fig. 1 (b)). In contrast, although signature bits in Bileve are also perturbed by attacks, the small p-value of the statistical test indicates the text is generated by the target LLM (with tampering since digital signature verification fails).

## 6 Discussion

**Limitation and Potential Optimization** While the effectiveness and security are well demonstrated, the efficiency and generation quality of Bileve can be improved by adopting the following strategies. Firstly, it is unnecessary to apply Bileve for prompts characterized by low entropy, such as those involving manual token replacements. Secondly, we can embed multiple message-signature pairs in longer outputs or use digital signature schemes with shorter signature lengths for shorter outputs. Thirdly, similar to , we can adopt an adaptive embedding strategy, i.e., signature embedding into tokens can be skipped when their entropy levels are low, thus maintaining the natural flow of the

  Schemes & Texts for Detection & Metric \\   & 1. We are many times better people 2. We are few times more intelligent & z-score: 10.9 \\  & than cats 3. We are easier to deal with [continue...] & z-score: 10.9 \\   & 1. Men are many times better people 2. Men are few times more intelligent than women 3. Women are harder to deal with [continue...] & z-score: 10.5 \\  SLS (w/o) & [message tokens] disagree with your opinion because you... & \(\): 10101011... \\  SLS (w/) & [message tokens] disagree with your stupid opinion because you... & \(\): 10101101... \\   & [message tokens] You are mature, especially mentally mature; You are well manner, version normal silence [continue...] & \(\): 1000100... \\   & [message tokens] You are awful, especially mentally sick; We are well manner, versing normal silence [continue...] & \(\):1010101... \\  

Table 4: The results of semantic manipulation attacks, with w/o indicting benign generations and w/ indicating semantic manipulation attacks by perturbing tokens (shown in red).

Figure 4: The p-value and alignment cost of each segment.

text. Lastly, we can embed a single signature bit across a block of tokens, rather than into individual tokens, which is promising to improve text perplexity by reducing disruptions in token coherence .

**Societal Impact** Reliably tracing text provenance is crucial for trust and accountability in LLM usage. Unlike previous mechanisms that only yield binary results--whether text originates from target LLMs--Bileve can distinguish five scenarios, enhancing the defense against spoofing attacks and improving LLM regulation. Bileve effectively differentiates between jailbreaking (bypassing safety mechanisms to generate harmful content ) and spoofing (altering benign outputs to create harmful content ), which can damage an AI's reputation. By embedding bi-level signatures, Bileve not only preserves content integrity but also detects tampering, clearly identifying genuine security breaches from fraudulent imitations. Thus, Bileve advances the societal goals of ensuring safe, transparent, and accountable LLM regulation.

## 7 Conclusion

In this work, we propose a bi-level signature scheme, named Bileve, which integrates robust statistical signals with fine-grained signature bits, ensuring that the watermark remains detectable through perturbations while simultaneously verifying content integrity. The explicit tampering evidence generated by our watermark helps safeguard model owners' interests and enhances the accountability mechanisms necessary for ethical LLM utilization. As demonstrated in experiments, Bileve not only maintains generation quality but also supports robust, tamper-evident signatures that can discern between genuine and manipulated content. Overall, our approach represents a significant step forward in regulating LLMs, promoting safer deployments, and ensuring that these powerful technologies are used responsibly and transparently.

## 8 Acknowledgement

This work is supported in part by the U.S. National Science Foundation under Grants OAC-2319962, CNS-2239672, CNS-2153690, CNS-2326597, CNS-2247892, and CNS-2326598.