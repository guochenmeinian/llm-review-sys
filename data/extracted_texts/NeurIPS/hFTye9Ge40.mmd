# Fixed Confidence Best Arm Identification in the Bayesian Setting

Kyoungseok Jang

Universita degli Studi di Milano

ksajks@gmail.com

&Junpei Komiyama

New York University / RIKEN AIP

junpei@komiyama.info

&Kazutoshi Yamazaki

The University of Queensland

k.yamazaki@uq.edu.au

###### Abstract

We consider the fixed-confidence best arm identification (FC-BAI) problem in the Bayesian setting. This problem aims to find the arm of the largest mean with a fixed confidence level when the bandit model has been sampled from the known prior. Most studies on the FC-BAI problem have been conducted in the frequentist setting, where the bandit model is predetermined before the game starts. We show that the traditional FC-BAI algorithms studied in the frequentist setting, such as track-and-stop and top-two algorithms, result in arbitrarily suboptimal performances in the Bayesian setting. We also obtain a lower bound of the expected number of samples in the Bayesian setting and introduce a variant of successive elimination that has a matching performance with the lower bound up to a logarithmic factor. Simulations verify the theoretical results.

## 1 Introduction

In many sequential decision-making problems, the learner repeatedly chooses an arm (option) to play with and observes a reward drawn from the unknown distribution of the corresponding arm. One of the most widely-studied instances of such problems is the multi-armed bandit problem , where the goal is to maximize the sum of rewards during the rounds. Since the learner does not know the distribution of rewards, they need to explore the different arms, and yet, exploit the arms of the most rewarding arms so far. Different from the classical bandit formulation, there are situations where one is more interested in collecting information rather than maximizing intermediate rewards. The best arm identification (BAI) is a sequential decision-making problem in which the learner is only interested in identifying the arm with the highest mean reward. While the origin of this problem dates back to at least the 1950s , recent work in the field of machine learning reformulated the problem . In the BAI, the learner needs to pull arms efficiently for better identification. To achieve efficiency and accuracy, the learner should determine which arm to choose based on the history, when to stop the sampling, and which arm to recommend as the learner's final decision.

There are two types of BAI problems depending on the optimization objective. In the fixed-budget (FB) setting , the learner attempts to minimize the probability of error (misidentification of the best arm) given a limited number of arm pulls \(T\). In the fixed confidence (FC) setting , the learner attempts to minimize the number of arm pulls, subject to a predefined probability of error \((0,1)\). In this paper, we shall focus on the FC setting, which is useful when we desire a rigorous statistical guarantee.

Most of the previous BAI studies focus on the frequentist setting, where the bandit model is chosen adversarially from some hypothesis class beforehand. In this setting, several algorithms, such as Track and Stop (Kaufmann et al., 2016a) and Top-two algorithms (Russo, 2016; Qin et al., 2017b; Jourdan et al., 2022), are widely known. These algorithms have an optimal sample complexity, meaning that they are one of the most sample-efficient algorithms among the class of \(\)-correct algorithms.

The sample complexity of these algorithms is problem-dependent. To see this, consider the following example.

**Example 1**.: (A/B/C testing) Consider A/B/C testing of web designs. We have three arms (web designs) from which we would like to find the largest retention rate via allocating users to web designs \(i=1,2,3\). If we attempt to find the best arm with confidence \(\), we may need a large number of samples (users) when the suboptimality gap (the gap between the retention rate of the best arm and the second best arm) is small because in such a case the identification of the best arm is difficult - the minimum number of samples required is inversely proportional to the square of the suboptimality gap(Kaufmann et al., 2014). For example, when comparing the testing of retention rates of (0.9, 0.5, 0.1) with (0.9, 0.89, 0.1), the second case requires around \(()^{2}=1600\) times more samples compared to the first case.

In practice, the retention rate of \(0.89\) in the second case may be acceptably good compared to the optimal retention rate of \(0.9\), and we may stop exploration at the moment the learner identifies a reasonably good arm, which is the first or the second arm in this example. This idea is formalized in several ways. The literature of Ranking and Selection (R&S) usually considers the indifference-zone formulation (Hong et al., 2021). In the context of best arm identification, a similar notion of \(\)-best answer identification has also been considered (Maron and Moore, 1993; Even-Dar et al., 2006; Gabillon et al., 2012; Kaufmann and Kalyanakrishnan, 2013; Jourdan et al., 2023). In these settings, the learner accepts a sub-optimal arm whose means are at most \(\) worse than the mean of the optimal arm. Other related settings include the good arm identification problem (Kano et al., 2019; Tabata et al., 2020; Zhao et al., 2023), where the goal is to identify an arm that exceeds the predefined threshold, and the thresholding bandit problem (Locatelli et al., 2016; Xu et al., 2019), where the goal is to identify whether the mean of each arm is above or below the threshold. All these problem settings require an extra parameter, like \(\) or an acceptance threshold, that directly determines the acceptance level. Even though the algorithm's performance depends on this parameter, it is often challenging to determine a reasonable value for it in advance.

In this paper, we study an alternative approach based on the Bayesian setting. In particular, we consider the prior distribution on the model parameters. We relax the requirement on the correctness of the best arm identification by using the prior belief. Rather than requiring the frequentist \(\)-correctness for any model, we require the learner to have marginalized correctness over the prior distribution, which we call Bayesian \(\)-correctness.

We study the fixed confidence BAI (FC-BAI) problem in the Bayesian setting. Our contributions are as follows.

* **First,** we find that in the Bayesian setting, the performance of the traditional frequentist setting-based algorithms, such as Track and Stop and Top-two algorithms, can be arbitrarily worse (Section 3). This is because frequentist approaches spend too many resources when the suboptimality gap is narrow.
* **Second,** we prove that the lower bound of the number of expected samples should attain at least the order of \(()^{2}}{})\) as \( 0\) (Section 4). Here \(L()\) is our novel quantity that represents the sample complexity with respect to the prior distribution \(\). This order is different from the existing lower bound in the frequentist setting1, implying that the Bayesian setting is essentially different from the frequentist setting. * **Third,** we design an algorithm whose expected sample size is upper-bounded by \(O()^{2}}{})}{})\) (Section 5). Our algorithm is based on the elimination algorithm (Maron and Moore, 1993; Even-Dar et al., 2006; Frazier, 2014), but we add an early stopping criterion to prevent over-commitment of the algorithm for a bandit model with a narrow suboptimality gap. Our algorithm has a matching upper bound up to the logarithmic factor.

We also conduct simulation to demonstrate that the sample complexity of frequentist algorithms does indeed diverge in a bandit model with a small suboptimality gap, even in very simple cases (Section 6).

### Related work

To our knowledge, BAI problems studied for the Bayesian setting have been limited to the fixed budget setting (Komiyama et al., 2023; Asidakou et al., 2023). Komiyama et al. (2023) showed that, in the fixed-budget setting, a simple non-Bayesian algorithm has an optimal simple regret up to a constant factor, implying that the advantage the learner could get from the prior is small when the budget is large. This is very different from our fixed-confidence setting, where utilizing the prior distribution is necessary.

Several FC-BAI algorithms used Bayesian ideas on the structure of the algorithm, although most of those studies used frequentist settings for measuring the guarantee. The 'Top-Two' type of algorithms are the leading representatives in this direction. The first instance of top-two algorithms, which is called Top-Two Thompson sampling (TTTS), is introduced in the context of Bayesian best arm identification. TTTS requires a prior distribution, and Russo (2016) showed that the sample complexity of posterior convergence of TTTS which is the same as the sample complexity of the frequentist fixed-confidence best arm identification. Subsequent research analyzed the performance of TTTS from the frequentists' viewpoint (Shang et al., 2020). Later on, the idea of top-two sampling is then extended into many other algorithms, such as Top-Two Transportation Cost (Shang et al., 2020), Top-Two Expected Improvement (TTEI, Qin et al., 2017), Top-Two Upper Confidence bound (TTUCB, Jourdan and Degenne 2022). Even though some of the top two algorithms adapt a prior, they implicitly solve the optimization that is justified in view of frequentist.

Another line of Bayesian sequential decision-making is Bayesian optimization (Srinivas et al., 2010; Mockus, 2012; Shahriari et al., 2016; Jamieson and Talwalkar, 2016; Frazier, 2018), where the goal is to find the best arm in Bayesian setting. Note that Bayesian optimization tends to deal with structured identification, especially for Gaussian processes, and most of the algorithms for Bayesian optimization do not have specific stopping criteria.

## 2 Problem setup

We study the fixed confidence best arm identification problem (FC-BAI) in a Bayesian setting. In this setup, we have \(k\) arms in the set \([k]:=\{1,2,,k\}\) with _unknown_ distribution \(=(P_{1},,P_{k})\) which is drawn from a _known_ prior distribution at time 0, namely \(=(H_{1},,H_{k})\). The unknown bandit model \(P_{i}\) is a one-parameter distribution, and \(\) is specified by \(:=(_{1},,_{k})\). To simplify the problem, we will focus on the Gaussian case, where each \(P_{i}\) is a Gaussian distribution with known variance \(_{i}^{2}\). Each mean of \(P_{i}\), denoted \(_{i}\), is drawn from a known prior Gaussian distribution \(H_{i}\), which can be written as \(N(m_{i},_{i}^{2})\).

At every time step \(t=1,2,\), the forecaster chooses an arm \(A_{t}[k]\) and observes a reward \(X_{t}\), which is drawn independently from \(P_{A_{t}}\). Since we focus on the Gaussian case, \(X_{t} N(_{A_{t}},_{A_{t}}^{2})\) conditionally given \(A_{t}\) and \(_{A_{t}}\). After each sampling, the forecaster must decide whether to continue the sampling process or stop sampling and make a recommendation \(J[k]\).

Let \(_{t}=(A_{1},X_{1},A_{2},X_{2},,A_{t},X_{t})\) be the \(\)-field generated by observations up to time \(t\). The algorithm of the forecaster \(:=((A_{t})_{t},,J)\) is defined by the following triplet (Kaufmann et al., 2016):

* A sampling rule \((A_{t})_{t}\), which determines the arm to draw at round \(t\) based on the previous history (each \(A_{t}\) must be \(_{t-1}\) measurable).
* A stopping rule \(\), which means when to stop the sampling (i.e., stopping time with respect to \(_{t}\)).
* A decision rule \(J\), which determines the arm the forecaster recommends based on his sampling history (i.e., \(J\) is \(_{}\)-measurable).

In FC-BAI, the forecaster aims to recommend arm \(J\) that correctly identifies (one of) the best arm(s) \(i^{*}():=_{i[k]}_{i}\) with probability at least \(1-\). Since the case of multiple best arms is of measure zero under \(\), we can focus on \(\) such that \(i^{*}()\) is unique. For the FC-BAI problem in the Bayesian setting, we use the _expected_ probability of misidentification:

\[(;):=_{} J i^{*}()|_{},\] (1)

where \(_{}:=\{\) is the correct bandit model\(\}\). Now we formally define the algorithm of interest as follows:

**Definition 1**.: (Bayesian \(\)-correctness) For a prior distribution \(\), an algorithm \(=((A_{t}),,J)\) is said to be Bayesian \((,)\)-correct if it satisfies \((;)\). Let \(^{b}(,)\) be the set of Bayesian \((,)\)-correct algorithms for the prior distribution \(\).

The objective of the FC-BAI problem in the Bayesian setting is to find an algorithm \(=((A_{t})_{t},,J)^{b}(,)\) that minimizes \(_{}[]\).

TerminologyDefine \(N_{i}(t)=_{s=1}^{t-1}[A_{s}=i]\) as the number of times arm \(i\) is pulled before timestep \(t\). Let \(h_{i}\) be the probability density function of \(H_{i}\). Since we consider Gaussian prior, \(h_{i}(_{i}):=(1/_{i})(-(_{i}-m_{i})^{2}/(2_{i}^{2}))\). Let \(i^{*},j^{*}:^{k}[k]\) be the best and the second best arm under the input such that for each \(\{^{k}:x_{i} x_{j} i j\}\), \(i^{*}()=_{i[K]}_{i}\) and \(j^{*}()=_{i[K]\{i^{*}()\}}_{i}\).

Let \(_{i}(a\|b):=}{2_{i}^{2}}\) represent the KL-divergence between two Gaussian distributions with equal variances (the variance of the \(i\)-th arm \(_{i}^{2}\)) but different means, denoted as \(a\) and \(b\). Similarly, \(d(a,b):=a(a/b)+(1-a)((1-a)/(1-b))\) is the KL divergence between two Bernoulli distributions with means \(a\) and \(b\). Throughout this paper, \(_{}\) and \(_{}\) denote the expectation and probability when the bandit model is fixed as \(^{k}\), i.e., \(_{}=[|_{}]\) and \(_{}=(|_{})\). We will abuse the notation \(\) so that for \(^{k}\), \((;)\) means

\[(;):=_{}J i^{*}( )|_{}.\]

Naturally, \((;)=_{}( ;)\).

Lastly, we introduce the constant \(L()\) that characterizes the sample complexity in the Bayesian setting.

**Definition 2**.: For each \(i,j[k]\), define \(L_{ij}()\) and \(L()\) as follows:

\[L():=_{i,j[k],i j}L_{ij}()L_{ij}():= _{-}^{}h_{i}(x)h_{j}(x)_{s:s[k]\{i,j\}}H_{s}( x)\,x.\]

This constant has the following interesting property which we call a volume lemma:

**Lemma 1** (Volume Lemma, informal).: For \((0,1)\), let

\[L(,):=_{} _{i^{*}()}-_{j^{*}()}.\]

Then, \(_{ 0^{+}}L(,)=L()\). In particular, for \(<}}\), \(L(H,)(L(H),2L(H))\).

The volume lemma states that the volume of prior where the suboptimality gap is smaller than \(\) is proportional to \(L()\) when \(\) is small. We will see in Section 3 that such small-gap cases, which require a large amount of exploration to identify the best arm, dominate the Bayesian expectation of the stopping time. Therefore, \(L()\) defines the Bayesian sample complexity. The formal version of this lemma, which involves some regularity conditions, is shown in Appendix B.

**Remark 1**.: Here, we elaborate on how the Bayesian sample complexity is defined. As will be shown in Section 3, for an algorithm to have a finite expected stopping time, it must determine whether the current instance is difficult or not. In particular, if an algorithm tries to identify even the top-\(O()\) 'hardest instances'2 in the prior, the algorithm cannot achieve the finite expected stopping time. ByLemma 1, the suboptimality gap of the top-\(O()\) hardest instance is given by \(L()\), and the corresponding (frequentist) sample complexity is proportional to \(^{-2}=(L()/)^{2}\)(Kaufmann et al., 2014). Such instances constitute an \(O()\) fraction of the prior, and thus the Bayesian sample complexity is:

\[O())^{2}}{^{2}})=O( ))^{2}}{}).\]

## 3 Limitation of traditional frequentist approaches in the Bayesian setting

Existing BAI studies mainly focused on the Frequentist \(\)-correct algorithms which are defined as follows:

**Definition 3** (Frequentist \(\)-correctness).: An algorithm \(=((A_{t}),,J)\) is said to be frequentist \(\)-correct if, for any bandit instance \(^{k}\) such that \(i^{*}()\) is unique, it satisfies \((;)\). Let \(^{f}()\) be the set of all frequentist-\(\)-correct algorithms.

For the frequentist \(\)-correct algorithms, Garivier and Kaufmann (2016) proved a lower bound for the expected stopping time as follows: for all bandit instance \(\)\(^{k}\) and for all \(((A_{t}),,J)^{f}()\),

\[_{}[](^{-1})T^{*}()+o( (^{-1}))\] (2)

where \(T^{*}()\) is a sample complexity function dependent on the bandit instance \(\).3 Moreover, many of the known frequentist \(\)-correct algorithms achieve asymptotic optimality (Garivier and Kaufmann, 2016; Russo, 2016; Tabata et al., 2023; Qin et al., 2017), meaning that they are orderwisely tight up to the lower bound on Eq. (2) as \( 0\). However, little is known, or at least discussed, about their performance in the Bayesian setting.

One can check that a frequentist \(\)-correct algorithm is also Bayesian \(\)-correct as well (\(^{f}()^{b}(,)\) for all \(\)). Naturally, our interest is whether or not the most efficient classes of frequentist \(\)-correct algorithms, such as Tracking algorithms and Top-two algorithms, are efficient in Bayesian settings. Somewhat surprisingly, the following theorem states that any \(\)-correct algorithm is suboptimal in Bayesian settings.

**Theorem 2**.: For all \(>0,\) and \(((A_{t}),,J)^{f}()\), \(_{}[]=+\).

Proof of Theorem 2 is found in Appendix C. To illustrate the proof, we will use a two-armed Gaussian instance as an example.

### Special case - two armed Gaussian case

Here we present one intuitive corollary of the lower bound theorem (Kaufmann et al., 2016; Garivier and Kaufmann, 2016; Kaufmann et al., 2016) that uses a standard information-theoretic technique.

**Corollary 3**(Kaufmann et al. 2014).: Let \((0,1)\). For any frequentist \(\)-correct algorithm \(((A_{t}),,J)\) and for any fixed mean vector \(=(_{1},_{2})^{2}\), \(_{}[]-_{2})^{ 2}}>}{(_{1}-_{2})^{2}}\).

In the frequentist setting, Corollary 3 implies the lower bound of \(_{}[]=((^{-1})/(_{1}-_{2})^{2})\), which is \(((^{-1}))\) when we view parameters \((_{1},_{2})\) as constants. However, in the Bayesian setting, the algorithm is given the prior distribution \(\) on \(\), and thus the stopping time is marginalized over \(\). In particular, limiting our interest to the case of \(|_{1}-_{2}|<\) for small enough \(>0\), we can obtain the following lower bound:

\[_{}[] _{}[[|_{1}-_ {2}|]]\] (Since \[\] is positive r.v.) \[_{}[[| ][|_{1}-_{2}|]]\] (Law of total expectation) \[_{}[}{( _{1}-_{2})^{2}}[|_{1}-_{2}|]]\] (Corollary 3) \[}{^{2}}_{ {H}}[|_{1}-_{2}|]}{^{2}})}{2}\] (Lemma 1)\[=()^{-1}}{}).\]

This inequality implies that if we naively use a known frequentist \(\)-correct algorithm in the Bayesian setting, the expected stopping time will diverge because we can choose an arbitrarily small \(\). The case of a small gap is _difficult to identify_, and the expected stopping time can be very large for such a case if we aim to identify the best arm for any model.

## 4 Lower bound

This section will elaborate on the lower bound of the stopping time in the Bayesian setting. Theorem 4 below states that any Bayesian \((,)\)-correct algorithm requires the expected stopping time of at least \(()^{2}}{})\).

**Theorem 4**.: Define \(_{}=_{i[k]}_{i}^{2}\) and \(N_{V}=)^{2}_{}^{2} 2}{1664}\). Let \(<_{L}()\) be sufficiently small.4 Then, for any BAI algorithms \(=((A_{t}),,J)\), if \(_{}[] N_{V}\), then \((;)\).

In this main body, we will use the two-armed Gaussian bandit model with homogeneous variance condition (i.e. \(_{1}=_{2}=\)) for easier demonstration of the proof sketch. Theorem 4, which is more general in the sense that it can deal with \(k>2\) arms with heterogeneous variances, is proven in Appendix D.

Sketch of the proof, for \(k=2\):It suffices to show that the following is an empty set:

\[^{b}(,,N_{V}):=\{^{b}(,): _{}[] N_{V}\}.\]

Assume that \(^{b}(,,N_{V})\) and choose an arbitrary \(^{b}(,,N_{V})\). We start from the following transportation lemma:

**Lemma 5** (Kaufmann et al. 2016a, Lemma 1).: Let \((0,1)\). For any algorithm \(((A_{t}),,J)\), any \(_{}\)-measurable event \(\), any bandit models \(,\{(x,y)^{2}:x y\}\) such that \(i^{*}() i^{*}()\),

\[_{}[_{i=1}^{2}_{i}(_{i},_{i}) N_{i}()] d(_{}(),_{}()).\]

Note that the above Lemma holds for any algorithm, and thus works for any stopping time \(\). Now define \(()\) as a swapped version of \(^{2}\), which means \((())_{1}=_{2},()_{2}=_{1}\), and let \(=\{J i^{*}()\}\), the event that the recommendation of the algorithm is wrong. Substituting \(\) with \(()\) from the above equation of Lemma 5 leads to

\[_{}-_{2})^{2}}{2^{2}}  d((;),1-(;)) (;)+(;))}.\] (3)

Note that the first inequality comes from the fact that \(\), the failure event of the bandit model \(\), is exactly a success event of \(()\) in this two-armed case, and the last inequality is from our modified lemma (Lemma 11) from Eq. (3) of Kaufmann et al. (2016a). One can rewrite the above inequality as

\[(;)+(;)}{2}_{}--_{2})^{2}}{2 ^{2}}.\] (4)

We can rewrite the conditions of \(^{b}(,,N_{V})\) as

\[(;)=_{^{2}}(; {})\,()_{^{2}}_{}[]\,()  N_{V}.\] (Opt0)

Using Eq. (4) and with some symmetry tricks, we get \(V_{0}(;)\) where

\[V_{0}:=_{^{2}}(- -_{2})^{2}}{2^{2}}_{}[])( )\;\;\;_{^{2}}_ {}[]\,() N_{V}.\] (Opt1)

[MISSING_PAGE_EMPTY:7]

\(_{0}:=)}\) which satisfies the following condition, thanks to Lemma 1:

\[_{}(_{i^{*}()}-_{j^{*}()} _{0}).\]

In each iteration of the **while** loop of Algorithm 1, the learner selects and observes each arm in the active set. After drawing each arm once, the algorithm calculates the confidence bounds for each arm in the active set using the formula as follows: let \((i,t)\) and \(_{i}(t)\) be the confidence width and the empirical mean of arm \(i\) at time \(t\) as

\[(i,t):=^{2}(t))^{2}/((}{2K})^{2}))}{N_{i}(t)}},_{i}(t):=_{s=1}^{t-1}X _{s}[A_{s}=i].\]

Then the upper and lower confidence bounds of arm \(i\) at timestep \(t\), denoted as \(\) and \(\) respectively, can be defined in the following manner:

\[(i,t):=_{i}(t)+(i,t),(i,t ):=_{i}(t)-(i,t).\] (5)

This confidence bounds ensure that, with high probability, for all \(t[T]\) and \(i[K]\), \(_{i}((i,t),(i,t))\) (See Lemma 15 in Appendix for details). After calculating \(\) and \(\), the algorithm eliminates arms with \(\) smaller than the largest \(\) and maintains only arms that could be optimal in the active set \(\). Up to this point, it follows the traditional elimination approach.

The main difference in our algorithm lies in the stopping criterion. At the end of each iteration, the algorithm checks the stopping criterion. Unlike typical elimination algorithms that continue until only one arm remains, we have introduced an additional indifference condition. This condition arises when the suboptimality gap is so small that identifying them would require an excessive number of samples. In such cases, our algorithm stops additional attempts to identify differences between arms in the active set and randomly recommends one from the active set instead.

**Remark 2**.: In the context of PAC-(\(\), \(\)) identification, Even-Dar et al. (2006, Remark 9) introduced a similar approach. The largest difference is that they use the parameter \(\) as a parameter that defines the indifference-zone level, whereas our parameter \(_{0}\) is spontaneously derived from the prior \(\) and the confidence level \(\) without specifying the indifference-zone.

Theorem 6 describes the theoretical guarantee of the Algorithm 1.

**Theorem 6**.: For \(<4L())}{_{i[k]}}},_{i,j[k]}_{i}L_{ij}()^{2}\), Algorithm 1 which consists of \(((A_{t}),,J)\) has the expected stopping time upper bound as follows:

\[_{}[] C_{}^{2})^{2}}{}()}{})+O(^{-1}),\] (6)

where \(C=320}{3}+1\) is a universal constant and \(_{}=_{i[k]}_{i}\). Here, \(O(^{-1})\) is a function of \(\) and \(\) that is proportional to \(^{-1}\) when we view prior parameters \(\) as constants. Plus, the strategy defined by Algorithm 1 is in \(^{b}(,)\).

See Appendix E for the formal proof of Theorem 6.

**Remark 3**.: When we compare the lower bound (Theorem 4) with the upper bound of Algorithm 1 (Theorem 6), we can see the algorithm is near-optimal. If we view \(_{}/_{}\) as a constant, the bounds are tight up to a \()}{}\) factor.

**Remark 4**.: The condition \(<4L()L()/_{i[k]}},_{i,j[k]}_{i}L_{ij}()^{2}\) is only for cleaner illustration of the regret bound in Theorem 6. The non-asymptotic result, when \(\) is a moderately large constant, can be found in Appendix E.1.

Proof sketch of Theorem 6We summarize the general strategy for the proof as follows. By the law of total expectation, \(_{}[]=_{} _{}[]\). Therefore, we first derive a frequentist upper bound of \(_{}[]\), and then marginalize it to obtain the expected Bayesian stopping time.

First, with the confidence bound defined as Eq. (5) we have the following guarantee that the true means for all arms are in the confidence bound interval with high probability.

**Lemma 7**.: For any fixed \(\{^{k}:v_{i} v_{j}i,j[k]\}\), let \(():=\{ i[k]t,\ _{i}( (i,t),(i,t))\}\). Then, \(_{}() 1-^{2}\).

Now we can rewrite \(_{}[]\) as follows:

\[_{}[]= _{}[_{}[ ]]\] (Law of Total Expectation) \[= _{}[_{}[ [()]]]+_{} [_{}[[()^{c}]]]\] \[= _{i}_{}_{}[N_{i}()[()]]+_{ }[_{}[[()^{ c}]]].\] (7)

Let \(_{i}=_{i}():=(_{s[k]}_{s})-_{i}\) and \(R_{0}() C_{}^{2}}{ ^{2}}\). For the first term, under \(()\), we can bound \(N_{i}()\) by \(R_{0}((_{0},_{i}))\) (Lemma 17 in Appendix E), and integrate it over the prior distribution to obtain the leading factor. For the second term, thanks to the indifference stopping condition (\(^{}(t)_{0}\)), one can prove that \(\) is always smaller than \(R(_{0})\) (Lemma 14 in Appendix E), which leads to non-leading term.

To check that the expected probability of error is below \(\), we have an additional lemma:

**Lemma 8** (Probability of dropping \(i^{*}()\)).: For any \(_{0}\), under \(_{_{0}}\), \((_{0})_{t}\{i^{*}(_{0}) (t)\}.\)

This lemma means under the event \(()\), the best arm is never dropped. We can also prove that under the event \(()\), if \(_{i}()>_{0}\), the sub-optimal arm will eventually be dropped before the algorithm terminates (Lemma 14 in Appendix E). These two facts mean there are only two cases in which the prediction of Algorithm 1 could be wrong.

* Under \(()^{c}\), both facts cannot guarantee the correct identification. From Lemma 7, \(_{}[()^{c}]^{2}\) for all \(\), and thus \(_{}[()^{c}]^{2}\).
* When \(_{i}()_{0}\). From Lemma 1 and the definition of \(_{0}\), the probability of drawing such \(\) from the prior is at most \(/2\).

Therefore, by union bound, Algorithm 1 has the expected probability of misidentification guarantee smaller than \(^{2}+/2<\).

## 6 Simulation

We conduct two experiments to demonstrate that the expected stopping times of frequentist \(\)-correct algorithms diverge in a Bayesian setting and that the elimination process in Algorithm 1 is necessary for more efficient sampling. In Tables 1 and 2, each column 'Avg', 'Max', and 'Error' represents the average stopping time, maximum stopping time, and the ratio of the misidentification, respectively.5 More details of these experiments are in Appendix F.

Frequentist algorithms diverge in Bayesian SettingWe evaluate the empirical performance of our Elimination algorithm (Algorithm 1) by comparing it with other frequentist algorithms such as Top-two Thompson Sampling (TTTS) (Russo, 2016) and Top-two UCB (TTUCB) (Jourdan and Degenne, 2022b).

We design an experiment setup that has \(k=2\) arms with standard Gaussian prior distribution, which means \(m_{i}=0,_{i}=1\) for all \(i[k]\). We set \(=0.1\) and ran \(N=1000\) Bayesian FC-BAI simulations to estimate the expected stopping time and success rate.

In Table 1, one can see that the two top-two algorithms exhibit very large maximum stopping time. This supports our theoretical result in Section 3 that the expected stopping time of Frequentist \(\)-correct algorithms will diverge in the Bayesian setting. We did not check the track and stop algorithm (Garivier and Kaufmann, 2016) because it needs to solve an optimization for each round, but the fact that the expected stopping time of the track and stop is at least half of the TTTS and TTUCB for a small \(\) implies that the performance of track and stop is similar to that of top-two algorithms. Algorithm 1 shows a significantly smaller average stopping time as well as an average computation time than that of these algorithms.

Effect of the elimination processWe implemented the modification of Algorithm 1 (denoted as NoElim) that never eliminates an arm from \((t)\)6 In this setup, we have \(k=10\) arms with standard Gaussian prior distribution, which means \(m_{i}=0,_{i}=1\) for all \(i[k]\). We set \(=0.01\) and ran \(N=1000\) Bayesian FC-BAI simulations.

As one can check from Table 2, elimination of arms helps the efficient use of samples and reduces stopping time and computation time.

## 7 Discussion and future works

We have considered the Gaussian Bayesian best arm identification with fixed confidence. We show that the traditional Frequentist FC-BAI algorithms do not stop in finite time in expectation, which implies the suboptimality of such algorithms in the Bayesian FC-BAI problem. We have established a lower bound of the Bayesian expected stopping time, which is of order \(()^{2}}{})\). Moreover, we have introduced the elimination and early stopping algorithm, which achieves a matching stopping time up to a polylogarithmic factor of \(L()\) and \(\). We conduct simulations to support our results.

In the future, we will attempt to tighten the logarithmic and \((_{i}}{_{i}_{i}})^{2}\) gap between the lower and upper bound, extend the indifference zone strategy for other traditional BAI algorithms in the Bayesian setting, extend our analysis from Gaussian bandit instances to general exponential families, and design a robust algorithm against misspecified priors.