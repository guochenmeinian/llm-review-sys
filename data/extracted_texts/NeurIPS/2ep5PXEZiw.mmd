# Foundation Model is Efficient

Multimodal Multitask Model Selector

 Fanqing Meng

OpenGVLab, Shanghai AI Laboratory

Shanghai Jiao Tong University

mengfanqing33@gmail.com&Wenqi Shao

OpenGVLab, Shanghai AI Laboratory

shaowenqi@pjlab.org.cn

&Zhanglin Peng

The University of Hong Kong

&Chonghe Jiang

The Chinese University of Hong Kong

&Kaipeng Zhang

OpenGVLab, Shanghai AI Laboratory

&Yu Qiao

OpenGVLab, Shanghai AI Laboratory

&Ping Luo

The University of Hong Kong

OpenGVLab, Shanghai AI Laboratory

pluo@cs.hku.edu

Corresponding Authors: shaowenqi@pjlab.org.cn; pluo@cs.hku.edu

###### Abstract

This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering.A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability, they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee. Extensive experiments on \(5\) downstream tasks with \(24\) datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models, making it the first model selection method in the multi-task scenario. For instance, compared with the state-of-the-art method LogME enhanced by our label embeddings, EMMS achieves 9.0%, 26.3%, 20.1%, 54.8%, 12.2% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, while bringing 5.13\(\), 6.29\(\), 3.59\(\), 6.19\(\), and 5.66\(\) speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector.

## 1 Introduction

Pre-trained models (such as neural network backbones) are crucial and are capable of being fine-tuned to solve many downstream tasks such as image classification , image captioning , question answering , and referring segmentation . This "pre-training \(\) fine-tuning" paradigm shows that the models pre-trained on various datasets (_e.g.,_ ImageNet and YFCC100M ) by many objectives (_e.g.,_ supervised and self-supervised) can provide generic-purpose representation, which is transferable to different tasks. A large number of pre-trained models have been produced with the rapid development of network architecture research, such as convolutional neural networks (CNNs) [6; 7; 8] and transformers [9; 10; 11]. When given a large collection of pre-trained models to solve multiple multi-modal tasks, an open question arises: _how to efficiently predict these models' performance on multiple tasks without fine-tuning them?_

Existing works [12; 13; 14; 15; 16; 17; 18; 19] answered the above question using model selection approaches, which are of great benefit to transfer learning. For example, when a neural network is properly initialized with a better pre-trained checkpoint, it will achieve faster convergence and better performance on a target task [20; 21]. However, it is challenging to quickly identify an optimal one from a large collection of pre-trained models when solving each multi-modal task. This is because of two reasons. Firstly, the ground truth of model ranking can only be obtained by brute-force fine-tuning and hyper-parameter grid search, which are computationally expensive . Secondly, the recent methods [12; 13; 14; 23] that can estimate the transferability of pre-trained models are not generic enough for a variety of multi-modal tasks. For instance, an approach [14; 12] that relies on the prior knowledge of a single specific task would be ineffective in others.

To address the above challenges, we need a unified representation to represent diverse label formats in each multi-modal task _e.g.,_ categories, texts and bounding boxes. Existing methods cannot be employed in multi-task scenarios because they only receive labels with one-hot or real-valued vectors, as shown in Fig.1. For example, LEEP  and PACTran  are carefully designed for classification tasks. GBC  and TransRate  measure transferability using class separability, which relies on prior knowledge in classification task. Although LogME  can be used in both classification and regression tasks, it relies on real-valued labels, making it inapplicable in other label formats such as text descriptions.

In contrast to the above works, we propose an Efficient Multi-task Model Selector with an acronym, EMMS, which can select the most appropriate pre-trained model for solving each multi-modal task.

Figure 1: Comparison between prior pre-trained model selectors and our multi-task model selector. (a) denotes that a model selector measures transferability by modeling the compatibility between the model feature and task label. Previous model selectors can only receive labels with one-hot or real-valued vectors. Our multi-task model selector can be employed in various tasks with diverse label formats. (b) denotes that our proposed EMMS is applicable and effective in various downstream tasks while previous transferability metrics can be only used in classification or regression tasks.

This is achieved by employing foundation models, such as CLIP  and GPT-2 , to transform diverse label formats into a unified label embedding space. The estimated label embedding contains more rich information than the conventional one-hot and real-valued label encoding.

In this way, EMMS can measure the compatibility between the models' features and corresponding label embeddings on various tasks, as shown in Fig. 1 and Fig. 2. This results in a more generic assessment of the models' transferability than previous methods. Specifically, EMMS treats the estimated label embeddings as noisy oracles of the ground-truth labels, and it turns a log-likelihood maximization problem into a simple weighted linear square regression (WLSR). We propose an alternating minimization algorithm to solve WLSR, which can be solved with a theoretical convergence guarantee efficiently. Extensive experiments validate the effectiveness of EMMS on multiple tasks, including image classification , image captioning , question answering on both image  and text , referring comprehension , and landmark detection .

The **contributions** of this work are summarized as follows. (1) We propose a generic transferability estimation technique, namely Efficient Multi-task Model Selector (EMMS). Equipped with a unified label embedding provided by foundation models and a simple weighted linear square regression (WLSR), EMMS can be fast, effective, and generic enough to assess the transferability of pre-trained models in various tasks. (2) We propose a novel alternating minimization algorithm to solve WLSR efficiently with theoretical analysis. (3) Extensive experiments on \(5\) downstream tasks with \(24\) datasets demonstrate the effectiveness of EMMS. Specifically, EMMS achieves 9.0%, 26.3%, 20.1%, 54.8%, 12.2%, performance gain on image recognition, referring, captioning, visual question answering, and text question answering, while bringing 5.13\(\), 6.29\(\), 3.59\(\), 6.19\(\), and 5.66\(\) speedup in wall-clock time compared with the state-of-the-art method LogME enhanced by our label embeddings, respectively.

## 2 Related Work

**Transferability Estimation.** Model selection is an important task in transfer learning. To perform model selection efficiently, methods based on designing transferability metrics have been extensively investigated. LEEP  pioneers to evaluate the transferability of source models by empirically estimating the joint distribution of pseudo-source labels and the target labels. But it can only handle classification tasks with supervised pre-trained models because the modeling of LEEP relies on the classifier of source models. Recent works propose several improvements over LEEP to overcome the limitation. For example, NLEEP  replaces pseudo-source labels with clustering indexes. Moreover, LogME , TransRate , and PACTran  directly measure the compatibility between model features and task labels. Although fast, these metrics can only be used on limited tasks such as classification and regression. This work deals with model selection in multi-task scenarios. We propose EMMS to evaluate the transferability of pre-trained models on various tasks.

**Label Embedding.** Label embedding represents a feature vector of task labels, which can be generated in various ways. The classical approach is to use one-hot encoding to represent the labels as sparse vectors, which is widely used in image classification. Another way is to transform labels into vectors by embedding layers. For example, an RNN module is employed to generate label representation in , which is encouraged to be compatible with input data vectors in text classification tasks. In addition, it is also common to treat the labels as words and use techniques such as word2vec  or GloVe  to learn vector representations of the labels. The main obstacle in the multi-task scenario is how to deal with diverse label formats. In this work, we follow the idea of word embedding and treat task labels as texts, which are then transformed into embeddings by publicly available foundation models [26; 27].

**Foundation Models.** CLIP  is the first known foundation model which learns good semantic matching between image and text. The text encoder of CLIP can perform zero-shot label prediction because it encodes rich text concepts of various image objects. By tokenizing multi-modal inputs into homogeneous tokens, recent work on foundation models such as OFA  and Uni-Perceiver  use a single encoder to learn multi-modal representations. In this work, we utilize the great capacity of foundation models in representing image-text concepts to generate label embedding. It is noteworthy that although foundation models can achieve good performance in various downstream tasks, they may not achieve good zero-shot performance on many tasks[37; 38; 39] and it is still computationally expensive to transfer a large model to the target task [40; 41]. On the contrary, a multi-task model selector can quickly select an optimal moderate-size pre-trained model that can generalize well in target tasks. In this sense, a multi-task model selector is complementary to foundation models.

## 3 Preliminary of Model Selection

**Problem Setup.** A target dataset with \(N\) labeled samples denoted as \(=\{(x^{n},y^{n})\}_{n=1}^{N}\) and \(M\) pre-trained models \(\{_{m}=(_{m},h_{m})\}_{m=1}^{M}\) are given. Each model \(_{m}\) consists of a feature extractor \(_{m}\) producing a \(D\)-dimension feature (i.e. \(=_{m}(x)^{D}\)) and a task head \(h_{m}\) outputting predicted label given input \(x\)[6; 9]. In multi-task scenarios, the ground-truth label comes in various forms, such as category, caption, and bounding box, as shown in 1. The task of pre-trained model selection is to generate a score for each pre-trained model thereby the best model can be identified to achieve good performance for various downstream tasks.

**Ground Truth.** The ground truth is obtained by fine-tuning all pre-trained models with hyper-parameters sweep on the target training dataset and recording the highest scores of evaluation metrics [31; 13] (e.g. test accuracy and BLEU4 ). We denote ine-tuning scores of different models as \(\{G_{m}\}_{m=1}^{M}\). Since fine-tuning all models on all target tasks requires massive computation cost, research approaches design lightweight transferability metrics which offer an accurate estimate of how well a pre-trained model will transfer to the target tasks.

**Transferability Metric.** For each pre-trained model \(_{m}\), a transferability metric outputs a scalar score \(T_{m}\) based on the log-likelihood, as written by

\[T_{m}=_{n=1}^{N} p(y_{n}|x_{n};_{m},h_{m})\] (1)

where \((x_{n},y_{n})\) denotes the \(n\)-th data point in target dataset \(\). A higher log-likelihood value for \(T_{m}\) indicates that the model \(_{m}\) is likely to achieve better performance on the intended task. Numerous transferability metrics have been proposed by modeling prediction probability \(p(y_{n}|x_{n};_{m},h_{m})\) in various ways. Although being efficient, they can hardly be used in multi-task scenarios.

**Challenges in Multi-task Scenarios.** Existing transferability metrics fail to generalize to various tasks for two reasons. Firstly, existing methods such as LEEP and LogME can only deal with real-value label formats. But \(y_{n}\) can be a sentence of words in the task of image caption. Secondly, a large number of the previous metrics estimate transferability through the target task's prior information such as maximizing inter-class separability, which is inapplicable in multi-task scenarios except for the classification. To overcome these difficulties, we introduce a simple regression framework with unified label embeddings provided by several foundation models in Sec.4.

## 4 Our Method

In this section, we introduce our Efficient Multi-task Model Selector (EMMS). To overcome the difficulty of diverse label formats, EMMS employs foundation models to transform various labels into unified label embeddings in Sec.4.1. By treating label embeddings provided by multiple foundation models as noisy oracles of ground truth labels, EMMS can calculate transferability metric under a simple weighted linear square regression (WLSR) framework in Sec.4.2. We design an alternating minimization algorithm to solve WLSR efficiently in Sec. 4.3. The illustration of our EMMS is provided in Fig. 2.

### Foundation Models Unify Label Embedding

In general, label embeddings or label representations should encode the semantic information such that two labels with low semantic similarity have a low chance to be grouped. A common scheme is to represent label embedding as a one-hot vector. However, one-hot representation can not embed labels with text formats such as captions in the image caption task. Following the design in multi-modality foundation models , we treat labels with diverse formats as a text sequence, which can be encoded by pre-trained foundation models, as shown in Fig. 2.

**Label Embedding via Foundation Models (F-Label).** Thanks to the great representational capacity, the foundation model can construct label embedding (termed F-label) while preserving its rich semantic information of labels. Given a label \(y\) in the target task, the label embedding \(z^{L}\) is obtained by \(z=F(y)/\|F(y)\|_{2}\) where \(F\) can be instantiated by various foundation models to process diverse label formats. \(_{2}\) normalization is utilized to normalize the representations extracted from different foundation models. Moreover \(F\) can be implemented as CLIP , BERT  and GPT-2  when task label \(y\) is text. Note that label embedding extraction can be fast enough with GPU parallel computation. We provide the runtime analysis in Appendix Sec.C.

**Benefits of F-Label.** F-Label has several advantages over one-hot label representations. Firstly, it embeds richer semantic information than one-hot label, leading to accurate modeling of the semantic relationships between different labels. As shown in Fig.3, F-Label leads to a higher correlation between fine-grained classes than one-hot encoding. Secondly, compared with one-hot labels, F-label can be obtained in a variety of tasks as long as the task label can be transformed into a text sequence. With the assistance of F-Labels, model selection can be established in multi-task scenarios.

### Regression with Unified Noisy Label Embeddings

To estimate the transferability of pre-trained models, the relationship between model features \(^{D}\) and F-Label \(z^{L}\) should be modeled in order to calculate the transferability score \(T_{m}\) in Eqn. (1). On the other hand, since a semantic label \(y\) can be embedded by several foundation models, the label embedding set can be constructed as \(=\{z_{k}=F_{k}(y)/\|F_{k}(y)\|_{2},k[K]\}\) where \(\{F_{k}\}_{k=1}^{K}\) denotes \(K\) foundation models. Now, we utilize data points \(\{(_{k}^{n},z_{1}^{n},,z_{K}^{n})\}_{n=1}^{N}\) to model the relationship between model features and F-Labels.

**Setup.** As shown in Fig.2, we assume that true label embedding \(z\) is a linear mapping of the model feature with additive Gaussian noise with a variance of \(_{0}^{2}\), as given by \(z=z_{0}+=w^{T}+\) and \( N(0,_{0}^{2}I_{L})\) where \(z_{0}=w^{T}\) is the regression prediction, \(w^{D L}\) and \(\) are regression weights and regression error, respectively, and \(I_{L}\) is a L-by-L identity matrix.

We assume that F-labels \(\{z_{k}\}_{k=1}^{K}\) obtained from different foundation models are oracles that independently provide noisy estimates of the true label embedding \(z\). Formally, we have \(P(z_{k}|z)=N(z,_{k}^{2}I_{L})\). By the above setup, EMMS would be performed with noisy labels. Hence, EMMS tends to select pre-trained models robust to the label noise.

Figure 3: Label embedding has richer semantic information than one-hot labels. (a) indicates that in the classification task, F-Label can capture the correlation of labels with different granularity than one-hot encoding. (b) shows that in the image caption task, F-label can model the semantic relevance of two captions corresponding to the same image better than the one-hot label.

Figure 2: Overview of our EMMS. (a) shows that labels in various tasks can be expressed by texts. (b) presents the graph model of regression with multiple noisy labels. We use several foundation models to encode text labels as label embeddings which are deemed as noisy oracles of true label embedding \(z\). Moreover, \(z\) is a linear mapping of model feature \(\) with Gaussian noise \( N(0,_{0}^{2})\).

**Reasonableness of the Linear Assumption.** Specifically, EMMS assumes that the true label embedding \(z\) is a linear mapping of the model feature with Gaussian noise. The linear assumption is reasonable in image and text classification tasks because a linear classifier is usually used when the pre-trained model is transferred to a target task, which is commonly used in recent methods. For example, LogME  assumes that: \(z N(w^{T},^{-1})\), which implies that there is a linear mapping from the model feature space to the label space. PACTran  also has a similar setting. The difference is that LogME takes a one-hot label as the true label embedding, which limits its applicability. But our EMMS treat the true label embedding \(z\) as an implicit variable. And F-Labels \(\{z_{k}\}_{k=1}^{K}\) obtained from different foundation models are assumed to be noisy oracles of true label embedding \(z\). Since labels in many tasks can be easily encoded into F-Labels, our EMMS can be used as a multitask model selector. We verify effectiveness the linear assumption in various multi-model tasks with extensive experiments in Sec.5.

**Computation of Log-Likelihood.** To model the relationship between model features and F-Labels, we need to estimate regression weights \(w\), strengths of label noises \(\{_{k}\}_{k=0}^{K}\). For simplicity of notation, we consider the case \(L=1\), i.e. F-labels are scalars. Given \(N\) data points, the log-likelihood is given by

\[=N A_{1}- A_{2}+_{n=1}^{N}(^{n} )^{2}}{4A_{2}}-A_{4}^{n})+\] (2)

where \(A_{1}=_{k=0}^{K}1/_{k},A_{2}=_{k=0}^{K}1/2_{k}^{2},A_{3} ^{n}=_{k=0}^{K}_{k}^{n}/_{k}^{2}\), and \(A_{4}^{n}=_{k=0}^{K}(z_{k}^{n})^{2}/_{k}^{2}\). The detailed derivation of Eqn.(2) is provided in the Appendix Sec.A.

**Maximizing Log-likelihood as Weighted Linear Square Regression (WLSR).** The remaining issue is to determine parameters \(w\) and \(\{_{k}\}_{k=0}^{K}\) by maximizing the log-likelihood in Eqn. (2). But it can be intractable because \(w\) and \(\{_{k}\}_{k=0}^{K}\) are heavily coupled. To mitigate this issue, we turn the log-likelihood maximization into a weighted linear square regression by rearranging Eqn. (2) as \(-=\|Xw-Zt\|_{2}^{2}+R(\{_{k}\}_{k=0}^{K})\), where \(X^{N D}\) is the data matrix whose \(n\)-th row is model feature \((^{n})^{T}\), \(w^{D 1}\) are weight parameters, \(Z^{N K}\) is F-Label matrix whose \(k\)-th column is the label embedding \(z_{k}\), and \(t^{K 1}\) satisfies that \(1_{K}^{T}t=1,t 0\) which is a \((K-1)\)-D simplex denoted as \(^{K-1}\). \(R()\) is a regularization term parameterized with \(\{_{k}\}_{k=0}^{K}\). We provide the derivations in Appendix Sec.A.

We note that the computational intractability comes from the data-dependent regularizer \(R()\). For efficient computation, we drop \(R()\), turning the log-likelihood maximization into a problem of WLSR, as given by

\[_{w^{D 1},t^{K-1}}s(w,t)=\|Xw- Zt\|_{2}^{2}\] (3)

When considering the case \(L>1\), Eqn. (3) becomes \(_{w^{D L},t^{K-1}}\|Xw-Zt\|_{F}^ {2}\) where \(Z^{N L K}\) and \(\|\|_{F}\) is Frobenius norm.

```
1:Input: Model feature \(X R^{N D}\); F-Label matrix \(Z R^{N K}\); Learning step-sizes \(\) and \(\) for \(w\) and t, respectively;
2:Output: Score of WLSR;
3:Initialize \(t=1_{K}\) and \(w=1_{D}\);
4:while s not converge do
5:\(s=\|Xw-Zt\|_{2}^{2}\);
6:\(w=w- X^{T}(Xw-Zt)\);
7:while t not converge do
8:\(t t- Z^{T}(Zt-Xw)\);
9:\(t=_{^{K-1}}(t)\); // Projection
10:endwhile
11:endwhile
12:Return:\(s\) ```

**Algorithm 1** Alternating Minimization

```
1:Input: Model feature \(X R^{N D}\), F-Label matrix \(Z R^{N K}\);
2:Output: Score of WLSR;
3:Initialize \(t=1_{K}\) and \(w=1_{D}\);
4:while s not converge do
5:\(s=\|Xw-Zt\|_{2}^{2}\);
6:\(w=(X^{T}X)^{-1}X^{T}Zt\); // LSR for \(w\)
7:\(t=(Z^{T}Z)^{-1}Z^{T}Xw\); // LSR for \(t\)
8:\(t=(t)\) ; // Projection
9:endwhile
10:Return:\(s\) ```

**Algorithm 2** Fast Alternating Minimization

When considering the case \(L>1\), Eqn. (3) becomes \(_{w^{D L},t^{K-1}}\|Xw-Zt\|_{F}^ {2}\) where \(Z^{N L K}\) and \(\|\|_{F}\) is Frobenius norm. From Eqn. (2) and Eqn. (3), \(s(w,t)\) is an approximation of negative log-likelihood. Hence, a smaller \(s(w,t)\) indicate the larger \(T_{m}\) in Eqn. (1) and better transferability. We design an efficient algorithm to solve WLSR.

### Fast Computation by Alternating Minimization

**Algorithm.** The optimization problem in Eqn. (3) can be formulated to a classical second-order conic program(simply called **SOCP**). However, the excessive data in our problem leads to a large dimension of the variable, making it inefficient for standard solvers. Therefore, we are motivated to find the smooth structure of the problem and design an alternating minimization algorithm to achieve fast computation. As shown in Algorithm 1, we separately fix \(w\) and \(t\) to optimize the other one until the function value in Eqn. (3) converges. Specifically, when we fix \(t\), the whole problem degenerates to a least square problem with respect to \(w\). When we fix \(w\), we also need to solve a least square problem concerning \(t\) under the simplex constraint.

**Convergence Analysis.** We will prove the convergence property of the function value. Indeed, we prove a stronger condition that the function value decreases after each round of iterations on \(w\) and \(t\). From the monotone convergence theorem, the convergence can thus be derived. We first present the decreasing result of inner loop of \(t\) by Theorem 1 and the same property holds for the update of \(s\). Then the convergence of the whole algorithm can be derived by Theorem 2. The detailed proofs are placed in the Appendix Sec.A.

**Theorem 1**.: _Suppose \(s(w,t)=\|Xw-Zt\|_{F}^{2}\) where \(X^{N D}\), \(Z^{N K}\), \(w^{D 1}\) and \(t^{K-1}\), the inner loop of \(t\) in Algorithm 1 lines \(7\) - \(10\) decreases after each iteration. Specifically, denote \(=1/\|2Z^{T}Z\|\) and \(t^{+}=_{^{K-1}}(t- s(w,t))\). For any \(t^{K-1}\), \(s(w,t^{+})-s(w,t)-\|t-t^{+}\|^{2} 0\)._

**Theorem 2**.: _Suppose \(s(w,t)=\|Xw-Zt\|_{2}^{2}\) where \(X^{N D}\), \(Z^{N K}\), \(w^{D 1}\) and \(t^{K-1}\), the function value in Algorithm 1 will converge. Specifically, denote \(w^{*},t^{*}\) as the result after one iteration of \(w,t\) respectively, we have \(0 s(w^{*},t^{*}) s(w^{*},t) s(w,t)\)._

**Computational Speedup.** Although this algorithm 1 guarantees convergence, it is a bit time-consuming due to the two-level loop, we optimized this part and achieved similar results in very little time. Since the least squares solution is extremely fast, we performs least squares on \(w\) and \(t\), and then replace projection onto simplex with explicit Sparsemax transformation [45; 46], iteratively. The fast solver is illustrated in Algorithm 2. we experimentally verify its convergence and find that the approach achieves impressive speedup.

## 5 Experiment

This section evaluates our method EMMS on different downstream tasks, including image classification, image caption, visual question answering, text question answering and referring expression comprehension. We put more experiments details in Appendix Sec.B. Moreover, we conduct a detailed ablation study to analyze our EMMS in Appendix Sec.C

### Training Details

**Benchmark.** For **image classification**, We adopt 11 classification benchmarks, including FGVC Aircraft , Caltech-101 , Stanford Cars , CIFAR-10 , CIFAR-100 , DTD , Oxford 102 Flowers , Food-101 , Oxford-IIIT Pets , SUN397 , and VOC2007 . For **image caption**, We use Flickr8k , Flickr30k , FlickrStyle10K-Humor , FlickrStyle10K-Romantic  and RSICD . For **visual question answer**, We apply COCOQA , DAQUAR  and CLEVR . For **text question answer** and **referring expression comprehension**, we separately use SQuAD1.1 ,SQuAD2.0  and RefCOCO , RefCOCO+ , RefCOCOg .

**Ground truth.** In order to obtain the ground truth, we finetune all pre-trained models on all target datasets with a grid search of hyper-parameters. Details of target datasets and fine-tuning schemes are described in Appendix Sec.B.

**Evaluation protocol.** To assess how well a model selector predict the transferability of pre-trained models, we calculate the rank correlation between \(\{T_{m}\}_{m=1}^{M}\) and \(\{G_{m}\}_{m=1}^{M}\). Following the common practice [13; 31], we use _weighted Kendall's \(_{w}\)_. The larger \(_{w}\) indicates a better correlation and better transferability metric. For computation complexity, we record the runtime of executing algorithmover all models given the feature and label on a target task and analyzed the computational complexity of EMMS as well as LogME. (Details can be found in Appendix Sec.C)

**Baseline.** For the image classification task, we choose NLEEP , TransRate , and LogME as the baseline; for other multimodal tasks, we choose LogME with F-Label as the baseline; in addition, for the VQA task, we additionally compare PACTran . Details of baselines and why we choose them are described in Appendix Sec.B.

### Image Classification with ViT Models

Vision transformer  (ViT) models have been increasingly used for a variety of tasks and have achieved better results than CNN models. The architecture of ViT models are more complex than CNN models. Hence, how to do the model selection on ViT models is a more challenging and rewarding task. Details of pre-trained models are described in Appendix Sec.B.

**Performance and wall-clock time comparison.** As shown in Table.1, our EMMS achieve the best average \(_{w}\) on 11 target datasets and the best \(_{w}\) on 9 target datasets with relatively short time. For example, EMMS outperforms LogME by 0.182 and 0.139 rank correlation \(_{w}\) on Aircraft, and VOC2007, respectively, showing the effectiveness of our EMMS in measuring the transfer-ability of pre-trained ViT models. On the other hand, for the remaining 2 target datasets (i.e. CF-10, DTD), our EMMS still has a marginal gap compared to the best-performing transferability metric. Besides, we find that the effect of model selection of EMMS in ViT models selection has an improvement compared to CNN models selection, we guess F-Label has spatial similarity with the model feature of ViT-base model because the foundation models are mostly transformer-based, which can model the relationship between model feature from ViT-base models and F-Labels more accurately.

### Image Captioning

Here we treat image caption as a vocab-based classification task. That is we use a vocabulary and classify the caption into the index of some words in the vocabulary. Afterward, training is done according to the classification task criteria.Here we calculate the average \(_{w}\) and time of LogME with \(K\) single F-label from \(K\) foundation models we use respectively. We wants to select the best combination of image encoder and language encoder. Details of pre-trained models and the model architecture are described in Appendix Sec.B.

**Performance and wall-clock time comparison.** As shown in Table.2, EMMS is significantly ahead of baseline in both time and effect for each dataset. For example, EMMS outperforms LogME with the relative improvements of 39% and 37% in rank correlation \(_{w}\) on Flickr8k and Flickr30k, respectively. In addition, the time of EMMS is reduced by 83.7% and 79.8% relative to LogME on these two datasets, which shows the efficiency of our algorithm. The average rank correlation \(_{w}\) alone the five datasets is 0.64, which denotes EMMS has sufficient confidence.

   & Aircraft & Caltech & Cars & CF-10 & CF-100 & DTD & Flowers & Food & Pets & SUN & VOC & Avg. \\  \)} \\  LogME & 0.299 & 0.382 & 0.633 & **0.741** & 0.727 & 0.569 & 0.512 & 0.580 & 0.528 & 0.619 & 0.591 & 0.561 \\ NLEEP & -0.282 & 0.027 & 0.693 & 0.674 & 0.538 & 0.123 & -0.262 & 0.105 & 0.40 & 0.268 & 0.109 & 0.218 \\ TransRate & 0.244 & 0.412 & 0.487 & 0.260 & 0.702 & 0.533 & **0.655** & 0.542 & 0.707 & 0.612 & 0.651 & 0.527 \\ EMMS(One) & 0.412 & 0.444 & 0.565 & 0.740 & 0.736 & 0.621 & 0.562 & 0.579 & 0.740 & 0.592 & 0.730 & 0.611 \\  EMMS & **0.481** & **0.444** & **0.706** & 0.718 & **0.745** & **0.621** & 0.562 & **0.673** & **0.740** & **0.619** & **0.730** & **0.639** \\   \\  LogME & 8.93 & 10.89 & 30.28 & 53.07 & 62.13 & 4.78 & 9.27 & 104.92 & 6.28 & 425.43 & 7.42 & 65.76 \\ NLEEP & 553.7 & 716.8 & 1.1e3 & 8.0e3 & 1.2e4 & 183.7 & 819.2 & 3.4e4 & 256.4 & 2.7e4 & 288.3 & 7719.8 \\ TransRate & 19.43 & 19.21 & 36.9 & 61.73 & 63.82 & 8.73 & 18.26 & 110.79 & 15.51 & 89.92 & 5.11 & 40.85 \\ EMMS(One) & **4.12** & **4.45** & **8.07** & **19.45** & **26.18** & **2.65** & **4.03** & **39.72** & **3.50** & **24.84** & **4.07** & **12.82** \\  EMMS & 21.31 & 17.23 & 28.06 & 154.61 & 182.11 & 13.87 & 15.95 & 265.99 & 17.93 & 63.86 & 16.63 & 72.55 \\  

Table 1: Comparison of different transferability metrics on ViT models regarding \(_{w}\) and the wall-clock time where EMMS(One) denotes EMMS with the one-hot label. Our proposed EMMS achieves the best transfer-ability assessment over 11 target tasks and exhibits higher efficiency than NLEEP.

### Text Question Answering

For natural language understanding, we consider Text Question Answering (TQA) as a reading comprehension task, where the response to each question is a text segment extracted directly from the affiliated reading passage, or the question may indeed be deemed unanswerable. Details of pre-trained models and how to finetune are described in Appendix Sec.B.

Performance and wall-clock time comparison.In Table 3, the performance improvement of EMMS on the TQA is consistent with the enhancements observed in the earlier mentioned computer vision tasks. More specifically, our EMMS attains accuracies of 60.3% and 46.3% on the Stanford Question Answering Dataset (SQuAD) versions 1.1 and 2.0 respectively, using rank correlation \(_{w}\) as an evaluation metric. This represents a significant relative increment of 11.2% and 13.2% compared to the performance of LogME.

### Referring Expression Comprehension Comprehension

Referring expression comprehension (REC) is a widely challenging task because it requires precise alignment between linguistic concepts and image features. To address this, the objects in each image are represented as a sequence of discrete tokens, while their bounding box corner coordinates are turned into integer location tokens. This allows for a unified F-Label to be extracted using various language models. More details about the pre-trained models can be found in Appendix Sec.B.

Performance and wall-clock time comparison.As shown in Table 4, our EMMS continues to exhibit its superiority in the enhancement of performance on the REC task, an _instance-level cross-modal_ localization task. Specifically, the proposed EMMS produces accuracies of 45.8%, 54.9%, and 52.1% on the RefCOCO, RefCOCO+, and RefCOCOg datasets respectively. This significantly surpasses its counterpart, LogME, in terms of margins when evaluated with rank correlation \(_{w}\).

### Ablation Analysis

Comparison with different number of F-LabelHere we denote the number of F-Label is \(K\) and choose the image caption task to illustrate the impact of \(K\) on our solution. As shown in Table 6. We find that increasing \(K\) in a certain range brings a gain in effectiveness to our method, but when K becomes larger, the time also increases and we find that \(K=4\) is not as effective as \(K=3\). We believe that the increase in \(K\) brings difficulties in fitting the true Label, resulting in a loss of effectiveness. Therefore, we use \(K=3\) for the sake of effect and time.

Performance on F-Label using small modelOn the one hand, using foundation model can extract the joint embedding compared to the small model, which allows EMMS to be extended to tasks with multiple forms of labels. On the other hand, the foundation model can handle many types of tasks,

   &  &  &  &  \\  \)} &  \\  LogME & 0.542 & 0.409 & 3587.22 & 3596.23 &  \\ EMMS & **0.603** & **0.463** & **571.23** & **589.78** &  \\   
   &  &  &  &  &  &  \\  \)} &  \\  LogME & 0.423 & 0.389 & 0.398 & 2457.87 & 2478.90 & 2298.76 \\ EMMS & **0.458** & **0.549** & **0.521** & **454.26** & **467.92** & **356.94** \\  

Table 3: Comparison of different transferability Table 4: Comparison of different transferability metrics on TQA models in rank correlation \(_{w}\) metrics on referring expression models in rank with the ground truth and the wall-clock time. correlation \(_{w}\) with ground truth and the time. The LogME denotes using LogME with F-Label. The LogME denotes using LogME with F-Label. The LogME denotes using LogME with F-Label. The LogME denotes using LogME with F-Label.

   &  &  &  &  &  &  &  &  &  &  \\  \)} &  \\  LogME & 0.483 & 0.368 & 0.501 & 0.780 & 0.654 & 425.67 & 1594.16 & 973.22 & 60.35 & 63.79 \\ EMMS & **0.660** & **0.504** & **0.704** & **0.802** & **0.678** & **69.01** & **321.32** & **88.77** & **16.56** & **14.59** \\  

Table 2: Comparison of different transferability metrics on image caption models in rank correlation \(_{w}\) with the ground truth and the wall-clock time. The LogME denotes using LogME with F-Label. Our proposed EMMS achieves the best transfer-ability assessment on each target task with much less time compared to LogME.

so we can use the foundation model for different tasks for label embedding. As shown in Table 5, we experimentally demonstrate that the use of the foundation model leads to more accurate F-Label extraction and thus to an improvement in the performance of the method.

**The effect of using a single foundation model** We investigate how EMMS is influenced when only a single foundation model is provided. We conduct experiments on image classification and image captioning. We consider EMMS with the single foundation model including language foundation model (1) GPT-2 , (2) BERT , (3) RoBerta , and multimodal foundation model (4) CLIP , (5) FLAVA , and (6) AltCLIP . For comparison, we include the result of our EMMS with default setting (K=3, i.e. CLIP, BERT, and GPT-2) and the result of previous state-of-the-art methods obtained from LogME, NLEEP and TransRate. The results are reported in Table 20 and Table 5.

We have several observations. (1) Different downstream tasks prefer F-Labels obtained from different foundation models. No single foundation model is dominant in all target tasks. In particular, CLIP is not the best model for extracting F-Labels. (2) For image captioning, multimodal foundation models are more appropriate for extracting F-Labels than language foundation models. (3) Our EMMS can achieve the best results by combining F-Labels obtained from multiple foundation models.

## 6 Conclusion

How to select a pre-trained model for different tasks quickly and effectively is an important issue in the field of transfer learning. This paper proposes an efficient multi-task model selector(EMMS) that can be applied to many types of tasks. EMMS uses foundation model for Label embedding in order to transform diverse label formats of different tasks into the same form and see them as noisy labels. To estimate a model's transferability, EMMS model this problem as a simple weighted linear regression, which can be solved use an alternating minimization algorithm. Compared with existing methods, EMMS achieves the first model selection in multi-task scenarios, including image caption, referring segmentation, etc., with high speed and great results. For the **limitations** of the method, if the foundation model generalize very poor on downstream tasks, it may lead to low-quality label embedding, which is a drawback of our method. Moreover, building a holistic benchmark of various label embeddings would be useful in many applications such as multi-modal adaptation . We leave it as a future work.

## 7 Acknowledgments

This paper is partially supported by the National Key RD Program of China No.2022ZD0161000, National Key RD Program of China(NO.2022ZD0160100) and the General Research Fund of Hong

    & F8k & F30k & RSD & F10k-H & F10k-R &  & F30k & RSD & F10k-H & F10k-R \\   & \)} & \)} \\  K=1 & 0.490 & 0.386 & 0.527 & 0.772 & 0.668 & K=2 & 0.574 & 0.454 & 0.553 & 0.762 & 0.646 \\ K=3 & **0.660** & **0.504** & **0.704** & **0.802** & **0.678** & K=4 & 0.660 & 0.504 & 0.704 & 0.802 & 0.644 \\   

Table 6: EMMS under different number of F-Label of transferability assessment on image caption task. The improvement of \(K\) in a certain range brought an increase in rank correlation \(_{w}\).

    & F8k & F30k & RSD & F10kH & F10kR & Avg & SOTA/All \\   & \)} & \\  LogME(Clip) & 0.530 & 0.393 & 0.618 & 0.764 & 0.634 & 0.588 & 0/5 \\ (1) Gpt2 & 0.566 & 0.393 & 0.431 & 0.715 & 0.618 & 0.545 & 0/5 \\ (2) Bert & 0.395 & 0.319 & 0.448 & **0.802** & **0.711** & 0.535 & 2/5 \\ (3) RoBerta & 0.346 & 0.111 & 0.587 & 0.571 & 0.566 & 0.436 & 0/5 \\ (4) CLIP\({}_{B}\) & 0.453 & 0.393 & **0.704** & **0.802** & 0.634 & 0.533 & 2/5 \\ (5) CLIP\({}_{L}\) & 0.510 & 0.448 & **0.704** & **0.802** & 0.678 & 0.628 & 2/5 \\ (6) FLAVA & 0.463 & 0.382 & 0.693 & 0.704 & 0.678 & 0.584 & 0/5 \\ (7) AltCLIP & 0.453 & 0.448 & 0.623 & **0.802** & 0.678 & 0.601 & 1/5 \\ EMMS & **0.660** & **0.504** & **0.704** & **0.802** & 0.678 & **0.670** & 4/5 \\   

Table 5: The effect of the single foundation model on EMMS. The results are obtained on image captioning regarding \(_{w}\).

Kong No.17200622. Besides, thanks Ruimao Zhang from CUHK(SZ) for thoughtful discussion and Prof. Anthony Man-Cho So from CUHK for his valuable discusion about solving the WLSR in the paper. At last, this work was done during the internship at Shanghai Artificial Intelligence Laboratory.