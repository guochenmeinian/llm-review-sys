# MVInpainter: Learning Multi-View Consistent

Inpainting to Bridge 2D and 3D Editing

 Chenjie Cao\({}^{1,2,3}\), Chaohui Yu\({}^{2,3}\), Fan Wang\({}^{2,3}\), Xiangyang Xue\({}^{1}\), Yanwei Fu\({}^{1}\)

\({}^{1}\)Fudan University, \({}^{2}\)DAMO Academy, Alibaba Group, \({}^{3}\)Hupan Lab

{caochenjie.ccj,huakun.ych,fan.wj@alibaba-inc.com

{xyxue,yanweifu}@fudan.edu.cn

Corresponding Author.

###### Abstract

Novel View Synthesis (NVS) and 3D generation have recently achieved prominent improvements. However, these works mainly focus on confined categories or synthetic 3D assets, which are discouraged from generalizing to challenging in-the-wild scenes and fail to be employed with 2D synthesis directly. Moreover, these methods heavily depended on camera poses, limiting their real-world applications. To overcome these issues, we propose MVInpainter, re-formulating the 3D editing as a multi-view 2D inpainting task. Specifically, MVInpainter partially inpaints multi-view images with the reference guidance rather than intractably generating an entirely novel view from scratch, which largely simplifies the difficulty of in-the-wild NVS and leverages unmasked clues instead of explicit pose conditions. To ensure cross-view consistency, MVInpainter is enhanced by video priors from motion components and appearance guidance from concatenated reference key&value attention. Furthermore, MVInpainter incorporates slot attention to aggregate high-level optical flow features from unmasked regions to control the camera movement with pose-free training and inference. Sufficient scene-level experiments on both object-centric and forward-facing datasets verify the effectiveness of MVInpainter, including diverse tasks, such as multi-view object removal, synthesis, insertion, and replacement. The project page is https://ewrfcas.github.io/MVInpainter/.

## 1 Introduction

This paper studies editing 3D scenes by expanding one or few 2D manipulated references to other observed views. Particularly, with the development of diffusion-based text-to-image (T2I) models , we have seen substantial success in novel view synthesis (NVS) , 3D generation , and controllable generation . But most existing synthesis methods  have only been proven useful in 2D scenarios. It is intuitive to extend these pioneering methods to multi-view scenarios to bridge the gap between 2D and 3D editing. This raises the question: how to make a unified framework to generate vivid multi-view foreground objects seamlessly integrated with their surroundings?

Despite the achievements in NVS and 3D generation, achieving multi-view consistent scene editing by inserting, removing, replacing objects like Fig. 1 still present challenges. 1) _3D object generation struggles to generalize to scene-level editing._ Most object-centric 3D generation methods  are trained on large-scale synthetic datasets  with simplistic backgrounds, neglecting real-world factors like illumination and shadows, which are essential for natural editing. While some methods integrate predefined 3D assets into NeRFs  and 3D Gaussian Splatting (3DGS) , they still struggle with blending foreground and background elements seamlessly. 2) _NVS methods have_difficulty generalizing across various categories._ Even when enhanced by diffusion models, existing NVS-based methods [12; 73; 29; 1] only work in specific scenarios and often fail to generalize to diverse or unseen categories in scene data. 3) _Instance-level 3D editing is time-consuming._ Approaches like instance-level 3D editing [26; 68; 3] and warp-and-inpaint NVS [23; 30; 97] integrate priors from _single-view_ T2I models , requiring costly dataset updates to address multi-view inconsistency. 4) _Heavy reliance on explicit camera poses._ All methods mentioned rely on accurate camera poses for both training and inference, limiting scalability with pose-free data and broader applicability, particularly in scenarios like short video editing where detailed poses may be unavailable.

To this end, we present a novel perspective to enable 3D editing with a _multi-view consistent inpainting manner_. By starting with an edited 2D reference image from any 2D generative models [98; 14] and applying our method to masked sequences from other viewpoints, we achieve consistent multi-view inpainting results, effectively extending 2D generation into 3D scenarios. The key idea behind this inpainting approach is to focus on seamlessly synthesizing local regions rather than generating entirely new views. Many contextual cues like illumination, shadow, and camera motions are implicitly captured in unmasked regions, which can be activated by foundational T2I models. Thus, our method provides an end-to-end multi-view synthesis solution without requiring test-time optimization or explicit pose conditions, opening up possibilities for both 2D and 3D editing.

Formally, this paper introduces MVInpainter, a multi-view consistent inpainting model built upon a pre-trained StableDiffusion (SD) inpainting model . We incorporate domain adapters and motion modules  to MVInpainter as video priors for multi-view consistent structures. Moreover, to encourage appearance consistency, we propose the Reference Key&Value concatenation (Ref-KV), spatially concatenating the key and value features from the reference view to target ones in self-attention modules. Furthermore, MVInpainter operates without explicit poses, utilizing slot-attention mechanisms [45; 87] of encoding and grouping motion priors from unmasked surroundings' optical flow for implicitly pose control. We train two MVInpainters sharing the same pre-trained SD backbone in object-centric (CO3D , MVIngNet ) and forward-facing (Scannet++ , Real10k , DL3DV ) scenes, respectively. Experiments on unseen scenes and zero-shot datasets as Omni3D  and SPInNeRF  show the efficacy of MVInpainter in various applications, such as multi-view object removal, synthesis, insertion, and replacement.

In summary, 1) we present MVInpainter as the first multi-view consistent inpainting model to bridge 2D and 3D scene editing. 2) MVInpainter is a pose-free end-to-end approach with high-level flow-based motion control from unmasked regions. 3) MVInpainter largely simplifies the NVS difficulty, which can be generalized to all categories of in-the-wild CO3D, MVIngNet, and Omni3D datasets, as well as complicated forward-facing scenes of Scannet++, Real10k, DL3DV, and SPInNeRF.

## 2 Related Work

**Image Inpainting** devoted to filling masked regions of the image with vivid textures and structures as a long-standing challenge in computer vision, which has been widely investigated in both classical and

Figure 1: MVInpainter addresses 2D/3D editing tasks: (a) novel view synthesis, (b) multi-view object removal, and (c) object insertion and replacement through multi-view consistent inpainting ability. Given one inpainted or edited reference image, MVInpainter spreads it to other masked views without pose conditions. (d) MVInpainter could be applied to real-world 3D scene editing for dense point clouds by Dust3R  or Multi-View Stereo (MVS)  and 3DGS  with consistent generation.

learning-based methods . Compared to the low-level feature-based traditional manners [61; 36], learning-based ones gradually dominate this field and achieved substantial successes based on GANs [53; 100; 37; 8], attention mechanism [93; 90], adapted convolutions [42; 94; 70], and diffusion models [47; 62; 59]. Moreover, the image inpainting task can be further extended to reference-guided inpainting and video inpainting. Reference-guided inpainting completes the target image based on one or several reference images, which incorporates 3D information for accurate structures [105; 101] or T2I priors for examplar-based recovery and editing [86; 14; 7]. On the other hand, video inpainting methods [24; 39; 102] often include the optical flow to capture motions for superior spatial and temporal coherence. However, we should clarify that the aforementioned inpainting methods cannot achieve multi-view consistent inpainting for 3D scene editing. The reference-based manners lack multi-view consistency across all generated views, while video-based methods focus on moving foregrounds rather than the synthesis with large viewpoint changes as verified in our experiments.

**3D Generation.** Given text descriptions or reference images, 3D generative models produce high-quality 3D assets, which have been widely investigated recently, benefiting from the rapid development of diffusion-based 2D T2I models [59; 63; 4; 54; 21]. Some pioneering works with score distillation sampling (SDS) loss [75; 55] leverage priors from diffusion-based 2D supervision for the 3D generation, which is further explored to better optimization objectives [106; 79] and multi-stage learning [40; 13; 71]. On the other hand, Zero123  fine-tuned the T2I model for object-level NVS, which is further investigated for consistent multi-view synthesis [67; 66; 43; 46]. Besides, enhanced by the good 3D feature presentations, like tri-plane , and scalable network backbones , foundational 3D generation models also achieved impressive results [109; 85; 31], training with extremely large 3D datasets from scratch. However, all these works are trained with large-scale synthetic 3D objects or optimized through SDS without any interaction with complicated backgrounds, making it difficult to generalize to real-world editing scenarios with reasonable illustrations and shadows.

**Novel View Synthesis (NVS).** Before the diffusion models, most NVS works focused on learning promising feature encoder [69; 91] with blur regression-based predictions. After the development of 3D-aware diffusion models [12; 73; 1] and fine-tuning from foundational T2I models [64; 29; 82], NVS results are significantly improved. However, generating whole novel views is too challenging. Thus these methods still suffer from constrained generalization with seen categories or expensive test-time optimization. Another way is to iteratively warp and inpaint the novel views through monocular depth estimation and 2D inpainting [92; 23; 30; 97]. Despite impressive scene-level synthesis results, these methods suffer from prohibitive time costs for the warp-and-inpaint dataset update. Moreover, ambiguous depth estimations would degrade the structures of foreground objects.

**NeRF and 3DGS Editing.** With the development of NeRF  and 3DGS , many follow-ups tried to integrate them into 2D generative models, including NeRF inpainting [51; 81; 50], textual-guided semantic editing [26; 15] and local editing [108; 3; 38] based on SDS loss. InseRF  unifies both NeRF editing and 3D generation by inserting an image-to-3D generation into multi-view images for the NeRF optimization. Unfortunately, local editing-based approaches suffer from unnatural and disharmonious results, especially for the illumination and shadow in boundary regions. Furthermore, all these works require scenes with accurate camera poses and costly test-time optimization to encourage multi-view consistency, limiting their real-world applications.

## 3 Approach

**Overview.** We show the overall pipeline and contributions in Fig. 2, which is detailed in Fig. 3(a). The inputs of MVInpainter are sequential images \(^{0:N}\) of the same scene and related masks \(^{0:N}\) with \(N+1\) total views; \(^{0}\) indicates the clean reference image manipulated by any 2D editing approach, while \(^{1:N}\) are other target views needed to be inpainted by MVInpainter with consistent results.

**Multi-View Inpainting Formulation.** We focus on multi-view inpainting rather than naive NVS as it suits real-world editing better, because 1) most real-world edits do not need to synthesize complete novel views; 2) the inpainting formulation activates the inherent in-context priors from T2I models to ensure natural illumination and shadow, which are intractable to be addressed by 3D generation trained with synthetic data; 3) such a simplified formulation alleviates the training difficulty and the dependency on camera poses. Specifically, MVInpainter enjoys consistent multi-view inpainting to bridge 2D and 3D editing, which is built upon a foundation 2D inpainting model, SD1.5-inpainting  with two data prerequisites. First, the input multi-view images \(^{0:N}\) should be captured in an ordered camera trajectory to alleviate the pose requirement. Second, \(^{0}\) is a zero matrix to ensure the first view is clean without masking, while other masks \(^{1:N}\) should cover all possible regions in respective views where the target object would be placed at. To elegantly meet this demand, we propose a heuristic masking technique for the inference detailed in Sec. 3.4. Hence, the input of MVInpainter could be formulated as an ordered video sequence with \((N+1)\) frames as:

\[x_{t}=[z_{t}^{0:N};z(1-})^{0:N};}^{0:N}] ^{(N+1) H W 9},\] (1)

where \(t\) indicates the timestep in the diffusion; \(z_{t}^{0:N}\) denote the 4-channel noised latent feature of \(^{0:N}\) after the VAE encoding ; \(}^{0:N}\) and \(z(1-})^{0:N}\) mean that the downsampled masks and noise-free latent features in unmasked regions are always concatenated as the input condition. We learn MVInpainter through the epsilon prediction \(_{}\), while the MSE loss can be written as:

\[=_{x_{0},(0,1),c,t}[\| -_{}(x_{t},_{}(c),t)\|^{2}],\] (2)

where \(c\) denotes the prompt text encoded by the textual CLIP \(_{}\).

### MVInpainter Tasks

Here we tackle two related but distinct tasks: multi-view consistent object removal, and object insertion or replacement. Due to the differing challenges of each task, we employ the same structure, called MVInpainter, but trained with different multi-view data, prompts, and masking strategies, leading to two variants: MVInpainter-O and MVInpainter-F.

**Object-Centric and Forward-Facing Datasets.** Particularly, we train these two MVInpainters to handle different distributions of multi-view data: _object-centric_ and _forward-facing_ data. Object

Figure 3: (a) The overview of the proposed MVInpainter. MVInpainter-O is trained on object-centric data, while MVInpainter-F is trained on forward-facing data with a shared SD-npainting backbone of different LoRA/motion weights and masking strategies. The object-centric MVInpainter focuses on the object-level NVS, while the forward-facing one is devoted to object removal and scene-level inpainting. (b) The Ref-KV is used in spatial self-attention blocks of denoising U-Net. (c) The slot-attention based flow grouping module is used to learn implicit pose features. Dashed boxes in (b) and (c) mean feature concatenation.

Figure 2: The overall pipeline and main contributions of MVInpainter. We primarily focus on multi-view inpainting, while the 3D reconstruction is detailed in Appendix Sec. C.

centric datasets (CO3D , MVImgNet , Omni3D )2 feature a single object at the center of all views, captured with the camera circling around it. On the other hand, forward-facing datasets (Real10K , Scannet++ , DL3DV ) resemble static video data with camera movements but no specific foreground objects. Essentially, object-centric images heavily favor a single foreground object in all views, so an MVInpainter trained on them struggles to reliably remove objects without hallucinating any artifacts. Conversely, forward-facing images are not conducive to modeling multi-view object synthesis with significant viewpoint changes.

To address these challenges, we separately train MVInpainter-O and MVInpainter-F on object-centric and forward-facing datasets, respectively. They involve different LoRAs, motion modules (Sec. 3.2), and flow grouping modules (Sec. 3.3). Note that we keep the SD-inpainting backbone frozen for both models. While fine-tuning the entire SD backbone for MVInpainter-O might help convergence with mixed CO3D and MVImgNet datasets, we maintain consistent settings with MVInpainter-F for simplified discussion in this study. Please refer to Sec. B.5 of Appendix for more details.

**Prompt.** For MVInpainter-O, we empirically find that meaningful prompts \(c\) benefit to preserve the identity and appearance of objects, as these prompts provide complementary information for the object synthesis with unseen viewpoints. Thus we utilize InterLM  to extract captions for training and inference. For MVInpainter-F which is mainly used for object removal and scene completion with minor viewpoint changing, we leverage the prompt tuning technique  with 16 trainable tokens as the global prompts instead of specific descriptions for stable generation.

**Masking Strategy.** We adopt hybrid inpainting masks [70; 8] for MVInpainter, including random irregular, rectangular, and segmentation-based masks. In particular, we further employ the object-level tracking masks for MVInpainter-O training, which could be easily accomplished by SAM-tracking . Moreover, to avoid the mask overfitting towards the object shape, we follow  to randomly disturb the object masks with rectangular and irregular masks. Note that MVInpainter-O preserves a little percentage (15%) to use random masks without the object ones to encourage the cross-view learning ability for the frame interpolation and sparse-view NVS (Appendix Sec. B.2).

### Multi-View Consistent Inpainting Model

**Motion Priors from Video Models.** Since the input of MVInpainter could be regarded as the video sequence as in Eq. 1, it is intuitive to leverage motion priors from video models to improve performance. Thus we employ the domain adapted LoRA  and temporal transformers from AnimateDiff  pre-trained on video data as the initialized motion parts of MVInpainter. Although AnimateDiff is not trained for an inpainting model, we surprisingly find that it could be well converged with only a few hundred steps of fine-tuning, while both MVInpainter and AnimateDiff share the same SD1.5 backbone. Empowered by motion priors, MVInpainter achieves significantly superior structural consistency as discussed in Sec. 4.4.

**Reference Key&Value Concatenation (Ref-KV).** Inspired by [66; 33], to further ensure appearance consistency, we adopted Ref-KV in the self-attention of denoising U-Net to activate the inherent capacity of T2I models. Ref-KV spatially concatenates reference features to target keys and values to inject appearance guidance during attention aggregation as in Fig. 3(b). We clarify the originality of Ref-KV as follows. Compared to aggregating all frames , Ref-KV only focuses on the first reference view, which reduces memory and computational costs, and also substantially enhances the appearance consistency as verified in Fig. 7. Different from the reference attention in [66; 33], MVInpainter is an inpainting model, which always captures the unmasked reference latent without noise (Eq. 1), thus it is unnecessary to re-scale the latent by multiplying a large scale  or use the noise-free latent from another U-Net .

### Pose-Free Flow Grouping

Benefiting from the inpainting formulation and the ordered input sequence of MVInpainter, our approach could be trained and tested without explicit camera poses. However, it is still non-trivial to inpaint the foreground object with correct poses while the masks are large or the unmasked environment is ambiguous and textureless. To overcome this, we leverage the low-level optical flowsextracted by RAFT  to guide the MVInpainter generation. We regard the reverse \(N\)-frame optical flow as \(^{0:N-1}^{(N-1) H W 2}\). All flow inputs are masked to avoid leakage3.

In the pilot study, we first applied the 2D CNN and self-attention modules to encode the flow features and added them to the U-Net input as additional conditions. But such simple incorporation failed to improve the MVInpainter as the 'dense flow' setting in Tab. 3b. Because such an explicit dense flow injection leads to the overfitting pitfall, _i.e._, MVInpainter is strongly controlled by the flow inputs and ignores other contextual clues learned from the foundational T2I model. We hope the flow feature should carry more high-level motion characters, such as the direction and speed of the camera trajectory rather than the detailed correlation. Thus we propose the flow grouping enhanced by the slot-attention [45; 87] as shown in Fig. 3(c).

Formally, slot-attention maintains \(K\) learnable query vectors as \(^{K d}\), where \(d\) denotes the channels; \(K=4\) in this paper. Key and value are the same flow features as \(=^{HW d}\). Then the slot-attention can be formulated as:

\[(,,)=softmax_{q}(}}^{T}}{})} ^{K d},\] (3)

where \(},},}\) indicate \(W_{q},W_{k},W_{v}\) encoded by linear weights \(W_{q},W_{k},W_{v}^{d d}\). Note that the slot-attention is similar to the cross-attention except that the former should be normalized in the query dimension, while the latter is normalized in the key dimension. As the global queries, all slot features \(\) enjoy the high-level motion information aggregated by dense flow features. We take these slot features and subsequently use an FC layer to encode them into a single flow grouping embedding as shown in Fig. 3(c). According to the ablation study in Tab. 3b, we concatenate this embedding to the CLIP feature as an extra motion 'token' with comparable performance and less trainable weights. Moreover, we found that using temporal 3D attention for the flow grouping can further improve the results, while the slot features are shared across all views with more general information. Flow grouping achieves more robust guidance for MVInpainter than the dense flow features, even with some corrupted or inaccurate flow inputs.

### Inference

We show the inference pipeline of MVInpainter in Fig. 4(a). Specifically, given sequential images, MVInpainter could address the general multi-view editing through object removal and object insertion mentioned in Sec. 3.1. The object removal stage can be omitted if there are no foreground objects (inserting only) or the target object shares similar masks as the original one (replacing). For the object removal, we utilize the MVInpainter-F based on the reference image inpainted by SD-inpainting with the caption "background". For the object insertion, the reference image can be achieved by any 2D

Figure 4: (a) The inference pipeline includes object removal, mask adaption, and object insertion. (b) The illustration of heuristic masking adaption, which is built from yellow points of the closed convex hull. (c) The perspective warping based on the basic plane and the bottom face. All matches are on the basic plane filtered by Grounded-SAM  with captions “table” and “tablecloth”.

generative models, such as T2I inpainting [59; 77; 84] and exemplar-based inpainting [86; 14]. Then MVInpainter-O is leveraged to expand the single-view generation to the multi-view scenario. We propose a heuristic masking adaption to tackle the most critical issue, _i.e._, building suitable masks without any pose conditions before the insertion.

**Masking Adaption** is based on points from the closed convex hull as shown in Fig. 4(b), which can be obtained from the 3D bounding box of the foreground object. We could use open-vocabulary 3D grounding  or manually annotate four landmarks to achieve the bottom face and estimate the top face through the height of the 2D mask. To reasonably warp the 3D box, our masking adaption should meet an important and easily satisfied condition: _the 3D box's bottom face and the basic plane on which the object is placed must be the same plane_. Intuitively, the bottom face should be close to the ground. Therefore, the basic plane and the bottom face of the 3D box from different image pairs share the same perspective transformation matrix. Thanks to the dense matching , it is easy to obtain dense matching pairs on the basic plane through the Grounded-SAM  as in Fig. 4(c). Subsequently, we achieve the transformation matrix \(M\) by RANSAC  with 100 sampled matching pairs and apply the perspective warping for the new bottom face. Note that this perspective warping cannot be used on the top face, because the top face is just parallel to the basic plane rather than close to it. Instead, we get the new top face following the bottom landmarks with a constant height from the 2D mask. Finally, we fill the convex hull and optionally mask it with irregular brushes as in  or dilate the 3D box masks. The masking adaption is a flexible strategy, which not only locates the position of the object across different views but also incorporates human priors by manually annotating for some objects with very special shapes, such as the baseball bat in Appendix Fig. 13.

## 4 Experiments

**Datasets.** MVInpainter-O is trained on the object-centric data that includes full categories of CO3D  and MVImgNet . Moreover, we regard the Omni3D  as the zero-shot validation. MVInpainter-F is trained on the forward-facing data with Real10K , Scannet++ , and DL3DV , including both indoor and outdoor scenes. We further employ comparison on SPInNeRF  to verify the object removal ability. To mitigate the imbalanced category distribution, we sample an equivalent subset of scenes for each category in every epoch. All images are resized and cropped into 256\(\)256 for both inpainting and flow extraction. More details about the dataset are discussed in Appendix Sec. B.1. To the best of our knowledge, MVInpainter is the first scene-level generative model that could be generalized on all categories of both CO3D and MVImgNet.

**Dynamic Frame Sampling.** To alleviate the training costs from long sequences, we first train MVInpainter with \((N+1)=12\) frames. Then, only a few steps (1/10) of fine-tuning with dynamic frame numbers from \((N+1)\) are sufficient for a good frame number adaption. For the inpainting of more frames, frame interpolation-based inpainting introduced in Appendix Sec. B.2 should be considered. Besides, we also randomly sample the frame interval to encourage generalization.

**Training Setup.** All trainings are accomplished on 8 A800 GPUs. We train MVInpainter-O and MVInpainter-F for 100k and 60k steps with batch size 64, frame number 12, learning rate 1e-4 for 3 days and 2 days respectively. Then we fine-tune the model with dynamic frames for 10k steps.

**Metrics.** In this paper, we evaluate our method with PSNR, LPIPS , FID , and KID . We also include CLIP score  for the NVS task to verify the identity maintenance capability. For the object removal, we further compare the similarity of DINOv2 features  extracted from masked regions to evaluate the inpainting consistency, denoting average DINOv2 similarity (DINO-A) and minimal DINOv2 similarity (DINO-M) among masked patches, respectively.

### Object-Centric Results

Results of object-centric data are compared in Tab. 1 and Fig. 5. We considered two types of approaches, including NVS manners: ZeroNVS ; inpainting-based methods: Nerfiller  and LeftRefill . Note that ZeroNVS requires explicit camera poses. Besides, the inpainting-based methods are pose-free with enlarged bounding box masks to avoid masking shape leakage. Without the inpainting formulation, the scene-level ZeroNVS struggles to directly synthesize novel views without SDS. For inpainting-based approaches, LeftRefill can capture the scene's main object but fails to maintain multi-view consistency. Nerfiller only retains the shape of several views, while other views suffer from significantly inferior structures and identities. Our model outperforms all competitors with prominent achievements on both the in-domain test set and the zero-shot Omni3D.

### Forward-Facing Results

**Object Removal.** We quantitatively verify the object removal ability of MVInpainter-F on SPInNeRF test set  in the left of Tab. 2, while qualitative comparisons of the train set are shown in Fig. 8 of Appendix. Other competitors include conventional single-view based inpainting manners: LaMa , MAT , and SD-inpainting . We further compare the pose-free reference-inpainting LeftRefill  and the video inpainting manner ProPainter . Formally, MAT and SD-inpainting suffer from inconsistent inpainting results with poor DINO-A and DINO-M. Though LaMa achieves the highest DINO-M, it generates consistent blur and artifacts as in Appendix Fig. 8(a). For the reference-based manners, LeftRefill only conditions on the first view without multi-view consistency, while ProPainter performs inferior in synthesis quality. Since the small test set, FID would be largely degraded if one result contains artifacts. Our method enjoys the best performance in LPIPS, FID, and DINO-A with consistent generations without any unstable hallucination.

**Scene-Level Inpainting.** We provide quantitative results of multi-view scene inpainting in the right of Tab. 2 corrupted with large irregular masks. Our method achieves the best results of all metrics, prominently outperforming video-based inpainting, reference-guided inpainting, and other single-view inpainting approaches.

### Real-World 3D Scene Editing

We verify the in-the-wild scene editing ability of MVInpainter following Sec. 3.4 in Fig. 6, where the background images are from the unseen MipNeRF360 . MVInpainter-F achieves stable and consistent object removal, and MVInpainter-O performs high-quality multi-view generation for various object shapes based on the flexible mask adaption. Benefiting from the consistent results, our method enjoys reliable reconstruction with stereo-based Dust3R  and MVS . Sec. C of the Appendix discusses more details about the point cloud and 3DGS reconstruction.

    &  &  \\   ZeroNVS  & 12.44 & 0.606 & 41.90 & 0.981 & 0.6028 & 9.38 & 0.627 & 82.81 & 5.421 & 0.5451 \\ Nerfiller  & 18.29 & 0.310 & 36.64 & 0.491 & 0.6603 & 16.10 & 0.272 & 37.04 & 1.056 & 0.6279 \\ LeftRefill  & 17.74 & 0.283 & 38.06 & 0.826 & 0.6392 & 17.09 & 0.239 & 27.81 & 0.775 & 0.6484 \\ Ours & **20.25** & **0.185** & **17.56** & **0.154** & **0.8182** & **19.19** & **0.153** & **16.40** & **0.345** & **0.7667** \\   

Table 1: Quantitative results of object-centric NVS with enlarged bounding box masks compared on CO3D  and MVImgNet . We also include Omni3D  as a zero-shot test set without being trained by any competitor. All KID results are multiplied by 100.

Figure 5: Object-centric results on CO3D, MVImgNet, and Omni3D. The first row denotes the reference (first column) and other masked inputs, while other results are sampled from LeftRefill , Nerfiller , ZeroNVS , and our MVInpainter. Please zoom-in for details.

    &  & LPIPS\(\) & CLIP\(\) \\  Baseline & 17.16 & 0.305 & 0.750 \\ Baseline (w.o. inp) & 14.35 & 0.443 & 0.648 \\ +AnimateDiff & 17.31 & 0.308 & 0.756 \\ +Ref-KV & 17.90 & 0.283 & 0.773 \\ +Object mask & 18.64 & 0.250 & 0.796 \\ +Flow emb & **18.93** & **0.240** & **0.798** \\    (a) Ablation results of different proposed components

Table 3: Ablation studies on CO3D. ‘w.o. inp’ means the baseline without the inpainting formulation.

    &  &  \\   & PSNR\(\) & LPIPS\(\) & FID\(\) & DINO-A\(\) & DINO-M\(\) & PSNR\(\) & LPIPS\(\) & FID\(\) & KID\(\) \\  LaMa  & 28.62 & 0.054 & 15.26 & 0.8909 & **0.6019** & 17.61 & 0.337 & 38.47 & 0.981 \\ MAT  & 27.05 & 0.067 & 28.81 & 0.8727 & 0.5760 & 15.47 & 0.377 & 37.38 & 0.899 \\ SD-inpaint  & 26.98 & 0.070 & 19.32 & 0.8556 & 0.4422 & 13.54 & 0.417 & 38.67 & 1.048 \\  LeftRefill  & 30.29 & 0.102 & 18.02 & 0.8931 & 0.5652 & 15.14 & 0.380 & 38.06 & 1.334 \\ ProPainter  & **31.72** & 0.047 & 12.25 & 0.8757 & 0.5534 & 20.42 & 0.306 & 61.76 & 2.642 \\ Ours & 28.87 & **0.036** & **7.66** & **0.8972** & 0.5937 & **20.91** & **0.173** & **15.58** & **0.252** \\   

Table 2: Quantitative results of scene-level forward-facing NVS with masks. The clean SPInNeRF  dataset with consistent object masks is used to evaluate the object removal, while the unseen scenes from Scannet++ , Real10k , and DL3DV  degraded by random masks are used to verify the basic inpainting ability. All KID results are multiplied by 100.

Figure 6: Results of multi-view scene editing, including (b) multi-view object removal, (c) mask adaption, (d) multi-view object insertion. More results are shown in Fig. 13 and Fig. 14 of Appendix.

Figure 7: Qualitative ablation studies of AnimateDiff initialization and Ref-KV on CO3D.

### Ablation Study

To verify the effectiveness of each component from MVInpainter, we conduct ablation studies on CO3D as in Tab. 3. From Tab. 3(a), the baseline is built upon SD1.5-inpainting with random-initialized LoRA and motion transformer blocks, and we also compare the alternative baseline without the inpainting formulation built upon naive SD1.5. The multi-view inpainting formulation largely improves the results, and more details about the inpainting comparison are discussed in Appendix Sec. B.4. Besides, we show qualitative ablation results in Fig. 7, which indicate that video priors from AnimateDiff and Ref-KV could substantially facilitate the structure and appearance consistency respectively. We also realize that object-level tracking masks are critical for training MVInpainter-O. Moreover, we analyze the effect of flow grouping in Tab. 3(b). Without any flow guidance, our method fails in some ambiguous cases, such as large pose changes of stop signs and laptops in Fig. 12 of Appendix, while dense flow slightly hinders the identity (PSNR and CLIP). The proposed slot-attention based flow grouping outperforms the vanilla flow injection, while incorporating flow embedding by cross-attention is more lightweight with comparable performance. Further, the flow grouping can be improved with 3D temporal attention learning.

## 5 Conclusion

This paper proposes MVInpainter, a multi-view consistent inpainting method to expand 2D generations into 3D scenes by multi-view object removal, insertion, and replacement. MVInpainter enjoys a pose-free inpainting formulation built upon the SD-inpainting backbone with motion modules. Motion initialization based on video priors and Ref-KV are presented to facilitate the structure and appearance consistency respectively. Furthermore, we propose to use flow grouping based on the slot-attention to encourage implicit motion control. For the inference, we present a novel mask adaption strategy to warp object masks to novel views. Sufficient experiments on both object-centric and forward-facing datasets verified the effectiveness of MVInpainter.

**Acknowledgements.** We would like to thank Yanwei Fu. Dr. Fu is with School of Data Science in Fudan, Fudan ISTBI--ZJNU Algorithm Centre for Brain-inspired Intelligence, Shanghai Key Lab of Intelligent Information Processing, and Technology Innovation Center of Calligraphy and Painting Digital Generation, Ministry of Culture and Tourism, China. The computations in this research were supported by the CFFF platform of Fudan University.