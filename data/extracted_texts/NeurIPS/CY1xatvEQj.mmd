# DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models

Ximing Xing

Beihang University

ximingxing@buaa.edu.cn

&Chuang Wang

Beihang University

chuangwang@buaa.edu.cn

&Haitao Zhou

Beihang University

zhouhaitao@buaa.edu.cn

&Jing Zhang

Beihang University

zhang_jing@buaa.edu.cn

&Qian Yu

Beihang University

qianyu@buaa.edu.cn

&Dong Xu

The University of Hong Kong

dongxu@cs.hku.hk

Corresponding author

###### Abstract

Even though trained mainly on images, we discover that pretrained diffusion models show impressive power in guiding sketch synthesis. In this paper, we present DiffSketcher, an innovative algorithm that creates _vectorized_ free-hand sketches using natural language input. DiffSketcher is developed based on a pre-trained text-to-image diffusion model. It performs the task by directly optimizing a set of Bezier curves with an extended version of the score distillation sampling (SDS) loss, which allows us to use a raster-level diffusion model as a prior for optimizing a parametric vectorized sketch generator. Furthermore, we explore attention maps embedded in the diffusion model for effective stroke initialization to speed up

Figure 1: **Top**: Visualizations of the vector sketches generated by our proposed method, _DiffSketcher_. **Bottom**: Visualizations of the drawing process. For each example, we show two sketches with a different number of strokes.

the generation process. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual details of the subject drawn. Our experiments show that DiffSketcher achieves greater quality than prior work. The code and demo of DiffSketcher can be found at https://ximining.github.io/DiffSketcher-project/.

## 1 Introduction

Minimal representations, such as sketches and natural language, are powerful tools for effectively conveying ideas by emphasizing the subject's essence. As the carriers of abstract concepts, natural language conveys abstract semantic understanding, whereas sketches embody the human visual abstract presentation. A sketch can provide more visual details than natural language, making the concept more concrete. When a designer discusses the design plan with a client, the designer may sketch a prototype based on the client's description to ensure a full understanding of his/her requirements. If the process of text-to-sketch could be learned automatically, it would significantly lessen associated labor costs.

Unfortunately, the task of text-to-sketch generation remains unexplored. Some studies conducted on generating image-conditioned sketches [3; 18; 45; 44]. One such method is Info-drawing , which treats a sketch as a photo and uses a generative adversarial network (GAN) to generate a raster sketch based on an input image. It also introduces a style loss to ensure the generated sketches have a similar style to the reference sketch used in training. CLIPasso  presents a novel pipeline to produce vector sketches through a differentiable rasterizer optimized by a semantic loss. CLIPasso can only handle the images with a single object, so its follow-up work CLIPascene  extends to scene-level images. However, all of these methods depend on input images and do not facilitate the generation of sketches directly from text inputs. Moreover, despite their ability to generate realistic sketches from photos, these methods have a limitation in that they cannot generate _new content_.

Recent breakthroughs in text-to-image generation have been driven by diffusion models [23; 28; 30; 31] trained on billions of image-text pairs . These models, conditioned on text, now support high-fidelity, diverse, and controllable image synthesis. Nonetheless, the present text-to-image diffusion models cannot produce highly abstract and vectorized free-hand sketches (as shown in Fig. 2). We draw inspiration from the highly effective text-to-image diffusion models and image-conditioned sketch generators to build a bridge between two fundamental forms of human expression, namely text and free-hand sketches, resulting in the development of our proposed text-to-sketch generator.

In this work, we present DiffSketcher, an algorithm that can synthesize novel and high-quality free-hand vector sketches based on natural language text input. DiffSketcher does not require any text-to-sketch training pairs or large datasets of sketches. Instead, with the guidance of a pretrained diffusion model , we define a sketch as a set of Bezier curves and optimize the curves' parameters through a differentiable rasterizer . The key idea behind DiffSketcher is to transfer the prior knowledge from text-to-image generation model into the differentiable rasterizer. This allows the optimization of text-guided sketches with semantic consistency, ensuring that the final output sketches are coherent and aligned with their corresponding text inputs. However, it is a non-trivial task to fully leverage the pre-trained diffusion model to efficiently generate high-quality free-hand sketches for both simple objects and complex scenes.

To this end, we propose three strategies to improve the generation quality and efficiency: (1) Present an extended Score Distillation Sampling (SDS) Loss to guide the optimization of curve parameters. Previous works optimize the parameters of the vector sketch with CLIP loss. We found that an extended version of SDS loss can give more diverse sketch synthesis results, and it can be combined with CLIP loss or LPIPS loss, providing an additional control term. (2) Explore attention maps embedded in the diffusion model for effective stroke initialization. The synthesis process can be very time-consuming if the starting point of each stroke is randomly initialized. Therefore, we explore different initialization strategies and present to initialize the curve control points with a fused product of cross-attention maps and self-attention maps in the U-Net  of the diffusion model, which significantly improves the efficiency compared to the random initialization. (3) Introduce opacity property during the optimization of Bezier curves to better mimic the style of human sketches, achieving the effect of heavy and light brushstrokes.

In summary our contributions are three-fold: (1) We propose a text-to-sketch diffusion model dubbed DiffSketcher. To the best of our knowledge, it is the first diffusion model to generate diverse and high-quality vector sketches with different levels of abstraction at the object and scene levels. (2) We present three strategies to improve the generation quality and efficiency, including an extended SDS loss and an attention-enhanced stroke initialization strategy. The opacity property of strokes is considered during synthesis to improve the visual effect further. (3) We conduct extensive experiments, and the experimental results show that our model outperforms other models in sketch quality and diversity. Thorough insights are provided for future research.

## 2 Related Work

Sketch Synthesis.Free-hand drawings convey abstract concepts through human visual perception with minimal abstraction. Unlike purely edge-map extraction methods , free-hand sketching aims to present sketches that are abstract in terms of structure  and semantic interpretation. Therefore, computational sketching methods that aim to mimic human drawing consider a wide range of sketch representations, ranging from those grounded in the edge map of the input image[46; 17; 15; 42; 18; 4] to those that are more abstract [9; 8; 1; 7; 26; 45], which are normally in vector format. Among the works synthesizing vector sketches, CLIPasso  and CLIPascene  are conditioned on an input image, while the rest are unconditional. Until now, no prior work has explored synthesizing a sketch based on text.

Vector Graphics.Our work builds upon the differentiable renderer for vector graphics introduced by Li et al . While image generation methods that operate over vector images traditionally require a vector-based dataset, recent work has shown how the differentiable renderer can be used to bypass this limitation [36; 14; 41; 29; 19]. Furthermore, recent advances in visual text embedding contrastive language-image pre-training (CLIP) have enabled a number of successful methods for synthesizing sketches, such as CLIPDraw , StyleCLIPDraw , CLIP-CLOP , and CliPascene . A very recent work VectorFusion  combine differentiable renderer with diffusion

Figure 2: Various free-hand sketches synthesized by DiffSketcher and the corresponding description prompts. DiffSketcher obtains prior information from LDM  composite images through score distillation  and achieves the same heavy and light drawing styles as human sketches by performing gradient descent on a set of Bézier curves with the opacity property. Our proposed DiffSketcher allows for varying levels of abstraction while matching its corresponding textual semantics. In each example, given the same text prompt and two different random seeds, two sketches with a different number of strokes are generated. The red words represent the cross-attention index used to initialize the control points (details about cross-attention are covered in Section 4.2).

model for vector graphics generation, _e.g._, iconography and pixel art. Our proposed algorithm, DiffSketcher, shares a similar idea with VectorFusion, but our focus is generating object- and scene-level sketches from a natural language prompt.

**Diffusion Models.** Denoising diffusion probabilistic models (DDPMs) [37; 39; 11; 40], particularly those conditioned on text, have shown promising results in text-to-image synthesis. For example, Classifier-Free Guidance (CFG)  has improved sample quality and has been widely used in large-scale diffusion model frameworks, including GLIDE , Stable Diffusion , DALL-E 2 , and Imagen . However, the majority of images available in web-scale datasets are rasterized, and this work follows the framework of _synthesis through optimization_, in which images are generated through evaluation-time optimization against a given metric. Our proposed algorithm, _DiffSketcher_, uses a pre-trained text-to-image diffusion model to synthesize free-hand sketches from natural language input. This is achieved by transferring image synthesis prior information into a differentiable renderer.xc

## 3 Preliminaries

### Diffusion Models

In this section, we provide a concise overview of diffusion models, which are a class of generative models that utilize latent variables to gradually transform a sample from a noise distribution to a target data distribution [37; 11]. Diffusion models consist of two components: a forward process \(q\) and a reverse process or generative model \(p\). The forward process, which is typically modeled as a Gaussian distribution, gradually removes structure from the input data \(\) by adding noise over time. The reverse process, on the other hand, adds structure to the noise starting from a latent variable \(_{t}\). Specifically, the generative model is trained to slowly add structure starting from random noise \(p(_{T})=(,)\) with transitions \(p_{}(_{t-1}|_{t})\). \(q(_{t}|)=(_{t},_{t}^{2})\). Training the diffusion model with a (weighted) evidence lower bound (ELBO) simplifies to a weighted denoising score matching objective for parameters \(\)

\[_{}(,)=_{t(0,1), (,)}[w(t)\|_{}( _{t}+_{t};t)-\|_{2}^{2}]\] (1)

where \(w(t)\) is a weighting function that depends on the timestep \(t\). Our work builds on text-to-image latent diffusion model (LDM) that learn \(_{}(_{t};t,y)\) conditioned on text embeddings \(y\). LDM uses classifier-free guidance(CFG) , which jointly learns an unconditional model to enable higher quality generation via a guidance scale parameter \(:_{}(_{t};y,t)=(1+w)_{}(_{t};y,t)-w_{}(_{t};t)\) (\(_{}\) denotes the guided version of the noise prediction). CFG alters the score function to prefer regions where the ratio of the conditional density to the unconditional density is large. In practice, setting \(w>0\) improves sample fidelity at the cost of diversity.

### Score Distillation Sampling

Many 3D generative approaches use a frozen image-text joint embedding model (_e.g._ CLIP) and an optimization-based approach to train a Neural Radiance Fields (NeRF) . Such models can be specified as a differentiable image parameterization (DIP) , where a differentiable generator \(g\) transforms parameters \(\) to create an image \(=g()\). In DreamFusion , \(\) be parameters of a 3D volume and \(g\) is a volumetric renderer. To learn these parameters, DreamFusion proposed _score distillation sampling_ (SDS) loss that can be applied to Imagen :

\[_{}_{}(,=g()) _{t,}[(t)(_{}(;y,t)- )}{}]\] (2)

where the constant \(_{t}=_{t}/\) is absorbed into \(w(t)\), and the classifier-free-guided \(_{}\) is used. In practice, SDS gives access to loss gradients, not a scalar loss. Their proposed SDS loss provides a way to assess the similarity between an image and a caption:

\[_{}_{}(,=g())=_{ }_{t}[_{t}/_{t}w(t)(q( _{t}|g();y,t)|p_{}(_{t};y,t))]\] (3)

where \(p_{}\) is the distribution learned by the frozen, pretrained Imagen model. \(q\) is a unimodal Gaussian distribution centered at a learned mean image \(g()\). DreamFusion  proposed an approach to use a pretrained pixel-space text-to-image diffusion model (Imagen ) as a loss function. However,diffusion models trained on pixels have traditionally been used to sample only pixels. We want to create what look like good sketches that match the text prompts when rendered from a set of vector strokes. Such models can be specified as a differentiable image parameterization, where a differentiable rasterizer \(\) transforms parameters \(\) to create a sketch \(=()\).

Inspired by DreamFusion , we extend _score distillation sampling_ (SDS) loss to use a pretrained latent diffusion model  as a prior for optimizing curve parameters. Intuitively, score distillation converts diffusion sampling into an optimization problem that allows the raster image to be represented by a differentiable rasterizer.

### Differentiable Rasterizer

Li _et al._ introduce a differentiable rasterizer \(\) that bridges the vector graphics and raster image domains. A raster image is a 2D grid sampling over the space of the vector graphics scene \(f(x,y;)\), where \(\) contains the curve parameters, _e.g.,_ coordinates of Bezier control points, opacity, line thickness. Given a 2D location \((x,y)^{2}\), they first find all the filled curves and strokes that overlap with the location. Then sort them with a user-specified order and compute the color using alpha blending . It enables the gradients of the rasterized curves (image) to be backpropagated to the curve parameters.

## 4 Methodology

In this section, we present our method for generating sketches using pre-trained text-to-image diffusion models. Let \(\) be a sketch that was rendered by a differentiable rendering \(\) using the text prompt \(\). Our goal is to optimize a set of parametric strokes to automatically generate a vector sketch that matches the description of the text prompt.

Our pipeline is illustrated in Figure 3. Given a text prompt \(y\) of the desired subject and a set of control points for the strokes, we synthesize the corresponding sketch \(\) while matching the semantic attributes of the text prompt. To initialize the control points of the strokes, we extract and fuse the attention map of the U-Net  used by the latent diffusion model . Our key observation is that the structure and appearance of the generated image depend on the interaction between the pixels and the text embedding through the diffusion process . We provide more details on strokes initialization in Section 4.2. As shown in Figure 4, in each step of the optimization, we feed the stroke parameters to a differentiable rasterizer \(\) to produce the raster sketch. We optimize over parameters \(\) such that \(=()\) is close to a sample from the frozen latent diffusion model . To perform this optimization, we propose a variation of the SDS loss function that enhances the texture of hand-sketched strokes and improves the resilience of drawing styles. This results in the raster sketch always being coherent with the text prompt. The resulting sketch, as well as the sample from the frozen latent diffusion model, then defines a joint semantic and perceptual loss. We backpropagate the loss through the differentiable rasterizer \(\) and update the control points and opacity of strokes directly at each step until convergence of the loss function.

### Synthesis Through Optimization

We define a sketch as a set of \(n\) strokes \(\{s_{1},,s_{n}\}\) with the opacity attribute placed on a white background. To represent each stroke, we use a two-dimensional Bezier curve with four control points \(_{i}=\{p_{i}^{j}\}_{j=1}^{4}=\{(x_{i},y_{i})^{j}\}_{j=1}^{4}\) (Notice that here, \(x\) and \(y\) represent the coordinates in the canvas) and one opacity attribute \(o_{i}\). We incorporate the opacity of the strokes into the optimization process and use DiffSketcher semantics understanding to achieve a human-like _heavy and light sketch style_. The parameters of the strokes are fed to a differentiable rasterizer \(\), which forms the raster

Figure 3: The overview of the pipeline. DiffSketcher accepts a set of control points (the locations of the strokes) and text prompts as input to generate a hand-drawn sketch.

sketch \(=((s_{1},o_{1}),(s_{n},o_{n}))=((\{p_{1}^{j} \}_{j=1}^{4},o_{1}),,(\{p_{n}^{j}\}_{j=1}^{4},o_{n}))\). For simplicity, we define the parameter in \(\) as \(\).

#### 4.1.1 Vanilla Version: Fidelity to Generated Image

We start with a two-stage pipeline: First, we sample an image from the latent diffusion model  using a text prompt. Next, we optimize the control points to obtain a sketch that is consistent with the text prompt. To preserve the fidelity of the generated sample, we incorporate a **J**oint **V**isual **S**emantic and **P**erceptual (JVSP) loss to optimize the similarity of the synthesized sketches and the instance sampled from the frozen latent diffusion model . We leverage the VAE  decoder \(\) to get the RGB pixel representation of \(}_{}(_{t}|y;t)\). Then, we jointly use the LPIPS  and CLIP visual encoders  as depth structural similarity and visual semantic similarity metrics, respectively. Specifically, we use the following loss function:

\[_{}=_{}((}_{}(_{t}|y;t)),())+_{l}\| _{l}((}_{}(_{t}|y;t)))- _{l}(())\|\] (4)

The \(_{}\) loss function encourages the synthesized sketch to match the underlying semantics and perceptual details of the image sampled from LDM, leading to more realistic and visually appealing results. While vectorizing a rasterized diffusion sample is lossy, an input **A**ugmentation version of the **SDS** (ASDS) loss can either finetune the results or optimize random control points from scratch to sample a sketch that is consistent with the text prompt. In the Sec 4.1.2, we introduce the ASDS loss to match the text prompt.

#### 4.1.2 Augmentation SDS Loss: Fidelity to Text Prompt

To synthesize a vector sketch that matches a given text prompt, we directly optimize the parameters \(\) of the differentiable rasterizer \(\) that produces the raster sketch \(\). We propose an input augmentation version of the SDS loss function to perform this optimization, which encourages plausible images to have low loss and implausible images to have a high loss. Given a raster sketch \(}^{H W 3}\), we combine _RandomPerspective_, _RandomResizedCrop_ and _RandomAdjustSharpness_ to get a data augmentation version of \(}}_{}^{512 512 3}\). These transformations preserve artistic styles, enhance style diversity, emphasize crucial features, and improve the model's adaptability to various artistic renditions and size variations commonly found in black-and-white hand-drawn sketches. Then, the LDM uses a VAE encoder  to encode \(}}_{}\) into a latent representation \(=(}}_{a})\), where \(^{(H/f)(W/f) 4}\) and \(f\) is the encoder downsample factor. In summary, we use the following

Figure 4: Optimization overview. To synthesize a sketch that matches the given text prompt, we optimize the parameters of the differentiable rasterizer \(\) that produces the raster sketch \(\), such that the resulting sketch is close to a sample from the frozen latent diffusion model (the blue part of the picture). Since the diffusion model directly predicts the update direction, we do not need to backpropagate through the diffusion model; the model simply acts like an efficient, frozen critic that predicts image-space edits.

ASDS loss function:

\[_{}_{}(,=()) _{t,,a}[w(t)(_{}(_{t};y,t)-)}{}_{a}}}_{a}}{}]\] (5)

Where the weighting function \(w(t)\) is a hyper-parameter. And we sample \(t(0.05,0.95)\), avoiding very high and low noise levels due to numerical instabilities. Like DreamFusion , we set \(=100\) for classifier-free guidance and higher guidance weights give improved sample quality. Intuitively, this loss perturbs \(}_{a}\) with a random amount of noise corresponding to the timestep \(t\), and estimates an update direction that follows the score function of the diffusion model to move to a higher density region. The ASDS loss function encourages the synthesized sketch to match the given text prompt, while also preserving the style and structure of the original sketch. At each iteration, we backpropagate the loss through the differentiable rasterizer and update the control points and opacity of strokes directly until convergence of the loss function.

**Loss objectives of our final model.** To further enhance the quality of the synthesized sketches, we incorporate the JVSP and ASDS losses. Specifically, we first obtain the initial results with the JVSP loss, then we fine-tune the differentiable rasterizer together with the ASDS loss. We found such design can achieve the best performance. The ASDS loss predicts the gradient update direction directly, thus its loss value does not require weight balancing. In the JVSP loss, we set the weight of the LPIPS item to 0.2, and the weight of the CLIP visual item to 1. To compute the L2 distance between intermediate level activations of CLIP, we follow the CLIPasso  method and use layers 3 and 4 of the ResNet101 CLIP model. As shown in Figure 8, we compare the performance of the vanilla version with that of the version using only the ASDS loss, and the proposed final version. Our experiments show that the final version improves both the generation quality and efficiency.

### Joint Attention-based Stroke Initialization

The highly non-convex nature of the ASDS loss function makes the optimization process susceptible to initialization, especially in multi-instance scenarios where strokes must be carefully placed to emphasize the overall semantics conveyed by free-hand sketching. We improve the convergence towards semantic depictions by initializing the curve control points based on the attention map of the text prompt conditional latent diffusion model. The UNet in the LDM has two types of attention mechanisms (the blue part), self-attention and cross-attention, as shown in Figure 5 where the yellow part represents the cross-attention visualization result, and the green part represents the self-attention visualization result. The structure and appearance of the image generated by the LDM  depend on

Figure 5: Strokes Initialization. The blue part of the figure represents the UNet in the LDM, which has two types of attention mechanisms: self-attention and cross-attention. The yellow and green parts respectively depict the visualization results of the cross-attention and self-attention. The gray part shows how the initial strokes are generated using a fused attention map. The dashed box represents the attention fusion, which is composed of the mean of the self-attention map and the cross-attention map corresponding to the \(5\)-th text prompt token (“Tower”). We start at \(1\)-th token, because \(0\)-th token is taken up by the CLIP starting token.

the interaction between the pixels to the text embedding through the diffusion process , which is manifested in the cross-attention layer . The visualization results indicate that the cross-attention layers control the relationship between the spatial layout of the image and each word in the prompt, while the self-attention layer affects the spatial layout and geometric structure of the generated image. With this observation, we linearly combine the probability distributions of the two attention maps to initialize the curve control points. Specifically, we pick a map in the cross-attention maps based on a text token and combine it with the averaged self-attention map. This process is formalized as \(=*_{i}+(1-)*\), where \(\) is the control coefficient and \(i\) indicates the \(i\)-th token in the text prompt. Finally, we normalize the fused attention map using softmax and use it as a distribution map to sample \(n\) positions for the first control point \(p_{i}^{1}\) of each Bezier curve. The other three control points (\(p_{i}^{2}\),\(p_{i}^{3}\),\(p_{i}^{4}\)) are sampled within a small radius (0.05 of image size) around \(p_{i}^{1}\) to define the initial set of Bezier curves \(\{\{p_{i}^{j}\}_{j=1}^{4}\}_{i=1}^{n}\). Empirical results show that our attentional fusion-based initialization contributes significantly to the quality and rendering speed of the final sketch in comparison to the random initialization.

## 5 Results

### Qualitative Evaluation

As shown in Figure 2, we demonstrate that our approach offers the ability to produce object-level and scene-level sketches based on a textual prompt, with the flexibility to manipulate the level of abstraction through the number of strokes. It is effective in generating accurate sketches regardless of prompt complexity, including simple or non-existent objects, iconic constructions, and detailed scenes. The utilization of the robust prior of stable diffusion allows for a favorable initialization, promoting the production of high-quality sketches with significantly fewer iterations. Figure 6 qualitatively compared our DiffSketcher with those of CLIP-based  and edge extraction-based methods . The Canny edge extraction algorithm extracts an excessive number of edges and produces untidy sketches, as observed in the first example in Fig. 6. CLIPasso uses visual distance metrics to guide gradient-based optimization. On the task of drawing scene-level sketches, CLIPasso can only draw part of the foreground, and the background part is missing.

We also compare our work with the recent work VectorFusion . The results are shown in Fig. 7. This work is highly relevant to ours as it also explores the potential of diffusion model in generating vector graphics. However, there are notable differences between our approach and VectorFusion in terms of task setting, model design, and performance. Firstly, our method, DiffSketcher, primarily focuses on generating vector sketches based on text input. In contrast, VectorFusion aims to generate a broader range of vector graphics, including iconography and pixel art. It is important to highlight that our method has the capability to easily extend its functionality to generate other types of vector graphics _without_ changing primitives (as shown in Fig. 10 of Supp. B). Secondly, DiffSketcher follows a distinct pipeline compared to VectorFusion. VectorFusion employs different model variants for different types of vector graphics. For generating vector sketches, our approach differs from VectorFusion mainly in two aspects: (1) **Initialization**: VectorFusion randomly initializes the

Figure 6: Comparison with existing methods, including edge extraction  and vector sketching [45; 44].

rasterizer (i.e., the location of the control points), while our approach utilizes the attention layer of LDM, guided by text prompts, for initialization. This significantly improve both efficiency and synthesis quality. (2) **Optimization**: While VectorFusion solely optimizes the position of control points, our method also optimize the opacity of stroke. This contributes to enhance the visual quality of the synthesized sketches. As shown in Fig. 7, our approach significantly outperforms VectorFusion in generating vector sketches, and achieves comparable performance in generating other types of vector graphics (see Fig. 10 of Supp. B). It is worth noting that our model variant shown in Fig. 7 utilizes only ASDS loss for optimization and initializes the strokes randomly. This was done to ensure a fair comparison with VectorFusion. Our full model is expected to deliver better performance.

### Quantitative Evaluation

Evaluating text-to-sketch synthesis is challenging due to the absence of ground truth sketches. Therefore, we focus on three indicators: consistency between the generated sketch and text prompt, the aesthetic quality of the sketch, and the recognition accuracy of the sketch. To measure the consistency between the generated sketch and input text, we calculate the mean of cosine similarity of CLIP embeddings for the generated sketches and the text captions used to generate them. Our method achieves a cosine similarity of \(0.3494\), which is higher than the \(0.328\) achieved by Canny algorithm and the \(0.3075\) achieved by CLIPasso. Aesthetic appeal is subjective, and being visually appealing is a personal experience and preference. However, when describing the visual appeal of a hand-drawn sketch, various factors can be considered, such as line quality, texture, and style. As an evaluation of our proposed method, we use CLIP-based aesthetic indicators  to calculate the aesthetic value for samples of multiple categories. Figure 6 compares aesthetic values achieved by various methods. Our method achieves a mean value of \(4.8206\) for the aesthetic value on a large number of examples, which is higher than the \(4.3682\) achieved by Canny and the \(4.0821\) achieved by CLIPasso .

### Ablation Study

We conducted a series of experiments to demonstrate the effectiveness of the proposed initialization strategy and the effects of ASDS loss and JVSP loss, respectively. The top row of Fig. 8 compares two initialization strategies used by DiffSketcher and CLIPasso , namely LDM Attention and CAM  Saliency map, respectively. The text-driven diffusion model produces more precise attention map than the saliency map obtained by the CLIP-based method due to LDM's superior generation ability. The cross-attention feature of LDM can efficiently activate relevant regions based on token areas, while the self-attention layer can effectively differentiate foreground from background down to the pixel level. Through combining both mechanisms, the initialization area becomes more precise. As a result, superior quality sketches are produced with a reduced number of optimization steps, as illustrated in the second and third rows of Fig. 8. The proposed initialization strategy improves sampling quality and efficiency, which is critical for non-convex objective function optimization.

Figure 7: Qualitative comparison with VectorFusion(VF) . VF’s results are based on its original paper and project website, with a text prompt suffix of “minimal 2D line drawing trending on ArtStation”. Note that our results were optimized from scratch using ASDS _without_ specifically designed text prompt suffix.

In DiffSketcher, our JVSP loss consists of a CLIP loss and a LPIPS loss, and it is important to note that ASDS and JVSP do not conflict with each other. As illustrated in Fig. 8, the 4th and 8th rows highlight the effects of JVSP loss and ASDS loss, respectively. When only JVSP loss is used (4th row), the generated sample closely approximates the result of LDM. On the other hand, when only ASDS loss is employed (8th row), the generated sample aligns with the text prompt semantically, but does not follow the attention map of the LDM. Additionally, using ASDS loss results in more diverse sampling outcomes during the synthesis process. For instance, the location of the astronaut's head may change throughout the drawing process when ASDS loss is used, while it remains in the same location when the JVSP loss is employed. We utilize both JVSP loss and ASDS loss for optimization. As shown in the 5th row (labeled as "DiffSketcher"), the combination of these two losses leads to synthesized sketches with more intricate details and a visually more realistic appearance.

## 6 Conclusion

In this work, we have proposed a novel and effective approach to bridge the gap between natural language and free-hand sketches. By leveraging the power of pretrained text-to-image diffusion models, we have developed a method that can generate diverse and high-quality vector sketches based on textual input, without the need for large-scale datasets of sketches or sketch-text pairs. We have also explored different aspects of the model design, such as the stroke initialization strategies, choice of loss functions, and properties of strokes in the differentiable rasterizer, which provide valuable insights for future research. With further research and development, our proposed model, DiffSketcher, has the potential to be a valuable tool in various domains, such as design and education.

**Limitation.** Our approach has two limitations. Firstly, there is a lack of correlation between the

Figure 8: Qualitative results of ablation study. **Top**: two initialization strategies used by DiffSketcher (_i.e._, LDM Attention) and CLIPasso  (_i.e._, CAM  Saliency map). A result of sampling from LDM. The **2nd** and **3rd** rows: Comparison of the two initialization strategies on the convergence speed. The **4th** and **5th** rows: The effect of JVSP (Section 4.1.1) and ASDS loss (Section 4.1.2). The **5th** row shows the loss of SDS without data augmentation. The **6th** and **7th** rows: ASDS loss leads to more diverse results when strokes are randomly initialized. Text prompt used in this example: “Astronaut on Asteroid, galaxy background”.

text prompt and sketch abstractness, which may result in unsatisfactory sketches. To address this, we suggest establishing a link between the complexity of the text prompt and the number of strokes used. More details can be found in Supp. I. Secondly, the style of the generated sketches is limited. A possible solution is introducing a style transfer module during the sketch synthesis process.

## 7 Acknowledgement

This work is supported by the National Natural Science Foundation of China (No. 62002012, No. 62006012, and No. 62132001) and CCF-Baidu Open Fund. It is also supported in part by The Hong Kong Jockey Club Charities Trust under Grant 2022-0174, in part by the Hong Kong Research Grant Council under General Research Fund (17203023), in part by the Startup Funding and the Seed Funding for Basic Research for New Staff from The University of Hong Kong, and in part by the funding from UBTECH Robotics.