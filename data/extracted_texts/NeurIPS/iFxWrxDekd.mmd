# Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks

Feng Chen

Equal contribution. Ordered alphabetically.

Daniel Kunin\({}^{*}\) &Atsushi Yamamura (\(\{\)\(\{\)\(\{\)\(\{\)\(\{\)\(\{\)\(\}\}\)\}\)\(\}^{*}\) &Surya Ganguli

Stanford University

{fengc,kunin,atsushi3,sganguli}@stanford.edu

Equal contribution. Ordered alphabetically.

###### Abstract

In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify _invariant sets_, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler (sparse or low-rank) subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of _stochastic attractivity_ towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss. We observe empirically the existence of attractive invariant sets in trained deep neural networks, implying that SGD dynamics often collapses to simple subnetworks with either vanishing or redundant neurons. We further demonstrate how this simplifying process of _stochastic collapse_ benefits generalization in a linear teacher-student framework. Finally, through this analysis, we mechanistically explain why early training with large learning rates for extended periods benefits subsequent generalization.

## 1 Introduction

The remarkable performance of modern deep learning systems relies on a complex interplay between a training dataset, a network's architecture, and an optimization strategy. Contrary to traditional statistical learning theory, these highly expressive models exhibit impressive generalization capabilities on natural tasks, even without explicit regularization, despite having the capacity to memorize random data . This phenomenon is often attributed to implicit biases introduced in the training process that drive the learning dynamics towards models with low-complexity, thereby improving generalization. It is widely believed that a central source of this implicit bias is the randomness introduced by stochastic gradient descent (SGD) . In this work we discuss SGD's role in attracting the dynamics, throughout training, towards subsets of parameter space that correspond to simpler subnetworks. We reveal that the architecture of modern deep neural networks as well as the nonlinear activation function plays a crucial role in forming these subsets, thus providing a novel perspective on the source of SGD's implicit bias. Our contributions are as follows:

1. We introduce _invariant sets_ as subsets of parameter space that, once entered, trap SGD. We characterize two such sets that correspond to simpler subnetworks and appear extensively in modern architectures: one for vanishing neurons and the other for identical neurons (Sec. 3).

2. We formulate a sufficient condition for _stochastic attractivity_ -- a process attracting SGD dynamics towards invariant sets -- that reveals a competition between the loss landscape's curvature around an invariant set and the noise introduced by stochastic gradients (Sec. 4).
3. We apply the attractivity condition to determine when neurons with origin-passing activation functions collapse their parameters to zero, effectively removing the neuron. We empirically show that the parameters of permutable neurons within the same hidden-layer can collapse towards invariant sets corresponding to identical neurons (Sec. 5).
4. We demonstrate how this process of _stochastic collapse_ influences generalization in a linear teacher-student framework. We apply these findings to shed light on the empirically observed importance of maintaining a large learning rate for an extended period during the early stages of training (Sec. 6).

## 2 Related Work

**Implicit biases of SGD.** Several recent works have explored the properties of SGD and its implicit biases. Barrett and Dherin  showed how SGD can be interpreted as adding implicit gradient norm regularization and Geiping et al.  showed that training full-batch gradient descent while explicitly adding this regularization can achieve the same performance as SGD. Kunin et al.  showed how the anisotropic structure of SGD noise effectively modifies the loss and HaoChen et al.  showed that parameter-dependent noise introduces an implicit bias towards local minima with smaller variance, while spherical Gaussian noise does not. Blanc et al.  and Damian et al.  studied the dynamics of gradient descent with label noise near a manifold of minima and proved that SGD implicitly regularizes the trace of the Hessian. Li et al.  developed a framework to study this bias of SGD by considering the projection of the trajectory to the manifold. Kleinberg et al.  showed how SGD is effectively operating on a smoother version of the original loss allowing it to escape sharp local minima. Zhu et al.  further demonstrated how the anisotropic structure of SGD noise helps SGD escape efficiently. Xie et al.  demonstrated that the covariance matrix of SGD approximates the Hessian around local minima, and thus the escape rate from a minima is linked to the Hessian eigenvalues or flatness. While numerous studies have sought to identify the origin of SGD's implicit bias, most have predominantly focused on how SGD introduces an implicit regularization term [3; 4; 5], minimizes a measure of curvature among equivalent minima [7; 8; 9], or escapes from sharp local minima [10; 11; 12; 13]. In our work, we provide a novel perspective and discuss how stochastic gradients introduce a strong attraction towards regions of parameter space associated with simpler subnetworks, even when this attraction is detrimental to the full-batch train loss.

**Simplicity biases.** Several works have explored implicit biases that encourage some notion of sparsity or low-rankness in neural network learning dynamics. Kunin et al.  showed how gradient flow training with an exponential loss can lead to sparse solutions via an implicit max-margin process. Woodworth et al.  demonstrated how an implicit \(L_{1}\) penalty occurs for diagonal linear networks trained with gradient flow in the limit of small initialization. Nacson et al.  and Pesme et al.  showed how large step sizes and stochastic gradients further bias these networks towards the sparse regime. Andriushchenko et al.  observed that longer training with large learning rates keeps SGD high in the loss landscape where an implicit bias towards sparsity is stronger and theoretically analyzed diagonal linear networks, showing that an associated SDE has implicit bias towards sparser representations. Vivien et al.  studied the role of label noise in inducing an implicit Lasso regularization for quadratically parameterized models. Kunin et al.  and Ziyin et al.  showed how weight decay induce a bias towards rank minimization for linear networks. Galanti et al.  and Wang and Jacot  extended this observation to deep settings trained with SGD. Jacot  theoretically, and Huh et al.  empirically, showed an implicit bias towards learning low-rank functions with increasing depth. Gur-Ari et al.  showed that gradients of SGD converge to a small subspace spanned by a few top eigenvectors of the Hessian. Ziyin et al.  demonstrated how data augmentation can promote dimensional collapse of representations in self-supervised learning. While many works have explored the emergence of sparsity or low-rankness during training, much of the analysis is confined to a particular architecture or consists of general empirical observations lacking a clear underlying mechanism. In our work, we focus on a fundamental characteristic of neural networks trained with SGD that leads to novel empirical observations.

Our analysis is closely related to a recent work studying failure modes of SGD  and a concurrent follow up work studying the stability of SGD near a fixed point . In App. A we further discuss these works and other relevant works.

## 3 Invariant Sets of SGD Generated by Reflection Symmetries

Throughout this work, we consider a feed-forward network2,

\[f_{}(x)=w^{(m)}(w^{(m-1)}(w^{(1)}x+b^{(1)} )+b^{(m-1)})+b^{(m)}, \]

parameterized by \(=(w^{(1)},b^{(1)},,w^{(m-1)},b^{(m-1)},w^{(m)},b^{(m)})^{d}\), where \(w\) and \(b\) denote weights and biases respectively, the superscript indexes the \(m\) layers, and \(()\) is an activation function. The network is trained by stochastic gradient descent (SGD) over a training dataset \(\{(x_{1},y_{1}),,(x_{n},y_{n})\}\) of size \(n\), where \(x_{i}^{p}\) and \(y_{i}^{c}\). The network parameters are randomly initialized at \(^{(0)}\) and iteratively updated according to

\[^{(t+1)}=^{(t)}-_{i^{(t)}} _{}(^{(t)};x_{i},y_{i}), \]

where \(>0\) is the learning rate, \(^{(t)}[n]\) is a random3 index set (mini-batch) of size \(\) at step \(t\), and \((;x_{i},y_{i})\) is a loss function. We let \(()=_{i[n]}(;x_{i},y_{i})\) denote the train loss averaged over the entire training dataset. Despite the simplicity of SGD's optimization procedure, understanding how it can navigate through complex, high-dimensional, non-convex loss landscapes to find generalizing solutions is still a mystery. Here, we take a dynamical systems perspective and identify subspaces of the parameters that are preserved under the SGD update equation (Eq. 2).

**Definition 3.1** (Invariant Set of SGD).: _A Borel-measurable set \(A^{d}\) is an invariant set of SGD if given any initialization \(^{(0)} A\), all future iterates of SGD \(^{(t)}\) for \(t 0\) are contained within \(A\) almost surely, for any batch size \(\), learning rate \(>0\), and mini-batches \(\{^{(t)}[n]:t\}\)._

From this definition, it is immediately clear that all of parameter space \(A=^{d}\) and any interpolating point \(A=\{_{*}\}\) such that \((_{*})=0\) are invariant sets of dimension \(d\) and \(0\) respectively. However, the highly over-parameterized and layer-wise structure of neural network architectures lead to many additional, non-trivial invariant sets. We will focus on two fundamental invariant sets that appear ubiquitously in neural networks and correspond to simpler (sparse or low-rank) subnetworks.

**Proposition 3.1** (Sign Invariant Sets).: _Consider a hidden neuron \(p\) within layer \(l\) of a feed-forward neural network. Let \((w^{(l)}_{in,p},b^{(l)}_{p})\) and \(w^{(l+1)}_{out,p}\) denote the parameters directly incoming and outgoing from the neuron respectively4. If the non-linearity \(\) is origin-passing (\((0)=0\)), then the axial subspace \(A=\{^{d}|w^{(l)}_{in,p}=0,b^{(l)}_{p}=0,w^{(l+1)}_{out,p}=0\}\) is an invariant set._

The subspace corresponding to a sign invariant set represents the parameter space of a sparse subnetwork obtained by removing a hidden neuron. Most modern neural network architectures employ origin-passing activation functions, which includes linear, hyperbolic tangent, Rectified Linear Unit (ReLU) , Leaky ReLU , Exponential Linear Unit (ELU) , Swish , Sigmoid Linear Unit (SiLU), and Gaussian Error Linear Unit (GELU) . Networks with any of these functions will exhibit invariant sets of this nature for each hidden neuron. A fully-connected network of depth \(m\) and width \(k\) possesses \((m-1)k\) distinct sign invariant sets.

**Proposition 3.2** (Permutation Invariant Sets).: _Consider two hidden neurons \(p,q\) within the same layer \(l\) of a feed-forward neural network. Let \((w^{(l)}_{in,p},b^{(l)}_{p}),(w^{(l)}_{in,q},b^{(l)}_{q})\) denote the parameters directly incoming to the neurons, and \(w^{(l+1)}_{out,p},w^{(l+1)}_{out,q}\) the parameters directly outgoing from the neurons. The affine subspace \(A=\{^{d}|w^{(l)}_{in,p}=w^{(l)}_{in,q},b^{(l)}_{p}=b^{(l)}_{ q},w^{(l+1)}_{out,p}=w^{(l+1)}_{out,q}\}\) is an invariant set._

The subspace corresponding to a permutation invariant set represents the parameter space of a low-rank subnetwork obtained by constraining two neurons to be identical. All feed-forward networks possess this permutation invariance among the neurons in each hidden-layer. For a fully-connected network of depth \(m\) and width \(k\) there are \((m-1)\) distinct permutation invariant sets.

**Invariant sets generated by symmetry.** The presence of sign and permutation invariant sets within neural networks is a direct result of reflection symmetries inherent in their architectural design. In thiscontext, a _symmetry_ is defined as any transformation of the parameter that preserves the network's input-output mapping, regardless of the input. As stated in the following theorem, any approximate linear symmetry defined by a symmetric or orthogonal matrix generates an invariant set:

**Theorem 3.1** (Symmetry Induced Invariant Sets).: _Let \(Q^{d d}\) be a symmetric (\(Q=Q^{}\)) or orthogonal matrix (\(QQ^{}=I_{d}\)). The affine subspace \(A=\{^{d}|Q=\}\) is an invariant set if the loss function \((;x_{i},y_{i})\), for any \(i[n]\), is approximately \(Q\)-symmetric around \(A\), i.e., for any \(>0\), there exists \(>0\) such that \(|(Q;x_{i},y_{i})-(;x_{i},y_{i})|< d(,A)\) for any \(^{d}\) satisfying \(d(,A)<\) where \(d(,A)\) is the Euclidian distance between \(\) and \(A\)5._

Remarkably, the sign and permutation invariant sets defined earlier can be understood through this symmetry perspective. In both cases, the symmetry is defined by a symmetric and orthogonal matrix. This class of matrices describes the generalized notion of a reflection in \(d\)-dimensional space and the invariant sets are the axes of reflection, which can be at most \((d-1)\)-dimensional. In the case of permutation invariant sets, any two hidden neurons in the same layer of a network can be permuted without changing the input-output function of the network, and thus the affine subspace \(A\) where these two neurons are identical is an invariant set. On the other hand, in the case of sign invariant sets, the symmetric transformation \(Q\) corresponds to a simultaneous sign flip of a neuron's input/output weights. This transformation does not change the input-output function of the network, as long as the activation function is odd. However, even for origin-passing activation functions that are not exactly odd, the loss function is approximately \(Q\)-symmetric around \(A\), since the activation function is approximately odd around its origin. Therefore, Theorem 3.1 implies that the axial subspace \(A\), where input and output weights are exactly zero, is an invariant set.

**Additional invariant sets.** In App. B, we discuss other classes of invariant sets, associated with softmax nonlinearities, low dimensional structure in data, and provide a further discussion on the connection between symmetry and invariant sets. We also discuss how our definition of invariant sets relates to other works  where symmetry and invariance are used to explore geometric properties of the loss landscape and the presence of critical points.

## 4 Gradient Noise Attracts SGD Dynamics towards Invariant Sets

To study the dynamics of SGD we will approximate6 its trajectory with a stochastic differential equation (SDE), a common analysis technique applied in many works . SGD can be interpreted as full-batch gradient descent plus a per-step gradient noise term introduced by the random minibatch. Let \(_{i}()\) represent the per-sample gradient noise at \(\). It is easy to check that \([_{i}()]=0\) for all \(\). This motivates studying the SDE, which we refer to as _stochastic gradient flow (SGF)_,

\[d_{t}=-(_{t})dt+}( _{t})dB_{t},_{0}=^{(0)}. \]

Here \(B_{t}\) denotes a standard \(d-\)dimensional Wiener process and \((_{t})=[_{i}()_{i}()^{}]}\). The drift term of this SDE is the negative full-batch gradient \(-(_{t})\), while the diffusion is determined by a spatially-dependent diffusion matrix \(D(_{t})=(_{t})(_{t})^{}\). Although this SDE cannot be interpreted as the continuous-time limit of SGD, it matches the first and second moments of the SGD process. See Appendix D for further discussions on the relationship between SGD and SGF. Additionally, SGF preserves all affine invariant sets of SGD:

**Proposition 4.1** (Informal).: _All affine invariant sets of SGD are also affine invariant sets of SGF. 7_

Like deterministic dynamical processes, the stochastic processes of SGF can be attracted toward the invariant sets. To describe this attraction, we adopt the concept from stochastic control theory .

**Definition 4.1** (Stochastic Attractivity).: _An invariant set \(A^{d}\) of a stochastic process \(\{_{t}^{d}:t 0\}\) is stochastically attractive8 if for any \(>0\) and \(>0\), there exists \(>0\) such that for any \(^{d}\) with the Euclidian distance \(d(,A)<\),_

\[[_{t 0}d(_{t},A)_{0}= ]. \]Intuitively, a set is stochastically attractive if there always exists a close initial condition that guarantees all future iterates remain close to the set with high probability. Understanding when the dynamics are attracted by a particular invariant set becomes fundamental for understanding the implicit bias of SGD. To gain insight into this process lets consider a canonical example in one-dimension.

**Geometric Brownian motion.** Geometric Brownian Motion (GBM) is a linear stochastic process given by the SDE, \(d_{t}=_{t}dt+_{t}dB_{t}\), where \(\) represents the drift rate, \(\) the volatility rate. Denote \(_{0}\) as the initialization. The set \(A=\{0\}\) is an invariant set of GBM, which, depending on the relationship between \(\) and \(\), can be stochastically attractive. As the trajectory of GBM approaches the invariant set, its movement diminishes. To account for this effect, we take the logarithmic transformation \(z_{t}=(_{t})\), where we assume without loss of generality that \(_{0}>0\). The stochastic process \(z_{t}\) obeys the transformed SDE, \(dz_{t}=(-^{2}/2)dt+ dB_{t}\), which has an additional drift term \(-^{2}/2\) from Ito's lemma that attracts the process towards the invariant set. The total drift is now determined by a competition between the original drift rate \(\) and the additional attractive force driven by the diffusion. This competition controls whether the invariant set \(A=\{0\}\) of GBM is stochastically attractive (i.e. when \(<^{2}/2\) the invariant set is attractive).

**Stochastic attraction in one-dimension.** Stochastic attractivity is a local property and thus we might expect SGF to act like GBM around an invariant set, enabling us to derive broader conclusions about stochastic attraction. If we consider SGF with a smooth loss and diffusion in one-dimension, where without loss of generality we assume \(A=\{0\}\) is an invariant set, we can Taylor expand the gradient and the diffusion around the set, yielding geometric Brownian motion near the set:

\[d_{t}-^{}(0)_{t}dt+(0)}_{t}dB_{t}. \]

Here we use \(^{}(0)=D(0)=D^{}(0)=0\), which is a consequence of \(\{0\}\) being an invariant set. From Eq. 5 we can formulate a necessary/sufficient condition for stochastic attractivity in one-dimension:

**Theorem 4.1** (Necessary/Sufficient Condition for Stochastic Attraction in One-Dimension).: _Let \(A=\{0\}\) be an invariant set of \(\{_{t}:t 0\}\) obeying Eq. 3. Suppose \(^{}():,D():\) are \(C^{2}\) functions such that \(D^{}(0)>0\). Define rate of attractivity \(=^{}(0)+D^{}(0)\). \(A\) is stochastically attractive if \(>0\), while it is not stochastically attractive if \(<0\). 9_

One of the most surprising implications of Theorem 4.1 is that SGF can potentially converge to a saddle-point or local maxima of a loss landscape, an observation previously made by Ziyin et al. . To see this, recall that \(D() 0\) by definition, and \(D(0)=0\) as \(\{0\}\) is an invariant set. As a result, \(D^{}(0)=0\) and \(D^{}(0) 0\) by the continuity assumption of \(D\). The collapsing condition, therefore crucially depends on the curvature of the loss function \(\) at \(=0\). When \(D^{}(0)\) is strictly positive, the collapsing condition can still be satisfied with negative curvature provided that \(^{}(0)>-D^{}(0)\). Given that \(D\) is proportional to \(/\), the learning rate to batch size ratio determines the maximum attainable steepness for an invariant set to be attractive.

**An illustrative example.** Consider SGF in a double-well potential \(()=(^{2}-)^{2}\) with multiplicative diffusion10, such that the dynamics are \(d_{t}=-(_{0}^{3}-_{t})dt+_{t}dB_{t}\) where \(,>0\). The minima of the potential are located at \(=\), while \(=0\) is a local maximum and forms an invariant set. This example is special as the dynamics have an analytical steady-state distribution \(p_{ss}()\), given any initialization. Thus, determining the stochastic attractivity of \(\{0\}\) can be achieved by examining the steady-state distribution. As in GBM, we assume, without loss of generality, that \(_{0}>0\) and consider the logarithm process \(z_{t}=(_{t})\). In this new coordinate, the noise term is constant, which allows us to determine the steady-state distribution. Transforming back to the original coordinate, we find that \(p_{ss}()\) is given by a Gibbs distribution \(p_{ss}() e^{-()}\) for a modified potential11\(()=^{2}/2-(2/^{2}-2)()\) with constant \(=2/^{2}\) and partition function \(Z=_{-}^{}e^{-()}d\). When \(^{2}/2\), the partition function diverges and \(p_{ss}()\) collapses to a Dirac delta distribution at \(=0\). This transition agrees with the collapsing condition from Theorem 4.1.

**Stochastic attraction in high-dimensions.** To extend the collapsing condition derived in Theorem 4.1 to high-dimensional cases, a natural idea would be to consider all one-dimensional slices of parameter space orthogonal to the invariant set. However, one challenge is that some of these slices might satisfy the collapsing condition while others do not. This can result in complex dynamics near the boundary of the invariant set making it difficult to derive a necessary and sufficient condition for attractivity in high-dimensions. Nonetheless, we can derive a sufficient condition:

**Theorem 4.2** (A Sufficient Condition for Stochastic Attraction in High-Dimensions).: _Let \(A^{d}\) be a \(d_{A}\)-dimensional affine subset, and a stochastic process \(\{_{t}^{d}:t 0\}\) obey Eq. 3 in \(A_{c}\), open \(c\)-neighborhood of \(A\) with some \(c>0\). Suppose \(:A_{c}\) is a \(C^{3}\)-function whose first and second-order derivatives are \(L\)-Lipschitz continuous. \(D:A_{c}^{d d}\) is the diffusion matrix such that the second-order derivatives of its elements are \(L\)-Lipschitz continuous. Furthermore, we assume that all the elements of \(:A_{c}^{d d}\) are \(L\)-Lipschitz continous. Let \(D_{}=P_{}DP_{}\) where \(P_{}:^{d}^{d}\) projects to the normal space of \(A\). If there exists \(>0\) such that \(^{2}_{}^{}D^{}>\) and_

\[^{2}_{}(-D_{}\,+ (1-)^{}D)>0, \]

_for any unit normal vector \(^{d}\) perpendicular to \(A\) and \( A\), then \(A\) is stochastically attractive._

To prove Theorem 4.2 (see App. E), we consider a family of distance measures between \(_{t}\) and the invariant set \(A\). Eq. 6 ensures that one of the distance measures is super-martingale in a neighborhood of the invariant set, allowing us to apply a version of Doob's maximal inequality (Lemma 1 in ).

## 5 Attractivity of Invariant Sets in Deep Neural Networks

In this section we explore theoretically and empirically the stochastic attractivity of invariant sets in deep neural networks.

**Sign invariant sets.** Sign invariant sets correspond to the parameter space of a sparse subnetwork obtained by removing a hidden neuron. Here, we demonstrate the stochastic collapse to a sign invariant set in a minimal model of neural networks--a scalar single neuron model: \(f(x;w_{1},w_{2})=w_{2}(w_{1}x)\) with \(w_{1},w_{2},x\). Suppose this model is trained via label-noise gradient descent and using the \(L^{2}\)-loss \((w_{1},w_{2})=_{i[n]}(y_{i}-f(x_{i};w_{1},w_{2}))^ {2}\) on a non-empty data

Figure 1: **Stochastic collapse in a quartic loss.** The left three plots show sample trajectories with the same, random initializations driven by the SDE \(d_{t}=-(_{t}^{3}-_{t})dt+_{t}dB_{t}\) for different values of \(\). The analytic stationary solution is plotted on the side plot. **Leftmost:** for small \(\) the steady-state distribution is two bumps around the minima \(=\). **Middle left:** with increasing \(\) the distribution spreads out and is biased towards \(=0\). **Middle right:** when \(\) surpasses the collapsing condition \(\) the steady-state distribution collapses to a dirac delta distribution at \(=0\). **Rightmost:** the empirical probability of the sample trajectories at different steps being within \(\) of the origin as a function of increasing gradient noise. As we see empirically, there is a sudden phase transition which aligns with the collapsing condition from Theorem 4.1.

Figure 2: **Stochastic collapse of a single neuron.** The SDE dynamics of a single-neuron model are simulated with various noise levels \(\). **Left:** A scatter map of trained weights \((w_{1},w_{2})\). The colors of the dots represent the noise level, corresponding to the vertical lines in the right panel. For each noise level, we plot 15 trained models with different noise realizations. The background heatmap shows the loss landscape. **Right:** The colored lines represent the empirical probability that \(^{2}+w_{2}^{2}}<=10^{-2}\) (from \(10^{3}\) samples) after a given number of update steps. We observe a sudden transition that aligns with the collapsing condition from Theorem 5.1.

set \(\{(x_{1},y_{1}),,(x_{n},y_{n})\}\) such that \(_{i[n]}x_{i}^{2} 0\). Exploiting Theorem 4.1, we can determine the attractivity of the sign invariant set \(A=\{(0,0)\}\) for this network.

**Theorem 5.1** (Informal).: _Let \(\{(w_{1,t},w_{2,t})^{2}:t 0\}\) be a process obeying label-noise gradient flow with \(L^{2}\)-loss of a single scalar neuron in a neighborhood \(U\) of \(A=\{(0,0)\}\), with learning rate \(>0\) and noise amplitude \(>0\). Suppose the activation function \(:\) is smooth satisfying \((0)=0\), \(^{}(0)>0\). The invariant set \(A\) is stochastically attractive if \((0)^{C^{2}}}{2}>x_{i}y_{i}|}{ _{i[n]}x_{i}^{2}}\)._

The term \(x_{i}y_{i}|}{_{i[n]}x_{i}^{2}}\) can be thought of as the signal of the dataset. Thus, Theorem 5.1 states that stochastic attractivity is determined by balancing the signal and noise - an idea we will see later in Sec. 6 as the origin of generalization benefits from stochastic collapse. We leave the poof in App. F.

**Permutation invariant sets.** Permutation invariant sets correspond to subnetworks with fewer unique hidden neurons. An important question is to what extent are permutation invariant sets attractive - as this would indicate an implicit bias towards a low-rank model. We provide intuitive insights into the attractivity of permutation invariant sets through a toy example of a two-neuron neural network in App. G. More importantly, we empirically test whether permutation invariant sets can be attractive in deep neural networks via experiments using VGG-16  and ResNet-18  trained on CIFAR-10 and CIFAR-100 respectively . Since the ReLU activation has additional symmetry resulting in a potentially larger invariant set , we replace ReLU with GELU activation in all our experiments below. To detect stochastic collapse to a permutation invariant set, we compute the normalized pairwise distance between parameters for neurons from the same layer, defined by \((w_{i}^{(l)},w_{j}^{(l)})^{(l)}-w_{j}^{(l)}\|_{ 2}^{2}}{\|w_{i}^{(l)}\|_{2}^{2}+\|w_{j}^{(l)}\|_{2}^{2}}\). Strikingly, hierarchical clustering based on this distance reveals multiple large clusters of many neurons with near identical incoming and outgoing parameters after training (Fig. 3). Such clustering implies SGD implicitly drives weight matrices towards low-rank structures in many layers via attraction towards permutation invariant sets.

To explore the influence of the learning rate and batch size on the attraction towards sign and permutation invariant sets, we trained VGG-16 and ResNet-18 on CIFAR-10 and CIFAR-100, respectively, while varying these hyperparameters. We defined _vanishing_ neurons as those with incoming and outgoing weights under 10% of the maximum norm for that layer, and identified the number of independent non-vanishing neurons by clustering the concatenated vector of incoming

Figure 3: **Evidence of stochastic collapse towards permutation invariant sets in deep neural networks**. Each pair of plots shows the normalized distance matrix between incoming parameters (\(w_{}\)) and outgoing parameters (\(w_{}\)) of neurons within the same hidden layer. We show two pairs of plots with little stochastic collapsing and four pairs of plots with strong stochastic collapsing: three pairs for three different layers in VGG-16 trained for \(10^{5}\) steps on CIFAR-10 (**top row**), and three pairs for three different layers in a ResNet-18 trained for \(10^{6}\) steps on CIFAR-100 (**bottom row**). For each pair of plots, we sort the neurons by hierarchical clustering according to normalized distances between incoming parameters _only_ (left plot in each pair), and then show the distance matrix between outgoing parameters using the _same_ sorting of neurons (right plot in each pair). The similarity between each plot in a pair indicates that clusters of neurons with similar incoming parameters _also_ have similar outgoing parameters. See App. J for experimental details.

and outgoing parameters based on normalized distance. Two neurons were considered _identical_ if their distance in parameter space was 10% of their norms, a stringent criterion given the high-dimensionality of the weight vectors.

We found that increased learning rates typically intensify the stochastic attraction to invariant sets by reducing the number of independent neurons (Fig. 4). A large reduction in the fraction of independent neurons was observed in VGG-16 between conv7 and conv8, where the number of channels increase from 256 to 512, indicating this excess model capacity is counteracted by stochastic collapse. Also, we note that Proposition 3.2 and 3.1 do not apply to neurons with residual connections. More strict definitions are required, as described in Proposition H.2 and H.1. We believe, this stricter constraint is the source of layer-to-layer oscillations in the fraction of independent neurons for ResNet18 in Fig. 4. Surprisingly, we noticed that unlike learning rate effects, changes in batch size produce complex shifts in the number of independent neurons. See Fig. 10 for a further discussion.

## 6 How Stochastic Collapse Can Improve Generalization

To understand how stochastic collapse can benefit generalization, we will theoretically analyze learning dynamics in a teacher-student model for linear networks, a well established framework for studying generalization [50; 51; 52]. This analysis not only provably connects the phenomenon of stochastic collapse to generalization benefits, but also suggests a stochastic collapse based mechanism for why successful learning rate schedules improve generalization. We then provide direct evidence for this theoretically suggested mechanism in deep neural networks.

**Insights from a two-layer linear neural network.** We build on the analysis of full-batch gradient learning dynamics of training error  and test error  in the limit of infinitesimal learning rates for two-layer linear neural networks in a student-teacher setting. In our new analysis here we incorporate the important new ingredient of stochastic gradients, which dramatically alters the learning dynamics. The details of our analysis are presented fully in App. I.

A teacher neural network with a low-rank weight matrix \(\) generates a training dataset by providing noise corrupted outputs to a set of random Gaussian inputs. The input-output data covariance matrix \(_{yx}\) then drives the learning dynamics of a two-layer student with composite weight matrix \(=W_{2}W_{1}\). We analyze the learning dynamics of the student under a set of four assumptions precisely stated in App. I. These assumptions are similar to the ones made in [51; 53], and are roughly stated as: (A1) whitened inputs to the teacher; (A2) structured gradient noise related to input-output covariance; (A3) spectral initialization with student singular vectors aligned to input-output covariance singular vectors; (A4) balanced initialization with equal power in the two weight layers of the student. Under these assumptions, the learning dynamics of the student weight matrix \(=W_{2}W_{1}\) can be decomposed into independent learning dynamics for each singular value \(_{i}\) of \(\). An associated singular value \(_{i}\) of \(_{yx}\) drives the dynamics of \(_{i}\) via the SDE,

\[d_{i}=2_{i}((_{i}+^{2}/2)- {s}_{i})dt+2}_{i}dB_{t}, \]

where \(>0\) is the amplitude of the gradient noise. In the small noise limit of \( 0\), this SDE aligns with the nonlinear ODE corresponding to the student network's dynamics under gradient flow, where exact solutions exist [51; 53]. Important insights gained from this previous analysis are that larger data singular modes are learned earlier  and larger data singular modes, when learned, help the student generalize, while smaller singular modes, when learned, impede generalization by

Figure 4: **Larger learning rates intensify stochastic collapse. This figure illustrates how the fraction of independent neurons per layer in VGG-16 trained on CIFAR-10 (left column) and ResNet-18 trained on CIFAR-100 (right column) varies with changes in learning rates. The networks are evaluated at training steps of \(10^{5}\). A reduced percentage of independent neurons indicates stronger stochastic collapse. See App. J for further details.**

overfitting to noise in the training data . Both these insights will apply in our analysis of SGF. However, we will also uncover a third behavior unique to SGF: stochastic collapse effectively governs the minimum learnable singular mode, serving as a natural mechanism to prevent overfitting. Our analysis in App. I reveals that the student singular values \(_{i}\) evolve according to the process:

**Theorem 6.1** (Stochastic Student Dynamics).: _Under assumptions A1 - A4, the dynamics of \(_{i}(t)\) given by Eq. 7 are governed by the stochastic process,_

\[_{i}(t)=e^{(2_{i}-^{2})t+2}B_{t}} (2_{0}^{t}e^{(2_{i}-^{2})+2}B _{}}d+_{i}(0)^{-1})^{-1}. \]

In terms of the per layer balanced student weight \(w_{i}=_{i}}\), the above process can be thought of as originating from the quartic loss landscape \(_{i}(w_{i})=(w_{i}^{2}-_{i})^{2}\), equivalent to the example presented in Sec. 4. Thus, there exists an invariant set for each student singular value corresponding to the affine space \(_{i}=w_{i}=0\) where both drift and diffusion terms vanish. Stochastic collapse to this invariant set would thus induce a low-rank regularization on the student in which some number of singular modes in the data are _never learned_ despite having nonzero data singular values \(_{i}\). To confirm stochastic collapse to this invariant set, we derive:

**Corollary 6.1**.: _In the limit \(_{i}(0) 0\), for any \(t>0\), then \(_{i}(t)/_{i}(0)e^{(2_{i}- ^{2})t+2}B_{t}}\)._

Corollary 6.1 has important implications. First, if \(_{i}<}{2}\), the distribution will exhibit stochastic collapse, converging to a delta distribution at the origin, consistent with Theorem 4.1 (Fig. 5: middle right panel). This in turn implies that early training with large learning rates promotes stochastic collapse of more student singular modes. Indeed, even after the training loss plateaus, further extended training with a large learning rate \(\) will continue to drive stochastic collapse of small singular modes. After a subsequent learning rate drop, some of these modes will cease to obey the collapsing condition and as a consequence, the associated invariant set will repulse rather than attract the dynamics. Nonetheless, the repulsive escape will take longer the closer it starts from the invariant set (Fig. 5: rightmost panel). This results in dynamics in which generalization error is reduced by the student not learning many data singular modes with large \(\), because \(_{i}<}{2}\), and then learning only the subset with large \(\) that have time to escape the vicinity of the invariant set after a learning rate drop. The overall effect is that the student selectively learns modes with higher signal-to-noise ratios, resulting in improved generalization after the learning rate drop as shown in Fig. 5: middle left panel. In summary, the linear teacher-student model highlights that with an appropriate learning rate schedule, a noisy student can effectively avoid overfitting through stochastic collapse and achieve superior generalization compared to a noiseless student.

**Stochastic collapse explains why early large learning rates helps generalization.** The analyses in the linear teacher student network provide valuable insights into how stochastic collapsing can enhance generalization. Importantly, it sheds light on the mystery of why training at large learning rates for a long period of time (even after training and test loss have plateaued) helps generalization after a subsequent learning rate drop. The key prediction is that a large learning rate induces stronger

Figure 5: **Demonstrating generalization benefits of stochastic collapse in a teacher-student setting.** We show the train loss (**leftmost**) and test loss (**middle left**) during the training of the student with different gradient noises. Dashed lines in the leftmost and middle left panels indicate the step where learning rate is dropped. Training with larger gradient noises \(\) generalizes better. **Middle right**: we show the noisy teacher signals against the learned student signals before dropping the learning rate. Larger noises have a stronger implicit bias towards zero. **Rightmost**: same as the middle right panel but we show the learned signals (for the brightest green in the middle right panel) at different training steps after the learning rate drop. All the results are averaged over 256 replicates.

stochastic collapse, thereby regularizing the model complexity. Furthermore, remaining in a phase of larger learning rates for a prolonged period drives SGD closer to the invariant set. Consequently, when the learning rate is eventually dropped, overfitting in these specific directions is mitigated.

To test this predicted mechanism, we trained VGG-16 and ResNet-16 on CIFAR-10 and CIFAR-100, respectively. The training loss and test accuracy had already plateaued at \(10^{4}\) steps (Fig. 6: left column). We dropped the learning rate after different lengths of the initial high learning rate training phase and confirmed that training with large learning rates for longer periods helps subsequent generalization (Fig. 6: middle column). We then tested our prediction that stochastic collapse is occurring during the early high learning rate training, thereby enhancing an implicit bias towards simpler subnetworks. In particular, we computed the fraction of independent neurons at each step where we drop the learning rate. We found, as predicted, that models with a longer initial training phase collapse towards invariant sets with fewer independent neurons (Fig. 6: right column).

## 7 Conclusion

In this study, we demonstrated how stochasticity in SGD biases overly expressive neural networks towards _invariant sets_ that correspond to simpler subnetworks, resulting in improved generalization. We established a sufficient condition for _stochastic attractivity_ in terms of the Hessian of the loss landscape and the noise introduced by stochastic gradients. We combined theory and empirics to study the invariant sets corresponding to vanishing neurons with origin-passing activation functions and identical neurons generated by permutation symmetry. Furthermore, we elucidated how this process of _stochastic collapse_ can be beneficial for generalization using a linear teacher-student framework and explain the importance of large learning rates early in the training process.

**Limitations and future directions.** Activation function with additional continuous symmetries, such as ReLU, might have a larger class of invariant sets than those studied in our work, which remains to be explored. As all the invariant sets in our study are affine, exploring the possibility of curved invariant sets and how curvature affects the analysis is an interesting direction for future work. Furthermore, the interplay between symmetries in data and invariant sets warrants investigation. Identifying the necessary conditions for stochastic attractivity of general affine invariant sets is an important goal for future work. Extending our analytic results from the continuous SGF to the discrete SGD updates is an interesting theoretical direction. Lastly, designing new optimization algorithms based on our insights into stochastic collapse is a major goal for our future work.

Figure 6: **Large learning rates aid generalization via stochastic collapse to simpler subnetworks.****Left**: The training loss (grey) and test accuracy (blue) during an initial training phase using high learning rate (lr=0.1). Vertical dashed lines highlight the training steps at which we will drop the learning rate. **Middle**: Test accuracy preceding and following a learning rate drop to lr=0.01. Colors indicate when the learning rate drop shown in the left plot occured. Despite plateauing of the training loss and test accuracy, a later learning rate drop yields higher final test accuracy. **Right**: The fraction of independent neurons per layer evaluated at different learning rate drop times (indicated by the color) during the initial high learning rate training phase. Prolonged training with a large learning rate induces implicit regularization by reducing the number of independent neurons. All curves represent the average across eight replicates. See App. J for experimental details.

## Author Contributions

D.K. and F.C. initiated the project. F.C. formulated an initial analysis on stochastic collapse and is primarily responsible for the experiments associated with deep neural networks. D.K. is primarily responsible for the linear teacher-student analysis and the original observation that SGD can collapse to a saddle-point introduced by symmetry. A.Y. primarily contributed to the theoretical formulations and proofs, which include the concept of invariant sets and conditions for stochastic attractivity. S.G. advised throughout the work and provided funding for computation. All of the authors have worked on writing the manuscript.