# TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models

Andrei Margeloiu\({}^{1}\), Xiangjian Jiang\({}^{1}\), Nikola Simidjievski\({}^{2,1}\), Mateja Jamnik\({}^{1}\)

\({}^{1}\)Department of Computer Science and Technology, University of Cambridge, UK

\({}^{2}\)PBCI, Department of Oncology, University of Cambridge, UK

{am2770, xj265, ns779, mj201}@cam.ac.uk

Equal contribution.

###### Abstract

Data collection is often difficult in critical fields such as medicine, physics, and chemistry, yielding typically only small tabular datasets. However, classification methods tend to struggle with these small datasets, leading to poor predictive performance. Increasing the training set with additional synthetic data, similar to data augmentation in images, is commonly believed to improve downstream tabular classification performance. However, current tabular generative methods that learn either the joint distribution \(p(,y)\) or the class-conditional distribution \(p( y)\) often overfit on small datasets, resulting in poor-quality synthetic data, usually worsening classification performance compared to using real data alone. To solve these challenges, we introduce TabEBM, a novel class-conditional generative method using Energy-Based Models (EBMs). Unlike existing tabular methods that use a shared model to approximate all class-conditional densities, our key innovation is to create distinct EBM generative models for each class, each modelling its class-specific data distribution individually. This approach creates robust energy landscapes, even in ambiguous class distributions. Our experiments show that TabEBM generates synthetic data with higher quality and better statistical fidelity than existing methods. When used for data augmentation, our synthetic data consistently leads to improved classification performance across diverse datasets of various sizes, especially small ones. Code is available at https://github.com/andreimargeloiu/TabEBM.

## 1 Introduction

Many scientific domains within medicine, physics, and chemistry often rely on intricate and challenging data acquisition procedures [5; 50; 4; 32; 76; 12] that typically render small-size tabular datasets [5; 46]. Using these to train machine learning models that can aid in tasks such as disease diagnosis [52; 37], material property prediction , and chemical compound classification , can lead to poor performance [74; 52; 37]. In the case of learning tasks which leverage image and text data, a standard remedy to address performance issues due to data scarcity is employing data augmentation techniques [72; 73; 60; 71] that generate additional synthetic samples from existing data.

Figure 1: **Evaluation of TabEBM and other state-of-the-art tabular generative methods across six key metrics** (larger area indicates better performance). The results demonstrate that TabEBM excels in data augmentation (utility), with a larger area than all other methods.

However, applying data augmentation to tabular data introduces additional challenges, as tabular datasets are often very diverse and lack explicit symmetries , such as rotations or translations seen in images. Consequently, existing tabular data augmentation methods often yield mixed results and can even degrade model performance [51; 71; 48], hindering their widespread adoption.

Tabular augmentation typically involves training generative models to approximate either the joint distribution \(p(,y)\)[85; 24] or the class-conditional distribution \(p(|y)\)[85; 42; 83; 47; 48]. A key challenge of joint distribution methods is maintaining the original training label distribution, as sampling from such generators can produce label distributions that deviate from the original and even fail to generate data for specific classes (see Appendix C for an example). These issues compromise the effectiveness of data augmentation  by undermining the label accuracy and distribution. On the other hand, while class-conditional models that learn \(p(|y)\) preserve the stratification of the original data, they often employ a _shared_ model to represent all class-conditional densities. This, however, can lead to overfitting, particularly in imbalanced datasets where the model may prioritise more frequent classes , ignoring unique features needed for generating label-invariant samples. Additionally, in datasets with limited data, this can lead to mode collapse [68; 70], where the model does not effectively capture the diversity of each class , and thus tends to perform poorly in a multi-class setting.

To address the challenges of class-conditional tabular generation, we introduce TabEBM (Figure 2), a new method for tabular data augmentation utilising Energy-Based Models (EBMs). Our method introduces two innovations: (i) _Distinct class-specific models:_ TabEBM constructs a collection of individual models - one for each class - which, by design, enables learning distinct marginal distributions for the inputs associated with each class. This, in turn, enables performing data augmentation while maintaining the original label distribution. (ii) _Generative models:_ we build novel class-specific generators that produce high-quality synthetic data even from extremely few samples. Specifically, we create a surrogate binary classification task for each class and fit it with a pre-trained tabular in-context classifier. We then convert the binary classifier into an EBM, a generative model, without additional training. Using class-specific EBMs makes the energy landscape more robust to class overlaps, compared to using a single shared EBM to approximate the class-conditional distribution.

Our contributions can be summarised as:

* **Technical:** We propose TabEBM, which is the first generative method to create class-specific EBMs, learning the marginal distribution for each class separately.
* **Empirical:** We present the first comprehensive analysis of tabular data augmentation across different dataset sizes and use cases beyond predictive performance. Our analysis compares TabEBM with eight leading tabular generative models across various datasets, demonstrating that TabEBM consistently improves data augmentation performance on small datasets, while our generated data demonstrates better statistical fidelity and privacy-preserving properties (Figure 1).
* **Library:** We release TabEBM as an open-source library, available at https://github.com/andreimargeloiu/TabEBM. Our library enables off-the-shelf data generation and data augmentation on any tabular dataset without requiring training. Further details are available in Appendix B.5.

Figure 2: **An overview of TabEBM. We learn _distinct_ class-specific Energy-Based Models (EBMs) \(E_{}()\) and \(E_{}()\) exclusively on the points of their respective class. Each EBM approximates a class-conditional distribution \(p(|y)\). TabEBM allows synthetic data generation by sampling from the estimated distributions for each class \(p(|y=)\) and \(p(|y=)\).**

TabEBM

**Notation.** We address classification problems with \(C\) classes, denoted by \(=\{1,2,,C\}\). Let \(\{(^{(i)},y_{i})\}_{i=1}^{N}\) represent a dataset of \(N\) samples, each being a \(D\)-dimensional vector \(^{(i)}^{D}\), with a corresponding label \(y_{i}\). For each class \(c\), we define \(_{c}=\{^{(i)} y_{i}=c\}\) as the subset of samples labelled with class \(c\). Let \(f_{}()\) denote a classifier. The expression \(f_{}()[y]\) represents the (unnormalised) logit assigned to the class \(y\) for the input \(\).

### Preliminaries on Energy-Based Models

An Energy-Based Model (EBM)  defines a probability density function \(p_{}()\) through an energy function \(E()\). Specifically, the model posits that \(p() e^{-E()}\), where \(E()\) represents the unnormalised negative log-density of the input \(\). In this framework, lower energy values correspond to higher probability densities. This relationship allows EBMs to model distributions by learning to assign lower energy to more probable configurations of \(\) and higher energy to less probable ones.

An important observation is that energy-based models can utilise the same model architectures as standard classification models . Typically, the logits \(f_{}()[y]\) from a classification model define a discriminative distribution through the softmax function, expressed as \(p_{}(y|)=(f_{}()[y])\). Intriguingly, these same logits can be reinterpreted to define an energy-based model for the joint distribution \(p(,y)\). This is achieved by setting the energy function to \(E(,y)=-f_{}()\). Furthermore, the energy function for the marginal distribution \(p()\) is obtained by marginalising over \(p(,y)\), resulting in \(E()=-_{y^{}}f_{}()[y^{ }]\).

Such an energy-based model, trained with EBM-specific protocols on multiple classes, is typically used as a classifier, as demonstrated on several computer vision tasks in . In contrast, in this work our focus is the opposite: we propose employing trained classifiers, one for each specific class, as a generative energy-based model for the class-conditional distributions \(p(|y)\). We apply our TabEBM method for generative tasks on tabular data.

### Distinct Class-Specific Energy-Based Models

TabEBM is a class-conditional generative model \(p(|y)\) implemented using a set of EBMs, \(\{E_{1}(),E_{2}(),,E_{C}()\}\). Our approach assumes that the class-conditional density \(p(|y=c)\) is best modelled using its class-specific data \(_{c}\). Thus, for each class \(c\), we construct a class-specific EBM, \(E_{c}()\), using only the data from that class, \(_{c}\), such that \(p(|y=c)(-E_{c}())\).

We derive each class-specific EBM \(E_{c}()\) by training a classifier on a novel task and reinterpreting its logits. Specifically, for each class \(c\), we propose a _surrogate binary classification task_ to determine if a sample belongs to class \(c\) by comparing \(_{c}\) against a set of surrogate negative samples \(_{c}^{}\), which we show in Figure 3. Specifically, we generate the negative samples at the corners of a hypercube in \(R^{D}\). For each dimension \(d\), the coordinates of a negative sample are either \(_{}^{}{}_{d}\) or \(-_{}^{}{}_{d}\), where \(_{}^{}\) is a fixed constant and \(_{d}\) is the standard deviation of dimension \(d\). For example, in \(R^{3}\), a negative sample might have coordinates \([_{}^{}_{1},_{}^{}_{2},-_{}^{}_{3}]\). Placing the negative samples at the corners of a hypercube ensures they are easily distinguishable from the real data, which is crucial for an accurate energy function (see Appendix D.1.1). This placement is also robust to variations in the number and distance of the negative samples (see Appendices D.1.2 and D.1.3).

We create the combined dataset \(_{c}\) for the surrogate binary classification task by labelling \(_{c}\) as \(1\) and \(_{c}^{}\) as \(0\):

\[_{c}=(_{c}_{c}^{},\{1\}^{| _{c}|}\{0\}^{|_{c}^{}|})\] (1)

We then train a binary classifier \(f_{}^{c}()\) on \(_{c}\) and use it to construct the class-specific energy \(E_{c}()\) for class \(c\). To do this, we reinterpret the logits \(\{f_{}^{c}(),f_{}^{c}()\}\) of the trained binary classifier as components of an approximated joint distribution for the surrogate binary task:

Figure 3: The class-specific energy function \(E_{c}()\) from the surrogate binary task, where the blue region represents low energy (i.e., high data density). Placing the negative samples in a hypercube distant from the data results in an accurate energy function.

\[p_{c}(,0)=^{c}())}{Z}, p_{c}( ,1)=^{c}())}{Z}\] (2)

Next, we derive the approximated distribution \(p_{c}()\) by marginalisation:

\[p_{c}() =p_{c}(,0)+p_{c}(,1)\] \[=^{c}())+(f_{}^{c}( ))}{Z}\] \[=^{c}())+ (f_{}^{c}())))}{Z}\] \[ E_{c}() =-((f_{}^{c}())+(f_{}^{ c}()))\] (TabEBM class-specific energy)

For the binary classifier \(f_{}^{c}()\) in the surrogate binary classification, we use TabPFN , a pre-trained tabular in-context model. Note that TabPFN is intended for inference only, with no updates to its parameters (see Section 4 for more details about TabPFN). In this context, "training" the TabPFN classifier is analogous to the K-Nearest Neighbour algorithm, which simply performs inference based on a training dataset provided to the model. We apply TabPFN multiple times on separate datasets \(\{_{1},_{2},,_{C}\}\) to obtain multiple classifiers \(\{f_{}^{1},f_{}^{2},,f_{}^{C}\}\). In Section 3.4, we explore why reinterpreting TabPFN's logits, trained on our surrogate binary tasks, can be useful for estimating an energy function. We emphasise that TabEBM is a general method, capable of using any gradient-based classifier that computes logits (using Equation (3)), and is not limited to TabPFN.

**Generating data with TabEBM** involves two steps. First, we sample a class \(c\) from the empirical distribution \(c p(y)\). Then, we sample a data point \(\) from the conditional distribution \( p(|y=c)\) approximated by the class-specific energy-based model \(E_{c}()\), as outlined in Algorithm 1. We employ Stochastic Gradient Langevin Dynamics (SGLD)  to perform this sampling. SGLD is an efficient method for high-dimensional data, combining stochastic gradient descent (SGLD) with Langevin dynamics. The update rule for SGLD at each iteration is:

\[_{t+1}=_{t}- E(_{t})+ _{t},_{t}(0,)\] (4)

where a Gaussian noise term \(_{t}\) introduces randomness into the sampling process, enhancing the exploration of the distribution. In practice, the step size and the noise standard deviations are often chosen separately, resulting in a biased sampler that allows for faster training. Appendix D.2 further shows that TabEBM is stable to hyperparameters for the sampling process.

In our method, SGLD performs iterative augmentation. We start by sampling close to real data and iteratively adjust these synthetic data points, steering them towards regions of higher probability under the learned energy model. TabEBM enables sampling from any specified class distribution, including the original class distribution, which is crucial for data augmentation.

```
0: Training data \(_{c}\) for class \(c\), step size \(_{}\), noise scale \(_{}\), initial perturbation \(_{}\), number of steps \(T\)
0: Set of synthetic samples for class \(c\)  Initialise a surrogate binary classification task and train the model
1: Assign new labels to the samples \(_{c}\) from class \(c\), setting them to class 1
2: Generate a set of surrogate negative samples \(_{c}^{}\) and assign them class 0 labels
3: Train a binary classifier \(f_{}^{c}\) on the dataset \(_{c}=(_{c}_{c}^{},\{1\}^{| _{c}|}\{0\}^{|_{c}^{}|})\)
4: _Synthesize samples using Stochastic Gradient Langevin Dynamics (SGLD)_
5: Initialise synthetic data points \(_{0}^{}\) by sampling from \((_{c},_{}^{2})\)
6:for each iteration \(t=0,1,,T-1\)do
7:\(E_{c}(_{t}^{})=-((f_{}^{c}(_{t}^{}))+(f_{}^{c}(_{t}^{})[1 ]))\)
8:\(_{t+1}^{}=_{t}^{}-_{} E_{c}(_{t}^{})+(0,_{}^{2})\)
9:endfor
10:return\(_{T}^{}\) as the generated synthetic data for class \(c\) ```

**Algorithm 1** TabEBM sampling from Class-Specific EBM \(E_{c}()\)Experiments

We evaluate TabEBM by focusing on four research questions:

* **Data Augmentation Improvement (Q1, Section 3.1):** Can TabEBM generate synthetic data that improves the accuracy of downstream predictors via data augmentation?
* **Statistical Fidelity (Q2, Section 3.2):** Can TabEBM generate synthetic data with high statistical fidelity (i.e., with similar distributions to those of real data)?
* **Privacy Preservation (Q3, Section 3.3):** Can TabEBM generate synthetic data that finds a competitive trade-off between downstream performance and privacy preservation?
* **Understanding TabEBM's energy formulation (Q4, Section 3.4):** Why is TabEBM's class-specific energy effective, and how do the proposed surrogate tasks influence this?

**Datasets.** We utilise eight open-source tabular datasets from OpenML  across five domains: Medicine, Chemistry, Engineering, Language and Economics. As TabPFN utilises many small-size OpenML datasets in its meta-validation , it can lead to data leakage when evaluating TabEBM. Therefore, to provide fair comparisons, we select six additional leakage-free datasets from UCI . These diverse datasets contain 7 to 77 features and 698 to 5500 samples across 2 to 26 classes. Five datasets contain both numerical and categorical features, while the remaining are numerical only. We further enlarge the evaluation scope by varying the degrees of data availability (i.e., \(N_{}\)), leading up to 33 different test cases for the eight OpenML datasets. Appendix B.1 provides detailed descriptions.

**Benchmark generators.** We compare TabEBM against eight existing tabular data generation methods of eight different categories: (i) a standard interpolation method SMOTE ; (ii) a Variational Autoencoders (VAE) based method TVAE ; (iii) a Generative Adversarial Networks (GAN) method CTGAN ; (iv) a normalising flow model Neural Spine Flows (NFLOW) ; (v) a diffusion model TabDDPM ; (vi) a tree-based method Adversarial Random Forests (ARF) ; (vii) a Graph Neural Network (GNN) based method GOGGLE ; and (viii) a Prior-Data Fitted Networks (PFN) based method TabPFGen . Furthermore, we also include a "Baseline" model, where no data augmentation is applied (i.e., only real data is used to train downstream predictors). In Appendix B.6, we detail the settings used for TabEBM and all other generators.

**Downstream predictors.** We select six representative downstream predictors, including three standard baselines: Logistic Regression (LR) , KNN  and MLP ; two tree-based methods: Random Forest (RF)  and XGBoost ; and a PFN method: TabPFN .

**General experimental setup.** For each dataset of \(N\) samples, we first split it into stratified train and test sets. We create large test sets to reduce the likelihood that the model's performance is accidentally inflated due to a small, unrepresentative set of samples , and thus the test size is computed via \(N_{}=(,500)\). The full train set approximates the upper bound of the quality of synthetic data, and we call this set "oracle". We subsample the full train set to simulate different levels of data availability, thus the subset size \(N_{}\) varies over \(\{20,50,100,200,500\}\). We split each subset into stratified training and validation sets with a ratio of 4:1. We provide detailed descriptions of data splitting in Appendix B.2 and preprocessing in Appendix B.3. We repeat the splitting ten times, summing up to 10 runs per subset size. The reported results are averaged by default over ten runs on the test sets. When aggregating results across datasets, we use the average distance to the minimum (ADTM) metric via affine renormalisation between the top-performing and worse-performing models . We provide the evaluation results averaged over six downstream predictors for a general conclusion, and the fine-grained numerical results for each predictor are in Appendix D.

**Data augmentation setup.** Given \(N_{}\) real samples, we first train generators on the real training data and then generate \(N_{}\) synthetic samples. For training the downstream predictors, we expand the real training split by adding the synthetic samples. The real validation data is used for early stopping, and the real test set is used for evaluating the predictor's performance. The optimal \(N_{}\) remains an open problem for tabular data . Prior works  mainly use synthetic sets with equivalent sizes to the real sets (i.e., \(N_{}=N_{}\)). However, we observe that \(N_{}=N_{}\) can lead to highly unstable results, especially on small datasets that we investigate. Recent work has used different \(N_{}\) for various generators, such as by applying post-processing . In this work, we want to provide a head-to-head comparison of the effect of data augmentation across subsampled datasets of varying sizes \(N_{}\{20,50,100,200,500\}\). Therefore, we perform data augmentation with a large synthetic set (\(N_{}=500\)) across all splits, and the synthetic data has the same class distribution as the real training data. We provide an illustrative figure of the data splitting setup in Appendix B.2.

[MISSING_PAGE_FAIL:6]

Baseline with notable improvements, particularly in datasets with more than ten classes. In contrast, an increased number of classes tends to cause a performance degradation in the benchmark generators.

**TabEBM is robust on imbalanced datasets.** For the three binary OpenML datasets (i.e., "biodeg", "steel" and "stock"), we adjust the class distribution in the training data to vary the class imbalance, while keeping the test data fixed. Figure 5 shows that TabEBM consistently outperforms Baseline, while the other generators exhibit performance degradation as data imbalance increases.

**TabEBM is computationally efficient.** Figure 6 shows the trade-off between accuracy and the time needed for generating stratified synthetic data (for data augmentation). We measure the total duration of (i) training the model and (ii) generating 500 synthetic samples. The results show that TabEBM is practical, as it achieves higher downstream accuracy with lower time costs.

### Statistical Fidelity (Q2)

We evaluate the fidelity of synthetic data by measuring the similarity of synthetic data to real _train_ data and to real _test_ data (Figure 6). We evaluate this similarity via (i) _average inverse of the Kullback-Leibler Divergence_ (inverse KL) , (ii) p-value of _Kolmogorov-Smirnov test_ (KS test)  and (iii) p-value of _Chi-squared test_ (\(^{2}\) test) . For full numerical results, including \(^{2}\) test, see Appendix D.6. For all three metrics, a bigger value denotes that synthetic data is more likely to have the same distribution as real data.

In Figure 6 (a1&a2), TabEBM consistently exhibits the highest accuracy and distribution similarity between real train data and synthetic data, indicating that TabEBM learns the distributions of real train data better than benchmark generators. In Appendix D.6, we further show that TabEBM remains the most competitive method in similarity between real test data and synthetic data. This indicates that TabEBM can extrapolate beyond real train data and thus generate synthetic data from a more

Figure 4: Mean normalised balanced accuracy improvement (%) across different sample sizes (**Left**) and across datasets with varying numbers of classes (**Right**). Because TabPFGen is not applicable for datasets with more than ten classes, we plot short bars at zeros for visual clearance. Positive values indicate that the generator improves downstream classification performance. TabEBM generally outperforms benchmark generators across varying sample sizes and number of classes.

general distribution that aligns with both train and test data. This extrapolation ability also explains why TabEBM can outperform Baseline via data augmentation (Section 3.1).

### Privacy Preservation (Q3)

More broadly, data privacy is a critical concern for organisations and governments handling sensitive data . Privacy-preserving synthetic data allows researchers and practitioners to bypass ethical and logistical issues while enabling model training and testing . We further explore the use of TabEBM-generated data for data sharing, where only synthetic data is accessible for downstream users . In this case, downstream models are trained exclusively on synthetic data.

Specifically, we evaluate synthetic data via three metrics: (i) _balanced accuracy_ of downstream predictors trained with only synthetic data (i.e., train-on-synthetic, test-on-real ); (ii) median Distance to Closest Record (DCR) , where a greater DCR denotes synthetic data is less likely to be copied from real data; and (iii) \(\)-presense , where a smaller value denotes a lower re-identification risk for real data from synthetic data. Full numerical results are in Appendix D.7.

Figure 7 (b1&b2) shows that TabEBM consistently finds a better trade-off between accuracy and privacy preservation. Notably, the "train-on-synthetic, test-on-real" scenario poses a greater challenge for generators in achieving high accuracy because real data is inaccessible for model training and data augmentation. Despite this difficulty, TabEBM is the only generator that surpasses the overall performance of training on real data (i.e., Baseline). The relatively high DCR for TabEBM indicates that it can extrapolate beyond real train data, aligning with the finding that TabEBM's synthetic data is statistically similar to real test data (Section 3.2). These results further suggest that TabEBM learns the general distribution of real data, and can generate high-quality synthetic data suitable for various purposes, including privacy preservation.

### Why is TabEBM effective for estimating Energy-Based Models? (Q4)

Having established that TabEBM excels in data augmentation, we explore why classifier logits can be useful when reinterpreted as a class-conditional energy function. Figure 8 shows the logit distribution of TabPFN trained on surrogate binary tasks and the corresponding energy function of TabEBM (with TabPFN as the binary classifier) as the Euclidean distance from the real data increases.

We found it essential to place the negative samples far from the real data, since TabPFN, which is pre-trained to approximate Bayesian inference , has its confi

Figure 8: (**Left**) Logit distribution of TabPFN trained on our surrogate binary tasks at increasing distances from the real data (on “steel”). (**Right**) The corresponding unnormalised density approximated by TabEBM. TabEBM assigns higher density closer to the real data.

Figure 7: (**a1&a2):** Median inverse KL and KS test vs. mean normalised balanced accuracy improvement (%) between real train data and synthetic data. (**b1&b2):** Median DCR and \(\)-presence vs. mean normalised balanced accuracy change (%) between real train data and synthetic data. Note that “accuracy improvement” is for data augmentation, and “accuracy change” is for data sharing. Complete results with standard deviations are in Appendix D.4. TabEBM generates high-fidelity synthetic data that can also be used for privacy preservation.

dence influenced by the distance from the training data . Figure 8 (left) shows that TabPFN outputs high logit values near the real data. As the distance from the real data increases, the logit \(f()\) decreases smoothly until the two logits become similar, making the classifier uncertain (because the class probabilities become equal). Figure 8 (right) shows that TabEBM's inferred density drops significantly as the maximum logit decreases, because \(p_{c}()((f())+(f()))\) from Equation (3). Since SGLD sampling performs gradient ascent on the density, the TabEBM-generated samples will be close to the real data. These findings are consistent across datasets (see Appendix D.3), where TabPFN's logits remain positive, with similar ranges and a relatively constant sum as distance increases, warranting further investigation. Overall, TabPFN's distance-based uncertainty is useful for inferring accurate energy functions within our TabEBM framework. Since TabEBM can be paired with any other gradient-based classifier that produces logits, we leave these extensions for future work.

## 4 Discussion & Related Work

Section 3 showed that TabEBM efficiently generates high-fidelity data that can effectively improve the downstream performance via data augmentation. In Table 2, we further provide a summary of tabular data generative models analysed from three important perspectives: (i) _Training:_ the type of distribution that the generators learn (crucial for preserving the original training label distribution), and the training costs associated with learning; (ii) _Generation_: do the generators employ class-specific models (reflecting their capability to capture unique features essential for label-invariant generation), and do models support stratified generation (crucial for effective data augmentation); (iii) _Practicability:_ the scalability of the generators with respect to the number of classes (a common requirement in real-world multi-class tasks), and consistent downstream performance improvement across different class sizes.

**Generative Models for Tabular Data.** The common paradigm for tabular data generation is to adapt Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) [85; 63]. For instance, TableGAN employs a convolutional neural network to optimise the label quality , and TVAE is introduced in  as a variant of VAE for tabular data. However, these methods learn the joint distribution and thus cannot preserve the stratification of the original data (Appendix C). CTGAN  refines the generation to be class-conditional. The recent ARF  is an adversarial variant of random forest for density estimation, and GOGGLE  enhances VAE by learning relational structure with a Graph Neural Network (GNN). Some recent work focuses on generation with denoising diffusion models [42; 87; 44; 40]. For instance, TabDDPM  demonstrates that diffusion models can approximate typical distributions of tabular data. Although these class-conditional models can preserve the label distribution, they struggle to outperform Baseline and standard SMOTE in data augmentation [71; 48].

We attribute the performance degradation in current class-conditional models to their reliance on a single shared model to approximate all class-conditional densities. For instance, another promising generative approach uses pre-trained models like Prior-Data Fitted Networks (PFNs), and the recent TabPFGen  adapts such models into one shared class-conditional generator. However, TabPFGen's shared generator can lead to inaccurate density estimates, particularly in high-noise and class-imbalance situations (see examples in Appendix C). As noise increases, TabPFGen's inferred densities fluctuate significantly and diverge from the true data distributions. In contrast, TabEBM uses class-specific EBMs to model each class's marginal distributions, and the results in Appendix C reveal that our design choice reduces the impact of noise and data imbalance. TabEBM focuses on approximating and generating for one class at a time, remaining unaffected by noise from other classes. Overall, our results demonstrate that TabEBM consistently improves performance across different datasets and sample sizes, outperforming TabPFGen. Moreover, TabPFGen is limited in usability (e.g., it supports only up to ten classes), while TabEBM scales to any number of classes.

In a broader context, some recent work attempts to adapt Large Language Models (LLMs) for tabular data generation [25; 71; 9]. However, data contamination is an inherent issue with such LLM-based models [19; 36; 18; 49]. As the pre-training data is not typically open-source, these models can have unfair advantages in downstream tasks (i.e., the full real dataset, including the real test data, may have been used for pre-training). Therefore, in this paper, we focus on models without support from LLMs, thus avoiding potential biases from data contamination.

**Data Augmentation (DA) for Tabular Data.** DA is an omnipresent technique in computer vision and natural language processing [82; 73; 72; 60; 26; 2]. However, DA for tabular data remains underexplored, and existing methods often perform poorly in real-world tasks, sometimes even reduce performance . Recent studies show that using the same transformations across all classes leads to varied performance impacts [3; 41], indicating that data augmentation effects are class-specific and suggesting that different classes may require distinct augmentations. Given the lack of symmetries in tabular data, we believe this class-dependent effect is even more pronounced. Therefore, we propose TabEBM as a class-specific generative model to produce tailored augmentations for each class.

**Prior-fitted Networks (PFNs) for Tabular Data.** Recent work proposes to approximate the posterior predictive distribution with transformers [59; 33; 61; 79; 20]. PFNs can be adapted for various purposes by pre-training the transformer with corresponding "prior data", and then it can make in-context predictions with unseen downstream data. For instance, TabPFN is a variant that is pre-trained on a prior designed for tabular data . We note that prior data is different to synthetic data in this paper. Specifically, prior data refers to manually crafted fake data (e.g., \(y=2x\)) with no real-world semantics. In contrast, synthetic data from generators is expected to have the same semantics as real data. Inspired by TabPFN's success in small-size classification tasks, TabEBM converts TabPFN into multiple EBMs that learn the marginal distribution for each class. The training-free nature of TabPFN enables TabEBM to generate high-quality tabular data without introducing extra training costs. Additionally, our class-specific design lets TabEBM surpass TabPFN's limits and scale to more than ten classes.

**Limitations and Future Work.** TabEBM is a general method that relies on an underlying binary classifier, and as such, its strengths and weaknesses are directly tied to this classifier. We used TabPFN because it is a well-established open-source pre-trained model for tabular data. Therefore, TabEBM inherits some of TabPFN's limitations, particularly in scaling to a larger number of features. TabEBM can handle datasets with over 1000 samples, overcoming TabPFN's limitation, as it processes one class at a time. In Appendix D.5.3, we show that TabEBM outperforms other generators on larger datasets, though the performance gains decrease as the sample size increases. Although we implement TabEBM with TabPFN in this paper, we stress that TabEBM is compatible with any classifier that can be adapted into EBMs, as described in Section 2. As foundational models for tabular data evolve , new models capable of handling more features and samples are expected. Integrating them into TabEBM will enhance its ability to manage high-dimensional datasets, increasing its versatility and utility. Finally, note that, generators that are limited in modelling multivariate distributions may still perform well on univariate fidelity metrics, which is a standard approach to evaluating such models. However, evaluating their ability to learn more complex, high-order, relationships between features remains an open research question , which we leave for future work.

## 5 Conclusion

We introduced TabEBM, the first tabular data augmentation method that creates class-specific EBM generators, learning the marginal distribution for each class separately. We also provide the first comprehensive analysis of tabular data augmentation across various dataset sizes. Our results demonstrate that TabEBM improves downstream performance through data augmentation on real-world datasets, outperforming other benchmark generators. The statistical evaluation confirms that TabEBM generates high-fidelity synthetic data, particularly for small datasets. We release our method as an open-source library, allowing users to generate data immediately without additional training.

   &  &  &  &  \\   & & Learned &  &  &  &  &  &  \\  & & distribution & & models & generation & classes & (\(\) 10 classes) & (\(>\) 10 classes) \\  SMOTE  & Interpolation & N/A & ✓ & N/A & ✓ & ✓ & ✗ & ✗ \\ TVAE  & VAE & \(p(,y)\) & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\ CTGAN  & GAN & \(p(,y)\) & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ \\ NTLOW  & Normal Flows & \(p(,y)\) & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ TabDDPM  & Diffusion & \(p(\,y)\) & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ \\ ARF  & Random Forest & \(p(,y)\) & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\ GOODGLLE  & GNN & \(p(\,y)\) & ✗ & ✗ & ✓ & ✗ & ✗ \\ TabFPen  & PFN & \(p(\,y)\) & ✓ & ✗ & ✓ & ✗ & ✗ \\ 
**TabEBM (Ours)** & **PFN** & \(p(\,y)\) & ✓ & ✓ & ✓ & ✓ & ✗ \\  

Table 2: **Comparison of the properties between TabEBM and prior tabular generative methods. TabEBM has novel design rationales of training-free class-specific models, and TabEBM is highly practicable with wide applicability and consistent accuracy improvement.**