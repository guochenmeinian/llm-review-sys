# BiDM: Pushing the Limit of Quantization

for Diffusion Models

 Xingyu Zheng\({}^{1}\), Xianglong Liu\({}^{}}1}\), Yichen Bian\({}^{1}\), Xudong Ma\({}^{1}\), Yulun Zhang\({}^{2}\), Jiakai Wang\({}^{3}\), Jinyang Guo\({}^{1}\), Haotong Qin\({}^{4}\)

\({}^{1}\)Beihang University \({}^{2}\)Shanghai Jiao Tong University

\({}^{3}\)Zhongguancun Laboratory \({}^{4}\)ETH Zurich

{zhengxingyu,xlliu,macranolin,jinyangguo}@buaa.edu.cn

{yichen.bian.work,yulun100}@gmail.com wangjk@zgclab.edu.cn

haotong.qin@pbl.ee.ethz.ch

###### Abstract

Diffusion models (DMs) have been significantly developed and widely used in various applications due to their excellent generative qualities. However, the expensive computation and massive parameters of DMs hinder their practical use in resource-constrained scenarios. As one of the effective compression approaches, quantization allows DMs to achieve storage saving and inference acceleration by reducing bit-width while maintaining generation performance. However, as the most extreme quantization form, 1-bit binarization causes the generation performance of DMs to face severe degradation or even collapse. This paper proposes a novel method, namely **BiDM**, for fully binarizing weights and activations of DMs, pushing quantization to the 1-bit limit. From a temporal perspective, we introduce the _Timestep-friendly Binary Structure_ (TBS), which uses learnable activation binarizers and cross-timestep feature connections to address the highly timestep-correlated activation features of DMs. From a spatial perspective, we propose _Space Patched Distillation_ (SPD) to address the difficulty of matching binary features during distillation, focusing on the spatial locality of image generation tasks and noise estimation networks. As the first work to fully binarize DMs, the W1A1 BiDM on the LDM-4 model for LSUN-Bedrooms 256\(\)256 achieves a remarkable FID of 22.74, significantly outperforming the current state-of-the-art general binarization methods with an FID of 59.44 and invalid generative samples, and achieves up to excellent 28.0\(\) storage and 52.7\(\) OPs savings.

## 1 Introduction

Diffusion models (DMs) , as a type of generative visual model , have garnered impressive attention and applications in various fields, such as image , speech , and video , because of their high-quality and diverse generative capabilities. The diffusion model can generate data from random noise through up to 1000 denoising steps . Although some accelerated sampling methods effectively reduce the number of steps required for generating tasks , the expensive floating-point computation of each timestep still limits its wide application on resource-constrained scenarios. Therefore, compression of the diffusion model becomes a crucial step for its broader application, and existing compression methods mainly include quantization , distillation , pruning , _etc._ These compression approaches aim to reduce storage and computation while preserving accuracy.

Quantization is considered a highly effective model compression technique , which quantizes the weights and/or activations to low-bit integers or binaries for compact storage andefficient computation in inference. Some existing works thus apply quantization to compress DMs, aiming to compress and accelerate them while maintaining the quality of generation. Among them, 1-bit quantization, namely binarization, can achieve maximum storage savings for models and has performed well in discriminative models such as CNNs . Furthermore, when both weights and activations are quantized to 1-bit, \(e\)._g_., fully binarized, efficient bitwise operations such as XNOR and bitcount can replace matrix multiplication, achieving the most efficient acceleration .

Some existing works have attempted to quantize DM to 1-bit , but their exploration mainly focuses on the weights, which are still far from full binarization. In fact, for generative models like DM, the impact of fully binarizing weights and activations is catastrophic: a) As generative models, DMs have rich intermediate representations closely related to timesteps and highly dynamic activation ranges, which are both very limited in information when binarized weights and activations are used; b) Generative models like DMs are typically required to output complete images, but the highly discrete parameter and feature space make it particularly difficult for binarized DMs to match the ground truth during training. The limited representational capacity, which is hard to match with timesteps dynamically, and the optimization difficulty of generative tasks in discrete space make it difficult for the binarized DM to converge or even collapse during the optimization process.

We propose **BiDM** to push diffusion models towards extreme compression and acceleration through complete binarization of weights and activations. It is designed to address the unique properties of DMs' activation features, model structure, and the demands of generative tasks, overcoming the difficulties associated with complete binarization. BiDM consists of two novel techniques: _From a temporal perspective_, we observe that the activation properties of DMs are highly correlated with timesteps. We introduce the Timestep-friendly Binary Structure (TBS), which uses learnable activation binary quantizers to match the highly dynamic activation ranges of DMs and designs feature connections across timesteps to leverage the similarity of features between adjacent timesteps, thereby enhancing the representation capacity of the binary model. _From a spatial perspective_, we note the spatial locality of DMs in generative tasks and the convolution-based U-Net structure. We propose Space Patched Distillation (SPD), which introduces a full-precision model as a supervisor and uses attention-guided imitation on divided patches to focus on local features, better guiding the optimization direction of the binary diffusion model.

Extensive experiments show that compared to existing SOTA fully binarized methods, BiDM significantly improves accuracy while maintaining the same inference efficiency, surpassing all existing baselines across various evaluation metrics. Specifically, in pixel space diffusion models, BiDM is the only method that raises the IS to 5.18, close to the level of full-precision models and 0.95 higher than the best baseline method. In LDM, BiDM reduces the FID on LSUN-Bedrooms from the SOTA method's 59.44 to an impressive 22.74, while fully benefiting from 28.0\(\) storage and 52.7\(\) OPs savings. As the first fully binarized method for diffusion models, numerous generated samples also demonstrate that BiDM is currently the only method capable of producing acceptable images with fully binarized DMs, enabling the efficient application of DMs in low-resource scenarios.

Figure 1: Overview of BiDM with _Timestep-friendly Binary Structure_, which improves DM architecture temporally, and _Space Patched Distillation_, which enhances DM optimization spatially.

[MISSING_PAGE_FAIL:3]

U-Net , due to its ability to fuse low-level and high-dimensional features, has become the mainstream backbone of Diffusion. The input-output blocks of U-Net can be represented as \(\{D_{m}\}_{m=1}^{d}\) and \(\{U_{m}\}_{m=1}^{d}\), where blocks corresponding to smaller \(m\) are more low-level. Skip connections propagate low-level information from \(D_{m}()\) to \(U_{m}()\), so the input received by \(U_{m}\) is expressed as:

\[(D_{m}(),U_{m+1}()).\] (5)

Binarization.The quantization compresses and accelerates the noise estimation model by discretizing weights and activations to low bit-width. In the baseline of the binarized diffusion model, the weights \(\) are binarized to 1-bit [49; 5; 20]:

\[^{}=()=,&  0,\\ -,&,\] (6)

where \(\) function confine \(\) to +1 or -1 with 0 thresholds. \(\) is a floating-point scalar, which is initialized as \(\|}{n}\) (\(n\) denotes the number of weights) and learnable during training following [49; 33]. Meanwhile, activations are typically quantized by naive BNN quantizers [23; 32]:

\[^{}=()=1,&  0,\\ -1,&.\] (7)

When both weights and activations are quantized to 1-bit, the computations of the denoising model can be replaced by XNOR and bitcount operators, achieving significant compression and acceleration.

### Timestep-friendly Binary Structure

Before delving into the detailed description of the proposed method, we summarize our observation on the properties of DMs:

Observation 1._The activation range varies significantly across long-term timesteps, but the activation features are similar in short-term neighbouring timesteps._

Previous works, such as TDQ  and Q-DM , have commonly demonstrated that the activation distribution of DMs largely depends on denoising process, manifesting as similarities between adjacent timesteps while difference between distant ones, as shown in Figure 2(a). Therefore, applying a fixed scaling factor to activations across all timesteps can cause significant distortion in the activation range. Beyond the distribution range, Deepcache  highlights the substantial temporal consistency of high-dimensional features across consecutive timesteps, as shown in Figure 2(b).

These phenomena prompt us to reexamine existing binary structures. Binaryization, especially the full binaryization of weights and activations, results in a greater loss of activation range and precision compared to low-bit quantizations like 4-bit . This makes it more challenging to generate rich activation features. Such deficiencies in activation range and output features significantly harm representation-rich generative models like DMs. Therefore, adopting binary quantizers with more

Figure 2: (a) The activation range of the 4th convolutional layer of the full-precision DDIM model on CIFAR-10 varies with the denoising timesteps. (b) The output features are similar at each step of the full-precision LDM-4 model on LSUN-Bedrooms compared to the previous step.

flexible activation ranges for DMs, and enhancing the model's overall expressive power by leveraging its feature outputs, are crucial strategies for improving its generative capability after full binaryization.

We first focus on the differences between various timesteps over the long term. Most existing activation quantizers, such as BNN  and Bi-Real , as shown in Eq. (7), directly quantize activations to {+1, -1}. This approach significantly disrupts activation features and negatively impacts the expressive power of generative models. Some improved activation binary quantizers, such as XNOR++ , adopt a trainable scale factor \(k\):

\[^{}=K()=K,&  0,\\ -K,&,\] (8)

where the form of \(K\) could be either a vector or the product of multiple vectors, but it remains a constant value during inference. Although this approach partially restores the feature expression of activations, it does not align well with diffusion models that are highly correlated with timesteps and may still lead to significant performance loss.

We turn our attention to the original XNOR, which employs dynamically computed means to construct the activation binary quantizer. Its operation for 2D convolution can be expressed as:

\[*(() ())(K)=(()())(A*k),\] (9)

where \(^{c w_{in} h_{in}}\), \(^{c w h}\), \(A=_{i};]}{c}\), \(=\|\|_{_{1}}\). \(k^{1 1 w h}\) represents a 2D filter, where \( ij\ k_{ij}=\). \(*\) and \(\) indicate convolution with and without multiplication, respectively. This approach naturally preserves the range of activation features and dynamically adapts with the input range across different timesteps. However, due to the rich expression of DM features, local activations exhibit inconsistency in range before and after passing through modules, indicating that the predetermined value of \(k\) does not effectively restore the activation representation.

Therefore, we make \(k\) adjustable and allow it to be learned during training to adaptively match the changes in the range of activations before and after. The gradient calculation process of our learnable tiny convolution \(k\) can be expressed as follows:

\[}{ k}=}{( *)}(()()).\] (10)

Notably, making \(k\) learnable does not add any extra inference burden. The computational cost remains unchanged, allowing for efficient binary operations.

On the other hand, we focus on the similarity between adjacent timesteps. Deepcache directly extracts high-dimensional features as a cache to skip a large amount of deep computation in U-Net, achieving significant inference acceleration. This process is expressed as:

\[F^{t}_{cache} U^{t}_{m+1}(),(D^{t-1} _{m}(),F^{t}_{cache}).\] (11)

However, this approach does not apply to binarized diffusion models, as the information content of each output from a binary network is very limited. For binary diffusion models, which inherently achieve significant compression and acceleration but have limited expressive power, we anticipate that the similarity of features between adjacent timesteps will enhance binary representation, thereby compensating for the representation challenges.

We construct a cross-timestep information enhancement connection to enrich the expression at the current timestep using features from the previous step. This process can be expressed as:

\[(D^{t-1}_{m}(),(1-^{t-1}_{m+1}) U^{t-1} _{m+1}()+^{t-1}_{m+1} U^{t}_{m+1}()),\] (12)

where \(^{t-1}_{m+1}\) is a learnable scaling factor. As shown in Figure 2, the similarity of high-dimensional features varies across different blocks and timesteps in DMs. Therefore, we set multiple independent \(\) values to allow the model to adaptively learn more effectively during training.

In summary, Timestep-friendly Binary Structure (TBS) includes learnable tiny convolution applied to scaling factors after averaging the inputs and connections across timesteps. Their combined effect adapts to the changes in the activation range of diffusion models over long-range timesteps and leverages the similarity of high-dimensional features between adjacent timesteps to enhance information representation.

From the perspective of error reduction, a visualization of TBS is shown in Figure 3. First, we abstract the output of the binary DM under the baseline method as vector \(B^{t-1}\). The mismatch in scaling factors creates a significant difference in length between it and the output vector \(F^{t-1}\) of the full-precision model. Using our proposed scaling factors and learnable tiny convolutions, \(B^{t-1}\) is expanded to \(L^{t-1}\). \(L^{t-1}\) is closer to \(F^{t-1}\), but there is still a directional difference from the full-precision model. The cross-timestep connection further incorporates the outputs \(F^{t}\) of the previous timestep, \(B^{t}\), and \(L^{t}\). The high-dimensional feature similarity between adjacent timesteps means the gap between \(F^{t-1}\) and \(F^{t}\) is relatively small, facilitating the combination of \(L^{t-1}\) and \(L^{t}\). Finally, we obtain the binarized DM's output with TBS applied as \(T^{t-1}=(1-) L^{t-1}+ L^{t}\), closest to the output \(F^{t-1}\) of the full-precision model. The learnable tiny convolution \(k\) in TBS allows scaling factors to adapt more flexibly to the representation of DM, while connections across timesteps enable the binarized DM to use the previous step's output information for appropriate information compensation.

### Space Patched Distillation

Due to the nature of generative models, the optimization process of diffusion models exhibits different characteristics from past discriminative models:

Observation 2._Conventional distillation struggles to guide fully binarized DMs to align with full-precision DMs, while the features of DM exhibit locality in space during the generation task._

In previous practices, adding distillation loss during the training of quantized models has been a common approach. As the numerical space of binary models is limited, directly optimizing them using naive loss leads to difficulties in adjusting gradient update directions and makes learning challenging. Therefore, adding distillation loss to intermediate features can better guide the model's local and global optimization process.

However, as a generative model, the highly rich feature representation of DMs makes it extremely difficult for binary models to finely mimic full-precision models. Although the L2 loss used in the original DM training aligns with the Gaussian noise in the diffusion process, it is not suitable for the distillation matching of intermediate features. During regular distillation, the commonly used L2 loss tends to prioritize optimizing pixels with larger discrepancies, leading to a more uniform and smooth optimization result. This global constraint learning process is challenging for binary models aimed at image generation, as their limited representation capacity makes it difficult for fine-grained distillation imitation to directly adjust them to fully match the direction of full-precision models.

At the same time, we note that DMs using U-Net as a backbone naturally exhibit spatial locality due to their convolution-based structure and generative task requirements. This is different from past discriminative models, where tasks like classification only require overall feature extraction without low-level requirements, making traditional distillation methods unsuitable for DMs with spatial locality in generative tasks. Additionally, most existing DM distillation methods focus on reducing the number of timesteps and do not address the spatial locality of features required for image generation tasks.

Therefore, given the difficulty in optimizing binary DMs with existing loss functions and the spatial locality of DMs, we propose Space Patched Distillation (SPD). Specifically, we designed a new loss function that partitions features into patches before distillation and then calculates spatial attention-guided loss patch by patch. While conventional L2 loss makes it difficult for binary DMs to achieve direct matching, leading to optimization challenges, the attention mechanism allows the distillation

Figure 3: An illustration of TBS. Since the feature space is high-dimensional, we illustrate it using schematic diagrams.

optimization to focus more on critical parts. However, this is still challenging for fully binarized DMs because the highly discrete binary outputs have limited information, making it difficult for the model to capture global information. Therefore, we leverage the spatial locality of DMs by dividing intermediate features into multiple patches and independently calculating spatial attention-guided loss for each patch, allowing the binary model to better utilize local information during optimization.

SPD first divides the intermediate features \(^{}\) and \(^{}^{b c w h}\), output by a block of the binary DM and the full-precision DM respectively, into \(p^{2}\) patches:

\[^{}_{i,j}=^{}_{[:,i:+w/p,j:j+h/p]}; ^{}_{i,j}=^{}_{[:,i:i+w/p,j:j+h/p]}.\] (13)

Then, attention-guided loss is calculated for each patch separately:

\[^{}_{i,j}=^{}_{i,j}^{ {fp}}_{i,j}{}^{T},^{}_{i,j}=^{}_{ i,j}^{}_{i,j}{}^{T}.\] (14)

After regularization, the losses at corresponding positions are calculated and summed up:

\[^{m}_{}=}_{i=0}^{p-1}_{j=0}^{p-1} \|^{}_{i,j}}{\|^{}_{i,j}\| _{2}}-^{}_{i,j}}{\|^{}_{i,j}\| _{2}}\|_{2},\] (15)

where \(\|\|_{2}\) denotes the L2 function. Finally, the total training loss \(\) is computed as:

\[=_{}+_{m}^{2d+1} ^{m}_{},\] (16)

where \(d\) denotes the number of blocks during the upsampling process or downsampling process, resulting in a total of \(2d+1\) intermediate features, including the middle block. \(\) is a hyperparameter coefficient to balance the loss terms, defaulting set to 4.

We visualize the intermediate features and attention-guided SPD mentioned above. As Figure 4 shown, our SPD allows the model to pay more attention to local information in each patch.

## 4 Experiment

We conduct experiments on various datasets, including CIFAR-10 \(32 32\), LSUN-Bedrooms \(256 256\), LSUN-Churches \(256 256\) and FFHQ \(256 256\) over pixel space diffusion models  and latent space diffusion models . The evaluation metrics used in our study encompass Inception Score (IS), Frechet Inception Distance (FID) , Sliding Frechet Inception Distance (sFID) , Precision and Recall. To date, there has been no research that compresses diffusion models to such an extreme extent. Therefore, we use classical binarization algorithms , the recent SOTA general binarization algorithms , and quantization methods suited to generative models  as baselines. We extract the outputs of TimestepEmbedBlocks from the DM to serve as the operating target for our TBS and SPD. And we employ the same shortcut connections in convolutional layers as those used in ReActNet. Detailed experiment settings are presented in the Appendix A.

Figure 4: Visualization of the last TimeStepBlock’s output of the LDM model on LSUN-bedroom dataset. FP32 denotes the full-precision model’s output \(^{}\). Diff denotes the difference between the output of the full-precision model and the binarized one \(\|^{}-^{}\|\). Ours denotes the attention-guided SPD.

### Main Results

Pixel Space Diffusion Models.We first conduct experiments on the CIFAR-10 \(32 32\) dataset. As the results presented in Table 1, W1A1 binarization of DM using baseline methods results in substantial degradation. However, BiDM demonstrated significant improvements across all metrics, achieving unprecedented restoration of image quality. Specifically, BiDM achieved remarkable enhancements from 4.23 to 5.18 in the IS metric, and reduced 27.9% in the FID metric.

Latent Space Diffusion Models.Our LDM experiments encompass the evaluation of LDM-4 on LSUN-Bedrooms \(256 256\) and FFHQ \(256 256\) datasets, along with the assessment of LDM-8 on the LSUN-Churches \(256 256\) dataset. The experiments utilized the DDIM sampler with 200 steps, and the detailed outcomes are presented in Table 2. Across these three datasets, our method achieved significant improvements over the best baseline methods. In comparison to other binarization algorithms, BiDM outperformed across all metrics. On the LSUN-Bedrooms, LSUN-Churches, and FFHQ datasets, the FID metric of BiDM decreased by 61.7%, 30.7%, and 51.4%, respectively, compared to the best results among the baselines.

In contrast to XNOR++, its adoption of fixed activation scaling factors in the denoising process results in a very limited dynamic range for its activations, making it difficult to match the highly flexible generative representations of DMs. BiDM addressed this challenge by making the tiny convolution \(k\) learnable, which acts on the dynamically computed scaling factors. This optimization led to substantial improvements exceeding an order of magnitude across all metrics. On the LSUN-Bedrooms and LSUN-Churches datasets, the FID metric decreased from 319.66 to 22.74 and from 292.48 to 29.70, respectively. Additionally, compared to the SOTA binarization method ReSTE, BiDM achieved significant enhancements across multiple metrics, particularly demonstrating notable improvements on the LSUN-Bedrooms dataset. We have supplemented our work with BBCU, a binarization method more akin to generative models like DMs rather than discriminative models. Experimental results indicate that even as a binarization strategy for generative models, BBCU faces significant breakdowns when applied to DMs, as FID dropped dramatically to 236.07 on LSUN-Bedrooms. As a work targeting QAT for DM, EfficientDM is indeed a suitable comparison, especially since it designs TALSQ to address the variation in activation range. The results show that EfficientDM struggles to adapt to the extreme scenario of W1A1, and this may be due to its quantizer having difficulty adapting to binarized DM, and using QALoRA for weight updates might yield suboptimal results compared to full-parameter QAT.

As we mentioned in the TBS section of our manuscript, most existing binarization methods struggle to handle the wide activation range and flexible expression of DMs, further highlighting the necessity of TBS. Their optimization strategies may also not be tailored for the image generation tasks performed by DM, which means they only achieve conventional but suboptimal optimization.

### Ablation Study

We perform comprehensive ablation studies for LDM-4 on the LSUN-Bedrooms \(256 256\) dataset to evaluate the effectiveness of each proposed component in BiDM. We evaluate the effectiveness of our proposed SPD and TBS, and the results are presented in Table 3. Upon separately applying our SPD or TBS methods to LDM, we observed significant improvements compared to the original performance. When the TBS method was incorporated, FID and sFID dropped sharply from 106.62 and 56.61 to 35.23 and 25.13, respectively. Similarly, when the SPD method was added, FID and sFID decreased

   Model & Dataset & Method & \#Bits & IS\(\) & FID\(\) & sFID\(\) & Precision\(\) \\   &  & FP & 32/32 & 8.90 & 5.54 & 4.46 & 67.92 \\   & & XNOR++ & 1/1 & 2.23 & 251.14 & 60.85 & 44.98 \\  & & DoReFa & 1/1 & 1.43 & 397.60 & 139.97 & 0.17 \\  & \(32 32\) & ReActNet & 1/1 & 3.35 & 231.55 & 119.80 & 18.37 \\  & & ReSTE & 1/1 & 1.26 & 394.29 & 125.84 & 0.18 \\  & & XNOR & 1/1 & 4.23 & 113.36 & 27.67 & 46.96 \\  & & **BiDM** & **1/1** & **5.18** & **81.65** & **25.68** & **52.92** \\   

Table 1: Binarization results for DDIM on CIFAR-10 datasets with 100 steps.

significantly from 106.62 and 56.61 to 40.62 and 31.61, respectively. Other metrics also exhibited substantial improvements. This demonstrates the effectiveness of our approach in continuously approximating the binarized model features to full-precision features during training by introducing a learnable factor \(_{m}^{t}\) and incorporating connections between adjacent time steps. Furthermore, when we combined our two methods and applied them to LDM, we observed an additional improvement compared to the individual application of each method. This further substantiates that performing distillation between full-precision and binarized models at the patch level can significantly enhance the performance of the binarized model. We also conducted additional ablation experiments, and the results are presented in the appendix B.

   Model & Dataset & Method & \#Bits & FID\(\) & sFID\(\) & Precision\(\) & Recall\(\) \\   &  & FP & 32/32 & 2.99 & 7.08 & 65.02 & 47.54 \\   & & XNOR++ & 1/1 & 319.66 & 184.75 & 0.00 & 0.00 \\  & & BBCU & 1/1 & 236.07 & 89.66 & 0.59 & 5.66 \\  & & EfficientDM & 1/1 & 194.45 & 113.24 & 0.99 & 9.20 \\  & & DoReFa & 1/1 & 188.30 & 89.28 & 0.86 & 0.18 \\  & & ReActNet & 1/1 & 154.74 & 61.50 & 4.63 & 9.30 \\  & & & ReSTE & 1/1 & 59.44 & 42.16 & 12.06 & 2.92 \\  & & & XNOR & 1/1 & 106.62 & 56.81 & 6.82 & 5.22 \\  & & & **BiDM** & 1/1 & **22.74** & **17.91** & **33.54** & **19.90** \\   &  & FP & 32/32 & 4.36 & 16.00 & 74.64 & 48.98 \\   & & XNOR++ & 1/1 & 292.48 & 168.65 & 0.02 & 0.00 \\   & & DoReFa & 1/1 & 162.06 & 95.37 & 7.85 & 0.74 \\   & & ReActNet & 1/1 & 56.39 & 54.68 & 45.13 & 2.06 \\   & & ReSTE & 1/1 & 47.88 & 52.44 & 51.98 & 3.34 \\   & & XNOR & 1/1 & 42.87 & 49.24 & 51.53 & 4.28 \\  & & & **BiDM** & 1/1 & **29.70** & **45.14** & **55.75** & **14.80** \\   &  & FP & 32/32 & 4.87 & 6.96 & 74.73 & 50.57 \\   & & XNOR++ & 1/1 & 379.49 & 320.64 & 0.00 & 0.00 \\   & & DoReFa & 1/1 & 214.06 & 177.63 & 2.09 & 0.00 \\    & & ReActNet & 1/1 & 147.88 & 141.31 & 3.36 & 0.69 \\   & & ReSTE & 1/1 & 144.37 & 97.43 & 4.03 & 0.03 \\   & & XNOR & 1/1 & 89.37 & 54.04 & 31.31 & 4.11 \\   & & & **BiDM** & 1/1 & **43.42** & **32.35** & **49.44** & **13.96** \\   

Table 2: Quantization results for LDM on LSUN-Bedrooms, LSUN-Churches and FFHQ datasets.

Figure 5: Visualization of samples generated by the W1A1 baseline and our BiDM. BiDM is the first fully binarized DM method capable of generating viewable images, significantly surpassing advanced binarization methods.

### Efficiency Analysis

**Inference Efficiency Analysis.** We conducted an analysis of the diffusion model's inference efficiency under complete binarization. During inference, BiDM requires only a very small number of additional floating-point additions for the connections across timesteps compared to the classic binarization work XNOR-Net, and there are no differences in the majority of calculations, such as convolutions. Performing a floating-point convolution with a depth of 1 for scaling factors requires only a small amount of computation, and the overhead for averaging matrix \(A\) is also minimal. The findings presented in Table 4 reveal that BiDM, while achieving the same 28.0\(\) memory efficiency and 52.7\(\) computational savings as the XNOR baseline, demonstrates significantly superior image generation capabilities, with the FID decreased from 106.62 to 22.74. See Appendix B for more details.

**Training Efficiency Analysis.** We also explored the training efficiency of BiDM, as the overhead required for the QAT of binarized DMs cannot be overlooked. Theoretical analysis and experimental results show that BiDM achieved significantly better generative results than baseline methods under the same training cost, demonstrating that it not only has a higher upper limit of generative capability but is also relatively efficient in terms of generative performance. See Appendix B for details.

**Limitations.** The techniques of BiDM increase the training time of DMs compared with the original process, and future efforts may thus focus on the efficient quantization process of DMs.

## 5 Conclusion.

In this paper, we present BiDM, a novel fully binarized method that pushes the compression of diffusion models to the limit. Based on two observations -- activations at different timesteps and the characteristics of image generation tasks -- we propose the Timestep-friendly Binary Structure (TBS) and Space Patched Distillation (SPD) from temporal and spatial perspectives, respectively. These methods address the severe limitations in representation capacity and the challenges of highly discrete spatial optimization in full binarization. As the first fully binarized diffusion model, BiDM demonstrates significantly better generative performance than the SOTA general binarization methods across multiple models and datasets. On LSUN-Bedrooms, BiDM achieves an FID of 22.74, greatly surpassing the SOTA method with an FID of 59.44, making it the only method capable of generating visually acceptable samples while achieving up to 28.0\(\) storage savings and 52.7\(\) OPs savings.