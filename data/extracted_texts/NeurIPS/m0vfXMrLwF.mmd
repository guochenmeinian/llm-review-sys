# Learn to Categorize or Categorize to Learn?

Self-Coding for Generalized Category Discovery

 Sarah Rastegar, Hazel Doughty, Cees G. M. Snoek

University of Amsterdam

Currently at Leiden University

###### Abstract

In the quest for unveiling novel categories at test time, we confront the inherent limitations of traditional supervised recognition models that are restricted by a predefined category set. While strides have been made in the realms of self-supervised and open-world learning towards test-time category discovery, a crucial yet often overlooked question persists: what exactly delineates a _category_? In this paper, we conceptualize a _category_ through the lens of optimization, viewing it as an optimal solution to a well-defined problem. Harnessing this unique conceptualization, we propose a novel, efficient and self-supervised method capable of discovering previously unknown categories at test time. A salient feature of our approach is the assignment of minimum length category codes to individual data instances, which encapsulates the implicit category hierarchy prevalent in real-world datasets. This mechanism affords us enhanced control over category granularity, thereby equipping our model to handle fine-grained categories adeptly. Experimental evaluations, bolstered by state-of-the-art benchmark comparisons, testify to the efficacy of our solution in managing unknown categories at test time. Furthermore, we fortify our proposition with a theoretical foundation, providing proof of its optimality. Our code is available at: https://github.com/SarahRastegar/InfoSieve.

## 1 Introduction

The human brain intuitively classifies objects into distinct categories, a process so intrinsic that its complexity is often overlooked. However, translating this seemingly innate understanding of categorization into the realm of machine learning opens a veritable Pandora's box of differing interpretations for _what constitutes a category?_. Prior to training machine learning models for categorization tasks, it is indispensable to first demystify this concept of a _category_.

In the realm of conventional supervised learning , each category is represented by arbitrary codes, with the expectation that machines produce corresponding codes upon encountering objects from the same category. Despite its widespread use, this approach harbors several pitfalls: it suffers from label inconsistency, overlooks category hierarchies, and, as the main topic of this paper, struggles with open-world recognition.

**Pitfall I: Label Inconsistency.** Assessing a model's performance becomes problematic when category assignments are subject to noise  or exhibit arbitrary variation across different datasets. For example, if a zoologist identifies the bird in Figure 1 as a _flying fox fruit bat_, it is not due to a misunderstanding of what constitutes a _bird_ or a _dog_. Rather, it signifies a more nuanced understanding of these categories. However, conventional machine learning

Figure 1: **What is the correct category?** Photo of a _flying fox fruit bat_. This image can be categorized as _bat_, _bird_, _manual_, _flying bat_, and other categories. How should we define which answer is correct? This paper uses self-supervision to learn an implicit category code tree that reveals different levels of granularity in the data.

models may penalize such refined categorizations if they deviate from the pre-defined ground-truth labels. This work addresses this limitation by assigning category codes to individual samples. These codes not only prevent over-dependence on specific labels but also facilitate encoding similarities across distinct categories, paving the way for a more robust and nuanced categorization.

**Pitfall II: Category Hierarchies.** A prevalent encoding method, such as one-hot target vectors, falls short when addressing category hierarchies. While we, as humans, intuitively distinguish the categories of _plane_ and _dog_ as more disparate than _cat_ and _dog_, our representations within the model fail to convey this nuanced difference, effectively disregarding the shared semantics between the categories of _cat_ and _dog_. While some explorations into a category hierarchy for image classification have been undertaken , these studies hinge on an externally imposed hierarchy, thus limiting their adaptability and universality. This paper proposes a self-supervised approach that enables the model to impose these implicit hierarchies in the form of binary trees into their learned representation. For instance, Figure 2 shows that we can address each sample in the dataset with its path from the root of this implicit tree, hence associating a category code to it. We show theoretically that under a set of conditions, all samples in a category have a common prefix, which delineates category membership.

**Pitfall III: Open World Recognition.** The final problem we consider is the encounter with the open world . When a model is exposed to a novel category, the vague definition of _category_ makes it hard to deduce what will be an unseen new category. While open-set recognition models  can still evade this dilemma by rejecting new categories, Novel Class Discovery  or Generalized Category Discovery  can not ignore the fundamental flaw of a lack of definition for a _category_. This problem is heightened when categories are fine-grained  or follow a long-tailed distribution .

In this paper, we confront these challenges by reframing the concept of a _category_ as the solution to an optimization problem. We argue that categories serve to describe input data and that there is not a singular _correct_ category but a sequence of descriptions that span different levels of abstraction. We demonstrate that considering categorization as a search for a sequence of category codes not only provides more flexibility when dealing with novel categories but also, by leveraging sequences, allows us to modulate the granularity of categorization, proving especially beneficial for fine-grained novel categories. Subsequently, we illustrate how to construct a framework capable of efficiently approximating this solution. Our key contributions are as follows:

* _Theoretical._ We conceptualize a _category_ as a solution to an optimization problem. We then demonstrate how to fine-tune this optimization framework such that its mathematical solutions align with the human-accepted notion of _categories_. Furthermore, under a set of well-defined constraints, we establish that our method theoretically yields an optimal solution.
* _Methodological._ Based on the theory we developed, we propose a practical method for tackling the generalized category discovery problem, which is also robust to different category granularities.
* _Experimental._ We empirically show that our method outperforms state-of-the-art generalized category discovery and adapted novel class discovery methods on fine-grained datasets while performing consistently well on coarse-grained datasets.

Before detailing our contributions, we first provide some background on category discovery to better contextualize our work.

Figure 2: **The implicit binary tree our model finds to address samples. Each leaf in the tree indicates a specific sample, and each node indicates the set of its descendants’ samples. For instance, the node associated with ‘11...11’ is the set of all birds with red beaks, while its parent is the set of all birds with red parts in their upper body.**

Background

The _Generalized Category Discovery_ problem introduced by Vaze et al.  tries to categorize a set of images during inference, which can be from the known categories seen during training or novel categories. Formally, we only have access to \(_{}\) or seen categories during training time, while we aim to categorize samples from novel categories or \(_{}\) during test time. For the Novel Class Discovery problem, it is assumed that \(_{}_{}\)=\(\). However, this assumption could be unrealistic for real-world data. So Vaze et al.  proposed to use instead the more realistic Generalized Category Discovery assumption in which the model can encounter both seen and unseen categories during test time. In short, for the Generalized Category Discovery problem, we have \(_{}_{}\).

One major conundrum with both Generalized Category Discovery and Novel Category Discovery is that the definition of the _category_ has remained undetermined. This complication can be overlooked when the granularity of categories at test time is similar to training time. However, for more realistic applications where test data may have different granularity from training data or categories may follow a long-tailed distribution, the definition of _category_ becomes crucial. To this end, in the next section, we formulate categories as a way to abstract information in the input data.

## 3 An Information Theory Approach to Category Coding

To convert a subjective concept as a _category_ to a formal definition, we must first consider why categorization happens in the first place. There are many theories regarding this phenomenon in human [38; 39; 40] and even animal brains [41; 42; 43]. One theory is _categorization_ was a survival necessity that the human brain developed to retrieve data as fast and as accurately as possible . Studies have shown that there could be a trade-off between retrieval speed and accuracy of prediction in the brain [45; 46; 47]. Meanwhile, other studies have shown that the more frequent categories can be recognized in a shorter time, with more time needed to recognize fine-grained nested subcategories [48; 49]. These studies might suggest shorter required neural pulses for higher hierarchy levels. Inspired by these studies, we propose categorization as an optimization problem with analogous goals to the human brain. We hypothesize that we can do the category assignment to encode objects hierarchically to retrieve them as accurately and quickly as possible.

**Notation and Definitions.** Let us first formalize our notation and definitions. We denote the input random variable with \(X\) and the category random variable with \(C\). The category code random variable, which we define as the embedding sequence of input \(X^{i}\), is denoted by \(z^{i}\)=\(z_{1}^{i}z_{2}^{i} z_{L}^{i}\), in which superscript \(i\) shows the \(i\)th sample, while subscript \(L\) shows the digit position in the code sequence. In addition, \(I(X;Z)\) indicates the mutual information between random variables \(X\) and \(Z\)[50; 51], which measures the amount of _information_ we can obtain for one random variable by observing the other one. Since category codes are sequences, algorithmic information theory is most suitable for addressing the problem. We denote the algorithmic mutual information for sequences x and z with \(I_{}(:)\), which specifies how much information about sequence x we can obtain by observing \(\). Both Shannon and algorithmic information-theory-based estimators are useful for hierarchical clustering [52; 53; 54; 55; 56], suggesting we may benefit from this quality to simulate the implicit category hierarchy. A more in-depth discussion can be found in the supplemental.

### Maximizing the Algorithmic Mutual Information

Let's consider data space \(\)=\(\{X^{i},C^{i}:i\{1, N\}\}\) where \(X\)s are inputs and \(C\)s are the corresponding category labels.

**Lemma 1**: _For each category \(c\) and for \(X^{i}\) with \(C^{i}\)=\(\), we can find a binary decision tree \(_{}\) that starting from its root, reaches each \(X^{i}\) by following the decision tree path. Based on this path, we assign code \(c(X^{i})\)=\(c_{1}^{i}c_{2}^{i} c_{M}^{i}\) to each \(X^{i}\) to uniquely define and retrieve it from the tree. \(M\) is the length of the binary code assigned to the sample._

Proof of Lemma 2 is provided in the supplemental. Based on this lemma, we can find a forest with categories \(c\) as the roots and samples \(X^{i}\)s as their leaves. We apply the same logic to find a super decision tree \(\) that has all these category roots as its leaves. If we define the path code of category \(c\) in this super tree by \(p(c)\)=\(p_{1}^{c}p_{2}^{c} p_{K}^{c}\) where \(K\) is the length of the path to the category \(c\); we find the path to each \(X^{i}\) in the supertree by concatenating its category path code with its code in the category decision tree. So for each input \(X^{i}\) with category \(c\) we define an address code as \(q_{1}^{i}q_{2}^{i} q_{K+M}^{i}\) in which \(q_{j}^{i}\)=\(p_{j}^{c}\) for \(j K\) and \(q_{j}^{i}\)=\(c_{j-K}^{i}\) for \(j>K\). Meanwhile, since all \(X^{i}\)s are the descendantsof root \(c\) in the \(_{c}\) tree, we know there is one encoding to address all samples, in which samples of the same category share a similar prefix. Now consider a model that provides the binary code \(^{i}{=}z_{1}^{i} z_{L}^{i}\) for data input \(X^{i}\) with category \(c\), let's define a valid encoding in Definition 2.

**Definition 1**: _A valid encoding for input space \(\) and category space \(\) is defined as an encoding that uniquely identifies every \(X^{i}\). At the same time, for each category \(c\), it ensures that there is a sequence that is shared among all members of this category but no member out of the category._

As mentioned before, if the learned tree for this encoding is isomorph to the underlying tree \(\), we will have the necessary conditions that Theorem 1 provides.

**Theorem 1**: _For a learned binary code \(^{i}\) to address input \(X^{i}\), uniquely, if the decision tree of this encoding isomorph to underlying tree \(T\), we will have the following necessary conditions:_

1. \(I_{}(:x) I_{}(}:x) ,\) _is a valid encoding for_ \(x\)__
2. \(I_{}(:c) I_{}(}:c) ,\) _is a valid encoding for_ \(x\)__

Proof of Theorem 1 is provided in the supplemental. Optimizing for these two measures provides an encoding that satisfies the necessary conditions. However, from the halting Theorem , this optimization is generally not computable .

**Theorem 1 Clarification**. The first part of Theorem 1 states that if there is an implicit hierarchy tree, then for any category tree that is isomorph to this implicit tree, the algorithmic mutual information between each sample and its binary code generated by the tree will be maximal for the optimal tree. Hence, maximizing this mutual information is a necessary condition for finding the optimal tree. This is equivalent to finding a tree that generates the shortest-length binary code to address each sample uniquely. The second part of Theorem 1 states that for the optimal tree, the algorithmic mutual information between each sample category and its binary code will be maximum. Hence, again, maximizing this mutual information is a necessary condition for finding the optimal tree. This is equivalent to finding a tree that generates the shortest-length binary code to address each category uniquely. This means that since the tree should be a valid tree, the prefix to the unique address of every category sample \(c\) should be the shortest-length binary code while not being the prefix of any sample from other categories.

**Shannon Mutual Information Approximation**. We can approximate these requirements using Shannon mutual information instead if we consider a specific set of criteria. First, since Shannon entropy does not consider the relationship between separate bits or \(z_{i}^{i}\), we convert each sequence to an equivalent random variable number by considering its binary digit representation. To this end, we consider \(Z^{i}{=}_{k=1}^{m}^{i}}{2^{k}}\), which is a number between \(0\) and \(1\). To replace the first item of Theorem 1 by its equivalent Shannon mutual information, we must also ensure that z has the minimum length. For the moment, let's assume we know this length by the function \(l(X^{i}){=}l_{i}\). Hence instead of \(Z^{i}\), we consider its truncated form \(Z^{i}_{l_{i}}{=}_{k=1}^{l_{i}}^{i}}{2^{k}}\). This term, which we call the address loss function, is defined as follows:

\[_{}=-_{i=0}^{N}I(X^{i};Z^{i}_{l_{i}})  s.t. Z^{i}_{l_{i}}=_{k=1}^{l_{i}}^{i}}{2^{k}} k,z_{k}^{i}\{0,1\}.\] (1)

We can approximate this optimization with a contrastive loss. However, there are two requirements that we must consider; First, we have to obtain the optimal code length \(l_{i}\), and second, we have to ensure \(z_{k}^{i}\) is binary. In the following sections, we illustrate how we can satisfy these requirements.

### Category Code Length Minimization

To find the optimal code lengths \(l_{i}\) in Equation 1, we have to minimize the total length of the latent code. We call this loss \(_{}\), which we define as \(_{}{=}_{i=0}^{N}l_{i}\). However, since the \(l_{i}\) are in the subscripts of Equation 1, we can not use the conventional optimization tools to optimize this length. To circumvent this problem, we define a binary mask sequence \(^{i}{=}m_{1}^{i}m_{2}^{i} m_{L}^{i}\) to simulate the subscript property of \(l_{i}\). Consider a masked version of \(^{i}{=}z_{1}^{i} z_{L}^{i}\), which we will denote as \(}^{i}{=}_{1}^{i}_{L}^{i}\), in which for \(1 k L\), we define \(_{k}^{i}{=}z_{k}^{i}m_{k}^{i}\). The goal is to minimize the number of ones in sequence \(^{i}\) while forcing them to be at the beginning of the sequence. One way to ensure this is to consider the sequence \(}^{i}{=}(m_{1}^{i}2^{1})(m_{2}^{i}2^{2})(m_{L}^{i}2^{L})\) and minimize its \(L_{p}\) Norm for \(p 1\)This will ensure the requirements because adding one extra bit has an equivalent loss of all previous bits. In the supplemental, we provide a more rigorous explanation.

\[_{}_{i=0}^{N}^{ i}_{p}\,.\] (2)

We extract the mask from the input \(X^{i}\), i.e., \(^{i}{=}Mask(X^{i})\). Mask digits should also be binary, so we need to satisfy their binary constraints, which we will address next.

**Satisfying Binary Constraints.** Previous optimizations are constrained to two conditions, _Code Constraint_: \( z_{k}^{i},\ z_{k}^{i}=0\ or\ z_{k}^{i}=1\) and _Mask Constraint_: \( m_{k}^{i},\ m_{k}^{i}=0\ or\ m_{k}^{i}=1\). We formulate each constraint in an equivalent Lagrangian function to make sure they are satisfied. For the binary code constraint we consider \(f_{}(z_{k}^{i}){=}(z_{k}^{i}){=}0\), which is only zero if \(z_{k}^{i}{=}0\) or \(z_{k}^{i}{=}1\). Similarly, for the binary mask constraint, we have \(f_{}(m_{k}^{i}){=}(m_{k}^{i}){=}0\). To ensure these constraints are satisfied, we optimize them with the Lagrangian function of the overall loss.

### Aligning Category Codes using Supervision Signals

The second item in Theorem 1 shows the necessary condition for maximizing mutual information with a category. If we replace algorithmic mutual information with its Shannon cousin, we will have:

\[_{}=-_{i=0}^{N}I(c^{i};Z_{l_{i}}^{i}) s.t. Z_{l_{i}}^{i}=_{k=1}^{l_{i}}^{i}}{2^{k}} k,z_{k}^{i}\{0,1\}.\] (3)

Subject to satisfying the binary constraints. Note that the optimal lengths might differ for optimizing information based on categories or input. However, here we consider the same length for both scenarios for simplicity.

**Overall Loss.** Putting all these losses and constraints together, we will reach the constrained loss:

\[_{}=_{}+_{ }+_{} s.t. k,i\ f_{}(z_{k}^{i})=0, k,i\ f_{}(m_{k}^{i})=0.\] (4)

Note that when we do not have the supervision signals, we can consider \(0\) and extract categories in an unsupervised manner. In the supplemental, we have shown how to maximize this function based on the Lagrange multiplier. If we indicate \(_{}{=}_{i=0}^{N}_{k=1}^{L}(z_{k}^{i})^{2}(1 -z_{k}^{i})^{2}\) and \(_{}{=}_{i=0}^{N}_{k=1}^{L}(m_{k}^{i})^{2} (1-m_{k}^{i})^{2}\). The final loss will be:

\[_{}=_{}+_{ {length}}+_{}+_{} +_{}.\] (5)

Note that for satisfying binary constraints, we can adopt other approaches. For instance, we can omit the requirement for this hyperparameter by using binary neural networks and an STE (straight-through estimator) . Another approach would be to benefit from Boltzmann machines  to have a binary code. Having defined our theoretical objective, we are now ready to make it operational.

## 4 InfoSieve: Self-supervised Code Extraction

In this section, using the equations from Section 3, we devise a framework to extract category codes. Note that as Theorem 1 indicates, when we train the model to extract the category codes instead of the categories themselves, we make the model learn the underlying category tree. The model must ensure that in its implicit category tree, there is a node for each category whose descendants all share the same category while there are no non-descendants of this node with the same category.

The overall framework of our model, named InfoSieve, is depicted in Figure 3. We first extract an embedding using the contrastive loss used by . Then our _Code Generator_ uses this embedding to generate binary codes, while our _Code Masker_ learns a mask based on these embeddings to minimize the code length. Ultimately, the _Categorizer_ uses this truncated code to discern ground-truth categories. In the next sections, we explain each component in more detail.

Figure 3: **InfoSieve framework.**_Feature Extractor_ extracts an embedding by minimizing contrastive loss \(_{}\). The _Code Generator_ uses these input embeddings to find category codes. The _Code Masker_ simultaneously learns masks that minimize the code length with \(_{}\). Finally, truncated category codes are used to minimize a contrastive loss for category codes while also predicting the seen categories by minimizing \(_{}\).

### Contrastive Learning of Code and Input

One of the advantages of contrastive learning is to find a representation that maximizes the mutual information with the input . For input \(X^{i}\), let's show the hidden representation learning with \(Z^{i}\), which is learned contrastively by minimizing the InfoNCE loss. Van den Oord et al.  showed that minimizing InfoNCE loss increases a lower bound for mutual information. Hence, contrastive learning with the InfoNCE loss can be a suitable choice for minimizing the \(_{}\) in Equation 1. We will use this to our advantage on two different levels. Let's consider that \(Z^{i}\) has dimension \(d\), and each latent variable \(z^{i}_{k}\) can take up \(n\) different values. The complexity of the feature space for this latent variable would be \((n^{d})\), then as we show in supplemental, the number of structurally different binary trees for this feature space will be \((4^{n^{d}})\). So minimizing \(n\) and \(d\) will be the most effective way to limit the number of possible binary trees. Since our model and the amount of training data is bounded, we must minimize the possible search space while providing reasonable performance. At the same time, the input feature space \(X^{i}\) with \(N\) possible values and dimension \(D\) has \((N^{D})\) possible states. To cover it completely, we can not arbitrarily decrease \(d\) and \(n\). Note that for a nearly continuous function \(N\), the probability of a random discrete tree fully covering this space would be near zero. To make the best of both worlds, we consider different levels of complexity of latent variables, each focusing on one of these goals.

**Minimizing Contrastive Loss on the Inputs.** Similar to , we use this unsupervised contrastive loss to maximize the mutual information between input \(X^{i}\) and the extracted latent embedding \(Z^{i}\). Akin to , we also benefit from the supervised contrastive learning signal for the members of a particular category. Let's assume that the number of categories in the entire dataset is \(\). Different members of a category can be seen as different views of that category, analogous to unsupervised contrastive loss. Hence, they combine these unsupervised contrastive loss or \(_{}^{s}\) and its supervised counterpart, \(_{}^{s}\) with a coefficient \(\), which we call \(_{}\) in a following manner:

\[_{C}=(1-_{})_{} ^{u}+_{}_{}^{s}\] (6)

For covering the input space, \(X^{i}\), the loss function from Equation 6 is more suited if we consider both input and latent features as approximately continuous. We have shown this loss by \(_{}\) in Figure 3.

**Minimizing Contrastive Loss on the Codes.** In order to also facilitate finding the binary tree, we map the latent feature extracted from the previous section to a binary code with minimum length. Hence we effectively decrease \(n\) to \(2\) while actively minimizing \(d\). Furthermore, we extract a code by making the value of each bit in this code correspond to its sequential position in a binary number, i.e., each digit has twice the value of the digit immediately to its right. This ensures the model treats this code as a binary tree coding with its root in the first digit. We consider unsupervised contrastive learning for raw binary digits \(_{}^{u}\) and supervised variants of this loss for the extracted code, which we will show by and \(_{}^{s}\), the total contrastive loss for the binary embedding is defined as:

\[_{}=(1-_{})_{}^{u}+_{}_{}^{s}\] (7)

In summary, the loss from Equation 6 finds a tree compatible with the input, while the loss from Equation 7 learns an implicit tree in compliance with categories. Then we consider \(_{}\) as their combination:

\[_{}=_{}+_{ }\] (8)

### Minimizing Code Length

As discussed in Section 4.1, we need to decrease the feature space complexity by using minimum length codes to reduce the search space for finding the implicit category binary tree. In addition, using Shannon's mutual information as an approximate substitute for algorithmic mutual information necessitates minimizing the sequence length. We must simultaneously learn category codes and their optimal length. Since each of these optimizations depends on the optimal solution of the other one, we use two different blocks to solve them at the same time.

**Code Generator Block.** In Figure 3, the _Code Generator_ block uses the extracted embeddings to generate binary category codes. At this stage, we consider a fixed length for these binary codes. The output of this stage is used for unsupervised contrastive learning on the codes in Equation 7. We also use \(_{}\) to enforce the digits of the codes to be a decent approximation of binary values.

**Code Masker Block.** For this block, we use the \(_{}\) to ensure the binary constraint of the outputs. In addition, to control the length of the code or, in other words, the sequence of \(1\)s at the beginning, we use \(_{}\) in Equation 2.

### Aligning Codes using Supervision Signals

In the final block of the framework, we convert category codes from the _Code Generator_ to a binary number based on the digit positions in the sequence. To truncate this code, we do a Hadamard multiplication for this number by the mask generated by the _Code Masker_. We use these truncated codes for supervised contrastive learning on the codes. Finally, we feed these codes to the _Catgorizer_ block to predict the labels directly. We believe relying solely on contrastive supervision prevents the model from benefiting from learning discriminative features early on to speed up training.

## 5 Experiments

### Experimental Setup

**Eight Datasets.** We evaluate our model on three coarse-grained datasets CIFAR10/100  and ImageNet-100  and four fine-grained datasets: CUB-200 , Aircraft , SCars  and Oxford-Pet . Finally, we use the challenging Herbarium19  dataset, which is fine-grained and long-tailed. To acquire the train and test splits, we follow . We subsample the training dataset in a ratio of \(50\%\) of known categories at the train and all samples of unknown categories. For all datasets except CIFAR100, we consider \(50\%\) of the categories as known categories at training time. For CIFAR100, \(80\%\) of the categories are known during training time, as in . A summary of dataset statistics and their train test splits is shown in the supplemental.

**Implementation Details.** Following , we use ViT-B/16 as our backbone, which is pre-trained by DINO  on unlabelled ImageNet 1K . Unless otherwise specified, we use \(200\) epochs and batch size of \(128\) for training. We present the complete implementation details in the supplemental. Our code is available at: https://github.com/SarahRastegar/InfoSieve.

**Evaluation Metrics.** We use semi-supervised \(k\)-means proposed by  to cluster the predicted embeddings. Then, the Hungarian algorithm  solves the optimal assignment of emerged clusters to their ground truth labels. We report the accuracy of the model's predictions on _All_, _Known_, and _Novel_ categories. Accuracy on _All_ is calculated using the whole unlabelled train set, consisting of known and unknown categories. For _Known_, we only consider the samples with labels known during training. Finally, for _Novel_, we consider samples from the unlabelled categories at train time.

### Ablative studies

We investigate each model's component contribution to the overall performance of the model and the effect of each hyperparameter. Further ablations can be found in the supplemental.

**Effect of Each Component.** We first examine the effect of each component using the CUB dataset. A fine-grained dataset like CUB requires the model to distinguish between the semantic nuances of each category. Table 1 shows the effect of each loss component for the CUB dataset. As we can see from this table, \(_{}\) and \(_{}\) have the most positive effect on novel categories while affecting known categories negatively. Utilizing \(_{}\) to enforce binary constraints on the embedding enhances performance for both novel and known categories. Conversely, applying \(_{}\) boosts performance for known categories while detrimentally impacting novel categories. This is aligned with Theorem 1 and the definition of the category because these losses have opposite goals. As discussed, minimizing the search space helps the model find the implicit category tree faster. \(_{}\) and \(_{}\) try to achieve this by mapping the information to a smaller feature space, while condition

   \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & All & Known & Novel \\  ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & 66.8 & 78.1 & 61.1 \\ ✓ & ✓ & ✗ & ✗ & ✗ & ✗ & 67.7 & 75.7 & 63.7 \\ ✓ & ✓ & ✓ & ✗ & ✗ & ✗ & 68.5 & 77.5 & 64.0 \\ ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & 68.4 & 76.1 & 64.5 \\ ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & 67.8 & 78.7 & 62.3 \\ ✓ & ✗ & ✗ & ✗ & ✗ & ✓ & 68.2 & 76.4 & 64.1 \\  ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & 69.4 & 77.9 & 65.2 \\   

Table 1: **Ablation study on the effectiveness of each loss function. Accuracy score on the CUB dataset is reported. This table indicates each component’s preference for novel or known categories. In the first row of the table, differences in results compared with  can be attributed to specific implementation details, which are elaborated in section B.2 in the supplemental.**losses solve this by pruning and discarding unnecessary information. Finally, \(_{}\) on its own has a destructive effect on known categories. One reason for this is the small size of the CUB dataset, which makes the model overfit on labeled data. However, when all losses are combined, their synergic effect helps perform well for both known and novel categories.

### Hyperparameter Analysis

Our model has a few hyperparameters: code binary constraint (\(\)), mask binary constraint (\(\)), code contrastive (\(\)), code length (\(\)), and code mapping (\(\)). We examine the effect of each hyperparameter on the model's performance on the CUB dataset. Our default values for the hyperparameters are: code constraint coeff \(0.01\), mask constraint coeff \(0.01\), code contrastive coeff \(1\), code mapping coeff \(0.01\) and code length coeff \(0.1\).

**Code Binary Constraint.** This hyperparameter is introduced to satisfy the binary requirement of the code. Since we use tanh to create the binary vector, the coefficient only determines how fast the method satisfies the conditions. When codes \(0\) and \(1\) are stabilized, the hyperparameter effect will be diminished. However, we noticed that more significant coefficients somewhat affect the known accuracy. The effect of this hyperparameter for the CUB dataset is shown in Table 2 (a). We can see that the method is robust to the choice of this hyperparameter.

**Mask Binary Constraint.** For the mask constraint hyperparameter, we start from an all-one mask in our Lagrange multiplier approach. A more significant constraint translates to a longer category code. A higher coefficient is more useful since it better imposes the binary condition for the mask. The effect of different values of this hyperparameter for the CUB datasets is shown in Table 2 (b).

**Code Contrastive.** This loss maintains information about the input. From Table 2 (c), we observe that minimizing the information for a fine-grained dataset like CUB will lead to a better performance.

**Code Length.** Table 2 (d) reports our results for different values of code length hyperparameter. Since the code length is penalized exponentially, the code length hyperparameter's effect is not comparable to the exponential growth; hence, in the end, the model is not sensitive to this hyperparameter's value.

**Code Mapping.** Since current evaluation metrics rely on predefined categories, our category codes must be mapped to this scenario. This loss is not an essential part of the self-coding that our model learns, but it accelerates model training. The effect of this hyperparameter is shown in Table 3.

**Overall Parameter Analysis** One reason the model is not very sensitive to different hyperparameters is that our model consists of three separate parts: Code masker, Code Generator, and Categorizer. The only hyperparameters that affect all of these three parts directly are \(\), the code contrastive coefficient, and \(\), the code length coefficient. Hence, these hyperparameters affect our model's performance more.

    \\ 
**Code Constraint Coef** & All & Known & Novel \\  \(=0.01\) & 69.4 & **77.9** & 65.2 \\ \(=0.1\) & 69.1 & 76.1 & 65.6 \\ \(=1\) & **69.9** & 76.1 & **66.8** \\ \(=2\) & 69.5 & 75.5 & 66.5 \\    \\ 
**Code Contrastive Coef** & All & Known & Novel \\  \(=0.01\) & 68.6 & 77.0 & 64.5 \\ \(=0.1\) & **69.9** & 76.3 & **66.7** \\ \(=1\) & 69.4 & **77.9** & 65.2 \\ \(=2\) & 68.6 & **77.9** & 64.0 \\    
    \\ 
**Mask Constraint Coef** & All & Known & Novel \\  \(=0.01\) & 69.4 & 77.9 & 65.2 \\ \(=0.1\) & 67.2 & 74.1 & 63.8 \\ \(=1\) & 69.3 & 76.0 & 65.9 \\ \(=2\) & **70.6** & **79.3** & **66.3** \\    \\ 
**Code Cut Length Coef** & All & Known & Novel \\  \(=0.01\) & 69.4 & 77.0 & 65.6 \\ \(=0.1\) & 69.4 & **77.9** & 65.2 \\ \(=1\) & 68.8 & 75.8 & 65.2 \\ \(=2\) & **69.8** & 76.9 & **66.2** \\   

Table 2: **Hyperparameter analysis.** This table indicates the effect on the accuracy score of each hyperparameter on the CUB dataset for novel or known categories.

    \\ 
**Mask Constraint Coef** & All & Known & Novel \\  \(=0.01\) & 69.4 & 77.9 & 65.2 \\ \(=0.1\) & 67.2 & 74.1 & 63.8 \\ \(=1\) & 69.3 & 76.0 & 65.9 \\ \(=2\) & **70.6** & **79.3** & **66.3** \\    
    \\ 
**Code Cut Length Coef** & All & Known & Novel \\  \(=0.01\) & 69.4 & 77.0 & 65.6 \\ \(=0.1\) & 69.4 & **77.9** & 65.2 \\ \(=1\) & 68.8 & 75.8 & 65.2 \\ \(=2\) & **69.8** & 76.9 & **66.2** \\   

Table 3: **Effect of Code Mapping.** Accuracy scores on the CUB dataset for novel and known categories.

### Comparison with State-of-the-Art

**Fine-grained Image Classification.** Fine-grained image datasets are a more realistic approach to the real world. In coarse-grained datasets, the model can use other visual cues to guess about the novelty of a category; fine-grained datasets require that the model distinguish subtle category-specific details. Table 4 summarizes our model's performance on the fine-grained datasets. As we can see from this table, our model has more robust and consistent results compared to other methods for fine-grained datasets. Herbarium 19, a long-tailed dataset, raises the stakes by having different frequencies for different categories, which is detrimental to most clustering approaches because of the extremely unbalanced cluster size. As Table 4 shows, our model can distinguish different categories even from a few examples and is robust to frequency imbalance.

**Coarse-grained Image Classification.** Our method is well suited for datasets with more categories and fine distinctions. Nevertheless, we also evaluate our model on three coarse-grained datasets, namely CIFAR10 and CIFAR100  and ImageNet-100 . Table 5 compares our results against state-of-the-art generalized category discovery methods. As we can see from this table, our method performs consistently well on both known and novel datasets. For instance, while UNO+ shows the highest accuracy on the _Known_ categories of CIFAR-10, this is at the expense of performance degradation on the _Novel_ categories. The same observations can be seen on ImageNet-100. Table 5 shows that our method consistently performs competitively for both novel and known categories. Based on our theory, the smaller CIFAR10/100 and ImageNet-100 improvement is predictable. For CIFAR 10, the depth of the implicit tree is 4; hence, the number of implicit possible binary trees with this limited depth is smaller, meaning finding a good approximation for the implicit category tree can be achieved by other models. However, as the depth of this tree increases, our model can still find the aforementioned tree; hence, we see more improvement for fine-grained data.

    &  &  &  &  &  \\ 
**Method** & All & Known & Novel & All & Known & Novel & All & Known & Novel & All & Known & Novel \\  k-means  & 34.3 & 38.9 & 32.1 & 12.9 & 12.9 & 12.8 & 12.8 & 10.6 & 13.8 & 77.1 & 70.1 & 80.7 & 13.0 & 12.2 & 13.4 \\ RankStats* & 33.3 & 51.6 & 24.2 & 26.9 & 36.4 & 22.2 & 28.3 & 61.8 & 12.1 & - & - & - & 27.9 & 55.8 & 12.8 \\ UNO+  & 35.1 & 49.0 & 28.1 & 40.3 & 56.4 & 32.2 & 35.5 & 70.5 & 18.6 & - & - & - & 28.3 & 53.7 & 14.7 \\ ORCA  & 36.3 & 43.8 & 32.6 & 31.6 & 32.0 & 31.4 & 31.9 & 42.2 & 26.9 & - & - & - & 24.6 & 26.5 & 23.7 \\ GCD  & 51.3 & 56.6 & 48.7 & 45.0 & 41.1 & 46.9 & 39.0 & 57.6 & 29.9 & 80.2 & 85.1 & 77.6 & 35.4 & 51.0 & 27.0 \\ XCON  & 52.1 & 54.3 & 51.0 & 47.7 & 44.4 & 44.9 & 40.5 & 58.8 & 31.7 & 86.7 & 91.5 & 84.1 & - & - & - \\ PromptC2k  & 62.9 & 64.4 & 62.1 & 52.2 & 52.2 & 52.3 & 50.2 & 70.1 & 40.6 & - & - & - & - & - & - \\ DCCL  & 63.5 & 60.8 & 64.9 & - & - & - & 43.1 & 55.7 & 36.2 & 88.1 & 88.2 & 88.0 & - & - & - \\ SimGCD  & 60.3 & 65.6 & 57.7 & 54.2 & 59.1 & 51.8 & 53.8 & 71.9 & 45.0 & - & - & - & **44.0** & **58.0** & **36.4** \\ GPC  & 52.0 & 55.5 & 47.5 & 43.3 & 40.7 & 44.8 & 38.2 & 58.9 & 27.4 & - & - & - & - & - & - \\ InfoSieve & **69.4** & **77.9** & **65.2** & **56.3** & **63.7** & **52.5** & **55.7** & **74.8** & **46.4** & **91.8** & **92.6** & **91.3** & 41.0 & 55.4 & 33.2 \\   

Table 4: **Comparison on fine-grained image recognition datasets.** Accuracy score for the first three methods is reported from  and for ORCA from . Bold and underlined numbers, respectively, show the best and second-best accuracies. Our method has superior performance for the three experimental settings (_All_, _Known_, and _Novel_). This table shows that our method is especially well suited to fine-grained settings.

    &  &  &  \\ 
**Method** & All & Known & Novel & All & Known & Novel & All & Known & Novel \\  k-means  & 83.6 & 85.7 & 82.5 & 52.0 & 52.2 & 50.8 & 72.7 & 75.5 & 71.3 \\ RankStats* & 46.8 & 19.2 & 60.5 & 58.2 & 77.6 & 19.3 & 37.1 & 61.6 & 24.8 \\ UNO+  & 68.6 & **98.3** & 53.8 & 69.5 & 80.6 & 47.2 & 70.3 & **95.0** & 57.9 \\ ORCA  & 96.9 & 95.1 & 97.8 & 74.2 & 82.1 & 67.2 & 79.2 & 93.2 & 72.1 \\ GCD  & 91.5 & 97.9 & 88.2 & 73.0 & 76.2 & 66.5 & 74.1 & 89.8 & 66.3 \\ XCON & 96.0 & 97.3 & 95.4 & 74.2 & 81.2 & 60.3 & 77.6 & 93.5 & 69.7 \\ PromptC2k  & **97.9** & 96.6 & **98.5** & **81.2** & 84.2 & 75.3 & **83.1** & 92.7 & **78.3** \\ DCCL  & 96.3 & 96.5 & 96.9 & 75.3 & 76.8 & 70.2 & 80.5 & 90.5 & 76.2 \\ SimGCD  & 97.1 & 95.1 & 98.1 & 80.1 & 81.2 & **77.8** & 83.0 & 93.1 & 77.9 \\ GPC  & 90.6 & 97.6 & 87.0 & 75.4 & **84.6** & 60.1 & 75.3 & 93.4 & 66.7 \\  InfoSieve & 94.8 & 97.7 & 93.4 & 78.3 & 82.2 & 70.5 & 80.5 & 93.8 & 73.8 \\   

Table 5: **Comparison on coarse-grained image recognition datasets.** Accuracy for the first three methods from  and for ORCA from . Bold and underlined numbers, respectively, show the best and second-best accuracies. While our method does not reach state-of-the-art for coarse-grained settings, it has a consistent performance for all three experimental settings (_All_, _Known_, _Novel_).

[MISSING_PAGE_FAIL:10]