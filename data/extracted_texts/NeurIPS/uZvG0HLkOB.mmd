# Small Total-Cost Constraints in Contextual Bandits

with Knapsacks, with Application to Fairness

 Evgenii Chzhen  Christophe Giraud

Universite Paris-Saclay, CNRS, Laboratoire de mathematiques d'Orsay, 91405, Orsay, France

{evgenii.chzhen, christophe.giraud}@universite-paris-saclay.fr

 Zhen Li

BNP Paribas Corporate and Institutional Banking, 20 boulevard des Italiens, 75009 Paris, France

zhen.li@bnpparibas.com

Gilles Stoltz

Universite Paris-Saclay, CNRS, Laboratoire de mathematiques d'Orsay, 91405, Orsay, France

HEC Paris, 78351 Jouy-en-Josas, France

gilles.stoltz@universite-paris-saclay.fr, stoltz@hec.fr

###### Abstract

We consider contextual bandit problems with knapsacks [CBwK], a problem where at each round, a scalar reward is obtained and vector-valued costs are suffered. The learner aims to maximize the cumulative rewards while ensuring that the cumulative costs are lower than some predetermined cost constraints. We assume that contexts come from a continuous set, that costs can be signed, and that the expected reward and cost functions, while unknown, may be uniformly estimated--a typical assumption in the literature. In this setting, total cost constraints had so far to be at least of order \(T^{3/4}\), where \(T\) is the number of rounds, and were even typically assumed to depend linearly on \(T\). We are however motivated to use CBwK to impose a fairness constraint of equalized average costs between groups: the budget associated with the corresponding cost constraints should be as close as possible to the natural deviations, of order \(\). To that end, we introduce a dual strategy based on projected-gradient-descent updates, that is able to deal with total-cost constraints of the order of \(\) up to poly-logarithmic terms. This strategy is more direct and simpler than existing strategies in the literature. It relies on a careful, adaptive, tuning of the step size.

## 1 Setting, literature review, and main contributions

We consider contextual bandits with knapsacks [CBwK], a setting where at each round \(t 1\), the learner, after observing some context \(_{t}\), where \(^{n}\), picks an action \(a_{t}\) in a finite set \(\). We do not impose the existence of a null-cost action. Contexts are independently drawn according to a distribution \(\). The learner may pick \(a_{t}\) at random according to a probability distribution, denoted by \(_{t}(_{t})=_{t,a}(_{t})_{a}\) for consistency with the notion of policy defined later in Section 2. The action \(a_{t}\) played leads to some scalar reward \(r_{t}\) and some signed vector-valued cost \(_{t}[-1,1]^{d}\). Actually, \(r_{t}\) and \(_{t}\) are generated independently at random in a way such that the conditional expectations of \(r_{t}\) and \(_{t}\) given the past, \(_{t}\), and \(a_{t}\), equal \(r(_{t},a_{t})\) and \((_{t},a_{t})\), respectively. We denoted here by \(r:\) and \(=(c_{1},,c_{d}):[-1,1]^{d}\) the unknown expected-reward and expected-cost functions. The modeling and the estimation of

**Know parameters:** finite action set \(\); context set \(^{n}\); number \(T\) of rounds; vector of average cost constraints \(^{d}\)

**Unknown parameters:** context distribution \(\) on \(\); scalar expected-reward function \(r:\) and vector-valued expected-cost function \(:[-1,1]^{d}\), both enjoying some modeling allowing for estimation, see Section 2.2

**For rounds \(t=1,2,3,,T\)**:

1. Context \(_{t}\) is drawn independently of the past;
2. Learner observes \(_{t}\) and picks an action \(a_{t}\), possibly at random according to a distribution \(_{t}(_{t})=_{t,a}(_{t})_{a}\);
3. Learner gets a reward \(r_{t}\) with conditional expectation \(r(_{t},a_{t})\) and suffers constraints \(_{t}\) with conditional expectations \((_{t},a_{t})\).

**Goal:** Maximize \(_{t T}r_{t}\) while ensuring \(_{t T}_{t} T\)

and \(\) are discussed in Section 2.2. The aim of the learner is to maximize the sum of rewards (or equivalently, to minimize the regret defined in Section 2) while controlling cumulative costs. More precisely, we denote by \(=(B_{1},,B_{d})^{d}\) the normalized (i.e., average per-instance) cost constraints, which may depend on the coordinates. The number \(T\) of rounds is known, and the learner must play in a way such that \(_{1}++_{T} T\). The example of Section 2.1 illustrates how this setting allows for controlling costs in absolute values. The setting described is summarized in Box A.

**Overview of the literature review.** Contextual bandits with knapsacks [CBwK] is a setting that is a combination of the problems of contextual bandits (where only rewards are obtained, and no costs: see, among others, Lattimore and Szepesvari, 2020, Chapter 18 for a survey of this rich area) and of bandits with knapsacks (without contexts, as initially introduced by Badanidiyuru et al., 2013, 2018). The first approaches to CBwK (Badanidiyuru et al., 2014, Agrawal et al., 2016) relied on no specific modeling of the rewards and costs, and made the problem tractable by using as a benchmark a finite set of so-called static policies (but picking this set is uneasy, as noted by Agrawal and Devanur, 2016). Virtually all subsequent approaches to CBwK thus introduced structural assumptions in one way or the other. The simplest modelings are linear dependencies of expected rewards and costs on the contexts (Agrawal and Devanur, 2016) and a logistic conversion model (Li and Stoltz, 2022). The problem of CBwK attracted attention in recent past: we discuss and contrast the contributions by Chohlas-Wood et al. (2021); Slivkins et al. (2022); Han et al. (2022); Ai et al. (2022); Li and Stoltz (2022) after stating our main contributions.

**Overview of our main contributions.** Each contribution calls or allows for the next one.

**1.** This article revisits the CBwK approach by Chohlas-Wood et al. (2021) to a problem of fair spending of resources between groups while maximizing some total utility. Fairness constraints require to deal with signed costs and with possibly small total-cost constraints (typically close to \(T^{1/2}\)), while having no null-cost action at disposal.

**2.** We therefore provide a new CBwK strategy dealing with these three issues, the most significant being handling cost constraint \(T\) as small as \(T^{1/2+}\), breaking the state-of-the-art \(T^{3/4}\) barrier for CBwK with continuous contexts, while preserving attractive computational efficiency.

**3.** This new strategy is a direct, simple, explicit, dual strategy, relying on projected-gradient-descent updates (instead of using existing general algorithms as subroutines). We may perform an ad hoc analysis. The latter leads to refined regret bounds, in terms of the norms of some optimal dual variables. We also discuss the optimality of the results achieved by offering a proof scheme for problem-dependent (not only minimax) regret lower bounds.

### Detailed literature review

We now contrast our main contributions to the existing literature.

**CBwK and fairness: missing tools.** In the setting of Chohlas-Wood et al. (2021), there is a budget constraint on the total spendings on top of the wish of sharing out these spendings among groups (possibly favoring to some maximal extent some groups that are more in need). However,Chohlas-Wood et al. (2021) incorporated the fairness constraints in the reward function (through some Lagrangian penalty) instead of doing so in the vector-cost function, which does not seem the most natural way to proceed. One reason is that (see a discussion below) the total cost constraints \(T\) were so far typically assumed to be linear, which is undesirable in fairness applications, where one wishes that \(T\) is sublinear--and actually, is sometimes as close as possible to the natural \(\) deviations suffered even in a perfectly fair random allocation scheme. Also, there is no null-cost action and costs are signed--two twists to the typical settings of CBwK that only Slivkins et al. (2022) and our approach deal with, to the best of our knowledge. We describe our alternative modeling of this fairness problem in Section 2.1.

**Orders of magnitude for total-cost constraints \(T\).** The literature so far mostly considered linear total-cost constraints \(T\) (see, e.g., Han et al., 2022 or Ai et al., 2022 among recent references), with two series of exceptions: (i) the primal strategy by Li and Stoltz (2022) handling total-cost contraints of the order of \(\) up to poly-logarithmic terms but critically assuming the finiteness of the context set \(\); and (ii) the two-stage dual strategies of Agrawal and Devanur (2016); Han et al. (2022) handling \(T^{3/4}\) total-cost constraints. These two-stage strategies use \(\) preliminary rounds to learn some key hyperparameter \(Z\) and then transform strategies that work with linear total-cost constraints into strategies that may handle total-cost constraints larger than \(T^{3/4}\); see discussions after Lemma 2. In contrast, we provide a general theory of small cost constraints for continuous context sets, while obtaining similar regret bounds as the literature does: for some components \(j\), we may have \(TB_{j} T^{3/4}\) (actually, any rate \(TB_{j} T^{1/2+}\) is suitable for a sublinear regret), while for other components \(k\), the total-cost constraints \(TB_{k}\) may be linearly large.

**Typical CBwK strategies: primal approaches suffer from severe limitations.** CBwK is typically solved through dual approaches, as the latter basically rely on learning a finite-dimensional parameter given by the optimal dual variables \(^{}\) (see the beginning of Section 3), while primal approaches rely on learning the distribution \(\) of the contexts in \(^{1}\)-distance. This may be achieved in some cases, like finiteness of the context set \(\)(Li and Stoltz, 2022; Ai et al., 2022) or, at the cost of degraded regret bounds and under additional assumptions on \(\) when resorting to \(^{1}\)-density-estimation techniques (Ai et al., 2022). On top of these strong limitations, such primal approaches are also typically computationally inefficient as they require the computation of expectations over estimates of \(\).

**Typical CBwK strategies: dual approaches are modular and somewhat indirect.** Typical dual approaches in CBwK take two general forms. One, illustrated in Slivkins et al. (2022) (extending the non-contextual approach by Immorlica et al., 2019), takes a game-theoretic approach by identifying the primal-dual formulation (4) as some minimax equilibrium, which may be learned by separate regret minimization of a learner picking actions \(a_{t}\) and an opponent picking dual variables \(_{t}\). The second approach (Agrawal and Devanur, 2016; Han et al., 2022) learns more directly the \(_{t}\) via some online convex optimization algorithm fed with the suffered costs \(_{t}\); this second approach however requires, at the stage of picking \(a_{t}\), a suitable bound \(Z\) on the norms of the \(_{t}\).

The dual strategies discussed above are modular and indirect as they all consist of using as building blocks some general-purpose strategies. We rather propose a more direct dual approach, tailored to our needs. (We also believe that it is simpler and more elegant.) Our strategy picks the arm \(a_{t}\) that maximizes the Lagrangian penalization of rewards by costs through the current \(_{t-1}\) (see Step 2 in Box B), as also proposed by Agrawal and Devanur (2016) (while Han et al., 2022 add some randomization to this step), and then, performs some direct, explicit, projected-gradient-descent update on \(_{t-1}\) to obtain \(_{t}\), with step size \(\). We carefully and explicitly tune \(\) (in a sequential fashion, via regimes) to achieve our goals. Such fine-tuning is more difficult to perform with approaches relying on the black-box use of subroutines given by existing general-purpose strategies.

**Regret (upper and lower) bounds typically proposed in the CBwK literature.** A final advantage of our more explicit strategy is that we master each piece of its analysis: while not needing a null-cost action, we provide refined regret bounds that go beyond the typical \(((r,,)/)\) bound offered by the literature so far (see, among others, Agrawal and Devanur, 2016; Li and Stoltz, 2022; Han et al., 2022), where \((r,,)\) denotes the expected reward achieved by the optimal static policy (see Section 2 for definitions). Namely, our bounds are of the order of \(\|^{}\|\), where \(^{}\) is the optimal dual variable; we relate the norm of the latter to quantities of the form \((r,,)-(r,,) /(1-)\), for some \(<1\). Again, we may do so without the existence of a null-cost action, but when one such action is available, we may take \(=0\) in the interpretation of the bound. We also offer a proof scheme for lower bounds in Section 4 and explain why \(\|^{}\|\) appears as the correct target. We compare therein our problem-dependent lower bound-approach to the minimax ones of Han et al. (2022) and Slivkins et al. (2022).

### Outline

In Section 2, we further describe the problem of CBwK (by defining the regret and recalling how \(r\) and \(\) may be estimated) and state our motivating example of fairness. Section 3 is devoted to our new dual strategy, which we analyze first with a fixed step size \(\), for which we move next to an adaptive version, and whose bounds we finally discuss. Section 4 offers a proof scheme for regret lower bounds and lists some limitations of our approach, mostly relative to optimality.

## 2 Further description of the setting, and statement of our motivating example

We define a static policy as a mapping \(:()\), where \(()\) denotes the set of probability distributions over \(\). We denote by \(=(_{a})_{a}\) the components of \(\). We let \(_{}\) indicate that the expectation is taken over the random variable \(\) with distribution \(\). In the sequel, the inequalities \(\) or \(\) between vectors will mean pointwise satisfaction of the corresponding inequalities.

**Assumption 1**.: _The contextual bandit problem with knapsacks \((r,,^{},)\) is feasible if there exists a stationary policy \(\) such that_

\[_{}_{a}(,a)\, _{a}()^{}\,.\]

We denote the average cost constraints by \(^{}\) in the assumption above as in Section 3, we will actually require feasibility for average cost constraints \(^{}<\), not just for \(\).

In a feasible problem \((r,,^{},)\), the optimal static policy \(^{}\) is defined as the policy \(:()\) achieving

\[(r,,^{})=_{}_{ }_{a}r(,a)\,_{a}() :\ \ _{}_{a}(,a)\,_{ a}()^{}}.\] (1)

Maximizing the sum of the \(r_{t}\) amounts to minimizing the regret \(R_{T}=T\,(r,,^{})-_{t=1}^{T}r_{t}\,\).

### Motivating example: fairness--equalized average costs between groups

This example is inspired from Chohlas-Wood et al. (2021) (who did not provide a strategy to minimize regret), and features a total budget constraint \(TB_{}\) together with fairness constraints on how the budget is spent among finitely many subgroups, whose set is denoted by \(\). We assume that the contexts \(_{t}\) include the group index \((_{t})\), which means, in our setting, that the learner accesses this possibly sensitive attribute before making the predictions (the so-called awareness framework in the fairness literature). Each context-action pair \((,a)\), on top of leading to some expected reward \(r(,a)\), also corresponds to some spendings \(c_{}(,a)\). We recall that we denoted that \(r_{t}\) and \(c_{t}\) (here, this is a scalar for now) the individual payoffs and costs obtained at round \(t\); they have conditional expectations \(r(_{t},a_{t})\) and \(c_{}(_{t},a_{t})\). We want to trade off the differences in average spendings among groups with the total reward achieved: larger differences lead to larger total rewards but generate feelings of inequity or of public money not optimally used. (Chohlas-Wood et al. (2021) consider a situation where unfavored groups should get slightly more spendings than favored groups.)

Formally, we denote by \(()\) the group to which a given context \(\) belongs and introduce a tolerance factor \(\), which may depend on \(T\). We issue a simplifying assumption: while the distribution \(\) of the contexts is complex and is unknown, the respective proportions \(_{g}=:()=g}\) of the sub-groups may be known.1 The total-budget and fairness constraints then read:

\[_{t=1}^{T}c_{t} TB_{} g ,}_{t=1}^{T}c_{t}\,_{\{(_{t})=g\}}-_{t=1}^{T}c_{t} \,,\]which corresponds, in the setting of Box A, to the following vector-valued expected-cost function \(\), with \(d=1+2||\) components and with values in \([-1,1]^{1+2||}\):

\[(c_{}}(,a),\ (c_{}}(,a) _{\{()=g\}}-_{g}c_{}}(,a),\ _{g}c_{}}(,a)-c_{}}(,a) _{\{g()=g\}})_{g})\]

as well as to the average-constraint vector \(=(B_{}},\ (_{g}\,,_{g}\, )_{g})\).

The regimes we have in mind, and that correspond to the public-policy issues reported by Chohlas-Wood et al. (2021), are that the per-instance budget \(B_{}}\) is larger than a positive constant, i.e., a constant fraction of the \(T\) individuals may benefit from some costly action(s), while the fairness threshold \(\) must be small, and even, as small as possible. Because of central-limit-theorem fluctuations, a minimal value of the order of \(1/\), or slightly larger (up to logarithmic terms, say), has to be considered for \(\). The salient point is that the components of \(\) may therefore be of different orders of magnitude, some of them being as small as \(1/\), up to logarithmic terms. More details on this example and numerical simulations about it are provided in Section 5 and Appendix G.

### Modelings for, and estimation of, expected-reward and expected-cost functions

As is common in the literature (see Han et al., 2022 and Slivkins et al., 2022, who use the terminology of regression oracles), we sequentially estimate the functions \(r\) and \(\) and assume that we may do so in some uniform way, e.g., because of the underlying structure assumed. Note that the estimation procedure of Assumption 2 below does not force any specific choice of actions, it is able to exploit actions \(a_{t}\) picked by the main strategy (this is what the "otherwise" is related to therein). We denote by \(_{t}:\) and \(}_{t}:[-1,1]^{d}\) the estimations built based on \((_{s},a_{s},r_{s},_{s})_{s t}\), for \(t 1\). We also assume that initial estimations \(_{0}\) and \(}_{0}\), based on no data, are available. We denote by \(\) the column-vector \((1,,1)\).

**Assumption 2**.: _There exists an estimation procedure such that for all individual sequences of contexts \(_{1},_{2},\) and of actions \(a_{1},a_{2},\) played otherwise, there exist known error functions \(_{t}:(0,1]\), relying each on the pairs \((_{s},a_{s})_{s t}\), where \(t=0,1,2,\), ensuring that for all \((0,1]\), with probability at least \(1-\),_

\[ 0 t T,\ \  ,\ \  a, |_{t}(,a)-r(,a)| _{t}(,a,)\] \[ |}_{t}(,a)-c(,a)| _{t}(,a,)\,\,,\]

_where we assume in addition that \(\ \ _{T,}}{=}_{t=1}^{T} _{t-1}(_{t},a_{t},)=O\,.\)_

This assumption is satisfied at least for the two modelings discussed below, which are both of the form: for known transfer functions \(\), link functions \(\), and normalization function \(\), for some unknown finite-dimensional parameters \(_{}\), \(_{,1},,_{,d}\),

\[,\ \  a, r(,a)=_{r}(a,)\ _{r}_{r}(,a)^{}_{}\,,\] \[1 i d, c_{i}(,a)=_{c,i}(a,)\ _{c,i}_{c,i}(,a)^{}_{, i}\,.\]

See also the exposition by Han et al. (2022, Section 3.3).

Based on Assumption 2 and given the ranges \(\) for rewards and \([-1,1]^{d}\) for costs, we may now define the following (clipped) upper- and lower-confidence bounds : given a confidence level \(1-\) where \((0,1]\), for all \(t T\), for all \(\) and \(a\),

\[_{,t}^{}}(,a)}{=} [_{t}(,a)+_{t}(,a, )]_{0}^{1}\ \ }_{,t}^{}}(,a)}{=}[}_{t}(,a)- _{t}(,a,)]_{-1}^{1}\,,\] (2)

where we define clipping by \([x]_{}^{u}=\{x,\},\,u}\) for a scalar value \(x\) and lower and upper bounds \(\) and \(u\), and apply clipping component-wise to vectors. Under Assumption 2, we have that for all \((0,1]\), with probability at least \(1-\),

\[ 0 t T,\ \  ,\ \  a, r(,a)_{,t}^{}}(,a) r(,a)+2_{t}(,a,)\] (3) \[ (,a)-2_{t}(,a,) }_{,t}^{}}()(,a)\,.\]

Doing so, we have optimistic estimates of rewards (they are larger than the actual expected rewards) and of costs (they are smaller than the actual expected costs) for all actions \(a\), while for actions \(a_{t+1}\) played, and only these, we will also use the control from the other side, which will lead to manageable sums of \(_{t}(_{t+1},a_{t+1},)\), given the control on \(_{T,}\) by Assumption 2.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

\[R_{T}}1+\|_{-b_{T} }^{}\|_{t T} _{t} T\,,\]

_where a fully closed-form, non-asymptotic, regret bound is stated in (19) in Appendix C._

A complete proof may be found in Appendix C. Its structure is the following; all statements hold with high probability. We denote by \(_{k}\) the realized lengths of the regime. First, the number of regimes is bounded by noting that if regime \(K=\|_{-b_{T}}^{}\|\) is achieved, then by Lemma 1 and the choice of \(M_{T,,K}\), the stopping condition of regime \(K\) will never be met; thus, at most \(K+1\) regimes take place. We also prove that \(\|_{-b_{T}}^{}\| K_{T}= g\,T\). Second, on each regime \(k\), the difference of cumulated costs to \(_{k}(-b_{T})\) is smaller than \(M_{T,,k}\) by design, so that the total cumulative costs are smaller than \(T(-b_{T})\) plus something of the order of \((K_{T}+1)\,M_{T,,K_{T}}\), which is a quantity that only depends on \(T\), \(\), \(d\), and not on the unknown \(\|_{-b_{T}}^{}\|\), and that we take for \(T\,b_{T}\). Third, we similarly sum the regret bounds of Lemma 2 over the regimes: keeping in mind that \(b_{T}=}()\), we have sums of the form

\[\|_{-b_{T}}^{}\|_{k K}b_{T} _{k}+\|_{-b_{T}}^{} \|\,}_{k  K}_{k}T}{}_{-b_{T}}^{}\|}{}\,.\]

### Discussion of the obtained regret bounds

The regret bound of Theorem 1 features a multiplicative factor \(\|_{-b_{T}}^{}\|\) instead of the typical factor \((r,,)/\) proposed by the literature (see, e.g., Agrawal and Devanur, 2016; Li and Stoltz, 2022; Han et al., 2022). In view of the lower bound proof scheme discussed in Section 4, this bound \(\|_{-b_{T}}^{}\|\,\) seems the optimal bound. We thus now provide bounds on \(\|_{-b_{T}}^{}\|\) that have some natural interpretation. We recall that feasibility was defined in Assumption 1. The elementary proofs of Lemma 3 and Corollary 1 are based on (4) and may be found in Appendix D.

**Lemma 3**.: _Let \(b[0,)\) and let \(}<\!-b\). If the contextual bandit problem with knapsacks \((r,,^{},)\) is feasible for some \(^{}<}\), then_

\[\|_{-b}^{}\|(r,,-b)-(r,,})}{- b-}}\,.\]

**Corollary 1**.: _When there exists a null-cost action, \(\|_{-b}^{}\| 2\,(r,,)- (r,,)}{}\) for all \(b[0,/2]\)._

The bounds provided by Lemma 3 and Corollary 1 must be discussed in each specific case. They are null (as expected) when the average cost constraints \(\) are large enough so that costs do not constrain the choice of actions; this was not the case with the typical \((r,,)/\) bounds of the literature. Another typical example is the following.

**Example 1**.: _Consider a problem featuring a baseline action \(a_{}\) with some positive expected rewards and no cost, and additional actions with larger rewards and scalar expected costs \(c(,a)>0\). Denote by \(B\) the average cost constraint. Then actions \(a a_{}\) may only be played at most \(B/\) times on average, and therefore, \((r,c,B)-(r,c,0) B/\). In particular, the bound stated in Corollary 1 may be further upper bounded by \(1/(2)\), which is fully independent of \(T\) and \(B\); i.e., a \(\)-regret only is suffered in Theorem 1._

## 4 Optimality of the upper bounds, and limitations

The main limitations to our work are relative, in our eyes, to the optimality of the bounds achieved--up to one limitation, already discussed in Section 2.1: our fairness example is intrinsically limited to the awareness set-up, where the learner has access to the group index before making the predictions.

**A proof scheme for problem-dependent lower bounds.** Results offering lower bounds on the regret for CBwK are scarce in the literature. They typically refer to some minimax regret, and either do so by indicating that lower bounds for CBwK must be larger in a minimax sense than the ones obtained for non-contextual bandits with knapsacks (see, e.g., Agrawal and Devanur, 2016; Li and Stoltz, 2022), or by leveraging a minimax regret lower bound for a subroutine of the strategy used (Slivkins et al., 2022). We rather focus on problem-dependent regret bounds but only offer a proof scheme, to be made more formal--see Appendix E.1. Interestingly, this proof scheme follows closely the analysis of a primal strategy performed in Appendix F. First, an argument based on the law of the iterated logarithm shows the necessity of a margin \(b_{T}\) of order \(1/\) to satisfy the total \(T\) cost constraints. This imposes that only \(T(r,,-b_{T})\) is targeted, thus the regret is at least of order

\[T(r,,)-(r,,-b_{T})+\,,1+\|^{}_{}\|\,.\]

This matches the bound of Theorem 1, but with \(^{}_{-b_{T}}\) replaced by \(^{}_{}\).

We now turn to the list of limitations pertaining to the optimality of our bounds.

**Limitation 1: Large constants.** First, we must mention that in the bounds of Lemmas 1 and 2, we did not try to optimize the constants but targeted readability. The resulting values in Theorem 1, namely, the recommended choice of \(b_{T}\) and the closed-form bound (19) on the regret, therefore involve constants that are much larger than needed.

**Limitation 2: Not capturing possibly fast regret rates in some cases.** Second, a careful inspection of our proof scheme for lower bounds shows that it only works in the absence of a null-cost action. When such a null-cost action exists and costs are non-negative, Example 1 shows that costly actions are played at most \(B/\) times on average. If the null-cost action has also a null reward, the costly actions are the only ones generating some stochasticity, thus, we expect that the \(\) factors (coming, among others, from a version of Bernstein's inequality) be replaced by \(\) factors. As a consequence, in the setting of Example 1, bounds growing much slower than \(\) should be achievable, see the discussion in Appendix E.1.

**Limitation 3: \(\) regret not captured in case of softer constraints.** For the primal strategy discussed in Appendix F, we could prove a general \(\) regret bound in the case where total costs smaller than \(T+}\) are allowed. We were unable to do so for our dual strategy.

## 5 Brief overview of the numerical experiments performed

In Appendix G, we implement a specific version of the motivating example of Section 2.1, as proposed by Chohlas-Wood et al. (2021) in the public repository https://github.com/stanford-policylab/learning-to-be-fair. Fairness costs together with spendings related to rideshare assistance or transportation vouchers are considered. We report below the performance of the strategies considered only in terms of average rewards and rideshare costs, and for the fairness threshold \(=10^{-7}\). The Box B strategies with fixed step sizes may fail to control the budget when \(\) is too large: we observe this for \(=0.01\). We see overall a tradeoff between larger average rewards and larger average costs. The Box C strategy navigates between three regimes, \(=0.01\) to start (regime \(k=0\)), then \(=0.02\) (regime \(k=1\)), and finally \(=0.04\) (regime \(k=2\)), which it sticks to. It overall adaptively finds a decent tuning of \(\).