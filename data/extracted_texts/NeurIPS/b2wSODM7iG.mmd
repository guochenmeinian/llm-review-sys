# LightSpeed: Light and Fast

Neural Light Fields on Mobile Devices

 Aarush Gupta\({}^{1}\)  Junli Cao\({}^{2,}\)\({}^{,}\)\({}^{}\)  Chaoyang Wang\({}^{2,}\)\({}^{}\)  Ju Hu\({}^{2}\)  Sergey Tulyakov\({}^{2}\)

**Jian Ren\({}^{2}\)  Laszlo A Jeni\({}^{1}\)**

\({}^{1}\)Robotics Institute, Carnegie Mellon University \({}^{2}\)Snap Inc.

Project page: https://lightspeed-r21.github.io

###### Abstract

Real-time novel-view image synthesis on mobile devices is prohibitive due to the limited computational power and storage. Using volumetric rendering methods, such as NeRF and its derivatives, on mobile devices is not suitable due to the high computational cost of volumetric rendering. On the other hand, recent advances in neural light field representations have shown promising real-time view synthesis results on mobile devices. Neural light field methods learn a direct mapping from a ray representation to the pixel color. The current choice of ray representation is either stratified ray sampling or Plucker coordinates, overlooking the classic light slab (two-plane) representation, the preferred representation to interpolate between light field views. In this work, we find that using the light slab representation is an efficient representation for learning a neural light field. More importantly, it is a lower-dimensional ray representation enabling us to learn the 4D ray space using feature grids which are significantly faster to train and render. Although mostly designed for frontal views, we show that the light-slab representation can be further extended to non-frontal scenes using a divide-and-conquer strategy. Our method offers superior rendering quality compared to previous light field methods and achieves a significantly improved trade-off between rendering quality and speed.

## 1 Introduction

Real-time rendering of photo-realistic 3D content on mobile devices such as phones is crucial for mixed-reality applications. However, this presents a challenge due to the limited computational power and memory of mobile devices. The current graphics pipeline requires storing tens of thousands of meshes for complex scenes and performing ray tracing for realistic lighting effects, which demands powerful graphics processing power that is not feasible on current mobile devices. Recently, neural radiance field (NeRF ) has been the next popular choice for photo-realistic view synthesis, which offers a simplified rendering pipeline. However, the computational cost of integrating the radiance field remains a bottleneck for real-time implementation on mobile devices. There have been several attempts to reduce the computational cost of this integration step, such as using more efficient radiance representations  or distilling meshes from radiance field . Among these approaches, only a handful of mesh-based methods  have demonstrated real-time rendering capabilities on mobile phones, but with a significant sacrifice in rendering fidelity. Moreover, all aforementioned methods require significant storage space (over \(200\)MB), which is undesirable for mobile devices with limited onboard storage.

Alternatively, researchers have used 4D light field1 (or lumigraph) to represent radiance along rays in empty space , rather than attempting to model the 5D plenoptic function as in NeRF-based approaches. Essentially, the light field provides a direct mapping from rays to pixel values since the radiance is constant along rays in empty space. This makes the light field suitable for view synthesis, as long as the cameras are placed outside the convex hull of the object of interest. Compared to integrating radiance fields, rendering with light fields is more computationally efficient. However, designing a representation of light field that compresses its storage while maintaining high view-interpolation fidelity remains challenging. Previous methods, such as image quilts  or multiplane images (MPI) , suffer from poor trade-offs between fidelity and storage due to the high number of views or image planes required for reconstructing the complex light field signal. Recent works  have proposed training neural networks to represent light fields, achieving realistic rendering with a relatively small memory footprint. Among those, MobileR2L  uses less than 10MB of storage per scene, and it is currently the only method that demonstrates real-time performance on mobile phones.

However, prior neural light field (NeLF) representations, including MobileR2L, suffer from inefficiencies in learning due to the high number of layers (over \(60\) layers), and consequently, a long training time is required to capture fine scene details. One promising strategy to address this issue is utilizing grid-based representations, which have proven to be effective in the context of training NeRFs . Nonetheless, incorporating such grid-based representation directly to prior NeLFs is problematic due to the chosen ray parameterization. R2L  and MobileR2L  parameterize light rays using a large number of stratified 3D points along the rays, which were initially motivated by the discrete formulation of integrating radiance. However, this motivation is unnecessary and undermines the simplicity of 4D light fields because stratified sampling is redundant for rays with constant radiance. This becomes problematic when attempting to incorporate grid-based representations for more efficient learning, as the high-dimensional stratified-point representation is not feasible for grid-based discretization. Similarly, the \(6\)-dimensional Plucker coordinate used by Sitzmann _et al_.  also presents issues for discretization due to the fact that Plucker coordinates exist in a projective \(5\)-space, rather than Euclidean space.

In this paper, we present _LightSpeed_, the first NeLF method designed for mobile devices that uses a grid-based representation. As shown in Fig. 1, our method achieves a significantly better trade-off between rendering quality and speed compared to prior NeLF methods, while also being faster to train. These advantages make it well-suited for real-time applications on mobile devices. To achieve these results, we propose the following design choices:

**First**, we revisit the classic 4D light-slab (or two-plane) representation  that has been largely overlooked by previous NeLF methods. This lower-dimensional parameterization allows us to compactly represent the rays and efficiently represent the light field using grids. To our knowledge,

Figure 1: Our LightSpeed approach demonstrates a superior trade-off between on-device rendering quality and latency while maintaining a significantly reduced training time and boosted rendering quality. **(a)** rendering quality and latency on the \(400 400\) Lego scene  running on an iPhone 13. **(b)** training curves for the \(756 1008\) Fern scene .

Attal _et al_.  is the only other NeLF method that has experimented with the light-slab representation. However, they did not take advantage of the grid-based representation, and their method is not designed for real-time rendering. **Second**, to address the heavy storage consumption of 4D light field grids, we take inspiration from k-planes  and propose decomposing the 4D grids into six 2D feature grids. This ensures that our method remains competitive for storage consumption compared to prior NeLF methods. **Third**, we apply the super-resolution network proposed by MobileR2L , which significantly reduces the computational cost when rendering high-resolution images. **Finally**, the light-slab representation was originally designed for frontal-view scenes, but we demonstrate that it can be extended to represent non-frontal scenes using a divide-and-conquer strategy.

Our contributions pave the way for efficient and scalable light field representation and synthesis, making it feasible to generate high-quality images of real-world objects and scenes. Our method achieves the highest PSNR and among the highest frame rates (\(55\) FPS on iPhone 14) on LLFF (frontal-view), Blender (\(360^{}\)), and unbounded \(360^{}\) scenes, proving the effectiveness of our approach.

## 2 Related work

**Light Field.** Light field representations have been studied extensively in the computer graphics and computer vision communities . Traditionally, light fields have been represented using the 4D light slab representation, which parameterizes the light field by two planes in 4D space [12; 19]. More recently, neural-based approaches have been developed to synthesize novel views from the light field, leading to new light field representations being proposed.

One popular representation is the multi-plane image (MPI) representation, which discretizes the light field into a set of 2D planes. The MPI representation has been used in several recent works, including [41; 16; 32; 9; 7]. However, the MPI representation can require a large amount of memory, especially for high-resolution light fields. Another recent approach that has gained substantial attention is NeRF  (Neural Radiance Fields), which can synthesize novel views with high accuracy, but is computationally expensive to render and train due to the need to integrate radiance along viewing rays. There has been a substantial amount of works [37; 26; 28; 21; 13; 40; 28; 17; 5; 10; 34; 6; 39; 35; 27; 29; 36; 4; 2; 31] studying how to accelerate training and rendering of NeRF, but in the following, we focus on recent methods that achieve real-time rendering with or without mobile devices.

**Grid Representation of Radiance Field.** The first group of methods trade speed with space, by precomputing and caching radiance values using grid or voxel-like data structures such as sparse voxels [30; 13], octrees , and hash tables . Despite the efficient data structures, the memory consumption for these methods is still high, and several approaches have been proposed to address this issue. First, Chen _et al_.  and Fridovich-Keil _et al_.  decompose voxels into matrices that are cheaper to store. Takikawa _et al_.  performs quantization to compress feature grids. These approaches have enabled real-time applications on desktop or server-class GPUs, but they still require significant computational resources and are not suitable for resource-constrained devices such as mobile or edge devices.

**Baking High Resolution Mesh.** Another group of methods adopts the approach of extracting high-resolution meshes from the learned radiance field [6; 29; 35]. The texture of the mesh stores the plenoptic function to account for view-dependent rendering. While these approaches have been demonstrated to run in real-time on mobile devices, they sacrifice rendering quality, especially for semi-transparent objects, due to the mesh-based representation. Additionally, storing high-resolution meshes with features is memory-intensive, which limits the resolution and complexity of the mesh that can be used for rendering.

**Neural Light Fields.** Recent works such as R2L , LFNS  and NeuLF  have framed the view-synthesis problem as directly predicting pixel colors from camera rays, making these approaches fast at inference time without the need for multiple network passes to generate a pixel color. However, due to the complexity of the 4D light field signal, the light field network requires sufficient expressibility to be able to memorize the signal. As a result, Wang _et al_.  end up using as many as 88 network layers, which takes three seconds to render one 200 x 200 image on iPhone 13. In this regard, Cao _et al_.  introduce a novel network architecture that dramatically reduces R2L's computation through super-resolution. The deep networks are only evaluated on a low-resolution ray bundle and then upsampled to the full image resolution. This approach, termed MobileR2L, achieves real-time rendering on mobile phones. NeuLF  also proposes to directly regress pixel colors using a light slab ray representation but is unable to capture fine-level details due to lack of any sort of high-dimensional input encoding and is limited to frontal scenes. Another notable work, SIGNET , utilizes neural methods to compress a light field by using a ultra spherical input encoding to the light slab representation. However, SIGNET doesn't guarantee photorealistic reconstruction and hence deviates from task at hand. Throughout the paper, we will mainly compare our method to MobileR2L , which is currently the state-of-the-art method for real-time rendering on mobile devices and achieves the highest PSNR among existing methods.

It is important to note that training NeLF's requires densely sampled camera poses in the training images and may not generalize well if the training images are sparse, as NeLF's do not explicitly model geometry. While there have been works, such as those by Attal _et al_. , that propose a mixture of NeRF and local NeLF's, allowing learning from sparse inputs, we do not consider this to be a drawback since NeLF's focus on photo-realistic rendering rather than reconstructing the light field from sparse inputs, and they can leverage state-of-the-art reconstruction methods like NeRF to create dense training images. However, it is a drawback for prior NeLF's [36; 4] that they train extremely slowly, often taking more than two days to converge for a single scene. This is where our new method comes into play, as it offers improvements in terms of training efficiency and convergence speed.

## 3 Methodology

### Prerequisites

**4D Light Fields** or Lumigraphs are a representation of light fields that capture the radiance information along rays in empty space. They can be seen as a reduction of the higher-dimensional plenoptic functions. While plenoptic functions describe the amount of light (radiance) flowing in every direction through every point in space, which typically has five degrees of freedom, 4D light fields assume that the radiance is constant along the rays. Therefore, a 4D light field is a vector function that takes a ray as input (with four degrees of freedom) and outputs the corresponding radiance value. Specifically, assuming that the radiance \(\) is represented in the RGB space, a 4D light field is mathematical defined as a function, _i.e._:

\[:^{M}^{3},\] (1)

where \(\) is \(M\)-dimensional coordinates of the ray depending how it is parameterized.

Generating images from the 4D light field is a straightforward process. For each pixel on the image plane, we calculate the corresponding viewing ray \(\) that passes through the pixel, and the pixel value is obtained by evaluating the light field function \(()\). In this paper, our goal is to identify a suitable representation for \(()\) that minimizes the number of parameters required for learning and facilitates faster evaluation and training.

**MobileR2L.** We adopt the problem setup introduced by MobileR2L  and its predecessor R2L , where the light field \(()\) is modeled using neural networks. The training of the light field network is framed as distillation, leveraging a large dataset that includes both real images and images generated by a pre-trained NeRF. Both R2L and MobileR2L represent \(\) using stratified points, which involves concatenating the 3D positions of points along the ray through stratified sampling. In addition, the 3D positions are encoded using sinusoidal positional encoding . Due to the complexity of the light field, the network requires a high level of expressiveness to capture fine details in the target scene. This leads to the use of very deep networks, with over 88 layers in the case of R2L. While this allows for detailed rendering, it negatively impacts the rendering speed since the network needs to be evaluated for every pixel in the image.

To address this issue, MobileR2L proposes an alternative approach. Instead of directly using deep networks to generate high-resolution pixels, they employ deep networks to generate a low-resolution feature map, which is subsequently up-sampled to obtain high-resolution images using shallow super-resolution modules. This approach greatly reduces the computational requirements and enables real-time rendering on mobile devices. In our work, we adopt a similar architecture, with a specific focus on improving the efficiency of generating the low-resolution feature map.

### LightSpeed

We first describe the light-slab ray representation for both frontal and non-frontal scenes in Sec. 3.2.1. Next, we detail our grid representation for the light-slab in Sec. 3.2.2 and explain the procedure for synthesizing images from this grid representation in Sec. 3.3. Refer to Fig. 2 for a visual overview.

#### 3.2.1 Ray Parameterization

**Light Slab (two-plane representation).** Instead of utilizing stratified points or Plucker coordinates, we represent each directed light ray using the classic two-plane parameterization as an ordered pair of intersection points with two fixed planes. Formally,

\[=(x,y,u,v),\] (2)

where \((x,y)^{2}\) and \((u,v)^{2}\) are ray intersection points with fixed planes \(P_{1}\) and \(P_{2}\) in their respective coordinate systems. We refer to these four numbers as the ray coordinates in the 4D ray space. To accommodate unbounded scenes, we utilize normalized device coordinates (NDC) and select the planes \(P_{1}\) and \(P_{2}\) as the near and far planes (at infinity) defined in NDC.

Divided Light Slabs for Non-frontal Scenes.A single light slab is only suitable for modeling a frontal scene and cannot capture light rays that are parallel to the planes. To model non-frontal scenes, we employ a divide-and-conquer strategy by using a composition of multiple light slab representations to learn the full light field. We partition the light fields into subsets, and each subset is learned using a separate NeLF model. The partitions ensure sufficient overlap between sub-scenes, resulting in a continuous light field representation without additional losses while maintaining the frontal scene assumption. To perform view synthesis, we identify the scene subset of the viewing ray and query the corresponding NeLF to generate pixel values. Unlike Attal _et al_. , we do not perform alpha blending of multiple local light fields because our division is based on ray space rather than partitioning 3D space.

For _object-centric_\(360^{}\) scenes, we propose to partition the scene into \(5\) parts using surfaces of a near-isometric trapezoidal prism and approximate each sub-scene as frontal (as illustrated in Fig. 3). For _unbounded_\(360^{}\) scenes, we perform partitioning using k-means clustering based on camera orientation and position. We refer the reader to the supplementary material for more details on our choice of space partitioning.

#### 3.2.2 Feature Grids for Light Field Representation

Storing the 4D light-slab directly using a high-resolution grid is impractical in terms of storage and inefficient for learning due to the excessive number of parameters to optimize. The primary concern arises from the fact that the 4D grid size increases quartically with respect to resolutions. To address this, we suggest the following design choices to achieve a compact representation of the light-slab without exponentially increasing the parameter count.

Figure 2: **LightSpeed Model for Frontal Scenes. Taking a low-resolution ray bundle as input, our approach formulates rays in two-plane ray representation. This enables us to encode each ray using multi-scale feature grids, as shown. The encoded ray bundle is fed into a decoder network consisting of convolutions and super-resolution modules yielding the high-resolution image.**

**Lower Resolution Feature Grids.** Instead of storing grids at full resolution, we choose to utilize low-resolution feature grids to take advantage of the quartic reduction in storage achieved through resolution reduction. We anticipate that the decrease in resolution can be compensated by employing high-dimensional features. In our implementation, we have determined that feature grids of size \(128^{4}\) are suitable for synthesizing full HD images. Additionally, we adopt the approach from Instant-NGP  to incorporate multi-resolution grids, which enables an efficient representation of both global and local scene structures.

**Decompose 4D Grids into 2D Grids.** Taking inspiration from k-planes , we propose to decompose the 4D feature grid using \(=6\) number of 2D grids, with each 2D grid representing a sub-space of the 4D ray space. This results in a storage complexity of \((6N^{2})\), greatly reducing the storage required to deploy our grid-based approach to mobile devices.

### View Synthesis using Feature Grids

Similar to MobileR2L , LightSpeed takes two steps to render a high resolution image (see Fig. 2).

**Encoding Low-Resolution Ray Bundles.** The first step is to render a low-resolution (\(H_{L} W_{L}\)) feature map from the feature grids. This is accomplished by generating ray bundles at a reduced resolution, where each ray corresponds to a pixel in a downsampled image. We project each ray's 4D coordinates \(=(x,y,u,v)\) onto 6 2D feature grids \(_{xy},_{xu},_{xv},_{yu},_{ yv},_{uv}\) to obtain feature vectors from corresponding sub-spaces. The feature values undergo bilinear interpolation from the 2D grids, resulting in six interpolated \(F\)-dimensional features. These features are subsequently concatenated to form a \(6F\)-dimensional feature vector. As the feature grids are multi-resolutional with \(L\) levels, features \(g_{l}()^{6F}\) from different levels (indexed by \(l\)) are concatenated together to create a single feature \(g()^{6LF}\). Combining the features from all rays generates a low-resolution 2D feature map \(}^{H_{L} W_{L} 6LF}\), which is then processed further in the subsequent step.

**Decoding High-Resolution Image.** To mitigate the approximation introduced by decomposing 4D grids into 2D grids, the features \(g()\) undergo additional processing through a MLP. This is implemented by applying a series of \(1 1\) convolutional layers to the low-resolution feature map. Subsequently, the processed feature map is passed through a sequence of upsampling layers (similar to MobileR2L ) to generate a high-resolution image.

## 4 Experiments

**Datasets.** We benchmark our approach on the real-world forward-facing , the realistic synthetic \(360^{}\) datasets  and unbounded \(360^{}\) scenes . The forward-facing dataset consists of \(8\) real-world scenes captured using cellphones, with \(20\)-\(60\) images per scene and 1/8th of the images used for testing. The synthetic \(360^{}\) dataset has \(8\) scenes, each having \(100\) training views and \(200\) testing views. The unbounded \(360^{}\) dataset consists of \(5\) outdoor and \(4\) indoor scenes with a central object and a detailed background. Each scene has between \(100\) to \(300\) images, with \(1\) in \(8\) images used for testing. We use \(756 1008\) LLFF dataset images, \(800 800\) resolution for the \(360^{}\) scenes, and 1/4th of the original resolution for the unbounded \(360^{}\) scenes.

Figure 3: **Space Partitioning for Non-frontal scenes.** We partition _object-centric_\(360^{}\) scenes into 5 parts as shown. Each colored face of the trapezoidal prism corresponds to a partitioning plane. Each scene subset is subsequently learned as a separate NeLF

**Training Details.** We follow a similar training scheme as MobileR2L: train the LightSpeed model using pseudo-data mined from a pre-trained NeRF teacher. We specifically train MipNeRF teachers to sample \(10\)k pseudo-data points for the LLFF dataset. For synthetic and unbounded \(360^{}\) scenes, we mine \(30\)k samples per scene using Instant-NGP  teachers. Following this, we fine-tune the model on the original data. We optimize for the mean-squared error between generated and ground truth images. We refer the reader to the supplementary material for more training details.

We use \(63 84\) (\(12\) downsampled from the desired \(756 1008\) resolution) input ray bundles for the forward-facing scenes. For \(360^{}\) scenes, we use \(100 100\) (\(8\) downsampled from the desired \(800 800\) image resolution) ray bundles. For unbounded scenes, we use ray bundles \(12\) downsampled from the image resolution we use. We train our frontal LightSpeed models as well as each sub-scene model in non-frontal scenes for \(200\)k iterations.

**Baselines and Metrics.** We compare our method's performance on bounded scenes with MobileR2L, MobileNeRF and SNeRG. We evaluate our method for rendering quality using three metrics: PSNR, LPIPS, and SSIM. For unbounded scenes, we report the PSNR metric on 6 scenes and compare it with MobileNeRF  and NeRFMeshing . To further demonstrate the effectiveness of our approach, we compare our approach with others on two other criteria: (a) **On-device Rendering Speed**: We report and compare average inference times per rendered frame on various mobile chips, including Apple A15, Apple M1 Pro and Snapdragon SM8450 chips; and (b) **Efficient Training**: We compare the number of iterations LightSpeed and MobileR2L require to reach a target PSNR. We pick Lego scene from \(360^{}\) scenes and Fern from forward-facing scenes as representative scenes to compare. We also report the storage requirements of our method per frontal scene and compare it with baselines.

### Results and Analysis

**Rendering Quality.** As in Tab. 1, we obtain better results on all rendering fidelity metrics on the two bounded datasets. We also outperform MobileNeRF and NeRFMeshing on 4 out of 6 unbounded \(360^{}\) scenes. We refer the reader to Fig. 4 for a visual comparison of our approach with MobileR2L and NeRF. Our method has much better rendering quality, capturing fine-level details where MobileR2L, and in some cases, even the original NeRF model, fails. Note that we use Instant-NGP teachers for \(360^{}\) scenes, which have slightly inferior performance to MipNeRF teachers used by MobileR2L. This further shows the robustness of our approach to inferior NeRF teachers.

**Storage Cost.** We report storage requirements in Tab. 1. Our approach has a competitive on-device storage to the MobileR2L model. Specifically, we require a total of \(16.3\) MB of storage per frontal scene. The increase in storage is expected since we're using grids to encode our light field. We also report storage values for lighter LightSpeed networks in the ablation study (see Tab. 5), all of which have similar or better rendering quality than the full-sized MobileR2L network.

**Training Speed.** We benchmark the training times and the number of iterations required for LightSpeed and MobileR2L in Tab. 2 with a target PSNR of \(24\) for Fern scene and \(32\) for the Lego scene. Our approach demonstrates a training speed-up of \(2.5\) on both scenes. Since we are modeling \(360^{}\) scenes as a composition of \(5\) light fields, we can train them in parallel (which is not

Figure 4: **Qualitative Results on frontal and non-frontal scenes. Zoomed-in comparison between NeRF , MobileR2L  and our LightSpeed approach.**possible for MobileR2L), further trimming down the training time. Moreover, the training speedup reaches \( 4\) when networks are trained beyond the mentioned target PSNR (see Fig. 1).

**Inference Speed.** Tab. 3 shows our method's inference time as compared to MobileR2L and MobileNeRF. We maintain a comparable runtime as MobileR2L while having better rendering fidelity. Since on-device inference is crucial to our problem setting, we also report rendering times of a smaller 30-layered decoder network that has similar rendering quality as the MobileR2L model (see Tab. 5).

### Ablations

**Data Requirements.** We use \(10\)k samples as used by MobileR2L to train LightField models for frontal scenes. However, for non-frontal scenes, we resort to using \(30\)k pseudo-data samples per

    & \)} &  \\   & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) & Storage \(\) \\  NeRF  & 31.01 & 0.947 & 0.081 & 26.50 & 0.811 & 0.250 & - \\ NeRF-PyTorch & 30.92 & 0.991 & 0.045 & 26.26 & 0.965 & 0.153 & - \\  SNeRG  & 30.38 & 0.950 & 0.050 & 25.63 & 0.818 & 0.183 & 337.3 MB \\ MobileNeRF  & 30.90 & 0.947 & 0.062 & 25.91 & 0.825 & 0.183 & 201.5 MB \\ MobileR2L  & 31.34 & 0.993 & 0.051 & 26.15 & 0.966 & 0.187 & **8.2 MB** \\  LightSpeed (Ours) & **32.23** & **0.994** & **0.038** & **26.50** & **0.968** & **0.173** & 16.3 MB \\  Our Teacher & 32.96 & - & - & 26.85 & 0.827 & 0.226 & - \\    & \)} \\   & Bicycle & Garden & Stump & Bonsai & Counter & Kitchen \\  MobileNeRF  & 21.70 & 23.54 & **23.95** & - & - & - \\ NeRFMeshing  & 21.15 & 22.91 & 22.66 & 25.58 & 20.00 & 23.59 \\  LightSpeed (Ours) & **22.51** & **24.54** & 22.22 & **28.24** & 25.46 & **27.82** \\  Instant-NGP (Our teacher)  & 21.70 & 23.40 & 23.20 & 27.4 & **25.80** & 27.50 \\   

Table 1: **Quantitative Comparison** on Forward-facing, Synthetic \(360^{}\) and Unbounded \(360^{}\) Datasets. HighSpeed achieves the best rendering quality with competitive storage. We use an out-of-the-box Instant-NGP  implementation  (as teachers for \(360^{}\) scenes) which dose not report SSIM and LPIPS values. We omit storage for NeRF-based methods since they are not comparable.

    &  & \): Lego} \\  Method & Duration \(\) & Iterations \(\) & Duration \(\) & Iterations \(\) \\  MobileR2L & 12.5 hours & 70k & 192 hours & 860k \\ LightSpeed & **4 hours** & **27k** & **75 hours** & **425k** \\ LightSpeed (Parallelized) & - & - & **15 hours** & **85k** \\   

Table 2: **Training Time** for Lego and Fern scenes with 32 and 24 target PSNRs. LightSpeed trains significantly faster than MobileR2L. It achieves even greater speedup when trained in parallel for \(360^{}\) scenes (parallel training is not applicable for frontal scenes).

    &  & \)} \\   & MobileNeRF & MobileR2L & Ours & Ours (30-L) & MobileNeRF & MobileR2L & Ours & Ours (30-L) \\  Apple A13 (Low-end) & - & 40.23 & 41.06 & 32.29 & - & 65.54 & 66.10 & 53.89 \\ Apple A15(Low-end) & 27.15 \(\) & 18.04 & 19.05 & 15.28 & 17.54 & 26.21 & 27.10 & 20.15 \\ Apple A15(High-end) & 20.98 \(\) & 16.48 & 17.68 & 15.03 & 16.67 & 22.65 & 26.47 & 20.35 \\ Apple M1 Pro & - & 17.65 & 17.08 & 13.86 & - & 27.37 & 27.14 & 20.13 \\ Snapdragon SM8450 & - & 39.14 & 45.65 & 32.89 & - & 40.86 & 41.26 & 33.87 \\   

Table 3: **Rendering Latency Analysis.** LightSpeed maintains a competitive rendering latency (ms) to prior works. MobileNeRF is not able to render \(2\) out of \(8\) real-world scenes (\(\) in table) due to memory constraints, and no numbers are reported for A13, M1 Pro and Snapdragon chips.

scene. Dividing \(10\)k samples amongst \(5\) sub-scenes assigns too few samplers per sub-scene, which is detrimental to grid learning. We experimentally validate data requirements by comparing MobileR2L and LightSpeed trained for different amounts of pseudo-data. We train one \(400 400\) sub-scene from the Lego scene for 200k iterations with 1/5th of \(10\)k and \(30\)k samples, _i.e._, \(2\)k and \(6\)k samples. Tab. 4 exhibits significantly decreased rendering quality for the LightSpeed network as compared to MobileR2L when provided with less pseudo-data.

**Decoder Network Size.** We further analyze the trade-off between inference speed and rendering quality of our method and MobileR2L. To this end, we experiment with decoders of different depths and widths. Each network is trained for \(200\)k iterations and benchmarked on an iPhone 13. Tab. 5 shows that a \(30\)-layered LightSpeed model has a better inference speed and rendering quality as compared to the \(60\)-layered MobileR2L model. This \(30\)-layered variant further occupies less storage as compared to its full-sized counterpart. Furthermore, lighter LightSpeed networks obtain a comparable performance as the \(60\)-layered MobileR2L. Note that reducing the network capacity of MobileR2L results in significant drops in performance. This means that we can get the same rendering quality as MobileR2L with considerably reduced on-device resources, paving the way for a much better trade-off between rendering quality and on-device inference speed.

**Ray-Space Grid Encoding.** We provide an ablation in Tab. 6 below on how the proposed ray-space grid encoder helps as compared to just using the light-slab representation with a traditional frequency encoder. We compare different LightSpeed configurations with grid-encoder and frequency encoders. Networks are trained for 200k iterations on a full-resolution 800\(\)800 Lego sub-scene from Synthetic

    &  &  \\  Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  MobileR2L & 30.19 & 0.9894 & 0.0354 & 30.56 & 0.9898 & 0.0336 \\ LightSpeed (Ours) & 30.44 & 0.9899 & 0.0299 & **31.2** & **0.9906** & **0.0284** \\   

Table 4: **Pseudo-Data Requirement for Non-Frontal Scenes.** We analyze the importance of mining more pseudo-data for non-frontal scenes. Using 1/5th of \(10\)k and \(30\)k sampled pseudo-data points, we find more pseudo-data is crucial for the boosted performance of the LightSpeed model.

Figure 5: **Test PSNR v/s Training Iterations.** We compare test set PSNR obtained by LightSpeed (Grid)(ours), LightSpeed (frequency encoded), and Plücker-based neural light field as the training progresses for 3 different network configurations.

   Method & PSNR \(\) & Latency \(\) & Storage \(\) & FLOPs \(\) \\ 
15-L W-256 MobileR2L & 27.69 & 14.54 ms & 2.4 MB & 12626M \\
30-L W-128 MobileR2L & 27.54 & 14.47 ms & 1.4 MB & 8950M \\
30-L W-256 MobileR2L & 29.21 & 18.59 ms & 4.5 MB & 23112M \\
60-L W-256 MobileR2L & 30.34 & 22.65 ms & 8.2 MB & 42772M \\ 
15-L W-256 LightSpeed & 30.37 & 14.94 ms & 10.5 MB & 12833M \\
30-L W-128 LightSpeed & 30.13 & 14.86 ms & 9.5 MB & 9065M \\
30-L W-256 LightSpeed & 31.70 & 20.35 ms & 12.6 MB & 23319M \\
60-L W-256 LightSpeed & 32.34 & 26.47 ms & 16.3 MB & 42980M \\   

Table 5: **Decoder Network Size.** Our approach maintains a much better tradeoff between inference speeds v/s rendering quality, with our smallest network achieving comparable quality to the MobileR2L. Benchmarking done on an iPhone 13. L is network depth, and W is network width.

\(360^{}\) dataset. Further, we show the training dynamics of all the trained variants in Fig. 5 (red and green plots). As claimed, our approach offers better visual fidelity and training dynamics (iterations to reach a target PSNR) for both computationally cheaper small networks as well as full sized networks.

Comparison with Plucker Representation.Given the challenges of discretizing Plucker representation, we compare between using positionally encoded Plucker coordinates and our grid-based light-slab approach in Tab. 7 below for different network sizes to demonstrate the effectiveness of our approach. We train all models for 200k iterations on one 800\(\)800 Lego sub-scene. We also share training curves for the variants in question in Fig. 5 (red and blue curves). As claimed, our integrated approach performs better in terms of training time and test-time visual fidelity for large and small models (having less computational costs) alike whereas the Plucker-based network shows a sharp decline in visual fidelity and increased training times to reach a target test PSNR as network size is reduced.

## 5 Discussion and Conclusion

In this paper, we propose an efficient method, LightSpeed, to learn neural light fields using the classic two-plane ray representation. Our approach leverages grid-based light field representations to accelerate light field training and boost rendering quality. We demonstrate the advantages of our approach not only on frontal scenes but also on non-frontal scenes by following a divide-and-conquer strategy and modeling them as frontal sub-scenes. Our method achieves SOTA rendering quality amongst prior works at same time providing a significantly better trade-off between rendering fidelity and latency, paving the way for real-time view synthesis on resource-constrained mobile devices.

**Limitations.** While LightSpeed excels at efficiently modeling frontal and \(360^{}\) light fields, it currently lacks the capability to handle free camera trajectories. The current implementation does not support refocusing, anti-aliasing, and is limited to static scenes without the ability to model deformable objects such as humans. We plan to explore these directions in future work.

**Broader Impact.** Focused on finding efficiencies in novel view synthesis, our study could significantly reduce costs, enabling wider access to this technology. However, potential misuse, like unsolicited impersonations, must be mitigated.

   Method & PSNR \(\) \\ 
15-L W-256 LS (PE) & 28.84 \\
30-L W-256 LS (PE) & 30.63 \\
60-L W-256 LS (PE) & 32.16 \\ 
15-L W-256 LS (Grid) & 30.37 \\
30-L W-256 LS (Grid) & 31.70 \\
60-L W-256 LS (Grid) & 32.34 \\   

Table 6: **Effect of using a Ray-Space Grid Encoder.** We demonstrate the effect of using a grid-based LightSpeed by comparing with a frequency encoded variant (no grid). L is network depth, and W is network width.

   Method & PSNR \(\) \\ 
15-L W-256 Plücker & 28.65 \\
30-L W-256 Plücker & 30.84 \\
60-L W-256 Plücker & 32.14 \\ 
15-L W-256 LS & 30.37 \\
30-L W-256 LS & 31.70 \\
60-L W-256 LS & 32.34 \\   

Table 7: **Light-Slab Grid Representation vs. Plücker Coordinates.** We compare the light-slab based LightSpeed (LS) with a positionally encoded variant of the Plücker ray representation. L is network depth, and W is network width.