# UQ for Credit Risk Management: A deep evidence regression approach

**Anonymous Author(s)**

Affiliation

Address

email

Machine Learning has invariantly found its way into various Credit Risk applications. Due to the intrinsic nature of Credit Risk, quantifying the uncertainty of the predicted risk metrics is essential, and applying uncertainty-aware deep learning models to credit risk settings can be very helpful. In this work, we have explored the application of a scalable UQ-aware deep learning technique, Deep Evidence Regression and applied it to predicting Loss Given Default. We contribute to the literature by extending the Deep Evidence Regression methodology to learning target variables generated by a Weibull process and provide the relevant learning framework. We demonstrate the application of our approach to both simulated and real-world peer to peer lending data.

## 1 Introduction

### Credit Risk Management

Credit risk management is assessing and managing the potential losses that may arise from the failure of borrowers or counterparties to fulfil their financial obligations. In other words, it identifies, measures, and mitigates the risks associated with lending money or extending credit to individuals, businesses, or other organizations.

Credit risk's anticipated loss (EL) comprises three components: Probability of Default (PD), Loss Given Default (LGD), and Exposure at Default (EAD). PD is the likelihood that a borrower will fail to fulfill their financial commitments in the future. LGD refers to the proportion of the outstanding amount that is lost in the event of default. Lastly, EAD refers to the outstanding amount at the time of default.

LGD prediction is important as accurate prediction of LGD not only supports a healthier and riskless allocation of capital, but is also vital for pricing the security properly.  & . There is a large body of literature using advanced statistical and machine learning methods for prediction of LGD . However the machine learning literature on LGD has yet to address an essential aspect, which is the uncertainty surrounding the estimates and predictions..

UQ techniques like Bayesian Neural Network, Monte Carlo Dropout and ensemble methods as outlined in  present a natural first step towards quantifying uncertainty. However, almost all these methods are computationally and memory intensive, and require sampling on test data after fitting the network, making them difficult to adapt for complex neural network architectures that involve a large number of parameters.

### Deep Evidence Regression

The primary inspiration of this work is taken from the work done by Amini et al in . The paper develops a unique approach, **Deep Evidence Regression** as a scalable and accurate UQ aware deep learning technique for regression problems. This approach predicts the types of uncertainty directly within the neural network structure, by learning prior distributions over the parameters of the target distribution, referred to as evidential distributions. Thus this method is able to quantify uncertainty without extra computations after training, since the estimated parameters of the evidential distribution can be plugged into analytical formulas for epistemic and aleatoric uncertainty, and target predictions.

The setup of the problem is to assume that the observations from the target variable, \(y_{i}\) are drawn i.i.d. from a **Normal distribution** with unknown mean and variance parameters \(=,^{2}\). With this we can write the log likelihood of the observation as:

\[Lik(,^{2})=log(p(y_{i}|,^{2})=-(2^{ 2})--)^{2}}{2^{2}}\]

Learning \(\) that maximises the above likelihood successfully models the uncertainty in the data, also known as the aleatoric uncertainty. However, this model is oblivious to its predictive epistemic uncertainty. . Epistemic uncertainty, is incorporated by placing higher-order prior distributions over the parameters \(\). In particular a Gaussian prior is placed on the unknown mean and an Inverse-Gamma prior on the unknown variance.

\[(,^{2}^{-1})^{2}^{-1} (,)\]

Following from above the posterior \(p(,^{2}|,,,)\) can be approximated as \(p(|,)*p(^{2}|,)\). Hence:

\[p(,^{2}|,,,)=}{ ()}}(1/^{2})^{+1}- }{2^{2}}\]

Amini et al  thus find the likelihood of target variable given evidential parameters, as:

\[p(y_{i}|,,,)=_{}p(y_{i}|)p(|, ,,)\]

where \(=\{,^{2}\}\). Then a Neural Network is trained t infer, the parameters \(m=\{,,,\}\), of this higher-order, evidential distribution.

### Weibull distribution

The Weibull distribution is a continuous probability distribution commonly used in reliability analysis to model the failure time of a system or component. The probability density function (PDF) of the Weibull distribution is given by:

\[f(x;,k)=()^{k -1}e^{-(x/)^{k}}&x 0,\\ 0&x<0,\] (1)

where \(>0\) is the scale parameter and \(k>0\) is the shape parameter. The scale parameter determines the location of the distribution, while the shape parameter controls the rate at which the failure rate changes over time. There is a body of literature that explores the application of the weibull distribution to various credit risk applications. .

The work by  assumes a normal distribution on LGD values. While this assumption might be true in a lot of settings, however it does not follow in the context of Loss Given Default. While normal distribution is symmetric and has a support over entire real line, however the LGD values are restricted to a range of \(\) and might not necessarily be symmetric.

Hence in the section below we provide a novel theoretical framework to learn target variables which follow Weibull distribution. We provide the following theoretical results, in the setting of target variables following a Weibull dataset.

* Log Likelihood
* Mean Prediction
* Prediction Uncertainty

We also provide results testing our approach on both simulated and real world dataset.

## 2 Deep Evidence Regression for Weibull Data

### Problem setup

We consider the problem where the observed targets, \(y_{i}\), are drawn iid from a Weibull distribution, with a known shape or rate parameter k and an unknown scale \(\). Although ideally we would want to keep both the parameters unknown, however with both \(\) and \(k\) there are no priors with which likelihood can be computed analytically . Hence we have decided to simplify the problem setup by assuming known shape \(k\).

\[y_{i} Weibull(k,)\] (2) \[k^{+},^{+}\] (3) \[y_{i}\] (4) \[ p(y_{i};,k)=( }{})^{k-1}e^{-(y_{i}/)^{k}},&y_{i} 0\\ 0,&\] (5)

For the above setting we want to place priors on the unknown parameter, \(\), such that we are able to get solve for the likelihood of \(y_{i}\) given the parameters of the prior distribution. Hence similar to work in  and , we define the following prior.

\[=^{k}\] (6)

Hence the pdf of \(y_{i}\) becomes: (7)

\[p(y_{i}|,k)=}^{k-1}^{k}/)}\] (8)

And we place a Inverse Gamma Prior on \(\) (9)

\[(,)(>2)\] (10)

Hence pdf of \(\) is (11)

\[p(|,)=}{()}})}\] (12)

### Learning Log-Likelihood

Hence we can define likelihood of \(y_{i}\) given the higher order evidential parameters \(,\) can be defined as :

[MISSING_PAGE_FAIL:4]

#### 2.3.2 Prediction Uncertainty

We quantify the total uncertainty as \(Var(Z)\) with defined as above, i.e. \(Z=E[y_{i}]\)

\[Var(Z|,)=Var()*^{2}(1+)\] (30)

\[=(E[^{2}]-E[])*^{2}(1+)\] (31)

With \(E()\) defined as in 26, we only need \(E(^{2})\)

\[E[^{2}]=_{}^{2}p()d\] (33)

Similar to approach outlined in 2.3.1, we get:

\[E[^{2}|,]=()}{ ()}\] (35)

Hence we can write

\[Var(Z)=^{2}(1+)*()}{()}-()}{()}^{2}\] (36)

or

\[Var(Z)}{()^{2}}() ()-^{2}()\]

### Regularisation Cost

In this section, we outline the process of regularization during training by implementing a regularisation penalty, which involves assigning a high uncertainty cost. The purpose of this penalty is to inflate the uncertainty associated with incorrect predictions, thereby improving the overall efficacy of the model. As followed in , the intuition behind the regularisation cost is to increases the variance of prediction in cases where it's unsure. This utility of this approach has been demonstrated in classification setting by  and in regression setting by .

Hence we define the Regularisation cost for the i'th observation as

\[L_{i}^{reg}=|error_{i}|*(}{_{i}})\]

where \(error_{i}=y_{i}-Z_{i}\) and Z is defined in 27.

**Note** The regularization cost mentioned earlier has been determined as the most effective through experimentation. However, in order to precisely determine the coefficients of \(\) and \(\) in the regularization cost, further theoretical analysis is required. By conducting a deeper theoretical investigation, we can establish the optimal values for these coefficients, which will enhance the regularization process and improve the overall performance of the model.

### Neural Network training schematic

The learning process is then set up with a deep neural network with two output neurons to predict the parameters of the prior/evidential distribution, \(\) and \(\). The neural network is trained using the cost function:

\[L_{i}^{NN}=-L_{i}^{lik}+c*L_{i}^{reg}\]

where c is the hyper-parameter governing the strength of regularisation.

## 3 Results and Experiments

In this section, we present the results of experiments conducted on both simulated and real data. The aim was to evaluate the performance of our proposed method and compare it with existing methods. The simulated data was generated based on example data given in , while the real dataset was obtained from a peer to peer lending company.

### Simulated Data

Here generate a target variable following a Weibull distribution. The target variable is generated as:

\[y_{i}=x_{i}^{2}+, Weibull(k=1.6,)\]

The train set is comprised of uniformly spaced \(x[-4,4]\) while test set is \(x[-5,5]\). The value of \(\) is varied between \([0.2,0.4]\) to test the effect of noise magnitude on the approach.

Next we fit both the original deep evidence regression and proposed weibull version of deep evidence regression. Since our approach assumes known \(k\), k is estimated from the training set.

Comparing the two versions qualitatively, we observe that the original model's predictions exhibit consistent uncertainty regardless of whether the data is within or outside the distribution. In contrast, the proposed version demonstrates improved capability in capturing prediction uncertainty. The proposed model's prediction interval gradually expands beyond the training data range \(|x|>4\), indicating its ability to account for uncertainty in Out-of-distribution data. On comparison, for the benchmark version of the model displays a slightly narrower prediction interval at the edges of the training window, contrary to expectations of interval widening.

Quantitatively to assess the performance of our proposed method, we compare it to the benchmark model by evaluating key metrics such as mean squared error (MSE) and negative log-likelihood (NLL).

We can see that the proposed version exhibits significantly lower negative log likelihood compared to the original Deep Evidence Regression model. This indicates that the proposed model better

Figure 1: NN training schematic

Figure 2: Synthetic data generated for varying \(\)aligns with the actual distribution of the data, capturing the uncertainty more accurately. However, despite this improvement, the original model outperforms in capturing the underlying signal beyond the training window, as evidenced by its lower mean squared error (MSE) values.

### Real Data: Loss Given Default for peer to peer lending

In this subsection, we showcase the utility of our proposed learning approach by extending it to the intricate and complex domain of credit risk management in the context of peer to peer lending. Peer-to-peer lending, which is an emerging form of credit aimed at funding borrowers from small lenders and individuals seeking to earn interest on their investments. Through an online platform, borrowers can apply for personal loans, which are typically unsecured and funded by one or more peer investors. The P2P lender acts as a facilitator of the lending process and provides the platform, rather than acting as an actual lender.

Credit risk management is crucial for peer-to-peer lending data as it helps mitigate the potential default risks associated with borrowers, ensuring a healthier loan portfolio and reducing financial

    &  &  \\   & benchmark & proposed & benchmark & proposed \\  _0.2_ & **0.099303** & **0.571519** & 70.64365 & **7.416122** \\ _0.25_ & **0.119299** & **0.504875** & 41.71667 & **6.667958** \\ _0.3_ & **0.142722** & 3.871369 & 36.16713 & **6.202275** \\ _0.35_ & **0.143117** & 3.202328 & 57.02918 & **5.773156** \\ _0.4_ & **0.172697** & 8.981477 & 42.53032 & **5.471559** \\   

Table 1: Deep Evidence regression original vs proposed results on simulated data for varying \(\). We see MSE (or mean squared error) is similar for both, while NLL (or Negative log likelihood) values are much better captured by proposed version.

Figure 3: Deep evidence regression (left) vs Weibull evidence Regression (right). We see that uncertainty is much better captured by proposed version.

losses. By effectively analyzing and managing credit risk, P2P lending platforms can maintain investor confidence, attract more participants, and sustain the long-term viability of the lending ecosystem.

The dataset under consideration pertains to peer to peer mortgage lending data during the period of 2007 to 2014 sourced from Kaggle . However, the data does not include the loss given default values. Instead, the recovery rate has been used as a proxy, which is calculated as the ratio of recoveries made to the origination amount. The dataset contains approximately 46 variables denoted as 'x,' which include features such as the time since the loan was issued, debt-to-income ratio (DTI), joint applicant status, and delinquency status, among others. In total, the dataset comprises around 23,000 rows.

As described in the approach the shape parameter was found as \(k=1.254\) by fitting a Weibull distribution on the train dataset. Also given the sensitivity of both the approaches to regression cost, hyper parameter optimisation was done to arrive at the best regularisation cost. After arriving at the best regularisation cost 10 trials of neural network training were conducted with this best regularization cost for benchmark and proposed model separately.

Similar to the synthetic case, we see that the proposed model demonstrates superior performance compared to the benchmark model in terms of mean squared error (MSE) and negative log likelihood. Additionally, it exhibits the ability to generate more accurate prediction intervals. The difference in performance between the benchmark and proposed models is even more pronounced in this case compared to the simulated data, and the benchmark fails even to retrieve the underlying signal, let alone the prediction uncertainty. To reinforce this, the benchmark model was also run with 0 regularisation cost and it was found to not improve the MSE. This behaviour outlines the difficulty of tuning regularisation parameter for benchmark model. It is also worth noting that the benchmark model also predicts \(\) as uncertainty for a significant number of observations.

## 4 Related Work

This work is primarily inspired by the work done by Amini et al , which proposes Deep Evidentail Learning approach. Maximillian et al  have used the same approach and shown it's utility to Loss given default for bonds from Moody's recovery database. Additionally there's a huge body of

    &  &  \\   & benchmark & proposed & benchmark & proposed \\ _test_ & 84.333 \(\) 0.352 & **40.498 \(\) 4.745** & 2.757 \(\) 0.012 & **2.311 \(\) 0.024** \\ _train_ & 84.320 \(\) 0.216 & **39.909 \(\) 4.911** & 2.766 \(\) 0.009 & **2.314 \(\) 0.0228** \\   

Table 2: Results for original vs proposed model for recovery rate. proposed version does not only has both lower MSE and NLL

Figure 4: Distribution and Weibull fit of the recovery rate (left). Histogram of recovery rate for train/test split (right). It appears that Weibull distribution might not be a good fit to this data.

prior work on uncertainty estimation  and the utilization of neural networks for modeling probability distributions. In another line of work, Bayesian deep learning utilizes priors on network weights estimated with variational inference . There also exists alternative techniques to estimate predictive variance like as dropout and ensembling which require sampling. 

## 5 Conclusion, limitations and future work

We propose an improvement over Deep Evidence Regression, specifically targeted to usecases where the target might follow weibull distribution. We then test the proposed method both on simulated and real world dataset in the context of Credit risk management. The proposed model exhibits enhanced suitability for applications in which the target variable originates from a weibull distribution, better capturing the uncertainty characteristics of such data. Although we have specifically tested the model in the credit risk domain, this method can be applicable to wide variety of safety critical regression tasks where the target variable follows a weibull distribution. The proposed approach thus serves as a valuable tool for capturing and quantifying uncertainty in cases characterized by weibull distributions, thereby enhancing the trustworthiness and explainability of model predictions, ultimately leading to improved confidence in the modeling process and the corresponding decision making.

However, we are not sure if the proposed approach would generalise to other distributions apart from Weibull. Additionally, the proposed model has only two outputs, which could limit its flexibility when compared to the benchmark model, which had four outputs from the neural network. In consequence the proposed model requires a deeper network architecture compared to the benchmark model. Furthermore we find that both the models exhibit a high sensitivity to regularization cost, which means that changes in the regularization coefficient can significantly impact the model's performance. The cylical learning rate as outlined in the , which proposes varying the learning rate between reasonable boundary values might be of help to mitigate this issue. Overall, these points suggest that both models have their strengths and weaknesses, and selecting the most appropriate model depends on the specific task requirements and considerations.

Considering the wide application of beta distributions in the credit risk domain, there may also be value in further extending the proposed technique to target variables characterized by beta distributions, as it has the potential to provide valuable insights and improved modeling capabilities in the context of credit risk management.

Figure 5: Predicted UQ for Benchmark Regression (left) vs proposed model(right). Again we see that the updated model is much better able to capture the UQ. With Uncertainty increasing after recovery rate increases beyond 40, which is a less dense region and has much fewer observations 4