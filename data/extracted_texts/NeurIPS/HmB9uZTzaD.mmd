# Check lemma for Dafny.

[MISSING_PAGE_FAIL:1]

algorithm is described in Figure 1 and Figure 2 and the details are presented in Section 2. VerMCTS creates a search tree with progressive widening so it is capable of handling large action spaces defined by lines of code. Within this search tree both expansion and evaluation are guided by the verifier which acts as a computationally cheap (relative to the LLM) upper bound on the value function in the code synthesis MDP, as we show in Section 2.

To evaluate VerMCTS we introduce a suite of 15 challenge problems (9 in Dafny and 6 in Coq). This suite probes essential skills needed for general verified programming like constructing algebraic data types, defining functions, and writing inductive proofs.

On this suite of problems we compare VerMCTS with several baselines including repeated sampling of full programs from the base model, an advanced prompting technique that uses access to the error messages generated by the verifier called Reflexion , and a traditional version of MCTS. We quantify performance in terms of pass@\(T\), the pass rate within a budget of \(T\) tokens. VerMCTS outperforms the baselines substantially, leading to a 30% absolute average performance improvement over repeated sampling from the base model. Note this repeated sampling is a strong baseline, similar to a pass@\(k\) evaluation often used as a skyline in program generation. Moreover, for several problems VerMCTS solves problems that are not solved at all by other methods within the given budget.

## 2 Method: VerMCTS

Our main contribution is to define a search algorithm inspired by MCTS that leverages a verifier and LLM to search for verified programs. We call this method _VerMCTS_. In this section, we first present the Markov Decision Process that we consider as the environment for verified program synthesis and then present VerMCTS in detail. VerMCTS is a variant of traditional MCTS that incorporates the LLM as a prior to generate candidates and the verifier as a heuristic to evaluate partial programs.

### MDP for verified program synthesis

We formulate our multi-step verified synthesis problem as a Markov Decision Process (MDP) \((,,T,r,H)\) defined by the LLM and the verifier. Here, \(\) refers to the state space, \(A\) refers to the action space, \(T\) refers to the (deterministic) transition dynamics of the environment, \(r:\) refers to the reward function, and \(H\) is the finite horizon (i.e. a limit on the number of transitions). Defining the MDP just consists of defining these four objects. The state, action, transition dynamics, and reward are defined as follows:

Figure 1: Overview of VerMCTS. The search tree is visualized with “widen” nodes marked with \(w\). (a) A leaf node is selected as in standard MCTS. (b) The selected node is evaluated and maybe expanded. If the selected node is a widen node, then it’s parent is selected and maybe expanded (i.e. made wider). See Figure 2 for a detailed description. (c) Once we have a value from the evaluate and maybe expand algorithm, we backpropagate that value up the tree. This figure illustrates the special case where we observed a failure, so no node is added and the score is -1.

Figure 2: Evaluate and maybe expand. Given a prefix, we query the LLM for expansions until the verifier is able to return a score. If that score is a failure, we do not add the node to the tree, but update the parent with a value of -1. If the score is pass, then we check if the program is complete. If incomplete, we add the expansion to the tree with a score of 1. If complete, we have found a successful program to return.

* Each state \(s\) is a string consisting of the initial user prompt and a partial program.
* Actions \(a\) are strings that represent a unit of a program. In Dafny each line is an action. In Coq each "command" (ending with a dot '.') is an action. We also limit the number of tokens in an action.
* The transition dynamics are just defined by string concatentation: \(T(s,a)=s+a\).
* The reward function \(r\) is defined by the verifier for a given verified programming language and is only defined on complete programs. This terminal reward is 1 if the complete program is accepted and -1 if it is rejected. The reward is 0 for incomplete programs.

With this simple MDP in place, we can define our search algorithm for finding verified programs.

### VerMCTS

Given this MDP with finite actions and deterministic dynamics, it would be possible to run standard MCTS to learn a stochastic policy, but the action space is much too large for this to be practical. Instead, we build a search algorithm inspired by MCTS that can leverage the LLM as a prior for program synthesis and the verifier to evaluate partial programs. Both components are key for a successful search in this large space.

Standard MCTS consists of four steps: select, expand, evaluate, and backpropagate. Our algorithm leaves the select and backpropagate steps essentially unchanged. We modify and combine the expand and evaluate steps to leverage the power of the LLM and the verifier in tandem. Our full algorithm is illustrated in Figure 1. In this section we first discuss progressive widening and then go through each step of VerMCTS in turn.

Progressive widening.To allow for potentially infinite width while still efficiently conducting deep searches, we adapt an idea from classical work on MCTS to progressivly widen nodes in the tree . In that work, the number of children available at a given node scales explicitly with the number of visits. In our setting since adding a child node requires an expensive call to the LLM, we instead opt to add a "widen" child to each node that is assigned 0 value and can be selected via the selection procedure described below. This allows the scoring mechanism to prioritize when to expand a node by essentially setting a prior that unexplored branches have 0 value. When the widen node \(w\) with parent \(s\) is selected, instead of adding a child to \(w\), we add a child to \(s\) (i.e. add a sibling to \(w\)). In this way, the tree can grow wider over the search process.

Selection: priors and UCT.We use a standard MCTS selection step, but we set a prior for the UCT (upper confidence bound for trees) bonus as in PUCT . We choose to let the prior \(p=1.0\) for standard nodes and let \(p=p_{widen}<1.0\) for widen nodes be a hyperparameter that we tune. This basic heuristic gives the model a preference to select the standard nodes which encourages deeper search trees while still allowing for potentially infinite width if needed. With this choice, the score of a node \(s\) is:

\[(s)=p_{s} c_{}}}{N_{s}}}+^{N}v_{i}}{N_{s}} \]

where \(p_{s}\) is the prior at this node, \(c_{}\) is a global exploration coefficient, \(N_{}\) is the number of visits at the parent node, \(N_{s}\) is the number of visits at this node, and \(v_{i}\) is the estimated value at the \(i\)th visit to \(s\). Note that this selection procedure has two hyperparameters: \(c_{}\) and \(p_{widen}\) that encourage selecting more rarely visited nodes and widen nodes respectively.

Combining expansion and evaluation.Traditionally, MCTS will first expand a node into children and evaluate it either by simulated rollouts (Chaslot et al., 2008; Zhang et al., 2023a) or a learned value function (Silver et al., 2016). Neither of these methods is a good fit for our problem because generating rollouts requires many expensive calls to the LLM and learning a value requires large amounts of training data. Moreover, both methods give noisy signal, but in our setting we have access to the ground truth verifier.

Beyond being noiseless, the verifier has one more important property: if a partial program fails the verifier, no subsequent completion can yield success. So, we want to make sure that we never add to the tree any expansion that is a guaranteed failure. Doing this require explicitly linking expansion to evaluation where we evaluate the node and _maybe_ expand it, as formalized in Algorithm 1.

In addition to only adding nodes with potential to the tree, we want to leverage the verifier to cheaply evaluate partial programs without extra calls to the LLM. Explicitly, from a node containing the string \(s\) we continue to extend \(a\) with the LLM until the verifier is able to return a valid score. At this point, we can return the estimated value \(v(s)\) of \(s\) as follows:

\[v(s)=(s+a)=+1&\\ -1& \]

If \(v(s)=+1\), we also add \(s+a\) as a child in the tree, while if \(v(s)=-1\), we do not add \(s+a\) since it is a verified failure. Appendix H gives explicit examples of scoring partial programs.

Backpropagation.The last step of an iteration of MCTS is to backpropagate the observed value from leaf back up to root. We do this in the standard way so that signal is propagated up the tree. The algorithm terminates when it finds a complete solution that verifies or when it exceeds some token limit or time limit.

Appendix A presents theory that VerMCTS optimizes an upper bound on the value function.

## 3 Results

A full description of the problem suite can be found in Appendix B and experimental methods in Appendix C respectively. Here we present the main results.

We run VerMCTS and our three baselines across our full suite of problems. The aggregate results are illustrated in Figure 3. In both programming languages VerMCTS convincingly outperforms the baselines. Generally, MCTS rollout is second best, followed by whole sampling and then Reflexion. As previewed in the introduction, we see about a 30% absolute improvement in pass@5000 for VerMCTS relative to whole sampling. Note that Coq is substantially more challenging since the verifier is less automated.

Examining the performance of the baselines more closely, we see that MCTS rollout does outperform whole sampling, even though the verifier is not used to guide the search at intermediate steps. But, using the verifier in VerMCTS provides even better performance. Looking at Reflexion, we see that performance is poor on these tasks. This could be due to many reasons including: (1) the base model is not good at responding to errors in low resource languages like Dafny and Coq, (2) the base model does not do well integrating the long contexts created by the Reflexion prompts, and (3) Reflexion does not make it as easy to backtrack.

Due to space constraints, extended results are in Appendix D, extended related work in Appendix E, and further discussion in Appendix F.

Figure 3: Average results for pass@T vs. T (the number of tokens) for various baseline methods on our suite of programming problems in Dafny and Coq.