# Inference via Interpolation:

Contrastive Representations Provably Enable Planning and Inference

 Benjamin Eysenbach

Princeton University

eysenbach@princeton.edu

&Vivek Myers

UC Berkeley

vmyers@berkeley.edu

&Ruslan Salakhutdinov

Carnegie Mellon University

rsalakhu@cs.cmu.edu

&Sergey Levine

UC Berkeley

svelvine@eecs.berkeley.edu

Equal contribution.

###### Abstract

Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical simulations on tasks up to 46-dimensions.1

## 1 Introduction

Probabilistic modeling of time-series data has applications ranging from robotic control  to material science , from cell biology  to astrophysics . These applications are often concerned with two questions: _predicting_ future states (e.g., what will this cell look like in an hour), and _inferring_ trajectories between two given states. However, answering these questions often requires reasoning over high-dimensional data, which can be challenging as most tools in the standard probabilistic toolkit require generation. Might it be possible to use discriminative methods (e.g., contrastive learning) to perform such inferences?

Many prior works aim to learn representations that are easy to predict while retaining salient bits of information. For time-series data, we want the representation to remain a sufficient statistic for distributions related to time -- for example, they should retain bits required to predict future states (or representations thereof). While generative methods  have this property, they tend to be computationally expensive (see, e.g., ) and can be challenging to scale to high-dimensional observations.

[MISSING_PAGE_EMPTY:2]

Ideally, these representations should retain information required to predict future observations and infer likely paths between pairs of observations. Many approaches use an autoencoder, learning representations that retain the bits necessary to reconstruct the input observation, while also regularizing the representations to compressed or predictable [6; 27; 28; 29; 30; 31]. A prototypical method is the sequential VAE , which is computationally expensive to train because of the reconstruction loss, but is easy to use for inference. Our work shares the aims of prior prior methods that attempt to linearize the dynamics of nonlinear systems [32; 33; 34; 35; 36; 37], including videos [38; 39]. Our work aims to retain uncertainty estimates over predictions (like the sequential VAE) without requiring reconstruction. Avoiding reconstruction is appealing _practically_ because it decreases the computational requirements and number of hyperparameters; and _theoretically_ because it means that representations only need to retain bits about temporal relationships and not about the bits required to reconstruct the original observation.

**Contrastive Learning.** Contrastive learning methods circumvent reconstruction by learning representations that merely classify if two events were sampled from the same joint distribution [40; 17; 41]. When applied to representing states along trajectories, contrastive representations learn to classify whether two points lie on the same trajectory or not [42; 43; 44; 10; 25]. Empirically, prior work in computer vision and NLP has observed that contrastive learning acquires representations where interpolation between representations corresponds to changing the images in semantically meaningful ways [45; 46; 47; 48; 49; 16].

Our analysis will be structurally similar to prior theoretical analysis on explaining why word embeddings can solve analogies [50; 51; 52]. Our work will make a Gaussianity assumption similar to Arora et al.  and our Markov assumption is similar to the random walks analyzed in Arora et al. , Hashimoto et al. . Our paper builds upon and extends these results to answer questions such as: "what is the distribution over future observations representations?" and "what is the distribution over state (representations) that would occur on the path between one observation and another?" While prior work is primarily aimed at explaining the good performance of contrastive word embeddings (see, e.g., ), we are primarily interested in showing how similar contrastive methods are an effective tool for inference over high-dimensional time series data. Our analysis will show how representations learned via temporal contrastive learning (i.e., without reconstruction) are sufficient statistics for inferring future outcomes and can be used for performing inference on a graphical model (a problem typically associated with generative methods).

**Goal-oriented decision making.** Much work on time series representations is done in service of learning goal-reaching behavior, an old problem [54; 55] that has received renewed attention in recent years [56; 57; 58; 59; 60; 61; 62; 63]. Some of the excitement in goal-conditioned RL is a reflection of the recent success of self-supervised methods in computer vision  and NLP . Our analysis will study a variant of contrastive representation learning proposed in prior work for goal-conditioned RL [42; 43]. These methods are widespread, appearing as learning objectives for learning value functions [66; 67; 68; 69; 70; 71; 72; 73], as auxiliary objectives [74; 75; 76; 77; 71; 78; 79], in objectives for model-based RL [80; 81; 82; 32], and in exploration methods [83; 84]. Our analysis will highlight connections between these prior methods, the classic successor representation [85; 86], and probabilistic inference.

**Planning.** Planning lies at the core of many RL and control methods, allowing methods to infer the sequence of states and actions that would occur if the agent navigated from one state to a goal state. While common methods such as PRM  and RRT  focus on building random graphs, there is a strong community focusing on planning methods based on probabilistic inference [19; 89; 90]. The key challenge is scaling to high-dimensional settings. While semi-parametric methods make progress on this problem this limitation through semi-parametric planning [91; 92; 93], it remains unclear how to scale any of these methods to high-dimensional settings when states do not lie on a low-dimensional manifold. Our analysis will show how contrastive representations may lift this limitation, with experiments validating this theory on 39-dimensional and 46-dimensional tasks.

## 3 Preliminaries

Our aim is to learn representations of time series data such that the spatial arrangement of representations corresponds to the temporal arrangement of the underlying data: if one example occurs shortly after another, then they should be mapped to similar representations. This problem setting arises in many areas, including video understanding and reinforcement learning. To de fine this problem formally, we will define a Markov process with states \(x_{t}\) indexed by time \(t\):4\(p(x_{1:T} x_{0})=_{t=0}^{T}p(x_{t+1} x_{t})\). The dynamics \(p(x_{t+1} x_{t})\) tell us the immediate next state, and we can define the distribution over states \(t\) steps in the future by marginalizing over the intermediate states, \(p_{t}(x_{t} x_{0})= p(x_{1:t} x_{0})\,x_{1:t-1}\). A key quantity of interest will be the \(\)-discounted state occupancy measure, which corresponds to a time-averaged distribution over future states:

\[p_{t+}(x_{t+}=x)=(1-)_{t=0}^{}^{t}p_{t}(x_{t}=x).\] (1)

Contrastive learning.Our analysis will focus on applying contrastive learning to a particular data distribution. Contrastive learning  acquires representations using "positive" pairs \((x,x^{+})\) and "negative" pairs \((x,x^{-})\). While contrastive learning typically learns just one representation, we will use two different representation for the two elements of the pair; that is, our analysis will use terms like \((x)\), \((x^{+})\) and \((x^{-})\). We assume all representations lie in \(^{k}\).

The aim of contrastive learning is to learn representations such that positive pairs have similar representations (\((x)(x^{+})\)) while negative pairs have dissimilar representations (\((x)(x^{-})\)). Let \(p(x,x^{+})\) be the joint distribution over positive pairs (i.e., \((x,x^{+}) p(x,x^{+})\)). We will use the product of the marginal distributions to sample negative pairs (\((x,x^{-}) p(x)p(x)\)). Let \(B\) be the batch size, and note that the positive samples \(x_{j}^{+}\) at index \(j\) in the batch serve as _negatives_ for \(x_{i}\) for any \(i j\). Our analysis is based on the infoNCE objective without resubstitution :

\[_{(),()}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B} p (x,x^{+})}\![_{i=1}^{B}\|(x_{i})-(x_{ i}^{+})\|_{2}^{2}}}{_{j i}e^{-\|(x_{i})-(x_{j}^{+})\|_{2}^{2} }}+\|(x_{i})-(x_{j}^{+})\|_{2}^{2}}}{_{j  i}e^{-\|(x_{j})-(x_{i}^{+})\|_{2}^{2}}}]\] (2)

We will use the symmetrized version of this objective , where the denominator is the sum across rows of a logits matrix and once where it is a sum across the.

While contrastive learning is typically applied to an example \(x\) and an augmentation \(x^{+} p(x x)\) of that same example (e.g., a random crop), we will follow prior work  in using the time series _dynamics_ to generate the positive pairs, so \(x^{+}\) will be an observation that occurs temporally after \(x\). While our experiments will sample positive examples from the discounted state occupancy measure (\(x^{+} p_{t+}(x_{t+} x)\)) in line with prior work , our analysis will also apply to different distributions (e.g., always sampling a state \(k\) steps ahead).

While prior work typically constrains the representations to have a constant norm (i.e., to lie on the unit hypersphere) , we will instead constrain the _expected_ norm of the representations is bounded, a difference that will be important for our analysis:

\[\,_{p(x)}[\|(x)\|_{2}^{2}] c.\] (3)

Because the norm scales with the dimension of the representation, we have scaled down the left side by the representation dimension, \(k\). In practice, we will impose this constraint by adding a regularization term \(\,_{p(x)}[\|(x)\|_{2}^{2}]\) to the infoNCE objective (Eq. 2) and dynamically tuning the weight \(\) via dual gradient descent.

### Key assumptions

This section outlines the two key assumptions behind our analysis, both of which have some theoretical justification. Our main assumption examines the distribution over representations:

**Assumption 1**.: _Regularized, temporal contrastive learning acquires representations whose marginal distribution representations \(p() p(x)\,((x)=)\,x\) is an isotropic Gaussian distribution:_

\[p()=(;=0,=c I).\] (4)

In Appendix A.1 we extend prior work  provide some theoretical intuition for why this assumption should hold: namely, that the isotropic Gaussian is the distribution that maximizes entropy subject to an expected L2 norm constraint (Eq. 3) . Our analysis also assumes that the learned representations converge to the theoretical minimizer of the infoNCE objective:

**Assumption 2**.: _Applying contrastive learning to the symmetrized infoNCE objective results in representations that encode a probability ratio:_

\[e^{-\|(x_{0})-(x)\|_{2}^{2}}=(x_{t+}=x x_{0})} {p(x)C}.\] (5)

This assumption holds under ideal conditions [98; 99] (see Appendix A.5),5 but we nonetheless call this an "assumption" because it may not hold in practice due to sampling and function approximation error. This assumption means the learned representations are sufficient statistics for predicting the probability (ratio) of future states: these representations must retain all the information pertinent to reasoning about _temporal_ relationships, but need not retain information about the precise contents of the observations. As such, they may be much more compressed than representations learned via reconstruction.

Combined, these assumptions will allow us to express the distribution over sequences of representations as a Gauss-Markov chain. The denominator in Assumption 2, \(p(x)\), may have a complex distribution, but Assumption 1 tells us that the distribution over _representations_ has a simpler form. This will allow us to rearrange Assumption 2 to express the conditional distribution over representations as the product of two Gaussian likelihoods. Note that the left hand side of Assumption 2 already looks like a Gaussian likelihood.

## 4 Contrastive Representations Make Inference Easy

In this section, our main result will be to show how representations learned by (regularized) contrastive learning are distributed according to a Gauss-Markov chain, making it straightforward to perform inference (e.g., planning, prediction) over these representations. Our proof technique will combine (known) results about Gaussian distributions with (known) results about contrastive learning. We start by discussing an important choice of parametrization (Section 4.1) that facilitates prediction (Section 4.2) before presenting the main result in Section 4.3.

### A Parametrization for Shared Encoders

This section describes the two encoders (\((),()\)) to compute representations of \(x\) and \(x^{+}\). While prior work in computer vision and NLP literature use the same encoder for both \(x\) and \(x^{+}\), this decision does not make sense for many time-series data as it would imply that our prediction for \(p(x_{t} x_{0})\) is the same as our prediction for \(p(x_{0} x_{t})\). However, the difficulty of transiting from \(x_{0}\) to \(x_{t}\) (e.g., climbing to the peak of a mountain) might be more difficult than the reverse (e.g., shedding down a mountain). Our proposed parametrization will handle this asymmetry.

We will treat the encoder \(()\) as encoding the contents of the state. We will additionally learn a matrix \(A\) so that the function \( A\) corresponds to a (multi-step) prediction of the future representation. To map this onto contrastive learning, we will use \((x) A(x)\) as the encoder for the initial state. One way of interpreting this encoder is as an additional linear projection applied on top of \(()\), a design similar to those used in other areas of contrastive learning . Once learned, we can use these encoders to answer questions about prediction (Section 4.2) and planning (Section 4.3).

### Representations Encode a Predictive Model

Given an initial state \(x_{0}\), what states are likely to occur in the future? Answering this question directly in terms of high-dimensional states is challenging, but our learned representations provide a straightforward answer. Let \(_{0}=(x_{0})\) and \(_{t+}=(x_{t+})\) be random variables representing the representations of the initial state and a future state. Our aim is to estimate the distribution over these future representations, \(p(_{t+}_{0})\). We will show that the learned representations encode this distribution.

Figure 2: A parametrization for temporal contrastive learning.

**Lemma 1**.: _Under the assumptions from Section3, the distribution over representations of future states follows a Gaussian distribution with mean parameter given by the initial state representation:_

\[p(_{t+}=_{0})==A_{0}, =I.\] (6)

The main takeaway here is that the distribution over future representations has a convenient, closed form solution. The representation norm constraint, \(c\), determines the shrinkage factor \([0,1)\); highly regularized settings (small \(c\)) move the mean closer towards the origin and decrease the variance, as visualized in \(}\). Regardless of the constraint \(c\), the predicted mean is a linear function \(A\). The proof is in AppendixA.2. The proof technique is similar to that of the law of the unconscious statistician.

### Planning over One Intermediate State

We now show how these representations can be used for a specific type of planning: given an initial state \(x_{0}\) and a future state \(x_{t+}\), infer the representation of an intermediate "waypoint" state \(x_{w}\). The next section will extend this analysis to inferring the entire sequence of intermediate states. We assume \(x_{0} x_{w} x_{t+}\) form a Markov chain where \(x_{w} p(x_{t+} x_{0}=x_{0})\) and \(x_{t+} p(x_{t+} x_{0}=x_{w})\) are both drawn from the discounted state occupancy measure (Eq.1). Let random variable \(_{w}=(x_{w})\) be the representation of this intermediate state. Our main result is that the posterior distribution over waypoint _representations_ has a closed form solution in terms of the initial state representation and future state representation:

**Theorem 2**.: _Under Assumptions1 and 2, the posterior distribution over waypoint representations is a Gaussian whose mean and covariance are linear functions of the initial and final state representations:_

\[p(_{w}_{0},_{t+})=_{w};=(A^{T} _{t+}+A_{0}),^{-1}=A^{T}A+I.\]

The proof (AppendixA.3) uses the Markov property together with Lemma1. The main takeaway from this lemma is that the posterior distribution takes the form of a simple probability distribution (a Gaussian) with parameters that are linear functions of the initial and final representations.

We give three examples to build intuition:

**Example 1:**\(A=I\) and the \(c\) is very large (little regularization). Then, the covariance is \(^{-1} 2I\) and the mean is the simple average of the initial and final representations \((_{0}+_{t+})\). In other words, the waypoint representation is the midpoint of the line \(_{0}_{t+}\).

**Example 2:**\(A\) is a rotation matrix and \(c\) is very large. Rotation matrices satisfy \(A^{T}=A^{-1}\) so the covariance is again \(^{-1} 2I\). As noted in Section4.2, we can interpret \(A_{0}\) as a _prediction_ of which representations will occur after \(_{0}\). Similarly, \(A^{-1}_{t+}=A^{T}_{t+}\) is a prediction of which representations will occur before \(_{t+}\). Theorem2 tells us that the mean of the waypoint distribution is the simple average of these two predictions, \((A^{T}_{t+}+A_{0})\).

**Example 3:**\(A\) is a rotation matrix and \(c=0.01\) (very strong regularization). In this case \(^{-1}=A^{T}A+I 100I\), so \((_{0}+_{t+}) 0\). Thus, in the case of strong regularization, the posterior concentrates around the origin.

### Planning over Many Intermediate States

This section extends the analysis to multiple intermediate states. Again, we will infer the posterior distribution of the representations of these intermediate states, \(_{w_{1}},_{w_{2}},\). We assume that these states form a Markov chain.

**Theorem 3**.: _Given observations from a Markov chain \(x_{0} x_{1} x_{t+}\), the joint distribution over representations is a Gaussian distribution. Using \(_{1:n}=(_{w_{1}},,_{w_{n}})\) to denote the concatenated representations of each observation, we can write this distribution as_

\[p(_{1:n})-_{1:n}^{T}^{-1}_{1: n}+^{T}_{1:n},\]

Figure 3: Predicting representations of future states.

_where \(^{-1}\) is a tridiagonal matrix_

\[^{-1}=A^{T}A+I&-A^{T}&\\ -A&A^{T}A+I&-A^{T}&= A_{0}\\ 0\\ \\ A^{T}_{t+}.\]

This distribution can be written in the canonical parametrization as \(=^{-1}\) and \(=\). Recall that Gaussian distributions are closed under marginalization. Thus, once in this canonical parametrization, the marginal distributions can be obtained by reading off individual entries of these parameters:

\[p(_{i}_{0},_{t+})=(_{i};_{i}=( )^{(i)},_{i}=(^{-1})^{(i,i)}).\]

The key takeaway here is that this posterior distribution over waypoints is Gaussian, and it has a closed form expression in terms of the initial and final representations (as well as regularization parameter \(c\) and the learned matrix \(A\)).

In the general case of \(n\) intermediate states, the posterior distribution is

\[p(_{w_{1}}_{w_{n}}_{0},_{t+}) e^{-}{_{i=1}^{n}\|A_{w_{i}}-_{w_{i+1}}\|_{2}^{ 2}}},\]

where \(_{w_{0}}=_{0}\) and \(_{w_{n+1}}=_{t+}\). This corresponds to a chain graphical model with edge potentials \(f(,^{})=e^{-}{\|A-^{ }\|_{2}^{2}}}\).

**Special case.** To build intuition, consider the special case where \(A\) is a rotation matrix and \(c\) is very large, so \(A^{T}A+ 2I\). In this case, \(^{-1}\) is a (block) second difference matrix :

\[^{-1}=2I&-I\\ -I&2I&-I\\ -I&.\]

The inverse of this matrix has a closed form solution [101, Pg. 471], allowing us to obtain the mean of each waypoint in closed form:

\[_{i}=(1-(i))A_{0}+(i)A^{T}_{t+},\] (7)

where \((i)=\). Thus, each posterior mean is a convex combination of the (forward prediction from the) initial representation and the (backwards prediction from the) final representation. When \(A\) is the identity matrix, the posterior mean is simple linear interpolation between the initial and final representations!

## 5 Numerical Simulation

We include several didactic experiments to illustrate our results. All results and figures can be reproduced by running make in the source code: https://github.com/vivekmyers/contrastive_planning. The expected compute time is a few hours on a A6000 GPU. Figures in this section show error across different training and dataset split seeds.

### Synthetic Dataset

To validate our analysis, we design a time series task with 2D points where inference over intermediate points (i.e., in-filling) requires nonlinear interpolation. Fig. 4_(Top Left)_ shows the dataset of time series data, starting at the origin and spiraling outwards, with each trajectory using a randomly-chosen initial angle. We applied contrastive learning with the parametrization in Section 4.1 to these data and used the learned representations to solve prediction and planning problems (see Fig. 4 for details). Note that these predictions correctly handle the nonlinear structure of these data -- states nearby the initial state in Euclidean space that are not temporally adjacent are assigned low likelihood.

### Solving Mazes with Inferred Representations

Our next experiment studies whether the inferred representations are useful for solving a control task. We took a 2d maze environment and dataset from prior work (Fig. 5, _Left_)  and learned encodersfrom this dataset. To solve the maze, we take the observation of the starting state and goal state, compute the representations of these states, and use the analysis in Section 4.3 to infer the sequence of intermediate representations. We visualize the results using a nearest neighbor retrieval (Fig. 5, _Left_). Figure 7 contains additional examples.

Finally, we studied whether these representations are useful for control. We implemented a simple proportional controller for this maze. As expected, this proportional controller can successfully navigate to close goals, but fails to reach distant goals (Fig. 5, _Right_). However, if we use the proportional controller to track a series of waypoints planned using our representations (i.e., the orange dots shown in Fig. 5 (_Left_)), the success rate increases by up to \(4.5\). To test the importance of _nonlinear_ representations, we compare with a "PCA" baseline that predicts waypoints by interpolating between the principal components of the initial state and goal state. The better performance of our method indicates the importance of doing the interpolation using representations that are _nonlinear_ functions of the input observations. While prior methods learn representations to encode temporal distances, it

Figure 4: **Numerical simulation of our analysis.**_(Top Left)_ _Toy dataset of time-series data consisting of many outwardly-spiraling trajectories. We apply temporal contrastive learning to these data._(Top Right)_ _For three initial observations (_\(\)_), we use the learned representations to predict the distribution over future observations. Note that these distributions correctly capture the spiral structure._(Bottom Left)_ _For three observations (_\(\)_), we use the learned representations to predict the distribution over preceding observations._(Bottom Right)_ _Given an initial and final observation, we plot the inferred posterior distribution over the waypoint (Section 4.3). The representations capture the shape of the distribution._

Figure 5: Using inferred paths over our contrastive representations for control boosts success rates by \(4.5\) on the most difficult goals (\(18\% 84\%\)). Alternative representation learning techniques fail to improve performance when used for planning.

is unclear whether these methods support inference via interpolation. To test this hypothesis, we use one of these methods ("VIP" ) as a baseline. While the VIP representations likely encode similar bits as our representations, the better performance of the contrastive representations indicates that the VIP representations do not expose those bits in a way that makes planning easy.

### Higher dimensional tasks

In this section we provide preliminary experiments showing the planning approach in Section4 scales to higher dimensional tasks. We used two datasets from prior work : door-human-v0 (39-dimensional observations) and hammer-human-v0 (46-dimensional observations). After learning encoders on these tasks, we evaluated the inference capabilities of the learned representations. Given the first and last observation from a trajectory in a validation set, we use linear interpolation (see Eq.7) to infer the representation of five intermediate waypoint representations.

We evaluate performance in two ways. **Quantitatively**, we measure the mean squared error between each of the true waypoint observations and those inferred by our method. Since our method infers representations, rather than observations, we use a nearest-neighbor retrieval on a validation set so that we can measure errors in the space of observations. **Qualitatively**, we visualize the high-dimensional observations from the validation trajectory using a 2-dimensional TSNE  embedding, overlying the infer waypoints from our method; as before, we convert the representations inferred by our method to observations using nearest neighbors.

We compare with three alternative methods in Fig.6. To test the importance of representation learning, we first naively interpolate between the initial and final observations ("no planning"). The poor performance of this baseline indicates that the input time series are highly nonlinear. Similarly, interpolating the principle components of the initial and final observations ("PCA") performs poorly, again highlighting that the input time series is highly nonlinear and that our representations are doing more than denoising (i.e., discarding directions of small variation). The third baseline, "VIP" ,

Figure 6: Planning for 39-dimensional robotic door opening. _(Top Left)_ We use a dataset of trajectories demonstrating door opening from prior work  to learn representations. _(Top Right)_ We use our method and three baselines to infer one intermediate waypoint between the first and last observation in a trajectory from a held-out validation set. Errors are measured using the mean squared error with the true waypoint observation; predicted representations are converted to observations using nearest neighbors on a validation set. _(Bottom)_ We visualize a TSNE  of the states along the sampled trajectory as blue circles, with the transparency indicating the index along the trajectory. The inferred plan is shown as red circles connected by arrows. Our method generates better plans than alternative representation learning methods (PCA, VIP).

learns representations to encode temporal distances using approximate dynamic programming. Like our method, VIP avoids reconstruction and learns nonlinear representations of the observations. However, the results in Fig. 6 highlight that VIP's representations do not allow users to plan by interpolation. The error bars shown in Fig. 6 (_Top Right_) show the standard deviation over 500 trajectories sampled from the validation set. For reproducibility, we repeated this entire experiment on another task, the 46-dimensional hammer-human-v0 from D4RL. The results, shown in Appendix Fig. 8, support the conclusions above. Taken together, these results show that our procedure for interpolating contrastive representations continues to be effective on tasks where observations have dozens of dimensions.

## 6 Discussion

Representation learning is at the core of many high-dimensional time-series modeling questions, yet how those representations are learned is often disconnected with the inferential task. The main contribution of this paper is to show how _discriminative_ techniques can be used to acquire compact representations that make it easy to answer inferential questions about time. The precise objective and parametrization we studied is not much different from that used in practice, suggesting that either our theoretical results might be adapted to the existing methods, or that practitioners might adopt these details so they can use the closed-form solutions to inference questions. Our work may also have implications for studying the structure of learned representations. While prior work often studies the geometry of representations as a post-hoc check, our analysis provides tools for studying _when_ interpolation properties are guaranteed to emerge, as well as _how_ to learn representations with certain desired geometric properties.

**Limitations.** Our analysis hinges on the two assumptions mentioned in Section 3.1, and it remains open how errors in those approximations translate into errors in our analysis. One important open question is whether it is always possible to satisfy these assumptions using sufficiently-expressive representations.