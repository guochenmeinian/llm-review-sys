# BLoB: Bayesian Low-Rank Adaptation by

Backpropagation for Large Language Models

Yibin Wang \({}^{*}{}^{}{}^{1}\)   Haizhou Shi \({}^{*}{}^{}{}^{1}\)   Ligong Han \({}^{12}\)   Dimitris Metaxas \({}^{1}\)   Hao Wang \({}^{}{}^{1}\)

Equal Contribution. \({}^{1}\)Rutgers University. \({}^{2}\)MIT-IBM Watson AI Lab. \({}^{}\)Correspondence to: Yibin Wang <yibin.wang@rutgers.edu>, Haizhou Shi <haizhou.shi@rutgers.edu>, Hao Wang <hw488@cs.rutgers.edu>.

###### Abstract

Large Language Models (LLMs) often suffer from overconfidence during inference, particularly when adapted to downstream domain-specific tasks with limited data. Previous work addresses this issue by employing approximate Bayesian estimation _after_ the LLMs are trained, enabling them to quantify uncertainty. However, such post-training approaches' performance is severely limited by the parameters learned _during_ training. In this paper, we go beyond post-training Bayesianization and propose **B**ayesian **L**ow-Rank Adaptation by **B**ackpropagation (**BLoB**), an algorithm that continuously and jointly adjusts both the mean and covariance of LLM parameters throughout the whole fine-tuning process. Our empirical results verify the effectiveness of BLoB in terms of generalization and uncertainty estimation, when evaluated on both in-distribution and out-of-distribution data. Code is available at https://github.com/Wang-ML-Lab/bayesian-pft.

## 1 Introduction

Despite the recent advancements in Large Language Models (LLMs) , addressing the challenges of reliability and responsibility remains imperative . LLMs often produce overconfident responses detached from factual grounding, posing potential harm to users . Therefore, accurately estimating response confidence (or uncertainty) is crucial to preemptively intervene before harm occurs. Current research predominantly focuses on eliciting the internal capability of uncertainty estimation of LLMs. For example, studies suggest that verbalized uncertainty yields better-calibrated results compared to conditional probability .

While effective, the aforementioned methods do not offer a universal solution for expressing LLM uncertainty across all scenarios, especially when adapted  to domain-specific corpora, human preferences, or downstream tasks . Even a well-calibrated LLM may struggle to estimate uncertainty during fine-tuning due to catastrophic forgetting of general knowledge . Moreover, when applied to limited-scale downstream tasks, excessively over-parameterized LLMs can rapidly overfit, leading to overconfidence. Thus, enabling accurate uncertainty estimation of LLMs is vital for their reliable and responsible deployment.

Bayesian methods emerge as a natural solution for learning uncertainty estimation abilities among their counterparts . These methods model predictive uncertainty \(P(|,)\) by marginalizing the posterior parameter distribution \(P(|)\) after observing the dataset \(\):

\[P(|,)= P(|,)P(| )d.\] (1)

However, adapting the Bayesian framework to LLMs poses significant challenges. LLM architectures typically incorporate complex components, including non-linear activation functions, rendering exactBayesian inference of parameter posteriors intractable, i.e., unable to compute the integral precisely. Consequently, finding an accurate approximation algorithm for the true posterior distribution becomes a primary challenge. Additionally, modeling parameter posterior distributions demands extra memory space, imposing a prohibitive burden on systems due to the massive scale of LLMs.

Contemporary methods leverage Parameter-Efficient Fine-Tuning (PEFT) to reduce the number of tunable parameters, thus alleviating computational and storage resource burdens [23; 41; 26; 121; 56; 53]. Built on this, recent research explores Bayesianizing only the PEFT module during fine-tuning to calibrate LLMs [8; 103; 116; 69], somewhat relieving the burden of introducing more parameters for posterior approximation. However, initial investigations suggest that straightforward combinations of PEFT and basic Bayesian techniques like Monte-Carlo Dropout (MCD, ) or Deep Ensemble (ENS, [51; 8; 103]) yield only marginal improvements in generalization and uncertainty estimation. The most promising results to date involve Kronecker factorized Laplace approximation, applied after maximum a posteriori (MAP) estimation provided by any optimization algorithm . Nevertheless, we argue that such post-training procedures bifurcate posterior approximation into two stages, inevitably leading to suboptimal estimation.

To address this challenge, we propose **B**ayesian **L**ow-Rank Adaptation by **B**ackpropagation (**BLoB**), a Bayesian Deep Learning framework for fine-tuning LLMs with LoRA. BLoB jointly estimates the low-rank variational distributions' mean and covariance throughout the entire fine-tuning stage via backpropagation. Unlike methods relying on post-training approximation, BLoB enables simultaneous estimation of both the parameter mode (i.e., the mean if one assumes Gaussian distributions) and the parameter variance. Random sampling of model parameters based on variance estimation can enhance mode estimation. It thereby improves model performance in terms of accuracy and uncertainty estimation on both in-distribution and out-of-distribution datasets, as verified by our extensive experiments across multiple datasets. In summary, our contributions are:

* We propose a principled Bayesianization framework for Low-Rank Adaptation (LoRA) in Large Language Models (LLMs) by assuming that full weights' approximate posterior distribution has a low-rank structure containing a linear combination of independent Gaussian distributions.
* We show that, under mild conditions, optimization of the full-weight variational distribution can be done efficiently in the low-rank space of the weight update matrices.
* We introduce BLoB, a variational Bayesian low-rank adaptation framework for LLMs that jointly learns the mean and covariance of the variational distribution during fine-tuning.
* Extensive evaluations demonstrate the superiority of BLoB in terms of generalization and uncertainty estimation across different scenarios.

## 2 Preliminaries

In this section, we describe the notation as well as some preliminaries.

Notation.In this paper, scalars are denoted by lowercase letters, vectors by lowercase boldface letters, and matrices by uppercase boldface letters. Probability, expectation, and the dataset are denoted by \(P\), \(\), and \(\), respectively. We use \([m]=\{1,2,,m\}\) to denote the set of consecutive integer numbers starting from \(1\) and ending at \(m\). For a matrix \(=[_{1},,_{n}]^{m n}\), we use \(()=[_{1}^{},_{2}^{},,_{n}^{ }]^{}^{(mn) 1}\) to denote the vectorization operation; we use \(\|\|_{p}=[_{ij}|X_{ij}|^{p}]^{}{{p}}}\) to define the \(p\)-norm of a matrix. We use \(\) and \(\) to denote the Kronecker product and the element-wise product, respectively.

### Low-Rank Adaptation (LoRA)

Inspired by the pioneering work on identifying and leveraging the low intrinsic rank of over-parameterized models during fine-tuning [55; 2], Low-Rank Adaptation (LoRA) assumes a low rank for the network's weight updates . Typically in a single linear layer, LoRA decomposes each update matrix \(=\) into the product of two low-rank matrices, where \(^{m r}\) and \(^{r n}\). Here, \(m\), \(n\), and \(r\) denote the number of input neurons, output neurons, and the rank of the decomposition, respectively . The forward pass of the linear layer with LoRA is formulatedas:

\[=_{0}+=_{0}+,\] (2)

where \(\) and \(\) denote the input and output of the layer. Since the rank \(r\{m,n\}\) is significantly smaller than the numbers of input and output neurons (e.g., \(r=8 m=n=4096\) in the attention layer ), LoRA can drastically reduce the number of trainable parameters by approximately three orders of magnitude compared to full-parameter fine-tuning, while achieving comparable performance to the full-rank fine-tuning. This also leads to a similar reduction in memory consumption for storing optimizer states, thereby reducing the hardware requirements for fine-tuning LLMs to a great extent.

### Variational Bayesian Networks (VBNs)

Bayesian Neural Networks (BNNs) estimate the posterior distributions of network parameters rather than relying on single-point estimates [10; 102]. Due to the intractability of exact inference of the true posterior, Variational Bayesian Networks (VBNs) approximate the true posterior using a variational distribution; this is done by minimizing its KL divergence from the true posterior distribution [39; 30; 11]. Specifically, if the weights \(\)'s variational distribution \(q(|)\) is parameterized by \(\), minimizing the divergence \([q(|)\|P(|)]\) is equivalent to minimizing the following variational free energy with respect to \(\)[67; 117; 28]:

\[(,)-_{q(|)}[ P(|)]+[q(|) P ()].\] (3)

The final formulation of the objective function in Eqn. 3 offers another interpretation beyond minimizing the KL divergence between the variational and true posterior distributions . Specifically, the first term maximizes the likelihood of the data, while the second term regularizes the variational distribution \(q(|)\). We refer to the first term as the likelihood cost and the second term as the complexity cost. Optimizing these two terms involves balancing the expressiveness of the approximate posterior distribution and its simplicity.

Optimizing the first term of Eqn. 3 requires integrating out the parameterized variational distribution, necessitating Monte Carlo gradient estimation [52; 81]. Using this approach, we can incorporate the re-parameterization trick to enable backpropagation of the gradient to the underlying parameter \(\)[72; 47; 78]. In Bayes By Backprop (BBB) , the variational distribution is further simplified as a diagonal Gaussian \((,^{2})\), where \(=(1+())\) ensures the standard deviation is positive. Then we have the Monte-Carlo estimation of Eqn. 3 that can pass the gradient to \(\):

\[(,)-_{k=1}^{K} P( |_{k})+_{k=1}^{K}[ q(_{k}|)- P(_{k})],\] (4)

where \(_{k}=+(1+())_{k}\) is the \(k\)-th sample of the weights yielded by parameterization and \(_{k}(,)\). In BBB, the authors assume the prior distribution \(P()=(,_{1}^{2})+(1-)(,_{2}^{2})\) to be a mixture of Gaussians. Consequently, they optimize the second term based on weight sampling. In different scenarios, a simpler form of the prior, which allows for a closed-form solution, can also be considered. Although our proposed method is largely based on the existing framework of BBB, trivially combining BBB with LoRA does not yield satisfactory results. It is important to note that our specific designs are necessary to encourage the fast convergence of the variational distribution, which will be introduced later in Sec. 3.

## 3 Methodology

In this section, we formally introduce our proposed method, **B**ayesian **L**ow-**R**ank Adaptation by **B**ackpropagation (BLoB). We begin by discussing the design choices for Bayesianizing LoRA parameters in Sec. 3.1, highlighting the assumptions BLoB makes about the approximate posterior in the full-weight space. Next, in Sec. 3.2, we explore the low-rank structure of the prior distribution in the full-weight space, which in turn motivates our choice of prior distributions in the low-rank parameter space. In Sec. 3.3, we introduce our parameterization method for the variational distributions. In Sec. 3.4, we integrate Flipout  into LoRA for improved sampling efficiency and faster convergence. Finally, we present the complete algorithmic description of BLoB in Sec. 3.5. Proof of the theorems and claims in this section can be found in Appendix A.

### Low-Rank Variational Approximate Posterior Distribution: LoRA Bayesianization

**Asymmetric LoRA Bayesianization.** In LoRA , the weights are treated asymmetrically. \(\) is randomly initialized, usually from the standard normal distribution or using Kaiming initialization, while \(=\) is initialized as a zero matrix to ensure that the model fully retains the capabilities of the pre-trained weights at the start of fine-tuning. The trivial solution of estimating the variational approximate posterior for the entire set of LoRA parameters can significantly hinder training convergence. For example, consider the Gaussian posteriors \(q(|)=(|_{A},_{A}^{2})\) and \(q(|)=(|,_{B}^{2})\), where \(_{A}\) and \(_{B}\) are variance estimates added to \(\) and \(\), respectively. Although the expectation \(_{,}[(_{0}+)]=_{0}+ _{,}[]=_{0}\) preserves the functionality of the pre-trained model, accurate estimation requires an impractically large number of weight samples. Such variational distributions lead to significant fluctuations during the early stages of fine-tuning, unless the initial variance of \(\), \(_{B}^{+}\), is intentionally minimized towards zero. Therefore, we take an _asymmetric_ approach to initialize \(_{B}=\) and keep it fixed throughout the fine-tuning process. This, in effect, gives up Bayesian modeling of the \(\) component and focuses only on the posterior of \(\) in LoRA, as shown in Fig. 1.

**Additional Advantages.** In addition to reducing sampling noise and improving convergence speed, our Bayesianization design has two further advantages. First, compared to modeling the variational distributions of both \(\) and \(\), our approach significantly reduces additional memory cost by approximately 50% per layer. Second, our design is equivalent to finding a posterior estimate for the full-weight matrix with a low-rank structure. For instance, by assuming a deterministic \(\) and Bayesianizing \(P(|)=(|,^{2})\), each element of the full weight matrix \(W_{ij}\) is calculated as

\[W_{ij}=W_{0,ij}+_{k=1}^{r}B_{ik}A_{kj},\] (5)

where \(A_{kj}(M_{kj},_{kj}^{2})\) is drawn independently \( k[r]\). It is noteworthy that due to the low-rank structure defined in Eqn. 5, the full-weight parameters of \(\) are no longer independent from each other. The correlation among them can be reflected by the following theorem:

**Theorem 3.1** (**Variational Distribution of the Full-Weight Matrix in BLoB)**.: _With the pre-trained weight matrix \(_{0}^{m n}\) and the low-rank weight update matrix \(^{m r}\), suppose that the variational distribution of the other low-rank update matrix \(^{r n}\) is Gaussian with \(q(|=\{,\})=_{ij}(A_{ij}|M_ {ij},_{ij}^{2})\), where \(=[M_{ij}]^{r n}\) and \(=[_{ij}]^{r n}\) are its mean and standard deviation, respectively. The equivalent variational distribution defined on the full weight matrix \(\) as in Eqn. 3 is given by_

\[q(()|,) =(()|_{q},_{q}),\] (6) \[_{q} =(_{0}+),\] (7) \[_{q} =[_{n}][(()^{2})][_{n}^{}].\] (8)

Theorem 3.1 shows that our asymmetric LoRA Bayesianization is equivalent to using a Gaussian variational distribution for the full weight \(\) (i.e., Eqn. 6), with a flexible covariance matrix (i.e., Eqn. 8), to approximate the postetior distribution of the full weight \(\).

Figure 1: Overview of our Bayesian Low-Rank Adaptation by Backpropagation, i.e., BLoB (**right**) as well as comparison with existing methods such as LoRA (**left**) and Laplace LoRA (**middle**).

**Remark**.: _The covariance matrix \(_{q}\) is strictly singular, which consequently inspires us to design a prior \(P()\) with such low-rank structure in Sec. 3.2. Previous work on low-rank Gaussians typically considers covariance with a similar structure \(^{2}+_{q}\), where \(\) is diagonal [89; 70; 86; 90; 73]. However, sampling from a Gaussian with this structure requires sampling noise of the same shape as the full-weight matrix, which is not parameter-efficient; we therefore do not adopt this in our work._

### Low-Rank Prior Distribution

In Eqn. 3, optimizing the KL divergence between the variational and prior distributions in the space of full weights can be burdensome. Therefore, we assume the prior distribution of the full weights to be a low-rank Gaussian, with its mean centered at the pre-trained weights \((_{0})\) and its covariance matrix parameterized by a rank-\(r^{}\) matrix \(}^{(mn) r^{}}\):

\[P(()) =(()|_{p},_{p}),\] (9) \[_{p} =(_{0}),\] \[_{p} =}}^{}.\]

Assuming a low-rank prior distribution and designing an appropriate \(}\) allows us to optimize the KL divergence in the decomposed low-rank weight space, as suggested by the following theorem.

**Theorem 3.2** (**Efficient Computation of Full-Weight KL Divergence)**.: _Suppose the pre-trained weights \(_{0}\), update matrix \(\), and the variational distribution \(q(|)\) are defined as in Theorem 3.1, and the prior distribution of the full-weight matrix \(P(())\) is defined as Eqn. 9. Consider the Gaussian prior distribution \(P()=_{ij}(A_{ij}|0,_{p}^{2})\); we then have:_

\[[q(()|,)\|P(( ))]=[q(|)\|P()],\] (10)

_if \(}=[_{p}_{n}]\), where \(\) satisfies \(^{}=^{}\)._

Theorem 3.2 shows that with a proper \(}\), one can compute the KL divergence for the high-dimensional full weight \(()\) simply by computing the KL divergence for \(\), which is much lower-dimension, more parameter-efficient, more memory-efficient, and faster. Note that the Gaussian distributions we define for both the prior and the posterior are degenerate. However, they are valid for probabilistic inference , as (i) their probability density is well-defined, and (ii) their KL divergence is computable under the assumptions of Theorem 3.2. See Appendix A.1 for a detailed discussion.

Concretely, we assume that the prior distribution in BLoB follows the low-rank structure described in Theorem 3.2 and minimize the KL divergence term for the low-rank component \(\) using its analytical solution in Eqn. 3:

\[[q(|=\{,\})\|P()]= {2_{p}^{2}}(\|\|_{2}^{2}+\|\|_{2}^{2})-_{ij} _{ij}.\] (11)

### Parameterization of the Low-Rank Variational Distribution

The parameterization of the Gaussian variational distribution \(q(|)\) significantly affects the convergence speed of the KL term in Eqn. 11. The mean matrix \(\) of \(q(|)\) has no additional constraints, we therefore parameterize it directly as the output of a neural network. Each entry of \(q(|)\)'s diagonal covariance matrix \(\) (i.e., standard deviation) is non-negative; we therefore use element-wise parameterization \(_{ij}=G_{ij}^{2}\), where \(=[G_{ij}]^{r n}\) is the real parameter matrix that determines the standard deviation \(\). Since \(\) is usually initialized with small positive values close to zero, our parameterization method provides large gradients initially, contributing to the rapid decrease of the KL term. We further show, both theoretically and empirically, that our parameterization method, unlike BBB's softplus function \((1+())\), is crucial for the fast convergence of \(\) when \(q(|)\) is close to the prior distribution \(P()\) (see more analysis in Appendix A.2).

### On Improving the Sample Efficiency of BLoB

**Improving Sample Efficiency with Flipout.** One main challenge in estimating the variational distribution (i.e., the approximate posterior) during fine-tuning lies in the sample efficiency of theweights [109; 25; 58]. During mini-batch stochastic gradient descent, a batch of examples typically share the same weights drawn from the variational distribution. This can lead to slow convergence of the likelihood cost in Eqn. 3. Drawing inspiration from , we introduce the technique of flipout to speed up the sampling procedure of our low-rank variational distributions \(q(|)\).

**LoRA Flipout.** Unlike the original approach, which applies rank-1 random flipping to the full weights, we apply flipout exclusively to the low-rank component \(\). Specifically, suppose we have a mini-batch of input vectors \(^{n b}\), where \(b\) represents the batch size. We randomly sample two low-rank flipping matrices \(\{-1,+1\}^{n b}\) and \(\{-1,+1\}^{b r}\). Denoting as \(^{r n}\) the weight noise sampled for this mini-batch, the batched output \(\) after applying flipout is then

\[=_{0}+(+[()( )]),\] (12)

It is crucial that the independent noises added to the low-rank weight noise \(\) ensure _sampling independence across examples_ within a mini-batch, thereby enhancing the sampling efficiency of the algorithm. This is done without violating the assumptions outlined in Theorem 3.1 and 3.2. As illustrated in algorithm 1, we use \(}_{kj}\) to represent the equivalent noise applied to parameter \(\) for the \(j\)-th example in the \(k\)-th batch after BLoBFlipout. Due to the low-rank structure of our Bayesianization method, the computational overhead of employing flipout in BLoB is also minimal.

```
0: dataset \(\), pre-trained weight \(_{0}\), low-rank component \(\), \(=\{,\}\) for parameterizing the mean and variance of \(\);
0: prior standard deviation \(_{p}\), initialization hyperparameter \(\), number of input features \(n\);
0: number of samples during training \(K\), number of iterations \(T\), learning rate \(\);
1:\((},)\), \((-},})\)\(\) Initialization of \(\)'s parameters.
2:\(\)\(\) Initialization of \(\).
3:for\(t=1,,T\)do
4: Sample a mini-batch of data \(_{t}\) containing \(b\) samples.
5:for\(k=1,,K\)do
6: Sample batched noise \(_{k}(,)\).\(\) Sample the noise.
7: Let \(\{}_{kj}\}_{j=1}^{b}(_{k})\).\(\) Eqn. 12
8: Let \(\{_{kj}=+^{2}}_{kj}\}_{j=1}^{b}\).
9:endfor
10: Let \(}_{t}=-_{k=1}^{K}_{j=1}^{b} P( _{t}|_{kj},)+^{2}}(\|\|_{2}^ {2}+\|\|_{2}^{4})-2_{ij} G_{ij}\).
11:\(\) Eqn. 13 and 11.
12: Calculate the gradient w.r.t. the parameters: \(_{}=}_{t}}}{{ }}\), \(_{}=}_{t}}}{{ }}\), \(_{}=}_{t}}}{{ }}\).
13: Update the parameters: \(-_{}\); \(-_{}\); \(-_{}\).
14:endfor ```

**Algorithm 1** Bayesian Low-Rank Adaptation by Backpropagation (**BLoB**)

### BLoB: Final Algorithm

We are now ready to present our full BLoB algorithm.

**During training**, under the assumptions outlined in Theorem 3.1 and 3.2, optimizing the evidence lower bound on the full weight \(\) can be efficiently done in the low-rank space, using the following final objective function:

\[(,,) =-_{q(|,)}[ P(| )]+[q(|,) P()]\] \[=-_{q(|)}[ P(|, )]+[q(|) P()],\] (13)

where \(=\{,\}\) denotes the set of the parameters underlying the variational distribution of the low-rank matrix \(\). Additionally, to trade off between data fitting and posterior approximation,we employ a KL re-weighting scheme, which is detailed in Appendix B.1. The full algorithmic description of BLoB training is shown in Algorithm 1.

**During inference,** for an input \(\), we approximate the expected output distribution \(P(|)\) of BLoB by drawing \(N\) samples from the variational distribution \(q(|)\). Empirically, \(N=10\) provides a good balance between estimation quality and computational efficiency:

\[_{q(|)}[P(|,)] _{n=1}^{N}P(|,_{n}),_{n} q(|).\] (14)

## 4 Experiments

In this section, we compare our BLoB with existing methods on real-world datasets. Sec. 4.1 introduces the experimental settings, including baselines, fine-tuning, and evaluation protocols. We then evaluate BLoB's generalization and uncertainty estimation abilities in both in-distribution (Sec. 4.2) and out-of-distribution scenarios (Sec. 4.3).

### Settings

**Fine-tuning and Evaluation.** We implement BLoB in the PEFT library  and fine-tune the LlaMA2-7B  model on common-sense reasoning tasks. Following Laplace-LoRA , we apply LoRA to the output layer as well as the queries and values of all the attention layers. For hyperparameters, we strictly adhere to the default settings in the PEFT library and the original LoRA paper [63; 41] to ensure maximal reproducibility. This includes the number of training steps, learning rate, and LoRA rank \(r\) (see Appendix B.1 for details). For common-sense reasoning tasks, we select

    &  &  \\   & & WG-S  & ARC-C  & ARC-E  & WG-M  & OBQA  & BoolQ  \\   & MLE & 68.99\(\)0.58 & 69.10\(\)2.84 & 85.65\(\)0.92 & 74.53\(\)0.66 & 81.52\(\)0.25 & 86.53\(\)0.28 \\  & MAP & 68.62\(\)0.71 & 67.59\(\)0.40 & 86.55\(\)0.55 & 75.61\(\)0.71 & 81.38\(\)0.65 & 86.50\(\)0.41 \\  & MCD  & 69.46\(\)0.62 & 68.69\(\)1.30 & 86.21\(\)0.46 & **76.45\(\)0.60** & 81.72\(\)0.10 & 87.29\(\)0.13 \\  & ENS [51; 8; 103] & 69.57\(\)0.66 & 66.20\(\)0.21 & 84.40\(\)0.81 & 75.32\(\)0.21 & 81.38\(\)0.91 & 87.09\(\)0.11 \\  & BBB  & 56.54\(\)37.80 & 68.13\(\)1.27 & 85.86\(\)0.74 & 73.63\(\)2.44 & 82.06\(\)0.99 & **87.11\(\)0.22** \\  & LAP  & 69.20\(\)1.50 & 66.78\(\)0.69 & 80.05\(\)0.22 & 75.55\(\)0.36 & 82.12\(\)0.65 & 86.95\(\)0.09 \\   & BLoB (N=0) & **70.89\(\)0.82** & **70.83\(\)1.57** & **86.68\(\)0.66** & 74.55\(\)1.94 & **82.73\(\)0.64** & 86.80\(\)0.23 \\  & BLoB (N=5) & 66.30\(\)0.62 & 67.34\(\)1.55 & 84.74\(\)0.33 & 72.89\(\)1.25 & 81.79\(\)0.84 & 86.47\(\)0.15 \\  & BLoB (N=10) & 69.07\(\)0.34 & 68.81\(\)1.49 & 85.56\(\)0.35 & 73.69\(\)0.17 & 81.52\(\)0.74 & 86.99\(\)0.24 \\   & MLE & 29.83\(\)0.38 & 29.00\(\)1.97 & 13.12\(\)1.39 & 20.62\(\)0.74 & 12.55\(\)0.46 & 3.18\(\)0.09 \\  & MAP & 29.76\(\)0.87 & 29.42\(\)0.68 & 12.07\(\)0.55 & 23.07\(\)0.14 & 13.26\(\)0.82 & 3.16\(\)0.23 \\  & MCD  & 27.98\(\)0.44 & 27.53\(\)0.00 & 12.02\(\)0.36 & 19.55\(\)0.47 & 13.10\(\)0.11 & 3.46\(\)0.16 \\  & ENS [51; 8; 103] & 25.52\(\)0.55 & 29.16\(\)2.37 & 12.57\(\)0.58 & 20.86\(\)0.40 & 15.34\(\)0.27 & 9.61\(\)0.24 \\  & BBB  & 21.81\(\)12.95 & 26.23\(\)1.47 & 12.28\(\)0.58 & 15.76\(\)0.71 & 11.38\(\)1.07 & 3.74\(\)0.10 \\  & LAP  & **4.15\(\)1.12** & 16.25\(\)2.61\({}^{}\) & 33.29\(\)0.57 & 7.40\(\)0.27 & 8.70\(\)1.77 & **1.30\(\)0.33** \\   & BLoB (N=0) & 20.62\(\)0.83 & 20.61\(\)1.16 & 9.43\(\)0.38 & 11.23\(\)0.69 & 8.36\(\)0.38 & 2.46\(\)0.07 \\  & BLoB (N=5) & 10.89\(\)0.63 & 11.22\(\)0.35 & 6.10\(\)0.22 & 4.51\(\)0.35 & **3.40\(\)0.57** & 1.63\(\)0.35 \\  & BLoB (N=10) & 0.35\(\)1.37 & **9.59\(\)1.88** & **3.64\(\)0.53** & **3.01\(\)0.12** & **3.71\(\)0.47** & 1.41\(\)0.19 \\   & MLE & 3.17\(\)0.37 & 2.85\(\)0.27 & 1.17\(\)0.13 & 0.95\(\)0.07 & 0.73\(\)0.03 & 0.32\(\)0.00 \\  & MAP & 2.46\(\)0.34 & 2.66\(\)0.11 & 0.90\(\)0.05 & 1.62\(\)0.29 & 0.75\(\)0.01 & 0.33\(\)0.00 \\  & MCD  & 2.79\(\)0.53 & 2.67\(\)0.15 & 1.00\(\)0.14 & 1.02\(\)0.03 & 0.77\(\)0.03 & 0.31\(\)0.00 \\  & ENS [51; 8; 103] & 27.14\(\)0.08 & 2.46\(\)0.22 & 0.82\(\)0.03 & 1.25\(\)0.03 & 1.06\(\)0.04 & 0.57\(\)0.02 \\  & BBB  & 1.40\(\)0.55 & 2.23\(\)0.04 & 0.91\(\)0.06 & 0.84\(\)0.15 & 0.66\(\)0.05 & **0.31\(\)0.00** \\  & LAP  & **0.60\(\)0.00** & 1.03\(\)0.00 & 0.88\(\)0.00 & 0.57\(\)the next token logits corresponding to possible answers from each dataset and fine-tune the LLM to maximize the likelihood of the correct token. For evaluation, in addition to Accuracy (**ACC**), we use Expected Calibration Error (**ECE**) and Negative Log-Likelihood (**NLL**) to assess the models' uncertainty estimation ability (see Appendix B.2 for details).

**Baselines and Implementation Details.** We compare **BLoB** with state-of-the-art uncertainty estimation methods applied to the LoRA adapters of LLMs, including Monte-Carlo Dropout (**MCD**) , Bayes By Backprop (**BBB**) , Deep Ensemble (**ENS**) [51; 8; 103], and the latest Laplace-LoRA (**LAP**) . We also report the performance of two standard PEFT baseline methods for reference: Maximum Likelihood Estimation (**MLE**)  and Maximum A Posteriori (**MAP**).

For MLE, we use the LoRA implementation. For MAP, we use a weight decay rate of \(1e-5\). For MCD, we use an ensemble of 10 LoRAs with a dropout rate of \(p=0.1\). For ENS, we independently fine-tune 3 LoRAs and average their logits during evaluation. For BBB, we adopt the default settings from the Bayesian-Torch library  and only Bayesianize the **_A_** matrix, similar to BLoB. We sample \(N=10\) times for BBB during test. We re-implement LAP and apply it to the MAP checkpoints. We keep all BLoB-specific hyperparameters consistent across all datasets. Typically, we set the number of samples \(K=1\) during training for all our BLoB experiments, which highlights BLoB's sampling efficiency. As shown in Table 1, we also report BLoB's performance with different numbers of samples during Bayesian inference, where \(N=0\) indicates directly using the mean of the weight distribution for prediction.

### Results on In-distribution Datasets

We fine-tune LLama2-7B on six common-sense reasoning tasks: Winogrande-small (WG-S), Winogrande-medium (WG-M) , ARC-Challenge (ARC-C) , ARC-Easy (ARC-E) , OpenBokQA (OBQA) , and BoolQ . For all baseline methods, using the same pre-trained LLM backbone, we maintain consistent hyperparameters across all datasets and do not use additional validation sets to achieve higher performance (See Appendix B.3 for detailed settings).

Table 1 shows the performance of BLoB compared to the baselines, including ACC, ECE, and NLL, on the in-distribution test set with the pre-trained LLama2-7B model. The high ECE and NLL for MLE indicate overconfidence in LLMs during conventional fine-tuning, except for BoolQ due to its large dataset size. Simple but popular baselines like MAP, MCD, and ENS show mixed results in terms of NLL and/or ECE, highlighting the challenge of uncertainty estimation during LLM fine-tuning. LAP, the most competitive post-training baseline for uncertainty estimation, significantly reduces NLL and ECE on some datasets but lacks consistent performance, as indicated by its failures on ARC-C and ARC-E. BBB mitigates the overconfidence issue in LLMs across almost all datasets, showcasing the advantage of jointly optimizing the mean and covariance of the variational weight distributions during fine-tuning. However, there remains considerable room for improvement.

BLoB consistently achieves better or comparable performance across all datasets. With the number of samples during inference set to \(N=10\), the same as MCD, BLoB provides the best uncertainty estimation performance, significantly reducing NLL and ECE, and greatly mitigating overconfidence while maintaining comparable or better ACC than MLE. Even with half the number of samples, \(N=5\), BLoB still delivers performance comparable to that of \(N=10\) and outperforms other baselines on most datasets. By abandoning the modeling of the posterior distribution, prediction using the mean of the weight distribution, i.e., BLoB (N=0) sacrifices some degree of calibration in exchange for improved accuracy. Appendix C.5 presents the trade-off between accuracy and calibration, which is controlled by the standard deviation of the prior Gaussian distribution).

Besides LLama2-7B, we also include additional results for RoBERTa-base  on text classification tasks in Appendix C.1. Our method consistently achieved either the best or runner-up performance across nearly all datasets, demonstrating its versatility across different architectures.

[MISSING_PAGE_FAIL:9]

Bayesianizing LoRA due to its widespread application in existing works. We also note that BLoB can be naturally adapted to handle different LoRA variants.

**Uncertainty Estimation in Large Language Models.** Large-scale pre-trained models are well-calibrated during pre-training , but fail to accurately express predictive uncertainty during inference [3; 107; 44; 43; 87], especially after fine-tuning [8; 103; 69; 116]. This indicates that measures effective during pre-training [93; 114; 16; 122; 14] may lose their power of uncertainty estimation after fine-tuning for domain-specific knowledge. To address this issue, [27; 123] define priors and approximate posteriors on the full attention weights during fine-tuning, achieving better uncertainty estimation but at a significant cost in time and space. Consequently, recent work integrates Bayesian methods and PEFT for efficient uncertainty estimation. For instance, [8; 103] train and store multiple copies of different LoRAs, ensembling their outputs during inference to achieve somewhat better results.  applies Kronecker factorized Laplace approximation on fine-tuned LoRA. However, such post-training procedures bifurcate posterior approximation into two stages, leading to suboptimal estimation. In contrast, our BLoB enables simultaneous estimation of both the mean and covariance of LLM parameters in a single fine-tuning stage, substantially improving performance.

## 6 Conclusion

In this work, we propose a principled Bayesianization framework for parameter-efficiently fine-tuning LLMs. Our theoretical analysis shows that a full-weight variational distribution can be efficiently optimized by approximately using a low-rank space of the weight update matrices. Our empirical evaluations corroborate this theoretical insight, demonstrating superior generalization and uncertainty estimation capabilities across diverse scenarios compared to various baseline methods. Building on LoRA, our approach seamlessly integrates with existing LLM architectures while imposing minimal additional memory overhead and training time. Our method highlights that jointly learning the mean and covariance of the variational distribution during fine-tuning can mutually improve both, underscoring the powerful potential of Bayesian methods in enhancing the reliability and generalization of LLMs.

## 7 Limitations

The main limitations of our proposed BLoB method are: (i) BLoB is confined to fine-tuning scenarios and is not applicable to training-free tasks, such as direct uncertainty estimation during inference . (ii) As a typical mean-field variational inference method, BLoB requires multiple sampling iterations during inference, which challenges stable and efficient deployment. (iii) While BLoB's effectiveness has been empirically demonstrated for downstream classification tasks, its application to generation tasks requires further investigation.