# Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

In this study, we introduce a novel, probabilistic viewpoint on adversarial examples, achieved through box-constrained Langevin Monte Carlo (LMC). Proceeding from this perspective, we develop an innovative approach for generating semantics-aware adversarial examples in a principled manner. This methodology transcends the restriction imposed by geometric distance, instead opting for semantic constraints. Our approach empowers individuals to incorporate their personal comprehension of semantics into the model. Through human evaluation, we validate that our semantics-aware adversarial examples maintain their inherent meaning. Experimental findings on the MNIST and SVHN datasets demonstrate that our semantics-aware adversarial examples can effectively circumvent robust adversarial training methods tailored for traditional adversarial attacks.

## 1 Introduction

The purpose of generating adversarial examples is to deceive a classifier by making minimal changes to the original data's meaning. In image classification, most existing adversarial techniques ensure the preservation of adversarial example semantics by limiting their geometric distance from the original image [18; 6; 2; 12]. These methods are able to deceive classifiers with a very small geometric based perturbation. However, when targeting robust classifiers trained using adversarial methods, an attack involving a relatively large geometric distance may be necessary. Unfortunately, these considerable distances can be so vast that they ultimately undermine the original image's semantics, going against the core objective of creating adversarial examples. As illustrated in the left portion of Figure 1, when applying the PGD attack  constrained by \(L_{2}\) norm on a robust classifier, the attacked images that successfully deceive the classifier consistently lose their original meaning, which is undesirable.

To counter this problem, we propose an innovative approach for generating semantics-aware adversarial examples. Instead of being limited by geometric distance, our approach hinges on a proposed semantic divergence. Specifically, we treat generating adversarial examples as a box-constrained non-convex optimization problem. We employ box-constrained Langevin Monte Carlo (LMC) to find near-optimal solutions for this complex problem. As LMC samples converge to a stationary distribution, we gain a probabilistic understanding of the adversarial attack. Within this probabilistic perspective, the geometric constraint of the adversarial attack can be viewed as a distribution. By replacing this geometric-based distribution with a semantic-based distribution, we can define a semantics-aware adversarial attack in a principled manner. The corresponding divergence induced by the semantic-based distribution is called semantic divergence. Our semantics-aware adversarial attack is capable of deceiving robust classifiers while preserving most of the original image's semantics, as demonstrated in the right section of Figure 1.

## 2 Preliminaries

### Adversarial examples

The notion of adversarial examples was first introduced by Szegedy et al. . Let's assume we have a classifier \(C:^{n}\), where \(n\) represents the dimension of the input space and \(\) denotes the label space. Given an image \(_{}^{n}\) and a target label \(y_{}\), the optimization problem for finding an adversarial instance for \(_{}\) can be formulated as follows:

\[(_{},_{}) C(_{})=y_{}_{}^{n}\]

Here, \(\) is a distance metric employed to assess the difference between the original and perturbed images. This distance metric typically relies on geometric distance, which can be represented by \(L_{0}\), \(L_{2}\), or \(L_{}\) norms.

However, solving this problem is challenging. As a result, Szegedy et al.  propose a relaxation of the problem:

\[(_{},y_{}):=c_{1} (_{},_{})+c_{2} f (_{},y_{})_{}^{n}\] (1)

Figure 1: **Top left**: Targeted attack on an adversarially trained MadryNet  for MNIST using Projected Gradient Descent (PGD) with \(L_{2}\) norm. To ensure successful targeted attacks in most cases, we increased the \(\) to \(5\). **Bottom left**: Targeted attack on an adversarially trained ResNet18  for SVHN using PGD with \(L_{2}\) norm and \(=5\). **Top right & Bottom right**: Our proposed method applied to targeted attacks on the same MadryNet and ResNet18 for MNIST and SVHN, respectively. A green border signifies a successful deception of the victim classifier, while a red border indicates failure. **Notably, with PGD, a successful attack often results in the alteration of the source image’s semantics, which is undesirable.** Additional PGD attack examples are provided in Appendix E.

where \(c_{1}\), \(c_{2}\) are constants, and \(f\) is an objective function closely tied to the classifier's prediction. For example, in , \(f\) is the cross-entropy loss function, while Carlini and Wagner  suggest several different choices for \(f\). Szegedy et al.  recommend solving (1) using box-constrained L-BFGS.

### Adversarial training

Adversarial training, a widely acknowledged method for boosting adversarial robustness in deep learning models, has been extensively studied [18; 6; 10; 12]. This technique uses adversarial samples as (part of) the training data, originating from Szegedy et al. , and has evolved into numerous variations. In this paper, we apply the min-max problem formulation by Madry et al.  to determine neural network weights, denoted as \(\). They propose choosing \(\) to solve:

\[_{}_{(,y) p_{}}[_{\| \|_{p}}_{}(,+,y)]\] (2)

where \(p_{}\) represents the data distribution, \(_{}\) is the cross-entropy loss, \(\|\|_{p}\) denotes the \(L_{p}\) norm, and \(\) specifies the radius of the corresponding \(L_{p}\) ball. In what follows, we will use the term "robust classifier" to refer to classifiers that have undergone adversarial training.

### Energy-based models (EBMs)

An Energy-based Model (EBM) [9; 4] involves a non-linear regression function, represented by \(E_{}\), with a parameter \(\). This function is known as the energy function. Given a data point, \(\), the probability density function (PDF) is given by:

\[p_{}()=())}{Z_{}}\] (3)

where \(Z_{}=(-E_{}())\) is the normalizing constant that ensures the PDF integrates to \(1\).

### Langevin Monte Carlo (LMC)

Langevin Monte Carlo (also known as Langevin dynamics) is an iterative method that could be used to find near-minimal points of a non-convex function \(g\)[13; 25; 20; 14]. It involves updating the function as follows:

\[_{0} p_{0},_{t+1}=_{t}-}{2}_{x}g(_{t})+_{t},_{ t}(0,I)\] (4)

where \(p_{0}\) could be a uniform distribution. Under certain conditions on the drift coefficient \(_{x}g\), it has been demonstrated that the distribution of \(_{t}\) in (4) converges to its stationary distribution [3; 14], also referred to as the Gibbs distribution \(p()(g())\). This distribution concentrates around the global minimum of \(g\)[5; 24; 14]. If we choose \(g\) to be \(-E_{}\), then the stationary distribution corresponds exactly to the EBM's distribution defined in (3). As a result, we can draw samples from the EBM using LMC. By replacing the exact gradient with a stochastic gradient, we obtain Stochastic Gradient Langevin Dynamics (SGLD) [23; 19].

### Training EBM

To train an EBM, we aim to minimize the minus expected log-likelihood of the data, represented by

\[_{}=_{X p_{d}}[- p_{}(X)]= _{X p_{d}}[E_{}(X)]- Z_{}\]

where \(p_{d}\) is the data distribution. The gradient is

\[_{}_{}=_{X p_{d}}[_{ }E_{}(X)]-_{} Z_{}=_{X p_{d}}[ _{}E_{}(X)]-_{X p_{}}[_{}E_{ }(X)]\] (5)

(see  for derivation). The first term of \(_{}_{}\) can be easily calculated as \(p_{d}\) is the distribution of the training set. For the second term, we can use LMC to sample from \(p_{}\).

Effective training of an energy-based model (EBM) typically requires the use of techniques such as sample buffering and regularization. For more information, refer to the work of Du and Mordatch .

## 3 Generating semantics-aware adversarial examples

In this section, we introduce a probabilistic approach to understanding adversarial examples. Through this lens, we establish the concept of semantic divergence, offering an alternative to conventional geometric distance. This concept of semantic divergence enables individuals to integrate their unique understanding of semantics into the model, thereby facilitating the creation of semantics-aware adversarial examples.

### A probabilistic perspective on adversarial examples

LMC and SGLD are not directly applicable to the optimization problem presented in (1) due to their incompatibility with box-constrained optimization problems. To overcome this limitation, Lamperski  proposed Projected Stochastic Gradient Langevin Algorithms (PSGLA). By employing PSGLA to generate samples near the solution of the optimization problem specified in (1), we obtain the subsequent update rule:

\[_{0} p_{0},_{t+1}=_{^{n}}(_{t}-}{2}_{x}(_{t},y_{ })+_{t}),_{t}(0,I)\] (6)

where \(^{n}\) is a clamp projection that enforces the constraints within the \(^{n}\) interval. We refer to the stationary distribution of PSGLA as the adversarial distribution \(p_{}(;y_{})(-(,y_{}))\), since samples drawn from this distribution are in close proximity to the optimal value of the optimization problem presented in (1).

Then by definition of \(\), the adversarial distribution can be represented as a product of expert distributions :

\[p_{}(_{};_{},y_{ }) p_{}(_{};y_{})p_{ }(_{};_{})\] (7)

where \(p_{}(_{};y_{})(-c_{2} f (_{},y_{}))\) denote the victim distribution and \(p_{}(_{};_{})(-c _{1}(_{},_{}))\) represent the distance distribution.

The victim distribution \(p_{}\) is dependent on the victim classifier. As suggested by Szegedy et al. , \(f\) could be the cross-entropy loss of the classifier. We can sample from this distribution using Langevin dynamics. Figure 2(a) presents samples drawn from \(p_{}\) when the victim classifier is subjected to standard training, exhibiting somewhat indistinct shapes of the digits. This implies that the classifier has learned the semantics of the digits to a certain degree, but not thoroughly. In contrast, Figure 2(b) displays samples drawn from \(p_{}\) when the victim classifier undergoes adversarial training. In this scenario, the shapes of the digits are clearly discernible. This observation suggests that we can obtain meaningful samples from adversarially trained classifiers, indicating that such classifiers depend more on semantics, which corresponds to the fact that an adversarially trained classifier is more difficult to attack. A similar observation concerning the generation of images from an adversarially trained classifier has been reported by Santurkar et al. .

The distance distribution \(p_{}\) relies on \((_{},_{})\), representing the distance between \(_{}\) and \(_{}\). By its nature, samples that are closer to \(_{}\) may yield a higher \(p_{}\), which is consistent with

Figure 2: **(a)** and **(b)** display samples drawn from \(p_{}(;y_{})\) with the victim classifier being non-adversarially trained and adversarially trained, respectively. **(c)** showcases samples from \(p_{}(;_{})\) when \(\) is the square of \(L_{2}\) norm. **(d)** illustrates \(t(_{})\) for \(t\), where \(\) represents a distribution of transformations, including TPS (see Section 4.2), scaling, rotation, and cropping. The \(_{}\)s in (c) and (d) consist of the first 36 images from the MNIST test set.

the objective of generating adversarial samples. Moreover, if \(\) represents the square of the \(L_{2}\) norm, then \(p_{}\) becomes a Gaussian distribution with a mean of \(_{}\) and a variance determined by \(c_{1}\). Figure 2(c) portrays samples drawn from \(p_{}\) when \(\) is the square of the \(L_{2}\) distance. The samples closely resemble the original images, \(_{}\)s, from the MNIST testset, because each sample is positioned near an optimal point, and these optimal points are the original images, \(_{}\)s.

### From Geometric Distance to Semantic Divergence

Based on the probabilistic perspective, we propose a semantic divergence, denoted by a non-symmetric divergence \(_{}(_{},_{}):=E( _{};_{})\), where \(E(;_{})\) represents the energy of an energy-based model trained on a dataset consisting of \(\{t_{1}(_{}),t_{2}(_{}),\}\). Here, \(t_{i}\), and \(\) is a distribution of transformations that do not alter the original image's semantics. In practice, the choice of \(\) depends on human subjectivity related to the dataset. Individuals are able to incorporate their personal comprehension of semantics into the model by designing their own \(\). For instance, in the case of the MNIST dataset, the transformations could include scaling, rotation, distortion, and cropping, as illustrated in Figure 2(d). We assume that such transformations do not affect the semantics of the digits in the MNIST dataset. Consequently, our proposed semantic divergence induces the corresponding distance distribution \(p_{}(_{};_{})(-c _{1} E(_{};_{}))\).

We claim that, given an appropriate \(\), semantic divergence can surpass geometric distance. Empirically, maintaining the semantics of the original image by limiting the geometric distance between the adversarial image and the original image when deceiving a robust classifier is challenging: as shown in Figure 1 and Figure 3, it is difficult to preserve the semantics of the original images. The attacked images either display a'shadow' of the target digits or reveal conspicuous tampering traces, such as in Figure 3(c), where the attacked digit turns gray. This phenomenon was empirically observed and tested by Song et al.  through an A/B test. Conversely, as depicted in Figure 4, the samples from \(p_{}\) neither exhibit the'shadow' of the target digits nor any obvious traces indicating adversarial attack. While semantic divergence can't entirely prevent the generation of a sample resembling the target class, as shown in Figure 4(a), we discuss certain techniques to mitigate this issue in Section 4.1.

A plausible explanation for this is that the utilization of geometric distance causes \(p_{}(,_{})\) to overly focus on \(_{}\). However, when applying semantic divergence induced by a suitable \(\), the density of the distance distribution \(p_{}(,_{})\) spreads out relatively more, resulting in a higher overlap between \(p_{}(,_{})\) and \(p_{}\). This, in turn, provides more opportunities for their product \(p_{}\) to reach a higher value.

## 4 Deceiving robust classifiers

In this section, we present several techniques that enhance the performance of our proposed method in generating high-quality adversarial examples.

### Victim distributions

The victim distribution \(p_{}(c_{2} f(_{},y_{}))\) is influenced by the choice of function \(f\). Let \(g_{}:^{n}^{||}\) be a classifier that produces logits as output with \(\) representing the neural network parameters, \(n\) denoting the dimensions of the input, and \(\) being the set of labels (the output of \(g_{}\) are logits). Szegedy et al.  suggested using cross-entropy as the function \(f\), which can be expressed as

\[f_{}(,y_{}):=-g_{}()[y_{} ]+_{y}(g_{}()[y])=-(g_{}() )[y_{}]\]

where \(\) denotes the softmax function.

Carlini and Wagner  explored and compared multiple options for \(f\). They found that, empirically, the most efficient choice of their proposed \(f\)s is:

\[f_{}(,y_{}):=(_{y}g_{} ()[y]-g_{}()[y_{}],0).\]

From Figure 3 and Figure 4, we observe that \(f_{}\) outperforms \(f_{}\) when the \(p_{}\) depends on either geometric distance or semantic divergence. A potential explanation for this phenomenon is that, according to its definition, \(f_{}\) becomes \(0\) if the classifier is successfully deceived during the iteration process. This setting ensures that the generator does not strive for a relatively high softmax probability for the target class; it simply needs to reach a point where the victim classifier perceives the image as belonging to the target class. Consequently, after the iteration, the victim classifier assigns a relatively low predictive probability to the target class \((g_{}(_{}))[y_{}]\), as demonstrated in Figure 3(d) and Figure 4(d).

In this study, we introduce two additional choices for the function \(f\). Although these alternatives are not as effective as \(f_{}\), we present them in Appendix C for further exploration.

### Data Augmentation by Thin Plate Splines (TPS) Deformation

Thin-plate-spline (TPS)  is a commonly used image deforming method. Given a pair of control points and target points, TPS computes a smooth transformation that maps the control points to the target points, minimizing the bending energy of the transformation. This process results in localized deformations while preserving the overall structure of the image, making TPS a valuable tool for data augmentation.

Figure 4: **(a) & (c): Samples from \(p_{}(;_{},y_{})(-c_{1} (_{},_{}))(-c_{2}  f(_{},y_{}))\), where \(_{}\) refers to the original image of digit “7” shown in Figure 1 and \(y_{}\) refers to class 9. \(\) represents our proposed semantic divergence. In (a), \(f\) is the cross-entropy \(f_{}\), while in (c), \(f\) is \(f_{}\). Constants are set as \(c_{1}=1.0\) and \(c_{2}=10^{-2}\). A green border indicates successful deception of the victim classifier, whereas a red border denotes failure. **(b) & (d)**: The predictive probability (softmax probability) of the target class, corresponding to each digit in Figures (a) and (c) on a one-to-one basis.

As introduced in Section 3.2, we aim to train an energy-based model on transformations of a single image \(_{}\). In practice, if the diversity of the augmentations of \(_{}\), represented as \(t(_{})\), is insufficient, the training of the probabilistic generative model is prone to overfitting. To address this issue, we use TPS as a data augmentation method to increase the diversity of \(t(_{})\). For each \(_{}\), we set a \(5 5\) grid of source control points, \(_{}=\{(x^{(i)},y^{(i)})\}_{i=1}^{5 5}\), and defining the target points as \(_{}=\{(x^{(i)}+_{x}^{(i)},y^{(i)}+_{y}^{( i)})\}_{i=1}^{5 5}\), where \(_{x}^{(i)},_{y}^{(i)}(0,^{2})\) are random noise added to the source control points. We then apply TPS transformation to \(_{}\) with \(_{}\) and \(_{}\) as its parameters. This procedure is depicted in Figure 5. By setting an appropriate \(\), we can substantially increase the diversity of the one-image dataset while maintaining its semantic content.

### Rejection Sampling

Directly sampling from \(p_{}(;_{},y_{})\) does not guarantee the generation of samples capable of effectively deceiving the classifier. To overcome this issue, we adopt rejection sampling , which eliminates unsuccessful samples and ultimately yields samples from \(p_{}(_{}|_{y}g_{}(_{})[y]=y_{};_{},y_{})\).

### Sample Refinement

After rejection sampling, the samples are confirmed to successfully deceive the classifier. However, not all of them possess high visual quality, as demonstrated in Figure 4(c). To automatically obtain \(N\) semantically valid samples1, we first generate \(M\) samples from the adversarial distribution. Following rejection sampling, we sort the remaining samples and select the top \(\) percent based on the softmax probability of the original image's class, as determined by an auxiliary classifier. Finally, we choose the top \(N\) samples with the lowest energy \(E\), meaning they have the highest likelihood according to the energy-based model.

The auxiliary classifier is trained on the data-augmented training set. We do not use the energy of the samples as the sole criterion for selection because some low-visual quality samples may also have a high likelihood. This occurrence is further explained and examined in Appendix D. The entire process of rejection sampling and sample refinement is portrayed in Algorithm 1.

```
0: A trained energy based model \(E(;_{})\) based on the original image \(_{}\), the victim classifier \(g_{}\), an auxiliary classifier \(g_{}\), number of initial samples \(M\), number of final samples \(N\), the percentage \(\).
0:\(N\) adversarial samples \(\). \(=\) for\(0 i<M\)do \(_{} p_{}(;_{},y_{ })\)\(\) Sample from the adversarial distribution. if\(_{y}g_{}(_{})[y]=y_{}\)then\(\) Accept if\(_{}\) deceive the classifier. \(=\{_{}\}\) endif endfor  Sort \(\) by \((g_{}(_{i}))[y_{}]\) for \(i\{1,,||\}\) in descent order \(=(_{i})_{i=1}^{|||}\)\(\) Select the first \(\) percent elements from \(\). Sort \(\) by \(E(_{i};_{})\) for \(i\{1,,||\}\) in ascent order \(=(_{i})_{i=1}^{N}\)\(\) Select the first \(N\) elements from \(\). ```

**Algorithm 1** Rejection Sampling and Sample Refinement

Figure 5: TPS as a data augmentation. **Left**: The original image \(_{}\) superimposed with a \(5 5\) grid of source control points \(_{}\). **Right**: The transformed image overlaid with a grid of target control points \(_{}\).

## 5 Experiment

### Implementation

We implemented our proposed semantics-aware adversarial attack on two datasets: MNIST and SVHN. For the MNIST dataset, the victim classifier we used was an adversarially trained MadryNet . For the SVHN dataset, we utilized an adversarially trained ResNet18, in accordance with the methodology outlined by Song et al. . On the distance distribution side, for every original image denoted as \(_{}\), we trained an energy-based model on the training set, which is represented as \(\{t_{1}(_{}),t_{2}(_{}),\}\). In this case, \(t_{i}\) follows a distribution of transformations, \(\), that do not change the semantics of \(_{}\). For the MNIST dataset, we characterized \(_{}\) as including Thin Plate Spline (TPS) transformations, scaling, and rotation. For the SVHN dataset, we defined \(_{}\) as comprising Thin Plate Spline (TPS) transformations and alterations in brightness and hue. Detailed specifics related to our implementation can be found in Appendix A.

### Evaluation

Our method generates adversarial samples that can deceive classifiers, but it does not guarantee the preservation of the original label's semantic meaning. As such, we consider an adversarial example successful if human annotators perceive it as having the same meaning as the original label, in line with the approach by Song et al. . To enhance the signal-to-noise ratio, we assign the same image to five different annotators and use the majority vote as the human decision, as done in . The screenshot of the annotator's interface is in Appendix B.

In detail, we begin with an original image \(_{}\), its label \(y_{}\), and a target class \(y_{}\). We draw \(M=2000\) samples from \(p_{}(;_{},y_{})\), rejecting those that fail to deceive the victim classifier. After sample refinement, we obtain \(N=100\) adversarial examples, \(_{}^{(i)}\) for \(i\{1,,N\}\). We express the human annotators' decision as function \(h\) and derive the human decision \(y_{}^{(i)}=h(_{}^{(i)})\). As previously mentioned, an adversarial example \(_{}^{(i)}\) is considered successful if \(y_{}^{(i)}\) is equal to \(y_{}\). We then compute the success rate \(s\) as follows:

\[s=^{N}(y_{}^{(i)}=y_{})}{N}\]

where \(\) represents the indicator function.

We randomly select 10 digits, each representing a different class, from the MNIST/SVHN test set to serve as the original image \(_{}\). These are depicted on the left side of Figure 1. For each \(_{}\), we iterate through the target class \(y_{}\) ranging from 0 to 9, excluding the class \(y_{}\) that signifies

Figure 6: The success rates (%) of our targeted unrestricted adversarial attack. Corresponding sample examples for each grid are depicted in the top right and bottom right sections of Figure 1. Refer to Table 1 for overall success rate.

the ground-truth label of \(_{}\). As previously described, for every pair of \(_{}\) and \(y_{}\), we generate \(N=100\) adversarial examples post sample refinement. The result of each pair is illustrated in Figure 6. The overall success rate is illustrated in Figure 1.

### Results

As depicted in Figure 6 and Table 1, our proposed method often succeeds in fooling robust classifiers, all the while preserving the original semantics of the input. It should be noted, however, that this does not occur in every instance.

## 6 Related work

Unrestricted adversarial examplesSong et al.  proposed generating unrestricted adversarial examples from scratch using conditional generative models. In their work, the term "unrestricted" indicates that the generated adversarial samples, \(_{}\), are not restricted by a geometric distance such as the \(L_{2}\) norm or \(L_{}\) norm. The key difference between their approach and ours is that their adversarial examples \(_{}\) are independent of any specific \(_{}\), while our model generates \(_{}\) based on a given \(_{}\). By slightly modifying (7), we can easily incorporate Song's "unrestricted adversarial examples" into our probabilistic perspective:

\[p_{}(_{};y_{},y_{}):=p_{ }(_{};y_{})p_{}(_ {};y_{})\] (8)

where \(y_{}\) is the source class. It becomes evident that the adversarial examples generated by our \(p_{}(;_{},y_{})\) adhere to Song's definition when \(_{}\) is labeled as \(y_{}\).

TPS as a Data Augmentation TechniqueTo the best of our knowledge, Vinker et al.  were the first to employ TPS as a data augmentation method. They utilized TPS as a data augmentation strategy in their generative model for conditional image manipulation based on a single image.

## 7 Limitation

This work's foremost limitation pertains to the inherent difficulties in training energy-based models (EBMs), as underscored in the earlier studies by Du and Mordatch  and Grathwohl et al. . The EBM training process is notoriously challenging, and a notable gap persists between the generation quality of EBMs and that of other widely-used probabilistic generative models, such as variational autoencoders and diffusion models. Consequently, we are currently unable to generate adversarial samples for images with higher resolution.

## 8 Conclusion

In this work, we present a probabilistic perspective on adversarial examples by employing Langevin Monte Carlo. Building on this probabilistic perspective, we introduce semantic divergence as an alternative to the commonly used geometric distance. We also propose corresponding techniques for generating semantically-aware adversarial examples. Human participation experiments indicate that our proposed method can often deceive robust classifiers while maintaining the original semantics of the input, although not in all cases.

    &  \\  & Song et al.  & Our Success Rate \\  MadryNet  on MNIST & 85.2 & **96.2** \\ ResNet18  (adv-trained) on SVHN & 84.2 & **86.3** \\   

Table 1: Success rate comparison between the method proposed by Song et al.  and ours. The results presented in this table are for reference only, as Song’s results are taken directly from their paper, and we did not use the same group of annotators for our evaluation.