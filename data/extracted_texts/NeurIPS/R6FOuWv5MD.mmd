# Understanding Model Selection for Learning in Strategic Environments

Tinashe Handina

Computing + Mathematical Sciences

California Institute of Technology

Pasadena, CA 91125

thandina@caltech.edu

Correspondence to Tinashe Handina <thandina@caltech.edu>.

&Eric Mazumdar

Computing + Mathematical Sciences

California Institute of Technology

Pasadena, CA 91125

mazumdar@caltech.edu

Correspondence to Tinashe Handina <thandina@caltech.edu>.

###### Abstract

The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model class one optimizes over--and the more data one has access to--the more one can improve performance. As models get deployed in a variety of real-world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects the relationship between performance at equilibrium and the expressivity of model classes. We find that strategic interactions can break the conventional view--meaning that performance does not necessarily monotonically improve as model classes get larger or more expressive (even with infinite data). We show the implications of this result in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning. In particular, we show that each of these settings admits a Braess' paradox-like phenomenon in which optimizing over less expressive model classes allows one to achieve strictly better equilibrium outcomes. Motivated by these examples, we then propose a new paradigm for model selection in games wherein an agent seeks to choose amongst different model classes to use as their action set in a game.

## 1 Introduction

Machine learning--and deep learning in particular-- has already demonstrated enormous potential to enable new services across a wide spectrum of everyday life. Examples range from chatbots , to hiring , and content moderation . Driving this proliferation is the fact that the increasing availability of compute resources coupled with the abundance of data provided by internet-scale datasets allows one to train larger and larger models while monotonically improving performance . Despite signs of diminishing returns, this consensus has broadly held true. Machine learning algorithms tend to follow a monotonic scaling law: with more compute and data, one can train more expressive models and eke out performance gains.

As algorithms are deployed into real-world scenarios, however, they will inevitably come into contact with some form of strategic decision-making--whether that be in the form of adversarial agents attempting to manipulate the output of the algorithm , gig-workers taking actions to enforce better working conditions from learning-powered platforms , or more broadly individuals whose goals are misaligned with those of the algorithm .

Reflecting this reality, recent years have seen a surge in research seeking to understand the effects of strategic decision-making on learning algorithms. Two sub-fields in particular include _strategic_ classification --in which one seeks to learn a classifier or predictor in the presence of agentswho strategically manipulate data-- and multi-agent reinforcement learning -- in which agents attempt to learn optimal decision-making policies in the presence of other learning agents. Both of these domains draw on ideas from game theory and economics to understand how to design algorithms for strategic settings.

In these different regimes, a common refrain is that the presence of strategic interactions invalidates many of the foundational assumptions underlying many machine learning algorithms. For example, strategic interactions result in highly non-stationary environments for multi-agent reinforcement learning (MARL)  and seemingly innocuous design decisions like stepsize choices and gradient estimators have been shown to give rise to qualitatively different outcomes in strategic classification . Despite these works, our understanding of model class selection in strategic settings remains underdeveloped. To that end, in this paper, we consider the following question:

_How do strategic interactions affect the relationship between model class_

_expressivity and equilibrium performance?_

Contributions:We show through simple theoretical models, illustrative examples, and experiments that strategic interactions can yield a non-trivial relationship between model class expressivity and equilibrium performance. In particular, we show how--even in highly structured regimes in which one has full access to the underlying data distribution--strategic interactions can result in a Braess' paradox-like phenomenon: the larger and more expressive the model class a learner optimizes over, the lower their performance at equilibrium.

To understand why this is possible, we make links with the literature in economics on comparative statics, which seeks to understand how the equilibria of games vary with exogenous factors. We show that _even_ in convex games with a unique equilibrium, if the equilibrium is _not_ Pareto optimal (i.e., there exists coordinated deviations that improve the utilities of both players), then there always exists a _unilateral_ restriction of one's action set over which one could have played and had a better equilibrium outcome. Conversely, we show that if an equilibrium is Pareto optimal (which encompasses not only traditional optimization but also adversarial games), performance at equilibrium will tend to scale monotonically with respect to model class expressivity. To make this result concrete, we give examples of strategic regression, strategic classification, and MARL in which _reverse_ scaling occurs.

Our result suggests that--if the model will be deployed into a strategic environment-- the choice of model class should be treated as a strategic action. Following up on this observation, we formulate a problem of model-selection in games. Whereas learning in games traditionally takes the action set for a player as given, we propose a new formulation in which a player has a number of action sets to choose from and must find the one that yields the best payoff. As a proof-of-concept, we provide an algorithm to identify the best set in a class of structured games.

### Related work

Before describing our model and results, we comment on related work in both machine learning and economics.

**Scaling laws in Machine Learning:** Within statistical machine learning, the study of scaling laws is motivated by the task of choosing a sufficiently expressive class of models to optimize over a given dataset . Non-monotonicity of scaling laws emerged classically due to overfitting --in which the model is too expressive relative to the size of the data set, which can degrade performance at deployment. Such problems are fundamentally linked to understanding the behavior of the empirical risk as one optimized over larger and larger model classes .

Deep learning upended this way of thinking with the development of a theory for generalization beyond what is called the threshold of interpolation. More recently, work has investigated how the performance of large language models scales with expressivity (measured in the number of parameters)  and the size of the dataset it is trained on . In each of these works, the scaling laws increase monotonically in both the amount of data and expressivity.

In our paper, we sidestep issues of sample complexity (i.e., dataset size) to isolate the interplay between expressivity and strategic interactions. While the minimum of the population risk for supervised learning monotonically decreases because optimizing over a larger space can only improve performance, we show that the population risk is non-monotonic in strategic environments. Thus, the phenomenon that we highlight holds even without consideration of sample sizes or generalization errors and adds to a growing body of literature on the difficulties of learning in game theoretic environments.

**Learning in strategic environments:** Recent years have seen a surge of interest in understanding the effects of strategic interactions on learning algorithms. Some of the most relevant areas of interest are strategic classification  and performative prediction , adversarial machine learning , and multi-agent reinforcement learning . A unifying theme across these areas is the integration of ideas from game theory into problems of machine learning, wherein one seeks to learn an optimal model given the presence of strategic agents who may themselves be learning. For example, papers on strategic classification , strategic regression , and participation dynamics [21; 22] all analyze games in which a learner deploys learning algorithms in game-theoretic environments. Similarly, work on MARL naturally builds upon the foundation of Markov games [23; 24].

One can view all of these problems as an instance of learning in games --which has seen a resurgence in the machine learning literature in recent years due to these connections [26; 27]. In this paper, we adopt, in particular, the framework of continuous games  in which players' action sets can be compact convex subsets of \(^{n}\).

In this class of games, recent work has made clear that learning can be much more complex than in stationary environments-- with non-trivial consequences including instability and convergence to cycles and chaos when using gradient-based learning , small design choices like stepsizes and gradient estimators leading to different equilibria , and strategic manipulations allowing for better causal discovery .

The question of whether our current understanding of scaling laws holds in these environments is still relatively understudied. Recent empirical work has shown that scaling laws in zero-sum MARL mirror those for deep reinforcement learning and deep learning more generally . Most relevant to our work is a recent paper that studied the non-monotonicity of users' social welfare as firms deploy larger and larger models . The paper considers an environment in which multiple firms compete over a set of users and analyzes the welfare of the users as all firms choose more complex models. They show through a simple model and extensive experiments that if all firms use larger models, the users' welfare can decrease. In this paper, we formulate a more general model that encompasses their interaction and more general problems of strategic classification and MARL. We take an orthogonal track, which is to ask whether it is rational for self-interested learners to _unilaterally_ restrict the expressivity of their models in strategic settings. We show that this is indeed the case under certain conditions.

**Changing equilibrium outcomes in game theory:** Finally, we would be remiss if we did not discuss the large body of work in economics that studies changes in equilibrium outcomes in games. A similar phenomenon to the one we highlight is the well-known Braess' paradox in strategic routing  in which one can add a road to a network and increase congestion. Even more related is the _informational_ Braess' paradox  in which more information over the network can yield worse equilibrium outcomes for agents in routing games. Many classic works in dynamic game theory have also highlighted the unintuitive ways information and statistical estimation affect equilibrium outcomes in games [35; 36].

More generally, a large body of work in economics studies comparative statics--i.e., how equilibrium payoffs change as exogenous variables are changed . The literature has mostly been concerned with deriving conditions under which payoffs change monotonically in the exogenous variables, a field known as a _monotone_ comparative statics . Our work can be seen as an attempt to understand these ideas in the context of strategic machine learning.

## 2 Preliminaries

To understand the dependencies between strategic decision-making and model complexity, we examine different strategic environments. In our model, the learner has access to an ordered set of model classes \(\), which are all subsets of one large class \(\).

**Definition 2.1**.: A set of model classes \(=\{_{k}\}_{k=1}^{N}\) is _ordered_ if for all \(_{i},_{j}\), if \(i<j\) it implies that \(_{i}_{j}\)The set \(\) may be a set of nested policy classes in MARL or a set of neural network architectures of increasing size for strategic classification. Importantly, the model classes have monotonically increasing expressivity when measured in classic notions of expressivity like V-C dimension .

Before engaging with the strategic environment, the learner chooses a model class \(_{i}\) over which to optimize. In some instances, we refer to model classes as action spaces, and we use these two terms interchangeably. The selection of a model class fixes the optimization problem the learner will then attempt to solve through interactions with the environment. We model interactions with the environment as a two-player game and assume that players find equilibrium outcomes. We assume the learner has a loss function \(f_{l}:\) which they seek to minimize that also depends on the action of the environment. Similarly, the environment will have a loss function \(f_{e}:\). Here \(\) is the learner's action space whilst \(\) is the environment's action space.

To model different strategic interactions, we make different assumptions on the nature of the game played and the equilibrium outcomes. We focus on four types of strategic environments: Stationary Environments where the environment actor only has a single action, Stackelberg Environments where the Learner leads, Stackelberg Environments where the learner follows, and General Nash Environments. We provide a concrete description of each of these environments in Appendix A.1.

In the next section, we analyze General Nash environments and show that payoffs do not necessarily monotonically increase in expressivity. In Appendix A.2, we provide a set of theoretical results that show how equilibrium payoffs _are_ monotonically increasing in expressivity in Stationary and Stackelberg environments where the learner leads. We then show in Appendix A.3 how payoffs also do not necessarily monotonically increase in expressivity in Stackelberg environments where the learner follows.

## 3 Non-Monotonic Scaling of Performance in Nash Settings

In this section, we present our investigation of the relationship between model class expressivity and equilibrium performance in Nash setting. We show how _even_ under strong assumptions on the regularity of the game, there always exists a way for a player to restrict their model class, resulting in a game with a Nash equilibrium that has a lower loss if the original Nash equilibrium is not Pareto optimal (i.e., the game is not zero-sum or strategically zero-sum). We note that this is a negative result which is an existence proof. To concretely establish this phenomenon, we illustrate through examples in multi-agent reinforcement learning and strategic classification how in settings where these assumptions are relaxed, this phenomenon still exhibits itself.

To prove our main result, we assume the two-player game is strongly monotone on the space \(^{n}\).

**Definition 3.1**.: A two-player game is \(\)-strongly monotone if the generalized gradient operator \(F:^{n}\) given by:

\[F(x)=_{}f_{l}(,e)\\ _{e}f_{e}(,e)\ \ x=(,e),\]

satisfies:

\[ F(x)-F(x^{}),x-x^{}\|x-x^{}\|^{2}\ \ \,x,x^{}\ \ \]

A strongly monotone game is a convex game . Implicitly, it assumes that the two players' losses are \(\) strongly convex in their own action and makes a further assumption on the interaction between players' actions . The assumption of strong monotonicity ensures that there is always a unique Nash equilibrium and that issues of multiple equilibria do not arise.

This assumption is once again made to isolate the phenomenon of interest. In the case with multiple Nash equilibria we believe that it is possible to have different equilibrium outcomes exhibiting different scaling behavior--though we leave such analyses for future work. We remark that we make these assumptions for illustrative purposes and that many of our numerical experiments show the same result under milder game structures.

On top of the assumption of strong monotonicity we require several smoothness conditions on the players' objectives as well as an assumption that their interaction is not trivially zero at Nash.

**Assumption 3.2**.: Assume the game defined on \(f_{e}\) and \(f_{l}\) is strongly monotone on \(\). Further assume that 1. \(f_{l}\) and \(f_{e}\) are jointly convex in \(\) and \(e\).
2. The gradient mappings, \( f_{l}\) and \( f_{e}\) exist and are well defined for all \((,e)\). Furthermore, the gradient mappings are \(L\)-Lipschitz continuous in the joint action space.
3. The Nash equilibrium \(^{*}\) is on the interior of \(\) with \(_{}BR_{e}(^{*}) 0\).

To show how the restriction of a model class yields a decrease in loss in a large class of games, we leverage the idea that in many games, a Nash equilibrium is not necessarily a Pareto optimal point .

**Definition 3.3**.: A point \((,e)\) is Pareto-optimal, if there _does not_ exist \((,)\) such that \(f_{l}(,)<f_{l}(,e)\) and \(f_{e}(,) f_{e}(,e)\)2

Given these assumptions, we prove the following theorem. For ease of exposition, we defer the proof to Appendix B.1.

**Theorem 3.4**.: _For a two-player monotone game \(G\) on \(\) which satisfies Assumption 3.2, if the unique Nash equilibrium in \(\), \((^{*},e^{*})\), is not Pareto optimal then there exists a restriction of the learner's model class (i.e., a set \(^{}\)) such that the restricted game \(G^{}\) on \(^{}\) admits a Nash equilibrium \((^{},e^{})\) with: \(f_{l}(^{},e^{})<f_{l}(^{*},e^{*})\)._

This theorem highlights the fact that the non-monotonicity of scaling laws is, in fact, something we should expect in large classes of games. Indeed, even under mild conditions one can show that there always exists a unilateral restriction that improves payoffs.

While the theorem guarantees the existence of a unilateral restriction, which improves equilibrium performance, we remark that it does not say anything about the ease with which one can find this restricted space. While the proof is constructive, it makes use of information of the environment's loss to construct the set--information that may not always be available to the learner. Furthermore, as we show in the following examples, the non-monotonicity can play out in complex ways.

Example 1: Multi-Agent Reinforcement LearningWe first demonstrate an extreme form of the reverse scaling predicted by Theorem 3.4 in the context of multi-agent reinforcement learning. To do so, we construct a Markov game in which the more the learner restricts their policy class, the more their expected payoff increases. Note that in keeping with the language of MARL, we consider the case when both players would like to maximize their long-run discounted rewards.

The Markov game in question is a two-player game with \(n\) states. In each state \(s_{i}\), with \(i\{1,2,,n\}\), both players have two actions available to them \(\{0,1\}\) with 0 corresponding to the top

Figure 1: (a.) A visual description of a 2-player Markov game in which the learner can unilaterally increase their payoff by restricting the expressivity of their policy class. (b.) the payoff of the learner at Nash in a 50-state version of this Markov game as their policy class is restricted to take the form \(_{l}(s)=[p,1-p]\) in all states \(s\) for \(p[1-,]\) for different discount factors (assumed to be the same for both players). In all cases, we see the learnersâ€™ payoff broadly _increase_ at Nash as they optimize over smaller policy classes.

row or left column. In each state, the environment is allowed to choose a policy that is unrestricted, meaning that \(_{e}(s)=[p,1-p]\) for any \(p\). The learner can choose from policies such that: \(_{l}(s)=[p,1-p]\) for all \(p[1-,]\) for some \([0.5,1]\). Varying \(\) generates model classes of varying expressivity. For example, when \(=1\), then they are allowed to choose any policy, and for \(=0.5\), they are constrained to only playing uniform policies.

Given actions \(a,b\) from the learner and environment, respectively, we define the transition probabilities as:

\[p(s_{i+1}|s_{i},a=0,b=1)=p(s_{i+1}|s_{i},a=1,b=1)=1\]

\[p(s_{i}|s_{i},a=0,b=0)=p(s_{i}|s_{i},a=1,b=0)=1\]

Thus, the transitions are deterministic, given the action of the environment. This results in the following utilities for the two players, which are simply their sum of discounted rewards3:

\[u_{i}(_{l},_{e})=_{_{1},_{e}}[_{t=0}^{} _{i}^{t}R_{i}(s_{t},a_{t},b_{t})]\]

where \(i\{e,l\}\) and \(_{e},_{l}\) are the player's discount factors. We construct the payoffs for the learner such that they have a dominant strategy of \(_{l}(s)=[,1-]\) in all states, and their expected cumulative payoff increases as the players end up further along the chain of states (as seen in Figure 1).

We construct the payoffs of the environment player such that they trigger a switch to the next state if and only if the probability that the learner puts on action \(0\) is below some threshold \(p^{*}(s)\). We do so for a sequence of thresholds \(p^{*}_{i}\) for \(i=1,...,n\) such that \(p^{*}_{i}>p^{*}_{i+1}>0.5\). With this construction, for \(p^{*}_{i+1}<<p^{*}_{i}\) the game will result in the players staying in state \(s_{i}\) for all time. More details on the construction of the payoff matrices are left to Appendix E and a plot of the equilibrium rewards for the learner as a function of \(\) is shown in Figure 1 for different discount factors for games having \(n=50\) states.

We empirically observe that as \(\) decreases (i.e., the policy class of the learner is restricted), the performance of the learner behaves in non-monotonic ways and can, in fact, be made to increase as the policy class gets closer to the uniform policy. The highly non-convex nature of the case where \(=0.95\) also highlights the difficulty in choosing a model class in general since it can be posed as a non-convex optimization problem.

A key takeaway of this example is that in general-sum MARL, restrictive policy parametrizations like e.g., softmax policies or function approximation may not lead to worse performance at equilibrium like in competitive and single-agent RL . Indeed our example suggests that the payoff in quantal response equilibria  of Markov games (i.e., equilibria in which agents constrain their strategies to a class of quantal responses-see e.g., ) can sometimes have a higher payoff than the unrestricted Nash equilibrium.

Example 2: Participation dynamicsOur second example is similar to problems considered in the literature on performative prediction  though the setup we consider also fits the literature on understanding participation dynamics  and algorithmic collective action .

In this model, there is a base distribution \(_{0}\) over the input-output space \(\) where \(\) is feature space and \(\) is the output space. The learner is trying to perform supervised learning to learn a mapping \(g:\). The environment, on the other hand, takes the form of a population of agents that selects a distribution on \(\) on the input-output space (i.e., \((,)\)) to maximize their own utility which depends on the choice of the learner.

The least restrictive class of models the learner has access to \(\) is the set of all functions \(g:\). We also consider a restricted function class \(\) which is the class of all functions \(g_{r}:^{}\) where \(^{}\) is the result of some feature mapping \(:^{}\). Thus, \(\) is the space of all functions of the form \(g_{r}((x))\). Clearly \(\).

We assume that the strategic manipulations of the environment take the form of manipulations to the data distribution which take place by mixing the base distribution \(_{0}\) with a manipulated data distribution \(_{e}\) such that the distribution seen by the learner is given by \(=_{e}+(1-)_{0}\) for some \(\). The parameter \(\) relates to the strength of the response distribution within the mixture that the learner observes. It might represent the fraction of the population that engages in strategic manipulations of their data.

Finally, we assume that the learner is optimizing the zero-one loss, such that for any distribution \(\), the best response \(g\) or \(g_{r}\) is the Bayes-Optimal classifier on \(\) and \(^{}\) respectively are:

\[g^{*}(x)=_{y}(y|x)\ \&\ g_{r}^{*}(x)=_{y}(y| (x)).\]

Thus, the learner's loss at equilibrium is given by:

\[f_{l}(g^{*},) =Pr(g^{*}(x) y)\] \[f_{l}(g^{*}_{r},) =Pr(g^{*}_{r}(x) y),\]

in each case, the probability is taken with respect to \(\).

For the population of strategic agents, we assume that they would like the learner to avoid making use of certain 'protected' features and focus on a set of restricted features \(^{*}\). To do so, the strategic agents' response to the learner's model depends on the set of features it makes use of. Concretely, if the learner makes use of a set of features \(^{}:^{}\) that are more informative than some \(^{*}:^{*}\)--i.e., \(^{*}^{}\), then the strategic agents add uniform noise to the base distribution, and if not they report their true data. This can be represented by the following utility function:

\[f_{e}(g,)=TV(_{e},U):Pr(g(x) g(^{* }(x)))>0\\ TV(_{e},_{0})\]

where \(U\) is the uniform distribution on \(\) and \(TV\) represents the \(TV\) distance between distributions.

As we will show, for sufficiently large \(\), the learner is always better off optimizing over the less expressive model class at equilibrium. To do so, we assume that \(^{*}\) preserves enough information for the Bayes optimal classifier on the space \(^{*}\) to be strictly better than random choice.

**Assumption 3.5**.: Let \(||=n\). Assume that the Bayes optimal classifier on \(^{*}\) for \(_{0}\) denoted \(g_{r}^{*}(x)=_{y}(y|^{*}(x))\) satisfies:

\[f_{l}(g_{r}^{*},_{0})=Pr(g_{r}^{*}(x)=y)<\]

This leads to the following result for this game.

**Proposition 3.6**.: _Under Assumption 3.5, consider two functions classes over which the learner can optimize, \(\), and \(\) which is the set of all functions from \(g_{r}:^{*}\), where \(^{*}=^{*}()\) such that \(^{*}(x)=x\) for \(x^{*}\). Consider the corresponding games denoted \(G\) and \(G^{*}\), respectively. Then the Nash equilibrium in \(G\) is \((g^{*},^{*})\) where \(^{*}=(1-)_{0}+ U\) and the Nash equilibrium in \(G^{*}\) is given by \((g_{r}^{*},_{0})\). Furthermore, there exists a range of \((0,1)\) such that:_

\[f_{l}(g^{*},^{*})>f_{l}(g_{r}^{*},_{0})\]

The proof of this proposition can be found in Appendix B.2. This proposition highlights the fact that interactions with strategic agents can make less expressive function classes yield better performance in strategic settings.

## 4 Online Learning for Model Selection in Games

The previous results emphasize the importance of careful model selection in strategic environments. In this section, we consider the problem of learning the best model class to optimize in strategic environments.

Due to the unknown and non-stationary nature of the environment, in game theoretic settings, the learner will have to interact repeatedly with the environment to learn which model class and, consequently, which strategy to play. Thus, we formulate a problem of learning in games in which the learner seeks to find the best model class across a set of candidate model classes as well as the best strategy. We frame this as a problem of _model selection_ for games.

We remark that model-selection is an area of recent interest in online learning [42; 43], though--to the best of our knowledge--the paradigm has not been applied to games as yet. Most similar to this problem is a line of work on meta-learning in games, which seeks to find good strategies that generalize across environments .

We describe an algorithm for how the learner can select model classes to identify which model class to use. As a proof-of-concept, we assume that all players use stochastic gradient descent and adopt the structure of a problem we analyzed in the Nash environment regime. In particular, we assume the learner has access to sets of subsets of \(=^{d}\) and that their loss and the environment's loss satisfy the following generalization of Assumption 3.2. For simplicity, we let the tuple of a particular model and the environment action be denoted by \(x\) (i.e., \((,e)=x\)). We note here that \(F\) is the generalized gradient mapping as described in Definition 3.1.

**Assumption 4.1**.: Assume the game defined on \(f_{e}\) and \(f_{l}\) is strongly monotone and that they are \(L\)-Lipschitz continuous on \(\). Further, assume that the players have access to stochastic gradient estimators such that the estimated monotone mapping \(\) satisfies, \( x\):

\[[(x)]=F(x)\ \ \ \ [\|(x)-F(x)\|^{2}] ^{2}.\]

Given this assumption and under the simplifying assumption that all players use decreasing stepsizes, we assume that for a given model class \(_{i}\) the players engage in projected stochastic gradient descent of the form:

\[x_{t+1}=_{_{i}}(x_{t}-_{t}(x_{t}) ),\]

where \(\) denotes the Euclidean projection onto \(_{i}\). The pseudocode for this is described in Algorithm 1. We show that the running average of the iterates resulting from running this algorithm in an environment satisfying Assumption 4.1 concentrates quickly around the payoff at the Nash equilibrium. The proof of this proposition can be found in Appendix C.

**Proposition 4.2**.: _Let \(\) correspond to a particular model class which results in an instance of continuous action \(-\)strongly monotone game with a unique Nash Equilibrium \(x^{*}\). Under Assumption 4.1 and the assumption that all players use stepsize schedule \(_{t}=\), for any \((0,1)\) Algorithm 1 yields an estimate \(_{T}\) such that:_

\[|f_{l}(_{T})-f_{l}(x^{*})|(()+L^{3}}{^{2}T}),\]

_with probability at least \(1-\)._

To derive this bound, we generalize an existing result from convex optimization . Given these confidence bounds, we now propose a successive elimination algorithm for identifying the best model class in a game. The underlying assumption remains that the environment player is simply doing stochastic gradient descent. This should also extend to the case when the environment performs stochastic mirror descent . The specific form of successive elimination is described in Algorithm 2.

```
1:procedurePSGD(\(,x_{0},T\))
2:for\(t 1\)to\(T\)do
3:\(_{t}=\)
4:\(x_{t+1}_{}(x_{t}-_{t}(x_{t}))\)
5:endfor
6:\(_{T}=_{t=1}^{T}x_{t}\)
7:endprocedure ```

**Algorithm 1** Stochastic gradient descent to find Nash equilibrium in a strongly monotone game

As we show, this algorithm has strong properties in terms of identification of the best model class due to the fast concentration of our estimator from Proposition 4.2. We show the results with respect to identification and defer the proof to Appendix C.

**Proposition 4.3**.: _Under the assumptions of Proposition 4.2, let \(=\{_{i}\}_{i=1}^{n}\). With probability at least \(1-\), Algorithm 2 identifies the model class whose Nash equilibrium yields the highest payoff after:_

\[(()+L^{3})}{^{2}^{*} })\]interactions with the environment, where \(^{*}\) is the minimum suboptimality gap of the Nash equilibrium of a function class compared to that of the best function class._

This result indicates that finding the best model class out of a set of candidate model classes may be computationally tractable in certain regimes. An interesting question that we leave for future work is whether it is possible to be no regret, not just within a model class, but across a set of model classes as well.

## 5 Conclusion

This work seeks to provide a framework for understanding the complexities that arise when models are released into strategic environments. We show that the prevailing understanding of scaling laws in machine learning fails to hold in large classes of strategic environments and show its implications for MARL and strategic classification, among other areas. Lastly we highlight a possible algorithmic solution to overcoming the problem of model selection in games in which we were able to design an algorithm to efficiently learn the best model class to optimize over without sacrificing performance in terms of regret.

Altogether, our results are a first step towards understanding scaling laws and hence, model selection in strategic environments. Our results suggest that we need to rethink our understanding of scaling laws before blindly deploying ever more complex models into real-world environments in which they will be faced with strategic behaviors. We leave many avenues of future work open, including questions about generalization and finite sample considerations, as well as the potential for more sophisticated algorithmic approaches to model selection.