# Higher Order Equivariant Graph Neural Networks

for Charge Density Prediction

 Teddy Koker  Keegan Quigley  Lin Li

MIT Lincoln Laboratory

Lexington, MA 02421

{thomas.koker,keegan.quigley,lin.li}@ll.mit.edu

###### Abstract

The calculation of electron density distribution in materials and molecules is central to the study of their quantum and macro-scale properties, yet accurate and efficient calculation remains a long-standing challenge in the field of material science. This work introduces ChargeE3Net, an E(3)-equivariant graph neural network for predicting electron density in atomic systems. Unlike existing methods, ChargeE3Net achieves equivariance through the use of higher-order tensor representations, and directly predicts the charge density at a set of desired locations. We demonstrate the effectiveness of ChargeE3Net on large and diverse sets of molecules and materials, where it achieves state-of-the-art performance over existing methods, and scales to larger systems than what is feasible to compute with density functional theory. Through additional experimentation, we demonstrate the effect of introducing higher-order equivariant representations, and why they yield performance improvements in the charge density prediction setting.

## 1 Introduction

Electronic charge density is one of the most fundamental quantities in quantum chemistry and physics and it is key to accurately modeling molecules and materials at the atomic scale. Hohenburg-Kohn theorem  states that the ground-state charge density contains the information necessary to obtain all ground-state properties of interest. Density functional theory (DFT) that solves the Kohn-Sham (KS) equations is the most widely used _ab initio_ method for performing electronic structure calculation of molecules or materials. However, KS DFT is computationally expensive with \(O(N^{3})\) complexity, making it infeasible for large-scale quantum calculations.

More recently, machine learning models have been developed to overcome the computational challenge of _ab initio_ calculations. Early approaches using machine learning for the prediction of electron density were based on the use of symmetry-adapted Gaussian process regression to predict coefficients for atom-centered basis functions [2; 3]. Coefficients are predicted from a kernel function that expresses the structural similarity and geometric relationship among a target atomic environment and a training set of atomic environments. More recent work has leveraged invariant and equivariant neural networks to predict these coefficients from atomic features. Qiao et al.  uses mean-field electronic structure, computed from the GFN-xTB  quantum mechanical model, as inputs for a higher-order equivariant neural network to predict basis set coefficients. Rackers et al.  use a higher-order equivariant neural network to predict basis set coefficients directly from atomic graphs. While these approaches are able to achieve high accuracy in some settings, they are limited by the expressivity of density fitting basis sets. These atom-centered basis sets must be hand selected, and are often not sufficient for periodic systems where plane wave basis functions are more appropriate.

Alternatively, several methods were proposed to learn electron density directly from a discretized grid of density points. By inserting each grid or "probe" point into the atomic graph, charge density

[MISSING_PAGE_FAIL:2]

### Architecture

We represent predicted charge density \(()\) as a neural network with inputs of atomic numbers \(\{z_{1},...,z_{N}\}\) with respective locations of the atoms \(\{_{1},...,_{N}\}^{3}\), as well as probe locations \(\{_{1},...,_{M}\}^{3}\) where charge densities are to be predicted, and an optional periodic boundary cell \(B^{3 3}\) for periodic systems.

As illustrated in Figure 1, a graph is constructed with atoms and probe points as vertices, with edges formed via proximity with a cutoff of 4 A. Our graph neural network is formulated such that message passing between atom and probes is unidirectional, with probe point only receiving messages. Through each layer \(n\) of the network, atoms and probe points will maintain tensor representations \(A^{n}\) and \(P^{n}\) respectively (section 2.1). \(A^{n}\) is initialized as a one-hot encoded \(z\), represented as a \(=0\), \(p=1\) tensor with \(N_{}\) equal to the number of atomic species. \(P^{n}\) is initialized as single scalar zero, with \(=0\), \(p=1\), and \(N_{}=1\). Each representation is updated through a series of alternating convolution \(()\) and non-linearity \(()\) layers. Atom representations are updated with \(A_{i}^{n+1}=(_{}^{n}(_{i},A_{i}^{n}))\). \(_{}\) is defined as:

\[_{}^{n}(_{i},A_{i}^{n})=W_{1}^{n}(_{j N (i)}W_{2}^{n}A_{j}^{n} R(r_{ij})Y(_{ij}))+W_{3}^{n}A_{i}^{n}\] (2)

where \(W_{1},W_{2},W_{3}\) are learned weights applied as a linear mix or self-interaction . The set \(N(i)\) includes all atoms within the cutoff distance, including those outside potential periodic boundaries. \(r_{ij}\) is the distance from \(_{i}\) and \(_{j}\), with unit vector \(_{ij}\). \(Y(_{ij})\) are spherical harmonics, and \(R(r_{ij})^{N_{}}\) is a learned radial basis function defined as:

\[R(r_{ij})=([_{1}(r_{ij}),...,_{N_{}}(r_{ij})])\] (3)

where \(()\) is a two layer multilayer perceptron with SiLU non-linearity , \(()\) is a gaussian basis function \((r_{ij})_{k}(-(r_{ij}-_{k})^{2})\) with \(_{k}\) uniformly sampled between zero and the cutoff, then normalized to have a zero mean and unit variance.

The convolution updates the representation of each atom to be the sum of tensor products between neighboring atom representations and the corresponding spherical harmonics describing their relative angles, weighted by a learned radial representation. This sum is followed by an additional self-connection, then a residual self-connection. The output of the convolution is then passed though an equivariant non-linearity \(()\) operation as described in . We use SiLU and \(\) activation functions for even and odd parity scalars respectively, as is done in .

Probe representations are updated similarly as the atoms for each layer, except their representations depend solely on the representations of neighboring atoms, with no probe-probe interaction. Each

Figure 1: Illustration of single charge density probe point. shown in black, in a periodic atomic system. A graph is formed through neighboring atoms, and messages composed of scalars (\(=0\)), vectors (\(=1\)) and higher-order tensors (\( 2\)) are aggregated at probe point vertices.

probe representation is updated with \(P_{k}^{n+1}=(_{}^{n}(_{k},P_{k}^{n}))\), where \(_{}\) is defined as:

\[_{}^{n}(_{k},P_{k}^{n})=W_{1}^{n}(_{i N (k)}W_{2}^{n}A_{i}^{n} R(r_{ik})Y(_{ik}))+W_{3}^{n}P_{k}^{n }.\] (4)

Note that weights \(W\) are not shared with those for \(_{}^{n}()\). Since atom representations are computed independently of probe positions, they can be computed once per atomic configuration, even if multiple batches are required to obtain predictions for all probe points. Finally, predicted charge density \((_{k})\) is computed as a linear combination of the scalar elements of the final representation \(P_{k}^{n=N_{}}\).

## 3 Experiments

We train and evaluate our models on VASP  computations of the organic molecules within the QM9 dataset , mixed transition metal layered oxide lithium ion battery cathode materials (NMC) , and inorganic materials collected from Materials Project (MP) . The QM9 and NMC datasets are provided by Jorgensen and Bhowmik . For the MP data, we collected 122,689 structures and associated charge densities from api.materialsproject.org1. Structures in the dataset that shared composition and space group were identified as duplicates and only the highest material_id structure was included, leaving 108,683 materials. The data was split into training, validation, and test splits with sizes 106,171, 512, and 2000 respectively. 26 materials in the test set were later found to have unrealistic volume per atom, in excess of \(100\) A\({}^{3}\)/atom, and these were removed from the test set and excluded from results.

Table 1 outlines the experimental setup for training Charge3Net on each of the datasets. For each gradient step, a random batch of materials is selected, from which a subset of the charge density probe points are used. We use the Adam optimizer , and decay the learning rate by \(0.96^{ s}\) at step \(s\). We find that optimizing for L1 error improves performance and training stability over mean squared error.

   Dataset & Learning Rate & Decay \(\) & \(L\) & Batch Size & Training Steps \\  NMC & 0.01 & \(10^{4}\) & 4 & 8 * 200 points & \(7.5*10^{5}\) \\ QM9 & 0.01 & \(10^{4}\) & 4 & 8 * 200 points & \(10^{6}\) \\ Materials Project & 0.005 & \(3*10^{3}\) & 4 & 16 * 200 points & \(10^{6}\) \\   

Table 1: Training setup

Figure 2: a: Overall Charge3Net architecture. Predicted charge density \((_{k})\) at probe point \(_{k}\) is computed from atomic species \(z_{i}\), interatomic displacement vectors \(r_{ij}\), and probe-atom displacement vectors \(r_{ik}\). b: Atom graph convolution. c: Atom-probe graph convolution.

Following recent work  we evaluate our models using mean absolute error normalized by the total number of electrons in the volume, \(_{}\) (Eq. 5). This is approximated via numerical integration on the charge density grid created by VASP.

\[_{}= V}|()-( )|}{_{ V}|()|}\] (5)

We compare the performance of ChargeE3Net to the models introduced in DeepDFT and OrbNet-Equi. For QM9 and NMC datasets we use identical training, validation, and test splits as Jorgensen and Bhowmik , and report \(_{}\) computed using the authors publicly released models. We have verified these to match the numbers reported in the original work. For the MP dataset, we train the DeepDFT models using the same experimental settings from the original work. As shown in Table 2, our model significantly outperforms the prior equivariant DeepDFT models  on the Materials Project and QM9 datasets, while achieving similar performance on the NMC dataset. In addition, our model achieves a lower \(_{}\) on the QM9 dataset than OrbNet-Equi  model, which leverages additional features based on quantum mechanical calculations, despite ChargeE3Net only using atomic species and position information.

### Effects of Higher Order Representations

To demonstrate the impact of introducing higher order tensor representations in our model, we examine the effect of training models while varying the maximum rotation order \(L\), with \(\{0,...,L\}\) up to \(L=4\). While higher order representations are achievable, they can be prohibitive to use in the network due to the \(O(L^{6})\) computational complexity of tensor products. The number of channels for each order is determined by \(N_{}=\). For example, the \(L=0\) model has representations consisting of 500 even scalars, and 500 odd scalars, while the \(L=1\) model has representations consisting of 250 even scalars, 250 odd scalars, 83 even vectors, and 83 odd vectors. In this way, each model is constructed such that the total representation size is approximately equal, as well as the proportion of the total representation used by each order. We train each model on 1,000 and 10,000 material subsets of the MP dataset as well as the full dataset, using the same validation and test sets. Figure 3 shows a consistent increase in performance on each subset with the addition of each rotation order and similar scaling behavior with respect to the training set size. This trend suggests that higher order representations are necessary for accurate charge density modeling, and can match the performance of lower order models with less data.

   Dataset & invDeepDFT & equiDeepDFT & OrbNet-Equi & ChargeE3Net (Ours) \\  NMC & \(0.089 0.001\) & \( 0.001\) & - & \( 0.001\) \\ QM9 & \(0.357 0.001\) & \(0.284 0.001\) & \(0.206 0.001\) & \( 0.001\) \\ Materials Project & \(0.859 0.011\) & \(0.799 0.010\) & - & \( 0.010\) \\   

Table 2: Error for each dataset, reported in average \(_{}\) (%), \(\) one standard error.

Figure 3: Log-log plot of training set size vs performance, measured in average \(_{}\) (%) on the Materials Project test set. We show models train with maximum rotation order \(L 0,1,2,3,4\).

In order to gain intuition behind why and when higher order representations yield higher performance, we consider two factors contributing to the variance of charge density within a material: _radial_ dependence, or a dependence on the distance from neighboring atoms, and _angular_ dependence, a dependence on the angle from a point with respect to the rest of the system. While most materials exhibit strong radial dependence, some also exhibit strong angular dependence. This would be difficult to model using an invariant architecture (\(=0\)) operating solely on interatomic distances, and likely would require higher order representations to model correctly. In Figure 4, we illustrate this concept with two materials. H4Cs2O8P2 exhibits high angular dependence, where charge density is dependent on angle as well as radial distance from the nearest atom. Conversely, Rb2Sn6 does not exhibit this, as its density appears to be almost entirely a function of the atomic distance, suggesting that a lower-order equivariant or invariant architecture could model its electron density well. We find this intuition to be consistent with model performance, as Figure 4 shows similar performance for \(L=0\) and \(L=4\) models for Rb2Sn6, while the \(L=4\) model exceeds the performance of the \(L=0\) model for H4Cs2O8P2.

To further quantify this angular dependence, we develop a metric \(\) to determine to what extent an atomic system exhibits more angular variation in its charge density with respect to atom locations. This is achieved by measuring the dot product of the unit vector between a probe point and its nearest neighboring atom and the gradient of the density at that probe point:

\[(G)=1-_{k} G}|(_{k}) _{ki}|}{_{_{k} G}||(_{k})||}\] (6)

where \(G\) is a set of probe points and \(_{ki}\) is a unit vector from probe point at location \(_{k}\) to the closest atom at location \(_{i}\). Intuitively, a material with charge density gradients pointing directly towards or away from the nearest atom will have dot products larger in magnitude and \( 0\), whereas a material with density that changes angularly with respect to the nearest atom will have dot products smaller in magnitude, and \( 1\). Figure 5 demonstrates that the differences in performance between the lower and higher rotation order networks correlates strongly with \(\). As electron density distributions with higher angular variance do occur naturally in the data, this further justifies the need for introducing higher rotation order representations into charge density prediction networks.

Figure 4: Comparison of material with high angular variance H4Cs2O8P2 (top) and low angular variance Rb2Sn6 (bottom), as determined by 95\({}^{}\)- and 5\({}^{}\)-percentile \(\) values in test set. a: visualization of charge density isosurfaces (gray). b: plot of charge density with respect to radial distance from nearest atom. c: \(_{}\) for ChargeE3Net model predictions on these materials.

### Runtime

To demonstrate the scalability of our model, we analyze the runtime duration of our model on systems with increasing number of atoms, and compare to the duration of DFT calculations. We run each method on a single material, BaVS\({}_{3}\) (mp-3451 in MP), creating supercells from \(1 1 1\) to \(10 10 10\) and recording the runtime to generate charge density on the system. Figure 6 shows an approximate linear, \(O(N)\), scaling of our model with respect to number of atoms, while DFT exhibits approximately \(O(N^{2.3})\) before exceeding our computational budget. This is to be expected, as the graph size increases linearly with cell volume if the resolution remains the same, while DFT has shown to scale at a cubic rate with respect to system size . Furthermore, like DeepDFT, our method can be fully parallelized up to each point in the density grid with no communication overhead.

## 4 Discussion

This work introduces an architecture for predicting charge density through equivariance with higher order representations. We demonstrate that introducing higher order tensor representations over scalars and vectors used in prior work achieves greater predictive accuracy, and show how this achieved through the improved modelling of systems with high angular variance. While our models use up to \(L=4\) representations, they generally use very few channels due to the weighting strategy mentioned in Section 3.1. As tensor products have a computation complexity of \(O(L^{6})\), there may be a more optimal model configuration with a uniform distribution of channels at a lesser \(L\). Furthermore, SO(3) convolutions can be approximated in SO(2)  which brings down the computational complexity to \(O(L^{3})\) and removes the need to compute Clebsch-Gordan coefficients.

Figure 5: Scatter of \(_{mae}\) improvement from \(L=0\) to \(L=4\) vs. \(\), showing greater performance gains from the higher order model when materials exhibit high angular variance.

Figure 6: Runtime comparison of DFT and our ChargeE3Net model with respect to number of atoms in the evaluated system. DFT is using a 48-core Intel Xeon CPU, while ChargeE3Net uses a single NVIDIA V100 GPU.

Future work may investigate these optimization and efficiency improvements, as well as the integration of charge density prediction models to existing DFT frameworks for downstream property prediction and simulation.