# Comparing Representational and Functional Similarity in Small Transformer Language Models

Dan Friedman, Andrew Lampinen, Lucas Dixon, Danqi Chen, Asma Ghandeharioun

###### Abstract

What kinds of solutions do neural networks learn? In many situations, it would be helpful to be able to characterize the solution learned by a neural network, including for answering scientific questions (e.g. how do architecture changes affect generalization) and addressing practical concerns (e.g. auditing for potentially unsafe behavior). One approach is to try to understand these models by studying the representations that they learn--for example, comparing whether two networks learn similar representations. However, it is not always clear how much representation-level analyses can tell us about how a model makes predictions. In this work, we explore this question in the context of small Transformer language models, which we train on a synthetic, hierarchical language task. We train models with different sizes and random initializations, evaluating performance over the course of training and on a variety of systematic generalization splits. We find that existing methods for measuring representation similarity are not always correlated with behavioral metrics--i.e. models with similar representations do not always make similar predictions--and the results vary depending on the choice of representation. Our results highlight the importance of understanding representations in terms of the role they play in the neural algorithm.

## 1 Introduction

Representation similarity analysis  is a common method for characterizing the relationship between different neural networks (e.g. 11; 18; 20; 13; 9; 6; 15; 16). However, it is unclear whether we can expect models with similar representations to make similar predictions [4; 8]. In general, the relationship between representation and behavior depends on how the representations are used by later parts of a model, which is a function of the model architecture and training setting.

Therefore, in this work, we study the relationship between representational and functional measures empirically in the context of a particular architecture and setting--small Transformer language models , trained on the Dyck balanced parenthesis languages. The Dyck languages exhibit fundamental properties of natural languages--recursive, hierarchical structure, which give rise to long-distance dependencies--but are simple enough to admit simple, human-interpretable algorithms [24; 22]. For this reason, they have been widely studied in prior work on the expressivity of Transformer language models [7; 24], and in interpretability . A benefit of this setting is that we can largely reverse-engineer the solutions that these models learn, to better understand the relationship between representations and behavior.

Specifically, we train models with different embedding sizes and random initializations and evaluate performance on different systematic generalization splits. We measure representation similarity using standard metrics from prior work , and evaluate whether models that have similar representations also make similar predictions. We find that measures of representational similarity do not always agree with functional similarity. In particular, the differences are greatest on out-of-distribution evaluation settings, and representations from different model components lead to different similarity profiles. This analysis highlights some of the interpretability challenges that have

[MISSING_PAGE_FAIL:2]

[MISSING_PAGE_EMPTY:3]

remains the same. In terms of prediction similarity, the similarity scores diverge over time, with the largest models becoming the least similar to each other by the end of training.

Comparing different model componentsIn the previous sections, we conjectured that prediction similarity would be correlated with similarity between the second-layer key and query embeddings, because these play a central role in the bracket matching algorithm. One possible explanation for our negative finding is that another model component plays a larger role in determining what the models will predict, especially in out-of-distribution settings. In Figure 4, we plot the similarity trajectories using different choices of representation on the depth-generalization split. The representations that most resemble the trajectory of prediction similarity (plotted in Fig. 3) are from the second layer MLP output, suggesting that the MLP might be more important for explaining prediction differences in this setting, and underscoring the limitations of analyzing model components in isolation.

## 4 Conclusion

In this work, we studied the connection between representational similarity and functional similarity in Transformer language models. In particular, we focused on small Transformers trained on Dyck balanced-parenthesis languages, a setup that would allow us to largely reverse-engineer the neural algorithm and study its behavior under systematically different distribution shifts. Across different model sizes and initializations, we found that representational similarity is not always associated with model behavior, especially when evaluated out of distribution. As model size increases, representational similarity can even diverge from behavioral similarity over the course of training. Finally, the role that the representation plays in the neural algorithm dictates the extent to which it would be associated with the model's behavior.

Figure 4: Average similarity (CKA) between models with the same width, over the course of training, broken down by representation, and evaluated on the depth generalization split.

Figure 3: Average similarity between models with the same width, over the course of training. The representations are the key and query embeddings from the second attention layer.