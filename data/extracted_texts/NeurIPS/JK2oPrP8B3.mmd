# Global Identifiability of \(\ell_{1}\)-based Dictionary Learning via Matrix Volume Optimization

# Global Identifiability of \(_{1}\)-based Dictionary Learning via Matrix Volume Optimization

 Jingzhou Hu &Kejun Huang

Department of Computer and Information Science and Engineering

University of Florida

Gainesville, FL 32611

{jingzhouhu,kejun.huang}@ufl.edu

###### Abstract

We propose a novel formulation for dictionary learning that minimizes the determinant of the dictionary matrix, also known as its volume, subject to the constraint that each row of the sparse coefficient matrix has unit \(_{1}\) norm. The main motivation for the proposed formulation is that it provides global identifiability guarantee of the groundtruth dictionary and sparse coefficient matrices, up to the inherent and inconsequential permutation and scaling ambiguity, if a set of vectors obtained from the coefficient matrix lies inside the \(_{}\) norm ball but contains the \(_{2}\) norm ball in their convex hull. Unlike existing work on identifiability of dictionary learning, our result is global, meaning that a globally optimal solution to our proposed formulation has to be a permuted and rescaled version of the groundtruth factors. Another major improvement in our result is that there is no additional assumption on the dictionary matrix other than it is nonsingular, unlike most other works that require the atoms of the dictionary to be mutually incoherent. We also provide a probabilistic analysis and show that if the sparse coefficient matrix is generated from the widely adopted Bernoulli-Gaussian model, then it is globally identifiable if the sample size is bigger than a constant times \(k k\), where \(k\) is the number of atoms in the dictionary, with overwhelming probability. The bound is essentially the same as those local identifiability results, but we show that it is also global. Finally, we propose algorithms to solve the new proposed formulation, specifically one based on the linearized-ADMM with efficient per-iteration updates. The proposed algorithms exhibit surprisingly effective performance in correctly and efficiently recovering the dictionary, as demonstrated in the numerical experiments.

## 1 Introduction

Dictionary learning is a famous unsupervised learning problem  that also goes by the name sparse coding  or sparse component analysis . It stems from the intuition that samples from a data set can all be approximately represented as a sparse combination of atoms from a complete or overcomplete dictionary, or \(_{l}=_{i}\) for \(i=1,,n\), where \(_{l}^{m}\) are the given data points, columns of \(^{m k}\) represents atoms of a dictionary, and \(_{i}^{k}\) are the sparse representations of \(_{i}\), indicating that \(_{l}\) is linearly dependent on only a few atoms from the dictionary \(\) with coefficients given in the nonzeros of \(_{l}\). Staking \(_{i}\) as columns of \(^{m n}\) and \(_{i}\) as columns of \(^{k n}\), it can be written as a matrix factorization model \(=\). Although in some cases there exist known dictionaries that admit good-quality sparse representations, such as the DCT or wavelet basis for image compression, sometimes it is better to learn a sparse-inducing dictionary from data in an unsupervised fashion, hence the task of dictionary learning that tries to estimate \(\) and \(\) jointly. In most cases, peopleassume \(m k\): the dictionary is complete if \(m=k\), and overcomplete if \(m<k\). In this paper we focus on the complete dictionary case when \(\) is square and nonsingular, which is common in a lot of prior works that this work is built upon (Gribonval and Schnass, 2010; Sun et al., 2016, 2016; Wu and Yu, 2017; Wang et al., 2020)

Dictionary learning has found numerous applications in signal denoising (Elad and Aharon, 2006), audio coding (Plumbley et al., 2009), and medical imaging (Tosic et al., 2010), to name just a few. On the theory side, most of the existing works have focused on algorithm design. Famous algorithms include \(k\)-SVD (Aharon et al., 2006a) and online dictionary learning (Mairal et al., 2009), among numerous other algorithms based on generic nonconvex algorithm design with guarantee of convergence to a stationary point. More recently, there have appeared a line of research that attempts to show global optimality for dictionary learning under more restrictive assumptions, such as (Spielman et al., 2012; Agarwal et al., 2016; Arora et al., 2014, 2015; Sun et al., 2016, 2016, 2019; Bai et al., 2019; Zhai et al., 2020, 2020, 2020; Tolooshams and Ba, 2022).

### Model identifiability

The dictionary learning problem can be considered an instance of the broader matrix factorization model \(=\) subject to the opaque requirement that \(\) is sparse. It is well-known that matrix factorization without any additional assumptions on the latent factors is not unique, since we can always "insert" an invertible matrix \(\) and \(^{-1}\) as \(=}\) where \(}=\) and \(}=^{-1}\), and one cannot distinguish whether \(\) or \(}\) are the groundtruth sources. Such rotation ambiguity cannot be resolved by the well-known principal component analysis (PCA) (Jolliffe, 2002).

On the other hand, most matrix factorization models allow scaling and permutation ambiguity, i.e., a permutation matrix \(\) and a diagonal matrix \(\) such that the recovered mixing matrix is \(\) and the recovered sources are \(^{T}^{-1}\). In addition to inherent constraints that fit the application, it is sometimes helpful to have an optimization criterion in search for the "correct" factorization (up to scaling and permutation). In light of this, we formally define the identifiability of any matrix factorization model as follows:

**Definition 1.1** (Identifiability).: Consider the generative model \(=^{}^{}\), where \(^{}\) and \(^{}\) are the groundtruth latent factors. Let \((^{},^{})\) be optimal for an identification criterion \(q\)

\[(^{},^{})=*{arg\,min}_{= ,}q(,),\]

where \(\) and \(\) are some constraint set that \(\) and \(\) should belong to, respectively. If \(^{}\) and/or \(^{}\) satisfy some condition such that for any \((^{},^{})\), there exist a permutation matrix \(\) and a diagonal matrix \(\) such that \(^{}=^{}\) and \(^{}=^{T}^{-1}^{}\), then we say that the matrix factorization model is essentially identifiable, up to permutation and scaling, under that condition.

For dictionary learning in specific, where the essential constraint on the latent factors is sparsity on \(\), there have been some existing works on its identifiability issue. It has been shown that simply by putting a hard sparsity constraint on \(\) is enough to guarantee a unique factorization, provided that the sample size \(n\) is large enough (Aharon et al., 2006b; Hillar and Sommer, 2015; Cohen and Gillis, 2019). The main drawback is that the required sample size \(n\) is usually too large to be practical--results from (Aharon et al., 2006b) and (Hillar and Sommer, 2015) both require \(n\) to be \(O((k+1){k s})\), where \(s\) is the number of nonzeros that each column of \(\) is allowed to have; it has been relaxed to \(O(k^{3}/(k-s)^{2})\) by Cohen and Gillis (2019), but still very large. The existence of \(s\) in the bound also shows that an outlier sample that does not admit a sparse representation from the dictionary is absolutely not allowed in the data set, which is also restrictive. Inspired by the success of compressive sensing and sparse recovery, it is perhaps more common to use the \(_{1}\) norm to approximately sparsify \(\). Identifiability results based on the \(_{1}\) norm formulation has been predominantly local, meaning the model is identifiable within a neighborhood of the groundtruth factors \((^{},^{})\), but on the bright side the sample size requirement is typically down to \(O(k k)\) and allows the existence of dense outliers (Gribonval and Schnass, 2010; Wu and Yu, 2017; Wang et al., 2020).

### This paper

In this paper, we intend to bridge the gap by providing _global_ identifiability guarantee of complete dictionary recovery using a \(_{1}\) norm based formulation, given as follows:

\[*{minimize}_{,}\ \ ||\ \ =,\|_{j,}\|_{1} 1,j=1, ,k,\] (1)

To the best of our knowledge, this is a novel formulation that has never appeared in the literature; however, as we will show in the next section, the intuition stems from the more commonly used \(_{1}\)-based formulations. We stress that the formulation is \(_{1}\)-based, as the \(_{1}\) norm is the most widely used term to promote sparsity of the solution. Although in the context of dictionary learning, using the \(_{1}\) norm as the convex surrogate for sparsity is not enough to make the overall problem tractable, there are still benefits of using it instead of other ones such as hard sparsity constraints, for example in terms of sample complexity as introduced in the previous subsection.

After some gentle derivation to motivate the proposed new formulation (1) for dictionary learning, we provide the following contributions:

1. We give a deterministic characterization of the identifiability condition on the sparse coefficient matrix \(\) under which the dictionary (as well as the sparse coefficients) can be uniquely determined, up to the inherent and inconsequential permutation and scaling ambiguity, via solving (1). Unlike existing work that shows local identifiability, our result is global, meaning that the groundtruth factors have to be a global minimizer to (1). Furthermore, perhaps surprisingly, our result only require the complete dictionary to be nonsingular--there are no additional assumptions that require the atoms to be mutually incoherent whatsoever.
2. The result shows that \(_{1}\)-based dictionary learning is globally identifiable if a certain set of points, which are by definition inside the hypercube \([-1,1]^{k}\), contains the unit ball in their convex hull. This geometric assumption is called "sufficiently scattered", with similar ideas appearing in pertinent unsupervised learning models. We provide intuition why this assumption generally leads to a sparse \(\). We further show that if the sparse coefficient follows the widely used Bernoulli-Gaussian generative model, then it is identifiable with very high probability, provided the sample size \(n\) is bigger than a constant times \(k(k)\). This is the same sample complexity bound obtained in some of the local identifiability analysis, but we show that it is also global, using the proposed new formulation (1).
3. We propose algorithms to solve (1), specifically one based on linearized-ADMM in the main paper; in the supplementary we also present one based on the Frank-Wolfe framework and the other based on block coordinate descent. Even though the proposed formulation (1) is nonconvex (just like any other dictionary learning formulation), numerical experiments on synthetic data show that they achieve global optimality in almost all cases, provided that the groundtruth latent factors satisfy the identifiability assumptions given before.

## 2 Problem Formulation

We start by proposing a novel formulation for dictionary learning. The purpose is to show that the seemingly different formulation does not come out of thin air, but is highly related, if not equivalent, to existing ones that have been studied extensively. Consider the following formulation adopted in :

\[*{minimize}_{,}\ \ \ \ \|\|_{1}\ \ =,\|_{,j}\| 1,j=1, ,k.\] (2)

We use the noiseless model \(=\) since the main focus of this work is identifiability, and a lot of algorithms are designed by replacing this term with a loss function that measures the residual \(-\), e.g. . The purpose of the \(_{1}\) norm of \(\) is to encourage it to be sparse, but since it is also dependent on the scaling of \(\), the constraint \(\|_{,j}\| 1\) is imposed onto the columns of \(\). Such constraints are without loss of generality due to the scaling ambiguity \(=^{-1}\), so for any admissible factorization, we can rescale the columns of \(\) to unit norm, while absorbing the scaling into the rows of \(\), without affecting the matrix product.

Due to the Lagrangian multiplier theorem, there exist positive scalars \(_{1},,_{k}\) and \(_{1},,_{k}\) such that any solution of the following problem is also optimal for (2)\[,}{}\ \ _{j=1}^{k}_{j}\|_{:j}\| \ \ =,\|_{j}.\|_{1}_{j},j=1,,k,\] (3)

since they would lead to exactly the same Lagrangian function with appropriate choices of \(_{j}\) and \(_{j}\). Again, due to the inherent scaling ambiguity, we can assume without loss of generality that \(_{1}==_{k}=1\), leaving the unknown parameters to just \(_{j}\)'s.

If we replace the summation in the objective of (3) with product, or equivalently replacing the arithmetic mean with the geometric mean, the term \(_{j}_{j}\|_{:j}\|\) would be intimately related to \(||\) if \(\) is square. Indeed, there must exist some unknown matrix \(\) such that \(||=_{j}_{j}\|_{:j}\|\). Furthermore, for the purpose of optimization it is completely unnecessary to know \(\) since \(||=||||\), which is just a positive scaling of \(||\) that does not affect the optimization solution. As a result, our proposed novel formulation of dictionary learning takes the form of (1).

Although our paper focuses on the case of complete dictionaries, i.e., when \(\) is square and nonsingular, they can be effortlessly extended to the overdetermined case when \(\) is tall, and the only modification is to change the objective to \(^{}\). This is sometimes called the volume of the matrix  with applications in change-of-variable integration  and probability . It has been used as the identification criterion for several variants of nonnegative matrix factorization with an interesting geometric interpretation of finding the minimum volume enclosing simplex of a set of points . The most related work is the polytopic matrix factorization framework proposed by Tatii and Erdogan , which includes a case that constraints the _columns_ of \(\) to have bounded \(_{1}\) norm. Our proposed formulation constrains the rows of \(\), which seems to be a minor difference but is in fact a great leap forward since row scaling of \(\) is inherent for any matrix factorization, while column scaling is not. It also turns out that the identifiability analysis would be drastically different, albeit the formulations look similar.

## 3 Global identifiability

In this section we study the conditions under which the proposed dictionary learning formulation (1) is identifiable, i.e., any solution to (1) equals to the groundtruth dictionary \(^{}\) and sparse coefficients \(^{}\) up to permutation and scaling, as per Definition 1.1. The groundtruth \(^{}\) can have arbitrary scaling, and it is without loss of generality to assume that its rows have unit \(_{1}\) norms; we denote such rescaled \(^{}\) to be \(}^{}\), i.e., \(\|}^{}_{j:}\|_{1} 1,j=1,,k\); we absorb the counterscaling into the columns of \(^{}\) to generate \(}^{}\), so that \(=^{}^{}=}^{} }^{}\).

Since we consider only complete dictionaries, for any feasible \((,)\) for (1), there must exist a nonsingular matrix \(\) such that \(=}^{}\) and \(=}^{}^{-1}\), therefore Problem (1) is, conceptually, equivalent to

\[}{}\ \ ||\ \ \|^{} }^{}\|_{1} 1,j=1,,k,\] (4)

where \(^{}_{j}\) denotes the \(j\)th row of \(\). Since \(=^{-1}}^{}\) and \(}^{}\) is feasible, the optimal value of (4) is at least 1. On the other hand, if the model is identifiable, then the optimal value of (4) should be exactly equal to 1, and the optimal solution to (4) must be a permutation matrix times a diagonal \( 1\) matrix. The rest of this section is dedicated to characterizing when such condition holds for the sparse coefficients \(\). Notice that all we need for the dictionary \(\) is that its columns are independent, so that the transform from (1) to (4) is possible--there is no additional assumptions such as "incoherence" for the rest of the arguments to hold, which is a great improvement to the existing works such as .

### Deterministic characterization

The intuition is simple: the Hadamard inequality states that \(||_{j}\|_{j}\|\), so a sufficient condition for identifiability is that the set of solutions to the following (nonconvex) quadratic programming

\[}{}\ \ \|\|^{2}\ \ \|^{}}^{}\|_{1} 1\] (5)is \(\{_{1},,_{k}\}\); in other words, a maximizer must be an indicator vector or its negative.

To proceed, we introduce the following notion:

**Assumption 3.1** (Sufficiently scattered in the hypercube).: Let \(\) denote the Euclidean ball \(=\{^{k}\|\| 1\}\) and \(\) denote the hypercube \(=\{^{k}\|\|_{} 1\}\). A set \(\) is sufficiently scattered in the hypercube if:

1. \(\);
2. \(=\{_{1},,_{ k}\}\), where \(\) denotes the boundary of the set.

A geometric illustration of a set that is sufficiently scattered in the hypercube is shown in Figure 1, which in this case is a polytope with vertices depicted as dots. As we can see, \(\) is a subset of the hypercube \([-1,1]^{k}\), but touches the boundary of \([-1,1]^{k}\) at points \(_{1},,_{k}\). If a set \(\) is sufficiently scattered in \([-1,1]^{k}\), \(\) contains \(\) as a subset and, as a second requirement, \(\) touches the boundary of \(\) only at those points too.

The term "sufficiently scattered" was first coined by Huang et al. (2016) to characterize the identifiability condition for nonnegative matrix factorization that has already appeared in (Huang et al., 2013). The difference is that in (Huang et al., 2013, 2016; Fu et al., 2018), the condition is defined over the conic hull of a set of points in the nonnegative orthant containing a specific hyperbolic cone. It has also been defined over the convex hull of a set of points in the probability simplex (Fu et al., 2015; Lin et al., 2015). The most related case is the sufficiently scattered condition defined for the hypercube (\(_{}\)-norm ball) (Tatli and Erdogan, 2021), as well as the standard orthoplex (\(_{1}\)-norm ball).

To see how the sufficiently scattered condition could be useful for dictionary learning, we first rewrite the single constraint in (5) as, equivalently, a set of \(2^{n}\) linear inequalities \(^{}}^{} 1, \{ 1\}^{n}\). Since the rows of \(}^{}\) have been rescaled to have unit \(_{1}\) norm, we must have that \(\|}^{}\|_{} 1\), meaning that all vectors \(}^{}\) should lie in the hypercube \([-1,1]^{k}\), and we further assume the convex hull of all such \(}^{}\) is sufficiently scattered. Define the \(n 2^{n}\) matrix \(\) that contains all elements of \(\{ 1\}^{n}\) as its columns, we have the following deterministic characterization of the identifiability of (1):

**Theorem 3.2**.: _Consider the generative model \(=^{}^{}\), where \(^{}^{k k}\) is the groundtruth dictionary and \(\) is the groundtruth sparse coefficients. If \((^{})=k\) and \((^{})\) is sufficiently scattered in the hypercube as in Assumption 3.1, then for any solution of (1), denoted as \((^{},^{})\), there exist a permutation matrix \(\) and a diagonal matrix \(\) such that \(^{}=^{}\) and \(^{}=^{T}^{-1}^{}\). In other words, dictionary learning is identifiable if the groundtruth \(^{}\) has full column rank and \(^{}\) is sufficiently scattered._

The full proof is relegated to Appendix A. Here we provide a concise sketch of the proof: if \(}^{}\) is sufficiently scattered, a solution to (5) must be \(_{j}\); as a result, \(||_{j}\|_{j}\| 1\), and equality holds only if \(\) is a permutation matrix with some sign flips of its rows.

How is this assumption relevant to the sparsity of \(\)? Notice that if \(\) is sufficiently scattered, then for _any_ vector \(\) such that \(\|\| 1\), there must exist a convex coefficient \(\), i.e., \( 0\) and \(^{T}=1\), such that \(=\). Due to the definition of \(\), we know that the vector \(=\) must satisfy \(\|\|_{} 1\). Let us instantiate it at \(=_{j}\). This means the \(j\)th row of \(\) inner product with \(\) equals to \(1\). But the \(j\)th row of \(\) has unit \(_{1}\) norm and \(\|\|_{} 1\), so their inner product is \(1\) only if \(^{}=(_{j,})\), which is uniquely defined if \(_{j,}\) is completely dense. Now \(_{j}=\) also implies the inner products between \(\) and the rest of the rows of \(\) all equal to zero, which is almost impossible if \(^{}=(_{j,})\) is the only candidate. On the other hand, if some of the entries in \(_{j,}\) are zeros, then the corresponding entries in \(\) are free to choose any value in \([-1,1]\); to make the rest of the \(k-1\) inner products equal to zero, we generically need at least \(k-1\) free variables in \(\), meaning \(_{j,}\) contains at least \(k-1\) zeros--it is still not guaranteed that \(\|\|_{} 1\), but it intuitively makes sense that the more zeros each row of \(\) contains, the more likely that \(_{j}=\) is feasible, which is necessary for \(\) to be sufficiently scattered.

Figure 1: An example of sufficiently scattered in 3D.

### Probabilistic characterization

The analysis given in SS3.1 gives an exact characterization of when dictionary learning is identifiable using the proposed formulation (1) with a matrix volume identification criterion. However, given some sparse coefficient matrix \(\), checking whether it satisfies the identifiability condition in Theorem 3.2 amounts to solving a nonconvex quadratic programming (5), which is NP-hard in general. In this section, we assume that the sparse coefficient matrix \(\) is generated from a probabilistic generative model, specifically the Bernoulli-Gaussian model that has been widely considered in the literature, such as (Gribonval and Schnass, 2010; Sun et al., 2016), and show that it would satisfy guarantee identifiability with high probability, provided that the number of data points \(n\) is \(O(k k)\).

**Assumption 3.3** (Bernoulli-Gaussian model).: The matrix \(^{k n}\) is generated from a Bernoulli-Gaussian model with parameter \(p(0,1)\), denoted as \((p)\), if its elements are i.i.d. with \(S_{ij}=b_{ij}_{ij}\), where \(b_{ij}\{0,1\}\) are i.i.d. Bernoulli random variables with \([b_{ij}=1]=p\) (thus \([b_{ij}=0]=1-p\)) and \(_{ij}(0,1)\) are i.i.d. standard normal random variables.

Note that due to the scaling ambiguity of matrix factorization the assumption that the normal random variable has unit variance is without loss of generality--its rows will be rescaled to have unit \(_{1}\) norms nonetheless. What we are essentially assuming is that 1) a significant portion of \(\) are zero; 2) nonzeros of \(\) have zero mean and approximately the same variance.

**Theorem 3.4**.: _Suppose \(^{k n}\) is generated from the Bernoulli-Gaussian model \((p)\), and \(}\) is obtained by scaling its rows to have unit \(_{1}\) norm. Then_

\[[_{\|^{}}\|_{1} 1}\|\|>1 ] 4(k(k)-np(1-p)).\] (6)

**Corollary 3.5**.: _Consider the generative model \(=^{}^{}\), where \(^{}^{k k}\) is the groundtruth dictionary and \(\) is the groundtruth sparse coefficients. If \((^{})=k\) and the matrix \(^{}^{k n}\) is generated from the Bernoulli-Gaussian model \((p)\), then \((^{},^{})\) are globally identifiable via optimizing (1) with probability at least \(1-4(k(k)-np(1-p)).\)_

The proof of Theorem 3.4 is relegated to Appendix B. It is appealing to see that the presented result looks much cleaner than prior work such as (Gribonval and Schnass, 2010; Wu and Yu, 2017; Wang et al., 2020), on top of the fact that such identifiability result is _global_, and does not require _any_ incoherence assumption on the dictionary, as long as it is nonsingular. An interesting observation is that most of the foundational results have appeared in the seminal work (Gribonval and Schnass, 2010); the difference is that they are used in the improved formulation (1), and in return leads to more significant result in Corollary 3.5. From the above result, we observe the following:

1. The right-hand-side of (6) would be a valid probability (\(<1\)) if \(n\) is bigger than a constant times \(k(k)/p(1-p)\), and very quickly approaches zero as it grows bigger. This agrees with the prior work on local identifiability that the sample complexity is of the same order (Gribonval and Schnass, 2010; Geng and Wright, 2014; Wu and Yu, 2017; Wang et al., 2020).
2. The Bernoulli parameter \(p\) cannot be exactly equal to \(1\) or \(0\), which affirms the obvious fact that \(\) can neither be completely dense nor entirely zero. The more interesting implication is that DL is identifiable (with high probability) as long as there is _some_ nonzero probability of having zeros in \(\), no matter how small that probability is, provided the sample size is large enough.

## 4 Proposed algorithm

In this section, we propose two algorithms for the volume-optimization based formulation (1). We will reformulate (1) as a determinant maximization problem subject to linear constraints, and then apply the Frank-Wolfe algorithm or block coordinate descent, which are both guaranteed to converge to a stationary point.

Since we assume \(\) is nonsingular, we can define \(=^{-1}\) and apply a change of variable to problem (1): the objective would become minimizing \(1/||\), which we apply the log function and make it \(-||\), and in the constraints we can now eliminate the \(\) variables by simply requiring \(\|_{j,:}\|_{1} 1,j=1,,k\). This leads to the following reformulation

\[*{minimize}_{}\ \ -||\ \ \ \|[]_{j,:}\|_{1} 1,j=1,,k.\] (7)Problem (7) now has a convex, or more specifically linear, constraint set, although the objective is still not convex. In Appendix 4, we propose two algorithms to directly tackle this formulation, one based on the Frank-Wolfe framework, and the other is an instance of block coordinate descent. Both algorithms guarantee convergence to a stationary point, which is the best one can get for a generic nonconvex problem. However, both algorithms require solving a linear programming problem in each iteration, making the overall algorithm double-looped, and the time efficiency is greatly affected by the specific subroutine to solve linear programming. In the rest of this section, we propose an algorithm based on the linearized alternating direction method of multipliers (L-ADMM) with well-defined and low-complexity iterations.

### Linearized alternating direction method of multipliers (L-ADMM)

We first modify the formulation (7) by introducing an auxiliary variable \(\):

\[,}{}-|| =,\;\;\|_{j,:}\|_{1} 1,j=1, ,k.\] (8)

Formulation (8) consists of two sets of variables (\(\) and \(\)) over two separable functions (one of them being an indicator function of \(\) that the \(_{1}\) norms of its rows are no bigger than 1) and linear equality constraints. It is easy to derive the alternating direction method of multipliers (ADMM) (Boyd et al., 2011) for (8):

\[_{(t+1)} _{}\;\;-||+(/2)\| -_{(t)}+_{(t)}\|^{2},\] (9a) \[_{(t+1)} _{}\;\;_{\|:\|1 1}()+( /2)\|_{(t+1)}-+_{(t)}\|^{2},\] (9b) \[_{(t+1)} _{(t)}+_{(t+1)}-_{(t+1)}.\] (9c)

The second step (9b) is well-defined, as it projects each row of the matrix \(_{(t+1)}+_{(t)}\) to the \(_{1}\) norm ball, which can be efficiently computed with linear complexity (Duchi et al., 2008). In our implementation we use the bisection method described in (Parikh and Boyd, 2014). The first step (9a), however, is not clear how to compute. One popular method to mitigate this issue, which is prevalent in ADMM, is to take a linear approximation of the loss function of \(\) at the previous iterate \(_{(t)}\)(Lu et al., 2021). The gradient of \(||\) is \(^{-}\), therefore the linear approximation of \(-||\) at \(_{(t)}\) is \(-|_{(t)}|-_{(t)}^{-1}(-_{(t)})\). The overall update of \(_{(t+1)}\) becomes minimizing a convex quadratic function, which can be done in closed-form. The derived linearized ADMM (L-ADMM) iterates are

\[_{(t+1)} ((_{(t)}-_{(t)})^{}+(1/) {P}_{(t)}^{-})(^{})^{-1},\] (10a) \[_{(t+1)} _{\|:\|1 1}(_{(t+1)}+ _{(t)}),\] (10b) \[_{(t+1)} _{(t)}+_{(t+1)}-_{(t+1)}.\] (10c)

**Algorithm 1** Solving (7) with L-ADMM

```
1: take the QR factorization of \(^{}=\)
2: set \(=nk\) and initialize \(_{(0)}\)
3:for\(t=0,1,2,\) until convergence do
4:\(_{(t+1)}(_{(t)}-_{(t)})+(1/)_{(t)}^ {-}\)
5:\(_{(t+1)}_{\|:\|1 1}(_{(t+1)}^{ }+_{(t)})\)
6:\(_{(t+1)}_{(t)}+_{(t+1)}^{}-_{(t+1)}\)
7:endfor
8:return\(_{(t)}^{-}\) ```

**Algorithm 2** Solving (7) with L-ADMM

Finally, we notice that formulation (8) and the derived L-ADMM algorithm are invariant under linear transformation of columns of \(\), meaning if we replace \(\) with \(}=\) in (8), where \(\) is a \(k k\) invertible matrix, then every iterate \(_{(t)}\) is uniquely mapped to \(_{(t)}^{-1}\) (while \(_{(t)}\) and \(_{(t)}\) are exactly the same), and the objective value has a constant difference \(-|_{(t)}^{-1}|=-|_{(t)}|+||\). This observation allows us to preprocess the data matrix \(\) by orthogonalizing its rows, so that the update for \(\) in (10a) can be further simplified. The overall algorithm is summarized in Algorithm 1. We empirically found that setting \(=nk\) works very well in practice.

Experiments

We now provide some numerical experiments to showcase the effectiveness of the proposed volume optimization formulation using the two algorithms described in SS4. All the experiments are conducted in MATLAB.

### Optimization performance

We synthetically generate random problems, and show the surprisingly effective performance of the two algorithms proposed in SS4. For \(k=20\) and \(n=1000\), we randomly generate the groundtruth sparse coefficient matrix \(^{}\) according to the Bernoulli-Gaussian model with \(p=0.5\), and the groundtruth dictionary matrix \(^{}\) completely random, and generate the data matrix \(=^{}^{}\). Due to Theorem 3.4, we know it is identifiable with very high probability, despite \(n\) being not that big compared to the number of atoms \(k\). Matrix \(\) is used as input to the L-ADMM algorithm as described in Alg. 1. Although Problem (7) is nonconvex, as long as it is identifiable, we know the global optimum is attained at \(^{}=(^{})^{-1}\), where \(\) is a diagonal matrix with the \(i\)th diagonal equal to the inverse of the \(_{1}\) norm of the \(i\)th row of \(^{}\). As a result, \(-|(^{})^{-1}|\) is the optimal value for Problem (7) as long as the model is identifiable, and we shall see whether the proposed algorithm is able to attain that optimal value. On the other hand, since L-ADMM directly tackles formulation 8, it is not guaranteed that \(_{(t)}\) is feasible in every iteration, which makes little sense to check the difference \(-|_{(t)}|+|^{}|\). We instead check the optimality gap of the Lagrangian function values, using the optimal dual variable \(\), since we have

\[-||+(-)^{}- |^{}|,\]

for any \(\) and a feasible \(\). Obviously, the gap equals to zero when \(=^{}\) and \(=^{}\), in which case \(^{}-^{}=0\). Furthermore, it is easy to show that an optimal \(\) is \((}^{})^{}\). In this simulation with known groundtruth factors, we will use this to measure the optimality gap.

The convergence behavior of 100 random trials of the L-ADMM Algorithm 1 are shown in Figure 2. The somewhat surprising observation is that it achieves essentially zero optimality gap in all instances, which is somewhat surprising as we are, after all, trying to solve NP-hard problems. Among the 100 random trials, L-ADMM usually requires approximately 500-1000 iterations to converge; together with the low-complexity iterations described in Algorithm 1, it usually takes just a few seconds to execute a trial. For a dictionary learning problem of this size, our proposed algorithm takes a lot less time than most existing state-of-the-arts. Some more detailed comparisons can be found in the supplementary material.

### Dictionary recovery

Next we fix the sample size \(n=1000\) and vary the dictionary size as well as the sparsity parameter \(\) in the Bernoulli-Gaussian model and check how often do we get exact recovery of the groundtruth dictionary \(^{}\) up to column permutation and scaling. In the previous experiment we simply check the optimality gap between the algorithm output and \(-|(^{})^{-1}|\), which is also a pretty good indicator of exact recovery since the only ambiguity is an orthogonal rotation, which is very unlikely to exist. Nevertheless, to ensure definitively that the dictionary is recovered as \(^{-1}\), we will normalize the column and use the Hungarian algorithm (Kuhn, 1955) to find the best column matching, and then calculate the estimation error. We declare success if the estimation error is smaller than 1e-5. The results are shown in Figure 3, which agrees with the bound in Theorem 3.4.

Figure 2: 100 random trials of the proposed L-ADMM for dictionary learning as in Algorithm 1. Since we know the groundtruth dictionary is the (essentially) unique minimizer, we can use it to calculate the optimality gap. We see that the optimality gap goes to zero in all random instances.

We would like to emphasize that compared to similar figures that have appeared in the literature, the groundtruth dictionaries that we try to identify are far from being orthogonal, nor do we care to check whether they satisfy the incoherence assumption; moreover, the recovery is of the _entire_ dictionary, not one of the atoms. More comparisons with some existing algorithms can be found in the supplementary.

### Real data experiment

We conclude this section with a classical application of dictionary learning on image compression. For a given image, it is first divided into \(8 8\) non-overlapping patches, reshaped into a vector in \(^{k}\) with \(k=64\), and stacked as columns of the data matrix \(\). After feeding it into the L-ADMM Algorithm 1, the output is inverted to obtain an estimate of the dictionary \(\). Each column of \(\) is then reshaped back into a \(8 8\) patch to be shown as an atom for the dictionary. For a monochrome image with \(512 512\) pixels, it would be reshaped into a data matrix \(\) of size \(64 4096\). Thanks to the high-efficiency of the proposed L-ADMM algorithm, learning a dictionary on a data set of this size takes only a few minutes from a random initialization. The resulting dictionary learned from a natural image is shown in Figure 4. The result agrees with most other dictionary learning algorithms, with some atoms representing edges and bright spots, while some others resemble a few DCT bases.

## 6 Conclusion

In this paper we aimed at answering the question of when dictionary learning is identifiable, with a specific requirement that the formulation uses the \(_{1}\) norm to promote sparsity. With a novel formulation that minimizes the volume of the dictionary subject to \(_{1}\) constraints, we close the case with an affirmative yes, complete dictionary learning is indeed _globally identifiable_ under mild conditions. Compared to similar works, our result is global, requires no conditions such as incoherence on the complete dictionary, and at the same time achieves the same sample complexity bound when the sparse coefficients satisfy the Bernoulli-Gaussian model. Finally, we propose algorithms to solve the volume optimization problem, specifically one based on linearized ADMM in the main paper, and demonstrate their exceptional performances on both synthetic and real data.