# Sub-optimal Experts mitigate Ambiguity

in Inverse Reinforcement Learning

Riccardo Poiani

DEIB, Politecnico di Milano

riccardo.poiani@polimi.it &Gabriele Curti

DEIB, Politecnico di Milano

gabriele.curti@mail.polimi.it &Alberto Maria Metelli

DEIB, Politecnico di Milano

albertomaria.metelli@polimi.it &Marcello Restelli

DEIB, Politecnico di Milano

marcello.restelli@polimi.it

###### Abstract

Inverse Reinforcement Learning (IRL) deals with the problem of deducing a reward function that explains the behavior of an expert agent who is assumed to act _optimally_ in an underlying unknown task. Recent works have studied the IRL problem from the perspective of recovering the _feasible reward set_, i.e., the class of reward functions that are compatible with a unique optimal expert. However, in several problems of interest it is possible to observe the behavior of multiple experts with different degree of optimality (e.g., racing drivers whose skills ranges from amateurs to professionals). For this reason, in this work, we focus on the reconstruction of the feasible reward set when, in addition to demonstrations from the optimal expert, we observe the behavior of multiple _sub-optimal experts_. Given this problem, we first study the theoretical properties showing that the presence of multiple sub-optimal experts, in addition to the optimal one, can significantly shrink the set of compatible rewards, ultimately mitigating the inherent ambiguity of IRL. Furthermore, we study the statistical complexity of estimating the feasible reward set with a generative model and analyze a uniform sampling algorithm that turns out to be minimax optimal whenever the sub-optimal experts' performance level is sufficiently close to that of the optimal expert.

## 1 Introduction

_Inverse Reinforcement Learning_ [IRL, 26] deals with the problem of recovering a reward function that explains the behavior of an expert agent who is assumed to act optimally in an underlying unknown task. Over the years, the IRL problem has consistently captured the attention of the research community (see, for instance,  and  for in-depth surveys). Indeed, this general scenario, where the reward function needs to be learned, emerges in numerous real-world applications. A example arises from human-in-the-loop settings , where the expert is a human solving a task, and an explicit specification of the human's goal in the form of a reward function is often unavailable. Notably, humans encounter difficulty in expressing their intentions in a numerical form, preferring instead to demonstrate what they perceive as the correct behavior. Once we retrieve a reward function, (i) we obtain explicit information for understanding the expert's choices, and, furthermore, (ii) we can utilize it to train reinforcement learning agents, even under shifts in the underlying system.

Since the seminal work , IRL has emerged as a significantly complex task. One of its primary challenges lies in the intrinsic _ill-posed_ nature of the problem, as multiple reward functions compatible with the expert's behavior exist. Recently, a promising avenue of research [24; 19; 22] has tackled this ambiguity issue from an intriguing perspective. Specifically, this strand of works focuses on estimating _all_ the reward functions that are compatible with the observed demonstration, thereby postponing the selection of the reward and directing their focus solely on the expert's intentions.

Nevertheless, these approaches based on recovering the feasible set [24; 19; 22] fall short in modeling more articulated situations that arise in the real world. In several problems of interest, it is possible to observe the behavior of _multiple agents with different degrees of expertise_. As an illustrative example, consider the human-in-the-loop settings, mentioned above, in which we are interested in recovering reward functions that explain the intent behind racing drivers. In this scenario, racing car companies typically have access to a variety of drivers with diverse skills, including professionals, test drivers, and emerging talents from developmental programs. In this context, while the focus is typically on the reward function of professional drivers, we expect a proficient IRL method to effectively leverage demonstrations provided by drivers with lower expertise. From an intuitive perspective, if we have some information on the degree of expertise of other drivers, we expect that, by exploiting their demonstrations, we can reduce the ambiguity of IRL problem.

With these motivations, in this work, we extend the IRL formulation as the reconstruction of the feasible reward set to settings where, in addition to demonstrations from an optimal expert, we observe the behavior of multiple sub-optimal experts, of which we know a _bound on their sub-optimality_. More specifically, we will primarily focus in answering the following theoretical questions:

1. _How does the presence of sub-optimal experts affect the set of reward functions that are compatible with the observed behaviors? Can they mitigate the intrinsic ambiguity of IRL?_
2. _What is the statistical complexity of estimating the set of reward functions that are compatible with the given experts? How does it compare against the one of single-experts IRL problems?_

Contributions and Outline.After providing the necessary background, we introduce the novel problem of Inverse Reinforcement Learning with multiple and sub-optimal experts (IRL-SE, Section 2). We then proceed by studying the _theoretical properties_ of the class of reward functions that are compatible with a given set of experts under the assumption that an upper bound on the performance between a sub-optimal expert and the optimal expert is available to the designer of the IRL system (Section 3). Our findings indicate that having multiple sub-optimal experts can significantly shrink the set of compatible rewards, thereby _mitigating_ the ambiguity issue that affects IRL. Leveraging our previous results, we continue by studying the _statistical complexity_ of estimating the feasible reward set with a generative model (Section 4). To this end, after formally introducing a Probabilistic Approximately Correct [PAC, 9] framework, we derive a novel lower bound on the number of samples that are required to obtain an accurate estimate of the feasible reward set. Then, we present a uniform sampling algorithm and analyze its theoretical guarantees. Our results show that (i) the IRL problem with sub-optimal experts is statistically harder than the single expert IRL setting, and (ii) that the uniform sampling algorithm is minimax optimal whenever the sub-optimal experts' performance level is sufficiently close to the one of the optimal expert. Finally, we conclude with a discussion on existing works (Section 5) and by highlighting potential avenues for future research (Section 6).

## 2 Preliminaries

In this section, we provide the notation and essential concepts employed throughout this document. Appendix A contains tables of symbols and a summary of the notation.

Notation.Let \(\) be a finite set, we denote with \(^{}\) the set of probability measures over \(\). Let \(\) be a set, we denote with \(^{}_{}\) the set of functions \(f:^{}\). Given \(f^{n}\), we denote with \(\|f\|_{}\) the infinite norm of \(f\). Let \(\) and \(^{}\) be two non-empty subsets of a metric space \((,d)\), we define the Hausdorff distance  between \(\) and \(^{}\) as \(H_{d}(,^{})=\{_{x}_{x^{ }^{}}d(x,x^{}),_{x^{}^ {}}_{x}d(x,x^{})\}\). The Hausdorff distance directly depends on the metric \(d\). We denote with \(_{n}\) the \(n\)-dimensional vector given by \((1,,1)^{}\).

Markov Decision Processes.A Markov Decision Process _without a reward function_ (MDPR) is a tuple \(=(,,p,)\), where \(\) is the set of states, \(\) is the set of actions, \(p^{}_{}\) is the transition probability kernel, and \([0,1)\) is the discount factor. We consider finite state and action spaces, namely \(||=S\) and \(||=A\). A Markov Decision Process [MDP, 27] is obtained by combining an MDPR \(\) with a reward function \(r^{S}\). Without loss of generality, we assume reward functions bounded in \(\). We denote with \( r\) the resulting MDP. The behavior of an agent is described by a policy \(_{}^{}\), that, for each state, prescribes a distribution over actions.

Operators.Let \(f^{}\) and \(g^{}\). We denote with \(P\) and \(\) the operators induced by the transition model \(p\) and the policy \(\) respectively.1 Specifically, \(Pf(s,a)=_{s^{}}p(s^{}|s,a)f(s^{})\), and \( g(s)=_{a}(a|s)g(s,a)\). Moreover, we introduce the operators \(E\) and \(^{}\) defined as: \(Ef(s,a)=f(s)\) and \((^{}g)(s,a)=1\)\(\{(a|s)=0\}\)\(g(s,a)\). Finally, we define \(d^{}\) as the expectation of \(f\) under the discounted occupancy measure: \(d^{}f=(I_{}- P)^{-1}f=_{t=0}^{+}(  P)^{t}f\).

Value Functions and Optimality.Given an MDP \( r\) and a policy \(\), the _Q-function_\(Q^{}_{ r}\) represents the expected discounted sum of rewards collected in \( r\) starting from \((s,a)\) and following policy \(\). Formally:

\[Q^{}_{ r}(s,a)=[_{t=0}^{+}^{ t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a],\]

where the expectation is taken w.r.t. the stochasticity of the policy and the environment, i.e., \(s_{t+1} p(|s_{t},a_{t})\) and \(a_{t}(|s_{t})\). The _V-function_\(V^{}_{ r}\) is the expectation of the _Q-function_ over the action space, namely \(V^{}_{ r}= Q^{}_{ r}\). The _advantage function_\(A^{}_{ r}=Q^{}_{ r}-EV^{}_{  r}\) is the immediate gain of taking a given action, rather than following policy \(\). A policy \(^{*}\) is optimal if it has non-positive advantage in each-state action pair; namely \(A^{^{*}}_{ r} 0\) holds element-wise.

Inverse Reinforcement Learning.An Inverse Reinforcement Learning [IRL, 26] problem is a tuple \(=(,_{E})\), where \(\) is an MDPR and \(_{E}_{}^{}\) is the expert policy. Given a reward function \(r^{}\), we say that \(r\) is _feasible_ for \(\) if it is compatible with the behavior of the expert, namely \(_{E}\) is an optimal policy for the MDP \( r\). We denote with \(_{}\) the set of feasible reward functions:

\[_{}=\{r^{}:A^{ _{E}}_{ r} 0\}.\] (1)

The set \(_{}\) is named _feasible reward set_. To characterize the set \(_{}\),  have shown that a reward function \(r\) belongs to \(_{}\) if and only if there exists \(^{}_{ 0}\) and \(V^{}\) such that:

\[r=-^{_{E}}+(E- P)V.\] (2)

Thus, each reward function in \(_{}\), is the sum of two components. The first one, \(-^{_{E}}\), which is non-zero only when \(_{E}(a|s)=0\), can be interpreted as the advantage function \(A^{_{E}}_{ r}\). The second one, \((E- P)V\), instead, can be interpreted as a reward-shaping via function \(V\), which maintains the optimality of the expert's policy . It follows that \(\|V\|_{}(1-)^{-1}\) and \(\|\|_{}(1-)^{-1}\).

## 3 Sub-Optimal Experts and the Feasible Reward Set

In this section, we extend the IRL formulation to problems where, in addition to demonstrations from an optimal expert, we observe the behaviors of multiple and sub-optimal experts. After having formulated and motivated the problem (Section 3.1), we delve into the theoretical properties of the induced feasible reward set, by providing both an _implicit_ (Section 3.2) and _explicit_ (Section 3.3) descriptions. Our results indicate that the presence of sub-optimal experts can significantly shrink the feasible set of compatible rewards, thus, mitigating the ambiguity issue of IRL.

### Problem Formulation

We define the _Inverse Reinforcement Learning problem with multiple and Sub-optimal Experts_ (IRL-SE) as a tuple \(}=(,_{E_{1}},(_{E_{i}})_{i=2}^{n+1},( _{i})_{i=2}^{n+1})\), where \(\) is an MDPR, \(_{E_{1}}\) is the policy of an optimal expert,2\((_{E_{i}})_{i=2}^{n+1}\) are a collection of \(n\) sub-optimal experts policies, and \((_{i})_{i=2}^{n+1}\) are the corresponding sub-optimality bounds. More concretely, \(_{i}\) represents a known _upper bound_ on the performance gap between the optimal expert and the \(i\)-th sub-optimal expert. Consequently, a reward function \(r^{}\) is feasible for \(\) if \(_{E_{1}}\) is an optimal policy for the MDP \( r\) and if:

\[ i\{2,,n+1\}:\|V_{ r}^{_{E_{1}}}- V_{ r}^{_{E_{i}}}\|_{}_{i}.\] (3)

Thus, a feasible reward \(r\) must make the value function \(V_{ r}^{_{E_{1}}}(s)\) of the \(i\)-th expert smaller than that of the optimal expert \(V_{ r}^{_{E_{1}}}(s)=V_{ r}^{*}(s)\) by no more than the threshold \(_{i}\), uniformly over \(s\). We denote by \(_{}\) the set of feasible rewards for \(\), i.e., \(r^{}\) belongs to \(_{}\) if (i) \(A_{ r}^{_{E_{1}}} 0\) and (ii) Equation (3) holds. Notice that, whenever no sub-optimal expert is present, we recover the definition of the feasible set for single-expert IRL problems, i.e., \(_{}\) in Equation (1).

We remark that \(_{i}\) can even be a crude overestimate (i.e., an upper bound) of the sub-optimality of expert \(i\). Nevertheless, as we shall see, the ability of the sub-optimal policy \(_{E_{i}}\) in mitigating the IRL ambiguity, i.e., shrinking the feasible reward set, will be tightly connected on the magnitude of \(_{i}\).The following examples illustrate how an expression \(_{i}\) can be obtained in common scenarios with no knowledge of the (possible) reward function optimized by the expert policy \(_{E_{1}}\). Formal proof for these statements are reported in Appendix C.

**Example 3.1**.: Suppose that the \(i\)-th sub-optimal expert \(_{E_{i}}\) is optimal for the same reward function that \(_{E_{1}}\) is optimizing for, but under different transition model \(P_{i}\). In this case, \(\|V_{ r}^{_{E_{1}}}-V_{ r}^{_{E_{i}}}\|_{ }}_{(s,a) }\|P(|s,a)-P_{i}(|s,a)\|_{1}=_{i}\) holds for all rewards \(r\).

**Example 3.2**.: Suppose that the \(i\)-th sub-optimal expert \(_{E_{i}}\) is optimal for the same reward function that \(_{E_{1}}\) is optimizing for, but using a different discount factor \(^{}\). In this case, \(\|V_{ r}^{_{E_{1}}}-V_{ r}^{_{E_{i}}}\|_{ } 2\|}{(1-)(1-^{})} _{i}\) holds for all rewards \(r\).

**Example 3.3**.: Suppose that the \(i\)-th sub-optimal expert \(_{E_{i}}\) is sufficiently close to the optimal policy \(_{E_{1}}\), namely that \(_{s}\|_{E_{1}}(|s)-_{E_{i}}(|s)\|_{1}\). In this case, \(\|V_{ r}^{_{E_{1}}}-V_{ r}^{_{E_{1}}}\|_{ }_{i}\) holds for all rewards \(r\).

### Implicit Formulation of \(_{}\)

In this section, we analyze the implicit description of the feasible reward set \(_{}\). From its definition (Equations (1) and (3)), a reward function \(r\) belongs to \(_{}\)_if and only if_ the following conditions are satisfied:

1. \(Q_{ r}^{_{E_{1}}}(s,a)=V_{ r}^{_{E_{1}}}(s) (s,a):_{E_{1}}(a|s)>0\),
2. \(Q_{ r}^{_{E_{1}}}(s,a) V_{ r}^{_{E_{1}}} (s)(s,a):_{E_{1}}(a|s)=0\),
3. \(V_{ r}^{_{E_{1}}}(s) V_{ r}^{_{E_{1}}} (s)+_{i} s,\  i\{2,,n+1\}\).

Specifically, conditions (i) and (ii) directly encode the optimality of policy \(_{E_{1}}\) for \( r\), i.e., the advantage function \(A_{ r}^{_{E_{1}}}\) is non-positive. Condition (iii), on the other hand, arises from the presence of sub-optimal experts, and directly follows from Equation (3). At this point, by closely examining these conditions, it is possible to gain insight into the advantages and limitations associated with the presence of multiple and sub-optimal experts. Consider, the following illustrative examples.

**Example 3.4**.: Suppose that \(_{E_{i}}=_{E_{1}}\) holds for all \(i\{2,,n+1\}\). Condition (iii) is clearly satisfied for any reward function \(r\). Thus, the feasible reward set \(_{}\) is determined by the requirement that the advantage function of \(_{E_{1}}\) is non-negative, and, as a consequence, the set \(_{}\) coincides with the one of the single-expert IRL problem, namely \(_{}=_{}\). Analogously, if \(_{i}(1-)^{-1}\) holds for all sub-optimal experts, condition (iii) is vacuous, and, similarly, \(_{}\) reduces to \(_{}\).

**Example 3.5**.: Consider the MDP with \(2\) states depicted in Figure 1, and suppose that only one additional sub-optimal expert is present. The optimal and the sub-optimal experts follow different policies in \(S_{0}\). From the definition of \(_{}\), we can see that, in addition to the constraint \(r(S_{0},A_{1}) r(S_{0},A_{2})\) (i.e., \(_{E_{1}}\) is optimal), condition (iii) enforces a further relationship between \(r(S_{0},A_{1})\) and \(r(S_{0},A_{2})\), i.e., \(r(S_{0},A_{1})-r(S_{0},A_{2})_{i}\). If \(_{i}\) is sufficiently small (i.e., \(_{i}<1\)), the presence of the sub-optimal experts significantly reduces the set of compatible rewards (Figure 2).

Abstracting away from the examples, we deduce that whenever (a) the sub-optimal experts exhibit behavior significantly different from that of the optimal expert and (b) their performance level is sufficiently close to being optimal, \(_{}}\) can notably shrink compared to \(_{}\). In the next section, through an explicit formulation of the feasible set, we analyze this phenomenon quantitatively.

### Explicit Formulation of \(_{}}\)

In this section, we continue by providing an explicit formulation of the feasible set \(_{}}\). The following result (proof in Appendix D) summarizes our findings.

**Theorem 3.6**.: _Let \(}\) be an IRL problem with sub-optimal experts. Let \(r^{}\). Then, \(r_{}}\) if and only if there exists \(_{ 0}^{}\) and \(V^{}\) such that the following conditions are satisfied:_

\[r=-^{^{E_{1}}}+(E- P)V,\] (4)

_and, for all \(i\{2,,n+1\}\):_

\[d^{_{E_{i}}}_{E_{i}}^{_{E_{1}}}_{}_{i}.\] (5)

Theorem 3.6 deserves some comments. First, from Equation (4), we see that a necessary condition for having \(r_{}}\) is that it can be expressed as the sum of two components, namely \(-^{^{E_{1}}}\) and \((E- P)V\). This result is a direct consequence of the fact that \(_{E_{1}}\) is an optimal policy for \( r\), and, in this sense, it recovers the existing condition of single expert IRL settings (Equation 2) .3

Sub-optimal experts constrain the sub-optimality gaps.The role of the sub-optimal experts is apparent in Equation (5) enforcing a set of _linear constraints_ on the values that \(\) can take.4 Since, as discussed in Section 2, \(-^{_{E_{1}}}\) represents the advantage function of the optimal policy \(_{E_{1}}\), Equation (5) limits how much sub-optimal the values of actions not played by \(_{E_{1}}\) can be. Indeed, the resulting \(Q\)-function of the optimal expert can be expressed as \(Q_{ r}^{_{E_{1}}}=-^{_{E_{1}}}+EV\).

Sub-optimal experts affect only states in which they behave sub-optimally.The linear constraints in Equation (5) are expressed in terms of \(_{E_{i}}^{_{E_{1}}}\). As a consequence, they will only affect state-action pairs \((s,a)\) that are played by the sub-optimal experts (i.e., \(_{E_{i}}(a|s)>0\)) and that are not played by the optimal expert (i.e., \(_{E_{1}}(a|s)=0\)). Therefore, as previously highlighted with the implicit formulation of \(_{}}\), a sub-optimal expert \(_{E_{i}}\) should behave differently w.r.t. the optimal expert \(_{E_{1}}\) in order to provide meaningful information and reduce the feasible reward set. Furthermore, the constraints over \(\) are expressed w.r.t. expected discounted occupancy of \(_{E_{i}}\).

Figure 1: MDP example, with \(2\) states and \(2\) experts, that highlights the benefits of sub-optimal experts (Example 3.5). In \(S_{1}\) both \(_{E_{1}}\) and \(_{E_{i}}\) are identical, i.e., \(_{E_{1}}(|S_{1})=_{E_{i}}(|S_{1})=1\).

Figure 2: Visualization of the feasible reward set (i.e., shaded red area) for the problems described in Example 3.5. On the left, the feasible reward set for the single-expert IRL problem and on the right the feasible reward set for the multiple and sub-optimal setting when using \(_{i}=0.5\).

Therefore, Equation (5) has provided a precise mathematical description (i.e., _if and only if conditions_) of the effect produced by the presence of sub-optimal experts on the feasible rewards \(r\).

While in classical IRL problems, we have \(\|\|_{}(1-)^{-1}\), the maximum value \(\) can take in the sub-optimal experts case may be smaller. Let us fix a state \(s^{}\) and a sub-optimal expert \(i\{2,,n+1\}\); then, the constraint associated to state \(s^{}\) in Equation (5) can be written as:

\[_{s}d_{s^{}}^{_{E_{i}}}(s)_{a: _{E_{1}}(a|s)=0}_{E_{i}}(a|s)(s,a)_{i},\] (6)

where \(d_{s^{}}^{_{E_{i}}}(s)\) is the discounted expected number of times that policy \(_{E_{i}}\) visits state \(s\) starting from state \(s^{}\). From Equation (6), we obtain necessary conditions on the values of \(\). More specifically, let \((s,a)\{2,,n+1\}\) be the subset of sub-optimal experts such that \(_{E_{i}}(a|s)>0\). Then, for each state-action pair \((s,a)\) such that \(_{E_{1}}(a|s)=0\) and \(_{E_{i}}(a|s)>0\), we have that:

\[(s,a)\{k(s,a),\} g(s,a), \ \ \ \ k(s,a)_{i(s,a),s^{}}}{d_{s^{}}^{_{E_{i}}}(s)_{E_{i}}(a|s)}.\] (7)

The term \(k(s,a)\) directly follows from Equation (6), while \((1-)^{-1}\) is the maximum value that any \((s,a)\) can take, and arises, as in the classical IRL setting, from the fact that advantage functions are bounded by \((1-)^{-1}\) for any reward function. Thus, confirming our previous observations, Equation (7) implies a significant potential reduction in the maximum values that the advantage function can take, i.e., how much sub-optimal an action not played by \(_{E_{1}}\) can be in terms of \(Q\)-function values. 5

**Example 3.7**.: Consider an IRL problem with only one additional sub-optimal expert. Suppose that \(_{E_{1}}\) and \(_{E_{i}}\) are deterministic. For all state-action pairs in which \(_{E_{1}}(a|s)=0\) and \(_{E_{i}}(a|s)=1\), Equation (7) implies that \((s,a)\{_{i},(1-)^{-1}\}\). If \(_{i}\) is significantly smaller than \((1-)^{-1}\), we obtain a notable restriction on the set of feasible reward functions.

**Remark 3.8** (About alternative Sub-optimality Formulations).: We have described the sub-optimal experts by means of _upper bounds_ on the value functions gaps; i.e., \(_{ r}^{_{E_{1}}}-_{ r}^ {_{E_{i}}}_{S}_{i}\). In principle, we may consider _lower bounds_ on the value functions gaps; i.e., \(_{ r}-_{ r}_{S}_{i}\). Intuitively, in the former case, we know that an expert is sub-optimal _at most_\(_{i}\), whereas, in the latter, that an expert is sub-optimal _at least_\(_{i}\). Our theory smoothly generalizes to these latter scenario obtaining an analogous of Theorem 3.6 where the constraints of Equation (5) are replaced with \(d^{_{E_{i}}}_{E_{i}}^{_{E_{i}}}_{S}_{i}\). We refer the reader to Theorem D.5 for a formal statement.

## 4 Learning the Feasible Set with Sub-Optimal Experts

In this section, we address the statistical complexity of estimating \(_{}}\) with a _generative model_. Specifically, we first introduce a Probabilistic Approximately Correct (PAC) framework (Section 4.1). Then, we study the statistical complexity of the problem by presenting lower bounds on the sample complexity any algorithm requires in order to correctly identify the feasible set (Section 4.1). Finally, we propose a uniform sampling algorithm and analyze its theoretical guarantees (Section 4.3).6

### PAC Framework

We define a learning algorithm for an IRL-SE problem \(}\) as a tuple \(=(,)\) where \(\) is a stopping and \(=(_{t})_{t}\) is a history-dependent sampling strategy, i.e., \(_{t}_{_{t-1}}^{}\), where \(_{t}=(()^{n+1})^{t}\) is the set of samples collected up to time step \(t\). At each time step \(t\), the algorithm selects a state-action pair \((s_{t},a_{t})_{t}\), and observes a sample \(s^{}_{t} p(|s_{t},a_{t})\) from the environment, together with actions sampled from the experts' policy, namely \((a^{(i)}_{i})_{i=1}^{n+1}\), where \(a^{(i)}_{t}_{E_{i}}(|s_{t})\). The observed realizations \(_{t}\) are then used to update the sampling strategy \(_{t}\), and the process goes on until the stopping rule \(\) is satisfied. Then, the algorithm leverages the collected data \(_{}\) to output the estimate of the feasible reward set \(}_{}\). We are interested in designing algorithms that, for any desired accuracy \((0,1)\) and any risk parameter \((0,1)\), guarantee that:

\[_{,}}H_{}(_{ }},}_{})>.\] (8)

We refer to these algorithms as \((,)\)-correct identification strategies and we define their sample complexity as the total number of interactions with the generative model before stopping, i.e., \(\).

### Lower Bound

In this section, we present lower bounds on the sample complexity that any \((,)\)-correct algorithm needs to perform to learn \(_{}}\). The following theorem (proof in Appendix E) reports our result.

**Theorem 4.1**.: _Let \(\) be a \((,)\)-correct algorithm for the IRL problem with sub-optimal experts. There exists a problem instance \(}\) such that the expected sample complexity is lower bounded by:_

\[_{,}}[]((1-)^{2}}(()+S) ),\] (9)

_where \(()\) hides constant dependencies. Let \(_{}>0\) be:_

\[_{}_{i\{2,+1\}\ (s,a) S :_{E_{i}}(a|s)>0}_{E_{i}}(a|s),\] (10)

_and let \(q_{0}_{}^{-1}_{i\{2,,n+1\}}\,_{i}\). Then, there exists an instance \(}^{}\) in which \(q_{0}<1\) such that:_

\[_{,}^{}}[] ^{2}S()}{^{2}_{ }}.\] (11)

Theorem 4.1 provides two distinct lower bounds (i.e., Equations 9 and 11) for IRL problems with sub-optimal experts. Whenever \(q_{0}<1\) holds, the lower bound for the IRL-SE setting can be expressed as the maximum between Equation (9) and (11). We now comment in-depth on these two equations.

Sub-optimal experts do not reduce the statistical complexity of IRL.Concerning Equation (9), as our analysis reveals, it directly arises from the problem of estimating rewards functions that are compatible with \(_{E_{1}}\) (i.e., with Equation (4) in Theorem 3.6). In this sense, it represents the complexity of single-expert IRL problems.7 As a consequence of the structure of the feasible set we derived in Theorem 3.6, this leads to a lower bound also for the multiple sub-optimal experts setting. Therefore, Equation (9) formally shows that the sub-optimal expert setting is always at least as difficult as the single expert IRL problem.

Sub-optimal experts have introduce further statistical complexities.Equation (11), on the other hand, is tightly related to the presence of sub-optimal experts. More precisely, under the assumption that \(q_{0}<1\) (e.g., for sufficiently small values of \(_{i}\)), it shows a dependency in the lower bound of a factor \(_{}^{-1}\), where \(_{}\) is the minimum probability with which sub-optimal experts plays their actions. From an intuitive perspective, its presence is related to the difficulty in estimating reward functions that are compatible with Equation (5) in Theorem 3.6. As we have shown in Section 3, the presence of sub-optimal experts can limit the value of \(\) with a relationship that involves \(_{}^{-1}\) (i.e., Equation (7)). As our analysis will reveal, the proof of Equation (11) is directly related to these worst-case upper-bounds on \(\). We remark that, according to the value of \(_{}\), Equation (11) can be significantly larger than Equation (5), thus showing an increased difficulty in the statistical complexity that is related to the stochasticity of sub-optimal experts.

We remark that \(\) represents the number of calls to the generative model and that each call, in our setting, provides demonstrations from _each_ (sub-)optimal expert. It can be shown that, by slightly modifying the learning formalism, Equation (11) actually represents a _lower bound to the number of samples that should be gathered from_ each _sub-optimal expert_.8 In this sense, the statistical complexity increases significantly in the sub-optimal expert setting compared to the single expert one.

Therefore, to conclude, we notice that, in order to gain the reduction in the feasible reward set that we discussed in Section 5, we need to gather additional sampled demonstrations from the sub-optimal experts. This unavoidable trade-off is a direct consequence of the structure of the feasible set \(_{}}\) that we derived in Theorem 3.6, and, it arises from the statistical complexity of estimating reward functions that are compatible with the linear constraints of Equation (5).

### Uniform Sampling Algorithm

In this section, we present the _Uniform Sampling algorithm for Inverse RL with Suboptimal Experts_ (US-IRL-SE).

Algorithm.The pseudo-code can be found in Algorithm 1 US-IRL-SE takes as input the number of samples \(m\) that will be queried to the generative model in each state-action pair. Then, it uniformly gathers data across the entire state-action space, and it updates the empirical estimates \(_{t}\) and \((_{t,E_{i}})_{i=1}^{n+1}\) of the transition model and expert's policies. Let \(_{t}\) be a dataset of \(t\{1,,m\}\) tuples \(_{t}=\{(s_{j},a_{j},s^{}_{j},(a_{j}^{(i)})_{i=1}^{n+1})\}_{j=1 }^{t}\), where \(s^{}_{j} p(|s_{j},a_{j})\), and \(a_{j}^{(i)}_{E_{i}}(|s_{j})\). Given \(_{t}\), we define the empirical transition model \(_{t}\) and the empirical experts' policy \(_{t,E_{i}}\) as follows:

\[_{t}(s^{}|s,a)=(s,a,s^{})}{N_{t}(s, a)}&N_{t}(s,a)>0\\ &,_{t,E_{i}}(a|s)= ^{(i)}(s,a)}{N_{t}(s)}&N_{t}(s)>0\\ &,\] (12)

where \(N_{t}(s,a,s^{})=_{j=1}^{t}\{(s_{j},a_{j},s^{}_{j})=( s,a,s^{})\}\), \(N_{t}(s,a)=_{s^{}}N_{t}(s,a,s^{})\), \(N_{t}(s)=_{(a,s^{})}N_{t}(s,a,s^{ })\), and \(N_{t}^{(i)}(s)=\{(s_{j},a_{j}^{(i)})=(s,a)\}\). Then, we denote with \(}}_{t}\) the empirical IRL problem induced by \(_{t}\) and \((_{t,E_{i}})_{i=1}^{n+1}\). Finally, the algorithm returns the feasible set \(_{}_{m}}\) corresponding to the estimated IRL-SE problem \(}_{m}\) defined in terms of \(_{m}\) and \((_{m,E_{i}})_{i=1}^{n+1}\).

Sample Complexity Upper BoundThe following theorem (proof in Appendix E), describes the theoretical guarantees of US-IRL-SE.

**Theorem 4.2**.: _Let \(q_{1}=\{_{}^{-1}_{i\{2,,n+1\}}_{i},(1- )^{-1}\}\), and \(q_{2}=\{1,q_{1}\}\). Then, with a total budget of:_

\[}(\{^{2}S()}{^{2}_{}},^{2}SA(S+( ))}{^{2}(1-)^{2}}\}),\] (13)

_US-IRL-SE is \((,)\)-correct and \(}()\) hides constant and logarithmic dependencies._

Theorem 4.2 deserves some comments. First of all, it shows that when the total number of queries to the generative is sufficiently large, US-IRL-SE is \((,)\)-correct, and its sample complexity is provided in Equation (13). Since \(m\) represents the number of calls to the generative model in each state-action pair, its expression can simply be calculated dividing Equation (13) by \(SA\).9 As a consequence, we remark that, in order to compute the value of \(m\), the algorithm requires knowledge of the minimum probability \(_{}\) with which the sub-optimal experts play their actions.

US-IRL-SE is minimax optimal when \(_{}^{-1}_{i} 1\).Equation (13) is the maximum between two terms whose expressions closely resemble the lower bound of Theorem 4.1. The only difference arises in the definition of \(q_{0}\), \(q_{1}\) and \(q_{2}\). We remark that, whenever the sub-optimal expert's performance level is sufficiently close to the one of the optimal expert, i.e., \(_{}^{-1}_{i} 1\) for all \(i\{2,,n+1\}\),Equation (13) exactly recovers the lower bound that we presented in Theorem 4.1. Indeed, under this condition, it holds that \(q_{0}=q_{1}\), and \(q_{2}=1\). We recall that, according to Theorem 3.6, as the values of \(_{i}\)'s decrease, the feasible reward set is substantially reduced. In this sense, _US-IRL-SE enjoys minimax optimality in the most interesting scenarios where the presence of sub-optimal experts is particularly useful for mitigating the intrinsic ambiguity that affects IRL problems_.

Technical challenges of Theorem 4.2.We highlight that the proof of Theorem 4.2 poses notable technical challenges beyond the ones of the analysis for standard IRL . In our setting, studying how the Hausdorff distance between \(_{}_{t}}\) and \(_{}_{t}}\) decreases as we collect more samples requires taking into account that these feasible reward sets are subject to the peculiar structure identified in Theorem 3.6, namely the set of constraints of Equation (5) arising from the presence of sub-optimal experts. We study, with probabilistic arguments, error terms of the form \(\|-_{_{t}}\|_{}\), where \(\) is feasible for the exact problem \(}\), \(_{t}\) denotes the set of \(\) that are compatible with the empirical problem \(}_{t}\), and \(()\) denotes the infinite norm projection. Analyzing these terms to obtain nearly optimal rates requires careful considerations on the geometry of feasible reward set that the sub-optimal experts induce. For further details, we refer the interested reader to Appendix E.

## 5 Related Works

In this section, we survey the works about IRL and the presence of multiple (sub-optimal) experts that are related to our proposal.

Inverse Reinforcement Learning.Historically, solving an IRL problem  involves determining a reward function that is compatible with the behavior of an optimal expert. Since the seminal work of , the problem has been recognized as ill-posed, as multiple reward functions that satisfies this requirement exists . For this reason, over the years, several algorithmic criteria have been introduced to address this ambiguity issue. These criteria includes maximum margin , Bayesian approaches , maximum entropy , and many others [e.g., 21, 23, 36]. More recently, a new line of works have circumvented the ambiguity issue by redefining the IRL task as the problem of estimating the entire feasible reward set . In our work, we take this novel perspective, and, in this sense, this recent research strand is the most related to our document. Specifically, of particular interests is the work of . In their work, the authors study, for the first time, lower bounds for the single-expert IRL problem in finite horizon settings. Furthermore, they show that uniform sampling algorithm is minimax optimal for this task. Nevertheless, it has to remarked that this recent strand of research focuses entirely on single expert problems. As we have shown, however, the extension to the multiple and sub-optimal experts setting requires non-trivial effort. The reason is that the feasible reward set significantly differ (see, e.g., Theorem 3.6), and the problem is harder from a statistical perspective (see, e.g., Theorem 4.1).

Multiple and/or Sub-optimal Experts.The presence of multiple/sub-optimal experts has garnered attention in the Imitation Learning [IL, 11] community. In IL problems, contrary to IRL, the goal lies in directly leveraging demonstrations of optimal behavior to accelerate the training process of reinforcement learning algorithms. In this context, works that are close in spirit to ours are . Here, the authors extend the IL formulation to account for the fact that demonstrations are provided from multiple and/or sub-optimal experts. However, unlike our specific focus, their emphasis is on understanding how to effectively exploit imperfect demonstrations to improve training of RL agents. In our work, instead, we exploit the presence of sub-optimal experts to reduce the intrinsic ambiguity that affects the IRL formulation. In this sense, our work is complementary to several studies that analyzed how to improve the identifiability of the reward function in IRL problems by making additional structural assumptions. These include the possibility of observing an optimal expert interacting with several MDPs [e.g., 30, 3, 2] and focusing on peculiar types of MDPs that allows for strong theoretical guarantees [e.g., 8, 15, 5]. Along this line of work, the most related to ours is . Here, the authors study how the presence of multiple experts impact the identifiability of the reward function. Contrary to our work, however, the authors assume each expert to follow an entropy regularized objective and, furthermore, they focus on the case in which all experts act optimally in the underlying environment. In this sense, our work encompasses a wider spectrum of applications, as we do not require optimality for each of the expert, nor an entropy regularized objective. Furthermore, it has to be remarked that the multiple expert setting and IRL have been studied in  with the goal of providing practical algorithms that can be used in real-world applications. Also in this scenario, each expert is assumed to act optimally in the underlying domain. Finally, our work is related to approaches that aimed at extracting a _single_ reward function by leveraging possibly sub-optimal demonstrations [e.g., 35, 16, 12, 29]. In our work, instead, we take a different theoretical perspective, and focus on the _set_ of reward functions that are compatible with multiple sub-optimal experts.

## 6 Conclusions

In this work, we studied the novel problem of Inverse RL where, in addition to demonstrations from an optimal expert, we can observe the behavior of multiple and sub-optimal experts. More precisely, we first investigated the theoretical properties of the class of reward functions that are compatible with a given set of experts, i.e., the feasible reward set. Our results formally show that, by exploiting this additional structure, it is possible to significantly reduce the intrinsic ambiguity that affects the IRL formulation. Secondly, we have tackled the statistical complexity of estimating the feasible reward set from a generative model. More precisely, we have shown that a uniform sampling algorithm is minimax optimal whenever the performance level of the sub-optimal expert is sufficiently close to the one of the optimal expert.

Our research opens up intriguing avenues for future studies. In the following, we highlight several possibilities.

Closing the Theoretical GapThe results that we presented in Section 4 do not completely match. When the performance of the sub-optimal experts are not sufficiently close to the one of the optimal agent, the upper bound differs from the lower bound. Closing this gap, either by (i) developing tighter lower bounds or (ii) proposing novel algorithms and/or refining the analysis of US-IRL to achieve tighter upper bounds, is an interesting future direction. This would allow for a complete understanding of the statistical complexity of the problem.

Offline IRL with Sub-Optimal ExpertsSecondly, we note that this work leverages the presence of a generative model in the algorithm design. In the future, it would be interesting to remove this assumption by considering an offline setting where only a dataset of collected demonstrations is available to the learning system. This formulation is more practical since, in several real-world IRL applications , only a dataset of pre-collected demonstrations is available to the designer of the IRL system. We also note that this setting is, in principle, more challenging, as the data coverage of the dataset is not under the control of the IRL system.

Large State-Action SpacesFor instance, since we have shown that sub-optimal experts can improve the identifiability of the reward function, future research should focus on building practical algorithms that can exploit this additional structure. To this end, as an intermediate step, it might be interesting to extend our results to the case in which the reward function is expressed as a linear combination of features. This approach would enable addressing infinite state-spaces [e.g., 26].

Funded by the European Union - Next Generation EU within the project NRPP M4C2, Investment 1.,3 DD. 341 - 15 march 2022 - FAIR - Future Artificial Intelligence Research - Spoke 4 - PE00000013 - D53C22002380006. AI4REALNET has received funding from European Union's Horizon Europe Research and Innovation programme under the Grant Agreement No 101119527. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them.