# Combining Statistical Depth and Fermat Distance for Uncertainty Quantification

Hai-Vy Nguyen\({}^{1,2,3}\), Fabrice Gamboa\({}^{2}\), Reda Chhaibi\({}^{2}\), Sixin Zhang\({}^{3}\), Serge Gratton\({}^{3}\),

Thierry Giaccone\({}^{1}\)

\({}^{1}\) Ampere Software Technology

\({}^{2}\) Institut de mathematiques de Toulouse

\({}^{3}\) Institut de Recherche en Informatique de Toulouse

{thierry.giaccone, hai-vy.nguyen}@ampere.cars,

{reda.chhaibi, fabrice.gamboa}@math.univ-toulouse.fr,

{serge.gratton, sixin.zhang}@irit.fr

###### Abstract

We measure the out-of-domain uncertainty in the prediction of Neural Networks using a statistical notion called "Lens Depth" (LD) combined with Fermat Distance, which is able to capture precisely the "depth" of a point with respect to a distribution in feature space, without any distributional assumption. Our method also has no trainable parameter. The method is applied directly in the feature space at test time and does _not_ intervene in training process. As such, it does _not_ impact the performance of the original model. The proposed method gives excellent qualitative results on toy datasets and can give competitive or better uncertainty estimation on standard deep learning datasets compared to strong baseline methods.

## 1 Introduction

We consider a multi-class classification problem with the input space \(\). In general, a classification model consists of a feature extractor (backbone) \(_{_{1}}\) and a classifier \(h_{_{2}}\): \(f_{}=h_{_{2}}_{_{1}}\), where \(=(_{1},_{2})\) is the set of parameters of the model. The backbone transforms inputs into fixed-dimension vectors in the so-called _feature space_\(\). The classifier \(h\) then maps the features to predictions. The model \(f_{}\) is trained on i.i.d. examples drawn from _In-Distribution_ (ID) \(P_{}\). \(f_{}\) denotes the trained model.

**OOD detection.** Classification by neural networks has proved highly effective in terms of precision. However, beside performance, in critical applications, one needs to detect out-of-distribution (OOD) data for safety reasons. Indeed, at the inference stage, the model should only predict for data coming from the ID and reject OOD samples. For this purpose, one needs to associate a confidence (or uncertainty) score \(\) with these data so that one can reject uncertain predictions. This is referred as _Out-of-domain uncertainty_. At the inference stage, \(x\) is considered as ID if \(S(x)\) (with some threshold \(\)) and OOD otherwise. We develop a method applicable directly in the feature space \(\) of a trained model \(f_{}\). It yields a score function \(S_{}\): \(S(x):=S_{F}(_{_{1}}(x))\). The high-level idea is to measure directly "how central" a point is with respect to (w.r.t.) clusters taking into account density and geometry of each cluster in the feature space. This provides an uncertainty score. For this objective, standard methods consist in assuming some _prior_ distribution such as GDA (Gaussian Discriminant Analysis) based on Gaussian fitting . However, the assumption that the data in a cluster is Gaussian distributed or follow any particular distribution is quite restrictive. We will show in our experiments section that the Gaussian assumption fails even in a very simple case (Section5.1). Let us take the example of a simple frame in the plane with 2 clusters corresponding to 2 classes in form of two-moons (Fig. 1.1a). In this example, GDA fails totally to capture the distribution of clusters. This motivates us to develop a non-parametric method that can measure explicitly how "central" a point is w.r.t. a cluster without the need of additional training and prior assumption. Furthermore, the method should accurately capture distribution with complex support and shape, in order to be adapted to a variety of cases. To measure how central a point is w.r.t. a distribution, we use the so-called notion of statistical _Lens Depth_ (_LD_) , that will be presented in Section 3.1. Furthermore, for _LD_ to correctly capture the shape of the distribution, an appropriate distance must be adopted that adaptively takes into account its geometry and density. Fermat distance is a good candidate for this purpose. However, it is not directly tractable as it stands on integrals along rectifiable paths. A recent paper  proposes the use of an explicit sampled Fermat distance and shows its consistency property (see also ). In our work, we make use of their results to compute the _LD_. The general scheme is illustrated in Figure 1.1b. In our experiments, the classification model is provided by a neural network, \(h\) is a softmax layer consisting of a linear transformation and a _softmax_ function, \(\) is the output space of the penultimate layer right before the softmax layer.

**Consistency of the uncertainty score.** A consistent uncertainty score function should allow us to detect OOD. Furthermore, when more samples are rejected based on this score, the accuracy of the multi-class classification on the retained samples should increase. In other words, the fewer examples retained (based on the score), the better the accuracy. Our method measures a natural "depth" of the considered example. Consequently, the larger the depth of this example, the more typical this point is (relative to the training set), and so the easier it is for the model to classify.

In summary, our contribution is at the following three levels:

* We are bringing to machine learning the mathematical tool of \(LD\), combined with Fermat distance. It proves particularly efficient for OOD uncertainty quantification. We also propose improvements that avoid undesirable artifacts, and simple strategies for reducing significantly the complexity in computing \(LD\).
* The method we propose is non-parametric and non-intrusive. We do not have priors on the distribution of data nor features. We do not require modifying the training algorithms.
* The method is almost hyperparameter-free, as we show that it is rather insensitive the only parameter used to define Fermat distance.

Tables 5.1 and 5.2 give benchmarks. Our code can be found at LD-experiment-code.

## 2 Related Work

**Intrusive approaches.** One approach to construct a confidence score consists in fine-tuning the model \(f_{}\) using some auxiliary OOD data so that the ID and OOD data are more separable in the feature space . One may even use very particular type of models and training mechanisms for the original classification task such as _Prior Networks_ in which the prior distribution is assumed on the output of the model \(f_{}\). More laborious methods to handle uncertainty in neural network is

Figure 1.1: Motivation example and general scheme of our method.

Bayesian modeling [17; 4]. Another approach is to train additional models such as Deep Ensembles  or LL ratio . In these approaches, one needs to carefully perform a supplementary training. Otherwise one could reflect wrongly the true underlying distribution. Moreover, the performance of the multi-class classification task could be impacted. For all these reasons, these methods can be considered _intrusive_.

**Non-intrusive approaches.** Independently from above methods, a _non-intrusive_ approach is to work directly in the feature space \(\) of the trained model \(f_{}\). This is _non-intrusive_ in the sense that there is no need of changing the model nor supplementary training. Besides, model performance is not impacted. One of the simplest method is to use the \(k\)-nearest neighbor distance . It is simple but has the drawback of ignoring the global cluster geometry and density as it considers only the nearest neighbors. A more sophisticated approach is GDA  that uses minimum Mahalanobis distance 1 based on _Gaussian prior_. Despite taking the distribution into account, Gaussian modeling is restrictive as it leads to an ellipsoid for shaping each cluster.

**Single forward-pass uncertainty.** A popular work which yields uncertainty score in a single forward pass is DUQ . In this method, one needs to train particular models, namely RBF models , with some carefully fine-tuned penalty to encourage sensitivity. This makes the training process more difficult, hence impacting negatively the classification performance.  proposes SNGP using a distance-aware output layer, based on Gaussian Processes, with Spectral Normalization (SN) in training. Again, these additionnal constrains can decrease the overall performance of model (compared to standard softmax model). More recently,  proposes DDU, which is based on a GDA approach, but one adds SN to encourage smoothness.  proposes Nonparametric Uncertainty Quantification (NUQ) using a kernel-based method. Although this method is non-parametric, it is highly dependent on the choice of kernel and the kernel bandwidth.

## 3 Background

### Lens Depth

Lens depth (LD)  is a specific notion of a more general quantity called Depth . A depth is a score measure of the membership of a point w.r.t. a distribution in a general space. The greater the depth, the more central the point is to the distribution. LD of a point w.r.t a distribution \(P_{X}\) is defined as the probability that it belongs to the intersection of two random balls. These balls are centered at two independent random points \(X\) and \(Y\), both having the distribution \(P_{X}\) and a radius equal to the distance between \(X\) and \(Y\). More formally, if we work on \(^{d}\), the LD of a point \(x^{d}\) w.r.t. \(P_{X}\) is defined as follows,

\[LD(x,P_{X}):=(x B_{1} B_{2})\;.\] (3.1)

Here, \(D\) is a given distance on \(^{d}\); \(X_{1}\),\(X_{2}\) are i.i.d with law \(P_{X}\); \(B(p,r)\) is the closed ball centered at \(p\) with radius \(r\); \(B_{i}=B(X_{i},D(X_{2},X_{1})),\;i=1,2\). Let \(A(X_{1},X_{2})=B_{1} B_{2}\). Equation (3.1) naturally gives rise to the following empirical version of LD,

\[_{n}(x):=^{-1}_{1 i_{1}<i_{2} n} _{A(X_{i_{1}},X_{i_{2}})}(x)\;.\] (3.2)

Note that for the empirical version, the intersection set can be rewritten as

\[A(X_{1},X_{2})=\{x:_{i=1,2}D(x,X_{i}) D(X_{1},X_{2})\}\;.\] (3.3)

Obviously, a crucial question is the choice of the distance \(D\). A naive choice is the Euclidean one. Examples of \(\) using Euclidean distance are depicted in Fig. 3.1. We see that in the Gaussian case, the level curves of \(\) rather well capture the distribution. However, for the moon distribution they fail miserably. This is not surprising as the Euclidean distance does not take into account the data distribution \(P_{X}\). This gives rise to a natural problem as stated by : _How to learn a distance that can capture both the geometry of the manifold and the underlying density?_ The Fermat distance allows us to solve this problem and it is presented in the following section.

### Fermat distance

Following , let \(S\) be a subset of \(^{d}\). For a continuous and positive function \(f:S_{+}\), \( 0\) and \(x,y S\), the Fermat distance \(_{f,}(x,y)\) is defined as

\[_{f,}(x,y):=_{}_{f,}()\,\ \ \ _{f,}():=_{}f^{-}\.\] (3.4)

The infimum is taken over all continuous and rectifiable paths \(\) contained in \(\), the closure of \(S\), that start at \(x\) and end at \(y\).

**Sample Fermat Distance.** Let \(Q\) be a non-empty, locally finite, subset of \(^{d}\), serving as dataset. \(|x|\) denotes Euclidean norm of \(x\), \(q_{Q}(x) Q\) is the particle closest to \(x\) in Euclidean distance - assuming uniqueness2. For \( 1\), and \(x,y^{d}\), the sample Fermat distance is defined as

\[D_{Q,}(x,y):=_{j=1}^{k-1}|q_{j+1}-q_{j}|^{}\,:(q_ {1},,q_{k}) Q^{k}\ \ q_{1}=q_{Q}(x),\ q_{k}=q_{Q}(y),\ k 1 }\.\] (3.5)

The paper  shows that the sample Fermat distance when appropriately scaled converges to the Fermat distance. For sake of brevity, we refer to Appendix A for exact statement. More theoretical insight of Fermat distance and applications to clustering can be found in .

**Intuition behind Sample Fermat Distance.** The sample Fermat distance searches for the shortest path relating the points. The length of each path is the sum of the Euclidean distances of consecutive points in the path powered by \(\). With \(=1\), the shorted path between \(x\) and \(y\) is simply the line relating \(q_{Q}(x)\) and \(q_{Q}(y)\) (Fig. 3.2a). However, with a sufficiently large \(\), this definition of path length discards consecutive points with a large Euclidean distance instead favoring points that are closely positioned in terms of Euclidean distance. So, this will qualify the path passing through high density areas. Moreover, as this distance depends also on the number of terms in the sum in Eq. (3.5), this enforces a path to be smooth enough. These two remarks show that Fermat distance naturally captures the density and geometry of the dataset.

In Fig. 3.2, we go back to the moon example where \(Q\) is a moon-shaped cluster of points. We randomly choose 2 points and compute the Fermat path. We see that with \(=1\), we recover the Euclidean distance and so the Fermat path is simply a line. For \(\) larger than 1 but not large enough (for instance, \(=1.2\), Fig. 3.2b), the Fermat path still does not capture the orientation of the dataset. However, as \(\) gets larger, the Fermat path rapidly tracks the orientation of the dataset. For instance, with \(=3\), the path follows very well the distribution shape.

Figure 3.1: \(\) using Euclidean distance. Using Euclidean distance cannot capture correctly the distribution.

Figure 3.2: Sample Fermat path between two randomly chosen points with different values of \(\).

## 4 Combining LD and Fermat Distance

### Artifacts from classical Fermat distance

In the computation of the depth, instead of using Euclidean distance, we use sample Fermat distance. The results for the moon and spiral datasets are depicted in Fig. 4.1a and 4.1b. We see that the shape of datasets is much better captured. However, we also observe some zones having constant LD value (represented by the same color). The existence of such zones are explained by the following proposition:

**Proposition 1**.: _For \(x^{d},\ (x)=(q_{Q}(x))\). In other words, the empirical lens depth is constant over the Voronoi cells3 associated to \(Q\)._

The proof of Proposition 1 is in **Appendix** E. The consequence of the last proposition is that, even for a point far removed from \(Q\), the value of \(\) remains the same as that of its nearest point in \(Q\). Consequently, \(\) does not vanish at infinity. This is totally undesirable, as an ideal property of any depth is to vanish at infinity. To avoid this undesirable artifact, we need to modify the sample Fermat distance so that it takes into account the distance to \(Q\).

### Modified Sample Fermat Distance

The modified distance is defined, for \(y Q,\ x^{d}\) as follows:

\[D^{}_{Q,}(x,y):=_{q Q}\{|x-q|^{}+D_{Q, }(q,y)\}\;.\] (4.1)

Here, \(D_{Q,}(q,y)\) has been defined in Eq. (3.5).

**Interpretation.** In the original definition in Eq. (3.5), the path always starts by the closest point in the dataset. Consequently, the distance to this closest point is totally ignored. To eliminate this drawback, the distance to a potential starting point lying in \(Q\) is added. Note that the optimization problem for calculating \(D^{}_{Q,}\) is of the same type as that for calculating \(D_{Q,}\) with only a change of starting point. Hence, the consistency of this empirical distance towards the theoretical Fermat distance remains true. Indeed, in the new formulation (4.1), the point \(q Q\) is not fixed at \(q_{Q}(x)\) but remains free and is a part of the optimization problem. Notice further that our modified version enjoys two nice properties. Firstly, if \(x Q\) then Eq. (4.1) coincides with Eq. (3.5) (\(D^{}_{Q,}(x,y)=D_{Q,}(x,y)\)). Secondly, \(D^{}_{Q,}(x,y)\) increases to infinity when \(x\) is going far away from \(Q\). Consequently, in this case, the corresponding \(\) w.r.t \(Q\) tends to \(0\). The \(\) using this modified version of the distance is displayed on two examples in Fig. 4.1c and 4.1d. With our modification, the undesirable artifact of constant-valued zone is erased. Furthermore, for points far away from the dataset, \(\) tends quickly to 0. In conclusion, our method captures the shape of distributions perfectly.

### Qualitative evaluation of stability

We experiment and evaluate the stability of our method on the spiral dataset. This is a tricky dataset, and a standard method like the Gaussian one cannot capture its shape.

Figure 4.1: \(\) with Sample Fermat Distance on moon and spiral datasets. (a) and (b) are results of using directly sample Fermat distance in Eq. (3.5). This produces undesirable artifacts where we observe zones of constant value of \(LD\). This phenomenon is explained by Proposition 1. (c) and (d) use our modified version in Eq. (4.1): it captures perfectly the distributions.

#### 4.3.1 Stability with respect to number of training points

When running a statistical algorithm, it is desirable to have as large a sample as possible. However, in many cases, only a very small amount of data is available. This motivates the study of the stability of our method in a small data regime. Here, we simulate the spiral dataset with 1000 points. Then, we choose randomly only \(20\%\) of the simulated points (i.e. 200 points) as the sample dataset to compute \(LD\). We perform different runs for different random samples with \(=5\) for a visual evaluation. For the sake of brevity, only the results of four tests are shown in Fig. 4.2b. More replications are displayed in **Appendix**C. We see that in the 4 tries, our method gives slightly different estimation of \(LD\). This small fluctuation is to be expected, as we take only \(20\%\) of the points at random each time. Nevertheless, the method captures the shape of the dataset really well (the full sample of 1000 points is displayed in the figures). Besides, we also perform an experiment where we reduce the number of points until the method fails to capture the shape of the distribution. This helps us to have a better idea how our method works at small-data regime. We refer to **Appendix**D for results.

#### 4.3.2 Stability with respect to the hyperparameter \(\)

In our method, only one hyperparameter (\(\)), governing the Fermat distance needs to be chosen. It is therefore important to assess the stability of the method w.r.t. \(\). For this purpose, we experiment with different values of \(>1\) (recall that \(=1\) corresponds to the Euclidean distance). For each \(\{3,5,10,15\}\), we test our method on the spiral dataset. Results are shown in Fig. 4.2a. The conclusion is that our method is very stable through different values of \(\). Indeed, in the 4 cases, it always captures almost perfectly the dataset support, which implies a strong stability of the method. Of course this stability is only achieved in the proper range when \(\) is large enough (See Fig. 3.2).

### From LD to OOD uncertainty score

Our ultimate objective is to use \(LD\) to provide an OOD uncertainty score. To do so, we apply \(LD\) to the feature space \(\) of our classification model. Let \(C\) be the number of separate clusters. Now, there are two ways for computing \(LD\) of a new point: (1) All the clusters are considered as a sole distribution to compute \(LD\); (2) Compute \(LD\) w.r.t. the different clusters and then take the \(max\) among the \(LD\)'s (i.e. \(LD\) w.r.t. the nearest cluster). It turns out that the first approach gives unsatisfying result as explained in **Appendix**F. So, we adopt the second approach in this paper. More formally, let us denote \(((x),_{i})\) the empirical LD of \(x\) w.r.t. the \(i^{th}\) cluster formed by training examples of class \(i\) (in the feature space) (i.e. the clustering is given by the labeling as we have labels here). Then, the confidence score of \(x\) is defined as

\[S(x):=_{i}((x),_{i})\.\] (4.2)

### Computational complexity: the main limitation of \(Ld\) and how to be more efficient

From Eq. (3.2), we can deduce that the complexity of calculating \(LD\) for a given point is \(O(CN^{2})\) (\(C\) is the number of classes, \(N\) is number of examples in each class). It is therefore useful to reduce the number of inner points \(N\) used to calculate \(LD\) while maintaining good precision. Keeping only \(n\) inner points among the \(N\) initial ones, we then have 3 different straightforward strategies:

* **I. Random.** Randomly sample without replacement \(n\) points among \(N\) initial points.

Figure 4.2: Stability with respect to number of training points and \(\).

* **II. K-mean/center.** We want the \(n\) points to cover well the support of the initial sample. Hence, we first apply a _K-mean clustering_ with \(n\) centroids on the \(N\) points. Then, the \(n\) resulted centroids are used as inner points.
* **III. K-mean/center+.** Same as strategy **II**, but instead of using directly the centroids, we use the inner point closest to each centroid.

We test and discuss about these strategies in Appendix G. It turns out that _K-mean/ Center_ outperforms the two other strategies with a very small number of inner points \(n\). We refer to Appendix G for more discussion and detailed experiment. So, for the rest of this paper, we use strategy **III.**

## 5 Experiments on Neural Networks

We first evaluate our method on the two-moon dataset. Then, we evaluate on 2 benchmarks FashionMNIST-MNIST and CIFAR10-TinyImageNet/CIFAR100/SVHN the ability of our method for the detection of OOD. Besides, we also evaluate the consistency property of our uncertainty score as presented in introduction section (shown in Fig. 5.2). Without further mention, we fix \(=7\) for all experiments. For a fair comparison, we use the same model architectures as in the previous work of . More details about the models and the training schemes can be found in the **Appendix B**.

### How is the input distribution represented in the feature space of _softmax_ model?

We first perform experiment on the two-moon dataset consisting of 2 classes, each having a moon shape. We train a neural network with 2 hidden layers for classification (more details can be found in the **Appendix B**). After training, the model parameters are fixed and different methods for uncertainty evaluation are applied in the feature space \(\) of this model. One popular way to provide an uncertainty score is to use the predictive distribution entropy4. It is maximized when the distribution is uniform. In this example, predictive distribution entropy is high only in a boundary zone (Fig. 5.1f). This is to be expected, as the model is trained to learn a boundary between the two classes. Nevertheless, it is desired to assign a high uncertainty to the region without training data. Indeed, it might be too risky to make decision in these zones, especially in critical applications.

**Is Gaussian prior suitable?** We consider the methods of Euclidean distance (Fig. 5.1d) and GDA  (Fig. 5.1e). For the Euclidean distance method, we compute the distance to the centroids of the different clusters (in \(\)) and then we take its minimum. Surprisingly, in this example, the crude use of Euclidean distance seems to better capture the input distribution than GDA (failing miserably on this dataset). This suggests that the distribution of clusters in feature space is more complicated than the Gaussian one. This remark shows the necessity to have a method able to capture better the distribution. \(LD\) can capture impressively well the zone where we have training data (Fig. 5.1a, 5.1b and 5.1c corresponding to \(=3,\,10\) and \(15\)). Hence, \(LD\) is able to pin down clusters with a complex support shape in the feature space. Furthermore, we intentionally use 3 values for \(\) with large gaps to show the stability w.r.t. \(\).

### FashionMNIST vs MNIST

We perform five different runs to train classification models on the dataset FashionMNIST . Firstly, we evaluate our method by studying the separation capacity between the test set of FashionMNIST

Figure 5.1: Methods for uncertainty estimation applied on the same neural net trained to classify 2 classes in moon-shape (represented by yellow and black points respectively). Uncertainty estimations are computed based solely on the feature space of the model without seeing directly the inputs. Red represents high uncertainty. Our method (Fig. 5.1a, 5.1b and 5.1c) gives excellent results and much better than other methods.

and of MNIST  based on _AUROC_ score. Results are reported in Table 1. We first compare our method to Euclidean and GDA method . Notice that our method outperforms these two distance-based methods. A more sophisticated method called DUQ  stands on a devoted neural architecture (RBF network). This particular type of model is much more difficult to train and so generally does not preserve the accuracy of the main classification task (compared to standard _softmax_ models). Once again, our method outperforms this method. This indicates that our method measures a natural "depth" directly in the feature space without the need of changing completely the model as in DUQ method. Another popular method is Deep Ensembles in which one trains and applies many independently-trained models for the same task. Despite its heavy demanding of resource, our method outperforms this approach in this experiment. A more advanced method for density estimation is LL ratio . In this method, one needs to train two supplementary generative models to estimate distributions. This method needs an adequate noise and really careful training of these 2 generative models so that they can reflect the true underlying input density. With this complex process 5, this method gives better AUROC score than ours in this experiment.

**Consistency curve.** Following some previous works (e.g. ), we compound test set of FashionMNIST and MNIST together and all the data from MNIST are considered to be incorrectly predicted by the model. Then, a certain percentage of data is rejected based on their \(LD\). If \(LD\) is an appropriate indicator for prediction uncertainty, then accuracy on the retained data has to be an increasing function of the rejection percentage. We call the resulted curve _consistency curve_. The results for five runs are depicted in Fig. 5.2a. We see that the curves are always increasing over 5 runs. Hence, \(LD\) is a good measure for uncertainty estimation.

### CIFAR10 vs SVHN/Tiny-ImageNet/CIFAR100

In this section, we compare LD to popular deterministic single-forward pass methods for uncertainty quantification (UQ). We also compare with Deep-Ensemble, as it stays a _de facto_ method for UQ. We train the models on the training set CIFAR10  and then the test set of CIFAR10 is considered as in-distribution (ID) data. We use test sets of 3 datasets SVHN , CIFAR100 and Tiny-ImageNet  as OOD data. AUROC scores are reported in Table 2. We first compare LD with GDA using ResNet18 (the first two lines). We see that LD consistently outperforms GDA on 3 OOD sets. Next, we compare our method with DDU . This method is basically GDA but one adds spectral normalization (SN) in the model. Using ResNet18, our method outperforms on CIFAR100 and SVHN. An interesting remark is that using SN seems to improve AUROC on CIFAR100 and SVHN. This is expected as SN encourages more smoothness. However, for Tiny-ImageNet, AUROC gets worse using SN. That is, somehow SN makes the features of TinyImageNet closer to those of CIFAR10. Using WideResNet, DDU seems to be a little better than ours on CIFAR100 and SVHN but on TinyImageNet, our method is better. Next, we compare LD with DUQ. Our method consistently

  Characteristics & Method & AUROC \\   & LD (our method) & \(0.971_{ 0.001}\) \\   & Euclidean Distance & \(0.943_{ 0.009}\) \\   & DUQ (\(\)) & \(0.955\) \\   & Deep Ensembles (5 Models) (\(\)) & \(0.861\) \\  Need to train extra generative models & LL ratio (\(\)) & \(0.994\) \\  

Table 1: Results on FashionMNIST, with MNIST as OOD set. Results marked by (\(\)) are extracted from  and (\(\)) are extracted from . Deep Ensembles by , Mahalanobis Distance by , LL ratio by , DUQ by .

Figure 5: Consistency curves for FashionMNIST/MNIST, CIFAR10/SVHN, CIFAR10/CIFAR100 and CIFAR10/Tiny-ImageNet over 5 runs (each curve corresponds to an independently trained model).

outperforms DUQ on all the benchmarks. Besides, our method also outperforms energy-based method . Considering the method SNGP , it is based on a Gaussian process layer with SN, making itself a very intrusive method. In spite of this, LD outperforms this method on the two pairs CIFAR10/Tiny-ImageNet and CIFAR10/CIFAR100. On CIFAR10/SVHN, this method performs a little better than ours. Finally, we observe that Deep Ensemble (with 5 models) seems to perform very well on the 3 pairs. This is to be expected, as it uses 5 models instead of one single model. However, surprisingly, on CIFAR10/Tiny-ImageNet, LD outperforms this latter method.

Besides, to see if our method scales well when number of classes of ID data increases, we also experiment with CIFAR100 as ID data. The results are shown in Fig. 5.3. Overall, through experiments with CIFAR10 and CIFAR100 as ID data, we see that our method performs very well and outperforms many strong baseline UQ methods. This proves that LD is a useful tool to capture the underlying distribution to provide an OOD uncertainty score.

**Consistency curve.** We plot the consistency curves over the 5 runs as in Section 5.2 for 3 pairs (Fig. 5.2b, 5.2c and 5.2d). Once again, accuracy is always an increasing function of the rejected percentage of the data based on \(LD\). This confirms again that \(LD\) is an appropriate indicator for uncertainty estimation and so it is useful for decision making.

### On the limitations of the Gaussian assumption: LD vs GDA

In Table 5.2, we see that in some cases, GDA performs a little better than LD. Does this mean the distribution in the feature space is Gaussian? In fact, if the OOD set is sufficiently far from the ID one, then OOD data lies outside the smallest ellipsoid containing the ID data. In this case, Gaussian fitting can perfectly separate ID and OOD, even if the distribution is not Gaussian. That is, a good AUROC score by GDA does not necessarily imply that the distribution is Gaussian. However, if OOD and ID sets get closer, sharper detection boundaries between ID and OOD data become necessary.

To assess this, we perform an experiment with OOD more similar to ID data, thanks to hold-one-out experiments. For each of the two datasets MNIST and CIFAR10, we train model on the 9 classes and hold out one class as OOD data. In this way, ID set is more similar to OOD one as they come from the same dataset. Results are shown in Table 5.4. In this setup, the gap in terms of AUROC score

   Method & AUROC \\  LD (ours) & \(0.8310_{ 0.0013}\) \\ Softmax Entropy & \(0.8153_{ 0.0005}\) \\ Energy-based & \(0.8133_{ 0.0006}\) \\ SNGP & \(0.7885_{ 0.0004}\) \\ DDU & \(0.8313_{ 0.0006}\) \\ 
5-Ensemble & \(0.8295_{ 0.0009}\) \\   

Table 5.3: AUROC score with CIFAR100 as ID data and Tiny-Imaget as OOD data. Results of other methods are extracted from  where all the methods were experimented on the same _Wide-ResNet-28-10_ model.

   Source & Method & Model & Penalty & AUROC Tiny-ImageNet & AUROC CIFAR100 & AUROC SVHN \\  ours & LD (ours) & ResNet18 & No & \(}\) & \(}\) & \(}\) \\ ours & GDA () & ResNet18 & No & \(0.945_{ 0.005}\) & \(0.864_{ 0.003}\) & \(0.914_{ 0.014}\) \\  ours & LD (ours) & ResNet18 & SN & \(0.927_{ 0.003}\) & \(}\) & \(}\) \\ ours & DDU (GDA + \(\)N)  & ResNet18 & SN & \(}\) & \(0.872_{ 0.005}\) & \(0.947_{ 0.015}\) \\
 & DUQ () & ResNet18 & JP & \(-\) & \(-\) & \(0.927_{ 0.013}\) \\  ours & LD (ours) & Wide-ResNet-28-10 & SN & \(}\) & \(0.906_{ 0.001}\) & \(0.939_{ 0.007}\) \\
 & DDU (GDA + \(\)N)  & Wide-ResNet-28-10 & SN & \(0.9107_{ 0.0005}\) & \(}\) & \(}\) \\
 & DUQ () & Wide-ResNet-28-10 & JP & \(0.868_{ 0.001}\) & \(0.859_{ 0.003}\) & \(0.937_{ 0.006}\) \\
 & SNGP () & Wide-ResNet-28-10 & SN & \(0.899_{ 0.002}\) & \(0.911_{ 0.002}\) & \(0.940_{ 0.001}\) \\
 & Energy-based () & Wide-ResNet-28-10 & No & \(0.881_{ 0.0006}\) & \(0.889_{ 0.0007}\) & \(0.945_{ 0.005}\) \\ 
 & 5-Ensemble () & Wide-ResNet-28-10 & No & \(0.9006_{ 0.0003}\) & \(0.921_{ 0.0002}\) & \(0.977_{ 0.003}\) \\   

Table 5.2: Results on CIFAR10 with Tiny-ImageNet, CIFAR100 and SVHN as OOD sets. SN: Spectral Normalisation, JP: Jacobian Penalty.

between LD and GDA is much larger than in Table 5.2. As such, LD seems to be adaptively finding a sharper boundary than the Gaussian method. This is to be expected, as the boundaries obtained from Gaussian fitting are necessarily elliptical.

## 6 Discussion

As one property of Fermat distance is being able to adapt itself to the manifold, one could think of alternative methods following manifold learning literature. However typical methods in manifold learning do not have the property of yielding shorter distances in high density areas. Moreover, LD allows the choice of any distance, unlike other typical depths (Half-space depth or Simplicial depth...). As such, the choice of combining LD with Fermat distance is synergistic and not independent at all.

The main advantage of our method is that we make no prior distribution assumption. However, there are still extreme cases where our method would not work well. Indeed, let us consider the case where there is a class with 2 clusters in the feature space. From theoretical viewpoint, for the Fermat distance to be well defined, it is crucial for the density \(f\) to remain bounded from above and away from zero - see Appendix A. Hence, in between clusters, we need "very small density" but not "zero density". However, from a practical viewpoint, in such cases, one could argue that 2 clusters of the same class should not be too distinct. Indeed, if the main model in trained to classify properly, semantically similar inputs should be close to each other, leading to connected clusters for each class. But in general, the cluster of each class should be sufficiently connected to yield an ideal result. This also explains why we propose to work in the feature space instead of using directly raw data points. Indeed, feature spaces help us to extract the low dimensionality of the data more efficiently at a semantic level, and to have proper clusters in the feature space.

Besides, note that the aim of LD is to measure the Out-of-domain uncertainty, which is due to zones in feature space that are scarce in data. As the model is not trained in these zones, one should be cautious with the model's predictions on these zones as it can behave very randomly due to scarcity of training data. Consider for example the two-moons experiment where the two moons have more spread (and even overlap). In such cases, we have enough data in the zone between the 2 classes. Hence, LD should not be able to detect the uncertainty in this case. Other metrics such as predictive entropy should be a good candidate in this case.

Finally, an interesting use-case is to apply our method on pre-trained models. This is because SOTA models become often too large to retrain ourselves. If we have no idea about the data distribution, we are convinced that our method should be a useful tool, at least as a starting point, for better understanding the data scarcity in feature space.

## 7 Conclusion

In this work, we use Lens Depth combined with a modified version of the sample Fermat distance. This combination captures naturally the shape and density of the input distribution. This is not the case with many previously proposed methods, which assume a prior distribution or use additional models with trainable parameters, or even modify the mechanism of the training process. Our method is non-parametric and non-intrusive. Through a toy dataset as well as experiments conducted on Deep Neural Networks, we show that our method adapts very well to many cases. Hence, our work opens new venues for non-parametric methods capturing the input distribution to quantify uncertainty in the context of Deep Learning. For future work, it would be interesting both to have an efficient algorithm for computing Lens Depth with some error bound and to mathematically investigate the impact of the hyper-parameter \(\). Finally notice that, while we were focused on neural networks, any classification model with a feature space, e.g. kernel methods, can benefit from our framework.

   Data pair & Hold-one-out MNIST & Hold-one-out CIFAR10 \\  LD (ours) & \(_{ 0.007}\) & \(_{ 0.015}\) \\ GDA & \(0.898_{ 0.019}\) & \(0.759_{ 0.032}\) \\   

Table 5.4: AUROC score for OOD data that are close to ID data 

## Broader impact statement

Our Out-of-Distribution (OOD) uncertainty quantification holds implications for enhancing safety in critical applications. By effectively addressing uncertainty beyond the training data, our approach contributes to robust decision-making, particularly in scenarios where reliability is crucial. Moreover, our method can play a role in ensuring fairness by recognizing and mitigating potential biases that may arise when the training data lacks sufficient representation.