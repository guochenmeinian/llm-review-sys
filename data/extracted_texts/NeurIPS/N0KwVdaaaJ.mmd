# Theoretical Analysis of Inductive Biases in

Deep Convolutional Networks

 Zihao Wang

Peking University

zihaowang@stu.pku.edu.cn &Lei Wu

Peking University

leiwu@math.pku.edu.cn

###### Abstract

In this paper, we provide a theoretical analysis of the inductive biases in convolutional neural networks (CNNs). We start by examining the universality of CNNs, i.e., the ability to approximate any continuous functions. We prove that a depth of \(( d)\) suffices for deep CNNs to achieve this universality, where \(d\) in the input dimension. Additionally, we establish that learning sparse functions with CNNs requires only \(}(^{2}d)\) samples, indicating that deep CNNs can efficiently capture _long-range_ sparse correlations. These results are made possible through a novel combination of the multichannel and downsampling when increasing the network depth. We also delve into the distinct roles of weight sharing and locality in CNNs. To this end, we compare the performance of CNNs, locally-connected networks (LCNs), and fully-connected networks (FCNs) on a simple regression task, where LCNs can be viewed as CNNs without weight sharing. On the one hand, we prove that LCNs require \((d)\) samples while CNNs need only \(}(^{2}d)\) samples, highlighting the critical role of weight sharing. On the other hand, we prove that FCNs require \((d^{2})\) samples, whereas LCNs need only \(}(d)\) samples, underscoring the importance of locality. These provable separations quantify the difference between the two biases, and the major observation behind our proof is that weight sharing and locality break different symmetries in the learning process.

## 1 Introduction

Convolutional neural networks (CNNs) (Fukushima, 1988; LeCun et al., 1998) are a fundamental model in deep learning, known for their exceptional performance in many tasks. In particular, CNNs consistently outperform the fully-connected neural network (FCN) counterparts in vision-related tasks (Krizhevsky et al., 2012; He et al., 2016; Huang et al., 2017). Uncovering the underlying mechanism behind the success of CNNs is thus of paramount importance in deep learning.

Zhou (2020, 2020); Feng et al. (2022); He et al. (2022) studied the approximation capabilities of CNNs for target functions in spaces such as continuous functions and Sobolev spaces. Although these results are important, they cannot explain why CNNs perform better than FCNs. The primary reason for this limitation is that these works did not take into account the specific structures of CNNs, including _multichanneling, downsampling, weight sharing, and locality_. The locality refers to the use of small filters, e.g., filter sizes can be as small as \(3\) in popular VGG nets (Simonyan and Zisserman, 2014) and ResNets (He et al., 2016). Comprehending the inductive biases of these architecture choices is critical to understand the exceptional performance of CNNs.

Li et al. (2020) designed a simple classification task to demonstrate the superiority of CNNs over FCNs. The authors prove that for this task, FCNs need \((d^{2})\) samples while CNNs need only \((1)\) samples, thereby providing theoretical support for the superiority of using convolutions. However, this study neither examined the individual impact of weight sharing and locality nor considered theinductive biases of multichannel and downsampling. Additionally, the analysis was limited to shallow CNNs and did not examine the interaction between these structures and network depth.

### Our Results

In this work, we conduct a systematic analysis of the inductive biases associated with the specific structures of CNNs. Our main contributions are summarized as follows.

**Universality.** We establish the universality of deep CNNs with a depth of \(( d)\). This is in contrast to existing works (Zhou, 2020; 20; He et al., 2022), where the universality requires a depth of at least \((d)\). The key to our improvement is an effective leveraging of the inductive biases of multichannel and downsampling:

* _Downsampling_ amplifies the size of the receptive field exponentially, thus explaining the need for logarithmic depth. Furthermore, we prove that if downsampling is not used, CNNs require a depth of at least \((d)\), demonstrating the cruciality of downsampling.
* _Multichanneling_ serves as a mechanism for storing extracted information. By increasing the number of channels whenever the spatial dimension is reduced by downsampling, we ensure that no information is lost. This combination of multichannel and downsampling is widely employed in practical CNNs, ranging from classical LeNet (LeCun et al., 1998) to modern VGG nets (Simonyan and Zisserman, 2014) and ResNets (He et al., 2016).

It is worth mentioning that while studies like Poggio et al. (2017) and Cagnetta et al. (2022) have examined similar CNN architectures with \(O( d)\) depth, they did not explicitly establish universality as our work does. Specifically, Cagnetta et al. (2022) focused on the kernel associated with deep CNNs having infinitely many channels. In retrospect, one could potentially show that the deep-CNN kernel is universal by verifying that kernel has no zero eigenvalues.

**Learning sparse functions.** A function \(f:^{d}\) is said to be sparse if it only depends on a few coordinates of the input, e.g., \(f():=g(x_{1},x_{d})\) for some \(g:^{2}\). We prove that learning sparse functions using CNNs requires only \(}(^{2}d)\) samples, which is nearly optimal as the information-theoretic lower bound of learning such functions is \(( d)\)(Han and Yuan, 2020).

This result is surprising because it has been widely believed that CNNs struggle to capture long-range correlations. However, our findings suggest that CNNs can efficiently learn long-range sparse ones, which is a valuable attribute for many applications.

In addition, it is important to note that the near-optimal sample complexity of learning sparse functions using CNNs is achieved with only \(( d)\) depth and \((k^{2} d)\) total parameters, where \(k\) is the number of critical coordinates in sparse functions. A lower bound is established to demonstrate the optimality of the depth requirement. The ability of CNNs to select any \(k\) coordinate with only \((k^{2} d)\) total parameters is remarkable, especially considering that even in the linear case, LASSO (Tibshirani, 1996) requires \((d)\) parameters. It is the synergy of increased depth, weight sharing, and multichannel that gives CNNs this exceptional capability.

Disentangling the weight sharing and locality.We next study the inductive biases of locality and weight sharing by comparing the performance of CNNs, LCNs, and FCNs on a synthetic regression task. This allows us to separate the effects of weight sharing and locality.

* CNNs vs. LCNs. We prove that CNNs requires only \(}(^{2}d)\) samples to learn, while LCNs trained by SGD or Adam with standard initialization need \((d)\) samples. This provides a separation between CNNs and LCNs and demonstrates the crucial role of weight sharing.
* LCNs vs. FCNs. We prove that LCNs requires only \(}(d)\) samples to learn, while FCNs trained by SGD with Gaussian initialization need \((d^{2})\) samples. This provably separates LCNs from FCNs and demonstrates the benefit of locality.

The difference in sample complexity can be attributed to the different symmetries encoded in the architecture biases. For instance, stochastic gradient descent (SGD) exhibits different symmetries for these models: a lack of equivariance for CNNs, a local permutation equivariance for LCNs, and a global orthogonal equivariance for FCNs. The size of the equivariance groups determines the minimax sample complexity and distinguishes between different architectures. Similar ideas have been previously explored in Li et al. (2020); Xiao and Pennington (2022) to understand CNNs; but these works did not differentiate the roles of weight sharing and locality. For a detailed comparison with these works, see the related work section in Appendix A.

Technical contribution.The lower bounds for LCNs and FCNs are established using Fano's method from minimax theory (Wainwright, 2019, Section 15). However, different from the traditional statistical setup where the estimator is deterministic, the estimators produced by stochastic optimizers are random. To address this issue, we develop a variant of Fano's method for random estimators, which might be of independent interest. Further details can be found in Appendix B.3.

Related work.We refer to Appendix A for a detailed comparison with other related works.

### Notations

We use \((z_{1},,z_{p})\) to denote a quantity that depends on \(z_{1},,z_{p}\) polynomially. We use \(X Y\) to denote \(X CY\) for some absolute constant \(C\) and \(X Y\) is defined analogously. We also use the standard big-O notations: \(()\), \(()\) and \(()\). In addition, we use \(}\) and \(\) to hide higher-order terms, e.g., \((( d)( d)^{2})=}( d)\) and \((d d)=}(d)\). In addition, we use \(G_{}(d)\) to denote the orthogonal group in dimension \(d\):

\[G_{}(d)=\{Q^{d d}:QQ^{}=Q^{}Q=I_{d}\}.\]

Let \(()\) be the set of probability distributions over \(\) and \(()\) be the set of random variables taking values in \(\). Given two functions \(f,g\) over \(\) and \(\{_{i}\}_{i=1}^{n}\), let \(_{n}(f,g)=_{i=1}^{n}|f(_{i})-g(_{i})|^{2}}\). Let \(^{d-1}=\{x^{d}:\|x\|_{2}=1\},r^{d-1}=\{x:x/r ^{d-1}\}\). Let \(a b=(a,b)\), \([k]=\{1,2,,k\}\) for \(k\), and \([a,b]=\{a,a+1,,b\}\) for \(b,a\) and \(b>a\). For a vector \(\), denote by \(\|\|_{p}:=(_{i}|v_{i}|^{p})^{1/p}\) the \(^{p}\) norm. For a matrix \(A\), let \(\|A\|\) and \(\|A\|_{F}\) be the spectral norm and Frobenius norm, respectively. Moreover, denote by \(A_{i,:}\) and \(A_{:,j}\) the \(i\)-th row and \(j\)-th column, respectively, and similar notations are also defined for tensors. Given \(=(i_{1},i_{2},,i_{k})\), let \(_{}=(x_{i_{1}},,x_{i_{k}})\). We use \(\) to denote both the activation function and standard deviation of label noise. To avoid ambiguity, we shall use \(()\) and \(\) to distinguish them. When applying \(()\) to a vector/matrix/tensor, it should be understood in an element-wise manner.

## 2 Preliminaries

We consider the standard setup of supervised learning. Let \(^{4d}\) and \(\) be the input and output domain, respectively. We are given the training set \(S_{n}=\{(x_{i},y_{i})\}_{i=1}^{n}\) with \(y_{i}=h^{*}(x_{i})+_{i},x_{i}P\), and \(_{i}(0,^{2})\). Here \(P\) and \(h^{*}\) denote the input distribution and target function, respectively. We assume \(_{2}d^{+}\) for simplicity. Let \(h:\) be our parametric model with \(=^{p}\), where \(p\) denotes the number of parameters; we often write \(h_{}=h(;)\) for short. Our task is to recover \(h^{*}\) from \(S_{n}\) by using \(h_{}\).

Given a threshold \(A>0\), we also consider the truncated model \(_{A} h_{}\), where the truncation operator \(_{A}\) is defined by \(_{A} h()=((h(),A),-A)\). In addition, we consider both the square loss \((y,y^{})=(y-y^{})^{2}/2\) and its truncated version \(_{B}(y,y^{})=(y-y^{})^{2}B^{2}\). This loss truncation is applied to handle the noise unboundedness; see Appendix B.2 for more details.

### Network Architectures

A \(L\)-layer neural network is given by \(h_{}()=_{o}_{L}_{L} _{1}_{1}(),\) where \(_{l}\) and \(_{l}\) denote the activation function and linear transform with bias at the \(l\)-th layer, respectively. Let \(^{(l)}\) denote the hidden state of \(l\)-th layer: \(^{(l)}()=_{l}_{l} _{1}_{1}()\) for \(l[L]\) and \(^{(0)}()=\). When it is clear from context, we will write \(^{(l)}=^{(l)}()\) and \(_{l}=_{l}()\) for short. In different architectures, \(\{_{l}\}_{l=1}^{L}\) are parameterized in different ways. \(_{o}\) denotes the output layer, which performs a linear combination of the output features: \(_{o}^{(L)}()=W_{o}( ^{(L)}()),\) where \(W_{o}\) is the weight used to parameterize \(_{o}\).

FCNs.\(_{l}:^{C_{l}}^{C_{l-1}}\) is a fully-connected transform parameterized by \(_{l}()=W^{(l)}+^{(l)}\) with \(W^{(l)}^{C_{l} C_{l-1}}\) and \(^{(l)}^{C_{l}}\). Here \(C_{l}\) denotes the width of \(l\)-th layer.

CNNs.The \(l\)-th hidden state is a feature matrix: \(^{(l)}()^{D_{l} C_{l}}\) with \(D_{l}\) and \(C_{l}\) denoting the spatial dimension and number of channels, respectively. \(_{l}:^{D_{l-1} C_{l-1}}^{D_{l}  C_{l}}\) is parameterized by a kernel \(W^{(l)}^{C_{l} C_{l-1} s}\) and bias \(^{(l)}^{C_{l}}\) as follows

\[(_{l}())_{:,j}=_{i=1}^{C_{l-1}}_{:,i}*_{s} W^{(l)}_{j,i,:}+b^{(l)}_{j},j=1,,C_{l}\] (1)

where \(*_{s}:^{*k}^{s}^{k}\) denotes the _convolution with stride_ given by

\[*_{s}=(_{I_{1}}^{},_{I_ {2}}^{},,_{I_{k}}^{})^{k},\] (2)

where \(I_{j}:=[(j-1)s+1,js]\) denotes the \(j\)-th patch, and \(\) and \(\) denote the signal and filter, respectively. As a comparison, we also consider the convolution without stride \(*:^{k}^{s}^{k-s+1}\) given by \((*)_{i}=_{j=1}^{s}v_{i+j-1}w_{j}\). Note that the stride plays a role of downsampling and in practice, it is also common to use other downsampling schemes such as max pooling and average pooling. All results in this paper hold regardless of which one is used and thus, we will focus on the stride case without loss of generality.

LCNs.A LCN has the same architecture as its CNN counterpart but lacks weight sharing. Consequently, LCNs have more parameters. Specifically, the linear transform \(_{l}:^{D_{l-1} C_{l-1}}^{D_{l}  C_{l}}\) is parameterized by \(W^{(l)}^{C_{l} C_{l-1} D_{l-1}}\) and \(^{(l)}^{D_{l} C_{l}}\) as follows

\[(_{l}())_{:,j}=_{i=1}^{C_{l-1}}_{:,i}*_{s} W^{(l)}_{j,i,:}+^{(l)}_{:,j},j=1,,C_{l}\]

where the local linear operator \(_{s}:^{ks}^{ks}^{k}\) is defined by \(*_{s}=(_{I_{1}}^{}_{I_{1}}, _{I_{2}}^{}_{I_{2}},,_{I_{k}}^{} _{I_{k}}),\) where \(I_{j}=[(j-1)s+1,js]\) denotes the indices of \(j\)-th patch.

Throughout this paper, we always assume the filter size \(s=2\) for technical simplicity and thus \(D_{l}=4d/2^{l}=D_{l-1}/2\) for both CNNs and LCNs.

Regularizer.To regularize CNNs and LCNs, we consider following \(_{2}\)-type norm:

\[\|\|_{}:=\|W_{o}\|_{2}+_{l=1}^{L}(\|W^{(l)}\|_{F}+_{ l}\|^{(l)}\|_{F}),\] (3)

where \(_{l}=}\) for CNNs and \(_{l}=1\) for LCNs. The factor \(_{l}\) is introduced such that \(\|\|_{}\) can control the Lipschitz norm of \(h_{}\), thereby yielding effective capacity controls for CNNs and LCNs. See Appendix C for more details.

## 3 Universal Approximation

The following theorem shows that deep ReLU CNNs are _universal_ as long as they are logarithmically deep with respect to the input dimension and the proof is deferred to Appendix D.1.

**Theorem 3.1** (Universality).: _Consider CNNs with all activation functions to be \(\). Suppose \(L=_{2}(4d)\) and \(C_{l}=2^{l+1}\) for \(l[L-1]\) to be fixed and allow the number of channels of the last layer \(C_{L}\) to increase. Then, the CNNs are universal: for any \(>0\), any compact set \(^{4d}\), and any \(h^{*} C()\), there exists a CNN \(h_{}\) such that \(_{}|h_{}()-h^{*}()|\)._

Proof idea.Write \(h_{}=_{o}_{L}^{(L-1)}\). First, we show there exists parameters (independent of \(h^{*}\)) such that

\[^{(L-1)}()=(x_{1})&(-x_{1})& (x_{2})&(-x_{2})&&(x_{2d})&(-x_{2d})\\ (x_{2d+1})&(-x_{2d+1})&(x_{2d+2})&(-x_{2d+2})&& (x_{4d})&(-x_{4d})^{2(4d)}.\]

This implies that after \(L-1\) layers, the spatial dimension is reduced to \(2\) but the spatial information is stored to different channels in the form of \(\{(x_{i}),(-x_{i})\}\). Comparing \(^{(L-1)}()\) with the input \(\), there is no information loss since \(x_{i}=(x_{i})-(-x_{i})\) for any \(i[4d]\). Then, the universality can be established by simply showing that \(_{o}_{L}\) can simulate any two-layer ReLU networks.

The synergy between multichannel and downsampling.The key to achieving universality with a depth of \(( d)\) lies in a unique synergy between multichannel and downsampling. Downsampling can expand the receptive field at an exponential rate, enabling CNNs to capture long-range correlations with only \(( d)\) depth. Meanwhile, multichanneling prevents information loss whenever downsampling operations reduce the spatial dimensions. It is worth noting that this specific collaboration between multichanneling and downsampling has been adopted in most practical CNNs, such as VGG Nets (Simonyan and Zisserman, 2014) and ResNets (He et al., 2016). Our universality analysis provides theoretical support for this widespread architectural choice.

The following proposition further shows that the depth requirement in Theorem 3.1 is optimal, whose proof can be found in Appendix D.2.

**Proposition 3.2**.: _Let \(=^{4d}\) and consider the target function \(h^{*}()=x_{1}x_{2d+1}\). If \(L_{2}(4d)-1\), then for any \(C_{l}\) for \(l[L]\) and any activation functions \(\{_{l}\}_{l=1}^{L}\), we have \(_{}_{}|h_{}()-h^{*}( )|\)._

The intuition behind this is straightforward: If the depth of the CNN is less than \(_{2}(4d)\), the size of the receptive field will not exceed \(2d\). Consequently, functions that encode longer-range correlations cannot be accurately approximated.

The cruciality of downsampling.The following proposition shows that without downsampling, a minimum depth of \((d)\) is necessary for achieving universality. This highlights the importance of downsampling as it enables universality with logarithmic depth (Theorem 3.1).

**Proposition 3.3**.: _We temporarily use \(h_{}\) to denote the CNN without downsampling. Let \(=^{4d}\) and \(h^{*}()=x_{1}x_{4d}\). If \(L 4d-2\), then for any \(C_{l}\) for \(l[L]\) and any activation functions \(\{_{l}\}_{l=1}^{L}\), we have \(_{}_{}|h_{}()-h^{*}( )|\)._

The proof is presented in Appendix D.3. The reason behind is simple: vanilla convolution (without stride) can only capture local correlations of length \(2\) (since our filter size is \(s=2\)). Stacking \(L\) layers of vanilla convolutions without downsampling will only allow the network to capture correlations of length \(L+1\).

## 4 Efficient Learning of Sparse Functions

**Definition 4.1** (Sparse function).: A function \(f:^{d}\) is said to be sparse if \(f\) only depends on a few coordinates. Given \(k\) with \(k d\), \(f\) is said to be \(k\)-sparse if there exist an index set \(=\{i_{1},,i_{k}\}[d]\) and \(g:^{k}\) such that \(f()=g(_{})\), where \(_{}=(x_{i_{1}},,x_{i_{k}})^{k}\).

The class of sparse functions includes functions of both functions with short-range correlations, such as \(f()=g(x_{1},x_{2})\), and those with long-range correlations, like \(f()=g(x_{1},x_{d})\). It is widely held that it is difficult for CNNs to capture long-range correlations due to locality bias. Consequently, it might seem that CNNs are not well-suited to learning sparse functions like \( g(x_{1},x_{d})\). However, for CNNs with downsampling, increasing depth can expand the receptive field exponentially, providing the opportunity to learn long-range correlations. Indeed this has been proven in the universality analysis.

In this section, we further show that deep CNNs are not only capable of, but also efficient at learning long-range sparse functions. The term "efficient" refers that the sample complexity depends on the input dimension logarithmically. This is because deep CNNs can effectively identify any \(k\) critical coordinates using only \((k^{2} d)\) parameters as demonstrated by the following lemma.

**Lemma 4.2** (Adaptive coordinate selection for Linear CNNs).: _Let \(=(i_{1},i_{2},,i_{k})[d]\). For the linear CNN model \(h_{}\) with \(C_{l}=k\) for \(l[L]\) and \(L=_{2}(d)\). Then, there exist parameters (depending on \(\)) such that \(^{(L)}()=(x_{i_{1}},x_{i_{2}},,x_{i_{k}})\)._

Proof.: First, consider the case of \(k=1\) where \(=(i)\). Let \(i-1=_{l=0}^{L-1}a_{l}2^{l}\) be the binary representation of \(i\). Set \(W_{l,l,:}=(1-a_{l},a_{l})\) for \(l=[L]\) and all other parameters (including the bias) to zero. Then, it is easy to verify that \(^{(L)}()=(x_{i})\); see Figure 1 for a diagram illustration.

For the case of \(k>1\), set the cross-channel weights to zero; for each channel, follow the case of \(k=1\) to set weights and bias. Under this setup, different channels have no interaction and proceed in a completely independent way. As a result, each channel selects one critical coordinate, and thus, \(^{(L)}()=(x_{i_{1}},x_{i_{2}},,x_{i_{k}})\). 

**Remark 4.3**.: _Lemma 4.2 indicates that deep CNNs are able to effectively identify any \(k\) critical coordinates with \(( d)\) depth and \((k^{2} d)\) parameters, which is significantly smaller than \(d\). The key to this achievement is the adaptivity of neural networks, in combination with weight sharing, multichanneling, downsampling, and depth, as demonstrated in the proof. Specifically, downsampling and increased depth allow for capturing long-range sparse correlations with only \(( d)\) depth, multichannel facilitates the storage of information regarding different critical coordinates, and weight sharing ensures the number of parameters of each layer to be independent of \(d\)._

We now proceed to consider the learning of nonlinear sparse functions. We first need the following feature selection result for ReLU CNNs. The proof is similar to the linear case (Lemma 4.2) and deferred to Appendix E.1.

**Lemma 4.4**.: _Given \(k,m\), consider a \(\) CNN with depth \(L=_{2}(4d)\) and the channel numbers \(C_{l}=2k\) for all \(l[L-1]\) and \(C_{L}=m\). Then for any \(=(i_{1},,i_{k})[4d]\), \(_{1},,_{m}^{k}\), and \(c_{1},,c_{m}\), there exists \(\) such that the \(L\)-th layer outputs:_

\[^{(L)}()=(_{1}^{}_{ }+c_{1}),,(_{m}^{}_{}+c_{m})^{1 m}.\] (4)

_Furthermore, this CNN has \((k^{2} d+km)\) parameters._

According to this lemma, deep CNNs are capable of generating adaptive features of the form: \((^{}_{}+c)\) with only \(( d)\) depth and \((k^{2} d)\) parameters, where the features depend only on the \(k\) critical coordinates. Note that linear combinations of this type of features give two-layer networks. Therefore, functions of the form \(f():=g(_{})\) with \(g:^{k}\) can be well approximated by deep CNNs, as long as \(g\) can be wll approximated by two-layer ReLU networks. To measure the learnability of \(g\), we adopt the Barron regularity as proposed in E et al. (2022).

**Definition 4.5** (Barron space).: Consider functions admitting the following integral representation \(g_{}()=_{}a(^{}+c )(a,,c)\) for all \(\), where \(=\), \(=^{1}^{k}^{1}\) and \(()\). For a function \(g:\), denote by \(A_{g}=\{:g_{}()=g()\; \}\). Then, we define

\[\|g\|_{}=_{ A_{f}}_{(a,,c)} [|a|(\|\|_{1}+|c|)],=\{g:\|g \|_{}<\}.\]

More explicitly, the Fourier-analytic characterization (Barron, 1993; Klusowski and Barron, 2016) showed that \(\|g\|_{}(1+\|\|_{1})^{2}|()|\,\), where \(\) denotes the Fourier transform of \(g\). The analysis with Barron regularity is often much simpler (E et al., 2019, 2022) and can yield a better dependence on \(k\) than using traditional smoothness measures. Specifically, the Barron regularity allows obtaining generalization bounds with dimension-independent rates like \((n^{-1/2})\). In contrast, for traditional smoothness measures such as assuming \(g C^{s}(^{k})\), the resulting error rate would scale like \((n^{-s/k})\), which depends on \(k\) exponentially.

Recall \(_{B}(y,y^{})=(y-y^{})^{2}B^{2}\). Consider the regularized estimator given by \(_{n}=_{}(_{i=1}^{ n}_{B}(_{A} h_{}(_{i}),y_{i})+\|\|_{ }),\) where \(A,B,\) are hyperparameters to be tuned.

Figure 1: A diagram illustration of how CNNs select coordinates adaptively. In this case \(d=8,L=3\). The nonzero coordinate is \(i=4\), for which \(a_{0}=1,a_{1}=1,a_{2}=0\). The values on edges represent the weights, which are set according to the proof of Lemma 4.2.

**Theorem 4.6**.: _Let \(=^{4d}\) and \(h^{*}():=g(_{})\) for some \([4d]\) be the target function. Let \(k=||\) and suppose \(g\), \(_{z}|g(z)| 1\), and \(d 3\). For any \((0,1/2),(0,1/2)\), there is a choice of \(A,B\), and \(\) such that w.p. at least \(1-2\) over the sampling of training set we have \(\|_{A} h_{_{n}}-h^{*}\|_{L^{2}(P)}^{2},\) whenever_

\[n(\|g^{*}\|_{},k,,,)(}{^{3}}+ (d)( d)^{3}}{^{2}}).\]

The proof is presented in Appendix E.2. This theorem shows that learning sparse functions with deep CNNs requires only \(}(^{2}d)\) samples, which is nearly optimal from an information-theoretic perspective. This is because Han and Yuan (2020) proved that learning sparse functions requires at least \(( d)\) samples. It is also important to mention that Cagnetta et al. (2022) showed that using deep convolutional kernels to learn long-range sparse functions suffers the curse of dimensionality. The comparison with our results also highlights the significance of _adaptivity_ in neural networks.

Experimental validation.In this experiment, both short-range and long-range sparse target functions are considered. We set the input dimension \(d=4096\), the sample size \(n=400\) and the noise level \(\) to be zero. For the CNN architecture, the filter size is \(s=4\), resulting in a depth \(L=_{4}(d)=6\); the number of channels is set to \(C=4\) across all layers. The Adam optimizer is employed to our models, and importantly, no regularization is applied. As a comparison, we also examine two-layer fully-connected networks (FCNs) with a width \(10\), as well as the ordinary least linear regression (OLS). The results are shown in Figure 2. One can see clearly that even without any explicit sparsity regularization, CNN can still learn sparse interactions efficiently in both short-range and long-range scenarios. In contrast, FCN and OLS overfit the data and fail to generalize to test data.

## 5 Disentangle the Inductive Biases of Weight Sharing and Locality

To facilitate our statement, in this section, we denote by \(h_{}^{},h_{}^{},h_{}^{}\) the CNN, LCN, and FCN model, respectively. We shall consider the following task.

**Separation Task.** Suppose the input distribution \(P=(0,I_{4d})\) and the target function \(^{*}()=_{A_{0}} h^{*}()\) where \(A_{0}\) is a universal absolute constant to be specified later and

\[h^{*}()=(_{i=1}^{d}(x_{2i-1}^{2}-x_{2i}^{2}) )(_{i=1}^{d}(x_{2d+2i-1}^{2}-x_{2d+2i}^{2})).\] (5)

The truncation is employed to ensure boundedness of the output. However, we believe that a more refined analysis could eliminate the need for this constraint. This target function possesses the structures of both "weight sharing" and "locality" (the global sum can be computed hierarchically through local additions). We then consider the following comparisons.

* CNNs vs. LCNs. This comparison allows us to isolate the bias of weight sharing as both models have the same structures, but LCNs do not have weight sharing.

Figure 2: CNN can learn sparse functions efficiently. Both short-range (**left**) and long-range (**right**) sparse target functions are considered in this experiment. The training is stopped when the training loss drops below \(10^{-5}\).

* LCNs vs. FCNs. This comparison allows us to isolate the bias of locality since the only difference between FCNs and LCNs is the presence or absence of locality.

By these comparisons, we can evaluate the effectiveness of each bias in contributing to the improved performance of CNNs over FCNs, and gain insights into how these biases interact with other structures such as depth, multichannel, and downsampling.

To establish provable separations, we need both upper and lower bounds of the sample complexity of learning \(^{*}\). For the upper bounds, we consider the regularized estimator given by

\[_{n}=*{argmin}_{}(_{i=1}^{ n}[_{B}(_{A} h_{}(_{i}),y_{i})]+ \|\|_{}),\] (6)

where the model \(h_{}\) and hyperparameters \(A,B\), and \(\) will be specified for each comparison. We will use the covering number-based technique (see Appendix B.2) to upper bound the generalization error. See also Appendix C for how to bound the covering numbers of deep CNNs and deep LCNs.

Establishing lower bounds is often much more challenging. We will adopt a similar approach to Ng(2004); Li et al.(2020); Abbe and Boix-Adsera(2022), which involves understanding the learning hardness through the equivariance group of the learning algorithm.

### Learning Algorithm, Group Equivariance, and Lower Bounds

A learning algorithm \(:()^{n}()\) generates a random variable taking values in \(\) by using the training set \(S_{n}\). This definition includes the situation where our models are returned by stochastic optimizers such as SGD and Adam. We will denote \(X}}{{=}}Y\) as \((X)=(Y)\).

\(G\)-equivariant/-invariant.Let \(G\) be a group acting on \(\). A learning algorithm \(\) is said to be \(G\)-equivariant1 if \(\ \{_{i},y_{i}\}_{i=1}^{n}( )^{n}\) and \( G,\):

\[h_{(\{(_{i}),y_{i}\}_{i=1}^{n})} }}{{=}}h_{(\{ _{i},y_{i}\}_{i=1}^{n})}.\] (7)

A distribution \(\) is said to be \(G\)-invariant if for any \( G\), \((X)\) if \(X\).

Sample Complexity.Given a target function class \(\) and a learning algorithm \(\), for a target accuracy \(>0\), the sample complexity \((,,)\) is defined as the smallest integer \(n\) such that

\[_{h^{*}}\|h_{(S_{n})}-h^{*}\| _{L^{2}(P)}^{2},\] (8)

where the expectation is taken over both the sampling of \(S_{n}\) and the randomness in \(\). In addition, we define the _minimax sample complexity_ by \(}(,):=_{}(,,)\), where the infimum is taken over all learning algorithms, i.e., all the mappings: \(()^{n}()\).

For a function class \(\), denote by \( G:=\{f:f, G\}\) the \(G\)-enlarged class.

**Lemma 5.1**.: _(A restatement of (Li et al., 2020, Lemma D.1)) Let \(_{G}\) be the set of all \(G\)-equivariant algorithms. For any function class \(\) and \(>0\), it holds that \(_{_{G}}(,,) }( G,)\)._

This lemma shows that the sample complexity of learning \(\) with a \(G\)-equivalent algorithm can be lower bounded by the minimax complexity of learning _the enlarged class_: \( G\). The latter can be handled using standard approaches from minimax theory (see (Wainwright, 2019, Section 15)). Specifically, we will adopt Fano's method to lower-bound the minimax sample complexity, which reduces the problem to find a proper packing of the enlarged class \( G\) (see Appendix B.3). In particular, when \(=\{f^{*}\}\), the packing number of \( G\) can be completely determined by the packing number of the symmetry group \(G\) if \(f^{*}\) is chosen to satisfy

\[\|f^{*}_{1}-f^{*}_{2}\|_{L^{2}()}^{2}\|_{1}-_{ 2}\|_{G}^{2},\] (9)

where \(\|\|_{G}\) is a proper norm defined on \(G\). Moreover, estimating the packing number of a symmetry group is often much easier.

Note that the specific target function (5) in our separation task is designed to satisfies (9) for the symmetry groups associated with SGD algorithms and thus, the sample complexity can be completely governed by the size of symmetry groups. Specifically, for SGD algorithms, we have

* The symmetry group of FCNs is the orthogonal group \(G_{}(4d)\), whose degree of freedom is \((d^{2})\). This explains the lower bound \((d^{2})\) for FCNs.
* The symmetry group of LCNs is the local permutation group \(G_{}\), whose degree of freedom is \((d)\). This explains the lower bound \((d)\) for LCNs.

See the sections below for more details.

### CNNs vs. LCNs

In this section, we will derive an upper bound of sample complexity for learning \(^{*}\) with CNNs and a lower bound for learning \(^{*}\) with LCNs. The two bounds together provide a quantification of the effect of weight sharing.

Consider the deep CNN \(h_{}^{}\) with \(L=_{2}(4d)\), \(C_{1}=C_{L}=4\), \(C_{l}=2\) for \(l[2,L-1]\), and \(_{1}(x)=_{L}(x)=^{2}(x),_{l}(x)= (x)\) for \(l[2,L-1]\).

**Theorem 5.2** (Upper bound of CNNs).: _Suppose \(d 3\) and \(A_{0} 0\) and consider the estimator \(_{n}\) given by (6). For any \((0,1/2)\) and \((0,1/2)\), there is a choice of \(A,B\), and \(\) such that \(\|_{A} h_{_{n}}^{}-^{*}\|_{ L^{2}(P)}^{2}\) holds w.p. at least \(1-2\), whenever_

\[n(A_{0},,(1/),(1/)) ^{-2}(^{2}d)( d)^{3}.\]

The proof is deferred to Appendix G.2. It is shown that CNNs need only \(}(^{2}d)\) samples to learn \(^{*}\). The reason behind this upper bound is that \(^{*}\) can be accurately represented by deep CNNs with only \(( d)\) parameters and well-controlled norms due to the well matching between the weight sharing and locality in CNNs and the unique characteristics of \(^{*}\).

Next we turn to establish a lower bound for LCNs. Let \((,)\) be a general differentiable loss function and \(^{}():=_{i=1}^{n}(h_{}^{ }(_{i}),y_{i})+r(\|\|_{p}),\) where \(p 1\) and \(r:[0,+)[0,+)\). Denote by \(_{T}^{}\) the algorithm that returns solutions by optimizing \(^{}()\) for \(T\) steps using SGD or Adam. Under mild conditions, it can be shown that \(_{T}^{}\) is equivariant under the local permutation group:

\[G_{}=\{U^{4d 4d}:U=(U_{1},U_ {2},,U_{2d}),U_{i}\{1&0\\ 0&1,0&1\\ 1&0\}\}.\] (10)

Obviously, \(G_{}\) is a group under matrix product. To ensure the \(G_{}\)-equivariance of \(_{T}^{}\), we need the following assumption, which is satisfied by all popular initialization schemes used in practice.

**Assumption 1**.: _At initialization, the fist layer weight satisfies: \(W_{j,1,2k-1}^{(1)}\) and \(W_{j,1,2k}^{(1)}\) have the same marginal distribution for any \(j[C_{1}],k[2d]\)._

**Lemma 5.3**.: _Suppose that the input distribution \(P\) is \(G_{}\)-invariant and Assumption 1 is satisfied. Then, for any \(T\), \(_{T}^{}\) is \(G_{}\)-equivariant for both SGD and Adam._

The proof is deferred to Appendix F.2. Xiao and Pennington (2022) showed that under Gaussian initialization, the equivariance group of \(_{T}^{}\) is \(O(2) I_{2d}\), which is slightly larger than \(G_{}\) obtained in the current work. It should be stressed that \(G_{}\) is obtained under a much milder assumption of initialization which holds for the popular uniform initialization scheme. Moreover, we show below that the \(G_{}\) equivariance is sufficient for separating LCNs from CNNs.

**Theorem 5.4** (Lower bound of LCNs).: _Under Assumption 1, there exists absolute constants \(C,c>0\) such that \(\,T,_{0}(0,c],d,A_{0} C\):_

\[(_{T}^{},\{^{*}\},_{0})= (^{2}d)\]

The proof is deferred to Appendix G.1. By comparing Theorem 5.2 and 5.4, we see that the weight sharing in CNNs yields an \((d)\) improvement on the sample complexity for learning \(^{*}\). This highlights the importance of exploiting the "weight sharing" in \(^{*}\) for efficient learning.

### LCNs vs. FCNs

We will derive an upper bound of sample complexity for learning \(^{*}\) with LCNs and a lower bound for learning \(^{*}\) with FCNs. The two bounds together provide a quantification of the effect of locality. We consider the deep LCN \(h_{}^{}\) with \(L=_{2}(4d)\), \(C_{1}=C_{L}=4\), \(C_{l}=2\) for \(l[2,L-1]\), and \(_{1}(x)=_{L}(x)=^{2}(x)\), \(_{l}(x)=(x)\) for \(l[2,L-1]\).

**Theorem 5.5** (Upper bound of LCNs).: _Suppose \(d 3\) and \(A_{0} 0\) and consider the estimator \(_{n}\) given by (6). Then, for any \((0,1/2)\) and \((0,1/2)\), there is a choice of \(A,B\), and \(\) such that \(\|_{A} h_{_{n}}^{}-^{*}\|_{L ^{2}(P)}^{2}\) holds w.p. at least \(1-2\), whenever_

\[n(A_{0},,(1/),(1/)) ^{-2}d^{4}d\]

The proof is deferred to Appendix H.2. It is shown that LCNs needs \((d)\) samples to learn \(^{*}\). The reason behind this upper bound is that \(^{*}\) can be well approximated by LCNs with \((d)\) parameters and well-controlled norm. However, as opposed to CNNs, LCNs lack weight sharing and can only imitate the local features of \(^{*}\) with the "weight sharing" structure completely ignored.

Next we turn to establish a lower bound for FCNs. Let \((,)\) be a general differentiable loss function. Denote by \(_{T}^{}\) the algorithm: Run SGD for \(T\) steps by optimizing \(^{}():=_{i=1}^{n}(h_{i}^{}(_{i}),y_{i})+r(\|\|_{2}),\) where \(r:[0,+)[0,+)\) is a general penalty function. We refer to Appendix F.1 for a rigorous definition.

**Lemma 5.6**.: _Suppose that the input distribution \(P\) is \(G_{}(4d)\)-invariant and entries of \(W^{(1)}\) are i.i.d. initialized from \((0,^{2})\) for some \(>0\). Then, for any \(T\), \(_{T}^{}\) is \(G_{}(4d)\)-equivariant._

This lemma was proved in (Li et al., 2020, Corollary C.2) and we state it here for completeness. It implies that training FCNs with SGD induces a larger equivariance group than that of LCNs since \(G_{} G_{}(4d)\). It is important to note that this lemma only holds for SGD, as Adam is only permutation invariant. In contrast, the result in Lemma 5.3 applies to both SGD and Adam.

**Theorem 5.7** (Lower bound of FCNs).: _Suppose that the input distribution \(P\) is \(G_{}(4d)\)-invariant and entries of \(W^{(1)}\) are i.i.d. initialized from \((0,^{2})\) for some \(>0\). Then there exists absolute constants \(C,c>0\) such that \(\,T,_{0}(0,c],d,A_{0} C\):_

\[(_{T}^{},\{^{*}\},_{0})= (^{2}d^{2})\]

The proof is deferred to Appendix H.1. By comparing Theorem 5.5 and 5.7, we see that the locality itself yields another \((d)\) improvement on the sample complexity for learning \(^{*}\). This highlights the importance of exploiting the locality in \(^{*}\) for efficient learning.

## 6 Conclusion

In this paper, we delve into the theoretical analysis of the inductive biases associated with the multichannel, downsampling, weight sharing, and locality in deep CNN architectures with a focus on understanding the interplay between network depth and these biases. Our results highlight the critical role of multichannel and downsampling in enabling deep CNNs to effectively capture long-range correlations. We also analyze the effects of weight sharing and locality on breaking the learning algorithm's symmetry, through which we make a clear distinction between the two biases: the global orthogonal equivariance vs. the local permutation equivariance. By leveraging these symmetries, we establish provable separations for FCNs vs. LCNs and LCNs vs. CNNs, providing a strong theoretical support for the unique nature of weight sharing and locality.