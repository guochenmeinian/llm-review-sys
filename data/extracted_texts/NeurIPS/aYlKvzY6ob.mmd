# Mining Math Conjectures from LLMs: A Pruning Approach

Jake Chuharski\({}^{1}\)Elias Rojas Collins\({}^{1}\)Mark Meringolo\({}^{2}\)

\({}^{1}\)Massachusetts Institute of Technology \({}^{2}\)Unaffiliated

{chuharsk,erojasc}@mit.edu

markpmeringolo@gmail.com

These authors contributed equally to this work

###### Abstract

We present a novel approach to generating mathematical conjectures using Large Language Models (LLMs). Focusing on the solubilizer, a relatively recent construct in group theory, we demonstrate how LLMs such as ChatGPT, Gemini, and Claude1 can be leveraged to generate conjectures. These conjectures are pruned by allowing the LLMs to generate counterexamples. Our results indicate that LLMs are capable of producing _original_ conjectures that, while not groundbreaking, are either plausible or falsifiable via counterexamples, though they exhibit limitations in code execution.

## 1 Introduction

Artificial intelligence, specifically deep learning, has created much discussion around the possibility to augment human creativity with computational capability. Among the leading technologies pushing this discussion are large language models (LLM's) such as OpenAI's ChatGPT, Anthropic's Claude, and Google's Gemini [1; 2; 3]. While LLMs have been widely recognized for their competence in text generation, their interactions within abstract academic fields such as mathematics, specifically with conjecture creation, remain under-explored. Initial work has evaluated LLMs' ability to pass exams like the SAT and MBA qualifying exams [4; 1]. More recently, efforts have focused on bench-marking their capacity to generate mathematical proofs . However, there has been little work on bench-marking the ability of language models to act as a creative agent towards coming up with new conjectures.

In this study, we use the Claude Sonnet, Gemini 1.5, and GPT-4 APIs to both generate conjectures and write GAP computer algebra code to check them for plausibility. GAP (Groups, Algorithms, and Programming) is a computer algebra system designed for computational group theory and related areas in abstract algebra. However, GAP is not a proof assistant so does not give the user proofs for theorems, but it can be used to check conjectures for immediate counterexamples. We work specifically on the solubilizer subset which is a relatively new/unexplored construction in group theory that contains much potential for novel conjectures (see Appendix A.1). GAP computer algebra can check the conjecture for chosen groups and allows for language models to "guess and check". The system provides a method to mine for conjectures using language models and a pruning step to remove conjectures that are false for obvious (or sometimes non-obvious) reasons. This approach offers a systematic method for generating and validating conjectures, combining model output with automated computational verification without requiring a strong formal theorem prover.

Related Work

Recent studies have explored LLMs' role in conjecture generation. Johansson and Smallbone observe that many of the symbolic structures generated by LLMs may already exist in training data, raising concerns about genuine originality [(6)]. They note that GPT-4 appears to have been trained on proof libraries like QuickSpec, Hipster, and Isabelle/HOL, providing a potential caveat for verifying the originality of any generated conjectures.

We mitigate this challenge by deliberately focusing on a mathematical area with limited prior exposure: the solubilizer (see Appendix A.1). By iteratively updating prompts, we also attempt to steer the models away from generating _as many_ redundant conjectures which they also found to be a problem because "GPT-4 usually produces the same kind of 'generic' lemmas every time" [(6)].

Other studies, such as Davies et al. [(7)], use machine learning to assist mathematicians in proof creation rather than conjecture generation. In Wu et al. [(8)], LLMs are shown to autoformalize natural language math into formal theorem provers like Isabelle, translating competition problems into formal proofs with impressive accuracy. In Si et al. [(9)], LLMs are evaluated on the ability to be creative agents in coming up with research ideas, however math was touched minimally. They additionally corroborate the claim that "LLMs lack diversity in idea generation"[(9)]. These approaches focus on proof generation, formalization, or assistance, whereas our work emphasizes the initial creative step of formulating new conjectures, and then provides an immediate 'guess-and-check' step to verify plausibility.

## 3 Methodology

The method that we propose to "mine" for math theorems, shown in Table 10, is as follows:

1. We begin with a prompt that includes literature on the solubilizer from [(10; 11; 12; 13; 14; 15; 16)]. The model is prompted to generate theorems related to the literature provided and write GAP code to test conjectures on groups. Full prompting is provided in Appendix A.2.
2. The LLM then generates GAP code and the GAP code is run. * If the code compiles and runs and the outcome is recorded. * If the code does not compile the LLM is prompted again to fix the code, provided with the output of the failing program. It is given the chance to do two revisions (in practice allowing for further revision almost never results in working code).
3. If the result is that the conjecture is false, the theorem and it's result is added to the prompt, and the process is repeated with the false conjecture added to the set of ideas that are known to fail.

This process was run with three models: ChatGPT 4 (gpt-4o-2024-05-13), Claude Sonnet (claude-3-5-sonnet-20240620), and Gemini 1.5 (gemini-1.5-flash). LLM's have a "Temperature" parameter which varies the level of randomness in the outputs to a given prompt. This is sometimes taken as a proxy for "Creativity", although this description is disuputed [(17)]. The temperature for the Claude model was set to 1 for conjecture generation and 1 for code generation. The GPT-4 conjecture was set to 1.08 for conjecture generation and was left at default for code generation. The Gemini 1.5 conjecture generation was set to 1.5 (top_k: 5, top_p: : 99) and default for code generation. The values were generated by trial and error where the authors observed qualitatively the most consistent conjecture variation without extreme hallucinations2.

### Area of Focus

The mathematical area of focus is called the _solubilizer_ and is defined as follows:

**Definition 3.1**.: _Let \(G\) be a finite group. For any element \(x G\), the solubilized of \(x\) in \(G\) is defined as:_

\[_{G}(x):=\{y G x,y\}.\]

More introductory and historical information on the solubilizer can be found in the Appendix A.1.

## 4 Results

### Performance Overview

The experiment provided three types of outcomes (Summarized in Table 3):

* Successful generation of counterexample-finding code in 25.95% of cases (109 out of 420 unique outputs).
* Generation of conjectures without counterexamples in 9.52% of cases (40 out of 420 unique outputs).
* Generation of non-executable code in 64.52% of cases (271 out of 420 unique outputs).

### Examples

The following is an example with no counter-examples from Claude:

**Conjecture 4.1**.: _Let \(G\) be a non-solvable group. For any element \(x\) in \(G\), if \(_{G}(x)\) is a subgroup, then the Frattini subgroup of \(_{G}(x)\) is contained in the Frattini subgroup of \(G\)._

This result is simple enough that the model can be prompted to prove the conjecture with slight modification. See appendix A.4.1. The following is a conjecture that failed from Gemini (see A.4.2):

**Conjecture 4.2**.: _Let \(G\) be a non-solvable group, and let \(x G\). If \(|Sol_{G}(x)|\) is divisible by exactly two primes, then one of them is 2._

**Output 4.3**.: _Conjecture failed for group: \((3,2)\)_

Where \((3,2)\) is the projective special linear group of 3x3 matrices over the finite field \(_{2}\). Lastly, we have an example where code could not be executed from GPT-4:

**Conjecture 4.4**.: _Let \(G\) be a finite non-solvable group and \(x G\). Then for every abelian subgroup \(A\) of \(G\), we have \(_{G}(x) A\{1\}\)._

### Similarity Analysis

To quantitatively measure diversity, we calculated the cosine self-similarity, and similarity between conjecture sets/literature. The similarity results are summarized in Table 2. Where we see a maintained level of similarity throughout the experiments and between models. See Appendix A.7 for heatmaps.

Scaling the system further by increasing the number of trials or generating more variations in the prompt failed to yield significantly more diverse conjectures. However the quality did not decrease either. This finding suggests that a different approach, such as multi-modal model interaction or combining LLMs with automated theorem provers, could help diversity.

  
**Category** & **ChatGPT** & **Claude** & **Gemini** & **Total** \\  Unique Conjectures & 94 & 89 & 237 & 420 \\ Total Output & 249 & 258 & 250 & 757 \\  No Counter-Examples & 25 & 4 & 11 & 40 \\ Couldnâ€™t Execute Code & 33 & 44 & 194 & 271 \\ Conjecture Failed & 36 & 41 & 32 & 109 \\   

Table 1: Classification of Outputs

Figure 1: Method

## 5 Discussion

### Observations

Among the 757 outputs generated by the LLMs, 420 unique conjectures were identified. The high number of duplicates shows a considerable redundancy in the results, as approximately 55.48% of the conjectures were deemed to be unique. While not unexpected, the duplicates suggest that LLMs likely rely on similar patterns when prompted similarly across trials as described in (6). However, this did not significantly hinder overall performance other than increase the number of total iterations needed to yield a desirable number of conjectures.

Not all of the conjectures generated by the models were entirely original and was verified by one of the authors of the seven original solubilizer papers for all 420 unique conjectures. For example,

**Theorem 5.1**.: _Let G be an insoluble group and x an element of G. Then the cardinality of cannot be equal to \(p^{2}\) for any prime \(p\)._

shows up in (12) and GPT-4 conjectured:

**Conjecture 5.2**.: _Let G be an insoluble group and \(x G\). Then the cardinality of \(_{G}(x)\) cannot be equal to \(p^{2}\) for any prime \(p\)._

That being said, this result was contained in the system prompt and can be ignored. In all other cases the models output conjectures that were distinct from anything found in literature or their system prompt.

In 109 cases (25.95%), the generated code successfully identified counterexamples, which is critical for falsifying conjectures. Secondly, of the 420 unique outputs, only 40 (9.52%) produced conjectures with no counterexamples. ChatGPT significantly outperformed both Claude and Gemini in this area, generating 26.60% valid conjectures compared to Gemini's 4.64% and Claude's 4.49%. This shows that ChatGPT was more effective at producing conjectures that are plausible at first glance. However, a large portion of the GPT-4 conjectures were looking at the size of the solubilizer rather than about interactions with other groups, group structure, or subgroup properties. Therefore, one could argue that they were easier to write code for, or at least more likely to succeed based on similarity. Further still, results classified as having "no counterexamples" by GPT-4 seemed to be qualitatively more obvious than those by Claude or Gemini (see Conjecture A.5 vs. Conjecture A.4 vs. Conjecture A.9). Lastly, the fact that the models are able to generate novel, original conjectures at all provides promise for these models to be used as useful tools when developing the theory of a new construction.

### Limitations

A limitation observed in both models was the generation of non-executable code, which occurred in 271 instances (64.52% of unique outputs). Gemini and Claude struggled more with code execution, having 81.86% and 49.44% instances of non-executable code respectively, compared to ChatGPT's 37.76%. This potentially points to differences in how the models handle code syntax in GAP, however the models were prompted to have a near identical code format (see Appendix A.2). This corroborates the idea that some conjectures set by Claude/Gemini are more difficult to write code for, and therefore more likely to fail in this system. Interestingly, there are some examples that Claude/Gemini gave

  
**Metric** & **Max** & **Min** & **Mean** & **Median** \\ 
**ChatGPT vs. ChatGPT** & 0.9284 & 0.0486 & 0.2911 & 0.2658 \\
**Claude vs. Claude** & 0.9950 & 0.0921 & 0.4238 & 0.3683 \\
**Gemini vs. Gemini** & 0.8742 & 0.0132 & 0.2451 & 0.2264 \\
**ChatGPT vs. Claude** & 0.8695 & 0.0356 & 0.2699 & 0.2552 \\
**ChatGPT vs. Gemini** & 0.8892 & 0.02638 & 0.2442 & 0.2292 \\
**Claude vs. Gemini** & 0.7662 & 0.0149 & 0.2220 & 0.2102 \\
**Claude vs. Literature** & 0.7101 & 0 & 0.1501 & 0.1332 \\
**Gemini vs. Literature** & 0.8274 & 0 & 0.1812 & 0.1510 \\
**GPT-4 vs. Literature** & 0.9388 & 0 & 0.2139 & 0.1978 \\   

Table 2: Output Cosine Similarity Distributionthat would be much harder to check in GAP with a constrained time limit where Claude/Gemini could not write executable code (see Appendix A.4.6). Lastly, we note that the models had different approaches to generating code to test conjectures, with Claude and Gemini being more similar. We found that ChatGPT liked to preemptively restrict the groups it would consider. For example, ChatGPT conjectured that the solubilizer couldn't be bigger than or that it couldn't be exactly equal to any of the following numbers (for all non-solvable groups) in seperate conjectures: [2; 3; 6; 8; 9; 10; 11; 12; 14; 15; 16; 18; 20; 24; 25; 27; 32; 49; 50; 126].

While this study focuses on group theory and solubilizers, a relatively unexplored area, the approach could be generalized to other domains. We acknowledge limitations of using GAP, an algebra software. However, future work could easily extend this methodology to fields like number theory, geometry, representation theory, or combinatorics by integrating tools like SageMath, MAGMA, or other computational solvers.

### Future Work

We propose the investigation of conjecture generation in fields where existing conjectures are sparse or absent. For example, LLMs could be applied to generate conjectures in newer or less explored areas such as tropical geometry or higher homotopy theory, where automated tools exist but have yet to be fully integrated with LLMs. Furthermore, the study above was limited to using a single LLM. If one model is better at writing code and the other is better at conjectures, using a combination structure could yield better results. We remark that a quantitative metric for 'interestingness' of a math conjecture or problem seems to be elusive, nontrivial, yet useful (see Appendix A.6).

## 6 Conclusion

The study opens up several promising avenues for the use of LLMs in research. Our work, while small, shows the potentially impactful way that LLM's augmented with other computational capacity can solve more complex problem. For example, further work integrating conjecture generation with proof validation systems could streamline the process of discovery.

That being said, LLM-based conjecture generation is still very limited to existing knowledge. Rather than producing fundamentally new ideas, LLMs are likely to lean on known results, limiting their ability to drive groundbreaking discoveries [(18)]. Indeed, when thinking of language models as statistical traversers of some sort of higher dimensional surface built from training data, it is easy to imagine that the models are not able to stray too far from what they are fed to generate the surface. Specifically, conjectures and theorems involving well-understood subgroups on which the solubilizer is inspired (think centralizer and normalizer) can serve as an incredibly large well from which an LLM can sample a new direction about the solubilizer. This may all be permissible to a practitioner if one is only interested in clearing out the brush around a new construct such as the solubilizer; but as of writing, it should not be expected that these models will conjecture something profound.

We demonstrate that combining LLMs with computational resources like GAP can successfully generate and test original, albeit simple math conjectures. Indeed, performance suggests that LLMs like ChatGPT, Claude, and Gemini have potential, but only on conjectures that are similar to existing ideas or are otherwise simple. Furthermore, the models face significant challenges in generating executable code and avoiding duplicate conjectures. Indeed, ChatGPT-4 demonstrated stronger performance in generating conjectures that could not be immediately falsified, Claude was slightly more effective at identifying counterexamples, and Gemini had the least redundancy likely due to the longer context window. The high percentage of non-executable code reinforces the need for robust error-checking and handling within the models. GAP is limited in the variety of error codes that are produced when code fails, so other more verbose computational algebra solvers could help with error correction. Lastly, further analysis of failed code generation to find patterns of failure could lead to better prompting for avoiding common bugs. Further work would likely include adding a formal automated theorem prover or another form of neuro-symbolic proof engine, giving an end-to-end system that can generate new conjectures and prove them in a single pass[(19; 20)]. The authors are also interested to see other new approaches for accurate conjecture generation in various abstract fields, or more generally, improvements to conjecture generation by non-LLM based models.