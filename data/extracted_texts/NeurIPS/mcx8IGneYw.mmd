# Neural Lighting Simulation for Urban Scenes

Ava Pun\({}^{1,3}\)\({}^{,}\)\({}^{,}\) Gary Sun\({}^{1,3}\)\({}^{,}\)\({}^{,}\)\({}^{,}\) Jingkang Wang\({}^{1,2}\)\({}^{,}\)\({}^{,}\) Yun Chen\({}^{1,2}\)\({}^{,}\) Ze Yang\({}^{1,2}\)

**Sivabalan Manivasagam\({}^{1,2}\) Wei-Chiu Ma\({}^{1,4}\) Raquel Urtasun\({}^{1,2}\)**

Waabi\({}^{1}\) University of Toronto\({}^{2}\) University of Waterloo\({}^{3}\) MIT\({}^{4}\)

Equal contributions.Work done while a research intern at Waabi.

###### Abstract

Different outdoor illumination conditions drastically alter the appearance of urban scenes, and they can harm the performance of image-based robot perception systems if not seen during training. Camera simulation provides a cost-effective solution to create a large dataset of images captured under different lighting conditions. Towards this goal, we propose _LightSim_, a neural lighting camera simulation system that enables diverse, realistic, and controllable data generation. LightSim automatically builds _lighting-aware digital twins_ at scale from collected raw sensor data and decomposes the scene into dynamic actors and static background with accurate geometry, appearance, and estimated scene lighting. These digital twins enable actor insertion, modification, removal, and rendering from a new viewpoint, all in a lighting-aware manner. LightSim then combines physically-based and learnable deferred rendering to perform realistic relighting of modified scenes, such as altering the sun location and modifying the shadows or changing the sun brightness, producing spatially- and temporally-consistent camera videos. Our experiments show that LightSim generates more realistic relighting results than prior work. Importantly, training perception models on data generated by LightSim can significantly improve their performance. Our project page is available at https://waabi.ai/lightsim/.

## 1 Introduction

Humans can perceive their surroundings under different lighting conditions, such as identifying traffic participants while driving on a dimly lit road or under mild sun glare. Unfortunately, modern camera-based perception systems, such as those in self-driving vehicles (SDVs), are not as robust . They typically only perform well in the canonical setting they were trained in, and their performance drops significantly under different unseen scenarios, such as in low-light conditions . To reduce distribution shift, we could collect data under various lighting conditions for each area in which we want to deploy the SDVs, generating a diverse dataset on which to train the perception system. Unfortunately, this is not scalable as it is too expensive and time-consuming.

Simulation is a cost-effective alternative for generating large-scale data with diverse lighting. To be effective, a simulator should be realistic, controllable, and diverse. Realistic simulation of camera data enables generalization to the real world. Controllable actor placement and lighting allow the simulator to generate the desired training scenarios. Diverse backgrounds, actor assets, and lighting conditions allow simulation to cover the full real-world distribution. While existing game-engine-based self-driving simulators such as CARLA  are controllable, they provide a limited number of manually designed assets and lighting conditions. Perception systems trained on this data generalize poorly to the real world .

To build a more realistic, scalable, and diverse simulator, we instead propose reconstructing "digital twins" of the real world at scale from a moving platform equipped with LiDAR and cameras. By reconstructing the geometry and appearance of the actors and static backgrounds to create composable neural assets, as well as estimating the scene lighting, the digital twins can provide a rich asset library for simulation. Existing methods for building digital twins and data-driven simulators [2; 40; 89; 73; 57] bake the lighting into the scene, making simulation under new lighting conditions impossible. In contrast, we create lighting-aware digital twins, which enable actor insertion, removal, modification, and rendering from new viewpoints with accurate illumination, shadows, occlusion, and temporal consistency. Moreover, by estimating the scene lighting, we can use the digital twins for relighting.

Given a digital twin, we must relight the scene to a target novel lighting condition. This is, however, a challenging task. Prior image-based synthesis methods [4; 5] perform relighting via 2D style transfer techniques, but they typically lack human-interpretable controllability, are not temporally or spatially consistent, and can have artifacts. Inverse neural rendering methods [82; 49; 39; 83] aim to decompose the scene into geometry, materials and lighting, which allows for relighting through physically-based rendering [7; 30]. However, the ill-posed nature of lighting estimation and intrinsic decomposition makes it challenging to fully disentangle each aspect accurately, resulting in unrealistic relit images. Both approaches, while successful on synthetic scenes or outdoor landmarks  with dense data captured under many lighting conditions, have difficulty performing well on large outdoor scenes. This is primarily due to the scarcity of real-world scenes captured under different lighting conditions. Moreover, most prior works perform relighting on static scenes and have not demonstrated realistic relighting for dynamic scenes where both the actors and the camera viewpoint are changing, resulting in inter-object lighting effects that are challenging to simulate.

In this paper, we present _LightSim_, a novel lighting simulation system for urban driving scenes that generates diverse, controllable, and realistic camera data. To achieve _diversity_, LightSim reconstructs lighting-aware digital twins from real-world sensor data, creating a large library of assets and lighting environment maps for simulation. LightSim then leverages physically-based rendering to enable _controllable_ simulation of the dynamic scene, allowing for arbitrary actor placement, SDV location, and novel lighting conditions. To improve realism, we further enhance the physically-based renderer

Figure 1: **LightSim builds digital twins from large-scale data with lighting variations and generates high-fidelity simulation videos.** Top: LightSim produces realistic scene relighting and shadow editing videos. Bottom: We generate a safety-critical scenario with two vehicles cutting in and perform lighting-aware camera simulation. See Appendix E and project page for more examples.

with an image-based neural deferred renderer to perform relighting, enabling data-driven learning to overcome geometry artifacts and ambiguity in the decomposition due to not having perfect knowledge of the scene and sensor configuration. To overcome the lack of real-world scenes captured under different lighting conditions, we train our neural deferred renderer on a mixture of real and synthetic data generated with physically-based rendering on our reconstructed digital twins.

We demonstrate the effectiveness of our approach on PandaSet , which contains more than \(100\) real-world self-driving scenes covering \(20\) unique kilometers. LightSim significantly outperforms the state of the art, producing high-quality photorealistic driving videos under a wide range of lighting conditions. We then showcase several capabilites of LightSim, such as its ability to create high-fidelity and lighting-aware actor insertion, scene relighting, and shadow editing (Fig. 1). We demonstrate that by training camera perception models with LightSim-generated data, we can achieve significant performance improvements, making the models more robust to lighting variation. We believe LightSim is an important step towards enhancing the safety of self-driving development and deployment.

## 2 Related Work

Outdoor lighting estimation:As a first step for lighting simulation, lighting estimation aims to recover the \(360^{}\) HDR light field from observed images for photo-realistic virtual object insertion [20; 28; 19; 27; 37; 96; 67; 71; 101; 82; 78; 74; 83]. Existing works generally use neural networks to predict various lighting representations, such as environment maps [71; 101; 78; 82; 74; 83], spherical lobes [8; 43], light probes , and sky models [28; 27; 96], from a single image. For outdoor scenes, handling the high dynamic range caused by the presence of the sun is a significant challenge. Moreover, due to the scarcity of real-world datasets and the ill-posed nature of lighting estimation [82; 74], it is challenging to precisely predict peak intensity and sun location from a single limited field-of-view low-dynamic range (LDR) image. To mitigate these issues, SOLDNet [101; 74] enhances the diversity of material and lighting conditions with synthetic data and introduces a disentangled global and local lighting latent representation to handle spatially-varying effects. NLFE  uses hybrid sky dome / light volume and introduces adversarial training to improve realism with differentiable actor insertion. In contrast, our work fully leverages the sensory data available in a self-driving setting (_i.e._, multi-camera images, LiDAR, and GPS) to accurately recover spatially-varying environment maps.

Inverse rendering with lighting:Another way to obtain lighting representations is through joint geometry, material, and lighting optimization with inverse rendering [67; 42; 84; 55; 79; 23; 49; 91; 64; 39; 41; 92; 83]. However, since optimization is conducted on a single scene and material decomposition is the primary goal, the optimized lighting representations are usually not generalizable and do not work well for relighting [67; 55; 23]. Inspired by the success of NeRF in high-fidelity 3D reconstruction, some works use volume rendering to recover material and lighting given image collections under different lighting conditions [64; 49; 39], but they are limited in realism and cannot generalize to unseen scenes. Most recently, FEGR , independent and concurrent to our work, proposes a hybrid framework to recover scene geometry, material and HDR lighting of urban scenes from posed camera images. It demonstrates realistic lighting simulation (actor insertion, shadow editing, and scene relighting) for static scenes. However, due to imperfect geometry and material/lighting decomposition, relighting in driving scenes introduces several noticeable artifacts, including unrealistic scene color, blurry rendering results that miss high-frequency details, obvious mesh boundaries on trees and buildings, and unnatural sky regions when using other HDR maps. In contrast, LightSim learns on many driving scenes and performs photorealistic enhanced deferred shading to produce more realistic lighting videos for _dynamic scenes_.

Camera simulation for robotics:There is extensive work on developing simulated environments for safer and faster robot development [33; 86; 53; 17; 75; 38; 15; 29; 17; 35; 9; 13]. Two major lines of work in sensor simulation for self-driving include graphics-based [18; 68] and data-driven simulation [80; 66; 50; 88]. Graphics-based simulators like CARLA  and AirSim  are fast and controllable, but they face limitations in scaling and diversity due to costly manual efforts in asset building and scenario creation. Moreover, these approaches can generate unrealistic sensor data that have a large domain gap for autonomy [26; 85]. Data-driven approaches leverage computer vision techniques and real-world data to build simulators for self driving [34; 3; 2; 81; 40]. Unfortunately, existing works tend to fall short of realism, struggle with visual artifacts and domain gap [34; 3; 2; 81;40], and lack comprehensive control in synthesizing novel views [12; 82; 79; 90; 72]. Most recent works [57; 36; 77; 89; 46] use neural radiance fields to build digital twins and represent background scenes and agents as MLPs, enabling photorealistic rendering and controllable simulation across a single snippet. However, these works bake lighting and shadows into the radiance field and therefore cannot conduct actor insertion under various lighting conditions or scene-level lighting simulation. In contrast, LightSim builds lighting-aware digital twins for more controllable camera simulation.

## 3 Building Lighting-Aware Digital Twins of the Real World

The goal of this paper is to create a diverse, controllable, and realistic simulator that can generate camera data of scenes at scale under diverse lighting conditions. Towards this goal, LightSim first reconstructs lighting-aware digital twins from camera and LiDAR data collected by a moving platform. The digital twin comprises the geometry and appearance of the static background and dynamic actors obtained through neural rendering (Sec. 3.1), as well as the estimated scene lighting (Sec. 3.2). We carefully build this representation to allow full controllability of the scene, including modifying actor placement or SDV position, adding and removing actors in a lighting-aware manner for accurate shadows and occlusion, and modifying lighting conditions, such as changing the sun's location or intensity. In Sec. 3.3, we then describe how we perform learning on sensor data to build the digital twin and estimate lighting. In Sec. 4, we describe how we perform realistic scene relighting with the digital twin to generate the final temporally consistent video.

### Neural Scene Reconstruction

Inspired by [89; 77; 57], we learn the scene geometry and base texture via neural fields. We design our neural field \(F:(s,_{d})\) to map a 3D location \(\) to a signed distance \(s\) and view-independent diffuse color \(_{d}^{3}\). We decompose the driving scene into a static background \(\) and a set of dynamic actors \(\{_{i}\}_{i=1}^{M}\) and map multi-resolution spatial feature grids  using two MLP networks: one for the static scene and one for the dynamic actors. This compositional representation allows for 3D-aware actor insertion, removal, or manipulation within the background. From our learned neural field, we use marching cubes  and quadric mesh decimation  to extract simplified textured meshes \(\) for the scene. For simplicity, we specify base materials  for all the assets and defer material learning to future work. Please see Appendix A.1 for details. Given the desired lighting conditions, we can render our reconstructed scene in a physically-based renderer to model object-light interactions.

### Neural Lighting Estimation

In addition to extracting geometry and appearance, we estimate the scene lighting (Fig. 3, left). We use a high-dynamic-range (HDR) panoramic sky dome \(\) to represent the light from the sun and the sky. This representation well models the major light sources of outdoor daytime scenes and is compatible with rendering engines [7; 30]. Unfortunately, estimating the HDR sky dome from sensor data is challenging, as most cameras on SDVs have limited field-of-view (FoV) and do not capture the full sky. Additionally, camera data are typically stored with low dynamic range (LDR) in self-driving datasets, _i.e._, intensities are represented with 8-bits. To overcome these challenges, we first leverage multi-camera data and our extracted geometry to estimate an incomplete panorama LDR image that

Figure 2: **Overview of LightSim. Given sensor observations of the scene, we first perform neural scene reconstruction and lighting estimation to build lighting-aware digital twins (left). Given a target lighting, we then perform both physically-based and neural deferred rendering to simulate realistic driving videos under diverse lighting conditions (right).**

captures scene context and available sky observations. We then apply an inpainting network to fill in missing sky regions. Then, we utilize a sky dome estimator network that lifts the LDR panorama image to an HDR sky dome and fuses it with GPS data to obtain accurate sun direction and intensity. Unlike prior works that estimate scene lighting from a single limited-FoV LDR image [82; 94; 74], our work leverages multi-sensor data for more accurate estimation. We now describe these steps (as shown in Fig. 3, left) in detail.

Panorama reconstruction:Given \(K\) images \(=\{_{i}\}_{i=1}^{K}\) captured by multiple cameras triggered close in time and their corresponding camera poses \(=\{_{i}\}_{i=1}^{K}\), we first render the corresponding depth maps \(=\{_{i}\}_{i=1}^{K}\) from extracted geometry \(\): \(_{i}=(,_{i})\), where \(\) is the depth rendering function and \(_{i}^{3 4}\) is the camera projection matrix (a composition of camera intrinsics and extrinsics). We set the depth values for the sky region to infinity. For each camera pixel \((u^{},v^{})\), we use the rendered depth and projection matrix to estimate 3D world coordinates, then apply an equirectangular projection \(E\) to determine its intensity contribution to panorama pixel \((u,v)\), resulting in \(_{}\):

\[_{}=(,, )=E(^{-1}(,,)),\] (1)

where \(\) is the pixel-wise transformation that maps the RGB of limited field-of-view (FoV) images \(\) at coordinate \((u^{},v^{})\) to the \((u,v)\) pixel of the panorama. For areas with overlap, we average all source pixels that are projected to the same panorama \((u,v)\). In the self-driving domain, the stitched panorama \(_{}\) usually covers a 360\({}^{}\) horizontal FoV, but the vertical FoV is limited and cannot fully cover the sky region. Therefore, we leverage an inpainting network  to complete \(_{}\), creating a full-coverage (\(360^{} 180^{}\)) panorama image.

Generating HDR sky domes:For realistic rendering, an HDR sky dome should have accurate sun placement and intensity, as well as sky appearance. Following [96; 82], we learn an encoder-decoder sky dome estimator network that lifts the incomplete LDR panorama to HDR, while also leveraging GPS and time of day for more accurate sun direction. The encoder first maps the LDR panorama image to a low-dimensional representation to capture the key attributes of the sky dome, including a sky appearance latent \(_{}^{d}\), peak sun intensity \(_{}\), and sun direction \(_{}\). By explicitly encoding sun intensity and direction, we enable more human-interpretable control of the lighting conditions and more accurate lighting estimation. The decoder network processes this representation and outputs the HDR sky dome \(\) as follows:

\[=(_{}, [_{},_{}]), _{},_{},_{ }=().\] (2)

When GPS and time of day are available, we replace the encoder-estimated direction with the GPS-derived sun direction for more precise sun placement. Please see Appendix A.1 for details.

### Learning

We now describe the learning process to extract static scenes and dynamic actor textured meshes, as well as training the inpainting network and sky dome estimator.

Optimizing neural urban scenes:We jointly optimize feature grids and MLP headers \(\{f_{},f_{_{d}}\}\) to reconstruct the observed sensor data via volume rendering. This includes a photometric loss on the rendered image, a depth loss on the rendered LiDAR point cloud, and a regularizer, as follows: \(_{}=_{}+_{}_{}+_{}_{}\). Specifically, we have

\[_{}=_{ }|}_{_{}}\|C()- ()\|_{2},\ _{}=_{}|}_{ _{}}\|D()-( )\|_{2}.\] (3)

Figure 3: **LightSim modules**. Left: neural lighting estimation. Right: neural deferred rendering.

Here, \(\) represents the set of camera or LiDAR rays. \(C()\) is the observed color for ray \(\), and \(()\) is the predicted color. \(D()\) is the observed depth for ray \(\), and \(()\) is the predicted depth in the range view. To encourage smooth geometry, we also regularize the SDF to satisfy the Eikonal equation and have free space away from the LiDAR observations .

Training panorama inpainting:We train a panorama inpainting network to fill the unobserved regions for stitched panorama \(_{}\). We adopt the DeepFill-v2  network and train on the Holicity  dataset, which contains 6k panorama images. During training, we first generate a camera visibility mask using limited-FoV camera intrinsics to generate an incomplete panorama image. The masked panorama is then fed into the network and supervised with the full panorama. Following , we use the hinge GAN loss as the objective function for the generator and discriminator.

Training sky dome estimator:We train a sky dome estimator network on collected HDR sky images from HDRMaps . The HDRs are randomly distorted (including random exposure scaling, horizontal rotation, and flipping) and then tone-mapped to form LDR-HDR pairs \((,)\) pairs. Following , we apply teacher forcing randomly and employ the \(L_{1}\) angular loss, \(L_{1}\) peak intensity, and \(L_{2}\) HDR reconstruction loss in the log space during training.

## 4 Neural Lighting Simulation of Dynamic Urban Scenes

As is, our lighting-aware digital twin reconstructs the original scenario. Our goal now is to enable controllable camera simulation. To be controllable, the scene representation should not only replicate the original scene but also handle changes in dynamic actor behavior and allow for insertion of synthetic rare objects, such as construction cones, that are challenging to find in real data alone. This enables diverse creation of unseen scenes. As our representation is compositional, we can add and remove actors, modify the locations and trajectories of existing actors, change the SDV position, and perform neural rendering on the modified scene to generate new camera video in a spatially- and temporally-consistent manner. Using our estimated lighting, we can also use a physically-based renderer to seamlessly composite synthetic assets, such as CAD models , into the scene in a 3D- and lighting-aware manner. These scene edits result in an "augmented reality" representation \(^{},^{}\) and source image \(^{}_{}\). We now describe how we perform realistic scene relighting (Fig. 3 right) to generate new relit videos for improving camera-based perception systems.

Given the augmented reality representation \(\{^{},^{},^{}_{ }\}\), we can perform physically-based rendering under a novel lighting condition \(^{}\) to generate a relit rendered video. The rendered images faithfully capture scene relighting effects, such as changes in shadows or overall scene illumination. However, due to imperfect geometry and noise in material/lighting decomposition, the rendering results lack realism (_e.g._, they may contain bluriness, unrealistic surface reflections and boundary artifacts). To mitigate this, we propose a photo-realism enhanced neural deferred rendering paradigm. Deferred rendering [16; 65] splits the rendering process into multiple stages (_i.e._, rendering geometry before lighting, then composing the two). Inspired by recent work [58; 61; 98], we use an image synthesis network that takes the source image and pre-computed buffers of lighting-relevant data generated by the rendering engine to produce the final relit image. We also provide the network the environment maps for enhanced lighting context and formulate a novel paired-data training scheme by leveraging the digital twins to generate synthetic paired images.

Generate lighting-relevant data with physically-based rendering:To perform neural deferred rendering, we place the static background and dynamic actor textured meshes \(\) in a physically-based renderer  and pre-compute the rendering buffers \(_{}^{h w 8}\), including position, depth, normal and ambient occlusion for each frame. Additionally, given an environment map \(\) and material maps, the physically-based renderer performs ray-tracing to generate the rendered image \(_{|}\). We omit \(\) in the following for simplicity. To model shadow removal and insertion, we also generate a shadow ratio map \(=_{}/}_{}\), where \(}_{}\) is the rendered image without rendering shadow visibility rays, for both the source and target environment light maps \(^{},^{}\).

We then use a 2D U-Net  that takes the source image \(^{}\), render buffers \(_{}\), and shadow ratio maps \(\{^{},^{}\}\), conditioned on the source and target HDR sky domes \(\{^{},^{}\}\). This network outputs the rendered image \(^{}\) under the target lighting conditions as follows:

\[^{}=([^{}, _{},^{},^{}], [^{},^{}]).\] (4)This enables us to edit the scene, perform scene relighting, and generate a sequence of images under target lighting as the scene evolves to produce simulated camera videos. The simulation is spatially and temporally consistent since our method is physically-based and grounded by 3D digital twins.

Learning:To ensure that our rendering network maintains controllable lighting and is realistic, we train it with a combination of synthetic and real-world data. We take advantage of the fact that our digital twin reconstructions are derived from real-world data, and that our physically-based renderer can generate paired data of different source and target lightings of the same scene. This enables two main data pairs for training the network to learn the relighting task with enhanced realism. For the first data pair, we train our network to map \(_{|^{}}_{ |^{}}\), the physically-based rendered images under the source and target lighting. With the second data pair, we improve realism by training the network to map \(_{|^{}}_ {}\), mapping any relit synthetic scene to its original real world image given its estimated environment map as the target lighting. During training, we also encourage self-consistency by ensuring that, given an input image with identical source and target lighting, the model recovers the original image. The training objective consists of a photometric loss (\(_{}\)), a perceptual loss (\(_{}\)), and an edge-based content-preserving loss (\(_{}\)):

\[_{}=_{i=1}^{N}_{i}^{}-}_{i}^{} \|_{2}}_{_{}}+_{} ^{M}\|V^{j}(_{i}^{})-V^{j}( }_{i}^{})\|_{2}}_{_{}}+ _{}_{i}^{}- }_{i}^{}\|_{2}}_{_{}},\] (5)

where \(N\) is the number of training images and \(^{}\)/\(^{}\) are the observed/synthesized label image and predicted image under the target lighting, respectively. \(V^{j}\) denotes the \(j\)-th layer of a pre-trained VGG network , and \(\) is the image gradient approximated by Sobel-Feldman operator .

## 5 Experiments

We showcase LightSim's capabilities on public self-driving data, which contains a rich collection of sensor data of dynamic urban scenes. We first introduce our experiment setting, then compare LightSim against state-of-the-art (SoTA) scene-relighting methods and ablate our design choices. We then show that our method can generate realistic driving videos with added actors and modified trajectories under diverse lighting conditions. Finally, we show that using LightSim to augment training data can significantly improve 3D object detection.

### Experimental Setup

Datasets:We evaluate our method primarily on the public real-world driving dataset PandaSet , which contains 103 urban scenes captured in San Francisco, each with a duration of 8 seconds

Figure 4: **Qualitative comparison of scene relighting. For the first and third rows, the real images (other PandaSet snippets) under target lighting conditions are provided for better reference.**

(80 frames, sampled at 10hz) acquired by six cameras (\(1920 1080\)) and a 360\({}^{}\) 64-beam LiDAR. To showcase generalizability, we also demonstrate our approach on ten dynamic scenes from the nuScenes  dataset. These driving datasets are challenging as the urban street scenes are unbounded; large-scale (\(>300\) m \(\) 80 m); have complex geometry, materials, lighting, and occlusion; and are captured in a single drive-by pass (forward camera motion).

Baselines:We compare our model with several SoTA scene-relighting methods. We consider several inverse-rendering approaches [94; 64], an image-based color-transfer approach , and a physics-informed image-synthesis approach . Self-OSR  is an image-based inverse-rendering approach that uses generative adversarial networks (GANs) to decompose the image into albedo, normal, shadow, and lighting. NeRF-OSR  performs physically-based inverse rendering using neural radiance fields. Color Transfer  utilizes histogram-based color matching to harmonize color appearance between images. Enhancing Photorealism Enhancement (EPE)  enhances the realism of synthetic images using intermediate rendering buffers and GANs. EPE uses the rendered image \(_{|^{}}\) and G-buffer data generated by our digital twins to predict the relit image.

### Neural Lighting Simulation

Comparison to SoTA:We report scene relighting results on PandaSet in Table 1. Since the ground truth is unavailable, we use FID  and KID  to measure the realism and diversity of relit images. For each approach, we evaluate on 1,380 images with 23 lighting variations and report FID/KID scores. 11 of the target lighting variations are estimated from real PandaSet data, while the remaining twelve are outdoor HDRs sourced from HDRMaps . See Appendix C.1 for more details.

Compared to Self-OSR, NeRF-OSR and EPE, LightSim achieves better performance on FID, which indicates that our relit images are more realistic and contain fewer visual artifacts when employed as inputs by ML models. We also show qualitative results in Fig. 4 together with source and target lighting HDRs (Row 1 and 3: relighting with estimated lighting conditions of other PandaSet snippets, Row 2 and 4: third-party HDRs). While Color Transfer achieves the best FID, visually we can see that it only adjusts the global color histogram and does not perform physically-accurate directional lighting (_e.g._, no newly cast shadows). Self-OSR estimates the source and target lighting as spherical harmonics, but since it must reason about 3D geometry and shadows using only a single image, it produces noticeable artifacts. NeRF-OSR has difficulty with conducting reasonable intrinsic decomposition (_e.g._, geometry, shadows) and thus cannot perform realistic and accurate scene relighting. EPE incorporates the simulated lighting effects from our digital twins and further enhances realism, but there are obvious artifacts due to blurry texture, broken geometry, and unrealistic hallucinations. In contrast, LightSim produces more reliable and higher-fidelity relighting resultsunder diverse lighting conditions. See Appendix E.1 for more results. In Appendix E.4, we also evaluate LightSim's lighting estimation compared to SoTA and demonstrate improved performance.

Downstream perception training:We now investigate if realistic lighting simulation can help improve the performance of downstream perception tasks under unseen lighting conditions. We consider a SoTA camera-based birds-eye-view (BEV) detection model BEVFormer . Specifically, we train on 68 snippets collected in the city and evaluate on 35 snippets in a suburban area, since these two collections are exposed to different lighting conditions. We generate three lighting variations for data augmentation. One lighting condition comes from the estimated sky dome for log-084 (captured along the El Camino Real in California), and the other two are real-world cloudy and sunny HDRs. We omit comparison to NeRF-OSR as its computational cost makes it challenging to render at scale. Table 2 demonstrates that LightSim augmentation yields a significant performance improvement (+4.5 AP) compared to baseline augmentations, which either provide smaller benefits or harm the detection performance.

Ablation Study:Fig. 5 showcases the importance of several key components in training our neural deferred rendering module. Pre-computed rendering buffers help the network predict more accurate lighting effects. The edge-based content-preserving loss results in a higher-fidelity rendering that retains fine-grained details from the source image. Training the network to relight synthetic rendered images to the original real image with its estimated lighting enhances the photorealism of the simulated results. Please see more ablations in Appendix E.2.

Realistic and controllable camera simulation:LightSim recovers more accurate HDR sky domes compared to prior SoTA works, resulting in more realistic actor insertion (Fig. 6). LightSim inserts the new actors seamlessly and can model lighting effects such as cast shadows for the actors and

Figure 6: **Lighting-aware multi-camera simulation. Inserted actors highlighted with boxes.**

Figure 7: **Lighting-aware camera simulation of novel scenarios.**

static scene, all in a 3D-aware manner for consistency across cameras. Our simulation system also performs realistic, temporally-consistent and lighting-aware scene editing to generate immersive experiences for evaluation. In Fig. 7, we start from the original scenario and perform scene editing by removing all dynamic actors and inserting traffic cones, barriers, and three vehicle actors in the crossroads. Then, we apply scene relighting to change the scene illumination to sunny, cloudy, etc. In Fig. 1, we show another example where we modify the existing real-world data to generate a challenging scenario with two cut-in vehicles.

Generalization study on nuScenes:We now showcase LightSim's ability to generalize to driving scenes in nuScenes . We build lighting-aware digital twins for each scene, then apply a neural deferred rendering model pre-trained on PandaSet. LightSim transfers well and performs scene relighting robustly (see Fig. 8). See Appendix E.6 for more examples.

## 6 Limitations

LightSim assumes several simplifications when building lighting-aware digital twins, including approximate diffuse-only reconstruction, separate lighting prediction, and fixed base materials. This results in imperfect intrinsic decomposition and sim-to-real discrepancies (see Fig. 9). One major failure case we notice is that LightSim cannot seamlessly remove shadows, particularly in bright, sunny conditions where the original images exhibit distinct cast shadows (see Fig. A26). This is because the shadows are mostly baked during neural scene reconstruction, thus producing flawed synthetic data that confuses the neural deferred rendering module. We believe those problems can be addressed through better intrinsic decomposition with priors and joint material/lighting learning [83; 64]. Moreover, LightSim cannot handle nighttime local lighting sources such as street lights, traffic lights and vehicle lights. Finally, faster rendering techniques can be incorporated to enhance LightSim's efficiency [69; 56].

## 7 Conclusion

In this paper, we aimed to build a lighting-aware camera simulation system to improve robot perception. Towards this goal, we presented LightSim, which builds lighting-aware digital twins from real-world data; modifies them to create new scenes with different actor layouts, SDV viewpoints, and lighting conditions; and performs scene relighting to enable diverse, realistic, and controllable camera simulation that produces spatially- and temporally-consistent videos. We demonstrated LightSim's capabilities to generate new scenarios with camera video and leveraged LightSim to significantly improve object detection performance. We plan to further enhance our simulator by incorporating material model decomposition, local light source estimation, and weather simulation.

Figure 8: **Scene relighting on nuScenes.** The real images are displayed in the leftmost column.