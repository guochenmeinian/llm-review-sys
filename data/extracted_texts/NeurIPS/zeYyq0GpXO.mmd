# Exploring Context Window of Large Language Models via Decomposed Positional Vectors

Zican Dong\({}^{1}\), Junyi Li\({}^{3}\), Xin Men\({}^{4}\), Wayne Xin Zhao\({}^{1}\), Bingning Wang\({}^{4}\)

Zhen Tian\({}^{1}\), Weipeng Chen\({}^{4}\), Ji-Rong Wen\({}^{1,2}\)

\({}^{1}\) Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\) School of Information, Renmin University of China

\({}^{3}\) Department of Computer Science, National University of Singapore

\({}^{4}\) Baichuan Inc.

dongzican@ruc.edu.cn, junyi_cs@nus.edu.sg

batmanfly@gmail.com, daniel@baichuan-inc.com

Equal Contribution.Corresponding author.

###### Abstract

Transformer-based large language models (LLMs) typically have a limited context window, resulting in significant performance degradation when processing text beyond the length of the context window. Extensive studies have been proposed to extend the context window and achieve length extrapolation of LLMs, but there is still a lack of in-depth interpretation of these approaches. In this study, we explore the positional information within and beyond the context window for deciphering the underlying mechanism of LLMs. By using a mean-based decomposition method, we disentangle positional vectors from hidden states of LLMs and analyze their formation and effect on attention. Furthermore, when texts exceed the context window, we analyze the change of positional vectors in two settings, _i.e.,_ direct extrapolation and context window extension. Based on our findings, we design two training-free context window extension methods, **positional vector replacement** and **attention window extension**. Experimental results show that our methods can effectively extend the context window length.

## 1 Introduction

Recently, Transformer-based large language models (LLMs) have demonstrated excellent capabilities on downstream tasks , in which positional encodings (_e.g.,_ absolute or relative) are widely used in Transformers to better capture positional information within input sequences . However, LLMs typically suffer from a limited input length (called _context window_), which is constrained by the maximum length of training data. Beyond the context window, the positional encodings at larger position indices are out-of-distribution (OOD), not encountered during the training phase. Therefore, when the input sequence exceeds the context window length, there would often be a significant degradation in model performances, as evidenced by a surge in perplexity (PPL) score .

Prior work has primarily focused on extending the context window of existing LLMs by manipulating positional encodings. Owing to its excellent performance and long-term decay nature, RoPE  has been widely used to learn positional encodings for existing LLMs . To circumvent the OOD positional encodings in RoPE, various methods have been proposed to modify the base  or positional indices . In addition, special relative positional encodings that apply larger negative biases to attention based on the relative distance have achieved promising length extrapolation, whichcan effectively stabilize the model performance beyond the context window [6; 17; 18]. Furthermore, decoder-only Transformers without positional encodings (NoPE) have been found to be capable of learning implicit positional information , and their context window size can be extended via the adjustment of temperature hyper-parameters . However, the above extension methods solely focus on adapting positional encodings or attention scores, lacking a detailed analysis of the underlying mechanisms of hidden states in LLMs.

In this work, we aim to investigate the inner working mechanism of LLMs within and beyond the context window to interpret these context window extension approaches. As the basis, our work is developed by analyzing the positional information implicitly encoded in the hidden states of LLMs across various layers and positions, both within and outside the context window. Inspired by previous work , we use a mean-based decomposition approach to disentangle **positional vectors** from the hidden states, which captures the information independent of semantics but related to positions.

Specifically, we first investigate how positional information is formed and examine its impact on the attention mechanism within the context window. Second, for inputs beyond the context window, we analyze the change of positional vectors in two settings, _i.e._, direct extrapolation and context window extension. Our key findings include: (1) After the first layer, initial tokens can form distinct positional vectors, serving as anchors for shaping positional vectors in subsequent tokens; (2) Positional vectors play a critical role in modulating the long-term decay and establishing attention sinks; (3) When exceeding the context window, OOD positional vector is the major factor contributing to performance degradation, while length extrapolation can effectively keep the consistency of positional vectors both within and beyond the context window; (4) Context window extension methods enable interpolation of positional vectors by adjusting the information flow from initial tokens to subsequent tokens.

Based on the empirical findings, we further propose two training-free context window extension methods from the perspective of interpolating positional vectors: **positional vector replacement** and **attention window extension**. For LLMs with NoPE, the former method replaces the positional vectors in critical layers with interpolated ones; while for LLMs with window attention and NoPE, the latter method directly scales the window size and adjusts the temperature hyper-parameter. We evaluate the length generalization capacities of the proposed methods on PG-19 . Experimental results demonstrate that our methods can effectively generalize to longer texts without fine-tuning, achieving comparable performance to previous methods.

Our main contributions are summarized as follows:

* We explicitly delineate the formation process and the effect of positional vectors, highlighting the anchoring role of initial tokens in shaping different positional vectors across tokens and their importance in achieving long-term decay and attention sinks.
* We are the first to unify length extrapolation and context window extension from the perspective of positional vectors, identifying that preventing OOD positional vectors is crucial for avoiding performance degradation.
* We propose two training-free context window extension methods via the lens of adjusting positional vectors, _i.e._,positional vector replacement and attention window extension. Experimental results show that our methods can effectively generalize to longer texts without fine-tuning.

## 2 Background

TransformerDecoder-only Transformer  has become the foundational architecture for LLMs [4; 8; 1]. For a Transformer with \(L\) layers and a context window size \(C\), given an input sequence \(\) of \(T\) tokens, _i.e._, \(\{x_{1},,x_{T}\}\), we denote the output of the \(l\)-th layer \(l\) as \(_{l}^{s}=\{_{l,1}^{s},,_{l,T}^{s}\}\). At each layer, the output \(_{l}^{s}\) is obtained through multi-head attention (MHA) and feed-forward network (FFN) with residual connections applied to both components as follows:

\[}_{l}^{s}=(_{l-1}^{s})+_{ l-1}^{s},_{l}^{s}=(}_{l}^{s})+ }_{l}^{s}.\] (1)

Finally, the output of the last layer \(_{L}^{s}\) is then projected into the logits, which will be used to generate the prediction probability for each token in the vocabulary.

Positional VectorPrevious work has found that positional information can be learned and encoded in the hidden states of Transformers . Drawing inspiration from prior work , we hypothesize that each hidden state (_e.g.,_ query, key, value, output of each layer) within Transformer can be decomposed into two parts, _i.e.,_ a _positional vector_ that captures positional information and a _semantic vector_ that captures the contextual information. Taking the output \(_{l,t}^{s}\) of the \(l\)-th layer at \(t\)-th position as an example, it can be decomposed into a positional vector \(_{l,t}\) and a semantic vector \(_{l,t}^{s}\):

\[_{l,t}^{s}=_{l,t}+_{l,t}^{s}.\] (2)

Such a decomposition can disentangle two primary factors, namely positional and semantic vectors, for interpreting the internal mechanism of LLMs. Notably, since positional vectors are globally shared across different inputs, there is no superscript \(s\) for \(_{l,t}\). Further, the positional vector \(_{l,t}\) can be decomposed into a _mean vector_\(_{l}\) and a _positional basis_\(_{l,t}\):

\[_{l,t}=_{l}+_{l,t},\] (3)

where the mean vector \(_{l}\) denotes the mean of the distribution of positional vectors and the positional basis \(_{l,t}\) denotes the offset of \(t\)-th position from the mean vector within the context window size \(C\). Following previous work , we adopt a mean-based decomposition method to obtain the above three vectors based on \(N\) samples from the training corpus as follows:

\[_{l,t}=_{s=1}^{N}_{l,t}^{s},\ \ \ _{l,t}=_{l,t}-_{t^{ }=1}^{C}_{l,t^{}},\ \ \ _{l,t}^{s}=_{l,t}^{s}-_{l,t}.\] (4)

With this decomposition, it offers an explicit way to analyze and explore the positional information encoded in the hidden states of Transformer models. For example, we can use similarity measurements to compare the positional vectors of different positions and also can visualize them in low-dimensional embedding space. In the following sections, we will mainly focus on studying the formation and impact of the positional vector \(_{l,t}\), and conduct the analysis experiments.

## 3 Empirical Analysis

### Experimental Settings

To better analyze positional information, we consider model variants with different positional encodings (PE) and attention mechanisms: variants without positional encodings (NoPE)  as well as variants with two different positional encodings: RoPE  and ALiBi . We continually pre-train the TinyLlama-1.1B checkpoint  on 50B tokens from RedPajama  with a context window \(C=2048\), resulting in a set of comparison models with different positional encodings and attention mechanisms, as shown in the Table 1. _Full attention_ means that each token can attend to all previous tokens, while _window attention_ restricts each token to attend only to previous tokens within a window size \(W\). The training details are described in Appendix A. We also evaluate common LLMs (_e.g.,_ Llama-3-8B) and LLMs without positional encodings trained from scratch, and the evaluated results are listed in Appendix F.

Specifically, we subsample 32K samples with the same number of tokens from RedPajama. We perform the inference on these data to obtain hidden states of LLMs. By using the mean-based decomposition method (Eq. 4), we can obtain the positional vectors \(_{l,t}\) of tokens in these sample texts.

### Formation and Effect of Positional Vectors within Context Window

In existing LLMs, the bottom (first) layer typically takes as input token embeddings that lack inherent positional information; while interestingly, the hidden states from top layers can implicitly capture positional information, even without explicit positional encodings [19; 21; 14]. In order to have a deep understanding of implicit positional information, we next investigate the formation and effect of positional vectors in Transformers, with both full attention and window attention.

   Model & TL-NoPE & TL-RoPE & TL-ALiBi & TL-Window & TL-Window-80 & TL-Window-RoPE \\  PE & NoPE & RoPE & ALiBi & NoPE & NoPE & RoPE \\ Attention & Full & Full & Full & Window (\(512\)) & Window (\(80\)) & Window (\(512\)) \\   

Table 1: The compared model variants. Full attention is denoted as _Full_ and window attention with a window size of \(W\) tokens is denoted as _Window (\(W\))_. We abbreviate TinyLLaMA as _TL_.

#### 3.2.1 Formation of Positional Vectors with Full Attention

Positional Vector After the First LayerTo study how positional information is distributed over different positions, we first visualize the positional vectors \(_{1,t}\) (Eq. 4) decomposed from the outputs of the first layer using principal component analysis (PCA). As shown in Figure 2 (left column), initial tokens (_e.g.,_\( 4\) tokens) exhibit significantly distinct positional vectors, while the positional vectors of subsequent tokens are similar to each other. As a comparison, we also present the PCA results of all positional vectors at the 7-th layer. Interestingly, position vectors are evenly distributed across all the positions in Figure 2 (right column). Such a finding indicates that position vectors have captured the corresponding positional information since these vectors are distinct from each other across positions. In other words, _being distinct_ can be considered as a kind of positional evidence. By comparing the left and right columns of Figure 2, it seems that only initial tokens are different from the rest tokens after the first layer, which might suggest that **after the first layer, initial tokens have already formed distinct positional information but subsequent tokens have not yet established such information.** To investigate the reasons behind this phenomenon, we select the first attention head in the first layer (similar to other heads) to analyze attention scores, as detailed in Appendix B. We can prove that the positional vector \(_{1,1}\) for the first token is different from the following tokens and the attention scores affect the formation of positional information. Thus, through several layers, the tokens after the first token will gradually form distinct positional vectors (Figure 2 right column).

Positional Information Flow From Initial TokensBy applying positional vectors at top layers (PCA visualized in Appendix G), we find that after forwarding several layers, tokens at all positions can also exhibit distinct positional vectors, and similar findings are also found in previous work . To trace back to the source of positional information, a reasonable speculation is that initial tokens play a key role in the formation of positional information for the rest tokens since only initial tokens capture positional information after the first layer. To validate this, we select two groups of reference tokens: initial tokens (1\(\)4) and secondary tokens (4\(\)256), and further analyze what information of these tokens is critical for the formation of positional information in subsequent tokens (\(>\)256). Thus, based on a top-down strategy, we conduct an ablation study for each group by respectively deleting the value \(_{l,t}^{s}\) of attention module (_w/o value_), the semantic vector \(_{l,t}^{s}\) (_w/o semantic vector_), positional vector \(_{l,t}\) (_w/o positional vector_), and positional basis \(_{l,t}\) (_w/o positional basis_). Then, for each variant, we average the outputs of all layers and decompose new positional vectors based on Eq. 4. Finally, we compute the average cosine similarity between the original and new positional vectors for those subsequent tokens (\(>\)256) and also report PPL on samples in RedPajama. From Table 2, we can see that removing the positional vector and basis of 1\(\)4 tokens largely affect the positional vectors at later positions (low similarity). Conversely, removing the semantic vector or altering secondary tokens has slight effects on both similarity and PPL. From these findings, we conclude that **the positional vectors of initial tokens seem to serve as the role of anchors, largely contributing to the formation of positional information in subsequent tokens.**

#### 3.2.2 Formation of Positional Vectors with Window Attention

Unlike full attention, LLMs employing window attention restrict each token to attend only to tokens within a window size. Previous work has shown that the maximum theoretical receptive field (TRF) in window attention is equal to the product of the window size \(W\) and the layer index \(l\).

To analyze how positional vectors change across layers, we compute _the number of distinct positional vectors_ within the maximum TRF. Notably, those tokens beyond the maximum TRF share the same positional vectors due to translation invariance . Specifically, we first randomly select a positional vector outside the maximum TRF and then compute the cosine similarity between positional vectors within the maximum TRF and the selected vector. We consider the positional vector with a similarity score lower than a threshold (_i.e.,_ 0.99) as _distinct_. Figure 2 presents the number of distinct positional vectors and TRF at each layer. We can see that after the first layer, only initial tokens show distinct positional vectors, further verifying the findings in Section 3.2.1. As the layer increases, more tokens display different positional information and the number of distinct positional vectors increases by 512 (a window size \(W\)) with each additional layer. The reason is that due to the constraint of window attention, each token at the preceding layer can only influence tokens within the window size at the next layer. As _being distinct_ indicates the formation of position information, **similar positional information flow from initial tokens to subsequent tokens also occurs for window attention, but gradually propagating across both windows and layers**.

#### 3.2.3 Effect of Positional Vectors on Attention

After discussing the formation of positional vectors, we explore their impact on the attention module, mainly focusing on the attention scores. We first extract queries and keys from each head in all layers, and then compute the average attention scores in the following four settings, including (1) _original:_ the original model, (2) _w/o semantic vector:_ removing the semantic vectors of keys and queries, (3) _w/o positional vector:_ removing the positional vectors of keys and queries, (4) _w/o positional basis:_ removing the positional basis of keys and queries. Figure 3 presents the logarithmic attention scores for the first 100 tokens in the first head and fifth layer (similar results in many other heads and layers).

    & & original &  &  &  &  \\  & & - & l\(\)4 & \(\)256 & 1\(\)4 & 4\(\)256 & 1\(\)4 & 4\(\)256 & 1\(\)4 & 4\(\)256 \\   & Sim & 1 & -0.1558 & 0.9797 & -0.1810 & 0.9086 & -0.1817 & 0.9046 & 0.9985 & 0.9514 \\  & PPL & 7.56 & \(>\)1000 & 8.97 & \(>\)1000 & 13.36 & \(>\)1000 & 10.23 & 8.20 & 10.55 \\   & Sim & 1 & 0.8394 & 0.9902 & 0.8505 & 0.9874 & 0.1711 & 0.9944 & 0.9970 & 0.9596 \\  & PPL & 6.06 & 11.98 & 6.44 & 12.28 & 6.24 & \(>\)1000 & 6.11 & 6.63 & 6.85 \\   

Table 2: Results of removing different components in attention. _Sim_ denotes the cosine similarity between original and new positional vectors, and _PPL_ denotes perplexity on RedPajama.

Figure 3: Logarithmic attention maps of TL-RoPE, and TL-NoPE.

Effect of Positional Vectors on Attention SinksPrevious work has found that the initial tokens will be assigned high attention scores, called "_attention sinks_" , which can be clearly observed in Figure 3. However, once the positional vector or positional basis is removed from the keys and queries, the attention scores between initial tokens and other tokens drop significantly for TL-NoPE and TL-RoPE. This finding suggests that **the presence of attention sinks is likely attributed to the inherent positional information in the positional vectors of initial tokens.**

Effect of Positional Vectors on Long-term DecayFor long texts, the attention scores of LLMs often exhibit a long-term decay pattern, which means that the score decreases as the relative distance between tokens increases [7; 6]. However, as shown in Figure 3, when removing the positional vector or positional basis, TL-NoPE fails to exhibit long-term decay. Even with explicit relative positional encoding, the distribution of attention scores in TL-RoPE tends to be smooth after removing decomposed positional vectors. Therefore, **positional vectors also play a crucial role in the long-term decay property of attention scores.**

### Effect of Positional Vectors beyond Context Window

Typically, when dealing with texts that exceed the context window, there are two lines of research, _i.e.,_ direct extrapolation and context window extension. In this section, we aim to investigate the change of positional vectors in these two methods for revealing their effectiveness.

#### 3.3.1 Direct Extrapolation

Relationship Between Positional Vectors and Length Extrapolation AbilityTo examine the impact of positional vectors in direct extrapolation, we reuse the trained model variants in Table 1 to perform inference on samples consisting of 8192 tokens. Further, we analyze the change in PPL score and the maximum cosine similarity between positional vectors within and beyond the context window. As shown in Figure 4 Left, only TL-Window-RoPE and TL-Window-80 demonstrate the length extrapolation ability, maintaining stable PPL across longer texts. These models can preserve the consistency of positional vectors both within and beyond the context window (high similarity in Figure 4 Right). Conversely, the rest models, including those with extrapolated positional encodings or window attention (_e.g.,_ TL-ALBiBi), struggle to generalize to longer contexts. Notably, these models exhibit rapid changes in positional vectors (beyond \(2048\)), diverging from the distributions observed within the context window. Thus, our findings underscore **the critical role of the stability of positional vectors in enhancing the capability for length extrapolation.**

Effect of OOD Positional VectorsBeyond the context window, position vectors are not encountered during training and are out-of-distribution from those vectors within the context window. To explore whether OOD positional vector is a key factor in performance degradation, we select TL-NoPE for evaluation, which does not use explicit positional encodings. First, we compare the attention distribution within and beyond the context window. Figure 5 shows the attention map and scores between initial and rest tokens by averaging all heads of the 5-th layer (similar results in other layers). Once exceeding the context window (\(T=2048\)), the attention distribution in these positions changes sharply, losing the characteristics of attention sinks and long-term decay. Since these properties

Figure 4: **Left**: The average PPL across positions during direct extrapolation. **Right**: The maximum cosine similarity between positional vectors within and beyond context window during extrapolation.

highly depend on the positional vectors within the context window, we speculate that **OOD positional vectors disrupt the original attention distribution**. Besides, we feed the positional vectors of the last layer into the linear projection of the softmax layer to get the logits at different positions. Figure 5 (Right) presents that the logits within the context window are tightly similar while others show different distributions. Thus, **the OOD positional vectors can damage the token prediction probability distribution**, thereby leading to performance degradation.

#### 3.3.2 Context Window Extension

Change of Positional Vectors When Extending Context WindowsTo investigate why context window extension can prevent performance degradation, we analyze the change of positional vectors in two training-free context window extension methods, including dynamic-NTK  for TL-RoPE and attention scaling (\(_{i}_{j}\) multiplied by a scaling factor \(\))  for TL-NoPE. From Figure 6, we can see that after context window extension, positional vectors have undergone interpolation compared to the original ones. Comparing the _Factor_ and _Ratio_ metrics in Table 3, we conclude that **the effective interpolation ratio is close to the expansion factor** (_e.g.,_\(2\) vs \(2.56\)). Besides, as the expansion factor increases, there is a decrease in _Similarity_ and an increase in _PPL_. Therefore, we suspect that imperfect interpolation may be a major reason for the decline in model performance.

   Model & Method & Target Length & Factor & Ratio & Similarity & PPL/\(\)PPL \\   & Attention Scaling (\(=1.2\)) & 4096 & 2 & 2.56 & 0.98 & 8.95/+1.42 \\  & Attention Scaling (\(=1.3\)) & 8192 & 4 & 4.30 & 0.94 & 17.87/+10.34 \\  & Initial Scaling (\(=1.2\)) & 4096 & 2 & 2.38 & 0.97 & 9.82/+2.29 \\  & Initial Scaling (\(=1.3\)) & 8192 & 4 & 4.10 & 0.91 & 32.78/+25.25 \\   & Dynamic NTK & 4096 & 2 & 2.05 & 0.99 & 6.00/-0.02 \\  & Dynamic NTK & 8192 & 4 & 3.75 & 0.96 & 6.78/+0.76 \\   

Table 3: The interpolation results of positional vectors, where _Factor_ (\(=\) Target Length\(/C\)) is the expansion factor of the context window, _Ratio_ is the effective interpolation ratio of positional vectors (detailed in Appendix C), and _Similarity_ is the average cosine similarity between the scaled positional vector and the original most similar positional vector by averaging all layers.

Figure 5: **Left**: Attention map of TL-NoPE. **Middle**: Attention Scores between initial token and others in TL-NoPE. **Right**: Similarity of logits of positional vectors across positions in TL-NoPE.

Figure 6: The average cosine similarity between the scaled and original positional vectors.

Effect of Initial Tokens on Context Window ExtensionSince the initial tokens serve as the anchor for the formation of subsequent positional vectors, we evaluate whether changing the information flow from the initial tokens to the rest tokens can achieve the interpolation effect. To avoid the effect of OOD positional encodings, we follow the attention scaling method on TL-NoPE but only scale the attention logits between the initial tokens and others, denoted as _Initial Scaling_. As shown in Table 3, it can achieve comparable performance and interpolation ratios closer than scaling all attention logits in Attention Scaling (_e.g._, 2.38 vs 2.56), further underscoring that the **interpolation of positional vectors is mainly achieved by adjusting the information flow of anchor tokens.**

## 4 Extending Context Window via Positional Vectors

Inspired by our analysis of the formation of positional vectors and the interpolation of positional vectors when extending the context window, we propose two training-free context window extension methods, _i.e._, **positional vector replacement** and **attention window extension**. The pseudocode of these methods can presented in Appendix E.

### Positional Vector Replacement

In Section 3.2.3, we show that when exceeding the context window, the OOD positional vectors tend to cause the collapse of attention distribution. Further, we observe that context window extension methods can achieve length interpolation of positional vectors and the effective interpolation ratio is close to the expansion factor of the context window. Thus, we propose to replace all the implicitly learned positional vectors with the interpolated ones, called _positional vector replacement_, to avoid the OOD issue in LLMs without positional encodings (NoPE).

Specifically, we linearly interpolate the positional vectors within the context window with an interpolation ratio \(r\) and multiply the interpolated ones with a times \(\)\(( 1)\). In practice, we find that properly increasing the interpolation ratio \(r\) and times \(\) can achieve better effectiveness of interpolation (details are discussed in Appendix D). Owing to the critical role of initial tokes, the positional vectors of the first four tokens remain unchanged, while those of subsequent tokens are replaced with the interpolated vectors. The replaced output \(}_{l,t}\) for each layer can be formulated as:

\[}_{l,t} = _{l,t}-_{l,t}+}_{l,t},\] (5) \[\{}_{l,5},,}_{l,r(C-4)+5}\} = (\{_{l,5},,_{l,C}\}),\] (6)

where \(C\), \(l\), and \(s\) represent the original context window size, replaced layer, and interpolation ratio. Since replacing positional vectors for all layers requires heavy recalculation efforts and the positional information is passed across layers, we only apply the replacement strategy to a single early layer. We find that the 4-th layer is the optimal layer for replacement in TL-NoPE, as shown in Figure 8.

### Attention Window Extension

As discussed in Section 3.2.2, the positional vectors are shaped across layers and windows by the distinct positional information of initial tokens. Inspired by these observations, we propose _attention window extension_, the first training-free length interpolation method for window attention-based LLMs without positional encodings. The core idea is to extend the attention window size to control the formation of positional vectors. When scaling the context window by a ratio, the window size also needs to be extended by the same interpolation ratio \(r\). However, for positions in the extended first window \(\{W+1,,rW\}\), their position vectors are OOD. To avoid this, we follow the attention scaling method  and scale the attention logits with a scaling factor \(\), achieving better interpolation of positional vectors. We define the attention score \(a_{ij}\) between query \(_{i}^{D_{H}}\) and key \(_{j}^{D_{H}}\) for any heads and layers as:

\[a_{ij}=_{i}_{j}/})}{_{z=i -rW}^{i}(_{i}_{z}/})}.\] (7)

### Results on Language Modeling

To assess the effectiveness of our proposed methods, we evaluate language modeling performance on the test set of PG-19 . In line with previous work , we measure PPL across various input lengths (from 2K to 8K) using a sliding window approach. We apply positional vector replacement to TL-NoPE and attention window extension to TL-Window. All the hyper-parameters are selected according to the PPL and the change of positional vectors across layers. For compared baselines, we select Dynamic-NTK  for TL-RoPE and Attention Scaling  for TL-NoPE.

The results are shown in Table 4. First, without interpolation, the PPL increases extremely after beyond the context window (_e.g._, \(>10^{3}\)). When using the positional vector replacement or attention window extension methods, we observe that PPL decreases substantially, showing the effectiveness of our proposed methods. Compared to attention scaling, our attention window extension method successfully extends the context window to 8K tokens with lower PPL. Moreover, our positional vector replacement method achieves similar performance to attention scaling within 6K tokens but shows increased PPL at 8K. We attribute this phenomenon to the decreasing effective interpolation ratio across layers, as shown in Figure 9. Additionally, an increase in PPL with the rising interpolation ratio \(r\) is also observed in both our methods, likely due to imperfect interpolation of positional vectors.

## 5 Related Work

Position Information in TransformersPositional information was crucial in Transformer-based LLMs, to enhance the sequence modeling abilities. The vanilla Transformer introduced absolute positional encodings, using a unique embedding to each position and adding it to the corresponding input embedding . In contrast, relative positional encodings introduced biases based on the relative distance between tokens within attention modules [25; 26; 6; 7]. Besides explicit positional encodings, some work investigated the implicit positional information within hidden states of Transformers. Even without positional encodings, positional information was found in hidden states of Transformer decoders [19; 28; 29]. Besides, prior work decoupled positional basis from hidden states in Transformers and analyzed geometric properties . Our work mainly explores positional information embedded in the hidden states of LLMs, examining the formation and impact of positional vectors, and using it to analyze the mechanism of context window for LLMs.

Extending Context WindowLLMs were often constrained by pre-defined context windows. When processing inputs that exceed these windows, models typically encountered OOD issues, leading to significant performance degradation. To meet the growing demands of long context tasks [30; 31], various methods were proposed to address this limitation and model longer texts, which can be roughly categorized into length extrapolation and context window extension . Length extrapolation techniques aimed to maintain stable PPL regardless of text length by designing specialized positional encodings or window attention mechanisms [6; 17; 18; 15; 14]. Conversely, context window extension methods focused on extending the context window of existing models by adapting positional encodings or temperature hyper-parameters, thereby enlarging the context window with minimal performance loss [13; 12; 16; 20; 11; 10]. This paper bridges the concepts of length extrapolation and context window extension through the lens of positional vectors, enhancing the interpretability of context windows in LLMs.

   Model & Interpolation Method & Factor & 2K & 4K & 6K & 8K \\   & - & - & 10.17 & \(>10^{3}\) & \(>10^{3}\) & \(>10^{3}\) \\   & Dynamic NTK & - & 10.17 & 10.45 & 11.28 & 28.58 \\   & - & - & 11.92 & \(>10^{3}\) & \(>10^{3}\) & \(>10^{3}\) \\   & Attention Scaling & \(=1.2\) & 17.03 & 17.05 & 54.26 & \(>10^{3}\) \\   & Positional Vector Replacement & \(r=2,=1.1\) & 13.54 & 15.58 & - & - \\  & (ours) & \(r=5,=1.3\) & 28.15 & 47.65 & 49.79 & 73.79 \\   & - & - & 12.86 & 713.51 & 660.30 & 660.51 \\   & Attention Window Extension & \(r=2,=1.1\) & 13.70 & 14.10 & - & - \\   & (ours) & \(r=4,=1.2\) & 17.23 & 31.66 & 29.27 & 29.30 \\   

Table 4: Results of language modeling in PG-19. The context window size \(C\) is \(2048\).

Conclusion

In this work, we explored the inner working mechanism of LLMs within and beyond the context window via decomposed positional vectors. We found that the initial tokens initially present different positional information and serve as anchors for shaping the positional vectors of subsequent tokens. Besides, after exceeding the context window, length extrapolation methods maintain the stability of positional vectors, while context window extension methods achieve the interpolation of positional vectors. Based on our observations, we proposed two methods: positional vector replacement and attention window extension, which achieve training-free context window extension for specific LLMs. We believe that positional vectors will serve as an effective tool for analyzing the context window of LLMs and promote the design of better algorithms for extending the context windows of LLMs.

## 7 Limitation

Our work provides an extensive discussion and analysis of the context window through the lens of positional vectors. However, our study is mainly constrained by the use of small-scale LLMs that we trained ourselves, due to the unavailability of existing LLMs with the specific positional encodings and attention patterns required for our experiments. Though some mainstream LLMs are evaluated, these models are all based on RoPE. Furthermore, we have demonstrated the effectiveness of our proposed methods solely on our own models, again limited by the absence of suitable external models. In future work, we aim to seek a broader range of models to validate our findings more comprehensively.