# KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization

Coleman Hooper\({}^{1}\) Sehoon Kim\({}^{1}\) Hiva Mohammadzadeh\({}^{1}\)

**Michael W. Mahoney\({}^{1,2,3}\) Yakun Sophia Shao\({}^{1}\) Kurt Keutzer\({}^{1}\) Amir Gholami\({}^{1,2}\)**

\({}^{1}\)University of California, Berkeley \({}^{2}\)ICSI \({}^{3}\)LBNL

{chooper, sehoonkim, hiva, mahoneymw, yshao, keutzer, amirgh}@berkeley.edu

###### Abstract

LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facilitates low precision KV cache quantization by incorporating several novel methods: (i) _Per-Channel Key Quantization_, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) _Pre-RoPE Key Quantization_, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) _Non-Uniform KV Cache Quantization_, where we derive per-layer sensitivity-weighted non-uniform daturges that better represent the distributions; and (iv) _Per-Vector Dense-and-Sparse Quantization_, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve \(<0.1\) perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to **1 million on a single A100-80GB GPU** and up to **10 million on an 8-GPU system**. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to \(\)1.7\(\) speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model. Code is available at https://github.com/SqueezeAILab/KVQuant.

## 1 Introduction

Large language models (LLMs) have revolutionized many natural language processing (NLP) tasks. In order to improve the capabilities of LLMs, there is significant interest in increasing the context lengths of LLMs. Longer context lengths enable new applications, including long document summarization, retrieval for answering questions about long documents, extended multi-turn applications , and code analysis. To support this pull from applications, there have been significant recent advances in long-context length models in industry , as well as in academia .

Given the importance of LLM workloads, there is strong motivation to improve their inference efficiency. LLM inference with large context lengths can be incredibly resource-intensive; serving LLMs requires high-end GPUs, and the largest LLMs require costly multi-GPU inference setups. When analyzing the computational nature of generative inference with LLMs, it becomes quickly apparent that, for relatively small batch sizes, the computation is memory bound . With the growing divergence between computational speeds and memory speeds, this problem is only going to get worse over time . This makes reducing the memory bottleneck preeminently important. Further analysis shows that the memory bottleneck is strongly related to context size. For shortsequence lengths, the dominant contributor to memory consumption is the weight matrices, and therefore the optimal strategy is to minimize the model size in order to reduce memory consumption as well as bandwidth requirements . However, as shown in Figure 1, the main bottleneck for long sequence lengths is the memory requirements for caching Key and Value (KV) activations throughout inference. This challenge is further exacerbated when one considers batched inference.

It is therefore crucial to develop methods for compressing the KV cache to enable efficient long-sequence length inference. Existing approaches lead to unacceptable accuracy degradation due to the outlier structures in KV cache activations as well as suboptimal bit allocation with existing uniform and non-uniform approaches. In this work, we perform an extensive analysis of KV cache activations in recent LLMs, revealing patterns which can be exploited to enable ultra-low precision quantization with minimal accuracy loss. In particular, we make the following contributions (summarized in Figure 1):

* We find that the Keys exhibit outliers in specific channels before applying RoPE. However, the outlier channel magnitudes become less consistent after applying RoPE, posing a distinct challenge for low precision quantization. We address this by quantizing Keys per-channel before RoPE is applied (see Section 3.1 and Section 3.2).
* We find that existing uniform and non-uniform quantization methods result in sub-optimal quantization signpost placement. Instead, we propose a Non-Uniform Quantization (NUQ) method which considers sensitivity and not just magnitude when quantizing activations. We show that one can apply sensitivity-weighted non-uniform quantization offline on a calibration set to derive accurate datatypes for KV cache quantization (see Section 3.3).
* Even with the above, we find that outlier values in cached KV activations can significantly degrade quantization resolution. Unlike for weights, it is non-trivial to extract outlier values from activations, given the dynamic nature of activations. However, we find that we can efficiently and accurately identify and compress outlier values in order to store them compactly in a separate sparse representation. We also find that per-vector outlier detection outperforms per-matrix outlier detection with no additional memory overhead. By removing only 1% of outliers, we can attain under 0.1 perplexity degradation on both Wikitext-2 and C4 for 3-bit KV cache quantization with the LLaMA, Llama-2, Llama-3, and Mistral models, thereby facilitating accurate inference with 4.8\(\) longer context length.
* We implement custom CUDA kernels to perform activation quantization efficiently during inference, achieving up to \(\)1.7\(\) speedups for Key and Value matrix-vector multiplications for LLaMA-7B at 4-bit precision relative to the fp16 baseline (see Section 3.7 and 4.4). These results demonstrate how our methodology allows for accurate and efficient low-bit KV cache quantization.

Figure 1: Left: Model size versus activation memory size for the LLaMA-7B model with sequence length 512 and 128K. For longer context lengths, the KV cache becomes the dominant memory bottleneck. Memory consumption of model weights and KV cache activations for different LLaMA models with different sequence lengths are provided in Table 7 in Appendix A. Right: Overview of the different components used in KVQuant that result in less than 0.1 perplexity degradation over the fp16 baseline when quantizing the KV cache for the LLaMA-7B model to 3-bit precision. As shown in Table 1, our 3-bit approach results in \(4.8\) reduction in cached activation memory footprint.

Background

### LLM Inference

When inferring a decoder-only LLM, inference proceeds in two distinct phases. In the prefill phase, the model takes in an input prompt, which it processes _in parallel_. During the generation phase, the model then generates the output sequence autoregressively, meaning that each token generation is dependent on all previously generated tokens. As such, for small batch sizes, the generation phase of LLM inference is typically _memory-bandwidth bound_, as the only available parallelism is across different sequences in a given batch. Additionally, during generation, the model needs to store intermediate Key and Value activations at each layer in order to condition generations on previously generated output tokens. Otherwise, we would need to recompute all prior Keys and Values at each timestep, which would be prohibitively expensive. These stored activations are referred to as the _Key-Value (KV) cache_. Throughout this paper, we will capitalize Key and Value to distinguish when we are referring to the KV cache tensors. Assuming a model with \(n\) layers and \(h\) attention heads with dimension \(d\) that is stored using \(e\) bytes per element, the KV cache size for batch size \(b\) and sequence length \(l\) is \(2 n h d e b l\), meaning that it grows linearly with both batch size and sequence length. As shown in Table 7, the KV cache becomes the dominant contributor to memory consumption for longer sequence lengths and larger batch sizes. Note that since each sequence in batched inference depends on separate past context, there is no available batch-level parallelism when loading the cached Keys and Values for their respective computations in batched inference. KV cache loading is therefore always _memory-bandwidth bound_. This motivates pursuing methods to optimally compress the KV cache, even at the expense of a more complex dequantization process.

### LLM Quantization

There have been many prior works on LLM quantization. Several have focused on weight-only quantization for LLMs, due to the greater contribution to memory consumption and runtime for small sequence lengths and batch sizes [21; 9; 17]. Prior works have noted the presence of distinct outliers in both weights and activations [7; 9; 17]. One approach that has been developed to address this outlier issue in the context of weight quantization is dense-and-sparse quantization, where each weight matrix is decomposed into a sparse outlier matrix and a dense low-precision matrix [9; 17]. Prior works have also leveraged non-uniform quantization methods to improve accuracy for the same bit precision by allowing for flexible quantization signpost placement [17; 8]. These approaches have either used a fixed non-uniform datatype such as NormalFloat , or derived quantization signposts using a sensitivity-weighted k-means approach . Appendix B provides a more detailed overview of related work for outlier-aware LLM quantization and non-uniform LLM quantization.

There has also been work on quantizing both weights and activations (including KV cache) [41; 33]. However, there is still a significant perplexity degradation when quantizing KV cache activations to low precision; [34; 44] quantized KV cache activations to 4-bits, but required fine-grained grouping for 4-bit quantization, while still observing some perplexity degradation, and  observed that 3-bit KV cache quantization with fine-grained grouping leads to unacceptable accuracy loss. Other works quantized KV cache activations to 4-bits but required retraining to maintain performance . One concurrent work also explores low precision KV cache quantization in order to enable larger batch size inference by reducing the KV cache size .

### KV Cache Compression

There have also been several prior works on compressing the KV cache. Some of these methods aim to only store important tokens in the KV cache and to evict less important tokens, thereby maintaining low memory usage [25; 43; 11; 20]. Other methods aim to only retrieve a subset of tokens at each step to achieve memory bandwidth savings . In this work, we explore KV cache quantization as an orthogonal direction for compressing the KV cache in order to enable long context inference.

## 3 Method

### Per-Channel Key Quantization

To inform our approach, we first performed a detailed analysis to understand the KV cache distributions. Figure 2 shows sample distributions for the KV cache activations. We observe that the Key matrices tend to have distinct outlier channels, which have larger average magnitudes than other channels; this corroborates previous observations about outlier channels in LLM activations [7; 41]. The Value matrices exhibit both outlier channels as well as outlier tokens (although these outliers are less extreme than the outlier Key channels).

Existing KV cache quantization approaches perform per-token quantization (meaning that the scaling factor and zero-point are shared by elements in the same token) [34; 44]. However, due to the differing average magnitudes between channels, the values within a channel are easier to quantize when grouped together than the values across different channels. As such, to better match the distributions, we investigate _per-channel_ KV cache quantization, meaning that the scaling factor and zero-point are shared by elements in the same channel. By sharing the scaling factor and zero-point along the channel dimension, this will naturally group together values with similar magnitudes, thereby mitigating the impacts of outlier channels on other channels when quantizing to low precision. As outlined in Appendix G, we find that per-channel quantization provides significant accuracy benefits for Keys but not for Values. By leveraging per-channel quantization for Keys and per-token quantization for Values, we observe a 3.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. Note that this can potentially add runtime overhead since the quantization dimension is now misaligned with the reduction dimension for the Keys when performing matrix-vector multiplications. However, we find that we are able to efficiently dequantize Keys and perform the Query-Key matrix-vector multiplication without adding runtime overhead, as shown in Section 4.4. Additionally, as outlined in Section 3.6, per-channel quantization can also be challenging due to the need to recompute scaling factors as tokens are added to the Key cache. We show that we can calibrate offline for scaling factors, thereby avoiding expensive online recomputation.

Per-channel Key quantization was also explored in another concurrent work , which leveraged similar intuition about grouping together large magnitude values in the same channel to minimize quantization error. Their methodology requires fine-grained grouping for per-channel quantization while maintaining a residual subset of the KV cache in fp16. In our work, we demonstrate that by leveraging offline calibration, we can accurately perform per-channel quantization without grouping.

### Pre-RoPE Key Quantization

One issue when quantizing Keys is handling the rotary positional embedding (RoPE), which is applied to Keys and Queries in most public LLMs, including LLaMA and Llama-2 . Given Query and Key vectors \(Q_{m}=W_{q}*x_{m}\) and \(K_{n}=W_{k}*x_{n}\) at positions \(m\) and \(n\) in the sequence, RoPE is applied as position-dependent rotations to each of these vectors to obtain \(_{m}=R_{,m}^{d} Q_{m}\) and

Figure 2: Example distributions of the activation values for Keys pre-RoPE, Keys post-RoPE, and Values for LLaMA-7B on a sample with 2K sequence length from Wikitext-2. We observe several patterns: (i) Keys pre-RoPE exhibit clear outliers in specific channels across different tokens; (ii) after applying RoPE, the distribution becomes less structured and there are less consistent magnitudes for outlier channels (this is expected, as RoPE applies a rotation operation between pairs of channels); and (iii) Values exhibit no fixed outlier pattern with outlier values across channels and tokens.

\(K_{n}=R_{,n}^{d} K_{n}\). This embeds the relative position between a Query and Key vector as an amount of an angle that is a multiple of its position index. When caching Key vectors, we therefore need to either cache \(_{n}\), or else we need to cache \(K_{n}\) and apply \(R_{,n}^{d}\) on-the-fly during inference. The challenge with caching Key vectors after applying this rotation is that it leads to mixing pairs of channels by different amounts for different positions in the sequence, as shown in Appendix C (since it jointly rotates pairs of channels by different angles depending on the position in the sequence). The post-RoPE activation distribution is also shown in Figure 2, demonstrating how the rotation between pairs of channels leads to less consistent channel magnitudes. This makes it harder to quantize Key activation channels which would typically have consistent large-magnitude values. This motivated our investigation into whether we could perform _pre-RoPE_ Key quantization (meaning that we quantize \(K_{n}\)) and then efficiently apply the positional embeddings on-the-fly after dequantization. The benefits of pre-RoPE Key quantization are highlighted in Appendix H, yielding 0.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. To be able to quantize Keys pre-RoPE, we develop a fused kernel to efficiently apply RoPE post-dequantization (the details of this approach will be discussed in Section 3.7).

### nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype

Uniform quantization is suboptimal for KV cache quantization since the Query and Key activations are non-uniform. Additionally, KV cache loading is memory bandwidth bound, regardless of batch size or sequence length, meaning that the dequantization overhead introduced by non-uniform quantization methods is not problematic (since the added computation does not introduce any additional latency). It is therefore desirable to leverage non-uniform quantization methods for KV cache quantization. In , the authors computed non-uniform quantization signposts using a sensitivity-weighted k-means approach. However, this cannot be applied directly to KV cache quantization as the Values are quantized dynamically at runtime, which means that we would need to apply K-means online during inference, and it is also difficult to estimate sensitivity for activations online. We therefore facilitate efficient online non-uniform KV cache quantization by deriving a per-tensor non-uniform datatype offline on calibration data, which is then rescaled per-channel or per-token to accurately represent the key and value distributions. We compute sensitivity-weighted quantization signposts offline on a calibration set prior to inference, while maintaining compatibility with per-vector quantization by separately normalizing each channel to the range \([-1,1]\) prior to deriving the shared datatype. Using the diagonal Fisher information matrix (derived in Appendix D), along with the quantization error for activation \(A\), we formulate the error minimization objective as follows, where \(A\) is flattened to one dimension and where \(N\) is the number of elements from all of the samples in our calibration set:

\[Q(A)^{*}*{arg\,min}_{Q}_{i=1}^{N}_{ii} A_{i}-Q(A_{i})^{2}.\] (1)

We modify the objective in Equation 1 as described in Appendix E in order to apply it using the normalized activation values. We then minimize it offline on a calibration set using a k-means solver in order to obtain the quantization signposts for the non-uniform datatype for each Key or Value layer. Appendix I compares our non-uniform quantization approach with existing uniform and non-uniform quantization baselines , demonstrating how our non-uniform approach provides 0.29 perplexity improvement on Wikitext-2 for LLaMA-7B relative to 3-bit uniform methods. Table 16 in Appendix L shows how computing the required Fisher information for the LLaMA-65B model takes only a few minutes, and how using the k-means solver takes only a few minutes per layer (with the computation for each layer being parallelizable).

### Per-Vector Dense-and-Sparse Quantization

As shown in Figure 4 in Appendix F, for both Keys and Values, the majority of elements are contained within a small percentage of the dynamic range. This means that by leveraging dense-and-sparse quantization, as demonstrated in , in order to isolate a small percentage of numerical outliers, we can restrict the range that we need to represent, thereby allowing us to represent the remaining elements with greater precision. However, when looking at the Key and Value distributions in Figure 2, different channels and tokens have different average magnitudes. Therefore, an element which counts as an outlier in one channel may not be an outlier in another channel (since that channel may have a greater average magnitude), making naive application of dense-and-sparse quantization suboptimal. It is therefore crucial to directly target the outlier values that skew the dynamic range _at the granularity that we are quantizing_ in order to address the values that are exaggerating the range along that particular dimension. In this work, we leverage _per-vector_ dense-and-sparse quantization, where we use a different outlier threshold per-vector (either a separate threshold per-channel for per-channel quantization, or a separate threshold per-token for per-token quantization), rather than a single outlier threshold for each layer.

Note that computing outlier thresholds for per-vector dense-and-sparse quantization poses potential accuracy and efficiency challenges. However, in Section 3.6, we show that we are able to accurately calibrate for per-channel outlier thresholds offline and efficiently compute per-token outlier thresholds online. After determining the upper and lower outlier thresholds, the remaining numbers in the vector are normalized to the range \([-1,1]\), and we then minimize Equation 1 (ignoring outliers) in order to obtain the quantization signposts for the non-uniform datatype for the remaining numbers. Appendix J will demonstrate the benefits of removing a small percentage of numerical outliers and keeping them in full precision, as well as the advantages of per-vector dense-and-sparse quantization over using a single global outlier threshold for each layer. As shown in Figure 1, by removing 1% of numerical outliers using per-vector outlier thresholds, we achieve an additional 0.19 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization, which is within 0.07 perplexity of the fp16 baseline.

### Attention Sink-Aware Quantization

Prior work has demonstrated that after the first few layers in LLMs, the model tends to allocate a large attention score to the first token . This occurs even when the initial token is not semantically important. This phenomenon happens because the model tends to use the inital token as a "sink". In our work, we demonstrate that due to the Attention Sink phenomenon, the model is disproportionately sensitive to quantization error in the first token. By keeping only the first token in fp16, we can attain perplexity benefits, particularly for 2-bit quantization. A similar observation has also been made in another concurrent work . Note that when retaining the first token in fp16, we account for this during the calibration process as well, meaning that we ignore the first token when deriving the nuqX datatype and when calibrating the scaling factors and zero points offline for the Keys. As demonstrated in Appendix K, this approach persistently yields performance benefits, particularly with lower bit widths and without dense-and-sparse quantization.

### Offline Calibration versus Online Computation

A crucial challenge for activation quantization is that we either need to compute statistics on-the-fly (which is potentially expensive) or else we need to use offline calibration data (which potentially has negative accuracy implications). The challenges with computing scaling factors (and zero-points) online versus offline for both Keys and Values are shown in Figure 5 in Appendix L. In per-channel quantization, it is challenging to update scaling factors online since the scaling factors corresponding to each incoming channel would potentially need to be updated whenever a new token is added to the KV cache. It is therefore desirable to be able to compute statistics offline (i.e., using calibration data before running inference). While this can have negative effects on model accuracy, in Appendix L we show that we can effectively calibrate offline for per-channel quantization, obviating the need for online updates of scaling factors for per-channel quantization. For per-token quantization, it is challenging to calibrate for scaling factors offline due to the presence of outlier Value tokens. It is therefore desirable to be able to compute scaling factors and outlier thresholds online for each incoming token. As shown in Appendix L, we can efficiently compute outlier thresholds online per-token by offloading to the CPU. By leveraging custom quantization function implementations for compressing activations, we are able to perform online per-token Value quantization without compromising on performance.

### Kernel Implementation

In order to efficiently perform activation quantization on-the-fly, we leverage dedicated kernel implementations with our 4-bit quantization method for compressing vectors to reduced precision and extracting the sparse outliers, performing matrix-vector multiplications using the compressed vectors, and performing sparse matrix-dense vector multiplications using the sparse outliers. We store the quantized Key and Value matrices as 4-bit elements which are used as indices into lookup tables to recover the corresponding fp16 values. We store the sparse outlier matrices in either Compressed-Sparse Row (CSR) or Compressed-Sparse Column (CSC) format (depending on which aligns better with appending new Key and Value tokens). The kernels for the Key matrix-vector operations apply RoPE on-the-fly in order to support pre-RoPE quantization. More kernel implementation details are provided in Appendix R.

## 4 Results

### Main Evaluation

We used the LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models to evaluate our methodology by measuring perplexity on both Wikitext-2 and C4 [36; 37; 1; 16; 27; 31]. Perplexity has been measured using teacher forcing with the output logits of all input tokens. We compared our method against (i) uniform quantization without grouping (intX), (ii) nonuniform quantization using NormalFloat  without grouping (nfX), as well as (iii) Atom  and FlexGen . Note that Atom and FlexGen use uniform quantization with group sizes of 64 and 128, respectively. All the KVQuant models throughout this experiment section are calibrated using 16 calibration samples of sequence length 2K from the Wikitext-2 training set. See Appendix M for details on our experimental setup, including our methodology for computing KV cache size estimates.

Table 1 shows the results for LLaMA models for the Wikitext-2 dataset. We compared our method with per-token quantization with and without grouping. The baseline configurations used by Atom and FlexGen are included for reference [44; 34]. We find that our method consistently outperforms baseline approaches by an especially large margin with 3-bit and 2-bit quantization. Once we incorporate outliers, we further push the performance of low-precision quantization, achieving 4-bit quantization with less than 0.02 perplexity degradation, 3-bit quantization with under 0.1 perplexity degradation, and 2-bit quantization with under 0.5 perplexity degradation on Wikitext-2, relative to the fp16 baseline, across all models (while attaining 3.7\(\), 4.8\(\), and 6.9\(\) memory savings, respectively).

    &  &  &  &  \\   & PPL & KV Cache (GB) & PPL & KV Cache (GB) & PPL & KV Cache (GB) & PPL & KV Cache (GB) \\   baseline & 5.68 & 64.0 & 5.09 & 100.0 & 4.10 & 195.0 & 3.53 & 320.0 \\  int4 & 5.98 & 16.0 & 5.32 & 25.0 & 4.34 & 48.8 & 3.73 & 80.1 \\ nf4 & 5.87 & 16.0 & 5.23 & 25.0 & 4.25 & 48.8 & 3.63 & 80.1 \\ ATOM-4bit & 5.77 & 16.6 & 5.16 & 26.0 & 4.16 & 50.7 & 3.57 & 83.1 \\ FlexGen-4bit & 5.73 & 17.3 & 5.14 & 27.0 & 4.14 & 52.6 & 3.56 & 86.3 \\ - KVQuant-4bit & 5.72 & 16.0 & 5.13 & 25.0 & 4.13 & 48.8 & 3.55 & 80.0 \\
**KVQuant-4bit-1\%** & **5.69** & 17.3 & **5.10** & 27.0 & **4.11** & 52.7 & **3.54** & **86.5** \\  int3 & 10.87 & 12.0 & 8.69 & 18.8 & 6.82 & 36.6 & 6.37 & 60.1 \\ nf3 & 7.33 & 12.0 & 6.21 & 18.8 & 5.46 & 36.6 & 4.44 & 60.1 \\ ATOM-3bit & 6.17 & 12.6 & 5.47 & 19.7 & 4.44 & 38.4 & 3.78 & 63.0 \\ FlexGen-3bit & 5.93 & 13.2 & 5.29 & 20.6 & 4.26 & 40.2 & 3.66 & 65.9 \\ KVQuant-3bit & 5.87 & 12.0 & 5.25 & 18.8 & 4.25 & 36.6 & 3.63 & 60.0 \\
**KVQuant-3bit & **5.75** & 13.3 & **5.14** & 20.8 & **4.15** & 40.5 & **3.57** & **66.5** \\  int2 & 11779 & 8.0 & 69965 & 12.5 & 1470 & 24.4 & 7272 & 40.1 \\ nf2 & 3210 & 8.0 & 5786 & 12.5 & 2044 & 24.4 & 5367 & 40.1 \\ ATOM-2bit & 37.37 & 8.6 & 41.77 & 13.4 & 16.49 & 26.1 & 13.63 & 42.8 \\ FlexGen-2bit & 11.09 & 9.1 & 9.84 & 14.3 & 6.60 & 27.8 & 5.54 & 45.6 \\  KVQuant-2bit & 7.23 & 8.0 & 5.82 & 12.5 & 4.87 & 24.4 & 4.03 & 40.0 \\  KVQuant-2bit-1\%** & **6.01** & **9.3** & **5.36** & **14.5** & **4.35** & **28.3** & **3.70** & **46.5** \\   

Table 1: Evaluation of our method for different models using the perplexity (PPL) on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. KV cache sizes are estimated assuming a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention Sink-Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Table 18 in Appendix O demonstrates a full evaluation on all LLaMA, Llama-2, Llama-3, and Mistral models.

### Long Context Length Evaluation

**Perplexity Evaluation.** We evaluated long context length performance using the LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation ) as well as the Llama-2-70B-32K LongLoRA model . For evaluating performance on longer context lengths, we first evaluated perplexity on Wikitext-2 using larger amounts of input context, as shown in Figure 3. The results demonstrate how our method maintains accuracy even for longer amounts of input context, thereby enabling efficient and accurate long sequence length inference.

**Passkey Retrieval Evaluation.** We also evaluated the performance of our quantization method on passkey retrieval to assess the model's ability to use its context. Passkey retrieval involves evaluating the model's capacity to locate specific information in long texts , and this can be used to effectively measure the maximum distance over which a token can attend during the inference stage. We used the passkey evaluation framework from  (which is based on the methodology from ) to evaluate retrieval performance. The passkey retrieval results are provided in Table 2, demonstrating how our method is able to maintain the retrieval performance for long context length models. We also include comparisons with the passkey retrieval when using KIVI for the LLaMA-2-7B-32K model , demonstrating that our approach can attain higher retrieval rate for the same compression level. Our improved performance on retrieval tasks can be attributed to our improved representation of all tokens equally. This approach differs from KIVI, which preserves a local window of residual tokens in fp16. Therefore, while KIVI is effective at representing the tail part of the context, it may provide less benefit for tasks requiring the utilization of the full context window.

**LongBench Evaluation.** Table 3 shows evaluation on LongBench  for the LLaMA-2-7B-32K model. LongBench contains a suite of long-context length evaluation benchmarks including QA tasks, summarization, and few-shot learning . The max input context length is set at 31500, and results using KIVI are also included for reference . Our results demonstrate that our 3-bit

  
**Model** & **Method** & **2K** & **4K** & **8K** & **16K** & **32K** & **Avg. Bit Width** \\    & fp16 & 1 & 1 & 1 & 1 & 1 & 16 \\  & KIVI-2-gs32+128 & 0.76 & 0.72 & 0.72 & 0.68 & 0.7 & 3.05 \\  & nu4-18 & 1 & 1 & 1 & 1 & 1 & 4.33 \\  & nu3-15 & 0.98 & 1 & 1 & 1 & 1 & 3.33 \\  & nu2-15 & 1 & 1 & 0.98 & 1 & 1 & 2.33 \\   & fp16 & 1 & 1 & 1 & 1 & 1 & 16 \\  & nu4-15 & 1 & 1 & 1 & 1 & 1 & 4.35 \\  & nu3-15 & 1 & 1 & 1 & 1 & 1 & 3.35 \\  & nu2-15 & 0.98 & 0.98 & 0.96 & 1 & 0.74 & 2.35 \\   

Table 2: _Passkey retrieval results across different context lengths for the LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation ) as well as the Llama-2-70B-32K LongLoRA model . The values reported are the success rate for retrieving the passkey, computed over 50 samples. We also include comparisons with KIVI for reference, using the 2-bit configuration with group size of 32 and 128-element fp16 residual . Average bit widths are estimated for each approach assuming 32K context length. Note that the open-source code for running KIVI with LLaMA does not support grouped-query attention, so we did not include comparisons with KIVI for Llama-2-70B-32K._

Figure 3: _Perplexity results for the LLaMA-2-7B-32K model  as well as the Llama-2-70B-32K LongLoRA model  on the Wikitext-2 dataset, evaluated using different sequence lengths._model can attain minimal degradation relative to the fp16 baseline, outperforming KIVI for a similar compression level.

**RULER Evaluation.** Finally in Table 4, we provide evaluation of KVQuant and KIVI on the RULER benchmark suite  using LLaMA-2-7B-32K. As can be seen in the table, 3-bit KVQuant achieves 14% better score against KIVI with a similar average bit-width. Furthermore, our 2-bit KVQuant achieves similar accuracy to KIVI with 1.5\(\) smaller bit-width.

### Joint Weight and KV Cache Quantization

Table 5 provides results for our KV cache quantization method when the weights are also quantized using the methodology in SqueezeLLM . We observe minimal perplexity degradation when leveraging our KV cache quantization approach, even when weights are also quantized to reduced precision. In particular, we observe small 0.02 and 0.1 perplexity degradation of 4-bit and 3-bit weight-only quantization, respectively, when quantizing the KV cache using nuq4-1% for the LLaMA-7B and LLaMA-13B models. These results demonstrate how our method is compatible with existing weight-only quantization methods.

### Performance Analysis and Memory Savings

Table 6 shows kernel benchmarking results using a batch size of 1 for the 4-bit dense-and-sparse compression and matrix-vector kernel implementations. We show results across different sequence lengths to assess the performance of the kernels at different points during generation. We report latency benchmarked on an A6000 GPU. The results show that for the Key and Value multiplications, we can achieve 1.2-1.6\(\) and 1.3-1.7\(\) latency savings, respectively, relative to the baseline. We have integrated these kernels into an end-to-end generation pipeline that is able to compress activations dynamically during inference, thereby achieving significant memory savings and allowing for either larger batch sizes or longer sequence lengths.

Appendix A highlights the benefits of KVQuant in supporting longer context lengths through reducing KV cache memory footprint. As shown in Table 8 in Appendix A, our nuq2 method provides 8\(\) KV cache compression and enables serving the quantized LLaMA-7B model with a context length of **1M tokens** on a single A100 GPU, as well as enabling serving the LLaMA-7B model with **10M context length** on an 8-GPU system. Our results show little degradation compared to baseline fp16 inference

    &  &  &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & & & & & & \\  f916 Baseline & 16 & 100 & 99.8 & 98.6 & 94 & 68.2 & 11 & 55.95 & 64.5 & 37.88 & 9.64 & 30.4 & 31.6 & 31.6 & 56.40 \\  KIVI-2-g32+128 & 3.05 & 76 & 85.6 & 59.6 & 72.6 & 11.4 & 0 & 34.7 & 46.45 & 39.6 & 8.26 & **30.53** & 24.8 & 27.6 & 39.78 \\ KVQuant-3bit-17\% & 3.33 & **99.8** & **98.8** & **95.2** & **92.8** & **61.6** & **6.4** & **47.5** & **54.45** & **41.04** & **8.52** & 29.33 & **31.0** & **31.0** & **53.65** \\  KVQuant-2bit-17\% & 2.33 & 95.4 & 86.8 & 49.8 & 73.6 & 23.4 & 0 & 16.65 & 22.95 & 22.52 & 5.14 & 24.0 & 26.4 & 28.4 & 36.54 \\   

Table 4: _RULER evaluation results for the LLaMA-2-7B-32K model with KVQuant quantization methods. We report accuracy across RULER tasks, comparing our KVQuant configurations to baseline and KIVI approaches. A maximum context length of 32K is used for evaluation. Our results show that our method retains baseline accuracy even with aggressive quantization and pruning._

    &  &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & & & & & & \\  fp16 Baseline & 16 & 17.96 & 10.51 & 33.43 & 12.55 & 12.53 & 6.19 & 29.65 & 16.99 & 22.15 & 71 & 87.79 & 43.97 & 59.99 & 62.14 & 23 & 1.50 & 31.96 \\  KIVI-2-g832+128 & 3.17 & **19.25** & 10.66 & 24.78 & **11.19** & **6.38** & 27.05 & 16.36 & 23.37 & **71** & 80.80 & 43.93 & 57.74 & 60.61 & 13.58 & 1.50 & 30.04 \\  KIVQuant-3bit-17\% & 3.33 & 18.87 & **13.67** & **10.93** & 12.07 & **12.55** & 6.25 & **27.10** & **16.53** & **16.54** & **71** & **87.55** & **43.95** & **59.59** & **61.52** & **19.5** & **17.5** & **31.21** \\   

Table 3: _LongBench evaluation for the LLaMA-2-7B-32K model using KVQuant-3bit-1%. Comparisons with KIVI are included for reference, using the configuration with group size of 32 and 128-element fp16 residual . Average bit widths are estimated for each approach assuming 12.2K context length, which was the average number of tokens across all tasks._while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference.

## 5 Conclusion

As context lengths in LLMs increase, the KV cache activations surface as the dominant contributor to memory consumption. Quantization is a promising approach to reduce the size of KV cache activations, but prior solutions failed to represent activations accurately in ultra-low precisions, such as sub-4-bit. In contrast, we achieve accurate ultra-low precision KV cache quantization. By quantizing Keys per-channel before applying RoPE, we are able to better match the outlier distribution and mitigate the impacts of RoPE on quantization (due to it mixing pairs of channels which may have different average magnitudes). We use non-uniform quantization to better allocate the small number of quantization signposts at low precision. We observe significant accuracy improvements when employing dense-and-sparse quantization, particularly when detecting outliers at the same granularity as we compute quantization scaling factors. Crucially, we demonstrate that we can perform accurate calibration offline for Keys, as well as efficient online scaling factor and outlier threshold computation for Values. By leveraging these methods, we are able to enable accurate low-precision activation quantization, achieving 4.8x compression (nuq3-1% outliers) with only 0.1 perplexity degradation across different LLMa, Llama-2, Llama-3, and Mistral models. Our methodology therefore supports inferring the LLMa-7B model with a context length of **10M** on an 8-GPU serving system. Through our efficient kernel implementation, we are able to show improved latency relative to the fp16 baseline, demonstrating how our method allows for improved latency in addition to the memory savings.