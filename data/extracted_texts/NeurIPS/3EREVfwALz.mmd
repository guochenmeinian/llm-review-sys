# Multiclass Transductive Online Learning

Steve Hanneke

Department of Computer Science

Purdue University

West Lafayette, IN 47907

steve.hanneke@gmail.com&Vinod Raman

Department of Statistics

University of Michigan

Ann Arbor, MI 48104

vkraman@umich.edu&Amirreza Shaeri

Department of Computer Science

Purdue University

West Lafayette, IN 47907

amirreza.shaeiri@gmail.com&Unqiue Subedi

Department of Statistics

University of Michigan

Ann Arbor, MI 48104

subedi@umich.edu

###### Abstract

We consider the problem of multiclass transductive online learning when the number of labels can be unbounded. Previous works by Ben-David et al. (1997) and Hanneke et al. (2023) only consider the case of binary and finite label spaces, respectively. The latter work determined that their techniques fail to extend to the case of unbounded label spaces, and they pose the question of characterizing the optimal mistake bound for unbounded label spaces. We answer this question by showing that a new dimension, termed the Level-constrained Littlestone dimension, characterizes online learnability in this setting. Along the way, we show that the trichotomy of possible minimax rates of the expected number of mistakes established by Hanneke et al. (2023) for finite label spaces in the realizable setting continues to hold even when the label space is unbounded. In particular, if the learner plays for \(T\) rounds, its minimax expected number of mistakes can only grow like \((T)\), \(( T)\), or \((1)\). To prove this result, we give another combinatorial dimension, termed the Level-constrained Branching dimension, and show that its finiteness characterizes constant minimax expected mistake-bounds. The trichotomy is then determined by a combination of the Level-constrained Littlestone and Branching dimensions. Quantitatively, our upper bounds improve upon existing multiclass upper bounds in Hanneke et al. (2023) by removing the dependence on the label set size. In doing so, we explicitly construct learning algorithms that can handle extremely large or unbounded label spaces. A key and novel component of our algorithm is a new notion of shattering that exploits the sequential nature of transductive online learning. Finally, we complete our results by proving expected regret bounds in the agnostic setting, extending the result of Hanneke et al. (2023).

## 1 Introduction

Imagine you are a famous musician who has released \(K\) songs. You are now on tour visiting \(T\) cities worldwide based on the pre-specified plan, each with unique musical preferences that you have some understanding of. At each city, you can perform only one song in your concert, and following each performance, the audience provides feedback indicating their preferred song from your repertoire. Your goal is to select the song that aligns with the majority's taste in eachcity to maximize satisfaction. How can you effectively select songs to ensure the highest audience satisfaction across most cities having minimal assumptions?

The above example and similar real-world situations, where entities operate according to a possibly adversarially chosen pre-specified schedule, can be formulated in a framework called _Multiclass Transductive Online Learning_. Formally, in this setting, an adversary plays a repeated game against the learner over some \(T\) rounds. Before the game begins, the adversary selects a sequence of \(T\) instances \((x_{1},,x_{T})^{T}\) from some non-empty instance space \(\) (e.g. images) and reveals it to the learner. Subsequently, during each round \(t\{1,,T\}\), the learner predicts a label \(_{t}\) from some non-empty label space \(\) (e.g. categories of images), the adversary reveals the true label \(y_{t}\), and the learner suffers the 0-1 loss, namely \(\{_{t} y_{t}\}\). Importantly, the label space \(\) is not required even to be countable; we assume only standard measure theoretic properties for it. Following the well-established frameworks in learning theory, given a concept class \(^{}\) of functions \(c:\), the goal of the learner is to minimize the number of mistakes relative to the best-fixed concept in \(\). If there exists \(c\) such that \(c(x_{t})=y_{t}\) for all \(t\{1,,T\}\), we say we are in the realizable setting, and otherwise in the agnostic setting. We briefly note that if the learner's predictions are randomized, we focus on the expected value of the mentioned objective.

In this paper, our main contribution is algorithmically answering the following question in the multiclass transductive online learning framework:

_Given a concept class \(^{}\), what is the minimum expected number of mistakes achievable by a learner against any realizable adversary?_

For the special case of binary classification (\(||=2\)), this question was first considered by Ben-David et al. (1997) and then later fully resolved by Hanneke et al. (2023). Additionally, Hanneke et al. (2023) considered the case where \(||>2\), but did not resolve this question when \(\) is unbounded. In fact, the bounds by Hanneke et al. (2023) break down even when \(|| 2^{T}\). As a result, Hanneke et al. (2023) posed the characterization of the minimax expected number of mistakes in the multiclass setting with an infinite label set as an open question, which we resolve in this paper.

### Online Learning and Multiclass Classification

In this work, we study _transductive_ online learning framework, where the adversary reveals the entire sequence of instances \((x_{1},,x_{T})\) to the learner before the game begins. In the _traditional_ online learning framework, the sequence of instances \((x_{1},,x_{T})\) are revealed to the learner sequentially, one at a time. That is, on round \(t\{1,,T\}\), the learner would have only observed \(x_{1},,x_{t}\). The celebrated work of Littlestone (1988) introduced this framework for binary classification and quantified the best achievable number of mistakes against any realizable adversary for a concept class \(\{0,1\}^{}\) in terms of a combinatorial parameter called the Littlestone dimension. Later, the work of Ben-David et al. (2009) showed that the Littlestone dimension of a concept class \(\{0,1\}^{}\) continues to quantify the expected relative mistakes (i.e expected regret) for the mentioned framework in the more general agnostic setting. More recently, Daniely et al. (2012) and Hanneke et al. (2023) extended these results to multiclass online learning in the realizable and agnostic settings, respectively. See Section A for more details.

In traditional online classification, there are two sources of uncertainty: one associated with the sequence of instances, and the other with respect to the true labels. Ben-David et al. (1997) initiated the study of transductive online classification with the aim of understanding how exclusively label uncertainty impacts the optimal number of mistakes. Furthermore, removing the uncertainty with respect to the instances can significantly reduce the optimal number of mistakes. For example, for the concept class of halfspaces in the realizable setting, the optimal number of mistakes grows linearly with the time horizon \(T\) in the traditional online binary classification framework, while only growing as \(( T)\) in the transductive online binary classification framework. So, it is natural to reduce the optimal number of mistakes or extend learnable classes whenever we have additional assumptions. Notably, Ben-David et al. (1997) initially called this setting "offline learning", but it was later renamed "Transductive Online Learning" by Hanneke et al. (2023) due to its close resemblance to _transductive_ PAC learning (Vapnik and Chervonenkis, 1974; Vapnik, 1982; Vapnik, 1988). See Section A for more details.

While Ben-David et al. (1997) and Hanneke et al. (2023) mainly focused on binary classification, in this work, we focus on the more general multiclass classification setting. Natarajan and Tadepalli, Natarajan  and Daniely et al.  initiated the study of multiclass prediction within the foundational PAC framework and traditional online framework, respectively. More recently, following the work by [Brukhim et al., 2021], there has been a growing interest in understanding multiclass learning when the size of the label space is unbounded, including Hanneke et al. , Raman et al. . This interest is driven by several motivations. Firstly, guarantees for the multiclass setting should not inherently depend on the number of labels, even when it is finite. Secondly, in mathematics, concepts involving infinities often provide cleaner insights. Thirdly, insights from this problem might also advance understanding of real-valued regression problems [Attias et al., 2023]. Finally, on a practical front, many crucial machine learning tasks involve classification into extremely large label spaces. For instance, in image object recognition, the number of classes corresponds to the variety of recognizable objects, and in language models, the class count expands with the dictionary size. See Section A for more details.

### Main Results and Techniques

In the following subsection, we present an overview of our main findings along with a summary of our proof techniques.

#### 1.2.1 Realizable Setting

In the realizable setting, we assume that the sequence of labeled instances \((x_{1},y_{1}),,(x_{T},y_{T})\), played by the adversary, is consistent with at least one concept in \(\). Here, our objective is to minimize the well-known notion of the expected number of mistakes. We provide upper and lower bounds on the best achievable worst-case expected number of mistakes by the learner as a function of \(T\) and \(\), which we denote by \(^{*}(T,)\).

Hanneke et al.  established a trichotomy of rates in the case of _binary_ classification. That is, for every \(\{0,1\}^{}\), we have that \(^{*}(T,)\) can only grow like \((T)\), \(( T)\), or \((1)\); where the Littlestone and Vapnik-Chervonenkis (VC) dimensions of \(\) characterize the possible rate. In this work, we extend this trichotomy to the multiclass classification setting, even when \(\) is unbounded. To do so, we introduce two new combinatorial parameters, termed the Level-constrained Littlestone dimension and Level-constrained Branching dimension.

To define the Level-constrained Littlestone dimension, we first need to define the Level-constrained Littlestone tree. A Level-constrained Littlestone tree is a Littlestone tree with the additional requirement that the same instance has to label all the internal nodes across a given level. Then, the Level-constrained Littlestone dimension is just the largest natural number \(d\), such that there exists a shattered Level-constrained Littlestone tree \(T\) of depth \(d\). To define the Level-constrained Branching dimension, we first need to define the Level-constrained Branching tree. The Level-constrained Branching tree is a Level-constrained Littlestone tree without the restriction that the labels on the two outgoing edges are distinct. Then, the Level-constrained Branching dimension is then the smallest natural number \(d\) such that for every shattered Level-constrained Branching tree \(T\), there exists a path down \(T\) such that the number of nodes whose outgoing edges are labeled by different elements of \(\) is at most \(d\). The Level-constrained Littlestone dimension reduces to the VC dimension when \(||=2\). Additionally, the finiteness of the Level-constrained Branching and Littlestone dimension coincide when \(||=2\). Finally, we note that the Level-constrained Branching dimension is exactly equal to the notion of rank in the work of Ben-David et al. . However, we believe it is simpler to understated. Using the Level-constrained Littlestone and Branching dimension, we establish the following trichotomy.

**Theorem 1**.: _(Trichotomy) Let \(^{}\) be a concept class. Then, we have:_

\[^{*}(T,)(1),&( )<.\\ ( T)&()<( )=.\\ (T),&()=.\]

_Here, \(()\) is Level-constrained Branching dimension, and \(()\) is Level-constrained Littlestone dimension defined in Section 2._

To prove the \(O( T)\) upper bound for binary online classification, Hanneke et al.  run the Halving algorithm on the projection of \(\) onto \(x_{1},...,x_{T}\) and use the Sauer-Shelah-Perles (SSP)lemma to bound the size of this projection by \(O(T^{()})\). However, this approach is not applicable when \(\) is unbounded. For example, when \(=\{x n\,:\,n\}\) is the set of all constant functions over \(\), the size of the projection of \(\) onto even a single \(x\) is infinity. Moreover, the mentioned class can be learned with at most one number of mistakes. Thus, fundamentally new techniques are required. To this end, we define a new notion of shattering which makes it possible to apply an analog of the Halving algorithm. Additionally, while the proof of the \(O(1)\) upper bound in Hanneke et al. (2023b) follows immediately from the guarantee of Standard Optimal Algorithm (SOA) by Littlestone (1988), our \(O(1)\) upper bound in terms of the Level-constrained Branching dimension requires a modification of the SOA. We complement our results by presenting matching lower bounds. See Section 3 for more details.

In Section F, we provide a comprehensive comparison between our dimensions and existing multiclass combinatorial complexity parameters.

#### 1.2.2 Agnostic Setting

In the agnostic setting, we make no assumptions about the sequence \((x_{1},y_{1}),(x_{2},y_{2}),,\)\((x_{T},y_{T})\) played by the adversary. Here, our focus shifts to the well-established notion of expected regret, which compares the expected number of mistakes made by the algorithm to that made by the best concept in the concept class over the sequence. As in the realizable setting, we aim to establish both upper and lower bounds on the optimal worst-case expected regret achievable by the learner, expressed as a function of \(T\) and the concept class \(\), denoted by \(^{}(T,)\).

The prior work by Hanneke et al. (2023b) showed that in the case of binary classification, \(^{}(T,)\) is \((()\,T})\) whenever \(()<\) and \((T)\) otherwise, where \(\) hides logarithmic factors in \(T\). Using the Level-constrained Littlestone dimension in hand, we extend these results to multiclass classification.

**Theorem 2**.: _For every concept class \(^{}\) and \(T()\), we have the following:_

\[()}{8}}\,^{}(T, )\,()}\,()} ,\]

_where \(()\) is Level-constrained Littlestone dimension defined in Section 2._

Our results in the agnostic setting can be proved using core ideas in the proof of the agnostic results from Ben-David et al. (2009); Hanneke et al. (2023a), and Hanneke et al. (2023b). See Section E for more details.

## 2 Preliminaries

### Notation

Let \(\) denote an example space and \(\) denote the label space. We make no assumptions about \(\), so it can be unbounded and even uncountable (e.g. \(=\)). Following the work of Hanneke et al. (2023a), if we consider randomized learning algorithms, the associated \(\)-algebra is of little consequence, except that singleton sets \(\{y\}\) should be measurable. Let \(^{}\) denote a concept class. We abbreviate a sequence \(z_{1},...,z_{T}\) by \(z_{1:T}\). Moreover, we also define \(z_{<t}:=(z_{1},,z_{t-1})\) and \(z_{ t}:=(z_{1},,z_{t})\). Finally for \(n\), we let \([n]:=\{1,...,n\}\).

### Transductive Online Classification

In the transductive online classification setting, a learner \(\) plays a repeated game against an adversary over \(T\) rounds. Before the game begins, the adversary picks a sequence of labeled instances \((x_{1},y_{1}),...,(x_{T},y_{T})()^{T}\) and reveals \(x_{1:T}\) to the learner. Then, in each round \(t[T]\), using \(x_{1:T}\) and \(y_{1:t-1}\), the learner makes a potentially randomized prediction \((x_{t})\). Finally, the adversary reveals the true label \(y_{t}\), and the learner suffers the loss \(\{(x_{t}) y_{t}\}\). Given a concept class \(^{}\), the goal of the learner is to output predictions such that its _expected regret_,

\[_{}(T,):=_{(x_{1},y_{1}),,(x_{T},y_{T })}[_{t=1}^{T}\{(x_{t})  y_{t}\}]-_{c}_{t=1}^{T}\{c(x_{t})  y_{t}\},\]is small. Moreover, we define \(^{}(T,):=_{A}_{}(T,)\), where the infimum is taken over all transductive online algorithms. We say that a concept class is transductive online learnable in the agnostic setting if \(^{}(T,)=o(T)\).

If the learner is guaranteed to observe a sequence of examples labeled by some concept \(c\), then we say we are in the realizable setting, and the goal of the learner is to minimize its _expected cumulative mistakes_

\[_{}(T,):=_{c}\;_{x_{1:T} }[_{t=1}^{T}\{(x_{t}) c(x_{t})\} ].\]

Similarly, we define \(^{}(T,):=_{}_{}( T,)\), and an analogous definition of transductive online learnability in the realizable setting holds.

### Combinatorial Dimensions

Combinatorial dimensions play an important role in providing a tight quantitative characterization of learnability in learning theory. In this section, we review existing combinatorial dimension in online classification and propose two new dimensions that help us establish the minimax rates for transductive online classification. We start by defining the Littlestone dimension which characterizes multiclass online learnability.

**Definition 1** (Littlestone dimension).: _The Littlestone dimension of \(\), denoted \(()\), is the largest \(d\) such that there exists sequences of functions \(\{X_{t}\}_{t=1}^{d}\) where \(X_{t}:\{0,1\}^{t-1}\) and \(\{Y_{t}\}_{t=1}^{d}\) where \(Y_{t}:\{0,1\}^{t}\) such that for every \(\{0,1\}^{d}\), the following holds:_

1. \(Y_{t}((_{<t},0)) Y_{t}((_{<t},1))\) _for all_ \(t[d]\)_._
2. \( c_{}\) _such that_ \(c_{}(X_{t}(_{<t}))=Y_{t}(_{ t})\) _for all_ \(t[d]\)_._

_If for every \(d\), there exists sequences \(\{X_{t}\}_{t=1}^{d}\) and \(\{Y_{t}\}_{t=1}^{d}\) satisfying (i) and (ii), we let \(()=\)._

On the other hand, in this paper, we show that a different dimension, termed the Level-constrained Littlestone dimension, characterizes transductive online classification.

**Definition 2** (Level-constrained Littlestone dimension).: _The Level-constrained Littlestone dimension of \(\), denoted \(()\), is the largest \(d\) such that there exists a sequence of instances \(x_{1},..,x_{d}^{d}\) and a sequence of functions \(\{Y_{t}\}_{t=1}^{d}\) where \(Y_{t}:\{0,1\}^{t}\), such that for every \(\{0,1\}^{d}\), the following holds:_

1. \(Y_{t}((_{<t},0)) Y_{t}((_{<t},1))\) _for all_ \(t[d]\)_._
2. \( c_{}\) _such that_ \(c_{}(x_{t})=Y_{t}(_{ t})\) _for all_ \(t[d]\)_._

_If for every \(d\), there exist sequences \(\{x_{t}\}_{t=1}^{d}\) and \(\{Y_{t}\}_{t=1}^{d}\) satisfying (i) and (ii), we let \(()=\)._

The Littlestone and Level-constrained Littlestone dimensions can also be defined in terms of complete binary trees. A Littlestone tree \(\) of depth \(d\) is a complete binary tree of depth \(d\) where the internal nodes are labeled by elements of \(\) and for every internal node, its two outgoing edges are labeled by distinct elements in \(\). Such a tree is _shattered_ by \(\) if for every root-to-leaf path \(\{0,1\}^{d}\), there exists a concept \(c_{}\) consistent with the sequence of instance-label pairs obtained by traversing down \(\) along \(\). The Littlestone dimension is then the largest \(d\) for which there exists a shattered Littlestone tree of depth \(d\). From this perspective, the functions \(\{X_{t}\}_{t=1}^{d}\) and \(\{Y_{t}\}_{t=1}^{d}\) in Definition 1 provide the labels on the internal nodes and the outgoing edges of \(\) respectively. Analogously, a Level-constrained Littlestone tree is simply a Littlestone tree with the additional requirement that the instances labeling the internal nodes are the same across each level. In Definition 2, \(x_{1}\) labels all the internal nodes on level one, \(x_{2}\) labels all the internal nodes on level two, and so forth. The functions \(\{Y_{t}\}_{t=1}^{d}\) provide the labels on the outgoing edges of a Level-constrained Littlestone tree. Then, the Level-constrained Littlestone dimension is the largest \(d\) for which there exists a shattered Level-constrained Littlestone tree of depth \(d\). We will use the function-based and tree-based definitions of these dimensions interchangeably.

Moreover, we show that the Level-constrained Branching dimension characterizes when constant minimax rates are possible in transductive online classification.

**Definition 3** (Level-constrained Branching dimension).: _The Level-constrained Branching dimension of \(\), denoted \(()\), is the smallest \(p\) such that for every \(d\), every sequence of instances \(x_{1},..,x_{d}^{d}\), and every sequence of functions \(\{Y_{t}\}_{t=1}^{d}\) where \(Y_{t}:\{0,1\}^{t}\):_

\[\{0,1\}^{d}, c_{}c_{}(x_{t})=Y_{t}(_{ t})t[d]\]

\[*{arg\,min}_{\{0,1\}^{d}}_{t=1}^{d} {1}\{Y_{t}((_{<t},0)) Y_{t}((_{<t},1))\} p.\]

_If no such \(p\) exist, we let \(()=\)._

For a given path \(\{0,1\}^{d}\), we refer to \(_{t=1}^{d}\{Y_{t}((_{<t},0)) Y_{t}((_{<t},1))\}\) as the _branching factor_ of the path. In terms of trees, a Level-constrained Branching tree is a Level-constrained Littlestone tree without the restriction that the labels on the outgoing edges of any internal node need to be distinct. Given a path in such a tree, the branching factor of a path counts the number of nodes in the path whose two outgoing edges are labeled by distinct labels in \(\). Finally, the Level-constrained Branching dimension can be equivalently defined as the smallest \(p\) such that every _shattered_ Level-constrained Branching tree \(\) of depth \(d\) must have at least one path with branching factor at most \(p\).

The following proposition, whose proof is in Appendix B, establishes the relationship between the three dimensions.

**Proposition 1**.: _For every \(^{}\), we have that \(()()()\)._

We also compare our dimensions to other existing dimensions in multiclass learning in Section F.

## 3 A Trichotomy in the Realizable Setting

We start by establishing upper and lower bounds on the minimax expected number of mistakes in the realizable setting in terms of the Level-constrained Littlestone dimension and the Level-constrained Branching dimension.

**Theorem 3** (Mistake bound).: _For every concept class \(^{}\), we have_

\[\,(), T [()=]},T}\, \,^{*}(T,)\,\,\{(),()\,(()}),T\}.\]

One can trivially upper bound \(^{*}(T,)\) by \(()\). However, by Proposition 1, our upper bound in terms of \(()\) is sharper. We can also infer from the proof in Section 3.3 that when \(T\) is large enough (namely \(T 2^{()}\)), the lower bound in the realizable setting is also \(()}{2}\).

Given Theorem 3, we immediately infer a trichotomy in minimax rates.

**Corollary 1** (Trichotomy).: _For every concept class \(^{}\), we have_

\[^{*}(T,)=(1),&( )<.\\ ( T)&()<( )=.\\ (T),&()=.\]

Proof.: (of Corollary 1) When \(()<\), Theorem 3 gives that \(\,()^{*}(T,) ()\) for \(T()\). When \(()=\) but \(()<\), Theorem 3 gives that \(\, T\,\,^{*}(T,) ()\!(()})\) for \( T()\). Finally, when \(()=\), Theorem 3 gives that \(\,\,^{*}(T,) T\). 

The remainder of this Section is dedicated to proving Theorem 3. The proof of the lowerbound \(()/2\) follows from standard techniques, so we defer it to Appendix C.

### Proof of Upperbound \(()\)

Proof.: Fix \(n\), a sequence of instances \(x_{1:n}:=(x_{1},,x_{n})^{n}\), a sequence of functions \(Y_{1:n}=(Y_{1},,Y_{n})\) such that \(Y_{t}:\{0,1\}^{n}\), and set of concepts \(V\). If \(\{0,1\}^{n}\), there exists \(c_{} V\) such that \(c_{}(x_{t})=Y_{t}(_{ t})\) for all \(t[n]\), then define \((V,x_{1:n},Y_{1:n}):=_{\{0,1\}^{n}}_{t=1}^{n} \{Y_{t}((_{<t},0)) Y_{t}((_{<t},1)).\) Otherwise, define \((V,x_{1:n},Y_{1:n}):=0\). Recall that we can represent \(x_{1:n}\) and \(Y_{1:n}\) with level-constrained trees \(\) of depth \(n\). With the tree representation, \((V,x_{1:n},Y_{1:n}):=0\) if \(V\) does not shatter \(\). If \(\) is shattered by \(V\), then \((V,x_{1:n},Y_{1:n})\) is the minimum branching factor across all the root-to-leaf paths in \(\). Recall that the branching factor of a path is the number of nodes in this path whose left and right outgoing edges are labeled by two distinct elements of \(\). In this proof, we work with an instance-dependent complexity measure of \(V\) defined as

\[(V,x_{1:n}):=_{Y_{1:n}}(V,x_{1:n},Y_{1:n}).\]

We now define a learning algorithm that obtains the claimed mistake bound of \(()\). Fix a time horizon \(T\), and let \(x_{1:T}=(x_{1},x_{2},,x_{T})\) denote the sequence of instances revealed by the adversary. Initialize \(V_{1}:=\). For every \(t\{1,,T\}\), if we have \(\{c(x_{t}):c V_{t}\}=\{y\}\), then predict \(_{t}=y\). Otherwise, define \(V_{t}^{y}=\{c V_{t}\,:\,c(x_{t})=y\}\) for all \(y\), and predict \(_{t}=_{y}(V_{t}^{y},x_{t+1:T})\). Finally, the learner receives a feedback \(y_{t}\), and updates \(V_{t+1} V_{t}^{y_{t}}\). When \(t=T\), the sequence \(x_{T+1:T}\) is null and we define \(_{T}=_{y}(V_{T}^{y})\). For this learning algorithm, we prove that

\[(V_{t+1},x_{t+1:T})(V_{t},x_{t:T})-\{y_{t} _{t}\}.\] (1)

Rearranging and summing over \(t[T]\) rounds, we obtain:

\[_{t=1}^{T}\{y_{t}_{t}\}_{t=1}^ {T}(V_{t},x_{t:T})-(V_{t+1},x_{t+1:T})= (V_{1},x_{1:T})-(V_{T+1},x_{T+1:T})\] \[(V_{1},x_{1:T})()\]

The equality above follows because the sum telescopes. The final inequality follows because \(V_{1}=\) and the level-constrained branching dimension of \(\) is defined as \(()=_{T}_{x_{1:T}^{T}} (,x_{1:T})\).

We now prove inequality (1). There are two cases to consider: (a) \(y_{t}=_{t}\) and (b) \(y_{t}_{t}\). Starting with (a), let \(y_{t}=_{t}\). Recall that \((V_{t+1},x_{t+1:T})=(V_{t}^{y_{t}},x_{t+1:T})\). Since \(c(x_{t})=y_{t}\) for all \(h V_{t}^{y_{t}}\), we must have \((V_{t}^{y_{t}},x_{t+1:T})(V_{t}^{y_{t}},x_{t:T})\). Finally, using the fact that \(V_{t}^{y_{t}} V_{t}\), we have \((V_{t}^{y_{t}},x_{t:T})(V_{t},x_{t:T})\). This establishes (1) for this case.

Moving to (b), let \(y_{t}_{t}\). Note that we must have \((V_{t},x_{t:T})>0\). Otherwise, if \((V_{t},x_{t:T})=0\), then we have \(\{c(x_{t}):c V_{t}\}=\{y\}\). So, by our prediction rule, we cannot have \(y_{t}_{t}\) under realizability. To establish (1), we want to show that \((V_{t+1},x_{t+1:T})<(V_{t},x_{t:T})\). Suppose, for the sake of contradiction, we instead have \((V_{t+1},x_{t+1:T})(V_{t},x_{t:T})\). Then, let us define \(d:=(V_{t+1},x_{t+1:T})\). If \(d=0\), then our proof is complete because \(0(V_{t},x_{t:T})-\{y_{t}_{t}\}\). Assume that \(d>0\) and recall that \(V_{t+1}=V_{t}^{y_{t}}\). By definition of \((V_{t}^{y_{t}},x_{t+1:T})\) and its equivalent shattered-trees representation, there exists a level-constrained tree \(_{y_{t}}\) of depth \(T-t\) whose internal nodes are labeled by \(x_{t},,x_{T}\) and is shattered by \(V_{t}^{y_{t}}\). Moreover, every path down \(_{y_{t}}\) has branching factor \( d\).

Next, as \(_{t}=_{y}(V_{t}^{y},x_{t+1:T})\), we further have \((V_{t}^{_{t}},x_{t+1:T})(V_{t}^{y_{t}},x_{t+1:T}) d\). Thus, there exists another level-constrained tree \(_{_{t}}\) of depth \(T-t\) whose internal nodes are labeled by \(x_{t},,x_{T}\), that is shattered by \(V_{t}^{_{t}}\), and every path down \(_{_{t}}\) has branching factor \( d\). Finally, consider a new tree \(\) with root-node labeled by \(x_{t}\), the left-outgoing edge from the root node is labeled by \(y_{t}\), and the right outgoing edge is labeled by \(_{t}\). Moreover, the subtree following the outgoing edge labeled by \(y_{t}\) is \(_{_{t}}\). Since both \(_{y_{t}}\) and \(_{_{t}}\) are valid level-constrained trees each with internal nodes labeled by \(x_{t+1},,x_{T}\), the newly constructed tree \(\) is a also a level-constrained trees of depth \(T-t+1\) with internal nodes labeled by \(x_{t},,x_{T}\). In addition, as \(_{y_{t}}\) and \(_{_{t}}\) are shattered by \(V_{t}^{y_{t}}\) and \(V_{t}^{_{t}}\) respectively, the tree \(\) must be shattered by \(V_{t}^{y_{t}} V_{t}^{_{t}}\). Finally, as every path down eachsub-trees \(_{y_{t}}\) and \(_{_{t}}\) has branching factor \( d\) and \(y_{t}_{t}\), every path of \(\) must have branching factor \( d+1\). This shows that \((V_{t}^{y_{t}} V_{t}^{_{t}},x_{t:T}) d+1\). And since \(V_{t}^{y_{t}} V_{t}^{_{t}} V_{t}\), we have \(d+1(V_{t}^{y_{t}} V_{t}^{_{t}},x_{t:T}) (V_{t},x_{t:T})\) by monotonicity. This contradicts our assumption that \(d:=(V_{t+1},x_{t+1:T})(V_{t},x_{t:T})\). Therefore, we must have \((V_{t+1},x_{t+1:T})<(V_{t},x_{t:T})\). This establishes (1), completing our proof. 

Proof of Upperbound \(()\!(()})\)

Proof.: Fix the time horizon \(T\) and let \(x_{1:T}:=(x_{1},...,x_{T})^{T}\) be the sequence of \(T\) instances revealed to the learner at the beginning of the game. We say a subsequence \(x_{1:n}^{}:=(x_{1}^{},...,x_{n}^{})\), preserving the same order as in \(x_{1:T}\), is shattered by \(V\) if there exists a sequence of functions \(\{Y_{t}\}_{t=1}^{n}\), where \(Y_{t}:\{0,1\}^{t}\), such that for every \(\{0,1\}^{n}\), we have that

* \(Y_{t}(_{<t},0) Y_{t}(_{<t},1)\) for all \(t[n]\),
* \( c_{} V\) such that \(c_{}(x_{t})=Y_{t}(_{ t})\) for all \(t[n]\).

For every \(V\), let \((V)\) be the number of subsequences of \(x_{1:T}\) shattered by \(V\). In addition, for every \((x,y)\), let \(V_{(x,y)}:=\{c V:c(x)=y\}\). Consider the following online learner. At the beginning of the game, the learner initializes \(V^{1}=\). Then, in every round \(t[T]\), the learner predicts \(_{t}*{arg\,max}_{y}(V_{(x_{t},y )}^{t})\), receives \(y_{t}\), and updates \(V^{t+1} V_{(x_{t},y_{t})}^{t}\).

For this learning algorithm, we claim that

\[(V^{t+1})\{y_{t}=_{t}\}, {2}}(V^{t})\]

for every round \(t[T]\). This implies the stated mistake bound since \(()_{i=0}^{()} (()})^{()}\) and the learner can make at most \((())\) mistakes before \((V^{t})=1\). We now prove this claim by considering the case where \(_{t}=y_{t}\) and \(_{t} y_{t}\) separately.

Let \(t[T]\) be a round where \(_{t}=y_{t}\). Then, \((V^{t+1})(V^{t})\) since \(V^{t+1}=V_{(x_{t},y_{t})}^{t} V^{t}\). Now, let \(t[T]\) be a round where \(_{t} y_{t}\). We need to show that \((V^{t+1})\,(V^{t})\). For any \(V\), let \((V)\) be the set of all subsequences of \(x_{1:T}\) that are shattered by \(V\). Then, for any subsequence \(q(V^{t})\), only one of the following properties must be true:

* \(q(V_{(x_{t},y_{t})}^{t})\) and \(q(V_{(x_{t},_{t})}^{t})\),
* \(q(V_{(x_{t},y_{t})}^{t})\,(V_{(x_{t},_{ t})}^{t})\),
* \(q(V_{(x_{t},y_{t})}^{t})(V_{(x_{t},_{t})}^{t})\),

where \(\) denotes the symmetric difference. For every \(i\{1,2,3\}\), let \(^{i}(V^{t})(V^{t})\) be the subset of \((V^{t})\) that satisfies property \((i)\). Note that \((V^{t})=_{i=1}^{3}^{i}(V^{t})\) and \(\{^{i}(V^{t})\}_{i=1}^{3}\) are pairwise disjoint. Therefore, \(\{^{i}(V^{t})\}_{i=1}^{3}\) forms a partition of \((V^{t})\). For each \(i\{1,2,3\}\), we compute how many elements of \(^{i}(V^{t})\) we drop when going from \((V^{t})\) to \((V^{t+1})\). We can then upperbound \(|\,(V_{(x_{t},y_{t})}^{t}|=(V^{t+1})\) by lower bounding \(|\,(V^{t})(V^{t+1})|\), the number of elements we drop across all of the subsets \(\{^{i}(V^{t})\}_{i=1}^{3}\) when going from \(V^{t}\) to \(V^{t+1}\).

Starting with \(i=1\), observe that for every \(q^{1}(V^{t})\), we have that \(q(V_{(x_{t},y_{t})}^{t})\). Therefore, \(|\,^{1}(V^{t})(V_{(x_{t},y_{t})}^{t})|=0\), implying that we drop all the elements from \(^{1}(V^{t})\) when going from \((V^{t})\) to \((V^{t+1})\).

For the case where \(i=2\), note that \((V_{(x_{t},y_{t})}^{t}),(V_{(x_{t},_{t})}^{t}) (V^{t})\) and \((V_{(x_{t},_{t})}^{t})(V_{(x_{t},y_{t})}^{t})\), where the latter inequality is true by the definition of the prediction rule. Moreover, usingthe fact that \(\{^{i}(V^{t})\}_{i=1}^{3}\) forms a partition of \((V^{t})\), we can write

\[(V^{t}_{(x_{t},_{t})})=|^{3}(V^{t})|+| ^{2}(V^{t})(V^{t}_{(x_{t},_{t})})|\]

and

\[(V^{t}_{(x_{t},y_{t})})=|^{3}(V^{t})|+| ^{2}(V^{t})(V^{t}_{(x_{t},y_{t})})|.\]

Since \((V^{t}_{(x_{t},_{t})})(V^{t}_{(x_{t },y_{t})})\), we get that \(|^{2}(V^{t})(V^{t}_{(x_{t},_{t})} )||^{2}(V^{t})(V^{t}_{(x_{t},y_{t})})|\). This implies that \(|^{2}(V^{t})(V^{t}_{(x_{t},y_{t})})| |^{2}(V^{t})|\) since \((^{2}(V^{t})(V^{t}_{(x_{t},_ {t})}))\!\!\!(^{2}(V^{t})( V^{t}_{(x_{t},y_{t})}))\!\!=^{2}(V^{t})\) and \((^{2}(V^{t})(V^{t}_{(x_{t},_ {t})}))\!\!\!(^{2}(V^{t})( V^{t}_{(x_{t},y_{t})}))\!\!=\). Thus, we drop at least half the elements from \(^{2}(V^{t})\) when going from \((V^{t})\) to \((V^{t+1})\).

Finally, consider when \(i=3\). Fix a \(q^{3}(V^{t})\). We claim that \(x_{1},...,x_{t} q\). This is because every \(c V^{t}\) outputs \(y_{j}\) on \(x_{j}\) for all \(j t-1\). In addition, \(x_{t} q\) because \(q(V^{t}_{(x_{t},y_{t})})(V^{t}_{(x_{t },_{t})})\) and every concept in \(V^{t}_{(x_{t},y_{t})}\) and \(V^{t}_{(x_{t},_{t})}\) outputs \(y_{t}\) and \(_{t}\) on \(x_{t}\) respectively. Thus, the sequence \(x_{t} q\), obtained by concatenating \(x_{t}\) to the front of \(q\), is a valid subsequence of \(x_{1}\). Since \(_{t} y_{t}\), we also have that \(x_{t} q\) is shattered by \(V^{t}\). Using the fact that \(x_{t} q(V^{t}_{(x_{t},y_{t})})\) and \(x_{t} q(V^{t}_{(x_{t},_{t})})\), gives that \(x_{t} q^{1}(V_{t})\). Since our choice of \(q\) was arbitrary, this implies that for every \(q^{3}(V_{t})\), there exists a subsequence \(q^{}=x_{t} q^{1}(V^{t})\), ultimately giving that \(|^{1}(V^{t})||^{3}(V^{t})|\).

To complete the proof, we lowerbound the total number of dropped elements when going from \((V^{t})\) to \((V^{t+1})\) by

\[|(V^{t})(V^{t}_{(x_{t },y_{t})})| |^{1}(V^{t})|+^{2}(V^ {t})|}{2}\] \[^{1}(V^{t})|}{2}+^{2}(V^{t})|}{2}+^{3}(V^{t})|}{2}\] \[=(V^{t})|}{2}=(V^{t })}{2}.\]

The number of remaining elements is then \((V^{t+1})=|(V^{t}_{(x_{t},y_{t})})|=| (V^{t})|-|(V^{t})(V^{t}_{(x_{t},y_{t})})|(V^{t}),\) as needed. 

We end this section by noting that the algorithm in the proof of Theorem 3 can be made conservative (i.e. does not update when it is correct) with the same mistake bound. This conservative-version of the realizable learner will be used when proving regret bounds in the agnostic setting (see Section E).

### Proof of Lowerbound \([()=]\)

If \(()=\), then for every \(q\), Definition 3 guarantees the existence of \(d\), a sequence of instances \(x_{1},,x_{d}\), and a sequence of functions \(Y_{1},,Y_{d}\) where \(Y_{t}:\{0,1\}^{t}\) such that the following holds: (i) \(\{0,1\}^{d}\), there exists \(c_{}\) such that \(c_{}(x_{t})=Y_{t}(_{ t})\) (ii) \(\{0,1\}^{d}\), we have \(_{t=1}^{d}\{Y_{t}((_{<t},0)) Y_{t} ((_{<t},1))\} q\). Equivalently, there exists a shattered level-constrained branching tree \(\) of depth \(d\) with internal nodes labeled by instances \(x_{1},,x_{d}\) such that every path down the tree \(\) has \( q\) branching factor. Recall that the branching factor of a path is the number of nodes in the path whose two outgoing edges are labeled by two distinct elements of \(\). We say that an internal node has branching if the left and right outgoing edges from the node are labeled by two distinct elements of \(\).

Without loss of generality, we will assume that the \(\) guaranteed by Definition 3 has the following properties: (a) every path in \(\) has exactly \(q\) branching factor and (b) \(\) is symmetric along its non-branching nodes- that is, for every node in \(\) that has no branching, the subtrees on its left and right outgoing edges are identical. There is no loss in generality because given \(\) without property (a), we can traverse down every path in \(\), and once the path has branching factor \(q\), label all the subsequent outgoing edges down the path by a concept in \(\) that shatters any completion of that path. For property (b), if any non-branching node has two different subtrees, then replace the right subtree with the left subtree. Given such tree \(\), let \(()\) denote the number of levels in \(\) with at least one branching node. The following Lemma, whose proof can be found in Appendix D, provides an upperbound on \(()\).

**Lemma 1**.: _Let \(\) be any level-constrained branching tree shattered by \(\) such that: (a) every path in \(\) has exactly \(q\) branching and (b) for every node in \(\) without branching, the subtrees on its left and right outgoing edges are identical. Then, \(() 2^{q}-1\)._

Given this Lemma, we now prove the claimed lowerbound of \(\,[()=]\). Assume \(()=\) and take \(q= T\). Definition 3 guarantees the existence of shattered Level-constrained branching tree \(\) that satisfies property (a) and (b) specified in Lemma 1. Next, Lemma 1 implies that \(() 2^{ T}-1 T-1\). Let \(d\) be the depth \(\) and \(S\{1,,d\}\) be the levels in \(\) with at least one branching node. By definition, we have \(|S|=() T-1\). Recall that \(\) can be identified by a sequence of instances \(x_{1},,x_{d}\) and a sequence of functions \(Y_{1},,Y_{d}\) where \(Y_{t}:\{0,1\}^{t}\) for every \(t[d]\). For any path \(\{0,1\}^{d}\) down \(\), the set \(\{Y_{t}(_{ t})\}_{t=1}^{d}\) gives the labels along this path. Moreover, as all the branching on \(\) occurs on levels in \(S\), we have \(_{t=1}^{d}\{Y_{t}((_{<t},0)) Y_{t}((_{<t},1))\} =_{t S}\{Y_{t}((_{<t},0)) Y_{t}((_{<t},1)) \}= T\) for every path \(\{0,1\}^{d}\).

We now specify the stream to be observed by the learner \(\). Draw \((\{0,1\}^{d})\) and consider the stream \(\{(x_{t},Y_{t}(_{ t}))\}_{t S}\). Repeat \((x_{m},Y(_{ m}))\) for remaining \(T-|S|\) timepoints, where \(m\) is the largest index in set \(S\). Since this stream is a sequence of instance-label pairs along the path \(\) in the shattered tree \(\), there exists a \(c_{}\) consistent with the stream. However, using similar arguments as in the proof of the lowerbound \(()/2\), we can establish

\[[_{t=1}^{T}\{x_{t } Y_{t}(_{ t})\}]\] \[[_{t S}\{x _{t} Y_{t}(_{ t})\}]\] \[\,[_{t S}\{ Y_{t}((_{<t},0)) Y_{t}(_{<t},1)\}]\] \[= T.\]

This completes our proof of lower bound.

## 4 Discussion

In this paper, we study the problem of multiclass transductive online learning with possibly arbitrary label space. In the realizable setting, we establish a trichotomy in the possible minimax rates of the expected number of mistakes. Furthermore, we show near-tight upper and lower bounds on the optimal expected regret in the agnostic setting. Along the way, we introduce two new combinatorial complexity parameters, called the Level-constrained Littlestone dimension and the Level-constrained Branching dimension.

Finally, we highlight some future directions of this work. First, can we extend our results to settings such as transductive online learning under bandit feedback, list transductive online learning, and transductive online real-valued regression? Moreover, as our shattering technique is general, can we use similar ideas to establish the possible minimax rates of the number of mistakes in the self-directed and the best-order settings initially studied in ?