# Alternation makes the adversary weaker in two-player games

Volkan Cevher

LIONS, EPFL

volkan.cevher@epfl.ch

&Ashok Cutkosky

Boston University

ashok@cutkosky.com

&Ali Kavis

LIONS, EPFL

ali.kavis@epfl.ch

&Georgios Piliouras

SUTD

georgios@sutd.edu.sg

Stratis Skoulakis

LIONS, EPFL

efstratios.skoulakis@epfl.ch

&Luca Viano

LIONS, EPFL

luca.viano@epfl.ch

###### Abstract

Motivated by alternating game-play in two-player games, we study an attenuating variant of the _Online Linear Optimization_ (OLO). In alternating OLO, a _learner_ at each round \(t[n]\) selects a vector \(x^{t}\) and then an _adversary_ selects a cost-vector \(c^{t}[-1,1]^{n}\). The learner then experiences cost \((c^{t}+c^{t-1})^{}x^{t}\) instead of \((c^{t})^{}x^{t}\) as in standard OLO. We establish that under this small twist, the \(()\) lower bound on the regret is no longer valid. More precisely, we present two online learning algorithms for alternating OLO that respectively admit \((( n)^{4/3}T^{1/3})\) regret for the \(n\)-dimensional simplex and \(( T)\) regret for the ball of radius \(>0\). Our results imply that in alternating game-play, an agent can always guarantee \(}(( n)^{4/3}T^{1/3})\) regardless the strategies of the other agent while the regret bound improves to \(( T)\) in case the agent admits only two actions.

## 1 Introduction

Game-dynamics study settings at which a set of selfish agents engaged in a repeated game _update_ their strategies over time in their attempt to minimize their overall individual cost. In _simultaneous play_ all agents simultaneously update their strategies, while in _alternating play_ only one agent updates its strategy at each round while all the other agents stand still. Intuitively, each agent only updates its strategy _in response_ to an observed change in another agent.

Alternating game-play captures interactions arising in various context such as animal behavior, social behavior, traffic networks etc. (see  for various interesting examples) and thus has received considerable attention from a game-theoretic point of view [11; 3; 33; 41; 40]. At the same time, _alternation_ has been proven a valuable tool in tackling min-max problems arising in modern machine learning applications (e.g. training GANs, adversarial examples etc.) and thus has also been studied from an offline optimization perspective [37; 35; 21; 42; 9; 8; 10].

In the context of two-players, alternating game-play admits the following form: Alice (odd player) and Bob (even player) respectively update their strategies on odd and even rounds. Alice (resp. Bob) should select her strategy at an odd round so as to exploit Bob's strategy of the previous (even) round while at the same time protecting herself from Bob's response in the next (even) round. As a result, the following question arises:

_Q1: How should Alice (resp. Bob) update her actions in the odd rounds so that, regardless of Bob's strategies, her overall cost (over the \(T\) rounds of play) is minimized?_

### Standard and Alternating Online Linear Minimization

Motivated by the above question and building on the recent line of research studying online learning settings with _restricted adversaries_, we study an online linear optimization setting , called _alternating online linear optimization_. We use the term _"alternating"_ to highlight the connection with alternating game-play that we subsequently present in Section 1.2.

In Algorithm 1 we jointly present both standard and alternating OLO so as to better illustrate the differences of the two settings.

```
1:Input: A feasibility set \(^{n}\) and \(c^{0}(0,,0)\).
2:for each round \(t=1,,T\)do
3: The learner selects a vector \(x^{t}\) based on \(c^{1},,c^{t-1}[-1,1]^{n}\)
4: The adversary learns\(x^{t}\) and selects a cost vector \(c^{t}[-1,1]^{n}\) (based on \(x^{1},,x^{t}\)).
5: The learner learns\(c^{t}[-1,1]^{n}\) and receives cost, \[(c^{t})^{}x^{t}\] Standard OLM \[(c^{t}+c^{t-1})^{}x^{t}\] Alternating OLM
6:endfor ```

**Algorithm 1** Standard and Alternating Online Linear Minimization

In both standard and alternating OLO, the adversary selects \(c^{t}\) after the the learner's selection of \(x^{t}\). The only difference between standard and alternating OLM is that in the first case the learner admits cost \((c^{t})^{}x^{t}\) while in the second its cost is \((c^{t}+c^{t-1})^{}x^{t}\). An online learning algorithm's selects \(x^{t}\) solely based on the previous cost-vector sequence \(c^{1},,c^{t-1}[-1,1]^{n}\) with the goal minimizing the overall cost that is slightly different in standard and alternating OLO.

The quality of an online learning algorithm \(\) in standard OLO is captured through the notion of _regret_, comparing \(\)'s overall cost with the overall cost of the best fixed action_,

\[_{}(T):=_{c^{1},,c^{T}}[_{t=1}^{T}(c ^{t})^{}x^{t}-_{x}_{t=1}^{T}(c^{t})^{}x].\] (1)

When \(_{}(T)=o(T)\), the algorithm \(\) is called _no-regret_ since it ensured that regardless of the cost-vector sequence \(c^{1},,c^{T}\), the time-averaged overall cost of \(\) approaches the time-averaged overall cost of the _best fixed action_ with rate \(o(T)/T 0\). Correspondingly, the quality of an online learning algorithm \(\) in alternating OLO is captured through the notion of _alternating regret_,

\[_{}^{}(T):=_{c^{1},,c^{T}}[ _{t=1}^{T}(c^{t}+c^{t-1})^{}x^{t}-_{x}_{t=1}^{T }(c^{t}+c^{t-1})^{}x].\] (2)

Over the years various no-regret algorithms have been proposed for different OLO settings2 achieving \(_{}(T)=}()\) regret . The latter regret bounds are optimal since there is is a simple probabilistic construction establishing that any online learning algorithm \(\) admits \(_{}(T)=()\) even when \(\) is the \(2\)-dimensional simplex. This negative results comes from the fact that the adversary has access to the action \(x^{t}\) of the algorithm and can appropriately select \(c^{t}\) to maximize \(\)'s regret.

At a first sight, it may seem that the adversary can still enforce \(()\) alternating regret to any online learning algorithm \(\) by appropriately selecting \(c^{t}\) based on \(x^{t}\) and possibly on \(c^{t-1}\). Interestingly enough the construction establishing \(()\) regret, fails in the case of alternating regret (see Section 2). As a result, the following question naturally arises,_Q2_: _Are there online learning algorithm with \(o()\) alternating regret?_

Apart from its interest in the context of online learning, answering _Q2_ implies a very sound answer to _Q1_. In Section 1.2 we present the connection between Alternating OLO and Alternating Game-Play.

### Alternating OLO and Alternating Game-Play

Alternating game-play in the context of two-player games can be described formally as follows: Let \((A,B)\) be a game played between Alice and Bob. The matrix \(A[-1,1]^{n m}\) represents Alice's costs, \(A_{ij}\) is the cost of Alice if she selects action \(i[n]\) and Bob selects action \(j[m]\) (respectively \(B[-1,1]^{m n}\) for Bob). Initially Alice selects a mixed strategy \(x^{1}_{n}\). Then,

* At the even rounds \(t=2,4,6,,2k:\) Bob plays a new mixed strategy \(y^{t}_{m}\) and Alice plays \(x^{t-1}_{n}\). Alice and Bob incur costs \((x^{t-1})^{}Ay^{t}\) and \((y^{t})^{}Bx^{t-1}\) respectively.
* At the odd rounds \(t=3,5,,2k-1:\) Alice plays a new mixed strategy \(x^{t}_{n}\) and Bob plays \(y^{t-1}_{m}\). Alice and Bob incur costs \((x^{t})^{}Ay^{t-1}\) and \((y^{t-1})^{}Bx^{t}\) respectively.

From the perspective of Alice (resp. Bob), the question is how to select her mixed strategies \(x^{1},x^{3},,x^{2k-1}_{n}\) so as to minimize her overall cost

\[(x^{1})^{}Ay^{2}+_{k=1}^{T/2-1}(x^{2k+1})^{}A(y^{2k}+y^{2k+2}).\]

In Corollary 1.1 we establish that if Alice uses an online learning algorithm \(\) then her overall regret (over the course of \(T\) rounds of play) is at most \(^{}_{}(T/2)\). As a result, in case _Q2_ admits a positive answer, then Alice can guarantee at most \(o()\) regret and improve over the \(}()\) regret bound provided by standard no-regret algorithms .

**Corollary 1.1**.: _In case Alice (resp. Bob) uses an online learning algorithm \(\) to update her strategies in the odd rounds, \(x^{2k+1}:=(Ay^{2},Ay^{4},,Ay^{2k})\) for \(k=1,,T/2-1\). Then no matter Bob's selected sequence \(y^{2},y^{4},,y^{T}_{m}\),_

\[(x^{1})^{}Ay^{2}+_{k=1}^{T/2-1}(x^{2k+1})^{}A(y^{2k}+y^{2k+2})- _{x_{n}}[x^{}Ay^{2}+_{k=1}^{T/2-1}x^{}A(y^{2k}+ y^{2k+2})]^{}_{}(T/2)\]

**Remark 1.2**.: We remark that Corollary 1.1 refers to the standard notion of regret  and \(^{}_{}(T/2)\) appears only as an upper bound. We additionally remark that if both Alice and Bob respectively use algorithms \(\) and \(\) in the context of alternating play, then the time-average strategy vector converges with rate \(((_{}(T),_{}( T))/T)\) to Nash Equilibrium in case of zero-sum games (\(A=-B^{}\)) and to Coarse Correlated Equilibrium for general two-player games . Our objective is more general: we focus on optimizing the performance of a single player regardless of the actions of the other player.

### Our Contribution and Techniques

In this work we answer _Q2_ on the affirmative. More precisely we establish that,

* There exists an online learning algorithm (Algorithm 3) with alternating regret \(}(( n)^{4/3}T^{1/3})\) for \(=_{n}\) (\(n\)-dimensional simplex).
* There exists an online learning algorithm (Algorithm 4) with alternating regret \(( T)\) for \(=(c,)\) (ball of radius \(\)).
* There exists an online learning algorithm with alternating regret \(( T)\) for \(=_{2}\) (\(2\)-dimensional simplex), through a straight-forward reduction from \(=(c,)\).

Due to Corollary 1.1 our results provide a non-trivial answer to _Q1_ and establish that Alice can substantially improve over the \(()\) regret guarantees of standard no-regret algorithms.

**Corollary 1.3**.: _In the context of alternating game play, Alice can always guarantee at most \(}(( n)^{4/3}T^{1/3})\) regret regardless the actions of Bob. Moreover in case Alice admits only \(2\) actions (\(n=2\)), the regret bound improves to \(( T)\)._

Bailey et al.  studied _alternating game-play in unconstrained two-player games_ (the strategy space is \(^{n}\) instead of \(_{n}\)). They established that if the \(x\)-player (resp. the \(y\)-player) uses _Online Gradient Descent_ (OGD) with constant step-size \(>0\) (\(x^{2k}:=x^{2k-2}- Ay^{2k-1}\)) then it experiences at most \((1/)\) regret regardless the actions of the \(y\)-player. In the context of alternating OLM this result implies that OGD admits \((1/)\) alternating regret as long as _it always stays in the interior of \(\)_. However the latter cannot be guaranteed for bounded domains (simplex, ball). In fact there is a simple example for \(=_{2}\) at which OGD with \(\) step-size admits \((1/+ T)\) alternating regret. More recently,  studied alternating game-play in zero-sum games (\(B=-A^{}\)). They established that if _both player_ adopt Online Mirror Descent (OMD) the individual regret of each player is at most \((T^{1/3})\) and thus the time-averaged strategies converge to Nash Equilibrium with \((T^{-2/3})\) rate. The setting considered in this works differs because where the \(y\)-player can behave adversarially.

In order to achieve \(}(( n)^{4/3}T^{1/3})\) alternating regret in case \(=_{n}\), we first propose an \(}(T^{1/3})\) algorithm for the special case of \(=_{2}\). For this special case our proposed algorithm is an _optimistic-type_ of _Follow the Regularized Leader_ (FTRL) with _log-barrier regularization_. Using the latter as an algorithmic primitive, we derive the \(}(( n)^{4/3}T^{1/3})\) alternating regret algorithm for \(=_{n}\), by upper bounding the overall alternating regret by the sum of _local alternating regret_ of \(2\)-actions decision points on a binary tree at which the leafs corresponds to the actual \(n\) actions.

In order to achieve \(( T)\) alternating regret for \(=(c,)\) we follow a relatively different path. The major primitive of our algorithm is FTRL with adaptive step-size [16; 5]. The cornerstone of our approach is to establish that in case Adaptive FTRL admits more than \(( T)\) alternating regret, then _unnormalized best-response_ (\(-c^{t-1}\)) can compensate for the additional cost. By using a recent result on _Online Gradient Descent with Shrinking Domains_, we provide an algorithm interpolating between Adaptive FTRL and \(-c^{t-1}\) that achieves \(( T)\) alternating regret.

### Further Related Work

The question of going beyond \(()\) regret in the context of _simultaneous game-play_ has received a lot of attention. A recent line of work establishes that if both agents simultaneously use the _same no-regret algorithm_ (in most cases Optimistic Hedge) to update their strategies, then the individual regret of each agent is \(}(1)\)[1; 14; 13; 2; 36; 26; 17].

Our work also relates with the more recent works in establishing improved regret bounds parametrized by the cost-vector sequence \(c^{1},,c^{T}\), sometimes also called "adaptive" regret bounds [16; 29; 38; 30; 12]. However these parametrized upper bounds focus on finding "easy" instances while still maintaining \(()\) in the worst case. Alternating OLO can be considered as providing a slight "hint" to the learner that fundamentally changes the worst-case behavior, since its cost is \((c^{t}+c^{t-1})^{}x^{t}\) with the learner being aware of \(c^{t-1}\) prior to selecting \(x_{t}\). Improved regret bounds under different notions of hints have been established in [4; 5; 15; 34; 24; 39].

Finally our work also relates with the research line of no-regret learning in the context of _Extensive Form Games_[44; 18; 19] and _Stackelberg Games_[27; 22].

## 2 Preliminaries

We denote with \(_{n}^{n}\) the \(n\)-dimensional simplex, \(_{n}:=\{x^{n}:\,x_{i} 0_{i=1}^{n}x_{i}=1\}\). \((c,)\) denotes the ball of radius \(>0\) centered at \(c^{n}\), \((c,):=\{x^{n}:\,\|x-c\|_{2}\}\). We also denote with \([x]_{}:=*{arg\,min}_{z}\|z- x\|^{2}\) the projection operator to set \(\).

### Standard and Alternating Online Linear Minimization

As depicted in Algorithm 1 the only difference between standard and Alternating OLM is the cost of the learner, \((c^{t})^{}x^{t}\) (OLM) and \((c^{t}+c^{t-1})^{}x^{t}\) (Alternating OLM). Thus, the notion of an _online learning algorithm_ is exactly the same in both settings.

**Definition 2.1**.: An online learning algorithm \(\), for an Online Linear Optimization setting with \(^{n}\), is a sequence of functions \(:=(_{1},,_{t},)\) where \(_{t}:^{d}^{d}}_{ t-1}\).

As Definition 2.1 reveals, the notion of an online learning algorithm depends only on the feasibility set \(\). As a result, an online learning algorithm \(\) simultaneously admits both standard \(_{}(T)\) and alternating regret \(^{}_{}(T)\) (see Equations 1 and 2 for the respective definitions). In Theorem 2.2, we present the well-known lower bound establishing that any online learning algorithm \(\) admits \(_{}(T)=()\) and explain why it fails in the case of alternating regret \(^{}_{}(T)\).

**Proposition 2.2**.: _Any online learning algorithm \(\) for \(=_{2}\), admits regret \(_{}(T)()\)._

Proof.: Let \(c^{t}\) be independently selected between \((-1,1)\) and \((1,-1)\) with probability \(1/2\). Since \(c^{t}\) is independent of \((c^{1},,c^{t-1})\) then \(_{t=1}^{T}[(c^{t})^{}x^{t}]=0\) where \(x^{t}:=_{t}(c^{1},,c^{t-1})\). At the same time, \([-_{x_{2}}_{t=1}^{T}(c^{t})^{}x] ()\). As a result, \(_{}(T)()\). 

We now explain why the above randomized construction does not apply for alternating regret \(^{}_{}(T)\). Let \(\) be the _best-response algorithm_, \(A_{t}(c^{1},,c^{t-1}):=*{argmin}_{x_{2}}(c^{t-1} )^{}x\). Since \(c^{t}=(1,-1)\) or \(c^{t}=(-1,1)\) we get that \(_{x_{2}}(c^{t-1})^{}x=-1\) while \([(c^{t})^{}x^{t}]=0\) since \(x^{t}:=*{argmin}_{x_{2}}(c^{t-1})^{}x\) and \(c^{t}\) is independent of \(c^{t-1}\). As a result,

\[[_{t=1}^{T}(c^{t}+c^{t-1})^{}x^{t}-_{t=1}^{T} _{x_{2}}(c^{t}+c^{t-1})^{}x]=-T+().\]

The latter implies that there exists at least one online learning algorithm (_Best-Response_) that admits \((-T)\) alternating regret in the above randomized construction. However the latter is not very informative since there is a simple construction at which _Best-Response_ admits linear alternating regret.

We conclude this section with the formal statement of our results. First, for the case that \(\) is the simplex, we show \((T^{1/3})\) alternating regret (Section 3):

**Theorem 2.3**.: _Let \(\) be the \(n\)-dimensional simplex, \(=_{n}\). There exists an online learning algorithm \(\) (Algorithm 3) such that for any cost-vector sequence \(c^{1},,c^{T}[-1,1]^{n}\), \(_{t=1}^{T}(c^{t-1}+c^{t})^{}x^{t}-_{x^{t}}_{t=1} ^{T}(c^{t-1}+c^{t})^{}x^{*}(T^{1/3}^{4/3} (nT))\) where \(x^{t}=_{t}(c^{1},,c^{t-1})\)._

Next, when \(\) is a ball of radius \(\), we can improve to \((1)\) alternating regret (Section 4):

**Theorem 2.4**.: _Let \(\) be a ball of radius \(\), \(=(c,)\). There exists an online learning algorithm \(\) (Algorithm 4) such that for any cost-vector sequence \(c^{1},,c^{T}\) where \(\|c^{t}\|_{2} 1\),_

\[_{t=1}^{T}(c^{t-1}+c^{t})^{}x^{t}-_{x^{t}}_{t=1}^ {T}(c^{t-1}+c^{t})^{}x^{*}( T)\ \ \ \ \ x^{t}=_{t}(c^{1},,c^{t-1}).\]

**Remark 2.5**.: Using Algorithm 4 we directly get an online learning algorithm with \(( T)\) alternating regret for \(=_{2}\).

### Alternating Game-Play

A _two-player normal form game_\((A,B)\) is defined by the payoff matrix \(A[-1,1]^{n m}\) denoting the payoff of Alice and the matrix \(B[-1,1]^{m n}\) denoting the payoff of Bob. Once the Alice selects a mixed strategy \(x_{n}\) (prob. distr. over \([n]\)) and Bob selects a mixed strategy \(y_{m}\) (prob. distr. over \([n]\)). Then Alice suffers (expected) cost \(x^{}Ay\) and Bob \(y^{}Bx\).

In alternating game-play, Alice updates her mixed strategy in the even rounds while Bob updates in the odd rounds. As a result, a sequence of alternating play for \(T=2K\) rounds (resp. for \(T=2K+1\)) admits the form \((x^{1},y^{2}),(x^{3},y^{2}),,(x^{2k+1},y^{2k}),(x^{2k+1},y^{2k+2}),,(x^ {2K-1},y^{2K})\). Thus, the _regret_ of Alice in the above sequence of play equals the difference between her overall cost and the cost of the _best-fixed action_,

\[_{x}(T):=)^{}Ay^{2}+_{k=1}^{T/2-1}(x^{2k+1} )^{}A(y^{2k}+y^{2k+2})}_{}-}[x^{}Ay^{1}+_{k=1}^{T/2-1}x^{}A(y^{2k}+y^{2k+2})]}_{ }\]If Alice selects \(x^{2k+1}:=_{k}(Ay^{2},Ay^{4},,Ay^{2k-2},Ay^{2k})\) for \(k[K-1]\) and \(x_{1}=_{1}()\) then by the definition of alternating regret in Equation 2, we get that

\[(x^{1})^{}Ay^{2}+_{k=1}^{K-1}(x^{2k+1})^{}(Ay^{2k}+Ay^{2k+2})-_ {x_{n}}[x^{}Ay^{2}+_{k=1}^{K-1}x^{}(Ay^{2k}+Ay^{2k+2} )]_{}^{}(K)\]

which establishes Corollary 1.1. The proof for \(T=2K+1\) is the same by considering \(Ay^{2K+2}=0\).

## 3 The Simplex case

Before presenting our algorithm for the \(n\)-dimensional simplex, we present Algorithm 2 that admits \((^{2/3}T T^{1/3})\) alternating regret for the \(2\)-simplex and is the basis of our algorithm for \(_{n}\).

**Definition 3.1** (Log-Barrier Regularization).: Let the function \(R:_{2}_{ 0}\) where \(R(x):=- x_{1}- x_{2}\).

```
1:Input:\(c^{0}(0,0)\)
2:for rounds \(t=1,,T\)do
3: The learner selects\(x^{t}:=_{x_{2}}[2(c^{t-1})^{}x+_{=1}^{t-1}(c^{ }+c^{-1})^{}x+R(x)/]\).
4: The adversary selects cost vector \(c^{t}^{n}\)
5: The learner suffers cost \((c^{t}+c^{t-1})^{}x^{t}\)
6:endfor ```

**Algorithm 2** Online Learning Algorithm for 2D-Simplex

In order to analyze Algorithm 2 we will compare its performance with the performance of the _Be the Regularized Leader algorithm_ with _log-barrier regularization_ that is ensured to achieve \(( T/)\) alternating regret . The latter is formally stated and established in Lemma 3.2.

**Lemma 3.2**.: _Let \(y^{1},,y^{T}_{2}\) where \(y^{t}:=_{x_{2}}[(c^{t}+c^{t-1})^{}x+_{s=1}^{t-1}(c^ {s}+c^{s-1})^{}x+R(x)/]\). Then, \(_{t=1}^{T}(c^{t}+c^{t-1})^{}y^{t}-_{i[n]}_{t=1}^{T}(c^{t}_ {i}+c^{t-1}_{i}) 2 T/+2\)._

In Lemma 3.3 we provide a closed formula capturing the difference between the output \(x^{t}_{2}\) of Algorithm 2 and the output \(y^{t}_{2}\) of _Be the Regularized Leader algorithm_ defined in Lemma 3.2.

**Lemma 3.3**.: _Let \(x^{t}=(x^{t}_{1},x^{t}_{2})_{2}\) as in Algorithm 2 and \(y^{t}=(y^{t}_{1},y^{t}_{2})_{2}\) as in Lemma 3.2. Then,_

\[x^{t}_{1}-y^{t}_{1}= A^{-1}(x^{t}_{1},y^{t}_{1})((c^{t}_{1}-c^ {t}_{2})-(c^{t-1}_{1}-c^{t-1}_{2}))\]

_with \(A(x_{1},y_{1}):=(x_{1}y_{1})^{-1}+(1-x_{1})^{-1}(1-y_{1})^{-1}\) and \(|A^{-1}(x^{t}_{1},y^{t}_{1})-A^{-1}(x^{t+1}_{1},y^{t+1}_{1})|()\)._

Up next we use Lemma 3.2 and Lemma 3.3 to establish that Algorithm 2 admits \((^{2/3}T T^{1/3})\) alternating regret.

**Theorem 3.4**.: _Let \(x^{1},,x^{T}_{2}\) the sequence produced by Algorithm 2 for the cost sequence \(c^{1},,c^{T}[-1,1]^{2}\) with \(=(^{1/3}T T^{-1/3})\) then \(^{}(T)=(^{2/3}T T^{1/3})\)._

Proof.: By Lemma 3.2 then \(_{t[T]}(c^{t}+c^{t-1})^{}x^{t}-_{i[n]}_{t[T]}(c^{t}_ {i}+c^{t-1}_{i})( T/)+_{t[T]}(c^{t} _{i}+c^{t-1})^{}(x^{t}-y^{t})\) where \(y^{t}_{2}\) as in Lemma 3.2. Using Lemma 3.3 we get that

\[_{t=1}^{T}(c^{t}+c^{t-1})^{}(x^{t}-y^{t})=_{t=1}^{T} ((c^{t}_{1}-c^{t}_{2})+(c^{t-1}_{1}-c^{t-1}_{2}))(x^{t}_{1}-y^{t}_ {1})\] \[= _{t=1}^{T}((c^{t}_{1}-c^{t}_{2})+(c^{t-1}_{1}-c^{t -1}_{2}))A^{-1}(x^{t}_{1},y^{t}_{1})((c^{t}_{1}-c^{t}_{2})-(c^{ t-1}_{1}-c^{t-1}_{2}))\] \[= _{t=1}^{T}A^{-1}(x^{t}_{1},y^{t}_{1})((c^{t}_{1}-c ^{t}_{2})^{2}-(c^{t-1}_{1}-c^{t-1}_{2})^{2})\] \[= _{t=1}^{T}(c^{t}_{1}-c^{t}_{2})^{2}(A^{-1}(x^{ t}_{1},y^{t}_{1})-A^{-1}(x^{t+1}_{1},y^{t+1}_{1}))(^{2}T)\]

Hence \(_{alt}(T)( T/+^{2}T)) (^{2/3}T T^{1/3})\) for \(:=(^{1/3}T/T^{1/3})\).

### The \(n\)-Dimensional Simplex

In this section we extend Algorithm 2 to the case of the \(n\)-dimensional simplex. Our extension is motivated and builds upon the CFR algorithm develloped in the context of EFGs .

Without loss of generality we assume that \(n=2^{H}\). We consider a complete binary tree \(T(V,E)\) of height \(H= n\) where the _leaves_\(L V\) corresponds to the \(n\)_actions_, \(|L|=n\). Each node \(s V/L\) admits exactly two children with \((s),r(s)\) respectively denoting the left and right child. Moreover, \(() V\) denotes the nodes lying at depth \(h\) from the root ( \((1)=\{\}\) and \(( n)=L\)). Up next we present the notion of _policy_ on the nodes of \(T(V,E)\).

**Definition 3.5**.:
* A policy over the nodes \(:V/L_{2}\) encodes the probability of selecting the left/right child at node \(s V\). Specifically \((s)=(((s)|s),(r(s)|s))\) where \(((s)|s)+(r(s)|s)=1\) and \(((s)|s)\) is the probability of selecting \((s)\) (resp. for \(r(s)\)).
* \((s,i,)\) denotes the probability of reaching leaf \(i L\) starting from node \(s V/L\) and following \(()\) at each step.
* \(x^{}_{n}\) denotes the probability distribution over the _leaves/actions_ induced by \(()\). Formally, we have \(x_{i}^{}:=(,i,)\) for each leaf \(i L\).

**Definition 3.6**.: Given a cost vector \(c[-1,1]^{n}\) for the _leaves/actions_, the _virtual cost_ of a node \(s V\) under policy \(()\), denoted as \(Q(s,,c)\), equals

\[Q(s,,c):=\{c_{s}&s L\\ _{i L}(s,i,) c_{i}&s L.\]

The _virtual cost vector_ of \(s V\) under \(()\) is defined as \(q(s,,c):=(Q((s),,c),Q(r(s),,c))\).

We remark that \(Q(s,,c)\) is the _expected cost_ of the random walk starting from \(s V\) and following policy \(()\) until a leaf \(i L\) is reached in which case cost \(c_{i}\) is occurred.

Our online learning algorithm for the \(n\)-dimensional simplex is illustrated in Algorithm 3.

```
1:Input: A sequence of cost vectors \(c^{1},,c^{T}[-1,1]^{n}\)
2: The learner constructs a complete binary tree \(T(V,E)\) with \(L=\).
3:for each round \(t=1,,T\)do
4:for each \(h= n\) to \(1\)do
5:for every node \(s(h)\)do
6: The learner computes \(q(s,^{t},c^{t-1}):=(Q((s),^{t},c^{t-1}),Q (r(s),^{t},c^{t-1}))\) and sets \(^{t}(s):=}{}[2q(s,^{t},c^{t-1})^{ }x+_{=1}^{t-1}(q(s,^{},c^{-1})+q(s,^{},c^{ }))^{}x+R(x)/]\)
7:endfor
8:endfor
9: The learner selects \(x^{t}:=x^{^{t}}_{n}\) (induced by policy \(_{t}\), Definition 3.5).
10: The adversary selects cost vector \(c^{t}^{n}\)
11: The learner suffers cost \((c^{t}+c^{t-1})^{}y^{t}\)
12:endfor ```

**Algorithm 3** An Online Learning Algorithm for the \(n\)-Dimensional Simplex

We remark that at each round \(t\), the learner computes a policy \(^{t}()\) as an intermediate step (Step 6) that then uses to select the probability distribution \(x^{t}:=x^{^{t}}_{n}\) (Step 9). Notice that the computation of policy \(^{t}()\) is performed in Steps (4)-(8). Since nodes are processed in decreasing order (with respect to their level), during Step 6\(^{t}()\) has already been determined for nodes \((s),r(s)\) and thus \(Q((s),^{t},c^{t-1}),Q(r(s),^{t},c^{t-1})\) are well-defined.

Up next we present the main steps for establishing Theorem 2.3. A key notion in the analysis of Algorithm 3 is that of _local alternating regret_ of a node \(s V\) presented in Definition 3.7. As established in Lemma 3.8 the overall alternating regret of Algorithm 3 can be upper bounded by the sum of the local alternating regrets of the nodes lying in the path of the _best fixed leaf/action_.

**Definition 3.7**.: For any sequence \(c^{1},,c^{T}[-1,1]^{n}\) the _alternating local regret_ of a node \(s V\), denoted as \(^{T}_{loc}(s)\), is defined as

\[^{T}_{loc}(s):=_{t[T]}(q(s,^{t},c^{t})+q(s,^{t},c^ {t-1}))^{}^{t}(s)-_{\{(s),r(s)\}}_{t[T]} (Q(,^{t},c^{t})+Q(,^{t},c^{t-1}))\]

**Lemma 3.8**.: _Let a leaf/action \(i L\) and consider the path \(p=(=s_{1},,s_{H}=i)\) from the root to the leaf \(i L\). Then, \(_{t=1}^{T}(c^{t}+c^{t-1})^{}x^{^{t}}-2_{t=1}^{T}c_{i}^{t} _{=1}^{H}_{loc}(s_{})\)._

Up to this point, it is evident that in order to bound the overall alternating regret of Algorithm 3, we just need to bound the local alternating regret of any node \(s V\). Using Theorem 3.4 we can bound the local regret of leaves \(i L\) for which \(q(i,^{t},c^{t-1})=q(i,^{t-1},c^{t-1})\). However this approach does apply for nodes \(s V/L\) since the local regret does not have the _alternating structure_, \(q(s,^{t},c^{t-1}) q(s,^{t-1},c^{t-1})\). To overcome the latter in Lemma 3.9 we establish that \(q(s,^{t},c^{t-1})\), \(q(s,^{t-1},c^{t-1})\) are in distance \(()\) which permits us to bound \(^{T}_{loc}(s)\) for \(s V/L\) by tweaking the proof of Theorem 3.4.

**Lemma 3.9**.: _Let \(^{1},,^{T}\) the policies produced by Algorithm 3 then for any node \(s V\), \(i)\)\(\|^{t}(s)-^{t-1}(s)\|_{1} 48\) and \(ii)\)\(\|q(s,^{t},c^{t-1})-q(s,^{t-1},c^{t-1})\|_{} 48 n\)._

Using Lemma 3.9 we can establish an upper bound on the local regret of any actions \(s V\). The proof of Lemma 3.10 lies in Appendix B and follows a similar structure with the proof of Theorem 3.4.

**Lemma 3.10**.: _Let \(:=(^{1/3}T/(T^{1/3}^{1/3}n))\) in Algorithm 3 then \(^{T}_{loc}(s)(^{2/3}T^{1/3}n T ^{1/3})\) for all \(s V\)._

Theorem 2.3 directly follows by combining Lemma 3.10, Lemma 3.8 and \(H= n\).

## 4 The Ball case

In Algorithm 4 we present an online learning algorithm with \(( T)\) for \(=(0,1)\) and \(\|c^{t}\|_{2} 1\). Then through the transformation \(_{t}:=c+ x^{t}\) with \(x^{t}(0,1)\), Algorithm 4 can be transformed to a \(( T)\)-alternating regret algorithm for \(=(c,)\).

Algorithm 4 may seem complicated at the first sight however it is composed by two basic algorithmic primitives. At Step \(4\) Algorithm 4 computes the output \(w_{t}(0,1)\) of the _Follow the Regularized Leader_ (FTRL) with Euclidean regularization and adaptive step-size \(r_{0:t-1}\) (Step \(3\) of Algorithm 4).At Step \(5\), it _mixes_ the output \(w_{t}(0,1)\) of \(\) with the _unnormalized best-response_\(-c^{t-1}(0,1)\). The selection of the _mixing coefficient_\(p_{t}\) is adaptively updated at Step \(7\).

### Proof of Theorem 2.4

In this section we present the main steps of the proof of Theorem 2.4. In Lemma 4.1 we provide a first upper bound on the alternating regret of Adaptive \(\).

**Lemma 4.1**.: _Let \(w_{1},,w_{T}(0,1)\) the sequence produced by Adaptive FTRL (Step \(4\) of Algorithm 4) given as input the cost-vector sequence \(c^{1},,c^{T}(0,1)\). Let \(t_{1}\) denote the maximum time-index such that \(_{s=1}^{t}(c^{s}+c^{s-1})^{}w_{t}-_{s=1}^{t}\|c^{s}+c^{s-1}\|_{ 2}^{2}/4\). Then,_

\[_{t=1}^{T}(c^{t}+c^{t-1})^{}w_{t}-_{x(0,1)}_{t=1} ^{T}(c^{t}+c^{t-1})^{}x 4^{t_{1}}\|c^{t}+c^{t-1}\|_{2}^{2} }+( T)\]

Lemma 4.1 guarantees that Adaptive \(\) admits only \(o()\) alternating regret in case \(t_{1}=o(T)\). Using Lemma 4.1, we establish Lemma 4.2 which is the cornerstone of our algorithm and guarantees that once Adaptive \(\) is _appropriately_ mixed with unnormalized best-response (\(-c^{t-1}\)), then the resulting algorithm always admits \(( T)\) regret.

**Lemma 4.2**.: _Let \(w_{1},,w_{T}(0,1)\) be produced by Adaptive FTRL given as input \(c^{1},,c^{T}(0,1)\) and \(t_{1}\) be the maximum round such that \(_{s=1}^{t}(c^{s}+c^{s-1})^{}w_{s}-_{s=1}^{t}\|c^{s}+c^{s-1}\|_{ 2}^{2}/4\)._Let \(p:=20/^{t_{1}}\|c^{t}+c^{t-1}\|_{2}^{2}}\) and let \(y_{t}:=(1-p)w_{t}-pc^{t-1}\) for \(t t_{1}\) and \(y_{t}:=w_{t}\) for \(t t_{1}+1\). Then \(_{t=1}^{T}(c^{t}+c^{t-1})^{}y_{t}-_{x(0,1)}_{t=1}^ {T}(c^{t}+c^{t-1})^{}x( T)\).

Lemma 4.2 establishes that in case at Step \(5\), Algorithm 4 mixed the output \(w_{t}\) of Adaptive FTRL with the unormalized best-response (\(-c^{t-1}(0,1)\)) as follows,

\[y_{t}:=(1-q_{t}) w_{t}+q_{t}(-c^{t-1})q_{t}:=[t t_{1}]}{^{t_{1}}\|c^{t}+ c^{t-1}\|_{2}^{2}}},\] (3)

then it would admit \(( T)\) alternating regret. Obviously, Algorithm 4_does not know a-priori_ neither \(t_{1}\) nor \(_{t=1}^{t_{1}}\|c^{t}+c^{t-1}\|_{2}^{2}\). However by using the recent result of  for _Online Gradient Descent in Shrinking Domains_, we can establish that the _mixing coefficients_\(p_{t}\) selected by Algorithm 4 at Step \(7\), admit the exact same result as selecting \(q_{t}\) described in Equation 3. The latter is formalized in Lemma 4.3.

**Lemma 4.3**.: _Let the sequences \(w_{1},,w_{T}(0,1)\) and \(p_{1},,p_{T}(0,1)\) produced by Algorithm 4 given as input \(c^{1},,c^{T}(0,1)\). Additionally let \(t_{1}\) denote the maximum time such that \(_{s=1}^{t}(c^{s}+c^{s-1})^{}w_{s}-_{s=1}^{t}\|c^{s}+c^{s- 1}\|_{2}^{2}/4\) and consider the sequence \(q_{t}:=[t t_{1}](20/^ {t_{1}}\|c^{t}+c^{t-1}\|_{2}^{2}})\). Then,_

\[_{t[T]}(c^{t-1}+c^{t})^{}(w_{t}+c^{t-1}) q_{t}-_{t[T]} (c^{t-1}+c^{t})^{}(w_{t}+c^{t-1}) p_{t}( T)\]

## 5 Conclusion

In this paper we introduced a variant of the Online Linear Optimization that we call Alternating Online Linear Optimization for which we developed the first online learning algoithms with \(o()\) regret guarantees. Our work is motivated by the popular setting of alternating play in two-player games and raises some interesting open questions. The most natural ones is understanding whether \(}(1)\) regret guarantees can be established the \(n\)-dimensional simplex as well as establishing \(o()\) for general convex losses.

**Limitations:** The current work is limited to the linear losses setting. Notice that the classic reduction from convex to linear losses in Standard OLM no longer holds in Alternating OLM. Therefore the generalization to general convex losses seems to require new techniques. We defer this study for future work.