# Delayed Algorithms for Distributed Stochastic Weakly Convex Optimization

Wenzhi Gao

Stanford University

gwz@stanford.edu

&Qi Deng

Shanghai University of Finance and Economics

qideng@sufe.edu.cn

Work done while at SHUFE.The corresponding author.

###### Abstract

This paper studies delayed stochastic algorithms for weakly convex optimization in a distributed network with workers connected to a master node. Recently, Xu et al. 2022 showed that an inertial stochastic subgradient method converges at a rate of \((_{}/)\) which depends on the maximum information delay \(_{}\). In this work, we show that the delayed stochastic subgradient method (DSGD) obtains a tighter convergence rate which depends on the expected delay \(\). Furthermore, for an important class of composition weakly convex problems, we develop a new delayed stochastic prox-linear (DSPL) method in which the delays only affect the high-order term in the complexity rate and hence, are negligible after a certain number of DSPL iterations. In addition, we demonstrate the robustness of our proposed algorithms against arbitrary delays. By incorporating a simple safeguarding step in both methods, we achieve convergence rates that depend solely on the number of workers, eliminating the effect of the delay. Our numerical experiments further confirm the empirical superiority of our proposed methods.

## 1 Introduction

In this paper, we consider the following stochastic optimization problem

\[_{x^{n}}(x)_{}[f(x,)]+ (x),\] (1)

where \(f(x,)\) is a nonconvex continuous function over \(x\) and \(\) is a random variable sampled from some distribution \(\); \((x)\) is lower-semicontinuous and proximal-friendly. We assume that both \(f(x,)\) and \((x)\) belong to a general class of nonsmooth nonconvex functions that exhibit weak convexity. Here, we say that a function \(g(x)\) is \(\)-weakly convex if \(g(x)+\|x\|^{2}\) is convex for some \( 0\). Weakly convex optimization has attracted growing interest in machine learning in recent years, and we are particularly interested in a general type of weakly convex problems with the following composition structure 

\[f(x,)=h(c(x,)),\] (2)

where \(h\) is a convex Lipschitz function and \(c(x,)\) is smooth. Optimization in the above composition form is pervasive in applications arising from machine learning and data science, including robust phase retrieval , blind deconvolution , robust PCA and matrix completion , among others.

Stochastic (sub)gradient method and its proximal variants [39; 10; 30; 28; 31] (all referred as SGD in our paper) are arguably the most popular approaches for solving problem (1). Typically, SGD iteratively solves \(x^{k+1}=_{x}\ \{ f^{}(x^{k},^{k}),x-x^{k}+ (x)+}{2}\|x-x^{k}\|^{2}\},\) where \(^{k}\) isa random sample and \(f^{}(x^{k},^{k})\) denotes a subgradient of \(f(x^{k},^{k})\). However, despite its wide popularity in machine learning, the sequential and synchronous nature of SGD is not suitable for modern applications that require parallel processing in multi-cores or over multiple machines.

To further improve SGD in parallel and distributed environments, recent work considers a more practical asynchronous setting where the parameter updates allow outdated gradient information. See . In the asynchronous setting, it is crucial to know how the stale updates based on delayed information affects convergence. For smooth convex optimization, Agarwal and Duchi  show that delayed stochastic gradient method (DSGD) obtains a rate of \((/+_{^{2}}/(^{2}K))\), where \(_{^{2}}\) bounds of the second moment of random delays. DSGD with adaptive stepsize has also been studied by  to achieve better empirical performance and the latest work  further improves the rate to \((/+_{}/K)\). For general smooth (nonconvex) problems, the work  shows that DSGD converges to stationarity at a rate of \((/+_{}/K)\). In a follow-up study , the authors propose a more robust DSGD whose rate depends on the average delay \(_{}\), rather than the maximum delay. Recently, Koloskova et al.  develop a sharp analysis for asynchronous SGD and then a simple delay-adaptive stepsize to achieve the best rate \((/+_{}/K)\). Based on novel delay-adaptive stepsizes and virtual iterate-based analysis, a concurrent work  has established new convergence rates that depend on the number of workers rather than the delay of the gradient.

Despite much progress in distributed smooth optimization, it remains unclear how to develop efficient asynchronous algorithms for nonsmooth and even nonconvex problems. For general nonsmooth convex problems, the pioneering work  has shown that the asynchronous incremental subgradient method obtains an \((}/K})\) convergence rate. The aforementioned work  shows that DSGD achieves a delay-independent rate of \(()\), where \(m\) is the number of workers. However, it is still unknown whether their key technique by telescoping the virtual iterates can be extended to composite nonconvex optimization.

For distributed weakly convex optimization, Xu et al.  shows that Delayed Stochastic Extrapolated Subgradient (DSEGD), an inertial version of DSGD, exhibits a convergence rate of \(((1+_{})/+_{}^{2}/K)\), which has an undesired dependence on the maximum delay \(_{}\). This issue is further exacerbated in real heterogeneous environments where the maximum delay is dictated by the slowest, or the "straggler" nodes. It remains unclear

_1) whether such delay dependence in DSGD is still improvable or not for weakly convex problems?_

Nevertheless, even if the rate is improvable in terms of \(\), there yet remains a fundamental challenge, as the delay significantly influences the leading term \((1/)\) of the convergence rate. This contrasts with smooth distributed optimization, where the delay only affects a higher order term \((1/K)\) and hence is negligible in the long run. Such a performance gap highlights a substantial limitation of the prior study when nonsmoothness is present. Hence, it is natural to ask: For distributed weakly convex optimization,

_2) can we design algorithms with a negligible penalty caused by delays?_

_3) Moreover, can we make convergence rates delay independent?_

The goal of this paper is to address the above three questions.

ContributionsIn this paper, we answer the above questions positively (Table 1). Our contributions are as follows.

* For distributed weakly convex optimization, we provide a sharp convergence analysis of DSGD under statistical assumptions on the delays and obtain a rate of \((/+_{^{2}}/K)\), where \(,_{^{2}}\) are respectively first and second moments of stochastic delays. Our result significantly improves upon the previous \((_{}/+_{}^{2}/K)\) rate for DSGD.
* For weakly convex problems with composition structure (2), we propose a new delayed stochastic prox-linear method (DSPL) which can exploit the structure (2) more effectively. Unlike SGD, the stochastic prox-linear algorithm () partially linearizes the inner function \(c(,)\) while retaining the outer function \(h()\). Then it iteratively solves:\( c(x^{k},),x-x^{k})+(x)+}{2}\|x-x^{k} \|^{2}}\). To the best of our knowledge, this is the first study of SPL in the asynchronous distributed setting. We show that the new DSPL method achieves a rate of \((1/+_{^{2}}/K)\), which is consistently better than that of DSGD in terms of the dependence on delay. Interestingly, our result implies that the delay is negligible when \(K\) is sufficiently large.
3. We propose a simple yet effective safeguarding step that skips the iteration when the delay is significantly large. This enhancement ensures that the rate depends only on the number of workers rather than on the delay explicitly. Specifically, in an environment of \(m\) workers, we obtain an \((/+m/K)\) rate for DSGD and \((1/+m^{2}/K)\) for DSPL, making both methods robust to arbitrary delays. As per our knowledge, these are the first delay-independent rates for distributed nonsmooth nonconvex optimization. Prior to our work, delay-independent rates were only known for smooth or convex optimization , which were derived through a distinctly different delay-adaptive stepsize strategy.

Structure of the paperSection 2 introduces the notations and problem setup. Section 3 analyzes the delayed stochastic (proximal) subgradient method (DSGD). Section 4 develops the delayed stochastic prox-linear method (DSPL). Section 5 proposes a simple strategy to make the asynchronous algorithms robust to arbitrary delays. Section 6 conducts experiments to verify our theoretical results. We draw conclusions in Section 7 and leave all the proofs and further discussions in the appendix.

## 2 Preliminaries

NotationsWe use \(\|\|\) and \(,\) to denote the Euclidean norm and inner product. The sub-differential of a function \(f\) is defined by \( f(x)\{v:f(y) f(x)+ v,y-x+o(\|x-y\|),y x\}\) and \(f^{}(x) f(x)\) denotes a subgradient. If \(0 f(x)\), then \(x\) is called a stationary point of \(f\). The domain of \(f\) is defined by \(f\{x:f(x)<\}\). At iteration \(k\), we use \(_{k}[]\) to denote the expectation conditioned on past iterations \(\{x^{1},,x^{k-1}\}\). \(1\!\{\}\) denotes the 0-1 indicator function of an event. Given a delay sequence \(\{_{k}\}\), we write \(_{1}_{k=1}^{K}_{k}\) and \(_{2}_{k=1}^{K}_{k}^{2}\).

Our analysis will adopt the Moreau envelope as the potential function, a technique initially identified in the work . Let \(f\) be a \(\)-weakly convex function. Given \(>\), the Moreau envelope and the associated proximal mapping of \(f\) are respectively defined by

\[f_{1/}(x)_{y}\{f(y)+\|x-y\|^{2}\} _{f/}(x)*{arg\,min} _{y}\{f(y)+\|x-y\|^{2}\}.\]

By the optimality condition and convexity of \(f(y)+\|x-y\|^{2}\), \(0 f(_{f/}(x))+(_{f/}(x)-x)\). The Moreau envelope can be nicely interpreted as a smooth approximation of the original function. Specifically, it can be shown that \(f_{1/}(x)\) is differentiable and its gradient is \( f_{1/}(x)=(x-_{f/}(x))\) (See ). Combining the above two relations, we obtain \( f_{1/}(x) f(_{f/}(x))\). Therefore, the Moreau envelope can be used as a measure of approximate stationarity: if \(\| f_{1/}(x)\|\), then \(x\) is in the proximity (i.e. \(\|x-_{f/}(x)\|^{-1}\)) of a near stationary point \(_{f/}(x)\) (i.e. \(( f(_{f/}(x)),0)\)).

   Delay & Work & Setting & Problems / algorithms & Convergence rate \\   &  & & \(f+\) / DSGD & \((}}{}+}^{2}}{K})\) \\  & Ours & weakly convex & \(f+\) / DSGD & \((}}{}+}^{2}}{K})\) \\  & & & \(h e+\) / DSPL & \((}}{}+}^{2}}{K})\) \\   &  & smooth nonconvex & \(f\) / DSGD & \((}+)\) or \((}+}}{K})\) \\  & & & \(f+\) / DSGD & \((}}{}+}}{K})\) \\   & & & \(h e+\) / DSPL & \((}+}{K})\) \\   

Table 1: Rates of delayed algorithms for nonconvex optimization. \(m\): the agent number; \(_{}\): the maximum delay; \(=[_{k}]\); \(_{^{2}}=[_{k}^{2}]\); \(_{}\) average over arbitrary delays.

### Assumptions

Throughout the paper, we make the following assumptions.

1. (i.d. sample) We draw i.i.d. samples \(\{^{k}\}\) from \(\).
2. (Lipschitz continuity) \((x)\) is \(L_{}\)-Lipschitz continuous over its domain.
3. (Weak convexity) \(\) is \(\)-weakly convex.
4. (Bounded moments) The distribution of the independent stochastic delays \(\{_{k}\}\) has bounded first and second moments. i.e., \([_{k}]<,[_{k}^{2}]_{ ^{2}}<, k\).

_Remark 1_.: Assumptions **A1** to **A3** are typical in stochastic weakly convex optimization , while **A4** is common in distributed optimization [3; 32]. Moreover, throughout our analysis, we regard \(\{_{k}\}\) as a pre-defined random sequence independent of data sampling and the algorithm [32; 34].

## 3 Delayed proximal subgradient method

In this section, we analyze the convergence rate of the delayed stochastic proximal subgradient method (DSGD) for weakly convex optimization. DSGD is the workhorse of most applications and is frequently analyzed in the literature on centralized distributed optimization.

**Input:**\(x^{1}\);

**for**\(k\) = 1, 2,... **do**

Let \(g^{k-_{k}} f(x^{k-_{k}},^{k-_{k}})\) be computed by a worker with delay \(_{k}\);

In the master node update

\[x^{k+1}=*{arg\,min}_{x^{n}} g^{k- _{k}},x-x^{k}+(x)+}{2}\|x-x^{k}\|^{2}\}.\] (3)

**end**

**Algorithm 1**Delayed stochastic proximal subgradient method

We restrict \(f\) to have a bounded subgradient and make the following extra assumption.

1. \(f(x,)\) is \(\)-weakly convex, \(_{}[\|g\|^{2}] L_{f}^{2},g f(x,)\) for all \(x(f)\) and all \(\).

Now we are ready to present the convergence analysis of DSGD. Our analysis relies on Moreau envelope, denoted as \(_{1/}(x^{k})\), which serves as the potential function. After assessing the descent of the potential function with delays separated as an error term, we derive the convergence result by bounding the stochastic delays. The following lemma provides the descent property.

**Lemma 1**.: _Let \(^{k}:=_{/}(x^{k})\). Suppose **A1**, **A2**, **A3** and **B1** hold. If \(>2+,_{k}\), then_

\[-2- )}\|^{k}-x^{k}\|^{2} _{1/}(x^{k})-_{k}[_{1/}(x^{k+1})]- -)}{2(_{k}-2-)}_{k}[\|x ^{k+1}-x^{k}\|^{2}]\] \[+-2-}_{ k}[\|x^{k+1}-x^{k-_{k}}\|^{2}]+}{_{k}-2- }_{k}[\|x^{k+1}-x^{k-_{k}}\|]\]

_Remark 2_.: **Lemma 1** shows that aside from the noise and delay-related terms, unless we are close to approximate stationarity characterized by \(\|^{k}-x^{k}\|^{2}\), there is always sufficient decrease in the potential function. Intuitively, if we take sufficiently large regularization \(\) and bound the delays using **A4**, convergence is almost immediate. Now we show convergence of DSGD in **Theorem 1**.

**Theorem 1**.: _Under the same conditions as_ **Lemma 1** _as well as_ **A4**_, taking \(_{k}=+4++/\) for some \(>0\), letting \(k^{*}\) be chosen between 1 and \(K\) uniformly at random, then_

\[\|_{1/}(x^{k^{*}})\|^{2}+}+(L_{f}+L_{})}{}(_ {1}+1)++L_{})^{2}^{2}}{K}_{2},\]

_where \(D=_{1/}(x^{1})-_{x}(x)\) and recall our notation \(_{1}=^{K}}_{k},_{2}=^{K}}_{k}^{2}\)._

_Remark 3_.: Note that \(\) controls the trade-off between noise, delay and optimization. In practice we can set \(\) as a hyper-parameter and tune it to improve performance.

[MISSING_PAGE_FAIL:5]

From **Proposition 1**, we see \(f(x,)\) is \(L_{h}C()\)-weakly convex since the error between \(f(x,)\) and a convex function is bounded by a quadratic function. Furthermore, we know that \(f(x)\) is \(L_{h}L_{c}\) Lipschitz-continuous and \(L_{h}C\)-weakly convex after taking expectation. For a unified analysis, we take \(L_{f}=L_{h}L_{c},=L_{h}C\) and use these constants to present the results. The following lemma characterizes the descent property for our potential function in DSPL.

**Lemma 2**.: _Suppose_ **A1**_,_ **A2**_,_ **A3** _and_ **C1** _hold, if \(>2+,_{k}\), then_

\[-2-)} \|^{k}-x^{k}\|^{2} _{1/}(x^{k})-_{k}[_{1/}(x^{k+1})]- -)}{2(_{k}-2-)}_{k}[ \|x^{k+1}-x^{k}\|^{2}]\] \[+^{2}}{(_{k}-)(_{k}-2 -)}+-2-)}_{k}[\|x^{k+1}-x^{k-_{k}}\|^{2}].\]

Similar to the analysis of DSGD, we bound the delays to give the convergence result.

**Theorem 2**.: _Under the same conditions as_ **Lemma 2** _as well as_ **A4**_, taking \(_{k}=+6++/\) for some \(>0\), letting \(k^{*}\) be uniformly chosen between \(1\) and \(K\), then_

\[[\|_{1/}(x^{k^{*}})\|^{2}]+}+ ^{2}}{}++L_{}) ^{2}^{2}}{K}_{2},\]

_where \(D=_{1/}(x^{1})-_{x}(x)\)._

If we take \([_{2}]=_{^{2}}\), then Theorem 2 implies that

\[[\|_{1/}(x^{k^{*}})\|^{2}]=(}+)\;\;\;\;[\|_{ 1/}(x^{k^{*}})\|^{2}]=(}+}}{K}).\]

Compared to DSGD, the delays for DSPL appear only in a higher-order term with respect to \(K\). Different from DSGD where \(\) only characterizes weak convexity, \(\) for DSPL also represents the quadratic upper-bound on \(f(x,)\) in **P2**, which is not 0 even if the problem is convex.

Dspl vs. DsgWe give some further insights on the behavior of DSPL and DSGD for the composition function (2). Intuitively, as the algorithm converges, we have \(_{k 0}\|x^{k}-x^{k+1}\|=0\) a.s. When \(x^{k} x^{k-_{k}}\), DSPL enjoys an increasingly stable estimation of the proximal mapping (4), as the influence of delay is diminishing and the error is mainly driven by stochastic sampling. The same conclusion holds on smooth DSGD due to the Lipschitz continuity of \( f\). On the other hand, when DSGD is applied for a nonsmooth problem, the master node receives an out-of-date subgradient \(f^{}\) from the worker and solves (3). Since \(f(x,)\) is nonsmooth, the subgradient \(f^{}(x^{k-_{k}},)\) may differ significantly from \(f^{}(x^{k},)\) even when the sequence \(\{x^{k}\}\) converges. Hence, DSGD will constantly suffer from delay during all the updates (3).

Dspl with momentumWe also remark that the momentum technique from DSGD can be extended to DSPL, which gives us the same \((_{}/+_{}^{2}/K)\) convergence rate as in . We refer the interested readers to the Appendix F.

When \(f\) is smooth, the analysis of DSPL can be adapted to yield a comparable convergence rate for the proximal stochastic gradient method for minimizing \(()\).

**Theorem 3**.: _Suppose all the assumptions in_ **Lemma 1** _and_ **A4** _hold, and that \(f\) is \(\)-smooth. Let \(_{k}=+6++/\) for some \(>0\), then \([\|_{1/}(x^{k^{*}})\|^{2}]=(}+}{K})\)._

So far we have spent two sections analyzing the two algorithms so that enough intuition can be established. All these serve for our ultimate goal: making DSGD and DSPL robust to arbitrary delays.

Figure 1: DSGD and DSPL in a master-worker architecture.

Weakly convex optimization robust to arbitrary delays

This section proposes robust variants of DSGD and DSPL, for which the explicit delays are eliminated from the convergence rate. What we will do is reduce the impact of delay to _the number of agents in the network_. Moving forward, we substitute **A4** with **D1**.

**D1:**: The distributed environment has \(m\) workers.

The previously established results have provided sufficient intuition on how delays impact our algorithms. In view of **Theorem 1** and **Theorem 2**, we have isolated delay-dependent \((_{1}/)\) and \((_{2}/K)\) in the proof. Although \(_{1}/\) seems larger, it turns out that \(_{2}\) stands in our way. The following lemma shows \(_{1}=_{k=1}^{K}_{k}\) is bounded by \(m\).

**Lemma 3**.: _In a distributed environment of \(m\) workers \(_{1}=_{k=1}^{K}_{k} m-1 m\)._

Given **Lemma 3**, we know \(_{1}/=(m/)\) and we can replace dependency on \(_{1}\) by \(m\). Now we consider \(_{2}=_{k=1}^{K}_{k}^{2}\). Even if we have a larger denominator \(K\) neutralizing the effect of \(_{2}\), the following example shows that in the worst case, \(_{2}\) can be of \((K)\) and result in \((1)\) error.

**Example 1** (Why \(_{2}\) hurts performance).: _Given sequence of delays \(\{_{k}\}\) such that \(_{k}=0,k K-1,_{K}=K\). Then \(_{2}=K\) and \(_{2}/K=1\)._

The example tells that delays of \((K)\) ruin our convergence. In other words, to recover an overall \((1/)\) convergence rate, we need \(_{2}/K=(1/)_{k=1}^{K}_{k}^{2}= (K^{3/2})\). The next lemma provides a hint for our algorithm design.

**Lemma 4**.: _If a sequence of nonnegative integers \(\{_{k}\}\) satisfy \(_{k=1}^{K}_{k} mK\), then given \(T 0\),_

1. _if_ \(_{k} T\) _for all_ \(k\)_, then_ \(_{k=1}^{K}_{k}^{2} mKT\)_;_
2. \(_{k=1}^{K}\{_{k} T\} K-mKT^{-1}\)_._

**Lemma 4** tells us two facts about a nonnegative integer sequence of length \(K\) with sum bounded by \((K)\): **1)**. if we restrict the elements to be less than \((T)\), then \(_{k=1}^{K}_{k}^{2}(KT)\). **2)**. there will be \((K-mKT^{-1})\) elements bounded by \(T\). Back to our context, this implies **1)**. To reduce \(_{2}/K\) to \((1/)\), we can discard the iterations of delays greater than \(T=()\). **2)**. We skip no more than \(()\) iterations and optimization works with \((K-)=(K)\) iterations left.

``` Input:\(x^{1},T=r^{-1}mK^{}\); for\(k=1\), 2,... do if\(_{k} T\)then  Update with (3) for DSGD or (4) for DSPL else \(x^{k+1}=x^{k}\)  end if end while ```

**Algorithm 3**Safeguarded DSGD/DSPL

Having established the foundational understanding, we outline the main steps in **Algorithm 3**. It is based on the aforementioned intuition and incorporates a "safeguarding" step to discard inaccurate information which could potentially hurt the convergence performance. As the above argument on the accumulated delay is independent of any specific algorithm, the safeguarding strategy can be applied to both DSGD and DSPL. To make our parameter setting more general, in our analysis we consider \(T=r^{-1}mK^{},r>0, 0\).

Intuitively, under worst-case scenarios, **Algorithm 3** will perform after \(K\) iterations in a manner similar to its non-safeguarded counterpart. However, the maximum delay will be capped at \(r^{-1}mK^{}\), and the iteration count will be \(K(1-rK^{-})\).

**Theorem 4** (Safeguarded DSGD).: _Under the same conditions as **Lemma 1** as well as **D1**, taking **1)**\(>0,K>r^{1/}\) or **2)**\(=0,r<1\), then letting \(_{k}=}{}+++4, =1+-r}\) for some \(>0\) and \(k^{*}\) be uniformly chosen between iterations where \(_{k} T=r^{-1}mK^{}\),_

\[[\|_{1/}(x^{k^{*}})\|^{2}]+D}{ }+ L_{f}(L_{f}+L_{})m}{}+ (L_{f}+L_{})^{2}m^{2}^{2}}{rK^{1-}} ,\]

_where \(D=_{1/}(x^{1})-_{x}(x)\)._

**Theorem 5** (Safeguarded DSPL).: _Under the same conditions as_ **Lemma 2** _as well as_ **D1**_, taking **I)**__\(>0,K>r^{1/}\) or **2)**__\(=0,r<1\), then letting \(_{k}=}{}+++6,=1+-r}\) for some \(>0\) and \(k^{*}\) be uniformly chosen between iterations where \(_{k} T=r^{-1}mK^{}\),_

\[[\|_{1/}(x^{k^{*}})\|^{2}]+D}{ }+ L_{f}}{}+(L_{f}+L_{})^{2}m^{2}^{2}}{rK^{1-}}.\]

_where \(D=_{1/}(x^{1})-_{x}(x)\)._

_Remark 5_.: **Theorem 4** and 5 show that by employing a safeguarding step, both DSGD and DSPL can achieve delay-independent rates. It is also interesting to see how the choice of \(\) and \(r\) affects performance. To recover a convergence rate of \((K^{-1/2})\), it is sufficient to have \( 1/2\). If we set \(>0\), then we skip \(rK^{-}\) of all the iterations and incur a penalty of up to \(^{3/2}\) in DSGD and \(^{1/2}\) in DSPL. However, this loss becomes negligible for large \(K\), as \(=1+-r} 1\). Alternatively, if we set \(=0\), DSGD achieves \((/+m/K)\) rate with \(=1/\) while DSPL yields a rate of \((1/+m^{2}/K)\) with \(=1\). This setting aligns with [7; 18] and achieves optimal rate on \(m\). However, the penalty from \(>1\) is non-negligible and adversely affects the overall convergence rate by a constant factor. Therefore, we can strike a balance in practice by choosing \(\) in \((0,1/2]\) and allow delays of up to \((K^{})\).

## 6 Experiments

This section performs numerical experiments on the robust phase retrieval problem to demonstrate the efficiency of our methods. Given a measuring matrix \(A^{m n}\) and a set of observations \(b_{i}| a_{i},|^{2},1 i m\) (\(m\) in this section represents the number of samples), robust phase retrieval aims to recover the true signal \(\) from

\[_{x^{n}}_{i=1}^{m}| a_{i},x^{2} -b_{i}|+_{\{x:\|x\| M\}},\]

where \(_{S}\) denotes the set indicator function and the \(_{1}\) loss function improves robustness. Our experiment contains three parts. The first part profiles algorithms in an asynchronous environment simulated via MPI; our second experiment runs sequentially with simulated delays from common distributions; our last experiment also runs in the simulated environment and demonstrates the effectiveness of safeguarding step under adversarially chosen delays.

### Experiment setup

**Synthetic data**. For the synthetic data, we take \(m=300,n=100\) in the experiments of simulated delay and \(m=1500,n=500\) in the asynchronous environment. Data generation follows the setup of , where, given some \( 1\), we compute \(A=QD,Q^{m n},q_{ij}(0,1)\) and \(D=(d),d^{n},d_{i}[1/,1]\) for all \(i\). Then we generate a true signal \((0,I)\) and obtain the measurements \(b\) using formula \(b_{i}= a_{i},^{2}\). Last we randomly choose \(p_{}\)-fraction of the measurements and add \((0,25)\) to them to simulate data corruption.

**Real-life data**. The real-life data is generated from zipcode dataset, where we vectorize a 16\(\)16 hand-written digit from  and use it as the signal. The measuring matrix \(A\) comes from a normalized Hadamard matrix \(H^{256 256}\): we generate three diagonal matrices \(S_{j}=(s_{j}),j=1,2,3\); each element of \(s_{j}^{256}\) is taken from \(\{-1,1\}\) randomly and we let \(A=H[S_{1},S_{2},S_{3}]^{}^{768 256}\). Finally \(p_{}\)-fraction of the observations are set \(0\).

**1) Dataset**. In the asynchronous environment, we keep up with  setting \(=1,p_{}=0\) and in the simulated environment, we follow  setting \(\{1,10\}\) and \(p_{}\{0.2,0.3\}\).

2. **Initial point and radius**. Synthetic data: we generate \(x^{}(0,I_{n})\) and start from \(x^{1}=}{\|x^{}\|}\); zipcode data: we generate \(x^{}(,I_{n})\) and take \(x^{1}=10x^{}\). \(M=1000\|x^{1}\|\).
3. **Stopping criterion**. We run algorithms for 400 epochs (\(K=400m\)). In the asynchronous environment, algorithms run until reaching the maximum iteration. In the simulated environment, algorithms stop if \(f 1.5f()\). When \(f\) contains corrupted measurements, \(f()>0\).
4. **Stepsize**. We set \(=\), where \(\{0.1,0.5,1.0\}\) in the asynchronous environment, \([10^{-2},10^{1}]\) for synthetic data and \([10^{1},10^{2}]\) for the zipcode dataset.
5. **Simulated delay**. In the simulated environment, we generate \(_{k}\) from two common distributions from literature, which are geometric \((p)\) and Poisson \(()\). After the delay is generated, it is truncated by twice the mean of the distribution.
6. **Adversarial delay**. We let delay happen at the last iteration of each epoch and use the information of \(x^{1}\) to update. Safeguarding parameter is set to \(T=0.1\).
7. **Trade-off between computation and communication**. In the asynchronous environment, the numerical linear algebra on the worker uses a raw implementation (not importing package) to balance the cost of gradient computation and communication.

### Asynchronous environment

Our first experiment runs in an asynchronous environment implemented by MPI Python interface and is profiled on an Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz machine with 10 cores and 20 threads. This experiment runs on a single machine to verify our theoretical finding rather than to test the algorithm's real performance on a specific distributed architecture.

The first figure plots the wall-clock time (in seconds) for DSGD and DSPL to complete 400 epochs when the number of workers increases. It is observed that both algorithms exhibit speed-up with more workers and note that DSPL takes more time than DSGD due to the need to pass the function value to the master and slightly more expensive updates. But as the second and the third figure suggest, this extra cost is justified by the superior convergence: in the first several epochs DSPL reaches a high accuracy of \(10^{-6}\) in both function value and distance to the optimal solution, while DSGD stagnates at a relatively low-accuracy solution of \(10^{-2}\). These observations suggest that DSPL offers better convergence behavior than the methods only based on subgradient. Finally, our experiments suggest both DSGD and DSPL are not sensitive to the increase in the number of workers when there are relatively few workers.

### Simulated environment

The second part of our experiment compares the performance between DSGD and DSPL and is based on the simulated delay, where the algorithm runs sequentially but the gradient information is computed from the previous iterates.

Figure 3 plots the number of iterations for each algorithm to converge under different datasets and delay parameters. We see that in spite of delays, DSPL admits a wider range of stepsize parameters ensuring convergence than DSGD, and the performance slightly deteriorates as delay increases.

Figure 2: First: speedup in time and the number of workers. Second: progress of \(\|x^{k}-\|\) in the first 40 epochs given \(=0.1\). Third: \(f(x^{k})-f()\) in the first 40 epochs given \(=0.5\). For more details about the two figures on the right, please refer to Figure 12 in the appendix.

### Adversarial delay

The final experiment verifies the efficacy of our safeguarding step when delays are introduced in an adversarial setting. We evaluate the performance of DSGD, DSPL, both with and without the safeguarding step, under the delay patterns we have generated.

Figure 4 clearly illustrates the superior performance of our method incorporating the safeguarding step. As suggested by our theory, DSGD is notably sensitive to delays of \((K)\). However, upon discarding outdated information, DSGD exhibits much greater robustness. Interestingly, even under our adversarial setup, the performance of DSPL remains acceptable, albeit slightly inferior to its safeguarded version, which aligns well with our theoretical findings.

## 7 Conclusions

We offer a sharp analysis of delayed stochastic algorithms for weakly convex optimization, discussing the widely utilized DSGD method and introducing the novel DSPL method for problems with a composition structure. Through careful examination of delay factors, we propose a straightforward safeguarding approach to eliminate the effect of delays, and instead derive bounds depending on the number of agents in the distributed environment. This makes our algorithms resilient to arbitrary delays. A promising future direction is the application of these prox-linear methods in more diverse distributed settings, such as decentralized networks.

## 8 Acknowledgement

The authors thank the anonymous reviewers for their constructive suggestions. This research is partially supported by National Natural Science Foundation of China (NSFC-72150001, 11831002).