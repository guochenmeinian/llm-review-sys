# RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions

Lingdong Kong\({}^{1,2}\) Shaoyuan Xie\({}^{3}\) Hanjiang Hu\({}^{4}\) Lai Xing Ng\({}^{5,6}\)

\({}^{1}\)National University of Singapore \({}^{2}\)CNRS@CREATE

\({}^{3}\)Huazhong University of Science and Technology

\({}^{4}\)Carnegie Mellon University \({}^{5}\)Institute for Infocomm Research, A*STAR

\({}^{6}\)IPAL, CNRS IRL 2955, Singapore \({}^{7}\)CerCo, CNRS UMR 5549, Universite Toulouse III

https://github.com/ldkong1205/RoboDepth

###### Abstract

Depth estimation from monocular images is pivotal for real-world visual perception systems. While current learning-based depth estimation models train and test on meticulously curated data, they often overlook out-of-distribution (OoD) situations. Yet, in practical settings - especially safety-critical ones like autonomous driving - common corruptions can arise. Addressing this oversight, we introduce a comprehensive robustness test suite, _RoboDepth_, encompassing **18** corruptions spanning three categories: _i)_ weather and lighting conditions; _ii)_ sensor failures and movement; and _iii)_ data processing anomalies. We subsequently benchmark **42** depth estimation models across indoor and outdoor scenes to assess their resilience to these corruptions. Our findings underscore that, in the absence of a dedicated robustness evaluation framework, many leading depth estimation models may be susceptible to typical corruptions. We delve into design considerations for crafting more robust depth estimation models, touching upon pre-training, augmentation, modality, model capacity, and learning paradigms. We anticipate our benchmark will establish a foundational platform for advancing robust OoD depth estimation.

## 1 Introduction

Monocular depth estimation (MDE) involves predicting a scene's depth information from monocular images, without relying on data acquired from more sophisticated sensors [11; 28; 25; 37]. These images are predominantly captured using RGB cameras mounted on diverse platforms like drones, mobile robots, and vehicles [65; 41; 9; 30]. As an instrumental facet of visual perception, precise MDE paves the way for a broad array of applications. Bolstered by the rise of learning-based paradigms, numerous MDE algorithms have emerged, demonstrating remarkable depth estimation performances on standard benchmark datasets [14; 52; 49; 8; 47].

However, the resilience of existing MDE models to out-of-distribution (OoD) challenges is yet to be thoroughly explored, especially under the lens of real-world corruptions such as adverse weather [19; 46] and sensor malfunctions . The prevailing learning-based visual perception models often display heightened sensitivity to nuances in lighting, noise, texture variations, among other factors, which are compromising the accuracy of depth predictions [18; 23]. The ability to generalize across new scenes, objects, and backgrounds, especially when they have not been part of the training data, is another pivotal challenge .

Despite the strides achieved on relatively pristine datasets [14; 49; 8], a lacuna exists: a robustness benchmark tailored to foster the evolution of resilient and scalable MDE systems. In light of thesechallenges, models dedicated to MDE often inadvertently embed systematic errors, stemming from real-world image imperfections like altered lighting, motion blur, shadows, and data compression, which the current MDE solutions rarely address effectively .

Seeking to bridge this gap, our contribution charts the inaugural path towards robust and reliable MDE, unveiling the _KITTI-C_, _NYUDepth2-C_, and _KITTI-S_ benchmarks. Contrasting prior works that merged datasets for cross-domain MDE  or devised adversarial patches to subvert MDE models , our benchmarks meticulously simulate commonplace corruptions that are intrinsic to real-world settings. As delineated in Fig. 2, we structure eighteen corruption varieties across _three_ cardinal categories: _i)_ weather and lighting conditions, _ii)_ sensor malfunctions and movements, and _iii)_ data processing complications. Further stratified by diverse severity, these corruptions encapsulate a gamut of scenarios fostering image distortions, texture shifts, or degraded visuals .

Given that MDE models intrinsically depend on lucid visual cues for depth inference, the aforementioned corruptions naturally impose significant hurdles. Our preliminary analysis, visualized in Fig.1, showcases a spectrum of responses from distinct MDE model architectures when faced with diverse corruptions. Penetrating into these dynamics is quintessential to understanding the underlying causes of performance fatering, enabling us to architect MDE models that are both robust and reliable. Pursuing this vision, we undertake a meticulous benchmarking of extant MDE models on these new datasets, embarking on an exhaustive study of their robustness vis-a-vis the spectrum of corruptions. We probe queries about the resilience of MDE models to real-world corruptions, the influence of training input modalities, and learning paradigms - addressed in Sec.4.2. Concurrently, we assess the fidelity of our simulated corruptions and delve into the impact of texture-shift corruptions (style alterations) on gauging MDE model resilience.

From our benchmark findings, we distill several intriguing insights and profer recommendations to amplify robustness - focusing on strategies like model pre-training, input resolution tuning, model sizing, complexity modulation, and corrupt-image fine-tuning (Sec. 4.3).

To encapsulate, our work offers the following seminal contributions:

\(\) We introduce _RoboDepth_, the first systematically designed robustness evaluation suite for MDE under data corruptions, sensor failure, and style shifts. See our repository at this link for more details.

\(\) We benchmark \(42\) state-of-the-art MDE models from indoor and outdoor scenes, on their robustness against corruptions, via three newly established datasets: _KITTI-C_, _NYUDepth2-C_, and _KITTI-S_. The corruption simulation toolkit has been open-sourced to facilitate future development.

\(\) Based on our observations, we draw in-depth discussion and analysis on the design considerations of building more robust MDE models for reliable, scalable, and practical applications.

\(\) Furthermore, we initiated the _RoboDepth Challenge_, which garnered participation from over one hundred teams, underscoring the community's interest and the challenge's relevance. Comprehensive details about the competition can be accessed on our competition website at this link.

Figure 1: The depth estimation robustness (in terms of depth estimation error (DEE) defined in Sec. 3.3) under \(18\) corruptions in radar charts. Different MDE models exhibit diverse strengths and weaknesses against different corruptions that occur in the real world.

## 2 Related Work

**Monocular Depth Estimation (MDE)**. Since the pioneering works [10; 12; 68; 16] first adopted deep neural networks to perform monocular depth estimation, significant progress has been made in many aspects. Notable innovations include network architectures [31; 43; 66; 63; 21], optimization functions [17; 64; 6], internal constraints [60; 67], multi-task learning [55; 22], geometry constraint [56; 51], and various sources of supervisions [44; 50; 33]. Based on the learning paradigm, most MDE methods can be split into supervised or self-supervised models. The former mainly focuses on indoor scenes and uses ground truth from RGB-D cameras or LiDAR sensors to train a regression model [1; 35]; while the latter formulates MDE as a novel view synthesis task to minimize the photometric loss between stereo pairs or from monocular video frames . Although promising results have been achieved, the robustness of MDE models under adverse scenarios is still unknown. Due to the lack of relevant datasets, existing models are at risk of being vulnerable to corruptions. In this work, we fill in this gap by establishing comprehensive evaluation benchmarks and testing \(42\) MDE models from both indoor and outdoor environments to analyze their OoD robustness.

**Robust MDE**. To the best of our knowledge, only a few works targeted robust learning of MDE and they focused on different aspects. Ranftl _et al._ proposed a unified objective for merging multiple datasets with different depth scales and ranges for training robust models. Similar works [58; 33; 53; 5; 62] resort to web stereo data or 3D movies to train MDE models and adapt them to unseen datasets. Kopf _et al._ estimate stable camera trajectories for hand-held cellphone videos. SC-DepthV3  generates pseudo-depth to refine depth details for scenes with dynamic objects. Li _et al._ proposed an attention module to choose scene-specific features for MDE on both indoor and outdoor scenes. SeasonDepth  contributed a dataset with depth maps under sunny, cloudy, and foliage weather. Most recently, there are works [7; 4] design adversarial patches to attack MDE models. Conversely, we aim to test the MDE robustness to corruptions that occur in real-world environments. We establish the first benchmark of this kind and incorporate an ample number of MDE models for in-depth analysis.

**Corruption Robustness**. ImageNet-C  is the pioneering work in this line of research which benchmarks classical image classification models to common corruptions and perturbations. Follow-up studies extend on the aspect to other visual perception tasks, _e.g._, object detection , segmentation [23; 26; 38], navigation , video classification , and pose estimation . The essentiality of evaluating model robustness has been repeatedly validated. Since we are targeting a different task, _i.e._, MDE, most of the well-studied corruption types become less realistic or suitable for such a data

Figure 3: **Corruption severity level**. We create versatile corruption sets with different levels of severity. Examples shown are from the proposed _NYUDepth2-C_ benchmark.

Figure 2: **Corruption taxonomy**. We break down common corruptions in depth estimation scenarios into _three_ categories: _i)_ Weather and lighting conditions, such as sunny, low-light, fog, frost, snow, and contrast conditions. _ii)_ Sensor failure and movement, such as potential blurs (defocus, glass, motion, zoom) caused by motion. _iii)_ Data processing issues, such as noises (Gaussian, impulse, ISO) happen due to hardware malfunctions. Examples shown are from the proposed _KITTI-C_ benchmark.

format. This motivates us to explore a new taxonomy for defining more proper corruption types for MDE. In this work, we contribute new datasets and benchmarks for probing the MDE robustness.

## 3 The RoboDepth Benchmark

In this section, we first introduce the taxonomy of corruptions included in our benchmarks (Sec. 3.1). We then elaborate on more details of the proposed datasets (Sec. 3.2) and corresponding robustness evaluation metrics (Sec. 3.3). Examples from our datasets are shown in Fig. 2, Fig. 3, and this page.

### Corruption Definition

**Weather & Lighting Condition**. The cameras on drones or vehicles operating under different weather and times of day capture distribution-shifted images which are rare or lacking in current MDE datasets [14; 49]. To probe the robustness of MDE models under adverse weather and lighting conditions, we simulate six corruptions, _i.e._, 'brightness', 'dark', 'fog', 'frost','snow', and 'contrast', which commonly occur in the real-world environment. Compared to clean images, these corruptions tend to affect the intensity and color of the light source, leading to hazy, blurry, and noise-contaminated images, which increase the difficulties for the MDE model to make accurate depth predictions.

**Sensor Failure & Movement**. An MDE system must behave robustly against motion perturbation and sensor failure to maintain safety requirements for practical applications. To achieve this pursuit, we mimic four motion-related corruptions, _i.e._, 'defocus', 'glass','motion', and 'zoom' blurs; we also generate images under 'elastic transformation' and 'color quantization', which happen during sensor malfunction. These corruption types are often associated with issues including edge distortions, contrast loss, and pattern shifts.

**Data & Processing Issue**. Data collection and transmission are inevitably associated with various sources of noise and potential loss of information. We include four such random variations, _i.e._, 'Gaussian', 'impulse','shot', and 'ISO' noises. In addition, we investigate the degradation caused by 'pixelate' and 'JPEG compression' which are common corruptions in handling image data. Compared to clean images, the noise-contaminated data introduce errors in the intensity values of pixels, leading to a grainy or speckled appearance. The pixelation and lossy compression tend to lead to a loss of detail and clarity in the image and can result in visible artifacts, such as blockiness or blurring.

### Benchmark Establishment

**KITTI-C**. Based on the KITTI Vision Suite , we establish a robustness benchmark for outdoor MDE. We simulate the defined \(18\) corruptions using data from the KITTI _val_ set under Eigen's split. Similar to , we design five severity levels for each corruption to further consolidate the evaluation

Figure 4: Benchmarking results of **42** MDE models on _KITTI-C_ and _NYUDepth2-C_. Figures from top to bottom: the depth estimation error (DEE) _vs._**[1st row]** mean corruption error (mCE), **[2nd row]** mean resilience rate (mRR), and **[3rd row]** sensitivity analysis among different corruption types.

of robustness changes. As a result, this robustness probing dataset has a total number of \(62,730\) RGB images with a resolution of \(192 640\). We also include the high-resolution version (\(320 1024\)) for evaluating the robustness of MDE models which take larger images as the input.

**NYUDepth2-C**. We construct a benchmark for robust indoor MDE based on NYU Depth V2 . \(15\) of the defined corruptions are used, excluding 'fog', 'frost', and'snow' which rarely occur in the indoor scenes. Since the indoor environments are less variant than outdoor ones, we only include four severity levels for each corruption. To sum up, this dataset contains \(39,240\) images of size \(480 640\), which cover \(23\) different types of indoor scenes, such as basement, bathroom, bedroom, study, _etc_.

**KITTI-S**. Style changes, consisting mostly of texture shifts, have proven helpful for analyzing model robustness . To further investigate the root cause of MDE robustness degradation, we form another collection based on KITTI  with stylized images via the style transfer model AdaIn . This dataset has \(8,364\) images from \(12\) styles, including 'cartoon', 'digital art', 'ink painting', 'kids' drawing','murals', 'oil painting', 'penciling','shadow play','sketch','stained glass','relief', and 'water color'. Due to space limitations, please refer to Appendix or this page for additional examples.

**Simulation Toolkit**. To facilitate a similar study on other MDE datasets, we have open-sourced the code at this link for simulating corruptions and style shifts given arbitrary "clean" images.

### Evaluation Metrics

**Depth Estimation Error (DEE)**. We combine Abs Rel (error rate) and \(_{1}\) (accuracy), the two main measures defined in [10; 36], into a unified metric as \(=-_{1}+1}{2}\), which is constantly used as the indicator of depth estimation error in our benchmark. See Appendix for more formal definitions.

**Corruption Error (CE)**. We follow  and use the mean CE (mCE) as the primary metric in comparing models' robustness. To normalize the severity effects, we choose MonoDepth2  and AdaBins  as the baseline models for the _KITTI-C_ and _NYUDepth2-C_ benchmarks, respectively. The CE across \(L\) levels of severity and mCE across \(N\) corruption types can be calculated as follows:

\[_{i}=^{L}(_{i,l})}{_{l=1}^{L}(_{i,l}^{})}\,=_{i=1}^{N}_{i}\.\] (1)

**Resilience Rate (RR)**. We define mean RR (mRR) as the relative robustness indicator for measuring how much accuracy can an MDE model retain when evaluated under the corruption scenarios, _i.e._,

\[_{i}=^{L}(1-_{i,l})}{L(1-_ {})}\,=_{i=1}^{N}_{i}\,\] (2)

where \(_{}\) denotes the task-specific accuracy (or error rate) score on the "clean" evaluation set.

## 4 Experiments

### Benchmark Configuration

**Depth Estimation Models**. We benchmark \(42\) depth estimation models and model variants, which cover most of the open-source MDE models so far. \(32\) of them are for outdoor MDE and the remaining \(10\) are for indoor MDE. More detailed descriptions of these models are attached in the Appendix.

**Datasets**. All benchmarked depth estimation models have been trained on the official _training_ splits of the _KITTI_ (for outdoor MDE) or _NYU-Depth V2_ (for indoor MDE) datasets, and are tested accordingly on the official _val_ splits and also our proposed _KITTI-C_, _NYUDepth2-C_, and _KITTI-S_ datasets. Additionally, we resort to the real-world _ACDC_, _nuScenes_, _Cityscapes_, and _Foggy-Cityscapes_ datasets for validating the fidelity of our simulated corruptions. Our datasets and model evaluation toolkit can be downloaded from this page.

**Evaluation Protocols**. To avoid any unfairness in the MDE robustness comparison, we unify the common configurations among different candidate models, such as _backbones_, _data augmentations_, and _post-processing_. We use public checkpoints whenever possible and reproduce the reported results based on official settings. More details on this aspect are included in the Appendix.

### MDE Robustness Probing

In this section, based on our benchmark, we aim to understand the corruption robustness among different MDE models by answering the following representative questions.

**Q-1:** _"Are existing MDE models robust under real-world corruptions?"_ **A:** No, to a certain extent. Our benchmark results in Fig. 4 reveal that state-of-the-art MDE models are at risk of being vulnerable to corruptions. Existing MDE models, either from indoor or outdoor scenes, show a flattened or even inverse relationship between the DEE scores and robustness metrics. Due to the lack of a suitable robustness evaluation suite, current MDE models are over-fitted on "clean" sets while ignoring the OoD scenarios that are likely to occur in the real world. Nevertheless, we observe varying behaviors of different MDE models under different corruption types (see Fig. 1), caused by different design choices on model architecture. The results from Tab. 1 and Tab. 2 further show that the Transformers-based MDE models exhibit better robustness compared to conventional CNNs. Diving deeper, we can observe from the per-severity error rates in Fig. 9 that the above conclusion holds true for most corruption types under different severity levels. The qualitative results shown in Fig. 10 also validate that models with long-range receptive fields, such as MonoViT  (\(74.95\%\) mCE) and Lite-Mono

   &  &  &  &  \\  & mC & mC & mR & mDEE & mC & mR & mDEE & mC & mR & mDEE & mCE & mR & mDEE \\   MonoDepth\({}_{}\) & \(100.00\) & \(84.46\) & \(0.256\) & \(100.00\) & \(84.37\) & \(0.257\) & \(100.00\) & \(90.33\) & \(0.204\) & \(100.00\) & \(78.66\) & \(0.307\) \\ MonoDepth\({}_{}\) & \(119.75\) & \(82.50\) & \(0.294\) & \(146.17\) & \(78.58\) & \(0.327\) & \(103.28\) & \(92.45\) & \(0.209\) & \(109.80\) & \(76.48\) & \(0.345\) \\ MonoDepth\({}_{}\) & \(106.06\) & \(82.44\) & \(0.270\) & \(109.95\) & \(80.38\) & \(0.288\) & \(115.99\) & \(85.59\) & \(0.242\) & \(92.25\) & \(81.36\) & \(0.279\) \\ MonoDepth\({}_{}\) & \(113.43\) & \(80.59\) & \(0.288\) & \(104.53\) & \(83.28\) & \(0.285\) & \(128.68\) & \(82.4\) & \(2.272\) & \(107.07\) & \(76.05\) & \(0.239\) \\ MaxGcc\({}_{}\) & \(104.05\) & \(82.97\) & \(0.267\) & \(100.98\) & \(84.13\) & \(0.257\) & \(108.85\) & \(87.67\) & \(0.226\) & \(102.30\) & \(77.12\) & \(0.319\) \\ DNet\({}_{}\) & \(104.17\) & \(83.34\) & \(0.265\) & \(100.41\) & \(83.56\) & \(0.203\) & \(115.56\) & \(86.07\) & \(0.241\) & \(95.53\) & \(80.39\) & \(0.291\) \\ CABpoth  & \(110.11\) & \(80.07\) & \(0.286\) & \(102.59\) & \(82.04\) & \(82.08\) & \(119.67\) & \(83.91\) & \(0.252\) & \(108.06\) & \(74.25\) & \(0.338\) \\ HR-Depth  & \(103.73\) & \(82.93\) & \(0.264\) & \(100.41\) & \(83.82\) & \(0.256\) & \(116.01\) & \(85.32\) & \(0.242\) & \(94.76\) & \(79.64\) & \(0.293\) \\ DiffNet\({}_{}\) & \(94.96\) & \(85.41\) & \(0.231\) & \(79.93\) & \(89.35\) & \(0.196\) & \(124.69\) & \(81.68\) & \(0.267\) & \(80.86\) & \(\) & \(0.237\) \\ ManyDepth\({}_{}\) & \(105.41\) & \(831.31\) & \(0.271\) & \(104.97\) & \(84.15\) & \(0.262\) & \(102.57\) & \(90.96\) & \(0.210\) & \(108.70\) & \(75.10\) & \(0.341\) \\ FSRL-Depth  & \(90.53\) & \(83.86\) & \(0.253\) & \(981.73\) & \(83.92\) & \(0.221\) & \(103.86\) & \(86.56\) & \(0.210\) & \(105.66\) & \(75.53\) & \(0.327\) \\ MonoViT  & \(793.33\) & \(89.15\) & \(0.197\) & \(292.92\) & \(91.16\) & \(0.179\) & \(81.52\) & \(92.67\) & \(0.165\) & \(83.47\) & \(83.61\) & \(0.247\) \\ MonoViT  & \(\) & \(\) & \(\) & \(\) & \(90.71\) & \(\) & \(\) & \(\) & \(\) & \(84.51\) & \(\) \\ DynDab\({}_{}\) & \(110.38\) & \(81.50\) & \(0.280\) & \(102.92\) & \(84.55\) & \(0.263\) & \(131.29\) & \(81.67\) & \(0.279\) & \(96.95\) & \(79.39\) & \(0.299\) \\ DynDab\({}_{}\) & \(119.99\) & \(77.98\) & \(0.308\) & \(105.81\) & \(81.70\) & \(0.275\) & \(143.38\) & \(78.99\) & \(0.307\) & \(110.79\) & \(74.15\) & \(0.342\) \\ RA-Depth  & \(112.73\) & \(78.79\) & \(0.288\) & \(89.93\) & \(83.54\) & \(0.229\) & \(137.47\) & \(78.19\) & \(0.293\) & \(111.12\) & \(72.82\) & \(0.342\) \\ TriDepth\({}_{}\) & \(109.26\) & \(81.56\) & \(0.280\) & \(115.07\) & \(80.79\) & \(82.02\) & \(104.61\) & \(88.79\) & \(0.216\) & \(108.10\) & \(71.0\) & \(0.337\) \\ Lite-Mono\({}_{}\) & \(92.86\) & \(66.02\) & \(0.233\) & \(90.57\) & \(83.10\) & \(0.219\) & \(95.47\) & \(0.87\) & \(0.196\) & \(92.71\) & \(80.90\) & \(0.284\) \\ Lite-Mono\({}_{}\) & \(100.3

[MISSING_PAGE_FAIL:7]

global awareness, _e.g._ Lite-Mono  and MonoViT , suffer less degradation in this case. Due to the page limit, kindly refer to the Appendix for more detailed findings on this style-shifted dataset.

### MDE Robustness Enhancement

**Pertaining strategy tends to help improve MDE robustness**. We observe that transferring knowledge from other tasks, such as classification and segmentation, brings both strengths and weaknesses to MDE robustness. Fig. 6 (top) highlights that MDE models pre-trained on object-centric datasets, _e.g._ ImageNet, are more robust against weather and lighting changes (except for'snow') and data processing noises, which are mostly texture-shifted corruptions. Motion and sensor corruptions, however, contain more edge and object distortions and could be eased by models without ImageNet pre-training. More concrete results in Tab. 1 imply that the CNN-based MDE models like MonoDepth2  could become more shape-biased when pre-trained on object-centric datasets. More evidence from the _KITTI-S_ benchmark (see Tab. 3) further verifies this finding, where the stylized data (texture-biased) are causing severe degradation for models with ImageNet pre-training.

**Training on high-resolution images yields more robust MDE models**. From Fig. 6 (bottom) we observe that MDE models trained with higher resolution inputs will likely yield more robust feature learning (relative \(30\%\) better) on noise-contaminated corruptions (Gaussian, impulse, shot, ISO noises). Since these noise contamination types mainly affect the global pixel distribution instead of the local ones, the CNN-based MDE models trained with high-resolution images are likely to capture more fine-grained information to suppress the degradation caused by noises. For more details, kindly refer to the per-corruption scores of benchmarked models in the Appendix.

**Larger model size might not lead to better MDE robustness**. It is often intuitive that larger models are likely to learn more general representations and lead to better performance on unseen data. However, we observe from Fig. 7 (left) that MDE models with more trainable parameters are getting less robust, mainly because they are deliberately tuned towards clean distribution and thus

    & **anDEE** & **Cart** & **Digit** & **lak** & **Kids** & **Mural** & **Oil** & **Pencil** & **Shadow** & **Shetch** & **Glass** & **Relief** & **Water** \\   MonoDepth\({}_{25}\)  & 365 &.324 &.434 &.351 &.293 &.326 &.328 &.418 &.388 &.416 &.566 &.255 &.317 \\ MonoDepth\({}_{25}\)\({}_{}\)  & 378 &.279 &.289 &.589 &.235 &.412 &.249 &.290 &.589 & **.288** &.596 &.259 &.464 \\ MaxGool\({}_{25}\)  & 368 &.358 &.356 &.260 &.265 &.358 &.375 &.333 &.314 &.576 &.551 &.288 &.384 \\ DNet as  & 400 & 444 &.381 &.283 &.280 &.389 &.336 &.422 &.290 &.608 &.553 &.290 &.516 \\ CADDepth  & 406 &.446 &.380 &.379 &.259 &.421 &.413 &.470 &.317 &.585 &.543 &.242 &.401 \\ HB-Depth  & 324 &.318 &.321 &.238 &.228 &.299 &.306 &.337 &.316 &.388 &.531 &.265 &.331 \\ DIPFNet  & 310 &.227 &.351 & **.206** &.206 &.386 &.244 &.276 &.289 &.385 &.502 &.294 &.360 \\ ManyDepth  & 323 &.251 &.373 &.212 &.235 &.344 &.300 &.360 &.343 &.331 &.553 &.310 &.300 \\ FSRCB  & 293 &.275 &.277 &.294 &.221 &.310 &.220 &.270 &.301 &.318 &.417 &.261 &.352 \\ MonoViT  & **238** & **179** & **229** &.252 & **196** & **240** & **203** & **237** & **208** & **.325** & **.356** & **.205** & **.221** \\ DynDepthas  & 371 &.349 &.358 &.291 &.264 &.288 &.293 &.338 &.363 &.492 &.534 &.298 &.585 \\ DynDepthas  & 447 &.502 &.408 &.299 &.287 &.298 &.437 &.492 &.372 &.595 &.565 &.460 &.589 \\ R-Depth  & 365 &.304 &.335 &.354 &.262 &.340 &.310 &.342 &.425 &.372 &.476 &.379 &.475 \\ TriDepth\({}_{25}\)  & 379 &.304 &.403 &.256 &.217 &.440 &.352 &.480 &.318 &.317 &.573 &.249 &.436 \\ LiLi-Mono \({}_{25}\)  & 280 &.236 &.310 &.251 &.221 &.254 &.283 &.285 &.232 &.356 &.446 &.237 &.252 \\ Lite-Mono \({}_{25}\)  &.303 &.210 &.346 &.218 &.210 & **.224** &.238 &.274 &.307 &.445 &.435 &.295 &.336 \\ Lite-Mono\({}_{25}\)  &.288 &.234 &.313 &.262 &.218 &.231 &.241 &.291 &.240 &.459 &.476 &.229 &.264 \\ Lite-Mono\({}_{129}\)  &.323 &.208 &.435 &.249 &.306 &.348 &.258 &.363 &.2100 &.458 &.507 &.263 &.266 \\   

Table 3: The **Depth Estimation Error (DEE)** of \(18\) models on _KITTI-S_. **Bold**: Best in column. Underline: 2nd best in column. Blue : Best in row. Red : 2nd best in row. Green : Worst in row.

Figure 7: MDE robustness comparisons based on **[left]:** Model size (# of parameters) and **[middle]:** Model complexity (FLOPs); and **[right]:** Corruption fidelity verification by fine-tuning MonoDepth2  with our corruptions on _KITTI_ and testing it on the official _val_ splits of _nuScenes_ (including its validation set and the nighttime split of the validation set) and _Foggy-Cityscapes_.

losing generalizability. Our results suggest that a moderate-sized model with suitable corruption suppression modules is likely to yield the best possible trade-off between robustness and efficiency.

**Model complexity shows a direct correlation with MDE robustness**. Based on Fig. 7 (middle) we reveal that higher training complexity tends to lead to better robustness. This is mainly because existing MDE models with high FLOPs per parameter are either trained with high-resolution monocular inputs or contain the computationally intensive self-attention mechanism and yield more stable depth predictions (as analyzed in previous discussions) but cause heavier training overhead.

**Training with corrupted images does not always enhance model robustness**. The robust fine-tuning study in Fig. 8 shows that overfitting the evaluation distribution (_e.g._ train and test on the same

Figure 8: The **Per-Corruption Error** scores of MonoDepth2  fine-tuned with typical corruption as data augmentations and tested on each of the corruption types. For each **column** (corruption) in this figure: The darker the color, the lower the Abs Rel score (better robustness), and vice versa.

Figure 9: The **Per-Severity Error Rate** of eight typical MDE models  trained on the clean _KITTI_ dataset  and tested on each of the corruption set in _KITTI-C_. For each **subfigure** (corruption) in this figure: The horizontal axis denotes severity levels from one to five. The vertical axis denotes the depth estimation error (Abs Rel). Best viewed in color.

orruption) may help improve MDE robustness for this specific type of corruption, as shown from the darker-colored cell in the diagonal. However, those corruptions with severe perturbations (weather and movement) will likely lead to sub-par performance on other tested corruptions and even hurt the overall robustness. This indicates that sophisticated considerations, _e.g._ case study, must be made before applying corruptions as data augmentations to the training stage.

## 5 Discussion & Conclusion

Throughout this research, we introduced the _RoboDepth_ benchmark, a dedicated tool tailored for probing the OoD robustness of MDE models under a range of corruptions. By crafting three distinct datasets (_KITTI-C_, _NYUDepth2-C_, and _KITTI-S_) coupled with two metrics (mCE and mRR), we offered a comprehensive landscape for gauging both indoor and outdoor MDE robustness. Our findings underscore the imperative nature of robustness evaluation in the realm of depth estimation. Moreover, by dissecting various influential factors - encompassing architecture, modality, pre-training approaches, resolution, and model capacity - we provided insights to fortify model resilience against corruptions. In essence, our endeavors serve as a compass directing the evolution of robust MDE methodologies. As we navigate forward, our ambition is to augment the breadth and granularity of our benchmark, aiming to encompass a wider spectrum of MDE models and corruption variants.

**Potential Limitations**. Despite our extensive evaluation of diverse MDE models across expansive corruption sets, and validating the authenticity of our simulated corruptions, certain avenues remain uncharted. Notably, our current framework does not accommodate scenarios wherein multiple corruptions manifest simultaneously. Additionally, evolving from rigidly defined five severity levels to a more nuanced, continuous scale might offer deeper insights into MDE robustness. These unexplored terrains present intriguing prospects for subsequent research endeavors.

Figure 10: Qualitative results of representative MDE models [17; 67; 42; 63; 66] under defined corruptions in the _KITTI-C_ benchmark. Best viewed in color and zoom-ed in for details.

[MISSING_PAGE_FAIL:11]

*  Clement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J. Brostow. Digging into self-supervised monocular depth prediction. In _IEEE/CVF Int. Conf. Comput. Vis. (ICCV)_, pages 3828-3838, 2019.
*  Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _Int. Conf. Learn. Represent. (ICLR)_, 2019.
*  Hanjiang Hu, Baoquan Yang, Zhijian Qiao, Shiqi Liu, Ding Zhao, and Hesheng Wang. Seasondepth: Cross-season monocular depth prediction dataset and benchmark under multiple environments. In _Int. Conf. Mach. Learn. Worksh. (ICMLW)_, 2022.
*  Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In _IEEE/CVF Int. Conf. Comput. Vis. (ICCV)_, pages 1501-1510, 2017.
*  Adrian Johnston and Gustavo Carneiro. Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 4756-4765, 2020.
*  Hyunyoung Jung, Eunhyeok Park, and Sungjoo Yoo. Fine-grained semantics-aware representation enhancement for self-supervised monocular depth estimation. In _IEEE/CVF Int. Conf. Comput. Vis. (ICCV)_, pages 12642-12652, 2021.
*  Christoph Kamann and Carsten Rother. Benchmarking the robustness of semantic segmentation models. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 8828-8838, 2020.
*  Oguzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir Zamir. 3d common corruptions and data augmentation. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 18963-18974, 2022.
*  Lingdong Kong, Youquan Liu, Runnan Chen, Yuexin Ma, Xinge Zhu, Yikang Li, Yuenan Hou, Yu Qiao, and Ziwei Liu. Rethinking range view representation for lidar segmentation. In _IEEE/CVF Int. Conf. Comput. Vis. (ICCV)_, pages 228-240, 2023.
*  Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu. Robo3d: Towards robust and reliable 3d perception against corruptions. In _IEEE/CVF Int. Conf. Comput. Vis. (ICCV)_, pages 19994-20006, 2023.
*  Lingdong Kong, Yaru Niu, Shaoyuan Xie, Hanjiang Hu, Lai Xing Ng, Benoit Cottereau, Ding Zhao, Liangjun Zhang, Hesheng Wang, Wei Tsang Ooi, Ruijie Zhu, Ziyang Song, Li Liu, Tianzhu Zhang, Jun Yu, Mohan Jing, Pengwei Li, Xiaohua Qi, Cheng Jin, Yingfeng Chen, Jie Hou, Jie Zhang, Zhen Kan, Qiang Lin, Liang Peng, Minglei Li, Di Xu, Changpeng Yang, Yuanqi Yao, Gang Wu, Jian Kuai, Xianming Liu, Junjun Jiang, Jiantian Huang, Baojun Li, Jiale Chen, Shuang Zhang, Sun Ao, Zhenyu Li, Runze Chen, Haiyong Luo, Fang Zhao, and Jingze Yu. The robodepth challenge: Methods and advancements towards robust depth estimation. _arXiv preprint arXiv:2307.15061_, 2023.
*  Lingdong Kong, Jiawei Ren, Liang Pan, and Ziwei Liu. Lasermix for semi-supervised lidar semantic segmentation. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 21705-21715, 2023.
*  Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 1611-1621, 2021.
*  Hamid Laga, Laurent Valentin Jospin, Farid Boussaid, and Mohammed Bennamoun. A survey on deep learning techniques for stereo-based depth estimation. _IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)_, 44(4):1738-1764, 2020.
*  Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. _arXiv preprint arXiv:1907.10326_, 2019.
*  Ruibo Li, Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, and Lingxiao Hang. Deep attention-based classification network for robust depth prediction. In _Asian Conf. Comput. Vis. (ACCV)_, pages 663-678, 2019.
*  Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 2041-2050, 2018.
*  Zhenyu Li, Zehui Chen, Ang Li, Liangji Fang, Qinhong Jiang, Xianming Liu, Junjun Jiang, Bolei Zhou, and Hang Zhao. Simipu: Simple 2d image and 3d point cloud unsupervised pre-training for spatial-aware visual representations. In _AAAI Conf. Artifi. Intell. (AAAI)_, 2022.
**  Zhenyu Li, Zehui Chen, Xianming Liu, and Junjun Jiang. Depthformer: Exploiting long-range correlation and local information for accurate monocular depth estimation. _arXiv preprint arXiv:2203.14211_, 2022.
*  Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. Learning depth from single monocular images using deep convolutional neural fields. _IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)_, 38(10):2024-2039, 2015.
*  Youquan Liu, Runnan Chen, Xin Li, Lingdong Kong, Yuchen Yang, Zhaoyang Xia, Yeqi Bai, Xinge Zhu, Yuexin Ma, Yikang Li, Yu Qiao, and Yuenan Hou. Uniseg: A unified multi-modal lidar segmentation network and the openpcseg codebase. In _IEEE/CVF Int. Conf. Comput. Vis. (ICCV)_, pages 21662-21673, 2023.
*  Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segment any point cloud sequences by distilling vision foundation models. _arXiv preprint arXiv:2306.09347_, 2023.
*  Xiaoyang Lyu, Liang Liu, Mengmeng Wang, Xin Kong, Lina Liu, Yong Liu, Xinxin Chen, and Yi Yuan. Hr-depth: High resolution self-supervised monocular depth estimation. In _AAAI Conf. Artifi. Intell. (AAAI)_, pages 2294-2301, 2021.
*  Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S. Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking robustness in object detection: Autonomous driving when winter is coming. _arXiv preprint arXiv:1907.07484_, 2019.
*  Yue Ming, Xuyang Meng, Chunxiao Fan, and Hui Yu. Deep learning for monocular depth estimation: A reviews. _Neurocomputing_, 438:14-33, 2021.
*  He Mu, Hui Le, Bian Yikai, Ren Jian, Xie Jin, and Yang Jian. Ra-depth: Resolution adaptive self-supervised monocular depth estimation. In _Eur. Conf. Comput. Vis. (ECCV)_, pages 565-581, 2022.
*  Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In _IEEE/CVF Int. Conf. Comput. Vis. (ICCV)_, pages 12179-12188, 2021.
*  Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. _IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)_, 44(3):1623-1637, 2022.
*  Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Acdc: The adverse conditions dataset with correspondences for semantic driving scene understanding. In _IEEE/CVF Int. Conf. Comput. Vis. (ICCV)_, pages 10765-10775, 2021.
*  Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Semantic foggy scene understanding with synthetic data. _Int. J. Comput. Vis._, 126(9):973-992, 2018.
*  Ashutosh Saxena, Min Sun, and Andrew Y. Ng. Make3d: Learning 3d scene structure from a single still image. _IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)_, 31(5):824-840, 2008.
*  Maarten Schellevis. Improving self-supervised single view depth estimation by masking occlusion. _arXiv preprint arXiv:1908.11112_, 2019.
*  Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In _Eur. Conf. Comput. Vis. (ECCV)_, pages 746-760, 2012.
*  Libo Sun, Jia-Wang Bian, Huangying Zhan, Wei Yin, Ian Reid, and Chunhua Shen. Sc-depthv3: Robust self-supervised monocular depth estimation for dynamic scenes. _arXiv preprint arXiv:2211.03660_, 2022.
*  Fabio Tosi, Filippo Aleotti, Matteo Poggi, and Stefano Mattoccia. Learning monocular depth estimation infusing traditional stereo knowledge. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 9799-9809, 2019.
*  Jonas Uhrig, Nick Schneider, Lukas Schneider, Uwe Franke, Thomas Brox, and Andreas Geiger. Sparsity invariant cnns. In _Int. Conf. 3D Vis. (3DV)_, pages 11-20, 2017.
*  Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver Wang. Web stereo video supervision for depth prediction from dynamic scenes. In _Int. Conf. 3D Vis. (3DV)_, pages 348-357, 2019.

*  Jiahang Wang, Sheng Jin, Wentao Liu, Weizhong Liu, Chen Qian, and Ping Luo. When human pose estimation meets robustness: Adversarial algorithms and benchmarks. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 11855-11864, 2021.
*  Lijun Wang, Jianming Zhang, Oliver Wang, Zhe Lin, and Huchuan Lu. Sdc-depth: Semantic divide-and-conquer network for monocular depth estimation. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 541-550, 2020.
*  Jamie Watson, Oisin Mac Aodha, Victor Prisacariu, Gabriel Brostow, and Michael Firman. The temporal opportunist: Self-supervised multi-frame monocular depth. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 1164-1174, 2021.
*  Jamie Watson, Michael Firman, Gabriel J. Brostow, and Daniyar Turmukhambetov. Self-supervised monocular depth hints. In _IEEE/CVF Int. Conf. Comput. Vis. (ICCV)_, pages 2162-2171, 2019.
*  Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo. Monocular relative depth perception with web stereo data supervision. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 311-320, 2018.
*  Feng Xue, Guirong Zhuo, Ziyuan Huang, Wufei Fu, Zhuoyue Wu, and Marcelo H. Ang. Toward hierarchical self-supervised monocular absolute depth estimation for autonomous driving applications. In _IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)_, pages 2330-2337, 2020.
*  Jiaxing Yan, Hong Zhao, Penghui Bu, and YuSheng Jin. Channel-wise attention-based network for self-supervised monocular depth estimation. In _Int. Conf. 3D Vis. (3DV)_, pages 464-473, 2021.
*  Chenyu Yi, Siyuan Yang, Haoliang Li, Yap peng Tan, and Alex Kot. Benchmarking the robustness of spatial-temporal models against corruptions. In _Adv. Neural Inform. Process. Syst. (NeurIPS)_, 2021.
*  Chi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and Chunhua Shen. Hierarchical normalization for robust monocular depth estimation. In _Adv. Neural Inform. Process. Syst. (NeurIPS)_, pages 14128-14139, 2022.
*  Ning Zhang, Francesco Nex, George Vosselman, and Norman Kerle. Lite-mono: A lightweight cnn and transformer architecture for self-supervised monocular depth estimation. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, 2023.
*  Sen Zhang, Jing Zhang, and Dacheng Tao. Towards scale-aware, robust, and generalizable unsupervised monocular depth estimation by integrating imu motion dynamics. In _Eur. Conf. Comput. Vis. (ECCV)_, pages 143-160, 2022.
*  Chaoqiang Zhao, Qiyu Sun, Chongzhen Zhang, Yang Tang, and Feng Qian. Monocular depth estimation based on deep learning: An overview. _Science China Technological Sciences_, 63(9):1612-1627, 2020.
*  Chaoqiang Zhao, Youmin Zhang, Matteo Poggi, Fabio Tosi, Xianda Guo, Zheng Zhu, Guan Huang, Yang Tang, and Stefano Mattoccia. Monovit: Self-supervised monocular depth estimation with a vision transformer. In _Int. Conf. 3D Vis. (3DV)_, 2022.
*  Hang Zhou, David Greenwood, and Sarah Taylor. Self-supervised monocular depth estimation with internal feature fusion. In _Brit. Mach. Vis. Conf. (BMVC)_, 2021.
*  Tinghui Zhou, Matthew Brown, Noah Snavely, and David G. Lowe. Unsupervised learning of depth and ego-motion from video. In _IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 1851-1858, 2017.

## Appendix

In this appendix, we supplement the following materials to support the findings and conclusions drawn in the main body of this paper:

* Sec. A documents necessary information about the proposed datasets and benchmarks.
* Sec. B elaborates on more details in terms of benchmark definitions and implementations, as well as the details of data collection, organization, licensing, and access.
* Sec. C introduces The RoboDepth Challenge, an academic competition held based on the datasets and benchmarks constructed in this work.
* Sec. D provides the complete quantitative results of different monocular depth estimation models in the established _KITTI-C_, _NYUDepth2-C_, and _KITTI-S_ benchmarks.
* Sec. E contains additional qualitative results of different monocular depth estimation models under out-of-distribution corruption scenarios.
* Sec. F acknowledges the public resources used during the course of this work.

## Appendix A Datasheets

In this section, we follow Gebru _et al._ to document necessary information about the proposed datasets and benchmarks.

### Motivation

The questions in this section are primarily intended to encourage dataset creators to clearly articulate their reasons for creating the dataset and to promote transparency about funding interests. The latter may be particularly relevant for datasets created for research purposes.

1. _"For what purpose was the dataset created?"_**A:** The dataset was created to facilitate relevant research in the area of depth estimation robustness under out-of-distribution corruptions.
2. _"Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?"_**A:** The dataset was created by Lingdong Kong (National University of Singapore), Shaoyuan Xie (Huazhong University of Science and Technology), Hanjiang Hu (Carnegie Mellon University), Lai Xing Ng (Institute for Infocomm Research, A*STAR), Benoit Cottereau (CNRS), and Wei Tsang Ooi (National University of Singapore).
3. _"Who funded the creation of the dataset?"_**A:** The creation of the dataset is funded by related affiliations of the authors in this work, as listed in the above item.
4. _"Any other comments?"_**A:** N/A.

### Composition

Most of the questions in this section are intended to provide dataset consumers with the information they need to make informed decisions about using the dataset for their chosen tasks. Some of the questions are designed to elicit information about compliance with the EU's General Data Protection Regulation (GDPR) or comparable regulations in other jurisdictions. Questions that apply only to datasets that relate to people are grouped together at the end of the section. We recommend taking a broad interpretation of whether a dataset relates to people. For example, any dataset containing text that was written by people relates to people.

1. _"What do the instances that comprise our datasets represent (e.g., documents, photos, people, countries)?"_**A:** The instances that comprise the dataset are mainly images captured by camera sensors, providing visual representations of indoor and outdoor scenes observed.
2. _"How many instances are there in total (of each type, if appropriate)?"_**A:** The _KITTI-C_ dataset contains a total number of \(62,730\) RGB images from \(18\) corruption types, with a resolution of \(192 640\). The _NYUDepth2-C_ dataset contains a total number of \(39,240\)RGB images from \(15\) corruption types, with a resolution of \(480 640\). The _KITTI-S_ dataset contains a total number of \(8,364\) RGB images from \(12\) style type, with a resolution of \(192 640\). Each image in these three datasets is associated with a single-channel depth map of the same size as the image.
3. _"Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?"_ **A:** Yes, our datasets contain all possible instances that have been collected so far.
4. _"Is there a label or target associated with each instance?"_ **A:** Yes, each instance in our datasets is associated with a single-channel depth map of the same size as the image.
5. _"Is any information missing from individual instances?"_ **A:** No.
6. _"Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?"_ **A:** Yes, the relationship between individual instances is explicit.
7. _"Are there recommended data splits (e.g., training, development/validation, testing)?"_ **A:** Yes, we provide detailed data splits for our datasets.
8. _"Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?"_ **A:** Yes, our datasets are self-contained.
9. _"Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)?"_ **A:** No, all data are clearly licensed.
10. _"Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?"_ **A:** No.
11. _"Any other comments?"_ **A:** N/A.

### Collection Process

In addition to the goals outlined in the previous section, the questions in this section are designed to elicit information that may help researchers and practitioners create alternative datasets with similar characteristics. Again, questions that apply only to datasets that relate to people are grouped together at the end of the section.

1. _"How was the data associated with each instance acquired?"_ **A:** Please refer to the details listed in Section B.
2. _"What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)?"_ **A:** Please refer to the details listed in Section B.
3. _"If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?"_ **A:** Please refer to the details listed in Section B.

### Preprocessing, Cleaning, and Labeling

The questions in this section are intended to provide dataset consumers with the information they need to determine whether the "raw" data has been processed in ways that are compatible with their chosen tasks. For example, text that has been converted into a "bag-of-words" is not suitable for tasks involving word order.

1. _"Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?"_ **A:** Yes, we preprocessed and cleaned data in our datasets.
2. _"Was the 'raw' data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?"_ **A:** Yes, raw data is accessible.
3. _"Is the software that was used to preprocess/clean/label the data available?"_ **A:** Yes, the necessary software used to preprocess and clean the data is publicly available.
4. _"Any other comments?"_ **A:** N/A.

### Uses

The questions in this section are intended to encourage dataset creators to reflect on tasks for which the dataset should and should not be used. By explicitly highlighting these tasks, dataset creators can help dataset consumers make informed decisions, thereby avoiding potential risks or harms.

1. _"Has the dataset been used for any tasks already?"_ **A:** No.
2. _"Is there a repository that links to any or all papers or systems that use the dataset?"_ **A:** Yes, we provide such links in our GitHub repository.
3. _"What (other) tasks could the dataset be used for?"_ **A:** The dataset could be used for relevant perception, tracking, and planning tasks based on camera sensors.
4. _"Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?"_ **A:** N/A.
5. _"Are there tasks for which the dataset should not be used?"_ **A:** N/A.
6. _"Any other comments?"_ **A:** N/A.

### Distribution

Dataset creators should provide answers to these questions prior to distributing the dataset either internally within the entity on behalf of which the dataset was created or externally to third parties.

1. _"Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?"_ **A:** No.
2. _"How will the dataset be distributed (e.g., tarball on website, API, GitHub)?"_ **A:** Very likely to be distributed by website, API, and GitHub repository.
3. _"When will the dataset be distributed?"_ **A:** The datasets are publicly accessible.
4. _"Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?"_ **A:** Yes, the dataset is under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.
5. _"Have any third parties imposed IP-based or other restrictions on the data associated with the instances?"_ **A:** No.
6. _"Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?"_ **A:** No.
7. _"Any other comments?"_ **A:** N/A.

### Maintenance

As with the questions in the previous section, dataset creators should provide answers to these questions prior to distributing the dataset. The questions in this section are intended to encourage dataset creators to plan for dataset maintenance and communicate this plan to dataset consumers.

1. _"Who will be supporting/hosting/maintaining the dataset?"_ **A:** The authors of this work serve for supporting, hosting, and maintaining the datasets.
2. _"How can the owner/curator/manager of the dataset be contacted (e.g., email address)?"_ **A:** The curators can be contacted via emails as follows: - Lingdong Kong (lingdong@comp.nus.edu.sg); - Shaoyuan Xie (shaoyuanxie@hust.edu.cn); - Hanjiang Hu (hanjianghu@cmu.edu); - Lai Xing Ng (ng_lai_xing@i2r.a-star.edu.sg); - Benoit Cottereau (benoit.cottereau@cnrs.fr); - Wei Tsang Ooi (ooiwt@comp.nus.edu.sg).
3. _"Is there an erratum?"_ **A:** There is no explicit erratum; updates and known errors will be specified in future versions.

4. _"Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?"_ **A:** No, for the current version. Future updates (if any) will be posted on the dataset website.
5. _"Will older versions of the dataset continue to be supported/hosted/maintained?"_ **A:** Yes. This is the first version of the release; future updates will be posted and older versions will be replaced.
6. _"If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?"_ **A:** Yes, we provide detailed instructions for future extensions.
7. _"Any other comments?"_ **A:** N/A.

## Appendix B Datasets and Benchmarks

In this section, we first provide the detailed definitions of different corruption types in the RoboDepth benchmark (Sec. B.1), we then elaborate on the data collection process (Sec. B.2), the license of our datasets (Sec. B.3), and the procedures for downloading these datasets (Sec. B.4). Lastly, we attach the summary of technical contributions and implementation details for the benchmarked monocular depth estimation methods (Sec. B.5). An overview of the \(18\) corruption types is shown in Fig. A. The histogram of pixel values under each corruption is shown in Fig. B.

### Corruption Definition

* **Brightness** refers to the level of lightness or darkness in an image. It is affected by the intensity and color of the light source, as well as the reflectivity of the objects in the scene. Images captured under different lighting conditions may have varying levels of brightness, which can affect the visibility and depth estimation quality.
* **Dark** images can result from low-lighting conditions or underexposure. These images can have low contrast and low visibility, making it difficult to distinguish the objects and backgrounds in the scene. Dark images can also suffer from increased electronic noise and color distortion.
* **Fog** is a type of atmospheric scattering that can reduce the contrast and visibility of objects and backgrounds in a scene. It occurs when water droplets or ice crystals in the air scatter and absorb light, leading to a hazy or blurry appearance.
* **Frost** forms when lenses or windows are coated with ice crystals, leading to image distortion and inaccuracies in depth estimation. Frost can be modeled as a convolution of the image with a non-uniform kernel that depends on the shape and size of the ice crystals. It can affect the accuracy of depth estimation by reducing the contrast of the image and distorting the edges of objects.

Figure A: The \(18\) corruption types from three main categories defined in the RoboDepth benchmark. Examples shown are from the proposed _KITTI-C_ benchmark.

* **Snow** is a visually obstructive form of precipitation that can obscure objects and backgrounds in the image and make it difficult to estimate accurate depth. It can be modeled as a random distribution of white pixels with a high probability of occurrence. Snow can affect the accuracy of depth estimation by introducing errors in the intensity values of pixels and obscuring the edges of objects.
* **Contrast** is the difference in luminance or color between different parts of an image. High-contrast images have large differences between light and dark areas, while low-contrast images have more similar levels of brightness. Contrast can be affected by lighting conditions and the color and texture of the objects and backgrounds in the scene.

Figure B: The histogram of pixel values under each of the \(18\) corruption types in _KITTI-C_. Note that the **horizontal axis** of each subfigure is scaled, shifted, and centered to fit the window. The numerical range is at the scale of \(1e6\). Zoomed-in for more details.

* **Defocus blur** occurs when an image is out of focus, causing objects and backgrounds in the image to appear blurred. It can be modeled as a convolution of the image with a Gaussian kernel, where the blur size depends on the distance between the camera and the object. Defocus blur can affect the accuracy of depth estimation by reducing the contrast of the image and distorting the edges of objects.
* **Glass blur** is a type of image distortion that appears with glass windows or panels. It can lead to inaccuracies in depth estimation due to the opaque and irregular nature of the glass. Glass blur can be modeled as a convolution of the image with a non-uniform kernel that depends on the shape and thickness of the glass.
* **Motion blur** appears when a camera is moving quickly, causing objects and backgrounds in the image to appear blurred. It can be modeled as a convolution of the image with a motion kernel that depends on the direction and speed of the camera movement.
* **Zoom blur** occurs when a camera moves toward an object rapidly, causing the image to appear blurred. It can be modeled as a convolution of the image with a non-uniform kernel that depends on the zoom factor and the distance between the camera and the object. Zoomblur can affect the accuracy of depth estimation by distorting the edges of objects and reducing the contrast of the image.
* **Elastic transformations** are spatial transformations that deform small regions of an image while preserving the overall structure. They are commonly used in data augmentation techniques for deep learning models to increase the robustness of the model to deformations and variations in the input images.
* **Color quantization** is the process of reducing the number of colors in an image while preserving its overall visual appearance. This technique is commonly used to reduce the storage and processing requirements for digital images. However, this process can result in a loss of detail and color accuracy in the image, particularly in areas with complex color gradients or patterns.
* **Gaussian noise** is a type of noise that appears in low-light conditions and can cause random fluctuations in image intensity. It is modeled as additive white Gaussian noise and has a normal distribution with zero mean and a standard deviation that represents the noise level. Gaussian noise can affect the accuracy of depth estimation by reducing the contrast of the image and introducing errors in intensity values.
* **Impulse noise** is a type of noise that can be caused by bit errors and appears as isolated pixels with incorrect intensity values. It can be modeled as a random distribution of black and white pixels with a low probability of occurrence. It can affect the accuracy of depth estimation by introducing errors in the intensity values of pixels and distorting the image.
* **Shot noise**, also called Poisson noise, is electronic noise caused by the discrete nature of light itself. It occurs when photons hit a sensor and is modeled as a Poisson distribution. Shot noise can lead to irregularities in image intensity and affect the accuracy of depth estimation, especially in low-light conditions.
* **ISO noise** is a type of noise that can appear in digital images captured with high ISO settings. ISO refers to the sensitivity of the camera's sensor to light, with higher ISO values resulting in brighter images. However, increasing the ISO setting can also introduce additional electronic noise into the image, leading to a grainy or speckled appearance. This type of noise can be particularly challenging to remove without losing important image details.
* **Pixelation** occurs when an image is displayed or printed at a low resolution, resulting in individual pixels becoming visible. This can lead to a loss of detail and clarity in the image, bringing extra challenges for depth estimation models.
* **JPEG** is a lossy image compression format commonly used for storing and sharing digital images. JPEG compression reduces the file size of an image by removing some of the image data that is deemed less important or less noticeable to the human eye. Such compression can result in visible artifacts, such as blockiness or blurring, in the compressed image.

### Corruption Simulation

The corruption simulation tools are from two resources. We use the imagecorruptions tool1 from Michaelis _et al._ to simulate 'brightness', 'fog', 'frost','smow', 'contrast', 'defocus blur', 'glass blur','motion blur', 'zoom blur', 'elastic transform', 'Gaussian noise', 'impulse noise','shot noise', 'pixelate', and 'JPEG compression'. Additionally, We use the 3DCC tool2 by Kar _et al._ to simulate 'dark', 'color quantization', and 'ISO noise'.

We follow Hendrycks and Dietterich  to split each corruption simulation into five severity levels for _KITTI-C_ and four severity levels for _NYUDepth2-C_. The split strategy is the same as the ImageNet-C paper . Illustrative examples of different severity levels in _KITTI-C_ are shown in Fig. C. Illustrative examples of different severity levels in _NYUDepth2-C_ are shown in Fig. D, Fig. E, and Fig. F.

We provide a comprehensive corruption simulation toolkit that can be used to generate the defined \(18\) corruption types on any image dataset. This toolkit is publicly accessible at: https://github.com/ldkong1205/RoboDepth/blob/main/docs/CREATE.md. Refer to this page for more details and the code implementations of corruption simulations.

To validate the simulated corruptions are of high fidelity, especially for the types that are related to weather and lighting changes, we conduct a study on the pixel distributions. Assuming that a corruption simulation is realistic enough to reflect real-world situations, the distribution of a corrupted "clean" set should be similar to that of the real-world corruption set. We validate this using _ACDC_, _nuScenes_, _Cityscapes_, and _Foggy-Cityscapes_, since these datasets contain: _i)_ real-world corruption data; and _ii)_ clean data collected by the same sensor types from the same physical locations.

We simulate corruptions using "clean" images and compare the distribution patterns with their corresponding real-world corrupted data. We do this to ensure that there is no extra distribution shift from aspects like sensor difference (_e.g._ FOVs and resolutions) and location discrepancy (_e.g._ environmental and semantic changes). The results are shown in this paper: https://github.com/ldkong1205/RoboDepth/blob/main/docs/VALIDITY.md.

What is more, we follow Geirhos _et al._ and use AdaIn  to generate stylized images in _KITTI-S_. AdaIn is a classical style transfer model that takes a pair of two images (a reference image and a style image) as input and outputs a stylized image. The checkpoint of this style transfer model can be downloaded at: https://github.com/naoto0804/pytorch-AdaIN. The detailed procedures for generating stylized images are attached at: https://github.com/rgeirhos/Stylized-ImageNet. Illustrative examples of different styles in _KITTI-S_ are shown in Fig. G.

### License

Our datasets and benchmark toolkit are released under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license3, under the following terms:

Figure D: Illustrative examples of each of the five corruption types (‘brightness’, ‘dark’, ‘contrast’, ‘defocus blur’, and ‘glass blur’) across four severity levels (level \(1\) is the lightest and level \(4\) is the hardest) in the _NYUDepth2-C_ benchmark. Best viewed in color. Zoomed-in for more details.

* **Attribution** -- You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.
* **NonCommercial** -- You may not use the material for commercial purposes.
* **ShareAlike** -- If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.

### Download

Our datasets and benchmark toolkit are publicly accessible. We provide detailed procedures for accessing and downloading them. Please refer to the following pages for more details:

* GitHub Repo: https://github.com/ldkong1205/RoboDepth.
* Installation & Environment: https://github.com/ldkong1205/RoboDepth/blob/main/docs/INSTALL.md.
* Data Preparation: https://github.com/ldkong1205/RoboDepth/blob/main/docs/DATA_PREPARE.md.
* Benchmark Details: https://github.com/ldkong1205/RoboDepth#benchmark.
* Corruption Simulation Tool: https://github.com/ldkong1205/RoboDepth/blob/main/docs/CREATE.md.

Figure 1: Illustrative examples of each of the five corruption types (‘motion blur’, ‘zoom blur’, ‘elastic transform’, ‘color quantization’, and ‘Gaussian noise’) across four severity levels (level \(1\) is the lightest and level \(4\) is the hardest) in the _NYUDepth2-C_ benchmark. Best viewed in color. Zoomed-in for more details.

### Benchmarked Methods

* **MonoDepth2**: A seminar work in the field of MDE which proposed a series of improvements on top of prior works, including a per-pixel minimum re-projection loss, an auto-masking strategy, and a multi-scale estimation framework. We evaluate three versions of this model, under the Mono, Stereo, and Mono+Stereo settings, respectively. We also test this model with different backbones (ResNet-18 and ResNet-50) and pretraining strategies. The code is accessible at: https://github.com/nianticlabs/monodepth2.
* **DepthHints**: A stereo model that makes use of depth hints, which are small but informative cues about the scenes and can be extracted from the image itself. These hints are then used to guide the network during training, allowing the model to learn a better depth estimation representation without requiring any explicit ground truth depth information. The code is accessible at: https://github.com/nianticlabs/depth-hints.
* **MaskOcc**: A monocular model takes into account occlusion in depth estimation. The occlusions occur when objects in the scene block each other from view. The proposed method masks out occluded regions in the input image during training, allowing the model to focus on learning from only the visible parts of the scene. This approach results in more accurate depth estimation, especially in regions with occlusion. The code is accessible at: https://github.com/schelv/monodepth2.
* **DNet**: A monocular model consists of a coarse-to-fine depth estimation pipeline, where a deep neural network is trained to predict absolute depth maps from a single input image. This model is built upon the MonoDepth2 baseline. The code is accessible at: https://github.com/TJ-IPLab/DNet.

Figure F: Illustrative examples of each of the five corruption types (‘impulse noise’, ‘shot noise’, ‘ISO noise’, ‘pixelate’, and ‘JPEG compression’) across four severity levels (level \(1\) is the lightest and level \(4\) is the hardest) in the _NYUDepth2-C_ benchmark. Best viewed in color. Zoomed-in for more details.

* **CADepth**: A new network architecture that employs attention mechanisms to selectively attend to certain image regions, which helps to improve the accuracy of the depth estimation. The code is accessible at: https://github.com/kamiLight/CADepth-master.
* **HR-Depth**: A new depth estimation model that can generate high-resolution depth maps from a single input image. The main contribution of the work is the introduction of a hierarchical residual pyramid network architecture that leverages multi-scale features to produce accurate depth estimates. The code is accessible at: https://github.com/shawLyu/HR-Depth.
* **DIFFNet**: A monocular model based on a deep neural network architecture that uses an internal feature fusion mechanism to combine information from different layers of the network. Such a mechanism used in the network helps to capture global and local features of the input images, which improves the accuracy of the depth estimates. The code is accessible at: https://github.com/brandleyzhou/DIFFNet.
* **ManyDepth**: A multi-monocular model that leverages temporal consistency between adjacent frames in a video sequence to estimate accurate depth maps. The proposed framework consists of a network that takes as input multiple frames of a video sequence and outputs a corresponding depth map for each frame. The code is accessible at: https://github.com/nianticalabs/manydepth.
* **FSRE-Depth**: A monocular model consists of a two-stage network architecture that enhances the representation of the input image by incorporating fine-grained semantic information. The first stage of the network uses a semantic segmentation module to extract semantic information from the input image. The second stage of the network utilizes a depth refinement module to refine the initial depth prediction and generate the final depth estimation. The code is accessible at: https://github.com/hyBlue/FSRE-Depth.
* **MonoViT**: A monocular model that consists of a novel training scheme that leverages the spatial and semantic representations learned by the Vision Transformers to predict depth

Figure G: Illustrative examples of \(12\) different styles in the _KITTI-S_ benchmark. Best viewed in color. Zoomed-in for more details.

from a single RGB image. The code is accessible at: https://github.com/zxcqlf/MonoViT.
* **DynaDepth**: A monocular model that incorporates the motion dynamics data from an inertial measurement unit (IMU) to improve the accuracy and robustness of the depth estimation in challenging conditions. The proposed method is able to estimate the scale of the scene, which is a major challenge in monocular depth estimation, and is also more robust to illumination changes and dynamic scenes. The code is accessible at: https://github.com/SenZHANG-GitHub/ekf-imu-depth.
* **RA-Depth**: A monocular model that can adaptively adjust the output resolution based on the input image resolution. This is achieved through a two-stage architecture that first generates a coarse depth map and then refines it to the desired output resolution using a depth super-resolution network. The code is accessible at: https://github.com/hmhemu/RA-Depth.
* **TriDepth**: A monocular model with a new depth estimation loss function that incorporates a scale-invariant gradient term. This allows the model to learn to predict sharper edges and finer details in the depth map, which is critical for accurate depth estimation. The code is accessible at: https://github.com/xingyuuchen/tri-depth.
* **Lite-Mono**: A monocular model with a lightweight convolution backbone and transformer architecture for self-supervised monocular depth estimation. The authors propose a hybrid approach that combines the strengths of both CNNs and transformers to achieve state-of-the-art performance on several benchmarks while using significantly fewer parameters compared to existing methods. The code is accessible at: https://github.com/noahzn/Lite-Mono.
* **BTS**: A monocular model involves detecting and utilizing local planar structures in the image at multiple scales to improve depth estimation accuracy. Specifically, the authors propose a novel deep neural network architecture that incorporates a local planar guidance module at each scale of a feature pyramid. This module predicts a set of local planar patches in the image and uses them to provide guidance for the depth estimation module. The code is accessible at: https://github.com/cleinc/bts.
* **AdaBins**: A monocular model with an adaptive binning scheme that dynamically adjusts the bin sizes of the network to better capture the distribution of depth values in the scene. In traditional binning schemes, fixed bin sizes are used, which can lead to under or over-representation of certain depth values. AdaBins overcomes this limitation by adapting the bin sizes based on the distribution of depth values in the input image. The code is accessible at: https://github.com/shariqfarooq123/AdaBins.
* **DPT**: A monocular model that leverages Vision Transformers in place of CNNs as the backbone for dense prediction tasks including MDE. Several new techniques are proposed to facilitate dense predictions using self-attention. The code is accessible at: https://github.com/isl-org/DPT.
* **SimIPU**: A multi-modal contrastive learning framework consists of a simple pertaining strategy that leverages the spatial perception module to learn a spatial-aware representation from images and point clouds. The code is accessible at: https://github.com/zhyever/SimIPU.
* **DepthFormer**: A monocular model that is tailored to facilitate the long-range correlation for accurate MDE. The backbone is adopted from Vision Transformers, where a hierarchical aggregation and heterogeneous interaction module is proposed to enhance the features via element-wise interaction. The code is accessible at: https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox/tree/main/configs/depthformer.

### Depth Estimation Metrics

In our benchmark, we adopt the conventional reporting of Abs Rel (error rate) and \(_{1}\) (accuracy) for measuring the depth estimation performance.

Abs Rel measures the absolute relative difference between the ground-truth (gt) and the prediction (pred), as calculated via the following equation:

\[=_{pred D}\;.\] (3)

The \(\) metric is the depth estimation accuracy given the threshold:

\[_{t}=|\{\;pred D|,)}< 1.25^{t}\}| 100\%\;,\] (4)

where \(_{1}=<1.25,_{2}=<1.25^{2},_{3}=<1.25^{3}\) are the three conventionally used accuracy scores in prior works.

We combine Abs Rel and \(_{1}\), the two main measures, into a unified metric as \(=-_{1}+1}{2}\), which is constantly used as the indicator of depth estimation error in our benchmark.

## Appendix C The RoboDepth Challenge

In this section, we introduce The RoboDepth Challenge - an academic competition that is established based on the datasets and benchmarks proposed in this work.

### Competition Overview

The RoboDepth Challenge  aims to facilitate relevant research in the area of robust monocular depth estimation. It is hosted by the 40th IEEE Conference on Robotics and Automation (ICRA 2023). More materials on this competition are provided in the following links:

Figure 1: The submission and scoring statistics for the two tracks in the RoboDepth competition.

* Competition Page: https://robodepth.github.io.
* Competition Report: https://arxiv.org/abs/2307.15061.
* Toolkit: https://github.com/ldkong1205/RoboDepth/tree/main/competition.
* Workshop Recordings: https://www.youtube.com/watch?v=mYhdTGiIGCY&list=PLxxrrIfcH-qBGZ6x_e1AT2_YnAxiHIKtkB.
* Server for Track 1: https://codalab.lism.upsaclay.fr/competitions/9418.
* Server for Track 2: https://codalab.lism.upsaclay.fr/competitions/9821.

### Statistics

The RoboDepth Challenge started on January 1, 2023, and ended on May 25, 2023. There are two tracks in this competition: a self-supervised learning track for robust outdoor depth estimation and a supervised learning track for robust indoor depth estimation.

This competition attracted a lot of participants from around the world. Specifically, \(\) teams registered at our evaluation servers. Among them, \(\) teams made a total number of \(\) valid submissions. The detailed statistics in terms of submission and scoring are shown in Fig. 1. The top-three performing teams of Track 1 achieved the Abs Rel scores of \(0.121\), \(0.123\), and \(0.123\), respectively. The top-three performing teams of Track 2 achieved the a1 scores of \(0.940\), \(0.928\), and \(0.898\). More results are attached to our evaluation servers and competition homepage.

For more details, please refer to our competition page at: https://github.com/ldkong1205/RoboDepth/tree/main/competition.

### Workshop

We hosted The RoboDepth Workshop at ICRA on June 02, 2023. The video recordings of this workshop are publicly available at: https://www.youtube.com/watch?v=mYhdTGiIGCY&list=PLxxrrIfcH-qBGZ6x_e1AT2_YnAxiHIKtkB.

The slides of this workshop can be downloaded from https://ldkong.com/talks/icra23_robodepth.pdf.

### Leaderboards

The results of the top-ten teams from each of the two tracks are shown in Table 1 and Table 2, respectively. For more details, please refer to our competition page at: https://github.com/ldkong1205/RoboDepth/tree/main/competition.

## Appendix D Additional Quantitative Result

In this section, we provide the complete results of different monocular depth estimation models in the established _KITTI-C_, _NYUDepth2-C_, and _KITTI-S_ benchmarks.

  
**Team Name** & **Abs Rel \(\)** & **Sq Rel \(\)** & **RMSE \(\)** & **log RMSE \(\)** & \(<1.25\) & \(<1.25^{2}\) & \(<1.25^{3}\) \\   OpenSpaceAI & \(0.121\) & \(0.919\) & \(4.981\) & \(0.200\) & \(0.861\) & \(0.953\) & \(0.980\) \\ USTC-IArl-United & \(0.123\) & \(0.932\) & \(4.873\) & \(0.202\) & \(0.861\) & \(0.954\) & \(0.979\) \\ YYQ & \(0.123\) & \(0.885\) & \(4.983\) & \(0.201\) & \(0.848\) & \(0.950\) & \(0.979\) \\ zs\_dlut & \(0.124\) & \(0.899\) & \(4.938\) & \(0.203\) & \(0.852\) & \(0.950\) & \(0.979\) \\ UMCV & \(0.124\) & \(0.845\) & \(4.883\) & \(0.202\) & \(0.847\) & \(0.950\) & \(0.980\) \\ THU\_ZS & \(0.124\) & \(0.892\) & \(4.928\) & \(0.203\) & \(0.851\) & \(0.951\) & \(0.980\) \\ THU\_Chen & \(0.125\) & \(0.865\) & \(4.924\) & \(0.203\) & \(0.846\) & \(0.950\) & \(0.980\) \\ seesee & \(0.126\) & \(0.990\) & \(4.979\) & \(0.206\) & \(0.857\) & \(0.952\) & \(0.978\) \\ amename & \(0.126\) & \(0.994\) & \(4.950\) & \(0.204\) & \(0.860\) & \(0.953\) & \(0.979\) \\ USTCxNetEaseFuxi & \(0.129\) & \(0.973\) & \(5.100\) & \(0.208\) & \(0.846\) & \(0.948\) & \(0.978\) \\  MonoDepth2  & \(0.221\) & \(1.988\) & \(7.117\) & \(0.312\) & \(0.654\) & \(0.859\) & \(0.938\) \\   

Table 1: The top-ten performing teams on the leaderboard of Track 1 in the RoboDepth competition.

### Kitti-C

The clean performances of \(32\) MDE models under the standard KITTI Eigen split are shown in Table C.

The per-corruption DEE scores, CE scores, and RR scores of \(32\) MDE models in the _KITTI-C_ benchmark are shown in Table D, Table E, and Table F, respectively.

### NYUDepth2-C

The per-corruption DEE scores, CE scores, and RR scores of \(10\) MDE models in the _NYUDepth2-C_ benchmark are shown in Table G, Table H, and Table I, respectively.

### Kitti-S

The per-corruption DEE scores of \(32\) MDE models in the _KITTI-S_ benchmark are shown in Table J.

## Appendix E Additional Qualitative Result

In this section, we provide additional qualitative results of different monocular depth estimation models under out-of-distribution corruption scenarios.

Specifically, the qualitative results of MonoDepth2 , Lite-Mono , and MonoViT  under each of the \(18\) corruption types across five severity levels in our benchmark are shown in Fig. J, Fig. K, and Fig. L, respectively.

The qualitative results of MonoDepth2 , DIFFNet , RA-Depth , Lite-Mono , and MonoViT , under each of the \(18\) corruption types in our benchmark are shown in Fig. M and Fig. N.

The qualitative results of MonoDepth2 , DIFFNet , RA-Depth , Lite-Mono , and MonoViT , under each of the \(12\) styles in our benchmark are shown in Fig. O.

[MISSING_PAGE_EMPTY:30]

[MISSING_PAGE_EMPTY:31]

[MISSING_PAGE_EMPTY:32]

[MISSING_PAGE_EMPTY:33]

[MISSING_PAGE_EMPTY:34]

[MISSING_PAGE_EMPTY:36]

[MISSING_PAGE_EMPTY:37]

Figure 1: Qualitative results of MonoDepth2  under each corruption type across five severity levels. We show examples from the third level in the first column. The lighter regions correspond to near distances and vice versa. Best viewed in color. Zoomed-in for more details.

Figure 10: Qualitative results of Lite-Mono  under each corruption type across five severity levels. We show examples from the third level in the first column. The lighter regions correspond to near distances and vice versa. Best viewed in color. Zoomed-in for more details.

Figure 1: Qualitative results of MonoViT  under each corruption type across five severity levels. We show examples from the third level in the first column. The lighter regions correspond to near distances and vice versa. Best viewed in color. Zoomed-in for more details.

Figure M: Qualitative results of five monocular depth estimation models in the _KITTI-C_ benchmark, including MonoDepth2 , DIFFNet , RA-Depth , Lite-Mono , and MonoViT . The lighter regions correspond to near distances and vice versa. Best viewed in color. Zoomed-in for more details.

Figure N: Qualitative results of five monocular depth estimation models in the _KITTI-C_ benchmark, including MonoDepth2 , DIFFNet , RA-Depth , Lite-Mono , and MonoViT . The lighter regions correspond to near distances and vice versa. Best viewed in color. Zoomed-in for more details.

Figure 10: Qualitative results of five monocular depth estimation models in the _KITTI-S_ benchmark, including MonoDepth2 , DIFFNet , RA-Depth , Lite-Mono , and MonoViT . The lighter regions correspond to near distances and vice versa. Best viewed in color. Zoomed-in for more details.

Public Resources Used

In this section, we acknowledge the use of public resources, during the course of this work:

* KITTI Vision Benchmark4  CC BY-NC-SA 3.0
* NYU Depth Dataset V25  Unknown
* nuScenes6  CC BY-NC-SA 4.0
* nuScenes-devkit7  Apache License 2.0
* Cityscapes8  Custom Cityscapes License
* Foggy-Cityscapes9  Custom Foggy-Cityscapes License
* ACDC10  Custom ACDC License
* SeasonDepth Benchmark Toolkit11  CC BY-NC-SA 4.0
* Make3D12  Custom Make3D License
* MonoDepth213  Custom MonoDepth2 License
* DepthHints14  Custom DepthHints License
* MaskOcc15  Custom MonoDepth2 License
* DNet16  Custom MonoDepth2 License
* CADepth17  MIT License
* HR-Depth18  MIT License
* DIFFNet19  Unknown
* ManyDepth20  Custom ManyDepth License
* FSRE-Depth21  MIT License
* MonoViT22  MIT License
* DynaDepth23  Unknown
* RA-Depth34  Unknown
* TriDepth25  GNU General Public License v3.0
* Lite-Mono26 ```

[MISSING_PAGE_POST]

* BTS28
* AdaBins29
* DPT30
* SimIPV31
* DepthFormer32
* ImageCorruptions33
* 3DCC34
* ImageNet-C35
* Style2Datasets36
* PyTorch-AdaIN37
Footnote 37: https://github.com/naoto0804/pytorch-AdaIN.