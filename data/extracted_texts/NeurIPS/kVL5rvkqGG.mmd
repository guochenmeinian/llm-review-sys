# Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe

Albert Q. Jiang

University of Cambridge

&Alicja Ziarko

IDEAS NCBR

University of Warsaw

IMPAN

&Bartosz Piotrowski

IDEAS NCBR

Wenda Li

University of Edinburgh

&Mateja Jamnik

University of Cambridge

&Piotr Milos

IDEAS NCBR

University of Warsaw

IMPAN, deepsense.ai

Equal contribution.Equal advising contribution.Equal advising contribution.Equal advising contribution.

###### Abstract

Text embeddings are essential for many tasks, such as document retrieval, clustering, and semantic similarity assessment. In this paper, we study how to contrastively train text embedding models in a compute-optimal fashion, given a suite of pre-trained decoder-only language models. Our innovation is an algorithm that produces optimal configurations of model sizes, data quantities, and fine-tuning methods for text-embedding models at different computational budget levels. The resulting recipe, which we obtain through extensive experiments, can be used by practitioners to make informed design choices for their embedding models. Specifically, our findings suggest that full fine-tuning and low-rank adaptation fine-tuning produce optimal models at lower and higher computational budgets respectively.

## 1 Introduction

_Text embeddings_ are vector representations of text sequences (Devlin et al., 2019). A desired property of text embeddings is that their distribution in the embedding space reflects the semantic relations of the embedded texts. This property benefits multiple practical tasks (Muennighoff et al., 2023), such as document retrieval (where documents are ranked based on the distance between their representations and that of a query), document clustering, sentiment analysis, or textual similarity measurement.

In the era of large language models (LLMs) that are pre-trained on vast textual data, it is a natural idea to capitalise on the language representations learned by these models to achieve high-quality embedding models. A common and effective approach has been to initialise an embedding model from a pre-trained LLM and fine-tune it with a _contrastive loss_(Neelakantan et al., 2022; Izacard et al., 2022). This phase is

Figure 1: The optimal loss achieved using four different fine-tuning methods (full fine-tuning, only tuning the bias, low-rank adaptation, and freezing transformer blocks) at given budgets. The horizontal axis is the computational budget in floating point operations (FLOP) and the vertical axis is the contrastive loss. The X marks are datapoints and dotted lines are fitted linear trends for different methods. The solid black line is the “optimal frontier,” i.e., the optimal loss achievable with a fixed budget and the best method.

of crucial importance, as the quality of the embeddings extracted directly from a pre-trained LLM (e.g., by averaging hidden states of the last layer) is not sufficient for most tasks.

However, the best performing modern LLMs are usually massive in terms of the parameter counts (e.g., 175B parameters for GPT3 (Brown et al., 2020) and 340B for PaLM2 (Anil et al., 2023)), and are therefore difficult to train with limited resources. This motivates a practical question, that to the best of our knowledge has not been addressed systematically, and thus we address it in this paper: _what is the best embedding model one can train from a backbone decoder-only LLM with a fixed training compute budget_?

To answer this, we performed an extensive empirical study. We started by identifying design choices one can make when fine-tuning language models into embedding models, including: model size, data volume, parameter-efficient fine-tuning technique, and the hyperparameters of the chosen technique. Then, we performed a grid search over the pre-defined design choices, and found the best performing configuration under each computational budget. Using these findings, we established scaling laws for contrastive fine-tuning, which enabled us to build an algorithm that produces a general recipe for efficient fine-tuning of a decoder-only LLM that obtains a high-quality embedding model.

ContributionWe comprehensively experimented with contrastively fine-tuning language models into text embedding models. We analysed how the choice of model sizes, data quantities, and fine-tuning methods affect the performance in the resource-constrained training regime. We compiled the findings into an algorithm that, given a fixed computational budget, predicts the optimal network architecture, data quantity, and parameter-efficient fine-tuning hyperparameters. We open-source the code to train and evaluate our models at: https://github.com/SeqDM/Efficient-Embeddings.

## 2 Related work

Embedding modelsTraining neural networks to represent text as continuous vectors was popularised by word2vec (Mikolov et al., 2013), which produced semantically meaningful embeddings of _words_. BERT (Devlin et al., 2019) and the further contrastively trained SimCSE (Gao et al., 2021) quickly established encoder-only transformers as the go-to architecture for embedding _text_.

More recently, decoder-only transformers have become increasingly more powerful and efficient at the same time (Brown et al., 2020; Touvron et al., 2023; Jiang et al., 2023). Building an embedding model based on them utilises the knowledge from their pretraining, and thus it is a natural step. Neelakantan et al. (2022) set a successful precedent in this paradigm by initialising embedder training with decoder-only GPT models.

Notably, the current open state-of-the-art (the top open model on the MTEB (Muennighoff et al., 2023) leaderboard, as of 18 May 2024), SFR-Embedding-Mistral (Meng et al., 2024), is also fine-tuned from the decoder-only Mistral 7B (Jiang et al., 2023).

Benchmarks for embedding modelsEmbedding models are versatile in their applications. Therefore, a broad and diverse benchmark to provide a robust measure of their performance is called for. The first such benchmark was BEIR introduced by Thakur et al. (2021). It has nine different information retrieval tasks (e.g., duplicate-question retrieval or citation-prediction) on 18 datasets. Recently, Muennighoff et al. (2023) introduced MTEB (Massive Text Embedding Benchmark), which is substantially larger than BEIR and, in addition to information retrieval, has seven other types of tasks (e.g., clustering, summarisation, and pair classification) using 58 datasets and covering 112 languages. The authors also provide a leaderboard that currently contains more than 220 entries.

Scaling lawsScaling laws predict the performance of models at larger scale (e.g., more parameters, more data) from experiments at smaller scales. Kaplan et al. (2020) and Hoffmann et al. (2022) used empirically fitted laws to predict the performance of neural language models at different model sizes and data scales, making it possible to calculate the optimal configuration before training. Muennighoff et al. (2023) inspected model training in data-constrained, multi-epoch regime. Frantar et al. (2023) looked at scaling laws for language models in the context of weight sparsity. Zhang et al. (2024) investigates the scaling laws for model fine-tuning, including parameter efficient methods: LoRA (Hu et al., 2022) and prompt tuning (Lester et al., 2021). They show that the scaling laws and the best fine-tuning method are task dependent. Biderman et al. (2024) investigates the properties of fine-tuning models with LoRA in detail, showing that LoRA underperforms full fine-tuning when fine-tuning the models on mathematical or code datasets.

The single most related investigation is a concurrent work Fang et al. (2024), where the scaling laws for encoder models for retrieval are investigated. There are significant differences in the settings we consider. Our focus is on investigating the process of fine-tuning decoder-only models for good quality embeddings. Moreover, our main goal is to find which strategy for achieving good embeddings is optimal in a budget-restricted settings, while taking into account the popularity of applying parameter efficient methods for fine-tuning, like LoRA or partial model freezing. In spirit, our work is similar to AutoML (He et al., 2021), which aims to automatically find the optimal ML architecture. However, our method goes beyond and targets the data and the training method as well.

Parameter-efficient fine-tuningModern language models have a lot of parameters, and fine-tuning them using consumer-grade hardware is difficult in general. Notwithstanding the recent turn to _in-context learning_, fine-tuning is still necessary for most applications. A range of techniques have been developed recently, to make fine-tuning more efficient (Houlsby et al., 2019; Li and Liang, 2021; Qi et al., 2022; He et al., 2021). These approaches are all applicable to embedding models. Muennighoff (2022) explored a simple parameter-efficient approach to repurpose GPT models into embedding models where only the bias tensors of the transformer model are updated (Zaken et al., 2022). (Sun et al., 2024) developed a parameter-efficient method designed specifically for fine-tuning embedding models. However, a systematic study of what parameter-efficient methods are optimal under what scenarios has not been performed, which is the aim of our work.

## 3 Preliminaries

### Scaling laws and compute-optimal models

The constraint optimisation problem that we aim to solve is the following: _given a fine-tuning corpus and a family of pre-trained decoder-only language models of different sizes, minimise the contrastive loss subject to a fixed computational budget._

Following Hoffmann et al. (2022, Section 3.2), we perform a hyperparameter search at different computational budget levels to obtain IsoFLOP profiles of models. We then find the optimal models at each level, fit a loss-size scaling law, and extrapolate it to unobserved FLOP budgets. We only focus on contrastive fine-tuning since state-of-the-art text embedder models are all trained in such fashion (Gao et al., 2021; Neelakantan et al., 2022; Wang et al., 2023).

We refer to the models with the lowest contrastive loss achievable with fixed computational costs as _compute-optimal models_. We focus on optimising three components specifically: model size, data quantity, and fine-tuning method. For each fine-tuning method, we find the optimal model configuration at each FLOP level and establish the relationship between the computational budget and optimal loss for that method. Then, for any unobserved computational budget, we use the scaling laws to predict the loss, and pick the method that gives the lowest loss and its corresponding configuration.

### Extracting representations from transformers

We start with a standard decoder-only transformer model architecture, such as GPT architectures (Radford et al., 2019; Brown et al., 2020), or Pythia (Biderman et al., 2023). Given a sequence of tokens \(S=(s_{1},,s_{n})\), we pass them through the transformer model and average the last layer representations as the embedding vector \(}(S)=_{i=1}^{n}h_{i}^{m}\), where \(h_{i}\) is the last layer hidden state of the token \(s_{i}\).

Some works apply the last token pooling approach (Neelakantan et al., 2022) instead of using mean pooling described above. Here, however, we default to mean pooling as in (Wang et al., 2022; Li et al., 2023; Gunther et al., 2023). It is a more popular method overall, and moreover, we find it to yield better performance, which we demonstrate in Appendix E.3.

### Contrastive fine-tuning

We use the contrastive loss as in (Neelakantan et al., 2022), which has been widely used in fine-tuning pre-trained language models (Wang et al., 2022, 2023; Gunther et al., 2023).

For a batch of \(n\) examples, that is, \(n\) pairs of texts \((x_{1},y_{1}),,(x_{n},y_{n})\), we only treat examples \((x_{i},y_{i})_{i=1}^{n}\) as positive matches, and all the pairs \((x_{i},y_{j})\), where \(i j\) as negatives. Concretely, we calculate the batch contrastive loss in two steps. First, we calculate the logits:

logits[i,j] \[=((x_{i}),(y_{j})) (),\]where sim is the cosine similarity function between two vectors, and \(\) is the temperature. The total loss is the sum of the cross entropy losses on both the row and the column directions:1

labels = [0, 1,..., n - 1] l_r = cross_entropy(logits, labels) l_c = cross_entropy(logits.T, labels) loss = (l_r + l_c) / 2

Hard negatives in contrastive learningIn our contrastive learning setting, we use in-batch examples as negatives. Some datasets additionally include tailored negative examples (_hard negatives_) for each positive datapoint, which can be incorporated into the contrastive loss to promote learning more precise representations. Moreover, there are works that focus on approaches for mining hard negatives which result in better training data in the context of specific downstream tasks (Xiong et al., 2021; Zhan et al., 2021).

However, recent works aiming at training powerful, general-purpose embedding models that do rely on datasets with hard negatives often arrive at embeddings by having two distinct training phases: the expensive "unsupervised" _phase one_ involving vast data from the internet, and then a "supervised" _phase two_, often targeted towards a specific downstream task, where training data of lower quantity - but containing high-quality hard negatives - is used (Li et al., 2023; Wang et al., 2022; Xiao et al., 2023). Since the first phase is more expensive, it will benefit more from scaling laws such as the ones we derive.

Moreover, the usefulness of hard negatives highly depends on their quality, which may vary wildly between datasets. Therefore, conclusions reached with hard negatives are more closely tied to the datasets the models are trained on. Since we develop scaling laws agnostic to the dataset, we abstain from using hard negatives in our experiments.

### Fine-tuning methods

The most basic and straightforward method of fine-tuning is _full fine-tuning_, where _all_ the weights of the model are updated under the contrastive objective.

We also study parameter-efficient fine-tuning (PEFT) methods, which are popular, especially in academic settings, because they have lower memory requirements than full fine-tuning. Since the PEFT methods update fewer parameters in the network than there are in total, the relationship between their computational cost, model size, and data quantity is different from full fine-tuning. Hence, we individually study the IsoFLOP profiles for four fine-tuning methods, namely, _full fine-tuning_, _block freezing_, _bias-only tuning_, and _Low-Rank Adaptation (LoRA)_. Each of the last three non-standard methods is briefly characterised below.

Block freezingIn this approach, we _freeze_ the LLM token-embedding layer as well as the first \(k\) transformer blocks so that their parameters stay fixed during fine-tuning. Only the latter part of the model is back-propagated through and updates its parameters. To process the same amount of data, this is more computationally efficient than full fine-tuning. By varying the number of frozen blocks, one trades-off between the computational efficiency and flexibility of the model.

Low-rank adaptation (LoRA)This method was introduced by Hu et al. (2022) to significantly reduce the number of trainable parameters of a neural network while maintaining high performance. LoRA introduces a small number of weights into the model and only trains those. It can be applied to a subset of dense layers of a model. It works by parametrising each dense layer \(W\) from a subset of linear layers of a model by two low-rank matrices \(A\) and \(B\), and using \(W+AB\) instead of \(W\), while training only \(A\) and \(B\).

Bias-only tuningIn this method, only bias parameters are fine-tuned, and the rest of the model remains frozen. This approach was proposed by Zaken et al. (2022) as a parameter-efficient fine-tuning method for BERT encoders, and recently applied by Muennighoff (2022) to contrastively fine-tune GPT models.

### Calculating computational cost

We denote the number of non-token-embedding parameters used in the forward pass to process each token as \(N_{F}\), the number of non-token-embedding parameters in backward-propagation as \(N_{B}\), the number of non-token-embedding parameters updated during backward-propagation as \(N_{U}\), the total number of tokens processed as \(D\), and the computational cost as \(C\) floating point operations (FLOP). Following the calculation by Kaplan et al. (2020), we derive the relationship between the variables to be

\[C=2N_{F}D+2N_{B}D+2N_{U}D.\]

In case of full fine-tuning, every parameter is trainable, so \(N_{F}=N_{B}=N_{U}\), and therefore

\[C=6N_{F}D.\]

This is the same as the formula for the computational cost used by Kaplan et al. (2020) for pre-training.

## 4 Experiments

We first specify the relevant details of our experimental setup (Section 4.1). Next, we present the results of our experiments where we contrastively train a grid of models of different sizes, using different computational budgets, and apply different compute-optimal fine-tuning methods with varying hyperparameters (Section 4.2). Based on the collected data, we fit a mathematical formula describing the observed scaling laws (Section 4.3). Finally, we provide general observations and recommendations for efficient contrastive fine-tuning (Section 4.6). In these experiments2 we investigate the compute-constrained setup, but we present limited results on the data-constrained setup in Appendix D.

### Experimental setup

We use eight decoder-only models from the Pythia suite (Biderman et al., 2023), which have sizes 14M, 31M, 70M, 160M, 410M, 1B, 1.4B, and 2.8B of parameters. All the models have been pre-trained on the Pile dataset (Gao et al., 2021) for 300 billion tokens. We consider six computational budgets equally spaced on the log-scale: \(1.515\), \(615\), \(2.416\), \(9.616\), \(3.817\), and \(1.518\) FLOP.

We fine-tune our models on the English partition of the BAAI BGE dataset (Xiao et al., 2023), which contains 200 million semantically related (_query_, _value_) pairs from various internet sources such as Wikipedia and Stack Exchange. Note that since the BAAI BGE dataset is massive, for all our experiments we fine-tune for less than one epoch on this corpus, which allows us to avoid the effect of diminishing returns on subsequent training epochs (Muennighoff et al., 2023).

We use the AdamW optimiser (Loshchilov and Hutter, 2019) and a cosine learning rate scheduler during training. The learning rate first goes through a linear warm-up phase of \(1/10\) of the total steps to a peak that is \(1/10\) of the pre-training peak learning rate, that is, to learning rates between \(1.2 10^{-5}\) and \(6 10^{-5}\). Then, it decays in a cosine schedule to \(1/10\) of the maximum at the end of training. We employ gradient checkpointing (Chen et al., 2016) and GradCache (Gao et al., 2021) to limit the peak GPU RAM usage during training. We use the Transformers (Wolf et al., 2020) library for the training of the models and use the Accelerate (Gugger et al., 2022) library to facilitate multi-GPU training. The batch size in our main line of experiments is 1024 sequences and the context length is 75 tokens. We give the full list of training hyperparameters in Appendix E.

We also perform a less extensive grid of experiments to confirm that our findings are robust to change in hyperparameters. We find that our scaling law formula fitted to losses from models with batch size 1024 and context length of 75 tokens describes the loss for models trained with batch size 2048 and context length of 512 tokens very well. The detailed description and plots are shown in Appendix G.

In addition to controlling the final training contrastive loss achieved by the models, we also measure downstream performance by evaluating the models on a representative subset of the MTEB benchmark (Muennighoff et al., 2023). The benchmark defines eight categories of tasks in total (e.g., retrieval, semantic text similarity). We select one task from each category by determining which ones are the most correlated with the performance for the whole category (see Appendix B for details).

### Experimental results for different methods

Full fine-tuningWith full fine-tuning, the numbers of forward and backward parameters are the same, and the computational cost is straightforwardly \(C=6N_{F}D\). In Figure 1(a), we present the final contrastive loss vs. the number of parameters in the network on a log-log scale.

As expected, larger computational budgets consistently improve the loss. The IsoFLOP profiles resemble the classical ones from Hoffmann et al. (2022). However, they tend to be flatter when the model size and computational budget are both small or both large. From the IsoFLOP profiles we can see what is the optimal model size that minimises the loss at each computational budget.

**Block freezing** Since the Pythia models we experimented with have \(6,12,16,24\) or \(32\) blocks, we freeze \(,,,\) of their blocks if their number is divisible by \(6\), and \(,,,\) it is divisible by \(8\). In this setup, unlike for full fine-tuning, the token-embedding parameters always remain frozen even when all the transformer blocks are active. We have \(N_{B}=N_{U}<N_{F}\), and hence cost \(C=2N_{F}D+4N_{B}D\).

In Figure 1(b), we present IsoFLOP profiles for optimal loss at given model sizes. The trend of the profiles is quite similar to that of full fine-tuning presented in Figure 1(a).

We plot the loss with varying model sizes, computational budgets, and fractions of frozen blocks in Figure 3. For smaller models (\( 160\)M parameters), a lower fraction of frozen blocks leads to a lower final loss across all computational budgets. However, for larger models, using more than \(70\%\) of transformer often only marginally improves the model. For instance, for the model with \(1e9\) parameters, it is visible that freezing \(50-70\%\) of the blocks gives the optimal results for all budgets except for \(3.8e17\) FLOP.

**Bias-only tuning** Here we simply update only the bias parameters and not the weights of the model. Figure 3(a) shows the final training loss across all the model sizes and computational budgets. Increasing the computational budget decreases the optimal loss that is achievable, although the absolute values of the losses are high compared to other fine-tuning methods (the lowest achievable loss is above 0.4 with bias-only tuning).

**Low-rank adaptation** We use the LoRA implementation from the PEFT library (Mangrulkar et al., 2022). We attach the adapter to all the dense layers of the network, and vary the adapter rank from \(8\) up to \(2048\), since even lower ranks resulted in an unstable training.

We compare the loss obtained for each model size and computational budget for different LoRA rank values in Figure 5. In almost all the combinations of the model size and computational budget, the rank of 32 or 128 (occasionally 512) is optimal. Wang et al. (2023) also use LoRA in the context of contrastive learning, but they fix the rank of 16 for their experiments. Our findings suggest that this choice should be reconsidered as being potentially suboptimal.

Figure 2: **(a) IsoFLOP profiles for _full fine-tuning_. The horizontal axis is the number of parameters in the model, and the vertical axis is the achieved loss. Both axes use log-scale. The optimal model size tends to increase as the computational budget increases. (b) IsoFLOP profiles for _block freezing_. The axes are the same as for full fine-tuning. Each data point denotes the optimal choice with respect to the fraction of active blocks during training, which is noted above the points. The optimal model size tends to increase as the computational budget increases, while the optimal active block fraction tends to slightly decrease as the model size gets larger.**

[MISSING_PAGE_FAIL:7]

### Scaling laws for embeddings

We aim to model the behaviour of the loss \(L\) with an analytical formula as a function of the number of parameters \(N\) and the number of training tokens \(D\). Following Hoffmann et al. (2022), we start with the following formula:

\[L(N,D)=C+}+},\]

where \(A,B,C,,_{+}\) are real-valued parameters to be fitted. We found that this formula reasonably describes the loss behaviour for each of our methods. However, the fraction of _trainable parameters_ is an important factor that is not considered in the model above. Following the modelling of Frantar et al. (2023), we propose a modified formula:

\[L(S,N,D)=C+(D)+b_{d}}{N^{}}+(1-S)^{b_{s}}+c_{s} }{D^{}},\]

where \(S\) is an additional variable representing the trainable fraction of parameters, and \(a_{d},b_{d},a_{s},b_{s},c_{s}\) are additional coefficients to be fitted. We check that this new formula fits the data better than the original one, and conjecture that it can be used for larger models and larger computational budgets. More details about the derivation are presented in Appendix F.

### Compute-optimal frontier and recipe

In Figure 1, we plotted the best achievable losses against the computational budgets for the four fine-tuning methods considered. We then fitted a linear trend for each method on a log-log scale (equivalent to a power law relationship on a normal scale), and highlighted the lowest loss achievable at given

Figure 5: The effect of different LoRA ranks across all model sizes. Different colours signify different computational budgets. The inflected curves indicate that it is less beneficial to use a rank from either extremes of the spectrum (8 or 2048). The detrimental effect of the high rank of 2048 is stronger for lower computational budgets. Ranks of 32 and 128 result in the lowest loss overall.

Figure 6: Optimal model size vs. computational budget for **(a)** full fine-tuning, and **(b)** LoRA.

budgets using the best corresponding fine-tuning method. We call the contour of this function that predicts the optimal loss across methods the compute-optimal frontier.

The equation describing the full fine-tuning and the LoRA linear fits are

\[()=-0.21()+8.39,()=-0.22()+8.93,\]

respectively. The equations suggest that at a budget lower than \(9.06e16\) FLOP, the optimal model is achieved with full fine-tuning, and at a higher budget, the optimal model is achieved with LoRA.

With this, one becomes fully equipped to deduce the optimal recipe for text embedding model training for a given budget, by carrying out the procedure detailed in Algorithm 1. Note that although the block freezing approach is not predicted to be optimal at any budget, its performance is quite close to optimal. It also has the advantage of being easy to implement and has lower memory requirements, hence might be a good method to choose in practical use cases for larger budgets. Its optimal model size and fraction of active blocks hyperparameter can be worked out in the same way as for the other methods, with the help of Figure 3 and Figure 13 (left pane) in Appendix A.

We briefly discuss the case of data-optimal frontier in Appendix D. We do not emphasise this setup due to it being less standard in language modelling research.

### Generalisation

To assess the generality of our findings, we investigate whether our analytical formula for predicting loss--fitted to datapoints from the Pythia suite (from Section 4.3) can also describe the behaviour of models from a different family. We conduct experiments on two language models, namely Gemma 2B (Mesnard et al., 2024) and Gemma 2 2B Team et al. (2024). We trained both models with the computational budgets of \(1.515\), \(615\), \(2.416\), \(9.616\), \(3.817\), and \(1.518\) FLOP.

Figure 7 presents results for Gemma 2B with both full fine-tuning and LoRA fine-tuning, while results for Gemma 2 2B are shown in Appendix A. In both cases, the formula provides a close approximation of the loss behavior. Although this study is limited to a single model size, it offers an initial indication that our findings may generalize to new models.

### Takeaways

We compile the takeaways from our extensive experiments by analysing each fine-tuning method individually and contrasting them afterwards.

* Bias-only tuning is not a good fine-tuning strategy for embedding model training, as it is consistently worse than other strategies under IsoFLOP comparisons.
* LoRA and block freezing are both effective approaches that improve the performance of the trained embedding models at higher budgets. For LoRA, the rank hyperparameter is not very sensitive to model size or computational budgets, with an optimum at around 128.
* Finding the optimal recipe for fine-tuning embedding models requires a non-trivial effort, and the resulting optimal configurations are sometimes counter-intuitive (e.g., not using the largest model size possible). Carefully tuning hyperparameters is crucial and should be practised systematically.

## 5 Limitations and future work

Other families of modelsIn this paper, we experimented with the Pythia [Biderman et al., 2023] suite of pre-trained models, which are of different sizes but have all been trained on 300B tokens. This means that they have been over-trained with respect to the Chinchilla scaling law [Hoffmann et al., 2022] to different degrees. The relationship between the optimal model size and the computational budget might differ for a suite of pre-trained models with different over-training ratios. We note that prior efforts [Dettmers et al., 2024] have found that fine-tuning conclusions reached with Pythia do generalise to other model families such as OPT [Zhang et al., 2022], LLaMA [Touvron et al., 2023], and BLOOM [Scao et al., 2022], in the quantised setting. Indeed, in Section 4.5, we find that a model from a different family (Gemma [Mesnard et al., 2024]) closely fits our loss predictions.

AveragingDue to constraints on our computational resources, we ran each experiment exactly once. Averaging over multiple random seeds for the experiments would reduce the variance involved in each, and potentially result in less noisy conclusions. This is a straightforward extension that we leave to carry out in future work, should we have more resources. We also plan to evaluate the trained models on the entirety of the MTEB benchmark [Muennighoff et al., 2023b], instead of using the losses from the last steps of training, to better reflect the performances of embedding models.

Other forms of embedding readoutWe only experimented with mean pooling as a way of extracting embeddings from the transformer models. Other readout methods could be used, like the max pooling or extracting the hidden state corresponding to the end-of-sequence token, which might result in different scaling laws and optimal frontiers.

Inference cost analysisIn this work, we only focus on the training cost and refrain from taking inference costs into account. However, in certain practical scenarios, the latter may also be relevant.

## 6 Conclusions

In this paper, we systematically investigated the influence of pre-trained model size, fine-tuning data quantity, and fine-tuning method in the final performance of embedding models repurposed from language models. As a result, we devised an algorithm that takes in the computational budget and returns the optimal configuration for the embedding model to train. This enables researchers who wish to adapt language models to embed their own data, especially those with limited computational resources, to obtain optimal text embedding models with greater efficiency in time and resources.