# PAC-Bayes Generalization Certificates for Learned Inductive Conformal Prediction

Apoorva Sharma

NVIDIA Research

apoorvas@nvidia.com &Sushant Veer

NVIDIA Research

sveer@nvidia.com &Asher Hancock

Princeton University

ah4775@princeton.edu &Heng Yang

Harvard University

& NVIDIA Research

hengy@nvidia.com &Marco Pavone

Stanford University

& NVIDIA Research

mpavone@nvidia.com &Anirudha Majumdar

Princeton University

ani.majumdar@princeton.edu

###### Abstract

Inductive Conformal Prediction (ICP) provides a practical and effective approach for equipping deep learning models with uncertainty estimates in the form of set-valued predictions which are guaranteed to contain the ground truth with high probability. Despite the appeal of this coverage guarantee, these sets may not be efficient: the size and contents of the prediction sets are not directly controlled, and instead depend on the underlying model and choice of score function. To remedy this, recent work has proposed learning model and score function parameters using data to directly optimize the efficiency of the ICP prediction sets. While appealing, the generalization theory for such an approach is lacking: direct optimization of empirical efficiency may yield prediction sets that are either no longer efficient on test data, or no longer obtain the required coverage on test data. In this work, we use PAC-Bayes theory to obtain generalization bounds on both the coverage and the efficiency of set-valued predictors which can be directly optimized to maximize efficiency while satisfying a desired test coverage. In contrast to prior work, our framework allows us to utilize the entire calibration dataset to learn the parameters of the model and score function, instead of requiring a separate hold-out set for obtaining test-time coverage guarantees. We leverage these theoretical results to provide a practical algorithm for using calibration data to simultaneously fine-tune the parameters of a model and score function while guaranteeing test-time coverage and efficiency of the resulting prediction sets. We evaluate the approach on regression and classification tasks, and outperform baselines calibrated using a Hoeffding bound-based PAC guarantee on ICP, especially in the low-data regime.

## 1 Introduction

Machine learning (ML) models have rapidly proliferated across numerous applications, including safety-critical ones such as autonomous vehicles (Schwarting et al., 2018; Waymo, 2021), medical diagnosis (Ahsan et al., 2022), and drug discovery (Vamathevan et al., 2019). Due to the severity of outcomes in these applications, decision making cannot solely hinge on "point" predictions from the ML model, but must also encapsulate a measure of the uncertainty in the predictions. As a result, accurate uncertainty estimation is a cornerstone of robust and trustworthy ML systems; however, overly-optimistic or overly-conservative estimates limit their usefulness. In this paper, we will present a method that builds upon inductive conformal prediction (ICP) (Vovk et al., 2005) and probably approximately correct (PAC) Bayes theory (McAllester, 1998) to furnish uncertainty estimates that _provably_ control the uncertainty estimate's conservatism while achieving desired correctness rates.

ICP has emerged as a practical approach for equipping deep learned models with uncertainty estimates in the form of set-valued predictions with high-probability coverage guarantees, i.e., with high probability, at test time, the ground truth labels will lie within the predicted set. However, post-hoc application of ICP can often lead to overly-conservative set sizes; the tightness of the predicted set (referred to as efficiency) highly depends on the underlying model as well as the choice of the score function used in ICP. Various approaches have worked towards alleviating this challenge either by developing new application-specific score functions (Romano et al., 2020; Yang and Pavone, 2023; Lindemann et al., 2023) or by optimizing the model and / or score function directly (Yang and Kuchibhotla, 2021; Cleaveland et al., 2023; Bai et al., 2022). The direct optimization approach is appealing as a general application-agnostic method for obtaining tight uncertainty estimates; however, it requires re-calibration on held-out data in order to retain generalization guarantees on coverage. By coalescing ICP with PAC-Bayes, we are able to train using all available data, while retaining generalization guarantees for coverage _and_ efficiency; see Fig. 1 for more details.

Contributions:In this work, we make the following core contributions:

1. We use PAC-Bayes theory to obtain generalization bounds on both the coverage and efficiency of set-valued predictors. In contrast to prior work (see Fig. 1 and Sec. 3), our framework allows us to utilize the _entire_ calibration dataset to learn the model parameters and score function (instead of requiring a held-out set for obtaining a guarantee on coverage).
2. We leverage these theoretical results to provide a practical algorithm (see Alg. 1) for utilizing calibration data to fine-tune the parameters of the model and score function while guaranteeing test-time coverage and efficiency. This algorithm allows the user to specify a desired coverage level, which in turn specifies the degree to which efficiency can be optimized (since there is a trade-off between the desired coverage level and efficiency).
3. We evaluate our approach on regression and image classification problems with neural network-based score functions. We show that our method can yield more efficient predictors than prior work on learned conformal prediction calibrated using a Hoeffding-bound based PAC guarantee on coverage (Vovk, 2012; Prop 2a).

## 2 Background: Inductive Conformal Prediction

We consider a supervised learning setup, wherein our goal is to predict labels \(\) given inputs \(\). In inductive conformal prediction (ICP) (Vovk et al., 2005), our goal is to develop a set-valued predictor which maps inputs \(\) to a subset of the label space \(C()\). Specifically, we want this set-valued function to satisfy a _coverage_ guarantee, ensuring that the chance that the prediction set fails to contain the ground truth label is bounded to a user-specified level \(\), i.e.

\[}_{,}( C()),\] (1)

Figure 1: Standard ICP uses a calibration dataset to guarantee coverage on exchangeable test data, but the efficiency of the resulting prediction sets is not directly optimized. Using the same data to select score function and model parameters as well as to perform ICP no longer provides guarantees on test-time coverage and efficiency. Therefore, prior work on optimal ICP has typically relied on holding out some calibration data for a post-optimization recalibration step to retain coverage guarantees. In this work, we leverage PAC-Bayes theory to optimize efficiency and provide generalization guarantees using the same dataset. Practically, we demonstrate that a hybrid approach using some data to tune a data-dependent prior, and use the rest to further optimize and obtain generalization guarantees yields the best performance.

where \(\) is the joint distribution over inputs and labels that test data are drawn from. In practice, \(\) is not known, but we assume we have access to a calibration dataset \(D_{}=\{(_{i},_{i})\}_{i=1}^{N}\) where each example \((_{i},_{i})\) is exchangeable with the test data. Given this set-up, ICP constructs a set-valued predictor of the form

\[C(;):=\{\;\;s(,)\},\] (2)

i.e., the \(\) sub-level set of the _nonconformity function_\(s:\) evaluated at the input \(\). The nonconformity function assigns a scalar value quantifying how poorly an example \(,\) conforms to the training dataset. It is common to define a nonconformity function that depends on the learned model \(f\), choosing \(s(,)=(f(),)\), measuring how poorly the prediction \(f()\) aligns with the label \(\)(Angelopoulos and Bates, 2021). Armed with the calibration dataset as well as the nonconformity function, all that remains to construct the set-valued predictor is to choose the proper threshold \(\). ICP defines a calibration strategy to choose \(\) that guarantees a desired miscoverage rate \(\). Specifically, let \(_{}=\{s(,), D_{}\}\) and choose \(^{*}(D_{},)\) to be the \(q=(n+1)(1-)/n\) quantile of the set \(\). Then, so long as the score function \(s\) is independent from \(D_{}\), we have the following guarantee on the prediction sets defined by (2):

\[-<}^{N},(, )}{}( C(;^{*}(D_{},)).\] (3)

Importantly, this probabilistic guarantee is marginal over the sampling of the test data point as well as the calibration dataset. In practice, we are given a fixed calibration dataset, and would like to bound the miscoverage rate conditioned on observing \(D_{}\). As shown in Vovk (2012, Prop 2a), we can obtain a probably-approximately-correct (PAC) style guarantee on the ICP predictor of the form

\[}}{}(, }{}( C(;^{*}(D_{ },))<+}) 1-..\] (4)

In other words, with probability at least \(1-\) over the sampling of a fixed calibration dataset, we can ensure the test-time miscoverage rate is bounded by \(\) by calibrating for a marginal coverage rate of \(=-\). Alternatively, one can achieve the same PAC guarantee using the tighter bound in (Vovk, 2012, Prop 2b) by choosing the largest \((0,)\) for which

\[ I_{1-}(N-(N+1)-1,(N+1)-1+1)\] (5)

where \(I_{x}(a,b)\) is the regularized incomplete beta distribution. This lacks an analytic solution for the optimal \(\), but is less conservative.

The strength of ICP lies in its ability to guarantee test-time coverage for _any_ base predictor and nonconformity score function. However, ICP does not directly control the efficiency of the resulting prediction sets; the size and make-up of the prediction set for any given input \(\) is highly dependent on the score function \(s\) (and thus also the base prediction model \(f\)). For this reason, naive application of ICP can often yield prediction sets that are too "loose" to be useful downstream. Often, both the model and the score function may depend on various parameters \(^{d}\), e.g., the weights of a neural network base prediction model. The values of these parameters influence the ICP procedure, from the computation of the score function \(s(,;)\), the resulting threshold \(^{*}(D_{},;)\), and finally, the prediction sets themselves \(C(;^{*},)\). Our goal in this work is to investigate how we can use calibration data to fine-tune these parameters to optimize an efficiency objective while retaining guarantees on test-time coverage.

## 3 Related Work

**Handcrafted score functions.** In classification (\(=[K]\)), suppose \(f()\) predicts the softmax class probabilities; typically the score function is chosen as either \(s(,y)=1-f()_{y}\) or \(s(,y)=_{k[K]}\{f()_{k} f()_{k} f()_{y}\}\), where \(f()_{y}\) denotes the softmax probability of the groundtruth label. While the former score function produces prediction sets with the smallest average size (Sadinle et al., 2019), the latter produces prediction sets whose size can adapt to the difficulty of the problem (Romano et al., 2020). In 1-D regression (\(=\)), (Romano et al., 2019) recommend training \(f()=[f_{a/2}(),f_{1-}()]\) using quantile regression to output the (heuristic) \(/2\) and \(1-/2\) quantiles, and then set the score function as \(s(,y)=\{f_{/2}()-y,y-f_{1-}()\}\)to compute the distance from the groundtruth \(y\) to the prediction interval \(f()\). In \(n\)-D regression (\(=^{n}\)), it is common to design \(s(f(),)=)-\|}{w()}\), where \(f()=((),u())\) outputs both the mean of the label \(()\) and a heuristic notion of uncertainty \(u()\)(Yang and Pavone, 2023; Lindemann et al., 2023). While this leads to a ball-shaped prediction set, designing \(s(f(),)=_{i[n]}\{|()_{i}-y_{i}|\}\) can produce a box-shaped prediction set (Bai et al., 2022).

**Learning score functions.**(Yang and Kuchibhotla, 2021) propose selection algorithms to yield the smallest conformal prediction intervals given a family of learning algorithms. (Cleaveland et al., 2023) consider a time series prediction setup and leverage linear complementarity programming to optimize a score function parameterized over multiple time steps. (Stutz et al., 2021) develop conformal training to shrink the prediction set for classification, which learns the prediction function and the score function end-to-end by differentiating through and simulating the conformal prediction procedure during training. (Bai et al., 2022) considers optimizing the efficiency of a parameterized score function subject to coverage constraints on a calibration dataset, and derives generalization bounds depending on the complexity of the parametrization. (Einbinder et al., 2022) focus the classification-specific adaptive predictive set (APS) score function (Romano et al., 2020) score function, and propose an auxiliary loss term to train the model such that its predictions more closely meet the conditions under which the APS score function yields optimal efficiency. The drawback of (Cleaveland et al., 2023; Stutz et al., 2021; Bai et al., 2022; Einbinder et al., 2022) is that a separate held-out dataset is required to _recallibrate_ the optimized score function to obtain test-time coverage guarantees. In this paper, we leverage PAC-Bayes theory to alleviate this drawback and allow learning the parameters of the model and score function using the entire calibration dataset, while offering efficiency and coverage guarantees.

**Generalization theory and PAC-Bayes.** Generalization theory seeks to quantify how well a given model will generalize to examples beyond the training set, and how to learn models that will generalize well. Early work includes VC theory (Vapnik and Chervonenkis, 1968), Rademacher complexity (Shalev-Shwartz and Ben-David, 2014), and the minimum description length principle (Rissanen, 1989; Blumer et al., 1987). In this work, we make use of PAC-Bayes theory (McAllester, 1998). In PAC-Bayes learning, one fixes a data-independent prior over models and then obtains a bound on the expected loss that holds for any (potentially data-dependent) choice of posterior distribution over models. One can then optimize the PAC-Bayes bound via the choice of posterior in order to obtain a certificate on generalization. In contrast to bounds based on VC theory and Rademacher complexity, PAC-Bayes provides numerically strong generalization bounds for deep neural networks for supervised learning (Dziugaite and Roy, 2017; Neyshabur et al., 2017; Neyshabur et al., 2017; Barlett et al., 2017; Arora et al., 2018; Rivasplata et al., 2019; Perez-Ortiz et al., 2021; Jiang et al., 2020; Lotfi et al., 2022) and reinforcement learning (Fard et al., 2012; Majumdar et al., 2021; Veer and Majumdar, 2020; Ren et al., 2021). We highlight that the standard framework of generalization theory (including PAC-Bayes) provides bounds for _point_ predictors (i.e., models that output a single prediction for a given input). Here, we utilize PAC-Bayes in the context of conformal prediction to learn _set-valued_ predictors with guarantees on coverage and efficiency.

## 4 PAC-Bayes Generalization Bounds for Inductive Conformal Prediction

Optimizing efficiency with ICP requires splitting the dataset to train on one part and calibrate on the other. The reduced data for calibration can result in weaker miscoverage guarantees. Drawing from PAC-Bayes generalization theory, we will develop a theory that facilitates simultaneous calibration and efficiency optimization for ICP using the _entire_ calibration dataset.

Our generalization bounds are derived by randomizing the parameters of the score function \(\). Let \(Q()\) be a distribution over parameters \(\). Note that, for a fixed target miscoverage rate \(\), every sample from \(\) induces a different prediction set \(C(;^{*}(D_{},;),)\). Test-time coverage for such a randomized prediction set corresponds to the probability of miscoverage, marginalizing over the sampling of \(\):

\[_{}(Q):=}_{ Q}( }_{,}( C(; ^{*}(D_{},;),))).\] (6)

The _efficiency_\(_{}\) of a predicted set \(C\) is a metric that can encode task-specific preferences (e.g., the volume of the predicted set) that we wish to minimize while satisfying the desired miscoverage rate.

Similar to (6), we can define the efficiency measure \(_{}\) on randomized prediction sets by taking an expectation of a single-set efficiency measure:

\[_{}(Q):=}_{ Q}[}_{,}[_{}(C(; ^{*}(D_{},;),),)]].\] (7)

Our core theoretical contributions are to provide bounds on \(_{}(Q)\) and \(_{}(Q)\) that can be computed using \(D_{}\) and, critically, hold uniformly for _all_ choices of \(Q\), even those that depend on \(D_{}\). This allows us to _learn_ the distribution \(Q\) over score function parameters using \(D_{}\). First, we state our generalization bound for test-time coverage.

**Theorem 1** (PAC-Bayes Bound on Coverage of ICP Prediction Sets).: _Let \(P()\) be a (data-independent) prior distribution over the parameters of the score function. Let \(D_{N}\) be a set of \(N\) samples from \(\). Choose empirical coverage parameter \((0,1)\) such that \(N>1/-1\). Then, with probability greater than \(1-\) over the sampling of \(D_{N}\), the following holds simultaneously for all distributions \(Q()\):_

\[(-1}{N-1}\|\ _{}(Q))(Q\|P)+( {B(N)}{})}{N-1},\] (8)

_where \((p\|q)\) is the KL divergence between Bernoulli distributions with success probability \(p\) and \(q\) respectively, and \(B(N):=(,k,N+1-k=O((1-))}))\), where \(k=(N+1)\)_

The proof for this theorem is presented in App. A1. This theorem states that applying the calibration procedure will yield a test-time coverage rate \(_{}(Q)\) close to \(-1}{N-1}\), and the amount that it can differ shrinks as the cardinality \(N\) of the calibration set increases, but grows as \(Q\) deviates from \(P\). Finally, we note that an upper bound on \(_{}(Q)\) can computed by inverting the KL bound (Dziugaite and Roy, 2017) using convex optimization (Majumdar et al., 2021, Sec. 3.1.1).

Next, we will provide a generalization bound on efficiency.

**Theorem 2** (PAC-Bayes Bound on Efficiency of ICP Prediction Sets).: _Let \(P()\) be a (data-independent) prior distribution over the parameters of the score function. Let \(D_{N}\) be a set of \(N\) samples from \(\) and \(>0\). Let the efficiency \(_{}\) of a predicted set \(C\) always lie within2\(\). Assume the score function is bounded, \(s(,;)<\), and the efficiency loss \(_{}(C(;,)\) is \(L_{}\)-Lipschitz continuous in \(\) for all values of \(\). Then, with probability greater than \(1-\), we have for all \(Q\),_

\[_{}(Q)}_{ Q}[ {1}{N}_{i=1}^{N}_{}(C(_{i};^{*}(D_{N},;),))]+}{}+(Q\|P)+()}}{}\] (9)

The proof for this theorem follows a similar approach to standard PAC-Bayes bounds, but is complicated by the dependence of \(^{*}\) on \(D_{N}\). To mitigate this, we instead consider the generalization error between empirical and test-time efficiency for the worst-case choice of \(\), which requires assuming \(\) is bounded and the efficiency objective smooth w.r.t. \(\). The full proof is presented in App. A.

## 5 Practical Algorithmic Implementation

The generalization bounds above suggest a practical algorithm for using calibration data to simultaneously conformalize a base predictor while also fine tuning model and score function parameters. Specifically, we leverage the theory to formulate a constrained optimization problem, where the objective aims to minimize an efficiency loss, while the constraint ensures that test-time coverage can still be guaranteed. The overall algorithm is summarized in Alg. 1.

### Guaranteeing test-time coverage via a KL constraint

Using the bound in Theorem 1, we derive a constraint over data-dependent posteriors \(Q()\) such that choosing a threshold using the conformal prediction procedure \((,D_{N},)\) will guarantee a test-time coverage greater than \(1-\), where the coverage we enforce over \(D_{N}\), (1-\(\)), maybe different from the coverage we wish to achieve at test time, \(1-\).

**Corollary 2.1** (Constraint on Data-Dependent Posterior).: _Fix \(\), and a prior distribution \(P()\). Given a calibration dataset \(D_{}\) of \(N\) i.i.d. samples from \(\), we have that with probability greater than \(1-\), then simultaneously for all distributions \(Q()\) which satisfy_

\[(Q\|P) B(,,,N):=(N-1)( -1}{N-1}\|\;)-( ),\] (10)

_it holds that \(_{}(Q)\)._

The proof for this corollary is provided in App. A. Importantly, this bound holds for all \(Q()\) that satisfy the KL constraint, including \(Q\) that depend on the calibration data \(D_{N}\). Thus, we are free to use any optimization algorithm to search for a feasible \(Q\) that minimizes any auxiliary loss; with \(B(,,,N)\) serving as a "budget" for optimization, limiting the degree to which \(Q\) can deviate from the data-independent prior \(P\). Fig. 2 visualizes this budget as a function of \(N\), and \(\) for a fixed \(\) and \(\). As the plots show, by choosing the threshold \(\) to attain a more conservative \(\), we can afford more freedom to vary \(Q\). The curve defined by \(B(,,,N)=0\) indicates the maximum \(\) for which our theory provides a guarantee on \(1-\) test time coverage with probability greater than \(1-\) for a randomized data-independent score function (\(Q=P\)). As visualized in the figure, this boundary aligns with maximum \(\) implied by the PAC guarantee from Vovk (2012, Prop 2a), where differences are likely due to differences in bound derivation. However, there remains a gap between our analysis and the results from Vovk (2012, Prop 2b), suggesting room for future tighter bounds.

### Optimizing efficiency via constrained stochastic gradient optimization

In order to select a \(Q\) from this set, we propose directly optimizing the efficiency objective3(7). However, direct optimization is difficult due to (i) the expectation over \(\) and \(\), and (ii) the non-differentiability of the efficiency objective \(_{}(C)\). To address (i), we use minibatches of inputs sampled from \(D_{}\) and a finite set of \(K\) samples of \(\) from \(Q()\) to construct a Monte-Carlo approximation of the expectation, providing a stochastic estimate of the objective that can be used in a stochastic gradient descent algorithm. To ensure differentiability, we follow the strategy proposed in (Stutz et al., 2021) and replace any non-differentiable operations in the computation of \(_{}\) with their "soft" differentiable counterparts. For example, the quantile operation needed to compute the optimal threshold \((,D_{},)\) can be replaced with a soft quantile implementation leveraging a soft sorting

Figure 2: Visualization of the KL budget allowed by the PAC-Bayes generalization bound to achieve a target test-time coverage of \(=0.1\), plotted as a function of \(\) and calibration set size \(N\) (left) as well as guarantee failure probability \(\) (right). Note the log scale on the x axis on both plots.

algorithm (Cuturi et al., 2019; Grover et al., 2019). As optimizing over the space of all probability distributions over \(\) is intractable, we restrict \(Q\) to be within a parametric class of distributions for which obtaining samples and evaluating the KL divergence with respect to the prior is tractable. In this work, we follow prior work in the PAC-Bayes and Bayesian Neural Network literature and fix both \(P\) and \(Q\) to be a Gaussian with a diagonal covariance matrix, \((,(^{2}))\), where \(^{d}\) and \(^{2}^{d}_{+}\). This allows analytic evaluation of the KL divergence, and differentiable sampling via the reparametrization trick, as shown in Alg. 1. In this way, both the optimization objective and the constraint can be evaluated in a differentiable manner, and fed into a gradient based constrained optimization algorithm. We choose the \(Q\) with the lowest loss that still satisfies Corollary 2. More details on the optimization procedure can be found in Appendix B.

```
0: Parameterized score function \(s(,,)\), Calibration dataset \(D_{}\), Target coverage rate \(1-\), Probability of correctness \(\), Differentiable efficiency objective \(_{}(C)\), Prior distribution \(P()=(;_{0},_{0})\), Posterior distribution family \(Q()=(;,)\), Empirical coverage \(1-\)  Initialize posterior distribution parameters \(,_{0},_{0}\) for each optimization iteration do \(}_{}(,)\) \(c(,)((,)\|(_{0},_{0}))-B(,,,N)\)  Update \(,\) to minimize \(_{}(,)\) subject to \(c(,)<0\) endfor ```

**Algorithm 1** Optimal Conformal Prediction with Generalization Guarantees

### Practical considerations

Choice of prior.A prior \(P()\) may give high likelihood to values of \(\) that yield good prediction sets, but also to \(\) that perform poorly. To optimize efficiency, \(Q\) must shift probability mass away from poor \(\) towards good \(\). However, due to the asymmetry of the KL divergence, \((Q||P)\) is larger if \(Q\) assigns probability mass where \(P\) does not than vice-versa. Thus, in order to effectively optimize efficiency under a tight KL budget, we must choose \(P\) that not only (i) assigns minimal probability density to bad values of \(\), but more importantly, (ii) assigns significant probability density to good choices of \(\). How do we choose an effective prior? In some cases, it suffices to choose \(P\) as an isotropic Gaussian distribution centered around a random initialization of \(\). However, especially when \(\) correspond to parameters of a neural network, using a data-informed prior can lead to improved performance (Perez-Ortiz et al., 2021; Dziugaite and Roy, 2018). Care is needed to ensure that a data-informed prior does not break the generalization guarantee; we find that a simple data-splitting approach similar to (Perez-Ortiz et al., 2021) is effective: We split \(D_{}\) into two disjoint sets: \(D_{0}\), used to optimize the prior without any constraints, and \(D_{N}\), used to optimize the posterior according to Alg. 1. In our experiments, we consider two methods of optimizing a Gaussian prior \(P=(,(^{2}))\): First, we can optimize only the mean \(\) to minimize \(_{}()\), keeping \(^{2}\) fixed. This aims to shift \(P\) to assign more density to good values of \(\), but the fixed \(^{2}\) may still assign significant mass to bad choices of \(\). Alternatively, we consider optimizing both \(\) and \(^{2}\) to minimize \(_{}(P)\). By also optimizing the variance, we can minimize probability mass assigned to bad choices of \(\), but this comes with a risk of overfitting to \(D_{0}\).

Test-time evaluation.The guarantees on coverage and efficiency generalization hold in expectation over both data sampled from the data distribution \(\) as well as model and score function parameters sampled from the learned posterior \(Q\). Thus, in order to use such a predictor in practice and attain the desired coverage and efficiency, one would need to sample parameters \(\) from \(Q\), and then compute the resulting prediction set \(C(;)\). However, as the threshold for this prediction set \(=^{*}(D_{},;)\) depends on \(D_{}\), this approach would require holding on to the calibration dataset to find the optimal threshold of each sample of \(\). In practice, once we optimize \(Q\), we pre-sample \(K\) values of \(\) from \(Q\), and pre-compute the optimal threshold for each. Then, at test-time, we randomly select one \(,\) pair from the set to evaluate each test input.

## 6 Experimental Results

We evaluate our approach on an illustrative regression problem as well as on an MNIST classification scenario with simulated distributional shift.4 For each domain, we evaluate our _PAC-Bayes_ approach against two baselines: (i) the _standard_ ICP approach, where we use a fixed, data-independent score function \(s_{}(,)\) and use the entirety of the calibration data to compute the critical threshold \(\); and (ii) a _learned_ ICP approach (Stutz et al., 2021) which uses a portion of \(D_{}\) to optimize parameters \(\) of a parametric score function \(s_{}(,;)\) to minimize an efficiency loss \(_{}\), and the remaining portion to re-estimate the threshold \(\) to guarantee test-time coverage. For the PAC-Bayes method, we consider the same parametric form as the learned baseline, but instead randomize the score function by modeling a distribution over \(\). Both the learned and the PAC-Bayes baseline require choosing a _data split_ ratio, i.e. what fraction is used for optimization (or prior tuning for the PAC-Bayes approach), and what fraction is held out for recalibration (or constrained optimization). For all methods, we target the same test-time coverage guarantee of \(1-\) coverage with probability greater \(1-\). For the standard and learned methods, the amount of data in the held-out recalibration set determines the empirical coverage level \(\) needed to attain this guarantee, using either Vovk Prop 2a (4), or Vovk Prop 2b (5). We compare against both bounds in our experiments. For the PAC-Bayes method, the choice of \(\) is a hyperparameter: choosing a lower value yields a larger budget for optimization, but entails using a more extreme quantile for the threshold \(\). As all methods provide guarantees on test-time coverage, our evaluation focuses on the efficiency of each method's predictions on held-out test data that is exchangeable with the calibration data. Additional results (including coverage results), as well as specific details for all experiments, can be found in App. B.

### Illustrative demonstration: 1-D regression

As an illustrative example, we consider a 1-D regression problem where inputs \(\) and targets \(\) are drawn from a heteroskedastic distribution where noise increases with \(\), as shown in Figure 3. We train a two hidden layer fully connected neural network to minimize the mean squared error on this data, obtaining a base predictor \(f()\) that produces a single point estimate for \(\). We aim

Figure 3: Standard ICP with a fixed score function (left) can yield over-conservative prediction sets. By using calibration data to also learn an uncertainty scaling factor in the nonconformity score function, both the learned (middle) and PAC-Bayes (right) approaches can yield more efficient prediction sets. When the calibration set size is large (bottom), both the learned and PAC-Bayes approaches do well. However, when calibration data is limited (top), our PAC-Bayes approach yields prediction sets with better test-time efficiency.

to obtain a set-valued predictor using a held-out set of calibration data with \(=0.1\) and \(=0.05\). For the sake of this example, we assume that we no longer have access to the training data, and only have calibration data and a pre-trained base predictor which only outputs point estimates and no input-dependent estimate of variance. Therefore, for the standard application of ICP, we must use the typical nonconformity function for regression, \(s_{}(,)=\|f()-\|\). Note that this choice yields prediction sets with a fixed, input-independent size, which is suboptimal in this heteroskedastic setting. To address this, we consider using the calibration dataset not only to determine the threshold \(^{*}\), but also to _learn_ a parametric score function that can capture this heteroskedasticity. Specifically, we learn an input-dependent uncertainty score \(u(;)\) which is used to scale the nonconformity score \(s_{}(,;)=\|f()-\|/u( ;)\)(Angelopoulos and Bates, 2021). We model \(u(,)\) as a separate neural network with the same architecture as \(f\), and optimize either \(\) or \(Q()\) to minimize the volume of the resulting prediction sets. To do so, we minimize a loss proportional to the log of the radius of this set, which in this case can be written as \(_{}(C(,))=(u(;))\).

As can be seen in Figure 3, when \(N=5000\), both the learned baseline and our PAC-Bayes approach are able to learn an appropriate input-dependent uncertainty function which yields prediction sets which capture the heteroskedasticity of the data, and thus yield tighter sets in expectation over \(\). However, if we reduce the amount of calibration data available to \(N=500\), the learned approach starts to overfit on the split used to optimize efficiency. In contrast, the KL-constraint of the PAC-Bayes approach mitigates this overfitting, and outperforms both baselines in terms of test set efficiency for smaller amount of calibration data.

### Corrupted MNIST

Next, we consider a more challenging distribution shift scenario, where we aim to deploy a model trained on "clean" data to a domain with distribution shift. As the base predictor, we use a LeNet convolutional neural network trained on a softmax objective to classify noise-free MNIST digits (LeCun et al., 1998). Then, we simulate distribution shift by adding Gaussian noise and applying random rotations to a held-out set of the dataset: examples of the clean and corrupted digits are shown in Figure 4. Our goal is to use a calibration dataset of this corrupted data to design a set-valued predictor that achieves coverage on held-out test data with the same corruption distribution.

As is conventional, when applying ICP to probabilistic classifiers, we use the negative log probability as the nonconformity score \(s_{}(,)=[f()]_{[]}\)(Stutz et al., 2021; Angelopoulos and Bates, 2021). This score function leverages the implicit uncertainty information encoded in the base model's probabilistic predictions; but these probabilities are far from calibrated, especially in the presence of distribution shift (Ovadia et al., 2019; Sharma et al., 2021). Thus, we consider using the calibration dataset, which is representative of the corrupted test distribution, to fine-tune the predicted probabilities, defining \(s_{}(,,)=[f(; )]_{[]}\), where \(\) are the parameters of the fully connected layers of the base predictor. As before, we measure efficiency as the size of the prediction set, \(_{}(C)=|C|\). In contrast to the regression setting, here \(C\) is a discrete set over the set of class labels, and thus the size of this set is non-differentiable. To smooth this objective for training,

Figure 4: Left: A base model is trained on clean MNIST digits (top), but calibrated and tested on corrupted digits (bottom). Middle: Average prediction set size on test data versus calibration set size (\(N\)) for a data split ratio of \(0.5\). Right: Prediction set size relative to standard ICP (Vovk 2a) as a function of the data split ratio, averaged over random seeds and across all calibration set sizes.

we follow the approach of (Stutz et al., 2021), and first use a sigmoid to compute soft set-membership assignment for each possible label, and then take the sum to estimate the size of the set,

\[_{}(C(,))=_{\{0,,9\}}(T ^{-1}(-s(,,))),\] (11)

where the temperature \(T\) is a hyperparameter which controls the smoothness of the approximation; in our experiments we use \(T=0.1\).

We evaluate each method for different calibration set sizes \(N\) and data split ratios, repeating each experiment using 3 random seeds. For all approaches, we desire a coverage guarantee of \(=0.1\) with \(=0.05\). For the PAC-Bayes approach, we run the algorithm with \(=0.01\), performing a grid search over 5 values of \(\) evenly spaced between \(0.2\) and \(0.8\), and choosing the approach with the best efficiency generalization certificate (9); using the union bound for probability ensures that the efficiency for the best of the five runs will hold with probability at least \(1-0.05\), i.e., an effective \(\) of \(0.05\). For the PAC-Bayes approach, we optimize both the prior mean \(\) and variance \(^{2}\) on \(D_{0}\), as we found that optimizing the mean alone yielded a poor prior; ablations are provided in App. B. The results are summarized in Figure 4. In the middle figure, we hold the data split fixed at \(0.5\), and plot the average test set size as a function of \(N\). First, we note that even for the standard ICP approach, increasing \(N\) leads to a smaller prediction set size, as we are able to calibrate to a larger \(\) while still maintaining the desired test-time guarantee. In general, both the learned and PAC-Bayes approaches improve upon the standard baseline by fine-tuning on the corrupted calibration data. However, when data is limited (\(N=1000\)), the learned baseline overfits during optimization on the first half of the calibration data, and subsequent calibration yields set sizes that are even larger than the standard method. Our method mitigates this by using all the data to simultaneously fine-tune and calibrate while ensuring test-time generalization, thanks to the KL constraint. Empirically, the PAC-Bayes method yields prediction sets with efficiency comparable or higher than the learned baseline calibrated using Vovk 2a. However, there still remains a gap between our results and the learned baseline calibrated with tighter bound in Vovk 2b, as is evident in our analysis of our bound in Figure 2. In the right figure, we explore the sensitivity of both learned methods on the choice of data split ratio. We observe that the PAC-Bayes method is less sensitive to the choice of data split ratio.

## 7 Conclusion

In this paper, we introduced theory for optimizing conformal predictors while retaining PAC generalization guarantees on coverage and efficiency _without_ the need to hold-out data for calibration. We achieved this by combining PAC-Bayes theory with ICP to furnish generalization bounds with the _same_ data used for training. We translated our theory into a practical algorithm and demonstrated its efficacy on both regression and classification problems.

**Limitations.** As shown in Figure 2, our theory remains overconservative relative to the tight Vovk 2b PAC bound, which future work could aim to address. Practically, a key limitation of our approach is its dependence on the availability of a good prior. Although, we were able to mitigate this issue by training a prior on a portion of the calibration dataset, this strategy can struggle when datasets are small or the number of parameters to optimize are very large. Furthermore, while diagonal Gaussian distributions are convenient to optimize, they may not represent the optimal posterior. These limitations may explain why we obtained only modest improvements in efficiency with respect to the Vovk 2a baselines in our experiments; we are optimistic that future advances in prior selection and representation may lead to improvements. Finally, while PAC-Bayes theory requires sampling from the posterior \(Q\), our set-up also requires computing the threshold \(^{*}\) for each parameter sample. Future work could explore applying disintegrated PAC-Bayes theory (Viallard et al., 2023) to yield (perhaps more conservative) generalization bounds that hold for a single sample from the posterior.

**Broader Impact.** This work is building towards safer and trustworthy ML systems that can reason about uncertainty in their predictions. In particular, this work is a step towards developing calibrated uncertainty set predictors that are efficient enough for practical applications.

**Future work.** This work opens up exciting new theoretical and practical questions to pursue. On the theoretical front, we are excited to explore the development of an online learning approach that can leverage this theory to provably adapt to distribution shifts on the fly. On the practical front, we look forward to using this framework for providing reasonable uncertainty estimates in robot autonomy stacks to facilitate decision making and safety certification.