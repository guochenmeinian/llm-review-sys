# Boosting Spectral Clustering on Incomplete Data via Kernel Correction and Affinity Learning

Fangchen Yu\({}^{1}\), Runze Zhao\({}^{1}\), Zhan Shi\({}^{1}\), Yiwen Lu\({}^{1}\)

Jicong Fan\({}^{1,2}\), Yicheng Zeng\({}^{2}\), Jianfeng Mao\({}^{1,2}\), Wenye Li\({}^{1,2}\)

\({}^{1}\) The Chinese University of Hong Kong, Shenzhen, China

\({}^{2}\) Shenzhen Research Institute of Big Data, Shenzhen, China

{fangchenyu, runzezhao, zhanshi1, yiwenlu1}@link.cuhk.edu.cn

fanjicong@cuhk.edu.cn, statzyc@sribd.cn, jfmao@cuhk.edu.cn, wyli@cuhk.edu.cn

Corresponding author.

###### Abstract

Spectral clustering has gained popularity for clustering non-convex data due to its simplicity and effectiveness. It is essential to construct a similarity graph using a high-quality affinity measure that models the local neighborhood relations among the data samples. However, incomplete data can lead to inaccurate affinity measures, resulting in degraded clustering performance. To address these issues, we propose an imputation-free framework with two novel approaches to improve spectral clustering on incomplete data. Firstly, we introduce a new kernel correction method that enhances the quality of the kernel matrix estimated on incomplete data with a theoretical guarantee, benefiting classical spectral clustering on pre-defined kernels. Secondly, we develop a series of affinity learning methods that equip the self-expressive framework with \(_{p}\)-norm to construct an intrinsic affinity matrix with an adaptive extension. Our methods outperform existing data imputation and distance calibration techniques on benchmark datasets, offering a promising solution to spectral clustering on incomplete data in various real-world applications.

## 1 Introduction

Spectral clustering [1; 2; 3] has become a widely used and effective method for clustering non-convex data and finds diverse applications in computer vision [4; 5], natural language processing [6; 7], and bioinformatics [8; 9]. Generally, the first step in spectral clustering involves constructing an affinity matrix that captures the similarity between data points, followed by performing normalized cut  on the corresponding graph to partition the data into clusters. The quality of the affinity matrix is a critical factor that determines the effectiveness of the clustering performance [2; 3]. However, incomplete data is commonly seen in practice, leading to inaccurate affinities and degraded clustering performance [10; 11; 12]. As such, obtaining a high-quality affinity matrix with missing data is a challenging task that requires specialized techniques. In this paper, we aim to improve the quality of affinity matrices, which naturally enhance the performance of spectral clustering on incomplete data.

In recent years, the development of methods to construct affinity matrices with full information for spectral clustering has garnered significant attention. Two types of affinity matrices are typically utilized, namely pre-defined similarity matrices [1; 2; 13; 14] and self-expressive affinity matrices [3; 15; 16; 17; 18; 19; 20]. Pre-defined similarity matrices are easy to compute and encompass well-known kernels, including the Gaussian kernel, Exponential kernel, and Polynomial kernel. The Gaussian kernel is the most widely used due to its simplicity and effectiveness . Differently, self-expressive affinity matrices learn the affinity matrix \(C\) by representing each data point as alinear combination of other data points, i.e., \(_{C}\|X-XC\|_{F}^{2}+(C)\), where the columns of \(X^{d n}\) are data points, \((C)\) denotes a regularization term, and \(\) is a hyperparameter, as exemplified by Sparse Subspace Clustering (SSC) , Low-Rank Representation (LRR) , and Least-Squares Representation (LSR) [17; 18]. Moreover, self-expressive methods have been expanded by integrating kernel functions into the framework, such as kernel SSC (KSSC) , kernel LRR (KLRR) , and kernel LSR (KLSR) , through \(_{C}\|(X)-(X)C\|_{F}^{2}+(C)\), where \((X)\) is a mapping function and \(K:=(X)^{}(X)\) denotes a kernel matrix. Despite the varying techniques on affinity matrices, the kernel matrix remains a cornerstone of spectral clustering for pre-defined similarities and self-expressive affinities. Nevertheless, constructing a high-quality kernel/affinity matrix is an ongoing challenge when facing incomplete data. The primary obstacle is the inability to directly calculate a kernel matrix or learn an affinity matrix due to missing values.

To address the challenge of incomplete data, data imputation techniques [21; 22; 23] such as statistical imputation and matrix completion, are conventional approaches to fill in the missing entries of the data matrix before clustering. Statistical imputation methods [24; 25], such as zero, mean, and \(k\)-nearest neighbors (\(k\)NN) imputation , are fast and easy to implement and can flexibly handle missing data. However, these methods may introduce bias and fail to capture the true underlying data structure, hindering the accurate affinity measures. Matrix completion methods [27; 28; 29; 30], on the other hand, aim to accurately recover the underlying structure of the data by finding a low-rank or high-rank approximation of the data matrix. Although they can handle missing data in a principled way, their performance heavily depends on the consistency of data structure and the assumptions being made, easily affected by the data distribution. Most importantly, almost all imputation methods do not offer a guarantee for the quality of the affinity matrix calculated on the imputed data .

Distance calibration techniques [32; 33] have emerged as an alternative solution by obtaining a high-quality distance matrix for incomplete data. These techniques calibrate an initial non-metric distance matrix estimated on incomplete data to a distance metric. Several methods have been proposed, such as the metric nearness model [34; 35; 36], which finds the nearest approximation that satisfies all triangle inequalities . The double-centering algorithm  converts the initial non-metric distance matrix into a Euclidean distance matrix, while the Euclidean embedding method [39; 40; 41] ensures the Euclidean embeddable property is satisfied through convex optimization. Although some of these methods guarantee the quality of the calibrated distance , they only apply to distance-based affinities like the Gaussian kernel. Moreover, these methods may not generate high-quality affinity matrices based on calibrated distance, making it necessary to develop dedicated methods that can apply to a family of kernels and affinities with theoretical guarantees.

To address the above issues, we propose an imputation-free framework to directly learn high-quality affinity matrices via two main techniques, i.e., kernel correction and affinity learning, aiming to improve the spectral clustering performance on incomplete data, with contributions as follows.

* We propose an imputation-free framework based on kernel correction and affinity learning with improved clustering performance, providing convenient tools to deal with incomplete data. To our best knowledge, this is the first systematical work to discuss spectral clustering on incomplete data, which is a commonly seen problem in practice.
* We introduce a novel kernel correction method that directly focuses on the kernel matrix estimated from the incomplete data, and corrects it to satisfy specific mathematical properties such as positive semi-definiteness (PSD). We show that the corrected kernel matrix becomes closer to the unknown ground-truth with a theoretical guarantee, which is beneficial to spectral clustering algorithms and cannot be assured by imputation and calibration methods.
* We develop a series of new affinity learning methods to equip the self-expressive framework with the \(_{p}\)-norm to capture the underlying structure of data samples better. Additionally, we combine kernel correction and affinity learning to arrive at an adaptive learning method that simultaneously learns the high-quality kernel and the self-expressive affinity matrix.
* We conduct extensive experiments that demonstrate the effectiveness of proposed methods on various benchmark datasets, showing superior results in terms of kernel estimation, spectral clustering, and affinity learning on incomplete data, compared to existing data imputation and distance calibration approaches.

Related Work

### Spectral Clustering and Affinity Learning

**Standard Spectral Clustering** involves three steps : first, a Gaussian kernel matrix \(K^{n n}\) is calculated to measure the similarity between \(n\) data points with \(K_{ij}=(-\|x_{i}-x_{j}\|^{2}/^{2})\) where \(\) is a hyperparameter. Then, an affinity graph \(A^{n n}\) is constructed using the kernel matrix, which can take the form of an \(\)-neighborhood graph, a fully connected graph, or a \(k\)-nearest neighbors (\(k\)NN) graph. An \(\)-neighborhood graph connects pairwise points with a threshold value \(\), while a fully connected graph connects all points. Empirically, a \(k\)NN graph is the most popular one that connects each point to its \(k\)-nearest neighbors, resulting in sparse local relationships and relatively high clustering accuracy . Finally, the normalized cut algorithm  is applied to the affinity graph \(A\) to partition the data into clusters based on the normalized Laplacian matrix.

**Self-expressive Affinity Learning** is a framework to learn affinity matrices by modeling the relationships between data points, i.e., \(_{C}\|X-XC\|_{F}^{2}+(C)\), with different types of regularization terms \((C)\). Sparse Subspace Clustering (SSC)  assumes that the data points lie in a low-dimensional subspace and seeks to find a sparse affinity matrix with \((C)=\|C\|_{1}=_{i,j=1}^{n}|c_{ij}|\) under a constraint \(()=0\). Low-Rank Representation (LRR)  is another approach that seeks a low-rank affinity matrix with \((C)=\|C\|_{*}\) (nuclear norm of \(C\)). Least-Squares Representation (LSR) [17; 18] involves solving a least-squares problem to find the representation matrix with \((C)=\|C\|_{F}^{2}\). Their kernel variants, i.e., KSSC , KLRR , and KLRR  are used to extend their applicability to non-linearly separable data by applying a kernel function to data points with \(_{C}\|(X)-(X)C\|_{F}^{2}+(C)\), showing promising performance in spectral clustering.

### Missing Data Processing Techniques

**Data Imputation** is a popular technique [21; 22; 23] for dealing with incomplete data by filling in missing values. Statistical imputation methods, such as zero, mean imputation, and \(k\)-nearest neighbors (\(k\)NN) approach , have been widely used in practice. These methods replace the missing value with a zero, mean, or \(k\)-weighted value of non-missing elements in the corresponding feature. Additionally, matrix completion [27; 28; 29; 30] is a machine learning-based technique that fills missing values by solving a matrix factorization problem under assumptions on data structures such as low-rank or high-rank. However, correctly estimating missing values based on observed data is difficult, especially for a large missing ratio, and there is no guarantee on the quality of the affinity matrix on imputed data. This motivates us to design imputation-free approaches in Sections 3 and 4.

**Distance Calibration** is a specialized approach to obtaining a valid distance metric from an initial non-metric distance matrix, which can be applied to incomplete data. For any two incomplete data samples \(x_{i},x_{j}^{d}\), a new vector \(x_{i}(I)^{|I|}\) is formed by selecting the observed values of \(x_{i}\) on the set \(I\), where \(I\) is an index set of all features observed in both samples. The pairwise Euclidean distance between \(x_{i}\) and \(x_{j}\) can then be heuristically estimated by [39; 41]

\[d_{ij}^{0}=\|x_{i}(I)-x_{j}(I)\|_{2}}.\] (1)

However, the initial Euclidean distance matrix \(D^{0}=[d_{ij}^{0}]^{n n}\) estimated on incomplete data is usually not a distance metric due to missing values. Distance calibration methods [34; 35; 38; 39] can correct \(D^{0}\) to a distance metric by making different assumptions and leveraging various properties. More details of these methods are discussed in Section 3.1.

## 3 Methodology-I. Kernel Correction

### Revisiting Distance Calibration Methods

We begin with the definition of a distance metric , then delve into distance calibration methods, discussing the assumptions and properties underlying each method, as well as their limitations.

**Definition 1**.: _A distance metric is defined as a \(n n\) real symmetric matrix \(D\) that satisfies_

\[d_{ij}=d_{ji} 0,\ d_{ii}=0,\ d_{ik} d_{ij}+d_{jk},1 i,j,k n.\]The properties of non-negativity, reflexivity, symmetry, and triangle inequality ensure that the distance metric produces meaningful and valuable distance measurements.

\(\)**The Triangle Fixing (TRF) Algorithm** to obtain a distance metric is based on the metric nearness model , which finds the nearest approximation of a non-metric \(D^{0}\) by solving:

\[^{n n}}{}\ \|D-D^{0}\|_{F}^{2},\ \ \ d_{ij}=d_{ji} 0,\ d_{ii}=0,\ d_{ik} d_{ij}+d_{jk},\ \ 1 i,j,k  n.\] (2)

However, there are two significant limitations. First, the scalability of the algorithm is severely limited by the matrix size \(n\). Since the number of \(O(n^{3})\) constraints grows rapidly as \(n\) increases, the optimization time can become lengthy, taking several hours for a few thousand samples. Second, the performance of the algorithm depends on the extent of violation of the triangle inequalities in the initial distance matrix \(D^{0}\). With a small missing ratio of features, \(D^{0}\) already satisfies most triangle inequalities, i.e., \(d^{0}_{ik} d^{0}_{ij}+d^{0}_{jk}\), then the algorithm typically yields only marginal improvement.

\(\)**The Double-Centering (DC) Algorithm** starts with the Euclidean Distance Matrix (EDM)  and utilizes Property 1 , a well-known result in classical Multi-Dimensional Scaling (cMDS) , to build the connection between the similarity matrix and the EDM.

**Definition 2**.: _A \(n n\) real symmetric matrix \(D\) is called an EDM if there exists \(x_{1},x_{2},,x_{n}^{d}\) such that \(d_{ij}=\|x_{i}-x_{j}\|^{2}\) for \(i,j=1,2,,n\). (Note that it uses squared Euclidean distance.)_

**Property 1**.: _A \(n n\) real symmetric matrix \(D\) is EDM if and only if \((D)=0,\ S:=-JDJ 0,\ J:=I-ee^{}/n,\) where \(I\) is the identity matrix and \(e\) is the \(n\)-dimension vector of all ones._

The DC algorithm first finds the nearest positive semi-definite \(\) by solving

\[^{n n}}{}\ \|S-(-J(D^{0}  D^{0})J)\|_{F}^{2},\ \ \ \ S 0,\] (3)

where \(D^{0}\) is the initial Euclidean distance matrix and \(\) denotes the Hadamard product. The algorithm then transforms \(\) into an EDM \(\) using \(_{ij}=_{ii}+_{jj}-2_{ij}\), and obtain the calibrated (non-squared) distance matrix by \([_{ij}^{1/2}]\), but unfortunately, the quality of \(\) cannot be guaranteed, and important information may be lost during the transformation .

\(\)**The Euclidean Embedding (EE) Algorithm** leverages Euclidean embeddable property  to obtain a calibrated embeddable distance matrix by solving

\[^{n n}}{}\ \|D-D^{0}\|_{F}^{2},\ \ \ (- D) 0,\ d_{ii}=0,\ d_{ij}=d_{ji} 0,\ \ 1 i,j n.\] (4)

**Definition 3**.: _A \(n n\) real symmetric matrix \(D\) is said Euclidean embeddable if there exists \(x_{1},,x_{n}\) in Euclidean space and a distance function \(\) such that \(d_{ij}=(x_{i},x_{j}) 0\) for \(i,j=1,,n\)._

**Property 2**.: _[Theorem 2, ] If \(a n\) real symmetric matrix \(D\) is Euclidean embeddable, then the kernel \(K:=(- D)\) is positive semi-definite for any \(>0\)._

It is important to note that these methods can only provide a calibrated distance matrix with benefits for distance-based kernels and are not a universal solution for dealing with incomplete data in spectral clustering tasks, which motivates us to further design a kernel-specialized method in Section 3.2.

### Kernel Correction Algorithm

To overcome the limitations of distance calibration, we propose a new kernel correction algorithm that directly focuses on the construction of a high-quality kernel with a theoretical guarantee. We consider an incomplete data matrix \(X^{d n}\) with missing values and an initial kernel matrix \(K^{0}^{n n}\) estimated from \(X\). Inspired by our previous work , our goal is to correct the initial kernel \(K^{0}\) to an improved estimate \(\) that satisfies the PSD property based on the Lemma 1 .

**Lemma 1**.: _A valid kernel matrix is a \(n n\) real symmetric matrix that satisfies the PSD property._

Naturally, we recover the PSD property with the minimum cost by solving the following model:

\[^{n n}}{}\ \|K-K^{0}\|_{F}^{2},\ \ \ \ K 0,\ k_{ij}=k_{ji}[l,u],\ \ 1 i,j n,\] (5)

where \(l,u\) denote the lower bound and upper bound, respectively.

It is worth noting that the solution \(\) to Eq. (5) provides an improved estimate of the unknown ground-truth \(K^{*}\) compared to \(K^{0}\), as illustrated in Theorem 1. The proof comes from Kolmogorov's criterion [50; 31; 47; 48], which characterizes the best estimation in an inner product space, provided in Appendix A.

**Theorem 1**.: \(\|K^{*}-\|_{F}\|K^{*}-K^{0}\|_{F}\)_. The equality holds if and only if \(K^{0} 0\), i.e., \(K^{0}=\)._

Regarding the kernel type, Gaussian kernel is a widely-used non-linear kernel that has elements in the range \(\). In this case, the feasible region in Eq. (5) is defined as \(=\{K^{n n} K 0,\ k_{ii}=1,\ k_{ij}=k_{ji} ,\ \ 1 i,j n\}\), which is a closed convex set. The solution to Eq. (5) is the projection of \(K^{0}\) onto \(\), denoted by \(\). However, finding the direct projection is complex, and no closed form of \(\) exists. Thus, we break down \(\) into two simpler, closed convex subsets \(_{1}\) and \(_{2}\): \(_{1}=\{K^{n n} K 0\},\ _{2}=\{K ^{n n} k_{ii}=1,\ k_{ij}=k_{ji},\ \ 1 i,j n\}\), with \(=_{1}_{2}\). Then \(\) can be efficiently solved by iteratively projecting \(K^{0}\) onto \(_{1}\) and \(_{2}\)[31; 40; 47]. Denote \(_{1},_{2}\) as the projection onto \(_{1},_{2}\), respectively, in the form of

\[_{1}(K)=UU^{}K=U U^{},\ _{ij}=\{_{ij},0\},\\ _{2}(K)=[_{2}(k_{ij})]_{2}(k_{ij})= \{0,k_{ij},1\},\ _{2}(k_{ii})=1,\] (6)

where \(U U^{}\) gives the spectral decomposition (SD) of \(K\).

We use Dykstra's projection algorithm  to find the optimal projection, summarized in Algorithm 1:

```
0:\(K^{0}^{n n}\): an initial (non-PSD) kernel matrix; \(_{1},_{2}\): the projection onto \(_{1}\) and \(_{2}\); \(maxiter\): maximum iterations (default \(100\)); \(tol\): tolerence (default \(10^{-5}\)).
0:\(^{n n}\): the optimal corrected kernel matrix.
1:\(\) Dykstra's Projection:\(Y,P,Q\) are auxiliary variables.
2: Initialize \(X_{0}=K^{0}\) and \(P_{0}=Q_{0}=_{n n}\).
3:for\(t=0,1,,maxiter\)do
4:\(Y_{t}=_{2}(X_{t}+P_{t})\),
5:\(P_{t+1}=X_{t}+P_{t}-Y_{t}\),
6:\(X_{t+1}=_{1}(Y_{t}+Q_{t})\),
7:\(Q_{t+1}=Y_{t}+Q_{t}-X_{t+1}\).
8:if\(\|X_{t+1}-X_{t}\|_{F}<tol\)then
9:break
10:endif
11:endfor
12: Set \(=X_{t}\). ```

**Algorithm 1** Kernel Correction (**KC**)

The convergence guarantee of Algorithm 1 relies on the Boyle-Dykstra's result :

**Lemma 2**.: _Given a real symmetric matrix \(K^{0}^{n n}\), the sequence \(\{X_{t}\}\) generated in Algorithm 1 converges to \(=_{K=_{1}_{2}}\|K-K^{0}\|_ {F}^{2}\) as \(t\)._

### Limitation and Complexity Analysis

The potential limitation primarily stems from the time complexity of Algorithm 1. The pre-iteration time complexity of the KC algorithm is currently at \(O(n^{3})\), which mainly arises from the spectral decomposition (SD) in the projection operation \(_{1}\) and poses challenges when dealing with large-scale datasets. To address this issue, a possible solution is to replace the spectral decomposition with a randomized singular value decomposition (rSVD) . The rSVD approach seeks top-\(k\) singular values and effectively reduces the time complexity to \(O(n^{2}(k)+2n k^{2})\). However, the trade-off between efficiency and efficacy necessitates further investigation. Besides, we can transform Dykstra's projection into a parallel version with cyclic projection  to achieve better scalability. The storage complexity is \(O(n^{2})\) to store the dense kernel matrix in memory.

### Discussion on Kernel Correction

The proposed kernel correction approach provides an imputation-free method with several benefits to spectral clustering, compared to data imputation and distance calibration methods.

Differences with Previous Kernel MethodsSome literature in kernel learning [55; 56] has studied missing data in a supervised manner. However, our work focuses on unsupervised learning and addresses _complete but inaccurate_ (noisy) kernels due to the presence of incomplete observations, which is fundamentally different from previous work  primarily dealing with incomplete kernels.

Advantages over Data ImputationOur approach eliminates the need for domain knowledge in handling incomplete data by bypassing the imputation step. This enables us to generate a kernel matrix that is theoretically guaranteed, offering potential advantages for various kernel-based applications. Notably, our approach demonstrates significant improvements over imputation methods for spectral clustering when dealing with a high proportion of missing data. Additionally, the high-quality kernel produced by our approach can serve as a valuable reference for improving the accuracy of missing value estimation in kernel-based imputation methods [58; 59], which is worth further investigation.

Advantages over Distance CalibrationFirstly, our approach can be applied to a wide range of kernels and yields an improved kernel matrix. In contrast, distance calibration methods only benefit distance-based kernels and lack quality guarantees. Moreover, our algorithm, which corrects the Gaussian kernel, can generate a high-quality distance matrix via \(d_{ij}=(k_{ij})}\). By incorporating the second-order term of \(d_{ij}^{0}\) in \(k_{ij}^{0}=(-(d_{ij}^{0})^{2}/^{2})\), our algorithm becomes more sensitive to changes in distance values, while the Euclidean embedding algorithm relies on \(K^{0}=(- D^{0})\) using the first-order term of \(d_{ij}^{0}\). Empirical evidence suggests that the corrected distance obtained from our corrected Gaussian kernel is more accurate than the calibrated distance derived from the Euclidean embedding method, benefiting to the distance-based spectral clustering.

Benefits to Spectral ClusteringOur approach is tailored to improve the kernel quality, making it highly advantageous for spectral clustering. This improvement benefits both standard spectral clustering using the Gaussian kernel and affinity learning based on the self-expressive framework. In the case of \(X\)-based self-expressive affinity, we can apply the corrected linear kernel \(K:=X^{}X\) to the optimization, i.e., \(_{C}\|X-XC\|_{F}^{2}+(C)=_{C} (K-2KC+C^{}KC)+(C)\). Similarly, for \(K\)-based self-expressive affinity, which involves using the Gaussian kernel or other kernels, our approach can be applied to \(K:=(X)^{}(X)\) on \(_{C}\|(X)-(X)C\|_{F}^{2}+(C)\).

## 4 Methodology-II. Affinity Learning

Taking advantage of the corrected kernel matrix, we have further designed a series of kernel-based affinity learning algorithms to acquire high-quality affinity matrices with an adaptive extension.

### Kernel Self-expressive Learning with \(_{p}\)-norm

We utilize the kernel self-expressive framework, i.e., \(_{C}\|(X)-(X)C\|_{F}^{2}+(C)\), and propose new affinity learning methods by enhancing the sparsity of the affinity matrix \(C\) using the \(_{p}\) norm . Specifically, our approach diverges from previous work [19; 20; 3] by incorporating two forms of \(_{p}\)-norm, i.e., proximal \(p\)-norm and Schatten \(p\)-norm (\(0<p<1\)), as regularization terms.

**Definition 4** (Kernel Self-expressive Learning with Proximal \(p\)-norm).: \[*{minimize}_{C^{n n}}\ \|(X)-(X)C\|_{F}^{2}+\|C\|_{p}^{p},\ \ \ \ 0 c_{ij} 1,\ \ 1 i,j n,\] (7)

_where the proximal \(p\)-norm is expressed as \(\|C\|_{p}^{p}:=_{i,j=1}^{n}|c_{ij}|^{p}\)._

We construct an augmented Lagrangian function \(_{p}(C,Z,U)\) in Eq. (8), and solve it by using the Alternating Direction Method of Multipliers (ADMM) approach  summarized in Algorithm 2.

\[_{p}=\|(X)-(X)C\|_{F}^{2}+\|Z\|_{p}^{p}+_ {i,j}(z_{ij}-1,0)^{2}+(U^{}(Z-C))+\|Z-C\|_{F}^ {2}.\] (8)

However, the KSL-Pp algorithm involves numerous hyper-parameters and entails a non-convex optimization process during the \(Z\)-update step (Line 5 in Algorithm 2), making it difficult to effectively utilize. Computation speedup needs further investigations. Thus, we focus on the Schatten \(p\)-norm.

**Definition 5** (Kernel Self-expressive Learning with Schatten \(\)-norm).: \[^{n n}}{}\ \|(X)-(X)C \|_{F}^{2}+\|C\|_{S_{p}},\ \ \ \ 0 c_{ij} 1,\ \ 1 i,j n,\] (9)

_where Schatten \(p\)-norm \(\|C\|_{S_{p}}:=(_{i=1}^{n}_{i}^{p}(C))^{1/p}\) and \(_{i}(C)\) denotes the \(i\)-th singular value of \(C\)._

Drawing from prior research , for \(<p<1\) and \(k(C)\), the following always holds true:

\[\|C\|_{S_{p}}=^{n k},V^{n k}, C=UV^{}}{}^{2}+\|V\|_{F}^{2}}{2}.\]

Thus, for \(<p<1\), we define \(_{S_{p}}(U,V)=\|(X)-(X)UV^{}\|_{F}^{2}+\|U\|_{F}^{2}+\|V\|_{F}^{2}\) as a relaxation of the optimization problem in Eq. (9) without loss of generality. By employing the ADMM approach  and the gradient descent method  in the \(U\)-update step (i.e., Line 4 in Algorithm 3), we design the KSL-Sp algorithm as follows with details in the Appendix B.2.

```
0:\(K\): a kernel; \(\): a hyperparameter; \(maxiter\): maximum iterations; \(tol\): tolerance.
0:\(^{n n}\): the optimal affinity matrix of Eq. (9).
1: Initialize \(U_{0},V_{0}\). \(\) Refer to Appendix B.2 for formula derivations.
2:for\(t=0,1,,maxiter\)do
3:\(U\)-update: \(U_{t+1}_{U}_{S_{p}}=2K(UV_{t}^{}-I)V_{t}+  U=0\);
4:\(V\)-update: \(V_{t+1} 2KU_{t+1}(2U_{t+1}^{}KU_{t+1}+ I)^{-1}\) by \(_{V}_{S_{p}}=0\).
5:if\(\|U_{t+1}-U_{t}\|_{F}<tol\)then; break; endif.
6:endfor
7: Set \(C_{t}=U_{t}V_{t}^{}\) and \(=(|C_{t}|+|C_{t}^{}|)/2\). ```

**Algorithm 3**Kernel Self-expressive Learning with Schatten \(\)-norm (**KSL-Sp**)

### Adaptive Kernel Self-expressive Learning

To achieve adaptive affinity learning on incomplete data, we formulate a new joint optimization problem by incorporating KLSR  and kernel correction to learn kernel and affinity matrices iteratively with the PSD constraint of \(K\):

\[^{n n}}{}\ \ \|K-K^{0}\|_{F}^{2}+(K-2KC+C^{}KC)+\|C\|_{F}^{2}.\] (10)

Firstly, we introduce an auxiliary variable \(A^{n n}\) into

\[^{n n}}{}\ \|K-K^{0}\|_{F}^{2}+(A-2AC+C^{} AC)+\|C\|_{F}^{2},\ \ \ \ K=A,\] (11)

and then we derive the augmented Lagrange function \((K,A,C,U)\) in preparation for ADMM:

\[=\|K-K^{0}\|_{F}^{2}+(A-2AC+C^{}AC)+\|C\|_{F}^{ 2}+(U^{}(K-A))+\|K-A\|_{F}^{2},\] (12)

where \(U\) is a Lagrange multiplier and \(\) is the updating step size, finally arriving at the Algorithm 4.

Our adaptive learning framework could be combined with other kernel self-expressive learning algorithms; however, due to space constraints, further research is required to explore these possibilities:

\[^{n n}}{}\ \ \|K-K^{0}\|_{F}^{2}+ _{1}\|(X)-(X)C\|_{F}^{2}+_{2}(C),K:=(X)^{}(X).\] (13)

```
0:\(K^{0}\): an initial kernel; \(,\): hyperparameters; \(maxiter\): maximum iterations; \(tol\): tolerance.
0:\(^{n n}\): the optimal affinity matrix of Eq. (10).
1: Initialize \(K_{0},A_{0},C_{0},U_{0}\). \(\) Refer to Appendix B.3 for formula derivations.
2:for\(t=0,1,,maxiter\)do
3:\(K\)-update: \(K(2K^{0}-U_{t}+ A_{t})\) by \(}{ K}=0\);
4:\(K\)-PSD: \(K_{t+1}_{1}(K)\) by projecting \(K\) onto the PSD set via Eq. (6);
5:\(A\)-update: \(A_{t+1}( K_{t+1}+U_{t}+2C_{t}^{}-C_{t}C_{t}^{ }-I)\) by \(}{ A}=0\);
6:\(C\)-update: \(C_{t+1} 2(2 I+A_{t+1}+A_{t+1}^{})^{-1}A_{t+1}^{}\) by \(}{ C}=0\);
7:\(U\)-update: \(U_{t+1} U_{t}+(K_{t+1}-A_{t+1})\).
8:if\(\|C_{t+1}-C_{t}\|_{F}<tol\)then; break; endif.
9:endfor
10: Set \(=(|C_{t}|+|C_{t}^{}|)/2\). ```

**Algorithm 4** Adaptive **Kernel Least-Squares **R**epresentation (**AKLSR**)

## 5 Experiments

We evaluate the performance on four benchmark datasets, including two face image datasets **Yale64** and **Umist**, a handwritten digit image dataset **USPS**, and a speech dataset **Isolet**. We use a subset of USPS with 1,000 randomly selected samples. All experiments 2 are conducted five times in MATLAB on a ThinkStation with a 2.1 GHz Intel i7-12700 Core and 32GB RAM.

Various methods dealing with incomplete data are considered for comparison: 1) statistical imputation: **ZERO**, **MEAN**, \(k\)-nearest neighbors (\(k\)**NN**) , Expectation Maximization (**EM**) ; 2) matrix completion: Singular Value Thresholding (**SVT**) , Grassmanian Rank-one Update Subspace Estimation (**GR**) , Kernelized Factorization Matrix Completion (**KFMC**) ; 3) distance calibration: Double-Centering (**DC**) , Triangle Fixing (**TRF**) , Euclidean Embedding (**EE**) . Our KC method uses the spectral decomposition (SD). Details are provided in Appendix C.

### Validation of Kernel Correction on Gaussian Kernel and Euclidean Distance

When handling incomplete data that is missing completely at random, the quality of the estimated Gaussian kernel matrix and Euclidean distance matrix is measured using the relative-mean-square error (\(=-A^{*}\|_{F}^{2}}{\|A^{0}-A^{*}\|_{F}^{2}}\)) and the relative error (\(=-A^{*}\|_{F}}{\|A^{0}-A^{*}\|_{F}}\)), where \(A^{0}\) is the naive kernel/distance matrix obtained by Eq. (1), \(\) is the estimated one using different methods, and \(A^{*}\) as the ground-truth. All Gaussian kernels are calculated by \(k_{ij}=(-d_{ij}^{2}/^{2})\), where \(=\{d_{ij}\}\). Our KC algorithm yields Gaussian kernel and Euclidean distance matrices with the lowest RMSEs and REs, validating the theoretical guarantee of kernel matrices in Theorem 1, as demonstrated in Table 1. Furthermore, the accuracy of the top-10 nearest neighbors is evaluated using Recall values . The KC algorithm achieves the highest Recalls with improved local relationships, which in turn benefits spectral clustering, particularly for \(k\)NN graphs. More numerical results are in Appendix D.

   Dataset & Metric & Naive & ZERO & MEAN & \(k\)NN & EM & SVT & GR & KFMC & DC & TRF & EE & KC \\   & RMSE-K\(\) & 1.000 & 2.849 & 1.535 & 2.451 & 1.527 & 2.849 & 1.797 & 3.818 & 1.748 & 0.923 & 0.438 & **0.382** \\  & RMSE-D\(\) & 1.000 & 20.80 & 27.64 & 21.20 & 27.62 & 20.80 & 8.397 & 14.05 & 110.4 & 0.916 & 0.440 & **0.431** \\   & RE-K\(\) & 0.189 & 0.319 & 0.234 & 0.295 & 0.233 & 0.319 & 0.253 & 0.369 & 0.250 & 0.181 & 0.125 & **0.117** \\   & RE-D\(\) & 0.107 & 0.487 & 0.561 & 0.492 & 0.561 & 0.487 & 0.309 & 0.400 & 1.122 & 0.102 & 0.071 & **0.070** \\   & Recall\(\) & 0.726 & 0.092 & 0.171 & 0.119 & 0.172 & 0.092 & 0.596 & 0.248 & 0.226 & 0.740 & 0.771 & **0.785** \\   

Table 1: Quality comparisons of Gaussian kernel and Euclidean distance under a missing ratio \(80\%\).

### Comparative Studies on Standard Spectral Clustering

We adopt the Gaussian kernel for standard spectral clustering and obtain its \(k\)NN graph (\(k=10\)), as the input for clustering via the normalized cut algorithm . We evaluate the clustering performance using Accuracy (**ACC**), Normalized Mutual Information (**NMI**), Purity (**PUR**), and Adjusted Rand Index (**ARI**). Table 2 shows that our KC algorithm, which corrects the Gaussian kernel, achieves the best clustering performance under a large missing ratio \(80\%\) (i.e., missing completely at random).

We also examine robustness across varying missing ratios. Fig. 1 shows the KC method's advantage over imputation, especially at large missingness. Numerical results are in Appendix E.

\(\) The KC method consistently surpasses others in kernel estimation, evidenced by reduced relative errors (RE-K) and enhanced neighborhood ties in the \(k\)NN graph, seen through higher Recall values.

\(\) At small missing ratios, the KC method offers incremental improvements over imputation. However, at large missing ratios, imputation methods falter due to scant observed data, leading to increased errors and compromised clustering. Conversely, the KC method maintains stable clustering outcomes.

Figure 1: Robustness analysis of kernel estimation, neighborhood relationship, and standard spectral clustering on the Yale64 and Umist datasets under a wide range of missing ratios, i.e., \(r[20\%,80\%]\).

   Dataset & Metric & Naive & ZERO & MEAN & \(k\)NN & EM & SVT & GR & KFMC & DC & TRF & EE & KC \\   & ACC & 0.561 & 0.218 & 0.365 & 0.227 & 0.374 & 0.224 & 0.218 & 0.259 & 0.513 & 0.562 & 0.573 & **0.578** \\  & NMI & 0.588 & 0.246 & 0.429 & 0.257 & 0.428 & 0.269 & 0.264 & 0.297 & 0.551 & 0.587 & 0.593 & **0.596** \\  & PUR & 0.572 & 0.233 & 0.390 & 0.241 & 0.396 & 0.241 & 0.232 & 0.273 & 0.525 & 0.570 & 0.581 & **0.584** \\  & ARI & 0.353 & 0.010 & 0.137 & 0.015 & 0.145 & 0.017 & 0.012 & 0.035 & 0.293 & 0.350 & 0.357 & **0.366** \\   & ACC & 0.462 & 0.220 & 0.351 & 0.230 & 0.349 & 0.218 & 0.410 & 0.314 & 0.350 & 0.462 & 0.461 & **0.463** \\  & NMI & 0.669 & 0.282 & 0.478 & 0.314 & 0.479 & 0.286 & 0.597 & 0.423 & 0.488 & 0.667 & 0.669 & **0.673** \\  & PUR & 0.549 & 0.245 & 0.415 & 0.261 & 0.419 & 0.247 & 0.502 & 0.366 & 0.408 & 0.546 & 0.551 & **0.553** \\  & ARI & 0.373 & 0.070 & 0.206 & 0.082 & 0.207 & 0.067 & 0.304 & 0.140 & 0.216 & 0.370 & 0.371 & **0.377** \\   & ACC & 0.343 & 0.350 & 0.362 & 0.360 & 0.222 & 0.353 & 0.375 & 0.351 & 0.418 & 0.464 & 0.511 & **0.523** \\  & NMI & 0.222 & 0.228 & 0.278 & 0.265 & 0.104 & 0.236 & 0.319 & 0.358 & 0.312 & 0.393 & 0.457 & **0.472** \\  & PUR & 0.395 & 0.400 & 0.434 & 0.427 & 0.260 & 0.402 & 0.440 & 0.460 & 0.473 & 0.535 & 0.594 & **0.609** \\  & ARI & 0.168 & 0.164 & 0.180 & 0.175 & 0.051 & 0.167 & 0.206 & 0.178 & 0.231 & 0.304 & 0.344 & **0.360** \\   & ACC & 0.495 & 0.260 & 0.245 & 0.297 & 0.172 & 0.263 & 0.357 & 0.324 & 0.243 & 0.515 & 0.560 & **0.561** \\  & NMI & 0.613 & 0.339 & 0.330 & 0.383 & 0.228 & 0.342 & 0.492 & 0.465 & 0.323 & 0.643 & 0.660 & **0.672** \\   & PUR & 0.520 & 0.278 & 0.261 & 0.318 & 0.183 & 0.277 & 0.377 & 0.354 & 0.260 & 0.542 & 0.584 & **0.593** \\   & ARI & 0.364 & 0.126 & 0.108 & 0.159 & 0.055 & 0.131 & 0.210 & 0.177 & 0.105 & 0.387 & 0.429 & **0.432** \\   

Table 2: Performance of standard spectral clustering on incomplete data under a missing ratio \(80\%\).

[MISSING_PAGE_FAIL:10]