# Synthetic Combinations:

A Causal Framework for Combinatorial Interventions

 Abhinneet Agarwal

Department of Statistics

University of California, Berkeley

aa3797@berkeley.edu

&Anish Agarwal

Department of IEOR

Columbia University

aa5194@columbia.edu

Work done while post-doc at Amazon, Core AI.

Suhas Vijaykumar

Amazon, Core AI

suhasv@mit.edu

Work done while at MIT.

###### Abstract

We consider a setting where there are \(N\) heterogeneous units and \(p\) interventions. Our goal is to learn unit-specific potential outcomes for any combination of these \(p\) interventions, i.e., \(N 2^{p}\) causal parameters. Choosing a combination of interventions is a problem that naturally arises in a variety of applications such as factorial design experiments and recommendation engines (e.g., showing a set of movies that maximizes engagement). Running \(N 2^{p}\) experiments to estimate the various parameters is likely expensive and/or infeasible as \(N\) and \(p\) grow. Further, with observational data there is likely confounding, i.e., whether or not a unit is seen under a combination is correlated with its potential outcome. We study this problem under a novel model that imposes latent structure across _both_ units and combinations of interventions. Specifically, we assume latent similarity in potential outcomes across units (i.e., the matrix of potential outcomes is rank \(r\)) and regularity in how combinations of interventions interact (i.e., the coefficients in the Fourier expansion of the potential outcomes is \(s\) sparse). We establish identification for all \(N 2^{p}\) parameters despite unobserved confounding. We propose an estimation procedure, Synthetic Combinations, and establish finite-sample consistency under precise conditions on the observation pattern. We show that Synthetic Combinations is able to consistently estimate unit-specific potential outcomes given a total of \((r)N\!+\!s^{2}p\) observations. In comparison, previous methods that do not exploit structure across both units and combinations have poorer sample complexity that scales as \((N s^{2}p,\ (r)(N+2^{p}))\).

## 1 Introduction

Modern-day decision makers--in settings from e-commerce to public policy to medicine--often must select a combination of actions, and would like to do so in a highly personalized manner. Examples include recommending a curated basket of items to customers on a commerce platform, deciding on a combination of therapies for a medical patient, enacting a collection of socio-economic policies for a specific geographic location, conjoint analysis in surveys, selecting important feature sets for machine learning models, etc. Despite the ubiquity of this setting, it comes with significant empirical challenges: with \(p\) interventions and \(N\) units, a decision maker must evaluate \(N 2^{p}\) potential combinations in order to confirm the optimal personalized policy. With large \(N\) and even with relatively small \(p\) (due to exponential dependence), it becomes infeasible to run that many experiments; in observational data there is the additional challenge of potential unobserved confounding. Current methods tackle this problem by following one of two approaches: (i) they impose structure on how combinations of interventions interact, or (ii) they assume latent similarityin potential outcomes across units. However, as we discuss in detail below, these approaches require a large number of observations to estimate all \(N\!\!2^{p}\) potential outcomes because they do not exploit structure across both units and combinations. This motivates the question: _how can one effectively share information across both units and combinations of interventions?_

**Contributions.** Our contributions may be summarized as follows. **(1)** For a unit \(n\!\![N]\), we represent its potential outcomes over the \(2^{p}\) combinations as a Boolean function from \(\{-1,1\}^{p}\) to \(\), expressed in the Fourier basis. To impose structure across combinations, we assume that for a unit \(n\), the linear coefficients \(_{n}\!\!^{2^{p}}\) induced by this Fourier basis representation is \(s\)-sparse, i.e., has at most \(s\) non-zero entries. To impose structure across units, we assume that this matrix of Fourier coefficients across units \(\!=\![_{n}]_{n[N]}\!\!^{N 2^ {p}}\) has rank \(r\). This simultaneous sparsity and low-rank assumption allows the researcher to share information across both units and combinations. **(2)** We establish identification for the \(N\!\!2^{p}\) potential outcomes of interest, which requires that any confounding is mediated by the (unobserved) matrix of Fourier coefficients \(\). **(3)** We design a two-step algorithm "Synthetic Combinations" and prove it consistently estimates the various causal parameters, despite potential unobserved confounding. The first step of Synthetic Combinations, termed "horizontal regression", learns the structure across combinations of interventions--assuming sparsity in the Fourier coefficients--via the Lasso. The second step, termed "vertical regression", learns the structure across units--assuming the matrix of Fourier coefficients are low-rank--via principal component regression (PCR). **(4)** We show that Synthetic Combinations is able to consistently estimate unit-specific potential outcomes given a total of \((r)\!\!N\!+\!s^{2}p\) observations (ignoring logarithmic factors). This improves over previous methods that do not exploit structure across both units and combinations, which have sample complexity scaling as \((N\!\!s^{2}p,\ \ (r)\!\!(N\!+\!2^{p}))\). A summary of the sample complexities required for different methods can be found in Table 1. A key technical challenge in our proofs is analyzing how the error induced in the first step of Synthetic Combinations percolates through to the second step. To tackle it, we reduce the problem to that of high-dimensional error-in-variables regression with linear model misspecification, and do a novel analysis of this statistical setting.

## 2 Related Work

**Learning over Combinations.** To place structure on the space of combinations, we use tools from the theory of learning (sparse) Boolean functions, in particular the Fourier transform. Sparsity of the Fourier transform was proposed as a complexity measure to characterize and design learning algorithms for low-depth trees, low-degree polynomials, and small circuits . Learning Boolean functions is now a central topic in learning theory, and is closely related to many important questions in ML more broadly; see e.g.  for discussion of the \(k\)-Junta problem and its relation to relevant feature selection. We refer to O'Donnell [34, Chapter 3] for further background on this area. In this paper, we focus on  which showed that Lasso can be used to efficiently learn sparse Boolean functions, an essential property in our setting since the dimension of the function class grows exponentially.

**Matrix Completion.** We build on the observation that imputing counterfactual outcomes in the presence of a latent factor structure can be equivalently expressed as low-rank matrix completion . The observation that low-rank matrices may typically be recovered from a small fraction of the entries by nuclear-norm minimization has had a major impact on modern statistics . In the noisy setting, proposed estimators have generally proceeded by minimizing risk subject to a nuclear-norm penalty, such as in the SoftImpute algorithm of , or minimizing risk subject to a rank constraint as in the hard singular-value thresholding (HSVT) algorithms analyzed by . We refer the reader to  for a comprehensive overview of this vast literature.

  Learning algorithm & Exploits structure across combinations (\(\|_{n}\|_{0}\!=\!s\)) & Exploits structure across units (rank\(()\!=\!r\)) & Sample complexity \\  Lasso & ✓ & ✗ & \(ON\!\!s^{2}p\) \\  Matrix Completion & ✗ & ✓ & \(O(r)\!\!(N\!+\!2^{p})\) \\  Synthetic Combinations & ✓ & ✓ & \(O(r)\!\!(N\!+\!s^{2}p)\) \\  

Table 1: Comparison of sample complexity of Synthetic Combinations to other methods.

**Econometrics/causal inference.** There is a rich literature on how to learn personalized treatment effects for heterogeneous units. This problem is of particular importance in the social sciences and in medicine, where experimental data is limited, and has led to several promising approaches including instrumental variables, difference-in-differences, regression discontinuity, and others; see [7; 25] for an overview. Of particular interest to us here is the "synthetic control" method , which exploits an underlying factor structure to effectively "share" counterfactual information between treatment and control units. Building on , recent work has shown that the same underlying structure can be used to estimate treatment effects with multiple treatments _and_ heterogeneous units despite unmeasured confounding. The resulting framework, called "synthetic interventions" (SI), uses the factor representation to efficiently share information across similar (yet distinct) units and treatments . We generalize the SI framework to settings where treatments are combinations of interventions and most treatments have no units that receive it, but there is structure across combinations of interventions. In doing so, we enable its use in highly practical cases where multiple interventions are delivered simultaneously, such as recommender systems, medical treatment regimens, and factorial design experiments.

## 3 Setup and Model

In this section, we first describe requisite notation, background on the Fourier expansion of real-valued functions over booleans, and how it relates to potential outcomes over combinations.

### Notation

**Representation of Combinations as Binary Vectors**. Let \([p]=\{1,...\,p\}\) denote the set of \(p\) interventions. Denote by \(\) the power set of \([p]\), i.e., the set of all possible combinations of \(p\) interventions, where we note \(||=2^{p}\). Then, any given combination \(\) induces the following binary representation \(()\{-1,1\}^{p}\) defined as follows: \(()_{i}=2\,\{i\}-1\).

**Fourier Expansion of Boolean Functions.** Let \(_{}=\{f:\{-1,1\}^{p}\}\) be the set of all real-valued functions defined on the hypercube \(\{-1,1\}^{p}\). Then \(_{}\) forms a Hilbert space defined by the following inner product: for any \(f,g_{}\), \( f,g_{B}=}_{\{-1,1\}^{p}}f( )g()\). This inner product induces the norm \( f,f_{B}\|f\|_{B}^{2}=}_{ \{-1,1\}^{p}}f^{2}()\). We construct an orthonormal basis for \(_{}\) as follows: for each subset \(S[p]\), define a basis function \(_{S}()=_{i S}x_{i}\) where \(x_{i}\) is the \(i^{}\) coefficient of \(\{-1,1\}^{p}\). One can verify that for any \(S[p]\) that \(\|_{S}\|_{B}=1\), and that \(_{S},_{S^{}}_{B}=0\) for any \(S^{} S\). Since \(|\{_{S}:S[p]\}|=2^{p}\), the functions \(_{S}\) are an orthonormal basis of \(_{}\).

Hence, any \(f_{}\) can be expressed via the following "Fourier" decomposition: \(f()=_{S[p]}\,_{S}_{S}(),\) where the Fourier coefficient \(_{S}\) is given by computing \(_{S}= f,_{S}_{B}\). We will refer to \(_{S}\) as the Fourier character. Define \(_{f}=[_{S}]_{S[p]}^{2^{p}}\) and \((x)=_{S}()_{S[p]}\{-1,1\} ^{2^{p}}\) as the vector of Fourier coefficients and characters respectively. Hence any function \(f\{-1,1\}^{p}\) can be re-expressed as follows: \(f()=_{f},(x)\). For \(\), abbreviate \(_{S}(())\) and \((())\) as \(_{S}^{}\) and \(^{}\) respectively.

**Observed and potential outcomes.** Let \(Y_{n}^{()}\) denote the _potential outcome_ for unit \(n\) under combination \(\) and \(Y_{n}\{\}\) as the _observed outcome_, where \(\) indicates a missing value, i.e., the outcome associated with the unit-combination pair \((n,)\) was not observed. Let \(=[Y_{n}]\{\}^{N 2^{p}}\). Let \([N][2^{p}]\), refer to the subset of unit-combination pairs we do observe, i.e.,

\[Y_{n}=Y_{n}^{()},&(n,)\\ ,&. \]

Note that (1) implies stable unit treatment value assignment (SUTVA) holds. Let \(_{S}\) denote a subset of combinations. For a given unit \(n\), let \(_{_{S},n}=[Y_{n_{i}}:_{i}_{S}]\{ \}^{|_{S}|}\) represent the vector of observed outcomes for all \(_{S}\). Similarly, let \(_{n}^{(_{S})}=[Y_{n}^{(_{i})}:_{i}_{S}]^{| _{S}|}\) represent the vector of potential outcomes. Denote \((_{S})=[^{_{i}}:_{i}_{S}] \{-1,1\}^{|_{S}| 2^{p}}\).

### Model & Target Causal Parameter

Define \(Y_{n}^{()}\!:\!\!\!\) as a real-valued function over the hypercube \(\{-1,1\}^{p}\) associated with unit \(n\). It takes as input a combination \(\), converts it to a \(p\)-dimensional binary vector \(()\), and outputs a real number \(Y_{n}^{()}\). Given the discussion in Section 3.1, it follows that \(Y_{n}^{()}\) always has the representation \(_{n},^{}\) for some \(_{n}\!\!^{2^{p}}\). Thus, without any loss of generality, the \(_{n}\) are unit-specific latent variables (Fourier coefficients) encoding the treatment response function. Below, we state our key assumption on these induced Fourier coefficients.

**Assumption 3.1** (Potential Outcome Model).: _For any unit-combination pair \((n,)\), we assume it has the following representation,_

\[Y_{n}^{()}\!=\!_{n},^{} \!+\!_{n}^{}, \]

_where \(_{n}\!\!^{2^{p}}\) and \(^{}\!\!\{-1,1\}^{2^{p}}\) are the Fourier coefficients and characters, respectively. We assume the following properties: (a) low-rank: the matrix \(\!=\![_{n}]_{n[N]}\) has rank \(r\!\![\{N,\!2^{p}\}]\); (b) sparsity: \(_{n}\) is \(s\)-sparse (i.e. \(\|_{n}\|_{0}\!\!s\), where \(s\!\![2^{p}]\)) for every unit \(n\!\![N]\); (c) \(_{n}^{}\) is a residual term specific to \((n,\!)\) which satisfies \([_{n}^{}]\!=\!0\)._

The assumption is then that each \(_{n}\)\(s\)-sparse and \(\) is rank-\(r\); \(_{n}^{}\) is the residual from this sparse and low-rank approximation, and it serves as the source of uncertainty in our model. Given \([_{n}^{}]\!=\!0\), the matrix \([_{N}^{()}]\!=\![[_{n}^{()}]\!:\! n\!\![N]]\!\!^{2^{p} N}\) also is rank \(r\), where the expectation is defined with respect to \(_{n}^{}\). This is because \([_{N}^{()}]\) can be written as \([_{N}^{()}]\!=\!()^{T}\), and since \(()\) is an invertible matrix, \(([_{N}^{()}])\!=\!()\). The low-rank property places _structure across units_; that is, we assume there is sufficient similarity across units so that \([_{n}^{()}]\) for any unit \(n\) can be written as a linear combination of \(r\) other rows of \([_{N}^{()}]\). This is a standard assumption used to encode latent similarity across units in matrix completion and its related applications (e.g., recommendation engines).

Sparsity establishes _unit-specific_ structure; that is, we assume that the potential outcomes for a given user only depend on a small subset of the functions \(\{_{S}\!:\!S\!\![p]\}\). We emphasize that this subset of functions can be different across units. As discussed in Section 2, sparsity is commonly employed when studying the learnability of Boolean functions. In the context of recommendation engines, sparsity is implied if the ratings for a set of goods only depend on a small number of combinations of items within that set. Sparsity is also often assumed implicitly in factorial design experiments, where analysts typically only include pairwise interaction effects between interventions and ignore higher-order interactions . We discuss further applications of combinatorial inference in greater detail in Appendix A.

Next, we present an assumption that formalizes the dependence (i.e., confounding) between the missingness pattern induced by the treatment assignments \(\) and the potential outcomes \(Y_{n}^{()}\), and provide an interpretation of the induced data generating process (DGP) for potential outcomes.

**Assumption 3.2** (Selection on Fourier coefficients).: _For all \(n\!\![N]\) and \(\!\!\), \(Y_{n}^{()}\!\!\!\!\)._

**DGP.** Given Assumptions 3.1 and Assumption 3.2, the DGP can be summarized as follows: (i) unit-specific latent Fourier coefficients \(\) are either deterministic or sampled from an unknown distribution; we will condition on this quantity throughout. (ii) Given \(\), we sample mean-zero random variables \(_{n}^{}\), and generate potential outcomes according to our model \(Y_{n}^{()}\!=\!_{n},\!^{}\!+\! _{n}^{}\). (iii) \(\) is allowed to depend on unit-specific latent Fourier coefficients \(\) (i.e. \(\!=\!f()\)). We define all expectations w.r.t. noise, \(_{n}^{}\). This DGP introduces unobserved confounding since \(Y_{n}^{()}\!\!\). However, this DGP does imply that Assumption 3.2 holds, i.e., conditional on the Fourier coefficients \(\), the potential outcomes are independent of the treatment assignments \(\). This conditional independence condition can be thought of as "selection on latent Fourier coefficients", which generalizes the widely made assumption of "selection on observables." The latter requires that potential outcomes are independent of treatment assignments conditional on _observed_ covariates-we reemphasize that \(\) is unobserved.

**Target parameter.** For any unit-combination pair \((n,\!)\), we aim to estimate \([Y_{n}^{()}\!\!]\), where the expectation is w.r.t. \(_{n}^{}\), and we condition on the set of Fourier coefficients \(\).

## 4 Identification of Potential Outcomes

We show that \([Y_{n}^{()}]\) can be written as a function of observed outcomes, i.e., we establish identification of our target causal parameter. As discussed earlier, our model allows for _unobserved confounding_: whether or not a unit is seen under a combination may be correlated with its potential outcome under that combination due to unobserved factors, as long as certain conditions are met. We introduce necessary notation and assumption required for our result. For a unit \(n[N]\), denote the subset of combinations we observe them under as \(_{n}\). For \(\), let \(_{n}^{}^{2^{p}}\) denote the vector where we zero out all coordinates of \(^{}^{2^{p}}\) that correspond to the coefficients of \(_{n}\) which are zero. For example, if \(_{n}=(1,1,0,0,...0)\) and \(^{}=(1,1,...1)\), then \(_{n}^{}=(1,1,0,...0)\). We the make the following assumption.

**Assumption 4.1** (Donor Units).: _We assume there exists a set of "donor units" \([N]\), such that the following two conditions hold:_

1. _Horizontal span inclusion: For any donor unit_ \(u\) _and combination_ \(\)_, suppose_ \(}_{u}^{} span(}_{u}^{ _{i}}:_{i}_{u})\)_. That is, there exists exists_ \(_{_{u}}^{}^{|_{u}|}\) _such that_ \(}_{u}^{}=_{_{i}_{u}}_{_{i}}^{ }}_{u}^{_{i}}\)_._
2. _Linear span inclusion: For any unit_ \(n[N]\)_, suppose_ \(_{n} span(_{u}:u)\)_. That is, there exists_ \(^{n}\) _such that_ \(_{n}=_{u}w_{u}^{n}_{u}\)__

Horizontal span inclusion requires that the set of observed combinations for any donor unit is "diverse" enough that the projection of the Fourier characteristic for a target intervention is in the span of characteristics of observed interventions. Linear span inclusion requires that the donor set is diverse enough such that the Fourier coefficient of any unit is in the span of the Fourier coefficients of the donor set.

### Identification Result

Given these assumptions, we now present our identification theorem.

**Theorem 4.2**.: _Let Assumptions 3.1, 3.2, 4.1 hold. Given \(_{_{u}}^{}\) and \(_{u}^{n}\) defined in Assumption 4.1, we have_

_(a) Donor units: For_ \(u\)_, and_ \(\)_,_ \([Y_{u}^{()}]=_{_{u}_{u}}_{_{u} }^{}[Y_{u,_{u}},]\)_._

_(b) Non-donor units: For_ \(n[N]\)_, and_ \(_{n}\)_,_ \([Y_{n}^{()}]=_{u,_{u}_{ u}}w_{u}^{n}_{_{u}}^{}[Y_{u,_{u}}, ]\)_._

Theorem 4.2 gives conditions under which the donor set \(\) and the observation pattern \(\) are sufficient to recover the full set of unit specific potential outcomes \([Y_{u}^{()}|]\) in the noise-free limit. Part (a) establishes that for every donor unit \(u\), the causal estimand can be written as a function of its _own_ observed outcomes \([_{_{u}}]\), given knowledge of \(_{_{u}}^{}\). Part (b) states that the target causal estimand \([Y_{n}^{()}]\) for a non-donor unit and combination \(\) can be written as a linear combination of the outcomes of the donor set \(\), given knowledge of \(_{n}^{n}\). Previous works that establish identification under a latent factor model requires a growing number of donor units to be observed under all treatments . This is infeasible in our setting because _the vast majority of combinations have no units that receive it_. As a result, we have to first to identify the outcomes of donor units under all combinations (part (a)), before transferring them to non-donor units (part (b)). In order to do so, Theorem 4.2 suggests that the key quantities in estimating \([Y_{n}^{()}]\) for any unit-combination pair \((n,)\) are \(_{_{u}}^{}\) and \(_{u}^{n}\). In the following section, we provide an algorithm to estimate both \(_{_{u}}^{}\) and \(_{u}^{n}\), as well as concrete ways of determining the donor set \(\).

## 5 The Synthetic Combinations Estimator

We now describe the Synthetic Combinations estimator, a simple and flexible two-step procedure for estimating our target causal parameter. See Figure 1 for a pictorial representation.

**Step 1: Horizontal Regression.** We denote the vector of observed responses \(_{n,_{n}}=[Y_{n}:_{n}]^{|_{n}|}\) for any unit \(n\) as \(_{_{n}}\). Then, for every unit \(u\) in the donor set \(\), we estimate \([Y_{u}^{()}]\)via the Lasso, i.e., by solving the following convex program with penalty parameter \(_{u}\):

\[}_{u}\!=\!*{argmin}_{}| }\|_{_{u}}\!-\!(_{u})\|_{2}^{2}\!+\! _{u}\|\|_{1} \]

where recall that \((_{u})=[^{}:_{u}]^{|_{u}| 2 ^{p}}\). Then, for any donor unit-combination pair \((u,)\), let \(}[Y_{u}^{()}]\!=\!}_{u}^{}\) denote the estimate of the potential outcome \([Y_{u}^{()}]\).

**Step 2: Vertical Regression.** Next, we estimate potential outcomes for all units \(n\!\![N]\). To do so, we define some required notation. For \(_{S}\!\!\), define the vector of estimated potential outcomes \(}[_{u}^{(_{S})}]\!=\![}[Y_{u}^{( )}]\!:\!\!\!^{S}]\!\!^{|_{S}|}\). Additionally, let \(}[_{}^{(_{S})}]\!=\![}[ _{u}^{(_{S})}]\!:\!u\!\!]\!\!R^{|_{S}|| |}\).

_Step 2(a): Principal Component Regression._ Perform a singular value decomposition (SVD) of \(}[_{}^{(_{n})}]\) to get \(}[_{}^{(_{n})}]\!=\!_{l=1}^{(| _{n}|,||)}_{l}}_{l}}_{l}^{T}\). Using a hyper-parameter \(\!\!(|_{n}|,||)\)3, compute \(}^{n}\!\!^{||}\) as follows:

\[}^{n}\!=\!(_{l=1}^{}\!_{l}^{-1}}_{l}}_{l}^{T})_{_{n}} \]

_Step 2(b): Estimation._ Using \(}^{n}\!=\![_{u}^{n}\!:\!u\!\!]\), we have the following estimate for any \(\!\!\)

\[}[Y_{n}^{()}]\!=\!_{u}\!_{u}^{n} }[Y_{u}^{()}] \]

**Suitability of Lasso and PCR.** Lasso is appropriate for the horizontal regression because \(_{u}\) is sparse. However, Synthetic Combinations allows for any ML algorithm (e.g., neural networks, random forests) to be used in the first step. This flexibility allows an analyst to tailor the horizontal learning procedure, and include prior information. However, we leave this model-agnostic analysis as future work, and focus on the Lasso for simplicity. PCR is appropriate for the vertical regression because \(\) is low rank. As [3; 4] show, PCR implicitly regularizes the regression by adapting to the rank of the covariates (\(_{_{n}}\)), i.e., the out-of-sample error of PCR scales with \(r\) rather than the ambient covariate dimension.

**Determining Donor Set \(\).** Synthetic Combinations requires the existence of a subset of units \(\!\![N]\) such that we are able to (i) accurately estimate their potential outcomes under all possible combinations, and (ii) transfer these estimated outcomes to a unit \(n\!\![N]\). Theoretically, we detail sufficient conditions on the observation pattern such that we are able to perform (i) and (ii) accurately via the Lasso and PCR respectively. In practice, we recommend the following to determine \(\). For every unit \(n\!\![N]\), learn a separate Lasso model \(_{n}\) and assess its performance through cross-validation (CV). Assign units with low CV error (with a pre-determined threshold) as the donor set \(\)

Figure 1: (a) depicts an example of a particular observation pattern with outcome for unit-combination pair \((n,)\) missing. (b) demonstrates horizontal regression for donor unit \(u\) where we estimate potential outcome \([Y_{u}^{()}]\). (c) visualizes vertical regression where we transfer estimated outcomes from the donor set \(\) to \((n,)\).

and estimate outcomes \(}[Y_{u}^{()}]\) for every unit \(u\) and \(\). For non-donor units, PCR performance can also be assessed via k-fold CV. For units with low PCR error, linear span inclusion (Assumption 4.1(b)) and the assumptions required for the generalization for PCR likely hold, and hence we estimate their potential outcomes as in (5). For units with large PCR error, it is either unlikely that these set of assumptions holds or that \(|_{n}|\) is not large enough (i.e., additional experiments need to be run for this unit), and hence we do not recommend estimating their counterfactuals.

## 6 Synthetic Combinations Theoretical Analysis

In this section, we establish finite-sample consistency of Synthetic Combinations, starting with a discussion of the additional assumptions required for our results.

### Additional Assumptions

**Assumption 6.1** (Bounded Potential Outcomes).: _We assume that \([Y_{n}^{()}][-1,1]\) for any unit-combination pair \((n,)\)._

**Assumption 6.2** (Sub-Gaussian Noise).: _Conditioned on \(\), for any unit-combination pair \((n,)\), \(_{n}^{}\) are independent zero-mean sub-Gaussian random variables with \([_{n}^{}]^{2}\) and \(\|_{n}^{}\|_{_{2}} C\) for some constant \(C>0\)._

**Assumption 6.3** (Incoherence of Donor Fourier characteristics).: _For every unit \(u\), assume \((_{u})\) satisfies incoherence: \(\|(_{u})^{T}(_{u})}{|_{u}|}-_ {2p}\|_{}}{s}\), for a universal constant \(C^{}>0\)._

To define our next set of assumptions, we introduce necessary notation. For any subset of combinations \(_{S}\), let \([_{}^{(_{S})}]=[[_{u}^{( _{S})}] u]^{|_{S}|||}\).

**Assumption 6.4** (Donor Unit Balanced Spectrum).: _For a given unit \(n[N]\), let \(r_{n}\) and \(s_{1}...s_{r_{n}}\) denote the rank and non-zero singular values of \([_{}^{(_{n})}]\). We assume that the singular values are well-balanced, i.e., for universal constants \(c,c^{}>0\), we have that \(s_{r_{n}}/s_{1} c\), and \(\|[_{}^{(_{n})}]\|_{F}^{2}  c^{}|_{n}|||\)._

**Assumption 6.5** (Subspace Inclusion).: _For a given unit \(n[N]\) and intervention \(_{n}\), assume that \([_{}^{()}]\) lies within the row-span of \([_{}^{(_{n})}]\)_

Assumption 6.3 is necessary for finite-sample consistency when estimating \(_{n}\) via the Lasso estimator (3), and is commonly made when studying the Lasso . Incoherence can also seen as a inclusion criteria for a unit \(n\) to be included in the donor set \(\). Assumption 6.3 for example holds (with high probability) if \(_{u}\) is chosen uniformly at random and grows as \((s^{2}p)\) as shown in Lemma 2 of . Assumption 6.4 requires that the non-zero singular values of \([_{}^{(_{n})}]\) are well-balanced. This assumption is standard when studying PCR [3; 5], and within the econometrics literature [10; 22]. It can also be empirically validated by plotting the spectrum of \(}[_{}^{(_{n})}]\); if the singular spectrum of \(}[_{}^{(_{n})}]\) displays a natural elbow point, then Assumption 6.4 is likely. Assumption 6.5 is also commonly made when analyzing PCR [4; 5; 2]. It can be thought of as a "causal transportability" condition from the model learnt using \(_{n}\) to the interventions \(_{n}\). That is, subspace inclusion allows us to generalize well, and accurately estimate \([Y_{n}^{()}]\) using \(}[_{}^{(_{n})}], }^{n}\).

### Finite Sample Consistency

The following result establishes finite-sample consistency of Synthetic Combinations. Without loss of generality, we will focus on estimating the pair of quantities \(([Y_{u}^{()}],[Y_{n}^{()}])\) for a given donor unit \(u\), and non-donor unit \(n[N]\) under treatment assignment \(\). To simplify notation, we will use \(O_{p}\) notation: for any sequence of random vectors \(X_{n}\), \(X_{n}\) = \(O_{p}(_{n})\) if, for any \(>0\), there exists constants \(c_{}\) and \(n_{}\) such that \((\|X_{n}\|_{2} c_{}_{n})\) for every \(n n_{}\). Similarly, we define \(}(_{n})\) which suppresses logarithmic terms. We will further absorb dependencies on \(\) into \(}()\).

**Theorem 6.6** (Finite Sample Consistency of Synthetic Combinations).: _Conditioned on \(\), let Assumptions 3.1-4.1, and 6.1-6.5 hold. Then, the following statements hold._1. _For the given donor unit-combination pair_ \((u,)\)_, let the Lasso regularization parameter satisfy_ \(_{u}\!=\!(|}})\)_. Then, we have that:_ \(|}[Y_{u}^{()}]\!-\![Y_{u}^{()}]|\!=\!_{p }\!(p}{|_{u}|}})\)_._
2. _For the given unit-combination pair_ \((n,)\) _where_ \(n\!\![N]\!\)_, let_ \(\!=\!([_{}^{(_{u})}])\!:=\!r _{n}\)_. Then, provided that_ \(_{u}\!|_{u}|\!:=\!M\!=\!(r_{n}^{2}s^{2}p)\)_, we have that:_ \[|}[Y_{n}^{()}]\!-\![Y_{n}^{()}]|\!= \!_{p}\!(^{2}p}}{|,||\}}}\!+\!^{2}s^{2}p|}}{M}\! +\!}{|_{n}|^{1/4}})\]

Establishing Theorem 6.6 requires a novel analysis of error-in-variables (EIV) linear regression. Specifically, the general EIV linear model is as follows: \(Y=+\), \(=+\), where \(Y\) and \(\) are observed. In our case, \(Y=_{_{n}}\), \(=[_{}^{(_{n})}]\), \(=^{n}\), \(=}[_{}^{(_{n})}]\), and \(\) is the error arising in estimating \([_{}^{(_{n})}]\) via the Lasso. Typically one assumes that \(\) is is a matrix of independent sub-gaussian noise. Our analysis requires a novel worst-case analysis of \(\) (due to the 2-step regression of Lasso and then PCR), in which each entry of is \(\) simply bounded.

Next, we describe the conditions placed on \(_{n}\), \(M\), \(\) to achieve consistent estimation of \(([Y_{u}^{()}],\![Y_{n}^{()}])\). To simplify the discussion, we assume that \(\{|_{n}|,||\}\!=\!||\). This condition can be enforced in practice by simply picking a subset of donor units such that \(|||_{n}|\) when performing PCR (i.e., step 2 of Synthetic Combinations). Given this assumption, it can be verified that \(|_{n}|\) and \(M\) need to scale as \((r_{n}^{4})\) and \((r_{n}^{4}s^{2}p)\) respectively to achieve \(\!(|}[Y_{u}^{()}]\!-\![Y_{u}^{()}]|, |}[Y_{n}^{()}]\!-\![Y_{n}^{()}]|)\!=\! _{p}(1)\). Next, we present a corollary that discusses how quickly the parameters \(r_{n}\), \(s\), and \(p\) can grow with the number of observations \(|_{n}|\) and \(M\).

**Corollary 6.7**.: _With the set-up of Theorem 6.6, if (a) \(r_{n}=o(|_{n}|^{1/4})\) and (b) \(s=o\!(|,||\})})\) then \(\!(|}[Y_{u}^{()}]\!-\![Y_{u}^{()}]|,| }[Y_{n}^{()}]\!-\![Y_{n}^{()}]|)= _{p}(1)\) as \(M\),\(|_{n}|\),\(||\!\!\)._

Corollary 6.7 quantifies how \(s\),\(r_{n}\) can scale with the number of observations to achieve consistency. That is, Corollary 6.7 reflects the maximum "complexity" allowed for a given sample size.

### Sample Complexity

We discuss the sample complexity of Synthetic Combinations to estimate all \(N\!\!2^{p}\) causal parameters, and compare it to that of other methods. To ease our discussion, we will ignore dependence on logarithmic factors and \(\). Even if potential outcomes \(Y_{n}^{()}\) were observed for all unit-combination pairs, consistently estimating \([_{N}^{()}]\) is not trivial. This is because, we only get to observe a single and noisy version \(Y_{n}\!=\!(_{n},\!^{})\!+\!_{n}^{}\). Hypothetically, if we observe \(K\) independent samples of \(Y_{n}\) for a given \((n,\!)\), denoted by \(Y_{n}^{1}\),...,\(Y_{n}^{K}\), the maximum likelihood estimator would be the empirical average \(_{i=1}^{K}Y_{n}^{i}\). The empirical average would concentrate around \([Y_{n}^{()}]\) at a rate \(O(1/)\) and hence would require \(K\!=\!(^{-2})\) samples to estimate \([Y_{n}^{()}]\) within error \(O()\). Therefore, this naive (unimplementable) solution would require \(N\!\!2^{p}\!\!^{-2}\) observations to estimate \([_{N}^{()}]\).

On the other hand, Synthetic Combinations produces consistent estimates of the potential outcome despite being given _at most only a single noisy sample_ of each potential outcome. As the discussion after Theorem 6.6 shows, if \(||\!\!|_{n}|\), Synthetic Combinations requires \(||\!\!r^{4}s^{2}p/^{2}\) observations for the donor set, and \((N-||)\!\!r^{4}/^{4}\) observations for the non-donor units to achieve an estimation error of \(O()\) for all \(N\!\!2^{p}\) causal parameters. Hence, we have that the number of observations required to achieve an estimation error of \(O()\) for all pairs \((n,\!)\) scales as \(O\!((r/)(N\!+\!s^{2}p))\).

**Sample Complexity Comparison to Other Methods.**_Horizontal regression:_ An alternative algorithm would be to learn an individual model for each unit \(n[N]\). That is, run a separate horizontal regression via the Lasso for every unit. This alternative algorithm has sample complexity that scales at least as \(O(N\!\!s^{2}p/^{2})\) rather than \(O\!((r)/^{4}\!\!(N\!+\!s^{2}p))\) required by Synthetic Combinations. It suffers because it does not utilize any structure across units (i.e., the low-rank property of \(\)), whereas Synthetic Combinations captures the similarity between units via PCR.

_Matrix completion:_ Synthetic Combinations can be thought of as a matrix completion method; estimating \([Y_{n}^{()}]\) is equivalent to imputing \((n,\!)\)-th entry of the observation matrix \(\!\!\{\}^{N 2^{p}}\), where recall \(\) denotes an unobserved unit-combination outcome. Under the low-rank property (Assumption 3.1(b)) and various models of missingness (i.e., observation patterns), recent works on matrix completion  (see Section 2 for an overview) have established that estimating \([Y_{n}^{()}]\) to an accuracy \(O()\) requires at least \(O((r/)\!\!(N\!+\!2^{p}))\) samples. This is because matrix completion techniques do not leverage the sparsity of \(_{n}\). Moreover, matrix completion results typically report error in the Frobenius norm, whereas we give entry-wise guarantees. This leads to an extra factor of \(s\) in our analysis as it requires proving convergence for \(\|_{n}}-_{n}\|_{1}\) rather than \(\|_{n}}-_{n}\|_{2}\). Hence, Synthetic Combinations combines the best of both approaches by leveraging the structure of the potential outcomes _and_ the similarity across units.

**Natural Lower Bound on Sample Complexity.** We provide an informal discussion on the lower bound sample-complexity to estimate all \(N 2^{p}\) potential outcomes. As established in Lemma E.1, \(\) has at most \(rs\) non-zero columns. Counting the parameters in the singular value decomposition of \(\), only \(r\!\!(N\!+\!rs)\) free parameters are required to be estimated. Hence, a natural lower bound on the sample complexity scales as \(O(Nr\!+\!r^{2}s)\). Hence, Synthetic Combinations is only sub-optimal by a factor (ignoring logarithmic factors) of \(sp\) and \((r)\). As discussed earlier, an additional factor of \(s\) can be removed if we focus on deriving Frobenius norm error bounds. It remains as interesting future work to derive estimation procedures that are able to achieve this lower bound.

## 7 Conclusion

In this work, we formulate a causal inference framework for combinatorial interventions, a setting that is ubiquitous in practice. We propose a model that imposes both unit-specific structure, and latent similarity across units. Under this model, we propose an estimation procedure, Synthetic Combinations, that exploits the sparsity and low-rankness of the Fourier coefficients to efficiently estimate all \(N\!\!2^{p}\) causal parameters. We formally establish finite-sample consistency of Synthetic Combinations in an observational setting. Our work also naturally suggests future directions for research such as extending Synthetic Combinations to permutations over items (i.e., rankings), or providing an analysis of Synthetic Combinations that is agnostic to the horizontal regression algorithm used. A related line of work to the question above is also deriving estimation algorithms that can achieve the sample complexity lower bound discussed in Section 6.3.

## 8 Acknowledgements

We thank Alberto Abadie, Peng Ding, Giles Hooker, Devavrat Shah, Vasilis Syrgkanis, and Bin Yu for useful discussions and feedback.