# One Sample Fits All: Approximating All Probabilistic Values Simultaneously and Efficiently

Weida Li

School of Computing

National University of Singapore

vidaslee@gmail.com &Yaoliang Yu

School of Computer Science

University of Waterloo

Vector Institute

yaoliang.yu@uwaterloo.ca

###### Abstract

The concept of probabilistic values, such as Beta Shapley values and weighted Banzhaf values, has gained recent attention in applications like feature attribution and data valuation. However, exact computation of these values is often exponentially expensive, necessitating approximation techniques. Prior research has shown that the choice of probabilistic values significantly impacts downstream performance, with no universally superior option. Consequently, one may have to approximate multiple candidates and select the best-performing one. Although there have been many efforts to develop efficient estimators, none are intended to approximate all probabilistic values both simultaneously and efficiently. In this work, we embark on the first exploration of achieving this goal. Adhering to the principle of maximum sample reuse and avoiding amplifying factors, we propose a one-sample-fits-all framework parameterized by a sampling vector to approximate intermediate terms that can be converted to any probabilistic value. Leveraging the concept of \((,)\)-approximation, we theoretically identify a key formula that effectively determines the convergence rate of our framework. By optimizing the sampling vector using this formula, we obtain i) a one-for-all estimator that achieves the currently best time complexity for all probabilistic values on average, and ii) a faster generic estimator with the sampling vector optimally tuned for each probabilistic value. Particularly, our one-for-all estimator achieves the fastest convergence rate on Beta Shapley values, including the well-known Shapley value, both theoretically and empirically. Finally, we establish a connection between probabilistic values and the least square regression used in (regularized) datamodels, showing that our one-for-all estimator can solve a family of datamodels simultaneously. Our code is available at https://github.com/watml/one-for-all.

## 1 Introduction

The problem of attribution is central in many aspects of machine learning (Rozemberczki et al., 2022). Examples include data valuation (Ghorbani and Zou, 2019), feature attribution (Lundberg and Lee, 2017), multi-agent reinforcement learning (Wang et al., 2022), data attribution (Ilyas et al., 2022), and the list goes on. One popular methodology is to leverage the concept of probabilistic values, which is uniquely characterized by the axioms of linearity, null, monotonicity and symmetry in cooperative game theory (Weber, 1988). Recent studies demonstrate that downstream performance employing this concept relies on the choice of probabilistic values, and the best one varies (Kwon and Zou, 2022; Li and Yu, 2023). Therefore, practitioners may resort to approximating multiple candidates of probabilistic values and then select the best-performing one (Kwon and Zou, 2022; Kwon and Zou, 2022).

In general, probabilistic values can only be approximated as they require exponentially many utility evaluations to compute exactly. Thus far, there has been a line of works devoted to designing efficient estimators for the Shapley value (e.g., Covert and Lee 2021; Jia et al. 2019; Kolpaczki et al. 2024; Zhang et al. 2023), while Li and Yu (2023) and Wang and Jia (2023b) proposed efficient estimators specific to weighted Banzhaf values. Despite recent progress in research on generic estimators for approximating any probabilistic value (Li and Yu 2024; Lin et al. 2022), none of them can approximate all probabilistic values _simultaneously and efficiently_. Overall, there is a strong demand for efficient one-for-all estimators, the possibilities of which will be explored in this work.

To sum up, we propose a **O**ne-sample-**F**its-**All (OFA) framework parameterized by a sampling vector to approximate intermediate terms that can be converted to any probabilistic value. Particularly, our framework i) adheres to the principle of maximum sample reuse and ii) does not include amplifying factors in the conversion. These two properties are considered indispensable as we observe that i) the empirical fastest estimators designed for the Shapley value or weighted Banzhaf values all follow she principle of maximum sample reuse and ii) amplifying factors could deteriorate the convergence rates of estimators. Then, using the concept of \((,)\)-approximation, i.e., \(P(\|}-\|_{2})\) where \(\) refers to some probabilistic value and \(}\) is its estimate, we theoretically identify a formula from our framework that effectively determines the corresponding convergence rate, through which the sampling vector can be optimized. Specifically, we deduce i) an efficient one-for-all estimator (OFA-A) while optimizing the formula for all probabilistic values on **A**verage and ii) a faster generic estimator (OFA-S) while the optimization is done for each **S**pecific probabilistic value. The results of our convergence analysis are summarized as follows:

1. Our OFA-A achieves the convergence rate \(O(n n)\) for all probabilistic values on average. Notably, \(O(n n)\) is the currently-known best time complexity for _some_ probabilistic values.
2. For Beta Shapley values parameterized by \(, 1\)(Kwon and Zou 2022a), our OFA-A estimator requires \(O(n n)\) utility evaluations to achieve an \((,)\)-approximation _simultaneously_. Note that \(==1\) corresponds to the commonly-used Shapley value (Shapley 1953). For the Shapley value, the previous best convergence rate is \(O(n( n)^{2})\),1 achieved by the group testing estimator (Wang and Jia 2023a, Theorem 6); however, we note that in our experiments the previous best-performing estimator is the complement estimator (Zhang et al. 2023), whose convergence rate is unknown. For Beta Shapley values with \((=1,>1)\) or \((>1,=1)\), the previous best estimator requires \(O(n( n)^{3})\) utility evaluations instead (Li and Yu 2024, Proposition 4 and Remark 3). 3. For weighted Banzhaf values parameterized by \(0<w<1\), the time complexity of our OFA-A is \(O(n^{} n)\), not rivaling the previous best convergence rate \(O(n n)\) achieved by the estimator exclusive to weighted Banzhaf values (Li and Yu 2023, Proposition 2). Nevertheless, our OFA-S achieves the convergence rate of \(O(n n)\) for both Beta Shapley values (with \(, 1\)) and weighted Banzhaf values.

In our experiments, the empirical convergence rates align well with the theoretical ones derived using the concept of \((,)\)-approximation. Additionally, we establish a connection between probabilistic values and the least square regressions employed in datamodels (Ilyas et al. 2022), demonstrating that our OFA-A estimator can solve a family of datamodels simultaneously if it is the distances between feature coordinates that matter. This condition is met while using datamodels to detect similar training examples to a given target. Furthermore, we also identify a group of regularized datamodels that our OFA-A estimator can solve simultaneously without this condition.

## 2 Preliminaries

Let \(n\) be the number of players and \([n]:=\{1,2,,n\}\) be the set of all players. In data valuation (feature attribution, respectively), \(n\) refers to the number of training data (features, respectively). For simplicity, we write \(S i\) and \(S i\) instead of \(S\{i\}\) and \(S\{i\}\), respectively. Meanwhile, (lowercase) \(s\) denotes the cardinality of the set (uppercase) \(S\). Then, each probabilistic value can be written as

\[_{i}=_{i}(U)=_{S[n] i}p_{s+1}[U(S i)-U(S)]\]where \(U:2^{[n]}\) is a utility function and \(^{n}\) is a non-negative vector such that \(_{s=1}^{n}p_{s}=1\). Take data valuation as an example, \(U(S)\) may measure the performance of models trained on \(S[n]\), with which \(_{i}(U)\) can be interpreted as the contribution of the \(i\)-th data point to the performance of models trained on \([n]\).

If there exists a (Borel) probability measure \(\) on the closed interval \(\) such that \(p_{s}=_{0}^{1}w^{s-1}(1-w)^{n-s}(w)\), then the resulting probabilistic value is referred to as a semi-value (Dubey et al., 1981). If \(\) represents a Dirac delta distribution \(_{a}\), the corresponding probabilistic value is referred to as the weighted Banzhaf value parameterized by \(a\), or WB-\(a\). For Beta Shapley values, denoted by \((,)\), \((A)=_{A}w^{-1}(1-w)^{-1}w\). In practice, the considered range of \(\) or \(\) is \([1,)\)(Kwon and Zou, 2022, 2020). Particularly, \((1,1)\), whose \(\) is the uniform distribution (over \(\)), corresponds to the Shapley value.

We will use the standard notion of \((,)\)-approximation to analyze a (randomized) estimate \(}\) of some probabilistic value \(\).

**Definition 1**.: _We say a (randomized) estimate \(}\) achieves an \((,)\)-approximation of \(\) if \(P(\|}-\|_{2})\)._

For instance, Wang and Jia (2023, Theorem 4.9) proved that their proposed estimator requires \(O()\) utility evaluations to achieve an \((,)\)-approximation for WB-\(0.5\), provided that \(\|U\|_{} 1\). When \(\) and \(\) are considered fixed constants, we then simply say the estimator converges at \(O(n n)\) rate.

## 3 Motivations

One-For-All EstimatorsIn this paper, an estimator is referred to as one-for-all if it is capable of sampling subsets **O**nce to approximate **A**ll probabilistic values.

Though existing estimators are not designed to approximate all probabilistic values simultaneously, some of them can be easily modified for this end by using the weighted sampling technique. Take the sampling lift (SL) estimator (Moehle et al., 2022) as an example, its approximation is based on

\[_{i}=_{S[n] i}[U(S i)-U(S)]P(S)=p_{s+1}.\]

If we fix the probability of sampling \(S\) to be the one, denoted by \(^{n}\), for the Shapley value, there is

\[_{i}=_{S[n] i}^{}[}{q_{s+1}}(U(S i)-U(S))],\] (1)

which is the weighted sampling lift (WSL) estimator employed by Kwon and Zou (2022). Therefore, we can store the accumulated results \(\{U(S i)-U(S)\}\) separately for each subset size of \(S\) so that they can be reweighted to be any probabilistic value.

The Effect of Amplifying FactorsHowever, the scalars \(\{}{q_{s+1}}\}\) potentially introduce a non-negligible factor into the theoretical convergence rate. To demonstrate, we take the WSL estimator as an example. In this case, \(_{i}=_{t=1}^{T}X_{t}\) where \(\{X_{t}\}_{t=1}^{T}\) are i.i.d. random variable such that \(P(X_{t}=}{q_{s+1}}(U(S i)-U(S)))=q_{s+1}\) and thus \([X_{t}]=_{i}\). Assume that \(\|U\|_{} 1\), by the Hoeffding inequality, \(P(|_{i}-_{i}|) 2(-}{8C^{2}})\) where \(C=_{1 k n}}{q_{k}}\). By solving \(2(-}{8C^{2}})\), we eventually obtain \(T}{^{2}}\) and therefore the convergence rate of \(_{i}\) is \(O(}{^{2}})\). Consequently, if \(C\) as \(n\), this theoretical convergence rate deteriorates asymptotically. For the Banzhaf value, \(p_{k}=}\); since \(q_{k}=\), if \(k=\), there is \(}{q_{k}}(n^{})\) by the Stirling's approximation \(d!()^{d}\). Therefore, \(C^{2}\) introduces a factor of \((n)\) into the theoretical convergence rate, though the derived formula may not be tight. If we switch the roles of \(\) and \(\), the introduced factor \(C^{2}\) is as worst as \((2^{2n})\). To generalize this idea, a formula is said to contain an amplifying factor if it involves \( U(S)\) such that \(\) as \(n\).

Regarding this, we notice that Kwon and Zou (2022) resort to a one-for-all estimator based on

\[_{i}=_{s=1}^{n}m_{s}_{R[n]  i\\ r=s-1}[U(R i)-U(R)]\] (2)

where \(m_{s}=p_{s}\) and each expectation is taken over the corresponding uniform distribution. We refer to this estimator as weightedSHAP in this work. As can be verified, Eq. (2) does not contain any amplifying factors, i.e., each \(m_{s}\) does not grow as \(n\).

The Principle of Maximum Sample ReuseHowever, estimators designed according to Eqs. (1) and (2) are not expected to be efficient as it does not obey the principle of maximum sample reuse. Precisely, an estimator adheres to the principle of maximum sample reuse if each sampled subset is used to update all estimates \(\{_{i}\}_{i[n]}\). As analyzed by Zhang et al. (2023, Section 4.2), estimators based on sampled marginal contributions \(\{U(S i)-U(S)\}\) are impossible to meet the principle of maximum sample reuse. By contrast, we observe that the SHAP-IQ estimator proposed by Fumagalli et al. (2024) can also be adopted for this end, which employs the formula

\[_{i}=p_{n}(U([n])-U())+2H_{ S [n]}[((n-s)m_{s}_{i S}-sm_{s+1}_{i S})(U(S)-U( ))]\] (3)

where \(m_{s}=p_{s}\), \(H=_{j=1}^{n-1}\), and \(P(S)^{-1}\). In particular, SHAP-IQ is equal to the unbiased KernelSHAP estimator (Covert and Lee, 2021) for the Shapley value; see (Fumagalli et al., 2024, Theorem 4.5). Although SHAP-IQ follows the principle of maximum sample reuse, it is apparent that Eq. (3) contains an amplifying factor \(H( n)\). Meanwhile, there is another line of research in quest of efficient estimators for the Shapley value by reducing the variance via the stratified sampling technique (Burgess and Chapman, 2021; Castro et al., 2017; Maleki et al., 2013; Wu et al., 2023). However, such a technique also does not verify the principle of maximum sample reuse.

Empirical EvidenceFor convenience, we formally define the two aforementioned desirable properties for estimators to possess as **P1:** The underlying formula contains no amplifying factors and **P2:** Each sampled subset is used to update all the estimates \(\{_{i}\}_{i=1}^{n}\). In Figure 1, we provide some experiment results while setting \(n=24\) to support our aforementioned informal analysis. Precisely, we implement six one-for-all estimators by combining the weighted sampling technique and the previous estimators. Some of our observations are:

1. On WB-0.5, weightedSHAP, which satisfies **P1** but not **P2**, is empirically not comparable to MSR-Banzhaf that possesses both **P1** and **P2**. This observation supports the necessity of **P2**.
2. On WB-0.5, SHAP-IQ sticks to **P2** but not **P1**. It is clear that SHAP-IQ also performs significantly worse than MSR-Banzhaf, which highlights the role of **P1**.
3. The sudden rises of relative differences stem from the existence of significantly large amplifying factors. For WSL-Banzhaf on \((1,1)\), the scalar for \(U(i)-U()\) is as large as \(}{n}\)!

In Table 1, we summarize the previous estimators in terms of **P1** and **P2**, and defer the technical details to Appendix D. Notably, the complement estimator is empirically the best for the Shapley value, while it is MSR for weighted Banzhaf values; both of them follow **P1** and **P2**.

   & WSL & SL & GLLS & ARM & MSR & SHAP-IQ & weightedSHAP \\ (Krou and Lai, 2022) & (Aglas et al., 2021) & (Aglas et al., 2021) & (Aglas et al., 2021) & (Aglas et al., 2021) & Winter and Rao (2021) & (Fumagalli et al., 2021) & (Krou and Lai, 2022) \\   & All & Study & All & all & weighted Bandzhaf & all \\  & **P1** & ✗ & ✗ & ✗ & ✓ & ✗ & ✓ \\  & **P2** & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\   & permutations & BertSLAM & unlocked & & & & & \\  & (Caiton et al., 2009) & (Lundberg and Lee, 2017) & (Covert and Lee, 2021) & (Wang and Liu, 2021) & (Zhang et al., 2020) & (Lin et al., 2020) & (OFA (ours)) \\  & Sparsity & Sparsity & Sparsity & Sparsity & Sparsity & Sparsity & Sparsity & semi-volaton & all \\  & **P1** & ✗ & ✗ & ✗ & ✓ & ✗ & ✓ \\  

Table 1: A scope of “all” indicates that the estimator can approximate any probabilistic value, whereas “weighted Banzhaf” suggests that the estimator can only approximate weighted Banzhaf values. **P1** refers to the property that the underlying formula does not contain any amplifying factors _for all probabilistic values in its scope_, while **P2** means whether each sampled subset is used to update all the estimates \(\{_{i}\}_{i=1}^{n}\). For AME, the range of \(\) in \( U(S)\) could be \((0,)\), independent of \(n\). Originally, AME only applies to a subfamily of semi-values, but we extend it for all semi-values in Appendix D.

## 4 Main Results

The framework we propose is built upon

\[_{i}=_{s=1}^{n}m_{s}(}_{i R}[U(R)]-}_{i R\\ r=s-1}[U(R)])\] (4)

where \(m_{s}=p_{s}\) and each expectation is taken over the corresponding uniform distribution. For simplicity, we write \(_{i,s}^{+}=_{i R,r=s}[U(R)]\) and \(_{i,s-1}^{-}=_{i R,r=s-1}[U(R)]\). Clearly, there is no amplifying factors in Eq. (4). Meanwhile, the structure of Eq. (4) suits the principle of maximum sample reuse. Since \(\{_{i,k}^{+}\}_{k=1,n-1,n}\) and \(\{_{i,k}^{-}\}_{k=0,1,n-1}\) can be calculated exactly using \(2n+2\) utility evaluations of \(U\), our focus is to efficiently approximate \(\{_{i,s}^{+},_{i,s}^{-}\}_{2 s n-2}\). The proposed framework is presented in Algorithm 1; \(q_{j}\) is the probability of drawing a subset of \([n]\) with size \(j+1\). To facilitate the choice of the sampling vector \(^{n-3}\) appearing in Algorithm 1, our first step is to theoretically ascertain a key formula that effectively determines the convergence rate of Algorithm 1.

**Theorem 1**.: _Assume i) \(\|U\|_{} u\) and ii) \(0<,)()^{2}u^{2}}\). For \(}\) in Algorithm 1, it requires \(D(,)}{^{2}}}{}\) evaluations of \(U\) to achieve \(P(\|}-\|_{2})\) where_

\[D(,)=_{s=2}^{n-2}}(^{ 2}}{s}+^{2}}{n-s})()=_{2 s  n-2}( s}{n},\ (n-s)}{n}).\]

We remark that \(D(,)\) is jointly convex in \(\) and \(\). The second assumption in Theorem 1 can be removed if we pre-allocate the number of sampled subsets for each \(_{i,s}^{+}\) or \(_{i,s}^{-}\) and draw subsets in a predetermined order; see the proof in Appendix A for details. Precisely, let \(T_{i,s}^{+}\) be the number of subsets for estimating \(_{i,s}^{+}\), and define \(T_{i,s}^{-}\) similarly; then the pre-allocated numbers are \(T_{i,s}^{+}}{n}T\) and \(T_{i,s}^{-}}{n}T\), which are the expected values of \(T_{i,s}^{+}\) and \(T_{i,s}^{+}\) while using Algorithm 1; \(T\) refers to the total number of sampled subsets. By Theorem 1, the convergence rate of Algorithm 1 is \(O(D(,) n n)\), and thus achieving the currently best convergence rate \(O(n n)\) requires \(D(,) O(1)\).

Figure 1: Comparison of ten one-for-all estimators. \((,)\) denotes Beta Shapley values, whereas WB-\(a\) refers to weighted Banzhaf values. Our OFA-S estimator is equal to the OFA-A estimator for the Shapley value. The suffix “Shapley” indicates that there is no reweighting for the Shapley value, while “Banzhaf” stands for the Banzhaf value. The permutation estimator is originally proposed for the Shapley value. The utility function \(U\) is the cross-entropy loss of LeNet trained on \(24\) data from FMNIST. All the results are averaged using \(30\) random seeds.

### A One-For-All Estimator

To obtain our one-for-all estimator, our goal is to find a \(^{}\) such that \(D(,^{}) O(1)\) for as many \(\) as possible. To this end, we define \(^{}\) to be the uniquely optimal solution to

\[*{argmin}_{^{n-3}}() =_{}D(,)()\]

where \(=\{^{n} m_{s} 0_{s=1}^{n}m_{s}=1\}\) and \(\) is the uniform distribution on \(\). In our work, our OFA-A estimator refers to the use of \(^{}\) in Algorithm 1.

**Proposition 1**.: \(^{}_{s-1}}\) _and \((^{}) O(1)\). In other words, our OFA-A estimator achieves the convergence rate of \(O(n n)\) simultaneously for all probabilistic values on average._

Our next proposition provides a condition on \(\) for semi-values such that our OFA-A estimator achieves the convergence rate of \(O(n n)\). In other words, we explicitly identify a subfamily of semi-values for which our OFA-A estimator achieves the currently best time complexity simultaneously.

**Proposition 2**.: _Our OFA-A estimator achieves the convergence rate of \(O(n n)\) simultaneously for all semi-values whose probability density functions exist and are bounded. Particularly, Beta Shapley values with \(, 1\) all satisfy this condition._

To our knowledge, the previous theoretically-fastest estimator for the Shapley value is demonstrated by Wang and Jia (2023a, Theorem 6) as \(O(n( n)^{2})\). By contrast, our OFA-A estimator achieves the convergence rate of \(O(n n)\). Meanwhile, it also surpasses the previous best time complexity for Beta Shapley values with \((=1,>1)\) or \((>1,=1)\), which is \(O(n( n)^{3})\)(Li and Yu 2024, Proposition 4 and Remark 3). Remarkably, our OFA-A estimator enjoys this fastest convergence rate _simultaneously for a broad subfamily of probabilistic values_.

**Proposition 3**.: _If \(p_{s}=a^{s-1}(1-a)^{n-s}\) with \(0<a<1\), which corresponds to the weighted Banzhaf value parameterized by \(w\), then \(D(,^{}) O(n^{})\). In other words, the OFA estimator achieves the convergence rate of \(O(n^{} n)\) simultaneously for all WB-\(a\) with \(0<a<1\)._

The previous best convergence rate for weighted Banzhaf values is \(O(n n)\)(Li and Yu 2023, Proposition 2), ours is slower by a factor of \(n^{}\). Nevertheless, we will demonstrate that our generic estimator, which is expected to be faster than our OFA-A estimator, achieves the best convergence rate for all weighted Banzhaf values.

### A Faster Generic Estimator

Our faster generic estimator (OFA-S) is obtained via optimizing \(\) for each \(\)pecific \(\). Precisely, for each \(\), we have

\[_{s-1}^{}^{2}}{s}+^{ 2}}{n-s}}\;\;\;\;^{}=*{argmin}_{ ^{n-3}}D(,)\;\;\;\;_{j =1}^{n-3}q_{j}=1,\] (5)

which can be obtained using the Cauchy-Schwarz inequality. For the Shapley value, \(^{}=^{}\). Our next proposition specifies a sufficient condition for semi-values such that \(D(,^{}) O(1)\).

**Proposition 4**.: _For semi-values, \(D(,^{}) O(1)\) if i) \(\) has a bounded probability density function or ii) \(_{(0,1)}(w)<\). Particularly, this condition covers all weighted Banzhaf values and Beta Shapley values with \(, 1\)._

All in all, we demonstrate that by sticking to the principle of maximum sample reuse and avoiding any amplifying factors, we are able to establish a generic estimator that achieves the currently best convergence rate for any previously-studied semi-value.

### A Connection between Probabilistic Values and Datamodels

A datamodel, proposed by Ilyas et al. (2022), is to learn an easy-to-interpret surrogate to represent a model output distribution related to a specific test example. In this circumstance, the set of players \([n]\) is identified with all the available training data. Precisely, the feature coordinates \(^{*}^{n}\) imputed to every data point in \([n]\) is defined to be the uniquely optimal solution (together with a bias \(b^{*}\)) to the optimization problem

\[*{argmin}_{^{n},b}_{S [n]}_{s+1}(U(S)-b-_{i S}_{i})^{2}\] (6)

where \(^{n+1}\) is non-negative and \(_{s=0}^{n}_{s+1}>0\). The weight vector \(\) can be scaled such that the objective in the problem (6) can be treated as an expectation, and thus the objective can be approximated through sampling a sufficient number of subsets, upon which an estimate of \((^{*},b^{*})\) can be obtained. We show below that \(^{*}\) to a family of such least square regressions can be cast as some probabilistic values if it is the pairwise differences \(_{j}^{*}-_{k}^{*}\) (for every \(j,k[n]\)) that matter.

**Theorem 2**.: _Let \((b^{*},^{*})\) be the uniquely optimal solution to the problem (6) where \(_{s}=p_{s-1}+p_{s}\) for \(2 s n\). Then, there is_

\[_{j}^{*}-_{k}^{*}=_{j}-_{k}\;\;\;\;j,k[n].\]

In other words, \(^{*}=+c\) for some constant \(c\); \(^{n}\) is the all-one vector. When using datamodels to detect similar training examples to a given target, what matters is the relative order of components in \(^{*}\). Meanwhile, Ilyas et al. (2022) showed that the corresponding performance depends on the choice of the weight vector \(\). Therefore, our OFA-A estimator serves as a sufficient proxy for a range of \(\{^{*}\}\) and would facilitate the fine-tuning of \(\) when using datamodels to detect similar training examples.

When \(^{*}\) Can Be Recovered From \(\)Interestingly, for specific choices of \(^{n}\) and \(^{n+1}\), it holds that \(=\). Theorem 2 can be seen as an extension to the previous result stated in the below.

**Proposition 5** (Marichal and Mathonet 2011, Proposition 4).: _Suppose \(0<a<1\) is given, if \(p_{j}=a^{j-1}(1-a)^{n-j}\) for \(1 j n\) and \(_{k}=a^{k-2}(1-a)^{n-k}\) for \(1 k n+1\), which leads to \(_{s}=p_{s-1}+p_{s}\) for \(2 s n\), there is_

\[^{*}=.\]

It is worth pointing out that \(\) in Proposition 5 is exactly the weighted Banzhaf value parameterized by \(a\), i.e., WB-\(a\). Even more, under the same setting, we can even solve a group of datamodels with \(_{1}\) or \(_{2}\) regularization simultaneously.

**Corollary 1**.: _Under the setting of Proposition 5, let \(^{*}\) be the uniquely optimal solution to_

\[*{argmin}_{^{n},b}(_{S [n]}_{s+1}(U(S)-b-_{i S}_{i})^{2})+ ()\] (7)

_where \(>0\), the following are true about the relation between \(^{*}\) and \(\):_

1. _If_ \(()=\|\|_{2}^{2}\)_, then_ \[^{*}=(1+)^{-1}.\]
2. _If_ \(()=\|\|_{1}\)_, then_ \[^{*}=*{sign}()(0,||- ).\]

_All operations are element-wise._

This corollary is immediate by combining Proposition 5, and Theorem 2.2 by Saunshi et al. (2022). We comment that replacing \(x_{i}\) by \(2x_{i}-1\), i.e., mapping \(0\) and \(1\) into \(-1\) and \(1\), respectively, in \(_{\{i\}}(x)\) used by Saunshi et al. (2022) yields \(v_{\{i\}}(x)\) used by Marichal and Mathonet (2011). A remarkable implication of the combination of Corollary 1 and our proposed OFA-A estimator is that we can solve a group of regularized datamodels covered by the problem (7) _simultaneously_! For example, the coefficient \(\) can be finetuned by running Algorithm 1 just once.

## 5 Experiments

In this section, we are to verify i) the simultaneous efficiency of our OFA-A estimator and ii) the faster convergence rate of our OFA-S estimator compared with the considered baselines and our OFA-A estimator. Particularly, if \(D(,)\) is effective in determining the convergence rate of Algorithm 1, our OFA-S estimator is expected to be faster than our OFA-A estimator. All the experiments are conducted using CPUs.

We use two types of utility functions for this end: i) following the experiment settings of (Li and Yu 2024), \(U(S)\) is set to be the cross-entropy of LeNets trained on \(S\) on the classification datasets FMNIST, MNIST and iris; to obtain the exact values, the number of training data \(n\) is set to be \(24\); ii) \(U\) is defined to be the sum of unanimity (SOU) games, i.e., \(U(S)=_{j=1}^{d}_{j}_{S_{j} S}\) where each \( S_{j}[n]\) is randomly sampled, for which each semi-value can be computed by \(_{i}=_{j=1}^{d}_{j}_{}w^{s_{j}-1}(w)\); specifically, we set \(n\{64,128,256\}\) with \(d=n^{2}\), which implies that the implemented SOU games require \(n^{2}\) utility evaluations to compute semi-values exactly. The random seed inside each utility function is fixed as \(2024\), and thus each \(U\) is deterministic.

For the simplicity of presenting our empirical results, we use the area under the convergence curve (AUCC) to assess the convergence quality of estimators, and thus the smaller the better. For \(n=24\), the value of each player is approximated using \(20,000\) utility evaluations, and we compute the AUCCs as \(_{j=1}^{100}}^{(200j)}-\|_{2}}{ \|\|_{2}}\) where \(}^{(200j)}\) refers to the estimate using \(200j\) utility evaluations for each player. For \(n\{64,128,256\}\), the value of each player is approximated using \(2,000\) utility evaluations, and the corresponding AUCCs are calculated as \(_{j=1}^{100}}^{(20j)}-\|_{2}}{ \|\|_{2}}\). All the AUCCs are reported with standard deviation using \(30\) different random seeds from \(\{0,1,2,,29\}\).

Verification of Our OFA-A EstimatorFor our OFA-A estimator where we substitute \(^{}\), which is defined in Proposition 1, into Algorithm 1, we choose the baselines according to Figure 1. The selected baselines include WSL-Shapley (Kwon and Zou 2022), SHAP-IQ (Fumagalli et al. 2024), weightedSHAP (Kwon and Zou 2022) and permutation-Shapley (Castro et al. 2009). The corresponding results are reported in Figure 2. Overall, our OFA-A estimator performs the best on all the employed \(18\) probabilistic values, which verify the simultaneous efficiency of our OFA-A estimator.

Verification of Our OFA-S EstimatorNext, we verify the faster convergence rate of our OFA-S estimator, using \(^{}\) as defined in Eq. (5). The baselines we employ include: (unbiased) kernelSHAP (Covert and Lee 2021; Lundberg and Lee 2017), GELS and GELS-Shapley (Li and Yu 2024), ARM (Kolpaczki et al. 2024; Li and Yu 2024), complement (Zhang et al. 2023), group testing (Jia et al. 2019; Wang and Jia 2023a), AME (Lin et al. 2022), MSR (Wang and Jia 2023b) and sampling lift (Moehle et al. 2022). Note that not all the baselines are designed for all the probabilistic values we employ. For example, the complement estimator only works for \((1,1)\), i.e., the Shapley value. According to (Li and Yu 2024, Remark 9), we implement the paired sampling technique for (unbiased) kernelSHAP and group testing. The corresponding results are presented in Figure 3.

First, our OFA-S estimator is indeed faster than our OFA-A estimator, which aligns exactly with our theory; in other words, it implies that our proposed \(D(,)\) indeed determines the convergence rate of our Algorithm 1. Second, our OFA-S estimator always performs the best except on the SOU games which require only \(n^{2}\) utility evaluation to get the exact values; by contrast, the utility function defined using the classification datasets require \(2^{n}\) utility evaluations instead. Third, our proposed estimator is consistently the fastest on the commonly-used \((1,1)\), i.e., the Shapley value; note that \(^{}=^{}\) for the Shapley value; therefore, our proposed estimator achieves the currently best convergence rate both empirically and theoretically.

Figure 2: Comparison of one-for-all estimators using six utility functions. All the AUCCs are reported with standard deviation using \(30\) random seeds. Smaller AUCC indicates faster convergence rate.

## 6 Conclusion

In this work, we propose a framework, termed OFA, that i) adheres to the principle of maximum sample reuse and ii) contains no amplifying factors for the goal of optimizing all probabilistic values simultaneously and efficiently. Particularly, our OFA framework is parameterized by a sampling vector \(^{n-3}\). To gain insights, we theoretically develop a key formula \(D(,)\) concerning this framework that effectively determines the corresponding convergence rate. By optimizing \(\) in \(D(,)\) for all probabilistic values on average, we obtain our one-for-all estimator that can theoretically approximate all probabilistic values simultaneously with the currently best convergence rate \(O(n n)\) on average. Meanwhile, we propose a faster generic estimator by optimizing \(\) for each specific probabilistic value, and we demonstrate that our generic estimate enjoys the best convergence rate for all previously-studied probabilistic values. All of our theoretical findings are verified in our experiments. Finally, we establish a connection between probabilistic values and the least square regressions used in datamodels, showing that our OFA-A estimator is capable of solving a family of (regularized) datamodels simultaneously.

Figure 3: Comparison of twelve estimators using six utility functions. The dashed lines correspond to the improved AME estimator developed in Appendix D. All the AUCCs are reported with standard deviation using \(30\) random seeds. Smaller AUCC indicates faster convergence rate.