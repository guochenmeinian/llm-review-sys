# MoleCLUEs: Molecular Conformers Maximally

In-Distribution for Predictive Models

 Michael Maser

Prescient Design, Genentech

South San Francisco, CA

maserm@gene.com

&Natasa Tagasovska

Prescient Design, Genentech

South San Francisco, CA

natasa.tagasovska@roche.com

Jae Hyeon Lee

Prescient Design, Genentech

South San Francisco, CA

leej226@gene.com

&Andrew M. Watkins

Prescient Design, Genentech

South San Francisco, CA

watkina6@gene.com

###### Abstract

Structure-based molecular ML (SBML) models can be highly sensitive to input geometries and give predictions with large variance. We present an approach to mitigate the challenge of selecting conformations for such models by generating conformers that explicitly minimize predictive uncertainty. To achieve this, we compute estimates of aleatoric and epistemic uncertainties that are differentiable w.r.t. latent posteriors. We then iteratively sample new latents in the direction of lower uncertainty by gradient descent. As we train our predictive models jointly with a conformer decoder, the new latent embeddings can be mapped to their corresponding inputs, which we call _MoleCLUEs_, or (molecular) counterfactual latent uncertainty explanations (Antoran et al., 2021). We assess our algorithm for the task of predicting drug properties from 3D structure with maximum confidence. We additionally analyze the structure trajectories obtained from conformer optimizations, which provide insight into the sources of uncertainty in SBML.

## 1 Introduction

Machine learning (ML) approaches have shown great potential to accelerate drug discovery (Jayatunga et al., 2022; Kirkpatrick, 2022), resulting in a plethora of ML-based algorithms preempting traditional molecular design pipelines (Wu et al., 2022; Nori et al., 2022; Stark et al., 2022). Among the most promising methods are those that leverage 3D structure representations, since molecular function (particularly in biological settings) is directly dependent on atomic structure (Verma et al., 2010; Zheng et al., 2017). Specifically, structure representations are those containing 3D positions of the atoms, atom and bond types, and (optionally) torsion angles.

In live drug discovery programs, access to high-fidelity geometries is often severely limited due to experimental and resource challenges. For example, X-ray measurements, if obtainable, are both expensive and time-consuming and quantum-accurate simulations are prohibitively expensive in most settings (Rackers et al., 2022). With the advancements in 3D molecular-structure prediction (Fu et al., 2022; Somnath et al., 2021; Isert et al., 2023; Askr et al., 2022; Ganea et al., 2021) and less expensive quantum approximations (van der Kamp and Mulholland, 2013; Riniker and Landrum, 2015; Nakata et al., 2020), it is becoming increasingly common to train structure-based ML (SBML) models with _predicted_ geometries, resulting in the following procedure for property prediction of, e.g., drug candidates:* **Step 1.** Obtain 3D structures of training data, either via experiment or computational prediction (generating conformers, Appendix A);
* **Step 2.** Train a property predictor with the conformers obtained in Step 1 (subsection A.1);
* **Step 3.** At inference time, pass computed (predicted) conformers of new candidate(s) through the predictor from Step 2.

In practice, Steps 1 and 2 clearly impose problematic biases. Essentially, we assume that new conformers computed in Step 3 will belong to the same distribution of conformers as seen during training, which is difficult to guarantee or even measure. As such, SBML models, e.g., Euclidean neural networks (E3NNs) (Geiger & Smidt, 2022), are prone to poor generalization and often give predictions with very high uncertainties on heldout data (Maser et al., 2023). This is an even greater liability when new samples derive from different structure methods, since each data source may impart structural particularities to datasets, such as preferred bond lengths and angles.

The challenges above are severely problematic for high-risk settings such as ML-based drug discovery (MLDD). The goal of our work is thus to correct or adjust for model biases contributed by 3D structure generation. As a measurable endpoint, we aim at reducing the uncertainty in label predictions for out-of-distribution (OOD) input geometries. We herein present a fully differentiable algorithm to this end called _MoleCLUEs_, which relies on differentiable uncertainty estimators to guide the sampling of learned representations corresponding to novel, in-distribution (ID) conformers.

## 2 Approach & Methods

### MoleCLUEs - counterfactual conformers with reduced uncertainty

**Problem setup.** The main task we are concerned with is prediction of molecular properties, either binary classification or regression. We consider a dataset of tuples \(=(,y)_{i=1}^{N}\) where \(\) is a canonical representation of a small molecule and \(y\) is either binary label or a scalar value of a property of interest such as potency or binding affinity, toxicity, clearance, etc.

**Uncertainty sources in SBML predictive models.** The errors of a SBML model can be contributed to bias, variance, or noise. These three terms relate to either the epistemic (lack of knowledge or data) or aleatoric (inherent data noise) uncertainty Tagasovska & Lopez-Paz (2019). The former motivates the need of confidence intervals that relate to the plausible input space as seen during training, the latter motivates inclusion of predictive variance in results to account for stochasticity in the data. In the context of (3D) molecular property prediction, substantial contributors to both of these uncertainties can be traced to the conformer (i.e., data) generation step (explained in Appendix A) and the predictive model itself (subsection A.1). First, for a given dataset and in general, it is common that some functional groups are better represented than others, which means that when predictors are confronted with a molecule composed of rarer functional groups, confidences (and accuracies) should

Figure 1: **Step 2. training a structure based predictive model** consisted of a VAE feature extractor - E3NN, property predictor - MLP, and orthonormal certificates - OC, a differentiable uncertainty quantification module.

naturally be lower. Second, some geometries might simply be too distinct from training conformers, e.g., in shape or asphericity, and hence the model might not be able to provide meaningful prediction for those cases due to lack of support. Both of these difficulties contribute to a higher epistemic uncertainty. Third, heterogeneity in the data might be caused by the different degrees of freedom per molecule that directly influence the number of possible conformers, which imparts noisiness in the predicted conformers for that molecule, e.g., if they're unreasonably diverse. Finally, additional stochasticity might be a result of systematic bias, such as choice of conformer generator method, type of training data (i.e. imprecise X-ray measurements, chemoinformatic tools, human factor). These latter two sources, additionally and critically including label error (especially in wet-lab experiments), further contribute to the aleatoric uncertainty.

We therefore note that for high fidelity models, it is important to address both sources of uncertainties, that is, account and provide estimates for both when leveraging ML models for molecular property prediction. To this end, we include two uncertainty quantification modules in our SBML predictors (1) orthonormal certificates (Tagasovska Lopez-Paz, 2019) for OOD/epistemic uncertainty, and (2) probabilistic predictions with estimated posterior variance for aleatoric uncertainty. An overview of such SBML predictor is represented in Figure 1. With a structured framework for evaluating uncertainties, we undertake the following challenge:

_Can we improve SBMLs predictive performance for new molecules by reducing the uncertainty stemming from their corresponding predicted conformer(s)?_

In what follows, we provide evidence that we can, and do so by generating _counterfactual_ conformers - _MoleCLUEs_, by sampling optimal latent representations in the direction of smaller uncertainty.

**MoleCLUEs**_CLUEs_(Antoran et al., 2021) are based on the idea of counterfactual explanations. We use the term "counterfactual" in the sense of "what would have happened if things had been different?" We adopt a formulation from the interpretability community that uses "counterfactual explanations" as a case of contrastive explanations (Dhurandhar et al., 2018; Byrne, 2019) that assesses how minimal changes in the input reflect over the output predictions. Our MoleCLUEs will thus seek to make small changes to an input conformer in order to reduce the uncertainty assigned to it by our SBML model. To impose that changes are indeed small, we are concerned with counterfactuals \(x\) that are close to an original conformer \(x_{0}\), according to some pairwise distance metric \(d(x,x_{0})\). Having a desired outcome \(y_{0}^{c}\) in mind that (potentially) differs from the original one \(y_{0}\) produced by the SBML predictor \(f\), in our case a probabilistic MLP, counterfactual explanations \(x_{0}^{c}\) are generated by solving the following optimization problem:

\[x_{i}^{c}=_{x}(f(y=y^{c}|x)-d(x,x_{i})) s.t. y_{i}=y^{c}. \]

We cannot simply optimize this objective in a high-dimensional input space because it may result in adversarial conformers which are not actionable (Goodfellow et al., 2014). An alternative that lends to high-dimensional data is to leverage deep generative models to ensure explanations are in-distribution. Antoran et al. (2021) suggest that searching for counterfactuals in the lower-dimensional latent space

Figure 2: MoleCLUE pipeline for optimizing input conformers with differentiable uncertainty reduction.

of an auxiliary generative model avoids the above issues. We denote an auxiliary latent variable \(z\) from a deep generative model: \(p_{}(x)=p_{}(x|z)p(z)dz\). In our setup, this corresponds to the latent representation of the E3NN VAE introduced in Figure 1. We write the predictive means of the E3NN as \(p_{}(x|z)(x)=_{}(x|z)\) and \(q_{}(z|x)(z)=_{}(z|x)\) from the decoder and encoder respectively. With MoeCLUEs, we aim to find points in latent space which decode into conformers similar to our original observation \(x_{0}\) but are assigned low uncertainty by some differentiable estimate of uncertainty \(H\), such as those described above.

This goal is achieved by minimizing the following objective:

\[(z)=H(y|_{}(x|z))+d(_{}(x|z),x_{0}). \]

CLUEs are then decoded as:

\[x_{CLUE}=_{}(x|z_{CLUE}) where z_{CLUE}=_{z} (z). \]

The pairwise distance metric takes the form \(d(x,x_{0})=_{x}d_{x}(x,x_{0})+_{y}d_{y}(f(x),f(x_{0}))\) such that we can enforce a degree of similarity between original data and CLUEs in both input (conformer) and output (predicted property) space. The hyperparameters \(_{x}\) and \(_{y}\) control the trade-off between producing low variance CLUEs and CLUEs which are close to the original inputs. In this work, we take \(d_{x}(x,x_{0})=||x-x_{0}||_{2}\), which is implemented as MSELoss and can be translated to conformer root-mean-squared deviation RMSD \(=(x,x_{0})}\). Following Antoran et al. (2021), we treat \(_{y}=0\) herein, i.e., do not enforce that label predictions must remain similar to those of original data.

We integrate the CLUE module above in an overall pipeline presented in Figure 2. Namely, our differentiable estimate \(H\) consists of two terms: (1) epistemic or model uncertainty (\(u_{e}\)) via orthonormal certificates (OCs, \(C\))(Tagasovska and Lopez-Paz, 2019), multiple linear classifiers trained on top of lower dimensional feature representation \((z|x)\), \(u_{e}(x)=||C_{}(z|x)||_{2}\) (OCs evaluate close to 0 if a conformer's latent is in distribution and far otherwise); and (2) \(u_{a}(y|x)=(f(_{}(x|z)))\), or the variance in the posterior over the predictions of the MLP \(f\). Having a separate estimate for the different sources in the predictive uncertainty is desirable, as it lets us explore different configurations when optimizing conformers, focusing on reducing the epistemic or the aleatoric portion. The choice can be made based on the starting molecule; for example, a domain expert can determine if the molecule has been sufficiently well represented in the training data, if it is highly unusual, or if its structural degrees of freedom might lead to unrealistic poses. Finally, our MoeCLUE objective loss has the following form:

\[_{MoeCLUE}(z)=u_{e}(x|z)+u_{a}(x|z)+d(_{}(x|z),x_{0}). \]

We note that the MoeCLUEs framework is modular; we can use any differentiable uncertainty estimate, whichever we find most suitable at inference. Further, additional objectives can optionally be added that regularize or enforce desirable latent properties of the newly encoded CLUE within our representation module \(\), such as L2-norm and Kullback-Leibler divergence (see subsection B.1).

### E3nnvae

To enable the generation of MoeCLUEs, we require a decoder module \(\) that takes latent vectors \(z\) as input and outputs position matrices \(x\), i.e., \(_{}(x|z)\). In particular, we require that:

1. decoded outputs maintain the initial graph structure (i.e., nodes \(v_{0}\) and edges \(e_{0}\)) of input molecule \(_{0}=\{v_{0},e_{0},x_{0}\}\); and
2. the latent space \(\) from which \(z\) are sampled must maintain the equivariance of the encoder E3NN (Geiger and Smidt, 2022).

Constructing our generative model as a typical VAE makes achieving 1) a challenge in that global pooling after the encoder module \(\) ablates graph structure, which existing decoder methods may fail to recover (Ganea et al., 2021; Liu et al., 2023; Xu et al., 2023). We therefore implement a novel architecture (depicted in Figure 3) that transfers the hidden node representations \(h_{}\) to the decoder by skip connection. In order to update \(h_{} h_{}\), we treat latent vectors \(z\) as "supernodes" and add virtual edges between sampled \(z\) and all existing nodes \(h_{} v_{0}_{0}\). Information is then propagated via a relational graph convolutional network (RGCN, Schlichtkrull et al. (2017)) as \(\)giving predicted position matrix \(x\) after projection of \(h_{}\) with MLP \(\), keeping topology intact (see subsection B.1).

For requirement 2), we construct separate latent distributions \(q_{}^{L}(z^{L}|x)\) for each spherical-harmonic level \(L\) modeled in \(\)(Geiger and Smidt, 2022). During sampling, we draw from each subspace \(q_{}^{L}\) independently, and concatenate the reshaped samples to give \(z\) that maintains the equivariant tensor structure of \(h_{}\). Details for the remaining E3NNVAE (and OC) components (implementation, training, and hyperparameters) are delegated to Appendix B.

It is worth noting that requirement 1) above can be loosened in settings where decoding an entirely new molecule (2D and 3D graph) is desirable. In this case, we do not require that the 2D topology \(\{v,e\}\) of output \(\) match that of \(_{0}\), and thus we can freely decode, e.g., via autoregressive or diffusion-based methods (Ganea et al., 2021; Xu et al., 2023). We leave investigations to this end for future works.

We also note that our VAE construction differs substantially from that in the seminal CLUEs work (Antoran et al., 2021). In Antoran et al. (2021), the CLUE VAE is trained as an auxiliary generative model with a separate "true" data source. Herein, we do not assume access to such additional data, and instead train both our predictor ('oracle') and generator (VAE) jointly and end-to-end as a single network. Beyond representing a novel approach to CLUE modeling, our design directly impacts the optimization stage, in that each loss term in \(_{(Mole)CLUE}\) is dependent on modules trained jointly under our framework, as opposed to separately in prior art. Future works will seek to understand the implications and differences of using each protocol. Moreover, other applications of counterfactual generation relying on 3D representations could benefit from our modification, e.g., data described as point clouds or mesh grids in graphics, engineering, and biomedical imaging (Rasal et al., 2022, 2020).

## 3 Experiments

### Data

In this work, we consider a regression task from a public benchmark dataset, the Therapeutic Data Commons (TDC, Huang et al. (2022), (link)). The task (**Clearance_Hepatocyte_AZ**) is to predict an input drug molecule's rate of clearance from the human body, a critical property for late-stage drug optimization (Di et al., 2012). 3D conformers of all molecules within are computed and processed as in Axelrod and Gomez-Bombarelli (2020); Maser et al. (2023), and we direct the reader to these works for further details.

Figure 3: E3NN VAE architecture. Latent distributions are separated by spherical harmonic level \(L\) to maintain equivariance in sampling & decoding. \(n=\) number of nodes (heavy atoms), \(s=\) scalar-feature (\(L=0\)) dimensionality (128 herein), \(v=\) vector-feature (\(L=1\)) dimensionality (\(64 3=192\) herein), \(d=\) hidden-node dimensionality (128 herein).

### Experiment setup

To evaluate MoleCLUEs, we use a held-out test set (as given in the TDC), the conformers of which we progressively contaminate with Gaussian noise as in Maser et al. (2023). First, we pass these conformers through our predictive model and rank the results by descending prediction errors and uncertainties, i.e., each term in \(_{MoleCLUE}\). Then, we select the top-10% of the most difficult molecules by each term and try to improve the predictions by bringing their 3D representation closer to the training data, i.e., optimizing \(_{MoleCLUE}\). We include results for increasing amounts of noise contamination \(\{0.0,0.01,0.1,1.0\}\), where \(=0\) (i.e., no noise) corresponds to the original held-out molecules with baseline error & uncertainty. With these different configurations of the analysis, we get to explore the sensitivity and performance improvement of MoleCLUEs.

Figure 4: CLUE-optimization loss curves (clue learning rate \(=0.1\)). Note that y-axis values are normalized across \(\) to be able to visualize important trends.

## 4 Results & Discussion

### Experimental results

Results are shown in Figure 4, aggregated over three runs of the full pipeline shown in Figure 2 and described in subsection 3.2. As can be seen, our method is able to afford substantial reductions in uncertainty on average over CLUE steps, as well as moderate improvements in target prediction error. This effect, as expected, is highly dependent on the level of noise \(\) added to the initial conformers. In most cases, intermediate noise levels (\(10^{-2} 10^{-1}\)) showed strongest and smoothest improvements (blue and yellow curves). On the other hand, uncorrupted examples (\(=0.0\), red curves) showed steady but only minor uncertainty reductions.

With _severe_ corruption (\(=1.0\), purple curves), however, strong remediation of both aleatoric and epistemic uncertainties was observed, though the corresponding improvement in target prediction was highly variable. That said, for this noise scale losses had typically not converged after the default 20 iterations; it is yet unclear if additional improvements could be made with longer simulations. Using higher CLUE learning rates (LR), however, we do observe strong convergence in epistemic and aleatoric uncertainty \(_{e,a}\) as well as target prediction error, (Figure 10), indicating the potential of our method to remedy even severely OOD samples. It is worth noting that early steps were typically accompanied by moves toward the clean, un-noised conformer input (reconstruction loss panel) but tended to diverge in later iterations. Full results are included in subsection C.1, including for sweeps over CLUE LR and normalization hyperparameters.

### Optimization trajectories

Individual examples were chosen at random from the experiments above to visualize the structural simulations resulting from our protocol (Figure 5). In each example, as desired, intermediate

Figure 5: Example CLUE simulations and corresponding loss trajectories; randomly chosen from worst 10% test set examples in respective loss term with original noisy input (cyan). Top) \(_{y}\), middle) \(_{e}\), bottom) \(_{a}\).

conformers are discovered that reduce the problematic loss term while remaining close to the input conformer. Additional randomly selected examples are provided in subsection C.2, which include challenging cases where the loss term of interest was not able to be improved. In particular, examples of initial noisy conformers with high RMSD to the original input (\(_{r}=(x,x_{0})}\)) were challenging to optimize, at least with the hyperparameters studied (\(=\,=0.1\)). Interestingly, and not unexpectedly, we observe in many cases that CLUE conformers continue to diverge from \(x_{0}\) as loss terms other than \(d_{x}(x,x_{0})\) are optimized.

It is additionally worth noting that many of the analyzed CLUE conformers contain non-physical sub-graph geometries (distorted bond lengths, angles, etc.). This is also not unexpected, in that we do not include any energetic constraints/evaluations in conformer generation. That said, in line with Figure 4 (top) we do see in all cases that the RMSD to the input conformer (denoted as \(L_{r}\) in each panel of Figure 5) does decrease over the initial CLUE steps. This indicates that the latent vectors we sample in the direction of lower uncertainty also correspond to decoded conformers that are more physically reasonable (as imposed by the distance term in \(_{MoleCLUE}(z)\)), which we consider a desirable result. Future work will investigate the use of physical priors and/or calls to physics-based score functions during CLUE optimization.

## 5 Conclusion

Herein, we presented a novel algorithm _MoleCLUEs_ for obtaining molecular conformers that minimize uncertainty and label error in a 3D predictive model, leveraging differentiable uncertainty quantifiers and a novel equivariant conformer generative model. An open question is how our method will perform in inference settings where ground-truth labels \(y\) are unavailable, as reducing uncertainty alone may not necessarily reduce label error (i.e., increase accuracy). One avenue for exploration includes training an auxiliary supervised oracle that infers our model's target-prediction error \(_{y}\) on heldout data, and including the reduction of this output in \(_{MoleCLUE}\).

A positive byproduct of our protocol is the inherent interpretability brought on by MoleCLUEs themselves being real conformers (see Figure 5). A practitioner can easily superimpose a starting conformer with higher uncertainty and a MoleCLUE. The difference between the two can pinpoint which parts of the conformers contributed mostly to the uncertainty in the prediction, analogous to Antoran et al. (2021). Other valuable application areas we anticipate include large-molecule (protein, antibody, etc.) property prediction as well as molecular structure prediction and even energetic optimization. Studies toward these ends are ongoing, which we expect to be highly insightful in SBML and MLDD more generally.