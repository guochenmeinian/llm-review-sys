# High-probability complexity guarantees for nonconvex minimax problems

Yassine Laguel

Laboratoire Jean Alexandre Dieudonne

Universite Cote d'Azur

Nice, France

yassine.laguel@univ-cotedazur.fr &Yasa Syed

Department of Statistics

Rutgers University

Piscataway, New Jersey, USA

yasa.syed@rutgers.edu &Ncedet Serhat Aybat

Department of Industrial Engineering

Penn State University

University Park, PA, USA

nsa10@psu.edu &Mert Gurbuzbalaban

Rutgers Business School

Rutgers University

Piscataway, New Jersey, USA

mg1366@rutgers.edu

Corresponding Author

###### Abstract

Stochastic smooth nonconvex minimax problems are prevalent in machine learning, e.g., GAN training, fair classification, and distributionally robust learning. Stochastic gradient descent ascent (GDA)-type methods are popular in practice due to their simplicity and single-loop nature. However, there is a significant gap between the theory and practice regarding high-probability complexity guarantees for these methods on stochastic nonconvex minimax problems. Existing high-probability bounds for GDA-type single-loop methods only apply to convex/concave minimax problems and to particular non-monotone variational inequality problems under some restrictive assumptions. In this work, we address this gap by providing the first high-probability complexity guarantees for nonconvex/PL minimax problems corresponding to a smooth function that satisfies the PL-condition in the dual variable. Specifically, we show that when the stochastic gradients are light-tailed, the smoothed alternating GDA method can compute an \(\)-stationary point within \((^{2}}{^{4}}+}(+^{2}(1/)))\) stochastic gradient calls with probability at least \(1-\) for any \((0,1)\), where \(\) is the PL constant, \(\) is the Lipschitz constant of the gradient, \(=/\) is the condition number, and \(^{2}\) denotes a bound on the variance of stochastic gradients. We also present numerical results on a nonconvex/PL problem with synthetic data and on distributionally robust optimization problems with real data, illustrating our theoretical findings.

## 1 Introduction

Minimax optimization problems arise frequently in machine learning (ML) applications; indeed, constrained optimization problems such as deep learning with model constraints , dictionary learning [10; 57] or matrix completion  can be recast as a minimax optimization problem through Lagrangian duality. Other applications include but are not limited to the training of GANs , fair learning , supervised learning [61; 49; 51; 74], adversarial deep learning , game theory [54; 58], robust optimization [4; 3], distributionally robust learning [46; 75; 24], meta-learning  and multi-agent reinforcement learning . Many of these applications can be reformulated inthe following minimax form:

\[_{x^{d_{1}}}_{y^{d_{2}}}f(x,y),\] (1)

where \(f:^{d_{1}}^{d_{2}}\) is a smooth function, i.e., differentiable with a Lipschitz gradient; \(f\) can possibly be nonconvex in \(x\) and nonconcave in \(y\). First-order primal-dual (FOPD) methods have been the leading computational approach for computing low-to-medium-accuracy stationary points for these problems because of their cheap iterations and mild dependence of their overall complexities on the problem dimension and data size [9; 8; 36]. In the context of FOPD methods, there are two key settings for (1):

1. the _deterministic_ setting, where the partial gradients \(_{x}f\) and \(_{y}f\) are exactly available,
2. the _stochastic_ setting, where we have only access to (inexact) stochastic estimates of the partial gradients, in which case the problem in (1) is called a _stochastic minimax problem_.

It can be argued that the stochastic setting is more relevant to modern machine learning applications where gradients are typically estimated randomly from mini-batches of data, or sometimes intentionally perturbed with random noise to ensure data privacy [14; 34; 1].

**Convex and nonconvex minimax optimization.** In the convex case (when \(f\) is convex2 in \(x\) and concave in \(y\)), several approaches have been considered including Variational Inequalities (VIs) and primal-dual algorithms, see. e.g. [29; 20; 5; 60; 12; 73; 61; 11] and the references therein. One disadvantage of using the VI approach for solving minimax problems (by identifying the _signed gradient map_\(G(x,y)[_{x}f(x,y)^{},-_{y}f(x,y)^{}]^{}\) as the corresponding operator in the VI) is that one needs to set the primal and dual stepsize to be the same. This can be restrictive in applications where \(f\) exhibits different smoothness properties in the primal (\(x\)) and dual (\(y\)) block coordinates -this is often the case in distributionally robust learning , adversarial learning  and in the Lagrangian reformulations of constrained optimization problems that involve many constraints . The gap function \((x_{k},y_{k})_{x,y}f(x_{k},y)-f(x,y_{k})\), and the squared distance to the set of saddle points \((x_{k},y_{k})\{\|x_{k}-x^{}\|^{2}+\|y_{k}-y^{ }\|^{2}(x^{},y^{})\}\) are standard metrics for assessing the quality of the output \(z_{k}=(x_{k},y_{k})\) generated by an FOPD algorithm after \(k\) iterations among many others [35; 73; 20].

In the nonconvex setting, i.e., when \(f\) is nonconvex in \(x\), the aim is to compute a stationary point. Let \((x_{k},y_{k})\) denote a measure for the _stationarity_ of iterates \((x_{k},y_{k})\); a common metric is the norm of the gradient, i.e., \((x_{k},y_{k})\| f(x_{k},y_{k})\|\) and its variants such as \(\|(x_{k})\|\) when \(f\) is strongly concave in \(y\), where \(()=_{y}f(,y)\) denotes the primal function -for other metrics and relation between them, see . There are several algorithms that admit (gradient) complexity guarantees for computing a stationary point of nonconvex minimax problems under various strong concavity, concavity or weak concavity-type assumptions in the \(y\) variable -see the references in .

In this paper, we consider smooth nonconvex-PL (NCPL) problems where \(f\) is a smooth function such that it is possibly nonconvex in \(x\) and it satisfies the Polyak-Lojasiewicz (PL) condition in \(y\). The PL condition is a weaker assumption (milder condition) than strong concavity in \(y\) -in fact, PL condition in the dual does not even require quasi-concavity. NCPL problems constitute a rich class of problems arising in many ML applications including but not limited to fair classification , robust neural network training with dual regularization [48, eqn. (14)], overparametrized systems and neural networks , linear quadratic regulators , smoothed Lasso problems  subject to constraints, distributionally robust learning with \(_{2}\) regularization in the dual , deep AUC maximization  and covariance matrix learning with Wasserstein GANs . For deterministic NCPL problems, the alternating gradient descent ascent (AGDA) method and its smoothed version (smoothed AGDA) have the complexity of \((^{2}/^{2})\) and \((/^{2})\), respectively, for finding a point \((,)\) satisfying \(\| f(,)\|\) as shown in [70; 66]. Here \(/\) is the condition number, where \(\) is the Lipschitz constant of the gradient, and \(\) is the PL constant. For Catalyst-AGDA,  shows also the rate \((/^{2})\) for deterministic NCPL problems.

**In expectation and high-probability bounds.** Most of the existing guarantees in the literature for _stochastic_ FOPD algorithms are provided in expectation, i.e., a bound on the number of iterations \(k\) (or the stochastic gradient evaluations) is provided for \([(x_{k},y_{k})]\) or \([(x_{k},y_{k})]\) to hold (see, e.g., [73; 72; 29] and the references therein). Yet having such guarantees _on average_ does not allow to control tail events, i.e., even if \([(x_{k},y_{k})]\) is small, \((x_{k},y_{k})\) can still be arbitrarily large with a non-zero probability. To this end, high-probability guarantees have been considered in the literature . These results allow to control the risk associated with the worst-case tail events as they specify how many iterations would be sufficient to ensure \((x_{k},y_{k})\) is sufficiently small for any given failure probability \((0,1)\). To derive high-probability bounds, one common approach involves running the algorithm in parallel multiple times and strategically selecting an optimal output to convert in-expectation bounds into high-probability guarantees . Alternatively, advanced concentration inequalities can be employed under light-tail assumptions to control noise accumulation across iterates without requiring multiple runs . For saddle point problems, we note that existing high-probability bounds mostly apply to the monotone VI setting, or to strongly convex/strongly concave (SCSC) minimax problems. To our knowledge, high-probability guarantees for nonconvex minimax problems are non-existent in the literature, even for nonconvex/strongly concave (NCSC) problems with the exception of trivial loose bounds one can obtain by a standard application of Markov's inequality (see Remark 13 for details). In particular, we note that the existing VI literature with high-probability bounds on non-monotone operators such as star-co-coercive operators , do not apply to NCSC problems.3

**New high-probability bounds for NCPL optimization.** To address these shortcomings, we focus on developing high-probability guarantees for NCPL problems. Among the existing algorithms in the stochastic NCPL setting , stochastic gradient descent ascent (SGDA) methods and their variants are quite popular for ML applications, e.g., training GANs and adversarial learning, as SGDA is easy to implement due to its single-loop structure. Guarantees in expectation for stochastic NCSC problems are well supported by the literature - see  and the references therein. To our knowledge, among single-loop methods for NCPL problems, the best guarantees in expectation are given by the _smoothed_ alternating gradient descent ascent (sm-AGDA) method , which can compute an almost stationary point \((,)\) satisfying \([\| f(,)\|\) in \((^{2}^{2}/^{4}+/^{ 2})\) stochastic gradient calls, where \(^{2}\) is an upper bound on the variance of the stochastic gradients. In this work, we consider the sm-AGDA algorithm, and to our knowledge we provide the first-time high-probability bounds (using a single-loop method that does not resort to restarts and parallel runs) for the minimax problem (1) in the NCSC and NCPL settings. More precisely, we focus on a purely stochastic regime in which data streams over time which renders the use of mini-batch schemes or running the method in parallel impractical; therefore, approaches based on Markov's inequality  are no longer applicable (see also Remark 13).

**Contributions.** Our contributions are threefold:

* We present the first _high-probability_ complexity result for the sm-AGDA algorithm in the NCPL setting by building upon a Lyapunov function first introduced in  for nonconvex-concave problems. Later, for the same Lyapunov function, state-of-the-art complexity bounds in _expectation_ are provided for the NCPL setting in . In this paper, we derive a novel descent property for this Lyapunov function in the almost sure sense (Theorem 7 and Corollary 8), allowing us to develop useful concentration arguments for it to derive high-probability bounds. Our Lyapunov analysis not only sheds light on the convergence properties of sm-AGDA, but also guides the parameter selection for sm-AGDA. Specifically, we show that sm-AGDA can compute an almost stationary point \((,)\) satisfying \(\| f(,)\|\) with probability \(1-(0,1)\) within \(T_{,}=(^{2}}{ ^{4}}+}(+^{2}(1/ {q})))\) stochastic gradient calls. The lower complexity bound of \((}+})\) for NCSC problems  in expectation (see also ) suggests that our high-probability bound for sm-AGDA is tight in terms of its dependence on \(\). Furthermore, to our knowledge, these are the first high-probability guarantees for any algorithm in the NCPL setting.
* Under light-tail (sub-Gaussian) assumption on the gradient noise (Assumption 3), which is common in the literature , we develop a new concentration result (Theorem 9) that can be of independent interest. From this concentration inequality, we observe that the cost of strengthening the existing complexity result in expectation to a high-probability one is relatively low, i.e., in the final complexity, the probability parameter \(\)_only_ appears in an additive term that scales with \(^{-2}\). Consequently, this represents a non-dominant overhead compared to the \(^{-4}\) term already present in state-of-the-art expectation bounds .

* Third, we provide experiments that illustrate our theoretical results. We first provide an example of an NCPL-game with synthetic data and then focus on distributionally robust optimization problems with real data, illustrating the performance of the sm-AGDA in terms of high-probability guarantees.

## 2 Preliminaries and Technical Background

**Stationarity metric**. We consider the minimax problem in (1) for \(f:^{d_{1}}^{d_{2}}\) such that \(f\) is smooth (Assumption 1) and \(f(x,)\) satisfies the PL property for all \(x R^{d_{1}}\) (Assumption 2); moreover, we also assume that we only have access to unbiased stochastic estimates of \( f\) such that the stochastic error \(G(x,y,)- f(x,y)\) has a light tail (Assumption 3) for any \((x,y)\), where \(G(x,y,)\) denote the stochastic estimate of \( f(x,y)\) and \(\) denotes the randomness in the estimator.

Our aim is to compute a \((_{x},_{y})\)-stationary point \((,)\) for (1) such that \(\|_{x}f(,)\|_{x}\) and \(\|_{y}f(,)\|_{y}\). We also call \((,)\) an \(\)-stationary point if \(\| f(,)\|\). Clearly, whenever \((,)\) is \((_{x},_{y})\)-stationary, then it is also \(\)-stationary for \(=(_{x}^{2}+_{y}^{2})^{1/2}\).

**Smoothed alternating gradient descent ascent (sm-AGDA):** The method can be considered as an _inexact_ proximal point method and was introduced in . More specifically, in each iteration of sm-AGDA, given a proximal center \(z_{t}\) and the current iterate \((x_{t},y_{t})\), the method computes the next iterate \((x_{t+1},y_{t+1})\) using a stochastic gradient descent ascent step on a regularized function \(\):

\[(x,y;z_{t}) f(x,y)+\|x-z_{t}\|^{2}.\] (2)

Following the stochastic alternating gradient descent ascent (stochastic AGDA) steps, the _proximal center_ at iteration \(t\), i.e., \(z_{t}\), is updated as shown in Algorithm 1, where \(G_{x}(x_{t},y_{t},_{t+1}^{x})\) and \(G_{y}(x_{t+1},y_{t},_{t+1}^{y})\) denote conditionally unbiased stochastic estimators of the gradients \(_{x}f(x_{t},y_{t})\) and \(_{y}f(x_{t+1},y_{t})\). Throughout the analysis we assume that \( f\) is Lipschitz, which is standard in the study of first-order optimization algorithms for smooth minimax problems; see, e.g., .

**Assumption 1**.: _(Lipschitz gradient) For all \((x_{1},y_{1}),(x_{2},y_{2})^{d_{1}}^{d_{2}}\), there exists \(>0\)_

\[\|_{x}f(x_{1},y_{1})-_{x}f(x_{2},y_{2})\|(\|x _{1}-x_{2}\|+\|y_{1}-y_{2}\|)\] (3) \[\|_{y}f(x_{1},y_{1})-_{y}f(x_{2},y_{2})\|(\|x _{1}-x_{2}\|+\|y_{1}-y_{2}\|).\] (4)

The following condition, known as Polyak-Lojaciewicz (PL) condition is weaker than assuming strong concavity in \(y\), and does not even necessitate \(f\) to be even quasi-concave in the \(y\) variable. It holds in many ML applications including those in .

**Assumption 2**.: _(PL condition in \(y\)) For every \(x^{d_{1}},\,_{y^{d_{2}}}f(x,y)\) has a non-empty solution set and a finite optimal value. Moreover, there exists \(>0\) such that:_

\[\|_{y}f(x,y)\|^{2} 2[_{y^{d_{2}}}f(x,y)-f(x,y)], \;x^{d_{1}}.\] (5)

 
**Algorithm** & **Complexity** & **Problem** & **Metric** & **NC?** \\  Epoch-GDA \({}^{}\) & \((}{}(1/))\) & SCSC & \((_{k})\) & ✗ \\  Clipped-SGDDA \({}^{}\) & \(}(\{},}{^{2}}(})(/ )\}.\) & SCSC & \((z_{k})\) & ✗ \\  Clipped-SEG \({}^{}\) & \(}(\{},}{^{2}}(})(/ )\}.\) & SCSC & \((z_{k})\) & ✗ \\  Stochastic APD \({}^{}\) & \((})}{_{x}}+))^{2}(1/)}{_{x} _{x}})\) & SCSC & \((z_{k})\) & ✗ \\  Mirror-Prox \({}^{}\) & \(((}{^{2}}, ^{2}}{^{2}})(1/))\) & MCMC & \((_{k})\) & ✗ \\  Clipped-SGDDA \({}^{}\) & \(}(\{}}{^{2}}(1/)\}.\) & MCMC & \(_{R}(_{k})\) & ✗ \\  Clipped-SEG  & \(}((}{^{2}},^{2}}{^{2}})(1/))\) & MCMC & \(_{R}(_{k})\) & ✗ \\   ^{}\)**} & \((^{2}}{^{4}}+}{ ^{2}}(+^{2}(1/)))\) & NCPL & \(_{j=0}^{}\| f(z_{j})\|^{2}\) & ✓ \\  

Table 1: Summary of the high-probability bounds for minimax problem classes when the gradient of \(f\) is Lipschitz (with parameter \(\)) and stochastic gradient variance is bounded by \(^{}\). The second column reports the complexity (number of calls to stochastic gradient oracle) required to achieve the (stationarity) metric reported in the fourth column to be at most \(\) with probability \(1-,(0,1)(O)^{}\) ignores some logarithmic terms. Here, \(_{s}\) is the strong convexity constant, \(\) is the PL constant, and \(/\). Let \(G(x)[_{x}f(z)^{},-_{y}f(z)^{}]^{}\) with \(z=(x,y)\) and \(_{h}=_{j=0}^{K}z_{j},(z)_{(  Z)}(G(z),-z_{})\), where \(Z\) is the domain of the problem with diameter \(D(0,+]\), and \(_{R}(z)_{( Z,\|x-z_{}\| R)}(G(z), -z_{})\) where \(z_{}=(x_{}^{},y_{}^{})^{}\) is a stationary point. The third column reports the minimax problem classes. The fifth column indicates whether the results supports nonconvexity, i.e., whether \(f\) can be a smooth function nonconvex in \(x\).  is a two-loop method. \({}^{}\) Applicable to quasi-strongly monotone \(G\) that is star-co-coercive around \(z_{}\) and supports heavy-tailed gradients. \({}^{}\) Ap supports proximal steps to handle non-smooth convex penalty. \({}^{}\) Applies to monotone \(G\) that is star-co-ercive around  assume that we have only access to stochastic estimates \(G_{x}(x_{t},y_{t},_{t+1}^{x})\) and \(G_{y}(x_{t+1},y_{t},_{t+1}^{y})\) of the partial gradients \(_{y}f(x_{k},y_{k})\) and \(_{x}f(x_{t+1},y_{t})\), where \(_{t+1}^{x}\) and \(_{t+1}^{y}\) are random variables defined on a probability space \((,)\), i.e., the source of randomness in the gradient estimates. Note that sm-AGDA has Gauss-Seidel updates, i.e., the stochastic estimate of the partial gradient \(G_{y}(x_{t+1},y_{t},_{t+1}^{y})\) is evaluated at the updated point \((x_{t+1},y_{t})\) instead of \((x_{t},y_{t})\). To capture the sequential information flow, we next introduce the natural filtrations that represent all the information available before an update: Let \(_{t}^{x}\) and \(_{t}^{y}\) be revealed sequentially in the natural order of the sm-AGDA updates, i.e., \(_{1}^{x}_{1}^{y}_{2}^{x}_{2}^{y}_{3}^{x}\), and let \((_{t}^{x})_{t 1}\) and \((_{t}^{y})_{t 1}\) denote the associated filtration4, i.e., let \(_{0}^{y}\{,\}\), and

\[_{t+1}^{x}=(_{t}^{y},(_{t+1}^{x})), _{t+1}^{y}=(_{t+1}^{x},(_{t+1}^{y}) ),\;t 0.\] (6)

Introducing multiple filtrations to represent the sequential information flow is common in the study of stochastic algorithms with Gauss-Seidel updates -see, e.g., papers on stochastic ADMM, and ; and we follow the same approach. Consider the gradient noise (errors) at time \(t\):

\[_{t}^{x} G_{x}(x_{t},y_{t},_{t+1}^{x})-_{x}f(x_{t},y _{t}),_{t}^{y} G_{y}(x_{t+1},y_{t},_{t+1}^{y})- _{y}f(x_{t+1},y_{t}).\]

Finally, we also assume that the gradient noise is unbiased conditionally on the past information and that it admits a light (sub-Gaussian) tail.

**Assumption 3**.: _(Light tail) For any \(t 0\), there exists scalars \(_{x},_{y}>0\) such that_

\[[_{t}^{x}_{t}^{y}]=0, [\|_{t}^{x}\| s_{t}^{y}]  2e^{}{2_{y}^{2}}},\] (7) \[[_{t}^{y}_{t+1}^{x}]=0, [\|_{t}^{y}\| s_{t+1}^{x}]  2e^{}{2_{y}^{2}}}.\] (8)

For developing high-probability bounds in the learning context, it is common to assume that gradient estimates are sub-Gaussian . While this assumption may not always hold (see e.g. ), it often holds when gradients are estimated via mini-batching, as a consequence of the central limit theorem. It will also hold when the gradient noise is bounded. Additionally, adoption of differential privacy mechanisms within gradient-based schemes , to enhance data privacy, results frequently in sub-Gaussian gradient errors.

## 3 High-probability bounds for sm-Agda

For analyzing sm-AGDA, similar to , we consider the following Lyapunov function:

\[V_{t} V(x_{t},y_{t};z_{t})=(x_{t},y_{t};z_{t})+2P(z_{t})-2 (y_{t};z_{t}),\] (9)

where \(P(z)\) and \((;z)\) denote the saddle point value and the dual function value, respectively, of the auxiliary problem \(_{x}_{y}(x,y;z)\) for any fixed \(z\) and \(\) defined in (2), i.e.,

\[(y;z)_{x^{d_{1}}}(x,y;z)  P(z)_{x^{d_{1}}}_{y^{d_{2}}} (x,y;z).\] (10)

Next, we introduce a natural assumption, commonly made in the literature . Without this assumption, there are pathological cases where primal function \((x)\) may be unbounded leading to divergence of gradient-based methods; an example would be \(f(x,y)=-x^{2}-y^{2}\) in dimension one.

**Assumption 4**.: _Consider the primal function \(:^{d_{1}}\), i.e., \((x)=_{y^{d_{2}}}f(x,y)\). There exists \(x^{*}^{d_{1}}\) such that \(^{*}(x^{*})=_{x^{d_{1}}}(x)\)._Under Assumption 4, it immediately follows that \(V_{t}^{*}\) for all \(t\) -since \(P(z)-(y,z) 0\), \((x,y;z)-(y;z) 0\) and \(P(z)^{*}\) for all \(x,y,z\). We will next study the change \(V_{t}-V_{t+1}\) in the Lyapunov function and show that an approximate descent property holds. First, we need two key lemmas that characterize the evolution of \((x_{t},y_{t};z_{t})\) and \((y_{t};z_{t})\) over the iterations.

**Lemma 5**.: _Suppose Assumptions 1, 2, 3 and 4 hold. Consider sm-AGDA given in Alg. 1 with \(_{1}(0,]\) and \((0,1]\). For any \(t\), we have:_

\[(x_{t+1},y_{t+1};z_{t+1})-(x_{t},y_{t};z_{t}) -}{2}\|_{x}(x_{t},y_{t};z_{t})\|^{2}+ _{2}(1+_{2})\|_{y}f(x_{t+1},y_{t})\|^{2}\] \[+_{1}((p+)_{1}-1)_{t}^{x},_{x} {f}(x_{t},y_{t};z_{t})+_{1}^{2}\|_{t}^{x}\|^ {2}\] \[+_{2}(1+_{2})_{y}f(x_{t+1},y_{t}),_ {t}^{y}-\|z_{t}-z_{t+1}\|^{2}+^{2}}{2 }\|_{t}^{y}\|^{2}.\]

Proof.: The proof is provided in Appendix B.1. 

From Assumption 1, when \(p>\), the auxilliary function \((,y;z)\) is \((p-)\)-strongly convex for any fixed \(y,z\); hence, there is a unique minimizer for every \(y,z\) fixed, denoted by

\[x^{*}(y,z)_{x^{d_{1}}}(x,y ;z),\] (11)

i.e., \((y,z)=(x^{*}(y,z),y;z)\). In the rest of the paper, we will take \(p>\) and exploit this property. The following lemma characterizes the change in the dual function \(\).

**Lemma 6**.: _Suppose Assumptions 1, 2, 3 and 4 hold. Consider the sm-AGDA iterate sequence \(\{(x_{t},y_{t},z_{t})\}_{t}\) for \(p>\). For any \(t\), it holds that_

\[(y_{t+1};z_{t+1})-(y_{t};z_{t}) _{2}_{y}f(x^{*}(y_{t},z_{t}),y_{t}),_{y}f(x _{t+1},y_{t})+_{2}_{y}f(x^{*}(y_{t},z_{t}),y_{t}), _{t}^{y}\] \[-}{2}_{2}^{2}(\|_{y}f(x_{t+1},y_{t} )\|^{2}+2_{y}f(x_{t+1},y_{t}),_{t}^{y}+\|_{t}^ {y}\|^{2})\] \[+ z_{t+1}-z_{t},z_{t+1}+z_{t}-2x^{*}(y_{t+1},z_ {t+1}),\]

_where \(L_{}(1+)\) and the map \(x^{*}(,)\) is defined by (11)._

Proof.: The proof is provided in Appendix B.2. 

The next result provides an approximate descent property on the Lyapunov function. Its proof builds on Lemmas 5 and 6 and a descent property on the function \(P\) (given in Lemma 15 of the Appendix); and leverages smoothness properties of the functions \(\) and \(\) and the map \((y,z) x^{*}(y,z)\) as well as the strong convexity of \(\) with respect to \(x\).

**Theorem 7**.: _Suppose Assumptions 1, 2, 3 and 4 hold. Consider the sm-AGDA algorithm with parameters \(p>\), \((0,1]\), \(_{1}(0,]\) and \(_{2}>0\) chosen such that_

\[c_{0}-_{2}^{2}+_{2}(1-_{2}-L_ {}_{2}) 0, c_{0}^{}-(}{p- }+48}{(p-)^{2}}) 0,\]

_for some constant \(>0\), where \(L_{}=(1+)\). Then,_

\[V_{t}-V_{t+1} c_{1}\|_{x}(x_{t},y_{t};z_{t})\|^{2}+c_{2}\|_{y}f(x^ {*}(y_{t},z_{t}),y_{t})\|^{2}+c_{3}\|x_{t}-z_{t}\|^{2}\] \[+c_{4}_{x}(x_{t},y_{t};z_{t}),_{t}^{x} + c_{5}_{y}f(x_{t},y_{t})+c_{6}_{y}f(x^{*}(y_{t},z_ {t}),y_{t}),_{t}^{y}\] (12) \[+c_{7}\|_{t}^{x}\|^{2}+c_{8}\|_{t}^{y}\|^{2},\]

_for some constants \(\{c_{i}\}_{i=1}^{8}\) that are explicitly given in Appendix C, which may depend on \(\), as well as the problem and sm-AGDA parameters that can be chosen such that \(c_{1},c_{2},c_{3}>0\)._

Proof.: The proof is given in Appendix C. 

With some specific choice of parameters in sm-AGDA, we can obtain simplifications to the coefficients \(\{c_{i}\}_{i=1}^{8}\) from Theorem 7 (explicitly given in Appendix C). As such, this yields the following corollary.

**Corollary 8**.: _Under the premise of Theorem 7, let \(p=2\), \(_{1}(0,]\), \(_{2}=}{48},=_{2}\) for \((0,]\). Then, \(_{t+1}-_{t}}{_{1}}-_{t}+_{t+ 1}+_{t+1}\) for all \(t\), where \(=t}\) and_

\(_{t}_{1}V_{t},_{t} \|_{}(x_{t},y_{t};z_{t})\|^{2}+}{8}\| _{y}f(x^{*}(y_{t},z_{t}),y_{t})\|^{2}+\|x_{t}-z_{t}\|^{ 2},\)__

\[_{t+1} 192 p ^{2}^{2}_{2}^{2}++4c_{0}^{2}+2c_{0}^{ }^{2}_{1}^{2}+(p+)_{1}-1_{1} _{x}(x_{t},y_{t};z_{t}),_{t}^{x}\] \[+_{2}(1+_{2}+2L_{}_{2})\,_{y}f( x_{t},y_{t})-2_{y}f(x^{*}(y_{t},z_{t}),y_{t}),_{t}^{y},\] \[_{t+1}  2_{1}^{2}\|_{t}^{x}\|^{2}+8_{2}^{2}\| _{t}^{y}\|^{2}.\]

Proof.: The proof is given in Appendix D. 

Next, we provide a concentration inequality which will be key to obtain our high-probability bounds.

**Theorem 9**.: _Let \(\{_{t}\}_{t}\) be a filtration on \((,,)\). Let \(A_{t},B_{t},C_{t},D_{t}\) be four stochastic processes adapted to the filtration such that there exist \(_{C},_{D}>0\) and \(_{1}>0\) such that for all \(t\): \((i)\)\(B_{t} 0\), \((ii)\)\([e^{ C_{t+1}}_{t}] e^{^{2}_{C}^{2}B_{ t}}\) for all \(>0\), \((iii)\)\([e^{ D_{t+1}}_{t}] e^{_{D}^{2}}\) for all \([0,^{2}}]\) and \((iv)\)\(-A_{t}}{_{1}}-B_{t}+C_{t+1}+D_{t+1}\). Then, for any \((0,1]\), we have_

\[(}{2}_{t=0}^{T-1}B_{t}(A_{0}-A_{T})+ _{1}_{D}^{2}T+2_{1}\{2_{C}^{2},_{D}^{2}\}( })) 1-.\]

Proof.: The proof is provided in Appendix E. 

**Remark 10**.: _While the above concentration inequality seems tailored to the analysis of sm-AGDA, it can also aid in deriving high probability bounds for many other first-order methods for nonconvex minimax problems that outputs a randomized iterate; indeed, the majority of existing Lyapunov arguments in the nonconvex setting are built upon constructing telescoping sums in line with Theorem 9, e.g., stochastic alternating GDA  for NCPL minimax problems and optimistic GDA  for strongly convex-strongly concave problems._

We next present our main result which provides a high-probability bound on the sm-AGDA iterates. The main idea of the proof is to apply Theorem 9 to the processes introduced in Corollary 8.

**Theorem 11**.: _In the premise of Corollary 8,_ sm-AGDA _iterates \((x_{t},y_{t})\) for \(_{1}\) satisfy_

\[_{t=0}^{T-1}\|_{x}f(x_{t},y_{t}) \|^{2}+\|_{y}f(x_{t},y_{t})\|^{2}_{, T}, 1-,\;T,\;(0,1],\]

_for some \(_{,T}=+b_{0})}{ _{1}T}+(_{x}^{2}+_{y}^{2}_{1}+ (})\) explicitly stated in Appendix F, where \(_{0}(z_{0})-^{*}\), \(b_{0} 2_{x,y}\{(x_{0},y;z_{0})-(x,y_{0};z_{0})\}\)._

Proof Sketch.: Let the stochastic processes \(A_{t},B_{t},C_{t},D_{t}\) in Theorem 9 be chosen as \(A_{t}=_{t}\), \(B_{t}=_{t}\), \(C_{t}=_{t}\), \(D_{t}=_{t}\) where \(_{t},_{t},_{t},_{t}\) are defined in Corollary 8 and \(_{1}>0\) be the primal stepsize in sm-AGDA; according to Corollary 8, we have \(-A_{t}}{_{1}}-B_{t}+C_{t+1}+D_{t+1}\) for \(t\). Since \(_{t}^{x}\) and \(_{t}^{y}\) admit sub-Gaussian tails, it can be shown that the conditions of Theorem 9 are satisfied for some appropriate constants \(_{C}^{2}\) and \(_{D}^{2}\). Therefore, Theorem 9 implies a tail bound on \(_{t=0}^{T-1}_{t}\). Using the relation between \(f\) and \(\), one can also show that \(\|_{x}f(x_{t},y_{t})\|^{2}+\|_{y}f(x_{t},y_{t})\|^{2}=(_{t})\), for all \(t\). This last inequality allows to translate the tail bound for \(_{t=0}^{T-1}_{t}\) to a tail bound for \(_{t=0}^{T-1}\|_{x}f(x_{t},y_{t})\|^{2}+\|_{y}f(x_{t},y_{t })\|^{2}\). The details of the proof is provided in Appendix F of the complementary material. 

**Remark 12**.: _Suppose_ sm-AGDA_, given in Alg. 1, is run for \(T\) iterations, and it outputs a randomly selected iterate \((x_{U},y_{U})\), where the random iteration index \(U\) is chosen uniformly at random from the set \(\{0,1,,T-1\}\), i.e., \((U=t)=1/T\) for \(t=0,1,,T-1\). Theorem 11 implies that_

\[\|_{x}f(x_{U},y_{U})\|^{2}+\|_{y}f(x_{U},y_ {U})\|^{2}_{,T} 1-.\]

_Furthermore, in comparison with existing complexity bounds in expectation for sm-AGDA , our quantile bound requires only an overhead of order \(}(^{-2}(1/))\). Unless \(\) is very small, this is typically negligible in comparison to the \((^{-4})\) already present in rates in expectation._

**Remark 13**.: _In contrast to high-probability bounds derived from standard Markov-type arguments, our approach achieves significantly better scaling with respect to both \(\) and \(\). Specifically, consider an oracle that can generate a sample \((,)\) with \([\| f(,)\|]\) after \(()\) iterations/stochastic samples. In particular,  shows that one can take \(()=(^{2}}{ ^{4}}+})\) for the sm-AGDA algorithm assuming the variance of the stochastic gradient is bounded by \(^{2}\). A naive high-probability bound could be constructed by ensuring \([\| f(,)\|]\) and applying Markov's inequality to yield an \(\)-stationary point with probability at least \(1-\). However, this approach results in a complexity bound of \(()=O(^{2}}{ ^{2}^{4}}+^{2}})\), leading to a significantly worse dependence on \(\) than ours. Alternatively, following the rationale in , to generate a high-probability bound, one can run the sm-AGDA algorithm \(m=((}))\) times in parallel; where in each run we generate an \(/2\)-solution and among the solutions, we select the one with the smallest estimated gradient norm. This would require \(m()=((1/)^{2}}{^{4}}+(1/)})\) iterations/stochastic samples. In this approach, the logarithmic term \((})\) multiplies the high-order \((})\) term, whereas in our approach it only affects the second-order \((^{2}})\) term. Therefore, our results scale better with respect to \(\) and \(\). In addition, such a (multiple) parallel run approach, is often impractical in streaming/online settings, where data arrives sequentially, and real-time processing is essential._

**Corollary 14**.: _Under the premise of Theorem 11, consider running the sm-AGDA method for some fixed number of iterations \(T\) with parameters chosen as \(_{1}=(,+b_{0}}}{}})\) and \(_{2}=_{1}/48\) where \(^{2}_{x}^{2}+_{y}^{2}\). Then, for any \((0,1)\), sm-AGDA can compute an \((,/)\) stationarity point with probability at least \(1-\) when the number of iterations \(T\) is fixed to \(T_{,}=+b_{0})_{ }}{^{2}}+(}) }{^{2}}+(_{0}+b_{0})^{2}}{ ^{4}}\) which requires \(T_{,}\) stochastic gradient calls._

Proof.: This is a direct consequence of Theorem 11, a proof is provided in Appendix G. 

## 4 Numerical Illustrations

In this section, we illustrate the performance of sm-AGDA. We consider an NCPL problem with synthetic data, as well as a nonconvex DRO problem using real datasets. For synthetic experiments, we used an ASUS Laptop model Q540VJ with 13th Generation Intel Core i9-13900H using 16GB RAM and 1TB SSD hard drive. For the DRO experiments, we used a high-performance computing cluster with automatic GPU selection (NVIDIA RTX 3050, RTX 3090, A100, or Tesla P100) based on GPU availability, ensuring optimal use of computational resources.

**Synthetic experiments on an NCPL game.** We consider the following NCPL problem:

\[_{x^{d_{1}}}_{y^{d_{2}}}m_{1}[\|x\|^{2 }+\!(3+1})]+x^{}Ky-m_{2}[\|y\|^{2 }+3^{2}(\|y\|)],\] (13)

which can be interpreted as a game between two players  where \(m_{1},m_{2}>0\) are constants and the symmetric matrix \(K\) is set randomly, similar to the standard bilinear game setting considered in . More specifically, we set \(K=10/\||\), \(=(M+M^{})/2\) where \(M\) is a \(d d\) matrix with entries being i.i.d centered Gaussian having variance \(^{2}\). This problem is nonconvex in \(x\) (without satisfying the PL condition in \(x\)). Though the exact gradient is known, we consider a stochastic gradient oracle, which returns _noisy_ gradients similar to the setting of , i.e., for each iteration \(t\{0,...,T-1\}\), \(G_{x}(x_{t},y_{t};_{t+1}^{x})=_{x}f(x_{t},y_{t})+_{t+1}^{x}\) and \(G_{y}(x_{t+1},y_{t},_{t+1}^{y})=_{y}f(x_{t+1},y_{t})+_{t+1}^{y}\), with \((_{t+1}^{x})_{t 0}(, ^{2}I_{d_{1}})\) and \((_{t+1}^{y})_{t 0}(,^{2}I_{d_{2}})\) where \(I_{d}\) is the \(d d\) identity matrix and \(^{2}\) is some constant variance. This setting satisfies all our assumptions, and our high-probability results (Theorem 11 and Coro. 14) are applicable. In this experiment, we fix \(d_{1}=d_{2}=30\), and \(m_{1}=m_{2}=^{2}=^{2}=1\). The solution to this problem is \((x^{*},y^{*})=(,)\).

_Experimental results._ The parameters of the problem are explicitly available as \(=2m_{2}\), and \(=\{12m_{1},8m_{2},\|K\|\}\). To illustrate Theorem 11, we set \(=}{1600},_{2}=}{48},p=2\) and we considered two cases: \(_{1}=\) (long step) and \(_{1}=}\) (short step) to explore the behavior of sm-AGDA for different stepsizes. We generated \(N=2\) sample paths for \(T=10,000\) iterations, and on the left panel of Fig. 1, for each iteration \(t\), we report the average of \(_{}(t)\|_{x}f(x_{t},y_{t})\|^{2}+\| _{y}f(x_{t},y_{t})\|^{2}\) over \(N=25\) realizations corresponding to different sample paths, and the shaded region depicts the range statistic, i.e., for every fixed iteration \(t\), we shade the

[MISSING_PAGE_FAIL:9]

\[_{x^{d_{1}}}_{y Y}}_{j=1}^{d_{2} }y_{j}_{j}(x;a_{j},b_{j})+r(x)-g(y),\] (14)

where \(_{j}(x;a_{j},b_{j})=1+-b_{j}_{j}^{ }\) denotes the logistic loss tied to an input-output pair \((a_{j},b_{j})^{d_{1}}\{-1,1\}\), and \(r(x)=_{1}_{i=1}^{d_{1}}^{2}}{1+ x_{i}^{2}}\) a primal regularization for the learning model \(x^{d_{1}}\). We allow the distribution \(y Y\{y^{d_{2}}:\;y 0,^{}y=1\}\) to deviate from the uniform distribution \(u}\) where \(\) denotes the vector of ones, and we penalize the distance between \(y\) and \(u\) through the regularization map \(g:yd_{2}}{2}\|y-u\|^{2}\). We set the regularization parameters as \(=10\), \(_{1}=10e^{-4}\), and \(_{2}=1\). Since \(r\) is nonconvex with a Lipschitz gradient and \(g\) is strongly convex, this is an NCSC problem.

_Datasets, Algorithms and Hyperparameters._ We consider three standard datasets for this problem, which are summarized as follows: The sido0 dataset  has \(d_{1}=4932\) and \(d_{2}=12678\). The gisette dataset  has \(d_{1}=5000\) and \(d_{2}=6000\). Finally, the a9a dataset  has \(d_{1}=123\) and \(d_{2}=32561\). We compare the performances of sm-AGDA against two other baselines that achieve state-of-the-art performance in expectation for these datasets . Specifically, we evaluate SAPD+, which is a two-loop method where the subproblems are solved by the SAPD algorithm , and SMDAVR, a variance reduced extension of SMDA algorithm . Since (14) is constrained, we augment sm-AGDA with a projection step in the update of the y variable onto the \(d_{2}\)-dimensional simplex and adopt the analogous stationarity metric \(\|_{x}f(x_{t},y_{t})\|^{2}+\|P_{Y}_{y}f(x_{t},y_{t})\|^{2}\) for constrained problems where \(P_{Y}\) is a projection to the dual domain \(Y\). For all datasets, the primal stepsize \(_{1}\) of sm-AGDA is tuned via a grid-search over \(\{10^{-k},1 k 4\}\). The dual stepsize \(_{2}\) is set as \(_{2}=}{48}\). Similarly, \(\) is estimated through a grid-search over \(\{10^{-k},3 k 5\}\). The parameter \(p\) is also tuned similarly on a grid, our code is provided as a supplementary document for the details. For other methods, our hyperparameters are tuned in accordance with .

_Experimental results._ In Figure 2, we plot histograms of our stationarity metric, across \(200\) runs in a logarithmic scale. We report the stationarity measure both in early phase of the training (i.e. \(t=20\) epochs), and in later phases (i.e. \(t=550\) epochs for gisette and \(t=250\) epochs for a9a and sido0). Our theoretical results are presented for unconstrained problems in the dual, therefore they are not directly applicable to the DRO problem where the dual domain is constrained. That being said, we observe that they are still predictive of performance in the DRO setting. More specifically, Figure 2 is supportive of our high-probability complexity bounds for sm-AGDA, in the sense that the distribution of the stationarity metric for sm-AGDA tends to concentrate. Notably, it outperforms the concentration behaviour of the other baselines. Furthermore, we observe that histograms for all baselines hardly evolve after \(20\) epochs. This is consistent with previous experiments carried on these datasets  where performance was measured in terms of the decay of the average loss and its standard deviation. As such, we conclude that sm-AGDA performs better both in the early phase and the later stage. In our experience, we observed sm-AGDA could accomodate larger stepsizes compared to the other algorithms, which may have contributed to its good performance.

## 5 Conclusion

Existing high-probability bounds only apply to convex/concave minimax problems or non-monotone variational inequality problems under restrictive assumptions to our knowledge. We close this gap by providing the first high-probability complexity guarantees for nonconvex/PL minimax problems satisfying the PL-condition in the dual variable for the sm-AGDA method. We also provide numerical results for an NCPL example and for nonconvex distributionally robust logistic regression.