# Public-data Assisted Private Stochastic Optimization: Power and Limitations

Enayat Ullah

Meta 1

enayat@meta.com

&Michael Menart

Department of Computer Science & Engineering

The Ohio State University 2

Department of Computer Science, University of Toronto

Vector Institute

menart.2@osu.edu

&Raef Bassily

Department of Computer Science & Engineering

Translational Data Analytics Institute (TDAI)

The Ohio State University

bassily.1@osu.edu

&Cristobal Guzman

Inst. for Mathematical and Comput. Eng.

Fac. de Matematicas and Esc. de Ingenieria

Pontificia Universidad Catolica de Chile

crguzmanp@uc.cl

&Raman Arora

Department of Computer Science

Johns Hopkins University

arora@cs.jhu.edu

###### Abstract

We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any \((,)\)-PA-DP has excess risk \(}}},}+}{n}}\), where \(d\) is the dimension, \(n_{}\) is the number of public samples, \(n_{}\) is the number of private samples, and \(n=n_{}+n_{}\). These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with _unlabeled_ public samples. In contrast to our previous result, we here show novel methods for leveraging public data in private supervised learning. For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given \((n_{})\) unlabeled public samples, achieves the dimension independent rate \(}}}+ }}}\). We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate. Finally, we provide extensions of this result to general hypothesis classes with finite _fat-shattering dimension_ with applications to neural networks and non-Euclidean geometries.

Introduction

The framework of differential privacy has become the primary standard for protecting individual privacy in data analysis and machine learning. Unfortunately, this rigorous framework has also been shown to lead to worse performance on such tasks both empirically and in theory . However, it is often the case that, in addition to a collection of privacy-sensitive data points, analysts have access to a pool of public data, for which guaranteeing privacy protections is not required. This can happen, for example, when consumers deem their own data non-sensitive and opt-in to sell this data to a company. This has motivated a long line of work analyzing how public data can be leveraged in tandem with private data to provide better utility . In machine learning, for example, two commonly proposed strategies are public pretraining and using public data to identify gradient subspaces . Public pretraining, in particular, has proven effective in practice , and prior work has even identified a _specific_ problem instance where public and private data used in tandem leads to better rates than is possible using only the public or private datasets in isolation . Despite this surge of work, theory has struggled to show that public data leads to fundamental rate improvements more generally. Recent work has even shown that, for the problem of pure PA-DP stochastic convex optimization, a small amount of public data, \(n_{} n/d\), leads to no rate improvement, where \(n=n_{}+n_{}\) and \(n_{}\) is the number of private samples .

One particularly important version of this problem is in supervised learning when the public data is unlabeled. This setting has found importance in medical domains and deep learning more generally . Notably, unlabeled data is much less time intensive to collect than labeled data. Due to this fact, and the fact that the unlabeled public data does not contain the same kind of information contained in the private data, the regime \(n_{}=(n_{})\) is meaningful both in theory and in practice. We also note this setting is a stronger (in terms of privacy) version of the label-private setting, where only the labels of the dataset are considered private .

Motivated by the importance of these settings and the lack of existing theory for them in stochastic optimization, we study fundamental limitations and applications of public data in \((,)\)-PA-DP stochastic optimization. In the case where the public data is complete/labeled, we show that the application of public data is fundamentally limited. We then contrast this result with new results in the unlabeled public data setting. In this setting, we provide new results for GLMs, and extend these results to more general hypothesis classes, with finite fat-shattering dimension, and non-Euclidean geometries.

### Our Contributions

We outline our primary contributions in the following.

**Limits of Private Stochastic Convex Optimization with Public Data.** First, we show a tight lower bound for the problem of differentially-private stochastic convex optimization (DP-SCO) assisted with complete public data, that is, the public data and private data have the same number of features (and labels when applicable). Specifically, we show a lower bound of \(}}},}+ }{n}}\) on the excess population risk for this problem. When \(d n\) and \(n_{}\), we further improve this lower bound to \(}}},}+ }{n}}\). This lower bound is matched by the simple upper bound strategy which either discards the private data entirely and outputs the public mean or simply treats all data-points as private. Barring constant factors, this shows more sophisticated attempts at leveraging public data will yield no benefit. These results also hold even for generalized linear models. Our results are based on new results we establish for DP mean estimation with public data, and a reduction of mean estimation to SCO. We note that previous work , on this problem either focused on the pure PA-DP case when \(n_{} n/d\), or, in the approximate PA-DP case, did not obtain the dimension dependence. Our mean estimation lower bound uses a novel analysis of fingerprinting codes , and our SCO reduction further builds on ideas from . We also show that, when \(d n\), our lower bounds for approximate PA-DP SCO directly imply a tight lower bound for _pure_ PA-DP.

**Private Supervised Learning with Unlabeled Public Data.** While the previously discussed results show there is no hope for leveraging public data in "interesting" ways, even for GLMs, they do not preclude settings where the public data is less informative. In particular, in the setting where the public data is _unlabeled_, it makes sense to even consider \(n_{} n_{}\). In this setting, we provide the following results.

* For (Euclidean) GLMs we develop an efficient algorithm which, given \((n_{})\) unlabeled public data points, achieves the dimension independent rate \((}}}+} }})\). We obtain this result via a dimensionality reduction procedure of the private feature vectors using the public data, and then running an efficient private algorithm in the lower dimensional space. The key idea is that public data can be used to identify a low dimensional subspace, which under the appropriate metric acts as a cover for the higher dimensional space. We elucidate the tightness of our upper bound by proving two new lower bounds which show that access to a greater number of unlabeled public samples cannot improve this rate, and that any fewer public samples lead to a worse rate. While dimension independent rates for the GLMs have previously been developed in the _unconstrained_ setting , in the constrained setting which we study, dependence on dimension is known to be unavoidable even for GLMs if no public data is available . Our result thus allows us to bypass these limitations.
* By observing that the key requirement in our GLM result is the construction of an appropriate cover, we extend this result to general hypothesis classes with bounded _fat-shattering dimension_. In the non-private setting, it is known that finiteness of fat-shattering dimension characterizes learnability of real-valued predictors with _scale-sensitive_ losses . In the private setting, such a result is not known, and is in fact impossible in the _proper learning_ setting. This follows from the fact that norm bounded linear predictors, regardless of their (ambient) dimension \(d\), have the same fat-shattering dimension . However, it is known that they are not learnable privately in high dimensions \(d(n)^{2}\). In contrast, in the PA-DP setting, we show that it is possible to properly learn such classes with a rate of roughly \(O(_{n_{}}()+_{>0}(_{}()}{n_{}}+))\), where \(_{n_{}}()\) denotes the Rademacher complexity of \(\) and \(_{}()\) denotes its fat-shattering dimension at scale \(\) (see Section 2 for preliminaries).
* As applications of our result for hypothesis classes with bounded fat-shattering dimension, we obtain guarantees for learning feed-forward neural networks and non-Euclidean GLMs. In particular, for depth \(M\) feed-forward neural networks with weights bounded as \( W_{j}_{F} R_{j}\) and \(1-\)Lipschitz positive homogeneous activation, we achieve an excess risk bound of essentially \((_{j=1}^{M}R_{j}}{}}}+ (^{M}R_{j})^{2}}{n_{}} )^{1/3})\). For non-Euclidean GLMs, our guarantees are dimension-independent which is not known to be achievable, as of yet, even in the unconstrained setting with no public data (unlike Euclidean GLMs).

### Related Work

With regards to labeled public data, the most directly related work to ours is the recent work of . This work proves a lower bound of \((\{}}},}+ \})\) for approximate PA-DP mean-estimation/SCO. We note that our results for approximate PA-DP crucially obtain a dependence on \(d\) that is the key "price" paid for privacy in this setting.  also show a lower bound of \((\{}}},}+ \})\) on a _pure_ PA-DP mean estimation/SCO, but this result only holds when \(d}}\). As such, their result is orthogonal to our result in the pure PA-DP setting, which operates in the regime \(d n\). In both cases, our proof technique is fundamentally different than theirs.3 Tangentially,  showed a small amount of public data is useful in pure-DP mean estimation when the range parameters on the data are unknown.

An important setting where public data is shown to be useful is _PAC learning_. Non-privately, it is known that the finiteness of _VC dimension_ characterizes learnability . However, under DP, it is impossible to PAC learn even the class of _thresholds_, which has VC dimension of one . The works of  showed that given access to a small unlabelled public data, it is possible to go beyond this limitation and privately learn VC classes, essentially by reducing a hypothesis class with finite VC dimension to a finite hypothesis class A number of works have studied the impact of public data in applied settings as well. A common technique is to use public data to reduce the problem dimension in some way . The work of  identified a specific problem instance which supports the method of public pretraining commonly used in practice.

With regards to unlabelled public data, there are several existing works. Transfer learning is a common approach in this setting. Besides the benefits in PAC learning, this setting also has applications in deep learning, where (empirically) unlabeled public data has been used to obtain performance improvements . Unlabeled public data has also yielded impressive results used for pre-training large language models . We also remark that, in practice, it is reasonable to expect the private and public datasets to come from slightly different distributions. Accounting for this distribution shift has also been the study of several recent works . However, in this work we focus on first characterizing the more fundamental problem where the public and private datasets are drawn i.i.d. from the same distribution.

## 2 Preliminaries

Here, we describe the concepts and assumptions used in the rest of this paper. In this work, \(\|\|\) always denotes the \(_{2}\) norm unless stated otherwise.

**Public-Data Assisted Differential Privacy.** We first present the traditional notion of differential privacy (DP). Let \(n,d\) and \(\) be some data domain. When no public data is present, we say that an algorithm \(\) satisfies \((,)\)-differential privacy (DP) if for all datasets \(S\) and \(S^{}\) differing in one data point and all events \(\) in the range of \(\), \([(S)] e^{}[(S^{})]+\).

In our work, we denote the number of public samples in the dataset, \(S=(S_{},S_{})^{n}\), as \(n_{}\) and the number of private samples as \(n_{}\), such that \(n=n_{}+n_{}\). In keeping with previous work , we define public data assisted differentially private algorithms in the following way 4.

**Definition 1** (Pa-Dp).: _An algorithm \(\) is \((,)\) public-data assisted differentially private (PA-DP) algorithm with public sample size \(n_{}\) and private sample size \(n_{}\) if for any public dataset \(S_{}^{n_{}}\), and any pair of private datasets \(S_{},S^{}_{}^{n_{}}\) differing in at most one entry, it holds for any event \(\) that \([(S_{},S_{}) ] e^{}[(S_{},S^{}_{ })]+\). When \(=0\), we refer to this notion as pure PA-DP, denoted as \(\)-PA-DP._

**Stochastic Convex Optimization** Let \(\) be a distribution supported on \(\). Given some constraint set \(^{d}\) of diameter at most \(D\), and a \(G\) Lipschitz convex loss \(:\), we are interested in minimizing the population loss, \(L(w;)=}_{x}[(w;x)]\). Denote the minimizer as \(w^{*}=_{w}\{L(w;)\}\). We evaluate the quality of the approximate solution, \(w\), via the excess risk, \(L(w;)-L(w^{*};)\). Specifically, we are interested in PA-DP algorithms which minimizes this quantity when given \(S_{},S_{}\)\(}{{}}\)\(\). For a dataset \(S\) we also define the empirical loss \((w;S)=_{x S}(w;x)\).

**Supervised Learning and Generalized Linear Models (GLMs)** In the supervised learning setting, in addition to the feature space \(\), we define the label space \(\). We here let \(\) be a joint probability distribution over \(\) and \(_{}\) and \(_{}\) denote the respective marginal distributions. Let \(^{}\) be a hypothesis class of real-valued predictors, and let \(_{}()\) denote its fat shattering dimension at scale \(\). Consider the loss function \(:\), such that \((h;x,y)=_{y}(h(x))\) for some function \(_{y}\). We assume that the map \(_{y}:\) is \(G\)-Lipschitz for all \(y\) and is \(B\)-bounded. Further, we assume that \(_{x}|h(x)| R\) and define \(_{x}\|x\|=\|\|\).

GLMs are a special case of supervised learning setting where the hypothesis class is that of linear predictors, \(=^{d}\), over \(^{d}\), and \(h(x)=w^{}x\). We refer to the public dataset of unlabeled feature vectors as \(X_{}\).

**Covering numbers, fat-shattering and Rademacher Complexity** Given \(X=(x_{1},x_{2},,x_{m})\) the \(_{p}\) distance between two hypothesis \(h_{1},h_{2}\) with respect to the empirical measure over \(X\), is defined as, \(\|h_{1}-h_{2}\|_{p,X}=(_{x X}|h_{1}(x)-h_{2 }(x)|^{p})^{1/p}.\) Similarly, the distance with respect to the population, is given by \(\|h_{1}-h_{2}\|_{p,_{X}}=(_{x_{ X}}|h_{1}(x)-h_{2}(x)|^{p})^{1/p}\). The covering number of \(\) at scale \(>0\) and given dataset \(X\), denoted as \(_{p}(,,X)\) is the size of the minimal set of hypothesis, \(}\), such that for any \(h\) there exists \(\) with \(\|h-\|_{p,X}\). We define \(_{p}(,,m)=_{X:|X|=m}_{p}(,,X)\), the covering number with respect to all datasets of size \(m\). We define fat-shattering dimension below.

**Definition 2**.: _[_2_]_ _Let \(^{}\) and \(>0\). We say that \(\)\(\)-shatters \(X=\{x_{1},x_{2},,x_{m}\}\) if \(_{r^{m}}_{y\{-1,1\}^{m}}_{h}_{i [m]}y_{i}(h(x_{i})-r_{i})\). The fat-shattering dimension, fat\({}_{}()\), is the size of the largest \(\)-shattered set._

We define \(_{m}()\), the worst-case Rademacher complexity of \(\) with respect to \(m\) data points, as \(_{m}()=_{X:|X|=m}_{_{i}}_{h }_{i=1}^{m}_{i}h(x_{i})\). An important example is that of norm-bounded linear predictors \(=\{w:x w,x:\|w\| D\}\) over \(=\{x:\|x\|\|\|\}\). Herein, \(_{}()=(\|\|^{2}} {^{2}})\) and \(_{m}()=(\|}{})\).

## 3 Private Stochastic Convex Optimization with Labeled Public Data

In this section, we present our lower bounds for private stochastic convex optimization with public data. When interpreting the following results, it is helpful to note that in the nontrivial regime, \(n_{}=(n)\) and \(n_{}=o(n)\), although our results hold regardless. Further, recall that an upper bound for this problem of \(OR\{}}},}+}{n}\}\) can be obtained by simply either applying an optimal SCO algorithm to only the public data (and discarding the private data) or applying an optimal DP-SCO algorithm and treating the entire dataset as private . As we will see, this strategy is essentially optimal.

### Lower Bound for Stochastic Convex Optimization

We start by stating our lower bound for public-data assisted differentially private SCO.

**Theorem 1**.: _Let \(\), \( 1\), and \(d\) be larger than some universal constant. For any \((,)\)-PA-DP algorithm, there exists a distribution \(\), and a \(G\)-Lipschitz loss such that \(L((S_{},S_{});)- _{w:\|w\| D}\{L(w;)\}=(GD (n_{},n,d,,))\), where for some universal constant \(c\),_

\[(n_{},n,d,,)=\{}}},}+}{n }\},&d c,\,n_{}])}\\ \{}}},}+}{n }\},&\]

The function \(\) is defined to avoid repetitive notation in the rest of this section. Barring the mild restriction on \(n_{}\), even though the \(\) term is only obtained when \(d n\), the "aggregate" lower bound is tight for all \(d[}{(1/)},n]\) since when \(d}{(1/)}\) the non-private \(}\) lower bound dominates. It is also pertinent to our results in Section 4 that the problem construction used to achieve this lower bound is a convex GLM, and as a result this lower bound holds even for GLMs.

Finally, similar statements can be made about strongly convex optimization. We again provide just one such statement here.

**Theorem 2**.: _Let \(\), \( 1\). For any \((,)\)-PA-DP algorithm there exists a distribution \(\), \(\)-strongly convex and \(G\)-Lipschitz loss such that_

\[L((S_{},S_{});)- _{w:\|w\| D}\{L(w;)\}=(}{}^{2}(n_{},n,,)).\]

The crux of the proofs for both the above results lies in establishing new mean estimation lower bounds for PA-DP mean estimation, which we give in Appendix B.1. These mean estimation lower bound use a novel application of a construction known as fingerprinting codes. In particular, the introduction of public data introduces significant challenges in the traditional analysis of fingerprinting codes. As these challenges are more technical in nature, we defer their discussion to Appendix B.2.

[MISSING_PAGE_FAIL:6]

Our main result for convex Lipschitz losses is the following.

**Theorem 3**.: _Let \(>0,>0\) and \((1/)\). For a \(G\)-Lipschitz, \(B\)-bounded convex loss function, Algorithm 1 satisfies \((,)\)-PA-DP. If the private subroutine \(}\) guarantees the following, with probability at least \(1-\),_

\[(}(_{});_{})-_{w}}(w;_{})=O (GD\|\|(}(1/ )}+}{n_{}} ))\] (1)

_then with \(n_{}=(}}{((2/ )+(1/))^{1/2}})\), with probability at least \(1-\), \(L(;)-L(w^{*};)\) is_

\[O(GD\|\|( }{}}}+}{}}})+}{}}}).\]

We note that DP algorithms such as projected noisy SGD  and the regularized exponential mechanism , both of which can be implemented efficiently, are can be used to achieve (1), since the projected problem is at most \(n_{}\) dimensional.

The above result shows that in the usual regime of \(=(1)\), there is no _price_ of privacy, thereby obtaining the non-private rate of \(O(}}})\). We contrast this with the rate of \(O(}}}+}{n_{}})\), achievable without public data. Our result is better when \(d n_{}\), which is the interesting regime since herein the private error dominates the non-private error. Further, our lower bound (Theorem 4 below) shows that this is the non-trivial regime (for any \(=O(1)\)), since otherwise, even with unlimited public data, the optimal rate is achieved without using any of it. We also note that the above rate is achievable without public data, but in the unconstrained setting where the output \(\) can have very large norm and so may lie outside \(\).

The proof of the result primarily follows from the more general result with fat-shattering hypothesis classes (Theorem 7). We provide the key ideas as well as some details pertaining to linear predictors in Section 4.2 after Theorem 7. The full proof of this result is deferred to Appendix C.

**Lower Bounds.** The above rate as well as the number of public samples used are nearly-optimal. The first claim is due to the following result, which gives a lower bound on excess risk of DP algorithms under full knowledge of the marginal distribution, for Lipschitz GLMs. As unlabeled public data can only reveal information about the marginal distribution, this shows that further unlabeled public samples cannot hope to improve the rate we give in Theorem 3.

**Theorem 4**.: _Let \( 1,\) and \(\) be an \((,)\)-DP algorithm. There exists a \(G\)-Lipschitz convex GLM loss function, and joint distribution \(\) such that given a dataset \(S\) comprising \(n\) i.i.d. samples from \(\) and full knowledge of the marginal distribution \(_{}\), we have the following:_

\[_{,S}[L((S);)-_{w:\|w\|  D}L(w;)]=(GD\|\|( }+\{},}{n }\}))\]

We note that the bound with \(}{n_{}}\) can be achieved without using any public data via standard results . This result is largely a corollary of [1, Theorem 6]. We provide full details in Appendix C.3.1.

To establish optimality of public sample complexity, we give the following lower bound which shows that \((n_{})\) samples are necessary to achieve the above rate. See Appendix C.3.2 for proof.

**Theorem 5**.: _Let \(n_{},n_{},d, 1,<}}\) and \(d=(n_{})\). If there exists an \((,)\)-PA-DP algorithm \(\), which, for any \(G\)-Lispchitz convex GLM, achieves excess risk \([L((X_{},S_{});)- _{w:\|w\| D}L(w;)]=OGD\| \|}}}+}{}}},\) for \(S_{}^{n_{}}\) and \(X_{}_{}^{n_{}}\), then \(n_{}=(}}{(1/)})\)._

**Optimistic rates.** We now consider additional assumptions that the loss function is non-negative and \(H\)-smooth, such as in the case of linear regression where \(_{y}(a)=(a-y)^{2}\). This is a well-studied setting  especially since it allows for obtaining optimistic rates: those that interpolate between a slow worst-case rate and a faster rate under (near) realizability or interpolation conditions. The main result is the following.

**Theorem 6**.: _Let \(>0,>0\) and \((1/)\). For a \(G\)-Lipschitz, \(B\)-bounded non-negative \(H\)-smooth loss function, Algorithm 1 satisfies \((,)\)-PA-DP. If the private subroutine \(}\) guarantees Equation (1) with probability at least \(1-\), then with \(n_{}=( n_{})^{2/3}}{G^{2/3}((1/))^{1/3}}+}}^{*};S_{})}}{G }\), with probability at least \(1-\),_

\[L(;)-(^{*};S_{}) =D\|X\|}{}}}+}}}(^{*};S_{})}+D\|X\| (^{*};S_{})^{1/4}}{}}}\] \[+(} }+(D^{2}\|X\|^{2}G}{n_{}} )^{2/3}+D^{2}}{n_{}}+}})\]

_where \(^{*}\) is the minimizer of \(\) w.r.t \(S_{}\) and \(\) hides \(((1/),(1/))\) terms._

A similar result as above can be obtained with \((^{*};S_{})\) replaced by \(L(w^{*};)\) above - see Theorem 14 for the full theorem statement. This rate, in the worst-case, is essentially the same as that of Theorem 3, which is \((}}}+}} })\). However, optimistically, when \(L(w^{*};)\) or \((^{*};S_{})\) is small, we get a faster rate of \((}}+})^{2 /3}})\). We note that this is seemingly weaker than what is known in the unconstrained setting, where  obtained a worst-case rate of \(}}}+} )^{2/3}}\). We show that we can recover this faster rate under an extra assumption that the global minimizer of the risk, lies in the constraint set \(\) - note that this is trivially true in the unconstrained setup; see Theorem 15 for the statement.

We note that projected noisy SGD  and the regularized exponential mechanism , both of which can be implemented efficiently, are possible choices for the private sub-routine \(}\) that realize the above theorem statements.

### PA-DP Supervised learning of Fat-Shattering Classes

In this section, we consider a general supervised learning setting with fat-shattering hypothesis classes and potentially non-convex losses, with unlabeled public data. Our proposed algorithm is similar to that of , which uses the public unlabeled data to construct a small finite, yet representative, subset of the hypothesis class. Our construction uses a cover of the hypothesis class with respect to the \(_{2}\) distance of predictions on the public data points. We then use the exponential mechanism to privately select a hypothesis using the empirical loss on private data as the score function.

We note that we operate under the pure DP setting (as opposed to approximate DP). Our techniques are based on selection which do not exhibit improved guarantees under approximate DP. Further, we note that, without public data, with non-convex losses, there is no separation of optimal rates between pure and approximate DP .

**Input:** Datasets \(X_{}\) and \(S_{}\), privacy parameter \(>0\), scale of cover \(>0\), \(>0\).

```
1: Construct \(}\), a minimal \(\)-cover of \(\), with respect to the following metric \[\|h_{1}-h_{2}\|_{2,X_{}}=}} _{x X_{}}(h_{1}(x)-h_{2}(x))^{2}}\]
2: Return \(\) sampled with probability \(p(h)(-(h;S_{}))\) over \(h}\) ```

**Algorithm 2** Supervised private learning with public unlabeled data

Our main result for the Lispchitz setting is the following.

**Theorem 7**.: _Algorithm 2 with \(=}}\) satisfies \(\)-PA-DP. For any \(>0\) and \(n_{}=O((2/)}{^{2}}, m:^{3}(m)_{m}^{2}()^{2} })<\), with probability at least \(1-\), we have \(L(;)-_{h}L(h;)\) is at most_\[2G_{n_{}}()+O(}{}}})+(_{}()+(4/ ))}{n_{}})+2G,\]

_where \(c\) is an absolute constant._

Our result shows that the model of PA-DP with unlabeled public data allows for obtaining non-trivial rates for supervised learning with any fat-shattering class, as is the case in the non-private setting. Further, in many standard settings, such as that of (Euclidean) GLMs, the Rademacher complexity is \(_{m}()=O(})\) which implies that \(_{}()=O(})\) (see Theorem 9). In those cases, our guarantee simplifies to essentially yield a rate of \(O(_{n_{}}()+} )^{1/3}}+}}})\) - see Corollary 4 for the exact statement for GLMs.

**Proof Idea.** We briefly discuss some main ideas in the proof. The key is to show that if \(}\) is a cover of \(\) with respect to the _empirical distance_ on public feature vectors, \(_{2,X_{}}\), then with enough public feature vectors, it is also a cover with respect to the _population distance_\(_{2,_{}}\). This is captured in the following result.

**Lemma 1**.: _Let \(}\) be a \(\)-cover of \(\) with respect to \(_{2,X_{}}\). For \(n_{}=O((1/)}{^{2}}, \{m:^{3}(m)_{m}^{2}()^{2} \})<\), for every \(h\), with probability at least \(1-\), there exists \(}\) such that \( h-_{2,_{}}+\)._

This result allows us to appropriately approximate a hypothesis class with enough public unlabeled points. This approximation roughly translates to the same additive error in the final bound while concurrently allowing for the use of the smaller finite hypothesis class \(}\) of size \(}=(_{}( ))\).

For linear predictors with convex losses, as in Theorem 3, we show that the \((X_{})\) is a valid \(0\)-cover w.r.t. \(_{2,X_{}}\). However, the cover being continuous and convex allows application of convex optimization techniques (as opposed to selection, as above), thereby obtaining stronger results with efficient procedures. The above procedure yields optimistic rates for non-negative and smooth losses; see Theorem 17 for details.

#### 4.2.1 Application: Neural Networks

In this section, we instantiate our general result to give a guarantee for learning feed-forward neural networks in the PA-DP setting. We use the result of  but note that other results which give bounds on the Rademacher complexity of neural networks, such as  can also be used.

We consider a depth \(M\) feed-forward neural network which implements the function \(x W_{M}((W_{M-1}(W_{1}x)))\). Here, \(W_{1},W_{2},,W_{M}\) are the weight matrices and \(\) is a (non-linear) activation function. We consider \(1\)-Lipschitz positive-homogeneous activation such as the ReLU function, \((z)=(0,z)\), applied coordinate-wise. Our main result is the following.

**Corollary 2**.: _Let \((R_{j})_{j=1}^{M}\) be a sequence of scalars and \(M\). In the setting of Theorem 7 with \(=\{x^{d}: x \}\) and \(\) being the class of depth \(M\) feed-forward neural networks, with \(1\)-Lipschitz positive-homogenous activation, and weight matrices, bounded as \( W_{j}_{F} R_{j}\), with \(n_{}=(((_{j=1} ^{M}R_{j}))^{2/3}(n_{})^{2/3}M^{1/3}(2/))\), with probability at least \(1-\), \(L(;)-_{h}L(h;)\) is at most_

\[O(_{j=1}^{M}R_{j}}{ }}}+}{}}}+}})+ ((M^{2}( _{j=1}^{M}R_{j})^{2}}{n_{}})^{1/3}).\]

We note that the above result has a polynomial dependence on the depth \(M\), which is a consequence of the (non-private) Rademacher complexity of . It is also possible to get fully size-independent bounds by utilizing such existing results, however they require more stringent norm bounds on the weight matrices . Further, a similar result follows for non-negative smooth losses from , but we omit this extension for brevity.

#### 4.2.2 Application: Non-Euclidean GLMs

In the non-Euclidean GLM setting, we consider \((,\|\|)\) as a \(d\) dimensional (where \(d\{\}\)) Banach space, and \((,\|\|_{*})\) is its dual space. The feature vectors \(x\) are bounded as \(=\{x:\|x\|\|\|\}\) and \(\{w:(w) D^{r}\}\) where \(\) is a \(r\)-uniformly convex function6 with respect to \(\|\|_{*}\). A canonical example is the \((_{p},_{q})\)-setup , wherein the functions \((w)=\|w\|_{1+(1/(d))}^{2}\), \((w)=\|w\|_{p}^{2}\) and \((w)=}{p}\|w\|_{p}^{p}\) are \(2\), \(2\) and \(p\)-uniformly convex with respect to \(\|\|_{p}\) for \(p=1\), \(1<p 2\) and \(p 2\) respectively.

The GLM loss function \((w;x,y)=_{y}( w,x)\) where \(,: \) is a duality pairing. In this case, the Rademacher complexity of linear functions, is bounded as \(O(\|}{m^{1/r}})\), where \(s\) is the conjugate of \(r\) i.e. \(+=1\) (see, e.g. ). We obtain the following result by instantiating Theorem 7 with the Rademacher complexity and fat-shattering dimension of non-Euclidean GLMs.

**Corollary 3**.: _In the setting of Theorem 7, together with \(=\{x^{d}:\|x\|\| \|\}\) and \(=\{x w,x,x, (w) D^{r}\}.\) Given \(n_{}=((n_{})^{r/(r+1)}(2/ ))\), with probability at least \(1-\), \(L(;)-_{w}L(w;)\) is at most_

\[(GD\|\|(}^{1/r} }+}{}}}+})^{}}+}})+}{}}}).\]

The above yields guarantees for the special case of \((_{p},_{q})\)-setup with \(r=\{2,p\}\). We remind that in the (constrained) _convex_ Euclidean GLM setting, our dimension-independent rate in Theorem 3, with public unlabeled data recover the rates which were known to be achievable in the unconstrained setting. Further the above rate for \(p=1\) case can be used to obtain guarantees for the polyhedral setting with \(\|w\|_{1} D\) constraint, resulting in a \(O(}}}+(}})^{1/3})\) rate. We note that  showed a rate of \((}+}{})\) for this setting, with convex losses without public data. Importantly, for the other cases, i.e. \(p>1,p 2\), there are no such (nearly) dimension-independent analogs of our result without public data, as of yet.