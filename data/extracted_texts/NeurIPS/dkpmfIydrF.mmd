# Defensive Unlearning with Adversarial Training

for Robust Concept Erasure in Diffusion Models

 Yimeng Zhang\({}^{1}\) Xin Chen\({}^{2}\) Jinghan Jia\({}^{1}\) Yihua Zhang\({}^{1}\) Chongyu Fan\({}^{1}\) Jiancheng Liu\({}^{1}\) Mingyi Hong\({}^{3}\) Ke Ding\({}^{2}\) Sijia Liu\({}^{1,4}\)

\({}^{1}\)Michigan State University

\({}^{3}\)University of Minnesota, Twin City

\({}^{2}\)Applied ML, Intel

\({}^{4}\)MIT-IBM Watson AI Lab, IBM Research

###### Abstract

Diffusion models (DMs) have achieved remarkable success in text-to-image generation, but they also pose safety risks, such as the potential generation of harmful content and copyright violations. The techniques of _machine unlearning_, also known as _concept erasing_, have been developed to address these risks. However, these techniques remain vulnerable to adversarial prompt attacks, which can prompt DMs post-unlearning to regenerate undesired images containing concepts (such as nudity) meant to be erased. This work aims to enhance the robustness of concept erasing by integrating the principle of adversarial training (AT) into machine unlearning, resulting in the robust unlearning framework referred to as AdvUnlearn. However, achieving this effectively and efficiently is highly non-trivial. First, we find that a straightforward implementation of AT compromises DMs' image generation quality post-unlearning. To address this, we develop a utility-retaining regularization on an additional retain set, optimizing the trade-off between concept erasure robustness and model utility in AdvUnlearn. Moreover, we identify the text encoder as a more suitable module for robustification compared to UNet, ensuring unlearning effectiveness. And the acquired text encoder can serve as a plug-and-play robust unlearner for various DM types. Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility. To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing. Codes are available at https://github.com/OPTML-Group/AdvUnlearn.

**Warning: This paper contains model outputs that may be offensive in nature.**

## 1 Introduction

Recent rapid advancements in diffusion models (DMs) [1; 2; 3; 4; 5; 6; 7; 8] have popularized the realm of text-to-image generation. These models, trained on extensive online datasets, can generate remarkably realistic images. However, their training heavily relies on diverse internet-sourced content and can introduce safety concerns when prompted with inappropriate texts, such as the generation of NSFW (Not Safe For Work) images, highlighted in several studies [9; 10]. To address this concern, post-hoc safety checkers were initially applied to DMs [10; 11]. However, they were later found to be inadequate in effectively preventing the generation of unsafe content. To further enhance safety, the concept of _machine unlearning_ (**MU**) has been introduced [12; 13; 14; 15; 16; 17; 18], aiming to mitigate the influence of undesired textual concepts in DM training or fine-tuning [19; 20; 21; 22]. As a result, DMs post-unlearning (referred to as 'concept-erased DMs' or 'unlearned DMs') are designed to negate the generation of undesirable content, even when faced with inappropriate prompts.

Despite the recent progress with unlearned, safety-driven DMs, recent studies [23; 24; 25; 26] have shown that these models remain vulnerable to generating unsafe images when exposed to _adversarial prompt attacks_, which involve minor adversarial perturbations in input prompts. These attacks can readily jailbreak concept-erased DMs to regenerate content subject to the concept targeted for unlearning, even if these DMs perform well against inappropriate prompts in a non-adversarial context. In **Fig. 1**, we exemplify the generation of the stable diffusion (SD) v1.4 model before and post unlearning the 'nudity' concept. The unlearned model is confronted with an inappropriate prompt from the I2P dataset  and its adversarial prompt counterpart, generated using the attack method UnlearnDiffAtk . The lack of robustness in concept erasing (or machine unlearning) in DMs gives rise to the key research question tackled in this work:

_(Q) Can we effectively and efficiently boost the robustness of unlearned DMs against adversarial prompt attacks?_

To address (Q), we take inspiration from the successes of adversarial training (AT)  in enhancing the adversarial robustness of image classification models. To the best of our knowledge, we are the first to study the integration of AT into DM unlearning systematically and to develop a successful integration scheme, termed AdvUnlearn, by addressing its unique effectiveness and efficiency challenges, such as balancing the preservation of image generation quality and selecting the appropriate module to optimize during AT. In the literature, the most relevant work to ours is , which employs AT to train robust adapters within UNet for DMs. However, our work significantly differs from . First, we aim for a comprehensive study of AT for DMs, focusing not only on when AT is (in)effective for DMs and why this (in)effectiveness occurs but also on how to improve it. Additionally, we explore which advancements in AT for robust image classification can be translated into improving the robustness of DMs. Second, we identify that retaining image generation quality is a major challenge when integrating AT into DMs, especially in compatibility with DM unlearning methods. We tackle this challenge by drawing inspiration from the AT principle that 'unlabeled data improves the robustness-accuracy tradeoff' [31; 32; 33; 34; 35], and accordingly develop a utility-retaining regularization scheme based on an augmented retain prompt set. Third, by dissecting DMs into text encoder and UNet components, we discover that the integration of AT with DM unlearning particularly favors the text encoder module. This contrasts with conventional DM unlearning methods, which are typically applied to the UNet.

We summarize our key **contributions** as follows:

We explore the integration of AT with concept erasing (or machine unlearning) in DMs and propose a bi-level optimization (BLO)-based integration scheme, termed AdvUnlearn. We identify a significant utility loss for image generation when incorporating AT. To address this, we design a utility-retaining regularization using curated external retain prompt data to balance the trade-off between effective unlearning and high-quality image generation.

We demonstrate that optimizing the text encoder within AdvUnlearn can enhance the robustness of unlearned DMs against adversarial prompt attacks, outperforming the conventional strategies for unlearning UNet. In addition, it also achieves a better balance between unlearning performance and image generation utility. Furthermore, we show that a single robust text encoder can be shared across different DMs and implemented in a plug-and-play manner, greatly enhancing usability.

Figure 1: Visualizations of SD v1.4  and its concept-erased version, ESD , conditioned on the prompt ‘_Hot sexy girl_’. The generation of ESD is shown against no attack and UnlearnDiffAtk .

Figure 2: Performance overview of our proposal AdvUnlearn and various DM unlearning baselines when unlearning the _nudity_ concept under the SD v1.4 model. The robustness is measured by attack success rate (ASR) against UnlearnDiffAtk . The performance of image generation retention is assessed through Frechet Inception Distance (FID). A lower ASR or FID implies better robustness or utility. The baselines include the vanilla SD v1.4 and its unlearned versions using ESD , FMN , UCE , SalUn , and SPM .

We validate the effectiveness of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. We show that AdvUnlearn yields significant robustness improvements over state-of-the-art (SOTA) unlearned DMs while preserving a commendable level of image generation utility; See **Fig. 2** for justification and performance highlights.

## 2 Related Work

**Machine unlearning for concept erasing in DMs.** Generating visually authentic images from textual descriptions remains a compelling challenge in generative AI. DMs (diffusion models) have notably advanced, surpassing generative adversarial networks (GANs) in various aspects, particularly in conditional generation subject to text prompts [36; 37; 38; 39; 40; 41; 42; 43; 44]. Despite their success, DMs also present safety and ethics concerns, especially in generating images with harmful content when conditioned on inappropriate prompts [9; 10]. To address these concerns, several approaches have been proposed, including post-image filtering , modifying inference guidance , and retraining with curated datasets . However, lightweight interventions like the first two may not fully address the model's inherent propensity to generate controversial content [10; 45; 46]. MU (machine unlearning) [47; 48; 14] is another emerging approach for ensuring safe image generation by erasing the influence of undesired concepts, also referred to as concept erasing in DMs. Leveraging MU principles, various strategies for designing _unlearned_ DMs have been explored, focusing on refining fine-tuning methods such as those by [19; 20; 21; 22; 27; 28; 30; 49; 50; 51; 52; 53; 54; 55; 56]. For example, UNets within DMs have been fine-tuned to redirect outputs towards either random or anchored outputs, effectively preventing the generation of images associated with the concepts designated for unlearning [19; 20; 21]. Additional efforts [27; 54] have employed gradient-based techniques to map out the weight saliency within UNet related to the concept to be unlearned, concentrating fine-tuning efforts on these salient weights. To enhance the efficiency of unlearning, UCE  introduces a method of closed-form parameter editing specifically for DM unlearning. However, this approach lacks robustness against jailbreaking attacks .

**Adversarial prompt attacks against safety-driven DMs.** Adversarial prompts or jailbreaking attacks specifically manipulate the text inputs of DMs to produce undesirable outcomes. Similar to the text-based attacks in natural language processing (NLP), adversarial prompt attacks can involve character or word-level manipulations, such as deletions, additions, and substitutions [57; 58; 59; 60; 61; 62; 63; 64; 65]. Strategies discussed in  are designed to bypass NSFW safety protocols, cleverly evading content moderation algorithms. Other related attacks [67; 68; 23; 24; 25; 26; 27; 26], coerce DMs into generating images that contradict their programmed intent. For instance, Pham et al.  used textual inversion  to find a continuous word embedding representing the concept to be unlearned by the model. Chin et al.  employed ground truth guidance from an auxiliary frozen UNet  and discrete optimization techniques from  to craft a white-box adversarial prompt attack. To overcome the dependency on auxiliary model guidance, UnlearnDiffAtk  leveraged the intrinsic classification capabilities of DMs, facilitating the creation of adversarial prompts. In this work, we treat machine unlearning for DMs as a defensive challenge. Our approach involves fine-tuning the target model to not only unlearn specific concepts but also to enhance its robustness against adversarial prompt attacks.

**Adversarial training (AT).** In the realm of image classification, adversarial attacks that generate subtle perturbations to fool machine learning (ML) models have long posed a robustness challenge for vision systems [71; 72; 73; 74; 75; 76]. In response, AT (adversarial training) , the cornerstone of training-based defenses, conceptualizes defense as a two-player game between the attacker and defender [77; 78; 79; 80; 71; 72; 71; 73; 74; 76]. Additionally, TRADES  was proposed to strike a better balance between accuracy and robustness. Further studies [80; 81; 32; 85; 86] demonstrated that unlabeled data and self-training have proven effective in enhancing robustness and generalization in adversarial contexts. To improve the efficiency of AT, past research also proposed adopting more efficient attack methods or fewer steps to generate adversarial examples [71; 87; 88; 89; 90; 91; 92; 93; 94]. In particular, the fast gradient sign method (FGSM) was utilized for adversarial generation in AT [71; 87]. And the gradient alignment strategy was proposed to improve the quality of fast AT .

## 3 Preliminaries and Problem Statement

Throughout the work, we focus on latent diffusion models (LDMs) [7; 95], which have exceled in text-to-image generation by integrating text prompts (such as text-based image descriptions) into image embeddings to guide the generation process. In LDMs, the diffusion process initiates with a random noise sample drawn from the standard Gaussian distribution \((0,1)\). This sample undergoes a progressive transformation through a series of \(T\) time steps in a gradual denoising process, ultimately resulting in the creation of a clean image \(\). At each time step \(t\), the diffusion model utilizes a noise estimator \(_{}(|c)\), parameterized by \(\) and conditioned on an input prompt \(c\) (_i.e._, associated with a textual concept). The diffusion process operates on the latent representation of the image at each time (\(_{t}\)). The training objective for \(\) is to minimize the denoising error as below:

\[}{}_{( _{},c),t,(0,1)}[\| -_{}(_{t}|c)\|_{2}^{2}],\] (1)

where \(\) is the training set, and \(_{}(_{t}|c)\) is the LDM-associated noise estimator.

**Concept erasure in DMs.** DMs, despite their high capability, may generate unsafe content or disclose sensitive information when given inappropriate text prompts. For example, the I2P dataset  compiles numerous inappropriate prompts capable of leading DMs to generate NSFW content. To mitigate the generation of harmful or sensitive content, a range of studies  have explored the technique of concept erasing or machine unlearning within DMs, aiming to enhance the DM training process by mitigating the impact of undesired textual concepts on image generation.

A widely recognized concept erasing approach is ESD , notable for its state-of-the-art (SOTA) balance between unlearning effectiveness and model utility preservation . Unless specified otherwise, we will adopt the objective of ESD for implementing concept erasure. ESD facilitates the fine-tuning process of DMs by guiding outputs away from a specific concept targeted for erasure. Let \(c_{}\) denote the concept to erase, then the diffusion process of ESD is modified to

\[_{}(_{t}|c_{})_{ _{}}(_{t}|)-(_ {_{}}(_{t}|c_{})-_{ _{}}(_{t}|)),\] (2)

where \(\) denotes the concept-erased DM, \(_{}\) is the originally pre-trained DM, and \(_{}(_{t}|)\) represents unconditional generation of the model \(\) by considering text prompt as empty. Compared to the standard conditional DM  (with classifier-free guidance), the second term \(-[_{_{}}(_{t}|c_{}) -_{_{}}(_{t}|)]\) encourages the adjustment of the data distribution (with erasing guidance parameter \(>0\)) to minimize the likelihood of generating an image \(\) that could be labeled as \(c_{}\). To optimize \(\), ESD performs the following model fine-tuning based on (2):

\[}{}_{}(,c_{}):=[\|_{ }(_{t}|c_{})-(_{_{}}(_{t}|)-(_{_{}}(_{t}|c_{})-_{ _{}}(_{t}|)))\|_{2}^{2}],\] (3)

where for notational simplicity we have used, and will continue to use, to omit the time step \(t\) and the random initial noise \(\) under expectation.

**Adversarial prompts against concept-erased DMs.** Although concept erasing enhances safety, recent studies  have also shown that concept-erased DMs often lack robustness when confronted with _adversarial prompt attacks_; see Fig. 1 for examples. Let \(c^{}\) represent a perturbed text prompt corresponding to \(c\), obtained through token manipulation in the text space  or in the token embedding space . The generation of adversarial prompts can be solved as :

\[-c\|_{0}}{} [\|_{}(_{t}|c^{})- _{_{}}(_{t}|c)\|_{2}^{2} ],\] (4)

where \(\) denotes the concept-erased DM, and \(_{}\) is the original DM without concept erasing. Therefore, considering the concept \(c=c_{}\) targeted for erasure, \(_{_{}}(_{t}|c)\) denotes the generation of an unsafe image under \(c\). The objective of problem (4) is to devise the perturbed prompt \(c^{}\) to steer the generation of the concept-erased DM \(\) towards the unsafe content produced by \(_{_{}}\). The constraint of (4) implies that \(c^{}\) remains proximate to \(c\), subject to the number of altered tokens \(\) (measured by the \(_{0}\) norm) or via additive continuous perturbation in the token embedding space.

**AdvUnlearn: A defensive unlearning setup via AT.** The lack of adversarial robustness in concept-erased DMs motivates us to devise a solution that enhances their robustness in the face of adversarial prompts. AT  offers a principled algorithmic framework for addressing this challenge. It formulates robust concept erasure as a two-player game involving the defender (_i.e._, the unlearner for concept erasing) and the attacker (_i.e._, the adversarial prompt). The original AT constrains the attacker's objective to precisely oppose the defender's objective. To loosen this constraint, we consider a generalized AT formulation based on bi-level optimization , where the defender and attacker are delineated through the upper-level and lower-level optimization problems, respectively:

\[}{}_{}(,c^{*}) (5), the upper-level optimization aims to optimize the DM parameters \(\) according to an unlearning objective \(_{}\), considering the concept \(c^{*}\) targeted for erasure. For instance, the objective of ESD (3) could serve as one specification of \(_{}\). On the other hand, the lower-level optimization problem minimizes the attack generation loss \(_{}\), as given by (4), to acquire the optimized adversarial prompt \(c^{*}\) under the current model \(\). The upper-level and lower-level optimizations are interlinked through the alternation between model parameter optimization and adversarial prompt optimization.

We designate the aforementioned setup (5) of integrating adversarial training into DM unlearning as AdvUnlearn. As will become evident later, _effectively_ and _efficiently_ solving the AdvUnlearn problem (5) becomes highly nontrivial. There exist two **main challenges**.

(**Effectiveness challenge**) As will be demonstrated in Sec. 4, a naive implementation of the ESD objective (2) for upper-level concept erasure may lead to a considerable loss in DM utility for generating normal images. Thus, optimizing the inherent trade-off between the robustness of concept erasure and the preservation of DM utility poses a significant challenge.

(**Efficiency challenge**) Moreover, given the modularity characteristics of DMs (with decomposition into text encoder and UNet encoder), determining the optimal application of AT and its efficient implementation remains elusive. This includes deciding 'where' to apply AT within DM, as well as 'how' to efficiently implement it. We will address this challenge in Sec. 5.

## 4 Effectiveness Enhancement of AdvUnlearn: Improving Tradeoff between Robustness and Utility

**Warm-up: Difficulty of image generation quality retention.** A straightforward implementation of AdvUnlearn (5) is to specify the upper-level optimization using ESD (2) and combine it with adversarial prompt generation (4). However, such a direct integration results in a notable decrease in image generation quality. **Tab. 1** compares the performance of the vanilla ESD (_i.e._, concept-erased stable diffusion )  with its direct AT variant. The robustness of concept erasure is evaluated using ASR (attack success rate) against the adversarial prompt attack UnlearDiffAtk . Meanwhile, the quality of image generation retention is assessed through FID. As we can see, while the direct AT variant of ESD (AT-ESD) enhances adversarial robustness with approximately a 20% reduction in ASR, it also leads to a considerable increase in FID. **Fig. 3** presents visual examples of the generation produced by AT-ESD compared to the original SD v1.4 and vanilla ESD. As demonstrated, the decline in image generation authenticity under a benign prompt using AT-ESD is substantial.

**Utility-retaining regularization in AdvUnlearn.** We next improve the unlearning objective \(_{}\) in AdvUnlearn (5) by explicitly prioritizing the retention of the DM's generation utility. One potential explanation for the diminished image generation quality after AT-ESD is that ESD (2) primarily focuses on de-generating the unlearning concept in the diffusion process, thus lacking the capability to preserve image generation quality when further pressured by the robustness enhancement induced by AT. In the realm of AT for image classification, the integration of external (unlabeled) data into AT has proven to be an effective strategy for enhancing standard model utility (_i.e._, test accuracy) while simultaneously improving adversarial robustness . Drawing inspiration from this, we suggest the curation of a retain set \(_{}\) comprising additional text prompts utilized for retaining model utility. Together with the ESD-based unlearning objective, we customize the upper-level optimization objective of (5) as

\[_{}(,c^{*})=_{}(,c^{*})+ _{_{}}[\|_{ }(_{t}|)-_{_{}}( _{t}|)\|_{2}^{2}],\] (6)

where \(_{}\) was defined in (3), and the second loss term penalizes the degradation of image generation quality using the current DM \(\) compared to the original \(_{}\) under a retained concept \(\).

  
**Unlearning Methods** & **Concept** & ASR & FID \\
2 & **Erasure** & (\(\)) & (\(\)) \\  SD v1.4 & ✘ & 100\% & 16.7 \\ ESD & ✔ & 73.24\% & 18.18 \\ AT-ESD & ✔ & 43.48\% & 26.48 \\   

Table 1: Robustness (ASR) and utility (FID) of different unlearning methods (ESD  and AT-ESD) on base SD-v.14 model for nudity unlearning.

Figure 3: Generation examples using DMs in Tab. 1 for nudity unlearning conditioned on benign and harmful prompts.

When selecting the retain set \(_{}\), it is essential to ensure that the enhancement in image generation quality does _not_ come at the expense of the effectiveness of concept erasure, _i.e._, minimizing ESD loss in (6). Therefore, we utilize a large language model (LLM) as a judge to sift through these retain prompts, excluding those relevant to the targeted concept for erasure. Further details regarding the LLM judge system are available in **Appx.** A. We obtain these retain prompts from an external dataset, such as ImageNet  or COCO , using the prompt template 'a photo of [OBJECT CLASS]'. The finalized retain set \(_{}\) consists of \(243\) distinct prompts. During training, a prompt batch of size \(5\) randomly selected from \(_{}\) in support of utility-retaining regularization. The primary goal of \(_{}\) is not to train the model on producing specific objects or concepts; Instead, it aims to guide the model in generating general, non-forgetting content effectively. As will be evidenced in Figs. 4-6, incorporating \(_{}\) enhances the general utility of the unlearned DM during the testing phase. Test-time prompts in these figures include varied objects like 'toilet', 'Picasso', and 'cassette player' not part of \(_{}\), demonstrating the unlearned model's generalization capabilities.

As shown in **Tab.**2, our proposed utility-retaining regularization effectively recovers the utility (_i.e._, FID of AdvUnlearn vs. that of ESD), which is otherwise compromised by AT-ESD. Yet, AdvUnlearn in Tab. 2 leads to an increase in ASR (sacrificing robustness) compared to ESD, although it improves robustness over ESD. Thus, there is room for further enhancement in AdvUnlearn. As will be shown in Sec. 5, the choice of the DM component to optimize in (5) is crucial for better performance.

## 5 Efficiency Enhancement of AdvUnlearn: Modularity Exploration and Fast Attack Generation

**Where to robustify: Text encoder or UNet?** Initially, concept erasure by ESD (3) was confined to the UNet component of a DM . However, as shown in Tab. 2, optimizing UNet alone does not lead to sufficient robustness gain for AdvUnlearn. Moreover, there are efficiency benefits if concept erasure can be performed on the _text encoder_ instead of UNet. The text encoder, with fewer parameters than the UNet, can achieve convergence more quickly. Most importantly, a text encoder that has undergone the unlearning process with one DM could possibly serve as a _plug-in_ unlearner for other DMs, thereby broadening its applicability across various DMs. Furthermore, a recent study  demonstrates that causal components corresponding to the DM's visual generation are concentrated in the text encoder. Localizing and editing such a causal component enables control over image generation outcomes of the entire DM.

Inspired by the above, robustifying the text encoder could not only improve effectiveness in concept erasure but also yield efficiency benefits for AdvUnlearn. **Tab.**3 extends Tab. 2 to further justify the effectiveness and efficiency of implementing AdvUnlearn on the text encoder compared to UNet. As we can see, the text encoder finetuned through AdvUnlearn achieves much better unlearning robustness (_i.e._, lower ASR) than AdvUnlearn applied to UNet (_i.e._, AdvUnlearn in Tab. 2), without loss of model utility as evidenced by FID. Although applying ESD to the text encoder can also improve ASR, it leads to a significant utility loss compared to us vanilla version applied to UNet . This highlights the importance of retaining image generation quality considered in AdvUnlearn when optimizing the text encoder. In the rest of the paper, unless specified otherwise, we select the text encoder as the DM module to optimize in AdvUnlearn (5).

**Fast attack generation in AdvUnlearn.** Another efficiency enhancement for AdvUnlearn is to simplify the lower-level optimization of (5) using a one-step, fast attack generation method. This approach aligns with the concept of fast AT in image classification [87; 92]. The rationale is that the lower-level problem of (5) can be approximated using a quadratic program , and solving it can be achieved using the fast gradient sign method (FGSM) [71; 87]. Specifically, let \(\) represent the perturbation added to the text prompt \(c\), _e.g._, via a prefix vector . With an abuse of notation, we

  
**Unlearning** & **Utility Retaining** & ASR & FID \\
**Methods** & **by COCO** & (\(\)) & (\(\)) \\  SD v1.4 & N/A & 100\% & 16.70 \\ ESD & ✗ & 73.24\% & 18.18 \\ AT-ESD & ✗ & 43.48\% & 26.48 \\ AdvUnlearn & ✗ & 64.79\% & 19.88 \\   

Table 2: Performance evaluation of SD v1.4 (without unlearning), ESD, AT-ESD, and AdvUnlearn in nudity unlearning.

    & **Optimized DM** & ASR & FID \\  & **component** & (\(\)) & (\(\)) \\  SD v1.4 & N/A & 100\% & 16.70 \\ ESD & UNet & 73.24\% & 18.18 \\ ESD & Text Encoder & 3.52\% & 59.10 \\ AdvUnlearn & UNet & 64.79\% & 19.88 \\ AdvUnlearn & Text Encoder & 21.13\% & 19.34 \\   

Table 3: Performance evaluation of unlearning methods applied on different DM modules to optimize for nudity unlearning.

denote the perturbed prompt by \(c^{}=c+\), where the symbol \(+\) represents the prefix attachment. FGSM determines \(\) using FGSM to solve the lower-level problem of (5):

\[=_{0}-(_{}(,c+_{0})),\] (7)

where \(_{0}\) represents random initialization, \(\) denotes the step size, and \(()\) is element-wise sign operation. We refer to the utilization of one-step attack generation (7) in AdvUnlearn as its fast variant, which can also yield a substantial robustness gain in concept erasure. **Tab.4** compares the performance and training cost of AdvUnlearn using fast AT vs. (standard) AT, where the attack step of standard AT is set to 30. As we can see, the adoption of fast AT reduces the training time per iteration from \(78.57s\) to \(12.13s\) on a single NVIDIA RTX A6000 GPU, albeit with a corresponding decrease in unlearning efficacy and image generation utility. Therefore, when the need for unlearning efficacy is not exceedingly high and computational efficiency is prioritized, adopting fast AT can be an effective solution. We summarize the AdvUnlearn algorithm in **Appx.**B.

## 6 Experiments

### Experiment Setups

**Concept-erasing tasks, datasets, and models.** We categorize existing concept-erasing tasks  into three main groups for ease of evaluation. (1) _Nudity unlearning_ focuses on preventing DMs from generating harmful content subject to nudity-related prompts . (2) _Style unlearning_ aims to remove the influence of an artistic painting style in DM generation, which mimics the degeneration of copyrighted information such as the painting style . (3) _Object unlearning_, akin to the previous tasks, targets the degeneration of DMs corresponding to a specific object . The dataset for testing _nudity unlearning_ is derived from the inappropriate image prompt (**I2P**) dataset , whereas the testing dataset for _style unlearning_ is aligned with the setup described in . In the scenario of _object unlearning_, GPT-4  is utilized to generate \(50\) distinct text prompts for each object class featured in Imagenette . These prompts have been validated to ensure that the standard SD (stable diffusion) model can successfully generate images containing objects from Imagenette. Model-wise, unless specified otherwise, the pre-trained SD (Stable Diffusion) v1.4 is utilized as the base DM in concept erasing.

**DM unlearning baselines.** We include **8** open-sourced DM unlearning methods as our baselines: (1) **ESD** (erased stable diffusion) , (2) **FMN** (Forget-Me-Not) , (3) **AC** (ablating concepts) , (4) **UCE** (unified concept editing) , (5) **SalUn** (saliency unlearning) , (6) **SH** (ScissorHands) , (7) **ED** (EraseDiff) , and (8) **SPM** (concept-SemiPermeable Membrane) . We note that these unlearning methods are not universally designed to address nudity, style, and object unlearning simultaneously. Therefore, our assessment of their robustness against adversarial prompt attacks is specific to the unlearning tasks for which they were originally developed and employed.

**Training setups.** The implementation of AdvUnlearn (5) follows Algorithm 1 in Appx... As demonstrated in Sec. 5, unlike existing DM unlearning methods, AdvUnlearn specifically focuses on optimizing the text encoder within DMs. In the training phase of AdvUnlearn, the upper-level optimization of (5) for minimizing the unlearning objective (6) is conducted over 1000 iterations. Each iteration uses a single data batch with the erasing guidance parameter \(=1.0\) in (3) and a batch of \(5\) retaining prompts with a utility regularization parameter of \(=0.3\) for nudity unlearning and \(0.5\) for style and object unlearning. These regularization parameter choices are determined through a greedy search over the range \(\). Additionally, a learning rate of \(10^{-5}\) is employed with the Adam optimizer for text encoder finetuning. Each upper-level iteration comprises the lower-level attack generation, minimizing the attack objective (4) with \(30\) attack steps and a step size of \(10^{-3}\). At each attack step, gradient descent is performed over a prefix adversarial prompt token in its embedding space, starting from a random initialization.

**Evaluation setups.** We focus on two main metrics for performance assessment: unlearning robustness against adversarial prompts and the preservation of image generation utility. For _robustness_ evaluation, we measure the attack success rate (**ASR**) of DMs in the presence of adversarial prompt attacks,

  
**AT scheme in AdvUnlearn:** & AT & Fast AT \\ 
**Attack step 9:** & 30 & 1 \\   ASR (\(\)) \\ FID (\(\)) \\ Train. time per iteration (s) \\  & 21.13\% & 28.87\% \\  \\  
 FID (\(\)) \\ Train. time per iteration (s) \\  & 19.34 & 19.92 \\  \\   

Table 4: Comparison of different AT schemes in AdvUnlearn for _nudity_ unlearning.

where a lower ASR indicates better robustness. Unless specified otherwise, we utilize UnlearnDiffAtk  for generating adversarial prompts at testing time, as it can be regarded as an unseen attack strategy different from (4) used in AdvUnlearn. Detailed settings for attack evaluation are presented in **Appx.** C. For _utility_ evaluation, we use **FID** to assess the distributional quality of image generations. We also use **CLIP score** to measure their contextual alignment with prompt descriptions. A lower FID score, indicative of a smaller distributional distance between generated and real images, signifies higher image quality. And a higher CLIP score reflects the better performance of DMs in producing contextually relevant images. To compute these utility metrics, we employ DMs to generate \(10k\) images under \(10k\) prompts, randomly sampled from the COCO caption dataset .

### Experiment Results

**Robustness-utility evaluation of AdvUnlearn for nudity unlearning.** In **Tab.**5, we compare the adversarial robustness (measured by ASR) and the utility (evaluated using FID and CLIP score) of our proposed AdvUnlearn with unlearning baselines when erasing the _nudity_ concept in DM generation. For ease of presentation, we also refer to the DM post-unlearning (_i.e._, the unlearned DM) with the name of the corresponding unlearning method. Here we exclude the baselines SH and ED from the performance comparison in nudity unlearning due to their exceptionally high FID scores (over 100), indicating significantly compromised image generation quality. Detailed results and visualizations for these unlearning baselines are provided in **Appx.** D. As we can see, our proposal (AdvUnlearn) demonstrates significantly improved ASR, with over 50% ASR reduction compared to ESD, except for the lowest ASR achieved by SalUn. However, unlike SalUn, our robustness improvement does not come at a substantial cost to the DM utility. This is evident from its significantly better FID and CLIP scores compared to SalUn. To complement our quantitative findings, **Fig. 4** showcases the visualizations of the generations produced by various unlearned DMs in response to both the nudity-sensitive adversarial prompt and the nudity-irrelevant standard prompt. As observed, SalUn and AdvUnlearn are the only unlearning methods that yield robust concept-erased DMs against adversarial attacks. Yet, AdvUnlearn outperforms SalUn in generation quality.

**Effectiveness in style unlearning.** In **Tab.**6, we compare the robustness and utility performance of AdvUnlearn with various DM unlearning methods when removing the 'Van Gogh' artistic style from image generation. This comparison excludes the unlearning baseline SalUn but includes AC, based on whether they were originally developed for style unlearning. As observed, our proposal demonstrates a significant improvement in robustness, with over a 30% decrease in ASR compared to the second-best unlearning method, ESD. Crucially, this is accomplished without sacrificing model utility, as

    & SD v1.4 &  &  &  &  &  & AdvUnlearn \\  & (Base) & & & & & & (Ours) \\  ASR (\(\)) & 100\% & 96\% & 88\% & 72\% & 52\% & 36\% & 2\% \\ FID (\(\)) & 16.70 & 16.31 & 16.65 & 17.50 & 16.59 & 18.71 & 16.96 \\ CLIP (\(\)) & 0.311 & 0.311 & 0.311 & 0.310 & 0.309 & 0.304 & 0.308 \\   

Table 6: Performance summary of unlearning the _Van Gogh_ style, following a format similar to Tab. 5.

Figure 4: Visualizations of generated images by SD v1.4 and its nudity-unlearned variants. First row: Image generation under the adversarial prompt attack UnlearnDiffAtk  applied to the original prompt ‘woman standing in the doorway’. Second row: Image generation under the standard benign prompt ‘clean white toilet’.

    & SD v1.4 &  &  &  &  &  & AdvUnLearn \\  & (Base) & & & & & & (Ours) \\  ASR (\(\)) & 100\% & 97.89\% & 91.55\% & 79.58\% & 73.24\% & 11.27\% & 21.13\% \\ FID (\(\)) & 16.76 & 16.86\% & 17.48 & 17.10 & 18.18 & 33.62 & 19.34 \\ CLIP (\(\)) & 0.311 & 0.308 & 0.310 & 0.309 & 0.302 & 0.287 & 0.290 \\   

Table 5: Performance summary of nudity unlearning: ASR characterizes the robustness of DMs, including the pre-trained SD v1.4 (base model) and nudity-unlearned DMs, against adversarial prompt attacks generated by UnlearnDiffAtk  to regenerate nudity-related content. FID and CLIP scores characterize the preserved image generation utility of DMs subject to nudity-irrelevant benign prompts.

indicated by the comparable FID and CLIP scores compared to the base SD v1.4. The effectiveness of our proposal is also demonstrated through the generated images in **Fig. 5** under adversarial prompt attack and 'Van Gogh'-irrelevant benign prompt, respectively.

**Effectiveness in object unlearning. Tab. 7** compares the performance of AdvUnlearn with baselines when unlearning the object concept 'Church'. As we can see, similar to style unlearning, our approach achieves the highest robustness in Church unlearning, significantly preserving the original DM utility compared to the unlearning baseline SH, which attains similar robustness gain. The superiority of AdvUnlearn can also be visualized in **Fig. 6**, showing DM generation examples. More detailed results and visualizations of other object unlearning can be found in **Appx. E**.

**Plug-and-play capability of adversarially unlearned text encoder.** Given the modular nature of the text encoder in DMs, we further explore whether the robustness and utility of the text encoder learned from AdvUnlearn on one DM (specifically, SD v1.4 in our experiments) can be directly transferred to other types of DMs without additional fine-tuning. **Tab. 8** summarizes the plug-in performance of the text encoder obtained from AdvUnlearn when applied to SD v1.5, DreamShaper , and Protogen  for nudity unlearning. As we can see, the considerable robustness improvement as well as utility in DM unlearning are preserved when plugging the text encoder obtained from AdvUnlearn in SD v1.4 into other DMs (see 'Transfer' performance vs. 'Original' performance). This is most significant when transferring to SD v1.5 due to its similarity with SD v1.4. For dissimilar DMs like DreamShaper  and Protogen , the AdvUnlearn-acquired text encoder in SD v1.4 still remains effective as a plug-in option, lowering the ASR without sacrificing utility significantly. **Fig. 7** offers visual examples of image generation associated with the results presented in Tab. 8.

**The effect of text encoder layers on DM unlearning.** In **Fig. 8**, we show the ASR (robustness metric) and the CLIP score (a utility metric) of post-nudity unlearning against various choices of text encoder layers for optimization in AdvUnlearn. Here the layer number equal to \(N\) signifies that the first \(N\) layers are optimized. We observe that the robustness gain escalates as more layers are optimized. In particular, optimizing only the initial layers failed to provide adequate robustness for DM unlearning against adversarial attacks, contrary to findings in , where shallower encoder layers suffice for guiding DMs in

  
**Metrics** &  SD v1.4 \\ (Base) \\  & FMN & SPM & SalUn & ESD & ED & SH & 
 AdvUnlearn \\ (Ours) \\  \\   ASR (\(\)) & 100\% & 96\% & 94\% & 62\% & 60\% & 52\% & 6\% & 6\% \\ FID (\(\)) & 16.70 & 16.49 & 16.76 & 17.38 & 20.95 & 17.46 & 68.02 & 18.06 \\ CLIP (\(\)) & 0.311 & 0.308 & 0.310 & 0.312 & 0.300 & 0.310 & 0.277 & 0.305 \\   

Table 7: Performance summary of unlearning the object _Church_ in DM generation, following a format similar to Tab. 5.

Figure 8: Performance of AdvUnlearn vs. text encoder layers to optimize in nudity unlearning.

Figure 6: Examples of generated images by DMs when unlearning the object _church_, following Fig. 4’s format with attack in 1st row.

Figure 7: Images generated by different personalized DMs with original or plug-in AdvUnlearn text encoder for _nudity_ unlearning.

image editing albeit in a _non-adversarial_ context. Yet, we also find that for object or style unlearning, optimizing only the first layer of the text encoder has demonstrated satisfactory robustness and utility in DM unlearning. This suggests that nudity unlearning presents a more challenging task in ensuring robustness. Utility-wise, we observe a slight performance degradation as more encoder layers are robustified, which is under expectation.

**Choice of adversarial attacks.** In **Tab.**9, we show the attack success rate (ASR), the robustness metric of post-nudity unlearning against various choices of adversarial prompt attacks: 1) CCE (circumventing concept erasure)  utilizes textual inversion to generate universal adversarial attacks in the embedding space. By inverting an erased concept into a 'new' token embedding, learned from multiple images featuring the target concept, this embedding is then inserted into the target text prompt. 2) PEZ (hard prompts made easy)  is to generate an entire text prompt for the target image by optimizing through the cosine similarity. 3) PH2P (prompting hard or hardly prompting)  is similar to PEZ but with different optimization objective. 4) UnlearnDiffAtk  has been used as the default method for generating attacks in this work. When the attack is based on discrete prompts (such as UnlearnDiffAtk, PEZ, and PH2P), our proposed method AdvVullean consistently achieves remarkable erasure performance and robustness. Notably, UnlearnDiffAtk consistently achieves a higher ASR than PEZ and PH2P, reaffirming its use as our primary tool for robustness evaluation among text-based adversarial attacks. In parallel, the CCE attack achieves a higher ASR compared to text prompt-based methods, as it leverages continuous embeddings, offering a larger search space with greater attack flexibility. This is not surprising as the textual inversion is engineered to learn a 'new' continuous token embedding, enabling the representation of objects not encountered during training.

**Other ablation studies.** In **Appx.** F, we demonstrate more ablation studies. This includes: 1) the impact of the utility-retaining regularization weight on AdvUnlearn (**Fig. A3**); 2) the selection of retain sets for utility-retaining regularization (**Tab.** A3**); 3) the impact of adversarial prompting strategy for AdvUnlearn (**Tab.** A4); 4) the robustness of SD v1.4 finetuned through AdvVUnlearn against different adversarial prompt attack (**Tab.** A5).

## 7 Conclusion

Current unlearned DMs (diffusion models) remain vulnerable to adversarial prompt attacks. Our proposed robust unlearning framework, AdvUnlearn, illuminates potential strategies for enhancing the robustness of unlearned DMs against such attacks while preserving image generation utility. Notably, our framework employs utility-retaining regularization on a retained prompt set and identifies the text encoder as a more effective module for robustification compared to the UNet within DMs. Through extensive experiments, we demonstrate that AdvUnlearn strikes a graceful balance between robust unlearning and image generation utility. Despite the possibility of using fast attack generation to speed up AdvUnlearn, continual improvement in computational efficiency remains a crucial area for future research. Limitations and broader impacts are further discussed in **Appx.** G and **Appx.** H.

## Appendix A System Prompt for LLM as a Judger

We utilize a Large Language Model (LLM) like GPT-4 to act as a judger to filter out prompts that are relevant to the concept to be unlearned. Since nudity is a global concept , even if 'nudity' is not mentioned in the prompt, the generated image conditioned on the prompt might still involve the 'nudity' concept. As a result, the system prompt for the LLM to act as a prompt judger for the 'nudity' concept is shown below:

_"Could you identify the prompts from my uploaded CSV file that are likely to lead the Stable Diffusion model to generate images with sensitive content? For instance, prompts that mention attire with an emphasis on attractiveness or those that focus on specific body parts may unintentionally prompt the generation of images with nudity."_

Unlike 'nudity', objects are local concepts, so we only need to filter out the name of the object that needs to be unlearned directly in the prompt. Thus, the system prompt for the LLM to act as a prompt judger for the specific object is shown below:

_"Could you analyze the prompts from my uploaded CSV file to identify those that are likely to direct the Stable Diffusion model to generate images containing '{OBJECT}' or any of its synonyms?"_

## Appendix B Algorithm Details: AdvUnlearn

**Algorithm 1** outlines the steps of AdvUnlearn. Each iteration consists of two stages: adversarial prompt generation and unlearning with utility-retaining regularization. During adversarial prompt generation, the optimized adversarial prompt embedding is randomly initialized and iteratively updated. When attack step number \(K=1\), one-step FGSM (7) is utilized to generate adversarial promtps. In the unlearning stage, the adversarial prompt optimized in the previous stage is used to compute the unlearning loss, while a batch of prompts from the retain set is used to compute the utility-retaining regularization loss. The combination of unlearning loss and retaining loss is then used to update the trainable module parameters.

```
1:Given Iteration Number \(I\), batch size \(b\) of retaining prompts \(c_{}\), regularization weight \(\), learning rate \(\), adversarial step size \(\), attack step number \(K\), unlearning concept \(c\), the DM to be unlearned \(\), and the frozen original DM \(_{o}\):
2:for\(i=1,2,,I\)do
3:\(\) Adversarial prompt generation
4: Randomly initialize adversarial soft prompt embedding \(_{0}\)
5:if K = 1 then
6:\(=_{0}-(_{} _{}(_{i},c+_{0}))\)\(\) Fast AT variant: One-Step FGSM (7)
7:else
8: Run _K_-Step attacks using (4)
9:endif
10:\(\)Defensive unlearning with utility-retaining regularization
11:\(_{}(_{i},c+_{K})=_{}(_ {i},c+_{K})+_{_{}} [\|_{_{i}}(_{t}|)-_{ _{o}}(_{t}|)\|_{2}^{2}]\)
12:\(_{i+1}=_{i}-_{}_{}( _{i},c+_{K})\)
13:endfor ```

**Algorithm 1**AdvUnlearn: Defensive Unlearning with Adversarial Training for DMsAttack Setup

Following the methodology of , we introduce prepended prompt perturbations using \(N\) tokens, where \(N=5\) is designated for _nudity unlearning_ and \(N=3\) for both _style_ and _object unlearning_. To optimize these adversarial perturbations, we sample \(50\) diffusion time steps and execute UnlearnDiffAtk over \(40\) iterations with a learning rate of \(0.01\), using the AdamW optimizer. For the classification of generated images, we use NudeNet  for nudity unlearning, a ViT-base model  fine-tuned on the WikiArt dataset for style unlearning, and an ImageNet-pretrained ResNet-50 for object unlearning tasks.

## Appendix D Other Results and Visualizations for _Nudity_ Unlearning

The metrics used for this evaluation are Attack Success Rate (ASR), Frechet Inception Distance (FID), and CLIP score. As shown in **Tab.**A1, although ScissorHands (SH)  and EraseDiff (ED)  achieve high unlearning robustness against adversarial prompt attacks, the trade-offs are significant. Their dramatically high FID and low CLIP scores indicate an inability to generate high-quality images that align with the condition text prompts. This is further corroborated by their visualization examples. In **Fig.**A1, we observe that, regardless of the condition prompts, the generated images are similar, demonstrating that SH and ED fail to generate varied and contextually appropriate images. Therefore, we do not include them in the main performance evaluation table for _nudity_ unlearning to maintain clarity.

Other Results and Visualizations for _Object_ Unlearning

In **Tab. A2**, we present a detailed evaluation of various unlearning methods applied to the base SD v1.4 model for three different objects: _Garbage Truck_, _Parachute_, and _Tench_. The unlearning methods compared include FMN (Forget-Me-Not) , SPM (concept-SemiPermeable Membrane) , SalUn (saliency unlearning) , ED (EraseDifff) , ESD (erased stable diffusion) , SH (ScissorHands) , and our proposed DM unlearning scheme, referred to as AdvUnlearn. As shown, our proposed DM unlearning method, AdvUnlearn, consistently achieves the best unlearning efficacy (around \(10\%\)) with competitive image generation utility. In comparison, FMN and SPM prioritize training image generation utility but exhibit weak unlearning robustness against adversarial prompt attacks. Conversely, SH achieves strong robustness but at the high cost of image generation utility degradation. The remaining methods (SalUn, ED, and ESD) attempt to find a balance between robustness and utility; however, their unlearning robustness is not stable across different object concepts, and their ASR is multiple times higher than that of our proposed AdvUnlearn. The visualization examples associated with the results presented in **Tab. A2** can be found in **Fig. A2**. Through visualization examples, we demonstrate that our proposed robust unlearning framework, AdvUnlearn, not only effectively removes the influence of target concepts to be unlearned in the text prompt but also retains the influence of other descriptions. For instance, a garbage truck-unlearned DM equipped with the AdvUnlearn text encoder generates a photo of a parking lot from the text prompt 'garbage truck in a parking lot.' Similarly, other object-unlearned DMs with corresponding AdvUnlearn text encoders produce a photo of a desert landscape from the prompt 'parachute in a desert landscape' and a photo of a baby in a pond from the prompt 'baby tench in a pond.' Furthermore, AdvUnlearn reduces the disruption on the image generation utility compared to other methods.

Other Ablation Studies

In this section, we explore the influence of utility-retaining regularization weight and attack step number of adversarial prompt generation for our proposed DM unlearning scheme, AdvUnlearn.

Regularization weight.As depicted in **Fig. A3**, there is a clear upward trend in both the CLIP Score and ASR as the utility-retaining regularization weight increases. The CLIP Score, represented by the blue line, shows a gradual and consistent rise, starting from \(0.282\) at a regularization weight of \(0.1\) and reaching \(0.3\) at a weight of \(0.7\), indicating improved image generation utility. However, this improvement comes at a cost, as the ASR, illustrated by the red bars, demonstrates a significant increase from \(13.18\%\) at a weight of \(0.1\) to \(36.63\%\) at a weight of \(0.7\), suggesting a degradation in unlearning efficacy. Consequently, we have selected a regularization weight of \(0.3\) as our default setting due to its balanced performance, providing a compromise between enhanced image generation utility and acceptable unlearning efficacy.

Retain set selection.We introduce a utility-retaining regularization, defined in (6), designed to reduce the degradation of image generation utility commonly associated with adversarial training for unlearning. In **Tab. A3**, we examine the influence of object class sources on the retain set, using the template _'a photo of [OBJECT]'_, and evaluate the effectiveness of employing a Large Language Model (LLM) as a prompt filter, which helps exclude prompts potentially related to the concept being erased. We ensure that each retain set contains an equal number of prompts to allow for fair comparison. Our findings indicate that retain sets sourced from the COCO dataset consistently outperform those from ImageNet in terms of unlearning efficacy, with only minor utility loss. Additionally, the table underscores the benefits of the LLM prompt filter: prompts refined through this filter significantly boost unlearning efficacy while preserving image generation utility, in stark contrast to datasets assembled without such filtering. Clearly, the choice of object class for prompt dataset creation plays a crucial role in balancing unlearning efficacy against image generation utility.

Adversarial prompting strategy.We evaluate three distinct adversarial prompting strategies for adversarial prompt generation in AdvUnlearn: _Replace_: This strategy involves directly replacing the original concept prompt with an optimized adversarial soft prompt. _Add_: This method adds the optimized adversarial soft prompt to the original concept prompt within the token embedding space. _Prefix_: This approach prepends the optimized adversarial soft prompt before the original concept prompt, and is the default setting for our study. As demonstrated in **Tab.** A4, the _Prefix_ strategy emerges as the most effective, achieving the highest unlearning efficacy--with nearly half the Attack Success Rate (ASR) of the other strategies--while maintaining competitive image generation utility compared to the _Replace_ and _Add_ strategies.

**Robustness gain from AdvUnlearn at different test-time attacks.** In **Tab.** A5, we present the robustness of SD v1.4 post-nudity unlearning when facing different test-time adversarial prompt attacks, UnLearmDiffAtk  (default choice) and P4D . As we can see, AdvUnlearn maintains its effectiveness in improving robustness under both attack methods at testing time. Notably, the ASR against P4D is even lower than that against UnLearmDiffAtk. This result is expected, as P4D employs the same attack loss (4) used during training for generating adversarial prompts in (5). Therefore, we default to UnLearmDiffAtk for test-time attacks in robustness evaluation due to its unseen nature during training.

## Appendix G Limitations

This work seeks to improve the robustness of concept erasing by incorporating the principles of adversarial training (AT) into the process of machine unlearning, resulting in a robust unlearning framework named AdvUnlearn. In AT, generating adversarial examples with a \(K\)-step attack typically requires nearly \(K\) times more computation time than vanilla training. Although we considered faster attack generation methods, such as the Fast Gradient Sign Method (FGSM), these were found to suffer from some robust performance degradation. Additionally, to maintain image generation utility, we introduced a utility-retaining regularization, which also demands additional computation time. Therefore, future efforts to enhance computational efficiency without significantly compromising performance are essential for improving the current work.

## Appendix H Broader Impacts

The broader impacts of this study include social and ethical implications, where improved reliability of concept erasing aligns AI technologies with societal norms and ethical standards, potentially reducing the spread of harmful digital content. Additionally, AdvUnlearn addresses significant legal concerns by reducing the likelihood of DMs inadvertently producing content that violates copyright laws, supporting the responsible deployment of AI in creative industries. This advancement also marks a significant step forward in AI safety and security by integrating adversarial training into machine unlearning, ensuring AI systems are not only capable of forgetting specific concepts but also resilient to manipulations intended to circumvent these protections. While demonstrating a balanced trade-off between robustness and utility, the complexity of AdvUnlearn's implementation highlights the need for further studies on the impacts of robustification techniques on AI model performance and scalability. Furthermore, this work opens new avenues for research in AI model robustness and necessitates continuous research, thoughtful policy-making, and cross-disciplinary collaboration to fully realize the potential of these technologies in a manner that benefits society as a whole.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In **Sec.**6, through extensive experiments, we accurately reflect the contributions and scope of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Discussions on limitations of our work can be found in **Appx.**\(\). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: Observing the vulnerability of existing unlearned Diffusion Models (DMs), we have systematically explored robust DM unlearning through adversarial training (AT). To address the challenges of effectiveness and efficiency introduced by incorporating AT into DM unlearning, we propose a utility-retaining regularization strategy. Additionally, we have identified the text encoder as a more effective component for unlearning. To further improve efficiency, we have adapted fast adversarial training techniques.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Experiment setups can be found in **Sec.**6.1 and **Appx.**C. Additionally, we have included the code and prompt datasets used for experiments in the supplementary material, ensuring that the experiments can be easily reproduced by following the provided instructions. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have included the code and prompt datasets used for experiments in the supplementary material, ensuring that the experiments can be easily reproduced by following the provided instructions. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experiment setups can be found in **Sec.**6.1 and **Appx.** C. We have included the code and prompt datasets used for experiments in the supplementary material, ensuring that the experiments can be easily reproduced by following the provided instructions. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Error bars are not reported because it would be too computationally expensive.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details of hardware and corresponding computation time can be found in Line 861 - 873. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have made sure to preserve anonymity. Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [Yes] Justification: The discussion on broader impacts can be found in **Appx.** H. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper is designed for defensive DM unlearning. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the original paper that produced the code package and dataset. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have included the code and prompt datasets used for experiments in the supplementary material, ensuring that the experiments can be easily reproduced by following the provided instructions. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work utilizes only text descriptions as input for the diffusion model we employed. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work utilizes only text descriptions as input for the diffusion model we employed.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.