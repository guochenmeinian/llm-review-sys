# DistrictNet: Decision-aware learning for geographical districting

Cheikh Ahmed

Polytechnique Montreal

cheikh-abdallaihi.ahmed@polymtl.ca &Alexandre Forel

Polytechnique Montreal

alexandre.forel@polymtl.ca &Axel Parmentier

CERMICS, Ecole des Ponts

axel.parmentier@enpc.fr &Thibaut Vidal

Polytechnique Montreal

thibaut.vidal@polymtl.ca

###### Abstract

Districting is a complex combinatorial problem that consists in partitioning a geographical area into small districts. In logistics, it is a major strategic decision determining operating costs for several years. Solving districting problems using traditional methods is intractable even for small geographical areas and existing heuristics often provide sub-optimal results. We present a structured learning approach to find high-quality solutions to real-world districting problems in a few minutes. It is based on integrating a combinatorial optimization layer, the capacitated minimum spanning tree problem, into a graph neural network architecture. To train this pipeline in a decision-aware fashion, we show how to construct target solutions embedded in a suitable space and learn from target solutions. Experiments show that our approach outperforms existing methods as it can significantly reduce costs on real-world cities.

## 1 Introduction

Districting aims to partition a geographical area made of several basic units (BUs) into small, balanced, and connected areas known as districts. Districting has a wide array of applications in many areas such as electoral politics (Williams, 1995; Webster, 2013; Ricca et al., 2013), sales territory design (Lopez-Perez and Rios-Mercado, 2013; Zoltners and Sinha, 2005), school zoning (Ferland and Guenette, 1990), and distribution (Zhou et al., 2002; Zhong et al., 2007). These problems are challenging because of the combinatorial complexity of assigning BUs to districts.

In this paper, we focus on districting and routing, one of the more complex forms of districting. In that case, the goal is to minimize the routing costs over all districts, where each district is serviced by a vehicle starting from a common depot. Since districting is a long-term strategic decision, the delivery requests in each district are unknown and modeled as a point process. As a two-stage stochastic and combinatorial problem, districting and routing can be solved to optimality only on very small instances. For example, in our experiments, finding the optimal solution of a districting problem with \(60\) BUs and \(10\) districts required around \(400\) CPU-core days. Further, while several heuristic methods have been proposed based on estimating district costs (Daganzo, 1984; Figliozzi, 2007), they tend to lead the search towards low-quality solutions on large instances (Ferraz et al., 2024).

In this study, we present a structured-learning approach called DistrictNet that integrates an optimization layer in a deep learning architecture, as shown in Figure 1. DistrictNet learns to approximate districting problems with a simpler one: the capacitated minimum spanning tree (CMST). DistrictNet predicts the cost of each arc of the CMST graph using a graph neural network (GNN)trained in a decision-aware fashion. This surrogate optimization model captures the structure of districting problems while being much more tractable. Using the CMST as a surrogate model is especially relevant because there is a surjection from the space of districting solutions to the space of CMST solutions. In other words, there always exists a CMST solution that is optimal for the original problem. The main challenge is to train a model that can find it.

DistrictNet is trained to imitate a few optimal solutions obtained on small instances. We show that it leads to high-quality solutions for large districting problems and great out-of-distribution generalization. Further, our approach is robust to changes in the problem parameters such as a varying number of districts. This is valuable for practitioners, who usually evaluate a family of problems before picking a solution.

**Our contributions.** We present a structured learning approach to obtain high-quality solutions to large districting problems. The main component of our pipeline is a combinatorial optimization layer, a parameterized CMST, that acts as a surrogate to the original districting problem. We show how to learn from districting solutions by embedding CMST solutions into a suitable space and constructing target CMST solutions from districting ones. This allows training DistrictNet by imitation, minimizing a Fenchel-Young loss on a set of target solutions obtained on small instances. The value of our approach is demonstrated on real-world problems as DistrictNet generalizes to large problems and outperforms existing benchmarks by a large margin thanks to the combination of GNN and structured learning. While we focus our experiments on districting and routing, the method presented is not tailored to this specific application. DistrictNet is generic and could be applied to any geographical districting problem.

## 2 Problem statement

We model a geographical area as an undirected graph \(G=(V,E)\), where \(V\) is a set of vertices representing the BUs and \(E\) is a set of edges representing the connections between them. A district \(d V\) is a subset of BUs. We say that a district \(d\) is connected if its graph is connected. Let \(()\) be the indicator function that returns one if its argument is true and zero otherwise. We denote by \(\) the set of connected districts and \(N=|V|\) the number of BUs.

**Routing costs.** Let \(d\) be a connected district. In districting and routing, the cost of a district is the expected distance of the smallest route that satisfies the demand requests. Hence, it is the expected cost of a traveling salesman problem \(C_{}(d)=_{}[(d,)]\), where \(\) is the collection of demand requests in a district \(d\).

Since the demand is not known a priori, it is modeled as a random variable. We assume that an average demand-generating distribution exists for each basic unit over the planning horizon so that \(_{i}_{v}\), \( v V\). Let \(v_{0}\) be an additional vertex that represents the depot and is connected to all other nodes. Let also \(()\) be the set of all routes that satisfy the demand requests of a district \(d\) while starting and finishing at the depot, and denote by \(()\) the total travel distance of a route \(\). Given a set of demand requests \(\), the minimum transportation costs are achieved by the route that visits all demand locations with minimum distance starting and ending at the depot so that

\[(d,)=_{()}().\]

Figure 1: DistrictNet solves a complex districting problem by parameterizing and solving a CMST. The GNN \(\) predicts a vector of edge weights \(\) based on the covariates of the instance \(x\). These edge weights parameterize a CMST, which is solved using a black-box combinatorial solver. The CMST solution \(\) is finally converted into a districting solution \(\). Training this pipeline in a decision-aware manner requires propagating a loss gradient back to the GNN.

The district cost can be computed through a Monte Carlo evaluation by sampling a large set of demand scenarios and solving multiple independent TSPs. This evaluation is costly for large cities and districts. Therefore, evaluating the cost of a districting solution is hard even if districts can be evaluated in parallel.

Districting.The districting and routing problem minimizes the sum of all district costs. Let \(C_{}(d)\) denote the cost of a district \(d\). A districting problem can be formulated in a general form as

\[_{\{0,1\}^{||}} _{d}C_{}(d)_{d},\] (1a) \[ _{d}(i d)_{d}=1,  i V,\] (1b) \[_{d}_{d}=k,\] (1c)

where \(_{d}\) is a binary variable that tracks whether a district is selected. Constraint (1b) states that each BU is selected in exactly one district of the solution. Constraint (1c) specifies that exactly \(k\) districts are selected.

Additional constraints on the districts can be readily added to the problem. For instance, districting and routing typically aim to obtain districts close to a target size \(t\) and include constraints on the minimum and maximum size of a district. Problem (1) can be formulated over a restricted set of districts \(^{r}\) that satisfy the minimum and maximum size constraints. Formally, it ensures that \( d^{r},|d|\), where \((,)\) are user-specified upper and lower bounds on the district size.

Although seemingly simple, Formulation (1) has an exponential number of variables. The number of feasible connected districts of size \(t\) within a geographical area comprising \(N\) BUs is on the order of \(((e(-1))^{t-1}.)\) where \(\) is the maximum number of neighbors for a given BU (Komusiewicz and Sommer, 2021). For instance, a city of \(N=120\) BUs and a target size of \(t=20\) with \(=13\) has on the order of \(10^{29}\) possible connected districts. The districting-and-routing problem has also been shown to be NP-hard by Ferraz et al. (2024). Hence, solving Problem (1) to optimality is only possible for small problem sizes.

Existing approaches.For real-world cities, evaluating a district's cost is too computationally demanding to be performed at each step of a search algorithm. Existing methods replace the cost \(C_{}(d)\) with a surrogate cost estimator. The defining aspect of existing methods is the specific model used to estimate the routing costs. Most approaches are based on the formula of Beardwood-Halton-Hammersley (BHH), which estimates the distance to connect randomly distributed points (Beardwood et al., 1959). This formula has been embedded into a hybrid search method combining a gradient method and a genetic algorithm (Novacs et al., 2000) and an adaptive large neighborhood search metaheuristic (Lei et al., 2015). Extensions of the BHH formula include Daganzo (1984), who adapted it for routing problems, and Figliozzi (2007) who extended it further for non-uniformly distributed demand requests. Recent works have shown that the BHH formula estimates stochastic TPS costs remarkably well empirically against more sophisticated regression functions for uniform distributions (see, e.g., Kou et al., 2022). Most closely related to our work, Ferraz et al. (2024) trained a GNN to predict district costs and embedded it in an iterative local search algorithm. They focused on solving single districting instances but did not investigate the generalization capabilities of their approach.

In contrast to the literature, we learn to approximate districting problems by using a surrogate optimization problem trained in a decision-aware fashion. An advantage of this framework is that it applies to a wide range of constrained partitioning problems: it can work with any general districting cost function \(C(d)\). Hence, it could readily consider other metrics such as fairness, balancing and compactness of the district.

Multi-instance learning and generalization.Often, practitioners do not solve a single districting problem but study a family of problems with varying settings and parameters. Because there is a substantial effort needed to train a model, our goal is to obtain a pipeline able to solve multiple instances of districting. In our setting, generalizing over multiple instances means being able to generalize across:

* _Cities_: each city has unique geographical and social characteristics, which affect the optimal districting solution. Our model should be able to identify the impact of these differences on districting and provide high-quality solutions for different cities.

* _Instance sizes_: the model should be able to handle instances with a different (typically larger) number of BUs than it was trained on. Since the training effort increases with the number of BUs, this allows solving large instances with a small training budget.
* _Problem parameters_: the model should be able to generalize to slight variations in the problem parameters, such as the minimum and maximum size of districts.

## 3 DistrictNet: From CMST to Districting

DistrictNet takes as input a labeled graph containing all the data of an instance. A graph neural network \(_{w}\) turns this labeled graph into edge weights. These weights parameterize a CMST instance, which is then solved using a dedicated algorithm. The CMST solution is then converted into a districting solution.

The first component of DistrictNet is a GNN \(_{w}:^{d_{f}}\) that assigns a weight to an edge depending on its feature vector. Applying this model to all edges of an instance returns a vector of edge weights \(^{|E|}\). Our model is made of two components: graph convolution layers that learn latent representations of graph edge features, and a deep neural network that converts these latent representations into edge weights.

The GNN is made of \(L\) graph convolutional layers that update the features of edges through message passing. Following Morris et al. (2019), the update rule at the \(l\)-th layer is given by

\[h_{e}^{(l+1)}=W_{0}^{(l)}h_{e}^{(l)}+(e)|} _{j(e)}W_{1}^{(l)}h_{j}^{(l)},\] (2)

where \(h_{e}^{(l)}\) is the feature vector of edge \(e\) at the \(l\)-th layer, \(\) is a non-linear activation function, \(W_{0}\) and \(W_{1}\) are learnable weight matrices, and \((e)\) is the set of neighboring edges of an edge \(e\). Convolutional layers allow to capture the structure of the graph while being able to apply the prediction model to any graph size and connectivity structure. After the convolutional layers, a fully connected deep neural network transforms the latent representation produced by the GNN into an edge weight.

### Combinatorial Optimization Layer

The second component of DistrictNet is the CMST layer parameterized by the vector of edge weights \(\). This layer acts as a surrogate for the original districting problem. The CMST is a well-studied graph optimization problem based on the minimum spanning tree problem. A minimum spanning tree is the subset of edges of a weighted graph that spans all vertices of the graph while minimizing the sum of the weight of its edges. Classically, the CMST extends this problem with the constraint that the number of vertices in each subtree does not exceed a predetermined capacity.

Let \(\) be the set of connected subtrees with minimum and maximum size \((,)\). We consider that an edge \(e\) is in a subtree \(s\) if both its extreme points are inside the subtree. The CMST problem with a target number of subtrees is then given by

\[_{\{0,1\}^{||}} _{s}_{s}_{e s}- _{e},\] (3a) \[ _{s}(i s)_{s}=1,  i V,\] (3b) \[_{s}_{s}=k.\] (3c)

Formulation (3) highlights the strong link between the CMST and the districting problem in (1). Both problems partition a graph into connected components with cardinality constraints. Any CMST solution can be converted into a districting solution by collecting all the nodes of a subtree into a district. Since several subtrees lead to the same district, this is a surjective mapping from the space of subtrees \(\) to the space of districts \(\). This also implies that, for any districting problem, there always exists a CMST problem such that their optimal solutions coincide.

**Solving the CMST.** Solving CMST problems is much more amenable than solving districting problems since it is a single-stage optimization problem with a linear objective. However, it remains a combinatorial problem. Several methods have been proposed to solve it, ranging from expensive exact methods to quick heuristics.

DistrictNet is agnostic to the choice of CMST solver. In our experiments, we use the exact formulation in (3) to solve small instances to optimality, and we apply an iterated local search (ILS) heuristic for large instances. ILS alternates between two steps: (i) a local improvement step that guides to a local minimum, and (ii) a perturbation step to diversify the search. A key component of the algorithm is the initial solution. Randomly allocating BUs to districts is unlikely to return feasible solutions and, even when it does, it often leads to very poor solutions. Here, we use a modified Kruskal algorithm to exploit the structure of the CMST and quickly find a good initial solution. The details of our implementation of the ILS are given in Appendix A.

## 4 Training DistrictNet by imitation

DistrictNet is trained to imitate the solutions of a training set \(x_{i},}}_{i=1}^{n}\). Each instance \(x_{i}=(G_{i},f_{i})\) is a labeled graph with \(G_{i}=(V_{i},E_{i})\) the instance graph and \(f_{i}=f_{i}^{e}^{d_{f}},\, e E_{i}}\) the set of edge feature vectors. A target districting solution \(}\) is associated with each instance \(x_{i}\). Since producing optimal or near-optimal districting problems is hard, building this training set is expensive. Thus, we train our model on a dataset of small instances with the hope that it generalizes well on large instances. Training DistrictNet by imitation amounts to solving

\[_{w}_{i=1}^{n}_{w}(x_{i}),},\]

where \(:(,)(,)\) is a loss function that quantifies the distance between a CMST solution corresponding to the edge weights \(\) with a target CMST solution \(\). However, our training set does not contain any CMST targets but only districting targets. Training DistrictNet is thus achieved by three main steps: (i) introducing a new embedding of CMST solutions, (ii) converting districting targets \(}\) into CMST targets \(}\), and (iii) defining a suitable loss function with desirable properties and deriving its gradient.

### Embedding and target solutions

A CMST solution is entirely characterized by its edges. We can therefore build an embedding \(\) of the set \(\) of solutions \(\) of the CMST problem given in (3) in \(^{|E|}\) as \(=y()}\) where \(y()=y_{e}()_{e E}\) and \(y_{e}()=_{s}_{s}\,(e s)\). We can then reformulate Problem (3) as

\[_{y}^{}y,\] (4)

where, with a slight abuse of notation, we changed the problem from \(\) to \(\), which is without loss of generality. Since \(\) is finite, its convex hull \(\) is a polytope. Therefore, the linear program given by \(_{}^{}\) is equivalent to Problem (4). The change of notation from a vertex \(y\) to a moment \(\) emphasizes that \(\) takes values inside the convex hull \(\). We now use the shorthand notation \(^{*}()\) to denote an optimal solution to \(*{argmax}_{}^{}\).

**From Districting to CMST.** As discussed previously, there is a surjection from the space of CMST solutions to the space of districting solutions. Given a target districting solution \(\), we denote by \(()\) the set of feasible CMST solutions that lead to \(\).

To recover a target CMST solution, we introduce the constructor algorithm \(: y\) that maps a districting solution \(\) to a CMST solution \(y()\). This algorithm can be randomized. For instance, DistrictNet constructs districting solutions by solving a minimum spanning tree problem with random edge weights for each district \(d\). This can be efficiently done by applying Kruskal's algorithm in parallel for all the selected districts.

Finally, we define our target CMST solution \(\) as

\[=[y|,],\] (5)

which is taken as an expectation over the realizations of the randomized algorithm \(\). This allows us to convert our training set \(x_{i},}}_{i=1}^{n}\) into \(\{x_{i},}\}_{i=1}^{n}\) and enables training DistrictNet by imitation.

### Fenchel-Young loss and stochastic gradient

Let \(\) be a target CMST solution. We want DistrictNet to minimize the non-optimality of \(^{*}()\) compared to the target \(\), that is, minimizing the loss \(_{}^{}-^{}\). Minimizing this loss directly does not work because \(=0\) is a trivial optimal solution. However, given a smooth strictly convex regularization function \((y)\), we can define the regularized problem (Blondel et al., 2020)

\[_{}^{}-()\] (6)

and the smoothed version of the non-optimality loss as:

\[_{}(,)=_{}^{ }-()-(^{}-()).\] (7)

Denote by \(^{*}()\) the Fenchel conjugate of \(\), the regularized loss in Equation (7) is equal to \(_{}(,)=^{*}()+( )-^{}\)(Dalle et al., 2022), which we recognize as the Fenchel-Young inequality (Blondel et al., 2020). Fenchel's duality theory then ensures that this loss has desirable properties. Notably, it is convex in \(\), non-negative, equal to \(0\) only if \(\) is the optimal solution of (6), and its gradient can be expressed as \(_{}(,)=*{argmax}_{ }^{}-()-\).

Practically, it remains to choose a suitable regularization function \(\). When using a black-box oracle, a convenient choice that exploits the link between perturbation and regularization (Berthet et al., 2020; Dalle et al., 2022) is to define \(()\) as the Fenchel dual of the perturbed objective \(F()=[_{}+Z^{ }]\), where \(Z\) is a random variable with positive and differentiable density on \(^{|E|}\). In that case, the gradient of the Fenchel Young loss with respect to \(\) can be computed as (Berthet et al., 2020)

\[_{}_{}(,)=_{Z}[^ {*}(+Z)]-.\] (8)

A stochastic gradient can therefore be conveniently computed using the Monte Carlo approximation \(_{m=1}^{M}*{argmax}_{}(+Z _{m})^{}\) for the expectation, where \(\{Z_{m}\}_{m=1}^{M}\) are sampled perturbations. If \(\) is randomized, we also use a Monte Carlo approximation to estimate \(\).

**Summary.** The novelty of our approach lies in our reconstruction of a CMST moment \(\) from a districting solution \(\), which can be seen as a partially specified target \(\). Alternatives in the literature generally consider completing the partially specified solution into the fully specified solution that minimizes the Fenchel Young loss (Cabannnes et al., 2020; Stewart et al., 2023). Our approach has the advantage of leading to the classic Fenchel Young loss, which is convex, whereas the infinum loss of Stewart et al. (2023) is only a difference of convex functions.

## 5 Numerical Study

We now evaluate the performance of DistrictNet on real-world districting and routing problems. We run repeated experiments and compare our approach to other learning-based benchmarks. We investigate the following aspects of DistrictNet: (i) its ability to generalize to large out-of-distribution instances from training on a few small instances, (ii) to variations in the instance parameters such as the district sizes, (iii) the role of the CMST surrogate model to allow this generalization.

Our experiments are implemented in Julia (Bezanson et al., 2017) except for the district evaluation methods, which are taken from Ferraz et al. (2024) and implemented in C++. All experiments are run on a computing grid. Each experiment is run on two cores of an AMD Rome 7532 with \(2.4\,\) and is allocated \(16\,\) RAM. The code to reproduce all experiments presented in this paper is publicly available at https://github.com/cheikh025/DistrictNet under an MIT license.

### Experimental Setting

Our instances include all the real-world cities in the United Kingdom used by Ferraz et al. (2024) and we extend it with additional cities in France. Hence, our test set contains seven real-world cities in the United Kingdom and France (Bristol, Leeds, London, Lyon, Manchester, Marseilles, and Paris), which contain between \(120\) and \(983\) BUs. Our goal is to provide high-quality solutions for severalvalues of the target district size \(t\). This target size sets the bounds \((,)\) on the district size \(t 20\%\) and the target number of districts as \(k= N/t\).

**Features.** Each BU is summarized by a set of characteristics including its population, density, area, perimeter, compactness, and distance to the depot. For test instances, these summarizing statistics are taken from real-world data. The edge feature vector is constructed by averaging the feature vectors of the two BUs it connects. Additionally, we include the distance between the center of the two connecting BU into the edge feature vector. Thus, an instance is fully described by its geographical and population data.

**Training set.** To assemble a large and diverse training set, we generate new cities by perturbing real-world ones. First, we read the geographical data of \(27\) real-world cities in England (excluding the ones from the test set). From these initial cities, we generate \(n=100\) random connected subgraphs of size \(N=30\) BUs and sample the population of each BU according to a normal distribution \((8\,000,2\,000)\) truncated between \(5\,000\) and \(20\,000\). For each instance \(x_{i}\), we compute its optimal solution for the target size \(t=3\) by fully enumerating the possible districting solutions and evaluating their costs. This procedure generates our training set of \(n=100\) instances and associated solutions.

**Benchmarks.** We evaluate our method against four benchmark approaches that are based on learning estimators of district costs \(C_{}(d)\). They can be combined with any search method. In these experiments, they are also integrated within an ILS with a time limit of \(20\,\).

We include (i) the method of Daganzo (1984) (BD), (ii) the method of Figliozzi (2007) (FIG), (iii) the method of Ferraz et al. (2024) (PredGNN), and (iv) a deterministic approximation of the stochastic districting problem called AvgTSP. The first two approaches are extensions of BHH's formula and therefore are simple linear regression models. The third benchmark is based on training graph neural networks to estimate the district costs. Ferraz et al. (2024) train a GNN model to predict district costs for a fixed city and set of problem parameters and show that this approach outperforms BD and FIG. Since our focus is on generalization to multiple cities and problem parameters, our implementation extends the one of Ferraz et al. (2024): we train a single GNN using data from multiple small cities and evaluate its ability to generalize out-of-distribution. In contrast, AvgTSP estimates district costs by solving a TSP over the barycentres of all BUs within a district. This is a single-scenario approximation of the original stochastic districting problem that considers the expected demand realization in each BU.

The above benchmarks do not use a surrogate optimization model. Instead, they are trained in a traditional supervised learning fashion: to minimize the cost-estimation error on a training set of \(10\,000\) districts and true costs taken from the same training instances as DistrictNet.

The details of our simulation setting, implementation, and additional experimental results are provided in the appendix.

### Main results

We evaluate the ability of the different methods to generate good solutions on a diverse set of out-of-distribution instances. All methods are evaluated using the same performance metric: the total districting cost \(C_{}(d)\) of a districting solution as presented in Problem (1), where the expected costs are evaluated using a Monte Carlo approximation. We restrict the cities to \(N=120\) BUs and vary the target district size \(t\{3,6,12,20,30\}\) for each of our seven test cities. This provides a set of \(35\) test instances, completely independent of the training data. The results are presented in Table 1 in the form of an ablation study. The table shows the relative difference in average costs achieved by each method on the test instances and the statistical significance is assessed using a one-sided Wilcoxon test. Example districting solutions are also given in Figure 2.

Table 1 shows that DistrictNet consistently outperforms the benchmarks as it produces districting solutions with significant cost reductions of around \(10\%\) compared to all other methods. The benchmarks all provide similar performance, even PredGNN, which uses a graph neural network but does not use structured learning. This highlights the ability of DistrictNet to generalize across various city structures and for larger instances thanks to the combination of a graph neural network and a differentiable optimization layer.

**Result 1**.: DistrictNet _can solve a large family of districting problems with varying geographical and population data as well as varying problem parameters._

**Large cities.** We perform an additional experiment to investigate the generalization to cities of large sizes. In Figure 3, we show the cost of the constricting solution obtained with the benchmark methods relative to DistrictNet for varying city sizes. A value greater than \(100\%\) means that the benchmark performs worse than DistrictNet. We increase the number of BUs for each city and keep constant the target number of BUs in a district as \(t=20\). The time limit of ILS is kept to \(20\) for all methods when \(N<400\) and increased to \(60\) when \(N 400\).

The results show that DistrictNet provides very good solutions up to the largest city sizes. It consistently outperforms the benchmarks even for large cities. These results are achieved despite DistrictNet being trained on small instances of size \(N=30\) BUs.

We investigate further the scalability of our approach by considering a large instance with 2,000 BUs in the Ile-de-France region. Each method is allowed 60 minutes to compute the districting solution, with the target district size set to 20 BUs. The results are presented in Table 2. DistrictNet provides the best performance, showing that it generalizes even to instances that are more than 60 times larger than the training ones.

**Result 2**.: DistrictNet _provides high-quality solutions to even the largest real-world problems._

**Why does unstructured learning fail?** One potential explanation for the poor performance of the benchmarks, in particular for PredGNN, is the change in distribution between the training and test instances. As shown in Figure 4, the distribution of district costs varies greatly with the city and parameters. Since there is a shift in the data-generating distribution, the benchmarks, which ignore the structure of the districting problems, are not able to accurately predict the district costs resulting in poor overall performance.

    & Average relative cost & \(p\)-value \\  Benchmark 1: BD, linear regression & 9.92 \% & 4.9e-09 \\ Benchmark 2: FIG, linear regression & 10.01 \% & 8.9e-09 \\ Benchmark 3: PredGNN, unstructured learning with GNN & 11.91 \% & 1.5e-10 \\ Benchmark 4: AvgTSP, no learning & 4.44 \% & 2.7e-04 \\ DistrictNet: structured learning with CMST and GNN & 0.0 \% & - \\   

Table 1: Ablation study showing the value of combining GNN and structured learning.

Figure 3: Cost relative to DistrictNet for target district size \(t=20\) and varying city size.

Figure 2: Districting solutions given by BD, FIG, PredictGNN, and DistrictNet for the city of **Manchester** with district target sizes of \(20\) BUs. The depot is shown as a white star.

**Result 3**.: _Thanks to its surrogate optimization layer, DistrictNet captures the general structure of the districting problem. Thus, it is less sensitive to shifts in the data-generating distribution._

**The value of data for decision-aware learning.** Finally, we investigate the value of training data for DistrictNet. In Figure 5, we study the out-of-distribution performance of DistrictNet as the size of the training set increases. We show the average districting cost over all cities and target sizes relative to the cost for \(n=20\) and add a \(95\%\) confidence interval as a shaded area. A value smaller than \(100\%\) means that DistrictNet improves compared to its training with \(n=20\). The figure shows that DistrictNet can achieve low costs even with a surprisingly small number of training examples (\(n=50\)). Increasing the number of examples tends to improve the results although with a diminishing return.

**Result 4**.: DistrictNet _can be trained with a small computational budget and benefits from increasing the number of training examples._

## 6 Related literature

In this work, we find near-optimal solutions to complex combinatorial problems by introducing a combinatorial layer in a deep neural network trained in a decision-aware fashion. Our work lies at the intersection of the learning-to-optimize literature and decision-aware learning.

**Learning to optimize.** Recent years have seen a significant increase in the use of machine learning for solving hard combinatorial optimization problems. Several graph-based learning approaches have been proposed for general combinatorial problems (Cappart et al., 2023). Deep reinforcement learning has been applied to solve typical combinatorial problems such as the traveling salesman and knapsack problems (Bello et al., 2016) and the minimum vertex cover and maximum cut problems (Dai et al., 2017). Gasse et al. (2019) presented a technique to improve the branch-and-bound process in mixed-integer linear programming by using graph convolutional neural networks.

The main advantage of learning-based methods is that, after a potentially expensive training procedure, they can generate good solutions to combinatorial problems in a short time. This has been shown to be effective in solving routing problems, which are complex combinatorial problems. Joshi et al. (2019) applied a beam search to solve the Euclidean TSP using GNNs and show notable improvements in solution quality, speed, and efficiency. Kool et al. (2019) combined a greedy rollout baseline and a deep attention model to solve several challenging routing problems such as the orienteering problem and the prize-collecting TSP.

**Decision-aware learning.** Decision-aware learning, on the other hand, looks into including an optimization layer in deep learning architectures. A key challenge in this area is to propagate a meaningful gradient through this non-smooth layer. Amos and Kolter (2017) developed a method to compute the gradients of quadratic programs by differentiating the Karush-Kuhn-Tucker optimality conditions. In the case of (integer) linear programs, propagating this gradient can be performed forinstance by introducing a log-barrier term to the LP relaxation (Mandi and Guns, 2020), using a piecewise-linear interpolation technique (Vlastelica et al., 2019), or using perturbation (Berthet et al., 2020). We refer the interested reader to Mandi et al. (2023) and Sadana et al. (2023) for surveys on decision-aware learning and its generalization as contextual stochastic optimization. Decision-aware learning has diverse applications such as approximating hard optimization problems by learning linear surrogate models (Ferber et al., 2023; Dalle et al., 2022).

Decision-aware learning allows the integration of complex algorithms with combinatorial behavior into deep learning architectures. Wilder et al. (2019) learn to solve hard combinatorial problems by learning from incomplete graphs using a differentiable k-means clustering algorithm. Stewart et al. (2023) present a differentiable clustering approach with a partial cluster-connectivity matrix. Our paper differs in two significant ways. First, we use a surrogate model with substantially more structure than clustering since it includes constraints on the size of the districts. This raises computational challenges, since the surrogate model remains NP-hard, but allows significant benefits in the quality of solutions. Second, while we have "full" districting solutions available, i.e., we know exactly what nodes need to be in the same clusters for a given training example, we have no information on the corresponding CMST solution. This leads us to introduce a randomized target construction algorithm, which leads to a well-defined loss function with advantageous properties.

Our approach belongs to the research stream that learns to approximate hard problems by easier ones. Key advantages of these approaches include being efficient at inference time since most of the computation effort is shifted offline, and great performance on test instances if they remain close to the training distribution. There are also downsides. First, until now, there are no known worst-case theoretical guarantees on the quality of the solution. Second, learning may be intensive computationally, both when generating the training instances and in the training algorithm. We refer the reader to Aubin-Frankowski et al. (2024) for a general discussion of these aspects and an analytical characterization of generalization bounds.

## 7 Conclusion

This paper presented a general pipeline to learn to solve graph-partitioning problems. It integrates a CMST as a surrogate optimization layer, which allows it to capture efficiently the structure of graph partitioning while providing solutions in a short time. We demonstrate the value of our pipeline on a districting and routing application. We show that our method outperforms recent and traditional benchmarks and is able to generalize to out-of-distribution instances. Thus, it can be trained on a small set of examples and applied to a wide array of cities, instance sizes, and hyperparameters.

Future work could investigate alternative approaches to solve the CMST problem during training and testing, such as exact methods based on column generation, which would integrate seamlessly with the pipeline presented in this paper. A limitation of our approach is applying DistrictNet only to districting and routing. Other geographical partitioning problems such as designing voting or school districts could be considered in future research.