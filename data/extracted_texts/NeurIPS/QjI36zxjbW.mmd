# Neural-Logic Human-Object Interaction Detection

Lilei Li\({}^{1}\), Jianan Wei\({}^{2}\), Wenguan Wang\({}^{2}\), Yi Yang\({}^{2}\)

\({}^{1}\)ReLER, AAII, University of Technology Sydney \({}^{2}\)CCAI, Zhejiang University

https://github.com/weijianan1/LogicHOI

Corresponding Author: Wenguan Wang.

###### Abstract

The interaction decoder utilized in prevalent Transformer-based HOI detectors typically accepts pre-composed human-object pairs as inputs. Though achieving remarkable performance, such paradigm lacks feasibility and cannot explore novel combinations over entities during decoding. We present LogicHOI, a new HOI detector that leverages neural-logic reasoning and Transformer to infer feasible interactions between entities. Specifically, we modify the self-attention mechanism in vanilla Transformer, enabling it to reason over the \(\)human, action, object\(\) triplet and constitute novel interactions. Meanwhile, such reasoning process is guided by two crucial properties for understanding HOI: _affordances_ (the potential actions an object can facilitate) and _proxemics_ (the spatial relations between humans and objects). We formulate these two properties in _first-order_ logic and ground them into continuous space to constrain the learning process of our approach, leading to improved performance and zero-shot generalization capabilities. We evaluate LogicHOI on V-COCO and HICO-DET under both normal and zero-shot setups, achieving significant improvements over existing methods.

## 1 Introduction

The main purpose of human-object interaction (HOI) detection is to interpret the intricate relationships between human and other objects within a given scene . Rather than traditional visual _perception_ tasks that focus on the recognition of objects or individual actions, HOI detection places a greater emphasis on _reasoning_ over entities , and can thus benefit a wide range of scene understanding tasks, including image synthesis, visual question answering , and caption generation , _etc_.

Current top-leading HOI detection methods typically adopt a Transformer -based architecture, wherein the ultimate predictions are delivered by an interaction decoder. Such decoder takes consolidated embeddings of predetermined human-object pairs as inputs, and it solely needs to infer the action or interaction categories of given entity pairs. Though achieving remarkable performance over CNN-based work, such paradigm suffers from several limitations: **first**, the principal focus of them is to optimize the samples with known concepts, ignoring a large number of feasible combinations that were never encountered within the training dataset, resulting in poor zero-shot generalization ability. **Second**, the human-object pairs are usually proposed by a simple MLP layer or simultaneously constructed when predicting humans and objects, without explicit modeling of the complex relationships among subjects, objects, and the ongoing interactions that happened between them. **Third**, existing methods lack the reasoning ability, caused by ignoring the key nature of HOI detection, _i.e_., the interaction should be mined between entities, but not pre-given pairs.

In light of the above, we propose to solve HOI detection via the integration of Transformer and logic-induced HOI relation learning, resulting in LogicHOI, which enjoys the advantage of robust distributed representation of entities as well as principled symbolic reasoning . Thoughit was claimed that Seq2Seq models (_e.g._, Transformer) are inefficient to tackle visual _reasoning_ problem, models based on Transformer with improved architectures and training strategies have already been applied to tasks that require strong _reasoning_ ability such as Sudoku, Raven's Progressive Matrices, compositional semantic parsing, spatial relation recognition, to name a few, and achieve remarkable improvements. Given the above success, we would like to argue that "Transformers are non-trivial symbolic reasoners, with only _slight architecture changes_ and _appropriate learning guidances_", where the later imposes additional constraints for the learning and reasoning of modification. Specifically, for _architecture changes_, we modify the attention mechanism in interaction decoder that sequentially associates each counterpart of the current _query_ individually (Fig. 1: left), but encourage it to operate in a triplet manner where states are updated by combining \(\)human, action, object\(\) three elements together, leading to _triplet-reasoning attention_ (Fig. 1: middle). This allows our model to directly reason over entity and the interaction they potentially constituted, therefore enhancing the ability to capture the complex interplay between humans and objects. To _appropripriately guide_ the learning process of such triplet reasoning in Transformer, we explore two HOI relations, _affordances_ and _proxemics_. The former refers to that an object facilitates only a partial number of interactions, and objects allowing for executing a particular action is predetermined. For instance, the human observation of a kite may give rise to imagined interactions such as launch or fly, while other actions such as repair, throw are not considered plausible within this context. In contrast, _proxemics_ concerns the spatial relationships between humans and objects, _e.g._, when a human is positioned below something, the candidate actions are restricted to airplane, kite, _etc_. This two kind of properties are stated in _first-order_ logical formulae and serve as optimization objectives of the outputs of Transformer. After multiple layers of reasoning, the results are expected to adhere to the aforementioned semantics and spatial knowledge, thereby compelling the model to explore and learn the reciprocal relations between objects and actions, eventually producing more robust and logical sound predictions. The integration of logic-guided knowledge learning serves as a valuable complement to _triplet-reasoning attention_, as it constrains _triplet-reasoning attention_ to focus on rule-satisfied triplets and discard unpractical combinations, enabling more effective and efficient learning, as well as faster convergence.

By enabling reasoning over entities in Transformer and explicitly accommodating the goal of promoting such ability through logic-induced learning, our method holds several appealing facets: **first**, accompanying reasoning-oriented architectural design, we embed _affordances_ and _proxemics_ knowledge into Transformer in a logic-induced manner. In this way, our approach is simultaneously a continuous neural computer and a discrete symbolic reasoner (albeit only implicitly), which meets the formulation of human cognition. **Second**, compared to neuro-symbolic methods solely driven by loss constraints, which are prone to be over-fitting on supervised learning and disregard reasoning at inference stage, we supplement it with task-specific architecture changes to facilitate more flexible inference (_i.e._, _triplet-reasoning_ attention). Additionally, the constraints are tailored to guide the learning of interaction decoders rather than the entire model, to prevent "cheating". **Third**, our method does not rely on any discrete symbolic reasoners (_e.g._, MLN or ProbLog) which increases the complexity of the model and cannot be jointly optimized with neural networks. **Fourth**, entities fed into Transformer are composed into interactions by the model automatically, such a compositional manner contributes to improved zero-shot generalization ability.

To the best of our knowledge, we are the first that leverages Transformer as the interaction _reasoner_. To comprehensively evaluate our method, we experiment it on two gold-standard HOI datasets (_i.e._, V-COCO and HICO-DET), where we achieve **35.47%** and **65.0%** overall mAP score, setting new state-of-the-arts. We also study the performance under the zero-shot setup from four different perspectives. As expected, our algorithm consistently delivers remarkable improvements, up to **+5.16%** mAP under the _unseen object_ setup, outperforming all competitors by a large margin.

Figure 1: **Left**: self-attention aggregates information across pre-composed interaction () _queries_. **Middle**: in contrast, our proposed _triplet-reasoning_ attention traverses over human (), action (), and object () _queries_ to propose plausible interactions. **Right**: logic-induced _affordances_ and _proxemics_ knowledge learning.

Related Work

**HOI Detection.** Early CNN-based solutions for HOI detection can be broadly classified into two paradigms: two-stage and one-stage. The two-stage methods  first detect entities by leveraging off-the-shelf detectors (_e.g._, Faster R-CNN) and then determine the dense relationships among all possible human-object pairs. Though effective, they suffer from expensive computation due to the sequential inference architecture, and are highly dependent on prior detection results. In contrast, the one-stage methods jointly detect human-object pairs and classify the interactions in an end-to-end manner by associating humans and objects with predefined anchors, which can be union boxes or interaction points. Despite featuring fast inference, they heavily rely on hand-crafted post-processing to associate interactions with object detection results. Inspired by the advance of DETR, recent work typically adopts a Transformer-based architecture to predict interactions between humans and objects, which eliminates complicated post-processing and generally demonstrates better speed-accuracy trade-off. To learn more effective HOI representations, several studies  also seek to transfer knowledge from visual-linguistic pre-trained models (_e.g._, CLIP), which also enhances the zero-shot discovery capacity of models.

Although promising results have been achieved, they typically directly feed the pre-composed union representation of human-object pairs to the Transformer to get the final prediction, lacking _reasoning_ over different entities to formulate novel combinations, therefore struggling with long-tailed distribution and adapting to _unseen_ interactions. In our method, we handle HOI detection by _triplet-reasoning_ within Transformer, where the inputs are entities. Such reasoning process is also guided by logic-induced _affordances_ and _proxemics_ properties learning with respect to the semantic and spatial rules, so as to discard unfeasible HOI combinations, and enhance zero-shot generalization abilities.

**Neural-Symbolic Computing** (NSC) is a burgeoning research area that seeks to seamlessly integrate the symbolic and statistical paradigms of artificial intelligence, while effectively inheriting the desirable characteristics of both. Although the roots of NSC can be traced back to the seminal work of McCulloch and Pitts in 1943, it did not gain a systematic study until the 2000s, when a renewed interest in combining neural networks with symbolic reasoning emerged. These early methods often reveal limitations when attempting to handle large-scale and noisy data, as the effectiveness is impeded by highly hand-crafted rules and architectures. Recently, driven by both theoretical and practical perspectives, NSC has garnered increasing attention in holding the potential to model human cognition, combine modern numerical connectionist with logic reasoning over abstract knowledge, so as to address the challenges of data efficiency, explainability, and compositional generalization in purely data-driven AI systems.

In this work, we address neuro-symbolic computing from two perspectives, **first**, we employ Transformer as the symbolic _reasoner_ but with slight architectural changes, _i.e._, modifying the _self-attention_ to _triplet-reasoning attention_ and the inputs are changed from interaction pairs to human, action, and object entities, so as to empower the Transformer with strong _reasoning_ ability for handling such relation interpretation task. **Second**, we condense _affordances_ and _proxemics_ properties into logical rules, which are further served to constrain the learning and reasoning of the aforementioned Transformer _reasoner_, making the optimization goal less ambiguous and knowledge-informed.

**Compositional Generalization**, which pertains to the ability to understand and generate a potentially boundless range of novel conceptual structures comprised of similar constituents, has long been thought to be the cornerstone of human intelligence. For example, human can grasp the meaning of _dax twice_ or _dax and sing_ by learning the term _dax_, which allows for strong generalizations from limited data. In natural language processing, several efforts have been made to endow neural networks with this kind of zero-shot generalization ability. Notably, the task proposed in , referred to as SCAN, involves translating commands presented in simplified natural language (_e.g._, _dax twice_) into a sequence of navigation actions (_e.g._, I_DAX, I_DAX). Active investigations into visual compositional learning also undergo in the fields of image caption and visual question answering. For instance, to effectively and explicitly ground entities, first creates a template with slots for images and then fills them with objects proposed by open-set detectors.

Though there has already been work rethinking HOI detection from the perspective of compositional learning, they are, **i)** restricted to scenes with single object, **ii)** utilizing compositionality for data augmentation or representation learning, without generalization during learning nor relation reasoning. A key difference in our work is that our method arbitrarily constitute interactions as final predictions during decoding. This compositional learning and inference manner benefit the generalization to _unseen_ interactions. Moreover, the _affordances_ and _proxemics_ properties can be combined, _i.e._, it is natural for us to infer that when a human is above a horse, the viable interactions options can only be sit_on and ride.

## 3 Methodology

In this section, we first review Transformer-based HOI detection methods and the self-attention mechanism (SS3.1). Then, we elaborate the pipeline of our method and the proposed triplet-reasoning attention blocks (SS3.2), which is guided by the logic-induced learning approach utilizing both _affordances_ and _proxemics_ properties (SS3.3). Finally, we provide the implementation details (SS3.4).

### Preliminary: Transformer-based HOI Detection

Enlightened by the success of DETR, recent state-of-the-arts[9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20] for HOI detection typically adopt the encoder-decoder architecture based on Transformer. The key motivation shared across all of the above methods is that Transformer can effectively capture the long range dependencies and exploit the contextual relationships between human-object pairs[10; 12] by means of self-attention. Specifically, an interaction or action decoder is adopted, of which the _query_, _key_, _value_ embeddings \(^{q}\), \(^{k}\), \(^{v}^{N D}\) are constructed from the unified embedding of human-object pair \(^{h q}\) by:

\[^{q}=(+^{h q})^{q},^{k}=(+ ^{h q})^{k},^{v}=(+^{h q}) ^{v},\] (1)

where \(\) is the input matrix, \(^{q}\), \(^{k}\), \(^{v}^{D D}\) are parameter matrices and \(^{h q}\) can be derived from the feature of union bounding box of human-object or simply concatenating the embeddings of human and object together. Then \(\) is updated through a self-attention layer by:

\[^{}_{i}=^{v^{}}_{n=1}^{N}( {F}^{q}_{i}^{k}_{n}/)^{v}_{n}.\] (2)

Here we adopt the single-head variant for simplification. Note that under this scheme, the attention is imposed over action or interaction embeddings, which has already been formulated before being fed into the action or interaction decoder. This raises two concerns **i)** may discard positive human- object pair and **ii)** cannot present novel combinations over entities during decoding.

### HOI Detection via Triplet-Reasoning Attention

In contrast to the above, we aim to facilitate the attention over three key elements to formulate an interaction (_i.e._, human, action, object, therefore referring to _triplet-reasoning attention_), by leveraging the Transformer architecture. The feasible \(,,\) tuples are combined and filtered through the layer-wise inference within the Transformer. Towards this goal, we first adopt a visual encoder which consists of a CNN backbone and a Transformer encoder \(\) to extract visual features \(\). Then, the learnable human _queries_\(^{h}^{N_{h} D}\), action _queries_\(^{a}^{N_{a} D}\), and object _queries_\(^{o}^{N_{a} D}\) are fed into three parallel Transformer decoders \(^{h}\), \(^{a}\), \(^{o}\) to get the human, action, and object embeddings respectively by:

\[^{h}=^{h}(,^{h}),^{a}=^{a}( ,^{a}),^{o}=^{o}(,^{o}).\] (3)

Here, \(N_{h}=N_{a}=N_{o}\) and the superscripts are kept for improved clarity. All of the three query embeddings are then processed by linear layers to get the final predictions. For human and object _queries_, we supervise it with the class and bounding box annotations, while for the action _queries_, we only supervise them with image-level categories, _i.e._, what kinds of actions are happened in this image. After that, we adopt an interaction decoder \(^{p}\) composed by multiple Transformer layers in which the self-attention is replace by our proposed _triplet-reasoning attention_, so as to empower Transformer with the _reasoning_ ability. Specifically, in contrast to Eq.1, given \(^{h}\), \(^{a}\), \(^{o}\), the input _query_, _key_, _value_ embeddings \(^{q}\), \(^{k}\), \(^{v}\) for _triplet-reasoning attention_ are computed as:

\[^{q}=(+^{h}+^{a})^{q}^{N_{h}  N_{a} D},\] (4) \[^{k}=(+^{a}+^{o})^{k} ^{N_{a} N_{o} D},\] \[^{v}=^{v}_{h}(+^{h}+^{o})( +^{a}+^{o})^{v}_{o}^{N_{h} N_{ a} N_{a} D},\]where \(\) is the element-wise production. Note that we omit the dimension expanding operation for better visual presentation. Concretely, for \(^{q}\!=\!(\!+\!^{h_{l}}\!\!+\!^{a})\!\!^{q}\), the human _queries_\(^{h}\!\!^{N_{h} D}\) and action _queries_\(^{a}\!\!^{N_{a} D}\) are expanded to \(^{N_{h} 1 D}\) and \(^{1 N_{a} D}\), respectively. In this manner, \(^{h}\!\!+\!^{a}\) associates each human and action entity, resulting in \(N_{h}\!\!\!N_{a}\) human-action pairs in total. \(N_{a}\!\!\!N_{o}\) viable action-object pairs are risen by \(^{a}\!\!+\!^{o}\) in the same way. For the _value_ embedding \(^{v}\), it encodes the representation of all \(N_{h}\!\!\!N_{a}\!\!\!N_{o}\) potential interactions. Given a specific element in \(^{v}\), for instance, \(^{v}_{inj}\), it is composed from \(^{q}_{in}\) and \(^{k}_{inj}\), corresponding to a feasible interaction of embeddings \(^{h}_{i}\), \(^{a}_{n}\), and \(^{o}_{j}\). Then each element in inputs \(\) is updated by:

\[^{}_{ij}=^{v^{}}\!\!_{n=1}^{N_{a}}(^{q}_{in}.^{k}_{nj}/)^{v}_{inj},\] (5)

where \(^{}\) denotes the output of _triplet-reasoning_ attention. In contrast to _self-attention_ (_cf._, Eq.2), our proposed _triplet-reasoning attention_ (_cf._, Eq.5) stretches edges between every human-action and action-object pairs sharing the identical action _query_. By aggregating information from the relation between human-action and action-object, it learns to capture the feasibility of tuple \(,,\) in a compositional learning manner, simultaneously facilitating reasoning over entities. The final output \(\) of \(^{p}\) is given by:

\[=^{p}(,^{h},^{a},^{o})^ {N_{h} N_{o} D},\] (6)

which delivers the interaction prediction for \(N_{h}\!\!\!N_{o}\) human-object pairs in total. We set \(N_{h}\), \(N_{a}\), \(N_{o}\) to a relatively small number (_e.g._, 32) which is enough to capture the entities in a single image, and larger number of queries will exacerbate the imbalance between positive and negative samples. Additionally, for efficiency, we keep only half the number of human, object and action _queries_ by filtering the low-scoring ones before sending them into the interaction decoder \(^{p}\).

### Logic-Guided Learning for Transformer-based Reasoner

In this section, we aim to guide the learning and reasoning of LogicHOI with the _affordances_ and _proxemics_ properties. Though there has already been several works concerning about these two properties, they typically analyze _affordances_ from the statistical perspective, _i.e._, computing the distribution of co-occurrence of actions and objects so as to reformulate the predictions , simply integrate positional encodings into network features, or proposing a two-path feature generator which introduces additional parameters. In contrast, we implement them from the perspective that the constrained subset is the logical consequence of pre-given objects or actions.

Concretely, we first state these two kinds of properties as logical formulas, and then ground them into continuous space to instruct the learning and reasoning of our Transformer reasoner (_i.e._, interaction decoder \(^{p}\)). For _proxemics_, we define five relative positional relationships with human as the reference frame, which are above (_e.g._, kite above human), below (_e.g._, stateboard below human), around (_e.g._, giraffe around human), within (_e.g._, handbag within human), containing (_e.g._, bus containing human). To make the Transformer reasoner spatiality-aware, the human and object embeddings retrieved from Eq.3 are concatenated with sinusoidal positional encodings generated from predicted bounding boxes. Then, given action\(v\) and position relationship \(p\), the set of infeasible interactions (_i.e._, triplet \(,,\)) \(\{h_{1},,h_{M}\}\) can be derived:

\[ x(v(x) p(x) h_{1}(x) h_{2}(x)  h_{M}(x)),\] (7)

where \(x\) refer to one human-object pair that is potential to have interactions. In first-order logic, the semantics of _variables_ (_e.g._, \(x\)) is usually referred to _predicates_ (_e.g._, launch(\(x\)), above(\(x\))). Eq.7

Figure 2: Overview of LogicHOI. We first retrieve human, action, and object _queries_ by \(^{h}\), \(^{a}\), and \(^{}\), respectively. Then \(^{p}\) take them as input, reasoning over entities and combining potential interaction triplets. Finally, such process is guided by _affordances_ and _proxemics_ properties, to be more efficient and knowledge-informed.

states that, for instance, if the \(v\) is launch, and \(p\) is above, then in addition to interactions composed of actions apart from launch, human-launch-boat should be included in \(\{h_{1},,h_{N}\}\) as well. Similarly, given the object category \(o\) and position relationship \(p\), we shall have:

\[ x(o(x) p(x) h_{1}(x) h_{2}(x)  h_{N}(x)).\] (8)

With Eq. 7 and Eq. 8, both _affordances_ and _proxemics_ properties, and the combination relationship between them are clearly stated. Next we investigate how to convert the above logical symbols into differentiable operation. Specifically, logical connectives (e.g., \(\), \(\), \(\), \(\)) defined on discrete Boolean variables are grounded to functions on continuous variables using product logic:

\[&=1-+ ,=1-,\\ &=+-, =.\] (9)

Similarly, the _quantifier_ are implemented in a generalized-mean manner following:

\[ x((x))&=(_{ k=1}^{K}(x_{k})^{q})^{},\\  x((x))&=1-(_{k=1}^{K}(1- (x_{k}))^{q})^{},\] (10)

Given above relaxation, we are ready to translate properties defined in _first-order_ logical formulae into sub-symbolic numerical representations, so as to supervise the interactions \(\{h_{1},,h_{M}\}\) predicted by the Transformer reasoner. For instance, Eq. 7 is grounded by Eq. 9 and Eq. 10 into:

\[_{v,p}=1-_{m=1}^{M}(_{k=1}^{K}(s_{k} [v] s_{k}[h_{m}])),\] (11)

where \(s_{k}[v]\) and \(s_{k}[h_{m}]\) refer to the scores of action \(v\) and interaction \(h_{m}\) with respect to input sample \(x_{k}\). Here \(K\) refers to the number of all training samples and we relax it to that in a mini-batch. As mentioned above, the spatial locations of human and objects are concatenated into the _query_ which means the spatial relation is predetermined and can be effortlessly inferred from the box predictions (details provided in _Supplementary Materials_). Thus, we omit \(p(x)\) in Eq. 11. Then, the action-position loss is defined as: \(_{v,p}=1-_{v,p}\). In a similar way, we can ground Eq. 8 into:

\[_{o,p}=1-_{n=1}^{N}(_{k=1}^{K}(s_{k} [v] s_{k}[h_{n}])),\] (12)

where \(s_{k}[o]\) refers to the score of object regarding to the input sample \(x_{k}\). The object-position loss is defined as: \(_{o,p}=1-_{o,p}\). For \(_{v,p}\), it scores the satisfaction of predictions to rules defined in Eq. 7. For example, given a high probability of action ride (_i.e_., a high value of \(s_{k}[v]\)) and the position relationship is above, if the probability of infeasible interactions (_e.g_., human-feed-fish") is also high, then \(_{v,p}\) would receive a low value so as to punish this prediction. \(_{o,p}\) is similar but it scores the satisfaction of predictions to Eq. 8 with given position and objects such as horse, fish, _etc_. Through Eq. 11 and Eq. 12, we aim to achieve that, given the embeddings and locations of a group of human and object entities, along with the potential actions happened within the image, the Transformer reasoner should speculate which pair of human-object engaged in what kind of interaction, while the prediction should respect to the rules defined in Eq. 7 and Eq. 8.

### Implementation Details

**Network Architecture.** To make fair comparison with existing Transformer-based work, we adopt ResNet-50 as the backbone. The visual encoder \(\) is implemented as six Transformer encoder layers, while the three parallel human, object and action decoders \(^{h}\), \(^{a}\), \(^{o}\) are all constructed as three Transformer decoder layers. For the interaction decoder \(^{p}\), we instantiate it with three Transformer decoder layers as well, but replacing _self-attention_ with our proposed _triplet-reasoning attention_. The number of human, object, action _queries_\(N_{h}\), \(N_{o}\), \(N_{a}\) is set to 32 for efficiency, and the hidden sizes of all the modules are set to \(D\!=\!768\). Since the state-of-the-art work usually leverages large-scale visual-linguistic pre-trained models to further enhance the detection capability, we follow this setup and adopt CLIP. To improve the inference efficiency of our framework, we further follow  which uses guided embeddings to decode humans and objects in a single Transformer decoder, _i.e_., merging \(^{h}\) and \(^{o}\) into a unified one to simultaneously output both human and object predictions.

**Training Objectives.**LogicHOI is jointly optimized by the HOI detection loss (_i.e._, \(_{}\)) and logic-induced property learning loss (_i.e._, \(_{}\)):

\[=_{}+_{}, _{}=_{v,p}+_{o,p}.\] (13)

Here \(\) is set to 0.2 empirically. Note that \(_{}\) solely update the parameters of the Transformer reasoner (_i.e._, interaction decoder \(^{p}\)) but not the entire network to prevent over-fitting. For \(_{}\), it is composed of human/object (_i.e._, output of \(^{h}\) and \(^{o}\), respectively) detection loss, action (_i.e._, output of \(^{a}\)) classification loss as well as interaction (_i.e._, output of \(^{p}\)) classification loss.

## 4 Experiments

### Experimental Setup

**Datasets.** We conduct experiments on two widely-used HOI detection benchmarks:

* V-COCO  is a carefully curated subset of MS-COCO  which contains 10,346 images (5,400 for training and 4,946 for testing). There are 263 human-object interactions annotated in this dataset in total, which are derived from 80 object categories and 29 action categories.
* HICO-DET  consists of 47,776 images in total, with 38,118 for training and 9,658 designated for testing. It has 80 object categories identical to those in V-COCO and 117 action categories, consequently encompassing a comprehensive collection of 600 unique human-object interactions.

**Evaluation Metric.** Following conventions , the mean Average Precision (mAP) is adopted for evaluation. Specifically, for V-COCO, we report the mAP scores under both scenario 1 (S1) which includes all of the 29 action categories and scenario 2 (S2) which excludes 4 human body motions without interaction to any objects. For HICO-DET, we perform evaluation across three category sets: all 600 HOI categories (Full), 138 HOI categories with less than 10 training instances (Rare), and the remaining 462 HOI categories (Non-Rare). Moreover, the mAP scores are calculated in two separate setups: **i)** the Default setup computing the mAP on all testing images, and **ii)** the Known Object setup measuring the AP for each object independently within the subset of images containing this object.

**Zero-Shot HOI Detection.** We follow the setup in previous work  to carry on zero-shot generalization experiments, resulting in four different settings: Rare First Unseen Combination (RF-UC), Non-rare First Unseen Combination (NF-UC), Unseen Verb (UV) and Unseen Object (UO) on HICO-DET. Specifically, the RF and NF strategies in the UC setting indicate selecting 120 most frequent and infrequent interaction categories for testing, respectively. In the UO setting, we choose 12 objects from 80 objects that are previously unseen in the training set, while in the UV setting, we exclude 20 verbs from a total of 117 verbs during training and only use them at testing.

**Training.** To ensure fair comparison with existing work , we initialize our model with weights of DETR pre-trained on MS-COCO. Subsequently, we conducted training for 90 epochs using the Adam optimizer with a batch size of 16 and base learning rate \(1e^{-4}\), on 4 GeForce RTX 3090 GPUs. The learning rate is scheduled following a "step" policy, decayed by a factor of 0.1 at the 60th epoch. In line with , the random scaling augmentation is adopted, _i.e._, training images are resized to a maximum size of 1333 for the long edge, and minimum size of 400 for the short edge.

**Testing.** For fairness, we refrain from implementing

   Method & VL Pretrain & Type & Unseen & Seen & Full \\  VCL AccV201 & & & RF-UC & 10.06 & 24.28 & 21.43 \\ ATL AccV201 & & & RF-UC & 9.18 & 24.67 & 21.57 \\ FCL AccV201 & & & RF-UC & 13.16 & 24.23 & 22.01 \\ GEN-VLKT AccV202 & & & RF-UC & 21.36 & 32.91 & 30.56 \\ SCL AccV201 & & & RF-UC & 19.07 & 30.39 & 28.08 \\ LociHOI **(ours)** & & & RF-UC & **25.97** & **34.93** & **33.17** \\  VCL AccV201 & & & NF-UC & 16.22 & 18.52 & 18.06 \\ ATL AccV201 & & & NF-UC & 18.25 & 18.78 & 18.67 \\ FCL AccV201 & & & NF-UC & 18.66 & 19.55 & 19.37 \\ GEN-VLKT AccV2021 & & & NF-UC & 25.05 & 23.38 & 23.71 \\ SCL AccV2021 & & & NF-UC & 21.73 & 25.00 & 24.34 \\ LociHOI **(ours)** & & & NF-UC & **26.84** & **27.86** & **27.95** \\  ATL AccV201 & & & UO & 5.05 & 14.69 & 13.08 \\ FCL AccV201 & & & UO & 0.00 & 13.71 & 11.43 \\ GEN-VLKT AccV2021 & & & UO & 10.51 & 28.92 & 25.63 \\ LociHOI (ours) & & & UO & **15.67** & **30.42** & **28.23** \\  GEN-VLKT AccV2021 & & & UO & 20.96 & 30.23 & 28.74 \\ LociHOI (ours) & & & UV & **24.57** & **31.88** & **30.77** \\   

Table 1: Comparison of zero-shot generalization with state-of-the-arts on HICO-DET  test. See §4.2 for details.

[MISSING_PAGE_FAIL:8]

**V-COCO.** As indicated by the last two columns of Table 2, we also compare LogicHOI with competitive models on V-COCO test. Despite the relatively smaller number of images and HOI categories in this dataset, our method still yields promising results, showcasing its effectiveness. In particular, it achieves a mean mAP score of **65.0%** across two scenarios.

### Diagnostic Experiment

For in-depth analysis, we perform a series of ablative studies on HICO-DET test.

**Key Component Analysis.** We first examine the efficiency of essential designs of LogicHOI, _i.e._, _triplet-reasoning attention_ (TRA) and logic-guided reasoner learning (LRL), which is summarized in Table 3. Three crucial conclusions can be drawn. First, our proposed _triplet-reasoning attention_ leads to significant performance improvements against the baseline across all the metrics. Notably, TRA achieves **4.53%** mAP improvement on Rare congeries, demonstrating the ability of our Transformer Reasoner to reason over entities and generate more feasible predictions. Second, we also observe compelling gains from incorporating logic-guided reasoner learning into the baseline, even with basic self-attention, affirming its versatility. Third, our full model LogicHOI achieves the satisfactory performance, confirming the complementarity and effectiveness of our designs.

**Logic-Guided Learning.** We guide the learning of Transformer reasoner with two logic-induced properties. Table 4 reports the related scores of _unseen_ categories under three zero-shot setups. The contributions of \(_{v,p}\) and \(_{o,p}\) are approximately equal in the RF-UC setup, since during training, all actions and objects can be seen and utilized to guide reasoning. On the other hand, under the UO and UV setups, the improvements heavily rely on \(_{v,p}\) and \(_{o,p}\) respectively, while the other one brings minor enhancements. Finally, the combination of them leads to LogicHOI, the new state-of-the-art.

**Number of Decoder Layer.** We further examine the effect of the number of Transformer decoder layer used in \(^{p}\). As shown in Table 4(a), LogicHOI achieves similar performance when \(L\) is larger than 2. For efficiency, we set \(L=3\) which is the smallest among existing work.

**Number of Query.** Next we probe the impact of the number of _query_ for human, object and action in Table 4(b). Note that the number of these three kind of queries is identical and we refer it to \(N\). The best performance is obtained at \(N=32\) and more _queries_ lead to inferior performance.

   Method & Backbone & Params & FLOPs & FPS \\  Two-stages Detectors: & & & & \\ iCAN \({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\) & R50 & 39.8 & - & 5.99 \\ DRG \({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\) & R50-FPN & 46.1 & - & 6.05 \\ SCG \({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\) & R50-FPN & 53.9 & - & 7.13 \\ STIP \({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\) & R50 & 50.4 & - & 6.78 \\  One-stages Detectors: & & & & \\ PPDM \({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\) & HG104 & 194.9 & - & 17.14 \\ HOTR \({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\) & R50 & 51.2 & 90.78 & 15.18 \\ HOITrans \({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\) & R50 & 41.4 & 87.69 & 18.29 \\ AS-Net \({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\) & R50 & 52.5 & 87.86 & 16.79 \\ QPIC \({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\) & R50 & 41.9 & 88.87 & 16.79 \\ CDN-S \({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\) & R50 & 42.1 & - & 15.54 \\ GEN-VLK\({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\)\({}_{}\) & R50 & 42.8 & 86.74 & 18.69 \\  LogicHOI (ours) & R50 & 49.8 & 89.65 & 16.84 \\   

Table 4: Analysis of LRL under the zero-shot setup of _unseen_ categories. See §4.4 for details.

    &  & Full & Rare & Non-Rare \\   & & 31.87 & 26.14 & 33.29 \\  ✓ & & 34.32 & 30.67 & 35.19 \\  & ✓ & 33.26 & 29.53 & 34.56 \\  ✓ & ✓ & **35.47\({}_{}\)\({}_{

Runtime Analysis.The computational complexity of our _triplet-reasoning attention_ is squared compared to _self-attention_. Towards this, we make some specific designs: **i)** both the number of _queries_ and Transformer decoder layers of our method are the smallest when compared to existing work, **ii)** as specified in SS3.2, we filter the human, action, object _queries_ and only keep half of them for efficiency, and **iii)**_triplet-reasoning attention_ introduces few additional parameters. As summarized in Table 6, above facets make LogicHOI even smaller in terms of Floating Point Operations (FLOPs) and faster in terms of inference compared to most existing work.

## 5 Conclusion

In this work, we propose LogicHOI, the first Transformer-based neuro-logic reasoner for HOI detection. Unlike existing methods relying on predetermined human-object pairs, LogicHOI enables the exploration of novel combinations of entities during decoding, improving effectiveness as well as the zero-shot generalization capabilities. This is achieved by **i)** modifying the _self-attention_ mechanism in vanilla Transformer to reason over \(\)human, action, object\(\) triplets, and **ii)** incorporating _affordances_ and _proxemics_ properties as logic-induced constraints to guide the learning and reasoning of LogicHOI. Experimental results on two gold-standard HOI datasets demonstrates the superiority against existing methods. Our work opens a new avenue for HOI detection from the perspective of empowering Transformer with symbolic reasoning ability, and we wish it to pave the way for future research.

**Acknowledgement.** This work was partially supported by the Fundamental Research Funds for the Central Universities (No. 226-2022-00051) and CCF-Tencent Open Fund.