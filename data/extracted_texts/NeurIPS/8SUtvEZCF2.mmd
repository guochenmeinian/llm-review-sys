# Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination

Yuchen Bai\({}^{1}\) Jean-Baptiste Durand\({}^{2}\) Gregoire Vincent\({}^{2}\) Florence Forbes\({}^{1}\)

\({}^{1}\)Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LJK, Grenoble, France

\({}^{2}\)AMAP, Univ. Montpellier, CIRAD, CNRS, INRAE, IRD, Montpellier, France

{yuchen.bai,florence.forbes}@inria.fr

jean-baptiste.durand@cirad.fr

gregoire.vincent@ird.fr

Corresponding author.

###### Abstract

LiDAR (Light Detection And Ranging) has become an essential part of the remote sensing toolbox used for biosphere monitoring. In particular, LiDAR provides the opportunity to map forest leaf area with unprecedented accuracy, while leaf area has remained an important source of uncertainty affecting models of gas exchanges between the vegetation and the atmosphere. Unmanned Aerial Vehicles (UAV) are easy to mobilize and therefore allow frequent revisits, so as to track the response of vegetation to climate change. However, miniature sensors embarked on UAVs usually provide point clouds of limited density, which are further affected by a strong decrease in density from top to bottom of the canopy due to progressively stronger occlusion. In such a context, discriminating leaf points from wood points presents a significant challenge due in particular to strong class imbalance and spatially irregular sampling intensity. Here we introduce a neural network model based on the Pointnet ++ architecture which makes use of point geometry only (excluding any spectral information). To cope with local data sparsity, we propose an innovative sampling scheme which strives to preserve local important geometric information. We also propose a loss function adapted to the severe class imbalance. We show that our model outperforms state-of-the-art alternatives on UAV point clouds. We discuss future possible improvements, particularly regarding much denser point clouds acquired from below the canopy.

## 1 Introduction

In the past decades, LiDAR technology has been frequently used to acquire massive 3D data in the field of forest inventory (Vincent et al. ; Ullrich & Pfennigbauer ). The acquisition of point cloud data by employing LiDAR technology is referred to as laser scanning. The collected point cloud data provides rich details on canopy structure, allowing us to calculate a key variable, leaf area, which controls water efflux and carbon influx. Monitoring leaf area should help in better understanding processes underlying flux seasonality in tropical forests, and is expected to enhance the precision of climate models for predicting the effects of global warming. There are various types of vehicles for data collection, with ground-based equipment and aircraft being the most commonly employed. The former operates a bottom-up scanning called terrestrial laser scanning (TLS), providing highly detailed and accurate 3D data. Scans are often acquired in a grid pattern every 10 m and co-registered into a single point cloud. However, TLS requires human intervention within the forest, which is laborious and limits its extensive implementation. Conversely, airborne laser scanning (ALS) is much faster and can cover much larger areas. Nonetheless, the achieved point density is typically twoorders of magnitude smaller due to the combined effect of high flight altitude and fast movement of the sensor. Additionally, occlusions caused by the upper tree canopy make it more difficult to observe the understory vegetation.

In recent years, the development of drone technology and the decreasing cost have led to UAV laser scanning (ULS) becoming one favored option (Brede et al. ). It does not require in-situ intervention and each flight can be programmed to cover a few hectares. The acquired data is much denser than ALS (see Figure 1(a) and Figure 1(b)), which provides us with more comprehensive spatial information. Increasing the flight line overlap results in multiple angular sampling, higher point density and mitigates occlusions. Although the data density is still relatively low, compared with TLS, ULS can provide previously unseen oversty details due to the top-down view and overlap flight line. Furthermore, ULS is considered to be more suitable for conducting long-term monitoring of forests than TLS, as it allows predefined flight plans with minimal operator involvement.

Consequently, leaf-wood semantic segmentation in ULS data is required to accurately monitor foliage density variation over space and time. Changes in forest foliage density are indicative of forest functioning and their tracking may have multiple applications for carbon sequestration prediction, forest disease monitoring and harvest planning. Fulfilling these requirements necessitates the development of a robust algorithm that is capable to classify leaf and wood in forest environments. While numerous methods have demonstrated effective results on TLS data (see Section 2), these methods cannot be applied directly to ULS, due in particular to the class imbalance issue: leaf points account for about \(95\%\) of the data. Another problem is that many methods rely on the extra information provided by LiDAR devices, such as intensity. In the context of forest monitoring, intensity is not reliable due to frequent pulse fragmentation and variability in natural surface reflectivity (see Vincent et al. ). Furthermore, the reflectivity of the vegetation is itself affected by diurnal or seasonal changes in physical conditions, such as water content or leaf orientation (Brede et al. ). Therefore, methods relying on intensity information (Wu et al. ) may exhibit substantial variations in performance across different locations and even within the same location for different acquisition batches. To address this issue, certain methods (LeWos proposed by Wang et al. ; Morel et al. ) have good results while exclusively utilizing the spatial coordinates of LiDAR data.

Inspired by the existing methods, we propose a novel end-to-end approach **SOUL** (Semantic segmentation On ULs) based on PointNet++ proposed by Qi et al.  to perform semantic segmentation on ULS data. SOUL uses only point coordinates as input, aiming to be applicable to point clouds collected in forests from various locations worldwide and with sensors operating at different wavelengths. The foremost concern to be tackled is the acquisition of labeled ULS data. Since no such data set existed up to now, we gathered a ULS data set comprising 282 trees labeled as shown in Figure 4. This was achieved through semi-automatic segmentation of a coincident TLS point cloud and wood/leaf label transfer to ULS point cloud. Secondly, the complex nature of tropical forests necessitates the adoption of a data pre-partitioning scheme. While certain methods (Krisanski et al. ; Wu et al. ) employ coarse voxels with overlap, such an approach leads to a fragmented representation and incomplete preservation of the underlying geometric information. The heterogeneous distribution of points within each voxel, including points from different trees and clusters at voxel boundaries, poses difficulties for data standardization. We introduce a novel data preprocessing methodology named geodesic voxelization decomposition (GVD), which leverages geodesic distance as a metric for partitioning the forest data into components and uses the topological features, like intrinsic-extrinsic

Figure 1: Point clouds produced by three scanning modes on the same area (\(20 20 42\)), illustrate how much the visibility of the understory differs. The colors in the figure correspond to different labels assigned to the points, where red and green indicate leaves and wood, respectively. Blue points are unprocessed, so labeled as unknown. TLS data were initially subjected to semantic segmentation using LeWos  and subsequently manually corrected. Next, the K-nearest neighbors (KNN) algorithm was used to assign labels to ALS and ULS data based on the majority label among their five nearest neighbors in TLS data.

ratio (IER) (He et al. ; Liu et al. ), to preserve the underlying geometric features at component level (see Section 1). The last issue concerns the class imbalance problem during the training stage. To address this issue, we developed a novel loss function named the rebalanced loss, which yielded improved performance compared with the focal loss (Lin et al. ) for our specific task. This enhancement resulted in a 23% increase in the ability to recognize wood points, see Table 1.

The contribution of our work is three-fold. First, SOUL is the first approach developed to tackle the challenge of semantic segmentation on tropical forest ULS point clouds. SOUL demonstrates better wood point classification in complex tropical forest environments while exclusively utilizing point coordinates as input. Experiments show that SOUL exhibits promising generalization capabilities, achieving good performance even on data sets from other LiDAR devices, with a particular emphasis on overstory. Secondly, we propose a novel data preprocessing method, GVD, used to pre-partition data and address the difficult challenge of training neural networks from sparse point clouds in tropical forest environments. Third, we mitigate the issue of imbalanced classes by proposing a new loss function, referred to as rebalanced loss function, which is easy to use and can work as a plug-and-play for various deep learning architectures. The data set (Bai et al. ) used in the article is already available in open access at https://zenodo.org/record/8398853 and our code is available at https://github.com/Na1an/phd_mission.

## 2 Related Work

Classical methods.Classical methods are prevalent for processing point cloud data in forest environments, especially for semantic segmentation of TLS data, where they have achieved high levels of efficacy and applicability in practice. One such method, LeWos (Wang et al. ) uses geometric features and clustering algorithms, while an alternative method proposed by the first author of LeWos (Wang ) employs superpoint graphs (Landrieu and Simonovsky ). Moorthy et al.  use random forests (RF), and Zheng et al.  explore Gaussian mixture models. Other approaches, such as those proposed by Lalonde et al. , Itakura et al. , Vicari et al. , and Wan et al. , have also demonstrated effective semantic segmentation of TLS data.

Deep Learning methods.Pioneering works like F5CT proposed by Krisanski et al.  and the method proposed by Morel et al.  have shown promising results by using PointNet++ . Shen et al.  use PointCNN (Li et al. ) as the backbone model to solve the problem of segmenting tree point clouds in planted trees. Wu et al.  propose FWCNN, which incorporates intensity information and provides three new geometric features. Windrim & Bryson  propose a method that combines Faster R-CNN  and Pointnet , which utilizes the bird's eye view technique for single tree extraction and performs semantic segmentation on individual trees. In general, deep neural network-based approaches have demonstrated better performance in semantic segmentation of TLS point clouds compared to classical methods.

Figure 2: Overview of SOUL. (a) We use only the coordinates of raw LiDAR data as input. (b) Four geometric features linearity, sphericity, verticality, and PCA1 are calculated at three scales using eigenvalues, then standardized. (c) GVD proposes partitioned components and performs data normalization within these components. (d) Training deep neural network. (e) Finally, output are point-wise predictions.

## 3 SOUL: Semantic segmentation on ULS

SOUL is based on PointNet++ Multi-Scale Grouping (MSG)  with some adaptations. The selection of PointNet++ is not only because of its demonstrated performance in similar tasks (Krisanski et al. ; Morel et al. ; Windrim & Bryson), but also because of the lower GPU requirements (Choe et al. ) compared with transformer-based models developed in recent years, like the method proposed by Zhao et al. . The main idea of SOUL lies in leveraging a geometric approach to extract preliminary features from the raw point cloud, these features are then combined with normalized coordinates into a deep neural network to obtain more abstract features in some high dimensional space . We will introduce our method in detail as follows.

### Geometric feature computation

At this stage, we introduce four point-wise features: linearity, sphericity, verticality, and PCA1, which are computed at multiple scales of 0.3 m, 0.6 m, and 0.9 m in this task. To calculate these features, we need to construct the covariance matrix \(_{p_{i}}\) for each point \(p_{i}\), compute its eigenvalues \(_{1,i}>_{2,i}>_{3,i}>0\) and the corresponding normalized eigenvectors \(e_{1,i}\), \(e_{2,i}\), \(e_{3,i}\). The local neighbors \(_{p_{i}}\) of \(p_{i}\) are given by:

\[_{p_{i}}=\{q q,\,d_{e}(p_{i},q) r\}\] (1)

where \(\) is the set of all points, \(d_{e}\) is the Euclidean distance and \(r\{0.3,0.6,0.9\}\).

The covariance matrix \(_{p_{i}}\) is:

\[_{p_{i}}=_{p_{i}}}_{p_{j}_{ p_{i}}}(p_{j}-)(p_{j}-)^{T}\] (2)

where \(\) is the barycenter of \(_{p_{i}}\).

The equations for computing the four characteristics are as follows:

Linearity\(L_{p_{i}}\) serves as an indicator for the presence of a linear 1D structure (Weinmann et al. ).

\[L_{p_{i}}=-_{2,i}}{_{3,i}}\] (3)

Sphericity\(S_{p_{i}}\) provides information regarding the presence of a volumetric 3D structure (Weinmann et al. ).

\[S_{p_{i}}=}{_{1,i}}\] (4)

Verticality\(V_{p_{i}}\) indicates the property of being perpendicular to the horizon (Hackel et al. ), LeWos  uses it as a crucial feature because of its high sensitivity to the tree trunk. Here \([0\ 0\ 1]\) is the vector of the canonical basis.

\[V_{p_{i}}=1-|\ [0\ 0\ 1],\ e_{3,i}\] (5)

Pca1\(PCA1_{p_{i}}\) reflects the slenderness of \(_{p_{i}}\). The direction of the first eigenvector \(e_{1}\) is basically the most elongated in \(_{p_{i}}\).

\[PCA1_{p_{i}}=}{_{1,i}+_{2,i}+_{3,i}}\] (6)

Each point is endowed now with 12 features encompassing 3 scales, enabling a precise depiction of its representation within the local structure. These features are subsequently standardized and integrated into the model's input as an extension to point coordinates for the deep learning model. Refer to Supplementary Section E for single-scale and multi-scale ablation studies.

### Data pre-partitioning

The GVD algorithm 1 is a spatial split scheme used to partition the ULS data while preserving the topology of the point cloud. This approach enables the extraction of a set of representative training samples from raw forest data, while preserving the local geometry information in its entirety. The point cloud data is first voxelized into a voxel grid \(\) with a voxel size \(s\) equal to 0.6 m, an empirical value, and equipped with a neighborhood system \(\). Each voxel \(v\) is considered as occupied if it contains at least one point. The geodesic-voxelization distance \(d_{gv}\) between two voxels is defined as 1 if they are adjacent, and as the Manhattan distance between \(v\) and \(v^{}\) in voxel space otherwise. This can be expressed mathematically as:

\[d_{gv}(v,\,v^{})=1,&$ are adjacent in $$}\\ |x-x^{}|+|y-y^{}|+|z-z^{}|,& \] (7)

where \((x,y,z)\) and \((x^{},y^{},z^{})\) denote the integers coordinates referring to the index of \(v\) and \(v^{}\) respectively. We can establish an inter-voxel connectivity information with \(d_{gv}\) that enables the computation of the intrinsic-extrinsic ratio (IER):

\[IER(v,\,v^{})=(v,\,v^{})}{d_{e}(v,\,v^{})}\.\] (8)

We believe that \(d_{gv}\) and \(IER\) may provide strong cues for the subsequent segmentation task.

Subsequently, we choose one lowest voxel \(\) on the vertical z-axis then employ the Breadth-first search (BFS) algorithm to explore its neighbors and the neighbors of neighbors \(()\) and calculate the corresponding \(d_{gv}(,)\) and \(IER(,)\) where \(()\). This procedure terminates when the geodesic-voxelization distance \(d_{gv}\) exceeds \(\) or the \(IER\) exceeds \(\). \(\) is set to 10 and \(\) is set to 1.5 in this task. All the points within \(()\) are extracted and consolidated as a component \(_{i}\) of the point clouds (see Figure 3(a) and Algorithm 1). The whole procedure is repeated until all the data is processed.

As the data is partitioned into multiple components, the points within one component \(_{i}\) are further sorted based on their geodesic-voxelization distance. In this arrangement, points inside one voxel \(v\) have the same geodesic distance \(d_{gv}(,v)\), where \(\) is the lowest voxel inside \(_{i}\) (see Figure 3(b)). Thus, the GVD algorithm concludes at this stage. Moreover, minimum voxel number and minimum point number within a component are configurable, more details are provided in Supplementary Section C.

### Normalization inside component

Shifting and scaling the point cloud coordinates before training a deep neural network is a widely adopted practice that can lead to improved training stability, generalization, and optimization efficiency, see Qi et al. . Here, prior to training the model, the point coordinates are shifted to (0,0,0) and normalized by the longest axis among the three, all the point coordinates being confined within the (0,1) range. This prevents certain coordinates from dominating the learning process simply because they have larger values.

### Deep neural network training

We employ a modified version of PointNet++ MSG for SOUL . The data combination of points and features within a component is further partitioned into batches of 3,000, which serve as the input for the network. If a batch within a component has fewer than 3,000 points, the remaining points are randomly chosen from the same component. The labels of the leaves and wood points are 0 and 1, respectively. Define \(B_{k}\) as such a batch, \(|B_{k}|=3,000\) and \(B_{k}=B_{k,0}+B_{k,1}\) where \(B_{k,0}\) and \(B_{k,1}\) respectively represent the disjoint sub-batches in \(B_{k}\) with ground-truth leaf points and wood points.

Addressing class imbalance issue.Within the labeled ULS training data set, only \(4.4\%\) are wood points. The model is overwhelmed by the predominant features of leaves. Therefore, we propose a rebalanced loss \(L_{R}\) that changes the ratio of data participating to 1:1 at the loss calculation stage by randomly selecting a number of leaf points equal to the number of wood points.

In practice, the rebalanced loss is:

\[L_{R}(Y_{B_{k}})=- y_{k}(_{k})+(1-y_{k})(1-_{k})\,, \ y_{k}(B_{k,0}^{{}^{}} B_{k,1}).\] (9)

where \(Y_{B_{k}}\) specifies the ground truth labels of batch \(B_{k}\), \(\) is the model-estimated probability for the label \(y=1\) and \(B_{k,0}^{{}^{}}\) is defined as

\[B_{k,0}^{{}^{}}=(B_{k,0},|B_{k,1}|)\,,& |B_{k,0}||B_{k,1}|\\ B_{k,0}\,,&\] (10)

where the downsampling procedure consists of randomly and uniformly selecting a subset of size \(|B_{k,1}|\) within \(B_{k,0}\) without replacement.

The ablation study for the loss function can be found in Supplementary Section D.

## 4 Experiments

We first provide more details about various LiDAR data present in each data set. Secondly, the data preprocessing procedure is elaborated upon. Next, we delve into the specific configurations of model hyperparameters during the training process, along with the adaptations made to the PointNet++ architecture. Following that, we showcase the performance of SOUL on the labeled ULS data set. Finally, we demonstrate the generalization ability of SOUL on the other data sets.

### Data Characteristics

Figure 3: (a) displays the components found by the GVD algorithm. (b) displays the geodesic distance within the corresponding component. The figures are illustrative, actual ULS data lacks such depicted complete tree trunks.

The ULS data for this study were collected at the Research Station of Paracou in French Guiana (N5\({}^{}\)18' W52\({}^{}\)53'). This tropical forest is situated in the lowlands of the Guiana Shield, the average height of the canopy is 27.2 m with top heights up to 44.8 m (Brede et al. ). Four flights of ULS data were collected at various times over the same site, with each flight covering 14,000 m\({}^{2}\) of land and consisting of approximately 10 million points, yielding a density of around 1,000 pts/m2. In comparison, the point density of the labeled TLS data from the same site is about 320,000 pts/m\({}^{2}\). Further information pertaining to LiDAR scanners can be found in Supplementary Section B.

In addition, we used two public data sets without labels only for exploring the model's generalization ability. One is a ULS data set from Sandhausen, Germany (Weiser et al. ), the other one is a mobile laser scanning (MLS) 3 data set from New South Wales (NSW), Australia (NSW & Gonzalez ). The Sandhausen data set uses the same equipment as we do, but exhibits a lower point density of about 160 pts/m\({}^{2}\). The point density of the NSW data set is approximately 53,000 pts/m\({}^{2}\).

### Data preprocessing

The first step of the routine involves data cleaning for TLS and ULS. The points exhibiting an intensity lower than -20 dB and those displaying high deviations (Pfennigbauer & Ullrich ) are removed, subsequently the TLS data are subsampled by setting a 0.001 m coordinate precision. The next stage is to separate ground by applying a cloth simulation filter (CSF) proposed by Zhang et al. , which mimics the physical phenomenon of a virtual cloth draped over an inverted point cloud. The ULS data were subjected to a similar preprocessing pipeline, except that CSF on ULS (or ALS) performs poorly and was not used. Here we used the software TerraSolid ground detection algorithm on ALS and used the common ground model for all point clouds of the same plot.

### Data annotation

A subset of TLS trees with large trunk is labeled using LeWos algorithm (Wang et al. ), followed by a manual refinement process (Martin-Ducup et al. ) to improve the outcome accuracy. Then the k-nearest neighbors algorithm (KNN) is employed to assign the tree identifiers (treeIDs) and the labels of TLS points to ULS points. Specifically, for each point in ULS data set, the KNN algorithm selects five nearest points in the TLS data set as reference points and subsequently determines the corresponding label based on the majority consensus of the reference points. It is noteworthy that within TLS data, there exist three distinct label types: unknown, leaf and wood, as demonstrated in Figure 1(c), which respectively correspond to labels: -1, 0, and 1. The unknown label is also transferred to ULS points, then such points are filtered out for the next step. Unlabeled and therefore excluded points represent 65% of the ULS point cloud.

To facilitate model training and quantitative analysis, the labeled ULS data was partitioned into training, validation, and test sets based on treeID, with 221 trees in the training data set, 20 in the validation data set, and 40 in the test data set. This partitioning is visually demonstrated in a video, where the trees are distinctly grouped together within each data set.

Figure 4: The labeled ULS data set in French Guiana, where only 4.4% of the points correspond to wood. This significant class imbalance of leaf & wood presents a considerable challenge in discriminating wood points from ULS forest point cloud.

### Implementation details

The Adam optimization algorithm is chosen as the optimizer and the learning rate is 1e-7. Following the practice proposed by Smith et al. , we employ a strategy of gradually increasing the batch size by a factor of two approximately every 1,000 epochs until reaching a final batch size of 128 instead of decreasing the learning rate during the training process. According to our experience, achieving good results requires a substantial duration, typically exceeding 3,000 epochs. In this paper, the prediction results are obtained from a checkpoint saved at the 3161st epoch. At this epoch, SOUL achieved a Matthews Correlation Coefficient (MCC) value of 0.605 and an AUROC value of 0.888 on the validation data set.

The MCC expression is:

\[=}\] (11)

For binary classification, the calculation of the MCC metric uses all four quantities (TP, TN, FP and FN)4 of the confusion matrix, and produces a high score only if the prediction is good for all quantities (see Yao & Shepperd ), proportionally both to the size of positive elements and the size of negative elements in the data set (Chicco et al. ).

In our task, an output in the form of a probability is preferred, as it is more consistent with the data collection process, since the LiDAR footprint yielding each single echo is likely to cover both wood and leaf elements. However, to facilitate performance comparison with other methods, the model generates a discrete outcome that assigns a label to each point. If the probability of a point being classified as a leaf exceeds the probability of being classified as wood, it is labeled as a leaf and vice versa. Additionally, the probabilistic outputs indicating the likelihood of being a leaf or wood point are retained. More details are provided in the Supplementary Section B.

### Results on French Guiana ULS data

Comparatively to the prevailing methods employed for forest point clouds, our SOUL approach improves semantic segmentation on ULS forest data by large margins. The results are summarized in Table 1, while Figure 5 highlights the performance within the tree canopies. In terms of quantitative analysis, SOUL demonstrates a specificity metric of 63% in discerning sparse wooden points, where the metric specificity stands for the ratio of true wood points predicted as wood. This value can be elevated to 79%, but at the cost of decreasing the accuracy metric to 80%, where the metric accuracy stands for the ratio of correctly predicted wood and leaf points. For better understanding how each class contributes to the overall performance, we introduced two more informative metrics mentioned in the study of Branco et al. , Geometric mean (G-mean) and Balanced Accuracy (BA). These metrics validate the performance of the SOUL model, providing more comprehensive insights into its capabilities.

### Results on the other data.

After testing on data sets from Sandhausen data set (Weiser et al. ) and NSW Forest data set , we observed that the SOUL exhibits good generalization capability across various forests. Qualitative results are shown in Figure 6. First, SOUL demonstrates good results on Sandhausen data set . However, SOUL does not outperform the others methods on TLS, but it shows better discrimination within the tree canopy. A possible improvement of SOUL performance is foreseen for denser data provided the model is retrained on similar data sets.

Figure 5: Qualitative results on ULS test data. Because of the class imbalance issue, methods such as FSCT, LeWos, and other existing approaches developed for dense forest point clouds, like TLS or MLS, (cf. Section 2) are ineffective.

## 5 Discussion

GVD as a spatial split scheme.Semantic segmentation in tropical forest environments presents distinctive challenges that differ from objects with regular shapes found in data sets such as ShapeNet (Chang et al. ) and ModelNet40 (Wu et al. ). Forest point clouds are inherently poorly structured and irregular because leaves and branches are small with regard to the LiDAR footprint size, a lot of points derived from LiDAR device are mixed between the two materials of leaves and wood. We propose GVD, an ad-hoc method for ULS data, to overcome the chaotic nature of forest point clouds with a pre-partition step. Similar to the enhancement that RCNN (Girshick et al. ) brings to CNN (LeCun et al. ) on image classification and semantic segmentation tasks, GVD serves as a region proposal method that provides improved training sample partitioning for deep learning networks. The neural network can benefit from more refined and informative data, leading to enhanced performance in the semantic segmentation task.

Rebalanced loss designed for class imbalance issue.In our model, PointNet++ is chosen as the backbone network due to its known robustness and effectiveness in extracting features from point clouds (Qian et al. ). But learning from imbalanced data in point cloud segmentation poses a persistent challenge, as discussed by Guo et al. . To address this issue, we employ a rebalanced loss function tailored to this task. Rather than adjusting class weights during training (Lin et al. ; Sudre et al. ), we adopt a random selection approach to balance the training data during the loss computation. This decision stems from the observation that misprediction of a subset of the minority class can lead to significant fluctuations in the loss value, consequently inducing drastic changes in gradients. In an extreme scenario, wherein a single point represents a positive sample, correct prediction of this point drives the loss towards zero, irrespective of other point predictions. Conversely, misprediction of the single point yields a loss value approaching one.

   Methods & Accuracy & Recall & Precision & Specificity & G-mean & BA1  \\  FSCT  & 0.974 & 0.977 & **0.997** & 0.13 & 0.356 & 0.554 \\ FSCT + retrain & **0.977** & **1.0** & 0.977 & 0.01 & 0.1 & 0.505 \\ LeWos  & 0.947 & 0.97 & 0.975 & 0.069 & 0.259 & 0.520 \\ LeWos (SoD2 )  & 0.953 & 0.977 & 0.975 & 0.069 & 0.260 & 0.523 \\ SOUL (focal loss ) & 0.942 & 0.958 & 0.982 & 0.395 & 0.615 & 0.677 \\ SOUL (rebalanced loss) & 0.826 & 0.884 & 0.99 & **0.631** & **0.744** & **0.757** \\   

Table 1: Comparison of different methods

Figure 6: Qualitative results on various LiDAR data from different sites.

SOUL model's generalization capability.Despite being originally developed for tropical forest environments, SOUL demonstrates promising performance when applied to less complex forests. We anticipate that with the increasing prevalence of ULS data, coupled with more high-quality labeled data sets, the performance of SOUL is poised to advance further. Additionally, there is potential for SOUL to be extended to other complicated LiDAR scenes. It is conceivable to envision the development of a universal framework based on SOUL that can effectively handle various types of forest LiDAR data, including ULS, TLS, MLS, and even ALS.

Ablation study of geometric features.An ablation study is conducted to evaluate the impact of individual geometric features proposed in our model. Comparative experiments, as detailed in Figure 7 and Table 2, illustrate the advantages of utilizing multiple precomputed geometric features at various scales. Specifically, we observe that features like linearity and verticality significantly enhance trunk recognition, while PCA1 and Sphericity are effective within the canopy. While PointNet++ possesses the capability to internally learn complex features, it may not precisely replicate our empirically selected geometric attributes. PointNet++ excels at adapting and acquiring sophisticated features from training data, whereas our predefined geometric features offer specific advantages in our model's context.

Limitations.The current observation is that SOUL may not perform as well as other methods on TLS data, as it was not trained for that. Additionally, SOUL may not perform effectively for trees that significantly deviate from the training data. By incorporating diverse samples during training, the model can mitigate the challenges associated with vegetation disparity.

## 6 Conclusion

We present SOUL, a novel approach for semantic segmentation in complex forest environments. It outperforms existing methods in the semantic segmentation of ULS tropical forest point clouds and demonstrates high performance metrics on labeled ULS data and generalization capability across various forest data sets. The proposed GVD method is introduced as a spatial split schema to provide refined training samples through pre-partition. One key aspect of SOUL is the use of the rebalanced loss function, which prevents drastic changes in gradients and improves segmentation accuracy. While SOUL shows good performance for different forest types, it may struggle with significantly different trees without retraining. Future work can focus on improving the performance of SOUL on denser forest point clouds to broaden its applications.