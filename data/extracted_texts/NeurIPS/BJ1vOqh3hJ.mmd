# Retaining Beneficial Information from Detrimental Data for Deep Neural Network Repair

Long-Kai Huang\({}^{1}\), Peilin Zhao\({}^{1}\), Junzhou Huang\({}^{2}\), Sinno Jialin Pan\({}^{3}\)

\({}^{1}\)Tencent AI Lab

\({}^{2}\)The University of Texas at Arlington

\({}^{3}\)The Chinese University of Hong Kong

hlongkai@gmail.com, masonzhao@tencent.com, jzhuang@uta.edu, sinnopan@cuhk.edu.hk

###### Abstract

The performance of deep learning models heavily relies on the quality of the training data. Inadequacies in the training data, such as corrupt input or noisy labels, can lead to the failure of model generalization. Recent studies propose repairing the model by identifying the training samples that contribute to the failure and removing their influence from the model. However, it is important to note that the identified data may contain both beneficial and detrimental information. Simply erasing the information of the identified data from the model can have a negative impact on its performance, especially when accurate data is mistakenly identified as detrimental and removed. To overcome this challenge, we propose a novel approach that leverages the knowledge obtained from a retained clean set. Concretely, Our method first identifies harmful data by utilizing the clean set, then separates the beneficial and detrimental information within the identified data. Finally, we utilize the extracted beneficial information to enhance the model's performance. Through empirical evaluations, we demonstrate that our method outperforms baseline approaches in both identifying harmful data and rectifying model failures. Particularly in scenarios where identification is challenging and a significant amount of benign data is involved, our method improves performance while the baselines deteriorate due to the erroneous removal of beneficial information.

## 1 Introduction

The effectiveness of deep neural networks (DNNs) is greatly influenced by the quantity and quality of the training data from which they are trained. However, obtaining a large amount of high-quality data can be costly. An alternative approach is to acquire training data from sources such as the Web , external data suppliers, or crowdsourcing . Although this allows for the collection of a substantial volume of data, the quality of the acquired data cannot be guaranteed. Various issues can arise, including annotation errors , data corruption , or even deliberate adversarial manipulation  by malicious providers. Consequently, a portion of the collected data may have a different distribution than the majority of the data . In this work, we refer to these two categories of data as "detrimental" and "target" data, respectively. The presence of detrimental data in the training set can significantly hinder the performance of a DNN . While recent research has proposed methods to mitigate specific issues like label noise , adversarial attacks , or data shifts during model training , it remains challenging to train a model that generalizes well to target data when the specific cause of the detrimental data is unknown.

It is important to note that in practical scenarios, a DNN model must undergo a quality assurance test before it can be deployed. During this evaluation process, the performance of the trained DNN model is usually assessed using a small reserved set of data known as clean data. This clean data is expectedto have the same distribution as the target data. However, since the training process involves the use of detrimental data, the trained DNN model may produce inaccurate predictions on the clean data, leading to a failure in the quality assurance test. In such cases, it becomes necessary to repair the DNN model to successfully pass the quality assurance test.

Recent approaches to model repair focus on two main steps. Firstly, they aim to identify the detrimental data instances within the original training dataset with a set of clean data, a.k.a, test data. Once identified, these instances can be categorized for further action. Secondly, the model is updated by eliminating the identified detrimental data instances. This repair process helps in improving the model's performance and ensuring that it meets the desired quality standards for deployment. Notable works in this area include studies by Tanno et al. and Kong et al..

In contrast to previous approaches that focus on eliminating the entire detrimental dataset to learn a repaired DNN model, our approach is motivated by the belief that the detrimental data may contain some valuable underlying information. Once this information can be extracted, it can be leveraged to train a more generalized model alongside the target data. Drawing inspiration from domain adaptation techniques , we propose a framework that aligns the identified detrimental data with the clean data. By aligning these datasets, we can utilize the aligned detrimental data to repair the DNN model and enhance its generalization ability, all without the need for complete retraining.

To elaborate, our proposed framework begins with an approach rooted in generalization theory to identify a potential set of detrimental data with the assistance of a clean data set, while simultaneously updating the model by mitigating the influence of the detrimental data during training. Subsequently, we introduce an energy-based algorithm  to align the identified detrimental data with the clean data. Through this alignment process, we selectively filter out harmful information that could adversely impact the model's generalization performance, while retaining the beneficial information that can bolster the model's overall generalization capabilities. By adopting this approach, we aim to strike a balance between effectively utilizing the beneficial aspects of the detrimental data and minimizing its negative impact on the model's performance. Our proposed framework offers a lightweight alternative to retraining the model from scratch, enabling the model to learn from both the target and aligned detrimental data, thereby improving its ability to generalize effectively.

It is crucial to acknowledge that identifying a perfect detrimental dataset from the training data is an arduous task, if not an impossible one. Previous approaches may discard useful information if certain target data instances are mistakenly identified as detrimental. In our proposed framework, we mitigate this issue by employing an energy-based alignment algorithm to reduce the divergence between the aligned detrimental data and target distributions. In essence, the alignment process serves as a corrective mechanism for potential errors introduced during the detrimental data identification step. By aligning the identified detrimental dataset with the clean data, our framework can rectify misidentifications and ensure that the model benefits from the valuable information contained within the misidentified instances. By leveraging the alignment process, our framework achieves a higher level of resilience, ensuring that the model's generalization performance remains intact, even in the presence of potential misidentifications. This robustness sets our proposed approach apart from previous methods and reinforces its effectiveness in dealing with the challenges associated with identifying and utilizing detrimental data.

**Summary of contribution**: We introduce a novel framework that acknowledges the potential value of detrimental data in enhancing the training of a more generalized model alongside target data. Our framework aligns the identified detrimental data with clean data, enabling us to leverage this aligned data to repair the DNN model and improve its generalization ability without the need for complete retraining. This approach strikes a balance between utilizing the beneficial aspects of detrimental data and minimizing its negative impact. By rectifying potential misidentifications through alignment, our framework maximizes the utilization of both target and aligned detrimental data, resulting in a more robust and resilient approach compared to previous methods for addressing the challenges associated with utilizing detrimental data. We conducted extensive experiments on real-world datasets to validate the effectiveness of our proposed framework, demonstrating its superiority over existing methods in identifying harmful data and repairing the model to enhance generalization.

Methodology

**Problem Settings**: Assume our goal is to maximize model performance on some target data distribution \(\) and we have trained a prediction model \(\) by minimizing the empirical risk \((;)\) over a training set \(=\{z_{i}\}_{i=1}^{N}\) consisting of \(N\) data pairs \(z_{i}=(x_{i},y_{i})\). While most of the training data are i.i.d. samples from the same distribution as the target data, a small set of training data is corrupted and assumed to be sampled from a detrimental data distribution \(_{c}\). We denote the detrimental data in the training set by \(_{c}\) and the remaining by \(_{l}\). Notably, the existence of detrimental data is unknown during data collection and model training.

Due to the existence of detrimental data, the model absorbs unexpected adverse information which adversely affects its generalization ability, resulting in inaccurate predictions. In this paper, we focus on developing a model repair algorithm that aims to improve performance by removing the detrimental information from the data that causes prediction failures. To facilitate model repair, we assume the availability of a small set \(_{r}\) of clean data that is reserved for validating the model's success or failure and remains concealed from the model during training. And we identify and remove the detrimental information in the data by leveraging the crucial information from these clean data.

Similar to , we formulate the process of model repairment as two steps:

* Step 1: Cause Identification. Given a reserved set \(_{r}\) from the target distribution, identify a set of detrimental data points in the training set \(\) that contribute to the prediction failures for data in \(_{r}\).
* Step 2: Model Treatment. Given the identified failure cause set, denoted by \(}_{c}\), refine the model to effectively correct the prediction failures in the target distribution while preserving the performance on remaining

### Step 1: Cause Identification

In order to detect the detrimental data points in the training set, it is necessary to first define a measure that quantifies the extent to which a subset of training data contributes to the failed predictions in the reserved set \(_{r}\). Then, based on this measure, we can screen out the data points with high failure contributions as the identified data.

Before introducing the measure, it is important to note that the data from detrimental dataset \(_{c}\) lead to generalization failure because they introduce a mismatch between train and test distribution. This is aligned with the generalization theory for mixture distributions [17; 38] which asserts that the degree of divergence between the two distributions impacts the generalization error bound. Additionally, the theory demonstrates that an increase in the amount of data from the target distribution during training reduces the generalization bound and decreases the loss of data from the same distribution. Besides, increasing the amount of target distribution data also enables the learned model to fit more closely to the target distribution and less to the detrimental distribution, thereby increasing the loss of detrimental data in \(_{c}\). Since the reserved set \(_{r}\) is drawn from the target distribution, we can train the target-distribution-enhanced model, denoted \(\) on the combined set of \(\) and \(_{r}\), and identify the data from \(_{c}\) based on the change of loss between the original trained model \(\) and the enhanced model \(\). The measure is defined as

\[s(z;,)=(z;)-(z;).\] (1)

By estimating \(s(z;,)\) for each data point \(z\), we can identify the data points responsible for the failure, which have a positive value of \(s(z;,)\). If we know the size of the cause set, we can identify the cause data by selecting the top \(|_{c}|\) data points with the largest \(s(z;,)\) values. We denote the identified data set by \(}_{c}\), and the remaining data set by \(}_{l}\).

#### 2.1.1 Obtaining the Enhanced Model \(\)

Considering the large scale of data, training a new model \(\) from scratch is computationally prohibitive. Instead, a practical solution is to approximate the model trained on the combined dataset of \(\) and \(_{r}\) by performing fine-tuning or continual learning on the well-trained \(\) from \(\) using the data in \(_{r}\). However, due to over-parameterization of the network and the small size of the reserved set \(_{r}\), a fine-tuned model may simply memorize the data in \(_{r}\), resulting in a negligible impact on the loss of the data in the original training set \(\). This makes the increase of loss \(s(z;,)\) ineffective in identifying the detrimental data. On the other hand, a fine-tuned model may completely overfit \(_{r}\) while forgetting the knowledge acquired from \(\), which can cause it to fail in approximating \(\).

Given that different layers in a neural network capture distinct patterns , selectively updating a subset of layers while freezing the remaining layers can update the detrimental knowledge from \(_{c}\) to effective knowledge in \(_{r}\), while preserving the acquired knowledge from \(\). After updating the conflict knowledge from \(_{c}\), the new model may result in a more significant increase in loss on \(_{c}\). Recent studies [27; 15] support the layer-selective update by observing that selectively fine-tuning a subset of layers improves the fine-tuning generalization for different distribution shifts towards the target distribution. And we empirically validate the effectiveness of layer-selective update in acquiring the knowledge from \(_{r}\) and improving the identification accuracy, as presented in Appendix A.4.

To enable layer-selective update, a criterion for layer selection is required. Note that neural networks have a tendency to minimize loss by learning data patterns in the early epochs, but may end up memorizing the data in the later epochs [4; 9]. Therefore, we assume that the reduction in loss after the first epoch can serve as an indicator of the ability of the selected layers to learn data patterns. Based on this assumption, we estimate the loss after performing a one-epoch update for each subset of layers and select the subset with the smallest loss for further updates. Subsequently, the selected subset is updated while the remaining layers are kept frozen. As the reserved set is small, the criterion for layer selection via loss estimation after a one-epoch update is computationally efficient. It should be noted that in practice, we consider only a pre-defined set of layer subsets, rather than all possible combinations of subsets.

**Connection to existing works:** EWC-influence, as proposed in , utilized the Bayesian view of continual learning to identify the cause data. Specifically, it defined the identification metric as the drop in log-likelihood between a model trained on the combined dataset \(\) and \(_{f}\), which is a subset of \(_{r}\) containing examples that are mispredicted by \(\), and the current model \(\) trained on \(\). By using negative log-likelihood as the loss function, replacing \(_{r}\) with \(_{f}\), and applying the continual learning algorithm Elastic Weight Consolidation (EWC)  to update all layers, our proposed identification measure recovers the EWC-influence introduced in .

Both EWC-influence and the proposed method measure the contribution of a training point to the prediction failure by examining the change in the loss. However, the two measures are derived from different purposes. EWC-influence aims to correct the prediction in \(_{f}\) of using the gain log-likelihood improvement estimated by deleting the data from \(\), whereas the proposed method aims to improve the generalization to the data from the same distribution of \(_{r}\) by eliminating detrimental information from \(\). Consequently, the proposed method proposes to measure the contribution to failure by assessing the conflict between \(\) and \(_{r}\) and update the model using all data in \(_{r}\).

Linear influence [20; 36; 23] is another effective metric to identify the training points that are most responsible for certain predictions. As analyzed in , the linear influence is the same as EWC-influence with the exception that it updates the model via a single update of natural gradient descent. Consequently, our proposed method can recover the linear influence in a similar manner.

Figure 1: The framework of the proposed method. Best view in color: beneficial and detrimental information are indicated in green and red.

### Step 2: Model Treatment

In step 1, we identify a partition of the training data \(}_{c}\) that may contain harmful information leading to a suboptimal performance to the test distribution \(\). In this section, we present the second step of our method, which involves refining the model using the identified cause set \(}_{c}\) and the trained model \(\) in conjunction with the enhanced model \(\). A straightforward solution for repairing the model is to eliminate the harmful information in \(}_{c}\) by removing the identified data from the training set by retraining the model using the remaining data  or performing machine unlearning  on the identified data . However, identifying a perfect detrimental dataset from the training data is difficult. Simply erasing the information of identified data can negatively impact the model's performance, especially when accurate data is mistakenly identified as detrimental and removed. Furthermore, even the detrimental data contain useful information. For example, an incorrectly annotated image contains a clean input. Recall that the generalization failure is caused by the mismatching between the detrimental distribution and target distribution. Inspired by the domain adaptation framework [35; 30], we propose to align the detrimental data with the target distribution, which can reduce the divergence between the training distribution and the target distribution. The aligning process can rectify the detrimental information in the data, making it possible to reuse the aligned data toward better performance. A theoretical analysis of the following Gaussian mean estimation task supports the potential of this approach to improve performance.

**Gaussian mean estimation task:** Consider the task where the objective is to estimate the mean of a Gaussian distribution \(=(,^{2})\) based on a training set \(\) consisting of \(n\) training data \(\{x_{i}\}_{i=1}^{n}\) sampled from the distribution \(\) as well as \(m\) training data \(\{x_{i}\}_{i=n+1}^{n+m}\) sampled from another Gaussian distribution \(^{}=(^{},^{2})\), where \(\) and \(^{}\) are the means of the two Gaussian distributions and \(^{2}\) denotes the variance. We define the loss function as \((x;)=(-x)^{2}\). In this example, we assume \(n\) is sufficiently large than \(m\) and the distribution divergence between \(\) and \(^{}\), meausured by \(|-^{}|\), is sufficiently large, i.e. \(|-^{}|>(-)\), such that the training data from \(^{}\) hurt the generalization performance of the model.

The optimal solution is \(^{*}=\). The empirical risk minimization solution is obtained by minimizing \((;)=_{i=1}^{n+m}(-x_{i})^{2}\) as \(=_{i=1}^{n+m}x_{i}\). We measure the generalization error as the difference between the empirical solution \(\) and the optimal solution \(^{*}\). Using Gaussian tail bounds , with probability at least \(1-p\) for some probability, the generalization error is upper bounded as

\[|-^{*}|+|-^{ }|:=B,\] (2)

where \(=\) is some constant.

Assume we have identified the failure cause set as \(\{x_{i}\}_{i=n+1}^{n+m}\). Directly removing the cause set and retraining the model achieves a generalization error bound as \(B_{}=<B\).

**Proposition 2.1**.: _In the Gaussian mean estimation task, there exists a set of alignment functions: \(h(x)=x-^{}+\) where \(|-|<<|-^{}|\) such that the optimal empirical solution on the combined set of \(\{x_{i}\}_{i=1}^{n}\) and \(\{h(x_{i})\}_{i=n+1}^{n+m}\), i.e., \(=(_{i=1}^{n}x_{i}+_{i=n+1}^{n+m}h(x_{ i}))\), achieves a generalization bound \(B_{}<B_{}\)._

The detailed proofs for the bound can be found in Appendix B. The alignment function \(h(x)=x-^{}+\) aligns the data in \(^{}\) with \(\), removing the detrimental information \(^{}\) and enhancing the beneficial information \(\). Upon alignment, the resulting data set, \(\{h(x_{i})\}_{i=n+1}^{n+m}\), approximates the target distribution closely, and utilizing it in training will enhance generalization performance.

We extend the idea of alignment-based model treatment from the Gaussian mean estimation task to general tasks. Concretely, we propose to perform alignment on the identified cause data to obtain the aligned data using an Energy-Based Model (EBM) , building upon the energy-based alignment proposed in . EBMs represent a probability density \(p(z)\) for \(z^{d}\) as \(p_{}(z)=(z))}{()}\), where \(E_{}(z)\) is the energy function which takes \(z\) as input and returns a scalar as energy, and \(Z()=_{z}(-E_{}(z))\) is the normalizing constant. To align a given data point \(z^{}\) with \(p_{}(z)\), we iteratively adapt \(z^{}\) to minimize the energy function  as

\[z^{}=z^{}-_{z^{}}E_{}(z^{}),\]where \(\) is a hyperparameter that controls the adaptation rate. In contrast to existing works  which trains an extra EBM to perform alignment, we adopt the approach proposed in , which utilizes the classification model \(\) as an energy-based model (EBM) to represent the data distribution density as \(p_{}(x,y)=(x)[y])}{Z()}\), where \(f_{}\) is a classification model which maps each data point \(x\) to the logits of \(K\) classes, \(f_{}(x)[y]\) denotes the logit corresponding to the \(y\)-th class and \(Z()\) is the unknown normalization constant.

We use target-distribution-enhanced model \(\) to approximately characterize the distribution \(\). Considering potential label noise, we update \(x\) using the energy function of \(p_{}(x)\) instead of \(p_{}(x,y)\), where we obtain \(p_{}(x)=_{y^{}}p_{}(x,y^{})=}(f_{}(x)[y^{}])}{Z()}\) by marginalizing out \(y\). The corresponding energy function is defined as

\[E_{}(x)=-_{y^{}}(f_{}(x)[y^{}]).\] (3)

With the energy function, we align the data \((x,y)}_{c}\) by first aligning the input \(x\) by one-step gradient descent as

\[=x-_{x}E_{}(x),\] (4)

then aligning the label \(y\) by

\[=p_{}(y|)=}())}{ _{y^{}}(f_{}()[y^{}])},\] (5)

which is the soft label of \(\).

After obtaining the aligned data, we update the model from \(\) using the aligned data. Similar to the update of enhanced model \(\) we update only the layers selected in the cause identification step while keeping the remaining layers frozen.

Finally, the proposed model repair method, named Beneficial Information Retaining (BIR) Model Repair, including the cause identification and model refinement, is summarized in Algo. 1 and illustrated in Fig. 1.

## 3 Related Works

Our work is closely related to , following which we formulate the model repairment problem into a cause identification step and a model treatment step. In , Tanno et al. discuss the model repairment problem from a Bayesian perspective and introduce EWC-influence for cause identification and EWC-deletion for model treatment which performs continual unlearning to remove the data. The cause identification relies on data influence estimation, which can be solved by various influence estimation methods  such as linear influence [20; 22], SGD influence  and Data Shapley Value [6; 16]. For model treatment, existing works [12; 23] proposes to remove the information of the failure cause data by retraining the model with the remaining data or by performing machine unlearning [8; 29]. RDIA  proposes an influence-based relabeling framework for reusing harmful training samples. However, RDIA's aggressive relabeling of benign data that is mistakenly identified as detrimental can introduce incorrect information and negatively impact the model's performance,making it a high-risk approach in the case of imperfect detrimental data identification. Our proposed approach BIR also reuses the identified data to improve model performance. By aligning the identified detrimental dataset with the clean data, our framework rectifies the detrimental information while preserving the beneficial information in the identified set, and updates the model using the aligned data. Unlike RDIA, our alignment strategy does not introduce incorrect information when applied to clean data and achieves satisfactory performance even in cases where there are mistakenly identified clean data in the cause set.

## 4 Experiments

To evaluate the effectiveness of the proposed BIR model repair algorithm, we conduct extensive experiments to answer the following research questions: **Q1**: How does BIR compare to the state-of-the-art baselines in repairing neural networks that produce error predictions? **Q2**: How does the proposed failure cause data identification metric compare to the baselines in identifying the failure cause data? **Q3**: How does the proposed align-then-update strategy compare to the baselines in model treatment when considering both perfect and imperfect failure cause sets?

**Datasets and Network Architectures:** We evaluate the effectiveness of model repair algorithms on corrupted versions of MNIST  and CIFAR-10  datasets following the setups in . We consider annotation error, input noise, and adversarial attacks in the training data. To simulate annotation error, we randomly flip about 40% of labels between specific similar digits or classes. For input noise, we randomly corrupt 40% of MNIST images and 30% of CIFAR-10 images in a selected set of target classes by adding specific types of noise, such as salt-and-pepper in MNIST or Gaussian noise in CIFAR-10. Additionally, we construct datasets by adversarially attacking 40% of randomly selected training images in the same target classes using FGSM. We denote the corrupted datasets with label noise, input noise, and adversarial attacks by MNIST-Label, MNIST-Input, and MNIST-Adv

    &  &  &  &  \\   & Metric & All & Clean & Noisy & All & Clean & Noisy & All & Clean & Noisy \\   & Base Model & 82.46 & 96.10 & 62.00 & 96.46 & 97.10 & 95.50 & 96.38 & 97.33 & 94.95 \\   & SGD Influence Repair & 84.40 & 95.67 & 67.30 & 96.12 & 96.30 & 94.95 & 96.06 & 96.39 & 94.70 \\   & Linear Influence Repair & 83.00 & **96.43** & 62.85 & **96.26** & **97.97** & 95.05 & **96.50** & **97.27** & 95.35 \\   & EVE Repair & 84.26 & 95.43 & 67.60 & 96.12 & 96.30 & 94.95 & 96.06 & 96.50 & 97.40 \\   & BIDA & 72.70 & 91.03 & 42.20 & 85.82 & 90.83 & 78.80 & 81.30 & 86.00 & 74.25 \\   & **BIR (Ours)** & **95.60** & 95.93 & **95.10** & 96.16 & 96.20 & **96.10** & 96.44 & 96.47 & **96.40** \\   & Base Model & 83.96 & 97.83 & 61.30 & 98.00 & 98.97 & 97.15 & 97.82 & 98.57 & 96.0 \\   & SGD Influence Repair & 88.86 & **96.80** & 73.75 & 97.76 & 98.40 & **96.80** & 97.00 & 97.93 & 95.60 \\    & Linear Influence Repair & 72.12 & 87.97 & 48.35 & 96.96 & 97.43 & 96.25 & 92.86 & **98.43** & 84.50 \\    & EVE Repair & 89.02 & 98.83 & 74.75 & 97.82 & 98.90 & **96.80** & 97.10 & 97.90 & 95.90 \\    & BIDA & 74.04 & 90.03 & 50.05 & 82.76 & 97.77 & 87.25 & 97.66 & 87.17 & 64.15 \\    & **BIR (Ours)** & **96.36** & 97.10 & **95.25** & **98.02** & **98.93** & 96.65 & **98.12** & **98.43** & **97.65** \\   

Table 1: Comparison of model repair performance on corrupted MNIST datasets. Performance better than base model is highlighted by cyan and the best performance among all baselines is in bold.

    &  &  &  \\   & Metric & All & Clean & Noisy & All & Clean & Noisy & All & Clean & Noisy \\   & Base Model & 74.80 & 83.58 & 87.20 & 91.77 & 80.35 & 87.88 & 93.04 & 97.60 \\   & SGD Influence Repair & 76.06 & **92.40** & 53.05 & **89.24** & 93.50 & **83.60** & 80.09 & 92.13 & 76.95 \\   & Linear Influence Repair & 74.80 & 92.30 & 48.55 & 87.88 & 92.83 & 80.45 & 87.72 & **93.00** & 79.80 \\   & EVE Repair & 77.06 & 90.77 & 56.50 & 87.44 & **93.37** & 87.85 & 86.70 & 92.00 & 78.75 \\   & RDIA & 67.50 & 85.17 & 41.00 & 96.58 & 52.10 & 71.35 & 77.54 & 82.23 & 70.50 \\   & **BIR (Ours)** & **83.95** & 91.10 & **73.30** & 87.96 & 91.40 & 82.80 & **87.94** & 92.17 & **81.60** \\   & Base Model & 71.00 & 90.60 & 50.95 & 86.62 & 94.30 & 75.10 & 87.60 & 93.13 & 79.30 \\   & SGD Influence Repair & 78.36 & **94.17** & 54.65 & 89.36 & **95.30** & 80.45 & 84.08 & **94.67** & 68.20 \\    & Linear Influence Repair & 75.82 & 98.33 & 88.80 & 88.28 & 93.50 & 79.85 & 87.60 & 93.67 & 78.70 \\    & EVE Repair & 78.00 & 93.57 & 54.65 & **89.94** & 94.47 & 83.15 & 84.08 & **94.67** & 68.20 \\    & RDIA & 66.42 & 90.13 & 30.85 & 76.24 & 87.33 & 59.60 & 71.44 & 84.20 & 52.30 \\    & **BIR (Ours)** & **83.02** & 91.73 & **69.95** & 88.64 & 91.80 & **83.90** & **90.26** & 93.20 & **85.88** \\   

Table 2: Comparison of model repair performance on corrupted CIFAR-10 datasets.

    &  &  &  \\   & Metric & All & Clean & Noisy & All & Clean & Noisy \\   & Base Model & 74.80 & 83.45 & 87.20 & 91.77 & 80.35 & 87.88 & 93.04 & 79.60 \\    & SGD Influence Repair & 76.06 & **92.40** & 53.05 & **89.24** & 93.50 & 80.06 & 90.21 & 76.95 \\    & Linear Influence Repair & 74.80 & 92.30 & 48.55 & 87.88 & 92.83 & 80.45(CIFAR10-Label, CIFAR10-Input, and CIFAR10-Adv). For MNIST, 3000 samples from the original training set are utilized while the complete original training dataset of CIFAR-10 is employed to train the base models, with a validation set consisting of 10% of the data. Furthermore, we partitioned the original test set, which is uncorrupted, into a reserved set consisting of 300 samples for MNIST and 1000 samples for CIFAR-10, while the remaining set was designated as the testing set. The selected target classes of MNIST are 1,7, 6, and 9; and the selected target classes of CIFAR-10 are 'plane', 'bird', 'cat', and 'dog'. LeNet and Conv4, which is a 4-layer CNN with a fully connected layer, are employed as classifiers for MNIST, while ResNet-20 and ResNet-50 are utilized for CIFAR-10. We also conduct experiments on CIFAR-10N dataset , which equips the training set of CIFAR-10 with real-world human-annotated noisy labels. There are 3 noise levels: _Worst_ (\(\)41%), _Random_ (\(\)18%), and _Aggregate_ (\(\)9%).

**Baselines:** We compare the proposed method with EWC Repair , RDIA , SGD-influence Repair  and Linear-influence-based Repair. EWC Repair utilizes EWC-influence to identify harmful data and EWC-deletion to remove them from the model. RDIA uses linear influence  to identify causes, relabel the identified data using an influence-based relabeling function, and retrains the model using the relabeled and remaining data. SGD-influence Repair identifies harmful data using SGD-influence and proposes to retrain the model using the remaining data. Linear-Influence Repair uses the linear influence function  to identify the cause data and performs Newton-update removal  to eliminate the identified data's information from the model. In practice, retraining the model from scratch can be computationally prohibitive. Therefore, for RDIA, we performed fine-tuning by using both the relabeled data and an equivalent amount of randomly selected data from the remaining dataset. For SGD-influence Repair, fine-tuning was conducted on a small partition of the remaining data that matched the size of the identified data.

**Performance Measure**: We measure the performance of model repair by the accuracy of the entire test set (denoted by "Test"). Besides, we measure the success rate of the repair by the test accuracy of the selected corrupted classes (denoted by "Noisy") and the performance maintaining rate by the test accuracy of the remaining classes (denoted by "Clean").

Additional information regarding the setups

### Comparison to Model Repairment Baselines

The comparison results for corrupted MNIST and CIFAR-10 are provided in Tabs. 1 and 2. We can observe from the results that the proposed BIR repair algorithm successfully repairs the model and improves the performance of noisy classes in all experiments except for MNIST-Input with Conv4. In contrast, most baselines fail to repair the model for MNIST-Input, MNIST-Adv and CIFAR10-Adv.

Figure 2: Comparison of Failure Cause Identification on LeNet trained with corrupted MNIST datasets and ResNet50 trained with CIFAR-10.

Furthermore, BIR achieves the best performance in noisy classes on 10/12 experiments, and the best performance in all classes on 8/12 experiments. These results demonstrate the effectiveness and superiority of BIR in removing detrimental information from the data to repair the model. We also observe that performance in clean classes of all baselines is generally slightly lower than the base model, which is a trade-off for repairing the corrupted data. The comparison results for CIFAR-10N in Tab. 3 show that our method achieves superior performance across all noise levels, demonstrating its effectiveness in mitigating real-world label noise.

### Comparison of Failure Cause Identification

We compare the proposed causes identification method using Eqn. (1) with EWC-influence , Linear-influence  and SGD-influence . To measure the performance of the failure cause identification, we report the number of identified data points corresponding to samples corrupted with noise. The results in Figure 2 show that all methods perform well when a small number of data points are identified. Our method performs the best overall, except on MNIST-Input. It is worth noting that our method identifies almost all the corrupted data points in MNIST-Label and CIFAR10-Label, where the model performance is significantly impacted by the detrimental information in the training data. However, it is also important to note that the identification is not perfect, and a sufficiently large set of clean data points may be identified as detrimental data. Therefore, the model repair baselines are at risk of removing useful information from the identified set and may fail to repair the model. In contrast, BIR can effectively remove the detrimental information while retaining beneficial information from the identification, achieving satisfactory performance.

### Comparison of Model Treatment

We evaluate model treatment algorithms given both precise and imprecise identified cause sets, with the latter containing 75% corrupted data and 25% clean data. The baselines include EWC-deletion , RDIA , and Newton-update removal  and fine-tuning of randomly selected data from remaining set as baselines. In addition, we employ retraining the model from scratch using the remaining data as a further baseline.

The results are reported in Tabs. 4 and 5. We can observe performance improvement compared to the results reported in Tabs. 1 and 2. And we also observe a decline in performance for most methods

   Dataset &  &  &  \\  Metric & Test All & Test Clean & Test Noisy & Test All & Test Clean & Test Noisy & Test All & Test Clean & Test Noisy \\  Base Model & 82.46 & 96.10 & 62.00 & 96.46 & 97.10 & 95.50 & 96.38 & 97.33 & 94.95 \\  Retrain & 96.66 & 97.43 & 95.50 & 96.84 & 97.57 & 95.75 & 96.96 & 97.53 & 96.10 \\  Identified set = ground-truth & cause set & & & & & & & \\  Fine-time & 95.56 & 96.90 & 93.55 & 96.42 & **97.17** & 95.30 & 96.26 & 97.17 & 94.90 \\  Newton Removal & 85.20 & 96.33 & 68.50 & 96.52 & **97.17** & 95.55 & 96.50 & **97.33** & 95.25 \\  EWC-Deletion & 95.10 & 96.00 & 93.75 & 96.14 & 96.50 & 95.60 & 96.24 & 95.57 & 95.75 \\  BOTA & **95.70** & **97.23** & 93.40 & 95.94 & 96.63 & 94.90 & 92.50 & 92.67 & 97.25 \\  BIR (Ours) & 95.64 & 95.67 & **95.60** & **96.78** & 97.11 & **96.30** & **96.66** & 97.13 & **95.95** \\  Identified set = 75\% & ground-truth & cause set & 129.52 & clean data & & & & \\  Fine-time & 88.18 & 96.23 & 76.10 & 95.98 & 96.37 & 95.40 & 96.22 & 96.53 & 95.45 \\  Newton Removal & 83.92 & **96.67** & 65.10 & 96.16 & **96.87** & 95.10 & 96.36 & **97.30** & 94.95 \\  EWC-Deletion & 87.47 & 94.70 & 77.30 & 95.74 & 96.40 & 94.75 & 93.46 & 96.03 & 94.60 \\  RDIA & 84.86 & 83.77 & 86.50 & 90.48 & 90.07 & 91.10 & 85.82 & 89.40 & 80.45 \\  BIR (Ours) & **95.70** & **96.47** & **94.55** & **96.48** & 96.67 & **96.19** & **96.62** & 96.80 & **96.35** \\   

Table 4: Comparison of Model Treatment for LeNet trained on corrupted MNIST.

   Dataset &  &  &  \\  Metric & Test All & Test Clean & Test Noisy & Test All & Test Clean & Test Noisy \\  Base Model & 77.10 & 99.60 & 50.95 & 86.62 & 94.30 & 75.10 & 87.60 & 93.13 & 79.30 \\  Retrain & 88.18 & 94.03 & 79.40 & 87.48 & 91.43 & 81.55 & 88.10 & 93.07 & 80.65 \\  Identified set = ground-truth & cause set & & & & & & \\  Fine-time & **85.16** & **94.83** & 70.65 & **89.86** & **94.40** & 83.05 & 89.66 & **94.67** & 82.15 \\  Newton Removal & 84.04 & 93.60 & 69.70 & 86.72 & 92.60 & 77.90 & 87.60 & 93.80 & 78.30 \\  EWC-Deletion & 84.50 & 94.83 & 69.15 & 89.76 & 94.93 & 83.20 & 88.42 & 94.20 & 79.70 \\  RDIA & 83.62 & 94.43 & 64.95 & 87.20 & 92.07 & 79.90 & 86.98 & 92.40 & 78.85 \\  BIR (Ours) & 83.26 & 91.43 & **71.00** & 88.84 & 91.97 & **84.15** & **90.60** & 93.43 & **86.10** \\  Identified set = 75\% & ground-truth & cause set & 125.56 & data & & & \\  Fine-time & **83.18** & **94.47** & 66.25 & **89.00** & **93.30** & 82.25 & 89.48 & **94.57** & 81.85 \\  Newton Removal & 76.08 & 94.17 & 83.95 & 86.72 & 91.50 & 79.45 & 87.88 & 93.00 & 80.20 \\  EWC-Deletion & 82.08 & 94.07 & 64.10 & 88.84 & 93.27 & 82.20 & 87.12 & 92.83 & 78.55 \\  RDIA & 74.58 & 86.17 & 57.20 & 79.88 & 85.30 & 71.65 & 78.70 & 83.57 & 71.40 \\  BIR (Ours) & 82.88 & 91.33 & **70.20** & 88.78 & 92.83 & **83.35** & **90.50** & 93.63 & **86.05** \\   

Table 5: Comparison of Model Treatment for ResNet50 trained on corrupted CIFAR-10.

when replacing the precise identified set with the imprecise one, particularly on MNIST-Label and CIFAR10-Label. These results indicate that the performance of the model treatment baselines is contingent on the correctness of identified set. Notably, the performance drop of BIR is significantly less than that of the baselines, illustrating the effectiveness and superiority of BIR in retaining beneficial information and eliminating harmful information from the identified data to repair the model, even when the identified set comprises clean data.

### Extra Experiment Results

We investigate the effect of the layer-selective update strategy on cause identification and model treatment. The results and analysis are presented in Appendix A.4. Moreover, we explore the solution for determining the appropriate size of the cause set when the ground-truth size is unknown and evaluate the effect of the size on performance in Appendix A.5. Additionally, in Appendix A.6, we studied the effect of varying the amount of clean reserved data on our method's performance.

## 5 Conclusion and Limitations

In this paper, we propose a novel framework for repairing neural networks trained with detrimental data. To identify the data that cause generalization failure, we propose to update the model with a reserved set sampled from the target distribution, which reduces the loss of clean data and increases the loss of corrupted data. After identifying the cause data, we acknowledge the potential value of detrimental data in enhancing the training of a more generalized model alongside target data and propose an energy-based alignment to remove harmful information while retaining beneficial information in the identified data. After obtaining the aligned data, we update the model on these data to repair the model. We conducted extensive experiments on real-world datasets to validate the effectiveness of our proposed framework, demonstrating its superiority over existing methods in identifying harmful data and repairing the model to enhance generalization.

Limitation: the proposed method entails access to the training data for model repairing purposes, a requirement that may be hampered by the possibility of discarding the data after training. Nonetheless, such accessibility is essential to the method's development, which is predicated on the assumption that erroneous training data engenders generalization failure, and thus seeks to remove the adverse information to effect model repair. We leave the investigation of training data-free model repair to the future.