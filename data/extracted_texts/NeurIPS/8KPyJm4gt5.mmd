# Is Behavior Cloning All You Need?

Understanding Horizon in Imitation Learning

 Dylan J. Foster

Microsoft Research

dylanfoster@microsoft.com &Adam Block

Department of Mathematics

MIT

ablock@mit.edu &Dipendra Misra

Microsoft Research

dipendrakumar.misra@databricks.com

###### Abstract

Imitation learning (IL) aims to mimic the behavior of an expert in a sequential decision making task by learning from demonstrations, and has been widely applied to robotics, autonomous driving, and autoregressive text generation. The simplest approach to IL, _behavior cloning_ (BC), is thought to incur sample complexity with unfavorable _quadratic_ dependence on the problem horizon, motivating a variety of different _online_ algorithms that attain improved _linear_ horizon dependence under stronger assumptions on the data and the learner's access to the expert.

We revisit the apparent gap between offline and online IL from a learning-theoretic perspective, with a focus on the realizable/well-specified setting with general policy classes up to and including deep neural networks. Through a new analysis of behavior cloning with the _logarithmic loss_, we show that it is possible to achieve _horizon-independent_ sample complexity in offline IL whenever (i) the range of the cumulative payoffs is controlled, and (ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Specializing our results to deterministic, stationary policies, we show that the gap between offline and online IL is smaller than previously thought: (i) it is possible to achieve _linear_ dependence on horizon in offline IL under dense rewards (matching what was previously only known to be achievable in online IL); and (ii) without further assumptions on the policy class, online IL cannot improve over offline IL with the logarithmic loss, even in benign MDPs. We complement our theoretical results with experiments on standard RL tasks and autoregressive language generation to validate the practical relevance of our findings.

## 1 Introduction

Imitation learning (IL) is the problem of emulating an expert policy for sequential decision making by learning from demonstrations. Compared to reinforcement learning (RL), the learner in IL does not observe reward-based feedback, and must imitate the expert's behavior based on demonstrations alone; their objective is to achieve performance close to that of the expert on an _unobserved_ reward function. Imitation learning is motivated by the observation that in many domains, demonstrating the desired behavior for a task (e.g., robotic grasping) is simple, while designing a reward function to elicit the desired behavior can be challenging. IL is also often preferable to RL because it removes the need for exploration, leading to empirically reduced sample complexity and often much more stable training. Indeed, the relative ease of applying IL (over RL methods) has led to extensive adoption, ranging from classical applications in autonomous driving  and helicopter flight  to contemporary works thatleverage deep learning to achieve state-of-the-art performance for self-driving vehicles [15; 6; 44], visuomotor control [30; 103], navigation , and game AI [46; 90]. Imitation learning also offers a conceptual framework through which to study autoregressive language modeling [21; 13], and a number of useful empirical insights have arisen as a result of this perspective. However, a central challenge limiting broader real-world deployment is to understand and improve the reliability and stability properties of algorithms that support general-purpose (deep/neural) function approximation.

In more detail, imitation learning algorithms can be loosely grouped into _offline_ and _online_ approaches. Offline imitation learning algorithms only require access to a dataset of logged trajectories from the expert, making them broadly applicable. The most widely used approach, _behavior cloning_, reduces imitation learning to a standard supervised learning problem in which the learner attempts to predict the expert's actions from observations given the collected trajectories. The simplicity of this approach allows the learner to leverage the considerable machinery developed for supervised learning and readily incorporate complex function approximation with deep models [10; 70]. On the other hand, BC seemingly ignores the problem of _distribution shift_, wherein small deviations from the expert policy early in rollout lead the learner off-distribution to regions where they are less able to accurately imitate. This apparent _error amplification_ phenomenon has been widely observed empirically [70; 54; 13], and motivates _online_ or _interactive_ approaches to imitation learning [70; 72; 71; 78], which avoid error amplification by interactively querying the expert and learning to correct mistakes on-policy.

In theory, online imitation learning enables sample complexity guarantees with improved (linear, as opposed to quadratic) dependence on horizon for favorable MDPs. Yet, while online imitation learning has found empirical success [73; 52; 38; 6; 26; 51; 7; 108; 59], online access to the expert can be costly or infeasible in many applications, and offline imitation learning remains a dominant empirical paradigm. Motivated by this disconnect between theory and practice, we aim to understand to what extent the apparent gap between offline and online imitation learning is fundamental. We ask: _Is online imitation learning truly more sample-efficient than offline imitation learning, or can existing algorithms or analyses be improved?_

### Background: Offline and Online Imitation Learning

To motivate our results, we begin by formally introducing the offline and online imitation learning frameworks, highlighting gaps in current sample complexity guarantees concerning _horizon dependence_. We take a _learning-theoretic_ perspective, with a focus on general policy classes.

Markov decision processes.We study imitation learning in episodic Markov decision processes. Formally, a Markov decision process \(M=(,,P,r,H)\) consists of a (potentially large) state

Figure 1: Suboptimality of a policy learned with log-loss behavior cloning (LogLossBC) as a function of the number of expert trajectories, for varying values of horizon \(H\). In each environment, an imitator is trained according to LogLossBC and the regret with respect to the expert is reported, with reward normalized to be horizon-independent. **(a)** Continuous control with MuJoCo environment Walker2d-v4. **(b)** Discrete control with Atari environment BeamRiderNoFrameskip-v4. For both environments, we find that the regret is _independent of horizon_ (or in the case of Atari, slightly improving with horizon), as predicted by our theoretical results. Full experimental details are provided in Appendix C.

space \(\), action space \(\), horizon \(H\), probability transition function \(P=\{P_{h}\}_{h=0}^{H}\), where \(P_{h}:()\), and reward function \(r=\{r_{h}\}_{h=1}^{H}\), where \(r_{h}:\). A (randomized) policy is a sequence of per-timestep functions \(=\{_{h}:()\}_{h=1}^{H}\). The policy induces a distribution over trajectories \((x_{1},a_{1},r_{1}),,(x_{H},a_{H},r_{H})\) via the following process. The initial state is drawn via \(x_{1} P_{0}()\),1 then for \(h=1,,H\): \(a_{h}(x_{h})\), \(r_{h}=r_{h}(x_{h},a_{h})\), and \(x_{h+1} P_{h}(x_{h},a_{h})\). For notational convenience, we use \(x_{H+1}\) to denote a deterministic terminal state with zero reward. We let \(^{*}[]\) and \(^{*}[]\) denote expectation and probability law for \((x_{1},a_{1}),,(x_{H},a_{H})\) under this process, respectively.2

The expected reward for policy \(\) is given by \(J()^{}_{h=1}^{H}r_{h}\), and the value functions for \(\) are given by \(V_{h}^{}(x)^{}_{h^{}=h}^{H}r_{h^{ }} x_{h}=x\) and \(Q_{h}^{}(x,a)^{}_{h^{}=h}^{H}r_{h^{ }} x_{h}=x,a_{h}=a\).

Reward normalization.To study the role of horizon in imitation learning in a way that disentangles the effects of reward scaling from other factors, we assume that rewards are normalized such that \(_{h=1}^{H}r_{h}[0,R]\) for a parameter \(R>0\). We refer to the setting in which \(r_{h}\) for all \(h[H]\), which is the focus of most prior work , as the _dense reward setting_, which has \(R H\); we will frequently specialize our results to this setting.

#### 1.1.1 Offline Imitation Learning: Behavior Cloning

Let \(^{}=\{^{}_{h}:()\}_{h=1 }^{H}\) denote the _expert policy_. In the offline imitation learning setting, we receive a dataset \(=\{o^{i}\}_{i=1}^{H}\) of (reward-free) trajectories \(o^{i}=(x^{i}_{1},a^{i}_{1}),,(x^{i}_{H},a^{i}_{H})\) obtained by executing \(^{}\) in the underlying MDP \(M^{}\). Using these trajectories, our goal is to learn a policy \(\) such that the rollout regret \(J(^{})-J()\) to \(^{}\) is as small as possible. _We emphasize that \(^{}\) is an arbitrary policy, and is not assumed to be optimal_.

Behavior cloning._Behavior cloning_, which reduces the imitation learning problem to supervised prediction, is the dominant offline imitation learning paradigm. To describe the algorithm in its simplest form, consider the case where \(^{}\{^{}_{h}:\}_{h=1 }^{H}\) is deterministic. For a user-specified policy class \(\{_{h}:()\}_{h=1}^{H}\), the most basic version of behavior cloning  solves the supervised classification problem \(=_{}_{i=1}^{n}_{h=1}^{H}\{_{h}(x^{i}_{h}) a^{i}_{h}\}=:_{}()\). Naturally, other classification losses (e.g., square loss, logistic loss, or log loss) may be used in place of the indicator loss.3 To provide sample complexity bounds for this algorithm, we make a standard _realizability assumption_ (e.g., Agarwal et al. , Foster and Rakhlin ).

**Assumption 1.1** (Realizability).: _The policy class \(\) contains the expert policy, i.e. \(^{}\)._

This assumption asserts that \(\) is expressive enough to represent the expert policy. Depending on the application, \(\) might be parameterized by simple linear models, or by flexible models such as convolutional neural networks or transformers. We primarily restrict our attention to the realizable setting throughout the paper, as it is meaningful and non-trivial, yet not fully understood. Our main results extend to provide guarantees for the misspecified case, but a thorough study of the role of misspecification is beyond the scope of this work.To simplify presentation, we adopt a standard convention in RL theory and focus on finite classes with \(||<\).

To proceed with analyzing the behavior cloning algorithm, a standard uniform convergence argument implies that if we define \(L_{}()=_{h=1}^{H}^{^{}}[_{h}(x_{h} )^{}_{h}(x_{h})]\), then with probability at least \(1-\), behavior cloning has

\[L_{}())}{n}.\]

Meanwhile, a standard error analysis for BC leads to the following bound on rollout performance:

\[J(^{})-J() RH L_{}( ).\] (1)Combining these bounds, we conclude that

\[J(^{})-J() RH)}{n}.\] (2)

For the _dense reward setting_ where \(R=H\), this leads to _quadratic_ dependence on horizon; that is, \((H^{2})\) trajectories are required to achieve constant accuracy. Unfortunately, both steps in this argument are tight in general:

* The generalization bound \(L_{}())}{n}\) is tight even when \(||=2\) (this is true not just for the indicator loss, but for other standardised losses such as square loss, absolute loss, and hinge loss). Since the amount of information in a trajectory grows with \(H\), one might hope a-priori that the generalization error would decrease with \(H\); alas, this does not occur due to the _dependence_ between samples in each trajectory.
* Ross and Bagnell  show that the inequality \(J(^{})-J() RH L_{}()\) is tight for MDPs with \(3\) states; the quadratic scaling in \(H\) this induces under dense rewards is often attributed to _error amplification_ or _distribution shift_ incurred by passing from error under the state distribution of \(^{}\) to the state distribution of \(\).

Combining, these observations, it is natural to conclude that offline imitation learning is fundamentally harder than supervised classification, where linear dependence on horizon might be expected (e.g., with \(H\) independent prediction tasks).

#### 1.1.2 Online Imitation Learning and Recoverability

The aforementioned limitations of behavior cloning have motivated _online_ approaches to IL . In the online framework, the learner can interactively choose policies to roll out and query the expert for the action at each state in the trajectory (see Appendix E.2 for a formal description), representing a substantially stronger (and in some cases unrealistic) assumption on the learner's access both to the MDP and the expert than in the offline setting. Online imitation learning can avoid error amplification and achieve improved dependence on horizon for MDPs that satisfy a _recoverability_ condition .

**Definition 1.1** (Recoverability parameter).: _The recoverability parameter for an MDP \(M^{}\) and expert \(^{}\) is given by4_ 
Under recoverability, the \(\) algorithm of Ross et al.  leverages online interaction by interactively querying the expert and learning to correct mistakes on-policy, leading to sample complexity

\[J(^{})-J() H\] (3)

for any finite class \(\) and deterministic expert policy \(^{}\) (for completeness, we include an analysis in Appendix E.2; see Propositions E.1 and E.2). For the dense reward setting where \(R=H\), we can have \(=H\) in the worst case, in which case Eq. (3) matches the quadratic horizon dependence of behavior cloning, but when \(=O(1)\) (informally, this means it is possible to "recover" from a bad action that deviates from \(^{}\)), the bound in Eq. (3) achieves linear dependence on horizon. Other online IL algorithms such as Forward, Smile, and Aggrevate achieve similar guarantees (we are not aware of an approach that improves upon Eq. (3) for general finite classes).

The improvements of online IL notwithstanding, Eq. (2) is known to be tight for BC, but this is an _algorithm-dependent_ (as opposed to information-theoretic) lower bound, and does not preclude the existence of more sample-efficient, purely offline algorithms. In this context, our central question can be restated as: _Can offline imitation learning algorithms achieve sub-quadratic horizon dependence for general policy classes \(\)?_ While prior work has investigated this question for tabular and linear policies , we approach the problem from a new (learning-theoretic) perspective by considering general policy classes.

### Contributions

We present several new results that clarify the role of horizon in offline and online imitation learning.

1. **Horizon-independent analysis of log-loss behavior cloning.** Through a new analysis of behavior cloning with the _logarithmic loss_ (LogLossBC), we show that **it is possible to achieve _horizon-independent_ sample complexity_ in offline imitation learning whenever (i) the range of the cumulative payoffs is normalized, and (ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Our result is facilitated by a novel information-theoretic analysis which controls policy behavior at the trajectory level, supporting both deterministic and stochastic expert policies.
2. **Deterministic policies: Closing the gap between offline and online IL.** Specializing LogLossBC to _deterministic stationary_ policies (more generally, policies with parameter sharing) and cumulative rewards in the range \([0,H]\), we show that it is possible to achieve sample complexity with _linear_ dependence on horizon in offline IL in arbitrary MDPs, matching was was previously only known of _online_ IL. We complement this result with a lower bound showing that, without further structural assumptions on the policy class (e.g., no parameter sharing ), **online IL cannot improve over offline IL with LogLossBC**, even for benign MDPs. Our results are summarized in Table 1. Nonetheless, as observed in prior work , online imitation learning can still be beneficial for _non-stationary_ policies.
3. **Stochastic policies: Tight understanding of optimal sample complexity.** For stochastic expert policies, our analysis of LogLossBC gives the first _variance-dependent_ sample complexity bounds for imitation learning with general policy classes, which we prove to be tight in a problem-dependent and minimax sense. Using this result, we show that for stochastic stationary experts, (i) _quadratic dependence on the horizon is necessary_ when cumulative rewards lie in the range \([0,H]\), in contrast to the deterministic setting, but (ii) LogLossBC--through our variance-dependent analysis--can sidestep this hardness and achieve linear dependence on horizon under a recoverability-like condition. Finally, we show that--as in the deterministic case--online IL cannot improve over offline IL with LogLossBC without further assumptions on the policy class. Our results are summarized in Table 2.

Toward a learning-theoretic understanding of imitation learning.Our findings highlight the need to develop a fine-grained, problem-dependent understanding of algorithms and complexity for IL. Instabilities of offline IL  and benefits of online IL  likely arise in practice, but existing assumptions in theoretical research are often too coarse to give insights into the true nature of these phenomena, leading to an important gap between theory and practice. As a first step in this direction, we highlight several under-explored mechanisms through which online IL can lead to improved sample complexity, including representational benefits and exploration (Appendix I). We also complement our theoretical results with empirical demonstrations of the phenomena we describe (Appendix C).

Experiments.In Appendix C (deferred to the appendix due to space constraints), we complement our theoretical results with an empirical demonstration of the horizon-independence of LogLossBC predicted by our theory (under parameter sharing and sparse rewards). We consider tasks where the horizon \(H\) can be naturally scaled up and down--for example, an agent walking for a set number of timesteps--and use an expert trained according to RL to generate expert trajectories, before training a policy using LogLossBC. We consider both continuous action space (MuJoCo environment Walker2d) and discrete action space (Atari environment Beamrider) tasks to demonstrate the broad applicability of our theoretical results. As can be seen in Figure 1, the performance of the learned policy is independent or improving with horizon, consistent with our theoretical results. We also perform simplified experiments on autoregressive language generation with transformers. Here, we

   & Parameter Sharing & No Parameter Sharing (\(=_{1}_{H}\)) \\  & (Corollary 2.1) & (e.g., ) \\  Sparse Rewards & \(O(|_{h}|)}{n})\) & \(O(|_{h}|)}{n})\) \\ Dense Rewards (\(R=H\)) & \(O()\) & \(O((_{h}|_{h}|)}{n})\) \\  

Table 1: Summary of upper bounds for deterministic experts; lower bounds are more nuanced, and discussed in Section 2.2. Each cell denotes the regret of a policy learned with log-loss behavior cloning (LogLossBC), which is optimal in each setting. Here, \(\) is the policy class, \(R\) is the reward range, \(H\) is the horizon, and \(n\) is the number of expert trajectories. In the dense-reward setting, we set \(R=H\).

find that the performance of the imitator is largely independent of \(H\), as predicted by our results, though the results are more nuanced.

Notation.We use \(_{x}()\) to denote the direct delta distribution, which places probability mass \(1\) on \(x\). We adopt standard big-oh notation, and write \(f=(g)\) to denote that \(f=O(g\{1,(g)\})\) and \(a b\) as shorthand for \(a=O(b)\).

## 2 Horizon-Independent Analysis of Log-Loss Behavior Cloning

This section presents the first of our main results, a horizon-independent sample complexity analysis of log-loss behavior cloning for the case of deterministic experts. Our second main result, which handles the case of stochastic experts, builds on our results here and is presented in Section 3.

### Log-Loss Behavior Cloning and Supervised Learning Guarantees

The workhorse for all of our results (both for deterministic and stochastic experts) is the following simple modification to behavior cloning. For a class of (potentially stochastic) policies \(\), we minimize the _logarithmic loss_:

\[=*{arg\,min}_{}_{i=1}^{n}_{h=1}^{ H}(a_{h}^{i} x_{h}^{i})}.\] (4)

This scheme is ubiquitous in practice [45; 31], and forms the basis for autoregressive language modeling ; we refer to it as LogLossBC. We will show that this seemingly small change--moving from indicator loss to log loss--has significant benefits. Following the classical tradition of imitation learning [70; 71; 72], our analysis proceeds via _reduction_ to supervised learning. We first show that LogLossBC satisfies an appropriate supervised learning guarantee, then translate this into rollout performance. Our starting point is to observe that LogLossBC, via Eq. (4), can be interpreted as performing maximum likelihood estimation over the set \(\{^{}\}_{}\) in order to estimate the law \(^{^{*}}\) over trajectories under \(^{*}\) (see Appendix E.1 for details). As a result, standard guarantees for maximum likelihood estimation [89; 102] imply convergence in distribution whenever \(^{}\). To be precise, define the squared _Hellinger distance_ for probability measures \(\) and \(\) by \(D_{}^{2}(,)=}- }^{2}\). Then for any finite policy class \(\), we have the following guarantee.5

**Proposition 2.1** (Supervised learning guarantee for LogLossBC (special case of Theorem E.1)).: _For any (potentially stochastic) expert \(^{}\), the LogLossBC algorithm ensures that with probability at least \(1-,D_{}^{2}^{},^{ ^{}} 2)}{n}\)._

That is, by performing LogLossBC, we are implicitly estimating the law \(^{^{*}}\); note that this result holds even if \(^{}\) is stochastic, as long as \(^{}\). We will focus on finite, realizable policy classes throughout this section to simplify presentation as much as possible, but guarantees for infinite classes under misspecification are given in Appendix E.1.

### Horizon-Independent Analysis of LogLossBC for Deterministic Experts

We first consider the case where the expert \(^{}\) is deterministic. Our main result is the following theorem, which translates the supervised learning error \(D_{}^{2}^{},^{^{}} \) into a bound on rollout performance in a horizon-independent fashion.

**Theorem 2.1** (Horizon-independent regret decomposition (deterministic case)).: _For any deterministic policy \(^{}\) and potentially stochastic policy \(\),_

\[J(^{})-J() 4R D_{}^{2} ^{},^{^{}}.\] (5)

This result shows that horizon-independent bounds on rollout performance are possible whenever (i) rewards are appropriately normalized, and (ii) the supervised learning error \(D_{}^{2}^{},^{^{}} \) is appropriately controlled. It is proven using novel trajectory-level control over deviations between \(\) and \(^{}\); we will elaborate upon this in the sequel. We emphasize that this result would be trivial if squared Hellinger distance were replaced by total variation distance in (5); that the bound scales with _squared_ Hellinger distance is crucial for obtaining fast \(1/n\)-type rates and linear horizon dependence. We further remark that this reduction is not specific to LogLossBC, and can be applied to any IL algorithm for which we can bound the Hellinger distance. Combining Theorem 2.1 with Proposition 2.1, we obtain the following guarantee for finite policy classes.

**Corollary 2.1** (Regret of LogLossBC).: _For any deterministic expert \(^{}\), the LogLossBC algorithm in Eq. (4) ensures that with probability at least \(1-\), \(J(^{})-J() 8R)}{n}\)._

To the best of our knowledge, this is the tightest available sample complexity guarantee for offline imitation learning with general policy classes. This bound improves upon the guarantee for indicator-loss behavior cloning in Eq. (2) by an \(O(H)\) factor, and improves upon the guarantee for Dagger in Eq. (3) (replacing \(H\) with \(R H\) under \(r_{h}\)) in the typical regime where \(=(1)\).

### Interpreting the Sample Complexity of LogLossBC

To understand the behavior of the bound for LogLossBC in Corollary 2.1 in more detail, we consider two special cases (summarized in Table 1).

Stationary policies and parameter sharing.If \(||=O(1)\), the bound in Corollary 2.1 is _independent of horizon_ in the case of sparse rewards (\(R=O(1)\)), and _linear in horizon_ in the case of dense rewards (\(R=O(H)\)). In other words, our work establishes for the first time that:

\(O(H)\) _sample complexity can be achieved in offline IL under dense rewards for general classes \(\),_

as long as \(||\) is appropriately controlled and realizability holds. This runs somewhat counter to intuition expressed in prior work , but we will show in the sequel that there is no contradiction.

Generally speaking, we expect to have \(||=O(1)\) if \(\) consists of stationary policies or more broadly, policies with parameter sharing across steps \(h[H]\) (as is the case in transformers used for autoregressive text generation). As an example, for a tabular (finite state/action) MDP, if \(\) consists of all stationary policies, we have \(||=||||\), so Corollary 2.1 gives \(J(^{})-J()|(|| ^{-1})}{n}\); that is, stationary policies can be learned with horizon-independent samples complexity under sparse rewards and linear dependence on horizon under dense rewards. Similar behavior holds for non-stationary policies with parameter sharing (e.g., log-linear policies of the form \(_{h}(a x)(_{h}(x,a),)\)); see Appendix E.1 for details.

Non-stationary policies or no parameter sharing.For non-stationary policies or policies with no parameter sharing across steps \(h\) (e.g., product classes where \(=_{1}_{2}_{H}\)), we expect \(||=O(H)\) (more generally, \(D_{}^{2}^{},^{^{}} =(H/n)\)). For example, in a tabular MDP, if \(\) consists of all non-stationary policies, we have \(||=H||||\). In this case, Corollary 2.1 gives linear dependence on horizon for sparse rewards (\(J(^{})-J()|(|| ^{-1})}{n}\)) and quadratic dependence on horizon for dense rewards (\(J(^{})-J()||(||^{-1})}{n}\)). The latter bound is known to be optimal  for offline IL.

### Optimality and Consequences for Online versus Offline Imitation Learning

We now investigate the optimality of Theorem 2.1 and discuss implications for online versus offline imitation learning, as well as connections to prior work. Our main result here shows that in the dense-reward regime where \(r_{h}\) and \(R=H\), Theorem 2.1 cannot be improved when \(||=O(1)\)--even with online access, recoverability, and known dynamics.

**Theorem 2.2** (Lower bound for deterministic experts).: _For any \(n\) and \(H\), there exists a (reward-free) MDP \(M^{}\) with \(||=||=2\), a class of reward functions \(\) with \(||=2\), and a class of deterministic policies \(\) with \(||=2\) with the following property. For any (online or offline) imitation learning algorithm, there exists a deterministic reward function \(r=\{r_{h}\}_{h=1}^{H}\) with \(r_{h}\) (in particular, \(R H\)) and (optimal) expert policy \(^{}\) with \(=1\) such that \([J(^{})-J()] c\) for an absolute constant \(c>0\). In addition, the dynamics, rewards, and expert policies are stationary._

Together, Theorems 2.1 and 2.2 show that without further assumptions on \(\), _online imitation learning cannot improve upon offline imitation learning_ in the realizable setting. That is, even if recoverability is satisfied, there is no online imitation learning algorithm that improves upon Theorem 2.1 uniformly for all policy classes. See Appendix H.1 for further lower bounds.

Benefits of online IL for policies with no parameter sharing.How can we reconcile our results with prior work showing that that online IL improves the horizon dependence of offline IL ? The important distinction here is that online IL can still improve on a _policy-class dependent_ basis. In particular, methods like Dagger can still lead to improved sample complexity for policy classes with _no parameter sharing_ across steps \(h[H]\). Let \(_{h}:=\{_{h}\}\) denote the projection of \(\) onto step \(h\). In Appendix E.2, we prove the following refined guarantee for a variant of Dagger based on the log-loss (LogLossDagger).

**Proposition 2.2** (Special case of Proposition E.2).: _When \(^{}\) is deterministic, LogLossDagger ensures that with probability at least \(1-\), \(J(^{})-J()_{h=1}^{H}|H^{-1})}{n}\)._

For classes with no parameter sharing (i.e., product classes where \(=_{1}_{2}_{H}\)), we have \(_{h=1}^{H}[_{h}]=[]\). In this case, Proposition 2.2 scales as \(J(^{})-J()^{-1} )}{n}\), improving on the bound for LogLossBC in Theorem 2.1 by replacing \(R\) with \( R\). Thus, online IL can indeed improve over offline IL for classes with no parameter sharing. This is consistent with Rajaraman et al. , who proved a \( H\) vs. \(H^{2}\) gap between online and offline IL for the special case of non-stationary tabular policies (where \(\) is a product class with \([] H\)) under dense rewards. However, for classes with parameter sharing (i.e., where \([_{h}][]\)), the bound in Proposition 2.2 scales as \(\), which does not improve over Theorem 2.1 unless \( 1\). Since virtually all empirical work on imitation learning uses parameter sharing across steps \(h[H]\), we believe the finding that online IL does not improve over offline IL in this regime is quite salient. Nevertheless, it is important to emphasize that there are various practical considerations (e.g., misspecification or geometric structure) which this result may not account for.6

### Proving Theorem 2.1: How Does LogLossBC Avoid Error Amplification?

The central object in the proof of Theorem 2.1 is the following _trajectory-level_ distance function between policies. For a pair of potentially stochastic policies \(\) and \(^{}\), define

\[(^{}):=^{}\,_{a^{}_{1:H }^{}(x_{1:H})}[\{ h:\,a_{h} a^{}_{h} \}],\] (6)

where we use the shorthand \(a^{}_{1:H}^{}(x_{1:H})\) to indicate that \(a^{}_{1}^{}(x_{1}),,a^{}_{H}^{}(x_{H})\). We begin by showing (Lemma F.2) that for all (potentially stochastic) policies \(^{}\) and \(\), \(J(^{})-J() R(^{})\). We then show (Lemma F.3) that whenever \(^{}\) is deterministic, Hellinger distance satisfies7\(D_{}^{2}(^{},^{^{}} )(^{})\). Finally, we show (Lemma F.1) that the trajectory-level distance is symmetric, i.e. \((^{})=(^{})\). This step is perhaps the most critical: by considering trajectory-level errors, we can switch from the state distribution induced by \(\) to that of \(^{}\) for free, without incurring error amplification or spurious horizon factors. Combining the preceding inequalities yields Theorem 2.1; see Appendix F for the full proof.

This analysis is closely related to a result in Rajaraman et al. . For the special case of deterministic, linearly parameterized policies with parameter sharing, Rajaraman et al.  consider an algorithm that minimizes an empirical analogue of the trajectory-wise distance in Eq. (6), and show that it leads to a bound similar to Corollary 2.1 (i.e., linear-in-\(H\) sample complexity under dense rewards). Relative to this work, our contributions are threefold: (i) we show that horizon-independent sample complexity can be achieved for _arbitrary_ policy classes with parameter sharing, not just linear classes; (ii) we show that said guarantees can be achieved by a natural algorithm, LogLossBC, which is already widely used in practice; and (iii), by virtue of considering the log loss, our results readily generalize to encompass stochastic expert policies, as we will show in the sequel.

## 3 Horizon-Independent Analysis of LogLossBC for Stochastic Experts

In this section, we turn out attention to the general setting in which the expert policy \(^{}\) is stochastic. Stochastic policies are widely used in practice, where they are useful for modeling multimodal behavior , but have received relatively little exploration in theory beyond the work of Rajaramanet al.  for tabular policies.8 Our main result for this section is a regret decomposition based on the supervised learning error \(D_{}^{2}^{},^{^{*}}\) that is horizon-independent and _variance-dependent_. To state the result, define \(_{^{*}}^{2}:=_{h=1}^{H}^{^{*}}(Q_{h}^{^{*}} (x_{h},_{h}^{*}(x_{h}))-Q_{h}^{^{*}}(x_{h},a_{h}))^{2}\) as the _variance_ for the expert policy.

**Theorem 3.1** (Horizon-independent regret decomposition).: _Assume \(R 1\). For any pair of (potentially stochastic) policies \(^{*}\) and \(\) and any \((0,e^{-1})\),_

\[J(^{*})-J()}^{2} D_{ }^{2}^{},^{^{*}}}+OR (R^{-1}) D_{}^{2}^{ },^{^{*}}+.\] (7)

Applying this result with LogLossBC leads to the following guarantee.

**Corollary 3.1** (Regret of LogLossBC).: _For any \(^{*}\), the LogLossBC algorithm in Eq. (4) ensures that with prob. at least \(1-\), \(J(^{*})-J() O(1)}^{2} (||^{-1})}{n}}+O(R(n)))}{n}\)._

As we show, when the expert policy is stochastic, we can no longer hope for a "fast" \(1/n\)-type rate, and must instead settle for a "slow" \(1/\)-type rate. The slow term in Corollary 3.1 is controlled by the variance \(_{^{*}}^{2}\) for the optimal policy. In particular, if \(^{*}\) is deterministic, then \(_{^{*}}^{2}=0\), and Corollary 3.1 recovers our bound for the deterministic setting in Corollary 2.1 up to a \((n)\) factor.

### Horizon-Independence and Optimality for Stochastic Experts

To understand the dependence on horizon in Corollary 3.1, we restrict our attention to the "parameter sharing" case where \(||=O(1)\), and separately discuss the sparse and dense reward settings (results summarized in Table 2).

Consider the sparse reward setting where \(R=O(1)\). Here, at first glance it would appear that the variance \(_{^{*}}^{2}\) should scale with the horizon. Fortunately, this is not the case: The following result--via a law-of-total-variance-type argument --implies that Corollary 3.1 is _fully horizon-independent_, with no explicit dependence on horizon when \(R=O(1)\) and \(||=O(1)\). For a function \(f(x_{1:H},a_{1:H})\), let \(^{}[f]\) denote the variance of \(f\) under \((x_{1},a_{1}),,(x_{H},a_{H})\).

**Proposition 3.1**.: _We have that \(_{^{*}}^{2}^{^{*}}[_{h=1}^{H}r_{h}]  R^{2}\)._

For the dense-reward regime where \(R=H\), Proposition 3.1 gives \(J(^{*})-J() H\). This is somewhat disappointing, as we now require \((H^{2})\) trajectories (quadratic sample complexity) to learn a non-trivial policy, even when \(||=O(1)\). We show now that this quadratic lower bound is qualitatively tight: the slow \(H/\) rate for \(_{^{*}}^{2}=H^{2}\) is necessary in both offline and online IL. This reveals a fundamental difference between deterministic and stochastic experts, since \(O(H)\) sample complexity is sufficient in the former case.

**Theorem 3.2** (informal).: _For any \(^{2}H,H^{2}\), there exists \(\) with \(||=2\) such that \(_{^{*}}^{2}^{2}\) and any (offline or online) IL algorithm must have \(J(^{*})-J()}{n}}\) with constant probability._

Nonetheless, it is possible to obtain linear-in-\(H\) sample complexity for dense rewards under a recoverability-like condition. Let us define the _signed recoverability constant_ via \(=_{x,,h[H]}(Q_{h }^{^{*}}(x,_{h}^{*}(x))-Q_{h}^{^{*}}(x,a)\). Note that \([0,R]\), and that \(\), since this version counts actions \(a\) that _outperform_\(^{*}\), not just those that underperform. It is immediate to see that \(_{^{*}}^{2}^{2}H\). Hence, even if \(R=H\), as long as \(=O(1)\), Corollary 3.1 yields \(J(^{*})-J()\), so that \(O}\) trajectories suffice to learn an \(\)-optimal policy.9

See Appendix H for further results concerning tightness of Theorem 3.1, including instance-dependent lower bounds.

Consequences for online versus offline IL.The lower bound in Theorem G.1 holds even for online imitation learning algorithms. Thus, similar to the deterministic setting, there is no online IL algorithm that improves upon Theorem 3.1 uniformly for all policy classes. This means that even for stochastic experts, online imitation learning cannot improve upon offline imitation learning in the realizable setting without further assumptions (e.g., no parameter sharing) on the policy class under consideration.

Proof sketch for Theorem 3.1.When the expert is stochastic, the trajectory-wise distance in Eq. (6), is no longer useful (i.e., \((^{*}^{*}) 0\)), which necessitates a more information-theoretic analysis. Our starting point is the following scale-sensitive change-of-measure lemma for Hellinger distance.

**Lemma 3.1** (Change-of-measure for Hellinger distance [34; 35]).: _Let \(\) and \(\) be probability distributions over a measurable space \((,)\). Then for all functions \(h:\),_

\[|_{}[h(X)]-_{}[h(X)]|(_{}[h^{2}(X)]+_{}[h^{2}(X)])  D_{}^{2}(,)}.\] (8)

_In particular, if \(h[0,R]\) almost surely, then \(_{}[h(X)] 2\,_{}[h(X)]+R D_{ }^{2}(,)\)._

We sketch how to use Lemma 3.1 to prove a weaker version of Theorem 3.1, and defer the full proof, which builds on this argument, to Appendix G.1. Define the _sum of advantages_ for a trajectory \(o=(x_{1},a_{1}),,(x_{H},a_{H})\) via \((o)=_{h=1}^{H}Q_{}^{*}(x_{h},_{h}^{*}(x_{h}))-Q_{ }^{*}(x_{h},a_{h})\). By the performance difference lemma, we can write \(J(^{*})-J()=^{}[(o)]\), so applying Eq. (8) yields

\[J(^{*})-J()=^{}[(o)]\, ^{^{*}}[(o)]+^{}[^{2}(o )]+^{^{*}}[^{2}(o)]) D_{}^{2}(^{ },^{^{*}})}.\]

From here, we observe that \(^{^{*}}[(o)]=0\) and \(^{^{*}}[^{2}(o)]=_{}^{2}\). (this follows because advantages are a martingale difference sequence under \(^{^{*}}\)), so all that remains is to bound the term \(^{}[^{2}(o)]\). A crude approach is to observe that \(|(o)|H\), so that applying Lemma 3.1 gives \(^{^{*}}[^{2}(o)]+(H)^{2}  D_{}^{2}(^{},^{^{*} })\), and consequently \(J(^{*})-J()^{2} D_{ }^{2}(^{},^{^{*}})}+H D_{}^{2}(^{},^{ ^{*}})\). This falls short of Eq. (9) due to the suboptimal lower-order term, which does not recover Theorem 2.1 when \(^{*}\) is deterministic (\(_{^{*}}^{2}=0\)). The full proof in Appendix G.1 corrects this disparity using a subtle and significantly more involved argument based on stopping times and martingale concentration.

## 4 Discussion and Additional Results

Our results clarify the role of horizon in offline and online IL, and show that--at least under standard theoretical assumptions--the gap between online and offline IL is smaller than previously thought.

Benefits of online interactionInstabilities of offline IL [60; 27; 13] and benefits of online IL [73; 52; 38; 6; 26; 51; 7; 108; 59] likely arise in practice, but existing assumptions in theoretical research on imitation learning appear be too coarse to give insights into the true nature of these phenomena. Toward developing a fine-grained, problem-dependent understanding of algorithms and complexity for IL, in Appendix I, we highlight several special cases in which online interaction _does_ lead to benefits over offline imitation learning, but in a policy class-dependent fashion not captured by existing theory. We identify three phenomena that can lead to improved sample complexity: (i) _representational benefits_; (ii) _value-based feedback_; and (iii) _exploration_.

Further directions.Additional directions for future research include (i) developing and analyzing imitation learning algorithms under control-theoretic assumptions that more directly capture practical notions of instability [61; 88; 13; 14], and (ii) developing a more refined theory in the context of language models, via the connection in Appendix B.3. For both settings, an important question is to understand whether the notion of supervised learning error \(D_{}^{2}(^{},^{^{*}})\) we consider is a suitable proxy for real-world performance, or whether more refined notions are required.

Additional results.Secondary results deferred to the appendix for space include (i) examples and additional guarantees for LogLossBC and LogLossDagger (Appendix E); and (ii) additional lower bounds and results concerning the tightness of Theorems 2.2 and 3.1 (Appendix H).

#### Acknowledgements

We thank Jordan Ash, Audrey Huang, Akshay Krishnamurthy, Max Simchowitz, and Cyril Zhang for many helpful discussions. We thank Drew Bagnell for valuable comments and pointers to related work.