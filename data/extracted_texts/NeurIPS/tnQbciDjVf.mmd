# TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration

Yiwei Guo\({}^{1,2}\) Shaobin Zhuang\({}^{*3,4}\) Kunchang Li\({}^{*1,2,3}\) Yu Qiao\({}^{3}\) Yali Wang\({}^{ 1,3}\)

\({}^{1}\)Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences

\({}^{2}\)University of Chinese Academy of Sciences

\({}^{3}\)Shanghai AI Laboratory

\({}^{4}\)Shanghai Jiao Tong University

###### Abstract

Vision-language foundation models (such as CLIP) have recently shown their power in transfer learning, owing to large-scale image-text pre-training. However, target domain data in the downstream tasks can be highly different from the pre-training phase, which makes it hard for such a single model to generalize well. Alternatively, there exists a wide range of expert models that contain diversified vision and/or language knowledge pre-trained on different modalities, tasks, networks, and datasets. Unfortunately, these models are "isolated agents" with heterogeneous structures, and how to integrate their knowledge for generalizing CLIP-like models has not been fully explored. To bridge this gap, we propose a general and concise TransAgent framework, which transports the knowledge of the isolated agents in a unified manner, and effectively guides CLIP to generalize with multi-source knowledge distillation. With such a distinct framework, we flexibly collaborate with 11 heterogeneous agents to empower vision-language foundation models, without further cost in the inference phase. Finally, our TransAgent achieves state-of-the-art performance on 11 visual recognition datasets. Under the same low-shot setting, it outperforms the popular CoOp with around 10% on average, and 20% on EuroSAT which contains large domain shifts. The code will be released at https://github.com/markywg/transagent.

## 1 Introduction

Recently, Vision-Language (V-L) foundation models are mainly pre-trained by contrastive learning with massive image-text pairs from web . As a result, they show the potential on a number of downstream visual recognition tasks, by transferring their representations with prompt learning  and/or model adaptation . However, target domain data in the open world are diversified, _e.g._, EuroSAT  refers to satellite images that are highly different from web images in the pre-training. With this large domain shift, it is challenging to achieve good generalization only by adopting such a single model (_e.g._, CLIP), especially under low-shot regime. Alternatively, with the fast development in vision and NLP, there arises a wide range of expert models  which contain rich knowledge by pre-training on different modalities, tasks, networks, and datasets. Hence, the natural question is, is it possible to integrate such knowledge to boost vision-language foundation models?

To answer this question, we should further analyze the form of knowledge in these models. The simplest form is the model output, which explicitly exhibits what kind of tasks these models can tackle. However, these models have heterogeneous network structures and outputs, making direct knowledge combinations infeasible. Hence, existing works often choose cascades of these models, according to their output forms. Apparently, such a design lacks flexibility in transfer learning, based on tool invocation in sequence. Moreover, the resulting pipeline is unfriendly for deployment, due to the ensemble of various models in the inference phase.

Another form is the latent representation which implicitly encodes data knowledge in these models . Compared to the explicit output, this implicit representation has a key advantage, _i.e._, it is the feature vector that has a homogeneous form among different models. In other words, such vectorized knowledge opens the possibility for a unified integration of these heterogeneous agents. Based on this observation, we propose a general **TransAgent** framework in Figure 1. To our best knowledge, it is the _first_ unified distillation framework for generalizing vision-language foundation models with _efficient_ heterogeneous agent collaboration. Compared to the previous works mentioned above, our TransAgent contains three distinct technical contributions.

(1) **Knowledge Versatility**. In our TransAgent, we leverage 11 heterogeneous agents from vision, language and multi-modal research, which comprehensively covers diversified knowledge that is complementary with CLIP-like models, from visual recognition to dense prediction, from chatbot to text encoder, from multi-modal generation to caption.

(2) **Transfer Flexibility**. First, we provide a generic knowledge extraction method for each modality, allowing us to flexibly extend more agents if necessary in the future. Especially for multi-modal agents, we design a novel manner to extract the prediction score vector of classes as multi-modal knowledge in the target domain, via elaborate mining of vision-language alignment in these models. Second, we introduce a mixture-of-agents gating mechanism for integrating external knowledge of different agents in each modality. This allows our TransAgent to automatically select agents via soft weighting so that it can adaptively tackle few-shot settings in different domains of target datasets.

(3) **Deployment Efficiency**. We leverage multi-source distillation to transfer knowledge of these heterogeneous agents into CLIP. Since all these pre-trained models are frozen, the fine-tuning effort is neglectable with a few learnable prompts. More importantly, we can unload all the external agents after distillation, _i.e._, the inference pipeline with the enhanced CLIP is just the same as the original one, achieving deployment efficiency without a heavy model ensemble.

Finally, we conduct extensive experiments on 11 visual recognition benchmarks, where our TransAgent achieves the state-of-the-art under the same low-shot transfer setting, _e.g._, via knowledge collaboration, it outperforms the well-known CoOp  with around 10% on average and 20% on EuroSAT which contains large domain shifts. Our method also achieves better results than CaFo , which adopts a model ensemble strategy.

Figure 1: **An overview of our TransAgent. (a) TransAgent transfers multi-source knowledge from heterogeneous agents to enhance the generalization ability of vision-language foundation models. It demonstrates knowledge versatility, transfer flexibility and deployment efficiency through elaborate agent collaboration and knowledge ensemble strategy. (b) SOTA comparison for base-to-novel generalization on 11 visual recognition benchmarks. Our method outperforms previous SOTA, especially on the more diversified target domains.**

Related Work

**Foundation models.** The rapid advancements in deep learning methods have brought abundant pre-trained models to the research area. We group these models into four categories and further demonstrate their ideas below. **(i) Vision models:** Vision foundation models [33; 9; 3; 59; 32; 13] pre-trained on ImageNet  have shown outstanding transfer capability in visual recognition by fine-tuning on downstream datasets. Moreover, various models [43; 49; 16; 17; 8] can be specialists in dense prediction tasks by pre-training on task-relevant domain data. **(ii) Large language models:** The emergence of large language models (LLMs) [7; 18; 69; 70; 27] has been raising increasing attention from the research community and the public. The astonishing comprehension ability of the LLMs is credited to the linguistic knowledge which can be further applied to solve vision tasks [51; 20; 88; 79]. **(iii) Text-to-image generative models:** Text-conditioned generation task requires high-level understanding of the given prompts. Recently, diffusion-based generative models [37; 23; 64; 11; 56; 62; 66] have become the state-of-the-art. These models can follow the text conditions faithfully and generate desired outcomes, owing to the semantic knowledge learned during the pre-training stage. **(iv) Image-to-text captioning models:** These models typically integrate visual knowledge into LLMs to obtain multi-modal understanding abilities [46; 12; 15; 2; 14; 25; 47], offering better experience in referential dialog scenario. In this work, we excavate the underlying knowledge in these heterogeneous models to empower the VL foundation models.

**Few-shot adaptation.** To efficiently transfer vision-language foundation models like CLIP  to downstream tasks, researchers have proposed various adaptation methods, which are primarily based on prompt learning [87; 86; 39; 40; 41; 65; 85; 53; 50] or adapter [31; 80; 68; 48; 81; 75; 89]. Lu et al.  explore the potential of collaborating CLIP's architectural variants and propose adaptive ensemble strategies to enhance the generalization performance. PromptKD  adapts a larger CLIP teacher to downstream datasets and distills the knowledge to a smaller student in an unsupervised manner, separating the need for labeled domain data during transfer. TaskRes  proposes to decouple the prior knowledge of the pre-trained models and the task-specific knowledge, enabling reliable old knowledge preservation and flexible new knowledge exploration. GraphAdapter  further utilizes the dual-modality structure knowledge for better adaptation in downstream tasks.

**Agent collaboration.** Considering the complementary knowledge of diverse pre-trained models specialized in different domains or tasks, several works have been proposed to solve vision tasks with agent collaboration [74; 82; 72; 52; 71; 28]. The most relevant to our work is CaFo , which transfers the external knowledge using cache models . However, such an ensemble manner introduces further costs in the inference stage. On the contrary, we adopt heterogeneous agent collaboration to aggregate the knowledge, and distillation strategy to inject the knowledge into CLIP, which demonstrates better performance and guarantees deployment efficiency.

**Multi-teacher distillation.** To improve the effectiveness of knowledge distillation , recent works [10; 30; 26; 76; 77; 78; 83; 54; 63] attempt to integrate the knowledge from multiple teacher networks. To be noted, how to perform multi-teacher distillation is not trivial, and how to extract and collaborate knowledge from various heterogeneous teachers has not been fully explored for CLIP-like foundation models. In this work, we devise a generic knowledge extraction method and flexible knowledge collaboration mechanism to further enhance the generalization ability of VL foundation models.

## 3 Method

In this section, we introduce our TransAgent in detail. As shown in Figure 1, it consists of vision-language foundation models and agents from different modalities. In the following section, we will illustrate how to collaborate with them for downstream visual recognition tasks. To start with, we briefly review the V-L foundation models by using the well-known CLIP . Specifically, CLIP consists of two branches. In the vision branch, an image is first divided into \(L\) non-overlapping equal-sized patches and then projected as input visual tokens \(_{in}\), which are fed to the vision encoder to obtain image feature. In the language branch, a text description is projected as input textual tokens \(_{in}\) which are then processed by the text encoder to generate text feature. Through contrastive learning over massive image-text pairs, CLIP achieves good alignment between the two modalities.

More interestingly, such large-scale V-L models show the potential in visual classification on the downstream tasks. By converting each class label into a text template such as "a photo of a {classname)", these models can easily achieve zero-shot inference. To further enhance their generalization ability under the few-shot settings, prompt learning methods are proposed, which introduce a number of learnable prompts while freezing the pre-trained CLIP . Following previous works, we add a set of learnable textual prompts \(_{T}^{N_{ctx} C}\), where \(N_{ctx}\) is the number of learnable prompts, in the language branch and concatenate them with the textual tokens, which are then processed by the text encoder to obtain the prompted textual feature \(^{N_{cls} C}\) for all \(N_{cls}\) categories. Similarly, a set of learnable visual prompts \(_{V}\) are inserted in the image branch to generate the prompted visual feature \(^{N C}\) where \(N\) denotes the number of images:

\[=(_{in},_{T}), =(_{in},_{V}).\] (1)

Consequently, we compute the prediction score vectors \(=\{^{c}\}\) for the image samples, where \(^{c}\) is the cosine similarity between the visual feature \(\) and the textual feature \(^{c}\) of specific class \(c\). As a result, we minimize the cross entropy loss between the score vectors and the ground truth labels:

\[_{}=((), ),\] (2)

to fine-tune the learnable prompts \(_{T}\) and \(_{V}\) for visual recognition. However, as mentioned in the introduction, it is difficult to achieve good generalization by adapting such a single CLIP model, especially when the domain shift of the target datasets is large. Hence, we propose to transfer diversified knowledge from heterogeneous agents in different modalities for better adaption of CLIP.

### Vision Agent Collaboration (VAC)

One important component of CLIP is its vision branch in Figure 2. Hence, we consider to enhance this branch by transferring visual knowledge from various vision agents. To achieve this goal, we have to answer three critical questions. The first question is which models should be used for collaboration. As we know, CLIP mainly establishes image-level alignment between the two modalities, neglecting visual details in the pixel space. To fill this gap, we choose vision agents from two aspects. On one aspect, we choose vision models pre-trained with self-supervision such as MAE  and DINO . Both agents focus on detailed image modeling via image masking  or patch self-distillation . On the other aspect, we choose vision models built on dense supervision such as ViTDet  and SAM . Both agents work on instance-level prediction with bounding boxes and masks. Detailed information on these models can be found in the supplementary.

The second question is how to extract visual knowledge by collaborating with these agents. As mentioned in the introduction, the latent feature is a common knowledge form among these heterogeneous models. Hence, given an input image, we extract the intermediate visual features

Figure 2: **Vision Agent Collaboration and Language Agent Collaboration.** (a) VAC integrates visual knowledge via MoA gating and transfers the knowledge through layer-wise feature distillation. (b) LAC enhances the textual representations through class-specific feature distillation between the prompted textual feature and the gated textual feature.

\(\{_{A}(i)^{N C}\}\) from the vision encoders of these agents. Moreover, the contribution of different agents may vary among different domains. To fully exploit these agents in a unified manner, we introduce a Mixture-of-Agents (MoA) gating mechanism to adaptively integrate \(\{_{A}(i)\}\) as the visual knowledge. Specifically, we concatenate all the agent features along the channel dimension \(C\), and feed them into an MLP network to generate the gating weight \(_{V}\). Next, we obtain the gated visual features \(_{A}\) by computing the weighted sum over \(\{_{A}(i)\}\):

\[_{V}=((\{_{A}(i)\})),\ \ \ \ \ _{A}=_{i}_{V}(i)_{A}(i).\] (3)

The final question is how to transfer the visual knowledge to enhance CLIP's vision encoder. We adopt feature distillation  where we compute the L1 loss between the prompted visual features \(\) in Eq. 1 and the gated visual features of the vision agents:

\[_{}=|-_{A}|.\] (4)

Since the original CLIP is frozen, the above loss term allows us to fine-tune the learnable visual prompts \(_{V}\) in Eq. 1 and MLP gating network in Eq. 3, which enables us to adaptively transfer external visual knowledge to the visual prompts to empower generalization ability.

### Language Agent Collaboration (LAC)

The other important component of CLIP is its language branch in Figure 2. Similar to the vision branch, we consider three critical questions to transfer the textual knowledge from various language agents. Recent studies [60; 42] have shown that it is coarse to use a simple template (_e.g._, "a photo of a {class name}") to describe a certain category. Hence, we first interact with the popular chatbots such as GPT-3  and Vicuna  to enrich the class descriptions using queries like "What does a {class name} look like?". After obtaining the detailed descriptions from these chatbots, we use a text encoder (_e.g._, BERT ) to extract the text features \(\{_{A}(j)^{N_{elj} C}\}\) of all descriptions. To adaptively integrate \(\{_{A}(j)\}\) as the textual knowledge, we also utilize the MoA gating mechanism:

\[_{T}=((\{_{A}(j)\})),\ \ \ \ \ _{A}=_{j}_{T}(j)_{A}(j).\] (5)

Finally, for each category, we perform feature distillation where we compute the L1 loss between the prompted textual feature in Eq. 1 and the gated textual feature from language agents in Eq. 5:

\[_{}=|-_{A}|.\] (6)

By fine-tuning the learnable textual prompts \(_{T}\) in Eq. 1 and MLP gating network in Eq. 5, we adaptively transfer the rich textual knowledge to enhance CLIP's textual representations.

### Multi-modal Agent Collaboration (MAC)

Through vision and language agent collaboration, we can enhance the learnable visual and textual prompts respectively. Considering the key success in vision-language foundation models is credited to the multi-modal alignment, we investigate how to further align the visual and textual prompts with external multi-modal agents. First, there exist two types of multi-modal agents, including Text-to-Image (T2I) generative models [64; 11] and Image-to-Text (I2T) captioning models [46; 12]. Since both types of agents involve conversion from one modality to the other, we believe they implicitly achieve vision-language alignment. Based on the above discussion, we choose our T2I agents built upon two mainstream structures including Stable Diffusion  in UNet style and PixArt-\(\) in DiT style. Moreover, we choose our I2T agents specialized in two mainstream tasks including BLIP-2  for general captioning and Shikra  for grounded dialogue.

Next, we consider what kind of knowledge can represent vision-language alignment. The most direct form may be the probability score vector that shows the prediction confidence over target classes. Hence, for each training image, we investigate such knowledge in these multi-modal agents.

In the T2I agents, cross attention effectively encodes relations between image and text [34; 84]. Hence, we leverage such prior to extract the score vector. Specifically, we use the template description of a class \(c\) as text. Then, we extract cross attention value \(_{k}^{c}\) between the text and the \(k\)-th token of the input image from the pre-trained T2I agents. Consequently, we sum over all the tokens to obtain the prediction score of the image _w.r.t._ class \(c\): \(_{T2I}^{c}=(_{k}(_{k}^{c}))\), where we adopt LogSumExp (LSE) pooling  to provide more accurate matching scores.

In the I2T agents, there exists a projection module (_e.g._, Q-Former in BLIP-2  and MLP in Shikra ) to adapt the visual features for the large language model. Hence, we extract the visual feature of an input image via the projection and the textual features of all the classes from the LLM. By computing the cosine similarity between them, we can obtain the prediction score \(_{I2T}\) of the image over all the classes. Next, we leverage the MoA gating mechanism to adaptively summarize the prediction score vectors from all the multi-modal agents \(_{A}=(\{_{T2I},_{I2T}\})\):

\[_{S}=(_{A}), 14.226378pt_{A}= _{n}_{S}(n)_{A}(n).\] (7)

Finally, we explore how to transfer \(_{A}\) to enhance the learnable prompts in CLIP. Specifically, after being processed by the vision and text encoders of CLIP, the learnable visual and textual prompts \(_{V}\) and \(_{T}\) are transformed into \(_{V}\) and \(_{T}\). Unlike \(_{V}\) and \(_{T}\) which are universal, \(_{V}\) is relevant to the image samples and \(_{T}\) is relevant to the target classes. Hence, we can compute the cosine similarity between \(_{V}\) and \(_{T}\) to obtain the learned score vectors \(_{P}^{N N_{cls}}\) of the input images over all the classes. We perform score distillation between \(_{P}\) and \(_{A}\):

\[_{}=((_{P})||(_{A})).\] (8)

Through computing the KL divergence, we can leverage external multi-modal knowledge \(_{A}\) as a semantic guidance to align the learnable visual and textual prompts.

### Multi-Source Knowledge Distillation

Finally, we combine all the distillation loss from multiple sources, achieving heterogeneous agent collaboration for knowledge transfer:

\[_{}=_{}+_{1}_{}+_{2}_{}+_{3}_{ },\] (9)

Figure 3: **Multi-modal Agent Collaboration.**_Top left:_ We first extract the cross attention maps from the T2I agents and then obtain the score vectors through LSE pooling. _Top right:_ We compute the score vectors from the I2T agents as the cosine similarity between the projected visual feature and the LLM’s textual feature. Finally, we perform score distillation between the learned score vectors and the gated score vectors to further align the learnable prompts.

where \(_{1}\), \(_{2}\), \(_{3}\) are hyperparameters. Since all the pre-trained models are frozen in the training phase, we only need to fine-tune the learnable vision and language prompts with negligible cost. More importantly, owing to the distillation strategy, all the agents can be unloaded and the modality-specific gates can be abandoned in the inference phase. We can simply use the enhanced CLIP just like the original one, which largely boosts deployment efficiency without a model ensemble.

## 4 Experiments

**Datasets and Metrics.** We evaluate our proposed method on 11 commonly used datasets covering a wide range of recognition tasks, including ImageNet , Caltech101 , OxfordPets , StanfordCars , Flowers102 , Food101 , FGVCAircraft , SUN397 , UCF101 , DTD  and EuroSAT . We explore two typical low-shot scenarios to evaluate the performance. (i) Base-to-novel generalization: The datasets are equally split into base and novel classes. The model is trained on base classes and evaluated on the test set of both classes. We report the base and novel class accuracy and the harmonic mean (HM) of the results. (ii) Few-shot classification: We assess the accuracy trained with 1/2/4/8/16 shot(s) per class to examine the model's learning capacity.

**Implementation Details.** We adopt CLIP ViT/B-16 as our backbone and conduct all experiments using 3 different seeds to obtain an averaged result, following previous works [40; 41; 50]. Our method ensembles knowledge from heterogeneous agents, including pre-trained vision models, LLMs, T2I generative models, and I2T captioning models. Detailed information on the agents and training settings are shown in Appendix A.

    &  &  &  &  \\   & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM \\  CLIP  & 69.34 & 74.22 & 71.70 & 72.43 & 68.14 & 70.22 & 96.84 & 94.00 & 95.40 & 91.17 & 97.26 & 94.12 \\ CoOp  & 82.69 & 63.22 & 71.66 & 76.47 & 67.88 & 71.92 & 98.00 & 89.81 & 93.73 & 93.67 & 95.29 & 94.47 \\ CoCoOp  & 80.47 & 71.69 & 75.83 & 75.98 & 70.43 & 73.10 & 97.96 & 93.81 & 95.84 & 95.20 & 97.69 & 96.43 \\ MapLe  & 82.28 & 75.14 & 78.55 & 75.40 & 70.32 & 72.72 & 98.27 & 93.23 & 95.68 & 95.43 & 97.83 & 96.62 \\ RPO  & 81.13 & 75.00 & 77.78 & 76.60 & **71.57** & 74.00 & 97.97 & 94.37 & 96.03 & 94.63 & 97.50 & 96.05 \\ PromptSRC  & 84.26 & 76.10 & 79.97 & 77.60 & 70.73 & 74.01 & 98.10 & 94.03 & 96.02 & 95.33 & 97.30 & 96.30 \\ 
**TransAgent** & **85.29** & **77.62** & **81.27** & **78.07** & 70.57 & **74.13** & **98.90** & **95.23** & **97.03** & **96.33** & **98.13** & **97.22** \\    
    &  &  &  &  \\   & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM \\  CLIP  & 63.37 & 74.89 & 68.65 & 72.08 & 77.80 & 74.83 & 90.10 & 91.22 & 90.66 & 27.19 & 36.29 & 31.09 \\ CoOp  & 78.12 & 60.40 & 68.13 & 97.60 & 59.67 & 74.06 & 88.33 & 82.26 & 85.19 & 40.44 & 22.30 & 28.75 \\ CoCoOp  & 70.49 & 73.59 & 72.01 & 94.87 & 71.75 & 81.71 & 90.70 & 91.29 & 90.99 & 33.41 & 23.71 & 27.74 \\ MaPLe  & 74.70 & 71.20 & 72.91 & 97.70 & 68.68 & 80.66 & 90.30 & 88.57 & 89.43 & 36.90 & 34.13 & 35.46 \\ RPO  & 73.87 & **75.53** & 74.69 & 94.13 & 76.67 & 84.50 & 90.33 & 90.83 & 90.58 & 37.33 & 34.20 & 35.70 \\ PromptSRC  & 78.27 & 74.97 & 76.58 & 98.07 & 76.50 & 85.95 & 90.67 & 91.53 & 91.10 & 42.73 & 37.87 & 40.15 \\ 
**TransAgent** & **79.53** & 74.73 & **77.06** & **98.37** & **77.13** & **86.46** & **90.87** & **92.20** & **91.53** & **43.77** & **39.00** & **41.25** \\    
    &  &  &  &  \\   & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM \\  CLIP  & 69.36 & 75.35 & 72.23 & 53.24 & 59.90 & 56.37 & 56.48 & 64.05 & 60.03 & 70.53 & 77.50 & 73.85 \\ CoOp  & 80.60 & 65.89 & 72.51 & 79.44 & 41.18 & 54.24 & 92.19 & 54.74 & 68.69 & 84.69 & 56.05 & 67.46 \\ CoCoOp  & 79.74 & 76.86 & 78.27 & 77.01 & 56.00 & 64.85 & 87.49 & 60.04 & 71.21 & 82.33 & 73.45 & 77.64 \\ MaPLe  & 78.47 & 76.93 & 77.79 & 80.67 & 56.48 & 66.44 & 83.90 & 66.00 & 73.88 & 85.23 & 71.97 & 78.04 \\ RPO  & 80.60 & 77.80 & 79.18 & 76.70 & 62.13 & 68.61 & 86.63 & 68.97 & 76.79 & 83.67 & 75.43 & 79.34 \\ PromptSRC  & 82.67 & 78.47 & 80.52 & 83.37 & 62.97 & 71.75 & 92.90 & 73.90 & 82.32 & 87.10 & 78.80 & 82.74 \\ 
**TransAgent** & **82.90** & **79.30** & **81.06** & **84.37** & **63.67** & **72.57** & **97.43** & **83.43** & **89.89** & **87.60** & **80.47** & **83.88** \\   

Table 1: **Accuracy comparison with state-of-the-art methods on base-to-novel generalization. All methods use CLIP’s ViT-B/16 as the vision encoder. Our TransAgent exhibits strong generalization ability and outperforms previous SOTA on all datasets. The best results are **bolded**.

### Comparison with State-of-the-Art

In Table 1, we compare the base-to-novel generalization performance of our proposed TransAgent with state-of-the-art prompt learning methods, including CoOp , CoCoOp , MaPLe , RPO  and PromptSRC . All approaches use the same CLIP ViT-B/16 during the evaluation stage. Our method demonstrates superior performance across 11 datasets, surpassing all competing methods in terms of base and novel accuracy, as well as the harmonic mean, particularly excelling on the EuroSAT dataset. In Figure 4, we present the few-shot classification performance of TransAgent and previous methods. Our method consistently outperforms the counterparts and demonstrates growing learning capability when the training samples increase. To be mentioned, TransAgent outperforms CaFo  with much lower deployment costs (see Table 13). Detailed comparisons on cross-dataset evaluation and domain generalization are provided in Appendix B.

### Ablative Analysis

In this section, we present the ablative analysis of our agent collaboration designs for each modality. We adopt IVLP  as our baseline model.

Figure 4: **Accuracy comparison in few-shot classification.** TransAgent demonstrates _state-of-the-art_ performance for all few-shot settings on different datasets, which proves promising learning capability even under extremely limited supervision.

**Effectiveness of individual agent.** In Table 2 - Table 4, we show the results of introducing an individual agent as a teacher to supervise the baseline model in the beginning rows. Nearly all vision agents contribute to the improvement in accuracy, except for DINO which is behind. However, we observe that DINO performs well on most datasets, except on EuroSAT (with a 5.60% decline). Nonetheless, we still keep DINO as one of the vision agents. While for the language and the multi-modal agents, they all boost the performance of the baseline model respectively.

**Distillation strategy.** In the middle rows of Table 2 - Table 4, we ablate the alignment strategy for agent collaboration of each modality. (i) For VAC, we choose to adopt feature distillation with either the average-pooled feature at the last layer or all features at all layers. As the results suggest, layer-wise distillation of all features performs better. (ii) For LAC, two special tokens from the output of the text encoder are considered to compute Equation 6. These tokens are inserted into the text sequence to compose a complete prompt. \([EOS]\) performs better owing to the causal attention module in Transformer blocks, so that it aggregates more information. (iii) For MAC, either the prompted logits of CLIP or the learned scores (Eq. 8) are chosen to align with the semantic logits from multi-modal agents. Since the prompted logits need to be aligned with ground-truth labels as well (Eq. 2), there might be confusion to align it with the external knowledge simultaneously. Experimental results show that using learned scores to align with semantic logits yields better results.

  
**VAC** & **LAC** & **MAC** & **HM** & \(\) \\  ✓ & ✓ & ✗ & 80.02 & +2.51 \\ ✓ & ✗ & ✓ & 79.79 & +2.28 \\ ✗ & ✓ & ✓ & 80.40 & +2.89 \\ ✓ & ✓ & ✓ & 81.27 & +3.76 \\   

Table 6: **Performance w/ module combinations.**

  
**Models** & **Base** & **Novel** & **HM** \\  baseline & 84.21 & 71.79 & 77.51 \\  GAPT-3 & 85.15 & 74.55 & 79.50 \\ Vicuna & 85.35 & 74.70 & 79.67 \\ ViTDet & 84.45 & 72.04 & 77.75 \\  Last-layer & 84.47 & 75.92 & 79.97 \\ Layer-wise & 85.29 & 77.62 & 81.27 \\  Average & 84.20 & 74.79 & 79.22 \\ Add & 84.40 & 75.17 & 79.52 \\ Gating & 85.29 & 77.62 & 81.27 \\   

Table 2: **VAC Design.**

Figure 5: **Averaged gating weights of each agent on different datasets.** Deeper color indicates more contributions to the gated feature(s) or score vectors.

  
**Models** & **Base** & **Novel** & **HM** \\  baseline & 84.21 & 71.79 & 77.51 \\  MAE & 84.51 & 71.66 & 77.55 \\ DINO & 84.60 & 70.90 & 77.15 \\ SAM & 84.73 & 71.87 & 77.77 \\ ViTDet & 84.45 & 72.04 & 77.75 \\  Last-layer & 84.47 & 75.92 & 79.97 \\ Layer-wise & 85.29 & 77.62 & 81.27 \\  Average & 84.20 & 74.79 & 79.22 \\ Add & 84.40 & 75.17 & 79.52 \\ Gating & 85.29 & 77.62 & 81.27 \\   

Table 3: **LAC Design.**

**Fusion design.** In the last rows of Table 2 - Table 4, we ablate the fusion design for each modality. "Average" refers to simply calculating the average of all output features from the agents along the channel dimension; "Add" refers to calculating the distillation loss separately for each agent; "Gating" denotes our proposed MoA gating mechanism. As can be seen, gating fusion achieves the best results for all collaboration designs, since it adaptively selects the useful information from the agents.

**Effectiveness of collaboration module(s).** As presented in Table 5, the first row indicates the baseline model. Each individual module is beneficial for improving the generalization ability of the foundation models, where the last column shows the relative increase. Results in Table 6 demonstrate that the combinations of collaboration modules further boost the results.

### Visualization

We calculate the averaged gating weights of each agent with 16-shot training samples from all classes of a certain dataset and present the results with heat maps. As shown in Figure 5, different agents do not contribute equally towards certain target datasets, which also verifies the superiority of our gating fusion design. For vision agents, DINO experts in recognizing general objects while lags behind on some fine-grained datasets (_e.g._, EuroSAT), whereas the others that focus more on details perform better. Language agents provide domain-specific linguistic knowledge accordingly. I2T agents consistently provide their knowledge thanks to the grounding nature, while T2I agents demonstrate better capability in fine-grained scenarios.

## 5 Conclusion

We propose TransAgent, a unified framework to transfer vision-language foundation models through heterogeneous agent collaboration. By adaptively integrating the external knowledge of agents from different modalities viac the MoA gating mechanism, TransAgent achieves _state-of-the-art_ performance on 11 datasets under the low-shot scenarios.

**Limitations.** Although TransAgent collaborates with heterogeneous agents in a unified manner, transferring the external knowledge through distillation may harm the original CLIP's representations even using prompt learning methods because the domain knowledge from agents are diversified, and irrelevant information may also be introduced. Moreover, one of the key characteristics of these agents is large-scale pre-training, few-shot scenarios may not meet their data-hungry nature. So our future work would be integrating the knowledge with directed focus and further unleashing the potential of the agents even when the domain-specific knowledge (_e.g._, labels) is absent.