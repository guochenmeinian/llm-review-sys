# Understanding and Improving Adversarial Collaborative Filtering for Robust Recommendation

Kaike Zhang\({}^{1,2}\), Qi Cao\({}^{1}\)1, Yunfan Wu\({}^{1,2}\), Fei Sun\({}^{1}\), Huawei Shen\({}^{1}\), Xueqi Cheng\({}^{1}\)

\({}^{1}\) CAS Key Laboratory of AI Safety, Institute of Computing Technology,

Chinese Academy of Sciences

\({}^{2}\) University of Chinese Academy of Sciences

Beijing, China

{zhangkaike21s, caoqi, wuyunfan19b, sunfei, shenhuawei, cxq}@ict.ac.cn

Corresponding author.

###### Abstract

Adversarial Collaborative Filtering (ACF), which typically applies adversarial perturbations at user and item embeddings through adversarial training, is widely recognized as an effective strategy for enhancing the robustness of Collaborative Filtering (CF) recommender systems against poisoning attacks. Besides, numerous studies have empirically shown that ACF can also improve recommendation performance compared to traditional CF. Despite these empirical successes, the theoretical understanding of ACF's effectiveness in terms of both performance and robustness remains unclear. To bridge this gap, in this paper, we first theoretically show that ACF can achieve a lower recommendation error compared to traditional CF with the same training epochs in both clean and poisoned data contexts. Furthermore, by establishing bounds for reductions in recommendation error during ACF's optimization process, we find that applying personalized magnitudes of perturbation for different users based on their embedding scales can further improve ACF's effectiveness. Building on these theoretical understandings, we propose **P**ersonalized **M**agnitude **A**dversarial **C**llaborative **F**iltering (**PamaCF**). Extensive experiments demonstrate that PamaCF effectively defends against various types of poisoning attacks while significantly enhancing recommendation performance.

## 1 Introduction

Collaborative Filtering (CF) is widely recognized as a powerful tool for providing personalized recommendations  across various domains . However, the inherent openness of recommender systems allows attackers to inject fake users into the training data, aiming to manipulate recommendations, also known as poisoning attacks . Such manipulations can skew the distribution of item exposure, degrading the overall quality of the recommender system, thus harming the user experience and hindering the long-term development of the recommender system .

Existing methods for defending against poisoning attacks in CF can be categorized into two types : (1) detecting and mitigating the influence of fake users , and (2) developing robust models via adversarial training, also known as Adversarial Collaborative Filtering (ACF) . The first strategy focuses on detecting and removing fake users from the dataset before training  or mitigating their impact during the training phase . These methods often rely on predefined assumptions about attacks  or require labeled data related to attacks . Consequently, deviations from predefined attack patterns may lead to misclassification, failing to resist attacks while potentially harming genuine users' experience .

In contrast, ACF provides a more general defense paradigm without prior knowledge [15; 16; 17; 18; 19; 20]. Poisoning attacks in recommender systems mainly affect the learned embeddings of users and items, i.e., the system's parameters [6; 7]. Predominant ACF methods, particularly those aligned with Adversarial Personalized Ranking (APR) framework , heuristically incorporate adversarial perturbations at the parameter level during the training phase to mitigate these attacks [15; 17; 19; 20]. This approach employs a "min-max" paradigm, designed to minimize the recommendation error while contending with parameter perturbations aimed at maximizing this error within a specified magnitude , thus enhancing the robustness of CF.

It is interesting to note that adversarial training in the Computer Vision (CV) domain [21; 22; 23] has been observed to degrade model performance on clean samples [24; 25]. Several studies have also theoretically demonstrated a trade-off between robustness against evasion attacks and the performance of adversarial training in CV . In contrast, ACF in recommender systems has been shown in numerous studies not only to enhance the robustness against poisoning attacks [8; 13; 18] but also to improve recommendation performance [15; 20; 27]. Despite the empirical evidence highlighting ACF's advantages, it still lacks a comprehensive theoretical understanding, which limits the ability to fully exploit the benefits and potential of ACF. To bridge this gap, in this paper, we propose the following research questions for further investigation:

1. _Why does ACF enhance both robustness and performance compared to traditional CF?_
2. _How can we further improve ACF?_

To answer these questions, we delve into a theoretical analysis of a simplified CF scenario. This analysis confirms that ACF can achieve a lower recommendation error at the same training epoch in both clean and poisoned data contexts, showing better performance and robustness compared to traditional CF. To investigate potential improvements to ACF, we establish upper and lower bounds for reductions in recommendation error during ACF's optimization process. Our findings indicate that (1) Users have varying constraints for perturbation magnitudes, i.e., different maximum perturbation magnitudes; (2) Within these constraints, applying personalized perturbation magnitudes as much as possible for each user can increase the error reduction bounds, further improving ACF's effectiveness.

Extending our theoretical results to practical CF scenarios, we establish a positive correlation between users' maximum perturbation magnitudes and their embedding scales. Building on these theoretical understandings, we introduce **P**ersonal**alized **M**agnitude **A**dversarial **C**ollaborative **F**iltering (**PamaCF**). PamaCF dynamically and personally assigns perturbation magnitudes based on users' embedding scales. Extensive experiments confirm that PamaCF outperforms baselines in both performance and robustness. Notably, PamaCF increases the average recommendation performance of the backbone model by 13.84% and reduces the average success ratio of attacks by 44.92% compared to the best baseline defense method. The main contributions of our study are summarized as follows:

* We provide theoretical evidence that ACF can achieve better performance and robustness compared to traditional CF in both clean and poisoned data contexts.
* We further identify upper and lower bounds of reduction in recommendation error for ACF during its optimization and demonstrate that applying personalized magnitudes of perturbation for each user can further improve ACF.
* Based on the above theoretical understandings, we propose Personalized Magnitude Adversarial Collaborative Filtering (PamaCF), with extensive experiments confirming that PamaCF further improves both performance and robustness compared to state-of-the-art defense methods.

The code of our experiments is available at https://github.com/Kaike-Zhang/PamaCF.

## 2 Preliminary

**Collaborative Filtering** (CF) methods are widely employed in recommender systems. Following , we define a set of users \(=\{u\}\) and a set of items \(=\{v\}\). Using data from user-item interactions, our objective is to learn latent embeddings \(=[^{d}]_{u}\) for users and \(=[^{d}]_{v}\) for items. Then, we employ a preference function \(f:^{d}^{d}\), which predicts user-item preference scores, denoted as \(_{u,v}=f(,)\).

**Adversarial Collaborative Filtering** (ACF) is acknowledged as an effective approach for enhancing both the performance and the robustness of CF recommender systems in the face of poisoning attacks. ACF methods, particularly within the framework of Adversarial Personalized Ranking (APR) , integrate adversarial perturbations at the parameter level (i.e., the latent embeddings \(\) and \(\)) during the training phase. Let \(()\) denote the loss function of the CF recommender system, where \(=(,)\) represents the recommender system's parameters. ACF methods apply perturbations \(\) directly to the parameters as:

\[_{}()= ()+(+^{ }),\] (1) \[^{}= _{,\,\|\|}(+ ),\]

where \(>0\) defines the maximum magnitude of perturbations, and \(\) is the adversarial training weight. Due to constraints on space, a detailed discussion of related works is provided in Appendix A.

## 3 Theoretical Understanding of ACF

In this section, we provide a theoretical analysis of why ACF achieves superior performance and robustness compared to traditional CF from the perspective of recommendation error. Then, we explore mechanisms to further improve ACF's effectiveness based on its error reduction bounds. For clarity and simplicity, we initially focus on a Gaussian Single-item Recommender System, aligning with the frameworks presented in [18; 28]. It's important to **note that** the insights and analytical frameworks developed here are also applicable to more practical scenarios, as discussed in Section 4.

**Definition 1** (Gaussian Recommender System).: _Given a rating set \(=\{r_{1},r_{2},,r_{n}\}\) corresponding to \(n\) users, where each rating \(r\) is randomly selected from \(\{ 1\}\), an average embedding vector \(}^{d}\), and \(>0\), the Gaussian Recommender System initializes each user's embedding \(\) from the normal distribution \((r},^{2})\). The item embedding \(\) is initialized as the average vector derived from these users: \(=_{i=1}^{n}r_{i}_{i}\). Then, a preference function \(f:^{d}^{d}\{ 1\}\) is employed to predict user preferences: \(f(,)=(,)\), where \(()\) denotes the sign function, returning 1 if \(,>0\) and -1 otherwise._

Based on Definition 1, we obtain \(=\{(_{1},r_{1}),,(_{n},r_{n})\}\), where \(\) represents the system-learned user embedding. With continued training, both each user embedding \(\) and item embedding \(\) are iteratively updated. Let \(_{(t)}\) and \(_{(t)}\) denote user and item embeddings at the \(t^{}\) epoch, respectively. For analytical simplicity and without loss of generality, we define the standard loss function \(()\) (as traditional CF) used in the Gaussian Recommender System as follows :

\[(_{(t)})=-_{(,r)}[r _{(t)},_{(t)}],\] (2)

where the model parameters \(_{(t)}=(_{(t)},[_{1,(t)},_{2,(t)},, {u}_{n,(t)}])\). To integrate ACF into the Gaussian Recommender System, we introduce the adversarial loss , \(_{}()\), defined as:

\[_{}(_{(t)})=(_{(t) })-_{(,r)}[r_{(t)}+ _{},_{(t)}+_{}],\] (3)

where \(\) is the adversarial training weight. The perturbations \(_{}\) and \(_{}\) are applied to the user and item embeddings, respectively, as computed based on Equation 1.

### Why Does Adversarial Collaborative Filtering Benefit Recommender Systems?

To analyze the performance and robustness of traditional CF and ACF within the Gaussian Recommender System, we evaluate them from the perspective of recommendation error during the training process. For each user, both performance and robustness are reflected by the user's recommendation error. Specifically, attacks--whether item promotion attacks [6; 29] or performance damage attacks --inevitably increase the user's recommendation error. Meanwhile, a smaller recommendation error means a higher recommendation performance. For a given user \(\), the initial item embedding \(_{(0)}\) in the Gaussian Recommender System can be approximately modeled2 as a sample from \((},}{n-1})\). Here, we provide the definition of recommendation error for user \(\).

**Definition 2** (Recommendation Error).: _Given a Gaussian Recommender System \(f_{(t)}\) that has been trained for \(t\) epochs, the recommendation error for the user \(\) with rating \(r\) at the \(t^{}\) epoch is defined as the probability that the system's prediction does not align with the user's actual rating, as:_

\[_{_{(0)}(},}{n-1}I)} [f_{(t)}(,) r(,r)]_{ _{(0)}(},}{n-1}I)}[(f_{(t)}(,) r(,r),_{(0)})],\]

_where \(()\) is an indicator function that returns 1 if the condition is true and 0 otherwise._

Based on the framework of ACF , which includes \(t\) epochs of pre-training with standard loss before adversarial training, we derive a theorem that identifies the difference in recommendation error between standard and adversarial loss at the \((t+1)^{}\) epoch. To distinguish between the recommendation error of traditional CF and ACF, we define \(_{_{(0)}(},}{n-1}I )}[f_{(t+1)}(,) r(,r)]\) as the recommendation error following standard training (Equation 2) at the \((t+1)^{}\) epoch, and \(_{_{(0)}(},}{n-1}I )}^{}[f_{(t+1)}(,) r(,r)]\) as the recommendation error following adversarial training (Equation 3) at the \((t+1)^{}\) epoch.

**Theorem 1**.: _Consider a Gaussian Recommender System \(f_{(t)}\), pre-trained for \(t\) epochs using the standard loss function (Equation 2). Given a learning rate \(\), an adversarial training weight \(\), and a perturbation magnitude \(\), when \(<_{(t)}()\|,\|\|)}{}\), and \(\|}\|^{3}\), the recommendation error for a user \(\) with rating \(r\) at the \((t+1)^{}\) epoch follows that:_

\[_{_{(0)}(},}{n-1}I )}[f_{(t+1)}(,) r(,r)]>_{_{(0)}(},}{n-1}I)}^{} [f_{(t+1)}(,) r(,r)].\]

For the proof, please refer to Appendix D.1.1. After the same epochs of pre-training, ACF at the next epoch achieves a lower recommendation error compared to traditional CF, thereby benefiting recommendation performance.

Next, our analysis extends to contexts where the recommender system is subject to poisoning attacks. These attacks involve injecting fake users into the system's training dataset to manipulate item exposure. We examine a Gaussian Recommender System with \(=\{(_{1},r_{1}),,(_{n},r_{n})\}\), where each tuple \((,r)^{d}\{ 1\}\) represents the learned embedding and the rating of a genuine user. A _poisoning attack_ on this system injects a _poisoning user set_, \(^{}=\{(^{}_{1},r^{}_{1}),(^{}_ {2},r^{}_{2}),,(^{}_{n^{}},r^{}_{n^{ }})\}\), with each tuple \((^{},r^{})^{d}\{ 1\}\) representing a fake user crrafted by attackers4. The _poisoned item embedding_\(^{}\) is reinitialized to include both genuine and malicious contributions:

\[^{}=}(_{(,r)}r +_{(^{},r^{})^{}}r^{} ^{}),\]

where \(n\) and \(n^{}\) represent the number of genuine and fake users, respectively.

To evaluate the impact of these attacks, we introduce a formal definition of recommendation error in poisoned data.

**Definition 3** (\(\)-Poisoned Recommendation Error).: _Given a boundary \(>0\), and a set of fake users injected by attackers within this boundary, i.e., \(^{}(^{},)=\{(^{ },r^{})(^{},r^{})^{d}\{  1\}\|^{}\|_{}\}\), the \(\)-poisoned recommendation error for the genuine user \(\) with rating \(r\) at the \(t^{}\) epoch is defined as the probability:_

\[_{_{(0)}(},}{n-1}I)} [f_{(t),}(,^{}) r(,r)] _{_{(0)}(},} {n-1}I)}[(f_{(t),}(,^{}) r (,r),_{(0)})],\]

_where \(f_{(t),}\) represents the Gaussian Recommender System under the \(\)-poisoned condition, and \(()\) is an indicator function that returns 1 if the condition is true and 0 otherwise._

For simplicity, we continue using the distribution of \(_{(0)}\) from the definition. This allows us to further analyze the \(\)-poisoned recommendation error based on the distribution of \(^{}_{(0)}=}_{(0)}+} _{(^{},r^{})^{}}r^{}^{}\).

Then we extend Theorem 1 to \(\)-Poisoned Recommendation Error in the following theorem:

**Theorem 2**.: _Consider a poisoned Gaussian Recommender System \(f_{(t),}\), pre-trained for \(t\) epochs using the standard loss function (Equation 2). Given a learning rate \(\), an adversarial training weight \(\), and a perturbation magnitude \(\), when \(<_{(t+k)}\|,\|}\|)}{n}\), and \(\|}\|\), the \(\)-poisoned recommendation error for a genuine user \(\) with rating \(r\) at the \((t+1)^{}\) epoch follows that:_

\[_{_{(0)}(},}{n-1}I)} [f_{(t+1),}(,^{}) r(,r)]> _{_{(0)}(},}{n-1}I)} ^{}[f_{(t+1),}(,^{}) r(,r)].\]

For the proof, please refer to Appendix D.1.2. Combining Theorem 1 and Theorem 2, we find that adversarial training, i.e., ACF, lowers recommendation errors compared to traditional CF in both clean and poisoned data contexts. Accordingly, ACF achieves better performance and robustness.

### How to Further Enhance Adversarial Collaborative Filtering

To explore mechanisms to further improve the effectiveness of ACF, we subsequently derive upper and lower bounds on the reduction of recommendation error between any two consecutive epochs after \(t\) epochs of pre-training.

**Theorem 3**.: _Consider a Gaussian Recommender System \(f_{(t)}\) which has been pre-trained for \(t\) epochs using standard loss (Equation 2) and subsequently trained on adversarial loss (Equation 3). For the \((t+k+1)^{}\) epoch, let the reduction in recommendation error of user \(\) with rating \(r\) relative to the \((t+k)^{}\) epoch from adversarial loss be denoted by:_

\[^{}_{(t+k+1)}_{_{(0)} (},}{n-1}I)}^{}[f(, ) r(,r)]=\] \[_{_{(0)}(},}{n-1}I)}^{}[f_{(t+k)}(,) r(,r) ]-_{_{(0)}(}, }{n-1}I)}^{}[f_{(t+k+1)}(,) r(,r) ].\]

_Given a learning rate \(\), an adversarial training weight \(\), and a perturbation magnitude \(\), when \(<_{(t+k)}\|,\|\|}\|)}{}\), and \(\|}\|\), it follows that:_

\[^{}_{(t+k+1)}_{_{(0)} (},}{n-1}I)}^{}[f(, ) r(,r)]\] \[(}{}(\|}\|+(\| }\|^{2}+}{n-1})(,t+k)))- (}{}\|}\|),\] \[^{}_{(t+k+1)}_{_{(0)} (},}{n-1}I)}^{}[f(, ) r(,r)] 2(}{2}(\|}\|^ {2}+}{n-1})(,t+k))-1,\]

_where \(d\) is the embedding dimension, and \(()\) denotes the cumulative distribution function (CDF) of the standard Gaussian distribution, and \((,t+k)\) is defined as:_

\[(,t+k)=(1+)_{(t+k)}^{}}{\|_{(t +k)}\|},_{(t+k)}^{}=(1-_{(t+k)}\|})^{-1},\] (4)

_where \(C_{t+k}\) is a constant at the \((t+k)^{}\) epoch._

For the proof, please refer to Appendix D.2.1. In light of Theorem 3, given a learning rate \(\) and an adversarial training weight \(\), we can establish the following: (1) When the conditions, i.e., \(<_{(t+k)}\|,\|}\|)}{n}\) and \(\|}\|\), are satisfied, the error reduction for ACF can be both upper and lower bounded. (2) Increasing the perturbation magnitude \(\) under the above conditions can further improve these bounds, thus benefiting ACF's effectiveness.

Then, similarly, we extend Theorem 3 to the \(\)-poisoned context.

**Theorem 4**.: _Consider a poisoned Gaussian Recommender System \(f_{(t),}\) which has been pre-trained for \(t\) epochs using standard loss (Equation 2) and subsequently trained on adversarial loss (Equation 3). For the \((t+k+1)^{}\) epoch, let the reduction in \(\)-poisoned recommendation error of a genuine user \(\) with rating \(r\) relative to the \((t+k)^{}\) epoch from adversarial loss be denoted by \(^{}_{(t+k+1)}_{_{(0)}(},}{n-1}I)}^{}[f_{}(,^{ }) r(,r)]\). Let \(=}{n}+\|}\|\) and \(=2nn^{}\|}\|_{0}\), where \(d\) is the embedding dimension, and given a learning rate \(\), an adversarial training weight \(\)_and a perturbation magnitude \(\), when \(<_{(t+k)}\|,\|}\|)}{}\), and \(\|}\|\), it follows that:_

\[_{(t+k+1)}^{}_{_{(0)}(},}{n-1}I)}^{}[f_{}(,^{ }) r(,r)]>\] \[(}{}(+(\|}\|^{2}-}{n(n+n^{})}+}{(n-1)(n+n^{ })})(,t+k)))-(}{ }),\] \[_{(t+k+1)}^{}_{_{(0)} (},}{n-1}I)}^{}[f_{}( ,^{}) r(,r)]\] \[ 2(}{2}(\|}\|^{2}+(n^{})^{2}d^{2}+}{n(n+n^{})}+ }{(n-1)(n+n^{})})(,t+k))-1,\]

_where \(()\) denotes the cumulative distribution function (CDF) of the standard Gaussian distribution, \(n^{}\) is the number of fake users, and \(()\) is defined in Equation 4._

For the proof, please refer to Appendix D.2.2. From Theorem 4, we understand that increasing \((,t+k)\) can further improve both the upper and lower bounds of error reduction, thereby mitigating the negative impact of poisons. Specifically, this involves the same mechanism as in the clean data context: increasing the perturbation magnitude \(\) within \(<_{(t+k)}\|,\|\|\|)}{}\).

In conclusion, the theorems in this section indicate that for each user \(\), when the user's perturbation magnitude meets \(<_{(t+k)}\|,\|\|)}{}\), we have the following: (1) ACF is theoretically shown to be more effective than traditional CF, and (2) Increasing the user's perturbation magnitude during training as much as possible can further improve both the performance and robustness of ACF. These theoretical understandings can further benefit exploring and fully unleashing the potential of ACF.

## 4 Methodology

To extend theoretical understandings from the simple CF scenario to more practical scenarios, such as multi-item recommendations with Bayesian Personalized Ranking (BPR) , which is a mainstream loss function used in CF recommendations, we first conduct a preliminary experiment shown in Figure 1. Using Matrix Factorization  on the Gowalla dataset , we observe results similar to those in Theorem 3 and Theorem 4: NDCG@20 for users improves within their maximum magnitudes, i.e., constraints, but significantly declines once these constraints are surpassed. Based on the theoretical understandings provided in Section 3, we derive the following corollary to identify the maximum perturbation magnitude for each user in practical CF scenarios.

**Corollary 1**.: _Given any dot-product-based loss function \(()\), within the framework of Adversarial Collaborative Filtering as defined in Equation 1, the maximum perturbation magnitude \(^{()}_{(t),}\) for user \(\) at the \(t^{}\) epoch is positively related to \(\|_{(t)}\|\)._

For the proof of Corollary 1, please refer to Appendix D.3. According to Corollary 1, we observe that for a user \(\), the larger \(\|\|\), the greater the maximum perturbation magnitude. Considering that maximum perturbation magnitudes will be affected by other factors in the actual training process, to ensure training stability, we decompose \(^{()}_{(t),}\) for a user \(\) at epoch \(t\) into two components: the uniform perturbation magnitude \(\), applicable to all users, and a user-specific perturbation coefficient \(c(,t)\), expressed as:

\[^{()}_{(t),}= c(,t).\] (5)

According to Corollary 1, \(c(,t)\) provides coefficients positively related to users' embedding scales. To avoid training instability caused by extreme scale values, we map \(c(,t)\) into the interval \((0,1)\)

Figure 1: NDCG@20 across various perturbation magnitudes for five users (subject to Random Attacks ).

defined by:

\[c(,t)=(_{(t)}\|-_{(t)}\|}}{ _{(t)}\|}}),\]

where \(_{(t)}\|}\) represents the average norm of all user embeddings at epoch \(t\), and \(()\) denotes the sigmoid function. Consequently, the loss function for our method, Personalized Magnitude Adversarial Collaborative Filtering (PamaCF), is defined as:

\[_{}()= ()+(+^{}),\] (6) \[^{}= _{,\,\|_{u}\| c(,t)} (+),\]

where \(\) is the weight of adversarial training, \(\) represents the uniform perturbation magnitude for all users, and \(_{u}\) is the perturbation relative to user \(u\). To maximize the perturbation magnitude for each user within \( c(,t)\), we use the perturbation along the gradient direction of the user's adversarial loss with a step length of \( c(,t)\) as \(_{u}\). The specific algorithm process is detailed in Appendix B.

## 5 Experiments

In this section, we conduct extensive experiments to address the following research questions (**RQs**):

* **RQ1:** Can PamaCF further improve the performance and robustness of traditional ACF?
* **RQ2:** Why does PamaCF perform better than traditional ACF?
* **RQ3:** How do hyper-parameters affect PamaCF?

### Experimental Setup

In this section, we briefly introduce the experimental settings. For detailed information, including dataset preprocessing, comprehensive baseline descriptions, and implementation details, please refer to Appendix C.1.

**Datasets**. We employ three common benchmarks: the _Gowalla_ check-in dataset , the _Yelp2018_ business dataset, and the _MIND_ news recommendation dataset .

**Attack Methods**. We employ both heuristic (Random Attack , Bandwagon Attack ) and optimization-based (Rev Attack , DP Attack ) attack methods within a black-box context, where the attacker does not have access to the internal architecture or parameters of the target model.

**Defense Baselines**. We incorporate a variety of defense methods, including detection-based approaches (GraphRfi  and LLM4Dec ), adversarial collaborative filtering methods (APR  and SharpCF ), and a denoise-based strategy (StDenoise ). In our study, we employ three common backbone recommendation models, Matrix Factorization (MF) , LightGCN , and NeurMF .

**Evaluation Metrics**. The primary metrics for assessing recommendation performance are the top-\(k\) metrics: \(@k\) and \(@k\), as documented in . To quantify the success ratio of attacks, we utilize \(@k\) and \(@k\) to measure the performance of target items within the top-\(k\) recommendations , as:

\[@k=|}_{tar }_{tar}}(tar L _{u,1:k})}{|_{tar}|},\] (7)

where \(\) is the set of target items, \(_{tar}\) denotes the set of genuine users who have interacted with target items \(tar\), \(L_{u,1:k}\) represents the top-\(k\) list of recommendations for user \(u\), and \([)\) is the indicator function that returns 1 if the condition is true. The \(@k\) mirrors \(@k\), serving as the target item-specific version of \(@k\).

### Performance Comparison (RQ1)

In this section, we answer **RQ1**. We focus on two key aspects: the recommendation performance and the robustness against poisoning attacks.

[MISSING_PAGE_FAIL:8]

Detection-based methods, such as GraphRfi and LLM4Dec, show robust defense against attacks similar to their training data, i.e., random attacks. However, the effectiveness of GraphRfi declines against other attack types. In contrast, ACF methods demonstrate stable defense capabilities across various attacks. Specifically, PamaCF significantly reduces the success ratio of attacks, decreasing \(\)-\(@50\) and \(\)-\(@50\) by 49.92% and 43.73% in average, respectively, compared to the best baseline. These results highlight PamaCF's advanced defense capabilities against various attacks.

Additionally, PamaCF's defense effectiveness against attacks targeting popular items is further evaluated. The corresponding results for LightGCN  and NeuMF , along with the recommendation performance at top-10, are also presented. All supplementary results are in Appendix C.2.

### Augmentation Analysis (RQ2)

In this section, we address **RQ2** by exploring why PamaCF can outperform traditional ACF (especially APR ) through embedding visualization and perturbation magnitude comparison.

**Embedding Visualization**. We randomly select a user and project the normalized embeddings of the user, real preference items, the target item given by attacks, and other items in the user's top-10 recommendation list into a two-dimensional space using T-SNE , as shown in Figure 2(a). We observe that PamaCF can bring real preference items closer, reducing the distance from the farthest real preference item from 0.736 to 0.365, while leading the target item farther away from all the real preference items. PamaCF's personalized perturbation magnitude lowers the ranking of both the target item and other items, thus improving robustness and performance.

**Perturbation Magnitude Comparison**. We compare the maximum perturbation magnitudes of APR and PamaCF, i.e., \(\) in Equation 1 for APR and \(\) in Equation 6 for PamaCF. Both \(\) and \(\) are selected through hyper-parameter tuning from \(\{0.1,0.2,,1.0\}\). In the left part of Figure 2(b), we observe that PamaCF finds a higher perturbation magnitude. Additionally, the right portion of Figure 2(b) illustrates the distribution of personalized perturbation magnitudes across all users. These varying magnitudes for different users contribute to the improved effectiveness of PamaCF.

### Hyper-Parameters Analysis (RQ3)

In this section, we answer **RQ3** by exploring the effects of the hyperparameters, magnitude \(\) and adversarial training weight \(\), as defined in Equation 6. The results are illustrated in Figure 3.

**Analysis of Hyper-Parameters \(\)**. With \(\) fixed at 1.0, we vary \(\) from 0.1 to 1.0 in increments of 0.1. Our findings demonstrate a significant improvement in both robustness and performance as \(\) increases. Notably, even when \(\) exceeds 0.1, there is an enhancement in recommendation

Figure 3: Left: Analysis of Hyper-Parameters \(\); Right: Analysis of Hyper-Parameters \(\).

Figure 2: (a) PamaCF brings real preference items closer; (b) PamaCF achieves larger magnitudes.

performance compared to that of the backbone model, i.e., MF, with the range between 0.7 and 0.9 yielding the most significant enhancements.

**Analysis of Hyper-Parameters \(\).** With \(\) set at 0.9, we adjust \(\) from 0.2 to 2.0 in increments of 0.2. The analysis indicates that the defensive ability becomes stable once \(\) surpasses 1.0 in most attacks. However, setting \(\) too high gradually diminishes the recommendation performance of PamaCF. Despite this, the performance of PamaCF remains considerably improved compared to MF.

## 6 Conclusion

In this work, we theoretically analyze why Adversarial Collaborative Filtering (ACF) enhances both the performance and robustness of Collaborative Filtering (CF) systems against poisoning attacks. Additionally, by establishing bounds for reductions in recommendation error during ACF's optimization process, we discover that applying personalized perturbation magnitudes for users based on their embedding scales can significantly improve ACF's effectiveness. Leveraging these theoretical understandings, we introduce Personalized Magnitude Adversarial Collaborative Filtering (PamaCF). Comprehensive experiments confirm that PamaCF effectively defends against various attacks and significantly enhances the quality of recommendations.

**Limitations**. Our study identifies several limitations that require further investigation. Firstly, our theoretical analysis is based on certain assumptions, specifically with the Gaussian Recommender System. We intend to relax these assumptions in future work. Secondly, this study only examines adversarial training within CF recommendations. In future research, we plan to extend our analysis to include more recommendation scenarios, such as sequential recommendations.

**Broader Impacts**. Our work focuses on enhancing both the performance and robustness of recommender systems against poisoning attacks, thereby benefiting the overall development of recommender systems. We do not foresee any negative impacts resulting from our work.