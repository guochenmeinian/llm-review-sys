# Understanding the Transferability of Representations via Task-Relatedness

Akshay Mehra, Yunbei Zhang, and Jihun Hamm

Tulane University

{amehra, yzhang111, jhamm3}@tulane.edu

###### Abstract

The growing popularity of transfer learning, due to the availability of models pre-trained on vast amounts of data, makes it imperative to understand when the knowledge of these pre-trained models can be transferred to obtain high-performing models on downstream target tasks. However, the exact conditions under which transfer learning succeeds in a cross-domain cross-task setting are still poorly understood. To bridge this gap, we propose a novel analysis that analyzes the transferability of the representations of pre-trained models to downstream tasks in terms of their relatedness to a given reference task. Our analysis leads to an upper bound on transferability in terms of task-relatedness, quantified using the difference between the class priors, label sets, and features of the two tasks. Our experiments using state-of-the-art pre-trained models show the effectiveness of task-relatedness in explaining transferability on various vision and language tasks. The efficient computability of task-relatedness even without labels of the target task and its high correlation with the model's accuracy after end-to-end fine-tuning on the target task makes it a useful metric for transferability estimation. Our empirical results of using task-relatedness on the problem of selecting the best pre-trained model from a model zoo for a target task highlight its utility for practical problems.

## 1 Introduction

Transfer learning (TL)  is a powerful tool for developing high-performance machine learning models, especially in current times when large models  pre-trained on huge amounts of data are being fine-tuned for various downstream tasks. While large pre-trained models achieve impressive performance on downstream tasks even in the zero-shot inference setting , their performance can often be improved by fine-tuning them on data from target tasks. However, our understanding of when representations from these models lead to classifiers that achieve high performance (i.e., high transferability) to downstream tasks is still lacking.

Analytical works based on domain adaptation  can only explain cross-domain tasks (i.e., when only features/label priors change across tasks) but in the TL setting, label sets can also change (i.e., cross-task setting). Recently,  showed that the relatedness between the label sets of the two tasks measured using conditional entropy can explain the difference in their transferability. However,  focused only on the cross-task setting, and analysis for transferability in a general cross-domain cross-task setting is not addressed. Apart from these analytical works, another line of work focuses on proposing transferability metrics that correlate well with performance on downstream tasks after end-to-end fine-tuning. We refer to these works as score-based transferability estimation (SbTE) metrics . These works focus on developing scores for selecting a pre-trained model from a model zoo, that achieves the best transferability on a target task. While these works address a practical problem, they do not focus on providing an analysis of transferability.

Thus, we first rigorously analyze the transferability of the representations in producing high-performing classifiers and propose a novel approach that studies transferability in terms of its relatedness to a reference task (see Fig. 1). This is in line with previous analytical works  which studied the model's performance on target tasks in terms of the source task in different settings such as domain adaptation/generalization and recently TL. However, there's a crucial difference: we study transferability in terms of a reference task instead of the source task since it is impractical to assume the knowledge of the source task used to train large models such as CLIP  or GPT, commonly used for TL.

Our approach works by transforming the distribution (and classifier) of a reference task, by transforming its class-prior distribution, label set, and feature space to obtain a new distribution that is similar to that of the target task (Fig. 2). Based on these transformations, we show that transferability can be provably explained (and is tightly upper bounded) using three interpretable terms. A weighted reference loss term appearing due to the class prior distribution difference between the tasks, a label mismatch term appearing as conditional entropy between the label distributions of the tasks, and a distribution mismatch term appearing as the Wasserstein distance between the transformed reference and target distributions (Theorem 3). We define task-relatedness as the sum of these three terms (a smaller value implies higher relatedness). We propose an optimization problem (Eq. 4) and an algorithm (Alg. 1) to learn the transformations to compute it. Using state-of-the-art (SOTA) pre-trained models, with different architectures, trained with various training methods on computer vision (CV) and natural language processing (NLP) tasks, we show that task-relatedness achieves a small gap to transferability (Sec. 4.1). Our analysis also leads to new insights into learning in the TL setting such as to improve the transferability of an encoder on a downstream task, one can improve the encoder's transferability on related reference tasks (Sec. 4.2). This is particularly useful when practitioners intend to develop encoders that achieve high transferability to proprietary (and potentially inaccessible) datasets.

We also demonstrate the utility of task-relatedness in estimating the accuracy of the model after end-to-end fine-tuning. While the TL setting assumes access to target labels, the high computational cost of end-to-end fine-tuning of a pre-trained model on a target task calls for developing metrics that are efficiently computable and highly correlated with end-to-end fine-tuning accuracy. To this end, we propose to use task-relatedness computed in the penultimate layer of the pre-trained model as our transferability estimation metric. To further improve the computational efficiency of task-relatedness we only measure the difference between the class-wise means and covariances of the distributions in lieu of the Wasserstein distance as required in Theorem 3. This enables the computation of task-relatedness with only the statistics of the reference/target tasks. Our empirical results (Sec. 4.3) attest that task-relatedness achieves a high correlation with the model's accuracy after end-to-end fine-tuning on the target task making it an effective metric for selecting a pre-trained model from a model zoo that achieves the best accuracy on the target task. Moreover, unlike previous SbTE metrics, task-relatedness can be estimated even without labeled target data, making it suitable for unsupervised transferability estimation, highlighting the advantage of a reference task as used in our analysis. Our main contributions are:

* We rigorously analyze transferability for classification tasks. Our analysis, to the best of our knowledge, leads to the first upper bound on transferability in terms of task-relatedness in a cross-domain cross-task setting.
* We propose an optimization problem to efficiently compute task-relatedness, using a small amount of target labels and show that it can even predict performance after end-to-end fine-tuning without requiring target labels.

Figure 1: Given a pre-trained encoder (e.g., CLIP ), how does the performance after fine-tuning it on a reference task (e.g., ImageNet) relate to the performance after fine-tuning it on other tasks? Through a rigorous bound on transferability (Theorem 3) in terms of the relatedness between a reference and a target task, we show that tasks related to the reference task achieve provably better performance after fine-tuning.

* Using SOTA models and CV/NLP tasks, we show that task-relatedness accurately predicts transferability and show that transferability to unseen tasks can be improved by improving transferability to known (related) tasks.

## 2 Related Work

**Transfer learning (TL):** TL [42; 59; 18; 49; 46; 15; 14] has been studied widely and consists of various settings including transductive transfer, inductive transfer, and task transfer learning. The transductive setting also referred to as domain adaptation [8; 7] focuses on reducing the shift between two domains. The task transfer setting focuses on identifying the relationship between tasks, regardless of the model, to explain the transfer performance (see Appendix B for more details). Lastly, the inductive transfer setting focuses on using an inductive bias such as fine-tuning a pre-trained model (trained via adversarial training , self-supervised learning [11; 10; 12] or by combining language and image information ) to improve the performance on a target task. Our work focuses on the inductive transfer learning setting and proposes an upper bound on transferability of the representations of pre-trained models to downstream tasks.

**Analytical works for learning under distribution shifts:** Prior works [8; 7; 52; 36; 32; 37; 38] analytically explained learning under distribution shifts using distributional divergence between the marginal distributions and a label mismatch term. However, these results are applicable under assumptions such as covariate or label shift which need not be satisfied in TL where both the data distribution and the label spaces can be different (see App. B for detailed comparison). Recently,  proposed an upper bound on transferability in a restrictive setting of the same features for both tasks, however, our analysis does not require such an assumption. Other works [9; 47; 41] analyzed the representation for the multi-task learning setting. These works showed that when tasks are weakly related, a single representation space (model) may not perform well for all tasks. However, the TL setting differs from both of these and our work aims to analyze transferability in this setting.

**Score-based transferability estimation (SbTE):** These works [5; 40; 29; 61; 55; 39] use data from the target task and produce a score correlated with transferability. Such a score is useful for selecting the model from a model zoo that leads to the best transferability to a target task.  proposed the Negative Conditional Entropy (NCE) score that predicts transferability using the negative conditional entropy between labels of the tasks but requires the two tasks to have the same input instances.  estimates transferability by solving the HGR maximum correlation problem and using normalized Hscore, in the same setting as .  proposed the LEEP score and computed NCE using soft (pseudo) labels for the target task from a pre-trained model. OT-CE  combined Wasserstein distance  and NCE whereas [5; 61] estimate likelihood and the marginalized likelihood of labeled target examples to estimate transferability.  proposes a model-agnostic approach that also relies on optimal transport to compute the distance between the tasks similar to OTDD . In contrast, we focus on analyzing transferability in terms of task-relatedness theoretically along with demonstrating its effectiveness as a transferability estimation metric for the pre-trained model selection problem.

## 3 Analysis of TL using task-relatedness

**Problem setting and notations:** Let \(P_{R}(x,y)\) and \(P_{T}(x,y)\) denote the distributions of the reference and the target tasks, defined on \(_{R}_{R}\) and \(_{T}_{T}\) respectively. We assume that the feature spaces are common (\(_{R}=_{T}=\)) such as RGB images, but the reference label set \(_{R}=\{1,2,,K_{R}\}\) and the target label set \(_{T}=\{1,2,,K_{T}\}\) can be entirely different. We assume the number of reference task classes (\(K_{R}\)) are greater than or equal to the number of target classes (\(K_{T}\)). In the TL setting, an encoder (feature extractor) \(g:\) is pre-trained on a dataset with or without labels depending on the training method (e.g., supervised vs. self-supervised). We denote the resultant push-forward distributions of \(R\) and \(T\) on the encoder output space as \(P_{R}(z,y)\) and \(P_{T}(z,y)\). With a fixed encoder \(g\), a classifier (linear or non-linear), \(h(z):\), that outputs a probability vector is learned for the reference (\(h_{R}\)) and the target (\(h_{T}\)) separately, where \(_{R/T}\) is a \(K_{R}/K_{T}\) simplex for \(R/T\). The classifier \(h_{R}=_{h}_{(z,y) P_{R}}[(h(z;g),y)]\) and \(h_{T}=_{h}_{(z,y) P_{T}}[(h(z;g),y)]\) where \(\) is the set of classifiers and \((h(z),y)=-(h(z)_{y})\) is the cross-entropy loss. Table 3 in App. A summarizes the notations used in our work. Next, we define transferability as commonly used in the literature.

**Definition 1**.: (Transferability). Transferability of the representations from an encoder \(g\) on a target task \(T\) for classifiers in \(\) is defined as \(_{(z,y) P_{T}}[(h_{T}(z;g),y)]\).

In the next section, we show the analysis with \(\) as the class of linear classifiers for ease of explanation and discuss its extension to non-linear classifiers in App. A.5. Proofs for Sec. 3 are in App. A.

### Our task transformation model

The reference and the target tasks share the same encoder but do not share label sets or data distributions. Therefore, to relate the two tasks, we propose a chain of three simple transformations: 1) prior transformation (from \(R\) to \(R^{}\)), 2) label transformation (from \(R^{}\) to \(R^{}\)), and 3) feature transformation (from \(R^{}\) to \(R^{}\)). The \(R^{},R^{},R^{}\) are intermediate domain names after each of the transformations are applied. The corresponding classifier in each domain is denoted by \(h_{R^{}}\), \(h_{R^{}}\), and \(h_{R^{}}\) as illustrated in Fig. 2. The distribution after the transformations (\(P_{R^{}}\)) has the same feature \(_{R^{}}=_{T}=\) and label sets \(_{R^{}}=_{T}\) as the target task \(T\), and consequently, the loss of the transformed classifier \(h_{R^{}}\) and the target classifier \(h_{T}\) can be related.

**Class-prior transformation \((R R^{})\):** Since the reference task has more classes than the target task (\(K_{R} K_{T}\)), many of the reference task classes are likely irrelevant for transfer to the target classes, e.g., while transferring from ImageNet to CIFAR10, only a small portion of ImageNet classes are relevant to CIFAR10 classes. The prior transformation accounts for the relative importance of the reference classes. This is illustrated in Fig. 2, where changing the class prior of \(R\) reduces the prior of the Bee class and increases the priors of Wolf and Lion classes (shown by the changed size of classes Wolf and Lion in \(R^{}\)). While transforming the prior of \(R\), we keep the conditional distribution and the classifier the same i.e., \(P_{R^{}}(z|y)=P_{R}(z|y)\) and \(h_{R^{}}(z)=h_{R}(z)\). Lemma 1 in App. A.2.1 shows that the expected loss of the classifier \(h_{R}\) on \(R^{}\) is a re-weighted version of the loss of \(h_{R}\) on \(R\).

**Label transformation \((R^{} R^{})\):** Next, we use a label transformation to match the label sets of the new domain \(R^{}\) and that of the target domain. To this end, we specify the conditional distribution \(B_{ij}:=P(y_{R^{}}=i|y_{R^{}}=j)\) (\(B_{ij},~{}~{}i,j,~{}~{}_{i}B_{ij}=1,~{} j\)). The label \(y_{R^{}}\) of an example from the domain \(R^{}\) is obtained via \(BP(y_{R^{}})\). This generative process doesn't require the feature, i.e., \(P_{R^{}}(y_{R^{}}|y_{R^{}},z)=P_{R^{ }}(y_{R^{}}|y_{R^{}})\). \(B\) with sparse entries (i.e., only one entry of a column is 1) models a deterministic map from \(_{R}\) to \(_{T}\); \(B\) with dense entries models a weaker association. This process is illustrated in Fig. 2 which shows the map from {Bee, Wolf, Lion} \(_{R^{}}\) to {Dog, Cat} \(_{T}\) after using \(B\). Under this model, a reasonable choice of classifier for \(R^{}\) is \(h_{R^{}}(z)=Bh_{R^{}}(z)\). Lemma 2 in App. A.2.2 shows that the expected loss of \(h_{R^{}}\) depends on the loss of \(h_{R^{}}\) and the conditional entropy between the label sets of the tasks \(R^{}\) and \(R^{}\) and Corollary 1 shows the conditions for optimality of \(h_{R^{}}\).

**Feature transformation (\(R^{} R^{}\)):** The final step involves changing the feature space of the distribution \(R^{}\). We apply an invertible linear transformation \(A\) to the distribution in \(R^{}\) to obtain the new distribution \(R^{}\). After the transformation, the classifier associated with the new domain \(R^{}\) is \(h_{R^{}}(z)=h_{R^{}}(A^{-1}(z))\). This is illustrated in Fig. 2 after feature transform using \(A\). Lemma 3 in

Figure 2: **: Overview of our task transformation model:** A series of transformations are applied to the reference distribution \(P_{R}(z,y)\) and classifier \(h_{R}\) to produce the transformed distribution \(P_{R^{}}\) and classifier \(h_{R^{}}\) to explain transferability to the downstream target task. Class-prior transformation (\(R R^{}\)) changes the class prior of the reference distribution (e.g., an irrelevant Bee class in \(R\) now has smaller prior) followed by label set transformation (\(R^{} R^{}\)) (e.g., to match {Lion, Wolf} with {Cat, Dog}), followed by feature space transformation (\(R^{} R^{}\)) to match the feature distribution of the target task \(P_{T}(z,y)\).

App. A.2.3 shows that a linear transform of the space and classifier does not incur any additional loss and Corollary 2 shows that the optimality of \(h_{R^{}}\) implies optimality of \(h_{R^{}}\). Using these, we get Theorem 1 by defining conditional entropy as follows

\[H(_{R^{}}|_{R^{}})=-_{y_{R^{ }}_{R^{}}}_{y_{R^{}}_{R^{ }}}P_{R^{}}(y_{R^{}})B_{y_{R^{}},y_{R^{ }}}(B_{y_{R^{}},y_{R^{}}}). \]

**Theorem 1**.: _Let \(C:=[}(y)}{P_{R^{}}(y)}]_{y=1}^{K_{R}}\) be a vector of probability ratios, \(B\) be a \(K_{T} K_{R}\) matrix with \(B_{ij}=P(y_{R^{}}=i|y_{R^{}}=j)\), \(A:\) be an invertible linear map of features. Let the classifiers \(h_{R^{}}(z):=h_{R}(z)\), \(h_{R^{}}(z):=Bh_{R^{}}(z)\), \(h_{R^{}}(z):=h_{R^{}}(A^{-1}(z))\). Assuming \(\) is the cross-entropy loss, we have_

\[_{P_{R^{}}(z,y)}[(h_{R^{}}(z ),y)]_{P_{R}(z,y)}[C(y)(h_{R}(z),y)]}_{}$}}+_{R^{ }}|_{R^{}})}_{}$}}.\]

Theorem 1 provides an upper bound on the loss of the final transformed classifier/distribution in terms of the loss of the reference classifier/distribution. The _re-weighted reference loss_ shows that the performance of the transformed classifier on the new domain is linked to the label-wise re-weighted loss of the reference classifier on \(R\). This implies that one can use only the relevant reference classes to contribute to the bound. The _label mismatch_ term shows that the performance of the distribution \(R^{}\) and \(R\) depends on the conditional entropy \(H(_{R^{}}|_{R^{}};B)\) between the label distributions of the domain \(R^{}\) and \(R^{}\). A high value of \(H\) implies that the labels of the reference task are unrelated leading to lower transferability, whereas a low \(H\) implies higher transferability. Corollary 3 in App. A.2.4 shows when the bound in Theorem 1 becomes equality.

### Distribution mismatch between \(P_{R^{}}\) and \(P_{T}\)

After the three transformations, the transformed reference \(P_{R^{}}(z,y)\) can be compared with the target \(P_{T}(z,y)\). However, these are only simple transformations and \(P_{R^{}}\) cannot be made identical to \(P_{T}\) in general. This mismatch can be measured by the Wasserstein or Optimal Transport distance . Since our goal is to match two joint distributions defined on \(\) we use

\[d((z,y),(z^{},y^{})):=\|z-z^{}\|_{2}+ 1_{y y^{ }}, \]

with \(z,z^{}\) and \(y,y^{}\) as our base distance  to define the (type-1) Wasserstein distance

\[W_{d}(P,Q):=_{(P,Q)}_{((z,y),(z^{},y^{})) }[d((z,y),(z^{},y^{}))]. \]

Using Eq. 2, the Wasserstein distance between the joint distributions is the weighted sum of the Wasserstein distance between conditional distributions (\(P(z|y)\)) (Lemma 4 in App. A). Theorem 2 below explains the gap between the losses due to the distribution mismatch.

**Assumption 1**.: 1) The composition of the loss function and the classifier \( h\) is a \(-\)Lipschitz function w.r.t to \(\|\|_{2}\) norm, i.e., \(|(h(z),y)-(h(z^{}),y)|\|z-z^{}\|_{2}\) for all \(y\), \(z,z^{}\) where \(h\). 2) \(P_{T}(y)=P_{R^{}}(y)\).

The assumption 2), can be satisfied since we have full control on the prior \(P_{R^{}}(y)\) via \(B\) and \(C\).

**Theorem 2**.: _Let the distributions \(T\) and \(R^{}\) be defined on the same domain \(\) and assumption 1 holds, then_

\[_{P_{T}(z,y)}[(h(z),y)]-_{P_{R^{}}(z, y)}[(h(z),y)](P_{R^{}},P_{T})}_{ $}}},\]

_with \(d\) as in Eq. 2._

Theorem 2 shows that when \( h\) is \(-\)Lipschitz then the performance gap between the \(R^{}\) and \(T\) is bounded by the type-1 Wasserstein distance between the two distributions. The Lipschitz coefficient of the composition can be bounded by \(\), by penalizing the gradient norm w.r.t \(z\) at training time. Thus, for linear fine-tuning, we train the classifiers \(h_{R}\) and \(h_{T}\) with an additional gradient norm penalty \(\{0,\|_{z}(h(z),y)\|_{2}-\}\) to make them conform to the Lipschitz assumption (see App. C.3). Note that constraining the Lipschitz constant restricts the hypothesis class. The trade-off between the Lipschitz constant and the performance of \(h\) is empirically evaluated in App. C.3.1.

[MISSING_PAGE_FAIL:6]

the reference task has classes semantically related to the target task, Alg. 1 learns transformations that achieve the smallest gap to transferability. However, since finding data semantically related to the target task may not always be possible we choose a reference task with the same number of classes as the target and fix that matrix \(B\) to a random permutation of identity (making the label mismatch term zero) and \(D\) to the prior of the reference task, learning only the transformation \(A\), in our experiments.

```
1:Randomly sample \(n_{R}\) points \((z^{i}_{R},y^{i}_{R})(Z_{R},Y_{R})\) as per the class prior \(D\).
2:if\(Y_{T}\) is available then
3: Randomly sample \(n_{T}\) points \((z^{j}_{T},y^{j}_{T})(Z_{T},Y_{T})\).
4:else
5: Randomly sample \(n_{T}\) points \((z^{j}_{T})(Z_{T})\).
6:# Compute pseudo-labels for the target samples \(z_{T}\).
7:\(y^{j}_{T}=_{y_{T}}Bh_{R}(A^{-1}z_{T})\) for \(j=1,,n_{T}\).
8:endif
9:Compute \((z^{i}_{R^{}},y^{i}_{R^{}})=(Az^{i}_{R}, \ _{y}Be(y^{i}_{R}))\), for \(i=1,,n_{R}\).
10: Assign \(_{R^{}}:=_{R}\) and \(_{R^{}}:=_{T}\).
11:Compute the optimal coupling \(^{*}\) between the distributions \(R^{}\) and \(T\) by minimizing \(W_{d}(P_{R^{}},P_{T})\), i.e., \[_{(P_{R^{}},P_{T})} _{i,j}_{ij}((z^{i}_{R^{}},y^{i} _{R^{}}),(z^{j}_{T},y^{j}_{T}))\] \[ _{j}_{ij}=}\  i,\ _{i}_{ij}= }\  j.\]
12: Using \(^{*}\), solve for \(A,,B,D\) using mini-batch SGD \[_{A,,B,D} _{i,j}^{*}_{i,j}((z^{i}_{R^{ }},y^{i}_{R^{}}),(z^{j}_{T},y^{j}_{T}))\] \[+ }_{i})}{P_{R}(y^{i})}(h_{R}(z^ {i}_{R}),y^{i})+H(_{R^{}}|_{R^{}})\] \[+ \|P_{T}(y)-BD\|_{2}^{2}+(\|A-I\|_{F}+\|A-I\|_{F}).\]
13: Repeat 1 - 12 until convergence.
```

**Algorithm 1** Minimization of the bound in Theorem 3

## 4 Empirical Analysis

Here, we empirically demonstrate the effectiveness of task-relatedness in explaining transferability in various settings. We present additional results in App. C and dataset/experimental details in App. D. Our codes can be found at [https://github.com/akshaymehra24/TaskTransferAnalysis](https://github.com/akshaymehra24/TaskTransferAnalysis).

### Task-relatedness achieves a small gap to actual transferability

_Task-relatedness tightly upper bounds transferability across various architectures, pretraining methods, and datasets._ We demonstrate this by using various pre-trained models with architectures such as Vision Transformers (ViT) , ResNet-18/50/101/152 , DistilRoBERTa  trained with various pretraining methods including supervised training, adversarial training , SimCLR , MoCo , SwAV , and MAE . We also consider a wide range of target datasets including, CIFAR10/100, Aircraft, Pets, DTD, AG-News, Yelp-5, and SST-5 whose details are in App. D.

For this experiment, we fix the reference task to be ImageNet  for image classification and to DBPedia for sentence classification tasks and use Alg. 1 to estimate task-relatedness. The results in Fig. 3 and 9 (in the Appendix) show that our bound achieves a small gap to actual transferability. As the task-relatedness between the reference and the target tasks improves, transferability also improves showing that task-relatedness and transferability are strongly correlated. _Task-relatedness is also strongly correlated with the accuracy of the end-to-end fine-tuned classifiers on the target task_. In Fig. 11 (in the Appendix), we show high Pearson correlation coefficients (\(-0.57\)) for task-relatedness and accuracy after fully fine-tuning various pre-trained encoders using data from various target tasks.

### Effect of the reference task on task-relatedness

_Highly related reference-target task pairs, based on task-relatedness, achieve higher transferability coinciding with the semantic relatedness between tasks._ To understand how a reference task affects task-relatedness and eventually transferability, we consider two experiments using convolutional and CLIP-trained models with various character recognition tasks such as MNIST, Fashion-MNIST (FMNIST), SVHN, MNIST-M, and USPS. Of these datasets, SVHN and MNIST-M contain colored images while the rest contain gray-scale images. In the first experiment, we train convolutional models on MNIST, FMNIST, and USPS and measure pairwise transferability. Here we use the reference task to be the same task as that used for training the models. The results in Fig. 4(a) show that transferability to those target tasks is higher for which task-relatedness metric's value is smaller. Specifically, USPS achieves the best transferability (\(1.23\)) and the smallest task-relatedness (\(2.80\)) when the reference task is MNIST. This is attributed to both datasets containing gray-scale images of digits. On the other hand, when the reference task is unrelated to the target task i.e., the task-relatedness value is high, transferability suffers, e.g., when the reference task is MNIST and the target task is FMNIST. Results in App. C.2.2 show similar results for the sentence classification task.

_The gap between task-relatedness and transferability is smaller when a reference task performs well with a given encoder._ Here we use MNIST and SVHN as two reference tasks and compute the task-relatedness and transferability with USPS and MNIST-M as target tasks, using CLIP (Vit B32) model. A linear classifier trained on top of the embeddings from the CLIP model achieves \(\)98% accuracy on MNIST but only \(\)61% accuracy for SVHN. Due to this, transferability (USPS:2.02, MNIST-M:2.20) explained using task-relatedness with MNIST as the reference task (USPS:**2.46**, MNIST-M:**2.48**) is better than that computed using SVHN (USPS:2.66, MNIST-M:2.65) as the reference, even though MNIST-M is intuitively more similar to SVHN (as both contain colored images of digits). This is evident from the results of PE (Pre-trained Encoder) in Fig. 4(b).

_Improving the performance of an encoder on a reference task improves transferability to other related (potentially unseen) tasks._ To show this we fully fine-tune the CLIP encoder on MNIST and SVHN tasks, increasing the accuracy of the classifiers for both MNIST and SVHN to 99% and 95%, respectively. Using the representations from these new encoders, we find that the transferability of both related target tasks improves along with task-relatedness (see FFE results in Fig. 4(b)). Here,

Figure 4: (a) Task-relatedness and transferability are highly correlated across various reference-target pairs. (b) Improving the transferability of an encoder on a reference task (in the plot title) leads to improved transferability of all related target tasks (x-axis). (e.g., compared to the original pre-trained CLIP encoder (PE), a end-to-end fine-tuned CLIP encoder (FFE) on the reference task achieves higher transferability to all related tasks.)

we see that task-relatedness for MNIST-M and USPS is the best when the reference task is SVHN and MNIST, respectively, aligning with our intuition of semantic relatedness between these tasks. This also suggests that transferability on other related tasks can be improved by fully fine-tuning the encoder on these reference tasks. Thus, in scenarios where target tasks are private (such as proprietary Chest X-rays), an encoder trained to work well on related tasks (such as publicly available Chest X-rays) is bound to achieve good transferability.

### Task-relatedness for end-to-end transferability estimation

In this section, we show an efficient way of computing task-relatedness which enables its use for estimating transferability after end-to-end fine-tuning. While Alg. 1, accurately estimates task-relatedness by minimizing the bound in Eq. 3, it could be inefficient due to the requirement of computing and minimizing the Wasserstein distance between distributions at every epoch. Thus, to make the computation efficient, we replace the Wasserstein distance computation in step 11 and 12 of Alg. 1, with mean and covariance matching terms. Specifically, we define the distance between two distributions \(R^{}\) and \(T\) as

\[(R^{},T):=\|_{R^{}}-_{T}\|_{2} ^{2}+\ \|_{R^{}}-_{T}\|_{2}^{2}, \]

where \(_{R^{}/T}:=/T}}_{z  P_{R^{}/T}}z,_{R^{}/T}:=/T}}_{z P_{R^{}/T}}(z-_{R^{ }/T})^{T}(z-_{R^{}/T})\), and \(\) is a regularization coefficient. Using \((R^{},T)\) in place of \(W_{d}(R^{},T)\), makes the computation of task-relatedness by learning transformations \(A,B,\) and \(C\) significantly more efficient.

_Task-relatedness is an effective metric for the pre-trained model selection problem._ The goal of this problem is to find a pre-trained model from a model zoo that achieves the best accuracy on a given target task after end-to-end fine-tuning of the model using labeled target data. Since end-to-end fine-tuning is costly (takes almost a day to fully fine-tune a single model on a single target task as shown by ), an effective transferability metric is significantly more efficient to compute and is correlated well with the accuracy after end-to-end finetuning. Using 5 different pre-trained models (supervised ResNet-50/101/152, adversarially pre-trained  ResNet50 with \(\{0.1,1\}\)) and ImageNet as the reference task, we show in Table 1, that task-relatedness achieves a high correlation with the accuracy after end-to-end fine-tuning on the target task. Our results also highlight the instability of various popular SbTE metrics, such as LogMe  and NCE  which can produce a high negative correlation, and those of PACTran  which achieve low correlation values on complex datasets. In comparison, task-relatedness consistently achieves a good correlation for various target tasks. Computationally, it takes a mere 3-4 minutes to learn the transformations to compute task-relatedness, providing a significant computation advantage over end-to-end fine-tuning. We also show that _task-relatedness remains highly correlated with end-to-end fine-tuning accuracy even with a limited amount of labeled data from the target task_ as shown in Fig. 5 unlike other SbTE metrics.

Next, we show that _task-relatedness can even be estimated without using labels from the target task._ For scenarios, where labeled data from the target task is unavailable, estimating transferability is challenging. This is because both fine-tuning and most SbTE methods require labels to compute the transferability scores. Here we show that task-relatedness can still be an effective measure to estimate transferability in this challenging setting. Since we use a transformative model and have access to a reference task/classifier, we can use the predictions from the reference task's classifier trans

  Target task & LogMe & Deep & NCE & PACTran & SFDA & H-Score & OT-NCE & OTCE & Ours \\  Pets & 0.82 & 0.80 & 0.73 & -0.82 & 0.57 & 0.77 & 0.88 & 0.86 & -0.77 \\ DTD & 0.88 & 0.96 & -0.19 & -0.85 & 0.90 & 0.89 & 0.84 & 0.82 & -0.97 \\ Aircraft & -0.60 & 0.92 & 0.97 & 0.11 & 0.72 & -0.80 & 0.56 & 0.60 & -0.72 \\ 
**Average** & 0.37 & 0.90 & 0.50 & -0.52 & 0.73 & 0.29 & 0.76 & 0.76 & -0.82 \\  

Table 1: Task-relatedness achieves high (negative) Pearson correlation to the accuracy after end-to-end fine-tuning for various tasks. For NCE , Leep , LogMe , SFDA , OT-NCE, OTCE , and H-score  **positive** correlation is better whereas for PACTran  and task-relatedness (ours) **negative** correlation is better.

formed via \(B\) (to obtain labels \(_{T}\)) and estimate the _pseudo_-labels of the target data. Concretely, pseudo-label for a target sample \(x_{T}\) is obtained as \(y_{T}^{pseudo}=_{y_{T}}Bh_{R}(A^{-1}(z_{T}))\). Results in Table 2 show that our task-relatedness estimated via pseudo-labeled target data still achieves a high correlation to transferability on most datasets. For datasets such as Pets and DTD, where transforming the reference task classifier produces high accuracy on the target task, the difference between the pseudo and true labels is small. Consequently, the difference in the correlations with pseudo and true labels is also small. Thus, when the reference and target tasks are related, transferability can be estimated accurately without requiring labels of the target task, showing that task-relatedness is an effective metric even for unsupervised transferability estimation.

## 5 Conclusion

We analyzed TL in terms of the relatedness between the target and a reference task. Our analysis works by transforming the distribution of a reference task to match that of the target. Using this we proved an upper bound on transferability, defined as task-relatedness, consisting of three interpretable terms, namely, the re-weighted reference task loss, label mismatch, and distribution mismatch. We proposed an algorithm to compute task-relatedness and demonstrated its effectiveness at accurately predicting transferability (even without target labels) with SOTA models. Moreover, the high correlation of task-relatedness with accuracy after end-to-end fine-tuning and its efficient computability, makes it an effective metric for transferability estimation.

**Limitations.** We studied transferability using the cross-entropy loss and used Wasserstein distance-based distribution shift analysis due to their popularity. However, due to accuracy being the primary metric of interest in classification tasks and the difficulty of computing the Wasserstein distance with limited samples in a high dimensional representation space, extending the analysis to 0-1 loss and other divergence measures are important directions which are not addressed here and are left for future works.

## 6 Acknowledgment

We thank the anonymous reviewers of this work for their insightful comments and suggestions. This work was supported by the NSF EPSCoR-Louisiana Materials Design Alliance (LAMDA) program #OIA-1946231.

Figure 5: Task-relatedness (Ours) remains highly correlated with accuracy after end-to-end fine-tuning on a target task even when using a small percentage of target data unlike other SbTE methods (LogME, Leep, NCE, PACTran, OT-NCE, OTCE, and H-Score) whose correlation is affected significantly. For LogMe, Leep, NCE, OT-NCE, OTCE, and H-score **positive** correlation is better whereas for PACTran and task-relatedness (ours) **negative** correlation is better.