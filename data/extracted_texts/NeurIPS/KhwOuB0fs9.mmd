# Effi-Learner: Enhancing Efficiency of

Generated Code via Self-Optimization

 Dong Huang

The University of Hong Kong

dhuang@cs.hku.hk

&Jianbo Dai

University of Edinburgh

j6dj6d@gmail.com

&Han Weng

Beijing University of

Posts and Telecommunications

han.weng@bupt.edu.cn

&Puzhen Wu

University College Dublin

puzhen.wu@ucdconnect.ie

&Yuhao Qing

The University of Hong Kong

yhqing@cs.hku.hk

&Heming Cui

The University of Hong Kong

Shanghai AI Laboratory

heming@cs.hku.hk

&Zhijiang Guo

University of Cambridge

zg283@cam.ac.uk

&Jie M. Zhang

King's College London

jie.zhang@kcl.ac.uk

Equal Contribution.Corresponding Author.

###### Abstract

Large language models (LLMs) have shown remarkable progress in code generation, but their generated code often suffers from inefficiency, resulting in longer execution times and higher memory consumption. To address this issue, we propose **Effi-Learner**, a self-optimization framework that utilizes execution overhead profiles to improve the efficiency of LLM-generated code. Effi-Learner first generates code using an LLM, then executes it locally to capture execution time and memory usage profiles. These profiles are fed back to the LLM, which then revises the code to reduce overhead. To evaluate the effectiveness of Effi-Learner, we conduct extensive experiments on the EffiBench, HumanEval, and MBPP with 16 open-source and 6 closed-source models. Our evaluation results demonstrate that through iterative self-optimization, Effi-Learner significantly enhances the efficiency of LLM-generated code. For example, the execution time (ET) of StarCoder2-15B for the EffiBench decreases from 0.93 (s) to 0.12 (s) which reduces 87.1% execution time requirement compared with the initial code. The total memory usage (TMU) of StarCoder2-15B also decreases from 22.02 (Mb*s) to 2.03 (Mb*s), which decreases 90.8% total memory consumption during the execution process. The source code of Effi-Learner was released in https://github.com/huangd1999/EffiLearner.

## 1 Introduction

Large language models (LLMs) have recently achieved significant advancements across various tasks . LLMs such as GPT-4  and Copilot  have demonstrated considerable efficacy in code-related applications, including code completion , debuggingg , and translation . These innovative tools have been seamlessly integrated into popular development environments, significantly augmenting developer productivity by providing intelligent coderecommendations based on natural language instructions. However, the primary focus of existing efforts has predominantly been on the correctness of the generated code , ensuring it meets the functional requirements and adheres to syntactical norms.

Despite advancements in ensuring code correctness, there remains a significant gap in the literature regarding the efficiency of code produced by LLMs. Efficiency is crucial as it translates to faster execution and lower memory and processing power consumption, which is especially important in resource-constrained environments such as mobile devices or embedded systems . Recent studies  reveal that LLM-generated code often exhibits lower efficiency in terms of execution time and memory usage when compared to canonical solutions. For instance, on a benchmark that evaluates efficiency, EffiBench , even the most powerful LLM (e.g., GPT-4-Turbo) generates code with suboptimal efficiency. The average and worst-case execution times are 1.69 and 45.49 times longer than those of the canonical solutions, respectively. This inefficiency underscores the need for developing new methods focused on evaluating and improving the efficiency of code generated by LLMs, ensuring that they produce correct and highly efficient code.

To bridge this gap, we draw inspiration from the methodology used by coders on coding platforms. When addressing a programming problem, coders typically write an initial program that is executable on the test cases. Next, they execute the code and obtain a profile of the execution time and memory usage overhead, as shown in Figure 1. Based on this overhead profile, the code optimizes the code to enhance its efficiency. During this process, the code extracts key information (e.g., execution times and memory usage of each line) from the overhead profile, which helps identify lines or operators that require significant overhead (e.g., loops that execute multiple times or unnecessary variables being saved). This information assists the coder in optimizing their code.

With this motivation, we propose **Effi-Learner** to improve the efficiency of LLM-generated code. As shown in Figure 2, Effi-Learner first requires the LLM to generate code based on the task description. Then, Effi-Learner executes the generated code locally and captures the execution time and memory usage profile. These overhead profiles are fed back into the LLM, which then revises the code to reduce the overhead. Through multi-iteration self-optimization, the efficiency of the LLM-generated code is improved. While it's true that the iterative process requires extra time, it's crucial to recognize the long-term advantages that come with this investment. By optimizing the code, we can enhance the overall efficiency once it's deployed.

To evaluate the effectiveness of Effi-Learner, we conduct extensive experiments on the EffiBench and two commonly used code generation benchmarks (i.e. HumanEval  and MBPP ) with 16 open-source and 6 closed-source models. We compare the efficiency of the code generated by the LLM before and after applying Effi-Learner. The experimental results demonstrate that Effi-Learner significantly improves the efficiency of the LLM-generated code. For example, the

Figure 1: A case for the task with code and Effi-Learner refined version. The lower left panel shows the initial completion generated by an LLM, its profile shows its inefficiency. The lower right panel shows the final efficient answer output by applying Effi-Learner.

execution time (ET) of StarCoder2-15B decreases from 0.93 (s) to 0.12 (s) which reduces 87.1% execution time requirement compared with the initial code. The max memory usage (MU) of DeepSeek-6.7B-Ins also decreased from 259.73 (Mb) to 36.97 (Mb), which reduces 85.8% max memory consumption for the code execution requirement. The total memory usage (TMU) of StarCoder2-15B also decreases from 22.02 (Mb*s) to 2.03 (Mb*s), which decreases 90.8% total memory consumption during the execution process.

## 2 Related Work

### LLMs for Code Generation

The increasing popularity of LLMs for code generation has coincided with the growing availability of open-source code repositories and the need to boost developer productivity. Initial efforts focused on training models specifically for coding tasks, such as CodeT5 , AlphaCode , CodeGen , InCoder , StarCoder , SantaCoder  and DeepSeek Coder . Contrastingly, models such as Codex  and CodeLLaMA  represent a subsequent stride, being finetuned from foundation models [8; 59]. These code LLMs have been applied to various tasks, including code generation [11; 14], program repair [25; 28], automated testing [31; 16], code translation [54; 1], type prediction [45; 64], and code summarization [26; 2]. Among these, code generation, where models generate code snippets based on natural language descriptions or docstrings, has become a critical domain for evaluating LLMs. While LLMs have achieved impressive results in code generation tasks like HumanEval  and MBPP , their efficiency has received less attention. Recent studies [56; 27; 48] have shown that LLM-generated code exhibits lower efficiency in execution time and memory usage compared to canonical solutions. These findings highlight the need for further research and development to improve the efficiency of LLM-generated code. In this work, we propose the first method that significantly improves the efficiency of code generated by a wide range of LLMs.

### Learning From Feedback

A prevalent strategy for improving the behavior of LLMs is learning from feedback, mirroring human learning where individuals refine their actions through trial, error, and correction [7; 43]. Early efforts involve using human feedback to evaluate and refine models [30; 51; 21]. To minimize human intervention, another strategy focuses on automated feedback. These methods iteratively learn from automatically generated feedback signals, understanding the consequences of their actions and adapting their behaviors. The source of this automated feedback can be diverse, ranging from the LLM itself [40; 57], external tools [22; 37] or verifiers , external knowledge sources [20; 68] and even generation logits . In code generation, the program executor is frequently used as a source of feedback for refining the model's initial code. For example, Self-Edit  and Self-Evolve  execute the initial program on example test cases and provide the execution results as feedback, prompting the LLM to refine the code. Self-Debug  explores using program explanation, unit tests, and program interpreters as feedback types. ALGO  employs a more fine-grained approach by generating a reference oracle program that solves the problem with an exhaustive search. Feedback is then collected by comparing the generated outputs with the oracle. While existing work primarily focuses on using feedback to edit the initial code to ensure correctness, our method explores using overhead profiles to improve the efficiency of the code.

## 3 Effi-Learner

Inspired by the optimization strategies employed by human coders on coding platforms, we propose a framework Effi-Learner to enhance the efficiency of LLM-generated code. Human coders typically analyze execution time and memory usage profiles to identify bottlenecks and optimize their code. Effi-Learner leverages this principle by integrating a self-optimization loop into the code generation process. As illustrated in Figure 2, Effi-Learner consists of three main components: **Code Generation, Overhead Profiling**, and **Code Refinement**, each playing a crucial role in the self-optimization process.

### Code Generation

Given a task description or code generation requirement, the LLM generates an initial version of the code. The LLM takes the task description as input and produces code that aims to solve the task.

### Overhead Profiling

The generated code is executed locally to capture its execution time and memory usage overhead profiles. During this step, the code is run on a set of open test cases, and the execution time and memory usage for each line of code are recorded. This information forms the overhead profiles that provide insights into the efficiency of the generated code.

Execution Time ProfilingIn this step, we measure the execution time of each line of code to identify potential bottlenecks and inefficiencies. To perform execution time profiling, we utilize the line_profiler library in Python. During the profiling process, we run the generated code on a set of open test cases provided by the dataset. The line_profiler library tracks the execution time of each line of code for all the test cases combined. This helps us assess the code's performance under different conditions and identify any performance bottlenecks. The execution time profiling results are reported based on the total consumption for all open test cases. The profiling output includes information such as the line number, the number of times each line is executed, and the total time spent on each line. These profiles serve as input for the subsequent code refinement step.

Memory Usage ProfilingMemory usage profiling is another essential aspect of the Efi-Learner framework. It helps us understand how the generated code utilizes memory resources and identifies any memory-related inefficiencies or leaks. To profile memory usage, we employ the memory_profiler library in Python. During the memory usage profiling process, we run the generated code on the set of open test cases. The memory_profiler library monitors the memory usage of each line of code throughout the execution of all the test cases combined. It captures the memory usage at different points, such as before and after function calls, loop iterations, and memory allocation statements. The memory usage profiling results are reported based on the total consumption for all open test cases. The profiling output includes information such as the line number and the corresponding memory usage. These profiles provide valuable insights into the memory efficiency of the generated code and help identify areas for optimization.

### Code Refinement

This component leverages the execution time and memory usage profiles to optimize the generated code. In this step, the LLM analyzes the overhead profiles to refine the code for better efficiency. To

Figure 2: Pipeline of Efi-Learner. LLMs first generate code for the given problem. This code is then executed locally to gather overhead profiles. These profiles are subsequently utilized by the LLMs to optimize the code in successive iterations, thereby enhancing the overall efficiency of the generated code. A comprehensive illustration is provided in the Appendix Figure 4-Figure 11.

enable self-optimization, we feed the overhead profiles back into the LLM along with the generated code. The LLM analyzes patterns in the overhead profiles, such as high execution time or excessive memory usage, and correlates them with specific code segments. It then applies optimization techniques, such as loop unrolling, memorization, data structure optimization, algorithm substitution, and code simplification, to improve the efficiency of the identified code segments.

During the self-optimization process, the LLM considers factors such as the impact of each optimization on the overall efficiency, the trade-offs between execution time and memory usage, and the preservation of code correctness. It aims to strike a balance between performance improvement and maintaining the functional integrity of the code. The LLM iteratively refines the code based on the overhead profiles, applying optimizations until a satisfactory level of efficiency is achieved or a predefined number of iterations is reached. The optimized code is then validated against the open test cases to ensure its functional correctness. By leveraging the execution time and memory usage profiles, the self-optimization step enables the LLM to improve the efficiency of the generated code.

### Prompt Construction

We carefully design prompts to guide LLMs in optimizing code efficiency while ensuring the optimized code passes predefined test cases. The prompt template (Figure 3) used in Effi-Learner's self-optimization stage includes a task description, test case, initial code, overhead analysis, and optimization rules. The task description provides context and requirements, the test case ensures correctness, and the initial code is the starting point for optimization. The overhead analysis highlights performance metrics and areas for improvement, while the optimization rules focus the LLM on enhancing efficiency, encapsulating the optimized code, and excluding the test case from the code block. This comprehensive prompt equips the LLM with the necessary information to effectively optimize code, maintain consistency across models and tasks, and facilitate comparison of their code optimization capabilities, advancing the field of LLM-driven code optimization. Details of the template can be found in Appendix A.3.

## 4 Evaluation

### Dataset and Metrics

We evaluate Effi-Learner on EffiBench . Following their settings, we use Execution Time (ET), Normalized Execution Time (NET), Max Memory Usage (MU), Normalized Max Memory Usage (NMU), Total Memory Usage (TMU), and Normalized Total Memory Usage (NTMU) as metrics. We provide a detailed definition of these metrics in A.5. Following the setup of EffiBench, evaluation metrics were only calculated on the tasks that generated code for both the initial version and Effi-Learner optimized code that can pass all private test cases provided by the dataset3.

Following Huang et al. , we utilize the open test cases to calculate the efficiency metrics during the self-optimization process, while private test cases provided by EffiBench were used for the final result evaluation. For HumanEval and MBPP datasets, we set the test cases provided by HumanEval and MBPP as open test cases, while test cases provided by EvalPlus  (i.e., HumanEval-Plus, MBPP-Plus) as private test cases that were used to calculate the final results.

### Implementation Details

All of the experiments are conducted in an edge server with an Intel Xeon Platinum 8336C CPU with 128 cores, and 8 * NVIDIA A100-SXM GPUs Total memory capacity of 2.0TiB.

ModelsWe evaluate Effi-Learner's effectiveness on both open-source and closed-source LLMs4. For open-source models, we evaluate Effi-Learner with OpenCodeInterpreter (1.3B, 6.7B, and 33B) , DeepSeek-Instruct (1.3B, 6.7B, and 33B) , CodeLlama (7B, 13B, 34B, and 70B) , XwinCoder (7B and 34B) , StarCoder (3B, 7B, and 15B) , and WizardCoder-13B , where the detailed versions of LLMs are demonstrated in supplementary file. For closed-source models, we use GPT-3.5-Turbo, GPT-4-Turbo, GPT-4 , Claude-3-haiku, and Claude-3-sonnet5. These LLMs have achieved competitive pass@1 scores in various code generation tasks [11; 6; 35].

**Setup** We first collect the generated code from each LLM and evaluate its correctness using open test cases. Only the code that passes all test cases is considered for efficiency evaluation. This approach ensures consistency in the evaluated tasks across different self-optimization iterations, as Effi-Learner focuses on improving the efficiency of initially correct code without altering its pass@1 score. By evaluating a diverse set of open-source and closed-source LLMs, we aim to provide a comprehensive assessment of the efficiency of LLM-generated code and the effectiveness of Effi-Learner in improving code efficiency across different models and architectures.

### Main Results

**Open-source LLMs** As shown in Table 1, we observe that the efficiency metrics for all models have been increased in most experiments once we apply Effi-Learner to optimize the efficiency of LLM-generated code. For example, in OpenCodeInterpreter-1.3B, the execution time for its generated code decreases from 1.60 (s) to 1.29 (s), a reduction of 19.4% in execution time. Additionally, the

   Model &  &  &  &  &  &  \\   \\   & 1.60 & 1.52 & 38.91 & 1.00 & 89.16 & 1.11 \\  & 1.29 (19.4\%) & 1.23 (19.1\%) & 38.91 (0.0\%) & 1.00 (0.0\%) & 70.63 (20.8\%) & 0.88 (20.7\%) \\  & 0.34 & 2.41 & 36.82 & 1.00 & 13.36 & 1.56 \\  & 0.28 (17.6\%) & 1.91 (20.7\%) & 38.60 (4.8\%) & 1.00 (0.0\%) & 14.16 (-6.0\%) & 1.44 (7.7\%) \\  & 0.29 & 2.10 & 3.58 & 1.00 & 1.306 & 1.93 \\  & 0.28 (3.4\%) & 2.00 (4.8\%) & 36.30 (-2.3\%) & 1.00 (0.0\%) & 11.54 (11.6\%) & 1.64 (15.0\%) \\   & 1.42 & 1.32 & 36.04 & 1.00 & 40.61 & 1.12 \\  & 1.15 (19.0\%) & 1.07 (18.9\%) & 36.04 (0.0\%) & 1.00 (0.0\%) & 35.48 (12.6\%) & 0.98 (12.5\%) \\  & 0.37 & 2.60 & 259.73 & 7.25 & 555.18 & 67.70 \\  & 0.34 (8.1\%) & 2.37 (8.8\%) & 36.97 (**58.5\%)** & 1.00 (**86.2\%)** & 13.66 (97.5\%) & 1.46 (97.8\%) \\  & 0.29 & 2.21 & 34.53 & 1.06 & 14.44 & 2.91 \\  & 0.25 (13.8\%) & 1.84 (16.7\%) & 32.67 (5.4\%) & 0.99 (6.6\%) & 8.15 (43.6\%) & 1.55 (46.7\%) \\   & 4.70 & 3.68 & 4.66 & 0.99 & 212.41 & 1.93 \\  & 4.52 (3.8\%) & 3.54 (3.8\%) & 38.67 (17.3\%) & 0.82 (17.2\%) & 157.66 (25.7\%) & 1.43 (25.9\%) \\  & 2.45 & 2.19 & 42.46 & 0.93 & 137.40 & 1.51 \\  & 2.28 (6.9\%) & 2.04 (6.8\%) & 42.12 (0.8\%) & 0.93 (0.0\%) & 119.36 (13.1\%) & 1.31 (13.2\%) \\  & 1.05 & 7.75 & 57.57 & 1.70 & 94.79 & 15.65 \\  & 1.02 (2.9\%) & 7.34 (5.3\%) & 40.62 (29.4\%) & 1.11 (34.7\%) & 52.12 (45.0\%) & 7.02 (55.1\%) \\  & 0.52 & 3.93 & 109.61 & 3.57 & 203.02 & 54.15 \\  & 0.47 (9.6\%) & 3.84 (2.3\%) & 26.42 (75.9\%) & 1.00 (72.0\%) & 14.53 (29.9\%) & 6.52 (88.0\%) \\   & 2.80 & 2.81 & 55.54 & 1.52 & 208.23 & 3.47 \\  & 2.43 (13.2\%) & 2.44 (13.2\%) & 19.40 (11.6\%) & 1.34 (11.8\%) & 158.20 (24.0\%) & 2.64 (23.9\%) \\  & 0.77 & 5.68 & 49.77 & 1.49 & 61.36 & 2.11 \\  & 0.69 (10.4\%) & 5.11 (10.0\%) & 52.12 (-4.7\%) & 1.47 (1.3\%) & 57.89 (5.7\%) & 9.92 (18.1\%) \\   & 1.10 & 1.25 & 24.31 & 1.00 & 17.47 & 1.19 \\  & 1.02 (7.3\%) & 1.15 (0.0\%) & 24.28 (0.1\%) & 1.00 (0.0\%) & 16.38 (6.2\%) & 1.12 (5.9\%) \\   & 3.69 & 5.34 & 26.42 & 1.08 & 82.38 & 7.62 \\   & 2.99 (19.0\%) & 4.32 (19.1\%) & 26.40 (10.1\%) & 1.08 (0.0\%) & 68.61 (16.7\%) & 6.35 (16.7\%) \\   & 0.93 & 7.58 & 26.35 & 1.00 & 22.02 & 10.88 \\   & 0.12 (**87.1\%**) & 1.03 (**86.4\%**) & 27.67 (-5.0\%) & 1.01 (-1.0\%) & 2.03 (**90.8\%**) & 1.06 (**90.3\%**) \\   & 3.43 & 2.11 & 86.72 & 1.35 & 324.83 & 1.92 \\   & 2.93 (14.6\%) & 1.80 (14.7\%) & 71.02 (18.1\%) & 1.11 (17.8\%) & 219.69 (32.4\%) & 1.30 (32.3\%) \\   \\   & 0.36 & 2.50 & 91.25 & 2.45 & 157.50 & 19.75 \\  & 0.28 (**22.2\%**) & 2.01 (**19.6\%**) & 36.08 (**60.5\%**) & 0.99 (**59.6\%**) & 12.43 (**92.1\%**) & 1.64 (**91.7\%**) \\   & 0.28 & 1.96 & 36.12 & 1.01 & 12.79 & 1.73 \\   & 0.26 (7.1\%) & 1.90 (3.1\%) & 34.02 (5.8\%) & 1.00 (10.0\%) & 11.41 (10.8\%) & 1.62 (6.4\%) \\   & 0.27 & 1.96 & 33.94 & 1.00 & 11.82 & 1.89 \\  & 0.25 (7.4\%) & 1.88 (1.4\%) & 33.17 (2.3\%) & 1.00 (0.0\%) & 10.18 (13.9\%) & 1.76 (6.9\%) \\   & 0.

TMU of OpenCodeInterpreter-1.3B decreases from 89.16 (Mb*s) to 70.63 (Mb*s). Furthermore, in certain edge cases, Effi-Learner significantly enhances efficiency. For example, the ET of StarCoder2-15B decreases from 0.93 (s) to 0.12 (s) and the NET also decreases from 7.48 to 1.03, reducing execution time requirements by 87.1% compared to the initial code. The MU and NMU of DeepSeek-6.7B-Ins also decrease from 259.73 (Mb) and 7.25 to 36.97 (Mb) and 1.06, reducing the maximum memory consumption by 85.8% for the code execution requirement. Moreover, we can also observe that the TMU and NTMU of StarCoder2-15B also decrease from 22.02 (Mb*s) and 10.88 to 2.03 (Mb*s) and 1.06, which decreases 90.8% memory consumption during the execution process. These results demonstrate the effectiveness of Effi-Learner in optimizing the code generated by open-source LLMs.

**Closed-source LLMs** Similar to open-source LLMs, we observe that the efficiency metrics for most closed-source LLMs have been improved after applying Effi-Learner to optimize the efficiency of the generated code. For instance, the execution time for code generated by GPT-3.5-Turbo-0301 decreases from 0.36 (s) to 0.28 (s), reducing the execution time by 22.2%. The MU and NMU of GPT-3.5-Turbo-0301 also decrease from 91.25 (Mb) and 2.45 to 36.08 (Mb) and 0.99, respectively, which reduces the max memory consumption for code execution by 60.5%. Furthermore, the TMU and NTMU of GPT-3.5-Turbo-0301 decrease from 157.50 (Mb*s) and 19.75 to 12.43 (Mb*s) and 1.64, respectively, decreasing memory consumption during the execution process by 92.1%.

The improvements in efficiency metrics across both open-source and closed-source LLMs highlight the generalizability and adaptability of Effi-Learner. By iteratively refining the generated code based on efficiency profiler feedback, Effi-Learner enables LLMs to produce more efficient code without compromising the correctness of the generated solutions. The consistent improvements across various models and architectures demonstrate the potential of Effi-Learner as a model-agnostic approach for optimizing the efficiency of LLM-generated code in real-world applications.

### Impact of Self-Optimization Steps

To investigate the impact of the number of self-optimization steps on the efficiency of the Effi-Learner-optimized code, we conduct an ablation study by varying the number of steps from 0 to 5. Table 2 for CodeLlama-70B and GPT-3.5-Turbo-0301 at different self-optimization steps.

**CodeLlama-70B** We can observe that after the first self-optimization step, the MU decreases from 109.61 (Mb) to 26.47 (Mb), reducing 75.9% maximum memory requirement compared with the initial code generated by CodeLlama-70B. Similarly, the TMU decreases from 54.15 (Mb*s) to 6.69 (Mb*s), reducing 87.6% of memory consumption during code execution. As the number of steps increases, the efficiency metrics gradually improve. By the fifth step, the ET reaches 0.47 (s), reducing the 1.9% execution time requirement compared with the first-step generated code, and the TMU settles at 14.53, reducing 0.2% total memory usage from the first step.

**GPT-3.5-Turbo-0301** Similar to CodeLlama-70B, the MU decreases from 91.25 (Mb) to 36.09 (Mb) after the first self-optimization step, reducing 60.4% maximum memory requirement compared with the initial code. The TMU also shows a substantial reduction from 157.50 (Mb*s) to 13.70 (Mb*s),

   Steps & **ET (s)** & **NET** & **MU (Mb)** & **NMU** & **TMU (Mb*s)** & **NTMU** \\   \\ 
0 & 0.52 & 3.93 & 109.61 & 3.57 & 203.92 & 54.15 \\
1 & 0.48 (7.7\%) & 3.94 (0.3\%) & 26.47 (75.9\%) & 1.00 (72.0\%) & 14.91 (92.7\%) & 6.69 (87.6\%) \\
2 & 0.48 (7.7\%) & 3.89 (1.0\%) & 26.47 (75.9\%) & 1.00 (72.0\%) & 14.69 (92.8\%) & 6.60 (87.8\%) \\
3 & 0.47 (9.6\%) & 3.85 (2.0\%) & 26.42 (75.9\%) & 1.00 (72.0\%) & 14.54 (92.9\%) & 6.56 (87.9\%) \\
4 & 0.47 (9.6\%) & 3.84 (2.3\%) & 26.42 (75.9\%) & 1.00 (72.0\%) & 14.54 (92.9\%) & 6.53 (87.9\%) \\
5 & 0.47 (9.6\%) & 3.84 (2.3\%) & 26.42 (75.9\%) & 1.00 (72.0\%) & 14.53 (92.9\%) & 6.52 (88.0\%) \\   \\ 
0 & 0.36 & 2.50 & 91.25 & 2.45 & 157.50 & 19.75 \\
1 & 0.33 (8.3\%) & 2.35 (6.0\%) & 36.09 (60.4\%) & 0.99 (59.6\%) & 13.70 (91.3\%) & 1.81 (90.8\%) \\
2 & 0.31 (13.9\%) & 2.18 (12.8\%) & 36.09 (60.4\%) & 0.99 (59.6\%) & 13.04 (91.7\%) & 1.72 (91.3\%) \\
3 & 0.29 (19.4\%) & 2.06 (17.6\%) & 36.08 (60.5\%) & 0.99 (59.6\%) & 12.57 (92.0\%) & 1.66 (91.6\%) \\
4 & 0.29 (19.4\%) & 2.03 (18.8\%) & 36.08 (60.5\%) & 0.99 (59.6\%) & 12.50 (92.1\%) & 1.65 (91.6\%) \\
5 & 0.28 (22.2\%) & 2.01 (19.6\%) & 36.08 (60.5\%) & 0.99 (59.6\%) & 12.43 (92.1\%) & 1.64 (91.7\%) \\   

Table 2: Effect of the number of self-optimization steps in Effi-Learner.

reducing 91.3% of memory consumption during code execution. As the number of steps increases, the efficiency metrics continue to improve steadily. By the fifth step, the ET reaches 0.28 (s), reducing the 15.2% execution time requirement compared with the first-step generated code, and the TMU settles at 12.43 (Mb*s), reducing 9.3% total memory usage from the first step.

Table 2 demonstrates the significant impact of the number of self-optimization steps on the efficiency of the Effid-Learner-optimized code. For both CodeLlama-70B and GPT-3.5-Turbo-0301, the first self-optimization step yields the most substantial improvements in code efficiency. As the number of steps increases, the efficiency metrics continue to improve, albeit with diminishing returns. By the fifth step, the efficiency metrics reach their lowest values, demonstrating the effectiveness of Effid-Learner's iterative self-optimization approach in enhancing the efficiency of LLM-generated code. The evaluation results highlight that the majority of efficiency improvements occur in the first few steps, with subsequent steps contributing to further refinements of the optimized code.

### Feedback of Overhead Profile

To show the effectiveness of the overhead profile in guiding LLMs to refine their generated code, we compare the performance of Effid-Learner with two alternative approaches: Unsupervised Self-Refine and Result-Aware Self-Refine [40; 57]. Unsupervised Self-Refine uses a prompt that directly requires the LLM to refine the code without providing additional information. Result-Aware Self-Refine feeds the ET, MU, and TMU, then requires the LLM to refine the code based on these metrics. Table 3 presents the code efficiency metrics for CodeLlama-70B and GPT-3.5-Turbo-0301 using different code refinement approaches.

**CodeLlama-70B** Unsupervised Self-Refine and Result-Aware Self-Refine result in significant increases in ET, memory usage (MU), and TMU compared to the initial version. Unsupervised Self-Refine increases ET by 51.9%, MU by 154.9%, and TMU by 518.8%, while Result-Aware Self-Refine increases ET by 51.9%, MU by 157.8%, and TMU by 523.2%. In contrast, Effid-Learner incorporates the overhead profile feedback and achieves a 9.6% reduction in ET, a 75.9% reduction in MU, and a 92.9% reduction in TMU compared to the initial version.

**GPT-3.5-Turbo-0301** Unsupervised Self-Refine and Result-Aware Self-Refine show some improvements in ET and MU compared to the initial version. Unsupervised Self-Refine reduces ET by 11.1% and MU by 14.1%, while Result-Aware Self-Refine reduces ET by 16.7% and MU by 35.7%. However, both approaches lead to substantial increases in TMU, with Unsupervised Self-Refine increasing TMU by 98.7% and Result-Aware Self-Refine increasing TMU by 24.1%. On the other hand, Effid-Learner achieves a 22.2% reduction in ET, a 60.5% reduction in MU, and a 92.1% reduction in TMU compared to the initial version.

These results highlight the importance of the overhead profile feedback in guiding LLMs to generate more efficient code. Without the overhead profile, the code refinement process using alternative

   Optimization Profile & **ET (s)** & **NET** & **MU (Mb)** & **NMU** & **TMU (Mb*s)** & **NMU** \\  CodeLlama-70B & & & & & & \\  Initial Version & 0.52 & 3.93 & 109.61 & 3.57 & 203.92 & 54.15 \\ Unsupervised Self-Refine & 0.79 (5.19\%) & 6.87 (7.24\%) & 279.41 (-5.94\%) & 105.85 (-196.74\%) & 1261.83 (51.88\%) & 600.95 (-100.95\%) \\ Result-Aware Self-Refine & 0.79 (5.19\%) & 6.87 (7.24\%) & 282.57 (-157.5\%) & 10.70 (-199.74\%) & 1270.93 (52.24\%) & 605.29 (-100.78\%) \\ Memory Profiler & 0.53 (-1.95\%) & 4.34 (-10.04\%) & 26.38 (-75.9\%) & 09.79 (23.23\%) & 157.77 (-92.53\%) & 706.86 (-07.03\%) \\ Execution Time Profiler & 0.51 (-1.95\%) & 4.17 (-16.13\%) & 264.45 (-75.9\%) & 10.00 (-72.03\%) & 15.53 (-29.4\%) & 6.97 (-87.1\%) \\ Effid-Learner & 0.47 (0.65\%) & 3.84 (-2.36\%) & 26.42 (-75.9\%) & 1.00 (-72.03\%) & 14.53 (-92.9\%) & 6.52 (-80.03\%) \\  GPT-3.5-Turbo-0301 & & & & & & \\  Initial Version & 0.36 & 2.50 & 91.25 & 2.45 & 157.50 & 19.75 \\ Unsupervised Self-Refine & 0.32 (11.16\%) & 2.46 (-6.69\%) & 78.39 (14.1\%) & 2.12 (13.5\%) & 312.99 (-87.1\%) & 42.42 (11.48\%) \\ Result-Aware Self-Refine & 0.30 (16.7\%) & 2.55 (10.9\%) & 58.65 (35.7\%) & 1.61 (34.3\%) & 195.49 (-24.1\%) & 271.6 (-37.5\%) \\ Memory Profiler & 0.34 (5.6\%) & 2.40 (-4.0\%) & 36.85 (59.6\%) & 1.00 (-92.9\%) & 16.54 (-39.6\%) & 2.10 (-90.9\%) \\ Execution Time Profiler & 0.33 (3.3\%) & 2.34 (-6.6\%) & 36.43 (-60.1\%) & 0.99 (-95.6\%) & 14.07 (-91.1\%) & 1.81 (-90.8\%) \\ Effid-Learner & 0.28 (22.2\%) & 2.01 (-9.6\%) & 36.08 (-60.5\%) & 0.99 (-95.6\%) & 12.43 (-92.1\%) & 1.64 (-91.7\%) \\   

Table 3: Contribution of different components in Effid-Learner. We evaluate how different feedback profilers affect the efficiency of LLM-generated code. Unsupervised self-refine only requires LLMs to optimize the efficiency of the code. Result-Aware Self-Refine feedback the ET, MU, and TMU to the LLMs and require it to improve the efficiency. Memory Profiler and Execution Time Profiler feedback the memory profile and execution time profiler to the LLMs and then LLMs can based on the profile optimize the efficiency of the code.

prompts fails to improve code efficiency and even leads to significant performance degradation. The overhead profile provides valuable insights into the resource consumption of the generated code, enabling LLMs to make targeted optimizations and achieve substantial efficiency improvements.

### Discussion

Generalizability across benchmarksIn Table 1, we evaluated Effi-Learner's effectiveness on the EffiBench dataset. To illustrate Effi-Learner's generalizability in other datasets, we conduct experiments on the HumanEval and MBPP datasets in Appendix Table 8 and Table 9. We also provide Effi-Learner's effectiveness on the HumanEval dataset in CodeLlama models in Table 4. We can

  Model & Initial Pass@1 & Effi-Learner Pass@1 \\  OpenCodeInterpreter-DS-1.3B & 5.8 & 5.4 \\ OpenCodeInterpreter-DS-6.7B & 13.6 & 13.2 \\ OpenCodeInterpreter-DS-33B & 24.7 & 24.4 \\ deepseek-1.3b-Ins & 4.8 & 4.5 \\ deepseek-6.7b-Ins & 7.2 & 7.0 \\ deepseek-33b-Ins & 10.0 & 9.9 \\ CodeLlama-7b & 7.0 & 7.0 \\ CodeLlama-13b & 9.7 & 9.6 \\ CodeLlama-34b & 13.5 & 13.0 \\ CodeLlama-70b & 7.8 & 7.4 \\ XwinCoder-13B & 10.5 & 10.2 \\ XwinCoder-34B & 21.2 & 21.2 \\ starcoder2-3b & 1.6 & 1.2 \\ starcoder2-7b & 1.9 & 1.8 \\ starcoder2-15b & 0.7 & 0.4 \\ WizardCoder-13B & 4.0 & 3.9 \\  

Table 6: Pass@1 of LLMs generated initial code and Effi-Learner optimized code.

   Steps &  &  &  &  &  &  \\  CodeLlama-7b & 0.20 & 0.71 & 57.39 & 0.91 & 7.08 & 0.70 \\  & 0.18 (10.0\%) & 0.63 (11.3\%) & 57.07 (0.6\%) & 0.91 (0.0\%) & 6.18 (12.7\%) & 0.61 (12.9\%) \\ CodeLlama-13b & 0.23 & 0.95 & 58.13 & 0.96 & 7.97 & 0.94 \\  & 0.20 (13.0\%) & 0.80 (15.8\%) & 58.03 (0.2\%) & 0.96 (0.0\%) & 6.64 (16.7\%) & 0.79 (16.0\%) \\ CodeLlama-34b & 0.24 & 0.95 & 61.79 & 1.01 & 8.45 & 0.96 \\  & 0.21 (12.5\%) & 0.81 (14.7\%) & 61.55 (0.4\%) & 1.00 (1.0\%) & 6.99 (17.3\%) & 0.80 (16.7\%) \\ CodeLlama-70b & 0.21 & 0.93 & 60.19 & 1.01 & 6.76 & 1.01 \\  & 0.18 (14.3\%) & 0.79 (15.1\%) & 59.49 (1.2\%) & 1.00 (1.0\%) & 5.75 (14.9\%) & 0.86 (14.9\%) \\   

Table 4: Results of Effi-Learner on HumanEval dataset, where we evaluate CodeLlama family generated code’s efficiency. Full results are listed in Appendix Table 8.

   Model &  &  &  &  &  &  \\   & 0.52 & 3.28 & 157.16 & 3.36 & 337.30 & 24.44 \\  & 0.40 (23.1\%) & 2.51 (23.5\%) & 68.27 (56.6\%) & 1.45 (56.8\%) & 65.64 (80.5\%) & 4.86 (80.1\%) \\ Artigenz-Coder-DS-6.7B & 0.39 & 2.75 & 65.73 & 1.70 & 95.65 & 10.87 \\  & 0.32 (17.9\%) & 2.30 (16.4\%) & 59.00 (10.2\%) & 1.62 (4.7\%) & 79.67 (16.7\%) & 10.73 (1.3\%) \\  & 0.22 & 1.59 & 40.19 & 1.09 & 17.58 & 2.28 \\  & 0.21 (4.5\%) & 1.50 (5.7\%) & 38.29 (4.7\%) & 107 (1.8\%) & 15.27 (13.1\%) & 2.22 (2.6\%) \\  & 2.36 & 18.40 & 28.88 & 1.00 & 57.92 & 24.36 \\  & 1.45 (38.6\%) & 12.17 (33.9\%) & 27.45 (5.0\%) & 1.03 (-3.0\%) & 35.46 (38.8\%) & 17.28 (29.1\%) \\ CodeFuse-DeepSeek-33B & 0.40 & 3.10 & 70.39 & 2.06 & 191.15 & 32.20 \\  & 0.39 (2.5\%) & 3.01 (2.9\%) & 63.22 (12.0\%) & 1.85 (10.2\%) & 156.81 (18.0\%) & 26.42 (18.0\%) \\ CodeLlama-34b-hf & 2.08 & 15.68 & 46.41 & 1.26 & 128.46 & 17.87 \\  & 1.95 (6.3\%) & 14.67 (6.4\%) & 46.40 (0.0\%) & 1.26 (0.0\%) & 125.22 (2.5\%) & 17.42 (2.5\%) \\ speechless-starcoder2-15b & 0.19 & 1.74 & 27.39 & 0.99 & 3.20 & 1.75 \\  & 0.13 (31.6\%) & 1.19 (31.6\%) & 27.25 (0.5\%) & 0.99 (0.0\%) & 2.17 (32.2\%) & 1.19 (32.0\%) \\ gpt-3.5-turbo-0613 & 0.56 & 4.32 & 35.48 & 1.00 & 20.11 & 3.00 \\  & 0.49 (12.5\%) & 3.75 (13.2\%) & 35.47 (0.0\%) & 1.00 (0.0\%) & 17.84 (11.3\%) & 2.66 (11.3\%) \\   

Table 5: Code efficiency of widely-studied LLMs reported by Effi-Learner.

observe that the coding efficiency of CodeLlama and other LLMs (See Table 8) also increases when we utilize Esfi-Learner to optimize LLM-generated code. For example, the ET of CodeLlama-70B decreases from 0.21 (s) to 0.18 (s), which reduces 14.3% execution time. As shown in Table 8 and Table 9, results demonstrate that Esfi-Learner can consistently improve the efficiency of LLM-generated code for other datasets.

**Generalizability across LLMs** In Table 1, we evaluate Esfi-Learner's effectiveness on six types of open-source LLMs. To illustrate Esfi-Learner's generalizability in other LLMs, we also conduct experiments on other LLMs in Table 5. Our evaluation results demonstrate that Esfi-Learner can improve the efficiency of LLM-generated code for different LLMs. For example, the execution time of Mistral-7B-codealpaca-lora decreases from 2.36 (s) to 1.45 (s), which reduces 38.6% execution time compared with the initial code. The total memory usage of Phind-CodeLlama-34B-v2 also decreases from 337.30 (Mb*s) to 65.64 (Mb*s), which reduces 80.5% total memory requirement.

**Impact on correctness** We provide the pass@1 of LLM-generated initial code and Esfi-Learner optimized code for EsfiBench in Table 6. We observe that the pass@1 of Esfi-Learner optimized code may be lower than LLM-generated initial code. The key reason is that during the self-optimization process, Esfi-Learner only uses public test cases to guide code efficiency optimization for correct initial code. However, since public test cases may not cover all edge cases in the private test cases (test cases used to evaluate pass@1 of LLMs), this can cause the pass@1 of Esfi-Learner generated code to be lower than the initial code. Nevertheless, we observe that the pass@1 of Esfi-Learner only decreases by about 0% to 0.5%, which means that only a few of the codes will be incorrect. As shown in Table 1, we can observe that the code efficiency is largely increased. We believe that this minor decrease in pass@1 is worthwhile considering the significant efficiency gains.

**Case study** To illustrate how Esfi-Learner improves the efficiency of LLM-generated code, we provide a case illustration in Appendix Figure 4-Figure 11. As shown in Figure 6, we can observe that the execution time of the initial code is 23.59 (s) while in the self-optimized code, the execution time decreases from 23.59 (s) to 3.36 (s). The key reason is that in the initial code, the algorithm uses a standard unidirectional Breadth-First Search (BFS), which explores all possible states level by level starting from the initial state. This method results in a large number of states to explore, leading to significant computational overhead. In contrast, the self-optimized code employs a bidirectional BFS, which simultaneously searches from both the initial state and the target state. This reduces the search space by meeting in the middle, significantly decreasing the number of states that need to be explored and thereby improving the execution time.

**Error Analysis** We also provide a case illustration to explain why some code efficiency does not improve significantly when Esfi-Learner is applied to LLM-generated code. As shown in Appendix Figure 12-Figure 18, we observe that the initial code only requires 0.0012 (s) to execute, while in the optimized code, the execution time is still 0.0011 (s). The key reason for this minimal improvement is that both implementations already operate with the same theoretical time complexity of \(O(((m,n)))\). Given the problem's constraints and small input sizes, the actual runtime differences are overshadowed by the inherent efficiency of the binary search algorithm. Additionally, the overhead of function calls and Python runtime operations can further minimize the observed performance gains. Therefore, while the optimized code may offer clearer partition management and slight improvements, the overall efficiency remains largely unchanged due to the already optimized nature of the initial approach.

## 5 Conclusion

This paper focuses on the critical issue of efficiency in code generated by LLMs. While LLMs have shown impressive capabilities in code generation, their output often suffers from suboptimal efficiency, leading to slower execution and higher resource consumption. To tackle this challenge, we propose Esfi-Learner, a novel self-optimization framework that leverages execution overhead profiles to guide LLMs in improving code efficiency. Extensive experiments and analysis demonstrate that Esfi-Learner significantly enhances the efficiency of LLM-generated code, achieving substantial reductions in execution time and memory usage. For future work, we would like to investigate the application of Esfi-Learner to other programming tasks and languages, as well as explore the potential benefits of incorporating domain-specific knowledge into the optimization process.

## 6 Acknowledgment

The work is supported in part by National Key R&D Program of China (2022ZD0160201), HK RGC RIF (R7030-22), HK ITF (GHP/169/20SZ), a Huawei Flagship Research Grant in 2023, HK RGC GRF (Ref: 17208223 & 17204424), and the HKU-CAS Joint Laboratory for Intelligent System Software.