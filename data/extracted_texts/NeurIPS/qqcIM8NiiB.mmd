# Photoswap:

Personalized Subject Swapping in Images

 Jing Gu\({}^{1}\) Yilin Wang\({}^{2}\) Nankuan Zhao\({}^{2}\) Tsu-Jui Fu\({}^{3}\) Wei Xiong\({}^{2}\) Qing Liu\({}^{2}\) Zhifei Zhang\({}^{2}\) He Zhang\({}^{2}\) Jianming Zhang\({}^{2}\) HyunJoon Jung\({}^{2}\) Xin Eric Wang\({}^{1}\)

\({}^{1}\)University of California, Santa Cruz

\({}^{2}\)Adobe \({}^{3}\)University of California, Santa Barbara

https://photoswap.github.io/

###### Abstract

In an era where images and visual content dominate our digital landscape, the ability to manipulate and personalize these images has become a necessity. Envision seamlessly substituting a tabby cat lounging on a sunlit window sill in a photograph with your own playful puppy, all while preserving the original charm and composition of the image. We present _Photoswap_, a novel approach that enables this immersive image editing experience through personalized subject swapping in existing images. _Photoswap_ first learns the visual concept of the subject from reference images and then swaps it into the target image using pre-trained diffusion models in a training-free manner. We establish that a well-conceptualized visual subject can be seamlessly transferred to any image with appropriate self-attention and cross-attention manipulation, maintaining the pose of the swapped subject and the overall coherence of the image. Comprehensive experiments underscore the efficacy and controllability of _Photoswap_ in personalized subject swapping. Furthermore, _Photoswap_ significantly outperforms baseline methods in human ratings across subject swapping, background preservation, and overall quality, revealing its vast application potential, from entertainment to professional editing.

Figure 1: _Photoswap_ can effortlessly replace the subject in a source image, which could be either synthetic (first two rows) or real (bottom row), with a personalized subject specified in reference images, while preserving the original subject pose and the composition of the source image.

Introduction

Imagine a digital world where the boundaries of reality and creativity blur, where a photograph of a tabby cat lounging on a sunlit window sill can effortlessly be transformed to feature your playful puppy in the same pose. Or envision yourself as a part of a famous movie scene, replaced seamlessly with the original character while preserving the very essence and composition of the scene. Can we achieve this level of personalized image editing, not just with expert-level photo manipulation skills, but in an automated, user-friendly manner? This question lies at the heart of _personalized subject swapping_, the challenging task of replacing the subject in an image with a user-specified subject, while maintaining the integrity of the original pose and composition. It opens up a plethora of applications in areas such as entertainment, advertising, and professional editing.

Personalized subject swapping is a complex undertaking that comes with its own set of challenges. The task requires a profound comprehension of the visual concept inherent to both the original subject and the replacement subject. Simultaneously, it demands the seamless integration of the new subject into the existing image. One of the critical objectives in subject swapping is to preserve the similar pose of the replacement subject. It is crucial that the swapped subject seamlessly fits into the original pose and scene, creating a natural and harmonious visual composition. This necessitates careful consideration of factors such as lighting conditions, perspective, and overall aesthetic coherence. By effectively blending the replacement subject with these elements, the final image maintains a sense of continuity and authenticity.

Existing image editing methods fall short in addressing these challenges. Many of these techniques are restricted to global editing and lack the finesse needed to seamlessly integrate new subjects into existing images. For example, for most text-to-image (T2I) models, a slightly prompt change could lead to a totally different image. Recent works (Nichol _et al._, 2022; Meng _et al._, 2022; Couairon _et al._, 2022; Cao _et al._, 2023; Zhang _et al._, 2023) allow user to control the generation with an additional input such as user brush, semantic layout, or sketches. However, it is still challenging to guide the generation process to follow users' intent on the generation of object shape, texture, and identity. Other approaches (Hertz _et al._, 2022; Tumanyan _et al._, 2023; Mokady _et al._, 2023) have explored the potential of using text prompts to edit image content in the context of synthetic image generation. Despite showing promise, these methods are not yet fully equipped to handle the intricate task of swapping subjects in existing images with user-specified subjects.

Therefore, we present _Photoswap_, a novel framework that leverages pre-trained diffusion models for personalized subject swapping in images. In our approach, the diffusion model learns to represent the concept of the subject (\(O_{t}\)). Then the representative attention map and attention output saved in the source image generation process will be transferred into the generation process of the target image to generate the new subject while keeping non-subject pixels unchanged. Our extensive experiments and evaluations demonstrate the effectiveness of _Photoswap_. Not only does our method enable the seamless swapping of subjects in images, but it also maintains the pose of the swapped subject and the overall coherence of the image. Remarkably, _Photoswap_ outperforms baseline methods by a large margin in human evaluations of subject identity preservation, background preservation, and overall quality of the swapping (_e.g._, 37.3% _vs._ 27.1% in terms of overall quality). The contributions of this work are as follows: **1)** We present a new framework for personalized subject swapping in images. **2)** We propose a training-free attention swapping method that governs the editing process. **3)** The efficacy of our proposed framework is demonstrated through extensive experiments including human evaluation.

## 2 Related Work

### Text-to-Image Generation

In the early stages of text-based image generation, Generative Adversarial Networks (GANs) (Goodfellow _et al._, 2020; Brock _et al._, 2018; Karras _et al._, 2019) were widely used due to their exceptional ability to produce high-quality images. These models aimed to align textual descriptions with synthesized images through multi-modal vision-language learning, achieving impressive results on specific domains (e.g., bird, chair and human face). When combined with CLIP (Radford _et al._, 2021), a large pre-trained model that learns visual-textual representations from millions of caption-image pairs, GAN models (Crowson _et al._, 2022) have demonstrated promising outcomes in cross-domain text-to-image (T2I) generation. Recently, T2I generation has seen remarkable progress with auto-regressive (OpenAI, 2021; Ding _et al._, 2021) and diffusion models (Nichol _et al._, 2022; Gu _et al._,2022; OpenAI, 2022; Saharia _et al._, 2022), offering diverse outcomes and can synthesize high-quality images closely aligned with textual descriptions in arbitrary domains.

Rather than focusing on T2I generation tasks without any constraints, subject-driven T2I generation (Nitzan _et al._, 2022; Casanova _et al._, 2021; Ruiz _et al._, 2023) requires the model to identify the specific object from a set of visual examples and synthesize novel scenes incorporating them based on the input text prompts. Building upon modern diffusion techniques, recent approaches such as DreamBooth (Ruiz _et al._, 2023) and Textual Inversion (Gal _et al._, 2023, 2023; Kumari _et al._, 2023; Mokady _et al._, 2023) learn to invert special tokens from a given set of images. By combining these tokens with text prompts, they generate personalized unseen images. To improve data efficiency, retrieval augmentation techniques (Sheynin _et al._, 2023; Blattmann _et al._, 2022; Chen _et al._, 2023) leverages external knowledge bases to overcome limitations posed by rare entities, resulting in visually relevant appearances and enhanced personalization. In our work, we aim to tackle personalized subject swapping, not only preserving the identity of subjects in reference images, but also maintaining the context of the source image.

### Text-guided Image Editing

Text-guided image editing manipulates an existing image based on the input textual instructions, while preserving certain aspects or characteristics of the original image. Early works based on GAN models (Karras _et al._, 2019) only limits to a certain object domain. Diffusion-based methods (Zhang _et al._, 2023; Nichol _et al._, 2022; Feng _et al._, 2023) break this barrier and support text-guided image editing. Though these methods generate stunning results, many of them suffer from conducting local editing, and additional manual masks (Meng _et al._, 2022; Zeng _et al._, 2023; Meng _et al._, 2022) are required to constrain the editing regions, which is often tedious to draw. By employing cross-attention (Hertz _et al._, 2022) or spatial characteristics (Tumanyan _et al._, 2023), the local editing can be achieved but struggles with non-rigid transformations (e.g., changing pose) and retaining the original image layout structure. While Imagic (Kawar _et al._, 2023) addresses the need for non-rigid transformations by fine-tuning a pre-trained diffusion model to capture image-specific appearances, it requires test-time finetuning, which is not time-efficient for deployment. Moreover, relying solely on text as input lacks precise control. In contrast, we propose a novel training-free attention swapping scheme that enables precise personalization based on reference images, without the need for time-consuming finetuning.

### Exemplar-guided Image Editing

Exemplar-guided image editing covers a broad range of applications, and most of the works (Wang _et al._, 2019; Huang _et al._, 2018; Zhou _et al._, 2021) can be categorized as exemplar-based image translation tasks, conditioning on various information, such as stylized images (Liu _et al._, 2021; Deng _et al._, 2022; Zhang _et al._, 2022), layouts (Yang _et al._, 2023; Li _et al._, 2023; Jahn _et al._, 2021), skeletons (Li _et al._, 2023c), sketches/edges (Seo _et al._, 2023). With the convenience of stylized images, image style transfer (Liao _et al._, 2017; Zhang _et al._, 2020) receives extensive attention, replying on methods to build a dense correspondence between input and reference images, but it cannot deal with local editing. To achieve local editing with non-rigid transformation, conditions like bounding boxes and skeletons are introduced, but require drawing efforts from users, which sometimes are hard to obtain. A recent work (Yang _et al._, 2023a) poses exemplar-guided image editing task as an inpainting task with the mask and transfers the semantic content from the reference image to the source one, with the context intact. Unlike these works, we propose a more user-friendly scenario by conducting personalized subject swapping with only reference images and obtain high-quality editing results. DreamEdit (Li _et al._, 2023b) uses iterative inpainting to achieve subject replacement. Nevertheless, the existing approach fails to establish a comprehensive correlation between the source and target subjects. Conversely, our technique ensures that pivotal attributes, such as body gestures and facial expressions, remain unaltered.

## 3 Preliminary

Diffusion models are a type of generative model that operates probabilistically. In this process, an image is created by gradually eliminating noise from the target that is characterized by Gaussian noise. In the context of text-to-image generation, a diffusion model typically involves a process where an initial random image is gradually refined step by step, with each step guided by a learned model, until it becomes a realistic image. The changes to the image spread out and affect many pixels over time. Given an initial random noise \(_{T}(0,)\), the diffusion model gradually denoise \(_{t}\), which gives \(_{t-1}\).

Diffusion models are probabilistic generative models that learn to generate images by simulating a random process called a diffusion process. In the image generation process, the diffusion model gradually predicts the noise at the current diffusion step and denoises to get the final image. In this study, we utilize a pre-trained text-to-image diffusion model, Stable Diffusion (Rombach _et al._, 2022), which encodes the image into latent space and gradually denoises the latent variable to generate a new image. Stable Diffusion is based on a U-Net architecture (Ronneberger _et al._, 2015), which generates latent variable \(_{t-1}\) conditioned on a given text prompt \(P\) and the latent variable \(_{t}\) from the previous step \(t\):

\[_{t-1}=_{}(_{t},P,t)\] (1)

The U-Net consists of layers that include repetition of self-attention and cross-attention blocks. This study focuses on manipulating self-attention and cross-attention to achieve the task of personalized subject swapping.

## 4 The _Photoswap_ Method

Providing a few reference images of a personalized target subject \(O_{t}\), _Photoswap_ can seamlessly swap it with another subject \(O_{s}\) in a given source image \(I_{s}\). The _Photoswap_ pipeline is illustrated in Figure 2. To learn the visual concept of the target subject \(O_{t}\), we fine-tune a diffusion model with reference images and do object inversion to represent \(O_{t}\) using special token *. Then, to substitute the subject in the source image, we first obtain the noise \(z_{T}\)1 that can be used to reconstruct the source image \(I_{s}\). Next, through the U-Net, we obtain the needed feature map and attention output in the self-attention and cross-attention layers, including \(M\), \(A\), and \(\) (which we will introduce in Sec. 4.2). Finally, during the target image generation process that is conditioned on the noise \(z_{T}\) and the target text prompt \(P_{t}\), in the first \(\) steps, those intermediate variables (\(M\), \(A\), and \(\)) would be replaced with corresponding ones obtained during the the source image generation process. In the last

Figure 2: **The _Photoswap_ framework. Given several images of a new concept, the diffusion model first learns the concept and converts it into a token. The upper part is the generation process of the source image, while the bottom part is the generation process of target image. The initial noise feature \(z_{T}^{t}\) is copied from \(z_{T}^{t}\) of the source. The attention output and attention map in the source image generation process would be transferred to the target image generation process. The final feature \(z_{0}^{t}\) is decoded to output the target image. Refer to Sec. 4 for more details.**

(\(T-\)) steps, no attention swapping is needed and we can continue the denoising process as usual to obtain the final resulting image. Sec. 4.1 discusses the visual concept learning technique we used, and Sec. 4.2 details the training-free attention swapping method for controllable subject swapping.

### Visual Concept Learning

Subject swapping requires a thorough understanding of the subject's identity and specific characteristics. This knowledge enables the creation of accurate representations that align with the source subject. The subject's identity influences the composition and perspective of the image, including its shape, proportions, and textures, which affect the overall arrangement of elements. However, existing diffusion models lack information about the target subject (\(O_{t}\)) in their weights because the training data for text-to-image generation models does not include personalized subjects. To overcome this limitation and generate visually consistent variations of subjects from a given reference set, we need to personalize text-to-image diffusion models accurately. Recent advancements have introduced various methods, such as fine-tuning the diffusion model with distinct tokens associated with specific subjects, to achieve this "personalization" (Gal _et al._, 2023; Ruiz _et al._, 2023; Kumari _et al._, 2023). In our experiments, we primarily utilize DreamBooth (Ruiz _et al._, 2023) as a visual concept learning method. It's worth noting that alternative concept learning methods can also be effectively employed with our framework.

### Controllable Subject Swapping via Training-free Attention Swapping

Subject swapping poses intriguing challenges, requiring the maintenance of the source image's spatial layout and geometry while integrating a new subject concept within the same pose. This necessitates preserving the critical features in the source latent variable, which encapsulates the source image information, and leveraging the influence of the target image text prompt \(P_{t}\), which carries the concept token, to inject the new subject into the image.

The central role of the attention layer in orchestrating the generated image's layout has been well-established in prior works (Hertz _et al._, 2022; Cao _et al._, 2023; Tumanyan _et al._, 2023). To keep non-subject pixels intact, we orchestrate the generation of the target image \(I_{t}\) by transferring vital

Figure 3: **SVD visualization of self-attention maps. Each image’s attention map is resized to 64x64 at every layer, and we calculate the average map across all layers for all diffusion time steps. Most significant components are extracted with SVD and visualized. Remarkably, the visualized results demonstrate a strong correlation with the layout of the generated image. The top two rows are visualization about synthetic images while the bottom two rows are about real images.**

variables to the target image generation process. Here, we explore how distinct intermediate variables within the attention layer can contribute to a controllable generation in the context of subject swapping.

Within the source image generation process, we denote the cross-attention map as \(_{i}^{s}\), the self-attention map as \(_{i}^{s}\), the cross-attention output as \(_{i}^{s}\), and the self-attention output as \(_{i}^{s}\). The corresponding variables in the target image generation process are denoted as \(_{i}^{t}\), \(_{i}^{t}\), \(_{i}^{t}\), \(_{i}^{t}\), where \(i\) represents the current diffusion step.

In the self-attention block, the latent feature \(z_{i}\) is projected into queries \(_{i}\), keys \(_{i}\), and values \(_{i}\). We obtain the self-attention block's output \(_{i}\) using the following equation:

\[_{i}=_{i}_{i}_{i}=(_{i}_{i}{}^{T})\] (2)

where \(_{i}\) is the self-attention map, and \(_{i}\) is the feature output from the self-attention layer. The cross-attention block's output \(_{i}\) is:

\[_{i}=_{i}_{i}_{i}=(_{i}_{i}{}^{T})\] (3)

where \(_{i}\) is the cross-attention map. In both self-attention and cross-attention, the attention map \(_{i}\) and \(_{i}\) are correlated to the similarity between \(q_{i}\) and \(k_{i}\), acting as weights that dictate the combination of information in \(v_{i}\). In this work, the manipulation of the diffusion model focuses on self-attention and cross-attention within U-Net, specifically, swapping \(\), \(\), and \(\), while keeping \(\) unchanged.

**Self-attention map \(\)**, as it calculates the similarity within spatial features after linear projection, plays a pivotal role in governing spatial content during the generation process. As visualized in Figure 3, we capture \(\) during the image generation and highlight the leading components via Singular Value Decomposition (SVD). This visualization reveals a high correlation between \(\) and the geometry and content of the generated image. Further, when visualizing the full steps of the diffusion process (Figure 4), we discern that the layout information is mirrored in the self-attention from the initial steps. This insight underscores the necessity of initiating the swap early on to prevent the emergence of a new, inherent layout.

**Cross-attention map \(\)** is determined by both latent variable and text prompt, as in Equation 3, and \(_{i}^{s}v\) can be viewed as a weighted sum of the information from a text prompt. Copying \(_{i}^{s}\) to \(_{i}^{t}\) during the target image generation process improves the layout alignment between the source image and the target image.

**Self-attention output \(\)**, derived from the self-attention layer, encapsulates rich content information from the source image, independent of direct computation with textual features. Hence, replacing \(_{i}^{t}\) with \(_{i}^{s}\) enhances the preservation of context and composition from the original image. Our observations indicate that \(\) exerts a more profound impact on the image layout than the cross-attention map \(\).

Figure 4: **Self-attention map visualization across diffusion time steps. This representation reveals that the layout of the generated image is intrinsically embedded in the self-attention map from the initial steps. Consequently, to assert control over the layout, it is imperative to commence the attention swap at the earliest stages of the process.**

**Cross-attention output \(\)**, emanating from the cross-attention layer, embodies the visual concept of the target subject. It is vital to note that substituting cross-attention output \(_{i}^{s}\) with \(_{i}^{t}\) would obliterate all information from the target text prompt \(P_{t}\), as illustrated in Equation 3. Given that \(k_{i}^{t}\) and \(v_{i}^{t}\) are projections of target prompt embeddings, we retain \(_{i}^{s}\) unchanged to safeguard the target subject's identity.

Algorithm 1 provides the pseudo-code of our full _Photoswap_ algorithm.

``` Inputs: source image \(I_{s}\), reference images \(O_{t}\), source image text prompt \(P_{s}\), target image text prompt \(P_{t}\), diffusion model \(\) \(^{*},O_{t}\) Finetune diffusion model to include the new concept \(_{T}^{s} DDIMInersion(ImageEncoder(I_{s}),P_{s})\) Using DDIM to guarantee re-construction \(_{T}^{s}_{T}^{s}\) Using the same starting noise for\(i=T,T-1,..,1\)do \(^{*},_{i}^{s},_{i}^{s},_{i}^{s}_{^{*}}(_{i}^{s},P_{s},i)\) Denoise to get the attention output and map for source image \(_{i}^{t},_{i}^{t},_{i}^{s}(_{i}^{s},_{i}^{s},_{i}^{s},_{i}^{t},_{i}^{t}, _{i}^{t},i)\) \(^{*}_{^{*}}(_{i}^{t},P_{t},i,_{i}^{*},_{i}^{t},_{i}^{s})\) Denoise the updated attention map and output \(_{i-1}^{s} DDIMISampler(_{i}^{s},^{*})\) Sample next latent variable for source image \(_{i-1}^{t} DDIMISampler(_{i}^{t},^{*})\) Sample next latent variable for source image endfor \(I_{t}=ImageDecoder(_{0}^{t})\) return\(I_{t}\) functionSWAP(\(^{s},^{s},^{s},^{t},^{t},^{t},i\)) \(^{*}(i<_{})^{}^{s}:^{t}\) Control self-attention feature swap \(^{*}(i<_{M})?^{s}:^{t}\) Control self-attention Map swap \(^{*}(i<_{A})?^{s}:^{t}\) Control cross-attention map swap return\(^{s},^{*},^{s}\) endfunction ```

**Algorithm 1** The _Photoswap_ Algorithm

## 5 Experiments

### Implementation Details

For the implementation of subject swapping on real images, we require an additional process that utilizes an image inversion method, specifically the DDIM inversion (Song _et al._, 2020), to transform the image into initial noise. This inversion method relies on a reversed sequence of sampling to achieve the desired inversion. However, there exist inherent challenges when this inversion process is applied in text-guided synthesis within a classifier-free guidance setting. Notably, the inversion can potentially amplify the accumulated error, which could ultimately lead to subpar reconstruction outcomes. To fortify the robustness of the DDIM inversion and to mitigate this issue, we further optimize the null text embedding, as detailed in Mokady _et al._ (2023). The incorporation of this optimization technique bolsters the effectiveness and reliability of the inversion process, consequently allowing for a more precise reconstruction. Without further notice, the DDIM inversion in this paper is enhanced by null text embedding optimization.

During inference, we utilize the DDIM sampling method with 50 denoising steps and classifier-free guidance of 7.5. The default step \(_{A}\) for cross-attention map replacement is 20. The default step \(_{M}\) for self-attention map replacement is 25, while the default step for self-attention feature \(_{}\) replacement is 10. Please refer to Appendix A for analysis on attention swapping step, and Appendix E for detailed Unet layer swapping. Note that the replacement steps may change to some specific checkpoint. As mentioned in Sec. 4, the target prompt \(P_{t}\) is just source prompt \(P_{s}\) with the object token being replaced with the new concept token. For concept learning, we mainly utilize DreamBooth (Ruiz _et al._, 2023) to finetune a stable diffusion 2.1 to learn the new concept from 3 5 images. The learning rate is set to 1e-6. We use Adawm optimizer with 800 hundred training steps.

We finetune both the U-net and text encoder. The DreamBooth training takes around 10 minutes on a machine with 8 A100 GPU cards.

### Personalized Subject Swapping Results

Figure 5 showcases the effectiveness of our _Photoswap_ technique for subject swapping. Our approach excels at preserving crucial aspects such as spatial layout, geometry, and the pose of the original subject while seamlessly introducing a reference subject into the target image. Remarkably, even in cartoon images, our method ensures that the background remains intact during the subject change process. A notable example is the "cat" image, where our technique successfully retains all the intricate details from the source image, including the distinctive "Whiskers." This demonstrates our framework's ability to accurately capture and preserve fine-grained information during subject swapping. Please refer to Appendix C.

We further demonstrate the versatility of _Photoswap_ by showcasing its effectiveness in multiple subject swap and occluded object swap scenarios. As depicted in Figure 5(a), we present a source image featuring two sunglasses, which are successfully replaced with reference glass while preserving the original layout of the sunglasses. Similarly, in Figure 5(b), we observe a source image with a dog partially occluded by a suit. The resulting swapped dog wears a suit that closely matches the occluded region. These examples serve to highlight the robustness of our proposed _Photoswap_ method in handling various real-world cases, thereby enabling users to explore a broader range of editing possibilities. Refer Appendix F for identity control.

### Comparison with Baseline Methods

Personalized object swap is a new task and there is no existing benchmark. However, we could modify the existing attention manipulation based methods. More specifically, we used the same concept learning method DreamBooth to finetune the same stable diffusion checkpoint to inject the new concept. To fairly compare with our results, we modified the existing prompt-based editing method P2P (Hertz _et al._, 2022), an editing method based on diffusion models. Note that origin P2P only works on a pair of synthetic images, in our setting we use the same concept learning DreamBooth and fix the seed to allow concept swapping. On the other hand, PnP (Tumanyan _et al._, 2023) could

Figure 5: _Photoswap_ **results across various object and image domains, demonstrating its wide applicability.** From everyday objects to cartoons, the diversity in subject swapping tasks has showcased the versatility and robustness of our framework across different contexts.

also be implemented in a similar setting, however, we found PnP usually can not lead to satisfactory object swapping and may lead to a huge difference between the source image and the generated image. We suspect that it is because PnP is designed for image translation so it does not initiate the attention manipulation step from the beginning step. The qualitative comparison between _Photoswap_ and P2P+DreamBooth is shown in Figure 7. Check Appendix B for performance on other concept learning method. We observe that P2P with DreamBooth could achieve basic object swap, but it still suffers from background mismatching issues.

Human Evaluation.We conduct a human evaluation to study the editing quality by (1) _Which result better swaps the subject as the reference and keeps its identity_; (2) _Which result better preserves the background_; (3) _Which result has better overall subject-driven swapping_. We randomly sample examples and adopt Amazon MTurk2 to compare between two results. Please refer to Appendix D for details. To avoid potential bias, we hired 3 Turkers for each sample. Table 1 demonstrates the comparison between our _Photoswap_ and P2P. Firstly, more turkers (42.4%) denote that our _Photoswap_ better swaps the subject yet keeps its identity at the same time. Moreover, we can also preserve the background in the source image (39.3% _vs._ 30.2%), which is another crucial goal of this editing. In summary, _Photoswap_ precisely performs subject swapping and preserves the remaining part from the input, leading to an overall superiority (37.3%) to P2P (27.1%). Ku _et al._ (2023) shows _Photoswap_

Figure 6: _Photoswap_ **results on multi-subject and occluded subject scenarios.** The results show that _Photoswap_ can disentangle and replace multiple subjects at once. Also, _Photoswap_ can identify the target object while avoiding influencing the non-subject pixels.

Figure 7: **Qualitative comparison between P2P+DreamBooth and _Photoswap_.** We can observe that P2P+DreamBooth is capable of achieving subject swapping. However, it faces challenges in preserving both the background and the reference subject accurately, while for _Photoswap_, it is robust to handle various cases.

achieve SOTA performance by comparing with DreamEdit (Li _et al._, 2023b) and BLIP-Diffusion (Li _et al._, 2023a).

### Ethics Exploration

Like many AI technologies, text-to-image diffusion models can potentially exhibit biases reflective of those inherent in the training data (Sasha Luccioni _et al._, 2023; Perera and Patel, 2023). Given that these models are trained on vast text and image datasets, they might inadvertently learn and perpetuate biases, such as stereotypes and prejudices, found within this data. For instance, should the training data contain skewed representations or descriptions of specific demographic groups, the model may produce biased images in response to related prompts.

However, _Photoswap_ has been designed to mitigate bias within the generation process of a text-to-image diffusion model. It achieves this by directly substituting the depicted subject with the intended target. In Figure 8, we present our evaluation of face swapping across various skin tones. It is crucial to note that when there is a significant disparity between the source and reference images, the swapping results tend to homogenize the skin color. As a result, we advocate for the use of _Photoswap_ on subjects of similar racial backgrounds to achieve more satisfactory and authentic outcomes. Despite these potential disparities, the model ensures the preservation of most of the target subject's specific facial features, reinforcing the credibility and accuracy of the final image.

## 6 Conclusion

This paper introduces _Photoswap_, a novel framework designed for personalized subject swapping in images. To facilitate seamless subject photo swapping, we propose leveraging self-attention control by exchanging intermediate variables within the attention layer between the source image and reference images. Despite its simplicity, our extensive experimentation and evaluations provide compelling evidence for the effectiveness of _Photoswap_. Our framework offers a robust and intuitive solution for subject swapping, enabling users to effortlessly manipulate images according to their preferences. In the future, we plan to further advance the method to address those common failure issues to enhance the overall performance and versatility of personalized subject swapping.