# Hongsheng Liu\({}^{4}\), Zidong Wang\({}^{4}\), Jian-Xun Wang\({}^{3}\), Ji-Rong Wen\({}^{1}\), Hao Sun\({}^{1,*}\), Yang Liu\({}^{5,*}\)

P\({}^{2}\)C\({}^{2}\)Net: Pde-Preserved Coarse Correction Network for Efficient Prediction of Spatiotemporal Dynamics

Qi Wang\({}^{1}\), Pu Ren\({}^{2}\), Hao Zhou\({}^{1}\), Xin-Yang Liu\({}^{3}\), Zhiwen Deng\({}^{4}\), Yi Zhang\({}^{4}\), Ruizhi Chengze\({}^{4}\),\({}^{1}\)Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China

\({}^{2}\)Department of Civil and Environmental Engineering, Northeastern University, Boston, MA, USA

\({}^{3}\)Department of Aerospace and Mechanical Engineering, University of Notre Dame, Notre Dame, IN, USA

\({}^{4}\)Huawei Technologies, Shenzhen, China

\({}^{5}\)School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China

Emails: qi_wang@ruc.edu.cn (Q.W.); haosun@ruc.edu.cn (H.S.); liuyang22@ucas.ac.cn (Y.L.)

Corresponding authors

###### Abstract

When solving partial differential equations (PDEs), classical numerical methods often require fine mesh grids and small time stepping to meet stability, consistency, and convergence conditions, leading to high computational cost. Recently, machine learning has been increasingly utilized to solve PDE problems, but they often encounter challenges related to interpretability, generalizability, and strong dependency on rich labeled data. Hence, we introduce a new Pde-Preserved Coarse Correction Network (P\({}^{2}\)C\({}^{2}\)Net) to efficiently solve spatiotemporal PDE problems on coarse mesh grids in small data regimes. The model consists of two synergistic modules: (1) a trainable PDE block that learns to update the coarse solution (i.e., the system state), based on a high-order numerical scheme with boundary condition encoding, and (2) a neural network block that consistently corrects the solution on the fly. In particular, we propose a learnable symmetric Conv filter, with weights shared over the entire model, to accurately estimate the spatial derivatives of PDE based on the neural-corrected system state. The resulting physics-encoded model is capable of handling limited training data (e.g., 3-5 trajectories) and accelerates the prediction of PDE solutions on coarse spatiotemporal grids while maintaining a high accuracy. P\({}^{2}\)C\({}^{2}\)Net achieves consistent state-of-the-art performance with over 50% gain (e.g., in terms of relative prediction error) across four datasets covering complex reaction-diffusion processes and turbulent flows.

## 1 Introduction

Complex spatiotemporal dynamical systems are pivotal and commonly seen in numerous fields such biology, meteorology, fluid mechanics, etc. The behaviors of these systems are primarily governed by partial differential equations (PDEs), conventionally solved by numerical methods . However, direct numerical simulation (DNS) demands in-depth knowledge of the underlying physics, and the efficacy of numerical solutions is intricately linked to the resolution of mesh grids and time steps. High-resolution spatiotemporal grids are essential for accurate and convergent calculations, yet leading to substantial computational costs. For instance, simulating the flow field around a large aircraft  involves creating over millions of mesh nodes and may consume vast simulation time even on high performance computing. Additionally, any changes in initial and boundary conditions (I/BCs) or design parameters necessitate recalculations, further compounding the complexity.

Recently, tremendous efforts have been placed on machine learning for data-driven simulation of these systems [7; 8; 9], demonstrating promising potential. These methods do not require the _a priori_ knowledge of physics and, meanwhile, help bypass some traditional constraints, e.g., the smallest size of mesh grid and time step to guarantee solution accuracy, stability and convergence . However, they typically face issues of poor interpretability, weak generalizability and strong dependency of rich labeled data. Their performance deteriorate significantly particularly in small data regimes.

Embedding prior physics knowledge into the learning process has demonstrated effective to overcome the aforementioned issues. A brute-force way lies in creating regularizers (e.g., the residual form of PDEs and I/BCs) as "soft" penalty in the loss function, e.g., the family of physics-informed neural works (PINNs) [11; 12; 13; 14; 15; 16; 17]. However, such a strategy has limited scalability and generalizability, and the solution accuracy relies largely on a proper selection of loss weight hyperparameters. Embedding physics explicitly into the network architecture, which imposes "hard" constraints such as physics-encoded recurrent convolutional neural network (PeRCNN) [18; 19], possesses better generalizability as well as offers better convergence and flexibility for model training without the need of fine-tuning hyperparameters. Nevertheless, existing methods fail to handle coarse mesh grids and suffer from instability issues especially for long-range prediction of dynamics. Hybridizing classical numerical schemes and neural networks, e.g., the learned interpolation (LI) model , can enable accelerated simulation on coarse mesh grids with satisfied accuracy. Yet, since the numerical part is non-trainable, such models still require rich labeled data to retain accuracy.

To tackle these critical challenges, we introduce the P\({}^{2}\)C\({}^{2}\)Net model for efficient prediction of spatiotemporal dynamics on coarse mesh grids in small training data regimes. Specifically, a trainable PDE block (a white box) is designed to learn the coarse solution at low resolution, where the temporal marching of system states is handled by a fourth-order Runge-Kutta (RK4) scheme. We also propose a learnable symmetric Conv filter for more accurate estimation of spatial derivatives on coarse grids, as required in PDE block. A neural network (NN) block, which serves as a correction module, is further introduced to correct the coarse solution, restoring information lost due to reduced resolution. We also encode BC into the solution via a padding strategy. Our primary contributions are threefold:

* We propose a new physics-encoded correction learning model (P\({}^{2}\)C\({}^{2}\)Net) to efficiently predict complex spatiotemporal dynamics on coarse mesh grids. The model requires only a small set of training data and possesses plausible generalizability.
* We introduce a structured Conv filter that preserves symmetry to improve the estimation accuracy of coarse spatial derivatives required in the solution updating process, which makes the PDE block trainable with flexibility of handling coarse grids.
* P\({}^{2}\)C\({}^{2}\)Net achieves consistent state-of-the-art performance with at least 50% gain (e.g., in terms of relative prediction error) across four datasets covering complex reaction-diffusion (RD) processes and turbulent flows, simultaneously retaining accuracy and efficiency.

## 2 Related work

Numerical methods.Conventionally, PDE systems are solved by classical numerical methods such as finite difference , finite volume , finite element , and spectral methods . These methods often require fine mesh grids and reasonable time stepping to meet stability, consistency and convergence conditions. When dealing with large scale simulations or inverse analyses, the computational costs remain remarkably high.

Deep learning methods.Given sufficient labeled training data, deep learning has been recently applied to solve PDE problems. Representative approaches include Conv-based NN models [9; 21], U-Net , ResNet , graph neural networks [24; 25], and Transformer-based models [26; 27; 28]. In addition, neural operators such as DeepONet , multiwavelet-based model (MWT) , Fourier neural operator (FNO) , and their variants [30; 31; 32] have been designed to directly learn mappings between function spaces, making them particularly well-suited for modeling PDE systems. Diffusion models have also been employed for prediction of spatiotemporal dynamics .

Physics-aware learning methods.Recently, physics-aware deep learning has demonstrated great potential in modeling spatiotemporal dynamics under conditions of small training data. This paradigm can be divided into two categories based on the way of embedding PDE information: (1) _physics-informed_, and (2) _physics-encoded_. The former formalizes PDE soft constraints including equationsand I/BCs via loss regularization on point-wise or mesh-based NNs (e.g., PINN [11; 12; 13; 17], PhytoNet , PhyCRNet , PhySR , etc.), while the latter imposes hard constraints via encoding PDE structures (e.g., equations, I/BCs, law of thermodynamics, symmetry) into NN architectures such as PeRCNN [18; 19], TiGNN , and EquNN . Other related works include the PDE-Net models [39; 40] with designed convolution kernels approximating differential operators, thereby modeling the dynamics of the system.

Hybrid learning methods.The hybrid learning scheme represents a novel research direction that has emerged in recent years, which integrates numerical methods with NNs. The resulting solver can leverage a variety of classical numerical methods such as finite difference (e.g., PPNN , numerical discretization learning ), finite volume (e.g., LI  and TSM , and spectral methods (e.g., machine-learning-augmented spectral solver ). These approaches can operate on coarse grids, enabling faster simulations compared with traditional numerical solvers while retaining accuracy. However, since the numerical part is non-trainable, such models generally require rich labeled data.

## 3 Methodology

### Problem formulation

Let us consider a spatiotemporal dynamical system governed by the general form of PDEs:

\[}{ t}-(,^{2}, ,\,,^{2}, ;)=,\] (1)

where \((,t)^{n}\) denotes the \(n\)-dimensional physical state within spatiotemporal domain \([0,T]\); \(/ t\) the first-order time derivative; \(\) a linear/nonlinear function; \(^{n}\) the Nabla operator; \(\) the PDE parameters (e.g., the Reynolds number \(Re\)); \(\) the external force (e.g., \(=\) for source-free cases). Besides that, we define the initial condition (IC) as \((,_{t};,t=0)=\) and the boundary condition (BC) as \((,,; )=\), where \(\) denotes the boundary of \(\).

Our aim is to develop a _learnable coarse model_ that accelerates the simulation and prediction of spatiotemporal dynamics based on a minimal set of sparse data (e.g., low-resolution data down-sampled across space and time). The learned model is expected to achieve high solution accuracy and superior generalizability over various PDE scenarios, including ICs, force terms, and PDE parameters.

### Network architecture

Herein, we introduce the P\({}^{2}\)C\({}^{2}\)Net architecture, as shown in Figure 1, taking the simulation of Navier-Stokes (NS) flows as an example. The model is composed of four blocks, namely, the state variable correction block, the learnable PDE block, the Poisson block, and the NN block. Note that the network architecture is flexible and features a Poisson block that solves for the pressure term \(p\), which is absent in other cases.

#### 3.2.1 The flow of data

In Figure 1 (**a**), the network architecture includes two paths: the upper path computes the coarse solution using a learnable PDE block, while the lower path is incorporated into the network to correct the solution on a coarse grid with a Poisson block and a NN block. The data flow operates as follows: (1) the network accepts \(_{k}\) as input and processes it by the PDE block on the upper path, where the PDE block computes the residual of the governing equation. A filter bank, defined as a learnable filter with symmetry constraints, calculates the derivative terms based on the corrected solution (produced by the correction block). These terms are combined into an algebraic equation (a learnable form of \(\)). This process is incorporated into the RK4 integrator for solution update. (2) In the lower path, \(_{k}\) is first corrected by the correction block, and \(p_{k}\) is computed by the Poisson block. Inputs, including solution states \(\{_{k},p_{k}\}\) and their derivative terms, forcing term, and Reynolds number, are fed into the NN block. The output from this block serves as a correction for the upper path. (3) The final result \(_{k+1}\) is obtained by combining the outputs from both the upper and lower paths. During the gradient back-propagation process, the NN block learns to correct the coarse solution output of the PDE block on the fly, and ensures that their combined results more closely approximate the ground truth solution.

#### 3.2.2 RK4 integration scheme

Similar to standard numerical solvers, the goal is to predict the solution at every time step satisfying the underlying PDEs given specific I/BCs. Here, we aim to address the challenge of spatiotemporal dynamics evolution on coarse grids (e.g., low resolution). Given the coarse solution at timestep \(t_{k}\), denoted by \(_{k}\), we expect the model yielding an accurate prediction of \(_{k+1}\), _v.i.z._,

\[_{k+1}=_{k}+_{t_{k}}^{t_{k+1}}[( (},),^{2}(},), ,(},),^{2}(},),;)+()]d\] (2)

where \(\) (the PDE block) denotes a learnable form of \(\), which is discussed in Section 3.2.3; \(}\) represents the coarse grid coordinates. We herein employ the RK4 scheme as the integrator for time marching of the dynamics, offering the fourth-order accuracy (\(( t^{4})\)), where \( t=t_{k+1}-t_{k}\) is the coarse time step. More details on RK4 can be found in Appendix Section B.3.

#### 3.2.3 Learnable PDE Block

We assume that the PDE formulation in Eq. (1) is given, e.g., the explicit expression of \(\) is known. With coarse mesh grids and large time stepping, numerical methods such as finite difference (FD) tend to diverge. This issue becomes more pronounced with greater grid coarsening. Hence, we propose a learnable PDE block (depicted in Figure 1(**c**)), denoted by \(\), to approximate \(\) on coarse grids, so as to enable the coarse simulation simultaneously retaining efficiency and accuracy, expressed as

\[(_{k},_{k}^{2},,_{k},^{2}_{k},;) (_{k},_{k}^{2},,_{k},^{2}_{k},;)\] (3)

where \(_{k}\) denotes the coarse solution at time \(t_{k}\); \(}_{k}\) the corresponding neural-corrected coarse solution state, obtained by the correction block shown in Figure 1(**a**) and (**c**), used for estimation of spatial derivatives, e.g., \(}_{k}=(_{k})\). Here, \(}\) represents a learnable Nabla operator composed of a Conv filter with symmetric constraint (e.g., an enhanced FD kernel for numerical approximation of spatial derivatives). Through the RK4 integration, we are then able to predict the coarse solution for the next time step. Note that the PDE parameters \(\) can be set as trainable if unknown. Despite information loss at the low resolution, such a learnable PDE block allows for more accurate derivative estimation on coarse grids and ensures better adherence of the updated coarse solution to underlying PDEs. Clearly, such a block adds a fully interpretable "white box" to the overall network architecture.

Figure 1: Schematic of P\({}^{2}\)C\({}^{2}\)Net for learning Navier-Stokes flows. (**a**), Overall model architecture. (**b**), Poisson block. (**c**), learnable PDE block. (**d**), NN block. (**e**), Poisson solver. (**f**), Symbol notations. (**g**), Conv filter with symmetric constraint.

Correction block:The correction block takes a NN to correct the coarse solution. In particular, we choose FNO  as the neural correction operator performed on the coarse solution field in such a block. In the Fourier space, FNO decomposes the input data into components with specific frequencies, processes each component separately, and then applies Fourier transform to restore the updated spectral features back to the physical domain. The layer-wise update can be expressed as:

\[^{l+1}(})=(^{l}^{l}( })+(()^{l}) (}))\] (4)

where, \(^{l}(})\) denotes the \(l\)-th layer latent feature map on the coarse grids \(}\). Note that \(^{0}(})=(_{k})\), in which \(\) is a local transformation function that lifts \(_{k}\) to a higher dimensional representation. Here, \(()()=(_{} ())\) denotes a kernel integral transformation of the latent feature map \(\), which encompasses Fourier transform, spectral filtering and convolution in the frequency domain \(_{}\), and inverse Fourier transform; \(\) the network parameters; \(()\) the GELU activation function; \(^{l}\) the weights of a linear transformation. Given an \(L\)-layer FNO, the corrected coarse solution reads \(}_{k}=(^{L}(}))\), where \(\) is a local projection function. Details of the FNO correction block are found in Appendix Section B.1.

Conv filter with symmetric constraint:Based on FD schemes, spatial derivatives can be approximated by central difference stencils represented by convolution kernels . Such an approach often requires fine mesh grids to ensure accuracy; otherwise on coarse grids, there exists solution divergence issue. To this end, we leverage our understanding of FD stencils and propose a Conv filter with symmetric constraints to improve the accuracy of spatial derivative approximation on coarse mesh grids. As shown in Figure 1(**g**), the filter weights are transformed into a \(5 5\) matrix \(\) with 7 learnable parameters (Table S1 demonstrates the significant differences in results across various kernel sizes when applied to the Burgers dataset.), which satisfies symmetry, to estimate the first-order derivative along the horizontal direction (e.g., the vertical direction takes \(^{T}\)), _v.i.z._,

\[=a_{1}&a_{4}&a_{7}&-a_{4}&-a_{1}\\ a_{2}&a_{5}&-2a_{7}&-a_{5}&-a_{2}\\ a_{3}&a_{6}&0&-a_{6}&-a_{3}\\ -a_{2}&-a_{5}&2a_{7}&a_{5}&a_{2}\\ -a_{1}&-a_{4}&-a_{7}&a_{4}&a_{1}\] (5)

Although the number of learnable parameters in the Conv kernel is limited, the coarse derivatives can still be accurately approximated after the model is trained (see the ablation study in Section 4.2). The structured filter is designed for Conv operation which satisfies the Order of Sum Rules stated in Lemma 1 (see Appendix Section A for more details).

**Lemma 1**: _A 2D filter \(_{m m}\) has sum rules of order \(=(_{1},_{2})\), where \(_{+}^{2}\), provided that_

\[_{k_{1}=-}^{}_{k_{2}=-}^{ {m-1}{2}}k_{1}^{_{1}}k_{2}^{_{2}}g[k_{1},k_{2}]=0\] (6)

_for all \(=(_{1},_{2})_{+}^{2}\) with \(||:=_{1}+_{2}<||\) and for all \(_{+}^{2}\) with \(||=||\) but \(\). If this holds for all \(_{+}^{2}\) with \(||<A\) except for \(\) with certain \(_{+}^{2}\) and \(||=B<A\), then we say \(g\) to have total sum rules of order \(A\{B+1\}\)._

**Corollary 1**: _The filter \(\) we designed in Eq. (5) has the sum rules of order (1, 0). By adjusting the parameters in \(\), e.g., \(a_{7} 0\), \(a_{6}+8a_{5} 0\), it satisfies the total sum rules of order \(5\{2\}\), and achieves an approximation to the first-order derivative with the fourth-order accuracy. For example, for a smooth function \((x,y)\) and small perturbation \(>0\), we have :_

\[_{k_{1}=-}^{}_{k_{2}=- }^{}g[k_{1},k_{2}](x+ k_{1},y+  k_{2})=C+( ^{4}),\ \  0.\] (7)

The proof of Corollary 1 can be found in Appendix Section A. Given that the order of sum rules is closely related to the order of vanishing moments in the wavelet theory, Lemma 1 ensures that the filter not only maintains high sensitivity to local changes in the feature but also effectively suppresses irrelevant low-frequency components, thereby enhancing the estimation accuracy of the first derivative . Furthermore, Corollary 1 further guarantees that the designed filter can approximate the first-order derivative with up to the fourth-order accuracy by properly adjusting the trainable parameters.

BC encoding:To ensure that the predicted solution complies with the given BCs, while also retaining the shape of the feature map after Conv operations, we employ a BC hard encoding method . In particular, we consider periodic BCs in this work and apply padding padding, as shown in Figure 2, on the predicted solutions. Such an encoding technique not only ensures the compliance of the predicted solution with the BCs, but also enhances the solution's accuracy.

#### 3.2.4 Poisson block

When solving NS equations, there exists the pressure term \(p\) in the governing PDEs. However, for incompressible flows, the pressure can be inferred by solving a Poisson equation. We designed this block to achieve this aim, the poisson equation namely, \( p=(,)\), where \((,)=-2(u_{y}v_{x}-u_{x}v_{y})+ \) for 2D problems and the subscripts denote spatial differentiation. Hence, we leverage the spectral method to estimate the pressure quantity (see Appendix Section B.2), as depicted in Figure 1(**e**), which updates \(p_{k}\) based on \((_{k},_{k})\). The resulting Poisson block (Figure 1(**b**)) efficiently derives the pressure field from the velocity field and external force on the fly, streamlining computations without the need for labeled training data of pressure.

#### 3.2.5 NN block

It is noted that the predictions by the learnable PDE block on coarse grids may encounter instability issue due to error accumulation over time, especially in scenarios involving long-range rollout prediction. Hence, we propose an additional NN block to consistently correct the coarse solution predicted by the PDE block on the fly, restoring information lost due to reduced resolution. This module can be any trainable NN model, such as FNO , DenseCNN , or UNet [13; 45]. In particular, we consider FNO as the NN block, with more details found in Appendix Section B.1. However, the NN block is not always necessary. For cases of simpler systems, such as the Burgers equation, the learnable PDE block alone can perform well where the NN block can be omitted.

#### 3.2.6 Model generalization

We herein introduce the setup of the model's generalizability over various PDE scenarios, including ICs, force terms, and PDE parameters (e.g., the Reynolds number \(Re\)). The time marching in our network design inherently integrates ICs, automatically ensuring generalization over ICs given a well trained model. We embed the force term in the learnable PDE Block (naturally in the PDE formulation) shown in Figure 1(**c**), and meanwhile incorporate it as a feature map into the NN Block as part of the input (see Figure 1(**c**)). This enables the joint learning of force feature variations for generalization. In addition, the Reynolds number (\(Re\)) is incorporated by creating a 2D feature map embedding (e.g., for the 2D case) by introducing two trainable vectors \(\) and \(\), namely, \(Re_{}=1/Re()\), in both the PDE Block and the the NN Block. Such an approach can correct the error propagation of the diffusion term in the PDE Block caused by coarse grids, and enhance the model's ability of generalization across different \(Re\)'s.

## 4 Experiments

We evaluate the performance of P\({}^{2}\)C\({}^{2}\)Net against several baseline models across diverse PDE systems, including fluid flows and RD processes. The results have demonstrated the superiority of our model in terms of solution accuracy and generalizability thanks to the unique design of embedding PDEs into the network. The source codes and data are found at https://github.com/intell-sci-comput/P2C2Net (PyTorch) and https://gitee.com/intell-sci-comput/P2C2Net.git (MindSpore).

Figure 2: Periodic BC padding.

### Setup

**Datasets.** We consider four PDE datasets, including 2D Burgers, FitzHugh-Nagumo (FN), Gray-Scott (GS), and NS equations. Each dataset exhibits intricate spatiotemporal patterns, presenting significant challenges for prediction on coarse grids. High-order FD and finite volume (FV) methods are utilized to generate these datasets. We segment the training samples based on different rollout lengths. Additionally, the simulated high-resolution data \(^{n_{t} n H W}\) is down-sampled across space and time to low-resolution counterparts \(}^{n^{}_{t} n H^{} W^ {}}\) where \(n^{}_{t}<n_{t}\), \(H^{}<H\), \(W^{}<W\). The low-resolution data serves as labels for training. Moreover, for each PDE case, only 3\(\)5 sets of data trajectories are used for training while 10 trajectories are applied to test the model performance. The summary of datasets and training configuration is shown in Table 1. More details regarding datasets and their simulations can be found in the Appendix (see Table S2 and Section E).

   Dataset & Numerical & Spatial & Time Steps & \# of Training & \# of Testing & Rollout \\  & Method & Grid & (Temporal Grid) & Trajectories & Trajectories & Steps \\  Burgers & FD & \(100^{2} 25^{2}\) & \(400\) & 5 & 10 & 20 \\ FN & FD & \(128^{2} 64^{2}\) & \(5500 1375\) & 3 & 10 & 32 \\ GS & FD & \(128^{2} 32^{2}\) & \(4000 1000\) & 3 & 10 & 50 \\ NS & FV & \(2048^{2} 64^{2}\) & \(153600 4800\) & 5 & 10 & 32 \\   

Table 1: Summary of datasets and training implementations. Note that “\(\)” denotes the downsampling process from the original resolution (simulation) to the low resolution (training).

Figure 3: An overview of the comparison between our P\({}^{2}\)C\({}^{2}\)Net and baseline models, including error distributions (left), error propagation curves (middle), and predicted solutions (right). (**a**)-(**d**) show the qualitative results on Burgers, GS, FN, and NS equations, respectively. These PDE systems are trained with grid sizes of 25\(\)25, 32\(\)32, 64\(\)64, and 64\(\)64 accordingly.

### Evaluation metrics.

Four metrics are used to assess the model's performance: Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Normalized Absolute Difference (MNAD), and High Correction Time (HCT). The definition of each metric is listed in Appendix Section D.

**Model training.** Given the low-resolution training data, we aim to learn the evolution of spatiotemporal dynamics on coarse grids with bigger time stepping. This learning task is formulated as an auto-regressive rollout problem, where the models are only constrained by a low-resolution data loss. The loss function is defined as \(()=_{i=1}^{B}_{j=1}^{N}MSE (}_{ij},_{ij})\), where \(}_{ij}\) denotes the rollout-predicted coarse solution for the \(j\)-th sample in the \(i\)-th batch, \(_{ij}\) the corresponding label, \(B\) and \(N\) the number of batches and the batch size, and \(\) the trainable parameters. The detailed network parameters and training configurations are provided in Appendix Section B.

**Baseline models.** To validate the superiority of the proposed P\({}^{2}\)C\({}^{2}\)Net model, we conducted comparisons with various baseline models, including FNO , UNet , PeRCNN , DeepONet , and Learned Interpolation (LI) . The descriptions and training setup of the baseline models are provided in Appendix Section C.

### Main results

Figure 3 presents the results of comparison between P\({}^{2}\)C\({}^{2}\)Net and baselines, including error distribution, error propagation, and predicted trajectories. Moreover, Table 2 provides the quantitative model performance results.

**Burgers Equation.** Generally, our P\({}^{2}\)C\({}^{2}\)Net and baseline models (PeRCNN and FNO) all capture the evolving dynamics and provide acceptable results as shown in the right part of Figure 3(a). However, our method shows a notably lower error level compared with baselines, as emphasized in the error distribution and error propagation presented in the left and middle parts of Figure 3(**a**). Moreover, Table 2 substantiates this observation, revealing a significant improvement in our model's performance compared with the baseline models from a minimum of 93.4% to a maximum of 211.7%. We further conducted boundary condition generalization tests, as detailed in Appendix Section G.1.

**GS Equation.** The primary challenge of this dataset lies in its sparsity and the intricate patterns it presents, as depicted in Figure 3(**b**) (right). Each baseline model struggles to accurately predict the trajectories, especially the UNet model demonstrating significant divergence. Nevertheless, our method exhibits superior stability and is capable of learning the underlying dynamics. This is further validated by the error analysis presented in Figure 3(**b**), where our P\({}^{2}\)C\({}^{2}\)Net model outperforms the baselines by one to two orders of magnitude. Table 2 shows a comprehensive summary of the performance metrics, with our model achieving an improvement of over 91% across all evaluations for the GS equation.

**FN Equation.** This RD system is another classic but challenging case due to its two-scale fast-slow evolving patterns. As illustrated in the predicted snapshots of Figure 3(**c**), the baseline models can capture the global patterns but struggle with the local details. Our method shows superiority in learning the multi-scale features. Moreover, the error analysis demonstrates that our P\({}^{2}\)C\({}^{2}\)Net achieves errors one to two orders of magnitude lower than those of the baseline models, with an improvement ranging from 53.8% to 77.5% compared to the best baseline (see Table 2).

   Case & Model & RMSE & MAE & MNAD & HCT (s) \\   & FNO & 0.0980 & 0.0762 & 0.062 & 0.3000 \\  & UNet & 0.3316 & 0.2942 & 0.2556 & 0.0990 \\  & DeepONet & 0.2522 & 0.2106 & 0.1692 & 0.0020 \\  & PeRCNN & 0.0967 & 0.1828 & 0.1875 & 0.4492 \\   & P\({}^{2}\)C\({}^{2}\)Net (Ours) & **0.0064** & **0.0046** & **0.0037** & **1.4000** \\  & Promotion (\(\)) & 93.4\% & 94.0\% & 94.0\% & 211.7\% \\   & FNO & NaN & NaN & NaN & 354 \\  & UNet & NaN & NaN & NaN & 4 \\  & DeepONet & 0.3921 & 0.2670 & 0.2670 & 852 \\  & PeRCNN & 0.1586 & 0.0977 & 0.0976 & 954 \\   & P\({}^{2}\)C\({}^{2}\)Net (Ours) & **0.0135** & **0.0062** & **0.0062** & **2000.0** \\  & Promotion (\(\)) & 91.5\% & 93.7\% & 93.6\% & 109.6\% \\   & FNO & 0.8935 & 0.5447 & 0.2593 & 3.5000 \\  & UNet & 0.1730 & 0.0988 & 0.0407 & 6.5000 \\  & DeepONet & 0.5474 & 0.3737 & 0.1779 & 5.1528 \\  & PeRCNN & 0.5703 & 0.2258 & 0.1075 & 5.3750 \\   & P\({}^{2}\)C\({}^{2}\)Net (Ours) & **0.0390** & **0.0149** & **0.0071** & **10.000** \\  & Promotion (\(\)) & 77.5\% & 84.9\% & 84.9\% & 53.8\% \\   & FNO & 1.0100 & 0.7319 & 0.0887 & 2.5749 \\  & UNet & 0.8224 & 0.5209 & 0.0627 & 3.9627 \\   & LI & NaN & NaN & NaN & 3.5000 \\   & PeRCNN & 1.2654 & 0.9787 & 0.1192 & 0.6030 \\    & P\({}^{2}\)C\({}^{2}\)Net (Ours) & **0.3533** & **0.1993** & **0.0235** & **7.1969** \\   & Promotion (\(\)) & 57.0\% & 61.7\% & 62.5\% & 81.6\% \\  

Table 2: Quantitative results of our model and baselines. For the case of Burgers, GS, and FN, our model inferred the test set’s upper time limits of 1.4 s, 2000 s, and 10 s, respectively, as the trajectories of dynamics get stabilized. We take these limits in HCT to facilitate evaluation metrics calculation.

NS Equation.We evaluate our method on an NS dataset with a Reynolds number of 1000, a benchmark dataset known for its significant challenges. As shown in Figure 3**(d)**, the snapshots produced by the baseline models at \(t\) = 7 s exhibit incorrect dynamical patterns, especially the LI model showing divergence. The average test error of our model is at least an order of magnitude lower than those of the baselines. Furthermore, P\({}^{2}\)C\({}^{2}\)Net consistently outperforms the baselines throughout the error propagation process. Table 2 further validates the superior performance of our model, with improvements across all metrics of at least 57%. Additionally, we examine the physical properties of the learned fluid dynamics, such as energy spectra. As illustrated in Figure 4, the energy spectra curve of P\({}^{2}\)C\({}^{2}\)Net closely aligns with that of the ground truth, demonstrating its effectiveness in capturing high-frequency features.

**Generalization test.** Taking the NS equation as an example, P\({}^{2}\)C\({}^{2}\)Net is able to generalize to different external forces \(\) and Reynolds numbers \(Re\). Our model is trained with \(=(4y)_{x}-0.1\) and \(Re=1000\), where \(_{x}=^{T}\). We conduct the generalization test under six external force scenarios and four distinct Reynolds numbers \(Re=\{200,500,800,200\}\). As depicted in Figure 5**(a)**, the error distribution presents a stable performance with errors generally below 0.1 across six distinct external forces. Figure 5**(b)** shows the acceptable error propagation with a gradual upward trend, which aligns with the long-term rollout outcomes of our model. For the generalization test on Reynolds numbers, Figure 5**(c)** demonstrates satisfactory and robust error levels for all Reynolds numbers, and smaller errors are observed when the Reynolds numbers are closer to those in the training set, as validated by the error propagation curves shown in Figure 5**(d)**. Overall, our P\({}^{2}\)C\({}^{2}\)Net model exhibits robust generalization capabilities and stable performance across test samples for external forces, and Reynolds numbers. Further details on the snapshots are provided in Appendix Figure S3.

**Robustness to noisy/incomplete data.** Using the Burgers equation as an example, we introduced Gaussian noise of varying scales during the training process and observed its minimal impact on the results. The results are presented in Table S3, which indicate that our model is robust to the training data noise and maintains the HCT (high correlation time) metric without reduction.

Moreover, for the Burgers case, the time steps of the training data consist of only 28% of the inference steps in the test data. Namely, our training data itself is incomplete, and the missing data accounts for 72% of the test set. Based on this sparse dataset, we further randomly reduced the training data by 20% to simulate data incompleteness (e.g., randomly deleting data according to the rollout step size to make the trajectory incomplete, where the specific size can be found in Table 1 in the paper). The results of this experiment are shown in Table S4, where values are averaged over 10 test sets. We can observe that, after making the data sparser, our model performance slightly decreases, which means that our model is capable of handling scenarios with incomplete data.

Figure 4: Energy spectra.

Figure 5: The error distribution and propagation of P\({}^{2}\)C\({}^{2}\)Net for generalization over different external forces (\(\), \(\)) and Reynolds numbers (\(\), \(\)).

**Ablation study.** To assess the impact of different components on model performance, we design and conduct a series of ablation experiments based on the Burgers equation. The experimental setups include: (1) Model 1, replacing symmetric convolutions with regular convolutions; (2) Model 2, using convolution kernels with finite difference stencils instead of symmetric convolutions; (3) Model 3, removing the Correction Block; (4) Model 4, substituting RK4 integration with first-order Euler methods; and (5) the full P\({}^{2}\)C\({}^{2}\)Net architecture. All experiments are performed using the same training data and model hyperparameters as previously defined.

As shown in Table 3, the network performs poorly when the Fourier block is removed, with error levels two orders of magnitude higher compared to using the complete P\({}^{2}\)C\({}^{2}\)Net architecture. This emphasizes the critical role of the physics-encoded variable correction learning method. Configurations using traditional finite difference stencils as kernels also exhibit similarly poor performance due to the large numerical errors when approximating derivatives caused by coarse grids. Moreover, removing symmetry constraints leads to a significant increase in errors, emphasizing the importance of symmetric features for model convergence, especially in scenarios with limited data. Using the Euler scheme for time stepping instead of RK4 results in reduced stability and increased error accumulation, leading to a decrease in performance compared to the complete P\({}^{2}\)C\({}^{2}\)Net. Overall, the results confirm that the physics-encoded variable correction learning method and convolution filters satisfying symmetry are indispensable components of the network framework.

## 5 Conclusion

This paper presents a physics-encoded variable correction learning method designed to embed prior knowledge on coarse grids for solving nonlinear dynamic systems. This approach enables the model to focus on fitting equations, ensuring excellent generalization capability and interpretability. It introduces a convolutional filter that follows symmetry constraints, requiring only seven learnable parameters to adaptively compute the derivatives of system state variables corresponding to the flow field on a coarse grid. This allows the model to learn spatiotemporal dynamics with limited data. Our model has been trained and tested on four different nonlinear dynamic systems, achieving the SOTA results. Even with minimal data, it can generalize to different initial conditions, external force terms, and PDE parameters. In summary, our model effectively adapts to various nonlinear dynamic systems. Moreover, the trained model shows remarkable speedup for simulation under the same condition of accuracy, shown in Figure 6.

Despite demonstrating excellent generalization ability, the model faces two challenges. Firstly, the model is based on regular grids with periodic boundaries, limiting its ability to solve problems on irregular grids. We will explore graph structures or coordinate transformations to handle irregular grids and incorporate special boundary treatment methods to adapt to various boundary conditions. Secondly, we expect to expand our research from 2D problems to 3D dynamical systems.

## Impact statement

The aim of this work is to develop a novel physics-encoded learning scheme to accelerate predictions and simulations of spatiotemporal dynamical systems. This method can be applied to various fields, including weather forecasting, turbulent flow prediction, and other simulation tasks. Our research is solely intended for scientific purposes and poses no potential ethical risks.