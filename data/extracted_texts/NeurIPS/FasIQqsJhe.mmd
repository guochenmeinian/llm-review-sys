# Towards In-context Scene Understanding

Ivana Balazevic

David Steiner

Nikhil Parthasarathy

Relja Arandjelovic

Olivier J. Henaff

Google DeepMind

Equal contribution. Current affiliation: NYU CNS, work done while interning at Google DeepMind. Correspondence to {balazevic, davidsteiner, henaff}@google.com.

###### Abstract

In-context learning--the ability to configure a model's behavior with different prompts--has revolutionized the field of natural language processing, alleviating the need for task-specific models and paving the way for generalist models capable of assisting with any query. Computer vision, in contrast, has largely stayed in the former regime: specialized decoders and finetuning protocols are generally required to perform dense tasks such as semantic segmentation and depth estimation. In this work we explore a simple mechanism for in-context learning of such scene understanding tasks: nearest neighbor retrieval from a prompt of annotated features. We propose a new pretraining protocol--leveraging attention within and across images--which yields representations particularly useful in this regime. The resulting _Hummingbird_ model, suitably prompted, performs various scene understanding tasks _without modification_ while approaching the performance of specialists that have been finetuned for each task. Moreover, _Hummingbird_ can be configured to perform new tasks much more efficiently than finetuned models, raising the possibility of scene understanding in the interactive assistant regime.

## 1 Introduction

In natural language processing (NLP), the pretrain-finetune paradigm has long been the dominant way of acquiring domain-specific knowledge and adapting a model's behavior to a particular task (e.g. question answering, natural language inference, summarization). More recently and predominantly due to the increase in model and dataset sizes, large language models have exhibited impressive, task-agnostic emergent capabilities [11; 37; 68], where a single model, given an appropriate prompt, can perform a wide range of downstream tasks without any change in its parameters.

While large-scale supervised and self-supervised pretraining in vision has yielded powerful encoders which capture useful semantics [15; 22; 31; 32; 35; 42; 43], applying these representations to solve downstream tasks has typically required bespoke decoders and end-to-end finetuning. The most readily applicable representations are trained for image-text alignment, enabling zero-shot classification  and image-based dialogue [2; 19; 80; 81], however these models are inherently limited by the coarseness of natural language outputs. Attempts have been made at casting fine-grained tasks (e.g. detection) as language modeling , but dense scene understanding tasks requiring millions of outputs do not lend themselves to this format. Indeed, deficiencies in fine-grained spatial understanding have been well documented in visual language models [36; 45; 64; 79].

In this work, we investigate the components required for in-context learning of scene understanding tasks, which we characterize along three axes: generality, data efficiency, and fast adaptation. To this end, we expand the well-known non-parametric nearest neighbor (NN) retrieval method [7; 9; 15; 75] to support a variety of dense scene understanding tasks. This retrieval-based decoding mechanism has the advantage of requiring no task-specific parameters or finetuning, thus enabling effortless adaption of standard encoders (e.g. ResNet  or ViT ) to any dense task of interest, as well as faster research iteration by allowing for simpler and more efficient model selection during pretraining.

We further show that the NN scene understanding capabilities of canonically pretrained vision transformers (such as MAE  and DINO ) vary greatly, despite similar finetuned performance. We find two pretraining components to yield reliable gains: (1) a simple modification to the standard self-supervised pretraining protocol, termed _contextual pretraining_, which performs _attention across images_ by updating the spatial representation of each image with features retrieved from a memory bank, and (2) a spatial _attention pooling_ mechanism (as opposed to the more standard mean pooling or the [CLS] token), which computes _attention within an image_ to summarize the (contextualized) spatial grid of features into a single image-level representation to be fed into the self-supervised objective. We showcase the benefits of this approach in a standard contrastive framework, demonstrating large gains in NN scene understanding over prior pretraining methods.

Finally we find that our model, named _Hummingbird_ due to its fast adaptation properties: **(1)** yields general-purpose representations which perform well in non-parametric semantic segmentation and monocular depth estimation using NN retrieval, **(2)** approaches the performance of fully finetuned models on some tasks, and **(3)** is more data-efficient and faster to adapt to new tasks when equipped with NN retrieval, compared to other pretraining methods and decoding mechanisms. By adapting quickly and efficiently to new tasks specified on the fly, _Hummingbird_ raises the possibility of vision systems providing general-purpose assistants with in-context scene understanding.

## 2 Related Work

**Retrieval-based perception.** Non-parametric evaluation has a long history with roots in the exemplar theory of human cognition [3; 38; 50] and case-based theories of artificial intelligence [1; 58]. In computer vision, non-parametric methods combined with simple features such as SIFT  and HOG  saw early success in image classification , shape matching [7; 8; 59], scene recognition [66; 76], and image parsing . Exemplar-SVMs  showcased the versatility of non-parametric methods by retrieving arbitrary meta-data (such as segmentations, geometry, even 3D models) from training examples. We leverage these insights with modern architectures and training paradigms coupled with dense retrieval.

**Retrieval-based training.** To improve retrieval-based performance at test time, retrieval-based classifiers [69; 74] shape their representations for this task, enabling fine-grained classification from coarse supervision. While not explicitly training for it, DINO  witnessed NN classification abilities emerge from self-supervised training of vision transformers, enabling global retrieval tasks such as landmark recognition and copy detection. In , tracking abilities emerge after pretraining on a colorization task via retrieval from reference frames of a video. Retrieval has also been proposed as a means of enriching the positive pairs used in self-supervised contrastive learning . These works differ from ours in that they encode and retrieve global representations of entire images, in contrast to the local inferences required by dense scene understanding tasks.

**Fast adaptation.** A number of methods have tackled the problem of adapting to newly specified tasks, most often from the perspective of meta-learning. For example, matching networks  and MAML  learn to solve new classification and reinforcement learning tasks specified on the fly. Architectural innovations, such as image prompting [4; 39; 82] and adapter layers [27; 55] have also facilitated transfer to new image recognition tasks. While fast adaptation to dense scene understanding tasks has been less studied, image inpainting [6; 73] and VTM  have made progress in this direction, particularly in the low-data regime. These approaches differ from ours in that they achieve fast adaptation by training on related dense tasks and (in the case of VTM) adapt to downstream tasks with task-specific weight updates and learned similarity functions. In contrast, we maintain the simplicity of pure retrieval-based approaches by adapting to new downstream tasks without modifying any model parameters, and the generality of self-supervised approaches by learning representations from generic pretraining data with no dense annotations.

**Self-supervised learning.** Methodologically, our representation learning method is most similar to self-supervised learning (SSL) techniques. Similarly to NLP, image-based SSL has witnessed great success in recent years, notably with the advent of contrastive methods [14; 15; 16; 23; 33], self-distillation [12; 18; 28], and masked auto-encoding . Due to their conceptual simplicity, we base our method on standard contrastive baselines such as SimCLR  and MoCo . Image-based SSL techniques have since been tailored to learning representations which transfer well to scene understanding tasks [13; 33; 70; 78], and although they have been shown to support zero-shot object discovery [34; 61], they generally still require task-specific decoders and end-to-end finetuning.

## 3 Method

The following sections describe the retrieval-based scene understanding decoding protocol (Section 3.1), followed by the contextual pretraining method (Section 3.2) and the self-supervised (Section 3.3) and supervised learning objectives (Section 3.4). We use subscripts \(_{i}\) to differentiate between representations and superscripts \(^{j}\) to denote spatial locations within a representation.

### Retrieval-based scene understanding

A general-purpose image representation should perform well across a variety of scene understanding tasks out-of-the-box, i.e. without modifying its parameters. To test whether a representation satisfies this condition, we extend the standard image-level nearest neighbor (NN) retrieval [7; 9] decoding mechanism to dense, patch-level retrieval (with patch size set to \(16 16\) across all models in this work). Given a prompt composed of training images from the downstream task and their corresponding labels \(\{(_{i},_{i}),i=1,...,N,_{i}\!\!^{H^{}  W^{} C}\}\), our aim is to enable a pretrained image encoder \(f_{}\) to make predictions about a new image \(\) from the test set. In tasks considered in this work, labels \(_{i}\) are spatial maps of either class labels \(_{i}\!\!^{H^{} W^{}}\) (e.g. for semantic segmentation, where \(\) is the space of all classes) or scalars \(_{i}\!\!^{H^{} W^{}}\) (e.g. for monocular depth estimation).

We encode each prompt image into a spatially flattened map \(_{i}\!=\!f_{}(_{i})\!\!^{H W D}\), where a feature \(^{j}_{i}\!\!^{D}\) at a spatial location \(j\) is aligned with the local label \(^{j}_{i}\) created by averaging the pixel labels \(^{j}_{i}\) of a patch. We then sample a subset of features and local labels for each image, which form the keys and values of the memory bank \(\!=\!\{(^{j}_{i},^{j}_{i}),i\!=\!1,...,N,j\!\! \}\) (see Appendix A.1 for details on the sampling distribution \(\)). In the following, we do not distinguish between entries from different images, and use a single integer \(j\) to index into the memory bank: \(\!=\!\{(^{j},^{j}),j\!=\!1,...,||\}\).

Given a test image \(\), we form a representation \(\!=\!f_{}()\) and use each spatial feature \(^{i}\) as a query to cross-attend over the memory bank with temperature \(\). The cross-attention weights are then used to combine the corresponding labels and form a local prediction \(}^{i}\):

\[s^{i,j}=^{i},^{j}}{\|^{i }\|\|^{j}\|},^{i}=}(^{i}), }^{i}=_{j}a^{i,j}^{j}.\] (1)

Equation 1 defines the cross-attention operation as \(}^{i}\!=\!(^{i},^{j},^{j})\). The final prediction \(}\) is simply the concatenation of local predictions \(}^{i}\) upsampled to the original image size via bilinear interpolation. As a result, nearest neighbor retrieval allows a simple image encoder to perform scene understanding tasks _without any decoders_ or _parameter adaptation_ (finetuning or otherwise) to the downstream dataset. The mechanism is also entirely agnostic to the format of the labels, enabling it to perform tasks as diverse as semantic segmentation and depth estimation.

Figure 1: **In-context scene understanding with nearest neighbor retrieval.** On the left, we provide the system with a “prompt” of annotated images. On the right, we ask the system to describe new query images. The network computes dense features for each location and uses them to query features computed from the prompt. The labels associated with the nearest prompt features are then aggregated to make predictions about the query. Note that the system makes no assumptions about the nature of the labels, and as such can be used to solve a variety of different scene understanding tasks in-context. The nearest neighbors and predictions in this example are computed with our _Hunmingbird_ model.

### Contextual pretraining

Memory retrieval allows an image encoder to perform various tasks by combining labels of nearby examples. To ensure that a model will perform well in this regime, we propose to train it in a similar manner, by enforcing its representation to be expressed as a combination of representations of nearby examples. Over the course of training, we populate a memory bank \(_{p}=\{(_{i},_{i}),i=1,...,|_{p}|\}\) with spatially averaged keys and values computed from training images \(_{i}\) from previous batches:

\[_{i}=f_{}(_{i})^{H W D},_ {i}=_{j=1}^{H W}_{i}^{j}^{D}, _{i}=_{}(_{i})^{D},\] (2)

where we use an MLP as the value head \(_{}\) (see Appendix B for implementation details). We then form a representation \(\!=\!f_{}()\) of a new training image \(\) and use each spatial feature \(^{i}\) to attend over the memory bank and compute an update \(}^{i}\!=\!(^{i},_{j},_{j})\). Each feature is "contextualized" as, where \(_{}\) is a linear layer and \(\) a weighting parameter. The contextualized image representation \(\!=\!g_{}(,\ _{p})\) is simply the concatenation of local features \(^{i}\). Note that the pretraining memory bank \(_{p}\) is discarded at test time and differs from the test time memory bank \(\) described in Section 3.1, allowing for straightforward comparison of our representations \(f_{}()\) to those trained without the memory bank.

### Self-supervised objective

While contextual pretraining updates representations by attending across images, we hypothesize that learning to attend within images will also enable fine-grained predictions required by dense tasks. To that end, we train representations to locate the most distinctive part of an image using a combination of attention pooling and contrastive learning. Following [16; 28], we construct different views of unlabeled images \(\) through random data augmentation \(_{1}_{1}(),_{2}_{2}()\), see Appendix C.1. Each view is encoded as \(_{i}\!=\!f_{}(_{i})\) and further contextualized with the mechanism described above as \(_{i}\!=\!g_{}(_{i},\ _{p})\!\!^{H  W D}\) (see Figure 2). Following , we compute attention pooled representations \(}_{i}\!\!^{D}\) using masks \(_{i}\) derived from a lightweight attention module \(a_{}\), which we augment with an additional value head \(_{}\). Pooled features are then used to compute projections \(_{i}^{}\):

\[_{i}=}(a_{}(_{i})),}_{i}=_{j=1}^{H W}m_{i}^{j}\ _{}(_{i}^{j}),_{i}^{}=p_{}(}_{i}).\] (3)

Finally, following [20; 28; 65], each view forms predictions \(q_{}(_{i}^{})\) about the other view's targets \(_{j}^{}\), which are computed with the same architecture and a different set of weights \(\) which vary more slowly (see Appendix C.2). The online weights \(\) are optimized using a standard contrastive loss:

\[_{}^{ij}(;)=-(_ {i}^{})_{j}^{})}{(q_{}(_{i}^{}) _{j}^{})+_{k}(q_{}(_{i}^{}) _{k}^{})}.\] (4)

Figure 2: _Hummingbird model components._

### Retrieval-based supervised objective

Given the availability of large labeled datasets, and noting that correctly designed supervision does not necessarily hurt generalization , we explore the use of label-supervision for learning representations that perform well in dense NN retrieval. While supervision is typically added with a linear classifier atop average pooled features [32; 43; 62], we instead use it to constrain contextual pretraining and further align our training methodology with NN retrieval [69; 74]. Specifically, we expand the memory bank \(_{p}\) to include the labels: \(_{p}^{}=\{(_{i},_{i},_{i}),i=1,...,|_{p}^{}|\}\) and query it with attention pooled features \(}_{i}\!\!^{D}\) (see Equation 3) to form predictions \(}_{i}\!=\!(}_{i},_{j},_{j})\). We then use the standard softmax cross entropy loss \(_{}^{i}(}_{i},_{i})\), which added to the self-supervised objective of Equation 4, forms the total loss \(^{ij}=_{}^{ij}+(_{ {CE}}^{i}+_{}^{j})\), with supervised weight \(\). Note that the memory bank \(_{p}^{}\) is only used during training and the added supervision relates to a global image classification task, not the downstream pixel-level tasks.

## 4 Experiments

We demonstrate the generality of _Hummingbird_ representations through retrieval-based scene understanding on several downstream tasks (Section 4.1): semantic segmentation on PASCAL VOC  and ADE20K  with mean IoU (mIOU) as metric, and monocular depth estimation on NYUv2  with root-mean-square error (RMSE) as metric. We further show that, in the low-data regime (Section 4.2) and when looking at adaptation speed (Section 4.3), _Hummingbird_ with NN retrieval outperforms other pretraining techniques and decoding mechanisms, including end-to-end finetuning. Section 4.4 compares the performance of fully finetuned _Hummingbird_ with prior work.

### Retrieval-based scene understanding

We consider the performance of learned representations in the retrieval-based scene understanding setup described in Section 3.1 across architecture (ViT-B and ViT-L) and dataset (ImageNet-1k and

  & & & &  & Depth pred. \\ Method & Encoder & Params (M) & Dataset & PASCAL \(\) & ADE20K \(\) & NYUv2 \(\) \\  Supervised\({}^{}\) & ViT-B & 86 & IN1K & 35.1 & 13.8 &.913 \\ DINO  & ViT-B & 86 & IN1K & 55.9 & 21.8 &.793 \\ MoCo-v3  & ViT-B & 86 & IN1K & 37.2 & 14.6 &.771 \\ MAE  & ViT-B & 86 & IN1K & 6.6 & 3.3 &.981 \\ LOCA  & ViT-B & 86 & IN1K & 57.5 & 18.5 &.880 \\ _Hummingbird_ & ViT-B & 86 & IN1K & 70.5 & 28.3 & **.718** \\ _Hummingbird_++ & ViT-B & 86 & IN1K & **72.1** & **30.5** &.738 \\ Supervised\({}^{}\) & ViT-B & 86 & IN22K & 63.5 & 28.0 & 1.07 \\ MAE\({}^{}\) & ViT-B & 86 & IN22K & 9.8 & 4.2 &.968 \\ LOCA  & ViT-B & 86 & IN22K & 56.4 & 16.8 &.829 \\ _Hummingbird_ & ViT-B & 86 & IN22K & 73.5 & 30.7 &.706 \\ _Hummingbird_++ & ViT-B & 86 & IN22K & **76.2** & **34.1** & **.695** \\  \\ CLIP\({}^{}\) & NFNet-F6  & 438 & ALIGN & 57.2 & 25.0 &.844 \\ Supervised\({}^{}\) & NeXt-XL  & 1300 & IN22K & 58.9 & 25.5 &.791 \\ Supervised\({}^{}\) & ViT-L & 307 & IN22K & 65.8 & 26.1 &.860 \\ MAE  & ViT-L & 307 & IN1K & 8.0 & 3.6 &.934 \\ LOCA  & ViT-L & 307 & IN22K & 59.5 & 17.6 &.912 \\ _Hummingbird_ & ViT-L & 307 & IN22K & 76.9 & 35.0 & **.671** \\ _Hummingbird_++ & ViT-L & 307 & IN22K & **77.3** & **35.8** & **.671** \\ 

Table 1: **In-context scene understanding.** All models are pretrained on source data in a supervised or self-supervised manner, and applied to downstream datasets without modification. All downstream tasks are performed using a single mechanism, nearest neighbor retrieval. \({}^{}\)indicates our reproduction of external work, all other models were evaluated using publicly available checkpoints.

-22k ) scales, trained with supervision (_Hummingbird++_) or without (_Hummingbird_). Figure 1 shows an example prediction made by _Hummingbird_ with a ViT-B encoder on PASCAL VOC.

The top part of Table 1 shows an apples-to-apples comparison of _Hummingbird_ with existing methods for pretraining ViT-B encoders, where it outperforms all baselines by a large margin. We also note that _Hummingbird_ scales well with increasing the dataset size from ImageNet-1k to ImageNet-22k, which does not hold for all other methods (e.g. MAE, consistent with ). Further, training with supervision is generally beneficial, particularly for semantic segmentation. For an ablation on the impact of retrieval-based supervision on performance, see Appendix D.5.

The bottom part of Table 1 contains the best performing methods across architectures, showing a performance increase for _Hummingbird_ with encoder size. Note that results achieved by _Hummingbird_ retrieval on PASCAL VOC and ADE20K, without any finetuning, approach the performance of methods fully finetuned on each of those tasks with specialized decoders (see Table 4).

### Data-efficient retrieval-based scene understanding

In addition to adapting to downstream tasks with minimal (or ideally no) alterations to the model, a second ingredient for in-context learning is adaptation given only a limited number of examples.

We therefore evaluate the performance of _Hummingbird_ retrieval in the low-data regime, and compare it with other decoding techniques: linear probing and end-to-end finetuning (Figure 3). For PASCAL VOC, NN retrieval outperforms the end-to-end finetuning for up to \(1/8\) of the data (\(\)1300 images). For ADE20K the effect is less pronounced, however NN retrieval still exceeds end-to-end finetuning when given up to \(1/32\) of the data (\(\)600 images). _Hummingbird_ retrieval outperforms linear decoding

  &  &  \\ Method & Decoder & 1/128 (\(n\)=83) & 1/64 (\(n\)=165) & 1/128 (\(n\)=158) & 1/64 (\(n\)=316) \\  Supervised  & E2E FT & 41.8 & 53.8 & 10.8 & 14.3 \\ DINO  & E2E FT & 36.1 & 44.3 & 11.7 & 14.4 \\ MoCo-v3  & E2E FT & 19.9 & 33.4 & 4.6 & 7.9 \\ MAE  & E2E FT & 34.2 & 44.1 & 8.2 & 12.2 \\ LOCA  & E2E FT & 40.1 & 53.9 & 11.2 & 15.5 \\ _Hummingbird_ & NN & 50.5 & 57.2 & 11.7 & 15.1 \\ _Hummingbird++_ & NN & **52.4** & **57.3** & **12.7** & **16.4** \\ 

Table 2: **Data-efficient scene understanding.** After pretraining, models are adapted to downstream tasks on small amounts of data with end-to-end fine-tuning with a linear head (E2E FT) or with nearest neighbor retrieval (NN). \(n\) refers to the number of images a fraction represents. All runs are averaged over five different seeds, with standard deviation of the order of 0.04 / 0.10% for NN and 0.36 / 1.35% for E2E FTon PASCAL / ADE20K. Each method is trained with ViT-B on ImageNet-1k.

Figure 3: **Data efficiency of _Hummingbird_.** The model is evaluated with retrieval-based evaluation (“NN retrieval”), linear probing (“Linear + frozen”), or full finetuning (“Linear + E2E FT”).

on top of the frozen encoder in all cases. These results show that given an appropriately designed encoder, NN retrieval provides a data-efficient alternative to end-to-end finetuning, and is strictly more expressive than linear decoding.

Second, we verify the generality of these findings by comparing _Hummingbird_ retrieval to several other representation learning algorithms which transfer to the low-data regime with finetuning (Table 2, see Appendix D.1 for higher-data regime and additional analysis). For PASCAL VOC, _Hummingbird_ with the NN retrieval decoder outperforms the end-to-end finetuned version of all other techniques for both 1/128 (83 images) and 1/64 (165 images) of the data, which holds for both the purely self-supervised _Hummingbird_ and its supervised variant. For ADE20K, _Hummingbird_ is competitive with DINO  for 1/128 of the data (158 images) and outperformed by LOCA for 1/64 of the data (316 images), whereas _Hummingbird++_ outperforms all other models, demonstrating the benefit of retrieval-based supervision during pretraining. In summary, in the few-shot regime (e.g. \(\)100 images) relevant for in-context learning, _Hummingbird_ retrieval provides a compelling and robust alternative to end-to-end finetuning.

### Fast adaptation to downstream tasks

While _Hummingbird_ retrieval displays useful data-efficiency properties relative to fully finetuned methods, finetuning yields better performance when given access to the entire dataset. Yet even in this large-data regime, assistant systems must be quickly adaptable to new tasks. We thus evaluate the amount of computation required to reach good performance with the different decoding schemes from Section 4.2. All decoders are given the full training set and varying compute budgets. We titrate the amount of computation given to NN retrieval by partially populating the memory bank with fractions of the dataset. Figure 4 shows that 5 minutes (1 epoch through the downstream training set)

Figure 4: **Adaptation time of _Hummingbird_. The model is evaluated with retrieval-based evaluation (“NN retrieval”), linear probing (“Linear + frozen”), or full finetuning (“Linear + E2E FT”).**

  & &  &  \\  Method & Decoder & Frozen & E2E FT & Frozen & E2E FT \\  Supervised  & Linear & 61.5 & 66.3 & 27.6 & 15.1 \\ DINO  & Linear & 54.9 & 64.0 & 25.6 & 23.4 \\ MoCo-v3  & Linear & 41.2 & 4.8 & 14.6 & 3.2 \\ MAE  & Linear & 20.1 & 42.5 & 8.3 & 7.9 \\ LOCA  & Linear & 61.9 & 62.9 & 25.4 & 14.6 \\ _Hummingbird_ & NN & 70.5 & & 28.3 \\ _Hummingbird++_ & NN & **72.1** & & **30.5** \\ 

Table 3: **Fast adaptation to new scene understanding tasks. After pretraining, models are transferred to downstream tasks with the full dataset, but a small amount of computation: 1 epoch. Models perform the task either with a linear classifier (Frozen), end-to-end fine-tuning (E2E FT) or with our mechanism for in-context scene understanding (NN).**are sufficient to build a performant NN decoder (70% mIoU on PASCAL VOC, 28% on ADE20K). In contrast, given the same amount of time, end-to-end finetuning still exhibits performance near chance, despite benefitting from hyperparameter tuning of learning rates, weight decay, and warm-up length. While a linear classifier converges more quickly than finetuning, it saturates with a significantly lower performance than NN retrieval (50% mIoU on PASCAL VOC, 20% on ADE20K).

We also quantify these benefits in terms of relative convergence: on PASCAL VOC, NN retrieval reaches the performance of full finetuning after 3 minutes rather than 3 hours, and the performance of a linear classifier in 2 seconds rather than 3 minutes. For ADE20K, the speedups are smaller, but significant: 7 minutes rather than 30 minutes (relative to full finetuning), and 1 minute rather than 30 minutes (relative to the linear classifier). By making substantial gains in this near-real-time use case, we believe NN retrieval lays the groundwork for scene understanding in an interactive setting.

Table 3 compares _Hummingbird_ retrieval to other models (equipped with linear or end-to-end finetuned decoders) in the fast-adaptation regime (i.e. when given a single pass over the full downstream dataset): _Hummingbird_ retrieval outperforms all other pretraining techniques and decoding mechanisms on both PASCAL VOC and ADE20K.

### Fully finetuned scene understanding

Although the primary focus of this work is on fast and effortless adaption to downstream tasks, for completeness, we include a comparison of fully finetuned _Hummingbird_ with fully finetuned state-of-the-art models on the semantic segmentation task. We follow the finetuning protocol of MAE  and use UperNet  as a decoder. Table 4 shows that both _Hummingbird_ and _Hummingbird++_ are competitive with state-of-the-art when finetuned. Further analysis shows retrieval-based performance to be correlated with the finetuning performance (see Appendix D.2), paving the way for using retrieval-based evaluation as a model selection tool during training.

  & &  \\  Method & Encoder & Dataset & PASCAL \(\) & ADE20K \(\) \\  Random & ViT-B & IN1K & 29.1 & 21.1 \\ Supervised  & ViT-B & IN1K & 76.1 & 47.3 \\ DINO  & ViT-B & IN1K & 74.1 & 44.1 \\ MoCo-v3  & ViT-B & IN1K & 74.5 & 47.3\({}^{}\) \\ BEIT  & ViT-B & IN1K+DAALLE  & - & 47.1\({}^{}\) \\ MAE  & ViT-B & IN1K & 75.0 & 48.1\({}^{}\) \\ LOCA  & ViT-B & IN1K & 76.7 & 47.9 \\ _Hummingbird_ & ViT-B & IN1K & 80.0 & 44.9 \\ _Hummingbird++_ & ViT-B & IN1K & 81.2 & 44.9 \\ _Hummingbird_ & ViT-B & IN22K & 81.6 & 46.9 \\ _Hummingbird++_ & ViT-B & IN22K & **82.1** & **48.2** \\ 

Table 4: **Scene understanding with end-to-end finetuning.** After pretraining, models are equipped with task-specific decoders and finetuned for that task on the entire downstream dataset. \({}^{}\)indicates results are taken from , using UperNet  as the decoder. Results for all other baselines are taken from  and use the linear decoder from . For ViT-L results, see Appendix D.4.

  & &  & Depth pred. \\  Method & Pool. & Cont. & PASCAL \(\) & ADE20K \(\) & NYUv2 \(\) \\  MoCLR  & mean & ✗ & 38.6 & 4.9 & 1.01 \\ + cont. & mean & ✓ & 55.6 & 15.3 &.901 \\ + [CLS] & [CLS] & ✗ & 64.5 & 23.9 &.741 \\ + [CLS] + cont. & [CLS] & ✓ & 65.6 & 25.1 &.731 \\ + QK att.  + cont. & QK att. & ✓ & 68.7 & 26.3 &.728 \\ + QKV att. & QKV att. & ✗ & 68.0 & 27.4 &.742 \\ _Hummingbird_ & QKV att. & ✓ & **70.5** & **28.3** & **7.18** \\ 

Table 5: **Ablation of pretraining components.** Effect of training with spatial attention pooling (as opposed to mean pooling or a [CLS] token) and memory contextualization (“Cont.”) on performance. All models were pretrained with ViT-B on ImageNet-1k.

Analysis

**Ablating the pretraining components.** We perform an ablation of pretraining components required for adaptation to downstream tasks through NN retrieval in Table 5. We find attention pooling to yield superior performance compared to mean pooling or a [CLS] token. Both contextual pretraining and attention pooling separately lead to large performance improvements over a baseline MoCLR  model and best results are achieved when combining the two. Note that although spatial attention pooling was initially introduced in the context of video understanding , this work is the first to show its utility for downstream task adaptation in the NN retrieval setup. We further find that modifying it ("QK att") with a value head ("QKV att.") improves its performance across all tasks.

Effect of evaluation memory length \(||\).When transferring to downstream tasks with many training images (e.g. PASCAL VOC and ADE20K contain \( 10\)k and \( 20\)k images respectively, each image providing 100s of tokens), we see benefits of using large memory banks (e.g. \(||\) of the order of 1-10 million tokens, see Figure 6, right). Since this makes the cross-attention operation computationally intractable, we leverage powerful libraries for approximate NN search  to limit cross-attention (Equation 1) to a small set of nearest neighbors for each query (e.g. \(k=30\), see Appendix A.2 for details, where we find increasing \(k\) not to have significant impact on performance).

Figure 5 shows the relationship between evaluation memory length and the cost of the nearest neighbor lookup at inference time. For small-to-medium sized memory banks (0 to 1 million keys), the lookup cost is minimal (\( 30\) ms), meaning the system is still fast enough to be used for real-time

Figure 5: **Effect of memory bank length on nearest neighbor lookup at inference time. Inference time is for a single image. Lookups were done on a single Nvidia A100 GPU.**

Figure 6: **Effect of the pretraining (_left_) and evaluation (_right_) memory length on performance. All models were pretrained with ViT-B on ImageNet-22k. _Left_: Since the retrieval-based supervised objective is only defined for memory banks of non-zero length, for the purpose of this ablation we replace it with a simple linear classifier when \(|_{p}|=0\). _Right_: For downsample=False, we store representations of all patches into the memory bank. If downsample=True, we sample \(||/N\) patches per image (\(N\) is the length of the downstream training set), allowing for greater diversity.**

applications, such as segmenting videos at 30 frames per second. When scaling to very large memory banks of 10 million keys or more, the scaling tends to be linear. However, the absolute performance is likely still suitable for most applications: with a memory bank size of 10 million, the overhead from NN lookup is only 0.2 seconds for dense tasks.

Effect of pretraining memory length \(|_{p}|\).In contrast to retrieval-based evaluation, we find contextual pretraining to be remarkably memory-efficient: small memory banks (e.g. \(|_{p}|\) = 40k, see Figure 6, left for PASCAL VOC and Appendix D.3 for ADE20K) are sufficient to yield robust gains in retrieval-based scene understanding, adding a relatively small computational overhead to training the representation (e.g. +22% for \(|_{p}|\) = 40k). The module is agnostic to how the representation is trained and it benefits both self-supervised and supervised pretraining. Note that contextual pretraining is only present at training time and does not affect inference speed, and that pretraining and evaluation memory length are fully decoupled, allowing us to set them independently.

## 6 Conclusion

Inspired by impressive examples of in-context learning in language models, we investigate components necessary for in-context learning of dense scene understanding tasks in computer vision. To this end, we propose a simple non-parametric nearest neighbor retrieval mechanism--which is agnostic to the downstream task and requires no finetuning or specialized decoders--to serve as a general-purpose decoder which we use to evaluate models on semantic segmentation and monocular depth estimation tasks. We further propose _Hummingbird_, a pretraining method which benefits from attention across images (through contextual pretraining) and within an image (through spatial attention pooling) to produce image representations that can be easily configured to perform downstream tasks in a fast and data-efficient manner. By combining _Hummingbird_ as the encoder with NN retrieval as the decoder, we take an important step towards in-context learning for dense vision tasks.

## 7 Broader Impact and Limitations

Broader impact.In laying the groundwork for scene understanding methods to be used in the interactive regime, our work could potentially benefit general-purpose assistants that are seeing rapid adoption. While these may enable a host of beneficial applications, they suffer from the biases and potential harms associated with visual language models and large language models more generally.

Limitations.Despite offering large relative improvements compared to finetuning and linear classification in the low-data regime, the absolute performance of _Hummingbird_ when given less than 100 examples in the prompt is still far from perfect. To truly match the in-context learning abilities displayed in NLP, we would ideally need good performance from a handful of examples.

Further, given that retrieval-based scene understanding is task-agnostic, we leave expanding _Hummingbird_ nearest neighbor evaluation to other tasks (e.g. object detection) to future work. Certain tasks, such as image rotation or image flipping, are not currently amenable to our framework as we assume a spatial correspondence between features and their labels. Explicitly post-processing to smooth outputs across patches was also not explored in this paper. Although the set of nearest neighbors and their weights vary smoothly as a function of the query image representation, it should be noted that this smoothness is dependent on the input prompt, since the memory bank needs to be sufficiently diverse and dense to allow a linear combination of neighbors to be expressive enough to cover all possible labels.

Finally, while we have showcased the benefits of attention within and across images in a contrastive framework, we defer adding them to more recent approaches using advanced data curation  and self-distillation [15; 84] to future work.