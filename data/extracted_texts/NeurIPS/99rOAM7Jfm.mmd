# Noise-Aware Differentially Private Regression via Meta-Learning

Ossi Raisa

University of Helsinki

ossi.raisa@helsinki.fi &Stratis Markou

University of Cambridge

em626@cam.ac.uk &Matthew Ashman

University of Cambridge

mca39@cam.ac.uk &Wessel P. Bruinsma

Microsoft Research AI for Science

wessel.p.bruinsma@gmail.com &Marlon Tobaben

University of Helsinki

marlon.tobaben@helsinki.fi &Antti Honkela

University of Helsinki

antti.honkela@helsinki.fi &Richard E. Turner

University of Cambridge

ret26@cam.ac.uk

Equal contribution.

###### Abstract

Many high-stakes applications require machine learning models that protect user privacy and provide well-calibrated, accurate predictions. While Differential Privacy (DP) is the gold standard for protecting user privacy, standard DP mechanisms typically significantly impair performance. One approach to mitigating this issue is pre-training models on simulated data before DP learning on the private data. In this work we go a step further, using simulated data to train a meta-learning model that combines the Convolutional Conditional Neural Process (ConvCNP) with an improved functional DP mechanism of Hall et al. (2013) yielding the DPConvCNP. DPConvCNP learns from simulated data how to map private data to a DP predictive model in one forward pass, and then provides accurate, well-calibrated predictions. We compare DPConvCNP with a DP Gaussian Process (GP) baseline with carefully tuned hyperparameters. The DPConvCNP outperforms the GP baseline, especially on non-Gaussian data, yet is much faster at test time and requires less tuning.

## 1 Introduction

Deep learning has achieved tremendous success across a range of domains, especially in settings where large datasets are publicly available. However, in many impactful applications such as healthcare, the data may contain sensitive information about users, whose privacy we want to protect. Differential Privacy (DP; Dwork et al., 2006) is the gold standard framework for protecting user privacy, as it provides strong guarantees on the privacy loss incurred on users participating in a dataset. However, enforcing DP often significantly impairs performance. A recently proposed method to mitigate this issue is to pre-train a model on non-private data, e.g. from a simulator (Tang et al., 2023), and then fine-tune it under DP on real private data (Yu et al., 2021; Li et al., 2022; De et al., 2022).

We go a step further and train a _meta-learning_ model with a DP mechanism inside it (Figure 1). While supervised learning is about learning a mapping from inputs to outputs using a learning algorithm, in meta-learning we learn a learning algorithm directly from the data, by _meta-training_, enabling generalisation to new datasets during _meta-testing_. Our model is meta-trained on simulated datasets,each of which is split into a _context_ and _target_ set, learning to make predictions at the target inputs given the context set. At meta-test-time, the model takes a context set of real data, which is protected by the DP mechanism, and produces noise-aware predictions and accurate uncertainty estimates.

**Neural Processes.** Our method is based on neural processes [NPs; Garnelo et al., 2018a], a model which leverages the flexibility of neural networks to produce well-calibrated predictions in the meta-learning setting. The parameters of the NP are meta-trained to generalise to unseen datasets, while adapting to new contexts much faster than gradient-based fine-tuning alternatives [Finn et al., 2017].

**Convolutional NPs.** We focus on convolutional conditional NPs [ConvCNPs; Gordon et al., 2020], a type of NP that has remarkably strong performance in challenging regression problems. That is because the ConvCNP is translation equivariant [TE; Cohen and Welling, 2016], so its outputs change predictably whenever the input data are translated. This is an extremely useful inductive bias when modelling, for example, stationary data. The ConvCNP architecture also makes it natural to embed an especially effective DP mechanism inside it using the _functional mechanism_[Hall et al., 2013] to protect the privacy of the context set (Figure 1). We call the resulting model the DPConvCNP.

**Training with a DP mechanism.** A crucial aspect of our approach is training the DPConvCNP on non-sensitive data _with the DP mechanism in the training loop_. The mechanism involves clipping and adding noise, so applying it only during testing would create a mismatch between training and testing. Training with the mechanism eliminates this mismatch, ensuring calibrated predictions (Figure 2).

**Overview of contributions.** In summary, our main contributions in this work are as follows.

1. We introduce the DPConvCNP, a meta-learning model which extends the ConvCNP using the functional DP mechanism [Hall et al., 2013]. The model is meta-trained with the mechanism in place, learning to make calibrated predictions from the context data under DP.
2. We improve upon the functional mechanism of Hall et al.  by leveraging Gaussian DP theory [Dong et al., 2022], showing that context set privacy can be protected with smaller amounts of noise (at least 25% lower standard deviation in the settings considered in Figure 4). We incorporate these improvements into DPConvCNP, but note that they are also of interest in any use case of the functional mechanism.
3. We conduct a study on synthetic and sim-to-real tasks. Remarkably, even with relatively few context points (a few hundreds) and modest privacy budgets, the predictions of the DPConvCNP are surprisingly close to those of the non-DP optimal Bayes predictor. Further, we find that a single DPConvCNP can be trained to generalise across generative processes with different statistics and privacy budgets. We also evaluate the DPConvCNP by training it on synthetic data, and testing it on a real dataset in the small data regime. In all cases, the DPConvCNP produces well calibrated predictions, and is competitive with a carefully tuned DP Gaussian process baseline.

## 2 Related Work

Training deep learning models on public proxy datasets and then fine-tuning with DP on private data is becoming increasingly common in computer vision and natural language processing applications

Figure 1: Meta-training (left) and meta-testing (right) using our method. We train a model on multiple tasks with non-private (simulated or proxy) data to predict on target \((t)\) points using the context \((c)\) points. Crucially, by including a DP mechanism, which clips and adds noise to the data _during training_, the parameter updates (dashed arrow) teach the model to make well-calibrated and accurate predictions in the presence of DP noise. At test time, we deploy the model on real data using the same mechanism, which protects the context set with DP guarantees.

[Yu et al., 2021; Li et al., 2022; De et al., 2022; Tobaben et al., 2023]. However, these approaches rely on the availability of very large non-sensitive datasets. Because these datasets would likely need to be scraped from the internet, it is unclear whether they are actually non-sensitive [Tramer et al., 2024]. On the other hand, other approaches study meta-learning with DP during meta-training [Li et al., 2020; Zhou and Bassily, 2022], but do not enforce privacy guarantees at meta-test time.

Our approach fills a gap in the literature by enforcing privacy of the meta-test data with DP guarantees (see Figure 1), and using non-sensitive proxy data during meta-training. Unlike other approaches which rely on large fine tuning datasets, our method produces well-calibrated predictions even for relatively small datasets (a few hundred datapoints). In this respect, the work of Smith et al. (2018), who study Gaussian process (GP) regression under DP for the small data regime, is perhaps most similar to ours. However, Smith et al. (2018) enforce privacy constraints only with respect to the output variables and do not protect the input variables, whereas our approach protects both.

In terms of theory, there is fairly limited prior work on releasing functions with DP guarantees. Our method is based on the functional DP mechanism of Hall et al. (2013) which works by adding noise from a GP to a function to be released. This approach works especially well when the function lies in a reproducing kernel Hilbert space (RKHS), a property which we leverage in the DPConvCNP. We improve on the original functional mechanism by leveraging Gaussian DP theory of Dong et al. (2022). In related work, Alda and Rubinstein (2017) develop the Bernstein DP mechanism, which adds noise to the coefficients of the Bernstein polynomial of the released function, and Mirshani et al. (2019) generalise the functional mechanism beyond RKHSs. Jiang et al. (2023) derive Renyi differential privacy (RDP; Mironov, 2017) bounds for the mechanism of Hall et al. (2013).

## 3 Background

We start by laying the necessary background. In Section 3.1, we outline meta-learning and NPs, focusing on the ConvCNP. In Section 3.2 we introduce DP, and the functional mechanism of Hall et al. (2013). We keep the discussion on DP lightweight, deferring technical details to Appendix A.

### Meta-learning and Neural Processes

**Supervised learning.** Let \(\) be the set of datasets consisting of \((x,y)\)-pairs with \(x^{d}\) and \(y\). The goal of supervised learning is to use a dataset \(D\) to learn appropriate parameters \(\) for a conditional distribution \(p(y|x,)\), which maximise the predictive log-likelihood on unseen, randomly sampled test pairs \((x^{*},y^{*})\), i.e. \((,(x^{*},y^{*}))= p(y^{*}|x^{*},)\). Let us denote the entire algorithm that performs learning, followed by prediction, by \(\), that is \((x^{*},D)=p(|x^{*},^{*}),\) where \(^{*}=_{}(r,D)\). Supervised learning is concerned with designing a hand-crafted \(\), e.g. picking an appropriate architecture and optimiser, which is trained on a single dataset \(D\).

**Meta-learning.** Meta-learning can be regarded as supervised learning of the function \(\) itself. In this setting, \(D\) is regarded as part of a single training example, which means that a meta-learning algorithm can handle different \(D\) at test time. Concretely, in meta-learning, we have \(_{,}(x^{*},D)=p(|x^{*},,r_{}(D)),\) where \(r_{}\) is now a function that produces task-specific parameters, adapted for \(D\). The meta-training set now consists of a collection of datasets \((D_{m})_{m=1}^{M},\) often referred to as _tasks_. Each task is partitioned into a context set \(D^{(c)}=(^{(c)},^{(c)})\) and a target set \(D^{(t)}=(^{(t)},^{(t)})\). We refer to \(^{(c)}\)and \(^{(c)}\) as the _context inputs and outputs_ and to \(^{(t)}\) and \(^{(t)}\) as the _target inputs and outputs_. To meta-train a meta-learning model, we optimise its predictive log-likelihood, averaged over tasks, i.e. \(_{D}[(,,D)]=_{D}[_{,} (^{(t)},D^{(c)}(^{(t)})].\) Meta-learning algorithms are broadly categorised into two groups, based on the choice of \(r_{}\)(Bronskill, 2020).

Figure 2: Training our proposed model with a DP mechanism inside it, enables the model to make accurate well-calibrated predictions, even for modest privacy budgets and dataset sizes. Here, the context data (black) are protected with different \((,)\) DP budgets as indicated. The model makes predictions (blue) that are remarkably close to the optimal (non-private) Bayes predictor.

**Gradient based vs amortised meta-learning.** On one hand, gradient-based methods, such as MAML (Finn et al., 2017) and its variants (e.g. (Nichol et al., 2018)) rely on gradient-based fine-tuning at test time. Concretely, these let \(r_{}\) be a function that performs gradient-based optimisation. For such algorithms, we can enforce DP with respect to a meta-test time dataset by fine-tuning with a DP optimisation algorithm, such as DP-SGD (Abadi et al., 2016). While generally effective, such approaches can require significant resources for fine-tuning at meta-test-time, as well as careful DP hyper-parameter tuning to work at all. On the other hand, there are amortised methods, such as neural processes (Garnelo et al., 2018), prototypical networks (Snell et al., 2017), and matching networks (Vinyals et al., 2016), in which \(r_{}\) is a learnable function, such as a neural network. This approach has the advantage that it requires far less compute and memory at meta-test-time. In this work, we focus on neural processes (NPs), and show how \(r_{}\) can be augmented with a DP mechanism to make well calibrated predictions, while protecting the context data at meta test time.

**Neural Processes.** Neural processes (NPs) are a type of model which leverage the flexibility of neural networks to produce well calibrated predictions. A range of NP variants have been developed, including conditional NPs (CNPs; Garnelo et al., 2018), latent-variable NPs (LNPs; Garnelo et al., 2018), Gaussian NPs (GNPs; Markou et al., 2022), score-based NPs Dutordoir et al. (2023), and autoregressive NPs (Bruinsma et al., 2023). In this work, we focus on CNPs because these are ideally suited for our purposes, but our framework can be extended to other variants. A CNP consists of an _encoder_\(_{}\), and a _decoder_\(_{}\). The encoder is a neural network which ingests a context set \(D^{(c)}\) and outputs a representation \(r\) in some representation space \(\). Two concrete examples of such encoders are DeepSets (Zaheer et al., 2017) and SetConv layers (Gordon et al., 2020). The decoder is another neural network, with parameters \(\), which takes the representation \(r\) together with target inputs \(^{(t)}\) and produces predictions for the corresponding \(^{(t)}\). In summary

\[_{,}(^{(t)},D^{(c)})=_{}( ^{(t)},r), r=_{}(D^{(c)}).\] (1)

In CNPs, a standard choice, which we also use here, is to let \(_{,}(^{(t)},D^{(c)})\) return a mean \(_{,}(x^{(t)},D^{(c)})\) and a variance \(^{2}_{,}(x^{(t)},D^{(c)})\), to parameterise a predictive distribution that factorises across the target points \(y^{(t)}|x^{(t)}(_{,}(x^{(t)},D^{(c)}),^{2}_ {,}(x^{(t)},D^{(c)}))\). We note that our framework straightforwardly extends to more complicated \(_{,}(^{(t)},D^{(c)})\). To train a CNP to make accurate predictions, we can optimise a log-likelihood objective Garnelo et al. (2018) such as

\[(,)=_{D}[( _{m}^{(t)}|_{,}(_{m}^{(t)},D^{(c)}),^{2}_{, }(^{(t)},D^{(c)}))],\] (2)

where the expectation is taken over the distribution over tasks \(D\). This objective is optimised by presenting each task \(D_{m}\) to the CNP, computing the gradient of the loss with back-propagation, and updating the parameters \((,)\) of the CNP with any first-order optimiser (see alg. 1). This process trains the CNP to make well-calibrated predictions for \(D^{(t)}\) given \(D^{(c)}\). At test time, given a new \(D^{(c)},\) we can use \(_{,}\) which can be queried at arbitrary target inputs, to obtain corresponding predictions (alg. 2).

**Convolutional CNPs.** Whenever we have useful inductive biases or other prior knowledge, we can leverage these by building them directly into the encoder and the decoder of the CNP. Stationarity is a powerful inductive bias that is often encountered in potentially sensitive applications such as time series or spatio-temporal regression. Whenever the generating process is stationary, the corresponding Bayesian predictive posterior is TE (Foong et al., 2020). ConvCNPs leverage this inductive bias using TE architectures (Cohen and Welling, 2016; Huang et al., 2023).

**ConvCNP encoder.** To achieve TE, the ConvCNP encoder produces an \(r\) that is itself a TE function.

Specifically, \(_{}\) maps the context \(D^{(c)}=((x_{n}^{(c)},y_{n}^{(c)}))_{n=1}^{N}\) to the function \(r:^{2}\)

\[r(x)=r^{(d)}(x)\\ r^{(s)}(x)=_{n=1}^{N}1\\ y_{n}^{(c)}(^{(c)}}{}),\] (3)

where \(\) is the Gaussian radial basis function (RBF) and \(=\{\}\). We refer to the two channels of \(r\) as the _density_\(r^{(d)}\) and the _signal_\(r^{(s)}\) channels, which can be viewed as a smoothed version of \(D^{(c)}\). The density channel carries information about the inputs of the context data, while the signal channel carries information about the outputs. This encoder is referred to as the SetConv.

**ConvCNP decoder.** Once \(r\) has been computed, it is passed to the decoder which performs three steps. First, it discreises \(r\) using a pre-specified resolution. Then, it applies a CNN to the discretised signal, and finally it uses an RBF smoother akin to Equation (3) to make predictions at arbitrary target locations. The aforementioned steps are all TE so, composing them with the TE encoder produces a TE prediction map (Bronstein et al., 2021). The ConvCNP has universal approximator properties and produces state-of-the-art, well-calibrated predictions (Gordon et al., 2020).

### Differential Privacy

Differential privacy (Dwork et al., 2006; Dwork and Roth, 2014) quantifies the maximal privacy loss to data subjects that can occur when the results of analysis are released. The loss is quantified by two numbers, \(\) and \(\), which bound the change in the distribution of the output of an algorithm, when the data of a single data subject in the dataset change.

**Definition 3.1**.: _An algorithm \(\)\(is\)\((,)\)-DP if for neighbouring \(D,D^{}\) and all measurable sets \(S\)_

\[((D) S) e^{}((D^{}) S)+.\] (4)

We consider \(D^{N d}\) with \(N\) users and \(d\) dimensions, and use the _substitution neighbourhood_ relation \(_{S}\) where \(D_{S}D^{}\) if \(D\) and \(D^{}\) differ by at most one row.

**Gaussian DP.** In Section 3.3 we discuss the functional mechanism of Hall et al. (2013), which we use in the ConvCNP. However, the original privacy guarantees derived by Hall et al. (2013) are suboptimal. We improve upon these using the framework of Gaussian DP (GDP; Dong et al., 2022). Dong et al. (2022) define GDP from a hypothesis testing perspective, which is not necessary for our purposes. Instead, we present GDP through the following conversion formula between GDP and DP.

**Definition 3.2**.: _A mechanism \(\) is \(\)-GDP if and only if it is \((,())\)-DP for all \( 0\), where_

\[()=(-+)-e^{ }(--)\] (5)

_and \(\) is the CDF of the standard Gaussian distribution._

**Properties of (G)DP.** Differential privacy has several useful properties. First, _post-processing immunity_ guarantees that post-processing the result of a DP algorithm does not cause privacy loss:

**Theorem 3.3** (Dwork and Roth 2014).: _Let \(\) be an \((,)\)-DP (or \(\)-GDP) algorithm and let \(f\) be any, possibly randomised, function. Then \(f\) is \((,)\)-DP (or \(\)-GDP)._

Figure 3: Left; Illustration of the ConvCNP encoder enc\({}_{}\). Black crosses show an example context set \(D^{(c)}\). The density channel \(r^{(d)}\) is shown in purple and the signal channel \(r^{(r)}\) is shown in red. The representation \(r\) consists of concatenating \(r^{(d)}\) and \(r^{(s)}\). Right; Illustration of the DPConvCNP encoder. Black crosses show an example context \(D^{(c)}\), clipped with a threshold \(C\) (gray dashed). Here, a single point (rightmost) is clipped (gray cross shows value before clipping). The density and signal channels are computed and GP noise is added to obtain the DP representation (red & purple).

_Composition_ of DP mechanisms refers to running multiple mechanisms on the same data. When each mechanism can depend on the outputs of the previous mechanisms, the composition is called _adaptive_. GDP is particularly appealing because it has a simple and tight composition formula:

**Theorem 3.4** (Dong et al. 2022).: _The adaptive composition of \(T\) mechanisms that are \(_{i}\)-GDP (\(i=1,,T\)), is \(\)-GDP with \(=^{2}++_{T}^{2}}\)._

**Gaussian mechanism.** One of the central mechanisms to guarantee DP, is the Gaussian mechanism. This releases the output of a function \(f\) with added Gaussian noise

\[(D)=f(D)+(0,^{2}I),\] (6)

where the variance \(^{2}\) depends on the \(l_{2}\)-sensitivity of \(f,\) defined as

\[=_{D D^{}}||f(D)-f(D^{})||_{2}.\] (7)

**Theorem 3.5** (Dong et al. 2022).: _The Gaussian mechanism with variance \(^{2}=}}{{^{2}}}\) is \(\)-GDP._

### The Functional Mechanism

Now we turn to the functional mechanism of Hall et al. (2013). Given a dataset \(D^{N d},\) the functional mechanism releases a function \(f_{D} T,\) where \(T^{d},\) with added noise from a Gaussian process. For simplicity, here we only define the functional mechanism for functions in a _reproducible kernel Hilbert space_ (RKHS), and defer the more general definition to Appendix A.2.

**Definition 3.6**.: _Let \(g\) be a sample path of a Gaussian process having mean zero and covariance function \(k\), and let \(\) be an RKHS with kernel \(k\). Let \(\{f_{D}:D\}\) be a family of functions indexed by datasets, satisfying_

\[_{}f}}{{=}}_{D D ^{}}||f_{D}-f_{D^{}}||_{}.\] (8)

_The functional mechanism with multiplier \(c\) and sensitivity \(\) is defined as_

\[(D)=f_{D}+cg.\] (9)

**Theorem 3.7** (Hall et al.).: _If \( 1\), the mechanism in Def. 3.6 with \(c=\) is \((,)\)-DP._

## 4 Differential privacy for the ConvCNP

Now we turn to our main contributions. First, we tighten the functional mechanism privacy analysis in Section 4.1 and then we build the functional mechanism into the ConvCNP in Section 4.2.

### Improving the Functional Mechanism

The privacy bounds given by Theorem 3.7 are suboptimal, and do not allow us to use the tight composition formula from Theorem 3.4. However, the proof of Theorem 3.7 builds on the classical Gaussian mechanism privacy bounds, which we can replace with the GDP theory from Section 3.2. As demonstrated in Figure 4, our bound offers significantly smaller \(\) for the same noise standard deviation, compared to the existing bounds of Hall et al. (2013) and Jiang et al. (2023).

**Theorem 4.1**.: _The functional mechanism with sensitivity \(\) and multiplier \(c=}{{}}\) is \(\)-GDP._

Proof.: The proof of Theorem 3.7 from Hall et al. (2013) shows that any \((,)\)-DP bound for the Gaussian mechanism carries over to the functional mechanism. Replacing the classical Gaussian mechanism bound with the GDP bound proves the claim. For details, see Appendix A.

### Differentially Private Convolutional CNP

**Differentially Private SetConv.** Now we turn to building the functional DP mechanism into the ConvCNP. We want to modify the SetConv encoder (Eq. 3) to make it DP. As a reminder, the SetConv outputs the density \(r^{(d)}\) and signal \(r^{(s)}\) channels

\[r^{(d)}(x)\\ r^{(s)}(x)=_{n=1}^{N}1\\ y_{n}^{(c)}(^{(c)}}{}),\] (10)

which are the two quantities we want to release under DP. To achieve this, we must first determine the sensitivity of \(r^{(d)}\) and \(r^{(s)}\), as defined in Eq. 8. Recall that we use the substitution neighbourhood relation \(_{S}\), defined as \(D_{1}^{(c)}_{S}D_{2}^{(c)}\) if \(D_{1}^{(c)}\) and \(D_{2}^{(c)}\) differ in at most one row, i.e. by a single context point. Since the RBF \(\) is bounded above by \(1\), it can be shown (see Appendix A.4) that the squared \(l_{2}\)-sensitivity of \(r^{(d)}\) is bounded above by \(2\), and this bound is tight. Unfortunately however, since the signal channel \(r^{(s)}\) depends linearly on each \(y_{n}^{(c)}\) (see Eq. 10), its sensitivity is unbounded. To address this, we clip each \(y_{n}^{(c)}\) by a threshold \(C\), which is a standard way to ensure the sensitivity is bounded. With this modification we obtain the following tight sensitivities for \(r^{(d)}\) and \(r^{(s)}\):

\[_{}^{2}r^{(d)}=2,_{}^{2}r^{(s)}=4C^{2}\] (11)

With these in place, we can state our privacy guarantee which forms the basis of the DPConvCNP. Post-processing immunity (Theorem 3.3) ensures that post-processing \(r^{(s)}\) and \(r^{(d)}\) with the ConvCNP decoder does not result in further privacy loss.

**Theorem 4.2**.: _Let \(g_{d}\) and \(g_{s}\) be sample paths of two independent Gaussian processes having zero mean and covariance function \(k\), such that \(0 k C_{k}\) for some \(C_{k}>0\). Let \(_{d}^{2}=2C_{k}\) and \(_{s}^{2}=4C^{2}C_{k}\). Then releasing \(r^{(d)}+_{d}g_{d}\) and \(r^{(s)}+_{s}g_{s}\) is \(\)-GDP with \(=^{2}}}{{_{s}^{2}}}+^{2}}}{{_{d}^{2}}}}\)._

Proof.: The result follows by starting from the GDP bound of the mechanism in Theorem 4.1 and using Theorem 3.4 to combine the privacy costs for the releases of \(r^{(d)}\) and \(r^{(s)}\). 

**Corollary 4.3**.: _Algorithm 2 with the DPSetConv encoder from Algorithm 3 is \((,)\)-DP with respect to the real context set \(D^{(c)}\)._

Proof.: The noise_scales method in Algorithm 3 computes the appropriate \(_{d}\) and \(_{s}\) values from Theorem 4.2 and Definition 3.2 such that releasing the functional encodings \(r^{(d)}+_{d}g_{d}\) and \(r^{(s)}+_{s}g_{s}\) is \((,)\)-DP. The \((,)\)-DP guarantee extends [11, Proposition 5] to the point evaluations \(^{(d)}\) and \(^{(s)}\) over the grid \(\) in Algorithm 3. Post-processing immunity (Theorem 3.3) extends \((,)\)-DP to Algorithm 2. 

### Training the DPConvCNP

**Training loss and algorithm.** We meta-train the DPConvCNP parameters \(,\) using the CNP log-likelihood (eq. 2) within Algorithm 1, and meta-test it using alg. 2. Importantly, the encoder

Figure 4: Noise magnitude comparison for the classical functional mechanism of Hall et al. , the RDP-based mechanism of Jiang et al.  and our improved GDP-based mechanism. The line for Hall et. cuts off at \(=1\) since their bound has only been proven for \( 1\). We set \(^{2}=10\) and \(=10^{-3}\), which are representative values from our experiments. See Appendix A.6 for more details.

\(_{}\) now includes clipping and adding noise (alg. 3) in its forward pass. Meta-training with the functional in place is crucial, because it teaches the decoder to handle the DP noise and clipping.

**Privacy hyperparameters.** By Definition 3.2 and Theorem 4.2, each \((,)\)-budget implies a \(\)-budget, placing a constraint on the sensitivities and noise magnitudes, namely \(^{2}=^{2}}}{{_{s}^{2}}}+^{2} }}{{_{d}^{2}}}\). Since \(\) is an RBF, \(_{d}^{2}=2\) and \(_{s}^{2}=4C^{2}\), and we need to specify \(C,_{s}\) and \(_{d}\), subject to this constraint. We introduce a variable \(0<t<1\) and rewrite the constraint as

\[_{s}^{2}=}{t^{2}}_{d}^{2}= {2}{(1-t)^{2}}\] (12)

allowing us to freely set \(t\) and \(C\). One straightforward approach is to fix \(t\) and \(C\) to hand-picked values, but this is sub-optimal since the optimal values depend on \(\), \(N\), and the data statistics. Instead, we can make them adaptive, letting \(t:^{+}(0,1)\) and \(C:^{+}^{+}\) be learnable functions, e.g. neural networks \(t(,N)=(_{t}(,N))\) and \(C(,N)=(_{C}(,N))\) where \(\) is the sigmoid. These networks are meta-trained along with all other parameters of the DPConvCNP.

## 5 Experiments & Discussion

We conduct experiments on synthetic and a sim-to-real task with real data. We provide the exact experimental details in Appendix E. We make our implementation of the DPConvCNP public in the repository https://github.com/cambridge-mlg/dpconvcnp.

**DP-SGD baseline.** Since, we are interested in the small-data regime, i.e. a few hundred datapoints per task, we turn to Gaussian processes (GP; Rasmussen and Williams, 2006), the gold-standard model for well-calibrated predictions in this setting. To enforce DP, we make the GP variational (Titsias, 2009), and use DP-SGD (Abadi et al., 2016) to optimise its variational parameters and hyperparameters. This is a strong baseline because GPs excel in small data, and DP-SGD is a state-of-the-art DP fine-tuning algorithm. We found it critical to carefully tune the DP-SGD parameters and the GP initialisation using BayesOpt, and devoted substantial compute on this to ensure we have maximised GP performance. We refer to this baseline as the DP-SVGP. For details see Appendix D.

**General setup.** In both synthetic and sim-to-real experiments, we first tuned the DP as well as the GP initialisation parameters of the DP-SVGP on synthetic data using BayesOpt. We then trained the DPConvCNP on synthetic data from the same generative process. Last, we tested both models on unseen test data. For the DP-SVGP, testing involves DP fine-tuning its variational parameters and its hyperparameters on each test set. For the DPConvCNP, testing involves a single forward pass through the network. We report results in Figures 6 and 7, and discuss them below.

### Synthetic tasks

**Gaussian data.** First, we generated data from a GP with an exponentiated quadratic (EQ) covariance (Figure 6; top), fixing its signal and noise scales, as well as its lengthscale \(\). For each \(\) we sampled datasets with \(N\) and privacy budgets with \([0.90,4.00]\) and \(=10^{-3}\). We trained separate DP-SVGPs and DPConvCNPs for each \(\) and tested them on unseen data from the same generative process (_non-amortised_; Figure 6). These models can handle different privacy budgets but only work well for the lengthscale they were trained on. In practice an appropriate lengthscale is not known _a priori_. To make this task more realistic, we also trained a single DPConvCNP on data with randomly sampled \([0.25,2.00]\) (_amortised_; Figure 6). This model implicitly infers \(\) and simultaneously makes predictions, under DP. We also show the performance of the non-DP Bayes posterior, which is optimal (_oracle_; Figure 6 top). See Appendix E.1 for more details.

**DPConvCNP competes with DP-SVGP.** Even in the Gaussian setting, where the DP-SVGP is given the covariance of the generative process, the DPConvCNP remains competitive (red and

Figure 5: Deployment-time comparison on Gaussian (top) and non-Gaussian (bottom) data. We ran the DP-SVGP for different numbers of DP-SGD steps to determine a speed versus quality-of-fit tradeoff. Reporting 95% confidence intervals.

purple in Figure 6; top). While the DP-SVGP outperforms the DPConvCNP for some \(N\) and \(\), the gaps are typically small. In contrast, the DP-SVGP often fails to provide sensible predictions (see \(=0.25,N 300\)), and tends to overestimate the lengthscale, which is a known challenge in variational GPs (Bauer et al., 2016). We also found that the DP-SVGP tends to underestimate the observation noise, resulting in over-smoothed _and_ over-confident predictions which lead to a counter-intuitive reduction in performance as \(N\) increases. By contrast, the DPConvCNP gracefully handles different \(N\) and recovers predictions that are close to the non-DP Bayesian posterior for modest \(\) and \(N\), with runtimes several orders of magnitude faster than the DP-SVGP (Figure 5).

**Amortising over \(\) and privacy budgets.** We observe that the DPConvCNP trained on a range of lengthscales (green; Figure 6) accurately infers the lengthscale of the test data, with only a modest performance reduction compared to its non-amortised counterpart (red). The ability of the DPConvCNP to implicitly infer \(\) while making calibrated predictions is remarkable, given the DP constraints under which it operates. Further, we observe that the DPConvCNP works well across a range of privacy budgets. In preliminary experiments, we found that the performance loss due to amortising over privacy budgets is small. This is particularly appealing because a single DPConvCNP can be trained on a range of budgets and deployed at test time using the privacy level specified by the practitioner, eliminating the need for separate models for different budgets.

**Non-Gaussian synthetic tasks.** We generated data from a non-Gaussian process with sawtooth signals, which has previously been identified as a challenging task Bruinsma et al. (2023). We sampled the waveform direction and phase using a fixed period \(\) and adding Gaussian observation noise with a fixed magnitude. We gave the DP-SVGP an advantage by using a periodic covariance function, and truncating the Fourier series of the waveform signal to make it continuous: otherwise, since the DP-SVGP cannot handle discontinuities in the sawtooth signal, it explains the data mostly as noise, failing catastropically. Again, we trained a separate DP-SVGP and DPConvCNP for each \(\), as well as a single DPConvCNP model on randomly sampled \(^{-1}[0.20,1.25]\). We report results in Figure 6 (bottom), along with a non-DP oracle (blue). The Bayes posterior is intractable, so we report the average NLL of the observation noise, which is a lower bound to the NLL.

**DPConvCNP outperforms the DP-SVGP.** We find that, even though we gave the DP-SVGP significant advantages, the DPConvCNP still outperforms it, and produces near-optimal predictions even for modest \(N\) and \(\). Overall, our findings in the non-Gaussian tasks mirror those of the Gaussian tasks. The DPConvCNP can amortise over different signal periods with very small performance drops (red, green in Figure 6; bottom). Given the difficulty of this task, the fact that the DPConvCNP can predict accurately for signals with different periods under DP constraints is especially impressive.

Figure 6: Negative log-likelihoods (NLL) of the DPConvCNP and the DP-SVGP baseline on synthetic data from a EQ GP (top two rows; EQ lengthscale \(\)) and non-Gaussian data from sawtooth waveforms (bottom two rows; waveform period \(\)). For each point shown we report the mean NLL with its 95% confidence intervals (error bars too small to see). See Appendix C.2 for example fits.

### Sim-to-real tasks

**Sim-to-real task.** We evaluated the performance of the DPConvCNP in a sim-to-real task, where we train the model on simulated data and test it on the the Dobe!Kung dataset [Howell, 2009], also used by Smith et al. , containing age, weight and height measurements of 544 individuals. We generated data from GPs with a Matern-\(3/2\) covariance, with a fixed signal scale of \(_{v}=1.00\), randomly sampled noise scale \(_{n}[0.20,0.60]\) and lengthscale \([0.50,2.00]\). We chose Matern-\(3/2\) since its paths are rougher than those of the EQ, and picked hyperparameter ranges via a back-of-the envelope calculation, without tuning them for the task. We trained a single DP-SVGP and a DPConvCNP with \([0.90,4.00]\) and \(=10^{-3}\). We consider two test tasks: predicting the height or the weight of an individual from their age. For each \(N\), we split the dataset into a context and target at random, repeating the procedure for multiple splits.

**Sim-to-real comparison.** While the two models perform similarly for large \(N\), the DPConvCNP performs much better for smaller \(N\) (Figure 7; left). The DPConvCNP predictions are surprisingly good even for strong privacy guarantees, e.g. \(=1.00,=10^{-3}\), and a modest dataset size (Figure 7; right), and significantly better-calibrated than those of the DP-SVGP, which under-fits. Note we have not tried to tune the simulator or add prior knowledge, which could further improve performance.

## 6 Limitations & Conclusion

**Limitations.** The DPConvCNP does not model dependencies between target outputs, which is a major limitation. This could be achieved straightforwardly by extending our approach to LNPs, GNPs, or ARNPs. Another limitation is that the efficacy of any sim-to-real scheme is limited by the quality of the simulated data. If the real and the simulated data differ substantially, then sim-to-real transfer has little hope of working. This can be mitigated by simulating diverse datasets to ensure the real data are in the training distribution. However, as simulator diversity increases, predictions typically become less certain, so there is a sweet spot in simulator diversity. While we observed strong sim-to-real results, exploring the effect of this diversity is a valuable direction for future work.

**Broader Impacts.** This paper presents work whose goal is to advance the field of DP. Generally, we view the potential for broader impact of this work as generally positive. Ensuring individual user privacy is critical across a host of Machine Learning applications. We believe that methods such as ours, aimed at improving the performance of DP algorithms and improve their practicality, have the potential to have a positive impact on individual users of Machine Learning models.

**Conclusion.** We proposed an approach for DP meta-learning using NPs. We leveraged and improved upon the functional DP mechanism of Hall et al. , and showed how it can be naturally built into the ConvCNP to protect the privacy of the meta-test set with DP guarantees. Our improved bounds for the functional DP mechanism are substantial, providing the same privacy guarantees with a \( 30\%\) lower noise magnitude, and are likely of independent interest. We showed that the DPConvCNP is competitive and often outperforms a carefully tuned DP-SVGP baseline on both Gaussian and non-Gaussian synthetic tasks, while simultaneously being orders of magnitude faster at meta-test time. Lastly, we demonstrated how the DPConvCNP can be used as a sim-to-real model in a realistic evaluation scenario in the small data regime, where it outperforms the DP-SVGP baseline.

Figure 7: Left; Negative log-likelihoods of the DPConvCNP and the DP-SVGP baseline on the sim to real task with the!Kung dataset, predicting individualsâ€™ height from their age (left col.) or their weight from their age (right col.). For each point shown here, we partition each dataset into a context and target at random, make predictions, and repeat this procedure \(512\) times. We report mean NLL with its 95% confidence intervals. Error bars are to small to see here. Right; Example predictions for the DPConvCNP and the DP-SVGP, showing the mean and 95% confidence intervals, with \(N=300,=1.00,=10^{-3}\). The DPConvCNP is visibly better-calibrated than the DP-SVGP.