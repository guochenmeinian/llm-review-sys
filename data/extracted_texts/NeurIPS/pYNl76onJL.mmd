# Wenhao Wang

VidProM : A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion ModelsUniversity of Technology Sydney

wangwenhao0716@gmail.com &Yi Yang*

Zhejiang University

yangyics@zju.edu.cn

**1.67 Million Unique Text-to-Video Prompts from Real Users**

_Create a vivid 3D village scene in the midst of a journey, where quaint cottages dot the landscape, surrounded by lush greenery_

_Take the viewers on a captivating cinematic journey in 8K ultra HD, following a group of chicks in Disney style as they explore a magical forest. They play until the sun sets, and the lush, enchanting environment enchants the audience. Convey the idea that time ceases to exist as they immerse themselves in their playful adventures._

**6.69 Million Generated Videos by Diffusion Models**

The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, along with other text-to-video diffusion models, is highly reliant on _prompts_, and there is no publicly available dataset that features a study of text-to-video prompts. In this paper, we introduce **VidProM**, the first large-scale dataset comprising 1.67 Million unique text-to-**Video Prompts from real users. Additionally, this dataset includes 6.69 million videos generated by four state-of-the-art diffusion models, alongside some related data. We initially discuss the curation of this large-scale dataset, a process that is both time-consuming and costly. Subsequently, we underscore the need for a new prompt dataset specifically designed for text-to-video generation by illustrating how VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Our extensive and diverse dataset also opens up many exciting new research areas. For instance, we suggest

Figure 1: VidProM is the first dataset featuring 1.67 million unique text-to-video prompts and 6.69 million videos generated from 4 different state-of-the-art diffusion models. It inspires many exciting new research areas, such as Text-to-Video Prompt Engineering, Efficient Video Generation, Fake Video Detection, and Video Copy Detection for Diffusion Models.

exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models to develop better, more efficient, and safer models. The project (including the collected dataset VidProM and related code) is publicly available at https://vidprom.github.io under the CC-BY-NC 4.0 License.

## 1 Introduction

The Sora  initializes a new era for text-to-video diffusion models, revolutionizing video generation with significant advancements. This breakthrough provides new possibilities for storytelling, immersive experiences, and content creation, as Sora  transforms textual descriptions into high-quality videos with ease. However, Sora  and other text-to-video diffusion models [2; 3; 4; 5] heavily relies on the prompts used. Despite their importance, there is no publicly available dataset focusing on text-to-video prompts, which may hinder the development of these models and related researches.

In this paper, we present the first systematic research on the text-to-video prompts. Specifically, our efforts primarily focus on building the first text-to-video prompt-gallery dataset VidProM, analyzing the necessity of collecting a new prompt dataset specialized for text-to-video diffusion models, and introducing new research directions based on our VidProM. The demonstration of VidProM is shown in Fig. 1.

\(\)**The first text-to-video prompt-gallery dataset.** Our large-scale VidProM includes \(1.67\) million unique text-to-video prompts from real users and \(6.69\) million generated videos by \(4\) state-of-the-art diffusion models. The prompts are from official Pika Discord channels, and the videos are generated by Pika , Text2Video-Zero , VideoCraft2 , and ModelScope . We distribute the generation process across 10 servers, each equipped with 8 Nvidia V100 GPUs. Each prompt is embedded using the powerful text-embedding-3-large model from OpenAI and assigned six not-safe-for-work (NSFW) probabilities, consisting of toxicity, obscenity, identity attack, insult, threat, and sexual explicitness. We also add a Universally Unique Identifier (UUID) and a time stamp to each data point in our VidProM. In addition to the main dataset, we introduce a subset named VidProS, which consists of _semantically unique_ prompts. That means, in this subset, the cosine similarity between any two prompts is less than 0.8, ensuring a high level of semantic diversity.

\(\)**The necessity of collecting a new prompt dataset specialized for text-to-video diffusion models.** We notice that there exists a text-to-image prompt-gallery dataset, DiffusionDB . By analyzing the basic information and the prompts, we conclude that the differences between our VidProM and DiffusionDB  mainly lies in: (1) Semantics: The semantics of our prompts are significantly different from those in DiffusionDB, with our text-to-video prompts generally being more dynamic, more complex, and longer. (2) Modality: DiffusionDB  focuses on _images_, while our VidProM is specialized in _videos_. (3) Techniques: We utilize some recent advanced techniques, such as latest text embedding model (OpenAI-text-embedding-3-large), to build our VidProM.

\(\)**Inspiring new research directions.** The introduction of our new text-to-video prompt-gallery dataset, VidProM, opens up many exciting research directions. With the help of our VidProM, researchers can develop better, more efficient, and safer text-to-video diffusion models: (1) For better models, researchers can utilize our VidProM as a comprehensive set of prompts to evaluate their trained models, distill new models using our prompt-(generated)-video pairs, and engage in prompt engineering. (2) For more efficient models, researchers can search for related prompts in our VidProM and reconstruct new videos from similar existing videos, thereby avoiding the need to generate videos from scratch. (3) For safer models, researchers can develop specialized models to distinguish generated videos from real videos to combat misinformation, and train video copy detection models to identify potential copyright issues.

To sum up, this paper makes the following contributions: **(1)** We contribute the first text-to-video prompt-gallery dataset, VidProM, which includes \(1.67\) million unique prompts from real users and \(6.69\) million generated videos by \(4\) state-of-the-art diffusion models. **(2)** We highlight the necessity of collecting a new prompt dataset specialized for text-to-video diffusion models by comparing our VidProM with DiffusionDB. **(3)** We reveal several exciting research directions inspired by VidProM and position it as a rich database for future studies.

## 2 Related Works

### Text-to-Video Diffusion Models

Text-to-video diffusion models [3; 7; 4; 8; 5; 1; 9; 10; 2; 11; 12] have become a powerful tool for producing high-quality video content from textual prompts. _Pika_ is a commercial text-to-video model by Pika Labs, which advances the field of video generation. _Text2Video-Zero_ enables zero-shot video generation using textual prompts. _VideoCrafter2_ generates videos with high visual quality and precise text-video alignment without requiring high-quality videos. _ModelScope_ evolves from a text-to-image model by adding spatio-temporal blocks for consistent frame generation and smooth movement transitions. This paper uses these four publicly accessible sources (access to generated videos or pre-trained weights) for constructing our VidProM. We hope that the collection of diffusion-generated videos will be useful for further research in text-to-video generation community.

### Existing Datasets

**Text-Video Datasets.** While several published text-video datasets exist [13; 14; 15; 16; 17; 18; 19; 20], they primarily consist of caption-(real)-video pairs rather than prompt-(generated)-video pairs. For example, _WebVid-10M_ is a large-scale text-video dataset with 10 million video-text pairs collected from stock footage websites . _HDVILA-100M_ is a comprehensive video-language dataset designed for multimodal representation learning, offering high-resolution and diverse content. _Panda-70M_ is a curated subset of HDVILA-100M , featuring semantically consistent and high-resolution videos. In contrast, our VidProM contains prompts authored by real users to generate videos of interest, and the videos are produced by text-to-video diffusion models.

Figure 2: A data point in the proposed VidProM. Please click the corresponding links to view the complete videos: Pika, VideoCraft2, Text2Video-Zero, and ModelScope. To better understand our dataset, you can also download \(10,000\) randomly-selected data points from here.

**Prompt Datasets.** Existing datasets underscore the significance of compiling a set of prompts. In the _text-to-text_ domain, studies  demonstrate that gathering and analyzing prompts can aid in developing language models that respond more effectively to prompts. _PromptSource_ recognizes the growing popularity of using prompts to train and query language models, and thus create a system for generating, sharing, and utilizing natural language prompts. In the _text-to-image_ domain, _DiffusionDB_, which is nominated for the best paper of ACL 2023, collects a large-scale prompt-image dataset, revealing its potential to open up new avenues for research. Given the importance of prompt datasets and the new era of text-to-video generation brought by _Sora_, this paper presents the first prompt dataset specifically collected for _text-to-video_ generation.

## 3 Curating VidProM

In Fig. 2, we illustrate a single data point in the proposed VidProM. This data point includes a prompt, a UUID, a timestamp, six NSFW probabilities, a 3072-dimensional prompt embedding, and four generated videos. We show the steps of curating our **VidProM** in this section.

**Collecting Source HTML Files.** We gather chat messages from the official Pika Discord channels between July 2023 and February 2024 using DiscordChatExplorter  and store them as HTML files. Our focus is on 10 channels where users input prompts and request a bot to execute the Pika text-to-video diffusion model for video generation. The user inputs and outputs are made available by Pika Lab under the Creative Commons Noncommercial 4.0 Attribution International License (CC BY-NC 4.0), as detailed in Section 4.5.a of their official terms of service. Consequently, the text-to-video prompts and Pika videos in our dataset are open-sourced under the same license.

**Extracting and Embedding Prompts.** The HTML files are then processed using regular expressions to extract prompts and time stamps. We subsequently filter out prompts used for image-to-video generation (because the images are not publicly available) and prompts without associated videos (these prompts may have been banned by Pika or hidden by the users). Finally, we remove duplicate prompts and assign a UUID to each prompt, resulting in a total of \(1,672,243\) unique prompts. Because the text-to-video prompts are significantly complex and long, we use OpenAI's text-embedding-3-large API, which supports up to \(8192\) tokens, to embed all of our prompts. We retain the original 3072-dimensional output, allowing any customized dimensionality reduction.

**Assigning NSFW Probabilities.** We select the public Discord channels of Pika Labs, which prohibit NSFW content, as the source for our text-to-video prompts. Consequently, if a user submits a harmful prompt, the channel will automatically reject it. However, we find VidProM still includes NSFW prompts that were not filtered by Pika. We employ a state-of-the-art NSFW model, Detoxify , to assign probabilities in six aspects of NSFW content, including toxicity, obscenity, identity attack, insult, threat, and sexual explicitness, to each prompt. In Fig. 3, we visualize the number of prompts with a NSFW probability greater than \(0.2\). We conclude that only a very small fraction (less than

Figure 3: The number of prompts with an NSFW probability greater than \(0.2\) constitutes only a very small fraction of our total of \(1,672,243\) unique prompts.

\(0.5\%\)) of prompts have a probability greater than \(0.2\). We provide six separate NSFW probabilities, enabling researchers to set a suitable threshold for filtering out potentially unsafe data for their tasks.

**Scraping and Generating Videos.** We enhance our VidProM diversity by not only scraping Pika videos from extracted links but also utilizing three state-of-the-art open-source text-to-video diffusion models for video generation. This process demands significant computational resources: we distribute the text-to-video generation across 10 servers, each equipped with 8 Nvidia V100 GPUs. It costs us approximately 50,631 GPU hours and results in 6.69 million videos (\(4 1,672,243\)), totaling 14,381,289.8 seconds in duration. The breakdown of video lengths is as follows: 3.0 seconds for Pika, 1.6 seconds for VideoCraft2, 2.0 seconds for Text2Video-Zero, and 2.0 seconds for ModelScope.

**Selecting Semantically Unique Prompts.** Beyond general uniqueness, we introduce a new concept: semantically unique prompts. We define a dataset as containing only semantically unique prompts if, for any two arbitrary prompts, their cosine similarity calculated using text-embedding-3-large embeddings is less than \(0.8\). After semantic de-duplication (see a detailed description of this process in the Appendix (Section A)), our VidProM still contains \(1,038,805\) semantically unique prompts, and we denote it as VidProS. More semantically unique prompts imply covering a broader range of topics, increasing the diversity and richness of the content.

## 4 The Necessity of Introducing VidProM

We notice that there exists a text-to-image prompt-gallery dataset, DiffusionDB . This section highlights how our VidProM is different from this dataset from two aspects, _i.e._ basic information and semantics of prompts.

### Basic Information

In Table 1, we provide a comparison of the basic information between our VidProM and DiffusionDB . We have the following observations:

**Prompt. (1)** Although the total number of unique prompts in our VidProM and DiffusionDB are similar, VidProM contains **significantly more** (\(+40.6\%\)) semantically unique prompts. This shows VidProM is a more diverse and representative dataset. **(2)** Unlike DiffusionDB, which uses the OpenAI-CLIP embedding method, our approach leverages the **latest** OpenAI text-embedding model, namely text-embedding-3-large. One advantage of this approach is its ability to accept much longer prompts compared to CLIP, supporting up to 8192 tokens versus CLIP's 77 tokens. As illustrated by the comparison of the number of words per prompt between VidProM and DiffusionDB in Fig. 4, the prompts used for generating videos are much more longer. Therefore, the capability of text-embedding-3-large is particularly suitable for them. Another advantage is text-embedding-3-large has stronger performance than CLIP on several standard benchmarks, potentially benefiting users of our VidProM. **(3)** The time span for collecting prompts in VidProM is **much longer** than in DiffusionDB. We collect prompts written by real users over a period of \(8\) months, while DiffusionDB's collection spans only \(1\) month. A longer collection period implies a broader range of topics and themes covered, as demonstrated by the comparison of the number of semantically unique prompts.

  Aspects & Details & DiffusionDB  & VidProM \\   & No. of unique prompts & \(1,819,808\) & \(1,672,243\) \\  & No. of semantically unique prompts & \(739,010\) & \(1,038,805\) \\  & Embedding of prompts & OpenAI-CLIP & OpenAI-text-embedding-3-large \\  & Maximum length of prompts & 77 tokens & 8192 tokens \\  & Time span & Aug 2022 & Jul 2023 \(\) Feb 2024 \\   & No. of images/videos & \( 14\) million images & \( 6.69\) million videos \\  & No. of sources & \(1\) & \(4\) \\   & Average repetition rate per source & \( 8.2\) & \\   & Collection method & Web scraping & Web scraping + Local generation \\   & GPU consumption & - & \( 50,631\) V100 GPU hours \\   & Total seconds & - & \( 14,381,289.8\) seconds \\  

Table 1: The comparison of basic information of VidProM and DiffusionDB . To ensure a fair comparison of semantically unique prompts, we use the text-embedding-3-large API to re-embed prompts in DiffusionDB .

**Prompts from DiffusionDB**

_halloween scary castle, pumpkin, bar, spiders, grave, web_

_earth from outer space, cosmic dust, nebula, star, other planets, highly detailed, high render,_

_photo of chris cornell holding a kitty_

_abstract watercolor painting of spanish street, white buildings, summer, magical and traditional, cinematic light, french cafe, sharp shadows, daylight, national romanticism by_

_anders zorn, by grey rutkowski, by greg manchess_

_a sandwich car pointing a balloon made of coke_

_It had been flying for a long time and was feeling extremely parched. Finally, after a while, it spotted a small village with a few houses and trees._

_zoom in Steampunk Android attached to wires cables Beksinski Alfred Stieglitz rolling shutter vhs glitch grainy black and white 1900s_

_This is a 1-minute realistic short video depicting a young person developing good habits by using a height-adjustable automated lift desk for work in a bright and minimalist bedroom. The true-to-life footage vividly shows him working on the computer, writing at the desk, as well as clips of him standing up for exercise. The concise narration explains the practical benefits of cultivating healthy work and lifestyle habits with this type of smart furniture._

**Takeaway:** Our VidProM dataset contains a larger number of semantically unique prompts, which are embedded by a more advanced model and collected over a longer period.

**Images or videos.** DiffusionDB focuses on images, while our VidProM is specialized in **videos**. Therefore, given that generating videos is much more expensive than images, it is reasonable that the number of videos in VidProM is smaller than the number of images in DiffusionDB. We also make several efforts to mitigate this disadvantage: **(1)** The number of source diffusion models for our VidProM is **much larger** than those of DiffusionDB. Our videos are generated by \(4\) state-of-the-art text-to-video diffusion models, while DiffusionDB contains only images generated by Stable Diffusion. As a result, the average repetition rate per source is only \(1\) for our VidProM compared to about \(8.2\) for DiffusionDB. **(2)** We devote **significantly more resources** to VidProM. Unlike DiffusionDB, which only collects images through web scraping, we also deploy three open-source text-to-video models on our local servers, dedicating over \(50,000\) V100 GPU hours to video

Figure 4: The differences between the prompts in DiffusionDB and our VidProM are illustrated by: (a) a few example prompts for illustration; (b) the number of long prompts; and (c) the t-SNE  visualization of \(10,000\) prompts randomly selected from DiffusionDB and VidProM, respectively.

generation. These efforts result in more than \(14\) million seconds of video. In the future, researchers can also generate more videos using the prompts with more advanced text-to-video diffusion models.

**Takeaway:** Our VidProM dataset contains a considerable amount of videos generated by various state-of-the-art text-to-video diffusion models, utilizing a substantial amount of resources.

### Semantics of prompts

In this section, we analyze how the semantics of prompts in our VidProM dataset are different from DiffusionDB .

**Firstly,** as shown in Fig. 4 (a), the semantics of the prompts differ in three aspects: (1) **Time dimension**: text-to-video prompts usually need to include a description of the time dimension, such as 'changes in actions' and 'transitions in scenes'; while text-to-image prompts typically describe a scene or object. (2) **Dynamic description**: text-to-video prompts often need to describe the dynamic behavior of objects, such as 'flying', 'working', and 'writing'; while text-to-image prompts focus more on describing the static appearance of objects. (3) **Duration**: text-to-video prompts may need to specify the duration of the video or an action, such as 'a long time' and '1-minute', while text-to-image prompts do not need to consider the time factor.

**Secondly,** as shown in Fig. 4 (a) and (b), text-to-video prompts are generally **more complex and longer** than text-to-image prompts, due to the need to describe additional dimensions and dynamic changes. This phenomenon is also observed in the prompts used by **Sora**. For instance, the prompt for the 'Tokyo Girl' video contains \(64\) words, while the longest prompt on the OpenAI official website comprises \(95\) words. Our VidProM dataset prominently features this characteristic: (1) the number of prompts with more than \(70\) words is nearly \(60,000\) for our VidProM, compared to only about \(15,000\) for DiffusionDB; and (2) our VidProM still has over \(25,000\) prompts with more than \(100\) words, whereas this number is close to \(0\) for DiffusionDB.

Figure 5: The WizMap  of our VidProM and DiffusionDB . Click here for an interactive one.

**Finally, as shown in Fig. 4 (c) and Fig. 5, the prompts from our VidProM dataset and DiffusionDB exhibit **different distributions**. We use the text-embedding-3-large model to re-extract the features of the prompts in DiffusionDB. For visualizing with t-SNE , we randomly select \(10,000\) prompt features from both our VidProM dataset and DiffusionDB, respectively. We find that these prompts have significantly different distributions and are nearly linearly separable. Beyond the traditional t-SNE , we use a recent method, WizMap , to analyze the topics preferred by people using all prompts from our VidProM and DiffusionDB. WizMap  is a visualization tool to navigate and interpret large-scale embedding spaces with ease. From the visualization results shown in Fig. 5, we conclude that, **despite some overlaps, there are distinct differences in their interests.** For example, both groups are interested in topics like cars, food, and cute animals. However, text-to-image users tend to focus on generating art paintings, whereas text-to-video users show little interest in this area. Instead, text-to-video users are more inclined to create general human activities, such as walking, which are rarely produced by text-to-image users.

**Takeaway:** The significant difference in semantics between text-to-image prompts and our text-to-video prompts indicates the need to collect a new dataset of prompts specifically for video generation. By analyzing prompt usage distribution, future researchers can design generative models to better cater to popular topics in prompts.

## 5 Inspiring New Research

The proposed million-scale VidProM dataset inspires new directions for researchers to develop better, more efficient, and safer text-to-video diffusion models.

**Video Generative Model Evaluation** aims to assess the performance and quality of text-to-video generative models. Current evaluation efforts, such as [27; 28; 29; 30], are conducted using carefully designed and small-scale prompts. _Our VidProM dataset_ brings imagination to this field: (1) Instead of using carefully designed prompts, researchers could consider whether their models can generalize to prompts from real users, which would make their evaluation more practical. (2) Performing evaluations on large-scale datasets will make their arguments more convincing.

**Text-to-Video Diffusion Model Development** aims to create diffusion models capable of converting textual descriptions into dynamic and realistic videos. The current methods [3; 7; 4; 8; 5; 1; 9; 10; 2; 11; 12] are trained on caption-(real)-video pairs. Two natural questions arise: (1) Will the domain gap between captions and prompts from real users (see the evidence of existing domain gap in the Appendix (Section B)) hinder these models' performance? For instance, \(60\%\) of the training data for Open-Sora-Plan v1.0.0  consist of landscape videos, leading to the trained model's suboptimal performance in generating scenes involving humans and cute animals. (2) Can researchers train or distill new text-to-video diffusion models on prompt-(generated)-video pairs? Although some studies indicate that this approach may lead to irreversible defects , as we will likely run out of data on websites and some methods are proposed to prevent such collapse , exploring synthetic data remains a valuable endeavor. The studies of training text-to-video diffusion models _with our VidProM_ may provide answers to these two questions.

**Text-to-Video Prompt Engineering** is to optimize the interaction between humans and text-to-video models, ensuring that the models understand the task at hand and generate relevant, accurate, and coherent videos. The prompt engineering field has gained attention in large language models [34; 35], text-to-image diffusion models [36; 37], and visual in-context learning [38; 39]. However, as far as we know, there is no related research in the text-to-video community. _Our VidProM_ provides an abundant resource for text-to-video prompt engineering. In the Section 6, we train a large language model on our VidProM for automatic text-to-video prompt completion for instance.

**Efficient Video Generation.** The current text-to-video diffusion models are very time-consuming. For example, on a single V100 GPU, ModelScope  requires \(43\) seconds, while VideoCrafter2  needs \(51\) seconds to generate a video, respectively. _Our large-scale VidProM_ provides a unique opportunity for efficient video generation. Given an input prompt, a straightforward approach is to search for the most closely related prompts in our VidProM and reconstruct a video from the corresponding existing videos, instead of generating a new video from noise.

**Fake Video Detection** aims to distinguish between real videos and those generated by diffusion models. While there are some works [40; 41; 42; 43; 44; 45; 46; 47; 48; 49] focusing on fake image detection, fake video detection presents unique challenges: (1) The generalization problem: Existing fake image detectors may not generalize well to video frames (see experiments in the Appendix (Section C)). For instance, a model trained on images generated by Stable Diffusion  may fail to identify frames from videos generated by Pika  or Sora  as fake. (2) The efficiency problem: Currently, there is no detector that can take an entire video as input. As a result, to achieve higher accuracy, we may use fake image detectors to examine all or representative frames, which can be time-consuming. _With our VidProM_, researchers can (1) train specialized Fake Video Detection models on millions of generated videos, and (2) use millions of prompts to generate more videos from more diffusion models to further improve the detection performance.

**Video Copy Detection for Diffusion Models** aims to answer whether videos generated by diffusion models replicate the contents of existing ones (see a detailed discussion in the Appendix (Section D)). Videos generated with replicated content may infringe on the copyrights of the original videos. However, current video copy detection researches [51; 52; 53; 54] focus on detecting hand-crafted copies generated by transformations like horizontal flips and random crops, and overlook the challenge from diffusion models. Researchers can utilize _our dataset VidProM_ as input queries and employ existing video copy detection models to pre-filter potential replications. Subsequently, these filtered videos can be manually annotated to determine whether they are indeed replications and regarded as training sources.

## 6 Automatic Text-to-video Prompt Completion

This section describes the training and output of our text-to-video prompt completion model.

**Training details.** Similar to text2image-prompt-generator , we treat the automatic text-to-video prompt completion as a general text generation task. The training code is available at: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py. We fine-tune the recent state-of-the-art large language model, Mistral-7B-v0.1 , on our dataset VidProM. The fine-tuning process is completed using DeepSpeed  with 8 A100 GPUs in 2 hours.

**Demonstration.** We release our trained model on: https://huggingface.co/WenhaoWang/AutoT2VPrompt. It can be freely downloaded and tested under CC BY-NC 4.0 License, and we also demonstrate some of its generation across various topics below.

**Input:** A cat sitting

**Generated prompts:**

1. A cat sitting on a desk, typing on its laptop. The room has soft, natural light from the window, casting a wm glow on the cat.

2. A cat sitting in a tree, blinking its eyes. The cat is talking to the camera.. a woman is eating a hodog in the street, sunset.

3. A cat sitting at an office desk, facing the camera. The cat has a human body shape. The cat is black in color, and the desk has some office supplies. The scene is shot at an angle.

**Input:** An underwater world

**Generated prompts:**

1. An underwater world. The ocean is filled with bioluminess as the water reflects a soft glow from a bioluminescent phosphorescent light source. The camera slowly moves away and zooms in.

2. An underwater world, the ocean is full of discded items, the water flows, and the light penetrating through the water.

3. An underwater world, 25 ye boy, with aqua-green eyes, dk sandy blond hair, from the back, and on his back a fish, 23 ye old, weing glasses,ctoon chacte.

**Input:** A landscape

**Generated prompts:**

1. A landscape, the camera zooms in on an open field where a lone tree stands. A single tree, weathered by time, stands tall and proud, its branches reaching towds the heavens.

2. A landscape of the Middle East, a vast and desolate desert, with dunes of golden sand stretching to the horizon.

3. A landscape filled with vibrant colors and a sense of peace and hmony.

**Input:** a woman

**Generated prompts:**

1. a woman in the style of a classic disney animation, dk white and dk beige and yellow, lofi, lush scenery, iconic album covers, 8k resolution, detailed chacter design.

2. a woman in a white dress, in the background a lge window, from outside the window you can see the heavy rain and thunder, night, cinematic, foggy, mysterious, 8k, HD, Fujifi.

3. a woman 25 yes old, weing black t shirt and blue jeans with black sneakers walking in a train station.

**Input:** Spiderman

**Generated prompts:**

1. Spiderman, 4k, cinematic light, high production value, intricate details, high resolution.

2. Spiderman and his incredible strength and speed e on full display. The cityscape ound him is a blur of colors, from bright blue to purple.

3. Spiderman and his face is a canvas of shifting geometric patterns, his eyes spkling in the reflection of a neby window.

**Input:** A Chinese dragon

**Generated prompts:**

1. A Chinese dragon flies high in the sky, with the word ;dragon; printed on its body.

2. A Chinese dragon flies in a dk and mysterious universe, with a Chinese aesthetic.. a girl walking on the street. a 1970;s thouse film about a woman standing on a rock..

3. A Chinese dragon is flying in the air, and the camera lens is rotating ound the dragon.

## 7 Conclusion

This paper provides the first systematic research on text-to-video prompts. Specifically, we introduce VidProM, the first dataset comprising 1.67 million unique text-to-video prompts, 6.69 million videos generated by four state-of-the-art diffusion models, along with NSFW probabilities, 3072-dimensional prompt embeddings, and additional related metadata. To highlight the necessity of VidProM, we compare the basic information and the semantics of prompts of our VidProM to DiffusionDB, a text-to-image prompt dataset. Finally, we outline the potential research directions inspired by our VidProM, such as text-to-video prompt engineering, fake video detection, and video copy detection for diffusion models. We hope the curated large and diverse prompt-video dataset will advance research in the text-video domain.

**Limitation**. We recognize that the videos currently generated are short and not of the highest quality. In the future, we intend to enhance our dataset by incorporating high-quality videos produced by more advanced models, like Sora, using our long and detailed prompts. At the time of camera-ready version, we utilize three new powerful text-to-video models models to generate 10,000 videos with each for example: 10,000 videos of 8 seconds at 720p quality for StreamingT2V , 10,000 videos of 8 seconds at 720p quality for Open-Sora 1.2 , and 10,000 videos of 6 seconds at 720p quality for CogVideoX-2B . They are publicly available at https://huggingface.co/datasets/WenhaoWang/VidProM/tree/main/example.