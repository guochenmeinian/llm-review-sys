# DisCoV: Disentangling Time Series Representations

via Contrastive based \(l\)-Variational Inference

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Learning disentangled representations is crucial for Time Series, offering benefits like feature derivation and improved interpretability, thereby enhancing task performance. We focus on disentangled representation learning for home appliance electricity usage, enabling users to understand and optimize their consumption for a reduced carbon footprint. Our approach frames the problem as disentangling each attribute's role in total consumption (e.g., dishwashers, fridges,...). Unlike existing methods assuming attribute independence, we acknowledge real-world time series attribute correlations, like the operating of dishwashers and washing machines during the winter season. To tackle this, we employ weakly supervised contrastive disentanglement, facilitating representation generalization across diverse correlated scenarios and new households. Our method utilizes innovative \(l\)-variational inference layers with self-attention, effectively addressing temporal dependencies across bottom-up and top-down networks. We find that **DisCoV** (**Dis**entangling via **Con**trastive \(l\)-**V**ariational) can enhance the task of reconstructing electricity consumption for individual appliances. We introduce TDS _(Time Disentangling _Score_) to gauge disentanglement quality. TDS reliably reflects disentanglement performance, making it a valuable metric for evaluating time series representations. Code available at https://anonymous.4open.science/r/DisCo

## 1 Introduction

Disentangled representation learning is crucial in various fields like computer vision, speech processing, and natural language processing . It aims to improve model performance by learning latent disentangled representations and enhancing generalizability, robustness, and explainability. These representations have latent units that respond to single attribute changes while remaining invariant to others. Existing approaches assume independent attributes, but in real-world time series data, latent attributes are often causally related. This necessitates a new framework for causal disentanglement. For instance, in Fig 1, the consumption profile of "Dishwasher" and "Profile 2" cause variations in "Washing machine" and "Profile 1," showing the inadequacy of existing methods in capturing these non-independent attributes [31; 29]. One of the most common frameworks for disentangled

Figure 1: Illustrative of Real-world data often showcases attributes exhibit strong positive correlation: seasonal changes.

representation learning is Variational Autoencoders (VAE) , a deep generative model trained to disentangle the underlying explanatory attributes. Disentanglement via VAE can be achieved by a regularization term of the Kullback-Leibler divergence between the posterior of the latent attributes and a standard Multivariate Gaussian prior , which enforces the learned latent attribute to be as independent as possible. It is expected to recover the latent variables if the observation in the real world is generated by countable independent attributes. To further enhance the independence, various extensions of VAE consider minimizing the mutual information among latent attributes .  further encourage independence by reducing the total correlation among attributes. Our focus in this work is a more general case, where the data does not have specificity like domain frequency, or amplitude to analysis. Household energy consumption disaggregation, also known as Non-Intrusive Load Monitoring (NILM), is a key application. Given only the main consumption of a household, the energy disaggregation algorithm identifies which appliances are operating. Such a capability is extremely vital given the growing interest in reducing carbon footprints through user energy behavior, which poses a challenge to conventional algorithms. Many households rely on past bills to adjust future energy use, underscoring the importance of energy disaggregation algorithms. Recent work [3; 32; 23] hold promising results, yet persistent challenges in generalisability and robustness stem from the correlations occurring within time series a challenge that spans beyond the domain of time series in general. In this work, we tackle the energy disaggregation problem from the perspective of disentanglement.

Our work is distinguished by instead of assuming independent factors we will only assume that the support of the distribution factorizes. We explore how to design an efficient and disentangling representation under correlated attributes using weak supervised contrastive learning. An ablation investigation to understand the impact of considering statical independence versus the case where we avoid it by giving the latent space a support factorization through weakly supervised contrastive learning. This addresses latent space misalignment between attributes, maintains generalizability, and preserves disentanglement through the _Pairwise similarity_ over \(\) setting it apart from methods relying on _independence_. More clearly, we break the concept of independence, allowing any combination of individual attributes, to be possible, even if some combinations are unlikely, our experiments on three datasets and increasingly difficult correlation settings, show that DisCoV improves robustness to attribute correlation and improves disentanglement (as measured by SAP, DCI, RMIG,TDS) by up to \(+21.7\%\) over state of the art (c.f. SS5.3). Furthermore, we introduce an in-depth \(l\)-variational-based self-attention for extracting high semantic representations from time series. An ablation study shows that \(l\)-VAE learns complex representations; added attention improves further (in-depth model

Figure 2: Latent attributes are causally  correlated, allows positive pairs \(\), \((^{+})\) to decrease their distance, while negative pairs increase it, and allows cases where unlikely combinations occur ((i) and (ii) lead to the existence of (iii)), although forcing statically independence does not prohibit these cases. Our framework is based on _contrastive disentanglement_ to relax \(z\) to have a support factorization (allowing for some dependency).

\(l=4,8,16,32\) c.f. fig. 6). This approach retains dimension reduction while avoiding temporal locality. Additionally, our proposed Time Disentanglement Metric (TDS) aligns more effectively with decoder output compared to existing metrics. These findings establish it as a strongly recommended for time series representations.

## 2 Related Work

Recent work [3; 23] has produced promising results. However, they are confronted with problems of interpretability, generalization, and robustness. Various approaches have been proposed to solve these problems. For instance,  introduced Convolutional Neural Networks (CNNs) for feature extraction from power consumption data, showing promise on the UK-DALE dataset . Generalization concerns persist despite leveraging Gated Recurrent Units (GRUs) and attention mechanisms. Other works attempt meaningful representation of time series, but disentangling remains challenging [29; 27; 22]. Recurrent VAE (RVAE)  for sequential data, D3VAE  improves prediction using a diffusion model after decoding the latent space. In representation learning,  employs contrastive learning, but in correlated data scenarios it is not explored.  based on specific propriety of time series like frequency and amplitude to disentangling Time series, but disentangling the latent space through data-driven methods poses a challenge. Nevertheless, recent approaches like Support Factorization as described in the works of [35; 24] show promise in addressing this challenge and have yielded encouraging results.

## 3 Formulation

We consider a \(c\)-variate time series observed at times \(t=1,,\). We denote by \(^{c}\) the \(c\) resulting matrix with rows denoted by \(x_{1},,x_{c}\). Each row can be seen as a univariate time series. In the electric load application, we have \(c=3\), and \(x_{1}\) is the sampled active power, \(x_{2}\) the sampled reactive power and \(x_{3}\) the sampled apparent power. The goal of non-intrusive load monitoring (NILM) is to use \(\) in order to express \(x_{1}\) as

\[x_{1}=_{m=1}^{M}y_{m}+\,\] (1)

where, for each \(m=1,,M\), \(y_{m}^{}\) represents the contribution of the \(m\)-th electric device among the \(M\) ones identified in the household, and \(^{}\) denotes a residual noise. We further denote by \(\) the \(M\) matrix with row-wise stacked devices' contributions.

The NILM mapping \(\{_{1}_{k}\}\), where \(=_{i}\) is generally learnt from a training data set \(=\{(_{n},_{n})\}_{n=1}^{N}\). VAEs rely on two main ingredients: 1) a generative model \((p_{})\) based on a latent variable, and a decoder \(g_{}\); 2) a variational family \((q_{})\), which approximates the conditional density of the latent variable given the observed variable based on an encoder \(f_{}\).

In a VAE, both (unknown) parameters \(\) and \(\) are learnt from the training data set \(=\{_{n}\}_{n=1}^{N}\). A key idea for defining the goodness of fit part of the learning criterion is to rely the **E**vidence **L**ower **B**ound (ELBO), which provides a lower bound on (and a proxy of) the log-likelihood

\[ p_{}()_{q_{}(|)} [ p_{}(|)]-(q_{}( |) p())\,\] (2)

where we denoted the latent variable by \(\), defined as a \((M+K) d_{z}\) matrix and \(p\) denotes its distribution. The use of ELBO goes back to traditional variational Bayes inference. An additional feature of VAE's is to define \(q_{}\) and \(p_{}\) through an encoder/decoder pair of neural networks \((f_{},g_{})\). A standard choice in a VAE is to rely on Gaussian distributions and, for instance, to set \(q_{}(|)=(;(,), ^{2}(,))\), where \((,)\) and \(^{2}(,)\) are the outputs of the encoder \(f_{}\).

As mentioned in Section 1, various additional features such as \(\)/TC/Factor/DIP-VAE have been proposed, where a specific distribution \(p()\) is learned. The objective is to disentangle the latent variable \(\), and align it with the corresponding attribute. However, they assume statistical independence among attributes, leading to the assumption: \(p()=p(_{1}) p(_{})\). As we explained in the introduction, appliances are not used independently. In , correlated attributes have been taken into account by replacing the factorization constraint with support factorization via Hausdorff FactorizedSupport (HFS). In order to meet this criterion, they penalize the Hausdorff pairwise estimate Eq.3, based solely on the distance without any alignment on the input.

\[_{H}()=_{i=1}^{(M+K)-1}_{j=i+1}^{(M+K)}_{z\{ _{:,i}\}\{_{:,j}\}}[_{z^{}\{_{:,(i,j)}\}}\|z-z^{}\|]\.\] (3)

We are investigating an alternate way to achieve both alignment and disentanglement leading to a generalizable representation. To that end, we draw on support factorization, and we replace \(_{H}()\) by a _Pairwise Similarity_ penalty. In the next section, we develop our proposed method based on weakly contrastive learning to have factorized support, and it provides an advantage in terms of computation and latent representation.

## 4 Proposed Methods

Our objective is to disentangle latent space by relaxing the independence, for this, we now define a concrete training criterion that encourages factorized support. Let us consider deterministic representations obtained by the encoder \(=f_{}()\). We enforce the factorial support criterion on the aggregate distribution \(_{}()=_{}[f_{}()]\), where \(_{}()\) is conceptually similar to the aggregate posterior \(q_{}(z)\) in, e.g., TCVAE, though we consider points produced by a deterministic mapping \(f_{}\) rather than a stochastic one. To match our factorized support assumption on the ground truth, we want to encourage the support of \(_{}(z)\) to factorize, i.e., that \(Supp(_{}(z))\) and the Cartesian product of each dimension support, \(Supp^{}(_{}(z))\), are equal. In practical scenarios, we often deal with a finite sample of observations \(\{_{i}\}_{i=1}^{N}\) and can only estimate support on a finite set of representations \(\{f_{}(_{i})\}_{i=1}^{N}\). To encourage such a pairwise factorized support, we can minimize sliced/pairwise contrastive with the additional benefit of keeping computation tractable when \(\) is large. Specifically, we approximate the support as \(Supp\) and the Cartesian product of each dimension's support as \(Supp^{} z_{:,1} z_{:,2} z_{:,k}=\{(z_{1},,z_{k}) z_{1} z_{:,1},,z_{k} z_{:,k}\}\).

### Support factorization via Weakly supervised Constrastive

Let us first formalize the contrastive learning setup. Each training triplet comprises a reference sample \(\) along with a positive (similar) sample \(^{+}\) and negative (dissimilar) samples \(_{1}^{-},,_{N}^{-}\) against which it is to be contrasted. As introduced in the previous section, we assume that these samples generate corresponding latent: \(\), \(^{+}\), \(_{1}^{-},,_{N}^{-}\). The positive sample, denoted as \(^{+}\), is generated from a closely related dataset in which appliance \(m\) is activated. In contrast, the negative samples, \(_{1}^{-},,_{N}^{-}\), are drawn from a dataset where appliance \(m\) remains inactive. This formalization of contrastive learning ensures that positive samples are semantically similar and negatives are dissimilar. Self-supervised contrastive learning is widely used in computer vision, in , the loss is defined as:

\[_{}=-_{i I} z_{j(i)}/ )}}{_{a(i)}e^{(z_{i} z_{a}/)}}.\] (4)

where, \(z_{i} Z,\)_where, \(Z=f_{}()\), the \(\) symbol denotes the inner (dot) product, \(^{+}\) is a scalar temperature parameter, and \((i) I\{i\}\). The index \(i\) is called the anchor \(z_{i}\), index \(j(i)\) is refer to the positive \(z_{i}^{+}\), and the other \(2(N-1)\) indices (\(\{k(i)\{j(i)\}\}\)) are called the negatives \(z_{k i}^{-}\). We note that for each anchor \(i\), there is \(1\) positive pair and \(2N-2\) negative pairs. The denominator has a total of \(2N-1\) terms (the positives and negatives). In a multiclass scenario, disentangling and aligning data encounters challenges when several samples belong to the same class, as we aim to match certain pairs of data points (e.g., \(z_{i,j}\) to \(z_{i,j}^{+}\)) and drive others away (i.e. \(z_{i,j}\) from \(z_{k i,j}\) or \(z_{k i,j}^{+}\)). We link the learned latent representation to ground-truth attributes using a limited number of pair labels. This connection is facilitated by employing positive and negative samples, as demonstrated in . We adapt this, by firstly, the loss should not rely on statically independent attributes, mirroring realistic data scenarios; secondly, it should prioritize attribute alignment to maintain sufficient information . To achieve this, the proposed disentanglement loss combines two terms. The first term enforces axis alignment based on the correlation between \(z_{:,m}\) and \(z_{:,m}^{+}\) (positive augmentation of \(z_{:,m}\)). This ensures that only one latent variable learns this alignment for fixed attributes (invariant). The second term minimises information redundancy by measuring the correlation between \(z_{:,m}\) and \(z^{+}_{:,p m}\) or (\(z_{:,m}\) and \(z_{:,p m}\)), which are almost equivalent in a contrasting sense.

\[_{DIS}=_{m=1}^{r}1-d(z_{:,m},z^{+}_{:,m})^{2}+ _{m=1}^{r}_{:,p m}^{r-1}d(z_{:,m},z^{+}_{:,p})^{2}\] (5)

\(r=M+K\), we define \(d(z_{:,m},z^{+}_{:,m})\) as the cosine similarity between vectors \(z_{:,m}\) and \(z^{+}_{:,m}\) in a mini-batch. Furthermore, this helps to support factorizing the latent space as it measures the correlation between \(z_{:,m}\) and \(z^{+}_{:,p m}\) or \(z_{:,p m}\), and performs better than estimating Eq.3. Augmentation affects only one attribute, with others remaining fixed. We assume sufficient augmentation for each factor across the batch. Our results indicate both terms equally contribute to improved disentangling without weighted hyperparameters (c.f. ablation SS6.1).

### Attentive \(l\)-Variational auto-encoders and Objective function

To avoid time locality during dimension reduction, and keep long-range capability we refer to an in-depth Temporal Attention with \(l\)-Variational layers. NVAE [26; 1] proposed an in-depth autoencoder for which the latent space \(\) is level-structured and attended locally , this shows an effective results for image reconstruction. In this work, we enable the model to establish strong couplings, as depicted. Our core idea aims to address construct \(^{l}\) (Time context) that effectively captures the most informative features from a given sequence \(T^{<l}=\{T^{i}\}_{i=1}^{l}\) across bottom-up and top-down, where \(T^{<l}\) is the output of the residual network. Both \(^{l}\) and \(T^{l}\) are features with the same dimensionality: \(^{l}^{T C}\) and \(T^{i}^{T C}\). In our model, we employ Temporal Self-attention  to construct either the prior or posterior beliefs of variational layers, which enables us to handle long context sequences with large dimensions \(\) effectively. The construction of \(^{l}\) relies on a query feature \(^{l}^{T Q}\) of dimensionality \(Q\) with \(Q C\), and the corresponding context \(T^{l}\) is represented by a key feature \(^{l}^{T Q}\). Importantly, \(^{l}(t)\) of time step \(i\) in sequence \(\) depends solely on the time instances in \(T^{<l}\). For more consistency, using Multihead-attention  allows the model to focus on different aspects of the input sequence simultaneously, which can be useful for capturing various relationships and patterns. which allows the model to jointly attend to information from different representation subspaces at different scales. Instead of computing a single attention function, this method first projects \(^{l}\), \(^{<l}\)), \(^{<l}\)) into \(h\) different vectors, respectively. Attention is applied individually to these \(h\) projections. The output is a linear transformation of the concatenation of all attention outputs. An in-depth description of this mechanism is given in Appendix 8.2. For the remainder of this paper, we presume that **DisCoV** employs self-attention.

We adopt the Gaussian residual parametrization between the prior and the posterior. The prior is given by \(p(_{l}|_{<l})=((T_{p}^{l},),(T_{ p}^{l},)\). The posterior is then given by \(q(_{l}|,_{<l})=((T_{p}^{l},)+ (_{q}^{l},),(T_{p}^{l},) (_{q}^{l},))\) where the sum (\(+\)) and product (\(\)) are pointwise, and \(T_{q}^{l}\) is defined in Eq 14. \(()\), \(()\), \(()\), and \(()\) are transformations implemented as convolutions layers. Based on this, For \(_{KL}\) in Eq 2, the last term is approximated by: \(0.5_{1}^{2}}{_{1}^{2}}+ _{1}^{2}-_{1}^{2}-1\). Our DisCoV objective function combines the VAE loss (Eq.2), consisting of a reconstruction term \(_{rec}\) (focused on minimizing Mean Squared Error), with the contrastive term on \(\) (Eq.5). We introduce balancing factors \(\) and \(\) (discussed in SS6.2) to control their impact.

\[_{DisCo}=_{rec}+_{KL}}_{ }}+_{DIS}\] (6)

### How to evaluate disentanglement for Time Series?

Evaluating disentanglement in series representation is more challenging than established computer vision metrics. Existing time series methods rely on qualitative observations and predictive performance, while metrics like Mutual Information Gap (MIG)  have limitations with continuous labels. To address this, we adapted RMIG  for continuous labels and used DCI metrics from .

Additionally, we employed SAP  to measure prediction error differences in the most informative latent dimensions for ground truth attributes. Our evaluation, including \(\)-VAE and FactorVAE scores, can be found in Appendix 8.1. These metrics face challenges with sequential data and do not provide measures of attribute alignment.

To overcome this limitation, we introduce the Time Disentanglement Score (TDS) from an information-gain perspective. TDS assesses how well the latent representation \(=f_{}()\) maintains the invariance of an attribute \(m\) in \(\) when this attribute changes. TDS relies on the correlation matrix between \(\) and \(^{+}\), where \(=f_{}()\) and \(^{+}=f_{}(())\), with \(\) denoting an augmentation function. This correlation matrix quantifies the consistency of attribute components. Additionally, TDS evaluates how well \(\) contributes to the reconstruction of \(\) and how \(^{+}\) contributes to the reconstruction of \(()\). Specifically, it assesses whether each \(z_{m}\) (or \(z_{m}^{+}\)) can effectively reconstruct the corresponding \(y_{m}\) (or \(y_{m}^{+}\)). TDS aligns with qualitative observations of disentanglement (c.f. fig. 5).

\[=[(1-_{i}Corr^{(I)}(,^{+})_{ii})^{2}+(1-_{i}Corr^{(I)}(-},^{+}-}^{+})_{ii})]^{2}\] (7)

where \(Corr^{(I)}_{ij}=_{b}_{b,i}^{+}_{b,j}\) divided by \((_{b,i})^{2}}(^{+}_{b,j})^{2}}\), \(b\) indexes batch samples and \(i,j\) index the vector dimension of the networks' \(f_{}\) outputs for \(Corr^{(I)}\) (resp. dimension of the networks' outputs of \(g_{}\) for \(Corr^{(II)}\)). \(Corr\) is a square matrix with the size of the dimensionality of the network's output and with values comprised between -1 (i.e. perfect anti-correlation) and 1 (i.e. perfect correlation). In practice, the augmentation function \(\) is effectively a sampling of appliance activation (i.e. from different sources, houses/datasets) for the positive case and sequences where the device is not activated for the negative case. We note that high TDS informativeness signifies strong disentanglement, while a significant distance implies reduced disentanglement and higher attribute correlation, aligning with . More in-depth explanation can be found in the appendix 8.1.4.

## 5 Experiments

### Experimental Setup

**Datasets.** We conducted experiments on two publicly available datasets, namely UK-DALE  and REDD . The dataset UK-DALE  consists of 5 dwellings with a varying number of sub-metered devices and includes aggregate and individual aggregate and individual equipment-level power measurements, sampled equipment, sampled at 1/6 Hz.

**Evaluation Metrics.** We adopt RMSE to evaluate the accuracy of all compared methods. Details of these three metrics can be found in Appendix 11.1.1

**Baseline.** We compare DisCoV with down task models in energy, Bert4NILM  and S2P , S2P , for those model we keep the same configuration as the original implementation. We provide also a variete de \(\)-TC/Factor/-VAE implemented for time series, compared to D3VA  and NVAE , and RVAE 

**Experimental Platform.** We conduct 5 rounds of experiments, reporting the averaged results and standard deviation. The experiments are performed on four NVIDIA A40 GPUs and 40 Intel(R) 281 Xeon(R) Silver 4210 CPU @ 2.20GHz. The models are implemented in PyTorch. Detailed hyperparameter settings are available in Appendix 8.3.

  
**Metric** & **Align-axis** & **Unbiased** & **General** \\  \(\)-VAE  & No & No & No \\ FactorVAE  & Yes & No & No \\ RMIG  & Yes & No & Yes \\ SAP  & Yes & No & Yes \\ DCI  & Yes & Yes & No \\
**TDS (Ours)** & Yes & Yes & Yes \\   

Table 1: In comparison to prior metrics, our proposed TDS detects axis alignment, is unbiased for all hyperparameter settings and can be generally applied to any latent distributions provided efficient estimation exists.

### Architecture Settings

Our model uses a bi-directional encoder, which processes the input data in a hierarchical manner to produce a low-resolution latent code that is refined latent code that is refined by a series of oversampling layers. This code is then refined by a series of oversampling layers in _Residual Decoders_ blocks, which progressively increases the resolution.

**Residual Blocs.** Activation functions are pivotal for enabling models to learn nonlinear representations, but vanishing and exploding gradients can hinder learning. The Temporal Convolutional Network (TCN)  tackles these issues using Rectified Linear Unit (ReLU), weight normalization, and dropout layers. In our Residual model, we simplify the residual block by replacing these components with the Sigmoid Linear Units, which offers advantages and immunity to gradient problems. It reduces training time, efficiently learns robust features, and outperforms weight normalization. SiLU  is defined as \((x)=x(x)\) where \((x)\) is the logistic sigmoid.

**Squeeze-and-Excitation on Spatial and Temporal.** SE block enhances our neural networks by selectively emphasizing important features and suppressing less relevant ones. It does this through global information gathering (squeezing) and feature recalibration (excitation). We find that extending SE for time series data improves the capture of significant temporal patterns in sequence. Our Residual encoders (Inference Model \(q_{}\)) in Fig 4 and Decodeur (Generative Model \(p_{}\)) in Fig 4.

### Performance and Informativity of Contrastive

_Finding: DisCoV retains its robustness in correlated scenarios and achieves comparable performance to baseline models._

In evaluating the robustness of DisCoV regarding correlations in appliance signatures or consumption, we consider several pairs of appliances. Firstly, there's the **No Correlation** scenario, where we examine the correlation between the refrigerator's signature and the dishwasher's signature. These appliances are typically active at different times, resulting in less correlated signatures. Moving on to specific pairs, **Pair 1** involves analyzing the correlation between the washing machine's signature and the dryer's signature. Given that these appliances are often used sequentially, their signatures might exhibit some level of correlation. In **Pair 2**, the focus is on evaluating the correlation between the microwave's signature and the oven's signature. These appliances have distinct power profiles and usage patterns, potentially leading to lower correlation. **Pair 3** explores the correlation between the lighting's power consumption and the television's power consumption. Since these appliances are often used independently, their signatures may exhibit a lower level of correlation. Lastly, the **Random Pair** approach involves selecting two random appliances from a dataset.

## 6 Ablation Studies

In this section, we conduct ablation experiments to assess DisCo's effectiveness and robustness in comparison to traditional variant VAEs. Our experiments utilize the Uk-Dale, REDD, and REFIT datasets with a fixed random seed. We include additional ablation results in Appendix **??**.

### In-depth self-attention \(l\)-VAEs learn an effective representation.

_Finding: DisCoV with increasing depth, the representation becomes over 20% more separable (40% in terms of TDS), downtasking improves performance by 50%, and attention mechanisms contribute to a 10% enhancement in results._

Table 6, we observe notable differences in performance as the depth (\(L\)) of the model architecture varies including Root Mean Square Error (RMSE), Relative Mutual Information Gain (RMIG), and Task Discriminative Score (TDS) for various methods, with a particular emphasis on DisCoV variants with and without attention as the depth (\(L\)) increases. Regarding RMSE, which measures the accuracy of the models, we find that the baseline methods VAE, \(\)-TCVAE, and DIP-VAE exhibit consistently higher RMSE values compared to the DisCoV variants. Furthermore, introducing the 'DIS' significantly improves RMSE values across all methods, indicating the effectiveness of the DisCoV loss in enhancing model performance. Additionally, as depth (\(L\)) increases from 4 to 16, we observe that the DisCoV variants consistently outperform the baseline methods in terms of RMSE. Notably, when \(L\) reaches 16, both DisCoV and DisCoV attention achieve the lowest RMSE value of \(0.48\), showcasing the superior performance of DisCo-based models with higher depth. It is also worth mentioning that RMIG and TDS metrics follow a similar trend, with DisCoV variants demonstrating superior performance, especially as \(L\) increases. These findings suggest that increasing the depth of the model architecture and incorporating DisCoV loss play pivotal roles in improving model accuracy and task discriminative capabilities, highlighting the significance of attention mechanisms in enhancing performance.

  
**Machine** & **Dataset Test** & **S2P** & **S2S** & **Bert4NILM** & **RVAE** & \(\)**-TCVAE** & **FactorVAE** & **NVAE** & **D3VAE** & **DisCoV (Ours)** \\   & UK-DALE & 25.70 & 25.68 & 25.69 & 25.74 & 27.36 & 26.70 & 27.36 & 28.36 & **19.55** \\  & REDD & 25.49 & 25.47 & 25.48 & 26.56 & 30.68 & 26.56 & 30.68 & 21.8 & **19.48** \\   & UK-DALE & 25.78 & 25.76 & 25.77 & 25.63 & 28.92 & 24.72 & 28.92 & 21.12 & **18.33** \\  & REDD & 25.59 & 25.57 & 25.58 & 25.34 & 28.40 & 24.78 & 28.40 & 23.22 & **18.31** \\   & UK-DALE & 25.61 & 25.59 & 25.60 & 25.46 & 25.28 & 23.98 & 25.28 & 22.18 & **19.30** \\  & REDD & 25.45 & 25.43 & 25.44 & 25.42 & 25.04 & 23.94 & 25.04 & 20.78 & **19.82** \\   

Table 4: RMSE in \(Watt^{2}\) on **UK-DALE** and **REDD** data.

  
**Method** & **No Corr** & **Pairs: 1** & **Pairs: 2** & **Pairs: 3** & **Random Pair.** & \(\) \\  REDD  & & & & & & & & \\ \(\)-VAE & 72.4 [68.1, 76.9] & 70.3 [62.8, 73.5] & 54.5 [49.3, 59.1] & 39.8 [34.2, 42.7] & 40.6 [37.8, 41.9] & 3.10 \\ HFS & 79.8 [76.5, 84.6] & 78.6 [75.2, 80.1] & 75.8 [52.0, 59.7] & 48.7 [43.4, 50.5] & 47.1 [41.9, 48.7] & 1.10 \\ \(\)-VAE + HFS & 93.1 [78.2, 101.3] & 81.9 [77.2, 82.4] & 69.4 [64.3, 71.7] & 49.2 [45.2, 52.2] & 65.1 [62.5, 67.5] & 2.12 \\ \(\)-TCVAE & 78.0 [77.5, 79.2] & 71.9 [67.1, 73.3] & 64.7 [61.0, 66.0] & 49.0 [38.3, 52.5] & 51.6 [47.5, 57.6] & 1.01 \\ \(\)-TCVAE + HFS & 87.2 [84.0, 98.8] & 75.6 [44.7, 77.9] & 69.9 [62.6, 73.4] & 52.1 [48.2, 53.3] & 62.1 [48.4, 64.8] & 1.01 \\ FactorVAE & 68.4 [53.5, 71.4] & 73.2 [72.9, 73.6] & 59.7 [58.4, 64.5] & 48.4 [42.5, 50.6] & 33.0 [29.3, 36.5] & 3.12 \\ DisCoV & **63.5 [62.0, 64.5]** & **58.5 [50.8, 60.3]** & **32.9 [28.2, 35.4]** & **34.9 [32.3, 39.3]** & **24.3 [21.4, 27.2]** & 1.35 \\   \\ \(\)-VAE & 34.2 [27.3, 39.9] & 11.5 [99.12, 13.2] & 9.5 [87.10.3] & N/A & 13.4 [119.15, 9.0] & 0.48 \\ HFS & 37.9 [30.4, 39.0] & 15.6 [9.6, 18.7] & 13.9 [117.1, 15.8] & N/A & 17.2 [13.1, 18.0] & 1.38 \\ \(\)-VAE + HFS & 52.1 [32.2, 52.6] & 21.9 [19.2, 23.3] & 19.5 [8.2, 21.8] & N/A & 17.9 [14.3, 18.8] & 0.22 \\ \(\)-TCVAE & 32.1 [30.1, 36.4] & 25.2 [24.8, 25.6] & 12.4 [8.6, 14.6] & N/A & 21.9 [18.5, 24.6] & 0.13 \\ \(\)-TCVAE + HFS & 55.44 [4.1, 55.5] & 27.9 [62.6, 28.6] & 29.2 [17.5, 33.0] & N/A & 26.2 [25.2, 27.7] & 0.11 \\ FactorVAE & 29.7 [24.9, 34.9] & 19.1 [15.9, 20.3] & 17.4 [16.4, 19.0] & N/A & 18.7 [17.5, 19.3] & 0.23 \\ DisCoV & **42.4 [41.7, 43.0]** & **16.8 [16.3, 17.9]** & **10.5 [8.9, 12.3]** & N/A & **16.3 [16.1, 16.5]** & 0.42 \\   

Table 3: Disentanglement by Contrastive on **UK-DALE**, ** **Uk-Dale** across various correlated appliances (columns) and correlation increasing from left (no correlation) to right (every appliance correlated to one confounder). Scores denote DCI metric computed on uncorrelated test data. Bold denotes the best performance per correlation. [\(x\), \(y\)] indicate 257/51 percentiles.

### Robustness, Disentanglement, and Strong Generalization

_Finding: DisCoV demonstrates robust disentanglement performance across varying dimensions, while FactorVAE exhibits degradation as dimensionality increases \(M\)._

We report the disentanglement performance of DisCoV and FactorVAE on the Uk-dale dataset as \(M\) is increased. FactorVAE  is the closest TC-based method: it uses a single monolithic discriminator and the density-ratio trick to explicitly approximate \(TC()\). Computing \(TC()\) is challenging to compute as \(M\) increases. The results for \(M=10\) (scalable \( 3\)) are included for comparison. The average disentanglement scores for DisCoV \(M=7\) and \(M=10\) are very close, indicating that its performance is robust in \(M\). This is not the case for FactorVAE it performs worse on all metrics when m increases. Interestingly, FactorVAE \(M=10\) seems to recover its performance on most metrics with higher \(\) than is beneficial for FactorVAE \(M=10\). Despite this, the difference suggests that FactorVAE is not robust to changes in \(M\).

## 7 Conclusion

To address the limitation of assuming independence in traditional disentangling methods, which doesn't align with real-world correlated data, we explore an approach focused on recovering correlated data. This method achieves untangling by enabling the model to encode diverse combinations of generative attributes in the latent space. Using DisCo, we demonstrate that promoting pairwise factorized support is adequate for traditional untangling techniques. Additionally, we find that DisCoV performs competitively with downstream tasks (i.e. NILM methods) and delivers significant relative improvements of over +60% on common benchmarks across various correlation shifts in datasets.

Figure 5: PCA visualization for \(M=3\), \(K=1\): Rows represent latent representations of activated appliances (Washing Machine, Oven, Fridge from top to bottom), columns correspond to \(_{m}\) components of structured latent variable \(\).

Figure 6: RMSE, RMIG, and TDS Scores for Variants DisCoV w/,w/o Attention, as \(L\) Increases. (\(\) lower values are better).

Figure 7: Disentanglement metric comparison of **DisCoV** with **VAE** baselines on UKDALE. **DisCoV**\(\) is plotted on the lower axis, and VAE-based method regularization strength \(\) is plotted on the upper axis. Dark lines average scores. Shaded areas one standard deviation.