# Optimal cross-learning for contextual bandits with unknown context distributions

Jon Schneider

Google Research

jschnei@google.com &Julian Zimmert

Google Research

zimmert@google.com

###### Abstract

We consider the problem of designing contextual bandit algorithms in the "cross-learning" setting of Balseiro et al., where the learner observes the loss for the action they play in all possible contexts, not just the context of the current round. We specifically consider the setting where losses are chosen adversarially and contexts are sampled i.i.d. from an unknown distribution. In this setting, we resolve an open problem of Balseiro et al. by providing an efficient algorithm with a nearly tight (up to logarithmic factors) regret bound of \(()\), independent of the number of contexts. As a consequence, we obtain the first nearly tight regret bounds for the problems of learning to bid in first-price auctions (under unknown value distributions) and sleeping bandits with a stochastic action set.

At the core of our algorithm is a novel technique for coordinating the execution of a learning algorithm over multiple epochs in such a way to remove correlations between estimation of the unknown distribution and the actions played by the algorithm. This technique may be of independent interest for other learning problems involving estimation of an unknown context distribution.

## 1 Introduction

In the contextual bandits problem, a learner repeatedly observes a context, chooses an action, and observes a reward for the chosen action only. The goal is to learn a policy that maximizes the expected reward over time, while taking into account the fact that the context can change from one round to the next. Algorithms for the contextual bandits problem are extensively used across various domains, such as personalized recommendations in e-commerce, dynamic pricing, clinical trials, and adaptive routing in networks, among others.

Traditionally, in contextual bandits problems, the learner only observes the reward for the current action and the current context. However, in some applications, the learner may be able to deduce the reward they would have received from taking this action in other contexts and attempt to make use of this additional information. For example, if a learner is repeatedly bidding into an auction (where their context is their private value for the item, and their action is the bid), they can deduce their net utility under different counterfactual values for the item.

This form of "cross-learning" between contexts was first introduced by Balseiro et al. (2019), who showed that with this extra information it is possible to construct algorithms for this problem with regret guarantees (compared to the best fixed mapping from contexts to actions) that are _independent_ of the total number of contexts \(C\) - in contrast, without this cross-learning information it is necessary to suffer at least \(()\) regret against such a benchmark. In particular, when contexts are drawn i.i.d. from a known distribution \(\) and losses are chosen adversarially, Balseiro et al. (2019) present an efficient algorithm that achieves \(()\) expected regret compared to the best fixed mapping from contexts to actions. However, this learning algorithm crucially requires knowledge of the distribution\(\) over contexts (knowledge which is unrealistic to have in many of the desired applications). Balseiro et al. (2019) present an alternate algorithm in the case where \(\) is unknown, albeit with a significantly worse regret bound of \(O(K^{1/3}T^{2/3})\).

Our main contribution in this paper is to close this gap, providing an efficient algorithm (Algorithm 1) which does not require any prior knowledge of \(\), and attains \(()\) regret (where the \(\) hides logarithmic factors in \(K\) and \(T\), but not \(C\)). Since there is an \(()\) regret bound from ordinary (non-contextual) bandits, this bound is optimal up to logarithmic factors in \(K\) and \(T\).

TechniquesAt first glance, it may seem misleadingly simple to migrate algorithms from the known context distribution setting to the unknown context distribution setting. After all, we are provided one sample from this context distribution every round, and these samples are unbiased and unaffected by the actions we take. This suggests the idea of just replacing any use of the true context distribution in the original algorithm by the current empirically estimated context distribution.

Unfortunately, this does not easily work as stated. We go into more detail about why this fails and the challenges of getting this to work in Section 2.1 after we have introduced some notation, but at a high level the algorithm of Balseiro et al. (2019) requires computing a certain expectation over \(\) when computing low-variance unbiased loss estimates. In particular, this expectation appears in the denominator of these estimates, meaning tiny errors in evaluating it can lead to large changes in algorithm behavior. Even worse, the quantity we need to take the expectation of depends on the previous contexts and therefore can be correlated with our empirical estimate of \(\), preventing us from applying standard concentration bounds.

We develop new techniques to handle both of these challenges. First, we present a new method of analysis that sidesteps the necessity of proving high probability bounds on each of the denominators individually, instead bounding their expected sum in aggregate. Secondly, we present a method of scheduling the learning algorithm into different epochs in a way which largely disentangles the correlation between learning \(\) and solving the bandit problem.

As a final note, we remark that dealing with unknown context distributions is a surprising challenge in many other contextual learning problems. For example, Neu and Olkhovskaya (2020) study a variant of linear contextual bandits where they can only prove their strongest regret bounds in the setting where they know the distribution over contexts. It would be interesting to see if the techniques we develop in this paper provide a general method for handling such issues - we leave this as an interesting future direction.

### Applications

As an immediate consequence of our bounds for Algorithm 1, we obtain nearly tight regret bounds for a number of problems of interest. We focus on two such applications in this paper: bidding in first-price auctions, and sleeping bandits with stochastic action sets.

**Learning to bid in first-price auctions (Balseiro et al., 2019; Han et al., 2020; Han et al., 2020; Zhang et al., 2021; Badauliyuru et al., 2023; Badauliyuru et al., 2023).** In a first-price auction, an item is put up for sale. Simultaneously, several bidders each submit a hidden bid for the item. The bidder with the highest bid wins the item and pays the value of their bid. Over the last few years, first-price auctions have become an increasingly popular format for a variety of large-scale advertising auctions (Chen, 2017).

Unlike second-price auctions (where the winning bidder pays the second-highest bid), first-price auctions are _non-truthful_, meaning that it is not the incentive of the bidder to bid their true value for the item - indeed, doing so guarantees the bidder will gain no utility from winning the auction. Instead, the optimal bidding strategy in a first-price auction is complex and depends on the bidders estimation of the other players' values and bids. As such, it is a natural candidate for learning over time (especially since advertising platforms run these auctions millions of times a day).

This problem was a motivating application for Balseiro et al. (2019), who proved an \(O(T^{3/4})\) regret bound for bidders with an unknown (but stochastic iid and bounded) value distribution participating in adversarial first-price auctions with no feedback aside from whether they won the item. Later, several works studied variants of this problem under more relaxed feedback models (for example, Han et al. (2020) introduced "winning-bid" feedback, where the bidder can always see the winning bid, and Zhang et al. (2022) study this problem in the presence of machine-learned advice), but none improve over the \(O(T^{3/4})\) bound in the binary feedback setting.

In Section 4.1, we show that Algorithm 1 leads to an efficient \((T^{2/3})\) regret algorithm for the setting of Balseiro et al. (2019). This nearly (up to logarithmic factors) matches an \((T^{2/3})\) lower bound proved by Balseiro et al. (2019).

**Sleeping bandits (Kanade et al., 2009; Kleinberg et al., 2010; Neu and Valko, 2014; Kanade and Steinke, 2014; Kale et al., 2016; Saha et al., 2020).** Sleeping bandits are a variant of the classical multi-armed bandit problem, motivated by settings where some actions or experts might not be available in every round. For instance, some items in retail stores might be out of stock, or certain servers in load balancing might be under maintenance. When both losses and arm availabilities are adversarial, the problem is known to be NP-hard (Kleinberg et al., 2010) and EXP4 obtains the optimal \((K)\) regret. However, when losses are adversarial but availabilities are stochastic, it is unknown what the minimax optimal \(K\)-dependency is and whether it can be obtained by a computationally efficient algorithm. The state of the art for efficient algorithm is either \(O((KT)^{})\)(Neu and Valko, 2014) or \(O(})\)(Saha et al., 2020). The latter work provides an improved algorithm with \(O(K^{2})\) regret when the arm availabilities are independent, however, in the general case, the computational complexity scales with \(T\).

In Section 4.2, we show that Algorithm 1 leads to an efficient (\(O(K)\) time per round) \(()\) regret algorithm for the sleeping bandits problem with arbitrary stochastic arm availabilities. Again, this nearly matches the \(()\) lower bound inherited from standard bandits.

**Other applications.** Finally, we briefly point out that our algorithm extends to the other applications mentioned in Balseiro et al. (2019), including multi-armed bandits with exogenous costs, dynamic pricing with variable costs, and learning to play in Bayesian games. In all cases, applying Algorithm 1 allows us to get nearly the same regret bounds in the unknown context distribution setting as Balseiro et al. (2019) can obtain in the known distribution setting.

## 2 Preliminaries

NotationFor any natural number \(N\), we use \([N]=\{1,2,,N\}\).

We study a contextual \(K\)-armed bandit problem over \(T\) rounds, with contexts belonging to some set \(\). At the start of the problem, an oblivious adversary selects a bounded loss function \(_{tk}:\) for every round \(t[T]\) and every arm \(k[K]\). In each round \(t\), then we begin by sampling a context \(c_{t}\) i.i.d. from an unknown distribution \(\) over \(\), and we reveal this context to the learner. Based on this context, the learner selects an arm \(A_{t}[K]\) to play. The adversary then reveals the function \(_{t,A_{t}}\), and the learner suffers loss \(_{t,A_{t}}(c_{t})\). Notably, the learner observes the loss for _every_ context \(c\), but only for the arm \(A_{t}\) they actually played.

We would like to design learning algorithms that minimize the expected regret of the learner with respect to the best fixed mapping from contexts to actions. In particular, letting \(=\{:[C][K]\}\), we define

\[=_{^{*}}[_{t=1}^{T}_{t,A _{t}}(c_{t})-_{t,^{*}(c_{t})}(c_{t})]\,.\]

  Algorithm & Regret & Computation \\  Exp4 Auer et al. (2002) & \(K\) & \(2^{K}\) \\  Saha et al. (2020) & \(T}\) (\(K^{2}\)) & \(KT\) \\  Algorithm 1 & \(\) & \(K\) \\  

Table 1: Related works in the sleeping bandits framework ignoring all logarithmic terms. The improved regret bound of Saha et al. (2020) is restricted to problem instances where availability of all arms are independent.

Note that here, the expectation is defined both over the randomness of the algorithm and the randomness of the contexts.

Non-uniform action setsFor some applications (specifically, for sleeping bandits), we will find it useful to restrict the action set of the learner as a function of the context. To do so, we associate every context \(c\) with a fixed non-empty set of active arms \(_{c}[K]\). In round \(t\), we then restrict the learner to playing an action \(A_{t}\) in \(_{c_{t}}\) and measure the regret with respect to policies in the restricted class \(=\{:[K][C]\ ;\ (c)_{c}\}\). All the analysis we present later in the paper will apply to the non-uniform action set case (which includes the full action set case above as a special case).

### Challenges to extending existing algorithms

It is not a priori obvious that any learning algorithm in this setting can obtain regret independent of the size of \(\). Indeed, without the ability to cross-learn between contexts (i.e., only observing the loss for the current action and the current context), one can easily prove a lower bound of \((|KT K})\) by choosing an independent hard bandits instance for each of the contexts in \(\). With the ability to cross-learn between contexts, we side-step this lower bound by being able to gain a little bit of information about each context in each round - however, this information may not be equally useful for every context (e.g., it may be the case that arm \(1\) is a useful arm to explore for context \(c_{1}\), but is already known to be very sub-optimal in context \(c_{2}\)).

In Balseiro et al. (2019), the authors provide an \(()\) regret algorithm for this problem in the setting where the learner is aware of the context distribution \(\). This algorithm essentially runs one copy of EXP3 per context using the following unbiased estimator of the loss (which takes advantage of the cross-learning between contexts):

\[_{tk}(c)=(c)}{_{c}[p_{tk}(c)]} (A_{t}=k),\] (1)

where in this expression, \(p_{tk}(c)\) is the probability that the algorithm would choose arm \(k\) in round \(t\) if the context was \(c\). A straightforward analysis of this algorithm (following the analysis of EXP3, but using the reduced variance of this estimator) shows that it obtains \(()\) regret.

The only place knowledge of \(\) is required in this algorithm is in computing the denominator \(f_{tk}(p)=_{c}[p_{tk}(c)]\) of our estimator. It is natural, then, to attempt to extend this algorithm to work in the unknown distribution setting by replacing the distribution \(\) with the empirical observed distribution of contexts so far; that is, replacing \(f_{tk}(p)\) with \(_{tk}(p)=_{s=1}^{t}p_{sk}(c_{s})\). Unfortunately, this approach runs into the following two challenges.

First, since this quantity appears in the denominator, small differences between \(f_{tk}(p)\) and \(_{tk}(p)\) can lead to big differences in the estimated losses. This can be partially handled by replacing the denominator with a high probability upper bound \(_{tk}(p)+C_{t}\) for some confidence constant \(C_{t}\). However, doing so also introduces a bias penalty of \(O(C_{T}T)\) into the analysis. Tuning this constant leads to another \(T^{2/3}\) algorithm.

Secondly, when we compute the estimator \(_{tk}(p)=_{s=1}^{t}p_{sk}(c_{s})\), we have the issue that \(p_{tk}\) is not independent from the previous contexts \(c_{t}\). This can cause the gap between \(_{tk}(p)\) and \(f_{tk}(p)\) to be larger than what we would expect via concentration inequalities. Avoiding this issue via union bounds leads to a polynomial dependency on \(||\).

### Our techniques

Avoiding high-probability bounds.While prior work ensured that \(_{t,k}(p)+C_{t} f_{t,k}(p)\) with high probability, the analysis only requires this to hold in expectation. The following lemma shows that this relaxation allows for smaller confidence intervals. While we don't use this specific lemma in our later proofs, we believe that this result might be of independent interest.

**Lemma 1**.: _Let \(X_{1},,X_{t}\) be i.i.d. samples from a distribution \(\) over \(\) with mean \(\), and let \(=_{s=1}^{t}X_{s}\) denote the empirical mean. Then_

\[[+16/t}][]\,.\]

This implies that we only need order \(\) many i.i.d. samples for estimating \(f_{t,k}(p)\) with sufficient precision.

Increasing the number of i.i.d. samples.In order for us to use samples from the context distribution in theorems like Lemma 1, these samples must be independent and must not have already been used by the algorithm to compute the current policy. We increase the number of independent samples via the idea of decoupling the estimation distribution from the playing distribution.

To elaborate, consider the setting of a slightly different environment that helps us in the following way: instead of observing the loss of the action we played from our distribution \(p_{t}\), the environment instead reveals to us the loss of a fresh "exploration action" sampled from a snapshot \(s\) of our action distribution from a previous round. If the environment takes a new snapshot of our policy every \(L\) rounds (i.e., \(s=p_{eL}\) for some integer \(e\)) then this reduces the number of times we need to estimate the importance weighting factor in the denominator of (1) to once in every epoch, since the importance weighting factor stays the same throughout the epoch. At the same time, we have \(L\) fresh i.i.d. samples of the context distribution available at the start of every new epoch.

We present a technique to operate in the same way without such a change in the environment. Instead of the environment taking snapshots of our policy, the algorithm will be responsible for taking snapshots \(s\) itself. In order to generate unbiased samples from \(s\) (while actually playing \(p_{t}\) in round \(t\)), we decompose the desired action distribution \(p_{t}\) into an equal mixture of the snapshot \(s\) and an exploitation policy \(q_{t}\): \(p_{t}=(s+q_{t})\). We implement this mixture by tossing a fair coin of whether to sample from \(s\) or \(q_{t}\), and only create a loss estimation for when we sample from \(s\). This approach fails when \(q_{t}\) is not a valid distribution over arms, but we show that this is a low probability failure event by ensuring stability in the policy \(p_{t}\).

Equipped with these two high level ideas, we now drill down into the technical details of our algorithm.

## 3 Main result and analysis

### The algorithm

We will now present an efficient algorithm for the unknown distribution setting which achieves \(()\) regret. We begin by describing this algorithm, which is written out in Algorithm 1.

At the core of our algorithm is an instance of the Follow the Regularized Leader (FTRL) algorithm with entropy regularization (i.e., EXP3). In each round \(t\), we will generate a distribution over actions \(p_{t,c_{t}}\) for the current context \(c_{t}\) via

\[p_{t,c_{t}}=_{x([K])} x,_{s=1}^{t-1} {}_{s}(c_{t})-^{-1}F(x)\,,\]

where \(F(x)=_{i=1}^{K}x_{i}(x_{i})\) is the unnormalized neg-entropy, \(\) is a learning rate and the \(\) are loss estimates to be defined later.

We will **not** sample the action \(A_{t}\) we play in round \(t\) directly from \(p_{t}\). Instead, we will sample our action from a modified distribution \(q_{t}\) that we will construct in a such way so that the probability \(q_{t,i}\) of playing a specific action is not correlated with every single previous context \(c_{s}\) (for \(s<t\)). This will then allow us to construct loss estimates in a way that bypasses the second obstacle in Section 2.1.

To do so, we will divide the time horizon into epochs of equal length \(L\) (where \(L=()\), to be specified exactly later). Without loss of generality, we assume \(L\) is even and that \(T\) is an integer multiple of \(L\). We let \(_{e}\) to denote the set of rounds in the \(e\)-th epoch.

At the end of each epoch, we take a single snapshot of the underlying FTRL distribution \(p_{t}\) for each context and arm; that is, we let

\[s_{e+2,c,k}=p_{eL,c,k}\,,s_{1,c,k}=s_{2,c,k}= _{c}|}&k_{c}\\ 0&\]

During epoch \(e\), the learner has two somewhat competing goals. First, they would like to play actions drawn from a distribution close to \(p_{t,c_{t}}\) (as this allows us to bound the learner's regret from the guarantees of FTRL). But secondly, the learner would also like to compute estimators of the true losses \(_{t,k}\) with small variance and sufficiently small bias. To do this, the learner requires a good estimation of the probability of observing each loss (which in turn depends on both the context distribution and the distribution of actions they are playing).

We avoid the problems inherent in estimating a changing distribution by committing to observe the loss function of arm \(k\) with probability \(f_{ek}=_{c}[s_{eck}/2]\) for any \(t_{e}\). This is guaranteed by the following rejection sampling procedure: we first play an arm according to the distribution

\[q_{t,c_{t}}=p_{t,c_{t}}&\,k[K]:\,p_{t,c_{t},k}  s_{e,c_{t},k}/2\\ s_{e,c_{t}}&\]

After playing arm \(k\) according to \(q_{t,c_{t}}\), the learner samples a Bernoulli random variable \(S_{t}\) probability \(}{2q_{tc_{t}}}\). If \(S_{t}=0\), they ignore the feedback from this round; otherwise, they use this loss. This subsampling ensures that the probability of observing the loss for a given arm is constant over each epoch. To address the first goal and avoid paying large regret due to the mismatch of \(p_{t}\) and \(q_{t}\), we tune the FTRL algorithm to satisfy \(p_{t}=q_{t}\) with high probability at all times.

To actually construct these loss estimates, we need accurate estimates of the \(f_{ek}\). To do this we use contexts from epoch \(e-1\) that were not used to compute \(s_{ec}\), and are thus free of potential correlations. For similar technical reasons, we will also want to use different sets of rounds for computing estimators \(_{ek}\) of \(f_{ek}\) and estimators \(_{tk}\) of the losses \(_{tk}\). To achieve this, we group all timesteps into consecutive pairs. In each pair of consecutive timesteps, we play the same distribution and randomly use one to calculate a loss estimate, and the other to estimate the sampling frequency.

To be precise, let \(_{e}^{f}\) denote the time-steps selected for estimation the sampling frequency and \(_{e}^{}\) the time-steps to estimate the losses. Then we have

\[_{ek}=_{e-1}^{f}|}_{t _{e-1}^{f}}}{2}\,,\]

which is an unbiased estimator of \(f_{ek}\). The loss estimators are

\[_{tk}=}{_{ek}+} (A_{t}=k S_{t} t_{e}^{})\,,\]

where \(\) is a confidence parameter (which again, we will specify later).

This concludes our description of the algorithm. In the remainder of this section, we will show that for appropriate settings of the parameters \(\), \(\), and \(L\), Algorithm1 achieves \(()\) regret (the parameter \(\) in the following theorem is a parameter solely of the analysis and determines the failure probabilities of various concentration inequalities).

Figure 1: Illustration of the timeline of Algorithm1. At the end of epoch \(_{e}\), the snapshot \(s_{e+2}\) is fixed. The contexts within epoch \(_{e}\) are used to compute loss estimators for epoch \(_{e+1}\), which are fed to the FTRL sub-algorithm.

**Theorem 1**.: _For any \(\), \(\), \((8K/)\), Algorithm 1 satisfies_

\[=O((++L}{}++ (-))KT++L)\,.\]

Tuning \(=2(8KT)\), \(L=}=()\), \(==(1/)\), and \(==(1/)\) yields a regret bound of

\[=()\,.\]

Computational efficiency.In general, the computational complexity is \(\{tK,||\}\) and the memory complexity \(\{t,||K\}\), where the agent is either keeping a table of all \(K||\) losses in memory, which are updated for all contexts in every round, or the agent keeps all previous loss functions in memory and recomputes the losses of all actions for the context they observe. (This is assuming that we can store and evaluate the loss function with \(O(1)\) memory and compute.) In special cases, this can be significantly more efficient. In both sleeping bandits as well as bidding in first-price auctions, we can store the accumulated loss functions in \(O(1)\), which means that we have a total per-step runtime and memory complexity of \(O(K)\). This is on par with the \(O(T^{2/3})\) regret algorithm of Balseiro et al. (2019), which also has a per-step runtime and memory complexity of \(O(K)\).

### Analysis overview

We begin with a high-level overview of the analysis. Fix any \(:[C][K]\), and let for each \(c\), let \(u_{c}=e_{k}([K])\). We can then write the regret induced by this policy \(\) in the form

\[(u)=[_{t=1}^{T} q_{t,c_{t}}-u_{c_{t}}, _{t,c_{t}}].\] (2)

We would like to upper bound this quantity (for an arbitrary \(u\)). To do so, we would like to relate it to \([_{t=1}^{T} p_{t,c_{t}}-u_{c_{t}},_{t,c _{t}}]\), which we can bound through the guarantees of FTRL. To do so, we will introduce two new proxy random variables \(_{tc}^{K}\) and \(_{tc}([K])\) which have the property that they are independent of \(_{e}\) conditioned on the snapshot at the end of epoch \(e-2\).

Specifically, recall that \(s_{e}\) is the snapshot determined at the end of \(e-2\). Then, conditioned on \(s_{e}\) (and in particular, writing \(_{e}[]\) to denote \([ s_{e}]\)), we define:

* \(f_{c}=_{c}[s_{ec}/2]\). Note that the \(_{e}\) used by Algorithm 1 is an unbiased estimator of \(f_{c}\).
* \(_{ek}=+}{f_{ek}+}\) is a deterministic function of \(_{ek}\).
* For each \(t_{e}\), we let \(_{tck}=_{tck}}{_{ek}}= }{f_{ek}+}(A_{t}=k S_{t} t_{e} ^{})\). \(_{tck}\) is a loss estimator independent of \(_{e}\) such that \(_{e}[_{tck}]=}{f_{ek}+}_{tck} 1\). Since \(f_{ek}\) is a determistic function of \(s_{e}\), \(\) is independent of \(_{ek}\) conditioned on \(s_{e}\).
* For each \(t_{e}\), we let \(_{tc}=_{x([K])} x,_{e^{} =1}^{e-1}_{s_{e^{}}}_{sc}+_{t^{ }_{e},t^{}<}_{t^{}c} -^{-1}F(x) s_{e+1,c}(-_{t^{} _{e},t^{}<}_{t^{}c})\). \(\) can be thought of as the action played by an FTRL algorithm which consumes the loss estimators \(\) up through epoch \(e-1\), but uses our new pseudo-estimators \(\) during epoch \(e\). Like \(\), \(\) is independent of \(_{ek}\) conditioned on \(s_{e}\).

We perform all our analysis conditioned on the following two events occurring with high probability. First, that our context frequency estimators \(_{ek}\) concentrate - i.e., that \(|_{ek}-f_{ek}|\) is small w.h.p. Second, that our loss proxies \(\) concentrate in aggregate over epochs, i.e. that \(_{t_{e}}_{tck}\) is never too large. Both conditions together are sufficient to guarantee that the aggregation of \(_{t_{e}}_{tck}\) is also never too large, which is crucial in guaranteeing \(q_{t}=p_{t}\).

Conditioned on these two concentration events holding, we can strongly bound many of the quantities. Most notably, we can show that, with high probability, \(q_{t,c_{t}}=p_{t,c_{t}}\) for all rounds \(t\). This allows us to replace the \(q_{t,c_{t}}\) terms in (2) with \(p_{t,c_{t}}\). We then split \((u)\) into four terms and label them as follows:

\[(u)= [_{t=1}^{T} p_{t,c_{ t}}-u_{c_{t}},_{t,c_{t}}-_{t,c_{t}} ]}_{_{1}}+[_{t=1}^{T} _{t,c_{t}}-u_{c_{t}},_{t,c_{t}}- _{t,c_{t}}]}_{_{2}}\] \[+[_{t=1}^{T} p_{t,c_{ t}}-_{t,c_{t}},_{t,c_{t}}-_{t,c_{t}} ]}_{_{3}}+[_{t= 1}^{T} p_{t,c_{t}}-u_{c_{t}},_{t,c_{t}} ]}_{}.\]

We then show (again, subject to these concentration bounds holding) that each of these terms is at most \(()\). Very briefly, this is for the following reasons:

* \(_{1}\) and \(_{2}\): Here we use the independence structure we introduce by defining \(\) and \(\) (along with scheduling the different pieces of the algorithm across different epochs). For example, conditioned on \(s_{e}\), we know that \(_{t}\) is independent of \(p_{t}\), so we can safely replacethe \(_{t}\) in \(_{1}\) with its expectation. Similarly, \(_{t}\) is independent of both \(_{t}\) and \(_{t}\) conditioned on \(s_{e}\).
* \(_{3}\): Here we do not have independence between the two sides of the inner product. But fortunately, we can directly bound the magnitude of the summands in this case, since we can show that \(|_{tc}-p_{tc}|\) and \(|_{tc}-_{tc}|\) are both small with high probability (in fact, for all \(c\) simultaneously).
* **ftrl**: Finally, this term is bounded from the standard analysis of FTRL.

The full proof of Theorem1 can be found in the Appendix (SectionC) of the Supplementary Material.

## 4 Applications

### Bidding in first-price auctions with unknown value distribution

We formally model the first-price auction bidding problem as follows. A bidder participates in \(T\) repeated auctions. In auction \(t\), they have a value \(v_{t}\) for the current item being sold, with \(v_{t}\) being drawn i.i.d. from some unknown distribution \(\) supported on \(\). They submit a bid \(b_{t}\) into the auction. We let \(m_{t}\) denote the highest other bid of any other bidder into this auction (and allow the adversary to choose the sequence of \(m_{t}\) obliviously in advance). If \(b_{t} m_{t}\), the bidder wins the auction and receives the item (and net utility \(v_{t}-b_{t}\)); otherwise, the bidder loses the auction and receives nothing. In both cases, the bidder only observes the binary feedback of whether or not they won the item - they do not observe the other bids or \(m_{t}\).

The bidder would like to minimize their regret with respect to the best fixed mapping \(b^{*}:\) from values to bids, i.e.,

\[=_{b^{*}}_{t=1}^{T}(v_{t}-b^{*}(v_{t}))(b^{*}(v_ {t}) m_{t})-_{t=1}^{T}(v_{t}-b_{t})(b_{t} m_{t}).\]

In (Balseiro et al., 2019), the authors prove a lower bound of \((T^{2/3})\) for this problem (based on a related pricing lower bound of Kleinberg and Leighton (2003)). By applying their algorithm for cross-learning between contexts, they show that it is possible to match this in the case where the buyer knows their value distribution \(\), but only achieve an upper bound of \((T^{3/4})\) in the unknown distribution case. By applying Algorithm1, we show that it is possible to achieve a regret bound of \((T^{2/3})\) in this setting, nearly (up to logarithmic factors in \(T\)) matching the lower bound.

**Corollary 1**.: _There exists an efficient learning algorithm that achieves a regret bound of \((T^{2/3})\) for the problem of learning to bid in a first-price auction with an unknown value distribution._

Proof.: Let \(K=T^{1/3}\). We first discretize the set of possible bids to multiples of \(1/K=T^{-1/3}\). Note that this increases the overall regret by at most \(T/K=T^{2/3}\); in particular, if bidding \(b\) results in expected utility \(U\) for a bidder with some fixed value \(v\), bidding any \(b^{}>b\) results in utility at least \(u-(b^{}-b)\).

Now, we have an instance of the contextual bandits problem with cross-learning where \(=\), \(\) is the distribution over contexts, the arms correspond to the \(K\) possible bids, and \(_{tb}(v)=1-(v-b)(b m_{t})\). This setting naturally has cross-learning; after bidding into an auction and receiving (or not receiving) the item, the agent can figure out what net utility they would have received under any value they could possibly have for the item. From Theorem1, this implies there is an algorithm which gets \(()=(T^{2/3})\) regret. 

### Sleeping bandits with stochastic action set

In the sleeping bandits problem, there are \(K\) arms. Each round \(t\) (for \(T\) rounds), a non-empty subset \(S_{t}[K]\) of the arms is declared to be "active". The learner must select one of the active arms \(k S_{t}\), upon which they receive some loss \(_{tk}\). We assume here that the losses are chosen by an oblivious adversary, but the \(S_{t}\) are sampled independently every round from an unknown distribution \(\). The learner would like low regret compared to the best fixed policy \(:[2^{K}][K]\) mapping \(S_{t}\) to an action \((S_{t})\) to play.

Note that this fits precisely within the contextual bandits with cross-learning framework, where the contexts \(c_{t}\) are the sets \(S_{t}\), we have non-uniform action sets \(A_{c}=S[K]\), and cross-learning is possible since the loss \(_{tck}\) of arm \(k\) in context \(c\) in round \(t\) does not depend on \(c\) as long as \(k\) belongs to the set corresponding to the context (and if \(k\) does not, we cannot even play \(k\)).

**Corollary 2**.: _There exists an efficient learning algorithm that achieves a regret bound of \(()\) for the sleeping bandits problem with stochastic action sets drawn from an unknown distribution._

## 5 Conclusion

We resolved the open problem of Balseiro et al. (2019) with respect to optimal cross-context learning when the distribution of contexts is stochastic but unknown. As a side result, we obtained an almost optimal solution for adversarial sleeping bandits with stochastic arm-availabilities. Not only is this algorithm the first to obtain optimal polynomial dependencies in the number of arms and the time horizon, it is also the first computationally efficient algorithm obtaining a reasonable bound. Finally, we closed the gap between upper and lower bounds for bidding in first-price auctions.