# Example for the Loan table (the \(financial\) database)

###### Abstract

Large Language Models (LLMs) have made significant progress in assisting users to query databases in natural language. While LLM-based techniques provide state-of-the-art results on many standard benchmarks, their performance significantly drops when applied to large enterprise databases. The reason is that these databases have a large number of tables with complex relationships that are challenging for LLMs to reason about. We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints. Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks.

## 1 Introduction

Large Language Models (LLMs) have significantly enhanced AI agents' capacity to assist humans in a variety of important tasks, including co-pilot programming (Chen et al., 2021; GitHub, Inc., 2021), program verification (Wu et al., 2024; Chakraborty et al., 2023), and math problem solving (Zhou et al., 2024). One of the fastest-growing areas in this space is the development of LLM-based assistants for querying SQL databases. In this task, a user poses a question to a database in natural language. The agent's goal is to generate an SQL query that, when executed against the database, answers the user's question. Such assistance enables users with different levels of expertise to effectively analyze their data.

Recently, LLM-based solutions have made significant progress in addressing the text-to-SQL problem (Gao et al., 2024; Li et al., 2024). While GPT-based methods have quickly reached near-human performance on academic benchmarks, like Spider (Yu et al., 2018), they struggle to provide high-quality user assistance on large industrial databases (Sequeda et al., 2023; Li et al., 2023). One of the core challenges is that industrial databases model many objects with complex relationships between them. To transform a natural language question into an SQL query, the LLM must effectively reason about these intricate relationships, which is highly non-trivial for LLM models. Interestingly, we found that GPT4 can even indicate in some cases that it needs help with logical reasoning on complex databases. Here is a common GPT4 output message on a question that requires multiple joins from ACME insurance database (Sequeda et al., 2023): _'This join may need adjustment based on the actual logic of relating claims to policy coverage details.'_. While we do provide the database schema as part of the input, it is still challenging for LLMs to formally reason about database logic.

In this work, we propose a new text-to-SQL framework, Lucy, designed for large databases with complex relationships between objects. Our main underlying idea is to combine the ability of LLM models to effectively relate user questions to database objects with the power of automated reasoning to analyze relationships between these objects. The Lucy workflow consists of three high-level steps. First, upon receiving a user's question, we identify the relevant objects and their attributes in the target database. In the second step, we employ an automated reasoner to build a view that joins the relevant tables based on relational constraints defined by the database schema. This view contains all the necessary informationto answer the user's questions. In the third step, we construct a query targeting this view to produce an answer for the user. Our contributions are summarized as follows:

* We propose a text-to-SQL framework Lucy capable of querying large industrial databases. To the best of our knowledge, Lucy is the first framework designed to support logical reasoning in the context of the text-to-SQL problem.
* Lucy offers several advantages:
* alleviates the need for complex reasoning from a LLM, allowing it to focus on tasks where it currently excels,
* supports modeling and reasoning about complex, commonly used design patterns to model relationships, like many-to-many, Star, and Snowflake,
* its modular workflow allows for effective debugging of failures,
* performs zero-shot generation and does not require fine-tuning of LLMs.
* Our experimental results demonstrate significant performance improvements on several standard benchmarks as well as introduced large benchmarks. We also demonstrate the debugging capabilities of Lucy.

## 2 Motivation

To provide high-quality user assistance in text-to-SQL tasks, we face two types of challenges. The first type of challenge comes from the formulation of the user's question. A question can be poorly specified, ambiguous, or require additional knowledge that is not present in the question. For example, the user might ask to list clients eligible for a loan; however, the eligibility criteria are not present in the question (Li et al., 2023, 2024b). The second class is related to the complexity of the queried database that can have a large number of tables with complex relations between them (Sequeda et al., 2023; Li et al., 2023). In this work, we focus on the second class. One approach to deal with complex relationships is to introduce an intermediate layer, like a knowledge graph or ontology structure, that contains rich information about the underlying database. Then, LLMs generate queries to this knowledge graph using specialized languages, e.g., SPARQL, (Sequeda et al., 2023). In turn, these queries can be automatically translated to SQL. While this approach does show promise, it does not alleviate the core issue: an LLM is still expected to reason about complex relations between objects in this intermediate representation. Moreover, such a rich intermediate layer, like an ontology, might not be easy to obtain for a database. Other standard techniques, like additional training, multi-shot or fine-tuning, also rely on LLMs to perform constrained reasoning steps (Gao et al., 2023; Pourreza and Rafiei, 2024; Gao et al., 2024). To the best of our knowledge, dealing with complex relationships in text-to-SQL remains an open problem. In order to isolate the underlying challenges in this problem, we created an example database

Figure 1: Objects and their relations in the database ddo.

that covers standard relationship patterns adopted in industry and academia. We identified a set of simple and clearly formulated questions and demonstrated that even on this simplified schema and clear questions, state-of-the-art LLMs struggle to assist the user.

### Database description

We describe a minimal example database schema that contains basic relations, like 1:1 and 1:m, and more advanced relationship patterns, like m:m and Star, and analyze the performance of LLMs on this schema (See Appendix A for relational database definitions). Suppose a business sells cloud compute resources to customers and uses a database, ddo, to manage its Day-to-Day Operations. Figure 1 shows objects' corresponding tables, their relationships, and a subset of attributes. In particular, each table has a primary key, e.g., Location_id_, and might have foreign keys to refer to another table, e.g., Client refers to Location using Client_loc_id. All attributes relevant to our examples are shown in Figure 1 with self-explanatory names. ddo manages payments (Payment) and marketing retention strategies (Retention) for clients (Client) and resources (ResourcePool) in datacenters (Datacenter). This example is in part inspired by the VMware vSphere data model (discussed in Section 5). The full data model contains hundreds of types of resources that form deep tree-like structures [Managed Object, 2024]. Next, we consider how relationships between objects are modeled in ddo. Figure 1 already defines basic relationships, including 1:1 (dotted edges) and 1:m (solid edges).

**Many-to-many (m:m).** Client and ResourcePool are related via a m:m relationship (the dashed edge) meaning that a client might use multiple resource pools and one resource pool can serve multiple clients. The table RsPool2Client models this relation.

**Star.** A Star pattern is a type of database schema composed of a single, central fact table surrounded by dimension tables. There are two groups of objects connected in a Star patterns in our example. Star A keeps track of retention marketing strategies for each client that can be either Gift or/and Bonus. Star B records clients' payments (Payment). Payments' amounts are stored in the PayAmount table. Each amount can be exactly one of three types: Tax, Supercharge, and Income.

**Snowflake.** A Snowflake schema consists of one fact table connected to many dimension tables, which can be connected to other dimension tables through a many-to-one relationship. In ddo, database resource pools are modeled using the snowflake pattern. Each resource pool has configurations (Config) and snapshots of the current usage (Runtime). Config and Runtime have two children nodes each to define CPU and memory properties.

**Lookup.** A lookup table is a table that contains descriptions and code values used by multiple tables, e.g., zip codes, country names. etc. In ddo, Location is a lookup table that stores geo-location related data for quick access.

  _Q2: List customers who use datacenters with names starting with ‘dev’. Output clients and datacenters names._ \\  /*OPT4 generated SQL+/: \\ select CLIENT.name, DATACENTER.name \\ from CLIENT \\ join LOCATION on Location:id = Client.loc\_id \\ join DATACENTER.name like ‘dev’. \\ /*Correct SQL+/ \\ select CLIENT.name, DATACENTER.name \\ from DATACENTER \\ join COMPUTE on DATACENTER.id = COMPUTE.dc\_id \\ join RSPool2Client on \\ RSPool2Client.d + RSPool2Client.rgbool\_id \\ join CLIENT on CLIENT.d = RSPool2Client.client\_id \\ where DATACENTER.name like ‘dev’. \\  

Table 1: User’s questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.

### User questions

We consider three simple questions to ddo that are well formulated: outputs are explicitly specified, so no additional information is needed to answer them. We use gpt4 ('gpt-4-0125-preview'), and promptB (Sequeda et al., 2023) for these questions. For each question, we present a ground truth answer and a GPT answer. Table 1 presents both questions (Q3 is presented in Appendix C.1).

Question Q1 is _'List customers who use datacenters with names starting with 'dev'_. _Output clients and datacenters names'_. The user asks for information that relates clients and datacenters. Consider GPT's answer. GPT misses the core logic of the database: _clients_ and datacenter _resources_ are related via a m:m relation (modeled with RsPool2Client). GPT outputs clients and datacenters that share the same location, which is incorrect.

Question Q2 is _'List resource pool names with CPU overhead limit greater than runtime overall usage by 100'_. Here the user asks about resource pool properties. However, the GPT answer ignores the database's primary/foreign relations. It performs an inner join between ResourcePool, cCPU, and rCPU tables, using non-existent attributes ResourcePool_config_id_ and ResourcePool_runtime_id, which is clearly incorrect.

In summary, these examples demonstrated that LLMs struggle to handle complex relationships between objects.

## 3 Framework design

In this section, we present our framework Lucy. Figure 2 illustrates the workflow diagram, and Algorithm 1 shows the main steps of the workflow. There are two inputs to the framework. The first input is a user question \(Q\). The second input is dbModel, which is a description of the database schema that we discuss in the next section (Section 3.1). The workflow consists of three sequential subtasks: MatchTables, GenerateView, and QueryView. MatchTables identifies the relevant tables and their attributes related to the user question (Section 3.2). GenerateView finds a combined view of relevant tables taking into account database constraints (Section 3.3). The third phase, QueryView, takes \(\) and the user question \(Q\) and produces an SQL query \(\) for \(\) (Section 3.4). To simplify notations, we assume that dbModel is a global variable in Algorithm 1.

### Database model (dbModel)

We start with dbModel, or dbm for short. dbm is a data structure that contains aggregated information about the database, maintained as a JSON structure. dbm should be constructed once for a database as the structure of the database is relatively stable. dbm can always be extended if the database requires modifications. Here are the two main blocks of dbm:

_Database schema._ The schema is written using the SQL Data Definition Language (CREATE TABLE statements). It includes table names, names and types of columns in each table, and database constraints such as primary and foreign keys. It can also contain optional user comments associated with each table and column. We refer to tables and constraints as dbm.tables and dbm.constraints, respectively. We extract this information in the form of JSON. Appendix D.1.1-D.1.2 shows examples of these structures.

Figure 2: Lucy’s high-level workflow. Red colored boxes indicate phases performed by LLMs, and a green colored box is a phase performed by an automated reasoner.

Patterns summary.The user can optionally list higher-level design patterns that are not captured by the schema explicitly. This information can help to improve the accuracy of the algorithm. We support m:m, Star, Snowflake, and lookup patterns, but the model is extendable to support other patterns. The user identifies these patterns manually, based on the logic of the target domain. In the future, we envision that the process can be partially automated. Appendix D.1.3 shows the JSON format used to specify pattern structures.

**Formal notations.** We introduce formal notations. dbm.tables contains a list of tables \(t_{i}\), \(i[1,m]\) where \(m\) is the number of tables. dbm.constraints contains a set of pairs \((t_{i},t_{j})\) such that \(t_{i}\) and \(t_{j}\) are related via 1:1, 1:m or m:1 relation. We denote dbm.m:m as a set of triplets \((t_{i},t_{j},t_{k})\), where a join table \(t_{k}\) models a m:m relation between tables \(t_{i}\) and \(t_{j}\). Note that \((t_{i},t_{k})\) and \((t_{j},t_{k})\) must be in dbm.constraints. Additionally, we denote dbm.lookup as the set of lookup tables. For example, in the ddo database, dbm.m:m = {(Client,ResourcePool,RsPool2Client)} and dbm.lookup = {Location}. For a tree-like pattern, like Star or Snowflake, we distinguish between root table and inner tables using two predicates, e.g., star_root(\(t\)) returns True if \(t\) is the root table of a Star and star_inner(\(t\)) returns True if \(t\) is an inner table (not root) of a Star.

### The MatchTables phase

The first phase, MatchTables, needs to find relevant tables and their attributes to the user question. One approach to achieve that can be to provide the schema and a question to an LLM and ask for this information. However, one of the distinguishing features of real-world databases is their large number of tables and attributes. Hence, feeding all of them along with their descriptions to the prompt might not be feasible for many LLM models. Therefore, we build an iterative procedure that takes advantage of database tree-like patterns. In general, this procedure can be customized to best support the structure of a database.

```
0:User question \(Q\), database model dbm.Model
0:Summary view \(V\), SQL query \(\)
1:Phase 1:MatchTables //LLM-based phase
2://get core tables (these are tables that are not inner tables in Star or Snowflake)
3:core_tables (\(\)\(\)\(\)\(\)\(\)(snowflake_inner(\(t\))\(\)star_inner(\(t\)))\(\))
4://identity relevant core tables to the user query
5:\(T\) = promfitA (\(Q\), core_tables, \(\{\}\))
6:\(_{T}\) = \(\{\}\)
7:for\(\)\(\)\(T\)do
8:if\(t\)\(\)\(\)nowflake_root(\(\))\(\)\(\)\(\)star_root(\(\))then
9://\(\)headish-first deepening to identify relevant tables and attributes inside a pattern rooted at \(t\)
10:\(_{T}\) = \(_{T}\)\(_{T}\)
11:else
12:\(_{T}\), = promptA(\(\), \(\),attributes), \(_{T}\) = \(_{T}_{T}^{}\) // identify \(t\)'s relevant attributes
13:Phase 2: GenerateView // constraint reasoner-based phase
14://formaticatic constraint satisfaction problem
15:\(\) = \(\)nowflake_exp(\(_{T}\))
16://solve \(\) to find a path in \(G\) that satisfies constraints \((_{1})\)-\((_{5})\)
17:\(\) = solve_exp(\(\))
18:\(\) build a view \(\) base on \(\) by joining tables along the path \(\).
19:\(\) = build_view(\(\))
20:Phase 3: QueryView //LLM-based phase
21:\(\)= promptC (\(Q\), \(\))
22:return\(\), \(\) ```

**Algorithm 1**Lucy

Algorithm 1 shows MatchTables in lines 2-12. First, the algorithm focuses on tables that are not inner tables of any patterns. We refer to such tables as core tables (core_tables in line 3). For example, Figure 3 shows core tables for ddo. Next, we ask LLM to find relevant tables among these core tables using promptA in line 5. (Appendix D.2.1 shows a promptA with a few examples.) As a result, we obtain a set of relevant core tables. We explore them one by one in the loop in line 7. If it is a root table of a pattern, we perform a search inside the corresponding pattern to find more relevant tables using a breadth-first deepening procedure, IterativePrompting, in line 10 (Algorithm 2 shows IterativePrompting's pseudocode in Appendix D.2). Otherwise, we use promptA to obtain relevant attributes in line 12.

_Example 3.1_.: _Consider questions \(Q1\) and \(Q2\) from Table 1. Figure 3 shows ddo's core tables. For \(Q1\), a LLM identifies relevant core tables: \(T\) = {Client, Datacenter}

(line 5). Since none of these tables is a root of a Snowflake or a Star, we prompt for relevant attributes for each table in line 12 to get \(_{T}\) = {Client.name, Client.gender, Datacenter.name}. Now consider \(Q2\). LLM identifies ResourcePool as a relevant table in line 5. As ResourcePool is the root table of Snowflake (see Figure 1), we begin to explore the pattern tree in a breadth-first order using IterativePrompting in line 10. ResourcePool has two child nodes, Config and Runtime, and several attributes. We query the LLM and find that both Config and Runtime are relevant as well as its attribute ResourcePool.name. Following the breadth-first search order, we consider Config with two descendants cCPU and cMemory and discover cCPU is relevant (Example D.4 in Appendix shows a full version).

### The GenerateView phase

The MatchTables phase identifies a set of relevant tables and their attributes. Next, we construct a view table that combines relevant tables and attributes into a single table.

We build an abstract schema graph \(G\) which provides a graph view of dbm, and define a CSP over this graph. For each table \(t_{i}\) in dbm.tables, we introduce a node in \(G\). We use the names \(t_{i}\) to refer to the corresponding nodes. For each pair of tables \(t_{i}\) and \(t_{j}\), s.t. \((t_{i},t_{j})\), we introduce an edge that connects them. We denote \(V\) the set of nodes in \(G\) and \(E\) its edges. Figure 3 illustrates a part of the graph (core tables) for ddo.

Algorithm 1 shows three main steps of this phase: build an abstract graph representation \(G\) of the schema (line 15); formulate and solve CSP to obtain a path \(\) (line 17); and perform joins along this path to obtain the designed view \(\) (line 19). Next, we describe these steps.

**Problem Formulation.** Let \(T=(_{T})\) be a set of relevant tables returned by MatchTables. We formulate the problem of finding a path \(\) in \(G\) that visits a set of nodes \(T\) and satisfies a set of database constraints.

* \(\) must be a valid path in \(G\). This ensures that we follow primary/foreign keys relationships, i.e., 1:1, 1:m, and build a valid view.
* \(\) visits all relevant tables \(T\). This ensures combining all relevant tables to a view.
* Consider \((t_{i},t_{j},t_{k})\). If \(t_{i}\) and \(t_{j}\) then \(t_{k}\) must occur in \(\) once between \(t_{i}\) and \(t_{j}\). These constraints enforce m:m relationships.
* If \(t\) and \(t\) then \(t\)'s predecessor equals its successor in \(\). This ensures that a lookup table serves as a look-up function for each table individually.
* Cost function: we minimize the number of occurrences of tables outside of \(T\) in \(\). A shorter path that focuses on the tables in \(T\) allows us to build more succinct views.
* \((C_{1})\)-\((C_{5})\) are common constraints that we encounter in the benchmark sets. In general, the user can specify more constraints to capture the logical relationships of the modeled data.
* **Constraint satisfaction problem (CSP).** We define a CSP formulation \(S\) of constraints \((C_{1})\)-\((C_{5})\). We start with a basic formulation. Let \(n\) be the maximum length of the path \(\). For each node \(t_{i}\) in \(G\) and step \(r\), where \(r[1,n]\), we introduce a Boolean variable \(b^{r}_{i}\). \(b^{r}_{i}\) is true iff \(t_{i}\) is the \(r\)th node in \(\). We also introduce a sink-node Boolean variable \(b^{r}_{d}\) for each layer to model paths that are shorter than \(n\). \(S\) contains the following logical constraints: \[(C_{5}):\] \[minimize_{i,t_{i} T}occ_{i}\] (1) \[ i.t_{i} V\] \[ i.t_{i} V\] \[ i.t_{i} V,r[1,n-1] b^{r}_{i}(_{j.(t_{i},t_{j}) E}b^{r+1}_{j}) b^{r+1}_{d}\] (3) \[(C_{2}):\] (4)

Figure 3: A part of the abstract schema graph \(G\) for ddo that includes core tables.

\[(C_{3}) :  k.(t_{i},t_{j},t_{k})\] \[(C_{3}) :  k.(t_{i},t_{j},t_{k}),r[2,n-1] b_{k}^{r}(b_{i}^{r-1} b_{j}^{r+1})(b_{j}^{r-1}  b_{j}^{r+1})\] (6) \[(C_{4}) :  i.t_{i},r[2,n-1] b_{i}^{r}(b_{j}^{r-1} b_{j}^{r+1})\] (7) \[ r[1,n] b_{1}^{r}++b_{|V|}^{r}=1\] (8) \[ r[1,n-1] b_{d}^{r} b_{d}^{r+1}\] (9)

Consider the encoding \(S\). Equations 2 specify integer variables, \(_{i}\), for \(i[1,n]\), that count the occurrences of each table in the path. Equations 8 encode that only one node belongs to a path at each step. Equations 9 encode that if the path visits the sink node, then it must stay there. Other equations encode constraints \((C_{1})\)-\((C_{5})\). By construction, Equations 1-9 generate a valid path in \(G\) that satisfies the constraints \((C_{1})\)-\((C_{5})\).

_Example 3.2_.: _For Q1, solving \(S\) gives the green path between Datacenter and Client in Figure 3. \(S\) rules out the red path as we enforce constraint \((C_{4})\) and optimization \((C_{5})\)._

**Improvements of CSP.** Our basic model \(S\) can be improved to take advantage of Star and Snowflake patterns. Namely, we can leverage the decomposition of \(G\) and find a path \(\) among core tables only. Then, for each core table in \(\) that is a pattern root, and for each inner relevant table in this pattern, we build a path \(^{}\) along the corresponding branch. For example, Figure 1 shows two paths from ResourcePool to cCPU (an orange path) and rCPU (a blue path). We use left join to combine tables along each such branch. Finally, we combine \(\) and \(\)'s into a single view.

**Summary view.** Given a path \(\) in a graph, we join tables along the path using their primary and foreign key relations. We keep the same set of attributes that MatchTables identified. An example of the \(\) for Q1 that corresponds to the green path in Figure 3 is shown in the listing in Table 7 in Appendix D.3.1.

### The QueryView phase.

QueryView takes the summary view \(\) along with the user question, and prompts an LLM to obtain the final SQL using promptC (line 2 in Algorithm 1). promptC is defined in Appendix D.4.1. The listing in Table 7 shows an SQL \(\) to answer Q1 (Appendix D.3.1).

## 4 Discussion on strengths and limitations

**Strengths.** Lucy is designed based on the principle of separation of responsibilities between generative tasks and automated reasoning tasks: each step focuses on either an NLP-related subproblem or a constraint reasoning subproblem. This separation allows us to support a number of unique capabilities. First, Lucy shifts the burden of complex reasoning from LLMs to constraint solvers. Second, we support reasoning on complex relationships, like m:m, lookup, Star or Snowflake. Third, our framework is flexible and extensible as it is easy to incorporate domain-specific constraints as soon as they can be expressed by constraint modeling language. This assumes that the user has a data analytics role and understands the logic of the database. Such formal reasoning capability is important, as it is hard to control LLMs via prompts when non-trivial reasoning is required. Fourth, we can evaluate each phase and diagnose Lucy failure modes. For example, if MatchTables misses relevant tables, this indicates that we need to provide more information about the schema to an LLM. Fifth, based on our evaluation, Lucy can support complex queries that include multiple filtering operators and aggregators, e.g. average or sum. This capability follows from the QueryView phase as the final call to an LLM is performed on a single view table.

**Limitations.** The first limitation is that we cannot guarantee that the SQL query answers the user's question. Given the current state of the art, providing such guarantees is beyond the reach of any copilot method that takes natural language descriptions and outputs structured text, like code or SQL. However, our solution does guarantee that \(\) satisfies database constraints, which is a step forward in this direction. Second, we do not support questionsthat require union operators in the GenerateView phase. In fact, there are no benchmarks available that require the union operator to answer questions. Supporting union would require an extension of MatchTables and GenerateView. Third, we observed experimentally that Lucy struggles with certain types of queries that involve a particular interleaving ordering of filtering and aggregate operators or question-specific table dependencies, like a lookup table that has to be used multiple times to answer the user's question. We further discuss such questions in our experiments.

## 5 Experimental evaluation

In our experimental evaluation, we aim to answer the main questions:

* Is Lucy competitive with existing LLM-based approaches?
* Can we debug Lucy to gain insights about failure modes?
* Can Lucy handle complex questions?

**Setup.** We compare with the following zero-shot baselines: gpt4, nsql, and chat2query (c2q for short). gpt4 and c2q methods are the best zero-shot techniques according to the BIRD leadership board that are accessible for evaluation (Li et al., 2024). nsql is the best open-source large foundation model designed specifically for the SQL generation task (Labs, 2023). chat2query is closed-source but the authors kindly extended their API that we can run experiments with gpt4. We provide all benchmarks and frameworks' results in the supplementary materials. For gpt4 and Lucy, we use the 'gpt-4-0125-preview' API without fine-tuning. We use OR-Tools as a constraint solver (Perron and Didier, 2024) (Appendix E.1 provides full details of the experimental setup).

**Evaluation metrics.** We use the standard Execution Accuracy (\(ex\)) (Li et al., 2023). In addition, we consider a relaxation of this metric. We noticed that frameworks often add additional attributes to the output as the exact format of the output is rarely specified. Hence, we extend \(ex\) to \(esx\) metrics that check if the output of a framework contains the ground truth outputs. To better understand performance characteristics and possible failure modes, we consider the coverage metric that captures whether a framework correctly identified a subset of relevant tables and attributes. Let \(sql_{G}\) be the ground truth answer and \(sql_{F}\) be a generated query. Then we assess the percentage of the ground truth content \(sql_{F}\) captures:

\[cov_{t}=(slq_{F})(slq_{G})|}{|(slq_{G})|}\ \ cov_{a}=(slq_{F})(slq_{G})|}{| (slq_{G})|},\] (10)

where tables () and attributes () are functions that return a set of tables and attributes.

**ACME insurance.** We consider the _ACME insurance_ dataset that was recently published (Sequeda et al., 2023). The dataset represents an enterprise relational database schema in the insurance domain. The authors focused on a subset of 13 tables out of 200 tables and proposed a set of 45 challenging questions. We identified two Star patterns in this database. The authors showed that their method (dw) solved 24 out of 45 problems using intermediate representation of a knowledge graph, while gpt4 solved only 8 problems. However, results are not publicly available, so we cannot perform coverage analysis and compute \(esx\).

We reran the experiment on gpt4 with the same promptB (Appendix C.1.1) and obtained similar results to those reported in (Sequeda et al., 2023). In addition, we extended the schema with descriptions of table attributes from dbModel in the form of comments, which we called gpt4ex (See Appendix E.2 for examples). Table 2 shows our results. First, we observe that there is a strong correlation between coverage and accuracy metrics in the results. c2q and Lucy show good coverage, meaning that they can correctly identify most of

   & gpt4 & cpt4ex & c2q & nsql & Lucy & DW \\   \(cov_{t}\) & 0.44 & 0.47 & 0.82 & 0.31 & **0.95** & - \\ \(cov_{a}\) & 0.36 & 0.42 & 0.81 & 0.25 & **0.93** & - \\  \(ex\) & 9 & 13 & 16 & 2 & **30** & 24 \\ \(esx\) & 9 & 13 & 16 & 3 & **33** & - \\  

Table 2: The _ACME insurance_ dataset.

   & gpt4 & gpt4ex & c2q & Lucy \\   \(cov_{t}\) & 0.46 & 0.44 & 0.44 & 0.**98** \\ \(cov_{a}\) & 0.50 & 0.44 & 0.48 & **0.98** \\  \(ex\) & 6 & 4 & 2 & **17** \\ \(esx\) & 9 & 5 & 2 & **18** \\  

Table 3: The _Cloud Resources_ dataset.

the required tables and attributes. They also demonstrate better performance compared to other methods. Our framework shows very high coverage and solves about 30 of benchmarks according to the \(ex\) metric, which outperforms Dw that solves 24 and other methods.

Lucy still cannot solve 13 benchmarks, which is surprising given high coverage. We performed a study to locate where Lucy fails on these benchmarks (See Appendix E.2.1 for all questions where Lucy was unsuccessful). In summary, the majority of failures come from under-specified output attributes or nonstandard aggregators, like specialized formulas to compute an average. In four cases, MatchTables missed a table, and in one case, QueryView missed the attribute to output. The most interesting mode of failure is when we need to perform multiple lookups on the same table. The reason for that is the MatchTables phase identifies only relevant tables but ignores possible relationships between them. Extending MatchTables to retrieve relationships between tables is interesting future work.

BIRD datasets.Next, we consider the state-of-the-art dataset BIRD . From the development set, we chose two datasets with complex relationships between objects: \(financial\) (106 instances) and \(formula1\) (174 instances)1. The accuracy of chat2query on the BIRD development set is \( 58\%\); however, its accuracy on \(financial\) and \(formula1\) are much lower, \( 45\%\). We compare with results from gpt4'23 and c2q available from  and , respectively. However, we reran these benchmarks with gpt4 and gpt4ex as the gpt4'23 results are nearly one year old. Table 5 and Table 4 show results on \(financial\) and \(formula1\), respectively. Lucy and c2q have higher coverage and good accuracy. Lucy shows the best results in most cases. Again, Lucy has very good coverage on \(financial\) but was able to solve only 68 out of 106 queries based on the \(esx\) metric. We manually performed an questions study on the failed questions. There are two major groups there that are interesting. First, Lucy has difficulty if there are multiple orderings, especially nested or ordering in different directions. Second, sometimes, MatchTables adds an additional table that is not needed to find the answer. The rest are either ambiguous questions or small mistakes like outputting a wrong attribute, i.e., \(id\) instead of _name_. See Appendix E.3.3 for examples of questions where Lucy was unsuccessful.

Cloud resources.Next, we propose a new benchmark based on the vSphere API data model . We experimented with this publicly available data model of an industrial product, as it is well-documented and easily accessible via a web interface. It describes the state of the system as well as its configuration parameters. States are stored in a database and queried by customers to keep track of performance, maintenance, and data analysis. We extracted the descriptions of main objects in Managed Object , including data centers, resource pools, hosts, and virtual machines and their properties, and built a database that captures these relationships using 52 tables. Overall, we have two Stars, five Snowflakes and two m:ms patterns. For each table and an attribute, we get descriptions from . As these can be a lengthy description, we use GPT to shorten it to 15 words (see promptD in Appendix E.3.2). We generated data randomly using sqlfaker . We create 20 challenging questions for this benchmark.

Table 3 shows our results. nsql cannot process this benchmark due to a limited context window. We again see that Lucy outperforms other models in both coverage and accuracy. c2q failed on 6 questions with an error 'Unable to generate SQL for this database due to its extensive tables' and it often does not follow instructions on the output columns. In terms of failure mode, Lucy failed in the third phase as it hallucinated some attribute names when names are long, e.g., 'Resourcepoolruntimemory' instead of 'Resourcepoolruntimememory'.

   & gpt4’23 & gpt4 & gpt4ex & c2q & nsql & Lucy \\   \(cov_{t}\) & 0.86 & 0.78 & 0.77 & 0.88 & 0.52 & **0.93** \\ \(cov_{a}\) & 0.84 & 0.75 & 0.75 & 0.81 & 0.50 & **0.94** \\  \(ex\) & 54 & 67 & 65 & 80 & 9 & **83** \\ \(esx\) & 66 & 80 & 79 & 93 & 10 & **103** \\  

Table 4: The \(formula1\) dataset.

   & gpt4’23 & gpt4 & gpt4ex & c2q & nsql & Lucy \\   \(cov_{t}\) & 0.81 & 0.84 & 0.87 & 0.92 & 0.50 & **0.97** \\ \(cov_{a}\) & 0.81 & 0.81 & 0.85 & 0.91 & 0.59 & **0.96** \\  \(ex\) & 36 & 47 & 52 & **59** & 6 & 56 \\ \(esx\) & 38 & 55 & 64 & 62 & 6 & **68** \\  

Table 5: The \(financial\) dataset.