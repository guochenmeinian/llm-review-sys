# FAST: a Fused and Accurate Shrinkage Tree for Heterogeneous Treatment Effects Estimation

Jia Gu

Equal Contribution Center for Statistical Science, Peking University

Caizhi Tang

Equal Contribution Center for Statistical Science, Peking University

Han Yan

Guang Cui

Ant Group

Longfei Li

Ant Group

Jun Zhou

Corresponding Author Guajia@pku.edu.cn, caizhi.tcz@antgroup.com, hanyan@stu.pku.edu.cn, {cuiqing.cq, longyao.llf, jun.zhoujun}@antgroup.com

###### Abstract

This paper proposes a novel strategy for estimating the heterogeneous treatment effect called the Fused and Accurate Shrinkage Tree (FAST). Our approach utilizes both trial and observational data to improve the accuracy and robustness of the estimator. Inspired by the concept of shrinkage estimation in statistics, we develop an optimal weighting scheme and a corresponding estimator that balances the unbiased estimator based on the trial data with the potentially biased estimator based on the observational data. Specifically, combined with tree-based techniques, we introduce a new split criterion that utilizes both trial data and observational data to more accurately estimate the treatment effect. Furthermore, we confirm the consistency of our proposed tree-based estimator and demonstrate the effectiveness of our criterion in reducing prediction error through theoretical analysis. The advantageous finite sample performance of the FAST and its ensemble version over existing methods is demonstrated via simulations and real data analysis.

## 1 Introduction

Causal effects are the magnitude of the response of an effect variable (also called outcome) caused by the effect variable (also called treatment), which is a fundamental and essential issue in the field of casual inference (Imbens and Rubin, 2016). And the heterogeneous treatment effect (abbr. HTE) is usually used to characterize the heterogeneity of causal effects across different subgroups of the population. In recent years, heterogeneous treatment effect estimation has been successfully applied in various fields such as epidemiology, medicine, and social sciences (Glass et al., 2013; Kosorok and Laber, 2019; Turney and Wildeman, 2015; Taddy et al., 2016).

In general, the causal problems can be studied through both experimental studies (also known as randomized control trials, RCTs) and observational studies. Experimental studies are widely regarded as the gold standard for assessing causal effects since the randomization process eliminates the possibility of confounding bias. However, large-scale RCTs can be challenging due to issues related to cost, time, and ethics (Edwards et al., 1999). On the other hand, observational data are often readily available with an adequate sample size. Under certain fairly strong assumptions, such as unconfoundedness assumption, there is a rich literature regarding the estimation of HTE in observational studies, such as tree-based methods (Athey and Imbens, 2016; Wager and Athey, 2018; Athey et al., 2019; Hahn et al., 2020), boosting (Powers et al., 2017), meta learners (Kunzel et al.,2019) and \(R\)-learner (Nie and Wager, 2020). However, the unconfoundedness assumption, which requires measuring all confounders, is in general untestable unless extra information such as the existing of instrumental variables is available (de Luna and Johansson, 2014). And any violations of this assumption may result in seriously invalid causal statements. Various methods have been proposed to mitigate the unmeasured confounding in observational studies, such as the sensitivity analysis (Rosenbaum and Rubin, 1983; Zhang and Tchetgen Tchetgen, 2022), the instrumental variables (IV) approach (Angrist et al., 1996) and the proximal causal inference (Kuroki and Pearl, 2014; Miao et al., 2018; Shi et al., 2020; Cui et al., 2023). However, the validity of these procedures also relies crucially on assumptions that are often difficult to verify in practice.

Given the limitations of relying on individual data sources, data fusion, as a branch of causal inference strategies that integrates both the trial and the observational data, has gained significant interest in the literature (Bareinboim and Pearl, 2016; Colnet et al., 2020; Shi et al., 2022). Existing data fusion methods for estimating the HTE include the \(\) estimator obtained by modeling the confounding function parametrically (Kallus et al., 2018), the semi-parametric integrative estimator under the parametric structural models (Yang et al., 2020) and the integrative R-learner (Wu and Yang, 2022). Besides, (Tang et al., 2022) proposed the Gradient Boosting Causal Tree (\(\)), which integrated the current observational data and their historical controls for estimating the conditional average treatment effect on the treated group (CATT).

This paper presents a novel approach for estimating heterogeneous treatment effects (HTE) in the context of causal data fusion. The proposed method, named Fused and Accurate Shrinkage Tree (\(\)), _avoids_ the need for a two-stage estimation process required in conventional data fusion strategies, which involves modeling and estimating the nuisance confounding bias function. The main contributions of this work can be summarized as follows (i) The authors propose a novel shrinkage method for combining an unbiased and biased estimator, which effectively reduces the mean square error of the unbiased estimator, and provides an easy implementation of the method tailored for the HTE estimation; (ii) The authors extend the conventional node split criterion via a re-scaling technique, which automatically penalizes the use of the observational data with low quality (namely large confounding bias); (iii) The authors also provide a theoretical analysis to explain the advantages of our splitting criterion.

## 2 Background and motivation

### Notations

Let \(=[-1,1]^{p}\) be a \(p\)-dimensional vector of pre-treatment covariates, \(^{q}\) (\(q 0\)) be a possibly unmeasured random vector consisting of the confounding variables, \(D\) be a binary treatment variable (\(D=0\) denotes the control and \(D=1\) denotes the treated) and let \(Y(d)\) be the potential outcome that would be observed when the treatment had been set to \(d\{0,1\}\). We follow the potential outcome framework (Rubin, 1974) to define the heterogeneous treatment effect \(()=(Y(1)-Y(0)|=)\).

Suppose that we can collect two kinds of data: trial data and observational data, and they are described by \(n+m\) quadruples, \(\{Y_{i},D_{i},_{i},S_{i}\}_{i=1}^{n+m}\), where \(S_{i}\) indicates if the \(i\)-th individual would have been recruited (\(S=1\)) or not (\(S=0\)) in the trial. We also denote \(=\{1,2,,n\}\) the set of indices of observations in the RCT study, and \(=\{n+1,n+2,,n+m\}\) the set of indices of observations in the observational study. We define \(e(,,S)=P(D=1|,,S)\) as the propensity score of the trial and observational population, respectively. In practice, due to \(\) being unknown, we usually use \((,S)\) to estimate \(e(,,S)\). In addition, \((,1)\) is unbiased, but \((,0)\) is biased because the unmeasured confounder \(\) in the observational study can be related to the assignment of treatment \(D\). Let \(_{1}()=(Y(1)-Y(0)|=,S=1)\) be the HTE on the trial population. We then make the following fundamental assumption on the trial and observational studies, which facilitates the potential for causal data fusion:

**Assumption 1**.: _(i) For any \(\), \(_{1}()=()\); (ii) \(Y(d) D|(,S=1)\) for \(d\{0,1\}\) and (iii) the propensity score \(<e(,S)<1-\) almost surely for some constant \(0<<1/2\)._

Assumption 1 (i) states that the HTE function is transportable from the trial population to the target population. Stronger versions of Assumption 1 include the ignorability of study participation (Buchanan et al., 2018) and the mean exchangeability (Dahabreh et al., 2019). In the following of this paper, we use \(||\) to denote the number of elements for any set \(\), \( c\) to denote the biggest integer less than or equal to the constant \(c\), and \([p]\) to denote the index set \(\{1,2,, p\}\). For two positive sequences \(\{a_{n}\}_{n 1}\) and \(\{b_{n}\}_{n 1}\), we write \(a_{n}=(b_{n})\) if \(|a_{n}/b_{n}|\) is bounded.

### Tree-based methods

To estimate the HTE, it is reasonable to perform subgroup analysis by appropriately stratifying or matching (Frangakis and Rubin, 2002) the samples into multiple subgroups that differ in the magnitude of treatment effects. In machine learning, tree-based methods (Breiman et al., 1984; Breiman, 2001; Friedman, 2001) are usually used for such stratification tasks, which greedily optimize the loss function, also called splitting criterion, via recursively partitioning feature space. In fact, many tree-based causal methods designed for the HTE estimation were also proposed (Radcliffe and Surry, 2012; Athey and Imbens, 2016; Athey et al., 2019). Along with the development of the tree-based methods, various regularization strategies, either implicit or explicit, have been proposed to mitigate overfitting (Mentch and Zhou, 2020; Agarwal et al., 2022). Recently, tree-based methods have been generalized to address the heterogeneous data from diverse data sources(Nasseri et al., 2022). For convenience, in the following we define a regression tree by two components: a set of leaves \(=\{Q_{j}\}_{j=1}^{J}\) and the associated parameter \(\). We can denote a causal tree by \(T(X;,)=_{j=1}^{J}(Q_{j})\{ Q_{j}\}\), where \(\{\}\) denotes the indicator function and \((Q_{j})\) denotes the casual effect of sub-area indexed by \(Q_{j}\).

### Shrinkage estimation

It is important to note that applying conventional methods, such as the generalized random forest (Athey et al., 2019), separately to trial data and observational data can readily lead to two estimators: the first is unbiased but may exhibit large variability, while the second is potentially biased but usually has a smaller variance due to the much larger amount of observational data. Therefore, the challenge becomes finding the optimal combination of an unbiased estimator and a biased estimator in the data fusion problem. To see this, suppose we have a parameter of interest \(\), an unbiased estimator \(_{u}\), and a (potentially) biased estimator \(_{b}\) of \(\), such that \((_{u})=\), \((_{b})=+b()\), \((_{u})=_{u}^{2}\), \((_{b})=_{b}^{2}\) and \((_{u},_{b})=0\). Consider the family of estimators \(_{w}=\{_{w}|_{w}=w_{b}+(1-w)_{u},0 w 1\}\), then the mean square error (MSE) of its elements admits the following expansion:

\[(_{w}-)^{2}=(_{b}^{2}+b^{2}()+_ {u}^{2})w^{2}-2_{u}^{2}w+_{u}^{2}.\] (1)

Figure 1: The probability density functions (pdfs) of the unbiased estimator (pink) and the biased estimator (blue) in the left panel and the pdf of the shrinkage (fused) estimator under the optimal weight \(W^{*}\) (green) in the right panel. The vertical dashed line represents the true parameter value \(^{*}=0\).

Minimizing (1) with respect to \(w\), we can obtain the unique minimizer \(w^{*}=_{u}^{2}/(_{b}^{2}+b^{2}()+_{u}^{2})\) and the gain of the optimal weighting over the single estimators \(_{u}\) and \(_{b}\) can be characterized by the following formula:

\[(_{w}^{*}-)^{2}=(1-w^{*})_{u}^{2}=w^{*}( _{b}^{2}+b^{2}()).\] (2)

**Comment** The weighting strategy is akin to the classical James-Stein shrinkage estimation (Efron and Morris, 1973; Green and Strawderman, 1991) method, in which it is shown that a multivariate normal vector \(\) (\(p 3\)), as a maximum likelihood estimator (MLE) of its population mean \(=()\), is not minimax optimal, and the MSE of the estimator \(\) can be reduced by shrinking it towards the zero vector \(\) by some factor \(0<w<1\). The zero vector can be viewed as a biased estimator of \(\) with zero variance in their setting. In comparison, we replace the deterministic estimator with a (potentially) biased estimator \(_{b}\): The larger the variance \(_{u}^{2}\) of the unbiased estimator is compared to \(b^{2}()+_{b}^{2}\), the more the fused estimator \(_{w^{*}}\) will be shrunk towards the biased estimator that is less fluctuating. By doing so, one can efficiently mitigate the occurrence of significant estimation error in the unbiased estimator caused by its high variance, as unbiasedness alone _does not_ guarantee reliable estimation performance with a limited sample size. Figure 1 illustrates a concrete example of the benefit provided by the shrinkage estimation, where \(=0,_{u}(,5)\) and \(_{b} N(+2,0.5)\). The fused estimator \(_{w^{*}}\) reduces over \(50\%\) of the MSE compared with the unbiased estimator \(_{u}\).

## 3 Methodology

In this section, we propose a new data fusion strategy, referred to as the Fused and Accurate Shrinkage Tree (FAST). We proceed in a bottom-up manner to provide a clear and intuitive illustration of the entire estimation: we will begin by applying the shrinkage estimation strategy for local data fusion within each sub-region of the feature space given by a pre-specified partition. Then, we propose a fused criterion that incorporates the information contained in the observational data via a simple re-scaling of the conventional criterion. Theoretical guarantees are established in Section 4.

### Local fusion for the HTE estimation

Under a pre-specified partition \(=\{Q_{j}\}_{j=1}^{J}\) of the feature space, let \(_{j}=\{i|i,_{i} Q_{j}\}\) and \(_{j}=\{i|i,_{i} Q_{j}\}\) represent the sets of indices of the trial and observational sub-samples, respectively, that fall within the region \(Q_{j}\). Let

\[=,S)}-,S)}\] (3)

be transformed outcomes of all data, e.g., the transformed outcomes of \(i\)-th sample can be denoted by \(_{i}\). Then under Assumption 1, one can immediately show for the trial population with \(S=1\):

\[(YD|,S=1) = (Y(1)|,S=1)(D|,S= 1)\] \[(Y(1-D)|,S=1) = (Y(0)|,S=1)(1-(D|,S=1)),\ \] \[((|=,S=1)=_{1} ()=().\] (4)

Thus, \(_{u}(Q_{j})=(1/|_{j}|)_{i_{j}} _{i}\) is an unbiased estimator of \((Y(1)-Y(0)| Q_{j},S=1)\), which can be seen as a reasonable approximation of \((Q_{j})\) if \(\) segments the feature space properly such that \(()\) varies slowly in each sub-region \(Q_{j}\). An estimator of \((_{u}(Q_{j}))\) is given by \(_{u}^{2}(Q_{j})=(1/|(_{j}|(|_{j}|-1)))_ {i_{j}}(_{i}-_{u}(Q_{j}))^{2}\). In contrast, for the observational population, the conditional independence no longer holds and \(_{b}(Q_{j})=(1/|_{j}|)_{i_{j}} _{i}\) is a biased estimator concerning \((Q_{j})\), due to the presence of unmeasured confounding (\(\)) on the observational data.

It remains to estimate the weight \(w^{*}(Q_{j})\) composed of the tuple \((_{u}^{2}(Q_{j}),_{b}^{2}(Q_{j}),b^{2}(Q_{j}))\). The first term \(_{u}^{2}(Q_{j})\) can be estimated by \(_{u}^{2}(Q_{j})\). To bypass the unmeasured confounding issue of the observational population, re-sampling techniques, such as the Bootstrap (Efron, 1979; Hall, 1992), can be applied to consistently estimate \(_{b}^{2}(Q_{j})\). However, in the causal data fusion setting, \(_{b}^{2}(Q_{j})=(|_{j}|^{-1})\) is expected to be of a smaller order term compared to \(_{u}^{2}(Q_{j})=(|_{j}|^{-1})\)which is a consequence of the relative sample size between the trial and the observational data. Thus, one can just avoid estimating the negligible term \(_{b}^{2}(Q_{j})\). For the last term, \()}=_{b}(Q_{j})-_{u}(Q_{j})\) serves as a natural estimator of the bias \(b(Q_{j})\). This leads to the following estimator of \(w^{*}(Q_{j})\) and the corresponding fused estimator

\[_{of}(Q_{j})=_{u}^{2}(Q_{j})/(_{u}^{2}(Q_{j})+( )})^{2})\;\;\] (5)

\[_{of}(Q_{j})=_{of}(Q_{j})_{b}(Q_{j})+(1-_{of} (Q_{j}))_{u}(Q_{j}).\] (6)

A fused estimator of the HTE function \(()\) under the partition \(\) can thus be defined as \(_{}()=_{j=1}^{J}_{of}(Q_{j})\{ {x} Q_{j}\}\).

### Adaptive fusion for segmentation

In order to obtain a tree-based partition \(\) designed for the fusion strategy (6), a split criterion is required, which is sufficient to be defined only at the root node given the recursive nature of the partitioning. We follow the honest estimation approach (Athey and Imbens, 2016) to prevent overfitting. Specifically, given a fraction \(0<r<1\) (typically \(r=0.5\)), \( rn\) observations are sampled without replacement from the trial data of sample size \(n\) for the tree structure estimation, while the rest of observations are used for local estimation of the HTE in each leaf node. Let the index sets of the trial data used for the partition and the HTE estimation be \(^{t}\) and \(^{e}\), respectively. We do not further split the observational data to reduce uncertainty, since we have already partitioned the trial data to avoid overfitting.

The conventional criterion for growing a regression tree chooses the index of the split variable and its split value at the root node by minimizing the following goodness-of-fit criterion

\[(,)=_{[p],}(_{i }_{L}^{t}}(_{i}-_{u}( {Q}_{L},^{t}))^{2}+_{i}_{R}^{t}} (_{i}-_{u}(_{R},^{t}) )^{2}),\] (7)

where \(_{L}=\{|_{}\}\), \(}_{L}^{t}=\{i|i^{t},_{i}_{L}\}\) and \(_{u}(_{L},^{t})=(1/|\{i|i^{t}, _{i}_{L}\}|)_{i\{i|i^{t},_{i} _{L}\}}_{i}\), and \(_{R}\), \(}_{R}^{t}\) and \(_{u}(_{R},^{t})\) can be defined correspondingly. Given a tree grown under (7), we fuse the trial data indexed by \(^{e}\) and the observational data indexed by \(\) at each leaf node according to (6) and refer to the resulting tree estimator as a **Shrinkage Tree (ST)**. A direct modification of (7), which aligns more with the fused estimator at the leaf nodes, should be

\[(,)=_{[p],}(_{i }_{L}^{t}}(_{i}-_{of}( {Q}_{L}))^{2}+_{i}_{R}^{t}}( {Y}_{i}-_{of}(_{R}))^{2}),\] (8)

where \(_{of}(_{L})=_{of}(_{L})_{b}( _{L})+(1-_{of}(_{L}))_{u}(_{ L},^{t})\) and \(_{of}(_{R})\) is defined correspondingly. The replacement of the unbiased estimators in (7) with the fused estimators in (8) facilitates a goodness-of-fit criterion of the proposed fusion strategy.

Alternatively, (7) can be interpreted as minimizing the sum of the estimated MSEs of the unbiased estimators at the child nodes, if the two terms on the right-hand side of (7) are divided by the square of their respective sample sizes. By contrast, since the fused estimator \(_{of}\) reduces variance by shrinking the original unbiased estimator to a potentially biased estimator, simply comparing the fused estimators with the outcomes of the trial data as in (8) fails to capture the variability at the child nodes. Instead, an appropriate criterion shall respect the MSE of the fused estimator. To this end, we introduce the following split criterion

\[(,)=_{[p],}((1-_{of}( _{L}))_{u}^{2}(_{L},^{t})+(1- _{of}(_{R}))_{u}^{2}(_{R},^ {t})),\] (9)

where \((1-_{of}(_{L}))_{u}^{2}(_{L},^{t})\) and \((1-_{of}(_{R}))_{u}^{2}(_{R},^{t})\) estimate the MSE of \(_{of}(_{L})\) and \(_{of}(_{R})\), respectively, according to formula (2). Compared to (7), the proposed criterion incorporates the additional information from the observational data into each node split in an adaptive manner by simply re-scaling the estimated MSE of the unbiased estimator.

**Comment** The criterion (9) offers the benefit of local adjustment, which can be intuitively justified. In sub-regions where the observational data exhibit moderate confounding biases, this criterion improves tree building by providing a sharper assessment of the variability of the fused estimator. On the other hand, for sub-regions where the observational data exhibit substantial confounding biases, the estimated weights of those sub-regions approach zero according to (5). In such cases, the criterion reduces to the conventional criterion (7), except for the standardization of the square of the sample size. It is worth mentioning that all the local adjustments achieved by applying this adaptive fusion strategy are data-driven, namely one can just avoid global modeling of the confounding bias function, which requires domain-specific knowledge of the observational studies. Additionally, it also enables the exclusion of the global impact of extremely large confounding biases of the observational data that only exist in certain sub-regions of the feature space.

We denote the partition obtained under criterion (9) as \(}_{of}=\{_{of,1},_{of,2},,_{of,|}_{of}|}\}\), and the corresponding tree-based estimator of the HTE is defined as

\[_{fast}()=_{j=1}^{|}_{of}|} _{of}^{e}(_{of,j})\{_{of,j}\},\] (10)

where the superscript "e" is to show that the RCT data used to construct the fused estimator at the leaf node is indexed by \(^{e}\) and "fast" is an acronym for the name Fused and Accurate Shrinkage Tree, which is due to the data fusion nature of the criterion (9), the shrinkage-type leaf node estimator (6) and its accuracy in terms of the MSE.

### Ensemble fusion

To reduce overfitting, improve robustness against outliers, and enhance generalization, we introduce the bagged version (Hastie et al., 2009) of the FAST, referred to as the rfFAST, as follows: We randomly draw index sets \(^{*}\) of size \(n\) and \(^{*}\) of size \(m\), separately from \(\) and \(\) with replacement. We repeat the process \(B\) times, resulting in \(\{^{*,(b)},^{*,(b)}\}_{b=1}^{B}\). Then, \(B\) estimators \(_{fast}^{*,(b)}()\) can be calculated based on the trial data indexed by \(^{*,(b)}\) and the observational data index by \(^{*,(b)}\). We then define \(_{rffast}()=(1/B)_{b=1}^{B}_{fast}^{*,(b)}()\). For the construction of the prediction intervals, see Zhang et al. (2020).

## 4 Theoretical guarantee

In this section, we formally establish the benefits of the proposed split criterion (9) compared with the conventional criterion (7). To present the theoretical result, we first pose the following regularity conditions that are standard in literature (see e.g., Gyorfi et al., 2002 and Scornet et al., 2015).

**Assumption 2**.: _(i) There exists a positive constant \(<\) such that \(\{(^{2})|S=i\}<\) for \(i=0,1\). (ii) There exists positive constants \(_{}<\) such that \(_{}^{2}<(|=,S=0)\) for any \(\)._

**Theorem 1** (MSE reduction of the proposed split criterion).: _Let \(=(q,c)\) and \(=[p]\). Suppose the node that needs to be partitioned is \(Q_{j}\), under which the sample sizes of the trial data and observational data are \(n_{j}\) and \(m_{j}\), respectively. Let \(M()\) and \(M_{of}()\) be the sum of MSEs of the conventional HTE estimator and the fused HTE estimator on the two child nodes of \(Q_{j}\) split by \(\), respectively. Denote \(b_{}=_{ Q_{j}}|\{(|=,S=0)- (|=,S=1)\}|\). Let \(\) be the solution of the conventional split criterion (7) and \(_{of}\) be the solution of the proposed split criterion (9). Under Assumptions 1-2, we have_

_(i) For any \(\),_

\[()}{M()}-1-^{2}}{ _{}^{2}+n_{j}b_{}^{2}}.\] (11)_(ii) With probability at least \(1-C_{1}e^{-t}\) for some positive constant \(C_{1}<\), it holds that_

\[M()-M(^{*}) C_{2})^{4}( n_{j})}{n_{j}},\] (12) \[\;\;M_{of}(_{of})-M_{of}(^{*}_{of})  C_{3}()^{4}(n_{j})}{m_{j}}+ )^{4}(n_{j})}{n_{j}}),\] (13)

_for some positive constant \(C_{2},C_{3}<\), where \(^{*}\) and \(^{*}_{of}\) are oracle splits defined as_

\[^{*}=*{arg\,min}_{}M()\;\; \;\;^{*}_{of}=*{arg\,min}_{}M_{of}().\]

In the above theorem, the (i) part establishes a uniform MSE reduction result for any split choice \(\) of the proposed split criterion (9). As revealed in (11), the criterion (9) leads to larger MSE reduction on the nodes with a larger variance of \(\) and less bias of the observational data. In addition, the upper bound in (11) decreases as the node sample size \(n_{j}\) decreases, implying that our proposed criterion leads to increasing relative benefits as the tree grows deeper. Besides, in the (ii) part we present non-asymptotic bounds for the discrepancies between the MSEs under the empirically estimated splits and the oracle splits, showing that the MSEs under the estimated splits can achieve a fast convergence rate. As a direct consequence of Theorem 1, the consistency of our final HTE estimator (10) can be established, since it is known from Sornet et al. (2015) and Athey et al. (2019) that the conventional tree-based estimator using only the trial data is mean-squared consistent, and our proposed method leads to a reduced MSE.

**Proposition 1** (Consistency of \(_{fast}\)).: _For almost every \([-1,1]^{p}\), we have \(_{fast}()()\) in probability as \(n,m\)._

## 5 Experiments

In this section, we demonstrate the results of a series of experiments to answer the following two questions: (i) Whether the proposed method can effectively alleviate the impact of confounding bias of observational data and limited sample size of trial data; (ii) Whether the techniques we proposed including local fusion in tree leaves and adaptive fusion in partitioning are valid, respectively.

In consequence, we conducted experiments on both simulated and real-world datasets to verify the effectiveness of our method. We evaluated our method against both traditional tree-based and data fusion-based casual methods. The former includes the classical Transformed Outcome Honest Tree (HT) Athey and Imbens (2016) and its ensemble version Generalized Random Forest (GRF) Athey et al. (2019). The latter includes the simplest fusion estimator (SF) training both trial data and observational data together without distinction and the KPS estimators Kallus et al. (2018). In order to facilitate better comparison and understanding of our proposed method, we demonstrate three versions: the simple implementation, Shrinkage Tree (ST), described in Section 3.1; the improved version, Fused and Accurate Shrinkage Tree (FAST), described in Section 3.2; and its final ensemble version rfFAST described in Section 3.3. The results of each simulation experiment were based on \(B=100\) replications. The ensemble size for all the ensemble estimators was set to \(100\). For the tree estimators, the minimum number of observations required to be at a leaf node was set to \(5\) and the maximum depth of the tree was set to \(10\).

### Simulation

We conducted two sets of simulation experiments to evaluate the finite sample performance of the fused estimator and various baseline estimators. In both experiments, we first generated the pre-treatment covariates \(=(X_{1},X_{2},,X_{p})^{T}\) from \([-1,1]^{p}\) and the unobserved variable \(U\) from \((0,1)\). Then, we generated the potential outcomes by \(Y(d)=d()+_{j=1}^{p}X_{j}+1.5U+(d)\), where \(()=1+X_{1}+X_{1}^{2}+X_{2}+X_{2}^{2}\) and \((d)(0,1)\) for \(d=0,1\). Thus The treatment assignments for the trial sample of size \(n\) and the observational sample of size \(m\) were generated as follows: \(D|(,U,S=1)(0.5)\) and \(D|(,U,S=0)(1/(1+(- U-0.5X_{1})))\). Thus, the magnitude of \(\) controls the strength of the unmeasured confounding: a larger \(\) leads to a larger confounding bias. The test data \(X_{test,j}\) for \(1 j p\) were generated from \((-1,1)\) with sample size \(1000\).

In the first experiment, we aim to verify the effectiveness of the proposed data fusion strategy via an ablation study. We compared the robustness of the \(\) and the \(\) against different levels of confounding bias parameter \(\). Two baselines were considered: (i) the \(\) using only the trial data and (ii) the \(\) estimator obtained by directly merging all the available data and constructing a Fit-Based Causal Tree (Athey and Imbens, 2016). We set the sample sizes of the trial data and the observational data be \(n=200\) and \(m=2000\), respectively, the dimension of covariates \(p=5\) and \(\{0.1c|c,c 19\}\). The following three conclusions could be drawn from Figure 2: (1) When confounding bias in observational data was small, the simple fusion (SF) strategy can effectively improve the model performance. But when it became large, the \(\) was very vulnerable to confounding bias in observational data; (2) Even with the increase of \(\), both \(\) and \(\) consistently showed resistance to confounding bias; (3) \(\) was significantly better than other methods including \(\), which verified the effectiveness of our proposed split criterion (9) numerically.

In the second experiment, we evaluated the RMSEs with respect to different \(n\) and \(\). We set \(m=2000\) and \(p=5\). We included seven estimators in the analysis: The first two estimators were calculated purely based on the trial data: (i) the Transformed Outcome Honest Tree (\(\)) (Athey and Imbens, 2016) and (ii) the Generalized Random Forest (\(\)) (Athey et al., 2019). The rest estimators were calculated using different data fusion strategies: (iii) the Shrinkage Tree (\(\)) estimator,(iv) the Fused and Accurate Shrinkage Tree (\(\)) estimator, (v& vi) the \(\) estimators (Kallus et al., 2018) with a parametric (OLS) estimator and a non-parametric (Random Forest) specification of the confounding function, respectively and (vii) the bagged \(\) estimator (\(\)).

Table 1 reports the RMSEs of the seven estimators, conveying a good estimation accuracy of both the \(\) and its ensemble version \(\). Among the three individual estimators, the \(\) and \(\), exhibited superior performance compared to the \(\), and the \(\) outperformed the \(\). These relative performances provided support for the \(\) approach compared to the classical honest regression tree, the proposed split criterion (9), and the shrinkage estimation strategy (6), which are implemented progressively. Among the three ensemble estimators, the rfFAST estimator demonstrated the best performance among all the six combinations of the trial sample size \(n\) and the confounding bias parameter \(\). On the other hand, the performance of the \(\) estimators appeared to be less stable. The \(_{ols}\) outperformed the \(\) only when the trial sample size was relatively large (\(n=200\)). Under the non-parametric specification of the confounding function, the \(_{RF}\) did not gain benefit from incorporating the observational data and was consistently inferior to the baseline estimator \(\).

Figure 2: The averaged root mean square error (RMSE) (mean with s.e. error bars) of the estimators on simulation datasets with different levels of the confounding bias parameter \(\).

### Real-world data

In this sub-section, we report an analysis of the Tennessee Student/Teacher Achievement Ratio (STAR) Experiment (Krueger, 1999) to demonstrate the proposed \(\) for the HTE estimation. We aim at quantifying the treatment effect of the class size on the student's academic achievement.

**Data description** The STAR Experiment was a randomized controlled trial conducted in the late 1980s. Students were randomly assigned to one of the two types of classes during the first school year: \(D=1\) for small classes containing \(13-17\) pupils and \(D=0\) for regular classes containing \(22-25\) pupils. The outcome \(Y\) is the average of the listening, reading, and math standardized tests at the end of first grade. The vector of covariates \(X\) includes gender, race, birth month, birthday, birth year, free lunch given or not, and teacher id. This made a universal sample of \(4218\) students, among which \(2413\) were randomly assigned to regular-size classes (\(D=0\)) and \(1805\) to small classes (\(D=1\)).

**Ground-truth** In practice, the ground-truth \(()\) is not accessible, so we replaced it with an estimate calculated by a generalized random forest (Athey et al., 2019) based on all the \(4218\) observations.

**Construction of the trial, observational and test data** Following Kallus et al. (2018), we introduced confounding bias by splitting the population over a variable which is known to strongly affect the observed outcome \(Y\)(Krueger, 1999): rural or inner-city (\(U=1\), \(2811\) students) and urban or suburban (\(U=0\), \(1407\) students). The trial data were generated by randomly sampling a fraction \(h\) of the students with \(U=1\), where \(h\) ranges from \(0.1\) to \(0.5\). The observational data were constructed as follows: From students with \(U=1\), we took the controls (\(D=0\)) that were not sampled in trial data, and the treated (\(D=1\)) whose outcomes were in the lower half of outcomes among students with \(D=1\) and \(U=1\); From students with \(U=0\), we took all of the controls (\(D=0\)), and the treated (\(D=1\)) whose outcomes were in the lower half of outcomes among students with \(D=1\) and \(U=0\). The test data consisted of a held-out sub-sample of all the observations in the universal sample excluding the trial data.

**Results** We compared the performance of the \(\) with various baseline estimators. In particular, the \(\) and the \(\) estimators were constructed using the Random Forest regressor. The \(\) estimator utilized only trial data, while the \(\) estimator utilized both trial data and observational data together without distinction. As shown in Figure 3, the proposed \(\) method consistently outperformed other estimators.

## 6 Discussion

This paper explores the estimation of heterogeneous treatment effects (HTE) within the framework of causal data fusion. Drawing inspiration from the classical James-Stein shrinkage estimation (Green and Strawderman, 1991) approach, the authors introduce a new method called Fused and Accurate Shrinkage Tree (\(\)) that effectively incorporates observational data in both feature

   \(n\) & \(\) & HT & ST & FAST & \(\) & \(_{ols}\) & \(_{RF}\) & \(\) \\   &  & & 1.89 & 1.84 & & 1.33 & 1.73 & **0.84** \\  & & & (0.06) & (0.06) & & (0.04) & (0.03) & (0.02) \\  & & 2.28 & 1.90 & 1.85 & 1.19 & 1.29 & 1.65 & **0.89** \\  & & (0.06) & (0.05) & (0.05) & (0.02) & (0.04) & (0.03) & (0.02) \\  & & 2.0 & & 2.05 & 2.02 & & 1.28 & 1.71 & **0.98** \\  & & & (0.05) & (0.04) & & (0.04) & (0.03) & (0.02) \\   &  &  & 1.87 & 1.71 & & 0.96 & 1.56 & **0.73** \\  & & & (0.04) & (0.04) & & (0.02) & (0.02) & (0.01) \\   & & 2.20 & 1.98 & 1.83 & 1.12 & 0.97 & 1.59 & **0.84** \\   & & (0.04) & (0.04) & (0.04) & (0.01) & (0.03) & (0.02) & (0.02) \\   & & & 2.08 & 1.97 & & 1.01 & 1.57 & **0.92** \\   & & & (0.03) & (0.03) & & (0.02) & (0.03) & (0.02) \\   

Table 1: The averaged RMSE (standard error in parentheses) of the estimators with respect to the trial sample size \(n\) and the confounding bias parameter \(\). The best performance is marked in **bold**.

space segmentation and leaf node value estimation. This new approach is shown to outperform existing data fusion methods via numerical experiments.

The above estimation framework can be generalized to any data fusion problem if there exists an unbiased estimator and a biased estimator of some functions of interest. It would be worthwhile to explore the combination of the FAST method with other ensemble methods, such as the boosting and the grf-style (Athey et al., 2019) bagging, in addition to Breiman-style (Breiman, 2001) bagging used in rfFAST. Moreover, extending the framework to handle time-series observational data would be an interesting direction for future research. Additionally, investigating statistical inference under the proposed fusion framework would also be valuable.