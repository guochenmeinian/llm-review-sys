# Optimal Algorithms for the Inhomogeneous

Spiked Wigner model

Justin Ko

ENS de Lyon

justin.ko@ens-lyon.fr &Florent Krzakala

EPFL

florent.krzakala@epfl.ch &Aleksandr Pak

ENS de Lyon, EPFL

aleksandr.pak@epfl.ch

###### Abstract

We study a spiked Wigner problem with an inhomogeneous noise profile. Our aim in this problem is to recover the signal passed through an inhomogeneous low-rank matrix channel. While the information-theoretic performances are well-known, we focus on the algorithmic problem. First, we derive an approximate message-passing algorithm (AMP) for the inhomogeneous problem and show that its rigorous state evolution coincides with the fixed-point equations satisfied by the Bayes-optimal estimator. Second, we deduce a simple and efficient spectral method that outperforms PCA and is shown to match the information-theoretic transition.

Low-rank information extraction from a noisy data matrix is a crucial statistical challenge. The spiked random matrix models have recently gained extensive interest in the fields of statistics, probability, and machine learning, serving as a valuable platform for exploring this issue Donoho and Johnstone (1995), Peche (2014), Baik et al. (2005). A prominent example is the spiked Wigner model, where a rank one matrix is observed through a component-wise homogeneous noise.

Here we consider the inhomogeneous version of the model which has been recently introduced in a series of papers Barbier and Reeves (2020), Behne and Reeves (2022), Alberici et al. (2021, 2022) and naturally arises in different contexts such as community detection Behne and Reeves (2022), deep Boltzmann machines Alberici et al. (2021) etc. In particular, it corresponds to the dense limit of the degree-corrected stochastic block model Karrer and Newman (2011) (see the explicit mapping in Guionnet et al. (2022)). In this model the signal is observed through an inhomogeneous Gaussian noise with block-constant variance profile. Such block-constant noise naturally arises in settings such as community detection and stochastic-block models. The assumption for the noise to be Gaussian is in no way restrictive as a universality result in Guionnet et al. (2022) shows that an entrywise transformation of data generated from a general inhomogeneous inference problem reduces any probabilistic noise to the spiked Wigner problem with Gaussian data. This generality is what makes the model appealing.

We now define the model: Consider a partition \(\{1,,N\}=[N]\) into \(q\) disjoint groups \(C_{1}^{N} C_{q}^{N}=[N]\). This partition is encoded by a function \(g:[N][q]\) which maps each index \(i[N]\) into its group \(q(i)[q]\). Let \(}^{q q}\) be a symmetric matrix encoding a block-constant symmetric matrix \(^{N N}\)

\[_{ij}=}_{g(i)g(j)}.\] (1)

We assume both the noise profile \(}\) and the partition function \(g\) to be known. This is often the case in practical applications, such as the degree-corrected block model, Karrer and Newman (2011)Additionally, in practical applications where the noise profile is not known, one can often empirically estimate the noise profile and assign group membership according to the empirical variances. Such an approach is likely to yield good results but lies outside the scope of the present work.

We observe the signal \(^{}^{N}\) which is assumed to have independent identically distributed coordinates generated from some prior distribution \(_{0}\) (i.e. \((^{}=)=_{i=1}^{N}_{0}(x_{i}^{}=x _{i})\)) through noisy measurements:

\[=}^{}(^{})^{T}+}.\] (2)

Here and throughout the article \(\) denotes the Hadamard product, \(}\) is the Hadamard square-root of \(\) and \(\) is a real-valued symmetric GOE matrix with off-diagonal elements of unit variance. The Bayes-optimal performance of this model in the asymptotic limit \(N\) was studied rigorously in Guionnet et al. (2022); Behne and Reeves (2022); Chen and Xia (2022); Chen et al. (2021) who characterized the fundamental information-theoretic limit of reconstruction in this model.

Here we focus instead on the algorithmic problem of reconstructing the (hidden) spike. **Our contributions are many-fold**:

\(\) We show how one can construct an Approximate Message Passing (AMP) algorithm for the inhomogeneous Wigner problem, whose asymptotic performance can be tracked by a rigorous state evolution, generalizing the homogeneous version of the algorithm for low-rank factorization (Bayati and Montanari, 2011; Deshpande et al., 2015; Lesieur et al., 2017).

\(\) We derive a fixed-point equation for AMP and show that it coincides with the fixed-point equation for the Bayes-optimal estimator obtained in Guionnet et al. (2022)

\(\) Finally, we present a linear version of AMP (Maillard et al., 2022), that is equivalent to a spectral method. We conjecture it to be optimal in the sense that it can detect the presence of the spike in the same region as AMP. This is quite remarkable since, as shown in (Guionnet et al., 2022, Section 2.4), the standard spectral method (PCA) fails to do so.

Related workThe class of approximate message passing algorithms (AMP) has attracted a lot of attention in the high-dimensional statistics and machine learning community, see e.g. (Donoho et al., 2009; Bayati and Montanari, 2011; Rangan, 2011; Deshpande et al., 2015; Lesieur et al., 2017; Gerbelt and Berthier, 2021; Feng et al., 2022). The ideas behind this algorithm have roots in physics of spin glasses Mezard et al. (1987); Bolthausen (2014); Zdeborova and Krzakala (2016). AMP algorithms are optimal among first order methods (Celentano et al., 2020), thus their reconstruction threshold provides a bound on the algorithmic complexity in our model. Our approach to the inhomogeneous version of AMP relies on several refinements of AMP methods to handle the full complexity of the problem, notably the spatial coupling technique Krzakala et al. (2012); Donoho et al. (2013); Javanmard and Montanari (2013); Gerbelt and Berthier (2021); Rossetti and Reeves (2023).

Factorizing low-rank matrices is a ubiquitous problem with many applications in machine learning and statistics, ranging from sparse PCA to community detection and sub-matrix localization. Many variants of the homogeneous problem have been studied in the high-dimensional limit (Deshpande et al., 2015; Lesieur et al., 2017; Barbier et al., 2018; Lesieur et al., 2017; Alaoui et al., 2020; Lelarge and Miolane, 2019; Louart and Couillet, 2018; Barbier and Reeves, 2020). The inhomogeneous version was discussed in detail in Guionnet et al. (2022); Behne and Reeves (2022); Alberici et al. (2021). Spectral methods are a very popular tool to solve rank-factorization problems (Donoho and Johnstone, 1995; Peche, 2014; Baik et al., 2005). Using AMP as an inspiration for deriving

Figure 1: Performance of the inhomogeneous AMP algorithm against the information-theoretical optimal MMSE. The variance profile is proportional to \(}=1&3\\ 3&2\) with two equally sized blocks with standard Gaussian prior when \(N=500\) at various snr.

new spectral methods was discussed, for instance, in Saade et al. (2014); Lesieur et al. (2017); Aubin et al. (2019); Mondelli and Montanari (2018); Mondelli et al. (2022); Maillard et al. (2022); Venkataramanan et al. (2022).

## 1 Main results

Message passing algorithm --For each \(t 0\), let \((f_{t}^{a})_{a[q]}\) be a collection of Lipschitz functions from \(\), and define for \(t\)\(f_{t}:^{N}^{N}\) by

\[f_{t}():=(f_{t}^{g(i)}(x_{i}))_{i[N]}^{N}.\]

These functions are often called denoiser functions and can be chosen amongst several options, such as the Bayes optimal denoisers for practical applications (see Section 2), or even linear denoisers (see Section 3). We shall consider the following AMP recursion

\[^{t+1}=(})f_{t}( ^{t})-_{t} f_{t-1}(^{t-1})\] (3)

with the so-called Onsager term \(_{t}=}f_{t}^{}(x^{t})^{N}\) where \(}\) is the Hadamard inverse of \(\) and \(f_{t}^{}\) is the vector of coordinate wise derivatives.

In practical implementations, we initialize the algorithm with some non-null \(^{0}\) and let it run for a certain number of iterations. One efficient way to do this is the spectral initialization (Mondelli and Venkataramanan, 2021) with the method described in sec. 3. In Figure 1 we provide an example of the performance of the AMP together with the Bayes-optimal estimator predicted by the asymptotic theory. Even at very moderate sizes, the agreement between theory and simulation is clear.

State evolution --AMP was the basis of many works for the homogeneous case e.g. Deshpande et al. (2015); Lesieur et al. (2017). Our first contribution is the introduction of an AMP and of its rigorous state evolution (Javanmard and Montanari, 2013, Theorem 1) in the inhomogeneous setting. To state a well-defined limit of the AMP, we have the following assumptions.

**Assumption 1.1**.: _To ensure that our inhomogeneous AMP has a well-defined limit, we assume that_

1. _For each_ \(a[q]\)_, we have_ \[_{N}^{N}|}{N} c_{a}(0,1).\]
2. _For each_ \(t[N]\) _and_ \(a[q]\)_, both_ \((f_{t}^{a})\) _and_ \((f_{t}^{a})^{}\) _are Lipschitz._
3. _For each_ \(a[q]\)_, there exists_ \((_{a}^{0})^{2}\) _such that, in probability,_ \[_{N}^{N}|}_{i C_{a}^{N}}f_{0}^{a}(x_{i}^{0 })f_{0}^{a}(x_{i}^{0})=(_{a}^{0})^{2}.\]

Our first result describes the distribution of the iterates in the limit. Our mode of convergence will be with respect to \(L\)-pseudo-Lipschitz test functions \(:^{M}\) satisfying

\[|(x)-(y)| L(1+\|x\|+\|y\|)\|x-y\|x,y ^{M}.\] (4)

We define the following state evolution parameters \(_{b}^{t}\) and \(_{b}^{t}\) for \(b[q]\) through the recursion

\[&_{b}^{t+1}=_{a[q]}}{}_{ab}}_{x_{0}^{*},Z}[x_{0}^{*}f_{t}^{a}(_{a}^{t} x_{0}^{*}+_{a}^{t}Z)]x_{0}^{*}_{0},Z(0,1)\\ &(_{b}^{t+1})^{2}=_{a=1}^{q}}{}_{ab}}_{x_{0}^{*},Z}[(f_{t}^{a}(_{a}^{t}x_{0}^{*}+ _{a}^{t}Z))^{2}]x_{0}^{*}_{0},Z (0,1),\] (5)

where \(x_{0}^{*}\) and \(Z\) are independent. We prove that the iterates \(_{i}^{t}\) are asymptotically equal in distribution to \(_{g(i)}^{t}x_{0}^{*}+_{g(i)}^{t}Z\) where \(x_{0}^{*}_{0}\) and \(Z\) is an independent standard Gaussian.

**Theorem 1.2** (State evolution of AMP iterates in the inhomogeneous setting).: _Suppose that Assumption 1.1 holds, and that \(_{0}\) has bounded second moment. Let \(:^{2}\) be a 2-pseudo-Lipschitz test functions satisfying (4). For any \(a[q]\), the following limit holds almost surely_

\[_{N}^{N}|}_{i C_{a}^{N}}(x_{i}^ {t},x_{i}^{})=_{x_{0}^{},Z}(_{a}^{t}x_{0}^{}+ _{a}^{t}Z,x_{0}^{})\]

_where \(Z\) is a standard Gaussian independent from all other variables._

**Remark 1.3**.: _The notion of convergence under the \(L\)-pseudo-Lipschitz test functions induces a topology that is equivalent to the one generated by the \(L\)-Wasserstein topology [10, Remark 7.18]. We can strengthen the second moment assumption on \(_{0}\) to finite \(k\)th moment, but the induced topology will then change to the \(k\)-Wasserstein topology, see [1, Theorem 1]._

Even though the theoretical result above applies in the high-dimensional limit, numerical simulations show that even for medium-sized \(N\) (around \(500\)), the behaviour of the iterates is well described by the state evolution parameters. Through the state evolution equations (5) we are able to track the iterates of the AMP iteration with just two vectors of parameters obeying the state evolution recursion: the overlap with the true signal \((_{a}^{t})_{a[q]}\) and its variance \((_{a}^{t})_{a[q]}\). We next obtain the following necessary and sufficient condition for the overlaps of a fixed point of the iteration (2):

**Theorem 1.4** (Bayes-Optimal fixed point).: _Assume AMP satisfies Assumption 1.1 and let the denoising functions be the Bayes ones (32). Then the overlaps \(=(_{a})_{a[q]}\) in (5) satisfy the following fixed point equation_

\[_{b}=_{a[q]}}{}_{ab}}_{x_{0}^{ },Z}[x_{0}^{}_{posterior}[x_{0}^{}|_{a}x_{0}^{ }+}Z]].\] (6)

**Remark 1.5**.: _The state evolution fixed point equation above coincides with the fixed point equation satisfied by the Bayes optimal estimator in [1, Equation 2.14]._

A spectral method --Given the matrix \(\) defined in (2) we consider the transformed matrix

\[}:=_{x_{0}^{}}[(x_{0}^{})^{2}]}{}-_{x_{0}^{}}[(x_{0}^{})^{2}]^{2} (}1\\ \\ 1).\] (7)

Let \(=(c_{a})_{a[q]}\). We define the inhomogeneous signal-to-noise (SNR) ratio of such a model by

\[():=()=_{x_{0}^{ }}[(x_{0}^{})^{2}]^{2}\|(}) }}(})\|_{op}.\] (8)

**Conjecture 1.6**.: _The top eigenvalue of \(}\) separates from the bulk if and only if the signal to noise ratio \(()>1\). In particular, if \(}\) is the top eigenvector of \(}\), then if and only if \(()<1\) we have:_

\[_{N}}^{}\| }{\|}\|\|^{}\|}=0\,.\]

The conjecture is based on heuristic arguments identifying the fixed point of AMP with a spectral method. This matches precisely the recovery transition in [1, Lemma 2.15 Part (b)]. In this paper, we rigorously show that with \(()<1\) our proposed spectral method fails to recover the signal. We illustrate the eigenvalue BBP-like transition in Fig.2.

## 2 The inhomogeneous AMP algorithm

In this section, we derive the formula for the inhomogeneous AMP iteration (25). We first recall the general matrix framework of AMP from :Matrix AMP --In the matrix setting an AMP algorithm operates on the vector space \(_{q,N}(^{q})^{N}^{N q}\). Each element of \(=(v_{1},,v_{N})_{q,N}\) will be regarded as \(N\)-vector with entries \(v_{i}^{q}\).

**Definition 2.1** (Amp).: _A matrix AMP acting on this space is represented by \((,,^{0})\), where:_

1. \(=+^{}\)_, where_ \(^{N N}\) _has iid entries_ \(G_{ij} N(0,)\)_._
2. \(\) _is a family of_ \(N\) _Lipschitz functions_ \(f_{t}^{i}:^{q}^{q}\)_,_ \(i[N]\) _indexed by time_ \(t\)_. The family_ \(\) _encodes a function_ \(f_{t}:_{q,N}_{q,N}\) _that acts separately on each coordinate_ \(v_{j}^{q}\)_,_ \[f_{t}()=(f_{t}^{1}(v_{1}),,f_{t}^{N}(v_{N})_{q,N}.\] (9)
3. \(^{0}_{q,N}\) _is a starting condition._

The algorithm itself is a sequence of iterates generated by:

\[^{t+1}=}{}f_{t}(^{t})-f_{t-1}( ^{t-1})_{t}^{T}\] (10)

where \(_{t}\) is the \(q q\) Onsager matrix given by

\[_{t}=_{j=1}^{N} f_{t}^{j}(_{j}^{t}).\] (11)

where \( f_{t}^{j}\) denotes the Jacobian matrix of \(f_{t}^{j}\). The limiting properties of the AMP sequences are well known and can be found in (Javanmard and Montanari, 2013, Theorem 1).

The inhomogeneous AMP -- We now define an inhomogeneous AMP iteration which takes into account the block-constant structure of the noise:

**Definition 2.2**.: _An inhomogeneous AMP on \(_{1,N}=^{N}\) is represented by \((,,^{0},)\), where the terms \(\), \(\), \(^{0}\) are defined in Definition 2.1 and \(\) is the \(N N\) variance profile encoded by \(}^{q q}\) and grouping \(g:[N][q]\) defined by (1). We further assume that the family of functions \(\) is encoded by functions \(f_{t}^{a}:\) for \(a[q]\) which define the group dependent function_

\[f_{t}()=(f_{t}^{g(1)}(x_{1}),,f_{t}^{g(N)}(x_{N}))^{ N}.\] (12)

The sequence of iterates \(^{t}^{N}\) of the \((,,^{0},)\) are defined as follows:

\[^{t+1}=(}})f_{t }(^{t})-_{t} f_{t-1}(^{t-1}),\] (13)

Figure 2: Illustration of the spectrum of \(}^{10^{3} 10^{3}}\) evaluated at noise profiles with snr \(()=0.7\) (left, before the transition) and \(1.8\) (right, after the transition), with the outlying eigenvector correlated with the spike arises at eigenvalue one.

where \(}\) is the Hadamard inverse square root of the noise, and the Onsager term \(_{t}\) has the following form

\[_{t}=}(f_{t}^{g(1)})^{ }(x_{1}^{t})&+&&+&}(f_{t}^{g(N)})^{}(x_{ N}^{t})\\ &&&&\\ }(f_{t}^{g(1)})^{}(x_{1}^{t})&+&&+&}(f_{t}^{g(N)})^{}(x_{N}^{t})=}f_{t}^{{}^{}}(x^{t})^{N}.\] (14)

State evolution of the inhomogeneous AMP --Through a continuous embedding, we will reduce our inhomogeneous AMP to the matrix AMP framework, and recover the state evolution of the inhomogeneous AMP. We define the diagonal matrix operator \(:^{N}_{q,N}\) which outputs a block diagonal matrix according to the block structure of our discretization of \([N]\):

\[()= M_{ ij}=v_{j}&g(j)=i\\ 0&.\]

Likewise, we define the projection operator \(:_{q,N}^{N}\) which extracts a vector of size \(N\) from a \(N q\) according to the block structure of \([N]\) by

\[()=(M_{ig(i)})_{i N}^{N}.\]

Under these changes of variables, we define

\[^{t}=(^{t})_{q,N} t 0\]

and \(_{t}(^{q})^{N}(^{q})^{N}\) by

\[(_{t}(^{t}))_{ij}=_{g(i)}j}}f_{t}^{g(i)}(x_{i}^{t})i,j[N][q].\] (15)

We encode the family of functions \(_{t}\) by \(}()\).

**Lemma 2.3**.: _Let \(^{t}\) be iterates from the AMP \((,,^{0},)\). Then the iterates \(^{t}:=(^{t})\) follow the generalized matrix AMP \((,}(),^{0})\)._

Proof.: We will show that the projection of the iterates \(^{t}\) from \((,}(),^{0})\) are the iterates from \((,,^{0},)\). It is easy to check that

\[}{}f_{t}(^{t})= (}{}_{t}( ^{t})).\] (16)

Next, notice that Jacobian is given by

\[_{t}^{i}(_{i}^{t})=[0 &&_{g(i)}}}(f_{t}^{g(i)})^{}(x_{i}^{ t})&&0\\ &&&&\\ 0&&_{g(i)}}}(f_{t}^{g(i)})^{}(x_{i}^{ t})&&0],\]

which is a matrix where only the column number \(g(i)\) has non-zero elements. Applying (11), we thus get for \(a,b[q][q]\)

\[(_{t})_{ab}=_{ab }}}_{i:g(i)=a}(f_{t}^{a})^{}(x_{i}^{t}).\] (17)

It follows that

\[_{t} f_{t-1}(^{t-1})=( _{t-1}(^{t-1})_{t}^{T}).\]As a consequence, the inhomogeneous state evolution equations in Theorem 1.2 follow immediately from the state evolution equations of \((,}(),^{0})\) discussed in (Javanmard and Montanari, 2013, Section 2.1). This follows from the observation that given the law of \(^{t}\) in the high dimensional limit, the law of \(^{t}=(^{t})\) is straightforward to compute. We define

\[(_{b}^{t+1})^{2}:=_{a=1}^{q}}{}_{ab}} [(f_{t}^{b}(Z_{b}^{t}))^{2}].\] (18)

We will show that the distribution of the iterate \(_{i}^{t}\) is asymptotically normal with mean \(0\) and variance \((_{g(i)}^{t})^{2}\).

**Lemma 2.4** (Behavior of the inhomogeneous AMP iterates).: _Suppose that Assumption 1.1 holds, and that \(_{0}\) has a bounded second moment. Let \(:^{2}\) be a \(L\)-pseudo-Lipschitz test function satisfying (4). For any \(a[q]\), then the following limit holds almost surely_

\[_{N}^{N}|}_{i C_{a}^{N}}(x_{i}^{t},x_{i }^{})=_{x_{0}^{},Z}(_{a}^{t}Z,x_{0}^{})\]

_where \(Z\) is an independent standard Gaussian._

Proof.: In matrix-AMP (Javanmard and Montanari, 2013, Th. 1), the marginals of the iterates \(^{t}\) from \((,}(),^{0})\) are approximately Gaussian and encoded by the positive definite matrices

\[}_{a}^{t}=[_{t}^{i} (^{t})_{t}^{i}(^{t})^{}] i C_{a}^{N},\] (19) \[\ ^{t} N(0,^{t})\ \ ^{t+1}= _{a=1}^{q}_{a}_{a}^{t}.\] (20)

We now show that \(a[q]\) and \(i C_{a}^{N}\), \(}_{a}^{t}\) depends only on \((_{a}^{t})^{2}=_{aa}^{t}\). Indeed, by the definition of \(_{t}^{i}\) we have

\[}_{a}^{t}(k,l)=[(_{t}^{i}( ^{t}))_{k}(_{t}^{i}(^{t}))_{l}]= [}_{ak}}}}_{al}}}(f_{t}^{a}(Z_{a}^{t}))^{2}],\] (21)

where \(Z_{a}^{t} N(0,(_{a}^{t})^{2})\) is the \(a\)th component of the Gaussian vector \(_{t}\). The key observation here is that by construction our function \(_{t}^{i},^{q}^{q}\) depends only on the \(i\)th component \(Z_{i}^{t}\) of the Gaussian vector \(^{t}\). To characterize the limiting distribution of \(^{t}=(^{t})\), we only need to keep track of the variances \((_{j}^{t})^{2},j[N]\). Using (21), for a given \(a[q]\) and \(i C_{a}^{N}\) we write

\[}_{a}^{t}(g(j),g(j))=[(_{t}^{i} (^{t}))_{g(j)}(_{t}^{i}(^{t}))_{g(j )}]=}_{ag(j)}}[(f_{t}^{i}(Z_{ i}^{t}))^{2}].\] (22)

Finally, with (20) we get that for any \(b[q]\) and any \(j C_{b}^{N}\), using \(Z_{a}^{t} N(0,(_{a}^{t})^{2})\),

\[(_{j}^{t+1})^{2}=(_{b}^{t+1})^{2}=_{bb}^{t+1}=_{a= 1}^{q}c_{a}}_{a}^{t}(g(j),g(j))=_{a=1}^{q} }{}_{ag(j)}}[(f_{t}^{a}(Z_{a}^{t}))^{2}]\] (23)

The inhomogeneous spiked Wigner model in the light of the AMP approach --We now generalize the state evolution equations from Lemma 2.4 to spiked matrices with an inhomogenous noise profile as was stated in Theorem 1.2. This reduction via a change of variables is standard, see for example (Deshpande et al., 2015, Lemma 4.4). Remember that in the inhomogeneous version of the spiked Wigner model we observe the signal \(^{}\) through an inhomogeneous channel:

\[=}^{}(^{})^{T}+}.\] (24)Our AMP algorithm is defined with the following recursion:

\[^{t+1}=(})\!f_{t}( ^{t})-_{t} f_{t-1}(^{t-1})\] (25)

where \(_{t}=}f_{t}^{{}^{}}(^{t})\) and \(f\) is encoded by the family of functions in Definition 2.2. The main difference in contrast to the iteration (13) is that our data matrix \(\) is no longer a centered matrix, while \(}}\) is. We would like to reduce (25) to an iteration of the form (13) with respect to a different parameter \(^{t}\) which is uniquely determined by \(^{t}\)

\[^{t+1}=(}})\!f_ {t}(^{t})-_{t} f_{t-1}(^{t-1}).\]

Doing so will allow us to recover the limiting laws of the iterates from Lemma 2.4. This is done via a standard change of variables to recenter \(\). We will sketch the argument in this section, and defer the full proof of Theorem 1.2 to the Appendix A in the Supplementary Material.

To simplify notation,let us denote \(f_{t}(^{t}):=}^{t}\). We proceed following the approach of [4, Lemma 4.4]. We rewrite (25) using the definition of Y to get

\[^{t+1} =(})\!}^{t}-_{t}}^{t-1}\] (26) \[=(}^{}^{ }^{T})\!}^{t}+(}})\!}^{t}-_{t}} ^{t-1}.\]

If indices are independent, then by the strong law of large numbers one would expect that

\[((}^{}^{} ^{T})\!}^{t})_{j}=x_{j}^{}_{a[q]} _{i C_{a}^{N}}^{}_{i}^{t}}{_{ji}} x_{j}^{}_{a[q]}}{_{ji_{a}}} [x_{0}^{}_{i_{a}}^{t}],\] (27)

where \(i_{a}\) is some index belonging to the group \(C_{a}^{N}\) and \(x_{0}^{}\) is a random variable distributed according to the prior distribution \(_{0}\). For \(b[q]\) and \(i C_{b}^{N}\), we define the block overlap \(_{b}^{t}\) using the recursion

\[_{i}^{t+1}=_{b}^{t+1}=_{a[q]}}{}_{ab}} _{x_{0}^{},Z}[x_{0}^{}f_{t}^{a}(_{a}^{}x_{0} ^{}+_{a}^{t}Z)],\] (28)

where \(Z\) is a standard Gaussian random variable independent from all others sources of randomness. Notice that (28) is precisely the asymptotic behavior of the summation appearing in (27).

We now make a change of variables and track the iterates

\[^{0}=^{0}-^{0}^{}^{t}=^ {t}-^{t}^{}, t 1\] (29)

where \(^{0}\) is the vector of block overlaps of the initial condition \(^{0}\) with the truth. We reduced the (25) iteration to the following iteration in which we easily recognize a version of (13):

\[^{t+1}=(}})f_{t} (^{t}+^{t}^{})-_{t} f _{t-1}(^{t-1}+^{t-1}^{})\] (30)

with the initial condition \(^{0}=^{0}-^{0}^{}\) and the Onsager term taken from (14) is given by

\[_{t}=}f_{t}^{{}^{}}(^{t}+^{t }^{}).\] (31)

Using Lemma 2.4, we can recover the asymptotic behavior of the iterates \(^{t}\) given in (25) by computing the iterates \(^{t}+_{t}^{}\) where \(^{t}\) follows (30) and \(_{t}\) satisfies (28). From this reduction we obtain the following state evolution equations describing the behaviour of (25):

1. \(x_{j}^{t}_{g(j)}^{t}x_{0}^{}+_{g(j)}^{t}Z\) for \(j[N]\), where \(Z(0,1)\)
2. \(_{b}^{t+1}=_{a[q]}}{_{ab}}_{x_{0}^{ },Z}[x_{0}^{}f_{t}^{a}(_{a}^{t}x_{0}^{}+_{a}^{t}Z )]\) with \(x_{0}^{}_{0},Z(0,1)\)
3. \((_{b}^{t+1})^{2}=_{a=1}^{q}}{_{ab}}_ {x_{0}^{},Z}[(f_{t}^{a}(_{a}^{t}x_{0}^{}+_{a}^{t}Z ))^{2}]\) with \(x_{0}^{}_{0}\), \(Z(0,1)\).

This (informally) characterizes the limiting distribution of the state evolution of the iterates from the inhomogeneous AMP stated in Theorem 1.2.

Fixed-point equation of state evolution in the Bayes-optimal setting --Suppose that we know the prior distribution \(_{0}\) of \(x_{0}^{}\). The Bayes-optimal choice for the denoising functions \(f_{t}^{j},j[N]\) is simply the expectation of \(x_{0}^{}\) with respect to the posterior distribution,

\[f_{t}^{j}(r)=f_{t}^{g(j)}(r)=_{posterior}[x_{0}^{}|_{g(j)}^{ t}x_{0}^{}+_{g(j)}^{t}Z=r].\] (32)

Under this Bayes-optimal setting, we can simplify the equations obtained in the previous section and see that AMP estimator is indeed an optimal one by studying its fixed point.

Proof of Theorem 1.4.: For this choice of \(f_{t}^{j}\) the Nishimori identity (see for example (Lelarge and Miolane, 2019, Proposition 16)) states that for \(a[q]\) and \(j C_{a}^{N}\),

\[_{a}^{t}:=_{x_{0}^{},Z}[x_{0}^{}f_{t}^{a}(_ {a}^{t}x_{0}^{}+_{a}^{t}Z)]=[(f_{t}^{a}(_{a}^{t} _{0}^{}+_{a}^{t}Z))^{2}].\] (33)

In this setting, the state evolution equations from Theorem 1.2 reduce to

\[_{a}^{t}=_{x_{0}^{},Z}[x_{0}^{}f_ {t}^{a}(_{a}^{t}x_{0}^{}+_{a}^{t}Z)]\\ _{b}^{t+1}=_{a[q]}}{_{ab}}_{a}^{t},b [q]\\ (_{b}^{t+1})^{2}=_{a[q]}}{_{ab}}_{a}^ {t},b[q].\] (34)

Remarkably with the Bayes-optimal choice of the denoising functions we have that for \(t 1\) for each block \(b[q]\), \(_{b}^{t+1}=(_{b}^{t+1})^{2}\). Therefore a necessary and sufficient condition for an estimator to be a fixed point of the state evolution is to simply have its overlaps \(_{b}^{t}\) unchanged by an iteration of the state evolution. This translates into the following equation for the overlaps \(_{b},b[q]\)

\[_{b}=_{a[q]}}{_{ab}}_{x_{0}^{ },Z}[x_{0}^{}_{posterior}[x_{0}^{}|_{a}x_{0}^{ }+}Z]].\] (35)

The result of Theorem 1.4 now follows immediately. 

## 3 A spectral method adapted to the inhomogeneous spiked Wigner model

From AMP to a spectral method --Remarkably, AMP and the state evolution machinery associated with it can help us design a simple spectral algorithm that matches the information-theoretic phase transition (Guionnet et al., 2022, Remark 2.16). Recall that Theorem 1.2 does not require the denoising functions \(f_{t}\) to be Bayes-optimal, but can be applied to any Lipschitz family of functions. In this section, we analyze the state evolution for the family of identity functions, \(f_{t}(x)=x\). We can gain some intuition motivating such a choice by considering simple priors, such as the Rademacher prior. In the case of the Rademacher prior a simple computation shows that the Bayes-optimal choice of the denoising functions yields \(f_{t}^{j}()=tanh()\). In the first order approximation we have \(tanh(x) x\). Thus, at least for the Rademacher prior, the choice of identity functions as denoising functions corresponds to the first order approximation of the Bayes-optimal choice. By Remark 3.1, we can assume that the entries of \(^{}\) have unit variance. With this choice of denoising functions the AMP iteration will simply become:

\[^{t+1}=(})^{t}- _{t}^{t-1}_{t}=}f_{t}^{ }=}1\\ \\ 1.\] (36)

If we denote \(_{t}=(_{t})\), it is easy to see that the fixed point of this iteration yields

\[=(})-_{t} \] (37)

so any \(\) fixed by the AMP iteration (36) must be an eigenvector of the matrix

\[}=(})-_{t }=(})-( 1\\  N&1\\ \\ 1).\] (38)

A simple spectral method consists in taking the principal eigenvector (associated to the largest eigenvalue) of the matrix \(}=}{}-_{t}\).

Analysis of the spectral method using state evolution --The spectral algorithm described above behaves like the AMP iteration with identity denoising functions around its fixed point. Therefore we can analyze this spectral algorithm using state evolution machinery for the AMP iteration. In the case of identity functions we have \(f_{t}^{a}(x)=x\) for all \(a[q]\), so

\[_{x_{0}^{*},Z}[x_{0}^{*}f_{t}^{a}(_{a}^{t}x_{0}^{*}+ _{a}^{t}Z)]=_{x_{0}^{*},Z}[x_{0}^{*}(_{a}^{t}x_{0}^{*}+ _{a}^{t}Z)]=_{a}^{t}\] (39) \[_{x_{0}^{*},Z}[(f_{t}^{a}(_{a}^{t}x_{0}^{*}+_{ a}^{t}Z))^{2}]=_{x_{0}^{*},Z}[(_{a}^{t}x_{0}^{*}+_{a}^{t}Z)^{2} ]=\ (_{a}^{t})^{2}+(_{a}^{t})^{2}\] (40)

which transforms state evolution equations (5) into the following simple form:

\[_{b}^{t+1}=_{a[q]}}{_{ba}}_{a}^{t}(_{b}^{t+1})^{2}=_{a[q]}}{_{ba}}((_{a}^{t})^{2 }+(_{a}^{t})^{2}).\] (41)

Rewriting the overlap state evolution in a matrix form we get for \(=(c_{a})_{a[q]}\) that

\[()^{t+1}=()}\,()(()^{t} ).\] (42)

The special form of state evolution above is rather informative and we can interpret it as follows. First of all, in the regime where \(()=\|()}\, ()\|_{op}<1,\) we can see from (42) that any iteration of the AMP recursion (36) contracts the vector overlap (multiplying it by a matrix with the operator norm smaller than 1). Thus the only possible overlap of a fixed point of (36) is 0. Moreover, we have defined the matrix \(}\) in a way that any eigenvector of this matrix is a fixed point of the AMP recursion (36). Thus any eigenvector of \(}\) must have zero expected overlap with the signal, meaning that the spectral method is uninformative in the regime \(()<1\).

Second of all, even though we cannot say anything rigorous in the regime \(()>1\), looking at state evolution equations of the AMP recursion (36), we see instability (overlap blows up but so does the variance). We conjecture that the principal eigenvector of \(}\) correlates with the signal exactly in the regime \(()>1\) which would imply that the BBP transition happens precisely at \(()=1\).

**Remark 3.1**.: _In general, we can let \(=_{x_{0}^{*}}[(x_{0}^{*})^{2}]\) and consider the normalized matrix_

\[}=}{}=}^{*}( ^{*})^{T}}{}+}}{}= {N}}}^{*}(}^{*})^{T}+}}\]

_for \(}=}{}\) and \(}=}{^{2}}\). Notice that the entries of \(}\) now have unit variance. Under this setting, the transition of the transformation in (38) applied to \(\), which appears in (7), has transition at_

\[(})=\|()}}\,()\|_{op}=_{x_{0}^{*}}[(x_{0}^{ *})^{2}]^{2}\|()}\, ()\|_{op}<1\]

_which is the generalized SNR defined in (8)._