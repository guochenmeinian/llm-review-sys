# H\({}_{2}\)O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models

Zhenyu Zhang\({}^{1}\), Ying Sheng\({}^{2}\), Tianyi Zhou\({}^{3}\), Tianlong Chen\({}^{1}\), Lianmin Zheng\({}^{4}\), Rusi Cai\({}^{1}\),

Zhao Song\({}^{5}\), Yuandong Tian\({}^{6}\), Christopher Re\({}^{2}\), Clark Barrett\({}^{2}\), Zhangyang Wang\({}^{1}\), **Bedid Chen\({}^{6,7}\)**

\({}^{1}\)University of Texas at Austin, \({}^{2}\)Stanford University, \({}^{3}\)University of California, San Diego,

\({}^{4}\)University of California, Berkeley, \({}^{5}\)Adobe Research, \({}^{6}\)Meta AI (FAIR), \({}^{7}\)Carnegie Mellon University

{zhenyu.zhang,tianlong.chen,ruisi.cai,atlaswang}@utexas.edu, ying1123@stanford.edu, {chrismre,barrett}@cs.stanford.edu, t8zhou@ucsd.edu, lianminzheng@gmail.com, zsong@adobe.com, yuandong@meta.com, beidic@andrew.cmu.edu

###### Abstract

Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens _Heavy Hitters_ (H\({}_{2}\)). Through a comprehensive investigation, we find that (\(i\)) the emergence of H\({}_{2}\) is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (\(ii\)) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H\({}_{2}\)O), a KV cache eviction policy that dynamically retains a balance of recent and H\({}_{2}\) tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H\({}_{2}\)O with \(20\%\) heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to \(29\), \(29\), and \(3\) on OPT-6.7B and OPT-30B. With the same batch size, H\({}_{2}\)O can reduce the latency by up to \(1.9\). The code is available at https://github.com/FMInference/H2O.

## 1 Introduction

Large Language Models (LLMs) have demonstrated remarkable proficiency in a wide range of natural language processing applications such as content creation, summarization, and dialogue systems . However, their deployment is very costly. In addition to the widely-studied bottlenecks of model size and the quadratic cost of attention layers, the problem of the size of the KV cache, which stores the intermediate attention key and values during generation to avoid re-computation, is becoming increasingly prominent . For instance, a 30 billion-parameter model with an input batch size of 128 and a sequence length of 1024 results in 180GB of KV cache. A natural approach is to limit its maximum size as is done in classical software or hardware caches . However, it is challenging to reduce KV cache memory footprints in LLMs without accuracy drops.

While there exists substantial literature on sparse attention approximation in training, they have not seen wide adoption for alleviating KV cache bottleneck. First, most existing methods, e.g., Reformer  and Flash Attention , are designed to overcome the quadratic memory required by attention mechanisms when modeling long sequences but still require a _large cache size_. Second, variants like sparse transformer , low-rank based transformers [10; 11] or multi-query attention [12; 13; 5] can reduce the cache size, but directly applying them on pre-trained LLMs for generation results in _high miss rates_ and degrades the accuracy as shown in Figure 1. Finally, some recent advances such as gisting tokens  can learn to compress the KV cache for documents, but their _expensive eviction policies_ are difficult to deploy during generation.

Therefore, an ideal KV cache should have (i) a _small cache size_ to reduce memory footprint, (ii) a _low miss rate_ to maintain the performance and long-content generation ability of LLMs, and (iii) a _low-cost eviction policy_ to reduce the wall-clock time during generation. However, there are three technical challenges. First, it is not immediately clear whether the size of the KV cache can be restricted--each decoding step might, in principle, require access to all previous attention keys and values. Second, identifying an optimal eviction policy that maintains generation accuracy is a combinatorial problem1. Finally, even if an optimal policy can be brute-forced, it is infeasible for deployment on real-world applications.

Fortunately, our preliminary exploration has yielded intriguing observations about the empirical properties of LLMs. These findings pave the way for the potential design of an efficient KV cache.

_Sparsity for small cache size_: We observe that even when trained densely, the attention matrices of LLMs are over 95% sparse at inference time (shown in Figure 2). This holds for a wide range of pre-trained LLMs. Therefore, only 5% of the KV cache is sufficient for decoding the same output token at each generation step, which suggests it may be possible to have up to a 20\(\) reduction in KV cache size without an accuracy drop.

_Heavy-Hitters for low miss rate_: We discover that the accumulated attention scores of all tokens in attention blocks adhere to a power-law distribution. It suggests that there exists a small set of influential tokens that are critical during generation, named heavy-hitters (H\({}_{2}\)). H\({}_{2}\) provides an opportunity to step away from the combinatorial search problem and identify an eviction policy that maintains accuracy.

_Greedy algorithm for low-cost policy_: we surprisingly find that retaining the H\({}_{2}\) based on local statistics at each decoding step--summing the attention scores of only the preceding tokens--is as effective as considering the attention of future tokens (shown in Figure 2).

Based on the above, we first rigorously define the generative process of LLMs operating with a size-constrained KV cache in Section 2.1. Then we propose Heavy-Hitter Oracle (H\({}_{2}\)O), a framework

Figure 1: Upper plots illustrate symbolic plots of an attention map deploying different KV cache policies in LLM generation. Lower right: contrasts their accuracy-memory trade-off. Left: the overview of H\({}_{2}\)O framework.

that exploits the properties of LLMs and uses simple, low-cost eviction policies that retrain the quality of LLMs throughout the generation process. Specifically,

* In Section 3, we explore the emergence of \(_{2}\) in attention, revealing their fundamental and critical roles: \((i)\)\(_{2}\) exhibit a strong correlation of frequently co-occurring words in textual data; and \((ii)\) removing \(_{2}\) completely damages the model's functionality. We demonstrate that \(_{2}\) can largely lower the cache miss rate of the existing policies mentioned above. Theoretically, assuming the attention scheme is submodular, \(_{2}\) corresponds to a greedy algorithm and is therefore near-optimal.
* In Section 4, we present a greedy but low-cost variant of \(_{2}\) which is dynamically determined by the accumulated attention score at each decoding step. We formulate the eviction policy with greedy \(_{2}\) as a variant of dynamic submodular maximization. The analysis shows that it results in a similar generative process as the one using the \(_{2}\) eviction policy.

We perform extensive experiments on OPT, LLaMA, and GPT-NeoX on a single NVIDIA A\(100\) (80GB) GPU to evaluate \(_{2}\) across a range of tasks from lm-eval-harness  and HELM . We implement \(_{2}\) on top of FlexGen that can easily adapt different cache eviction techniques to produce a system with high-throughput inference. Performance experiments show our framework achieves \(29\), \(29\), \(3\) higher throughputs compared to three leading inference systems, DeepSpeed Zero-Inference , Hugging Face Accelerate , and FlexGen  respectively. With the same batch size, \(_{2}\) achieves up to \(1.9\) lower latency compare to FlexGen.

## 2 Related Work and Problem Setting

Efficient Inference of LLMs.The substantial parameter counts of large language models (LLMs) present significant challenges for inference. To overcome this limitation, previous efforts have employed model compression techniques with specific designs to achieve efficient LLM inference, such as the method described in [20; 21; 22], which employs one-shot pruning on LLMs, resulting in negligible performance degradation even without retraining. Additionally, alternative approaches explore quantization methods specifically tailored to LLMs, as discussed in [23; 24; 25; 26; 27; 28]. Also, CoLT5  employs a token-wise conditional computation strategy to reduce the overall computation cost. These methods address efficient inference from orthogonal perspectives and can be organically integrated. The techniques investigated in this study are closely associated with pruning or sparsity but focus on a distinct inference bottleneck, namely, KV cache. One closely related work utilizes a learnable mechanism that determines necessary tokens during inference but requires an extra fine-tuning process, which makes it less practical.

Sparse, Low-rank Attention Approx.The quadratic computational complexity of attention modules is one of the major bottlenecks of transformer inference . Various efforts are devoted to addressing this challenge [7; 9; 10]. For example, Reformer  reduces the computational cost from quadratic to superlinear complexity via locality-sensitive hashing. Performer  employs positive orthogonal random features to approximate attention kernels. One relevant work, Sparse Transformer , introduces sparsity to reduce KV cache memory footprint and achieve an efficient attention mechanism, considered as our baseline in this paper. Moreover, SpAtten  utilizes accumulated attention scores to select important tokens for efficient attention inference while they don't consider the variance of token importance across attention heads and layers. Comparison with SpAtten is detailed in Appendix C.9.

Caching.Caching, which plays a pivotal role in optimizing system performance, entails the development of effective eviction policies to handle frequently accessed data. Conventional approaches such as Least Recently Used and Least Frequently Used [33; 34] prioritize the recency and frequency of data access. And the design of KV cache encounters many similar challenges as traditional caching.

LLM Inference Breakdown.The generative procedure of LLMs encompasses two distinct phases: (i) the _prompt_ phase, in which an input sequence is utilized to produce the KV cache (consisting of the key and value embeddings), similar to the forward pass employed during LLM training; and (ii) the _token generation_ phase, which leverages and updates the KV cache to generate new tokens incrementally. Each generation step relies on the previously generated tokens. The primary focus of this paper is to enhance the efficiency of the KV cache in attention during the token generation phase, thereby accelerating LLM inference.

### Problem Formulation

We formally define the generative process with limited KV cache size. Denote attention query matrix as \(Q^{n d}\) and key matrix as \(K^{n d}\). \(Q_{i,*}\) represents the \(i\)-th row of \(Q\) and \(K_{ i,*}\) represents the first \(i\) rows of \(K\). Let \(k\) denote the budget of space and \(k<n\). For simplicity, \(K_{S_{i,*}}\) (\(^{i d}\)) denotes a sub-matrix of \(K\) which selects \(S_{i}\) rows from \(K\). (For the non-selected rows \([i] S_{i}\), we put all zeros in that row) Eviction policy is defined as:

**Definition 2.1** (Eviction Policy, informal).: _Let \(S_{i-1}\) denote the source set. Let \(S_{i}\) denote the target set. We defined the eviction policy \(g:S_{i-1} S_{i}\) such that_

* \(|S_{i}|=k\) _(_K_V cache _size is not changing over the time)_
* \(|S_{i} S_{i-1}| 1\) _or equivalently_ \(|S_{i} S_{i-1}| k-1\) _(we can evict at most_ \(1\)__K_V_ _in the_ KV cache)_

Then, we define the generative process with our eviction policy.

**Definition 2.2** (The generative process with eviction policy, informal).: _Let \(k\) denote the size of the KV cache. For each \(i[n]\), for the \(i\)-th token, we have_

* _Let_ \(S_{i}[n]\) _denote the tokens in_ KV cache _when predicting the_ \(i\)_-th token._
* _The information we have is a length-_\(i\) _vector_ \(o_{i}:=D_{i}^{-1}(Q_{i,*}(K_{S_{i},*})^{})\) _(normalized attention)_
* _scalar_ \(D_{i}:=((Q_{i,*}(K_{S_{i},*})^{})-1_{[i] S_{i}}) _{i}\) _(the evicted_ KV _is set to_ \(0\)_, and we need to subtract them when computing the normalization)_
* _Replacing_ \(S_{i}\) _by_ \([i]\) _in the above definition of_ \(o_{i}\) _and_ \(D_{i}\) _leads to standard generative process._

\({}^{}\) _The eviction policy (Definition 2.1) updates \(S_{i}\) based on \(S_{i-1}\) and their corresponding information._

**Remark 2.3**.: _Our goal is to find a KV cache eviction policy such that the output of the generative process is similar or comparable to the original one without limiting the cache size._

## 3 Observations

We present two key empirical insights of LLMs that inspire the design of H2O, as follows.

### Sparsity for Small Cache Size

Inspired by previous literature, which reveals the existence of attention sparsity in DistillBERT  and bounded-norm self-attention heads . We first show an observation on the sparsity of attention in pre-trained LLMs. Then we discuss how it can potentially unlock the possibility of reducing KV cache size without an accuracy drop. Given the normalized attention score \((QK^{})\) matrix that is calculated by the query matrix \(Q\) and the key matrix \(K\), we set the threshold as one percent of the maximum value in each row and calculates the corresponding sparsity.

Observation.We conduct zero-shot inference with the pre-trained OPT model on the validation set of Wiki-Text-103. We plot the layer-wise sparsity within attention blocks and visualize the normalized attention score matrix. The results are presented in Figure 2 (a). We observe that although the LLMs are densely trained, the resulting attention score matrices are highly sparse, with a sparsity over \(95\%\) in almost all layers.

Figure 2: (a) Attention Sparsity in pre-trained LLMs. (b) The distribution of accumulated attention scores with respect to the corresponding word (red scatter) and the co-occurrence times of words in the data (gray curve). The x-axis represents the word index in the vocabulary. (c) The performance comparison between the baseline model with full KV and the model _w.o._ heavy hitter. (d) Comparison between the baseline model with full KV, H2O with the local statistic, H2O with the global statistic, and the model with only the most recent KV (Local). Apart from the baseline model, each model is evaluated with \(20\%\) KV cache budget.

Insights.The attention blocks' sparsity suggests that access to all previous key and value embeddings is unnecessary for generating the next token. This suggests it is possible to evict unessential KV embeddings and reduce the requirement of KV cache during generation.

### Heavy-Hitters for Low Miss Rate

The previous section showed the sparse nature of attention blocks in pre-trained LLMs, which provides the opportunity for designing small KV cache size while still maintaining the performance of LLMs. However, determining the best eviction policy that preserves generation accuracy presents a combinatorial challenge. Although Belady's Algorithm  is optimal and easy to compute for standard cache (offline), it is not applicable for KV cache design. Because once evicting important KV, it could destroy the performance of LLMs due to the sequential dependency of LLM generation.

Observation.Fortunately, in the early stage of our exploration, we find that the accumulated attention scores of all the tokens within attention blocks follow a power-law distribution, as shown in Figure 2. This suggests the existence of a small set of tokens that are critical during generation. We denote those tokens as heavy-hitters (H\({}_{2}\)). In order to verify the importance of these tokens, we compare the quality of LLM generation after masking heavy hitters with that of the original model. Not surprisingly, as shown in Figure 2, the accuracy drops drastically, confirming the importance of those tokens. Additionally, we can see the accumulated attention score of each word (in red dots) have a high correlation with their co-occurrences in the data (gray curve).

Analysis.First, based on H\({}_{2}\), we see an opportunity to side-step from the combinatorial search problem and design a KV cache eviction policy that preserves the LLM generation quality. We conduct an empirical study implementing a KV cache eviction policy that retains only the H\({}_{2}\) and the recent KV embeddings in the cache. The intuition is that recent words typically exhibit stronger correlations with current tokens. We assess the effectiveness of this eviction policy through pre-trained OPT-30B and six downstream tasks. The outcomes of these evaluations are illustrated in Figure 2. It is obvious that the H\({}_{2}\) based eviction policy can largely reduce the KV cache size without degrading the performance of OPT-30B.

Moreover, during the post analysis, inspired by , we find that H\({}_{2}\) based policy is related to the classical greedy algorithm (a polynomial-time algorithm with provable guarantees) under the assumption that the attention schema is submodular. We present details in Appendix D.

**Lemma 3.1** (informal).: _Assuming the attention scheme is submodular, then greedily constructing the set \(S_{i}\) (without cache size limitation) satisfies the near-optimal property in terms of submodular._

## 4 Heavy-Hitter Oracle

The goal of this section is to propose the greedy algorithm using the H\({}_{2}\)-based policy and to show the provable guarantees. We first present the H\({}_{2}\)-based policy called H\({}_{2}\)O cache eviction policy and formulate its deployment in LLM generation as a variant of submodular maximization problem, named _dynamic submodular_. Then we present H\({}_{2}\)O in the generative process, followed by a practical example of deploying our proposal. Finally, we provide theoretical guarantees for H\({}_{2}\)O and show our efficient system implementation.

### Greedy Algorithm for Low-Cost Policy

We have shown a simple yet effective KV cache policy based on H\({}_{2}\). However, it is impractical to deploy such an algorithm because we do not have access to the future-generated tokens. Fortunately, we empirically observe that local H\({}_{2}\), which is calculated using local statistics at every decoding step by summing up the attention scores of the previous tokens, is equally effective as taking into account the attention of future tokens (Figure 2). In the following, we formally define this dynamic attention score computation (with space limitation) as a novel dynamic submodular type problem.

**Definition 4.1** (Dynamic submodular framework, informal).: _Define function \(F:2^{[n]} 2^{[n]}\), then for any set \(Z[n]\), we assume that \(F(Z,.):2^{[n]}\) is a submodular function w.r.t. to \(Z\), i.e., \(\) For all sets \(X,Y[n]\) satisfy that \(Z X Y\), \(\) For all element \(x[n]\) satisfy that \(x[n] Y\), we have \(f(X\{x\})-f(X) f(Y\{x\})-f(Y),\) where \(f():=F(Z,)\)._

**Remark 4.2**.: _We provide practical insights of Definition 4.1. \(X\) denotes the existing words in the KV cache. \(Y\) is any superset of \(X\). \(x\) can be viewed as a "word" which is either newly added to KV cache or existing deleted from KV cache. An example \(f\) can be attention score, i.e., see Algorithm 1. If we load the sequence of \(S_{1},S_{2},,S_{n}\) (we promise that \(|S_{i}| k\) and \(|S_{i} S_{i-1}| 1\)) into Definition 4.1, i.e., for each \(i[n]\), we choose \(Z=S_{i}\), then it becomes a particular instance of the dynamic submodular problem._

Next, we provide a formal description of our algorithm, followed by an example.

**Definition 4.3** (\(_{2}\)O Eviction Policy).: _Let \(F_{}:2^{[n]}\) denote certain score function. Let \(S_{i-1}\) denote the source set. Let \(S_{i}\) denote the target set. We defined the eviction policy \(g:S_{i-1} S_{i}\) s.t._

* \(|S_{i}|=k\) _(_KV cache _size is not changing over the time)_
* \(|S_{i} S_{i-1}| 1\) _or equivalently_ \(|S_{i} S_{i-1}| k-1\) _(we can evict at most_ \(1\) KV _in the_ KV _cache_)_
* _We construct_ \(S_{i}(S_{i-1}\{i\})\{u\}\) _as_ \(u_{v(S_{i-1}\{i\})}F_{}(S_{i-1}\{i\} \{v\})\)__

To describe our algorithm (Algorithm 1), we choose a particular instantiation of the function \(F_{}\), _i.e._, the summation of that sets in the attention matrix.

Figure 3 presents an illustrative example of our \(_{2}\) Eviction Algorithm. We assume that the budget size of KV cache is \(3\). Following the completion of the fourth decoding step, the KV embeddings associated with the third token are evicted based on the accumulated attention score. Consequently, these evicted KV embeddings become inaccessible in the subsequent decoding steps.

### Theoretical Guarantee and System Implementation

We state a theoretical result as follows. The proofs and more details are provided in Appendix D.

**Theorem 4.4** (informal).: _Under the mild assumption, let \(k\) denote the budget of space limitation. If for each token, we greedily compute the attention score based on top-\(k\) choice, then we can show the set \(_{i}\) we generate each for token \(i\) satisfy that \(f(_{i})(1-)(1-1/e)_{|S|=k}f(S)-\), where \(,>0\) are parameters._

**Remark 4.5**.: _We remark the above theorem provides a theoretical explanation of why can we hope our greedy algorithm (with cache limitation) can provide a good solution to the problem._

Implementation Details.We provide a general framework that can support any KV cache eviction algorithm and enhance throughput and reduce the latency of LLM generation with careful implementation. For example, to ensure I/O efficiency, we do not swap memory when stored KV is evicted, but directly fill with newly-added KV. More details are included in Appendix A.

Figure 3: Illustration of Algorithm 1 during two consecutive decoding steps.

## 5 Empirical Evaluation

In this section, our goal is to demonstrate that \(_{2}\), a remarkably simple KV cache eviction policy is capable of enhancing end-to-end throughput and reducing latency in wall-clock while maintaining generation quality across a broad spectrum of domains and tasks.

* In Section 5.1, we show that \(_{2}\) can reduce the memory footprint of KV cache by up to \(5\) without accuracy degradation on a wide range of model architectures (OPT, LLaMA, GPT-NeoX), sizes (from 6.7B to 175B) and evaluation benchmarks (HELM and lm-eval-harness). More importantly, can enhance the performance of existing KV cache sparsification techniques.
* In Section 5.2, we demonstrate that \(_{2}\) can increase the inference throughput by up to \(3\), \(29\), \(29\) compared to the state-of-the-art inference engine FlexGen, DeepSpeed and the widely used Hugging Face Accelerate without compromising model quality.
* In Section 5.3, we present extensive ablation studies to show the effectiveness of \(_{2}\) under different sequence lengths, especially the input with infinite sequence length and its compatibility with quantization.

All details (hyperparameters, data splits, etc.), along with additional experiments, are in Appendix A.

### End-to-End Results

We demonstrate that \(_{2}\) can reduce KV cache memory footprint by \(5\)-\(10\) while achieving comparable accuracy on a majority of tasks.

Setup.Our experiments are based on three representative model families of LLMs, including the OPT  with model sizes, LLaMA , and GPT-NeoX-20B . We sample eight tasks from two popular evaluation frameworks (HELM  and lm-eval-harness ): COPA , MathQA , OpenBookQA , PiQA , RTE , Winogrande , XSUM , CNN/Daily Mail . Also, we evaluate our approach on recent generation benchmarks, AlpaceEval  and MT-bench , and the details are included in Appendix. We use NVIDIA A\(100\)\(80\)GB GPU.

Baselines.Since \(_{2}\) evenly assigns the caching budget to \(_{2}\) and the most recent KV, except for full KV cache, we consider the "Local" strategy as a baseline method. In addition, we also provide two different variants of Sparse Transformers (strided and fixed) as strong baselines. Also, the full

Figure 4: Comparison results between the baseline model with full cache, our \(_{2}\), and the “Local” strategy that utilizes the most recent KV embeddings.

KV cache with fewer shots (0/1-shot) prompts are considered as the baseline, which has a similar sequence length of the \(5\)-shot tasks with \(20\%\) KV cache budget.

Main Results.We evaluate LLMs with KV cache budget ranging from \(4\%\) to \(100\%\) on \(5\)-shot downstream tasks. Results are summarized in Figure 4 and Table 1& 2. The following observations can be drawn: (1) With different KV cache budgets, our H\({}_{2}\)O demonstrates consistent and significant improvements against the "Local" strategy across various model sizes, model types, and downstream tasks. We can draw similar conclusions comparing H\({}_{2}\)O with other baselines like Sparse Transformer; (2) Meanwhile, with less than \(20\%\) KV cache budget(_i.e._, more than \(5\) memory reduction), H\({}_{2}\)O achieves comparable performance as the model with full KV embeddings; (3) H\({}_{2}\)O with \(20\%\) KV cache budget approximately uses \(1.2\) samples per input and show consistent improvement over zero-shot and one-shot full model that use \(1\) and \(2\) samples, respectively. (4) Our H\({}_{2}\)O shows consistent effectiveness in the more challenging long sequence generation tasks, XSUM, and CNN/Daily Mail.

Analysis.Since the evicted KV will not be seen in the future steps, dropping certain critical KV embeddings can cause a severe functional collapse, resulting in significant performance degradation, _e.g._, in {LLaMA-\(13\)B, XSUM} {LLaMA-\(7\)B, CNN/Daily Mail}, the "Local" strategy collapses at \(60\%\) budgets while our H\({}_{2}\)O can still match the full cache performance with \(20\%\) budgets. In some tasks, our methods even surpass the baseline models, which demonstrates a regularization effect of our H\({}_{2}\)O. For example, in {OPT-\(66\)B, RTE}, {OPT-\(30\)B, MathQA} and {GPT-NeoX-\(20\)B, XSUM}, our H\({}_{2}\)O achieves an extra performance improvement of \(0.73\%\), \(0.64\%\) and \(0.18\) with \(20\%\) KV cache budget, respectively. These consistent results validate the effectiveness of our H\({}_{2}\)O framework.

Enhancing Baseline Techniques.Importantly, we observe other sparsification baselines fail under an extremely low cache budget while combining the most recent KV embeddings with the ones of heavy hitters successfully achieves comparable performance as using full KV embeddings. From Table 2, we can observe that both "strided" and "fixed" sparse attention fail under \(20\%\) KV cache budgets, encountering a significant performance drop (up to \(35\%\) compared with the full cache). After combining with H\({}_{2}\), both approaches reach a similar performance as using full KV embeddings.

### Heavy Hitter for High-Throughput Generative Inference

   Methods & PiQA & COPA & OpenbookQA & Winogrande \\  Full & 80.09 & 81.00 & 44.80 & 71.51 \\
0-shot Full & 78.89 & 76.00 & 41.40 & 70.00 \\
1-shot Full & 79.11 & 76.00 & 43.60 & 70.24 \\ Local & 57.94 & 56.00 & 28.40 & 51.30 \\ H\({}_{2}\)O & 79.22 & 85.00 & 43.80 & 71.67 \\   

Table 1: Quantitatively comparison between H\({}_{2}\)O with Full methods of different number of shots.

   Models & COPA & OpenBookQA & PiQA & Winogrande \\  Full & \(85.00\) & \(43.20\) & \(78.51\) & \(70.24\) \\  Local w.o. H\({}_{2}\) & \(48.00\) & \(25.20\) & \(55.82\) & \(49.17\) \\  Local w.H\({}_{2}\) & \(84.00\) & \(43.00\) & \(78.45\) & \(69.06\) \\  Sparse Transformer (strided) w.o. H\({}_{2}\) & \(50.00\) & \(24.60\) & \(56.20\) & \(47.59\) \\ Sparse Transformer (strided) w.o. H\({}_{2}\) & \(83.00\) & \(42.60\) & \(78.24\) & \(69.61\) \\  Sparse Transformer (fixed) w.o. H\({}_{2}\) & \(61.00\) & \(23.80\) & \(58.60\) & \(49.88\) \\ Sparse Transformer (fixed) w.o. H\({}_{2}\) & \(76.00\) & \(41.40\) & \(77.80\) & \(64.96\) \\   

Table 2: Results of different sparsification methods _w._ or _w.o._ H\({}_{2}\). Experiments are conducted with OPT-\(30\)B with \(20\%\) KV cache budget.

   Seq. length &  &  &  \\  Model size & 6.7B & 30B & 6.7B & 30B & 6.7B & 30B \\  Accelerate & 20.4 (2, G) & 0.6 (8, C) & 15.5 (1, G) & **0.6** (8, C) & **5.6** (16, C) & **0.6** (8, C) \\ DeepSpeed & 10.2 (16, C) & 0.6 (4, C) & 9.6 (16, C) & **0.6** (4, C) & 10.1 (16, C) & **0.6** (4, C) \\ FlexGen & 20.2 (2, G) & 8.1 (144, C) & **16.8** (1, G) & **8.5** (80, C) & **16.9** (1, G) & **7.1** (48, C) \\  H\({}_{2}\)O (20\%) & 35.1 (4, G) & 12.7(728, C) & 51.7(4, G) & **18.83**(416, C) & **52.1** (4, G) & **13.82**(264, C) \\   

Table 3: Generation throughput (token/s) on a T4 GPU with different systems. In the sequence length row, we use “512 + 32” to denote a prompt length of 512 and a generation length of 32. “OOM” means out-of-memory. The gray text in the bracket denotes the effective batch size and the lowest level of the memory hierarchy that the system needs for offloading, where “C” means CPU and “G” means GPU.

[MISSING_PAGE_FAIL:9]

lion tokens, achieving a better performance (lower perplexity) than the original StreamLLM method  across various cache size. Further comparisons are reported in Appendix C.4.

_Q2:_ **Does the number of shots during inference effects the effectiveness of \(_{2}\)? _A2:_ Effective across zero-shot to ten-shots inference. We further examine \(_{2}\) under different numbers of shots during inference, and the results are reported in Table 10 and Figure 8. With different shots inference, our \(_{2}\) achieves matching performance (difference less than \(1.00\%\)) as the full model across different downstream tasks. The "Local" strategy encounters significant performance degradation (up to \(37.00\%\). Such results demonstrate the effectiveness of our \(_{2}\) under different inference scenarios. More details about zero-shot and one-shot inference are reported in Appendix C.3.

_Q3:_ **Compatible with Qualtization? _A3:_ Yes. To pursue further efficiency, we show the compatibility of \(_{2}\) with another orthogonal approach, _i.e._, quantization in Table 6. We use OPT-30B as our base model and COPA, OpenBookWA, and PiQA as evaluation tasks. Intuitively sparsity and quantization are highly related so combining them might introduce larger errors. Surprisingly the combination almost always achieves better accuracy than \(_{2}\) or quantization alone. Experiments about throughput improvement are detailed in Appendix C.2.

_Q4:_ **When does \(_{2}\) match the baseline with full \(\) embeddings? _A4:_ With both \(_{2}\) and the recent tokens. We investigate the separate effects of \(\) embeddings of \(_{2}\) and the local tokens. We conduct experiments on \(4\) tasks with OPT-\(13\)B and OPT-\(30\)B. For each task, we compare the performance of three \(\) cache eviction policies, including only the \(\) embeddings of \(_{2}\), only the ones of local tokens, and our \(_{2}\) that keep both. As shown in Table 9, only retaining the embeddings of \(_{2}\) or local tokens can't maintain a similar performance as the model using full embeddings, with a performance degradation from \(2.85\%\) to \(22.75\%\). Incorporating both components, our \(_{2}\) successfully retains the baseline performance with full embeddings. Besides, the model with only \(_{2}\) shows a consistent improvement against the one with only local tokens, which indicates \(_{2}\) might contribute more to maintaining the performance.

_Q5:_ **Extra benefits from \(_{2}\)? _A5:_ Increased diversity of generated text. Besides all the benefits of our \(_{2}\), we also observe an bonus introduced by \(_{2}\), _i.e._, the improved diversity of generated content. The results are reported in Appendix C.1. Given the same prompts, we visualize the generated text of the models with different \(\) cache budgets. Compared with the model of full \(\) cache, our \(_{2}\) can generate sentences with fewer repeated words and more creativity.

## 6 Conclusion and Discussion

In this paper, we study one of the key bottlenecks of LLM deployment, \(\) cache, particularly for long-content and large-batch generation applications. We propose \(_{2}\), a simple \(\) cache eviction policy for significantly reducing its memory footprint. The main insight of our approach is the recognition of a subset of tokens, known as Heavy Hitters, which contribute the most value when computing attention scores. We formulate the \(\) cache eviction as a dynamic submodular problem and provide the theoretical guarantees for our algorithm. Through extensive evaluations, we demonstrate that \(_{2}\) can significantly improve end-to-end throughput and decrease latency in wall-clock time, without compromising the generation quality of LLMs across a variety of tasks.

## 7 Acknowledgement

Ying Sheng and Clark Barrett are partly supported by NSF-2110397 and the Stanford Center for Automated Reasoning. Z. Wang is in part supported by a Google Research Scholar Award and the NSF AI Institute for Foundations of Machine Learning (IFML).

Figure 5: (Upper) streaming with \(_{2}\) to handle inputs with sequence lengths of four million tokens. (Bottom) Perplexity comparison between the original StreamLLM method and our \(_{2}\), results are collected on the first text sample of PG-19 .

[MISSING_PAGE_FAIL:11]

*  Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, et al. Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale. _arXiv preprint arXiv:2207.00032_, 2022.
*  HuggingFace. Hugging face accelerate. https://huggingface.co/docs/accelerate/index.
*  Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. High-throughput generative inference of large language models with a single gpu. _arXiv preprint arXiv:2303.06865_, 2023.
*  Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. _arXiv preprint arXiv:2301.00774_, 2023.
*  Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. _arXiv preprint arXiv:2306.11695_, 2023.
*  Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity. _arXiv preprint arXiv:2310.05175_, 2023.
*  Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. _arXiv preprint arXiv:2210.17323_, 2022.
*  Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. _arXiv preprint arXiv:2211.10438_, 2022.
*  Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. _arXiv preprint arXiv:2206.01861_, 2022.
*  Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. In _Advances in Neural Information Processing Systems_, 2022.
*  Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. _arXiv preprint arXiv:2208.07339_, 2022.
*  Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llvm compression and acceleration. _arXiv preprint arXiv:2306.00978_, 2023.
*  Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontanon, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. Colt5: Faster long-range transformers with conditional computation. _arXiv preprint arXiv:2303.09752_, 2023.
*  Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas Hoffmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. _arXiv preprint arXiv:2305.15805_, 2023.
*  Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. _arXiv preprint arXiv:2009.06732_, 2020.
*  Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning. In _2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)_, pages 97-110. IEEE, 2021.
*  Elizabeth J O'neil, Patrick E O'neil, and Gerhard Weikum. The lru-k page replacement algorithm for database disk buffering. _Acm Sigmod Record_, 22(2):297-306, 1993.

*  Donghee Lee, Jongmo Choi, Jong-Hun Kim, Sam H Noh, Sang Lyul Min, Yookun Cho, and Chong Sang Kim. Lrfu: A spectrum of policies that subsumes the least recently used and least frequently used policies. _IEEE transactions on Computers_, 50(12):1352-1361, 2001.
*  Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of self-attention matrices. _arXiv preprint arXiv:2106.03764_, 2021.
*  Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_, pages 5793-5831. PMLR, 2022.
*  Laszlo A. Belady. A study of replacement algorithms for a virtual-storage computer. _IBM Systems journal_, 5(2):78-101, 1966.
*  Simeng Han, Xiang Lin, and Shafiq Joty. Resurrecting submodularity for neural text generation. _arXiv preprint arXiv:1911.03014_, 2019.
*  Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.
*  Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
*  Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSNi Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. In _Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large Language Models_, 2022.
*  Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In _AAAI spring symposium: logical formalizations of commonsense reasoning_, pages 90-95, 2011.
*  Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2357-2367, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
*  Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In _EMNLP_, 2018.
*  Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In _Thirty-Fourth AAAI Conference on Artificial Intelligence_, 2020.
*  Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.
*  Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.
*  Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. _arXiv preprint arXiv:1808.08745_, 2018.
*  Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. _arXiv preprint arXiv:1602.06023_, 2016.

*  Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.
*  Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-jadge with mt-bench and chatbot arena, 2023.
*  Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. _arXiv preprint arXiv:2309.17453_, 2023.
*  Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. _arXiv preprint arXiv:2308.16137_, 2023.
*  Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. In _The International Conference on Learning Representations (ICLR)_, 2020.
*  Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. _arXiv preprint arXiv:1510.00149_, 2015.
*  Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2704-2713, 2018.
*  Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1325-1334, 2019.
*  Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In _International conference on machine learning_, pages 7543-7552. PMLR, 2019.
*  Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. _arXiv preprint arXiv:1611.06440_, 2016.
*  Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. _arXiv preprint arXiv:1810.05270_, 2018.
*  Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4340-4349, 2019.
*  Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. _J. Mach. Learn. Res._, 22(241):1-124, 2021.
*  Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2(7), 2015.
*  Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 4794-4802, 2019.
*  Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific knowledge from bert into simple neural networks. _arXiv preprint arXiv:1903.12136_, 2019.
*  Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _International Conference on Machine Learning_, pages 10347-10357. PMLR, 2021.

*  Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NIPS_, 2017.
*  Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. _Advances in neural information processing systems_, 32, 2019.
*  Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
*  Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. _arXiv preprint arXiv:1811.00937_, 2018.
*  Ajay Jaiswal, Liyan Tang, Meheli Ghosh, Justin Rousseau, Yifan Peng, and Ying Ding. Radbert-cl: Factually-aware contrastive learning for radiology report classification. _Proceedings of machine learning research_, 158:196-208, 2021.
*  Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. End-to-end open-domain question answering with bertserini. _arXiv preprint arXiv:1902.01718_, 2019.
*  Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for multi-hop reading comprehension at scale. _arXiv preprint arXiv:1905.05460_, 2019.
*  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.
*  Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond. _arXiv preprint arXiv:2304.13712_, 2023.
*  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
*  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.
*  Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
*  Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020.
*  Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.
*  Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv Kumar, and Suvrit Sra. Why {adam} beats {sgd} for attention models, 2020.
*  Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. _arXiv preprint arXiv:1908.03265_, 2019.
*  Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difficulty of training transformers. _arXiv preprint arXiv:2004.08249_, 2020.

*  Dusan Varis and Ondrej Bojar. Sequence length is a domain: Length-based overfitting in transformer models. _arXiv preprint arXiv:2109.07276_, 2021.
*  Wancong Zhang and Ieshan Vaidya. Mixup training leads to reduced overfitting and improved calibration for the transformer architecture. _arXiv preprint arXiv:2102.11402_, 2021.
*  Xiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng Gao. Very deep transformers for neural machine translation. _arXiv preprint arXiv:2008.07772_, 2020.
*  Peng Xu, Dhruv Kumar, Wei Yang, Wenjie Zi, Keyi Tang, Chenyang Huang, Jackie Chi Kit Cheung, Simon JD Prince, and Yanshuai Cao. Optimizing deeper transformers on small datasets. _arXiv preprint arXiv:2012.15355_, 2020.
*  Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W Ronny Huang, and Tom Goldstein. Gradinit: Learning to initialize neural networks for stable and efficient training. _Advances in Neural Information Processing Systems_, 34:16410-16422, 2021.
*  Jeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive gradient methods at the edge of stability. _arXiv preprint arXiv:2207.14484_, 2022.
*  Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. _arXiv preprint arXiv:2203.00555_, 2022.
*  Qiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming Tan, Jun Xiao, and Shiliang Pu. Unified normalization for accelerating and stabilizing transformers. _arXiv preprint arXiv:2208.01313_, 2022.
*  Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
*  Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pages 1097-1100, 2018.
*  Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. _arXiv preprint arXiv:2212.09720_, 2022.
*  Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. _arXiv preprint arXiv:2307.03172_, 2023.
*  Pranjal Awasthi and Anupam Gupta. Improving length-generalization in transformers via task hinting. _arXiv preprint arXiv:2310.00726_, 2023.
*  Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating transformers via kernel density estimation. _arXiv preprint arXiv:2302.02451_, 2023.
*  Josh Alman and Zhao Song. Fast attention requires bounded entries. _arXiv preprint arXiv:2302.13214_, 2023.
*  Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the performance edge over linear attention. _arXiv preprint arXiv:2310.11685_, 2023.
*  Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness for dynamic attention maintenance in large language models. _arXiv preprint arXiv:2304.02207_, 2023.
*  Yeqi Gao, Zhao Song, and Xin Yang. Differentially private attention computation. _arXiv preprint arXiv:2305.04701_, 2023.
*  Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers. _arXiv preprint arXiv:2306.02896_, 2023.

*  Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic attention sparsification algorithms for over-parameterized feature dimension. _arxiv preprint: arxiv 2304.03426_, 2023.
*  Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh and sinh regression problems. _arXiv preprint, 2303.15725_, 2023.
*  Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression. _arXiv preprint arXiv:2304.10411_, 2023.
*  Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. Polysketchformer: Fast transformers via sketches for polynomial kernels. _arXiv preprint arXiv:2310.01655_, 2023.
*  Yeqi Gao, Zhao Song, and Junze Yin. An iterative algorithm for rescaled hyperbolic functions regression. _arXiv preprint arXiv:2305.00660_, 2023.
*  Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P Woodruff, and Amir Zandieh. Hyperattention: Long-context attention in near-linear time. _arXiv preprint arXiv:2310.05869_, 2023.
*  Timothy Chu, Zhao Song, and Chiwun Yang. How to protect copyright data in optimization of large language models? _arXiv preprint arXiv:2308.12247_, 2023.
*  Ritwik Sinha, Zhao Song, and Tianyi Zhou. A mathematical abstraction for balancing the trade-off between creativity and reality in large language models. _arXiv preprint arXiv:2306.02295_, 2023.
*  Yeqi Gao, Zhao Song, Weixin Wang, and Junze Yin. A fast optimization view: Reformulating single layer attention in llm based on tensor and svm trick, and solving it in matrix multiplication time. _arXiv preprint arXiv:2309.07418_, 2023.
*  Gary Marcus, Ernest Davis, and Scott Aaronson. A very preliminary analysis of dall-e 2. _arXiv preprint arXiv:2204.13807_, 2022.
*  Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm for attention computation. _arXiv preprint arXiv:2307.08045_, 2023.
*  Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis. On the optimization and generalization of multi-head attention. _arXiv preprint arXiv:2310.12680_, 2023.
*  Yeqi Gao, Zhao Song, and Junze Yin. Gradientcoin: A peer-to-peer decentralized large language models. _arXiv preprint arXiv:2308.10502_, 2023.
*  Yichuan Deng, Zhao Song, Shenghao Xie, and Chiwun Yang. Unmasking transformers: A theoretical approach to data recovery via attention weights. _arXiv preprint arXiv:2310.12462_, 2023.
*  Yichuan Deng, Zhihang Li, Sridhar Mahadevan, and Zhao Song. Zero-th order algorithm for softmax attention optimization. _arXiv preprint arXiv:2307.08352_, 2023.
*  Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing matrix softmax attention to kronecker computation. _arXiv preprint arXiv:2310.04064_, 2023.
*  Yichuan Deng, Zhao Song, and Shenghao Xie. Convergence of two-layer regression with nonlinear units. _arXiv preprint arXiv:2308.08358_, 2023.
*  Timothy Chu, Zhao Song, and Chiwun Yang. Fine-tune language models to approximate unbiased in-context learning. _arXiv preprint arXiv:2310.03331_, 2023.
*  Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, and Sanjeev Arora. Trainable transformer in transformer. _arXiv preprint arXiv:2307.01189_, 2023.
*  Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while predicting the masked word? _arXiv preprint arXiv:2303.08117_, 2023.

*  Alexander Schrijver. _Combinatorial optimization: polyhedra and efficiency_, volume 24. Springer, 2003.
*  Kasper Green Larsen, Jelani Nelson, Huy L Nguyen, and Mikkel Thorup. Heavy hitters via cluster-preserving clustering. In _2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 61-70. IEEE, 2016.
*  Vasileios Nakos and Zhao Song. Stronger l2/l2 compressed sensing; without iterating. In _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing_, pages 289-297, 2019.
*  Vasileios Nakos, Zhao Song, and Zhengyu Wang. (nearly) sample-optimal sparse fourier transform in any dimension; ripless and filterless. In _2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 1568-1577. IEEE, 2019.
*  Andreas Krause and Carlos Guestrin. Beyond convexity: Submodularity in machine learning. _ICML Tutorials_, 2008.
*  Jeff Bilmes. Submodularity in machine learning applications. In _Twenty-Ninth Conference on Artificial Intelligence, AAAI-15 Tutorial Forum_, 2015.
*  Simeng Han, Xiang Lin, and Shafiq Joty. Resurrecting submodularity for neural text generation. _arXiv preprint arXiv:1911.03014_, 2019.
*  George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular set functions--i. _Mathematical programming_, 14(1):265-294, 1978.
*  Lianke Qin, Zhao Song, and Yitan Wang. Fast submodular function maximization. _arXiv preprint arXiv:2305.08367_, 2023.
*  Junda Wu, Tong Yu, Rui Wang, Zhao Song, Ruiyi Zhang, Handong Zhao, Chaochao Lu, Shuai Li, and Ricardo Henao. Infoprompt: Information-theoretic soft prompt tuning for natural language understanding. _arXiv preprint arXiv:2306.04933_, 2023.
*  Shuai Li, Zhao Song, Yu Xia, Tong Yu, and Tianyi Zhou. The closeness of in-context learning and weight shifting for softmax regression. _arXiv preprint_, 2023.
*  Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication time. In _STOC_, 2019.
*  Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix multiplication time. In _Conference on Learning Theory_, pages 2140-2157. PMLR, 2019.
*  Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior point method for semidefinite programming. In _2020 IEEE 61st annual symposium on foundations of computer science (FOCS)_, pages 910-918. IEEE, 2020.
*  Zhao Song and Zheng Yu. Oblivious sketching-based central path method for linear programming. In _International Conference on Machine Learning_, pages 9835-9847. PMLR, 2021.
*  Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm for solving general lps. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 823-832, 2021.
*  Bahae Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. Solving sdp faster: A robust ipm framework and efficient implementation. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 233-244. IEEE, 2022.
*  Yuzhou Gu and Zhao Song. A faster small treewidth sdp solver. _arXiv preprint arXiv:2211.06033_, 2022.

*  Yuzhou Gu, Zhao Song, and Lichen Zhang. A nearly-linear time algorithm for structured support vector machines. _arXiv preprint arXiv:2307.07735_, 2023.
*  Lianke Qin, Zhao Song, Lichen Zhang, and Danyang Zhuo. An online and unified algorithm for projection matrix vector multiplication with application to empirical risk minimization. In _AISTATS_, 2023.
*  Zhao Song, Mingquan Ye, and Lichen Zhang. Streaming semidefinite programs: \(O()\) passes, small space and fast runtime. _arXiv preprint arXiv:2309.05135_, 2023.
*  Haotian Jiang, Yin Tat Lee, Zhao Song, and Lichen Zhang. Convex minimization with integer minima in \((n^{4})\) time. In _ACM-SIAM Symposium on Discrete Algorithms (SODA)_, 2024.
*  S Cliff Liu, Zhao Song, Hengjie Zhang, Lichen Zhang, and Tianyi Zhou. Space-efficient interior point method, with applications to linear programming and maximum weight bipartite matching. In _ICALP_. arXiv preprint arXiv:2009.06106, 2020.

## Appendix

### Table of Contents

* A More Implementation Details
* B Extended Related Works, Discussions, and Limitations
* B.1 Extended Related Works
* B.2 Discussions and Limitations
* C Extended Experiments
* C.1 Increased Diversity of Generated Text
* C.2 Throughput Improvement of H\({}_{2}\)O Combining with Quantization
* C.3 Effectiveness on Zero-shot and One-shot Inference
* C.4 Comparison with StreamingLLM
* C.5 Enhancing the "Top-K" Baseline
* C.6 Separate Effects of Heavy-Hitter and Local Tokens
* C.7 Performance of Inference with Different Number of Shots.
* C.8 Heavy-Hitter in Attention Blocks
* C.9 Comparison with SpAtten
* C.10 Heavy-Hitter in MLP Blocks
* D Theoretical Analysis
* D.1 Notations
* D.2 Submodular
* D.3 Dynamic Submodular
* D.4 Static Attention
* D.5 Recursive Attention Definition
* D.6 Eviction Policy
* D.7 Explaining Submodular Diminishing Return Property in Attention Scheme
* D.8 Submodular: High Level Ideas
* D.9 Robust Greedy with Error Propagation
* D.10 Robust Submodular and Adding Items
* D.11 Universal Conditions
* D.12 Induction Lemma for Exact Function
* D.13 Induction Lemma for Approximate Function
* D.14 Theoretical Result
* D.15 Extended Related Work for Theoretical Attention Problems
* D.16 Sparsity Preserving
* D.17 Definition of Loss Function
* D.18 Gradient
* D.19 Hessian
* D.20 Hessian is Positive Definite
* D.21 Hessian is Lipschitz
* D.22 Greedy Type Algorithm
More Implementation Details

In this section, our goal is to provide the details of system implementation (mentioned in Section 4.2) and experiment settings (mentioned in Section 5), as well as the pseudocode.

System Details.We implement \(_{2}\) on top of FlexGen. FlexGen is a white-box implementation of OPT models, and we have done some surgery on handling the KV cache. Specifically, for the given parameter \(K\), we always maintain a list of KV cache with the first \(K\) entries as heavy hitters, and the last \(K\) entries as most recent tokens. In order to avoid data movement in memory, the memory for KV cache is preallocated. We use a circular queue to update the last \(K\) entries efficiently.

Experiment Details.Our study involves the evaluation with varying sizes of KV cache that encompass \(4\%\), \(10\%\), \(20\%\), and \(60\%\) of the prompt's length. We select two tasks (XSUM and CNN/Daily Mail) from the HELM framework  and present the performance based on \(1000\) test samples. Additionally, we employ the lm-eval-harness framework  for six other tasks (COPA, MathQA, OpenBookQA, PiQA, RTE, and Winogrande). For the tasks derived from the HELM framework, we report the performance for zero-shot inference, while for the tasks from lm-eval-harness, we default to conduct five-shot inference.

Pseudocode.We show a simplified pseudocode below to demonstrate our implementation skeleton. The function generation_loop() is the base loop in FlexGen that controls prefetch and overlap the I/O streams and computation. Then in function compute_layer(), the function attention_forward() will be called for attention layers. During prefill iterations, the function compute_attention() will return \(K\) heavy hitters and \(K\) recent tokens if the prompt has a length greater than or equal to \(2K\), otherwise return all the KV cache. The function compute_attention() will return new KV cache and the indices for evicted entries during decoding iterations, which would be the place to implement a customized eviction strategy. (If the current number of stored KV cache is less than \(2K\), the eviction will be ignored.) During each decoding iteration, the oldest one among the last \(K\) tokens (if we have no less than \(K\) tokens' KV cache stored) will be removed from the reserved last \(K\) entries, one heavy hitter will be evicted for each head, and the newest token will be added to the KV cache with a position in the last \(K\) entries. This happens in the function store_cache().

``` defgeneration_loop(...): #Prologue...
#Generate foriinrange(gen_len): forjinrange(num_layers): forkinrange(num_gpu_batches): load_weight(i,j+1,k) load_cache(i,j,k+1) store_hidden(i,j,k-1) load_hidden(i,j,k+1) compute_layer(i,j,k) store_cache(i,j,k-1) sync() #Epilogue...
#histhehiddenstates(activations) defattention_forward(h,...): #theread/writebufferareintermediatestopsforprefetching ifprefill: h,new_k_cache,new_v_cache=compute_attention(h,...) #selectKheavyhittersandKrecenttokens new_k_cache,new_v_cache=select(new_k_cache,new_v_cache,K) cache_writebuffer.store(new_k_cache,new_v_cache) else: k_cache,v_cache=cache_read_buf.pop() #evict_idstracktheentriesthatwillbecvicted h,new_k_cache,new_v_cache,evict_ids= compute_attention(h,k_cache,v_cache,...)

cache_write_buffer.store(new_k_cache,new_v_cache,evict_ids)  return h defstore_cache(...):  ifprefill:  #storeacheddirectly... else:  k_new,v_new,evict_ids=cache_write_buffer.pop()  #circularqueuefortthelastKentries  #extracttheindexfortheoldesttokenati-thiteration  oldest=((i-1)%K)-K  #updatetheKVcache(k_homeandw_home)  cache_replace(k_home,evict_ids,k_new,K,oldest)  cache_replace(v_home,evict_ids,v_new,K,oldest) ```

## Appendix B Extended Related Works, Discussions, and Limitations

The goal of this section is to first introduce more background and related works for Section 2, then describe some previous attempts in our experiments as well as discuss the social impact and limitations of this work.

### Extended Related Works

Quantization, Pruning, Distillation for Inference.Previously, model compression algorithms have been extensively investigated as a viable approach for mitigating the computational resource requirements of model inference. These algorithms can be broadly categorized into three groups: (1) quantization , which involves mapping model parameters or activations from high-precision data types to low-precision counterparts, such as using 8-bit integers instead of the commonly employed 32-bit floating point format; (2) pruning or sparsity , which aims to eliminate unnecessary neurons or weights within the models; (3) and distillation  where predictions from larger models are utilized as supervised information to train smaller models.

Transformer in NLP.Transformers  as a popular option have been frequently adopted by plenty of natural language processing (NLP) applications with prevailing successes . Roughly, modern transformer-based networks can be categorized into two groups: (1) Encoder-Decoder or Encoder-only (_i.e._, BERT-style models ). This type of transformers commonly leverages the Masked Language Modeling task which encourages models to capture the intrinsic relationship between words and their context. Notable examples include BERT , RoBBERTA  and T5 . (2) Decoder-only (_i.e._, GPT-style models ). Usually, this group of transformers adopts the Casual Language Modeling task, which is optimized to generate the next word/token in a sequence based on the preceding words/tokens. Such an autoregressive manner is highly preferred by downstream tasks like text generation and question answering. GPT-3 , OPT , PaLM , and BLOOM  are representative architectures within this huge family.

Training of Transformer.Training a gigantic transformer-based model is not trivial. It notoriously suffers from various issues such as overfitting, instability, etc.  analyze these bottlenecks from the optimization perspective. To address the issues, a great amount of pioneering effort is devoted, including data augmentations , a better initialization , customized optimizers , improved normalization , weight decay , and early stopping. However, there is still a long way to go before we can fully clarify the mystery of transformer training.

### Discussions and Limitations

Previous Attempts.During our experiments, we find several noteworthy observations. In \(O}\), employing the accumulated attention score to evict KV embeddings can lead to a potential bias favoring the least recent tokens. This bias arises because most previous tokens have a higher number of attention scores, resulting in a higher accumulated attention score and, consequently, a greater likelihood of being retained. To address this concern, we conducted an additional experiment utilizingthe averaged attention score to determine which \(\) embeddings should be retained. However, this alternative approach resulted in performance degradation. Additionally, we observed a significant proportion of \(_{2}\) occurrences at the beginning of sentences. This finding suggests that the initial tokens play a substantial role in subsequent generation tasks.

Social Impact.Our work represents an initial effort in designing a KV Cache policy, a realm that has been relatively unexplored and yet is a significant bottleneck in LLMs. The proposed Heavy Hitter Oracle (\(_{2}\)) provide a solution to improve the efficiency of LLM generation, which can save energy cost and contribute to green AI. Besides, our approach also serves as a source of inspiration for future advanced algorithm designs. We envision \(_{2}\) as a foundational framework that could facilitate further innovation in this area. Moreover, long content generation is an area of growing importance that currently grapples with several efficiency issues. We hope our work that supports the generation of very long sequences will support further research in this direction, particularly in terms of enhancing consistency, devising superior evaluation methods, and establishing robust benchmarks.

Furthermore, another contribution of this study is the formulation of a dynamic submodular framework. We believe that this theoretical framework possesses the potential to be applicable beyond specific domains of interest. For instance, there may exist numerous other dynamic problems where the task involves solving a submodular problem with slight variations at each time.

Limitations.Furthermore, despite the notable advancements in throughput of our \(_{2}\), implementing LLMs for generative inference remains challenging due to the immense parameter count. As a substantial portion of these parameters is encompassed within the MLP blocks, building upon our observations of \(_{2}\) occurrences in the MLP blocks, future research efforts can be directed towards leveraging the characteristics of \(_{2}\) to devise an offloading policy. Such a policy can potentially enhance the efficiency of LLM inference even further.

Extended Experiments

In this section, our goal is to demonstrate that \(O}\) can improve the diversity of generated text (mentioned in Section 5.3), throughput is further improved when combined with quantization (mentioned in Section 5.3), and superior performance to handle extremely long length inputs (up to four middle tokens, mentioned in Section 5.3). Moreover, additional investigations about \(}\) are reported, including experiments under zero-shot/one-shot inference regime; \(O}\) can also enhance the "Top-K" baseline; extra results of \(}\) in attention blocks; the emergence of \(}\) in MLP blocks as well as its properties.

### Increased Diversity of Generated Text

Given the same prompt text, we visualize the generated text with OPT-\(6.7\)B and LLAMA-\(7\)B across different methods, including the baseline model with full cache, our \(O}\), and the "Local" strategy. Results are reported in Figure 6 and 7. Even with less \(\) cache budget, our \(O}\) can generate more diverse content. Specifically, with the OPT-\(6.7\)B, the full model generates some redundant works, like "a few years after the events of the first game" while our \(O}\) describes "the game is a first-person exploration game". As a comparison, when all \(\) cache budget is assigned to the most recent tokens, the model fails to generate meaningful text and only repeats the word "." and ".". Similar observations can also be drawn from the results of LLAMA-\(7\)B, in which the full model repeatedly says "so moving that", "so moved that", and "began to cry" while our \(O}\) describes both the people and the environment.

Moreover, We conducted a quantitative comparison via diversity metric(Self-BELU ). We randomly sample \(100\) instances from the XSUM dataset, as prompts. Subsequently, we employed the LLAMa-\(7\)B to generate text of equal length. The full model reaches a Self-BELU score of \(0.0057\) while \(O}\) and the local method achieve \(0.0051\) and \(0.0436\), respectively. The comparatively lower Self-BELU score of \(O}\) indicates the slightly increased diversity of generated text.

Figure 6: Visualization of one generation example with OPT-\(6.7\)B. Results are compared between the baseline model with full cache, our \(O}\), and the “Local” strategy that utilizes the most recent \(\) embeddings.

### Throughput Improvement of \(O}\) Combining with Quantization

Table 6 demonstrates \(O}\) are capable of quantization and perverse the full model's performance with even \(4\)-bit quantization. To further explore the compatibility of our \(O}\) with quantization with respect to throughput improvement, we conduct an additional evaluation with the quantization method implemented in FlexGen (Note that  employed a different 4-bit quantization method). The corresponding results are presented in Table 7. Notably, for OPT-6.7B, we observed extra performance enhancements in \(O}\) when utilizing quantization compared to the vanilla version. This improvement results from the GPU memory freed by weight quantization, which allows for a significant increase in batch size. However, it should be emphasized that the quantization method employed in FlexGen is not implemented most efficiently, resulting in considerable computational overhead. Despite the batch size being enlarged by \(20\) times, the actual throughput improvement is less than \(2\) times. Nevertheless, it is important to acknowledge the potential benefits of combining \(O}\) with quantization, as exemplified by the ability to increase the batch size further. For instance, the implementation of \(4\)-bit quantization could be accelerated by an optimized CUDA kernel.

   Models & COPA & OpenBookQA & PiQA \\  Full & \(85.00\) & \(43.20\) & \(78.51\) \\  \(O}\) & \(84.00\) & \(43.00\) & \(78.45\) \\  Quant-4bit & \(84.00\) & \(43.28\) & \(78.67\) \\  \(O}\)_w._ Quant-4bit & \(84.00\) & \(43.20\) & \(78.80\) \\   

Table 6: Compatibility with Quantization.

   Seq. length & 512+32 & 512+512 & 512+1024 \\ Model size & 6.7B & 6.7B & 6.7B \\  Accelerate & 20.4 (2) & 15.5 (1) & OOM \\ DeepSpeed & OOM & OOM & OOM \\ FlexGen & 20.2 (2) & 16.8 (1) & 16.9 (1) \\  \(O}\) (20\%) & 35.1 (4) & 51.7 (4) & 52.1 (4) \\ \(O}\)-c (20\%) & 50.5 (70) & 72.5 (52) & 62.3 (44) \\   

Table 7: Generation throughput (token/s) with different systems without offloading. We use \(O}\)-c denotes the \(O}\) with 4-bit weights compression. In the sequence length row, we use “512 + 32” to denote a prompt length of 512 and a generation length of 32. “OOM” means out-of-memory. The gray text in the bracket denotes the batch size. We run OPT-6.7B on a single T4 GPU.

Figure 7: Visualization of one generation example with LLaMA-7B. Results are compared between the baseline model with full cache, our \(O}\), and the “Local” strategy that utilizes the most recent KV embeddings.

### Effectiveness on Zero-shot and One-shot Inference

When facing zero-shot and one-shot inference tasks, H2O can successfully mitigate the memory requirements of the KV cache by up to \(5\) under both one/zero-shot inference across different tasks, achieving matching performance as the model with full KV cache while the local method fails. However, unlike 5-shot, we also found that some tasks are more difficult, and the model requires more information to generate the correct content, resulting in a higher KV cache budget (30-40%). This might be expected since 0/1 shots in our benchmarks have shorter sequence lengths ranging from \(100\) to \(300\).

### Comparison with StreamingLLM

Recent works, such as StreamLLM  and LM-Infinite , have shown promising potential in enabling Language Models (LLMs) to handle input of infinite length. They achieve this by only retaining the initial tokens and a limited local context. However, this approach may pose challenges for certain tasks where vital information lies within the middle of the input and would be lost using this strategy. We investigate it through two specific types of tasks:

Figure 8: Comparison results on zero-shot and one-shot inference.

**Multi-document question answering**: In this task, each test sample comprises ten documents, followed by a question. The key information to answer this question is stored in one of the documents. We rearranged the crucial document's position and found that the eviction strategy in StreamLLM or LM-Infinite can not perform well when the key document has been dropped.

**Text Summarization Task** (XSUM and CNN-DailyMail): Text summarization tasks require models to generate concise summaries of lengthy texts. Effective summarization demands a comprehensive understanding of the entire document, making it challenging when crucial information is dispersed throughout the input. In particular, summarization often relies on long-context attention, and critical information may not be effectively captured within the limited local tokens.

The results are reported in Figure 9, illustrating a consistent decline in the performance of StreamLLM. Since StreamLLM will always maintain the first few tokens as well as the local tokens, regardless of various input content, such a strategy will inevitably result in the loss of crucial information and subsequently lead to a decrease in performance. In contrast, our H\({}_{2}\)O delivers markedly superior performance. Also, with H\({}_{2}\)O, The model can successfully stream to four million tokens.

### Enhancing the "Top-K" Baseline

We find H\({}_{2}\) can further enhance another strong baseline with a "Top-K" strategy. The results are reported in Table 8. After combing with H\({}_{2}\), the "Top-K" method achieves an extra improvement with up to \(2.00\%\) accuracy across \(4\) different tasks.

### Separate Effects of Heavy-Hitter and Local Tokens

Table 9 demonstrates the separate effects of Heavy-Hitters and the local tokens. And we can observe Heavy-Hitters contribute more to maintaining the performance of models.

### Performance of Inference with Different Number of Shots.

Table 10 demonstrates our H\({}_{2}\)O achieves consistent improvements across different number of shots during inference and maintains the full model's performance with \(20\%\) memory budget.

   Models & COPA & OpenBookQA & PiQA & Winogrande \\  Full & \(85.00\) & \(43.20\) & \(78.51\) & \(70.24\) \\  TopK _w.o._ H\({}_{2}\) & \(80.00\) & \(41.40\) & \(76.96\) & \(65.35\) \\  TopK _w._ H\({}_{2}\) & \(82.00\) & \(42.80\) & \(77.96\) & \(66.48\) \\   

Table 8: Results of the “Top-K” method _w._ or _w.o._ H\({}_{2}\). Experiments are conducted with OPT-\(30\)B with \(20\%\) KV cache budget.

Figure 9: Comparison results of StreamLLM  and our H\({}_{2}\)O on generization tasks. The number in each method represents the KV Cache budget of the start/heavy-hitter tokens and the local tokens, respectively. For example, H2O-256-256 means maintaining 256 Heavy-Hitters and 256 local tokens.

### Heavy-Hitter in Attention Blocks

The distribution of accumulated attention scores of all the tokens within attentions blocks is illustrated in Figure 10. We can observe that H\({}_{2}\) broadly exists in each layer.

### Comparison with SpAtten

Compared with SpAtten , the main differences of H\({}_{2}\)O are i) They accumulate attention scores across attention heads and layers, while in our algorithm, each token can be kept or evicted independently across heads and layers, providing more flexibility for selecting critical KV embeddings; ii) We also use KV of the most recent tokens during generation and demonstrate that such H\({}_{2}\) tokens can effectively enhance other sparse attention strategies; iii) we formulate the KV cache eviction as a dynamic submodular problem and prove a theoretical guarantee (under mild assumptions) for our novel algorithms. Moreover, we also provide a quantitative comparison with SpAtten, and the results are reported in Table 11.

### Heavy-Hitter in MLP Blocks

Besides the attention blocks, the presence of Heavy-Hitters (H\({}_{2}\)) is observed within the MLP blocks of LLMs. We utilize the Wiki-Text-103 dataset as the input and record the activated frequency of neurons in the hidden layer of MLP blocks. As depicted in Figure 11, the activated frequency of neurons follows a power-law distribution, wherein a small number of neurons are activated by nearly all input tokens (with a \(100\%\) frequency) while the majority of other neurons are rarely activated.

Subsequently, a thorough examination of various characteristics pertaining to H\({}_{2}\) in MLP blocks is conducted, encompassing the following aspects: (1) The elimination of H\({}_{2}\) leads to a substantial decline in performance, although such degradation can be easily recovered even with a mere \(1\%\) of the training data; (2) H\({}_{2}\) exhibits a significant degree of overlap across different type of input content; (3) The emergence of H\({}_{2}\) occurs early in the training process, thus exhibiting an "early-bird" characteristic, and their positions undergo gradual changes during subsequent training phases.

    &  &  &  \\  & & OPT-30B & OPT-66B & OPT-30B & OPT-66B \\   & Full & 43.20 & 44.40 & 43.40 & 44.80 \\  & Local & 25.20 & 30.60 & 26.00 & 38.80 \\  & H\({}_{2}\)O & 43.00 & 44.20 & 42.80 & 44.80 \\   & Full & 58.00 & 83.00 & 86.00 & 85.00 \\  & Local & 48.00 & 59.00 & 60.00 & 76.00 \\  & H\({}_{2}\)O & 84.00 & 82.00 & 85.00 & 86.00 \\   & Full & 26.23 & 27.87 & 28.67 & 27.00 \\  & Local & 20.87 & 25.49 & 21.11 & 23.08 \\   & H\({}_{2}\)O & 26.87 & 27.67 & 26.47 & 27.30 \\   

Table 10: Results under different sequence length of OPT-30B with \(20\%\) KV cache budget.

   Models & COPA & OpenBookQA & PiQA \\  Full & \(85.00\) & \(43.20\) & \(78.51\) \\  SpAtten & \(82.00\) & \(41.90\) & \(77.06\) \\  H\({}_{2}\)O & \(84.00\) & \(43.00\) & \(78.45\) \\   

Table 11: Comparison between SpAtten  and H\({}_{2}\)O across various tasks with OPT-\(30\)B.

    &  & Full & \(w\): Local & \(w\): H\({}_{2}\) & \(w\): Local + H\({}_{2}\) \\   & OPT-\(13\)B & \(77.37\) & \(54.62\) & \(76.12\) & \(77.26\) \\  & OPT-\(30\)B & \(78.51\) & \(55.82\) & \(67.25\) & \(78.45\) \\   & OPT-\(13\)B & \(41.40\) & \(25.60\) & \(30.40\) & \(41.20\) \\  & OPT-\(30\)B & \(43.20\) & \(25.20\) & \(26.60\) & \(43.00\) \\   & OPT-\(13\)B & \(26.67\) & \(22.04\) & \(23.82\) & \(26.93\) \\  & OPT-\(30\)B & \(26.23\) & \(20.87\) & \(21.98\) & \(26.87\) \\   & OPT-\(13\)B & \(68.59\) & \(49.96\) & \(51.85\) & \(67.32\) \\  & OPT-\(30\)B & \(70.24\) & \(49.17\) & \(47.36\) & \(69.06\) \\   

Table 9: Ablation study of H\({}_{2}\)O across different tasks.

Figure 10: The distribution of accumulated attention scores with respect to the corresponding word. The x-axis represents the word index in the vocabulary, and the y-axis represents the accumulated attention score. Results are obtained from OPT-\(1.3\)B.

Figure 11: The emergence of H\({}_{2}\) in MLP blocks of OPT-\(6.7\)B. The x-axis represents the index of neurons in the hidden layers of MLP blocks, and the y-axis represents the activated frequency.

Elimination of H\({}_{2}\).We first train a GPT-2 using Wiki-Text-\(103\) dataset and subsequently identify and prune the neurons exhibiting an activation frequency exceeding \(20\%\) (_i.e.,_ H\({}_{2}\)). This pruning operation leads to a substantial decline in performance, as evidenced by an increase in perplexity from \(19.32\) to \(31.78\). The results emphasize the criticality of H\({}_{2}\) in preserving the functionality of the model. To assess the recoverability of the discarded information, we conduct a few-shot fine-tuning experiment, and the results are summarized in Table 12. The pruned model is fine-tuned with varying ratios of training data for \(500\) iterations, and it successfully regains performance levels equivalent to those of the pre-trained model. In contrast, when training the model from scratch using only \(1\%\) of the training data, the resulting model achieves a perplexity of \(554.12\) only. These findings demonstrate that the knowledge encoded in H\({}_{2}\) can be easily restored.

Overlap across Diverse Input Contents.Moreover, we conduct a comparative analysis of the activation frequencies acquired from various input contents. Specifically, utilizing the pretrained OPT-\(1.3\)B model, we evaluate three datasets, namely Wiki-Text-103, Penn Treebank, and Amazon Review. The positioning of H\({}_{2}\) is depicted in Figure 12, revealing significant concurrence across multiple datasets.

Early-Bird Property.Furthermore, our investigation reveals that H\({}_{2}\) displays an "early-bird" characteristic, as illustrated in Figure 13. By visualizing the distribution of activation frequencies across various checkpoints throughout the training process, we observe the emergence of a power-law behavior at an initial stage, specifically as early as a training budget of \(4\%\). Subsequently, the positions of H\({}_{2}\) exhibit gradual and minimal changes.

Figure 13: The distribution of activated frequency during training. Experiments are conducted with different checkpoints of OPT-\(2.7\)B during training.

   Settings & \(1\%\) & \(10\%\) & \(40\%\) & \(100\%\) \\  Pretrained Model &  \\ Remove H\({}_{2}\) &  \\  Fine-tuning & \(19.86\) & \(19.84\) & \(19.76\) & \(19.83\) \\   

Table 12: : Perplexity on the test-set of Wiki-Text-\(3\) with GPT-\(2\).

Figure 12: The distribution of activated frequency across diverse input content. The x-axis represents the index of neurons, which is ordered by the activated frequency from Wiki-Text-103.

Theoretical Analysis

Recently, a number of works have studied the attention scheme in LLMs from a theoretical perspective . In this work, we provide a different and novel angle compared to the previous work. We present the concept of the submodular property and propose an eviction policy, known as greedy \(_{2}\), which is a modification of dynamic submodular maximization. Furthermore, assuming the attention scheme to be submodular, we establish that constructing the set \(S_{i}\) without any cache size limitation satisfies the near-optimal property in terms of submodularity. We provide theoretical guarantees for our robust and approximate greedy eviction policy algorithm (Algorithm 2). Due to space limitation, we only give informal description of algorithm (Algorithm 2) in Section 4.1 In Section D.6, we give an algorithm (Algorithm 2) which has full and complete implementation details for Algorithm 1. We also offer a mathematical formulation for sparsity preservation that is observed in Section 3 and proposed an algorithm (Algorithm 4) to solve the problem.

Specifically, in Section D.1, we provide several basic definitions and notations. In Section D.2, we briefly the definition of submodular function. In Section D.3, we define the dynamic submodular framework, which gives the formal version of Definition 4.1. In Section D.4, we briefly review the static attention computation problem. In Section D.6, we formulate the attention computation in recursive fashion. In Section D.6, we briefly review our eviction policy, which gives the formal version of Definition 4.3. In Section D.7, we discuss the diminishing return for submodular. In Section D.8, we discuss the high-level ideas for submodular. In Section D.9, we analyze the robust greedy algorithm error propagation. In Section D.10, we explain how to add items into sets via approximate function. In Section D.11, we provide several definitions related to dynamic properties. In Section D.12, we prove an induction lemma for the exact function. In Section D.13, we prove an induction lemma for the approximation function. In Section D.14, we provide theoretical guarantees for both the full-knowledge version (formal version of Lemma 3.1) and the limited-cache-size version (formal version of Theorem 4.4). In Section D.15, we provide a more detailed discussion of theoretical work about attention computation and regression-related problems. In Section D.16, we provide a mathematical formulation for sparsity preserving. In Section D.17, we provide the definition of loss function which can potentially generate sparse (heavy hitter type attention sore). In Section D.18, we explain how to compute the gradient of the loss function. In Section D.19, we show how to compute the Hessian of the loss function. In Section D.20, we show that Hessian is positive definite. In Section D.21, we prove the Lipschitz property for the Hessian matrix. In Section D.22, we show that using a gradient-type algorithm is sufficient to optimize that (heavy hitter type) loss function.

### Notations

For a positive integer \(n\), let \([n]:=\{1,2,,n\}\).

For a vector \(x^{n}\), let \(^{n}\) denote the vector with the \(i\)-th entry being \(}\) and \((x)^{n n}\) denote the diagonal matrix with the \(i\)-th diagonal entry being \(x_{i}\). For two matrices \(A,W^{n n}\), let \(\|A\|_{W}:=(_{i=1}^{n}_{j=1}^{n}W_{i,j}A_{i,j}^{2})^{1/2}\) and \(W A\) denote the matrix where \((W A)_{i,j}=W_{i,j}A_{i,j}\). For matrix \(W^{n n}\), let \(D_{W_{i}}:=(W_{i,:})\) with \(i[n]\).

For two vectors \(x^{n}\) and \(w_{ 0}^{n}\), let \(\|x\|_{w}:=(_{i=1}^{n}w_{i}x_{i}^{2})^{1/2}\). For a vector \(x\), its \(_{2}\) norm is defined as \(\|x\|_{2}:=(_{i=1}^{n}x_{i}^{2})^{1/2}\) and its \(_{p}\) norm is defined as \(\|x\|_{p}:=(_{i=1}^{n}|x_{i}|^{p})^{1/p}\). For a square matrix \(A\), we denote \([A]\) as the trace of matrix \(A\).

For a matrix \(A^{n k}\) (suppose \(n k\)), we use \(\|A\|\) to denote its spectral norm, i.e., \(\|A\|=_{x}\|Ax\|_{2}/\|x\|_{2}\). We use \(\|A\|_{F}\) to denote its Frobenius norm \(\|A\|_{F}:=(_{i=1}^{n}_{j=1}^{k}A_{i,j}^{2})^{1/2}\).

Suppose matrix \(A^{n k}\) has SVD decomposition \(U V^{}\) where \(U^{n k}\) (this matrix has orthonormal columns), \(^{k k}\) is a diagonal matrix, and \(V^{k k}\). We call columns of \(U\) singular vectors. We use \(A^{}^{k n}\) to denote the Moore-Penrose pseudoinverse, then \(A^{}=V^{-1}U^{}\). Suppose \(^{k k}\) is sorted diagonal matrix, let \(_{1},,_{k}\) denote the diagonal entries of \(\). Then we call \(_{i}\) the \(i\)-th singular value of the matrix, and we write it as \(_{i}(A)\).

For any symmetric matrix \(B^{k k}\), we denote its eigenvalue decomposition as \(U U^{}\), where \(\) is a diagonal matrix. Let \(_{1},,_{k}\) denote the entries on diagonal of \(^{k k}\). We say \(_{i}\) is the \(i\)-th eigenvalue. Usually, we write it as \(_{i}(B)\).

The connection between eigenvalues and singular values is

\[_{i}^{2}(A)=_{i}(A^{}A)\]

We use the notation \(A 0\) to denote that matrix \(A\) is positive semidefinite (psd). Mathematically, \(A 0\) means for all vectors \(x\), we have \(x^{}Ax 0\).

Similarly, for two squarer matrices \(A\) and \(B\), we use \(A B\) to denote the case where for all vectors \(x\), \(x^{}Ax x^{}Bx\).

Let \([]\) and \([]\) denote the probability and expectation. We define the maximum between \(a\) and \(b\) as \(\{a,b\}\). We denote \(\{a,b\}\) (resp. \(\{a,b\}\)) as the minimum (reps. maximum) between \(a\) and \(b\).

Throughout, for non-negative real numbers a and b, we use the notation \(a=(1)b\) if \(a[(1-)b,(1+)b]\).

### Submodular

We provide the standard definition of the submodular function.

**Definition D.1** (Submodular function ).: _For a finite set \(\), a submodular function is defined as \(f:2^{}\), where \(2^{}\) denotes the power set of \(\). It is characterized by the fulfillment of any of the following equivalent criteria:_

* _Condition 1._
* _For_ \(S,T,\) _with_ \(S T\) _and every_ \(a T\) _we have that_ \(f(S\{a\})-f(S) f(T\{a\})-f(T)\)__
* _Condition 2._
* _For every_ \(S,T\) _we have that_ \(f(S)+f(T) f(S T)+f(S T)\)_._
* _Condition 3._
* _For every_ \(S\) _and_ \(a_{1},a_{2} S\) _such that_ \(a_{1} a_{2}\) _we have that_ \(f(S\{a_{1}\})+f(S\{a_{2}\}) f(S\{a_{1},a_{2}\})+f(S)\)_._

For convenience of discussion, in this paper, we always choose \(=[n]\) when we want to discuss the submodular function.

Next, we provide some examples/types of submodular functions. One important class is called monotone,

**Definition D.2** (Monotone).: _A set function \(f\) is monotone if for every \(T S\) we have \(f(T) f(S)\)._

Here are a number of monotone submodular functions

* Linear (Modular) functions
* A linear function can be represented as \(f(S)=_{i S}w_{i}\). If all the weights \(w_{i}\) are nonnegative, then the function \(f\) is considered monotone.
* Budget-additive functions
* A budget-additive function has the form \(f(S)= B,_{i S}w_{i}\) where each weight \(w_{i}\) and the budget \(B\) are nonnegative.
* Coverage functions
* We have a set \(=\{E_{1},E_{2},,E_{n}\}\) where each \(E_{i}\) is a subset of a broader set \(^{}\). The coverage function can be expressed as \(f(S)=|_{E_{i} S}E_{i}|\) for any \(S\). This function can be generalized by assigning non-negative weights to the elements.

### Dynamic Submodular

The standard submodular function is mapping \(2^{[n]}\) to real. Here we need a more general definition that maps \(2^{[n]} 2^{[n]}\) to real.

**Definition D.3** (Strong submodular).: _We define function \(F:2^{[n]} 2^{[n]}\), then for any set \(Z[n]\), we assume that \(F(Z,):2^{[n]}\) is a submodular function._

In fact, the above definition is stronger than we want, what we really need can be written as follows. We remark that in Definition 4.1 we provide an informal definition. Here we provide a more detailed version of the definition.

**Definition D.4** (Dynamic submodular framework, formal version of Definition 4.1).: _Define function_

\[F:2^{[n]}[n] 2^{[n]}.\]

_Then for any index \(i[n]\), any set \(Z[i-1]\), we assume that_

\[F(Z,i,):2^{[n]}\]

_is a submodular function w.r.t. to \(Z\), i.e.,_

* _For all sets_ \(X,Y[n]\) _satisfy that_ \(Z X Y\)_,_
* _For all element_ \(x[n]\) _satisfy that_ \(x[n] Y\)_,_

_we have_

\[f_{Z,i}(X\{x\})-f_{Z,i}(X) f_{Z,i}(Y\{x\})-f_{Z,i}(Y),\]

_where \(f_{Z,i}():=F(Z,i,)\)._

**Remark D.5**.: _We remark that Definition D.4 is a weaker version of Definition D.3. We also like to mention that the informal definition (see Definition 4.1) only contains two input parameters, but in fact we need three input parameters (see Definition D.4)._

_In the later, when we use \(f_{Z,i}()\), we will replace \(Z\) by \(S_{i}\) for convenient of analysis, for example see Definition D.23, Definition D.24, Definition D.25 and Definition D.26._

### Static Attention

Before we describe the recursive attention computation, we will first describe the static version of attention computation as follows (for examples, see Definition 1.1 in  and others ):

**Definition D.6**.: _Given three matrices \(Q,K,V^{d d}\), the goal is to compute_

\[(Q,K,V):=D^{-1}A V\]

_where square matrix \(A^{n n}\) can be rewritten as follows_

\[A=(QK^{})\]

_and diagonal matrix \(D^{n n}\) can be written as follows_

\[D=(A_{n})\]

_Here we apply \(()\) to a matrix entry-wisely to a matrix. \(_{n}\) is a length-\(n\) vector where all the entries are ones. The operator \(()\) is turning a vector into a diagonal matrix._

### Recursive Attention Definition

We first provide some definitions.

**Definition D.7**.: _Given a query matrix \(Q^{n d}\), we use \(Q_{i,*}\) to denote a length-\(d\) row vector that represents the \(i\)-th row of \(Q\) for each \(i[n]\)._

**Definition D.8**.: _Given a key matrix \(K^{n d}\), we use \(K_{ i,*}\) to denote a \(i d\) matrix that selects the first \(i\) rows from \(K\)._Next, we show the exact version of computing attention.

**Definition D.9** (Exact Version).: _Given \(Q,K^{n d}\)_

* _For each_ \(i[n]\)_, we use_ \(o_{i}^{i}\) _to denote a length-_\(i\) _vector._
* _For each_ \(j[i]\)_, we use_ \(o_{i,j}\) _to denote the_ \(j\)_-th coordinate of vector_ \(o_{i}^{i}\)__

_For the first layer, we have_

* \(o_{1,1}=1\)_._

_For the second layer, we have_

* _length-_\(2\) _vector_ \(o_{2}:=^{-1}}_{}(}_{1 d})}_{d 2}^{})\)__
* _scalar_ \(D_{2}:=(K_{ 2,*})}_{1 2}^{}) _{2}}_{2 1}\)__

_For each \(i[n]\), for the \(i\)-th layer, we have_

* _length-_\(i\) _vector_ \(o_{i}:=^{-1}}_{}(}_{1 d})}_{d i}^{})\)__
* _scalar_ \(D_{i}:=(K_{ i,*})}_{1 i}^{}) _{i}}_{i 1}\)__

Now, we show the approximate version of computing attention. Instead of computing the entire attention \(o_{i}\), we only compute the attention of the tokens that are being tracked.

**Definition D.10** (Approximate Version).: _Given \(Q,K^{n d}\)_

* _For each_ \(i[n]\)_, we use_ \(o_{i}^{i}\) _to denote a length-_\(i\) _vector._
* _For each_ \(j[i]\)_, we use_ \(o_{i,j}\) _to denote the_ \(j\)_-th coordinate of vector_ \(o_{i}^{i}\)__
* _Let_ \(k[n]\) _be the budget of the number of tokens we can track (due to the memory issue)._

Figure 14: (a) Exact version of the attention computation. Here, our example is about the language modeling task. At this stage, the model predicts the word ‘apple’ and computes the exact attention vector \(o_{4}\). (b) Approximate version of the attention computation. Let the budget of the number of tokens we can track be \(3\). We truncate the key matrix for the first token \(K_{1,*}\) and only keep \(K_{2,*}\), \(K_{3,*}\) and \(K_{4,*}\) in the memory. Compared with the exact version, we don’t compute \(o_{4,1}\).

_For the first layer, we have_

* \(o_{1,1}=1\)_._

_For the second layer, we have_

* _length-_\(2\) _vector_ \(o_{2}:=^{-1}}_{}(}_{ 1 d})^{}}_{d 2})\)__
* _scalar_ \(D_{2}:=(K_{ 2,*})^{})}_{1 2} _{2}}_{2 1}\)__

_For each \(i[n]\), for the \(i\)-th token, we have_

* _Let_ \(S_{i}[n]\) _denote the tokens we're tracking when we are predicting the_ \(i\)_-th token._
* \(|S_{i}|=k\)__
* \(|S_{i} S_{i-1}| 1\) _or equivalently_ \(|S_{i} S_{i-1}| k-1\)__
* _length-_\(i\) _vector_ \(o_{i}:=^{-1}}_{}(}_{ 1 d},*})^{}}_{d i})\)__
* _scalar_ \(D_{i}:=(K_{S_{i,*}})^{})-1_{[i]  S_{i}})}_{1 i}_{i}}_{i 1}\)__

In a certain sense, the above definition is related to finding heavy hitters in compressive sensing (for more background on compressive sensing, we refer readers to ).

### Eviction Policy

The goal of this section is to our eviction policy under space limitation. We start by giving a process without having space limitations. We denote the attention query matrix as \(Q^{n d}\) and the key matrix as \(K^{n d}\). \(Q_{i,*}\) represents the \(i\)-th row of \(Q\) and \(K_{ i,*}\) represents the first \(i\) rows of \(K\). For simplicity, \(K_{S_{i,*}}\) denotes a sub-matrix of \(K\) which selects \(S_{i}\) rows from \(K\).

**Definition D.11** (The generative process with Full knowledge).: _For each \(i[n]\), for the \(i\)-th token, we have_

* _length-_\(i\) _vector_ \(o_{i}:=D_{i}^{-1}(Q_{i,*}(K_{ i,*})^{})\)_, which denotes the attention of the_ \(i\)_-th word._
* _scalar_ \(D_{i}:=(Q_{i,*}(K_{ i,*})^{})_{i}\)_, which denotes the normalization factor._

In the above process, since we have no space limitation, then we can keep all the scores. In the following process, we show how to handle things when there is a space limitation. In this case, we need to dynamically maintain set \(S_{i}\) such that \(|S_{i}| k\) (Compared to the full knowledge case, we can think of \(|S_{i}|=i\)).

**Definition D.12** (\(_{2}\) Eviction Policy, Formal version of Definition 4.3).: _Let \(k\) denote the budget of space and \(k<n\). Let \(F_{}:2^{[n]}\) denote certain score function. Let \(S_{i-1}\) denote the source set. Let \(S_{i}\) denote the target set. We defined the eviction policy \(g:S_{i-1} S_{i}\) s.t._

* \(|S_{i}| k\) _(_\(\) _cache_ _size is not changing over the time)_
* \(|S_{i} S_{i-1}| 1\) _(we can evict at most_ \(1\) _\(\) _in the_ \(\) _cache_)_
* _We construct_ \(S_{i}(S_{i-1}\{i\})\{u\}\) _as_ \(u_{v(S_{i-1}\{i\})}F_{}(S_{i-1}\{i \}\{v\})\)__

**Remark D.13**.: _We remake that, in Definition 4.3, we introduce a simplified notation where we state that \(|S_{i}|=k\) in the first bullet point, but in general, we consider the case where \(|S_{i}| k\) for the first \(k\) tokens. To accommodate this more general scenario, we present Definition D.12, which provides a broader definition that handles the situation where \(|S_{i}| k\)._

**Remark D.14**.: _We remark the above function \(F_{}\) can have multiple choices. In the later analysis, we provide two choices. If we use the exact function, then \(F_{}\) is \(f_{S_{i},i}\) (see Definition D.23 and Definition D.25). If we use the approximate function, then \(F_{}\) if \(_{S_{i},i}\) (see Definition D.24 and Definition D.26). Let \(h:\) (the \(h\) being used Line 10 in Algorithm 2) denote function. We hope to choose \(h\) such that it gives the submodular property for the function \(F_{}\) in the sense of our dynamic framework. For example, \(h(z)\) is usually chosen to be some non-decreasing concave function such as \(\) and \((z+1)\). For simplicity, in Algorithm 1 (which is the informal version of Algorithm 2), we choose \(h(z)=z\) for the purpose of providing readers with an intuitive understanding and due to space limitation._

**Remark D.15**.: _We remark that, in Line 10 in Algorithm 2, the \(_{i,s}\) is the accumulation of attention score for the token \(s\) in set \(T\). In our Algorithm 1, we only use \(o_{s}\) in Line 10 (see Algorithm 1) to represent that accumulation of the attention score for simplicity._

_In Algorithm 2, for simplicity of the presentation, we can create multiple vectors \(_{0},_{n}\) (see Line 2, Line 9, Line 10). However, every time at \(i\)-th, we only need to use the information for \(_{i}\) which has at most length \(n\) size. Thus, a straightforward and better implementation for only using one length-\(n\) to store accumulative score can reduce \(n^{2}\) usage to \(n\)._

### Explaining Submodular Diminishing Return Property in Attention Scheme

In the standard submodular problem , we are given a ground set \([n]\) and a function \(f:2^{[n]}\). The goal is to find a set of elements \(S\) such that \(f(S)\) is maximized.

We say function \(f\) is submodular (Recall the formal definition in Definition D.1), if for every \(X,Y[n]\) with \(X Y\) and every \(x[n] Y\) we have

\[f(X\{x\})-f(X) f(Y\{x\})-f(Y)\]

Submodular functions can represent the cost of items, due to their diminishing returns property . It suggests that the increase in information obtained from selecting a candidate object, such as a word or sentence, becomes smaller as more objects have already been chosen for the summary.

For instance, we introduce a new token, denoted as "w", into two sets, \(S\) and \(S_{0}\), where the concepts covered by \(S_{0}\) are a subset of those covered by \(S\). By intuition, the information added to \(S_{0}\) by "w" should be larger compared to adding it to \(S\), as the new concepts carried by "w" might have already been covered by the concepts present in \(S\) but not in \(S_{0}\). This property is known as the diminishing return property. Hence, we propose that the neural coverage function  should exhibit a desirable characteristic called submodularity.

### Submodular: High Level Ideas

We formalize the submodular function maximization problem with cardinality constraint in this section. Informally, a set function is submodular if it has decreasing marginal increment.

**Definition D.16** (Submodular function).: _We denote \(f:2^{[n]}\) as a set function. The discrete derivative \(_{f}\) is defined as follows:_

\[_{f}(i S):=f(S\{i\})-f(S).\]

_A function \(f\) is submodular if, for any \(S T\) and \(i[n] T\), the following inequality holds:_

\[_{f}(i T)_{f}(i S).\]

For simplicity, we present the problem of maximizing a submodular function with a cardinality constraint (1). Our goal is to solve the optimization problem efficiently.

\[_{S[n]} f(S)\] (1) s.t. \[|S| k\]

Representation of \(f(S)\)One challenge in designing the algorithm is determining the representation of input instances. Since the constraint in optimization problem (1) is straightforward, we need to decide how to represent \(f(S)\). Suppose \(S=i_{1},i_{2},,i_{m}[n]\), we can decompose \(f(S)\) into a sum of increments as follows:

\[f(S)=f(S_{0})+_{j=1}^{m}(f(S_{j})-f(S_{j-1})),\] (2)

where \(S_{0}=\) and \(S_{j}=S_{j-1}+i_{j}\). Without loss of generality, we assume \(f()=0\). By the definition of \(_{f}(i|S)\), we have \(f(S_{j})-f(S_{j-1})=_{f}(i_{j}|S_{j-1})\). Therefore, the decomposition (2) can be simplified as follows:

\[f(S)=_{j=1}^{m}_{f}(i_{j}|S_{j-1})\] (3)

To introduce our advanced data structure later, we further represent \(_{f}(i|S)\) in the form of

\[_{f}(i|S)=u_{i}^{}h(S)u_{i}\] (4)

where \(u_{i}^{d}\) is a \(d\)-dimensional vector and \(h(S)^{d d}\) is a \(d\)-by-\(d\) matrix.

In practice, a significant subclass of submodular functions is the monotone submodular functions, i.e. functions \(f(A) f(B)\) for all \(A B[n]\). When \(f\) is monotone, we could restrict all \(h(S)\) to be positive semidefinite (PSD) matrixes. When the matrix \(h(S)\) is positive semidefinite (PSD), it makes us achieve a faster acceleration.

### Robust Greedy with Error Propagation

The procedure for the greedy selection algorithm initiates with empty set \(S_{0}=\). For each iteration in the main loop, the algorithm chooses the element that maximizes the marginal increment to add to the set. When the size of the set eventually reaches \(k\), the algorithm return that set \(S\). Specifically, for iteration \(t\{1,2,,k\}\), we let

\[S_{t} S_{t-1}\{j_{t}\}\]

where the element in the singleton is \(j_{t}=*{arg\,max}_{j_{t-1}}f(S_{t-1}\{j\})\). The greedy strategy is effective in the sense that the approximation error of it is \(1-1/e\).

**Theorem D.17** ().: _For a monotone submodular function \(f\), the greedy algorithm (Algorithm 3) guarantees to output a set \(S\) satisfying_

\[f(S)(1-1/e)_{|T|=k}\{f(T)\}.\]

**Corollary D.18** ().: _Given_* _accuracy parameter_ \(>0\)_._
* _integer_ \(k 1\)__
* _Let_ \(O\) _denote an oracle that takes an arbitrary set_ \(S[n]\) _and_ \(i[n] S\)_, returns a value_ \(O(S,i)\) _with guarantee that_ \((i|S)- O(S,i)(i|S)+\)_._
* _Let_ \(A\) _denote an algorithm that each time step_ \(t=1,2,,k\)_, it selects_ \(j_{t}=_{j}\{O(S_{t-1},j)\}\) _and lets_ \(S_{t} S_{t-1}\{j_{t}\}\)_._

_Then this algorithm \(A\)_

* _returns a set_ \(S_{k}\)__
* _the_ \(S_{k}\) _satisfy that_ \[f(S_{k})(1-1/e)_{|T|=k}\{f(T)\}-k(2-1/e).\]

### Robust Submodular and Adding Items

We first propose a lemma that shows the robustness of the submodular.

**Lemma D.19**.: _Suppose that for each \(i[n]\), we have_

* _for all_ \(S_{i}[n]\) _with_ \(|S_{i}| k\)__
* _for all_ \(X[n]\)_,_ \(X S_{i}\{i\}\) _and_ \(|X S_{i}||S_{i}|\)_,_ \[|(_{S_{i},i}(X))-_{S_{i},i}(S_{i})-(f_{S_{i},i}(X)- f_{S_{i},i}(S_{i}))|/(2n)\]

_Let \(_{i}\) denote the optimal cost that can be achieved by using \(f\) in \(i\)-th iteration. If the greedy algorithm use \(f\) to find the solution has performance at least \((1-1/e)_{i}\), then using \(\), we can obtain a solution that has performance at least \((1-1/e)_{i}-\)_

Proof.: The proof follows from Lemma D.18. 

Next, we explain how to add items into sets based on exact values.

**Definition D.20** (Expanding items based on exact value).: _If the following conditions hold_

* _Let_ \(S_{i}[i-1]\)_._
* _Let_ \(f_{S_{i},i}:2^{[i]}\)_._

_Then, we can define \(S_{i+1}\) as follows_

* _If_ \(|S_{i}|=k\) _then_ \(S_{i+1}=S_{i}\{i\} u\) _where_ \(u=_{v S_{i}\{i\}}f_{S_{i},i}(S_{i}\{i\} v)-f_{S_{ i},i}(S_{i})\)__
* _If_ \(|S_{i}|<k\)_, then_ \(S_{i+1}=S_{i}\{i\}\)_._

**Remark D.21**.: _We remark that \(u=_{v S_{i}\{i\}}f_{S_{i},i}(S_{i}\{i\} v)-f_{S_{i},i }(S_{i})\) is the same as \(u=_{v S_{i}\{i\}}f_{S_{i},i}(S_{i}\{i\} v)\). For the convenience of discussion and analysis, we will switch to using both cases in different places._

Here, we explain how to add items into sets via approximate values.

**Definition D.22** (Expanding items based on approximate value).: _If the following conditions hold_

* _Let_ \(S_{i}[i-1]\)_._
* _Let_ \(_{S_{i},i}:2^{[i]}\)_._

_Then, we can define \(S_{i+1}\) as follows_

* _If_ \(|S_{i}|=k\) _then_ \(S_{i+1}=S_{i}\{i\} u\) _where_ \(u=_{v S_{i}\{i\}}_{S_{i},i}(S_{i}\{i\} v )-_{S_{i},i}(S_{i})\)__
* _If_ \(|S_{i}|<k\)_, then_ \(S_{i+1}=S_{i}\{i\}\)_._

### Universal Conditions

We state several definitions for both the exact function and approximation function.

**Definition D.23** (Universal Monotone Condition (for exact function)).: _We say \(f\) has universal monotone condition, if for all \(i[n]\) for all \(S_{i}[i-1]\), we have_

\[f_{S_{i},i}(X) f_{S_{i},i}(Y),\ \ \  Y X\]

**Definition D.24** (Universal Monotone Condition (for approximate function)).: _We say \(\) has universal monotone condition, if for all \(i[n]\) for all \(S_{i}[i-1]\), we have_

\[_{S_{i},i}(X)_{S_{i},i}(Y),\ \ \  Y X\]

**Definition D.25** (Universal Dynamic Condition 1(for exact function)).: _We say \(f\) has universal dynamic condition 1(for exact function), if for all \(i[n]\) for all \(S_{i}[i-1]\), \(S_{i-1}[i-2]\), \(|S_{i} S_{i-1}| 1\), we have_

\[f_{S_{i},i}(S_{i})(1-) f_{S_{i-1},i-1}(S_{i})\]

**Definition D.26** (Universal Dynamic Condition 1 (for approximate function)).: _We say \(\) has universal dynamic condition 1(for approximate function), if for all \(i[n]\) for all \(S_{i}[i-1]\), \(S_{i-1}[i-2]\), \(|S_{i} S_{i-1}| 1\), we have_

\[_{S_{i},i}(S_{i})(1-)_{S_{i-1},i-1}( S_{i})\]

We define \(_{i}\) as follows:

**Definition D.27**.: _Let \(k\) denote the budget length. For each \(i[n]\), we define \(_{i}\) as_

\[\{f_{X,i}(Y) X[i-1],Y[i],|X| k,|Y| k,|Y  X| 1\}.\]

**Definition D.28** (Universal Dynamic Condition 2).: _For \(i[n]\), we define \(_{i}\) as Definition D.27. We say it has universal dynamic condition if for each \(i[n]\), we have_

\[_{i}(1-)_{i-1}.\]

### Induction Lemma for Exact Function

The goal of this section is to prove Lemma D.29

**Lemma D.29** (Induction Lemma).: _For a fixed \(i\), suppose the following conditions hold_

* **Set Condition.**__\(S_{i}[i-1]\)__
* **Budget Condition.**__\(|S_{i}| k\)__* **Value Condition.**\(f_{S_{i-1},i-1}(S_{i})(1-1/e)(1-)^{i}(1-)^{i}_{i}\)__
* **Universal Dynamic Condition 1 (for exact function).**_(See Definition D.25)_\(f_{S_{i},i}(S_{i})(1-) f_{S_{i-1},i-1}(S_{i})\)__
* **Universal Dynamic Condition 2.**_(See Definition D.28)_\(_{i}(1-)_{i+1}\)__
* **Universal Monotone Condition (for exact function).**_(See Definition D.23)_\(f_{S_{i},i}(X) f_{S_{i},i}(Y)\) _for all_ \(Y X\)__

_Then if we construct \(S_{i+1}\) as Definition D.20, then we have_

* **Set Condition.**\(S_{i+1}[i]\)__
* **Budget Condition.**\(|S_{i+1}| k\)__
* **Value Condition.**\(f_{S_{i},i}(S_{i+1})(1-1/e)(1-)^{i+1}(1-)^{i+1}_{i+1}\)__

Proof.: **Proof of Set Condition.**

Note that \(S_{i}[i-1]\), by using the way we construct \(S_{i+1}\), then it is obvious that \(S_{i+1}[i]\).

**Proof of Budget Condition.**

Note that \(|S_{i}| k\), by using the way we construct \(S_{i+1}\), then it is straightforward that \(|S_{i+1}| k\).

**Proof of Value Condition.**

We can show that

\[f_{S_{i},i}(S_{i+1})  f_{S_{i},i}(S_{i})\] \[(1-) f_{S_{i-1},i-1}(S_{i})\] \[(1-)((1-1/e)(1-)^{i}(1-)^{i}_{i}-i_{0})\] \[(1-)((1-1/e)(1-)^{i}(1-)^{i+1} _{i+1})\] \[(1-1/e)(1-)^{i+1}(1-)^{i+1}_{i +1}.\]

where the first step follows from **Universal Monotone Condition**, the second step follows from **Universal Dynamic Condition 1**, the third step follows from **Value Condition**, the forth step follows from **Universal Dynamic Condition 2**, and the last step follows from simple algebra.

Thus, we complete the proof.

### Induction Lemma for Approximate Function

The goal of this section is to prove Lemma D.30

**Lemma D.30** (Induction Lemma).: _For a fixed \(i\), suppose the following conditions hold_

* **Set Condition.**\(S_{i}[i-1]\)__
* **Budget Condition.**\(|S_{i}| k\)__
* **Value Condition.**\(f_{S_{i-1},i-1}(S_{i})(1-1/e)(1-)^{i}(1-)^{i}_{i}-i_{0}\)__
* **Universal Dynamic Condition 1 (for approximate function).**_(see Definition D.26)_\(_{S_{i},i}(S_{i})(1-)_{S_{i-1},i-1}(S_{i})\)__
* **Universal Dynamic Condition 2.**\(_{i}(1-)_{i+1}\)__
* **Universal Approximate Condition.**_(See Definition D.28)_\(f_{S_{i},i}(X)_{S_{i},i}(X)-_{0}\) _for all_ \(X\)__
* **Universal Monotone Condition (for approximate function).**_(see Definition D.24)_\(_{S_{i},i}(X)_{S_{i},i}(Y)\) _for all_ \(Y X\)__

_Then if we construct \(S_{i+1}\) as Definition D.22, then we have_* **Set Condition.**\(S_{i+1}[i]\)__
* **Budget Condition.**\(|S_{i+1}| k\)__
* **Value Condition.**\(f_{S_{i},i}(S_{i+1})(1-1/e)(1-)^{i+1}(1-)^{i+1}_{i+1}-(i+1)_{0}\)__

Proof.: **Proof of Set Condition.**

Note that \(S_{i}[i-1]\), by using the way we construct \(S_{i+1}\), then it is obvious that \(S_{i+1}[i]\).

**Proof of Budget Condition.**

Note that \(|S_{i}| k\), by using the way we construct \(S_{i+1}\), then it is straightforward that \(|S_{i+1}| k\).

**Proof of Value Condition.**

We can show that

\[f_{S_{i},i}(S_{i+1}) _{S_{i},i}(S_{i+1})-_{0}\] \[_{S_{i},i}(S_{i})-_{0}\] \[(1-)_{S_{i-1},i-1}(S_{i})-_{0}\] \[(1-)((1-1/e)(1-)^{i}(1-)^{i}_{i}-i_{0})-_{0}\] \[(1-)((1-1/e)(1-)^{i}(1-)^{i+1} _{i+1}-i_{0})-_{0}\] \[(1-1/e)(1-)^{i+1}(1-)^{i+1}_{i+ 1}-(i+1)_{0}.\]

where the first step follows from **Universal Approximate Condition**, the second step follows from **Universal Monotone Condition**, the third step follows from **Universal Dynamic Condition 1**, the forth step follows from **Value Condition**, the fifth step follows from **Universal Dynamic Condition 2**, and the last step follows from simple algebra.

Thus, we complete the proof. 

### Theoretical Result

We first give the guarantee of our full-knowledge version (without cache size limitation).

**Lemma D.31** (Formal version of Lemma 3.1).: _Under the mild assumption, let \(k\) denote any target size. If we greedily compute the attention score based on full information, then we can find the set \(S_{i}\) such that_

\[f(S_{i})(1-1/e)(1-)_{i},\]

_where \((0,1)\) are parameters._

Proof.: The proof follows from using Theorem D.17, Corollary D.18, Lemma D.29 with choosing \(==/(10n)\). 

Next, we show the guarantee for our robust and approximate greedy eviction policy algorithm (Algorithm 2).

**Theorem D.32** (Formal version of Theorem 4.4).: _Under the mild assumption, let \(k\) denote the budget of space limitation. If for each token, we greedily compute the attention score based on top-\(k\) choice, then we can show the set \(_{i}\) we generate each for token \(i[n]\) satisfy that_

\[f(_{i})(1-1/e)(1-)_{i}-,\]

_where \((0,1),>0\) are parameters._

Proof.: The proof follows from using Theorem D.17, Corollary D.18, Lemma D.30 with choosing \(_{0}=/(10n)\) and \(==/(10n)\).

### Extended Related Work for Theoretical Attention Problems

The static attention computation is asking the following question that given \(Q,K,V^{n d}\), the goal is to \(D^{-1}(QK^{})V\) where \(D=((QK^{})_{n})\).  studied the static attention computation from both algorithm and hardness. On the positive, they provide an almost linear time algorithm to approximately compute the attention matrix. On the negative side, assuming a strong exponential time hypothesis (SETH), they prove a hardness result. Their hardness result is, unless SETH fails, there is no algorithm that runs in truly subquadratic time to approximately compute the attention matrix. Further,  considers the dynamic of attention computation problem. They also provide both algorithmic results and hardness results. In the work of , they consider the sparsification of attention matrix construction. In particular, they assume that situation that \(d n\), and show how to sparsify the columns of matrix \(Q\).  provides two algorithms, one is a randomized algorithm, and the other is a deterministic algorithm. Differential privacy is a famous and textbook topic in graduate school, recently the work of  shows how to give a differentially private algorithm for computing the attention matrix. For a given \(A^{n d}\) and vector \(b^{n}\),  formulates and studies exponential regression \(_{x}\|(Ax)-b\|_{2}\). Then  considers the normalization factor in exponential regression and defines the softmax regression problem \(_{x}\|(Ax),_{n}^{-1}(Ax)-b\|_{2}\).  moves the scaling factor from \((Ax)\) to \(b\) and defines a rescaled softmax regression problem \(_{x}\|(Ax)-(Ax),_{n} b\|_{2}\).

### Sparsity Preserving

Recall that in Figure 2, we observe that even when trained densely, the attention matrices of LLMs are over 95% sparse at inference time. Only 5% of the KV cache is sufficient for decoding the same output token at each generation step. Here, we provide some formal formulations for sparsity.

**Definition D.33**.: _Suppose the following conditions_

* _Let_ \(S_{0}[m]\)_._
* _Let_ \(k=|S_{0}|\)_._
* _Let_ \((0,1)\) _denote a threshold for truncating the value._
* _Let_ \((0,1)\) _denote a fraction of mass (larger than_ \(\)_) outside_ \(S_{0}\)_._
* _Let mapping_ \(:^{d}^{m}_{ 0}\)_._
* _For each_ \(x^{d}\)_,_ \((x)^{m}\) _is a vector that has length_ \(m\)_._

_We say the distribution \(\) is \((,,k)\)-good if the following conditions hold_

* _For all_ \(x^{d}\)_,_ \(S_{0}_{}((x))\)__
* _For all_ \(x^{d}\)_,_ \(|_{}((x)) S_{0}| k\)__

**Claim D.34**.: _Suppose we sample \(n\) points \(\{x_{1},x_{2},,x_{n}\}^{d}\) from \((,,k)\)-good distribution uniformly at random, then we have_

* \(S_{0}_{i[n]}_{}(x_{i})\)__
* \(|(_{i[n]}_{}((x))) S_{0}|  kn\)__

Proof.: Since for all \(i[n]\), we have \(S_{0}_{}((x_{i}))\), thus

\(S_{0}_{i[n]}_{}(x_{i})\).

Therefore we proved the first property.

We know that for all \(i[n]\), we have \(|_{}((x)) S_{0}| kn\). Thus

\[|(_{i[n]}_{}((x_{i}))) S_ {0}|_{i=1}^{n}|_{}((x_{i})))  S_{0}| n k\]

Therefore, we finish the proof for the second property.

### Definition of Loss Function

In this section, we follow the theoretical softmax regression literature  and define a number of functions to make the calculations of gradient and Hessian convenient. We also proposed a new penalty term (\(_{1}\) type sparsity penalty, see Definition D.41) into the final loss function, which is not studied in previous work . We first provide some function definitions.

**Definition D.35** (Function \(u\), ).: _Given matrix \(A^{n d}\), let function \(u:^{d}^{n}\) be defined as follows_

\[u(x):=(Ax)\]

**Definition D.36** (Function \(\), see Definition 5.4 in  as an example).: _We define \(u(x)\) as Definition D.35. Then we define \(:^{d}\) as follows_

\[(x):= u(x),_{n}\]

**Definition D.37** (Function \(f\), see Definition 5.1 in  as an example).: _Provided that the following conditions are true_

* _We define_ \(u(x)\) _as Definition D.35._
* _We define_ \((x)\) _as Definition D.36_

_Let function \(f:^{d}^{n}\) be defined as follows_

\[f(x):=(x)^{-1}u(x).\]

**Definition D.38** (Function \(c\), see Definition 5.5 in  as an example).: _Provided that the following conditions are true_

* _Given a vector_ \(b^{n}\)_._
* _Let_ \(f(x)\) _be defined as Definition D.38._

_Then, let function \(c:^{d}^{n}\) defined as follows_

* \(c(x):=f(x)-b\)_._

**Definition D.39** (Loss function \(L_{}\), see Definition 5.3 in  as an example).: _We define \(L_{}:^{d}\)_

\[L_{}(x):=0.5\|c(x)\|_{2}^{2}.\]

**Definition D.40** (Loss function \(L_{}\)).: _Given \(A^{n d}\)._

_Let function \(L_{}:^{d}\) be defined as follows_

\[L_{}(x):=0.5\|(w)Ax\|_{2}^{2}\]

We define a novel penalty function

**Definition D.41** (Implicitly controlling the sparsity).: _Given \(A^{n d}\)._

_We define_

\[L_{}(x):=\|(Ax)\|_{1}.\]

Then it is obvious that we have

**Claim D.42**.: _Given \(A^{n d}\). Let \(u\) be defined as Definition D.36._

_We have_

* \(L_{}(x)=(Ax),_{n}\)__
* \(L_{}(x)= u(x),_{n}\)__* \(L_{}(x)=(x)\)__

Proof.: The proof is trivially following from the definition of \(u(x)\) (see Definition D.35) and \((x)\) (see Definition D.36). 

The final loss function can be defined as follows. Intuitively, we can write attention \(D^{-1}(QK^{})\) into \(n\) subproblems where each subproblem can be viewed as one softmax problem.

**Definition D.43**.: _If the following conditions hold_

* _We define_ \(L_{}\) _as Definition D.39._
* _We define_ \(L_{}\) _as Definition D.40._
* _We define_ \(L_{}\) _as Definition D.41._

_Then we define \(L\) function_

\[L(x):=L_{}(x)+L_{}(x)+L_{}(x).\]

### Gradient

Next, we show the gradient of \(L_{}\).

**Lemma D.44** (Gradient, Lemma 5.6 in ).: _Provided that the following conditions are true_

* _Given matrix_ \(A^{n d}\) _and a vector_ \(b^{n}\)_._
* _Let_ \(A_{*,i}^{n}\) _denote the_ \(i\)_-th column of matrix_ \(A\)_, for every_ \(i[d]\)_._
* _We define_ \((x)\) _as Definition D.36._
* _We define_ \(f(x)\) _as Definition D.37._
* _We define_ \(c(x)\) _as Definition D.38._
* _We define_ \(L_{}(x)\) _as Definition D.39._
* _Let_ \(\) _denote hadamard product._

_For every \(i[d]\), we have_

* _Part 1._

\[(Ax)}{x_{i}}=(Ax) A_{*,i}\]
* _Part 2._

\[(Ax),_{n}}{x_{i}}= (Ax),A_{*,i}\]
* _Part 3._

\[(x)^{-1}}{x_{i}}=-(x)^{-1} f (x),A_{*,i}\]
* _Part 4._

\[f(x)}{x_{i}}=c(x)}{x_{i}} = - f(x),A_{*,i} f(x)+f(x) A_{*,i}\]
* _Part 5._

### Hessian

Here, we compute the Hessian for several functions.

**Lemma D.45** (Hessian of \(u(x)\), Lemma 5.9 in ).: _Provided that the following conditions are true_

* _Given a matrix_ \(A^{n d}\)_._
* _For every_ \(i[d]\)_, let_ \(A_{*,i}^{n}\) _denote the_ \(i\)_-th column of matrix_ \(A\)_._
* _Let_ \(\) _denote hadamard product._

_Then, we have, for each \(i[d]\)_

* _Part 1._ \[^{2}(Ax)}{x_{i}^{2}}=A_{*,i} u(x) A_{*,i}\]
* _Part 2._ \[^{2}(Ax)}{x_{i}x_{j}}=A_{*,j} u(x ) A_{*,i}\]

**Lemma D.46** (Lemma 5.10 in ).: _Provided that the following conditions are true_

* _We define_ \((x)\) _as Definition D.36._
* _For every_ \(i[d]\)_, let_ \(A_{*,i}^{n}\) _denote the_ \(i\)_-th column of matrix_ \(A\)_._
* _Let_ \(\) _denote hadamard product._

_Then, we have_

* _Part 1._ \[^{2}(x)}{x_{i}^{2}}= u(x),A_{*,i} A _{*,i}\]
* _Part 2._ \[^{2}(x)}{x_{i}x_{j}}= u(x),A_{ *,i} A_{*,j}\]
* _Part 3._ \[^{2}(x)}{x^{2}}=A^{}(u(x ))A\]

### Hessian is Positive Definite

It is well known that in literature , the Hessian \(H\) of loss function can be written as \(A^{}(B(x)+W^{2})A\) for some matrix function \(B(x)^{n n}\) (for example see explanation in Section 5.10 in ). In this section, we show that Hessian is positive definite.

**Lemma D.47**.: _If the following conditions hold_

* _Given matrix_ \(A^{n d}\)_._
* _We define_ \(L_{}(x)\) _as Definition D.41._
* _We define_ \(L_{}(x)\) _Definition D.40._
* _We define_ \(L_{}(x)\) _Definition D.39._
* _Let_ \(L(x)=L_{}(x)+L_{}(x)+L_{}(x)\)* _Let_ \(A^{}(B(x)+W^{2})A\) _be the Hessian of_ \(L(x)\)__
* _Let_ \(W=(w)^{n n}\)_. Let_ \(W^{2}^{n n}\) _denote the matrix that_ \(i\)_-th diagonal entry is_ \(w_{i,i}^{2}\)_._
* _Let_ \(_{}(A)\) _denote the minimum singular value of_ \(A\)_._
* _Let_ \(l>0\) _denote a scalar._

_Then, we have_

* _Part 1. If all_ \(i[n]\)_,_ \(w_{i}^{2} 20+l/_{}(A)^{2}\)_, then_ \[^{2}L}{x^{2}} l I_{d}\]
* _Part 2 If all_ \(i[n]\)_,_ \(w_{i}^{2} 200(R^{2})+l/_{}(A)^{2}\)_, then_ \[(1-1/10)(B(x)+W^{2}) W^{2}(1+1/10)(B(x)+W^{2})\]

Proof.: The entire proof framework follows from , in the next few paragraphs, we mainly explain the difference.

The \(B(x)\) based on \(L_{}\) is \((u(x))\). Note that it is obvious that

\[(u(x)) 0.\]

From the upper bound size, we know that

\[(u(x)) \|u(x)\|_{} I_{n}\] \[(R^{2})\]

where the last step follows from Proof of Part 0 in Lemma 7.2 in .

To prove Part 1, following from , we only use the lower bound of \((u(x))\). By putting things together, we get our results.

To prove Part 2, we follow from  and use both the upper bound and lower bound of \((u(x))\).

### Hessian is Lipschitz

In this section, we show Hessian is Lipschitz.

**Lemma D.48**.: _If the following conditions hold_

* _Let_ \(H(x)=A^{}(B(x)+W^{2})A\) _denote the Hessian of_ \(L\)_._

_Then, we have_

* \(\|H(x)-H(y)\| n^{2}(40R^{2})\|x-y\|_{2}\)__

Proof.: The entire proof framework follows from , in the next few paragraphs, we mainly explain the difference.

Note that the \(B(x)\) based on \(L_{}+L_{}\) have been proved by 

We only need to prove \(B(x)\) based on \(L_{}\) and add them together.

Note that \(B(x)\) based on \(L_{}\) is in fact \((u(x))\).

Using Lemma 7.2 in , we know that

\[\|(u(x))-(u(y))\| \|u(x)-u(y)\|_{2}\] \[ 2R(R^{2})\|x-y\|_{2}\]

where the last step follows from Part 1 in Lemma 7.2 in .

Thus, putting things together, we complete the proof.

### Greedy Type Algorithm

In this section, we propose a greedy-type algorithm (based on the approximate Newton method) to solve the optimization problem.

```
1:procedureOurIterativeMethod(\(A^{n d},b^{n},w^{n},,\))
2: Initialize \(x_{0}\)
3:\(T(\|x_{0}-x^{*}\|_{2}/)\)\(\) Let \(T\) denote the number of iterations.
4:for\(t=0 T\)do
5:\(D B_{}(x_{t})+(w w)\)
6:\((D,A,_{1}=(1),_ {1}=/T)\)
7: Compute gradient \(g\) exactly
8: Get the approximate Hessian \(\) by computing \(A^{}A\)
9: Update \(x_{t+1}\) by using the Newton step \(x_{t}+^{-1}g\)
10:endfor
11:\( x_{T+1}\)
12:return\(\)
13:endprocedure ```

**Algorithm 4** A greedy type algorithm.

**Theorem D.49**.: _Given matrix \(A^{n d}\), \(b^{n}\), and \(w^{n}\)._

* _We use_ \(x^{*}\) _to denote the optimal solution of_ \[_{x^{d}}L_{}+L_{}+L_{}\] _that_
* \(g(x^{*})=_{d}\)_, where_ \(g\) _denotes the gradient function._
* \(\|x^{*}\|_{2} R\)_._
* _Suppose that_ \(R 10\)_,_ \(M=((R^{2}+ n))\)_, and_ \(l>0\)_._
* _Assume that_ \(\|A\| R\)_. Here_ \(\|A\|\) _denotes the spectral norm of matrix_ \(A\)_._
* _Suppose that_ \(b_{n}\) _and_ \(\|b\|_{1} 1\)_. Here_ \(_{n}\) _denotes a length-_\(n\) _vector where all the entries are zeros. (Here_ \(b_{n}\) _denotes_ \(b_{i} 0\) _for all_ \(i[n]\)_)_
* _Assume that_ \(w_{i}^{2} 200(R^{2})+l/_{}(A)^{2}\) _for all_ \(i[n]\)_. Here_ \(_{}(A)\) _denotes the smallest singular value of matrix_ \(A\)_._
* _Let_ \(x_{0}\) _denote an starting/initial point such that_ \(M\|x_{0}-x^{*}\|_{2} 0.1l\)_._
* _We use to_ \((0,0.1)\) _represent our accuracy parameter._
* _We use_ \((0,0.1)\) _to represent failure probability._

_There is a randomized algorithm that_

* _runs_ \((\|x_{0}-x^{*}\|_{2}/)\) _iterations_
* _spend_ \[O(((A)+d^{})((n/))\] _time per iteration,_
* _and finally outputs a vector_ \(^{d}\) _such that_ \[[\|-x^{*}\|_{2}] 1-.\]
Proof.: The proof framework follows from approximate Newton (second order method) literature .

Following from Lemma D.47, we know the Hessian of the loss function is positive definite.

Following from Lemma D.48, we know the Hessian of \(L\) is Lipschitz.

Following Section 9 in , by running Algorithm 4, we complete the proof.

We remark that \(\) denotes the exponent of matrix multiplication (i.e., \(n^{}\) is the time of multiplying an \(n n\) matrix with another \(n n\) matrix). The most naive algorithm gives \(=3\). Currently, the state-of-the-art algorithm gives \(=2.373\).