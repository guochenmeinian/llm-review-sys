# MHSA-based model.

Improving Visual Prompt Tuning by Gaussian Neighborhood Minimization for Long-Tailed Visual Recognition

 Mengke Li

Guangming Laboratory

Shenzhen, China

limengke@gml.ac.cn &Ye Liu

Guangming Laboratory

Shenzhen, China

zbdly226@gmail.com &Yang Lu

Xiamen University

Xiamen, China

luyang@xmu.edu.cn &Yiqun Zhang

Guangdong University of Technology

Guangzhou, China

yqzhang@gdut.edu.cn &Yiu-ming Cheung

Hong Kong Baptist University

Hong Kong SAR, China

ymc@comp.hkbu.ed &Hui Huang

Shenzhen University

Shenzhen, China

hhzhiyan@gmail.com

Corresponding author.

###### Abstract

Long-tailed visual recognition has received increasing attention recently. Despite fine-tuning techniques represented by visual prompt tuning (VPT) achieving substantial performance improvement by leveraging pre-trained knowledge, models still exhibit unsatisfactory generalization performance on tail classes. To address this issue, we propose a novel optimization strategy called Gaussian neighborhood minimization prompt tuning (GNM-PT), for VPT to address the long-tail learning problem. We introduce a novel Gaussian neighborhood loss, which provides a tight upper bound on the loss function of data distribution, facilitating a flattened loss landscape correlated to improved model generalization. Specifically, GNM-PT seeks the gradient descent direction within a random parameter neighborhood, independent of input samples, during each gradient update. Ultimately, GNM-PT enhances generalization across all classes while simultaneously reducing computational overhead. The proposed GNM-PT achieves state-of-the-art classification accuracies of 90.3%, 76.5%, and 50.1% on benchmark datasets CIFAR100-LT (IR 100), iNaturalist 2018, and Places-LT, respectively. The source code is available at https://github.com/Keke921/GNM-PT.

## 1 Introduction

Long-tailed visual recognition provides solutions to the challenges posed by the prevalent imbalance and multitude of classes in real-world data. Its training data mirror the real-world distribution, wherein a few categories (head classes) boast abundant samples, while a substantial number of categories (tail classes) exhibit very few samples, conforming to a long-tail distribution . Given its ubiquity and practicality, long-tailed visual recognition has attracted considerable attention, and numerous approaches have been proposed in recent years. Based on the data processing workflow, these methods can be broadly categorized into three types : \(1)\) data manipulation , \(2)\) representation improvement , and \(3)\) model output modification . These methods address the challenge of long-tailed learning from diverse perspectives by extending the traditional training-from-scratch approach.

Recently, leveraging the robust discriminative capabilities of pre-trained models through the integration of multi-head self-attention (MHSA) based networks [54; 11] and parameter-efficient fine-tuning (PEFT) techniques [21; 69; 63; 15; 16] has led to substantial enhancements in model performance on long-tailed data. For example, Tian et al.  introduced the text modality by CLIP  to aid in visual representation. Dong et al.  exploited visual prompt tuning (VPT) to learn class-shared and group-specific prompts for long-tailed data. These methods essentially increase model compatibility. However, even with the assistance of large-scale pre-trained knowledge, PEFT represented by VPT still exhibits inferior generalization performance on tail classes compared to head classes.

Chen et al.  emphasize that converged ViTs exhibit extremely sharp local minima, hindering their generalization , particularly for tail classes with limited samples. The imperative necessity to improve tail-class accuracy resides in advancing the generalization capability of PEFT, a facet extensively elucidated within the optimization framework . Searching for flat minima by sharpness-aware minimization (SAM)  represents a promising optimization technique to improve model performance (as shown in Figure 1). SAM first captures the sharpness of loss landscape, which correlates with the generalization gap, based on gradient directions, and then searches for flat minima. Nevertheless, SAM encounters two challenges when applied to long-tail data: 1) flat minima primarily target head classes [71; 70], and 2) it involves two sequential gradients computation.

This paper proposes a novel optimization strategy, named Gaussian neighborhood minimization prompt tuning (GNM-PT), inspired by SAM of flattening the loss landscape to enhance model generalization. Since the widespread usage, the flexibility of prompt and the suitability amount of trainable parameters for visualizing the loss landscape, we select VPT as a representative of PEFT technology to study. GNM-PT shows superior performance than SAM-based methods, particularly targeting long-tailed visual recognition tasks.

Specifically, we propose to minimize a novel Gaussian neighborhood loss named Gaussian neighborhood minimization (GNM) to obtain flat minima, substantiated by rigorous theoretical proof. GNM minimizes the mean value of the loss function within the parameter neighborhood, in contrast to the approaches of minimizing the maximum value of the parameter neighborhood employed by SAM [14; 26; 36; 43]. The mean value is a tighter upper bound than the maximum. It is evident from Figure 1(b) that GNM yields a distinctly pronounced convexity, characterized by relatively lower loss values, thereby leading to a more optimal solution. The calculation is achieved by random sampling from a normal distribution as the perturbation for the training parameters. The proposed GNM equally constrains smoothness optimization through a sample-independent perturbation without extra gradient calculations, which eventually improves model performance for long-tailed data. As shown in Figure 1(a), GNM, resembling SAM, flattens the landscape of cross-entropy (CE) loss. To further enhance the classification capability of the PEFT methods exemplified by VPT, we harness information from high-level prompts by merging the prompt with the class token for the ultimate classification. We theoretically validate the rationale behind the proposed method. Extensive experiments on benchmark datasets demonstrate that GNM-PT shows great generalization ability on long-tail data, surpassing existing methods. Ablation experiments further prove that GNM improves model performance as well as maintains computational efficiency. Our main contributions are summarized as follows: 1) We identify pressing concerns, explicitly focusing on the imperative need for pre-trained models to enhance the generalization abilities across all classes while concurrently mitigating computational time. 2) We propose the efficient GNM-PT, tailored for long-tailed visual recognition based on VPT, which can improve model generalization while minimizing computational overhead. 3) We provide theoretical evidence supporting the superiority of the proposed GNM-PT. Comprehensive experiments also demonstrate that GNM-PT outperforms its state-of-the-art counterparts.

Figure 1: Loss landscape comparison of VPT based on ViT-B/16 with CE loss (best view in color). The dataset used is CIFAR100-LT with an imbalance ratio of 100.

Related work

### DNN-Based Model for Long-Tailed Learning

Deep neural networks (DNNs) have made significant advancements in long-tail visual recognition in the last few decades. Re-balancing the data distribution, including re-sampling the input data [3; 40; 23; 56; 64] and re-weighting the loss function [13; 49; 8; 24; 44] is the most direct and effective way to improve the performance of the tail classes. Re-margining methods [2; 33; 42; 32; 55] leave larger margins for tail classes than for head classes to improve the separability of tail classes, which can alleviate overfitting and improve model generalization. However, these methods, while improving the performance of tail classes, come at the cost of sacrificing the accuracy in head classes. Ensembling learning encompasses redundant ensembling [57; 1; 27; 29; 22], which aggregates separate classifiers or networks in a multi-expert framework, and complementary ensembling [68; 6; 61], which involves statistically selecting different data divisions. Studies have demonstrated that ensembling methods, particularly redundant ensembling, can achieve SOTA performance and generate more robust predictions by reducing model variance [27; 57] and/or increasing data diversity [29; 34; 61]. Additionally, various alternative methodologies within the realm of DNN-based long-tailed learning. For example, data augmentation [45; 65], decoupling representation [23; 66], logit adjustment [2; 42; 32; 31], to name a few.

### MHSA-Based Fine-Tuning for Long-Tail Learning

Recent advancements in the field of computer vision have harnessed the potential of pre-trained MHSA-Based models, as exemplified by CLIP  and the Visual Transformers (ViTs) . In contrast to the conventional practice of training DNNs from scratch, recent proposed PEFT techniques [21; 4; 20], adopted in RAC , VL-LTR , LPT , and PEL , to name a few, showcase that the meticulous fine-tuning of pre-trained models can yield surprising improvements in the performance of long-tailed visual recognition tasks. For example, VL-LTR employs a contrastive language-image pre-training approach, known as CLIP, and integrates supplementary image-text web data for fine-tuning. LPT fine-tunes a vision Transformer using visual prompt tuning , employing a two-stage training strategy. Nevertheless, it is worth noting that their performance in tail classes still exhibits inferior results compared to that in head classes.

### Sharpness-Aware Minimization

Hochreiter and Schmidhuber  first pointed out that flat minima corresponds to low network complexity and high generalization performance. Li et al.  proposed to visualize the loss landscape and used it to find the flat minima. Subsequently, Foret et al.  proposed SAM to seek flat minima and minimize the loss function, so as to improve model generalization capability. Chen et al.  demonstrated that ViTs converge at extremely sharp local minima, and they can surpass ResNets in both accuracy and robustness when combined with SAM optimizer. As for long-tailed data, models often showcase varying levels of generalization performance across different classes, with the tail class typically exhibiting inferior performance. Based on this, CCSAM  scales the intensity of SAM for classifier inversely with the number of samples available for each class. Zhou et al.  observed that, despite the utilization of SAM, the tail classes, due to their substantially smaller sample sizes in comparison to head classes, have limited influence on model parameters. As a result, the loss landscape for tail classes lacks the desired flatness. To address this issue, they proposed ImbSAM, which isolates SAM for head classes and concentrates exclusively on tail classes. Both CCSAM and ImbSAM are designed to bolster generalization capabilities, particularly for tail classes, albeit at the cost of a slight reduction in the performance of head classes.

## 3 Methodology

### Preliminaries

**Visual Prompt Tuning.** In VPT , \(n_{p}\) prompt tokens \(=[_{1},_{2},,_{n_{p}}]^{n_{p} D}\) are trained to facilitate transfer learning on new datasets with a constraint on the number of learnable parameters, where \(D\) is the dimension of tokens in the pre-trained ViT . The prompt tokens encode task-specific information through collaboration with patch representations obtained from ViTblocks. The only parameters that need training are prompt and classification header. There are two variations: 1) VPT-Shallow, which inserts prompts only at the first block, and 2) VPT-Deep, which inserts prompts at all blocks. Take VPT-deep as an example, it is expressed as:

\[[_{cls}^{l},^{l}]=^{l}([ ^{l-1},_{cls}^{l-1},^{l-1}]),\] (1)

where \(^{l}\) is the learnable prompt, \(_{cls}^{l}\) is the class token, and \(^{l}=[_{1}^{l},_{2}^{l},,_ {N_{x}}^{l}]\) represents \(N_{z}\) patch tokens. \(^{l}\) denotes the \(l\)-th layer of the pre-trained ViT model.

VPT demonstrates notable efficacy in low-data scenarios and maintains its advantages across varying data scales . Despite achieving significant performance improvements, it is noteworthy that VPT exhibits substantial differences in accuracy across different categories. For example, on CIFAR100-LT, the original VPT achieves Top-1 accuracies of 92.11%, 82.86%, and 64.83% for the head, median, and tail classes, respectively. Its generalization ability towards tail classes can be further improved.

**Sharpness-Aware Minimization.** SAM  can improve the generalization ability for models by finding an optimal with low curvature. That is, SAM minimizes a specific point and its neighborhoods in the loss landscape of criterion \(L_{}()\) w.r.t data distribution \(\). It is derived from PAC-Bayesian generalization bound  and following , which is, for any \(>0\) and distribution \(\), with probability \(1-\) over a training set \(\) i.i.d sampled from \(\), the criterion \(L_{}()\) satisfies:

\[L_{}()_{\|\|_{ 2}^{2}}L_{}(+)+ h(\|_{2}^{2}}{^{2}}),\] (2)

where \(h\) is a strictly increasing function. It can be theoretically substituted by an \(L_{2}\) weight decay regularizer \(\|\|_{2}^{2}\) due to its monotonicity. \(\) denotes the weight decay coefficient. Foret et al.  define the sharpness aware loss \(L_{}^{SAM}()=_{\| \|_{2}}L_{}(+)\) and sharpness of the loss function \(L\) as \(L_{}^{SAM}()-L_{}()\), which measures the loss increasing rate by perturbing \(\) with a nearby parameter value \(\). They propose a methodology wherein parameter values are selected by solving the sharpness aware minimization (SAM) problem:

\[^{*}=_{}_{\|\|_{2}}L_{}(+)+\|\|_{2}^{2}.\] (3)

When comparing Equation (3) to the standard training loss, it requires that the maximum loss value of the parameter within the neighborhood of radius \(\) centered on \(\) also remains low. The direction of the gradient of \(L_{}()\) indicates the maximum value of the loss within the neighborhood. Subsequently, for step \(t\), the optimal perturbation vector \(}_{t}\) obtained based on the gradient of \(L_{()}\) to obtain \(L_{}^{SAM}\). The parameters are updated w.r.t. the perturbed model parameters \(+\):

\[}_{t} =_{SAM}}L_{}( _{t})}{||_{}L_{}( _{t})||_{2}^{2}},\] (4) \[_{t+1}^{SAM} =_{t}-_{t}(_{_{t}}L_{}(_{t})|_{_{t }+}_{t}}+_{t}).\] (5)

where \(_{SAM}\) represents the radius of the parameter neighborhood for SAM, and \(_{t}\) denotes the learning rate scheduled in step \(t\).

### Prompt Tuning with Gaussian Neighborhood Minimization

Despite the effectiveness of SAM and its strong theoretical foundation, it exhibits twofold deficiencies: \(\) For long-tailed data, \(}\) for tail classes is often negligible due to the dominance of head classes with a large number of samples in determining the gradient direction. Consequently, this leads to a challenge in achieving effective generalization for tail classes . \(\) At each step, two gradient computations are required, namely \(_{}L_{}(_{t})\) and \(_{}L_{}(_{t}+}_{t})\), resulting in a duplication of the computational overhead.

To address the aforementioned issues of head-class dominant optimization and double gradient computation, we propose GNM-PT.

**Gaussian Neighborhood Minimization (GNM).** To mitigate the presence of sharp minima and enhance the performance of VPT on long-tailed data, we can directly minimize the loss within the parameter neighborhood, thereby attaining a flattened loss landscape. We introduce the Gaussian neighborhood loss \(L_{}^{GN}\) on \(\), which is defined as:

\[L_{}^{GN}()=_{_{i}(0, ^{2})}[L_{}(+)].\] (6)

Optimizing \(L_{}^{GN}\) is equivalent to optimizing an upper bound of the distribution \(\) using the training set \(\) sampled i.i.d. from \(\). The detailed theoretical proof will be discussed in the following section. Then, substituting \(L_{}^{SAM}\) in Equation (3) with \(L_{}^{GN}\), we can obtain the parameter update strategy of GNM:

\[}_{t} =_{GNM}[_{i}]_{i=1}^{k},\ _{i}(0,^{2}),\] (7) \[_{t+1}^{GNM} =_{t}-_{t}(_{_{t}}L_{ }(_{t})|_{_{t}+}_{t} }+_{t}).\] (8)

\(_{GNM}\) in Equation (7) represents the radius of the parameter neighborhood for GNM. The detailed derivation of the gradient for GNM can be found in Appendix A. Figure 2 schematically illustrates a single GNM parameter update.

**Remark 1**.: _Handling Long-Tailed Data._ GNM is better suited for long-tailed data.

Proof.: If \(\) is a long-tailed training set i.i.d. sampled from \(\), the direction of gradients in existing methods such as SGD and SAM is predominantly influenced by head classes. (The detailed proof can be found in Appendix B.) Consequently, optimization through Equation (4), which is sample-dependent, will be determined mainly by the head classes. Conversely, Equation (7) is in a sample-independent manner, avoiding classes with large numbers of samples that dominate the direction of the perturbation vector. 

**Remark 2**.: _Computational Time._ GNM saves computational overhead compared to SAM.

Proof.: Even when disregarding second- and higher-order terms (for example, see Foret et al. , Zhou et al. , Mi et al.  for more details), it is apparent from Equation (4) that solving for \(_{t}\) necessitates the computation of one gradient involving a forward and backward pass, while calculating \(_{t+1}\) requires another forward and backward pass. As a result, in SAM calculation, which retains only first-order terms, the parameter update already requires an additional forward and backward pass, undesirably doubling the computation time. Conversely, Equation (7) mitigates the computational burden and improves the precision of perturbations. 

**Remark 3**.: _Loss landscape._ GNM can achieve a flat loss landscape for VPT.

Figure 1 and Section 4.4 empirically demonstrate the loss landscape obtained by GNM for VPT is flattened than the original VPT and SAM. Appendix I and Appendix L demonstrate that besides VPT, GNM can also improve other PEFT methods such as AdapterFormer  and other backbones such as ResNet based models.

Although GNM is not affected by class sizes and improves the generalization performance of each category equally, head-class bias caused by the classifier still exists. Two-stage strategy [56; 23; 10] is effective. Classifier re-balance strategy, including deferred re-weighting/sampling (DRW or DRS) , classifier Re-training (cRT) , nearest class mean classifier (NCM) , to name a few, can be employed. The overall training procedure of GNM-PT is summarized in Appendix D.

### Theoretical Analysis of GNM

This section explains GNM from the theoretical perspective. We introduce the following theorem to demonstrate the compactness of the upper bound of the loss function across the distribution \(\).

Figure 2: Schematic of optimization direction in GNM3. \(_{t+1}^{Orig}\) and \(_{t+1}^{GNM}\) represent the gradient update w.o. and w. GNM for step \(t+1\).

**Theorem 1**.: _For any \(0<<1\), and number of samples \(n^{+}\), with probability \(1-\) over the training set \(\) sampled i.i.d. from a distribution \(\), the following generalization bound w.r.t. model parameters \(\) holds:_

\[L_{}()_{_{i}(0, ^{2})}[L_{}(+)]\,+h( \|_{2}^{2}}{4^{2}}),\] (9)

_where \(h:^{+}^{+}\) is a strictly increasing function._

Proof.: Based on the condition that adding Gaussian perturbation should not decrease the test error, \(L_{}\) satisfy:

\[L_{}()_{_{i}(0, ^{2})}[L_{}(+)].\] (10)

By Theorem 2 in Foret et al.  and Theorem 1 in Zhou et al. , the Gaussian perturbation satisfy:

\[_{_{i}(0,^{2})}[L_ {}(+)]_{ _{i}(0,^{2})}[L_{}(+)]\] \[+k(1+\|_{2}^ {2}}{k^{4}})+()+2(6n+3k)+}{n-1}},\] (11)

where \(k\) is the dimension of \(\). Since \((1+x)<x\) holds for all \(x>0\), Equation (11) can be simplified to:

\[_{_{i}(0,^{2})}[L_ {}(+)]_{ _{i}(0,^{2})}L_{}(+)\] \[+\|_{2}^{2}}{4^{2}}+( )+(1)}{n-1}}.\] (12)

The term containing square roots in the above expression is a strictly increasing function. Therefore, by combining it with Equation (10), Theorem 1 is proved. 

Similar to , \(h\) in Equation (9) can be replaced by \(L_{2}\) weight decay regularizer \(\|\|_{2}^{2}\). Minimizing \(L_{}\) can be achieved by solving the following GNM problem:

\[_{}L_{}^{GN}()+\|\|_{2}^{2}.\] (13)

Hence, the parameter updates given by Equation (7) and Equation (8) for GNM can minimize the upper bound given of \(L_{}()\) by Theorem 1.

**Remark 4**.: _Upper Bound for Loss Function._ GNM achieves a tighter upper bound for loss function than SAM.

Proof.: According to Equation (2), \(L_{}^{SAM}\) is obtained by minimizing the maximum of the loss within the parameter neighborhood of radius \(_{SAM}\). By adjusting the variance \(\), \(L_{}^{GN}\) is obtained by minimizing the average value of the loss function within the parameter neighborhood \(r_{GNM}\). It is evident that when \(_{SAM}_{GNM}\), \(_{_{SAM}}[L_{}(+) ]_{_{GNM}}[L_{}(+)]\). Therefore, \(L_{}^{GN}\) is a tighter upper bound on the loss over \(\) than \(L_{}^{SAM}\). 

## 4 Experiment

### Datasets

**CIFAR100-LT.** We adopt the same settings utilized in  to establish the long-tailed version by downsampling the original CIFAR100 dataset  with different imbalance ratios \(IR=n_{}/n_{}\), where \(n_{}\), \(n_{}\) represent the class sizes of the most and the least frequent classes, respectively. Following , we set the imbalance ratios at 200, 100, 50, and 10.

**Places-LT.** It is artificially truncated from its balanced version, Places365 . The long-tailed version was first created by Liu et al. . Places-LT consists of 62.5K training images with an imbalance ratio of 996.

**iNaturalist2018.** iNaturalist is a substantial real-world dataset that inherently exhibits an exceedingly imbalanced distribution. In our experiments, we leverage the widely employed 2018 version  (iNat for short), encompassing 437.5K images across 8,142 distinct species. This dataset features an imbalance ratio of 512.

### Implementation Details

**Evaluation Protocol.** Following the fundamental assumption that every class carries equal importance, all classes with varying frequencies in the training set are granted an equal number of samples during testing. Top-1 classification accuracy is the primary metric for assessing the performance of various methods. Following Liu et al. , we additionally provide accuracy measurements for three class splits based on the number of training data: Head (\(>100\) images), Medium (Med for short, \(20 100\) images), and Tail (\( 20\) images).

**Model and Parameter Settings.** Following Dong et al. , we employ ViT-B/16 pre-trained on ImageNet-21K and VPT-deep for prompt tuning, and GCL  as the loss function. The same data augmentation strategies outlined in  are adopted, consistent with widely adopted practices among mainstream methods. We employ SGD with GNM as an optimizer and set the batch size to 128, a learning rate of 0.01, accompanied by a cosine learning rate scheduler. For parameter settings of the Gaussian distribution parameters \((0,^{2})\) mentioned in Section 3.2, we exploit the same strategy as Li et al. . Specifically, we set \(=\) meanwhile clamping \(\) within the range \([-1,1]\) to ensure that its amplitude remains within one and use a hyper-parameter \(\) to control the perturbation magnitude. We adopt DRW for classifier re-balance. Notably, LPT  also employs a re-balance strategy during the group prompt tuning stage. For CIFAR100-LT and iNat, we fine-tuned models for 70 epochs, with the final 10 epochs for DRW. For Places-LT, the models undergo a fine-tuning process spanning 100 epochs, with the last 40 epochs for DRW.

### Comparison with Prior Arts

#### 4.3.1 Compared Methods

We compare our proposed GNM-PT with several state-of-the-art methods, broadly categorized into two main types.

**DNN-based model.** We compare with (1) two-stage methods, namely BBN , LWS , MiSLAS ; (2) logit adjustment methods, i.e., GCL  and LDAM ; (3) ensembling learning methods, including, RIDE , NCL , and SHIKE ; and (4) contrastive learning, represented by GPaCo . Additionally, we compared two recently proposed SAM-based methods, CCSAM  and ImbSAM , explicitly designed to address long-tail data.Recently, MHSA-based models represented by ViT have been employed in long-tail visual recognition. We compare with visual-only methods, including LiVT , VPT , BAL-LAD , LPT , and Decoder . All methods were implemented using ViT-B/16 for a fair comparison. In addition, VL-LTR , RAC  and GML  are also MHSA-based models which use supplementary data (i.e., text information). We also report the results obtained by these methods for reference.

#### 4.3.2 Comparison Results

**Comparison on CIFAR100-LT.** We present the comparison results for CIFAR100-LT in Table 1. Our proposed GNM-PT exhibits superior performance across all commonly used imbalance ratios compared to the competing methods. Notably, as the imbalance ratio increases, the effectiveness of our GNM-PT becomes increasingly apparent on CIFAR100-LT. Specifically, our proposed method

   Method & 200 & 100 & 50 & 10 \\   \\  BBN  & 37.2 & 42.6 & 47.0 & 59.1 \\ RIDE  & 45.8 & 50.4 & 55.0 & - \\ MisLAS  & 43.5 & 47.0 & 52.3 & 63.2 \\ BCL  & - & 51.9 & 56.6 & 64.9 \\ GCL  & 44.8 & 48.6 & 53.6 & - \\ NCL  & - & 54.2 & 58.2 & - \\ GPaCo  & - & 52.3 & 56.4 & 65.4 \\ - SHIKE  & - & 56.3 & 59.8 & \\  \\  & 45.7 & 50.8 & 53.9 \\ ImbSAM  & - & 54.8 & 59.3 & 59.7 \\   \\  VPT  & 72.8 & 81.0 & 84.8 & 89.6 \\ LiVT  & - & 58.2 & - & 69.2 \\ LPT  & **87.9** & **89.1** & **90.0** & **91.0** \\  & **89.2** & **90.3** & **91.2** & **91.8** \\   

* **Note**: The best and second-best results are shown in **underline** **bold** and **bold**, respectively.

Table 1: Comparison on CIFAR100-LT w.r.t top-1 classification accuracy (%).

achieves improvements of 1.3%, 1.2%, 1.2%, and 0.8% over the second-best method, namely LPT , for imbalance ratios of 200, 100, 50, and 10, respectively.

**Comparison on iNat.** Table 2 provides results on iNat. The proposed GNM-PT achieves a top-1 classification accuracy of 76.5%, surpassing DNN-based methods. Compared with other visual-only MHSA-based methods that exclusively rely on visual data, our improvement may not be substantial (76.5% However, it is noteworthy that GNM-PT is trained with a relatively small number of epochs (70 and 80 epochs without and with DRW, respectively). In contrast, LPT  requires 160 epochs (80 for shared prompt tuning and 80 for group prompt tuning), while LiVT  requires 100 epochs. Our proposed method can even achieve results comparable to those with supplementary data. For example, VL-LTR , which requires image-text pairs, achieves an accuracy of 76.8%, only 0.3% and 0.5% higher than GNM-PT with and without DRW, respectively. Notably, the adoption of DRW in GNM-PT has the potential to enhance overall performance, albeit with the trade-off of sacrificing head-class accuracy to bolster tail-class accuracy. This observation may be attributed to suboptimal parameter selection in calculating effective numbers in DRW, an aspect that we plan to delve into in future work.

**Comparison on Places-LT.** From Table 3, we can observe that GNM-PT continues to outperform existing methods. Similarly to iNat, GNM-PT obtains performance equivalent to LPT with fewer training epochs and outperforms LiVT by nearly 10%. Even when compared to VL-LTR and RAC, which leverage additional auxiliary data, GNM-PT still achieves satisfying performance. Additionally, from Table 3, it can be observed that DRW improves tail classes at the expense of significant degradation in head classes on Places-LT. While it resulted in an overall improvement of 1%, the head classes decreased by 2%. This indicates that the chosen effective number employed by DRW may not be optimal, warranting further investigation. More results on imageNet-LT can be found in Appendix G.

   Method & Head & Med & Tail & Overall \\   \\  LWS  & 72.9 & 71.2 & 69.2 & 70.5 \\ RIDE  & 76.5 & 74.2 & 70.5 & 72.8 \\ MisLAS  & 73.2 & 72.4 & 70.4 & 71.6 \\ GCL  & - & - & - & 72.0 \\ NCL  & 72.7 & 75.6 & 74.5 & 74.9 \\ GPaCo  & - & - & - & 75.4 \\ SHIKE  & - & - & - & 75.4 \\  \\  & 64.1 & 70.5 & 71.2 & 70.1 \\ CCSAM  & 65.4 & 70.9 & 72.2 & 70.9 \\ ImbSAM  & 68.2 & 72.5 & 72.9 & 71.1 \\  MHSA-based model (Backbone: ViT-B/16) &  \\  &  &  & ^{}\)} \\ RAC  & 75.9 & 80.5 & 81.1 & \(^{}\) \\  \\ Decoder  & - & - & 59.2 \\ LPT  & - & - & 79.3 & 76.1 \\ LiVT  & 78.9 & 76.5 & 74.8 & 76.1 \\ GNM-PT (ours) & 61.5 & 77.1 & 79.3 & \(\) \\ GNM-PT (ours) & 76.3 & 77.6 & 75.0 & \(^{}\) \\   

Table 2: Acc. (%) comparison on iNat.

   Method & Head & Med & Tail & Overall \\   \\  LWS  & 40.6 & 39.1 & 28.6 & 37.6 \\ RIDE  & 44.4 & 40.6 & 33.0 & 40.4 \\ MiLAS  & 39.6 & 43.3 & 36.1 & 40.4 \\ GCL  & 38.6 & 42.6 & 38.4 & 40.3 \\ NCL  & - & - & - & 41.8 \\ GPaCo  & 39.5 & 47.2 & 33.0 & 41.7 \\ SHIKE  & 43.6 & 39.2 & 44.8 & 41.9 \\  \\  & 41.2 & 42.1 & 36.4 & 40.6 \\  \\  \\  &  &  & ^{}\)} \\ RAC  & 75.9 & 80.5 & 81.1 & \(^{}\) \\  \\ Decoder  & - & - & 59.2 \\ LPT  & - & - & 79.3 & 76.1 \\ LiVT  & 78.9 & 76.5 & 74.8 & 76.1 \\ GNM-PT (ours) & 61.5 & 77.1 & 79.3 & \(\) \\ GNM-PT (ours) & 76.3 & 77.6 & 75.0 & \(^{}\) \\   

Table 3: Acc. (%) comparison on Places-LT.

### Further Analysis

To ensure a fair comparison, the experiments in this section are all executed on the following hardware: Core(TM) i9-13900K, operating at 3.00GHz, equipped with 128GB RAM, and a single NVIDIA GeForce RTX 4090 GPU. The dataset is CIFAR100-LT with \(IR=100\).

**GNM vs. SAM.** To demonstrate the superiority of GNM, we conducted a comparative analysis with the SAM from two perspectives: classification accuracy and computational efficiency. We employ both CE loss and GCL loss utilizing the CIFAR100-LT dataset with an imbalance ratio of 100. Except for incorporating the optimization of SAM or GNM, all other settings remain identical. Table 4 shows the results. We can observe that SAM entails a computation time exceeding 1.8 times that of the original method compared to the baseline methods without additional optimization technologies. In contrast, GNM incurs only a negligible increase in computation time, namely, less than 2 seconds per epoch. In addition to time savings, GNM also manifests a discernible improvement in accuracy. Despite our straightforward adoption of random perturbation vector \(}\) (as detailed in Section 3.2), its performance is superior to that of the theoretical optimal perturbation vector \(}\) for seeking the maximum loss value within the neighborhood. It is worth noting that all experiments in this section employ the same number of training epochs. SAM and GNM _do not_ affect the convergence speed of the network. The effectiveness across various classes is visualized in Figure 3. While SAM declines the performance of GCL within the tail classes, our proposed GNM consistently improves performance across all categories in every scenario. Additional comparison results for the long-tailed SAM method can be found in Appendix J and Appendix K. Further comparisons for balanced softmax (BASM)  and ResNet-152 backbone can be found in Appendix L

**Visualization of Loss Landscape.** We employ the method in  to visualize the loss landscape of the model. Figure 4 shows the results of the learnable prompts obtained by different optimizers. The absence of a perceptible change in flatness explains the marginal improvement of GCL+DRW+SAM over GCL+DRW in Table 4. In comparison, GNM, by inducing a flattened loss landscape, further accentuates the improvement over GCL+DRW. An unforeseen advantage is that GNM results in a smaller loss, indicating that GNM enhances the model fitting to the training data. A flatter landscape with lower minima contributes to discovering a more optimal solution. By referring to Figure 1 in Section 1, it can be observed that GNM is effective across various loss functions.

   Method & Acc. (\%) & NET (s) \\  CE & 81.02 & 39.78 \\ CE+SAM & 82.48 & 72.51 \\ CE+GNM & **82.50** & 40.16 (\(\) 44.61\% ) \\ GCL+DRW & 89.58 & 40.00 \\ GCL+DRW+SAM & 89.69 & 74.36 \\ GCL+DRW+GNM & **90.28** & 41.87 (\(\) 43.69\% ) \\   

Table 4: Comparison between SAM and the proposed GNM. NET represents Native Execution Time.

Figure 3: Effectiveness comparison of different classes.

## 5 Concluding Remarks

In this paper, we observed that class biases persist even when employing large-scale pre-trained models such as VPT in long-tail learning. While SAM can enhance the generalization performance of the VPT model on long-tailed data, it still exhibits several shortcomings: neglecting higher-order terms leads to a suboptimal perturbation vector, additional forward and backward passes double the computational time, and the generalization is relatively affected by gradients predominantly originating from head classes. Based on this, we have proposed GNM-PT, which involves fine-tuning pre-trained models using the innovative Gaussian Neighborhood Minimization (GNM) optimizer. GNM leverages random noise as a substitute for gradients in the initial step of SAM. The proposed GNM not only balances the generalization capabilities of both head and tail classes but also reduces computational time. To fully leverage model information, enhance classifier robustness, and enable end-to-end training, we additionally employ merging prompt strategy. We have conducted extensive comparative experiments and ablation studies to demonstrate the effectiveness of the proposed method and the individual component.

While GNM-PT proves effective, it is not exempt from limitations. Table 2 and Table 3 show that we still need to further re-balance the classifier. However, the re-balanced strategy adopted compromises performance in head classes to enhance overall performance. Our further research will focus on a more effective optimization strategy that simultaneously improves feature representation and classifier performance, while also enhancing the generalization ability across all classes.