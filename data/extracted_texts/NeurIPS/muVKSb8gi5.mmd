# Reliable Off-Policy Learning for Dosage Combinations

Jonas Schweisthal, Dennis Frauen, Valentyn Melnychuk & Stefan Feuerriegel

LMU Munich

Munich Center for Machine Learning

{jonas.schweisthal,frauen,melnychuk,feuerriegel}@lmu.de

###### Abstract

Decision-making in personalized medicine such as cancer therapy or critical care must often make choices for dosage combinations, i.e., multiple continuous treatments. Existing work for this task has modeled the effect of multiple treatments _independently_, while estimating the _joint_ effect has received little attention but comes with non-trivial challenges. In this paper, we propose a novel method for reliable off-policy learning for dosage combinations. Our method proceeds along three steps: (1) We develop a tailored neural network that estimates the individualized dose-response function while accounting for the joint effect of multiple dependent dosages. (2) We estimate the generalized propensity score using conditional normalizing flows in order to detect regions with limited overlap in the shared covariate-treatment space. (3) We present a gradient-based learning algorithm to find the optimal, individualized dosage combinations. Here, we ensure reliable estimation of the policy value by avoiding regions with limited overlap. We finally perform an extensive evaluation of our method to show its effectiveness. To the best of our knowledge, ours is the first work to provide a method for reliable off-policy learning for optimal dosage combinations.

## 1 Introduction

In personalized medicine, decision-making frequently involves complex choices for _dosage combinations_. For example, in cancer therapy, patients with chemotherapy or immunotherapy receive a combination of two or three drugs, and, for each, the dosage must be carefully personalized . In critical care, medical professionals must simultaneously control multiple parameters for mechanical ventilation, including respiratory rate and tidal volume .

One way to find optimal dosages (i.e., multiple continuous treatments) is through randomized controlled trials. Yet, such randomized controlled trials are often impractical due to the complexity of dosing problems or even unethical . Instead, there is a direct need for learning optimal, individualized dosage combinations from observational data (so-called off-policy learning).

Several methods for off-policy learning aim at discrete treatments  and are thus _not_ applicable to dosage combinations. In contrast to that, there are only a few methods that aim at off-policy learning for continuous treatments, that is, dosages . To achieve this, some of these methods utilize the individualized dose-response function, which is typically not available and must be estimated from observational data. However, existing works model the effect of multiple continuous treatments _independently_. Because of that, existing works ignore the dependence structure among dosage combinations, that is, drug-drug interactions. For example, in chemotherapy, the combination of antineoplastic agents with drugs such as warfarin can have adverse effects on patient health . In contrast to that, methods for off-policy learning that account for the _joint_ effect of dosage combinations have received little attention and present the focus of our paper.

Crucially, off-policy learning for dosage combinations comes with non-trivial challenges. (1) Empirically, datasets in medical practice are often high-dimensional, yet of limited sample size. Hence, in practice, there are often sparse regions in the covariate-treatment space, which, in turn, may lead to estimated individualized dose-response functions that have regions with high uncertainty. (2) Mathematically, off-policy learning commonly relies upon the overlap assumption . Yet, certain drug combinations are rarely used in medical practice and should be avoided for safety reasons. Both challenges lead to areas with limited observed overlap in the data, which results in unreliable estimates of the policy value. Motivated by this, we thus develop a method for _reliable_ off-policy learning.

**Proposed method:** In this paper, we propose a novel method for reliable off-policy learning for dosage combinations. Our method proceeds along three steps. (1) We develop a tailored neural network that estimates the individualized dose-response function while accounting for the joint effect of multiple dependent dosages. Later, we use the network as a plug-in estimator for the policy value. (2) We estimate the generalized propensity score through conditional normalizing flows in order to detect regions with limited overlap in the shared covariate-treatment space. (3) We find the optimal individualized dosage combinations. To achieve that, we present a gradient-descent-ascent-based learning algorithm that solves the underlying constrained optimization problem, where we avoid regions with limited overlap and thus ensure a reliable estimation of the policy value. For the latter step, we use an additional policy neural network to predict the optimal policy for reasons of scalability.

**Contributions:1** (1) We propose a novel method for reliable off-policy learning for dosage combinations. To the best of our knowledge, ours is the first method that accounts for the challenges inherent to dosage combinations. (2) As part of our method, we develop a novel neural network for modeling the individualized dose-response function while accounting for the joint effect of dosage combinations. Thereby, we extend previous works which are limited to mutually exclusive or independent treatments. (3) We perform extensive experiments based on real-world medical data to show the effectiveness of our method.

## 2 Related Work

**Estimating the individualized dose-response function:** There are different methods to estimate the outcomes of continuous treatments, that, is the individualized dose-response function. One stream uses the generalized propensity score [26; 28; 29], while others adapt generative adversarial networks . An even other stream builds upon the idea of shared representation learning from binary treatments (e.g., [33; 53; 68; 69; 84]) and extends it to continuous treatments [57; 67]. However, all of the previous methods model the marginal effects of multiple dosages _independently_ and thus _without_ considering the dependence among when multiple treatments are applied simultaneously. Yet, this is unrealistic in medical practice as there are generally drug-drug interactions. To address this, we later model the _joint_ effect of multiple _dependent_ dosages.

There are some approaches for estimating the individualized effects of multi- or high-dimensional treatments, yet _not_ for dosages. For example, some methods target conditional multi-cause treatment effect estimation [55; 58; 59; 73; 78; 86] but are restricted to discrete treatments and can _not_ be directly applied to dosages. Other works use techniques tailored for high-dimensional structured treatments [21; 36; 54]. However, they do not focus on smooth and continuous estimates, or reliable off-policy learning, which is unlike our method.

**Off-policy learning for dosages:** There is extensive work on off-policy learning for _discrete_ treatments [1; 3; 4; 14; 17; 23; 37; 40; 49; 50; 72; 74; 85]. While, in principle, these methods could be applied to our setting by discretizing the dosages, crucial information about the exact individually prescribed dosage values would be lost. This results in less accurate and less personalized dosage recommendations. However, there are only a few papers that address off-policy learning directly for _continuous_ treatments ("dosages").

For dosages, previous works have derived efficient estimators for policy evaluation and subsequent optimization (e.g., [10; 41]). Other works focus directly on the policy optimization objective to find an optimal dosage [6; 87; 44]. In principle, some of these methods are applicable to settings with multiple dosages where they act as naive baselines. However, all of them have crucial shortcomingsin that they do _not_ account for multiple dependent dosages and/or that they do _not_ aim at reliable decision-making under limited overlap. Both of the latter are contributions of our method.

**Dealing with limited overlap:** Various methods have been proposed to address limited overlap for estimating average treatment effects of binary treatments by making use of the propensity score (e.g., [7; 9; 24; 30; 38; 66]). In contrast, there exist only a few works for off-policy learning with limited overlap, typically for binary treatments [39; 42; 70].

Some works deal with the violation of the overlap assumption by leveraging uncertainty estimates for binary  and continuous treatment effect estimation . However, these works do not tackle the problem of direct policy learning. In particular for dosage combinations, this is a clear shortcoming, as determining and evaluating policies becomes computationally expensive and infeasible with a growing number of samples and treatment dimensions.

**Research gap:** Personalized decision-making for optimal dosage combinations must (1) estimate the _joint_ effect of dosage combinations, (2) then perform off-policy learning for such dosage combinations, and (3) be _reliable_ even for limited overlap. However, to the best of our knowledge, there is no existing method for this composite task, and we thus present the first method aimed at reliable off-policy learning for dosage combinations.

## 3 Problem Formulation

**Setting:** We consider an observational dataset for \(n\) patients with i.i.d. observations \(\{(x_{i},t_{i},y_{i})\}_{i=1}^{n}\) sampled from a population \((X,T,Y)\), where \(X^{d}\) are the patients' covariates, \(T^{p}\) is the assigned \(p\)-dimensional dosage combination (i.e., multiple continuous treatments), and \(Y\) is the observed outcome. For example, in a mechanical ventilation setting, \(X\) are patient features and measured biosignals, such as age, sex, respiratory measurements, and cardiac measurements; \(T\) are the dosages of the ventilation parameters, such as respiratory rate and tidal volume; and \(Y\) is patient survival. For simplicity, we also refer to the dosages as treatments.

We adopt the potential outcomes framework , where \(Y(t)\) is the potential outcome under treatment \(T=t\). In medicine, one is interested in learning a policy \(:\) that maps covariates to suggested dosage combinations. Then, the policy value \(V()=[Y((X))]\) is the expected outcome of policy \(\). Our objective is to find the optimal policy \(^{*}\) in the policy class \(\) that maximizes \(V()\), i.e.,

\[^{*}*{arg\,max}_{}\,V().\] (1)

In other words, we want to find \(^{*}\) that minimizes the regret given by \(R(^{*})=_{}V()-V(^{*})\), where \(\) denotes the set of all possible policies with \(\).

We further introduce the following notation, which is relevant to our method later. We denote the _conditional potential outcome function_ by \((t,x)=[Y(t)\;\;X=x]\). For continuous treatments, this is also called _individualized dose-response function_. Further, the _generalized propensity score_ (GPS) is given by \(f(t,x)=f_{T\,\,X=x}(t)\), where \(f_{T\,\,X=x}\) denotes the conditional density of \(T\) given \(X=x\) with respect to the Lebesgue measure. The GPS is the stochastic policy used for assigning the observed treatments (i.e., logging / behavioral policy). In our method, both nuisance functions, \((t,x)\) and \(f(t,x)\), are not known and must be later estimated from observational data.

**Assumptions:** To ensure identifiability of the outcome estimation, the following three standard assumptions are commonly made in causal inference : (1) _Consistency_: \(Y=Y(T)\). Consistency requires that the patient's observed outcome is equal to the potential outcome under the observed treatment. (2) _Ignorability_: \(Y(t)\;\;\;\!\!\!\;\;\;T\;\;\;X,\; t\). Ignorability ensures that there are no unobserved confounders. (3) _Overlap_: \(f(t,x)>,\, x,\,t\) and for some constant \([0,)\). Overlap (also known as common support or positivity) is necessary to ensure that the outcomes for all potential treatments can be estimated accurately for all individuals.

We distinguish between _weak_ overlap for \(=0\) and _strong_ overlap for \(>0\). Since the estimation of the individualized dose-response function becomes more reliable with increasing \(\) in the finite sample setting, we refer to _strong_ overlap in the following when we mention overlap, unless explicitly stated otherwise.

**Challenges due to limited overlap:** Limited overlap introduces both empirical and theoretical challenges. (1) Datasets with high dimensions and/or small sample size are especially prone to have sparse regions in the covariate-treatment space \(\) and thus limited overlap . As a result, the estimated individualized dose-response function will have regions with high uncertainty, which preclude reliable decision-making. (2) It is likely that \(\) is small for some regions as some drug combinations are rarely prescribed and should be avoided. To address this, we aim at an off-policy method that is reliable, as introduced in Sec. 4.

## 4 Reliable Off-Policy Learning for Dosage Combinations

In the following, we propose a novel method for reliable off-policy learning for dosage combinations. Our method proceeds along three steps (see Fig. 1): **(1)** We develop a tailored neural network that estimates the individualized dose-response function while accounting for the joint effect of multiple dependent dosages (Sec. 4.1). Later, we use the network as a plug-in estimator for the policy value. **(2)** We estimate the generalized propensity score through conditional normalizing flows in order to detect regions with limited overlap in the shared covariate-treatment space (Sec. 4.2). **(3)** We find the optimal individualized dosage combinations (Sec. 4.3). To achieve that, we present a gradient-descent-ascent-based learning algorithm that solves the underlying constrained optimization problem (Sec. 4.4), where we avoid regions with limited overlap and thus ensure a reliable estimation of the policy value. For the latter step, we use an additional policy neural network to predict the optimal policy for fast inference during deployment.

### Neural network for estimating the joint effect of multiple dependent dosages

In the first step, we aim to estimate the individualized dose-response function \((t,x)\), which is later used as a plug-in estimator for the policy value \(V()\). However, using a naive neural network with \(t\) and \(x\) concatenated as inputs can lead to diminishing effect estimates of \(t\) when \(x\) is high-dimensional . In addition, in our setting with multiple continuous treatments, we want to yield smooth estimates of the dose-response surface to ensure better generalization when interpolating between sparse observations. To address this, we thus develop a tailored _dosage combination network_ (DCNet) in the following.

**Architecture of DCNet:** Our DCNet builds upon the varying coefficient network (VCNet)  but extends it to learn the _joint_ effect of _multiple_ dependent dosages. DCNet consists of (i) a representation network \((x)\) and (ii) a prediction head \(h_{(t)}()\), where \(h\) is a network with \(d_{}\) different parameters \((t)=(_{1}(t),,_{d_{}}(t))^{T}\). The parameters \((t)\) are not fixed but depend on the dosage \(t\). This is crucial to ensure that the influence of \(t\) does not diminish in a setting with high-dimensional covariates \(x\).

Figure 1: Overview of our method along steps (1)–(3).

To handle multiple dosages \((t^{(1)},,t^{(p)}\), a naive way  would be to incorporate a separate prediction head for each treatment with corresponding parameters \(^{(i)}(t^{(i)})\). However, this would be ineffective for our task, as it captures only the marginal effect of a dosage but not the joint effect of the dosage combination. Instead, in order to learn the joint effect, we must not only model the marginal effect of a dosage (as in ) but also drug-drug interactions. We achieve that through a custom prediction head.

**Prediction head in DCNet:** In DCNet, we use a tailored prediction head, which is designed to model the _joint_ effect of dosage combinations. For this, we leverage a tensor product basis (see, e.g., ) to estimate smooth interaction effects in the dose-response surface. As a result, we can directly incorporate all \(p\) dosages simultaneously into the parametrization of the network \(h\) by using only one head containing the joint dosage information. This is unlike other networks that use \(p\) different independent heads  and thus cannot capture the joint effect but only marginal effects.

We define each scalar parameter \(_{j}(t)\) of our prediction head as

\[_{j}(t)=_{k_{1}=1}^{K_{1}}_{k_{2}=1}^{K_{2}}_{k_{p}=1}^{ K_{p}}_{j,k_{1}k_{2} k_{p}}_{k_{1}}^{(1)}(t^{(1)}) _{k_{2}}^{(2)}(t^{(2)})_{k_{p}}^{(p)}(t^{(p)}),\] (2)

where \(K_{1},,K_{p}\) are the number of basis functions \(_{k_{i}}^{(i)}\) of the \(p\) dosages \(t^{(1)},,t^{(p)}\), and \(_{j,}\) are the trainable coefficients for each element of the tensor product. In order to preserve the continuity of the non-linear dose-response surface, we choose \(_{k_{i}}^{(i)}\) to be polynomial spline basis functions. Hence, we yield the parameter vector \((t)=(t)\), where

\[=_{1,1 1}&&_{1,K_{1} K_{p}} \\ &&\\ _{d_{q},1 1}&&_{d_{q},K_{1} K_{p}} ^{d_{q}(_{i=1}^{p}K_{i})}\] (3)

with matrix rows arranged as \((_{j,11 1},\ ,\ _{j,K_{1}1 1},\ ,\ _{j,K_{1}  K_{p-1}1},\ ,\ _{j,K_{1} K_{p-1}K_{p}})\), with

\[(t)=(^{(1)}(t^{(1)})\ \ ^{(2)}(t^{(2)})\ \ ^{(p)}(t^{(p)}))^{(_{i=1}^{p}K_{i})}\] (4)

and with \(^{(i)}=(_{1}^{(i)},,_{K_{i}}^{(i)})^{T}\), where \(\) is the Kronecker product. For training DCNet, we minimize the mean squared error loss \(_{}=_{i=1}^{n}((t_{i},x_{i})-y_{i} )^{2}\).

Our prediction head has three key benefits: (1) When estimating \((t,x)\), the tensor product basis enables us to model non-linear interaction effects among different dosage dimensions while still having a dose-response surface that offers continuity, smoothness, and expressiveness. (2) Having a dose-response surface that is smooth between different dosages is beneficial for our task as it allows for better interpolation between sparse data points. (3) Smoothness is also exploited by our gradient-based learning algorithm later. The reason is that a smooth plug-in estimator \((t,x)\) for the estimated policy value \(()\) allows us to have more stable gradients and thus more stable parameter updates during learning.

### Conditional normalizing flows for detecting limited overlap

In the second step, we estimate the GPS \(f(t,x)\) in order to detect limited overlap, which later allows us to avoid regions with high uncertainty and thus to yield a reliable policy.

_Why is the GPS relevant for reliable decision-making?_ Due to sparsity in the data or otherwise limited overlap, the estimates of the individualized dose-response function \((t,x)\) from our DCNet may be naturally characterized by regions with high uncertainty, that is, regions in the covariate-treatment space with low GPS. If we would use the estimated \((t,x)\) in a naive manner as a plug-in estimator for policy learning, we may yield a policy where there can be a large discrepancy between the predicted outcome \((t,x)\) and the true outcome \((t,x)\) and, therefore, a large discrepancy between \(()\) and \(V()\). By avoiding regions with a low GPS, we can thus ensure reliable decision-making in safety-critical applications such as medicine.

**Estimation of GPS:** The GPS (i.e., the logging policy) is typically not known in real-world settings. Instead, we must estimate the GPS from observational data. To this end, we leverage _conditional_normalizing flows_ (CNFs) [75; 80]. CNFs are a fully-parametric generative model built on top of normalizing flows [60; 71] that can model conditional densities \(p(y x)\) by transforming a simple base density \(p(z)\) through an invertible transformation with parameters \((x)\) that depend on the input \(x\).2

In our method, we use CNFs with parameters \((x)\) to estimate the probability density function \(f(t,x)=f_{T\,|\,X=x}(t)\) of \(T\) conditioned on the patient covariates \(X=x\). For our method, CNFs have several advantages.3 First, CNFs are universal density approximators [11; 12; 15; 27], making them flexible and able to capture even complex density functions for dosage combinations. Second, CNFs are properly normalized. Third, unlike kernel methods, CNFs are fully parametric, and, once trained, the inference time is constant . This is particularly important later in the third step of our method (Sec. 4.3): For each training step and training sample in the batch, the predicted conditional density has to be evaluated again under the updated policy, and, hence, a fast inference time of density is crucial to achieving a significant speed up when training the policy network.

In our method, we use _neural spline flows_, as they allow for flexible estimation of multimodal densities, and, in combination with masked auto-regressive networks , we can extend them to multiple dimensions. This combination allows for fast and exact evaluation of the multidimensional densities. We also set the base density, \(p(z)\), to standard normal distribution. The CNFs are trained by minimizing the negative log-likelihood loss \(_{f}=-_{i=1}^{n}(t_{i},x_{i})\).

In the following section, we use the estimated GPS \((t,x)\) during policy learning in order to avoid regions with limited overlap and thus ensure reliable decision-making.

### Constrained optimization to avoid regions with limited overlap

**Reliable decision-making**: Our aim is to find the policy \(^{}\) which maximizes the estimated policy value \(()\) while ensuring that \(V()\) can be estimated _reliably_. For this, we suggest to constrain our search space in the shared covariate-treatment space to regions that are sufficiently supported by data, i.e., where overlap is not violated. Formally, we rewrite our objective from Eq. (1) as

\[^{}*{arg\,max}_{^{}}\,( )^{}=\{\ \,f((x),x)>),\; x \},\] (5)

where \(\) is a reliability threshold controlling the minimum overlap required for our policy. In our setting with finite observational data, this yields the constrained optimization problem

\[_{}\ \ \ _{i=1}^{n}((x_{i}),x_{i} )\ \ \ \ \ \ ((x_{i}),x_{i}),\; i \{1,,n\}.\] (6)

Here, we use our DCNet as a plug-in dose-response estimator \((t,x)\), and we further use our conditional normalizing flows to restrict the search space to policies with a sufficiently large estimated GPS \((t,x)\).

**Neural network for reliable policy learning:** To learn the optimal reliable policy, we further suggest an efficient procedure based on gradient updates in the following. Specifically, we train an additional policy neural network to predict the optimal policy. Our choice has direct benefits in practice. _Why not just optimize over the dosage combinations for each patient individually?_ In principle, one could also use methods for constrained optimization and leverage the nuisance functions \(\) and \(\) to directly optimize over the dosage assignments \(t\) for each patient, i.e., \(t_{i}^{}=*{arg\,max}_{_{i}}[ (},x_{i})]\ \ (},x_{i} )\). However, this does _not_ warrant scalability at inference time, as it requires that one solves the optimization problem for _each_ incoming patient. The complexity of this optimization problem scales with both the number of dosages and the number of samples in the data to be evaluated. This makes treatment recommendations costly, which prohibits real-timedecision-making in critical care. As such, direct methods for optimization are rendered impractical for real-world medical applications. Instead, we suggest to directly learn a policy that predicts the optimal dosage combination for incoming patients.

As the constrained optimization problem in Eq. (6) cannot be natively learned by gradient updates, we must first transform Eq. (6) into an unconstrained Lagrangian problem

\[&_{}~{}-_{i=1}^{n} (_{}(x_{i}),x_{i})~{}(_{ }(x_{i}),x_{i}), i\\ &_{}_{_{i}  0}-_{i=1}^{n}\{(_{}(x_{i}),x_{i} )-_{i}[(_{}(x_{i}),x_{i})- ]\},\] (7)

where \(_{}(x_{i})\) is the policy learner with parameters \(\), and \(_{i}\) are the Lagrange multipliers for sample \(i\). The Lagrangian min-max-objective can be solved by adversarial learning using gradient descent-ascent optimization (see, e.g.,  for a background). Details are presented in the next section where we introduce our learning algorithm.

### Learning algorithm

The learning algorithm for our method proceeds along the following steps (see Algorithm 1):

\(\) (1) We use our DCNet to estimate the individualized dose-response function \((t_{i},x_{i})\) for dosage combinations.

\(\) (2) We estimate the GPS \((t_{i},x_{i})\) using the CNFs to detect areas with limited overlap.

\(\) (3) We plug the estimated \((t,x)\) and \((t,x)\) into the Lagrangian objective from Eq. (7). As the policy learner, we use a multilayer-perceptron neural network \(_{}(x)\) to predict the optimal dosage combination \(t\) given the patient covariates \(x\). We use stochastic gradient descent-ascent. In detail, we perform adversarial learning with alternating parameter updates. First, in every training step, we perform a gradient _descent_ step to update the parameters \(\) of the policy network \(_{}\) with respect to the policy loss

\[_{}(,)=-_{i=1}^{n}\{ (_{}(x_{i}),x_{i})-_{i}[( _{}(x_{i}),x_{i})-]\},\] (8)

where \(\) is the reliability threshold and where \(=\{_{i}\}_{i=1}^{n}\) are trainable penalty parameters. Then, we perform a gradient _ascent_ step to update the penalty parameters \(\) with respect to \(_{}\). Here, each \(_{i}\) is optimized for each sample \(i\) in the training set.

As there is no guarantee for global convergence in the non-convex optimization problem, we perform \(k=1,,K\) random restarts and select the best run as evaluated on the validation set by

\[_{}^{}=_{}^{(j)}, j= {arg\,max}_{k}\;_{i=1}^{n}(_{}^{(k)}(x_{i}),x_{i} )~{}~{}\{(_{}^{(k)}(x_{i}),x_{ i})\},\] (9)

where \(_{}^{(k)}\) is the learned policy in run \(k\) and \(\{\}\) denotes the indicator function. As a result, we select the policy \(_{}^{(j)}\) which maximizes \(()\) under the overlap constraint \((_{}^{(k)}(x_{i}),x_{i}),~{} i\). Note that we use the validation loss from Eq. (9) because \(_{}\) cannot be directly applied for evaluating the performance on the validation set, as \(_{i}\) can only be optimized for observations in the training set during the policy learning.

## 5 Experiments

We perform extensive experiments using semi-synthetic data from real-world medical settings to evaluate the effectiveness of our method. Semi-synthetic data are commonly used to evaluate causal inference methods as it ensures that the causal ground truth is available  and thus allows us to benchmark the performance.

**MIMIC-IV:** MIMIC-IV  is a state-of-the-art dataset with de-identified health records from patients admitted to intensive care units. Analogous to , we aim to learn optimal configurations of mechanical ventilation to maximize patient survival (\(Y\)). Here, treatments (\(T\)) involve the configuration of mechanical ventilation in terms of respiratory rate and tidal volume. When filtering for \(T\), we yield a dataset with \(n=5476\) patients. We select 33 variables as patient covariates \(X\) (e.g., age, sex, respiratory measurements, cardiac measurements). Unlike  where treatments \(T\) are modeled as \(p\) mutually exclusive dosages, we model the more realistic setting, where \(T\) are dosage combinations with a joint effect. Our choice is well aligned with medical literature according to which the parameters of mechanical ventilation are dependent and must be simultaneously controlled . Further details for MIMIC-IV are in Appendix B.

**TCGA:** TCGA  is a diverse collection of gene expression data from patients with different cancer types. Our aim is to learn the optimal dosage combination for chemotherapy (\(T\)). In chemotherapy, combining different drugs is the leading clinical option for treating cancer . Let patient survival denote the outcome (\(Y\)). We select the same version of the dataset as in , containing the measurement of 4,000 genes as features \(X\) from \(n=9659\) patients. Unlike previous works , we again model \(T\) as different, simultaneously assigned dosages with a joint effect. This follows medical practice where drug combinations in cancer therapy must be jointly optimized for each patient profile . Given that TCGA is high-dimensional, we expect it to be an especially challenging setting for evaluation. Further details for TCGA are in Appendix B.

In both datasets, the response \(Y N((T,X),0.5)\) can be interpreted as the log-odds ratio of patient survival, which is to be maximized. Due to the fact that we work with actual data from medical practice, certain dosage combinations conditional on the patients' features are more likely to be observed. Even other dosage combinations may not be assigned at all as they can cause harm. For our experiments, we introduce a parameter \(\) controlling the dosage bias, which changes the observed policy of dosage assignments from random uniform (\(=0\)) to completely deterministic (\(=\)). Hence, with increasing \(\), the areas in the covariate-treatment space will be increasingly prone to limited overlap. We refer to Appendix B for details. When not stated otherwise, we choose \(=2\) and \(p=2\) as the default setting for our experiments.

**Baselines:** To the best of our knowledge, there are no existing methods tailored to off-policy learning for dosage combinations under limited overlap. Hence, we compare our method with the following baselines, which vary different steps in our method and thus allow us to attribute the source of performance gain. Specifically, we vary dose-response estimation (\((t,x)\)) vs. optimization (\(\)). (1) For estimating \((t,x)\), we use two baselines: (i) a simple multi-layer perceptron (**MLP**) as a standard neural alternative to our DCNet, and (ii) **VCNet** as the SOTA method for dose-response estimation for _independent_ treatments with a separate head per treatment dimension. (2) For optimization, we minimize the policy loss \(-_{i=1}^{n}(_{}(x_{i}),x_{i})\) without constraints for dealing with limited overlap (**naive**).

**Evaluation:** For training, we split the data into train / val / test sets (64 / 16 / 20%). We first train the nuisance models \((t,x)\) and \((t,x)\). We then perform \(k=5\) random restarts to train \((t,x)\). We select the best run with respect to the policy loss on the factual validation dataset. Thereby, we carefully adhere to the characteristics in real-world settings, where one does _not_ have access to the true dose-response function \((t,x)\) on the validation set for model selection. In our method, we setthe reliability threshold \(\) to the \(5\%\)-quantile of the estimated GPS \((t,x)\) of the train set. Further details including hyperparameter tuning are in Appendix E.

For evaluation, we compare the methods on the test set using the regret \(R()=_{}V()-V()\). We report: (1) the regret for the selected policy, which is the policy with the best policy loss on the validation set out of all \(k\) policies; (2) the average regret across all \(k\) restarts; and (3) the standard deviation to better assess the reliability of the method.

**Results:** Table 1 compares the performance of our method against the baselines. We make the following observations: (1) DCNet performs similarly to MLP for low-dimensional data such as MIMIC-IV, as expected. However, we find substantial improvements from DCNet over MLP for high-dimensional data such as TCGA, showing the advantage of enforcing smoothness and expressiveness in our prediction head. (2) DCNet outperforms VCNet on both datasets, which demonstrates the importance of modeling the joint effect of dosage combinations. (3) Choosing a policy that is reliable greatly benefits overall performance. For example, the baselines (+naive) even perform worse than the observed policy for TCGA, whereas our method (+reliable) is clearly superior. (4) We also observe that our selection of reliable policies greatly reduces the standard deviation of the observed regrets. This further demonstrates the reliability of our proposed method. Overall, our method works best across both datasets. For example, the regret on the TCGA dataset for the selected policy drops from 2.52 (DCNet+naive) to only 0.03 (DCNet+reliable), which is a reduction by 98.8%. In sum, this demonstrates the effectiveness of our method.

**Comparison to discretization:** There exist several methods for off-policy learning for discrete treatments (see Sec. 2), including methods targeted for overlap violations and treatment combinations. In principle, we could also discretize the dosages in our setting and apply such methods. However, we would then lose information about the exact dosage values and could not exploit the continuity of the dose-response surface. To demonstrate the benefits of our method, we benchmark against an "oracle" discretized baseline where we assume perfect knowledge of the dose-response function and perfect individual treatment assignment. This serves as an upper bound for all policy learning methods with discretization. We display the results for different granularities of equally-spaced grid discretization in Table 2. We observe that, even in the oracle setting, discretization itself leads to a higher regret than our proposed method, which confirms the advantages of leveraging continuous dosage information.

**Robustness checks:** We evaluate the robustness of our method across settings with different levels of limited overlap. For this, we vary the dosage bias \(\) (in Fig. 2) and the number of dosages \(p\) (in Fig. 3). Here, we use DCNet for modeling \((t,x)\) in both methods, but then vary the policy selection (naive vs. reliable, as defined above). Our method (DCNet+reliable) shows robust behavior. As desired, it achieves a low regret and a low variation in all settings. In contrast, the naive method leads to high variation. This must be solely attributed to that our constrained optimization for avoiding regions with limited overlap leads to large performance gains. We also observe another disadvantage of the naive method: it may select a suboptimal policy out of the different runs due to the fact that the estimation of \((t,x)\) is erroneous in regions with limited overlap. Again, this underlines the benefits of our method for reliable off-policy learning of dosage combinations in settings with limited overlap. In Appendix C, we demonstrate the robustness of our method in further setups.

    &  &  \\  \(\) & \(\) & Selected & Mean & Std & Selected & Mean & Std \\  Oracle (\(\)) & observed & 1.21 & – & – & 1.06 & – & – \\  MLP & naive & 0.02 & 1.44 & 1.56 & 1.94 & 1.88 & 0.12 \\ MLP & reliable & 0.03 & 0.03 & 0.00 & 1.78 & 1.78 & 0.03 \\  VCNet & naive & 2.13 & 2.49 & 0.92 & 3.64 & 2.47 & 1.19 \\ VCNet & reliable & 0.07 & 0.07 & 0.00 & 0.06 & 0.06 & 0.00 \\  DCNet & naive & 2.81 & 2.07 & 1.17 & 2.52 & 0.94 & 1.21 \\ DCNet & reliable (**ours**) & 0.04 & 0.04 & 0.00 & 0.03 & 0.03 & 0.00 \\   & & & & & \\ 

Table 1: Performance against baselines. Regret on test set over \(k=5\) restarts.

    &  &  \\  \(\) & \(\) &  \\  Oracle (\(\)) & discrete (3x3) & 3.32 & 2.94 \\ Oracle (\(\)) & discrete (4x4) & 0.96 & 0.95 \\ Oracle (\(\)) & discrete (5x5) & 0.05 & 0.06 \\  DCNet & reliable (**ours**) & 0.04 & 0.03 \\   & & \\ 

Table 2: Performance against methods based on discretized treatments.

Shown: selected policy (line) and the range over 5 runs (area). The naive baseline has a large variability across runs while ours is highly robust as expected (i.e., we see no variability).

## 6 Discussion

**Limitations:** Our method makes an important contribution over existing literature in personalized treatment design for dosage combinations by adjusting for the naturally occurring but non-trivial problems of drug-drug interactions and limited overlap. However, the complexity of real-world data in medical practice can limit the applicability of our method in certain ways. (i) Our method does not account for the ignorability assumption, which can result in biased estimates in the case of unobserved confounders or other missing data. (ii) We apply our method in a static treatment setting, whereas, in several clinical applications, time-series data are available, e.g., sequences of varying dosages, multiple treatment cycles, and right-censored data. Thus, future work could extend our method to account for causal effect estimates in a time-varying setting (e.g., ). (iii) Our method relies on the - for medical applications reasonable - implicit assumption of smooth dose-response surfaces. In settings with extremely unsmooth or even stepwise dose-response surfaces, other methods may be more suitable for dose-response estimation. (iv) Our method aims at reliable policy learning by avoiding areas with limited overlap to minimize potential harm. In other applications such as marketing, one may want to explicitly target unreliable regions to acquire new customers.

**Broader impact:** Our method can have a significant impact in the field of medicine, where reliable dosing recommendations are crucial for effective treatment. Our approach addresses the limitations of previous methods in terms of reliability and estimating joint effects of dosage combinations, and, hence, can improve decision support for clinicians. The reliability of dosage recommendations is particularly important, as incorrect dosing can have serious consequences for patients, such as adverse effects or treatment failure. By addressing this gap, our approach provides a valuable tool for decision support in medicine, empowering healthcare professionals to make more personalized and safe treatment recommendations.

Figure 3: Robustness for number of dosages \(p\).

Figure 2: Robustness for dosage bias \(\).