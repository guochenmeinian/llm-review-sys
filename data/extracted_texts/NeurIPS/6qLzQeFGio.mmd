# Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?

Arjun Majumdar\({}^{1}\), Karmesh Yadav\({}^{1}\), Sergio Arnaud\({}^{2}\)

**Yecheng Jason Ma\({}^{3}\)**, Claire Chen\({}^{4}\), Sneha Silwal\({}^{2}\), Aryan Jain\({}^{5}\), Vincent-Pierre Berges\({}^{2}\)

**Tingfan Wu\({}^{2}\), Jay Vakil\({}^{2}\)**, Pieter Abbeel\({}^{5}\), Jitendra Malik\({}^{2,5}\), Dhruv Batra\({}^{1,2}\)

**Yixin Lin\({}^{2}\), Oleksandr Maksymets\({}^{2}\)**, **Aravind Rajeswaran\({}^{2}\), Franziska Meier\({}^{2}\)

\({}^{1}\)Georgia Tech, \({}^{2}\)Meta AI, \({}^{2}\)UPenn, \({}^{2}\)Stanford University, \({}^{2}\)UC Berkeley

https://eai-vc.github.io

Equal Contribution

Equal Contribution

###### Abstract

We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data size and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over \(4.3\)M images) and ImageNet to train differentiated vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does _not_ improve performance universally (but does so on average). Our largest model, named **VC-1**, outperforms all prior PVRs on average but does not universally dominate either. Next, we show that task- or domain-specific adaptation of **VC-1** leads to substantial gains, with **VC-1** (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. Finally, we present real-world hardware experiments, in which **VC-1** and **VC-1** (adapted) outperform the strongest pre-existing PVR. Overall, this paper presents no new techniques but a rigorous systematic evaluation, a broad set of findings about PVRs (that in some cases, refute those made in narrow domains in prior work), and open-sourced code and models (that required over 10,000 GPU-hours to train) for the benefit of the research community.

## 1 Introduction

The visual cortex is a region of an organism's brain, which together with the motor cortex, enables sight to be converted into movement. In this work, we ask the same question that Fukushima  asked nearly 50 years ago - how do we design an _artificial visual cortex_, the module in a computational system that (together with a policy) enables an agent to convert camera input into actions? In contemporary AI, this question has been operationalized as the design of pre-trained visual representations (PVRs) or visual 'foundation models' for embodied AI (EAI).3 Indeed, recent work has shown that PVRs can substantially improve performance and learning efficiency for navigation  and manipulation tasks . Unfortunately, prior studies are incommensurable - using different self-supervised learning (SSL) algorithms on different pre-training datasets, designedfor, and evaluated on different downstream EAI tasks. Naturally, one might ask: _Does an artificial visual cortex already exist?4_

To answer this question, we conduct the most comprehensive empirical study of visual foundation models for EAI to-date. We curate CortexBench, a benchmark for evaluating PVRs, consisting of 17 tasks spanning low-level locomotion , table-top manipulation , dexterous manipulation , multi-finger coordinated manipulation , indoor visual navigation , and mobile manipulation . The visual environments span from flat infinite planes to table-top settings to photorealistic 3D scans of real-world indoor spaces. The agent embodiments vary from stationary arms to dexterous hands to articulated mobile manipulators. The learning conditions vary from few-shot imitation learning to large-scale reinforcement learning. The exhaustiveness of this study enables us to draw conclusions with unprecedented scope and confidence.

Our first finding is a _negative result_. We discover that while existing PVRs generally outperform learning-from-scratch baselines, no single PVR is universally dominant. Instead, we find that PVRs tend to work best in the domains (navigation, manipulation etc.) they were originally designed for. We note that no claims of universality were made in prior work, so this finding is illustrative rather than refutative, but nonetheless a significant finding that was not known _a priori_. Overall, serendipity did not come to pass - an artificial visual cortex does not already exist.2 However, curiously, the _kinds of PVRs_ that are locally-dominant in CortexBench differ significantly in the size and type of pre-training datasets: CLIP  used \(400\)M image-text pairs from the web; MVP  used \(4.5\)M frames from web-images and many egocentric-video datasets - yet, each performs best on some subset of tasks in CortexBench. This leads to a natural question: _how does scaling model size, dataset size, or diversity affect performance on CortexBench?_ Can we use scaling as a means to learn a single PVR that works for all of the diverse tasks in CortexBench?

To study these questions, we combine over \(4{,}000\) hours of egocentric videos from 7 sources containing humans manipulating objects and navigating indoor spaces, together with ImageNet. From this union, we create 4 pre-training datasets of varying size and diversity, with the largest containing over \(5.6\)M images. We train vision transformers (ViT-B and ViT-L)  on these 4 datasets using Masked Auto-Encoding (MAE) , and systematically analyze their performance on CortexBench. To benefit the EAI community, we will open-source these models, which required over 10,000 GPU hours to train.

We do find evidence supporting the scaling hypothesis, but the picture that emerges is more nuanced than what a superficial reading might suggest. Our largest model trained on all data, named **VC-1**, outperforms the best existing PVR by 1.2% on average. However, **VC-1** does _not_ universally dominate either - i.e., there are PVRs trained on smaller amounts of data that outperform it on specific tasks. A similar trend emerges for data diversity - more is better on average, but not universally. For instance, the best performance on the Mobile-Pick task from Habitat 2.0  is achieved by pre-training on the subset of video data focused on manipulation; presumably because the mobility involved in the task is fairly limited. Thus, our second key finding is: _Naively scaling dataset size and diversity does

Figure 1: An artificial visual cortex for embodied intelligence must support a diverse range of sensorimotor skills, environments, and embodiments; we curate CortexBench to systematically measure progress towards this ambitious goal. Our strongest model, denoted **VC-1** (adapted), is _competitive with or outperforms the best prior results (success rates) on all benchmarks_ in CortexBench. This comparison is particularly unforgiving because best prior results are benchmark-specific and not constrained to share any aspect of their design.

not improve performance uniformly across benchmarks_. We note that this broad evaluation refutes a naive extrapolation of the positive scaling trends observed in prior work on robot manipulation .

Our findings reveal a challenge and opportunity for the community - the search for a PVR that is universally dominant (or "foundational") for EAI calls for innovations in architecture, learning paradigm, data engineering, and more. As a first step towards this open problem, we study _adapting_**VC-1** with either task-specific training losses or datasets (via MAE ) to specialize **VC-1** for each domain. We find that adapting **VC-1** results in it becoming competitive with or outperforming the _best prior results on all of the benchmarks_ in CortexBench. We highlight that this comparison is particularly unforgiving, since best prior results are highly domain-specific and are not constrained to share any aspect of their design. To our knowledge, **VC-1** (adapted) is the first PVR that is competitive with (or outperforms) state-of-art results on such a diverse set of EAI tasks (Figure 1).

Finally, we conduct proof-of-concept hardware experiments using **VC-1** in a few-shot imitation learning setting with two platforms: a TriFinger robot and a Franka Emika Panda arm. In this real-world setting, we find that **VC-1** and **VC-1** (adapted) substantially outperform pre-existing PVRs like MVP . We will release code for CortexBench to enable the EAI, robotics, and CV communities to benchmark their own models, and share our pre-trained models (including **VC-1**) that we believe can serve as a starting point for all visuomotor tasks of interest today.

## 2 Related Work

**Pre-trained visual representations (PVRs).** The last few years have seen increasing interest in the self-supervised learning (SSL) of visual representations [18; 19; 20; 21; 22]. These algorithms use contrastive [21; 22], distillation-based [19; 20], or reconstructive [18; 23] objectives for training. Recently, a flurry of works have proposed using the vision transformers (ViTs)  with masked image modeling [18; 25; 26], which among other benefits reduces the computation time required for pre-training. In this work, we use one such pre-training algorithm (MAE ) to explore scaling and adapting pre-trained visual representations.

**PVRs for embodied AI.** Inspired by the advancements in self-supervised learning, recent work has incorporated visual representation learning into the training pipelines for EAI agents [3; 4; 5; 6; 7; 8; 9]. Specifically,  evaluate several PVRs trained with supervised or self-supervised learning on a range of EAI tasks, demonstrating promising results under a few-shot imitation learning evaluation protocol. [7; 8; 9] introduce new methods for pre-training visual representations using egocentric video data, targeting robot manipulation tasks. Similarly, [3; 4; 5] use pre-trained visual representations to improve performance on multiple visual navigation tasks. Closely related,  demonstrate that MAE pre-training on internet-scale video and image data can produce effective visual representations for robot manipulation tasks. In contrast, our work studies a larger range of embodied AI tasks (collected in CortexBench) to understand how PVRs can provide a general-purpose foundation for embodied agents and explores in-domain model adaptation for various tasks.

**Language guided foundation models in EAI.** There has also been some recent works in the area of language-guided representation learning for control.  trains a ViT with a masked encoding objective on pairs of image frames and text.  focuses on self-supervised representation learning for goal-conditioned value functions using language-aligned videos. Additionally, [29; 30] employ open-vocabulary detectors and vision-language models to detect objects in tabletop views. These detections, along with the image and vision-language model instructions, are then used to train a policy. In , a multimodal transformer is pretrained on web-scale image-and-text data, and then used with a transformer-based policy for table-top manipulation tasks.

**Scaling model and dataset size.** Several works show that scaling model and dataset size improves performance on vision tasks like image classification [32; 33; 34]. In EAI, Radosavovic et al.  find that scaling model and data sizes consistently improves downstream policy performance for robot manipulation tasks. Our work is the first to study this question of scaling on a broad range of EAI tasks and refutes a naive extrapolation of the positive scaling trends observed in .

**Adapting PVRs.** When and how to adapt PVRs for downstream applications remains an open research question [35; 36; 37; 38; 39]. In the context of EAI,  and  show that naively fine-tuning PVRs with behavior cloning can reduce performance in simulation, and  observe minimal gains in real-world manipulation tasks. In large-scale RL settings, [4; 5] show that end-to-end finetuning considerably improves performance for indoor visual navigation. By comparison,  find simple \(k\)-nearest-neighbor adaptation works well for real-world visual imitation tasks. Our work neither aims nor expects to be the final word on this fertile topic.

## 3 Benchmarking Progress Towards an Artificial Visual Cortex

We curate CortexBench (as shown in Figure 1) to evaluate the ability of pre-trained visual representations (PVRs) to support a wide variety of EAI applications. Specifically, we chose 17 diverse tasks drawn from 7 existing EAI benchmarks that have been deemed important by the EAI community. For each task, we delineate a downstream policy learning paradigm (e.g., few-shot imitation learning) and evaluation protocol that follows community standards in each domain (Appendix A.2). By fixing the tasks and downstream learning methods (Figure 2), we are able to focus evaluations on the contribution of PVRs, which in turn allows measuring progress towards the development of an artificial visual cortex for embodied intelligence. We use CortexBench to conduct the largest and most comprehensive empirical study to-date.

We recommend two evaluation metrics: **Mean Success** and **Mean Rank**. **Mean Success**: the average success rate across all benchmarks. **Mean Rank**: for each benchmark, we rank PVRs based on their success rate; then we average these rankings across all benchmarks.

### Embodied AI Tasks in CortexBench

CortexBench includes the tasks listed in Table 1, illustrated in Figure 1, and described here:

**Adroit (AD)** is a suite of dexterous manipulation tasks in which an agent must control a 28-DoF anthropomorphic hand to perform a variety of tasks. We study the two hardest tasks from Adroit: \(\) and \(\)-\(\). In these tasks, an agent must manipulate an object into a goal position and orientation, where the goal must be inferred from the scene.

**MetaWorld (MW)** is a collection of tasks in which agents command a Sawyer robot arm to manipulate objects in a tabletop environment. We consider five tasks from MetaWorld: \(\), \(\)-\(\), \(\)-\(\), \(\)-\(\), and \(\), following the evaluations in .

**DeepMind Control (DMC)** is a widely studied image-based continuous control benchmark, in which agents perform locomotion and object manipulation tasks. We consider five DMC tasks: \(\)-\(\), \(\)-\(\), \(\)-\(\), \(\)-\(\), and \(\)-\(\), following .

**TriFinger (TF)** is a robot, introduced in , composed of a three-finger hand with 3-DoF per finger. We focus on the \(\)-\(\) task, which was a part of the Real Robot Challenge 2020 . In this tasks, the agent may use all three fingers to push the cube and move it to a goal location (\(\)-\(\)). Additionally, we consider the easier \(\)-\(\) task, which was also studied in .

**Habitat** is a simulation platform that includes several visual navigation tasks in which agents explore highly photo-realistic unseen 3D environments. We consider two tasks in Habitat: image-goal navigation (\(\))  and object-goal navigation (\(\)) . In both, the agent starts at a random location in an unknown 3D environment and must find a goal location - specified with an image from the goal location in \(\) or the name of an object (e.g., 'chair') in \(\)

  
**Benchmark** & **Observation** & **Action** & **Goal** & **Policy** \\
**Side** & **Space** & **Space** & **Specification** & **Learning** \\  Adroit (AD) & RGB \(\) & Continuous & - & IL \\ Model(MW) & RGB \(\) & Continuous & - & IL \\ DoF(DMC) (DMC) & RGB \(\) & Continuous & - & IL \\ \(\) (DT) & RGB \(\) & Continuous & Goal & \(\) & IL \\ \(\) (DT) & RGB \(\) & Continuous & \(\) & IL \\ \(\) (DT) & RGB \(\) & Continuous & Goal & \(\) & IL \\ \(\) (DT) & RGB \(\) & Continuous & Goal & \(\) & IL \\ \(\) (DT) & RGB \(\) & Continuous & Goal & \(\) & IL \\   

Table 1: CortexBench includes tasks from 7 diverse benchmarks with different combinations of observations, actions, goals, and standard policy learning paradigms.

Figure 2: CortexBench: We systematically evaluate pre-trained visual representations by varying datasets and representation learning algorithms, coupled with reinforcement or imitation learning on diverse EAI tasks.

**Habitat 2.0** includes mobile manipulation tasks in which agents control a Fetch robot with a 7-DoF arm, mobile base , and suction gripper to rearrange objects in apartment scenes. We consider a challenging version of the Mobile-Pick (MP) task from Habitat 2.0, in which an agent must pick up a target object from a cluttered receptacle (e.g., a counter) while starting from a position in which the object is outside of the robot's reach (thus, requiring navigation), using a relaxed dense goal specification (described in Appendix A.3).

**Discussion**CortexBench encompasses a wide range of tasks, from navigation to manipulation and locomotion. These tasks employ different policy learning methods, including low-level imitation learning (MW, DMC, and TF) and large-scale reinforcement learning (ImageNav, ObjectNav, and Habitat 2.0). Some tasks require a high-level understanding of semantic scene information (ImageNav and ObjectNav), while others focus on minor changes in the agent's pose for low-level control (DMC). This diversity in CortexBench allows us to draw generalized new conclusions about existing and new PVRs.

## 4 Do we already have a visual foundation model for EAI?

First, we evaluate several existing PVRs on CortexBench to study whether existing open-sourced visual backbones can consistently perform well across all tasks. For all evaluations preceding Section 6, we consider frozen visual representations to disentangle the effect of learned representations from downstream task learning. Specifically, we include the following models:

* CLIP  Contrastive image-language pre-training objective; Trained on 400M images-text pairs from the internet (WIT); ViT-B backbone.
* R3M  Time-Contrastive video-language alignment pre-training objective; Trained on 5M images from a subset of Ego4D; ResNet-50 backbone.
* MVP . Masked Auto Encoding (MAE) pre-training objective; Trained on 4.5M images from egocentric videos and ImageNet; ViT-B and ViT-L backbones.
* VIP . Goal-conditioned value function pre-training objective; Trained on 5M images from a subset of Ego4D; ResNet-50 backbone.

These models form a representative set for comparisons, spanning different architectures, pre-training objectives and datasets. Additionally, we include randomly initialized ViTs with frozen- and fine-tuned weights to assess the necessity of pre-training and the limitations of pure in-domain learning.

Table 2 shows the evaluation results aggregated by benchmark; no single model excels in all cases. Among all of the models evaluated, R3M performs the best on Adroit, MetaWorld, and DMControl. While MVP (ViT-L) performs best on TriFinger, ImageNav, and Mobile Pick. CLIP, on the other hand, achieves the best results on ObjectNav. The variance in performance of existing PVRs on CortexBench is further illustrated in Figure 5 in Appendix A.4 and highlights that we do not yet have a single, strong performing artificial visual cortex for EAI.

## 5 Analyzing the Scaling Hypothesis for EAI

The previous section investigated models pre-trained on datasets of varying size and diversity. Interestingly, while the model pre-trained on the largest dataset (CLIP) performs well on one benchmark (ObjectNav) it does not perform well across all tasks. We now ask: _how much does the relevance and

    & &  &  &  \\   \# & **Model** & & **A**droit & **M**e**droit & **D**MControl & **TriFinger & **O**i**berNav & **ImageNav** & **Mobile Pick** & **R**ank & **S**ecess \\ 
1 & Best prior result (any setting) & 75 & 80 & 77 & - & 70.4 & 32.0 & - & - & - \\
2 & Best prior result (frozen PVR) & 75 & 80 & 77 & - & 54.4 & 61.8 & - & - & \\ 
3 & Random (ViT-B) Frozen & 20 \(\) 0.0 & 0.5 \(\) 0.5 & 10.1 \(\) 0.6 & 57.8 \(\) 0.5 & 1.2 \(\) 0.9 & 19.3 \(\) 0.9 & 4.21 \(\) 0.8 & 10.8 \(\) 1.4 & 7.2 & 20.4 \\
4 & Random (ViT-L) Frozen & 2 \(\) 1.8 & 0.5 \(\) 0.5 & 9.1 \(\) 0.2 & 57.1 \(\) 0.9 & 19.3 \(\) 0.9 & 45.2 \(\) 0.8 & 20.6 \(\) 1.8 & 6.9 \(\) 2.1 \\
5 & Random (ViT-B) Fine-tuned & 40.0 \(\) 2.0 & 49.9 \(\) 7.3 & 43.5 \(\) 2.4 & 56.1 \(\) 1.3 & 28.5 \(\) 1.0 & 62.5 \(\) 0.7 & 47.6 \(\) 2.2 & 5.3 & 47.4 \\ 
6 & MVP (ViT-B) & 48.0 \(\) 3.3 & 9.1 \(\) 2.9 \(\) 6.9 & 4.2 \(\) 1.5 & 59.7 \(\) 0.3 & 52.1 \(\) 1.1 & 64.7 \(\) 0.7 & 56.0 \(\) 2.2 & 3.1 & 62.4 \\
7 & MVP (ViT-B) & 53.3 \(\) 4.1 & 87.5 \(\) 3.4 & 69.2 \(\) 1.5 & 47.1 \(\) 0.3 & 55.0 \(\) 1.1 & 68.1 \(\) 0.7 & 65.6 \(\) 2.1 & 2.1 & 67.5 \\
8 & CLIP (ViT-B) & 47.3 \(\) 3.0 & 75.5 \(\) 3.3 & 53.5 \(\) 1.4 & 62.0 \(\) 0.5 & 36.6 \(\) 1.1 & 52.2 \(\) 0.8 & 49.8 \(\) 2.2 & 3.9 & 57.0 \\
9 & VIP (ViT-B) & 54.0 \(\) 4.8 & 90.1 \(\) 2.2 & 72.5 \(\) 2.7 & 67.0 \(\) 0.2 & 26.4 \(\) 1.0 & 48.8 \(\) 0.8 & 7.8 \(\) 2.2 & 4.0 & 52.3 \\
10 & R3M (RN-50) & 73.3 \(\) 2.0 & 96.0 \(\) 1.1 & 81.1 \(\) 0.7 & 69.2 \(\) 0.8 & 22.7 \(\) 0.9 & 30.6 \(\) 0.7 & 33.2 \(\) 2.1 & 3.4 & 58.0 \\   

Table 2: Performance of **frozen** pre-trained visual representations (PVRs) on CortexBench. Best prior results are the best reported in literature prior to this work. Overall, we find that no single PVR consistently performs the best across all benchmarks. However, we find that several of these pre-trained models often outperform a random training from scratch baseline. Best prior results sources (row 1): Adroit and MetaWorld approximated from , DMControl from , ImageNav from , ObjectNav from . Frozen PVR Sources (row 2): Adroit, MetaWorld, and DMControl are the same as SOTA, ImageNetNav from , ObjectNav from .

diversity of the pre-training dataset and the model size matter?_ To study this, we fix the pre-training objective (MAE ) and vary the composition of the pre-training dataset and the size of the visual backbone (ViT-B with 86M and ViT-L with 307M parameters). We measure the corresponding changes in performance on CortexBench. MAE is selected for these experiments due to the strong performance on CortexBench of MVP  (Table 2), which uses the MAE pre-training objective.

### Constructing a Pre-training Dataset for EAI

To evaluate the impact of dataset size and diversity on CortexBench, which involve navigation and manipulation tasks, we employ a combination of 7 datasets. One cluster of datasets - Ego4D , 100 Days of Hands (100DOH) , Something-Something v2 (SS-V2) , and Epic Kitchens  - contains videos of people manipulating objects and is comparable to the datasets used in MVP . A second cluster consists of egocentric indoor navigation datasets: the Real Estate 10K dataset  and the OpenHouse24 dataset (described in Appendix A.5.1). Finally, we include ImageNet . We strategically select dataset combinations (shown in Table 3) to answer the following questions:

* What is the impact of scaling dataset size and diversity?
* How do _less-relevant_ datasets influence the performance of PVRs on EAI tasks?

**Ego4D** (our base dataset) includes a wide range of egocentric videos consisting of _daily life activities_ such as home, leisure, transportation, and workplace activities.

**Ego4D+M** extends **Ego4D** with videos from 100DOH, SS-v2, and Epic Kitchens, resulting in 3.5M frames and making this dataset primarily focused on object manipulation scenarios.5

**Ego4D+N** extends **Ego4D** with two indoor navigation datasets: OpenHouse24 and RealEstate10K. This dataset is similar in size to **Ego4D+M** (3.5M frames) but is more diverse because it contains a larger proportion of navigation data than the manipulation-centric datasets **Ego4D** and **Ego4D+M**.

**Ego4D+MN** combines **Ego4D** with the three object manipulation-centric datasets and two indoor navigation dataset, resulting a dataset with 4.3M frames. While larger than **Ego4D+M** and **Ego4D+N**, it does not include any new types of data beyond the manipulation and navigation videos in the previous subsets. Thus, it is no more diverse than **Ego4D+N** (which includes both types of data).

**Ego4D+MN** includes **Ego4D**, all manipulation and navigation datasets, and ImageNet for a total of 5.6M frames. This dataset is used to study the role of internet images for our benchmark tasks.

### Scaling Hypothesis Findings

We now turn to analyzing the effect of increasing model size, dataset size, and dataset diversity. The full set of results is shown in Figure 3 and Table 4. The key takeaways are:

**Model Size.** We find that increasing model size positively impacts performance on CortexBench. Specifically, in Figure 2(a), we find that with all pre-training datasets, switching from ViT-B to ViT-L

Figure 3: Visualization of scaling hypothesis model performance averaged over CortexBench. We see modest but positive scaling trends in both (a) scaling model size and (b) dataset diversity. (c) Average ranking of existing PVRs (Table 2) and scaling models (Table 4); **VC-1**: **Ego4D+MNI** (ViT-L) has the highest average rank.

   Name & Frames Used \\ 
**Ego4D** & 2,790,520 \\
**Ego4D+M** (Manip) & 3,538,291 \\
**Ego4D+N** (Nav) & 3,593,049 \\
**Ego4D+MN** (Manip, Nav) & 4,340,820 \\
**Ego4D+MNI** (Manip, Nav, ImageNet) & 5,621,987 \\   

Table 3: Datasets assembled to study effects of pre-training dataset size, diversity, and relevance â€“ the largest (**Ego4D+MNI**) has 5.6M frames. More details in A.5.

improves average performance on CortexBench. However, in Table 4, we find exceptions where this general trend does not hold. For instance, when pre-trained on **Ego4D+MNI**, the ViT-B model outperforms the ViT-L model on MetaWorld and TriFinger.

**Dataset Size and Diversity.** Figure 2(b) shows that, in general, increasing dataset size and diversity leads to improved performance. Models are are ordered from right to left by increasing size and the diversity of their pre-training dataset, and we mostly see improvements for both ViT-B and ViT-L. For instance, **Ego4D+M** slightly improves upon **Ego4D** by 0.6 and 0.9 points (62.2 \(\) 62.8 and 63.5 \(\) 64.4) in the case of ViT-B and ViT-L, respectively. The gains with **Ego4D+N** are larger and it outperforms **Ego4D** by 1.6 points using ViT-B (62.2 \(\) 63.8) and by 3.6 points for ViT-L (63.5 \(\) 67.1). It is interesting to note that **Ego4D+N** has a larger improvement over the base **Ego4D** dataset than **Ego4D+M**, even though **Ego4D+N** and **Ego4D+M** dataset are similar in size. In these results, we find that increasing diversity by adding indoor navigation data improves performance more than adding additional manipulation data to **Ego4D**.

Additionally, we find that pre-training on **Ego4D+MN** is roughly on par with pre-training on **Ego4D+N**. We see a 0.3 and 0.1 point difference (63.8 \(\) 64.1 and 67.1 \(\) 67.2) for ViT-B and ViT-L, respectively, even though **Ego4D+MN** has about 800K more training frames. Together with the results from above this demonstrates that increasing data diversity seems to matter more than simply increasing dataset size.

Next, we find that adding ImageNet positively impacts average performance on CortexBench. For example, models pre-trained on **Ego4D+MN** outperform those pre-trained on **Ego4D+MN** by 1.9 points (64.1 \(\) 66.2) for ViT-B and 1.5 points (67.2 \(\) 68.7) for ViT-L. Interestingly, these results demonstrate that including static internet images can significantly boost performance on EAI tasks. This finding further highlights the importance of seeking data diversity to build better representations.

Finally, our largest model (ViT-L) pre-trained on all datasets (**Ego4D+MNI**), achieves the best rank when averaged across all benchmark tasks (Table 4 row 11), with a mean rank of 2.4. We call this model **VC-1**, and will open-source it. **VC-1** is superior to the second-best model (**Ego4D+MN** ViT-L, Table 4 row 9), which has an average rank of 3.1.

However, upon further dis-aggregation, we observe we find that while **VC-1** performs best on average, it is not the best for each benchmark. For example, the best model for Mobile Pick, a mobile manipulation task, is a ViT-L trained on **Ego4D+M** and the best model for ImageNav, an indoor navigation task, is the ViT-L trained on **Ego4D+N**. These findings suggest that task-specific pre-training datasets could enhance the performance of models on individual tasks. However, it is important to note that this approach would lead to multiple pre-trained models, each tailored to a specific task, and not a unified visual foundation model.

### How does VC-1 compare to existing PVRs?

We now compare **VC-1** to PVRs from Section 4. On average, **VC-1** has the best rank across all benchmarks (Figure 2(c)). In terms of mean success, **VC-1** (Table 4 row 11) outperforms MVP (ViT-L) by +1.2 points (67.5 \(\) 68.7), R3M by +10.7 (58.0 \(\) 68.7), CLIP by +11.7 (57.0 \(\) 68.7), and end-to-end fine-tuning from scratch +19.6 (49.1 \(\) 68.7).

Impressively, **VC-1** outperforms CLIP _on every benchmark_ (Figure 4), despite training on a 70X smaller dataset, emphasizing the importance of egocentric interaction datasets. **VC-1** also outperforms fine-tuning from scratch on every benchmark, indicating that PVRs trained with out-of-domain data can outperform in-domain, end-to-end learning.

   \# & Model & Advert & Meta-World & DMControl & TriFinger & ObjectNav & ImageNav & Mobile Pick & Mean Rank & Mean Success \\ 
1 &  &  &  &  &  &  &  &  \\
2 & Rand (ViT-B) due-tuned & 44.0 & 49.9 & 34.2 & 55.0 & 28.5 & 65.0 & 47.6 & & & \\
3 & Best Result Table (2) (Forum Proto-trained, ViT-B) & 73.3 & 96.0 & 81.1 & 74.1 & 56.6 & 68.1 & 65.4 & & & \\
4 & Ego4D (ViT-B) & 48.7 \(\) 1.3 & 86.1 \(\) 2.1 & 64.1 \(\) 2.3 & 88.3 \(\) 1.6 & 46.8 \(\) 1.1 & 65.4 \(\) 0.7 & 57.4 \(\) 2.2 & 8.6 & 62.2 \\
5 & Ego4D (ViT-B) & 50.0 \(\) 1.2 & 89.2 \(\) 2.9 & 60.8 \(\) 2.3 & 67.0 \(\) 2.5 & 47.6 \(\) 1.1 & 55.8 \(\) 0.8 & 67.6 \(\) 2.1 & 5.9 & 63.5 \\
6 & Ego4D+N (ViT-B) & 50.0 \(\) 2.4 & 86.4 \(\) 2.9 & 55.5 \(\) 2.4 & 67.8 \(\) 1.3 & 54.7 \(\) 1.1 & 68.7 \(\) 0.7 & 59.4 \(\) 2.2 & 7.2 & 63.8 \\
7 & Ego4D (ViT-B) & 54.0 \(\) 1.2 & 89.1 \(\) 2.9 & 60.4 \(\) 1.6 & 66.9 \(\) 0.4 & 47.3 \(\) 1.1 & 70.5 \(\) 0.7 & 65.2 \(\) 2.1 & 3.5 & 67.1 \\
8 & Ego4D+MNI (ViT-L) & 51.3 \(\) 2.4 & 83.3 \(\) 2.6 & 64.2 \(\) 1.8 & 69.1 \(\) 0.4 & 47.3 \(\) 1.1 & 65.8 \(\) 0.7 & 59.8 \(\) 2.2 & 7.0 & 63.0 \\
9 & Ego4D+MN (ViT-L) & 52.0 \(\) 1.3 & 83.3 \(\) 2.6 & 67.2 \(\) 2.4 & 64.7 \(\) 0.9 & 47.3 \(\) 1.1 & 65.5 \(\) 0.7 & 68.6 \(\) 2.1 & 6.0 & 64.4 \\
10 & Ego4D+MNI (ViT-L) & 48.7 \(\) 2.4 & 85.3 \(\) 2.2 & 62.4 \(\) 1.9 & 73.0 \(\) 0.5 & 52.8 \(\) 1.1 & 68.9 \(\) 0.7 & 58.6 \(\) 2.2 & 6.9 & 64.1 \\
11 & Ego4D+MN (ViT-L) & 52.7 \(\) 4.2 & 86.7 \(\) 3.9 & 69.7 \(\) 3.3 & 72.4 \(\) 0.5 & 55.8 \(\) 1.1 & 69.9 \(\) 0.7 & 61.2 \(\) 2.2 & 3.1 & 67.2 \\
12 & Ego4D+MNI (ViT-B) & 54.0 \(\) 4.0 & 89.6 \(\) 3.9 & 63.8 \(\) 2.7 & 72.2 \(\) 0.6 & 55.4 \(\) 1.1 & 67.9 \(\) 0.7 & 60.6 \(\) 2.2 & 4.4 & 66.2 \\
11 & **VC-1** (Ego4D+MNI) & 59.3 \(\) 5.2 & **88.8 \(\) 2.2 & **66.9 \(\) 1.4** & 71.7 \(\) 0.4 & **60.3 \(\) 1.1** & 70.3 \(\) 0.7 & 63.2 \(\) 2.2 & **2.4** & **68.7** \\   

Table 4: Performance of scaling hypothesis models on CortexBench. We find that on average the **VC-1** Ego4D+MNI** (ViT-L) model performs best, but is not the best for each benchmark.

The MVP model is the most similar in terms of results, architecture, and pre-training objective to **VC-1**, with the main difference being the addition of a _convolutional stem_ in MVP. **VC-1** outperforms MVP VIT-L by 1.3 points on mean success and performs better on four out of seven benchmarks (Figure 4), likely due to the use of a more diverse dataset.

When compared to R3M, **VC-1** demonstrates superior performance both on average and in 4 out of 7 benchmarks (Figure 4). However, R3M outperforms **VC-1** on Adroit, MetaWorld and DMControl benchmarks. The cause of this gap is unclear - it could be due to differences in pre-training objectives, datasets, or the backbone architecture. That said, the fact that R3M, which uses a ResNet-based architecture, performs better on some tasks than **VC-1** which employs a Transformer-based backbone, suggests the potential value of exploring new architectures that integrate the strengths of both approaches - inductive biases and scalability. Taken together, these observations highlight the need for more robust and standardized evaluations on benchmarks like CortexBench.

Overall, **VC-1** is effective across a broad set of tasks and thus a reasonable starting point for novel EAI problems. However, it is not always the best model for a specific task. This leads us to theorize that there is a domain gap that might be bridged with dataset engineering or adaptation of the PVR.

## 6 Adapting VC-1

In prior sections, we focused on evaluating **VC-1** as a **frozen** PVR. We now study if _adapting_**VC-1** can improve results in downstream tasks. We use a broad definition of adaptation , which, in the context of large pre-trained foundation models, can take several forms from simple prompting , to selectively updating some or all weights of the backbone .

In the context of PVRs for EAI, adaptation can serve at least two purposes. The first is _task-specialization_ in the feature extraction stage. Since **VC-1** was trained with MAE , it captures features that are generally useful for reconstructing images (see **VC-1** attention maps in Appendix A.10). Adaptation can specialize the visual backbone to extract features required for performing specific EAI tasks. Secondly, adaptation can also help _mitigate domain-gap_ that might exist between pre-training and evaluation settings. In general, domain-gap can arise for several reasons such as poor coverage in pre-training datasets or deployment in novel conditions (e.g., on robots) not seen in the pre-training data (e.g., in human-centric video datasets). Domain gap is naturally instantiated in our setup, since **VC-1** was pre-trained on real-world, human video data while our downstream evaluation in CortexBench uses simulated EAI domains with different visual characteristics.

In this work, we explore two methods for adaptation: end-to-end (E2E) fine-tuning and MAE adaptation. While we do not explore prompting-based adaptation, because our visual encoders were not originally designed to utilize prompts, it may be an interesting direction for future work.

**End-to-end (E2E) fine-tuning** with a task-specific loss function can in-principle capture both of the aforementioned benefits of adaptation, and is widely used in computer vision literature . To study E2E fine-tuning of **VC-1**, we use the same policy learning methods described in Appendix A.2, except we allow updates to the **VC-1** weights. In the CortexBench results in Table 5, we find an interesting mixed result. In domains that involve large-scale IL or RL (ObjectNav, ImageNav, and Mobile Pick), the strategy proposed in  of adapting **VC-1** with E2E fine-tuning significantly improves performance over using a frozen **VC-1** backbone. Specifically, we see an improvement in ObjectNav success rate (SR) of +7.4 (60.3 \(\) 67.7), ImageNav SR of +11.3 (70.3 \(\) 81.6), and Mobile Pick SR of +10.8 (63.2 \(\) 74.0). These results suggest that E2E

Figure 4: Comparison of **VC-1** with existing PVRs. **VC-1** matches or exceeds existing PVRs on all benchmarks except R3M on AD, MW, and DMC, indicating an opportunity for model adaptation.

fine-tuning of **VC-1** can achieve the benefits of both task-specialization and domain adaptation. A comparison of **VC-1** attention maps before and after adaptation is provided in Appendix A.10.

However, in few-shot IL domains (Advit, MetaWorld, DMC, and TriFinger), E2E fine-tuning does not result in improvements. In fact, in these domains, it leads to a drop in performance, a finding consistent with prior work [6; 40]. We hypothesize that the poor performance of E2E fine-tuning in few-shot IL domains is caused by overfitting, due to fine-tuning a large model with 307M parameters on a small dataset (\( 50K\) frames).

**MAE adaptation to mitigate domain-gap.** As an alternative to E2E fine-tuning, we explore adapting **VC-1** with self-supervised learning (SSL). Specifically, in MAE adaptation we continue training the backbone network with the MAE  pre-training objective on task-specific data. Then, we freeze these adapted representations and use them to learn task-specific policies. We note that in MAE adaptation, the backbone is adapted using the same data that is used for training the policy (e.g., frames from expert demonstrations), and no additional in-domain datasets are used. While this adaptation strategy cannot address task-specialization, it may serve to mitigate domain gap.

For MAE adaptation, we initialize with **VC-1** weights, and then train with MAE for 100 epochs. In domains where expert demonstrations are available (i.e., Adroit, MetaWorld, DMControl, TriFinger, and ObjectNav), we use the RGB frames from these demonstrations for adaptation. In the remaining two benchmarks (ImageNav and Mobile Pick) we sample frames from training environments to create adaptation datasets. Finally, to isolate the importance of initializing with **VC-1** weights, we train in-domain MAE baselines by starting from a random initialization and then following the same approach used for MAE adaptation.

In the CortexBench results in Table 5, we observe MAE adaptation substantially improves performance in few-shot learning domains. Specifically, on Adroit performance improves by +12.7 (59.3 \(\) 72.0), MetaWorld by +7.2 (88.8 \(\) 96.0), DMC by +14.0 (66.9 \(\) 80.9), TriFinger by +8.9 (71.7 \(\) 80.6). Interestingly, in DMC and TriFinger, the in-domain MAE baseline (row 3) performs surprisingly well, highlighting the importance of in-domain data for representation learning. Finally, in large-scale IL or RL domains (ObjectNav, ImageNav, and Mobile Pick), we find MAE adaptation results in small reductions in performance from **VC-1** (row 4 vs. 6). In these domains, where substantial amounts of data is available for task-specific training (large-scale IL or RL), we find that E2E fine-tuning is the superior approach for adaptation. In aggregate, these results suggests that MAE adaptation can be explored as a powerful alternative in few-shot domains or where E2E fine-tuning fails.

Overall, we find that _adapting_ the **VC-1** model leads to performance improvement in all the benchmark domains. Furthermore, on MetaWorld, DMControl, and TriFinger, **VC-1** with MAE adaptation (row 6) is comparable with the best known results (SoTA) and the best results from previous sections (rows 1 and 2). Similarly, on ImageNetNav and Mobile Pick, **VC-1** with E2E fine-tuning (row 5) matches or exceeds the best results. Together, these results demonstrate that **adaptation** of PVRs can be a powerful paradigm for EAI, especially when compared to training representations from scratch.

## 7 Proof-of-Concept Hardware Experiments

In addition to simulation experiments with CortexBench, we also explore proof-of-concept hardware experiments utilizing **VC-1** as a backbone PVR for training policies with IL. Our hardware evaluation spans two platforms: TriFinger (1 task) and a Franka-Emika Panda arm (4 tasks). Setup details are provided in Appendix A.11 and A.12. We follow a similar experiment protocol to the simulated counterparts by studying few-shot imitation learning, where the demonstrations are collected directly in the real-world via tele-operation or hand-designed controllers.

    & & &  &  \\    & Method & Adroit & MetaWorld & DMControl & TriFinger & ObjectNav & ImageNav & Mobile Pick & TP Push-Cotre & Parika \\ 
**1** & Best proof result & 75 & 80 & 77 & - & 70.4 & 82.0 & - & - & - & - \\
2 & Best proof result (error ) & 73.3 \(\) 2.0 & 96.1 \(\) 1.1 \(\) 0.7 & 74.1 \(\) 0.3 & 96.3 \(\) 1.1 & 79.5 \(\) 0.7 & 68.6 \(\) 2.1 & 31.9 \(\) 4.3\({}^{*}\) & 55.0 \(\) 10 \({}^{*}\) \\
3 & In-domain MAE (baseline) & 47.3 & 83.4 & 776 & 80.4 \(\) 0.32 & 93.9 \(\) 1.09 & 47.6 \(\) 0.77 & 51.6 \(\) 2.23 & 22.9 \(\) 5.4 & 35.0 \(\) 13 \\ 
4 & **VC-1** & 93.5 \(\) 5.2 & 88.8 \(\) 2.2 \(\) 6.9 \(\) 1.1 \(\) 0.7 & 64.0 \(\) 1.1 & 70.3 \(\) 1.7 & 70.3 \(\) 6.7 & 63.2 \(\) 2.2 & 45.8 \(\) 6.8 \(\) 6.5 & 70.0 \(\) 109 \\
5 & **VC-1** & 82E fine-tuning & 15.9 \(\) 9.8 & 22.7 \(\) 9.7 & 67.3 \(\) 3.8 & 70.9 \(\) 0.5 & 67.7 \(\) 1.85 & 81.6 \(\) 7.6 & 74.4 \(\) 19.6 & 52.7 \(\) 9.2 & 67.5 \(\) 13.4 \\
6 & **VC-1** & 8MAE adaptation & 72.0 \(\) 3.3 & 96.0 \(\) 1.8 & 80.9 \(\) 1.8 & 80.6 \(\) 0.25 & 57.4 \(\) 1.11 & 67.0 \(\) 0.73 & 62.4 \(\) 2.16 & 43.5 \(\) 6.7 & 85.0 \(\) 9.1 \\   

Table 5: Adapting **VC-1** with end-to-end fine-tuning or self-supervised learning (MAE) on task-specific data leads to substantial gains. Best prior results are across any setting (frozen/finetuned). Best result (our exp.) are from Section 5. \({}^{**}\) indicates that, on hardware, we only evaluated MVP (viT-L), the closest competitor to **VC-1**. The TF Push-Cube results are averaged across 12 trials with randomized start/goal configurations. The Franka results are averaged across 4 tasks and 10 trials per task (details in Appendix A.12).

We study the cases when using **VC-1** as a frozen PVR, **VC-1** with MAE adaptation and **VC-1** with E2E adaptation (as specified in Section 6), and the MVP model as a baseline (best PVR from Section 4). The results are summarized in the hardware section of Table 5. Overall, we observe similar trends to our findings in Section 6. We observe that in frozen mode (row 4), **VC-1** substantially outperforms both MVP (row 2) and in-domain MAE (row 3) in both robot setups. We also find that adaptation via E2E fine-tuning improves real-world Trifinger performance and that MAE adaptation leads to large improvements on Franka tasks. We note that while MAE adaptation does not help TriFinger performance, this is likely due to the mere 300 real robot images available for adaptation. Together, these results suggest that learning task-specific features (via fine-tuning) was more important than closing any domain gap (via MAE adaptation) for TriFinger. Overall, these results demonstrate that **VC-1** can effectively function as a PVR for multiple hardware platforms, and can outperform prior PVRs that have shown success on hardware, such as MVP. Furthermore, it reinforces finding from Section 6 that adapting **VC-1** with SSL objectives (MAE) can improve the performance.

## 8 Discussion

This work introduced CortexBench, which comprises 17 different EAI task spanning locomotion, indoor navigation, and dexterous and mobile manipulation; and conducted the most comprehensive study to-date of PVRs (or visual foundation models) for EAI. We find that (1) despite significant progress in a number of narrow domains, we do not yet have a universal visual backbone for all EAI tasks of interest, (2) naively scaling model size and pre-training data diversity does not improve performance universally across all tasks, but does so on average, (3) adapting our largest pre-trained model (**VC-1**) results in performance that is _competitive with or outperforms the best known results on all benchmarks_ in CortexBench, and (4) **VC-1** and adaptation show proof-of-concept generalization to hardware. Our study is an attempt to unify various EAI areas using the perception module as a cornerstone. The study and development of visual representations for sensorimotor control today appears splintered across different sub-communities studying egocentric computer vision, locomotion, navigation, dexterous and mobile manipulation. However, we contend that this cannot be the final solution. Biological organisms have one visual cortex, not one per 'task'. Analogously, it must be possible for an embodied AI agent to have one universal artificial visual cortex supporting a diverse range of sensorimotor skills, environments, and embodiments. We speculate that learning visual representation using temporal signals, 3D spatial priors, or objectness will help make progress towards this goal. Our final contention is that in order for the research community to develop such a model, we need to create benchmarks that test broad generalization capabilities; we hope CortexBench will help the community make progress towards that.