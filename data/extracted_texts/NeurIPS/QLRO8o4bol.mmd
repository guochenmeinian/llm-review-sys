# Generate Universal Adversarial Perturbations for Few-Shot Learning

Yiman Hu  Yixiong Zou1  Ruixuan Li1  Yuhua Li

School of Computer Science and Technology, Huazhong University of Science and Technology

{imane, yixiongz, rxli, idcliyuhua}@hust.edu.cn

###### Abstract

Deep networks are known to be vulnerable to adversarial examples which are deliberately designed to mislead the trained model by introducing imperceptible perturbations to input samples. Compared to traditional perturbations crafted specifically for each data point, Universal Adversarial Perturbations (UAPs) are input-agnostic and shown to be more practical in the real world. However, UAPs are typically generated in a close-set scenario that shares the same classification task during the training and testing phases. This paper demonstrates the ineffectiveness of traditional UAPs in open-set scenarios like Few-Shot Learning (FSL). Through analysis, we identify two primary challenges that hinder the attacking process: the task shift and the semantic shift. To enhance the transferability of UAPs in FSL, we propose a unifying attacking framework addressing these two shifts. The task shift is addressed by aligning proxy tasks to the downstream tasks, while the semantic shift is handled by leveraging the generalizability of pre-trained encoders.The proposed Few-Shot Attacking FrameWork, denoted as FSAFW, can effectively generate UAPs across various FSL training paradigms and different downstream tasks. Our approach not only sets a new standard for state-of-the-art works but also significantly enhances attack performance, exceeding the baseline method by over 16%.

## 1 Introduction

Deep neural networks[16; 12] have made significant advancements in a variety of computer vision tasks. Nowadays, there is a growing trend of pre-training a model that achieves strong generalization capabilities and subsequently fine-tuning it for different unseen downstream tasks. A promising method to handle this open set problem is Few-Shot Learning (FSL), which learns a model that can rapidly adapt to unseen tasks with only a limited number of samples.

Meanwhile, deep networks have shown to be vulnerable to adversarial attacks [41; 14; 5], which are deliberately designed to deceive a trained model by introducing imperceptible perturbations to the input samples. Given the widespread adoption of the pre-training and fine-tuning paradigm, it is crucial to acknowledge and address the security concerns associated with such approaches.

For a more applicable way to attack various downstream tasks, we focus on Universal Adversarial Perturbations (UAPs) . This kind of perturbation can be applied to all images once generated, eliminating the need for crafting image-dependent perturbations for each task.

However, current approaches for generating UAPs have primarily concentrated on close-set scenarios [26; 15; 30], where the classification tasks for both training and finetuning are essentially identical. Despite achieving a high Attack Success Rate (ASR) in their respective settings, our research revealsthat in FSL scenarios these techniques are ineffective, and in some cases, incapable of crafting UAPs, as shown in the left panel of Fig. 1.

To better understand how to effectively attack the FSL tasks, we conducted a comprehensive evaluation and analysis of the challenges involved, namely task shift and semantic shift, as depicted in the right panel of Fig. 1. To further fill up these two shifts and achieve a more generalizable UAP, we establish a baseline attacking framework as an initial point and progressively improve upon it. To address the task shift, we emphasize the significance of learning task bias for UAP, and handle this shift by constructing proxy tasks. To tackle the semantic shift, we demonstrate the effectiveness brought by the encoder's generalizability and eliminate the need for fine-tuning in the process of UAP generation.

By systematically addressing the shifts, we successfully generate the single perturbation only based on the parameters of the pre-trained encoder, without any prior knowledge of the pre-training dataset or downstream tasks. The ASRs of our universal perturbation surpass others by 16.49% for 5-way 1-shot tasks and 17.27% for 5-way 5-shot tasks. Additionally, our attacking framework(FSAFW) unifies the generation of UAPs in various FSL training paradigms, including finetuning-based [10; 47; 7], meta-based [13; 35; 39], and metric-based [43; 36; 50; 20] approaches, yielding state-of-the-art results in all cases. We summarize our main contributions as follows:

\(\) We propose a new standard for the study of UAP in FSL scenarios, which highlights the limitations of traditional UAPs in the context of FSL.

\(\) We provide a thorough analysis of the associated challenges, the task shift and semantic shift, which diminish the effectiveness of UAPs on downstream FSL tasks.

\(\) We construct an attacking framework and fill up the two shifts step by step to enhance the attack performance, which is effective across various FSL paradigms.

\(\) Our proposed method significantly advances state-of-the-art methods in FSL attacking performance, with an increase of over 16% in ASR in our standard.

## 2 Related Works

**Adversarial attacks** can be categorized into two types: image-dependent attacks and universal attacks. Image-dependent attacks have been widely studied [14; 23; 5; 22; 11]. The concept of Universal Adversarial Perturbations (UAPs) was initially introduced by  using an iterative Deepfool attack  applied to individual image samples.  developed a data-independent approach to generate UAPs. Furthermore,  validated the efficacy of using random source images. Several studies have explored the use of generative models to produce more generalized and natural-appearing UAPs.  were the first to utilize the generative network.  introduced GAP, a framework applicable to both classification and semantic segmentation models.  trained a generator by leveraging a

Figure 1: (Left) A decline in the Attack Success Rate (ASR) can be observed when conventional UAPs are applied to the FSL scenario. The yellow bar represents performance in traditional scenarios, while the other three indicate performance in different FSL training paradigms. For the metric-based paradigm, traditional methods even fail to generate UAPs. The detailed setting can be found in Section 4.1 (Right). Solid arrows represent the pre-training phase, while dotted arrows depict the testing phase. The attacker can only manipulate the pre-trained model, which suffers from the semantic shift of different datasets and the task shift from base to novel tasks.

contrastive loss function. However, none of these studies address the specific challenges associated with attacking few-shot learning tasks.

**Few-shot learning** is a machine learning paradigm that aims to recognize novel classes from a few labeled samples . Existing FSL methods can be broadly grouped into three categories according to : finetuning-based, meta-based, and metric-based methods. Finetuning-based strategies [47; 7; 10; 42; 32; 24] follow a transfer learning process that includes pre-training on base classes and fine-tuning on novel classes. Meta-based approaches [13; 33; 38; 35; 39; 3; 31] adopts a meta-learning paradigm to learn the cross-task knowledge through the optimization between the meta-learner and base-learner. In this way, the model adopts a quick adaptation to the novel dataset. Metric-based techniques [2; 40; 43; 36; 50; 20] focus on learning transferable representations and making predictions based on the distance between feature representations. This strategy eliminates the need for test-time fine-tuning. These paradigms have significantly advanced the progress of few-shot learning, yet a unified method for generating UAPs across all paradigms remains undeveloped.

## 3 UAP Setting for Few-Shot Learning

In this section, we outline the threat model of attacking the few-shot tasks and present key definitions and notations to facilitate a clearer understanding of the concepts involved.

### Threat Model

We consider an attacker who aims to craft a universal adversarial perturbation to attack a pre-trained victim model and further impair the performance of downstream few-shot tasks. The attacker only has access to the pre-trained model (_e.g._, by downloading from public repositories), but can not obtain the datasets used for pre-training and have no knowledge of the following few-shot tasks. Once the perturbation is generated, the attacker attaches it to each query sample. The crafted perturbation is imperceptible, and is expected to greatly mislead the few-shot classification.

### Definition and Notations

In the domain of few-shot learning, abundant annotated images of the base dataset \(_{b}\) can be used for pre-training. Subsequently, the model would be fine-tuned using limited samples from the novel dataset \(_{n}\), where the categories in \(_{n}\) do not overlap with those in \(_{b}\). In the pre-training stage, the victim model \(f()\) that composed of an encoder \(f_{e}()\) and a base classifier \(f_{bc}()\) is trained on \(_{b}\). In the evaluation stage, different forms of tasks are sampled from \(_{n}\). Each task contains a support set \(\) used for finetuning and a query set \(\) used for evaluation. The support set includes \(n\) different classes with \(k\) samples per class, referred to as \(n\)-way \(k\)-shot. Similarly, the query set contains \(n\) classes with \(q\) samples per class. For each task, a novel class classifier \(f_{nc}()\) is newly trained on \(\). For any image \(x\) in \(\), \(f_{nc}(f_{e}(x),)\) yields the classifier's output. The goal of FSL is to learn a

Figure 2: Comparison of different scenarios. In contrast to traditional settings, the FSL framework introduces two distinct challenges: the task shift from pre-training to testing, and the semantic shift from the base to the novel dataset. Attackers must overcome the two shifts to mount successful attacks in the FSL scenario.

generalizable \(f_{e}()\) that exhibits good performance on \(\) across various tasks after learning from the limited examples in \(\).

Once get the pre-trained encoder \(f_{e}()\), an attacker can craft the adversarial images as follows: \(x^{adv}=x+\), where \(\) represents the perturbation. To ensure that the adversarial images are visually indistinguishable from the original ones, \(\) is usually restricted by a chosen distance metric, such as the \(_{}\) norm. This constraint can be expressed as \(\|x^{adv}-x\|_{}=\|\|_{}\). An effective approach to generate \(\) is to train a generator \(g_{}()\) on the proxy dataset \(_{p}\). The generator takes a random vector \(z\) as input and it aims to produce perturbations that satisfy the given constraints while maximizing adversarial impact.

## 4 A Closer Look at UAP Generation in Few-Shot Learning

In this session, we conduct a thorough analysis of the challenges associated with generating Universal Adversarial Perturbations (UAPs) in the context of Few-Shot Learning (FSL). We first compare the attack performance in the traditional scenario with that in the FSL scenario. Then we point out the lower performance in the FSL scenario is due to the presence of two shifts.

### Poor Performance of Traditional UAPs in the FSL Scenario

To better understand the performance of existing attacking methods on FSL tasks, we directly apply the traditional method in the FSL scenario. We compare the Attack Success Rate (ASR) in the FSL scenario with that observed in the original scenario. The ASR refers to the success rate of the UAP in fooling the classifier. A higher ASR means a stronger attack capability.

We adopt the _Generative Adversarial Perturbation (GAP)_ method, due to its straightforward applicability to FSL tasks and its high attack performance within its original context. The GAP method generates UAP through a ResNet Generator, denoted as \(g_{GAP}\). The perturbation \(\) is produced as follows:

\[=g_{GAP}(z),\] (1)

where \(z\) represents a random vector. The generator is optimized with respect to the following loss term:

\[L_{GAP}=-((f(x+),y)).\] (2)

Here, \((x,y)\) is the data point sampled from proxy dataset, \(f()\) denotes the model pre-trained on the base dataset, and \((f(x^{adv}),y)\) represents the cross-entropy loss.

For the traditional GAP setting, we compute the mean ASR on VGG16, VGG19, and ResNet152 reported in the original paper to gauge the average performance in the traditional scenario. In the FSL scenario, where the attacker does not have access to the base or novel datasets, we utilize CIFAR-FS  as the proxy dataset to generate UAPs. We attack three pre-trained FSL models representative of different paradigms: finetuning-based, metric-based, and meta-based approaches.

Figure 3: The ASRs under different task shifts and semantic shifts. In (b), Cifar, Mini, Tiered are different datasets detailed in Section 6.1.

As illustrated in the left panel of figure 1, GAP achieves a pretty high mean ASR of 89.43% in traditional tasks. However, when switched to the FSL setting, there is a marked decrease in performance, with a reduction of at least 25% observed in both 5-way 1-shot and 5-way 5-shot tasks. Moreover, the GAP method is unable to generate UAPs for metric-based models, as its generation process depends on the presence of a fixed classifier, which is lacking for metric-based methods. This observation leads us to pose the question: **What makes the attack success rate of UAPs decrease so much in FSL?** In the next subsection, we will discuss the factors.

### Two Shifts in FSL Affect the Attack Transferability

We compare the differences in the UAP generation process between the traditional scenario and the FSL scenario, as depicted in Figure 2. In traditional scenarios, UAP generation relies on a fixed classifier that remains unchanged throughout both the training and testing phases. In contrast, FSL scenarios present a variety of classification tasks during testing--encompassing different categories and shapes, such as 5-way 1-shot and 5-way 5-shot, thereby posing challenges to UAP generalization. Therefore, our first hypothesis is that **the task shift in FSL hinders the transferability of the UAP**.

To validate our hypothesis, we conducted experiments to evaluate the performance of the UAP across different downstream tasks. The experiments vary the number of ways from 5 to 64 and the number of shots from 1 to 20. To maintain the semantic consistency between the pre-training and testing datasets, we sampled an additional 100 images for each category in the training split of _mini_-ImageNet to serve as the downstream dataset. The results are presented in Figure 3 (a), demonstrating that as the downstream task becomes less similar to the pre-training task(64-way full shot), the attacking performance of the UAP diminishes.

From Figure 2, it can also be observed that the categories in training and testing datasets do not overlap in the FSL scenario. This semantic shift may contribute to a reduction in the attacking performance. Therefore, we derive a second hypothesis that **the semantic shift in FSL hinders the transferability of the UAP**. To verify the impact of the semantic shift, we keep the downstream task constant and vary the proxy dataset and downstream dataset. We choose a model pre-trained on the _mini_-ImageNet as the victim model. As illustrated in Figure 3 (b), the attack performance declines with increasing distances between both the downstream dataset and the pre-trained dataset(mini), as well as between the proxy dataset and the pre-trained dataset(mini).

To summarize, we empirically demonstrate that the two shifts that existed in the FSL scenario degrade the attack performance of traditionally generated UAPs.

## 5 Method

In this section, we propose a baseline framework aimed at generating effective Universal Adversarial Perturbations (UAPs) within a Few-Shot Learning (FSL) scenario. Starting with an initial attacking framework, we progressively refine our approach to address the two shifts mentioned before. Finally, we have developed a universally applicable UAP generation strategy, capable of attacking models

Figure 4: (a) The Base Classifier is the pre-trained classifier. Proxy Linear and Proxy Protos are newly designed to address the task shift and the semantic shift, as detailed in Section 5. (b) The detailed Attack Success Rate(ASR) of different methods on 5-way 1-shot and 5-way 5-shot tasks. TS, SS represents whether the Task Shift and the Semantic Shift are handled respectively.

trained under all kinds of FSL paradigms and showing strong generalizability to different downstream tasks.

### A Baseline Attacking Framework

The attacking framework is illustrated in Figure 4 (a). The victim model, built on a ResNet12 backbone, is pre-trained using the training split of the _mini_-ImageNet dataset, following the training paradigm established in . For the attack, we train a generator to transform random vectors into a UAP, which is also used in . Since the attacker lacks access to both the training and testing data from the _mini_-ImageNet dataset, we opt for the training split of the CIFAR-FS dataset as a proxy. To optimize our generator \(g_{}\), we employ a negative cross-entropy loss as the attacking objective, defined by the following equation:

\[L_{fool}=-(f_{bc}(f_{e}(x+g_{}(z))),y).\] (3)

As the original approach, we utilize a 64-way classifier \(f_{bc}()\) from the baseline model to infer, without any fine-tuning. The results are presented in the first row of Figure 4 (b), denoted as **base classifier**. The results are far from satisfactory compared to the performance in traditional tasks, highlighting the need for further refinement in the FSL scenario.

### Fill Up the Task Shift through Task Alignment

As demonstrated in Section 4.2, a misalignment between upstream and downstream tasks can adversely affect the performance of UAPs. Given that the attacker cannot foresee the specific downstream tasks, we modify the tasks used in UAP generation to better align with those downstream tasks. Importantly, we only aim to approximate the shape of the downstream tasks rather than achieving exactness, which is sufficient to let the UAP acquire the necessary task bias. This is supported by the ablation study detailed in Section 6.3. We sample _proxy tasks_ with the shape of 5-way 1-shot from the proxy dataset \(_{p}\). For each task, we construct a linear classifier \(f_{pc}()\) based on its support set \(_{p}\). We refer to this method as **proxy linear**. The objective function for training the classifier is given by:

\[L_{pc}=(f_{pc}(f_{e}(x)),y),\] (4)

where \((x,y)\) represents the data sampled from \(_{p}\). Once the proxy linear classifier has been trained, the attacker generates perturbations guided by the perturbed query set \(_{p}^{adv}\) from the proxy task, as depicted in Figure 4 (a). The fooling loss is quite like equation 3 except that the classifier is newly constructed and the data \((x,y)\) is sampled from \(_{p}\), rather than the entire proxy dataset. The fooling loss is calculated as follows to optimize the generator \(g_{}\) :

\[L_{fool}=-(f_{pc}(f_{e}(x+g_{}(z))),y),\] (5)

where \((x,y)\) is sampled from \(_{p}\).

By integrating task alignment into the training of UAPs, we empower them to generalize more effectively to downstream tasks, even in the absence of direct access to those tasks. From the results presented in the second row of Figure 4 (b), it is evident that by addressing the task shift, there is a marked improvement in the ASR, with an increase of 5% for 1-shot tasks and 7% for 5-shot tasks.

### Fill Up the Semantic Shift by Leveraging the Encoder's Generalizability

**Analysis of the semantic shift.** In Section 4.2, we discussed the impact of the semantic shift on UAPs, a phenomenon widely present in FSL. As attackers lack access to upstream or downstream datasets, our strategy emphasizes utilizing the generalization capabilities of the pre-trained encoder. A well-trained encoder possesses the capability to transfer useful information to the novel dataset, even trained on the base dataset. Consequently, we hypothesize that _if the proxy dataset closely aligns with the base dataset, the generated UAP can be more transferable to the novel dataset._

We substitute the proxy dataset with the base dataset to create an ideal scenario for testing our assumption. While attackers cannot access the base dataset in real-world situations, utilizing it in our experiments allows us to better understand and validate our hypotheses. To be specific, we sample different tasks from the base dataset and train a linear classifier for each task, following the process outlined in equation 4. The results are documented in the third row of Figure 4 (b), denoted as **base

**linear**. Compared to the Proxy liner, a further improvement can be observed in the table. The results suggest that a closer simulation of the base dataset distribution enhances the transferability of the UAP to the novel dataset.

**Alleviation of the semantic shift.** While utilizing the base dataset proves effective, the attacker can only utilize the proxy dataset. As discussed in , when the proxy dataset is fed into the encoder, its output distribution tends to shift towards the base dataset, which is exactly what we need.

However, the previously mentioned proxy linear method introduces a supervisory signal that corrects the distribution, as shown in Figure 5 (a). The output distribution, initially altered by the encoder, is adjusted by the constructed proxy classifier.

This causes the UAP to lose its connection with the base dataset during training, thereby diminishing its ability to generalize to downstream tasks. Consequently, we abandon the proxy linear classifier and adopt a metric-based evaluation to mitigate the influence of labels from the proxy dataset. For each task sampled from the proxy dataset, we extract the output features from the support set and compute the mean of each class (i.e., class prototypes). The prototype for class \(k\) in support set \(_{p}\) is calculated using the equation:

\[c_{k}=}_{y=k}f_{e}(x).\] (6)

\((x,y)\) is sampled from \(_{p}\) and \(N_{k}\) represents the number of samples for class \(k\) in the support set. We then employ a distance metric \(D(,)\) to quantify the distance between the query feature and each class prototype. The classification probability for a query sample \(x\) in class \(k\) is then determined by:

\[p(y=k|x)=(x),c_{k})}}{_{k^{}}e^{D(f_{e}(x),c_{k^{ }})}},\] (7)

where \(k^{}\) represents a class in the task. In accordance with prior work , we utilize cosine similarity as the distance metric.

The **proxy protos** strategy facilitates a closer approximation to the novel dataset, regardless of the specific proxy dataset used. The efficacy of this approach is documented in the final row of Figure 4 (b), which presents the most favorable results among all methods tested.

**A further improvement.** As suggested by , an alternative formulation for the fooling loss is as follows:

\[L_{fool}=-(f_{proto}(f_{e}(x+g(z))),y),\] (8)

where \(f_{proto}\) denotes the predicted classification computed using the prototype-based approach previously described. Furthermore, to avoid reliance on the proxy query label and enhance transferability, we can supervise the perturbation generator with smoothed labels, denoted as \(\). The fooling loss is redefined as:

\[L_{fool}=-(f_{proto}(f_{e}(x+g(z))),).\] (9)

Figure 5: (a) A qualitative illustration showing the effect of the supervisory signal. (b) The attack performance comparison of different fooling losses.

The adoption of this revised fooling loss yields further performance enhancements, as evidenced in Figure 5 (b). Consequently, we have adopted equation 9 as the final formulation for our fooling loss.

### Conclusion and Discussion

Through a thorough comparison of the traditional scenario and the FSL scenario, we point out two critical shifts that reduce the attack performance on FSL tasks. To address the task shift, we sample proxy tasks to mirror the shape of the downstream tasks. To handle the semantic shift, we employ class prototypes to train UAPs, circumventing the introduction of the proxy labels. Moreover, the application of smoothed labels yields additional enhancements in attack performance.

Our final method can be summarized in the following steps: _(1)_ Sample 5-way 1-shot tasks from the proxy dataset. _(2)_ Calculate prototypes for each class in the proxy tasks. _(3)_ Compute the fooling loss based on the distance between the query sample and class prototypes. _(4)_ Optimize the perturbation generator to produce a highly transferable UAP in FSL. By following these steps, our method systematically generates universal adversarial perturbations that are robust and generalizable across different tasks.

## 6 Experiments

### Implementation Details

**Datasets.** We utilize three widely used datasets in Few-Shot Learning (FSL) as the proxy dataset: CIFAR-FS, _mini_-ImageNet, and Tiered-ImageNet. Both CIFAR-FS and _mini_-ImageNet

   Victim & Proxy & Backbone & Baseline & Baseline++ & ANIL-1 & R2D2-1 & ProtoNet & DN4 \\   &  & RN12 & 81.56\(\)0.29 & 58.94\(\)0.43 & 77.84\(\)0.28 & 70.34\(\)0.29 & 69.03\(\)0.36 & 73.31\(\)0.32 \\  & & RN18 & 71.55\(\)0.35 & 74.67\(\)0.28 & 72.11\(\)0.29 & 70.00\(\)0.31 & 60.02\(\)0.34 & 67.66\(\)0.30 \\   &  & RN12 & 81.63\(\)0.28 & 68.55\(\)0.46 & 78.08\(\)0.28 & 77.22\(\)0.28 & 68.54\(\)0.36 & 74.91\(\)0.33 \\  & & RN18 & 72.32\(\)0.36 & 80.01\(\)0.26 & 76.87\(\)0.30 & 77.42\(\)0.30 & 76.09\(\)0.31 & 69.36\(\)0.32 \\   &  & RN12 & 79.68\(\)0.30 & 67.51\(\)0.43 & 77.81\(\)0.28 & 77.09\(\)0.27 & 72.87\(\)0.34 & 75.23\(\)0.33 \\  & & RN18 & 72.11\(\)0.36 & 79.45\(\)0.27 & 76.36\(\)0.30 & 77.11\(\)0.30 & 74.80\(\)0.32 & 73.31\(\)0.31 \\   &  & RN12 & 76.03\(\)0.32 & 62.56\(\)0.29 & 68.26\(\)0.28 & 76.49\(\)0.27 & 76.73\(\)0.32 & 78.47\(\)0.34 \\  & & RN18 & 69.27\(\)0.32 & 73.82\(\)0.27 & 69.27\(\)0.29 & 70.31\(\)0.30 & 67.60\(\)0.31 & 63.66\(\)0.39 \\   &  & RN12 & 78.91\(\)0.30 & 75.67\(\)0.29 & 75.02\(\)0.31 & 77.68\(\)0.28 & 76.71\(\)0.32 & 78.80\(\)0.34 \\  & & RN18 & 74.74\(\)0.33 & 80.03\(\)0.24 & 75.68\(\)0.32 & 78.08\(\)0.29 & 71.65\(\)0.33 & 70.58\(\)0.43 \\   &  & RN12 & 78.84\(\)0.30 & 75.99\(\)0.28 & 75.48\(\)0.31 & 77.64\(\)0.28 & 76.83\(\)0.32 & 78.94\(\)0.35 \\  & & RN18 & 73.51\(\)0.32 & 80.44\(\)0.25 & 75.33\(\)0.33 & 78.25\(\)0.28 & 71.45\(\)0.34 & 69.15\(\)0.43 \\   

Table 1: 5-way 1-shot ASR results of our framework in different FSL victim models.

   Victim & Proxy & Backbone & Baseline & Baseline++ & ANIL-1 & R2D2-1 & ProtoNet & DN4 \\   &  & RN12 & 79.00\(\)0.18 & 63.41\(\)0.27 & 78.31\(\)0.21 & 70.42\(\)0.22 & 67.96\(\)0.28 & 74.88\(\)0.22 \\  & & RN18 & 71.82\(\)0.21 & 73.87\(\)0.24 & 71.37\(\)0.25 & 69.74\(\)0.23 & 59.94\(\)0.31 & 69.75\(\)0.22 \\   &  & RN12 & 79.05\(\)0.19 & 75.01\(\)0.22 & 78.40\(\)0.21 & 79.14\(\)0.18 & 70.09\(\)0.27 & 76.90\(\)0.20 \\  & & RN18 & 73.89\(\)0.21 & 79.31\(\)0.20 & 77.46\(\)0.21 & 79.09\(\)0.17 & 76.34\(\)0.19 & 68.70\(\)0.24 \\   &  & RN12 & 76.78\(\)0.21 & 74.03\(\)0.23 & 78.43\(\)0.20 & 79.02\(\)0.18 & 74.26\(\)0.22 & 76.91\(\)0.21 \\  & & RN18 & 73.14\(\)0.23 & 78.99\(\)0.20 & 77.42\(\)0.21 & 78.92\(\)0.17 & 74.75\(\)0.21 & 75.93\(\)0.21 \\   &  & RN12 & 75.09\(\)0.21 & 59.40\(\)0.30 & 68.96\(\)0.23 & 76.94\(\)0.19 & 78.33\(\)0.17 & 75.91\(\)0.21 \\  & & RN18 & 70.32\(\)0.24 & 71.30\(\)0.25 & 70.73\(\)0.23 & 71.55\(\)0.22 & 70.87\(\)0.22 & 66.09\(\)0.28 \\   &  & RN12 & 78.43\(\)0.17 & 78.07\(\)0.17 & 76.05\(\)0.22 & 78.72\(\)0.16 & 78.34\(\)0.17 & 76.42\(\)0.22 \\   &  & RN18 & 78.01\(\)0.16 & 79.41\(\)0.18 & 77.90\(\)0.21 & 79.25\(\)0.15 & 76.65\(\)0.17 & 74.85\(\)0.26 \\   &  & RN12 & 78.48\(\)0.16 & 78.09\(\)0.18 & 76.61\(\)0.22 & 78.79\(\)0.16 & 78.37\(\)0.17 & 77.09\(\)0.23 \\   &  & RN18 & 76.

datasets comprise images from 100 categories, divided into 64 classes for training, 16 for validation, and 20 for testing. Tiered-ImageNet, a more extensive subset of ImageNet , consists of 608 classes (779,165 images) organized into 34 high-level categories, which are further split into 351 training classes, 97 validation classes, and 160 testing classes. To prevent the attacker from gaining knowledge of the downstream images, only the training portions of these datasets are utilized for the generation of UAPs.

**Training Details.** All the victim models are downloaded from the Libfewshot, a comprehensive Library for FSL. In line with the library's classification of FSL training paradigms, we selected Baseline and Baseline++  as representatives of the finetuning-based paradigm, ANIL  and R2D2  for the meta-based paradigm, and ProtoNet  along with DN4  for the metric-based paradigm. For each paradigm, we adopted ResNet12 and ResNet18 as the backbone of the victim models, following the previous works[8; 48]. All these victim models are pre-trained on the _mini_-ImageNet and Tiered-ImageNet.

We adopt the Attack Success Rate (ASR) to evaluate the attack performance of UAPs on 2000 FSL tasks. The generator network, following the approach described in , was optimized using the Adam optimizer with an initial learning rate of 0.0002. The perturbation was kept within an \(_{}\)-norm bound of \(=10\), considering pixel values in the range of 0 to 255. Additional FSL testing configurations follow the details provided in Libfewshot.

### Results

**Effectiveness of our attacking framework.** Our investigation into the effectiveness of our attacking framework (FSAFW) across different FSL models on different tasks is demonstrated in 1 and 2. For each FSL paradigm, we targeted four models pre-trained on _mini_-ImageNet and TieredImageNet, utilizing ResNet12 and ResNet18 as backbone architectures. We report the mean Attack Success Rate (ASR, %, top-1) alongside 95% confidence intervals. For the meta-based methods ANIL and R2D2, we chose the victim models trained by 5-way 1-shot episodes denoted as ANIL-1 and R2D2-1 in the table (ANIL-5 and R2D2-5 are present in Section A). Notably, when the victim's dataset is consistent with the proxy dataset, attack performance generally improves. Our proposed attacking framework can achieve comparable ASR even when the proxy dataset is different from the victim one.

   Victim & Method & Mark & Baseline & Baseline++ & ANIL-1 & R2D2-1 & ProtoNet & DN4 \\   & UAN & SPW-18 & 52.27\(\)0.33 & 47.68\(\)0.42 & 43.64\(\)0.26 & - & - & - \\  & GAP & CVPR-18 & 47.71\(\)0.31 & 49.40\(\)0.35 & 66.47\(\)0.32 & - & - & - \\  & AdvEncoder & ICCV-23 & 76.68\(\)0.31 & 57.37\(\)0.38 & 68.51\(\)0.31 & 59.10\(\)0.29 & 66.63\(\)0.34 & 72.85\(\)0.31 \\   & FSAFW & Ours & **81.56\(\)0.29** & **58.94\(\)0.43** & **77.84\(\)0.28** & **70.34\(\)0.29** & **69.03\(\)0.36** & **73.31\(\)0.32** \\   & UAN & SPW-18 & 40.34\(\)0.31 & 33.00\(\)0.27 & 51.91\(\)0.28 & - & - & - \\  & GAP & CVPR-18 & 49.72\(\)0.33 & 58.23\(\)0.28 & 61.19\(\)0.30 & - & - & - \\   & AdvEncoder & ICCV-23 & 75.99\(\)0.32 & 62.16\(\)0.29 & 53.82\(\)0.30 & 71.01\(\)0.31 & 60.23\(\)0.33 & 68.86\(\)0.32 \\    & FSAFW & Ours & **76.03\(\)0.32** & **62.56\(\)0.29** & **68.26\(\)0.28** & **76.49\(\)0.27** & **76.73\(\)0.32** & **78.47\(\)0.34** \\   

Table 3: Comparison of different attack methods on ASR for 5-way 1-shot tasks.

   Victim & Method & Mark & Baseline & Baseline++ & ANIL-1 & R2D2-1 & ProtoNet & DN4 \\   & UAN & SPW-18 & 52.27\(\)0.33 & 47.68\(\)0.42 & 43.64\(\)0.26 & - & - & - \\  & GAP & CVPR-18 & 47.71\(\)0.31 & 49.40\(\)0.35 & 66.47\(\)0.32 & - & - & - \\  & AdvEncoder & ICCV-23 & 76.68\(\)0.31 & 57.37\(\)0.38 & 68.51\(\)0.31 & 59.10\(\)0.29 & 66.63\(\)0.34 & 72.85\(\)0.31 \\   & FSAFW & Ours & **81.56\(\)0.29** & **58.94\(\)0.43** & **77.84\(\)0.28** & **70.34\(\)0.29** & **69.03\(\)0.36** & **73.31\(\)0.32** \\   & UAN & SPW-18 & 40.34\(\)0.31 & 33.00\(\)0.27 & 51.91\(\)0.28 & - & - & - \\  & GAP & CVPR-18 & 49.72\(\)0.33 & 58.23\(\)0.28 & 61.19\(\)0.30 & - & - & - \\  & AdvEncoder & ICCV-23 & 75.99\(\)0.32 & 62.16\(\)0.29 & 53.82\(\)0.30 & 71.01\(\)0.31 & 60.23\(\)0.33 & 68.86\(\)0.32 \\    & FSAFW & Ours & **76.03\(\)0.32** & **62.56\(\)0.29** & **68.26\(\)0.28** & **76.49\(\)0.27** & **76.73\(\)0.32** & **78.47\(\)0.34** \\   

Table 3: Comparison of different attack methods on ASR for 5-way 1-shot tasks.

**Comparison with other attacking methods.** We compare the ASR of our attacking framework with the classic generator-based adversarial methods, such as UAN , GAP , and AdvEncoder , against the aforementioned FSL victim models. For this comparison, we utilized ResNet12 as the backbone and CIFAR-FS as the proxy dataset. ASR results for both 5-way 1-shot and 5-way 5-shot tasks are detailed in 3 and 4, respectively. Our framework consistently outperforms the other methods across victim models trained under various configurations. Note that some attacking methods rely on a fixed classifier in testing to generate UAPs, while models like R2D2, ProtoNet, and DN4 do not have one. These methods are not applicable to such models, and we have indicated this incompatibility with a dash ('-') in the relevant table cells.

### Ablation Study

**The attack performance of contrastive losses.** To generalize the attack ability to the downstream tasks, a simple way is to adopt the contrastive losses. In this section, we probe the effectiveness of various contrastive losses in crafting generalizable UAPs. We adopt the InfoNCE loss, SimCLR loss, MSE loss, Cosine similarity loss, SupCon loss and feature scatter loss to train the generator. We use a victim model trained on _mini_-ImageNet and with a backbone of ResNet12. The model is trained and tested under the Baseline paradigm. We compare their ASR performance with that of our method in Table 5. The results suggest that contrastive losses in attacking FSL tasks are inferior to our FSAFW.

**The influence of different forms of proxy tasks.** We evaluate how different shapes of the proxy tasks affect the attack performance of the generated UAP. For the proxy tasks, we keep the number of shots constant at 1 while varying the number of ways, and vice versa, maintaining 5 ways while changing the number of shots. We utilize 5-way 5-shot and 5-way 1-shot as the shapes for downstream tasks. The attack results are displayed in the left and right panels of Figure 6. It can be observed that when the shapes of the proxy tasks do not deviate too much from the downstream tasks, the attack performance can be maintained. This suggests that when constructing the proxy tasks, a rough estimation of the downstream tasks' shape is enough.

## 7 Conclusion

In this work, we propose a unifying Few-Shot Attacking FrameWork (FSAFW) to generate transferable UAPs on the FSL scenario. We identify and analyze two major challenges in FSL: the task shift and the semantic shift. We adopt proxy tasks to handle the task shift and proxy prototypes to address the semantic shift. Our framework significantly outperforms existing methods, establishing a new benchmark for launching attacks on FSL tasks.

   Tasks & Ours & InfoNCE & SimCLR & MSE & Cosine & SupCon & feature scatter \\ 
5-way 1-shot & **81.56\(\)0.29** & 77.67\(\)0.31 & 77.26\(\)0.31 & 64.19\(\)0.33 & 79.32\(\)0.30 & 77.48\(\)0.31 & 75.85\(\)0.31 \\
5-way 5-shot & **79.00\(\)0.18** & 75.40\(\)0.23 & 74.69\(\)0.23 & 60.38\(\)0.29 & 76.87\(\)0.22 & 74.94\(\)0.23 & 72.90\(\)0.23 \\   

Table 5: Comparison of different contrastive losses on ASR.

Figure 6: An illustration of the ASR that different forms of proxy tasks bring.