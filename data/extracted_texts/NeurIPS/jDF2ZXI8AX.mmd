# MV2Cyl: Reconstructing 3D Extrusion Cylinders

from Multi-View Images

 Eunji Hong \({}^{1}\), Minh Hieu Nguyen\({}^{1}\), Mikaela Angelina Uy\({}^{2,3}\), Minhyuk Sung\({}^{1}\)

\({}^{1}\)Korea Advanced Institute of Science and Technology, \({}^{2}\)Stanford University, \({}^{3}\)NVIDIA

{eunji.hong, hieuristics, mhsung}@kaist.ac.kr

mikaelaangel@nvidia.com

###### Abstract

We present MV2Cyl, a novel method for reconstructing 3D from 2D multi-view images, not merely as a field or raw geometry but as a sketch-extrude CAD model. Extracting extrusion cylinders from raw 3D geometry has been extensively researched in computer vision, while the processing of 3D data through neural networks has remained a bottleneck. Since 3D scans are generally accompanied by multi-view images, leveraging 2D convolutional neural networks allows these images to be exploited as a rich source for extracting extrusion cylinder information. However, we observe that extracting only the surface information of the extrudes and utilizing it results in suboptimal outcomes due to the challenges in the occlusion and surface segmentation. By synergizing with the extracted base curve information, we achieve the optimal reconstruction result with the best accuracy in 2D sketch and extrude parameter estimation. Our experiments, comparing our method with previous work that takes a raw 3D point cloud as input, demonstrate the effectiveness of our approach by taking advantage of multi-view images. Our project page can be found at https://mv2cyl.github.io.

## 1 Introduction

Most human-made objects in our daily lives are created using computer-aided design (CAD). Reconstructing the structural representation from raw geometry, such as 3D scans, is essential to enablethe fabrication of 3D shapes and manipulate them for diverse downstream applications. Sketch-Extrude CAD model is particularly notable not only as the most common but also as a versatile CAD representation, which enables the expression of diverse shapes with 2D sketches and extruding heights.

The reconstruction of a set of extrusion cylinders from raw 3D geometry has been extensively explored in previous studies [58; 65; 49; 30], yet the suboptimal quality of the results remains a challenge. Unsupervised methods [49; 30] have demonstrated notable capabilities in parsing raw geometry but often struggle to precisely segment the shape and part-wise fit the output extrusions to the given shape. Supervised methods using a language-based architecture, such as those described in , often face challenges in producing valid outputs. A notable exception is Point2Cyl , which is a supervised method proposing to first segment the given raw point cloud into each extrusion region and estimate the 2D sketch and the extrusion parameters using the predicted segments and surface normal information. While it has shown successful results, the bottleneck remains in the limited performance of the 3D backbone network for segmentation.

To address the limitation, we focus on the fact that 3D scans are commonly accompanied by _multi-view images_, which are now even solely used to reconstruct a 3D shape without depth or other information thanks to the recent advancements in neural rendering techniques [40; 61; 13; 62; 5]. 2D multi-view images are a rich resource for extracting 3D extrusion information. Also, 2D convolutional neural networks have been utilized in various 3D tasks [34; 63], owing to their superior performance compared to 3D processing networks. In light of these observations, we introduce a novel framework called MV2Cyl, which reconstructs a set of extrusion cylinders from multi-view images of an object without relying on raw geometry as input.

A straightforward approach to exploiting multi-views in extrusion reconstruction is to first segment 2D regions of the extrusion in each image and reconstruct the 3D shape using the extrusion labels. However, achieving precise 3D surface reconstruction solely from the multi-view images poses challenges due to the ambiguity between the object's intrinsic color (albedo) and the effects of lighting (shading) or shadows within images (Fig. 5). In contrast, we observe that reconstructing only the 2D sketches of the two ends of the extrusion cylinders--the start and end planes of extrusion--provides much more accurate results while avoiding the ambiguity issue and the consequential error accumulation. However, this approach also results in failure cases when one of the bases is not properly detected in 2D image due to the sparse viewpoints. Therefore, we propose a framework that synergizes the reconstructions of labeled surfaces and labeled curves of the extrusion bases so that parameters such as extrusion center and height can be better calculated from the surface information, while the 2D sketches can be precisely recovered from the curve information. In our experiments, we demonstrate the superior performance of our method MV2Cyl on two sketch-extrude datasets: Fusion360  and DeepCAD .

## 2 Related Work

**3D Primitive Fitting**. Primitive fitting has been extensively investigated in computer vision. One line of work involves decomposing into surface/volumetric primitives. Classical approaches include detecting simple primitives such as planes [6; 11; 42] through RANSAC [51; 31], region growing , or Hough voting [3; 8]. The emergence of neural networks gave rise to data-driven methods [29; 56; 28] learning frameworks that fit simple primitives such as planes, cylinders, cones, and spheres. Primitive fitting has also been used to approach the task of shape abstraction [75; 57] that fit cuboids given an input shape or image. However, these works all assume a fixed predefined set of primitives that may not be able to cover complex real-world objects. The focus of our work is to recover a more general primitive, _i.e._, extrusion cylinders, that are defined by any arbitrary closed loop and hence can explain more complex objects.

Another line of work tackles fitting 3D parametric curves, where a common strategy is to first detect edges  and corners [35; 70; 39] from an input point cloud, and then fit or predict parametric curves from the initial proposals. A recent work, NEF , takes multi-view images as input and introduces a self-supervised approach to the parametric curve fitting problem using neural fields, thus allowing them to optimize without needing clean point cloud data. Despite having multi-view images as input, NEF only tackles recovering parametric curves and not extrusion cylinders, which is a basic building block for the sketch-extrude reverse engineering task and is the focus of our work.

**CAD Reconstruction for Reverse Engineering**. CAD reverse engineering has been a well-studied problem in computer graphics and CAD communities [4; 60; 2; 1; 38], enabling the recovery of a structured representation of shapes for applications such as editing and manufacturing. Earlier works focused on reconstructing CAD through inverse CSG modeling [9; 55; 22; 48; 69], which decomposes shapes into simple primitives combined with boolean operations. While achieving good reconstruction, CSG-like methods tend to combine a large number of shape primitives, making them less flexible or easily manipulated by users. Recent efforts in collating large-scale CAD datasets (ABC , Fusion 360 , SketchGraphs ) have pushed this boundary, enabling the learning of data-driven methods for various applications such as B-Rep classification  and segmentation [19; 26; 10], parametric surface inference [16; 36], and CAD reconstruction [23; 27] and generation . For the reverse engineering task, a common CAD modeling paradigm uses _sketch-extrude_[64; 58] as the basic building block, where a sketch [67; 47] is defined as a sequence of parametric curves forming a closed loop. This results in primitives that are more flexible and complex, not a predefined finite set, which will be the focus of this work.

Previous research uses NLP-based language models conditioned on raw geometry to represent CAD models as token sequences [65; 14; 18; 72]. However these methods are not designed to understand and decompose geometry into individual primitives and may also result in invalid CAD sequences. A recent work  also models CAD with language but face similar issues despite allowing multi-view inputs. To address these issues, Point2Cyl  is the pioneering work that poses the CAD reverse engineering task as a geometry-aware decomposition problem. The idea was to first decompose an input point cloud into _extrusion cylinders_ (sketch-extrude primitives) and introduce differentiable loss functions to recover the parameters. ExtrudeNet  and SECAD-Net  further built on this setting and extended it to the unsupervised and self-supervised settings, respectively. Our work tackles a similar setting that takes a geometry-aware approach to the reverse engineering task, except we take multi-view images as input, as opposed to existing works that only handle point cloud inputs.

## 3 MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images

We propose a method that reconstructs 3D extrusion cylinders from multi-view images without relying on raw 3D geometry as input. The idea is to leverage 2D neural networks to learn 2D priors that provide 3D extrusion information (Sec. 3.2), i.e. extrusion curves and surfaces. The integration of the information into 3D is achieved through optimizing neural fields (Sec. 3.3). MV2Cyl is a combination of a curve field and a surface field that is used to recover the parameters and reconstruct the extrusion cylinders only given 2D input images (Sec. 3.4).

### Problem Statement and Background

Given a set of 2D RGB images \(\{_{i}\}_{i=1}^{N}\) where \(^{H W 3}\), the goal is to recover a set of extrusion cylinders \(\{_{j}\}_{j=1}^{K}\) that represent the underlying 3D shape. We provide an overview here but refer the readers to Uy _et al._ for more details.

Figure 1: **Full Pipeline of MV2Cyl.**

**Sketch-Extrude CAD** is a designing paradigm or the designed model itself in the area of computer-aided design (CAD) consists of extrusion cylinders as its building blocks. An ** extrusion cylinder**\(\) is 3D space defined as \(=(,,,},)\) with the **extrusion axis**\(^{2}\), the **extrusion center**\(^{3}\), the **extrusion height**\(\), and the normalized **sketch**\(}\) scaled by \(\). A sketch-extrude CAD model is a 3D shape that is reconstructed from a set of extrusion cylinders, \(\{E_{1},E_{2},...,E_{K}\}\) where K is the number of extrusion cylinders. A **sketch**\(}\) is a set of non-self-intersecting closed loops drawn in a normalized plane.

We further classify the surfaces of a sketch-extrude CAD model as **base** or **barrel**. A surface is a **base** if its surface normal is parallel to its extrusion axis \(\), and is a **barrel** if its surface normal is perpendicular to its extrusion axis. By this definition, base surfaces are parameterized by points on the planes and a normal (\(\)). The base surfaces can further be distinguished into start plane and end plane where the **start plane** contains the point \(-}{2}\), while the **end plane** contains the point \(+}{2}\).

With these definitions, we will later show how our method, MV2Cyl, is able to recover the set of extrusion cylinders and their corresponding parameters given only 2D multi-view images.

### Learning 2D Priors for 3D Extrusions

To reconstruct extrusion cylinders from multi-view images, we first learn 2D priors for 3D extrusions by exploiting 2D convolutional neural networks. We train two U-Net-based 2D segmentation frameworks: \(^{}\) that extracts curve information and \(^{}\) that extracts surface information. The 2D information extracted from the two frameworks is integrated into 3D using a neural field and then utilized for more robust reverse engineering. We detail each 2D segmentation frameworks below.

**2D Surface Segmentation Framework \(^{}\)**. The goal of the surface segmentation network is to extract the extrusion instance segmentation as well as start plane, end plane, and barrel (start-end-barrel) segmentation given an input image. That is \(^{}:^{H W 3}\{ ^{}\{0,...,K\}^{H W},^{} \{0,1,2,3\}^{H W}\}\), where \(K\) is the number of instances, \(^{}\) represents the extrusion instance segmentation label, and \(^{}\) denotes the start-end-barrel segmentation label. The zero index is used for background annotation. \(^{}\) is implemented as two distinct U-Nets predicting each segmentation and is trained using a multi-class cross entropy loss with ground truth labels \(}^{}\) and \(}^{}\). An important property is that the problem does not admit a unique solution as i) the extrusion segment can be ordered arbitrarily and ii) the start and end planes can also be arbitrarily labeled. To handle this, we use Hungarian matching to find the best one-to-one matching with the ground truth labels and reorder it as \(}^{}\) and \(}^{}\). This gives us the loss function of the instance segmentation:

\[^{}_{}=-_{b=1}^{B}_{h=1 }^{H}_{w=1}^{W}_{k=0}^{K}[}^{ }_{hw}=k]^{}_{hwk},\] (1)

where \(^{}_{hwk}\) is the model's predicted probability that pixel \((h,w)\) belongs to the \(k\)-th instance in the \(b\)-th image in the batch, and \(}^{}_{hw}\) is the pseudo GT label for that pixel, with the number of images in a batch \(B\) and the number of possible instances including the background \(K\). The loss function of the start-end-barrel segmentation appears the similar, replacing the \(\) as \(\) and the range of classes {0,..., K} to {0, 1, 2, 3}.

Figure 2: **Example of segmentation prediction. From left to right: input rendered image, surface instance segmentation, surface start-end-barrel segmentation, curve instance segmentation, and curve start-end segmentation.**

**2D Curve Segmentation Framework \(^{}\)**. We observe that detecting and extracting curves on the base planes of each extrusion cylinder segment provides additional information to recover the parameters and reconstruct the extrusion cylinders. Moreover, (feature) curves have been a longstanding and well-explored problem dating back to classical computer vision literature  thanks to their strong expressiveness. This leads us to learn a 2D curve prior that provides more discriminative and detailed outputs to make reverse engineering more robust.

The goal of the curve segmentation framework is to extract both extrusion instance segmentation and start-end segmentation, where the start-end plane segmentation distinguishes the curves of the two base planes of extrusion cylinders. Concretely, \(^{}:^{H W 3}\{ ^{}\{0,...,K\}^{H W},^{ }\{0,1,2\}^{H W}\}\), where \(^{}\) is the extrusion instance segmentation label and \(^{}\) is the start-end segmentation label.

Similar to \(^{}\), the zero index is used for background annotation in both segmentation tasks and \(^{}\) is also implemented with two U-Nets and trained using a multi-class cross-entropy loss against the pseudo ground truth \(\{}^{},}^{}\}\) that is reordered with Hungarian matching to handle order invariance and ambiguities. An additional challenge for curves compared to surfaces is the labels are a lot sparser with the majority of the images being background pixels. Hence, the model can easily fail to predict meaningful labels due to this label imbalance. To alleviate this, we additionally employ a dice loss  to handle the strong foreground-background imbalance and a focal loss  to circumvent the class imbalance between the extrusion instances. Hence the loss function used to train the curve prior is given as:

\[^{}_{}=_{CE}_{}+_{}_{}+_{} _{}.\] (2)

Additional details about the segmentation framework are available in the appendix E.1.

### Integrating 2D Segments into a 3D Field

To establish correspondences and collate the information extracted from multi-view images, a natural approach to _integrate_ 2D information into a consistent 3D representation is through learning a 3D field . For each of our 2D priors \(^{}\) and \(^{}\), we learn a density field \(\) and a semantic field \(\) as detailed below, respectively.

**Density Field \(\)**. We learn a density field for surfaces \(^{}\) and curves \(^{}\). Given a query point in 3D space \(^{3}\), the density field network \(:^{3}\) outputs a scalar value that indicates how likely the 3D point \(\) is on a surface or a curve for \(^{}\) and curves \(^{}\), respectively. To optimize the field differentiably with only 2D images, we use volume rendering  and a learnable transformation  to convert the density field \(()\) to an opacity field \(()\). This transformation is in the form of a learnable sigmoid function given as:

\[()=(1+e^{-(() -)})^{-1}\] (3)

Figure 3: **Overview of the learned surface and curve fields.** (Left-to-Right) Density field of surface, instance semantic field of surface, start-end semantic field of surface, density field of curve, instance semantic field of curve, and start-end semantic field of curve.

where \(\) is a learnable parameter that adjusts the density scale, and \(\) and \(\) are constant hyperparameters. We use the same transformation in Eq. 3 for the surface field \(^{}\) and the curve field \(^{}\), resulting in corresponding opacity fields \(^{}\) and \(^{}\). We then directly render the density by using the volume rendering equation to get the expected surface and curve density for a camera ray \(\), which is given by the integrals

\[^{}() =_{t_{n}}^{t_{f}}T^{}(t)^{}((t))dt,\] (4) \[^{}() =_{t_{n}}^{t_{f}}T^{}(t)^{}( (t))dt,\] (5)

where \(T^{}(t)=_{0}^{t}^{}((s))ds\) and \(T^{}(t)=_{0}^{t}^{}((s))ds\) are the corresponding transmittance. Inspired by NEF , we train our surface field \(^{}\) and curve field \(^{}\) using an adaptive L2 loss for the reconstruction of pixel density and a sparsity loss for learning of a sharp distribution of density along the ray. The objective function is given as follows:

\[L_{}= }}_{}W_{}( )||E()-()||_{2}^{2}\] (6) \[+_{sparsity}}}_{i,j}()(1+(_{i}(t)_{j})^{2}}{s}),\]

where \(E()\) and \(()\) are the 2D-observed and predicted field density, respectively, for both the surface and curve models, \(i\) indexes background pixels from input maps, \(j\) indexes the sample points along the rays, and \(s\) determines the scale of the regularizer. The 2D-observed density \(E()\) is derived from the output of the 2D prior being a foreground pixel, either start, end, or barrel pixel.

Following Ye _et al._, \(W_{}()\) is given as:

\[W_{}()=}}{N_{}}&>_{}\\ 1-}}{N_{}}&\] (7)

The weight \(W_{}(r)\) addresses the imbalance between foreground and background areas in the density maps, preventing the network from converging to a local minimum that falsely classifies most pixels as background, which are more prevalent.

**Semantic Field \(\)**. Now with the density field, we can distill the information from the 2D priors by learning semantic fields \(\) for both surfaces and curves, which we denote as \(^{}\) and \(^{}\). Following Zhi _et al._, each semantic is modeled as an additional feature channel branching out from the density field \(\). For surfaces, the additional semantic field \(^{}=\{p^{},q^{}\}\) is comprised of two networks \(p^{}:^{3}^{(K+1)}\) that predicts the extrusion instance label, and \(q^{}:^{3}^{4}\) that predicts the start-end-barrel segmentation. Similarly for the curves, the semantic field is given as \(^{}=\{p^{},q^{}\}\), where \(p^{}:^{3}^{(K+1)}\) predicts the extrusion instance label, and \(q^{}:^{3}^{3}\) predicts the start-end segmentation. The expected semantic for a ray \(\) is then computed using the volume rendering equation which is given as:

\[()=_{t_{n}}^{t_{f}}T(t)((t))a(( t))dt,\] (8)

for each \(a\{p^{},q^{},p^{},q^{}\}\) that correspond to rendered predicted semantics \(\{}^{},}^{},}^{},}^{}\}\) from the 3D field.

We train our semantic field using the predictions from our learned 2D priors \(^{}\) and \(^{}\). For each of the 2D multi-view images, we can extract the 2D observed labels \(A\{^{},^{},^{ },^{}\}\) from the learned priors, which we use as supervision. As mentioned in Sec. 3.2, since the labels may not individually be consistent across the multiple views, we use Hungarian matching during training to align the labels across multiple views based on the best one-to-one matching. For each semantic, we use a multi-class cross-entropy loss to train and optimize the semantic field with the given supervision:\[L_{semantic}=-_{}_{l=0}^{L}A^{l}()^{l}( ),\] (9)

where \(A^{l}()\) and \(^{l}()\) represent the class \(l\) semantic probabilities from the learned 2D prior and the predicted field, respectively, aligned with Hungarian matching.

**Training and Implementation Details**. The integration into a coherent 3D field is optimized independently for each 3D shape. We use TensoRF  as the backbone representation for the 3D fields, where we have one model for the surface field and the other for the curve field. In both models, we train the density fields \(\) and the semantic fields \(\) together for 1500 iterations. Each semantic field is parameterized by a 2-layer MLP that outputs logit values. Additional implementation details are provided in the Appendix E.2.

### Reverse Engineering from 3D Fields

We now elaborate on how we leverage our optimized surface field \(^{}\) and curve field \(^{}\) for the reverse engineering task, specifically to recover extrusion cylinders and their defining parameters, \((,,,},)\). Each point within the extracted point clouds is assigned semantic information queried from the associated semantic fields, providing context for each feature in 3D space. Using these enriched point clouds, we employ a multi-step process to reconstruct each extrusion cylinder, iteratively refining the parameters to accurately fit the geometry and semantics. Further details can be found in Appendix E.3.

1. **Extrusion Axis \(\) Estimation**: For each extrusion cylinder, the extrusion axis \(\) is computed through plane fitting via RANSAC  on the surface base points with corresponding extrusion cylinder instance label. The axis \(\) is then set as the normal of the fitted plane. This process relies on surface information because plane fitting is more reliable using the surface points. If the base face of the cylinder fails to be reconstructed due to occlusion, we use curve points as a substitute.
2. **2D Sketch \(}\) Estimation**: To obtain the sketch \(}\) for each extrusion cylinder, we first project the curve point cloud for each extrusion instance onto the plane with normal \(\). From the projected point cloud, we then compute for the sketch scale \(\). We then optimize an implicit signed distance function for each closed loop using the projected point cloud with IGR . Finally, we obtain the closed loop by extracting the zero-level set of the implicit function. Implicit sketches can be converted to a parametric representation using an off-the-shelf spline fitting module. We provide example visualizations in Appendix E.7.
3. **Extrusion height \(\) Estimation**: The height \(\) of each extrusion cylinder is computed as the distance between the two sketch centers -- from the start plane and the end plane. The sketch centers are computed from the projected curve point cloud for each paired labels (extrusion instance segmentation, start-end segmentation) onto the plane with normal \(\). If any of the start-end planes are occluded, the height is computed using the barrel points from the surface point cloud with the corresponding extrusion cylinder instance label.
4. **Extrusion centroid \(\) Estimation**: The centroid \(\) of each extrusion cylinder can be obtained by taking the average position of the 3D curve point cloud with the corresponding extrusion cylinder instance label. Similar to the above, if any of the start-end planes are occluded or missing, the centroid is computed as the average of the barrel points from the surface point cloud with the corresponding extrusion cylinder instance label.

## 4 Experiments

In this section, we present our experimental evaluation to demonstrate the feasibility of our MV2Cyl in reconstructing extrusion cylinders given only multi-view images. To the best of our knowledge, our proposed pipeline is the first to tackle extrusion cylinder recovery directly from 2D input images, and we show that our method is outperforms existing methods that take clean geometry as input and their modifications to handle raw input images.

**Datasets**. We evaluate our approach on two sketch-extrude CAD datasets Fusion360  and DeepCAD . We use the train and test splits as released in . We enrich these datasets with multi-view image renderings and 2D curve and surface segmentation maps for the training and evaluation of our method and the baselines. More details about dataset generation are in Appendix E.4.

**Evaluation Metrics**. Following , we report the average extrusion-axis error (E.A.), extrusion-center error (E.C.), per-extrusion cylinder fitting loss (Fit Cyl.), and global fitting loss (Fit Glob.). Additionally, we also report the extrusion-height error (E.H.) defined as the L1 distance between the GT and predicted height value: \(_{k=1}^{K}|}_{k}-_{k}|\). These metrics evaluate how well the methods reconstruct and recover the parameters of the extrusion cylinders. The final scores are the average of the metrics calculated over all test shapes.

**Baselines**. In the absence of established methods for transforming multi-view images into sketch-extrude CAD models within shape space, we benchmark our model against a point cloud to sketch-extrude CAD reconstruction technique  and its variant to validate the effectiveness of our proposed approach.

* We compare with **Point2Cyl **, a technique for reconstructing sketch-extrude CAD given input 3D point clouds. The point clouds are sampled from clean ground truth CAD meshes and are utilized as the input for this baseline. Point2Cyl  initially segments the input point cloud into distinct instances and base/barrel segments, then identifies the extrusion axis as the direction that aligns with the normals of the points in the base segments and is orthogonal to the normals of the points in the barrel segments. The rest of the parameters are inferred based on this predicted axis and the segmentation labels. To evaluate the network, we trained it using the official code released by the authors.
* **NeuS2 +Point2Cyl ** combines NeuS2  with Point2Cyl . Since no prior work has reconstructed extrusion cylinders directly from multi-view images, we combine methods for i) shape reconstruction from multi-view images, with techniques for ii) extrusion cylinder reconstruction from raw geometry. This approach utilizes NeuS2 , an off-the-shelf image-based surface reconstruction technique, that optimizes the photometric loss to learn underlying implicit 3D representations. For this baseline, we first reconstruct the object surface from multi-view images using NeuS2 , and then sample a point cloud from the reconstructed mesh. The sampled point cloud is then directly fed into Point2Cyl  to predict sketches and extrusion parameters of CAD models.

### Results

Tab. 1 shows the quantitative results of CAD reconstruction on the Fusion360  and DeepCAD  datasets. We see that MV2Cyl outperforms the baselines by considerable margins across both datasets in all evaluated metrics. Qualitative comparisons are shown in Fig. 5.

Figure 4: Converting 3D reconstructed geometry and semantics into CAD parameters.

The results of NeuS2 +Point2Cyl  illustrate the challenges in naively using multi-view images for CAD reconstruction, where inaccurate 3D geometry reconstructed from images serve as a critical bottleneck in recovering CAD parameters. We observe that it struggles to distinguish the object's intrinsic color (albedo) from the effects of lighting or shadows (shading). This can result in discrepancies, such as omitted details or subtle smoothing at the intersection between different extrusion segments, as illustrated in (Fig. 5). As seen in Fig. 5 (c), a common failure arises when NeuS2  fails to reconstruct fine geometric details, in this case, the boundaries between the extrusion cylinders. Such artifacts in the reconstructed mesh make the conversion from the point cloud to the CAD model unreliable.

MV2Cyl also outperforms Point2Cyl , which takes precise 3D geometric information as input. An advantage of taking multi-view images instead of point clouds is the images contain richer information such as edges and curves. Moreover, 2D neural networks have also shown superior performance compared to 3D processing networks leading to superior performance of MV2Cyl compared to Point2Cyl . The comparison of the 2D segmentation network in MV2Cyl with the 3D segmentation network in Point2Cyl are found in Appendix F.1.

   Dataset & Method & E.A.(\({}^{}\))\(\) & E.C.\(\) & E.H.\(\) & Fit. & Fit. \\  & & & & & & \(\) & Glob.\(\) \\  Fusion & NeuS2+Point2Cyl & 35.0562 & 0.2198 & 0.7802 & 0.1036 & 0.0596 \\
360 & Point2Cyl & 9.5228 & 0.0839 & 0.2918 & 0.0731 & 0.0293 \\  & **MV2Cyl (Ours)** & 1.3939 & 0.0385 & 0.1423 & 0.0284 & 0.0212 \\  Deep & NeuS2+Point2Cyl & 54.8715 & 0.0650 & 0.8471 & 0.0903 & 0.0638 \\ CAD & Point2Cyl & 7.9156 & 0.0266 & 0.1632 & 0.0420 & 0.0250 \\  & **MV2Cyl (Ours)** & 0.2202 & 0.0121 & 0.0761 & 0.0246 & 0.0229 \\   

Table 1: **Quantitative results using Fusion360  and DeepCAD  datasets.** MV2Cyl consistently outperforms all baselines in all metrics. Red denotes the best; orange denotes the second best.

Figure 5: **Qualitative comparisons with the baselines.** Each instance is identified by a different color. MV2Cyl produces high-quality geometry and even outperforms Point2Cyl  that directly consumes 3D point clouds. Furthermore, the comparison against a naive baseline that pipelines NeuS2 , a multi-view surface reconstruction technique, to Point2Cyl demonstrates the importance of edge information when inferring 3D structures.

Unlike the baselines that require accurate 3D surface information, MV2Cyl is able to directly handle 2D multi-view images as input by leveraging on learned 2D priors to extract 3D extrusion information. We show that extracting extrusion surfaces and curves from 2D images allows us to leverage both the curve and surface representation for better extrusion cylinder reconstruction compared to baselines that even require clean 3D geometry as input.

### Real Object Reconstruction

We validate MV2Cyl on a real-world demo by 3D-printing various sketch-extrude objects from the Fusion360 test set, by capturing multi-view images with an iPhone 12 Mini and then obtaining camera poses via COLMAP. These captured images were processed through our pipeline to extract 2D information, converted to 3D using our density and semantic fields to recover extrusion cylinders and parameters. To address the domain gap between synthetic training images and real-world data, we applied preprocessing techniques, including grayscale conversion, background removal, and then fine-tuning a large vision model. Additional details are provided in Appendix B.

## 5 Conclusion

We introduce MV2Cyl, a novel method for reconstructing a set of extrusion cylinders in 3D from multi-view images. Unlike previous approaches that rely only on raw 3D geometry as input, our framework utilized 2D multi-view images and demonstrates superior performance by leveraging the capabilities of 2D convolutional neural networks in image segmentation. Through the analysis of curve and surface representations, each having its own advantages and shortcomings, we propose an integration of both that takes the best of both worlds, achieving optimal performance.

**Limitations and Future Work**. Our proposed method is not without its limitations. While MV2Cyl demonstrates state-of-the-art performance in reconstructing sketch-extrude CAD models, it shares a limitation with previous approaches, such as Point2Cyl , regarding the prediction of binary operations among primitives. MV2Cyl does not explicitly predict these binary operations, which we leave as future work. Nonetheless, we propose an initial straightforward method for recovering binary operations through exhaustive search. Details are provided in the appendix E.6. Additionally, MV2Cyl does not directly accommodate textured CAD models, as the 2D segmentation models are trained on a synthetic dataset derived from rendering untextured CAD models. By utilizing generalizable large 2D models, such as Segment Anything , we can preprocess images of textured CAD models for segmentation by extracting object masks and removing textures. We also leave this for future investigation.

MV2Cyl is also susceptible to occlusion since it relies on 2D image inputs. Specifically, if one side of an extrusion cylinder is completely hidden by others, our 2D segmentation model cannot detect the hidden side. In such cases, the extrusion cylinder cannot be reconstructed. In Fig. 6, the left shape represents the target CAD model, while the right one shows the reconstructed model by MV2Cyl. The target shape features an inset hexagonal cylinder with one end hidden by an outer cylinder, but MV2Cyl was unable to reconstruct the inset extrusion cylinder.

**Societal Impacts**. This technology could make CAD more accessible to non-professionals or educational sectors, allowing more people to engage in design and engineering tasks without needing extensive training in traditional CAD software. However, such a 3D reconstruction method can also raise privacy concerns. By converting a physical object into a digital format, the shape could be reused for design purposes without the original creator's permission.

Figure 6: **Example of a failure case.**