# Linear Mode Connectivity in

Differentiable Tree Ensembles

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

_Linear Mode Connectivity_ (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for validating the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging. While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question. In this paper, we first achieve LMC for _soft tree ensembles_, which are tree-based differentiable models extensively used in practice. We show the necessity of incorporating two invariances: _subtree flip invariance_ and _splitting order invariance_, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees. Moreover, we demonstrate that it is even possible to exclude such additional invariances while keeping LMC by designing _decision list_-based tree architectures, where such invariances do not exist by definition. Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC.

## 1 Introduction

A non-trivial empirical characteristic of modern machine learning models trained using gradient methods is that models trained from different random initializations could become functionally almost equivalent, even though their parameter representations differ. If the outcomes of all training sessions converge to the same local minima, this empirical phenomenon can be understood. However, considering the complex non-convex nature of the loss surface, the optimization results are unlikely to converge to the same local minima. In recent years, particularly within the context of neural networks, the transformation of model parameters while preserving functional equivalence has been explored by considering the _permutation invariance_ of neurons in each hidden layer [1; 2]. Notably, only a slight performance degradation has been observed when using weights derived through linear interpolation between permuted parameters obtained from different training processes [3; 4]. This demonstrates that the trained models reside in different, yet functionally equivalent, local minima. This situation is referred to as _Linear Mode Connectivity_ (LMC) . From a theoretical perspective, LMC is crucial for supporting the stable and successful application of non-convex optimization. In addition, LMC also holds significant practical importance, enabling techniques such as model merging [6; 7] by weight-space parameter averaging.

Although neural networks are most extensively studied among the models trained using gradient methods, other models also thrive in real-world applications. A representative is tree ensemble models, such as random forests . While they are originally trained by not gradient but greedy algorithms, differentiable _soft tree ensembles_, which learn parameters of the entire model through gradient-basedoptimization, have recently been actively studied. Not only empirical studies regarding accuracy and interpretability [9; 10; 11], but also theoretical analyses have been performed [12; 13]. Moreover, the differentiability of soft trees allows for integration with various deep learning methodologies, including fine-tuning , dropout , and various stochastic gradient descent methods [16; 17]. Furthermore, the soft tree represents the most elementary form of a hierarchical mixture of experts [18; 19; 20]. Investigating soft tree models not only advances our understanding of this particular structure but also contributes to broader research into essential technological components critical for the development of large-scale language models .

A research question that we tackle in this paper is: "Can LMC be achieved for soft tree ensembles?". Our empirical results, which are highlighted with a green line in the top left panel of Figure 1, clearly show that the answer is "Yes". This plot shows the variation in test accuracy when interpolating weights of soft oblivious trees, perfect binary soft trees with shared parameters at each depth, trained from different random initializations. The green line is obtained by our method introduced in this paper, where there is almost zero performance degradation. Furthermore, as shown in the bottom left panel of Figure 1, the performance can even improve when interpolating between models trained on split datasets.

The key insight is that, when performing interpolation between two model parameters, considering only tree permutation invariance, which corresponds to the permutation invariance of neural networks, is _not sufficient_ to achieve LMC, as shown in the orange lines in the plots. An intuitive understanding of this situation is also illustrated in the right panel of Figure 1. To achieve LMC, that is, the green lines, we show that two additional invariances beyond tree permutation, _subtree flip invariance_ and _splitting order invariance_, which inherently exist for tree architectures, should be accounted for.

Moreover, we demonstrate that it is possible to exclude such additional invariances while preserving LMC by modifying tree architectures. We realize such an architecture based on _a decision list_, a binary tree structure where branches extend in only one direction. By designating one of the terminal leaves as an empty node, we introduce a customized decision list that omits both subtree flip invariance and splitting order invariance, and empirically show that this can achieve LMC by considering only tree permutation invariance. Since incorporating additional invariances is computationally expensive, we can efficiently perform weight-space averaging in model merging on our customized decision lists.

Our contributions are summarized as follows:

* First achievement of LMC for tree ensembles with accounting for additional invariances beyond tree permutation.
* Development of a decision list-based tree architecture that does not involve the additional invariances.
* A thorough empirical investigation of LMC across various tree architectures, invariances, and real-world datasets.

## 2 Preliminary

We prepare the basic concepts of LMC and soft tree ensembles.

### Linear Mode Connectivity

Let us consider two models, \(A\) and \(B\), that have the same architecture. In the context of evaluating LMC, the concept of a "barrier" is frequently used [4; 23]. Let \(_{A},_{B}^{P}\) be vectorized parameters of models \(A\) and \(B\), respectively, for \(P\) parameters. Assume that \(:^{P}\) measures the performance of the model, such as accuracy, given its parameter vector. If higher values of \(()\)

Figure 1: A representative experimental result on the MiniBooNE  dataset (left) and conceptual diagram of the LMC for tree ensembles (right).

mean better performance, the barrier between two parameter vectors \(_{A}\) and \(_{B}\) is defined as:

\[(_{A},_{B})=_{} [\,(_{A})+(1-)(_ {B})-(_{A}+(1-)_{B})\,].\] (1)

We can simply reverse the subtraction order if lower values of \(()\) mean better performance like loss.

Several techniques have been developed to reduce barriers by transforming parameters while preserving functional equivalence. Two main approaches are _activation matching_ (AM) and _weight matching_ (WM). AM takes the behavior of model inference into account, while WM simply compares two models using their parameters. The validity of both AM and WM has been theoretically supported . Numerous algorithms are available for implementing AM and WM. For instance,  uses a formulation based on the Linear Assignment Problem (LAP) to find suitable permutations, while  employs a differentiable formulation that allows for the optimization of permutations using gradient-based methods.

Existing research has focused exclusively on neural network architectures such as multi-layer perceptrons (MLP) and convolutional neural networks (CNN). No study has been conducted from the perspective of linear mode connectivity for soft tree ensembles.

### Soft Tree Ensemble

Unlike typical hard decision trees, which explicitly determine the data flow to the right or left at each splitting node, soft trees represent the proportion of data flowing to the right or left as continuous values between 0 and 1. This approach enables a differentiable formulation.

We use a \(\) function, \(:(0,1)\) to formulate a function \(_{m,}(_{i},_{m},_{m}):^{F} ^{F}^{1}(0,1)\) that represents the proportion of the \(i\)th data point \(_{i}\) flowing to the \(\)th leaf of the \(m\)th tree as a result of soft splittings:

\[_{m,}(_{i},_{m},_{m}) =_{n=1}^{}_{m,n}^{} _{i}+b_{m,n})}_{}}^{}_{ ^{} n}}(_{m,n}^{}_{i}+b_{m,n })}_{})^{}_{n}},\] (2)

where \(\) denotes the number of splitting nodes in each tree. The parameters \(_{m,n}^{F}\) and \(b_{m,n}\) correspond to the feature selection mask and splitting threshold value for \(n\)th node in a \(m\)th tree, respectively. The expression \(_{^{} n}\) (resp. \(_{n}\)) is an indicator function that returns 1 if the \(\)th leaf is positioned to the left (resp. right) of a node \(n\), and 0 otherwise.

If parameters are shared across all splitting nodes at the same depth, such perfect binary trees are called _oblivious trees_. Mathematically, \(_{m,n}=_{m,n^{}}\) and \(b_{m,n}=b_{m,n^{}}\) for any nodes \(n\) and \(n^{}\) at the same depth in an oblivious tree. Oblivious trees can significantly reduce the number of parameters from an exponential to a linear order of the tree depth, and they are actively used in practice [9; 11].

To classify \(C\) categories, the output of the \(m\)th tree is computed by the function \(f_{m}:^{F}^{F}^{1 }^{C}^{C}\) as sum of the leaf parameters \(_{m,}\) weighted by the outputs of \(_{m,}(_{i},_{m},_{m})\):

\[f_{m}(_{i},_{m},_{m},_{m})=_{= 1}^{}_{m,}_{m,}(_{i},_{m},_{m}),\] (3)

where \(\) is the number of leaves in a tree. By combining this function for \(M\) trees, we realize the function \(f:^{F}^{M F} ^{M 1}^{M C} ^{C}\) as an ensemble model consisting of \(M\) trees:

\[f(_{i},,,)=_{m=1}^{M}f_{m}(_ {i},_{m},_{m},_{m}),\] (4)

with the parameters \(=(_{1},,_{M})\), \(=(_{1},,_{M})\), and \(=(_{1},,_{M})\) being randomly initialized.

Despite the apparent differences, there are correspondences between MLPs and soft tree ensemble models. The formulation of a soft tree ensemble with \(D=1\) is:

\[f(_{i},,,) =_{m=1}^{M}\,(_{m,1}^{}_{i}+b_{m,1 })_{m,1}+(1-(_{m,1}^{}_{i}+b_{m,1}))_{m,2 }\,\] \[=_{m=1}^{M}\,(_{m,1}-_{m,2})( {w}_{m,1}^{}_{i}+b_{m,1})+_{m,2}\,.\] (5)

When we consider the correspondence between \(_{m,1}-_{m,2}\) in tree ensembles and second layer weights in the two-layer perceptron, the tree ensembles model matches to the two-layer perceptron. It is clear from the formulation that the permutation of hidden neurons in a neural network corresponds to the rearrangement of trees in a tree ensemble.

## 3 Invariances Inherent to Tree Ensembles

In this section, we discuss additional invariances inherent to trees (Section 3.1) and introduce a matching strategy specifically for tree ensembles (Section 3.2). We also show that the presence of additional invariances varies depending on the tree structure, and we present tree structures where no additional invariances beyond tree permutation exist (Section 3.3).

### Parameter modification processes that maintains functional equivalence in tree ensembles

First, we clarify what invariances should be considered for tree ensembles, which are expected to reduce the barrier significantly if taken into account. When we consider perfect binary trees, there are three types of invariance:

* **Tree permutation invariance.** In Equation (4), the behavior of the function does not change even if the order of the \(M\) trees is altered. This corresponds to the permutation of internal nodes in neural networks, which has been a subject of active interest in previous studies on LMC.
* **Subtree flip invariance.** When the left and right subtrees are swapped simultaneously with the inversion of the inequality sign at the split, the functional behavior remains unchanged, which we refer to _subtree flip invariance_. Figure 2(a) presents a schematic diagram of this invariance, which is not found in neural networks but is unique to binary tree-based models. Since \((-c)=1-(c)\) for \(c\) due to the symmetry of \(\), the inversion of the inequality is achieved by inverting the signs of \(_{m,n}\) and \(b_{m,n}\).  also focused on the sign of weights, but in a different way from ours. They pay attention to the amount of change from the parameters at the start of fine-tuning, rather than discussing the sign of the parameters.
* **Splitting order invariance.** Oblivious trees share parameters at the same depth, which means that the decision boundaries are straight lines without any bends. With this characteristic, even if the splitting rules at different depths are swapped, functional equivalence can be achieved if the positions of leaves are also swapped appropriately as shown in Figure 2(b). This invariance does not exist for non-oblivious perfect binary trees without parameter sharing, as the behavior of the decision boundary varies depending on the splitting order.

Figure 2: (a) Subtree flip invariance. (b) Splitting order invariance for an oblivious tree.

Note that MLPs also have an additional invariance beyond just permutation. Particularly in MLPs that employ ReLU as an activation function, the output of each layer changes linearly with a zero crossover. Therefore, it is possible to modify parameters without changing functional behavior by multiplying the weights in one layer by a constant and dividing the weights in the previous layer by the same constant. However, since the soft tree is based on the sigmoid function, this invariance does not apply. Previous studies [3; 4; 23] have consistently achieved significant reductions in barriers without accounting for this scale invariance. One potential reason is that changes in parameter scale are unlikely due to the nature of optimization via gradient descent. Conversely, when we consider additional invariances inherent to trees, the scale is equivalent to the original parameters.

### Matching Strategy

Here, we propose a matching strategy for binary trees. When considering invariances, it is necessary to compare multiple functionally equivalent trees and select the most suitable one for achieving LMC. Although comparing tree parameters is a straightforward approach, since the contribution of all the parameters in a tree is not equal, we apply weighting for each node for better matching. By interpreting a tree as a rule set with shared parameters as shown in Figure 3, we determine the weight of each splitting node by counting the number of leaves to which the node affects. For example, in the case of the left example in Figure 3, the root node affects eight leaves, nodes at depth \(2\) affect four leaves, and nodes at depth \(3\) affect two leaves. This strategy can apply to even trees other than perfect binary trees. For example, in the right example of Figure 3, the root node affects four leaves, a node at depth \(2\) affects three leaves, and a node at depth \(3\) affects two leaves.

In this paper, we employ the LAP, which is used as a standard benchmark  for matching algorithms. The procedures for AM and WM are as follows. Detailed algorithms (Algorithms 1 and 2) are described in Section A in the supplementary material.

* **Activation Matching (Algorithm 1).** In trees, there is nothing that directly corresponds to the activations in neural networks. However, by treating the output of each individual tree as an activation value of a neural network, it is possible to optimize the permutation of trees while examining their output similarities. Regarding subtree flip and splitting order invariances, it is possible to find the optimal pattern from all the possible patterns of flips and changes in the splitting order. Since the tree-wise output remains unchanged, the similarity between each tree, generated by considering additional invariances, and the target tree is evaluated based on the inner product of parameters while applying node-wise weighting.
* **Weight Matching (Algorithm 2).** Similar to AM, WM also involves applying weighting while extracting the optimal pattern by exploring possible flipping and ordering patterns. Although it is necessary to solve the LAP multiple times for each layer in MLPs , tree ensembles require only a single run of the LAP since there are no layers.

The time complexity of solving the LAP is \((M^{3})\) using a modified Jonker-Volgenant algorithm without initialization , implemented in SciPy , where \(M\) is the number of trees. If only considering tree permutation, this process needs to be performed only once in both WM and AM. However, when considering additional invariances, we need to solve the LAP for each pattern generated by considering these additional invariances. In a non-oblivious perfect binary tree with depth \(D\), there are \(2^{D}-1\) splitting nodes, leading to \(2^{2^{D}-1}\) possible combinations of sign flips. Additionally, in the case of oblivious trees, there are \(D!\) different patterns of splitting order invariance. Therefore, for large values of \(D\), conducting a brute-force search becomes impractical.

In Section 3.3, we will discuss methods to eliminate additional invariance by adjusting the tree structure. This enables efficient matching even for deep models. Additionally, in Section 4.2, we will present numerical experiment results and discuss that the practical motivation to apply these algorithms is limited when targeting deep perfect binary trees.

Figure 3: Weighting strategy.

### Architecture-dependency of the Invariances

In previous subsections, tree architectures are fixed to perfect binary trees as they are most commonly and practically used in soft trees. However, tree architectures can be flexible as we have shown in the right example in Figure 3, and here we show that we can specifically design tree architecture that has neither the subtree flip nor splitting order invariances. This allows efficient matching as considering such two invariances is computationally expensive.

Our idea is to modify a _decision list_ shown on the left side of Figure 4, which is a tree structure where branches extend in only one direction. Due to this asymmetric structure, the number of parameters does not increase exponentially with the depth, and the splitting order invariance does not exist. Moreover, subtree flip invariance also does not exist for any internal nodes except for the terminal splitting node, as shown in the left side of Figure 4. To completely remove this invariance, we virtually eliminate one of the terminal leaves by leaving the node empty, that is, a fixed prediction value of zero, as shown on the right side of Figure 4. Therefore only permutation invariance exists for our proposed architecture. We summarize invariances inherent to each model architecture in Table 1.

## 4 Experiment

We empirically evaluate barriers in soft tree ensembles to examine LMC.

### Setup

Datasets.In our experiments, we employed Tabular-Benchmark , a collection of tabular datasets suitable for evaluating tree ensembles. Details of datasets are provided in Section B in the supplementary material. As proposed in , we randomly sampled \(10,000\) instances for train and test data from each dataset. If the dataset contains fewer than \(20,000\) instances, they are randomly divided into halves for train and test data. We applied quantile transformation to each feature and standardized it to follow a normal distribution.

Hyperparameters.We used three different learning rates \(\{0.01,0.001,0.0001\}\) and adopted the one that yields the highest training accuracy for each dataset. The batch size is set at \(512\). It is known that the optimal settings for the learning rate and batch size are interdependent . Therefore, it is reasonable to fix the batch size while adjusting the learning rate. During AM, we set the amount of data used for random sampling to be the same as the batch size, thus using \(512\) samples to measure the similarity of the tree outputs. As the number of trees \(M\) and their depths \(D\) vary for each experiment, these details will be specified in the experimental results section. During training, we minimized cross-entropy using Adam  with its default hyperparameters1. Training is conducted for \(50\) epochs. To measure the barrier using Equation (1), experiments were conducted by interpolating between two models with \(\{0,1/24,,23/24,1\}\), which has the same granularity as in .

Randomness.We conducted experiments with five different random seed pairs: \(r_{A}\{1,3,5,7,9\}\) and \(r_{B}\{2,4,6,8,10\}\). As a result, the initial parameters and the contents of the data mini-batches during training are different in each training. In contrast to spawning  that branches off from the exact same model pathway through, we used more challenging practical conditions. The parameters \(\), \(\), and \(\) were randomly initialized using a uniform distribution, identical to the procedure for a fully connected layer in the MLP2.

    & **Perm** & **Flip** & **Order** \\  Non-Oblivious Tree & ✓ & ✓ & \(\) \\ Oblivious Tree & ✓ & ✓ & ✓ \\ Decision List & ✓ & (✓) & \(\) \\ Decision List (Modified) & ✓ & \(\) & \(\) \\   

Table 1: Invariances inherent to each model architecture.

Figure 4: Tree architecture where neither subtree flip invariance nor splitting order invariance exists.

**Resources.** All experiments were conducted on a system equipped with an Intel Xeon E5-2698 CPU at 2.20 GHz, 252 GB of memory, and Tesla V100-DGXS-32GB GPU, running Ubuntu Linux (version 4.15.0-117-generic). The reproducible PyTorch  implementation is provided in the supplementary material.

### Results for Perfect Binary Trees

Figure 5 shows how the barrier between two perfect binary tree model pairs changes in each operation. The vertical axis of each plot in Figure 5 shows the averaged barrier over datasets for each considered invariance. The results for both the oblivious and non-oblivious trees are plotted separately in a vertical layout. The panels on the left display the results when the depth \(D\) of the tree varies, keeping \(M=256\) constant. The panels on the right show the results when the number of trees \(M\) varies, with \(D\) fixed at \(2\). For both oblivious and non-oblivious trees, we observed that the barrier significantly decreases as the considered invariances increase. Focusing on the test data results, after accounting for various invariances, the barrier is nearly zero, indicating that LMC has been achieved. In particular, the difference between the case of only permutation and the case where additional invariances are considered tends to be larger in the case of AM. This is because parameter values are not used during the rearrangement of the tree in AM. Additionally, it has been observed that the barrier increases as trees become deeper, and the barrier decreases as the number of trees increases. These behaviors correspond to the changes observed in neural networks when the depth varies or when the width of hidden layers increases . Figure 6 shows interpolation curves when using AM in oblivious trees

Figure 5: Barriers averaged across 16 datasets with respect to considered invariances for non-oblivious (top row) and oblivious (bottom row) trees. The error bars show the standard deviations of 5 executions.

Figure 6: Interpolation curves of test accuracy for oblivious trees on 16 datasets from Tabular-Benchmark . Two model pairs are trained with on the same dataset. The error bars show the standard deviations of \(5\) executions. We used \(M=256\) trees with a depth \(D=2\).

with \(D=2\) and \(M=256\). Other detailed results, such as performance for each dataset, are provided in Section C in the supplementary material.

Furthermore, we conducted experiments with split data following the protocol in , where the initial split consists of randomly sampled 80% negative and 20% positive instances, and the second split inverts these ratios. There is no overlap between the two split datasets. We trained two model pairs using these separately split datasets and observed an improvement in performance by interpolating their parameters. Figure 7 illustrates the interpolation curves under AM in oblivious trees with parameters \(D=2\) and \(M=256\). We can observe that considering additional invariances improves performance after interpolation. Note that the data split is configured to remain consistent even when the training random seeds differ. Detailed results for each dataset using WM or AM are provided in Section C of the supplementary material.

Table 2 compares the average test barriers of an MLP with a ReLU activation function, whose width is equal to the number of trees, \(M=256\). The procedure for MLPs follows that described in Section 4.1. The permutation for MLPs is optimized using the method described in . Since  indicated that WM outperforms AM in neural networks, WM was used for the comparison. Overall, tree models exhibit smaller barriers compared to MLPs while keeping similar accuracy levels. It is important to note that MLPs with \(D>1\) tend to have more parameters at the same depth compared to trees, leading to more complex optimization landscapes. Nevertheless, the barrier for the non-oblivious tree at \(D=3\) is smaller than that for the MLP at \(D=2\), even with more parameters. Furthermore, at the same depth of \(D=1\), tree models have a smaller barrier. Here, the model size is evaluated using \(F=44\), the average input feature size of 16 datasets used in the experiments.

In Section 3.2, we have shown that considering additional invariances for deep perfect binary trees is computationally challenging, which may suggest developing heuristic algorithms for deep trees. However, we consider it is rather a low priority, supported by our observations that the barrier tends to increase as trees deepen even if we consider invariances. This trend indicates that deep models are fundamentally less important for model merging considerations. Furthermore, deep perfect binary trees are rarely used in practical scenarios.  have demonstrated that generalization performance degrades with increasing depth in perfect binary trees due to the degeneracy of the Neural Tangent Kernel (NTK) . This evidence further supports the preference for shallow perfect binary trees, and increasing the number of trees can enhance the expressive power while reducing barriers.

    &  \\ 
**Depth** &  &  & **Size** \\   & **NaNa** & **Perm**  & **Accuracy** & **Size** \\ 
1 & 8.955 \(\) 0.877 & 0.491 \(\) 0.062 & 76.286 \(\) 0.094 & 12034 \\
2 & 15.341 \(\) 1.125 & 2.997 \(\) 0.709 & 75.981 \(\) 0.139 & 77826 \\
3 & 15.915 \(\) 2.479 & 5.940 \(\) 2.153 & 75.935 \(\) 0.117 & 143618 \\    
    &  \\ 
**Depth** &  &  & **Size** \\   & **NaNa** & **Perm** & **Ours** & & & \\ 
1 & 8.965 \(\) 0.963 & 0.449 \(\) 0.235 & 0.181 \(\) 0.078 & 76.464 \(\) 0.167 & 12544 \\
2 & 6.801 \(\) 0.464 & 0.811 \(\) 0.333 & 0.455 \(\) 0.105 & 76.631 \(\) 0.052 & 36688 \\
3 & 5.602 \(\) 0.926 & 16.85 \(\) 0.334 & 0.740 \(\) 0.158 & 76.339 \(\) 0.115 & 84736 \\    
    &  \\ 
**Depth** &  &  & **Size** \\   & **NaNa** & **Perm** & **Ours** & & & \\ 
1 & 8.965 \(\) 0.963 & 0.449 \(\) 0.235 & 0.181 \(\) 0.078 & 76.464 \(\) 0.167 & 12544 \\
2 & 7.831 \(\) 0.886 & 0.918 \(\) 0.092 & 0.348 \(\) 0.172 & 76.623 \(\) 0.042 & 25088 \\
3 & 7.096 \(\) 0.856 & 1.283 \(\) 0.139 & 0.484 \(\) 0.092 & 76.535 \(\) 0.063 & 38656 \\   

Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.

Figure 7: Interpolation curves of test accuracy for oblivious trees on 16 datasets from Tabular-Benchmark . Two model pairs are trained on split datasets with different class ratios. The error bars show the standard deviations of \(5\) executions. We used \(M=256\) trees with a depth \(D=2\).

### Results for Decision Lists

We present empirical results of the original decision lists and our modified decision lists, as shown in Figure 4. As we have shown in Table 1, they have fewer invariances.

Figure 8 illustrates barriers as a function of depth, considering only permutation invariance, with \(M\) fixed at \(256\). In this experiment, we have excluded non-oblivious trees from comparison as the number of their parameters exponentially increases as trees deepen, making them infeasible computation. Our proposed modified decision lists reduce the barrier more effectively than both oblivious trees and the original decision lists. However, barriers of the modified decision lists are still larger than those obtained by considering additional invariances with perfect binary trees. Tables 3 and 4 show the averaged barriers for 16 datasets, with \(D=2\) and \(M=256\). Although barriers of modified decision lists are small when considering only permutations (Perm), perfect binary trees such as oblivious trees with additional invariances (Ours) exhibit smaller barriers, which supports the validity of using oblivious trees as in [9; 11]. To summarize, when considering the practical use of model merging, if the goal is to prioritize efficient computation, we recommend using our proposed decision list. Conversely, if the goal is to prioritize barriers, it would be preferable to use perfect binary trees, which have a greater number of invariant operations that maintain the functional behavior.

## 5 Conclusion

We have presented the first investigation of LMC for soft tree ensembles. We have identified additional invariances inherent in tree architectures and empirically demonstrated the importance of considering these factors. Achieving LMC is crucial not only for understanding the behavior of non-convex optimization from a learning theory perspective but also for implementing practical techniques such as model merging. By arithmetically combining parameters of differently trained models, a wide range of applications such as task-arithmetic , including unlearning  and continual-learning , have been explored. Our research extends these techniques to soft tree ensembles that began training from entirely different initial conditions. We will leave these empirical investigations for future work.

This study provides a fundamental analysis of ensemble learning, and we believe that our discussion will not have any negative societal impacts.

    &  &  \\   &  &  &  &  \\    & **Naïve** & & & & & & & & & \\   & **Naïve** & **Perm** & **Ours** & & & & & & \\  Non-Oblivious Tree & 13.079 \(\) 0.755 & 14.963 \(\) 1.520 & 4.500 \(\) 0.527 & 85.646 \(\) 0.090 & 6.801 \(\) 0.464 & 8.631 \(\) 1.444 & 0.943 \(\) 0.435 & 76.631 \(\) 0.052 \\ Oblivious Tree & 14.580 \(\) 1.008 & 17.380 \(\) 0.590 & 3.557 \(\) 0.201 & 85.088 \(\) 0.146 & 7.831 \(\) 0.086 & 1.349 \(\) 0.476 & 0.395 \(\) 0.185 & 76.623 \(\) 0.042 \\ Decision List & 13.853 \(\) 0.788 & 12.85 \(\) 1.942 & — & 85.337 \(\) 0.147 & 7.513 \(\) 0.944 & 7.25 \(\) 1.840 & — & 76.7629 \(\) 0.119 \\ Decision List (Modified) & 12.922 \(\) 1.131 & 3.328 \(\) 0.204 & — & 85.563 \(\) 0.141 & 6.734 \(\) 1.096 & 2.114 \(\) 0.243 & — & 76.773 \(\) 0.051 \\   

Table 4: Barriers averaged for 16 datasets under AM with \(D=2\) and \(M=256\).

Figure 8: Averaged barrier for 16 datasets as a function of tree depth. The error bars show the standard deviations of 5 executions. The solid line represents the barrier in train accuracy, while the dashed line represents the barrier in test accuracy.

    &  &  \\   &  &  &  &  \\    & **Naïve** & & & & & & & & \\  Non-Oblivious Tree & 13.079 \(\) 0.755 & 4.707 \(\) 0.332 & 3.303 \(\) 0.104 & 85.646 \(\) 0.059 & 6.801 \(\) 0.464 & 0.811 \(\) 0.333 & 0.455 \(\) 0.105 & 76.631 \(\) 0.052 \\ Oblivious Tree & 13.079 \(\) 1.058 & 4.534 \(\) 0.176 & 2.874 \(\) 0.108 & 85.908 \(\) 0.106 & 7.831 \(\) 0.066 & 0.919 \(\) 0.093 & 0.348 \(\) 0.172 & 76.632 \(\) 0.042 \\ Decision List & 13.835 \(\) 0.788 & 8.678 \(\) 0.230 & — & 85.337 \(\) 0.147 & 7.513 \(\) 0.944 & 0.462 \(\) 0.120 & — & 76.629 \(\) 0.119 \\ Decision List (Modified) & 12.922 \(\) 1.131 & 3.328 \(\) 0.204 & — & 85.563 \(\) 0.141 & 6.734 \(\) 1.096 & 0.468 \(\) 0.150 & — & 76.773 \(\) 0.051 \\   

Table 3: Barriers averaged for 16 datasets under WM with \(D=2\) and \(M=256\).