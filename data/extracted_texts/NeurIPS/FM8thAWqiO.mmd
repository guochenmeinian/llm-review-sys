# Core-sets for Fair and Diverse Data Summarization

Sepideh Mahabadi

Microsoft Research

Redmond, WA, USA

smahabadi@microsoft.com

&Stojan Trajanovski

Microsoft

London, United Kingdom

sttrajan@microsoft.com

This is an equal contribution paper.

###### Abstract

We study core-set construction algorithms for the task of Diversity Maximization under fairness/partition constraint. Given a set of points \(P\) in a metric space partitioned into \(m\) groups, and given \(k_{1},,k_{m}\), the goal of this problem is to pick \(k_{i}\) points from each group \(i\) such that the overall diversity of the \(k=_{i}k_{i}\) picked points is maximized. We consider two natural diversity measures: sum-of-pairwise distances and sum-of-nearest-neighbor distances, and show improved core-set construction algorithms with respect to these measures. More precisely, we show the first constant factor core-set w.r.t. sum-of-pairwise distances whose size is independent of the size of the dataset and the aspect ratio. Second, we show the first core-set w.r.t. the sum-of-nearest-neighbor distances. Finally, we run several experiments showing the effectiveness of our core-set approach. In particular, we apply constrained diversity maximization to summarize a set of timed messages that takes into account the messages' recency. Specifically, the summary should include more recent messages compared to older ones. This is a real task in one of the largest communication platforms, affecting the experience of hundreds of millions daily active users. By utilizing our core-set method for this task, we achieve a 100x speed-up while losing the diversity by only a few percent. Moreover, our approach allows us to improve the space usage of the algorithm in the streaming setting.

## 1 Introduction

Data summarization problem is a class of tasks where a small subset of items must be selected as a summary to represent the whole data. Typically in applications, the selected summary is required to fulfill various criteria such as _diversity_. In this paper, we focus on _Diversity Maximization_ (DM) which is a topic that has attracted significant attention over the past decades . The goal of this line of research is to provide a summary of a large dataset that preserves the diversity of the data as much as possible. This has many applications in various domains including summarization, recommendation systems, search, and facility location . Formally, in this task, given a universe \(P\) of \(n\) items, its goal is to choose a small subset of size \(k\) that maximizes a given pre-specified measure of diversity.

_Diversity Maximization under Partition Constraints_ is a variant of the problem that has been studied to find a diverse summary while satisfying some additional orthogonal constraints . Here, the items in the input are partitioned into \(m\) disjoint groups \(P=P_{1} P_{m}\) and additionally, one is given a pre-specified number of desired results from each group \(k_{1},,k_{m}\), and the goal is to return \(k_{i}\) objects from each group \(P_{i}\) such that the overall diversity among all \(k=_{i}k_{i}\) points is maximized. This formulation allows to control the number of objects from each category in the output, e.g., the number of movies from each genre shown to a user in a recommendation system, or to bound the number of old messages included in a summary of a user's feed. Moreover, thisformulation has been studied in the context of fairness (e.g. see [28; 4; 10]), where one wishes to control the number of results from each population in the produced summary. We will therefore refer to the Diversity Maximization under Partition Constraints as _Fair Diversity Maximization_ (FDM), and use \(_{k_{1},,k_{m}}(P)\) to refer to the optimal achievable diversity under the fairness constraint. In this work, we consider fair diversity maximization with respect to three common diversity measures.

**Diversity Measures.** Several measures (or objective functions) have been proposed in the literature to model the notion of diversity [11; 3; 6; 25; 28; 9; 20; 16; 10; 31; 30]. A main category of such measures are based on pairwise distances (see  for a complete list). In this work, we will consider three of the most common pairwise distance-based measures: (1) the _minimum-pairwise_ distance, where for a subset of points \(S P\), their diversity is measured as the minimum pairwise distance between the points in the subset, i.e., \(_{p,q S}(p,q)\); (2) _sum-of-pairwise_ distances between the points in the subset, i.e., \(_{p,q S}(p,q)\); and (3) the _sum-of-nearest-neighbor_ distances which is an interpolation between the first two measures, and is defined formally as \(_{p S}_{q S\{p\}}(p,q)\). We refer to these measures respectively as Min-Pairwise Dist, Sum-Pairwise Dist, and Sum-NN Dist. All these three measures have been used previously to model diversity (e.g. see [11; 20; 6; 25], where they are respectively referred to as _remote-edge_, _remote-clique_, and _remote-pseudoforest_). In this work, we study Fair Diversity Maximization (FDM) over large datasets with respect to these three diversity measures.

**Core-sets for Diversity Maximization.** As one of the main applications of diversity maximization is related to data summarization, there has been a large body of work to solve diversity maximization in massive data models of computation, and in particular the design of _core-sets_[20; 19; 24; 27; 28; 8; 9; 41; 25; 26; 14]. Core-set is a small subset of the data which is sufficient for computing an approximate solution of a pre-specified optimization problem on the whole data.

More specifically, in this work we focus on construction of _composable core-sets_: we present a summarization algorithm \(\) that processes each group \(P_{i}\)_independently_ and produces a small subset of it as its summary \(S_{i}=(P_{i}) P_{i}\), with the property that the fair diversity of the data is approximately preserved, i.e., \(_{k_{1},,k_{m}}(S) _{k_{1},,k_{m}}(P)\), where \(S\) is defined to be the union of all core-sets, i.e., \(S=_{i}S_{i}\). Composable core-sets are in particular useful for big data models of computation such as distributed and streaming settings as discussed in details in . For example, in a distributed setting where the dataset is partitioned over multiple machines based on groups, each machine can compute a core-set for its own dataset, and only send this small summary over to a single aggregator. The aggregator then processes the union of the summaries and outputs the solution. We provide further applications of the core-sets for processing timed datasets in the experiments sections of our paper.

### Prior Work

Table 1 shows a summary of prior and our results.

**Fair Diversity Maximization.** Moumoulidou _et al._ studied fair diversity maximization under Min-Pairwise Dist and gave an \(O(m)\) approximation algorithm for the problem. The bound was later improved  to \(m+1\). However, it is not known whether a linear dependence on \(m\) is necessary. In fact the problem admits an \(O(1)\) approximation in the unconstrained version [17; 33]. For Sum-Pairwise Dist the problem admits a \((1/2+)\) approximation [3; 7], which matches the performance of the best algorithm for the unconstrained diversity maximization. Finally, for Sum-NN Dist, Bhaskara _et al._ presented a randomized \(O(1)\) approximation algorithm based on solving an LP for both DM and FDM.

**Core-sets for Fair Diversity Maximization.** Core-set construction algorithms for the unconstrained DM has been shown in  with respect to all three diversity measures. For FDM, Moumoulidou _et al._ showed that running a greedy algorithm on each group independently provides a core-set with a constant approximation factor, with respect to the Min-Pairwise Dist. For the Sum-Pairwise Dist, Ceccarello _et al._ provides a core-set with the almost optimal \((1-)\) approximation factor. However, the size of the core-set depends on a parameter similar to the aspect ratio of the dataset. In particular, they show that for the case of doubling metrics, the core-set size could have an exponential dependence on the doubling dimension. Under Sum-NN Dist, there has been no prior work on core-sets for FDM.

### Our Results

**Theoretical results.** We show the following theoretical results in this paper.

* We show the first core-set construction algorithm for FDM under Sum-Pairwise Dist, with constant approximation factor whose size is independent of the number of points and the aspect ratio. In fact, we show a core-set of size only \(k_{i}^{2}\) for each group \(P_{i}\), that together achieve a constant factor approximation (see Theorem 2.1).
* We also show the first core-set construction algorithm for the FDM under the Sum-NN Dist notion of diversity. We provide a core-set of size \(O(k^{2})\) for each group \(P_{i}\) that together achieve an \(O(m k)\) approximation factor (see Theorem 4.1). We remark that although the approximation factor is probably not optimal, we see in the experiments section that the algorithm performs well on real data. To get the above core-set algorithm, we first show in Section 3, an approximation algorithm for FDM under the Sum-NN Dist. More precisely, we show an \(O(m^{2} k)\) approximation with polynomial running time (see Theorem 3.4) and \(O(m k)\) approximation with an exponential runtime in \(k\), (see Theorem 3.5). This algorithm might be of independent interest as the best previous algorithm by , despite having the ideal \(O(1)\) approximation, is randomized and based on solving an LP, whereas our algorithm is deterministic and based on a greedy approach (see Algorithm 1). In fact all of our algorithms are simple to implement and thus practical as we show next.
* Finally, in Section C of the Appendices we show how to apply our results to the setting where the partitioning is not necessarily done according to the colors and thus get a fully composable core-set for these notions.

**Experiments.** We perform experiments for all three diversity measures. Our first set of experiments uses FDM as a tool to account for recency in a summary computed on timed datasets. Here, we group a dataset of messages by their created time. The goal is to compute a diverse summary of the messages, where we additionally require to include less number of messages in the summary from the older batches of messages, and more from the recent batches. This is a real task in one of the largest commercial communication platforms. We get the following experiments and results.

* First, we show that running diversity maximization algorithms without the group constraint does not satisfy our requirement and reports about equally number of old messages and the new ones.
* Next, we compute the price of fairness (i.e., price of _balancedness_ in this context), that is we compute how much we lose diversity by imposing the grouping requirement. Our experiments on a Reddit dataset shows that the diversity only decreases by around 1% for sum-of-pairwise distances; few percent up to no more than 20% for sum-of-NN distances and around 50% for minimum-pairwise distances (due to its fragility) metrics.
* We use the core-set construction algorithms to summarize each group first, and then run our diversity maximization algorithms on the union of the core-sets. Our experiments show that using core-sets, the runtime of our algorithm improves on average by factor of \(100\), while only losing diversity by few percent. We further remark that using core-sets in this context has an additional benefit: it removes the need to recompute the summary on the whole data when new messages arrive: once we summarize a batch of old message, we no longer need to process the batch and only need to work with the core-set that is computed once.

We further show experiments that uses FDM as a tool for controlling the desired contribution of each genre in a movie recommendation system.

    & Min-Pairwise Dist & Sum-Pairwise Dist & Sum-NN Dist \\  FDM & \(O(m)\) & \(O(1)\) & \(O(1)\) \\ Core-set for FDM & \(O(1)\) & \(O(1)\)**Section 2** & \(O(m k)\)**Section 4** \\   

Table 1: The summary of prior and our results for FDM. Here \(k\) is the size of the solution and \(m\) is the number of groups. All the core-set results mentioned in the table have size \(poly(k)\).

Finally, we run experiments to compare our proposed core-set of Section 2 to the prior algorithm of  for core-set construction. We see that while the core-set produced by our algorithm is on average smaller by a factor of 200x, its performance is only worse by 1.3%.

### Preliminaries

Throughout this work, we assume that we are given a point set \(P=P_{1} P_{m}\) in a metric space \((,)\). Each point in \(P\) comes from one of the \(m\) groups in \([m]\), which we also refer to as _colors_. We use \(P_{i}\) to denote the points of color \(i\). We use \(n\) to denote \(|P|\) and \(n_{i}\) to denote \(|P_{i}|\). We denote the distance between two points \(p,q\) by \((p,q)\) and for a set of points \(S\), we use \((p,S)\) to denote \(_{q S}(p,q)\).

#### 1.3.1 Optimization problem

The optimization problem considered in this paper is Fair Diversity Maximization as defined below.

**Definition 1.1** (Fair Diversity Maximization (FDM)).: Given a colored point set \(P=_{i}P_{i}\), and \(k_{1},,k_{m}\), the goal is to pick subsets \(S_{i} P_{i}\), such that first \(|S_{i}|=k_{i}\), and second \((S)\) is maximized where \(S=_{i}S_{i}\). We will use \(_{k_{1},,k_{m}}(P)\) to denote the optimal diversity one can achieve this way, i.e.,

\[_{k_{1},,k_{m}}(P)=_{S_{1} P_{1},,S_{m}  P_{m}:|S_{i}|=k_{i}}(_{i m}S_{i})\]

We will use \(k\) to denote \(_{i}k_{i}\), and thus \(|S|=k\).

**Definition 1.2** (Diversity Measures).: For a set of points \(S\), we consider the following diversity measures in this work : i) Min-Pairwise Dist: \(_{p,q S}(p,q)\); ii) Sum-Pairwise Dist: \(_{p,q S}(p,q)\); iii) Sum-NN Dist: \(_{p S}_{q S\{p\}}(p,q)\).

#### 1.3.2 Summarization task

The goal in our paper is to provide an intermediate summarization (i.e., a core-set) algorithm such that the union of the core-sets contains a good solution relative to the whole data. Let us formally define this notion.

**Definition 1.3** (Core-set).: Given a point set \(P\), a subset \(T P\) is called an \(\)-approximate core-set with respect to an optimization function \(f\) if the optimal value of \(f\) over \(T\) is within a factor \(\) of the optimal value of \(f\) over \(P\).

It is desired that the size of the core-set is small. In this paper we focus on the optimization function \(f\) being the diversity maximization function. Further, we emphasize that our algorithm for constructing core-sets for \(P\), processes each group \(P_{i}\) independent of other groups. Therefore, it provides a form of composability property as defined below.

**Definition 1.4** (Color-Abiding Composable Core-set).: An algorithm \(\) is said to construct an \(\)-approximate color-abiding composable core-set, if it produces a subset \(T_{i}=(P_{i}) P_{i}\) independent of the points in other colors, s.t. \(_{k_{1},,k_{m}}(T)_{k_{1}, ,k_{m}}(P)\), where \(T=_{i}T_{i}\).

Again, we note that although it is desireable that the size of the core-set \(|T_{i}|\) is small, it does not necessarily need to be \(k_{i}\). Later on, as a post-processing, one can use any exact or approximation algorithm on the union of the computed core-sets, i.e., \(T\) to get a final solution. However the focus of this work is on the core-set computation algorithm \(\).

Throughout this work, for brevity we use _composable core-set_ to refer to Definition 1.4. However, we remark that the above definition of color-abiding composable core-set is only applicable when points are partitioned based on the colors, and differs from the standard notion of _composable core-sets_ defined in , where the partitioning is done arbitrarily. Later in Section C of the Appendices, we show how to employ our algorithms and get fully composable core-sets (that is not necessarily color-abiding) as defined in .

#### 1.3.3 The GMM algorithm

In this work we will use the greedy algorithm of  which we denote by GMM and is depicted in Algorithm 1. Let us also state three well-known properties of GMM:1. \(r_{i}\)'s are decreasing, i.e., for \(j>i\), we have \(r_{i} r_{j}\).
2. For any \(i<k\), let \(S_{i}=\{p_{1},,p_{i}\}\). Then for any \(p P S_{i}\), we have that \((p,S_{i}) r_{i+1}\).
3. Let \(T_{k}P\) be any subset of \(k\) points in \(P\). Then the minimum pairwise distance in \(T\) is at most \(2 r_{k}\).

**Input** a point set \(P\), and \(k\)

**Output** subset \(S=\{p_{1},,p_{k}\} P\), and radii \(r_{1},,r_{k}\).

```
1:\(S\) an arbitrary point from \(P\), and \(r_{1}\)
2:for\(i=2\)to\(k\)do
3:\(p_{i}\) the farthest point in \(P\) from \(S\), i.e., \(_{p P}(p,S)\)
4:\(S S p_{i}\)
5:\(r_{i}\) minimum pairwise distance in \(S\), i.e., \(_{p_{1},p_{2} S}(p_{1},p_{2})\)
6:endfor
7:return\(S\) ```

**Algorithm 1** The GMM Algorithm

## 2 Core-set for FDM under Sum-Pairwise Dist

In this section we show an algorithm for constructing a core-set for FDM with respect to the Sum-Pairwise Dist notion of diversity. Given a colored point set \(P=P_{1} P_{m}\), and \(k_{1},,k_{m}\), the goal is to come up with a core-set construction algorithm \(\) that independently summarizes each point set \(P_{i}\) and produces a small subset of it \(S_{i}=(P_{i}) P_{i}\), such that \(_{k_{1},,k_{m}}(S) _{k_{1},,k_{m}}(P)\), where again \(S=_{i}S_{i}\).

**Overview of the algorithm.** Suppose that we want to find a solution that maximizes FDM under Sum-Pairwise Dist notion of diversity. Consider the optimal solution \(\) and let \(_{i}\) be the set of points from color \(i\). Our algorithm for constructing a core-set proceeds by running GMM for \(k_{i}\) iterations on \(P_{i}\), to get a _seed_ of size \(k_{i}\). It is then a property of GMM that if each point \(p_{i}\) is mapped (i.e. moved) to the closest seed in \(P_{i}\), its distance does not change by more than \(r_{i}\) (as defined in GMM). Now the points in different colors can have different \(r_{i}\) and therefore the movements can be of different scales. For two points \(p,q\), if the scales of their movements w.r.t. their original distance are small, then the new distance is still large compare to before. Otherwise, if the scale of these movements are large, we show how to charge the loss in the diversity of this pair onto other pairs that have relatively small movements. If there are not enough such pairs, one can then show that the solution returned by the seeds themselves have had large diversity. Finally, in order to make the map injective (so that no two points are mapped to the same core-set point), for each of the seed points, the algorithm also stores in the core-set, the closest \(k_{i}\) points to the seed, and thus increasing the size of the core-set to \(k_{i}^{2}\).

There is one hard case in the above approach and that is when \(k_{i}=1\). In this case, the GMM algorithm can return any arbitrary point as the seed and that could provably be a bad core-set. To handle this case, we show that it is enough for the core-set algorithm to find at least a minimum of two seeds. Then we show that it is always possible to pick one of the seeds and get a reasonably good approximation. This modification is proved to work in Appendix B.

**Algorithm description.** For simplicity, let us assume that for all \(i\), we have \(k_{i} 2\). Otherwise, we show in Appendix B that a slight modification of the algorithm works (although the proof becomes more involved). The core-set construction algorithm is shown in Algorithm 2. The algorithm proceeds by running GMM for \(k_{i}\) iterations to compute a set of \(k_{i}\) centers. Then for each of these \(k_{i}\) centers such as \(p\), the algorithm stores at most \(k_{i}\) points from \(P_{i}\) whose closest center among all \(k_{i}\) centers is \(p\).

**Theorem 2.1**.: _Algorithm 2 produces a core-set with size \(O(k_{i}^{2})\) and a constant factor approximation: \(_{k_{1},,k_{m}}(S)_{k_{1},,k_{m}}(P)\) for a constant \(C\). (Proof in Appendix A.1)_```
0: a point set \(P_{i}\), together with parameters \(k_{i}\) and \(k\) (where \(k=k_{1}++k_{m}\))
0: a subset \(S_{i} P_{i}\)
1:\(S_{i}=\{p_{1},,p_{k_{i}}\}(P_{i},k_{i})\)
2:\(T\)
3:for\(p S_{i}\)do
4:for\(j=1\)to\(k_{i}\)do
5:\(T T\) any point \(p_{j} P_{i} T\) s.t. \(*{argmin}_{q S_{i}}*{dist}(p_{j},q)=p\).
6:endfor
7:endfor
8:\(S_{i} S_{i} T\)
9:return\(S_{i}\) ```

**Algorithm 2** Core-set Construction Algorithm for Sum-Pairwise

## 3 Approximation Algorithm for FDM under Sum-NN Dist

As mentioned in the results section, in order to obtain our core-set construction algorithm under Sum-NN Dist in Section 4, first in this section we show an approximation algorithm for FDM under the Sum-NN Dist notion of diversity.

**Overview of the algorithm.** Consider the optimal solution \(*{OPT}\) that maximizes FDM under Sum-NN Dist notion of diversity, and let \(*{OPT}_{i}\) be the set of points from color \(i\). Now each point \(p*{OPT}\) has a contribution towards the diversity, i.e., \(*{dist}(p,*{OPT}\{p\})\), and thus the contribution of each group \(*{OPT}_{i}\) towards the optimal solution is well-defined. Let us call the color with the maximum contribution as the _most significant_ color. Clearly if we find a solution \(*{SOL}\) whose value is at least as large as the contribution of the most significant color, then we are within a factor of \(m\) of the optimal solution.

Next, suppose that the most significant color is \(i\) and let \(d_{1} d_{k_{i}}\) be the contributions of the points in the optimal solution that are of color \(i\). It is not hard to prove (as we will show later) that \(_{j}(j d_{j})\) approximates \(_{j=1}^{k_{i}}d_{j}\) up to a factor of \( k\). So we will construct a solution whose value is at least \(j d_{j}\) and this will be within a factor of \((m k)\) of the optimal solution as promised.

More concretely, we will construct a solution such that it contains \(j\) points of color \(i\) where no other point (from any color) lies within a distance \(d_{j}\) of them (in reality, we find \(O(j)\) points such that no other point lies within a distance \(O(d_{j})\) of them). Of course, we do not know \(i\), \(j\), and \(d_{j}\)_a priori_. We can enumerate \(i\) and \(j\). Further, if we run the GMM algorithm on color \(i\) for \(j\) iterations, then the minimum pairwise distance between the retrieved points is an approximation to \(d_{j}\).

This itself is enough to pick \(O(j)\) points of color \(i\) that are \(O(d_{j})\) far from each other. Let us call these points the seeds. However, the remaining challenge is to pick the rest of the points (that is the \(k_{i}-O(j)\) points of color \(i\), as well as \(k_{}\) points of color \(\) for all \( i\)) away from these seeds.

This brings us to the following problem: given a colored point set \(P\) and a set of at most \(k\) balls \(B\), how to pick the largest subset of balls \(B^{} B\), such that enough points exist _outside_ of \(B^{}\). (Note that \(B\) is basically the set of balls with radius \(O(d_{j})\), that are centered at the \(j\) points returned by the first \(j\) iterations of GMM). This problem can be solved exactly by an exhaustive search and spending exponential in \(k\) time. One can also get a polynomial time \(O(m)\)-approximate solution for the problem by iteratively excluding half of the remaining balls from the solution, and at the same time satisfying the condition for half of the colors. Thus, after \(O( m)\) iterations, all color constraints are satisfied, and a \(1/m\) fraction of the balls remain. There are further technical details needed for the proof to go through which we need to take care of.

### Algorithm Description

Given a colored point set \(P=P_{1} P_{m}\), and \(k_{1},,k_{m}\), the goal is to find a solution \(*{SOL}=*{SOL}_{1}*{SOL} _{m}\) where \(*{SOL}_{i} P_{i}\), and \(|*{SOL}_{i}|=k_{i}\), and that \(*{div}(*{SOL})* {div}_{k_{1},,k_{m}}(P)\). The approximation algorithm is shown in Algorithm 3. The only unspecified part of the algorithm is in Line 8, where we need to specify how we find the subset \(B^{}\) given the set of balls \(B\). One can take different approaches resulting in different trade-offs described below.

#### 3.1.1 Finding an optimal subset of balls (Line 8 of Algorithm 3)

Given a colored point set \(P\) and a set of disjoint balls \(B\), the goal here is to find an (approximately) largest subset of balls \(B^{} B\), such that

* For a pre-specified color \(i[m]\), we have \(|P_{i} B^{}| k_{i}-|B^{}|\),
* For all other colors \([m]\{i\}\), we have \(|P_{} B^{}| k_{}\).

Moreover, we may assume that \(|B| t(j) k\), and that we are interested only in subsets \(B^{}\) of size upto \(|B^{}| k_{i}\).

**Approach 1.** If the algorithm is allowed to spend exponential time in \(k\), one can find the optimal subset \(B^{}\) in time \(k^{k_{i}} m\) as stated below.

**Observation 3.1** (Approach 1).: _There is an algorithm that finds the optimal largest subset \(B^{} B\) in time \(}{j} m j=k^{O(k_{i})}\). (Proof in Appendix A.2)._

**Approach 2.** One can get an approximate solution in polynomial time if the number of points of each color is sufficiently large, i.e, for each \([m]\), we have \(n_{}=|P_{}| 2 k_{}\). Note that this is a realistic assumption (also appeared in ), as we expect the number of data points to be much larger than the target size for the summary. Further, this assumption can be relaxed to \(n_{}\) being larger than a constant times \(k_{}\) similar to .

**Lemma 3.2**.: _There is an algorithm that finds an \(O(m)\)-approximate largest subset \(B^{} B\) in polynomial time, under the assumption that for each \([m]\), we have \(n_{} 2 k_{}\). (Proof in Appendix A.3)._

### Algorithm Analysis

Let us now describe the intuition behind Algorithm 3 along with defining a set of notations. Let \( P\) be the optimal solution and let \(_{i}= P_{i}\) be the points of color \(i\) in the optimal solution. Moreover let \(d_{1}^{i},,d_{k_{i}}^{i}\) be the values of the optimal solution, i.e., \(d_{j}^{i}=(p_{j}^{i},\{p_{j}^{i}\})\), where \(p_{j}^{i}\)'s are the points in \(_{i}\). Therefore, we have that \(_{k_{1},,k_{m}}(P)=()=_{i m }_{j k_{i}}d_{j}^{i}\).

Let \(i^{*} m\) be the color with the maximum contribution in the optimal solution, i.e., \(i^{*}=*{argmax}_{i[m]}_{j k_{i}}d_{j}^{i}\). Clearly, \(_{j k_{i}}d_{j}^{*}()/m\). Moreover, WLOG assume that for each \(i\), \(d_{j}^{i}\)'s are sorted, i.e., \(d_{1}^{i} d_{k_{i}}^{i}\). Now, let \(j^{*}(i)\) be defined as \(j^{*}(i)=*{argmax}_{j k_{i}}j d_{j}^{i}\), and define \(j^{*}=j^{*}(i^{*})\).

**Claim 3.3**.: _Let \(i[m]\) be any color. Then we have \(j^{*}(i) d_{j^{*}(i)}^{i}+1)}_{j  k_{i}}d_{j}^{i}\). (Proof in Appendix A.4)_

**Theorem 3.4**.: _Algorithm 3 using Approach 2, runs in polynomial time and produces a solution with an approximation factor of \(O(m^{2} k)\). (Proof in Appendix A.5)_

Similarly, running Algorithm 3 with Approach 1 would give us the following theorem.

**Theorem 3.5**.: _There exists an \(O(m k)\) approximation algorithm that runs in time \(O(k^{k} poly(m,k))\)._

## 4 Core-set for FDM under Sum-NN Dist

In this section we show how to get a composable core-set for FDM with respect to the Sum-NN Dist notion of diversity.

**Overview of the algorithm.** In order to construct a core-set for FDM under the Sum-NN Dist notion of diversity, we show that it is enough to construct a subset of the points such that the solution to the problem of finding the maximum number of balls such that enough points exist outside of them (discussed in Section 3) remains unchanged. We show that the GMM algorithm has this property. Intuitively, since GMM chooses the points that are pairwise-far from each other, not too many points are picked inside individual balls and thus the solution to the balls problem remains relatively unchanged. Therefore, if we run the algorithm of Section 3 on the core-sets produced by GMM, the solution is similar as if we had run it on the whole data.

**Algorithm description.** In this section, the goal is to give a summarization algorithm \(\) that processes each dataset \(P_{i}\) independently of other color datasets \(P_{j}\) and produces a summary \(S_{i}=(P_{i})\) such that \(_{k_{1},,k_{m}}(S) _{k_{1},,k_{m}}(P)\) where again \(S=_{i}S_{i}\). The core-set construction algorithm is simple and is shown in Algorithm 4. The algorithm proceeds by running the GMM algorithm \(k\) times, and each time for \(k+1\) iterations, and without replacement.

```
0: a point set \(P_{i}\), together with parameters \(k_{i}\) and \(k\) (where \(k=k_{1}++k_{m}\))
0: a subset \(S_{i} P_{i}\)
1:\(S_{i}\)
2:for\(j=1\)to\(k\)do
3:\(G_{i}=\{p_{1},,p_{k+1}\}(P_{i},k+1)\)
4:\(S_{i} S_{i} G_{i}\)
5:\(P_{i} P_{i} G_{i}\)
6:endfor
7:return\(S_{i}\) ```

**Algorithm 4** Core-set Construction Algorithm for Sum-NN

**Theorem 4.1**.: _Algorithm 4 returns a composable core-set of size \(O(k^{2})\) for each of the \(m\) colors, with an approximation factor of \(O(m k)\). (Proof in Appendix A.6)_

## 5 Experiments

To demonstrate the effectiveness of our algorithms, we run simulations on public and timed datasets.

### Tasks and Datasets

**FDM as a tool to account for recency in a summary.** Here the task is to produce a summary of a set of timed messages with a property that (i) the summary is a diverse subset of the messages, and (ii) there are more recent messages shown in the summary. We model this task by partitioning the messages into groups / colors based on their creation times and assign a desired budget to each group.

We use a _Reddit_ dataset  of text messages that are semantically embedded using BERT  into a metric space. We also utilize message creation time stamps as we aim to show diverse, yet timely and relevant messages to a user created across different time-window intervals within a certain period and thus assign a color to each message based on to which time-window interval (e.g., a week, thus having 4 colors in total) the message creation time belongs to.

**FDM as a tool for controlling the desired fairness contribution of each group.** We further use FDM to control the contribution of each genre (besides time) in a movie dataset. Apart from the _Reddit_ dataset, for this task we also use the _MovieLens_ dataset  where the movie titles are semantically embedded into a metric space. For this dataset, we have assigned a movie to a group represented by a color based on two criteria: (i) movie genre, and (ii) the time-window interval the movie review creation time belongs to. We include the result of this dataset in Appendix D.2.

More details on the datasets preparation are given in Appendix D.1 and additional result figures are provided in Appendix D.2. We have also made the code of the algorithms publicly available2.

### Summary of the experiments and results

We run the following experiments using all three diversity measures.

**Need for FDM.** We first show why we need to resort to FDM. In particular, we show that if we run DM on the data, the results are not as fairly balanced as we want, as depicted in Figure 1 and Figures 2 & 3 (Appendix D.2.1). In the case of time periods as colors dividing the data into \(m=4\) colors (i.e., quarters within a month), for \(k=20\) DM algorithms give: (i) a certain color that is clearly dominantly present in the outcome (for all diversity distances), and (ii) this color is not the one from the most recent messages (i.e., from the last color), see Figure 1 for the _Reddit_ dataset.

**Price of fairness (balancedness).** Our next set of experiments show that when using FDM the diversity is not decreased by a lot compared to DM. We use the _loss_ of diversity (% Div. loss) expressed as a relative change of diversity distances as a measure. This is shown in Table 2, and Tables 4 & 5 (Appendix D.2.2). For most of these experiments we use the state-of-the-art algorithms. However for Sum-NN Dist we use our proposed deterministic algorithm of Section 3 as an alternative to the LP-based randomized algorithm of . When we run FDM, we gain in fairness (or balancedness), but we lose in diversity over the DM, expressed in different metrics. Experiments from Table 2 show that we lose around 1% for Sum-Pairwise Dist; from few percent up to no more than 20% for Sum-NN Dist and around 50% for Min-Pairwise Dist (due to its fragility) for various color distributions per group, while achieving the desired per group distribution, in both cases where we enforce balancedness (i.e., uniform \(k_{i}\)), and when we enforce recency (i.e., \(k_{i}\) increasing). The results also show a similar trend if we use alternative message embeddings (Table 7 in Appendix D.2.4).

**Effectiveness of our core-sets.** We then show the effectiveness of the core-sets. We run our core-set construction algorithms (of Section 2 and Section 4) on each color independently, to get a smaller size dataset. We then run the FDM optimization once on the union of the core-sets and once on the whole data. We also measure the _loss_ of diversity and runtime improvement achieved by the use of core-sets. The results for all three measures are given in Table 3 and Table 6 (Appendix D.2.3). In

Figure 1: DM algorithm outcomes for all colors (\(m=4\)) with equidistant time period as fairness colors in the Reddit dataset (\(k=20\) items).

terms of diversity loss, the results show the diversity values are on-par or with marginal difference if we apply the FDM to the union of core-sets, compared to FDM applied to the full data. (In the case of Sum-NN Dist, there are cases where one is marginally better than the other, due to higher approximation factor.) Our experiments show that using core-sets, the runtime of our algorithm improves on average by factor of few \(10\) or \(100\), while only losing diversity by few percent. We remark that using core-sets in this context has an additional benefit: it removes the need to recompute the summary on the whole data when new messages arrive: once we summarize a batch of old message, we no longer need to process the batch and only need to work with the core-set that is computed once.

**Comparison to state-of-the-art core-set algorithm for Sum-Pairwise Dist.** Finally, we compare the core-set construction algorithm in  for Sum-Pairwise Dist to our algorithm of Section 2. The experiments in Appendix D.2.5 show that, the size of the core-set obtained by the algorithm in  for getting a factor 2 approximation is close to the data size in practice. In addition, the final diversity loss of our algorithm is negligible (less than 1% to few percent) even though it produces a much smaller core-set. Further, our core-set construction algorithm is also significantly faster.

   DM vs. FDM & Sum-Pairwise & Sum-NN & Min-Pairwise \\  \)\( k_{i}\) \% Div. loss} &  &  \\  \(\) & 8 & \(1.22\%\) & \(9.66\%\) & \(51.57\%\) \\ \(\) & 12 & \(0.98\%\) & \(14.27\%\) & \(49.99\%\) \\ \(\) & 16 & \(0.50\%\) & \(13.72\%\) & \(48.78\%\) \\ \(\) & 20 & \(0.47\%\) & \(18.96\%\) & \(48.05\%\) \\ \(\) & 24 & \(0.19\%\) & \(9.48\%\) & \(47.20\%\) \\  \(\) & 20 & \(0.42\%\) & \(15.40\%\) & \(48.05\%\) \\ \(\) & 30 & \(0.29\%\) & \(13.29\%\) & \(46.34\%\) \\ \(\) & 40 & \(0.25\%\) & \(1.98\%\) & \(45.52\%\) \\ \(\) & 50 & \(0.16\%\) & \(9.62\%\) & \(44.48\%\) \\ \(\) & 60 & \(0.12\%\) & \(3.98\%\) & \(43.60\%\) \\   

Table 2: The _loss_ of diversity (% Div. loss) between DM vs. FDM on the full data, expressed as a relative change, of the concerned Sum-Pairwise, Sum-NN and Min-Pairwise distances for uniform (upper part) or increasing (lower part) color values \(k_{i}\) for the Reddit dataset.

   FDM full data vs. core-sets &  & Sum-NN & Min-Pairwise \\  \)\( k_{i}\) \% Div. loss} &  &  &  &  \\  \(\) & 8 & \(1.35\%\) & \(196.24\) & \(2.22\%\) & \(1\,769.70\) & \(0.00\%\) & \(208.64\) \\ \(\) & 12 & \(0.67\%\) & \(333.13\) & \(0.29\%\) & \(88.55\) & \(0.00\%\) & \(152.48\) \\ \(\) & 16 & \(1.21\%\) & \(539.69\) & \(-1.59\%\) & \(474.26\) & \(0.00\%\) & \(122.29\) \\ \(\) & 20 & \(1.17\%\) & \(432.68\) & \(-0.44\%\) & \(294.23\) & \(0.00\%\) & \(89.08\) \\ \(\) & 24 & \(0.94\%\) & \(130.87\) & \(-3.03\%\) & \(183.28\) & \(0.00\%\) & \(63.69\) \\  \(\) & 20 & \(1.50\%\) & \(845.98\) & \(-1.80\%\) & \(285.68\) & \(0.00\%\) & \(91.44\) \\ \(\) & 30 & \(1.06\%\) & \(134.76\) & \(2.27\%\) & \(110.36\) & \(0.00\%\) & \(53.05\) \\ \(\) & 40 & \(1.02\%\) & \(182.06\) & \(-0.88\%\) & \(57.88\) & \(0.00\%\) & \(36.51\) \\ \(\) & 50 & \(1.16\%\) & \(194.36\) & \(0.71\%\) & \(34.90\) & \(0.00\%\) & \(26.97\) \\ \(\) & 60 & \(1.27\%\) & \(172.25\) & \(-0.49\%\) & \(23.71\) & \(0.00\%\) & \(20.53\) \\   

Table 3: The _loss_ of diversity (% Div. loss) expressed as a relative change of diversity distances, and the running time _gains_ (\(\) times faster) of the FDM when applied to the union of core-sets compared to FDM applied to the full data for uniform or increasing color values \(k_{i}\) for the Reddit dataset. We remark that the results for Min-Pairwise have % Div. loss being 0% as for this data the two points connected with the minimum distance always end up in the union of the core-sets.