# Jointly Modeling Inter- & Intra-Modality Dependencies for Multi-modal Learning

Divyam Madaan\({}^{1}\)1  Taro Makino\({}^{2}\)  Sumit Chopra\({}^{1,2,3}\)  Kyunghyun Cho\({}^{1,2,4,5}\)

\({}^{1}\)Courant Institute of Mathematical Sciences, New York University

\({}^{2}\)Center for Data Science, New York University

\({}^{3}\)Grossman School of Medicine, New York University

\({}^{4}\)Prescient Design, Genentech

\({}^{5}\)CIFAR LMB

###### Abstract

Supervised multi-modal learning involves mapping multiple modalities to a target label. Previous studies in this field have concentrated on capturing in isolation either the inter-modality dependencies (the relationships between different modalities and the label) or the intra-modality dependencies (the relationships within a single modality and the label). We argue that these conventional approaches that rely solely on either inter- or intra-modality dependencies may not be optimal in general. We view the multi-modal learning problem from the lens of generative models where we consider the target as a source of multiple modalities and the interaction between them. Towards that end, we propose inter- & intra-modality modeling (I2M2) framework, which captures and integrates both the inter- and intra-modality dependencies, leading to more accurate predictions. We evaluate our approach using real-world healthcare and vision-and-language datasets with state-of-the-art models, demonstrating superior performance over traditional methods focusing only on one type of modality dependency. The code is available at https://github.com/divyam3897/I2M2.

## 1 Introduction

Supervised multi-modal learning involves mapping input data to a target label, where the data is derived from multiple modalities and information about the boundaries between different modalities is available. This problem has garnered interest in numerous applications, such as autonomous driving , healthcare , robotics , to name a few. We encourage readers to refer to the latest survey papers  for recent developments in this field.

Despite multi-modal learning being a key paradigm in machine learning, its effectiveness varies across different applications. In some cases, a multi-modal learner outperforms a uni-modal learner , while in others, it may not be as effective as individual uni-modal learners  or a simple combination of uni-modal learners . These differing results beg for a principled framework that can explain such discrepancies in multi-modal model performance and provide a general recipe for designing models that can leverage multi-modal data more efficiently and without such shortcomings.

In this work, we aim to uncover the underlying factors behind such discrepancies and introduce a more principled approach to multi-modal learning to resolve them. We view the supervised multi-modal learning problem from a probabilistic lens and define the underlying data-generating process. More formally, the proposed data-generating process is shown in Figure 0(a). Without lossof generality, we consider the output \(\) generating data \(\) and \(^{}\) for the two modalities. We define the statistical dependency between the set of modalities and the label using a selection variable \(\{0,1\}\). Formally,

\[p(,,^{},=1)=p()p( )p(^{})p(=1 ,^{},).\]

This selection variable is always set to one because it is the mechanism that induces the dependencies between the modalities and the label. The strength of this selection mechanism varies across datasets. When the selection effect is strong, inter-modality dependencies (dependencies between the modalities and label) become more significant. Conversely, when the selection effect is weak, intra-modality dependencies (dependencies between the individual modalities and label) are more crucial.

At a high-level, our framework assumes that the output label generates the data associated with individual modalities. In addition, it also defines the relationship among different modalities and the label with the selection mechanism. The extent to which the output is dependent on the data from individual modalities and on the cross-modality relationships, differs from use-case to use-case. Given the lack of prior knowledge about the strength of these dependencies on the final task, a multi-modal system must model both the inter- and intra-modality dependencies. We achieve this by building a classifier for each modality to capture the intra-modality dependencies and another classifier to capture the dependencies between the output label and the inter-modality interactions. These classifiers are then combined by constructing a product or log-ensemble of their outputs. We name this approach inter- & intra-modality modeling (I2M2), stemming directly from the above multi-modal generative model.

The proposed framework can be used to categorize all the previous works on multi-modal learning into two categories. The first category correspond to the _inter-modality modeling_ methods . This includes methods that predominantly focus on capturing the dependencies between the modalities to predict the target. From our graphical model's perspective these methods are based on the assumption that there are no direct edges from \(\) to \(,^{}\) (see Figure 0(b)). Although these methods can technically capture both inter- and intra-modality dependencies, they often fail to do so effectively . This ineffectiveness stems from their reliance on incomplete underlying assumptions about the generative model for multi-modal learning. The second category correspond to the _intra-modality modeling_ methods . This include approaches that consider the interactions between different modalities that occur only through the label (see Figure 0(c)). These approaches do not capture the relationship between the modalities for prediction, which contradicts the objective of multi-modal learning. Inter-modality approaches excel when modalities share significant information to predict the label, while intra-modality approaches are effective when cross-modality information is sparse or absent. Often times, such information is not provided to us when building the multi-modal models.

The proposed I2M2 framework addresses this shortcoming by not requiring prior knowledge of the strength of these dependencies. It explicitly models both inter- and intra-modality dependencies, making it adaptable and effective across various conditions. We validate our claims on multiple datasets, demonstrating the benefits of I2M2 over both inter- and intra-modality methods. We apply

Figure 1: **Data generating process for various scenarios** with two modalities \(,^{}\) and output \(\). In the context of multi-modal learning \(\)), the label modulates the individual modalities (referred to as intra-modality dependencies) and the interaction between them (referred to as inter-modality dependency) through the selection variable \(\). In contrast, conventional approaches assume the graphical model in **b)** or **c)**. In the graphical model shown in **b)**, the dependency between each individual modality and the label only modulates through the selection variable \(\). On the other hand, the graph in **c)** assumes that the dependency between two modalities is independent of the label.

our method to multiple tasks in healthcare, including automatic diagnosis using knee MRI exams  and for mortality and ICD-9 code prediction in the MIMIC-III dataset . We also demonstrate the benefits of I2M2 in multiple vision-and-language tasks such as VQA [65; 3] and NLVR2 . Our evaluation shows the varying strength of dependencies across datasets; intra-modality dependencies are more beneficial for fastMRI dataset, while inter-modality dependencies are more relevant for NLVR2 dataset. Both dependencies are pertinent for the AV-MNIST, MIMIC-III and VQA datasets. I2M2 excels across the board, ensuring robust performance regardless of which dependencies are most significant.

## 2 What is Multi-modal Learning?

Multi-modal learning refers to the problem setup where the input is expressed as a set of observations from different modalities. Unlike conventional learning involving data set from a single modality, multi-modal learning can and should exploit the information from all the provided modalities for the purpose of prediction. In this work, we are interested in supervised multi-modal learning, where the goal is to map the inputs from multiple modalities to the targets.

We begin with the dataset \(~{}=~{}\{(_{i},^{}_{i},_{i}) \}_{i=1}^{n}\) with \(n\) examples. Without loss of generality, \(_{i}\) is the label and \(_{i}^{d}\) and \(^{}_{i}^{}^{d}\) represent data from the two modalities. To define multi-modal learning more formally, we define a _multi-modal data generating process_ in which label \(\) gives rise to both the modalities \(\) and \(^{}\) and the interaction between them (see Figure 0(a)). The variable \(\) represents a selection variable that captures the statistical dependencies across the modalities given the label. This selection variable is a binary random variable that is conditioned on all the input modalities and the target. As mentioned earlier, this variable is always present (i.e., \(=1\)), but its influence varies across datasets. The joint probability in this case can be written as:

\[p(,,^{},=1)=p()p( ~{}|~{})p(^{}~{}|~{})p(=1~{}|~{},^{},).\] (1)

While this might appear similar to the use of selection variables in modeling selection bias [26; 10; 5], in our context, selection does not refer to selecting examples, but to the mechanism that induces the dependencies between the modalities and the label. Particularly, we use this mechanism to break the conditional independence among the input modalities given the label, which is often referred to as the 'explaining away' phenomenon. The challenge is that, prior to analysis, the relative importance of inter- and intra-modality dependencies for classification is often unknown. Therefore, a multi-modal classifier needs to account for both inter- and intra-modality dependencies.

Our data generating process is commonly observed in many real-world scenarios. As a concrete example, consider VQA , a task that involves answering an open-ended question using information from an associated image. Each individual modality - either the image or question - can independently provide clues towards the correct answer [21; 8; 11; 65], yet these hints alone are often not sufficient to predict the answer. It is only by examining both modalities together that we can accurately infer the correct answer. This combination is captured in our generative model by the selection variable \(\). Thus, this task requires building separate models to capture the image and text specific information conditioned on the answer (intra-modality dependencies) and the dependency between the image, text modality given the answer (inter-modality dependency) to make the best prediction. This underlines the importance of a modeling approach that not only considers the interaction between the modalities but also uses each modality independently to predict the correct label.

## 3 Three Ways to Capture Modality Dependencies

Traditional approaches in multi-modal learning model the interaction between different modalities in order to predict the target, primarily by building novel architectures [54; 33; 64; 46; 2; 19; 75; 45; 39; 66]. Although these approaches occasionally outperform uni-modal models, there are cases where they fall short and are less effective than either the uni-modal learners [21; 8; 78] or their ensemble  counterparts. Furthermore, to the best of our knowledge, no prior work exists that sheds light on the reasons behind this discrepancy in model performance and provides a solution for the same. In this work we move our focus away from the question of model parameterization given a multi-modal data. Instead, we focus towards uncovering the probabilistic assumptions required to study multi-modal learning.

We describe our I2M2 approach that incorporates the modality grouping information by considering models trained on individual modalities, while simultaneously capturing the interaction between the modalities. Next, we group existing studies into inter-modality modeling [4; 6; 19; 27; 45; 43; 2; 32; 75; 78; 56], which only uses the interaction between different modalities to predict the correct label, and intra-modality modeling [54; 33; 64; 23; 39; 66], which assumes the conditional independence between the modalities given the target.

### Inter- & Intra-Modality Modeling (I2M2)

Starting from Equation1, we can write the conditional probability over labels as the product of four terms as follows:

\[p(,^{},=1)= )p()p(^{ })p(=1,, ^{})}{_{^{}}p(^{ })p(,^{}^{} )p(=1^{},,^{ })},\] (2)

where \(p(),p(^{})\) and \(p(=1,,^{})\) are functions that map inputs \((,)\), \((^{},)\) and \((,^{},)\) to a positive scalar. For clarity, we will use \(q_{}()\), \(q_{^{}}(^{})\), and \(q_{,^{}}(,^{ })\) in lieu of \(p(),p(^{})\) and \(p(=1,,^{})\) and rewrite the above equation as follows:

\[p(,^{},=1) p()\ }()q_{^{ }}(^{})}_{},^{}}(,^{ })}_{},\] (3)

where \(q_{}()\) captures the conditional probability of the target given \(\), \(q_{^{}}(^{})\) the conditional probability of the target given the other modality \(^{}\), \(q_{,^{}}(,^{ })\) the conditional probability of the target given both the modalities. We omit \(\) from the right hand side for brevity.

This suggests that we should build a separate predictive model for each modality and a model that takes as input the concatenated pair of both modalities. We combine these modality-specific and multi-modal classifiers by building a product of experts (or an additive ensemble in the log-probability space). In this approach, we separately capture the intra-modality dependency within each modality and inter-modality interaction across the modality boundary, to predict the target. This modeling paradigm captures the influence of both individual modalities on the label, as well as the combined impact of both modalities.

I2M2 closely aligns with the mutual information framework proposed in the multi-view framework [71; 44] and captures the three different types of mutual information. We motivate and explain this perspective for multi-modal learning from the principles of probabilsitic graphical models by explaining the underlying graphical models that give rise to such a factorization of mutual information.

### Inter-Modality Modeling

Current research on multi-modal learning mainly focuses on how to parameterize the conditional distribution over the label given multiple modalities. It has seen the development of various architectural strategies, which can be categorized into two main types: The first type is the _early fusion_[4; 6; 19; 27; 45; 43] which combines the input modalities at an initial stage and processes them using a single, shared model. The second type is the _intermediate fusion_[2; 75; 32; 78; 56] which uses distinct models for each modality, linked together by fusion modules at different layers. We collectively call these modeling paradigms as inter-modality modeling, as it considers all the modalities together and does not explicitly use the information about modality boundaries, beyond parameterization. Thus, the predictive probability over \(\) can be written as:

\[p(,^{},=1) p()q_{,^{}}(,^{ }).\] (4)

This is derived from Equation3, where \(q_{}()\) and \(q_{^{}}(^{})\) are set to constant functions. This corresponds to removing the direct edges \(\) and \(^{}\) in the graphical model depicted in Figure1a, resulting in the graphical model shown in Figure1b. Consider the example of NLVR [69; 70], which involves determining whether a sentence accurately describes a pair of images or not. This task requires a model to possess joint understanding of visual and textual information to determine the accuracy of the statement. The dataset was curated to explicitly avoid visual or language bias . To achieve this, each sentence was associated with multiple examples with conflicting labels. This was accomplished by showing workers a set of visually similar yet distinct images and asking them to write sentences that are true for some of the images but not for others.

Prior studies for inter-modality modeling [4; 6; 19; 27; 45; 43; 8; 78; 2; 32; 75; 78; 56; 57; 15; 83] mostly focus on emphasizing inter-modality dependencies and identifying an issue with under utilizing these dependencies. While these methods are capable of capturing both inter and intra-modality dependencies, they often fail to do so effectively, especially when inter-modality information is absent or sparse . Additionally, many of these approaches are developed for specific applications, such as person re-identification , multimedia recommendation  and sentiment analysis . Unlike these specialized approaches, I2M2 is agnostic to the types of modalities and captures both the inter and intra-modality dependencies explicitly.

### Intra-Modality Modeling

Intra-modality modeling [54; 33; 64; 23; 39; 66] processes each input modality through separate encoders. It then uses a product of experts model consisting of these uni-modal predictors, where the correlation between the modalities does not predict the target. Consider an example of the tiger detection task, where \(y=1\) indicates the presence of a tiger. The first modality is shape information, and the second modality is texture information. When \(y=0\), the first modality has a random shape, and the second modality a random texture. On the contrary, when the label is "tiger", i.e., \(y=1\), the first modality (shape) has a tiger-like shape, regardless of the second modality (texture) and the same applies to the texture modality. This implies that all the statistical dependencies between modalities manifest themselves via the label (\(y\)).

We refer to the predictive model for this generative process as intra-modality modeling, where we assume conditional independence among the modalities given the label \(\). In this case the predictive distribution can be written as:

\[p(,^{},=1)  p()q_{}( )q_{^{}}(^{})\] (5)

Similar to Equation (4), this model stems from Equation (3), where \(q_{,^{}}(,^{})\) is treated as a constant function. This corresponds to eliminating the directed edge \(\) in the graphical model in Figure 0(a), leading to the graphical model shown in Figure 0(c). We build a classifier for each modality and combine them by building a product (or a log-ensemble) of these classifiers, ignoring higher-order interactions among the modalities.

## 4 Experiments

To differentiate various methods for multi-modal learning, we use audio-vision MNIST (AV-MNIST) dataset , healthcare datasets, such as fastMRI  and MIMIC-III , vision-and-language datasets, including VQA-VS  (a recently introduced version of VQA), and NLVR2  (the latest version of NLVR). We consider state-of-the-art models for all the datasets. We defer the details of datasets to Appendix A and model hyper-parameters to Appendix B.

### Av-Mnist

Experimental setup.AV-MNIST combines audio and visual modalities for MNIST digit (0-9) recognition task. We use LeNet  with the top-performing methods from recent studies. Specifically, we employ late fusion (LF), which concatenates the uni-modal feature representations followed by a classification head, and low-rank multimodal fusion (LRTF)  from the recent multi-modal benchmark  for this dataset.

Results.Table 1 shows that inter-modality approaches outperform both intra-modality and single-modality approaches, underscoring the significance of inter-modal interactions as anticipated by the data construction. I2M2 improves the performance by \(1\%\), supporting our claim that both intra- and inter-modality interactions are crucial for this task. Moreover, as expected from the dataset construction, the model trained with the visual modality more effectively predicts the label than the audio component. The performance is enhanced by \(4\%\) with intra-modality modeling compared to using only the visual modality. I2M2 eliminates the need to pre-determine which specific dependencies should be modeled, offering a more flexible and effective modeling approach.

    & Accuracy \\  Image-only & 64.73 (\(\) 0.13) \\ Audio-only & 39.59 (\(\) 1.44) \\  Intra-modality & 68.63 (\(\) 0.48) \\    & 71.68 (\(\) 0.50) \\  & 71.49 (\(\) 0.48) \\   \\ 
**LF** & **72.24 (\(\) 0.14)** \\
**LRTF** & **72.28 (\(\) 0.17)** \\   

Table 1: **AV-MNIST accuracy comparison** between various methods.. Best results are highlighted in **bold**.

### FastMRI

Experimental setup.This dataset [81; 82] consists of MR scans that include DICOM images and the corresponding raw measurements in the frequency domain (also known as \(k\)-space in the MR community), along with slice-level labels. We dissect the complex \(k\)-space data into magnitude and phase components, treating them as two distinct modalities for identifying the most significant knee pathologies: 1) anterior cruciate ligament (ACL), 2) meniscus tear, 3) cartilage and 4) others grouping all the other pathologies. We use PreactResNet-18  following Madaan et al. , the only study targeting diagnosis for this task.

Results.Consistent with our previous experiments, I2M2 outperforms the unimodal magnitude and phase models, inter-modality and intra-modality methods, as shown in Figure 2. In this experiment, unlike the AV-MNIST dataset, inter-modality modeling degrades the performance compared to intra-modality modeling. Despite the opposite trend, I2M2 generally worked better than either of them across all pathologies. The superior performance of I2M2 highlights its ability to perform well even when one type of modality dependency is missing by effectively capturing the other.

Even when compared with root-sum-of-squares (RSS) - _de facto_ standard way of representing MR images in deep learning that benefits from the enhanced signal-to-noise ratio (SNR) offered by multi-coil data, I2M2 achieves performance superior to the model trained with RSS images across all pathologies. This finding is notable, given the higher SNR of RSS images due to the use of multi-coil data compared to our alternative representations synthesized by simulated single-coil output, resulting in lower SNR. This is useful because, until this point, we could not benefit from multiple modalities in complex MR images relative to RSS.

Lastly, the acquisition of MRI data is often a combination of multiple signals, which introduces background noise. Such background noise is easily impacted by the acquisition environments. It is thus important to evaluate the generalization of our methodology while varying the SNR ratios during inference. We show the effectiveness of I2M2 over inter-modality modeling for varying SNR levels by increasing levels of Rician noise [61; 22] in Appendix C.

   Intra & Inter & Mortality & ICD-9 \\  S & T & & \(140-239\) & \(460-519\) \\  ✓ & ✗ & ✗ & \(76.32(0.08)\) & \(91.42(0.09)\) & \(55.99(0.37)\) \\ ✗ & ✓ & ✗ & \(77.04(0.16)\) & \(83.36(0.25)\) & \(68.15(0.41)\) \\  ✓ & ✓ & ✗ & \(77.65(0.33)\) & \(91.42(0.04)\) & \(67.98(0.42)\) \\ ✗ & ✗ & ✓ & \(77.86(0.27)\) & \(91.54(0.12)\) & \(68.59(0.46)\) \\   

Table 2: **Accuracy on MIMIC-III for mortality and ICD code prediction.** I2M2 obtains higher performance in comparison to static (S) and time-series (T) uni-modal models, intra-modality modeling and inter-modality modeling.

   Intra & Inter & NLVR2 & VQA-VS \\  I & T & & & IID & OOD \\  ✓ & ✗ & ✗ & \(52.05(0.91)\) & \(25.92(0.03)\) & \(7.37(0.15)\) \\ ✗ & ✓ & ✗ & \(52.97(0.71)\) & \(43.78(0.07)\) & \(22.03(0.35)\) \\  ✓ & ✓ & ✗ & \(54.31(0.37)\) & \(57.59(0.09)\) & \(40.15(0.25)\) \\ ✗ & ✗ & ✓ & \(85.29(0.101)\) & \(68.04(0.03)\) & \(46.04(0.45)\) \\  ✗ & ✓ & ✓ & \(85.36(0.47)\) & \(68.63(0.46)\) & \(48.74(0.27)\) \\   

Table 3: **Accuracy and VQA score for NLVR2 and VQA-VS respectively.** I2M2 obtains comparable performance to inter modality method for NLVR2, while outperforming it for VQA-VS. I and T denote the image and text modalities respectively. Best results are highlighted in **bold**.

Figure 2: **Results on fastMRI dataset.** We compare **root-sum-of-squares, magnitude** and phase unimodal models, intra-modality modeling, inter-modality modeling, and I2M2 models (bars are in the same order). I2M2 obtains comparable performance to the intra-modality model by ignoring the inter-modality dependency, because comparatively to intra-modality, it contributes less to predicting the label.

### Mimic-Iii

Experimental setup.The MIMIC-III dataset  encompasses ten years of intensive care unit (ICU) patient data from Beth Israel Deaconess Medical Center. We divide the dataset into two modalities [60; 41]: 1) the time-series modality, which includes hourly medical measurements over 24 hours, and 2) the static modality, capturing a patient's medical information. We consider three tasks, namely a) mortality prediction of a patient within one day, two days, three days, one week, one year and beyond, and b) two binary classification tasks for ICD-9 codes, one to assess if a patient falls under group 1 (codes 140-239; neoplasms) and another for group 7 (codes 460-519; diseases of the respiratory system). We adopt the best-performing models from the recent multi-modal learning benchmarks [60; 41] for this dataset (see Appendix B for more details).

Results.Table 2 shows that I2M2 enhances performance across all tasks when compared to methods that focus solely on either inter-modality or intra-modality dependencies. Both modalities are predictive of mortality, but their effectiveness varies across different groups in predicting ICD-9 codes. Specifically, for ICD-9 codes 140-239, which are associated with neoplasms, the static modality proved more effective. This is likely because it includes factors such as the patient's advanced age and the presence of chronic diseases, which are known to increase the risk of neoplasms. For ICD-9 codes 460-519, the model trained with time-series modality showed better performance, as hourly measurements are useful to identify minor changes in respiratory functions, potentially signaling the onset or exacerbation of the disease.

For both mortality and ICD-9 prediction, we found that the intra-modality model obtains comparable performance to the inter-modality model, highlighting that both the individual modalities and their interaction are predictive of the target for these tasks. As expected from our generative model, which formed the basis for our I2M2, the importance of these dependencies is contingent on the specific task label, and our method uses them effectively.

### Natural Language Visual Reasoning

Experimental setup.NLVR2  represents a binary classification task in which the goal is to determine whether the text description correctly describes a pair of two images. The model takes as input two images and a text statement describing those images and predicts whether the text describes both images correctly. As discussed in Section 3.2, this dataset was constructed to minimize unimodal bias. For this dataset, we use the state-of-the-art FIBER model [13; 53], which takes a pair of images and associated text as input and produces a binary label as its output. We fine-tune the full model consisting of Swin Transformer  for the image backbone, and RoBERTa  for the text backbone and an MLP classifier on top of the encoder for five seeds.

Results.Table 3 shows that inter-modality modeling and I2M2 yield similar performance, which is substantially higher than the unimodal models and intra-modality model. This is because each of the image-only and the text-only models attains a chance-level accuracy for this dataset. This shows that neither the isolated text nor the image alone can make meaningful predictions in this problem. This can be attributed to the careful construction of the dataset, which eliminates language and visual biases and underscores the importance of inter-modal interaction within this dataset . It demonstrates the ability of I2M2 to effectively disregard the uninformative intra-modality dependencies when predicting the target for this dataset. This aligns with our observations from fastMRI, where I2M2 disregarded the inter-modality dependencies.

### Visual Question Answering

Experimental setup.The objective of VQA is to answer questions about images, as detailed in Section 2. The labels comprise \(3,129\) of the most common answers in the training and validation sets. The evaluation comprises IID and out-of-distribution (OOD) test sets released by VQA-VS . We report the average OOD accuracy across nine OOD test sets. Additional details on the OOD test sets are provided in Appendix A, along with a detailed performance breakdown in Appendix C. Similar to NLVR2, we use the state-of-the-art FIBER model  for training on VQA-VS dataset. We use the pre-trained model weights - Swin-Base  and RoBERTa-Base  for vision and text encoders and train an MLP classifier on top of the pre-trained encoders following Makino et al. . We use the VQA score metric across five random seeds to compare model performance.

Results.Table 3 shows the IID accuracy for the VQA-VS dataset. It is evident that I2M2 surpasses inter-modality modeling and unimodal models in performance, emphasizing the importance of using both inter-modality and intra-modality dependencies. This improved performance is achieved by leveraging the dependencies between the modalities and individual modalities to predict the target. While the image-only model does not effectively predict the final task, the text-only model obtains \(17.86\%\) higher performance. This improvement can be primarily attributed to the language bias present within this dataset, as highlighted in previous works . While all models suffer a drop in performance for the OOD test-sets, I2M2 achieves a relative improvement of \(5.86\%\) and \(19.35\%\) in comparison to inter-modality modeling and intra-modality modeling respectively. This highlights that addressing distribution shifts involves not only improving the individual experts but the robustness can also be enhanced through redundancy.

### Further Analysis: Beyond Aggregate Metrics

While aggregate metrics provide a broad overview, they often miss crucial details. We show that I2M2 surpasses other ensemble and wider models, even with a fixed parameter budget. Moreover, I2M2 avoids spurious dependencies between modalities and labels, a common issue in other models.

Comparison between models with identical parameter counts.To determine whether the performance improvement by I2M2 is due to the additional parameters or the specific manner in which the experts are combined, we compare it with various mixture of expert models. Particularly, we compare with an ensemble of three magnitude models, an ensemble of three phase models, an ensemble of magnitude and phase models and an ensemble of magnitude, phase and inter-modality model in Figure 3. We observe that across all pathologies, the ensemble outperforms individual unimodal models but I2M2 obtains better performance than all the ensemble models. This demonstrates that our I2M2 is more effective, even when constrained with a fixed parameter budget. We further contrast I2M2 using PreactResNet-18 with unimodal and inter-modality models using WideResNet-20-3 in Figure 4. Across various pathologies, I2M2 outperforms in terms of AUROC, reinforcing our view that the effectiveness stems more from our training approach and the integration of individual experts rather than merely from expanding the number of model parameters.

Analysis of common mistakes.The training, validation, and test sets of VQA-VS contain a range of inter- and intra-modality spurious dependencies that are conditioned on specific labels. For example, the label "tennis" often correlates with words like "what", "sport", and "is" in the question, whereas the label "kite" is linked with a "kite" in the image. Similarly, the words "how" and "many" in

Figure 4: **AUROC comparison with WideResNet models.** We compare magnitude-only, phase-only models, inter-modality model trained with architecture WideResNet-20-3 to our I2M2 trained with PreactResNet-18 across various knee pathologies in the fastMRI dataset (bars are in the same order). I2M2 obtains higher performance across all pathologies in comparison to the wider models.

Figure 3: **AUROC performance for models with identical parameter count.** We compare an ensemble of three magnitude-only, an ensemble of three **phase-only** models, an ensemble of magnitude and phase, an ensemble of magnitude, phase, and inter-modality model with our I2M2. Despite having the same number of parameters, our proposed model consistently outperforms the ensemble models.

[MISSING_PAGE_FAIL:9]

Conclusion

In this paper, we proposed inter- & intra-modality modeling (I2M2) for multi-modal learning, capturing both inter-modality and intra-modality dependencies. Applied to real-world datasets in healthcare and vision-language domains, I2M2 consistently outperformed conventional methods that rely solely on inter- or intra-modality dependencies and excelled in both in-distribution and out-of-distribution scenarios. Its versatility and modality-agnostic nature make I2M2 a valuable tool, providing a solid foundation for future research and applications in multi-modal learning.

## 7 Societal Impact

Contents posted on social media and other internet platforms are becoming increasingly more complex. They are no longer in just one modality, such as text-only, image-only or audio-only. These online contents are often multi-modal, requiring one to consider multiple modalities simultaneously to grasp the true meaning of these contents. As was pointed out earlier by Kiela et al.  in their hateful meme challenge, for instance, many harmful contents online require a holistic understanding of the text and the associated images, and text-only or image-only interpretation may miss the harmful nature of those contents. Advances in multi-modal learning, such as this work, will help us build a more effective content understanding system that can enable us to build a better automated filtering system to keep online platforms less harmful. We also acknowledge, however, that a better capability of multi-modal understanding can be used to build more advanced recommendation systems that may negatively contribute to the consumption of media and news by users.