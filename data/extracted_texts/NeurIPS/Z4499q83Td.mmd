# Variational Bayes Gaussian Splatting

Toon Van de Maele\({}^{1}\)

toon.vandemaele@verses.ai ozan.catal@verses.ai alexander Tschantz\({}^{1,2}\)

&Christopher L. Buckley\({}^{1,2}\)

christopher.buckley@verses.ai tim.verbelen@verses.ai

&Tim Verbelen\({}^{1}\)

\({}^{1}\) VERSES

Los Angeles, CA, USA \({}^{2}\) School of Engineering and Informatics

University of Sussex

Brighton, UK

###### Abstract

3D Gaussian Splatting has shown that mixture models can be used to represent high-dimensional data, such as 3D scene representations. Currently, the most prevalent method for optimizing these models is by backpropagating gradients of an image reconstruction loss through a differentiable rendering pipeline. These methods are susceptible to catastrophic forgetting in many real-world situations, where data is continually gathered through sensory observations. This paper proposes Variational Bayes Gaussian Splatting (VBGS), where we cast learning as variational inference over model parameters. Through conjugacy of the multivariate Gaussian, we find a closed-form update rule for the variational posterior, which allows us to continually apply updates from partial data, using only a single update step for each observation.

## 1 Introduction

Representing 3D scene information is a long-standing challenge for robotics and computer vision (Ozyesil et al., 2017). A recent breakthrough in this domain relies on representing the scene as a radiance field, i.e. using neural radiance fields (Mildenhall et al., 2020). Recently, Gaussian Splatting (Kerbl et al., 2023) has shown that mixture models are a competent model for this task, evidenced by a plethora of subsequent research (see Chen and Wang (2024) for a survey). This method relies on the fact that Gaussians can represent physical space as a collection of ellipsoids. The most prevalent method for optimizing the model parameters is by backpropagating gradients through a differentiable renderer, with respect to the parameters of this mixture model.

In many real-world scenarios (e.g. autonomous navigation), data is continually streamed and must be processed sequentially. Since backpropagation-based methods are susceptible to catastrophic forgetting (French, 1999), a replay buffer is typically used to retain and retrain on older data (Matsuki et al., 2024). Instead, in this paper, we cast fitting a Gaussian splat as variational inference over the parameters of a generative mixture model, for which a closed-form update rule exists (Blei et al., 2017). Continual learning is naturally enabled as these updates are a sum and can be applied iteratively. Notably, unlike gradient-based optimization, this does not require any replay buffer.

We benchmark our approach on two datasets: (i) 2D images (Tiny ImageNet (Le and Yang, 2015)), and (ii) 3D point clouds (Blender dataset (Mildenhall et al., 2020)). We evaluate these on reconstruction performance in both a setting where all data is readily available and in a continual learning setting where partial data is streamed.

Method

The generative model considered here is a particular case of mixture models (Bishop, 2006), where each mixture component consists of two conditionally independent modalities: space (\(s\)) and color (\(c\)). For 2D images, the space component represents the pixel row and column (\(s^{2}\)), while for 3D data it represents the Cartesian coordinate (\(s^{3}\)). For both data types, we consider the color as an RGB value (\(c^{3}\)).

We consider both \(s_{k}\), and \(c_{k}\) to be distributed as multivariate Normal distributions with parameters \((_{s,k},_{s,k})\), and \((_{c,k},_{c,k})\) respectively. The components have a mixture weight \(z\), distributed as a Categorical with parameters \(\). Considering these parameters as latent random variables allows us to cast learning as inference. Specifically, \((_{s,k},_{s,k})\) are distributed according to a Normal Inverse Wishart distribution, \(_{c,k}\) is distributed as a Normal distribution, and \(_{c,k}\) as a delta distribution. The prior over the component weight is a Dirichlet distribution. Appendix C.1 contains a table with the used hyperparameters. The joint distribution is factorized as:

\[p(s,c,z,_{s},_{s},_{c},_{c},)= _{n=1}^{N}p(s_{n}|z_{n},_{s},_{s})p(c_{n}|z_ {n},_{c},_{c})p(z_{n}|)\] (1) \[_{k=1}^{K}p(_{k,s},_{k,s})p(_{k,c}, _{k,c})p().\] (2)

During parameter inference, we aim to find the posterior over model parameters which maximizes model evidence concerning the observed data \((s,c)\). However, as computing this posterior is intractable, we resort to variational inference (Jordan et al., 1998). We introduce a variational posterior (denoted by q) with the following mean-field approximation

\[q(z,_{s},_{s},_{c},_{c},)= _{n=1}^{N}q(z_{n})_{k=1}^{K}q( _{k,c},_{k,c})_{k=1}^{K}q(_{k,s},_{k,s} )q(),\] (3)

We consider each of these distributions from the same family as the generative model described above. We optimize the evidence lower bound (ELBO) with respect to the variational parameters (Jordan et al., 1998)

\[=_{}[q(z,_{s},_{s},_{c},_{c}, ) p(z,_{s},_{s},_{c},_{c},|s,c)],\] (4)

using coordinate ascent variational inference (Beal, 2003; Bishop, 2006; Blei et al., 2017). This method leverages the mean field factorization and alternates updating the model assignments \(q(z)\), using the variational posterior over parameters \(q(_{s},_{s},_{c},_{c},)\) from the previous step, and vice versa. As each of the conjugate priors are conjugate, and part of the exponential family, computing a model update reduces to summing the sufficient statistics of the data together with their prior's natural parameters (see Appendix B for the update rules).

This optimization scheme naturally lends itself to continual learning, as the parameter updates for each component are computed as the sum of the prior's natural parameters and the sufficient statistics of the data associated with this component, through assignments \(q(z)\). These updates are inherently order invariant and therefore allow for iterative updates. Note that components without assignments revert to their prior. Crucially, in this setting, the assignments \(q(z)\) should always be computed with respect to the parameters of the _initial_ approximate posterior over the parameters \(q(_{s},_{s},_{c},_{c},)\). This ensures that components without prior assignments can still be used to model the data.

Images are generated using the mixture model by computing the expected value of the color, conditioned on a spatial coordinate (\(_{p(c|s)}[c]\)). For 3D rendering, we use the renderer from Kerbl et al. (2023), where the spatial component is first projected onto the image plane using the camera parameters, and the estimated depth is used to deal with occlusion. For more details, see Appendix D.

## 3 Results

We benchmark our approach against backpropagating gradients through a differentiable renderer. In particular, we compare the following models on both the Tiny ImageNet (Le and Yang, 2015) and Blender 3D (Mildenhall et al., 2020) datasets:

**VBGS (Ours)**: We consider a variant of VBGS where the means of the initial posteriors (\(q(_{s},_{s},_{c},_{c})\)) are initialized on sampled points from the normalized data set (Data Init), as well as randomly initialized (\(m_{k,s}[-1,1]\), \(m_{k,c}(0)\)) (Random Init). Note that in the case of random initialization, the data is first normalized using the estimated statistics, refer to Appendix E for more details.

**Gradient**: In 3DGS (Kerbl et al., 2023) the parameters are directly optimized using stochastic gradient descent on a weighted image reconstruction loss (\((1-)+\)). In the case of image data, \(\) is set to \(0\), in the case of 3D data \(\) it is set to \(0.2\). We use spherical harmonics with no degrees of freedom, i.e., the specular reflections are not modeled. In order to be able to compare performance w.r.t. model size, we also don't do densification or shrinking (Kerbl et al., 2023) (for these results see Appendix F.3). When optimizing for images, we use a fixed camera pose at identity and keep the z-coordinate of the Gaussians fixed at a value of 1. Similar to the VBGS approach, we also consider a variant where the means of the Gaussian components are randomly initialized (Random Init), and on sampled points from the dataset (Data Init).

We first evaluate performance on the Tiny ImageNet test set (10k images). We measure the reconstruction accuracy in terms of PSNR (dB) for varying numbers of components (Figure 1 (left)). We observe that our approach performs on par in terms of reconstruction error. Next, we evaluate the model in a continual learning setting: here, patches of data are sequentially observed and processed (Figure 1 (right)). While VBGS can represent all the previously observed patches with an equal level of quality, the gradient-based approach emphasizes the last observed patch. For a quantitative analysis of the performance of continual learning over the entire validation set, see Appendix F.2.

As an additional experiment, we evaluated the wall-clock time it takes for the Gradient approach to reach the performance that VBGS achieves after a single update step. This was computed over all images of the Tiny ImageNet validation set. We observe that VBGS is significantly (t-test, \(p=0\)) faster in wall clock time (\(0.03 0.03\) seconds) compared to Gradient (\(0.05 0.02\) seconds).

Next, we evaluate VBGS for 3D structures using the Blender dataset from Mildenhall et al. (2020). We fit model parameters using the 200 frames from the test set because this also contains depth information. VBGS is trained on the 3D point cloud, which is acquired by transforming the RGBD frame to a shared reference frame. In contrast, the gradient-based approach is optimized using multi-view image reconstruction. We evaluate reconstruction performance on 100 frames from the validation set. The results, measured as PSNR, for a model with a capacity of 100K components are shown in Table 1. We observe that both approaches perform better when the components are initialized using the data instead of randomly. We find that our approach performs on par with the gradient-based approach for Data Init. In random initialization, our approach outperforms the gradient-based approach except for the "ship" object. For a more in-depth analysis where we vary the number of components, see Appendix F.1.

Novel view predictions for the 8 blender objects are shown in Figure 2 (left). These renders are generated using a VBGS with 100K components, observed from a camera pose selected from the validation set. Note that these are rendered on a white background, and only the 3D object is modeled by the VBGS. Figure 2 (right) shows the reconstruction performance as a function of the number of available components. It can be observed that for lower component regimes, VBGS renders patches

Figure 1: **Mixture model performance on image data.** (left) Shows the reconstruction performance, in PSNR (dB), for various numbers of components. (right) Shows reconstruction performance, in PSNR (dB), at various stages in a continual learning setting for a random initialized model (both VBGS and Gradient-based) with 1K components.

of ellipsoids, while the gradient approach fills the areas more easily. We attribute this to a strong prior over the covariance shape, encoded in the Wishart hyperparameters.

Finally, we also conduct the continual learning experiment for 3D and observe that the same properties from the 2D experiment hold, reaching an average reconstruction error over all objects of \(11.19 3.53\) dB for VBGS (Random Init) and \(21.26 1.76\) dB for Gradient (Random Init). Crucially, in the continual learning setting the model does not have access to the data for initialization, and has to be initialized randomly. For a quantitative evaluation of performance as a function of the amount of observations, see Appendix F.2.

## 4 Discussion and Conclusion

In this paper, we proposed a method for optimizing Gaussian splats using Variational Bayes. We evaluated this method on both a 2D and a 3D dataset and showed that we achieved on-par performance while gaining the advantage that this approach enables continual learning. One of the main limitations of our approach compared to 3DGS is that we require RGBD data, as opposed to optimizing directly on RGB projections. We did not consider a mechanism to grow the model, whereas 3DGS achieves the best results when it can dynamically grow and shrink the model. Future research could investigate a principled approach to dynamically determine model size, as we can measure the model evidence (Friston et al., 2023). While VBGS only requires a single update step, it's important to note that this step is computationally more expensive than a single backpropagation step. This means that in the case of a large dataset that doesn't fit in memory, we are required to split it into distinct update steps and stochastic gradient descent might be faster overall. Of course, one might not need all this data, which could be an interesting avenue of research, i.e. active data selection. We believe that VBGS could open the door toward active learning, e.g. in robot SLAM. Due to continual learning, the model can continuously integrate novel information without the need for a replay buffer (Sucar

    & **chair** & **drums** & **focus** & **hotdog** & **lego** & **materials** & **mic** & **ship** \\  VBGS & \(22.82\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(24.78\) & \(21.23\) \\ (Data Init) & \( 0.94\) & \( 0.47\) & \( 0.79\) & \( 1.23\) & \( 0.89\) & \( 1.64\) & \( 0.60\) & \( 0.64\) \\ VBGS & \(21.35\) & \(18.71\) & \(21.49\) & \(22.24\) & \(20.59\) & \(20.47\) & \(24.42\) & \(20.80\) \\ (Random Init) & \( 0.69\) & \( 0.43\) & \( 0.75\) & \( 0.97\) & \( 0.86\) & \( 1.36\) & \( 0.58\) & \( 0.90\) \\ Gradient & \(\) & \(19.05\) & \(21.08\) & \(21.47\) & \(19.97\) & \(20.53\) & \(\) & \(\) \\ (Data Init) & \( 1.23\) & \( 0.57\) & \( 0.92\) & \( 1.67\) & \( 2.24\) & \( 1.64\) & \( 0.90\) & \( 1.20\) \\ Gradient & \(20.59\) & \(15.04\) & \(19.41\) & \(19.81\) & \(19.10\) & \(16.11\) & \(23.03\) & \(21.15\) \\ (Random Init) & \( 0.85\) & \( 1.13\) & \( 0.90\) & \( 1.86\) & \( 1.02\) & \( 1.45\) & \( 0.72\) & \( 0.82\) \\   

Table 1: **Reconstruction performance for the 3D dataset**. Measured as PSNR (dB). Values (\(\)) are computed over 100 validation frames for each of the 8 blender objects. All models in this table have 100K components. The best performance for each column is marked in bold.

Figure 2: **Mixture model performance on 3D data. (left) Image reconstructions for each of the 8 objects, given a VBGS with 100k components. (right) Qualitative performance when using various amounts of components for reconstructing “lego” for both VBGS and the gradient approach.**

et al., 2021; Matsuki et al., 2024). Combining this with the variational posterior over parameters, an embodied agent would be able to do parameter-based exploration (Schwartenbeck et al., 2013), efficiently exploring and building a model of the environment in real-time.

## 5 Acknowledgments and Disclosure of Funding

The authors would like to thank the members of VERSES for critical discussions and feedback that improved the quality of this work, with special thanks to Jeff Beck, Tommaso Salvatori, and Conor Heins.