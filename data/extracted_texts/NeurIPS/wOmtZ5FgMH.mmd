# RWKU: Benchmarking Real-World Knowledge

Unlearning for Large Language Models

 Zhuoran jin1,2, Pengfei Cao1,2, Chenhao Wang1,2, Zhitao He1,2,

**Hongbang Yuan1,2, Jiachun Li1,2, Yubo Chen1,2,*, Kang Liu1,2,3, Jun Zhao1,2**

\({}^{1}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{2}\)Institute of Automation, Chinese Academy of Sciences

\({}^{3}\)Shanghai Artificial Intelligence Laboratory

{zhuoran.jin, pengfei.cao, yubo.chen, kliu, jzhao} @nlpr.ia.ac.cn

Corresponding author.

###### Abstract

Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a **Real-World**K**nowledge **U**nlearning benchmark (\(\)) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the **task setting**, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the **knowledge source**, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the **evaluation framework**, we design the forget set and the retain set to evaluate the model's capabilities across various real-world applications. Regarding the forget set, we provide four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of _neighbor perturbation_, _general ability_, _reasoning ability_, _truthfulness_, _factuality_, and _fluency_. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.

## 1 Introduction

Large language models (LLMs) [60; 44], trained on massive internet corpora, can encapsulate a vast amount of knowledge within their parameters, and possess the capability to recall and manipulate this knowledge during the generation process. However, this capability is dual-use, potentially leading to privacy problems, copyright concerns, and harmful issues [45; 28]. For instance, LLMs may memorize personally identifiable information (_e.g._, social security numbers) or copyrighted material (_e.g._, Harry Potter series) from the training data, and emit it verbatim when prompted with adversarial attacks . Besides, AI assistants for biology could troubleshoot bottlenecks in biological weapons development, increasing the risk of such attempts. According to regulations such as the European General Data Protection Regulation (GDPR) upholding individuals' "right to be forgotten" (RTBF) , sensitive and toxic knowledge within LLMs should also be erasable. A straightforward solution is to retrain the model from scratch, ensuring it excludes any data that users have requested to be removed. However, this is not feasible for LLMs that consume extensive computational resources.

To efficiently remove specific knowledge by post hoc modifying models, **machine unlearning** has emerged as a solution [9; 25; 64; 13; 39]. An optimal unlearning method needs to satisfy the following criteria: completely forget the target knowledge, maintain the utility for downstream applications effectively, and accomplish the unlearning process efficiently. Recent works have proposed several techniques to enable LLMs to forget specific knowledge by fine-tuning on the data that needs to be unlearned. Although unlearning is a promising direction, there is a significant lack of comprehensive benchmarks and datasets for evaluating **real-world** knowledge unlearning. Designing a benchmark for real-world knowledge unlearning requires consideration of the following three key factors:

**Task Setting**. The task setting for unlearning should be practical to _real-world_ scenarios. Existing unlearning methods rely on fine-tuning the model on the forget corpus (_i.e._, a subset of the pre-training corpus). However, such a simplified task setting may not be feasible in real-world scenarios. On one hand, providing sensitive or copyrighted data to the model during the unlearning process can lead to secondary information leakage. Additionally, the pre-training corpus of most open-source LLMs is also inaccessible. On the other hand, during the model's pre-training process, the memory of a piece of parameterized knowledge may originate from multiple training points. Therefore, finding all the training points corresponding to this knowledge is like searching for a needle in a haystack.

**Knowledge Source**. The target to be unlearned should come from _real-world_ knowledge sources. Different from the fictitious unlearning task , we need to ensure that the knowledge to be forgotten should originally exist within various LLMs, without the need first to fine-tune the model with this knowledge. This affirms a more realistic unlearning process. Moreover, compared to forgetting a certain ability (_e.g._, hazardous knowledge) , the boundaries of knowledge to be forgotten should be clear, ensuring that the unlearning process is precise and the evaluation result is reliable.

**Evaluation Framework**. Evaluating the model after unlearning requires considering the impact on _real-world_ downstream applications. Current benchmarks for assessing the efficacy of unlearning are non-adversarial, simply using multiple-choice or question-answer formats. However, malicious users may use jailbreak techniques  to induce the model to generate knowledge that has been deleted. Therefore, it is necessary to assess the model after unlearning under adversarial-attack probes. We should also consider the side effects on the model's original capabilities, particularly the neighboring knowledge that is closely related to the unlearning target. This requires the unlearning method to accurately delineate the scope of forgetting. Additionally, we should thoroughly evaluate the impact on the model's general and reasoning capabilities. Since unlearning essentially negates the knowledge originally acquired by the model, we should also assess its effects on truthfulness and factuality.

In this paper, we propose a **Real-World **K**nowledge **U**llearning benchmark (**RWKU**). RWKU is designed based on the three key factors mentioned above: (1) For the **task setting**, we consider a more practical and challenging setting, similar to "_zero-shot knowledge unlearning_". We provide only the unlearning target and the original model, without offering any forget corpus or retain corpus. In this way, it avoids secondary information leakage caused by the forget corpus and is not affected by the distribution bias of the retain corpus. (2) For the **knowledge source**, we choose real-world famous people from Wikipedia as the unlearning targets and demonstrate that such popular knowledge is widely present in various LLMs through memorization quantification, making it more suitable for knowledge unlearning. Additionally, choosing entities as unlearning targets can well clearly define the unlearning boundaries. (3) For the **evaluation framework**, we carefully design the forget set and the retain set to evaluate the model's capabilities from multiple real-world applications. Regarding the forget set, we evaluate the **efficacy** of knowledge unlearning at both the knowledge memorization (fill-in-the-blank style) and knowledge manipulation (question-answer style) abilities. Specifically, we also evaluate these two abilities through adversarial attacks to induce forgotten knowledge in the model. We adopt four membership inference attack (MIA) methods for knowledge memorization on our collected MIA set. We meticulously designed nine types of adversarial-attack probes for knowledge manipulation, including _prefix injection_, _affirmative suffix_, _role playing_, _reverse query_, and others. Regarding the retain set, we design a neighbor set to test the impact of _neighbor perturbation_, specifically focusing on the **locality** of unlearning. In addition, we assess the model **utility** on various capabilities, including _general ability_, _reasoning ability_, _truthfulness_, _factuality_, and _fluency_.

In detail, RWKU contains 200 real-world unlearning targets and 13,131 multi-level forget probes, including 3,268 fill-in-the-blank probes, 2,879 question-answer probes, and 6,984 adversarial-attack probes. To construct the forget probes, we first use GPT-4  to generate an excess of query-answer pairs related to the unlearning targets. Then, we filter these queries using mainstream open-sourcemodels to ensure that the knowledge is already present in these models. Finally, we manually check these probes to ensure their format and type are correct. Similarly, we construct the neighbor set to test the perturbation of neighboring knowledge, which includes 11,379 neighbor probes.

Based on the RWKU benchmark, we conduct extensive experiments across two unlearning scenarios (single-target and batch-target unlearning), two models (LLaMA3  and Phi-3 ) and six baseline methods. Our experimental results reveal the following findings: (1) Compared to question-answer probes, models after unlearning is more susceptible to adversarial-attack probes and fill-in-the-blank probes, which can induce them to reveal knowledge that appears to have been removed. Additionally, these methods seem to be ineffective against MIAs. (2) It is challenging to balance the unlearning efficacy and locality. While unlearning the target knowledge, there are also side effects on neighboring knowledge. Meanwhile, unlearning can also affect model utility, such as truthfulness and fluency. (3) Batch-target unlearning is significantly more challenging than single-target unlearning and can potentially lead to model collapse. (4) Among all the baseline methods, the classic gradient ascent , the recent negative preference optimization , and a simple in-context unlearning method perform relatively well. This highlights the need for further research and indicates significant room for improvement on this benchmark. In summary, our key contributions are as follows:

(1) We introduce the Real-World Knowledge Unlearning benchmark (RWKU), which contains 200 real-world unlearning targets, 13,131 forget probes, and 11,379 neighbor probes. We consider a more practical and challenging setting, where neither the forget corpus nor the retain corpus is accessible.

(2) We design the forget set and retain set to evaluate the model's capabilities across various real-world applications. For the forget set, we provide four MIA methods and nine kinds of adversarial attack probes to rigorously test unlearning **efficacy**. For the retain set, we assess **locality** and **utility** in terms of _neighbor perturbation_, _general ability_, _reasoning ability_, _truthfulness_, _factuality_, and _fluency_.

(3) We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods. From the experimental results, we observe several interesting findings that highlight the need for further research and indicate significant room for improvement on this benchmark.

(4) Beyond knowledge unlearning, our benchmark could also contribute to research in knowledge probing, knowledge localization, and model jailbreak. To enable further research, we release our datasets and code publicly at http://rwku-bench.github.io.

## 2 Related Work

### Knowledge Unlearning for Large Language Models

Machine unlearning [9; 8] focuses on effectively removing specific memorized content from trained machine-learning models to ensure the right to be forgotten. In the field of computer vision, machine unlearning has been extensively studied [17; 18; 19; 29; 61], primarily focusing on the removal of specific training samples in classification tasks. However, this may not be sufficient for generative LLMs, considering their vast parametric knowledge and the interwovem capabilities they possess.

Recently, there has been increasing attention on how to perform knowledge unlearning on LLMs [25; 13; 64; 63; 49; 7; 34; 39; 35; 43; 67]. From the perspective of knowledge sources, existing work primarily focuses on forgetting specific classification tasks [11; 47], memorized sequences [25; 4], copyrighted books [64; 13], and toxic capacities [37; 5; 30; 22]. Most unlearning methods rely on fine-tuning the model on the forget corpus, such as applying gradient ascent (GA) on the loss [25; 39]. Recently, there have been complementary methods to GA that adopt preference optimization , representation controlling , and rejection tuning  to unlearn the model. Moreover, task arithmetic (TA) is also an unlearning method, enabling efficient model editing through parameter merging [23; 22]. Although unlearning methods for large language models have rapidly developed, some studies [46; 36; 38; 54] have shown that it remains easy to extract supposedly forgotten knowledge from the models after unlearning. Therefore, there remains significant room for research on unlearning methods.

### Unlearning Benchmarks for Large Language Models

As knowledge unlearning methods for LLMs continue to emerge, the demand for unlearning datasets and benchmarks has become increasingly urgent. Eldan and Russinovich  propose a "Who is Harry Potter" task (WHP), which involves fine-tuning the model on the forgetting corpus consisting of the Harry Potter series. The goal of WHP is to make it difficult for the unlearning model to generate content related to Harry Potter. WHP collects 300 prompts related to the Harry Potter universe as the forget set, and adopts some common datasets (such as Winogrande  and Hellaswag ) as the retain set. Li et al.  propose a Weapons of Mass Destruction Proxy Benchmark (WMDP), which requires unlearning methods to remove hazardous knowledge in biosecurity and cybersecurity. WMDP collects relevant papers from PubMed and documents from GitHub as the forget corpus and adopts Wikitext as the retain corpus. To evaluate the unlearning efficacy of hazardous capabilities, WMDP provides 4,157 expert-written multiple-choice questions as the forget set. For the retain set, WMDP uses MMLU  to evaluate the preservation of general knowledge and MT-Bench  to evaluate the fluency. Maini et al.  propose a task of fictitious unlearning (TOFU), which contains 200 fictitious authors with question-answer pairs synthesized by GPT-4. Different from previous unlearning datasets, TOFU first fine-tunes the model on the synthetic QA pairs to ensure that the model possesses this knowledge. During the unlearning process, TOFU selects a subset of QA pairs as the forget set and uses another subset of QA pairs as the retain set. Besides, TOFU also evaluates the model utility on Real Authors and World Facts. The detailed comparison between these benchmarks and RWKU is shown in Table 3.

## 3 The RWKU Benchmark

### Task Definition and Setting

Formally, given an unlearning target, a model \(g_{}\) with parameters \(\) is updated with a certain unlearning method, which results in an unlearned model with new parameters \(^{}\). Traditional unlearning tasks [25; 30; 39; 70], usually provide a forget corpus \(_{f}\), which is typically a subset of the pre-training corpus \(\) that contains the content to be forgotten. Unlearning methods aim to fine-tune the model to make it behave like the model trained only on \(_{f}\). Moreover, some tasks also collect a retain corpus \(_{r}_{f}\) to maintain the original general capabilities. However, existing methods make strong assumptions about the unlearning setting that simplifies the task considerably [14; 15]. Regarding the forget corpus \(_{f}\), it may contain private and copyrighted data. Providing \(_{f}\) to the model again during the unlearning process could result in secondary information leakage. Moreover, during the model's pre-training process, the memory of a specific piece of parameterized knowledge may be derived from multiple training points. Consequently, identifying all the training points corresponding to this knowledge is akin to searching for a needle in a haystack. For the retain corpus \(_{r}\), considering the efficiency of unlearning, it is typically a very small subset. Its selection is crucial because any deviation from the distribution of the corpus \(\) can potentially affect the model's performance.

Therefore, we consider a more practical and challenging setting in the RWKU benchmark: a novel zero-shot knowledge unlearning scenario for LLMs. We provide only the unlearning target \(t\) and the original model \(g_{}\), without offering any forget corpus \(_{f}\) or retain corpus \(_{r}\). Meanwhile, we also propose an effective solution for this novel task setting. Considering the powerful generative capabilities of LLMs, we can have the original model \(g_{}\) first generate texts related to the unlearning targets to serve as a synthetic forget corpus \(_{f}^{s}\), then apply existing unlearning methods to \(_{f}^{s}\).

### Data Collection and Construction

Knowledge Source.A general unlearning benchmark should be applicable to various mainstream open-source LLMs. This means ensuring that the knowledge to be forgotten is widely present in these models. Therefore, we choose famous people as the unlearning targets, requiring the unlearning method to erase factual knowledge about the targets from the model without affecting the neighbor knowledge. To collect the unlearning targets, we first scrape a list of famous people from The Most Famous All-time People Rank1. Then, we link these entities to Wikipedia and query the Wikipedia page views as a measure of entity popularity . By sorting the entities based on their popularity, we select the top 200 most popular entities as the unlearning targets.

Memorization Quantification.To further validate our collected unlearning targets, we quantify the memorization of various LLMs regarding knowledge from different sources. We adopt exact memorization (EM) to characterize the knowledge of a model \(g_{}\) with parameters \(\) over a textual sequence \(x_{1:T}=(x_{1},x_{2},,x_{T})\)[59; 4]. We compute exact memorization as follows: where \(()\) matches the \(N\)-gram \(x_{i:i+N-1}\) greedily decoded by \(g_{}\) with the ground truth text. We also use negative log-likelihood (NLL)  to measure the model's knowledge retention: \(_{}(x)=^{T-1} p(x_{i}|x_{<i})}{T-1}\). Higher EM and lower NLL indicate better memorization performance. We choose four different knowledge sources: (1) RWKU Knowledge: relevant descriptions from the Wikipedia pages of famous people in RWKU. (2) General Knowledge: relevant descriptions from the low-popularity Wikipedia pages . (3) Unseen Knowledge: relevant descriptions from the latest Wikipedia pages, which the models have not trained on. (4) C4 Corpus: training sequences from C4 Corpus . As shown in Figure 1, RWKU Knowledge achieves better memorization performance across Phi-2, LLaMA2, and LLaMA3. The results of memorization quantification reveal that the unlearning targets in RWKU are widely present across various LLMs.

Probe Construction.To construct the forget probes, we first use GPT-4 Turbo to generate an excess of query-answer pairs related to the unlearning targets. Specifically, we collect relevant passages about each unlearning target from their Wikipedia pages and then prompt GPT-4 to generate query-answer pairs related to the targets based on these passages. For knowledge memorization, knowledge manipulation and adversarial attack probes, we provide detailed prompt templates in Appendix E.1. Then, we use the auto-generated queries to probe LLaMA3 8B, retaining only those queries whose correct answers are recalled in the model's outputs. This approach ensures the consistency of the QA pairs and confirms that the model possesses this knowledge. Finally, we manually verify these probes to ensure their format and type are correct, particularly focusing on adversarial attack types.

To construct the neighbor probes, we primarily focus on selecting neighboring knowledge that is closely related to but not entirely contained within the unlearning targets. For each unlearning target, we consider the hyperlinks on its Wikipedia page as related entities. We then filter these related entities based on their popularity and GPT-4 analysis to obtain the neighboring knowledge. Finally, we construct neighbor probes like that of the forget probes. Refer to Appendix E.1 for more details.

Quality Assessment.We evaluate the quality of probes from the perspectives of diversity and correctness. For the diversity, we conduct a manual clustering analysis on 200 randomly sampled probes. We identify 48 distinct cluster centers, demonstrating that the questions cover a wide range of topics. We list the top 15 manually annotated cluster categories in Table 4, which are ranked based on the number of probes contained within each category. Furthermore, we also analyze the lexical similarity of questions within each category. Our findings show that for most categories, the questions are indeed diverse. For the correctness, we conduct a random sampling evaluation of 1,000 probes to assess their accuracy. The evaluation by GPT-4 shows an accuracy rate of 98.7%, while the manual evaluation achieved an accuracy rate of 99.1%, demonstrating the high quality of the probes. Additionally, we also adopt an NER toolkit  to classify the knowledge points in the probes. As shown in Table 5, these knowledge points are meaningful entities rather than simple nouns.

### Evaluation Framework

We illustrate the RWKU evaluation framework in Figure 2. RWKU uses real-world famous people as unlearning targets (_e.g._, "_Forgetting Stephen King_"), typically briefly describing the target to avoid ambiguity. Below, we comprehensively introduce the forget assessment and the retain assessment.

Figure 1: Memorization quantification of different knowledge sources.

#### 3.3.1 Forget Assessment.

We conduct the forget assessment to evaluate the unlearning **efficacy** at both the knowledge memorization [73; 66; 26] and knowledge manipulation abilities [62; 3]. Knowledge memorization aims to measure the model's ability to recall knowledge fragments seen during the training stage. Knowledge manipulation focuses on evaluating the model's ability to use acquired knowledge to complete downstream tasks. Compared with knowledge memorization, knowledge manipulation is a higher-level and more commonly used ability. Moreover, we also evaluate these two abilities through **adversarial attacks** to induce forgotten knowledge in the unlearned model.

Knowledge Memorization.We use fill-in-the-blank style probes (FB) to examine the memory of the original training data related to the unlearning targets. We extract some sentences from the Wikipedia page of the unlearning target, replace knowledge points with " ----", and ask the model to complete the blanks. We use the ROUGE-L recall score  to measure the relevance between the model's predictions and the ground truth answers. For unlearning efficacy, lower scores are better.

To rigorously audit whether the model still retains the target knowledge, we employ membership inference attacks (MIAs) [53; 12; 55], which have been used for privacy auditing and investigating copyright violations. MIAs attempt to infer whether a particular input is a member of the model's training data. We collect some knowledge fragments about the unlearning target as the forget member set (FM). For comparison, we also sample some unrelated knowledge fragments as the retain member set (RM). We provide four MIA methods, including LOSS , Zlib Entropy , Min-K% Prob  and Min-K%++ Prob  in RWKU. In our experiments, we primarily report the LOSS scores. Higher scores indicate a lower likelihood that the model retains the specific knowledge. Therefore, a well-learned model should exhibit significantly higher LOSS scores on the FM compared to the RM.

Knowledge Manipulation.We adopt question-answer style probes (QA) to assess the ability of the unlearned model to utilize knowledge in practical applications. We construct questions by paraphrasing and restructuring knowledge fragments related to the unlearning targets. Meanwhile, malicious users may use jailbreak techniques  to bypass restrictions and access forgotten knowledge. We should consider more rigorous adversarial attack probes (AA) when evaluating unlearning efficacy. Therefore, we carefully design nine types of adversarial attacks, which are detailed as follows:

1. **Prefix Injection**: Adding some requests or commands before the question to instruct the model to answer the question;
2. **Affirmative Suffix**: Adding affirmative phrases after the question to elicit positive answers;
3. **Role Playing**: Letting the model play specific roles, such as experts, historians and scientists;
4. **Multiple Choice**: Letting the model choose from multiple options rather than answer;
5. **Reverse Query**: Querying the unlearning target based on target-related information, ensuring that the answer is the target itself;

Figure 2: The evaluation framework of RWKU.

6. **Synonym Manipulation**: Using synonyms to replace key terms related to the target or other entities in the question, such as aliases for people and abbreviations for places;
7. **Background Hint**: Adding some target-related background information before the question;
8. **In-context Learning**: Adding a question-answer pair related to the target before the question to guide the model to answer;
9. **Cross Linequal**: Asking the question in other languages, including French, German, Spanish.

We provide additional data examples in Appendix E.2. For both QA and AA probes, we also use the ROUGE-L recall score for evaluation. Lower scores are better for evaluating unlearning efficacy.

#### 3.3.2 Retain Assessment.

When evaluating the unlearned model, we should also consider the side effects on the model's original capabilities. We conduct the retain assessment from two perspectives: (1) **Locality**: The unlearning process should be precise, without exceeding the boundaries of the target knowledge and perturbing the surrounding neighboring knowledge. (2) **Model Utility**: Beyond neighboring knowledge, the model's performance on various real-world applications should not be impacted.

Neighbor Perturbation.We define neighboring knowledge in the unlearning task as that which is closely related to, but not entirely contained within the scope of the unlearning targets. For example, when the target is "_Forgetting Stephen King_", the model should forget "_Who the author of 'The Shining' is_", but not forget "_Who plays the character Jack Torrance in the film 'The Shining'?_" We assess the neighbor perturbation based on knowledge memorization and manipulation. For evaluating unlearning locality, higher scores are better.

Model Utility.We assess the model utility on various capabilities, which are detailed as follows:

1. **General Ability (Gen)**: We use MMLU , which consists of multiple-choice questions from various branches of knowledge. We report 5-shot accuracy based on answer perplexity;
2. **Reasoning Ability (Rea)**: We use Big-Bench-Hard (BBH)  with 27 subtasks. The evaluation uses chain-of-thought prompts with 3-shot examples, and EM scores are reported;
3. **Truthfulness (Tru)**: To evaluate whether the model becomes dishonest after unlearning, we use the TruthfulQA's MC1 task , and 6-shot accuracy scores are reported;
4. **Factuality (Fac)**: Considering unlearning negates the original knowledge acquired by the model, we evaluate the factuality on TriviaQA  with 6-shot, and F1 scores are reported;
5. **Fluency (Flu)**: To measure the generation quality of models, we adopt the instructions in AlpacaEval , and report the weighted average of bi- and tri-gram entropies [71; 42].

For all the above datasets, higher scores are better. For better reproducibility, we provide detailed evaluation prompts and dataset statistics in Appendix F.1 and F.2.

## 4 Experimental Setup

### Model and Data Preparation

We conduct unlearning experiments on LLaMA3-Instruct (8B) and Phi-3 Mini-4K-Instruct (3.8B). As shown in Table 8, we also report the original performance of LLaMA2-Chat (7B) and Mistral-Instruct-v0.2 (7B) for reference. Because most existing unlearning methods for LLMs rely on fine-tuning with the forget corpus \(_{f}\), they may not be directly applicable to our novel task setting. To address this, we propose a simple and effective solution for this novel task setting. Specifically, we prompt the original model to generate text descriptions related to the unlearning targets, which can serve as the synthetic forget corpus \(C^{s}_{f}\). The specific prompt and generated data examples are presented in Appendix G.1 and G.3. For comparison, we also provide the text descriptions from the corresponding Wikipedia pages of each unlearning target as the pseudo ground-truth forget corpus \(^{s}_{f}\).

### Baseline Methods

1. **In-Context Unlearning (ICU)**: We use specific instructions to make the model behave as if it has forgotten the target knowledge, without actually modifying the model parameters.

2. **Representation Engineering (RepE)**[74; 30]: We provide the model with expert and novice keywords as prompts, respectively, and then store the model's hidden states. Subsequently, we calculate the unlearning control vector, which represents the absence of target knowledge. We use it to control the model's activation space during the inference process.
3. **Gradient Ascent (GA)**: In contrast to the gradient descent during the pre-training phase, we maximize the negative log-likelihood loss on the forget corpus. This approach aims to steer the model away from its initial predictions, facilitating the process of unlearning.
4. **Direct Preference Optimization (DPO)**: We apply preference optimization to enable the model to generate incorrect target knowledge. DPO requires positive and negative examples to train the model. For the positive example, we sample it from the counterfactual corpus \(^{c}_{f}\), which consists of intentionally fabricated descriptions generated by the model about the target. For the negative example, we sample it from the synthetic forget corpus \(^{s}_{f}\).
5. **Negative Preference Optimization (NPO)**: NPO is a simple drop-in fix of the GA loss. Compared to DPO, NPO retains only the negative examples without any positive examples.
6. **Rejection Tuning (RT)**: First, we have the model generate some questions related to the unlearning targets, then replace its responses with _"I do not know the answer."_. Then, we use this refusal data to fine-tune the model so that it can reject questions related to the target.

We train the models using three approaches: full fine-tuning, partial-layer fine-tuning and LoRA . The main experiment adopts the single-target unlearning setting, where one target is forgotten at a time, and the results are averaged over 100 unlearning targets. We provide all the implementation details and hyper-parameter settings in Appendix H.

## 5 Results

Overall Results.Table 1 and Table 7 present the main experimental results of various unlearning methods on LLaMA3 and Phi-3, respectively. We can find the following conclusions: (1) Compared to question-answer probes, models after unlearning is more susceptible to fill-in-the-blank probes and adversarial-attack probes. It implies that although the unlearned model (especially after RT) may forget how to utilize the knowledge, it can still be detected through knowledge memorization probes. Besides, this also demonstrates the effectiveness of our carefully designed adversarial attacks in eliciting seemingly forgotten knowledge from the unlearned model. (2) The method achieves even greater unlearning efficiency when fine-tuned on the synthetic forget corpus \(^{s}_{f}\) compared to the pseudo ground-truth forget corpus \(^{s}_{f}\). This may be because, while \(^{s}_{f}\) encompasses a broader spectrum of knowledge about the target, it also includes a significant amount of irrelevant knowledge. Conversely, \(^{s}_{f}\), generated by the original model itself, is likely more aligned with the model's internal memory. (3) Nevertheless, almost all methods trained on \(^{s}_{f}\) fail under MIA, indicating a need for more robust unlearning methods. (4) Compared to full fine-tuning, LoRA unlearns less (on the forget set) and forgets less (on the retain set), consistent with recent findings on continued pretraining .

    &  &  &  &  \\   & FB & QA & AA & All & FB & QA & All & FM \(\) & RM \(\) & Gen & Rea & Tru & Fac & Flu \\  Before & 85.9 & 76.4 & 77.7 & 79.6 & 95.6 & 85.3 & 90.7 & 226.7 & 230.4 & 65.7 & 42.3 & 36.8 & 53.5 & 705.8 \\  ICU & **26.2** & **1.9** & **10.3** & **12.8** & 65.0 & 46.5 & 55.7 & 247.1 & 258.4 & 63.6 & 39.3 & 36.4 & 48.2 & 705.0 \\ RepE & 29.8 & 33.6 & 37.8 & 34.8 & 46.2 & 38.8 & 42.6 & 292.0 & 290.0 & 64.8 & 26.3 & 37.6 & 17.9 & 703.7 \\  GA* (Full) & 40.7 & 36.5 & 43.7 & 41.4 & 68.6 & 68.6 & 68.1 & **1640.9** & 766.2 & **65.5** & 39.7 & **37.8** & 41.9 & 692.4 \\ GA* (LoRA) & 70.3 & 65.6 & 67.8 & 68.2 & 80.6 & 75.5 & 77.5 & 879.5 & 665.1 & 64.0 & 37.8 & 37.3 & 43.8 & 711.3 \\ GA (Full) & 39.1 & 31.6 & 46.7 & 41.9 & 84.6 & 73.6 & 79.0 & 258.6 & 231.0 & 64.9 & **42.0** & 35.9 & 52.5 & 705.1 \\ GA (LoRA) & 67.0 & 53.2 & 61.8 & 61.3 & 90.1 & 80.4 & 85.3 & 224.1 & **212.6** & 64.7 & 41.5 & 36.6 & 52.8 & 697.3 \\ DPO (Full) & 46.3 & 38.5 & 41.6 & 41.9 & 59.2 & 51.3 & 55.2 & 243.6 & 240.8 & 64.1 & **42.0** & 31.5 & 25.8 & **725.9** \\ DPO (LoRA) & 75.3 & 65.4 & 68.6 & 69.5 & 90.0 & 81.5 & 85.6 & 228.0 & 231.2 & 65.6 & **42.0** & 34.5 & 55.5 & 702.7 \\ NPO (Full) & 33.4 & 21.0 & 24.8 & 26.2 & 76.0 & 69.9 & 72.6 & 278.9 & 26.3 & 64.8 & 41.5 & 34.9 & 41.2 & 712.2 \\ NPO (LoRA) & 75.1 & 64.3 & 69.0 & 69.7 & **91.3** & **82.2** & **86.7** & 225.1 & 227.0 & 64.9 & 41.7 & 36.0 & 54.0 & 707.3 \\ RT (Full) & 72.7 & 13.4 & 22.8 & 33.1 & 86.9 & 45.6 & 67.4 & 222.7 & 226.6 & 65.4 & 41.4 & 34.9 & **59.3** & 588.1 \\ RT (LoRA) & 85.4 & 49.6 & 53.2 & 60.5 & 87.3 & 74.1 & 81.9 & 226.0 & 223.9 & 64.5 & 41.2 & 33.6 & 58.2 & 667.7 \\   

Table 1: Results of our main experiment on LLaMA3-Instruct (8B). The best results are highlighted in **bold**, and the second-best results are in underlined. * denotes the method trained on the pseudo ground truth forget corpus. \(\) means higher is better, and \(\) means lower is better.

(5) Among all the baseline methods, ICU achieves the best results on LLaMA3, while it has almost no effect on Phi-3, depending on the model's ability to follow instructions. For those methods that change the model parameters, the classic GA and the recent NPO perform relatively well. These findings highlight the need for further research on unlearning methods.

**Trade Off.** We show the trade-off between unlearning efficacy, locality and model utility in Figure 3 (where trainable methods sample different training epochs and RepE samples different intervention weights). A good unlearning method should be a straight line down from the top right to the bottom right. We can observe the following phenomenon: (1) It is challenging to balance the unlearning efficacy and locality. While unlearning the target knowledge, there are also side effects on neighboring knowledge, even with ICU which does not require training. (2) Meanwhile, unlearning can also affect model utility. For example, DPO rewards the model for fabricating relevant information about the target knowledge, which encourages the model to generate hallucinations, thereby significantly affecting factuality and truthfulness. RT requires the model to simply respond with _"I don't know"_ during training, which may affect the model's generative capability.

**Adversarial Attack Types.** Figure 4 illustrates the effectiveness of different types of adversarial attacks in inducing target knowledge from the model after forgetting. We can observe that prefix injection, affirmative suffix, multiple choice and reverse query attacks effectively elicit unlearned knowledge from the model. Because RT is fine-tuned on refusal data, it achieves the best unlearning efficiency under adversarial attacks. NPO also demonstrates the potential to resist adversarial attacks.

**Batch-target Unlearning.** We also explore a particularly challenging unlearning scenario, involving the forgetting of multiple targets simultaneously. As illustrated in Figure 5, we conduct batch-unlearning experiments with target sizes of 10, 20, 30, 40, and 50. We can observe that the unlearning methods exhibit three phenomena: (1) DPO and NPO fail to complete unlearning while maintaining the original performance on the forget set and the retain set. (2) GA starts to lead to model collapse when the target size equals 30. (3) RT, as a variant of instruction tuning, can complete the unlearning task more stably and will not have a significant impact on neighbor knowledge.

Figure 4: Comparison of different adversarial attack types on LLaMA3-Instruct (8B).

Partial-layer Unlearning.We conduct an interesting experiment to verify that updating the parameters in which layers can achieve more effective unlearning. We choose to fine-tune four consecutive layers of LLaMA3 (_e.g._, layers 0-3) and then freeze the remaining layers (_e.g._, layers 4-32). As shown in Figure 7, we can observe a phenomenon: fine-tuning the early layers leads to better unlearning effects without affecting neighbor knowledge. One possible explanation is that unlearning in the early layers may involve twisting the meanings of keywords related to the forgetting target. Another possible explanation is that the early layers store more factual knowledge [42; 16]. The localization of the unlearning target knowledge is also a fascinating problem. If only a specific few parameters need to be updated in the model to achieve unlearning, it could greatly preserve the original capabilities.

Case StudyWe conduct a case study on the forgetting effects of unlearning methods. As shown in Appendix J, we can observe that ICU and RT methods usually lead the model to refuse to answer, while GA, DPO and NPO incline the model towards providing an erroneous answer as an alternative.

## 6 Conclusion and Future Work

In this paper, we propose a **R**eal-**W**orld **K**nowledge **U**nlearning benchmark (**RWKU**) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the **task setting**, we consider a more practical and challenging unlearning setting. (2) For the **knowledge source**, we choose 200 real-world famous people as the unlearning targets. (3) For the **evaluation framework**, we provide membership inference attacks and adversarial attack probes to rigorously test unlearning efficacy. We also assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. In the future, we plan to diversify knowledge sources (_e.g._, event knowledge and concept knowledge), incorporate more attack methods (_e.g._, gradient-based attacks), and adopt more comprehensive evaluation metrics (_e.g._, balancing efficacy and locality).

Figure 5: Results of batch-target unleaning experiments on LLaMA3-Instruct (8B).