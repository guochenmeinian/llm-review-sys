# Towards Efficient and Optimal Covariance-Adaptive Algorithms for Combinatorial Semi-Bandits

Julien Zhou

Criteo AI Lab

Paris, France

Univ. Grenoble Alpes, Inria,

CNRS, Grenoble INP, LJK,

38000 Grenoble, France

julien.zhou@inria.fr

&Pierre Gaillard

Univ. Grenoble Alpes, Inria,

CNRS, Grenoble INP, LJK,

38000 Grenoble, France

&Thibaud Rahier

Criteo AI Lab,

Paris, France

&Houssam Zenati

Universite Paris-Saclay, Inria,

Palaiseau, France

&Julyan Arbel

Univ. Grenoble Alpes, Inria,

CNRS, Grenoble INP, LJK,

38000 Grenoble, France

###### Abstract

We address the problem of stochastic combinatorial semi-bandits, where a player selects among \(P\) actions from the power set of a set containing \(d\) base items. Adaptivity to the problem's structure is essential in order to obtain optimal regret upper bounds. As estimating the coefficients of a covariance matrix can be manageable in practice, leveraging them should improve the regret. We design "optimistic" covariance-adaptive algorithms relying on online estimations of the covariance structure, called OLS-UCB-C and COS-V (only the variances for the latter). They both yield improved gap-free regret. Although COS-V can be slightly suboptimal, it improves on computational complexity by taking inspiration from Thompson Sampling approaches. It is the first sampling-based algorithm satisfying a \(O()\) gap-free regret (up to poly-logs). We also show that in some cases, our approach efficiently leverages the semi-bandit feedback and outperforms bandit feedback approaches, not only in exponential regimes where \(P d\) but also when \(P d\), which is not covered by existing analyses.

## 1 Introduction

In sequential decision-making, the bandit framework has been extensively studied and was instrumental to several applications, e.g. A/B testing (Guo et al., 2020), online advertising and recommendation services (Zeng et al., 2016), network routing (Tabei et al., 2023), demand-side management (Bregere et al., 2019), etc. Its popularity stems from its relative simplicity, allowing it to model and analyze a wide range of challenging real-world settings. Reference books like Bubeck and Cesa-Bianchi (2012) or Lattimore and Szepesvari (2020) offer a wide perspective on the subject.

In this framework, a _decision-maker_ or _player_ must make choices and receives associated rewards, but it lacks prior knowledge of its environment. This naturally leads to an exploration-exploitation trade-off: the player must explore different actions to determine the best one, but an inefficient exploration strategy may harm the cumulative rewards. Efficient algorithms rely on exploiting theenvironment's structure, such as estimating the parameters of a reward function instead of exploring every action.

This paper focuses on the stochastic combinatorial semi-bandit framework. At each round, the player chooses a subset of _base items_ and receives a feedback for each item chosen. The action set is included in the base items' power set, and can therefore be exponentially big and difficult to explore. The main challenge in this framework is to effectively combine the information collected through different actions (that may share common base items).

Problem formulation.We consider a set of \(d^{*}\)_base items_, each item \(i[d]=\{1,,d\}\) yielding stochastic rewards. A _player_ accesses these rewards through a set \(\{0,1\}^{d}\) of \(P^{*}\)_actions_, each corresponding to a subset of at most \(m 5\) items(a). We refer to actions \(a\) using their components vector \(a=(a_{i})_{i[d]}\{0,1\}^{d}\) where for all \(j[d]\), \(a_{j}=1\) if and only if action \(a\) contains base item \(j\).

Footnote (a): Throughout the paper, the term _item_ (or _base item_) refers to an element in the set \([d]\), while an _action_ denotes a subset of base items in \(\).

The player interacts with an _environment_ over a sequence of \(T^{*}\)_rounds_. At each round \(t[T]\), the player chooses an action \(A_{t}\), the environment samples a reward vector \(Y_{t}^{d}\), the player observes the realization for every item contained in \(A_{t}\), and receives their sum. The interactions between the player and the environment are summarized in Framework 1.

**Framework 1** Stochastic Combinatorial Semi-Bandit

For each \(t\{1,,T\}\):

* The player chooses an action \(A_{t}\).
* The environment samples a vector of rewards \(Y_{t}^{d}\) from a fixed unknown distribution.
* The player receives the reward \( A_{t},Y_{t}=_{i}A_{i,i}Y_{t,i}\).
* The player observes \(Y_{t,i}\) for all \(i[d]\) s.t. \(A_{t,i}=1\).

Assumptions.We make the following assumptions. For all \(t[T]\), \(Y_{t}\) is independent of the past rewards and the player's decision \((A_{1},Y_{1},,A_{t-1},Y_{t-1},A_{t})\). There exists a mean reward vector \([Y_{t}]=^{d}\) and a second order moment matrix \(=[Y_{t}Y_{t}^{}] M_{d}()\). The positive semi-definite covariance matrix is denoted \( M_{d}()\), with \(=-^{}\).There exists a known _vector_\(B^{d}_{+}\) such that for all \(t[T]\) and \(i[d]\), \(|Y_{t,i}| B_{i}/2\) almost surely (and \(|Y_{t,i}-_{i}| B_{i}\)).

The objective of the decision-maker is to minimize the expected cumulative pseudo-regret defined as:

\[[R_{T}]=[_{t=1}^{T} a^{*}-A_{t}, ]=_{t=1}^{T}_{A_{t}},\] (1)

where \(,\) denotes the usual inner product in \(^{d}\), \(a^{*}_{a} a,\) is an optimal action, and \(_{a}= a^{*}-a,\) is the _sub-optimality gap_ for action \(a\).

### Existing work and limitations

Combinatorial semi-bandit problems have been extensively studied by the bandit community since their introduction by Chen et al. (2013). Here, we only highlight key earlier works related to this paper. For a comprehensive introduction to this literature, we refer the interested reader to the monograph by Lattimore and Szepesvari (2020).

A first line of works considers deterministic algorithms based on the optimistic principle and upper confidence bounds (UCBs). Chen et al. (2013) first designed CUCB, computing UCBs for the items' average rewards, converting these into UCBs for the actions' rewards, and choosing the action with the highest one. It was later analyzed by Kveton et al. (2015), who proved a regret upper bound uniform over all possible covariance matrices \(\) (hence, paying the worst-case). Combes et al. (2015) highlighted the importance of designing \(\)-adaptive algorithms by showing that the regret could be improved by a factor of \(m\) when the items' average rewards are independent. Subsequently, Degenne and Perchet (2016) developed OLS-UCB, an algorithm intended to leverage the covariance structure. However, OLS-UCB requires prior knowledge of a positive semi-definite covariance-proxy matrix \(\), such that for all \(t 1\) and for all \(u^{d}\), \([( u,Y_{t}-)](\|u\|_{}^{2})\). Estimating \(\) in practice is challenging and leads to regret bounds depending on it instead of the "true" covariance matrix \(\), potentially resulting in significantly looser bounds. This issue was addressed by Perrault et al. (2020), who proposed a covariance-adaptive algorithm, \(\), with asymptotically optimal gap-dependent regret upper bounds. Yet, it suffers from an additive constant of order \(_{}^{-2}\), which prevents its conversion into an \(()^{}\) gap-free bound. Thus, none of the above works proposes a \(()\) gap-free and covariance-adaptive regret bound, which is one of the key contributions of this paper. A common drawback of these works is also their potentially prohibitive computational complexity, due to the need to solve a maximization step over a large action set \(\{0,1\}^{d}\) that can be exponentially large. Some works, such as Cuvelier et al. (2021) or Liu et al. (2022), propose solutions to achieve polynomial time complexity, for example by applying UCB at the item level only rather than the action level. However, these approaches only work for independent rewards or under specific assumptions on their distribution, making the analysis for generic and unknown distributions extremely challenging. Another approach to tackle the computational burden in combinatorial semi-bandits is to resort to sampling algorithms, which we detail below.

A second line of works for stochastic combinatorial semi-bandits considers randomized algorithms inspired by Thompson Sampling (TS) for multi-armed bandits (Thompson, 1933). These algorithms involve sampling a random vector \(_{t}^{d}\) at each round \(t+1[T-1]\) from a distribution representing a "belief" over the parameter \(\), taking a decision \(A_{t+1}*{arg\,max}_{a} a,_{t}\), and updating the belief distribution using the observations. The main appeal of these approaches lies in their computational complexity, especially when solving a linear maximization problem in particular action spaces (such as matroids). Recent works have designed and analyzed such algorithms. Notably, Wang and Chen (2018) consider independent item's rewards. Perrault et al. (2020) refine it and assume a known variance-proxy \(\) and therefore suffers from the same drawbacks as Degenne and Perchet (2016). Their technical analysis also yields a gap-dependent regret bound with an undesirable \(_{}^{-m}\) term, preventing a \(()\) gap-free rate. A central contribution in our paper is the combination of the computational efficiency for sampling algorithms with the covariance-adaptivity \(()\) gap-free from our UCB approach.

Besides, the literature concerning our setting has historically mostly focused on cases where the action set is exponentially large, namely \(P d\), and the way to get quasi-optimal regret rates in these instances. However, outside of these regimes, the commonly derived regret bounds are too rough and fail to show the benefit of the semi-bandit feedback. While the conventional stochastic combinatorial semi-bandit regret upper bound grows as \(()\)(Kveton et al., 2015), a \(()\) could be achieved using bandit feedback only (Auer et al., 2002). Intriguingly, the latter appears to outperform the semi-bandit rate as soon as \(P<d\), making the extra information obtained through a richer feedback seemingly useless. Fine-grained analyses, clearly taking the structure into account, are therefore needed.

### Contributions

A new deterministic optimism-based algorithm (Section 2).We present \(\) (Online Least Squares Upper Confidence Bound with Covariance estimation), relying on the optimism principle. The analysis of \(\) sketched in Section 5.2 shows the following properties:

* _First optimal gap-free regret upper bound._\(\) yields a similar gap-dependent regret bound as \(\) from Perrault et al. (2020) up to logarithmic factors, and _the first optimal covariance-adaptative gap-free \(()\)_ regret bound (Theorem 1).
* _Improved performance over UCB in all regimes of \(P/d\)._ Under some conditions on the covariance matrix \(\), we prove that \(\) has a uniformly better regret than \(\), showing that properly leveraging semi-bandit feedback indeed consistently offers an advantage on (simple) bandit algorithms, which is not straightforward from existing analyses.
* _Improved complexity over concurrent algorithms._\(\) circumvents the convex optimization problem that \(\) requires to solve at each round and is therefore more efficient, despite suffering in the very large \(P\) regime as many other deterministic algorithms.

The first stochastic optimism-based algorithm (Section 3).We introduce \(\) (Combinatorial Optimistic Sampling with Variance estimation), a TS-inspired algorithm exploiting the "frequentist" confidence regions derived in Section 4. It satisfies the following:* _Improved complexity for \(P 1\) compared to other deterministic semi-bandit algorithms._CQS-V can be efficient in the very large \(P\) regime, which is the main blind spot of OLS-UCB-C.
* _First gap-free \(()\) regret upper bound for a sampling algorithm._ The analysis we provide in Section 5.3 exploits the common structure of the OLS-UCB-C and COS-V algorithms. It enables the derivation of a gap-dependent bound for COS-V that does not involve the \(^{-m}\) term we typically find in the analysis for other TS algorithms (Wang and Chen, 2018; Perrault et al., 2020), consequently leading to a new \(()\) variance-adaptive gap-free regret upper bound for a sampling algorithm.

A novel gap-free lower bound (Section 2.2).We show a gap-free lower bound on the regret for stochastic combinatorial semi-bandits, explicitly involving the structure of the problem (the items forming each action) and the covariance matrix \(\). This lower bound highlights the optimality of the gap-free upper bound we establish for OLS-UCB-C.

Technical details are deferred to Section 4, Section 5, and the Appendix.

## 2 Covariance-adaptative deterministic algorithm: OLS-UCB-C

In this section, we design a new algorithm that efficiently leverages the semi-bandit feedback. It approximates the coefficients of the covariance matrix \(\) online. The approximation is symmetric by construction and yields a coefficient-wise upper bound of \(\), but it is not necessarily positive semi-definite, a constraint that can be challenging to impose in practice.

### Algorithm: OLS-UCB-C

We present OLS-UCB-C described in Alg. 2 and detail below the successive steps it performs.

Initial exploration.The algorithm first explores by choosing every base item \(i[d]\) and every "reachable" couple \((i,j)[d]^{2}\) at least once.

   Fdbck. & Algorithm & Info. & Time Complexity & Gap-Free Asymptotic Regret \\   & UCB & \(B\) & \(P\) & \(T_{a}(a^{}B)^{2}^{1/2}\) \\  & UCBV & \(\) & \(P\) & \(T_{a}a^{}a^{1/2}\) \\   & CUCB & \(B\) & \(mP\) & \(Tmd^{1/2}\|B\|_{}\) \\  & OLS-UCB\({}^{}\) & \(\) & \(m^{2}+Pd^{2}\) & \(\) \\  & ESCB-C & \(\) & \(m^{2}+P\;C_{1/T}^{}\) & \(\) \\  & OLS-UCB-C & \(\) & \(m^{2}+Pd^{2}\) & \(T_{i}_{a/i a}_{j a}(_{i,j})_{+}^{1/2}\) \\   & CTS-Gaussian\({}^{}\) & \(\) & poly\((d)\) & \(\) \\  & COS-V\({}^{}\) & \(\) & poly\((d)\) & \(Tm_{i[d]}_{i,i}^{1/2}\) \\   

Table 1: Asymptotic \(()\) regret bounds and per-round time complexities up to poly-logarithmic terms in \(d\), for the following deterministic algorithms: UCB (Auer et al., 2002), UCBV (Audibert et al., 2009), CUCB (Kveton et al., 2015), OLS-UCB-C (Degenne and Perchet, 2016), ESCB-C (Perrault et al., 2020), and OLS-UCB-C (ours); as well as the two stochastic algorithms: CTS-Gaussian (Perrault et al., 2020) and COS-V (ours). Notations: \(a\) refers to actions; \(i\) and \(j\) refer to items; \(m\) denotes the maximum number of items per action; \(B\) is a vector of bounds on the items’ rewards; \(\) is a covariance-proxy matrix; \(\) is the maximum of “correlations-proxy”; we abbreviate \(\{x,0\}\) to \((x)_{+}\) for any \(x\) ; \(C_{1/T}^{}\) refers to the complexity of the optimisation step needed in ESCB-C.

Rewards means estimation.At each round \(t+1[T-1]\), the algorithm uses an empirical mean \(_{t}\) for \(\) defined as

\[_{t}=_{t}^{-1}_{s=1}^{t}_{A_{s}}Y_{s},\] (2)

where \(_{a}=(a) M_{d}()\) is the diagonal matrix of the elements in \(a\); \(n_{t,(i,j)}\) is the number of times items \(i\) and \(j\) (with possibly \(i=j\)) have been chosen together; \(_{t}=((n_{t,(i,i)})_{i[d]}) M_{d}()\) is the diagonal matrix of item counts.

Rewards covariances estimation.The covariances are estimated by

\[_{t,(i,j)}=}_{t,(i,j)}-_{t,i}_{t,j}\,,\] (3)

where \(}_{t,(i,j)}=}_{s=1}^{t}A_{s,i}A_{s,j}Y _{s,i}Y_{s,j}\). The algorithm uses \(}_{t}\), a coefficient-wise upper-confidence bound of \(\) whose coefficients are defined for a fixed \(>0\) as

\[}_{t,(i,j)}=_{t,(i,j)}+B_{j}}{4} }{}}+^{2}}{n_{t,(i,j)}}+^{2}}\,,\] (4)

where \(h_{t,}=1+2(1/)+2t(t)^{2}d(d+1)+ (1+t)^{1/2}\).

Optimistic action choice.Following the 'optimistic' principle of UCB-like algorithms, the estimated rewards \((_{t},a)_{a}\) are inflated by bonuses, yielding corresponding upper confidence bounds. The bonuses involve the history through a _regularized empirical design matrix_ (with empirical covariances):

\[}_{t}=_{s=1}^{t}_{A_{s}}}_{t }_{A_{s}}+_{}_{t}}_{t}+\|B \|^{2}\,,\] (5)

where \(_{}_{t}}=(}_{t }) M_{d}()\), \(\) is the identity matrix and \(}_{t}\) is the coefficient-wise upper bound for the covariance matrix defined in (4). Formally, OLS-UCB-C chooses

\[A_{t+1}_{a}\{ a,_{t}+f_ {t,}\|_{t}^{-1}a\|_{}_{t}}\},\] (6)

where \(f_{t,}=6(1/)+6(t)+(d+2)((t))+3d 2(2)+(1+e)\).

Efficiency improvement.While Perrault et al. (2020) use an axis-realignment technique to derive their confidence regions, our approach builds ellipsoidal confidence regions. This simplifies the computation of an upper confidence bound for each action as we have a closed-form expression. In comparison, Perrault et al. (2020) need to solve linear programs in convex sets at each iteration.

### Regret upper bounds

**Theorem 1**.: _Let \(T^{*}\) and \(>0\). Then,_ OLS-UCB-C _(Alg. 2) satisfies the_ gap-dependent _regret upper bound_

\[[R_{T}]=(m)^{2}_{i=1}^{d}_{a /i a,_{a}>0}^{2}}{_{a}}\,,\]

_where \(_{a,i}^{2}=_{j a}\{_{i,j},0\}\), and the gap-free regret upper bound_

\[[R_{T}]=(m)^{d}_{a /i a}_{a,i}^{2}}\,.\]

The proof is outlined in Section 5 and the specific details are presented in Appendix E.

Optimal gap-free bound.This result shows that OLS-UCB-C yields the same gap-dependent regret upper bound as ESCB-C (Perrault et al., 2020) (up to poly-logarithmic factors) and more importantly yields a novel covariance-adaptive and optimal \(O()\) gap-free bound, as shown by the following lower-bound proven in Appendix A. Unfortunately, only the positive coefficients of \(\) are considered in our bound but the inclusion of negative correlations could be advantageous to reduce the rate at which the regret increases. However, it could complicate the analysis greatly and is thus deferred to future research.

**Theorem 2**.: _Let \(d,m^{*}\) such that \(d/m 2\) is an integer, \(T^{*}\), and \( 0\) a covariance matrix. Then, there exists a stochastic combinatorial semi-bandit with \(d\) base items, and a reward distribution with covariance matrix \(\) on which for any policy \(\), the pseudo regret satisfies_

\[[R_{T}]T_{i[d]}_{a, i a}_{j a}_{i,j}^{1/2}.\]

Improvement over \(\).Our gap-free and gap-dependent bounds outperform those of \(\)(Kveton et al., 2015) no matter the covariance structure, as \((Tmd)^{1/2}\|B\|_{}(()T)^{1/2}\)(c) Besides, in the particular case of a diagonal \(\), our gap-free upper bound gains a factor at least \(\) over the one of \(\). In this scenario, \(_{a,i}^{2}=_{i,i}\) for all \(a\) and \(i a\). Our gap-dependent and gap-free upper bounds are then roughly bounded as

Footnote c: We denote \(\) for \(\) up to a constant factor.

\[_{i=1}^{d}_{i,i}}{_{a/i a}_{a} }()T}\,,\]

respectively.

Improvement over \(\).Assuming that \(_{i,j} 0\) for all \(i,j\), our upper-bound uniformly improves the one of \(\) of order \(T_{a}a^{}a^{1/2}\), since in this case \(_{i=1}^{d}_{a i}_{a,i}^{2} _{a}\|a\|_{}\). Existing semi-bandit analyses could only leverage semi-bandit feedback in the regime \(P d\), which is natural in combinatorial bandits but not systematic in real-world applications.

## 3 New sampling algorithm for combinatorial semi-bandits: \(\)

In this section, we introduce a randomized algorithm inspired from \(\), enabling to get potentially computational complexity at the cost of not leveraging off-diagonal covariances.

The difficulty in designing and analysing \(\) algorithms generally stems from controlling the random exploration. To that end, we parametrize the exploration distribution using the same estimators as \(\).

### Algorithm: \(\)

We propose a sampling strategy using "frequentist" estimators, \(\), described in Algorithm 3.

``` Input \(>0\), \(B_{+}^{d}\). for\(t[T]\)do if\(a_{(i,j) a}n_{t,(i,j)}<1}\)then  Choose \(A_{t}\) in the above set. else  Compute \(_{t-1}\) (2).  Compute \((}_{t-1,(i,i)})_{i[d]}\) (4).  Compute \((}_{t-1,(i,i)})_{i[d]}\) from (5).  Sample \(_{t-1}\) from (7)  Choose \(A_{t}*{arg\,max}_{a} a,_{t-1}\).  Environment samples \(Y_{t}^{d}\).  Receive reward \( A_{t},Y_{t}=_{i}A_{t,i}Y_{t,i}\). endif endfor ```

**Algorithm 3**\(\)

The algorithm begins with the same exploration phase as \(\). Thereafter at each round \(t+1[T-1]\), we sample parameters \((_{i,t})_{i[d]}\) using \(1\)-dimensional normal distributions biased toward the positive orthant. Formally, for all \(i[d]\), we sample

\[_{t,i}_{t,i}+(1+g_{t,})f_{t, }}_{t,(i,i)}^{1/2}}{n_{t,(i,i)}},\ f_{t,}^{2}}_{t,(i,i)}}{n_{t,(i,i)}^{2}},\] (7)

where \(g_{t,}=1+22dt(t)^{2}/^{1/2}\) and \(f_{t,}\) is the same as for \(\).

### Regret upper bound

**Theorem 3**.: _Let \(T^{*}\), and \(>0\). Then, \(\) (Alg. 3) satisfies the gap-dependent regret upper bound_

\[[R_{T}]=(m)^{2}_{i=1}^{d}_ {i,i}}{_{i,}}\,,\] (8)

_where \(_{i,}=\{_{a},\ ai a\}\), and the gap-free regret upper bound_

\[[R_{T}]=(m)^{d}_{i,i}}\,.\] (9)

The proof is outlined in Section 5 and the specific details can be found in Appendix F.

Novel variance-dependent bound.Theorem 3 presents the first variance-dependent bound for a sampling-based semi-bandit algorithm. Unfortunately, integrating the covariances \(_{i,j}\) in the leading term is still an open problem. Possible leads include exploring other biasing strategies for sampling, or using oversampling approaches like Abeille and Lazaric (2017) which inflate the confidence regions in the linear bandits setting.

Novel gap-free regret bound.An important novelty of our gap-dependent bound Eq. (16) is the absence of \(_{}^{-m}\) terms present in the previous analyses of CTS (Wang and Chen, 2018; Perrault et al., 2020). In particular, this improvement yields the first \(()\) gap-free regret upper bound for a sampling strategy.

## 4 Mean and covariance estimation

In this section, we present concentration results for \(_{t}\) (rewards means) and \(}_{t}\) (rewards covariances, estimated with \(_{t}\)) used in OLS-UCB-C and COS-V, which are central to prove Theorem 1 and Theorem 3 (sketched in Section 5).

### Covariance-aware confidence region for the average reward

Average reward estimation.Let \(a\), \(t d(d+1)/2\), as introduced in Section 2.1, the least square estimator for the mean reward vector \(\) using all the data gathered after round \(t\) is

\[_{t}=_{t}^{-1}_{s=1}^{t}_{A_{s}}Y_{s}=+ _{t}^{-1}_{s=1}^{t}_{A_{s}}_{s}\,,\]

where the \(_{s}\) denote the deviations \(Y_{s}-\).

Confidence region design.We design confidence regions inspired from LinUCB literature (Russ-mevichientong and Tsitsiklis, 2010; Filippi et al., 2010; Abbasi-Yadkori et al., 2011) and the work of Degenne and Perchet (2016). Major differences with those works include using Bernstein's style concentration inequalities involving the covariance matrix \(\), assuming a multidimensional noise term, and combining them with a covering argument to relax dependence in \(d\) (peeling trick from Degenne and Perchet, 2016). We introduce the _regularized design matrix_ defined by

\[_{t}=_{t}+_{t}+\|B\|^ {2}\,,\]

where \(_{t}=_{s=1}^{t}_{A_{s}}_{A_{s}}\) is the design matrix (of which the OLS-UCB-C and COS-V use an empirical version). Let \(S_{t}=_{t}(_{t}-)\), the deviations of \( a,_{t}\) are bounded as

\[| a,_{t}-|\|_{t}^{-1}a\|_{_ {t}}\ \|S_{t}\|_{_{t}^{-1}}\,.\] (10)

Designing a confidence region for \(\|S_{t}\|_{_{t}^{-1}}\) therefore allows to control the deviations \(| a,_{t}-|\) uniformly on \(\). Let \(>0\), we define the event

\[_{t}=\ \|S_{t}\|_{_{t}^{-1}} f_{t,} }\,,\] (11)

with \(f_{t,}=6(1/)+6[(t)+(d+2)((t))]+3d[2(2)+(1+ e)]\).

This event can also be written \(_{t}=\ \|_{t}-\|_{_{t}_{t}^{-1} _{t}} f_{t,}}\) and is therefore equivalent to \(_{t}\) belonging to an ellipsoid around the true reward mean vector \(\).

Confidence region probability.The following result proven in Appendix B presents an upper bound for \((_{t}^{c})\).

**Proposition 1**.: _Let \(t d(d+1)/2\) and \(>0\). Then, \((_{t}^{c})/(t(t)^{2})\)._

Proving this result relies on an argument adapted from Faury et al. (2020) and a covering trick from Degenne and Perchet (2016).

### Confidence interval for covariances estimator

Rewards covariances estimator.Let \(t d(d+1)/2\) and a "reachable" couple \((i,j)[d]^{2}\). The coefficients of \(\) can be estimated online by \(_{t}\) as introduced in Section 2.1

\[_{t,(i,j)}=}_{t,(i,j)}-_{t,i}_{t,j}\,.\]

Rewards covariances upper confidence bound.Let \(>0\). We use the following coefficient-wise upper estimates of \(\) in our algorithms

\[}_{t,(i,j)}=_{t,(i,j)}+B_{j}}{4} }{}}+^{2}}{n_{t,(i,j)}} +^{2}}\,,\]

with \(h_{t,}=1+2(1/)+2t(t)^{2}d(d+1)+ (1+t)^{1/2}\).

Favorable event design.We define \(_{t}\) as the event where all the coefficients of \(}_{t}\) are indeed upper bounding those of \(\):

\[_{t}=(i,j)[d]^{2},\ }_{t,(i,j)}_{i,j}}\,.\] (12)

Favorable event probability.The following result proven in Appendix C presents an upper bound for \((_{t}^{c})\).

**Proposition 2**.: _Let \(t d(d+1)/2\) and \(>0\). Then, \((_{t}^{c})/(t(t)^{2})\)._

## 5 Regret upper bounds

In this section, we provide a sketch of the proof for Theorem 1 and Theorem 3. For both OLS-UCB-C and COS-V, the idea to bound the regret is to find a sequence of _favorable events_\((_{t})_{t d(d+1)/2}\) that are true with high probability, and under which the regret grows logarithmically with time.

### Template bound

Let \((_{t})_{t[T]}\) be a sequence of events, then for both OLS-UCB-C and COS-V standard derivations yield

\[[R_{T}]_{}d(d+1)/2+_{t=d(d+1)/2}^{T-1} (_{t}^{c})+_{t=d(d+1)/2}^{T- 1}\{_{t}\}_{A_{t+1}}\,.\] (13)

Assuming that the sequence of events \((_{t})_{t d(d+1)/2}\) happens with high enough probability, it is sufficient to control what happens conditionally to it. In particular, Proposition 6 in Appendix D states that if we can bound \(_{A_{t+1}}^{2}\) with a linear combination of terms evolving as \(n_{t,(i,j)}^{-k}\) for every couple \((i,j) A_{t+1}\) and different \(k 1\), then we can infer a worst-case behaviour, which yields Theorem 1 and Theorem 3.

In the following, we will refer to the term \(_{t=d(d+1)/2}^{T-1}(_{t}^{c})\) as the _unfavorable event probability_ and to the term \(_{t=d(d+1)/2}^{T}\{_{t}\}_{A_{ t+1}}\) as the _high-probability regret_.

### Regret of OLS-UCB-C

For OLS-UCB-C we consider the sequence of events \(_{t}=\{_{t}_{t}\}\) for all \(t d(d+1)/2\), corresponding the confidence regions of \((_{t})_{t d(d+1)/2}\) and of \((}_{t,(i,j)})_{t d(d+1)/2,\ (i,j)[d]^{2}}\) defined in Section 4. Under these events, we can upper-bound the high-probability regret from Eq. (13) with the following proposition (proven in Appendix E.1).

**Proposition 3**.: _Let \(>0\). Then, \(\) yields_

\[_{t=d(d+1)/2}^{T-1}_{A_{t+1}} _{t}_{t}}=O(T)^{2}(m)^ {2}_{i=1}^{d}_{a/i a}^{2}}{_{ a}}\,,\]

_as \(T\), where \(_{a,i}^{2}=_{j a}(_{i,j})_{+}\)._

Conclusion of the proof.Injecting results from Proposition3 (high-probability regret) as well as Proposition1 and Proposition2 (unfavorable event probability) into the template bound (13), we get

\[[R_{T}]=O(T)^{2}(m)^{2}_{i[d]}_{a /i a}^{2}}{_{a}}\,,\] (14)

for \(\) as \(T\). This provides the gap-dependent bound of Theorem1. The gap-free bound is detailed in AppendixE.4. It is enabled by the fact that our gap-dependent bound does not incur any term in \(_{}^{-2}\), unlike Perrault et al. (2020); Degenne and Perchet (2016).

### Regret of \(\)

For the analysis of our stochastic algorithm \(\), we need to consider events related to the sampling distributions in addition to the events \(^{}_{t}\) and \(_{t}\) introduced in the precedent section. For this purpose, we denote the event \(_{t}\) defined as

\[_{t}=\{ i[d],\,|(_{t,i}+(1+g_{t, })f_{t,}\,}_{t,i})^{1/2}}{n_{t,i}})- _{t,i}| g_{t,}f_{t}}_{t,i}) ^{1/2}}{n_{t,i}}\}.\] (15)

The high-level idea of the event \(_{t}\) is to ensure that the sampled rewards \(_{t,i}\) upper-bound the true mean \(_{i}\) while not being too far for all the items \(i a^{*}\). Showing that the event \(_{t}\) indeed occurs with high-probability (Lemma7 in AppendixF) and setting the events \(_{t}=\{_{t}_{t}_{t}\}\), we can upper-bound the high-probability regret in the following proposition (proof is in AppendixF.2).

**Proposition 4**.: _Let \(>0\). Then \(\) yields_

\[_{t=d(d+1)}^{T-1}_{A_{t+1}} _{t}_{t}_{t}}=O (T)^{3}(m)^{2}_{i=1}^{d}_{i,i}}{_{i,}}\,.\]

Conclusion of the proof.Injecting results from Proposition4 (high-probability regret) as well as Lemma7, Proposition1 and Proposition2 (unfavorable event probability) into the template bound (13) yields

\[[R_{T}]=O(T)^{3}(m)^{2}_{i[d]}_{i,i}}{_{i,}}\,,\] (16)

as \(T\). This provides the gap-dependent bound of Theorem3. As it does not incur any term in \(_{}^{-m}\) as in Wang and Chen (2018); Perrault et al. (2020), this result can be used to derive a \(()\) gap-free bound for a sampling-based combinatorial semi-bandit algorithm.

## 6 Concluding remarks

We propose and analyze two algorithms for combinatorial semi-bandits. \(\) is a deterministic, covariance-adaptive algorithm. Compared to other existing approaches, our algorithm is typically less computationally demanding and yields the first \(()\) gap-free regret rate that explicitly depends on the covariance of the base item rewards and the structure. \(\) is a variance-adaptive, \(\)-like algorithm. Its complexity is significantly lower under certain types of constraints, but its regret is suboptimal as it assumes worst-case correlations. However, leveraging the analysis of \(\), it also yields the first \(()\) gap-free regret upper bound among sampling-based approaches.