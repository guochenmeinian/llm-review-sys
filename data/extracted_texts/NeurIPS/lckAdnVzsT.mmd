# Coherent 3D Scene Diffusion

From a Single RGB Image

 Manuel Dahnert\({}^{1}\)  Angela Dai\({}^{1}\)  Norman Muller\({}^{2}\)  Matthias Niessner\({}^{1}\)

\({}^{1}\)Technical University of Munich, Germany \({}^{2}\)Meta Reality Labs Zurich, Switzerland

###### Abstract

We present a novel diffusion-based approach for coherent 3D scene reconstruction from a single RGB image. Our method utilizes an image-conditioned 3D scene diffusion model to simultaneously denoise the 3D poses and geometries of all objects within the scene. Motivated by the ill-posed nature of the task and to obtain consistent scene reconstruction results, we learn a generative scene prior by conditioning on all scene objects simultaneously to capture the scene context and by allowing the model to learn inter-object relationships throughout the diffusion process. We further propose an efficient surface alignment loss to facilitate training even in the absence of full ground-truth annotation, which is common in publicly available datasets. This loss leverages an expressive shape representation, which enables direct point sampling from intermediate shape predictions. By framing the task of single RGB image 3D scene reconstruction as a conditional diffusion process, our approach surpasses current state-of-the-art methods, achieving a 12.04% improvement in AP\({}_{}\) on SUN RGB-D and a 13.43% increase in F-Score on Pix3D.

## 1 Introduction

Holistic 3D scene understanding is crucial for various fields and lays the foundation for many downstream tasks in robotics, 3D content creation, and mixed reality. It bridges the gap between 2D perception and 3D understanding. Despite impressive advancements in 2D perception and 3D reconstruction of individual objects [56; 5; 12; 38], 3D scene reconstruction from a single RGB observation remains a challenging problem due to its ill-posed nature, heavy occlusions, and the

Figure 1: Given a single RGB image of an indoor scene, our model reconstructs the 3D scene by jointly estimating object arrangements and shapes in a globally consistent manner. Our novel diffusion-based 3D scene reconstruction approach achieves highly accurate predictions by utilizing a novel generative scene prior that captures scene context and inter-object relationships, and by employing an efficient surface alignment loss formulation for joint pose- and shape-synthesis.

complex multi-object arrangements found in real-world environments. While previous works [15; 32; 33] have shown promising results, they often recover 3D shapes independently and thus do not leverage the scene context nor inter-object relationships. This leads to unrealistic and intersecting object arrangements. Additionally, common feed-forward reconstruction methods [48; 77; 37] struggle with heavy occlusions and weak shape priors, resulting in noisy or incomplete 3D shapes, which hinders immersion and hence limits the applicability in downstream tasks. To address these challenges and to advance 3D scene understanding, we propose a novel generative approach for coherent 3D scene reconstruction from a single RGB image. Specifically, we introduce a new diffusion model that learns a generative scene prior capturing the relationships between objects in terms of arrangement and shapes. When conditioned on a single image, this model simultaneously reconstructs poses and 3D geometries of all scene objects. By framing the reconstruction task as a conditional synthesis process, we achieve significantly more accurate object poses and sharper geometries. Publicly available 3D datasets [47; 62] typically only provide partial ground-truth annotations, which complicates joint training of shape and pose. To overcome this, we propose a novel and efficient surface alignment loss formulation \(_{}\) that enables joint training of shape and pose even under the lack of full ground-truth supervision. Unlike previous methods [48; 77] that involve costly shape decoding and point sampling on the reconstructed surface, our approach employs an expressive intermediate shape representation that enables direct point sampling from the conditional shape prior. This provides additional supervision and results in more globally consistent 3D scene reconstructions. Our method not only outperforms current state-of-the-art methods by 12.04% in AP\({}_{}^{15}\) on SUN RGB-D  and by 13.43% in F-Score on Pix3D  but also generalizes to other indoor datasets without further fine-tuning.

In summary, our contributions include:

* A novel diffusion-based 3D scene reconstruction approach that jointly predicts poses and shapes of all visible objects within a scene.
* A novel way for modeling a generative scene prior by conditioning on all scene objects simultaneously to capture scene context and inter-object relationships.
* An efficient surface alignment loss formulation \(_{}\) that leverages an expressive intermediate shape representation for additional supervision, even in the absence of full ground-truth annotation.

## 2 Related Works

The task of 3D scene reconstruction from a single view combines the fundamental domains of 2D perception and 3D modeling into a unified challenge of holistic 3D understanding. Given the multi-faceted nature of the task, we are providing a comprehensive overview of the relevant research directions and contextualizing our contributions.

### Single-View 3D Reconstruction

Object Reconstruction.Since the foundational work by Roberts , numerous methods have been developed to learn cues for deriving 3D object structures, thereby bridging the gap between 2D perception and the 3D world. These methods typically involve an image encoder network that processes the input image of a single object, capturing its features. The extracted features are either correlated with an encoded shape database to retrieve a suitable shape [32; 33; 17], or used by a 3D decoder to reconstruct the object in a specific 3D representation, such as voxel grids [8; 72], point clouds [14; 43], meshes [70; 66], or neural fields [73; 27].  uses a message-passing graph network between geometric primitives to reason about the structure of the shape.

Scene Reconstruction.Early works formulated single-view scene reconstruction as 3D scene completion from given or estimated depth information [63; 10; 78; 9] in a volumetric grid. While these methods have produced promising results, their representational power to model fine details is limited by the spatial resolution of the 3D grid. Multi-object reconstruction and scene parsing methods represented objects using primitives [13; 23], voxel grids [68; 35; 52], or CAD models [26; 24], while also considering the relation between the objects . The approach presented by Nie _et al_.  is particularly relevant, proposing a holistic method for joint pose and shape estimation from a single image. Zhang _et al_.  extended this idea by incorporating an implicit shape representation and an additional pose refinement using a graph neural network. Although these methods provided significant advances in holistic scene understanding, they struggled with accurate pose estimation and produced noisy scene objects, leading to intersecting or incomplete objects. In contrast to these previous works, we are proposing a generative method to obtain a strong scene prior and formulate the reconstruction task as a conditional synthesis task. This allows for more robust reconstruction that is less prone to object insections or implausible object geometries.

### 3D Diffusion Models

In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as a versatile class of generative models, demonstrating impressive results in image and video generation. Unlike other classes of generative models such as auto-regressive models [46; 75; 59], Generative Adversarial Networks (GANs) [71; 79] and Variational Autoencoders (VAEs), diffusion models iteratively reverse a Markovian noising process. This method ensures stable training and has the ability to capture diverse modes while producing detailed outputs. Several approaches have utilized diffusion models to learn the distribution of individual 3D shapes using various 3D representations, including volumetric grids [6; 7; 25], point clouds [42; 74], meshes , implicit functions , neural fields [45; 58; 29] or hybrid representations [80; 76].  propose a hierarchical voxel diffusion model, which is capable of modelling large-scale and fine-detailed geometry. While these methods can synthesize high-quality 3D shapes, they typically focus on single objects in canonical space. In contrast, we are proposing a diffusion-based approach that addresses the more challenging problem of multi-object scene reconstruction, encompassing accurate pose estimations and an understanding of inter-object relationships.

Conditional Diffusion for 3D Reconstruction.Recent works also use diffusion models for single-view object reconstruction [6; 7; 44]. For instance,  learns the shape distribution of a single category by denoising a set of 2D images for each object, while  projects image features onto noisy point clouds during the diffusion process to ensure geometric plausibility. Recently, several works proposed to leverage multi-view consistency within pre-trained text-conditional 2D image diffusion models to reconstruct individual 3D objects [38; 51; 57]. Similar to our work, Tang _et al._ use a diffusion model to learn scene priors from synthetic data, showing unconditional scene synthesis of a single room type and text-conditional generation. However, their approach does not support image-based scene reconstruction. Furthermore, it depends on clean synthetic data, which provides full 3D ground truth supervision and CAD model retrieval, thereby limiting shape diversity. While these existing methods have shown promising results on single objects or synthetic scenes, our approach targets real-world scenes. By framing the reconstruction task as a conditional generation process, our scene prior accurately delivers poses and shapes of multiple objects, even in the presence of strong occlusions, significant clutter, and challenging lighting conditions.

## 3 Method

### Overview

Our method takes a single RGB image of an indoor scene as input and generates a globally consistent 3D scene reconstruction that matches the input image. To this end, we are framing the reconstruction task as a conditional generation problem using a diffusion model conditioned on the input view (Sec. 3.2), which simultaneously predicts the poses (Sec. 3.3) and shapes (Sec. 3.4) of all objects in the scene. Given the ill-posed nature of single-view reconstruction, such a probabilistic formulation is particularly well-suited for this task. To ensure accurate reconstructions and to learn a strong scene prior, we model inter-object relationships within the scene using an intra-scene attention module (Sec. 3.5). Additionally, recognizing the incomplete ground truth in many 3D indoor scene datasets, we introduce a loss formulation for joint shape and pose training, which enables training under only partially available supervision (Sec. 3.6). An overview of our approach is illustrated in Fig. 1. In the following sections, we describe each individual contribution in more detail.

### Conditional 3D Scene Diffusion

We frame the scene reconstruction task as a conditional generation process via a diffusion formulation . Given an instance-segmented RGB image \(\) containing a variable number of 2D objectsfor \(i\{1,,n\}\), our model \(\) simultaneously estimates all 3D objects \(}=(_{i},_{i})\) with 7-DoF poses \(_{}\) and 3D geometries \(_{}\):

\[(}_{1},,}_{n})=(|( _{1},,_{n})).\] (1)

During the _forward process_, we gradually add Gaussian noise to a data point \(x_{0}\) to \(x_{T}\) over a series of discrete time steps \(T\). For a given data point \(x_{0}\), _e.g._, shapes \(_{i}\) and poses \(_{i}\), the noisy version \(x_{t}\) at time step \(t\) is given by a Markovian process \(q(x_{t}|x_{t-1})\) and its joint distribution \(q(x_{1:T}|x_{0})\) can be expressed as:

\[q(x_{t}|x_{t-1}) =(x_{t};}x_{t-1},_{t}),\] (2) \[q(x_{1:T}|x_{0}) =_{i=1}^{T}q(x_{t}|x_{t-1})\] (3)

with \(t[1,T]\) and \(_{t}\) a pre-defined linear variance schedule.

During the _reverse process_, the denoising network \(\) tries to remove the noise and recover \(x_{0}\) from \(x_{T}\) as \(p_{}(x_{t-1}|x_{t},y)\)

\[p_{}(x_{t-1}|x_{t},y) =(x_{t-1};_{}(x_{t},t,y),_{}(x_{t},t,y )),\] (4) \[p_{}(x_{0:T}|y) =p_{}(x_{T})_{t=1}^{T}p_{}(x_{t-1}|x_{t},y)\] (5)

with \(y\) being the conditional information from the input image \(\).

Conditioning.To effectively guide the diffusion process \(p_{}(x_{0:T}|y)\), it is crucial to accurately model the conditional information \(y\). First, we encode the input image \(\) using a 2D backbone \(_{I}\) and apply 2D instance segmentation to get \(n\) detected 2D objects \(b_{i}\), comprising of its 2D bounding box, image feature patch, and semantic class (cls). Each element is encoded using a specific embedding function \(\). The per-instance \(y_{i}\) and scene condition \(y\) is then formed as:

\[y_{i} =(_{}(_{i}),_{ {feat}}(_{i}),_{}(_{i})),\] (6) \[y =(y_{1},,y_{n}).\] (7)

To learn a scene prior over all objects in the scene, we condition the denoising network on the scene condition \(y\). This not only enables learning the individual object representations \(o_{i}\) but also facilitates learning to capture the scene context and inter-object relationships (Sec. 3.5). Furthermore, we adopt classifier-free guidance  for our model by dropping the condition \(y\) with probability \(p=0.8\), _i.e._, using a special 0-condition \(\). This allows our model to function as a conditional model \(p_{}(x_{0}|y)\) and unconditional model \(p_{}(x_{0})\) at the same time, thus enabling unconditional synthesis (Appendix B).

Figure 2: **Scene Prior and Surface Alignment Loss Overview.** (Left) We propose a novel way to model scene priors (Sec. 3.5) by modeling the scene context and the relationships between all objects during the denoising process. (Right) For additional supervision and joint training, we use a surface alignment loss (Sec. 3.6) between a given ground truth depth map and point samples directly drawn from the intermediate shape representation \(_{i}\) and transformed to camera space with the predicted object pose \(_{i}\).

Loss Formulation.Unlike related works like [23; 48; 77] that regress object poses \(_{}\) and shape parameters \(_{}\) using a multitude of highly-tuned losses, we train our model \(\) to minimize simple diffusion and alignment losses:

\[_{}() =_{}()+_{ }()+_{},\] (8) \[_{}() =_{(0,1),}\|_{ }((t),t,,)-\|,\] (9) \[_{}() =_{(0,1),}\|_{ }((t),t,,)-\|,\] (10)

where we define \((t)=_{t}}z+_{t}}\) for \(z\{,\}\) with pre-defined noise coefficients \(_{t}\), while \(_{z}\) denotes the predicted noise. We use \(=0.01\) to balances the effect of \(_{}\).

Due to the lack of full ground truth supervision in publically available 3D datasets, we introduce an additional alignment loss \(_{}\) for joint training of pose and shape (Sec. 3.6). Depending on the availability of ground-truth data (see Sec. 4.2, we mask out individual losses.

### Object Pose Parameterization

We adopt the object pose parameterization of , defining the pose \(_{i}=(c_{i},s_{i},_{i})\) of an object by its 3D center \(c_{i}^{3}\), the spatial size \(s_{i}^{3}\), and orientation \(_{i}[-,)\) in. The 3D center \(c_{i}\) is further represented by the 2D offset \(_{i}^{2}\) between the 2D bounding box center coordinate and the projected coordinate of the 3D center on the image plane, along with the distance \(d_{i}\) from the object center to the projected center. Our model learns to denoise this \(7\)-dim. pose representation.

### Shape Encoding

We represent object shapes using the disentangled shape representation from . A shape is represented as a shape code \(_{i}^{256}\) which is factorized into a set of \(g\) oriented, anisotropic 3D Gaussians \(G_{j},j\{1,...,g\}\) and an associated \(512\)-dim. latent feature vector per Gaussian. Each Gaussian consist of 16 main parameters: \(_{j}^{3}\) (center), factorized covariance matrix \(U_{j}^{3 3}\) (rotation), \(_{j}^{3}\) (scale) and \(_{j}^{1}\) ("mixing" weight). We use \(g=16\) Gaussians to form a scaffolding of the shape's geometry. Together with their latent features, these Gaussians are decoded into high-fidelity occupancy fields, and the final mesh is extracted by applying marching cubes .

While similar to , our model learns to denoise this shape parameterization \(_{i}\), our additional surface alignment loss \(_{}\) (Sec. 3.6) provides relational signal between predicted shapes and poses. This enables additional guidance in the face of missing joint pose and shape annotations as in SUN RGB-D dataset .

### Scene Prior Modeling

Given the ill-posed nature of single-view reconstruction, a robust scene prior is essential for achieving good performance. Effectively capturing the scene context and modeling the relationships between objects within the scene is crucial for learning this strong scene prior [31; 77]. Previous methods either reconstruct each object individually  or refine their features using graph networks . In contrast, our approach considers the entire scene by conditioning on all scene objects simultaneously \(p_{}(x_{0}|y)\) and \(y=(y_{1},,y_{N})\) and additionally allows objects to exchange relational information throughout the entire process. We model the inter-object relationships using an attention formulation , which has proven to be powerful for aggregating contextual information.

We denote this formulation as Intra-Scene Attention (ISA), which allows all objects within the scene to attend to each other, effectively modeling their relationships. Please refer to Appendix E for more details and to Tab. 2 for the corresponding ablation study, which demonstrates the effectiveness of our learned scene prior.

### Surface Alignment Loss

Publically available 3D scene datasets often only provide partial ground-truth annotations [47; 62]. To facilitate joint training of our model on pose and shape estimation, even in the absence of complete ground-truth annotations, we propose to leverage our expressive intermediate shape representation to provide additional supervision and to align shapes efficiently with the available partial depth information \(\). An illustration of the surface alignment loss formulation is provided in Fig. 2.

During training, for each object \(o_{i}\), we use the expected shape code \(_{i}\) estimation by our model to obtain the predicted Gaussian \(_{i,j}\) distribution. Given this scaffolding representation, we directly sample \(m=1000\) points \(p_{(j,l)}(_{j},_{j})\) per Gaussian \(_{i,j}\) resulting in a shape point cloud \(P_{i}=\{p_{(j,l)}|j\{1,,g\},l\{1,,m\}\}\). We transform the resulting shape points \(P_{i}\) into the camera frame by the predicted object pose \(_{i}\). Using the instance segmentations and ground-truth depth maps, we obtain \(K_{i}\) surface points \(q_{k}^{i}\) for object \(o_{i}\) and define the surface alignment loss for all visible objects as 1-sided Chamfer Distance [16; 48]

\[_{}=_{i=1}^{n}}_{k=1} ^{K_{i}}_{p P_{i}} q_{k}^{i}-p_{2}^{2}.\] (11)

Unlike previous works such as  that perform costly sampling of points on the decoded shape surface, our approach enables direct point sampling from the conditional shape prior \(_{i,j}\). This loss formulation facilitates joint training of pose and shape for all objects simultaneously and its efficacy is demonstrated through ablation studies in Tab. 2.

### Architecture

Our architecture consists of a pre-trained image backbone, a novel image-conditional scene prior diffusion model, and a conditional shape decoder diffusion module. We utilize an off-the-shelf 2D instance segmentation model, Mask2Former , which is pre-trained on COCO  using a Swin Transformer  backbone, to obtain instance segmentation and image features. Please refer to Appendix E for details about the condition embedding functions.

To denoise object poses \(_{i}\), we use a 1-dim. UNet  architecture with 8 encoding and decoding blocks with skip connections. Each block consists of a time-conditional ResNet  layer, multi-head attention between the per-object condition \(y_{i}\) and the pose representation, and our intra-scene attention module (Sec. 3.5) to enable relational information exchange and effectively train a scene prior. We use 8 attention heads, with 64 features per head.

To estimate object shapes \(_{i}\) from the input view \(\), we denoise the unordered set of Gaussian \(G_{i,j}\) using a Transformer  model with 2 encoder layers, 6 decoder layers, and multi-head attention with 4 heads to the object condition information, similar to . The per-Gaussian latent features are denoise with a shape decoder diffusion model, realized as another Transformer model with 6 encoder and decoder layers, which is conditioned on the shape Gaussians.

### Training and Implementation Details

For all diffusion training processes, we uniformly sample time steps \(t=1,...T,T=1000\), and use a linear variance schedule with \(_{1}=0.0001\) and \(_{T}=0.02\). We implement our model in PyTorch and use the AdamW  optimizer with a learning rate of \(1 10^{-4}\) and \(_{1}=0.9,_{2}=0.999\). We train our models on a single RTX3090 with 24GB VRAM for 1000 epochs on Pix3D, for 500 epochs on SUN RGB-D and for 50 epochs of additional joint training using \(_{}\).

During inference, we employ DDIM  with 100 steps to accelerate sampling speed. For classifier-free guidance , we drop the condition \(y\) with probability \(p=0.8\).

## 4 Experiments

In the following sections, we will demonstrate the advantages of our method and contributions by evaluating it against common 3D scene reconstruction benchmarks.

### Baseline Methods

We compare our method against current state-of-the-art methods for holistic scene understanding: Total3D , Im3D , and InstPIFu . Total3D  directly regresses 3D object poses from image features and uses a mesh deformation and edge-removal approach  to reconstruct a shape. Im3D  utilizes an implicit shape representation and a graph neural network to refine the pose predictions. InstPIFu  focuses on single-object reconstruction and proposes to query instance-aligned features from the input image in their implicit shape decoder to handle occlusion. For scene reconstruction, they rely on the predicted 3D poses of Im3D. We use the official code and checkpoints provided by the authors of these baseline methods and evaluate with ground truth 2D instance segmentation and camera parameters to ensure a fair comparison. We further compare against a retrieval-based method, ROCA  in Appendix D.

### Datasets

Following [23; 48; 77], we train and evaluate the performance of our 3D pose estimation on the SUN RGB-D  dataset with the official splits. This dataset consists of 10,335 images of indoor scenes (offices, hotel rooms, lobbies, furniture stores, etc.) captured with four different RGB-D cameras. Each image is annotated with 2D and 3D bounding boxes of objects in the scene. During joint training, we use the provided depth maps together with instance masks to compute \(_{}\).

We train and evaluate the performance of our 3D shape reconstruction on the Pix3D  dataset, which contains images of common furniture objects with pixel-aligned 3D shapes from 9 object classes, comprising 10,046 images. We use the train and test splits defined in , ensuring that 3D models between the respective splits do not overlap.

### Evaluation Protocol

For quantitative comparison against baseline methods, we follow the evaluation protocol of . For pose estimation, we report the intersection over union of the 3D bounding box (IoU\({}_{}\)) and average precision with an IoU\({}_{}\) threshold of 15% (AP\({}_{}^{15}\)) on the SUN RGB-D dataset . In line with previous works [48; 77], we evaluate with oracle 2D detections but also provide camera parameters to all methods during evaluation. To further assess the alignment of the 3D shapes in the scene, we calculate \(_{}\) between reconstructed shapes and the instance-segmented ground-truth depth map.

For single-view 3D shape reconstruction, we follow evaluate on the Pix3D  dataset. We follow  and sample 10,000 points on the predicted shape surface, extracted with Marching Cubes  at a resolution of \(128^{3}\), and on the ground truth shapes and evaluate Chamfer distance (CD \( 10^{3}\)) and F-score after mesh alignment.

### Comparison to State of the Art

3D Scene Reconstruction.In Fig. 3, we present qualitative comparisons of our approach against state-of-the-art methods for single-view 3D scene reconstruction. The results from Total3D often exhibit intersecting objects and lack global structure. Additionally, their deformation and edge-removal approach results in 3D shapes with visible artifacts and limited details. While the implicit shape representation of Im3D is more flexible, it often produces incomplete and floating surfaces. In contrast, our diffusion-based reconstruction method, as shown in Tab. 1, learns strong scene priors, resulting in a +0.2 improvement in \(_{}\) and more coherent 3D arrangements of the objects in the scene (+12.04% AP\({}_{}^{15}\)), as well as high-quality and clean shapes (+13.43% F-Score).

Furthermore, we demonstrate the generalizability of our model to other indoor datasets. We evaluate our approach on individual frames from the ScanNet  dataset using 2D instance predictions from Mask2Former without additional fine-tuning. As shown in Fig. 4, our method accurately reconstructs the given input view with matching poses and high-quality 3D geometries.

In Appendix D, we additionally train on ScanNet and compare against ROCA . Due to its retrieval approach, the shapes are complete. However, the resulting quality can limited by the diversity of the shape database, which can lead to suboptimal results, see Fig. 11.

3D Pose Estimation & Scene Arrangement.As shown in Tabs. 1 and 6, our method outperforms all baseline methods by a significant margin in terms of IoU\({}_{}\) and AP\({}_{}^{15}\), _i.e_., improving mAP\({}_{}^{15}\) by 12.04% over Im3D . Detailed per-class results are provided in Tabs. 6 and 8. Figs. 3 and 7 demonstrate that our approach effectively learns common object arrangements, such as multiple chairs surrounding a table, while ensuring that furniture pieces do not intersect or float in the air. We attribute these improvements to our model's robust scene understanding, which is derived from learning a strong scene prior that accounts for inter-object relationships.

3D Object Reconstruction. In Tab. 1, we quantitatively compare the single-view shape reconstruction performance of our approach against baseline methods on the Pix3D dataset. The results demonstrate that modeling single-view reconstruction as conditional generation over a robust shape prior leads to significant improvements in Chamfer Distance (+9.6%) and F-Score (+13.43%). Detailed per-class results can be found in Tabs. 7 and 9. Fig. 9 illustrates that InstPiFU often reconstructs noisy and incomplete shapes. In contrast, our approach produces clean 3D geometries with fine details, such as thin chair legs and the crease between pillows of a sofa.

In Fig. 5, we show unconditional results by injecting \(\) as a condition (Sec. 3.2), showcasing that our shape prior models detailed and diverse shape modes across several semantic classes. In Fig. 10, we additionally visualize the shape decomposition capabilities resulting from our shape encoding and the scaffolding Gaussian representation.

### Ablations Studies

We conduct a series of detailed ablation studies to verify the effectiveness of our design decisions and contributions. The quantitative results are provided in Tab. 2.

What is the effect of the denoising formulation? To assess the benefits of the denoising diffusion formulation, we construct a 1-step feed-forward regression model that uses the same conditional information as input features and model architecture but regresses the object outputs directly in a single timestep. As shown in Tab. 2, modeling 3D scene reconstruction as a conditional diffusion process, rather than using a feed-forward regression formulation, results in significant improvements of \(+11.08\)% AP\({}_{}^{15}\) and \(+0.19\)\(_{}\).

What is the effect of our scene prior modeling? We evaluate the impact of learning a scene prior by modeling the distribution of all objects and their relationships compared to learning the marginal per-object distribution, _i.e_., predicting each object individually. As shown in Tab. 2, our joint-object scene prior yields a significant improvement of \(+9.30\)% AP\({}_{}^{15}\) over per-object prediction. This improvement underscores the importance of learning a robust scene prior that effectively captures inter-object relationships.

What is the effect of joint training? We investigate the benefit of joint training for pose and shape using \(_{}\) compared to individual training of pose estimation and shape reconstruction. Although our model already learns strong scene and shape priors, Tab. 2 shows that joint training provides additional benefits, resulting in an improvement of \(+2.11\)% in AP\({}_{}^{15}\) and \(+0.07\) in \(_{}\).

    &  &  \\  &  & _{}^{15}\)**} & _{}\)**} &  &  \\  Total3D  & \(20.52\) & (-15.85) & \(30.56\) & (-27.62) & \(1.35\) & (-0.36) & \(44.32\) & (-29.27) & \(36.20\) & (-22.51) \\ Im3D  & \(28.31\) & (-7.79) & \(46.14\) & (-12.04) & \(1.24\) & (-0.25) & \(51.31\) & (-36.26) & \(21.45\) & (-37.26) \\ InstPIFu  & \(26.14\) & (-9.96) & \(45.02\) & (-13.16) & \(1.19\) & (-0.20) & \(24.65\) & (-9.6) & 45.28 & (-13.43) \\  Ours & **36.10** & **58.18** & **0.99** & & **15.05** & & **58.71** \\   

Table 1: **Quantitative evaluation of 3D scene reconstruction on SUN RGB-D  (left) and 3D shape reconstruction on Pix3D  (right). Our 3D scene diffusion approach outperforms all baseline methods on both tasks on common 3D scene reconstruction metrics.**

  
**Diffusion** & **ISA** & **Joint** & **IoU3D \(\)** & **AP\({}_{}^{15}\)** & \(_{}\) \\  ✗ & ✓ & ✗ & 28.98 & (-1.2) & 47.10 & (-11.08) & 1.18 & (-0.19) \\ ✓ & ✗ & ✗ & 28.82 & (-7.28) & 48.88 & (-9.30) & 1.12 & (-0.13) \\ ✓ & ✓ & ✗ & 35.16 & (-0.94) & 56.07 & (-2.11) & 1.06 & (-0.07) \\  ✓ & ✓ & ✓ & **36.10** & & **58.18** & & **0.99** \\   

Table 2: **Ablations. We ablate the effect of our contributions and design decisions. We observe significant gains by introducing our proposed scene prior and intra-scene attention module, using denoising diffusion compared to regression, and jointly training shape and pose together.**

### Limitations

While our conditional scene diffusion approach for single-view 3D scene reconstruction demonstrates significant improvements, there are some limitations. First, our method relies on accurate 2D object detection, making it dependent on the performance of 2D perception models. Upcoming state-of-the-art 2D detection models  can be seamlessly integrated to enhance the performance of our approach. Second, our shape prior, trained on a diverse set of semantic classes using 3D shape supervision, does not generalize to unseen object categories. This can be mitigated by combining our model for known categories with single-object diffusion models that leverage pre-trained text-image generation models for 3D shape synthesis  of uncommon shape categories. While accurate 3D scene reconstruction

Figure 4: **Inference results on ScanNet . We use our model trained on SUN RGB-D  and perform inference on individual frames of ScanNet without fine-tuning. We observe strong generalization capabilities with respect to different camera parameters and scene arrangements.**

Figure 3: **Qualitative comparison of 3D scene reconstruction on SUN RGB-D . While the baselines often produce noisy or incomplete shape reconstruction of intersecting or misplaced objects, our method produces plausible object arrangements as well as high-quality shape reconstructions.**forms the foundation for subsequent downstream tasks like mixed reality applications, our current model assumes a static scene geometry. Future work could integrate object affordance and articulation into our shape prior  to enable more immersive human-scene interactions.

**Broader Impact** We do not anticipate any societal consequences or negative ethical implications arising from our work. Our approach advances the holistic understanding of 2D perception and 3D modeling, benefiting various research areas.

## 5 Conclusion

In this paper, we present a novel diffusion-based approach for coherent 3D scene reconstructions from a single RGB image. Our method combines a simple yet powerful denoising formulation with a robust generative scene prior that learns inter-object relationships by exchanging relational information among all scene objects. To address the issue of missing ground-truth annotations in publicly available 3D datasets, we introduce a surface alignment loss \(_{}\) to jointly train shape and pose, effectively leveraging our shape representation. Our approach significantly enhances 3D scene understanding, outperforming current state-of-the-art methods across various benchmarks, with +12.04% AP\({}_{}^{15}\) on SUN RGB-D and +13.43% F-Score on Pix3D. Extensive experiments demonstrate that our contributions - 3D scene reconstruction as a conditional diffusion process, scene prior modeling, and joint shape-pose training enabled by \(_{}\) - collectively contribute to the overall performance gain. Additionally, we show that our model supports unconditional synthesis and generalizes well to other indoor datasets without further fine-tuning. We believe these advancements lay a solid foundation for future progress in holistic 3D scene understanding and open up exciting applications in mixed reality, content creation, and robotics.

## 6 Acknowledgements

This work was funded by the ERC Starting Grant Scan2CAD (804724) of Matthias Niessner and the ERC Starting Grant SpatialSem (101076253) of Angela Dai.

Figure 5: **Unconditional results.** Injecting \(\) as a condition to our conditional diffusion model, i.e., effectively disabling the conditioning mechanism, results in high-quality and diverse results.