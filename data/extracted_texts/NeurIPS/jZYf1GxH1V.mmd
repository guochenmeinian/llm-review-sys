# Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization

Jinxin Liu\({}^{1,2}\) Hongyin Zhang\({}^{1,2}\) Zifeng Zhuang\({}^{1,2}\) Yachen Kang\({}^{1,2}\) Donglin Wang\({}^{1}\) Bin Wang\({}^{3}\)

\({}^{1}\)Westlake University \({}^{2}\)Zhejiang University \({}^{3}\)Huawei Noah's Ark Lab

Corresponding author: Donglin Wang <wangdonglin@westlake.edu.cn>

###### Abstract

In this work, we decouple the iterative bi-level offline RL (value estimation and policy extraction) from the offline training phase, forming a non-iterative bi-level paradigm and avoiding the iterative error propagation over two levels. Specifically, this non-iterative paradigm allows us to conduct inner-level optimization (value estimation) in training, while performing outer-level optimization (policy extraction) in testing. Naturally, such a paradigm raises three core questions that are _not_ fully answered by prior non-iterative offline RL counterparts like reward-conditioned policy: Q1) What information should we transfer from the inner-level to the outer-level? Q2) What should we pay attention to when exploiting the transferred information for safe/confident outer-level optimization? Q3) What are the benefits of concurrently conducting outer-level optimization during testing? Motivated by model-based optimization (MBO), we propose DROP (**D**esign f**RO**m** Policies), which fully answers the above questions. Specifically, in the inner-level, DROP decomposes offline data into multiple subsets and learns an MBO score model (A1). To keep safe exploitation to the score model in the outer-level, we explicitly learn a behavior embedding and introduce a conservative regularization (A2). During testing, we show that DROP permits test-time adaptation, enabling an adaptive inference across states (A3). Empirically, we find that DROP, compared to prior non-iterative offline RL counterparts, gains an average improvement probability of more than 80%, and achieves comparable or better performance compared to prior iterative baselines.

## 1 Introduction

Offline reinforcement learning (RL) [37; 40] describes a task of learning a policy from static offline data. Due to the overestimation of values at out-of-distribution (OOD) state-actions, recent iterative offline RL methods introduce various policy/value regularization to avoid deviating from the offline data distribution in the training phase. Then, these methods directly deploy the learned policy in an online environment to test its performance. To unfold our following analysis, we term this kind of learning procedure as _iterative bi-level offline RL_ (Figure 1_left_), where the inner-level optimization refers to trying to eliminate the OOD issue, the outer-level optimization refers to trying to infer an optimal policy that will be employed at testing. Here, we use the "iterative" term to emphasize that the inner-level and outer-level are iteratively optimized in the training phase. However, due to the iterative error exploitation and propagation  over the two levels, performing such an iterative bi-level optimization completely in training often struggles to learn a stable policy/value function.

In this work, we thus advocate for _non-iterative bi-level optimization_ (Figure 1 _right_) that decouples the bi-level optimization from the training phase, _i.e._, performing inner-level optimization (eliminating OOD) in training and performing outer-level optimization (maximizing return) in testing. Intuitively, incorporating the outer-level optimization into the testing phase can eliminate the iterative error propagation over the two levels, _i.e._, there is no iteratively learned value function that bootstraps off of itself and thus propagates errors further. Then, three core questions are: Q1) What information ("\(\)") should we transfer from the inner-level to the outer-level? Q2) What should we pay special attention to when exploiting "\(\)" for safe/confident outer-level optimization? Q3) Notice that the outer-level optimization and the testing rollout form a new loop, what new benefit does this give us?

Intriguingly, prior works under such a non-iterative framework have proposed to transfer ("\(\)" in Q1) filtered trajectories , a reward-conditioned policy , and the Q-value estimation of the behavior policy , all of which, however, partially address the above questions (we will discuss these works in Section 3.4). In this work, we thus propose a new alternative method that transfers an embedding-conditioned score model (Q-value) and we will show that this method sufficiently answers the raised questions and benefits most from the non-iterative bi-level framework.

Before introducing our method, we introduce a conceptually similar task (to the non-iterative bi-level optimization) -- offline model-based optimization (MBO2), which aims to discover, from static input-score pairs, a new design input that will lead to the highest score. Typically, offline MBO first learns a score model that maps the input to its score via supervised regression (_corresponding to inner-level optimization_), and then conducts inference with the learned score model (as "\(\)"), for instance, by optimizing the input against the learned score model via gradient ascent (_corresponding to the outer-level_). To enable this MBO implementation in offline RL, we are required to decompose an offline RL task into multiple _sub-tasks_, each of which thus corresponds to a behavior policy-return (parameters-return) pair. However, there are practical optimization difficulties when using high-dimensional policy's parameter space (as input for the score model) to learn such a score model (_inner-level_) and conduct inference (_outer-level_). At inference, directly extrapolating the learned score model ("\(\)") also tends to drive the high-dimensional candidate policy parameters towards OOD, invalid, and low-scoring parameters , as these are falsely and over-optimistically scored by the learned score model in inner-level optimization.

To tackle these problems, we suggest3 A1) learning low-dimensional embeddings for those sub-tasks decomposed in the MBO implementation, over which we estimate an embedding-conditioned Q-value ("\(\)" in Q1), and A2) introducing a conservative regularization, which pushes down the predicted scores on OOD embeddings. Intuitively, this conservation aims to avoid over-optimistic exploitation and protect against producing unconfident embeddings when conducting outer-level optimization (Q2). Meanwhile, we suggest A3) conducting test-time adaptation, which means we can dynamically adjust the inferred embeddings across testing states (aka deployment adaptation). We name our method DROP (**D**esign **fR0m** Policies). Compared with standard offline MBO for parameter design , test-time adaptation in DROP leverages the sequential structure of RL tasks, rather than simply conducting inference at the beginning of testing rollout. Empirically, we demonstrate that DROP can effectively extrapolate a better policy that benefits from the non-iterative framework by answering the raised questions, and achieves better performance compared to state-of-the-art offline RL algorithms.

Figure 1: **Framework of (_iterative_ and _non-iterative_) bi-level offline RL optimization**, where the inner-level optimization refers to regularizing the policy/value function (for OOD issues) and the outer-level refers to updating the policy for reward maximizing. The “\(\)” transferred from inner-level to outer-level depends on the specific choice of algorithm used (see different choices for \(\) in Table 1).

## 2 Preliminaries

**Reinforcement learning.** We model the interaction between agent and environment as a Markov Decision Process (MDP) , denoted by the tuple \((,,P,R,p_{0})\), where \(\) is the state space, \(\) is the action space, \(P:\) is the transition kernel, \(R:\) is the reward function, and \(p_{0}:\) is the initial state distribution. Let \(:=\{:\}\) denotes a policy. In RL, we aim to find a stationary policy that maximizes the expected discounted return \(J():=_{}[_{t=0}^{}^{t}R(_{t},_{t})]\) in the environment, where \(=(_{0},_{0},r_{0},_{1},_{1}, )\), \(r_{t}=R(_{t},_{t})\), is a sample trajectory and \((0,1)\) is the discount factor. We also define the state-action value function \(Q^{}(,):=_{}[_{t=0}^{ }^{t}R(_{t},_{t})|_{0}=, _{0}=]\), which describes the expected discounted return starting from state \(\) and action \(\) and following \(\) afterwards, and the state value function \(V^{}()=_{(|)} [Q^{}(,)]\). To maximize \(J()\), the actor-critic algorithm alternates between policy evaluation and improvement. Given initial \(Q^{0}\) and \(^{0}\), it iterates

\[Q^{k+1} _{Q}\ \ _{(,, ^{})^{+},^{}^{k}(^{}|^{})}[(R(,)+ Q^{k}( ^{},^{})-Q(,))^{2}],\] (1) \[^{k+1} _{}\ \ _{^{+}, (|)}[Q^{k+1}(,) ],\] (2)

where the value function (critic) \(Q(,)\) is updated by minimizing the mean squared Bellman error with an experience replay \(^{+}\) and, following the deterministic policy gradient theorem , the policy (actor) \((|)\) is updated to maximize the estimated \(Q^{k+1}(,(|))\).

**Offline reinforcement learning.** In offline RL , the agent is provided with a static data \(=\{\}\) which consists of trajectories collected by running some data-generating policies. Note that here we denote static offline data \(\), distinguishing from the experience replay \(^{+}\) in the online setting. Unlike the online RL problem where the experience \(^{+}\) in Equation 1 can be dynamically updated, the agent in offline RL is not allowed to interact with the environment to collect new experience data. As a result, naively performing policy evaluation as in Equation 1 may query the estimated \(Q^{k}(^{},^{})\) on actions that lie far outside of the static offline data \(\), resulting in pathological value \(Q^{k+1}(,)\) that incurs large error. Further, iterating policy evaluation and improvement will cause the inferred policy \(^{k+1}(|)\) to be biased towards OOD actions with erroneously overestimated values.

**Offline model-based optimization.** Model-based optimization (MBO)  aims to find an optimal design input \(^{*}\) with a given score function \(f^{*}:\), _i.e._, \(^{*}=_{}f^{*}()\). Typically, we can repeatedly query the oracle score model \(f^{*}\) for new candidate design, until it produces the best design. However, we often do not have the oracle score function \(f^{*}\) but are provided with a static offline dataset \(\{(,y)\}\) of labeled input-score pairs. To track such a _design from data_ question, we can fit a parametric model \(f\) to the static offline data \(\{(,y)\}\) via the empirical risk minimization (ERM): \(f_{f}_{(,y)}[(f()-y )^{2}]\). Then, starting from the best point in the dataset, we can perform gradient ascent on the design input and set the inferred optimal design \(^{*}=_{K}^{}:=_{f}(_{0}^{ },K)\), where

\[_{k+1}^{}_{k}^{}+_{}f()|_{=_{k}^{}},\ \ \ k=0,,K-1.\] (3)

For simplicity, we will omit subscript \(f\) in \(_{f}\). Since the aim is to find a better design beyond all the designs in the dataset while directly optimizing score model \(f\) with ERM can not ensure new candidates (OOD designs/inputs) receive confident scores, one crucial requirement of offline MBO is thus to conduct confident extrapolation.

## 3 DROP: Design from Policies

We present our framework in Figure 2. In Sections 3.1 and 3.2, we will answer questions 1 and 2, setting a learned MBO score model as "(1)" (1) and introducing a conservative regularization over the score model (2). In Section 3.3, we will answer 3, where we show that conducting outer-level optimization during testing can produce an adaptive embedding inference across states (3).

### Task Decomposition

Our core idea is to explore MBO in the non-iterative bi-level offline RL framework (Figure 1_right_), while capturing the sequential characteristics of RL tasks and answering the raised questions (1, 2, and 3). To begin with, we first decompose the offline data \(\) into \(N\) offline subsets\(_{[N]}:=\{_{1},,_{N}\}\). In other words, we decompose an offline task, learning with \(\), into multiple offline sub-tasks, learning with \(_{n}_{[N]}\) respectively. Then, for sub-task \(n[1,N]\), we can perform behavior cloning (BC) to fit a parametric behavior policy \(_{n}:\) to model the corresponding offline subset \(_{n}\):

\[_{n}_{_{n}}_{(,) _{n}}_{n}(|).\] (4)

Additionally, such a decomposition also comes with the benefit that it provides an avenue to exploit the hybrid modes in offline data \(\), because that \(\) is often collected using hybrid data-generating behavior policies , which suggests that fitting a single behavior policy may not be optimal to model the multiple modes of the offline data distribution (see Appendix A for empirical evidence). In real-world tasks, it is also often the case that offline data exhibit hybrid behaviors. Thus, to encourage the emergence of diverse sub-tasks that capture distinct behavior modes, we perform a simple task decomposition according to the returns of trajectories in \(\), heuristically ensuring that trajectories in the same sub-task share similar returns and trajectories from different sub-tasks have distinct returns4.

### Task Embedding and Conservative MBO

**Naive MBO over behavior policies.** Benefiting from the above offline task decomposition, we can conduct MBO over a set of input-score (\(,y\)) pairs, where we model the (parameters of) behavior policies \(_{n}\) as the design input \(\) and the corresponding expected returns at initial states \(J(_{n})\) as the score \(y\). Note that, ideally, evaluating behavior policy \(_{n}\), _i.e._, calculating \(J(_{n})_{_{0}}[V^{_{n}}(_{ 0})]\), with subset \(_{n}\) will never trigger the overestimation of values in the inner-level optimization. By introducing a score model \(f:\) and setting it as the transfer information "\(\)" in \(\)1, we can then perform outer-level policy inference with

\[^{*}(|)_{}\;f(),\; \;f=_{f}\;_{n}[(f(_{n})-J(_ {n}))^{2}].\] (5)

However, directly performing optimization-based inference (outer-level optimization), \(_{}f()\), will quickly find an invalid input for which the learned score model \(f\) outputs erroneously large values (\(\)2). Furthermore, it is particularly severe if we perform the inference directly over the parameters of policy networks, accounting for the fact that the parameters of behavior policies lie on a narrow manifold in a high-dimensional parametric space.

**Task embedding.** To enable feasible out-level policy inference, we propose to decouple MBO techniques from high-dimensional policy parameters. We achieve this by learning a latent embedding space \(\) with an information bottleneck (\(()(N,())\)). As long as the high-dimensional parameters of behavior policies can be inferred from the embedding \(\), we can use \(\) to represent the corresponding sub-task (or the behavior policy). Formally, we learn a deterministic task embedding5\(:^{N}\) and a contextual behavior policy \(:\) that replaces

Figure 2: **Overview of DROP. Given offline dataset \(\), we decompose the data into \(N\) (\(=4\) in the diagram) subsets \(_{[N]}\). Over the decomposed sub-tasks, we learn a task embedding \((|n)\) and conduct conservative MBO by learning a score model \(f(,,)\) and \(N\) behavior policies (modeled by a contextual policy \((|,)\)). During test-time, at state \(\), we perform test-time adaptation by adapting the policy (contextual variable) with \(^{*}(|)=(|,^{*})\), where \(^{*}=_{}f(,(|, ),)\).**

\(N\) separate behavior policies in Equation 4:

\[(|,),(|n)_{ ,}\ _{_{n}_{[N]}}_{(, )_{n}}[(|,( |n))],\] (6)

**Conservative MBO.** In principle, by substituting the learned task embedding \((|n)\) and contextual behavior policy \((|,)\) into Equation 5, we can then conduct MBO over the embedding space: first learning \(f:\) with \(_{f}_{n,(|n)}[(f()-J(_{n} ))^{2}]\), and then setting the optimal embedding with \(^{*}=_{}f()\) and the corresponding optimal policy with \(^{*}(|)=(|,^{*})\). However, we must deliberate a new OOD issue in the \(\)-space, stemming from the original distribution shift in the parametric space when directly optimizing Equation 5.

Motivated by the energy model  and the conservative regularization , we introduce a conservative score model objective, additionally regularizing the scores of OOD embeddings \(()\):

\[f_{f}\ _{n,(|n)}[(f( )-J(_{n}))^{2}],\ \ _{()}[f()]- _{n,(|n)}[f()],\] (7)

where we set \(()\) to be the uniform distribution over \(\)-space. Intuitively, as long as the scores of OOD embeddings \(_{()}[f()]\) is lower than that of in-distribution embeddings \(_{n,(|n)}[f()]\) (up to a threshold \(\)), conducting embedding inference with \(^{*}=_{}f()\) would produce the best and confident solution, avoiding towards OOD embeddings that are far away from the training set.

Now we have reframed the non-iterative bi-level offline RL problem as one of offline MBO: in the inner-level optimization (Q1), we set the practical choice for \(\!\!\!\)1 as the learned score model \(f\) (A1); in the outer-level optimization (Q2), we introduce task embedding and conservative regularization to avoid over-optimistic exploitation when exploiting \(f\) for policy/embedding inference (A2). In the next section, we will show how to slightly change the form of the score model \(f\), so as to leverage the sequential characteristic (loop in Figure 1) of RL tasks and answer the left Q3.

### Test-time Adaptation

Recalling that we update \(f()\) to regress the value at initial states \(_{_{0}}[V^{_{n}}(_{0})]\) in Equation 7, we then conduct outer-level inference with \(^{*}=_{}f()\) and rollout the \(z^{*}\)-conditioned policy \(^{*}(|):=(|,^{*})\) until the end of testing/deployment rollout episode. In essence, such an inference can produce a confident extrapolation over the distribution of behavior policy embeddings. Going beyond the outer-level policy/embedding inference only at the initial states, we propose that we can benefit from performing inference at any rollout state in deployment (A3).

To enable test-time adaptation, we model the score model with \(f:\), taking a state-action as an extra input. Then, we encourage the score model to regress the values of behavior policies over all state-action pairs, \(_{f}_{n,(|n)}_{(,) _{n}}[(f(,,)-Q^{_{ n}}(,))^{2}]\).

For simplicity, instead of learning a separate value function \(Q^{_{n}}\) for each behavior policy, we learn the score model directly with the TD-error used for learning the value function \(Q^{_{n}}(,)\) as in Equation 1, together with the conservative regularization in Equation 7:

\[f_{f}_{_{n}_ {[N]}}_{(,,^{},^{ })_{n}}[(R(,)+ (^{},^{},(|n))-f(,,(|n)))^{2}],\] (8) \[\ _{n,()}_{ _{n},(|,)}[f( ,,)]-_{n,(|n)} _{_{n},(| ,)}[f(,,)],\]

where \(\) denotes a target network and we update the target \(\) with soft updates: \(=(1-)+ f\).

In test-time, we thus can dynamically adapt the outer-level optimization, setting the inferred optimal policy \(^{*}(|)=(|,^{*}( ))\), where \(z^{*}()=_{z}f,(|, ),\). Specifically, at any state \(\) in the test-time, we can perform gradient ascent to find the optimal behavior embedding: \(z^{*}()=_{K}^{}():=(,_{0}^{},K)\), where \(_{0}^{}\) is the starting point and for \(k=0,1,K-1\),

\[_{k+1}^{}()_{k}^{}() +_{}f(,(|,), ))|_{=_{k}^{}}.\] (9)

### Connection to Prior Non-iterative Offline Methods

In Table 1, we summarize the comparison with prior representative non-iterative offline RL methods. Intuitively, our DROP (using returns to decompose \(\)) is similar in spirit to F-BC6 and RvS-R ,both of which use return \(()\) to guide the inner-level optimization. However, both F-BC and RvS-R leave Q2 unanswered. In outer-level, F-BC can not enable policy extrapolation, which heavily relies on the data quality in offline tasks. RvS-R needs to handcraft a target return (as the contextual variable for \((|,)\)), which also probably triggers the potential distribution mismatch7 between the hand-crafted contextual variable (_i.e._, the desired return) and the actual rollout return induced by the learned policy when conditioning on the given contextual variable.

```
0: Dataset of trajectories: \(=\{\}\).
1: Decompose \(\) into \(N\) sub-sets \(_{[N]}\).
2: Initialize \((|n)\), \((|,)\), and \(f(,,)\).
3:while not converged do
4: Sample a sub-task: \(_{n}_{[N]}\).
5: Learn \(\), \(\), and \(f\) with Equations 6 and 8.
6:endwhile Return: \((|,)\) and \(f(,,)\). ```

**Algorithm 1** DROP (Training)

### Practical Implementation

```
0: Env, \((|,)\), and \(f(,,)\).
1:s\({}_{0}\) = Env.Reset().
2:while not done do
3:Test-time adaptation: \(^{*}(_{t})=_{}f(_{t},( _{t}|_{t},),)\).
4: Sample action: \(_{t}(_{t}|_{t},^{*}(_{t}))\).
5: Step Env: \(_{t+1} P(_{t+1}|_{t},_{t})\).
6:endwhile ```

**Algorithm 2** DROP (Testing/Deployment)

We now summarize the DROP algorithm (see Algorithm 1 for the training phase and Algorithm 2 for the testing phase). During training (inner-level optimization), we alternate between updating \((|n)\), \((|,)\), and \(f(,,)\), wherein we update \(\) with both maximum likelihood loss and TD-error loss in Equations 6 and 8. During testing (outer-level optimization), we use the gradient ascent as specified in Equation 9 to infer the optimal embedding \(z^{*}\). Instead of simply sampling a single starting point \(_{0}^{}\), we choose \(N\) starting points corresponding to all the embeddings \(\{_{n}\}\) of sub-tasks, and then choose the optimal \(^{*}()\) from those _updated embeddings_ for which the learned \(f\) outputs the highest score, _i.e._, \(^{*}()=_{}\;f(,(|,),)\;\) s.t. \(\{ GradAscent}(,_{n},K)|n=1,,N\}\). Then, we sample action from \(^{*}(|):=(|,^{*}( ))\).

## 4 Related Work

**Offline RL.** In offline RL, learning with offline data is prone to exploiting OOD state-actions and producing over-estimated values, which makes vanilla iterative policy/value optimization challenging. To eliminate the problem, a number of methods have been explored, in essence, by either _introducing a policy/value regularization in the iterative loop_ or _trying to eliminate the iterative loop itself_.

_Iterative methods:_ Sticking with the normal iterative updates in RL, offline policy regularization methods aim to keep the learning policy to be close to the behavior policy under a probabilistic

    & Inner-level & Outer-level & \(\) in A1 & A2 & A3 \\  F-BC & BC over filtered \(\) with high \(()\) & — & \((|)\) & ✗ & ✗ \\ RvS-R  & \(_{}-[(|,())]\) & handcraft \(_{}\) & \((|,)\) & ✗ & ✗ \\ Onestep  & \(_{Q}(Q(,),(,))\) & \(_{}Q_{}(,(|))\) & \(Q_{}(,)\) & ✗ & ✗ \\ COMs  & \(_{f}(f(_{}),())\) & \(_{}f()\) & \(f()\) & ✗ & ✗ \\ DROP (ours) & \(_{f}(f(,,),(, ,))\) & \(_{}f(,(|),)\) & \(f(,,)\) & ✗ & ✗ \\   

Table 1: **Comparison of non-iterative bi-level offline RL methods**, where we use \(()\) to represent the return of a sampling trajectory \(\) and use \((,)\) to represent the expected return of trajectories starting from \((,)\). The checkmark in A2 indicates whether the exploitation to \(\) is regularized for outer-level optimization and that in A3 indicates whether test-time adaptation is supported.

distance [8; 19; 30; 33; 45; 46; 52; 54; 58; 67; 73]. Some works also conduct implicit policy regularization with variants of importance sampling [39; 47; 51]. Besides regularizing policy, it is also feasible to constrain the substitute value function in the iterative loop. Methods constraining the value function aim at mitigating the over-estimation, which typically introduces pessimism to the prediction of the Q-values [9; 27; 35; 41; 48; 49] or penalizes the value with an uncertainty quantification [3; 5; 56; 68], making the value for OOD state-actions more conservative. Similarly, another branch of model-based methods [29; 71; 72; 57] also perform iterative bi-level updates, alternating between regularized evaluation and improvement. Different from this iterative paradigm, DROP only evaluates values of behavior policies in the inner-level optimization, avoiding the potential overestimation for values of learning policies and eliminating error propagation between two levels.

_Non-iterative methods:_ Another complementary line of work studies how to eliminate the iterative updates, which simply casts RL as a _weighted_ or _conditional_ imitation learning problem (Q1). Derived from the behavior-regularization RL [22; 64], _the former_ conducts weighted behavior cloning: first learning a value function for the behavior policy, then weighing the state-action pairs with the learned values or advantages [1; 12; 66; 54]. Besides, some works propose an implicit behavior policy regularization that similarly does not estimate values of new learning policies: initializing the learning policy with a behavior policy  or performing only a "one-step" update (policy improvement) over the behavior policy [20; 24; 75]. For _the latter_, this branch method typically builds upon the hindsight information matching [4; 15; 28; 36; 44; 43; 55; 65], assuming that the future trajectory information can be useful to infer the middle decision that leads to the future and thus relabeling the trajectory with the reached states or returns. Due to the simplicity and stability, RvS-based methods advocate for learning a goal-conditioned or reward-conditioned policy with supervised regression [10; 13; 14; 21; 25; 42; 60; 69]. However, these works do not fully exploit the non-iterative framework and fail to answer the raised questions, which either does not regularize the inner-level optimization when exploiting "\(\)" in outer-level (Q2), or does not support test-time adaptation (Q3).

**Offline MBO.** Similar to offline RL, the main challenge of MBO is to reason about uncertainty and OOD values [7; 16], since a direct gradient-ascent against the learned score model can easily produce invalid inputs that are falsely and highly scored. To counteract the effect of model exploitation, prior works introduce various techniques, including normalized maximum likelihood estimation , model inversion networks , local smoothness prior , and conservative objective models (COMs) . Compared to COMs, DROP shares similarities with the conservative model, but instantiates on the embedding space instead of the parameter space. Such difference is nontrivial, not only because DROP avoids the adversarial optimization employed in COMs, but also because DROP allows test-time adaptation, enabling dynamical inference across states at testing.

## 5 Experiments

In this section, we present our empirical results. We first give examples to illustrate the test-time adaptation. Then we evaluate DROP against prior offline RL algorithms on the D4RL benchmark. Finally, we provide the computation cost regarding the test-time adaptation protocol. For more offline-to-online fine-tuning results, ablation studies w.r.t. the decomposition rules and the conservative regularization, and training details on the hyper-parameters, we refer the readers to the appendix.

**Illustration of test-time adaptation.** To better understand the test-time adaptation of DROP, we include four comparisons that exhibit different embedding inference rules at testing/deployment:

(1) DROP-Best: At initial state \(_{0}\), we choose the best embedding from those embeddings of behavior policies, \(_{0}^{*}(_{0})=_{z}f(_{0},(_{0}|_{0},),)\) s.t. \(_{[N]}:=\{_{1},,_{N}\}\), and keep this embedding fixed for the entire episode, _i.e._, setting \(^{*}(_{t}|_{t})=(_{t}|_{t}, _{0}^{*}(_{0}))\).

(2) DROP-Grad: At initial state \(_{0}\), we conduct inference (gradient ascent on starting point \(_{0}^{*}(_{0})\)) with \(^{*}(_{0})=_{z}f(_{0},(_{0}|_{0},),)\), and keep \(^{*}(_{0})\) fixed throughout the test rollout.

(3) DROP-Best-Ada: We adapt the policy by setting \(^{*}(_{t}|_{t})=(_{t}|_{t}, _{0}^{*}(_{t}))\), where we choose the best embedding \(_{0}^{*}(_{t})\) directly from those embeddings of behavior policies for which the score model outputs the highest score, _i.e._, \(_{0}^{*}(_{t})=_{}f(_{t},( _{t}|_{t},),)\) s.t. \(_{[N]}\).

(4) DROP-Grad-Ada (as described in Section 3.5): We set \(^{*}(_{t}|_{t}){=}(_{t}|_{t}, ^{*}(_{t}))\) and choose the best embedding \(^{*}(_{t})\) from those _updated embeddings_ of behavior policies, _i.e._, \(^{*}(_{t})=_{}f(_{t},( _{t}|_{t},),)\) s.t. \(\{(_{t},_{n},K)|n=1,,N\}\).

In Figure 3, we visualize the four different inference rules and report the corresponding performance in the halfcheetah-medium-expert task . In Figure 3 (a), we set the starting point as the best embedding \(_{0}^{*}(_{0})\) in \(_{[N]}\), and perform gradient ascent to find the optimal \(_{0}^{*}(_{0})\) for DROP-Grad. In Figure 3 (b), we can find that at different time steps, DROP-Best-Ada chooses different embeddings for \((_{t}|_{t},)\). Intuitively, performing such a dynamical embedding inference enables us to combine different embeddings, _thus stitching behavior policies at different states_. Further, in Figure 3 (c1, c2), we find that performing additional inference (with gradient ascent) in DROP-Grad-Ada allows to extrapolate beyond the embeddings of behavior policies, and _thus results in sequential composition of new embeddings (policies) across different states_. For practical impacts of these different inference rules, we provide the performance comparison in Figure 3 (d), where we can find that performing gradient-based optimization (*-Grad-*) outperforms the natural selection among those embeddings of behavior policies/sub-tasks (*-Best-*), and rollout with adaptive embedding inference (DROP-*-Ada) outperforms that with fixed embeddings (DROP-Best and DROP-Grad). In subsequent experiments, if not explicitly stated, DROP takes the DROP-Grad-Ada implementation by default.

**Empirical performance on benchmark tasks.** We evaluate DROP on a number of tasks from the D4RL dataset and make comparisons with prior non-iterative offline RL counterparts8.

As suggested by Agarwal et al. , we provide the aggregate comparison results to account for the statistical uncertainty. In Figure 4, we report the median, IQM , and mean scores of DROP and prior non-iterative offline baselines in 12 D4RL tasks (see individual comparison results in Appendix C). We can find our DROP (Best-Ada and Grad-Ada) consistently outperforms prior non-iterative baselines in both median, IQM, and mean metrics. Compared to the most related baseline Onestep and COMs (see Table 1), we provide the probability of performance improvement in Figure 4 _right_. We can find that DROP-Grad-Ada shows a robust performance improvement over Onestep and COMs, _with an average improvement probability of more than 80%_.

**Comparison with latent policy methods.** Note that one additional merit of DROP is that it naturally accounts for hybrid modes in \(\) by conducting task decomposition in inner-level, we thus compare DROP to latent policy methods (PLAS  and LAPO ) that use conditional variational autoencoder (CVAE) to model offline data and also account for multi-modes in offline data. Essentially, both our DROP and baselines (PLAS and LAPO) learn a latent policy in the inner-level optimization, except that we adopt the non-iterative bi-level learning while baselines are instantiated

Figure 4: (_Left_) **Aggregate metrics  on 12 D4RL tasks**. Higher median, IQM, and mean scores are better. (_Right_) Each row shows the probability of improvement that the algorithm X on the left outperforms algorithm Y on the right. The CIs are estimated using the percentile bootstrap (95%) with stratified sampling. For all results of our method, we average the normalized returns across 5 seeds; for each seed, we run 10 evaluation episodes. (GA: Grad-Ada. BA: Best-Ada.)

under the iterative paradigm. By answering Q3, DROP permits test-time adaptation, enabling us to dynamically stitch "skills"(latent behaviors/embeddings as shown in Figure 3) and thus encouraging high-level abstract exploitation in testing. However, the aim of introducing the latent policy in PLAS and LAPO is to regularize the inner-level optimization, which fairly answers Q2 in the iterative offline counterpart but can not provide the potential benefit (test-time adaptation) by answering Q3.

Note in our naive DROP implementation, we heuristically use the return to conduct task decomposition (motivated by RvS-R), while PLAS and LAPO take each trajectory as a sub-task and use CVAE to learn the latent policy. For a fair comparison, here we also adopt CVAE to model the offline data and afterward take the learned latent embedding in CVAE as the embedding of behaviors, instead of conducting return-guided task decomposition. We provide implementation details (DROP+CVAE) in Appendix A.4 and the comparison results in Table 2 (taking DROP-Grad-Ada implementation). We can observe that our DROP consistently achieves better performance than PLAS in all tasks, and performs better or comparably to LAPO in 10 out of 12 tasks. Additionally, we also provide the results of CQL and IQL . We can find that DROP also leads to significant improvement in performance, consistently demonstrating the competitive performance of DROP against state-of-the-art offline iterative/non-iterative baselines.

**Computation cost.** One limitation of DROP is that conducting test-time adaptation at each state is usually expensive for inference time. To reduce the computation cost, we find that we can resample (infer) the best embedding \(^{*}\) after a certain time interval, _i.e._, \(^{*}\) does not necessarily need to be updated at each testing state. In Figure 5, we plot the average inference time versus different models and DROP implementations with varying inference time intervals. We see that compared to Diffuser (one offline RL method that also supports test-time adaptation, see quantitative performance comparison in Table 4) , DROP can achieve significantly lower computation costs. In Figure 5_top-right_, we also illustrate the trade-off between performance and runtime budget as we vary the inference time intervals. We find that by choosing a suitable time interval, we can reduce the inference DROP(50) brings less computational burden while providing stable performance.

**Comparison to a "distilled" DROP implementation.** Our answer to Q3 is that we conduct outer-level optimization in the testing phase. Similarly, we can design a "distilled" DROP implementation: we keep the same treatment as DROP for Q1and Q2, but we perform outer-level optimization on the existing offline data instead of on the testing states, _i.e._, conducting optimization in \(\)-space for all states in the dataset. Then, we can "distill" the resulting contextual policy and the inferred

    & u & u-d & um-p & um-d & ul-p & ul-d & wa-r & ho-r & ha-r & wa-m & ho-m & ha-m & mean \\  CQL & 74.0 & 84.0 & 61.2 & 53.7 & 15.8 & 14.9 & -0.2 & 8.3 & 22.2 & **82.1** & 71.6 & 49.8 & 44.8 \\ IQL & 87.5 & 62.2 & 71.2 & 70.0 & 39.6 & 47.5 & **5.4** & 7.9 & 13.1 & 77.9 & 65.8 & 47.8 & 49.7 \\ PLAS & 70.7 & 45.3 & 16.0 & 0.7 & 0.7 & 0.3 & 9.2 & 6.7 & 26.5 & 75.5 & 51.0 & 44.5 & 28.9 \\ LAPO & 86.5 & 80.6 & 68.5 & 79.2 & 48.8 & **64.8** & 1.3 & **23.5** & 30.6 & 80.8 & 51.6 & 46.0 & 55.2 \\ DROP & **90.5** & **92.2** & **74.1** & **82.9** & **57.2** & 63.3 & 5.2 & 20.8 & **32.0** & **82.1** & **74.9** & **52.4** & **60.6** \\ DROP std & \( 2.4\) & \( 1.7\) & \( 3.9\) & \( 3.5\) & \( 5.5\) & \( 2.4\) & \( 1.6\) & \( 0.3\) & \( 2.5\) & \( 5.2\) & \( 2.8\) & \( 2.2\) \\   

Table 2: **Comparison in AntMaze and Gym-MuJoCo domains (v2).** Results are averaged over 5 seeds; for each seed, we run 10 evaluation episodes. u: umaze (antmaze). um: umaze-medium. ul: umaze-large. p: play. d: diverse. wa: walker2d. ho: hopper. ha: halfcheetah. r: random. m: medium.

Figure 5: **Computation cost across different models and DROP implementations with varying inference time intervals.** The number in parenthesis denotes the inference time intervals. In _top-right_, we also provide the average performance of DROP on AntMaze tasks when using different inference time intervals. (Diffuser* denotes the warm-start implementation of Diffuser.)

\(^{*}\) into a fixed rollout policy. As shown in Table 3, we can see that such a "distilled" DROP implementation achieves competitive performance, approaching DROP's overall score and slightly outperforming CQL and IQL, which further supports the superiority afforded by the non-iterative offline RL paradigm versus the iterative one.

**Comparison to adaptive baselines.** One merit of DROP resides in its test-time adaptation ability across rollout states. Thus, here we compare DROP with two offline RL methods that also support test-time adaptation: 1) APE-V  learns an uncertainty-adaptive policy and dynamically updates a contextual belief vector at test states, and 2) Diffuser  employs a score/return function to guide the diffusion denoising process, _i.e._, score-guided action sampling. As shown in Table 4, we can see that in medium-replay (mr) and medium-expert (me) domains (v2), DROP can achieve comparable results to APE-V, with a clear improvement over Diffuser.

## 6 Discussion

In this work, we introduce non-iterative bi-level offline RL, and based on this paradigm, we raise three questions (Q1, Q2, and Q3). To answer that, we reframe the offline RL problem as one of MBO and learn a score model (A1), introduce embedding learning and conservative regularization (A2), and propose test-time adaptation in testing (A3). We evaluate DROP on various tasks, showing that DROP gains comparable or better performance compared to prior methods.

**Limitations.** DROP also has several limitations. First, the offline data decomposition dominates the following bi-level optimization, and thus choosing a suitable decomposition rule is a crucial requirement for policy inference (see experimental analysis in Appendix A.2). An exciting direction for future work is to study generalized task decomposition rules (_e.g._, our DROP+CVAE implementation). Second, we find that when the number of sub-tasks is too large, the inference is unstable, where adjacent checkpoint models exhibit larger variance in performance (such instability also exists in prior offline RL methods, discovered by Fujimoto and Gu ). One natural approach to this instability is conducting online fine-tuning (see Appendix A.3 for our empirical studies).

Going forward, we believe our work suggests a feasible alternative for generalizable offline robotic learning: by decomposing a single robotic dataset into multiple subsets, offline policy inference can benefit from performing MBO and the test-time adaptation protocol.