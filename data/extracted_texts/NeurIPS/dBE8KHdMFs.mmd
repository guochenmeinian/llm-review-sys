# ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence

Wenjie Mei1\({}^{}\)

Dongzhe Zheng1\({}^{}\)

Shihua Li\({}^{}\)

Equal contribution. Correspondence to wenjie.mei@seu.edu.cn and lsh@seu.edu.cn Wenjie Mei and Shihua Li are with the School of Automation and the Key Laboratory of MCCSE of the Ministry of Education, Southeast University, Nanjing, China. Dongzhe Zheng is with the Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China.

###### Abstract

Neural ODEs (NODEs) are continuous-time neural networks (NNs) that can process data without the limitation of time intervals. They have advantages in learning and understanding the evolution of complex real dynamics. Many previous works have focused on NODEs in concise forms, while numerous physical systems taking straightforward forms, in fact, belong to their more complex quasi-classes, thus appealing to a class of general NODEs with high scalability and flexibility to model those systems. This, however, may result in intricate nonlinear properties. In this paper, we introduce ControlSynth Neural ODEs (CSODEs). We show that despite their highly nonlinear nature, convergence can be guaranteed via tractable linear inequalities. In the composition of CSODEs, we introduce an extra control term for learning the potential simultaneous capture of dynamics at different scales, which could be particularly useful for partial differential equation-formulated systems. Finally, we compare several representative NNs with CSODEs on important physical dynamics under the inductive biases of CSODEs, and illustrate that CSODEs have better learning and predictive abilities in these settings.

## 1 Introduction

Neural ODEs (NODEs)  were developed from the limiting cases of continuous recurrent networks and residual networks and exhibit non-negligible advantages in data incorporation and modeling unknown dynamics of complex systems, for instance. Their continuous nature makes them particularly beneficial to learning and predicting the dynamical behavior of complex physical systems, which are often difficult to realize due to sophisticated internal and external factors for the systems.

Starting from the introduction of NODEs, many types of variants of NODEs have been studied (see, _e.g._, [6; 19; 17; 15]). Nevertheless, there are rare studies concerning highly scalable and flexible dynamics that also present complex nonlinear natures, bringing difficulties in their modeling and analyses. No in-detail research attention has been paid to the scalability of depth and structure in NODEs despite numerous physical systems in the real world having inscrutable dynamics and compositions. To fill in this gap, we propose ControlSynth Neural ODEs (CSODEs), in whose structure another sub-network is also incorporated for enlarging the dexterity of the composition and controlling the evolution of the state. Different from most of the existing methods and experiments, we focus on widely investigated physical models, with known state evolution under necessary conditions. Subsequently, we will show that the proposed NODEs are effective in learning and understanding those models.

Our contributions are mainly composed of the novel structure of CSODEs, their convergence guarantees, and the comparative experiments among CSODEs, several representative NODEs, and theirdivisions, illustrating the beneficiiality of CSODEs in the setting of prediction. The convergence conditions provide tractable solutions for constraining the learned model to a convergent one. The preliminary experiment demonstrates that our CSODEs can learn the evolution of the dynamics faster and more precisely. Also, we show that introducing sub-networks into CSODE does not impact the overall computational performance more than the other comparable NNs. Finally, we compare NODE, Augmented Neural ODE (ANODE), Second Order Neural ODE (SONODE), CSODE, and its variant in real dynamical systems. The experimental results indicate that our CSODE is beneficial to more accurate time series prediction in the systems. Our code is available online at https://github.com/ContinuumCoder/ControlSynth-Neural-ODE.

## 2 ControlSynth Neural Ordinary Differential Equations

We begin by introducing the form of CSODEs as follows:

\[(t)=A_{0}x(t)+_{j=1}^{M}A_{j}f_{j}(W_{j}x(t))+g(u(t)),\] (1)

where \(x_{t}:=x(t)^{n}\) is the state vector; the matrices \(A\). are with approximate dimensions; \(W\). are weight matrices; the input \(u_{t}:=u(t) U^{m}\), \(u_{}^{m}\) (refer to Appendix C.1); \(f_{j}=[f_{j}^{1} f_{j}^{k_{j}}]^{}\)\(f_{j}:^{k_{j}}^{k_{j}}\) and \(g:U^{n}\) are the functions ensuring the existence of the solutions of the neural network (NN) (1) at least locally in time, and \(g=[g_{1} g_{n}]^{}\); _w.l.o.g._, the time \(t\) is set as \(t 0\).

CSODEs extend the concept of Neural ODEs, which are typically expressed as \((t)=f(x(t))\), where \(f\) is a neural network. CSODEs incorporate control inputs \(u(t)\) and create a combination of subnetworks \(_{j=1}^{M}A_{j}f_{j}(W_{j}x(t))\). This formulation enhances expressiveness and adaptability to complex systems with external inputs. CSODEs provide a more universal framework and improve upon Neural ODEs by offering greater flexibility, interpretability through separated linear and nonlinear terms, and natural integration with techniques in control theory.

For simplicity, in the experiments (see Section 6), we select two common NNs as the specific examples of the function \(g\): Traditional Multilayer Perceptron (MLP) and two-layer Convolutional Neural Network (CNN), and \(\) as the used activation functions: a particular subclass of the functions \(f_{j}\). Note that in the case that \(g(u)\) represents an MLP, there are many different types of neurons, for example, functional neurons , and in CSODEs (1) there may exist \(f_{j}^{i} L^{1}(X,,)\), where \((X,,)\) denotes a \(\)-finite measure space.

## 3 Related Work

SONODEs, ANODEs, and NCDEs vs CSODEsSONODEs  are particularly suitable for learning and predicting the second-order models, and ANODEs  are useful for cases where the state evolution does not play an important role. In contrast, many real models of second-order or even higher orders can be transformed into the common first-order but emerge numerous nonlinearities that are difficult to accurately treat by NODEs and their variants. Despite the intricate structure of CSODEs, they can be equipped with a significant convergence attribute. The experiments show that our CSODEs are more appropriate for model scaling and natural dynamics that exhibit complex nonlinear properties. Although both Neural CDEs (NCDEs)  and CSODEs extend NODEs through control mechanisms, their architectural philosophies differ. NCDEs introduce control through path-valued data driving the vector field, following \((t)=f(x(t))\). In contrast, CSODEs propose a more sophisticated structure combining multiple subnetworks with a dedicated control term \(g(u(t))\). This design not only provides theoretical convergence guarantees but also enables CSODEs

Figure 1: Schematic of the CSODEs solver, showing integration via NNs at one time step. Using the forward Euler method as an example, it shows how \(u_{t}\) and \(x_{t}\) evolve through the update neural function \(h()\) and NNs \(g()\), \(A_{1}f_{1}(W_{1})\),..., \(A_{M}f_{M}(W_{M})\) to yield \(u_{t+ t}\) and \(x_{t+ t}\) as the next variables.

to capture intricate physical dynamics through its hierarchical structure, particularly beneficial for systems exhibiting complex nonlinear behaviors that elude simpler controlled formulations.

Convergence and StabilityRelevant to our work,  studies the asymptotic stability of a specific class of NODEs as part of its fundamental property investigations on NODEs. In contrast, our study focuses on the scalability aspects related to both the number of subnetworks and the width of NNs, extending beyond the scope of the NNs in [13; 15; 16]. Furthermore, our experiments are primarily toward learning and predicting the dynamic behaviors of physical models rather than image classifications. Stability analyses have also been performed on NODE variants, such as SODEF  SNDEs , and Stable Neural Flows.

Physical Information Based DynamicsSimilar to the way CSODEs embody physical dynamics, various other models have been developed to incorporate physical information, ensuring the derivation of physically plausible results, both in discrete and continuous time. For example,  employs physics-informed neural networks (PINNs) (In contrast to NODEs and their variants, which mainly focus on implicitly learning dynamical systems, PINNs explicitly incorporate physical laws into the loss function. These two types of methods tackle the problem of integrating physical knowledge into deep learning models from different perspectives and are complementary research directions) to solve the Shallow Water Equations on the sphere for meteorological use, and Raissi et al.  propose a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations using PINNs. Shi et al.  and O'Leary et al.  utilize enhanced NODEs for learning complex behaviors in physical models, while Greydanus et al.  and Cranmer et al.  propose Hamiltonian and Lagrangian NNs, respectively, to learn physically consistent dynamics.

## 4 Theoretical Results: Convergence Analysis

Consider the CSODEs given in Equation (1), where \(f_{j}:^{k_{j}}^{k_{j}}\) are nonlinear activation functions. For them, an imposed condition on \(f_{j}^{i}\) (the \(i\)-th element of the vector-valued \(f_{j}\)) is presented as follows:

**Assumption 1**: _For any \(i\{1,,k_{j}\}\) and \(j\{1,,M\}\), \(sf_{j}^{i}(s)>0\) for all \(s\{0\}\)._

**Remark 1**: _Assumption 1 applies to many activation functions, such as \(\) and parametric ReLU. It picks up the activation functions passing through the origin and the quadrants I and III. For more explanations, the reader is referred to Appendix B.1._

In this study, to analyze the convergence property of the NN (1), we first define the concept of _convergence_:

**Definition 1**: _The model (1) is convergent if it admits a unique bounded solution for \(t\) that is globally asymptotically stable (GAS)._

In order to investigate the convergence, two properties have to be satisfied, that is, the boundedness and the GAS guarantees of the solution \(x^{*}\) for (1). In this respect, two assumptions are given as follows.

**Assumption 2**: _Assume that the functions \(f_{j}^{i}\) are continuous and strictly increasing for any \(i\{1,,k_{j}\}\) and \(j\{1,,M\}\)._

Assumption 2 aligns with CSODE's structure, reflecting continuity and monotonicity of activation functions. This relates to model dynamics and is satisfied by most common activations.

In the analysis of convergence, one needs to study two models in the same form but with different initial conditions and their contracting properties. To that end, along with (1), we consider the model \((t)=A_{0}y(t)+_{j=1}^{M}A_{j}f_{j}(W_{j}y(t))+g(u(t))\) with the same input but different initial conditions \(y(0)^{n}\). Let \(:=y-x\). Then the corresponding error system is

\[=A_{0}+_{j=1}^{M}A_{j}p_{j}(x,),\] (2)

where \(p_{j}(x,)=f_{j}(W_{j}(+x))-f_{j}(W_{j}x)\). Note that for any fixed \(x^{n}\), the functions \(p_{j}\) in the variable \(^{n}\) satisfy the properties formulated in Assumptions 1, 2. The following assumption is imposed for analyzing the contracting property of (2).

**Assumption 3**: _Assume that there exist positive semidefinite diagonal matrices \(S_{0}^{j},S_{1}^{j},S_{2}^{j},S_{3}^{j,r},H_{0}^{j},H_{1}^{j},H_{2}^{j},H_{3}^{j,r }(j,r\{1,,M\})\) with appropriate dimensions such that_

\[p_{j}(x,)^{}p_{j}(x,)  ^{}W_{j}^{}S_{0}^{j}W_{j}+2^{}W_{j}^{}S_ {1}^{j}p_{j}(x,)+2^{}W_{j}^{}S_{2}^{j}f_{j}(W_{j})\] \[+2_{r=1}^{M}p_{j}(x,)^{}W_{j}^{}W_{j}S_{3}^{j,r}W_{ r}^{}W_{r}f_{r}(W_{r})\] \[ f_{j}(W_{j})^{}f_{j}(W_{j})  ^{}W_{j}^{}H_{0}^{j}W_{j}+2^{}W_{j}^{}H_ {1}^{j}p_{j}(x,)+2^{}W_{j}^{}H_{2}^{j}f_{j}(W_{j})\] \[+2_{r=1}^{M}p_{j}(x,)^{}W_{j}^{}W_{j}H_{3}^{j,r}W_{ r}^{}W_{r}f_{r}(W_{r})\]

_for all \(x,y^{n}\) and \(=x-y\)._

Notice that Assumption 3 is at least more relaxed than Lipschitz continuity (see Appendix B.2 for an intuitive example of activation functions satisfying Assumption 3).

Convergence ConditionsWe are now ready to show the convergence conditions for the CSODEs (1):

**Theorem 1**: _Let Assumptions 1-3 be satisfied. If there exist positive semidefinite symmetric matrices \(P,\); positive semidefinite diagonal matrices \(\{^{j}=(_{1}^{j},,_{n}^{j})\}_ {j=1}^{M},\ \{^{j}=(_{1}^{j}, ,_{n}^{j})\}_{j=1}^{M},\ \{^{s}\}_{s=0}^{M},\ \{_{s,r}\}_{0 s<r M},\{ _{j,j^{}}\}_{j,j^{}=1}^{M},\ \{_{j}\}_{j=1}^{M},\ \{_{j}\}_{j=1}^{M},\ ^{0};\) positive definite symmetric matrix \(\); and positive scalars \(,\) such that the linear matrix inequalities (LMIs) in Appendix B.4 hold true. Then, a forward complete system (1) is convergent._

Proof in Appendix C.3. Note that the used conditions on \(f_{j}^{i}\) in Assumption 3 can be relaxed to "non-decreasing", which enlarges the scope of activation functions, including non-smooth functions like ReLU, then the resulting modifications for the formulations of Theorem 1 can be readily obtained, highlighting the CSODE framework's adaptability.

Those LMI conditions ensure system convergence. From an energy perspective, this indicates the error system's generalized energy (represented by the energy (or Lyapunov) function) is monotonically non-increasing, leading to convergence towards the equilibrium point: origin. These conditions can be easily verified, thanks to CSODE's structural characteristics and LMIs' highly adjustable elements.

The matrices, such as \(^{0}\) and \(_{j,j^{}}\), in the LMIs act as compensation terms balancing the effects of linear and nonlinear terms, ensuring the derivative of the energy function \(\) remains non-positive. Properties of \(f_{j}\) (Assumptions 1 and 2) provide facilitation in constructing these matrices. Assumption 3 allows for non-restrictive conditions on activation functions, avoiding strong global Lipschitz continuity assumptions and providing precise local asymptotic stability characterization.

## 5 Preliminary Experiments

Convergence and StabilityConvergence, an important attribute showcasing a model's learning ability, refers to its capability to consistently approach the theoretical optimal solution throughout the learning process over iterations. To validate the convergence and stability of CSODEs, we design an experiment that involves learning simple spiral trajectories, chosen for their fundamental complexity which all models should ideally handle. In this experiment, we compare CSODEs and NODEs, both based on the Latent ODE structure . This setup provides a fundamental baseline for assessing convergence and ensures a fair comparison, enabling each model to demonstrate its learning capabilities under comparable conditions without bias toward specific structural advantages. The Mean Absolute Error (MAE) loss function, which measures the average of the absolute differences between the estimated trajectories and the true trajectories, is used as the indicator. We train the model on 100 noisy spiral data observation points to learn this trajectory. The MAE loss values collected over training epochs consistently, plotted in Figure 2, show that the CSODE model not only converges faster but also maintains a lower error rate compared to the traditional NODE model, particularly under the constraints of limited and noisy observational data points. Our model's ability to accurately predict the target trajectory despite the noisy data illustrates its noise tolerance, a critical aspect of its robust stability.

Generalization and ExtrapolationFigure 3 presents a visual comparison of the prediction results from the CSODE model and the traditional NODE model with the actual ground truth trajectory after various training epochs in trajectories learning experiment in the previous paragraph. It is evident from the figure that the CSODE not only learns the trajectory within the observation period more precisely in fewer training epochs but also aligns more accurately with the original trajectories beyond the observation period, which accounts for 25% of the total duration. This demonstrates the robust generalization and extrapolation capabilities of CSODEs for predicting future states, illustrating its better understanding of the underlying time-dependent structures and dynamics in the data.

Computational PerformanceTo evaluate whether introducing subnetworks in CSODEs impacts the overall computational performance, we conduct experimental validations on image classification, focusing on aspects such as operational efficiency and system load. These experiments are based on the ODE-Nets framework , using the forward Euler method, and span 100 epochs on the MNIST dataset to compare the performance of NODEs , Augmented Neural ODEs (ANODEs) , Second Order Neural ODEs (SONODEs) , and CSODEs. All experiments in this study are conducted on a system equipped with an NVIDIA GeForce RTX 3080 GPU and CUDA, ensuring a consistent computational environment across all tests. Performance metrics include test error rate, number of parameters (# Params), floating-point operations per second (FLOPS), average batch processing time (Time), and peak memory usage (Mem). The results (see Table 1) show that although CSODEs theoretically involves iterative computation of \(g(u)\), it does not significantly reduce computational efficiency in practice. While MNIST is a relatively simple dataset, the experimental outcomes preliminarily confirm the benefits of incorporating a learnable function in enhancing the performance of implicit networks, effectively maintaining efficiency, and alleviating concerns over the potential trade-off between expressive capability and computational efficiency of the networks.

## 6 Complex Systems Time Series Prediction Experiments

In this section, we experimentally analyze the performance of CSODEs, based on NODEs, compared to other models based on NODEs and traditional time series forecasting methods in extrapolative time series prediction for complex systems. We select challenging physical dynamical systems from neuroscience (Hindmarsh-Rose model ), chemistry and biology (Reaction-Diffusion model ), and geophysics and fluid dynamics (Shallow Water Equations ), which exhibit rich spatiotemporal variations over long time scales.

  
**Model** & **Test Err** & **\# Params** & **FLOPS** & **Time** & **Mem** \\  NODE & 0.42\% & 0.22M & 2.11B & 9e-3s & 372MB \\ ANODE & 0.41\% & 0.22M & 2.12B & 9e-3s & 372MB \\ SONODE & 0.41\% & 0.22M & 2.11B & 9e-3s & 372MB \\ CSODE & 0.39\% & 0.22M & 2.11B & 9e-3s & 372MB \\   

Table 1: Performance Comparison on the MNIST

Figure 3: Qualitative comparison of the CSODE (top) and the NODE (bottom) predictions against ground truth trajectories at 400, 600, and 800 training epochs.

Figure 2: Comparison of Mean Absolute Error (MAE) loss reduction curves across 1500 training epochs between CSODE and NODE models until convergence.

### Model Architectures for Experiments

To comprehensively evaluate the performance of different model architectures in dynamic system modeling, this study compares several representative NODEs-based models, including the traditional NODEs , ANODEs , SONODEs , and our proposed CSODEs. Additionally, these models are compared with traditional MLPs  and Recurrent Neural Networks (RNNs) .

Notable that while CSODE can be integrated with the Latent ODE framework, as demonstrated in our preliminary experiments (in Section 5), we opted to conduct our main experiments without this integration. This decision was driven by our observation that NODE itself effectively models long-sequence dynamical systems, allowing us to evaluate CSODE's performance more directly by eliminating potential influences from additional Latent ODE components like encoders and decoders.

All models within our experiment that are based on the NODE framework utilize a uniform MLP to parameterize the core dynamical system \(=f(y)\). These models employ the forward Euler method for integration, though other numerical solvers like Dopri5 are also viable options (detailed performance comparisons of different solvers can be found in Appendix I.1). The MLP serves as the update function, iterating to act on the evolution of dynamic system states. Specifically, the MLP receives the current state as input and outputs the next state, thereby modeling the change in state over time. Meanwhile, the RNN employs the same structure as the MLP, using current input and the previous timestep's hidden state to update the current timestep's hidden state. To ensure a fair comparison, we finely align the number of parameters across all models, including the parameterization of the core dynamical system and other components (such as the subnetwork in the CSODE model and the augmented dimensions in the ANODE model). All designs ensure that the total number of parameters remains consistent within \( 1\%\).

The unique feature of CSODEs lies in the introduction of an additional subnetwork to model the control term \(g(u(t))\), extending the original dynamical system. We employ a unified auxiliary NN to model the changing rate of \(g(u(t))\), with the initial value \(g(u_{0})\) set to be the same as \(y_{0}\). These subnetwork structures are similar to the MLP or use simplified single-layer networks. We also introduce the CSODE-Adapt variant, replacing the function representing the changing rate of the control term with a network consisting of two convolutional layers, to explore the scalability and flexibility of CSODEs. Notably, for fair comparison of fundamental architectures, we used the standard Adam optimizer in our main experiments, though we note that alternative optimizers like L-BFGS could further enhance CSODE's performance (see Appendix I.2 for more details).

At the current research forefront, some researchers have proposed integrating NODEs with Transformer layers (TLODEs) . TLODEs can be considered a special case of CSODEs, where the Transformer layers implement the function \(f()\). Building on TLODEs, we introduce the control term \(g()\), creating a more complete CSODEs structure with Transformer layers (CSTLODEs). Considering the widespread application of Transformers in time-series prediction, we conduct comparative experiments between the Transformer, TLODE, and CSTLODE models. However, due to significant architectural differences between Transformers and MLPs and RNNs, directly incorporating them into the main experiment might introduce additional confounding factors, deviating from the theoretical discussion of general NODEs. Therefore, to maintain the focus and clarity of the main text, we have placed the experimental results and discussions related to TLODEs and CSTLODEs in Appendix F.

### Experimental Tasks and Datasets Description

In this subsection, we detail the experimental tasks and datasets used to explore the application of NNs in simulating various dynamic systems. For more comprehensive simulation and experimental details, see Appendix D.

Modeling Hindmarsh-Rose Neuron DynamicsIn this task, we explore the application of NNs in simulating the Hindmarsh-Rose neuron model . We validate their potential in simulating complex neuronal dynamics and assess their prospects in broader neuroscience and biophysical research. The Hindmarsh-Rose model is widely used to describe neuronal dynamic behavior, particularly suitable for studying neuronal firing behavior and chaotic phenomena. This model describes the evolution of the neuron's voltage and recovery variables over time with the following three coupled nonlinear differential equations:\[=y-ax^{3}+bx^{2}-z+I,=c-dx^{2}-y,= r(s(x-x_{0})-z),\] (3)

where \(x\) represents the membrane potential, \(y\) represents the recovery variable associated with the membrane potential, and \(z\) is the variable for adaptive current. The parameters \(a\), \(b\), \(c\), \(d\), \(r\), \(s\), \(x_{0}\), and \(I\) determine the neuron's nonlinear firing behavior, voltage response curve, recovery process rate, and adaptive current, simulating the voltage changes and behavior of the neuron.

We conduct 1000 simulations, each lasting 20 seconds. In the first 10 seconds of each simulation, we collect 1500 data points reflecting the changes in the neuron's three-dimensional coordinates (\(x,y,z\)). We divide these data points into 50 sequences, each containing 30 time points. Our goal is to use these sequences to train an NN to predict the dynamic changes over the next 10 seconds, specifically the sequences of the next 30 time points.

Modeling Reaction-Diffusion SystemIn this task, we employ NN models to simulate the Gray-Scott equations , a Reaction-Diffusion system widely used to describe chemical reactions and diffusion processes. This system is particularly significant in pattern formation, biological tissue, and self-organizing chemical processes, holding important theoretical and practical implications. This task allows us to explore the potential applications of these models in environmental and bioengineering contexts. The model describes the interactions and spatial diffusion of two chemical substances, \(U\) and \(V\), through a set of partial differential equations (PDEs):

\[=D_{U}^{2}U-UV^{2}+j(1-U),=D_{V}^{2}V+UV^{2}-(j+k)V,\] (4)

where \(D_{U}\) and \(D_{V}\) are the diffusion coefficients, and \(j\) and \(k\) are reaction rate parameters.

We conducted 1000 simulations with periodic boundary conditions  for 150 seconds, each randomizing initial conditions and diffusion coefficients \(D_{U}\) and \(D_{V}\), and used data from the first 100 seconds. These 100 seconds of data were divided into 100 sequences, each spanning 25 seconds, to train the NNs. Our goal is to utilize the training results to predict chemical dynamics over the next 50 seconds.

Modeling Shallow Water EquationsIn this task, we explore the application potential of NODE systems in simulating the Shallow Water Equations , which describe the horizontal flow and surface behavior of liquids in confined spaces. These equations have significant physical relevance in fields such as hydrology and environmental engineering, especially in flood simulations and wave dynamics analysis. Mathematically, the Shallow Water Equations are expressed through a set of nonlinear PDEs that represent changes in water depth (\(h\)) and flow velocity (\(\)), incorporating the principles of continuity and conservation of momentum, with gravity \(g\), as follows:

\[+(h)=0,)}{ t}+(h)+g h^{2}=0.\] (5)

We conduct 1000 periodic boundary condition wave propagation simulations  lasting 7 seconds, generating 1500 data points in the first 4.7 seconds to describe water wave depth variations. After normalizing the data using a sliding window, we segment it into 100 sequences, each containing 15 points, to train a global water surface dynamics model to predict water wave depth changes during the remaining 2.3 seconds of the simulation.

### Metrics for Assessing Prediction Accuracy

We employ the following metrics to compare time series predictions and ground truth:

**Mean Squared Error (MSE):** Calculates the mean of squared differences between predicted and actual values, sensitive to large errors. Used in all three tasks.

**Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and actual values, robust to noise. Used in all three tasks.

\(}\) **Score:** Quantifies the proportion of variance in predictions relative to actual data, reflecting explanatory power and accuracy. Used in the Hindmarsh-Rose model's 3D time series prediction.

**Chamfer Distance (CD):** Measures the average shortest distance between points in two sets, emphasizing spatial structure matching accuracy . Used to compare predicted and ground truth physical field sequences in Reaction-Diffusion systems and Shallow Water Equations tasks.

### Experimental Results and Analysis

Experiments are conducted ten times for each task, and the average metrics are presented in Table 2, the reader is referred to Appendix E for more statistical details. NNs in Group B, based on NODE and its variants, outperform traditional models in Group A, such as MLP and RNN, in predicting complex physical dynamic systems. Traditional models struggle with complex time dependencies and nonlinear dynamics, while models based on differential equations demonstrate greater adaptability and accuracy.

Within Group B, the CSODE model surpasses other models due to its control elements that enhance precision and adaptability to changes in initial conditions and system parameters. Figures 5 and 5 show that CSODEs accurately reflect complex dynamic system details through qualitative results, with more results available in Appendix G.

Moreover, the CSODE-Adapt model (Group C) integrates convolutional layers, enhancing its applicability and effectiveness, particularly in dynamic systems with significant spatial features, such as Reaction-Diffusion systems. This model performs better than all others, highlighting the flexibility and highly customizable structure of the CSODEs and its advantages and potential in predicting complex physical dynamic systems.

### Comparison with Observation-aware Baselines

We conducted supplementary experiments (CharacterTrajectories and PhysioNet Sepsis Prediction) with CSODE-Adapt following the experimental setup in , maintaining similar network structures,parameter counts, and optimization methods. Overall, CSODE performs slightly worse than Neural CDE in irregular observation experiments but better in other time-series-related tasks. We have also performed our main experiments for neural CDE. The corresponding results are shown as follows:

As shown in Table 3, for the **CharacterTrajectories** task with irregular observations, Neural CDE achieves the best performance across all missing data ratios, followed by CSODE and then ODE-RNN. In the **PhysioNet Sepsis Prediction** task, without considering observation intensity (OI), CSODE-Adapt achieves the highest AUC value of 0.871, while with OI consideration, Neural CDE performs best with an AUC of 0.885, followed closely by CSODE-Adapt and CSODE. For the **Reaction-Diffusion** modeling task, CSODE-Adapt demonstrates superior performance across all metrics (MSE, MAE, CD), while CSODE and Neural CDE show comparable performance, both outperforming ODE-RNN. Overall, while Neural CDE exhibits advantages in handling irregular observations, CSODE-Adapt shows competitive or superior performance in tasks requiring complex dynamic system modeling and clinical prediction, demonstrating its effectiveness as a general-purpose time series modeling tool.

## 7 Model Scaling Experiment

In the experiments above, CSODEs demonstrate significant superiority over traditional NODEs and their variants, under the maintenance of the same number of parameters and architectural configuration. Based on these findings, our scaling experiments focus primarily on exploring the scalability and architectural robustness of CSODEs, without further comparison to other models.

We also observe changes in system performance after scaling CSODEs. To maintain consistency in the experiments, each sub-network is configured with two dense layers. We select the Reaction-Diffusion task in Section 6 as an example to explore the impact of increasing the number of sub-networks and the width of each sub-network on system performance. Specifically, the network widths, which refer to the number of hidden dimensions in the dense layers, are set at 128, 256, 512, 1024, and 2048. The number of sub-networks, equivalent to \(M\) in NNs (1), is set at 1, 2, 3, 4, and 5.

The experimental design varies network width and number of sub-networks. We employ a learning rate formula: learning rate \(=}\), where \(k\) is a constant, \(W\) is the network width, and \(N\) is the number of sub-networks. This adjusts the learning rate based on width and moderates it for sub-network count to handle complexity. For instance, with a width of 1024 and three sub-networks, the learning rate is \(}\). We use the Adam optimizer for training.

In terms of overall performance, the heatmap in Figure 6 shows that under the CSODEs, increasing the network width and number of subnetworks results in stable and enhanced overall performance. Additionally, the scatter plot demonstrates that increasing the number of subnetworks significantly improves the model's generalization ability, with training and validation performance showing a stronger correlation. For further details on comparative experiments, the model's stability, and convergence despite the increase in the number of subnetworks and width, refer to Appendix H.

 
**Metric / Model** & **Neural CDE** & **ODE-RNN** & **CSODE** & **CSODE-Adapt** \\
**Group** & A & A & B & C \\   \\
30\% Missing & 97.8\% & 96.8\% & 97.3\% & 97.1\% \\
50\% Missing & 98.2\% & 96.5\% & 97.8\% & 97.6\% \\
70\% Missing & 97.2\% & 95.9\% & 96.8\% & 96.5\% \\   \\ w/o UI & 0.865 & 0.855 & 0.868 & 0.871 \\ w/ OI & 0.885 & 0.870 & 0.881 & 0.883 \\   \\ MSE & 7.1e-3 & 7.5e-3 & 7.0e-3 & 6.8e-3 \\ MAE & 0.60 & 0.62 & 0.59 & 0.58 \\ CD & 0.945 & 0.950 & 0.942 & 0.940 \\   

Table 3: Performance comparison with observation-aware neural networks. Group A includes observation-aware baselines like ODE-RNN and Neural CDE; Group B contains our base model CSODE; Group C showcases CSODE-Adapt with enhanced observation mechanisms. Best performing models are marked in blue, second-best in brown.

## 8 Conclusion

In this work, we analyzed the learning and predicting abilities of Neural ODEs (NODEs). A class of new NODEs: ControlSynth Neural ODEs (CSODEs) was proposed, which has a complex structure but high scalability and dexterity. We started by investigating the convergence property of CSODEs and comparing them with traditional NODEs in the context of generalization, extrapolation, and computational performance. We also used a variety of representative NODE models and CSODEs to model several important real physical dynamics and compared their prediction accuracy.

We presented that although the control subjects (NODEs, Augmented Neural ODEs (ANODEs), and Second Order NODEs (SONODEs)) do not have the physics information-based inductive biases specifically owned by our CSODEs, they can learn and understand complex dynamics in practice. In particular, SONODEs own inductive biases for second-order ODE-formulated dynamics, while the ones of CSODEs mainly are for first-order models with high nonlinear natures, scalability, and flexibility that belong to a broad class of real systems. The experimental results on dynamical systems governed by the Hindmarsh-Rose Model, Reaction-Diffusion Model, and Shallow Water Equations preliminarily demonstrate the superiority of our ControlSynth ODEs (CSODEs) in learning and predicting highly nonlinear dynamics, even when represented as partial differential equations.

Limitations and Future WorkThe effectiveness of inductive biases of CSODEs varies which, depending on the specific application scenarios, may not be preferable; in the evolution of partial differential equations, there often exists mutual interference between different scales (_e.g._, spatial and temporal scales), which, however, is approximately learned by CSODEs. We believe this work could provide promising avenues for future studies, including: For enlarging the use scope of inductive biases of CSODEs in complex dynamics and reflecting the mutual intervention between scales, one can consider a more general NODE: \(=f(x,u)\), with guaranteed stability and convergence. This allows a greater scope of dynamics and thus may prompt the improvement of the accuracy of modeling and predicting systems with more complex structures and behaviors. Furthermore, in practice, CSODEs are more sensitive to learning rate selections due to their more complex architectures. Our preliminary investigation in Appendix I.3 reveals the significant impact of hyperparameter adjustments on model performance. Building upon these initial findings, future research will examine this sensitivity more thoroughly and consider methods like adaptive learning rate adjustment and model simplification to address it. Finally, this work maintained standard optimization settings for fair comparison, future research could explore specialized training algorithms that leverage CSODE's structural properties and theoretical foundations.

Figure 6: Performance comparison of CSODE models with varying numbers of sub-networks. The scatter plot visualizes the performance trajectory of CSODE models during training, where each point represents the training loss (x-axis) and validation loss (y-axis) at a specific epoch. Models with 1-5 sub-networks are compared, each depicted in a distinct color while maintaining a fixed network width of 512 neurons. Points clustering near the bottom-left corner indicate superior model performance, while their distribution relative to the diagonal reveals the balance between training and validation performance.