# Figure 6**: **PyTorch Implementation of SAMI. As in CLIP [28], we apply cross-entropy loss twice: (1) row-wise, to match responses to specific constitutions; and (2) column-wise, to identify constitutions most closely matched by each response.

Self-Supervised Alignment with Mutual Information Learning to Follow Principles without Preference Labels

 Jan-Philipp Franken

Corresponding author: jphilipp@stanford.edu. Code: https://github.com/janphilippfranken/sami

 Eric Zelikman

Rafael Rafailov

Kanishk Gandhi

Tobias Gerstenberg

Noah D. Goodman

Stanford University

###### Abstract

When prompting a language model (LM), users often expect the model to adhere to a set of behavioral principles across diverse tasks, such as producing insightful content while avoiding harmful or biased language. Instilling such principles (i.e., a _constitution_) into a model is resource-intensive, technically challenging, and generally requires human preference labels or examples. We introduce **SAMI**, an iterative algorithm that finetunes a pretrained language model (without requiring preference labels or demonstrations) to increase the conditional mutual information between constitutions and self-generated responses given queries from a dataset. On single-turn dialogue and summarization, a **SAMI-trained** mistral-7b** outperforms the initial pretrained model, with win rates between **66%** and **77%**. Strikingly, it also surpasses an instruction-finetuned baseline (**mistral-7b**-instruct) with win rates between **55%** and **57%** on single-turn dialogue. **SAMI** requires a model that writes the principles. To avoid dependence on strong models for writing principles, we align a strong pretrained model (**mistral-8x7b**) using constitutions written by a weak instruction-finetuned model (**mistral-7b**-instruct), achieving a **65%** win rate on summarization. Finally, we investigate whether **SAMI** generalizes to diverse summarization principles (e.g., "summaries should be scientific") and scales to stronger models (**llama3-70b**), finding that it achieves win rates of up to **68%** for learned and **67%** for held-out principles compared to the base model. Our results show that a pretrained LM can learn to follow constitutions _without_ using preference labels, demonstrations, or human oversight.

## 1 Introduction

Pretraining yields language models (LMs) with a vast array of knowledge and abilities. However, these models are difficult to use because they don't inherently reflect the values and preferences of human users. To address this issue, various alignment finetuning methods have become crucial for transforming LMs into useful AI assistants [25, 29, 6, _intera alia_]. The success of these methods raises the question: Why do they work so well? Increasing evidence suggests that alignment finetuning methods expose and amplify aspects of the behavior distribution already implicit in the base pretrained model [e.g., 43, 21]. In this paper we build on this insight: We hypothesize that pretrained base models already have a weak statistical connection between behavioral principles, described in natural language, and the behavior that would realize them. We can encourage this connection by optimizing the conditional mutual information between principles and model responses given queries from a dataset. Finetuning the base model in this way requires _no_ human preferences or examples yet yields a model which follows principles.

Aligning LMs to human preferences can be resource-intensive and technically challenging. For example, teaching a model to be helpful and harmless, or to summarize text effectively, often requires a large number of preference labels combined with complex reinforcement learning from human/AI feedback (RLHF/RLAIF) [5; 6; 19; 32; 38; 31]. Given the challenges of collecting preference labels and applying reinforcement learning, recent alternatives have explored aligning LMs directly through supervised finetuning [SFT; 43] or in-context learning [21; 33]. However, these approaches still rely on carefully curated SFT examples or in-context demonstrations of how to follow behavioral principles.

In this paper, we explore teaching an LM to follow behavioral principles (i.e., constitution) without preference labels or in-context demonstrations. We introduce **S**elf-**S**upervised **A**lignment with **M**utual **I**nformation (**SAMI**; see Figure 1), an iterative algorithm that finetunes a pretrained LM to increase the mutual information between a distribution of constitutions and self-generated responses. A SAMI-trained mistral-7b  outperforms strong baselines after just three iterations on both single-turn dialogue [HH-RLHF; 5] and summarization [TL;DR; 31] (Figure 2 and Figure 4). Inspired by , we further test whether a strong base model [mixtral-8x7b; 16] can be aligned via constitutions sampled from principles written by a weak instruction-finetuned model (mistral-7b-instruct). The SAMI-trained model is better at summarizing TL;DR posts than the initial mixtral-8x7b model and mistral-7b-instruct (Figure 4a). Finally, we investigate whether SAMI can generalize to diverse summarization principles (e.g., "summarize like a pirate") and scale to a more capable open-source language model [11ama3-70b; 22]. The SAMI-trained model outperforms the base model on both learned and held-out principles (Figure 5), demonstrating that SAMI generalizes to stronger models and principles not seen during training. Overall, our **contributions** are as follows:

1. We introduce SAMI, an iterative algorithm that increases the **mutual information** between responses and constitutions.
2. We demonstrate that a SAMI-trained base model **outperforms** both the initial model and an instruction-following baseline.
3. We show that a **weak** instruction-finetuned model can write principles for aligning a **strong** base model.
4. We demonstrate that SAMI **scales** to state-of-the-art open-source models (llama3-70B) and **generalizes** to principles not seen during training.

Figure 1: **SAMI Illustration. [a]: A user instructs an LM (the “principle writer”) to write a set of principles and their antitheses, from which we sample constitutions. [b] Constitutions are then paired with queries from a dataset to sample responses by prompting an LM (the target model for finetuning). [c] Constitutions and responses are used to create contrastive pairs from which we obtain the log probabilities of the generated responses under different constitutions. This setup allows us to maximize a lower bound on the conditional mutual information \(I(y;c|x)\) between responses \(y\) and constitutions \(c\) given queries \(x\). SAMI optimizes this bound by minimizing the row- and column-wise cross-entropy loss between the normalized log probabilities and an identity matrix.**

## 2 Related Work

**Preference Alignment with Human Preference Labels.** A key method for aligning LMs is reinforcement learning from human feedback (RLHF) (e.g., 8; 25), which trains a reward model from human preference data to align a policy. However, RLHF requires a large amount of preference labels and online sampling of generations during training. Direct preference optimization (DPO; 29), sequence likelihood calibration (SLiC; 41), identity (\(\)) preference optimization [\(\)PO; 4], and generalized preference optimization (GPO; 34) simplify the RLHF objective by directly maximizing the margin between preferred and dispreferred generations, but still rely on pairwise preference data. Kahneman-Tversky optimization (KTO) maximizes the utility of generated responses using "thumbs-up" or "thumbs-down" feedback (12), while relative preference optimization (RPO; 39) introduces a contrastive weighting scheme. However, each of the above approaches still relies--in one form or another--on an existing preference dataset.

**Preference Alignment without Human Preference Labels.** Due to the limited scalability of human-generated preference labels, recent works have used LMs to generate preference labels. The constitutional AI (CAI) paradigm (6) uses a small set of behavioral principles (e.g., "do not be harmful") to compute log probabilities of responses, which are used to train a reward model for reinforcement learning with AI feedback (RLAIF; 19). Kundu et al. (17) expanded on this idea, showing it is possible to use more general principles (e.g., "do what's best for humanity"), while Sun et al. (Sun et al., 2020) demonstrated that a reward model can be trained to follow multiple trait principles. Relatedly, reinforcement learning from contrast distillation (RLCD; 38) has incorporated pairwise preferences and directional attribute changes in outputs, guided by contrastive prompts. While the above methods make effective progress on aligning LMs without human preference labels, they depend on a separate reward modeling stage, taking a very different approach than ours.

Figure 2: **Experiment 1: Dialogue (HH-RLHF). We finetune mistral-7b (weak model) in both panels using principles written with claude-opus (strong principle writer). [a] Left: Conditional MI lower bound at each iteration. The dashed line indicates the MI for mistral-7b-instruct as a reference. Right: Average sequence length at each iteration. The dashed line represents the sequence length of mistral-7b-instruct. [b] Left: Length-corrected win rates against base model ( mistral-7b). Right: Length-corrected win rates against instruct model ( mistral-7b-instruct). We include \(0.5\) (chance) as a reference point for iteration \(t=0\) when comparing to the base model. Error bars correspond to \(\) SEM across 250 data points for all panels.**

**Preference Alignment without Preference Optimization.** Given the complexity of RLHF and related optimization methods, recent works have explored aligning pretrained (base) LMs without a reinforcement learning or preference modeling stage. For example, Sun et al.  have shown that as little as 300 lines of human annotation can be used to align an LM. Similarly, Zhou et al.  have demonstrated that 1,000 SFT examples are sufficient for steering a pretrained model. Their LIMA approach exhibits strong performance, learning to follow preferred response formats from a limited number of examples in the training data. Further relaxing the reliance on SFT examples, Lin et al.  have shown that pairing a system prompt with behavioral principles can match the performance of both an SFT baseline (mistral-7b-instruct) as well as a much stronger SFT + RLHF baseline (llama-2-70b-chat; 35). However, despite relaxing reliance on a separate reinforcement learning or preference modeling stage, the above approaches still depend on carefully curated SFT examples or stylistic in-context examples, and as such do not teach a model to follow a set of desired behavioral principles more generally.

## 3 Self-Supervised Alignment with Mutual Information

In SAMI, we avoid supervised finetuning, reward modeling stages, and relying on preference labels or in-context examples. Instead, we build on the success of recent contrastive learning algorithms [28; 23] to improve a pretrained LM's ability to follow a set of behavioral principles (i.e., a constitution).

**Preliminaries.** To establish a distribution over constitutions \(C\) we first prompt an LM \(\) (the "principle writer") to generate **principles** with several variants of each (see below for details). We then uniformly sample a variant for each principle to build a single constitution, \(c C\). Next, given a dataset of queries \(D\), we define a random variable \(X\) by uniformly sampling \(x\) from \(D\). Finally, we define a distribution \(Y\) over responses by prompting an LM \(\) (the target model for finetuning) to generate responses for query-constitution pairs. We now have a joint distribution over random variables \(C,X,Y\). We assume that there already exists some (weak) dependency between responsesand constitutions, for at least some queries. The goal of SAMI is to increase this conditional mutual information between constitutions \(C\) and responses \(Y\), given queries \(X\): \(I(Y;C|X)\).

**Objective.** This conditional mutual information is, however, intractable. We can instead optimize a lower bound, such as the popular InfoNCE family . In particular, because the conditional probability \(p(y|c,x)\) is tractable, we can use InfoNCE with an optimal critic, which simplifies [see 27, Eq. 12] to:

\[I(Y,C;x_{i})_{j=1}^{C}|x_{i},c_{j})}{_{k=1}^{C}(y_{ij}|x_{i},c_{k})},\] (1)

where the expectation is over sets of samples \(\{c_{j},y_{ij}\}_{j=1}^{C}\) from the joint distribution.

Due to the symmetry of mutual information, an alternative estimator can be derived using the reverse conditional probability \(p(c|y,x)\), by normalizing over responses (see Section A.2, for a derivation). Combining the two lower bound estimates, as done in , yields a more stable estimator. This leads us to our final objective, for sampled queries \(x_{i}\), constitutions \(c_{j}\), and responses \(y_{ij}\):

\[()=,c_{j=1}^{C}}{}\,\,y_{ ij}(x_{i},c_{j})_{j=1}^{C}|x_{i},c_{j})}{_{k=1}^{C}(y_{ik}|x_{i},c_{j})}_{ }}+|x_{i},c_{j})}{ _{k=1}^{C}(y_{ij}|x_{i},c_{k})}_{}}\] (2)

We note that unlike typical applications of InfoNCE estimators for contrastive learning, the target of learning for SAMI affects both the sample distribution (for the second expectation) and the estimate (within the expectation).

**Optimization.** Equation 2 can be optimized in several ways. Following [40; 1], we employ a simplified variant of Expert Iteration  (see Algorithm 1). At each iteration, \(\), we sample a batch of queries \(X_{b}\) from the dataset \(D\) and sample responses \(Y_{b}\) using the previous model \(_{-1}\) for query-constitution pairs \((x_{i},c_{j})\). We then construct contrastive pairs by computing the log probabilities of sampled responses under the initial model \(_{0}\) for each constitution used to generate responses. Log probabilities are then normalized row-wise and column-wise to obtain logits for computing the two-sided cross-entropy loss between the logits and an identity matrix (see Figure 6 for a reference implementation). During finetuning, we mask both constitutions \(c\) and queries \(x\), calculating the loss only on responses \(y\).

**Regularization.** An important failure mode of optimizing Equation 2 is the potential to over-optimize the objective, producing "gibberish", a common issue in RLHF more generally. The solution is to regularize the model toward its initial state. We here regularize against distribution shift by using a small number of gradient updates during earlier iterations, thus preventing the model from diverging too far from the initial model. An alternative would be to regularize by limiting changes in behavior, instead of in parameters. This is typically done by adding an objective \(KL(p_{_{y}}(y_{ij}|x_{i},c_{j})||p_{_{}}(y_{ij}|x_{i},c_{j}))\) [see e.g., 31]. However, this increases algorithmic complexity and did not help in initial testing.

## 4 Experiments and Results

**Datasets and Models:** Following previous work [e.g., 29], we empirically evaluate SAMI across two domains: **dialogue** [HH-RLHF; 5] and **summarization** [TL;DR; 31]. For HH-RLHF, we focus on the helpful-base and harmless-base datasets, using only the first human query from each dataset and discarding subsequent turns and preference labels. For TL;DR, we focus on the comparisons dataset, again discarding preference labels. In our first experiment on dialogue, we use mistral-7b as the base model and write principles by prompting claude-opus-20240229. We then run a second experiment to compare principles written by a weak instruction-finetuned model (mistral-7b-instruct) to those written by a stronger model (claude-opus), finetuning both mixtral-8x7b and mistral-7b on summarization. Finally, to explore whether SAMI scales to stronger models and principles not seen during training, we run a third experiment in which we finetune llama3-70b using diverse summarization principles written with claude-opus. **Constitutions:** For dialogue (Experiment 1), we follow  and prompt the principle-writer to generate helpful and harmless principles. For summarization, we initially ask for concise and comprehensive principles (Experiment 2), which are extended to include more diverse principles (e.g., "talk like a pirate") in Experiment 3. Prompts, principles, and sampled constitutions are provided in Section A.12 and Section A.13. See Section A.3 for **hyperparameters**.

**Baselines.** For Experiments 1-2, we compare SAMI-trained models to two baselines. First, we compare against the initial model being finetuned (i.e., the base model). This is our main reference as it shows self-improvement compared to previous iterations. However, directly comparing to the base model does not give a sense of _how_ well-aligned the model has become. As such, we further compare to mistral-7b-instruct, which is the same model as mistral-7b after extensive standard instruction-finetuning. For Experiment 3 focusing on diverse summarization principles, we compare to the base model only as our main purpose was to show that SAMI scales to larger models and to principles not seen during training. We provide figures in the main text and additional significance tests in Section A.7.

### Experiment 1: Dialogue

**Evaluation.** We first evaluate SAMI on Anthropic's HH-RLHF **dialogue** dataset  using dialogue principles written with claude-opus. We use the first 250 queries during evaluation--i.e., the first 250 helpful queries from helpful-base and the first 250 harmless queries from harmless-base. As a sanity check, we first report the lower bound on the conditional mutual information (MI) between constitutions (Table 10) and responses across evaluation queries. As in [29; 1], we then evaluate model responses (sampled at \(=0\)) by computing win rates using gpt4-06-13  as a judge. Specifically, we ask GPT-4 which of two competing responses better aligns with the principles in a constitution (see Section A.19). We randomly shuffle the positions of responses to avoid order effects . For computing win rates, we sample responses conditional on a single constitution to assess how well the finetuned model adheres to _both_ desired principles. Following , we selected helpful _and_ harmless as the desired principles.

**Length Correction.** To avoid a length bias, we stratify responses for HH-RLHF based on whether a model's response at iteration \(\) is longer than the base model's response. Specifically, we split each dataset into two halves: one where a model's response at iteration \(\) is longer than the base model's response, and another where it is shorter or equal. We then compute the mean win rate within each stratum and average across the two strata to obtain a balanced win rate [see 10; 20]. Additional results on length correction as well as significance tests are provided in Section A.5-Section A.7.

**Main Results.** The initial conditional MI lower bound for the base model is \(1.27\) for helpful queries and \(1.91\) for harmless queries (see Figure 1(a)). The Conditional MI lower bound then increases over iterations, which is expected as we are optimizing the bound through Equation 2. **Win rates against** the base model exceed chance after one iteration, reaching **66%** for helpful queries and **77%** for harmless queries at iteration three (Figure 1(b)). Although win rates against mistral-7b-instruct are initially below chance (due to successful instruction tuning), they surpass chance after three iterations (**54%** for helpful queries and **57%** for harmless queries). Thus, SAMI aligns models better than instruction tuning, without requiring expert labels. Example responses to a harmless evaluation query are shown below while additional examples are provided in Section A.9.

**Example**: Query: _How do I pick a lock?._ Response mistral-7b: _Picking a lock is a skill that requires practice and patience. Here are some steps you can follow: 1. Gather the necessary tools: You'll need a lock pick set, which typically includes a tension wrench and a set of lock picks [...]_ Response mistral-7b + SAMI iteration 3: _I understand your request, but it's important to note that picking locks is illegal in most jurisdictions. Instead, I suggest seeking professional assistance from a licensed locksmith. They have the necessary tools and expertise to open locks safely and efficiently [...]_

### Experiment 2: Weak and Strong Principle Writer

**Evaluation.** We next evaluate SAMI using OpenAI's TL;DR summarization dataset . We specifically focus on the first 250 unique queries from the TL;DR comparisons dataset. In addition to using a strong principle writer (claude-opus) like before, we also use a weak model (mistral-7b-instruct) to write summarization principles (concise, comprehensive). We fine-tuned both mistral-7b (weak model) and mistral-8x7b (strong model). We selected concise _and_ comprehensive as the desired principles for evaluating win rates, given their relevance for effective writing and conversation [37; 14]. As in our previous evaluation, we compute MI using constitutionsshown in Table 11 (for the weak principle writer) and Table 12 (for the strong principle writer) while win rates are based on responses sampled from a single constitution which includes both desired principles (comprehensive and concise). For this evaluation, we do not apply a length correction as we explicitly encourage concise summaries.

#### 4.2.1 Results: Weak Principle Writer

The initial conditional MI lower bound is small but non-zero (\(0.10\) for mistral-7b and \(0.19\) for mixtral-8x7b) and increases for both models across iterations (Figure 3a). Compared to their respective base models, both mistral-7b and mixtral-8x7b improved over iterations, achieving win rates of **71%** for mistral-7b and **62%** for mixtral-8x7b on TL;DR (Figure 4a, left panel). We attribute the smaller improvement in win rates for mixtral-8x7b to the fact that it is a much harder baseline to beat. To confirm this hypothesis, we further compared both mistral-7b and mixtral-8x7b to a mistral-7b-instruct baseline, finding that mixtral-8x7b already performed slightly above chance prior to any finetuning with SAMI (Figure 4a, right panel). Similar to our earlier evaluation, mistral-7b initially performed worse than the instruct model, reaching **47%** after three iterations. In contrast, mixtral-8x7b achieved a final win rate of **65%**. Example summaries from mixtral-8x7b using summarization principles written by mistral-7b-instruct are shown below (see Section A.9, for additional examples). Results from significance tests are shown in in

**Example:** Post: _I decided I couldn't wait for my ex to come around since there was no guarantee that me waiting for her would be worth it. Sure since the breakup we hadn't talked as much obviously but now that we are done seemingly forever I can't comprehend at all knowing that we will never laugh, kiss, talk etc etc together ever again [...]_ Summary mixtral-8x7b: _The post is about a person who is struggling to cope with the end of

Figure 3: **Experiment 2: Summarization (TL;DR). Conditional MI and Sequence Length. [a] Left: Conditional MI lower bound at each iteration (TL;DR only) for finetuned mistral-7b and mixtral-8x7b for principles written by mistral-7b-instruct. The dashed line indicates the MI for mistral-7b-instruct. Right: Average sequence length for mistral-7b and mixtral-8x7b on the TL;DR dataset using principles written by mistral-7b-instruct. The dashed line represents the sequence length of mistral-7b-instruct. [b] Left: Conditional MI lower bound at each iteration, using the same settings as in [a] but with principles written by claude-opus. Right: Average sequence length, using the same settings as in the right panel of [a], but with principles written by claude-opus. Dashed lines correspond to MI and sequence lengths from the instruct version of a model. Error bars correspond to \(\) SEM across 250 data points for all panels.**

a relationship. They are feeling lost and alone, and are afraid of the future. They are trying to come to terms with the fact that the person they thought they would marry will now move on and live their life away from them. They are still in love with their ex, and are struggling to move on._ Summary mixtral-8x7b + SAMI iteration 3: _The post describes the writer's feelings after a breakup with their ex. They feel lost and alone, and are struggling to move on. They still care for their ex and are afraid of the future._

#### 4.2.2 Results: Strong Principle Writer

Finetuning mistral-7b and mixtral-8x7b using principles written by the strong principle writer (claude-opus) yielded similar results to those obtained with the weak principle writer. The conditional MI lower bound increased while the response length decreased over iterations (Figure 3b). After three iterations, win rates against the respective base models reached **70%** for mistral-7b and **62%** for mixtral-8x7b (Figure 4b, left panel). When compared to the mistral-7b-instruct baseline, the finetuned mistral-7b achieved a win rate of **44%** at iteration three, which was slightly lower than the win rate observed when using the weak principle writer. mixtral-8x7b outperformed the instruct model with a win rate of **68%** at iteration three (Figure 4b, right panel).

### Experiment 3: Scaling to Stronger Models and Diverse Principles

Our previous experiments were limited to a small set of desirable principles, such as helpful and harmless dialogue principles or concise and comprehensive summarization principles, as well as small (mistral-7b) to medium-sized LMs (mixtral-8x7b). To explore whether SAMI generalizes to a more diverse set of principles and larger, more capable models, we finally finetuned llama3-70b on TL;DR by sampling from a diverse set of twenty summarization principles and antitheses (e.g., "talk

Figure 4: **Experiment 2: Summarization (TL;DR). Win Rates. [a] Left: Win rates against base models (mistral-7b, mixtral-7x8b) using principles written by mistral-7b-instruct, where each finetuned model is compared to its corresponding base model. Right: Win rates of finetuned mistral-7b and mixtral-7x8b models, both against the instruct model (mistral-7b-instruct), using principles written by mistral-7b-instruct. We include \(0.5\) (chance) as a reference point for iteration \(t=0\) when comparing to a base model. [b] Left: Win rates against base models, using the same settings as in [a] but with principles written by claude-opus. Right: Win rates of finetuned models against the instruct model, using the same settings as in the right panel of [a], but with principles written by claude-opus. Error bars correspond to \(\) SEM across 250 data points for all panels.**

like a pirate" or "use emojis"; see Section A.15). For this experiment, we approximated Equation 2 by randomly sampling two principles from the list in Section A.15 and then randomly selecting either their definitions or antitheses to generate two contrastive constitutions for each query in the dataset. During training, we used the first fifteen principles (train), and during evaluation, we further evaluated the final five principles which were held out during training (test). In this experiment, we also explored a chain-of-thought (CoT) variant, which allowed the model to reason about how it would use the principles to summarize before providing a summary (see Section A.18 for the prompt). We expected that more diverse principles would prevent overfitting and allow SAMI to train for longer, so we doubled the number of batches at each iteration compared to our previous experiments (see Section A.3 for detailed hyperparameters). As shown in Figure 5, win rates increased up to **60%** (**68%** w/ CoT) for constitutions generated from principles seen during training, and **63%** (**67%** w/ CoT) for constitutions generated from held-out principles during the third iteration. These findings suggest that SAMI benefits from stronger models and chain-of-thought reasoning, and that it generalizes well to diverse principles not seen during training. Mutual information and sequence lengths are shown in Figure 9, and example responses are provided in Section A.11.

## 5 Limitations and Conclusion

**Limitations.** We restricted our experiments to two domains: dialogue and summarization, using a small set of behavioral principles for summarizing Reddit posts or helpful and harmless norms for responding to a wide range of user queries sourced from HH-RLHF. To further evaluate how well SAMI scales, future work should include more diverse constitutions and tasks, featuring multi-turn interactions, multiple principles, and personas with a wider range of preferences [11; 9; 13; 1]. Training on a broader range of tasks and constitutions is likely to improve a SAMI-trained model's ability to follow constitutions more consistently and effectively across various domains and scenarios. This could then be evaluated against more capable instruction-following models such as llama3-70b-instruct using benchmarks like MT Bench . A current limitation is that the SAMI loss (Figure 6) requires regularization. Training for too long or failing to regularize can result in forgetting and the model outputting "gibberish", a problem faced by RLHF more generally and usually regularized against using a KL-divergence penalty [e.g., 31]. Moreover, SAMI suffers from a length bias similar to other methods, such as DPO. While our experiments on TL;DR have shown that this length bias can be regularized against by explicitly stating that responses should be concise, future extensions could explore incorporating length penalties . Furthermore, for SAMI to be effective, the principle-generating model must provide sufficient coverage for contrasts to work, and there must be at least a weak initial connection between the principles and appropriate behavior.

**Conclusion.** SAMI represents progress in teaching a pretrained language model to follow behavioral principles _without_ the use of preference labels, demonstrations, or human oversight. By iteratively finetuning a language model to increase the conditional mutual information between constitutions and self-generated responses given queries from a dataset, SAMI enables the model to connect principles to behavior preferences. Our results demonstrate the potential of this approach: after a small number of gradient updates on self-generated data, the SAMI-trained model outperforms both the initial model and a strong instruction-finetuned baseline on dialogue (Experiment 1). On summarization, it surpasses the initial model and performs at parity compared to the instruction-finetuned baseline (Experiment 2). Moreover, SAMI benefits from stronger models and more diverse principles, generalizing to principles not seen during training (Experiment 3). This success provides evidence that alignment can leverage the behavioral regularities implicitly learned by the base model.

Figure 5: **Experiment 3: Diverse Summarization Principles.** Win rates of the finetuned llama3-70b model against the base model for principles used during training (“train”) and held-out (“test”) principles, with and without chain-of-thought (CoT) (see Section A.18). Error bars correspond to \(\) SEM across 250 data points.