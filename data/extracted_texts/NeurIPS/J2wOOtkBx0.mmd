# DiffuBox: Refining 3D Object Detection with Point Diffusion

Xiangyu Chen\({}^{*,1}\)

\({}^{1}\)Zhenzhen Liu\({}^{*}\)\({}^{1}\)Katie Z Luo\({}^{*}\)\({}^{1}\)Siddhartha Datta\({}^{2}\)

**Adhitya Polavaram\({}^{1}\)Yan Wang\({}^{3}\)Yurong You\({}^{3}\)**Boyi Li\({}^{3}\)**Marco Pavone\({}^{3}\)

**Wei-Lun Chao\({}^{4}\)Mark Campbell\({}^{1}\) Bharath Hariharan\({}^{1}\)**Kilian Q. Weinberger\({}^{1}\)

\({}^{1}\)Cornell University \({}^{2}\)University of Oxford \({}^{3}\)NVIDIA Research \({}^{4}\)The Ohio State University

Denotes equal contribution.Correspondences could be directed to xc429@cornell.edu

###### Abstract

Ensuring robust 3D object detection and localization is crucial for many applications in robotics and autonomous driving. Recent models, however, face difficulties in maintaining high performance when applied to domains with differing sensor setups or geographic locations, often resulting in poor localization accuracy due to domain shift. To overcome this challenge, we introduce a novel diffusion-based box refinement approach. This method employs a domain-agnostic diffusion model, conditioned on the LiDAR points surrounding a coarse bounding box, to simultaneously refine the box's location, size, and orientation. We evaluate this approach under various domain adaptation settings, and our results reveal significant improvements across different datasets, object classes and detectors. Our PyTorch implementation is available at https://github.com/cxy1997/DiffuBox.

## 1 Introduction

3D object detection is a fundamental task for embodied agents to safely navigate in complex environments. For autonomous vehicles to navigate complicated traffic conditions, this amounts to identifying and localizing other road agents. Detection models under this setting need to make sense of LiDAR point clouds to identify accurate bounding boxes for pre-specified objects. Given the diverse driving environments that occur in practice it is common, however, for the train- and test-time distributions to differ significantly. Domain distributional differences mainly arise from differences in object size, point cloud density, and LiDAR beam angles. Consequently, models trained in one region or particular dataset (_e.g_. Germany) may not perform well in another region or dataset (_e.g_. USA) . As a result, the domain adaptation problem raises concerns over the reliability and safety of 3D object detection in self-driving, that are often trained in a particular setting, then deployed into a diverse set of regions and locations.

Wang _et al_.  have obtained reductions in the domain adaptation gap by resizing boxes with a simple scaling heuristic after the fact. Consequently, we share the belief that the performance gap associated with domain adaptation is dominated by incorrect box sizes, shapes, and orientations, rather than false positives and negatives in detections -- _e.g_. a model trained in Germany can detect US cars, but struggles to capture their larger dimensions.

In this paper we observe that, although the relationship of bounding boxes to the surrounding environment varies across domains, the relative position of LiDAR points with respect to their bounding boxes is surprisingly consistent . Bounding boxes of these detections are, by definition, supposed to tightly fit the corresponding objects. Furthermore, the objects within the same object class (_e.g_., cars) have a similar shape with minor variance across different domains. What mostlyvaries, then, is the dimensions of the object, as opposed to this "surface" shape when normalized to be the same size. Specifically, the distribution of points the LiDAR detector receives, is therefore consistent when normalized across object sizes, regardless of domains; points will always land near the edge of the bounding box no matter where the object is located. Thus, if we can somehow capture the distribution of points relative to a box's coordinate system, we would be able to use same process to fix incorrectly positioned bounding boxes to fit the correct point-distribution, even across domains.

Recognizing this observation, we propose DiffuBox, a novel point diffusion model that learns the distribution of points relative to the object's bounding box in order to refine noisy bounding box proposals from the detection models for off-the-shelf domain adaptation. Given a set of noisy bounding box proposals, DiffuBox denoises them into accurate detection boxes conditioned on the points near proposed bounding boxes. Our method naturally avoids the domain gap caused by the scale difference , since DiffuBox is designed to operate on object scale-invariant data, where we transform the LiDAR points around bounding box proposals into a normalized box view that is relative to the box instead of in absolute measure. This eliminates the size priors presented in the source domain and forces the diffusion model to recover the accurate bounding box solely based on the relative position of points to the bounding box proposal, allowing for improved robustness in self-driving systems.

To summarize, our contributions include: We empirically validate our method, DiffuBox, by adapting models trained on a dataset from Germany (KITTI ) into two large, real-world datasets from the USA (Lyft L5  and Ithaca365 ). Under both settings, we observe that DiffuBox is able to refine the output bounding boxes drastically from the noisy initial predictions (Figure 1). Quantitatively, we observe strong improvements in mAP performance (up to 24 mAP), particularly in near-range boxes, where more points are present for DiffuBox to refine the box predictions. When paired with a representative set of domain adaptation methods, including Output Transform, Statistical Normalization , and Rote-Domain Adaptation , DiffuBox is able to further improve the results, and closing the gap between all method's final performance.

## 2 Related Work

3D Object Detection.In general, most 3D object detection methods require supervision from human-annotated data. They take 3D sensory data (e.g. LiDAR point clouds) and infer bounding boxes around 3D objects. 3D detection methods can be grouped into two categories based on the input representations: Point-based methods [32; 33; 31; 39; 40; 55; 29] that directly operate on point clouds, and grid-based methods [51; 65; 20; 38; 49; 26] that first voxelize point clouds into 3D grids and then leverage convolutional architectures. Like other supervised models, 3D detection models suffer from decreased performance when the data distribution during inference differs from that during training. Our method DiffuBox is designed to reduce the domain gap for general 3D object detection, agnostic to underlining model design.

Domain Adaptation in 3D.Domain adaptation aims to alleviate the performance drop of 3D perception models under domain shift.  by Wang _et al_. is one of the first works studying the domain gap in 3D object detection and proposes Statistical Normalization (SN) that reduces the shape bias across domains. ST3D , Rote-DA , and ST3D++  propose a self-training pipeline that iteratively improve the target domain 3D detection performance with pseudo-label training

Figure 1: **Box refinement through denoising steps. We visualize the correction of a noisy prediction, shown in yellow, using DiffuBox. The ground truth box is visualized in green for reference. Boxes being refined are colored blue based on timestep. The output is refined iteratively though the denoising steps, resulting in the final, corrected output of our method.**

and auxiliary priors. Other methods can be grouped into feature-based [50; 21; 28; 19; 37; 17] and architecture-based [12; 35; 46; 22; 36] methods. Some of them also apply data augmentation to construct and train domain-invariant representations to reduce the domain gap [19; 37; 12]. Our proposed method is orthogonal to these methods and can be applied together with these models.

Diffusion Models.Recently, diffusion models [41; 9; 42; 43] have shown high-quality generative ability for image [4; 34], video [10; 8] and 3D shape [25; 61; 27] modalities. Zhou _et al_.  uses diffusion models with a point-voxel representation for shape generation and point-cloud completion. LION  uses a hierarchical VAE mapped to a latent space and trains diffusion models on latent encodings to generate point clouds. In perception tasks, Chen _et al_.  and Zhou _et al_.  propose diffusion-based object detection frameworks. Kim _et al_.  proposes a diffusion-based module to enhance the proposal refinement stage of two-stage object detectors. Unlike these approaches, our work focuses on leveraging diffusion for post-processing in a detector-agnostic manner that shows superior performance over previous methods.

## 3 Method

### Problem Setup

Despite great in-domain performance, 3D object detection models often struggle to maintain their accuracy when generalized to new domains (datasets). It has been concluded that such poor performance is mainly caused by mislocalization rather than misdetection . That is, although objects can be correctly recognized by the object detector, the detected boxes lack sufficient overlap with the ground truth box and do not count as true positive (_i.e_., detections with \(<0.7\) with ground-truth).

In this work we introduce DiffuBox, which focuses on correcting the localization of bounding box proposals, as illustrated in Figure 3, to improve domain adaptation for 3D object detection. Unlike existing domain adaptation algorithms that require careful re-training on the target [59; 53] or source  domain data, DiffuBox can be deployed off-the-shelf as a post-processing procedure in any novel domain.

Let \(^{N 3}\) denote a \(N\)-point 3D point cloud from the target domain. Let \(=\{_{1},,_{M}\}\) be a set of \(M\) imperfect bounding boxes proposed by an underadapted object detector given \(\), where each bounding box \(_{i}\) is a \(7\)-DoF (degrees of freedom) upright box, parameterized with center \([x_{i},y_{i},z_{i}]\), size \([w_{i},l_{i},h_{i}]\) and yaw angle \(_{i}\). We aim to obtain better localized object proposals \(}\) by refining the boxes in \(\)_without any re-training_.

\[}=\{}_{1},,}_{M}}_{ i}=(_{i},)\}.\] (1)

### Learning Shapes in the Normalized Box View

While domain differences between 3D object detection datasets exist in many aspects, the analysis from  shows that the most significant hurdle for adaptation comes from the difference in object size. For instance, the American cars in the Lyft dataset  are about \(20\%\) larger than German cars in the KITTI dataset  on average, and an object detector trained on KITTI will tend to still predict small boxes when tested on Lyft. Unfortunately, as long as 3D object detectors are trained to explicitly predict object sizes, such size priors will be inevitably memorized during training and carried on to other domains as learned bias.

We aim to achieve **scale-invariant object detection**, which would be naturally immune to size priors. Motivated by Luo _et al_.'s  finding that the relative distribution of points to ground-truth bounding boxes is consistent across domains, _i.e_. points tend to concentrate near the surface of boxes, we propose to disentangle object size from shape by transforming pointclouds into a normalized box view (NBV), where point coordinates are box-relative rather than absolute.

Using homogeneous transformation, we define \(_{}^{}^{N 3}\), the normalized box view of point cloud \(\) relative to bounding box \(=[x,y,z,w,l,h,]\), to be

\[_{}^{}\\ =&0&0&0\\ 0&&0&0\\ 0&0&&0\\ 0&0&0&1&&0&0\\ -&&0&0\\ 0&0&1&0\\ 0&0&0&11&0&0&-x\\ 0&1&0&-y\\ 0&0&1&-z\\ 0&0&0&1\\ \] (2)As shown in Figure 2, Equation 2 transforms the bounding box \(\) into a \([-1,1]^{3}\) cube, eliminating the size prior. The same transform also transforms \(\) into box-relative, scale-invariant \(_{}^{}\).

In practice, we only consider points within a certain depth range of the bounding box for efficiency. We refer to this range as _context limit_. In the sections below, we overload \(_{}^{}\) as the point cloud within the context limit for ease of reference.

### Bounding Box Refinement via Diffusion

Inspired by recent works on diffusion-based shape generation [52; 63; 25; 61] and knowledge distillation from pretrained diffusion models , we show that size-agnostic shape knowledge learned by a point cloud diffusion model can help to improve object localization across domains. The underlining assumption is that despite size difference, objects of the same category (_ear_, _cyclist_, _pedestrian_) share similar shapes.

Figure 2 illustrates our hypothesis that the good localization of a bounding box \(\) is closely correlated to its corresponding \(_{}^{}\) forming a "standard" point distribution, _a.k.a._ shape. Thus, improving the shape of \(_{}^{}\) will also lead to better localization of \(\). Since our ultimate goal is to optimize the bounding box \(}\), we propose to use a point diffusion model to learn to "denoise" \(_{}^{}_{}^{}\).

Specifically, we begin by discussing the training of the diffusion model to learn the probabilistic flow for each point to a good box in Section 3.3.1. Then, we go into how our method refines the bounding box by computing the improvement step relative to the learned probabilistic flow in Section 3.3.2. Finally, we go into how we leverage the shape guidance to embed heuristics into our training procedure in Section 3.3.3. To enhance clarity, we additionally provide an algorithmic description of DiffuBox's training and inference workflow in Appendix S1.

#### 3.3.1 Diffusion Training

The learning objective of a diffusion model can be viewed as a variant of the score function \(_{} p(;)\), where \(\) indicates the noise level and \(p(;0)=p_{}\), the true data distribution that is hard to directly sample from. As the score function points data to a higher likelihood, samples can instead be drawn from \(p(;_{})\) -- which is usually modeled as an i.i.d. Gaussian distribution -- and denoised into \(p_{}\) by solving a probabilistic flow ODE /SDE .

Let \(F_{}\) denote a diffusion model with parameter \(\). Considering the full design space , in general its training loss can be written as:

\[_{,,}[()c_{}( )^{2}\|F_{}(c_{}()( +);c_{}())-}()}(-c_{}( )(+))\|_{2}^{2}],\] (3)

where \( p_{}\), \( p_{}\), and \((0,^{2})\). \(()\) denotes the effective training weight, \(c_{}()\) denotes noise level preconditioning. \(c_{}()\), \(c_{}()\), and \(c_{}()\) are input/output scaling factors.

Modelling \(p(;_{})\) as \((0,)\) allows for easy sampling. However, an i.i.d. Gaussian noise doesn't suit the shift of point cloud in NBV caused by object mislocalization. As shown in Figure S8,

Figure 2: Example _Car_ objects converted into normalized box view (NBV). Foreground/background points are marked in black/gray, respectively for better visualization. Foreground LiDAR points distributing tightly within a \([-1,1]^{3}\) NBV cube is a domain-consistent sign for good localization.

a noisy NBV point cloud is formed from a 3D distortion on the standard shape, rather than adding Gaussian noise. The distortion includes rotation (caused by incorrect raw angle), rescaling (caused by incorrect size), and translation (caused by incorrect box center).

Because of this, we made a few adaptations to the diffusion process. We set the effective training weight \(()=1\), all input/output scaling factors \(c_{}()=c_{}()=c_{}()=1\), as the noise level \(\) is unknown during inference. We apply Gaussian noise on the bounding box, rather than on the point cloud, to simulate mislocalized bounding boxes. With these adaptations, our new training loss becomes:

\[_{,(,^{}),}[( )\|F_{}(_{^{}+}^{};c_{}())-(_{^{}}^{}-_{^{ }+}^{})\|_{2}^{2}]\] (4)

where \( p_{}\), \(,^{}_{}\) and \((0,^{2}diag())\). \(diag()\) is the variance of box noise, which is roughly estimated from _direct_ domain adaptation performance.

#### 3.3.2 Bounding Box Updates

Since \(F_{}\) is trained to approximate the score function \(_{} p(;)\), the regular denoising process can be implemented by solving a probabilistic flow ODE:

\[=-(t)(t)_{} p(; (t))t,\] (5)

where \((t)\) denotes a noise schedule in which \((T)=_{}\), \((0)=0\), and the dot stands for a time derivative. Thus, \(_{0} p_{}\) can be generated by evolving \(_{T} p(;_{})=(0; ^{2})\) from time \(t=T\) to \(t=0\).

The NBV point cloud diffusion model \(F_{}(_{}^{};c_{}()) _{}^{};)}{ _{}^{}}\), to denoise the bounding box, rather than the point cloud, we take a further step following the chain rule (\(_{}^{}\) is differentiable according to Equation 2 of the main text):

\[;,)}{}=_{}^{};)}{_{ }^{}}_{}^{}}{}  F_{}(_{}^{};c_{}() )_{}^{}}{}.\] (6)

Let \(_{T} p(;_{})\) be the imperfect bounding box predicted by an adapted object detector, and let \(_{0} p(;0)\) denote the corresponding box after refinement. Similarly, bounding box refinement can be achieved by evloving \(_{T}\) to \(_{0}\) following:

\[=-(t)(t);, )}{}t,\] (7)

#### 3.3.3 Shape Guidance

The probabilistic flow ODE allows adding objectives other than the score function to bounding box refinement without any retraining. For instance,  assumes the average object size (\(,,\)) in the target domain is available. Such information can be used to further improve domain adaptation performance by simply rewriting Equation 7 as:

\[=-(t)(t)[;, )}{}+}(,,,)}{}]t,\] (8)

where \(\) denotes shape weight, and

\[_{}(,,,)=\|w- \|^{2}+\|h-\|^{2}+\|l-\|^{2}.\] (9)

## 4 Experiments

### Experimental Setup

Datasets.We primarily consider three datasets: The KITTI dataset , the Lyft Level 5 Perception dataset , and the Ithaca365 dataset . For KITTI, we follow the official splits. For Lyft, we follow various existing works [58; 59; 24] and use the splits separated by geographical locations, consisting of 11,873 point clouds for training and 4,901 for testing. For Ithaca365, we utilize the annotated point clouds with 4,445 for training and 1,644 for testing. Additionally, we include experiments with the nuScenes dataset in the supplementary to evaluate DiffuBox's performance on larger-scale and more diverse data.

Baselines.We consider five domain adaptation baselines: (1) directly applying an out-of-domain detector without adaptation (Direct); (2) Output Transformation (OT) ; (3) Statistical Normalization (SN) ; (4) Rote-DA ; (5) ST3D . OT and SN perform resizing based on the average sizes from the target domain. OT directly resizes the predicted bounding boxes on the target domain, while SN trains the detector with resized objects and boxes from the source domain. Rote-DA and ST3D perform self-training. Rote-DA leverages an additional context in the form of persistency-prior  and enforces consistency across domains. ST3D leverages better data augmentation and a memory bank for high-quality detections. As DiffuBox is complementary to these methods, we compare the detection performance of these methods before and after refining with DiffuBox.

Evaluation Metrics.We evaluate the detection performance in Bird's Eye View (BEV) and 3D. At depth ranges of 0-30m, 30-50m and 50-80m, we report the mean Average Precision (mAP) with Intersection over Union (IoU) thresholds set at 0.7 for cars, and 0.5 for pedestrians and cyclists. We also consider the nuScenes true positive metrics : translation error, scale error and rotation error. These measure the error in center offset, size difference, and orientation offset, respectively, of all true positive detections.

Implementation Details.We use the implementation and configurations from OpenPCDet  for detectors, and 's implementation for diffusion models. We set the context limit to 4x the bounding box size. We use shape weight 0.1 for cars and pedestrians, and 0.01 for cyclists as cyclists have more shape variation. More details can be found in the supplementary.

### Experimental Results

We present the results for KITTI \(\) Lyft cars in Table 1 (mAP@IoU 0.7) and Table S8 (nuScenes TP metrics), and the results for KITTI \(\) Ithaca365 cars in Table 2 (mAP@IoU 0.7) and Table S9 (nuScenes TP metrics). We additionally include KITTI \(\) nuScenes results in Table S10 in the supplementary. We use PointRCNN  detectors; evaluations with other detectors can be found in the section below. DiffuBox consistently attains significant performance gain across different domain adaptation methods and datasets. The improvement of DiffuBox is especially significant for near-range and middle-range detections. Notably, for KITTI \(\) Lyft cars, DiffuBox applied upon the _Direct_ outputs is able to attain comparable performance with domain adaptation methods that require training such as ST3D. We hypothesize that this is because there are more LiDAR points for near-range and middle-range objects, which allows DiffuBox to better correct the detections.

DiffuBox with Other Detectors.To further demonstrate the robustness and versatility of DiffuBox, we present DiffuBox's performance for refining predictions from other detectors. We consider PointPillar , SECOND , PV-RCNN , CenterPoint  and DSVT  trained on KITTI. We report the mAP@IoU 0.7 for KITTI \(\) Lyft cars in Table 3. Results show that DiffuBox consistently improves the predictions from different detectors.

    &  &  \\   & 0-30m & 30-50m & 50-80m & 0-80m & 0-30m & 30-50m & 50-80m & 0-80m \\  Direct & 68.03 & 38.62 & 9.99 & 39.06 & 25.76 & 7.84 & 1.04 & 12.07 \\ Direct+DiffuBox & **88.95** & **73.27** & **23.84** & **59.70** & **62.94** & **35.44** & **6.67** & **35.56** \\  OT & 75.07 & 61.84 & 20.44 & 51.95 & 18.67 & 10.57 & 1.82 & 11.89 \\ OT+DiffuBox & **92.67** & **74.46** & **23.95** & **60.98** & **50.99** & **33.06** & **6.87** & **31.21** \\  SN & 92.88 & 69.97 & **25.68** & 61.67 & **70.40** & 32.96 & 6.18 & 36.64 \\ SN+DiffuBox & **94.77** & **72.09** & 25.47 & **62.70** & 69.62 & **40.39** & **7.17** & **38.72** \\  Rote-DA & 89.64 & 70.10 & **27.96** & 60.63 & 50.65 & 24.92 & **7.43** & \(28.63\) \\ Rote-DA+DiffuBox & **95.10** & **75.10** & 23.35 & **62.14** & **71.00** & **48.89** & 6.48 & **41.06** \\  ST3D & 72.86 & 64.23 & 34.96 & 55.58 & 35.22 & 26.33 & 6.06 & 22.47 \\ ST3D+DiffuBox & **92.08** & **75.03** & **35.35** & **66.08** & **61.58** & **45.44** & **10.16** & **39.81** \\   

Table 1: **mAP@IoU 0.7 for KITTI \(\) Lyft (cars).** Higher is Better. DiffuBox leads to improvement in almost all cases, with especially significant gain for the Direct and OT detections.

**DiffuBox on Other Object Classes.** We present DiffuBox's performance on other object classes, specifically pedestrians and cyclists. We use the same configurations as cars, except cyclist shape weight 0.01 as mentioned in the implementation details. We report the KITTI \(\) Lyft performance in Table 4, and the KITTI \(\) Ithaca365 performance in Table S11. For Ithaca365, we only evaluate for pedestrians as Ithaca365 has very few cyclists. Results show that DiffuBox consistently attains significant improvement across object classes and domains, and can improve upon other domain adaptation methods. This shows the robustness and versatility of DiffuBox.

### Qualitative Results

Figure 3 visualizes four scenes from the Lyft and Ithaca365 datasets. We compare the ground truth bounding boxes (green), the detections directly obtained from a PointRCNN trained on KITTI (yellow), and the refined detections using DiffuBox (blue). The out-of-domain PointRCNN produces reasonable results, but occasionally it produces false positives or boxes with incorrect shapes or alignment. DiffuBox effectively moves the incorrect boxes towards having better location, shape and alignment. Also observe that for the already accurate boxes, DiffuBox makes little change to them.

### DiffuBox Extension: Detector Retraining

    &  &  \\   & 0-30m & 30-50m & 50-80m & 0-80m & 0-30m & 30-50m & 50-80m & 0-80m \\  Direct & 52.59 & 21.19 & 3.20 & 25.08 & 25.09 & 6.25 & 0.17 & 10.53 \\ Direct+DiffuBox & **61.89** & **32.09** & **6.05** & **32.27** & **42.23** & **17.79** & **1.47** & **20.51** \\  OT & 59.34 & 29.18 & 5.26 & 30.11 & 32.05 & 12.00 & 1.16 & 14.71 \\ OT+DiffuBox & **60.76** & **32.56** & **6.07** & **31.89** & **40.43** & **18.33** & **1.61** & **19.80** \\  SN & 60.48 & 31.04 & **4.04** & 29.80 & 32.17 & 13.03 & 0.85 & 15.02 \\ SN+DiffuBox & **60.79** & **34.49** & 3.79 & **30.81** & **37.21** & **18.90** & **1.33** & **18.31** \\  Rote-DA & 71.14 & 44.76 & 14.00 & **42.38** & 43.07 & 22.42 & 2.46 & 22.38 \\ Rate-DA+DiffuBox & **71.52** & **45.44** & **14.56** & 42.28 & **46.77** & **25.72** & **4.17** & **25.02** \\   

Table 2: **mAP@IoU 0.7 for KITTI \(\) Ithaca365 (cars).** DiffuBox leads to significant improvement upon different adaptation methods.

    &  &  \\   & 0-30m & 30-50m & 50-80m & 0-80m & 0-30m & 30-50m & 50-80m & 0-80m \\  PointPillar (Direct) & 65.77 & 39.02 & 11.28 & 36.80 & 16.58 & 5.06 & 0.56 & 6.87 \\ PointPillar (Direct)+DiffuBox & **84.76** & **65.57** & **17.46** & **53.67** & **65.41** & **32.77** & **4.55** & **33.82** \\  SECOND (Direct) & 65.61 & 38.83 & 13.92 & 38.06 & 24.39 & 8.32 & 0.86 & 10.68 \\ SECOND (Direct)+DiffuBox & **89.70** & **66.98** & **19.11** & **57.04** & **61.90** & **32.24** & **4.39** & **33.19** \\  PV-RCNN (Direct) & 73.56 & 46.39 & 13.63 & 43.72 & 34.20 & 14.03 & 1.53 & 16.17 \\ PV-RCNN (Direct)+DiffuBox & **92.40** & **68.35** & **20.68** & **59.30** & **63.82** & **35.78** & **5.03** & **35.43** \\  PV-RCNN (OT) & 80.48 & 54.40 & 17.54 & 51.03 & 19.87 & 8.15 & 0.75 & 10.74 \\ PV-RCNN (OT)+DiffuBox & **93.65** & **69.06** & **21.35** & **50.16** & **53.68** & **31.31** & **4.00** & **30.63** \\  PV-RCNN (SN) & **94.16** & 68.58 & **22.22** & **62.16** & **72.72** & 27.86 & 3.43 & 33.47 \\ PV-RCNN (SN)+DiffuBox & 93.99 & **69.24** & 21.13 & 61.23 & 67.83 & **35.08** & **4.50** & **34.93** \\  CenterPoint (Direct) & 74.91 & 36.64 & 2.47 & 36.23 & 28.63 & 4.05 & 0.15 & 10.29 \\ CenterPoint (Direct)+DiffuBox & **90.25** & **58.65** & **6.95** & **51.38** & **71.02** & **34.91** & **1.48** & **34.40** \\  CenterPoint (OT) & 82.81 & 51.02 & 6.26 & 46.10 & 25.34 & 12.35 & 0.52 & 13.34 \\ CenterPoint (OT)+DiffuBox & **91.79** & **59.24** & **7.45** & **51.88** & **63.01** & **32.53** & **1.24** & **30.95** \\  DSVT (Direct) & 68.93 & 47.49 & 11.32 & 41.77 & 33.72 & 11.74 & 1.42 & 15.67 \\ DSVT (Direct)+DiffuBox & **89.01** & **63.41** & **17.50** & **55.27** & **65.22** & **36.31** & **5.02** & **35.61** \\  DSVT (OT) & 71.85 & 42.93 & 13.18 & 43.05 & 15.66 & 4.47 & 0.31 & 8.06 \\ DSVT (OT)+DiffuBox & **90.21** & **63.50** & **17.91** & **55.61** & **56.84** & **31.19** & **4.63** & **31.12** \\   

Table 3: **mAP@IoU 0.7 for KITTI \(\) Lyft (cars) with other detectors.** DiffuBox consistently improves the detections from different detectors.

One extension of DiffuBox for domain adaptation is to retrain detectors  with DiffuBox's refined boxes. We can take a detector trained on the source domain, obtain its predictions on the target domain, refine the predictions with DiffuBox, and then retrain a detector using the refined boxes as labels.

We provide the KITTI \(\) Lyft results in Table 5. We compare the performance of (1) directly applying the KITTI detector, (2) retraining for one round with the KITTI detector's predictions as labels, and (3) retraining for one round with the KITTI detector's predictions after DiffuBox's refinement as labels. Retraining is performed based on the KITTI detector's predictions on the Lyft training split, and the evaluation is conducted over Lyft's testing split. Retraining using

   &  & \)} &  \\   & & 0-30m & 30-50m & 50-80m & 0-80m & 0-30m & 30-50m & 50-80m & 0-80m \\   & Direct & 31.89 & 25.75 & 0.51 & 20.74 & 21.29 & 16.59 & 0.18 & 14.17 \\  & Direct+DiffuBox & **43.64** & **26.97** & **0.61** & **25.10** & **34.30** & **22.54** & **0.33** & **20.46** \\   & OT & 35.63 & **25.47** & **0.64** & 21.69 & 27.76 & 19.02 & 0.35 & 16.77 \\  & OT+DiffuBox & **42.63** & 25.28 & 0.55 & **24.02** & **37.34** & **22.16** & **0.42** & **20.94** \\   & SN & 43.75 & **36.87** & 0.67 & 28.18 & 34.12 & 26.08 & 0.45 & 21.00 \\  & SN+DiffuBox & **50.98** & 35.88 & **1.08** & **29.45** & **38.55** & **30.24** & **0.60** & **23.50** \\   &  & 49.14 & 46.86 & **1.23** & 33.60 & 37.68 & 39.25 & 1.01 & 26.75 \\  &  & **54.14** & **50.30** & 1.22 & **36.04** & **42.76** & **43.25** & **1.04** & **29.93** \\   & Direct & 48.50 & 8.91 & **0.13** & 26.96 & 38.13 & 5.19 & **0.02** & 21.11 \\  & Direct+DiffuBox & **61.28** & **10.60** & 0.06 & **34.90** & **49.76** & **6.87** & **0.02** & **27.35** \\    & OT & 55.37 & 9.99 & **0.14** & 30.90 & 20.09 & 4.11 & **0.02** & 10.96 \\   & OT+DiffuBox & **65.93** & **10.26** & 0.06 & **36.86** & **32.93** & **6.11** & **0.02** & **18.81** \\    & SN & 46.75 & 11.42 & 0.05 & 26.30 & 36.87 & 6.23 & **0.03** & 20.18 \\   & SN+DiffuBox & **59.48** & **15.82** & **0.10** & **34.62** & **48.92** & **10.31** & 0.02 & **27.38** \\    &  & 77.19 & **34.61** & **0.09** & 48.66 & 70.35 & **30.77** & **0.05** & 44.29 \\   &  & **83.80** & 31.36 & 0.05 & **51.49** & **75.79** & 27.98 & 0.04 & **45.77** \\  

Table 4: **mAP@IoU 0.5 for KITTI \(\) Lyft (pedestrians/cyclists).** DiffuBox attains consistent improvement across object classes and is able to improve upon other domain adaptation methods.

Figure 3: **Illustration of 3D object detection on Lyft/Ithaca365 before and after DiffuBox’s refinement.** We visualize detections from an out-of-domain PointRCNN on four scenes from each dataset. We color the ground truth boxes in green, the detector outputs in yellow, and DiffuBox’s refinements in blue. The out-of-domain detector sometimes produces false positives or boxes with incorrect shape or alignment. DiffuBox effectively improves the wrong or inaccurate boxes, while making little change to the accurate boxes.

the KITTI detector's predictions directly only provides limited improvement over directly applying the KITTI detector, while retraining with DiffuBox's refinement leads to significant improvement.

### Ablation Studies and Analysis

Context Limit.We conduct ablation study on the context limit, and consider ranges of 2x, 4x and 6x the box size. We evaluate under the setting of no adaptation (_Direct_) for KITTI \(\) Lyft cars, and compare the mAP@IoU 0.7 in Table 6. All three context limits lead to significant performance improvement. Larger limit attains more gain, and the gain saturates at around the 4x limit.

Denoising Steps.We perform ablation on the number of denoising steps used, and present the results in Figure 5. In general, a majority of the performance is already reached using 8 diffusion steps, and it saturates around using 14 steps.

Shape Weight.We perform ablation on shape guidance, and consider shape weight 0, 0.01, 0.1 and 0.5. We evaluate under the setting of no adaptation (_Direct_) for KITTI \(\) Lyft cars and report the mAP@IoU 0.7 in Table 7. DiffuBox improves the detector's output even without shape weight. Using shape weights leads to additional gain, which saturates at around shape weight 0.1.

IoU Performance Analysis.We visualize the comparison of IoU of the bounding boxes with the ground truth bounding boxes both before and after using DiffuBox in Figure 4. The IoU of predictions after DiffuBox refinement (in blue) improves significantly over those before refinement (in yellow). This suggests that a majority of our refinement is in improving the bounding boxes' shape and alignment to fit into the new domain, thus resulting in higher IoU values.

Figure 5: **mAP vs. Number of Diffusion Steps. We report the BEV (left) and 3D (right) mAP @ IoU 0.7 for the setting of KITTI \(\) Lyft Cars and PointRCNN detector.**

    &  &  &  \\   & & 0-30m & 30-50m & 50-80m & 0-80m & 0-30m & 30-50m & 50-80m & 0-80m \\   & Direct & 68.03 & 38.62 & 9.99 & 39.06 & 25.76 & 7.84 & 1.04 & 12.07 \\  & Retraining w/ Direct & 70.98 & 45.53 & 15.12 & 43.49 & 30.45 & 10.54 & 1.69 & 14.55 \\  & Retraining w/ Direct+DiffuBox & **91.79** & **77.52** & **35.33** & **64.71** & **67.80** & **36.17** & **10.56** & **37.55** \\   & Direct & 31.89 & 25.75 & 0.51 & 20.74 & 21.29 & 16.59 & 0.18 & 14.17 \\  & Retraining w/ Direct & 34.23 & 34.65 & **0.95** & 24.37 & 27.58 & 30.33 & **0.37** & 20.22 \\  & Retraining w/ Direct+DiffuBox & **44.17** & **39.39** & 0.84 & **29.22** & **35.60** & **34.67** & 0.36 & **24.71** \\   & Direct & 48.50 & **8.91** & 0.13 & 26.96 & 38.13 & 5.19 & 0.02 & 21.11 \\  & Retraining w/ Direct & 44.06 & 6.05 & 0.21 & 24.29 & 33.55 & **5.73** & **0.14** & 18.73 \\   & Retraining w/ Direct+DiffuBox & **57.57** & 6.54 & **0.23** & **31.36** & **45.79** & 5.19 & 0.11 & **24.39** \\   

Table 5: **Retraining Performance (mAP@IoU 0.7) for KITTI \(\) Lyft. Retraining with DiffuBox’s refined detections attains significant performance improvement.**
Recall Analysis.We perform analysis on DiffuBox's effect on detection recall in Figure 6. As DiffuBox improves IoU for mislocalized detections, it reduces false negatives that arise from match IoU being lower than the threshold. The improvement is observed for objects across different sizes.

## 5 Discussion and Future Work

In this work, we propose DiffuBox, a diffusion-based approach that refines bounding boxes for better domain adaptation. While DiffuBox effectively improves the existing bounding boxes, one limitation is that DiffuBox currently does not consider false negatives that are due to completely missed detections. This could potentially be addressed through further distilling the detectors by DiffuBox's refined boxes, or by incorporating exploration strategies to capture possibly missed objects. Alternatively, a view that we did not discuss in this work, but is of potential interest to the field, is the use of DiffuBox for automatic label refinement. This can be useful for correcting slightly mis-aligned boxes, or labels across sensors that may have slightly unsynchronized sensors. We leave this discussion for further work, and will provide code and model checkpoints for this use case. With our work, we do not foresee any negative societal impacts and hope the field continues to develop such label refinement methods for 3D object detection.

    &  &  \\   & 0-30m & 30-50m & 50-80m & 0-80m & 0-30m & 30-50m & 50-80m & 0-80m \\  Direct & 68.03 & 38.62 & 9.99 & 39.06 & 25.76 & 7.84 & 1.04 & 12.07 \\ Direct+DiffuBox 2x & 83.16 & 68.91 & 17.65 & 55.53 & 54.63 & 27.63 & 3.62 & 29.51 \\ Direct+DiffuBox 4x & 88.95 & **73.27** & 23.84 & 59.70 & **62.94** & **35.44** & **6.67** & **35.56** \\ Direct+DiffuBox 6x & **91.68** & 72.56 & **24.60** & **60.59** & 61.92 & 30.91 & 5.39 & 33.61 \\   

Table 6: **Ablation on Context Limit. DiffuBox is robust against the choice of context limit. Larger limit could lead to better performance, with the gain saturated at around 4x limit.**

    &  &  \\   & 0-30m & 30-50m & 50-80m & 0-80m & 0-30m & 30-50m & 50-80m & 0-80m \\  Direct & 68.03 & 38.62 & 9.99 & 39.06 & 25.76 & 7.84 & 1.04 & 12.07 \\ No SW & 72.62 & 50.70 & 14.32 & 45.27 & 34.63 & 13.47 & 2.17 & 17.54 \\ SW 0.01 & 74.81 & 57.48 & 17.18 & 48.89 & 47.11 & 17.74 & 2.99 & 23.10 \\ SW 0.1 & 88.95 & **73.27** & 23.84 & 59.70 & **62.94** & **35.44** & 6.67 & **35.56** \\ SW 0.5 & **91.36** & 73.09 & **24.48** & **60.23** & 50.70 & 32.73 & **7.21** & 30.54 \\   

Table 7: **Ablation on Shape Weight. DiffuBox improves the detector’s output significantly even without using shape weight, and using shape weight attains additional gain.**

Figure 6: **Recall improvement with DiffuBox. We report recall on the _car_ class (KITTI \(\) Lyft, PointRCNN) before and after refinement with DiffuBox.**