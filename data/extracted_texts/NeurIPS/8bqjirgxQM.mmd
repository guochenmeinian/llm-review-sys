# Understanding Social Reasoning in Language Models

with Language Models

Kanishk Gandhi &J.-Philipp Franken &Tobias Gerstenberg &Noah D. Goodman

Stanford University

{kanishk.gandhi, jphilipp}@stanford.edu

Equal Contribution.

###### Abstract

As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations _with_ LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (**BigToM**) _for_ LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using **BigToM**, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.2

## 1 Introduction

Humans continually try to understand what others think, want, and feel.

We try to understand what people have done and predict what they might do next by inferring their mental states. This capability, often referred to as "Theory of Mind" (ToM), is the foundation of social interaction . With Large Language Models (LLMs) playing a growing role in our lives, assessing their ability to model human mental states is key for guaranteeing effective interactions. This involves evaluating the current abilities of LLMs, understanding their failure modes, and discovering ways to improve them. LLMs with ToM-like abilities could be better at teaching us, learning from us, communicating with us, collaborating with us, and understanding us .

Recent attempts at understanding social reasoning in LLMs have used crowd-sourced data, SocialIQA , data from synthetic templates, ToMi , or (modified) tests from psychology designed to evaluate human capabilities [e.g. 24, 42, 18, 5, 23, 41]. Sap et al.  used SocialIQA and ToMi to show that GPT-3 had limited social reasoning capabilities. However, their findings are challenging to interpret due to limitations in their methodology. SocialIQA has several ambiguous examples and stories that do not effectively test the desired social reasoning behaviors. In comparison, ToMi suffers from ambiguous narratives with unclear perceptual descriptions and additional confounding factors in reasoning like memory loads or tracking requirements. Moreover, both of these datasets lack control conditions making it difficult to identify precisely where models make mistakes. The results of studies with tests developed by psychologists show some signs of ToM capabilites in LLMs[18; 5]. However, when LLMs such as GPT-3  succeed in scenarios, they often fail dramatically on trivial alterations [42; 24; 35]. Despite their careful design, concerns about the limited test set [24; 18] and potential dataset leakage from modifications to the Sally-Anne task  in [5; 18; 24], suggest caution in the interpretation of these results (see App. D for a detailed discussion).

To address these shortcomings, we present a novel framework for procedurally designing synthetic ToM evaluations from causal templates (Fig. 1). By representing ToM scenarios as causal graphs, we can systematically intervene on variables, generate control conditions, and probe different aspects of an LLM's ToM capabilities. More concretely, consider the scenario in Fig. 1a: Here, _"Noor"_ is an agent with a desire, _"to make a latte with oat milk"_, who performed an action, _"fills it with oat milk"_, resulting in a belief, _"she believes that the pitcher has oat milk"_. Next, a _"Causal Event"_

Figure 1: Illustration of our template-based Theory-of-Mind (ToM) scenarios. [a] The causal template and an example scenario including prior desires, actions, and beliefs, and a causal event that changes the state of the environment. [b] Testing _Forward Belief_ inference by manipulating an agent’s percepts. TB = True Belief. FB = False Belief. [c]_Forward Action_ inference from an agent’s percepts which requires additional inferences over unknown beliefs. [d]_Backward Belief_ inference requires joint inferences over unknown percepts and beliefs from an agent’s observed actions. Error bars for human performance represent 95% bootstrapped confidence intervals of the mean.

changes the state of the environment ("_oat milk_" \(\)_"almond milk_"). Given this setup, we can now manipulate the agent's percept to create True Belief and False Belief conditions. In the True Belief condition, the perception of the causal event is presented, _"Noor sees her coworker swapping the milk"_, and then we test a model's _forward belief_ inference abilities; _"What does Noor believe is in the pitcher?"_ (Fig. 1b). Moreover, we can probe more difficult inferences, such as _forward action_ inferences from an agent's percepts via inferred beliefs (Fig. 1c). In addition to manipulating percepts, we can intervene on an agent's actions to examine a model's _backward belief_ inferences, which is even more difficult as it requires a joint inference over unknown percepts and beliefs (Fig. 1d; SS3).

We design a framework for systematic and diverse evaluations of LLMs in three steps. First, we build a causal template (an abstracted causal model) for the domain of interest, which in our case is ToM. Second, we prompt a language model to populate the variables in the template (yielding a concrete causal model). Third, we construct different evaluation conditions by combining variables from the populated causal template (Fig. 2 and SS3). Our approach is a general method for generating evaluations, applicable in any domain where reasoning traces can be represented as causal graphs.

Overall, our contributions are as follows: (1) We present a framework for generating systematic evaluations from causal templates that help us understand a model's behavior, its failures and successes, through automated, controlled tests. (2) We show the effectiveness of our scalable, cost-efficient method for writing evaluations with language models by comparing its quality to crowd-sourced and expert written tests. (3) Finally, we test ToM reasoning in a variety of LLMs3 using different prompting techniques, and compare model performances with human performance. We find that gpt-4 shows human-like ToM inference patterns, although less reliable, while other LLMs struggle.

## 2 Related Work

**Theory-of-Mind in Humans.** Infants, arguably from 12 months of age, can attribute mental states to agents, exhibiting theory of mind reasoning . A classic test to probe this reasoning is the false-belief task : Sally has a doll and puts it in a basket, then leaves the room. While Sally is away, Anne takes the ball out of the basket and puts it into a box. Participants are then asked to predict what happens next: "When Sally comes back, where will she look for her ball?". To answer this question, participants need to infer Sally's beliefs, and realize that her beliefs aren't the same as theirs. Through well-planned experiments, cognitive scientists probe reasoning aspects relating to agents' desires and beliefs . These studies employ control conditions to rule out simple heuristics people might use, while searching for the cognitive mechanisms that underlie human reasoning and behavior . Such experiments have inspired AI researchers to design "behavioral" experiments for probing ToM in AI models .

**Theory-of-Mind in Machines.** Initial attempts at building ToM representations in neural network based models  used ToM specific tasks to train and test the models. As LLMs scaled and became better at reasoning, researchers used a small set of tests from cognitive science to claim that ToM reasoning had emerged in LLMs (GPT-3, GPT-4) . But, further probing using alterations and diverse scenarios showed that this reasoning was quite brittle . Other tests for social reasoning used crowd-sourced and synthetic evaluations to find mixed results . Despite the abundance of research in this domain, we still don't understand the strengths and weaknesses of LLMs in ToM reasoning. Previous evaluations suffer from one or more of the following issues: reliance on limited evaluations designed for humans [e.g. 18, 24], insufficient control conditions [e.g. 33, 42], limited test cases [e.g. 41, 5], noisy/ambiguous crowd-sourced evaluations [e.g. 33], the risk of dataset leakage [e.g. 18, 41], confounding factors in reasoning [e.g. 21, 42] and possible overfitting of the prompting method  (see App. D for a detailed discussion). The goal of our work is to come up with a scalable, replicable framework to understand the reasoning behind predictions made by language models while avoiding the pitfalls that other methods fall into.

**Model-Written Evaluations.**

Advancements in aligning LLMs with instruction-tuning and RL from human feedback (RLHF) have recently shown promising results, such as the generation of a high-quality hate-speech detection dataset with GPT-3 , red-teaming , and training data generation . The latest work has extended this to the generation of evaluations directly . Perez et al.  examined whether generated data can serve as high-quality evaluation data with minimal errors for a variety of novel language model behaviors. These tests, while being scalable, cost-effective and easy to replicate, are still challenging to interpret as they lack structure in the generation of tests. In contrast, Dasgupta et al.  show how carefully designed automated tests can find specific failure modes in reasoning. Our work aims to integrate the benefits of these methods, creating a more structured approach to generating and interpreting tests, while preserving scalability, cost-effectiveness, and ease of replication.

## 3 Model-Written Evaluations with Causal Templates

**Preliminaries.** Theory of Mind is the ability to attribute mental states like beliefs, intents, desires, emotions and knowledge to oneself and others. It involves understanding that other people's mental states (latent causes) guide their actions (see Fig. 1a). In this work, we focus on the causal graph linking precepts, beliefs, desires, and actions. We want to test if models are able to perform forward and backward inference over different variables in this graph.

Our goal is to generate ToM evaluations that meet the following criteria: (1) they include control conditions to systematically assess language models' response tendencies and failure modes across different aspects of ToM, (2) they don't directly involve human-designed test items, and (3) they are diverse and scalable. By generating a diverse set of tasks, we wish to specifically target the reasoning involved in ToM inferences, while not focusing on other errors in common-sense reasoning4. To achieve this, we follow  and propose using language models to generate their own evaluations, specifically story(\(s\))-question(\(q\))-answer(\(a\)) test items of the format of \((s_{1},q_{1},a_{1}),(s_{2},q_{2},a_{2}),...(s_{N},q_{N},a_{N})\) (examples are shown in Tab. 1). To generate these evaluations, we propose a novel three stage-method: (1) Building a causal template of the domain, (2) populating causal templates using language models, and (3) composing test items for a given condition by "stitching" together template variables into fluent stories (Fig. 2a).

Figure 2: [a] Three-stage method for generating evaluations: Building a causal template for the domain (left). Creating a prompt template (simplified here; see Fig. 4 for the prompt) from the causal graph and populating template variables using a language model (middle). Composing test items by combining template variables (right). [b] Crowdworker ratings of our model-generated Theory-of-Mind (ToM) evaluations compared to crowd-sourced ToM evaluations and expert-written ToM evaluations. Error bars represent 95% bootstrapped confidence intervals of the mean.

### Stage 1: Building a Causal Template

To build a causal template, we start by defining the variables (see Fig. 0(a) and Fig. 1(a)). The world is set up with a context and description of the agent (_"Noor is a barista [...]"_). Next, we add the initial (prior) values of the variables in the template: desire (_"Noor wants to make a latte"_), percept (_"Noor fills a pitcher with oat milk"_) and belief (_"Noor believes that the pitcher has oat milk"_). Next, a _Causal Event_ changes the state of the environment (_"oat milk"_ _= _"almond milk"_). We can now manipulate the agent's percept of the causal event and the resulting action the agent will take. In this paper, we focus on the following inferences:

**Initial Percept to Initial Belief.** This tests if models understand that percepts (and actions) give rise to beliefs: _"Noor grabs a pitcher and fills it with oat milk"_ _= "Noor believes that the milk pitcher contains oat milk"_. This is a preliminary inference that a model must perform before being able to answer more complicated questions about beliefs or actions following the causal event.

**With vs. Without Initial Belief.** We consider two version of the background (prior) scenario. In version one (_"without initial belief"_), we do not explicitly reveal the agent's initial belief (i.e. we exclude the sentence _"Noor believes that the pitcher has oat milk"_). In version two (_"with initial belief"_), we include the agent's initial belief in the scenario. Revealing the initial belief should make the inference problem easier as we can skip the inference from percept to belief. Moreover, it allows us to test whether explicitly stating the initial belief biases the answers of LLMs.

**Forward Belief.** In this condition, the model must infer the belief of the agent given the agent's percepts of the causal event (see Fig. 0(b)). This inference can be written as: \(P()\).

**Forward Action.** Here, the model must infer the agent's action given percepts (see Fig. 0(c)). Implicitly, this inference requires the model to first infer the agent's belief before predicting the agent's action given percept and desire: \(_{}P(,,)\).

**Backward Belief.** In this condition (Fig. 0(d)), the goal is to infer the agent's belief from observed actions. This is the most difficult condition as it requires joint inference over unknown beliefs and percepts from an observed action: \(_{}_{}P(, ,)\).

**Additional Controls.** To control for context effects, we further include a control condition in which the "Causal Event" is replaced with a "Random Event" that does not change the state of the environment (e.g., _"A musician starts playing music while Noor is making the latte."_).

### Stage 2: Populating Causal Templates With Language Models

Unlike previous work [28; 43], we do not directly use language models to generate individual test items. Instead, we create prompt templates (Fig. 1(a), App. A) from the causal template developed in the previous section and use a language model(gpt-4-0314 with a temperature of \(0.5\) and default parameters) to fill template variables. For a given prompt, we generate \(3\) new completions using \(3\) few-shot examples. We constrain the model to generate exactly one sentence for a each variable in our template. Here we make an assumption that the model is good at forward prediction, coming up with plausible actions from the context, and the belief and desire of the agent (see App. C for a discussion).

### Stage 3: Composing Test Items from Template Variables

Having generated a sentence for each variable of the template, we choose the sentences to include in the story; this varies by condition depending on the inferences we wish to test. For example, we can create a story for the _Forward Belief inference for the True Belief condition_ by combining the sentences for variables context, desire, action, percept, belief with the sentences for causal event and percept, followed by the belief question and the answer options for the true belief and false belief versions (see Fig. 1(a)). In total, we generate 200 templates and extract 25 conditions from each template (resulting in a new benchmark consisting of 5,000 test items; see App. A for examples). For our main results with both humans and language models, we will focus on the \(6\) most important conditions _Forward Belief_ (True Belief, False Belief), _Forward Action_ (True Belief, False Belief), and _Backward Belief_ (True Belief, False Belief). Results for the remaining conditions are in App. E.

### Quality of Generated Data

**Expert Evaluations.** Tab. 1 shows random examples from human-and model-written datasets. Our model-written examples are high-quality and closely match the pattern of examples generated by human experts. To assess the quality of our model-written dataset, we first had two experts (two authors) independently evaluate \(100\) model-written templates including all \(25\) conditions (\(2500\) test items overall). During their evaluations, experts answered the following questions: **Question 1**: _"Does the story follow the assigned structure?"_ **Answers**: \(1\) (Yes), \(0\) (No). **Question 2**: _"Does the story test the desired behavior?"_ **Answers**: \(1\) (_"Strongly Disagree"_) to \(5\) (_"Strongly Agree"_). The overall percentage agreement between experts on the first question was 93.94% with mean ratings of \(0.919\) (95% CI: \(0.859\)-\(0.970\)) for expert 1 and \(0.960\) (95% CI: \(0.919\)-\(0.990\)) for expert 2. For the second question, average expert ratings were \(4.33\) (95% CI: \(4.13\)-\(4.53\)) for expert 1 and 4.35 (95% CI: \(4.18\)-\(4.52\)) for expert 2, both with a median rating of \(5\).

**Participant Evaluations.** We evaluate the quality of \(200\) items from BigToM with human participants5. Due to the large number of conditions, we gather participant ratings for the true belief and false belief versions of the forward belief condition, as exemplary versions representing the conditions. We compare participants' ratings of our model-written evaluations ("**BigToM**") with 50 random items sampled from a large-scale (38,000 items), human-written (crowd-sourced) ToM benchmark ("**socialIQa"**)  as well as 50 random items sampled from ToM scenarios written by human researchers ("**Expert**") [7; 42; 18]. Both socialIQa and the Expert test items were selected as they have recently been used to evaluate language models' ToM capabilities [e.g. 33; 42; 18; 24; 35]. Fig. 2b shows participants' average item ratings for each dataset and question. Our model-written test items (**BigToM**) received the highest ratings for each question. Results from a Bayesian linear mixed effects regression confirmed that test-items extracted from our model-written templates were better than the crowd-sourced items, particularly in coherence and un-ambiguity, and comparable to (or better than) expert-written test items (details in SSB.1).

## 4 Experiments

**Evaluating Models.** We test five large language models: text-davinci-003, gpt-3.5-turbo, gpt-4-0314, claude-v1.3, and llama-65b-q5 (quantized)[40; 12]. All models are used with the

}   Dataset & Human-written or Model-written Test Example \\   & **Story**: _“Here is a bag filled with popcorn, There is no chocolate in the bag. Yet, the label on the bag says “clocolate” and not ‘popcorn.’ Sum finds the bag. She had never seen the bag before. She cannot see what is inside the bag. She reads the label.”_ **Question**: _"What does Sam believe the bag contains?"_ **Answers**: Popcorn, Choocolate (correct answer). **Written by**: Human. **Source**: _“_ \\  & **Story**: _“Inside the fridge of the Airbnb’s kitchen is a can labeled as “tuna. However, the can actually contains sandiness and not man. Illi, a grass who jari arrived at the house. open the fridge and sees the can. She cannot see what is inside the can, but she reads the label.”_**Question**: _"What does Jill think is in the can?"_ **Answers**: Sardnes, Turna (correct answer).**Written by**: Human. **Source**: _“_ \\   & **Story**: _“Noor is working as a bartista at a busy coffee shop. Noor wants to make a delicious campacion for a customer who asked for our milk. Now grabs a milk picture and fills it with out an milk. Noor believes that the milk picture contains couldn. A cowrote, who didn’t know her customer’s request, swaps the our milk in the picture with allmond milk while Nor’s it attending to another task. Noor sees her coworker swapping the milk._ **Question**: _"Dober voice believe the milk picture contains all milk’s answers. Now believes the milk picture contains almost milk (correct answer), Noor believes the milk picture because the milk picture contains out milk. **Written by**: GPT-4. **Condition**: Forward Belief (True Belief) With Initial Belief. **Story**: _“Mei is a pearl?er in a small coastal village in Japan. Mei wants to find a perfect pearl to give to her grandmother for her birthday. Mei spans on oyster the hot beans of the seat that looks to be the right size and age to contain a pearl. Mei believes that the oyster she spotted contains a pearl. A cartoon octopus opens the concrete version that there is no pearl inside, and then swim anyway. Mei does down to collect the oyster.”_ **Question**: _"Does Mei believe the oyster she spotted contains a pearl (correct answer), Mei believes the oyster she spotted is empty. **Written by**: GPT-4. **Condition**: Backward Belief (False Belief) With Initial Belief. **Story**: _“Kendall persisted after being told no, and eventually had a positive effect on Lee.”_ **Question**: _"What will be meant to do next?"_ **Answers**: Reuse to bek **Kendall**, Give into Kendall (correct answer), Give a punch to Kendall’s face. **Written by**: Human. **Source**: _“_ \\  & **Story**: _“Lee tried to remain calm when nobody answered the phone call.”_ **Question**: _“What does Lee need to do before this?”_ **Answers**: send a text, try again, pick up the phone (correct answer). **Written by**: Human. **Source**: _“_ \\   

Table 1: Test examples from human-written and model-written datasets.

most deterministic setting with a temperature of \(0\). We test these models with four types of prompts: 0-shot, 0-shot-chain-of-thought , 1-shot, and 1-shot-chain-of-thought . The example used for the 1-shot prompt is from the _Forward Belief - False Belief_ condition, where the inference variable is the belief of the agent. The task is presented to the model in the form of a comprehension question with a story, followed by a question and two answer options. We compare models on their accuracy to answer the questions. We have released our prompts and evaluation scripts on the project page6. We compare models to a human baseline7 (details in B.2).

### Results and Discussion

The results of our investigation are detailed in Tab. 2, Tab. 7 and App. E, spanning different conditions, models, and prompts. We discuss results for the true belief and false belief conditions. Importantly, success on the false belief version of the task is evaluated _only_ if the model succeeded on the true belief version, as otherwise a model might succeed on the false belief version for the wrong reasons (i.e. failing to comprehend the change in the environment rather than comprehending the change in the environment _and_ understanding that the agent was not aware of this change). Therefore, we label the success on the false belief task as "TB" \(\) "False Belief".

**Initial Percept to Initial Belief.** All models are proficient at making this inference, and understand how percepts lead to the formation of beliefs (App. E to table).

Figure 3: blueModel performance (0-shot) across conditions. [a] _Forward Belief_ inferences from percepts to beliefs. TB = True Belief. FB = False Belief. [b] _Forward Action_ inferences from an agent’s percepts which require additional inferences over unknown beliefs. [c] _Backward Belief_ inferences over unknown percepts and beliefs from an agent’s observed actions. Error bars for humans represent 95% bootstrapped confidence intervals of the mean.

[MISSING_PAGE_FAIL:8]

our tests, indicating this gap between situation knowledge and inferential understanding. Further, to validate our hypothesis about circularity not being a confound, we generate an evaluation set with claude-2. We find that gpt-4 gets comparable scores on an evaluation set generated by a different model, outperforming the model that created the dataset (see App. F for details).

Our method shares limitations with other model-generated evaluations (as discussed in Perez et al. ): the generated evaluations can be biased in the content and the contexts that are generated. While synthetic datasets generated from language models offer advantages in scale and control, they also come with inherent biases reflecting those embedded in the underlying model and training data. As large language models are trained on internet text, they inevitably pick up stereotyped associations for gender, race, and other attributes in certain contexts. This could lead to normative, stereotyped roles in different situations in the synthetic dataset. A related issue could arise from biases leading to over-generation of certain situations, effectively yielding imbalanced evaluation data. (We note this is also a problem for human-generated items!) However, language models are also steerable through detailed instructions, allowing mitigation of some biases. Careful steering might be needed during dataset generation to ensure diversity and balance along different dimensions. In domains where the models capabilities are lacking, the model will struggle to generate good evaluations. Such limitations could be resolved through shared generation with a human expert while populating the causal graph (see App. A for an example interface). The stories produced by the model at times exhibit errors in common sense, yet these instances represent a small fraction (\(\)3%) of the overall tests generated; as language models continue to improve, we can expect these errors to reduce. Our test items tend to have syntactic similarities which might reduce the diversity of the items in our benchmark; this could perhaps be fixed by asking the model to paraphrase the generated stories.

**Future Work.** Our causal template method can be used for other domains where the effects of hidden causes or the underlying causes of effects must be inferred. These include many studied by cognitive scientists interested in the "intuitive theories" underlying human reasoning. For instance, morality, affect, and desire within social cognition, and extending to physical reasoning and more abstract reasoning such as medical diagnosis and mathematics.

In the future, testing social reasoning should move towards more realistic scenarios that are not limited to traditional ToM tests. We believe that we should focus on creating social reasoning tests or benchmarks in use-cases where LLMs are being deployed. We believe that there is a need to move towards more dynamic benchmarks for social reasoning, by creating environments where people or simulated agents (LLMs as people) interact with a language model. Such environments could also be used as a playground where the capabilities of models are not only measured, but also improved.

**Conclusion.** We have demonstrated a novel approach for assessing LLMs, and while there are limitations, we believe our findings offer a promising direction for future research in understanding and enhancing the capabilities of these powerful models. The nascent ability of LLMs to reason about mental states of people is a foundational capability for exciting use cases and problematic misuse. Systematic and broad benchmarking of these abilities is thus a pressing concern, and we believe BigToM is an important step.