# Clustering with Non-adaptive Subset Queries

Hadley Black

UC San Diego

&Euiwoong Lee

University of Michigan

&Arya Mazumdar

UC San Diego

&Barna Saha

UC San Diego

###### Abstract

Recovering the underlying clustering of a set \(U\) of \(n\) points by asking pair-wise _same-cluster_ queries has garnered significant interest in the last decade. Given a query \(S U,|S|=2\), the oracle returns _yes_ if the points are in the same cluster and _no_ otherwise. We study a natural generalization of this problem to _subset queries_ for \(|S|>2\), where the oracle returns the number of clusters intersecting \(S\). Our aim is to determine the minimum number of queries needed for exactly recovering an arbitrary \(k\)-clustering. We focus on non-adaptive schemes, where all the queries are asked in one round, thus allowing for the querying process to be parallelized, which is a highly desirable property.

For _adaptive_ algorithms with pair-wise queries, the complexity is known to be \((nk)\), where \(k\) is the number of clusters. In contrast, _non-adaptive_ pair-wise query algorithms are extremely limited: even for \(k=3\), such algorithms require \((n^{2})\) queries, which matches the trivial \(O(n^{2})\) upper bound attained by querying every pair of points. Allowing for _subset queries_ of unbounded size, \(O(n)\) queries is possible with an adaptive scheme. However, the realm of non-adaptive algorithms remains completely unknown. Is it possible to attain algorithms that are _non-adaptive_ while still making a near-linear number of queries?

In this paper, we give the first non-adaptive algorithms for clustering with subset queries. We provide, (i) a non-adaptive algorithm making \(O(n^{2}n k)\) queries which improves to \(O(n k)\) when the cluster sizes are within any constant factor of each other, (ii) for constant \(k\), a non-adaptive algorithm making \(O(n n)\) queries. In addition to non-adaptivity, we take into account other practical considerations, such as enforcing a bound on query size. For constant \(k\), we give an algorithm making \((n^{2}/s^{2})\) queries on subsets of size at most \(s\), which is optimal among all non-adaptive algorithms within a \( n\)-factor. For arbitrary \(k\), the dependence varies as \((n^{2}/s)\).

## 1 Introduction

Clustering is one of the most fundamental problems in unsupervised machine learning, and permeates beyond the boundaries of statistics and computer science to social sciences, economics and so on. The goal of clustering is to partition items so that similar items are in the same group. The applications of clustering are manifold. However, finding the underlying clusters is sometimes hard for an automated process due to data being noisy, incomplete, but easily discernible by humans. Motivated by this scenario, in order to improve the quality of clustering, early works have studied the so-called clustering under "limited supervision" (e.g.,). Balcan and Blum initiated the study of clustering under active feedback  where given the current clustering solution, the users can provide feedback whether a cluster needs to be merged or split. Perhaps a simpler query model would be where users only need to answer the number of clusters, and that too only on a subset of points without requiring to analyze the entire clustering. This scenario is common in unsupervised learning problems, where a centralized algorithm aims to compute a clustering by crowdsourcing. The crowd-workers play the role of an oracle here, and are able to answer simple queries that involve a small subset of the universe.

Mazumdar and Saha , and in independent works Mitzenmacher and Tsourakis , as well as Asthani, Kushagra and Ben-David  initiated a theoretical study of clustering with pair-wise aka _same-cluster_ queries. Given any pair of points \(u,v\), the oracle returns whether \(u\) and \(v\) belong to the same cluster or not. Such queries are easy to answer and lend itself to simple implementations . This has been subsequently extremely well-studied in the literature, e.g. . In fact, triangle-queries have also been studied, e.g. . Moreover, clustering with pair-wise queries is intimately related to several well-studied problems such as correlation clustering , edge-sign prediction problem , stochastic block model  etc.

Depending on whether there is an interaction between the learner/algorithm and the oracle, the querying algorithms can be classified as adaptive and non-adaptive . In adaptive querying, the learner can decide the next query based on the answers to the previous queries. An algorithm is called _non-adaptive_ if all of its queries can be specified in one-round. Non-adaptive algorithms can parallelize the querying process as they decide the entire set of queries apriori. This may greatly speed up the algorithm in practice, significantly reducing the time to acquire answers . Thus, in a crowdsourcing setting being non-adaptive is a highly desirable property. On the flip side, this makes non-adaptive algorithms significantly harder to design. In fact, when adaptivity is allowed, \(nk\) pair-wise queries are both necessary and sufficient to recover the entire clustering, where \(n\) is the number of points in the ground set to be clustered and \(k\) (unknown) is the number of clusters. However as shown in  and our Theorem C.1, even for \(k=3\), even randomized _non-adaptive algorithms can do no better than the trivial \(O(n^{2})\) upper bound attained by querying all pairs_.

We study a generalization of pair-wise queries to subset queries, where given any subset of points, the oracle returns the number of clusters in it. We consider the problem of recovering an unknown \(k\)-clustering (a partition) on a universe \(U\) of \(n\) points via black-box access to a _subset query oracle_. More precisely, we assume that there exists a groundtruth partitioning of \(U=_{i=1}^{k}C_{i}\), and upon querying with a subset \(S U\), the oracle returns \(q(S)=|\{i:C_{i} S\}|\), the number of clusters intersecting \(S\). Considering the limitations of pair-wise queries for non-adaptive schemes, we ask the question if it is possible to use subset queries to design significantly better non-adaptive algorithms.

In addition to being a natural model for interactive clustering, this problem also falls into the growing body of work known as _combinatorial search_ where the goal is to reconstruct a hidden object by viewing it through the lens of some indirect query model (such as group testing ). The problem is also intimately connected to coin weighing where given a hidden vector \(x\{0,1\}^{n}\), the goal is to reconstruct \(x\) using queries of the form \(q(S):=_{i S}x_{i}\) for \(S[n]\). It is known that \((n/ n)\) is the optimal number of queries , which can be obtained by a non-adaptive algorithm. There are improvements for the case when \(\|x\|_{1}=d\) for \(d n\). Moreover, there has been significant work on graph reconstruction where the task is to reconstruct a hidden graph \(G=(V,E)\) from queries of the form \(q(S,T):=|\{(u,v) E:u S,v T\}|\) for subsets \(S,T V\). . There are also algorithms that perform certain tasks more efficiently than learning the whole graph (sometimes using different types of queries) , and quantum algorithms that use fewer queries than classical algorithms .

It is not too difficult to show that an algorithm making \(O(n k)\) queries (Appendix H) is possible for \(k\)-clustering, while \((n)\) queries is an obvious information theoretic lower bound since each query returns \( k\) bits of information and the number of possible \(k\)-clusterings is \(k^{n}=2^{n k}\). In fact, it is possible to have an algorithm with \(O(n)\) query complexity (personal communication, Chakrabarty and Liao). However, both of these algorithms are adaptive, ruling them out for the non-adaptive setting. So far, the non-adaptive setting of this problem remained unexplored.

### Results

Our main results showcase the significant strength of using subset queries in the non-adaptive setting. We give randomized algorithms that recover the exact clustering with probability \(1-\), for any arbitrary constant \(>0\) using only near-linear number of subset queries.

**Theorem 1.1**.: _(Theorem 2.5, simplified) There is a randomized, non-adaptive \(k\)-clustering algorithm making \(O(n^{2}n k)\) subset queries._

For constant \(k\), this dependency can be further improved.

**Theorem 1.2**.: _(Theorem 2.2, simplified) There is a randomized, non-adaptive \(k\)-clustering algorithm making \(O(n n)\) subset queries when \(k\) is any constant._

Note that the algorithm of Theorem 1.2 works for any value of \(k\), but its dependence on this parameter is inferior to that of Theorem 1.1 (see the formal version Theorem 2.2 for the exact dependence on \(k\)). Thus, we state the theorem above for constant \(k\) to emphasize the much improved dependence on \(n\).

Our algorithms also run in polynomial time, and generalizes to work with queries of bounded size.

**Bounding query size:** Another practical consideration is query size, \(s\). Depending on the scenarios, and capabilities of the oracle, it may be easier to handle queries on small subsets. An extreme case is _pair-wise queries_ (\(s=2\)), where \(O(nk)\) pair queries are enough with adaptivity but any non-adaptive algorithm has to use \((n^{2})\) queries even for \(k=3\). Since a subset query on \(S\) can be simulated by \(\) pair queries, we immediately get the following theorem.

**Theorem 1.3**.: _(Corollary C.2, restated) Any non-adaptive \(k\)-clustering algorithm that is only allowed to query subsets of size at most \(s\) must make at least \(((}{s^{2}},n))\) queries._

Theorems 1.1 and 1.2 above show that this can be bypassed by allowing larger subset queries. However, some of these queries are of size \((n)\), and this raises the question, _is there a near-linear non-adaptive algorithm which only queries subsets of size at most \(O()\)?_ We answer this in the affirmative, implying that our lower bound is tight in terms of \(s\).

**Theorem 1.4** (Theorem A.1, informal).: _There is a non-adaptive \(k\)-clustering algorithm making \(O(n n n)\) subset queries of size at most \(O()\) when \(k\) is any constant. For all sufficiently small \(s=o()\), the algorithm makes \(O(}{s^{2}} n)\) subset queries of size at most \(s\)._

The result also extends to arbitrary \(k\) with slightly worse dependency on \(s\) (Theorem 2.5). Our algorithm for bounded queries from Theorem 1.4 has the additional desirable property of being _sample-based_ meaning that each of its queries is a set formed by independent, uniform samples. I.e. the algorithm specifies a query size \(t s\), and then receives \((S,q(S))\) where \(S\) is formed by \(t\) i.i.d. uniform samples from \(U\). Being sample-based enables the algorithm to leave the task of curating each query up to the individual answering the query. The algorithm needs only to specify the query sizes, and then recover the clustering once the queries have been curated and answered.

**The "roughly balanced" case:** Next, we consider the natural special case of recovering a \(k\)-clustering when the cluster sizes are within a constant factor of one another. Informally, let us call such a clustering "roughly balanced".

**Theorem 1.5** (Theorems B.1 and E.1, informal).: _There are non-adaptive algorithms for recovering a roughly balanced \(k\)-clustering which make (a) \(O(n k)\) subset queries when \(k O(n})\), and (b) \(O(n^{2}k)\) subset queries for any \(k n\)._

Allowing two rounds of adaptivityFinally, we show if we allow an extra round of adaptivity, then that helps to improve the dependency on the logarithmic factors further. Specifically, we prove the following theorems.

**Theorem 1.6** (Theorems F.1 and F.3, informal).: _There is a \(2\)-round deterministic \(k\)-clustering algorithm making \(O(n k)\) subset queries. There is a randomized \(2\)-round algorithm for recovering a roughly-balanced \(k\)-clustering making \(O(n k)\) subset queries._

Organization:The remainder of the paper is organized as follows. In Section 2, we give our main results developing non-adaptive algorithms with near-linear query complexity Theorems 1.1 and 1.2. Our results for sample-based, bounded query algorithms are given in Appendix A. Finally, we prove our results for the balanced setting in Appendix B, our lower bounds in Appendix C, and our results for two-round algorithms in Appendix F.

## 2 Algorithms with Nearly Linear Query Complexity

In this section we describe the algorithms behind our main results, Theorems 1.1 and 1.2, and give formal proofs of their correctness. In Section 2.1 we describe an algorithm making \(O(n n)\) subset queries when the number of clusters \(k\) is assumed to be a constant. In general, the dependence on the number of clusters is \(O(k k)\). In Section 2.2, we give an alternative algorithm with \((n)\) query complexity for any \(k n\).

### An \(O(n n)\) Algorithm for Constant \(k\)

Warm Up.When there are only \(2\) clusters, there is a trivial non-adaptive algorithm making \(O(n)\)_pair queries_: Choose an arbitrary \(x U\) and query \(\{x,y\}\) for every \(y U\). The set of points \(y\) where \(q(\{x,y\})=1\) form one cluster, and the second cluster is the complement. If we allow one more round of adaptivity, then for \(3\)-clustering we could repeat this one more time and again get an \(O(n)\) query algorithm. However, for _non-adaptive_\(3\)-clustering it is impossible to do better than the trivial \(O(n^{2})\) algorithm (see Theorem C.1). Essentially, this is because in order to distinguish the clusterings \((\{x\},\{y\},U\{x,y\})\) and \((\{x,y\},,U\{x,y\})\) the algorithm must query \(\{x,y\}\) and their are \(\) ways to hide this pair. Overcoming this barrier using subset queries require significant new ideas.

Our main ideas are best communicated by focusing on the case of \(3\)-clustering. It suffices to correctly reconstruct the two largest clusters, since the third cluster is just the complement of their union. Let \(A,B\) denote the largest, and second largest clusters, respectively. Since \(|A| n/3\), it is easy to find: sample a random \(x U\) and query \(\{x,y\}\) for every \(y U\). The cluster containing \(x\) is precisely \(\{y U q(\{x,y\})=1\}\). With probability at least \(1/3\), we have \(x A\) and so repeating this a constant number of times will always recover \(A\). On the other hand, \(B\) may be arbitrarily small and in this case the procedure clearly fails to recover it. The first observation is that once we know \(A\), we can exploit larger subset queries to explore \(U A\) since \(q(S A)=q(S)-(S A)\). Importantly, the algorithm is non-adaptive and so the choice of \(S\) cannot depend on \(A\), but we are still able to exploit this trick with the following two strategies. Let \( n=|B|\) denote the size of \(B\) and note that this implies \(|A|(1-2)n\) since the third cluster is of size at most \(B\).

_Strategy 1:_ Suppose a query \(S\) contains exactly one point outside of \(A\), i.e. \(S A=\{x\}\). Then, for \(y A\), \(q(S\{y\})=q(S)\) iff \(x,y\) belong to the same cluster. Thus, we can query \(S\{y\}\) for every \(y U\) to learn the cluster containing \(x\). If \(S\) is a random set of size \(t 1/\), then the probability that \(|S A|=1\) is at least \(t(1-2)^{t-1}=(1)\). Of course, we do not know \(\), but we can try \(t=2^{p}\) for every \(p n\) and one of these choices will be within a factor of \(2\) from \(1/\). This gives us an \(O(n n)\) query algorithm since we make \(n\) queries per iteration.

_Strategy 2:_ Suppose \(S\) intersects \(A\) and contains exactly two points outside of \(A\), i.e. \(S A=\{x,y\}\). Then, \(q(\{x,y\})=q(S)-1\) which tells us whether or not \(x,y\) belong to the same cluster. If \(x,y\) belong to same cluster, add it to a set \(E\), and let \(G(U A,E)\) denote a graph on the remaining points with this set of edges. By transitivity, a connected component in this graph corresponds to a subset of one of the remaining two clusters. In particular, if the induced subgraph, \(G[B]\), is connected, then we recover \(B\). Moreover, if \(S\) is a random set of size \(t 1/\), then the probability that two points land in \(B\) and the rest land in \(A\) is at least \(^{2}(1-2)^{t-2}=(1)\). A basic fact from random graph theory says that after \(|B||B| n\) occurrences of this, \(G[B]\) becomes connected with high probability and so querying \(( n n)\) random \(S\) of size \( 1/\) will suffice. Again, we try \(t=2^{p}\) for every \(p n\), resulting in a total of \( n n_{p}2^{-p}=O(n n)\) queries.

Finally, we can combine strategies (1) and (2) as follows to obtain our \(O(n n)\) query algorithm. The main observation is that the query complexity of strategy (2) improves greatly if we assume that \(|B|\) is small enough. If we know that \(\), then we only need to try \(t=2^{p} n\) and so the query complexity becomes \( n n_{p n}2^{-p}=O(n)\). On the other hand, if we assume that \(>\), then in strategy (1) we only need to try \(p n\) yielding a total of \(O(n n)\) queries. Combining these yields the final algorithm.

**Remark 2.1** (On approximate clustering).: _We point out that these ideas can be used to obtain more efficient algorithms for the easier task of correctly clustering a \((1-)\)-fraction of points. In this setting we can ignore the case of \(</2\) (recall the definition of \(\) above) as this will only result in an incorrect classification of an \(\)-fraction of points. Thus, for example, one can employ "strategy 1" above, but only iterate over \(p(2/)\), leading to an \(O(n)\) query algorithm. However, in this paper we focus on the more challenging task of recovering the clustering exactly, and leave the possibility of more efficient approximate algorithms as a possible direction of future work._

Algorithm.A full description of the algorithm is given in pseudocode Alg. 1, which is split into two phases: a "query selection phase", which describes how queries are chosen by the algorithm, and a "reconstruction phase" which describes how the algorithm uses the query responses to determine the clustering. Both phases contain a for-loop iterating over all \(p\{0,1,, n\}\) where the goalof the algorithm during the \(p\)'th iteration is to learn all remaining clusters of size at least \(}\). This is accomplished by two different strategies depending on whether \(p\) is small or large.

When \(p n\), the algorithm samples \(O(k k)\) random sets \(T\) formed by \(2^{p}\) samples from \(U\) and makes a query on \(T\) and \(T\{x\}\) for every \(x U\) (see lines 5-9 of Alg. 1). Let \(_{p}\) be the union of all clusters reconstructed before phase \(p\) (i.e., clusters of size at least \(}\)). If such a \(T\) contains exactly one point \(z T_{p}\) belonging to an unrecovered cluster, then we can use these queries to learn the cluster containing \(z\) (see lines 24-28 of Alg. 1), since for \(x U_{p}\), \(q(T)=q(T\{x\})\) if and only if \(x,z\) belong to the same cluster. Moreover, we show that this occurs with probability \((1)\) and repeat this \(O(k k)\) times to ensure that every cluster \(C\) where \(|C|},}\)) is learned with high probability. The total number of queries made during iterations \(p n\) is \(O(n n k k)\).

When \(p> n\), the algorithm queries \(O(nk})\) random sets \(T\) again formed by \(2^{p}\) samples from \(U\) (see lines 11-14 of Alg. 1). Note that \(_{p> n}2^{-p}=O()\) and so the total number of queries made during these iterations is \(O(nk)\).

We now describe the reconstruction phase (see lines 32-37 of Alg. 1). If \(T\) contains exactly two points \(x,y T_{p}\) belonging to unrecovered clusters, then we can use the fact that we already know the clustering on \(_{p}\) to tell whether or not \(x,y\) belong to the same cluster or not, i.e. we can compute \(q(\{x,y\})\{1,2\}\) from \(q(T)\). We then consider the set of all such pairs where \(q(\{x,y\})=1\) (this is \(Q_{p}^{n}\) defined in line 34) and consider the graph \(G\) with this edge set, and vertex set \(U_{p}\), the set of points whose cluster hasn't yet been determined. If two points belong to the same connected component in this graph, then they belong to the same cluster. Thus, the analysis for this iteration boils down to showing that with high probability, the induced subgraph \(G[C]\) will be connected for every \(C\) where \(|C|[},})\). This is accomplished by applying a basic fact from the theory of random graphs, namely Fact 2.4.

AnalysisWe restate the main theorem for this section.

**Theorem 2.2**.: _There is a non-adaptive algorithm for \(k\)-clustering that uses \(O(n n k k)\) subset queries and succeeds with probability at least \(1-\) for any constant \(>0\)1._

The following Lemma 2.3 establishes that after the first \(p\) iterations of the algorithm's query selection and reconstruction phases, all clusters of size at least \(}\) have been learned with high probability. This is the main technical component of the proof. After stating the lemma we show it easily implies that Alg. 1 succeeds with probability at least \(99/100\) by an appropriate union bound. The choice of \(99/100\) is arbitrary, and can be made \(1-\) for any constant \(\).

**Lemma 2.3**.: _For each \(p=0,1,, n\), let \(_{p}\) denote the event that all clusters of size at least \(}\) have been successfully recovered immediately following iteration \(p\) of Alg. 1. Then,_

\[[_{0}]\ \ [_{p}_{p-1}]\ p\{1,2, n\}.\]

Proof of Theorem 2.2:.: Before proving Lemma 2.3, we first observe that it immediately implies the correctness of Alg. 1 and thus proves Theorem 2.2. Let \(I_{0}=(,n]\) and for \(1 p n\), let \(I_{p}=[},})\). If there are no clusters \(C\) for which \(|C| I_{p}\), then trivially \([_{p}_{p-1}]=0\), and otherwise \([_{p}_{p-1}]\) by the lemma. Since there are \(k\) clusters, clearly there are at most \(k\) values of \(p\) for which there exists a cluster with size in the interval \(I_{p}\). Using this observation and a union bound, we have

\[[_{ n}][_{0}]+_{p=1}^{ n} [_{p}_{p-1}]\]

which completes the proof of correctness since the algorithm succeeds iff \(_{ n}\) occurs.

Query complexity:During iterations \(p< n\) the algorithm makes at most \(O(n n k k)\) queries. During iterations \(p> n\), it makes at most \(O(nk n)_{p> n}2^{-p}=O(nk)\) queries since \(k n\).

Time complexity:We assume that obtaining a uniform random sample from a set of size \(n\) can be done in \(O(1)\) time. Thus, since the algorithm makes \(O(n n k k)\) queries and each is on a set of size at most \(n\), the total runtime of the query selection phase (lines 3-15) is bounded by \(O(n^{2} n k k)\). We now account for the runtime in the reconstruction phase. Lines (25-28) clearly can be performed in \(O(n)\) time and so the time spent in lines (24-28) is \(O(|Q_{p}| n)\). Now, for \(T Q_{p}\), checking if \(|T_{p}|=2\) can clearly be done in \(O(n)\) time and so lines (33-34) run in time \(O(|Q_{p}| n)\). Line (36) amounts to finding every connected component in \(G_{p}\) which can be done in time \(O(|Q_{p}^{}|+n)=O(|Q_{p}|+n)\) by iteratively running a BFS (costing time linear in the number of edges plus the number of vertices). Thus, the runtime of the \(p\)'th iteration of the for-loop is always dominated by \(O(|Q_{p}| n)\). Since the total number of queries is \(O(n n k k)\), the total runtime of the reconstruction phase is \(O(n^{2} n k k)\).

We now prove the main Lemma 2.3.

Proof.: _of Lemma 2.3._ Let \(_{p}\) denote the set of clusters recovered before phase \(p\) and let \(_{p}=_{C_{p}}C\). When \(p=0\), both of these sets are empty. We will consider three cases depending on the value of \(p\).

**Case 1:**\(p=0\). Let \(C\) denote some cluster of size \(|C|\). Note that in this iteration the sets \(T\) sampled by the algorithm in line (7) are singletons. We need to argue that one of these singletons will land in \(C\), and thus \(C\) is recovered in line (28), with probability at least \(1-}\). Since there are at most \(k\) clusters, applying a union bound completes the proof in this case.

A uniform random element lands in \(C\) with probability at least \(\) and so this fails to occur for all \(|Q_{0}| 4k 10k\) samples with probability at most \((1-)^{4k 10k}(-2 10k)=}\), as claimed.

**Case 2:**\(1 p n\). Let \(C\) denote some cluster with size \(|C|[},})\). Note that we are conditioning on the event that every cluster of size \(}\) has already been successfully recovered after iteration \(p-1\). Thus, the number of elements belonging to unrecovered clusters is \(|U_{p}| k}=}\). We need to argue that the set \(Q_{p}\) will contain some \(T\) sampled in line (7) such that \(T_{p}=\{z\}\) where \(z C\), and thus \(C\) is successfully recovered in line (28), with probability at least \(1-}\). Once this is established, the lemma again follows by a union bound. We have

\[_{T|T|=2^{p}}[|T_{p}|=1T _{p} C]=|T|( _{p}|}{n})^{|T|-1}}{k 2^{p+1}}(1-} )^{2^{p}}\]

and so the probability that this occurs for some \(T Q_{p}\) is at least \(1-(1-)^{4ek 10k} 1-}\), as claimed.

**Case 3:**\(p> n\). Let \(C\) denote some cluster with size \(|C|[},})\). Note that \(|U_{p}| k}=}\). Recall from lines (34-35) the definition of \(Q_{p}^{}\) and recall that \(G_{p}\) is the graph with vertex set \(U_{p}\) and edge set \(Q_{p}^{}\). We need to argue that the induced subgraph \(G_{p}[C]\) is connected, and thus \(C\) is successfully recovered in lines (36-37), with probability at least \(1-}\). Once this is established, the lemma again follows by a union bound. We rely on the following standard fact from the theory of random graphs. For completeness, we give a proof in Appendix D.2.

**Fact 2.4**.: _Let \(G(N,p)\) denote an Erdos-Renyi random graph. That is, the graph contains \(N\) vertices and there is an edge between each pair of vertices with probability \(p\). If \(p 1-(/3N)^{2/N}\), then \(G(N,p)\) is connected with probability at least \(1-\)._

Consider any \(x,y C\) and observe that

\[_{T|T|=2^{p}}[T_{p}=\{x,y\}]=}{2} }(_{p}|}{n})^{2^{p}-2} }{3n^{2}}(1-})^{2^{p}}}{10n^{2}}.\]```
1Input: Subset query access to a hidden partition \(C_{1} C_{k}=U\) of \(|U|=n\) points;
2(Query Selection Phase)
3for\(p=0,1,, n\)do
4 Initialize \(Q_{p}\);
5if\(p n\)then
6 Repeat\(4ek(10k)\) times;
7
8 Sample \(T U\) formed by \(2^{p}\) independent uniform samples from \(U\);
9
10 Query\(T\) and \(T\{x\}\) for all \(x U\);
11
12 Add \(T\) to \(Q_{p}\);
13
14 end if
15if\(p> n\)then
16 Repeat\()}{2^{p}}\) times;
17
18 Sample \(T U\) formed by \(2^{p}\) independent uniform samples from \(U\);
19
20 Query\(T\) and add it to \(Q_{p}\);
21
22 end if
23
24 end for
25
26
27(Reconstruction Phase)
28 Initialize learned cluster set \(_{0}\);
29for\(p=0,1,, n\)do
30 Let \(_{p}\) denote the collection of clusters reconstructed before iteration \(p\);
31 Let \(_{p}=_{C_{p}}C\) denote the points belonging to these clusters;
32 Initialize \(_{p+1}_{p}\);
33if\(p n\)then
34for\(T Q_{p}\)do
35if\(|T_{p}|=1\)then
36 Let \(z\) denote the unique point in \(T_{p}\);
37 If \(x U_{p}\), then \(q(T)=q(T\{x\})\) iff \(x,z\) are in the same cluster;
38 Thus, we add \(\{x U_{p} q(T)=q(T\{x\})\}\) to \(_{p+1}\);
39 end if
40
41 end for
42
43 end for
44if\(p> n\)then
45 Let \(Q^{}_{p}=\{T_{p} T Q_{p}|T_{p}|=2\}\). Since each \(T Q_{p}\) is a uniform random set, the elements of \(Q^{}_{p}\) are uniform random pairs in \(U_{p}\);
46 Let \(Q^{}_{p}=\{\{x,y\} Q^{}_{p} q(\{x,y\}=1)\}\) denote the set of pairs in \(Q^{}_{p}\) where both points lie in the same cluster. This set can be computed since \(q(T_{p})=q(T)-q(T_{p})\) and \(q(T_{p})\) is known since at this point we have reconstructed the clustering on \(_{p}\);
47 Let \(G_{p}\) denote the graph with vertex set \(U_{p}\) and edge set \(Q^{}_{p}\);
48 Let \(C_{1},,C_{}\) denote the connected components of \(G_{p}\) with size at least \(}\);
49 Add \(C_{1},,C_{}\) to \(_{p+1}\);
50
51 end for
52
53 end for Output clustering \(_{ n+1}\) ```

**Algorithm 1**Non-adaptive Algorithm for Constant \(k\)Recall that the algorithm queries \(|Q_{p}|=)}{2^{p}}\) random sets \(T\) of size \(2^{p}\). Thus,

\[_{Q_{p}}[(x,y) E(G_{p}[C])] =_{Q_{p}}[\{x,y\} Q_{p}^{}]=_{Q_{p }}[ T Q_{p} T_{p}=\{x,y\}]\] \[ 1-(1-}{10n^{2}})^{40}  k(300nk^{2})}\] \[ 1-(-}{n} 4k(300nk^{2}))\]

and using \(|C|}\) and \(|C| n\), we obtain

\[_{Q_{p}}[(x,y) E(G_{p}[C])]  1-(-)}{|C|})\] \[ 1-(-|C|)}{|C|})=1-( |C|})^{}.\]

Thus, \((x,y)\) is an edge in \(G_{p}[C]\) with probability at least \(1-(|C|})^{}\) and so by Fact 2.4\(G_{p}[C]\) is connected with probability at least \(1-}\), as claimed. 

Bounded Query SizeWe can restrict the query size to \(s\), and still achieve a near-linear query complexity. We sketch the main ideas here for the case of \(k=3\) similar to the "warm-up" in Section2.1. Details are provided in AppendixA. Our Theorem1.4 gives an \(O(n n n)\) query non-adaptive sample-based algorithm using subset queries of size at most \(O()\). The main idea is to employ "Strategy 2" described in the warm-up section of Section2.1 with a slight alteration. Let \(A,B\) denote the largest, and second largest clusters, respectively, where \(|B|= n\) and so \(|A|(1-2)n\). Observe that if we take a random set \(S\) of size \(t\), then the probability that two points land in \(B\) and the rest land in \(A\) is at least \(^{2}(1-2)^{t-2}=()\). Recalling the definition of the graph \(G\) and the discussion in Section2.1, after querying \((n n)\) such \(S\), the induced subgraph \(G[B]\) becomes connected with high probability, thus recovering the clustering. Similar ideas let us generalize to any \(s\), and achieve an optimal dependency on \(s\) as stated in CorollaryC.2 for constant \(k\).

### An \(O(n^{2}n k)\) Algorithm for General \(k\)

We now consider the situation with general \(k\), for which our algorithm and analysis follow a completely different approach by using techniques from combinatorial group testing.

Warm up.The main subroutine in our algorithm is a procedure for recovering the support of a Boolean vector via \(\) queries. Given a vector \(v\{0,1\}^{n}\), an \(\) query on a set \(S[n]\) returns \(_{S}(v)=_{i S}v_{i}\), i.e. it returns \(1\) iff \(v\) has a \(1\)-valued coordinate in \(S\). The problem of recovering the support of \(v\), \((v)=\{i v_{i}=1\}\) via \(\) queries is a basic problem from the group testing and coin-weighing literature. The relevance of this problem for \(k\)-clustering with subset queries is as follows. Consider a hidden clustering \(C_{1} C_{k}=U\). Given \(x U\), let \(C(x)\) denote the cluster containing \(U=\{x_{1},,x_{n}\}\) (an arbitrary ordering of \(U\)), and let \(v^{(x)}\{0,1\}^{n}\) denote the Boolean vector where \(v_{i}^{(x)}=(x_{i} C(x))\). An \(\) query on set \(S\) to \(v^{(x)}\) can be simulated by a subset query to the clustering on sets \(S\) and \(S\{x\}\) since

\[_{S}(v^{(x)})=_{i S}v_{i}^{(x)}=(C(x) S )=(q(S\{x\})=q(S)).\]

Thus, the problem or reconstructing \(C(x)\) via subset queries is equivalent to the problem of recovering \(v^{(x)}\) via \(\) queries, up to a factor of \(2\) in the query complexity.

Then, to learn a cluster \(C\) with size \(}|C|}\) it suffices to sample \(O(2^{p})\) random \(x\) (one of which lands in \(C\) with high probability) and then recover \(C(x)\) using \(O(})\)\(\) queries. Iterating over every \(p n\) and boosting the number of samples to guarantee a high probability of success for all \(k\) clusters yields our algorithm.

This algorithm can also be restricted to only make subset queries of size at most \(s\), and the query complexity scales with \(\).

**Theorem 2.5**.: _For every \(s[2,n]\), there is a non-adaptive \(k\)-clustering algorithm making \(O(n n k(+ s))\) subset queries of size at most \(s\). In particular, for unbounded query size the algorithm makes \(O(n^{2}n k)\) queries._

Proof of Theorem 2.5We will use the following lemma for recovering \((v)=\{i v_{i}=1\}\) via \(\) queries. We prove and discuss this lemma in Appendix D.1 (see Lemma D.5).

**Lemma 2.6**.: _Let \(v\{0,1\}^{n}\) and \(s,t 1\) be positive integers where \(s\). There is a non-adaptive algorithm that makes \(O()\)\(\) queries on subsets of size \(s\), and if \(|(v)| t\), returns \((v)\) with probability \(1-\), and otherwise certifies that \(|(v)|>t\). The algorithm runs in time \(O(n)\)._

Recall that \(_{S}(v^{(x)})=(q(S\{x\})=q(S))\), i.e. an \(\) query on \(S\) is simulated by subset queries on sets \(S\) and \(S\{x\}\). Thus, we immediately get the following corollary.

**Corollary 2.7**.: _Let \(x U\) and \(r 2,t 1\) be positive integers where \(r\). There is a non-adaptive algorithm that makes \(O()\) subset-queries on sets of size at most \(r\), and if \(|C(x)| t\), returns \(C(x)\) with probability \(1-\), and otherwise certifies that \(|C(x)|>t\). The algorithm runs in time \(O(n)\)._

AlgorithmThe pseudocode for the algorithm is given in Alg. 2. The idea is to draw random points \(x U\) (line 5) and then use the procedure from Corollary 2.7 as a subroutine to try to learn \(C(x)\) (line 6). By the corollary, this will succeed with high probability in recovering \(C(x)\) as long as \(t\) is set to something larger than \(|C(x)|\). Note that the query complexity of this subroutine depends2 on \(t\). If a cluster \(C\) is small, then \([x C]\) is small, but we can call the subroutine with small \(t\), while if \(C(x)\) is large, then \([x C]\) is reasonably large, though we will need to call the subroutine with larger \(t\). Concretely, the algorithm iterates over every \(p\{1,, n\}\) (line 3), and in iteration \(p\) the goal is to learn every cluster \(C\) with \(|C|[},}]\). To accomplish this, we sample \((2^{p} k)\) random points \(x U\) (line 4-5) and for each one, call the subroutine with \(t=}\) (line 6), which is an upper bound on the sizes of the clusters we are trying to learn.Note that we always invoke the corollary with query size \(r=(s,2^{p-1}) s\), enforcing the query size bounded stated in Theorem 2.5.

```
1Input: Subset query access to a hidden partition \(C_{1} C_{k}=U\) of \(|U|=n\) points;
2Initialize hypothesis clustering \(\);
3for\(p=1,, n\)do
4Repeat \(2^{p}(200k)\) times:
5 Sample \(x U\) uniformly at random;
6 Run the procedure from Corollary 2.7 on \(x\) with \(t=}\), query-size \(r=(s,2^{p-1})\), and error probability \(=\). This outputs \(C(x)\), the cluster containing \(x\), with probability at least \(1-\) if \(|C(x)| t\);
7\(\) If the procedure returns a set \(C\), then set \(\{C\}\). Otherwise, continue;
8
9 end for
10Output the clustering \(\). ```

**Algorithm 2**Non-adaptive Algorithm for General \(k\)

Query complexity:Note that the number of queries made in line (6) during the \(p\)'th iteration is \(O( n)\) when \(2^{p-1} s\), and \(O(} n)\) when \(2^{p-1}<s\). Therefore, the total number of queries made is at most

\[O( k)(_{p\ 1 2^{p-1}<s}O(2^{p}} n )+_{p s 2^{p-1} n}O(2^{p} n)).\]

The first sum is bounded by \(O(n n s)\) and the second sum is bounded by \(O(}{s} n)\). The time-complexity is clearly identical by Corollary 2.7.

Time complexity:We assume that attaining a uniform sample from a set of size \(n\) can be performed in \(O(1)\) time. The procedure in line (6) has runtime at most \(O(n n)\) since we set \(=()\). Thus, the total runtime of the algorithm is \(O(n n k)_{p n}2^{p}=O(n^{2} n k)\).

Correctness:Consider any cluster \(C\) and let \(p\{1,, n\}\) be such that \(}|C|}\). Let \(_{C}\) denote the event that some element \(x C\) is sampled in line (5) during iteration \(p\). Let \(_{C}\) denote the event that \(C\) when the algorithm terminates. Observe that by Corollary 2.7, \([_{C}_{C}] 1-=1-\). Moreover, using our lower bound on \(C\) we have

\[[_{C}](1-)^{2^{p} 200k} (1-})^{2^{p} 200k}.\]

Thus, \([_{C}][_{C}]+[_{C }_{C}]\) and taking another union bound over all \(k\) clusters completes the proof.

Acknowledgements.Hadley Black, Arya Mazumdar, and Barna Saha were supported by NSF TRIPODS Institute grant 2217058 (EnCORE) and NSF 2133484. Euiwoong Lee was also supported in part by NSF grant 2236669 and Google. The collaboration is the result of an EnCORE Institute Workshop.