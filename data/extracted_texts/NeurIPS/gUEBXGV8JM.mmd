# Alias-Free Mamba Neural Operator

Jianwei Zheng, Wei Li, Ni Xu, Junwei Zhu, Xiaoxu Lin, and Xiaoqin Zhang

Zhejiang University of Technology, Hangzhou, Zhejiang

Corresponding author.

###### Abstract

Benefiting from the booming deep learning techniques, neural operators (NO) are considered as an ideal alternative to break the traditions of solving Partial Differential Equations (PDE) with expensive cost. Yet with the remarkable progress, current solutions concern little on the holistic function features-both global and local information- during the process of solving PDEs. Besides, a meticulously designed kernel integration to meet desirable performance often suffers from a severe computational burden, such as GNO with \(O(N(N-1))\), FNO with \(O(NlogN)\), and Transformer-based NO with \(O(N^{2})\). To counteract the dilemma, we propose a mamba neural operator with \(O(N)\) computational complexity, namely MambaNO. Functionally, MambaNO achieves a clever balance between global integration, facilitated by state space model of Mamba that scans the entire function, and local integration, engaged with an alias-free architecture. We prove a property of continuous-discrete equivalence to show the capability of MambaNO in approximating operators arising from universal PDEs to desired accuracy. MambaNOs are evaluated on a diverse set of benchmarks with possibly multi-scale solutions and set new state-of-the-art scores, yet with fewer parameters and better efficiency.

## 1 Introduction

Numerous scientific and engineering problems entail recurrently resolving intricate Partial Differential Equation (PDE)  for various parameter values, including morphology across biomes , physical system state , and Radcliffe wave oscillation , to name just a few. Nonetheless, traditional solvers, such as Finite Element Methods (FEM) and Finite Difference Methods (FDM), necessitate an equilibrium between the speed of obtaining solutions and the level of refinement due to the requirement of resolving equations through domain discretization. Another disadvantage of these methods lies in the unavoidable resolving stage when the initial conditions of PDEs are varied. Recently, data-driven methods have emerged as an alternative for the development of faster, more robust, and more accurate solvers due to the natural ability to directly ascertain the trajectory of an PDE family from the data, instead of a single instance of the equation.

These techniques, collectively termed operator learning or neural operator, strive to approximate a well-behaved mapping from input function spaces, such as initial and boundary conditions, to solutions of some PDEs valued also within function spaces. As a burgeoning subject, most well-known NOs have been substantiated, encompassing Operator networks , DeepONets (DON) , Graph Neural Operator (GNO) , Fourier Neural Operator (FNO) , [9; 10], Transformer-based learning architectures [11; 12], and the most recent Convolution Neural Operators (CNO) .

Despite the substantial success of recently introduced operator learning frameworks, existing algorithms continue to demonstrate specific limitations to various degrees. The focus of operator network learning is on mapping between finite-dimensional function spaces, yet the truthful NO often engages with functions owning infinite dimensions. The practice of DON cannot take inputs at any point,while GNO delivers very low efficiency when dealing with heterogeneous data. Transformer-based architectures suffer from slow inference speeds due to the quadratic complexity of attention computation, which serves as a special kernel integration. The use of Galerkin-style attention  can only alleviate rather than solve this problem. FNO accelerates the efficiency of the model by converting global convolution operation, which is a special form of kernel integration, into frequency domain multiplication through Fourier transformation. However, FNO may not respect the framework of alias-free operator learning, mentioned in , and suffers aliasing errors, a fact already identified in . The most recent CNO has established a new state-of-the-art score in this field, which relies on U-Net as the core architecture and proceeds with several specified operations to circumvent aliasing errors. However, as a local operator, CNO falls short in capturing global information, which is of vital importance for functions. Another recently reported model, Mamba [17; 18], has also attracted a great deal of attention due to its capability in capturing global information with linear complexity. Unfortunately, the connection of state space model in Mamba to the integral kernel in NOs is far from intuitive. In addition, Mamba often produces artifacts  or pixel adhesion , raising the challenge of seeking continuous-discrete equivalence (CDE), as highlighted in . Both these two have plagued the naive use of Mamba in the context of NO learning. In this work, we attempt to marry the current merits yet with the disadvantages suppressed. The practical contributions are threefold.

* We propose a novel integral form as the NO kernel, namely mamba integration, costing only \(O(N)\) computational complexity and enabling the grasp of global function information. On that basis, with the local function feature further furnished by convolution, we present an avant-garde Mamba Neural Operator (MambaNO), behaving as a deep PDE solver.
* Apart from proving that MambaNO is intrinsically a representation-equivalent neural operator in the sense of , we also provide a universality result to demonstrate that MambaNO can approximate continuous operators, fitting a large class of PDEs, to desired accuracy.
* We test MambaNO on a full set of benchmarks that span across massive PDEs ranging from linear elliptic and hyperbolic to nonlinear parabolic and hyperbolic equations, with potentially multiscale solutions. It is evidenced that MambaNO outperforms competing baselines on all benchmarks, both when testing in-distribution and in out-of-distribution cases, yet with reduced model parameters and computational costs. Codes are available 2. 
Hence, we offer a new Mamba-based operator learning model incorporating more holistic features, with favorable properties in theory and excellent performance in practice.

## 2 Related Work

### Learnable PDE Solvers

The development of deep learning has revitalized various fields . In recent years, numerous investigations have been conducted on the application of learning networks to solve PDEs [23; 24]. A prevalent treatment involves initially encoding the data spatially, followed by the employment of diverse schemes for temporal evolution. For example, physics-informed approaches utilize PDE supervision to approximate the solution , commonly parameterized by neural networks, within a consolidated space-time domain. Yet, the accuracy is negatively correlated with solving efficiency. Moreover, retraining is required across different instances of a PDE.

To design more robust and efficient algorithms, Lu _et al._ pioneeringly introduced the practical implementation of universal operator approximation theorem , which can further be integrated with prior knowledge of the system . Independently, infinite-dimensional solution operators are approximated by iteratively learnable integral kernels . Such kernel can be parameterized by message-passing , neural networks , attention mechanism [11; 29], convolution in Fourier domain  or in bandlimited function space . Following this, we further present a novel form called as mamba integration, capturing global function information in \(O(N)\) time complexity.

### Alias-free Framework

Aligned with the discussion in , a neural operator or an operator learning architecture should hold the ability in managing functions as both inputs and outputs. Yet, in digital environments, engagement and computation with functions are implemented via discrete representations, e.g., grid point values, cell averages, or generally coefficients of a specific basis, instead of the original continuous entities. Recently, the alias-free framework , in which the aliasing error is zero, was proposed. We provide a detailed definition in the Supplementary Material (SM) A.1. Within this framework, continuous functions should be consistently, or uniquely and stably, derived from the discrete counterparts, e.g., point evaluations, basis coefficients, etc., at any resolution, hammering at reconciling potential inconsistencies between operators and their discrete representations. Models that do not follow this framework, such as CNNs, may generate inconsistent outputs as the resolution varies, and thus are not Representation equivalent Neural Operators (ReNO), whose definition is given in SM A.2. Similarly, the property of continuous-discrete equivalence (CDE), proposed by  that measures the consistency between discrete and continuous representations, cannot be guaranteed. In addition, the high-frequency information introduced by point-wise activation functions also leads to similar inconsistencies [31; 20]. Therefore, FNO cannot be considered as a ReNO that meets the alias-free framework. More recently, CNO was elaborated as the first instance that conforms to the alias-free framework , paving the way for the implementation of our MamboNO.

## 3 Mamba Neural Operator

**Setting.** The core purpose here involves learning a mapping between two infinite-dimensional spaces via a finite array of observed input-output pairings from this correspondence. Generally, the problem concerned can be formally delineated as follows. Let \(=H^{r}(D,^{d_{}})\) and \(=H^{s}(D,^{d_{}})\) be Sobolev spaces of functions defined on \(2\)-dimensional bounded domains \(D=^{2}\) and \(^{}:\) be a non-linear map. By constructing a parametric map \(_{}\), the practice would be to build an approximation of \(^{}\) from data pairs \(u_{i},^{}(u_{i})_{i=1}^{N} \), i.e.,

\[_{}:,^{p},\] (3.1)

with parameters from the finite-dimensional space \(^{p}\) by seeking \(^{}^{p}\) so that \(_{}^{}\). The practical learning of \(_{}\) can be naturally addressed through the empirical-risk minimization problem,

\[_{^{p}}\|^{}(u)-_{}(u)\|_{}^{2}_{^{p}}_{i=1}^{N}\|^{}(u_{i})-_{}(u_{i}) \|_{}^{2}, u.\] (3.2)

Given that our methodology is conceived within the infinite-dimensional context, each finite-dimensional approximation enjoys a shared set of network parameters which are consistent in infinite environment devoid of approximations.

**Bandlimited Approximation.** Since the original Sobolev space is too large to allow for any form of continuous-discrete equivalence (A.1), we select a subspace \(\) instead of the original \(\). On that basis, the possibility of achieving equivalence between the underlying operator and its discrete representations would be basically guaranteed. In this respect, we elaborate the space of bandlimited functions defined by

\[_{w}(D)=\{f L^{2}(D):[-w,w]^{2}\},\] (3.3)

in which \(w>0\) denotes the frequency bound and \(\) represents the Fourier transform of \(f\). Note that if a bandlimited function can approximate the original function with arbitrary precision (depending on \(w\)), then a bandlimited operator mapping between bandlimited functions can also approximate the original operator with arbitrary precision. In other words, for any \(>0\), there exist a \(w\) and a continuous operator \(_{}:_{w}(D)_{w}(D)\) such that \(\|^{}-_{}\|<\), with \(\|\|\) pertaining to the corresponding operator norm. In addition, let \(P_{w}\) denote a certain Fourier projection, \(P_{w}(g)\) is capable of discarding the high-frequency components higher than frequency \(w\), where \(g(D)\) is any function in that space.

**Definition of MambaNO.** As the underlying operator maps between the spaces of bandlimited functions, the operator approximation architecture shall be constructed in a structure-preserving fashion. That is to say, it is dedicated to form a corresponding mapping in-between bandlimited functions, thus respecting the CDE property. To that end, a Mamba Neural Operator (MamboNO) is engineered to approximate the operator \(_{}:_{w}(D)_{w}(D)\). Following the common paradigm of most NOs [12; 32; 33], our practical elaboration also lies in a compositional mapping,

\[_{}:= P_{T}(_{T}(W_{T-1}+_{T- 1}+b_{T-1})) P_{1}(_{1}(W_{0}+_{0}+b_{0})) ,\] (3.4)where

\[:\{u_{w}(D,^{d_{}}) \}\{v_{0}_{w}(D,^{d_{0}})\},\] (3.5a) \[Q:\{v_{T}_{w}(D,^{d_{T}})\} \{_{w}(D,^{d_{Y}})\}.\] (3.5b)

are the lifting and projection mappings respectively, \(W_{t}^{d_{t+1} d_{t}}\) are linear operators in a matrix form, \(b_{t}:D^{d_{t+1}}\) are bias functions, \(_{t}\) are continuous functions, and \(P_{t}\) are either upsampling or downsampling operators in each layer. Note that all these operations are performed locally or pointwisely, following the principle of discretization invariance  to some extent. The general formulation of the kernel integral operator \(_{t}:\{v_{t}:D^{d_{t}}\}\{v_{t+1}: D^{d_{t+1}}\}\) is parameterized by \(\) such as:

\[(_{t}(v_{t});)(x)=_{D}K_{t}(x,y,v_{t}(x),v_{t}(y))v_{t}( y)y x,y D,\] (3.6)

in which the parameter of kernel \(K_{t}\) is learnt from given data. For instance, FNO  employs convolution as the primary operation, while Transformer-based neural operators  leverage attention mechanisms. In this work, a new integral form fitting Mamba architecture is crafted.

**Mamba Integration.** For simplicity of the exposition, we omit the subscript \(t\) from Eq. (3.6) hereafter, which is originally used to denote the number of iterations during integration flow.

\[(_{t}(v_{t});)(x)=_{D}K_{t}(x,y,v(x),v(y))v(y)y.\] (3.7)

Following the tradition of FNO , we first assume that \(K_{t}:^{D}^{D}^{d_{t+1} d _{t}}\) concerns little on the spatial variables \((v(x),v(y))\), but only on the input pair \((x,y)\). Then, we let

\[K_{t}(x,y)=Ce^{Ax} Be^{-Ay},\] (3.8)

where \(A\), \(B\) and \(C\) are temporarily constant parameters, as for an easier deduction purpose. To further ensure a possible employment of the scanning pattern used in Mamba , we set the integration interval to \(y(-,x)\) instead of the entire definition domain \(D\), hence Eq. (3.7) becomes

\[(_{t}(v_{t});)(x)=_{-}^{x}(Ce^{Ax} Be^{-Ay})v( y)y.\] (3.9)

Figure 1: The overall architecture of MambaNO.

Clearly, \(Ce^{Ax}\) is independent of the integral variable \(y\), from which Eq. (3.9) can be rewritten as:

\[(_{t}(v_{t});)(x)=Ch(x),h(x)=e^{Ax}_{-}^{x}B(e^ {-Ay})v(y)y.\] (3.10)

Furthermore, with simple operation of differential performed on \(x\), we can get

\[h^{{}^{}}(x)=Ah(x)+Bv(x),\] (3.11)

which together with Eq. (3.10) leads to

\[h^{}(x)= Ah(x)+Bv(x),\] (3.12) \[u(x)= Ch(x),\]

where \(u(x)=(_{t}(v_{t});)(x)\). More details of the deduction can be found in SM B.1. By now, we have seamlessly married the computation of kernel integral in Eq. (3.7) with a State Space Model (SSM) . Drawing inspiration from the theory of continuous systems, the goal of Eq. (3.12) is to map a two-dimensional function, denoted as \(v(x)\), to \(u(x)\) through the hidden space \(h(x)\). Within this context, \(A\) serves as the evolution parameter, while \(B\) and \(C\) act as the projection parameters. To integrate Eq. (3.12) into deep learning paradigm, a discretization process is initially necessary. Note this transformation is crucial to align the model with the sampling rate of the underlying signal embodied in the input data, enabling computationally efficient operations. To address the drift and diffusion effects within most PDEs, unlike the Monte Carlo approximation used in conventional integrals, our approach employs the Scharfetter-Gummel method , which approximates the matrix exponential using Bernoulli polynomials, and can be formally defined as follows:

\[}= ( A),\] (3.13) \[}= ( A)^{-1}(( A)-) B,\]

where \(\) is a timescale parameter converting continuous parameters \(A\) and \(B\) into their discrete counterparts \(}\) and \(}\). The discrete representation of Eq. (3.12) can be formulated as follows:

\[h(x_{k})= }h(x_{k-1})+}v(x_{k}),\] (3.14) \[u(x_{k})= Ch(x_{k}),\]

More details can be found in SM A.3. Recall that our intention is to integrate the two-dimensional function in a scanning manner for integration, capturing global information with \(O(N)\) complexity. In Eq. (3.14), \(h(x)\), serving as the hidden space, encapsulates relevant information about the integrated points before \(x\). Therefore, through Carleman bilinearization, we can construct a kernel to approximate the nonlinear state space evolution , i.e., the output can be derived through global convolution:

\[}=(C},C}},...,C }^{(k-1)}}),\] (3.15)

\[u(x_{k})=v(x_{k})*},\]

where \(}^{k}\) denotes a structured convolution kernel and \(k\) is the sampling points of input \(v\).

**Convolution Integration.** Mamba integration enjoys a global receptive field, yet we believe that introducing local convolution integration would further bring benefits in capturing more holistic features. To commence with, the convolution integration for \(_{w}:_{w}(D)_{w}(D)\) is defined as:

\[_{w}f(x)=_{D}_{w}(x-y)f(y)y=_{i,j=1}^{k} _{ij}f(x-z_{ij}), x D,\] (3.16)

where \(f_{w}\), \(\) is a discrete kernel with size \(k\), \(z_{ij}\) is the resultant grid points. Thus, the convolution operator can be intuitively parameterized in physical space, deviating far from the treatments of Fourier transformation and then followed by matrix multiplication, as in FNO .

**Upsampling, Downsampling, and Activation Operators.** Ideal upsampling is recognized as not altering the continuous representation, simply transforming the original function to a larger bandwidth-limited space. In other words, just viewing the function from a band-limited space as belonging to a higher-bandwidth space does not actually change any values of the function. The upsampling operators for some \(>w\) are defined as,

\[_{w,}:_{w}(D)_{ }(D),_{w,}f(x)=f(x), x D.\] (3.17)On the other side, for certain \(<w\), the concerned function \(f_{w}\) can be downsampled to lower band \(_{}\) by setting \(_{w,}:_{w}(D)_{ }(D)\), defined by

\[_{w,}f(x)=()^{2}_{D}h_{ }(x-y)f(y)y, x D,\] (3.18)

where \(h_{}\) is an interpolation sinc filter, playing the role of removing high-frequency components exceeding the lower band limit \(w\) and converting the function from a higher band-limited space \(_{w}(D)\) to a lower band-limited space \(_{}(D)\). As known, applying the activation function pointwise directly would break the band-limits of the underlying function space, potentially introducing arbitrarily higher-frequency information and causing aliasing errors . We aim to apply the activation function within a sufficiently large band-limited \(\) space to minimize the introduction of high-frequency information and thereby reduce aliasing errors. To this end, we define the activation operators in (3.4) as,

\[_{w,}:_{w}(D)_{w}(D), _{w,}f(x)=_{,w}(_{w,}f)(x), x D.\] (3.19)

Recall that the activation operator within a broader band-limited space is confined in a'sandwich-like' structure, as shown in Eq. (3.4), with lifting \(P\) and projection \(Q\) lying at the edges and kernel integrations occupying the middle position.

``` Input:\(v(x)\), a continuous funtion with shape [sampling points (\(N\)), dimension (\(D\))] Parameters:\(A\), an evolution parameter; \(\), a timescale parameter; \(B\) and \(C\), projection parameters Linear Projection Layer:\(()\) Output:\(u(x)\), a function also with shape [sampling points, dimension]
1:\(\), \(B\), \(C\) = Linear(\(v\)), Linear(\(v\)), Linear(\(v\))
2:\(}=( A)\)
3:\(}=( A)^{-1}(( A)- ) B\) ```

**Algorithm 1** SSM Block (Mamba Integration)

**MambaNO Architecture.** Given bandlimited functions as inputs and outputs, all the concerned ingredients can be assembled in a U-shaped operator architecture, which is graphically given in Fig. 1. As seen, the input function, say \(u_{w}(D,^{d})\) is first lifted and then processed through a series of layers. Five main layers are used, i.e., convolution integration (3.16), Mamba integration (3.15), activation layer (3.19), upsampling layers (3.17), and downsampling layers (3.18). Each layer is fed with a band-limited function, and another band-limited function holding the same band returns as the output. As the entire operation flow runs solely in the channel width, the underlying bandlimits are confirmed since the spatial resolution is left unchanged. Thus, MambaNO assumes a function input and throws it into a set of encoders, where the input is space-wise downsampled but channel-wise widened. Then, the function is passed through a set of decoders, where the channel width is shrunk, yet the space resolution is enlarged. In the meantime, the encoder and decoder layers sharing the same spatial resolution and bandlimits are coupled with a resnet architecture. Thus, as we go deeper into the encoder flow, transferring high-frequency information via skip connections is allowed, avoiding

Figure 2: The cross-scan operation integrates pixels from four directions with \(O(N)\) complexity.

which to be totally filtered out with the sinc operation. In other words, the high-frequency information is not just produced by the activation function but also altered through the intermediate modules.

Practically, the constant parameters \(A\), \(B\), and \(C\) are better learned from the data, allowing attractive model adaptability. This together with the shifted integral interval, as shown in Eq. (3.9), enables cross-scan operation within the state space. As illustrated in Fig. 2, we choose to unfold sampling points into sequences along with rows and columns and then proceed with scanning along four different directions, i.e., top-left to bottom-right, bottom-right to top-left, top-right to bottom-left, and bottom-left to top-right. These sequences are further processed by the SSM block for kernel integration, ensuring that information from various directions is thoroughly scanned, thus capturing diverse function features. Afterwards, the sequences from the four directions are merged, restoring the output function. The pseudo-code for the SSM block is presented in Algorithm 1.

**Continuous-discrete Equivalence for MambaNO.** We have defined MambaNO (3.4) as an mapping operator in-between bandlimited functions and that runs in a scanning pattern within a state space. In practice, like any other computational methods, MambaNO shall be performed in a discrete manner, with discretized variants of individual layers specified in SM A.3. Given the practical implementations of each elementary block, i.e., convolution, up- and down-sampling, activation, etc., we then prove the following proposition, whose details are given in SM B.2:

**Proposition 3.1**: _Mamba Neural Operator \(:_{w}(D,^{d})_{w}(D,^{d})\) is a Representation equivalent Neural Operator (ReNO). That is, MambaNO enjoys the CDE property._

More details on the notion of ReNOs can be found in SM A.2. As a ReNO, MambaNO is representation-equivalent, allowing its migration between grids of different scales, yet with little aliasing errors. This property, which can also be called resolution-discrete invariance, as a significant characteristic of the neural operator, is highlighted in Ref. .

**Complexity Analysis of MambaNO.** Given a discrete counterpart \(u R^{H W D}\) of a two-dimensional continuous function, one can reshape it to get \(u R^{N D}\) with \(N=H W\). For simplicity, we assume the sequence dimensions of \(,A,B,C\) are all \(D\). As shown in step 1 of Algorithm 1, the complexity of generating three learnable projections is \(O(3ND^{2})\). The dicretization steps 2 and 3 involve four matrix multiplications, which cost \(O(4ND^{2})\). Step 4 is the state space model with \(O(N)\) complexity . In summary, the computations of Algorithm 1 are all linear with the sequence length, i.e., \(O(N)\). Please refer to SM C for the time complexity of other models.

## 4 Experiments and Analysis

### Experimental Settings

**Training Details and Baselines.** For fairness and reliability, all experiments are consistently conducted on standardized platform with an NVIDIA RTX 3090 GPU and 2.40GHz Intel(R) Xeon(R) Silver 4210R CPU. Several well-known PDE solvers are used as the competing baselines, such as CNO , FNO , DeepONet (DON) , Galerkin Transformer (GT) , as well as the very typical ResNet and U-Net architectures.

**Representative PDE Benchmarks (RPB).** As a standard set of benchmarks for machine learning of PDEs, RBP focuses solely on two-dimensional PDEs since conventional numerical methods have already yielded quite pleasing outcomes on one-dimensional functions; however, procuring training data for those in three dimensions or higher is immensely cost-prohibitive. With these considerations in mind, RBP covers Poisson Equation, Wave Equation, Transport Equation, Allen-Cahn Equation, Navier-Stokes Eqns, Darcy flow, and Flow past airfoils, which are defined on Cartesian domains. We have roughly listed the related information of the equations in SM D.

### In and Out-of-distribution Results

The test results for both the in- and out-of-distribution evaluations from all competing models are shown in Table 1. Specific for in-distribution experiments, it can be easily observed that, among all competitors, except for the Allen-Cahn equation, CNO enjoys an evident superiority compared to others. Moreover, our MambaNO, benefiting from the introduction of both global and local integrations, outperforms even further. Taking the Poisson equation as an instance, CNO performs twenty times better than FNO, while MambaNO further reduces the error by one-third. On the other hand, we can see that UNet and ResNet also behave well in some cases. This not only verifies the feasibility of convolution as kernel integration, but also shows that deep learning can directly ascertain the trajectory of an equation family from the data. However, the limitations are also evident, lying specifically in the fact that little consideration is given on the alias-free framework. Therefore, when migrating to out-distribution experiments, the experimental results concerned show a significant decline. We can see that in the out-distribution experiments of the Navier-Stokes equation, their performance drops more than that of CNO, which is another instance that adheres to the alias-free modeling. MambaNO again sets a new state-of-the-art score in this case, which not only follows this framework, but also benefits from the global integration. More intuitively, Fig. 3 plots the in-distribution and out-of-distribution visualization of the Navier-Stokes equation, taking the three best-performing models, e.g., FNO, CNO, and MambaNO, as examples. Our first observation is that, whether in the in-distribution or out-distribution environments, the predictions of MambaNO are the closest to the ground truth. The regions within the black box provide the most clear comparisons. Note that FNO is limited by the pointwise activation function that introduces aliasing errors, and CNO is limited by the convolution integral paying little attention to global information, while MambaNO has made improvements on both these issues.

### Resolution Invariance

In the left and central segments of Fig. 4, we compare UNet, FNO, CNO, and MambaNO vis-a-vis the metric of varying errors across resolutions, which is an important property for robust operator learning. The selected equation is the Navier-Stokes benchmark. In this figure, we can see that both UNet and FNO are practically not discrete invariant. With resolution changing, a sharp decline

    & **In/Out** & **GT** & **Unet** & **ResNet** & **DON** & **FNO** & **CNO** & **MambaNO** \\  
**Poisson** & In & 4.09\% & 1.05\% & 0.63\% & 19.07\% & 7.35\% & 0.31\% & 0.17\% \\
**Equation** & Out & 3.47\% & 1.55\% & 1.34\% & 11.18\% & 8.62\% & 0.33\% & 0.21\% \\ 
**Wave** & In & 0.91\% & 0.96\% & 0.70\% & 1.43\% & 0.65\% & 0.40\% & 0.38\% \\
**Equation** & Out & 1.97\% & 2.24\% & 2.50\% & 3.12\% & 1.95\% & 1.29\% & 1.22\% \\ 
**Smooth** & In & 1.18\% & 0.59\% & 0.47\% & 1.38\% & 0.34\% & 0.29\% & 0.26\% \\
**Transport** & Out & 666.07\% & 2.97\% & 2.73\% & 119.61\% & 1.97\% & 0.35\% & 0.34\% \\ 
**Discontinuous** & In & 1.70\% & 1.44\% & 1.41\% & 6.35\% & 1.26\% & 1.11\% & 1.08\% \\
**Transport** & Out & 27270.96\% & 1.62\% & 1.54\% & 140.73\% & 3.47\% & 1.31\% & 1.21\% \\ 
**Allen-Cahn** & In & 1.30\% & 1.38\% & 2.36\% & 22.97\% & 0.87\% & 0.91\% & 0.72\% \\
**Equation** & Out & 3.03\% & 3.28\% & 3.91\% & 20.75\% & 2.18\% & 2.33\% & 2.11\% \\ 
**Navier-Stokes** & In & 4.61\% & 4.94\% & 4.10\% & 12.95\% & 3.97\% & 3.07\% & 2.74\% \\
**Equation** & Out & 17.23\% & 16.98\% & 15.04\% & 23.39\% & 14.89\% & 10.94\% & 5.95\% \\ 
**Darcy** & In & 0.86\% & 0.54\% & 0.42\% & 1.13\% & 0.80\% & 0.38\% & 0.33\% \\
**Flow** & Out & 1.17\% & 0.64\% & 0.60\% & 1.61\% & 1.11\% & 0.50\% & 0.44\% \\ 
**Compressible** & In & 2.33\% & 0.72\% & 1.89\% & 2.15\% & 0.49\% & 0.39\% & 0.34\% \\
**Euler** & Out & 3.14\% & 0.91\% & 2.20\% & 3.08\% & 0.74\% & 0.63\% & 0.61\% \\   

Table 1: Relative median \(L^{1}\) test errors for various benchmarks and models.

Figure 3: Visual predictions on representative in- (top row) and out-of-distribution (bottom row).

happens in the performance of UNet. For FNO, when the resolution is less than 64, there is a \(25\%\) decrease in performance. As resolution increases, a modest fluctuation in errors also occurs. Thanks to the alias-free framework, CNO and MambaNO enjoy much flatter curves along with different resolutions, validating the advantage of respecting CDE property. Between these two, MambaNO further enjoys much lower errors, which is again attributed to the combination of convolution and Mamba integration, introducing the more holistic function features.

Variations in model performance against different data scales are also evaluated on Navier-Stokes. The right segment of Fig. 4 provides the test errors in the log domain. As shown, MambaNO consistently achieves optimal performance compared to GT, FNO, and CNO, regardless of the number of samples used. More encouragingly, the performance lead is yet achieved with the least model parameters.

## 5 Ablation Study

To affirm the benefit from combining both global and local information, Table 2 discloses results pertaining to the ablation of Mamba and Convolution integration. Based on our final configuration, the mamba integration is replaced with convolution integral, thus obtaining baseline 1 with pure local terms. Similarly, we can also obtain baseline 3, which holds pure mamba integration. To ensure a fair comparison, the parameters of all three competitors are configured at the same level. As expected, due to the lack of partial information, the performance of pure convolution integration and pure Mamba integration drops by \(14\%\) and \(32\%\), respectively. In terms of efficiency, while pure convolution treatment enjoys less inference time and FLOPs in this case, it cannot be ensured in practice since more network layers are needed to enlarge the receptive field. Due to space limitations, we have placed more ablation experiments in SM E.

## 6 Conclusion

We propose MambaNO, a novel neural operator for solving PDEs, which incorporates Mamba integration and convolution integration to capture global and local function information holistically, yet in linear complexity. The basic design principle is to marry the integral kernel with the state space model. On that basis, with respect to spaces of bandlimited functions, CDE property is naturally satisfied to authentically assimilate the innate operators, as opposed to the discrete representation surrogation. A suite of experiments conducted on representative PDE benchmarks demonstrate that MambaNO outperforms the recent baselines, such as GT, FNO, and CNO, on many practical metrics namely test errors, running efficiency, resolution invariance, out-of-distribution generalization, and data scaling. **Limitation:** We have presented MambaNO for operators in a fundamental two-dimensional Cartesian domain. The expansion to three-space dimensions is theoretically intuitive, but computationally laborious. This intensifies the computational load for global integration, which inherently escalates the model parameters.

 
**Configuration** & Test errors \(\) & Time \(\) & Params \(\) & FLOPs \(\) \\ 
1. Pure Convolution integration & 1.14 \(\) & 0.83 \(\) & 0.98 \(\) & 0.79 \(\) \\
2. Mamba+Convolution integration* & 1.00 \(\) & 1.00 \(\) & 1.00 \(\) & 1.00 \(\) \\
3. Pure Mamba integration & 1.32 \(\) & 1.14 \(\) & 1.02 \(\) & 1.19 \(\) \\  

Table 2: The ablation results by using different components. * indicates our default choice.

Figure 4: Left \(\&\) Center: Test errors vs. Resolutions. Right: Errors vs. Training samples.