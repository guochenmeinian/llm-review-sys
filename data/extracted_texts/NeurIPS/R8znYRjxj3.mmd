# Bayes-optimal learning of an extensive-width neural network from quadratically many samples

Antoine Maillard

Department of Mathematics

ETH Zurich, Switzerland

&Emanuele Troiani

Statistical Physics Of Computation Laboratory

EPFL, Switzerland

&Simon Martin

INRIA & Laboratoire de Physique

ENS, Universite PSL, France

&Lenka Zdeborova

Statistical Physics Of Computation Laboratory

EPFL, Switzerland

&Florent Krzakala

Information Learning and Physics Laboratory

EPFL, Switzerland

###### Abstract

We consider the problem of learning a target function corresponding to a single hidden layer neural network, with a quadratic activation function after the first layer, and random weights. We consider the asymptotic limit where the input dimension and the network width are proportionally large. Recent work  established that linear regression provides Bayes-optimal test error to learn such a function when the number of available samples is only linear in the dimension. That work stressed the open challenge of theoretically analyzing the optimal test error in the more interesting regime where the number of samples is quadratic in the dimension. In this paper, we solve this challenge for quadratic activations and derive a closed-form expression for the Bayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE, which combines approximate message passing with rotationally invariant matrix denoising, and that asymptotically achieves the optimal performance. Technically, our result is enabled by establishing a link with recent works on optimal denoising of extensive-rank matrices and on the ellipsoid fitting problem. We further show empirically that, in the absence of noise, randomly-initialized gradient descent seems to sample the space of weights, leading to zero training loss, and averaging over initialization leads to a test error equal to the Bayes-optimal one.

## 1 Introduction

Learning with multi-layer neural networks brought impressive progress and applications in many areas. It is well established that a large enough non-linear neural network can represent a large class of functions . Yet the conditions under which the values of the weights can be found efficiently, and from how many samples of the data, remain theoretically elusive. While one may hope that a detailed understanding of these fundamental limitations will eventually allow for a more efficient training, answering such questions for general data and target function remains, however, beyond the reach of current theoretical methods.

In an early attempt to overcome the difficulty of the above generic question, a long line of work originating in Gardner and Derrida (1989); Sompolinsky et al. (1990) proposed to study the optimalsample-complexity in the so-called teacher-student setting, where the target function corresponds to a "teacher" neural network. The architecture of this teacher neural network is chosen to be fully connected feed-forward with a given number of layers, their widths, and activations. The values of each of the weights are generated independently, from a Gaussian distribution. This teacher neural network is then used to generate an output label \(y_{i}\) for each input data sample \(_{i}^{d}\). Given the architecture of the teacher networks (but not the values of the teacher-weights \(^{*}\)) and the training set of input-output pairs \(\{y_{i},_{i}\}_{i=1}^{n}\), the smallest achievable test error can then be obtained by averaging the output of a student-neural network (with the same architecture as the teacher) over the values of weights drawn from the posterior distribution. We will refer to the accuracy reached this way as the Bayes-optimal one. It yields the fundamental limitations in learning such tasks, by any possible means, and can therefore serve as a benchmark.

In the so-called _high-dimensional limit_(Donoho, 2000), when the input training data are \(d\)-dimensional Gaussian vectors, in the limit \(d\), the above research program has been carried out in detail over the last decades for small neural networks having only \(m=O_{d}(1)\) hidden units, and learning from \(n= d\) data samples, where \(=O_{d}(1)\)(see, e.g. Gyorgyi (1990); Opper and Haussler (1991); Seung et al. (1992); Watkin et al. (1993); Schwarze (1993); Barbier et al. (2019); Aubin et al. (2019)). In the more recent literature, this setting is sometimes referred to as learning single-index and multi-index functions (Damian et al., 2022; Bietti et al., 2023; Collins-Woodfin et al., 2023; Dandi et al., 2023, 2024b; Damian et al., 2024). While early works in this line originated in statistical physics and used the heuristic replica method (Mezard et al., 1987) to derive the closed-form expressions for quantities of interest in the high-dimensional limit (with \(d\), \(m=O_{d}(1)\) and \(n=O_{d}(d)\)), a mathematical establishment followed using rigorous probabilistic methods (Barbier et al., 2019; Aubin et al., 2019).

Reaching a closed-form expression for the Bayes-optimal sample complexity for target functions corresponding to multi-layer teacher neural networks is the next open and very challenging task. Among the recent work is Cui et al. (2023), that established (non-rigorously, using the replica method) the Bayes-optimal error for a target function corresponding to a multi-layer neural network of _extensive width_ (i.e. linearly proportional to the dimension) from a number of samples also _linear_ in the dimension. Interestingly, in this limit, the Bayes-optimal error resulted in a quite poor approximation of the function, which can be achieved as well by a simple linear regression on the input-output pairs. No method, be it a multi-layer neural network (or even refinements like a transformer), will be able to achieve better performance. (Cui et al., 2023) further argue, based on numerical evidence, that _quadratically_ many samples in the dimension are necessary in order to be able to learn the target function with non-linear activations1 to an infinitesimally small test error. This is perhaps intuitive as, with an extensive width, the number of parameters/weights in the teacher network is quadratic in dimension. However, such a regime is challenging for current theoretical tools. Reaching an analytical explicit expression for the Bayes-optimal performance in this regime, for the target function in the form of a neural network of extensive width, is an open, challenging, theoretical problem that has not yet been solved even for a single hidden layer architecture.

**Our contributions -** In this paper, we step up to this challenge and derive a closed-form expression for the Bayes-optimal test error for a target/teacher function corresponding to a one-hidden layer neural network of extensive width, from quadratically many samples, for a particular case where the activation function (after the hidden layer) is quadratic. In particular, our main contributions are:

* We provide a closed-form expression for the Bayes-optimal error of learning an extensive-width neural network from quadratically many samples, which is the first type of such result to the best of our knowledge. Such a form is enabled by the high-dimensional limit and corresponding concentration of quantities of interest. It notably follows from our formula that, in the absence of noise in the target function, zero test error is achievable for a sample complexity \(=n/d^{2}\) larger than a perfect-recovery threshold \(>_{}\) where \[_{}=-}{2} 1;_{ }= 1,\] (1) with \(=m/d\) the ratio between the width \(m\) and the dimension \(d\). We further notice that this matches a naive counting of the number of degrees of freedom in the target function.

* We introduce the GAMP-RIE algorithm that combines generalized approximate message passing (GAMP) (Donoho et al., 2009; Rangan, 2011; Zdeborova and Krzakala, 2016) with a matrix denoiser that is based on so-called rotationally-invariant estimators (RIE) (Bun et al., 2016), and show that in the large size limit, this algorithm reaches the Bayes-optimal error for all \(,=(1)\).
* On the technical level, our result is enabled by combining results from the analysis of single-layer neural networks (Barber et al., 2019) and extensive-rank matrix denoising (Maillard et al., 2022). The derived formula involves the asymptotics of the Harish-Chandra-Itzykson-Zuber integral of random matrix theory (Harish-Chandra, 1957; Itzykson and Zuber, 1980). Our approach is notably inspired by recent results on the ellipsoid fitting problem (Maillard and Kunisky, 2024; Maillard and Bandeira, 2023). These tools are of independent interest to the machine learning community, and we anticipate they will have other applications in the theory of learning.
* We empirically compare the Bayes-optimal performance to the one obtained by gradient descent. In the noiseless case we observe a rather unusual and surprising scenario, as randomly-initialized gradient descent seems to be sampling the space of interpolants, and leads to twice the Bayes-optimal error. When averaged over initialization the gradient descent reaches an error that is very close to the Bayes-optimal. The rigorous establishment of these properties of gradient descent is left open.

Our experiments are reproducible, and accessible freely in a public repository (Maillard et al., 2024).

**Further related works -** The problem studied in this work is known as _phase retrieval_ in the case of a single hidden unit (\(m=1\)). Many works considered this problem in the high-dimensional limit \(d\), in the regime of \(n=O(d d)\) samples; see e.g. Candes et al. (2013); Chen et al. (2019); Demanet and Hand (2014). A subsequent line of work established that the problem can be solved with only \(O(d)\) samples (Candes and Li, 2014; Chen and Candes, 2015; Cai et al., 2022).

Eventually, for Gaussian i.i.d. input data and i.i.d. teacher weights, the optimal sample complexity for learning phase retrieval in the high-dimensional limit has been established down to the constant in \(=n/d\). Authors of Mondelli and Montanari (2019) derived the weak recovery threshold for the noiseless case to be \(_{}=1/2\) for phase retrieval, and optimal spectral methods were shown to match this threshold in Luo et al. (2019); Maillard et al. (2022). The information-theoretically optimal accuracy and the one achieved by an approximate message passing algorithm were then derived in Barbier et al. (2019) for a general i.i.d. prior for the teacher weights. In the absence of noise, these results imply sample complexities \(_{}=1\) and \(_{} 1.13\) needed to achieve perfect learning for a Gaussian prior. Authors of Song et al. (2021) proposed a non-robust polynomial algorithm capable of solving noiseless phase retrieval for \(_{}\). Algorithms based on gradient descent were argued not to achieve the optimal sample complexity in Sarao Mannelli et al. (2020); Mignacco et al. (2021). Maillard et al. (2020) derived the MMSE for more general input data distributions, including the complex-valued case. Phase retrieval with generative priors was studied in Hand et al. (2018); Aubin et al. (2020). We refer to a recent review (Dong et al., 2023) for an overview of the relations between these theoretical studies and practical applications of phase retrieval in imaging.

The case with different numbers of hidden units \(m^{}\) in the teacher and \(m\) in the student model, was also discussed in the literature. For \(m^{}=O_{d}(1)\), the problem is a special case of a multi-index model that has been recently considered, e.g. in Aubin et al. (2019); Bietti et al. (2023); Damian et al. (2022, 2024); Collins-Woodfin et al. (2023); Dandi et al. (2023, 2024). This line of work has not focused on the quadratic activations, as it does not bring particular simplification in this case.

The geometry of loss landscapes of one hidden-layer networks with quadratic activations was studied, and the absence of spurious local minima was established for \(m d\) (when the read-out layer is fixed as in our setting) in Du and Lee (2018). Similar results were established in Soltanolkotabi et al. (2018); Venturi et al. (2019) for a slightly more general setting where the readout layer is learned.

Establishing results about sample complexity required for generalization in cases where \(m\) (or both \(m\) and \(m^{}\)) are \((d)\) is technically challenging, and so far, only a handful of works made progress in that direction. In particular, Gamarnik et al. (2019) considered \(m^{} d\) and \(m d\), and have shown that a sample complexity \(n d(d+1)/2\) is sufficient for perfect recovery of the target function. Sarao Mannelli et al. (2020) considered the overparametrized case with \(m^{}=O_{d}(1)\) and \(m>d\), and showed that gradient descent reaches exact recovery for a sample complexity \(n>d(m^{}+1)-(m^{}+1)m^{}/2\), again considering the high-dimensional limit. Gradient descent of the population risk has been studied for general values of \((m^{},m)\) in Martin et al. (2024), along with a discussion of the role of overparametrization.

Setting

As discussed above, we are studying the Bayes-optimal accuracy in the _teacher-student_ setting. More concretely, we consider a dataset of \(n\) samples \(=\{y_{i},_{i}\}_{i=1}^{n}\) where the input data is normal Gaussian of dimension \(d\): \((_{i})_{i=1}^{n}}{}(0,_{d})\). We then draw i.i.d. \(d\)-dimensional teacher-weight vectors \((_{k}^{*})_{k=1}^{m}}{}(0, _{d})\), and noise \((_{i})_{i=1}^{n}}{}(0, _{m})\). Finally, the output labels \((y_{i})_{i=1}^{n}\) are obtained by a one-hidden layer teacher network with \(m\) hidden units and quadratic activation:

\[y_{i}=f_{^{*}}(_{i})_{k=1}^{m} [}(_{k}^{*})^{}_{i}+ {}z_{i,k}]^{2}.\] (2)

Crucially, we assume we know the form of the (stochastic) target function \(f_{^{*}}()\) (i.e. the value of \(m\), \(\), and the form of eq. (2), including the fact that the activation function is quadratic) but we do not know the realization of neither the teacher weights \(^{*}=(_{1}^{*},,_{m}^{*})\) nor the noise \(_{i}\).

**Universality over the noise and weights distribution -** While we consider Gaussian distributions for the sake of our theoretical analysis, we expect our results to hold under more general i.i.d. models on both the noise and the teacher weights, under mild conditions of existence of moments. This is related to a recent conjecture of Semerjian (2024), see Sections 3 and 4.

**Bayes-optimal test error -** Since we know the law of the dataset \(\), we can study the _Bayes-optimal (BO) estimator_, which minimizes the test error over all possible estimators. To do this, we use Bayes' theorem to obtain the posterior distribution \((|)\) of the weights \(\) given the dataset:

\[(|)=()}P_{ }()(|,\{_{i} \}_{i=1}^{n})\]

where \(P_{}()\) is a prior distribution on the teacher weights \(^{*}\), and the likelihood \((|,)\) can be seen as a probabilistic channel that generates the labels given the input data \((_{i})_{i=1}^{n}\) and the teacher weights \(^{*}\), and \(()\) is a normalization constant. The Bayes-optimal (BO) estimator of the labels for a test sample \(_{}\) not seen in the training set \(\) then involves the average over the posterior distribution as follows (where \(_{}\) denotes the expectation over \(z_{1},,z_{k}\))

\[_{}^{}(_{}) [y_{}|_{}, ]=\,_{}[f_{}(_{})]\,(|)\,\,.\] (3)

We will evaluate the BO estimator in terms of its average generalization error, i.e. the mean squared error (MSE) achieved on a new sample. We define it in the following way:

\[_{d}_{^{*},} _{y_{},_{}}[(y_{ }-_{}^{}(_{} ))^{2}]-(2+)\,.\] (4)

We denote it \(_{d}\), standing for minimum-MSE, as it is the minimum MSE achievable given the setting of the model, and we call \(_{d}_{d}\).

**Conventions for the MMSE -** Notice the peculiar multiplicative factor \((m/2)\) and the additive term \(-(2+)\) in eq. (4). As we detail in Appendix F.1, these factors ensure that \( 1\) for \( 0\) (i.e. in the absence of data), and \( 0\) if the posterior concentrates around the true \(^{*}\) (i.e. if \(_{}^{}()=_{}[f_{ ^{*}}()]\)). Moreover, as we also detail in Appendix F.1, eq. (4) matches the MMSE of a matrix estimation task to which we will reduce the original problem, see Section 3.

As motivated above, our goal is to analyze the MMSE in the high-dimensional limit, with an extensive-width architecture and quadratically many data samples:

\[d,}{{d^{2}}}=(1), }{{d}}=(1),\] (5)

In all that follows, we consider the limit of eq. (5), so that \(n,d,m\) all go to infinity together when we write e.g. \(_{d}\). As we will see, in this limit, the value of the MMSE for a given realization of the randomness concentrates on the averaged value defined in eq. (4).

**Empirical risk minimization estimator -** A more standard way of learning the target function (2) is to minimize the empirical loss \(\) corresponding to a "student" neural network

\[()=_{i=1}^{n}(y_{i}-_{ }(_{i}))^{2},_{ }()_{k=1}^{m}[}(_{k})^{}]^{2}.\] (6)Note that this does not account for the noise, but activations in neural networks are commonly considered deterministic, so we consider this the most natural choice.

Minimization of the loss over the weights \(=(_{k})_{k=1}^{m}\) is commonly done using gradient descent (GD): one initializes the weights as \(^{(0)} P_{}\) and then updates them to minimize the empirical loss, for an appropriately choice of learning rate, until convergence. Denoting the weights at convergence as \(}(^{(0)},)\) the estimator for test labels reads \(_{^{(0)},}^{}(_{}) _{}(^{(0)},)}( _{})\). As we will see, it will be interesting to consider also an estimator \(^{}\) obtained by averaging the GD estimator on the labels over the initializations \(^{(0)}\) of the weights.

## 3 Main results

**Notations -** We use \(()(1/d)[]\) for the normalized trace. We denote \((d)\) the distribution of symmetric matrices \(^{d d}\) such that \(_{ij}}{}(0,(1+_ {ij})/d)\), for \(i j\). For \(m= d\) with \(>0\), we denote \(_{m,d}\) the Wishart distribution, and \(_{,}\) the Marchenko-Pastur distribution with ratio \(\). More details on classical definitions and notational conventions are given in Appendix A.

We start by stating the main result of our analysis, applied to the problem of eq.2.

**Result 1**.: _The MMSE of eq.4 is given in the high-dimensional limit of eq.5 by:_

\[=}-}{2},\] (7)

_where \( 2(2+)/\), and where \(\) is a solution of the following equation:_

\[(1-2)+}{2}=}{3 }_{1/}(y)^{3}y.\] (8)

_Here, \(_{t}_{,}_{,}\) (for \(t 0\)) is the free convolution of the Marchenko-Pastur law and a scaled semicircular density, see Appendix A for its precise definition._

Eq.8 can be efficiently solved using a numerical scheme, which is detailed in Appendix H.1. We present the results in Fig.1. In what follows, we detail our approach towards deriving Result1, which is a consequence of our main theoretical result stated in Claim2.

**Reduction to a matrix estimation problem -** We first notice that by expanding the square in eq.2, we can effectively reduce our learning task to an estimation problem in terms of \(^{}(1/m)_{k=1}^{m}_{k}^{}( _{k}^{})^{}\). We give an analytical argument backing this observation in Appendix F.5.

Figure 1: Left: The asymptotic MMSE of eq.7 for the noiseless \((=0)\) case, as a function of the sample complexity \(\), for various width ratios \(\). Right: Phase diagram representing the MMSE, brighter color indicates a higher value. The red curve is the perfect recovery transition line \(_{}\), see eq.1, and its origin is discussed in Section5.

Its conclusion is that, at leading order, the distribution of \(y=f_{^{}}()\) can be reduced to the following form, with \((y-1-)\):

\[=[^{}]+},\] (9)

with \((0,1)\), \( 2(2+)/\), and where we defined \((^{}-_{d})/\).

**Generalization error and MMSE on S -** This equivalent problem gives us a way to interpret the convention we chose for eq. (4). Indeed, if we denote \(}^{}=[|},]\) the Bayes-optimal estimator related to the problem of eq. (9), then \(=[(^{}-} ^{})^{2}]\), as proven in Lemma F.1.

**The limit of the MMSE -** We now describe the general form of estimation problems covered by our theoretical analysis, which encompasses the one described in eq. (9) (and thus the original eq. (2)). The goal is to recover the symmetric matrix \(^{}^{d d}\), which was generated from the Wishart distribution \(_{m,d}\), from observations \((y_{i})_{i=1}^{n}\), generated as

\[y_{i} P_{}(|[,^{ }]),\] (10)

with \(_{i}(_{i}_{i}^{}-_{ d})/\). The "channel" \(P_{}\) accounts for possible non-linearities and noise, encompassing the case of additive Gaussian noise in eq. (9). We define the partition function as:

\[(\{y_{i},_{i}\}_{i=1}^{n})_{_{m,d}}_{i=1}^{n}P_{}(y_{i}|[_{i}]).\] (11)

Notice that the averaged logarithm of \(\) is (up to an additive constant) equal to the _mutual information_ between the observations and the hidden variables: \(I(^{};\{y_{i}\}|\{_{i}\})=+n  P_{}(y_{1}|[_{1}^{ }])\). This links \(\) to the optimal estimation of \(\), an important idea behind our study. We are now ready to state our main theoretical result. It gives a sharp characterization of the Bayes-optimal error in any estimation problem of the type of eq. (10). By the reduction described above, it can be directly applied to the original model of eq. (2), and will imply Result 1.

**Claim 2**.: _Assume that \(m= d\) with \(>0\), and \(n= d^{2}\) with \(>0\). Let \(Q_{0} 1+^{-1}\). Then:_

* _The limit of the averaged log-partition function (sometimes called the free entropy) is given by_ \[_{d}}_{\{y_{i},_{i}\}} =_{q[1,Q_{0}]}[I(q)+_{ }y\,J_{q}(y,) J_{q}(y,)],\] (12) _where_ \[I(q)&_{ 0}[-q)}{ 4}-(_{1/})-- ],\\ J_{q}(y,)&z}{-q)}}\{ -)^{2}}{4(Q_{0}-q)}\}\,P_{}(y|z).\] (13) _Here,_ \(()_{X,Y}|X-Y|\)_, and, for_ \(t 0\)_,_ \(_{}_{,}_{, }}\) _is the free convolution of the Marchenko-Pastur distribution and a (scaled) semicircle law, see Appendix_ A _for its definition._
* _For any_ \(>0\)_, except possibly in a countable set, the supremum in eq. (_12_) is reached in a unique_ \(q^{}[1,Q_{0}]\)_. Moreover, the asymptotic minimum mean-squared error on the estimation of_ \(^{}\)_, achieved by the Bayes-optimal estimator_ \(}^{}[|\{y_{i},_{i}\}]\)_, is equal to_ \(Q_{0}-q^{}\)_:_ \[_{d}[(^{}-}^{ })^{2}]=Q_{0}-q^{}.\] (14) _It is related to the_ \(\) _of eq. (_4_) by_ \(=(Q_{0}-q^{})\)_._

Specifying Claim 2 to the problem of eq. (9), we derive (details are given in Appendix F.7) Result 1, more precisely eqs. (7) and (8).

**Polynomial-time optimal estimation with the GAMP-RIE algorithm -** In Appendix B, we motivate the definition of an algorithm (that we call GAMP-RIE ) to solve the problem of eq. (10). We further argue (based on a combination of theoretical results and numerical observations) that this algorithm is able to reach, in all regions of parameters we investigated, the optimal error described by Claim 2.

**The condition \(q 1\) -** Notice that \(q^{}=_{d}[(^{}}^{})]\) according to Claim 2. As the MMSE decreases with \(\), it is clear that \(q^{} q^{}(=0)\). When \(=0\), we have \(}^{}=[^{}]=_{d}\), and thus \(q^{}(=0)=1\). We check in Appendix F.8 that the value \(q^{}(=0)=1\) is recovered by eq. (12).

Derivation of the main results

We derive our main result (Claim 2) in two ways. First, we show how one can show Claim 2 using the _replica method_, a heuristic but exact method (hence the word "claim") which originated in statistical physics (Mezard et al., 1987), and has been used extensively in theoretical physics, as well as in a growing body of work in high-dimensional statistics, theoretical computer science, and theoretical machine learning (Mezard and Montanari, 2009; Zdeborova and Krzakala, 2016; Gabrie, 2020; Charbonneau et al., 2023). The derivation, that has an interest on its own, is performed in detail in Appendix D and leverages recent progress on the problems of ellipsoid fitting (Maillard and Kunisky, 2024; Maillard and Bandeira, 2023) and extensive-rank matrix denoising (Maillard et al., 2022; Pourkamali et al., 2024; Semerjian, 2024).

Despite the replica method being conjectured to yield exact results in a large class of high-dimensional models, a rigorous treatment of it remains elusive. It is important, we feel, to present as well a more mathematically sound derivation of our claims, and we thus give an alternative derivation of the Claim 2 using probabilistic techniques amenable to rigorous treatment. In what follows, we present a three-step sketch of a mathematical proof of Claim 2 that combines recent progress performed on the study of a problem known as the ellipsoid fitting conjecture (Maillard and Kunisky, 2024; Maillard and Bandeira, 2023) with the analysis of the fundamental limits of so-called generalized linear models (Barbier et al., 2019), as well as matrix denoising problems (Bun et al., 2016; Maillard et al., 2022; Pourkamali et al., 2024; Semerjian, 2024). While a complete mathematical treatment requires more work, we detail the main challenges arising in each of these steps, outlining a fully rigorous establishment of Claim 2.

We denote the _free entropy_\(_{d}(1/d^{2})(\{y_{i},_{i}\})\), cf. eq. (11). We detail three precise results (two conjectures and a theorem), motivated by recent mathematical works, whose combination would rigorously establish the results of Claim 2. Recall that we consider the high-dimensional limit of eq. (5).

Step 1: Universality with a "Gaussian equivalent" problem -The first step of our approach is inspired by recent literature on the _ellipsoid fitting problem_(Maillard and Kunisky, 2024; Maillard and Bandeira, 2023). It amounts to notice that, if \(_{i}(_{i}_{i}^{T}-_{d})/ \), by the central limit theorem, for any symmetric matrix \(\), \([_{i}]\) is (under mild boundedness conditions on the spectrum of \(\)) approximately distributed as \((0,2[^{2}])\) as \(d\). A large body of recent literature has established that the free entropy is universal for all data distributions sharing the same asymptotic distribution of their "one-dimensional projections", see e.g. Hu and Lu (2022); Montanari and Saeed (2022); Dandi et al. (2024); Maillard and Bandeira (2023). This motivates the conjecture that the free entropy should remain identical (to leading order) if one replaces the matrices \(_{i}\) with \(_{i}(d)\).

**Conjecture 4.1** (Universality).: _We define_

\[_{d}^{(G)}}_{(\{y_{i}^{},_{ i}\})}_{_{m,d}}_{i=1}^{n}P_{}(y_{i}^{}|[_{i}]),\] (15)

_where \(y_{i}^{} P_{}(|[_{i}^{ }])\), with \(^{}_{m,d}\) and \(_{i}}{}(d)\). Then_

\[_{d}|_{d}-_{d}^{(G)}|=0.\]

Conjecture 4.1 can be seen as an extension of Corollary 4.10 of Maillard and Bandeira (2023), in the context of a teacher-student model. In particular, we expect it to hold under mild regularity conditions on the channel density \(P_{}\) (which are satisfied by the Gaussian additive noise we consider).

Step 2: A matrix generalized linear model with a Wishart prior -By the first step above, we can focus on \(_{d}^{(G)}\), and the corresponding estimation problem. A key observation is that one can view this problem as an instance of a _generalized linear model_ on \(^{}\), with a Gaussian data matrix whose \(i\)-th row is the flattening of the matrix \(_{i}\). The limiting free entropy of such models has been worked out in Barbier et al. (2019), when the "ground-truth vector" (here \(^{}\)) has i.i.d. elements. However, here the prior is far from being i.i.d. since \(^{}_{m,d}\). The results of Barbier et al. (2019) generalize naturally to other priors, but such extensions have only been rigorously analyzed in specific settings, e.g. for generative priors rather than i.i.d. (Aubin et al., 2019, 2020). In our setting, the structure of the Wishart prior raises several technical difficulties preventing to directly transpose the proof approaches of Barbier et al. (2019), so we state the following result as a conjecture.

**Conjecture 4.2** (The free entropy of a matrix generalized linear model).: _We have_

\[_{d}_{d}^{(G)}=_{q[1,Q_{0}]}_{ 0}[ -q)}{4}+()+_{}yD J_{q}(y,) J_{q}(y,)],\]

_where_

\[()+_{d}}_ {}_{_{m,d}}\,(- [(-})^{2}])\] (16)

_is the asymptotic free entropy of the matrix denoising problem \(=}^{}+\), with \((d)\), and \(^{}_{m,d}\), and we assume that the \(d\) limit in eq.16 is well-defined._

Step 3: Extensive-rank matrix denoising -As a last step, we study the function \(()\) defined in eq.16. The optimal estimators and limiting free entropy in matrix denoising have been worked out in Bun et al. (2016); Maillard et al. (2022), and formally proven (under some assumptions) in Pourkamali et al. (2024); Semerjian (2024).

**Theorem 4.1** (Free entropy of matrix denoising).: _For any \( 0\), the limit in eq.16 is well-defined, and moreover (recall the definition of \(()\) and \(_{t}\) in Claim2)_

\[()=-(_{1/})-- {1}{8}.\] (17)

We provide a very short and assumption-free proof of Theorem4.1 in AppendixF.2, which combines a relation between \(()\) and HCIZ integrals of random matrix theory, proven in Pourkamali et al. (2024) (without any assumptions), and fundamental results on the large deviations of the Dyson Brownian motion (Guionnet and Zeitouni, 2002). As a final remark, we notice that a recent conjecture2 of Semerjian (2024) states that the free entropy of matrix denoising of \(^{}=(1/m)_{k=1}^{m}_{}^{}(_{k}^{})^{}\) remains the same if one considers any i.i.d. prior for \(_{k}^{}\), under the matching of its first two moments with the Gaussian and the existence of all other moments. While the validity of this conjecture is subject to debate (see SectionVII of Semerjian (2024), and the findings of Camilli and Mezard (2023, 2024)), in the present model it would imply universality of the generalization error given by Claim2 for any such teacher weight distribution.

**The second part of Claim 2 -** We briefly discuss the second part of Claim 2, concerning the large \(d\) limit of \(\,[(^{}-}^{})^{ 2}]\). The fact that the maximizer of eq.12 is unique for almost all values of \(\) can be seen by simple convexity arguments, see AppendixF.6. The relationship of \(q^{}\) with the asymptotic MMSE on the estimation of \(^{}\) is a classical consequence of the I-MMSE theorem in generalized linear models of which eq.9 is an instance, see e.g. Barbier et al. (2019) and SectionD.5 of Maillard et al. (2020).

## 5 Discussion of the main results

**Analysis of the Bayes-optimal estimator -** We start by discussing the noiseless case (\(=0\)), which is described by the phase diagram in Fig.1. Since there is no noise in the target function, we expect a sharp transition to zero MMSE at a critical sample complexity \(_{}\). We analytically show in AppendixF.3 from eq.8 that \(_{}\) is given by the expression of eq.1, and discuss how it is related to a naive counting argument of the "degrees of freedom" of the target function. This transition was known for \( 1\) where the problem is convex, where Gamarnik et al. (2019) shows that there is perfect recovery as soon as \(>1/2\). For all values of \(\) we see the MMSE is a smooth curve going continuously from \(1\) at \(=0\) to \(0\) at \(_{}\). We derived the slope of the curve at \(_{}\) to be (see AppendixF.4)

\[}{}_{_{}}= -2-+& 1,\\ -2+& 1.\]It is interesting to observe that the convexity of the curve changes. While we are observing concave dependence on \(\) for small \(\) it becomes convex when \(\) increases and \(\) is close to \(_{}\). We also note that the smooth limit \( 1\) as \( 0\) supports the result of Cui et al. (2023) about a quadratic number of samples being needed to learn better than linear regression.

We also evaluated the MMSE in the presence of noise, where we observed it to decrease smoothly as \(\) increases with no particular phase transition. We show an example of the theoretical prediction for the MMSE in this case in Fig. 2 right. As expected, in the presence of noise, it decreases monotonically and smoothly, and goes to zero as \(\!\!\).

We considered analytically the limits \( 0\) and \(\), i.e. the limits of small and large (but still extensive in \(d\)) hidden layer. The analysis of these limits are detailed in Appendix E.

Further, in Appendix B.3 we compare the asymptotic theoretical result for the Bayes-optimal error with the performance of the GAMP-RIE algorithm on finite-size instances. In all the cases we evaluated, we observed that GAMP-RIE reached the Bayes-optimal error characterized by Claim 2.

Finally, while we assumed in eq. (2) that the second layer weights are fixed and equal to \(1\), in Appendix G we generalize all our main results, theoretical and algorithmic, to learnable second layer weights.

**Comparison to the ERM estimator obtained by gradient descent -** The results discussed so far concern the Bayes-optimal MMSE, which requires evaluating the marginals of the posterior distribution. We now investigate numerically the performance of empirical risk minimization via gradient descent, which is the standard method of machine learning. It would be typical to expect a gradient based approach to be suboptimal, as the problem is non-convex for \(<1\). In Fig. 2, we compare (a) the MSE \([(^{}-}_{})^{2}]\) reached by gradient descent (GD) minimizing the loss (6) from random initialization, (b) the MSE reached by GD averaged over initializations, and (c) the MMSE derived from the theory.

In the noiseless case, \(=0\), we very remarkably observe that the MSE reached by gradient descent is very close to exactly twice larger than the asymptotic MMSE. Such a relation is known in high-dimensional generalized linear regression to hold between the Gibbs estimator, where test error is evaluated for weights that are sampled uniformly from the posterior, and the Bayes-optimal estimator that averages over the weights sampled from the posterior (Engel, 2001; Barbier et al., 2019). In

Figure 2: Mean squared error (MSE) as a function of the sample complexity \(\) for \(\!=\!1/2\). Dots are simulations using GD with a single initialization averaged over \(32\) realizations of the dataset, crosses are averages over \(64\) initializations with \(2\) realizations of the dataset. The continuous lines are the asymptotic MMSE given by (7). Left: noiseless \(=0\) case. The colors indicate the size \(d\). We can see how AGD appears to be well described by the theoretical MMSE. We used the learning rates \(0.2\) for \(d\!=\!200\) and \(0.07\) for \(d\!=\!100\). Right: Comparison of GD between the noisy \(\!=\!0.25\) case (red) and noiseless \(\!=\!0\) case (blue). Adding noise makes AGD worse than the MMSE, and for sample complexity \(\!\!0.3\), all the initializations of GD converge to the same point, making the GD and AGD curves collapse.

general, there is no reason why the randomly initialized gradient descent should be able to sample the posterior measure. We nevertheless evaluate the average over the initialization of gradient descent and observe that, indeed, the MSE reached this way is consistent with the MMSE. This leads us to conjecture that in the noiseless one-hidden layer neural network with quadratic activation and a target function matching this architecture, randomly-initialized gradient descent samples the posterior despite the problem being non-convex, and hence its average achieves the MMSE.

Let us offer a heuristic argument for this perhaps intriguing phenomenon. It starts with the equivalent of the representer theorem: one can write \(\) in the span of \(\{_{i}_{i}^{T}\}_{i=1}^{n}\), plus a matrix in the orthogonal space, that is \(=_{i=1}^{n}_{i}_{i}_{i}^{T}+\,.\) This means that gradient descent reaches one solution of the minimization with one additional spurious component. The Bayes optimal procedure would be to set this spurious reminder to zero since the data are not informative in this direction. It is reasonable (although non-trivial) to assume that this is what is achieved by averaging over initialization.

When comparing the MMSE to the performance of GD in the noisy setting, we observe a gap between the MMSE and the performance of gradient descent, even averaged over initialization or regularized (as shown in Appendix H.3, Figure 5 left). In particular, for the noisy case, we see that for small sample complexity, the averaged GD is close to matching the MMSE, but as the number of available samples increases, the error of the averaged and non-averaged versions of GD coincide. This is a sign of the trivialization of the landscape, in the sense that GD converges to the same function independently of the initialization: it can be quantified using the variances of the function reached by GD. This is investigated further in Appendix H.3, together with the effect of \(_{2}\) regularization. We can characterize empirically another phase transition: for a sample complexity larger than \(_{T}()\), GD converges to the same function independently of the initialization. In the noiseless \(=0\) case, this is simply the perfect recovery transition, and \(_{}=_{T}(=0)\), while increasing the noise intensity makes the threshold lower until it reaches a plateau, which for \(=0.5\) is at \(_{T}() 0.2\). We display this numerical finding in Figure 5 (right) in Appendix H.3. A tight analytical study of the landscape-trivialization threshold \(_{T}()\) as a function of the noise variance \(\) is left for future work.

## 6 Conclusion and limitations

In this work, we provide an explicit formula for the generalization MMSE when learning a target function in the form of a one-hidden layer neural network with quadratic activation in the limit of large dimensions, extensive width and a quadratic number of samples. The techniques deployed to obtain this result are novel and, we believe, of independent interest. There are many natural extensions of the present works. While we presented, additionally to the replica derivation, a mathematically sound derivation, a fully rigorous treatment, a technical and lengthy task, is left for an extended version of this work. We analyzed the Bayes-optimal MMSE, presented the GAMP-RIE algorithm that is able to reach it in polynomial time, and compared it to the performance of gradient descent numerically. We leave for future work the theoretical analysis of the properties of gradient descent that we discovered numerically. Of particular interest is the role played by the implicit nuclear norm regularization when starting from small initialization, as discussed for the matrix sensing problem e.g. in Gunasekar et al. (2017); Li et al. (2020); Stoger and Soltanolkotabi (2021). Finally, we also presented the natural extension of our results and techniques to the case of a learnable second layer.

The main limitations of our setting are its restriction to Gaussian input data, random i.i.d. weights of the target/teacher neural network, quadratic activation, and a single hidden layer. Going beyond any of these limitations would be a compelling direction of research, in particular for more generic activation such as the ReLU or sigmoid function (we sketch this extension in Appendix C) and multiple layers, and we hope our work will spark interest in these directions.