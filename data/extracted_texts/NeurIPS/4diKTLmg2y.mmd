# RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content

Joao Monteiro\({}^{,3}\), Pierre-Andre Noel\({}^{,1}\), Etienne Marcotte\({}^{,1}\), Sai Rajeswar\({}^{,1}\), Valentina Zantedeschi\({}^{,1,1}\), David Vazquez\({}^{1}\), Nicolas Chapados\({}^{1,2}\), Christopher Pal\({}^{1,2}\), Perouz Taslakian\({}^{,}\)

\({}^{1}\)ServiceNow Research

\({}^{2}\)Mila - Quebec Artificial Intelligence Institute

\({}^{3}\)Autodesk - Work done while at ServiceNow

\({}^{}\)Core contributors

###### Abstract

Large Language Models (LLMs) are trained on vast amounts of data, most of which is automatically scraped from the internet. This data includes encyclopedic documents that harbor a vast amount of general knowledge (_e.g._, Wikipedia) but also potentially overlap with benchmark datasets used for evaluating LLMs. Consequently, evaluating models on test splits that might have leaked into the training set is prone to misleading conclusions. To foster sound evaluation of language models, we introduce a new test dataset named RepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a collection of five splits of test sets, four of which have not been released to the internet or exposed to LLM APIs prior to this publication. Each sample in RepLiQA comprises (1) a reference document crafted by a human annotator and depicting an imaginary scenario (_e.g._, a news article) absent from the internet; (2) a question about the document's topic; (3) a ground-truth answer derived directly from the information in the document; and (4) the paragraph extracted from the reference document containing the answer. As such, accurate answers can only be generated if a model can find relevant content within the provided document. We run a large-scale benchmark comprising several state-of-the-art LLMs to uncover differences in performance across models of various types and sizes in a context-conditional language modeling setting. Released splits of RepLiQA can be found here: https://huggingface.co/datasets/ServiceNow/repliqa.

Figure 1: Creating RepLiQA. RepLiQA\({}_{0}\) was exposed to the web in May 2024 through LLM inference. See Table 1 for the release schedule of the remaining splits.

Introduction

The availability of a vast amount of quality data has made recent advances in large language model (LLM) capabilities possible. Models trained on large data stacks, in particular by scraping the internet, have shown their superiority in language generation [Devlin et al., 2018, Brown et al., 2020, Zhang et al., 2022, OpenAI Team, 2023] and many LLM capabilities (such as answering questions, summarizing documents, translating between languages and completing sentences) have reached or even surpassed human performance. At the same time, such a data bonanza has made evaluation, hence measuring progress in the field, ever more complex. Indeed, models are typically tested and compared against publicly-available benchmarks to assess their ability to generalize to novel samples, such as those encountered in production. This evaluation approach relies on the holdout method (see Dwork et al.  for a description) whereby the portion of the data for testing is not used at training time to ensure the validity of the conclusions [Hastie et al., 2009]. Today, the integrity of many test benchmarks hosted on the Web is potentially compromised because we cannot rule out the possibility that models have been trained on them. This uncertainty around data contamination stems either from a lack of transparency in the training processes of many LLMs (see Bommasani et al.  for a report) or general difficulties in membership testing in large data corpora. Recent research Balloccu et al. , Oren et al.  has exposed such contamination problems and proposed solutions for measuring its extent. Nevertheless, testing for contamination by online benchmarks remains a tedious and challenging task with weak guarantees.

To illustrate this problem in more detail, consider TriviaQA [Joshi et al., 2017], a dataset for reading comprehension. It consists of question, answer, and reference document triplets, where reference documents are collected retrospectively from Wikipedia and the Web. Chances are that popular LLMs have been pre-trained or fine-tuned on the widely accessible Wikipedia content and, hence, have been exposed to at least a subset of the TriviaQA content. In such a context, one cannot attribute good performance to acquired reading skills (and not memorization) with certainty. Thus, evaluating a model on TriviaQA is insufficient, and complementary evaluations are needed to assess whether a model's performance would persist on new reference documents.

In this paper, we introduce RepLiQA: **Rep**ository of **L**kely **Q**uestion-**A**nswer data, a novel test benchmark for evaluating language models using samples previously inaccessible on the Web. Specifically, RepLiQA is designed to assess open-domain question answering based on reference documents and document topic retrieval. RepLiQA is composed of a total \(89,770\) question-answer pairs based on \(17,954\) reference documents. To produce it, we mandated a Vendor to hire human content writers to invent reference documents in a range of topics about imaginary scenarios, people, and places. We also mandated them to obtain question-answer pairs for each document from human annotators, with the caveat that the questions would not be answerable without the associated reference document. Figure 2 shows an example from the dataset.

We make efforts to limit the exposure of our documents and annotations to potential scraping, limiting their use in LLM training. However, completely preventing data leakage while ensuring easy access to our dataset poses significant challenges. Therefore, we have opted for merely delaying the risk of leakage by staggered dataset releases: two of the five RepLiQA splits are available at the time of publication, and one split will be released every two months until June 2025. The experiments reported in Sec. 4 and comprehensively presented in Appendix A, showing the importance of evaluating language models on unseen content, were all performed on the zeroth split. As we used external service providers to carry out LLM inference, we consider this split potentially leaked in late May 2024.

Figure 2: A sample from RepLiQA showing the topic, an excerpt from the supporting document, and a question-answer pair.

Our contributions are as follows:

**1.**_Data_: We built RePliQA, a new dataset for testing LLMs on data concerning facts unseen during the training of any existing LLM. RePliQA contains approximately 90,000 question-answer pairs and 18,000 reference documents across 17 categories.

**2.**_Benchmark_: Through broad experimentation covering 18 state-of-the-art widely-used LLMs, we show that models tend to rely more on internal memory acquired during pre-training than on reference documents provided via prompting. We further report on scaling effects and the ability of different LLMs to refuse to answer.

**3.**_Challenges on data preparation_: We analyze the limitations of our datasets and touch upon the challenges of curating NLP datasets through third-party contractors.

## 2 Creating RePliQA's Content and Annotations

RePliQA is a reading comprehension and question-answering dataset consisting of synthetic documents, each approximately 1,000 words in length, and each accompanied by five question-answer pairs such that the answers can be located within the associated document's text (or there is a mention that the question cannot be answered from the document).

We now give high-level details about the creation process, illustrated on the left of Figure 1. We contracted a for-profit data annotation company specializing in data curation for AI applications; the rest of this document refers to this company as the "Vendor". The annotation process took place over approximately three months.On two occasions during the first month, we reviewed a small subset of documents with their associated questions and answers, providing comprehensive feedback. We describe the creation process as agreed upon and reported to us by the Vendor. Additional details are provided in Appendix D.

### Content Creators and Annotators

The Vendor assigned between 80-90 _Content Creators_ to generate the documents, and 40-50 _Annotators_ to create questions and answers, as well as performing quality control. All workers were based in India, in the 20-40 year-old age group, and either enrolled in or holding a Bachelor's degree. They were paid per document, and it took them between 1 and 1.5 hours to create and annotate each document (excluding research time). No potential participant risks were identified, and no Institutional Review Board (IRB) was involved. Table 2 provides additional information for the four work categories: their definition, demographics, and compensations.

### Dataset Creation by the Vendor

We now summarize the Vendor's creation process corresponding to the first three boxes of Figure 1, on the left of the dashed separator line. See Appendix D.1 for the full text of instructions handed to the Content Creators and Annotators.

  
**Dataset** & **\# Documents** & **\# Questions** & **\# Words** & **\% Unanswerable** & **Release Date** \\  RePliQA\({}^{*}\) & 3,591 & 17,955 & 970 & 21.04\% & June 12th, 2024 \\ RePliQA\({}_{1}\) & 3,591 & 17,955 & 972 & 20.97\% & December 9th, 2024 \\ RePliQA\({}_{2}\) & 3,591 & 17,955 & 969 & 20.59\% & February 10th, 2025 \\ RePliQA\({}_{3}\) & 3,591 & 17,955 & 972 & 20.59\% & April 14th, 2025 \\ RePliQA\({}_{4}\) & 3,590 & 17,950 & 969 & 20.57\% & June 9th, 2025 \\ 
**Totals** & 17,954 & 89,770 & & & & \\   

Table 1: RePliQA statistics (number of documents, number of questions, average number of words per document, percentage of questions marked as unanswerable) and release date by test split. Split RePliQA\({}_{0}\) was released in June 2024, RePliQA\({}_{1}\) is released together with this paper, and the rest will be released over the next six months. The \(*\) indicates that RePliQA\({}_{0}\) has been exposed to LLM providers as of May 2024, as we used it to evaluate state-of-the-art LLMs through their API. Released splits of RePliQA can be found and downloaded at https://huggingface.co/datasets/ServiceHow/repliqa.

**Reference Document** Given one of the 17 topics listed in Figure 3, Content Creators were tasked to produce reference documents of approximately 1000 words. These documents are _synthetic_ in the sense that they are the output of creative writing and thus should not relate to real-world events. Content Creators researched these topics and used different tools (such as random name generators and anonymization techniques to create fictitious entities and avoid unintentional references to real ones) to help them in their work. Vendor Quotes 6-10 in Appendix D.2 provides additional information. A topic ambiguity analysis is reported in Appendix G showing little topic overlap within RePlQA's documents.

**Question** Reference documents were then automatically summarized, and this summary was provided to Question Annotators, who were tasked to write \(5\) specific and direct questions related to the document's content. To prevent ambiguities, particularly in view of using the dataset in a retrieval context, they were asked to include sufficient contextual information in the question (_e.g._, instead of "Where was he born?", ask "Where was John Smith born?"). The rationale for providing summaries instead of the original reference documents was to also produce unanswerable questions, _i.e._, questions whose answers are not contained in the reference document (these constitute \( 20\%\) of the final dataset). The Vendor reports that the summaries were not saved and cannot be exactly re-generated hence they are not part of RePlQA. See Vendor Quotes 12-13 for details.

**Answer** Questions and associated reference documents were then provided to Answer Annotators, who were instructed to give straight answers solely based on the reference document, hence not relying on external knowledge. Answer Annotators were also instructed to start the answer with the most direct piece of information (_e.g._, "yes" or "no") and, if necessary, complete it with details or clarification. If the answer to the question was not found in the document, they were tasked to tag it

  
**Worker Category** & **Role Description** & **Rate \& Gender** \\  Content Creators & Researching and writing the documents. They focused solely on creating high-quality, diverse content that met the project guidelines. & INR 400 per document 74\% Female 26\% Male \\  Question Annotators & Devising insightful and relevant questions based on a document’s summary. & INR 75 per document 45\% Female 55\% Male \\  Answer Annotators & Providing accurate answers to the questions, ensuring the text in the documents directly supported the responses. & INR 75 per document 45\% Female 55\% Male \\  Quality Control & Oversaw all annotation stages to ensure the content met the high standards required. This included checking the accuracy of information, relevance, quality of questions and answers, and overall coherence of the documents. & INR 70 per document 32\% Female 68\% Male \\    
   Company Policies (688) \\  Cybersecurity News (960) \\  Local Technology and Innovation (1008) \\  Local Environmental Issues (1017) \\  Regional Folkore and Myths (1022) \\  Local Politics and Governance (1028) \\  News Stories (1036) \\  Local Economy and Market (1067) \\  Local Education Systems (1070) \\  Local Arts and Culture (1073) \\  Local News (106) \\  Incident Report (1141) \\  Small and Medium Enterprises (1141) \\  Regional Cuisine and Recipes (1142) \\  Neighborhood Stories (1143) \\  Local Sports and Activities (1145) \\  Local Health and Wellness (1167) \\   

Table 2: Four worker categories, a brief description of their roles, their Indian Rupee rates, and their gender distribution.

Figure 3: RePlQA\({}_{0}\) reference documents topics, with their occurrence counts within parentheses.

as unanswerable.1 For each answerable question, the Answer Annotators were instructed to provide the document's paragraph from which the answer is derived, hereafter called "long answer".

**Quality Control** Thus, a fully annotated reference document has a topic and \(5\) question, answer, and long answer triplets. All samples were then vetted, with a reported initial rejection rate of about 5-10%, which decreased as the work progressed. Common reasons for rejection include lack of depth in the content, inaccuracies in the information provided, failure to meet the formatting guidelines, and issues with the relevance or clarity of the questions and answers. More details about quality control are provided in Appendix D.2 under Vendor Quote 11. Ultimately, the Vendor delivered reference documents in PDF format and annotations in JSON format. Although reference documents were originally created using Microsoft Word, these files are no longer available.

## 3 Dataset Finalization by the Authors

Post-processing and AssemblageUsing the PDFs and JSONs delivered to us by the Vendor, we performed additional sanity checks and fixed what we deem to be obvious mistakes: see Appendix B for an extensive list of such edits. We are very conservative in these edits, preferring to leave the data as-is if there is no single clear way to correct it. Section C documents the irregularities we have identified but left unaltered in RepLiQA.

We release the reference documents in their original format (PDFs) as part of RepLiQA, but also their corresponding raw text that we have extracted to perform the experiments of Section 4.1. The raw text is obtained using pdfminer.six,2 the output of which we apply minor cleanup to (_i.e._, removing page breaks, end-of-line hyphenation, and line breaks unless there are two of them in a row). Finally, we create five splits from this data, dubbed RepLiQA\({}_{i}\) for \(i\{0,1,2,3,4\}\). Each document is assigned to a split by stratified random sampling based on the document topic attribute.3

Release Schedule and Potential LeaksAll five splits of the RepLiQA test set will be released under the CC BY 4.0 license. Two splits are available at the time of publication, and one split will be released every two months until June 2025; Table 1 shows the exact release dates. The rationale behind this gradual release is to delay the risk that our test samples are used to train LLMs as much as possible. We do not impose additional legal restrictions to using RepLiQA for training language models beyond those implied by the CC BY 4.0 license. However, doing so goes against the purpose of RepLiQA, so we kindly ask users to refrain from training on RepLiQA.

Notice that in our first public release, we make RepLiQA\({}_{0}\) and RepLiQA\({}_{1}\) available. We release more than one split because RepLiQA\({}_{0}\) should be considered as potentially leaked. Indeed, the experiments reported in Section 4.1 required exposing RepLiQA\({}_{0}\) to online third-party APIs in late May and early June 2024.4 Moreover, during the review process of the present work (starting June 2024), RepLiQA\({}_{0}\) was made available to the anonymous referees through a not-advertised-elsewhere hyperlink. This is a precautionary measure as, to the best of our understanding, we did not license anyone to use any RepLiQA split prior to their official release date.

MaintenanceUsers will be able to raise issues with the dataset starting with the first release. This can be done in the official data repo at: https://huggingface.co/datasets/ServiceNow/repliqa. We will consider all the incoming recommendations and update the dataset accordingly. Otherwise, we will actively maintain the dataset until at least the end of 2025.

## 4 Benchmarking LLMs with RepLiQA on Reading Comprehension

Enabled by RepLiQA, we test to what extent popular state-of-the-art LLMs can _read_ provided contexts and find useful information to correctly answer user queries (_question answering_) and identify the topic of the content (_topic retrieval_). Note that reading comprehension is key to many practical use cases of LLMs, such as in the context of retrieval-augmented generation where proprietary non-public context is provided to a model to respond to user queries.

We select eighteen widely-used LLMs: GPT-3.5 and GPT-4o by OpenAI , Llama 3 by Meta  with both 8 and 70 billion parameters, Gemini 1.0 and 1.5 by Google , two variants of WizardLM by Microsoft , Mistral and Mistral variants by MistralAI , Command R and Command R+ by Cohere , Arctic by Snowflake , and Anthropic 's Claude Haiku and Sonnet. Inference is carried out using OpenRouter,5 which serves as a unified framework enabling access to multiple LLM providers through a single API. Our entire benchmarking evaluation has cost approximately USD 5k.

For _question answering_, we follow the evaluation protocol by Adlakha et al.  and measure performance metrics such as the F1 score and recall. Around 20% of questions in RepLiQA are not answerable from provided documents, in which situation we expect (and prompt) models to reply with unanswerable. We further evaluate models in terms of their ability to detect such situations and refuse to reply. For _topic retrieval_, models were prompted to determine the topic or document category out of the occurrences in the dataset (cf. Figure 3), with the set of candidates passed through the prompt. Further evaluation details, such as the prompts we used, can be found in Appendix F.

As point of reference, we additionally report _question answering_ results on TriviaQA 6, a well-known dataset whose context documents are factual and contain largely documented information. We make use of its subset derived from Wikipedia and expect models to perform well on it even when no context information is provided, with models relying purely on memory.

Figure 4: (top) Recall of various models on question answering for RepLiQA\({}_{0}\) and TriviaQA. (bottom) Difference in recall on question answering between RepLiQA\({}_{0}\) and TriviaQA.

### Question-Answering

In Figure 4 we report the question answering results on RepliQA\({}_{0}\) and investigate how they differ from those on TriviaQA. We observe that all models perform significantly better on TriviaQA than on RepliQA, hinting to the fact that models might rely on memory and not acquired reading skills to solve these tasks. Note that standard errors for recall are too small to be readable in the figures (0.25% to 0.28% for RepliQA and 0.29% to 0.50% for TriviaQA). Recall gaps, shown in the bottom-left plot of Figure 4, underline the extent to which model performance drops in the situation where knowledge obtained during training is not useful (_e.g._, Command R+'s recall is halved). For some models the gap is not as dramatic: Claude Sonnet, Llama 3 8B and Arctic behave similarly across the two datasets, although their performance is suboptimal. The best-performing model on RepliQA is Mistral Large whose recall gap is also amongst the smallest. We show further evidence of performance differences across the two datasets in Figure 5. For all models we evaluated across the situations where reference documents are based on information available during training or not. Moreover, in Figure 6, we show the performance distributions across the two datasets. We observe a pronounced skewness towards high recall on TriviaQA. Extra results with a models specialized on Question-Answering are reported in Appendix H.

For a subset of the evaluated models, we conducted further testing to assess the effect of memory obtained during pre-training in question-answering accuracy. In Figure 7, we report recall for inference run without contexts. In other words, we prompted models with just a question and no reference document. Interestingly, on TriviaQA the performance of all considered models does not significantly drop without context, and even slightly improves for GPT likely due to confounding or distracting information in the reference document. For RepliQA\({}_{0}\), evaluating models without providing context documents leads to poor performance as desirable from a dataset testing reading capabilities. This observation confirms our claim that most facts and entities in RepliQA are novel, and were not part of the pre-training data of any of the evaluated models.

Figure 5: Side-by-side performances for each model on RepliQA and TriviaQA.

Figure 6: Violin plots depicting differences in performance distributions across RepliQA and TriviaQA. We observe a pronounced skewness towards high recall on TriviaQA.

### Effect of Scaling Model Size

We further study the effect of scale on performance across groups of models, expecting larger models to have better reading skills, but at the same time also be more affected by memorization. We note that most of the models we evaluated are closed-source and their providers did not disclose their precise parameter counts. In some cases, we trust providers and sort by model naming (_e.g._, we assume Mistral Large is larger than Mistral small). In some other cases, plots are ordered by our assumption of model sizes based on how the models are qualified and priced by their providers (_e.g._, we assume that GPT-4o is larger than GPT-3.5 since the former is more costly than the latter).

From Figure 7 we remark that on TriviaQA increasing model size indeed leads to increased performance. However, this improvement is partly due to memorization, as larger models are consistently better than their smaller counterparts when tested on TriviaQA without context, while results are mixed when tested on RePliQA\({}_{0}\) with context. We observe a similar pattern in Figure 8, where, for TriviaQA, models generally improve with size, while results are not as consistent for RePliQA\({}_{0}\). One exception Claude 3, whose performance surprisingly decreases on both datasets.

Overall, these results suggest that scale improves the ability of a model to retrieve useful information from its internal memory, obtained during pre-training. For reading ability, however, model scale has a mixed effect and does not necessarily translate into better performance. These results highlight the importance of datasets such as RePliQA, and lead to non-obvious insights. For instance, in a retrieval-augmented generation setting with non publicly available context data, GPT-3.5 is likely to outperform GPT-4 (as we observe). The same is true for Claude Haiku vs. Claude Sonnet and Command R vs. Command R+, where a reverse trend is observed and the smaller models outperform bigger ones for RePliQA\({}_{0}\).

### Testing The Ability of Models to Admit Lack of Knowledge

We leverage the fact that some of the questions in RePliQA cannot be answered from the context documents (and are clearly marked as such) to probe models for their ability to admit that an answer

Figure 8: Comparison of recall scores for various models, sorted in ascending size. For RePliQA, scores are not computed for the unanswerable questions.

Figure 7: Impact of the presence or absence of context when answering questions, measured using recall, for various models on both RePliQA and TriviaQA. The results on RePliQA are restricted to the questions whose answers are **not** unanswerable. Note that recall can be non-zero for a model that only answers wrongly if it outputs a few tokens that appear in the ground-truth answers.

is not found. Specifically, we prompt a subset of the LLMs on the unanswerable subset of RepLiQA\({}_{0}\) and ask them to generate unanswerable when they do not know the answer. We run two separate evaluations for each model, with or without distracting contextual information in the prompt. The frequency with which models reply with unanswerable are shown in the bar plots in Figure 9. Note that a perfect model would score 100% in this evaluation.

Interestingly, all models tend to refuse answering unanswerable questions more often when context is provided than when one is not provided, and they tend to come up with an answer, even if it is not possible to do so. We highlight that, as shown in Appendix F, prompts include instructions for models to refuse to answer in situations where documents (or knowledge obtained during pre-training) do not offer useful information.

### Topic Retrieval

We benchmark models on RepLiQA in a text classification setting. Specifically, we prompt models to determine document topics in a zero-shot fashion. Given a context document and a set of topics covering the 17 possibilities presented in Figure 3, we perform inference and generate one of the topics, instructing models to choose the one that best qualifies the document. Performance in terms of F1 score is shown in the bar chart in Figure 10, while the full set of results is presented in Appendix A.

Results do not follow the same trends as in the question-answering evaluation. For instance, Gemini Flash 1.5 was the top performer in this text classification setting, while being at the bottom half of the ranking of models for question-answering on both RepLiQA\({}_{0}\) and TriviaQA. This evaluation further highlights the performance of Mistral Large, which, despite not being the top performer in this task, is typically within the best set of models for the evaluations we considered. These differences in performance can be explained by the fact that different model abilities are required for global understanding of documents versus for finding and making use of specific bits of information within a large document.

## 5 Conclusion

We created and partially released RepLiQA, a new dataset containing (document, question, answer) triplets. Critically, RepLiQA was made to look natural, with documents that read like actual informative articles, all the while covering untrue information. That is, RepLiQA is such that unreal events, places, individuals, or any other kind of non-existing entities are covered in its documents. This approach ensures that LLMs, which may be trained on existing public datasets containing real-world facts, do not have information embedded in their weights that would enable them to answer questions from RepLiQA with no access to a reference document. By doing so, we can evaluate LLMs without the confounding factor of pre-trained knowledge, allowing us to directly assess how well different models can interpret and utilize documents provided by the user. RepLiQA was annotated by humans and covers 17 diverse document topics. To ensure the _unseen-ness_ of RepLiQA over time, we sliced the dataset into five splits and scheduled their releases. By spacing out these releases, we can ensure that available LLMs have not been pre-trained on any new split, allowing the dataset to effectively test information-seeking abilities without interference from model memory

Figure 9: Comparison of selected models on the unanswerable detection task (higher is better) on RepLiQA, with or without context. When including the context, scores are computed only for unanswerable questions. A perfect model would reach 100%.

eased every two months until June 2025.

Enabled by the first released split RePLiQA\({}_{0}\), we performed a large-scale benchmark of 18 popular state-of-the-art LLMs and ranked them in terms of their ability to carry out sparse information seeking tasks, where questions were posed to models that then had to find answers within long documents with distracting content. In addition, we tested these models for other skills such as their ability to say that they don't have the answer when one cannot be obtained from the provided document. We also evaluated LLMs for text classification, in which the models were prompted to determine the document topic out of a set of candidates.

From our evaluation, we observed that no single model consistently ranks first across all settings, with different models excelling in different areas. However, we did identify a Pareto-optimal set, indicating that there exists a subset of models that outperforms others overall. For instance, Mistral Large consistently ranks high in terms of recall. Finally, contrary to what we observed in a more standard question-answering evaluation, we did not notice a clear pattern when it comes to scaling model size for evaluations on RePLiQA\({}_{0}\), and larger models are not necessarily better at finding useful information in provided contexts. Determining what it is that makes different models less dependent on memory and better at parsing user-provided content is the object of future work.

## Limitations

As LLMs improve in quality and become more of a part of our everyday activities, it is likely that annotation tasks will exhibit some level of LLM interference. As such, some of the data we release may have been the results of some level of interaction with generative models. We highlight however that annotators were instructed to not use LLMs while annotating RePLiQA, and we controlled for it ourselves by running the data through detectors as reported in Appendix C.2, and by testing for the ability of models to answer questions without context, with results reported in Figure 7. Despite the quality control measures in place, remaining unresolved irregularities and quality issues are highlighted in Appendix C.1. We also note limitations in reproducibility in our benchmarking due to the reliance on third party LLM providers. Models may change over time beyond our control, and the cost to reproduce our experiments is non-negligible.

We also highlight a possible bias toward Indian English with respect to other variants of English within RePLiQA due to the demographics of annotators. That being said, this kind of bias (if present) would be more important in a dataset meant for training, which RePLiQA is not. RePLiQA is meant for benchmarking a model's ability to answer (or identify as unanswerable) questions based on a never-seen-before context. _A priori_, this capability should be orthogonal to the model's performances in different English dialects. Moreover, our manual inspection of the dataset indicates that a rather "international" English dialect is used.

Figure 10: F1 scores on the topic retrieval task for RePLiQA\({}_{0}\).