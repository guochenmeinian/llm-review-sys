# Calibration by Distribution Matching:

Trainable Kernel Calibration Metrics

Charles Marx

Stanford University

ctmarx@cs.stanford.edu

&Sofian Zalouk1

Stanford University

szalouk@stanford.edu

&Stefano Ermon

Stanford University

ermon@cs.stanford.edu

Equal Contribution.

###### Abstract

Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification tasks, outperforming methods relying solely on post-hoc recalibration.

## 1 Introduction

Probabilistic forecasts are valuable tools for capturing uncertainty about an outcome of interest. In practice, the exact outcome distribution is often impossible to recover , leading to overconfident forecasts . Calibration provides statistical guarantees on the forecasts, ensuring that the predicted probabilities align with the true likelihood of events. For example, consider a weather forecast that assigns an 80% probability of rain tomorrow. A well-calibrated forecast would imply that, on average, it rains on 80% of the days with such predictions. By appropriately quantifying uncertainty, calibration empowers decision-makers to efficiently allocate resources and mitigate risks, even when the outcome distribution cannot be recovered exactly.

Many forms of calibration have been proposed in both classification [8; 21; 35] and regression [42; 23; 53] to ensure that uncertainty estimates match true frequencies. From a methodological perspective, the literature on enforcing calibration is fractured; many algorithms to enforce calibration are specialized for a particular form of calibration [39; 29; 52; 10; 17] or are designed exclusively for post-hoc calibration [27; 23]. Although post-hoc methods are effective at enforcing calibration, they often lead to a degradation in the predictive power, i.e. sharpness, of the forecaster . This motivates the need for versatile metrics that can enforce various forms of calibration during training.

In this work, we introduce a unified framework wherein existing notions of calibration are presented as distribution matching constraints. To enforce distribution matching, we use the Maximum Mean Discrepancy (\(\)), giving us unbiased estimates of miscalibration that are amenable to gradient-based optimization. We frame these metrics as regularizers and optimize them alongside proper scoring rules. This allows us to enforce calibration while preserving sharpness.

Framing calibration as distribution matching  allows us to use the Maximum Mean Discrepancy (\(\))  to give unbiased estimates of miscalibration that are amenable to gradient-based optimization. An advantage of this approach is that we express existing calibration measures by varying the kernel in the \(\) metric. Moreover, we can express new notions of calibration specific to a downstream decision task. This allows us to prioritize errors that impact decision-making . Our contributions can be summarized as follows:

* We propose a framework based on distribution matching to unify many notions of calibration.
* We provide differentiable training objectives to incentivize a wide variety of popular forms of calibration during empirical risk minimization in supervised learning. We frame these metrics as regularizers that can be optimized alongside proper scoring rules. This allows us to enforce calibration without degrading sharpness.
* We show how to define and enforce new calibration metrics tailored to specific downstream decision problems. When these metrics are successfully optimized, decision makers can accurately estimate the decision loss on unlabeled data.
* We perform an empirical study across 10 tabular prediction tasks (5 regression and 5 classification) and find that using our metrics as regularizers improves calibration, sharpness, and decision-making relative to existing recalibration methods.

## 2 Related Work

CalibrationOur work builds on the literature on calibrated forecasting [31; 8; 9; 13]. Recently, many forms of calibration have been proposed in classification [5; 25; 24; 32; 35; 33; 10; 17; 28; 36; 27], regression [23; 42; 53; 39; 51], and beyond [47; 22]. Algorithms to enforce calibration are often designed specifically for one form of calibration. For example, significant attention has been devoted to effectively estimating the Expected Calibration Error (ECE) to calibrate confidence levels in binary classification [19; 44; 4; 15; 35]. Our work aims to supplement this work by providing general calibration metrics that can be used as training objectives. These methods can be used in concert with post-hoc calibration, to encourage calibration during model training and correct any remaining miscalibration after training is complete.

Decision CalibrationAn interesting line of recent work focuses on calibrating predictions for downstream decision problems [39; 52]. Zhao et al.  shows that decision calibration can be achieved when the set of possible actions is finite. We show cases in which this result can be extended to infinite action spaces by leveraging a low-dimensional sufficient statistic of the decision problem. Whereas Zhao et al.  compute calibration by approximating a worst-case decision problem with a linear classifier, we use the kernel mean embedding form of an MMD objective, making it simple to calibrate with respect to a restricted set of loss functions by adjusting the kernel.

Kernel-Based Calibration MetricsMotivated by issues arising from the histogram binning of the Expected Calibration Error, a recent stream of research uses kernels to define smooth calibration metrics. Zhang et al.  and Popordanoska et al.  use kernel density estimates to estimate calibration in classification settings. In contrast, our work applies to regression and takes a distribution matching perspective. Widmann et al.  and Kumar et al.  are the most similar existing works to our own, and also introduce MMD-based calibration metrics. Our work differs from Widmann et al.  due to our focus on trainability and optimization, connections to existing forms of calibration, and applications to decision-making. Kumar et al.  can be seen as a special case of our method applied to top-label calibration in multiclass classification (see Table 1). Our work extends the literature on MMD-based calibration metrics, showing how to express and optimize 11 existing forms of calibration by varying the kernel in the MMD objective (see Tables 1 and 2).

## 3 Calibration is Distribution Matching

We consider the regression setting, where, given a feature vector \(x=^{d}\), we predict a label \(y\). We are given access to \(n\) i.i.d. examples \((x_{1},y_{1}),,(x_{n},y_{n})\) distributed according to some true but unknown cumulative distribution function (cdf) \(P\) over \(\). A lower case \(p\) denotes the probability density function (pdf) or probability mass function (pmf), when it exists. We use subscripts for marginal and conditional distributions, so \(P_{Y|x}()\) is the conditional cdf for \(Y\) given \(X=x\) and \(p_{Y}()\) is the marginal pdf for \(Y\). When the input \(X\) is random, we write \(P_{Y|X}\) as the cdf-valued random variable that takes value \(P_{Y|x}\) upon the event that \(X=x\).

Our goal is to learn a forecaster \(Q\) that, given a feature vector \(x\), forecasts a cdf \(Q_{Y|x}()\) over \(\). A forecaster implicitly defines a joint distribution over \(\) by combining the forecasted conditional pdf with the marginal pdf for the features, i.e. \(q(x,y)=p_{X}(x)q_{Y|x}(y)\). The forecaster's joint cdf \(Q(x,y)\) is defined by integrating the pdf \(q(x,y)\) over the appropriate region of \(\). In the same way as \((X,Y) P\) is a sample from the true distribution, we write a sample from \(Q\) as \((X,) Q\), where \(X P_{X}\) and \(\) is a label drawn from the forecast \((|X=x) Q_{Y|x}\).

An ideal forecaster perfectly recovers the true probabilities so that \(Q=P\). However, in practice we often only observe each feature vector \(x\) once, making perfect forecasts unattainable . Instead, calibration enforces that forecasts are accurate on _average_, leaving us with a tractable task that is strictly weaker than perfect forecasting. Each form of calibration requires the forecasts to respect a particular law of probability. For example, if \(Y\) is binary, distribution calibration states that on examples for which the forecast is \(q_{Y|x}(Y=1)=c\), we should observe that \(Y=1\) with frequency \(c\). In regression, quantile calibration  states that \(y\) should exceed each quantile \(c\) of the forecast with frequency \(c\). Calibration can also be enforced within subgroups of the data (i.e. group calibration ), in smooth neighborhoods of the input space (i.e. local calibration ), and even for a single feature vector (i.e. individual calibration ) by using randomization.

Calibration can be expressed as a conditional distribution matching problem, requiring that certain variables associated with the true distribution match their counterparts from the forecasts.

**Lemma 3.1** (informal).: _Quantile calibration is equivalent to distribution matching between \(Q_{Y|X}(Y)\) and \(P_{Y|X}(Y)\). Distribution calibration is equivalent to distribution matching between \(\) and \(Y\) given \(Q_{Y|X}\). Individual calibration is equivalent to distribution matching between \(\) and \(Y\) given \(X\)._

The choice of random variables that we match in distribution determines the form of calibration recovered. We show how to express many popular forms of calibration in terms of distribution matching in Table 1, and include additional details on these correspondences in Appendix A. In Section 4, we describe how to express and estimate these forms of calibration using kernel-based calibration metrics. In Section 5, we show how to tailor these metrics to downstream decision problems. Finally, in Section 6, we apply these metrics as regularizers and explore the empirical effects on calibration, sharpness, and decision-making.

## 4 Calibration Metrics as Trainable Regularizers

In this section, we introduce a framework to enforce calibration using distribution matching. We quantify calibration using integral probability metrics which admit unbiased estimates from data

   Calibration Objective & Forecast Variable & Target Variable & Conditioning Variable \\  Quantile Calibration  & \(Q_{Y|X}(Y)\) & \(P_{Y|X}(Y)\) & – \\ Threshold Calibration  & \(Q_{Y|X}(Y)\) & \(P_{Y|X}(Y)\) & \(\{Q_{Y|X}(y_{0})\}\) \\ Marginal Calibration  & \(\) & \(Y\) & – \\ Decision Calibration  & \(\) & \(Y\) & \(_{}^{*}(Q_{Y|X})\) \\ Group Calibration  & \(\) & \(Y\) & Group(\(X\)) \\ Distribution Calibration  & \(\) & \(Y\) & \(Q_{Y|X}\) \\ Individual Calibration  & \(\) & \(Y\) & \(X\) \\ Local Calibration  & \(\) & \(Y\) & \((X)\) \\   

Table 1: We express popular forms of calibration in terms of distribution matching, where \(P\) is the true distribution and \(Q\) is the forecast. The forecaster achieves calibration if the forecast variable and target variable are equal in distribution, given the conditioning variable. Here, \(_{}^{*}(Q_{Y|X})\) is a Bayes optimal action under \(Q_{Y|X}\), \(y_{0}\) is a fixed label in \(\), \(\) is a threshold, and \((X)\) is a feature mapping. See Appendix A for additional details.

and are amenable to gradient-based optimization. This allows us to optimize calibration alongside a standard training objective, achieving calibration without degrading sharpness. Jointly optimizing calibration and sharpness has been shown to outperform only enforcing calibration post-hoc .

Integral probability metrics (IPMs) are a flexible set of metrics to quantify the difference between two distributions  in terms of a family of witness functions. Since we need to perform distribution matching _conditionally_, we consider witness functions over both the label \(Y\), as well as a conditioning variable \(Z\) that assumes the same distribution under \(P\) and \(Q\).

\[D(,P,Q)=_{f}|_{P}[f(Y,Z) ]-_{Q}[f(,Z)]|.\] (1)

Different choices for the witness functions \(\) and the conditioning variable \(Z\) measure different notions of calibration. Similar to Widmann et al. , we focus on the case where \(\) is the unit ball of a Reproducing Kernel Hilbert Space (RKHS) \(\), so that the IPM corresponds to the Maximum Mean Discrepancy , denoted \((,P,Q)\). MMD can either be expressed in terms of the canonical feature map \((y,z)\) or the reproducing kernel \(k((y,z),(,z^{}))=(y,z),(,z^{ })_{}\) for \(\):

\[^{2}(,P,Q) =\|_{P}[(Y,Z)]-_{Q}[ (,Z)]\|_{}^{2}\] (2) \[=_{P}[k((Y,Z),(Y^{},Z^{}))]+ _{Q}[k((,Z),(^{},Z^{})) ]-2_{P}_{Q}[k((Y,Z),(^{},Z^{ }))]\]

Given a dataset \(D\) consisting of \(n\) i.i.d. pairs \((x_{i},y_{i}) P\), we can generate forecasted labels \(_{i} Q_{Y|x_{i}}\). The conditioning variables \(z_{i}\) are computed from \((x_{i},y_{i})\) according to the expressions given in Table 1. The plug-in MMD estimator is unbiased and takes the form :

\[}^{2}(,D,Q)=_{i=1}^{n} _{j i}^{n}h_{ij},\] (3)

where \(h_{ij}\) is a one-sample U statistic:

\[h_{ij}:=k((y_{i},z_{i}),(y_{j},z_{j}))+k((_{i},z_{i}),(_{j},z_{j}))-k((y_{i},z_{i}),(_{j},z_{j})-k((y_{j},z_{j}),( {y}_{i},z_{i}))\]

The variance of this estimator can be reduced by marginalizing out the simulated randomness used to sample \(_{i} Q_{Y|x_{i}}\). This can be done either analytically, or empirically by resampling the forecasted labels multiple times per example. We find that empirically marginalizing out the simulated randomness improves training stability in practice. The full training objective combines a proper scoring rule (we use negative log-likelihood) with an MMD estimate to incentivize calibration. To train, we divide the dataset \(D\) into batches \(D_{b}\{1,,n\}\) large enough so that the MMD estimate on a batch has low enough variance to provide useful supervision (we find that \(D_{b}\) of size 64 suffices in practice). Given a family of predictive models \(\{Q_{}:\}\) (e.g., a neural network), we optimize the following training objective on each batch:

\[_{}_{i D_{b}}- q_{Y|x_{i};}(y_{i})+ }^{2}(,D_{b},Q_{})\] (4)

In order to differentiate \(}^{2}(,D_{b},Q_{})\) with respect to \(\), we require that the kernel \(k\) is differentiable and the samples can be expressed as a differentiable function of the model parameters (e.g., using a reparameterization trick). For parameterizations that do not admit exact differentiable sampling, we can apply a differentiable relaxation (e.g., if \(Q_{}\) is a mixture distribution, we can use a Gumbel-Softmax approximation). In principle, it is possible to remove the need for differentiable sampling altogether by optimizing our MMD objective using Monte-Carlo RL techniques (e.g., REINFORCE ). However, this is beyond the scope of our work. Note that regularizing for calibration at training time does not give calibration guarantees, unless one can guarantee the objective will be optimized out-of-sample. In this sense, regularization and post-hoc calibration are complementary: regularization mitigates the trade-off between calibration and sharpness while post-hoc methods can provide finite-sample calibration guarantees (e.g., ). For this reason, we suggest using regularization and post-hoc calibration together (see Table 3).

On the Choice of the KernelWhen the kernel \(k\) is universal, the MMD metric is zero if and only if \(Y\) and \(\) are equal in distribution, given \(Z\). Universality is preserved when we decompose the kernel over \(\) into a pointwise product of two universal kernels, one over \(\) and one over \(\).

**Lemma 4.1**.: _Let \(k\) be the function defined by the pointwise product \(k((y,z),(y^{},z^{}))=k_{y}(y,y^{})k_{z}(z,z^{})\) where \(k_{y}\) and \(k_{z}\) are universal kernels over spaces \(\) and \(\) respectively. Then, under mild conditions, \(k\) is a universal kernel over \(\)._

See [43, Theorem 4] for a proof. In our experiments, we explore using a universal RBF kernel over \(\), since it satisfies the mild conditions needed for Lemma 4.1. To enforce decision calibration, we also consider a kernel \(k_{Y}\) that is intentionally not universal, and instead only measures differences between \(P\) and \(Q\) that are relevant for decision-making (see Section 5). Furthermore, to measure Quantile Calibration and Threshold Calibration, we must match the distributions of the probability integral transforms \(P_{Y|X}(Y)\) and \(Q_{Y|X}(Y)\) instead of \(Y\) and \(\). Thus, we replace \(k_{Y}\) with a universal kernel over \(\), the codomain of the probability integral transform.

### Calibration Metrics for Classification

Thus far, we have focused on estimating calibration metrics in the regression setting. We now describe how our framework applies to classification.

In the classification setting, given a feature vector \(x=^{d}\) we predict a label \(y=\{1,,m\}\). As before, we are given access to \(n\) i.i.d. examples \((x_{1},y_{1}),,(x_{n},y_{n})\) distributed according to some unknown distribution \(P\) over \(\). Our goal is to learn a forecaster \(Q\) that, given a feature vector \(x\), forecasts a probability mass function (pmf) \(q_{Y|x}()\) over labels \(\).

We express popular notions of calibration used in classification (i.e., canonical, top-label, marginal) in terms of distribution matching in Table 2. See Appendix A.2 for additional details on calibration and distribution-matching in classification. We use the same unbiased plug-in estimator for MMD shown in Equation 3. In the classification setting, we can analytically marginalize out the randomness originating from the forecast when computing the MMD estimate. After performing this marginalization, the one-sample U-statistic \(h_{ij}\) is is given by

\[h_{ij}=k((y_{i},z_{i}),(y_{j},z_{j}))+_{y}_{y^{} }q_{i}(y)q_{j}(y^{})k((y,z_{i}),(y^{},z_{j}))-2 _{y}q_{i}(y)k((y,z_{i}),(y_{j},z_{j})),\]

where the conditioning variables \(z_{i}\) are computed from \((x_{i},y_{i})\), as detailed in Table 2. Here, \(q_{i}(y)=q_{Y|x_{i}}(y)\) is the predicted probability of the label \(Y=y\) given features \(x_{i}\). When the forecast and target variables depend on variables other than \(Y\) (e.g., \(Y^{*}\) depends on \(q_{Y|X}\) in top-label calibration), we add those variables as inputs to the kernel. Note that the plug-in MMD estimator with this U-statistic is differentiable in terms of the predicted label probabilities, and that it can be computed in \(O(n^{2}m^{2})\) time.

## 5 Calibration and Decision-Making

Some of the most practically useful results of calibration are the guarantees provided for downstream decision-making . Universal kernels guarantee distribution matching in principle, but in some cases require many samples. However, if we have information about the decision problem in advance, we can design calibration metrics that are only sensitive to errors relevant to decision-making. In this section, we show how to design such metrics and show that they enable us to (1) accurately evaluate the loss associated with downstream actions and (2) choose actions that are in a sense optimal.

   Calibration Objective & Forecast Variable & Target Variable & Conditioning Variable \\  Canonical Calibration [44, Eq 1] & \(\) & \(Y\) & \(q_{Y|X}\) \\ Top-label Calibration [44, Eq 2] & \(\{=Y^{*}\}\) & \(\{Y=Y^{*}\}\) & \(q_{Y|X}(Y^{*})\) \\ Marginal Calibration [44, Eq 3] & \(\{=y\}\) & \(\{Y=y\}\) & \(q_{Y|X}(y), y\) \\   

Table 2: Popular forms of calibration expressed as distribution matching for a classification problem with classes \(=\{1,,m\}\). Recall that \(q_{Y|X}()\) is the forecasted pmf. The forecaster is calibrated if the forecast variable and target variable are equal in distribution, given the conditioning variable. For top-label calibration, we use \(Y^{*}:=_{y}q_{Y|X}(y)\), which is random due to \(X\). The marginal calibration condition expresses \(m\) distribution matching constraints, one for each \(y\).

We begin by introducing a general decision problem. For each example, a decision-maker chooses an action \(a\) and incurs a loss \((a,y)\) that depends on the outcome. Specifically, given information \(z\) (such as the forecast), a decision policy \(:\) assigns an action \((z)\). The set of all such policies is denoted \(()\). The Bayes optimal policy under a label distribution \(P_{Y}\) is denoted \(_{}^{*}(P_{Y}):=_{a}_{P}[(a,Y)]\). Decision calibration  provides two important guarantees:

\[_{Q}[(a,) Z=z] =_{P}[(a,Y) Z=z], a ,z\] \[_{P}[(_{}^{*}(Q_{Y x}),Y) Z =z] _{P}[((z),Y) Z=z], z ,()\]

_Loss estimation_ says that the expected loss of each action is equal under the true distribution and the forecast. This means we can estimate the loss on unlabeled data. _No regret_ says that the Bayes optimal action according to the forecast \(_{}^{*}(Q_{Y x})\) achieves expected loss less than or equal to any decision policy \(()\) that depends only on \(z\). If we condition on the forecast by choosing \(z=Q_{Y x}\), then a decision-maker who only has access to the forecast is incentivized to act as if the forecast is correct. Additionally, we consider cases where the loss function is not known in advance. For example, when a vendor releases a foundation model to serve a large population of users, it is likely that different users have distinct loss functions. Thus, we also show settings in which we can achieve accurate loss estimation and no regret for all loss functions \(\) included in a family of losses \(\).

A General Recipe for Decision CalibrationWe now describe an approach to measure decision calibration using our kernel-based metrics. For each action \(a\) and loss \(\), we want to measure the discrepancy between the expectation of the loss \((a,Y)\) under \(P\) and \(Q\). To achieve this, we define the feature map that computes the losses of all actions \((y)=((a,y))_{a,}\). This assigns to \(P\) and \(Q\) the mean embeddings \(_{P}=_{P}[((a,Y))_{a, }]\) and \(_{Q}=_{Q}[(a,)_{a ,}]\), respectively. Letting \(\) be the unit ball of the Hilbert space associated with this feature map, the mean embedding form of MMD gives us that \((,P,Q)^{2}=\|_{P}-_{Q}\|_{2}^{2}= _{a}_{}(_{P}[ ((a,Y))-_{Q}[(a,)]] )^{2}\). When either \(\) or \(\) is infinite, the corresponding sum is replaced by an integral. This metric measures whether each action has the same expected loss _marginally_ under the true distribution and the forecasts. Thus, accurate loss estimation is only guaranteed marginally and the no regret result is relative to constant decision functions that choose the same action for all examples.

Next, we refine the decision calibration results to hold conditionally on a variable \(Z\). Recall that the kernel over \(y\) is given by \(k_{y}(y,y^{})=(y),(y^{})\). We lift this kernel into \(\) by defining \(k((y,z),(y^{},z^{}))=k_{y}(y,y^{})k_{z}(z,z^{})\) where \(k_{z}\) is a universal kernel. The MMD metric corresponding to this kernel measures decision calibration conditioned on \(Z\). If we choose \(Z\) to be the forecast \(Z=Q_{Y X}\), then the no regret guarantee says that acting as if the forecasts were correct is an optimal decision policy among all policies depending only on the forecast.

Note that the feature maps defined above are infinite dimensional when \(\) or \(\) are infinite. However, applying the kernel trick sometimes allows us to estimate the MMD metric efficiently. Still, optimizing the MMD metric may be difficult in practice, especially when the conditioning variable \(Z\) is very expressive. We suggest using the metric as a regularizer to incentivize decision calibration during training, and then still apply post-hoc recalibration if needed. We now give two concrete examples, one where the action space is infinite and one where the set of loss functions is infinite.

**Example 1** (Point Estimate Decision).: Consider a regression problem in which a decision-maker chooses a point estimate \(a\) and incurs the squared loss \((a,y)=(a-y)^{2}\). The expected loss of an action \(a\) is \(_{P}[(a-Y)^{2}]=(a-_{P}[Y]) ^{2}+_{P}[Y^{2}]-_{P}[Y]^{2}\). Observe that the expected loss only depends on \(Y\) through the first two moments, \(_{P}[Y]\) and \(_{P}[Y^{2}]\). We use the representation \((y)=(y,y^{2})\), so that the kernel is \(k(y,y^{})=(y,y^{2}),(y^{},y^{ 2})\). From the mean embedding form of MMD, we see that \((,P,Q)=\|_{P}-_{Q}\|^{2}=( [Y]-_{Q}[])^{2}+(_{P}[Y^{2}]-_{Q}[^{2}])^{2}\). Therefore, the metric is zero exactly when the expected loss of each action \(a\) is the same under the true distribution and the forecasts. The MMD metric incentivizes the forecasts to accurately reflect the marginal loss of each action. However, we often want to know the optimal action given a particular forecast, not marginally. We can now choose the conditioning variable to refine the guarantee. Let \(Z\) be the forecast \(Z=Q_{Y X}\) and choose the kernel \(k((y,z),(y^{},z^{}))=k_{y}(y,y^{})k_{z}(z,z^{})\) where \(k_{y}(y,y^{})=(y,y^{2}),(y^{},y^{ 2})\) as before and \(k_{z}(z,z^{})\) is universal. For this kernel, the MMD will equal zero if and only if \(_{P}[Y Q_{Y X}]=_{Q}[  Q_{Y X}]\) and \(_{P}[Y^{2} Q_{Y X}]=_{Q}[ {Y}^{2}.\)\(Q_{Y|X}\) almost surely. The Bayes optimal action suggested by the forecast is optimal among decision policies that depend only on the forecast:

\[_{P}\,[(^{*}(Q_{Y|X}),Y)]_{P}\,[((Q_{Y|X}),Y)], (Q_{Y|X})\] (5)

In other words, the decision maker is incentivized to behave as if the forecasts are correct.

**Example 2** (Threshold Decision).: The above example had an infinite action space and a single loss function. Here, we consider a problem where the action space is small but the set of loss functions is infinite. This setting applies for a model vendor who serves a model to multiple decision makers, each with a potentially different loss function. For example, suppose that \(Y[0,T]\) is a blood value for a patient for some \(T>0\), and the decision task is to determine whether \(Y\) exceeds a threshold \(t[0,T]\) that indicates eligibility for a clinical trial. Formally, the decision maker chooses a threshold \(a\{ 1\}\) to minimize the loss \(_{t}(a,y)=\{a(y-t)\}\). Our goal is to simultaneously calibrate our forecasts for all thresholds \(t[0,T]\), i.e. \(=\{_{t}:t[0,T]\}\). In this case, we choose the feature map \((y)=(\{y t\})_{t[0,T]}\). The corresponding kernel is \(k(y,y^{})=(y),(y^{})=\{y,y^{}\}\). The mean embeddings are \(_{P}\,[(Y)]=P_{Y}\) and \(_{Q}[()]=Q_{Y}\), the marginal cdfs for \(Y\) and \(Y_{Q}\). Thus, the MMD metric measures \(\|P_{Y}-Q_{Y}\|_{2}^{2}=_{t=0}^{T}(P_{Y}(t)-Q_{Y}(t))^{2}dt\), which is zero exactly when \(P_{Y}=Q_{Y}\). When \((,P,Q)=0\), the expected loss of each action \(a\) is the same under the true distribution and the forecasts, for all \(\). Similar to the previous example, we can achieve conditional decision calibration by adding a universal kernel over the conditioning variable.

DiscussionThe connections between decision-making and calibration have been previously studied by Zhao et al.  and Sahoo et al. . Here, we extend results from Zhao et al.  to include infinite action spaces and further refine the decision guarantees to depend on an arbitrary conditioning variable. In Example 2, we connect our methods to the special case of threshold calibration . Notably, our framework provides a strategy to optimize decision calibration at training time.

## 6 Experiments

The main goals of our experiments are to: (1) compare the performance our method with other trainable calibration metrics, and with standard post-hoc recalibration methods; (2) study the impact of tailoring the \(\) kernel to a specific decision task; and (3) study whether trainable calibration metrics can be used to improve calibration across local neighborhoods of data.2

### Datasets

We consider standard benchmark datasets and datasets relating to decision-making tasks. For each dataset, we randomly assign 70% of the dataset for training, 10% for validation, and 20% for testing.

Regression DatasetsWe use four tabular UCI datasets (superconductivity, crime, blog, fb-comment), as well as the Medical Expenditure Panel Survey dataset (medicalexpenditure). They are common benchmarks in uncertainty quantification literature . The number of features ranges from \(d=53\) to \(280\). The total number of examples \(n\), and features \(d\) for each dataset can be found in Table 3.

Classification DatasetsWe use five tabular UCI datasets: breast-cancer, heart-disease, online-shoppers, dry-bean, and adult. The datasets range from \(m=2\) to \(7\) label classes, and \(d=16\) to \(104\) features. The total number of examples \(n\), features \(d\), and classes \(m\) for each dataset can be found in Table 4.

Crop Yield DatasetWe introduce a crop-yield dataset, where the task is to predict yearly wheat crop yield from weather data for different counties across the US. We use the NOAA database to track summary weather statistics across each year (temperature, precipitation, cooling/heating degree days), and the NASS database to track annual wheat crop yield. The dataset spans from 1990-2007, where data from 1995 was held-out for visualization, and the remaining data was used for training. We use this dataset to evaluate local calibration with respect to the latitude and longitude coordinates.

### Experimental Setup and Baselines

ModelsTo represent the model uncertainty, we use a feed-forward neural network with three fully-connected layers. In the classification setting, the model outputs the logits for all \(m\) classes. In the regression setting, the model outputs the predicted mean \((Y)\) and variance \(^{2}(Y)\), which are used to parameterize a Gaussian probability distribution. This allows us to tractably compute the inverse cdf \(Q_{Y|X}^{-1}\) and perform differential sampling, enabling us to test our calibration metrics in multiple settings. See Appendix B for more details on model hyperparameter tuning.

Training ObjectivesIn the regression setting, we consider two training objectives: Negative Log-likelihood (\(\)), and our trainable calibration metric as a regularizer (\(+\)). For \(\), we enforce a notion of individual calibration (Table 1) by choosing the conditioning variable \(Z=X\) to match the distributions of \((X,)\) and \((X,Y)\). More specifically, we optimize an MMD objective where the kernel \(k\) is decomposed among the pair of random variables as \(k((x,y),(x^{},y^{}))=k_{X}(x,x^{}) k_{Y}(y,y^{})\). The form of \(k_{X}\) and \(k_{Y}\) varies across experiments. In the classification setting, we consider cross-entropy (XE) loss, and three trainable calibration metrics as regularizers: \(\), \(\), and \(\) (_Ours_).

BaselinesIn the regression setting, we compare our approach against an uncalibrated forecaster trained with \(\), and with post-hoc recalibration using isotonic regression . In the classification setting, we compare our approach against an uncalibrated forecaster trained with \(\) loss. Furthemore, we compare our approach to two trainable calibration metrics (\(\) and \(\)). We compare all approaches against temperature-scaling , a standard post-hoc recalibration method. For both classification and regression, we train the post-hoc recalibration methods on a validation set, and subsequently evaluate on the held-out test set.

MetricsTo quantify predictive performance in regression, we evaluate the negative log-likelihood, quantile calibration error (QCE) , and decision calibration error (see Section 6.3). In classification, we compute accuracy, expected calibration error (calibration), and average Shannon entropy (sharpness) of the forecast. We also report kernel calibration error (KCE), defined as the MMD

  
**Dataset** & Training Objective & NLL \(\) & Quantile Cal. \(\) & Decision Cal. \(\) \\   crime \\ \(n=1992\) \\ \(d=102\) \\  } & NLL & -0.716 \(\) 0.007 & 0.220 \(\) 0.004 & 0.151 \(\) 0.009 \\  & + Post-hoc & -0.387 \(\) 0.012 & 0.154 \(\) 0.003 & 0.089 \(\) 0.005 \\  & NLL + MMD (_Ours_) & **-0.778 \(\) 0.008** & 0.164 \(\) 0.004 & 0.042 \(\) 0.011 \\  & + Post-hoc & -0.660 \(\) 0.014 & **0.105 \(\) 0.004** & **0.041 \(\) 0.005** \\   blog \\ \(n=52397\) \\ \(d=280\) \\  } & NLL & 0.997 \(\) 0.040 & 0.402 \(\) 0.006 & 4.087 \(\) 0.004 \\  & + Post-hoc & **0.624 \(\) 0.021** & 0.053 \(\) 0.001 & 4.090 \(\) 0.001 \\  & NLL + MMD (_Ours_) & 0.957 \(\) 0.008 & 0.302 \(\) 0.002 & **4.051 \(\) 0.002** \\  & + Post-hoc & 0.945 \(\) 0.007 & **0.042 \(\) 0.001** & 4.067 \(\) 0.001 \\   medical-expenditure \\ \(n=33005\) \\ \(d=107\) \\  } & NLL & 1.535 \(\) 0.000 & 0.078 \(\) 0.001 & 0.476 \(\) 0.002 \\  & + Post-hoc & 1.808 \(\) 0.002 & 0.059 \(\) 0.001 & 0.472 \(\) 0.002 \\  & NLL + MMD (_Ours_) & **1.532 \(\) 0.000** & 0.059 \(\) 0.001 & 0.438 \(\) 0.002 \\  & + Post-hoc & 1.780 \(\) 0.001 & **0.045 \(\) 0.000** & **0.431 \(\) 0.001** \\   superconductivity \\ \(n=21264\) \\ \(d=81\) \\  } & NLL & 3.375 \(\) 0.008 & 0.062 \(\) 0.003 & 0.211 \(\) 0.003 \\  & + Post-hoc & 3.707 \(\) 0.007 & 0.066 \(\) 0.001 & 0.202 \(\) 0.003 \\  & NLL + MMD (_Ours_) & **3.269 \(\) 0.012** & **0.042 \(\) 0.003** & **0.182 \(\) 0.003** \\  & + Post-hoc & 3.643 \(\) 0.010 & 0.050 \(\) 0.002 & 0.186 \(\) 0.003 \\   fp-comment \\ \(n=40949\) \\ \(d=53\) \\  } & NLL & 0.634 \(\) 0.016 & 0.334 \(\) 0.004 & 3.151 \(\) 0.003 \\  & + Post-hoc & 0.734 \(\) 0.015 & 0.063 \(\) 0.001 & 3.151 \(\) 0.002 \\   & NLL + MMD (_Ours_) & **0.605 \(\) 0.004** & 0.258 \(\) 0.003 & **3.131 \(\) 0.003** \\   & + Post-hoc & 0.639 \(\) 0.005 & **0.054 \(\) 0.001** & 3.138 \(\) 0.002 \\   

Table 3: Comparison of model performance on five different regression datasets. Models were trained with two objectives: NLL and NLL + MMD (Ours). We display the metrics on the test set for each training procedure, both with and without post-hoc Quantile Calibration  fit to the validation set. \(n\) is the number of examples in the dataset and \(d\) is the number of features.

objective for an RBF kernel over \(\). Each metric is computed on a held-out test set. We repeat all experiments across 50 random seeds and report the mean and standard error for each metric.

ResultsThe results for regression and classification are shown in Table 3 and Table 4, respectively. In regression, our objective (\(+\)) achieves better QCE than the \(\) objective across all datasets while maintaining comparable (or better) \(\) scores on the test set. On most datasets, we find that our method also reduces the sharpness penalty induced by post-hoc calibration. Importantly, we also observe that post-hoc calibration and our method are complementary for calibrating fore-casters; post-hoc methods can guarantee calibration on held-out data, and calibration regularization mitigates the sharpness penalty. For classification, we find that our method improves accuracy, ECE, and entropy relative to the baselines we tested for calibration regularization. These trends also generally hold with post-hoc calibration, where we observe that our method achieves better calibration and accuracy across most datasets (See Table 5).

Computational RequirementsRelative to training with an unregularized NLL objective, our method with MMD regularization requires an average wall clock time per epoch of 1.2x for the 5 regression datasets, and 1.3x for the 5 classification datasets.

### Experiment: Decision Calibration

One of the key advantages of our trainable calibration metrics is their flexibility in enforcing multiple notions of calibration. We consider a decision-making task to study the effectiveness of our method in improving decision calibration. The decision task is to choose an action \(a\{ 1\}\), whose loss \((a,y)=\{a(y-c)\}\) depends only on whether the label \(y\) exceeds a threshold \(c\). Given a true distribution \(P\) and a forecaster \(Q_{Y|X}\), the **Decision Calibration Error** (\(\)) is \(^{2}(P,Q)=_{a}||_{P}|(Y,a)|- _{X}_{} Q_{Y|X}}[(,a)]||_{2}^{2}\).

MMD ObjectiveTo optimize for decision calibration, we tailor the kernels \(k_{X},k_{Y}\) to reflect the decision problem. More concretely, we choose a kernel function that penalizes predicted labels which are on the wrong side of \(c\), namely \(k_{Y}(y,y^{})=+1\) if \((y-c)=(y^{}-c)\), and \(-1\) otherwise.

  
**Dataset** & Training Objective & Accuracy \(\) & ECE \(\) & Entropy \(\) & KCE \(\) \\  breast-cancer & XE & 95.372 \(\) 0.160 & 0.194 \(\) 0.003 & 0.453 \(\) 0.004 & 6.781 \(\) 0.601 \\ \(n=569\) & XE + MMCE & 94.770 \(\) 0.147 & 0.060 \(\) 0.001 & 0.076 \(\) 0.002 & 0.392 \(\) 0.086 \\ \(d=30\) & XE + ECE KDE & 94.351 \(\) 0.163 & 0.062 \(\) 0.001 & 0.065 \(\) 0.001 & 0.225 \(\) 0.083 \\ \(m=2\) & XE + MMD (_Ours_) & **95.789 \(\) 0.060** & **0.052 \(\) 0.000** & **0.006 \(\) 0.000** & **0.014 \(\) 0.000** \\  heart-disease & XE & 55.904 \(\) 0.196 & 0.373 \(\) 0.003 & 1.512 \(\) 0.007 & 0.581 \(\) 0.016 \\ \(n=921\) & XE + MMCE & 60.787 \(\) 0.208 & 0.267 \(\) 0.002 & 0.947 \(\) 0.003 & 0.097 \(\) 0.002 \\ \(d=23\) & XE + ECE KDE & 50.036 \(\) 2.801 & 0.304 \(\) 0.012 & 1.392 \(\) 0.016 & 0.450 \(\) 0.050 \\ \(m=5\) & XE + MMD (_Ours_) & **61.616 \(\) 0.255** & **0.261 \(\) 0.003** & **0.945 \(\) 0.006** & **0.077 \(\) 0.002** \\  online-shoppers & XE & 89.816 \(\) 0.037 & 0.129 \(\) 0.001 & 0.221 \(\) 0.003 & 0.022 \(\) 0.001 \\ \(n=12330\) & XE + MMCE & 89.933 \(\) 0.036 & 0.128 \(\) 0.001 & 0.220 \(\) 0.002 & -0.019 \(\) 0.001 \\ \(d=28\) & XE + ECE KDE & **90.019 \(\) 0.034** & **0.127 \(\) 0.000** & 0.225 \(\) 0.002 & -0.022 \(\) 0.001 \\ \(m=2\) & XE + MMD (_Ours_) & 89.976 \(\) 0.031 & **0.127 \(\) 0.001** & **0.218 \(\) 0.002** & **-0.026 \(\) 0.000** \\  dry-bean & XE & 92.071 \(\) 0.025 & 0.113 \(\) 0.000 & 0.264 \(\) 0.001 & 0.061 \(\) 0.002 \\ \(n=13612\) & XE + MMCE & 92.772 \(\) 0.035 & 0.099 \(\) 0.000 & 0.224 \(\) 0.002 & 0.048 \(\) 0.004 \\ \(d=16\) & XE + ECE KDE & 92.760 \(\) 0.037 & 0.097 \(\) 0.000 & 0.232 \(\) 0.001 & 0.041 \(\) 0.003 \\ \(m=7\) & XE + MMD (_Ours_) & **92.894 \(\) 0.035** & **0.089 \(\) 0.000** & **0.174 \(\) 0.002** & **0.040 \(\) 0.004** \\  adult & XE & 84.528 \(\) 0.040 & 0.186 \(\) 0.000 & 0.320 \(\) 0.002 & -0.014 \(\) 0.002 \\ \(n=32561\) & XE + MMCE & 84.203 \(\) 0.042 & 0.191 \(\) 0.000 & 0.340 \(\) 0.002 & -0.015 \(\) 0.002 \\ \(d=104\) & XE + ECE KDE & 84.187 \(\) 0.045 & **0.178 \(\) 0.001** & 0.414 \(\) 0.002 & **-0.020 \(\) 0.003** \\ \(m=2\) & XE + MMD (_Ours_) & **84.565 \(\) 0.035** & **0.178 \(\) 0.001** & **0.315 \(\) 0.002** & -0.018 \(\) 0.001 \\   

Table 4: Experimental results for five tabular classification tasks, comparing MMCE regularization , KDE regularization , and MMD regularization (Ours), alongside a standard cross-entropy (XE) loss without post-hoc calibration. Here, \(n\) is the number of examples in the dataset, \(d\) is the number of features, and \(m\) is the number of classes.

To allow for gradient based optimization, we relax the objective to yield the differentiable kernel \(k_{Y}(y,y^{})=(y-c)(y^{}-c)\). Notice that \(k_{Y}(y,y^{})+1\) when \((y-c)=(y-c)\), and \(-1\) otherwise. To encourage decision calibration for all features, we let \(k_{X}\) be an RBF kernel.

ResultsThe results in Table 3 demonstrate that the \(\) training objective tailored to the decision problem achieves the best decision calibration across all datasets. Additional results in Appendix B.3 show that using an \(\) kernel tailored to a decision problem (i.e. the \(\) kernel) improves decision calibration scores across all datasets.

### Experiment: Local Calibration

SetupTo provide intuition on how our metrics can facilitate calibration in local neighbourhoods of features, we study how location affects the reliability of forecasts on the crop-yield regression dataset. We tailor the kernels \(k_{X},k_{Y}\) to local calibration for geospatial neighborhoods. Namely, we select \((x)\) which extracts the geospatial features (latitude, longitude) from the full input vector \(x\). We then define \(k_{X}(x,x^{})=k_{}((x),(x^{}))\) and \(k_{Y}(y,y^{})=k_{}(y,y^{})\).

MetricExtending prior work on local calibration , given a cdf forecaster \(Q_{Y|X}\), a feature embedding \(()\) and kernel \(k\), we define **Local Calibration Error** (\(\)) for regression is is computed as \(_{}(x)=_{i=1}^{B}(x;c_{i})^{2}\), where \(c_{i}=\). Intuitively, \(\) measures the QCE across local regions determined by \(()\). We visualize the results in Figure 1 for \(B=20\).

## 7 Conclusion

We introduced a flexible class of trainable calibration metrics based on distribution matching using \(\). These methods can be effectively combined with existing post-hoc calibration methods to achieve strong calibration guarantees while preserving sharpness.

LimitationsOptimizing the \(\) metric requires that samples from the forecast are differentiable with respect to the model parameters. This restricts the family of applicable forecasters, but differentiable relaxations such as Gumbel-softmax can mitigate this challenge. Furthermore, in practice it is often not possible to achieve zero calibration error via regularization due to computational and data limitations. To mitigate this limitation, we suggest pairing our calibration regularizers with post-hoc calibration methods. Lastly, the scale of the \(\) metric is sensitive to the chosen kernel (e.g., the bandwidth of a RBF kernel), so it can be difficult to understand how calibrated a model is in an absolute sense based on the \(\) metric.

Figure 1: We visualize Local Calibration Error for a forecaster trained to predict annual wheat crop-yield based on climate and geospatial data. The standard \(\) objective (**Left**) leads to a forecaster that is miscalibrated across local geospatial neighbourhoods, as seen by areas of brighter color. Our kernel-based calibration objective (**Right**) leads to better calibration across local neighborhoods.

Acknowledgements

CM is supported by the NSF GRFP. This research was supported in part by NSF (#1651565), ARO (W911NF-21-1-0125), ONR (N00014-23-1-2159), and CZ Biohub.