# Supported Value Regularization for Offline Reinforcement Learning

Yixiu Mao\({}^{1}\), Hongchang Zhang\({}^{1}\), Chen Chen\({}^{1}\), Yi Xu\({}^{2}\), Xiangyang Ji\({}^{1}\)

\({}^{1}\)Department of Automation, Tsinghua University

\({}^{2}\)School of Artificial Intelligence, Dalian University of Technology

myx21@mails.tsinghua.edu, xyji@tsinghua.edu

###### Abstract

Offline reinforcement learning suffers from the extrapolation error and value overestimation caused by out-of-distribution (OOD) actions. To mitigate this issue, value regularization approaches aim to penalize the learned value functions to assign lower values to OOD actions. However, existing value regularization methods lack a proper distinction between the regularization effects on in-distribution (ID) and OOD actions, and fail to guarantee optimal convergence results of the policy. To this end, we propose Supported Value Regularization (SVR), which penalizes the \(Q\)-values for _all_ OOD actions while maintaining _standard_ Bellman updates for ID ones. Specifically, we utilize the bias of importance sampling to compute the summation of \(Q\)-values over the entire OOD region, which serves as the penalty for policy evaluation. This design automatically separates the regularization for ID and OOD actions without manually distinguishing between them. In tabular MDP, we show that the policy evaluation operator of SVR is a contraction, whose fixed point outputs unbiased \(Q\)-values for ID actions and underestimated \(Q\)-values for OOD actions. Furthermore, the policy iteration with SVR guarantees strict policy improvement until convergence to the optimal support-constrained policy in the dataset. Empirically, we validate the theoretical properties of SVR in a tabular maze environment and demonstrate its state-of-the-art performance on a range of continuous control tasks in the D4RL benchmark.

## 1 Introduction

Offline Reinforcement Learning (RL) aims to learn a policy from a fixed dataset collected by some behavior policy . It can tap into existing large-scale datasets  for safe and efficient learning. However, it suffers from the extrapolation error  caused by out-of-distribution (OOD) actions, which can further result in severe overestimation of value functions .

To mitigate this issue, value regularization approaches attempt to penalize the \(Q\)-values of OOD actions to introduce conservatism in value estimation . However, we observe that existing value regularization methods not only fall short in penalizing all OOD \(Q\)-values but also may introduce detrimental changes to in-distribution (ID) ones. Specifically, most of them involve adding penalties to the critic learning objective . However, due to the difficulty of distinguishing between ID and OOD actions, they typically adopt the idea of being pessimistic about the actions under the current policy and optionally optimistic about the ones within the dataset . Rather than relying on ID or OOD, this regularization is essentially based on the policy density, which we show is problematic when the dataset is heavily corrupted by sub-optimal actions. Other works involve reducing the Bellman target with uncertainty quantifiers  or ensembles . However, if not incorporating additional penalties, they do not provide learning signals on OOD \(Q\)-values, logically only suppressing the overestimation in the ID region .

In this work, we revisit the original objective of value regularization and ask a question - "Can we devise a value regularization method that penalizes _all_ OOD \(Q\)-values _without_ affecting ID ones"? We point out that the inability of prior methods to achieve this leads to a lack of strong theoretical guarantees for policy performance. In offline RL, the best policy that can be guaranteed should lie within the support of the behavior policy, known as the optimal support-constrained policy . However, existing value regularization methods do not provide reliable guarantees for converging to it. On the other hand, several endeavors aim to learn it through specific policy constraints [10; 22; 11], but their empirical performance leaves considerable room for improvement.

To this end, we propose Supported Value Regularization (SVR), a new value regularization method that penalizes the \(Q\)-values for _all_ OOD actions while maintaining _standard_ Bellman updates for ID ones. Specifically, we leverage the bias of importance sampling to calculate the summation of \(Q\)-values over the entire OOD region, which serves as the penalty term for policy evaluation. This design circumvents the dilemma of distinguishing between ID and OOD actions and separates the regularization effects on them automatically. Theoretically, SVR offers stronger guarantees than previous methods. Under tabular MDP, we show that the SVR policy evaluation operator is a contraction in the whole state-action space and its fixed point outputs unbiased \(Q\)-values for ID actions and underestimated \(Q\)-values for OOD ones, while the fixed point of \(Q\) in CQL  may underestimate or overestimate \(Q\)-values. More importantly, SVR guarantees strict policy improvement until convergence to the optimal support-constrained policy, while prior value regularization methods lack such each-step improvement and global optimal convergence guarantees.

In practice, SVR is easy to implement by adding a penalty term to the ordinary policy evaluation loss. Empirically, we validate the support-constrained optimality of SVR in a tabular maze environment, where baselines fail to converge. Moreover, SVR achieves state-of-the-art performance on a range of continuous control tasks in the D4RL benchmark  and shows strong advantages on noisy datasets.

## 2 Preliminaries

Offline RL.The environment in RL is typically modeled as a Markov Decision Process (MDP) \(=(,,P,R,,d_{0})\), with state space \(\), action space \(\), transition dynamics \(P:()\), reward function \(R:[R_{},R_{}]\), discount factor \([0,1)\), and initial state distribution \(d_{0}\). The goal of RL is to find a policy \(:()\) that maximizes the expected discounted return: \(_{s_{0} d_{0},a_{}(|s_{t}),s_{t+1} P(|s_{t },a_{t})}[_{0}^{}^{t}R(s_{t},a_{t})]\). For any policy \(\), we define the value function as \(V^{}(s)=_{}[_{t=0}^{}^{t}R(s_{t},a_{t})|s_{0}=s]\) and the state-action value function (\(Q\)-value function) as \(Q^{}(s,a)=_{}[_{t=0}^{}^{t}R(s_{t},a_{t})|s_{0} =s,a_{0}=a]\). By the boundedness of rewards, we have \(Q^{}[Q_{},Q_{}]\), where \(Q_{}:=R_{}/(1-)\) and \(Q_{}:=R_{}/(1-)\). In addition, we use \(_{sa}^{}\) to denote the normalized and discounted state-action occupancy of policy \(\) with the initial state-action pair \((s,a)\): \(_{sa}^{}(s^{},a^{})=(1-)_{t=0}^{}^{t} _{}[[s_{t}=s^{},a_{t}=a^{}] |s_{0}=s,a_{0}=a|\).

Actor-critic methods  alternate between evaluating the policy by iterating the Bellman operator \((^{}Q)(s,a):=R(s,a)+_{a^{}} _{a^{}(|s^{})}[Q(s^{},a^{ })]\), and improving the policy by maximizing the \(Q\)-value. In offline RL, the agent is provided with a fixed dataset \(\) collected by some behavior policy \(\). Ordinary actor-critic algorithms [9; 12; 39] minimize the following losses alternately:

\[L_{Q}()=_{(s,a,s^{})}[(Q_{ }(s,a)-R(s,a)-_{a^{}_{}(|s^{ })}Q_{^{}}(s^{},a^{}))^{2}]\;\;() \] \[L_{}()=-\;_{s,a_{}}\; [Q_{}(s,a)]\;\;() \]

where \(_{}\) is a policy parameterized by \(\), \(Q_{}(s,a)\) is a \(Q\) function parameterized by \(\), and \(Q_{^{}}(s,a)\) is a target \(Q\) function whose parameters are updated via Polyak averaging .

Value regularization.In offline RL, OOD actions \(a^{}\) can produce erroneous Bellman targets and lead to an inaccurate estimation of \(Q\)-values. Then in policy improvement, the policy tends to prioritize OOD actions whose values have been overestimated, resulting in poor performance.

To address this issue, value regularization methods regularize the \(Q\) function to introduce conservatism in value estimation [23; 20]. As the most representative one, CQL  minimizes the following policy evaluation loss, which guarantees to obtain underestimated \(V\) functions:

\[_{Q}_{s,a}[(Q(s,a)-^{}Q^{}(s,a))^{2}]+(_{s,a}[Q(s,a)]-_{s,a}[Q(s,a)]) \]

where \(Q^{}\) is the target \(Q\) function and \(\) is a hyperparameter.

## 3 Supported Value Regularization

In this section, we first briefly analyze the existing density-based value regularization and identify some important issues. Then we propose Supported Value Regularization (SVR) to penalize all OOD \(Q\)-values while maintaining standard updates for ID ones. Next, we conduct a thorough analysis of SVR and demonstrate its theoretical superiority. Last, we present the implementation details of SVR.

### Density-based value regularization

A large portion of value regularization methods are built on the idea of being pessimistic about the actions under current policy and optimistic about the actions within the dataset [23; 50; 27; 6]. Instead of relying on ID or OOD, we show that this regularization is essentially based on the policy density, which we refer to as density-based value regularization. Specifically, we take CQL (Eq. (3)) , the foundational work on value regularization, as an example for mathematical analysis. Here we remove the strong assumption \(()()\) in their paper and analyze its regularization effects on the \(Q\) functions. From Eq. (3), we can obtain the analytical solution of \(Q\), which corresponds to the following policy evaluation operator \(^{}_{}\):

\[^{}_{}Q(s,a)=\{ ^{}Q(s,a)-(-1),& (a|s)>0,\\ -,&(a|s)=0,(a|s)>0,\\ Q(s,a),&(a|s)=0,(a|s)=0.. \]

Considering the iteration of policy evaluation: \(Q^{k+1}(s,a)=^{}_{}Q^{k}(s,a)\), we have Observation 1:

**Observation 1**.: _In each iteration \(k\), compared to \(^{}\), \(^{}_{}\) (1) lowers \(Q^{k+1}(s,a)\) when \((a|s)>(a|s)\); (2) raises \(Q^{k+1}(s,a)\) when \((a|s)<(a|s)\); (3) obtains the same \(Q^{k+1}(s,a)\) when \((a|s)=(a|s)>0\); (4) does not update \(Q^{k+1}(s,a)\) when \((a|s)=(a|s)=0\)._

For (1)(2)(3), lowering or raising \(Q\)-values based on the relative density between \(\) and \(\) could be problematic, especially when the dataset contains a large portion of bad actions. In such cases, assuming the policy \(\) has found the optimal action \(a^{*}\), it holds that \((a^{*}|s)>(a^{*}|s)\), and thus CQL will lower \(Q(s,a^{*})\); for some bad action \(\), it holds that \((|s)<(|s)\), and thus CQL will raise \(Q(s,)\). That is, CQL tends to raise the \(Q\)-values of numerous bad actions and lower the \(Q\)-values of scare good actions, forcing the policy to choose bad actions in policy improvement stage. Regarding (4), in the entire OOD region (\((a|s)=0\)), the penalization is only performed where \((a|s)>0\). Since the distribution of \(\) is narrow compared to the whole action space, the regularization region of CQL and most existing methods [50; 6; 27; 3; 48] tends to be too narrow.

### Supported value regularization

In this paper, we aim to penalize \(Q\)-values for all OOD actions (\(a()\)) and maintain standard Bellman updates for ID ones (\(a()\)). That is, the regularization is solely determined by the support of \(\), which we refer to as supported value regularization.

However, it is a well-known challenge to distinguish between ID and OOD actions. Directly determining if the behavior density exceeds a threshold  would necessitate an extremely precise density estimator, and it is particularly difficult to estimate the density of OOD actions. In this work, we draw inspiration from the link between support and Importance Sampling (IS) to circumvent this dilemma and achieve the same goal.

We begin by explaining its principle with simplified notations. IS computes \(l_{1}=_{q}[p(x)f(x)/q(x)]\) to estimate \(l_{2}=_{p}f(x)\). When assuming \((p)(q)\), IS is unbiased: \(l_{1}=l_{2}\). However, when removing the support assumption, IS actually computes \(l_{1}=_{x(q)}p(x)f(x)\). We observe that \(l:=l_{1}-l_{2}=_{x(q)}p(x)f(x)\) gives the summation over the out-of-support region. In the offline RL setting, let \(f\) be the \(Q\) function. If we choose the behavior policy \(\) as \(q\) and any samplable distribution that covers the whole action space as \(p\), then minimizing \(l\) would lower all OOD \(Q\)-values without affecting the ID ones. Besides, since both distribution \(q\) and \(p\) are samplable in this case, we can obtain \(l_{1}\) and \(l_{2}\) based on sampling, which provides an unbiased \(l\) to optimize.

Therefore, we minimize the following loss for policy evaluation in SVR:

\[_{Q}&_{s,a }[(Q(s,a)-^{}Q^{}(s,a))^{2}] \\ &+(_{s,a u}[(Q(s,a)-Q_{})^{2}]-_{s,a}[ (Q(s,a)-Q_{})^{2}]) \]

where \(u(|s)\) is any samplable distribution (e.g., Gaussian, uniform) whose support covers the whole action space, and \(Q_{}:=R_{}/(1-)\) is the minimal possible \(Q\) of the MDP. According to the analysis above, Eq. (5) is equivalent to the following minimization problem:

\[_{Q}_{s,a}[(Q(s,a)- ^{}Q^{}(s,a))^{2}]+_{s} [_{a((|s))}u(a|s)(Q(s,a)-Q_{})^{2} ] \]

It is clearer from Eq. (6) that the loss in Eq. (5) penalizes all OOD \(Q\)-values without affecting the ID ones. Note that optimizing Eq. (5) requires pre-training a behavior model \(\) to output the behavior density \((a|s)\) in the IS ratio. Compared with other methods that also require the behavior model [43; 20; 11; 27], SVR is less susceptible to model errors. This is because SVR only needs to query the behavior density of in-dataset \((s,a)\) pairs, thus not requiring much generalization ability of the model, making it relatively easier to estimate accurately.

From an optimization perspective, minimizing Eq. (6) reduces all OOD \(Q\)-values with a strength proportional to \(u(a_{}|s)\). Therefore, we can even choose various \(u\) to flexibly penalize the OOD region. For example, let \(u\) assign higher weight to areas with a high probability of overestimation. Note that \(u\) can have any positive density in the ID region without affecting the ID \(Q\)-values, thereby eliminating the necessity to manually distinguish between ID and OOD regions. From an optimal solution perspective, Eq. (5,6) lead to the following SVR policy evaluation operator:

\[^{}_{}Q(s,a)=\{^{ }Q(s,a),&(a|s)>0,\\ Q_{},&. \]

In contrast to Observation 1, we make the following claim for SVR.

**Claim 1**.: _In each iteration k, compared to \(^{}\), \(^{}_{}\) obtains the same \(Q^{k+1}(s,a)\) when \((a|s)>0\) (ID region); and lowers \(Q^{k+1}(s,a)\) when \((a|s)=0\) (OOD region)._

### Analysis

In this section, we refer to the policy iteration, whose evaluation part is repeatedly applying \(^{}_{}\) and whose improvement part is vanilla maximization of \(Q\), as SVR. We will give a comprehensive analysis of SVR, including the fixed point in policy evaluation, the monotonic improving performance in policy improvement, and the support-constrained optimal convergence for the whole policy iteration.

We first define the support-constrained policy set , which plays an important role in our analysis. However, it is worth noting that SVR does not impose any constraint or regularization on the policy.

**Definition 1** (Support-constrained policy).: _The support-constrained policy class \(\) is defined as_

\[=\{(a|s)=0(a|s)=0\} \]

Following prior works , we also define the optimal support-constrained policy \(^{*}_{}\).

**Definition 2** (Optimal support-constrained policy).: _The optimal support-constrained policy \(^{*}_{}\) is:_

\[^{*}_{}(a|s):=[a=*{argmax}_{a^{} ((|s))}Q^{*}_{}(s,a^{})] \]

_where \(Q^{*}_{}\) satisfies the support-constrained Bellman optimality equation:_

\[Q^{*}_{}(s,a)=R(s,a)+_{s^{} P(|s,a)}[ _{a^{}((|a^{}))}Q^{*}_{}(s^{ },a^{})]. \]

**Proposition 1** (Contraction).: _In the whole \(\) space and for any \(\), \(^{}_{}\) is a \(\)-contraction operator in the \(_{}\) norm._Therefore, in the policy evaluation stage of SVR, any initial \(Q\) function can converge to a unique fixed point by repeatedly applying \(_{}^{}\). We give this fixed point in the following theorem.

**Theorem 1** (Fixed point).: _SVR yields support-constrained \(_{i}:_{i}\). The fixed point of \(_{}^{_{i}}\) is_

\[f^{_{i}}(s,a)=\{Q^{_{i}}(s,a),&(a|s)>0,\\ Q_{},&(a|s)=0.. \]

_Therefore, for the policies \(_{i}\) in SVR, this fixed point provides unbiased \(Q\)-values for all ID actions and underestimated \(Q\)-values for all OOD actions._

Though Theorem 1 guarantees that SVR only obtains support-constrained policies during learning, we show that, even for any \(\) (\(\) due to various errors in practice), the fixed point of \(_{}^{}\) still ensures that \(Q\) will not be overestimated over the entire action space.

**Proposition 2**.: _For any \(\), the fixed point of \(_{}^{}\) satisfies_

\[\{Q_{} f^{}(s,a) Q^{}(s,a),&(a |s)>0,\\ f^{}(s,a)=Q_{},&(a|s)=0.. \]

For comparison, the following proposition characterizes the contraction property of CQL.

**Proposition 3**.: _Only in the ID region (\((a|s)>0\)) and when the evaluated policy is support-constrained (\(\)), \(_{}^{}\) is a contraction operator. Its fixed point satisfies_

\[f^{}(s,a)=Q^{}(s,a)-(_{sa}^{})^{T}( -1),\;(a|s)>0. \]

Therefore, \(_{}^{}\) has no fixed point in the OOD region. Even in the ID region, the condition \(\) is required and its fixed point may underestimate or overestimate \(Q\)-values in a complicated way.

Finally, we show that SVR guarantees strict policy improvement for each iteration until convergence to the optimal support-constrained policy \(_{}^{}\). Note that existing value regularization methods fail to guarantee such each-step policy improvement and optimal convergence results.

**Theorem 2** (Strict policy improvement to support-constrained optimal).: _SVR yields support-constrained \(_{i}\) and guarantees monotonic performance improvement:_

\[V^{_{i+1}}(s) V^{_{i}}(s) s, \]

_where the improvement is strict in at least one state until \(_{}^{}\) is found._

### Practical implementation of SVR

SVR is easy to implement and we design the practical algorithm to be as simple as possible to avoid some complex modules confusing our algorithm's impact on the final performance 1.

```
1:Initialize behavior policy \(_{}\), policy network \(_{}\), \(Q\)-network \(Q_{}\), and target \(Q\)-network \(Q_{^{}}\)
2:// Behavior Policy Pre-training
3:for each gradient step do
4: Sample minibatch \((s,a)\)
5: Update \(\) by maximizing \(J_{}()\) in Eq. (15)
6:endfor
7:// Policy Training
8:for each gradient step do
9: Sample minibatch \((s,a,r,s^{})\)
10: Update \(\) by minimizing \(L_{Q}()\) in Eq. (16)
11: Update \(\) by minimizing \(L_{}()\) in Eq. (2)
12: Update target network: \(^{}(1-)^{}+\)
13:endfor
```

**Algorithm 1** Supported Value Regularization (SVR)

**Behavior model.** Following previous works , we learn a Gaussian model \(_{}\) for the behavior policy by maximizing

\[J_{}()=_{s,a}_{}(a|s) \]

Sampling distribution \(u\).In our method, \(u\) in Eq. (5) can be any samplable distribution that covers the entire action space. In our implementation, we set \(u\) to the Gaussian with the same mean as \(\) and a fixed variance that is much larger than that of \(\). This choice is made because during learning, \(\) is typically located where \(Q\) is maximized and where overestimation is most likely to occur. So if \(\) is OOD, this choice will assign higher weight to the area around \(\) to reduce the \(Q\)-value. Note that if \(\) is ID, it will not penalize the \(Q\)-values in the ID region even though \(u\) may have a higher weight there.

Policy evaluation.With policy \(_{}\), \(Q\) function \(Q_{}\), and target \(Q\) function \(Q_{^{}}\), we minimize the following loss for policy evaluation:

\[ L_{Q}()&=_{(s,a,s^{ })}[Q_{}(s,a)-R(s,a)-_{a^{ }_{}(|s^{})}Q_{^{}}(s^{},a^{ })^{2}]\\ &+(_{s,a u}[(Q_{ }(s,a)-Q_{})^{2}]-_{s,a}[(a|s)}(Q_{}(s,a)-Q_{})^{2}]) \]

where \(\) is a hyperparameter and \(Q_{}:=R_{}/(1-)\). For the environment where \(R_{}\) is unknown, we choose the smallest reward in the dataset as \(R_{}\).

Overall Algorithm.Putting everything together, we summarize our final algorithm in Algorithm 1. Our algorithm first trains the estimated behavior policy to obtain the behavior density. Then it turns to the actor-critic framework for policy training.

## 4 Experiments

In this section, we conduct several experiments to justify the validity of our method. We aim to answer five questions: (1) Does SVR actually converge to the optimal support-constrained policy? (2) Does SVR perform better than previous methods on standard offline RL benchmarks? (3) When does SVR empirically benefit the most compared to the density-based regularization? (4) How should we select the sampling distribution of SVR in practice? (5) How does the implementation of each component affect SVR? More experimental details and results are provided in Appendix B and C.

### Support-constrained optimality in the tabular setting

We use a simple maze environment to verify the support-constrained optimality of SVR. As depicted in Fig. 1(a), the task is to navigate from bottom-left to top-right, with a wall in the middle. The agent receives a reward of \(0\) for reaching the goal and \(-1\) for all other transitions. Episodes are terminated after 100 steps and \(\) is set to \(0.9\). We first collect \(10,000\) transitions using a random policy. Then we remove all the transitions containing rightward actions in the bottom-left \(4 4\) region to introduce OOD actions. It makes the optimal support-constrained policy (see Fig. 1(b)) differ from the actual optimal policy in the environment.

We test three algorithms: vanilla policy iteration , CQL , and SVR. Fig. 1(c, d, e) depict their learned value functions and policies respectively. To show convergence results, we also present the learning curves in Fig. 2, along with the performance of the behavior policy and the optimal support-constrained policy. The results indicate that vanilla policy iteration has a severe over-estimation of value functions, leading to poor performance. On the other hand, although CQL does not overestimate \(V\) functions, it fails to converge and performs poorly when the dataset is highly sub-optimal. In contrast, SVR converges to the optimal support-constrained policy and the learned value function closely matches the true support-constrained optimal value function, verifying Theorem 2.

Figure 1: (a) The maze environment with OOD actions. (b) The optimal support-constrained policy and its value function. (c) (d) (e) The value functions and policies learned by vanilla policy iteration, CQL and SVR, respectively. The resulting trajectories are illustrated by white dashed lines. SVR is capable of obtaining the optimal support-constrained policy and value function.

Figure 2: Learning curves of SVR, CQL and vanilla policy iteration on the toy maze.

### Comparisons on D4RL benchmarks

Then we evaluate our approach on the D4RL benchmarks . We compare SVR with prior state-of-the-art offline RL methods, including BC , BCQ , BEAR , OneStep RL , TD3+BC , AWAC , UWAC , CQL , and IQL .

The results are reported in Table 1. In both Gym-MuJoCo and Adroit domains, SVR achieves state-of-the-art performance and outperforms prior value regularization methods (CQL, UWAC) by a large margin. We also observe that SVR has strong advantages on the sub-optimal datasets (medium, medium-replay, random). This is because SVR exploits the optimal support-constrained policy in the dataset in a theoretically sound way and is less affected by the average quality of the dataset. In addition, while BCQ and BEAR are the policy constraint methods that are also designed to search for the optimal policy within the behavior support, they have much poorer performance. As stated in previous works [11; 44], they are limited respectively by the errors of the generative model and the unsuitability of using Maximum Mean Discrepancy (MMD) to characterize the support constraint. In contrast, without querying out-of-dataset behavior density, SVR takes the form of value regularization to search for the optimal support-constrained policy and achieves superior performance. For learning curves and more experimental details, please see Section B in the supplementary material.

### Comparisons on noisy datasets.

In this section, we aim to validate that, compared with the existing density-based value regularization, the supported value regularization of SVR will benefit when the dataset contains a large portion of bad actions. To this end, we construct a "noisy" dataset by combining the random dataset and the expert dataset, and then evaluate SVR and CQL under different expert ratios.

The results are shown in Fig. 3. In all environments, SVR outperforms CQL over nearly all expert ratios. Moreover, as the expert ratio gets lower, the improvement is more significant. The performance of CQL is susceptible to the expert ratio and exhibits a sharp decrease at the expert ratio \(20\%\), while SVR can still retain good or even expert performance (most spark in Hopper and Walker2d).

### Empirical study on the sampling distribution of SVR

In this section, we investigate how to practically select the sampling distribution \(u\) in SVR, which controls the relative strength of penalizing different OOD \(Q\)-values. In our implementation, \(u\) is the

   Dataset & BC & OneStep & TD3BC & AWAC & BCQ & BEAR & UWAC & CQL & IQL & SVR (Ours) \\  halfcheetah-m & 42.0 & 50.4 & 48.3 & 47.9 & 46.6 & 43.0 & 42.2 & 47.0 & 47.4 & **60.5\(\)1.2** \\ hopper-m & 56.2 & 87.5 & 59.3 & 59.8 & 59.4 & 51.8 & 50.9 & 53.0 & 66.2 & **103.5\(\)0.4** \\ walker2d-m & 71.0 & 84.8 & 83.7 & 83.1 & 71.8 & -0.2 & 75.4 & 73.3 & 78.3 & **92.4\(\)1.2** \\ halfcheetah-m-r & 36.4 & 42.7 & 44.6 & 44.8 & 42.2 & 36.3 & 35.9 & 45.5 & 44.2 & **52.5\(\)3.0** \\ hopper-m-r & 21.8 & 98.5 & 60.9 & 69.8 & 60.9 & 52.2 & 25.3 & 88.7 & 94.7 & **103.7\(\)1.3** \\ walker2d-m-r & 24.9 & 61.7 & 81.8 & 78.1 & 57.0 & 7.0 & 23.6 & 81.8 & 73.8 & **95.6\(\)2.5** \\ halfcheetah-m-e & 59.6 & 75.1 & 90.7 & 64.9 & **95.4** & 46.0 & 42.7 & 75.6 & 86.7 & **94.2\(\)2.2** \\ hopper-m-e & 51.7 & 108.6 & 98.0 & 100.1 & 106.9 & 50.6 & 44.9 & 105.6 & 91.5 & **111.2\(\)0.9** \\ walker2d-m-e & 101.2 & **111.3** & **110.1** & **110.0** & 107.7 & 22.1 & 96.5 & 107.9 & **109.6** & **109.3\(\)0.2** \\ halfcheetah-e & 92.9 & 88.2 & **96.7** & 81.7 & 89.9 & 92.7 & 92.9 & **96.3** & 95.0 & **96.1\(\)0.7** \\ hopper-e & **110.9** & 106.9 & 107.8 & 109.5 & 109.0 & 54.6 & **110.5** & 96.5 & 109.4 & **111.1\(\)0.4** \\ walker2d-e & 107.7 & **110.7** & **110.2** & **110.1** & 106.3 & 106.6 & 108.4 & 108.5 & **109.9** & **110.0\(\)0.2** \\ halfcheetah-r & 2.6 & 2.3 & 11.0 & 6.1 & 2.2 & 2.3 & 2.3 & 17.5 & 13.1 & **27.2\(\)1.2** \\ hopper-r & 4.1 & 5.6 & 8.5 & 9.2 & 7.8 & 3.9 & 2.7 & 7.9 & 7.9 & **31.0\(\)0.3** \\ walker2d-r & 1.2 & 6.9 & 1.6 & 0.2 & 4.9 & **12.8** & 2.0 & 5.1 & 5.4 & 2.2\(\)1.5 \\  gym-v2 total & 784.2 & 1041.2 & 1013.2 & 975.6 & 968.0 & 581.7 & 756.2 & 1010.2 & 1033.1 & **1200.5** \\  pen-expert & 85.1 & 61.6 & 111.0 & 115.2 & 114.9 & 105.9 & 111.9 & 107.0 & 110.2 & **138.9\(\)9.2** \\ pen-human & 34.4 & **73.7** & 54.9 & 25.5 & 68.9 & -1.0 & 21.7 & 37.5 & 71.5 & **73.1\(\)12.1** \\ pen-cloned & 56.9 & 31.8 & 63.8 & 10.4 & 44.0 & 26.5 & 33.1 & 39.2 & 37.3 & **70.2\(\)17.4** \\  adroit-v0 total & 176.4 & 167.1 & 229.7 & 151.1 & 227.8 & 131.4 & 166.7 & 183.7 & 219.0 & **282.2** \\   

Table 1: Averaged normalized scores on the D4RL benchmarks over five random seeds. Note that m = medium, m-r = medium-replay, m-e = medium-expert, e = expert, r = random.

Gaussian with the same mean as the current policy. Here we vary its standard deviation \(\) and present the corresponding results in Fig. 4. We do not show the results for \(>0.7\) because such a large \(\) would cause most sampled actions to lie at the boundary of the action space. Instead, we use Uniform distribution to represent \(=\).

As shown in Fig. 4, SVR is able to converge to good performance over a very wide range of \(\). However, if \(\) is too small, SVR may not be able to adequately penalize all OOD \(Q\)-values and the IS ratio will have a large variance, thereby disrupting the learning process (see \(=0.1\) in hopper-m-r). Conversely, if \(\) is too large or if the uniform distribution is used, sampling from \(u\) may fail to emphasize the key areas where overestimation is most likely to occur (as indicated by the current policy), leading to insufficient mitigation of overestimation and inferior performance (see Uniform in hopper-m, hopper-m-r, walker2d-m-r and \(=0.7\) in hopper-m).

### Effects of components in SVR

Density estimator.We speculate that a more precise behavior density estimator may further improve SVR. So following previous works , we consider to replace the Gaussian density estimator \(_{}\) with the conditional variational auto-encoder . We refer to this variant as SVR-VAE. The results of SVR-VAE are shown in Fig. 5 (left). Overall, SVR-VAE has only marginal performance gains over SVR.

IS techniques.Self-Normalized Importance Sampling (SNIS)  is sometimes adopted to reduce the high variance of IS . The SNIS estimator has lower variance but is biased . Here we test an SVR variant SVR-SNIS, which normalizes the IS ratio across the batch. As shown in Fig. 5 (right), the perfor

Figure 4: Learning curves of SVR with different sampling distribution \(u\). \(\) is the standard deviation of Gaussian. SVR is able to converge to good performance under various sampling distributions.

Figure 5: Percentage performance difference of the SVR variant compared to the original algorithm. (Left) SVR with VAE density estimator. (Right) SVR with self-normalized importance sampling. HC = HalfCheetah, H = Hopper, W = Walker2d.

Figure 3: Evaluations of SVR (supported value regularization) vs CQL (density value regularization) on noisy datasets, which are made by mixing random and expert datasets with varying expert ratios.

mance of SVR-SNIS is worse in hopper-m and hopper-m-e, and slightly better in other tasks. This discrepancy may be due to the varying requirements for variance and bias across different tasks.

## 5 Related Work

Offline RL.In offline RL, extrapolation error and overestimation caused by OOD actions pose significant challenges . Among various solutions, value regularization methods aim to introduce conservatism in value estimation [23; 20; 29; 6; 3; 27], while policy constraint approaches enforce proximity between the trained policy and the behavior policy, either explicitly via divergence penalties [44; 22; 13; 8], implicitly by weighted behavior cloning [5; 36; 33; 42; 31], or directly through specific parameterization of the policy [10; 11; 53]. Another line of the methods, instead, opt for in-sample learning, which involves formulating the Bellman target with only the actions in the dataset [4; 28; 21; 51; 47]. However, the performance of most existing methods is largely confined by the average quality of the dataset . In contrast, SVR guarantees the convergence to the optimal support-constrained policy and achieve superior performance in sub-optimal datasets.

Value regularization.We divide the value regularization methods in offline RL into two categories: direct \(Q\) penalization and Bellman target reduction. The former involves adding penalties to the critic learning loss, trying to penalize OOD \(Q\)-values [23; 50; 46; 6; 27]. Due to the difficulty of determining the OOD region, they typically penalize \(Q\)-values under the current policy distribution [23; 3; 27; 48] and optionally incorporate a maximization term under the data distribution for milder pessimism [23; 50; 6]. Thus, existing works of this category belong to policy density based value regularization, rather than ID/OOD based. On the other hand, the Bellman target reduction methods either subtract an uncertainty quantifier from the Bellman target [14; 3] or directly use the minimum of \(Q\) ensembles  to compute the target [2; 9]. However, if not incorporating additional penalties, they do not provide learning signals on OOD \(Q\)-values . It empirically works because after the minimization over ensemble (or subtracting the uncertainty quantifier), the random generalized OOD \(Q\)-values will be smaller than ID ones with high probability, reducing the impact of OOD \(Q\)-values on learning. However, it requires a large number of \(Q\) networks and is much more computationally expensive .

Support-constrained optimality.In absence of additional information beyond the dataset, the optimal support-constrained policy represents the best policy that we can hope to obtain. Recent works have considered various policy constraints to learn it, but their performance still leaves considerable room for improvement. For example, BEAR  attempts to keep the learned policy within the support of the behavior policy by minimizing MMD between them. However, this choice lacks a theoretical guarantee, and Wu et al.  empirically found that MMD has no performance gain over KL. Other works adopt a specific parameterization of the policy to constrain it to the behavior support [10; 11]. They first pre-train a generative model (e.g., VAE) for the behavior policy. Then during learning, they sample several actions at each state and choose the one with the highest \(Q\)-value as the output of the policy, aiming to obtain the maximum within the behavior support. However, these methods are vulnerable to model errors  and have a high computational cost for generating sufficient actions for each state. Instead of working on policy constraints, SVR is the first value regularization algorithm to achieve support-constrained optimality. Moreover, SVR only needs to query the behavior density of in-dataset \((s,a)\) pairs, making it less susceptible to model errors.

## 6 Conclusions and Limitations

In this work, we propose a novel value regularization method, SVR, which penalizes the \(Q\)-values for all OOD actions while maintaining standard Bellman updates for ID ones. We show that the policy evaluation operator of SVR is a contraction, whose fixed point outputs unbiased \(Q\)-values in ID region and underestimated \(Q\)-values in OOD region. Furthermore, SVR guarantees strict policy improvement until convergence to the optimal support-constrained policy. Empirical results validate the theoretical properties of SVR and demonstrate its SoTA performance on the D4RL benchmarks.

One limitation of SVR lies in the need of pre-training the behavior model. An exciting direction for future work would be to achieve supported value regularization without explicit behavior policy estimation.