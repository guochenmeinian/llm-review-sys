# Reference-Based POMDPs

Edward Kim

School of Computing

Australian National University

Canberra, Australia

edward.kim@anu.edu.au

&Yohan Karunanayake

School of Computing

Australian National University

Canberra, Australia

yohan.karunanayake@anu.edu.au

&Hanna Kurniawati

School of Computing

Australian National University

Canberra, Australia

hanna.kurniawati@anu.edu.au

###### Abstract

Making good decisions in partially observable and non-deterministic scenarios is a crucial capability for robots. A Partially Observable Markov Decision Process (POMDP) is a general framework for the above problem. Despite advances in POMDP solving, problems with long planning horizons and evolving environments remain difficult to solve even by the best approximate solvers today. To alleviate this difficulty, we propose a slightly modified POMDP problem, called a Reference-Based POMDP, where the objective is to balance between maximizing the expected total reward and being close to a given reference (stochastic) policy. The optimal policy of a Reference-Based POMDP can be computed via iterative expectations using the given reference policy, thereby avoiding exhaustive enumeration of actions at each belief node of the search tree. We demonstrate theoretically that the standard POMDP under stochastic policies is related to the Reference-Based POMDP. To demonstrate the feasibility of exploiting the formulation, we present a basic algorithm RefSolver. Results from experiments on long-horizon navigation problems indicate that this basic algorithm substantially outperforms POMCP.

## 1 Introduction

Computing motion strategies that are robust to uncertainty is a fundamental problem in robotics. A robot often needs to strategically plan in an environment where it only has partial knowledge about itself and the state of the world due to actuator disturbances, limited knowledge about the environment, and imperfect sensory information. Such planning problems can generally be framed as a Partially Observable Markov Decision Process (POMDP) , which is a general mathematical framework for planning in partially observable and non-deterministic scenarios. POMDPs explicitly model partial observability using a probability distribution over the state space, called a _belief_, and compute the best strategy with respect to beliefs. Finding the exact solution to a POMDP problem is computationally intractable in the worst case  for two main reasons. Firstly, the size of the belief space grows exponentially with respect to the size of the state space. Secondly, the number of action-observation histories grows exponentially with respect to the planning horizon. These challenges are known as the _curse of dimensionality_ and the _curse of history_ respectively. To deal with these challenges, many practical solvers have been proposed in the past few decades . Sampling-based approximate solvers  have made good progress on both curses by employing particle filters to maintain a belief search tree and judiciously exploring a _subset_ of histories in this search tree.

Notwithstanding these advances, some partially observed problems, such as those involving long planning horizons and progressively evolving environments remain relatively difficult to solve by even the best approximate POMDP solvers today. In particular, all of the aforementioned solvers struggle with a planning horizon of more than 15 steps primarily because, to infer the best action, they rely on exhaustive enumeration of actions at each belief node in the search tree.

To alleviate the above difficulties, and hence further improve POMDP solving capabilities, we propose the idea of a _Reference-Based POMDP_. A Reference-Based POMDP is a modification of the standard POMDP where, in addition to maximising total rewards, an agent's objective function is penalised for deviating too far from a given stochastic reference policy. This approach leads to an interesting interpretation where the agent must trade off between two possibly competing objectives. The agent should respect the given initial stochastic policy unless deviating substantially from it leads to much higher rewards.

A key outcome of this formulation is that solving a Reference-Based POMDP does not require exhaustive enumeration of the actions at each belief node. Rather, optimisation can be performed analytically because: (i) the space of policies is relaxed to include _stochastic policies_; and (ii) the form of the penalty is represented as the KL divergence from the reference policy. Under suitable transformations, this leads to a linear Bellman backup (Theorem 3.1) where the sampling distribution is the reference policy itself. As the reference policy is known a priori, the above insight facilitates algorithms where the actions can be sampled (as opposed to enumerated) at each step which, in turn, can be efficiently computed using Monte Carlo methods.

This work can be viewed as an extension of a body of literature related to Linearly Solvable Optimal Control[24; 26; 27; 1; 19; 28; 3] in fully observable environments (see Section 7 for a brief summary) to partially observable ones. While it is true that a POMDP can be viewed as an MDP in the belief space , extending the aforementioned works to partially observable domains is non-trivial because of the infinite and continuous nature of the belief space and the difficulty of computing belief updates. To the authors' knowledge, the approaches outlined above have not yet been extended to partially observable domains.

The paper is organised as follows. Section 2 recounts the basic setup of a POMDP. Section 3 introduces the Reference-Based POMDP and outlines the relevant theory. Section 4 demonstrates that the standard POMDP can be related to the Reference-Based POMDP via an embedding. Section 5 presents RefSolver a preliminary approximate solver for the Reference-Based POMDP. Experimental results on long-horizon 2D and 3D navigation problems are presented in Section 6 and indicate that our solver can be employed to substantially outperform POMCP . Note that the problems we have presented, are not trivial due to the long planning horizon and the challenges of partial observability and transition uncertainty. Finally, we outline the relevant literature in Section 7 and summarise the contributions, limitations and future directions of the paper in Section 8.

## 2 POMDP Preliminaries

In this paper, we focus on an infinite-horizon formulation of the POMDP. Formally, a standard POMDP is completely specified by a tuple \(,,,,,R,\). Here, \(\) denotes the set of all possible states of the agent and its environment, \(\) denotes the set of all possible actions the agent can perform, and \(\) denotes all the possible observations that the agent can perceive. Although our transformation could be applied to POMDPs with continuous state, action, and observation spaces, for simplicity, in this paper, we will assume that \(\), \(\) and \(\) are all discrete.

At each time step, the agent occupies a state \(s\) and executes an action \(a\), after which its state transitions to a new state \(s^{}\). The outcome of this execution is non-deterministic, and this uncertainty is represented in the Markovian transition function \(\) as a conditional probability function \((s^{}\,|\,s,a)\). At the new state \(s^{}\), the agent perceives an observation \(o\). Uncertainty in the observation perceived - having undertaken action \(a\) and transitioned to state \(s^{}\) - is described by the conditional probability function \((o\,|\,s^{},a)\), referred to as the observation function. The agent's action incurs a reward, which is defined by a bounded real-valued function \(R:\). The parameter \((0,1)\) represents the discount factor.

As the agent does not know its true state, at each time step, the agent maintains a _belief_ about its state. A belief, denoted as \(b\), is a probability distribution over the state space \(\). The set of all possible beliefs forms a belief space of \((||-1)\)-dimensional simplex, which we denote as \(\). The belief the agent maintains is updated via Bayesian inference using the transition and observation models \(\) and \(\), respectively. The exact belief update is completely determined by an action-observation pair. Formally, if \(b^{}=(b,a,o)\) denotes the agent's next belief after taking action \(a\) and perceiving observation \(o\), then \(b^{}\) is defined as:

\[b^{}(s^{})=\,(o\,|\,s^{},a)_{s }(s^{}\,|\,s,a)\,b(s)\] (1)

where \(\) is a normalising factor. The solution to a POMDP problem is an optimal policy \(^{*}:\) that maximises the value function, defined as:

\[V^{*}(b)=_{a}R(b,a)+_{o}P(o\, |\,a,b)V^{*}(b,a,o)\] (2)

where

\[R(b,a):=_{s}R(s,a)\,b(s)\] (3)

is the expected reward under the belief \(b\). The notation \(P(o\,|\,a,b)\) is the probability the agent perceives an observation \(o\) having performed the action \(a\) under the belief \(b\), and is defined as

\[P(o\,|\,a,b):=_{s^{}}(o\,|\,s^{},a) _{s}(s^{}\,|\,s,a)b(s).\] (4)

Computing the exact solution \(^{*}\) and optimal value function \(V^{*}\) is intractable [14; 15; 17] in general, and therefore state-of-the-art solvers approximate solutions via sampling .

## 3 Reference-Based POMDP

We now extend the analytical approach for the Linearly Solvable MDP (see Appendix Section A.1) to the POMDP. To this end, we define a _Reference-Based POMDP_, which is specified by the tuple \(,,,,,R,, U\). Similar to the standard POMDP, \(\), \(\), and \(\) refer to the state, action, and observation spaces respectively. We denote by \(\), \(\), and \(R\) the observation, transition, and reward functions, respectively, while \((0,1)\) denotes the discount factor.

**Assumption 3.1**.: _A reference belief-to-belief transition probability:_

\[(,\,|\,b)()\] (5)

_is given for all \(b\)._

This reference transition is assumed by the problem and could be constructed in multiple ways. At the most abstract the level this could be given in a top-down fashion (e.g. an offline policy that has been generated for a similar version of a POMDP problem). On the other hand, it could also be constructed from the ground up. For example, one could take a fully observed policy \(^{}:\) and infer a reference stochastic action \((\,|\,b)\) according to

\[(a\,|\,b):=_{s}I_{\{^{ }(s)=a\}}b(s)+(1-)\,(a\,|\,b)\] (6)

where \(\) is some default distribution for sampling actions and the parameter \((0,1)\) indicates the saturation level of the fully observed policy. The exact choice of fully observed policy depends on the context. Examples might include: an MDP policy for the fully observed version of the problem, a policy induced by feasible paths generated by motion planners, a policy induced by all source shortest path in the case of a navigation problem, and even a policy representing the passive dynamics of a system at any given state. Regardless of how the reference stochastic action is constructed, in a ground-up construction, the reference belief-to-belief transition probability will take on the more specific form:

\[(a,o\,|\,b):=P(o\,|\,a,b)\,(a\,|\,b)\] (7)

where \(P(o\,|\,a,b)\) is given by (4).

Inspired by Linearly-Solvable MDPs (Section A.1), a Reference-Based POMDP asserts that, for any belief \(b\), we can realise "belief-to-belief" transition probabilities \(U(,\,|\,b)()\) of our choosing. Note that this is an _ideal_ transition probability which may not actually be realisable in practice even by a stochastic policy (see Section 3.3 and Section 8 for further commentary). To prohibit arbitrary transitions, at any belief \(b\), we will further restrict our choice of distribution \(U\) to one that satisfies the property

\[(a,o\,|\,b)=0 U(a,o\,|\,b)=0 a,o .\]

Denote the class of such distributions by \((b)()\) which will form our admissible reference transition probabilities (which can be viewed as admissible controls) from a given belief. Then, \(:=_{b}(b)\) is the class of all admissible reference transition probabilities. Similar to (25), we penalise choices of \(U(,\,|\,b)\) that represent large deviations from \((,\,|\,b)\) using KL divergence: \(U(,\,|\,b)\,\,(,\,| \,b)\).

Now, the above reference transition probability affects the definition of the reward function too. At the most abstract level, the reward function \(R:\) is parameterised by belief-action pairs. Hence, the expected immediate cost of taking a stochastic belief-to-belief transition governed by \(U(,\,|\,b)(b)\) is given by

\[R(b,U):=_{a,o}R(b,a)U(a,o\,|\,b)\] (8)

where again \(R(b,a):=_{s}}R(s,a)b(s)\). The solution to a Reference-Based POMDP problem is then an optimal stochastic policy \(^{*}:\) that maximises:

\[^{*}(b)=_{U(,\,|\,b)(b)} R(b,U)-U(,\,|\,b)\,\|\,(,\,|\,b)\\ +\,_{a,o}U(a,o\,|\,b)^{*}(b,a,o) .\] (9)

### Existence and Uniqueness of the Solution

One might ask if the reformulation (9) is satisfied uniquely by some function \(:\). The following lemma ensures that there exists a unique (up to \(\|\|_{}\)-equivalence) bounded function \(:\) satisfying the Bellman equation. Its proof is deferred to Appendix A.2.

**Lemma 3.1**.: _Consider the Banach space \(()\) of bounded functions \(:\) equipped with the supremum norm \(\|\|_{}\). Let \(:()()\) be a self-mapping defined by the Reference-Based POMDP backup operator described by equation (9). Then, for \((0,1)\), the mapping \(\) is a contraction and_

\[\|_{1}-_{2}\|_{}\,\| _{1}-_{2}\|_{}.\] (10)

Hence, by the Banach fixed point theorem , the solution to the Bellman equation exists uniquely in this space whenever \((0,1)\).

### Transforming the Value Function

Here, we show that under an appropriate transformation, the value function of a Reference-Based POMDP (9) can also be linearised. Its proof is given in Appendix A.3.

**Theorem 3.1**.: _Define the transformation \((b):=e^{(b)}\) for any \(b\). The value function (9) of the Reference-Based POMDP \(,,,,,R,, \) is equivalent to_

\[(b)=_{a,o}(a,o\,|\,b)e^{R(b,a)}^{} (b,a,o).\] (11)

_The solution \(^{*}\) to equation (11) exists and is unique. And, the optimal stochastic "belief-to-belief" transition to the Bellman equation (9) is given by_

\[U^{*}(a,o\,|\,b)=(a,o\,|\,b)e^{R(b,a)}^{*\,} (b,a,o)}{[^{*\,}](b)}\] (12)

_where \([^{*\,}](b):=_{,}(, \,|\,b)e^{R(b,)}^{*\,}(b,, )\) is a normaliser._Intuitively, the solution \(^{*}\) represents the desirability of a given belief (or history of action-observation sequences) where, (12) represents distorting the reference dynamics to higher desirability scores. Moreover, Theorem 3.1 shows that we can converge to \(^{*}\) by iterating the transformed Reference-Based POMDP backup operator (11). Lemma 3.1 indicates that the speed of convergence depends on the size of \(\) with faster convergence as \(\) tends to \(0\). Moreover, Theorem 3.1 also implies that once \(^{*}\) is computed, \(U^{*}\) can be computed _exactly_ via (12).

Unlike the linearly solvable MDP, exactly computing \(^{*}\) is still intractable in practice due to the continuity of the belief space and the cost of belief estimation. However, if we leverage the advances made by sampling-based POMDP solvers and assume that the belief space is sufficiently represented by a finite number of beliefs, then this quantity can be approximated. Of course, finding such a set of beliefs is in itself difficult. Nevertheless, the fact that the optimal value function of a Reference-Based POMDP is equivalent to (11) implies that the optimal value function of Reference-Based POMDP can be computed by purely computing expectations under the reference transition probabilities recursively, which can be approximated efficiently (e.g. via Monte Carlo approximation).

### Recovering the Stochastic Actions

While restricting the agent's transition probabilities to \((b)\) limits the agent's level of control, \(U^{*}\) as given in (12) is still an _ideal_ transition probability and may therefore not be realisable under a stochastic action in general. In other words, one cannot guarantee the existence of a stochastic action \((a\,|\,b)\) such that \(U(a,o\,|\,b):=P(o\,|\,a,b)\,(a\,|\,b)=U^{*}(a,o\,|\,b)\) for every \(a\) and \(o\).

Nevertheless, we can still find a \((a\,|\,b)\) such that \(U\) and \(U^{*}\) are close in some sense. A suitable way to formulate the problem is to minimise the KL divergence between \(U^{*}\) and \(U\). That is, for each \(b\), we minimise

\[_{a,o}P(o\,|\,a,b)\,(a\,|\,b)(a,o\,|\,b)}\] (13)

over all stochastic actions \((\,|\,b)()\). Solving the constrained minimisation problem using Lagrange multipliers yields the following result. See Appendix A.4 for its proof.

**Proposition 3.1**.: _The minimiser of (13) is_

\[^{*}(a\,|\,b)=}[ -(\,|\,b)]} a\] (14)

_where_

\[(a\,|\,b):=_{o}P(o\,|\,a,b)(a,o\,| \,b)}.\] (15)

## 4 Relating the Reference-Based POMDP and the Standard POMDP

In A.1, we recount a procedure for embedding the standard MDP \(,,,R,\) inside the reformulated MDP \(,p,,\). The key advantage of this is to apply the more efficient machinery of the reformulation to quickly determine good approximate solutions to the standard MDP. Furthermore, it establishes a theoretical basis to relate the two formulations. Here, we present an analogous embedding for the POMDP. Proofs are deferred to Section A for conciseness.

Suppose an initial belief \(b_{0}\) is given and let \(_{b_{0}}\) be the set of all reachable beliefs from \(b_{0}\). Let \((_{b_{0}})\) denote the set of all bounded functions from \(_{b_{0}}\) to \(\). Consider the family of Reference-Based POMDPs with purely belief-dependent rewards \((_{b_{0}})\).

\[\{,,,,,, ,U\}_{(_{b_{0}}),}\] (16)

and a standard POMDP of the form

\[,,,,,R,.\] (17)

For fixed \((,)(_{b_{0}})\), their respective Bellman equations are:

\[(b)=_{U(,\,\,|\,b)(b)} (b)-U(,\,|\,b)\,\|\,(,\,|\,b)\\ +\,_{a,o}U(a,o\,|\,b)(b,a,o) \] (18)\[V(b)=_{a}R(b,a)+_{o}P(o\,|\,a,b)V (b,a,o)\] (19)

for all \(b_{b_{0}}\).1

**Definition 4.1**.: _Let \((_{b_{0}})\) be the set of all bounded functions from \(_{b_{0}}\) to \(\). We say that the pair \((,)(_{b_{0}})\) is an embedding of the standard POMDP (17) in the Reference-Based POMDP (16) with purely belief-dependent rewards if, for each \(\) and \(b\) there exists a \(U^{}(,\,|\,b)\) such that:_

\[_{a}U^{}(a,o\,|\,b)=P(o\,|\,,b)\] (20)

_and_

\[R(b,)=(b)-(U^{}(,\,|\,b)\,\|\,( ,\,|\,b)).\] (21)

**Proposition 4.1**.: _Suppose \((_{b_{0}})\) is such that_

\[(b)=_{}e^{R(b,)}\] (22)

_holds where \(R(b,):=_{s}R(s,)b(s)\). Then, if_

\[(a,o\,|\,b):=P(o\,|\,a,b)}{_{ }e^{R(b,)}} b\] (23)

_the pair \((,)(_{b_{0}})\) is an embedding of the standard POMDP in the Reference-Based POMDP with purely belief-dependent rewards._

## 5 An Online Planning Algorithm

We have developed a preliminary on-line solver RefSolver for a Reference-Based POMDP to demonstrate some of the advantages of the formulation presented in Section 3. Pseudo code for an implemented algorithm where the reference policy is constructed using a fully-observed policy \(^{}\) is presented in the Appendix Section A.6 Algorithm 1. Note that although our description of RefSolver is very specific in terms of the reference policy and reference stochastic action, they can readily be replaced with other types of policies and induced reference stochastic actions.

Similar to many state-of-the-art on-line POMDP solvers (e.g., [21; 29]), RefSolver constructs a search tree of action-observation histories and uses particles to represent the beliefs. The key insight of RefSolver, however, is to exploit the additional information afforded by the fully observed policy to limit exploring every possible action at each history node. Instead, as each backup of the transformed value (9) is essentially an expectation under the reference policy, actions are sampled using the reference policy and the backup is estimated via a straightforward Monte Carlo method. This allows sparser sampling and simulations to resemble a depth-first search traversing to a pre-specified depth before rolling out the leaf nodes using the fully observed policy. Hence, rather than expanding all the actions as POMCP , ABT and DESPOT  do, RefSolver constructs a deeper tree thereby exploiting more information about the long-term behaviour of the system.

To allow for RefSolver to also search forward for actions that are not supported by the fully observed policy, some additional noise \(\) can be added according to (6) so that \((1-)\) can be interpreted as an exploration constant. In our implementation the noise \(\) is a uniform distribution over the actions. The parameter \((1-)\) can thus be interpreted as RefSolver's belief that the fully observed policy can be trusted.

## 6 Experiments

We tested RefSolver against the implementation of POMCP  in the pomdp_py library  in two different \(60 60\) long-horizon grid-world navigation problems. As RefSolver is written in Python, the implementation of POMCP was de-Cythonised for a fair comparison.

We did not compare with DESPOT  because the problem scenarios require long horizons and the planning horizon that DESPOT can perform is much lower than POMCP because of its requirement to construct a tree of size \(||^{d}K^{d}\) where \(d\) is the depth of the DESPOT (this difficulty has also been observed in ). Given the long horizon nature of the problems, one may question comparison with solvers specifically designed for long horizon problems (as summarised in ). Most, if not all, of such solvers rely on reducing the effective planning horizon by constructing macro actions or sub-policies. Since RefSolver does not change the overall structure of action and observation spaces and transition and observation functions, it is trivial to apply most techniques that reduce the effective planning horizon to Reference-Based POMDP solvers (including RefSolver), which in turn will further improve the performance of RefSolver too.

All experiments were performed on a desktop computer with an 8 Core Intel Xeon Silver 4110 Processor and 128GB DDR4 RAM.

### Problem Scenarios and Testing Parameters

Figure 1: Illustration of testing scenarios. (a) Navigation1: landmarks (pink), goal states (green), obstacles (black), danger zones (yellow), initial belief (light blue). (b) and (c) Navigation2: landmarks (pink), goal states (green), obstacles that are present in both the initial map and the modified environment (black), additional obstacles that were not known in the initial map (grey), initial belief (blue). The map in (b) represents the environment used to generate the A* policy. The map in (c) represents the environment used for planning and execution. Navigation3: 3D navigation scenario with uneven terrain, goal area (red).

#### 6.1.1 Navigation1: Long Horizon

The environment is a \(60 60\) static gridworld populated with obstacles, danger zones, and landmarks, as illustrated in Figure 0(a). Details of the actions, observations and reward are presented in Section A.7.1.

POMCP's rollout policy and RefSolver's fully observed policy are the A* policy for the fully observed and deterministic version of the problem. POMCP was run with an exploration constant of 300 and maximum depth of 180 - this maximum depth is the upper bound for (tree depth + rollout steps). For RefSolver, the maximum tree depth was 90, the maximum rollout depth was 180 and \(=0.5\). Both methods were executed for a maximum of 180 steps.

#### 6.1.2 Navigation2: Slightly Perturbed Environment

The environment is a \(60 60\) gridworld populated with obstacles and landmarks. The robot is provided with an initial environment map that was slightly different from the environment used during execution. Details of the scenario (actions, observations, and reward) are presented in Section A.7.2.

For testing purposes, we generated 64 different initial maps with randomly placed obstacles and landmarks. For each initial map, we randomly generate four slight modifications of the initial map by adding obstacles at random. An illustration of an initial environment map and a corresponding environment used during execution are illustrated in Figure 0(b) and 0(c).

Both the POMCP rollout policy and RefSolver's fully observed policy are the A* policy for the fully observed and deterministic version of the robot operating in the _initial_ environment map. However, the dynamics, observation, and reward that the robot experiences and receives during execution follow the deformed map, which is different from the initial map.

POMCP was run with an exploration constant of 300 and maximum depth of 60 - this maximum depth is the upper bound for (tree depth + rollout steps). For RefSolver, the maximum tree depth was 30, the maximum rollout depth was 60 and \(=0.5\).

#### 6.1.3 Navigation3: 3D Navigation Environment

Navigation3 is a 3D environment where a drone has to fly over uneven terrain. The drone starts at the location illustrated in Figure 0(d) and needs to reach the red goal region, which is located behind the hill. State and observation spaces are continuous, while actions are discrete. See Appendix Section A.7.3 for details.

The rollout and fully observed policy recommend the action that minimises the Euclidean distance between the current state and goal location with no consideration of obstructing terrain. POMCP was run with an exploration constant of 400 and a maximum depth of 150 (upper bound for tree depth + rollout depth). For RefSolver, the maximum tree depth was 150, the maximum rollout depth was 200, and \(=0.4\). Both methods were executed for a maximum of 200 steps.

### Performance Comparison

To evaluate the performance on RefSolver, we ran each method on each problem scenario 256 times. For each scenario, each method was given 30 seconds planning time per step. The initial policy generation used by POMCP for rollout and by RefSolverfor the 2D experiments took an average of \(45\) minutes for each scenario. We only need to compute this initial policy once for Navigation1 and as many as the number of initial maps for Navigation2.

For Navigation3, the initial policy is generated on-line and takes a small fraction of the planning time per step. The initial policy is derived from a simple shortest path distance policy that assumes the environment is totally empty, allowing it to be computed fast. Note that this way of generating the initial policy substantially alleviates the

Figure 2: The red line indicates the shortest path from the initial to the goal positions, while the blue line indicates a trajectory trace of executing the policy generated by RefSolver.

computational cost, especially for complex problems. The quality of such an initial policy is of course relatively low, as indicated by the red-line in Figure 2. However, our results indicate that RefSolver is able to iteratively deform the initial policy to generate a good strategy for the environment the robot operates in. Note that the trace of the deformed policy indicates that the agent is aware of the potential collision danger quite early, even though it requires more than 100 steps lookahead.

Table 1 compares the success rates and the average total discounted reward (with a 95% confidence interval) achieved by POMCP and RefSolver, respectively. To better understand the performance improvement with respect to the planning time, we also ran Navigation2 with different planning times per step. The results of 256 runs are presented in Table 2. The results indicate that in all instances and for all planning times, the basic implementation of RefSolver was able to reach the goal with a significantly greater likelihood and average total discounted reward.

One may be discouraged by the success rate presented. However, all the problem scenarios presented here have a horizon of over 60 steps, which remains a tremendous challenge for many general approximate on-line POMDP solvers today, as indicated by the performance of POMCP.

## 7 Related Work

### MDPs with KL Divergence Minimisation and Reinforcement Learning

The idea of using KL divergence minimisation in fully observable environments goes back to a series of works on _Linearly Solvable MDPs2_[24; 25; 26; 2] who used it to efficiently approximate solutions to certain classes of (fully-controllable) MDPs. A learning-based algorithm (_Z-Learning_) was also proposed for problems where the model was not available . Azar et al.  extended the approach to general MDPs with stochastic policies and introduced _Dynamic Policy Programming_ which converges asymptotically to the optimal (stochastic) policy of the MDP. The authors also demonstrated that the formulation could be suitably adapted using basis functions to deal with large-scale (continuous) state-action problems. A unifying perspective on this approach was then offered by Rawlik et al.  who demonstrated that the stochastic control formulation can be related to approximate inference control in trajectory optimisation  and provided further experimental validation for a \(Z\)-Learning-like algorithm. Finally, we note that the literature on KL divergence minimisation is closely related to Maximum Entropy RL where some recent theoretical developments  have demonstrated robustness to the model (i.e. dynamics and rewards) in fully observable domains.

    &  &  &  \\  Solver & \% Succ. &  Avg Tot \\ Dis Rw \\  & \% Succ. &  Avg Tot \\ Dis Rw \\  & \% Succ. &  Avg Tot \\ Dis Rw \\  & \% Succ. & 
 Avg Tot \\ Dis Rw \\  \\  POMCP & 1 & \(-284\ 5\) & 11 & \(-181 13\) & 0 & \(-87\ 0\) \\ RefSolver & 31 & \(-201 24\) & 49 & \(-95 16\) & 64 & \(-5 21\) \\   

Table 1: POMCP vs RefSolver with 30 sec planning time per step

    &  \\   &  &  &  \\  Solver & \% Succ. &  Avg Tot \\ Dis Rw \\  & \% Succ. &  Avg Tot \\ Dis Rw \\  & \% Succ. &  Avg Tot \\ Dis Rw \\  & \% Succ. & 
 Avg Tot \\ Dis Rw \\  \\  POMCP & 2 & \(-205\ 7\) & 1 & \(-208\ 4\) & 11 & \(-181 13\) \\ RefSolver & 30 & \(-137 15\) & 36 & \(-128 15\) & 49 & \(-95 16\) \\   

Table 2: POMCP vs RefSolver in Navigation2 with different planning time per step.

As far as the authors are aware, all of the above papers have no parallel in partially-observable environments as of yet. As such, this paper can be seen as first step to extending the above results to such environments.

### Sampling-Based Approximate POMDP Solvers

Methods for finding exact solutions to POMDPs  are impractical for realistic robotics problems due to their computational complexity. In recent decades, sampling-based approximate methods have significantly scaled up the capabilities of both online and offline POMDP solvers [18; 11; 21; 29; 12] (see  for a comprehensive survey). A major constraint in all state-of-the-art methods is that they rely on a dynamic programming paradigm which necessitates enumeration of actions at every belief node of the search tree. As such, existing solvers cannot effectively deal with the curse of history because they do not have sufficient computational resources to perform a sufficiently long look-ahead.

To deal with this problem, methods that abstract the problem using _macro-actions_ have been proposed [23; 5; 13; 10] where the agent only plans with respect to judiciously chosen _sequences_ of actions, called macro actions, or sub-policies. They reduce the effective planning horizon, but can be sensitive to the class of macro-actions or sub-policy chosen. In contrast, we propose to alleviate the issue via a slight reformulation of the POMDP, which allows numerical computation to infer a close to optimal policy via sparse sampling of both the action and observation spaces. Moreover, existing techniques that reduce the effective planning horizon can also be applied to Reference-Based POMDPs, which will further improve its ability to compute good solutions to POMDPs with long planning horizons.

## 8 Discussion and Further Work

We have introduced the Reference-Based POMDP, which is a modification of the standard POMDP. This modified formulation enables the problem of computing the optimal policy to be analytically transformed into a problem of computing expectations, which can be approximated without exhaustive enumeration of the actions. We showed that, under mild assumptions, the standard POMDP can be related to a Reference-Based POMDP via an embedding (Proposition 4.1). We also presented a preliminary solver RefSolver which solves a Reference-Based POMDP. Initial performance comparisons on 2D and 3D navigation problems indicate that this preliminary method is promising and can be employed to substantially outperform state-of-the-art solvers in long-horizon problems.

By introducing KL minimisation with respect to reference probabilities, Reference-Based POMDPs are likely to be suitable for problems with dynamically changing POMDP models with long planning horizons where one needs to deform the policy being followed without making abrupt changes to the policy. We also see applications of this kind of framework in the context of artificial agents that are guided by a human to make responsible decisions while also judiciously transgressing guidance in the case of significant human error.

One limitation of this paper is that the formulation of a Reference-Based POMDP asserts that arbitrarly "belief-to-belief" transitions can be realised which is a remnant of following the approach set out in Todorov  too closely.3 This assumption can be relaxed by stating the Bellman equation with respect to stochastic policies over _actions_ in a manner similar to Azar et al.  rather than abstract _belief-to-belief_ transitions. Such an approach yields very similar analytical results and avoids the need to recover approximate stochastic actions as set out in Section 3.3. We expect to set out these results in a subsequent paper.

The above limitation is just one of the many possible directions for expanding this work. We hope this paper opens new avenues to further scale the capability of POMDP solvers.

## 9 Acknowledgements

This work is done in collaboration with Safran Electronics & Defense Australia Pty Ltd and Safran Group under the ARC Linkage project LP200301612.