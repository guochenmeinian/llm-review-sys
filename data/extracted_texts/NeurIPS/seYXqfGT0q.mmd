# Prototypical Hash Encoding for

On-the-Fly Fine-Grained Category Discovery

 Haiyang Zheng\({}^{1}\)1, Nan Pu\({}^{1}\)1, Wenjing Li\({}^{2}\)2, Nicu Sebe\({}^{1}\), Zhun Zhong\({}^{2}\)2

\({}^{1}\)University of Trento \({}^{2}\)Hefei University of Technology

Equal contribution.Corresponding authors.

###### Abstract

In this paper, we study a practical yet challenging task, On-the-fly Category Discovery (OCD), aiming to online discover the newly-coming stream data that belong to both known and unknown classes, by leveraging only known category knowledge contained in labeled data. Previous OCD methods employ the hash-based technique to represent old/new categories by hash codes for instance-wise inference. However, directly mapping features into low-dimensional hash space not only inevitably damages the ability to distinguish classes and but also causes "high sensitivity" issue, especially for fine-grained classes, leading to inferior performance. To address these issues, we propose a novel Prototypical Hash Encoding (PHE) framework consisting of Category-aware Prototype Generation (CPG) and Discriminative Category Encoding (DCE) to mitigate the sensitivity of hash code while preserving rich discriminative information contained in high-dimension feature space, in a two-stage projection fashion. CPG enables the model to fully capture the intra-category diversity by representing each category with multiple prototypes. DCE boosts the discrimination ability of hash code with the guidance of the generated category prototypes and the constraint of minimum separation distance. By jointly optimizing CPG and DCE, we demonstrate that these two components are mutually beneficial towards an effective OCD. Extensive experiments show the significant superiority of our PHE over previous methods, _e.g.,_ obtaining an improvement of +5.3% in ALL ACC averaged on all datasets. Moreover, due to the nature of the interpretable prototypes, we visually analyze the underlying mechanism of how PHE helps group certain samples into either known or unknown categories. Code is available at https://github.com/HaiyangZheng/PHE.

## 1 Introduction

While deep learning based machines have surpassed humans in visual recognition tasks , their capability is often limited to providing closed-set answers, _e.g._, category names. In contrast, humans possess the ability to recognize novel categories upon first observation without knowing their category names. To bridge this gap, Novel/Generalized Category Discovery (NCD/GCD) techniques  are proposed to transfer knowledge from known categories to distinguish unseen ones. However, current NCD/GCD methods operate under an offline inference paradigm where the category discovery is often implemented by applying clustering / unsupervised classification algorithms on a pre-collected batch of query data that needs to be discovered. This severely limits the practicability of NCD/GCD techniques in real-world applications, where the systems are expected to provide online feedback for every newly-coming instance.

To tackle this drawback, Du et al. introduce the On-the-fly Category Discovery (OCD) task , which removes the assumption of a predefined query set and requires instance feedback with stream data input. OCD poses two primary challenges: **1)** The requirement for real-time feedback is

[MISSING_PAGE_EMPTY:2]

Related Works

**Novel Category Discovery** (NCD), initially introduced by DTC , aims to categorize unlabeled novel classes by transferring knowledge from labeled known classes. However, existing NCD methods [5; 6; 7] assume that all unlabeled data exclusively belong to novel classes. Generalized Category Discovery (GCD) is proposed in , allowing unlabeled data to be sampled from both novel and known classes. While existing NCD/GCD methods [6; 7; 8; 9; 13; 15; 16] have shown promising results, two key assumptions still impede their real-world application. Firstly, these models heavily rely on a predetermined query set (the unlabeled dataset) during training, limiting their ability to handle truly novel samples and hindering generalization. Secondly, the offline batch processing of the query set during inference makes these models impractical for online scenarios where data emerges continuously and the model requires instant feedback. To address these limitations, Du et al. introduced the On-the-Fly Category Discovery (OCD) , which removes the assumption of a predefined query set and requires instance feedback with stream data input. They proposed the SMILE method, which identifies the category of each instance by the signature of its representation (a hash-form code). The comparison among different settings is shown in Table 1.

In this paper, we address the "high sensitivity" issue associated with hash-based category descriptors in SMILE, particularly in fine-grained scenarios. We encode category prototypes into hash centers, ensuring maximal separation between these centers. Additionally, we employ Hamming balls centered on these hash centers to represent each category, effectively mitigating the "high sensitivity" issue.

**Deep Hashing** is a popular method for large-scale image retrieval. It uses deep neural networks to learn a hash function that converts samples into fixed-length binary codes, ensuring similar samples share similar codes. Early methods, such as HashNet , DPSH , and DSH , optimize hash functions based on pairwise similarities or triplet-based similarities. Both approaches suffer from low training efficiency and insufficient data distribution coverage. By defining points as hash centers that are sufficiently spaced apart, methods like DPN , OrthoHash , and CSQ  largely enhance training efficiency and retrieval accuracy. However, in the worst case, hash centers derived from these methods can be arbitrarily close. To address this, Wang et al.  introduce the Gilbert-Varshamov bound from coding theory to ensure a large minimal distance between hash centers.

Drawing inspiration from deep hash methods, we employ a hash center-based approach for category encoding. Unlike deep hashing methods used in image retrieval tasks [22; 23], which rely on predefined center points, our method generates category-specific hash centers directly from category prototypes. This adaptation is particularly suitable for the category discovery task. Furthermore, we utilize Hamming balls centered on these hash centers to represent categories, effectively mitigating the "high sensitivity" issue associated with using hash-form descriptors in category discovery tasks.

**Prototype-based Interpretable Models.** Chen et al.  introduce the Prototypical Part Network (ProtoPNet), a model designed for Interpretable classification. ProtoPNet features a set number of prototypical parts per class, enabling clear decision-making process. In addition, ProtoPNet offers a post-hoc analysis, in which it explains decisions for individual images by displaying all prototypes alongside their weighted similarity scores. This method employs multiple prototypes to represent a category, effectively distinguishing subtle differences between categories and demonstrating strong performance in fine-grained classification. Numerous subsequent studies have adapted ProtoPNet for various applications such as medical image processing and explanatory debugging, among others [25; 26; 27; 28; 29]. Later, ProtoPFormer  further integrates the Vision Transformer as a backbone, utilizing both global and part prototypes for interpretable image classification.

In this paper, we utilize prototype-based models for representation learning and prototype acquisition. Unlike existing methods predominantly tailored for closed-set classification, our approach extends to the generation of discriminative hash codes from the learned category prototypes. This extension allows for broader applicability in handling new and unseen categories, _i.e._, open-set scenarios.

   &  &  \\   & Old Cls & New Cls & Require \#Cls & Old Cls & New Cls & Require \# Cls & Online \\  NCD & ✓ & ✓ & Old+New\({}^{*}\) & ✗ & ✓ & Old+New\({}^{*}\) & ✗ \\  GCD & ✓ & ✓ & Old+New\({}^{*}\) & ✓ & ✓ & Old+New\({}^{*}\) & ✗ \\  OCD & ✓ & ✗ & Old & ✓ & ✓ & Old & ✓ \\  

Table 1: Comparison between different category discovery settings. \({}^{*}\) indicates that the number of new classes (Cls) is set as the ground-truth or previously estimated.

## 3 Prototypical Hash Encoding

**Problem Setup.** The setting of OCD is defined as follows. We are provided with a support set, denoted as \(_{S}=\{(_{i},y_{i}^{s})\}_{i=1}^{M} _{S}\) for training, and a query set, denoted as \(_{Q}=\{(_{i},y_{i}^{q})\}_{i=1}^{N} _{Q}\) for testing. Here, \(M\) and \(N\) represent the number of samples in \(_{S}\) and \(_{Q}\), respectively. \(_{S}\) and \(_{Q}\) are the label spaces for the support set and query set, respectively, where \(_{S}_{Q}\). We define classes in \(_{S}\) as known/old classes and classes in \(_{Q}/_{S}\) as unknown/new classes. Only the support set \(_{S}\) is used for model training. During testing, \(_{Q}\) includes samples from both known and unknown categories, which are inferred one by one, allowing for instant/online feedback.

**Framework Overview**. To achieve accurate and online category discovery, we design a Prototypical Hash Encoding (PHE) framework, which mainly consists of a Category-aware Prototype Generation (CPG) module and a Discriminative Hash Encoding (DHE) module, as illustrated in Fig. 2. CPG aims at modeling diverse intra-category information and generating category-specific prototypes for representing fine-grained categories. DHE leverages generated prototypical hash centers to further facilitate discriminative hash code generation. Finally, based on a theoretically derived bounding of the Hamming ball, we can determine the under-discovered category of instance and acquire instant feedback.

### Category-aware Prototype Generation

SMILE  directly utilizes instance-level supervised contrastive learning on low-dimensional hash features for simultaneous representation learning and category encoding. This approach may result in inadequate representation learning, as low-dimensional features struggle to capture complex data structures and patterns, especially in challenging fine-grained scenarios (see Table 10).

To solve this issue, we choose to perform representation learning upon the high-dimensional features instead of the low-dimensional hash features. Specifically, given a batch of input images, \(\), the image features are denoted as \(=_{f}(())^{N L}\), where \(\) represents the backbone, \(_{f}\) denotes a linear head, and \(\) represents the feature dimension. We use a prototype layer \(g_{}\) to transform \(\) into a similarity score vector \(^{m}\), with \(g_{}\) containing \(m\) learnable prototypes \(\{_{1},_{2},,_{m}\}\). In the design of the prototype layer, CNN-based methods  typically utilize the maximum pooled value from the similarity map, which is calculated between the feature map and the prototype, as the similarity score. In this paper, to align with the Vision Transformer (ViT)  backbone commonly-used in OCD, we propose to employ its _cls token_ to compute the prototype similarity score, inspired by ProtoFormer . The similarity score \(_{ij}\) for the \(i\)-th sample to the \(j\)-th prototype is calculated as follows:

\[_{i j}=g_{_{j}}(_{i})= _{i}-_{j}\|_{2}^{2}+1}{\|_{i}-_{j}\|_{2}^{2}+},\] (1)

where \(\) is a small constant for numerical stability.

**Probabilistic Masking Strategy.** To encourage models to fully capture intra-category diversity, we assign \(k\) prototypes equally to each category, thus \(m=k*_{S}|\). Given the prototype similarity score to all prototypes in the prototype layer of \(i\)-th image \(_{i}=[_{i 1},_{i 2},...,_{i m}]\), a fully connected layer (FC) is employed for supervised classification. To further discretize the prototypes,

Figure 2: Our PHE framework is composed of the CPG and DHC modules. First, CPG generates category-specific prototypes and prototype-guided instance representations. Then, DHC encodes the generated prototypes as hash centers to encourage the model to learn discriminative instance hash codes. Finally, depending on the Hamming distance between instance hash codes and hash centers, we can obtain instant feedback and online group instances into both known and unknown categories.

avoid redundancy, and unleash the full potential of the prototypes, we utilize a probabilistic masking strategy. This strategy masks some units of the similarity score according to probability, thereby disabling the corresponding prototypes and leaving the remaining prototypes activated during training. Specifically, for each unit of similarity score \(_{i}\), we mask it with a probability following the Bernoulli distribution \(()\), with \(\) empirically set to 0.1. We use the following loss function to learn the image representations and prototypes:

\[_{p}=_{i B}(_{i},FC(( )_{i})),\] (2)

where \(B\) indicates the mini-batch of support set, \(\) is the traditional cross-entropy loss and \(_{i}\) is the ground truth of image \(_{i}\).

**Discussion.** The benefits of our CPG module are threefold: i) _Fine-Grained Category Distinction:_ Fine-grained categories often exhibit only subtle differences. Our CPG module allows for representing a category with multiple prototypes, which can effectively capture and model both intra-class similarities and inter-class variances. ii) _Category-specific Hash Center Generation:_ Based on the generated category prototypes of CPG, we further map them into low-dimensional hash features to serve as hash centers for each category. This helps in maintaining the distinctiveness of each category in the encoding space. iii) _A New perspective for Classification Analysis:_ Beyond merely using hash encoding for category decisions, we can introduce a new prototype perspective for classification analysis. CPG enables a visualizable reasoning process for classifying unseen categories, providing a more intuitive and interpretable method for understanding category distinctions.

### Discriminative Hash Encoding

**Category Encoding Learning.** Given the set of learned category prototypes \(P_{c_{i}}\) for category \(c_{i}\) in CPG, we map the mean vector of each category's prototypes \( P_{c_{i}}/k\) to its hash center, denoted as \(_{i}=_{h}( P_{c_{i}}/k)^{L}\), where \(_{h}\) represents a linear head and \(L\) represent the feature dimension. The image feature \(_{i}\) is mapped to a hash feature \(_{i}\), where \(_{i}=_{h}(_{i})\). To ensure that image representations of the same category as closely as possible share the same category descriptor, we constrain each hash feature to be close to its corresponding category hash center and distant from other hash centers. We employ the following loss to optimize the hash features of the images:

\[_{f}=_{i B}(_{i},sim(_{i}, )),\] (3)

where \(sim(_{i},)\) represents a pair-wise similarity vector consisting of the cosine similarities between the hash feature of image \(_{i}\) and all hash centers.

**Hash Centers Optimization.** Due to the subtle differences between fine-grained categories, different category hash centers may become closely similar or even share identical hash codes. This hinders separability between fine-grained categories and leads to incorrect classification results. Therefore, we aim to maximize the differences between hash centers for enhancing inter-class separation.

Given the hash center \(_{i}\) of category \(c_{i}\), we denote its hash code as \(}_{i}\), where \(}_{i}=sign(_{i})\) and \(sign()\) equals 1/-1 for positive/negative values. Since the sign function is non-differentiable, we use a smoothed version of the sign function for back-propagation during training, defined as:

\[sign^{*}(a)-e^{-a 7}}{e^{a 7}+e^{-a  7}},\] (4)

where \(\) is a hyper-parameter that controls the smoothness of the sign function. Larger values of \(\) make the function more closely approximate the true sign function. In this paper, we set \(=3\). Consequently, \(}_{i}=sign^{*}(_{i})\). The difference between the \(i\)-th and \(j\)-th hash centers can be evaluated by the Hamming distance of their hash codes: \(||}_{i}-}_{j}||_{H}=}_{i }}_{j}}{2}\). Although we aim to maximize the differences between hash centers, we cannot simply increase the Hamming distance between all hash centers, as this can lead to model non-convergence. Since the encoding space is fixed, excessively distancing one hash code can inadvertently bring it closer to another.

We design a center separation loss, \(_{sep}\), to ensure that the Hamming distance between any two hash centers is greater than or equal to \(d\), denoted as \(||}_{i}-}_{j}||_{H} d\). The center separation loss is defined as:

\[_{sep}=_{i}_{j}(0,d-||}_{i}-}_{j}||_{H}),\] (5)indicating that we only optimize the pairs of hash centers whose Hamming distance is less than \(d\). However, determining the appropriate value of \(d\) is challenging. If \(d\) is too large, it can lead to model non-convergence; if it is too small, it can cause inadequate separation between hash centers. We choose a maximum \(d_{max}\) according to the Gilbert-Varshamov bound  in coding theory, as stated in Lemma 3.1.

**Lemma 3.1**: _For binary symbols, there exists a set of hash codes of length \(L\), \(\{-1,1\}^{L}\), with a minimum Hamming distance \(d\), and a number of hash codes \(\) that satisfies the following inequality:_

\[}{_{i=0}^{d-1}}.\] (6)

Therefore, given a number of hash codes equal to \(|_{S}|\), the maximum \(d_{max}\) can be determined as:

\[|_{S}|}{_{i=0}^{d_{max}-1}},\\ |_{S}|}{_{i=0}^{d_{max}-2}}.\] (7)

We apply this upper bound \(d_{max}\) to \(_{sep}\). Considering that we use a smoothed version of the sign function, where computed values do not equate to precisely \( 1\), we impose a quantization loss to constrain the values of hash codes to be close to \( 1\):

\[_{q}=_{i}(1-|}_{i}|).\] (8)

The optimization loss \(_{c}\) for hash centers is defined as a combination of the center separation loss \(_{sep}\) and the quantization loss \(_{q}\):

\[_{c}=_{sep}+_{q}.\] (9)

### Training and Inference

**Model Training.** During the model training process, the total loss is formulated as follows:

\[=_{p}+*_{c}+*_{f},\] (10)

where \(\) and \(\) control the importance of center optimization and hash encoding, respectively.

**Hamming Ball Based Model Inference.** During on-the-fly testing, given an input image \(x_{i}\) in the query set \(D_{Q}\), we use \(}_{i}=sign(_{h}(_{f}(( _{i}))))\) as its category descriptor. Due to the introduction of the center separation loss, the Hamming distance between any two hash centers is not less than \(d_{max}\). We consider a Hamming ball centered on the hash centers with a radius of \((}{2},1)\) to represent a category. Specifically, during inference, if the Hamming distance between \(}_{i}\) and any existing hash center is less than or equal to \((}{2},1)\), we classify the image as belonging to the corresponding category of that hash center. Otherwise, the image is used to establish a new hash center and category. The pseudo-code for the inference is provided in Algorithm 1.

Experiment

### Experiment Setup

**Datasets.** We have conducted experiments on eight fine-grained datasets, including CUB-200 , Stanford Cars , Oxford-IIIT Pet , Food-101 , and four super-categories from the more challenging dataset, iNaturalist , including Fungi, Arachinda, Animalia, and Mollusca. Following the setup in OCD , the categories of each dataset are split into subsets of seen and unseen categories. Specifically, 50% of the samples from the seen categories are used to form the labeled set \(}_{S}\) for training, while the remainder forms the unlabeled set \(_{Q}\) for on-the-fly testing. Detailed information about the datasets used is provided in the Appendix. A.1.

**Evaluation Metrics.** We follow Du et al.  and adopt clustering accuracy as an evaluation protocol, formulated as \(ACC=_{Q}|}_{i=1}^{|_{Q}|}(y_{i }=C(_{i}))\), where \(_{i}\) represents the predicted labels and \(y_{i}\) denotes the ground truth. The function \(C\) denotes the optimal permutation that aligns predicted cluster assignments with the actual class labels.

**Implementation Details.** For a fair comparison, we follow OCD  and use the DINO-pretrained ViT-B-16  as the backbone. During training, only the final block of ViT-B-16 is fine-tuned. In our approach, the Projector \(_{f}()\) is a single linear layer with an output dimension set to \(=768\), meaning the feature dimension and prototype dimension are equal to 768. Each category has \(k=10\) prototypes. The FC layer in Eq. 2 is non-trainable, which uses positive weights 1 for prototypes from the same category and negative weights -0.5 for prototypes from different categories. The Projector \(_{h}()\) consists of three linear layers with an output dimension set to \(L=12\), which produces \(2^{12}=4096\) binary category encodings. By default, we follow OCD to set this dimension for fair comparison. Additional experiments with varying \(L\) are reported in Sec. 4.4. The estimated values of \(d_{max}\) in center separation loss \(_{sep}\) can be found in Appendix. A.2. The ratio \(\) and \(\) in the total loss are set to 0.1 and 3, respectively, for all datasets. More details and the pseudo-code for the training process can be found in the Appendix. A.2.

**Compared Methods.** Given that OCD is a relatively new task requiring instantaneous inference, traditional baselines from NCD and GCD are unsuitable for this setting. Consequently, we compared with the **SMILE** along with three strong competitors in : i) **Sequential Leader Clustering (SLC)**: A classical clustering technique suitable for sequential data. ii) **Ranking Statistics (RankStat)**: RankStat utilizes the top-3 indices of feature embeddings as category descriptors. iii) **Winner-take-all (WTA)**: WTA employs indices of maximum values within feature groups as category descriptors. These three strong baselines are set following SMILE , and detailed implementation can be found in the Appendix. A.3.

### Comparison with State of the Art

We conduct comparison experiments with the aforementioned competitors across eight datasets. The experimental results are reported in Table 2. It is evident that the proposed PHE outperforms all state-of-the-art competitors across nearly all metrics. In particular, when compared with three strong baselines--SLC, RankStat, and WTA--our method demonstrates significant improvements. Additionally, our method surpasses the top competitor, SMILE, by an average of 5.4% in All classes accuracy on four common fine-grained datasets, and by an average of 5.1% in All classes accuracy on the four challenging datasets from the iNaturalist dataset. We achieve an average improvement of 11.3% on Old classes across the eight datasets compared to SMILE, demonstrating the effectiveness of our method in alleviating the "high sensitivity" issue of hash-based OCD methods. Importantly, our method consistently improves accuracy for unseen/new classes compared to SMILE. For instance, we achieve a 4.1% improvement on CUB-200 and a 4.4% improvement on the Oxford Pets dataset for new classes. This demonstrates that our method exhibits stronger generalization capabilities to new classes compared to SMILE. This advantage becomes more pronounced as the dimensionality \(L\) of hash features increases, as discussed in Sec. 4.4.

### Ablation Study

**Components Ablation.** We report an ablation analysis of the proposed components in our PHE on CUB dataset and Stanford Cars dataset, as shown in Table 3. "Without \(_{p}\)" refers to the removal of the representation learning in CPG, where we use randomly initialized vectors which are further mapped to hash centers. This configuration results in an average reduction of 1.95% in All classes accuracy across the two datasets. This variant of PHE shares the same architecture as SMILE, achieving clear decline on both datasets, demonstrating the effectiveness of our hash center-based category encoding method. It is noteworthy that removing \(_{c}\) causes a significant performance reduction, especially in seen categories--for instance, a 12.4% decrease in CUB. This decline is due to the lack of \(_{c}\), which results in insufficient separation of hash centers, causing multiple old classes to share the same hash encoding and thereby being classified as the same. Additionally, removing \(_{p}\) also leads to performance degradation. This is because the hash encodings of features are not sufficiently close to the hash centers, resulting in inadequate learning of hash features.

**Strategy Ablation.** In Table 4, we evaluate the training strategies of our method. "Fixed-\(\)" refers to the use of handcrafted hash points that satisfy Eq. 7. Although this design maintains separation between hash centers, it may alter the relationships between categories learned in CPG, leading to sub-optimal outcomes. "Linear CIs." and "Supcon CIs." represent the use of simple linear classification and supervised contrastive learning classification methods for representation learning, respectively, where "Fixed-\(\)" is also applied due to the absence of category prototypes. Although these variants perform well on seen categories, their generalization capabilities are inferior to our full prototype-based method. This demonstrates the importance of integrating prototype learning to enhance generalization across unseen categories.

### Evaluation

**Evaluation on Hash Code Length.** The hash code length \(L\) is crucial for category inference as it directly determines the size of the prediction space, which equals to \(2^{L}\). A larger \(L\) value results in a greater number of category encodings, making the "high sensitivity" issue more severe. We evaluate our method and SMILE with different hash code lengths in Table 5. When small changes occur to \(L\) (from 12 to 16), SMILE demonstrates stable results. However, with \(L=32\), SMILE experiences a decrease in average all-classes accuracy by 5.1% on two datasets, and with \(L=64\), the decrease is by 10.2% averaged on the two datasets. In contrast, our method, which employs hash center-based

    &  &  \\   & All & Old & New & All & Old & New \\  Fixed-\(\) & 35.4 & 54.7 & 25.8 & 30.0 & 61.5 & 14.8 \\ Linear CIs & 35.6 & **57.8** & 24.6 & 29.4 & 63.6 & 13.0 \\ Supcon CIs & 35.3 & 57.5 & 24.2 & 29.5 & **64.1** & 12.8 \\ Ours & **36.4** & 55.8 & **27.0** & **31.3** & 61.9 & **16.8** \\   

Table 4: Ablation study on training strategy. The best results are marked in **bold**.

    &  &  &  &  &  \\   & All & Old & New & All & Old & New & All & Old & New & All & Old & New & All & Old & New \\  SLC  & 31.3 & 48.5 & 22.7 & 24.0 & 45.8 & 13.6 & 35.5 & 41.3 & 33.1 & 20.9 & 48.6 & 6.8 & 27.9 & 46.1 & 19.1 \\ RankStat  & 27.6 & 46.2 & 18.3 & 18.6 & 36.9 & 9.7 & 33.2 & 42.3 & 28.4 & 22.3 & 50.7 & 7.8 & 25.4 & 44.0 & 16.1 \\ WTA  & 26.5 & 45.0 & 17.3 & 20.0 & 38.8 & 10.6 & 35.2 & 46.3 & 29.3 & 18.2 & 40.5 & 6.1 & 25.0 & 42.7 & 15.8 \\ SMILE  & 32.2 & 50.9 & 22.9 & 26.2 & 46.7 & 16.3 & 41.2 & 42.1 & 40.7 & 24.0 & 54.6 & 8.4 & 30.9 & 48.6 & 22.1 \\ PHE (Ours) & **36.4** & **55.8** & **27.0** & **31.3** & **61.9** & **16.8** & **48.3** & **53.8** & **45.4** & **29.1** & **64.7** & **11.1** & **36.3** & **59.1** & **25.1** \\   &  &  &  &  &  \\   & All & Old & New & All & Old & New & All & Old & New & All & Old & New \\  SLC  & 27.7 & 60.0 & 13.4 & 25.4 & 44.6 & 11.4 & 32.4 & **61.9** & 19.3 & 31.1 & 59.8 & 15.0 & 29.2 & 56.6 & 14.8 \\ RankStat  & 23.8 & 50.5 & 12.0 & 26.6 & 51.0 & 10.0 & 31.4 & 54.9 & 21.6 & 29.3 & 55.2 & 15.5 & 27.8 & 52.9 & 14.8 \\ WTA  & 27.5 & 65.6 & 12.0 & 28.1 & 55.5 & 10.9 & 33.4 & 59.8 & 22.4 & 30.3 & 55.4 & 17.0 & 29.8 & 59.1 & 15.6 \\ SMILE  & 29.3 & 64.6 & 13.6 & 29.9 & 57.9 & 12.2 & 35.9 & 49.4 & 30.3 & 33.3 & 44.5 & **27.2** & 32.1 & 54.1 & 20.8 \\ PHE (Ours) & **31.4** & **67.9** & **15.2** & **37.0** & **75.7** & **12.6** & **40.3** & 55.7 & **31.8** & **39.9** & **65.0** & 26.5 & **37.2** & **66.1** & **21.5** \\   

Table 2: Comparison with the State of the Art methods. The best results are marked in **bold**, and the second best results are marked by underline.

   _{p}\)} & \(_{c}\) &  &  \\   & All & Old & New & All & Old & New \\   & ✓ & ✓ & 34.9 & 53.0 & 25.8 & 28.9 & 58.4 & 14.6 \\ ✓ & & ✓ & 32.0 & 43.4 & 26.4 & 24.1 & 40.2 & 16.3 \\ ✓ & ✓ & & 34.1 & 54.3 & 24.0 & 26.0 & 52.6 & 13.1 \\ ✓ & ✓ & ✓ & **36.4** & **55.8** & **27.0** & **31.3** & **61.9** & **16.8** \\   

Table 3: Ablation study on training components. The best results are marked in **bold**.

optimization and a Hamming ball-based inference process, effectively mitigates the "high sensitivity" issue and maintains stable performance as \(L\) increases. Furthermore, when \(L\) is increased from 16 to 64, the estimated number of classes by SMILE increased by 1986 on the CUB dataset and 3892 on the Stanford Cars dataset, underscoring the impact of "high sensitivity" of hash-form category descriptors on accuracy. Conversely, our method exhibits remarkable stability.

**Visualization Analysis.** We use an images from the support set whose latent representations is most similar to \(_{j}\) as the visualization for \(_{j}\). During on-the-fly inference, the hash code functions as the category descriptor. Additionally, the learned prototypes allow us to visually analyze why the model categorizes certain samples into known or unknown categories. Fig. 3 illustrates this reasoning process from the prototype perspective. On the left, an image labeled in green depicts a Le Conte Sparrow, a category recognized during training. Under it, a red-labeled image represents a Grasshopper Sparrow, a new category. In the center, we display the visualization of the prototypes, showing only five per class. The top five prototypes most similar to the green-labeled image are exclusively associated with the Le Conte Sparrow category, while those closest to the red-labeled, unseen image span two different categories. The Grasshopper Sparrow exhibits significant body stripe similarities to the Le Conte Sparrow and shares head similarities with the Horned Lark. Upon calculating similarities with the top five activated prototypes, the similarity score for the green-labeled image to the Le Conte Sparrow category is 9.0, whereas the unseen Grasshopper Sparrow achieves a similarity of 3.32 with the Le Conte Sparrow and 3.54 with the Horned Lark. This reasoning process is similar to how humans cognitively recognize new species. Given a new species, we use the characteristics of known categories to describe the unknown new category. Consequently, an entity that exhibits high similarity to multiple known categories, rather than only one known category, is likely to be classified as a previously unseen species.

**Hyper-parameters Analysis.** 1) The impact of the ratios \(\) and \(\) in Eq. 10 is illustrated in Fig. 4. We use All classes accuracy as the evaluation metric. \(\) and \(\) control the relative importance of \(_{c}\) and \(_{f}\) during the training process, respectively. A lower value of \(\) is found to be preferable, as higher values can lead to excessive changes in hash centers, thereby affecting training stability.

Figure 3: Case Study: Why is a Grasshopper Sparrow classified as a new category?

    &  &  &  &  &  \\   & & All & Old & New & \#Class & All & Old & New & \#Class \\   & SMILE & 31.9 & 52.7 & 21.5 & 924 & 27.5 & 52.5 & 15.4 & 896 \\  & Ours & **37.6** & **57.4** & **27.6** & **318** & **31.8** & **65.4** & **15.6** & **709** \\   & SMILE & 27.3 & 52.0 & 14.97 & 2146 & 21.9 & 46.8 & 9.9 & 2953 \\  & Ours & **38.5** & **59.9** & **27.8** & **474** & **31.5** & **64.0** & **15.8** & **762** \\   & SMILE & 22.6 & 45.3 & 11.2 & 2910 & 16.5 & 38.2 & 6.1 & 4788 \\  & Ours & **38.1** & **60.1** & **27.2** & **493** & **32.1** & **66.9** & **15.3** & **917** \\   

Table 5: Results with different hash code length \(L\). The best results are marked in **bold** for each \(L\).

Regarding \(\), the results are generally stable; however, higher values show improved performance across both datasets. Consequently, we selected \(=0.1\) and \(=3\) for all datasets during training. 2) Additionally, we examine the impact of the number of prototypes per class on the CUB and Stanford Cars datasets, as shown in Fig. 4. When only one prototype per class is used, performance is suboptimal, as it fails to effectively represent the complexity of a category. For instance, a single bird species might exhibit different behaviors, such as flying or standing, that are not adequately captured by a single prototype. More prototypes allow for a better expression of the nuances within a category, which is crucial in fine-grained classification. Therefore, we opt to utilize 10 prototypes per class across all datasets.

**Hash Centers Analysis.** We conduct an analysis of the distribution of hash centers before and after training to further evaluate the impact of the proposed hash center optimization loss, \(_{c}\). Specifically, we analyze the Hamming distances between hash centers on the CUB dataset. As depicted in Fig. 5, prior to training, the hash centers, derived from randomly initialized prototypes, are distributed relatively uniformly. Notably, some centers exhibit a Hamming distance of zero, indicating multiple centers sharing a single hash code. After training, the Hamming distance between all hash centers is at least \(d_{max}\). This significant improvement demonstrates the effectiveness of \(_{c}\) in ensuring that multiple categories do not share identical hash codes or reside excessively close to one another.

## 5 Conclusion

In this paper, we introduce a Prototypical Hash Encoding (PHE) framework for fine-grained On-the-fly Category Discovery. Addressing the limitations of existing methods, which struggle with the high sensitivity of hash-form category descriptors and suboptimal feature representation, our approach incorporates a prototype-based classification model. This model facilitates robust representation learning by developing multiple prototypes for each fine-grained category. We then map these category prototypes to corresponding hash centers, optimizing image hash features to align closely with these centers, thereby achieving intra-class compactness. Additionally, we enhance inter-class separation by maximizing the distance between hash centers, guided by the Gilbert-Varshamov bound. Experiments on eight fine-grained datasets demonstrate that our method outperforms previous methods by a large margin. Moreover, a visualization study is provided to understand the underlying mechanism of our method.

Figure 4: Impact of hyper-parameters.

Figure 5: Evolution of hash centers distribution during the training process.

**Acknowledgement.** This work has been supported by the National Natural Science Foundation of China (62402157), the MUR PNRR project FAIR (PE00000013) funded by the NextGenerationEU, the EU Horizon project ELIAS (No. 101120237), the MUR PNRR project iNEST-Interconnected Nord-Est Innovation Ecosystem (ECS00000043) funded by the NextGenerationEU, and the EU Horizon project AI4Trust (No. 101070190).