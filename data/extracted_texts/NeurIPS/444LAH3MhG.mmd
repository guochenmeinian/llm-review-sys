# Boundary Matters: A Bi-Level

Active Finetuning Method

 Han Lu\({}^{1}\), Yichen Xie\({}^{2}\), Xiaokang Yang\({}^{1}\), Junchi Yan\({}^{1,}\)

\({}^{1}\) Dept. of CSE & School of AI & Moe Key Lab of AI, Shanghai Jiao Tong University

\({}^{2}\)University of California, Berkeley

{sjtu_luhan, xkyang, yanjunchi}@sjtu.edu.cn, yichen_xie@berkeley.edu

https://github.com/Thinklab-SJTU/BiLAF

Corresponding author. This work was supported by NSFC (92370201, 62222607) and Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0102.

###### Abstract

The pretraining-finetuning paradigm has gained widespread adoption in vision tasks and other fields. However, the finetuning phase still requires high-quality annotated samples. To overcome this challenge, the concept of active finetuning has emerged, aiming to select the most appropriate samples for model finetuning within a limited budget. Existing active learning methods struggle in this scenario due to their inherent bias in batch selection. Meanwhile, the recent active finetuning approach focuses solely on global distribution alignment but neglects the contributions of samples to local boundaries. Therefore, we propose a Bi-Level Active Finetuning framework (BiLAF) to select the samples for annotation in one shot, encompassing two stages: core sample selection for global diversity and boundary sample selection for local decision uncertainty. Without the need of ground-truth labels, our method can successfully identify pseudo-class centers, apply a novel denoising technique, and iteratively select boundary samples with designed evaluation metric. Extensive experiments provide qualitative and quantitative evidence of our method's superior efficacy, consistently outperforming the existing baselines.

## 1 Introduction

The advancement of deep learning significantly relies on extensive training data. However, annotating large-scale datasets is challenging, requiring significant human labor and resources. To address this challenge, the pretraining-finetuning paradigm has gained widespread adoption. In this paradigm, models are first pretrained in an unsupervised manner on large datasets and then finetuned on a smaller, labeled subset. While there is substantial research about both unsupervised pretraining  and supervised finetuning , the optimization of sample set selection for annotation has received less attention, especially in scenarios with limited labeling resources.

Active learning methods , though effective in identifying valuable samples for training from scratch, face significant challenges when integrated into the pretraining-finetuning framework . The primary limitation stems from the batch-selection strategy commonly used by these methods. Allocating a limited annotation budget across multiple iterations can introduce harmful biases, which leads to overfitting. Consequently, this undermines the general representational quality of the pretrained model and leads to the accumulation of errors in the iterative selection process.

To fill in the research gap, the _Active Finetuning_ task has been formulated in , which focuses on the selection of samples for supervised finetuning using pretrained models. This method optimizes sample selection by minimizing the distributional gap between the selected subset and the entire data pool.

Despite its notable performance, it fundamentally concentrates on the global diversity of selected samples, which neglects the local decision boundaries--also referred to as sample uncertainty in data selection. As the volume of data increases, we found the model's capabilities drops dramatically, which indicates the selected samples become increasingly redundant and less informative.

To mitigate the inherent limitations of existing approaches, we introduce the innovative **Bi-Level Active Finetuning Framework (BiLAF)**, which can effectively capture both diversity and uncertainty of selected samples, as depicted in Fig. 1. The key challenge lies in measuring uncertainty within the pretraining-finetuning paradigm. Without labels and a trained classifier head, traditional methods based on posterior probability [24; 39], entropy [18; 29] and loss metrics [16; 45] are infeasible. Notably, we observe that the global feature space of pretrained models inherently captures the interrelations among samples from different classes. Consequently, we can effectively utilize their feature outputs to facilitate the identification of support samples that are proximate to the model's decision boundary. This concept has been extensively supported by theoretical  and methodological research [4; 5] across various domains.

To elucidate, our BiLAF framework operates through two distinct stages. The initial phase, _Core Samples Selection_, is dedicated to identifying pivotal samples for each class. The selection mechanism employed can vary, including options such as K-Means or ActiveFT . The second stage, _Boundary Samples Selection_, begins with our innovative unsupervised denoising technique that precisely isolates noisy outliers. Following this, we systematically identify boundary samples adjacent to each core sample and efficiently eliminate redundant samples, employing our newly proposed boundary score.

In conclusion, our contributions are summarized as follows:

* We propose a Bi-Level Active Finetuning Framework (BiLAF) emphasizing boundary importance to balance sample diversity and uncertainty. This framework exhibits remarkable flexibility and can accommodate various methods seamlessly.
* The proposed unsupervised denoising technique in BiLAF can effectively eliminate the outlier samples and an iterative strategy with newly designed metric can identify the marginal boundary samples in feature space. Compared to other methods, our approach has high accuracy and efficiency.
* Extensive experiments and ablation studies demonstrate the effectiveness of our method. Compared to the current state-of-the-art approach, our method achieves a remarkable improvement of nearly 3% absolute accuracy on CIFAR100 and approximately 1% on ImageNet. What's more, it outperforms the other baselines in object detection, semantic segmentation, and the long-tail tasks.

## 2 Related Work

### Active Learning / Finetuning

Active learning maximizes annotation efficiency by selecting the most informative samples. Typically, uncertainty-driven methods select difficult samples based on heuristics like posterior probability [24;

Figure 1: **Design philosophy of our BiLAF framework. In contrast to the previous method, our method ensures the selection of central samples to maintain diversity while also reserving capacity to choose boundary samples to enhance decision boundary learning.**

39], entropy [18; 29], or loss metrics [16; 45], while diversity-based approaches approximate the original data distribution using metrics like sample distance or gradient directions [33; 1; 40; 2; 20; 36]. However, these methods struggle within the pretraining-finetuning paradigm [12; 41]. ActiveFT  addresses this by aligning the sample distribution with the unlabeled pool, often prioritizing high-density over boundary areas. Our algorithm, by integrating both diversity and uncertainty, refines the selection process to enhance decision boundary samples selection for following supervised finetuning.

### Decision Boundaries in Neural Networks.

Decision boundaries are pivotal in neural network-based classifiers, influencing both performance and interpretability [23; 25]. Their optimization enhances generalizability and accuracy in complex data spaces [42; 5]. In SVMs, decision boundaries separate hyperplanes and maximize geometric margins for robust classification . This concept extends to neural networks, where margin maximization also promotes generalization. For imbalanced datasets, adjusting the decision boundary is crucial for accurate minority class classification, with approaches like LDAM loss  and ELM loss  developed to modify the boundary and balance errors across classes. Neural networks tend to utilize simple or highly discriminative features for decision boundaries [30; 34]. A theoretical framework exploring decision boundary complexity and its inverse relation to generalizability is introduced in . Notably, this is not trivial in unsupervised scenarios. Despite the challenge, we effectively leverage features from pretrained models, thereby introducing innovative denoising and selection methods without labels. This approach addresses a significant gap in the active finetuning domain.

## 3 BiLAF: Bi-Level Active Finetuning

In this section, we introduce our novel **Bi-Level** Active **F**inetuning Framework (**BiLAF**), as illustrated in Fig. 2. BiLAF operates in two distinct stages: Firstly, _Core Samples Selection_ (Sec. 3.2) involves identifying multiple pseudo-class centers, which ensures comprehensive coverage across all classes. Secondly, _Boundary Samples Selection_ (Sec. 3.3) focuses on the accurate identification of boundary samples for each pseudo-class, using the novel denoising and iterative selection method. Algorithm 1 summarizes the complete workflow. The theoretical time complexity is analyzed in Appendix D.

### Preliminary: Active Finetuning Task

The active finetuning task is defined in . Apart from a large unlabeled data pool \(^{u}=\{_{i}\}_{i[N]}\) where \([N]=\{1,2,,N\}\) identical to the traditional active learning task, we have the access to a deep neural network model \(f(;w_{0})\) with the pretrained weight \(w_{0}\). It should be noted that the function \(f(;w_{0})\) can be pretrained either on the current data pool \(^{u}\) or any other external data

Figure 2: **Our BiLAF framework in the Active Finetuning task. In the high-dimensional feature space, the Core Sample Selection focuses on pinpointing pseudo-class centers. Following this, we have devised a denoising method to eliminate noise samples. Subsequently, we compute the Boundary Score metric for each sample, which aids in the iterative selection of samples and the removal of candidates from the pool. Ultimately, the selected samples are labeled for supervised finetuning.**

sources. Using the pretrained model, we can map the data sample \(_{i}\) to the high dimensional feature space as \(_{i}=f(_{i};w_{0})^{d}\), where \(_{i}\) is the _normalized_ feature of \(_{i}\). From this, we can derive the feature pool \(^{u}=\{_{i}\}_{i[N]}\) from \(^{u}\), which assists us in selecting the optimal samples.

Our task is to select the subset \(^{u}_{}\) from \(^{u}\) for annotation and the subsequent supervised finetuning. The subset \(^{u}_{}=\{_{s_{j}}\}_{j[B]}\) is determined by the sampling strategy \(=\{s_{j}[N]\}_{j[B]}\), where \(B\) represents the annotation budget. The labels \(\{_{s_{j}}\}_{j[B]}\), a subset of the label space \(\), are accessed through an oracle, resulting in a labeled data pool \(^{l}_{}=\{_{s_{j}},_{s_{j}}\}_{j[ B]}\). Subsequently, the pretrained model \(f(;w_{0})\) undergoes supervised finetuning on \(^{l}_{}\). Our objective is to optimize the sampling strategy \(\) to select the labeled set that minimizes the expected error of the finetuned model under the given annotation budget constraints.

### Core Samples Selection

We start data selection procedure by selecting core samples. Those core samples can represent the distribution of the entire dataset. Popular methods include K-Means, Coreset , and ActiveFT , differing in their design targets and optimization procedures. We employ ActiveFT, the most advanced method to date, as our primary approach in the initial phase. For detailed implementation of ActiveFT, see Appendix B. Using ActiveFT, we obtain \(K\) core samples, which serve as the center for each pseudo-class \(i\), where \(K\) represents the predefined budget for core samples.

### Boundary Samples Selection

In addition to those \(K\) core samples, we continue to select samples close to the boundaries of semantic categories, which can enhance the training of the model's decision boundaries.

By leveraging pretrained models, data samples are mapped to robust feature representations that elucidate the relationships among samples, their intra-class counterparts, and inter-class samples from diverse classes. We introduce a novel method for boundary sample selection-- the first in this field. Given the pseudo-class centers, our method involves three steps: _1) Identifying the samples associated with each pseudo-class center; 2) Implementing denoising processes to refine these samples; 3) Developing precise metrics to select boundary samples for each pseudo-class_.

#### 3.3.1 Sample Clustering

Given \(K\) pseudo-class centers denoted by \(C=\{c_{1},c_{2},,c_{K}\}\) where \(c_{i}[N]\), a sample \(_{j}\) with its feature vector \(_{j}^{d}\) is assigned to the pseudo-class center \(c_{i}\) that minimizes the distance \(D(_{j},_{c_{i}})\). This assignment can be mathematically represented as:

\[c_{i}=_{c C}D(_{j},_{c})\] (1)

where \(D(,)\) denotes the distance function. While the choice of \(D\) can vary based on the design of the clustering model, we employ the Euclidean distance in our implementation, which efficiently aligns each sample point with a corresponding pseudo-class center during the optimization process.

#### 3.3.2 Boundary Sample Denoising

Boundary samples within a pseudo-class are crucial for optimizing decision boundaries. However, they can also introduce noise, which potentially hinders the model's performance. For each class \(i\), with \(N_{i}\) samples, we define a removal ratio \(P_{rm}\) to identify and eliminate \(N_{i,rm}=N_{i} P_{rm}\) peripheral noisy samples from the candidate boundary set. This elimination process is based on the density of samples in the feature space, where we define the concept of density distance as follows:

**Definition 1** (Density Distance).: _The density distance of a sample is defined as the average distance to its \(k\) nearest neighbors. Formally, for a given sample \(x_{j}\) characterized by its feature vector \(_{j}\), its density distance \((_{j})\) is defined as:_

\[(_{j})=_{l=1}^{k}D(_{j},_{n_{ jl}}),\] (2)

_where \(n_{jl}\) represents the index of the \(l\)-th nearest neighbor of \(x_{j}\) in the feature space, and \(D\) denotes a distance function._Inspired by the classical clustering method DBSCAN , we propose an Iterative Density-based Clustering (IDC) algorithm. IDC clusters the candidate samples \(_{i}=\{_{a_{i,1}},_{a_{i,2}},..._{a_{i,N_{ i}}}\}\) where the index \(a_{i,j}[N]\) and \(j[N_{i}]\). The algorithm initiates with the core sample index \(c_{i}\) as the seed set \(U_{i}=\{c_{i}\}\). In each subsequent iteration, a fraction \(P_{in}\) of samples are included, which corresponds to \(N_{i,in}=N_{i} P_{in}\) peripheral samples being integrated into the existing cluster \(U_{i}\).

The density distance of each candidate sample is redefined from Eq. 2 as the average distance to the nearest \(k\) selected sample in the cluster \(U_{i}\). Specifically, the density distance \((_{j})\) for each remaining point \(_{j}\) is calculated as:

\[(_{j})=_{l=1,n_{jl} U_{i}}^{k}D(_{j},_{n_{jl}}),\] (3)

where \(n_{jl}\) represents the index of the \(l\)-th nearest sample in \(U_{i}\) to \(_{j}\). In each iteration, the \(N_{i,in}\) samples with the lowest density distance are selected. This process repeats until all samples have been included. The order of inclusion into the cluster reflects each point's proximity to the center, with later-added samples more likely to be peripheral and noisy. Consequently, the last \(N_{i,rm}\) samples can be removed from \(U_{i}\).

#### 3.3.3 Iterative Sample Selection

After successfully eliminating noise samples, we advance to the selection of boundary samples from the centers of pseudo-classes. To streamline this process, we introduce a set of definitions that facilitate the computation of a boundary score for each sample.

**Definition 2** (Intra-class Distance).: _Let \(_{j}\) be a sample in the set \(U_{i}\) of the pseudo-class \(c_{i}\). The intra-class distance for \(_{j}\) is the average distance from \(_{j}\) to all other samples within the same class set \(U_{i}\). It can be formally defined as:_

\[d_{intra}(_{j})=|}_{l U_{i}}D(_{j}, _{l})\] (4)

_where \(D\) is the distance function and \(|U_{i}|\) is the number of samples in pseudo-class \(c_{i}\)._

**Definition 3** (Inter-class Distance).: _The inter-class distance for a sample \(_{j}\) in the pseudo-class \(c_{i}\) is defined as the distance from \(_{j}\) to the nearest center of other pseudo-classes. It is defined as:_

\[d_{inter}(_{j})=_{c_{l} C,i l}D(_{j}, _{c_{l}})\] (5)

_where \(D\) is the distance function, \(C=\{c_{1},,c_{K}\}\) are the pseudo-class centers, and \(i l\) ensures that the pseudo-class \(c_{i}\) of the sample \(_{j}\) is excluded from the calculation._

**Definition 4** (Boundary Score).: _The Boundary Score for sample \(_{j}\) in pseudo-class \(c_{i}\) can be defined as a function of both intra-class and inter-class distances. It is defined as:_

\[BS(_{j})=(_{j})-d_{intra}(_{j})}{ (d_{inter}(_{j}),d_{intra}(_{j}))}\] (6)

_where \(d_{intra}\) and \(d_{inter}\) are the intra-class and inter-class distances, respectively. A smaller Boundary Score indicates closer proximity to the boundary._For each pseudo-class \(c_{i}\) with the samples set \(U_{i}\), we allocate the sample selection budget in proportion to the size of the pseudo-class. Specifically, the budget \(B_{i}\) for pseudo-class \(c_{i}\) is calculated as: \(B_{i}=B|U_{i}|/_{j=1}^{K}|U_{j}|\) where \(|U_{i}|\) represents the number of samples in pseudo-class \(c_{i}\).

Initially, one might consider simply selecting the top \(B_{i}\) samples with the lowest boundary scores. However, this approach risks over-concentration of selections within specific areas of the feature space. To address this, we employ an **iterative selection and removal** strategy, which progressively selects and eliminates candidate samples. Furthermore, to prevent the aggregation of multiple samples near the same pseudo-class boundary that faces the same opposing pseudo-class center, we introduce an **opponent penalty**. This mechanism increases the influence of previously selected boundary samples on subsequent selections, encouraging greater diversity across different boundaries.

**Iterative selection and removal** begins by selecting samples with the lowest Boundary Score in each iteration. Subsequently, the nearest \(|U_{i}|/B_{i}\) samples surrounding the selected sample are removed to prevent clustering. This process is repeated \(B_{i}\) times, starting from the center samples.

**opponent penalty** monitors the relationship between the currently selected samples and the boundaries to other pseudo-classes, imposing a penalty on opponent pseudo-classes whose boundaries have already been selected. Specifically, for the selected pool of pseudo-class \(c_{i}\), if boundary samples related to an opposing pseudo-class \(l i\) have been selected \(t_{l}\) times, the inter-class distance for the subsequent samples to this pseudo-class \(l\) will be scaled by a factor of \(^{t_{l}}\), where the opponent penalty coefficient \(\) is a hyperparameter greater than 1. This indicates that the more often a sample is selected in relation to pseudo-class \(l\)'s boundary, the higher its Boundary Score with that pseudo-class becomes, making it less likely to be chosen in future rounds. Consequently, we recalibrate the Boundary Score for each sample in each iteration, leading to a modification from Eq. 6:

\[BS(_{j})=_{c_{i} C;i l}}D(_{ j},_{c_{l}})-d_{intra}(_{j})}{(D(_{j}, _{c_{l}}),d_{intra}(_{j}))}\] (7)

where \(D(_{j},_{c_{l}})\) measures the distance from sample \(_{j}\) to the opponent pseudo-class \(l\) and the \(^{t_{l}}\) reflects the imposed penalty.

In each pseudo-class \(i\), we select the \(B_{i}\) samples through an iterative selection and removal strategy that incorporates this opponent penalty in the increasing order of \(BS(_{j})\). The selected samples from various pseudo-classes are ultimately aggregated to form the comprehensive annotation subset \(_{}^{u}\).

## 4 Experiments

Our approach has been rigorously evaluated using three primary image classification benchmarks, alongside various tasks with different sampling ratios, as detailed in Sec. 4.1. We compare its performance against multiple baselines and conventional active learning techniques, with results discussed in Sec. 4.2. The analysis of our method is presented in Sec. 4.3, and a comprehensive ablation study is provided in Sec. 4.4. All experiments were conducted using GeForce RTX 3090(24G) GPUs and Intel(R) Core(TM) i9-10920X CPUs. The source code will be made publicly available.

### Experiment Setup

**Datasets and Evaluation Metrics.** Firstly, we evaluate our method using three widely recognized classification datasets: CIFAR10, CIFAR100 , and ImageNet-1k . The raw training data from these datasets constitute the unlabeled pool \(^{u}\) from which selections are made. For performance evaluation, we employ the _Top-1 Accuracy_ metric. In addition to these classification tasks, extensive experiments have also been conducted in object detection, semantic segmentation, and long-tail tasks. Further details on these experiments can be found in Appendix E and F.

**Baselines.** We compare our approach with three heuristic baselines Random, FDS and K-Means; five active learning methods CoreSet , VAAL , LearnLoss , TA-VAAL , and ALFA-Mix ; and the well-designed active finetuning method ActiveFT . We utilize the overlapping results reported in . The detailed information of the baselines is listed in Appendix E.

**Implementation Details.** In line with the SOTA method ActiveFT , we use DeiT-Small  model, pretrained using the DINO  framework on ImageNet-1k in the unsupervised pretraining phase. For all the datasets, we resize images to \(224 224\) consistent with the pretraining for both data selection and supervised finetuning. In the core samples selection stage, we utilize ActiveFT and optimize the parameters \(_{}\) using the Adam  optimizer (learning rate 1e-3) until convergence. We set the core number \(K\) as \(50(0.1\%),250(0.5\%),6405(0.5\%)\) for CIFAR10, CIFAR100 and ImageNet separately. In the boundary samples selection stage, we consistently set nearest neighbors number \(k\) as 10, both removal ratio \(P_{rm}\) and clustering fraction \(P_{in}\) as \(10\%\), opponent penalty coefficient \(\) as \(1.1\). The experiment details of supervised finetuning are listed in the Appendix E.

### Overall Peformance Comparison

The average performance and standard deviation from three independent runs are presented in Tab. 1. Under a low sampling ratio of 0.5% for CIFAR10, our method is marginally outperformed by ActiveFT. This can be attributed to the more stable model training aided by core point selection at extremely low budgets, whereas boundary samples tend to introduce greater instability and perturbation. However, as the volume of data increases, the advantages of constructing precise boundaries become more pronounced. Our approach BiLAF exhibits significant superiority across most scenarios, markedly outperforming competing methods. Notably, on CIFAR100, BiLAF consistently outperforms the previously best-performing model, ActiveFT, by approximately 3%. On ImageNet, BiLAF achieves a consistent improvement of about 1%. These significant enhancements underscore the effectiveness of the BiLAF method. Further details on the robust performance of our method across various pretraining paradigms and model architectures, which substantiate both its effectiveness and generality, are provided in Appendix F.

Tab. 2 presents the performance of our method across diverse tasks and scenarios. Although our design, based on boundary sample selection, appears to be tailored for classification tasks, the performance of BiLAF still effectively surpasses that of other models in these domains. We attribute this success to our effective denoising of outlier samples and the method's focus on selected sample uncertainty. The less pronounced advantage in certain tasks may be due to global features not fully representing task requirements such as in object detection involving multiple objects, and the default denoising removal ratio parameters might remove minority samples in long-tail distributions. Despite these challenges, BiLAF demonstrates robust performance across various scenarios, confirming its

    &  &  &  \\  & 0.5\% & 1\% & 2\% & 1\% & 2\% & 5\% & 10\% & 1\% & 2\% & 5\% \\ 
**Random** & 77.3\(\)2.6 & 82.2\(\)1.9 & 88.9\(\)0.4 & 14.9\(\)1.9 & 24.3\(\)2.0 & 50.8\(\)3.4 & 69.3\(\)0.7 & 45.1\(\)0.8 & 52.1\(\)0.6 & 64.3\(\)0.3 \\
**FDS** & 64.5\(\)1.5 & 73.2\(\)1.2 & 8.1\(\)0.7 & 8.1\(\)0.6 & 12.8\(\)0.3 & 16.9\(\)1.4 & 52.3\(\)1.9 & 26.7\(\)0.6 & 43.1\(\)0.4 & 55.5\(\)0.1 \\
**K-Means** & 83.0\(\)3.5 & 85.9\(\)0.8 & 89.6\(\)0.6 & 17.6\(\)1.1 & 31.9\(\)0.1 & 42.4\(\)1.0 & 70.7\(\)0.3 & 49.8\(\)0.5 & 55.4\(\)0.3 & 64.0\(\)0.2 \\ 
**CoreSet** & - & 81.6\(\)0.3 & 88.4\(\)0.2 & - & 30.6\(\)0.4 & 48.3\(\)0.5 & 62.9\(\)0.6 & - & 52.7\(\)0.4 & 61.7\(\)0.2 \\
**VALL** & - & 80.9\(\)0.5 & 88.8\(\)0.3 & - & 24.6\(\)1.1 & 46.4\(\)0.8 & 70.1\(\)0.4 & - & 54.7\(\)0.5 & 64.0\(\)0.3 \\
**LearnLoss** & - & 81.6\(\)0.6 & 86.7\(\)0.4 & - & 19.2\(\)2.2 & 82.8\(\) 65.7\(\)1.1 & - & 54.3\(\)0.6 & 65.2\(\)0.4 \\
**TA-VALL** & - & 82.6\(\)0.4 & 88.7\(\)0.2 & - & 34.7\(\)0.7 & 46.4\(\)1.1 & 66.8\(\)0.5 & - & 55.0\(\)0.4 & 64.3\(\)0.2 \\
**ALFA-Mix** & - & 83.4\(\)0.3 & 89.6\(\)0.2 & - & 35.3\(\)0.8 & 50.4\(\)0.9 & 69.9\(\)0.6 & - & 55.3\(\)0.3 & 64.5\(\)0.2 \\ 
**ActiveFT** & **85.0\(\)**0.4 & 88.2\(\)0.4 & 90.1\(\)0.2 & 26.1\(\)2.6 & 40.7\(\)0.9 & 54.6\(\)2.3 & 71.0\(\)0.5 & 50.1\(\)0.3 & 55.8\(\)0.3 & 65.3\(\)0.1 \\ 
**BILAF (ours)** & 81.0\(\)1.2 & **89.2\(\)**0.6 & **92.5\(\)**0.4 & **31.8\(\)**1.6 & **43.5\(\)**0.8 & **62.8\(\)**1.2 & **73.7\(\)**0.5 & **50.8\(\)**0.4 & **56.9\(\)**0.3 & **66.2\(\)**0.2 \\   

Table 1: **Benchmark Results. Experiments are conducted on three popular datasets with different annotation ratios. We report the mean and standard deviation over three trials. Traditional active learning methods require random initial data to start, thus we use “-” to represent. BiLAF has shown a significant competitive advantage across the majority of scenarios, affirming its effectiveness.**

    &  &  &  \\  & \(\)1000 & \(\)2000 & \(\)3000 & \(\)5000 & 5\% & 10\% & 15\% & 5\% & 10\% & 15\% \\ 
**Random** & 51.1\(\)0.9 & 60.9\(\)0.6 & 64.2\(\)0.4 & 67.5\(\)0.4 & 14.5\(\)0.1 & 20.2\(\)1.6 & 23.5\(\)1.35 & 21.4\(\)9.2 & 37.2\(\) & 28.5\(\)1.9 & 34.5\(\)1.29 \\
**FDS** & 49.1\(\)0.6 & 58.4\(\)0.4 & 62.1\(\)0.3 & 64.5\(\)0.2 & 12.1\(\)0.2 & 17.6\(\)0.3 & 23.5\(\)0.53 & 16.53\(\)0.65 & 23.45\(\)0.57 & 28.76\(\)0.36 \\
**K-Means** & 52.3\(\)0.4 & 60.7\(\)0.4 & 64.9\(\)0.3 & 67.8\(\)0.3 & 13.6\(\)0.52 & 19.1\(\)0.47 & 23.0\(\)0.69 & 22.98\(\)0.99 & 29.26\(\)0.82 & 34.47\(\)0.61 \\ 
**Corset** & - & 61.5\(\)0.3 & 66.0\(\)0.3 & 69.0\(\)0.2 & - & 20.25\(\)0.44 & 23.68\(\)0.62 & - & 28.37\(\)0.71 & 35.12\(\)0.55 \\
**VALL** & - & 61.2\(\)0.5 & 65.7\(\)0.4 & 68.9\(\)0.3 & - & 20.5\(\)0.75 & 23.57\(\)0.88 & - & 29.97\(\)0.66 & 35.2\(\)0.35 \\
**LearnLoss** & - & 60.8\(\)0.6 & 64.9\(\)0.4 & 68.9\(\)0.3 & - & 19.7\(\)1.05 & 22.41\(\)1strength as a general sample selection method. As a flexible framework, there is still considerable scope for improvement in our method by further designing it based on task-specific features.

### Analysis

Data Selection Efficiency.Traditional active learning methods typically follow a paradigm involving multiple iterations of selection and training. However, this approach not only demands substantial computational resources due to repeated training but also results in prolonged data selection times and increased management costs associated with multiple annotation processes in practical scenarios. In contrast, our method, alongside ActiveFT , adopts a one-shot selection strategy, offering distinct advantages. Tab. 3 presents a comparison of the time spent in selecting varying proportions of data across different methods on CIFAR100. For a deeper understanding of the theoretical time complexity and further analysis, please refer to Appendix D.

Selected Samples Visualization.Fig. 3 illustrates the sample selection process of our method. Firstly, we identify the central samples represented by pentagrams, and then expand to the boundary points denoted by circles from each center. Our method focuses on the boundaries between two categories rather than purely fitting the entire distribution. This approach allows for the selection of more valuable samples. For a more detailed comparison with other methods and the visualization of denoising process, please refer to the Appendix J.

### Ablation Study

Effectiveness of Designs.Tab. 4 demonstrates the contributions of all proposed components outlined in Sec. 3.3 to the model performance. Our framework is structured around three core components: Denoising Process, Selection Criterion, and Selection Process. IDs 1 to 3 evaluate the impact of the Denoising Process, comparing iterative density-based clustering with other strategies detailed in Appendix C, and scenarios without denoising. We observe that performance degradation is substantial under smaller budgets but diminishes as the budget increases. This suggests that with fewer data selected, the adverse effects of noisy samples are more pronounced; however, with an increased data volume, these deficiencies are mitigated, underscoring the efficacy of our denoising strategy. ID 4 explores the impact of our Selection Criterion design. In the absence of the Boundary Score metric,

    &  &  &  &  \\  & DG & DB & IDC & BD & BS & OS & ISR & OP & \(1\%\) & \(2\%\) & \(5\%\) & \(10\%\) \\  BILAF &. &. & ✓ &. & ✓ &. & ✓ & ✓ & **31.82** & **43.48** & **62.75** & **73.67** \\ 
1 &. & ✓ &. &. & ✓ &. & ✓ & ✓ & \(31.10(-0.72)\) & \(41.06(-.42)\) & \(61.60(-.115)\) & \(73.39(-.28)\) \\
2 & ✓ &. &. &. & ✓ &. & ✓ & ✓ & \(29.27(-.25)\) & \(36.42(-.706)\) & \(61.16(-.15)\) & \(72.87(-.80)\) \\
3 &. &. &. &. & ✓ &. & ✓ & ✓ & \(28.88(-.294)\) & \(36.78(-.70)\) & \(60.83(-.192)\) & \(72.76(-.091)\) \\ 
4 &. &. & ✓ & ✓ &. &. & ✓ & ✓ & \(27.94(-.38)\) & \(33.76(-.92)\) & \(54.03(-8.72)\) & \(71.17(-.20)\) \\ 
5 &. &. & ✓ &. & ✓ &. & ✓ &. & **32.13**\((+0.31)\) & \(43.09(-.39)\) & \(62.07(-.68)\) & \(72.92(-.075)\) \\
6 &. &. & ✓ &. & ✓ & ✓ &. &. & 30.52 (\(-1.30)\) & \(42.13(-1.35)\) & \(60.24(-.251)\) & \(69.66(-.01)\) \\   

Table 4: **Ablation on CIFAR100. In the denoising process,“DG”, “DB” and “IDC” indicate the basic distance-guide method, density-based method and iterative density-based clustering method. In the selection criterion,“BD”, “BS” indicate the basic distance metric and the boundary score metric. In the selection process,“OS”, “ISR”, “OP” indicate selecting the top samples in one shot, iterative selection and removal and whether to use opponent penalty. BILAF represents the complete implementation and we explore the influence of three designs separately.**

Figure 3: **tSNE Embeddings on CIFAR10 with 1% annotation budget of BiLAF.** Pentagrams represent the chosen core samples, while circles denote the chosen boundary samples.

  
**Ratio** & **K-Means** & **Concept** & **val.** & **LearLearLoss** & **ActiveFT** & **BILAF(sum)** \\  \(2\%\) & 16.6s & 18.75m & 78.52m & 20m & **12.6s** & 18.6s \\ \(5\%\) & 37.0s & 70.4s & 70.4s & 120.13m & 18.37m & 21.9s & **19.2s** \\ \(10\%\) & 70.2s & 200.3s & 360.24m & 90.69m & 37.3s & **20.3s** \\   

Table 3: **Time compelxity.** Our method exhibits significant advantages beyond conventional active learning approaches and comparable speed to ActiveFT.

models struggle to accurately identify potential boundary samples and are negatively impacted by irrelevant marginal samples, significantly reducing performance. IDs 5 and 6 assess the effects of our Selection Process design. While removing the opponent penalty does not critically undermine performance and might even enhance it at a 1% data volume, iterative selection and removal are essential, particularly as data volume increases. These steps effectively increase the diversity of boundary samples selected. Conversely, a one-time selection approach at increased data volumes leads to the accumulation of redundant samples, resulting in wastage and performance degradation.

Core Samples Selection Number.Without ground truth labels, ensuring that each pseudo-class center selected during the core sample selection stage of our BiLAF method represents a distinct category is challenging. Tab. 5 illustrates the impact of varying the number of core samples on accuracy across different annotation budgets within the CIFAR100 dataset. We found that performance significantly suffers when fewer centers are selected. For instance, selecting 125 centers for 100 categories resulted in suboptimal performance, primarily due to the limited number of centers being unable to represent all categories adequately. This limitation poses significant challenges for subsequent boundary sample selection. However, performance stabilizes once a sufficient number of centers to encompass all categories is established. Optimizing the ratio of center samples to boundary samples can yield the performance gains. For example, the best performance at 5% data volume was achieved with 375 centers, and at 10% data volume, 500 centers were optimal. Although there is potential to further enhance our method, we maintained the number of centers across all experiments with different annotation ratio for consistency.

Core Samples Selection Method.In the core sample selection, we primarily utilized ActiveFT. However, there are numerous existing methods, such as Random, FDS, and K-Means. Tab. 6 presents the model performance based on boundary selection using different pseudo-class centers. We found that the accuracy of the BiLAF framework is closely tied to the quality of the method used for selecting pseudo-class centers. Mis-selection of centers can introduce significant bias, adversely affecting subsequent sample selection. Notably, ActiveFT tended to yield the highest performance, while the traditional K-Means method also demonstrated strong results and outperform ActiveFT even in 10% budget, validating the robustness of our framework with well-defined centers.

Hyperparameter Influence.Our primary handcrafted parameters include the removal rate of noise samples during the denoising process and the opponent penalty applied during the selection process to penalize boundaries within the same class. Fig. 4 examines the impact of varying these parameters on our model's performance on the CIFAR100 dataset, with annotation budgets of 2% and 5%. We observe that the removal rate significantly influences performance. A low setting allows excessive noise accumulation, while a high setting depletes boundary sample points as the budget increases, adversely affecting performance. In contrast, the opponent penalty exerts a more subtle effect and can modestly enhance model performance.

Threshold of Core Numbers.Tab. 5 illustrates the impact of different Core Numbers on performance. In practical applications, determining the optimal Core Number directly is a question worth exploring. Similarly, in traditional active learning, the effectiveness of different methods often varies with the scale of the data. The classic Coreset  seeks to cover all samples using selected points. However, ProbCover  highlights that Coreset struggles when the dataset is very small, prompting the introduction of a coverage radius to ensure each selected point influences a fixed area.

  
**Budget** &  \\  & Random & FDS & K-Means & ActiveFT \\  \(1\%\) & 25.58 & 20.69 & 28.80 & **31.82** \\ \(2\%\) & 36.62 & 33.22 & 41.17 & **43.48** \\ \(5\%\) & 60.68 & 60.13 & 62.39 & **62.75** \\ \(10\%\) & 72.84 & 71.05 & **74.27** & 73.67 \\   

Table 6: **Ablation for Core Selection Method.**

Figure 4: **The Hyperparameter Influence.**

  
**Budget** &  \\  & 0.25\% / 125 & 0.50\% / 250 & 0.75\% / 375 & 1.00\% / 500 \\  \(1\%\) & 21.51 & **31.82** & 28.37 & 27.24 \\ \(2\%\) & 36.64 & **43.45** & 42.68 & 42.18 \\ \(5\%\) & 59.20 & 62.75 & **63.32** & 62.46 \\ \(10\%\) & 71.86 & 73.67 & 73.58 & **74.32** \\   

Table 5: **Ablation for Core Samples Numbers.**In our case, using central points to approximate dense distributions seems more appropriate, especially when the budget is highly constrained, without selecting additional boundary samples. However, as the budget increases, the selection of boundary samples becomes more meaningful. To further investigate, we explore what an appropriate threshold might look like. We employed the Core Sample Selection method to derive different numbers of central points and analyzed their benefits. Specifically, we define _Distance_ as the average Euclidean distance from each sample to its nearest selected sample in the feature space. Additionally, we examine the _Rate of Return_ (incremental benefit per core sample) within different ranges, where _Rate of Return_ = Distance Difference / Core Number Difference between two adjacent columns.

In Tab. 7, We found that the _Rate of Return_ diminishes gradually, indicating that core samples are crucial in the early stages, while the benefits decrease significantly later on. A clear demarcation point can serve as a guide for when to begin Boundary Sample Selection, such as the range of 250-375 for CIFAR-10 and 375-500 for CIFAR-100. This provides a simple yet effective guideline. Additionally, in practical applications, we discovered that introducing boundary points earlier may yield better results, such as CIFAR100 with 1% (500 samples) annotation samples.

Generality on Pretraining Frameworks and Model Architectures.Our method BiLAF demonstrates versatility across various pretraining frameworks and models. BiLAF has effectively integrated with the DINO  framework and the DeiT-Small  model. Here, we apply the method to a DeiT-Small  trained with generative unsupervised pretraining framework iBOT  and CNN model ResNet50  trained with DINO . All models are pretrained on ImageNet-1k and finetuned on CIFAR10 with other same implementation details described in Appendix E. Tab. 8 highlights our approach's substantial improvement over other sample selection baseline across different sampling ratios, illustrating our method's broad applicability to diverse pretraining strategies and model types.

## 5 Conclusion

In this paper, we underscore the significance of active finetuning tasks and critically examine existing methods, which often overlook uncertainty aspects, particularly under the pretraining-finetuning paradigm. We propose an innovative solution: the Bi-Level Active Finetuning Framework (BiLAF). This framework not only ensures diversity in the selection of central points but also prioritizes boundary samples with higher uncertainty. BiLAF effectively amalgamates existing core sample selection models and introduces a novel strategy for boundary sample selection in unsupervised scenarios. Our extensive empirical studies validate BiLAF's effectiveness, demonstrating its capability to enhance predictive performance. Through comparative experiments, we explore new avenues, such as finding the optimal balance between central and boundary points. We believe our work offers valuable insights into Active Finetuning and will serve as a catalyst for further research in this field.

  
**Core Num** & **50** & **100** & **150** & **250** & **375** & **500** & **1000** & **1500** & **2500** & **5000** \\   \\
**Distance** & 0.7821 & 0.7588 & 0.7472 & 0.7307 & 0.7203 & 0.7117 & 0.6896 & 0.6746 & 0.6506 & 0.6010 \\
**Rate of Return**\( 10^{-4}\) & - & 4.6575 & 2.3264 & 1.6422 & 0.8362 & 0.6887 & 0.4420 & 0.3002 & 0.2398 & 0.1984 \\   \\
**Distance** & 0.8378 & 0.8082 & 0.7913 & 0.7724 & 0.7564 & 0.7478 & 0.7229 & 0.7059 & 0.6800 & 0.6272 \\
**Rate of Return**\( 10^{-4}\) & - & 5.9221 & 3.3791 & 1.8906 & 1.2797 & 0.6845 & 0.4985 & 0.3404 & 0.2589 & 0.2112 \\   

Table 7: **Threshold of Core Numbers. Distance and Rate of Return on CIFAR10 and CIFAR100.**

    & DeiT-S Pretained with iBOT & ResNet50 Pretrained with DINO \\  & \(1\%\) & \(2\%\) & \(1\%\) & \(2\%\) \\ 
**Random** & 83.0 & 89.8 & 76.2 & 83.7 \\
**CoreSet** & 82.8 & 89.2 & 70.4 & 83.2 \\
**LearnLoss** & 83.6 & 89.2 & 71.7 & 81.3 \\
**VAAL** & 85.1 & 89.3 & 75.0 & 83.3 \\ 
**ActiveFT** & 88.3 & 90.9 & 78.6 & 84.9 \\
**BiLAF(Ours)** & **89.1** & **92.2** & **79.3** & **85.8** \\   

Table 8: **Performance on Different Pretraining Frameworks and Models on CIFAR10.**