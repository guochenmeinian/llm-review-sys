# MMGP: a Mesh Morphing Gaussian Process-based machine learning method for regression of physical problems under non-parameterized geometrical variability

MMGP: a Mesh Morphing Gaussian Process-based machine learning method for regression of physical problems under non-parameterized geometrical variability

Fabien Casenave Brian Staber Xavier Roynard

Safran Tech, Digital Sciences & Technologies

78114 Magny-Les-Hameaux, France

{fabien.casenave, brian.staber, xavier.roynard}@safrangroup.com

###### Abstract

When learning simulations for modeling physical phenomena in industrial designs, geometrical variabilities are of prime interest. While classical regression techniques prove effective for parameterized geometries, practical scenarios often involve the absence of shape parametrization during the inference stage, leaving us with only mesh discretizations as available data. Learning simulations from such mesh-based representations poses significant challenges, with recent advances relying heavily on deep graph neural networks to overcome the limitations of conventional machine learning approaches. Despite their promising results, graph neural networks exhibit certain drawbacks, including their dependency on extensive datasets and limitations in providing built-in predictive uncertainties or handling large meshes. In this work, we propose a machine learning method that do not rely on graph neural networks. Complex geometrical shapes and variations with fixed topology are dealt with using well-known mesh morphing onto a common support, combined with classical dimensionality reduction techniques and Gaussian processes. The proposed methodology can easily deal with large meshes without the need for explicit shape parameterization and provides crucial predictive uncertainties, which are essential for informed decision-making. In the considered numerical experiments, the proposed method is competitive with respect to existing graph neural networks, regarding training efficiency and accuracy of the predictions.

## 1 Introduction

Many problems in science and engineering require solving complex boundary value problems. Most of the time, we are interested in solving a partial differential equation (PDE) for multiple values of input parameters such as material properties, boundary conditions, initial conditions, or geometrical parameters. Traditional numerical methods such as the finite element method, finite volume method, and finite differences require fine discretization of time and space in order to be accurate. As a result, these methods are often computationally expensive, especially when the boundary value problem needs to be repeatedly solved for extensive exploration of the input parameters space. To overcome this issue, machine and deep learning have been leveraged for various tasks in computational physics, namely, solving and learning solutions to PDEs [37; 56; 65; 81], accelerating linear solvers [5; 34], reduced-order modeling , domain decomposition , closure modeling , and topology optimization , to name a few. As reported in the review papers [13; 17; 75], most of the recent advances have been relying on deep neural networks for their flexibility and expressiveness. In this work, we focus on learning simulations of physical phenomena, that are discretized on a non parameterized unstructured mesh. In this situation, traditional machine learning approaches cannot easily be leveraged as the inputs of the problem are given by graphs with different numbers of nodes and edges. In contrast, deep learning models such as graph neural networks (GNNs)  can easily overcome this limitation thanks to their ability to operate on meshes with different resolutions and topologies. While GNNs show promising results and their flexibility is highly appealing, they still suffer from a few shortcomings that prevent their deployment in engineering fields where decisions involve high stakes. Training GNNs usually requires large datasets and computational resources, and predicting their uncertainties is still an open and challenging problem of its own .

We propose a novel methodology, called Mesh Morphing Gaussian Process (MMGP), that relies on standard and well-known morphing strategies, dimensionality reduction techniques and finite element interpolation for learning solutions to PDEs with non-parameterized geometric variations. In contrast to deep learning methods, such as GNNs, the model can easily and efficiently be trained on CPU hardware and predictive uncertainties are readily available. Our method shares some limitations with any machine learning regressor for PDE systems: (i) within the predictive uncertainties, our method produces predictions with an accuracy lower than the reference simulations, (ii) unlike many methods used in reference simulators, like the finite element method, our method provides no guaranteed error bounds and (iii) our method requires a well sampled training dataset, which has a certain computational cost, so that the workflow becomes profitable only for many-query contexts where the inference is called a large number of times. Regarding (i), rough estimates may be sufficient in preproject phases, and accuracy can be recovered by using the prediction as an initialization in the reference simulator, or by allowing the designer to run the reference simulator on the identified configuration if the regressor is used in an optimization task.

We start by providing the background and assumptions of our method while mentioning some related works in Section 2. Then, the proposed methodology is detailed in Section 3. Three numerical experiments are presented in Section 4. Finally, a conclusion is given in Section 5.

## 2 Preliminaries and related works

Notations.Vectors and matrices are denoted with bold symbols. The entries \(i\) of a vector \(\) and \(i,j\) of a matrix \(\) are respectively denoted \(v_{i}\) and \(M_{i,j}\). The i-th row of a matrix \(\) is denoted by \(_{i}\).

Background.Let \(_{}:^{d}\) be a solution to a boundary value problem, where \(^{d_{}}\) denotes the physical domain of the geometry under consideration, and \(d_{}=2\) or \(3\). The domain \(\) is discretized into a conformal mesh \(\) as \(=_{e=1}^{N_{e}}_{e}\). In traditional numerical approaches such as the finite element method , an approximation \(\) of the solution \(_{}\) is sought in the finite-dimensional space spanned by a family of trial functions, \(\{_{I}()\}_{I=1}^{N}\), supported on the mesh \(\):

\[_{k}()=_{I=1}^{N}U_{k,I}_{I}()\,,  k=1,,d\,,\] (1)

where \(N\) is the total number of nodes in the mesh \(\), \(^{d N}\) is the discretized solution (featuring \(d\) fields), and \(^{d_{}}\) denotes the spatial coordinates. For simplicity of the presentation and without loss of generality, we consider the particular case of a Lagrange \(_{1}\) finite element basis, so that the solution is uniquely determined by its value at the nodes of \(\). In this setting, the basis \(\{_{I}\}_{I=1}^{N}\) spans the space \(\{v^{0}():\,v|_{_{e}}_{1},\, _{e}\}\), and the discretized solution \(\) is determined by solving the discretized weak formulation of the underlying boundary value problem. This problem also depends on some parameters \(^{p}\), such as material properties and boundary conditions. It is assumed that there are scalar output quantities of interest \(^{q}\) that depend on the discretized solution \(\), and possibly on \(\) and \(\). We restrict ourselves to stationary, time-independent, scalars and fields of interest, which still falls in the scope of many industrial problems of interest. The learning task that we consider herein consists in learning the mapping

\[:(,)(,).\] (2)

For this purpose, it is assumed that we are given a training set of size \(n\) made of input pairs \((^{i},^{i})\) of parameters and meshes, and output pairs \((^{i},^{i})\) of discretized fields and scalars. Each input mesh \(^{i}\) has a number of nodes denoted by \(N^{i}\), and corresponds to a finite element discretization of an input geometry \(^{i}\). The associated discretized solution \(^{i}\) is a matrix of size \((d N^{i})\). For any \(i=1,,n\), the mesh \(^{i}\) can be represented as an undirected graph \(G^{i}=(V^{i},E^{i})\), where \(V^{i}\) denotes the set of nodes and \(E^{i}\) is the set of edges.

Assumptions and limitations.We assume that the observed input geometries, \(^{1},,^{n}\), share a common topology. The parameterization that generates the input geometries is unknown, and we are left with the associated finite element meshes \(^{1},,^{n}\). Being the discretization of physical domains involved in a boundary value problem, the input meshes inherit important features such as boundary conditions applied to subsets of nodes and elements. In finite element methods, error estimates strongly depend on the quality of the mesh . Hence, in our context, it is assumed that the input meshes exhibit good quality in terms of elements aspect ratios and node densities, adapted to the regularity of the fields of interest. Our focus centers on the design optimization of industrial components with respect to specific physical phenomena. Consequently, we assume precise control over the geometry, free from any noise. Additionally, the employed geometrical transformations are constrained to avoid extreme distortions, as they are selected from sets of admissible designs that adhere to limitations on mass, volume, and mechanical resistance.

Related works.In recent years, there has been a substantial focus on advancing neural networks to emulate solutions to physical systems, either through the integration of domain-specific knowledge  or by devising efficient architectures for GNNs . GNNs learn the mapping \(\) by relying on the message passing framework introduced by Gilmer et al.  and extended by Battaglia et al. . In the context of physical systems, only a few contributions address non-parameterized geometric variabilities. The early work of Baque et al.  explores the use of GNNs to emulate physics-based simulations in the presence of geometric variabilities by relying on geodesic convolutions [58; 61]. More recently, Pfaff et al.  develop the MeshGraphNets (MGNs) model, a GNN that updates nodes and edges features in order to learn time-dependent simulations. Most notably, the model can handle various physics, three-dimensional problems, and non-parameterized geometric variabilities. Fortunato et al.  introduce MultiScale MGNs that relies on two different mesh resolutions in order to overcome the computational cost of the message passing algorithm on dense meshes, and to increase the accuracy of MGNs. The efficiency of MGNs has been illustrated by Allen et al.  for inverse problems, and by Hirsch et al.  for time-independent systems. There exist several variants of such GNNs for learning mesh-based solutions. A multi-scale GNN that learns from multiple mesh resolutions has been proposed in Lino et al.  and is illustrated on two dimensional PDEs with geometric variabilities. Lino et al.  also devise a multi-scale and rotation-equivariant GNN that extrapolates the time evolution of the fluid flow, and Cao et al.  propose a novel pooling strategy that prevents loss of connectivity and wrong connections in multi-level GNNs. Regarding morphing strategies, Gao et al.  and Li et al.  deform irregular meshes into a reference one in order to learn solution of PDEs, but rely on complex coordinate transformation to compute a physical residual-based loss in the reference domain, and on input meshes with equal number of nodes. It is worth emphasizing that while the aforementioned works show promising results, they do not provide predictive uncertainties. There exist several methods for quantifying the uncertainties of deep neural networks , but it remains an open problem to provide well calibrated uncertainty estimates at a reasonable computational cost.

## 3 MMGP methodology

The proposed methodology is based on two main ingredients that allow us to leverage classical machine learning methods for regression tasks in the context of non-parameterized geometrical variability: (i) the data is pretreated by morphing each input mesh into a reference shape, and resorting to finite element interpolation to express all fields of interest on a common mesh of this reference shape, and (ii) a low-dimensional embedding of the geometries is built by considering the coordinates of the nodes as a continuous input field over the meshes. Formally, the proposed methodology consists in constructing a graph kernel by relying on three well chosen transformations such that the transformed inputs can be compared with any classical kernel functions defined over Euclidean spaces. Figure 1 illustrates the proposed strategy for a two-dimensional problem where we aim at predicting output fields of interest. The first transformation morphs the input mesh onto a chosen common shape. The second transformation performs a finite element interpolation on the chosen reference mesh of the common shape. Finally, a dimensionality reduction technique is applied to obtain low-dimensional embeddings of the inputs and outputs. These three steps are all deterministic and described in the following subsections. The proposed kernel function can be plugged into any kernel method. Herein, we rely on Gaussian process regression in order to learn steady-state mesh-based simulations in computational fluid and solid mechanics.

### Deterministic preprocessings of the input meshes and fields of interest

In this section, we describe the methodology for building low-dimensional representations of the input meshes and output fields.

Mesh morphing into a reference shape \(\).Each input mesh \(^{i}\), \(i=1,,n\), is morphed onto a mesh \(}^{i}\) associated to a fixed reference shape \(\). The morphed mesh has the same number of nodes and same set of edges as the initial mesh \(^{i}\), but their spatial coordinates differ. In this work, we consider two morphing algorithms, namely, Tutte's barycentric mapping  onto the unit disk, and the Radial Basis Function (RBF) morphing [9; 21] onto a chosen reference shape. Regardless of the morphing algorithm, physical features inherited from the boundary value problem are carefully taken into account. More precisely, points, lines and surfaces of importance in the definition of the physical problem are mapped onto their representant on the reference shape. Doing so, rigid body transformations that may occur in the database are corrected in the mesh morphing stage, and boundary conditions of same nature are matched together.

Common mesh \(}_{c}\) of the reference shape \(\).At this stage, it should be noted that although the morphed meshes \(}^{i}\) are associated to a common reference shape \(\), they do not share the same nodes and edges. This prevents us from measuring similarities between the input meshes and output fields with classical techniques. A strong advantage of the finite element method is that it provides accurate solution fields with a continuous description over the mesh, and a natural way to transfer fields from one mesh to another. This motivates us to introduce a common mesh \(_{c}\) of the reference shape \(\), as a common support for all the sample fields data. A possibility is to choose an input mesh in the training set, e.g. \(^{1}\), and define \(}_{c}\) as its morphing onto the chosen reference shape. The aim is twofold. First, it allows us to express the output fields on the common morphed mesh \(}_{c}\), leading to vector representations of same sizes. Second, the coordinates fields of the meshes \(}^{i}\) are also transferred onto this common mesh in order to build a shape embedding. These procedures rely on classical finite element interpolation that is described in the rest of this section.

Transporting the fields of interest on the common mesh \(}_{c}\).The discretized solution \(^{i}\), \(i=1,,n\), is first transferred on the morphed mesh \(}^{i}\) as follows:

\[}^{i}_{k}(})=_{I=1}^{N^{i}}U^{i}_ {k,I}^{i}_{I}(})\,, k=1,,d\,,\] (3)

where \(\{^{i}_{I}\}_{I=1}^{N^{i}}\) is the finite element basis associated to the morphed mesh \(}^{i}\). The transported fields \(}^{1}_{k},,}^{n}_{k}\) share the same geometric support (the reference shape). This implies that they can be

Figure 1: Illustration of the MMGP inference workflow for the prediction of an output field of interest. The lower rectangle in the illustration of the input of the GP represents the scalar inputs \(^{i}\).

interpolated onto the common mesh \(}_{c}\) using the finite element interpolation operator \(P\) defined as:

\[P(}^{i}_{k})(})=_{J=1}^{N_{c}} }^{i}_{k}(}^{c}_{J})^{c}_{J}(})=_{I=1}^{N^{i}}_{J=1}^{N_{c}}U^{i}_{k, I}^{i}_{I}(}^{c}_{J})^{c}_{J}( })\,,\] (4)

where \(\{^{c}_{I}\}_{I=1}^{N^{i}}\) is the finite element basis associated to \(}_{c}\), \(}^{c}_{J}\) is the coordinates of the \(J\)-th node of \(}_{c}\) and \(}^{i}_{k}(}^{c}_{J})\) is evaluated using Equation (3). We are now in the much more favorable situation where all the fields of interest are expressed on a common mesh \(}_{c}\). More specifically, for each field of interest \(k\) and input mesh \(^{i}\), we let \(}^{i}_{k}^{N_{c}}\) be the transported output fields onto the common mesh, such that \(}^{i}_{k,I}=}^{i}_{k}(}^{c}_{J})\). In this setting, the vector representations of the output fields now have the same sizes \(N_{c}\). Notice that the derivation of the finite element interpolation is identical with higher-order Lagrange finite elements.

Transporting the coordinates fields on the common mesh \(}_{c}\).The same procedure can be applied to the coordinate fields of the input meshes in order to build a shape embedding of the input meshes. Let \(^{i}_{}\) be the \(\)-th component of the coordinate field over the mesh \(^{i}\), \(=1,,d_{}\). Using the finite element basis associated to the mesh \(^{i}\), the coordinates fields can be written as

\[^{i}_{}()=_{I=1}^{N^{i}}Z^{i}_{,I}^{i }_{I}()\,,\]

where \(Z^{i}_{,I}\) denotes the \(\)-th of the coordinates of the node \(I\) in the mesh \(^{i}\). Notice that the notation \(Z^{i}_{,I}\) is preferred to \(x^{i}_{,I}\), since it denotes here the degrees of freedom of the coordinate fields defined over \(^{i}\), whose continuity property is essential for the finite element interpolation stage. Then, in the same fashion as for the fields of interest, the coordinates fields are transferred on the morphed mesh \(}_{i}\) and interpolated on the common mesh \(}_{c}\) using the operator \(P\) given by Equation (4). For each coordinate \(=1,,d_{}\) and input mesh \(^{i}\), we have the common representations \(}^{i}_{}^{N_{c}}\) of the coordinate fields on the common mesh \(}_{c}\).

Dimensionality reduction.At this stage, the input coordinates fields of the meshes and the output fields are expressed on the same common mesh \(}_{c}\), and can be compared using standard machine learning techniques. We propose to build low-dimensional embeddings of these quantities using Principal Component Analysis (PCA). For each output field, PCA is applied to the set of observations \(\{}^{i}_{k}\}_{i=1}^{n}\), leading to the fields low-dimensional embeddings that we denote by \(\{}^{i}_{k}\}_{i=1}^{n}\). Similarly, PCA is applied to concatenated transported coordinate fields, \(\{(}^{i}_{1},,}^{i}_{d_{ }})\}_{i=1}^{n}\), leading to low-dimensional embeddings of the input geometries \(\{}^{i}\}_{i=1}^{n}\), that we refer to as the shape embeddings.

### MMGP training

Once the operations of mesh morphing, finite element interpolation on a common mesh and dimensional reduction described in the previous subsection have been carried out, we are left with reduced-size objects of same dimension. Let \(\{^{i}\}_{i=1}^{n}^{l+p}\), where \(p\) the number of nongeometrical parameters and \(l_{}\) is the size of shape embedding, be such that \(^{i}=(}^{i},^{i})\). Denoting \(l_{_{k}}\) the size of the embedding of field \(_{k}\), the machine learning task given by Equation (2) can be approximated by the following set of scalar and vector regression problems:

\[}_{,m}: ^{i} w^{i}_{m}, m=1,,q\,,\] (5a) \[}_{,k}: ^{i}}^{i}_{k}^{ l_{k}}\,, k=1,,d\,.\] (5b)

Gaussian processes can be trained in a classical fashion to address the regression problems (5a)-(5b).

MMGP for a scalar output.Let \(=\{(^{i},w^{i}_{m_{0}})\}_{i=1}^{n}\) be a training dataset for one of the problems given by Equation (5a), _i.e._ for the \(m_{0}\)-th output scalar. It can be shown by standard conditioning  that the posterior mean and variance of the prediction on some given test input \(^{}\) are given by

\[[w^{}] =^{T}_{}(+^{2})^{-1} _{m_{0}}\,,\] \[[w^{}] =K_{}-^{T}_{}(+^{2} )^{-1}_{}\,,\]where \(_{m_{0}}=\{w^{i}_{m_{0}}\}_{i=1}^{n}\), and \(\) is the Gram matrix such that \(K_{i,j}=c(^{i},^{j})\) for \(1 i,j n\), the vector \(_{}\) such that \(k_{_{j}}=c(^{},^{j})\), and the scalar \(K_{}=c(^{},^{})\), with \(c\) denoting the chosen kernel function which lengthscales are optimized, and \(\) denotes the optimized nugget parameter. This training procedure is repeated for the \(q\) scalar outputs.

MMGP for an output field.Let \(=\{(^{i},}^{i}_{k_{0}})\}_{i=1}^{n}\) be a training dataset for one of the problems given by Equation (5b), _i.e._ for output field \(k_{0}\). A multioutput GP is first trained to predict the output embeddings \(}_{k_{0}}\). The predictions of the GP are then decoded with the inverse PCA mapping, and morphed back to the original input mesh \(^{i}\). Due to this last nonlinear operation, the posterior distribution of the output field of interest \(_{k_{0}}\) is no longer Gaussian. The predictive uncertainties are thus obtained through Monte Carlo simulations. This training procedure is repeated for each of the \(d\) output fields.

### Properties of the methodology

The sequence of preprocessing operations, including mesh morphing, finite element interpolation, and PCA, leads to a non-linear dimensionality reduction. Leveraging these deterministic processes reduces the burden on the machine learning stage, potentially necessitating fewer training examples to achieve robust model performance on complex mesh-based data. In the numerical experiments presented in Section 4, the morphing technique is chosen a priori, with ongoing research focused on optimizing this morphing to minimize the number of PCA modes, which leads to a highly nonlinear dimensionality reduction stage that is finely tuned to the specific characteristics of the data.

Gaussian process regression between the input and output embeddings has several advantages. From a theoretical perspective, there exists conditions on the features of a continuous kernel so that it may approximate an arbitrary continuous target function . Gaussian processes also come with built-in predictive uncertainties, that are marginally valid under the a priori Gaussian assumption. Nevertheless, the proposed methodology can be combined with any other regressor such as a deep neural network instead of the Gaussian process.

For clarity of the presentation, the MMGP methodology is illustrated with very simple, if not the simplest, morphing and dimensionality reduction techniques. Alternatives are possible for each algorithm brick. In particular, the fixed topology restriction may be lifted with other morphing algorithms, see Appendix B for more details.

## 4 Numerical experiments

Three regression problems are considered in order to assess the efficiency of the proposed methodology. The chosen datasets are first described in Section 4.1. The experimental setup is summarized in Section 4.2, and the results are discussed in Section 4.3.

### Datasets

Three datasets in computational fluid and solid mechanics are considered, described below, and summarized in Table 1. All the considered datasets involve geometric variabilities and meshes with possibly different number of nodes and edges. Additional details can also be found in Appendix A.

Rotor37 dataset.We consider a 3D compressible steady-state Reynold-Averaged Navier-Stokes (RANS) simulation solved with elsA  using the finite volumes method. The inputs are given

   Datasets & train/test sizes & \(d_{}\) & \(p\) & \(d\) & \(m\) & Avg. \(\#\) nodes \\  Rotor37 & \(1000/200\) & \(3\) & \(2\) & \(2\) & \(4\) & \(29,773\) \\ Tensile2d & \(500/200\) & \(2\) & \(6\) & \(6\) & \(4\) & \(9,425\) \\ AirfRANS & \(800/200\) & \(2\) & \(2\) & \(3\) & \(2\) & \(179,779\) \\ AirfRANS-remeshed & \(800/200\) & \(2\) & \(2\) & \(3\) & \(2\) & \(19,527\) \\   

Table 1: Summary of the considered datasets with \(d_{}\): dimension of the physical problem, \(p\): number of input scalars, \(d\): number of output fields, \(m\): number of output scalars.

by a mesh representing the surface of a 3D compressor blade , and two additional parameters that correspond to an input pressure and a rotation speed. The outputs of the problem are given by \(4\) scalars (massflow \(m\), compression rate \(\), isentropic efficiency \(\), polyentropic efficiency \(\)), and \(2\) fields on the surface of the input mesh (temperature \(T\), pressure \(P\)).

Tensile2d dataset.The second dataset corresponds to a 2D quasi-static problem in solid mechanics. The geometrical support consists of a 2D square, with two half-circles that have been cut off in a symmetrical manner. The inputs are given by a mesh, a pressure applied on the upper boundary, and \(5\) material parameters modeling the nonlinear elastoviscoplastic law of the material . The boundary value problem is solved with the finite element method and the Z-set software . The outputs of the problem are given by \(4\) scalars (\(p_{}\), \(v_{}\), \(_{22}^{}\), and \(_{v}^{}\)) and \(6\) fields of interest (\(u\), \(v\), \(p\), \(_{11}\), \(_{12}\), and \(_{22}\)). For the sake of brevity, the reader is referred to Appendix A for a description of these quantities.

AirfRANS dataset.The last dataset is made of 2D incompressible RANS around NACA profiles and taken from Bonnet et al. . The inputs are given by a mesh of a NACA profile and two parameters that correspond to the inlet velocity and the angle of attack. The outputs are given by \(2\) scalars (drag \(C_{D}\) and lift \(C_{L}\) coefficients), and \(3\) fields (the two components of the fluid velocity \(u\) and \(v\), and the pressure field \(p\)). An additional version of this dataset is also considered, where the input meshes have been coarsened using the MMG remesher , and the output fields have been transferred to the coarsened meshes. The output scalars are unchanged. Illustrations of the input meshes can be found in the original paper .

### Experimental setup

Morphings.The Tutte's barycentric mapping onto the unit disk is used for morphing the meshes in the Rotor37 and Tensile2d datasets. The input meshes in the AirfRANS dataset are morphed onto the first mesh using RBF.

PCA embeddings.Embeddings of sizes \(32\) and \(64\) are retained for respectively the spatial coordinates and output fields in the Rotor37 and AirfRANS datasets. Smaller embeddings of sizes \(8\) are considered for both the spatial coordinates and output fields in the Tensile2d dataset. Note that for the Tensile2d and AirfRANS, a more effective variant of PCA has been used, which can easily deal with very large meshes (see, _e.g._, [24; 36]), with up to hundreds of millions of degrees of freedom. Details about this variant can be found in Appendix C.

Gaussian processes.Anisotropic Matern-5/2 kernels and zero mean functions are used for all the Gaussian processes priors. The lengthscales and nugget parameter are optimized by maximizing the marginal log-likelihood function with a L-BFGS algorithm and \(10\) random restarts using the GPy package .

Baselines.The performance of MMGP is compared with two baselines, namely, a graph convolutional neural network (GCNN) with a UNet-type architecture  and the GeneralConv , and MeshGraphNets (MGN) . The hyperparameters of the GNNs are chosen by relying on a grid search. The GCNN and MGN models are implemented with PyTorch Geometric and DGL, respectively. Additional details about the architectures and hyperparameters can be found in Appendix D. Due to the sizes of the input meshes in the AirfRANS dataset, the considered GNN-based baselines are prohibitively expensive. Similarly to the work of , the GNNs are trained using coarsened input meshes as described in Section 4.1. The output fields predicted on the coarse meshes are then transferred back on the original fine meshes thanks to finite element interpolation.

Evaluation metrics.Accuracy of the trained models is assessed by computing relative RMSE errors. Let \(\{_{}^{i}\}_{i=1}^{n_{*}}\) and \(\{_{}^{i}\}_{i=1}^{n_{*}}\) be respectively test observations and predictions of a given field of interest. The relative RMSE considered herein is defined as

\[_{f}(_{},_{})= (}_{i=1}^{n_{*}}}\|_{ }^{i}-_{}^{i}\|_{2}^{2}}{\|_{ }^{i}\|_{}^{2}})^{1/2}\,,\]

[MISSING_PAGE_FAIL:8]

arbitrary test input mesh of the Rotor37 experiment, together with the predictive variance and the point-wise relative absolute error. High relative errors are localized where the pressure field exhibits a discontinuity, known as a shock in compressor aerodynamics. The predictive variance is also higher near this region, reflecting that the GP-based surrogate model is uncertain about its prediction of the shock position. Figure 5 shows graphs of two output scalars with respect to the input pressure in the Tensile2d problem. The predictive intervals of the MMGP model are discriminative: they get wider as the input pressure falls out of the support of the training distribution. In order to assess the validity of the prediction intervals, we compute the prediction interval coverage probability (PICP), _i.e._ the average of test targets that fall into the \(95\%\) prediction interval. For the AirfRANS dataset, PICPs of \(93.05\%\) and \(93.5\%\) for respectively the outputs \(C_{L}\) and \(C_{D}\) are obtained by averaging the individual PICPs of \(10\) independent MMGP models. The prediction intervals are slightly over-confident but this could be corrected by _e.g._ conformalizing the Gaussian process .

Figure 4: (Rotor37) MMGP: prediction, predictive variance, and \(L^{1}\) relative error of the pressure field for an arbitrary geometry in the test dataset.

Figure 3: (AirfRANS) Test sample 787, fields of interest \(u\) (\(UX\)), \(v\) (\(UY\)) and \(p\): (left) reference, (middle) MMGP prediction, (right) relative error.

Figure 2: (Tensile2d) Test predictions versus test targets obtained for the output scalars of interest.

Computational times.The MMGP model can easily be trained on CPU hardware and with much lower computational times, see Table 3.

## 5 Conclusion

In summary, our proposed method presents an innovative approach to approximating field and scalar quantities of interest within the context of solving complex physics problems for design optimization. Our work introduces two key contributions: firstly, the utilization of mesh morphing pretreatment in conjunction with finite element interpolation, and secondly, the incorporation of shape embedding through dimensional reduction of coordinates, treating them as continuous fields over the geometric support. These innovations alleviate the machine learning task from the challenges of handling variable-sized samples and the need to learn implicit shape embedding. By reducing the dimensionality of inputs and outputs, our approach allows for the application of efficient Gaussian process regression. Notably, our MMGP model exhibits several key advantages. It can seamlessly handle very large meshes, is amenable to efficient CPU training, is fairly interpretable, demonstrates high accuracy in our experimental results, and provides readily available predictive uncertainties.

Future works will explore the extension of our method to accommodate time-dependent quantities of interest and investigate the optimization of the morphing process to enhance data compression and overall performance. Our research opens exciting avenues for advancing the capabilities of machine learning in the realm of physics-based design optimization.

Figure 5: (Tensile2d) MMGP: graphs of the predicted \(v_{}\) and \(_{v}^{}\) with respect to the pressure, for four different test input meshes, and \(11\) values of input pressure that go beyond the training range \((-50,-40)\), with \(95\%\) confidence intervals.

   Dataset & GCNN & MGN & MMGP \\  Rotor37 & \((200)\) 24 h & (6 \(\)) 13 h 14 min & \((10)\)**2 min 49 s** \\ Tensile2d & \((200)\) 1 h 25 min & \((6)\) 6 h 50 min & \((10)\)**1 min 38 s** \\ AirfRANS & \((200)\) 5 h 15 min & \((6)\) 5 h 00 min & \((10)\)**5 min 47 s** \\   

Table 3: Training computational times: GCNN and MGN on a Nvidia A100 Tensor Core GPU (neural network training), MMGP on a 48 cores Intel Xeon Gold 6342 CPU (Gaussian process regressors training). Between parenthesis are indicated the numbers of trainings carried-out to optimize hyperparameters (best is **bold**).