# Incentives in Federated Learning: Equilibria, Dynamics, and Mechanisms for Welfare Maximization

Aniket Murhekar\({}^{1}\)   Zhuowen Yuan\({}^{1}\)   Bhaskar Ray Chaudhury\({}^{1}\)   Bo Li\({}^{1}\)   Ruta Mehta\({}^{1}\)

\({}^{1}\)University of Illinois, Urbana-Champaign

{aniket2,zhuowen3,bryacha,lbo,rutameht}@illinois.edu

###### Abstract

Federated learning (FL) has emerged as a powerful scheme to facilitate the collaborative learning of models amongst a set of agents holding their own private data. Although the agents benefit from the global model trained on shared data, by participating in federated learning, they may also incur costs (related to privacy and communication) due to data sharing. In this paper, we model a collaborative FL framework, where every agent attempts to achieve an optimal trade-off between her learning payoff and data sharing cost. We show the existence of Nash equilibrium (NE) under mild assumptions on agents' payoff and costs. Furthermore, we show that agents can discover the NE via best response dynamics. However, some of the NE may be bad in terms of overall welfare for the agents, implying little incentive for some fraction of the agents to participate in the learning. To remedy this, we design a _budget-balanced mechanism_ involving _payments_ to the agents, that ensures that any \(p\)-mean welfare function of the agents' utilities is maximized at NE. In addition, we introduce a FL protocol FedBR-BG that incorporates our budget-balanced mechanism, utilizing best response dynamics. Our empirical validation on MNIST and CIFAR-10 substantiates our theoretical analysis. We show that FedBR-BG outperforms the basic best-response-based protocol without additional incentivization, the standard federated learning protocol FedAvg (McMahan et al. (2017)), as well as a recent baseline MWFed (Blum et al. (2021)) in terms of achieving superior \(p\)-mean welfare.

## 1 Introduction

Federated Learning (FL) has emerged as an effective collaborative training paradigm, where a group of agents can jointly train a common machine learning model. The success of collaborative learning paradigms is visible in the domains of autonomous vehicles (Elbir et al. (2020)), digital healthcare (Dayan et al. (2021); Xu et al. (2021); Nvidia (2019)), multi-devices (Learning (2017)), and biology (Bergen and Petryshen (2012)). Although such collaborative learning is immensely beneficial to the agents, individually, they may not be incentivized to share their data. This is because sharing data may incur costs attributed to bandwidth use, privacy leakage, and the use of computing resources. In turn, high costs and low learning payoffs may cause some of the agents to drop out of FL, resulting in subpar learning. As noted by Blum et al. (2021), the success of FL systems depends on its ability to attract and retain a large number of federating agents.

_Thus, it is crucial to achieve fairness and welfare guarantees for all participating agents._

This calls for game-theoretic modeling and analysis of the agent's payoff and costs, and subsequent mechanism design to incentivize participation. Towards the former, Blum et al. (2021) introduce a model, where every agent receives a _payoff_ from the collaboration measuring the "learning benefit" the agent derives from the total data shared. In particular, when each agent \(i\) contributes \(s_{i}\) units of data, the payoff for agent \(i\) is captured by \(a_{i}()\), where \(= s_{1},s_{2},,s_{n}\) is the data contributionprofile, or _sample vector_. Blum et al. (2021) consider the _constrained cost-minimizing model_, where each agent minimizes her data contribution (\(s_{i}\)) subject to the requirement that her payoff should be larger than a threshold (\(a_{i}(s_{i},_{-i})_{i}\)). They show that a Nash equilibrium (NE) (Nash (1951)), arguably the most sought after solution concept in game theory, may not always exist in this model. They derive sufficient conditions for existence of NE, as well as provide novel structural results about the equilibria.

We note that an agent's cost for sharing data may be more complex than just the size of the total data shared. There are studies dedicated to quantifying the losses (attributed to data collection, processing, and privacy) that are incurred with increased sharing of data (Li and Raghunathan (2014); Laudon (1996); Jaisingh et al. (2008)). Furthermore, within game theory (and economics) typically agents are considered as net utility maximizers, _i.e.,_ maximizing payoff minus cost, for example value minus payment in auctions (Myerson (1981)), and revenue minus cost in markets (Huber et al. (2001); Borgers (2015)). To capture these aspects, in this paper we propose an _unconstrained utilitarian model_, where we define the _utility_\(u_{i}()\) of each agent \(i\) as the difference between her payoff and her cost of sharing data, i.e., \(u_{i}()=a_{i}()-c_{i}(s_{i})\), where \(c_{i}(s_{i})\) denotes the cost incurred by agent \(i\) on sharing \(s_{i}\) data samples. Each agent aims to maximize their utility. This model is natural in FL settings where there is a natural coupling between payoffs and costs, and there are no hard cost constraints, i.e., having an upper bound on the total cost that an agent can incur. Since utility functions are directly indicative of the welfare for the agents, this facilitates defining global fairness and welfare criteria in terms of the utilities of the agents. This motivates the following questions:

When agents strategize on data contribution to maximize their net utility, does a Nash equilibrium always exist? If yes, then can agents actually _discover a Nash equilibrium_ when acting independently? Can any welfare function of utilities of the agents be optimized at such a Nash equilibrium by _designing new rules_ (_mechanisms_)?

The goal of this paper is to address the above questions. Before we describe our contributions, we note that Karimireddy et al. (2022) also consider a framework similar to ours, by defining the utility function to be payoff minus cost. However, their focus is _data maximization_ and avoiding _free-riding_, while the focus of our paper is to design mechanisms that achieve fairness and welfare for all agents. Furthermore, our model generalizes the model of Karimireddy et al. (2022) as we do not require the agents' data to be identically distributed and in turn to have identical payoff functions.

### Our Contributions

We work under the _concavity assumption_ on the utility functions. In particular, we assume that the payoff functions are concave and cost functions are convex. These assumptions are standard in the literature (Blum et al. (2021); Karimireddy et al. (2022)). Convexity of cost functions is a natural choice since it captures the property of increasing marginal costs. For instance, data sharing through _ordered selection_, i.e., sharing records in ascending order of costs involved for collecting the records, results in convex cost functions. There are more models that result in strictly convex cost functions (Li and Raghunathan (2014)). Similarly, several important ML models exhibit concave payoff functions; for instance payoffs in linear or random discovery models, random coverage models, and general PAC learning are all concave - see Section 2.1. Furthermore, there is empirical evidence that the accuracy function in neural networks under the cross-entropy loss is also concave (Kaplan et al. (2020)).

Existence and Reachability of Nash Equilibrium:Under the concavity assumptions, we show that a Nash equilibrium (NE) is guaranteed to exist. We note that our existence result holds for more general settings than those of Blum et al. (2021) - we do not assume that the cost functions are linear, or that the utility functions have bounded derivatives. In particular, our result shows the existence of NE under the general PAC learning framework (see Section 2.1); under this setting an equilibrium in the model of Blum et al. (2021) fails to exist. We also demonstrate that if the concavity assumption is relaxed, there exist instances which do not admit NE.

Furthermore, we show that the agents can _discover_ a NE through an intuitive _best response dynamics_, where agents update their data contribution proportional to the gradient of their utility functions in the direction that increases their utility. We show that for strongly concave utility functions and under the mild assumption of the gradient of the utilities being bounded, the best response dynamics convergesto an approximate NE in time polynomial in \(O((^{-1}))\). This contribution may be of independent interest to equilibrium computation in concave games.

A Fair and Welfare Maximizing Mechanism:In general, a Nash equilibrium need not be fair or maximize any notion of welfare for the agents (see Example 1). Therefore, we next focus on designing mechanisms that optimize welfare of the agents at its NE. The generalized \(p\)-mean welfare, defined to be the \(p^{th}\)-norm of the utilities of the agents \((_{i[n]}u_{i}()^{p})^{}\) for \(p(-,1]\), constitutes a family of functions characterized by natural fairness axioms including the Pigou-Dalton principle (Moulin (2003)). This notion encompasses well studied notions of fairness and economic welfare, such as (i) the average social welfare \((_{i[n]}u_{i}())\) when \(p=1\), (ii) the egalitarian welfare \((_{i[n]}u_{i}())\) when \(p=-\), which is a fundamental measure of fairness, and (iii) the Nash welfare \((_{i[n]}u_{i}())^{1/n}\) in the limiting \(p 0\) case, which is a popular notion in social choice theory that achieves a balance between welfare and fairness (Varian (1974); Caragiannis et al. (2019)).

As our second main contribution, for linear costs, we design a _budget-balanced mechanism_ that always admits a Nash equilibrium. The mechanism involves _payments_, however, budget-balancedness ensures that the total payment of all the agents is zero, and thereby the central server operates on no-profit-no-loss. Moreover we show that when the sample vector at NE is positive, i.e., all agents contribute positive amount of data samples, then the NE maximizes the \(p\)-mean welfare among all positive sample vectors. Since we can ensure the server does not communicate with an agent who does not contribute any data points, insisting on positive sample vectors is a mild assumption.

Our mechanism builds on ideas from a mechanism of Falkinger et al. (2000) for the efficient provisioning of public goods. The key idea is to compensate an agent who incurs a cost higher than the average cost incurred by other agents via a subsidy proportional to her excess cost; likewise, agents incurring a lower cost than average of others are proportionally taxed. By setting the level of tax/subsidy carefully, we show that the NE of the mechanism are \(p\)-mean welfare maximizing. To the best of our knowledge, our work is the first to explore this intimate connection between FL and public goods provisioning. Our results highlight the promise of bridging the fields of machine learning and social choice theory.

Once we have established the mechanism, we show that a corollary of our first main result implies that the _best response dynamics_ under this mechanism will lead to the discovery of the \(p\)-mean welfare maximizing Nash equilibrium.

Empirical Evaluation:We design a distributed training protocol for FL based on our mechanism, called FedBR-BG. We compare our algorithm with three other algorithms: the distributed protocol for the vanilla mechanism without budget balancing FedBR, the standard federated learning protocol FedAvg (McMahan et al. (2017)), and a recent adaptation of FedAvg called MWFed (Blum et al. (2021)). We show that our budget-balancing algorithm achieves superior \(p\)-mean welfare under different settings on two datasets, MNIST (LeCun et al. (2010)) and CIFAR-10 (Krizhevsky (2009)).

### Related Work

We mention some additional literature on welfare maximization and incentives in FL, as well as other related mechanisms in public goods theory.

Welfare maximization in FL.Typically federated learning protocols compute a model which maximize some weighted sum of agent accuracies (i.e. utilities). Examples of such protocols include the standard FedAvg (McMahan et al. (2017)), AFL (agnostic FL) (Mohri et al. (2019)), and \(q\)-FFL (Li et al. (2020)). However, unlike our work, all these methods ignore the strategic aspects arising from costs involved in data sharing, and instead assume agents honestly contribute all their data.

Incentives in FL.This line of work adopts game theory for incentivizing the contribution of data owners. Common models include the Stackelberg game, non-cooperative game, and coalition game. More specifically, the Stackelberg game is employed to optimize the utility of both the server and agents Feng et al. (2019). On the other hand, in non-cooperative games, the server or the agent seeks to maximize its own utility Zou et al. (2019). While most previous methods analyze the properties of a certain scenario, we aim to design mechanisms that achieve fairness and welfare for all agents.

Mechanisms for public good provisioning.There is a long line of work for the efficient provisioning of public goods, beginning from Samuelson (1954). Several works such as Falkinger et al. (2000); Andreoni and Bergstrom (1996) and Bergstrom et al. (1986) use the idea of imposing a tax/subsidy on the agents but differ in the specific manner in which this tax/subsidy is imposed. While our mechanism is inspired by Falkinger et al. (2000), our model is more general than theirs. The design of our mechanism for this general model and the proof of its properties are our novel contributions.

## 2 Problem formulation

We consider a federated learning problem with \(n\) agents who wish to jointly solve a common learning problem. Let \(_{i}\) be the distribution of data points available to agent \(i\). Towards jointly solving the learning problem, each agent \(i\) contributes some set \(T_{i}_{i}^{s_{i}}\) of \(s_{i}\) data samples. Under the assumption that agent \(i\)'s data is i.i.d. from their distribution \(_{i}\), each agent's contribution can be captured simply by their contribution level, i.e., amount of data samples they contribute. Let \(S_{i}_{ 0}\) be the set of feasible contribution levels of agent \(i\), and let \(:=_{j}S_{j}\). Given a _sample vector_\(=(s_{1},,s_{n})\), the central server returns model(s) trained using the samples \(_{i}T_{i}\).

In our model, each agent \(i\) derives a _payoff_ from the jointly learned model, e.g. the payoff could be the accuracy of the model. We assume a general framework which models the payoff of agent \(i\) as a function \(a_{i}:_{ 0}\). We typically assume each payoff function \(a_{i}\) is bounded, continuous in \(\), and non-decreasing in the contribution \(s_{i}\) of agent \(i\). Moreover each agent \(i\) incurs a _cost_ associated with data sharing captured through a non-decreasing cost function \(c_{i}:S_{i}_{ 0}\). The net utility of agent \(i\) is the payoff minus cost, _i.e.,_

\[u_{i}()=a_{i}()-c_{i}(s_{i}).\]

Given the above framework, the goal of an agent \(i\) is to decide how many samples to contribute so that her net utility \(u_{i}()\) is maximized. Note that the utility of agent \(i\) depends on the contributions of other agents as well. Further, we can assume without loss of generality that each set \(S_{i}=[0,_{i}]\) for some threshold \(_{i}>0\). This is because we can discard contribution levels where an agent gets negative utility. Since the payoff \(a_{i}()\) is bounded above by some constant \(A_{i}>0\) and costs are increasing, agent \(i\) cannot obtain a positive utility by contributing more than \(_{i}:=c_{i}^{-1}(A_{i})\) samples.

Payoff and cost functions are assumed to be concave and convex respectively (Blum et al. (2021); Karimireddy et al. (2022)). As discussed in Section 1.1, it is natural to assume that cost functions are convex to capture increasing marginal costs (Li and Raghunathan (2014)). Similarly, there are ample justifications to concave payoff functions, as discussed in Section 2.1 where we analyze payoff functions arising from some of the canonical learning paradigms, and their _concavity properties_.

_Remark 1_.: Our framework generalizes those of Blum et al. (2021) and Karimireddy et al. (2022). In the former, an agent's goal is to contribute the fewest number of data samples subject to ensuring that their payoff crosses a certain threshold; and in the latter all agents have a common payoff function that is a function of \( s_{1}=s_{1}++s_{n}\), and linear cost functions.

**Nash Equilibrium (NE).** Arguably the most sought after solution concept within game theory is of _Nash Equilibrium_(Nash (1951)), a stable state or an equilibrium state of the system where no agent gains by unilaterally changing their data contribution level. Formally,

**Definition 1** (Nash equilibrium (NE)).: A sample vector \(\) is said to be at a Nash equilibrium if for every \(i[n]\), and every \(s^{}_{i}\), we have \(u_{i}() u_{i}(s^{}_{i},_{-i})\) where \((s^{}_{i},_{-i})=(s_{1},,s^{}_{i},,s_{n})\).

An alternate view of a Nash equilibrium relies on the concept of _best response_. Given the sample contributions \(_{-i}\) of other agents, the set \(f_{i}(_{-i})\) of contribution levels of agent \(i\) that maximize her utility is the best response set of agent \(i\):

\[f_{i}(_{-i})=_{x S_{i}}\{a_{i}(x,_{-i})-c_{i}(x) \} S_{i}.\]

The _best response correspondence_\(f\) is then defined as a set-valued function \(f:_{i}2^{S_{i}}\), where \([f()]_{i}=f_{i}(_{-i})\). It is then clear that:

**Proposition 1**.: _A sample vector \(\) is a Nash equilibrium if and only if it is a fixed point of the best response correspondence, i.e., \( f()\)._

### Canonical examples of payoff functions

We now discuss a few examples of payoff functions that are captured by our general framework. In all the examples below, the payoff functions \(a_{i}()\) are non-negative, bounded, continuous, non-decreasing, and concave in \(s_{i}\) for any fixed strategy profile \(_{-i}\) of the other agents.

Linear or Random discovery.In this model, the payoff is linear in the sample vector and is given by \(a_{i}()=(W)_{i}\) for a matrix \(W\). For example, Blum et al. (2021) consider a setting where each agent has a sampling probability distribution \(_{i}\) over the instance space \(X\) and gets a reward equalling \(q_{ix}\) whenever the instance \(x\) is sampled by any agent. Then the expected payoff to agent \(i\) is \(a_{i}()=(QQ^{T})_{i}\), where \(Q\) is a matrix given by \(Q[i,x]=q_{ix}\) for \(i[n]\) and \(x X\). Here \(W=QQ^{T}\) is a symmetric PSD matrix with an all one diagonal.

Random coverage.In the above setting, agent \(i\) obtains a reward \(q_{ix}\) each time some agent samples instance \(x\). In the random coverage model arising in binary classification (Blum et al. (2021)), an agent gets this reward only once. Under this model, the payoff given by expected accuracy takes the form:

\[a_{i}()=1-_{x X}q_{ix}_{j=1}^{n}(1-q_{jx})^{s_{j} }.\] (1)

Generalization bounds from general PAC learning.Consider a general learning setting where we want to learn a model \(h\) from a hypothesis class \(\) which minimizes the error over a data distribution \(\) given by \(R(h)=_{(x,y)}e(h(x),y)\), for some loss function \(e()\). Given \(m\) i.i.d. data points, the empirical risk minimizer (ERM) can be computed as the model \(h_{m}=_{h}_{[m]}e(h(x_{}),y_{})\). Mohri et al. (2018) show the following bound on the error of \(h_{m}\), which holds with high probability:

\[1-R(h_{m}) a(m):=a_{0}-}{},\] (2)

where \((1-a_{0})\) is the accuracy of the optimal model from \(\), and \(k\) is a (constant) measure of the difficulty of the learning problem depending on \(e()\) and \(\). Using this we can define the agent payoff \(a_{i}\) function as the accuracy of the learning task as \(a_{i}()=a(_{1})\)1.

Empirical evidence.Kaplan et al. (2020) discuss empirical scaling laws relating the cross-entropy loss on neural language models. They observe that the loss scales with the dataset size \(m\) as a power law \((m)= m^{-}\), for some parameters \(>0\) and \((0,1]\). This naturally defines the payoff function as the accuracy of the learning task as

\[a_{i}()=1-_{i}_{1}^{-_{i}}.\] (3)

In addition, the pay-off functions of current large language models (e.g., accuracies) are also non-negative and non-decreasing as a function of the data size (Henighan et al. (2020)).

## 3 Nash Equilibrium: Existence and Best Response Dynamics

In this section, we investigate the existence and computation of a Nash equilibrium. We start with two positive results showing the existence of a Nash equilibrium for a broad class of payoff and cost functions.

**Theorem 3.1**.: _A Nash equilibrium exists in any federated learning problem where for every agent \(i\) the payoff function \(a_{i}()\) is continuous in \(\) and concave in \(s_{i}\), and cost function \(c_{i}\) is increasing and convex in \(s_{i}\)._

Proof.: We will show the existence of a Nash equilibrium by showing that the best response correspondence \(f\) has a fixed point. First observe that \(f\) is defined over a compact, convex domain \(=_{j}S_{j}\) since each \(S_{j}\) is convex. Next, note that agent \(i\)'s utility function \(u_{i}()=a_{i}()-c_{i}(s_{i})\) is continuous in \(\) due to the continuity of \(a_{i}\) and \(c_{i}\). The continuity of \(u_{i}\) in \(\) and the concavity of\(u_{i}\) in \(s_{i}\) implies the upper semi-continuity of the best response correspondence \(f_{i}\). Moreover, \(u_{i}\) is concave in \(s_{i}\) for each fixed \(_{-i}\), since \(a_{i}\) and \(-c_{i}\) are concave in \(s_{i}\). Thus for each fixed \(s_{-i}\), the best response set \(f_{i}(s_{-i})_{ 0}\) is a non-empty interval, and hence is also convex. Thus \(f\) is a upper semi-continuous non-empty and convex valued correspondence defined over a compact, convex domain. By the Kakutani fixed-point theorem (Kakutani ), \(f\) admits a fixed point.

The above result can also be proved directly by invoking Rosen (), who showed the existence of a Nash equilibrium of \(n\)-person concave games, where the utility function of an agent \(i\) is defined over closed, compact set, is continuous and concave in \(i\)'s own strategy. 

Implications.Theorem 3.1 shows that a Nash equilibrium exists when payoffs are concave and costs are convex. All our motivating examples from Section 2.1 lie under this concave/convex regime and therefore admit a NE. In Appendix A we discuss existence and non-existence of Nash equilibrium in our model when we go beyond the concave-convex regime of payoff and cost functions. In particular, we show that Nash equilibrium exists even with decreasing payoff function of an agent as long as the function is separable between her and other agents' data contribution. We also show that a NE need not exist even with linear cost functions if the payoff functions are non-concave.

Best response dynamics.We now turn to computation and consider a natural procedure by which agents can find a Nash equilibrium: _best response (BR) dynamics_. Agents start with some initial sample vector \(^{0}\). In each step \(t\) of the BR dynamics, every agent \(i\) updates their sample contribution proportional to the gradient \(}{ s_{i}}\) in the direction of increasing utility. Concretely, for a scalar step size \(^{t}>0\), the updates take the following form:

\[^{t+1}=^{t}+^{t} g(^{t},^{t}),\] (4)

where \(g(^{t},^{t})_{i}=(^{t})}{ s_{ i}}+_{i}^{t}\) and \(^{t}\) is chosen so that the updated sample vector \(^{t+1}\) lies in the feasible region \(\). Specifically, \(^{t}_{ K}\|g(^{t},)\|_{2}\), where \(K=\{:^{t}+^{t} g(^{t},)\}\). For instance, if \(0 s_{i}_{i}\), then:

\[_{i}^{t}=-}{ s_{i}},s_{i}=0}{ s_{i}}<0,s_{i}=_{i}}{ s_{i}}>0\\ 0,\]

We measure convergence of the above dynamics to a Nash equilibrium via the \(L^{2}\) norm of the update direction \(g(^{t},^{t})\). Under mild assumptions on the utility functions, we show the dynamics (4) converges to an approximate Nash equilibrium where \(\|g(^{t},^{t})\|_{2}<\):

**Theorem 3.2**.: _Let \(G()\) be the Jacobian of \(:^{n}\), i.e., \(G()_{ij}=u_{i}()}{ s_{j} s_{i}}\). Assuming agent utility functions \(u_{i}\) satisfy_

1. _Strong concavity:_ \((G+ I_{n n})\) _is negative semi-definite,_
2. _Bounded derivatives:_ \(|G_{ij}| L\)_,_

_for constants \(,L>0\), the best response dynamics (4) with step size \(^{t}=L^{2}}\) converges to an approximate Nash equilibrium \(^{T}\) where \(\|g(^{T},^{T})\|_{2}<\) in \(T\) iterations, where_

\[T=L^{2}}{^{2}}^{0},^{0})\|_{2}}{}.\]

Below we sketch the proof and defer the full proof to Appendix A.

Proof sketch.: We measure convergence of the above dynamics by the error term \(\|g(^{t},^{t})\|_{2}\). We show the following bound:

\[\|g(^{t+1},^{t})\|_{2}^{2}\|g(^{t}, {}^{t})\|_{2}^{2}+_{t}^{2}\|G(^{})g(^{t},^{t})\|_{2}^{2}+2_{t}g(^{t},^{t})^{T} G(^{})g(^{t},^{t}).\]

We then use strong concavity to show \(g(^{t},^{t})^{T}G(^{})g(^{t},^{t}) -\|g(^{t},^{t})\|_{2}^{2}\), and the bounded derivatives property to show \(\|G(^{})g(^{t},^{t})\|_{2} nL\|g( ^{t},^{t})\|_{2}\). For our choice of the step size \(^{t}=L^{2}}\), we can relate the error in subsequent iterations as follows:

\[\|g(^{t+1},^{t+1})\|_{2}^{2}(1-}{n^{2}L^{2}})\|g(^{t},^{t})\|_{2}^{2}.\]This then allows us to conclude that after \(T=L^{2}}{^{2}}(^{0},^{0})\|_{2}}{ })\) iterations, we will have an approximate Nash equilibrium \(^{T}\) with \(\|g(^{T},^{T})\|_{2}\). 

Implications.The above theorem implies that agents playing the natural best response update strategy will converge quickly (in \(O((^{-1}))\) iterations) to a NE. We note that our theorem applies to the payoff functions defined by generalization bounds (eq. (2)) and observed in practice (eq. (3)) as they are strongly concave and have bounded derivatives. Moreover, under the assumptions of Theorem 3.2, our proof also implies the fast convergence of the best response dynamics in the budget-balanced mechanism we discuss in the next section.

## 4 Welfare Maximization: A Budget-Balanced Mechanism

We first note through an example that Nash equilibria need not be welfare maximizing.

_Example 1_.: Consider two agents with identical payoff functions \(a()=8-(s_{1}+s_{2})^{-1}\), and linear cost functions given by \(c_{1}(s_{1})=5c_{1}\) and \(c_{2}(s_{2})=4c_{2}\). The NE is given by \(^{*}=(0,0.5)\), i.e., agent 1 does not contribute any data samples. The Nash welfare (which is the \(p\)-mean welfare in \(p 0\) limiting case) of this NE is \(u_{1}(^{*}) u_{2}(^{*})=24\). However consider another sample vector \(^{}=(0.2,0.4)\) where agent \(1\) increases her contribution and agent \(2\) reduces her contribution. Then \(^{}\) has a higher Nash welfare of \(u_{1}(^{}) u_{2}(^{})=25.25>24\).

To address the issue of NE not being welfare-maximizing, by designing a budget-balanced mechanism for agents with linear cost functions. Our mechanism is inspired from a mechanism for the efficient provisioning public goods (Falkinger et al. (2000)). We show that our mechanism always admits a NE. Moreover, when the sample vector at NE is positive, i.e. every agent participates by contributing data, the NE maximizes the \(p\)-mean welfare among all positive sample vectors. For the FL setting, assuming positive sample vectors is natural since the only way for an agent to participate is by making some positive data contribution.

We present our result for a more general model than one discussed so far. Not only does this generalization strengthen our result, it also naturally allows expressing agent utilities in terms of the taxes/subsidies they receive from our mechanism. In this generalization, each agent \(i\) has a budget \(B_{i}\) of which \(b_{i}\) is unspent and the remaining is used towards the cost of sampling \(s_{i}\) data points, i.e., \(b_{i}+c_{i}(s_{i})=B_{i}\). We assume agents have arbitrary continuous, concave utility functions of the form \(u_{i}(b_{i},\|\|_{1})\). Agents have linear cost functions \(c_{i}(s_{i})=c_{i} s_{i}+d_{i}\), with \(c_{i}>0\). This model already captures some previously discussed settings (Sec 2.1) as follows. For e.g., when payoff functions are derived from generalization bounds (Mohri et al. (2018)) or empirical evidence (Kaplan et al. (2020)), they take the form \(a_{i}()=_{i}(\|\|_{1})\), for some function \(_{i}\) which depends on \(\|\|_{1}\). Then for \(u_{i}(b_{i},\|\|_{1})=b_{i}+(\|\|_{1})\), the budget constraint implies the utility takes the form \(_{i}(\|\|_{1})-c_{i}(s_{i})+B_{i}\), which is the same as the utility under the original model with a constant offset. We also assume that for all \(i\), \((b_{i},S)}{ S}>0\) and \((b_{i},S)}{ b_{i}} 0\).

**Mechanism \(_{}\).** We design a mechanism parametrized by a scalar \((0,1)\). It uses the following payment scheme. At a data contribution vector \(\), each agent \(i\) is rewarded an amount

\[p_{i}()=(c_{i}(s_{i})-_{j i}c_{j}(s_{j})).\]

Thus if an agent incurs higher (resp. lower) sampling cost than the average cost borne by other agents, we compensate (resp. penalize) her by a subsidy (resp. tax) of \(\) times her excess cost. By design, our mechanism is budget-balanced: at any sample vector \(\), we have:

\[_{i}p_{i}()=(_{i}c_{i}(s_{i})-_{j i}c_ {j}(s_{j}))=0.\]Under this mechanism, given a vector of contributions \(_{-i}\) of agents other than agent \(i\), the best response of agent \(i\) is any solution to the following optimization problem:

\[ u_{i}(b_{i},s_{i}+\|s_{-i}\|_{1})\] s.t. \[ i:\;b_{i}+(1-)c_{i}(s_{i})+_{j  i}c_{j}(s_{j})=B_{i}\] (5) \[ i:\;b_{i} 0\]

We next define a \(\) value that plays a crucial role in our mechanism.

**Definition 2**.: (Optimal parameter \(^{*}\)) Let \(A:=(_{i}c_{i}^{-1})^{-1}\) and \(C:=_{i}c_{i}\). Then we define \(^{*}\) as the solution to the following equation.

\[C^{2}-(An(n-2)+C)+A(n-1)^{2}=0,\] (6)

which satisfies \(0^{*} 1-1/n\).

The next lemma shows that (6) indeed has such a root - the proof is deferred to Appendix B.

**Lemma 1**.: _The equation \(C^{2}-(An(n-2)+C)+A(n-1)^{2}=0\) of (6) has a real root \(^{*}\) where \(0^{*} 1-1/n\)._

We now state the main result of this section. We show that for every \(\), a Nash equilibrium of \(_{}\) exists. Additionally, for a specific value of \(=^{*}\) (Definition 2), our mechanism admits Nash equilibria which are socially efficient: it maximizes the \(p\)-mean welfare: \(W_{p}(,)=(_{i}u_{i}(b_{i},\|s\|_{1})^{p})^{1/p}\), for \(p 1\) among all positive sample vectors \(\).

**Theorem 4.1**.: _For each \(\), the mechanism \(_{}\) admits a Nash equilibrium. For \(=^{*}\) (Definition 2), whenever the NE \(^{*}\) of \(_{^{*}}\) satisfies \(^{*}>0\), the NE \(^{*}\) maximizes the \(p\)-mean welfare among all vectors \(>0\), for any \(p 1\)._

We now sketch the proof of the above theorem. The full proof is deferred to Appendix B.

Proof sketch.: When \(0 1\), the first constraint in the above program is a convex constraint even for general convex cost functions. Since \(u_{i}()\) is concave, a proof similar to the proof of Theorem 3.1 shows the existence of a Nash equilibrium.

The program maximizing \(p\)-mean welfare as follows.

\[ W_{p}(,):=(_{i}u_{i}(b_{i},\|s\|_{1})^{p})^{1 /p}\] s.t. \[ i:\;b_{i}+(1-)c_{i}(s_{i})+_{j  i}c_{j}(s_{j})=B_{i}\] (7) \[ i:\;b_{i} 0\]

We first show that the above program is convex. With \(_{i}\) and \(_{i}\) as the dual variables to the first and second constraints respectively, we write down the KKT conditions of program (7) with \(s_{i}>0\). We then use the KKT conditions satisfied by a \((^{*},^{*})\) to find values of the dual variables \(^{*}\) and \(^{*}\) so that \((^{*},^{*},^{*},^{*})\) satisfy the KKT conditions of (7). Since KKT conditions are sufficient for optimality, this shows that the NE \((^{*},^{*})\) also maximizes \(p\)-mean welfare. 

**Implications.** Theorem 4.1 shows that by augmenting the federated learning with a simple payment protocol that is budget-balanced, one can obtain NE that maximize any \(p\)-mean welfare function of the net-utilities of the agents. Furthermore, note that the payment augmented utility function is still essentially of the form \(u_{i}()=_{i}()-(1-)c_{i}(s_{i})-c_{j} (s_{j})\), with a constant offset. Since \(\), when \(a_{i}()\) are \(\)-strongly concave and \(c_{i}()\) are \(\)-strongly convex, the functions \(u_{i}()\) are \(\)-strongly concave. Therefore Theorem 3.2 applies, which ensures that the welfare maximizing NE can be reached through the simple best response dynamics quickly. Finally, we remark that when cost functions are identical, the value of the optimal parameter \(^{*}\) is \((1-1/n)\), which is exactly the value used by Falkinger et al. (2000).

_Remark 2_.: Our mechanism requires that costs of the agents be publicly known in order to calculate the value of \(^{*}\) by solving (6) in Definition 2. This is a common assumption made in previous work (e.g. Karimireddy et al. (2022) and Blum et al. (2021)) and is justified in practice. Indeed, costs are common knowledge in many real-world applications e.g. (1) in many ML applications, each agent derives their training data from manually labeling a subset of a publicly available dataset like CIFAR or ImageNet, and the cost of labeling dataset is usually known; (2) in autonomous driving, where each data point is a random path taken under random external conditions.

## 5 Distributed Algorithm and Empirical Evaluation

In this section, we realize our budget-balanced mechanism in a real-world FL system. We perform the evaluation on the MNIST (LeCun et al. (2010)) and CIFAR-10 (Krizhevsky (2009)) datasets. We compare our mechanism with two baselines: the standard FedAvg and MWFed Blum et al. (2021). We denote the vanilla mechanism without budget balancing as FedBR and the budget-balanced mechanism as FedBR-BG. We demonstrate that compared to the FedBR, FedBR-BG achieves better \(p\)-mean welfare for \(p 1\). We also show that the standard FL protocol FedAvg gives significantly lower welfare since it does not allow agents to change their contribution.

### Algorithm Details

We first define the concrete forms for payoff and cost functions. According to standard practice in FL, an agents payoff is measured through the accuracy evaluated on her local test data given model \(\), which has the form \(a_{i}(,)=^{*}|}_{(x,y) D_{i}^{*} }[(x)=y]\), where \(D_{i}^{*}\) is the test data for agent \(i\). We note that this form inherently aligns with \(a_{i}()=_{i}(_{1})\) because the received global model \(\) is trained on \(_{1}\) samples. We consider linear cost functions where \(c_{i}(s_{i})=c_{i}s_{i}\).

Now we derive the update rule for agent contributions for FedBR-BG based on best-response dynamics. We rewrite the utility of agent \(i\) in the budget-balancing mechanism as \(u_{i}()=_{i}(_{i}s_{i})-(1-)c_{i}s_{i}-_{j i}c_{j}s_{j}\). We then compute its gradient with respect to \(s_{i}\): \(}{ s_{i}}=_{i}(_{i}s_{i} )}{ s_{i}}-(1-)c_{i}\). Since the accuracy function is generally unknown in practice, the server can train a public accuracy function \(:}_{ 0}\) and broadcast it to all agents. We obtain an estimate of the accuracy \(\) by evaluating models trained on different numbers of samples, in increasing intervals of \( s\). For example, if the server trains models on \(0,2,4,\) samples and evaluates their accuracy to obtain the estimated accuracy \(\), the interval \( s=2\) in this case. We assume \((_{1},,_{n})}\) as the server can be a service provider with plenty of data sources. With \(\), we can approximate the gradient as \(}{ s_{i}}(_{i}s_{i}+  s)-(_{i}s_{i})}{ s}-(1-)c_{i}\). Similarly, \(}{ s_{i}}(_{i}s_{i}+  s)-(_{i}s_{i})}{ s}-c_{i}\) for FedBR, and \(u_{i}()=a_{i}(_{i}s_{i})-c_{i}s_{i}\) for FedAvg, MWFed and FedBR.

We further assume that \(s_{i}\) is a multiple of \( s\) to ensure that \(\) is always well-defined for our chosen contributions. Finally, we leverage best-response dynamics to update agent contributions. We present the full description of the algorithm for FedBR-BG and FedBR as Algorithm 1 and Algorithm 2 in Appendix C, respectively.

### Experiment Setup

We conduct the experiments with 10 agents for MNIST and 100 agents for CIFAR-10. For MNIST, we use a CNN as the global model, which has two \(5 5\) convolution layers followed by two fully connected layers with ReLU activation. For CIFAR-10, we use VGG11 (Simonyan and Zisserman (2014)). We consider the _i.i.d._ setting, i.e., the local data of agents are sampled from the same distribution. For both datasets, each agent has 100 training images and 10 testing images, i.e.,\(_{i}=100, i[n]\).

We randomly initialize the contributions as a multiplier of 10 in \(\). In each contribution updating step, we re-initialize the global model and perform FedAvg for 50 communication rounds. We set global learning rate \(\) to 1.0, local learning rate \(\) to 0.01, and momentum to 0.9. We set the number of contribution updating steps to 100 and the sample number interval to 10. For evaluating FedAvg, we simply optimize the global model with the same hyperparameters while keeping individual contribution to \(_{i}\).

### Experiment Results

We show the \(p\)-mean welfare of our method and baselines on MNIST and CIFAR-10 in Table 1. We observe that FedBR-BG achieves better \(p\)-mean welfare on both datasets compared to FedBR and FedAvg, verifying our theoretical results. Note that the \(p\)-mean of FedAvg is significantly lower since agents always contribute all their data in FedAvg, which incurs a high cost so that the marginal gain becomes limited.

## 6 Discussion

In this paper, we formulated a federated learning framework which incorporates both payoffs an agent receives from sharing data as well as the cost she incurs due to sharing data. We show the existence of Nash equilibria under the assumption of concave payoffs and convex costs, which are mild assumptions observed in practice. We then note that while NE exist, they may not maximize any notion of welfare for the agents, leaving agents with less incentive to participate. We address this issue via a budget-balanced mechanism with payments whose NE maximize the \(p\)-mean welfare of the agent utilities. Our experiments show that FedBR-BG achieves better \(p\)-mean welfare compared to FedBR and FedAvg, verifying our theoretical results.

We conclude by discussion some directions for future work. Our mechanism required that costs of the agents be publicly known, or at least verifiable. Studying incentives when costs are not common knowledge is an interesting question. Another assumption of our mechanism was that an agent's payoff depends on the number of data samples. Designing welfare-maximizing mechanisms for general settings where payoff functions take more general forms is another direction for future work.

**Acknowledgements.** This work is partially supported by the National Science Foundation under grant No. 1750436, No. 1910100, No. 2046726, No. 2229876, DARPA GARD, the National Aeronautics and Space Administration (NASA) under grant no. 80NSSC20M0229, the Alfred P. Sloan Fellowship, and the Amazon research award.

    &  &  \\   & \(p=0.2\) & \(p=0.4\) & \(p=0.6\) & \(p=0.8\) & \(p=1.0\) & \(p=0.2\) & \(p=0.4\) & \(p=0.6\) & \(p=0.8\) & \(p=1.0\) \\  FedAvg & 48985.23 & 154.99 & 22.763 & 8.726 & 4.909 & 42386.21 & 135.92 & 23.528 & 8.381 & 4.582 \\ MWFeed & 51326.49 & 158.92 & 21.648 & 8.803 & 5.230 & 48178.29 & 142.91 & 23.981 & 8.879 & 4.891 \\ FedBR & 53395.21 & 168.85 & 24.784 & 9.495 & 5.340 & 58297.23 & 178.32 & 26.187 & 9.675 & 5.681 \\ FedBR-BG & **54589.31** & **172.63** & **25.340** & **9.708** & **5.459** & **60385.32** & **183.23** & **27.958** & **9.981** & **5.891** \\   

Table 1: \(p\)-mean welfare of our budget-balanced mechanism FedBR-BG and baselines on MNIST and CIFAR-10. We report the results for different \(p\). The cost for adding one data sample \(c_{i}\) is 0.005 for every agent.