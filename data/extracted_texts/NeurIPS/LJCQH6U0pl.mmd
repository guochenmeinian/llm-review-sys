# Towards Principled Graph Transformers

Luis Muller

RWTH Aachen University

luis.mueller@cs.rwth-aachen.de

&Daniel Kusuma

RWTH Aachen University

&Blai Bonet

Universitat Pompeu Fabra

&Christopher Morris

RWTH Aachen University

###### Abstract

The expressive power of graph learning architectures based on the \(k\)-dimensional Weisfeiler-Leman (\(k\)-WL) hierarchy is well understood. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice. However, comparing their expressivity with the \(k\)-WL hierarchy remains challenging, particularly since attention-based architectures rely on positional or structural encodings. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has \(3\)-WL expressive power when provided with the right tokenization. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance and is competitive with state-of-the-art models on algorithmic reasoning and molecular regression tasks while not relying on positional or structural encodings. Our code is available at https://github.com/luis-mueller/towards-principled-gts.

## 1 Introduction

Graph Neural Networks (GNNs) are the de-facto standard in graph learning  but suffer from limited expressivity in distinguishing non-isomorphic graphs in terms of the \(1\)_-dimensional Weisfeiler-Leman algorithm_ (\(1\)-WL) . Hence, recent works introduced _higher-order_ GNNs, aligned with the \(k\)-dimensional Weisfeiler-Leman (\(k\)-WL) hierarchy for graph isomorphism testing , resulting in more expressivity with an increase in \(k>1\). The \(k\)-WL hierarchy draws from a rich history in graph theory and logic , offering a deep theoretical understanding of \(k\)-WL-aligned GNNs. While theoretically intriguing, higher-order GNNs often fail to deliver state-of-the-art performance on real-world problems, making theoretically grounded models less relevant in practice . In contrast, graph transformers  recently demonstrated state-of-the-art empirical performance. However, they draw their expressive power mostly from positional/structural encodings (PEs), making it difficult to understand these models in terms of an expressivity hierarchy such as the \(k\)-WL. While a few works theoretically aligned graph transformers with the \(k\)-WL hierarchy , we are not aware of any works reporting empirical results for \(3\)-WL-equivalent graph transformers on established graph learning datasets.

In this work, we aim to set the ground for graph learning architectures that are theoretically aligned with the higher-order Weisfeiler-Leman hierarchy while delivering strong empirical performance and, at the same time, demonstrate that such an alignment creates powerful synergies between transformers and graph learning. Hence, we close the gap between theoretical expressivity and real-world predictive power. To this end, we apply the _Edge Transformer_ (ET) architecture, initiallydeveloped for _systematic generalization_ problems , to the field of graph learning. Systematic (or compositional) generalization refers to the ability of a model to generalize to complex novel concepts by combining primitive concepts observed during training, posing a challenge to even the most advanced models such as GPT-4 .

Specifically, we contribute the following:

1. We propose a concrete implementation of the Edge Transformer that readily applies to various graph learning tasks.
2. We show theoretically that this Edge Transformer implementation is as expressive as the \(3\)-\(\)_without_ the need for positional/structural encodings.
3. We demonstrate the benefits of aligning models with the \(k\)-\(\) hierarchy by leveraging well-established results from graph theory and logic to develop a theoretical understanding of systematic generalization in terms of first-order logic statements.
4. We demonstrate the superior empirical performance of the resulting architecture compared to a variety of other theoretically motivated models, as well as competitive performance compared to state-of-the-art models in molecular regression and neural algorithmic reasoning tasks.

## 2 Related work

Many graph learning models with higher-order WL expressive power exist, notably \(\)-\(k\)-GNNs , PSeqNets , \(k\)-IGNs [35; 34], PPGN , and the more recent PPGN++ . Moreover, Lipman et al.  devise a low-rank attention module possessing the same power as the folklore \(2\)-\(\) and Bodnar et al.  propose CIN with an expressive power of at least \(3\)-\(\). For an overview of Weisfeiler-Leman in graph learning, see Morris et al. .

Many graph transformers exist, notably Graphormer  and GraphGPS . However, state-of-the-art graph transformers typically rely on positional/structural encodings, which makes it challenging to derive a theoretical understanding of their expressive power. The Relational Transformer (RT)  operates over both nodes and edges and, similar to the ET, builds relational representations, that is, representations on edges. Although the RT integrates edge information into self-attention and hence does not need to resort to positional/structural encodings, the RT is theoretically poorly understood, much like other graph transformers. Graph transformers with higher-order expressive power are Graphormer-GD  and TokenGT  as well as the higher-order graph transformers in Kim et al. . However, Graphormer-GD is strictly less expressive than the \(3\)-\(\). Further, Kim et al.  and Kim et al.  align transformers with \(k\)-IGNs and, thus, obtain the theoretical expressive power of the corresponding \(k\)-\(\) but do not empirically evaluate their transformers for \(k>2\). In addition, these higher-order transformers suffer from a \((n^{2k})\) runtime and memory complexity. For \(k=3\)the ET offers provable \(3\)-WL expressivity with \((n^{3})\) runtime and memory complexity, several orders of magnitude more efficient than the corresponding \(3\)-WL expressive transformer in Kim et al. . For an overview of graph transformers, see Muller et al. .

Finally, systematic generalization has recently been investigated both empirically and theoretically [6; 15; 26; 43]. In particular, Dziri et al.  demonstrate that compositional generalization is lacking in state-of-the-art transformers such as GPT-4.

## 3 Edge Transformers

The ET was originally designed to improve the systematic generalization abilities of machine learning models. To borrow the example from Bergen et al. , a model that is presented with relations such as \((x,y)\), indicating that \(y\) is the mother of \(x\), could generalize to a more complex relation \((x,z)\), indicating that \(z\) is the grandmother of \(x\) if \((x,y)(y,z)\) holds. The particular form of attention used by the ET, which we will formally introduce hereafter, is designed to explicitly model such more complex relations. Indeed, leveraging our theoretical results of Section 4, in Section 5, we formally justify the ET for performing systematic generalization. We will now formally define the ET; see Appendix D for a complete description of our notation.

In general, the ET operates on a graph \(G\) with nodes \(V(G)\) and consecutively updates a 3D tensor state \(^{n n d}\), where \(d\) is the embedding dimension and \(_{ij}\) or \(()\) denotes the representation of the node pair \((i,j) V(G)^{2}\); see Figure 1 for a visualization of this construction. Concretely, the \(t\)-th ET layer computes

\[_{ij}^{(t)}_{ij}^{(t-1)}+_{ij}^{(t-1)},\]

for each node pair \((i,j)\), where \(\) is a feed-forward neural network, \(\) denotes layer normalization  and \(\) is defined as

\[(_{ij})_{l=1}^{n}_{ilj}_{ ilj},\] (1)

which computes a tensor product between a three-dimensional _attention tensor_\(\) and a three-dimensional _value tensor_\(\), by multiplying and summing over the second dimension. Here,

\[_{ilj}_{l[n]}} _{il}^{Q}_{lj}^{K}^{T}\] (2)

is the attention score between the features of tuples \((i,l)\) and \((l,j)\), and

\[_{ilj}_{il}^{V_{1}}_{lj}^{V_{2}},\] (3)

we call _value fusion_ of the tuples \((i,l)\) and \((l,j)\) with \(\) denoting element-wise multiplication. Moreover, \(^{Q},^{K},^{V_{1}},^{V_{2}}^{d d}\) are learnable projection matrices; see Figure 2 for an overview of the tensor operations in triangular attention and see Algorithm 1 for a comparison to standard attention  in pseudo-code. Note that similar to standard attention, triangular attention can be straightforwardly extended to multiple heads.

As we will show in Section 4, the ET owes its expressive power to the special form of triangular attention. In our implementation of the ET, we use the following tokenization, which is sufficient to obtain our theoretical results.

TokenizationWe consider graphs \(G(V(G),E(G),)\) with \(n\) nodes and without self-loops, where \(V(G)\) is the set of nodes, \(E(G)\) is the set of edges, and \( V(G)\) assigns an initial _color_ to each node. We construct a feature matrix \(^{n p}\) that is _consistent_ with \(\), i.e., for nodes \(i\) and \(j\) in \(V(G)\), \(_{i}=_{j}\) if and only if, \((i)=(j)\). Note that, for a finite subset of \(\), we can always construct \(\), e.g., using a one-hot encoding of the initial colors. Additionally, we consider an edge feature tensor \(^{n n q}\), where \(_{ij}\) denotes the edge feature of the edge \((i,j) E(G)\). If no edge features are available, we randomly initialize learnable vectors \(_{1},_{2}^{q}\) and assign \(_{1}\) to \(_{ij}\) if \((i,j) E(G)\). Further, for all \(i V(G)\), we assign \(_{2}\) to \(_{ii}\). Lastly, if \((i,j) E(G)\) and \(i j\), we set \(_{ij}=\). We then construct a 3D tensor of input tokens \(^{n n d}\), such that for node pair \((i,j) V(G)^{2}\),

\[_{ij}[_{ij}_{i} _{j}],\] (4)

where \(^{2p+q}^{d}\) is a neural network. Extending Bergen et al. , our tokenization additionally considers node features, making it more appropriate for the graph learning setting.

EfficiencyThe triangular attention above imposes a \((n^{3})\) runtime and memory complexity, which is significantly more efficient than other transformers with \(3\)-WL expressive power, such as the higher-order transformers in Kim et al.  and Kim et al.  with a runtime of \((n^{6})\). Nonetheless, the ET is still significantly less efficient than most graph transformers, with a runtime of \((n^{2})\)[32; 42; 53]. Thus, the ET is currently only applicable to mid-sized graphs; see Section 7 for an extended discussion of this limitation.

Positional/structural encodingsAdditionally, GNNs and graph transformers often benefit empirically from added positional/structural encodings [13; 32; 42]. We can easily add PEs to the above tokens with the ET. Specifically, we can encode any PEs for node \(i V(G)\) as an edge feature in \(_{ii}\) and any PEs between a node pair \((i,j) V(G)^{2}\) as an edge feature in \(_{ij}\). Note that typically, PEs between pairs of nodes are incorporated during the attention computation of graph transformers [32; 53]. However, in Section 6, we demonstrate that simply adding these PEs to our tokens is also viable for improving the empirical results of the ET.

ReadoutSince the Edge Transformer already builds representations on node pairs, making predictions for node pair- or edge-level tasks is straightforward. Specifically, let \(L\) denote the number of Edge Transformer layers. Then, for a node pair \((i,j) V(G)^{2}\), we simply readout \(_{ij}^{(L)}\), where on the edge-level we restrict ourselves to the case where \((i,j) E(G)\). In the case of nodes, we can for example read out the diagonal of \(^{(L)}\), that is, the representation for node \(i V(G)\) is \(_{ii}^{(L)}\). In addition to the diagonal readout, we also design a more sophisticated read out strategy for nodes which we describe in Appendix A.1.

With tokenization and readout as defined above, the ET can now be used on many graph learning problems, encoding both node and edge features and making predictions for node pair-, edge-, node-, and graph-level tasks. We refer to a concrete set of parameters of the ET, including tokenization and positional/structural encodings, as a _parameterization_. We now move on to our theoretical result, showing that the ET has the same expressive power as the \(3\)-WL.

## 4 The expressivity of Edge Transformers

Here, we relate the ET to the _folklore_ Weisfeiler-Leman (\(k\)-FWL) hierarchy, a variant of the \(k\)-WL hierarchy for which, for \(k>2\), \((k-1)\)-FWL is as expressive as \(k\)-WL. Specifically, we show that the ET can simulate the \(2\)-FWL, resulting in \(3\)-WL expressive power. To this end, we briefly introduce the \(2\)-FWL and then show our result. For detailed background on the \(k\)-WL and \(k\)-FWL hierarchy, see Appendix D.

Folklore Weisfeiler-LemanLet \(G(V(G),E(G),)\) be a node-labeled graph. The \(2\)-FWL colors the tuples from \(V(G)^{2}\), similar to the way the \(1\)-WL colors nodes . In each iteration, \(t 0\), the algorithm computes a _coloring_\(C_{t}^{2,} V(G)^{2}\) and we write \(C_{t}^{2,}(i,j)\) or \(C_{t}^{2,}()\) to denote the color of tuple \((i,j) V(G)^{2}\) at iteration \(t\). For \(t=0\), we assign colors to distinguish pairs \((i,j)\) in \(V(G)^{2}\) based on the initial colors \((i),(j)\) of their nodes and whether \((i,j) E(G)\) or \(i=j\). For a formal definition of the initial node pair colors, see Appendix D. Then, for each iteration, \(t>0\), the coloring \(C_{t}^{2,}\) is defined as

\[C_{t}^{2,}(i,j)(C_{t-1}^{2,} (i,j),\,M_{t-1}(i,j)),\]

where \(\) injectively maps the above pair to a unique natural number that has not been used in previous iterations and

\[M_{t-1}(i,j)\!C_{t-1}^{2,}(i,l),\,C_{t-1}^{2, }(l,j) l V(G)}.\]

We show that the ET can simulate the \(2\)-FWL, resulting in at least \(3\)-WL expressive power. Further, we show that the ET is also, at most, as expressive as the \(3\)-WL. As a result, we obtain the following theorem; see Appendix E for a formal statement and proof details.

**Theorem 1** (Informal).: _The ET has exactly \(3\)-WL expressive power._

Note that following previous works , our expressivity result is _non-uniform_ in that our result only holds for an arbitrary but fixed graph size \(n\); see Proposition 7 and Proposition 8 for the complete formal statements and proof of Theorem 1.

In the following, we provide some intuition of how the ET can simulate the \(2\)-FWL. Given a tuple \((i,j) V(G)^{2}\), we encode its color at iteration \(t\) with \(_{ij}^{(t)}\). Further, to represent the multiset

\[\{\!C_{t-1}^{2,}(i,l),\,C_{t-1}^{2,}(l,j) l  V(G)\},\]

we show that it is possible to encode the pair of colors

\[(C_{t-1}^{2,}(i,l),\,C_{t-1}^{2,}(l,j)) _{il}^{(t-1)}^{V_{1}}_{lj}^{(t-1)}^{V_{2}},\]

for node \(l V(G)\). Finally, triangular attention in Equation (1), performs weighted sum aggregation over the \(2\)-tuple of colors \((C_{t-1}^{2,}(i,l),\,C_{t-1}^{2,}(l,j))\) for each \(l\), which we show is sufficient to represent the multiset; see Appendix E. For the other direction, namely that the ET has at most \(3\)-WL expressive power, we simply show that the recolor function can simulate the value fusion in Equation (3), as well as the triangular attention in Equation (1).

Interestingly, our proofs do not resort to positional/structural encodings. The ET draws its \(3\)-WL expressive power from its aggregation scheme, the triangular attention. In Section 6, we demonstrate that this also holds in practice, where the ET performs strongly without additional encodings. In what follows, we use the above results to derive a more principled understanding of the ET in terms of systematic generalization, for which it was originally designed. Thereby, we demonstrate that graph theoretic results can also be leveraged in other areas of machine learning, further highlighting the benefits of theoretically grounded models.

## 5 The logic of Edge Transformers

After borrowing the ET from systematic generalization in the previous section, we now return the favor. Specifically, we use a well-known connection between graph isomorphism and first-order logic to obtain a theoretical justification for systematic generalization reasoning using the ET. Recalling the example around the grandmother relation composed from the more primitive mother relation in Section 3, Bergen et al.  go ahead and argue that since self-attention of standard transformers is defined between pairs of nodes, learning explicit representations of grandmother is impossible and that learning such representations implicitly incurs a high burden on the learner. Conversely, the authors argue that since the ET computes triangular attention over triplets of nodes and computes explicit representations between node pairs, the Edge Transformer can systematically generalize to relations such as grandmother. While Bergen et al.  argue the above intuitively, we will now utilize the connection between first-order logic (FO-logic) and graph isomorphism established in Cai et al.  to develop a theoretical understanding of systematic generalization; see Appendix D for an introduction to first-order logic over graphs. We will now briefly introduce the most important concepts in Cai et al.  and then relate them to systematic generalization of the ET and similar models.

Language and configurationsHere, we consider FO-logic statements with counting quantifiers and denote with \(_{k,m}\) the language of all such statements with at most \(k\) variables and quantifier depth \(m\). A _configuration_ is a map between first-order variables and nodes in a graph. Concretely, configurations let us define a statement \(\) in first-order logic, such as three nodes forming a triangle, without speaking about concrete nodes in a graph \(G=(V(G),E(G))\). Instead, we can use a configuration to map the three variables in \(\) to nodes \(v,w,u V(G)\) and evaluate \(\) to determine whether \(v,w\) and \(u\) form a triangle in \(G\). Of particular importance to us are \(k\)-configurations \(f\) where we map \(k\) variables \(x_{1},,x_{k}\) in a FO-logic statement to a \(k\)-tuple \( V(G)^{k}\) such that \(=(f(x_{1}),,f(x_{k}))\). This lets us now state the following result in Cai et al. , relating FO-logic satisfiability to the \(k\)-FWL hierarchy. Here, \(C_{t}^{k,}\) denotes the coloring of the \(k\)-FWL after \(t\) iterations; see Appendix D for a precise definition.

**Theorem 2** (Theorem 5.2 , informally).: _Let \(G(V(G),E(G))\) and \(H(V(H),E(H))\) be two graphs with \(n\) nodes and let \(k 1\). Let \(f\) be a \(k\)-configuration mapping to tuple \( V(G)^{k}\) and let \(g\) be a \(k\)-configuration mapping to tuple \( V(H)^{k}\). Then, for every \(t 0\),_

\[C_{t}^{k,}()=C_{t}^{k,}(),\]

_if and only if \(\) and \(\) satisfy the same sentences in \(_{k+1,t}\) whose free variables are in \(\{x_{1},x_{2},,x_{k}\}\)._

Together with Theorem 1, we obtain the above results also for the embeddings of the ET for \(k=2\).

**Corollary 3**.: _Let \(G(V(G),E(G))\) and \(H(V(H),E(H))\) be two graphs with \(n\) nodes and let \(k=2\). Let \(f\) be a \(2\)-configuration mapping to node pair \( V(G)^{2}\) and let \(g\) be a \(2\)-configuration mapping to node pair \( V(H)^{k}\). Then, for every \(t 0\),_

\[^{(t)}()=^{(t)}(),\]

_if and only if \(\) and \(\) satisfy the same sentences in \(_{3,t}\) whose free variables are in \(\{x_{1},x_{2}\}\)._

Systematic generalizationReturning to the example in Bergen et al. , the above result tells us that a model with \(2\)-FWL expressive power and at least \(t\) layers is equivalently able to evaluate sentences in \(_{3,t}\), including

\[(x,z)= y(x,y)(y,z),\]

i.e., the grandmother relation, and store this information encoded in some 2-tuple representation \(^{(t)}()\), where \(=(u,v)\) and \(^{(t)}()\) encodes whether \(u\) is a grandmother of \(v\). As a result, we have theoretical justification for the intuitive argument made by Bergen et al. , namely that the ET can learn an _explicit_ representation of a novel concept, in our example the grandmother relation.

However, when closely examining the language \(_{3,t}\), we find that the above result allows for an even wider theoretical justification of the systematic generalization ability of the ET. Concretely, we will show that once the ET obtains a representation for a novel concept such as the grandmother relation, at some layer \(t\), the ET can re-combine said concept to generalize to even more complex concepts. For example, consider the following relation, which we naively write as

\[(x,a)= z y(x, y)(y,z)(z,a).\]

At first glance, it seems as though \(_{4,2}\) but \(_{3,t}\) for any \(t 1\). However, notice that the variable \(y\) serves merely as an intermediary to establish the grandmother relation. Hence, we can, without loss of generality, write the above as i.e., we _re-quantify a_ to temporarily serve as the mother of \(x\) and the daughter of \(y\). Afterwards, \(a\) is released and again refers to the great grandmother of \(x\). As a result, \(_{3,2}\) and hence the ET, as well as any other model with at least \(2\)-FWL expressive power, is able to generalize to the \(\) relation within two layers, by iteratively re-combining existing concepts, in our example the \(\) and the mother relation. This becomes even more clear, by writing

\[(x,a)= y(x,y) (y,a),\]

where \(\) is a generalized concept obtained from the primitive concept mother. To summarize, knowing the expressive power of a model such as the ET in terms of the Weisfeiler-Leman hierarchy allows us to draw direct connections to the logical reasoning abilities of the model. Further, this theoretical connection allows an interpretation of systematic generalization as the ability of a model with the expressive power of at least the \(k\)-FWL to iteratively re-combine concepts from first principles (such as the mother relation) as a hierarchy of statements in \(_{k+1,t}\), containing all FO-logic statements with counting quantifiers, at most \(k+1\) variables, and quantifier depth \(t\).

## 6 Experimental evaluation

We now investigate how well the ET performs on various graph-learning tasks. We include tasks on graph-, node-, and edge-level. Specifically, we answer the following questions.

1. How does the ET fare against other theoretically aligned architectures regarding predictive performance?
2. How does the ET compare to state-of-the-art models?
3. How effectively can the ET benefit from additional positional/structural encodings?

The source code for our experiments is available at https://github.com/luis-mueller/towards-principled-gts. To foster research in principled graph transformers such as the ET, we provide accessible implementations of ET, both in PyTorch and Jax.

DatasetsWe evaluate the ET on graph-, node-, and edge-level tasks from various domains to demonstrate its versatility.

On the graph level, we evaluate the ET on the molecular datasets Zinc (12K), Zinc-Full , Alchemy (12K), and PCQM4MV2 . Here, nodes represent atoms and edges bonds between atoms, and the task is always to predict one or more molecular properties of a given molecule. Due to their relatively small graphs, the above datasets are ideal for evaluating higher-order and other resource-hungry models.

On the node and edge level, we evaluate the ET on the CLRS benchmark for neural algorithmic reasoning . Here, the input, output, and intermediate steps of 30 classical algorithms are translated into graph data, where nodes represent the algorithm input and edges are used to encode a partial ordering of the input. The algorithms of CLRS are typically grouped into eight algorithm classes: Sorting, Searching, Divide and Conquer, Greedy, Dynamic Programming, Graphs, Strings, and Geometry. The task is then to predict the output of an algorithm given its input. This prediction is made based on an encoder-processor-decoder framework introduced by Velickovic et al. , which is recursively applied to execute the algorithmic steps iteratively. We will use the ET as the processor in this framework, receiving as input the current algorithmic state in the form of node and edge features and outputting the updated node and edge features, according to the latest version of CLRS, available at https://github.com/google-deepmind/clrs. As such, the CLRS requires the ET to make both node- and edge-level predictions.

Finally, we conduct empirical expressivity tests on the BREC benchmark . BREC contains 400 pairs of non-isomorphic graphs with up to 198 nodes, ranging from basic, \(1\)-WL distinguishable graphs to graphs even indistinguishable by \(4\). In addition, BREC comes with its own training and evaluation pipeline. Let \(f^{d}\) be the model whose expressivity we want to test, where \(f\) maps from a set of graphs \(\) to \(^{d}\) for some \(d>0\). Let \((G,H)\) be a pair of non-isomorphic graphs. During training, \(f\) is trained to maximize the cosine distance between graph embeddings \(f(G)\) and \(f(H)\). During the evaluation, BREC decides whether \(f\) can distinguish \(G\) and \(H\) by conducting a Hotelling's T-square test with the null hypothesis that \(f\) cannot distinguish \(G\) and \(H\).

BaselinesOn the molecular regression datasets, we compare the ET to an \(1\) expressive GNN baseline such as GIN(E) .

On Zinc (12K), Zinc-Full and Alchemy, we compare the ET to other theoretically-aligned models, most notably higher-order GNNs , Graphormer-GD, with strictly less expressive power than the \(3\), and PPGN++, with strictly more expressive power than the \(3\) to study **Q1**. On PCQM4Mv2, we compare the ET to state-of-the-art graph transformers to study **Q2**. To study the impact of positional/structural encodings in **Q3**, we evaluate the ET both with and without relative random walk probabilities (RRWP) positional encodings, recently proposed in Ma et al. . RRWP encodings only apply to models with explicit representations over node pairs and are well-suited for the ET.

On the CLRS benchmark, we mostly compare to the Relational Transformer (RT)  as a strong graph transformer baseline. Comparing the ET to the RT allows us to study **Q2** in a different domain than molecular regression and on node- and edge-level tasks. Further, since the RT is similarly motivated as the ET in learning explicit representations of relations, we can study the potential benefits of the ET provable expressive power on the CLRS tasks. In addition, we compare the ET to DeepSet and GNN baselines in Diao and Loynd  and the single-task Triplet-GMPNN in Ibarz et al. .

On the BREC benchmark, we study questions **Q1** and **Q2** by comparing the ET to selected models presented in Wang and Zhang . First, we compare to the \(\)-\(2\)-LGNN , a higher-order GNN with strictly more expressive power than the \(1\). Second, we compare to Graphormer , an empirically strong graph transformer. Third, we compare to PPGN  with the same expressive power as the ET. We additionally include the \(3\) results on the graphs in BREC to investigate how many \(3\) distinguishable graphs the ET can distinguish in BREC.

Experimental setupSee Table 6 for an overview of the used hyperparameters.

For Zinc (12K), Zinc-Full, and PCQM4Mv2, we follow the hyperparameters in Ma et al. . For Alchemy, we follow standard protocol and split the data according to Morris et al. . Here, we simply adopt the hyper-parameters of Zinc (12K) from Ma et al.  but set the batch size to 64.

We choose the same hyper-parameters as the RT for the CLRS benchmark. Also, following the RT, we train for 10K steps and report results over 20 random seeds. To stay as close as possible to the experimental setup of our baselines, we integrate our Jax implementation of the ET as a processor into the latest version of the CLRS code base. In addition, we explore the OOD validation technique presented in Jung and Ahn , where we use larger graphs for the validation set to encourage size

    & Zinc (12K) & Alchemy (12K) & Zinc-Full \\   & MAE \(\) & MAE \(\) & MAE \(\) \\  GIN(E)  & 0.163 \(\)0.03 & 0.180 \(\)0.006 & 0.180 \(\)0.006 \\  CIN  & 0.079 \(\)0.06 & – & 0.022 \(\)0.022 \\ Graphormer-GD  & 0.081 \(\)0.009 & – & 0.025 \(\)0.004 \\ SignNet  & 0.084 \(\)0.06 & 0.113 \(\)0.02 & **0.024** \(\)0.035 \\ BasisNet  & 0.155 \(\)0.007 & 0.110 \(\)0.001 & – \\ PPGN++  & 0.071 \(\)0.01 & 0.109 \(\)0.01 & **0.020** \(\)0.001 \\ SPE  & **0.069** \(\)**0.044 & 0.108 \(\)**0.011** & – \\  ET & 0.062 \(\)0.04 & 0.099 \(\)0.001 & 0.026 \(\)0.033 \\ ET-\({}_{}\) & **0.059** \(\)**0.044 & **0.098** \(\)**0.001 & **0.024** \(\)**0.035 \\   

Table 1: Average test results and standard deviation for the molecular regression datasets. Alchemy (12K) and Zinc-Full over 5 random seeds, Zinc (12K) over 10 random seeds.

generalization. This technique can be used within the CLRS code base through the experiment parameters.

Finally, for BREC, we keep the default hyper-parameters and follow closely the setup used by Wang and Zhang  for PPGN. We found learning on BREC to be quite sensitive to architectural choices, possibly due to the small dataset sizes. As a result, we use a linear layer for the FFN and additionally apply layer normalization onto \(_{il}^{Q}\), \(_{lj}^{K}\) in Equation (2) and \(_{ilj}\) in Equation (3).

For Zinc (12K), Zinc-Full, PCQM4Mv2, CLRS, and BREC, we follow the standard train/validation/test splits. For Alchemy, we split the data according to the splits in Morris et al. , the same as our baselines.

All experiments were performed on a mix of A10, L40, and A100 NVIDIA GPUs. For each run, we used at most 8 CPU cores and 64 GB of RAM, with the exception of PCQM4Mv2 and Zinc-Full, which were trained on 4 L40 GPUs with 16 CPU cores and 256 GB RAM.

Results and discussionIn the following, we answer questions **Q1** to **Q3**. We highlight **first**, **second**, and third best results in each table.

We compare results on the molecular regression datasets in Table 1. On Zinc (12K) and Alchemy, the ET outperforms all baselines, even without using positional/structural encodings, positively answering **Q1**. Interestingly, on Zinc-Full, the ET, while still among the best models, does not show superior performance. Further, the RRWP encodings we employ on the graph-level datasets improve the performance of the ET on all three datasets, positively answering **Q3**. Moreover, in Table 5, we compare the ET with a variety of graph learning models on Zinc (12K), demonstrating that the ET is highly competitive with state-of-the-art models. We observe similarly positive results in Table 4 where the ET outperforms strong graph transformer baselines such as GRIT , GraphGPS  and Graphormer  on PCQM4Mv2. As a result, we can positively answer **Q2**.

  
**Algorithm** & Deep Sets  & GAT  & MPNN  & PGN  & RT  & Triplet-GMPN  & ET (ours) \\  Sorting & 68.89 & 21.25 & 27.12 & 28.93 & 50.01 & **60.37** & **82.26** \\ Searching & 50.99 & 38.04 & 43.94 & **60.39** & **65.31** & 58.61 & **63.00** \\ DC & 12.29 & 15.19 & 16.14 & 51.30 & 66.52 & **76.36** & **64.44** \\ Greedy & 77.83 & 75.75 & **89.40** & 76.72 & 85.32 & **91.21** & 81.67 \\ DP & 68.29 & 63.88 & 68.81 & 71.13 & **83.20** & **81.99** & **83.49** \\ Graphs & 42.09 & 55.53 & 63.30 & 64.59 & 65.33 & **81.41** & **86.08** \\ Strings & 2.92 & 1.57 & 2.09 & 1.82 & 32.52 & **49.09** & **54.84** \\ Geometry & 65.47 & 68.94 & 83.03 & 67.78 & **84.55** & **94.09** & **88.22** \\  Avg. class & 48.60 & 41.82 & 49.23 & 52.83 & **66.60** & **74.14** & **75.51** \\ All algorithms & 50.29 & 48.08 & 55.15 & 56.57 & **66.18** & **75.98** & **80.13** \\   

Table 2: Average test micro F1 of different algorithm classes and average test score of all algorithms in CLRS over ten random seeds; see Appendix B.3 for test scores per algorithm and Appendix B.4 for details on the standard deviation.

  
**Model** & Basic & Regular & Extension & CFI & _All_ \\  \(\)-2-LGNN & 60 & 50 & 100 & 6 & **216** \\ PPGN & 60 & 50 & 100 & 23 & **233** \\ Graphormer & 16 & 12 & 41 & 10 & 79 \\  ET & 60 \({}_{ 0.0}\) & 50 \({}_{ 0.0}\) & 100 \({}_{ 0.0}\) & 48.1 \({}_{ 1.9}\) & **258.1 \({}_{ 1.9}\)** \\   \(3\)-WL & 60 & 50 & 100 & 60 & 270 \\   

Table 3: Number of distinguished pairs of non-isomorphic graphs on the BREC benchmark over 10 random seeds with standard deviation. Baseline results (over 1 random seed) are taken from Wang and Zhang . For reference, we also report the number of graphs distinguishable by \(3\)-WL.

In Table 2, we compare results on CLRS where the ET performs best when averaging all tasks or when averaging all algorithm classes, improving over RT and Triplet-GMPNN. Additionally, the ET performs best on 4 algorithm classes and is among the top 3 in 7/8 algorithm classes. Interestingly, only some models are best on a majority of algorithm classes. These results indicate a benefit of the ETs' expressive power on this benchmark, adding to the answer of **Q2**. Further, see Table 7 in Appendix B.2 for additional results using the OOD validation technique.

Finally, on the BREC benchmark, we observe that the ET cannot distinguish all graphs distinguishable by \(3\)-WL. At the same time, the ET distinguishes more graphs than PPGN, the other \(3\)-WL expressive model, providing an additional positive answer to **Q1**; see Table 3. Moreover, the ET distinguishes more graphs than \(\)-\(2\)-LGNN and outperforms Graphormer by a large margin, again positively answering **Q2**. Overall, the positive results of the ET on BREC indicate that the ET is well able to leverage its expressive power empirically.

## 7 Limitations

While proving to be a strong and versatile graph model, the ET has an asymptotic runtime and memory complexity of \((n^{3})\), which is more expensive than most state-of-the-art models with linear or quadratic runtime and memory complexity. We emphasize that due to the runtime and memory complexity of the \(k\)-WL, a trade-off between expressivity and efficiency is likely unavoidable. At the same time, the ET is highly parallelizable and runs efficiently on modern GPUs. We hope that innovations for parallelizable neural networks can compensate for the asymptotic runtime and memory complexity of the ET. In Figure 4 in the appendix, we find that we can use low-level GPU optimizations, available for parallelizable neural networks out-of-the-box, to dampen the cubic runtime and memory scaling of the ET; see Appendix C for runtime and memory experiments and an extended discussion.

## 8 Conclusion

We established a previously unknown connection between the Edge Transformer and \(3\)-WL, and enabled the Edge Transformer for various graph learning tasks, including graph-, node-, and edge-level tasks. We also utilized a well-known connection between graph isomorphism testing and first-order logic to derive a theoretical interpretation of systematic generalization. We demonstrated empirically that the Edge Transformer is a promising architecture for graph learning, outperforming other theoretically aligned architectures and being among the best models on Zinc (12K), PCQM4Mv2 and CLRS. Furthermore, the ET is a graph transformer that does not rely on positional/structural encodings for strong empirical performance. Future work could further explore the potential of the Edge Transformer in neural algorithmic reasoning and molecular learning by improving its scalability to larger graphs, in particular through architecture-specific low-level GPU optimizations and model parallelism.

  
**Model** & Val. MAE (\(\)) & \# Params \\  EGT  & 0.0869 & 89.3M \\ GraphGPS\({}_{}\) & 0.0938 & 6.2M \\ GraphGPS\({}_{}\) & 0.0858 & 19.4M \\ TokenGT\({}_{}\) & 0.0962 & 48.6M \\ TokenGT\({}_{}\) & 0.0910 & 48.5M \\ Graphormer  & 0.0864 & 48.3M \\ GRIT  & 0.0859 & 16.6M \\ GPTrans-L & 0.0809 & 86.0M \\  ET & 0.0840 & 16.8M \\ ET\({}_{}\) & 0.0832 & 16.8M \\   

Table 4: Average validation MAE on the PCQM4Mv2 benchmark over a single random seed.

  
**Model** &  \\   & MAE \(\) \\  SignNet  & 0.084 \(\)0.006 \\ SUN  & 0.083 \(\)0.090 \\ Graphormer-GD  & 0.081 \(\)0.090 \\ CNN  & 0.079 \(\)0.096 \\ Graph-MLP-Mixer  & 0.073 \(\)0.091 \\ PPGN++  & 0.071 \(\)0.091 \\ GraphGPS  & 0.070 \(\)0.090 \\ SPE  & 0.069 \(\)0.094 \\ Graph Diffuser  & 0.068 \(\)0.020 \\ Specformer  & 0.066 \(\)0.020 \\ GRIT  & **0.059 \(\)0.082** \\  ET & **0.062 \(\)0.084** \\ ET\({}_{}\) & 0.059 \(\)0.084 \\   

Table 5: Zinc (12K) leaderboard.