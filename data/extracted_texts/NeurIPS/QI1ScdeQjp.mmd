# Upping the Game: How 2D U-Net Skip Connections Flip 3D Segmentation

Xingru Huang\({}^{1}\)1, Yihao Guo\({}^{1}\)1, Jian Huang\({}^{1}\)1, Tianyun Zhang\({}^{1}\),

**Hong He\({}^{1}\)2, Shaowei Jiang\({}^{1}\)3, Yaoqi Sun\({}^{1}\)4**

\({}^{1}\)Hangzhou Dianzi University

These authors contributed equally.Corresponding author. Email: hehong@hdu.edu.cn, jiangsw@hdu.edu.cn, syq@hdu.edu.cn

###### Abstract

In the present study, we introduce an innovative structure for 3D medical image segmentation that effectively integrates 2D U-Net-derived skip connections into the architecture of 3D convolutional neural networks (3D CNNs). Conventional 3D segmentation techniques predominantly depend on isotropic 3D convolutions for the extraction of volumetric features, which frequently engenders inefficiencies due to the varying information density across the three orthogonal axes in medical imaging modalities such as computed tomography (CT) and magnetic resonance imaging (MRI). This disparity leads to a decline in axial-slice plane feature extraction efficiency, with slice plane features being comparatively underutilized relative to features in the time-axial. To address this issue, we introduce the U-shaped Connection (uC), utilizing simplified 2D U-Net in place of standard skip connections to augment the extraction of the axial-slice plane features while concurrently preserving the volumetric context afforded by 3D convolutions. Based on uC, we further present uC 3DU-Net, an enhanced 3D U-Net backbone that integrates the uC approach to facilitate optimal axial-slice plane feature utilization. Through rigorous experimental validation on five publicly accessible datasets--FLARE2021, OIMHS, FeTA2021, AbdomenCT-1K, and BTCV, the proposed method surpasses contemporary state-of-the-art models. Notably, this performance is achieved while reducing the number of parameters and computational complexity. This investigation underscores the efficacy of incorporating 2D convolutions within the framework of 3D CNNs to overcome the intrinsic limitations of volumetric segmentation, thereby potentially expanding the frontiers of medical image analysis. Our implementation is available at https://github.com/IMOP-lab/U-Shaped-Connection.

## 1 Introduction

3D volumetric data segmentation extensively relies on the utilization of axial symmetrical 3D convolutions to extract features based on a volumetric representation. Imaging modalities such as CT  and MRI  yield high-precision images along the slice plane and repeat this process along the temporal axis, resulting in non-uniform information density across the three axes. We visualize the difference in information density between the axial and the slice plane in 3D medical imaging in Fig. 1. The information density variance arising from non-simultaneous imaging in three dimensions of volumetric data engenders distinctiveness between time-axial and slice plane features, rendering them unsuitable for the same processing. Typically, features extracted from the slice plane possess higher information density and are more adept at delineating the fine-grained, voxel-level neighboring structures. These localized features are essential in understanding the precise 3D structure of tissues, thereby surpassing the utility of time-axial features. However,traditional 3D convolutions [10; 11; 12], although excelling in time-axial information extraction, significantly increase computational complexity to accommodate time-axial information, leading to axial-slice plane performance drop-off in 3D CNNs. Thus, the structural constraints of 3D convolutions inherently weaken the rich local features of the slice plane. When confronted with sequences abundant in slice plane features, symmetric 3D convolutions face substantial inefficiencies and performance degradation.

To address the axial-slice plane performance drop-off issue, an efficient strategy involves increasing channel depth to enhance feature extraction performance. The depth of the channels, indicative of the high-dimensional feature richness, directly influences network performance. Increasing channel depth can enhance the network's capacity to capture more high-dimensional information and improve feature representation quality. However, the relationship between channel depth and network performance in 3D CNNs is intricate and nonlinear. While initially increasing channel depth improves model performance by introducing more detailed and abstract features, this improvement comes at the expense of ever-increasing computational overhead. Moreover, an excessive number of channel depths can complicate the model's information processing, potentially introducing noise and redundancy, culminating in dimensional explosion and performance decline. The linear relationship between channel depths and computational overhead means that increasing channel depth significantly inflates computational costs, leading to reduced model efficiency under identical hardware constraints. Conversely, 2D convolutions, with their focus on slice-wise information extraction, exhibit higher efficiency and performance peaks, boasting a superior parameter performance ratio. For a kernel size \(K\), a 2D convolution requires only \(\) of the parameters compared to its 3D counterpart, thereby significantly enhancing local feature extraction efficiency within a slice. Consequently, 2D convolutions present a key solution to the low parameter-to-performance ratio of 3D CNN architectures, markedly increasing axial-slice plane feature extraction efficiency while minimizing parameter count and computational load.

Given the excellent computational efficiency and parameter-to-performance ratio of 2D convolutions for axial-slice plane feature utilization, and considering the prevalent use of skip connections in 3D medical image segmentation architectures to integrate early layer details, we propose employing 2D convolutions within skip connections to supplement axial-slice plane information. For this purpose, we introduce a plug-and-play U-shaped Connection (uC), leveraging a simplified 2D U-Net to replace skip connections in 3D segmentation architectures for improving the utilization efficiency of axial-slice plane features. Using classical 3D U-Net as the backbone, we further propose the uC 3DU-Net, which substitutes the original skip connections with uC, thereby enhancing the network's ability to comprehend slice plane information and efficiently utilize initial encoded layer details. For more effective integration of 3D spatial features from 3D CNNs and 2D slice plane features introduced by uC, we employ a Dual Feature Integration (DFi) module to combine multi-dimensional features.

Experimental evaluations demonstrate the incorporation of uC significantly enhances segmentation performance even with reduced channels and surpasses parameter-to-performance ratios of state-of-the-art networks. Comparative analyses on five publicly available datasets--FLARE2021, OIMHS, FeTA2021, AbdomenCT-1K, and BTCV--corroborate the superiority of proposed uC 3DU-Net over existing state-of-the-art methods, achieving persuasive performance with reduced computational complexity and model parameters.

The principal contributions of our work can be summarized as follows:

Figure 1: Visualization of the difference in information density between the time-axial (Green) and slice planes (Red). Panels (a), (b), and (c) respectively illustrate the information density differences in time-axial and slice planes for volumetric data of the abdomen, retina, and brain tissues.

1. We proposed uC, utilizing simplified 2D U-Net to replace traditional skip connections in 3D segmentation backbones, enhancing model capacity to capture axial-slice plane features.

2. uC 3DU-Net is further proposed, adopting uC to replace original skip connections, and applying the DFi module to effectively merge 3D sequential spatial features with 2D axial-slice plane features, thus improving feature utilization efficiency.

3. We explore the complementary relationship between uC and 3D CNNs, revealing that incorporating 2d convolutions in 3D CNNs can achieve superior performance with fewer parameters, striking perfect balances between efficiency and performance in volumetric segmentation.

4. Comparative experiments on five public datasets--FLARE2021, OIMHS, FeTA2021, AbdomenCT-1K, and BTCV--demonstrate that uC 3DU-Net surpasses all previous models, achieving SOTA performance with fewer parameters and lower computational cost.

## 2 Related Work

### 3D medical image segmentation

The realm of 3D medical image segmentation has experienced substantial advancements, primarily propelled by the progressive evolution of deep learning paradigms and the augmentation of computational prowess [13; 14]. At the key of these advancements lies the continuous enhancement of CNNs [15; 16; 17; 18; 19] and the advent of Vision Transformers (ViTs) , both of which have significantly promoted segmentation methodologies [21; 22; 23; 24; 25]. The 3D U-Net , a foundational model in this field, the subsequent iterations have incorporated attention mechanisms, augmenting feature extraction capabilities by prioritizing important regions while mitigating noise interference [27; 28]. Transformer-based architectures, exemplified by the UNETR framework [29; 30], leverage multi-head self-attention mechanisms to capture long-range dependencies, thus facilitating more precise and meticulous segmentation. Hybrid models that combine CNNs with Transformers, such as TransUNet  and TransBTS , effectively balance local spatial feature extraction with global contextual understanding, culminating in superior segmentation performance [33; 34]. The nnU-Net , an exemplary self-adaptive framework, has demonstrated exceptional performance across a diverse array of medical imaging tasks by autonomously configuring itself to specific datasets.

Beyond architectural innovations, a multitude of research endeavors have concentrated on refining 3D medical image segmentation mask techniques. Chen et al.  propose a novel method integrating Active Appearance Models (AAM) with live wire (LW) and Graph Cuts (GC) techniques, thereby enhancing the efficacy of 3D medical image segmentation. Zhang et al.  develop the 3D Context Residual Network (ConResNet), which employs a context residual module to interlink the segmentation decoder with the context residual decoder, explicitly learning inter-slice contextual information to improve segmentation accuracy. Advanced loss functions and optimization strategies, such as extensive implementations of Dice loss , have been designed to address class imbalance issues, ensuring a more balanced learning process between easy and hard examples. Furthermore, data augmentation and transfer learning techniques have been beneficial in enhancing model robustness and generalization capabilities [38; 39]. These unremitting innovations promise increasingly precise and reliable analysis of 3D medical images, significantly advancing the fields of medical diagnostics and therapeutic planning.

### Advancements in skip connection structures

Innovations in skip connection structures have further augmented the performance and computational efficiency of CNNs [40; 41; 42] in the domain of medical image segmentation. Skip connections are important in facilitating direct gradient propagation, thereby mitigating the vanishing gradient issue and enabling the training of substantially deeper networks. This method permits unimpeded gradient flow throughout the network, effectively circumventing intermediate layers, minimizing information degradation, and achieving more stable and efficient optimization. Prominently exemplified in the ResNet architecture, skip connections mitigate the vanishing gradient dilemma, thereby fostering the training of deep neural networks. In the U-Net architecture, skip connections are crucial in the transmission of initial detailed spatial information from the encoder to the decoder, thereby preserving high-resolution details indispensable for precise segmentation tasks.

Recent endeavors have extensively refined traditional skip connections to augment their functional efficacy. Attention U-Net  replaces the normal application of skip connections by incorporating attention mechanisms that dynamically emphasize important features while attenuating irrelevant information. Dense U-Net  leverages densely connected convolutional networks (DenseNet)  to facilitate feature reuse and improve gradient flow. Interconnecting each layer with every other layer in a feed-forward manner facilitates the assimilation of richer and more diverse feature representations. Skip connections in the Residual Channel Attention Network (RCAN)  network mitigate the training difficulty of deep neural networks and enhance feature reuse and information flow efficiency, thereby improving the network's ability to learn high-frequency information. Hybrid Densely Connected UNet (H-DenseUNet)  incorporates skip connections within dense blocks, combining the strengths of DenseNet and U-Net. By harnessing advanced feature reuse, optimized gradient flow facilitated by dense connectivity, and efficient multi-scale feature fusion, it produces a robust architecture adept at capturing intricate details and nuanced contextual information.

## 3 Method

### Preliminaries: 3D convolutional segmentation networks and skip connections

Current 3D medical image segmentation networks primarily utilize architectures based on 3D convolutions, as exemplified by classic models such as 3D U-Net, SegResNet, and the recent 3D UX-Net. These networks adopt an end-to-end encoder-decoder framework with skip connections. The encoder progressively increases channel depth while downsampling spatial feature maps to abstract and consolidate contextual information, thereby reducing parameter complexity and improving computational efficiency. In contrast, the decoder gradually upsamples these encoded features to restore the original resolution, refining segmentation boundaries and overall segmentation accuracy. Skip connections effectively reintroduce fine-grained details from the original images during the upsampling stage, aiding the refinement of segmentation results.

Several 3D medical image segmentation networks, such as UNETR, TransBTS, and SwinUNETR, have incorporated multi-head self-attention and advanced skip connections within the 3D Conv-based encoder-decoder framework. These hybrid approaches effectively capture long-range dependencies and facilitate multi-scale feature utilization. Multi-head self-attention ensures global consistency, while 3D convolution operations excel in preserving local spatial details. The combination of self-attention with 3D convolutions ensures the model's ability to retain image details while comprehending and processing global information more effectively. However, relying solely on transformer architectures, such as Vision Transformers, would result in a substantial increase in parameters and computational load. Additionally, without skip connections, it is difficult to achieve satisfactory segmentation details. Hence, 3D convolutions and skip connections remain crucial for achieving optimal segmentation performance in 3D medical image segmentation.

### U-shaped Connection

Considering the objective of skip connections to capture detailed features of original images, it is essential to note that 3D medical images fundamentally represent a sequence of 2D images. Given the anisotropic nature of medical images, the 2D slice plane shows richer feature information compared to the temporal sequence axis. Although basic skip connection methods like cat and addition can supplement original feature information to some extent, these methods fail to fully utilize the rich axial-slice plane information in 3D medical images. To address the challenges posed by the anisotropy of 3D medical images, the U-shaped Connection(uC) is proposed. It offers a solution to anisotropy in medical imaging. This approach employs a simplified 2D U-Net to implement skip connections, thereby supplementing the rich original 2D slice plane feature information, and enhancing the 2D slice plane feature extraction capabilities of any 3D image segmentation network. The uC combines features extracted by 2D convolution with those extracted by 3D upsampling, offering more efficient feature extraction compared to pure 3D convolution, as detailed structure in Sec3.3.

The fundamental structure of uC is based on a 2D U-Net. In comparison to the basic 2D U-Net, uC omits the initial and final conv layers, retaining only the downsampling and upsampling layers to minimize computational load and enhance the extraction efficiency of original slice plane features. Each downsampling layer comprises an average pooling layer and two conv layers, each consisting of 2D conv, Group Normalization (GN), and ReLU. The average pooling layer reduces the feature map size by half, and the subsequent conv layers double the channel number, with the group number of GN set to half the current channel number. Each upsampling layer includes a transposed convolution layer and two conv layers, where the transposed convolution layer restores the feature map to its original size, and the conv layers reduce the channel number back to its half.

### uC 3DU-Net

Based on uC, we proposed uC 3DU-Net, which employs the 3D U-Net as the backbone, incorporating uC with a dual feature integration (DFi) module. The network architecture is illustrated in Fig. 2. Given the input 3D volumetric image data denoted as \(x\), we divide the encoder of the 3D U-Net into five stages. Stage 1 is the initial Two Conv layer, comprising two 3D convolutions, expanding the input feature channel depth to 24. Stages 2 to 5 are downsampling layers, each comprising a max-pooling operation and two 3D convolutions, doubling the feature channels while halving the feature map dimensions. The outputs of stages 1 to 5 are denoted as \(x^{(:1)}\) to \(x^{(:5)}\). The decoder is also divided into five stages. Stages 5 to 2 utilize transposed convolutions to upsample, effectively doubling the spatial dimensions of the feature maps. Stages 5 to 3 apply DFi to effectively halve the feature channel depth, while Stage 2 retains two convolution layers. Stage 1 is the Final Conv layer, converting the feature channels to the number of categories by a 3D convolution. All 3D convolution operations are subsequently followed by Instance Normalization and a LeakyReLU activation function, with the negative slope set to 0.1.

Considering the parameter-efficiency trade-off, we employ the uC in only stages 1 to 3, denoted as \(uC_{1}\) to \(uC_{3}\). The inputs \(x^{(:1)}\) to \(x^{(:3)}\) are reshaped into 4D dimensions by stacking slices along the batch dimension using a rearrange operation, resulting in \(^{(:1)}\) to \(^{(:3)}\). The structure of \(uC_{1}\) to \(uC_{3}\) resembles that of a simplified version of 2D U-Net, with detailed descriptions of the uC architecture provided in Section 3.2. \(uC_{1}\), \(uC_{2}\), and \(uC_{3}\) consist of 4, 3, and 2 downsampling and upsampling layers, respectively, to match the dimensions of the input 4D tensor. The outputs of \(^{(:1)}\) to \(^{(:3)}\) after passing through the uC are \(^{(:1)}\) to \(^{(:3)}\), which are reshaped back to 5D dimensions using a rearrange operation, resulting in \(^{(:1)}\) to \(^{(:3)}\).

We utilize the dual feature integration (DFi) module to seamlessly integrate the 3D spatial features extracted by the 3D CNN with the rich axial-slice plane features introduced by the uC. Specifically, \(x^{(:4)}\) and \(x^{(:5)}\) are fused using the DFi block to obtain \(x^{}\), \(x^{}\) and \(^{(:3)}\) are fused to obtain \(x^{v}\), and \(x^{v}\) and \(^{(:2)}\) are fused to obtain \(x^{}\). Finally, \(x^{}\) and \(^{(:1)}\) are fused using the DFi block and undergo the final convolutional block to obtain the final output \(y\).

**Dual Feature Integration.** In conventional 3D U-Net architectures, skip connections integrate the original feature information from the encoder during the upsampling process through cat, followed by two 3x3 convolutions to reduce the number of channels and consolidate useful information. However, this basic feature fusion approach may lead to inefficiencies when reconciling the disparity between 3D spatial features and 2D slice plane features. Therefore, we devise a streamlined, efficient multi-scale feature fusion module DFi to better integrate the axial-slice plane information introduced by uC. The 2D slice plane features extracted by uC and the 3D spatial features from the 3D CNN

Figure 2: **Overview of the proposed uC 3DU-Net architecture. The backbone is a 3D U-Net with five encoding-decoding stages and each downsampling block comprises a max-pooling layer followed by two 3D convolutional layers. In stages 1 to 3, 5D tensors are rearranged into 4D tensors for 2D uC input by stacking slices along the batch dimension. Each upsampling layer employs a transposed convolution to upsampling and a DFi block to effectively adjust the feature channel depth.**encoder are cat to retain maximal original feature information. However, subsequent convolution operations to reduce channel depths are computationally intensive and inefficient. Utilizing addition for feature fusion could result in information loss due to inherent discrepancies between the two types of features. Hence, an approach combining cat, addition, and subsequent convolution layers, with the aim of minimizing parameter count and FLOPs, resulting in the design of the DFi module. For the 2D slice plane features F1 extracted by uC and the 3D spatial features F2 obtained during 3D U-Net upsampling, branch 1 of the DFi module cats F1 and F2, then employs a 1x1 conv to halve the channel count, preserving important features to produce feature map Fc. Branch 2 initially extracts critical information from F1 and F2 using 1x1 convs, followed by addition and sigmoid activation to generate the attention map Fa. The element-wise multiplication of Fc and Fa adjusts and integrates the feature map Fc from branch 1, thereby learning the relative importance of F1 and F2 during the fusion process. The proposed DFi module integrates and simplifies addition, cat, and subsequent convolution layers, achieving a more lightweight and efficient fusion of 2D and 3D spatial features.

## 4 Experiments and results

### Datasets

To validate the efficacy of the proposed method, multiple experiments are conducted on five widely-used, publicly accessible datasets: FLARE2021 , OIMHS , FeTA2021 , AbdomenCT-1K , and BTCV . To ensure experimental rigor and fairness, we apply identical data preprocessing and hyperparameter settings across all datasets, uniformly utilizing the \(_{DiceCE}\) for training.

**FLARE2021.** An anisotropic CT dataset dedicated to abdominal organ segmentation comprises 361 training instances, 50 validation instances, and 100 test instances, categorized into four organ classes: spleen, kidney, liver, and pancreas. The spatial resolution ranges from 0.61 mm to 0.98 mm in-plane and 0.5 mm to 7.5 mm through-plane. We utilize the full set of 361 publicly labeled instances.

**OIMHS.** A fundus retinal 3D OCT segmentation dataset comprises 125 sequences, each containing 19 to 73 scans, and is categorized into four classes: retina, choroid, macular hole, and macular edema. It is a anisotropic dataset, the spacing ranges from 10.7 um to 14.0 um in-plane and 7.0 um to 40.0 um through-plane. All 125 publicly available sequences are utilized in the experiments.

**FeTA2021.** A dataset consists of 120 T2-weighted fetal brain MRI reconstructions, categorized into seven classes: external cerebrospinal fluid, grey matter, white matter, ventriculus, cerebellum, deep grey matter, and brainstem. It is a typical isotropic dataset, the spacing ranges from 0.43 mm to 1 mm across all dimensions. All 80 publicly available sequences are utilized in the experiment.

**AbdomenCT-1K.** A large-scale abdominal CT dataset comprises 1,112 instances, focusing on the segmentation of four abdominal organs: liver, kidney, spleen, and pancreas. The spatial resolution ranges from 0.45 mm to 1.04 mm in-plane and 0.45 mm to 8 mm through-plane. It is also an anisotropic dataset, for our experiments, we utilize 361 instances from Task 2.

**BTCV.** A dataset consists of 50 abdominal CT instances, categorized into 13 classes. It is an anisotropic dataset, with in-plane resolution ranging from 0.59 mm to 0.98 mm and through-plane resolution ranging from 2.5 mm to 5.0 mm. All 30 labeled instances are selected for the experiments.

### Implementation details and evaluation metrics

**Implementation details.** The datasets are randomly partitioned in an 8:1:1 ratio for training, validation, and testing. To enhance segmentation performance, the data are resampled, intensity clipped, and normalized using min-max normalization. Specifically, the intensity clipping ranges are as follows: FLARE2021 \([-125,275]\), OIMHS \(\), FeTA \(\), AbdomenCT-1K \([-125,300]\), and BTCV \([-175,250]\). During the training stage, random cropping to \(96 96 96\) volumes is performed, and a sliding window method with a 0.5 overlap is used for validation.

All training utilizes the \(_{DiceCE}\) function with the AdamW  optimizer, a learning rate of 0.0001, 80,000 training iterations, and a batch size of 2. Data augmentation techniques, including random flip, random rotation, random scaling, and random 3D elastic transformation, are applied to enhance dataset diversity and model generalization. Validation occurs every 1000 iterations during training to save the model weight with the best validation performance. The experiments are conducted on identical hardware and software environments, each workstation equipped with two NVIDIA

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

is consistently observed across different layers of uC, highlighting its robust and efficient feature fusion capabilities. The focus of the proposed DFi thus shifts towards optimizing feature fusion efficiency rather than merely achieving performance improvements. Continuous improvement efforts will prioritize refining 2D and 3D feature integration while maintaining computational efficiency.

#### 4.4.4 Performance Comparison with Varying Channel Depths in 3D UX-Net

The low parameter performance ratio observed in 3D CNNs is largely due to their reliance on axially symmetric 3D convolutions, which extract spatial features well but struggle to capture critical axial-slice plane details. Introducing the uC structure significantly enhances the capability of 3D CNNs to extract axial-slice plane features. Can incorporating uC allow a reduction in channel count, thereby decreasing computational cost? To explore this, we conduct an analytical experiment utilizing 3D UX-Net as the backbone of FLARE2021 and OIMHS datasets to investigate the impact of channel depth and uC integration on segmentation performance. As illustrated in Table 8, achieving a 0.66% Dice improvement with the 3D UX-Net on FLARE2021 requires a tenfold model increase, highlighting the challenge of enhancing performance with 3D CNNs. The incorporation of uC mitigates the axial-slice plane performance drop-off, resulting in performance enhancements across 3D UX-Net with different channel depths. Notably, a 24-channel depth uC 3D UX-Net surpasses the original 48-channel depth 3D UX-Net on both datasets, with parameter count and FLOPs reduced to only 29% and 34.3%, respectively. This highlights uC's capability to achieve superior performance with fewer parameters, suggesting that substituting traditional skip connections with uC and reducing channel depth is a viable approach to achieve substantial reductions in parameter count and computational load while maintaining or even improving performance. The qualitative results are further illustrated in Fig. 4.

### Ablation Studies

**Stage Selection for uC integration.** By replacing skip connections with uC at different stages, we validate the impact of introducing 2D axial-slice plane features at varying network layers on model performance. All models had an initial channel depth of 32.

   &  &  &  &  &  \\  & & & & & & & & & & & \\ 
3D UX-Net (16) & 6.1M & 72.20 & 87.664.43 & 69.772.42 & 830.116.9 & 9.7647.33 & 9.6646.33 & 9.2154.21 & 9.5709.160 & 91.542.18 \\
3D UX-Net(16) **uC** & 6.7M & 97.30 & **80.982.36** & **93.214.05** & **1.005.40** & **2.9984.38** & **93.084.39** & **93.084.39** & **93.2582.09** & **93.080.09** & **93.084.287** \\ 
3D UX-Net (24) & 13.4M & 40.30 & **80.981.93** & 93.542.14 & 1.661.21 & 7.461.18 & 9.112.14 & 9.5766.13 & 9.1273.73 & 9.649.49 & 3.5684.09 & 92.746.20 \\
3D UX-Net (25) **uC** & 15.4M & 220.20 & **80.782.49** & **93.543.19** & **93.543.07** & **93.543.09** & **92.244.44** & **93.543.18** & **90.212.20** & **93.084.47** & **93.084.24** & **23.756.09** & **93.543.19** \\ 
3D UX-Net (48) & 53.1M & 69.94 & 88.625.50 & 93.411.16 & 2.994.00 & 1.3776.73 & 9.3152.00 & 8.3834.23 & 93.562.01 & 9.514.18 & 2.7049.16 & 93.016.02 \\
3D UX-Net (48) **uC** & 60.6M & 80.976.310 & **80.761.33** & **70.646.35** & **82.581.34** & **93.041.05** & **93.083.40** & **93.743.125** & **93.084.02** & **2.995.00** & **93.533.22** \\  

Table 8: Analysis experiments to evaluate parameters and performance of 3D UX-Net across various channel depths and the integration of uC on FLARE2021 and OIMHS datasets.

  Method & mIoU & Dice & ASSD & HD95 & AdjRand \\ 
3D U-Net & 87.08.33.32 & 92.792.92.09 & 1.311.35 & 7.3934.16.99 & 92.2242.15 \\
*uC (stage 1) & 90.064.51.4 & 95.552.6 & 90.661.85 & 7.9349.15 & 91.4142.20 \\
*uC (stage 2) & 89.733.12.31 & 94.371.92 & 9.5350.38 & 2.3644.09 & 93.944.19 \\
*uC (stage 3) & 88.713.31 & 93.752.12 & 0.4300.42 & 2.7005.17 & 93.2662.18 \\
*uC (stage 5) & 87.324.36 & 92.932.27 & 0.696.03 & 3.0545.19 & 92.734.23 \\
*uC (stage 1, 2, 3) & 90.522.77 & 94.541.68 & 6.5300.33 & 2.2046.047 & 94.4341.76 \\  

Table 9: Ablation study of different stage selection for uC integration on the OIMHS dataset.

Figure 3: Qualitative results of the uC’s impact on segmentation performance in 3D UX-Net and SegResNet backbones, applied to the FLARE2021 dataset. Segmentation results for different categories are represented in distinct colors. For improved visual clarity, the images have been cropped. Please kindly zoom in for a better view.

As shown in Table 9, supplementing initial feature information in shallower layers, closer to the original image, resulted in better performance improvements, with deeper layers showing diminishing returns. Hence, to balance parameter efficiency, we only replace skip connections at stages 1 to 3. Results indicate that using skip connections in the first three stages significantly enhances model performance, with the DFi module effectively integrating features extracted by 3D CNNs and uC.

Channel Depth Configuration.We experiment with channel depths of 16, 24, and 32 for the proposed uC 3DU-Net, all surpassing the original 32-channel U-Net, as shown in Table 10. Performance improved with increased channels, but efficiency dropped drastically at 32 channels compared to 24. Notably, the 16-channel uC 3DU-Net performed better than the 32-channel U-Net with similar parameter counts and FLOPs, demonstrating a 1.74% Dice score improvement and a 4.95 HD95 reduction. This underscores that efficiently capturing slice plane features with uC significantly enhances computational efficiency in 3D CNN architecture. Thus, replacing skip connections with uC and reducing channel depth is a straightforward strategy to achieve substantial parameter and computational load reductions while maintaining or enhancing performance.

## 5 Conclusion and future work

In this paper, we propose a U-shaped Connection (uC) for enhancing 3D CNN-based medical image segmentation architecture. This approach is specifically designed to address the inherent axial-slice plane performance drop-off in 3D CNNs, characterized by their inefficiency in extracting high-density axial-slice plane features, which are crucial for accurate 3D medical image segmentation. By replacing traditional skip connections in 3D U-Net with uC, we further develop the uC 3DU-Net, capitalizing on more efficient feature extraction capabilities of both 3D sequential spatial features and 2D axial-slice plane features, reaching the best segmentation accuracy and computational efficiency ratio among all previous SoTA methods. Empirical evaluations on diverse datasets demonstrate that uC 3DU-Net consistently outperforms previous SoTA 3D medical segmentation methods, with notably reduced parameters and FLOPs. This underscores the transformative potential of the uC structure in revolutionizing medical volumetric segmentation by breaking through the intrinsic limitations of 3D convolutions. Future research will extend the application of the uC structure to a broader range of volumetric segmentation tasks, with the aim of continually advancing the performance and efficiency of 3D image segmentation models.

  Method & \#Params & FLOPs & mIoU & Dice & ASSD & HD95 & AdjRank \\ 
3D U-Net (32) & 4.81M & 135.96 & 50.8783.5 & 92.7949.6 & 3.1341.8 & 7.9949.6 & 92.224.15 \\
3D U-Net (16) & 5.3M & 134.36 & 50.9882.9 & 94.5341.6 & 6.0364.2 & 4.2446.66 & 34.114.60 \\
3D U-Net (24) & 21.7M & 286.42 & 90.6712.9 & 94.9118.4 & 0.3449.4 & 2.3552.2 & 94.531.99 \\
3D U-Net (123) & 385.6M & 465.161.9 & **90.8623.5** & **95.0346.0** & **3.3444.4** & **2.3443.52** & **96.461.72** \\  

Table 10: Ablation study of different Channel depths of uC 3DU-Net on the OIMHS dataset.

Figure 4: Qualitative results of the segmentation performance on 3D UX-Net and 3D UXNET+uC with various channel depths on the FLARE2021 and OIMHS datasets are presented. Segmentation results for different categories are represented in distinct colors. For improved visual clarity, the images have been cropped. Key regions of the qualitative results have been locally magnified for better viewing.