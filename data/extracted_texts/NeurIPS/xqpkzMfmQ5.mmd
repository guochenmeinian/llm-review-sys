# Data curation via joint example selection

further accelerates multimodal learning

Talfan Evans\({}^{1,*}\) Nikhil Parthasarathy\({}^{1,*}\) Hamza Merzic\({}^{1,2}\) Olivier J. Henaff\({}^{1,*}\)

\({}^{1}\)Google DeepMind \({}^{2}\)University College London

London, UK

###### Abstract

Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly prioritizing _batches_ of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the _joint learnability_ of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individually-prioritized data points. As performance improves by selecting from large super-batches, we also leverage recent advances in model approximation to reduce the computational overhead of scoring. As a result, our approach--multimodal contrastive learning with joint example selection (JEST)--surpasses state-of-the-art pretraining methods with up to 13\(\) fewer iterations and 10\(\) less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing data curation as a new dimension for neural scaling laws.

## 1 Introduction

Data quality is an essential driver of performance for large-scale pretraining. Whether in language , vision , or multimodal modeling , training on well-curated datasets has consistently demonstrated that strong performance can be achieved with significantly less data. However, current data pipelines rely heavily on manual curation, which is difficult and expensive to scale. In contrast, model-based data curation , which uses features of the model being trained to select high quality data, holds promise for improving the slow, power-law scaling of large-scale pretraining across modalities, both in theory  and in practice .

Existing methods apply curation at the level of individual data points . Yet the quality of a batch is also a function of its composition, in addition to the summed quality of the data points considered independently. In computer vision, hard negatives (i.e. clusters of points which lie close to one another but contain different labels) have been found to provide a more effective learning signal than trivially solvable ones . In this work we seek to generalize this notion by asking whether model-based data-selection criteria applied to batches of data can accelerate learning beyond what is possible by selecting examples independently.

In multimodal learning, contrastive objectives directly expose the interactions between examples in a batch. We therefore derive a simple and tractable algorithm for joint example selection (JEST), which efficiently selects relevant'sub-batches' of data from much larger'super-batches' given their model-based scores. When scoring batches with a pretrained reference model (i.e. _easy-reference_), JEST accelerates learning relative to uniform batch selection, significantly improving independent example selection with the same model (as in CLIPScore ). When scoring batches according to their _learnability_, which also takes into account the learner's loss , JEST improves further, matching the performance of state-of-the-art models  with up to 13\(\) fewer training iterations.

Discovering highly learnable batches requires sifting through much larger super-batches of raw data. We make learnability scoring of large batches tractable by leveraging recent advances in online model approximation which reduce computation while still providing useful predictions [4; 27; 53]. By training a single model at multiple resolutions in parallel, we efficiently apply the model for scoring large super-batches, find their most learnable sub-batch, and spend more valuable computation for learning on them. Thanks to savings in both learning and example scoring, we reduce the overhead of scoring from 133% to 10% additional FLOPs while maintaining significant gains in training efficiency. This approximate scoring framework (Flexi-JEST) produces state-of-the-art models with 11\(\) fewer iterations _and_ 10\(\) fewer FLOPs.

Finally, we find that central to the performance of our framework is the ability to steer the curation process towards the distribution of smaller, well-curated datasets. This occurs naturally with the model-based selection criteria we consider through the concept of a pretrained reference model, which prioritizes examples that most resemble the data it was trained on. Crucially, we find this process to enable strong _data quality bootstrapping_: a reference model trained on a small curated dataset can effectively guide the curation of a much larger dataset, allowing the training of a model which strongly surpasses the quality of the reference model on many downstream tasks.

## 2 Related Work

**Offline curation: example-level data pruning.** Methods for collecting and filtering noisy image-text data initially focused on the quality of the textual captions [22; 8], and proximity to high-quality reference datasets [50; 14]. Instead, model-based filtering approaches use pretrained models (such as CLIP  and BLIP ) evaluation metrics for curating data via image-text alignment [14; 20; 30]. Critically, all of these methods are applied _independently across examples_, which fails to account for the relevance of dependencies across examples in a batch.

**Offline curation: cluster-level data pruning.** Other methods such as semantic redundancy reduction [1; 2; 45] or core-set selection [6; 18] have proposed to curate based on the marginal importance of data points given other data points in their vicinity. However these methods are based on a heuristic that is decoupled from the training objective. In contrast, our method enables joint-example selection that is specifically tailored to accelerating the contrastive pretraining objective function.

**Online data curation with model-based scoring.** Pre-filtering using the curation procedures described above can lead to large increases in data quality. However, fixed curation strategies do not

Figure 1: **Joint Example Selection accelerates multimodal pretraining.** Our JEST / JEST++ methods bootstrap from two small, strongly curated datasets (WebLI-curated / WebLI-curated++) to actively curate web-scale datasets. Flexi-JEST++ uses variable patch sizing to reduce the cost of curation. **Left**: Training with JEST matches the performance of the uniform 40B SigLIP baseline with up to 13\(\) fewer iterations. **Right**: Even when accounting for the cost of scoring, our best variant is almost 10\(\) more FLOP efficient.

take into account that the relevance of a training example can change over the course of learning, limiting their utility at scale . These concerns are addressed by online data curation methods [13; 28; 29; 31], which identify high-quality examples _not yet learned by the model_. Our work generalizes these by applying model-based criteria to batch-level (rather than example-level) losses, and selecting data accordingly.

**Hard negative mining.** A long literature has described the efficiency gains afforded by choosing the right set of negative examples in classical metric-learning [5; 19; 32; 43; 48; 51] as well as modern contrastive learning [39; 47]. We generalize hard negative mining in two ways: 1) we jointly mine for both positive and negative pairs 2) we explore prioritizing _learnable_ negatives, which are hard for the learner but easy for a pretrained model.

**Model approximation.** Several works have demonstrated that smaller models can be used as proxies for much larger models [10; 49; 13] for data selection. However, several techniques have recently been developed that allow inference-time trade-offs between computation and performance, allowing smaller models to be "embedded" without the need for separate training. For Vision Transformers , dropping patches  or layers , or reducing token resolution  produce characteristic trade-offs . This work is the first to use these techniques in the context of online data selection.

## 3 Methods

### Model-based batch-selection criteria

We refer to the model which we are interested in training as the _learner_. Assuming we have a "super-batch" \(\) (of size \(B\)) examples to learn from, we wish to extract a sub-batch \(=\{_{i},i[1,...,b]\}\) that is maximally relevant for learning. Prioritized sampling [29; 41] performs this by scoring individual examples, then sampling in proportion to these scores. In this work we instead score entire sub-batches, and sample according to these batch-level scores. We consider model-based scoring functions, which use the losses from the learner model and/or pre-trained _reference_ models.

**Hard learner.** An intuitive heuristic would be to prioritize batches \(\) that have a high loss under the learner with parameters \(\): \(s^{}(|)=(|)\), which has the desirable property of discarding trivial data. This heuristic has been proven to work for small, clean datasets [34; 45] but tends to do more harm than good for larger, less curated datasets  since it will also up-sample noisy data.

**Easy reference.** In contrast, one could also choose to up-sample data that is "easy" (has low loss) for a pre-trained _reference_ model with parameters \(^{*}\): \(s^{}(|^{*})=-(|^{*})\). This _easy reference_ heuristic has been used successfully in multi-modal learning to identify high-quality examples [20; 42], but does not reflect the current state of the learner and can therefore be overly dependent on the choice of reference model  and not scale to large compute budgets .

**Learnability.** Finally, Mindermann et al.  propose to combine these scores, prioritizing with the difference of losses: \(s^{}(|,^{*})=s^{}(| )+s^{}(|^{*})=(|)- (|^{*})\). This heuristic, which we refer to as _learnability_ scoring throughout, has the advantage of up-sampling data that is both unlearned and learnable, and has been shown to accelerate large-scale learning even when prioritizing individual examples in isolation . In this work, we therefore mainly consider _learnability_ scoring but for completeness also provide ablations with _easy reference_ scoring.

The ratio of the "sub-batch" and "super-batch" sizes define the _filtering ratio_\(f=1-b/B\), i.e. the proportion of data discarded at each iteration. For a given learner batch size \(b\), higher filtering ratios increase the cost of scoring as they require more inference passes on the super-batch.

### Joint example selection (JEST) for multimodal learning

**Multimodal learning losses.** Given the availability of internet-scale datasets of paired images and text, multimodal learning has become the default means of training visual representations. Contrastive learning aims to maximize the alignment of these two modalities for paired examples, while minimizing the alignment of unpaired examples. Both sigmoid-  and softmax-contrastive  losses achieve this with a batch-level loss \((|)=_{i=1}^{b}(_{i}|, )\), where the conditional loss \((_{i}|,)\) can use a sigmoid or softmax contrast function (see Equations 1 and 2). SinceZhai et al.  demonstrate the sigmoid-contrastive loss to be a more scalable alternative to the softmax-contrastive one, we adopt it by default.

**Joint example selection.** Because the contrastive loss of a batch decomposes into a sum of conditional losses, the _joint learnability_ of the batch \(s(|,^{*})(|)-( |^{*})=_{i=1}^{b}(_{i}|, )-(_{i}|^{*},)=_{i=1}^{b}s (|,^{*},)\) also decomposes into a sum of _conditional learnabilities_\(s(|,^{*},)\) of each example given other examples in the batch. We wish to sample batches in proportion to their joint learnability, i.e. \(p(\{X_{k}\}=)(s(|,^{*}))\), which is enabled by a sequential approach inspired by blocked Gibbs sampling (see Algorithm 1). Given a subset of examples \(_{n}\) already included in the batch at iteration \(n\), we compute the _conditional learnability_ of remaining candidate examples \(_{i}\) with \(s(_{i}|,^{*},_{n})\), and sample a new chunk of examples \(\{X_{k}\}\) independently without replacement according to these probabilities: \(p(X_{k}=_{i})(s(_{i}|,^{*},_{n}))\).

We update the batch by appending this chunk to the previous subset: \(_{n+1}=_{n}\{X_{k}\}\), and iterate until \(n=N\), the number of chunks. The first chunk \(_{1}\) is sampled using unconditional learnability (i.e. self-similarity only) \(s(_{i}|,^{*},))=(_{i}|, )-(_{i}|^{*},)\) where the unconditional losses are computed as \((_{i}|,)=-_{i}^{}_{i }^{}\) for the softmax-contrastive loss and \((_{i}|,)=[1+(-_{i}^{ {im}}_{i}^{}+)]\) for the sigmoid-contrastive loss. We find that a relatively small number of chunks (\(N=16\), sampling \(b/N\) = 2,048 examples independently at each iteration) is sufficient to recover batches with very high learnability (see section 4.1).

### Efficient scoring and multi-resolution training

**Efficient scoring with online model approximation.** Scoring large super-batches increases the cost per iteration, lowering the efficiency gains in terms of total FLOPs. While  required additional small, proxy models to efficiently score data on behalf of larger learners, we remove this requirement by using online model approximation. We only approximate the image encoding since this accounts for the bulk of the cost of each inference pass . For this we adopt the FlexiViT architecture , which lowers the image resolution while minimally degrading performance (see Appendix Figure A.4 for a comparison to patch dropping [12; 27]). In our experiments, we evaluate the super-batch with 32\(\)32-pixel patches, which gives a 72% reduction in FLOPs and 67% reduction in wall-clock time vs. full-resolution scoring at patch size 16\(\)16  (see Section A.3).

**Multi-resolution training.** While we want to score examples at low resolution (i.e. with large patches), at test-time we wish to evaluate the model at full resolution (i.e. with small patches). To enable both resolutions of computation, we simply train at both resolutions. Specifically, given a sub-batch \(\) for learning, we randomly split it into two halves, \(^{lo}\) and \(^{hi}\), and encode each half with a different resolution: \(^{lo}=\{f^{}(;,p=32),^{lo} \},^{hi}=\{f^{}(;,p=16), ^{hi}\}\). These images embeddings are then concatenated together as \(=^{lo}^{hi}\) and the rest of training proceeds as usual. In addition to allowing for efficient scoring, multi-resolution training itself yields a gain in efficiency: since \(^{lo}\) is processed with 4\(\) fewer tokens, it also benefits from close to a 4\(\) speed-up. If \(^{lo}\) and \(^{hi}\) each account for half of the batch, the cost of multi-resolution training on \(\) is 64% of the FLOPs and 67% of the time of full-resolution training. Pseudocode for the full-resolution JEST and multi-resolution Flexi-JEST implementation is detailed in Algorithm A.1.

**Training datasets.** We train the learner model in all JEST experiments on the WebLI dataset , specifically a billion-scale subset of English image-text pairs loosely filtered with image-text alignment , using the big_vision codebase . To train reference models, we use smaller high-quality datasets. JEST/Flexi-JEST reference models are trained on a strongly filtered 100M scale subset of WebLI filtered for high text and image quality and image-text alignment, which we refer to as "WebLI-curated". We additionally explore _scaling data curation_ (JEST++/FlexiJEST++) using reference models trained on "WebLI-curated++" which adds approximately 600M additional web-scraped image-text pairs filtered with the same strong curation pipeline.

## 4 Experiments

### Joint example selection yields learnable batches

We start by evaluating the efficacy of joint example selection (JEST) for selecting learnable batches. To gain an intuition for our method, we start by visualizing the learnability matrix (i.e. the difference in loss between learner and reference models, for all pairs of examples in the batch). JEST is designed to sample sub-matrices of examples in proportion to their summed learnability. Since the matrix is strongly non-diagonal (Figure 2, left), independent selection will clearly be sub-optimal.

With a small number of iterations (corresponding populating the batch with \(N=16\) chunks), we find the learnability of the sub-batch to quickly increase, matching the learnability of batches extracted by brute-force Gibbs sampling requiring thousands of iterations (Figure 2, middle).

For filtering ratios of 0.5, 0.8, and 0.9, we select sub-batches of 32,768 examples from super-batches of size 65,536, 163,840 and 327,680 respectively. In Figure 2, right, we find that the learnability of the sub-batch increases with larger filtering ratios. In summary, our joint example selection (JEST) algorithm is an effective and efficient means of selecting learnable batches during training.

### Joint example selection accelerates multimodal learning

We now investigate the effect of training on more learnable batches, as selected by our JEST algorithm. All runs use a reference model trained on WebLI-curated, a ViT-B/16 and Bert-B image-text dual encoder, 3 billion training examples, and the sigmoid-contrastive loss. Figure 3 (left) shows the average performance on multiple downstream tasks (ImageNet 0-Shot/10-Shot accuracy and COCO image-to-text/text-to-image retrieval) over the course of training. We find that JEST significantly accelerates learning, reaching the final performance of the 3B-uniform baseline after only 2B, 1B, and 0.67B training examples, when using filtering ratios of 50%, 80%, and 90% respectively. At larger filtering ratios we observe similar training instabilities to those observed for larger batch sizes , necessitating a modification to stabilize the Adam optimizer (\(_{2}=0.95\)) and suggesting that data curation with JEST can be thought of as increasing the effective batch size (Appendix A.1, A.2).

Figure 2: **Joint example selection yields more learnable batches. Left:** the learnability of a batch is highly structured and non-diagonal. **Middle:** Joint example selection quickly discovers sub-batches with high learnability, on-par with brute-force Gibbs sampling. **Right:** the learnability of sampled batches improves with higher filtering ratios (i.e. selecting from larger super-batches).

In terms of final performance, JEST also delivers significant gains of up to 6% when filtering 90% of data (Figure 3, middle, blue curve). Notably, this scaling behavior is absent from previous selection methods based on independent prioritization of individual examples (Figure 3, middle, orange curve). Finally, we assess whether JEST also improves prioritization criteria other than learnability. Figure 3, right, shows the performance of models with _easy-reference_ prioritization, for varying filtering ratios. Consistent with learnability-based prioritization, JEST strongly outperforms independent example selection, particularly for high filtering ratios (where independent example selection leads to a regression in performance). Prioritising data with the highest loss produced smaller gains and degrades more quickly as we filter more data (Appendix Figure A.6). Since learnability-based JEST yielded the best scaling behavior we retain this criterion for subsequent experiments.

### Synergies between multi-resolution training and online batch selection

Joint example selection with _learnability_ scores becomes more efficient as larger fractions of each batch are filtered. However, the cost of scoring results in a significant overhead: filtering 80% of the super-batch results in 4\(\) more FLOPs per iteration than IID training, or 2.3\(\) when caching the reference-model scores (Appendix A.3). Although JEST is significantly more efficient in terms of training iterations (hereinafter 'training efficiency'), the additional scoring FLOPs reduce its compute efficiency relative to the IID baseline (Figure 1, left vs. right). We therefore also investigated a compute efficient variant, Flexi-JEST, which uses multi-resolution training and low-resolution scoring to reduce the total overhead to only 10% vs. the baseline (Figure 4, left; see Section A.3).

What is the effect of these approximations on performance? As might be expected, the per-iteration performance of Flexi-JEST decreases relative to the JEST, although still produces significant speed-ups over IID (Figure 1, left; Figure 4, middle). However, the decrease in per-iteration performance is more than favorable when accounting for the decrease in total FLOPS: our best Flexi-JEST model produces the same average performance as a 40B Siglp run with 9.9\(\) fewer FLOPs, and 2\(\) fewer than full-resolution JEST (Figure 1, right; Figure 4, middle).

What is the relative contribution of efficient scoring and multi-resolution training in Flexi-JEST? We conducted an ablation where we varied the fraction of the selected batch trained at full and low resolution (i.e. the relative sizes of \(^{hi}\) and \(^{lo}\); see Methods). For fair comparison we ensure the learner spends the same FLOPs by increasing the number of training iterations as we send more data to the approximate model, since the FLOPs per iteration decrease in this case (see Section A.6 for details). Figure 4 (right) shows that the IID baseline performance increases with larger fractions of data sent to the approximate model, consistent with a growing literature on the FLOP-efficiency of approximate training [4; 27; 11; 38]. Nevertheless, Flexi-JEST significantly outperforms the multi-resolution baseline as soon as it trains the approximate model (e.g. with as little as 25% data). These experiments demonstrate a synergy between multi-resolution training and joint example selection, as the former yields efficient and accurate scoring capabilities for accelerating the latter.

Figure 3: **Joint example selection accelerates multimodal learning. Left:** training on the most learnable sub-batch selected from super-batches that are 2\(\), 5\(\), or 10\(\) larger significantly accelerates multimodal learning. **Middle:** Jointly prioritizing _learnable batches_ yields significantly better results than simply prioritizing individual examples. **Right:** joint examples selection also improves _easy reference_ prioritization, although _learnability_ scales better with more aggressive filtering.

Our results also point to a pareto front of data curation strategies. If maximizing training speed or training efficiency is desirable at the expense of computation, the full-resolution JEST method produces up to a 13\(\) speed up relative to a comparable IID training run. If FLOPs should be minimized at the expense of training efficiency, Flexi-JEST produces the most favorable trade-off. We note that the scoring of the next batch can be implemented on separate devices, in parallel with training, potentially further reducing the additional wall-clock time.

### Joint example selection enables strong data-quality bootstrapping

At the heart of learnability-based scoring is a reference model trained on a small, curated dataset of our choosing. How does JEST performance vary as a function of different curation strategies that trade off quality vs. quantity? Furthermore, do improvements in JEST training correlate with the performance of the reference models or are these metrics decoupled?

**Understanding quality vs. quantity trade-offs**. We explore three scales of curation, each being a subset of the original WebLI dataset: _weak_ (billion-scale) curation with image-text alignment (ITA) filters, _moderate_ (300M scale) curation with either ITA filters or text-quality (TQ) filters, and _strong_ (100M scale) curation with a combination of TQ, ITA, and additional image-quality (aesthetic) filters. Throughout, we refer to this strongly curated subset as "WebLI-curated".

We train standard SigLIP encoders on these four WebLI subsets for 10 epochs each, and use them as reference models for JEST training on the full WebLI dataset. Across curation methods, reference model performance and JEST performance appear to be decoupled (or even anti-correlated; Figure 5, left), consistent with previous findings for fixed data curation . Whereas increasing curation (and decreasing dataset size) yields weaker models, when used as reference models for JEST pretraining they have the opposite effect: JEST with a strongly-curated reference benefits from a 2.7% improvement, moderate a 1.5% improvement, and weak a 0.3% improvement.

**Scaling data curation**. We hypothesized that the general decoupling between reference model performance and JEST performance might simply be explained by the dataset size limits imposed by data curation. To understand this effect, we trained 5 reference models on WebLI-curated while varying the total examples seen (250M to 3B). In this context, Figure 5 (right) shows a striking correlation between improved reference models and better JEST pretraining. This suggests that the "decoupling" phenomenon can be mostly attributed to the saturation of the reference model as a result of the reductions in dataset size following curation.

We note that the correlation in Figure 5 (right) starts to break down when the dataset is saturated, i.e. after 10 epochs or 1B examples seen. We therefore demonstrate how scaling data curation benefits JEST by augmenting WebLI-curated to a total of approximately 600M examples sourced from an

Figure 4: **Efficient scoring and multi-resolution training**. **Left:** In scoring large super-batches with the learner and reference models, JEST incurs a large computational cost per iteration. By caching the fixed reference model scores in the dataset, this overhead can be cut in half. Efficient scoring and multi-resolution training further reduce this to be comparable to standard IID training. **Middle:** _F_exi-JEST improves the total FLOP-efficiency of JEST over standard IID training. **Right:** Multi-resolution training improves FlexiJEST more than standard IID training. Without multi-resolution training (left-most point) Flexi-JEST underperforms the IID baseline (due to an untrained approximate model), but quickly improves with even a small amount of co-training (25%).

expanded set of image-text pairs. Critically, these examples all still pass the same set of strong TQ, ITA, and IQ filters--we denote this dataset as "WebLI-curated++". We find that this scaled dataset allows us to break the 2B saturation point for "WebLI-curated" as both reference model and JEST performance (Figure 5, Right: \(\)) improves significantly. We therefore use WebLI-curated++ for our best models, JEST++ and FlexiJEST++.

### Comparison to prior art

We now compare to prior art, including the state-of-art SigLIP model trained for 40 billion examples  as well as recent strong CLIP variants. Table 1 shows that our most training-efficient model, JEST++, sets a new state-of-the-art on both ImageNet and COCO all while using 10\(\) fewer iterations and 4\(\) less compute. On COCO in particular, JEST++ improves the previous state of the art by over 5%. Our most compute-efficient model, Flexi-JEST++, also surpasses the previous SoTA on average, while using 9\(\) less compute. Training JEST for longer furthered these gains (see Appendix Table 3).

Our results also scale gracefully with model size. Training with a ViT-L learner and ViT-L reference trained on the same WebLI-curated++ dataset, JEST++ continues to yield strongly accelerated learning, matching the SigLIP ViT-L 40B baseline with only 4B examples seen (Table 1, bottom).

Finally, we apply JEST++ for pretraining on two publicly available datasets: LAION-2B  and DataComp-1B . The DataComp experiments compare against Data Filtering Networks (DFN)  and are shown in Appendix Table 5. We find that JEST++ strongly surpasses DFN, and verify that these gains come primarily from switching from offline, independent selection to online joint-example selection using learnability scoring. For the LAION experiments, we follow the standard practice of removing unsafe image-text pairs , but do not otherwise pre-filter the dataset. JEST++ strongly surpasses previous methods for offline data curation, despite using 4\(\) fewer training examples than the previous state-of-the-art (Table 2). With this training budget, SigLIP pretraining severely under-performs all methods, further highlighting the benefits of JEST. JEST with a reference model trained on the smaller LAION-400m subset, which is not a curated subset but instead an independent set collected in a similar manner to LAION-2B, did not outperform IID training on the latter.

Figure 5: **Scaling strong data curation improves JEST performance. Left:** We compare JEST performance vs. reference model performance (relative to the uniform baseline) for 4 curation types: ’weak’ curation with image-text alignment (ITA), ’moderate’ curation with ITA or text-quality (TQ), and ‘strong’ curation (using a combination of TQ, ITA, and additional image-quality (IQ). **Right:** We use our best reference dataset (TQ+ITA+IQ) and evaluate JEST vs. reference performance varying the number of examples seen during reference pretraining. There is a strong correlation between additional reference training and JEST performance that saturates after 1B examples seen. By scaling strong data curation to a 600M dataset, this saturation is broken as both reference model and JEST performance improve for the 1B _and_ 2B reference training.

## 5 Discussion

We proposed a method, JEST, for jointly selecting the most learnable batches of data which significantly accelerates large-scale multimodal learning, surpassing the previous state-of-the-art with up to 10\(\) fewer FLOPs and 13\(\) fewer examples. Our experiments point to the strong potential for "data quality bootstrapping", using small curated datasets to guide learning on much larger, uncurated ones.

Recent work has shown that filtering datasets without knowledge of downstream training can ultimately limit performance . Our results further demonstrate that dynamically constructing useful batches improves pretraining efficiency beyond individually selected examples. These findings therefore advocate for _foundation distributions_--either through pre-scored datasets with _easy-reference_ JEST, or dynamically adjusted to the demands of the model with _learnability_ JEST--as a more general and effective replacement to generic foundation datasets.

**Limitations.** While our method has accelerated multimodal learning of canonical downstream tasks, it has relied on small, well-curated reference datasets which specify the distribution to prioritize within much larger uncurated data. We would therefore encourage future work exploring the inference of reference datasets from the set of downstream tasks of interest.