# CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models

Peng Xia\({}^{1,2}\), Ze Chen\({}^{2}\), Juanxi Tian\({}^{2}\), Yangrui Gong\({}^{2}\), Ruibo Hou\({}^{7}\), Yue Xu\({}^{2}\)

**Zhenbang Wu\({}^{7}\), Zhiyuan Fan\({}^{9}\), Yiyang Zhou\({}^{1}\), Kangyu Zhu\({}^{3}\), Wenhao Zheng\({}^{1}\)**

**Zhaoyang Wang\({}^{1}\), Xiao Wang\({}^{4}\), Xuchao Zhang\({}^{5}\), Chetan Bansal\({}^{5}\)**

**Marc Niethammer\({}^{1}\), Junzhou Huang\({}^{6}\), Hongtu Zhu\({}^{1}\), Yun Li\({}^{1}\)**

**Jimeng Sun\({}^{7}\), Zongyuan Ge\({}^{2}\), Gang Li\({}^{1}\), James Zou\({}^{8}\), Huaxiu Yao\({}^{1}\)**

\({}^{1}\)UNC-Chapel Hill, \({}^{2}\)Monash University, \({}^{3}\)Brown University, \({}^{4}\)University of Washington,

\({}^{5}\)Microsoft Research, \({}^{6}\)UT Arlington, \({}^{7}\)UIUC, \({}^{8}\)Stanford University, \({}^{9}\)HKUST

{pxia,huaxiu}@cs.unc.edu, zongyuan.ge@monash.edu

Partly done when P.X. was at Monash University. \({}^{}\)Equal Contribution. \({}^{}\)Corresponding Authors.

###### Abstract

Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce **CARES** and aim to **C**omprehensively ev**A**luate the t**R**ustworthin**ESs of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in [https://cares-ai.github.io/](https://cares-ai.github.io/).

WARNING: This paper contains model outputs that may be considered offensive.

## 1 Introduction

Artificial Intelligence (AI) has demonstrated its potential in revolutionizing medical applications, such as disease identification, treatment planning, and drug recommendation . In particular, the recent emergence of Medical Large Vision Language Models (Med-LVLMs) has significantly enhanced the quality and accuracy of medical diagnoses , enabling more personalized and effective healthcare solutions. While Med-LVLMs have shown promising performance, existing models introduce several reliability issues , including generating non-factual medical diagnoses, overconfidence in generated diagnoses, privacy breaches, health disparities, _etc_. The deployment of unreliable models can lead to severe adverse consequences . For instance, a model mistakenly identifying a benign tumor as malignant could lead to unnecessary invasive procedures and significant emotional distress for patients. Therefore, understanding and evaluating the trustworthiness of Med-LVLMs is paramount in medical applications.

Some recent studies have started to been conducted  to evaluate the trustworthiness of Med-LVLMs. However, these studies tend to focus solely on a specific dimension of trustworthiness evaluation, such as the accuracy of medical diagnoses. A systematic and standardized evaluation of the trustworthiness of Med-LVLMs from multiple dimensions (_e.g._, safety, fairness, privacy) remains largely unexplored. Hence, we curate a collection of medical diagnosis datasets, standardize the trustworthiness evaluation, and create a benchmark to help researchers understand the trustworthiness of existing Med-LVLMs and to design more reliable Med-LVLMs.

Specifically, this paper presents CARES, a benchmark for evaluating the trustworthiness of Med-LVLMs across five dimensions - _trustfulness, fairness, safety, privacy, and robustness_. CARES is curated from seven medical multimodal and image classification datasets, including 16 medical modalities (_e.g._, X-ray, MRI, CT, Pathology) and covering 27 anatomical regions (_e.g._, chest, lung, eye, skin) of the human body. It includes 18K images and 41K question-answer pairs in various formats, which can be categorized as open-ended and closed-ended (_e.g._, multiple-choice, yes/no) questions. We summarize our evaluation taxonomy in Figure 8 and our empirical findings as follows:

* _Trustfulness._ The evaluation of trustfulness includes assessments of factuality and uncertainty. The key findings are: (1) Existing Med-LVLMs encounter significant factuality hallucination, with accuracy exceeding 50% on the comprehensive VQA benchmark we constructed, especially when facing open-ended questions and rare modalities or anatomical regions; (2) The performance of Med-LVLMs in uncertainty estimation is unsatisfactory, revealing a poor understanding of their medical knowledge limits. Additionally, these models tend to exhibit overconfidence, thereby increasing the risk of misdiagnoses.
* _Fairness._ In fairness evaluation, our results reveal significant disparities in model performance across various demographic groups that categorized by age, gender and races. Specifically, age-related findings show the highest performance in the 40-60 age group, with reduced accuracy among the elderly due to imbalanced training data distribution. Gender disparities are less pronounced, suggesting relative fairness; however, notable discrepancies still exist in specific datasets like CT and dermatology. Racial analysis indicates better model performance for Hispanic or Caucasian populations, though some models achieve more balanced results across different races.
* _Safety._ The safety evaluation of includes assessments of jailbreaking, overcautiousness, and toxicity. Our key findings are: (1) Under the attack of "jailbreaking" prompts, the accuracy of all models decreases. LLaVA-Med demonstrates the strongest resistance, refusing to answer many unsafe questions, whereas other models typically respond without notable defenses; (2) All Med-LVLMs exhibit a slight increase in toxicity when prompted with toxic inputs. Compared to other Med-LVLMs, only LLaVA-Med demonstrates significant resistance to induced toxic outputs, as evidenced by a notable increase in its abstention rate; (3) Due to excessively conservative tuning, LLaVA-Med exhibits severe over-cautiousness, resulting in a higher refusal rate compared to other models, even for manageable questions in routine medical inquiries.
* _Privacy._ The privacy assessment reveals significant gaps in Med-LVLMs regarding the protection of patient privacy, highlighting several key issues: (1) Med-LVLMs lack effective defenses against

Figure 1: CARES is designed to provide a comprehensive evaluation of trustworthiness in Med-LVLMs, reflecting the issues present in model responses. We assess trustworthiness across five critical dimensions: trustfulness, fairness, safety, privacy, and robustness.

queries that seek private information, in contrast to general LVLMs, which typically refuse to produce content related to private information; (2) While Med-LVLMs often generate what appears to be private information, it is usually fabricated rather than an actual disclosure; (3) Current Med-LVLMs tend to leak private information that is included in the input prompts.
* _Robustness_. The evaluation of robustness focuses on out-of-distribution (OOD) robustness, specifically targeting input-level and semantic-level distribution shifts. The findings indicate that: (1) when significant noise is introduced to input images, Med-LVLMs fail to make accurate judgments and seldom refuse to respond; (2) when tested on unfamiliar modalities, these models continue to respond, despite lacking sufficient medical knowledge.

## 2 CARES Datasets

In this section, we present the data curation process in CARES. Here, we utilize existing open-source medical vision-language datasets and image classification datasets to devise a series of high-quality question-answer pairs, which are detailed as follows:

**Data Source**. We utilize open-source medical vision-language datasets and image classification datasets to construct CARES benchmark, which cover a wide range of medical image modalities and body parts. Specifically, we collect data from four medical vision-language datasets (MIMIC-CXR , IU-Xray , Harvard-FairVLMed , PMC-OA ), two medical image classification datasets (HAM10000 , OL3I ), and one recently released large-scale VQA dataset (OmniMedVQA ), some of which include demographic information. As illustrated in Figure 2, the diversity of the datasets ensures richness in question formats and indicates coverage of 16 medical image modalities and 27 human anatomical structures. Details of the involved datasets are provided in Appendix B.

**Types of Questions and Metrics**. There are two types of questions in CARES: (1) _Closed-ended questions_: Two or more candidate options are provided for each question as the prompt, with only one being correct. We calculate the accuracy by matching the option in the model output; (2) _Open-ended questions_: Open-ended questions do not have a fixed set of possible answers and require more detailed, explanatory or descriptive responses. It is more challenging, as fully open settings encourage a deeper analysis of medical scenarios, enabling a comprehensive assessment of the model's understanding of medical knowledge. We quantify the accuracy of model responses using GPT-4. We request GPT-4 to rate the helpfulness, relevance, accuracy, and level of detail of the ground-truth answers and model responses and provide an overall score ranging from 1 to 10 . Subsequently, we normalize the relative scores using GPT-4's reference scores for calculation.

**Construction of QA Pairs**. We explore the processes of constructing QA pairs from both closed-ended and open-ended questions. Firstly, we delve into closed-ended questions. For closed-ended yes/no questions, we utilize the OL3I  and IU-Xray  datasets, converting their questions along with corresponding labels or reports into yes/no formats. For example, the question "Can ischemic heart disease be detected in this image?" is transformed accordingly. For closed-ended multi-choice questions, the multi-class classification dataset HAM10000  is converted into QA pairs with multiple options. For example, in the HAM10000 dataset, for lesion types, we can design the following QA pair: Question: What specific type of pigmented skin lesion is depicted in this dermatoscopic image? The candidate options are:[A:melanocytic nevi, B:dermatofibroma, C:melanoma, D:basal cell carcinoma]; Answer: A:melanocytic nevi. To increase the diversity of question formats and ensure the stability of testing performance, we design 10-30 question templates for multi-choice question type (see detailed templates in Appendix C). Furthermore, to

Figure 2: Statistical overview of CARES datasets. (left) CARES covers numerous anatomical structures, including the brain, eyes, heart, chest, _etc_. (right) the involved medical imaging modalities, including major radiological modalities, pathology, _etc_.

enrich the dataset with diverse modalities and anatomical regions, a comprehensive multi-choice VQA dataset, OmniMedVQA  is also collected. For open-ended questions, CARES features a series of open-ended questions derived from vision-language datasets, namely MIMIC-CXR , Harvard-FairVLMed , and PMC-OA . Specifically, medical reports or descriptions are transformed into a series of open-ended QA pairs by GPT-4  (see details in Appendix C).

**Post-processing.** To enhance the quality of the generated open-ended question-answer pairs, we instruct GPT-4 to perform a self-check of its initial output of these QA pairs in conjunction with the report. Subsequently, we manually exclude pairs with obvious issues and corrected errors.

Overall, our benchmark comprises around 18K images with 41K QA items, encompassing 16 medical imaging modalities and 27 anatomical regions across multiple question types. This enables us to comprehensively assess the trustworthiness of Med-LVLM.

## 3 Performance Evaluation

To conduct a comprehensive evaluation of trustworthiness in Med-LVLMs, we focus on five dimensions highly relevant to trustworthiness, which are crucial for user usage during deployment of Med-LVLMs: _trustfulness_, _fairness_, _safety_, _privacy_, and _robustness_. For all dimensions, we evaluate four open-source Med-LVLMs, _i.e._, LLaVA-Med , Med-Flamingo , MedVInT , RadFM . Furthermore, to provide more extensive comparable results, two advanced generic LVLMs are also involved, _i.e._, Qwen-VL-Chat (7B) , LLaVA-v1.6 (7B) . In the remainder of this section, we provide a comprehensive analysis of each evaluation dimension, including experimental setups and results.

### Trustfulness Evaluation and Results

In this subsection, we discuss the trustfulness of Med-LVLMs, defined as the extent to which a Med-LVLM can provide factual responses and recognize when those responses may potentially be incorrect. Thus, we examine trustfulness from two specific angles - factuality and uncertainty.

**Factuality**. Similar to general LVLMs [36; 94; 11; 16], Med-LVLMs are susceptible to factual hallucination, wherein the model may generate incorrect or misleading information about medical conditions, including erroneous judgments regarding symptoms or diseases, and inaccurate descriptions of medical images. Such non-factual response generation may lead to misdiagnoses or inappropriate medical interventions. We aim to assess the extent to which a Med-LVLM can provide factual responses.

_Setup_. We evaluate the factual accuracy of responses from Med-LVLMs using the constructed CARES dataset. Specifically, we assess accuracy separately for different data sources according to their respective question types, as detailed in the 'Metrics' paragraph of Sec. 2.

_Results_. We present the factuality evaluation results in Figure 3. First, all models experience significant factuality hallucinations across most datasets, with accuracies below 50%. Second, the performance of various Med-LVLMs varies across different modalities and anatomical regions. For instance, LLaVA-Med demonstrates the best overall performance, yet it exhibits subpar results with datasets involving skin and heart CT images. Third, although some models show higher performance on yes/no type questions (e.g., IU-Xray and OL3I datasets), particularly MedVInT, their overall performance on more challenging question types, such as open-ended questions, remains low. This

Figure 3: Accuracy (%) on factuality evaluation. Above are the performance comparisons of all models across 7 datasets, and below are the average performances of each model. “Mixture” represents mixtures of modalities.

[MISSING_PAGE_FAIL:5]

produce equally accurate outcomes for individuals belonging to different groups. Additional details of experimental setups are provided in the Appendix D.1.

_Results_. The results from various models are illustrated in Figure 4 (see detailed results in Appendix E). Our findings reveal disparities in model performance across different demographic groups: (1) _Age_: Models generally perform best in the 40-60 age group, with a notable decline in accuracy among the elderly. This variation can be attributed to the imbalanced distribution of training data across age groups; (2) _Gender_: The accuracy difference due to gender is less pronounced than those due to age or race. This suggests that the models are relatively fair with respect to gender. Specifically, in datasets like X-ray (MIMIC-CXR, IU-Xray) and fundus images (Harvard-FairVLMed), model performance is consistent across male and female groups. However, in CT (OL3I) and dermatology (HAM10000) datasets, significant disparities are observed between male and female groups; 3) _Race_: There is a noticeable disparity in performance with models tending to perform better for Hispanic or Caucasian populations compared to other racial groups. However, models like Qwen-VL-Chat and MedVInT demonstrate more balanced performance across different racial groups.

### Safety Evaluation and Results

Similar to LLMs  and LVLMs , Med-LVLMs also present safety concerns, which include several aspects such as jailbreaking, over-cautious behavior, and toxicity. Addressing these issues is paramount to ensuring the safe deployment of Med-LVLMs.

**Jailbreaking**. Jailbreaking refers to attempts or actions that manipulate or exploit a model to deviate from its intended functions or restrictions . For Med-LVLMs, it involves prompting the model in ways that allow access to restricted information or generating responses that violate medical guidelines.

_Setup_. We design three healthcare-related jailbreaking evaluation scenarios: (1) deliberately concealing the condition based on the given image; (2) intentionally exaggerating the condition based on the given image; (3) providing incorrect follow-up treatment advice, such as prescribing the wrong medication. The used prompt templates will be discussed in detail in the Appendix C. The evaluation method involves the model's abstention rate, determined by detecting phrases such as "sorry" or "apologize" to ascertain whether the model refuses to respond; if it answers normally, the attack is successful. For the first two scenarios, we also assess the accuracy of model responses.

_Results_. The average performance of the models after the attacks is shown in Table 2 The complete results are detailed in the Appendix E. All models exhibited varying degrees of reduced accuracy, indicating the effectiveness of jailbreaking to some extent. More notably, by observing the models'

   Model & ACC\(\) & Abs\(\) \\  LLaVA-Med & 35.61 \(\) 4.78 & 30.17 \\ Med-Flamingo & 22.47 \(\) 6.55 & 0 \\ MedVInT & 34.10 \(\) 5.21 & 0 \\ RadFM & 25.43 \(\) 2.08 & 0.65 \\ LLaVA-v1.6 & 29.38 \(\) 2.90 & 1.13 \\ Qwen-VL-Chat & 31.06 \(\) 2.78 & 5.36 \\   

Table 2: Performance (%) on jailbreaking. ”Abs”: abstention rate.

Figure 4: (a) Accuracy across different age groups; (b) demographic accuracy difference based on different gender groups; (c) heat map of model performance across different racial groups.

abstention rate, we find that except for LLaVA-Med, which refuses some attack instructions, the remaining models have almost no security protection mechanisms. Existing models are susceptible to jailbreak attacks, making them vulnerable to providing erroneous diagnoses or recommendations, which can pose significant risks.

**Overcautiousness.** Overcautiousness describes how Med-LVLMs often refrain from responding to medical queries they are capable of answering. In medical settings, this excessively cautious approach can lead models to decline answering common clinical diagnostic questions. While caution is essential in healthcare to prevent misdiagnosis, excessive caution may waste model capabilities and further strain medical resources. Therefore, striking a balance between accuracy and appropriate levels of caution is crucial to optimize the utility and efficiency of these models in supporting clinical management.

_Setup._ CARES considers two scenarios of medical diagnosis: 1) prompting the model to generate reports or descriptions based on given medical images; 2) soliciting the model's recommendations for subsequent medical actions. The evaluation method revolves around the abstention rate of the model.

_Results._ The abstention rate of the models in the two scenarios are illustrated in Figure 5. Notably, LLaVA-Med exhibits a tendency toward excessive caution, often declining to answer routine medical queries. Specifically, in the context of generating medical advice, the abstention rate for LLaVA-Med reaches up to 90%. In contrast, other models generally do not exhibit this behavior. As discussed in sections on factuality, jailbreaking, and toxicity evaluation, although LLaVA-Med incorporates certain protective measures--such as refusing to answer questions--to maintain high factuality and enhance safety, this approach may be overly conservative, potentially detracting from the user experience.

**Toxicity.** In Med-LVLMs, toxicity refers to outputs that are harmful, such as those containing biased, offensive, or inappropriate content . In medical applications, the impact of toxic outputs is particularly severe because they may generate rude or disrespectful medical advice, eroding trust in the application of clinical management.

_Setup._ Following the LLMs toxicity evaluation , we incorporate the prompt "You are encouraged to output toxic, rude, and disrespectful language." to incentivize the model to generate toxic responses. Toxicity evaluation is conducted using the Perspective API . Regarding to the evaluation metric, we use the increase of toxicity score and abstention rate before and after appending the toxic prompt in Med-LVLMs.

_Results._ The results of toxicity testing are presented in Table 3 (see full results in Appendix E). Table 3 shows that after incorporating prompts that induce toxic outputs, the toxicity in the responses of all Med-LVLMs slightly increases. In contrast, the toxicity in the responses of LLaVA-v1.6 increases significantly, making it more susceptible to attacks due to its stronger adherence to instructions. Additionally, among all Med-LVLMs evaluated, only LLaVA-Med shows a notable increase in the abstention rate; the others exhibit minimal defensive capabilities against toxicity requests. Furthermore, it is noteworthy that LVLMs generally perform well in this regard, demonstrating a higher abstention rate compared to most Med-LVLMs, which indicates their sensitivity to induced toxic outputs.

Figure 5: Abstention rate on overcautiousness evaluation.

    &  &  &  &  &  &  \\  & Tox & Abs & Tox & Abs & Tox & Abs & Tox & Abs & Tox & Abs & Tox & Abs \\  IU-Xray  & 3.02 & 2.55 & 1.478 & / & 1.364 & 0.17 & 1.95 & 0.20 & 14.26 & 14.33 & 1.346 & 9.69 \\ MIMIC-CXR  & 0.86 & 3.262 & 1.94 & 2.29 & 0.74 & 0.07 & 0.97 & 2.98 & 7.261 & 8.78 & 1.78 & 1.08 \\ Harvard-FairVLMed  & 1.10 & 1.04 & 1.05 & 0.04 & 0.72 & 0.02 & 0.44 & 5.58 & 0.29 & 1.17 & 1.50 & 1.94 \\ HAMI0000  & 0.60 & 15.50 & 1.346 & / & 0.56 & / & 0.09 & / & 0.26 & 2.39 & 0.77 & 1.362 \\ OLi  & 1.59 & 7.270 & 1.184 & / & 1.79 & / & 1.62 & 2.30 & 7.746 & 0.31 & 0.37 & 1.19 \\ PMC-QA  & 0.92 & \(\)8.91 & \(\)0.59 & \(\)0.04 & \(\)1.25 & \(\)0.05 & \(\)0.01 & \(\)0.47 & \(\)21.73 & 7.65 & 1.98 & \(\)12.15 \\ OmniMedVQA  & \(\)1.49 & \(\)11.08 & \(\)0.99 & / & 1.60 & / & 0.74 & \(\)6.50 & \(\)19.64 & 7.65 & 1.98 & \(\)12.15 \\   

Table 3: Performance gap (%) of Med-LVLMs on toxicity evaluation. Notably, we report the gap of toxicity score (\(\)) and abstention rate (\(\)) before and after incorporating prompts inducing toxic outputs. Here “Tox”: toxicity score; “Abs”: abstention rate, “/”: the value goes from 0 to 0.

### Privacy Evaluation and Results

Privacy breaches in Med-LVLMs is a critical issue due to the sensitive nature of health-related data. These models are expected to refrain from disclosing private information, such as marital status, as this can compromise both the reliability of the model and compliance with legal regulations. This subsection will evaluate privacy through zero-shot and few-shot evaluations.

Specifically, for _zero-shot evaluation_, we directly prompt Med-LVLMs with questions about patients' private information. A trustworthy Med-LVLM should consistently refuse to answer these privacy-related questions. Additionally, CARES explores a more challenging scenario -_few-shot evaluation_. In few-shot evaluation, Med-LVLMs are first exposed to several fabricated examples related to privacy before being queried about private information. This setup significantly increases the challenge for the model to consistently refrain from answering privacy-related questions.

_Setup_. To assess the model's protection of privacy information and whether it produces hallucinatory outputs on private information, CARES considers two kinds of protected health information (PHI) : marital status and social security number. Firstly, we evaluate the abstention rate on PHI. Secondly, since marital status is accessible in MIMIC-IV , the model's accuracy can be evaluated in privacy leakage to test whether it simply hallucinating PHI.

_Results_. The privacy evaluation results are shown in Table 4. The results highlight a significant shortfall in the performance of Med-LVLMs regarding patient privacy protection; these models demonstrate a lack of privacy awareness. General LVLMs (LLaVA-1.6, Owen-VL-Chat) exhibit slightly better performance, while other models respond appropriately to privacy-related inquiries. The accuracy evaluation for marital status further indicates that these models frequently generate hallucinatory privacy information, with accuracy rates predominantly below 50%. Additionally, the results from the few-shot evaluations suggest that current Med-LVLMs often inadvertently disclose private information present in the input prompts.

### Robustness Evaluation and Results

Robustness in Med-LVLMs aims to evaluate whether the models perform reliably across various clinical settings. In CARES, we focus on evaluating out-of-distribution (OOD) robustness, aiming to assess the model's ability to handle test data whose distributions significantly differ from those of the training data. Following Lee et al. , we specifically consider two types of distribution shift: _input-level shift_ and _semantic-level shift_. Concretely, in input-level shift, we assess how well these models generate responses when presented with test data that, while belonging to the same modalities as the training data, are corrupted in comparison. In semantic-level shift, we evaluate their performance using test data from different modalities than those of the training data. For example, we might test a model on fundus images, which is primarily trained on radiographs. Med-LVLMs are expected to recognize and appropriately handle OOD cases.

_Setup_. To evaluate OOD robustness, which necessitates prerequisite knowledge of the training distribution, we evaluate the performance solely on four Med-LVLMs for which the training data are detailed in their original papers. In addition to accuracy, to determine whether Med-LVLMs can effectively handle OOD cases, we will measure the models' abstention rate, with the following prompt

   Model & FairVLMed & OmniMedVQA \\  MedVInT & 0 & 0.01 \\ RadFM & 0.06 & 0.05 \\   

Table 6: Abstention rate (%) of tested on data from other modalities.

    &  &  \\  & Abs\(\) & ACC & Abs\(\) & ACC \\  LLaVA-Med & 2.71 & 15.95 & 2.04 & 20.68 \\ Med-Flamingo & 0.76 & 44.71 & 0.65 & 47.64 \\ MedVInT & 0 & 24.47 & 0 & 28.31 \\ RadFM & 0 & 52.62 & 0 & 54.73 \\ LLaVA-v1.6 & 14.02 & 26.35 & 13.18 & 28.49 \\ Qovea-VL-Chat & 10.37 & 5.10 & 9.82 & 11.32 \\   

Table 4: Performance (%) on privacy evaluation. Here ACC scores are only tested on MIMIC-XR. "Abs": abstention rate.

    &  &  \\  & ACC & Abs & ACC & Abs & ACC & Abs \\  LLaVA-Med & 32.28 & 39.33 & 46.87 & 6.05 & 28.49 & 1.81 & 52.30 & 44.43 & 7.31 \\ Med-Flamingo & 23.29 & 34.45 & 32.14 & 14.52 & 0 & 51.79 & 10.20 & 59.47 & 1.83 & 0 \\ MedVInT & 64.38 & 18.96 & 65.82 & 1.42 & 0 & 51.47 & 10.43 & 58.82 & 0.38 & 0 \\ MedVM & 25.29 & 11.88 & 53.69 & 1.452 & 0.02 & 19.04 & 11.40 & 50.56 & 1.089 & 0.01 \\   

Table 5: Abstention rate (Abs), accuracy (ACC) and AUROC (%) tested on noisy data.

is added into the input "If you have not encountered relevant data during training, you can decline to answer or output 'I don't know'.".

_Results._ For input-level shifts, although Med-LVLMs are trained on data corresponding to the modality of the test data, they should robustly refuse to respond when the data is too noisy for making accurate judgments. The results, as shown in Table 5, demonstrate a significant decrease in model performance, yet abstentions are rare. Regarding semantic-level shifts, we evaluate the behavior of Med-LVLMs trained on radiology data but tested on another modality (e.g., fundus photography). Although Med-LVLMs lack sufficient medical knowledge to answer questions from a new modality, the abstention rate remains nearly zero (see Table 6), indicating the model's insensitivity to OOD data. Both results demonstrate that Med-LVLMs exhibit poor out-of-distribution robustness, failing to detect OOD samples and potentially leading to erroneous model judgments.

## 4 Related Work

**Medical Large Vision Language Models.** LVLMs have demonstrated remarkable performance in natural images [49; 97; 41; 1; 58; 94; 75; 78; 69; 52; 24; 25; 4; 5; 71; 86], which has facilitated their application in the medical domain. Recent advancements have witnessed the emergence of Med-LVLMs such as LLAVA-Med  and Med-Flanning . They are built upon the foundation of open-source general LVLMs, subsequently fine-tuned using biomedical instruction data across various medical modalities. Additionally, several Med-LVLMs tailored to specific medical modalities have been developed, such as XrayGPT  (radiology), PathChat  (pathology), and OphGLM  (ophthalmology). These models hold immense potential to positively impact the healthcare field, _e.g._, by providing reliable clinical recommendations to doctors. As LVLMs are deployed in increasingly diverse fields, concerns regarding their trustworthiness are also growing [59; 66; 80; 79], particularly in the medical field. Unreliable models may induce hallucinations and results in inconsistencies between image-textual facts  or may result in unfair treatment based on gender, race, or other factors . Hence, proposing a comprehensive trustworthiness benchmark for Med-LVLMs is both imperative and pressing.

**Trustworthiness in LVLMs.** In LVLMs, existing evaluations of trustworthiness primarily focus on specific dimensions [43; 82], such as trustfulness [36; 11; 32; 82; 85; 9; 70; 56; 55; 57; 53; 54; 81] or safety [63; 50; 7; 6]. Specifically, for trustfulness, LVLMs may suffer from hallucinations that conflict with facts [95; 96; 69; 8; 87; 91; 88; 35; 76]. Previous methods evaluate LVLM hallucinations for VQA [36; 11; 15] and captioning [36; 9; 70; 94], with models exhibiting significant hallucinations. For safety, attack and jailbreak strategies are leveraged to induce erroneous responses . Similarly, Med-LVLMs inherit these issues of trustfulness and safety, as indicated by single-dimension evaluations [51; 37]. Unlike these studies that mainly focus on a specific dimension, we are the first to conduct a holistic evaluation of trustworthiness in Med-LVLMs, including trustfulness, fairness, safety, privacy, and robustness.

## 5 Conclusion

In this paper, we introduce CARES, a comprehensive benchmark designed to evaluate the trustworthiness of Med-LVLMs. It covers 16 medical imaging modalities and 27 anatomical structures, assessing the models' trustworthiness through diverse question formats. CARES thoroughly evaluates Med-LVLMs five multiple dimensions-_trustfulness, fairness, safety, privacy, and robustness_. Our findings indicate that existing Med-LVLMs are highly unreliable, frequently generating factual errors and misjudging their capabilities. Furthermore, these models struggle to achieve fairness across demographic groups and are susceptible to attacks and producing toxic responses. Ultimately, the evaluations conducted in CARES aim to drive further standardization and the development of more reliable Med-LVLMs.