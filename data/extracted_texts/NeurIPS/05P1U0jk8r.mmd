# Exploiting Hidden Structures in Non-Convex Games

for Convergence to Nash Equilibrium

 Iosif Sakos

Engineering Systems and Design (SUTD)

iosif_sakos@mymail.sutd.edu.sg

&Emmanouil V. Vlatakis-Gkaragkounis

UC Berkeley

emvlatakis@cs.columbia.edu

&Panayotis Mertikopoulos

Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP

LIG 38000 Grenoble, France

panayotis.mertikopoulos@imag.fr &Georgios Piliouras

Engineering Systems and Design (SUTD)

georgios@sutd.edu.sg

Equal Contribution.

###### Abstract

A wide array of modern machine learning applications - from adversarial models to multi-agent reinforcement learning - can be formulated as non-cooperative games whose Nash equilibria represent the system's desired operational states. Despite having a highly non-convex loss landscape, many cases of interest possess a latent convex structure that could potentially be leveraged to yield convergence to an equilibrium. Driven by this observation, our paper proposes a flexible first-order method that successfully exploits such "hidden structures" and achieves convergence under minimal assumptions for the transformation connecting the players' control variables to the game's latent, convex-structured layer. The proposed method - which we call preconditioned hidden gradient descent (PHGD) - hinges on a judiciously chosen gradient preconditioning scheme related to natural gradient methods. Importantly, we make no separability assumptions for the game's hidden structure, and we provide explicit convergence rate guarantees for both deterministic and stochastic environments.

## 1 Introduction

Many powerful AI architectures are based on the idea of combining conceptually straightforward settings coming from game theory with the expressive power of neural nets. Some prominent examples of this type include generative adversarial networks (GANs) , robust reinforcement learning , adversarial training , multi-agent reinforcement learning in games , and even multi-player games that include free-form natural-language communication . Intuitively, in all these cases, the game-theoretic abstraction serves to provide a palpable, easy-to-understand target, i.e., an equilibrium solution with strong axiomatic justification. However, from a complexity-theoretic standpoint, such targets are excessively ambitious, requiring huge amounts of data to express and compute, even approximately. Because of this, the agents' policies must be encoded via a universal function approximator (such as a neural net) and training this architecture boils down to iteratively updating these parameters until the process - hopefully! - converges to the target equilibrium.

Unfortunately, despite the ubiquitousness of these settings, the design of algorithms with provable convergence guarantees is still relatively lacking. This deficit is not surprising if one considers that even the - comparatively much simpler - problem of equilibrium learning in finite games is hindered by numerous computational hardness  as well as dynamic impossibility results. In this regard, our best hope for designing provably convergent algorithms is to focus on specific classes of games with some _useful structure_ to exploit.

One of the most well-established frameworks of this type is the class of _monotone games_ whose study goes back at least to Rosen . As special cases, this setting includes single-agent convex minimization problems, two-player convex-concave min-max games, diagonally convex \(N\)-player games, etc. Owing to this connection, there has been a proliferation of strong positive results at the interface of game theory and optimization, see e.g.,  and references therein. In our case however, the agents do not play this monotone game directly, but can only access it _indirectly_ via an encoding layer of _control variables_ - like the weight parameters of a neural net that outputs a feasible strategy profile for the game in question. In this sense, from a machine learning perspective, the strategies of the game are _latent variables_, so we can think of each player as being equipped with a smooth mapping from a high-dimensional space of control variables to the strategy space of the game. Importantly, in contrast to the control variables, the latent variables are not directly accessible to the players themselves and should only be viewed as auxiliary variables - to all extents and purposes, the goal remains to find an operationally desirable control layer configuration. In this sense, the convex structure of the game becomes "hidden" behind the control layer, which entangles multiple input/control variables into nonlinear manifolds of latent variables.

As a result, the convex structure of the underlying game is effectively destroyed, resulting in highly non-convex end-to-end interactions. This raises the following central challenge:

_Can we design provably convergent algorithms for non-convex games with a hidden structure in the presence of general couplings between control and latent variables?_

Prior work in the area has shown that this is a promising and, at the same time, highly challenging question. In , the setting of hidden bilinear games was introduced and a number of negative results were presented, to the effect that gradient-descent-ascent can exhibit a variety of non-convergent behaviors, even when the game admits a hidden _bilinear_ structure. Subsequently,  provided an approximate minimax theorem for a class of two-agent games where the players pick neural networks, but did not provide any convergent training algorithm for this class of games. Instead, the first positive result on the dynamics of hidden games was obtained by  who established a series of non-local convergence guarantees to the von Neumann max-min solution of the game in the case of two-agent hidden strictly convex-concave games for all initial conditions satisfying a certain genericity assumption. This approach, however, only applied to _continuous-time dynamics - not algorithms_ - and it further imposed strong separability assumptions on the representation of the game in the control layer. More recently,  established the first global convergence guarantees in hidden games but, once again, these apply only to _continuous-time dynamics_ and a special case of two-agent convex-concave games (akin to playing a convex combination of hidden games with one-dimensional latent spaces). This paper is the closest antecedent to our work as it introduces a dynamical system, called Generalized Natural Gradient Flow, which makes the \(L^{2}\) norm between the equilibrium in the hidden game and the current set of latent variables a Lyapunov function for the system.

Our results & techniques.Our paper seeks to provide an affirmative answer to the key challenge above under minimal assumptions on the coupling between latent and control variables. To that effect, we only assume that each agent is able to affect a measurable change along any latent variable by updating their control variables appropriately; without this assumption spurious equilibria can emerge due to the deficiency of the control layer architecture. Importantly however, even though the map from control to latent variables is known to the players, _we do not assume_ that it can be efficiently inverted (e.g., to solve for a profile of control variables that realizes a profile of latent variables). Otherwise, if it could, the entire game could be solved directly in the latent layer and then ported back to the control layer, thus rendering the whole problem moot - and, indeed, when working with realistic neural net architectures, this inversion problem is, to all intents and purposes, impossible.

For intuition, we begin by designing a new continuous-time flow, that we call preconditioned hidden gradient dynamics, and which enjoys strong convergence properties in games with a hidden strictly monotone structure (Proposition 1). Similarly to , this is achieved by using the \(L^{2}\) norm in the latent layer as a Lyapunov function in the control layer; however, the similarities with the existing literature end there. Our paper does not make any separability or low-dimensionality assumptions, and is otherwise _purely algorithmic:_ specifically, building on the continuous-time intuition, we provide a concrete, implementable algorithm, that we call _preconditioned hidden gradient descent_ (PHGD);this algorithm is run with _stochastic gradients_, and enjoys a series of strong, global convergence guarantees in hidden games.

First, as a baseline, Theorem 1 shows that a certain averaged process achieves an \((1/)\) convergence rate in all games with a hidden monotone structure and Lipschitz continuous loss functions. If the hidden structure is strongly monotone, Theorem 2 further shows that this rate can be improved to \((1/t)\) for the _actual_ trajectory of the players' control variables; and if the algorithm is run with full, deterministic gradients, the rate becomes _geometric_ (Theorem 3). To the best of our knowledge, these are the first bona fide algorithmic convergence guarantees for games with a hidden structure.

## 2 Problem setup and preliminaries

Throughout the sequel, we will focus on continuous \(N\)-player games where each player, indexed by \(i\{1,,N\}\), has a convex set of _control variables_\(_{i}_{i}^{m_{i}}\), and a continuously differentiable _loss function_\(_{i}\), where \(_{i}_{i}\) denotes the game's _control space_. For concreteness, we will refer to the tuple \((,,)\) as the _base game_.

The most relevant solution concept in this setting is that of a _Nash equilibrium_, i.e., an action profile \(^{*}\) that discourages unilateral deviations. Formally, we say that \(^{*}\) is a _Nash equilibrium_ of the base game \(\) if

\[_{i}(^{*})_{i}(_{i};_{-i}^{*})_{i}$, $i$}\] (NE)

where we employ the standard game-theoretic shorthand \((_{i};_{-i})\) to distinguish between the action of the \(i\)-th player and that of all other players in the game. Unfortunately, designing a learning algorithm that provably outputs a Nash equilibrium is a very elusive task: the impossibility results of Hart & Mas-Colell  already preclude the existence of uncoupled dynamics that converge to a Nash equilibrium in all games; more recently, Milionis et al.  established a similar impossibility result even for possibly _coupled_ dynamics (in both discrete and continuous time), while Daskalakis et al.  has shown that even the _computation_ of an approximate equilibrium can be beyond reach.

In view of this, our work focuses on games with a hidden, _latent_ structure that can be exploited to compute its Nash equilibria. More precisely, inspired by  we have the following definition.

**Definition 1**.: We say that the game \((,,)\) admits a _latent_ - or _hidden_ - _structure_ if:

1. Each player's control variables can be mapped faithfully to a closed convex set of _latent variables_\(x_{i}_{i}^{d_{i}}\); formally, we posit that there exists a Lipschitz smooth map \(_{i}_{i}_{i}\) with no critical points and such that \((_{i}(_{i}))=_{i}\).
2. Each player's loss function factors through the game's _latent space_\(_{i}_{i}\) as \[_{i}()=f_{i}(_{1}(_{1}),,_{N}(_{N}))\] (1) for some Lipschitz smooth function \(f_{i}\) called the player's _latent loss function_.

For concreteness, we will refer to the product map \(()=(_{i}(_{i}))_{i}\) as the game's _representation map_, and the tuple \((,,f)\) will be called the _hidden_ /_latent game_. To simplify notation later on, we also assume that \(_{i}\) has nonempty topological interior, so, in particular, \((_{i})=d_{i} m_{i}\).

We illustrate the above notions in two simple - but not simplistic - examples below; for a schematic representation, cf. Fig. 2.

Figure 1: A hidden game of Rock-Paper-Scissors with strategies encoded by two multi-layer perceptrons (MLPs), whose \(4\)-dimensional input is dictated by the two players (cf. ). The nonlinearity of the MLP representation maps leads to a highly non-convex non-concave zero-sum game. However, by employing PHGD, both players’ MLP control variables accurately identify the \((1/3,1/3,1/3)\) equilibrium in the game’s latent space.

**Example 2.1**.: Consider a single-agent game (\(N=1\)) where the objective is to minimize the non-convex function \(()=_{=1}^{||}((_{})-s _{})^{2}\) over a dataset \(=\{s_{}\}\). This problem can be recast as a hidden convex problem with \(f(x)=_{=1}^{||}(x_{}-s_{})^{2}\) and \(()=()\).

**Example 2.2**.: A more complex scenario involves non-convex / non-concave min-max optimization problems of the form \(_{_{1}}_{_{2}}_{1}(_{1})^{}C_{2}( _{2})\) where \(_{1}\) and \(_{2}\) are preconfigured multi-layer perceptrons (MLPs) constructed with smooth activation functions (such as CeLUs). This problem can be reformulated as a hidden bilinear problem by setting \(f_{1}(x_{1},x_{2})=x_{1}^{}Cx_{2}=-f_{2}(x_{1},x_{2})\).

Before moving forward, there are some points worth noting regarding the above definitions.

_Remark 1_.: First, the requirement \((())=\) means that all latent variable profiles \(x\) can be approximated to arbitrary accuracy in the game's control space. The reason that we do not make the stronger assumption \(()=\) is to capture cases like the sigmoid map: if \(()=[1+(-)]^{-1}\) for \(\), the image of \(\) is the interval \((0,1)\), which is convex but not closed.

_Remark 2_.: Second, the requirement that \(\) has no critical points simply means that, for any control variable configuration \(\), the Jacobian \((())\) of \(\) at \(\) has full rank. By the implicit function theorem , this simply means that \(\) locally looks like a projection (in suitable coordinates around \(\)), so it is possible to affect a measurable change along any feasible latent direction by updating each player's control variables appropriately. This is no longer true if \(\) has critical points, so this is a minimal requirement to ensure that no spurious equilibria appear in the game's latent space.

To proceed, we will assume that the latent game is _diagonally convex_ in the sense of Rosen , a condition more commonly known in the optimization literature as _monotonicity_. Formally, let

\[g_{i}(x)=_{i}f_{i}(x)_{x_{i}}f_{i}(x)\] (2)

denote the individual gradient of the latent loss function of player \(i\), and write \(g(x)=(g_{1}(x),,g_{N}(x))\) for the profile thereof. We then say that the latent game \(\) is:

* _Monotone_ if \( g(x^{})-g(x),\,x^{}-x 0\) for all \(x,x^{}\). (3a)
* _Strictly monotone_ if (3a) holds as a strict inequality whenever \(x^{} x\). (3b)
* _Strongly monotone_ if \( g(x^{})-g(x),\,x^{}-x\|x^{}-x\|^{2}\) for some \(>0\) and all \(x,x^{}\). (3c)

Clearly, strong monotonicity implies strict monotonicity, which in turn implies monotonicity; on the other hand, when we want to distinguish between problems that are monotone but not strictly monotone, we will say that \(g\) is _merely monotone_. Finally, extending the above to the base game, we will say that \(\) admits a _hidden monotone structure_ when the latent game \(\) is monotone as above (and likewise for strictly / strongly monotone structures).

Examples of monotone games include neural Kelly auctions and Cournot oligopolies , covariance matrix optimization problems and power control , certain classes of congestion games , etc. In particular, in the case of convex minimization problems, monotonicity (resp. strict / strong monotonicity) corresponds to convexity (resp. strict / strong convexity) of the problem's objective function. In all cases, it is straightforward to check that the image \(x^{*}=(^{*})\) of a Nash equilibrium \(^{*}\) of the base game satisfies a Stampacchia variational inequality of the form

\[ g(x^{*}),\,x-x^{*} 0x.\] (SVI)

In turn, by monotonicity, this characterization is equivalent to the Minty variational inequality

\[ g(x),\,x-x^{*} 0x.\] (MVI)

Figure 2: Schematic representation of a game with a hidden / latent structure.

To avoid trivialities, we will assume throughout that the solution set \(^{*}\) of (SVI)/(MVI) is nonempty. This is a standard assumption without which the problem is not well-posed - and hence, meaningless from an algorithmic perspective.

Notation.To streamline notation (and unless explicitly mentioned otherwise), we will denote control variables by \(_{i}\) and \(\), and we will write \(x_{i}=_{i}(_{i})\) and \(x=()\) for the induced latent variables. We will also write \(m_{i}m_{i}\) for the dimensionality of the game's control space \(\) and \(d_{i}d_{i}\) for the dimensionality of the latent space \(\) (so, in general, \(m d\)). Finally, when the representation map \(\) is clear from the context, we will write \(_{i}(_{i})(_{i}(_{i})) ^{d_{i} m_{i}}\) for the Jacobian matrix of \(_{i}\) at \(_{i}_{i}\), and \(()=_{i}_{i}(_{i})\) for the associated block diagonal sum.

## 3 Hidden gradients and preconditioning

We are now in a position to present our main algorithmic scheme for equilibrium learning in games with a hidden monotone structure. In this regard, our aim will be to overcome the following limitations in the existing literature on learning in hidden games: (_i_) the literature so far has focused exclusively on continuous-time dynamics, with no discrete-time algorithms proven to efficiently converge to a solution; (_ii_) the number of players is typically limited to two; and (_iii_) the representation maps are _separable_ in the sense that the control variables are partitioned into subsets and each subset affects exactly one latent variable.1

We start by addressing the last two challenges first. Specifically, we begin by introducing a gradient preconditioning scheme that allows us to design a convergent _continuous-time_ dynamical system for hidden monotone games with an _arbitrary_ number of players and _no separability_ restrictions for its representation maps. Subsequently, we propose a bona fide algorithmic scheme - which we call _preconditioned hidden gradient descent_ (PHGD) - by discretizing the said dynamics, and we analyze the algorithm's long-run behavior in Section 4.

### Continuous-time dynamics for hidden games.

To connect our approach with previous works in the literature, our starting point will be a simple setting already captured within the model of , min-max games with a hidden convex-concave structure and unconstrained one-dimensional control and latent spaces per player, i.e., \(_{i}==_{i}\) for \(i=1,2\). We should of course note that this specific setting is fairly restrictive and not commonly found in deep neural network practice: in real-world deep learning applications, neural nets generally comprise multiple interconnected layers with distinct activation functions, so the input-output relations are markedly more complex and intertwined. Nevertheless, the setting's simplicity makes the connection with natural gradient methods particularly clear, so as in , it will serve as an excellent starting point.

Concretely, Mladenovic et al.  analyze the _natural hidden gradient dynamics_

\[_{i}=-^{}(_{i})|^{2}}}{_{i}}i.\] (NHGD)

The reason for this terminology - and the driving force behind the definition of (NHGD) - is the observation that, in one-dimensional settings, a direct application of the chain rule to the defining relation \(_{i}=f_{i}\) of the game's latent loss functions yields \(_{i}/_{i}=_{i}^{}(_{i}) f _{i}/ x_{i}\) so, in turn, (NHGD) becomes

\[_{i}=-^{}(_{i})}} { x_{i}}\] (4)

where, for simplicity, we are tacitly assuming that \(_{i}(_{i})^{}>0\).

This expression brings two important points to light. First, even though (NHGD) is defined in terms of the actual, control-layer gradients \(_{i}/_{i}\) of \(\), Eq. (4) shows that the dynamics are actually driven by the latent-layer, "hidden gradients" \(g_{i}(x)= f_{i}/ x_{i}\) of \(\). Second, the preconditioner \(|_{i}^{}(_{i})|^{-2}\) in (NHGD) can be seen as a Riemannian metric on \(\): instead of defining gradients relative to the standard Euclidean metric of \(^{N}\), (NHGD) can be seen as a gradient flow relative to the Riemannian metric \(g_{ij}()=_{ij}^{}_{i}(_{i})\), which captures the "natural" geometry induced by the representation map \(\).2

From an operational standpoint, the key property of (NHGD) that enabled the analysis of  is the observation that the \(L^{2}\) energy function

\[E()=\|()-x^{*}\|^{2}\] (5)

between the latent representation \(x=()\) of \(\) and a solution \(x^{*}\) of (SVI) / (MVI) is a Lyapunov function for (NHGD). Indeed, a straightforward differentiation yields

\[()=\|x-x^{*}\|^{2}=_{i}_{i}(x_{i}-x_{i}^{*})=-[g(())]^{}(()-x^{*}) 0\] (6)

with the penultimate step following from (4) and the last one from (MVI). It is then immediate to see that the latent orbits \(x(t)=((t))\) of (NHGD) converge to equilibrium in strictly monotone games.

The approach in the example above depends crucially on the separability assumption which, among others, trivializes the problem. Indeed, if there is no coupling between control variables in the game's latent space, the equations \(x_{i}=_{i}(_{i})\) can be backsolved easily for \(\) (e.g., via binary search), so it is possible to move back-and-forth between the latent and control layers, ultimately solving the game in the latent layer and subsequently extracting a solution configuration in the control layer. Unfortunately however, extending the construction of (NHGD) to a non-separable setting is not clear, so it is likewise unclear how to exploit the hidden structure of the game beyond the separable case.

To that end, our point of departure is the observation that the Lyapunov property (6) of the energy function \(E()\) is precisely the key feature that enables convergence of (NHGD). Thus, assuming that control variables are mapped to latent ones via a general - but possibly highly coupled - representation map \(\), we will consider an abstract preconditioning scheme of the form

\[_{i}=-_{i}(_{i})v_{i}()\] (7)

where

\[v_{i}()_{_{i}}_{i}()\] (8)

denotes the individual loss gradient of player \(i\), while the preconditioning matrix \(_{i}(_{i})^{m_{i} m_{i}}\) is to be designed so that (6) still holds under (7). In this regard, a straightforward calculation (which we prove in Appendix A) yields the following:

**Lemma 1**.: _Under the dynamics (7), we have_

\[()=-_{i}[g_{i}(())]^{ }_{i}(_{i})_{i}(_{i})[_{i} (_{i})]^{}(_{i}(_{i})-x_{i}^{*})\] (9)

_where, as per Section 2, \(_{i}(_{i})^{d_{i} m_{i}}\) denotes the Jacobian matrix of the map \(_{i}_{i}_{i}\). More compactly, letting \(_{i}_{i}\) denote the block-diagonal ensemble of the players' individual preconditioning matrices \(_{i}\) (and suppressing control variable arguments for concision), we have_

\[=-g(x)^{}^{}( x-x^{*})\] (10)

In view of Lemma 1, a direct way to achieve the target Lyapunov property (6) would be to find a preconditioning matrix \(\) such that \(^{}=\). However, since \(\) is surjective (by the faithfulness assumption for \(\)), the Moore-Penrose inverse \(^{+}\) of \(\) will be a right inverse to \(\), i.e., \(^{+}=\). Hence, letting \(=(^{})^{+}=^{+}[^{ +}]^{}\), we obtain:

\[^{}=(^{} )^{+}^{}=^{+}(^{ })^{+}^{}==\] (11)

In this way, unraveling the above, we obtain the _preconditioned hidden gradient flow_

\[_{i}=-_{i}(_{i})_{i}_{i}(_{i}) _{i}(_{i})=[_{i}(_{i})^{ }_{i}(_{i})]^{+}\] (PHGF)

By virtue of design, the following property of (PHGF) is then immediate:

**Proposition 1**.: _Suppose that \(\) admits a latent strictly monotone structure in the sense of (3b). Then the energy function (5) is a strict Lyapunov function for (PHGF), and every limit point \(^{*}\) of \((t)\) is a Nash equilibrium of \(\)._Proposition 1 validates our design choices for the preconditioning matrix \(\) as it illustrates that the dynamics (PHGF) converge to Nash equilibrium, without any separability requirements or dimensionality restrictions; in this regard, Proposition 1 already provides a marked improvement over the corresponding convergence result of  for (NHGD). To streamline our presentation, we defer the proof of Lemma 1 and Proposition 1 to Appendix A, and instead proceed directly to provide a bona fide, algorithmic implementation of (PHGF).

### Preconditioned hidden gradient descent.

In realistic machine learning problems, any algorithmic scheme based on (PHGF) will have to be run in an iterative, discrete-time environment; moreover, due to the challenges posed by applications with large datasets and optimization objectives driven by the minimization of empirical risk, we will need to assume that players only have access to a stochastic version of their full, deterministic gradients. As such, we will make the following blanket assumptions that are standard in the stochastic optimization literature :

**Assumption 1**.: Each agent's loss function is an expectation of a random function \(L_{i}\) over a complete probability space \((,,)\), i.e., \(_{i}()=[L_{i}(;)]\). We further assume that:

1. \(L_{i}(;)\) is measurable in \(\) and \(\)-Lipschitz smooth in \(\) (for all \(\) and \(\) respectively).
2. The gradients of \(L_{i}\) have bounded second moments, i.e., \(_{}[\| L_{i}(;)\|^{2}] M^{2}\).

Taken together, the two components of Assumption 1 imply that each \(_{i}\) is differentiable and Lipschitz continuous (indeed, by Jensen's inequality, we have \(\|[ L_{i}(;)]\|^{2}[\| L_{i}( ;)\|^{2}] M^{2}\)). Moreover, by standard dominated convergence arguments, \( L_{i}(;)\) can be seen as an unbiased estimator of \(_{i}()\), that is, \(_{i}()=[ L_{i}(;)]\) for all \(\). In view of this, we will refer to the individual loss gradients \(_{i}L_{i}(;)\) of player \(i\) as the player's individual _stochastic gradients_.

With all this in mind, the _preconditioned hidden gradient descent_ algorithm is defined as the stochastic first-order recursion

\[_{i,t+1}=_{i,t}-_{t}_{i,t}V_{i,t}\] (PHGD)

where:

1. \(_{i,t}\) denotes the control variable configuration of player \(i\) at each stage \(t=1,2,\)
2. \(_{t}>0\) is a variable step-size sequence, typically of the form \(_{t} 1/t^{p}\) for some \(p\).
3. \(_{i,t}_{i}(_{i,t})=[_{i}(_ {i,t})^{}_{i}(_{i,t})]^{+}\) is the preconditioner of player \(i\) at the \(t\)-th epoch.
4. \(V_{i,t}_{i}L(_{i,t};_{t})\) is an individual stochastic loss gradient generated at the control variable configuration \(_{t}\) by an i.i.d. sample sequence \(_{t}\), \(t=1,2,\)

The basic recursion (PHGD) can be seen as a noisy Euler discretization of the continuous flow (PHGF), in the same way that ordinary stochastic gradient descent can be seen as a noisy discretization of gradient flows. In this way, (PHGD) is subject to the same difficulties underlying the analysis of stochastic gradient descent methods; we address these challenges in the next section.

## 4 Convergence analysis and results

In this section, we present our main results regarding the convergence of (PHGD) in hidden monotone games. Because we are interested not only on the asymptotic convergence of the method but also on its _rate_ of convergence, we begin this section by defining a suitable merit function for each class of hidden monotone games under consideration; subsequently, we present our main results in

Figure 3: Exploiting a hidden convex structure in the control layer. On the left, we present the dynamics (PHGF) in the example of minimizing the simple function \(f()=((_{1},_{2})\|(1/2,1/3,1/6))\) over \(^{2}\). In the subfigure to the right, we illustrate the hidden convex structure of the energy landscape, from the non-convex sublevel sets of \(\) to the latent space \(\{(x_{1},x_{2})_{ 0}^{2}:x_{1}+x_{2} 1\}\).

Section 4.2, where we also discuss the main technical tools enabling our analysis. To streamline our presentation, all proofs are deferred to Appendix B.

### Merit functions and convergence metrics

In the variational context of (SVI) / (MVI), the quality of a candidate solution \(\) is typically evaluated by means of the _restricted merit function_

\[_{}()_{x}  g(x),\,-x\] (12)

where the "test domain" \(\) is a relatively open subset of \(\). The _raison d'etre_ of this definition is that, if \(g\) is monotone and \(x^{*}\) is a solution of (SVI) /(MVI), we have

\[ g(x),\,x^{*}-x g(x^{*}),\,x^{*}-x 0x ,\] (13)

so the supremum in (B.6) cannot be too positive if \(\) is an approximate solution of (SVI) / (MVI). This is encoded in the following lemma, which, among others, justifies the terminology "merit function":

**Lemma 2**.: _Suppose that \(g\) is monotone. If \(\) is a solution of (SVI)/(MVI), we have \(_{}()=0\) whenever \(\). Conversely, if \(_{}()=0\) and \(\) is a neighborhood of \(\) in \(\), then \(\) is a solution of (SVI)/(MVI)._

Lemma 2 extends similar statements by Auslender & Teboulle  and ; the precise variant that we state above can be found in , but, for completeness, we provide a proof in Appendix B.

Now, since a latent variable profile \(x^{*}=(^{*})\) solves (SVI) if and only if the control variable configuration \(^{*}\) is Nash equilibrium of the base game, the quality of a candidate solution \(\) with \(=()\) can be assessed by the induced gap function

\[()_{}( ())=_{x} g(x),\,-x.\] (14)

Indeed, since \(=()\), Lemma 2 shows that \(() 0\) with equality if and only if \(\) is a Nash equilibrium of the base game \(\). In this regard, Eq. (14) provides a valid equilibrium convergence metric for \(\), so we will use it freely in the sequel as such; for a discussion of alternative convergence metrics, we refer the reader to Appendix B.

### Convergence results

We are now in a position to state our main results regarding the equilibrium convergence properties of (PHGD). To streamline our presentation, we present our results from coarser to finer, starting with games that admit a hidden _merely_ monotone structure and _stochastic_ gradient feedback, and refining our analysis progressively to games with a hidden _strongly_ monotone structure and _full_ gradient feedback.3 In addition, to quantify this distortion between the game's latent and control layers, we will require a technical regularity assumption for the game's representation map \(\).

**Assumption 2**.: The singular values of the Jacobian \(()\) of the representation map \(\) are bounded as

\[_{}^{2}(()()^ {})_{}^{2}\] (15)

for some \(_{},_{}(0,)\) and for all \(\).

With all this in hand, we begin by studying the behavior of (PHGD) in games with a hidden monotone structure.

**Theorem 1** (PHGD in hidden monotone games).: _Suppose that players run (PHGD) in a hidden monotone game with learning rate \(_{t} 1/t^{1/2}\). Then, under Assumptions 1 and 2, the averaged process \(_{t}^{-1}t^{-1}_{s=1}^{t}x_{s}\) enjoys the equilibrium convergence rate_

\[[(_{t})]=( t/).\] (16)

As far as we are aware, Theorem 1 is the first result of its kind in the hidden games literature - that is, describing the long-run behavior of a discrete-time algorithm with stochastic gradient input. At the same time, it is subject to two important limitations: the first is that the averaged state \(_{t}\) cannot be efficiently computed for general representation maps; second, even if it could, the \(( t/)\)convergence rate is relatively slow. The two results that follow show that both limitations can be overcome in games with a hidden _strongly_ monotone structure.

In this case (SVI)/(MVI) admits a (necessarily) unique solution \(x^{*}\) in the game's control space, so we will measure convergence in terms of the latent equilibrium distance

\[()\|()-x^{*}\|^{ 2}.\] (17)

With this in mind, we have the following convergence results:

**Theorem 2** (PHGD in hidden strongly monotone games).: _Suppose that players run (PHGD) in a hidden \(\)-strongly monotone game with \(_{t}=/t\) for some \(>\). Then, under Assumptions 1 and 2, the induced sequence of play \(_{t}\), \(t=1,2,\), enjoys the equilibrium convergence rate_

\[[_{}(_{t})]=(1/t).\] (18)

This rate is tight, even for standard strongly monotone games. To improve it further, we will need to assume that (PHGD) is run with full, _deterministic_ gradients, i.e., \(V_{t}=g(_{t})\). In this case, we obtain the following refinement of Theorem 2.

**Theorem 3** (PGD with full gradient feedback in hidden strongly monotone games).: _Suppose that players run (PHGD) in a hidden \(\)-strongly monotone game with full gradient feedback, and a sufficiently small learning rate \(>0\). Then, under Assumptions 1 and 2, the induced sequence of play \(_{t}\), \(t=1,2,\), converges to equilibrium at a geometric rate, i.e.,_

\[_{}(_{t})=(^{t})\] (19)

_for some constant \((0,1)\) that depends only on the primitives of \(\) and the representation map \(\)._

Importantly, up to logarithmic factors, the convergence rates of Theorems 1-3 mirror the corresponding rates for learning in monotone games. This take-away is particularly important as it shows that, _when it exists, a hidden convex structure can be exploited to the greatest possible degree, without any loss of speed in convergence relative to standard, non-hidden convex problems._

The proofs of Theorems 1-3 are quite involved, so we defer the details to Appendix B. That said, to give an idea of the technical steps involved, we provide below (without proof) two lemmas that play a pivotal role in our analysis. The first one hinges on a transformation of the problem's defining vector field \(v()=(_{i}_{i}())_{i}\) which, coupled with the specific choice of preconditioner \(_{i}(_{i})=[_{i}(_{i})^{}_ {i}(_{i})]^{+}\) in (PHGD) allows us to effectively couple the latent and control layers of the problem in a "covariant" manner:

**Lemma 3**.: _Fix some \(\), and consider the energy function \(E(;)=(1/2)\|()-\|^{2}\). Then, for all \(\), we have_

\[()()_{}E(;)=( )-.\] (20)

Our second intermediate result builds on Lemma 3 and provides a "template inequality" for the energy function \(E(;)\) in the spirit of [7; 11; 18].

**Lemma 4** (Template inequality).: _Suppose that Assumptions 1 and 2 hold. Then, with notation as in Lemma 3, the sequence \(E_{t}(1/2)\|(_{t})-\|^{2}\), \(t=1,2,\), satisfies_

\[E_{t+1} E_{t}-_{t}g(x_{t})^{}(x_{t}-)+_{t} _{t}+_{t}^{2}_{t},\] (21)

_where \(x_{t}(_{t})\), \(_{t}((_{t})(_{t})V_{t}-g(x_{t})) ^{}(x_{t}-)\) and \(_{t}\) is a random error sequence with \(_{t}[_{t}]<\)._

This inequality plays a pivotal role in our analysis because it allows us to couple the restricted merit function (B.8) in the game's control space with the evolution of the algorithm's quasi-Lyapunov function in the game's latent space. We provide the relevant details and calculations in Appendix B.

## 5 Experiments

This section demonstrates our method's applicability in a couple of different and insightful setups. Technical details of those setups, as well as additional experimental results are deferred to the supplementary material. We start with a regularized version of Matching Pennies zero-sum game where the players' strategies are controlled by two individual preconfigured differentiable MLPs. Each MLP acts as the player's representation map \(_{i}\), which for each input \(_{i}\) outputs a uni-dimensional latent variable \(x_{i}=_{i}(_{i})\) guaranteed to lie in \(_{i}\); the player's latent space.

Figure 4 illustrates the trajectory of (PHGD), represented by the black curve. The algorithm employs a constant step-size of \(0.01\) and is initialized at the arbitrary state \((1.25,2.25)\) in the control variables' space. The color map in the figure serves as a visual representation of the level sets associated with the proposed energy function (5). Notably, the trajectory of the algorithm intersects each of the energy function's level sets at most once, indicating its non-cycling behavior, which is an issue that shows up often in the equilibration task. Due to the design of our hidden maps, the stabilization at the point \((0,0)\) corresponds to the \((_{1}(0),_{2}(0))=(,)\) the unique equilibrium of the game.

In the second, more complex, example, we consider a strongly-monotone regularized modification of an (atomic) El Farol Bar congestion game among \(N=30\) players . In this setup, we let the control space of each player \(i\) be multi-dimensional, namely, \(_{i}^{5}\), \(i\), and, as in the previous example, the representation map of each player is instantiated by some preconfigured differentiable MLP whose output \(x_{i}_{i}(_{i})\) is guaranteed to lie in \(\). The MLP's output is going to be the probability with which player \(i\) visits the El Farol bar. For the interested reader, further details, including technical specification of the MLPs, and loss functions of the games, can be found in the supplementary material.

Figure 5 provides a comparative analysis of the performance between (PHGD), and the standard GD method in the aforementioned two game scenarios. In Figure 5 (left) we explore the Matching Pennies game, where GD exhibits slightly erratic behavior. Despite eventually converging to the game's equilibrium point, GD's convergence rate, in this case, can be described as linear at best. In Figure 5 (right), we examine the El Farol Bar game. Interestingly, in this highly complex setup, GD fails to reach the equilibrium point entirely. In contrast, (PHGD) not only manages to converge in both of these setups, but it also consistently maintains an exponential rate of convergence. This stark difference underscores the efficacy and robustness of our algorithm.

## 6 Conclusion

This paper proposed a new algorithmic framework with strong formal convergence guarantees in a general class non-convex games with a latent monotone structure. Our algorithmic method - which we call preconditioned hidden gradient descent - relies on an appropriately chosen gradient preconditioning scheme akin to natural gradient ideas. Our class of games combines the useful structure of monotone operators as well as the notion of latent/hidden variables that arise in neural networks and can thus model numerous AI applications. Our results indicate the possibility of deep novel algorithmic ideas emerging at the intersection of game theory, non-convex optimization and ML and offers exciting directions for future work.

Figure 4: The trajectory of (PHGD) in a hidden Matching Pennies game over the sublevel sets of the energy function in (5)

Figure 5: The evolution of the \(L^{2}\) error function \(_{}(())\) of (PHGD) and gradient descent (GD) with constant step-size \(0.01\) in the regularized games of Matching Pennies and El Farol Bar as depicted in a semi-logarithmic scale. In the Matching Pennies game (left) we depict a single trajectory initialized at \((1.25,2.25)\), while in the El Farol Bar game (right) game we depict the mean and confidence bounds of \(100\) random trajectories.