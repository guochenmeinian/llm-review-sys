# Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithms

Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithms

 Miao Lu1  Han Zhong2\({}^{*}\) Tong Zhang\({}^{3}\) Jose Blanchet\({}^{1}\)

\({}^{1}\)Department of Management Science and Engineering, Stanford University

\({}^{2}\)Center for Data Science, Peking University

\({}^{3}\)Department of Computer Science, University of Illinois Urbana-Champaign

Equal contribution. Email to miaolu@stanford.edu, hanzhong@stu.pku.edu.cn

###### Abstract

Distributionally robust reinforcement learning (DRRL), often framed as a robust Markov decision process (RMDP), seeks to find a robust policy that achieves good performance under the worst-case scenario among all environments within a pre-specified uncertainty set centered around the training environment. Unlike previous work, which relies on a generative model or a pre-collected offline dataset enjoying good coverage of the deployment environment, we tackle robust RL via interactive data collection, where the learner interacts with the training environment only and refines the policy through trial and error. In this robust RL paradigm, two main challenges emerge: managing the distributional robustness while striking a balance between exploration and exploitation during data collection. Initially, we establish that sample-efficient learning without additional assumptions is unattainable owing to the curse of support shift; i.e., the potential disjointedness of the distributional supports between training and testing environments. To circumvent such a hardness result, we introduce the vanishing minimal value assumption to RMDPs with a total-variation distance robust set, postulating that the minimal value of the optimal robust value function is zero. Such an assumption effectively eliminates the support shift issue for RMDPs with a TV distance robust set, and we present an algorithm with a provable sample complexity guarantee. Our work makes the initial step to uncovering the inherent difficulty of robust RL via interactive data collection and sufficient conditions for sample-efficient algorithms with sharp sample complexity.

## 1 Introduction

Reinforcement learning (RL) serves as a framework for addressing complex decision-making problems through iterative interactions with environments. The advancements in deep reinforcement learning have enabled the successful application of the general RL framework across various domains, including mastering strategic games, such as Go (Silver et al., 2017), robotics (Kober et al., 2013), and tuning large language models (LLMs; Ouyang et al. (2022)). The critical factors contributing to these successes encompass not only the potency of deep neural networks and modern deep RL algorithms but also the availability of substantial training data. However, there are scenarios, such as healthcare (Wang et al., 2018) and autonomous driving (Kiran et al., 2021), among others, where collecting data in the target domain is challenging, costly, or even unfeasible.

In such cases, the _sim-to-real transfer_(Kober et al., 2013; Sadeghi and Levine, 2016; Peng et al., 2018; Zhao et al., 2020) becomes a remedy - a process in which the RL agents are trained in some simulated environment and subsequently deployed in real-world settings. Nevertheless, the training environment may differ from the real-world environment. Such a discrepancy, also known as the _sim-to-real gap_, will typically result in suboptimal performance of RL agents in real-world applications. One promising strategy to control the impact in performance degradation due to the sim-to-real gap is robust RL (Iyengar, 2005; Pinto et al., 2017; Hu et al., 2022), which aims to learn policies exhibiting strong (i.e. robust) performance under environmental deviations from the training environment, effectively hedging the epistemological uncertainty arising from the differences between the training environment and the unknown testing environments.

A robust RL problem is often formulated within a robust Markov decision process (RMDP) framework, with various types of robust sets characterizing different environmental perturbations. In this robust RL context, prior works have developed algorithms with provable sample complexity guarantees. However, these algorithms typically rely on either a generative model (Yang et al., 2022; Panaganti and Kalathil, 2022; Xu et al., 2023; Shi et al., 2023) or offline data with good coverage of deployment environments (Zhou et al., 2021; Panaganti et al., 2022; Shi and Chi, 2022; Ma et al., 2022; Blanchet et al., 2023). Notably, the current literature does not explicitly address the _exploration_ problem, which stands as one of the fundamental challenges in reinforcement learning through trial-and-error (Sutton and Barto, 2018). Meanwhile, the empirical success of robust RL methods (Pinto et al., 2017; Kuang et al., 2022; Moos et al., 2022) typically relies on reinforcement learning through _interactive data collection_ in the training environment, where the agent iteratively and actively interacts with the environment, collecting data, optimizing and robustifying its policy. Given that all the existing literature on robust RL theory relies on a generative model or pre-collected data, it is natural to ask:

_Can we design a provably sample-efficient robust RL algorithm that relies on interactive data collection in the training environment?_

Answering the above question faces a fundamental challenge, namely, that during the interactive data collection process, the learner no longer has the oracle control over the training data distributions that are induced by the policy learned through the interaction process. In particular, it could be the case that certain data patterns that are crucial for the policy to be robust across all testing environments are not accessible through interactive data collection, even through a sophisticated design of an exploration mechanism during the interaction process. For example, specific states may not be accessible within the training environment dynamics but could be reached in the testing environment dynamics.

In contrast, previous work has demonstrated that robust RL through a generative model or a pre-collected offline dataset with good coverage does not face such difficulty. In the generative model setup, fortunately, the learner can directly query any state-action pair and obtain the sampled next state from the generator. Intuitively, once the states that could appear in the testing environment trajectory are queried enough times, it is possible to guarantee the performance of the learned policy in testing environments. The situation is similar if one has a pre-collected offline dataset that possesses good coverage of the testing environment. This motivates us to take the initial steps towards answering the above questions regarding robust RL with interactive data collection.

### Contributions

In this work, we study robust RL in a finite-horizon RMDP with an \(\)-rectangular total-variation distance (TV) robust set (see Assumption 2.1 and Definition 2.4) through _interactive data collection_. We give both a fundamental hardness result in the general case and a sample-efficient algorithm within tractable settings. More specifically, our contributions are three folds.

**Fundamental hardness.** We construct a class of hard-to-learn RMDPs (see Example 3.1) and demonstrate that _any_ learning algorithm inevitably incurs an \(( HK)\)-online regret (Theorem 3.2) under at least one RMDP instance. Here, \(\) signifies the radius of the TV robust uncertainty set, \(H\) is the horizon, and \(K\) is the number of interactive episodes. This linear regret lower bound underscores the impossibility of sample-efficient robust RL via interactive data collection in general.

**Identifying a tractable case.** Upon close examination of the challenging instance, we recognize that the primary obstacle to achieving sample-efficient learning lies in the _curse of support shift_, i.e., the disjointedness of distributional support between the training environment and the testing environments. In a broader sense, the curse of support shift also refers to the situation when the states appearing in testing environments are extremely hard to arrive in the training environment.

To rule out these pathological instances, we propose the _vanishing minimal value_ assumption (Assumption 4.1), positing that the optimal robust value function reaches zero at a specific state. Such an assumption naturally applies to the sparse reward RL paradigm and offers a broader scope compared to the "fail-state" assumption utilized in prior studies on offline RMDP with function approximation (Panaganti et al., 2022). For a comprehensive discussion on this comparison, please see Remark B.3. On the theoretical front, we establish that the vanishing minimal value assumption effectively mitigates the support shift issues between the training and the testing environments (Proposition 4.2), rendering robust RL with interactive data collection feasible for RMDPs with TV robust sets.

**Efficient algorithm with sharp sample complexity.** Under the vanishing minimal value assumption, we develop an algorithm named OPtimistic Robust Value Iteration for TV Robust Set (OPROVI-TV, Algorithm 1), that is capable of finding an \(\)-optimal robust policy with a total number of

\[}\{H,^{-1}\} H^{2}SA/ ^{2}\] (1.1)

interactive samples (Theorem 4.3). Here \(S\) and \(A\) denote the number of states and actions, \(\) represents the radius of the TV robust set, and \(H\) is the horizon length of each episode. To our best knowledge, this is the first provably sample-efficient algorithm for robust RL with interactive data collection.

According to (1.1), the sample complexity of finding an \(\)-optimal robust policy decreases as the radius \(\) of the robust set increases. When the radius \(=0\), an RMDP reduces to a standard MDP, and the sample complexity (1.1) recovers the minimax-optimal sample complexity for online RL in standard MDPs up to logarithm factors, i.e., \(}(H^{3}SA/^{2})\).

In the end, we further extend our algorithm and theory to a new type of RMDPs, \(\)-rectangular discounted RMDP equipped with robust sets consisting of transition probabilities with bounded ratio to the nominal kernel (See Appendix B.4.2). This newly identified class of RMDPs naturally does not suffer from the support shift issue. It is equivalent to the \(\)-rectangular RMDP with TV robust set and vanishing minimal value assumption in an appropriate sense due to Proposition 4.2. Consequently, by a clever usage of Algorithm 1, we can also solve this new model sample-efficiently, as shown in Corollary B.5. Such a result echoes our intuition on the curse of support shift.

**Comparison to related works.** Due to the space limit, we only compare with the most related works through Table 1. A detailed discussion of related works is in Appendix A.

## 2 Preliminaries

**Notations.** For a set \(\), we denote \(()\) as the set of probability distributions on \(\). For a distribution \(p()\), we define the shorthand for expectation and variance as \(_{p()}[f]:=_{X p()}[f(X)]\) and \(_{p()}[f]=_{p()}[f^{2}]-(_{p()} [f])^{2}\). Given any set \(()\), we define the robust expectation

  Model Assump. & Algorithm & Data oracle &  Sample complexity \\ \([0,1)\) \\  \\   & RPVL (Xu et al., 2023) & generative model & \(}(SA}{^{2}})\) \\   & DRVI (Shi et al., 2023) & generative model & \(}(,^{-1}\}H^{2}_{} SA}{^{2}})\) \\   & lower bound (Shi et al., 2023) & generative model & \((,^{-1}\}H^{2}_{}SA}{^{2}})\) \\   & P\({}^{2}\)MPO(Blanchet et al., 2023) & offline dataset & \(}(_{}^{}H^{4}SA^{ }}{^{2}})\) \\   & lower bound (this work) & interactive data collection & intractable \\   “fail-state” \\ assumption \\  & RFQI (Panaganti et al., 2022) & offline dataset & \(}(_{}H^{4}SA}{^{2} ^{2}})\) \\  
 vanishing \\ minimal value \\ (Assumption 4.1) \\  & OPROVI-TV (this work) & interactive data collection & \(}(,^{-1}\}H^{2}SA}{ ^{2}})\) \\  

Table 1: Comparison between OPROVI-TV and prior results on RMDP with \(\)-rectangular TV robust sets under various settings (generative model/offline dataset/interactive data collection). For the infinite horizon \(\)-discounted RMDPs, we denote \(H_{}:=(1-)^{-1}\) as the effective horizon length. In the offline setting, \(_{}^{}\) and \(_{}\) represent the robust partial coverage coefficient and full coverage coefficient, respectively. In the general case, our lower bound reads intractable, meaning that there exist hard instances where it is impossible to learn the nearly optimal robust policy via a finite number of interactive samples.

operator as \(_{}[f]:=_{p()}_{X p( )}[f(X)]\). For any \(x,a\), we denote \((x)_{+}=\{x,0\}\) and \(x a=\{x,a\}\). We use \(()\) to hide absolute constant factors and use \(}\) to further hide logarithmic factors. For a positive integer \(H_{+}\), we denote the set \(\{1,2,,H\}\) by \([H]\).

### Robust Markov Decision Processes

We first introduce our underlying model for doing robust RL, the episodic robust Markov decision process (RMDP), denoted by a tuple \((,,H,P^{},R,)\). Here the set \(\) is the state space and the set \(\) is the action space, both with finite cardinality. The integer \(H\) is the length of each episode. The set \(P^{}=\{P^{}_{h}\}_{h=1}^{H}\) is the collection of _nominal_ transition kernels where \(P^{}_{h}:()\). The set \(R=\{R_{h}\}_{h=1}^{H}\) is the collection of reward functions where \(R_{h}:\). For simplicity, we denote \(=\{P(|,):( )\}\) as the space of all possible transition kernels, and we denote \(S=||\) and \(A=||\). Most importantly and different from standard MDPs, the RMDP is equipped with a mapping \(: 2^{}\) that characterizes the _robust set_ of any transition kernel in \(\). Formally, for any transition kernel \(P\), we call \((P)\) the _robust set_ of \(P\). One could interpret the nominal transition kernel \(P^{}_{h}\) as the transition of the training environment, while \((P^{}_{h})\) contains all possible transitions of the testing environments.

Given an RMDP \((,,H,P^{},R,)\), we consider using a Markovian policy to make decisions. A Markovian decision policy (or simply, policy) is defined as \(=\{_{h}\}_{h=1}^{H}\) with \(_{h}:()\) for each step \(h[H]\). To measure the performance of a policy \(\) in the RMDP, we introduce its _robust value function_, defined as for any \((s,a)\),

\[V^{}_{h,P^{},}(s) :=_{_{h}(P^{}_{h}),1  h H}_{\{_{h}\}_{h=1}^{H},\{_{h}\}_{h=1}^{H }}[_{i=h}^{H}R_{i}(s_{i},a_{i})|\,s_{h}=s],.\] \[.Q^{}_{h,P^{},}(s,a) :=_{_{h}(P^{}_{h}),1  h H}_{\{_{h}\}_{h=1}^{H},\{_{h}\}_{h=1}^{H }}[_{i=h}^{H}R_{i}(s_{i},a_{i})|\,s_{h}=s,a_{h}=a]..\]

Here the expectation is taken w.r.t. the state-action trajectories induced by policy \(\) under the transition \(\). One can also extend the definition of the robust value functions in terms of any collection of transition kernel \(P=\{P_{h}\}_{h=1}^{H}\) as \(V^{}_{h,P,}\) and \(Q^{}_{h,P,}\), which we usually use in the sequel.

Among all the policies, we define the optimal robust policy \(^{}\) as the policy that can maximize the robust value function at the initial time step \(h=1\), i.e.,

\[^{}*{argmax}_{=\{_{h}\}_{h=1}^{H}}V^{}_{1,P^ {},}(s_{1}), s_{1}.\] (2.1)

In other words, the optimal robust policy \(^{}\) maximizes the worst case expected total rewards in all possible testing environments. For simplicity and without loss of generality, we assume in the sequel that the initial state \(s_{1}\) is fixed. Our results could be directly generalized to \(s_{1} p_{0}()()\). Similarly, we can also define the optimal robust policy associated with a given stochastic process defined through any collection of transition kernels \(P=\{P_{h}\}_{h=1}^{H}\) in the same way as (2.1). We denote the optimal robust value functions associated with \(P\) as \(V^{}_{h,P,}\) and \(Q^{}_{h,P,}\) respectively.

\(\)**-rectangularity and robust Bellman equations.** We consider robust sets \(\) that have the \(\)-rectangular structure (Iyengar, 2005). which requires that the robust set is decoupled and independent across different \((s,a)\)-pairs. This kind of structure results in a dynamic programming representation of the robust value functions (efficient planning), and is thus commonly adopted in the literature of distributionally robust RL. More specifically, we assume the following.

**Assumption 2.1** (\(\)-rectangularity).: _We assume that the mapping \(\) satisfies for any transition kernel \(P\), the robust set \((P)\) is in the form of_

\[(P)=_{(s,a)}(s,a;P), where(s,a;P)().\]

Under above Assumption 2.1, we have the so-called robust Bellman equation (Iyengar, 2005; Blanchet et al., 2023) which gives a dynamic programming representation of robust value functions.

**Proposition 2.2** (Robust Bellman equation).: _Under Assumption 2.1, for any transition \(P=\{P_{h}\}_{h=1}^{H}\) and any policy \(=\{_{h}\}_{h=1}^{H}\) with \(_{h}:()\), it holds that_

\[V_{h,P,}^{}(s)=_{_{h}(|s)}Q_{h,P, }^{}(s,), Q_{h,P,}^{}(s,a)=R_{ h}(s,a)+_{(s,a;P_{h})}V_{h+1,P,}^{} .\]

Regarding the robust value functions of the optimal robust policy, we also have the following dynamic programming solution which plays a key role in our algorithm design and theoretical analysis.

**Proposition 2.3** (Robust Bellman optimal equation).: _Under Assumption 2.1, for any \(P=\{P_{h}\}_{h=1}^{H}\), the robust value functions of any optimal robust policy of \(P\) satisfies that,_

\[V_{h,P,}^{}(s)=_{a}Q_{h,P,}^ {}(s,a), Q_{h,P,}^{}(s,a)=R_{h}(s,a)+_{ (s,a;P_{h})}V_{h+1,P,}^{}.\]

_Taking \(_{h}^{}(|s)=*{argmax}_{a}Q_{h,P, }^{}(s,a)\), then \(^{}=\{_{h}^{}\}_{h=1}^{H}\) is optimal robust policy under \(P\)._

**Total-variation distance robust set.** In Assumption 2.1, the robust set \((s,a;P)\) is often modeled as a "distribution ball" centered at \(P(|s,a)\). In this paper, we mainly consider this type of robust sets specified by a _total-variation distance_ ball. We put it in the following definition.

**Definition 2.4** (Total-variation distance robust set).: _Total-variation distance robust set is defined as_

\[_{}(s,a;P):=\{()():D _{}()P(|s,a) \},\]

_for some \([0,1)\), where \(D_{}(\|)\) denotes the total variation distance defined as_

\[D_{}p()\|q():=_{s }p(s)-q(s), p(),q()( ).\] (2.2)

The TV robust set has recently been extensively studied by Yang et al. (2022); Panaganti and Kalathil (2022); Panaganti et al. (2022); Xu et al. (2023); Blanchet et al. (2023); Shi et al. (2023), which all focus on robust RL with a generative model or with a pre-collected offline dataset. More importantly, we emphasize that by (2.2) in Definition 2.4, we _do not_ define the TV distance through the notion of \(f\)-divergence which requires that the distribution \(p\) is absolute continuous w.r.t. \(q\), as is generally adopted by the above previous works on learning RMDPs with TV robust sets. According to (2.2), we _allow \(p\) to have a different support than \(q\)_. That is, there might exist an \(s\) such that \(p(s)>0\) and \(q(s)=0\). Given that, the TV robust set in Definition 2.4 could contain transition probabilities that have different supports than the nominal transition probability \(P^{}(|s,a)\).

An essential property of the TV robust set is that the robust expectation involved in the robust Bellman equations (Propositions 2.2 and 2.3) has a duality representation that only uses the expectation under the nominal transition kernel, as is shown in the following theorem and proved in Appendix C.1.

**Proposition 2.5** (Strong duality representation).: _Under Definition 2.4, the following duality representation for the robust expectation holds, for any \(V:[0,H]\) and \(P_{h}:()\),_

\[_{_{}(s,a;P_{h})}V=_{[0,H]} -_{P_{h}(|s,a)}(-f)_{+}- -_{s}V(s)_{+}+}.\] (2.3)

**Value gap between maximum and minimum.** Finally, another useful property of the robust value functions of an RMDP with TV robust sets is a fine characterization of the gap between the maximum and the minimum of the robust value function, which is first identified and utilized by Shi et al. (2023) for an infinite horizon RMDP with TV robust sets. In this work, we prove and use a similar result for the finite horizon case, concluded in the following proposition. The proof is in Appendix C.2.

**Proposition 2.6** (Gap between maximum and minimum).: _Under Assumption 2.1 with the robust set specified by Definition 2.4, the robust value functions satisfies that_

\[_{(s,a)}Q_{h,P,}^{ }(s,a)-_{(s,a)}Q_{h,P,}^{ }(s,a) H,^{-1}},\] \[_{s}V_{h,P,}^{}(s)-_{s }V_{h,P,}^{}(s) H,^{-1}},\]

_for any transition \(P=\{P_{h}\}_{h=1}^{H}\), any policy \(\), and any step \(h[H]\)._

### Robust RL with Interactive Data Collection

We study how to learn the optimal robust policy \(^{}\) in (2.1) from interactive data collection. Specifically, the learner is required to interact with _only_ the _training environment_, i.e., \(P^{}\), for some \(K\) episodes. In each episode \(k\), the learner adopts a policy \(^{k}\) to interact with the training environment \(P^{}\) and to collect data. When the \(k\)-th episode ends, the learner updates its policy to \(^{k+1}\) based on historical data and proceeds to the subsequent \((k+1)\)-th episode. The process ends after \(K\) episodes.

**Sample complexity.** We use the notion of _sample complexity_ as the key evaluation metric. For any given algorithm and predetermined accuracy level \(>0\), the sample complexity is the minimum number of episodes \(K\) required for the algorithm to output an \(\)-optimal robust policy \(\) satisfying \(V^{}_{1,P^{},}(s_{1})-V^{}_{1,P^{},}(s_{1}) \). The goal is to design algorithms whose sample complexity has small or even optimal dependence on the problem parameters \(S,A,H,\), and \(1/\).

**Online regret.** Another evaluation metric that is related to the minimization of sample complexity is the _online regret_, which is the cumulative difference between the optimal robust policy \(^{}\) and the executed policies \(\{^{k}\}_{k=1}^{K}\). Formally, we define \(_{}(K):=_{k=1}^{K}V^{}_{1,P^{},}(s_ {1})-V^{^{k}}_{1,P^{},}(s_{1})\). The goal is to design algorithms that can achieve a sublinear-in-\(K\) regret with small dependence on \(S,A,H,\). It turns out that any sublinear-regret algorithm can be easily converted to a polynomial-sample complexity algorithm by applying the standard online-to-batch conversion (Jin et al., 2018).

## 3 A Hardness Result: The Curse of Support Shift

Unfortunately, we show in this section that in general such a problem of robust RL with online data collection is _impossible_ - there exists a simple class of two RMDPs such that an \((K)\)-online regret lower bound exists. However, previous works on robust RL with a generative model or offline data with good coverage do provide sample-efficient ways to find the optimal robust policy for this class of RMDPs. This is a separation between robust RL with interactive data collection and generative model/offline data. Please see also Figure 1 for an illustration of the example.

**Example 3.1** (Hard example of robust RL with interactive data collection).: _Consider two RMDPs \(_{0}\) and \(_{1}\) which only differ in their nominal transition kernels. The state space is \(=\{s_{},s_{}\}\), and the action space is \(=\{0,1\}\). The horizon length \(H=3\). The reward function \(R\) always is \(1\) at the good state \(s_{}\) and is \(0\) at the bad state \(s_{}\), i.e.,_

\[R_{h}(s,a)=1,&s=s_{}\\ 0,&s=s_{},(a,h)[H].\]

_For the good state \(s_{}\), the next state is always \(s_{}\). For the bad state \(s_{}\), there is a chance to get to the good state \(s_{}\), with the transition probability depending on the action it takes. Formally,_

\[P^{,_{0}}_{h}(s_{}|s_{},a) =1,(a,h)\{1,2\},\{0,1\},\] \[P^{,_{0}}_{2}(s_{}|s_{},a) =p,&a=\\ q,&a=1-,\{0,1\},\]

_where \(p,q\) are two constants satisfying \(0<q<p<1\). Intuitively, when at the bad state, the optimal action would result in a higher transition probability \(p\) to the good state than the transition probability \(q\) induced by the other action. Finally, we consider the robust set being specified by a total-variation distance ball centered at the nominal transition kernel, that is, for any \(P\),_

\[(P)=_{(s,a)} _{}(s,a;P),\ _{}(s,a;P)=\{()():D_{ }()P(|s,a) \},\] (3.1)

_where \([0,q]\) is the parameter characterizing the size of the robust set. We set \(s_{1}=s_{}\)._

For this class of RMDPs, we have the following hardness result for doing robust RL with interactive data collection, an \(( K)\)-online regret lower bound. The proof is in Appendix D.1.

**Theorem 3.2** (Hardness result (based on Example 3.1)).: _There exists two RMDPs \(\{_{0},_{1}\}\), the following regret lower bound holds,_\[_{}_{\{0,1\}}[_{}^{ _{},}(K)] HK,\]

_where \(_{}^{_{},}(K)\) refers to the online regret of algorithm \(\) for RMDP \(_{}\)._

The reason why any algorithm fails for this class of RMDPs is the _support shift_ of the worst-case transition kernel. In robust RL, the performance of a policy \(\) is evaluated via the robust expected total rewards, or equivalently, the expected return under the most adversarial transition kernel \(P^{,}\). In such an example, as we explicitly show in the proof, when in the good state \(s_{}\), the worst-case transition kernel \(P^{,}\) would transit the state to \(s_{}\) with a constant probability \(>0\). But the state \(s_{}\) is out of the scope of the data collection process because starting from \(s_{1}=s_{}\) the nominal transition kernel always transits the state to \(s_{}\). As a result, the performance of the learned policy at the bad state \(s_{}\) is not guaranteed, and inevitably incurs an \(( K)\)-lower bound of regret, a hardness result. Furthermore, by strategically constructing RMDPs with the horizon \(3H\) based on Example 3.1, we can derive a lower bound of \(( HK)\). See Appendix B.3 for more discussions.

## 4 A Solvable Case, Efficient Algorithm, and Sharp Analysis

Motivated by the hard instance (Example 3.1), we now investigate a special subclass of RMDPs with \(\)-rectangular total variation robust set that we show allows for doing sample-efficient robust RL through interactive data collection. In Section 4.1, we introduce the assumption we impose on the RMDP. We propose our algorithm design in Section 4.2, with theoretical analysis in Section 4.3.

### Vanishing Minimal Value: Eliminating Support Shift

To overcome the difficulty of support shift identified in Section 3, we make the following _vanishing minimal value_ assumption on the underlying RMDP.

**Assumption 4.1** (Vanishing minimal value).: _We assume that the underlying RMDP satisfies that \(_{s}V_{1,P^{},}^{}(s)=0\). Also, WLOG, we assume that the initial state \(s_{1}*{argmin}_{s}V_{1,P^{},}^{} (s)\)._

Assumption 4.1 imposes that the minimal robust expected total rewards over all possible initial states is \(0\). Assuming that the initial state \(s_{1}*{argmin}_{s}V_{1,P^{},}^{ }(s)\) avoids making the problem trivial. A close look at Assumption 4.1 actually gives that the minimal robust value function of any policy \(\) at any step is zero, that is, \(_{s}V_{h,P^{},}^{}(s)=0\) for any policy \(\) and any step \(h[H]\). With this observation, the following proposition explains why this assumption helps to overcome the difficulty, with the proof of the proposition in Appendix C.3.

**Proposition 4.2** (Equivalent expression of TV robust set with vanishing minimal value).: _For any function \(V:[0,H]\) with \(_{s}V(s)=0\), we have that_

\[_{_{}(s,a;P_{h}^{})}[V]=^{} _{_{^{}}(s,a;P_{h}^{})}[V], with ^{}=1->0,\]

_where the TV robust set \(_{}(s,a;P_{h}^{})\) is defined in (3.1) and the set \(_{^{}}(s,a;P_{h}^{})\) is defined as2_

\[_{^{}}(s,a;P_{h}^{})=\{() ():_{s^{}}(s^{ })}{P_{h}^{}(s^{}|s,a)}}\}.\]As Proposition 4.2 indicates, under Assumption 4.1, the robust Bellman equations (Propositions 2.2 and 2.3) at step \(h[H]\) is equivalent to taking an infimum over another robust set \(_{^{}}(s,a;P_{h}^{*})\) that shares the _same_ support as the nominal transition kernel \(P^{*}(|s,a)\), discounted by a constant \(^{}<1\). Intuitively, this new robust set rules out the difficulty originated in unseen states in training environments and the discount factor \(^{}\) hedges the difficulty from prohibitively small probability of reaching certain states that may appear often in the testing environments. This renders robust RL with interactive data collection possible. See Appendix B.4.1 for discussions/examples of Assumption 4.1.

### Algorithm Design: Oprovi-Tv

In this section, we propose our algorithm that solves robust RL with interactive data collection for RMDPs with \(\)-rectangular total-variation (TV) robust sets (Assumption 2.1 and Definition 2.4) and satisfying the vanishing minimal value assumption (Assumption 4.1). Our algorithm, OOptimistic RObust Value Iteration for TV Robust Set (OPROVI-TV, Algorithm 1), can automatically balance exploitation and exploration during the interactive data collecting process while managing the distributional robustness of the learned policy. The full algorithm OPROVI-TV is given in Algorithm 1.

```
1:Initialize: dataset \(=\).
2:for episode \(k=1,,K\)do
3: Training environment transition estimation:
4: Update the count functions \(N_{h}^{k}(s,a,s^{})\) and \(N_{h}^{k}(s,a)\) based on \(\).
5: Calculate the transition kernel estimator \(_{h}^{k}\) as \(N_{h}^{k}(s,a,s^{})/(N_{h}^{k}(s,a) 1)\).
6: Optimistic robust planning:
7: Set \(_{H+1}^{k}=_{H+1}^{k}=0\).
8:for step \(h=H,,1\)do
9: Set \(_{h}^{k}(,)\) and \(_{h}^{k}(,)\) as (4.2) and (4.3), with \(_{h}^{k}(,)\) defined in (4.5).
10: Set \(_{h}^{k}(|)=*{argmax}_{a}\, {Q}_{h}^{k}(,a),\,\,_{h}^{k}()=_{_{h}^{k}( |)}[_{h}^{k}(,)]\), and \(_{h}^{k}()=_{_{h}^{k}(|)}[ {Q}_{h}^{k}(,)]\).
11:endfor
12: Execute the policy in training environment and collect data:
13: Receive the initial state \(s_{1}^{k}\).
14:for step \(h=1,,H\)do
15: Take action \(a_{h}^{k}_{h}^{k}(|s_{h}^{k})\), observe reward \(R_{h}(s_{h}^{k},a_{h}^{k})\) and the next state \(s_{h+1}^{k}\).
16:endfor
17: Set \(\) as \(\{(s_{h}^{k},a_{h}^{k},s_{h+1}^{k})\}_{h=1}^{H}\).
18:endfor
19:Output: Randomly (uniformly) return a policy from \(\{^{k}\}_{k=1}^{K}\). ```

**Algorithm 1** OOptimistic Robust Value Iteration for TV Robust Set (OPROVI-TV)

**Step I: Training Environment Transition Estimation (Line 3 to 5).** At the beginning of each episode \(k[K]\), we maintain an estimate of the transition kernel \(P^{*}\) of the training environment by using the historical data \(=\{(s_{h}^{r},a_{h}^{r},s_{h+1}^{r})\}_{r=1,h=1}^{k-1,H}\) collected from the interaction with the training environment. Specifically, we simply adopt a vanilla empirical estimator, defined as \(_{h}^{k}(s^{}|s,a)=N_{h}^{k}(s,a,s^{})/(N_{h}^{k}(s,a)  1)\) for any \((s,a,h,s^{})[H]\), where the count functions \(N_{h}^{k}(s,a,s^{})\) and \(N_{h}^{k}(s,a)\) are calculated based on the current dataset \(\) by \(N_{h}^{k}(s,a,s^{})=_{r=1}^{k-1}((s_{h}^{r},a_{h}^ {r},s_{h+1}^{r})=(s,a,s^{})}\) and \(N_{h}^{k}(s,a)=_{s^{}}N_{h}^{k}(s,a,s^{})\) for any \((s,a,h,s^{})[H]\). This just coincides with the transition estimator adopted by existing non-robust online RL algorithms (Auer et al., 2008; Azar et al., 2017; Zhang et al., 2021).

**Step II: Optimistic Robust Planning (Line 6 to 11).** Given \(^{k}(|,)\) that estimates the training environment, we perform an optimistic robust planning to construct the policy \(^{k}\) to execute. Basically, the optimistic robust planning follows the robust Bellman optimal equation (Proposition 2.3) to approximate the optimal robust policy, but differs in that it maintains an upper bound and a lower bound of the optimal robust value function and chooses the policy that maximizes the optimistic estimate to incentivize exploration during data collection. Here the purpose of maintaining the lower bound estimate is to facilitate the construction of the variance-aware optimistic bonus (see following), which helps to sharpen our theoretical analysis.

\(\)_Simplifying the robust expectation._ To utilize the vanishing minimal value condition (Assumption 4.1), we take a closer look into the robust Bellman equation. By strong duality (Proposition 2.5), the robust expectation \(_{_{}(s,a;P)}[V]\) for any \(V[0,H]\) satisfying \(_{s}V(s)=0\) is equivalent to

\[_{_{}(s,a;P)}V=_{ [0,H]}-_{P(|s,a)}-V_{+} +1-}.\] (4.1)

Consequently, with a slight abuse of the notation, in the remaining of the paper, we **re-define** the operator \(_{_{}(s,a;P)}[V]\) as the right hand side of (4.1). Due to Assumption 4.1, the robust Bellman (optimal) equation (Proposition 2.2 and Proposition 2.3) still holds under this new definition.

\(\)_Optimistic robust planning._ With this in mind, the optimistic robust planning goes as follows. Starting from \(^{k}_{H+1}=^{k}_{H+1}=0\), we recursively define that for any \((s,a)\),

\[^{k}_{h}(s,a) =\{R_{h}(s,a)+_{_{}(s,a; ^{k}_{h})}^{k}_{h+1}+^{k}_ {h}(s,a),\{H,^{-1}\}\},\] (4.2) \[^{k}_{h}(s,a) =\{R_{h}(s,a)+_{_{}(s,a; ^{k}_{h})}^{k}_{h+1}-^{k}_ {h}(s,a),0\},\] (4.3)

where the robust expectation \(_{_{}(s,a;^{k}_{h})}\) follows the definition in the right hand side of (4.1), and the bonus function \(^{k}_{h}(s,a) 0\) is defined later. Here we truncate the optimistic estimate \(^{k}_{h}\) via the upper bound \(\{H,^{-1}\}\) of the true optimal robust value function \(Q^{*}_{h,P^{*},\,}\). This truncation arises from the combined implication of Proposition 2.6 and the fact that \(_{(s,a)}Q^{*}_{h,P^{*},\,}(s,a)=0\) under Assumption 4.1. As we establish in Lemma E.2, \(^{k}_{h}\) and \(^{k}_{h}\) form upper and lower bounds for \(Q^{*}_{h,P^{*},\,}\) and \(Q^{^{k}}_{h,P^{*},\,}\) under a proper choice of the bonus. After performing (4.2) and (4.3), we choose the data collection policy \(^{k}_{h}\) to be the optimal policy with respect to the optimistic estimator \(^{k}_{h}\) and define \(^{k}_{h}\) and \(^{k}_{h}\) accordingly by

\[^{k}_{h}(|)=*{argmax}_{a} \,\,^{k}_{h}(,a),^{k}_{h}(s)=_{ ^{k}_{h}(|s)}^{k}_{h}(s,), ^{k}_{h}(s)=_{^{k}_{h}(|s)}^{k}_{h}(s,).\] (4.4)

We remark that the purpose of maintaining the lower bound estimate (4.3) is to facilitate the construction of the bonus and to help sharpen our theoretical analysis. The construction of the policy \(^{k}\) is still based on the optimistic estimator, which is why we call it optimistic robust planning. As indicated by theory, the optimistic robust planning can effectively guide the policy to explore uncertain _robust_ value function estimates, striking a balance between exploration and exploitation while managing distributional robustness.

\(\)_Bonus function._ The bonus function \(^{k}_{h}(s,a)\) is a Bernstein-style bound defined as

\[_{^{k}_{h}(|s,a)} ^{k}_{h+1}+^{k}_{h+1}/2c_{1} }{H}}+_{^{k}_{h}(|s,a)} {V}^{k}_{h+1}-^{k}_{h+1}}{H}+H^{2}S}{N^{k} _{h}(s,a) 1}+},\] (4.5)

where \(=(S^{3}AH^{2}K^{3/2}/)\), \(c_{1},c_{2}>0\) are absolute constants, and \(\) signifies a pre-selected fail probability. Under (4.5), \(^{k}_{h}\) and \(^{k}_{h}\) become upper and lower bounds of the optimal robust value functions (Lemma E.2). More importantly, the bonus (4.5) is carefully designed for robust value functions such that the summation of this bonus term (especially the leading variance term in (4.5)) over time steps is well controlled, for which we also develop new analysis methods. This is critical for obtaining a sharp sample complexity of Algorithm 1.

### Theoretical Guarantees

This section establishes the online regret and the sample complexity of OPROVI-TV (Algorithm 1). Our main result is the following, upper bounding the online regret of Algorithm 1, proved in Appendix E.

**Theorem 4.3** (Online regret of Oprovi-tv).: _Given an RMDP with \(\)-rectangular total-variation robust set of radius \([0,1)\) (Assumption 2.1 and Definition 2.4) satisfying Assumptions 4.1, choosing the bonus function as (4.5) with sufficiently large \(c_{1},c_{2}>0\), then with probability at least \(1-\), Algorithm 1 satisfies_

\[_{}(K) H,^{-1}}H^{2}SAK^{}},\]

_where \(^{}=^{2}(SAHK/)\) and \(()\) hides absolute constants and lower order terms in \(K\)._

Theorem 4.3 shows that Algorithm 1 enjoys a sublinear online regret of \(}()\), meaning that it is able to approximately find the optimal robust policy through interactive data collection. This is in contrast with the general hardness result in Section 3 where sample-efficient learning is impossible in the worst case. Thus we show the effectiveness of the minimal value assumption for robust RL with interactive data collection. As a corollary, we have the following sample complexity bound for Algorithm 1. It is obtained directly from Theorem 4.3 and a standard online to batch conversion.

**Corollary 4.4** (Sample complexity of Oprovi-tv).: _Under the same setup and conditions as in Theorem 4.3, with probability at least \(1-\), Algorithm 1 can output an \(\)-optimal policy within_

\[(H,^{-1}}H^{2}SA^{}/ ^{2})\] (4.6)

_episodes, where \(^{}=(SAH/)\) and \(()\) hides absolute constants. The valid range of \(\) satisfies \((0,c\{1,1/( H)\}]\) for some constant \(c>0\)._

We compare the sample complexity (4.6) with prior arts on non-robust online RL and robust RL with a generative model. On the one hand, (4.6) with \(=0\) equals to \(}(H^{3}SA/^{2})\), matching the minimax sample complexity lower bound for online RL in non-robust MDPs (Azar et al., 2017). This means that our algorithm design can naturally handle non-robust MDPs as a special case (please also see Remark B.4 for why one can reduce Algorithm 1 to general non-robust MDPs under Assumption 4.1). On the other hand, the previous work of Shi et al. (2023) for robust RL in infinite horizon RMDPs with a TV robust set and a generative model showcases a minimax optimal sample complexity of

\[}(H_{},^{-1}}H_{ }^{2}SA/^{2}),\]

for \([0,1)\), where we \(H_{}:=1/(1-)\) is the effective horizon of the infinite \(\)-discounted RMDPs. As a result, the sample complexity (4.6) of Algorithm 1 matches their result. We highlight that our algorithm does not rely on a generative model and operates purely through interactive data collection.

**Extensions of Algorithm 1 and its theory.** In Appendix B.4.2, we extend Algorithm 1 to solve a new type of RMDPs whose robust set consists of transition probabilities with bounded ratio to the nominal kernel. The intuition is because it is equivalent to the \(\)-rectangular RMDP with a TV robust set and vanishing minimal value assumption in an appropriate sense (Proposition 4.2) Consequently, by a clever usage of Algorithm 1, we can also solve this new model sample-efficiently, as is shown in Corollary B.5. Such a result echoes our intuition on the curse of support shift.

## 5 Conclusions and future works

This work shows that in the absence of any structural assumptions, robust RL via interactive data collection necessarily induces a linear regret lower bound in the worst case due to the curse of support shift. Under the vanishing minimal value assumption, an assumption that is able to effectively rule out the potential support shift issues for RMDPs with a TV robust set, we propose a sample-efficient robust RL algorithm for those RMDPs with sharp analysis. Potential future works include extending to function approximation settings and other types of robust sets. See discussion in Appendix B.5.