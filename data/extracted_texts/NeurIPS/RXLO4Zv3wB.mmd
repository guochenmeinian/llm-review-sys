# DDR: Exploiting Deep Degradation Response as Flexible Image Descriptor

Juncheng Wu\({}^{1,3}\), Zhangkai Ni\({}^{1}\)1, Hanli Wang\({}^{1}\), Wenhan Yang\({}^{2}\), Yuyin Zhou\({}^{3}\), Shiqi Wang\({}^{4}\)

\({}^{1}\) School of Computer Science and Technology, Tongji University, China

\({}^{2}\) Pengcheng Laboratory, China

\({}^{3}\) Department of Computer Science and Engineering, University of California, Santa Cruz, USA

\({}^{4}\) Department of Computer Science, City University of Hong Kong, Hong Kong

jwu418@ucsc.edu, {zkni, hanliwang}@tongji.edu.cn

yangwh@pcl.ac.cn, yzhou284@ucsc.edu, shiqwang@cityu.edu.hk

Corresponding author: Zhangkai Ni (zkni@tongji.edu.cn)

###### Abstract

Image deep features extracted by pre-trained networks are known to contain rich and informative representations. In this paper, we present Deep Degradation Response (DDR), a method to quantify changes in image deep features under varying degradation conditions. Specifically, our approach facilitates flexible and adaptive degradation, enabling the controlled synthesis of image degradation through text-driven prompts. Extensive evaluations demonstrate the versatility of DDR as an image descriptor, with strong correlations observed with key image attributes such as complexity, colorfulness, sharpness, and overall quality. Moreover, we demonstrate the efficacy of DDR across a spectrum of applications. It excels as a blind image quality assessment metric, outperforming existing methodologies across multiple datasets. Additionally, DDR serves as an effective unsupervised learning objective in image restoration tasks, yielding notable advancements in image deblurring and single-image super-resolution. Our code is available at: https://github.com/eezkni/DDR

## 1 Introduction

Deep features extracted by pre-trained neural networks are well-known for their capacity to encode rich and informative representations . Extensive research efforts have aimed to quantify the information encoded within these deep features for use as image descriptors. For example, the distance between deep features has been employed as a metric for image quality assessment (IQA) in various studies . Additionally, researchers have studied differences between images by comparing the distributions  or frequency components  of their deep features. Moreover, the statistical properties of deep features have been found to correlate with the style and texture of images in prior works . Recent research has also highlighted that the internal dissimilarity between deep features at different image scales can serve as a potent visual fingerprint .

This paper delves into an intriguing and unexplored property of image deep features: their response to degradation. Specifically, when subjecting images with diverse content and textures to various types of degradation, such as blur, noise, or JPEG compression, the deep features of these images exhibit varying degrees of change. This phenomenon is illustrated in Fig. 1, where Gaussian Blur is applied to images. The degrees of changes in feature space reflect the deep feature response to specific degradation. As shown in Tab. 1, a strong correlation exists between this response and the quality scores of blurred images. We validate that the response of deep features to degradation effectivelycaptures different image characteristics by varying the type of degradation. Therefore, we propose the Deep Degradation Response (DDR), which quantifies the response of image deep features to specific degradation types, serving as a powerful and flexible image descriptor.

One straightforward approach to compute the DDR is to apply handcrafted degradation to the image, extract degraded features from the degraded image, and then calculate the distance between these degraded features and the features of the original image. However, as shown in Fig. 2, adjusting the level of degradation applied to the image significantly affects the distribution of DDR. Therefore, meticulous adjustment of the degradation level is crucial to achieve optimal performance in various downstream tasks. To address this challenge, we propose a text-driven degradation fusing strategy. Inspired by manifold data augmentation algorithms [12; 13], we adaptively fuse text features representing specific degradation onto the original image features, resulting directly in degraded features. By manipulating the text, we can effectively control the type of degradation fused to the image features. This approach allows us to flexibly assess the response of image features to various degradation types, thereby enhancing adaptability across different downstream tasks.

We evaluate the performance of our proposed DDR across multiple downstream tasks. Firstly, we assess its effectiveness as an image quality descriptor on the opinion-unaware blind image quality assessment (OU-BIQA) task, where DDR demonstrates superior performance compared to existing OU-BIQA methods across various datasets. Secondly, we employ DDR as an unsupervised learning objective, training image restoration models specifically to maximize the DDR of the output image, which includes tasks such as image deblurring and real-world single-image super-resolution. Incorporating DDR as an external training objective consistently improves performance in both tasks, highlighting the strength of DDR as a flexible and powerful image descriptor.

## 2 Related Works

**Deep Feature Based Image Descriptors.** Image descriptors aim to quantify fundamental characteristics of images, such as texture , color [23; 24], complexity , and quality [26; 27]. With the informative representations in deep features extracted by pre-trained networks, various efforts have been made to develop image descriptors based on these features. Many existing descriptors regress the deep features of an image to a score, training the model by minimizing the loss between these predicted scores and the ground truth scores labeled by humans [25; 20; 28]. However, these methods are somewhat inflexible for two reasons: (1) they rely on human-labeled opinion scores, and (2) they are designed to evaluate fixed image characteristics. In this paper, we propose a flexible alternative by measuring the degradation response of deep features.

**Image Degradation Representation for Image Restoration.** Various deep learning-based image restoration methods leverage image degradation representation to enhance model performance [29; 30; 31; 32]. For instance, some methods utilize contrastive-based  and codebook-based  techniques to encode various degradation types, enhancing the model's robustness to unknown forms of degradation. Moreover, other methods design degradation-aware modules to extract degradation representations from images and guide the removal of degradation [31; 32]. However, these methods depend on task-specific training. In contrast, the proposed DDR flexibly obtains representations for different types of degradation, making it an effective image descriptor for various image restoration tasks.

**Multimodal Vision Models.** Recent advancements in multimodal vision models, achieved through extensive training on paired image-text data [33; 34; 35; 36; 37], have significantly enhanced the capability of these models to understand and describe image textures using natural language . Researchers have explored various methods to leverage this capability for modifying image texture attributes through language guidance. For instance, in language-guided image style transfer [39; 40], natural language descriptions are used to define the target texture style. Additionally, Moon et al.  introduced a manifold data augmentation technique that integrates language-guided attributes into image features. Building on these ideas, our work aims to adaptively fuse degradation information into image deep features using language guidance, thereby facilitating the measurement of our proposed DDR.

## 3 Deep Degradation Response as Flexible Image Descriptor

### Deep Degradation Response

We define the Deep Degradation Response (DDR) as the measure of change in image deep features when specific types of degradation are introduced, which can be mathematically expressed as:

\[_{d}(i)=(,_{d} ),\] (1)

where \(d\) represents the type of degradation. \((,)\) denotes a disparity metric, such as \(L_{n}\) distance or cosine distance. \(=_{v}(i)\) represents the original image features extracted by a pre-trained network \(_{v}()\), while \(_{d}\) denotes the degraded features.

The core of the proposed DDR lies in how to model \(_{d}\). A naive approach to this involves synthesizing _degradation in the pixel domain_, _i.e._, generating degraded images. As shown in Fig. 3 (a), a handcrafted degradation process is applied to the image, leading to the creation of a degraded image \(i_{d}\). The extent of degradation is controlled by the parameter \(_{d}\). Then a pre-trained visual encoder \(_{v}()\) is utilized to extract the features of \(i_{d}\), generating degraded features. Therefore, the pixel space degradation synthesizing process can be formulated as:

\[_{d}=_{v}((i,_{d})),\] (2)

where \(()\) denotes the handcrafted degradation synthesis process. However, as depicted in Fig. 2, varying levels of degradation significantly affect DDR. For downstream tasks, it is imperative to meticulously determine the optimal \(_{d}\) for different manual processes. This not only poses a substantial challenge but also diminishes the robustness of DDR as an image descriptor.

In this study, we propose a novel and efficient method for modeling \(_{d}\) by synthesizing _degradation in the feature domain_ using text-driven prompts. Specifically, to construct the degradation representation, we first design a pair of prompts: one describing an image with a specific type of degradation and the other describing the same image without degradation. These prompts are then separately encoded using the text encoder \(_{t}()\) of CLIP , yielding text-driven degradation representations \(_{d}^{-}\) and \(_{d}^{+}\), respectively. We obtain the degradation direction in the feature space by calculating the difference between these representations, as follows:

\[_{d}=_{d}^{-}-_{d}^{+},\] (3)

where \(_{d}^{-}=_{t}(P_{d}^{-})\) and \(_{d}^{+}=_{t}(P_{d}^{+})\). \(P_{d}^{-}\) and \(P_{d}^{+}\) represent the degraded and clean prompts, respectively. However, due to the gap between text and image modality within the feature space [41; 42] of the CLIP model, we cannot effectively obtain the degraded image feature by directly fusing the features from different modalities. To address this challenge, we propose an adaptive degradation adaptation strategy by'stylizing' the text-driven degradation representation using the image feature. Inspired by AdaIN , we propose to align the mean and variance of \(_{d}\) to match those of the image feature, which can be formulated as follows:

\[}_{d}=()(_{d}-( _{d})}{(_{d})})+(),\] (4)

where \(}_{d}\) denotes the adapted degradation representation. Finally, we fuse the image feature with \(}_{d}\), and the feature space text-driven degradation process can be represented as:

\[_{d}=+}_{d}.\] (5)

Our proposed degradation fusion method allows us to measure DDR across various types of degradation simply by modifying the text prompt, eliminating the need for handcrafted design processes. Additionally, our adaptation strategy enables the application of text-driven degradation to image features without adjusting any hyper-parameters. As shown in Fig. 2, in the LIVEitw  dataset, DDR with text-driven feature degradation method achieves a distribution similar to DDR with carefully adjusted optimal degradation level, demonstrating the flexibility of our method.

### DDR as a Flexible Image Descriptor

By modifying the degradation type, DDR can capture different characteristics in natural images. We demonstrate this by measuring DDR with different degradation types across all images in the

Figure 3: **The framework of our proposed DDR** with two different degradation fusing methods. (a) Synthesizing degradation with a handcrafted process _in the pixel domain_. (b) Fusing text-driven degradation _in the feature domain_.

LIVEitw  dataset. Specifically, we set five pairs of prompts, representing five types of degradation, including color, noise, blur, exposure, and content. We employ a fixed prompt formatting for different types of degradation, as follows:

\[P_{d}^{-}=\ \{d^{-}\}\ ; P_{d}^{+}=\ \{d^{+}\}\ .\] (6)

For example, when the degradation type is blur, the \(d^{-}\) and \(d^{+}\) are set as 'blurry' and'sharp' respectively. The images with high and low DDR for each type of degradation are shown in Fig. 4. We observe a negative correlation between the DDR and the level of degradation within the image. For example, as demonstrated in Fig. 4(e), an image with a high DDR to content degradation retains clear content, while the corresponding image with a low DDR exhibits unrecognizable content.

To further quantify the correlation between DDR and other image characteristics, we calculate the Spearman's Rank Correlation Coefficient (SRCC) between DDR and four types of image characteristics. Specifically, we measure the complexity , colorfulness , and sharpness  of images in the CSIQ  dataset. Additionally, we use the Mean Opinion Score (MOS) of each image as its quality score. The results are presented in Tab. 2. It is interesting to note that there is a negative correlation between the complexity of an image and DDR. This suggests that more complex images are capable of enduring more degradation with a smaller degree of change in deep features. Furthermore, the DDR to color and blur degradations show the highest correlation with colorfulness and sharpness respectively. Overall, with different degradation types, DDR tends to emphasize different image characteristics. Therefore, DDR shows promise as a versatile image descriptor for diverse downstream tasks through simple prompt adjustments, including IQA and image restoration.

### DDR as a Blind Image Quality Assessment Metric

DDR can function as an image quality descriptor. As shown in Tab. 2, there is a positive correlation between the quality score and the DDR of the image. In cases where image quality is predominantly affected by a specific degradation type, an image with a high DDR to this degradation would likely

   Degradation Type & Complexity  & Colorfulness  & Sharpness  & Quality \\  color & -0.223 & 0.757 & 0.715 & 0.790 \\ noise & -0.444 & 0.673 & 0.600 & 0.694 \\ blur & -0.206 & 0.612 & 0.732 & 0.756 \\ exposure & -0.435 & 0.684 & 0.699 & 0.770 \\ content & -0.357 & 0.612 & 0.561 & 0.642 \\   

Table 2: **SRCC between DDR and image characteristics.** With different types of degradation, DDR exhibits varying degrees of correlation with each image characteristic.

Figure 4: **Images with high and low DDR to different degradation types.** We measure DDR with five types of degradation by setting their corresponding prompt pair. We observe that image with lower DDR to a specific type of degradation is likely to obtain this degradation of a higher level.

obtain a higher quality. For instance, in Fig. 4, when the degradation type is blur, comparing the image with a high DDR to the image with a low DDR, it is evident that the former exhibits a sharper content with less blur. However, real-world images often feature a mix of degradations. To evaluate image quality in such scenarios, we formulate a set of degradations denoted as \(\) and compute the mean DDR for each degradation in \(\). The blind image assessment metric based on DDR can thus be formulated as follows:

\[_{}(i)=|}_{d }^{d}_{d}(i).\] (7)

### DDR as an Unsupervised Learning Objective

We can also utilize DDR as a learning objective in image restoration tasks, where the goal is to train a deep learning-based restoration model to predict a clean image from a degraded one. This is achieved by optimizing the restoration model to minimize the reconstruction loss function, which quantifies the difference between the pixel values of the model's output and the ground truth. In this work, we demonstrate that incorporating DDR as an external unsupervised learning objective can improve the optimization of the restoration models. Specifically, we measure the DDR of the model output and aim to simultaneously minimize the reconstruction loss while maximizing the DDR. The learning objective of the image restoration model is thus formulated as:

\[_{}(_{rec}(R_{}(i),i_{gt})- _{d}_{d}^{d}_{d}(R_{}(i))),\] (8)

where \(R_{}()\) is a restoration model parameterised by \(\), and \(_{rec}(,)\) denotes the reconstruction loss, \(_{d}\) is the weight of DDR in learning objective. Similarly, by adjusting the degradation prompt and combining different types of degradation, we can tailor the approach to various restoration tasks.

## 4 Experiments

### Experiment Setting

To demonstrate the versatility and efficacy of DDR as an image descriptor, we conduct comprehensive experiments covering (1) opinion-unaware blind image quality assessment (OU-BIQA), which does not require training model with human-labeled Mean Opinion Score (MOS) values, and (2) image restoration tasks, including image deblurring and real-world image super-resolution.

**Implementation Details.** For different tasks, we tailor the degradation set \(\) in Eq. 7 and Eq. 8 to focus on distinct image attributes. Specifically, in BIQA, we define \(=\{,,,\}\). Meanwhile, for image restoration tasks, we set \(=\{,,\}\). In all experiments related to image restoration, we empirically set the weight of DDR in the learning objective in Eq. 8 as \(_{d}=2.0\). We use the CLIP  ViT-B/32 model as the image feature extractor, and employ the cosine distance to quantify the disparity between original and degraded image features, which can be defined as follows:

\[_{cos}(x,y)=1-,\] (9)

**Baseline Datasets.** To evaluate the effectiveness of the proposed DDR as an image quality descriptor, we conduct extensive experiments on eight public IQA datasets, including CSIQ , TID2013 , KADID , KonIQ , LIVE in-the-wild , LIVE , CID2013 , and SPAQ , which encompass both synthetic and real-world degradation scenarios. For image deblurring, we train and test the model using the GoPro dataset  and RealBlur dataset , respectively. The GoPro dataset  consists of synthetic blurred images, while RealBlur  contains images with real-world motion blur. For SISR, we combine two real-world datasets together for training and testing, including the RealSR  and City100  datasets.

**Baseline Methods.** For the OU-BIQA task, we compare DDR with representative and state-of-the-art opinion-unaware BIQA (OU-BIQA) methods, which do not require training with human-labeled MOS. The compared methods include NIQE , QAC , PIQE , LPSI , ILNIQE , diqIQ , SNP-NIQE , NPQI , and ContentSep . Among all compared methods, DDRis the only _zero-shot_ method that does not require any training. For image restoration, we compare our proposed method, as illustrated in Eq. 8, with a combination of reconstruction loss and feature domain loss \(_{f}(,)\), which quantifies the distance between deep features extracted from images. Generally, the reconstruction loss is combined with feature domain losses to enhance the overall quality of the restored image, forming the learning objective of the restoration model \(R_{}()\) as follows:

\[_{}(_{rec}(R_{}(i),i_{gt})+ _{f}_{f}(R_{}(i),i_{gt})),\] (10)

where \(_{f}\) is the weighting factor for \(_{f}(,)\). In all experiments on image restoration, we utilize PSNR loss as the reconstruction loss. We consider four types of representative feature domain losses for comparison, including LPIPS , CTX , PDL , and FDL . To ensure a fair comparison, we set \(_{f}=0.1\) for FDL  and \(_{f}=1.0\) for the other feature domain losses, ensuring that the magnitudes of the different feature domain losses are in a similar range.

Moreover, to fully assess the robustness of our proposed DDR across various architectural models, we conduct all image restoration experiments using two representative image restoration models: NAFNet  and Restormer . NAFNet  is a convolutional neural network (CNN)-based model, while Restormer  is a Transformer -based model. These models have demonstrated impressive performance in their respective tasks and are widely recognized as representative models in recent years. We empirically train the model at a resolution of \(128 128\). For the learning rate, we adhere to the official settings for NAFNet and Restormer. Specifically, the initial learning rate for NAFNet is set to \(1e-3\), and for Restormer, it is set to \(3e-4\). We also adopted a cosine annealing strategy for both models.

### Opinion-Unaware Blind Image Quality Assessment

Tab. 3 presents the results across all datasets. Our proposed DDR consistently outperforms all competing methods on datasets with both synthetic [45; 46; 47] and in-the-wild [48; 14; 49; 50] degradation, underscoring its robustness across diverse degradation types. Especially its substantial improvement in SRCC on the LIVE in-the-wild dataset, rising from 0.5060 to 0.6613, showcasing the effectiveness of DDR as an image quality descriptor for images with real-world degradation. Furthermore, comparing the SRCC performance in Tab. 3 and Tab. 2, it is obvious that on the CSIQ dataset , DDRs that integrate multiple types of degradation perform significantly better than DDRs that only focus on a single type of degradation. This underscores the superiority of DDR as a more comprehensive image quality descriptor simply by integrating multiple types of degradation.

### Image Motion Deblurring

The objective of image deblurring is to restore a high-quality image with clear details. The quantitative analysis in Tab. 4 illustrates that our proposed DDR surpasses all compared loss functions across datasets with both synthetic and real-world blur. Compared to optimizing solely the PSNR loss, our approach achieves a notable enhancement in PSNR, with an increase of at least 0.16 dB across all models and datasets. These results suggest that maximizing the DDR of the predicted image results in higher fidelity and reduced degradation. This is further evident in the qualitative results shown in Fig. 5, where the PSNR loss alone produces blurry textures, and combining PSNR with feature

   Datasets & NIQE & QAC & PIQE & LPSI & ILNIQE & dipIQ & SNP-NIQE & NPQI & ContentSep & Ours \\  CSIQ & 0.6191 & 0.4804 & 0.5120 & 0.5218 & 0.8045 & 0.5191 & 0.6090 & 0.6341 & 0.5871 & **0.8289** \\ LIVE & 0.9062 & 0.8683 & 0.8398 & 0.8181 & 0.8975 & **0.9378** & 0.9073 & 0.9108 & 0.7478 & 0.8793 \\ TID2013 & 0.3106 & 0.3719 & 0.3636 & 0.3949 & 0.4938 & 0.4377 & 0.3329 & 0.2804 & 0.2530 & **0.5844** \\ KADID & 0.3779 & 0.2394 & 0.2372 & 0.1478 & 0.5406 & 0.2977 & 0.3719 & 0.3909 & 0.5060 & **0.5968** \\ KonIQ & 0.5300 & 0.3397 & 0.2452 & 0.2239 & 0.5057 & 0.2375 & 0.6284 & 0.6132 & 0.6401 & **0.6455** \\ LIVEitw & 0.4495 & 0.2258 & 0.2325 & 0.0832 & 0.4393 & 0.2089 & 0.4654 & 0.4752 & 0.5060 & **0.6613** \\ CID2013 & 0.6589 & 0.0299 & 0.0448 & 0.3229 & 0.3062 & 0.3776 & 0.7159 & 0.7698 & 0.6116 & **0.8009** \\ SPAQ & 0.3105 & 0.4397 & 0.2317 & 0.0001 & 0.6959 & 0.2189 & 0.5402 & 0.5999 & 0.7084 & **0.7249** \\   

Table 3: **Quantitative result of OU-BIQA. Performance comparisons of different OU-BIQA models on eight public datasets using SRCC. The top performer on each dataset is marked in bold.**domain losses introduces noticeable artifacts. In contrast, incorporating DDR substantially reduces artifacts, yielding predicted images with sharper and more natural textures.

### Single Image Super Resolution

SISR is a task aimed at enhancing the resolution of a low-resolution image to match or surpass the quality of a high-resolution counterpart. In our study, we evaluate our proposed DDR method against state-of-the-art loss functions. Tab. 5 showcases the quantitative results on a real-world dataset by two representative models (NAFNet and Restormer). Our findings reveal that our method outperforms all competing methods in terms of PSNR. Particularly noteworthy is the improvement achieved with NAFNet, where the incorporation of DDR alongside the reconstruction loss elevates the PSNR from 27.08 to 27.31. Additionally, as depicted in Fig. 6, our method yields visual results with finer texture compared to those optimized solely for PSNR or combined with LPIPS.

    &  &  &  \\  & & PSNR & SSIM & PSNR & SSIM \\   & PSNR & 33.1717 & 0.9482 & 30.6373 & 0.9038 \\  & PSNR + LPIPS  & 33.1660 & 0.9481 & 30.7245 & 0.9044 \\  & PSNR + CTX  & 32.7879 & 0.9436 & 30.4394 & 0.8985 \\  & PSNR + PDL  & 32.9417 & 0.9463 & 30.6270 & 0.9039 \\  & PSNR + FDL  & 32.8321 & 0.9420 & 30.1743 & 0.8864 \\  & PSNR + DDR(ours) & **33.3427** & **0.9500** & **30.7982** & **0.9049** \\   & PSNR & 33.3398 & 0.9494 & 31.9816 & 0.9098 \\  & PSNR + LPIPS  & 33.3717 & 0.9495 & 31.9639 & 0.9099 \\   & PSNR + CTX  & 33.2834 & 0.9483 & 31.9893 & 0.9101 \\   & PSNR + PDL  & 33.2905 & 0.9487 & 31.9900 & 0.9106 \\   & PSNR + FDL  & 33.3560 & 0.9489 & 31.7673 & 0.9034 \\   & PSNR + DDR(ours) & **33.4946** & **0.9513** & **32.1759** & **0.9121** \\   

Table 4: **Quantitative result of image motion deblurring. Experiment is conducted on datasets with synthetic  and real-world  blur respectively. The best results are marked in **bold**. Combining proposed DDR with reconstruction loss leads to result with less degradation and higher fidelity. Our proposed method demonstrates the robustness to model architecture and dataset.**

Figure 5: **Qualitative result on RealBlur  dataset. The training of model is supervised by (1) reconstruction loss (PSNR) (2) reconstruction loss combined with feature domain loss, and (3) reconstruction loss combined with DDR. The red area is cropped from different results and enlarged for visual convenient. Appending DDR as an external self-supervised learning objective leads to result with more natural texture and less artifacts.**

### Ablation Study

For image deblurring, we conduct a series of ablation experiments on the NAFNet using the GoPro dataset, with all results detailed in Table 6. Firstly, we adjusted \(\) to evaluate the effect of the degradation set defined in Eq. 8. Notably, a decrease in performance is observed when any type of degradation is removed, suggesting that combining multiple types of degradation results in a more comprehensive image description. Secondly, we investigate the effect of \(_{d}\) in Eq. 8. Minor fluctuations in performance are observed when adjusting \(_{d}\) to 1.0 and 3.0, indicating the robustness of our method to this hyper-parameter. Next, we explore the effect of the visual feature extractor. Increasing the scale of \(_{v}\) in DDR does not lead to improved performance, suggesting that a larger visual model may not necessarily enhance the ability to understand low-level texture. Finally, we examine the impact of the adaptation strategy in Eq. 4. A significant drop in performance is observed when the adaptation is removed, highlighting the critical role of this strategy in DDR calculation.

For opinion-unaware blind image quality assessment task, we conduct ablation experiment on four datasets. As demonstrated in Tab. 7, comparing with measuring DDR to single type of degradation, combining mutiple degradation types consistently leads to significant performance improvement. Furthermore, we can observe a performance boost by utilizing degradation adaptation strategy, improving SRCC from 0.6074 to 0.8289 on CSIQ dataset.

## 5 Limitations and Discussion

In this section, we discuss the limitations of DDR and provide potential solutions to address them.

**The ability of the visual feature extractor to understand low-level degradation.** We currently employ the CLIP model's visual feature extractor to facilitate text-driven degradation fusion. These feature extractors may incline to focus on high-level information such as image content, while their ability to understand low-level degradation may be limited. This could impact the measurement of the degradation response. In future work, we plan to fine-tune the feature extractor on tasks such

    &  &  \\  & PSNR & SSIM & PSNR & SSIM \\  PSNR & 27.0856 & 0.8917 & 28.1491 & 0.8986 \\ PSNR + LPIPS  & 27.2835 & **0.8938** & 28.1221 & 0.8985 \\ PSNR + CTX  & 27.0985 & 0.8867 & 28.0933 & 0.8964 \\ PSNR + PDL  & 26.9467 & 0.8907 & 28.1413 & 0.8985 \\ PSNR + FDL  & 27.0263 & 0.8809 & 28.0746 & 0.8966 \\ PSNR + DDR(ours) & **27.3121** & 0.8923 & **28.1668** & **0.8990** \\   

Table 5: **Quantitative result on real-world SISR dataset [53; 54].**

Figure 6: **Qualitative result on real-world SISR dataset [53; 54]. DDR leads to results with sharper texture.**as degradation classification or description, to enable it to extract more fine-grained degradation features.

**The selection of degradation prompts for different downstream tasks.** The suitable degradation prompts may vary for different downstream tasks. In future work, we hope to append learnable tokens in the degradation prompts, and fine-tune these tokens to better adapt our method to different tasks. Specifically, we can utilize the strategy such as adversarial training, training DDR as a discriminator. This is an interesting and promising direction for our further investigation.

## 6 Conclusions

This paper introduces a flexible and powerful image descriptor, which measures the response of image deep features to degradation. We propose a text-driven approach to adaptively fuse degradation into image features. Experimental results demonstrate that DDR achieves state-of-the-art performance in blind image quality assessment task, and optimizing DDR results in images with reduced distortion and improved overall quality in image restoration tasks. We believe that DDR can facilitate a better understanding and application of image deep features.