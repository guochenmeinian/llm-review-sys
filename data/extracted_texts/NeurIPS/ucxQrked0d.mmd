# Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning

Qi Wang\({}^{1,2}\) Junming Yang\({}^{3}\) Yunbo Wang\({}^{1}\) Xin Jin\({}^{2}\) Wenjun Zeng\({}^{2}\) Xiaokang Yang\({}^{1}\)

\({}^{1}\) MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China

\({}^{2}\) Ningbo Institute of Digital Twin, Eastern Institute of Technology, China

\({}^{3}\) School of Computer Science and Engineering, Southeast University, China

https://qiwang067.github.io/coworld

Equal contribution.Corresponding author: Yunbo Wang <yunbow@sjtu.edu.cn>.

###### Abstract

Training offline RL models using visual inputs poses two significant challenges, _i.e._, the overfitting problem in representation learning and the overestimation bias for expected future rewards. Recent work has attempted to alleviate the overestimation bias by encouraging conservative behaviors. This paper, in contrast, tries to build more flexible constraints for value estimation without impeding the exploration of potential advantages. The key idea is to leverage off-the-shelf RL simulators, which can be easily interacted with in an online manner, as the "_test bed_" for offline policies. To enable effective online-to-offline knowledge transfer, we introduce CoWorld, a model-based RL approach that mitigates cross-domain discrepancies in state and reward spaces. Experimental results demonstrate the effectiveness of CoWorld, outperforming existing RL approaches by large margins.

## 1 Introduction

Learning control policies with visual observations can be challenging due to high interaction costs with the physical world. Offline reinforcement learning (RL) is a promising approach to address this challenge . However, the direct use of current offline RL algorithms in visual control tasks presents two primary difficulties. Initially, _offline visual RL_ is more prone to overfitting issues during representation learning, as it involves extracting hidden states from the limited, high-dimensional visual inputs. Moreover, like its state-space counterpart, offline visual RL is susceptible to the challenge of value overestimation, as we observe from existing methods .

Improving offline visual RL remains an under-explored research area. We aim to balance between overestimating and over-conservatian of the value function to avoid excessively penalizing the estimated values beyond the offline data distribution. Intuitively, _we should not overly constrain the exploration with potential advantages._ Our basic idea, as illustrated in Figure 1, is to leverage readily available online simulators for related (not necessarily identical) visual control tasks as auxiliary source domains, so that we can frame offline visual RL as an _offline-online-offline_ transfer learning problem to learn mildly conservative policies.

Figure 1: Our approach for offline visual RL.

We present a novel model-based transfer RL approach called Collaborative World Models (CoWorld). Specifically, we train separate world models and RL agents for source and target domains, each with domain-specific parameters. To mitigate discrepancies between the world models, we introduce a novel representation learning scheme comprising two iterative training stages. These stages, as shown in Figure 1, facilitate the alignment of latent state distributions (_offline to online_) and reward functions (_online to offline_), respectively. By doing so, the source domain critic can serve as an online "test bed" for assessing the target offline policy. It is also more "knowledge" as it can actively interact with the online environment and gather rich information. Another benefit of the domain-collaborative world models is the ability to alleviate overfitting issues of offline representation learning, leading to more generalizable latent states derived from limited offline visual data.

For behavior learning in the offline dataset, we exploit the knowledge from the source model and introduce a mild regularization term to the training objective of the target domain critic model. This regularization term encourages the **source critic** to reevaluate the **target policy**. As illustrated in Figure 2, it allows for flexible constraint on overestimated values of trajectories that receive low values from the "knowledgeable" source critic. Conversely, if a policy yields high values from the source critic, we prefer to retain the original estimation by the offline agent. This approach is feasible because the source critic has been aligned with the target domain during world model learning.

We showcase the effectiveness of CoWorld in offline visual control tasks across the Meta-World, RoboDesk, and DeepMind Control benchmarks. Our approach is shown to be readily extendable to scenarios with multiple source domains. It effectively addresses value overestimation by transferring knowledge from auxiliary domains, even in the presence of diverse physical dynamics, action spaces, reward scales, and visual appearances. In summary, our work brings the following contributions:

* We innovatively frame offline visual RL as a domain transfer problem. The fundamental idea is to harness cross-domain knowledge to tackle representation overfitting and value overestimation in offline visual control tasks.
* We present CoWorld, a method that follows the offline-online-offline paradigm, incorporating specific techniques of world model alignment and flexible value constraints.

## 2 Problem Setup

We consider offline visual reinforcement learning as a partially observable Markov decision process (POMDP) that aims to maximize the cumulative reward in a fixed target dataset \(^{(T)}\). We specifically focus on scenarios where auxiliary environments are accessible, enabling rich interactions and efficient online data collection. The goal is to improve the offline performance of the target POMDP \(^{(T)},^{(T)},^{(T)}, ^{(T)},^{(T)}\) through knowledge transfer from the source POMDPs \(^{(S)},^{(S)},^{(S)},^{(S)},^{(S)}\). These notations respectively denote the space of visual observations, the space of actions, the state transition probabilities, the reward function, and the discount factor.

For example, in one of our experiments, we employ RoboDesk as the offline target domain and various tasks from Meta-World as the source domains. As illustrated in Table 1, these two environments present notable distinctions in physical dynamics, action spaces, reward definitions, and visual

Figure 2: To address value overestimation in offline RL **(a)**, we can directly penalize the estimated values beyond the distribution of offline data, which may hinder the agentâ€™s exploration of potential states with high rewards **(b)**. Unlike existing methods, CoWorld trains a cross-domain critic model in an online auxiliary domain to reassess the offline policy **(c)**, and regularizes the target values with flexible constraints **(d)**. The feasibility of this approach lies in the domain alignment techniques during the world model learning stage.

appearances as the observed images are from different camera views. Our priority is to address domain discrepancies to enable cross-domain behavior learning.

## 3 Method

In this section, we present the technical details of CoWorld, which consists of a pair of world models \(\{_{^{}},_{}\}\), actor networks \(\{_{^{}},_{}\}\), and critic networks \(\{v_{^{}},v_{}\}\), where \(\{,,\}\) and \(\{^{},^{},^{}\}\) are respectively target and source domain parameters. As potential cross-domain discrepancies may exist in all elements of \(\{,,,\}\), the entire training process is organized into three iterative stages, following an _offline-online-offline_ transfer learning framework:

* _Offline-to-online state alignment_: Train the offline world model \(_{}\) by aligning its state space with that of the source world model \(_{^{}}\).
* _Online-to-offline reward alignment_: Train \(_{^{}}\) and \(\{_{^{}},v_{^{}}\}\) in the online environment by incorporating the target reward information.
* _Online-to-offline value constraint_: Train the target offline-domain agent \(\{_{},v_{}\}\) with value constraints provided by the source critic \(v_{^{}}\).

### Offline-to-Online State Alignment

Source model pretraining.We start with a source domain warm-up phase employing a model-based actor-critic method known as DreamerV2 . To facilitate cross-domain knowledge transfer, we additionally introduce a state alignment module, which is denoted as \(g()\) and implemented using the softmax operation. The world model \(_{^{}}\) consists of the following components:

\[& h_{t}^{(S)}=f_{ ^{}}(h_{t-1}^{(S)},z_{t-1}^{(S)},a_{t-1}^{(S)})\ & e_{t}^{(S)}=e_{^{}}(o_{t}^{(S)})\\ & z_{t}^{(S)} q_{^{ }}(h_{t}^{(S)},e_{t}^{(S)})\ &_{t}^{(S)} p_{^{ }}(h_{t}^{(S)})\\ &_{t}^{(S)} p_{^{ }}(h_{t}^{(S)},z_{t}^{(S)})\ &_{t}^{(S)} r_{^{ }}(h_{t}^{(S)},z_{t}^{(S)})\\ &_{t}^{(S)} p _{^{}}(h_{t}^{(S)},z_{t}^{(S)})\ \ \ \ \ \ \ \ \ & s_{t}^{(S)}=g(e_{t}^{(S)}),\] (1)

where \(^{}\) represents the combined parameters of the world model. We train \(_{^{}}\) on the dynamically expanded source domain experience replay buffer \(^{(S)}\) by minimizing

\[(^{})&= _{q_{^{}}}_{t=1}^{N}}(o_ {t}^{(S)} h_{t}^{(S)},z_{t}^{(S)})}_{} }(r_{t}^{(S)} h_{t}^{(S)},z_{t}^{(S)})}_ {}}(_{t}^{(S)} h _{t}^{(S)},z_{t}^{(S)})}_{}\\ &q_{^{}}(z_{ t}^{(S)} h_{t}^{(S)},o_{t}^{(S)}) p_{^{}}(_{t}^{(S)}  h_{t}^{(S)})}_{}.\] (2)

We train the source actor \(_{^{}}(_{t})\) and critic \(v_{^{}}(_{t})\) with the respective objectives of maximizing and estimating the expected future rewards \(_{p_{^{}},p_{^{}}}[_{ t} _{-t}_{}]\) generated by \(_{^{}}\). Please refer to Appendix A.3 for more details. We deploy \(_{^{}}\) to interact with the auxiliary environment and collect new data for further world model training.

State alignment.A straightforward transfer learning solution is to train the target agent in the offline dataset upon the checkpoints of the source agent. However, it may suffer from a potential mismatch issue due to the discrepancy in tasks, visual observations, physical dynamics, and action spaces across various domains. This becomes more severe when the online data is collected from

    & Source: _Meta-World_ & Target: _RoboDesk_ & Similarity / Difference \\  Task & Window Close & Open Slide & Related manipulation tasks \\ Dynamics & Simulated Sawyer robot arm & Simulated Franka robot arm & Different \\ Action space & Box(-1, 1, (4), float64) & Box(-1, 1, (5,), float32) & Different \\ Reward scale &  &  & Different \\ Observation & Right-view images & Top-view images & Different view points \\   

Table 1: RoboDesk (_target domain_) vs. Meta-World (_auxiliary source domain_).

environments that differ from the offline dataset (_e.g._, Meta-World \(\) RoboDesk). We tackle this issue by separating the parameters of the source and the target agents while explicitly aligning their latent state spaces. Concretely, the target world model \(_{}\) has an identical network architecture to the source model \(_{^{}}\). We feed the same target domain observations sampled from \(^{(T)}\) into these models and close the distance of \(e_{^{}}(o_{t}^{(T)})\) and \(e_{}(o_{t}^{(T)})\). We optimize \(_{}\) by minimizing

\[()&=_{_{ }}_{t=1}^{N}(o_{t}^{(T)} h_{t}^{(T )},z_{t}^{(T)})}_{}(r_{t}^{(T)}  h_{t}^{(T)},z_{t}^{(T)})}_{}( _{t}^{(T)} h_{t}^{(T)},z_{t}^{(T)})}_{}\\ &[q_{}(z_{t}^{(T)} h _{t}^{(T)},o_{t}^{(T)})]p_{}(_{t}^{(T)} h_{t}^{(T)}) }_{}[(g(e_{ ^{}}(o_{t}^{(T)})))]|g(e_{}(o_{t}^{(T)})) ]}_{},\] (3)

where \(()\) indicates gradient stopping and we use the encoding from the source model as the state alignment target. As the source world model can actively interact with the online environment and gather rich information, it keeps the target world model from overfitting the offline data. The importance of this loss term is governed by \(_{2}\). We examine its sensitivity in the experiments.

### Online-to-Offline Reward Alignment

To enable the source agent to value the target policy, it is essential to provide it with prior knowledge of the offline task. To achieve this, we train the source reward predictor \(r_{^{}}()\) using mixed data from both of the replay buffers \(^{(S)}\) and \(^{(T)}\). Through the behavior learning on source domain imaginations, the target-informed reward predictor enables the source RL agent to assess the imagined states produced by the target model and provide a flexible constraint to target value estimation (as we will discuss in Section 3.3).

Specifically, we first sample a target domain data trajectory \(\{(o_{t}^{(T)},a_{t}^{(T)},r_{t}^{(T)})\}_{t=1}^{T}\) from \(^{(T)}\) (**Line 14** in Alg. 1). We then use the source world model parametrized by \(^{}\) to extract corresponding latent states and relabel the _target-informed source reward_ (**Line 15** in Alg. 1):

\[_{t}&=f_{^{}}( _{t-1},_{t-1},a_{t-1}^{(T)})_{t}=e_{^{ }}(o_{t}^{(T)})\\ _{t}& q_{^{}}(_{t}, _{t})_{t}^{(S)}=(1-k) r_ {^{}}(_{t},_{t})+k r_{t}^{(T)},\] (4)

where \(k\) is the target-informed reward factor, which acts as a balance between the true target reward \(r_{t}^{(T)}\) and the output of the source reward predictor \(r_{^{}}()\) provided with target states. It is crucial to emphasize that using the target data as inputs to compute \(r_{^{}}()\) is feasible due to the alignment of the target state space with the source state space.

We jointly use the relabeled reward \(_{t}^{(S)}\) and the original source domain reward \(r_{t}^{(S)}\) sampled from \(^{(S)}\) to train the source reward predictor. This training is achieved by minimizing a maximum likelihood estimation (MLE) loss:

\[_{r}(^{})=\ _{^{(S)}} _{t=1}^{N}- r_{^{}}(r_{t}^{(S)}|h_{t}^{(S)},z_{t}^{(S)}) +(1-)_{^{(T)}}_{t=1}^{N}- r_{ ^{}}(_{t}^{(S)}|h_{t}^{(T)},z_{t}^{(T)}),\] (5)

where the second term measures the negative log-likelihood of observing the relabelled source reward \(_{t}^{(S)}\). \(\) represents a hyperparameter that gradually decreases from \(1\) to \(0.1\) throughout this training stage. Intuitively, \(\) controls the progressive adaptation of the well-trained source reward predictor to the target domain with limited target reward supervision. We integrate Eq. (5) into Eq. (2) to train the entire world model \(_{^{}}\) for the source domain agent (**Line 16** in Alg. 1) and subsequently perform behavior learning to enable the source critic to assess the target policy (**Lines 17-19** in Alg. 1).

### Min-Max Value Constraint

In the behavior learning phase of the target agent (**Lines 8-10** of Alg. 1), we mitigate value overestimation in the offline dataset by introducing a min-max regularization term to the objective function of the target critic model \(v_{}\). Initially, we use the auxiliary source critic \(v_{^{}}\) to estimate the value function of the imagined target states. Following that, we train \(v_{}\) by additionally _minimizing the maximum value_ among the estimates provided by source and target critics:

\[()=\ _{p_{},p_{}}_{t=1}^{H-1} (v_{}(_{t}^{(T)})-(V_{t}^{(T )}))^{2}}_{}+\ (_{t}^{(T)}),\ (v_{^{}}(_{t}^{(T)})))}_{},\] (6)

where \(V_{t}^{(T)}\) incorporates a weighted average of reward information over an \(n\)-step future horizon. The first term in the provided loss function fits cumulative value estimates (whose specific formulation can be located in Appendix A.3), while the second term regularizes the overestimated values for out-of-distribution data in a mildly conservative way. The hyperparameter \(\) represents the importance of the value constraint. The \(()\) operator indicates that we stop the gradient to keep the source critic from being influenced by the regularization term.

This approach provides flexibly conservative value estimations, finding a balance between mitigating overestimation and avoiding excessive conservatism in the value function. When the target critic overestimates the value function, the source critic is less vulnerable to the value overestimation problem as it is trained with rich interaction data. Thus, it is possible to observe \(v_{}(_{t}^{(T)})>v_{^{}}(_{t}^{(T)})\), and our approach is designed to decrease the output of \(v_{}\) to the output of \(v_{^{}}\). This prevents the target critic from overestimating the true value. Conversely, when the source critic produces greater values in \(v_{^{}}(_{t}^{(T)})\), the min-max regularization term does not contribute to the training of the target critic \(v_{}\). This encourages the exploration of potentially advantageous states within the imaginations of the target world model. In line with DreamerV2 , we train the target actor \(_{}\) by maximizing a REINFORCE objective function with entropy regularization, allowing the gradients to backpropagate directly through the learned dynamics:

\[()=_{p_{},p_{}}_{t=1}^{H-1}([a_{t}^{(T)}\ |\ _{t}^{(T)}]}_{}+ ^{(T)}}_{}+(_{t}^{(T)}\ |\ _{t}^{(T)})(V_{t}^{(T)}-v_{}(_{t}^{(T)}))}_{ }).\] (7)

As previously mentioned, \(V_{t}^{(T)}\) involves a weighted average of reward information over an \(n\)-step future horizon, with detailed formulation provided in Appendix A.3.

Furthermore, it is crucial to note that CoWorld can readily be extended to scenarios with multiple source domains by adaptively selecting a useful task as the auxiliary domain. This extension is easily achieved by measuring the distance of the latent states between the target domain and each source domain. For technical details of the adaptive source domain selection, please refer to Appendix C.

## 4 Experiments

### Experimental Setups

Datasets.We evaluate CoWorld across three visual control environments, _i.e._, Meta-World , RoboDesk , and DeepMind Control Suite (DMC) , including both cross-task and cross-environment setups (Meta-World \(\) RoboDesk). Inspired by D4RL , we build offline datasets of _medium-replay_ quality using DreamerV2 . The datasets comprise all the samples in the replay buffer collected during the training process until the policy attains medium-level performance, defined as achieving \(1/3\) of the maximum score that the DreamerV2 agent can achieve. Please refer to Appendix B.2 for further results of CoWorld trained with _medium-expert_ offline data.

Compared methods.We compare CoWorld with both model-based and model-free RL approaches, including _Offline DV2_, _DrQ+BC_, _CQL_, _CURL_, and _LOMPO_. In addition, we introduce the _DV2 Finetune_ method, which involves taking a DreamerV2  model pretrained in the online source domain and subsequently finetuning it in the offline target dataset. Furthermore, _DV2 Finetune_ can be integrated with the continual learning method, Elastic Weight Consolidation (EWC) , to regularize the model for preserving source domain knowledge, _i.e._, _Finetune+EWC_. Please refer to Appendix E for more details.

### Cross-Task Experiments on Meta-World

Meta-World is an open-source simulated benchmark designed for solving a wide range of robot manipulation tasks. We select \(6\) tasks as either the offline dataset or potential candidates for the online auxiliary domain. These tasks include: _Door Close_ (**DC\({}^{*}\)**), _Button Press_ (**BP**), _Window Close_ (**WC**), _Handle Press_ (**HP**), _Drawer Close_ (**DC**), _Button Topdown_ (**BT**).

Main results.As shown in Table 2, we compare the results of CoWorld with other models on Meta-World. CoWorld achieves the best performance in all 6 tasks. Notably, it outperforms _Offline DV2_, a method also built upon DreamerV2 and specifically designed for offline visual RL. For the online-to-offline finetuning models, _DV2 Finetune_ achieves the second-best results by leveraging transferred knowledge from the auxiliary source domain. However, we observe that its performance experiences a notable decline in scenarios (_e.g._, Meta-World \(\) RoboDesk) involving significant data distribution shifts between the source and the target domains in visual observation, physical dynamics, reward definition, or even the action space of the robots. Another important baseline model is _DV2 Finetune+EWC_, which focuses on mitigating the catastrophic forgetting of the knowledge obtained in source domain pretraining. Nevertheless, without additional model designs for domain adaptation, retaining source domain knowledge may eventually lead to a decrease in performance in the target domain. The LOMPO model suffers from the _negative transfer_ effect when incorporating a source pretraining stage. It achieves an average return of \(1,\!712\) when it is trained from scratch in the offline domain while achieving an average return of \(792\) for online-to-offline finetuning. It implies that a naive transfer learning method may degenerate the target performance due to unexpected bias.

Results with a random source domain.Given that we present the _best-source_ results in Table 2, where we manually select one source task from Meta-World, one may cast doubt on the influence of

   Model & BP\(\) DC\({}^{*}\) & DC \(\) BP & BT\(\) WC & BP\(\) HP & WC\(\) DC & HP\(\) BT & Avg. \\  Offline DV2 & 2143\(\)579 & 3142\(\)533 & 3921\(\)752 & 278\(\)128 & 3899\(\)679 & 3002\(\)346 & 2730 \\ DrQ + BC & 567\(\)19 & 587\(\)68 & 623\(\)85 & 1203\(\)234 & 134\(\)64 & 642\(\)99 & 626 \\ CQL & 1984\(\)13 & 867\(\)330 & 683\(\)268 & 988\(\)39 & 577\(\)121 & 462\(\)67 & 927 \\ CURL & 1972\(\)11 & 51\(\)17 & 281\(\)73 & 986\(\)47 & 366\(\)52 & 189\(\)10 & 641 \\ LOMPO & 2883\(\)183 & 446\(\)458 & 2983\(\)569 & 2230\(\)223 & 2756\(\)331 & 1961\(\)287 & 1712 \\  DV2 Finetune & 3500\(\)414 & 2456\(\)661 & 3467\(\)1031 & 3702\(\)451 & 4273\(\)1327 & 3499\(\)713 & 3781 \\ DV2 Finetune + EWC & 1566\(\)723 & 167\(\)86 & 978\(\)772 & 528\(\)334 & 2048\(\)1034 & 224\(\)147 & 918 \\ LOMPO Finetune & 259\(\)191 & 95\(\)53 & 142\(\)70 & 332\(\)452 & 3698\(\)1615 & 224\(\)88 & 792 \\  CoWorld (Best-Source) & **3967\(\)312** & **3623\(\)543** & **4521\(\)367** & **4570\(\)677** & **4845\(\)14** & **3889\(\)159** & **4241** \\ CoWorld (Multi-Source) & 3864\(\)352 & 3573\(\)541 & 4507\(\)59 & 4460\(\)783 & 4678\(\)137 & 3626\(\)275 & 4094 \\   

Table 2: Mean episode returns and standard deviations of \(10\) episodes over \(3\) seeds on Meta-World.

domain discrepancies between the auxiliary environment and the target offline dataset. In Figure 3 (Left), the transfer matrix of CoWorld among the \(6\) tasks of Meta-World is presented, where values greater than \(1\) indicate positive domain transfer effects. Notably, there are challenging cases with weakly related source and target tasks. In the majority of cases (\(26\) out of \(30\)), CoWorld outperforms _Offline DV2_, as illustrated in the heatmap.

Results with multiple source domains.It is crucial to note that CoWorld can be easily extended to scenarios with multiple source domains by adaptively selecting a useful task as the auxiliary domain. From Table 2, we can see that the multi-source CoWorld achieves comparable results to the models trained with manually designated online simulators. In Figure 3 (Left), multi-source CoWorld achieves positive improvements over _Offline DV2_ in all cases, approaching the best results of models using each source task as the auxiliary domain. In Figure 3 (Right), it also consistently outperforms the _DV2 Finetune_ baseline model. These results demonstrate our approach's ability to execute without strict assumptions about domain similarity and its ability to automatically identify a useful online simulator from a set of both related and less related source domains.

### Cross-Environments: Meta-World to RoboDesk

To explore cross-environment transfer with more significant domain gaps, we employ four tasks from RoboDesk to construct individual offline datasets, _i.e., Push Button_, _Open Slide_, _Drawer Open_, _Upright Block off Table_. These tasks require handling randomly positioned objects with image inputs. Table 1 presents the differences between the two environments in physical dynamics, action space, reward definitions, and visual appearances.

Figure 4 presents quantitative comparisons, where CoWorld outperforms _Offline DV2_ and _DV2 Finetune_ by large margins. For the _best-source_ experiments, we manually select one source domain from Meta-World. For the _multi-source_ experiments, we jointly use all Meta-World tasks as the source domains. In contrast to prior findings, directly finetuning the source world model in this cross-environment setup, where there are more pronounced domain discrepancies, does not result in significant improvements in the final performance. In comparison, CoWorld more successfully

Figure 4: Quantitative results in domain transfer scenarios of Meta-World \(\) RoboDesk.

Figure 3: **Left: The value in each grid indicates the ratio of returns achieved by CoWorld compared to _Offline DV2_. Highlighted grids represent the top-performing source domain. Right: Returns on _Drawer Close_ (DC*) with different source domains, where the multi-source CoWorld (yellow line) is shown to automatically discover (_i.e._, _Door Close_) as the source domain and achieve comparable results with the top-performing single-source CoWorld (red line).**

addresses these challenges by leveraging domain-specific world models and RL agents, and explicitly aligning the state and reward spaces across domains. We also showcase the performance of multi-source CoWorld, which achieves comparable results to the _best-source_ model that exclusively uses our designated source domain.

### Cross-Dynamics Experiments on DMC

DMC is a widely explored benchmark for continuous control. We use the _Walker_ and _Cheetah_ as the base agents and make modifications to the environment to create a set of \(8\) distinct tasks, _i.e._, _Walker Walk_ (**WW**), _Walker Downhill_ (**WD**), _Walker Uphill_ (**W**U), _Walker Nofoot_ (**WN**), _Cheetah Run_ (**CR**), _Cheetah Downhill_ (**CD**), _Cheetah Uphill_ (**CU**), _Cheetah Nopaw_ (**CN**). Particularly, _Walker Nofoot_ is a task in which we cannot control the right foot of the _Walker_ agent. _Cheetah Nopaw_ is a task in which we cannot control the front paw of the _Cheetah_ agent.

We apply the proposed multi-source domain selection method to build the domain transfer settings shown in Table 3. It is worth noting that CoWorld outperforms the other compared models in \(5\) out of \(6\) DMC offline datasets, and achieves the second-best performance in the remaining task. On average, it outperforms _Offline DV2_ by \(169.6\%\) and outperforms _DrQ+BC_ by \(37.5\%\). Corresponding qualitative comparisons can be found in Appendix B.1.

### Further Analyses

Ablation studies.We conduct a series of ablation studies to validate the effectiveness of state space alignment (Stage A), reward alignment (Stage B), and min-max value constraint (Stage C). We show corresponding results on the offline _Push Green Button_ dataset from RoboDesk in Figure 5(a). The performance experiences a significant decline when we abandon each training stage in CoWorld.

Can CoWorld address value overestimation?We evaluate the values estimated by the critic network of CoWorld on the offline Meta-World datasets when the training process is finished. In Figure 5(b), we compute the cumulative value predictions throughout \(500\) steps. The _true value

Figure 5: **(a) Ablation studies on state alignment, reward alignment, and min-max value constraint. (b) The disparities between the estimated value by various models and the true value. Please see the text in Section 4.5 for the implementation of CoWorld w/o Max.**

   Model & WW \(\) WD & WW \(\) WU & WW \(\) WN & CR \(\) CD & CR \(\) CU & CR \(\) CN & Avg. \\  Offline DV2 & 435\(\)22 & 139\(\)4 & 214\(\)4 & 243\(\)7 & 3\(\)1 & 51\(\)4 & 181 \\ DrQ+BC & 291\(\)10 & 299\(\)15 & 318\(\)40 & 663\(\)15 & 202\(\)12 & 132\(\)33 & 355 \\ CQL & 46\(\)19 & 64\(\)32 & 29\(\)2 & 2\(\)1 & 52 \(\)57 & 111\(\)157 & 51 \\ CURL & 43\(\)5 & 21\(\)3 & 23\(\)3 & 26\(\)7 & 4\(\)2 & 11\(\)4 & 21 \\ LOMPO & 462\(\)87 & 260\(\)21 & **460\(\)9** & 395\(\)52 & 46\(\)19 & 120\(\)4 & 291 \\  DV2 Finetune & 379\(\)23 & 354\(\)29 & 407\(\)37 & 702\(\)41 & 208\(\)22 & 454\(\)82 & 417 \\ LOMPO Finetune & 209\(\)21 & 141\(\)27 & 212\(\)9 & 142\(\)29 & 17\(\)11 & 105\(\)12 & 137 \\ CoWorld & **629\(\)9** & **407\(\)141** & 426\(\)32 & **745\(\)28** & **225\(\)20** & **493\(\)10** & **488** \\   

Table 3: Mean rewards and standard deviations of \(10\) episodes in offline DMC over \(3\) seeds.

is determined by calculating the discounted sum of the actual rewards obtained by the actor in the same \(500\)-steps period. We observe that existing approaches, including _Offline DV2_ and _CQL_, often overestimate the value functions in the offline setup. The baseline model "_CoWorld w/o Max_" is a variant of CoWorld that incorporates a brute-force constraint on the critic loss. It reformulates Eq. (6) as \(_{t=1}^{H-1}(v_{}(_{t})-}(V_{t}))^{2}+  v_{}(_{t})\). As observed, this model tends to underestimate the true value function, which can potentially result in overly conservative policies as a consequence. In contrast, the values estimated by CoWorld are notably more accurate and more akin to the true values.

**Dependence of CoWorld to domain similarities.** We further investigate the dependence of CoWorld on domain similarity from the perspectives of different observation spaces and reward spaces. We first explore how CoWorld performs when we only have source domains with significantly distinct observation spaces from the target domain. As illustrated in Table 4, the agent receives low-dimensional state inputs in the source domain (Meta-World) and high-dimensional images in the target domain (RoboDesk). We can see that CoWorld outperforms _Offline DV2_ by \(13.3\%\) and \(34.0\%\) due to the ability to leverage low-dimensional source data effectively. Notably, the finetuning method (_DV2 Finetune_) is not applicable in this scenario. In Table 5, we also observe that CoWorld benefits from a source domain, even with a significantly different reward signal. Unlike previous experiments, we use a sparse reward function for the source Meta-World tasks. It is set to \(500\) only upon task completion and remains \(0\) before that. The experimental results demonstrate that although excessively sparse rewards can hinder the training process, CoWorld still achieves an average performance gain of \(6.6\%\) compared to _DV2 Finetune_ under the same setting.

**Comparison to jointly training one world model across domains.** Notably, CoWorld is implemented with separate world models for the source and target domains. Alternatively, we can employ a jointly trained world model across various domains for more efficient memory usage. In Table 6, we compare the results from the original CoWorld and "_Multi-Task DV2_". Multi-Task DV2 involves training DreamerV2 on both offline and online data with a joint world model and separate actor-critic models. CoWorld consistently performs better. Intuitively, using separate world models allows the source and target domains to have different physical dynamics, observation spaces, or reward formations, as the scenarios shown in Table 4 and Table 5.

**Hyperparameter sensitivity.** We conduct sensitivity analyses on Meta-World (\(DC BP\)). From Figure 6, we observe that when \(_{2}\) for the domain KL loss is too small, the state alignment between the source and target encoders becomes insufficient, hampering the transfer learning process. Conversely, if \(_{2}\) is too large, the target encoder becomes excessively influenced by the source encoder, resulting in a decline in performance. We also find that the target-informed reward factor \(k\) plays a crucial role in balancing the influence of source data and target reward information, which achieves a consistent

   Method & MW: _Button Press_\(\) RD: _Push Button_ & MW: _Window Close_\(\) RD: _Open Slide_ \\  Multi-Task DV2 & 342 \(\) 29 & 173 \(\) 22 \\ CoWorld & **428 \(\) 42** & **202 \(\) 19** \\   

Table 6: Comparison to jointly training **one world model** across domains (_Multi-Task DV2_).

   Method & MW: _Button Press_\(\) RD: _Push Button_ & MW: _Window Close_\(\) RD: _Open Slide_ \\  Offline DV2 & 347 \(\) 24 & 156 \(\) 46 \\ CoWorld & **393 \(\) 64** & **209 \(\) 43** \\   

Table 4: Experiments with significantly distinct observation spaces across domains. We use _low-dimensional_ state data as inputs for the RL agents in the source domain and _high-dimensional_ image observations in the target domain. **MW** represents Meta-World and **RD** stands for RoboDesk.

   Method & MW: _Button Press_\(\) RD: _Push Button_ & MW: _Window Close_\(\) RD: _Open Slide_ \\  DV2 Finetune & 314 \(\) 51 & 173 \(\) 39 \\ CoWorld & **335 \(\) 28** & **184 \(\) 32** \\   

Table 5: Experiments with significantly distinct reward formations across domains. We use _sparse_ rewards in the source domain while maintaining the _dense_ rewards in the target domain.

improvement over _DV2 Finetune_ (\(2456 661\)) in the range of \([0.1,0.7]\). Moreover, we discover that the hyperparameter \(\) for the target value constraint performs well within \(\), while an excessively larger \(\) may result in value over-conservatism in the target critic.

## 5 Related Work

Learning control policies from images is critical in real-world applications. Existing approaches can be grouped by the use of model-free [22; 41; 44; 48; 36] or model-based [15; 14; 16; 43; 35; 13; 28; 29; 61; 51] RL algorithms. In offline RL, agents leverage pre-collected offline data to optimize policies and encounter challenges associated with value overestimation . Previous methods mainly suggest taking actions that were previously present in the offline dataset or learning conservative value estimations [11; 21; 4; 55; 53; 40]. Recent approaches have introduced specific techniques to address the challenges associated with offline visual RL [27; 7; 23; 2; 39; 52; 43; 57; 5; 25]. Rafailov _et al._ proposed to handle high-dimensional observations with latent dynamics models and uncertainty quantification. Cho _et al._ proposed synthesizing the raw observation data to append the training buffer, aiming to mitigate the issue of overfitting. In a related study, Lu _et al._ established a competitive offline visual RL model based on DreamerV2 , so that we use it as a significant baseline of our approach.

Our work is also related to transfer RL, which is known as to utilize the knowledge learned in past tasks to facilitate learning in unseen tasks [64; 42; 58; 45; 59; 8; 49; 46; 12; 20; 38; 24; 33]. Most existing approaches related to offline dataset + simulator focus on the offline-to-online setup, where the policy is initially pretrained on the offline dataset and then finetuned and deployed on an interactive environment [33; 60; 56; 63]. These methods aim to bridge the gap between offline and online learning and facilitate fast adaptation of the model to the online environment. In contrast, we explore the online-to-offline setup, which provides a new remedy for the value over-estimation problem. Additionally, Niu _et al._ introduces a dynamics-aware hybrid offline-and-online framework to integrate offline datasets and online simulators for policy optimization. Unlike CoWorld, this method primarily focuses on low-dimensional MDPs and cannot be directly used in visual control tasks. In the context of visual RL, CtrlFormer  learns a transferable state representation via a sample-efficient vision Transformer. APV  executes action-free world model pretraining on source-domain videos and finetunes the model on downstream tasks. Choreographer  builds a model-based agent that exploits its world model to learn and adapt skills in imaginations, the learned skills are adapted to new domains using a meta-controller. VIP  presents a self-supervised, goal-conditioned value-function objective, which enables the use of unlabeled video data for model pertaining. Unlike previous methods, we handle offline visual RL using auxiliary simulators, mitigating the value overestimation issues with co-trained world models.

## 6 Conclusions and Limitations

In this paper, we proposed a transfer RL method named CoWorld, which mainly tackles the difficulty in representation learning and value estimation in offline visual RL. The key idea is to exploit accessible online environments to train an auxiliary RL agent to offer additional value assessment. To address the domain discrepancies and to improve the offline policy, we present specific technical contributions of cross-domain _state alignment_, _reward alignment_, and _min-max value constraint_. CoWorld demonstrates competitive results across three RL benchmarks. An unsolved problem of CoWorld is the increased computational complexity associated with the training phase in auxiliary domains (see B.7). It is valuable to improve the training efficiency in future research.

Figure 6: Sensitivity analysis of the hyperparameters on Meta-World (\(DC BP\)).