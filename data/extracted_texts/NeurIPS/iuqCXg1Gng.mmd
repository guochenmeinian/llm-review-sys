# Saddle-to-Saddle Dynamics

in Diagonal Linear Networks

 Scott Pesme

EPFL

scott.pesme@epfl.ch &Nicolas Flammarion

EPFL

nicolas.flammarion@epfl.ch

###### Abstract

In this paper we fully describe the trajectory of gradient flow over \(2\)-layer diagonal linear networks for the regression setting in the limit of vanishing initialisation. We show that the limiting flow successively jumps from a saddle of the training loss to another until reaching the minimum \(_{1}\)-norm solution. We explicitly characterise the visited saddles as well as the jump times through a recursive algorithm reminiscent of the LARS algorithm used for computing the Lasso path. Starting from the zero vector, coordinates are successively activated until the minimum \(_{1}\)-norm solution is recovered, revealing an incremental learning. Our proof leverages a convenient arc-length time-reparametrisation which enables to keep track of the transitions between the jumps. Our analysis requires negligible assumptions on the data, applies to both under and overparametrised settings and covers complex cases where there is no monotonicity of the number of active coordinates. We provide numerical experiments to support our findings.

## 1 Introduction

Strikingly simple algorithms such as gradient descent are driving forces for deep learning and have led to remarkable empirical results. Nonetheless, understanding the performances of such methods remains a challenging and exciting mystery: (i) their global convergence on highly non-convex losses is far from being trivial and (ii) the fact that they lead to solutions which generalise well  is still not fully understood.

To explain this second point, a major line of work has focused on the concept of implicit regularisation: amongst the infinite space of zero-loss solutions, the optimisation process must be implicitly biased towards solutions which have good generalisation properties for the considered real-world prediction tasks. Many papers have therefore shown that gradient methods have the fortunate property of asymptotically leading to solutions which have a well-behaving structure .

Aside from these results which mostly focus on characterising the asymptotic solution, a slightly different point of view has been to try to describe the full trajectory. Indeed it has been experimentally observed that gradient methods with small initialisations have the property of learning models of increasing complexity across the training of neural networks . This behaviour is usually referred to as _incremental learning_ or as a _saddle-to-saddle process_ and describes learning curves which are piecewise constant: the training process makes very little progress for some time, followed by a sharp transition where a new "feature" is suddenly learned. In terms of optimisation trajectory, this corresponds to the iterates "jumping" from a saddle of the training loss to another.

Several settings exhibiting such dynamics for small initialisation have been considered: matrix and tensor factorisation , simplified versions of diagonal linear networks , linear networks , \(2\)-layer neural networks with orthogonal inputs , learning leap functions with \(2\)-layer neural networks  and matrix sensing . However, all these results requirerestrictive assumptions on the data or only characterise the first jump. Obtaining a complete picture of the saddle-to-saddle process by describing all the visited saddles and jump times is mathematically challenging and still missing. We intend to fill this gap by considering diagonal linear networks which are simplified neural networks that have received significant attention lately [50; 48; 25; 43; 20] as they are ideal proxy models for gaining a deeper understanding of complex phenomenons such as saddle-to-saddle dynamics.

### Informal statement of the main result

In this paper, we provide a full description of the trajectory of gradient flow over \(2\)-layer diagonal linear networks in the limit of vanishing initialisation. The main result is informally presented here.

**Theorem 1** (Main result, informal).: _In the regression setting and in the limit of vanishing initialisation, the trajectory of gradient flow over a \(2\)-layer diagonal linear network converges towards a limiting process which is piecewise constant: the iterates successively jump from a saddle of the training loss to another, each visited saddle and jump time can recursively be computed through an algorithm (Algorithm 1) reminiscent of the LARS algorithm for the Lasso._

The incremental learning stems from the particular structure of the saddles as they correspond to minimisers of the training loss with a constraint on the set of non-zero coordinates. The saddles therefore correspond to sparse vectors which partially fit the dataset. For simple datasets, a consequence of our main result is that **the limiting trajectory successively starts from the zero vector and successively learns the support of the sparse ground truth vector until reaching it. However, we make minimal assumptions on the data and our analysis also holds for complex datasets**. In that case, the successive active sets are not necessarily increasing in size and coordinates can deactivate as well as activate until reaching the minimum \(_{1}\)-norm solution (see Figure 1 (middle) for an example of a deactivating coordinate). The regression setting and the diagonal network architecture are introduced in Section 2. Section 3 provides an intuitive construction of the limiting saddle-to-saddle dynamics and presents the algorithm that characterises it. Our main result regarding the convergence of the iterates towards this process is presented in Section 4 and further discussion is provided in Section 5.

## 2 Problem setup and leveraging the mirror structure

### Setup

**Linear regression.** We study a linear regression problem with inputs \((x_{1},,x_{n})(^{d})^{n}\) and outputs \((y_{1},,y_{n})^{n}\). We consider the typical quadratic loss:

\[L()=_{i=1}^{n}(,x_{i}-y_{i})^{2}\,.\] (1)

Figure 1: Gradient flow \((_{t}^{})_{t}\) with small initialisation scale \(\) over a \(2\)-layer diagonal linear network (for the precise experimental setting, see Appendix A). _Left:_ Training loss across time, the learning is piecewise constant. _Middle:_ The magnitudes of the coordinates are plotted across time: the process is piecewise constant. _Right:_ In the \(^{3}\) space in which the iterates evolve (the remaining coordinates stay at \(0\)), the iterates jump from a saddle of the training loss to another. The jumping times \(t_{i}\) as well as the visited saddles \(_{i}\) are entirely predicted by our theory.

We make no assumption on the number of samples \(n\) nor the dimension \(d\). The only assumption we make on the data throughout the paper is that the inputs \((x_{1},,x_{n})\) are in _general position_. In order to state this assumption, let \(X^{n d}\) be the feature matrix whose \(i^{th}\) row is \(x_{i}\) and let \(_{j}^{n}\) be its \(j^{th}\) column for \(j[d]\).

**Assumption 1** (General position).: _For any \(k(n,d)\) and arbitrary signs \(_{1},,_{k}\{-1,1\}\), the affine span of any \(k\) points \(_{1}_{j_{1}},,_{k}_{j_{k}}\) does not contain any element of the set \(\{_{j},j j_{1},,j_{k}\}\)._

This assumption is slightly technical but is standard in the Lasso literature . Note that it is not restrictive as it is almost surely satisfied when the data is drawn from a continuous probability distribution [47, Lemma 4]. Letting \(=*{arg\,min}_{}L()\) denote the affine space of solutions, Assumption 1 ensures that the minimisation problem \(_{^{*}}^{*}_{1}\) has a unique minimiser which we denote \(^{*}_{_{1}}\) and which corresponds to the minimum \(_{1}\)-norm solution.

**2-layer diagonal linear network.** In an effort to understand the training dynamics of neural networks, we consider a \(2\)-layer diagonal linear network which corresponds to writing the regression vector \(\) as

\[_{w}=u v\;\;\;\;w=(u,v)^{2d}\,.\] (2)

This parametrisation can be interpreted as a simple neural network \(x u,(*{diag}(v)x)\) where \(u\) are the output weights, the diagonal matrix \(*{diag}(v)\) represents the inner weights, and the activation \(\) is the identity function. We refer to \(w=(u,v)^{2d}\) as the _weights_ and to \( u v^{d}\) as the _prediction parameter_. With the parametrisation (2), the loss function \(F\) over the parameters \(w=(u,v)^{2d}\) is defined as:

\[F(w) L(u v)=_{i=1}^{n}( u v,x_{i} -y_{i})^{2}\,.\] (3)

Though this parametrisation is simple, the associated optimisation problem is non-convex and highly non-trivial training dynamics already occur. The critical points of the function \(F\) exhibit a very particular structure, as highlighted in the following proposition proven in Appendix B.

**Proposition 1**.: _All the critical points \(w_{c}\) of \(F\) which are not global minima, i.e., \( F(w_{c})=\) and \(F(w_{c})>_{w}F(w)\), are necessarily saddle points (i.e., not local extrema). They map to parameters \(_{c}=u_{c} v_{c}\) which satisfy \(|_{c}| L(_{c})=\) and:_

\[_{c}*{arg\,min}_{[i]=0}*{arg\,min}_{ [i]=0}L()\] (4)

_where \(*{supp}(_{c})=\{i[d],_{c}[i] 0\}\) corresponds to the support of \(_{c}\)._

The optimisation problem in Eq. (4) states that the saddle points of the train loss \(F\) correspond to **sparse vectors that minimise the loss function \(L\) over its non-zero coordinates**. This property already shows that the saddle points possess interesting properties from a learning perspective. In the following we loosely use the term of'saddle' to refer to points \(_{c}^{d}\) solution of Eq. (4) **that are not saddles of the convex loss function \(L\)**. We adopt this terminology because they correspond to points \(w_{c}^{2d}\) that are indeed saddles of the non-convex loss \(F\).

**Gradient Flow and necessity of "accelerating" time.** We minimise the loss \(F\) using gradient flow:

\[w_{t}=- F(w_{t})t\,,\] (5)

initialised at \(u_{0}=^{d}_{>0}\) with \(>0\), and \(v_{0}=^{d}\). This initialisation results in \(_{0}=^{d}\) independently of the chosen weight initialisation scale \(\). We denote \(^{}_{t} u^{}_{t} v^{}_{t}\) the prediction iterates generated from the gradient flow to highlight its dependency on the initialisation scale \(\)1. The origin \(^{2d}\) is a critical point of the function \(F\) and taking the initialisation \( 0\) therefore arbitrarily slows down the dynamics. In fact, it can be easily shown for any fixed time \(t\), that \((u^{}_{t},v^{}_{t})\) as \( 0\), indicating that the iterates are stuck at the origin. Therefore if we restrict ourselves to a finite time analysis, there is no hope of exhibiting the observed saddle-to-saddle behaviour. To do so, we must find an appropriate bijection \(_{}\) in \(_{ 0}\) which "accelerates" time (_i.e._\(_{}(t)+\) for all \(t\)) and consider the accelerated iterates \(^{}_{_{}(t)}\) which can escape the saddles. Finding this bijection becomes very natural once the mirror structure is unveiled.

### Leveraging the mirror flow structure

While the iterates \((w_{t}^{})_{t}\) follow a gradient flow on the non-convex loss \(F\), it is shown in  that the iterates \(_{t}^{}\) follow a mirror flow on the convex loss \(L\) with potential \(_{}\) and initialisation \(_{t=0}^{}=\):

\[_{}(_{t}^{})=- L(_{t}^{} )t,\] (6)

where \(_{}\) is the hyperbolic entropy function  defined as:

\[_{}()=_{i=1}^{d}_{i} (}{^{2}})-^{2}+^{4} }+^{2}.\] (7)

Unveiling the mirror flow structure enables to leverage convex optimisation tools to prove convergence of the iterates to a global minimiser \(_{}^{}\) as well as a simple proof of the implicit regularisation problem it solves. As shown by Woodworth et al. , in the overparametrised setting where \(d>n\) and where there exists an infinite number of global minima, the limit \(_{}^{}\) is the solution of the problem:

\[_{}^{}=*{arg\,min}_{y_{i}= x _{i},, i}\ _{}().\] (8)

Furthermore, a simple function analysis shows that \(_{}\) behaves as a rescaled \(_{1}\)-norm as \(\) goes to \(0\), meaning that the recovered solution \(_{}^{}\) converges to the minimum \(_{1}\)-norm solution \(_{_{1}}^{}=*{arg\,min}_{y_{i}= x_{i}, }\|\|_{1}\) as \(\) goes to \(0\) (see  for a precise rate). To bring to light the saddle-to-saddle dynamics which occurs as we take the initialisation to \(0\), we make substantial use of the nice mirror structure from Eq. (6).

**Appropriate time rescaling.** To understand the limiting dynamics of \(_{t}^{}\), it is natural to consider the limit \( 0\) in Eq. (6). However, the potential \(_{}\) is such that \(_{}()(1/)\|\|_{1}\) for small \(\) and therefore degenerates as \( 0\). Similarly, for \(\), \(\|_{}()\|\) as \( 0\). The formulation from Eq. (6) is thus not appropriate to take the limit \( 0\). We can nonetheless obtain a meaningful limit by considering the opportune time acceleration \(_{}(t)=(1/) t\) and looking at the accelerated iterates

\[_{t}^{}_{t_{}(t)}^{ }=_{(1/)t}^{}.\] (9)

Indeed, a simple chain rule leads to the "accelerated mirror flow": \(_{}(_{t}^{})=-)} L(_{t}^{})t\). The accelerated iterates \((_{t}^{})_{t}\) follow a mirror descent with a rescaled potential:

\[_{}(_{t}^{} )=- L(_{t}^{})t, _{}_{},\] (10)

with \(_{t=0}=\) and where \(_{}\) is defined Eq. (7). Our choice of time acceleration ensures that the rescaled potential \(_{}\) is non-degenerate as the initialisation goes to \(0\) since \(_{}()\|\|_{1}\).

## 3 Intuitive construction of the limiting flow and saddle-to-saddle algorithm

In this section, we aim to give a comprehensible construction of the limiting flow. We therefore choose to provide intuition over pure rigor, and defer the full and rigorous proof to the Appendix E. The technical crux of our analysis is to demonstrate the existence of a piecewise constant limiting process towards which the iterates \(^{}\) converge to. The convergence result is deferred to the following Section 4. **In this section we assume this convergence and refer to this piecewise constant limiting process as \((_{t}^{})_{t}\)**. Our goal is then to determine the jump times \((t_{1},,t_{p})\) as well as the saddles \((_{0},,_{p})\) which fully define this process.

To do so, it is natural to examine the limiting equation obtained when taking the limit \( 0\) in Eq. (10). We first turn to its integral form which writes:

\[-_{0}^{t} L(_{s}^{})s= _{}(_{t}^{}).\] (11)Provided the convergence of the flow \(^{}\) towards \(^{}\), the left hand side of the previous equation converges to \(-_{0}^{t} L(^{}_{s})s\). For the right hand side, recall that \(_{}()}{{}}\| \|_{1}\), it is therefore natural to expect the right hand side of Eq. (11) to converge towards an element of \(\|^{}_{t}\|_{1}\), where we recall the definition of the subderivative of the \(_{1}\)-norm as:

\[\|\|_{1}=\ \{1\}\ \ \ \ \ >0,\ \ \ \ \{-1\}\ \ \ \ <0,\ \ \ \ [-1,1]\ \ \ \ =0.\]

The arising key equation which must satisfy the limiting process \(^{}\) is then, for all \(t 0\):

\[-_{0}^{t} L(^{}_{s})s \|^{}_{t}\|_{1}.\] (12)

We show that **this equation uniquely determines the piecewise constant process \(^{}\)** by imposing the number of jumps \(p\), the jump times as well as the saddles which are visited between the jumps. Indeed the relation described in Eq. (12) provides \(4\) restrictive properties that enable to construct \(^{}\). To state them, let \(s_{t}=-_{0}^{t} L(^{}_{s})s\) and notice that it is continuous and piecewise linear since \(^{}\) is piecewise constant. For each coordinate \(i[d]\), it holds that:

* \(s_{t}[i]=1^{}_{t}[i] 0\) and \(s_{t}[i]=-1^{}_{t}[i] 0\)

* \(^{}_{t}[i]>0 s_{t}[i]=1\) and \(^{}_{t}[i]<0 s_{t}[i]=-1\)

To understand how these conditions lead to the algorithm which determines the jump times and the visited saddles, we present a \(2\)-dimensional example for which we can walk through each step. The general case then naturally follows from this simple example.

### Construction of the saddle-to-saddle algorithm with an illustrative \(2d\) example.

Let us consider \(n=d=2\) and data matrix \(X^{2 2}\) such that \(X^{}X=((1,0.2),(0.2,-0.2))\). We consider \(^{}=(-0.2,2)^{2}\) and outputs \(y=X^{}\). This setting is such that the loss \(L\) has \(^{}\) as its unique minimum and \(L(^{})=0\). Furthermore the non-convex loss \(F\) has \(3\) saddles which map to: \(_{c,0}(0,0)=*{arg\,min}_{_{i}=0, i}L()\), \(_{c,1}(0.2,0)=*{arg\,min}_{=0}L()\) and \(_{c,2}(0,1.6)=*{arg\,min}_{=0}L()\). The loss function \(L\) is sketched in Figure 2 (_Left_). Notice that by the definition of \(_{c,1}\) and \(_{c,2}\), the gradients of the loss at these points are orthogonal to the axis they belong to. When running gradient flow with a small initialisation over our diagonal linear network, we obtain the plots illustrated Figure 2 (_Middle and Right_). We observe three jumps: the iterates jump from the saddle at the origin to \(_{c,1}\) at time \(t_{1}\), then to \(_{c,2}\) at time \(t_{2}\) and finally to the global minimum \(^{}\)at time \(t_{3}\).

Let us show how Eq. (12) enables us to theoretically recover this trajectory. A simple observation which we will use several times below is that for any \(t^{}>t\) such that \(^{}\) is constant equal to \(\) over the time interval \((t,t^{})\), the definition of \(s\) enables to write that \(s_{t^{}}=s_{t}-(t^{}-t) L()\).

Figure 2: _Left_: Sketch of the \(2d\) loss. _Middle and right_: Outputs of gradient flow with small initialisation scale: the iterates are piecewise constant and \(s_{t}\) is piecewise linear across time. We refer to the main text for further details.

Zeroth saddle:The iterates are at the saddle at the origin: \(_{t}^{c}=_{0}_{c,0}\) and therefore \(s_{t}=-t L(_{0})\). Our key equation Eq. (12) is verified since \(s_{t}=-t L(_{0})\|_{0}\|_{1}=[-1,1]^{d}\). However the iterates cannot stay at the origin after time \(t_{1} 1/\| L(_{0})\|_{}\) which corresponds to the time at which the first coordinate of \(s_{t}\) hits \(+1\): \(s_{t_{1}}=1\). If the iterates stayed at the origin after \(t_{1}\), 1 for \(i=1\) would be violated. The iterates must hence jump.

First saddle:The iterates can only jump to a point different from the origin which maintains Eq. (12) valid. We denote this point as \(_{1}\). Notice that:

* \(s_{t_{1}}=-t_{1} L(_{0})(-1,1)\) and since \(s_{t}\) is continuous, we must have \(_{1}=0\) (K3)
* \(s_{t_{1}}=1\) and hence for \(t t_{1}\), \(s_{t}=1-(t-t_{1}) L(_{1})\). We cannot have \( L(_{1})<0\) (K1), and neither \( L(_{1})>0\) since otherwise \(s_{t}(-1,1)\) and \(_{1}=\) (K3)

The two conditions \(_{1}=0\) and \( L(_{1})=0\)**uniquely defines \(_{1}\) as equal to \(_{c,1}\)**. We now want to know if and when the iterates jump again. We saw that \(s_{t}\) remains at the value \(+1\). However since \(_{1}\) is not a global minimum, \( L(_{1}) 0\) and \(s_{t}\) hits \(+1\) at time \(t_{2}\) defined such that \(-(t_{1} L(_{0})+(t_{2}-t_{1}) L(_{1}))=1\). The iterates must jump otherwise 1 would break.

The iterates cannot jump to \(^{}\) yet:As the second coordinate of the iterates can activate, one could expect the iterates to be able to jump to the global minimum. However note that \(s_{t}\) is a continuous function and that \(s_{t_{2}}\) is equal to the vector \((1,1)\). If the iterates jumped to the global minimum, then the first coordinate of the iterates would change sign from \(+0.2\) to \(-0.2\). Due to 4 this would lead \(s_{t}\) jumping from \(+1\) to \(-1\), violating its continuity.

Second saddle:We denote as \(_{2}\) the point to which the iterates jump. \(s_{t_{2}}\) is now equal to the vector \((1,1)\) and therefore _(i)_\(_{2} 0\) (coordinate-wise) from 2 and 3 and the continuity of \(s\). Since \(s_{t}=s_{t_{2}}-(t-t_{2}) L(_{2})\), we must also have: _(ii)_\( L(_{2}) 0\) from 1 for \(i\{1,2\}\), if \(_{2}[i] 0\) then \( L(_{2})[i]=0\) from 4. The three conditions _(i)_, _(ii)_ and _(iii)_ precisely correspond to the optimality conditions of the following problem:

\[*{arg\,min}_{ 0, 0}L().\]

The unique minimiser of this problem is \(_{c,2}\), hence \(_{2}=_{c,2}\), which means that the first coordinate deactivates. Similar to before, 1 is valid until the time \(t_{3}\) at which the first coordinate of \(s_{t}=s_{t_{2}}-(t-t_{2}) L(_{2})\) reaches \(-1\) due to the fact that \( L(_{2})>0\).

Global minimum:We follow the exact same reasoning as for the second saddle. We now have \(s_{t_{3}}\) equal to the vector \((-1,1)\) and the iterates must jump to a point \(_{3}\) such that _(i)_\(_{3} 0\), \(_{3} 0\) (K2 and K3), _(ii)_\( L(_{3}) 0\), \( L(_{3}) 0\) (K1), _(iii)_ for \(i\{1,2\}\), if \(_{3}[i] 0\) then \( L(_{3})[i]=0\) (K4). Again, these are the optimality conditions of the following problem:

\[*{arg\,min}_{ 0, 0}L().\]

\(^{}\) is the unique minimiser of this problem and \(_{3}=^{}\). For \(t t_{3}\) we have \(s_{t}=s_{t_{3}}\) and Eq. (12) is satisfied for all following times: the iterates do not have to move anymore.

### Presentation of the full saddle-to-saddle algorithm

We can now provide the full algorithm (Algorithm 1) which computes the jump times \((t_{1},,t_{p})\) and saddles \((_{0}=,_{1},,_{p})\) as the values and vectors such that the associated piecewise constant process satisfies Eq. (12) for all \(t\). This algorithm therefore defines our limiting process \(^{}\).

Algorithm 1 in words.The algorithm is a concise representation of the steps we followed in the previous section to construct \(^{}\). We explain each step in words below. Starting from \(k=0\), assume we enter the loop number \(k\) at the saddle \(_{k}\) computed in the previous loop:

* The set \(_{k}\) contains the set of coordinates "which are unstable": by having a non-zero derivative, the loss could be decreased by moving along each one of these coordinates and one of these coordinates will have to activate.

* The time gap \(_{k}\) corresponds to the time spent at the saddle \(_{k}\). It is computed as being the elapsed time just before (1) breaks if the coordinates do not jump.
* We update \(t_{k+1}=t_{k}+_{k}\) and \(s_{k+1}=s_{k}-_{k} L(_{k})\): \(t_{k+1}\) corresponds to the time at which the iterates leave the saddle \(_{k}\) and \(s_{k+1}\) constrains the signs of the next saddle \(_{k+1}\)
* The solution \(_{k+1}\) of the constrained minimisation problem is the saddle to which the flow jumps to at time \(t_{k+1}\). The optimality conditions of this problem are such that Eq. (12) is maintained for \(t t_{k+1}\).

Various comments on Algorithm 1.First we point out that any solution \(_{c}\) of the constrained minimisation problem which appears in Algorithm 1 also satisfies \(_{c}=_{|^{d}\ \ i( _{c})}\ L()\) as in Eq. (4): the algorithm hence indeed outputs saddles as expected. Up until now we have never checked whether the algorithm's constrained minimisation problem has a unique minimum. This is crucial otherwise the assignment step would be ill-defined. Showing the uniqueness is non-trivial and is guaranteed thanks to the general position Assumption 1 on the data (see Proposition 7 in Appendix D.1). In this same proposition, we also show that the algorithm terminates in at most \((2^{d},_{k=0}^{n})\) steps, that the loss strictly decreases at each step and that the final output \(_{p}\) is the minimum \(_{1}\)-norm solution. These last two properties are expected given the fact that the algorithm arises as being the limit process of \(^{}\) which follows the mirror flow Eq. (10).

``` Initialise:\((t,,s)(0,,)\); while\( L()\)do \(\{j[d], L()(j) 0\}\) \(\{>0\ \  i,\ s(i)- L()(i)= 1\}\) \((t,\ s)(t+,\ s- L())\) \(\ L()\ \ \ \ \{^{d}\ \ _{i} 0\ \ s(i)=+1\\ _{i} 0\ \ s(i)=-1\\ _{i}=0\ \ s(i)(-1,1)\}\)  end while Output: Successive values of \(\) and \(t\) ```

**Algorithm 1**Successive saddles and jump times of \(_{ 0}^{}\)

**Links with the LARS algorithm for the Lasso.** Recall that the Lasso problem [46; 15] is formulated as:

\[_{}^{}=*{arg\,min}_{^{d}}\ L()+\|\|_{1}.\] (13)

The optimality condition of Eq. (13) writes \(- L(_{}^{})\|_{}^{}\|_ {1}\). Now notice the similarity with Eq. (12): the two would be equivalent with \(=1/t\) if the integration on the left hand side of Eq. (12) did not average over the whole trajectory but only on the final iterate, in which case \(-_{0}^{t} L(_{t}^{})s=-t L( _{t}^{})\). Though the difference is small, the trajectories of our limiting trajectory \(^{}\) and the lasso path \((_{}^{})_{}\) are quite different: one has jumps, whereas the other is continuous. Nonetheless, the construction of Algorithm 1 shares many similarities with that of the Least Angle Regression (LARS) algorithm  (originally named the Homotopy algorithm ) which is used to compute the Lasso path. A notable difference however is the fact that each step of our algorithm depends on the whole trajectory through the vector \(s\), whereas the LARS algorithm can be started from any point on the path.

### Outputs of the algorithm under a RIP and gap assumption on the data.

Unlike previous results on incremental learning, complex behaviours can occur when the feature matrix is ill designed: several coordinates can activate and deactivate at the same time (see Appendix A for various cases). However, if the feature matrix satisfies the \(2r\)-restricted isometry property (RIP)  and there exists an \(r\)-sparse solution \(^{}\), the visited saddles can be easily approximated using Algorithm 1. We provide the precise characterisation below.

**Sparse regression with RIP and gap assumption.** _(RIP) Assume that there exists an \(r\)-sparse vector \(^{}\) such that \(y_{i}= x_{i},^{}\). Furthermore we assume that the feature matrix \(X^{n,d}\) satisfies the \(2r\)-restricted isometry property with constant \(<-1<1/2\): i.e. for all submatrix \(X_{s}\) where we extract any \(s 2r\) columns of \(X\), the matrix \(X_{s}^{}X_{s}/n\) of size \(s s\) has all its eigenvalues in the interval \([1-,1+]\)._ (_Gap assumption) Furthermore we assume that the \(r\)-sparse vector \(^{}\) has coordinates which have a "sufficient gap'_. _W.l.o.g we write \(^{}=(_{1}^{},,_{r}^{},0,,0)\) with \(|_{1}^{}||_{r}^{}|>0\) and we define \(_{i[r]}(|_{i}^{}|-|_{i+1}^{}|) 0\) which corresponds to the smallest gap between the entries of \(|^{}|\). We assume that \(5\|^{}\|_{2}</2\) and we let \( 5\)._

A classic result from compressed sensing (see Candes (13, Theorem 1.2)) is that the \(2r\)-restricted isometry property with constant \(-1\) ensures that the minimum \(_{0}\)-minimisation problem has a unique \(r\)-sparse solution which is \(^{}\). This means that Algorithm 1 will have \(^{}\) as final output and the following proposition shows that we can precisely characterise each of its outputs when the data satisfies the previous assumptions.

**Proposition 2**.: _Under the restricted isometry property and the gap assumption stated right above, Algorithm 1 terminates in \(r\)-loops and outputs:_

\[_{1} =(_{1},0,,0) _{1}[_{1}^{}-\| ^{}\|,_{2}^{}+\|^{}\|]\] \[_{2} =(_{2},_{2},0,,0) \{_{2}[_{1 }^{}-\|^{}\|,_{1}^{}+\|^{ }\|]\\ _{2}[_{2}^{}-\|^{}\|,_{2}^{ }+\|^{}\|].\] \[\] \[_{r-1} =(_{r-1},,_{r-1}[r-1],0,,0) \ \ _{r-1}[i][_{i}^{}-\|^{}\|,_{i}^{ }+\|^{}\|\;]\] \[_{r} =^{}=(_{1}^{},,_{r}^{},0, ,0),\]

_at times \(t_{1},,t_{r}\) such that \(t_{i}[^{}|+\|^{}\|},\, ^{}|-\|^{}\|}]\) and where \(\|\|\) denotes the \(_{2}\) norm._

Informally, this means that the algorithm terminates in exactly \(r\) loops and outputs jump times and saddles roughly equal to \(t_{i}=1/|_{i}^{}|\) and \(_{i}=(_{1}^{},,_{i}^{},0,,0)\). Therefore, in simple settings, the support of the sparse vector is learnt a coordinate at a time, without any deactivations. We refer to Appendix D.2 for the proof.

## 4 Convergence of the iterates towards the process defined by Algorithm 1

We are now fully equipped to state our main result which formalises the convergence of the accelerated iterates towards the limiting process \(^{}\) which we built in the previous section.

**Theorem 2**.: _Let the saddles \((_{0}=,_{1},,_{p-1},_{p}=_{t_{1}}^{ })\) and jump times \((t_{0}=0,t_{1},,t_{p})\) be the outputs of Algorithm 1 and let \((_{t}^{})_{t}\) be the piecewise constant process defined as follows:_

\[_{t}^{}=_{k}t(t_{k},t_{k+1})0 k p,\ \ t_{p+1}=+.\]

_The accelerated flow \((_{t}^{})_{t}\) defined in Eq. (9) uniformly converges towards the limiting process \((_{t}^{})_{t}\) on any compact subset of \(_{ 0}\{t_{1},,t_{p}\}\)._

**Convergence result.** We recall that from a technical point of view, showing the existence of a limiting process \(_{ 0}^{}\) is the toughest part. Theorem 2 provides this existence as well as the uniform convergence of the accelerated iterates towards \(^{}\) over all closed intervals of \(\) which do not contain the jump times. We highlight that this is the strongest type of convergence we could expect and a uniform convergence over all intervals of the form \([0,T]\) is impossible given that the limiting process \(^{}\) is discontinuous. In Proposition 3, we give an even stronger result by showing a graph convergence of the iterates which takes into account the path followed between the jumps. We also point out that we can easily show the same type of convergence for the accelerated weights \(_{t}^{} w_{^{}(t)}^{}\). Indeed, using the bijective mapping which links the weights \(w_{t}\) and the predictors \(_{t}\) (see Lemma 1 in Appendix C), we immediately get that the accelerated weights \((^{},^{})\) uniformly converge towards the limiting process \((^{}|},(^{}) ^{}|})\) on any compact subset of \(_{ 0}\{t_{1},,t_{p}\}\).

**Estimates for the non-accelerated iterates \(^{}_{t}\).** We point out that our result provides no speed of convergence of \(^{}\) towards \(^{}\). We believe that a non-asymptotic result is challenging and leave it as future work. Note that we experimentally notice that the convergence rate quickly degrades after each saddle. Nonetheless, we can still write for the non-accelerated iterates that \(^{}_{t}=^{}_{t/(1/)}^{ }_{t/(1/)}\) as \( 0\). Hence, for \(\) small enough the iterates \(^{}_{t}\) are roughly equal to \(0\) until time \(t_{1}(1/)\) and the minimum \(_{1}\)-norm interpolator is reached at time \(t_{p}(1/)\). **Such a precise estimate of the global convergence time is rather remarkable** and goes beyond classical Lyapunov analyses which only leads to \(L(^{}_{t})(1/)/t\) (see Proposition 4 in Appendix C).

**Natural extensions of our setting.** More general initialisations can easily be dealt with. For instance, initialisations of the form \(u_{t=0}=}^{d}\) lead to the exact same result as it is shown in  (Discussion after Theorem 1) that the associated mirror still converges to the \(_{1}\)-norm. Initialisations of the form \([u_{t=0}]_{i}=^{k_{i}}\), where \(k_{i}>0\), lead to the associated potential converging towards a weighted \(_{1}\)-norm and one should modify Algorithm 1 by accordingly weighting \( L()\) in the algorithm. Also, deeper linear architectures of the form \(_{w}=w^{D}_{+}-w^{D}_{-}\) as in  do not change our result as the associated mirror still converges towards the \(_{1}\)-norm. Though we only consider the square loss in the paper, we believe that all our results should hold for any loss of the type \(L()=_{i=1}^{n}(y_{i}, x_{i},)\) where for all \(y\), \((y,)\) is strictly convex with a unique minimiser at \(y\). In fact, the only property which cannot directly be adapted from our results is showing the uniform boundedness of the iterates (see discussion before Proposition 5 in Appendix C).

High level sketch of proof of \(^{}^{}\) which leverages an arc-length parametrisation

In this section, we give the high level ideas concerning the proof of the convergence \(^{}^{}\) given in Theorem 2. A full and detailed proof can be found in Appendix E. The main difficulty stems from the non-continuity of the limit process \(^{}\). To circumvent this difficulty, a clever trick which we borrow to [18; 36] is to "slow-down" time when the jumps occur by considering **an arc-length parametrisation of the path**. We consider the \(_{ 0}\) arclength bijection \(^{}\) and leverage it to define the 'appropriately slowed down' iterates \(^{}_{}\) as:

\[^{}_{}=^{}_{^{}() }^{}_{}=(^{})^{-1}()\ \ \ \ ^{}(t)=t+_{0}^{t}}^{}_{s} s.\]

This time reparametrisation has the fortunate but crucial property of leading to \(}^{}()+}^{}_{} =1\) by a simple chain rule, which means that the speed of \((^{}_{})_{}\)**is uniformly upperbounded by \(1\) independently of \(\)**. This behaviour is in stark contrast with the process \((^{}_{t})_{t}\) which has a speed which explodes at the jumps. This change of time now allows us to use Arzela-Ascoli's theorem to extract a subsequence which uniformly converges to a limiting process which we denote \(\). Importantly, \(\) enables to keep track of the path followed between the jumps as we show that its trajectory has two regimes:

**Saddles:**\(_{}=_{k}\)**Connections:**\(}_{}=-_{} L(_{})}{_{} L(_{ })}\).

The process \(\) is illustrated on the right: the red curves correspond to the paths which the iterates follow during the jumps. These paths are called _heteroclinic orbits_ in the dynamical systems literature [31; 3]. To prove Theorem 2, we can map back the convergence of \(^{}\) to show that of \(^{}\). Moreover from the convergence \(^{}\) we get a more complete picture of the limiting dynamics of \(^{}\) as it naturally implies the convergence of the graph of the iterates \((^{}_{t})_{t}\) converges towards that of \((_{})_{}\). The graph convergence result is formalised in this last proposition.

**Proposition 3**.: _For all \(T>t_{p}\), the graph of the iterates \((^{}_{t})_{t T}\) converges to that of \((_{})_{}:\)_

\[(\{^{}_{t}\}_{t T},\{_{}\}_{  0})\ \ 0\]Further discussion and conclusion

**Link between incremental learning and saddle-to-saddle dynamics.** The incremental learning phenomenon and the saddle-to-saddle process are often complementary facets of the same idea and refer to the same phenomenon. Indeed for gradient flows \(w_{t}=- F(w_{t})t\), fixed points of the dynamics correspond to critical points of the loss. Stages with little progress in learning and minimal movement of the iterates necessarily correspond to the iterates being in the vicinity of a critical point of the loss. It turns out that in many settings (linear networks , matrix sensing [8; 41]), critical points are necessarily saddle points of the loss (if not global minima) and that they have a very particular structure (high sparsity, low rank, etc.). We finally note that an alternative approach to realising saddle-to-saddle dynamics is through the perturbation of the gradient flow by a vanishing noise as studied in .

**Characterisation of the visited saddles.** A common belief is that the saddle-to-saddle trajectory can be found by successively computing the direction of most negative curvature of the loss (i.e. the eigenvector corresponding to the most negative eigenvalue) and following this direction until reaching the next saddle . However this statement cannot be accurate as it is inconsistent with our algorithm in our setting. In fact, it can be shown that this algorithm would match the orthogonal matching pursuit (OMP) algorithm [42; 17] which does not necessarily lead to the minimum \(_{1}\)-norm interpolator. In , which is the closest to our work and the first to prove convergence of the iterates towards a piece-wise constant process, the successive saddles are entirely characterised and connected to the Lasso regularisation path in the underparameterised setting. Recently,  extended the diagonal linear network setting to diagonal parametrisations of the form \(f_{u v}\), but at the cost of stronger assumptions on the trajectory.

**Adaptive Inverse Scale Space Method.** Following the submission of our paper, we were informed that Algorithm 1 had already been proposed and analysed in the compressed sensing literature. Indeed it exactly corresponds to the Adaptive Inverse Scale Space Method (aISS) proposed in . The motivations behind its study are extremely different from ours and originate from the study of Bregman iteration [12; 40; 52] which is an efficient method for solving \(_{1}\) related minimisation problems. The so-called inverse scale space flow which corresponds to Eq. (12) in our paper can be seen as the continuous version of Bregman iteration. As in our paper,  show that this equation can be solved through an iterative algorithm. We refer to [51; Section 2] for further details. However we did not find any results in this literature concerning the uniqueness of the constrained minimisation problem due to Assumption 1, nor on the maximum number of iterations, the behaviour under RIP assumptions and the maximum number of active coordinates.

**Subdifferential equations and rate-independent systems.** As in Eq. (12), subdifferential inclusions of the form \( L(_{t})}{t} h(_{t})\) for non-differential functions \(h\) have been studied by Attouch et al.  but for strongly convex functions \(h\). In this case, the solutions are continuous and do not exhibit jumps. On another hand, [18; 36; 37] consider so-called _rate-independent systems_ of the form \(_{g}E(t,q_{t}) h(_{t})\) for \(1\)-homogeneous _dissipation_ potentials \(h\). Examples of such systems are ubiquitous in mechanics and appear in problems related to friction, crack propagation, elastoplasticity and ferromagnetism to name a few [35, Ch. 6 for a survey]. As in our case, the main difficulty with such processes is the possible appearance of jumps when the energy \(E\) is non-convex.

Conclusion.Our study examines the behaviour of gradient flow with vanishing initialisation over diagonal linear networks. We prove that it leads to the flow jumping from a saddle point of the loss to another. Our analysis characterises each visited saddle point as well as the jumping times through an algorithm which is reminiscent of the LARS method used in the Lasso framework. There are several avenues for further exploration. The most compelling one is the extension of these techniques to broader contexts for which the implicit bias of gradient flow has not yet fully been understood.

Acknowledgments.S.P. would like to thank Loucas Pillaud-Vivien for introducing him to this beautiful topic and for the many insightful discussions. S.P. also thanks Quentin Rebjock for the many helpful discussions and Johan S. Wind for reaching out and providing the reference of . The authors also thank Jerome Bolte for the discussions concerning subdifferential equations, Aris Daniilidis for the reference of , as well as Aditya Varre and Mathieu Even for proofreading the paper.