# Dense Connector for MLLMs

Huanjin Yao\({}^{1,3^{}}\), Wenhao Wu\({}^{2}\)\({}^{\)\(}\), Taojiannan Yang\({}^{4}\), Yuxin Song\({}^{3}\), Mengxi Zhang\({}^{3}\)

**Haocheng Feng\({}^{3}\), Yifan Sun\({}^{3}\), Zhiheng Li\({}^{1}\), Wanli Ouyang\({}^{5}\), Jingdong Wang\({}^{3}\)**

\({}^{1}\)Shenzhen International Graduate School, Tsinghua University \({}^{2}\)The University of Sydney

\({}^{3}\)Baidu Inc. \({}^{4}\)Amazon \({}^{5}\) The Chinese University of Hong Kong

\({}^{}\) Equal Contribution \({}^{}\)Corresponding Author

###### Abstract

_Do we fully leverage the potential of visual encoder in Multimodal Large Language Models (MLLMs)?_ The recent outstanding performance of MLLMs in multimodal understanding has garnered broad attention from both academia and industry. In the current MLLM rat race, the focus seems to be predominantly on the linguistic side. We witness the rise of larger and higher-quality instruction datasets, as well as the involvement of larger-sized LLMs. Yet, scant attention has been directed towards the visual signals utilized by MLLMs, often assumed to be the final high-level features extracted by a frozen visual encoder. In this paper, we introduce the _Dense Connector_ - a simple, effective, and plug-and-play vision-language connector that significantly enhances existing MLLMs by leveraging multi-layer visual features, with minimal additional computational overhead. Building on this, we also propose the Efficient Dense Connector, which achieves performance comparable to LLaVA-v1.5 with only 25% of the visual tokens. Furthermore, our model, trained solely on images, showcases remarkable zero-shot capabilities in video understanding as well. Experimental results across various vision encoders, image resolutions, training dataset scales, varying sizes of LLMs (2.7B\(\)70B), and diverse architectures of MLLMs (_e.g._, LLaVA-v1.5, LLaVA-NeXT and Mini-Gemini) validate the versatility and scalability of our approach, achieving state-of-the-art performance across 19 image and video benchmarks. We hope that this work will provide valuable experience and serve as a basic module for future MLLM development. Code is available at https://github.com/HJYao00/DenseConnector.

## 1 Introduction

In recent years, Large Language Models (LLMs) led by ChatGPT  have made remarkable advancements in text comprehension and generation. Furthermore, cutting-edge Multimodal Large Language Models (MLLMs)  have rapidly expanded the capabilities of LLMs to include visual understanding, evolving into models capable of integrating both vision and text modalities. This has elevated MLLMs to become a new focal point for research and discussion .

In broad terms, the architecture of existing MLLMs can be delineated into three components: the pre-trained vision encoder (_e.g._, CLIP's ViT-L  or EVA-CLIP's ViT-G ), the pre-trained LLM (_e.g._, OPT , Llama , Vicuna , _etc._), and the connector (_e.g._, Q-former  or linear projection ) trained from scratch to bridge the vision and language models. An intriguing trend in current MLLM research is that the focus of model learning and performance improvement seems to primarily center around the language aspect (_e.g._, utilizing larger-scale and higher-quality visual instruction data , larger-sized LLMs ), with less exploration into the visual signals fed into the connector. Typically, the visual encoder is frozen to extract high-level visual features, which are then fed into the connector. This leads us to rethink: _Have we fully utilized the existing pre-trained visual encoder?_In addition to the common practice of feeding the connector with final high-level visual features from visual encoder, an intuitive yet overlooked idea is to integrate visual features from various layers to complement the high-level features. In Fig. 1 (a), we illustrate attention maps from different layers of a 24-layer CLIP  pre-trained ViT-L , showing that different layers of the same visual encoder emphasize different regions of interest. Moreover, looking back at the history of computer vision, classic models (_e.g._, Densenet , FPN ) utilize multi-layer features to enhance visual representations. In MLLMs, the vision encoder is typically frozen to mitigate significant computational costs. In this context, our idea leverages the "free lunch" of utilizing offline features from different layers as an implicit enhancement of visual information without the need for additional computational overhead. Furthermore, this way also complements techniques that directly increase visual signals, _e.g._, increasing image resolution  or introducing additional visual encoders . This idea is both simple and efficient, while also being sufficiently generic, logically allowing for seamless integration with any existing MLLMs.

In light of this, we propose the _Dense Connector (DC)_, serving as a plug-and-play vision-language connector that involves offline features from various layers of the frozen visual encoder to provide the LLM with more visual cues. We explore three intuitive instantiations for the Dense Connector: 1) _Sparse Token Integration (STI)_: We explicitly consider increasing the number of visual tokens by aggregating visual tokens from different specified layers and the final visual token. These tokens are then fed into a learnable projector for mapping into the text space. 2) _Sparse Channel Integration (SCI)_: To avoid increasing the number of tokens, we concatenate visual tokens from different specified layers in the feature dimension. They are then passed to the projector, which not only maps visual tokens into the text space but also serves to reduce the feature dimensionality. 3) _Dense Channel Integration (DCI)_: In addition to incorporating features from specified layers, we further attempt to utilize visual features from all layers. All of these instantiations yield significant improvements while utilizing just one simple learnable projector (comprising two linear layers) without introducing any extra parameters. Moreover, we conduct extensive empirical studies to demonstrate its scalability and compatibility. We summarize our contributions as follows:

* We propose a simple, effective, and plug-and-play _Dense Connector_ that enhances the visual representation of existing MLLMs with minimal additional computational overhead. We hope it can serve as a basic module to continuously benefit future MLLMs.
* We demonstrate the versatility and scalability of our approach across various visual encoders, image resolutions (336px\(\)768px), training dataset scales, varying sizes of LLMs (2B\(\)70B), and diverse MLLMs architectures (_e.g._, LLaVA-v1.5 , LLaVA-NeXT , Mini-Gemini ).
* Our method exhibits exceptional performance across 11 image benchmarks and achieves state-of-the-art results on 8 video benchmarks without the need for specific video tuning.

Figure 1: Exploring Multi-layer Visual Features Empowering existing MLLMs.

Related Work

### Large Pre-trained Vision Models

The advent of pre-trained Vision Transformers (ViT)  has significantly propelled the advancement of computer vision. Furthermore, pre-training ViT models on web-scale image-text pairs, _e.g._, CLIP  and its subsequent iterations [9; 31; 32; 33], where vision and text encoders are simultaneously trained to bridge the gap between visual and textual modalities, has introduced zero-shot visual perception capabilities. Since then, CLIP-like models have served as effective initializations and have been incorporated into various vision-language cross-modal models (_e.g._, video-text alignment [34; 35; 36; 37], large vision-language models [14; 15; 38], _etc._). Recently, SigLIP  introduced pairwise sigmoid loss during training, enabling the visual encoder to demonstrate more advanced visual perception capabilities. To validate the compatibility of our _Dense Connector_, this paper conducted experiments on different visual encoders, including those of CLIP  and SigLIP .

### Large Language Models

The exceptional text understanding and generation capabilities demonstrated by auto-regressive Large Language Models (LLMs) [39; 40; 41] have garnered significant attention. Subsequently, a plethora of LLMs [42; 11; 10; 43] have emerged, with notable open-source efforts like LLaMA  greatly propelling community contributions to LLMs research. Through instruction fine-tuning techniques, these models showcase human-like language interaction abilities, further further propelling advancements in natural language processing. Recent developments have seen LLMs scaled up or down to meet various application needs. Lightweight LLMs [44; 45; 20; 46] have been developed to address computational constraints, facilitating edge deployment. Conversely, in the pursuit of exploring the upper limits of LLMs, works such as [47; 19; 11; 20] have expanded LLM parameters, continuously pushing the boundaries of language capabilities. In this study, we validated the scalability of our _Dense Connector_ by employing multiple LLMs ranging from 2.7B to 70B parameters.

### Multimodal Large Language Models

After witnessing the success of LLMs, researchers have shifted their focus towards enabling LLMs to understand visual signals. To achieve this, prior research has proposed compressing visual embeddings using Q-former  into query embeddings, followed by transforming them into text embeddings through linear projection, or directly employing MLP projection  to connect the visual encoder with LLM. Furthermore, following the instruction tuning paradigm [48; 49], pioneering works [38; 15; 50] significantly boost the development of MLLMs through visual instruction tuning. Subsequently, by introducing larger-scale and higher-quality datasets, efforts such as [18; 25; 17; 24] have notably enhanced the visual understanding and reasoning capabilities of MLLMs. Additionally, there are works that introduce additional visual encoders [29; 30] or utilize higher-resolution images [25; 18; 24] to provide richer visual signal sources. Meanwhile, many studies [51; 52; 53] directly extend these above image-based methods to video conversational models by leveraging video instruction tuning datasets. In summary, these studies typically utilize high-level features from the frozen visual encoder as visual embeddings. However, we find that effectively leveraging offline features from different layers--the overlooked "free lunch"--can yield significant benefits. Then, we follow FreeVA  to directly extend the image model for video understanding without any additional video training.

## 3 Method

### Overview

In Fig. 2(a), we illustrate the overall architecture of our model, using the mainstream LLaVA  framework as an example. It includes the pre-trained visual encoder \(Vis()\) and the Large Language Model \(LLM()\), alongside with our proposed _Dense Connector_\(DC()\). Similarly, our _DC_ can be seamlessly extended to other high-resolution or dual-branch MLLMs, such as LLaVA-NeXT  and Mini-Gemini . Formally, the introduction is as follows:

**Visual Encoder:** We utilize a CLIP pre-trained Vision Transformer (ViT)  as the visual encoder for extracting visual features. Initially, ViT partitions an image \(X_{i}^{H W C}\) into a sequence of non-overlapping patches. Each patch is then processed via convolution to produce visual tokens, which are subsequently input into ViT. This procedure yields \(L\) layers of visual features \(V^{L N D_{v}}\), where \(N\) denotes the number of the visual tokens and \(D_{v}\) denotes the feature dimension.

**Dense Connector:** The Dense Connector comprises two components: the first integrates multi-layer visual features, elaborated upon in detail in Sec. 3.2, while the second employs a learnable MLP to map the integrated visual features to the LLM's text space. The MLP consists of two linear layers with a GELU  activation function sandwiched between them. The first layer adjusts the visual hidden size \(D_{v}\) to align with the LLM's hidden dimension \(D_{t}\), while the second layer maintains the dimensionality at \(D_{t}\). Upon processing through the Dense Connector, we acquire visual embeddings \(e_{v}^{N D_{t}}\) that encapsulate information from multiple layers.

**Large Language Model:** The LLM processes textual data using a tokenizer and text embedding module to convert language into its input feature space. These text embeddings are concatenated with transformed visual embeddings before being fed into the LLM for subsequent predictions.

### Dense Connector

Here we delve into three intuitive instantiations of the **Dense Connector**, each demonstrating superior performance compared to the baseline (_e.g._, LLaVA-1.5 ). Among them, _Sparse Token Integration (STI)_ and _Sparse Channel Integration (SCI)_ sparsely select visual features from \(K\) layers (indexed as \(l_{n}\), where \(1 l_{n}<L\) and \(1 n K\)) spanning shallow, middle, and high levels out of the total \(L\) layers of ViT, while _Dense Channel Integration (DCI)_ utilizes features from all layers. These features are then fed into Dense Connector to generate visual embedding that can be "understood" by LLM.

**Sparse Token Integration (STI):** While existing methods typically rely solely on features from the final layer as the visual representation input for the LLM, our STI approach diverges from this by integrating features from multiple layers to enrich the visual input for the LLM. Recognizing that higher-level features contain richer semantic information crucial for visual signal perception in VLMs, we maintain the final layer features unchanged while downsampling additional visual features from other layers by using average pooling \(avg()\) with a stride \(\). This downsampling reduces the number of visual tokens to \(N^{}=N/\), mitigating computational overhead and redundancy. These visual features from various layers are concatenated along the token dimension and processed through a shared \(MLP()\), yielding more robust visual embedding \(e_{v}^{(N+(k-1) N^{}) D_{t}}\):

Figure 2: Dense Connector in MLLM: Overview and Three Instantiations. \(N\) is the number of tokens, \(D\) is the feature dimension, and \(\) is the downsampling ratio.

\[e_{v}=MLP(Concatenate([avg(V_{l_{1}}),...,avg(V_{l_{K}}),V_{L}],dim=token)).\] (1)

**Sparse Channel Integration (SCI):** We delve deeper into connecting multi-level features along the channel dimension. Subsequently, this feature is processed through an MLP projector to obtain visual embedding \(e_{v}^{N D_{t}}\):

\[e_{v}=MLP(Concatenate([V_{l_{1}},...,V_{l_{K}},V_{L}],dim=channel)).\] (2)

The MLP projector serves dual functions: integrating various features as a fusion tool and facilitating the transformation of visual inputs into linguistic representations. This design ingeniously leverages the dimensionality scaling effect of the MLP projector, enabling the transformation of connected multi-layer features into the feature space of the LLM without requiring additional modules. Moreover, this method does not increase the number of tokens fed into the LLM, thereby avoiding any increase in the computational overhead of the LLM.

**Dense Channel Integration (DCI):** While _Sparse Channel Integration_ incorporates features from \(K\) layers, many visual feature from other layers remain unused. Concatenating all visual feature layers using STI or SCI leads to excessively high dimensions, posing challenges during training. To address these issues, we propose DCI, which builds upon the SCI method by integrating adjacent layers to reduce redundancy and high dimensionality. This approach ensures dense connectivity across a wider range of visual layers. Specifically, we partition the features of \(L\) layers into \(G\) groups, where each group comprises \(M\) adjacent visual features, with \(M=L/G\). Summing the features within each group, denoted as \(GV_{g}\), finally yields \(G\) fused visual representations:

\[GV_{g}=_{i=(g-1)M+1}^{gM}V_{i}, 1 g G.\] (3)

Subsequently, we concatenate these features from the \(G\) groups with the final layer's features along the channel dimension before passing them through an MLP:

\[e_{v}=MLP(Concatenate([GV_{1},...,GV_{G},V_{L}],dim=channel)).\] (4)

### Efficient Dense Connector for Visual Token Optimization

For MLLMs, each image is converted into hundreds or even thousands of visual tokens, and this large number of tokens increases the computational burden on autoregressive models. However, reducing the number of visual tokens generally leads to a noticeable drop in performance. In this paper, we leverage multi-layer visual features to compensate for the information loss caused by reducing visual tokens, enabling our method to achieve performance on par with LLaVA-v1.5  while using several times fewer visual tokens, and surpassing other carefully designed efficient connectors [24; 38; 50; 56; 57] Specifically, after obtaining the visual embedding \(e_{v}\) through the Dense Connector, we apply a parameter-free module, _i.e_. a 2D interpolation function, to downsample visual tokens. Then, these discrete visual tokens \(e_{v}^{}\) are concatenated with the text tokens and fed into the LLM, resulting in a 3 times improvement in inference speed.

### Training-Free Extension from Image to Video Conversational Models

Following FreeVA , we extend the image-based models trained as described above to video domain for video understanding. Specifically, given a video, we uniformly sample \(T\) frames, and each frame is processed through the visual encoder and dense connector to obtain a visual embedding \(e_{v}\). Consequently, we obtain an embedding sequence \(\{e_{v_{1}},...,e_{v_{T}}\}\), which is then fed into the LLM.

Experiments

### Implementation Details

**Architecture.** 1) Visual Encoders: To explore the generalization of the Dense Connector across different visual encoders, we select two mainstream options, namely CLIP-ViT-L-336px  and SigLIP-ViT-SO . 2) LLMs: The Dense Connector is applied across various LLMs, spanning from 2.7B to 70B parameters. This includes Phi-2-2.7B , Vicuna-7B&13B , Hermes-2-Yi-34B , and Llama3-8B&70B-Instruct. 3) Dense Connector: For the 24-layer CLIP-ViT-L-336px , we specifically target visual features from the 8th, 16th, and final 24th layers for both STI and SCI. For STI, we apply a downsampling factor of \(=8\) for the features from the 8th and 16th layers. For DCI, we divide all layer features into two groups, each containing 12 layers (_i.e_., 1-12, 13-24). Note that we use 1 to denote the stem layer of ViT, _e.g_., 2-25 correspond to the 24 layers of ViT-L. Similarly, for the SigLIP ViT-SO , which has 27 layers, we partition the first 26 layers into two groups (_i.e_., 1-13, 14-26).

**Training Datasets.** Data quality plays a crucial role in determining the performance of MLLMs. In this study, we examine the impact of two high-quality training datasets on our model: LLaVA-1.5  and Mini-Gemini . The LLaVA-1.5 pre-training dataset comprises 558K image captions, while its instruction tuning dataset contains 665K conversations. Mini-Gemini builds upon LLaVA-1.5, offering a larger dataset with 1.2M image-text caption pairs for alignment and 1.5M conversations for instruction tuning. Unless otherwise specified, all experimental results are based on the LLaVA-1.5 dataset to reduce training costs.

**Training Recipe.** We train all models on 8 NVIDIA A100 GPUs with 40GB VRAM, except for the Hermes-2-Yi-34B and LLama-3-70B-Instruct, which utilize 32 NVDIA A100 GPUs with 80GB VRAM. Our training process comprises two stages: pre-training and instruction fine-tuning. In the pre-training phase, we initialize the visual encoder and LLM with pre-trained weights, while the Dense Connector is randomly initialized. Here, we freeze the visual encoder and the LLM, updating only the parameters of the Dense Connector. The model undergoes pre-training for one epoch with a global batch size of 256 and a learning rate of 1e-3. Subsequently, in the instruction fine-tuning stage, we maintain the visual encoder frozen while updating the Dense Connector and the LLM. Fine-tuning is performed for 1 epoch with a global batch size of 128 and a learning rate of 2e-5. For models using LoRA fine-tuning, we set the LoRA rank to 128 and LoRA alpha to 256. When scaling up the LLM to larger parameter sizes, such as LLama-3-70B-Instruct, we apply LoRA fine-tuning due to memory constraints. In this setup, we set the LoRA rank to 128 and LoRA alpha to 256.

**Evaluation.** We present comprehensive results across various image and video evaluation benchmarks. For image datasets, we include GQA , VQAV2 (VQA\({}^{v2}\)) , ScienceQA (SQA\({}^{I}\)) , TextVQA (VQA\({}^{T}\)) , POPE , MathVista (Math) , MMBench (MMB) , MM-Vet (MMV) , MMMU , LLAVA-Bench-In-the-Wild (LBW) , and MME . Additionally, we evaluate zero-shot performance on open-ended video question-answering benchmarks such as MSVD-QA , ActivityNet-QA , MSRVTT-QA , and the newly proposed generative performance benchmark , include evaluation metrics such as Correctness of Information (CI), Detail Orientation (DO), Contextual Understanding (CU), Temporal Understanding (TU), and Consistency (CO).

### Ablation Study

**Study on Instantiations of Dense Connector.** In Tab. 1, we discuss three proposed methods of Dense Connector. 1) **Sparse Token Integration (STI):** This method uses visual features from different hierarchical levels as independent visual prompts for the LLM, allowing it to perceive a more diverse set of visual features. We select features from the 8th, 16th, and 24th layers, resulting in significant improvements across various datasets, particularly achieving a 2.9% increase on the MMB . Further expanding the selection to include the 8th, 16th, 20th, and 24th layers enhances performance but also increases token count, leading to higher training costs and inference time. 2) **Sparse Channel Integration (SCI):** This method efficiently uses the MLP projector for feature fusion and projection, integrating multiple layers of visual features into visual embeddings. SCI boosts performance with minimal additional computational cost, achieving peak performance with features from the 8th, 16th, and 24th layers, resulting in a 1.7% improvement on GQA. SCI performs better than STI and mitigates the computational costs associated with higher token counts. However, expanding the range of visual layers within SCI does not yield additional performance gains, suggesting that merely extending the range of visual feature layers is ineffective. 3) **Dense Channel Integration (DCI):** Building on the performance enhancements of SCI, DCI integrates a broader array of visual features using grouped additive fusion to produce robust visual representations. We divide the visual features into 2 or 3 groups, each with an equal number of layers. For the CLIP-L model, each group combines features from 12 or 8 layers, respectively. Splitting them into 2 groups demonstrates superior performance, achieving improvements of 2.7% on SQA  and 2.5% on MMB  compared to the baseline. The experimental results illustrate that utilizing multi-layer visual features enhances the visual perception capabilities of the MLLMs, leading to more accurate responses. Unless otherwise specified, we employ DCI as the default instantiation of the Dense Connector for optimal performance.

**Study on Visual Encoders and Training Dataset Impacts.** Given our method's reliance on multi-layer visual features, it is crucial to assess its impact across various visual backbones. As shown in Tab. 2, we first replace the CLIP-ViT-L  with the more advanced visual encoder SigLIP-ViT

   Method & VE & Res. & PT+IT & LLM & **GQA** & **SQA\({}^{J}\)** & **VQA\({}^{T}\)** & **PODE** & **MMB** & **MMV** & **MMMU\({}^{v}\)** & **Math** \\   \\  LLaVA  & CLIP-L & 336 & 0.5M+0.6M & Vicuna-7B & 62.0 & 66.8 & 58.2 & 64.3 & 31.1 & 35.3\({}^{*}\) & 24.9\({}^{*}\) \\ LLaVA  & CLIP-L & 336 & 0.5M+0.6M & Vicuna-13B & 63.3 & 71.6 & 61.3 & 67.6 & 36.1 & 36.4 & 27.6 \\  DC (w/ LLaVA) & CLIP-L & 336 & 0.5M+0.6M & Vicuna-7B & 63.8 & 69.5 & 59.2 & 66.8 & 32.7 & 34.8 & 26.9 \\ DC (w/ LLaVA) & SigLIP-SO & 384 & 0.5M+0.6M & Vicuna-7B & 64.2 & 70.5 & 62.6 & 68.4 & 35.4 & **36.7** & 25.5 \\ DC (w/ LLaVA) & SigLIP-SO & 384 & 0.5M+0.6M & Vicuna-13B & **65.4** & **73.0** & **64.7** & **71.4** & **41.6** & 34.3 & **29.6** \\    \\  DC (w/ LLaVA) & SigLIP-SO & 384 & 1.2M+1.5M & Vicuna-7B & 63.8 & 72.9 & 64.6 & 71.7 & 45.0 & 35.8 & 33.1 \\ DC (w/ LLaVA) & SigLIP-SO & 384 & 1.2M+1.5M & Vicuna-13B & **64.6** & **77.1** & **65.0** & **74.4** & **47.7** & **37.2** & **36.5** \\    \\  MGM  & CLIP-L & 336 & 1.2M+1.5M & Vicuna-7B & 62.6\({}^{*}\) & 70.4\({}^{*}\) & 65.2 & 69.3 & 40.8 & 36.1 & 31.4 \\ MGM  & CLIP-L & 336 & 1.2M+1.5M & Vicuna-13B & 63.4\({}^{*}\) & 72.6\({}^{*}\) & 65.9 & 68.5 & 46.0 & 38.1 & 37.0 \\  DC (w/ MGM) & CLIP-L & 336 & 1.2M+1.5M & Vicuna-7B & 63.3 & 70.7 & 66.0 & 70.7 & 42.2 & 36.8 & 32.5 \\ DC (w/ MGM) & CLIP-L & 336 & 1.2M+1.5M & Vicuna-13B & **64.2** & **74.9** & **66.7** & **70.7** & **49.8** & **39.3** & **38.1** \\ DC (w/ MGM) & CLIP-L & 336 & 1.2M+1.5M & Vicuna-13B & **64.2** & **74.9** & **66.7** & **70.7** & **49.8** & **39.3** & **38.1** \\    \\  LLaVA-NeXT  & CLIP-L & AnyRes & 0.5M+0.6M & Vicuna-7B & 64.0 & 69.5 & 64.5 & 66.5 & 33.1 & 35.4 & 25.7 \\  DC (w/ LLaVA) & CLIP-L & AnyRes & 0.5M+0.6M & Vicuna-7B & 64.6 & **70.5** & 65.6 & **67.4** & 33.7 & **37.6** & 26.2 \\ DC (w/ LLaVA) & SigLIP-SO & AnyRes & 0.5M+0.6M & Vicuna-7B & **64.8** & 69.3 & **66.5** & 67.2 & **34.8** & 36.3 & **27.0** \\   

Table 2: Exploring the Compatibility and Scalability of Dense Connector (DC). Scaling results on visual encoder (VE), resolution (Res.), pre-training (PT) / instruction tuning (IT) data, and LLM are provided. \({}^{*}\)0.5M+0.6M\({}^{*}\) denotes the training data from LLaVA-1.5 , while \({}^{*}\)1.2M+1.5M\({}^{*}\) denotes the data from Mini-Gemini . \({}^{*}\) indicates results evaluated using official model.

   Model & Layer Index & **GQA** & **VQA\({}^{v2}\)** & **SQA\({}^{J}\)** & **VQA\({}^{T}\)** & **PODE** & **MMB** & **MMV** & **LBW** \\  Baseline & 24 & 62.0 & 78.5 & 66.8 & 58.2 & 85.9 & 64.3 & 31.1 & 65.4 \\  + _STI_ & 8,16,24 & 63.3 & 79.1 & 68.0 & 58.0 & 85.8 & 67.2 & 30.9 & 65.5 \\ + _STI_ & 8,16,20,24 & 63.0 & 79.1 & 68.0 & 58.8 & 85.9 & 67.6 & 30.8 & 65.7 \\  + _SCI_ & 8,16,24 & 63.7 & 79.2 & 68.9 & 58.2 & 86.1 & 66.2 & 32.2 & 66.0 \\ + _SCI_ & 16,24 & 63.0 & 79.0 & 67.6 & 58.2 & 86.0 & 65.6 & 31.7 & 65.6 \\ + _SCI_ & 8,16,20,24 & 63.6 & 79.2 & 67.0 & 58.1 & 86.0 & 65.8 & 31.9 & 66.0 \\  + _DCI_ & (1-8),(9-16),(17-24) & 63.6 & 79.3 & 67.8 & 58.6 & 86.3 & 66.5 & 32.6 & 66.0 \\ + _DCI_ & (1-12),(13-24) & **63.8\({}^{**1.8!}\)** & **79.5\({}^{**1.0!}\)** & **69.5\({}^{**2.7!}\)** & **59.2\({}^{**1.0!}\)** & **56.6\({}^{**9.7!}\)** & **66.8\({}^{**2.5!}\)** & **32.7\({}^{**1.6!}\)** & **66.1\({}^{**1.7!}\)** \\   

Table 1: Ablations on Visual Layer Selection in Dense Connector. Here, we explore three instantiations (_STI_, _SCI_, and _DCI_) of our Dense Connector integrated with the baseline (_i.e._, LLaVA-1.5 ), which utilizes a 24-layer CLIP-ViT-L-336px.

SO . Leveraging the enhanced multi-layer visual features from SigLIP-ViT-SO, our Dense Connector demonstrates further performance improvements. Additionally, we investigate the influence of training datasets on the effectiveness of the Dense Connector. By fine-tuning our model using the larger dataset , we observe notable performance gains across most benchmark evaluations. The results indicate that more training data significantly enhance model performance. Specifically, our model with Vicuna-13B achieves accuracy of 36.5% on MathVista  and 77.1% on SQA , underscoring the significant benefits of increased training data. Moreover, when using the same training data and the LLM (_i.e_., Vicuna-7B), our Dense Connector surpasses the dual encoder structure of Mini-Gemini  across the majority of benchmarks. Specifically, it achieves performance gains of 2.4% on the MMB , 4.2% on MM-Vet , and 1.7% on the MathVista  benchmark.

**Study on High-Resolution Setting.** The use of high-resolution images to enhance detail representation in MLLMs has garnered considerable attention [25; 18; 26; 27; 28]. In this paper, we extend Dense Connector to the Mini-Gemini (MGM)  and LLaVA-NeXT , showcasing its plug-and-play capability. For MGM, we keep the high-resolution features from ConvNeXT  intact, applying DCI exclusively to the CLIP features. We observe significant improvements across various benchmarks, as detailed in Tab. 2, including MathVista , MMB , and MM-Vet , with enhancements of 1.1%, 2.2%, and 3.8%, respectively. In addition to the high-resolution architecture of the dual visual encoder, we also extend the Dense Connector to dynamic high-resolution, specifically using the AnyRes technology from LLaVA-NeXT . For a fair comparison, we provide a baseline for LLaVA-NeXT trained on the same dataset. As shown in Tab. 2, Dense Connector achieves overall improvements compared to the dynamic resolution method LLaVA-NeXT as well.

**Study on Efficient Dense Connector.** To achieve faster inference speed, we investigate an efficient Dense Connector in this study, which can accelerate inference by 3 times. As described in Sec. 3.3, we use a 2D bilinear interpolation function to downsample the visual tokens by a factor of 4, reducing the number of tokens from 576 to 144, which decreases the training time during in the second stage on 8 A100 GPUs from 9 hours to 6.5 hours. With the same configuration of using 144 visual tokens, Dense Connector outperforms the carefully designed efficient connector method Tokenpacker  by 0.9%, 1.5%, 2.5% and 1.4% on GQA , VQAv2 , MMB , and MM-Vet , respectively.

### Main Results

**Comparison with SoTAs in Image Understanding.** In Tab. 4, we scale the LLMs from 2.7B to 70B parameters and compare them with state-of-the-art MLLMs. When considering lightweight models, our Dense Connector surpasses the previous MLLM, TinyLlava , achieving a 1.7% enhancement on the MM-Vet benchmark using the same fine-tuning data and foundation model. Furthermore, using same training data and LLM, our Dense Connector outperforms the LLaVA-1.5 Vicuna 13B  with substantial gains of 2.1%, 3.7%, and 5.5% on the GQA , MMB , and MM-Vet  benchmarks, respectively. Notably, even with data solely from LLaVA-1.5, our 13B model achieves performance comparable to MGM , which is trained on larger datasets, including 1.2M+1.5M data. Moreover, utilizing the advanced open-source LLM Llama3-8B-Instruct, our model significantly surpasses LLaVA-LLama3  with improvements of 5.5%, and 52 on MMB , and MME\({}^{p}\), respectively, highlighting the contribution of our Dense Connector. By scaling up the LLM to 34B and 70B, Dense Connector achieves further improvements leveraging more powerful language models. The 70B model attains scores of 82.4% on SQA  and 79.4% on MMBench . We then increase the resolution using AnyRes technology  and fully fine-tuned the LLM. Our 13B model outperforms MGM and LLaVA-NeXT on MMBench  and SQA , achieving scores of 72.3% and 72.6%. The 34B model achieves scores of 81.2%, 59.2%, and 97.7% on MMBench , MM-Vet , and LLaVA-Bench-in-the-Wild , respectively.

   Method & Res. & \#Token & PT+IT & LLM & **GQA** & **VQA\({}^{/2}\)** & **SQA\({}^{}\)** & **VQA\({}^{T}\)** & **MMB** & **MMV** & **Math** \\  LLaVA  & 336 & 576 & 0.5M+0.6M & Vicuna-7B & 62.0 & 78.5 & 66.8 & **58.2** & 64.3 & 31.1 & 24.9\({}^{*}\) \\ Qwen-VL-Chat  & 448 & 256 & 1.4B+50M & Qwen-7B & 57.5 & 68.2 & 61.5 & - & - & - & - \\ TokenPacker  & 336 & 144 & 0.5M+0.6M & Vicuna-7B & 61.9 & 77.9 & - & - & 65.1 & 33.0 & - \\  Dense Connector & 336 & 144 & 0.5M+0.6M & Vicuna-7B & **62.8** & **79.4** & **68.8** & 58.1 & **67.6** & **34.4** & **25.8** \\   

Table 3: Comparison of Efficient Dense Connector with Other Efficient Methods. \({}^{*}\) indicates results evaluated using official model.

**Comparison with SoTAs in Video Understanding.** Building on the training-free paradigm of FreeVA  for image-to-video adaptation, we directly apply our models, originally trained on image-text datasets, to video dialogues. As indicated in Tab. 5, the visual enhancement capabilities of the Dense Connector significantly enhance the video comprehension of our 13B model. This model surpasses the baseline LLaVA-1.5  with FreeVA  on MSVD  and ActivityNet . By scaling up the LLM to 34B, our model demonstrates a marked improvement in video understanding capabilities, achieving state-of-the-art performance on video evaluation benchmarks with accuracies of 77.4% on MSVD , 62.1% on MSR-VTT , and 55.8% on ActivityNet .

   Method &  & Res. & LLM & ^{}\)} &  & ^{}\)} &  & ^{}\)} & ^{W}\)} &  \\  MobileVLM V2  & 1.2M+3.0M & 336 & ML-2.7B & 70.0 & 63.2 & 1441 & – & – & – & – & 61.1 \\ TinyLLaVA  & 0.5M+0.6M & 384 & Phi2-2.7B & 69.9 & – & – & 32.1 & – & – & 67.9 & 61.3 \\ mPLUG-Owl2  & 348M+1.2M & 448 & Llama-27B & 68.7 & 64.5 & 1450 & 36.2 & 32.7 & 22.2 & – & 56.1 \\ Queen-VL-Chat\({}^{}\) & 1.4B+50M & 448 & Qwen-7B & 68.2 & 60.6 & 1488 & – & – & – & – & 57.5\({}^{*}\) \\ LLaVA-V1.5  & 0.5M+0.6M & 336 & Vicuna-13B & 716.6 & 67.7 & 1531 & 36.1 & 36.4 & 27.6 & 72.5 & 63.3 \\ ShareGPTRV  & 1.2M+0.7M & 336 & Vicuna-13B & 71.2 & 68.5 & 1619 & 43.1 & – & – & 79.9 & 64.8 \\ MobileVLM V2  & 1.2M+3.6M & 336 & Vicuna-7B & 74.8 & 70.8 & 1559 & – & – & – & – & 64.6 \\ LLaMA-VID  & 0.8M+0.7M & 336 & Vicuna-7B & 70.0 & 66.6 & 1542 & – & – & – & 65.0\({}^{*}\) \\ SPHINX-Plus  & 160M & 448 & Llama-13B & 74.2 & 71.0 & 1458 & 47.9 & – & 36.8 & 71.7 & – \\ LLaVA-ILaMA3  & 0.5M+0.6M & 336 & Llama3-8B & 73.3 & 68.9 & 1506 & – & 36.8 & – & – & 63.5 \\ CuO  & 0.5M+0.6M & 336 & Mistist-7B & 71.7 & 69.6 & 1429 & 34.3 & – & – & 68.8 & 63.2 \\ MMI  & 381M+4M & 3144 & MIMI-7B & 72.6 & 79.0 & 1529 & 42.1 & 37.0 & 35.9 & 81.5 & – \\ VILA  & 50M+1M & 336 & Llama-2-13B & 73.7 & 70.3 & 1570 & 38.8 & – & – & 73.0 & 63.3\({}^{*}\) \\ Mini-Gemini  & 1.2M+1.5M & 336+768 & Vicuna-13B & 72.6 & 68.5 & 1565 & 46.0 & 38.1 & 37.0 & 87.7 & 63.4 \\ LLaVA-NeXT  & 0.5M+0.7M & 336 & Llama3-13B & 73.6 & 70.0 & 1575 & 48.4 & 36.2 & 35.3 & 87.3 & 65.4 \\    
    \\  Dense Connector & 0.5M+0.6M & 384 & Phi2-2.7B & 70.3 & 70.5 & 1487 & 33.8 & 36.6 & 28.2 & 65.1 & 61.5 \\ Dense Connector & 0.5M+0.6M & 384 & Vicuna-7B & 70.5 & 68.4 & 1523 & 35.4 & 36.7 & 25.5 & 67.4 & 64.4 \\ Dense Connector & 0.5M+0.6M & 384 & Vicuna-13B & 73.0 & 71.4 & 1569 & 41.6 & 34.3 & 29.6 & 73.6 & **65.4** \\ Dense Connector & 0.5M+0.6M & 384 & Llama3-8B & 75.2 & 74.4 & 1558 & 34.6 & 40.4 & 28.6 & 68.8 & 65.1 \\ Dense Connector & 0.5M+0.6M & 384 & Yi3-4B\({}_{B-0.6M}\) & 80.5 & 77.7 & 1588 & 41.0 & 47.1 & 33.5 & 75.1 & 63.9 \\ Dense Connector & 0.5M+0.6M & 384 & Llama3-70B\({}_{LLoRA}\) & **82.4** & 79.4 & 1622 & 46.1 & 47.0 & 32.9 & 74.5 & 64.0 \\ Dense Connector & 1.2M+1.5M & 384 & Vicuna-13B & 77.1 & 74.4 & 1579 & 47.8 & 37.2 & 36.5 & 88.9 & 64.6 \\  Dense Connector & 1.2M+1.5M & 384 & \(44n_{Web}\) & Vicuna-7B & 72.0 & 69.2 & 1535 & 44.4 & 36.4 & 32.7 & 88.8 & 63.9 \\ Dense Connector & 1.2M+1.5M & 384 & \(4n_{Web}\) & Vicuna-13B & 75.2 & 72.3 & 1573 & 47.0 & 36.8 & 35.5 & 93.2 & 64.3 \\ Dense Connector & 1.2M+1.5M & 384 & \(4n_{Web}\) & Vic-34B & 78.0 & **81.2** & **1696** & **59.2** & **51.8** & **40.0** & **97.7** & **66.6** \\   

Table 4: Comparisons with State-of-the-Arts. \({}^{*}\) indicates the dataset have been used for training, and \({}^{}\) indicates the dataset is not publicly accessible. \({}^{}\)PT, \({}^{}\)IT, and \({}^{}\)Res.\({}^{}\) denote pre-training data, instruction fine-tuning data, and image resolution, respectively.

    Method &  &  &  &  &  &  \\ Method & Size & Version & Free & Acc & Score & Acc & Score & Acc & Score & CI & DO & CU & TU & CO \\  FrozenBiLM  & 0.9B & MAR & & 33.8 & – & 16.7 & – & 25.9 & – & – & – & – & – & – \\ Video-LLaMA & 7B & MAR & & 51.6 & 2.5 & 29.6 & 1.8 & 12.4 & 1.1 & 1.96 & 2.18 & 2.16 & 1.82 & 1.79 \\ LLaMA-Adapter  & 7B & MAR & & & - & – & – & – & 2.03 & 2.32 & 2.30 & 1.98 & 2.15 \\ VideoChat  & 7B & MAR & & & 56.3 & 2.8 & 45.0 & 2.5 & 26.5 & 2.2 & 2.2 & 2.23 & 2.50 & 2.53 & 1.94 & 2.24 \\ Video-Chat\({}^{}\)GT  & 7B & MAR & & & 64.9 & 3.3 & 49.8 & 2.8

**Qualitative Results.** In Fig. 3, we illustrate our model's exceptional visual understanding and text generation capabilities across various scenarios, encompassing both image and video. More qualitative results are provided in Appendix A.5.

## 5 Conclusion and Limitation

In this paper, we introduce the Dense Connector, a novel plug-and-play module that enhances visual perception capabilities of MLLMs by densely integrating multi-layer visual features. We instantiated three types of Dense Connector and validate the efficacy of it across a diverse array of vision encoders, LLMs, and training datasets, demonstrating substantial improvements in performance across multiple evaluation benchmarks. Dense Connector can be easily integrated into existing MLLMs. In this work, we incorporate the Dense Connector into mainstream model LLaVA and high-resolution method Mini-Gemini, demonstrating its versatility and generalization capabilities.

**Limitations:** Our three Dense Connector instantiations do not introduce additional parameters, leaving room for further exploration. We have not yet found an effective method for incorporating additional parameters. Future research will focus on discovering more efficient ways to connect visual and language models for better modality alignment.

Figure 3: Quantitative Results for Image and Video dialogues. Figures (a) through (d) pertain to image understanding, while figures (e) and (f) relate to video understanding.