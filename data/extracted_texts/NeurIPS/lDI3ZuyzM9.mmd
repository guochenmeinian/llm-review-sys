# AutoGO: Automated Computation Graph Optimization for Neural Network Evolution

Mohammad Salameh\({}^{1}\), Keith G. Mills\({}^{1,2}\), Negar Hassanpour\({}^{1}\), Fred X. Han\({}^{1}\),

Shuting Zhang\({}^{3}\), Wei Lu\({}^{1}\), Shangling Jui\({}^{3}\), Chunhua Zhou\({}^{3}\), Fengyu Sun\({}^{3}\), Di Niu\({}^{2}\)

\({}^{1}\)Huawei Technologies Canada. \({}^{2}\)Dept. ECE, University of Alberta. \({}^{3}\)Huawei Kirin Solution, China.

{mohammad.salameh, negar.hassanpour2, fred.xuefei.han1,

jui.shangling, zhouchunhua}@huawei.com,

{kgemills, dniu}@ualberta.ca, {zhangshuting8, robin.luwei, sunfengyu}@hisilicon.com

Equal contribution.Work done during an internship at Huawei.

###### Abstract

Optimizing Deep Neural Networks (DNNs) to obtain high-quality models for efficient real-world deployment has posed multi-faceted challenges to machine learning engineers. Existing methods either search for neural architectures in heuristic design spaces or apply low-level adjustments to computation primitives to improve inference efficiency on hardware. We present Automated Graph Optimization (AutoGO), a framework to evolve neural networks in a low-level Computation Graph (CG) of primitive operations to improve both its performance and hardware friendliness. Through a tokenization scheme, AutoGO performs variable-sized segment mutations, making both primitive changes and larger-grained changes to CGs. We introduce our segmentation and mutation algorithms, efficient frequent segment mining technique, as well as a pretrained context-aware predictor to estimate the impact of segment replacements. Extensive experimental results show that AutoGO can automatically evolve several typical large convolutional networks to achieve significant task performance improvement and FLOPs reduction on a range of CV tasks, ranging from Classification, Semantic Segmentation, Human Pose Estimation, to Super Resolution, yet without introducing any newer primitive operations. We also demonstrate the lightweight deployment results of AutoGO-optimized super-resolution and denoising U-Nets on a cycle simulator for a Neural Processing Unit (NPU), achieving PSNR improvement and latency/power reduction simultaneously. Code available at https://github.com/Ascend-Research/AutoGO.

## 1 Introduction

Deep Neural Networks (DNNs) have achieved great success in Computer Vision (CV) and Natural Language Processing (NLP) tasks. A major trend toward achieving better performance on benchmarks is adopting large and computationally demanding deep models . However, successful and efficient deployment of deep neural networks onto diverse and specific hardware devices, including neural processing units on the edge, significantly hinges upon engineering proper neural architectures that are both excellent in task performance while meeting hardware friendliness objectives.

A range of techniques have been proposed by academia and industry to solve the hardware-friendly deployment challenges of DNNs . Neural Architecture Search (NAS) replaces the manual design process of DNNs, achieving remarkable performance in several applications in CV [67; 11; 9; 6] and NLP [32; 10; 12]. While NAS can utilize a flexible range of search algorithms [53; 11; 79; 47], the search space adopted by NAS is based on heuristics, either searching for an optimal macro-netconstruction based on predefined blocks, e.g., MBConv blocks in MobileNets [59; 26], or stacking searchable cells by heuristic rules [39; 17; 73]. These heuristic rules may not be efficient to the target hardware device for deployment and may limit the potential gain from NAS methods. On the other hand, graph rewriting [70; 30] operates on the tensor computation graph of a DNN to improve its inference efficiency on hardware. Rewriting involves applying a set of subgraph substitution rules that preserve mathematical functionality of the original DNN, which does not alter or reduce the neural architecture to achieve better task performance or fitness to hardware.

In this paper, we propose Automated Graph Optimization (AutoGO), a generic graph optimization framework to evolve a given neural architecture for efficient and low-power deployment onto a specific hardware device. Unlike traditional NAS which builds networks from scratch in a heuristic search space or from hand-crafted building blocks [39; 63; 15], AutoGO enhances both hardware-friendliness and task performance of a DNN, by evolving its underlying Computation Graph (CG) using computational units composed of operations extracted from NAS benchmarks. AutoGO automatically improves typical neural networks in terms of benchmark performance on several CV tasks ranging from classification, semantic segmentation, to super-resolution without relying on newer operations. It also automates lightweight DNN deployment onto mobile neural processing units while preserving task performance, hence replacing manual tweaking efforts by ML engineers. We present the following key contributions in designing the AutoGO framework:

AutoGO optimizes DNNs on a Computation Graph (CG) of low-level primitives extracted from TensorFlow  models, which allow us to optimize all types of operations and their hyperparameters like filter size and latent tensor dimensions and thus offer a holistic fine-grained view of DNNs. Unlike graph rewriting which preserves mathematical equivalence, AutoGO alters the CG of a DNN for task performance and fitness to hardware.

Rather than relying on predefined blocks, the basic units for mutation in AutoGO are computation subgraphs, which we call segments, as illustrated in Figure 1. Segments are mined from a large number of CGs from several NAS benchmarks based on frequent subgraph mining in a data-driven manner. We use Byte Pair Encoding (BPE), an efficient tokenization technique  from NLP, to segment CGs and merge frequent operations into segments. By extracting and including segments of variable sizes into our database, AutoGO enables both primitive operation changes and larger-scaled block changes to a DNN.

AutoGO leverages an evolutionary algorithm to mutate our BPE-segmented source network. The segment mutation process is guided by hardware friendliness metrics and a pre-trained neural predictor to estimate the performance of the mutant network resulting from segment replacement. We propose a neural predictor which explicitly considers the positional and contextual information of a segment replacement made in a CG and directly models the performance gain. Mutations are also coupled with a resolution propagation scheme that solves for the tensor shapes in the replacement segment to ensure architectural validity.

Extensive experiments demonstrate that AutoGO can enhance the performance of the best architectures in several public architecture benchmarks, e.g., NAS-Bench-101 , NAS-Bench-201 , HiAML, Inception, and Two-Path . Additionally, AutoGO can automatically optimize several typical large CNN architectures, including ResNets , VGG , and EDSR  on a breadth of CV tasks including Classification, Semantic Segmentation, Human Pose Estimation, and Super Resolution. We show that AutoGO can improve their performance while making them lightweight, without using newer generations of operations that do not appear in the original network. Finally, to demonstrate the real-world applicability of our framework, we show results of AutoGO-optimized FSRCNN  (for super-resolution) and image denoising U-Net  for low-power or low-latency deployment using a cycle simulator for a Huawei mobile Neural Processing Unit (NPU).

Figure 1: A DNN can be partitioned into disjoint subgraphs (segments). Segments contain a variable number of inputs, outputs, and nodes ranging from primitive operations to complex subgraphs.

Related Work

**NAS Benchmarks and Neural Predictors.** NAS-Benchmarks comprise architectures from a given search space and their accuracy performance. NAS-Bench-101  and NAS-Bench-201  provide the performance of 423k and 15.6k architectures, respectively, on CIFAR-10. Benchmarks enable the rapid development of search algorithms and neural predictors. Neural predictors [43; 54; 41; 73; 65; 40] treat NAS benchmarks as datasets and learn to estimate the performance of architectures in a given search space, and thus constitute a low-cost avenue for performance evaluation.

However, NAS benchmarks suffer from several drawbacks. First, benchmarks only provide performance annotations for architectures inside a manually designed fixed search space. Thus, any tweaks to decrease FLOPs or latency beyond the search space requires training the new architecture from scratch. Second, existing NAS Benchmarks are mostly cell-based  and compose a network by stacking the same cell structure multiple times. This structure forms a high-level architecture representation that is insensitive to spatial details such as latent tensor dimensions, which vary along the depth of the network and influence hardware-friendliness . As most existing neural predictors learn using high-level cell representations, these drawbacks hamper their deployment generalizability. In contrast, AutoGO can mutate an architecture beyond its original, manually-defined design space by utilizing a low-level, spatially-sensitive representation.

**Computational Graphs for DNN Hardware Friendliness.** Multiple subfields explore how to reduce the carbon footprint and time cost of DNNs. Pruning and quantization methods  aim to reduce the number of parameters or lower the bit precision of model weights, respectively. Graph rewriting methods like TASO  and TENSAT  consider mathematically equivalent substitutions, e.g., merging or splitting parallel convolutions and applying the associative/distributive properties. These schemes usually require spatial details like resolution and channel size to perform rewrites. Hence, they represent DNNs using Computation Graphs (CG) [50; 22], which treat each primitive operation as a node and use the network forward pass to define edge connectivity. Similarly, AutoGO also uses a low-level CG representation. But different from these approaches, it aims to evolve the architecture of an untrained DNN to improve performance while also optimizing hardware friendliness.

**Neural Architecture Design Space.** Several works employ NAS over large spaces by jointly searching over macro and micro-structures for both block type and tensor dimensions [64; 15]. Human expertise is at the core of these design choices to constrain the search for high-performing architectures.  model a search space as a 3-level hierarchical graph to overcome the reliance on expert knowledge.  propose Neural Search Space Evolution, which progressively grows a current search space by adding unseen operation candidates. Unlike the above work, we do not limit ourselves to a pre-designed skeleton with spatial or topological constraints at any network position. Rather, we incorporate search space and architectural knowledge into a neural predictor. Also, instead of manually defining the search units [17; 59; 62; 42], we mine these units from NAS benchmarks. In particular, we utilize Frequent Subgraph Mining (FSM)  to discover interesting and frequent patterns in the computation graphs in a data-driven way. FSM requires conducting expensive steps when graphs are large such as extracting patterns, inspecting isomorphism, and checking if subgraphs are frequent enough to be considered interesting. Algorithms [68; 69; 27] trade result completeness and accuracy for efficiency to overcome run time and memory inefficiency . In NAS,  utilize Weisfeiler-Lehman (WL) graph kernel to extract useful network features but only applies it shallow cell-based DAG structure of NAS-Bench-201. In contrast, we propose an efficient approach to mine frequent subgraphs by converting CGs to sequences and applying BPE  to extract subsequences, which produces a fixed-size vocabulary of subgraphs of varying sizes.

## 3 AutoGO: Automated Computation Graph Optimization

The AutoGO framework operates on the Computation Graph (CG) of an input DNN architecture extracted from the in-memory graph structure of a tensorflow.keras model or.pb file. CGs are directed acyclic graphs (DAGs), where nodes represent primitive operations that are indecomposable computation operations in deep learning frameworks like ONNX  and PyTorch , e.g., Convolutions, Pooling, ReLU, Add, etc., while edges represent forward-passes between operations. Specifically, node features include operation type, input and output tensor resolution dimensions and weight tensor shape if applicable.

Figure 2 provides an overview of the proposed AutoGO operating on the CG level. AutoGO mutates the CG of an input DNN by utilizing a database of _segments_ (Sec. 3.1), which are frequent subgraphs mined from a variety of NAS-Benchmarks via an efficient tokenization method. A Pareto front evolution strategy (Sec. 3.2) performs segment mutations to the CG, while using resolution propagation to ensure network validity. Finally, a pretrained Predecessor-Segment-suCessor (PSC) neural predictor (Sec. 3.3) together with selected hardware metrics will guide the evolution by assessing the performance gain when a certain segment is stitched into the architecture.

### Computation Graph Segmentation via Tokenization

We partition a CG \(g\) into a contiguous sequence of subgraphs, which we call segments. A segment, denoted by \(s\), may have multiple input and output nodes, or could even be a single primitive operation. For \(g\) to be a valid neural network, any two contiguous segments \(s_{i}\) and \(s_{j}\) in \(g\) must maintain the correctly matched height, width and channel \((H,W,C)\) resolutions. The resolutions of output nodes of \(s_{i}\), have to match the resolutions of input nodes of its succeeding segment \(s_{j}\). Our definition of segment spans a wider range of topologies and provides flexible operation grouping than predefined or handcrafted blocks, e.g., ResNet and MBConv blocks or DARTS cells .

The Segment Database \(\) is the core component that stores the segment units that AutoGO mutations are based on. AutoGO uses the segment database \(\) (or vocabulary) to either partition an input CG into segment units or select a segment from \(\) to replace an existing segment in input CG. To alleviate the memory and time complexities of mining common subgraphs from a large number of CGs, we relax the problem into mining segments from sequences. This allows us to utilize much more efficient tokenization techniques over sequences.

Given a set of neural networks represented in a CG format \(=\{g_{1},,g_{k}\}\), we convert each CG into a topologically sorted sequence of nodes. We enrich node representation by labeling a node in the form of _[current op, incoming ops, outgoing ops]_. 3 Each unique node label is further encoded into a single character symbol. Thus, a topologically sorted CG can be mapped into a string of character symbols, where each character encodes an operation and its neighboring operations. By converting all graphs in \(\) into sequences, we essentially have built a corpus for training a tokenizer to extract common segments of character symbols. Specifically, we use Byte-Pair Encoding (BPE), a data compression  technique with a wide use for text tokenization in NLP , to tokenize the string representations of CGs. BPE operates iteratively by collecting frequent pairs of consecutive symbols to build a vocabulary of tokens (segments). Using BPE, we extract the most common subsequences from the string representation of the CGs and build a vocabulary of size \(|V|\). We revert each discovered subsequence in the vocabulary back to its corresponding subgraph representation from the CG to form a segment database \(\). Given a new CG, BPE utilizes its built vocabulary and applies a greedy algorithm to segment it. Figure 1 provides an example of a segmented CG. Our approach brings several benefits over mining on large graphs with WL-kernels. The extraction process on sequences is efficient. Using BPE enables segment extraction from all benchmark families simultaneously without facing memory inefficiencies like WL-Kernel.

Figure 2: AutoGO takes the CG of a neural network as input and improves it using an algorithmlined segment database and pre-trained mutation performance predictor.

### Computation Graph Mutation

We use an evolutionary search strategy to perform segment mutations on the CG of an input architecture and iteratively update a Pareto front of architectures in terms of predicted accuracy and a chosen hardware-friendliness objective, e.g., FLOPs, latency, and power. The mutation made to a parent architecture comprises the following steps: segmentation, source segment selection, replacement segment selection, tensor resolution (shape) propagation, and performance evaluation. First, AutoGO partitions the parent architecture into segments using the BPE-generated vocabulary \(V\). BPE adopts a greedy segmentation approach, which will lead to deterministic partitioning. To diversify the segmentation outcome, we select a subset of vocabulary \(V^{} V\) that BPE uses during segmentation, thus leading to different partitioning outcomes every time the CG is segmented. After segmentation, we select a set of candidate source segments to mutate. For each source segment \(s_{i}\), we randomly select multiple replacement segments \(s_{i}^{*}\) that have the same number of inputs and outputs as the corresponding source segment. If \(s_{i}^{*}\) has multiple inputs and/or outputs, AutoGO randomly maps these to the outputs and/or inputs created by removing \(s_{i}\).

The mutation process must maintain a valid architecture, by correctly combining the replacement segment with the rest of the model. Given a CG \(g\), let \(_{g}=\{s_{0},s_{1},...,s_{n-1}\}\) be the set of disjoint segment subgraphs generated by applying BPE to \(g\). Let \(s_{i},0 i n-1\) be a source segment within \(g\) that we wish to replace. We partition the segments within \(_{g}\) into three distinct groups that reflect their positions within \(g\):

* The **P**redecessor group denotes all segments \(P=\{s_{p};0 p<i\}\) between the input and \(s_{i}\).
* The specific **S**egment \(s_{i}\) that we are aiming to replace by mutating \(g\).
* The suCessor group denotes all the remaining network segments \(C=\{s_{c};i<c n-1\}\).

Let \(\{P,S,C\}\) refer to a CG partitioned in this manner, denoted by grey, purple and orange blocks in Fig. 3. Hence, for a mutation to be valid, the shape of the output tensor from the Predecessor \(P\) must match that of the input to the replacement segment \(s_{i}^{*}\) and the output shape of \(s_{i}^{*}\) must match the shape of the expected input to suCessor \(C\). AutoGO adapts replacement segment \(s_{i}^{*}\) to the remainder of the architecture \(P\) and \(C\) by adjusting the hyperparameters of operations in \(s_{i}^{*}\) to achieve the desired resolutions. Adjustments are applied to mutable operations, e.g., increasing the stride of convolutions and pooling operations to induce downsampling, whereas operations such as activations and batch normalization are immutable. Depending on the \(P\), \(C\) and \(s_{i}^{*}\) subgraphs, stitching the replacement segment into the overall CG may be infeasible. We cast this "resolution propagation" task as a Mixed-Integer Linear Program (MILP) over the adjustable hyperparameters of mutable nodes within \(s_{i}^{*}\), which is an optimization problem with linear objectives and constraints, and integer-valued decision variables. We define MILP constraints that regulate the correct resolution propagation within \(s_{i}^{*}\) when stitched to the rest of the architecture.

At the end of each mutation iteration, we retain all the segment replacements that have a feasible solution to the resolution propagation MILP, and profile these valid segment mutations in terms of the predicted accuracy gain given by the PSC predictor (Sec. 3.3) and a selected hardware friendliness metric, based on which a Pareto front \(\) of architectures is maintained and updated. During the first iteration, we only mutate the input architecture. At the beginning of each successive iteration, AutoGO selects the \(k\) architectures from the Pareto front as parents to mutate. If the Pareto front contains less than \(k\) architectures, AutoGO will select additional non-Pareto optimal architectures having the minimum sum of accuracy and FLOPs ranks. Further technical details on the \(\{P,S,C\}\) partitioning scheme, resolution propagation, parent selection process and overall algorithm, including examples, illustrations and pseudocode, are provided in supplementary Sections A.3.1 and A.5.

Figure 3: An example of segment mutation. AutoGO takes an input CG (left) and replaces a source segment \(s_{i}\) with \(s_{i}^{*}\). The predecessor (grey) and successor (yellow) are unchanged.

### Context-Aware Mutation Performance Estimation

AutoGO uses a neural predictor to estimate the performance of mutant architectures in order to search for high-quality networks. We propose a novel predictor that assesses the potential performance benefit from a segment mutation, based on its topology, context and its location within the architecture.

Just as we can construct subgraphs from the individual segments produced by BPE, we can construct larger predecessor and successor subgraphs from all segments within \(P\) and \(C\), respectively. We use this format to train a **PSC** neural predictor. Let \(h_{*}\) denote a fixed-length graph embedding for an arbitrary graph produced by a graph neural network (GNN) . The PSC predictor operates by separately encoding the predecessor, segment, and successor CGs into distinct graph embeddings. These embeddings are then concatenated and fed into a multi-layer perceptron (MLP) regressor to predict the performance \(y\) of \(g\). Stated more formally:

\[h_{P}=(P);\;h_{s_{i}}=(s_{i});\;h_{C}=(C);\;y= ([h_{P},h_{s_{i}},h_{C}]).\]

It is also possible to predict performance using the entire CG, \(y=((g))\). However, our PSC predictor enjoys several advantages over this approach. By separately encoding the \(\{P,S,C\}\) partitions, the PSC predictor is sensitive to the position and context of the segment \(s_{i}\) within the overall network. This allows us to more directly estimate the performance impact that mutating \(s_{i}\) will have. As shown in Figure 3, we are considering mutating \(g\) into \(g^{*}\) by replacing the source segment \(s_{i}\) with a segment \(s_{i}^{*}\). We want the mutant network to outperform the original, i.e., \(y^{*}>y\). Our training process emphasizes learning a separate embedding for each \(s_{i}\) in our segment database \(\). It encodes the required knowledge to estimate the effect of small changes from segment \(s_{i}\) to any replacement segment \(s_{i}^{*}\), given a fixed \(P\) and \(C\).

## 4 Experimental Results

We construct our database by extracting segments from 5 CIFAR-10  benchmark families: NAS-Bench-101 (NB-101) , NAS-Bench-201 (NB-201) , HiAML, Inception, and Two-Path . Initially, we set the BPE vocabulary size to 2000 and obtain a database with 428 unique segments after filtering out isomorphisms. Segments vary in size ranging from primitives (containing a single operation node) to blocks with up to 16 nodes and edges. The average segment contains 5 nodes and 3 edges, and some segments are disconnected subgraphs spanning parallel branches of a CG. We provide in-depth statistics and visualizations in Section A.1.

In the remainder of this section, we apply AutoGO and our Segment Database to search for better architectures on several NAS benchmarking families to demonstrate the benefits of our framework. We further use AutoGO to improve several open-sourced, popular network architectures. We consider various high-resolution CV tasks, including Classification, Semantic Segmentation and Human Pose Estimation, with the aim of improving hardware friendliness in terms of FLOPs. We also provide examples of deployment where AutoGO minimizes the energy consumption or on-chip latency of already lightweight neural networks for Super Resolution and Image Denoising using a cycle simulator for a Huawei Neural Processing Unit (NPU). We provide implementation details, dataset metrics, and training setup in Sections A.2 and A.4.

### Rank Correlation of Performance Predictors

AutoGO uses a neural predictor to estimate the performance of mutated architectures. We train and evaluate our PSC predictor on five CIFAR-10 benchmarks. Our CG format provides the advantage of training simultaneously on multiple benchmark architecture families. We split each family into training, validation, and testing partitions containing 80%, 10% and 10% of the overall CGs in that family. We combine the training

  
**Arch. Family** & **GNN** & **PSC 1:1 Ratio** & **PSC** \\ 
**NB-101** & 0.627 \(\) 0.031 & 0.666 \(\) 0.025 & **0.849**\(\) 0.054 \\
**NB-201** & 0.809 \(\) 0.016 & 0.865 \(\) 0.015 & **0.983**\(\) 0.003 \\
**HiAML** & 0.010 \(\) 0.013 & 0.170 \(\) 0.042 & **0.734**\(\) 0.031 \\
**Inception** & 0.209 \(\) 0.037 & 0.066 \(\) 0.071 & **0.496**\(\) 0.022 \\
**Two-Path** & 0.023 \(\) 0.018 & 0.236 \(\) 0.043 & **0.724**\(\) 0.022 \\   

Table 1: Test SRCC of all 5 architecture families for the **PSC** predictor and baselines. Results averaged over 5 runs.

partitions for each family to form an overall training set for the predictors while setting the test partitions aside individually. When training the PSC predictor, we partition each CG into multiple \(\{P,S,C\}\) instances to use as training samples. We compare our proposed **PSC** predictor with two baselines: As each CG contains multiple \(\{P,S,C\}\) instances, we consider an intermediate setting, **PSC 1:1 Ratio**, where we only consider one random \(\{P,S,C\}\) instance per CG in the training set. Moreover, we also consider a baseline **GNN** that estimates the performance of whole unpartitioned CGs but is not sensitive to segment-level changes.

We measure the Spearman's Rank Correlation Coefficient (SRCC) of each predictor on the test partitions for each benchmark family. SRCC is defined in the range [-1, 1] and higher values are better. Table 1 summarizes our results. We note the exceptional performance of the PSC predictor as it can obtain SRCC above 0.72 on HiAML and Two-Path while the GNN barely achieves positive SRCC. Moreover, while the GNN and PSC 1:1 predictors can obtain SRCC above 0.8 and 0.6 for NB-201 and NB-101, respectively, if we train the PSC predictors on all \(\{P,S,C\}\) samples, we can obtain a near perfect SRCC of 0.98 on NB-201 and almost 0.85 on NB-101. Overall, these findings demonstrate the merit of our segment decomposition and PSC encoding scheme for CGs.

### Improving CIFAR-10 NAS Benchmark Architectures

We test the effectiveness of AutoGO by refining the best architectures from each family. Specifically, AutoGO aims to increase accuracy while reducing FLOPs. We consider three scenarios that allow us to ablate the effectiveness of our PSC predictor when applied to search. We also compare our segment-level mutation to a simpler, operation-level mutation that mutates single primitive operation.

We run AutoGO for 10 iterations in the segment-level mutation, and 50 iterations for the operation-level mutation for a fair comparison since segments have 5 nodes on average. At the end of each iteration, we randomly select 10 architectures from the accuracy-FLOPs Pareto frontier to qualify for the next iteration as parent architectures. For each parent candidate, we consider up to 100 replacement mutations. We allow AutoGO to mutate sequences of 1 to 3 contiguous source segments simultaneously. We randomly mask out 50% of segments with more than 1 node in our segment database and force BPE to segment the input with the remaining ones. Further, we consider two search settings that limit the FLOPs decrease of child architectures. In the first case, we allow AutoGO to freely reduce FLOPs, while in the second case, we do not allow FLOPs to fall by more than 20% relative to the original network we are optimizing. After the search is complete, we train architectures on the accuracy-FLOPs Pareto frontier 3 times on CIFAR-10 . We report the accuracy and FLOPs of the overall best architecture found across both FLOPs constraint settings. Finally, it takes 45 to 90 minutes to execute the search depending on the size of the input architecture CG. We provide an ablation study across FLOPs constraints, a detailed breakdown of wall-clock time cost, and enumerate our hardware platform in Sections A.6, A.7 and A.9, respectively.

Table 2 reports our findings across all 5 architectures families. We observe that the segment-level mutation is a better fit for finding high-performance architectures, as the best architectures are found using it. For example, on HiAML, it can increase the accuracy by up by 0.43% while reducing

    &  &  &  \\ 
**Family** & **Acc.** & **FLOPs** & **Acc.** & **FLOPs** & **Acc.** & **FLOPs** & **Acc.** & **FLOPs** \\  NB-101 & 95.18\% & 11722 & 95.16\% & 9407 & _95.31\%_ & _10817_ & **95.45**\% & **11118** \\ \(\) & – & – & -0.02\% & -19.75\% & _+0.13\%_ & _-7.72\%_ & **+0.27**\% & **-5.15**\% \\  NB-201 & 93.50\% & 313 & 93.37\% & 232 & _93.57\%_ & _294_ & **93.84**\% & **303** \\ \(\) & – & – & -0.13\% & -25.88\% & _+0.07\%_ & _-6.07\%_ & **+0.34**\% & **-3.19**\% \\  HiAML & 92.32\% & 246 & 92.00\% & 198 & _92.62\%_ & _168_ & **92.75**\% & **198** \\ \(\) & – & – & -0.32\% & -19.51\% & _+0.30\%_ & _-31.71\%_ & **+0.43**\% & **-19.51**\% \\  Inception & 93.20\% & 494 & 92.97\% & 319 & _93.31\%_ & _461_ & **93.52**\% & **474** \\ \(\) & – & – & -0.23\% & -35.43\% & _+0.11\%_ & _-6.68\%_ & **+0.32**\% & **-4.05**\% \\  Two-Path & 87.90\% & 116 & 88.63\% & 106 & **89.16\%** & **48** & _88.94\%_ & _91_ \\ \(\) & – & – & -0.73\% & -8.62\% & **+1.26**\% & **-58.62**\% & _+1.04_\% & _-21.55\%_ \\   

Table 2: AutoGO results across all 5 CIFAR-10 architecture families while aiming to increase accuracy [%] and reduce FLOPs . We consider 3 experimental configurations that vary in unit of mutation and predictor used. We bold and italicize the best and second best result per family.

FLOPs by -19.76%. By contrast, the operation-level mutation only manages to improve performance on Two-Path. However this gain of +0.73% is substantially less than what we can achieve using segment mutation. On NAS-Bench-101, operation mutation manages to break even with the baseline architecture, while incurring an accuracy drop of more than 0.10% on all other families.

Next, we compare the effectiveness of the PSC and GNN predictors. The PSC predictor finds the best architecture in 4 of the 5 architecture families. PSC improves accuracy on NB-101 and NB-201 by 0.34% and 0.27%, respectively. By contrast, the GNN only achieves the best performance on Two-Path, which is the smallest benchmark family with a baseline architecture of only 116 MegaFLOPs. Thus, segment-level mutations span more considerable changes that are distinguishable with GNN and PSC predictors. In sum, the segment-aware encoding is better at increasing performance while reducing FLOPs than the operation-level mutation. Moreover, our results demonstrate the superiority of the PSC predictor compared to the GNN in most cases.

### Application to High-Resolution

**Classification, Segmentation and Pose Estimation**

To demonstrate the extensibility and generalizability of our framework, we apply it to several stand-alone architectures for higher-resolution computer vision tasks. Specifically, we perform NAS using AutoGO with the PSC predictor and segment-level mutation for 5 iterations on ResNet-50, ResNet-101  and VGG-16 . For a fair comparison, we do not allow AutoGO to select segments containing operations that were not available or popularized when the network was first proposed, e.g., depthwise convolutions .

After the search, we examine the architectures on the Pareto frontier and select 1-2 with noticeably different FLOPs reductions to train and evaluate against the original architecture. To form the first point of comparison, we train each network on ImageNet . Then, we fine-tune the network on different tasks. For Semantic Segmentation (SS), we use a PSPNet  head structure and fine-tune on Cityscapes  to obtain mean Intersection over Union (mIoU) performance. For Human Pose Estimation (HPE), we adopt the method of  to fine-tune on MPII  to measure the Percentage of Correct Keypoints (PCK) of an architecture.

Table 3 shows our results on ImageNet, Cityscapes, and MPII. First, we note how in every case, the architectures generated by AutoGO consistently outperform the original on all 3 CV benchmarks.

Figure 4: Example of a segment mutation from that helped create ResNet-50 AutoGO Arch 2 (Tab. 3). A ResNet block is replaced by a HiAML block.

  
**Architecture** & **ImageNet Top-1/5** & **Cityscapes mIoU** & **MPII PCK** & **FLOPs ** & **Lat. [ms]** \\  ResNet-50 Original & 74.02\%/91.22\% & 63.42\% & 82.36\% & 6.29 & 7.18 \\ ResNet-50 AutoGO Arch 1 & 75.34\%/92.16\% & 65.88\% & **84.07**\% & 6.71 & 7.50 \\ ResNet-50 AutoGO Arch 2 & **75.66\%/92.45\%** & **66.65**\% & 82.70\% & 5.88 & 6.92 \\  ResNet-101 Original & 75.09\%/91.94\% & 65.92\% & 82.77\% & 13.76 & 15.86 \\ ResNet-101 AutoGO Arch 1 & **76.56\%/93.09\%** & **67.12**\% & 83.59\% & 13.66 & 15.56 \\ ResNet-101 AutoGO Arch 2 & 75.09\%/92.15\% & 66.38\% & **84.64**\% & 13.35 & 15.36 \\  VGG-16 Original & 74.18\%/91.83\% & 65.36\% & 85.92\% & 30.81 & 4.65 \\ VGG-16 AutoGO & **74.91\%/93.23\%** & **66.91**\% & **85.99**\% & 24.34 & 4.20 \\   

Table 3: Results running AutoGO on Computation Graphs for ResNet-50, 101 and VGG-16. Specifically, we compare ImageNet  Top-1/Top-5 accuracy, Cityscapes test mIoU  using a PSPNet head , MPII  PCK as well as FLOPs. For performance metrics, higher is better. We measure latency on an Nvidia RTX 2080 Ti GPU using an input resolution size of \(224 224\).

For example, ResNet-50 AutoGO Arch 2 outperforms the original by over 1.64% ImageNet top-1 accuracy, while the found architecture on VGG outperforms the original on Cityscapes by 1.55% mIoU. Also, we measure inference latency on an RTX 2080 Ti GPU. We note some correlation between FLOPs and GPU latency; as one metric increases or decreases, so does the other metric.

Figure 4 illustrates how AutoGO splices a HiAML segment into ResNet-50 to create a new architecture. The longer branch performs multiple convolutions at reduced channels, while the shorter branch applies lightweight operations on the original number of channels, and the MILP performs resolution propagation to ensure functionality.

### Application to Super Resolution with EDSR

We use AutoGO to optimize networks for Super Resolution (SR). Specifically, we optimize the backbone feature extractors of EDSR . As the original EDSR only uses convolution and ReLU operations, we do not let AutoGO select segments that contain depthwise, pooling, or batch normalization. Figure 9 (Sec. A.8) provides sample illustrations of the mutations AutoGO performs on EDSR. We train SR networks on DIV2K [2; 29] in the 2x resolution setting and evaluate on several public benchmarks [8; 74; 44; 28; 45; 3]. Table 4 demonstrates how EDSR architectures produced by AutoGO can handily outperform the original network while substantially reducing FLOPs and GPU latency, e.g., AutoGO Arch 3 is 36 gigaFLOPs smaller and 4.2ms faster.

### Using AutoGO to Automate Neural Network Deployment on a Neural Processing Unit

We demonstrate the real-world deployment capabilities of AutoGO by optimizing neural network performance using a cycle-accurate counter that simulates Huawei NPU performance for cellphones . We optimize for power or on-chip latency by pairing our pretrained PSC accuracy predictor with power/latency measurements fed back by either a hardware profiling tool or the cycle-accurate hardware simulator.

**Super Resolution Power Optimization** We use AutoGO to optimize proprietary lightweight SR models similar to FSRCNN . Table 5 reports the network performance on several public datasets as well as the change in power and FLOPs. We note the effectiveness of AutoGO at optimizing the energy efficiency of even a small network, e.g., the FSRCNN-4 AutoGO variant can reduce the

  
**Denoising** & **PSNR** & \(\)**Latency** & **Power [mW]** & \(\)**Power** & **FLOPs ** & **Lat. [ms]** \\  Base Model & 139.4 & – & 724.59 & – & 17.05 \\ AutoGO & **139.9** & -24.94\% & 657.82 & -9.21\% & 16.26 \\   

Table 6: Results of using AutoGO to optimize a Proprietary U-Net Denoising network to improve PSNR and minimize on-chip latency. We report changes in latency and power measured on a mobile NPU using a cycle simulation model.

  
**SR Architecture** & **DIV2K** & **Set5** & **Set14** & **BSD100** & **Urban100** & **Manga109** & **FLOPs ** & **Lat. [ms]** \\  EDSR Original & 36.19 & 36.86 & 32.57 & 31.39 & 29.14 & 36.09 & 141 & 18.04 \\ EDSR AutoGO Arch 1 & **37.28** & **38.01** & **33.62** & **32.18** & **31.56** & **38.49** & 118 & 15.38 \\ EDSR AutoGO Arch 2 & 37.27 & 37.97 & 33.55 & 32.16 & 31.53 & 38.47 & 110 & 14.52 \\ EDSR AutoGO Arch 3 & 37.25 & **38.01** & 33.58 & 32.16 & 31.46 & 38.44 & 105 & 13.81 \\   

Table 4: Peak Signal-to-Noise Ratio (PSNR) for EDSR on the DIV2K validation set and several SR benchmarks in the 2x upscaling setting. Higher is better. We measure latency on an RTX 2080 Ti.

  
**SR Architecture** & **Set5** & **Set14** & **BSD100** & **Urban100** & **Manga109** & **Power [mW]** & \(\)**Power** & **FLOPs** \\  FSRCNN-3 Original & **35.12** & **31.43** & **30.56** & **27.65** & **32.75** & 774.77 & – & 2.67 \\ FSRCNN-3 AutoGO & **35.12** & **31.43** & **30.56** & 27.64 & 32.60 & 644.10 & -16.87\% & 2.09 \\  FSRCNN-4 Original & **35.22** & 31.50 & **30.60** & **27.71** & **32.88** & 892.89 & – & 3.74 \\ FSRCNN-4 AutoGO & 35.17 & **31.52** & **30.60** & **27.71** & 32.77 & 508.37 & -43.06\% & 2.07 \\   

Table 5: SR PSNR results on Proprietary FSRCNN networks. FSRCNN-{3, 4} denotes the number of Conv3x3 operations in the middle of the architecture. We report change in power according to a cycle-accurate simulation model that uses a 64x640 input resolution. FLOPs  are measured using an input resolution of 640x360.

instantaneous power of an already small FSRCNN (with 4 Conv3x3 in the body network) by over 43%, while the FSRCNN-3 AutoGO variant reduces it by over 16%. Moreover, AutoGO maintains or even enhances the PSNR performance as compared to the original networks, demonstrating the generalizability of the pretrained PSC predictor to other tasks.

**Image Denoising Latency Optimization** We use AutoGO to optimize a proprietary Image Denoising U-Net similar to  to reduce on-chip latency. Table 6 reports our findings on an in-house dataset. We observe how the mutated network can exceed the original denoising PSNR by 0.5. While substantially improving latency, we have also reduced other resource consumption metrics including power and FLOPs.

## 5 Limitations and Future Discussions

The AutoGO framework consists of many components: frequent subgraph mining (FSM) via topological sorting and BPE tokenization, the position-aware PSC predictor, the mutation-based evolutionary strategy and use of Computational Graphs. Each of these components has its own strengths and weaknesses. Our paper demonstrates the feasibility of using topological sorting and BPE to perform FSM, although it faces limitations due to the non-deterministic nature of topological sorting, resulting in generation of segments for isomorphic subgraphs that must be filtered out from our database. However, the primary strength of FSM through topological sorting and BPE is the economic advantage of speed, as neither BPE-based segment extraction nor isomorphic segment filtering is time-consuming, even on large Computational Graphs. Moreover, FSM only considers the frequency of a given subgraph (represented as a segment) while ignoring its contribution to performance and hardware-friendliness metrics. Extracting frequent subgraphs that can explain performance is a subject for further studies.

The quality of our segment database depends on the types of operations and subgraphs present in the benchmark families we extract from. For example, a few segments in our database use depthwise convolutions (see Fig. 5 in Sec. A.1) as the only NAS-Benchmark we consider that contains them is Inception, and in limited quantity. These were not widely used in our experiments, since to achieve a fair comparison with the baseline architectures that AutoGO aimed to improve, we constrained the vocabulary of AutoGO during the search to use only same generation of operations. On the flip side, one could use AutoGO to mine newer operations like depthwise convolutions, StarReLU , or even older operations that have gained popularity like GELU . Segments containing these operations could then be used to further refine older architectures like ResNets, EDSR, and FSRCNN for performance improvement.

A future variant of AutoGO could replace the mutation-driven search with a policy network  to select replacement segments. Another avenue for future research is performing frequent and important computational subgraph mining for Transformer and attention-based models for their hardware-friendly deployment, as the Computational Graph representation and subgraph mining presented in this paper are principally designed for convolutional neural networks currently.

## 6 Conclusion

We propose AutoGO, or Automatic Graph Optimization, a new framework for optimizing neural networks outside the bounds of predefined, fixed search spaces. AutoGO represents architectures using a computation graph format of primitive operations. We partition computation graphs into segment subgraphs using Byte-Pair Encoding. Using a segment database and guided by a predictor which is sensitive to segment size and position, AutoGO modifies the network by incrementally mutating its segments while a resolution propagation MILP ensures network functionality. We build a segment database by extracting a vocabulary of segments from 5 open-source NAS benchmarks using Frequent Subgraph Mining. We use AutoGO to improve the accuracy of the best architectures from each of the 5 CIFAR-10 search spaces while reducing FLOPs. Furthermore, we use AutoGO to evolve several open-sourced large CNNs, including ResNets, VGG-16, and EDSR, and successfully improve their performance on a breadth of CV tasks with reduced or comparable FLOPs. Finally, we demonstrate how to utilize AutoGO to automatically reduce the hardware energy consumption and on-chip latency of realistic convolutional neural network applications, when deployed onto a mobile Neural Processing Unit.