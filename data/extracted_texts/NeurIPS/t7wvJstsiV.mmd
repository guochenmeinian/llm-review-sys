# SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models

Jianyi Zhang1, Da-Cheng Juan2, Cyrus Rashtchian2, Chun-Sung Ferng2, Heinrich Jiang2,

**Yiran Chen1**

1 Duke University, 2 Google Research

Project Website

###### Abstract

Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce **S**elf **L**ogits **E**volution **D**ecoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to 20% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.

## 1 Introduction

Large Language Models (LLMs) have achieved remarkable breakthroughs in recent years, demonstrating exceptional performance across various domains . However, a significant challenge associated with LLMs is their tendency to hallucinate or distort the truth, resulting in outputs that are not factual . This issue of hallucination undermines the reliability and trustworthiness of LLMs in practical applications. A popular strategy for improving the LLM factuality involves refining the decoding process . Decoding focuses on how the model selects the next token during the generation process, which can significantly influence the factual accuracy of the output. The decoding methods can be cost-effective since (a) they do not rely on external knowledge and (b) no additional training is required. Furthermore, decoding methods can be synergistically combined with other techniques aimed at improving the LLM factuality, such as retrieving information from external knowledge bases , various fine-tuning strategies for better alignment , or ensemble learning methods .

Figure 1: Factuality decoding overview.

Recent studies  suggest that LLMs sometimes have learned the factual content based on extensive pretraining or fine-tuning, although they fail to produce the correct answer when a user queries the model. This has inspired the development of several factuality decoding methods  to reveal what the model implicitly "knows." Figure 1 summarizes the underlying mechanism of these factuality decoding methods. The LLMs' output distribution is derived by applying the softmax function to the output logits from the final layer. During the training phase, this distribution is optimized based on the real-world factuality distribution represented by the training dataset. However, during the inference phase, "what the LLM tells" might still contain factual errors, which implies a discrepancy between the output distribution and the real-world factuality distribution. While the real-world distribution remains inaccessible during the inference phase, the model's latent knowledge ("what the model knows") may have implicitly learned some factual content correctly during the training phase . Therefore, a key challenge for factuality decoding strategies lies in effectively harnessing the latent knowledge embedded within LLMs to refine the output distribution (logits) during inference.

To address this challenge, we propose **S**elf **L**ogits **E**volution **D**ecoding (SLED), a novel factuality decoding approach that leverages the latent knowledge within LLMs by contrasting the final layer's logits with early layers' logits. During the decoding process, as LLMs progress from early to final layers, they progressively incorporate factual information stored in each layer into the output. SLED tracks this evolution process to unearth the latent knowledge within LLMs, and enables the "self-evolution" of the output distribution further to align it more closely with real-world facts. Furthermore, our approach recognizes that the latent knowledge within LLMs, while valuable, may not always be perfect. Therefore, instead of simply replacing the original outputs with this latent knowledge, SLED integrates it into the original logits through an operation similar to "single-step gradient descent" over the output logits during the inference time. This operation minimizes the Kullback-Leibler (KL) divergence between the latent knowledge distribution and the output distribution, effectively balancing the two and mitigating potential drawbacks such as overfitting or biased outputs. Figure 2 illustrates the SLED workflow, highlighting how SLED optimizes the output logits, leading to a more factual output distribution. We evaluate SLED on various LLMs (e.g., LLMaA 2 , LLMaA 3 , Gemma ) and benchmarks to demonstrate its state-of-the-art performance in layer-wise contrastive decoding methods. In summary, our main contributions are:

* We propose SLED, a novel decoding method that aligns LLMs outputs with factual knowledge without requiring an external knowledge base or fine-tuning data.
* We conduct extensive experiments across a range of LLMs, with varying configurations and scales. The results demonstrate that SLED consistently improves factual accuracy on various tasks and benchmarks, including multiple-choice, open-ended generation, and chain-of-thought reasoning tasks.
* SLED can be flexibly integrated with other factuality decoding methods to enhance their effectiveness further.
* We provide a new interpretable perspective for understanding layer-wise contrastive decoding methods, paving the way for further developments in factuality decoding.

Figure 2: Illustration of our Self Logits-Evolution Decoding (SLED) workflow.

## 2 Self Logits Evolution Decoding

A large language model, equipped with \(N\) layers and a vocabulary \(=[v_{1},v_{2},,v_{d}]\), typically generates text in the next-token prediction fashion. For each given prefix, the model computes the logits at the final (\(N\)-th) layer, \(_{N}(_{(1,N)},_{(2,N)},,_{(d,N)})\), which are obtained by applying a linear transformation to the hidden states of the final layer, projecting the high-dimensional hidden state vectors onto the space of the vocabulary size. Subsequently, the output distribution \(_{logits_{N}}\) at the final (\(N\)-th) layer for the next token is derived by applying softmax function on the logits,

\[_{logits_{N}}(p_{(1,N)},,p_{(d,N)})= softmax(_{N}/),\]

where \(\) is the temperature parameter. Therefore, for each \(p_{(i,N)}\)\((1 i d)\), we have

\[p_{(i,N)}=(_{(i,N)}/)/S,\ S=_{j=1}^{d} (_{(j,N)}/).\]

Similarly, we can also derive the logits from early layers by applying the same linear transformation mentioned above to their hidden states. For any early layer \(n\)\((n<N)\), we denote its logits as \(_{n}(_{(1,n)},,_{(d,n)})\) and the corresponding distribution as \(_{logits_{n}}(p_{(1,n)},,p_{(d,n)})\).

### Logits Evolution

To improve factual accuracy, it is crucial that the correct token \(v_{i}\) receives a higher value of \(_{N}\) to ensure a higher probability value \(p_{(i,N)}\) in the output distribution \(_{logits_{N}}\). From a mathematical perspective, this means aligning the model's output distribution \(_{logits_{N}}\) closely with the real-world factuality distribution \(_{real}\). Specifically, we can formulate this goal as optimizing the following loss function \(\) regarding the \(\):

\[() KL(_{real},_{ logits}),=(_{1},...,_{d}),\ _{logits}=softmax(/)\] (1)

We describe the above optimization as **Logits Evolution**. Interestingly, the training of LLMs also aims at minimizing the divergence (typically the \(KL\) divergence, as the training loss function is often the cross-entropy loss) between the ground truth \(_{real}\) and the output distribution \(_{logits_{N}}\). During the training phase, the logits evolution is driven externally by the real-world distribution \(_{real}\) presented in the training dataset, and the corresponding solution is \(=_{N}\). However, \(_{real}\) is not accessible during the inference phase. To address this challenge, SLED utilizes the model's latent knowledge to estimate \(_{real}\) and enables "self-evolution" of the logits. We denote the estimation as \(_{latent}\) and the self logits evolution can be achieved by the following gradient-descent operation:

\[}_{N}=_{N}-_{ logits_{N}}KL(_{latent},_{logits_{N}}).\] (2)

The parameter \(\), termed the **Evolution Rate**, governs the magnitude of adjustments applied to \(_{N}\) in the direction of the gradient \(_{logits_{N}}KL(_{latent},_{logits_{N}})\). In the following Section 2.2 and 2.3, we discuss how we derive the \(_{latent}\) as the estimation of the real-world distribution \(_{real}\)

Figure 3: We analyze the next-token predictions of three LLaMA-2-base models using the logits from each layer individually. This analysis is performed on 200 true claims from the FACTOR dataset. The results verify that the logits distribution at the final layer is closer to the real-world distribution than all the early layers in terms of KL divergence.

### Estimate \(_{real}\) by Tracking the Logits Evolution Direction throughout Layers

The core principle of our method involves leveraging the difference between each early layer's logits and the final layer's logit, \({ logits}_{n}-{ logits}_{N}\) to approximate the gradient of \(KL(_{real},_{logits})\) at \({ logits}={ logits}_{n}\). Then we estimate \(_{real}\) based on this approximation.

This is inspired by a new perspective of interpreting the training phase of LLMs as the evolution of logits described in Problem 1. As mentioned above, the solution derived by the training phase is the final layer's logits \({ logits}={ logits}_{N}\), since the final layer's \({ logits}_{N}\) directly engage with the real-world distribution \(_{real}\) through the loss function in training. This implies that we can generally consider the final logits \({ logits}_{N}\) to be a better solution than the logits from an early layer \({ logits}_{n}\), with \(KL(_{real},_{logits})<KL(_{real},_{ logits})\). We present some examples in Figure 3 to demonstrate this. Based on this discussion, if we contrast the final layer's logits with the early layer's logits, we can consider the direction (orientation) of \({ logits}_{n}-{ logits}_{N}\) can approximately align with the direction of the gradient \(_{{ logits}}KL(_{real},_{logits})|_{{ logits}={ logits}_{n}}\). To further verify this motivation, we calculate the cosine similarity between \({ logits}_{n}-{ logits}_{N}\) and \(_{{ logits}_{n}KL(_{real},_{logits})}\) for thousands of tokens across different models in Figure 7. We find that the majority of these values are positive, which means that the directions of these two vectors are close.

Hence, for each early layer \(n\), we propose to maximize the following function of cosine similarity and derive the \(_{latent}^{(n)}\) to estimate the \(_{real}\).

\[_{latent}^{(n)}=_{}(({ logits}_{n}-{ logits}_{N},_{logits}KL(,_{logits}),0)\] (3)

### Achieving the Self Logits Evolution in Three Phases

Based on the above analysis, we can introduce the procedures of SLED: First, we estimate \(_{latent}^{(n)}\) for each early layer \(n\) using the gradient approximation in Section 2.2. Subsequently, we apply a weighted average on \(\{_{latent}^{(n)}\}\) across all early layers \(n<N\) to derive \(_{latent}\), which serves as the final estimation of the real-world distribution. Finally, we apply \(_{latent}\) in Equation 2 to facilitate the self-evolution of \({ logits}_{N}\), thereby derive the updated logits, \({}_{N}}\).

\[{ logits}_{n}-{ logits}_{N} }}{{}}_{{ logits}_{n}}KL(_{real},_{ logits})\] \[}}{{}} _{latent}^{(n)} }}{{}} _{latent} }}{{}}}_{N}\]

Phase 1:An exhaustive search for an exact solution to the complex optimization problem (Equation 3) is computationally impractical. We can reduce the solution space by the following. Suppose the real-world factuality distribution dictates that the next word to be generated is the \(i\)-th token \(v_{i}\) from the vocabulary \(\). Thus \(_{real}=_{e_{i}}\), where \(_{e_{i}}\) represents a standard basis vector (one-hot vector) with the \(i\)-th component set to 1 and all other components set to 0. Then, we can simplify the aforementioned optimization problem by limiting the solution space to \(\{_{e_{i}}\}_{i=0}^{d}\) and decide which token \(i\) should be selected. The corresponding gradient when \(=_{e_{i}}\) has the following formulation.

**Proposition 1**.: _The gradient of \(KL(_{e_{i}},_{logits})\) at \({ logits}={ logits}_{n}\) is:_

\[_{{ logits}_{n}}KL(_{e_{i}},_{{ logits}_{n}} )=(_{{ logits}_{n}}-_{e_{i}})/=(p_{(1,n)}, ,p_{(i,n)}-1,,p_{(d,n)})/\] (4)

We calculate the cosine similarity between the gradient \(_{{ logits}_{n}}KL(_{e_{i}},_{{ logits}_{n}})\) and the difference \({ logits}_{n}-{ logits}_{N}\) for each token in the vocabulary \(\). Then we select the \(_{e_{i^{*}}}\) of which the gradient is closest to \({ logits}_{n}-{ logits}_{N}\) as the estimation \(_{latent}^{(n)}\). Mathematically, this involves selecting \(i^{*}\) according to the following criterion

\[i^{*}=_{1 i d}_{i}^{(n)},_{i}^{(n)}=(({ logits}_{n}-{ logits}_{N},_{logits}-_{e_{i}}),0),\]

and adopting \(_{latent}^{(n)}=_{e_{i^{*}}}\) as the "hard estimation" of \(_{real}\). Drawing from the concept of hard and soft targets in label smoothing and knowledge distillation, we further extend it to the "soft estimation",

\[_{latent}^{(n)}=(m_{1}^{(n)},,m_{i}^{(n)},,m_{d}^{(n)})/m^{ (n)},m_{i}^{(n)}=(_{i}^{(n)})^{2}m^{(n)}=_{i=1}^{d}m_{i}^{(n)}\]

We square \(\{_{i}^{(n)}\}\) to moderately amplify their differences. Prior studies prove that soft targets usually offer stronger generalization capabilities, more information, and more robustness to noise than hard targets . Hence, we adopt the soft estimation in lieu of the hard estimation.

Example from GSM8K demonstrating SLED's mechanism. SLED derives the estimations \(_{latent}^{(n)}\) by contrasting final layer's logits \(logits\) with early layers' logits \(\{ logits_{n}\}\). We list the token with the highest probability value from the \(_{latent}^{(n)}\) for different early layers. As shown, SLED downplays incorrect tokens by assigning lower weights \(s^{(n)}\) to the corresponding \(_{latent}^{(n)}\). Conversely, if the estimation is correct, the weights are relatively larger. The parameter evaluation scale is set to 2.

Phase 2:We ensemble \(_{latent}^{(n)}\) across all layers by computing a weighted average of the set \(\{_{latent}^{(n)}\}\) and adopt it as the final estimation of the \(_{latent}\):

\[_{latent}=_{n=0}^{N}s^{(n)}_{latent}^{(n) },s^{(n)}=m^{(n)}/(_{n=0}^{N}m^{(n)})\]

This estimation suggests that the weight \(s^{(n)}\) of certain layer \(n\) will be larger if the corresponding gradient approximation \(logits_{n}-logits_{N}\) is more closely aligned with the gradients \(\{_{logits},KL(_{e_{i}},_{logits_{n}})\}\) for the tokens in the vocabulary. This in turn amplifies the influence of layer \(n\) on the final estimation, which is a desirable effect in our method. Figure 4 demonstrates that SLED can downplay incorrect tokens based on the gradient alignment. One can further validate that for each component \(m_{i}\) in the final estimation \(_{latent}(m_{1},m_{2},,m_{d})\), the following relationship holds: \(m_{i}=_{n=0}^{N}m_{i}^{(n)}/(_{n=0}^{N}_{j=1}^{d}m_{j}^{(n)}).\) This property simplifies the description in Algorithm 1.

Phase 3:Applying \(_{latent}\) in Equation 2 enables us to derive the gradient necessary for steering the self-evolution on the final layer's logits \( logits_{N}\).

**Proposition 2**.: _The gradient of \(KL(_{latent},_{logits})\) at \(logits=_{N}\) is:_

\[_{logits_{N}}KL(_{latent},_{logits_{N}})=( _{logits_{N}}-_{latent})/=(p_{(1,N)}-m_{1}, ,p_{(d,N)}-m_{d})/\]

_Then we can derive the self-evolved logits \( logits_{N}\)_

\[_{N}(_{(1,N)},,_{( i,N)},,_{(d,N)}),_{(i,N)}=_{(i,N)}-(p_{(i,N)}-m_{i})/.\] (5)

### Computational Complexity and Design Decisions

For each layer, computing \((logits_{n}-logits_{N},_{logits_{n}}-_{e_{i}})\) for every token \(v_{i}\) in the vocabulary \(\) needs \((d^{2})\) operations. To reduce the computational complexity, we select only a subset \(_{I_{k}}\), where the token \(v_{i}_{I_{k}}\) has the top-\(k\) highest logits in the final layer. In this scenario, we only initiate the self-evolution in Equation 2 of the logits corresponding to these top-\(k\) tokens. For the remaining tokens, which have lower probabilities, their logits are adjusted to a very lower numerical value, _e.g._, \(-1000\). This strategy significantly reduces the computational complexity, while maintaining focus on the most relevant tokens. We name the parameter \(k\), as **Evolution Scale**, since it determines the number of top-probability tokens active for self-evolution.

_Q 2.1: Why SLED contrast the final layer with all the early layers, instead of picking one premature layer to contrast based on JSD?_

DoLa selects a subset of early layers to form a candidate set. Then it calculates the Jensen-Shannon Divergence (JSD) between the final layer and each layer in this set. Their strategy is to choose the

Figure 4: An example from GSM8K demonstrating SLED’s mechanism. SLED derives the estimations \(_{latent}^{(n)}\) by contrasting final layer’s logits \( logits\) with early layers’ logits \(\{ logits_{n}\}\). We list the token with the highest probability value from the \(_{latent}^{(n)}\) for different early layers. As shown, SLED downplays incorrect tokens by assigning lower weights \(s^{(n)}\) to the corresponding \(_{latent}^{(n)}\). Conversely, if the estimation is correct, the weights are relatively larger. The parameter evaluation scale is set to 2.

layer with the highest JSD as the premature layer, and the chosen layer will be contrasted with the final layer to update probabilities. Obviously, if this strategy is reasonable, a larger candidate set should lead to a better choice of the premature layer and, consequently, enhanced overall performance. However, a paradoxical finding from their experimental results, which our tests also confirm in the discussion in Section 3.5, is that a larger candidate set for DoLa leads to decreased performance. Specifically, when the candidate set for DoLa ranged from 0 to 32 layers for LLaMA-2-7B-Base, the performance was inferior compared to a smaller set of 0 to 16 layers. This fundamental flaw indicates that selecting a good candidate set remains a challenge when applying DoLa. In contrast, our method does not face this concern as it applies an ensemble approach to all early layers. It is also important to note that our method works well even when only contrasting the final layer with part of the early layers, as demonstrated in Section 3.5 and B, proving the robustness of our approach.

_Q 2.2: Why not use \(_{latent}\) directly as the model's output distribution?_

It is crucial to understand that \(_{latent}\) is merely an estimation of the real-world distribution based on the model's latent knowledge instead of the exact \(_{real}\). Consequently, relying solely on \(_{latent}\), similar to DoLa, might lead to inaccuracies, as the latent knowledge can be imperfect. The original logits \({ logits}_{N}\) are still important as they are refined directly by real-world data during training. The evolution rate \(\) in Equation 2, serves to balance this trade-off, enabling a reciprocal enhancement between \(_{latent}\) and the original \({ logits}_{N}\). More ablation studies are provided in Section 3.5 and B.

_Q 2.2: Considering that SLED adopts \({ logits}_{n}-{ logits}_{N}\) as the estimation of the gradient, why not directly apply it in Equation 2?_

It is important to note that while \({ logits}_{n}-{ logits}_{N}\) is unconstrained, the gradients estimated in Equation 2 (e.g., \(p_{(1,N)}-m_{1},,p_{(d,N)}-m_{d}\)) are constrained within \([-1,1]\). Thus, direct substitution could lead to a mismatch in magnitudes and might also introduce unexpected noise. Proper normalization and subsequent aggregation of estimations from different layers are precisely what our method addresses in Section 2.2 and 2.3. Further analysis is provided in Section B.

## 3 Experiments

As a novel layer-wise contrastive decoding approach, we first benchmark SLED against the state-of-the-art approach DoLa  across a diverse range of model families (LLaMA 2, LLaMA 3, Gamma) and model scales (from 2B to 70B), including the more advanced mixture of experts (MoE) architecture, as detailed in Section 3.2 and 3.3. The results showcase notable factuality improvements across a variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. Then, in Section 3.4, we integrate our method with other established factuality decoding techniques, illustrating that SLED can further enhance their performance. In Section 3.5, we further conduct in-depth studies on mitigating the repetition issue, layer selection, various parameter settings, and latency overhead to gain more comprehensive insights into SLED's performance. Wealso extend our analysis with additional ablation studies and results across more benchmarks in Section B and D in the Appendix, and provide several examples of generated text as the qualitative study in Section C.

### Experimental Setup

BenchmarksWe compare our method with baselines on several multiple-choice and open-ended generation tasks. For multiple-choice question tasks, we use the TruthfulQA  and FACTOR (Wiki)  datasets to assess the LLMs' factuality in short-answer/long-paragraph scenario, respectively. For open-ended generation tasks, we adopt TruthfulQA  and tasks involving chain-of-thought reasoning : StrategyQA  and GSM8K .

Models & BaselinesWe evaluate the performance of SLED on six LLaMA-2 models  ({7B,13B,70B}-Base, {7B,13B,70B}-Chat), four LLaMA-3 family models  ({8B,70B}-Base, {8B,70B}-IT), two Gemma models (2B,7B), two MoE models (Mixtral-8\(\)7B-IT) . We adopt the following baselines: 1) standard decoding (greedy decoding or sampling depending on the tasks), 2) DoLa , 3) Inference Time Intervention (ITI) , 4) Activation Decoding (AD) , 5) Contrastive Decoding (CD) , and 6) Induce-then-Contrast Decoding (ICD) .

MetricsWe adopt the factual accuracy evaluation implemented in  for multiple-choice tasks and chain-of-thought reasoning tasks. For the open-ended generation task on TruthfulQA, we follow the evaluation procedure in , using "finetuned-GPT3-judge"s to measure the truthfulness, informativeness, and rejection rate of generated outputs respectively.

### Evaluation on a Broad Range of LLM Benchmarks

Multiple-Choices TasksThe objective of these tasks is to employ decoding methods that enable LLMs to assign higher probabilities to correct completions/answers over incorrect alternatives. We demonstrate the effectiveness of SLED for both Short-Answer Factuality on the TruthfulQA and Long-Paragraph Factuality on the FACTOR dataset. For both DoLa and our SLED, we contrast the results from the final layer against all preceding layers. We randomly sample approximately 5% of the data for validation regarding parameter selection. The results, as shown in Table 1, indicate that SLED achieves superior outcomes in almost all metrics across six LLaMA-2 models. Notably, SLED

    &  &  &  &  \\   & MC1 & MC2 & MC3 & & \%Truth & \%Info & \%T*I & \%Reject & StrQA & GSM8K \\  LLaMA-2-7B-Base & 33.17 & 59.42 & 31.78 & 58.15 & 32.80 & 90.09 & 23.99 & 8.45 & 60.96 & 14.03 \\ +DoLa & 32.56 & **63.03** & 30.57 & 62.49 & 35.74 & **95.23** & 32.31 & 2.57 & 60.61 & 14.71 \\ +SLED (ours) & **34.15** & 62.57 & **31.89** & **67.27** & **55.81** & 94.61 & **52.87** & **0.12** & **61.31** & **15.01** \\  LLaMA-2-7B-Chat & 35.62 & 57.46 & 32.07 & 56.78 & 59.24 & 78.95 & 38.68 & 17.50 & 63.67 & 21.08 \\ +DoLa & 33.41 & 61.93 & 30.35 & 56.65 & 58.02 & 87.03 & 45.78 & 13.10 & 64.32 & 21.00 \\ +SLED (ours) & **37.08** & **63.86** & **32.90** & **64.70** & **67.07** & **88.13** & **55.69** & **11.02** & **64.67** & **21.15** \\  LLaMA-2-13B-Base & 33.69 & 62.75 & 31.74 & 63.69 & 31.21 & 91.55 & 23.26 & 7.96 & 66.07 & 28.66 \\ +DoLa & 29.25 & 62.13 & 30.29 & 57.08 & 37.58 & 92.41 & 30.11 & 7.47 & 65.55 & 18.88 \\ +SLED (ours) & **34.15** & **63.62** & **31.89** & **70.91** & **38.31** & **94.85** & **33.29** & **5.02** & **66.81** & **29.34** \\  LLaMA-2-13B-Chat & 36.47 & 63.05 & **32.77** & 62.06 & 60.34 & 86.54 & 47.12 & 13.59 & 69.87 & 36.47 \\ +DoLa & 34.52 & 63.24 & 31.48 & 58.08 & 60.22 & 90.33 & 51.16 & 9.67 & 67.90 & 34.57 \\ +SLED (ours) & **37.09** & **63.75** & 32.60 & **67.50** & **63.65** & **95.23** & **58.87** & **5.26** & **69.96** & **36.54** \\  LLaMA-2-70B-Base & 33.66 & 61.10 & 32.33 & 72.78 & 55.45 & 62.55 & 18.48 & 36.74 & 75.20 & 56.33 \\ +DoLa & 26.93 & 60.33 & 29.42 & 61.92 & **60.95** & 70.62 & 32.07 & 17.72 & 73.45 & 43.37 \\ +SLED (ours) & **35.13** & **64.92** & **33.52** & **77.49** & 59.24 & **82.99** & **43.70** & **13.10** & **75.20** & **57.09** \\  LLaMA-2-70B-Chat & 35.98 & 64.18 & 32.99 & 69.07 & 49.57 & 81.27 & 31.33 & 29.13 & 77.25 & 54.59 \\ +DoLa & 31.58 & 54.40 & 32.31 & 58.28 & 61.44 & 77.97 & 39.90 & 21.28 & 74.41 & 49.05 \\ +SLED (ours) & **38.31** & **66.71** & **34.66** & **73.98** & **62.55** & **84.70** & **47.74** & **14.98** & **77.38** & **54.81** \\   

Table 1: Comparison on LLaMA 2 model family. The best results are in bold for each dataset/metric. SLED outperforms DoLa and vanilla greedy decoding.

achieves better performance under the MC1/MC3 metrics on TruthfulQA, which are more sensitive to fluctuations and pose a greater challenge. For long sentences in FACTOR, our method shows improvements over baselines by 5-13%. These results not only underscore the benefits of our method for factuality but also demonstrate its robustness across different lengths of text.

Open-Ended Generation TasksIn open-ended settings, we prompt the model to generate answers for the same questions from TruthfulQA, following the settings outlined in [29; 7; 27]. In Table 1, we compare the performance of six LLaMA-2 models using standard greedy decoding, (greedy) DoLa, and (greedy) SLED. All the generated answers are then evaluated by a fine-tuned GPT-3 model for both truthfulness and informativeness scores. Considering that a 100% truthful score can be easily achieved by simply responding with '1' have no comment,' which would result in a 0% informative score and thus is not very useful, we have introduced additional metrics--%Truth \(\) Info and the rejection ratio %Reject --to demonstrate that SLED is a mutual-gains approach to achieve better both truthful and informative scores. We have improved the overall %Truth x Info scores by 3-20% across different models and reduced the rejection ratio by up to 95%. These enhancements demonstrate that our method effectively avoids the'rejection pitfall,' making it more helpful.

Adaptation to Chain-of-thought Reasoning TasksAlthough the StrategyQA and GSM8K tasks are also open-ended and require factual accuracy, the primary focus here is to evaluate how different decoding methods adapt to the Chain-of-Thought (COT) approach for handling complex reasoning tasks. We maintain a repetition penalty of 1, as we will discuss the repetition flaws associated with DoLa in Section 3.5. StrategyQA demands multi-hop reasoning, and as shown in Table 1, our method boosts accuracy across six models, whereas DoLa generally worsens it without a repetition penalty. GSM8K, a benchmark for math word problems that require arithmetic reasoning, also shows consistent accuracy improvement with SLED in 7B, 13B and 70B models.

### Evaluation Across Diverse LLM Configurations

As discussed above and shown in Table 1, our method, SLED, demonstrates strong generalization capabilities across the LLaMA-2 model family, proving robust from 7B to 70B model sizes. In Table 2, we further showcase SLED's impressive performance on the more recent LLaMA-3 family models, both at 8B and 70B sizes, in terms of long paragraph factuality and short answer factuality. Interestingly, SLED is also applicable to different pre-trained models, such as Gemma at both 2B and 7B sizes, and can even be adapted to the increasingly popular Mixture of Experts (MoE) architectures. These results confirm the exceptional adaptability of our method across various LLM configurations.

### Evaluation on Integrating SLED with Other LLM Factuality Decoding Methods

SLED exclusively focuses on contrasting differences between layers without altering other parts of the model. Thus, it remains compatible with other techniques that incorporate additional strategies or utilize auxiliary models. This compatibility allows SLED to be seamlessly integrated into existing

    &  &  &  &  &  \\   & & MC1 & MC2 & MC3 & & & MC1 & MC2 & MC3 \\  LLaMA-3-8B & 64.33 & 33.78 & 63.00 & 32.59 & Mixtral-8\(\)7B & 71.41 & 35.13 & 49.98 & **34.17** \\ +DoLa & 68.04 & 33.29 & 63.35 & 32.16 & +DoLa & 58.28 & 32.44 & 35.91 & 33.68 \\ +SLED (ours) & **68.67** & **35.13** & **64.09** & **32.50** & +SLED (ours) & **74.92** & **35.86** & **57.26** & 32.96 \\  LLaMA-3-8B-IT & 59.49 & 38.92 & 68.16 & 36.50 & Mixtral-8\(\)7B-IT & 70.51 & 37.94 & 62.51 & 35.25 \\ +DoLa & 61.06 & 35.86 & 65.30 & 33.78 & +DoLa & 56.15 & 32.19 & 39.17 & 33.76 \\ +SLED (ours) & **67.17** & **42.23** & **69.03** & **37.97** & +SLED (ours) & **75.55** & **41.73** & **68.52** & **37.70** \\  LLaMA-3-70B & 78.72 & 35.62 & 65.66 & **34.18** & Gemma-2B & 50.87 & 23.38 & 37.16 & 17.42 \\ +DoLa & 77.56 & 33.29 & 64.83 & 32.81 & +DoLa & 32.93 & **26.07** & 48.97 & 26.55 \\ +SLED (ours) & **80.83** & **37.58** & **66.19** & 34.11 & +SLED (ours) & **57.05** & 25.21 & **50.20** & **26.94** \\  LLaMA-3-70B-IT & 73.95 & 44.80 & 70.29 & 41.02 & Gemma-7B & 60.42 & 31.58 & 47.63 & 22.75 \\ +DoLa & 71.51 & 38.43 & 68.70 & 35.21 & +DoLa & 36.07 & 25.21 & 43.14 & **26.13** \\ +SLED (ours) & **76.85** & **48.35** & **74.03** & **43.16** & +SLED (ours) & **65.56** & **32.31** & **49.88** & 25.22 \\   

Table 2: Using SLED with other LLM families also improves the factuality.

methods, enhancing factuality further without the need for modifications to SLED. We integrate SLED with the following approaches: ITI, AD, CD and ICD. Table 3 shows that SLED leads to accuracy improvements from 1% to 12% across four LLaMA-2 models.

### Ablation Studies and Analysis

Mitigating Repetition IssuesTable 4 demonstrates that our method, SLED, effectively addresses a significant issue in DoLa: repetitive content in open-ended generation tasks. Our approach outperforms DoLa without the need for excessive repetition penalty. While a slight increase in the repetition penalty further enhances the performance of our method, excessive penalties, such as 1.1, tend to degrade it. This suggests that SLED does not inherently require heavy adjustments for repetition issues. In contrast, DoLa's performance improves with higher penalties (e.g., 1.1, 1.2, 2), indicating a more critical need for addressing repetitive content. We also employ two intuitive metrics, Repetition-4 and Repetition-Sen, to gauge the severity of repetition issues, following prior research . Regardless of the repetition penalty imposed, our method consistently exhibits lower repetition rates. Table 7 includes some examples of generated text to illustrate this further.

Layer SelectionAs discussed in Section 2.4, how to choose a good candidate set is still a paradoxically difficult task when applying DoLa. Our method does not exhibit this issue. Instead of selecting a single premature layer from the candidate set like DoLa, SLED contrasts the final layer with all layers in the candidate set and then ensembles all the results. Figure 5 shows that setting a larger candidate set, such as all the 32 layers for LLaMA-2-7B-Base, yields better performance than focusing solely on either the first \([0,16)\) or second half \([16,32)\). This implies that our layer-wise contrast approach captures more useful information in a more scientific manner. Furthermore, our tests confirm the robustness of our method even when the candidate set is minimal, such as a single layer, consistently demonstrating strong performance. Our settings mirror those of DoLa.

Parameter AnalysisWe next investigate the impact of parameters -- evolution rate \(\) and evolution scale \(k\) -- on the performance of SLED using a subset of the FACTOR dataset. We test evolution rates from \(\{0.01,0.1,1,2,5,10\}\) and evolution scale values from \(\{5,10,20,50\}\). Without extreme

   Metric & Method & 1 & 1.02 & 1.04 & 1.06 & 1.08 & 1.1 & 1.2 & 2 \\   & DoLa & 65.55 & 65.98 & 66.37 & 65.98 & 65.59 & 66.37 & 67.16 & 66.64 \\  & SLED (Ours) & 66.81 & **69.39** & 68.51 & 68.47 & 67.07 & 65.72 & 60.87 & 54.75 \\   & DoLa & 7.63 & 7.19 & 6.45 & 5.98 & 5.50 & 5.10 & 3.73 & 2.05 \\  & SLED (Ours) & **3.73** & **2.45** & **1.89** & **1.36** & **1.05** & **0.69** & **0.20** & **0.10** \\   & DoLa & 2.16 & 2.04 & 1.66 & 1.37 & 1.12 & 0.89 & 0.23 & 0.03 \\  & SLED (Ours) & **0.88** & **0.39** & **0.10** & **0.02** & **0.03** & **0** & **0** & **0** \\  

Table 4: Accuracy of LLaMA 2 13B Base on StrategyQA with Varying Repetition Penalties

   Model &  &  \\   &  & AD & AD &  & AD & AD &  &  & ICD \\  & & +DoLa & +SLED & & +DoLa & +SLED & & +SLED & & +SLED \\  MC1 & 32.80 & 25.58 & **33.29** & 35.37 & 33.41 & **36.23** & 36.60 & **43.33** & 46.32 & **46.87** \\ MC2 & 59.59 & 39.06 & **62.55** & 58.14 & 50.31 & **63.15** & 65.62 & **65.75** & 69.08 & **72.09** \\ MC3 & 31.05 & 17.89 & **31.80** & 31.84 & 23.15 & **32.23** & 34.89 & **37.66** & 41.25 & **43.64** \\   Model &  &  \\   &  & AD & AD &  & CD &  & AD & AD &  & CD \\  & & +DoLa & +SLED & & +SLED & & +DLa & +SLED & & +SLED \\  MC1 & 33.90 & 24.72 & **33.90** & 30.11 & **33.78** & **36.84** & 34.72 & 36.35 & 28.15 & **36.47** \\ MC2 & 62.93 & 37.74 & **63.69** & 50.31 & **63.22** & 63.75 & 50.42 & **64.83** & 54.87 & **64.93** \\ MC3 & 31.61 & 17.66 & **31.38** & 28.18 & **32.21** & 32.69 & 23.83 & **32.85** & 29.75 & **33.39** \\   

Table 3: Comparison of decoding strategies on TruthfulQA datasets. SLED can also be seamlessly combined with other decoding strategies to improve performance further.

evolution rates (e.g., 10), our method performs well, confirming its robustness. As analyzed in our methodology and Eq. 2, the evolution rate balances the logit distribution (\(_{N}\)) with the latent knowledge distribution (\(_{latent}\)). A lower evolution rate works better for larger models (13B) and chat models as their logits already better represent real-world distributions.

LatencyOur method, SLED, does not incur significant latency overhead. The latencies presented in Table 5 demonstrate that our method, SLED, just increases the decoding time of DoLa by factors ranging from 0.1% to 10%. Notably, even with an atypical setting such as evolution scale \(=100\), which is seldom used, the increase remains around 10%. The latency for DoLa and SLED is much higher compared to the vanilla greedy decoding because we set all early layers as candidate layers set for both DoLa and SLED for a fair comparison.

## 4 Conclusion

We introduced Self Logits Evolution Decoding (SLED), which is a new method to improve accuracy and factuality without requiring external knowledge (e.g., RAG) or fine-tuning (e.g., SFT). The key idea is to optimize the output logits based on the LLMs' latent knowledge to improve factuality during inference. On several datasets, SLED achieved the SOTA results, improving over the vanilla decoding and DoLa. We also show that SLED does not increase the inference time significantly and that it can be combined with other factuality decoding methods. For future work, it would be interesting to combine SLED with supervised fine-tuning methods, e.g., to adapt to other domains.

  
**Model** & **Greedy** & **DoLa** & **SLED (ES=5)** & **SLED (ES=20)** & **SLED (ES=50)** & **SLED (ES=100)** \\  LLaMA-2-7B & 23.64 & 29.93 & 30.41 & 31.15 & 32.70 & 34.63 \\ LLaMA-2-13B & 30.41 & 39.57 & 39.61 & 41.14 & 43.30 & 45.09 \\ LLaMA-2-70B & 82.63 & 136.42 & 138.33 & 140.24 & 143.12 & 148.85 \\   

Table 5: Latency (ms/token) comparison across different configurations. (ES: evolution scale)

Figure 6: WE explore the impact of evolution scale and rate based on the factual accuracy of a subset of the FACTOR dataset. (G: Greedy, D: DoLa)

Figure 5: Evaluating using different premature layers for SLED and DoLa on a 10% subset of the GSM8K dataset. Contrasting all layers for SLED is better than using only the first half [0, 16) or the second half . Hence, there are no improvements for SLED from strategic layer subset selection.