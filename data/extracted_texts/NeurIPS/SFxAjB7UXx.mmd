# Scalable Optimization in the Modular Norm

Tim Large\({}^{}\)

Columbia University

&Yang Liu

Lawrence Livermore National Lab

&Minyoung Huh

MIT CSAIL

&Hyojin Bahng

MIT CSAIL

&Phillip Isola

MIT CSAIL

&Jeremy Bernstein\({}^{}\)

MIT CSAIL

denotes equal contribution. Correspondence to {jbernstein,minhuh}@mit.edu.

###### Abstract

To improve performance in contemporary deep learning, one is interested in scaling up the neural network in terms of both the number and the size of the layers. When ramping up the width of a single layer, graceful scaling of training has been linked to the need to normalize the weights and their updates in the "natural norm" particular to that layer. In this paper, we significantly generalize this idea by defining the _modular norm_, which is the natural norm on the full weight space of any neural network architecture. The modular norm is defined recursively in tandem with the network architecture itself. We show that the modular norm has several promising applications. On the practical side, the modular norm can be used to normalize the updates of any base optimizer so that the learning rate becomes transferable across width and depth. This means that the user does not need to compute optimizer-specific scale factors in order to scale training. On the theoretical side, we show that for any neural network built from "well-behaved" atomic modules, the gradient of the network is Lipschitz-continuous in the modular norm, with the Lipschitz constant admitting a simple recursive formula. This characterization opens the door to porting standard ideas in optimization theory over to deep learning. We have created a Python package called Modula that automatically normalizes weight updates in the modular norm of the architecture. The package is available via pip install modula with source code here.

## 1 Introduction

Given the practical impact of deep learning systems trained at the largest scale, there is a need for training algorithms that scale gracefully: without instability and--if possible--without manual tuning. However, current best practices for training have developed somewhat organically and do not live on a bedrock of sound numerical analysis. For example, while the Adam optimizer  is ubiquitous in the field, errors have been found in its proof of convergence , and empirically Adam has been found to scale poorly as either the width  or the depth  of the network is ramped up.

To remedy this situation, a patchwork of learning rate correction factors have recently been proposed . The general idea is to retrofit a base optimizer such as Adam or SGD with special correction factors intended to render the optimizer's optimal learning rate invariant to scale. But this situation is not ideal: the correction factors are reportedly difficult to use. Lingle  suggests that this may be due to their "higher implementation complexity, many variations, or complex theoretical background". What's more, the correction factors are optimizer-specific, meaning that if one switches to a different optimizer one must either look up or recalculate a separate set of correction factors.

The goal of this paper is to simplify matters. We show that both Adam and SGD can be made to scale gracefully with width and depth by simply normalizing their updates in a special norm associated withthe network architecture--see Figure 1. We call this norm the _modular norm_, and provide a Python package called Modula that constructs this norm automatically and in tandem with the architecture.

The modular norm is constructed recursively, leveraging the module tree perspective on neural architectures. It is enough to define how the modular norm propagates through only two elementary operations: composition and concatenation. We show how other basic operations on modules, such as addition and scalar-multiplication, can be implemented through composition and concatenation. And then higher-order structures, such as residual networks, can be built using these basic operations.

Beyond its practical relevance, the modular norm may also prove useful to theoreticians. Various optimization-theoretic quantities are accessible and efficiently calculable in the modular norm. For instance, we show that the gradient of any neural network built from "well-behaved" atomic modules is Lipschitz-continuous in the modular norm of the architecture. This opens the door to porting several more-or-less textbook optimization theory analyses  over to the world of deep learning.

### Related work

MetrizationIt is by now well-known that deep networks do not easily or naturally admit Lipschitz-continuity or smoothness guarantees in the Euclidean norm [9; 10; 11; 12; 13]. Researchers have attempted to address this problem: for instance, Bernstein et al.  propose a distance function called _deep relative trust_, which combines Frobenius norms across network layers. However, deep relative trust is only constructed for the multilayer perceptron and, when used to normalize updates, its employment of the Frobenius norm precludes good width scaling. In contrast, Yang et al.  equip individual layers with the RMS-RMS operator norm, finding this to enable good width scaling. Researchers have also looked at building neural net distance functions outside the context of scalability [15; 16; 17].

AsymptoticsThe metrization-based approach to scaling developed in this paper contrasts with the tradition of asymptotic scaling analyses--the study of infinite width and depth limits--more common in the deep learning theory literature [3; 4; 5; 18; 19]. These asymptotic analyses follow an old observation of Neal  that interesting properties of the neural network function space are exactly calculable in the infinite width limit and at initialization. This tradition has continued with asymptotic studies of the neural tangent kernel  as well as infinite depth limits [4; 5; 22]. However, there is increasing recognition of the limits of these limits, with researchers now often trying to relax limiting results [23; 24; 25]. And ultimately, from a practitioner's perspective, these results can be difficult to make sense of . In contrast, our framework eschews any kind of limiting or probabilistic analysis. As a consequence, we believe our framework is simpler, more easily relatable to basic mathematical concepts, and ultimately more relevant to what one may encounter in, say, a PyTorch  program.

Figure 1: **Learning rate transfer in the modular norm.** We train GPT with context length 128 for 10k steps on OpenWebText. **Left:** Learning rate sweeps for normed Adam (Adam with updates normalized in the modular norm) with three transformer blocks and varying width. The optimal learning rate (marked by red dots) transfers well across scales. **Mid-left:** The same, but varying the number of blocks at width 128. **Mid-right:** Comparing normed versus unnormed Adam and SGD at fixed learning rate and varying width. For each method, we tune the learning rate at the scale marked by the dotted line. The normed methods scale better. **Right:** The same, but scaling number of blocks.

MajorizationIn recent work, Streeter and Dillon  propose a _universal majorize-minimize algorithm_: a method that automatically computes and minimizes a majorizer for any computational graph. Despite its generality, current downsides to the method include its overhead, which can be \(2\) per step , as well as the risk that use of a full majorization may be overly pessimistic. Indeed, Cho and Shin  find that an optimization approach leveraging second-order information converges significantly faster than a majorization-inspired approach. Related ideas appear in [31; 32].

## 2 Descent in Normed Spaces

We define the modular norm in SS3. This section is intended to prime the reader for what is to come. In this section, and the rest of the document, the diamond operator \(\) denotes tensor contraction.

### What's in a norm?

Suppose that we wish to use gradient descent to minimize a loss function \(:\) over a weight space \(=^{N}\). What properties of the loss \(\) and weight space \(\) would we desire for this to be sensible? Three such properties are:

1. the loss function is differentiable, meaning that the gradient map \(_{}:\) exists;
2. the weight space \(\) carries a norm \(\|\|:\), which need not be the Euclidean norm;
3. the loss is Lipschitz smooth in the norm \(\|\|\), with sharpness constant \(>0\), meaning that: \[(+)()+_{} ()+\|\|^{2}.\] (2.1)

Under these conditions, the weight update given by \(=*{arg\,min}[_{}() +\|\|^{2}]\) is guaranteed to reduce the loss. The particular norm \(\|\|\) influences the direction of this weight update, while the sharpness constant \(\) influences the size of the update.

In deep learning, we would ideally like the optimal step-size to remain invariant as we scale, say, the width and the depth of the network. Thus, a fundamental problem is to design a norm such that, first, Inequality (2.1) actually holds (and is not hopelessly lax), and second, the corresponding sharpness constant \(\) is invariant to the relevant architectural dimensions. If the norm is chosen poorly, the practitioner may end up having to re-tune the step size as the network is scaled up. In this paper, we design a norm for neural networks that meets these requirements: the _modular norm_.

### Preview of the modular norm

The weight space of a deep neural network is a Cartesian product \(=_{1}_{L}\), where \(_{k}\) is the weight space at layer \(k\). Yang et al.  consider the problem of metrizing individual layers. For instance, if layer \(k\) is a linear layer with weight space \(_{k}=^{d_{} d_{}}\), then they equip this layer with the _RMS-RMS operator norm_, \(\|\|_{}\). This is the matrix norm induced by equipping the input and output space of the layer with the root-mean-square (RMS) vector norm, \(\|\|_{}^{}:=_{i}\,_{i}^{2}\) for \(^{d}\). The advantage of this non-standard matrix norm is that it allows one to estimate the amount of feature change induced by a gradient update. In other words, the inequality

\[\|\|_{}\|\|_{} \|\|_{},\] (2.2)

turns out to hold quite tightly when \(\) is a gradient update and \(\) is a corresponding layer input. This is because gradient updates to a layer are (sums of) outer products that align with layer inputs.

Once we know how to metrize individual layers, a natural question is: can we combine layer-wise norms to produce a norm on the full weight space \(=_{k}_{k}\) of the network? Naively, there are many ways to do this: one could take any positive linear combination of the layer-wise norms (\(L^{1}\) combination), the square root of any combination of the squared layer-wise norms (\(L^{2}\) combination), and so on. But we want the norm to be useful by the criteria of SS2.1. To this end, we propose the _modular norm_\(\|\|_{}\), which ends up as a max (\(L^{}\) combination) of scaled layer-wise norms \(\|\|_{_{k}}\):

\[\|(_{1},,_{L})\|_{}:=(s_{1}\|_{1} \|_{_{1}},,s_{L}\|_{L}\|_{_{L}}).\] (2.3)

The positive scalar constants \(s_{1},,s_{L}\) are determined by both the architecture of the network and a set of user-specified "mass" parameters. The precise construction of the modular norm, working recursively over the module tree of the network, is given in SS3; there, we also explain how the modular norm satisfies the criteria of SS2.1, and the role played by the mass parameters. For now, let us explain what good the modular norm yields in practice.

### Normed optimization

The main practical use of the modular norm is to normalize weight updates. With reference to Equation (2.3), we define the following operation on weight updates \(=(_{1},,_{L})\):

\[():=(_{1}}{s_{1}\| _{1}\|_{_{1}}},,_{L}}{s_{L}\| _{L}\|_{_{L}}}).\] (2.4)

Provided none of the \(_{k}\) are zero, then \(()\) is a unit vector in the modular norm. We propose using normalize as a wrapper, along with an explicit learning rate schedule, for any base optimizer such as Adam or SGD. The resulting _normed optimizer_ is thus made architecture-aware via the normalize function. In pseudo-code--and actual Modula code--this amounts to:

delta_w = optim(w.grad()) # get update from base optimizer net.normalize(delta_w) # normalize update in the modular norm w -- eta(step) * delta_w # apply update with learning rate eta

We find this wrapper to significantly improve the scalability of the base optimizer. It renders the optimal learning rate roughly invariant to width and depth, with seemingly no cost to accuracy. In some instances, it enables training with a simpler optimizer--for example, training GPT with SGD rather than Adam--thus incurring a smaller memory footprint.

Normalization in the modular norm essentially forces individual layers to learn at specified, regulated rates. We view this as _balancing_ learning across the network; no individual layer can learn too fast and destabilize training. This balance is determined by the architecture, along with user-specified mass parameters that provide precise control over the relative learning speed in different submodules.

For a variety of experiments with normed optimization, see SS4 and Appendix D. But first, we detail the construction of the modular norm along with its core properties.

## 3 Constructing the Modular Norm

Our strategy is to first define the abstract notion of a _module_, which includes a norm as an attribute. We depict this concept in Figure 2. Then, by providing rules for composing and concatenating modules, we recursively define a norm for any module built via an arbitrary sequence of compositions and concatenations: the modular norm!

Figure 2: **Modules and trees of modules.** A module is an object that maps an input and a weight vector to an output. **Left:** In addition to the standard _forward_ function, our modules are endowed with two numbers—a _mass_ and _sensitivity_—and a _norm_. **Middle:** New _compound modules_ are built via the binary operations of composition and concatenation. We provide rules for composing and concatenating all module attributes. **Right:** Compound modules are binary trees, where the leaves are modules and the internal nodes compose and concatenate their children. Here we illustrate a sum of modules, which leverages a special utility module Add—see Table 1 for more on this.

[MISSING_PAGE_FAIL:5]

* \(.=_{1}.+_{2}.\);
* \(.=_{1}.*_{2}. \);
* \(.((_{1},_{2}))\) given by: \[(_{2}.*. }{_{1}.}*_{1}.(_{1}),.}{_{2}.}*_{2}. (_{2})),\] _where if_ \(_{1}.\) _or_ \(_{2}.\) _is zero, the corresponding term in the_ \(\) _is set to zero._

At this stage, we make two comments about this definition. First, in the definition of the composite norm, notice that the norm of the first module couples with the sensitivity of the second module. This reflects the fact that the output of the first module is fed into the second module and not vice versa. Second, observe that the masses of the submodules are involved in setting the balance of the composite norm. Before we further motivate this definition, let us first define module concatenation:

**Definition 4** (Module concatenation).: _Consider module \(_{1}\) with input, output and weight space \((_{1},_{1},_{1})\) and module \(_{2}\) with input, output and weight space \((_{2},_{2},_{2})\). We say that \(_{1}\) and \(_{2}\) are concatenatable if their input spaces match: \(_{1}=_{2}\). The tuple \(=(_{1},_{2})\) has input, output and weight space \((_{1},_{1}_{2},_{1} _{2})\) and attributes:_

* \(.((_{1},_{2}),))=(_{1}. (_{1},),_{2}.(_{ 2},))\)_;_
* \(.=_{1}.+_{2}.\)_;_
* \(.=_{1}.+_{2}. \)_;_
* \(.(_{1},_{2})\) _given by:_ \[(.}{_{1}.}* _{1}.(_{1}),.}{ _{2}.}*_{2}.(_{2})),\] _where if_ \(_{1}.\) _or_ \(_{2}.\) _is zero, the corresponding term in the_ \(\) _is set to zero._

Concatenation is simpler than composition in the sense that neither module is fed through the other, and therefore, sensitivity does not appear in the concatenated norm. To further motivate these definitions, observe that two basic and desirable properties follow as immediate consequences:

**Proposition 1** (Composition and concatenation are associative).: _If modules \(_{1},_{2},_{3}\) are successively composable, then \(_{3}(_{2}_{1})\) equals \((_{3}_{2})_{1}\) in all attributes. If modules \(_{1},_{2},_{3}\) are mutually concatenatable, then \(((_{1},_{2}),_{3})\) equals \((_{1},(_{2},_{3}))\) in all attributes._

**Proposition 2** (Composition and concatenation preserve well-normedness).: _If modules \(_{1}\) and \(_{2}\) are well-normed and composable, then their composite \(_{2}_{1}\) is also well-normed. If modules \(_{1}\) and \(_{2}\) are well-normed and concatenatable, then their tuple \((_{1},_{2})\) is also well-normed with respect to the \(L^{1}\) combination norm on the output space: \(\|(,)\|_{_{1}_{2}}=\|\|_{ _{1}}+\|\|_{_{2}}\)._

The proofs follow directly from the definitions and the chain rule. Proposition 1 implies that one may build complicated compound modules without worrying in which order successive combinations are taken. Proposition 2 implies that complicated compounds automatically inherit Lipschitz guarantees.

Taken together, Definitions 3 and 4 define the _modular norm_\(.\) of any compound module \(\).

 
**Operation** & **Shorthand** & **Definition** & **Modula Expression** \\   module addition & \(_{1}+_{2}\) & \((_{1},_{2})\) & \(_{1}+_{2}\) \\ scalar multiplication & \(a*\) & \(_{a}\) & a * \(\) \\ iterated composition & \(^{L}\) & \(^{L-1}\) with \(^{0}:=\) & \(**L\) \\  

Table 1: **Arithmetic with modules.** Composition and concatenation let us define an extended arithmetic on modules. The utility modules \(,_{a}\) and \(\) are defined in Appendix B.2.

### Mass allocation in compound modules

Suppose we wish to train a network with an input layer, an output layer, and \(L\) blocks between:

\[ =\] (3.3) \[=^{L}.\] (3.4)

Then how much learning should happen in the output layer, compared to the blocks, compared to the input layer? And what if we scale the number of blocks \(L\)--do we want relatively less learning to occur in the network's extremities? Or do we want the input and output layers to learn non-trivially even in the \(L\) limit? Since answering these questions is difficult a priori, we introduced the mass parameter to allow a user to set the proportional contribution each module has toward learning:

**Proposition 3** (Feature learning is apportioned by mass).: _Consider a compound module \(\) derived in any fashion from \(L\) well-normed modules \(_{1},,_{L}\). Given weight setting \(=(_{1},,_{L})\), where \(_{k}\) denote the weights of module \(_{k}\), let us perturb \(\) by \(=(_{1},,_{L})\). If we decompose the linearized change in the output of module \(\) into one contribution per sub-module:_

\[_{}(,)=_{_{1}} (,)_{1}++_{_{L}} (,)_{L},\] (3.5)

_then the \(k\)th term in this decomposition satisfies:_

\[\|_{_{k}}(,)_{k}\|_{}_{k}.}{.}* .().\] (3.6)

In words: module mass provides the flexibility needed to build complicated compound modules involving many sub-modules, while maintaining precise control over how much learning any sub-module can contribute to the overall compound. Proposition 3 is proved in Appendix E.

In practice, we obtained the best training performance by maintaining a constant amount of learning in the input and output layers even as the number of blocks is scaled (Figure 6). In other words, it seems to be a good idea to assign \(::\) in proportion \(1:m:1\), where \(m\) is independent of the number of blocks \(L\). The exact mass of the hidden layers \(m\) needs to be tuned on a new architecture--just as one needs to tune separate learning rates in the input and output layers in \(\)P ; this tuning can be done on a small model prior to scaling (Figure 3). We further discuss mass allocation in Appendix D.6.

### Smoothness in the modular norm

In this section, we study the second derivatives of a module using the modular norm as a measuring stick. Let us start by defining the notion of sharpness that we will consider:

Figure 3: Exploring mass allocation. We tune the total mass of the hidden layers, training with normed Adam. **Left group:** Learning rate sweeps for ResMLP on CIFAR-10, for varying depth and mass. The bottom right subplot reports the best train loss at each mass and depth. Mass 0.5 was best at all depths. **Right group:** Learning rate sweeps for GPT on OpenWebText, for varying mass. Both optimal mass and learning rate transferred from the small model (top) to the large model (bottom).

**Definition 5** (Module sharpness).: _Let \(\) be a module on \((,,)\), where the input and output spaces have respective norms \(_{}\) and \(_{}\). We say that \(\) is \((,,)\)-sharp for constants \(,, 0\) if, at all inputs \(\) and weights \(\), the second derivatives of \(\) are bounded as:_

\[^{2}_{} (,)}_{} _{} }_{}\] (3.7) \[^{2}_{} (,)_{} _{} _{}\] (3.8) \[^{2}_{} (,)}_{} _{} }\|_{}\] (3.9)

While one may ultimately be interested in the sharpness of a module with respect to weight perturbations, Definition 5 also tracks sharpness with respect to input perturbations. In fact, tracking this extra information is essential for propagating sharpness bounds up the module tree. Appendix C details the procedure for automatically calculating the sharpness constants of a compound module starting from the sharpness constants of all its submodules; see Propositions 8 and 9 for the specific formulae. Here we highlight one major corollary of these formulae, proved in Appendix E: _for a specific choice of block multipliers, the sharpness constant of a residual network is independent of depth_:

**Proposition 4**.: _Suppose \(\) is a well-normed, \((,,)\)-sharp module on \((,,)\) with unit sensitivity. Define the depth \(L\) residual module \(_{L}()\) via the module arithmetic of Table 1 as:_

\[_{L}():=(*+*)^{L}.\] (3.10)

_Then this residual module \(_{L}()\) is in fact \((++,+,)\)-sharp, independent of depth \(L\)._

For optimization purposes, one may be more interested in the sharpness of the loss function rather than the sharpness of the neural network. Fortunately, it is possible to convert sharpness bounds on modules into sharpness bounds on loss functions, provided a little is known about the error measure:

**Proposition 5** (Loss functions are smooth in the modular norm).: _Let \(\) be a module on \((,,)\) and let \(:\) measure the error between a module output and a target in target space \(\). The loss \(:\) records the module's average error on data distribution \(\) over \(\):_

\[():=_{,}\,((,),).\] (3.11)

_Suppose that the error measure \(\) is \(\)-Lipschitz and \(\)-smooth in the module output, in the sense that:_

\[_{}(,) _{} ;\] (3.12) \[^{2}_{}(,)} _{} }_{} ,}.\] (3.13)

_If the module \(\) is well-normed and \((,,)\)-sharp, then the loss function \(\) satisfies the following three inequalities at all weight settings \(\) and for all weight perturbations \(,}\):_

1. \(^{2}_{} }(+) _{}}_{ }\)_;_
2. \(_{}(+)-_{} ()_{}^{*}(+) _{}\)_,_ _where_ \(_{}^{*}\) _is the dual norm of_ \(_{}\)_;_
3. \((+)-[()+_{ }]( +)_{}^{2}\)_._

The proof is given in Appendix E, and we present estimates for \(\) and \(\) for common error measures in Appendix C.4. Notice that inequalities (i), (ii) and (iii) are the standard inequalities of smooth optimization , albeit expressed in the modular norm. In fact, (i) implies (ii) implies (iii). In words, inequality (ii) says that the gradient of the loss is Lipschitz-continuous in the modular norm. The Lipschitz constant depends on the module only through the module's first sharpness coefficient \(\).

## 4 Experiments

Our experiments aimed to test the _scalability of training with normed versions of Adam and SGD_: whether one can tune the learning rate on a small model, and expect the learning rate to remain close to optimal on models of much larger width and depth. In addition to the learning rate, normed optimization in Modula requires a _mass parameter_ to apportion feature learning between the input, output and hidden layers; we also tested the sensitivity of this parameter, whether it affects learning rate transfer, and to what extent the optimal mass itself transfers across width and depth.

All SGD experiments were done with momentum \(=0.9\), and all Adam experiments used \(_{1}=0.9\) and \(_{2}=0.99\). No weight decay was used in any experiment. Every experiment was done with a linear decay learning rate schedule. As for initialization, we used orthogonal initialization for Linear and Conv2D modules, and Gaussian weights projected to a unit norm ball for our Embed module. This was to ensure all modules were well-normed at initialization. Precise versions of our architectures are described in Appendices B.5 and B.7. We compare with nanoGPT using standard initialization in Appendix D.4 to make sure our changes recover standard performance. We actually found unnormed Adam using our GPT architecture transferred learning rate _better_ than in nanoGPT.

We found that normed optimization, with both Adam and SGD as the base optimizer, allows for successful learning rate transfer across width and depth for GPT training on OpenWebText (Figure 1), as well as ResMLP and ResNet training on CIFAR-10 (Figure 4). We present expanded results in Appendix D.5, including results on test loss. We reproduce the standard finding that train and test loss are remarkably similar in large language model pretraining. As for mass allocation, Figure 3 shows that optimal mass transfers with depth for training a ResMLP on CIFAR-10 with normed Adam, and also that both mass and learning rate transfer quite well from a smaller GPT on OpenWebText to a larger one. We detail more experiments on mass allocation in Appendix D.6.

## 5 Discussion: Limitations and Future Work

This paper was influenced by four main streams of work: first, the Tensor Programs series, starting at TP-IV [3; 4; 18]; second, the papers on universal majorize-minimize algorithms [27; 28]; third, work on deep network metrization [12; 14; 31]; and fourth, the open source deep learning ecosystem [26; 33; 34] including the PyTorch module tree and Karpathy's YouTube video on autograd . We have distilled and synthesized key ideas from these sources, creating a framework that we believe to be simpler than Tensor Programs, computationally lighter than universal majorization-minimization, more general than prior work on metrization and more scalable than the PyTorch module tree. We have packaged these ideas into a (soon-to-be) open-source library called Modula. Inevitably, Modula has limitations. We highlight some of them here, along with associated avenues for future work.

**Loss of well-normed-ness.** We have emphasized well-normed-ness (Definition 2) as an important criterion in module design. We show in Appendix B.1 that, for example, the Linear module is well-normed when its weights lie within a spectral norm ball. In our experiments, we initialize all weights so that all modules are well-normed, but we do not enforce this property throughout training. Future work could explore regularization as a means to enforce well-normed-ness throughout training, with the hope of attaining better scalability or improved generalization.

**Overhead of normalization.** As discussed in Appendix A.3, we implement normalization for Linear and Conv2D modules using two steps of online power iteration. While online power iteration is an established and fast primitive in deep learning--in fact, coming from the GAN literature --it does add a modest overhead to training time, as discussed in Appendix A.4. We think it may be possible to mitigate this overhead by constructing atomic modules with more exotic operator norms. For example, if one equips feature vectors with the \(L^{}\) norm rather than the RMS norm, then the induced \(L^{}\)-\(L^{}\) matrix norm is cheaper to compute than the RMS-RMS operator norm. In fact,

Figure 4: **Learning rate transfer on CIFAR-10.** We tune the learning rate on a small model—at the scale marked by the dotted line—and test the performance on models of increasing width and depth at this fixed learning rate. We find that normed Adam and SGD scale better than their unnormed counterparts on both ResMLPs and ResNets. See Figure 1 for the same experiment on GPT.

operator normalization has the convenient feature that it decouples over matrix rows, making it more _local_ than spectral normalization and, dare-we-say, more _biologically plausible_.

**Automatic step-size selection.** Beyond scalability, recent work has explored the question of automatic learning rate selection , with the Prodigy optimizer  serving as a popular example. We tested the Adam version of Prodigy and found it performs well at small scales, essentially working by an implicit form of line search. However, Prodigy will always break at large enough widths, since it requires a lower bound (\(d_{0}\)) on Adam's initial learning rate; Yang et al.  showed that no such lower bound exists. We believe this issue could be fixed by rebuilding Prodigy on top of Modula. More broadly, we think that designing line search methods in a properly-normed space is a good idea.

## Contribution Statement

All authors were involved in project conception and discussions, which were initiated by JB. TL and JB developed the theory. MH and YL made core experimental observations. YL, MH, JB, and HB ran experiments. TL and JB did most of the writing, while JB, MH and YL made the figures. PI contributed guidance and helpful feedback throughout the course of the project. JB wrote the Modula package with help from MH.