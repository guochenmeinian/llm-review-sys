# DPOK: Reinforcement Learning for

Fine-tuning Text-to-Image Diffusion Models

 Ying Fan*,\({}^{1,2}\), Olivia Watkins\({}^{3}\), Yuqing Du\({}^{3}\), Hao Liu\({}^{3}\), Moonkyung Ryu\({}^{1}\), Craig Boutilier\({}^{1}\),

Pieter Abbeel\({}^{3}\), Mohammad Ghavamzadeh\({}^{1,4}\), Kangwook Lee\({}^{2}\), Kimin Lee*,\({}^{1,5}\)

* Equal technical contribution \({}^{}\)Work was done at Google Research

\({}^{1}\)Google Research \({}^{2}\)University of Wisconsin-Madison \({}^{3}\)UC Berkeley \({}^{4}\)Amazon \({}^{5}\)KAIST

###### Abstract

Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on _diffusion models_, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning with respect to both image-text alignment and image quality. Our code is available at https://github.com/google-research/google-research/tree/master/dpok.

## 1 Introduction

Recent advances in _diffusion models_, together with pre-trained text encoders (e.g., CLIP , T5 ) have led to impressive results in text-to-image generation. Large-scale text-to-image models, such as Imagen , Dalle-2 , and Stable Diffusion , generate high-quality, creative images given novel text prompts. However, despite these advances, current models have systematic weaknesses. For example, current models have a limited ability to compose multiple objects . They also frequently encounter difficulties when generating objects with specified colors and counts .

_Learning from human feedback (LHF)_ has proven to be an effective means to overcome these limitations . Lee et al.  demonstrate that certain properties, such as generating objects with specific colors, counts, and backgrounds, can be improved by learning a _reward function_ from human feedback, followed by fine-tuning the text-to-image model using supervised learning. They show that simple supervised fine-tuning based on reward-weighted loss can improve the reward scores, leading to better image-text alignment. However, supervised fine-tuning often induces a deterioration in image quality (e.g., over-saturated or non-photorealistic images). This is likely due to the model being fine-tuned on a fixed dataset that is generated by a pre-trained model (Figure 1(a)).

In this work, we explore using _online reinforcement learning (RL)_ for fine-tuning text-to-image diffusion models (Figure 1(b)). We show that optimizing the expected reward of a diffusion model's image output is equivalent to performing policy gradient on a multi-step diffusion model under certainregularity assumptions. We also incorporate Kullback-Leibler (KL) divergence with respect to the pre-trained model as regularization in an online manner, treating this as an implicit reward.

In our experiments, we fine-tune the Stable Diffusion model  using ImageReward , an open-source reward model trained on a large dataset comprised of human assessments of (text, image) pairs. We show that online RL fine-tuning achieves strong text-image alignment while maintaining high image fidelity by optimizing its objective in an online manner. _Crucially, online training allows evaluation of the reward model and conditional KL divergence beyond the (supervised) training dataset._ This offers distinct advantages over supervised fine-tuning, a point we demonstrate empirically. In our empirical comparisons, we also incorporate the KL regularizer in a supervised fine-tuning method for a fair comparison.

Our contributions are as follows:

* We frame the optimization of the expected reward (w.r.t. an LHF-reward) of the images generated by a diffusion model given text prompts as an online RL problem. Moreover, we present DPOK: **D**iffusion **P**olicy **O**ptimization with **KL** regularization, which utilizes KL regularization w.r.t. the pre-trained text-to-image model as an implicit reward to stabilize RL fine-tuning.
* We study incorporating KL regularization into supervised fine-tuning of diffusion models, which can mitigate some failure modes (e.g., generating over-saturated images) in . This also allows a fairer comparison with our RL technique.
* We discuss the key differences between supervised fine-tuning and online fine-tuning of text-to-image models (Section 4.3).
* Empirically, we show that online fine-tuning is effective in optimizing rewards, which improves text-to-image alignment while maintaining high image fidelity.

## 2 Related Work

Text-to-image diffusion models.Diffusion models [10; 37; 39] are a class of generative models that use an iterative denoising process to transform Gaussian noise into samples that follow a learned data distribution. These models have proven to be highly effective in a range of domains, including image generation , audio generation , 3D synthesis , and robotics . When combined with large-scale language encoders [27; 28], diffusion models have demonstrated impressive performance in text-to-image generation [29; 30; 32]. However, there are still many known weaknesses of existing text-to-image models, such as compositionality and attribute binding [6; 20] or text rendering .

Figure 1: Illustration of (a) reward-weighted supervised fine-tuning and (b) RL fine-tuning. Both start with the same pre-trained model (the blue rectangle). In supervised fine-tuning, the model is updated on a fixed dataset generated by the pre-trained model. In contrast, the model is updated using new samples from the previously trained model during online RL fine-tuning.

Learning from human feedback.Human assessments of (or preferences over) learned model outcomes have been used to guide learning on a variety of tasks, ranging from learning behaviors  to language modeling . Recent work has also applied such methods to improve the alignment of text-to-image models. Human preferences are typically gathered at scale by asking annotators to compare generations, and a _reward model_ is trained (e.g., by fine-tuning a vision-language model such as CLIP  or BLIP ) to produce scalar rewards well-aligned with the human feedback . The reward model is used to improve text-to-image model quality by fine-tuning a pre-trained generative model . Unlike prior approaches, which typically focus on reward-filtered or reward-weighted supervised learning, we develop an online fine-tuning framework with an RL-based objective.

RL fine-tuning of diffusion models.Fan & Lee  first introduced a method to improve pre-trained diffusion models by integrating policy gradient and GAN training . They used policy gradient with reward signals from the discriminator to update the diffusion model and demonstrated that the fine-tuned model can generate realistic samples with few diffusion steps with DDPM sampling  on relatively simple domains (e.g., CIFAR  and CelebA ). In this work, we explore RL fine-tuning especially for large-scale text-to-image models using human rewards. We also consider several design choices like adding KL regularization as an implicit reward, and compare RL fine-tuning to supervised fine-tuning.

Concurrent and independent from our work, Black et al.  have also investigated RL fine-tuning to fine-tune text-to-image diffusion models. They similarly frame the fine-tuning problem as a multi-step decision-making problem, and demonstrate that RL fine-tuning can outperform supervised fine-tuning with reward-weighted loss  in optimizing the reward, which aligns with our own observations. Furthermore, our work analyzes KL regularization for both supervised fine-tuning and RL fine-tuning with theoretical justifications, and shows that adopting KL regularization is useful in addressing some failure modes (e.g., deterioration in image quality) of fine-tuned models. For a comprehensive discussion of prior work, we refer readers to Appendix D.

## 3 Problem Setting

In this section, we describe our basic problem setting for text-to-image generation of diffusion models.

Diffusion models.We consider the use of _denoising diffusion probabilistic models (DDPMs)_ for image generation and draw our notation and problem formulation from . Let \(q_{0}\) be the data distribution, i.e., \(x_{0} q_{0}(x_{0}),\ x_{0}^{n}\). A DDPM approximates \(q_{0}\) with a parameterized model of the form \(p_{}(x_{0})= p_{}(x_{0:T})dx_{1:T}\), where \(p_{}(x_{0:T})=p_{T}(x_{T})_{t=1}^{T}p_{}(x_{t-1}|x_{t})\) and the _reverse process_ is a Markov chain with the following dynamics:

\[p(x_{T})=(0,I), p_{}(x_{t-1}|x_{t})= _{}(x_{t},t),_{t}.\] (1)

A unique characteristic of DDPMs is the exploitation of an approximate posterior \(q(x_{1:T}|x_{0})\), known as the _forward_ or _diffusion process_, which itself is a Markov chain that adds Gaussian noise to the data according to a variance schedule \(_{1},,_{T}\):

\[q(x_{1:T}|x_{0})=_{t=1}^{T}q(x_{t}|x_{t-1}), q(x_{t}|x_{t-1})= (}\ x_{t-1},_{t}I).\] (2)

Let \(_{t}=1-_{t}\), \(_{t}=_{s=1}^{t}_{s}\), and \(_{t}=_{t-1}}{1-_{t}}_{t}\). Ho et al.  adopt the parameterization \(_{}(x_{t},t)=}}(x_{t}- }{_{t}}}_{}(x_{t},t))\).

Training a DDPM is performed by optimizing a variational bound on the negative log-likelihood \(_{q}[- p_{}(x_{0})]\), which is equivalent to optimizing:

\[_{q}_{t=1}^{T}q(x_{t-1}|x_{t},x_{0})|p _{}(x_{t-1}|x_{t}).\] (3)

Note that the variance sequence \((_{t})_{t=1}^{T}(0,1)^{T}\) is chosen such that \(_{T} 0\), and thus, \(q(x_{T}|x_{0})(0,I)\). The covariance matrix \(_{t}\) in (1) is often set to \(_{t}^{2}I\), where \(_{t}^{2}\) is either \(_{t}\) or \(_{t}\), which is not trainable. Unlike the original DDPM, we use a latent diffusion model , so \(x_{t}\)'s are latent.

Text-to-image diffusion models.Diffusion models are especially well-suited to _conditional_ data generation, as required by text-to-image models: one can plug in a classifier as guidance function , or can directly train the diffusion model's conditional distribution with classifier-free guidance .

Given text prompt \(z p(z)\), let \(q(x_{0}|z)\) be the data distribution conditioned on \(z\). This induces a joint distribution \(p(x_{0},z)\). During training, the same noising process \(q\) is used regardless of input \(z\), and both the unconditional \(_{}(x_{t},t)\) and conditional \(_{}(x_{t},t,z)\) denoising models are learned. For data sampling, let \(_{}=w_{}(x_{t},t,z)+(1-w)_{}( x_{t},t)\), where \(w 1\) is the guidance scale. At test time, given a text prompt \(z\), the model generates conditional data according to \(p_{}(x_{0}|z)\).

## 4 Fine-tuning of Diffusion Models

In this section, we describe our approach for online RL fine-tuning of diffusion models. We first propose a Markov decision process (MDP) formulation for the denoising phase. We then use this MDP and present a policy gradient RL algorithm to update the original diffusion model. The RL algorithm optimizes an objective consisting of the reward and a KL term that ensures the updated model is not too far from the original one. We also present a modified supervised fine-tuning method with KL regularization and compares it with the RL approach.

### RL Fine-tuning with KL Regularization

Let \(p_{}(x_{0:T}|z)\) be a text-to-image diffusion model where \(z\) is some text prompt distributed according to \(p(z)\), and \(r(x_{0},z)\) be a reward model (typically trained using human assessment of images).

**MDP formulation:** The denoising process of DDPMs can be modeled as a \(T\)-horizon MDP:

\[s_{t}=(z,x_{T-t}), a_{t}=x_{T-t-1}, P_{0}(s_{0})= p(z),(0,I), P(s_{t+1} s_{t},a_{t})=(_ {z},_{a_{t}}),\] \[R(s_{t},a_{t})=r(s_{t+1})=r(x_{0},z)&\ t=T-1,\\ 0&.,_{}(a_{t} s_{t})=p_{}(x_{ T-t-1} x_{T-t},z),\] (4)

in which \(s_{t}\) and \(a_{t}\) are the state and action at time-step \(t\), \(P_{0}\) and \(P\) are the initial state distribution and the dynamics, \(R\) is the reward function, and \(_{}\) is the parameterized policy. As a result, optimizing policy \(_{}\) in (4) is equivalent to fine-tuning the underlying DDPM.1 Finally, we denote by \(_{z}\) the Dirac distribution at \(z\).

It can be seen from the MDP formulation in (4) that the system starts by sampling its initial state \(s_{0}\) from the Gaussian distribution \((0,I)\), similar to the first state of the dinoising process \(x_{T}\). Given the MDP state \(s_{t}\), which corresponds to state \(x_{T-t}\) of the denoising process, the policy takes the action at time-step \(t\) as the next denoising state, i.e., \(a_{t}=x_{T-t-1}\). As a result of this action, the system transitions deterministically to a state identified by the action (i.e., the next state of the denoising process). The reward is zero, except at the final step in which the quality of the image at the end of the denoising process is evaluated w.r.t. the prompt, i.e., \(r(x_{0},z)\).

A common goal in re-training/fine-tuning the diffusion models is to maximize the expected reward of the generated images given the prompt distribution, _i.e._,

\[_{}\ _{p(z)}_{p_{}(x_{0}|z)}[-r(x_{0},z)].\] (5)

The gradient of this objective function can be obtained as follows:

**Lemma 4.1** (A modification of Theorem 4.1 in ).: _If \(p_{}(x_{0:T}|z)r(x_{0},z)\) and \(_{}p_{}(x_{0:T}|z)r(x_{0},z)\) are continuous functions of \(\), then we can write the gradient of the objective in (5) as_

\[_{}_{p(z)}_{p_{}(x_{0}|z)}[-r(x_{0},z)] =_{p(z)}_{p_{}(x_{0:T}|z)}[-r(x_{0},z)_{t=1} ^{T}_{} p_{}(x_{t-1}|x_{t},z)].\] (6)

Proof.: We present the proof in Appendix A.1. 

Equation (6) is equivalent to the gradient used by the popular policy gradient algorithm, REINFORCE, to update a policy in the MDP (4). The gradient in (6) is estimated from trajectories \(p_{}(x_{0:T}|z)\) generated by the current policy, and then used to update the policy \(p_{}(x_{t-1}|x_{t},z)\) in an online fashion.

Note that REINFORCE is not the only way to solve (5). Alternatively, one could compute the gradient through the trajectories to update the model; but the multi-step nature of diffusion models makes this approach memory inefficient and potentially prone to numerical instability. Consequently, scaling it to high-resolution images becomes challenging. For this reason, we adopt policy gradient to train large-scale diffusion models like Stable Diffusion .

Adding KL regularization.The risk of fine-tuning purely based on the reward model learned from human or AI feedback is that the model may overfit to the reward and discount the "skill" of the initial diffusion model to a greater degree than warranted. To avoid this phenomenon, similar to , we add the KL between the fine-tuned and pre-trained models as a regularizer to the objective function. Unlike the language models in which the KL regularizer is computed over the entire sequence/trajectory (of tokens), in text-to-image models, it makes sense to compute it only for the final image, i.e., \(p_{}(x_{0}|z)p_{}(x_{0}|z)\). Unfortunately, \(p_{}(x_{0}|z)\) is a marginal (see the integral in Section 3) and its closed-form is unknown. As a result, we propose to add an upper-bound of this KL-term to the objective function.

**Lemma 4.2**.: _Suppose \(p_{}(x_{0:T}|z)\) and \(p_{}(x_{0:T}|z)\) are Markov chains conditioned on the text prompt \(z\) that both start at \(x_{T}(0,I)\). Then, we have_

\[_{p(z}[(p_{}(x_{0}|z))\|p_{}(x_{0}|z))]\! \!_{p(z)}\![_{t=1}^{T}\!_{p_{}(x_{t}|z )}\![\!p_{}(x_{t-1}|x_{t},z)\|p_{}(x_{t- 1}|x_{t},z)\!]\!]\!.\] (7)

We report the proof of Lemma 4.2 in Appendix A.2. Intuitively, this lemma tells us that the divergence between the two distributions over the output image \(x_{0}\) is upper-bounded by the sum of the divergences between the distributions over latent \(x_{t}\) at each diffusion step.

Using the KL upper-bound in (7), we propose the following objective for regularized training:

\[_{p(z)}[_{p_{}(x_{0:T}|z)}[-r(x_{0},z)] ..+_{t=1}^{T}_{p_{}(x_{t}|z)}\![\! p_{}(x_{t-1}|x_{t},z)\|p_{}(x_{t-1}|x_{t},z) \!]],\] (8)

where \(,\) are the reward and KL weights, respectively. We use the following gradient to optimize the objective (8):

\[_{p(z)}_{p_{}(x_{0:T}|z)}\![- r(x_{0},z) _{t=1}^{T}_{} p_{}(x_{t-1}|x_{t},z)..+ _{t=1}^{T}_{}p_{}(x_{t-1}|x_{t},z) \|p_{}(x_{t-1}|x_{t},z)].\] (9)

Note that (9) has one term missing from the exact gradient of (8) (see Appendix A.3). Removing this term is for efficient training. The pseudo-code of our algorithm, which we refer to as DPOK, is summarized in Algorithm 1. To reuse historical trajectories and be more sample efficient, we can also use importance sampling and clipped gradient, similar to . We refer readers to Appendix A.6 for these details.

``` Input: reward model \(r\), pre-trained model \(p_{}\), current model \(p_{}\), batch size \(m\), text distribution \(p(z)\)  Initialize \(p_{}=p_{}\) while\(\) not converged do  Obtain \(m\) i.i.d. samples by first sampling \(z p(z)\) and then \(x_{0:T} p_{}(x_{0:T}|z)\)  Compute the gradient using Eq. (9) and update \(\) endwhile ```

**Output**: Fine-tuned diffusion model \(p_{}\) ```

**Algorithm 1** DPOK: Diffusion policy optimization with KL regularization

### Supervised Learning with KL Regularization

We now introduce KL regularization into supervised fine-tuning (SFT), which allows for a more meaningful comparison with our KL-regularized RL algorithm (DPOK). We begin with a supervisedfine-tuning objective similar to that used in , i.e.,

\[_{p(z)}_{p_{}(x_{0}|z)}[-r(x_{0},z) p_{}( x_{0}|z)].\] (10)

To compare with RL fine-tuning, we augment the supervised objective with a similar KL regularization term. Under the supervised learning setting, we consider \((p_{}(x_{0}|z)\|p_{}(x_{0}|z))\), which is equivalent to minimizing \(_{p_{}(x_{0}|z)}[- p_{}(x_{0}|z)]\), given any prompt \(z\). Let \(q(x_{t})\) be the forward process used for training. Since the distribution of \(x_{0}\) is generally not tractable, we use approximate upper-bounds in the lemma below (see derivation in Appendix A.4).

**Lemma 4.3**.: _Let \(\) be the regularization weight and \(_{t}(x_{t},x_{0}):=}_{t}}{1-_{t}}x_{0}+}(1-_{t-1})}{1-_{t}}x_{t}\). Assume \(r(x_{0},z)+>0\), for all \(x_{0}\) and \(z\). Then, we have_

\[&_{p_{}(x_{0}|z)}[-(r(x_{0},z)+ ) p_{}(x_{0}|z)]\\ &_{p(z)}_{p_{}(x_{0}|z)}[( r(x_{0},z)+)_{t>1}_{q(x_{t}|x_{0},z)}[ ^{2}}||_{t}(x_{t},x_{0})-_{}(x_{t},t,z)||^{2}]]+C_{1}. \] (11)

_Moreover, we also have another weaker upper-bound in which \(C_{1}\) and \(C_{2}\) are two constants:_

\[&_{p(z)}_{p_{}(x_{0}|z)}[-( r(x_{0},z)+) p_{}(x_{0}|z)]\\ &_{p(z)}_{p_{}(x_{0}|z)}[ _{t>1}_{q(x_{t}|x_{0},z)}[,z)||_{t}(x_{ t},x_{0})-_{}(x_{t},t,z)||^{2}}{2_{t}^{2}}\\ &+}(x_{t},t,z)-_{}(x _{t},t,z)||^{2})}{2_{t}^{2}}]]+C_{2}.\] (12)

In Lemma 4.3, we introduce KL regularization (Eq. (11)) for supervised fine-tuning, which can be incorporated by adjusting the original reward with a shift factor \(\) in the reward-weighted loss, smoothing the weighting of each sample towards the uniform distribution.2 We refer to this regularization as KL-D since it is based only on data from the pre-trained model. The induced supervised training objective for KL-D is as follows:

\[_{p(z)}_{p_{}(x_{0}|z)}[(r(x_{0},z)+ )_{t>1}_{q(x_{t}|x_{0},z)}[-f_{}(x_{t}, t,z)||^{2}}{2_{t}^{2}}]].\] (13)

We also consider another KL regularization presented in Eq. (12). This KL regularization can be implemented by introducing an additional term in the reward-weighted loss. This extra term penalizes the \(L_{2}\)-distance between the denoising directions derived from the pre-trained and current models. We refer to it as KL-O because it also regularizes the output from the current model to be close to that from the pre-trained model. The induced supervised training objective for KL-O is as follows:

\[_{p(z)}_{p_{}(x_{0}|z)}[_{t>1}_{q(x_{t}|x_{0},z)}[,z)||x_{0}-f_{}(x_{t},t,z)||^{2}+ \|f_{}(x_{t},t,z)-f_{}(x_{t},t,z)\|^{2}}{2_{t}^{2} }]].\]

We summarize our supervised fine-tuning in Algorithm 2. Note that in comparison to online RL training, our supervised setting only requires a pre-trained diffusion model and no extra/new datasets.

### Online RL vs. Supervised Fine-tuning

We outline key differences between online RL and supervised fine-tuning:

* **Online versus offline distribution.** We first contrast their objectives. The online RL objective is to find a new image distribution that maximizes expected reward--this can have much different support than the pre-trained distribution. In supervised fine-tuning, the objective only encourages the model to "imitate" good examples from the supervised dataset, which always lies in the support of the pre-trained distribution.

* **Different KL regularization.** The methods also differ in their use of KL regularization. For online training, we evaluate conditional KL in each step using online samples, while for supervised training we evaluate conditional KL only using supervised samples. Moreover, online KL regularization can be seen as an extra reward function to encourage small divergence w.r.t. the pre-trained model for online optimization, while supervised KL induces an extra shift in the original reward for supervised training as shown in Lemma 4.3.
* **Different evaluation of the reward model.** Online RL fine-tuning evaluates the reward model using the updated distribution, while supervised fine-tuning evaluates the reward on the fixed pre-training data distribution. As a consequence, our online RL optimization should derive greater benefit from the generalization ability of the reward model.

For these reasons, we expect online RL fine-tuning and supervised fine-tuning to generate rather different behaviors. Specifically, online fine-tuning should be better at maximizing the combined reward (human reward and implicit KL reward) than the supervised approach (similar to the difference between online learning and weighted behavior cloning in reinforcement learning).

## 5 Experimental Evaluation

We now describe a set of experiments designed to test the efficacy of different fine-tuning methods.

### Experimental Setup

As our baseline generative model, we use Stable Diffusion v1.5 , which has been pre-trained on large image-text datasets [33; 34]. For compute-efficient fine-tuning, we use Low-Rank Adaption (LoRA) , which freezes the parameters of the pre-trained model and introduces low-rank trainable weights. We apply LoRA to the UNet  module and only update the added weights. For the reward model, we use ImageReward  which is trained on a large dataset comprised of human assessments of images. Compared to other scoring functions such as CLIP  or BLIP , ImageReward has a better correlation with human judgments, making it the preferred choice for fine-tuning our baseline diffusion model (see Appendix C for further justification). Further experimental details (e.g., model architectures, final hyper-parameters) are provided in Appendix B. We also provide more samples for qualitative comparison in Appendix E.6.

### Comparison of Supervised and RL Fine-tuning

We first evaluate the performance of the original and fine-tuned text-to-image models w.r.t. specific capabilities, such as generating objects with specific colors, counts, or locations; and composing multiple objects (or _composition_). For both RL and SFT training we include KL regularization (see hyperparameters in Appendix B). For SFT, we adopt KL-O since we found it is more effective than KL-D in improving the visual quality of the SFT model (see the comparison between KL-O and KL-D in Appendix E.1). To systematically analyze the effects of different fine-tuning methods, we adopt a straightforward setup that uses one text prompt during fine-tuning. As training text prompts,we use "A green colored rabbit", "A cat and a dog", "Four wolves in the park", and "A dog on the moon". These test the models' ability to handle prompts involving _color_, _composition_, _counting_ and _location_, respectively. For supervised fine-tuning, we use 20K images generated by the original model, which is the same number of (online) images used by RL fine-tuning.

Figure 3 compares ImageReward scores of images generated by the different models (with the same random seed). We see that both SFT and RL fine-tuning improve the ImageReward scores on the training text prompt. This implies the fine-tuned models can generate images that are better aligned with the input text prompts than the original model because ImageReward is trained on human feedback datasets to evaluate image-text alignment. Figure 2 indeed shows that fine-tuned models add objects to match the number (e.g., adding more wolves in "Four wolves in the park"), and replace incorrect objects with target objects (e.g., replacing an astronaut with a dog in "A dog on the moon") compared to images from the original model. They also avoid obvious mistakes like generating a rabbit with a green background given the prompt "A green colored rabbit". Of special note, we find that the fine-tuned models generate better images than the original model on several unseen text prompts consisting of unseen objects in terms of image-text alignment (see Figure 10 in Appendix E).

As we can observe in Figure 3, RL models enjoy higher ImageReward than SFT models when evaluated with the same training prompt in different categories respectively, due to the benefit of online training as we discuss in Section 4.3. We also evaluate the image quality of the three models using the aesthetic predictor , which is trained to predict the aesthetic aspects of generated images.3 Figure 3 shows that supervised fine-tuning degrades image quality relative to the RL approach (and sometimes relative to the pre-trained model), even with KL regularization which is

Figure 3: (a) ImageReward scores and (b) Aesthetic scores of three models: the original model, supervised fine-tuned (SFT) model, and RL fine-tuned model. ImageReward and Aesthetic scores are averaged over 50 samples from each model. (c) Human preference rates between RL model and SFT model in terms for image-text alignment and image quality. The results show the mean and standard deviation averaged over eight independent human raters.

Figure 2: Comparison of images generated by the original Stable Diffusion model, supervised fine-tuned (SFT) model, and RL fine-tuned model. Images in the same column are generated with the same random seed. Images from seen text prompts: “A green colored rabbit” (color), “A cat and a dog” (composition), “Four wolves in the park” (count), and “A dog on the moon” (location).

intended to blunt such effects. For example, the supervised model often generates over-saturated images, corroborating the observations of Lee et al. . By contrast, the RL model generates more natural images that are at least as well-aligned with text prompts.

We also conduct human evaluation as follows: We collect 40 images randomly generated from each prompt, resulting in a total of 160 images for each model. Given two (anonymized) sets of four images from the same random seeds, one from RL fine-tuned model and one from the SFT model, we ask human raters to assess which one is better w.r.t. image-text alignment and image quality. Each query is evaluated by 8 independent raters and we report the average win/lose rate in Figure 3(c). The RL model consistently outperforms the SFT model on both alignment and image quality.

### The Effect of KL Regularization

To demonstrate the impact of KL regularization on both supervised fine-tuning (SFT) and online RL fine-tuning, we conduct an ablation study specifically fine-tuning the pre-trained model with and without KL regularization on "A green colored rabbit".

Figure 4(a) shows that KL regularization in online RL is effective in attaining both high reward and aesthetic scores. We observe that the RL model without KL regularization can generate lower-quality images (e.g., over-saturated colors and unnatural shapes) as shown in Figure 4(b). In the case of SFT with KL-O (where we use the same configuration as Section 5.2), we find that the KL regularization can mitigate some failure modes of SFT without KL and improve aesthetic scores, but generally suffers from lower ImageNetward. We expect that this difference in the impact of KL regularization is due to the different nature of online and offline training--KL regularization is only applied to fixed samples in the case of SFT while KL regularization is applied to new samples per each update in the case of online RL (see Section 4.3 for related discussions).

### Reducing Bias in the Pre-trained Model

To see the benefits of optimizing for ImageNetward, we explore the effect of RL fine-tuning in reducing bias in the pre-trained model. Because the original Stable Diffusion model is trained on

Figure 4: Ablation study of KL regularization in both SFT and RL training, trained on a single prompt “A green colored rabbit”. (a) ImageReward and Aesthetic scores are averaged over 50 samples from each model. (b) Images generated by RL models and SFT models optimized with and without KL regularization. Images in the same column are generated with the same random seed.

Figure 5: Comparison of images generated by the original model and RL fine-tuned model on text prompt “Four roses”. The original model, which is trained on large-scale datasets from the web , tends to produce whiskey-related images from “Four roses” due to the existence of a whiskey brand bearing the same name as the prompt. In contrast, RL fine-tuned model with ImageNetward generates images associated with the flower “rose”.

large-scale datasets extracted from the web [33; 34], it can encode some biases in the training data. As shown in Figure 5, we see that the original model tends to produce whiskey-related images given the prompt "Four roses" (which happens to be the brand name of a whiskey), which is not aligned with users' intention in general. To verify whether maximizing a reward model derived from human feedback can mitigate this issue, we fine-tune the original model on the "Four roses" prompt using RL fine-tuning. Figure 5 shows that the fine-tuned model generates images with roses (the flower) because the ImageReward score is low on the biased images (ImageReward score is increased to 1.12 from -0.52 after fine-tuning). This shows the clear benefits of learning from human feedback to better align existing text-to-image models with human intentions.

### Fine-tuning on Multiple Prompts

We further verify the effectiveness of the proposed techniques for fine-tuning text-to-image models on multiple prompts simultaneously. We conduct online RL training with 104 MS-CoCo prompts and 183 Drawbench prompts, respectively (the prompts are randomly sampled during training). Detailed configurations are provided in Appendix E. Specifically, we also learn a value function for variance reduction in policy gradient which shows benefit in further improving the final reward (see Appendix A.5 for details.) We report both ImageReward and the aesthetic score of the original and the RL fine-tuned models. For evaluation, we generate 30 images from each prompt and report the average scores of all images. The evaluation result is reported in Table 1 with sample images in Figure 11 in Appendix E, showing that RL training can also significantly improve the ImageReward score while maintaining a high aesthetic score with much larger sets of training prompts.

## 6 Discussions

In this work, we propose DPOK, an algorithm to fine-tune a text-to-image diffusion model using policy gradient with KL regularization. We show that online RL fine-tuning outperforms simple supervised fine-tuning in improving the model's performance. Also, we conduct an analysis of KL regularization for both methods and discuss the key differences between RL fine-tuning and supervised fine-tuning. We believe our work demonstrates the potential of _reinforcement learning from human feedback_ in improving text-to-image diffusion models.

Limitations, future directions.Several limitations of our work suggest interesting future directions: (a) As discussed in Section 5.5, fine-tuning on multiple prompts requires longer training time, hyper-parameter tuning, and engineering efforts. More efficient training with a broader range of diverse and complex prompts would be an interesting future direction to explore. (b) Exploring advanced policy gradient methods can be useful for further performance improvement. Investigating the applicability and benefits of these methods could lead to improvements in the fine-tuning process.

Broader ImpactsText-to-image models can offer societal benefits across fields such as art and entertainment, but they also carry the potential for misuse. Our research allows users to finetune these models towards arbitrary reward functions. This process can be socially beneficial or harmful depending on the reward function used. Users could train models to produce less biased or offensive imagery, but they could also train them to produce misinformation or deep fakes. Since many users may follow our approach of using open-sourced reward functions for this finetuning, it is critical that the biases and failure modes of publicly available reward models are thoroughly documented.

    &  &  \\  & Original model & RL model & Original model & RL model \\  ImageReward score & 0.22 & 0.55 & 0.13 & 0.58 \\ Aesthetic score & 5.39 & 5.43 & 5.31 & 5.35 \\   

Table 1: ImageReward scores and Aesthetic scores from the original model, and RL fine-tuned model on multiple prompts from MS-CoCo (104 prompts) and Drawbench (183 prompts). We report the average ImageReward and Aesthetic scores across 3120 and 5490 images on MS-CoCo and Drawbench, respectively (30 images per each prompt).