# What Can We Learn from Unlearnable Datasets?

Pedro Sandoval-Segura\({}^{1}\)  Vasu Singla\({}^{1}\)  Jonas Geiping\({}^{1}\)

Michal Goldblum\({}^{2}\)  Tom Goldstein\({}^{1}\)

\({}^{1}\)University of Maryland \({}^{2}\)New York University

{psando, vsingla, jgeiping, tomg}@umd.edu goldblum@nyu.edu

###### Abstract

In an era of widespread web scraping, unlearnable dataset methods have the potential to protect data privacy by preventing deep neural networks from generalizing. But in addition to a number of practical limitations that make their use unlikely, we make a number of findings that call into question their ability to safeguard data. First, it is widely believed that neural networks trained on unlearnable datasets only learn shortcuts, simpler rules that are not useful for generalization. In contrast, we find that networks actually can learn useful features that can be reweighed for high test performance, suggesting that image protection is not assured. Unlearnable datasets are also believed to induce learning shortcuts through linear separability of added perturbations. We provide a counterexample, demonstrating that linear separability of perturbations is not a necessary condition. To emphasize why linearly separable perturbations should not be relied upon, we propose an orthogonal projection attack which allows learning from unlearnable datasets published in ICML 2021 and ICLR 2023. Our proposed attack is significantly less complex than recently proposed techniques.1

## 1 Introduction

Deep learning is fueled by an abundance of data, the collection of which is largely unregulated [17; 20; 14; 32]. Images of human faces , artwork , and text [22; 24] are increasingly scraped at scale without consent. In an attempt to prevent the unauthorized use of data, unlearnable dataset methods make small perturbations to data so that deep neural networks (DNNs) trained on the modified data result in poor test accuracy [10; 7; 6; 36; 27]. The idea is that if generalization performance is harmed by incorporating the modified, "unlearnable" data, third parties will be disincentivized from scraping it. Unlearnable dataset methods answer the following question: _How should one imperceptibly modify the clean training set to cause the largest generalization gap?_

In this paper, we analyze properties of unlearnable dataset methods in order to assess their future viability and security promises. Currently, unlearnable dataset methods are unlikely to be used for safeguarding public data because they require perturbing a large portion (more than \(50\%\)) of the training set [10; 27] and adversarial training is a principled attack . Additionally, published data is immutable and must withstand current and future attacks . While there are a number of reasons why these unlearnable dataset methods are unlikely to be employed, our results shed light on privacy implications and challenge common hypotheses about how they work. We make several findings by analyzing a number of unlearnable datasets developed from diverse objectives and theory. In particular,

* We demonstrate that, in many cases, neural networks can learn generalizable features from unlearnable datasets. Our results imply that while resulting test accuracy may be low, DNNs may still learn useful features from protected data.

* We challenge the common belief that unlearnable datasets work due to linearly separable perturbations. We construct a counterexample, suggesting that there is room for new unlearnable dataset methods that evade common attacks.
* Inspired by our results on linear separability of perturbations, we present a new Orthogonal Projection attack which allows learning from unlearnable datasets perturbed by class-wise, linearly separable perturbations. Our attack demonstrates that class-wise applied perturbations do not protect data from being learned. Our attack is often more effective than adversarial training, at a fraction of the computational cost.

TerminologyData poisoning methods that perturb the entire training dataset, and which we refer to as "unlearnable datasets" or simply "poisons", are also known as availability attacks [7; 36], generalization attacks , delusive attacks , or simply unlearnable examples . Throughout this work, unlearnable datasets are considered defenses, given that their primary use case is to prevent the exploitation of data. Aiming to learn from unlearnable datasets is considered an attack.

## 2 Related Work

Unlearnable DatasetsOne of the earliest instances of a data poisoning attack intended to reduce overall test performance is from , who optimize a single sample that corrupts SVM training. Poisoning a convex model like SVM or logistic regression can be solved exactly, but poisoning deep neural networks is more challenging. A number of approaches have been proposed, validated primarily by empirical evidence due to the complexity of the objective (See Eq. 1 and 2).  propose using an auto-encoder-like network to generate perturbations against a victim classifier,  use NTKs to understand network predictions and optimize perturbations to produce misclassifications,  generate linearly separable perturbations, [10; 28] optimize error-minimizing noise whereas  optimize error-maximizing noise, and  generate autoregressive patterns, among other methods. The diversity of approaches motivates us to understand whether these unlearnable datasets share any properties in common, as we explore in Section 4.3.

Privacy VulnerabilitiesIn the context of image classification, unlearnable datasets are said to protect privacy by ensuring the modified data is not used or included as part of a larger dataset. That is, training on only poisoned data should prevent test set generalization and training on a dataset consisting of clean and poisoned data should be no better than training just on the clean portion. If poisoned training data is rendered useless for test set generalization, the data will not be used. In this way, only the initial owner can train on the original, unperturbed data and achieve high generalization performance. In recent work,  introduce the ability to "lock" and "unlock" training data by leveraging a class-wise perturbation, removable only by someone with knowledge of the exact perturbation. Unfortunately, as we empirically demonstrate in Section 4.4, class-wise perturbations can give a false sense of security. An argument against the use of poisoning for data privacy is the unavoidable reality that published data is immutable, and thus must withstand current _and_ future methods which may exploit the data . While it remains possible to recover relatively high test

Figure 1: We study unlearnable datasets constructed from additive perturbations. Although unlearnable datasets are created to prevent the exploitation of the clean dataset, we show that unlearnable datasets can be used to learn features which generalize to clean test data. We demonstrate that unlearnable datasets can be created without linearly separable perturbations. And we develop a novel method for recovering class-wise, linearly separable perturbations from unlearnable datasets.

accuracy from unlearnable datasets, our experiments also suggest that there is not yet a way to restore test accuracy to levels seen during clean training. Unlearnable datasets are known to be vulnerable to adversarial training [10; 31]. By formalizing unlearnable datasets as finding the worst-case training data within a \(\)-Wasserstein ball,  find that adversarial training  is a principled defense and can recover a high degree of accuracy by preventing models from relying on non-robust features.  challenge this assertion and optimize perturbations designed to degrade model performance under adversarial training. In Section 4.4, we show that adversarial training remains a strong defense against unlearnable datasets. In fact, we find that hyperparameter changes to the adversarial training recipe allows learning from _Robust Unlearnable Examples_, demonstrating the brittleness of unlearnable datasets.

How Unlearnable Datasets WorkThere are many explanations for how unlearnable datasets prevent networks from test set generalization: error-minimizing perturbations cause overfitting , error-maximizing noise stimulates learning non-robust features , convolutional layers are receptive to autoregressive patterns , and more. There are a variety of explanations because different methods arose from different optimization objectives and theory. But the leading explanation comes from Yu et al. , who find near perfect linear separability of perturbations for all the unlearnable datasets they consider. They explain that unlearnable datasets cause learning shortcuts due to linear separability of perturbations. In Section 4.3, we find a counterexample, demonstrating that while linear separability of perturbations may certainly be a property that helps unlearnable datasets function, the property is not necessary.

## 3 Problem Setting

We consider the problem of creating a clean-label unlearnable dataset in the context of a \(K\)-way image classification task, following . We denote the clean training and test datasets as \(_{}\) and \(_{}\), respectively.

Suppose there are \(n\) samples in a clean training set, i.e. \(_{}=\{(x_{i},y_{i})\}_{i=1}^{n}\) where \(x_{i}^{d}\) are the inputs and \(y_{i}\{1,...,K\}\) are the labels. We consider a unlearnable dataset, denoted \(}_{}=\{(x^{}_{i},y_{i})\}_{i=1}^{n}\) where \(x^{}_{i}=x_{i}+_{i}\) is the perturbed or poisoned version of the example \(x_{i}_{}\) and where \(_{i}^{d}\) is the perturbation. The set of allowable perturbations, \(\), is typically an \(_{p}\)-ball with radius \(\) where \(\) is small enough that \(\) is imperceptible.

Unlearnable datasets are created by applying a perturbation to a clean image in either a _class-wise_ or _sample-wise_ manner. When a perturbation is applied class-wise, every sample of a given class is perturbed in the same way. That is, \(x^{}_{i}=x_{i}+_{y_{i}}\) and \(_{y_{i}}_{C}=\{_{1},...,_{K}\}\). When samples are perturbed in a sample-wise manner, every sample has a unique perturbation.

All unlearnable dataset methods aim to solve the following bi-level maximization:

\[_{}_{(x,y)_{}}[ (f(x),y;())]\] (1)

\[()=*{arg\,min}_{}_{(x_{i},y_{i}) _{}}[(f(x_{i}+_{i}),y_{i}; )]\] (2)

Eq. 2 describes the process of training a model on unlearnable data, where \(\) denotes the model parameters. Eq. 1 states that the unlearnable data should be chosen so that the trained network has high test loss, and thus fails to generalize to the test set.

## 4 Experiments

### Datasets and Training Settings

For all of our experiments, we make use of open-source unlearnable datasets: From _Unlearnable Examples_, we use their sample-wise and class-wise poisons. From _Adversarial Poisoning_, we use their targeted PGD attack poison. From _Autoregressive Poisoning (AR)_, _Neural Tangent Generalization Attacks (NTGA)_, _Robust Unlearnable Examples_, and _Linearly Separable_Perturbations (LSP)_, we use their main poison. For _One-pixel Shortcut_, we use their OPS and CIFAR-10-S poisons, but we refer to their CIFAR-10-S poison as _OPS+EM_ for ease of attribution. We also include two class-wise _Regions-4_ and _Random Noise_ poisons which contain randomly sampled perturbations, following . To poison a \(K\)-class dataset in a class-wise manner, we need \(K\) perturbation vectors. For _Regions-4_, each perturbation is independently created by sampling \(4\) vectors of size \(3\) from a Bernoulli distribution. Each vector is then scaled to lie in the range \([-,]\). Finally, each of the \(4\) vectors are repeated along height and width dimensions to create patches of size \(16 16\), which are then arranged side by side to achieve a shape of \(32 32\). For _Random Noise_ perturbations, we sample an i.i.d. vector, one entry per pixel, from a Bernoulli and scale perturbations to fit the imperceptibility constraint. See Appendix A.5 for image samples for all unlearnable datasets we consider.

Results in this section are for CIFAR-10 , and additional results for SVHN , CIFAR-100 , and ImageNet  Subset are in Appendix A.2.2 and A.4.2. Unlearnable datasets constructed with class-wise perturbations are prefixed with \(\), otherwise the perturbations were added sample-wise. For the imperceptibility constraint, the _AR_ (\(_{2}\)) dataset contains perturbations \(\) of size \(\|\|_{2} 1\). _LSP_ has perturbations of size \(\|\|_{2}^{}\), where \(d\) is the image dimension and \(^{}=\). _OPS_ and _OPS+EM_ contain unbounded perturbations. All other datasets contain perturbations of size \(\|\|_{}\).

We primarily use the ResNet-18 (RN-18)  architecture for training on the above datasets. We include results for additional network architectures including VGG-16 , GoogleNet , and ViT  in Appendix A.2.1 and A.4.1. Training hyperparameters can be found in Appendix A.1.

### DNNs Can Learn Useful Features From Unlearnable Datasets

Unlearnable datasets are meant to be unlearnable; models trained on unlearnable datasets should not be capable of generalizing to clean test data. Presumably, the modified, unlearnable dataset protects the clean version of the dataset from unauthorized use in this fashion. But by reweighting deep features, we find that DNNs are still able to recover some generalizable features from unlearnable datasets.

DFR MethodThe technique of training a new linear classification layer on top of a fixed feature extractor is known as Deep Feature Reweighting (DFR)  and it is a method for learning from data with spurious correlations, where predictive features in the train distribution are not present in the test

   Training Data & Max DFR Test Accuracy & Min DFR Test Loss \\  None & 35.97 & 2.379 \\  Unlearnable Examples  & 39.56 (+3.59) & 1.798 \\ Adversarial Poisoning  & 69.99 (+34.02) & 1.036 \\ AR (\(_{2}\))  & 58.73 (+22.76) & 1.531 \\ NTGA  & 57.12 (+21.15) & 1.391 \\ Robust Unlearnable  & 41.02 (+5.05) & 1.790 \\ LSP  & 43.31 (+7.34) & 1.675 \\ OPS+EM  & 38.98 (+3.01) & 1.869 \\ \(\) OPS  & 47.70 (+11.73) & 1.697 \\ \(\) Unlearnable Examples  & 34.09 (-1.88) & 2.037 \\ \(\) Regions-4  & 48.09 (+12.12) & 1.706 \\ \(\) Random Noise & 66.79 (+30.82) & 1.352 \\   

Table 1: **Generalizable features can be learned from unlearnable datasets.** While published unlearnable datasets cause DNNs to train to low test accuracy, the features learned by DNNs can be reweighted using DFR to high test accuracy in many cases. For each unlearnable dataset, we report test accuracy and test loss for the best performing checkpoint after DFR. In gray, we indicate test accuracy improvement/deterioration over DFR on a randomly initialized RN-18. DFR uses \(5,000\) clean samples for finetuning. For the majority of unlearnable datasets, peak performance seems to occur early in training. High DFR Test Accuracy, and Low DFR Test Loss, indicates that useful features are learned during training.

distribution. We borrow the DFR method to better understand the utility of features learned during poison training. The higher the test accuracy after DFR, the more likely it is that the model has learned features present in the original clean data. If DFR works well on poison-trained networks, this would suggest that even poisoned weights contain a semblance of relevant information for generalization.

To evaluate the extent that generalizable features are learned, we start by saving network weights at every epoch of poison training. In the context of image classification, network weights consist of a feature extractor followed by a fully-connected classification layer mapping feature vectors to class logits. Next, for each checkpoint, we utilize a random subset of \(5,000\) clean CIFAR-10 training samples (\(10\%\) of the original training set) to train a new classification head (and keep the feature extractor weights fixed). Finally, we plot test accuracy in Figure 2 and evaluate the maximum CIFAR-10 test accuracy achieved through DFR for each dataset and report results in Table 1. As a baseline, we train a classification head on a randomly initialized RN-18 feature extractor and find that these random features can be reweighted to achieve \(35.97\%\) test accuracy.

DFR ResultsWe find that, to different extents, _DNNs do learn generalizable features_ from unlearnable datasets. Surprisingly, on _Adversarial Poisoning_ and _NTGA_, RN-18 features actually _improve throughout training_, as we show in Figure 2**(a)** and (**d**), despite low test accuracy during poison training (solid lines in Figure 2**(a)** and **(d)**). On _AR_ and \(\)_Random Noise_, test accuracy peaks early in training before dropping to random chance accuracy (solid lines in Figure 2**(b)** and **(c)**). Still, when early checkpoints are used, one can use features from the poison-trained feature extractors

Figure 2: **Features learned during poison training can be reweighed for high test accuracy. (a) When training on _Adversarial Poisoning_, RN-18 test accuracy (solid line) drops below random chance test accuracy. For the dashed line, each data point at epoch \(i\) represents the final test accuracy obtained after DFR using the poisoned checkpoint from epoch \(i\) of standard training. Using DFR, the final RN-18 poisoned checkpoint can achieve nearly \(70\%\) test accuracy (dashed line). Using DFR on a randomly initialized RN-18 only achieves \(40\%\) test accuracy (dotted line). (b-d) For different unlearnable datasets, at different points during poison training (marked by star), DFR test accuracy is more than \(20\%\) above the accuracy of a randomly initialized feature extractor, suggesting that features learned from unlearnable data are relevant for clean test set generalization.**

to achieve high accuracy (dashed lines in Figure 2**(b)** and **(c)**). Accuracy curves for _Adversarial Poisoning_ are unique because images are perturbed with error-maximizing noise, which correspond to actual features models use during classification . In this case, DFR is reweighting useful, existing features for classification, leading to higher test accuracy. On the other hand, _Random Noise_ and _AR_ poisons do not perturb images with useful features; instead, both perturb with synthetic noise. In these cases, useful features are still learned during poison training, but only in the first epochs. As training progresses, the model checkpoints are continually corrupted by synthetic noise features which cannot be useful for classification despite reweighting.

As we show in Table 1, \(\)_Unlearnable Examples_ are very effective at corrupting network weights during training; using DFR on checkpoints from poison training only yields a maximum test accuracy of \(34.09\%\), nearly \(2\%\) worse than a randomly initialized feature extractor. Six of the eleven unlearnable datasets we consider yield checkpoints which achieve DFR test accuracies that are \(10\%\) or more higher than a randomly initialized feature extractor. Generally, it is interesting that unlearnable dataset methods with the lowest max DFR test accuracy are those which use error-minimizing perturbations in some capacity, _e.g.__Unlearnable Examples_, _Robust Unlearnable Examples_, and _OPS+EM_. Analyzing test loss follows trends from test accuracy: _Adversarial Poisoning_, _AR_, _NTGA_, and _Random Noise_ achieve lowest losses - and those poisoned checkpoints also have the highest DFR test accuracy in Table 1. More interestingly, we find that _all_ poisoned models have a lower loss than the randomly initialized model.2 This reinforces our claim that models learned useful features from poisoned data.

Overall, our results suggest that network weights during poison training are not fully corrupted in many cases. While poison-trained networks may be evaluated to have low test accuracy, we show that, for some unlearnable datasets, the networks have learned generalizable features which can be reweighted for high test performance. It is entirely possible that these features, which encode the original data amount to useful image information that the original data owner did not want incorporated into an ML system.

Privacy concerns are often cited as primary motivation for unlearnable datasets, yet our results raise concerns about this framing. A model trained on an apparently unlearnable dataset might have low test error, but this does not imply, as we show, that it does not contain usable information about the original data, and might not keep promises to protect data privacy.

### Linearly Separable Perturbations Are Not Necessary

The work of Yu et al.  demonstrated that unlearnable datasets contain linearly separable perturbations and suggested that linear separability is a necessary property of unlearnable datasets. However,

   Training Data & Train Accuracy \\  Clean & 53.88 \\  Unlearnable Examples  & 100 \\ Adversarial Poisoning  & 100 \\ AR (\(_{2}\))  & **39.58** \\ NTGA  & 100 \\ Robust Unlearnable  & 100 \\ LSP  & 100 \\ OPS+EM  & 100 \\ \(\) OPS  & 99.93 \\ \(\) Unlearnable Examples  & 100 \\ \(\) Regions-4  & 100 \\ \(\) Random Noise & 100 \\   

Table 2: **Linearly separable perturbations are not necessary to create unlearnable datasets. We train a linear logistic regression model on perturbations and report train accuracy. High train accuracy indicates linear separability of perturbations. Unlike perturbations from other unlearnable datasets, Autoregressive Perturbations (AR) are not linearly separable and are, in fact, less separable than clean CIFAR-10 images.**by finding a counterexample, we confirm that linear separability is not necessary to have an effective unlearnable dataset.

Following , we train linear logistic regression models on image perturbations by subtracting the clean image from the perturbed image. Given the publication of new unlearnable datasets, we add to Yu et al.'s analysis by including _AR_, _Robust Unlearnable Examples_, and _OPS_ poisons. It is possible to obtain image perturbations given our access to clean CIFAR-10, but should not be possible in practice if the clean version of the unlearnable dataset is unavailable to the public. We optimize the logistic regression weights (\(3072 10\) parameters for CIFAR-10) using L-BFGS  for 500 steps using a learning rate of \(0.5\). For reference, we also train a logistic regression model on clean CIFAR-10 image pixels and report the train accuracy in the first row of Table 2. For results on the linear separability of the perturbed _images_, see Appendix A.3.2.

While most current unlearnable datasets contain linearly separable perturbations, as was initially shown by Yu et al. , _AR_ perturbations stand out as a counterexample in Table 2. After training on _AR_ perturbations, a logistic regression model can only achieve \(39.58\%\) train accuracy while all other unlearnable datasets we tested contain perturbations that are almost perfectly linearly separable. Because most unlearnable dataset methods developed from diverse optimization objectives result in linearly separable perturbations, we posit that linear separability is an easy solution for optimization of Eq. 1 and 2 to find. For unlearnable datasets that are not optimized, class-wise perturbations are trivially linearly separable, so long as no two class perturbations are the same. Thus, although _AR_ perturbations are not linearly separable, they remain separable by a simple 2-layer CNN , suggesting that simple perturbations may be a more accurate way of defining effective unlearnable perturbations.

Being the first counterexample of its kind, we believe there may be underexplored unlearnable dataset methods. This finding sheds light on the complexity of unlearnable datasets, whose behavior depends on choices of loss function, optimizer, and even network architecture. An important detail about _AR_ perturbations is that they are not optimized - they are generated . We leave to future work an investigation as to why most optimized perturbations in unlearnable datasets to date result in linear separability.

### Orthogonal Projection for Learning From Datasets with Class-wise, Linear Perturbations

In this section, we develop a method to learn from unlearnable datasets. Given our understanding of linear separability from Section 4.3, learning the most predictive features in an unlearnable dataset can amount to learning the perturbation itself. We leverage this intuition in our Orthogonal Projection attack, which works by learning a linear model, then projecting the data to be orthogonal to linear weights. In other words, we remove the most predictive dimensions of the data. For unlearnable datsets perturbed by class-wise, linearly separable perturbations, the most predictive dimensions are easily found by a linear model. Attacks for learning from class-wise perturbed unlearnable datasets have included using diffusion models , adversarial training , and error-maximizing augmentations . In contrast to these techniques, our method is simpler and computationally cheaper.

#### 4.4.1 Orthogonal Projection Method

Unlearnable datasets with class-wise perturbations are so simple that visualizing the average image of a class can expose the class perturbation (see Figure 5). In order to adaptively learn these perturbations and remove them from poisoned data, we propose a method that first learns the simple perturbations and then orthogonally projects samples to omit them. Our Orthogonal Projection method is designed to exploit class-wise perturbations by design and is meant to emphasize why class-wise perturbations should not be relied on for creating unlearnable datasets.

Given an unlearnable dataset, we train a simple logistic regression model on poison image pixels, in an attempt to optimize the most predictive linear image features. The result is a learned feature matrix \(W\), on which we perform a QR decomposition such that \(W=QR\), where \(Q\) consists of orthonormal columns. Image samples \(X\) from the unlearnable dataset can be orthogonally projected using the matrix transformation \(I-QQ^{T}\). Orthogonal projection ensures that the dot product of a row of \(X\) with every column of \(Q\) is zero, so that learned perturbations from the logistic regression model are not seen during the final training run on recovered data. The recovered image samples are then written as \(X_{r}=(I-QQ^{T})X\). Finally, we train the final network on recovered data, \(X_{r}\). A detailed algorithm is shown in Algorithm 1. We include additional intuition in Appendix A.4.3.

The added computational cost of Orthogonal Projection over standard training is only in training the logistic regression model and projecting the dataset, which can be done once. For CIFAR-10, we train the logistic regression model for 20 epochs using SGD with an initial learning rate of \(0.1\), which decays by a factor of \(10\) on epochs \(5\) and \(8\). We find that training the linear model for longer is often detrimental, potentially because of overfitting.

#### 4.4.2 Orthogonal Projection Results

We compare our attack against \(_{}\) adversarial training, as previous work has shown that adversarial training is a principled defense  against unlearnable datasets. For adversarial training, we use a \(3\)-step PGD adversary with a perturbation radius of \(\) in \(_{}\)-norm. Note that large adversarial training perturbation radii harm clean accuracy . We use SGD with momentum of \(0.9\) and an initial learning rate of \(0.1\), which decays by a factor of \(10\) on epoch \(75\), \(90\), and \(100\).

We find our method is _more effective than adversarial training_ when learning from unlearnable datasets corrupted in a class-wise manner. In Table 3, our Orthogonal Projection attack leads to greater test accuracy on all class-wise poisons prefixed by \(\). By visualizing the learned weights from

Figure 3: **Our Orthogonal Projection attack learns linearly separable features. In (a), we visualize how LSP  and OPS  unlearnable data is created by taking clean image and adding a perturbation. In (b), we visualize learned weights (\(W\) in Algorithm 1) after training a linear model on LSP and OPS images; we also include learned weights after training on clean CIFAR-10 for reference. Learned weights from LSP and OPS unlearnable data nearly match the original perturbation, while learned weights from clean data resemble the corresponding class. In our Orthogonal Projection attack, we project each perturbed image to be orthogonal to each of these learned weights (Algorithm 1, Line 6).**

the linear model in Figure 3, we see that the class-wise, linearly separable perturbations are recovered. Projecting orthogonal to these vectors effectively removes those features from the unlearnable dataset, allowing training to proceed without predictive, but semantically meaningless features. It should not be surprising that Orthogonal Projection is also effective against _OPS_ (ICLR 2023), given that all images of a class have the same pixel modified. In Figure 3, we visualize the linear model weights and show that it is able to pinpoint the modified class pixel, whose influence is effectively removed after projection in our method. We visualize additional linear model weights for other datasets in Appendix A.4.5. Orthogonal Projection is also surprisingly effective against _NTGA_ (ICML 2021), such that an RN-18 model can achieve \(82.21\%\) test accuracy when trained on orthogonally projected _NTGA_ images. For seven of the eleven unlearnable datasets we consider, Orthogonal Projection can achieve above \(80\%\) test accuracy without adversarial training. Unlike the linear model from Section 4.3 which is trained on only perturbations, the linear model that we train for Orthogonal Projection trains on perturbed images and the resulting data is no longer linearly separable. For this reason, Orthogonal Projection struggles against _Adversarial Poisoning_, _AR_, and _Unlearnable Examples_; the training distribution of the linear model is more complex than just perturbations. Given that _OPS+EM_ is a combination of _OPS_ and class-wise error-minimizing noise from _Unlearnable Examples_, both Orthogonal Projection and adversarial training are ineffective. For both _OPS_ and _OPS+EM_, adversarial training achieves approximately \(12\%\) accuracy, a dismal result that is expected given the unbounded _OPS_ perturbations. A limitation of our approach is that, by orthogonally projecting data, we effectively remove \(K\) dimensions from the data manifold, where \(K\) is the number of classes in a dataset. While this may not be a problem for high-resolution images with tens of thousands of dimensions, this detail could impact applicability for low-resolution datasets.

The susceptibility of using class-wise perturbations to craft unlearnable datasets should be expected, given that every image of a class contains the same image feature that is perfectly predictive of the label. Class-wise unlearnable datasets contain simple features that are perfectly correlated with the label and the linear logistic regression model is able to optimize features resembling the original perturbations. By design, our orthogonal projection attack is most effective against class-wise perturbed unlearnable datasets. It serves as evidence that class-wise, linearly separable perturbations cannot be relied upon for protecting data.

## 5 Conclusion

We design experiments to test prevailing hypotheses about unlearnable datasets, and our results have practical implications for their use. First, while all unlearnable datasets cause low test accuracy for

    &  \\ Training Data & None & Adv Training & Ortho Proj (Ours) \\  Clean & 94.37 & 87.16 (-7.21) & 90.16 (-4.21) \\  Unlearnable Examples  & 24.56 & **85.64** (+61.08) & **65.17** (+40.61) \\ Adversarial Poisoning  & 7.96 & **85.32** (+77.36) & 14.74 (+6.78) \\ AR (\(_{2}\))  & 13.77 & **84.09** (+70.32) & 13.03 (-0.74) \\ NTGA  & 40.78 & **84.85** (+44.07) & 82.21 (+41.43) \\ Robust Unlearnable  & 26.86 & **87.07** (+60.21) & **25.83** (-1.03) \\ LSP  & 25.75 & 86.18 (+60.43) & **87.99** (+62.24) \\ OPS+EM  & 20.11 & **12.22** (-7.89) & **39.43** (+19.32) \\ \(\) OPS  & 15.35 & 11.77 (-3.58) & **87.94** (+72.59) \\ \(\) Unlearnable Examples  & 14.49 & **85.56** (+71.07) & **89.98** (+75.49) \\ \(\) Regions-4  & 10.32 & 85.74 (+75.42) & **86.87** (+76.55) \\ \(\) Random Noise & 10.03 & 86.36 (+76.33) & **90.37** (+80.34) \\   

Table 3: **Orthogonal Projection can make class-wise unlearnable data learnable.** Especially for unlearnable datasets with class-wise, linearly separable perturbations, our Orthogonal Projection attack improves CIFAR-10 test accuracy over \(_{}\) Adversarial Training at a fraction of the computational cost. We train RN-18 using different kinds of unlearnable training data and, for every attack, we indicate accuracy improvement/deterioration over standard training (no attack) in gray. For every dataset, we bold the highest test accuracy achieved.

trained models, we demonstrate that generalizable features can still be learned from data. In many cases, high test accuracy can be achieved from a feature extractor trained on unlearnable datasets. If data were modified for protection, an adversary may still be able to train a reasonable feature extractor, a harm for data thought to be secure. We advocate for our evaluation framework to be used for auditing whether new unlearnable datasets prevent useful features from being learned. Second, we find that the reason unlearnable datasets cause low test accuracy is not as simple as previously thought. AR perturbations serve as a counterexample to the hypothesis that unlearnable datasets contain linearly separable perturbations. Our finding that AR perturbations are not linearly separable suggests there could be underexplored methods for protecting data using imperceptible perturbations. Although unlearnable datasets need not have linearly separable perturbations, if they do have them applied in a class-wise manner, we present an Orthogonal Projection attack that can effectively remove them. While learning the perturbations from perturbed images is challenging, our results imply class-wise perturbations _cannot_ not be relied upon for data protection. We can learn a significant amount from unlearnable datasets. DNNs can learn generalizable features from unlearnable datasets assumed to be protected, linear separability is not a necessary condition for preventing generalization from unlearnable datasets, and class-wise perturbations can be optimized against and effectively removed.