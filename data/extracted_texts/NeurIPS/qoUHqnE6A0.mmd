# Repeated examples help learn arithmetic

Francois Charton

FAIR, Meta & Ecole des Ponts

fcharton@meta.com &Julia Kempe

FAIR, Meta & NYU CDS and Courant Institute

kempe@meta.com

###### Abstract

We study small transformers trained on two problems of arithmetic: the greatest common divisor (GCD) and modular multiplication, and show that models trained on a limited set of repeated examples achieve better performance than models trained from unlimited data. In fact, modular multiplication is only learned on small training sets. We also demonstrate that _two-set training_ - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. These experiments highlight that the benefits of repetition can outweigh those of data diversity; and shed light on the still poorly understood interplay between generalization and memorization in deep learning.

## 1 Introduction

When training neural networks, it has become customary to use the largest and most diverse datasets available, and to limit example reuse as much as possible. On problems of arithmetic, the training data is easy to generate in very large quantities, sometimes even on the fly. For this reason, models are trained on very large sets of single-use examples, with very low repetition. In this paper, we investigate the _low data regime_: transformers trained on smaller sets of repeated examples. We consider two problems: the greatest common divisor (GCD, Charton (2024)) of two integers, and multiplication modulo \(67\) of two positive integers between \(1\) and a million, and measure model performance for different _data budgets_ (DB, the number of distinct examples in the training set), and _training budgets_ (TB, the total number of training examples). On the GCD problem, we show that, for a given training budget, models trained on small data budgets (but large enough to avoid overfitting) outperform models trained on large or unlimited data budgets. We also show that modular multiplication is _only learned_ for small data budgets. These experiments demonstrate the benefit of training on repeated examples, challenging the common idea that one or two epochs is all we need.

Pushing this observation further, we demonstrate that for a given data budget, model performance can be greatly improved by _two-set training_: selecting at random a small subset of training examples, and repeating them more often during training. This two-set effect is all the more surprising as the repeated examples are not curated, and only differ from the rest of the training sample by their frequency of reuse. In fact, ablation experiments indicate that the performance of two-set training cannot be improved by curating the set of repeated examples, or refreshing it as training proceeds. We also show that mixing repeated and non-repeated examples in the same mini-batches is a necessary step for the two-set effect to appear.

The benefits of repetition are significant in both problems, but come in different flavors. For GCD, repetition allows for better performance and faster learning. For modular multiplication, it unlocks an emergent capability: without repetition, the model does not learn. We believe these findings have profound implications and should lead to a paradigm shift where the training set size becomes a mere hyper-parameter, not solely governed by the availability of data and the belief that more is always better.

Experimental settings

We focus on two problems of arithmetic: computing GCD and multiplication modulo \(67\). The GCD was studied in prior work (Charton, 2024; Dohmatob et al., 2024).

In the **greatest common divisor** problem, the model is tasked to predict the GCD of two integers uniformly distributed between \(1\) and \(1\) million, encoded in base \(1000\). Following Charton (2024), who observes that throughout training almost all pairs of integers with the same GCD are predicted the same, we evaluate model performance by the number of GCD below \(100\) predicted correctly, measured on a random test sample of \(100,000\) pairs: \(1000\) pairs for each GCD from \(1\) to \(100\). On this metric, Charton (2024) reports a best performance of \(22\) correct GCD for a model trained on uniformly distributed inputs. This test metric is preferable to a more standard measure of accuracy on random input pairs, because GCD are distributed according to an inverse square law (\(61\%\) of random pairs have GCD \(1\), \(15\%\) have GCD \(2\)), making accuracy a very optimistic measure of performance.

In **modular multiplication**, we train models to predict the product, modulo \(67\), of two integers between \(1\) and a million. Arithmetic modulo \(p\) was studied in several previous works, in the context of grokking (Power et al., 2022; Liu et al., 2022) and mechanistic interpretability (Zhong et al., 2023), but with model inputs sampled from \(0\) to \(p-1\), which results in a very small problem space for small \(p\). We evaluate model accuracy as the percentage of correct predictions of \(a b 67\), on a test set of \(10,000\) examples (generated afresh at every evaluation).

**Models and tokenizers.** We use sequence-to-sequence transformers (Vaswani et al., 2017) with \(4\) layers in the encoder and decoder, an embedding dimension of \(512\), and \(8\) attention heads (\(35\) million trainable parameters). Models are trained to minimize a cross-entropy loss, using the Adam optimizer (Kingma & Ba, 2014), with a learning rate of \(10^{-5}\), and batches of \(64\). The integer inputs and outputs of both problems are tokenized as sequences of digits in base \(1000\), preceded by a sign which serves as a separator. All experiments are run on one V100 GPU with \(32\) GB of memory.

## 3 Repetition Helps

In a first series of experiments, we compare the performances of models trained on different data budgets (number of distinct examples) for increasing training budgets (total examples). We consider DB of \(1,5,10,25,50\) and \(100\) million examples, together with the unlimited case, where examples are generated on the fly (yielding DB\(\)TB). Figure 1 (Left) presents the average number of GCD predicted by \(5\) models trained on different DB, for increasing TB. For a small TB of \(30\) million examples, models trained on \(1\) and \(5\)M DB achieve the best performance: \(20\) GCD vs \(13\) for all others. As TB increases, 1M-models start overfitting, as shown by increasing test losses in Figure 1 (Right), and their performance saturates at \(21\) correct GCD. The performance of the \(5\)M models keeps improving to \(36\) GCD, for a TB of \(150\) million examples, then begins to overfit, and saturates around \(38\). For TB of \(150\) and \(300\) million examples, the best performing models are the \(10\)M.

Figure 1: **GCD problem:** (Left) GCD accuracy for different data and training budgets (average of 5 models). (Right) Test loss of models as a function of training budget, for fixed data budgets.

For TB between \(450\)M and \(1.05\) billion examples, models trained on \(25\)M DB achieve the best performances. Throughout training, the models trained on small data budgets learn fastest. Past a certain TB, they overfit their training data, and their performance saturates. On the other hand, models trained on large or unlimited DB perform the worst. For a TB of one billion examples, models trained on \(100\)M or unlimited data budgets only predict \(37\) and \(27\) GCD, way worse than models trained on \(25\) and \(50\)M (\(62\) and \(60\)). When learning GCD, smaller data budgets and more frequent repetition allow for faster learning, and much better performance.

**Modular multiplication**, with a TB of \(600\) million, tells a different story. Models with DB of \(10\)M or less, or \(100\)M or more do not learn the task, and achieve about chance level accuracies, predicting all outcomes as \(0\), for an accuracy slightly over \(3\%\) (Table 1). On the other hand, models trained on \(25\) and \(50\)M distinct examples (repeated \(24\) and \(12\) times on average) do learn the task: \(25\%\) of models trained with this DB achieve \(99\%\) accuracy, and a majority achieves \(50\%\). On this task, learning emerges from repetition: models trained on small DB learn a task inaccessible to models trained on larger DB. (note: increasing TB to \(2\)B examples, some models trained on \(100\)MDB do learn, but none of the unlimited data models do).

These experiments clearly indicate that repetition helps learning. On both tasks, for a fixed training budget, models trained on a small data budget, i.e. fewer distinct examples, repeated several times, achieve much better performance than models trained from single-use examples, or repeated very few times, as is customary in most recent works on language models (Muennighoff et al., 2023). Smaller data budgets and repeated examples elicit "emergent learning".

## 4 Two-set training

We now turn to a different problem: how to best use a given data budget? Because repetition helps learning, we want a small subset of repeated examples, but we also observed that, after a certain training budget, models trained on small datasets start overfitting, which cause their performance to saturate. To balance these two effects, we propose **two-set training**: randomly splitting the training sample into a small set of \(S\) examples, used with probability \(p\), and repeated many times, and a large set, used with probability \(1-p\), and repeated a few times. By doing so, we hope that the small set fosters learning, while the large set prevents overfitting.

On the **GCD problem**, with a data budget of \(100\) million examples and a training budget of \(600\) million, models trained on a repeated set of 250,000 examples or less, used with a probability of \(p\) of \(0.25\) or \(0.5\), predict more than \(62\) GCD on average (Figure 2), vs \(27\) in the single-set case. The best results, \(69\) GCD, are achieved for \(S=50,000\) and \(p=0.25\). In this setting, small set examples are repeated \(3000\) times on average, large set examples \(4.5\) times.

These results extend to unlimited data budgets, using a fixed set of \(S\) examples. The best choices of \(p\) and \(S\) are roughly the same as with a DB of \(100\)M (Figure 3 in Appendix B). For \(p=0.25\) and \(S=50,000\), two-set training achieves an average performance of \(67\) GCD on \(6\) models, a spectacular improvement over models trained on a single set, which predict \(25\) GCD on average. For smaller DB (\(25\) or \(50\)M), two-set training provides for faster learning (Figure 4, Appendix B).

For **modular multiplication**, we need larger samples of repeated examples: \(S\) between \(2.5\) and \(10\)M examples for \(100\)M DB, and \(S=25\)M with unlimited DB (Figure 5 Appendix C). Examples from the small set are repeated less: from \(20\) to \(60\) times, vs \(3000\) for the GCD. Yet, two-set training results in improved performances for all data budgets over \(25\)M (Table 2). More than \(50\%\) of all the models we trained could learn modular multiplication with over \(99\%\) accuracy (\(90\%\) with more than \(50\%\) for DB larger than \(50\)M). In contrast, single-set training achieved \(99\%\) accuracy for \(25\%\) of models with DB \(50\)M or less, and for none of the models trained on larger data budgets (Table 1).

    & &  \\  & 1 & 5 & 10 & 25 & 50 & 100 & unlimited \\  Average accuracy (\(\%\)) & 1.6 & 3.8 & 4.4 & 40.4 & **59.5** & 5.4 & 3.0 \\ Number of models achieving 99\% accuracy & 0/5 & 0/5 & 0/5 & 6/25 & **7/25** & 0/30 & 0/30 \\ Number of models achieving 50\%+ accuracy & 0/5 & 0/5 & 0/5 & 13/25 & **22/25** & 0/30 & 0/30 \\ Number of models trained & 5 & 5 & 5 & 25 & 25 & 30 & 30 \\   

Table 1: **Multiplication modulo 67**. Accuracy of models trained on a budget of 600 million data points.

Ablation studies (Appendix D) indicate that curating the small sample - selecting easier, or particular examples for the repeated set (an obvious strategy for improving two-set training), brings at best a marginal increase in performance. They also indicate that mixing repeated and non-repeated examples in the same mini-batches is an essential element of two-set training. Models trained on batches exclusively selected from the small and large sets do not learn.

## 5 Conclusion

Our findings indicate that the performance of math transformers can be greatly improved by training them on datasets which include repeated examples. This can be done by using smaller train set, or randomly selecting a repeated subset from a larger corpus. On the GCD problem, repeated examples allow for faster learning and better performance. For modular arithmetic, they are necessary for the model to learn. This suggests that abandoning the customary practice of training models on the largest possible set of single-use example may be beneficial.

Our observations on two-set training, in particular the fact that the repeated set can be selected at random (and that curation does not help) are thought-provoking. All that seems to matter is that the _exact same_ examples are repeated, i.e. not their informational content. This is all the more shocking as repetition occurs at a very low frequency. In the GCD experiments, examples in the small set are repeated \(3000\) times over a TB of \(600\) millions: once in 200,000 examples on average. For modular multiplication, the frequency is even lower. Besides, the repeated examples _need to be mixed_ with non-repeated examples into mini-batches for the two-set effect to appear.

This raises several tantalizing questions: how does the transformer "figure" that a given example, lost in a minibatch, has been seen, hundreds of thousands of examples before? Our research suggests that there exists a qualitative difference between "deja vu" and "jamais vu" examples - data points the model has already seen, or never seen. How do transformers, and perhaps other architectures, identify, and then process, "deja vu" examples? To our knowledge, this aspect was overlooked in many prior works on model interpretation. It is an intriguing subject for further study.

    &  &  &  \\  & & \(>50\%\) & \(>99\%\) & \(>50\%\) & \(>99\%\) \\ 
25M & 0.1 / 1M & 50 & **50** & 52 & 24 \\
50M & 0.25 / 2.5M & 90 & **50** & 88 & 28 \\
100M & 0.5 / 10M & 88 & **54** & 0 & 0 \\ Unlimited & 0.25 / 2.5M & 92 & **58** & 0 & 0 \\   

Table 2: **Two-set training on modular multiplication.** Percentage of models (different random initializations) learning to compute modular multiplication with \(50\) and \(99\%\) accuracy. Training budget: \(600\)M. For DB \(25\)M and \(50\)M, \(10\) models with two-set training, and \(25\) with single set training. For DB \(100\)M and unlimited, \(26\) models with two-set training, and \(30\) with single set training.

Figure 2: **Two-set training for the GCD problem:** Number of correctly predicted GCD as a function of \(S\) and \(p\). Each measurement is the average of \(6\) models. Data budget \(100\)M, training budget \(600\)M. Note the high performance for very small sets \(S\) of sizes \(50\), \(75\), \(100\), \(150\) and \(200\) thousand, with \(p=0.25\) and \(p=0.5\).