# Data Distribution Valuation

Xinyi Xu

Department of Computer Science

National University of Singapore

xinyi.xu@u.nus.edu

&Shuaiqi Wang

Department of Electrical and Computer Engineering

Carnegie Mellon University

shuaiqiu@andrew.cmu.edu

&Chuan-Sheng Foo

Institute for Infocomm Research

Agency for Science, Technology and Research

foo_chuan_sheng@i2r.a-star.edu.sg

&Bryan Kian Hsiang Low

Department of Computer Science

National University of Singapore

lowkh@comp.nus.edu.sg

Giulia Fanti

Department of Electrical and Computer Engineering

Carnegie Mellon University

gfanti@andrew.cmu.edu

###### Abstract

Data valuation is a class of techniques for quantitatively assessing the value of data for applications like pricing in data marketplaces. Existing data valuation methods define a value for a discrete dataset. However, in many use cases, users are interested in not only the value of the dataset, but that of the _distribution_ from which the dataset was sampled. For example, consider a buyer trying to evaluate whether to purchase data from different vendors. The buyer may observe (and compare) only a small preview sample from each vendor, to decide which vendor's data distribution is most useful to the buyer and purchase. The core question is _how should we compare the values of data distributions from their samples?_ Under a Huber characterization of the data heterogeneity across vendors, we propose a maximum mean discrepancy (MMD)-based valuation method which enables theoretically principled and actionable policies for comparing data distributions from samples. We empirically demonstrate that our method is sample-efficient and effective in identifying valuable data distributions against several existing baselines, on multiple real-world datasets (e.g., network intrusion detection, credit card fraud detection) and downstream applications (classification, regression).

## 1 Introduction

_Data valuation_ is a widely-studied practice of quantifying the value of data . Today, data valuation methods define a value for a discrete dataset \(D\), i.e., a fixed set of samples . However, many emerging use cases require a data user to evaluate the quality of not just a dataset, but the _distribution_ from which the data was sampled. For example, data vendors in markets like Datarade and Snowflake, financial data streams , information security data ) offer a preview in the form of a sample dataset to prospective buyers . Similarly, enterprises selling access to generative models may offer a limited preview of the output distributions to prospective buyers . Buyers use these sample datasets to decide whether to pay for a full dataset or data stream--i.e., access to the data distribution. Concretely, the buyers would compare between different data distributions (via their respective sample datasets) to determine and select the more valuable one.

In such applications, existing dataset valuation metrics are missing two components: (1) They do not formalize the value of the underlying sampling distribution, (2) nor do they provide a theoretically principled and actionable policy for comparing different sampling distributions based on the sample datasets. For example, most existing data valuation techniques are designed only to value a dataset . To our knowledge, there are no methods designed to value an underlying distribution.

To model this problem, we consider a data buyer who wishes to evaluate \(n\) data vendors, each with their own dataset \(D_{i}\) drawn i.i.d. from distribution \(P_{i}\), where \(i[n]\). The vendors' distributions are _heterogeneous_, i.e., the \(P_{i}\)'s can differ across vendors. Such distributional heterogeneity can arise from natural variations in data  or adversarial data corruption . The buyer's goal is to select the vendor whose data distribution is closest in some sense (to be defined) to a reference distribution \(P^{*}\), which is fixed but unknown. Our goal is to provide both a precise definition for the value of the vendor distribution \(P_{i}\) with respect to \(P^{*}\), as well as a corresponding valuation for the sample dataset \(D_{i} P_{i}\). In particular, we want precise conditions under which, given two datasets \(D_{i}\) and \(D_{j}\) drawn from distributions \(P_{i}\) and \(P_{j}\), respectively, we can conclude that \(P_{i}>P_{j}+_{}\) for some user-specified \(_{}>0\) with fixed probability. While this problem is straightforward when different vendors have the same underlying distribution, the main challenge is accounting for the heterogeneity in the data.

We thus identify three technical and modeling challenges: _(i)_ What is a suitable heterogeneity model that captures realistic data patterns, while also admitting theoretical analysis? _(ii)_ How should one define the value of a distribution under a given heterogeneity model? _(iii)_ Many existing data valuation methods use a reference \(P^{*}\), either explicitly  or implicitly . However, there are practical difficulties: (1) different data vendors may disagree on the choice of reference , (2) such a reference may not be available _a priori_, or (3) dishonest vendors may try to overfit to the reference . To address this, some works consider alternatives (to \(P^{*}\)) based on the vendors' distributions \(P_{i}\)'s such as their union  or a certain combination , but without theoretical analysis or justifications for their specific choices. So what is a practical alternative, and how can we theoretically justify it?

To address these challenges, we make three key choices.

_(a) Heterogeneity model._ We assume that each vendor's data distribution \(P_{i}\) is a Huber model , which is a mixture model of the unknown distribution \(P^{*}\) and an arbitrary outlier distribution \(Q_{i}\). While the Huber model does not capture all kinds of statistical heterogeneity, mixture models both are a reasonable model for data heterogeneity in practice  and have deeper roots in robust statistics . More importantly, the Huber model enables a direct and precise characterization of the effect of heterogeneity on the value of data (under our design choices _(b)_ and _(c)_ below), which has not been considered in prior works .

_(b) Value of a sampling distribution._ We use the negative maximum mean discrepancy (MMD)  between a reference distribution \(P^{*}\) and \(P\) as the value of the sampling distribution \(P\). Then, we leverage a (uniformly converging) MMD estimator  to derive actionable policies for comparing sampling distributions with theoretical guarantees. In other words, a buyer can compare (the values of) sampling distributions \(P_{i},P_{i^{}}\) (from vendors \(i,i^{}\)) based on the respective samples \(D_{i},D_{i^{}}\) to determine which is more valuable, and by how much.

_(c) Choice of reference data distribution and dataset._ Unlike in prior works (e.g., ), we do not specify a reference distribution outright. Instead, we consider a class of convex mixtures of vendor distributions as the reference. We first derive error guarantees and an actionable comparison policy for using general convex mixture of the vendors' distributions as the reference. We then propose to use the special case of a uniform mixture, justified by a game-theoretic argument, stating that the uniform strategy is worst-case optimal in a two-player zero-sum game.

These design choices are not isolated answers to each technical challenge, but collectively address these challenges (e.g., the analytic properties of both Huber and MMD are needed to compare distributions effectively). Our specific contributions are summarized as follows:

* We formulate the problem of data distribution valuation in data markets and study an MMD-based method for data distribution valuation. Under a Huber model of data heterogeneity, we show that this data valuation metric admits actionable, theoretically-grounded policies for comparing sampling distributions from samples.

* We derive an error guarantee (Proposition 2) and comparison policy (Theorem 1) for a general convex mixture \(P_{}\) as a reference (in place of \(P^{*}\)), thus relaxing the common assumption of knowing \(P^{*}\). We then identify the uniform mixture as a game-theoretic special case for \(P_{}\).
* We demonstrate on real-world classification (e.g., network intrusion detection) and regression (e.g., income prediction) tasks that our method is sample-efficient, and effective in identifying the most valuable sampling distributions against existing baselines. For example, on classification tasks, we observed that an MMD-based valuation method outperformed four leading valuation metrics on 3 out of 4 classification settings (Table 2).

## 2 Related Work

Existing dataset valuation methods fall roughly in \(2\) categories: those that assume a given reference dataset, and those that do not. We defer additional discussion to App. B due to space constraints.

**With a given reference.** Several existing methods require a _given reference_ in the form of a validation set (e.g., [24; 39]) or a baseline dataset . Data Shapley , Beta Shapley  and Data Banzhaf [43; 63] utilize the validation accuracy of a trained model as the value of the training data. Class-wise Shapley  evaluates the effects of a dataset on the in-class and out-of-class validation accuracies. Both LAVA  and DAVINZ  use a proxy for the validation performance of the training data as their value, instead of the actual validation performance, to be independent of the choice of downstream task ML model  or to remove the need for model training . Differently,  assume that the buyer provides a baseline dataset as the reference to calculate a relevance score used to evaluate the vendor's data. Therefore, these methods _cannot_ be applied without such a given reference, which can be difficult to obtain in practice [14; 56]. In contrast, our method can be applied without a given reference, by carefully constructing a reference (Sec. 4.2).

**Without a given reference.** To relax the assumption of a given reference, [14; 60; 66] construct a reference from the data from all vendors. While the settings of [14; 60; 66] can include heterogeneity in the vendors' data, they do not explicitly formalize it and thus cannot precisely analyze its effects on data valuation. In contrast, our method, via the careful design choices of the Huber model (for heterogeneity) and MMD (for valuation), uniquely offers a precise analysis on the effect of heterogeneity on the value of data (Eq. (2)). Furthermore, these methods did not provide theoretical guarantees on the error arising from using their constructed reference in place of the ground truth (i.e., \(P^{*}\)). In contrast, by exploiting Observation 1 in the setting of multiple vendors and the MMD-based valuation (Eq. (1)), we provide such theoretical guarantees (e.g., Proposition 2). In a different approach to relax the assumption of a given reference, [56; 70] remove the dependence on a reference; as a result they can produce counter-intuitive data values under heterogeneous data (experiments in Sec. 5). The closest related work to ours is , which adopts the MMD\({}^{2}\) as a valuation metric, primarily for computational reasons. However, this work does not consider data _distribution_ valuation, nor does it describe how to compare (the values of) distributions, let alone with theoretical guarantees. This comparison (with MMD\({}^{2}\)) is expanded in App. B.2.

Table 1 provides a comparison between our choice (MMD) and three others: the Kullback-Leibler (KL) divergence [1; 66], the Wasserstein distance (WD) , and squared MMD (MMD\({}^{2}\)) .

## 3 Model, Problem Statement and MMD

We consider a set of data vendors \(i N:=\{1,,n\}\), each with a _sample_ dataset \(D_{i}:=\{z_{i,1},,z_{i,m_{i}}\}\) of size \(m_{i}\), where \(z_{i,j}\) is sampled i.i.d. from the distribution \(P_{i}\). Slightly abusing notations, we write \(D_{i} P_{i}\). We assume the existence of an unknown ground truth distribution \(P^{*}\), called the test distribution [24; 32], true data distribution  or the task distribution .

**Huber model.** We assume that each sampling distribution \(P_{i}\) follows a Huber model , defined as follows: \(P_{i}=(1-_{i})P^{*}+_{i}Q_{i}\) where \(_{i}[0,1)\) and \(Q_{i}\) is a distribution that captures the heterogeneity of vendor \(i\). For notational simplicity, we omit the subscript \(i\) and write \(D,P\) instead of \(D_{i},P_{i}\), when it is clear. We adopt the Huber model because (i) it is sufficiently general to model various sources of heterogeneity [12; 13]; (ii) Huber models are "closed" under mixtures (i.e., a mixture of Hubers is a Huber model), so we can define a mixture over data vendors' distributions:

**Observation 1**.: For a mixture weight \((n-1)\),1 define the mixture \(P_{}_{i N}_{i}P_{i}\). Then, \(P_{}=(1-_{})P^{*}+_{}Q_{}\) where \(_{}=_{i N}_{i}_{i}\) and \(Q_{}=(1/_{})_{i N}(_{i}_{i}Q_{i})\).

The mixture distribution \(P_{}\) (of individual \(P_{i}\)'s) is a Huber model, which is used in the theoretical results in Sec. 4.2. Define \(D_{}\) as the sample dataset by randomly sampling from each \(D_{i}\) w.p. \(_{i}\), so effectively \(D_{} P_{}\). In particular, if \(\) is uniform (i.e., \( i,_{i}=1/n\)), we denote the corresponding \(P_{}\) as \(P_{}\), \(D_{}\) as \(D_{}\), \(_{}\) as \(_{}\) and \(Q_{}\) as \(Q_{}\). We further expand our considerations of the Huber model to characterize data heterogeneity w.r.t. existing works in App. B.3. Later, we also empirically investigate non-Huber settings where our method (in Sec. 4) remains effective (App. D.3.4).

**Problem statement**.: Given two datasets \(D P\) and \(D^{} P^{}\), we seek a distribution valuation function \(()\) and a dataset valuation function \(()\) which enable a set of conditions under which to conclude that \((P)>(P^{})\), given only \((D)\) and \((D^{})\). Moreover, we seek a practical implementation of \(\) that does _not_ require access to the ground truth distribution \(P^{*}\) as reference or any prior knowledge about the vendors (except each \(P_{i}\) is Huber).

Existing methods cannot be easily applied to solve this problem. First, existing methods (e.g., [24; 32] do not define \(\); hence, they cannot analyze the conditions under which \((P)>(P^{})\). In App. C, we elaborate why a dataset valuation \(\) cannot be easily extended to a data distribution valuation \(\) and also highlight the theoretical appeal of directly considering \(\) instead. Additionally, methods that explicitly require access to the reference distribution via \(D^{*} P^{*}\) (e.g., [39; 55; 67]) _cannot_ be applied here. For other methods, additional non-trivial assumptions (e.g., [14, Assumption 3.2], [66; Assumption 3.1]) are required; we elaborate on these in App. B.1.

### Maximum Mean Discrepancy (MMD)

The MMD is an integral probability metric proposed to test if two distributions are the same.

**Definition 1** (MMD, [26, Definition 2]).: For a class \(\) of functions \(f\) in the unit ball of the reproducing kernel Hilbert space associated with a kernel function \(k\), the MMD, which is symmetric, between two distributions \(P,P^{}\) is \(d(P,P^{};)_{f}_{X P }f(X)-_{W P^{}}f(W)\).

The MMD has a (biased) estimator for \(D P\) and \(D^{} P^{}\), and \(|D|=m,|D^{}|=m^{}\)[26, Eq. (5)]: \((D,D^{})[}_{x,x^{} D}k(x,x^{ })-}_{(x,w) D D^{}}k(x,w)+}_{w,w^{} D^{}}k(w,w^{})]^{0.5}\). Importantly, this estimator satisfies uniform convergence (Lemma 1), which is used in our theoretical results (e.g., Proposition 1). We denote with \(K\) an upper bound on the kernel \(k\): \( x,x^{}\), \(k(x,x^{}) K\). As \(\) is associated with \(k\) and kept constant throughout, its notational dependence is suppressed.

## 4 MMD-based Data Distribution Valuation

A distribution valuation function should intuitively reward distributions \(P\) that are "closer" to the reference distribution \(P^{*}\); accordingly, it should assign greater reward to datasets \(D P\) that are drawn from distributions that are closer to \(P^{*}\). We study the following data distribution valuation:

\[(P)-d(P,P^{*})\,(D)-(D,D^{*})\.\] (1)

To interpret, the value \((P)\) of a vendor's sampling distribution \(P\) is defined as the negated MMD between \(P\) and \(P^{*}\), while the value \((D)\) of its sample dataset \(D\) is defined as the negated MMD estimate between \(D\) and the reference dataset \(D^{*}\).

**On the choice of MMD.** We summarize a comparison with three alternatives (i.e., KL-divergence (KL), Wasserstein Distance (WD), and MMD2) in Table 1 to highlight the suitability of MMD for data distribution valuation. Despite its wide adoption, KL is difficult to estimate, and the available estimator only has asymptotic convergence guarantees  (rather than a finite-sample result), which can be arbitrarily slow. Its implementation also suffers from the curse of dimensionality [59; 64]. In addition, KL does not satisfy the triangle inequality, which our proof technique uses in Proposition 2. WD, also known as the optimal transport (OT) distance , suffers from the curse of dimensionality, as seen in the complexity results  in Table 1, and is more computationally costly to evaluate than MMD. MMD2, though shares similar complexity results to MMD, does not satisfy desirable analytic properties, such as the triangle inequality or the property with Huber model. This comparison over divergences is expanded in App. B.2.

MMD is both practically and theoretically appealing for data distribution valuation. Practically, MMD has lower sample and computational complexities in the dimension of the data, which is important because the real-world datasets can be complex and have a high dimension. Specifically, we leverage the uniform convergence (with sample complexity as in Table 1) of an MMD estimator to derive an actionable policy for comparing two distributions (i.e., Proposition 1, Theorem 1). Theoretically, MMD satisfies the triangle inequality, making it amenable to theoretical analysis, such as the derivation of our error guarantee (i.e., Proposition 2). Moreover, MMD pairs well with the Huber model in providing a precise characterization of the effect of heterogeneity on the value of data. In contrast, existing valuation works have _not_ established a formal analysis on the heterogeneity (w.r.t. a specific choice for heterogeneity) on the value of data, elaborated in App. B.3.

**Effect of heterogeneity on data valuation.** Intuitively, the quality of a Huber distribution \(P\) depends on both the size of the outlier component \(\) and the statistical difference \(d(P^{*},Q)\) between \(Q\) and \(P^{*}\). A larger \(\) and/or a larger \(d(P^{*},Q)\) decreases the value \((P)\). Our choice of MMD makes this intuition precise and interpretable: By Lemma 2, for \(P=(1-)P^{*}+ Q\),

\[(P)=- d(P^{*},Q)\;.\] (2)

Eq. (2) shows that for a fixed \(\), \(P\)'s value decreases linearly w.r.t. \(d(P^{*},Q)\); similarly, for a fixed \(d(P^{*},Q)\), \(P\)'s value decreases linearly w.r.t. \(\). Importantly, Eq. (2) enables subsequent results and a theoretically justified choice for the reference (e.g., Lemma 4 to derive Theorem 1 in Sec. 4.2).

### Data Valuation with a Ground Truth Reference

With Eq. (1), we return to the problem statement described above: given two datasets \(D P\) and \(D^{} P^{}\), under what conditions can we conclude that \((P)>(P^{})\)? We first assume access to a reference dataset \(D^{*} P^{*}\). We then relax this assumption in Sec. 4.2.

**Proposition 1**.: Given datasets \(D P\) and \(D^{} P^{}\), let \(m:=|D|\) and \(m^{}:=|D^{}|\). Let \(D^{*} P^{*}\) and \(m^{*}|D^{*}|\) be its size. For some bias requirement \(_{} 0\) and a required decision margin \(_{} 0\). If \((D)>(D^{})+_{,}\) where the _criterion margin_\(_{,}_{}+2[_{}++}+2}]\). Let \( 2(}^{2}m^{*}}{2K( +m^{*})})\) where \(=\{m,m^{}\}\). Then, \((P)>(P^{})+_{}\) with probability at least \((1-2)\).

_(Proof in App. A)_ Proposition 1 describes the criterion margin \(_{,}\) such that if \((D)-(D^{})>_{,}\)--i.e., the criterion is met--we can draw the conclusion that \((P)>(P^{})+_{}\) at a confidence level of \(1-2\). Hence, a smaller \(_{,}\) corresponds to an "easier" criterion to satisfy. The expression \(_{,}=(_{}+_{}+1/})\) where \(\{m,m^{},m^{*}\}\) highlights three components that are in tension: a buyer-defined decision margin \(_{}\), a bias requirement \(_{}\) from the MMD estimator (Lemma 1), and the minimum size \(\) of the vendors' sample datasets (assuming \(m^{*}\{m,m^{}\}\)). If the buyer requires a higher decision margin \(_{}\) (i.e., the buyer wants to determine if \(P\) is more valuable than \(P^{}\) by a larger margin), then it may be necessary to (i) set a lower bias requirement \(_{}\) and/or (ii) request larger sample datasets from the vendors. In (i), suppose \(\) remains unchanged, a lower \(_{}\) reduces the confidence level \(1-2\) since \(\) increases as \(_{}\) decreases. Hence, although the buyer concludes that \(P\) is more valuable than \(P^{}\) by a higher decision margin, the corresponding confidence level is lower. In (ii), suppose \(_{}\) remains unchanged, a higher minimum sample size \(\) increases the confidence level.2 In other words, to satisfy the buyer's higher decision margin, the

    & sample & computational & triangle inequality & Huber \\  KL & asymptotic & N.A. & ✗ & ✗ \\ WD & \((1/m^{1/})\) & \((m^{3} m)\) & ✓ & ✗ \\ MMD2  & \((1/)\) & \((m^{2})\) & ✗ & ✗ \\  MMD & \((1/})\) & \((m^{2})\) & ✓ & ✓ \\   

Table 1: Comparison with KL, WD and MMD2, on sample and computational complexities, triangle inequality and connection with the Huber model. dim is the dimension of the random variable/data.

vendors need to provide larger sample datasets. This can also help the buyer increase their confidence level if the criterion is satisfied. Proposition 1 illustrates the interaction between a buyer and data vendors: The buyer's requirement is represented by the decision margin, and the vendors must provide sufficiently large sample datasets to satisfy this requirement.

### Approximating the Reference Distribution

Previously we assumed \(P^{*}\) could be accessed as the reference. We now relax this assumption by replacing \(P^{*}\) with a mixture distribution \(P_{}\) over all the vendors' distributions, as defined in Observation 1. We first prove an error guarantee to generalize Proposition 1 when using \(P_{}\) instead of \(P^{*}\). We then use a game-theoretic formulation to motivate the choice of the _uniform_ mixture, \(P_{}\). Formally, using \(P_{}\) as the reference (with \(D_{} P_{}\)) instead of \(P^{*}\) gives the following valuation:

\[(P)-d(P,P_{})\,(D)- (D,D_{})\.\] (3)

Namely, \(\) is an approximation to \(\) (equiv. \(\) to \(\)), with a bounded (approximation) error as follows,

**Proposition 2**.: Recall \(_{},Q_{}\) from Observation 1. Then, \( P,|(P)-(P)|_{}d(Q_{},P ^{*})\).

(_Proof in App. A_) Proposition 2 provides an error bound from using \(P_{}\) as the reference, which linearly depends on \(_{}\) and \(Q_{}\): A lower \(_{i}\) (i.e., \(P_{i}\) has a lower outlier probability) gives a lower \(_{}\), and a lower \(d(Q_{i},P^{*})\) (i.e., \(P_{i}\)'s outlier component is closer to \(P^{*}\)) leads to a lower \(d(Q_{},P^{*})\), resulting in a smaller error from using \(P_{}\) as the reference. Using this error guarantee, we give our main result, which provides a decision criterion for concluding that for candidate vendor distributions \(P\) and \(P^{}\), their valuations satisfy \((P)>(P^{})+_{}\), for some user-specified decision margin \(_{}\). Unlike Proposition 1, this result does not require access to ground truth \(P^{*}\), but instead uses a practically-realizable mixture \(P_{}\).

**Theorem 1**.: Given datasets \(D P\) and \(D^{} P^{}\), let \(m|D|\) and \(m^{}|D^{}|\). Let \(D_{}\), \(\) be from Eq. (3) and \(m_{N}|D_{}|\). For some bias requirement \(_{} 0\) and a required decision margin \(_{} 0\), suppose \((D)>(D^{})+^{}_{,}\) where the _criterion margin_\(^{}_{,}_{}+2[_ {}++}+2}+_{ }d(Q_{},P^{*})]\). Let \(^{} 2(^{2}m_{N}}{2K( +m_{N})})\) where \(=\{m,m^{}\}\). Then \((P)>(P^{})+_{}\) with probability at least \((1-2^{})\).

(Proof in App. A) Compared with Proposition 1, the criterion margin \(^{}_{,}\) has an additional term of \(2_{}d(Q_{},P^{*})\), which depends on both the size of the outlier component \(_{}\) and the statistical difference \(d(Q_{},P^{*})\) between \(Q_{}\) and \(P^{*}\).3 This term explicitly accounts for the statistical difference \(d(P_{},P^{*})\) to generalize Proposition 1: \(d(P_{},P^{*})=0\) recovers Proposition 1. Importantly, this result implies that using \(P_{}\) (to replace \(P^{*}\)) retains the previous analysis and interpretation: a buyer's requirement via the decision margin can be satisfied by the vendors providing (sufficiently) large sample datasets, which is empirically investigated in a comparison against existing valuation methods (Sec. 5). We highlight that Theorem 1 exploits the closed property of Huber models (via Observation 1), the triangle inequality of MMD (via Proposition 2) and the uniform convergence of the MMD estimator. Hence, the modeling and design choices of Huber and MMD are both necessary.

#### 4.2.1 A Game-theoretic Choice of Mixture

The above results hold for general mixture distributions \(P_{}\), begging the question: Which mixture should one use (i.e., what \(\))? A game-theoretic formulation reveals that the uniform strategy is worst-case optimal, so we propose to use the uniform mixture \(P_{}\) as the special case of \(P_{}\).

Consider the following two-player zero-sum game. A payoff matrix \(\) consists of \(n\) rows, one corresponding to each vendor index, and \(n!\) columns, one corresponding to each permutation over the vendor indices. The row player (i.e., the agent conducting the data valuation) picks a vendor index \(r N\). The column player (hypothetical adversary) then adversarially chooses a permutation \(_{c}\) over the indices in \(N\). Hence, the action space for the row player is \(N\) and that for the column player is all possible permutations of \(\{1,2,n\}\).4 The column player represents the fact that the row player lacks prior knowledge about vendors: hence it selects an index \(r\) in any possible arbitrary permutation \(_{c}\). Then, for a pair of actions \((r,_{c})\), the quality of the distribution \(P_{_{c}[r]}\) is the row player's payoff \(_{r,c}-d(P^{*},P_{_{c}[r]})\), defined as the negated MMD between \(P_{_{c}[r]}\) and the optimal distribution \(P^{*}\) (i.e., a lower MMD means a higher payoff), specifying the following optimization:

\[_{r}_{c}_{r,c}\] (4)

where \(^{n(n!)}\) is the payoff matrix and \(\) (\(\)) denotes the row (column) player's action. A strategy \(_{}(n-1)\) (as an \(n\)-dimensional probability vector) specifies the probability with which the row player picks a data vendor (at a position).

While efficient linear program solvers to Eq. (4) are available for explicitly specified \(\), in our setting, \(\) is not explicitly specified due not knowing \(P^{*}\). Fortunately, we show that the uniform strategy is optimal _without_ knowing \(\) explicitly:

**Proposition 3**.: The optimal solution for the row player to Eq. (4) is \(_{}^{*}=[,,, ]\,.\)

(_Proof in App. A_) Intuitively, a uniform strategy over the vendors cannot be exploited by the column player, and is thus worst-case optimal. We highlight that while uniform strategy being worst-case optimal may seem intuitive, the mathematical properties and derivations needed are less straightforward. In particular, the proof depends on the "closed" property of Huber (i.e., Observation 1) and the "linearity" of MMD applied to Huber (i.e., Eq. (2)) to exploit the strong duality of a linear program. We also discuss alternative formulations to Eq. (4) in App. A.

Then, we adopt the uniform mixture \(P_{}\) as the special case of \(P_{}\) in Eq. (3):

\[(P)=-d(P,P_{})\;,(D)-(D, D_{})\;.\] (5)

Proposition 2 and Theorem 1 are applied directly in App. A. The uniform mixture \(P_{}\) is inspired from the solutions to Eq. (4), which is a game based on _not_ having prior knowledge about the vendors. In this setting of no ground truth and no prior knowledge about the vendors, one might wonder if/how we can derive a lower bound of error to crystalize the difficulty of the problem (our Proposition 2 gives an upper bound of error for general \(P_{}\), and Corollary 1 is for \(P_{}\)). We expand this in App. C, making references to robust statistics and mechanism design.

## 5 Empirical Results

We first compare the sample efficiency of several baselines, and then investigate the effectiveness of our method in ranking \(n\) data distributions. Additional information on experimental settings is in App. C and additional results under non-Huber settings, and on scalability are in App. D. Our code is available at https://github.com/XinyiYS/Data_Distribution_Valuation.

**Baselines.** To accommodate the existing methods which explicitly require a validation set (Sec. 2), we perform some experiments using a validation set \(D_{} P^{*}\). This assumption is made only for empirical comparison, and subsequently relaxed. The baselines that explicitly require \(D_{}\) are class-wise Shapley (CS) [55, Eq. (3)], LAVA  and DAVINZ [67, Eq. (3)]; the baselines that do not require \(D_{}\) are information-gain value (IG) [56, Eq. (1)], volume value (VV) [70, Eq. (2)] and MMD\({}^{2}\)[60, Eq. (1)], which implements a biased estimator of MMD\({}^{2}\). For each baseline, we adopt their official implementation if available. Note that though theoretically MMD\({}^{2}\) is obtained by squaring MMD, the estimator for MMD\({}^{2}\) is _not_ obtained by squaring the MMD estimator (elaborated in App. B), so they give different results. Note that DAVINZ also includes the MMD as a specific implementation choice, linearly combined with a neural tangent kernel (NTK)-based score. However, their theoretical results are specific to NTK and not MMD, while our result (e.g., Theorem 1) is MMD-specific.

Our implementation of MMD, including the radial basis function kernel, follows . To implement our proposed uniform mixture \(P_{}\) in cases where \(D_{i}\)'s have different sizes, we do the following: denote the minimum dataset size by \(m_{}_{i}|D_{i}|\). Then for each \(D_{i}\), uniformly randomly sample a subset \(D_{i,} D_{i}\) of size \(m_{}\) from \(D_{i}\), and use the union \(D_{}_{i}D_{i,}\).

**Datasets.** We consider both classification (Cla.) and regression (Reg.) since some baselines (i.e., CS, LAVA) are specific to classification while some (i.e., IG, VV) are specific to regression. Our method is applicable to both. CaliH (resp. KingH) is a housing prices dataset in California  (resp. in Kings county ). Census15 (resp. Census17) is a personal income prediction dataset from the 2015 (resp. 2017) US census. . Credit7  and Credit31  are two credit card fraud detection datasets. TON  and UGR16  are two network intrusion detection datasets.

Many of our evaluations are conducted under a Huber model, which requires matched supports of \(P^{*}\) and \(Q\), such as MNIST, EMNIST and FaMNIST, all in \(^{32 32}\), CIFAR10 and CIFAR100, and Census15 and Census17. Other datasets require additional pre-processing: CaliH and KingH are standardized and pre-processed separately to be in \(^{10}\). Additional pre-processing details in App. D. Subsequently, each \(P_{i}\) follows a Huber: \(P_{i}=(1-_{i})P^{*}+_{i}Q\) (i.e., \( i,Q_{i}=Q\)). We also run experiments on non-Huber settings in App. D, where our method remains effective.

**ML model M.** For model-specific baselines such as DAVINZ and CS, in Sec. 5.2, we adopt a \(2\)-layer convolutional neural network (CNN) for MNIST, EMNIST, FaMNIST; ResNet-18  for CIFAR10 and CIFAR100; logistic regression (LogReg) for Credit7 and Credit31, and TON and UGR16; linear regression (LR) for CaliH and KingH, and Census15 and Census17. Details are in App. D.

### Sample Efficiency via Empirical Convergence

Our goal is to find a sample-efficient policy that correctly compares \((P)\) vs. \((P^{})\) by comparing \((D)\) vs. \((D^{})\), even if the sizes of \(D,D^{}\) are small. In practice, there is no direct access to \(P,P^{}\) but only \(D,D^{}\); the sizes of \(D,D^{}\) may not be very large. Here, we compare \(\{(D_{i})\}_{i N}\) to \(\{(P_{i})\}_{i N}\) as we vary dataset size \(m_{i}\).

**Setting.** We implement \((P_{i})\) as approximated by a \((D_{i}^{*})\) where \(D_{i}^{*} P_{i}\) with a large size \(m_{i}^{*}\) (e.g., \(10,000\) for \(P^{*}=\) MNIST vs. \(Q=\) EMNIST). Denote the values of the samples as \(_{m_{i}}\{(D_{i})\}_{i N}\) where the sample size \(m_{i}=|D_{i}|\) and the approximated ground truths as \(^{*}\{(D_{i}^{*})\}_{i N}\); in this way, \(^{*}\) is well-defined respectively for each comparison baseline (i.e., _not_ our MMD definition Eq. (1)). We highlight that each \(\) (i.e., each baseline) is evaluated against its corresponding \(^{*}\) to demonstrate the empirical convergence. This is to examine the practical applicability of each \(\) when the sizes of the provided \(\{D_{i}\}_{i N}\) are limited.

**Evaluation and results.** We evaluate three criteria--the \(_{2}\) and \(_{}\) errors and the number of pair-wise inversions as follows, \(\|_{m_{i}}-^{*}\|_{2},\|_{m_{i}}-^{*}\|_{}\) and \((_{m_{i}},^{*})(1/2)_{i,i^{ }[n],i i^{}}((D_{i}^{*})>(D_{i^{*}}^{*}) (D_{i})<(D_{i^{}}))\). In words, if the conclusion via \((D_{i})\) vs. \((D_{i^{}})\)_differs_ from that via \((D_{i}^{*})\) vs. \((D_{i^{}}^{*})\), it is an inversion. For all \(3\) criteria, lower is better.

Fig. 1 (and Figs. 2 to 5 in App. D) demonstrate that our MMD-based method is overall (one of) the _most sample-efficient_ for different evaluation criteria and datasets, validating the theoretical results (Table 1) that MMD is more sample-efficient than WD.

### Ranking Data Distributions

Motivated by the use-case of a buyer identifying the best data vendor(s), we measure our ability to rank \(n\) distributions based on the values of sample datasets.

**Setting.** For a valuation metric \(\) (e.g., Eq. (1)), denote the values of datasets from all vendors as \(\{(D_{i})\}_{i N}\). To compare against different baselines (i.e., other definitions of \(\)), we define the

Figure 1: The \(3\) criteria (on \(y\)-axis) for \(P^{*}\) = MNIST vs. \(Q\) = EMNIST. \(n=5,m_{i}^{*}=10,000\). \(x\)-axis shows sample size in percentage, i.e., \(m_{i}/m_{i}^{*}\) where \(m_{i}^{*}\) is fixed to investigate how the criteria change w.r.t. \(m_{i}/m_{i}^{*}\): If the criteria decrease quickly w.r.t. \(m_{i}/m_{i}^{*}\), it means the metric converges quickly (i.e., sample-efficient). Results averaged (standard errors bars) over \(5\) independent trials.

following _common_ ground truth, the expected test performance \(_{i}_{D_{i} P_{i}}[((D_{i});D_{ })]\) of an ML model \((D_{i})\) trained on \(D_{i}\), over a fixed test set \(D_{}\) where the expectation is over the randomness of \(D_{i}\). Let \(\{_{i}\}_{i N}\). The ML model \(\) is specified previously and the test set \(D_{}\) is from the respective \(P^{*}\) and _not_ seen by any data vendor. Note that \(D_{}\) is used to obtain \(_{i}\) for comparison purposes, and it is _not_ to be confused with \(D_{}\), which is required by some baselines as part of their methods.

**Utilizing labels.** We also extend our method to explicitly consider the label information via the conditional distributions of labels given features (i.e., \(P_{Y|X}\)), denoted as _Ours cond_. Other baselines such as LAVA and CS already explicitly use label information. Specifically, for \(D_{i}\) containing paired features and labels, we fit a learner \((D_{i})\) on \(D_{i}\) and use its predictions on \(D_{}\) (thus we require \(D_{}\)) as an empirical representation of the \(P_{Y|X}\) for \(D_{i}\) and compute the MMD between the conditional distributions (more implementation details in App. D). Unlike our original method (i.e., Ours), this variant differs in exploiting the feature-label pairs in \(D_{i}\). We relax the assumption \(D_{} P^{*}\) by replacing \(D_{}\) with \(D_{}\), namely for the baselines needing an explicit reference, we use \(D_{}\). The resulting data values are denoted as \(}\) (the data values based on \(D_{}\) are denoted as \(\)).

**Evaluation metric.** Here \(\) is effective if it identifies the most valuable sampling distribution, or more generally, if \(\) preserves the ranking of \(\). In other words, the ranking of the data vendors' sampling distributions \(\{P_{i}\}_{i N}\) is correctly identified by the values of their datasets \(\{D_{i}\}_{i N}\), quantified via the Pearson correlation coefficient: \((,)\) (higher is better). Note that we compare the rankings of different baselines instead of the actual data values which can be on different scales.

**Results.** We report the average and standard error over \(5\) independent random trials, on CIFAR10/CIFAR100, TON/UGR16, CaliH/KingH and Census15/Census17 in Tables 2 and 3 respectively, and defer the others to App. D. Note that when \(D_{}\) is unavailable (i.e., right columns), Ours cond. is not applicable because the label-feature pair information is not well-defined under the Huber model (e.g., for \(P^{*}=Q=()\) for \(\) for classification (resp. regression) is accuracy (resp. coefficient of determination (COD)), so higher is better.

For classification, Table 2 shows that our method performs well when \(D_{}\) is available (e.g., Ours cond. is the highest for CIFAR10 vs. CIFAR100 under \((,)\)) and also when \(D_{}\) is unavailable (e.g., Ours as highest for CIFAR10 vs. CIFAR100 under \((},)\)). MMD\({}^{2}\) performs comparably to Ours, which is expected since in theory their values differ only by a square and the evaluation mainly focuses on the rank, instead of the absolute values. We also note that CS, by exploiting the label information in classification, performs competitively with \(D_{}\), but performs sub-optimally without \(D_{}\). This is because the label information in \(D_{}\) is no longer available in \(D_{}\) (due to \(D_{}\) being Huber). LAVA and DAVINZ, both exploiting the gradients of the ML model, do not perform well. The reason could be that under the Huber model, the gradients are not as informative about the values of the data. Intuitively, while the gradient of (the loss of) a data point on an ML model can be informative about the value of this data point, this reasoning is not applicable here, because the data point may not be from the same true distribution \(P^{*}\): The value of a gradient obtained on a CIFAR100 image to an ML model intended for CIFAR10 may not be informative about the value of this CIFAR100 image. We highlight that neither of LAVA and DAVINZ was originally proposed for such cases (i.e., the Huber model).

    &  &  \\  & \((,)\) & \((},)\) & \((,)\) & \((},)\) \\  LAVA & -0.907(0.01) & -0.924(0.01) & 0.254(0.26) & -0.159(0.38) \\ DAVINZ & -0.437(0.10) & -0.481(0.13) & -0.201(0.26) & -0.529(0.21) \\ CS & 0.889(0.03) & -0.874(0.02) & 0.451(0.19) & 0.256(0.28) \\ MMD\({}^{2}\) & 0.764(0.02) & 0.563(0.01) & 0.526 (0.11) & **0.480(0.15)** \\  Ours & 0.763(0.02) & **0.564(0.02)** & **0.584(0.17)** & 0.461(0.14) \\ Ours cond. & **0.989(0.01)** & N.A. & 0.562(0.16) & N.A. \\   

Table 2: Pearson correlations between data sample values and data distribution values for classification.

For regression, Table 3 shows that Ours and MMD\({}^{2}\) continue to perform well while baselines (i.e., IG and VV) that completely remove the reference perform poorly, as they cannot account for the statistical heterogeneity without a reference. Notably, DAVINZ performs competitively for when \(D_{}\) is available, due to its implementation utilizing a linear combination of an NTK-based score (i.e., gradient information) and MMD (similar to Ours), via an auto-tuned weight between the two. We find that for classification, the NTK-based score is dominant while for regression (and available \(D_{}\)) the MMD is dominant. This could be because the models are more complex for the classification tasks (e.g., ResNet-18) as compared to linear regression models for regression, so the obtained gradients are more significant (i.e., higher numerical NTK-based scores). Thus, for regression, DAVINZ produces values similar to Ours, hence the similar performance. We highlight that DAVINZ focuses on the use of NTK w.r.t. a given reference, while our method focuses on MMD _without_ such a reference, as evidenced by Ours outperforming DAVINZ without \(D_{}\) (i.e., the columns under \((},)\) in Table 3).

## 6 Discussion

Under a Huber model of vendor heterogeneity, we propose an MMD-based data distribution valuation and derive theoretically-justified policies for comparing distributions from their respective samples. To address the lack of access to the true reference distribution, we use a convex mixture of the vendors' distributions as the reference, and derive a corresponding error guarantee and comparison policy. Then, we specifically select the uniform mixture as a game-theoretic choice when no prior knowledge about the vendors is assumed. Empirical results demonstrate that our method performs well in efficiently identifying the most valuable data distribution. While our theoretical results are limited to the Huber model, MMD is observed to be effective under two non-Huber settings (App. D.3.4). Extending the theory to more general heterogeneity models is an interesting direction for future study.