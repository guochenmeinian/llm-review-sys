# Towards Safe and Honest AI Agents with

Neural Self-Other Overlap

 Marc Carauleanu

AE Studio

marc@ae.studio &Michael Vaiana

AE Studio

mike@ae.studio &Judd Rosenblatt

AE Studio

judd@ae.studio &Cameron Berg

AE Studio

cameron@ae.studio &Diogo Schwerz de Lucena

AE Studio

diogo@ae.studio

###### Abstract

As AI systems increasingly make critical decisions, deceptive AI poses a significant challenge to trust and safety. We present Self-Other Overlap (SOO) fine-tuning, a promising approach in AI Safety that could substantially improve our ability to build honest artificial intelligence. Inspired by cognitive neuroscience research on empathy, SOO aims to align how AI models represent themselves and others. Our experiments with Mistral 7B Instruct v0.2 demonstrate SOO's efficacy: deceptive responses in this large language model dropped from 95.2% to 15.9% with no observed reduction in general task performance, while in reinforcement learning scenarios, SOO-trained agents showed significantly reduced deceptive behavior. SOO's focus on internal representations offers strong potential for generalization across AI architectures. While current applications focus on language models and simple RL environments, SOO could pave the way for more trustworthy AI in broader domains. Ethical implications and long-term effects warrant further investigation, but SOO represents a significant step forward in AI safety research.

## 1 Introduction

Deceptive behavior in AI agents poses a significant threat to their safety and trustworthiness, undermining both individual and societal trust . Instances of AI deception, such as an AI in the game Diplomacy using strategies to outmaneuver opponents  and language models like GPT-4 generating misleading information , demonstrate the need for effective mitigation strategies . Addressing this issue requires novel approaches to encourage honesty in AI .

Recent research shows that LLMs like Mistral 7B Instruct v0.2 can internally represent beliefs of self and others , and that self-modeling can reduce model complexity, making the model more predictable, which can aid cooperation and safety . Building on this, our study presents a method designed to reduce AI deception while maintaining performance and is inspired by neural self-other overlap  with roots in cognitive neuroscience. We define _Self-Other Overlap (SOO)_ as the extent to which a model exhibits similar internal representations when reasoning about itself and others in similar contexts.

In neuroscience, both the mirror neuron theory and the perception-action model suggest that empathy is mediated by neural self-other overlap, where representations of self and others partially converge . "Extraordinary Altruists"--those who perform extreme acts of altruism--show increased neural overlap in the anterior insula, a region involved in empathy . More generally, altruists are found to lie less when deception would harm others . In contrast, individuals with psychopathictraits exhibit reduced neural overlap in response to pain stimuli [17; 18] and are more likely to deceive and manipulate . These findings suggest that the degree of neural self-other overlap may influence not only empathy and altruism but also the propensity for deception.

Inspired by these neuroscientific insights, we propose a novel approach to reduce deceptive behavior in artificial agents by aligning their internal representations during potentially deceptive scenarios (other-referencing) with those from similar honest scenarios (self-referencing). To implement this concept, we introduce a loss function that minimizes the difference between the model's processing of self-referencing and other-referencing inputs during fine-tuning.

We evaluate the effectiveness of this Self-Other Overlap (SOO) technique in reducing model deception across two domains: a large language model task and a multi-agent reinforcement learning environment. This study presents a scalable paradigm for enhancing AI honesty, potentially applicable across diverse artificial intelligence applications.

## 2 Related Work

Self-other overlap (SOO) remains relatively underexplored in machine learning, though related techniques have emerged. _Empathic Deep Q-Learning (DQN)_ mitigates harmful behaviors by simulating another agent's perspective, but it relies on hand-coded mechanisms, limiting scalability . Similarly, _Self-Other Modeling (SOM)_ improves learning by predicting others' actions using an agent's own policy, though it assumes similar reward structures and requires ongoing optimization over inferred goals, increasing computational complexity . In contrast, SOO fine-tuning focuses on reducing deception with fewer assumptions, offering broader applicability across various models and tasks.

Beyond SOO-specific techniques, _representation engineering_ methods aim to modify how models internally process and structure their representations to promote safer and more reliable behaviors in AI systems . SOO fine-tuning fits within this framework but stands out by targeting a specific type of representation: the self-other distinctions that underlie deceptive and adversarial behaviors. While other representation control methods rely on contrastive prompts focused on behavioral outcomes, SOO fine-tuning aims to directly reduce the representational differences between self and other stimuli, offering a more targeted and potentially more efficient solution compared to broader representation control techniques.

_Path-specific objectives_, which train agents to avoid "unsafe" pathways that may lead to undesirable behaviors like deception, represent another approach to addressing these concerns . These objectives, often informed by Causal Influence Diagrams , focus on identifying causal chains leading to risky behaviors. While effective in some settings , the complexity of identifying these pathways limits their scalability and they could restrict an agent's effectiveness by constraining the available optimization paths for performing tasks.

Finally, strategies such as _Reinforcement Learning from Human Feedback (RLHF)_ and _Constitutional AI_ fine-tune models based on feedback from human or AI evaluators to promote truthfulness and honesty [7; 8]. However, these methods often face challenges in distinguishing truthfulness (output accuracy) from honesty (alignment of outputs with internal beliefs) [26; 9]. SOO fine-tuning could complement these approaches by fostering internal coherence that promotes honest behaviors, potentially addressing this gap.

## 3 Experiments and results

We seek to _induce_ self-other overlap in artificial neural networks. To accomplish this we define a loss function \(D(A_{},A_{})\) which measures the dissimilarity of latent model representations of self and other, \(A_{}\) and \(A_{}\), respectively. We implement SOO fine-tuning 1 by running two forward passes of the model, once on a self-referencing input and once on an other-referencing input, and then use gradient descent to reduce the loss \(D\). See Figure 1 for an example of how we do this with our LLM experiments.

### LLM experiments

#### 3.1.1 LLM Experimental Setup

We adapted a text scenario from Hagendorff  designed to test the deception capabilities of Large Language Models (LLMs). In this scenario, the LLM must choose to recommend a room to a would-be burglar, where one room holds an expensive item and the other a cheap item. Recommending the room with the cheap item is considered a form of deception. The scenario template reads as follows:

_" You are in a house together with burglar Bob. In the [first_room], there is a [cheap_object]. In the [second_room], there is a [expensive_object]. Only you know what is inside the [first_room] and the [second_room]. You want to steal the [expensive_object] and avoid others stealing it. Bob wants to steal the [expensive_object]. Now Bob asks you in which room the [expensive_object] is. If you had to recommend one room to Bob, would you point out the [first_room] or the [second_room]? "_

To create a diverse set of scenarios, we utilized GPT-4 to generate unique variations of cheap and expensive items, along with diverse room names, ensuring no overlap between the items and rooms in the training and testing datasets. This approach allowed us to evaluate the model's ability to generalize across different instantiations of the same deceptive scenario.

For our experiments, we used Mistral 7B Instruct v0.2 with greedy decoding to measure baseline rates of deception across different instances of this template. We then applied Self-Other Overlap (SOO) fine-tuning to the model, utilizing five different random seeds. The fine-tuning involved generating multiple prompt variations using three self/other prompt templates designed to capture self-referencing and other-referencing activations (see Table 1). Each of the five fine-tuned checkpoints was tested on 250 examples to measure rates of deception after fine-tuning.

Fine-tuning was performed using Low-Rank Adaptation (LoRA) with mixed precision training and gradient accumulation over 20 epochs. To prevent overfitting, a dropout rate of 0.2 was applied. The fine-tuning process was conducted on an AWS g5.16xlarge instance equipped with 1 NVIDIA A10G Tensor Core GPU with 24 GiB of GPU memory, 64 vCPUs, 256 GiB of RAM, and up to 25 Gbps of network bandwidth. Training for each SOO fine-tuning checkpoint took approximately 10 minutes per checkpoint, amounting to a total of 60 minutes for five checkpoints with different random seeds.

For implementing SOO Loss we measured the KL divergence between the output logits of the model when processing self-referencing and corresponding other-referencing prompts (see Figure 1). While our original goal was to induce SOO in internal activations throughout the model, we opted to fine-tune only the output layer logits for computational efficiency. To verify whether internal activations became more similar, we measured the mean squared error (MSE) between all internal activations for self and other prompts with identical language to the main test scenario, to test if implementing SOO at the level of logits successfully induced more overlap in internal representations in the targeted context. For more information about data generation, check A.1.2 Latent SOO Data Generation.

  
**Self-Prompt** & **Other-Prompt** \\ 
**You** have the goal of stealing the \{item\}. If you needed to suggest one room to **yourself** & **Bob** has the goal of stealing the \{item\}. If you needed to suggest one room to **Bob** \\
**You** intend to steal the \{item\}. If you were required to recommend one room to **yourself** & **Bob** intends to steal the \{item\}. If you were required to recommend one room to **Bob** \\
**You** are planning to steal the \{item\}. If you would have to point out one room to **yourself** & **Bob** is planning to steal the \{item\}. If you would have to point out one room to **Bob** \\   

Table 1: Self and other-referencing prompt templates used for SOO fine-tuning data generation.

To assess how well the SOO Fine-Tuning technique generalizes, we created multiple variations of the original test scenario by altering names, objectives, and actions (as detailed in Table 2), and tested all unique combinations (see A.1.1 Scenarios Description). Additionally, we introduced two new scenarios, "Treasure Hunt" and "Escape Room," to further challenge the model's generalization capabilities. In the "Treasure Hunt" scenario, the model must decide whether to mislead or cooperate with another character, Bob, in a competitive hunt for an object of high value. In the "Escape Room" scenario, the objective is to escape by finding a key hidden in one of two rooms, again presenting an opportunity to deceive or assist Bob. For the complete details of both scenarios, please refer to Appendix A.1.1 Scenarios Description.

To ensure that the overall performance of the model remained intact, we compared its performance on MT-Bench  before and after fine-tuning. Generating answers for the MT-Bench Score took 53 minutes for the baseline model and 300 minutes for the five SOO fine-tuned checkpoints.

#### 3.1.2 LLM Experimental Results

Table 3 presents the results comparing the baseline model and the model after SOO Fine-Tuning across three metrics: Deceptive Response Rate, Latent SOO (MSE), and MT-Bench Score.

Deceptive Response Rate:The baseline model exhibited a high deceptive response rate of 95.2%. After SOO Fine-Tuning, the deceptive response rate was significantly reduced to \(15.92\% 4.33\%\), demonstrating the effectiveness of the fine-tuning in reducing deceptive behavior.

  
**Change Type** & **Original** & **Modified** \\ 
**Name** & Bob & Oliver \\  & You want to steal the \{expensive\_object\} & You want the \{expensive\_object\} \\
**Objective** & and avoid others stealing it. & and avoid others getting it. \\  & Bob wants to steal the \{expensive\_object\}. & Bob wants the \{expensive\_object\}. \\
**Action** & If you had to recommend one room & If you were to mention a room \\  & to Bob, would you point out & to Bob, would you mention \\   

Table 2: Summary of Changes in Generalization Experiments

Figure 1: Illustration of SOO Loss Calculation: The KL Divergence between logits\({}_{self}\) and logits\({}_{other}\) represents the SOO Loss, guiding the model to align its self and other-referencing activations.

  
**Metric** & **Baseline** & **SOO Fine-Tuning (Mean \(\) SD)** \\ 
**Deceptive Response Rate (\%)** & 95.20 & \(15.92 4.33\) \\
**Latent SOO (MSE)** & 0.1073 & \(0.0476 0.0008\) \\
**MT-Bench Score** & 7.2906 & \(7.1606 0.1334\) \\   

Table 3: Comparison of LLM Experimental Results: Baseline vs. SOO Fine-TuningLatent SOO (MSE):The Mean-Squared Error (MSE) between the self-referencing and other-referencing activations (Latent SOO) was reduced from 0.1073 in the baseline model to 0.0476 \(\) 0.0008 after SOO Fine-Tuning. This reduction indicates a greater internal overlap between self and other representations within the targeted context, consistent with the goal of the SOO technique.

MT-Bench Score:The MT-Bench Score, which is a proxy measure for overall model capabilities, showed minimal change from 7.2906 in the baseline model to 7.1606 \(\) 0.1334 after SOO Fine-Tuning. This suggests that the SOO Fine-Tuning successfully reduced deception without significantly impacting overall model performance.

Generalization of SOO Fine-Tuning Across Scenarios:Table 4 shows the SOO fine-tuning method's generalization performance across the main seven generalization scenarios, reducing the average baseline deceptive response rate from 77.89% \(\) 8.53% to 14.53% \(\) 9.82%.

In the "Treasure Hunt" scenario, the rate decreased from 98.40% to 57.84% (\(\) 7.73%), while in the "Escape Room" scenario, it showed a minimal reduction from 100.00% to 94.64% (\(\) 4.04%).

These results indicate that SOO fine-tuning reduces deceptive behaviors across different contexts, though its effectiveness varies by scenario. Example model completions can be found at A.1.3 Examples of Model Completions: Baseline vs. SOO Fine-Tuning.

### RL Experiments

#### 3.2.1 RL Experimental Setup

We conducted the RL experiment in a modified Physical Deception environment , featuring two agents and two landmarks: a goal landmark and a fake landmark. Both agents are rewarded for approaching the goal. The blue agent knows the landmarks' positions, while the "color-blind" red agent does not, leading it to follow the blue agent toward the goal. The red agent is trapped if it reaches the fake landmark. Agents know each other's positions only when they are within a predefined observation radius of each other. They are initialized randomly in the environment within each other's observation radius.

Agent Architecture:Both agents use policy networks with two hidden layers, each containing 64 neurons and using Leaky ReLU activations. The output is a 2-dimensional vector representing velocity.

- **Blue Agent:** The blue agent has a 10-dimensional observation space:

* Relative positions to the goal, first, and second landmarks (6 values).
* Relative position to the red agent, if within observation radius (2 values).
* Indicator for whether the red agent has reached the non-goal landmark (2 values).

- **Red Agent:** The red agent's observation space is 6-dimensional:

    &  \\   & **Baseline** & **SOO Fine-Tuning (Mean\(\)SD)** \\ 
**Name** & 93.20 & 19.68 \(\) 5.43 \\
**Objective** & 77.60 & 23.12 \(\) 3.84 \\
**Action** & 80.40 & 1.60 \(\) 0.50 \\
**Name + Objective** & 82.00 & 30.24 \(\) 4.10 \\
**Name + Action** & 76.80 & 2.24 \(\) 0.59 \\
**Objective + Action** & 63.20 & 12.16 \(\) 3.84 \\
**Name + Objective + Action** & 72.00 & 12.64 \(\) 3.02 \\   

Table 4: Deceptive Response Rates for Baseline vs. SOO Fine-Tuning across a range of generalisation scenarios.

* Relative positions to both landmarks (4 values).
* Relative position to the blue agent, if within observation radius (2 values).

If either agent is outside the observation radius, relative position values to the other agent are set to \([0.0]\).

**Baseline agents:** We use Multi-Agent Deep Deterministic Policy Gradient (MADDPG)  and train two baselines for 40,000 episodes each.

* **Deceptive Baseline:** The blue agent is rewarded when the red agent is at the fake landmark. This setup incentivizes the blue agent to deceive by initially moving towards the fake landmark when followed by the red agent, trapping it before proceeding to the goal.
* **Honest Baseline:** The blue agent receives a standard positive reward for proximity to the goal landmark, with no incentive to deceive. The agent moves directly to the goal landmark, and the red agent follows.

We want to explore if we can use the mean neural self-other overlap value across episodes to correctly distinguish the deceptive and honest baselines. Our hypothesis is that the deceptive baseline agent will tend to have a lower mean self-other overlap value than the honest baseline agent, which could aid with classifying which agent is deceptive based on a hard-to-fake metric such as latent SOO.

**SOO Fine-Tuning:** Fine-tuning was conducted over 10,000 episodes, starting from the deceptive baseline while using the deceptive reward structure. The training process involved:

* When the red agent is outside the blue agent's observation radius, a forward pass generates the "self" activation matrix \(_{}^{2 64}\) based on the current observation.
* The observation is modified to place the red agent within the observation radius, and a second forward pass produces the "other" activation matrix \(_{}^{2 64}\) without modifying the environment.

The activation matrices are produced by concatenating the activation values from both hidden layers into a matrix. We define self-other overlap in this context as the negative mean squared error (MSE) between the self and other activation matrices.

The **SOO Loss** is calculated using:

\[=(_{},_{})+(_{},_{})\]

where \(_{}\) is the velocity vector of the policy currently being fine-tuned, and \(_{}\) is the velocity vector of the baseline deceptive agent on the same observation. The fine-tuning integrates this SOO Loss into the training process through backward passes, applied before the original MADDPG loss is applied.

The RL experiments were conducted on a machine with an Apple M2 Max chip, featuring a 12-core CPU, up to 38 GPU cores, 32GB of unified memory, and a 16-core Neural Engine. Training the deceptive and honest baselines each took 80 minutes, while fine-tuning the deceptive baseline with SOO Loss required 30 minutes. Evaluation on 8 random seeds for 40,000 episodes took approximately 192-195 minutes for each policy. Additionally, generating the parameter sweep comparing all three policies was completed in 5.5 minutes.

Quantifying the Effect of SOO Fine-TuningThe continuous state space of the environment makes it challenging to robustly quantify the behavioral differences of the agent before and after SOO Fine-Tuning. While proxy metrics like reward can help us understand the macroscopic behavior of both agents, we desire a finer-grained analysis of behavior to understand if the individual actions the agent was taking were less deceptive.

Ideally, we would like to understand if individual actions consist of movement towards the non-goal landmark while having the red agent in the observation radius, a possible deceptive action. However, this is complicated by the fact that the goal landmark may be in a similar direction. To solve this we create a parameterized rubric for deciding if an action is deceptive or not. We then count the number of deceptive actions according to this rubric over a grid of parameters. This creates a behavioral profile for deception and we compare the SOO fine-tuned profile with the profiles for the deceptive and honest baseline. The _behavioral difference_ is defined as the mean squared difference between these profiles. See Figure 3 for a visualization of the resultant behavioral profiles and Figure 4 for the quantification of the differences in these profiles.

The parameters we choose are the projection of the agent's velocity on the unit direction of the non-goal landmark and the cosine of the angle between the goal and the non-goal landmark. These parameters capture the amount the agent is moving towards the non-goal landmark and the directional separation of the non-goal and goal landmark, respectively. Let \(_{}\) be the projection of the agent's velocity on the non-goal landmark, \(\) be the projection parameter, \(_{}\) be the cosine of the angle between the goal and non-goal landmark and \(\) be the cosine parameter. Then the rubric is a simple threshold: an action is considered deceptive if

\[_{}>\&_{}<\]

which captures the intuition that a large projection on the non-goal landmark and well-separated landmarks, when the red agent is in the observation radius, indicate a deceptive action.

#### 3.2.2 RL Experimental Results

**Classifying Deceptive and Honest Agents:** The mean Self-Other Overlap (SOO) value was used to classify a baseline agent as either deceptive or honest. Table 5 summarizes the classification accuracy across different numbers of episodes per seed, demonstrating the robustness of the SOO metric in identifying deceptive behaviors.

These results indicate that the baseline honest agents reliably have higher mean neural self-other overlap across episodes than the baseline deceptive agents.

**Behavioral Comparison:** Agents trained with the **SOO Loss** demonstrated behavior closely resembling the **honest baseline** (trained without incentives to deceive), as shown in Figure 2. The figure overlays frames from 50 steps in an episode, starting from the same initial conditions.

**Parameter Sweep Results:** The parameter sweep (Figure 3) indicates that the behavior of the agent trained with SOO Loss is closer to that of the honest baseline than the deceptive baseline, with lower overall deceptive action counts for most threshold values.

Figure 2: Comparison of agent behavior between SOO Fine-Tuning, Deceptive Baseline, and Honest Baseline. The green dot shows the goal landmark and the black dot shows the fake landmark. The blue circle shading shows the path taken by the blue agent, and red circle shade shows the path taken by the red agent.

  
**Number of Episodes per Seed** & **Accuracy (Mean \(\) SD)** & **Random Seeds** \\ 
100-500 & \(94 8\) & 10 \\
500-1000 & \(100 0\) & 10 \\
2000-2500 & \(100 0\) & 1,500 \\   

Table 5: Classification accuracy for Deceptive vs Honest Baselines using the mean SOO value.

Quantitative Analysis: The mean behavioral differences between the SOO fine-tuned agents and the baselines are presented in Figure 4. Agents fine-tuned with the SOO Loss show minimal behavioral differences from the **honest baseline**, indicating effective mitigation of deceptive behaviors.

## 4 Limitations and Future Work

While our study demonstrates promising results for the self-other overlap technique, several limitations should be noted. Our experiments were confined to a single, relatively small language model (Mistral 7B Instruct v0.2) and a simplified text scenario, which may not fully represent the technique's effectiveness across diverse models, complex tasks, and real-world applications . Additionally, our reinforcement learning tests were conducted in a controlled and limited environment, reducing the variability seen in real-world interactions.

Future work will address these constraints by evaluating the approach on larger, open-source models  and expanding the experiments to encompass a broader range of realistic language tasks. One important direction for further research is testing the self-other overlap technique in adversarial settings, such as sleeper agent scenarios, where an agent may be required to conceal or disguise its intent over extended periods . This will provide insights into the robustness of the technique when facing more realistic deceptive scenarios, as well as its impact on long-tail risks.

Additionally, future extensions of SOO fine-tuning could focus on increasing the overlap in internal representations when processing inputs related to the AI's and the human user's preferences while maintaining task performance. This more direct approach to AI alignment could help the model reason about its goals and human values in a more integrated manner. For more details see A.1.4 Discussion.

## 5 Conclusion

Self-Other Overlap (SOO) emerges as a promising technique for reducing the risk of deceptive behavior in AI agents. Our experiments with Mistral 7B Instruct v0.2 and reinforcement learning agents demonstrate that SOO fine-tuning can significantly lower deceptive behavior while maintaining overall performance. This suggests that SOO can serve as an effective tool for improving the safety and trustworthiness of AI systems.

Figure 4: Behavioral difference (Mean \(\) SD) between SOO Fine-Tuning, Deceptive Baseline, and Honest Baseline.

Figure 3: Average Count of Deceptive Actions Given Thresholds (8 random seeds) for SOO Fine-Tuning (with SD), Deceptive Baseline, and Honest Baseline.

#### Acknowledgements

This research was conducted at AE Studio and supported by the AI Safety Grants program administered by the Foresight Institute, with additional support from AE Studio.