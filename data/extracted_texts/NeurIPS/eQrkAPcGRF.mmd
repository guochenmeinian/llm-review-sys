# Math2Sym: A System for Solving Elementary Problems via Large Language Models and Symbolic Solvers

Minh Phu Nguyen\({}^{1}\) Minh Phuong Pham\({}^{1}\) Tuan Minh Kha\({}^{1,2}\) Minh Man Ngo\({}^{1,2}\)

\({}^{1}\)VNUHCM-University of Science

\({}^{2}\)Maverick AI

nguvenminhphu200@gmail.com, pmphuong1704@gmail.com

kha.minh@finsavvy.vn, nmman@hcmus.edu.vn

###### Abstract

Traditional models for solving math word problems (MWPs) often struggle to capture both linguistic context and arithmetic reasoning. We propose Math2Sym, a novel approach integrating large language models (LLMs) with symbolic solvers. This method leverages LLMs' language comprehension and symbolic computation's precision to efficiently convert MWPs into solvable symbolic form. We introduce the EMSF dataset for training models to formalize math problems across various complexities. On our defined test set benchmark, fine-tuned models outperform GPT-3.5 by 17% in few-shot tasks and perform comparably to GPT-4-mini on elementary math problems.1

## 1 Introduction

Math word problems (MWPs) present a unique challenge in artificial intelligence, requiring the integration of linguistic comprehension and mathematical reasoning to solve questions based on contextual descriptions . The primary obstacles lie in understanding the problem's context and translating it into appropriate mathematical operations, particularly when mapping linguistic information to complex mathematical expressions .

MWP solving has evolved from early rule-based systems like STUDENT  to machine learning methods , improving accuracy but still facing challenges in complex domains. Recent advancements include Chain-of-Thought (CoT) prompting , which enhances reasoning by breaking down problems into structured steps, and Program-Aided Language models (PaL) , which generate Python code for external computation.

Our work advances the field with the following key contributions:

* We introduce a novel approach that fine-tunes models to generate symbolic forms of MWPs, enhancing language models' capabilities in converting problems into representations compatible with our custom SymPy-based solver .
* We present the EMSF dataset for converting elementary math word problems into symbolic form across five problem types, facilitating improved formalization of MWPs.
* We demonstrate that fine-tuning 7B-parameter models on EMSF outperforms larger models such as GPT-3.5 in MWP solving.

This approach enhances MWP solving accuracy and versatility, paving the way for more robust AI systems in mathematical problem-solving. Its transparent step-by-step reasoning also offers educational value, fostering a deeper understanding of problem-solving processes.

## 2 Related Work

### MWP Solvers

MWP solving has evolved from early rule-based systems like STUDENT , which relied on predefined schemas, to statistical machine learning models , improving the mapping from linguistic input to mathematical representations. Deep learning approaches, such as encoder-decoder architectures, further advanced the field . However, many models remain limited to basic arithmetic problems or linear equations, struggling with more complex tasks like systems of equations or inequalities.

### Integration of External Tools with Language Models

Recent research has focused on enhancing language models (LMs) by integrating external tools like calculators, search engines, and symbolic solvers to address limitations in precise calculations or accessing real-time information [10; 11]. Two main approaches for training LMs to use these tools have emerged: creating large, supervised datasets with explicit examples of tool usage and using few-shot learning with prompts demonstrating tool use [6; 12].

### Auto-Formalization

Auto-formalization, the task of converting natural language into symbolic representations, plays a central role in mathematical reasoning. Recent work in this area leverages symbolic manipulation tools like SymPy , alongside proof assistants such as Isabelle/HOL , to enable computational formal reasoning. Unlike , which uses BERT for simpler problems, and , which employs LLMs via prompting without any fine-tuning, our method targets more complex problem types with enhanced LLM-based approaches.

## 3 Math2Sym

Math2Sym integrates large language models (LLMs) with symbolic solvers to address math word problems (MWPs). By transforming natural language problem descriptions into symbolic representations, this approach tackles two key challenges: understanding linguistic complexity and ensuring precise computation. LLMs extract variables and conditions from word problems, while a symbolic solver handles mathematical computations.

### Method Framework

Math2Sym converts natural language word problems into standardized _Symbolic Forms_ through three core steps:

1. **Extraction of Variables:** Identify relevant entities (variables, constants, relationships) from the natural language description.
2. **Formulation of Mathematical Expressions:** Formalize extracted elements into precise mathematical expressions, adhering to the problem's logic and conditions.
3. **Conversion to Symbolic Form:** Transform mathematical expressions into a predefined _Symbolic Form_ compatible with a symbolic solver.

To illustrate this process, consider the following word problem and its symbolic formalization:

**Word Problem:** The length of a rectangle is equal to triple the width. Which system of equations can be used to find the dimensions of the rectangle if the perimeter is 86 centimeters?

**Answer:** Define the variables and formulate the linear system of equations: Let variable \(x\) represent the length of the rectangle and variable \(y\) represent the width of the rectangle. The length of a rectangle is equal to triple the width, so the equation is \(x=3y\). The perimeter of the rectangle is 86 centimeters, leading to the equation \(2x+2y=86\).

System of equations: \(\{x=3y,2x+2y=86\}\)

Symbolic Form: \([x-3y,2x+2y-86,x,y,]\)

This structured symbolic form provides the solver with a clear and unambiguous mathematical representation, ensuring consistent and accurate solutions across various problem types.

### Symbolic Solver

The problem-solving process involves the language model systematically normalizing problems into standard forms, which are then converted into predefined Symbolic Forms. To mitigate the frequent arithmetic errors produced by language models, our approach trains the model to formalize problems while avoiding direct calculations. Unlike other LLMs that attempt computations within their reasoning steps, we delegate all arithmetic to an external solver in Symbolic Form, allowing the model to focus on formalization.

Our solver is built using SymPy , a Python library for symbolic computation. SymPy's capability to handle a wide range of mathematical problems and its ease of use make it suitable for both current needs and future scalability. The Symbolic Form follows this structure:

Symbolic Form: [[constants or expressions, variables, actions]]

Each mathematical problem type is associated with a specific action, which corresponds directly to a SymPy method (e.g.,'solve' for equations or inequalities, 'igcd' for greatest common divisors).

To address LLMs' tendency to produce lengthy outputs, we enclose Symbolic Form answers in double square brackets. This formatting is achieved through prompting or fine-tuning with structured data, facilitating the consistent conversion of problems into Symbolic Forms.

## 4 Experiments

We developed a custom test dataset of 92 questions across five categories: greatest common divisor, least common multiple, systems of equations, linear inequalities, and compound inequalities. Questions vary in difficulty (levels 1-5) and include both word and purely mathematical problems to assess generalization. The questions in the dataset were inspired by problems found in high school mathematics textbooks and reputable online educational resources.

### Training Dataset and Models

Language models ranging from a few hundred million to approximately 7 billion parameters were fine-tuned on the EMSF dataset using Low-rank adaptation (LoRA) . Detailed parameters are in A.

The EMSF dataset consists of three parts, as detailed in B:

* Pretrain: Focuses on standard mathematical formalization.
* Basic: Synthetically generated using Mixtral 8x7B , involves direct extraction of simple problem elements.
* Advanced: Generated using LLaMA3 70B , requires reasoning steps for complex problems.

In short, the basic dataset involves direct extraction and declaration of simple problem elements, whereas the advanced dataset requires reasoning steps and aggregation of information from more complex problems.

### Answer Evaluation

Performance was evaluated through direct evaluation, few-shot learning, and two fine-tuned settings (on basic and on advanced datasets). Model-generated symbolic forms were compared to ground truths, both processed through the symbolic solver.

Evaluation scores were weighted by problem's difficulty, with the total score \(S_{}\) calculated as:

\[S_{}=_{i=1}^{N}C_{i} D_{i}\]

where \(C_{i}\{0,1\}\) is the correctness score for problem \(i\), and \(D_{i}\{1,2,3,4,5\}\) is based on problem's difficulty.

## 5 Results

**Enhanced Performance with Solver Integration**: Our experiments demonstrate that integrating symbolic solvers with LLMs significantly improved performance across models like Mistral 7B, Qwen 7B, WizardMath 7B, and GPT-3.5. This integration outperformed approaches such as Program-aided Language models (PaL) and Zero-shot Chain-of-Thought (CoT) prompting. For instance, Mistral 7B showed a 37% improvement with solver integration compared to Few-Shot PaL, while Qwen 7B demonstrated a 10% increase in performance. These findings underscore the efficacy of the Math2Sym framework, which leverages symbolic solvers to enhance the natural language comprehension and reasoning capabilities of LLMs.

**Comparison with Qwen2-Math**: Our approach outperformed Qwen2-Math-Instruct 7B  in overall performance across problem complexities. While Qwen2-Math-Instruct excelled in high-difficulty problems (scoring 170 and solving 5/6 of the most difficult problems), it showed inconsistencies on simpler tasks. In contrast, our models, particularly WizardMath 7B, maintained consistent performance across all difficulty levels, achieving a total score of 217. This consistency demonstrates Math2Sym's versatility in handling both simple and complex tasks.

**Dataset-Driven Success in High-Difficulty Problem Solving** : Models fine-tuned on our EMSF dataset excelled in high-difficulty problems (levels 4 and 5). WizardMath 7B achieved a score of 217, significantly outperforming Zero-shot CoT (133) and GPT-3.5-turbo's best in-context learning (179). This success stems from our dataset's ability to teach diverse problem-to-symbolic-form mapping,

    & **Zero-shot-** & **Few Shot** & **Few Shot +** & **Fine-tuned** & **Fine-tuned** \\  & **CoT** & **PaL** & **Solver** & **basic** & **advanced** \\  Mistral 7B & 69 & 113 & 155 & 183 & **210** \\ Mistral 8x7B & 135 & 145 & 154 & Nan & Nan \\ Orca 7B & 25 & 78 & 76 & 133 & **171** \\ Qwen 7B & 116 & 126 & 139 & 170 & **207** \\ WizardMath 7B & 133 & 135 & 140 & 183 & **217** \\ Llama3.1 8B & 156 & 140 & 168 & 197 & **211** \\ Qwen 0.5B & 15 & 22 & 7 & **168** & 132 \\ Qwen 1.8B & 24 & 76 & 52 & **153** & 136 \\ Gemma 2B & 10 & 20 & 39 & 133 & **135** \\ GPT-neo 350M & 0 & 70 & 36 & **130** & 120 \\ Qwen2-Math- & 163 & **170** & 144 & Nan & Nan \\ Instruct 7B & & & & & \\ GPT 3.5 & 172 & 171 & **179** & Nan & Nan \\ gpt-4o mini & **217** & 182 & 182 & Nan & Nan \\    Max score: 231

Table 1: Score on our test dataset for the few-shot PaL and solver, both ran on 5-shot prompt with specific prompt for each type of problemenhancing solver utilization. Notably, our approach yielded results comparable to GPT-4o Mini, demonstrating its competitiveness in challenging problems.

**Performance of Smaller Models in Specific Contexts**: In specific contexts, models with fewer than 1 billion parameters occasionally outperformed mid-sized models. For instance, the fine-tuned Qwen 0.5B model scored approximately 10% higher than the Qwen 1.8B model, suggesting that smaller models may benefit from improved learning efficiency before encountering overfitting issues. However, while smaller models excelled in simpler tasks, larger models like WizardMath 7B consistently outperformed them on complex problems, highlighting the importance of model size in managing problem complexity (See 1).

**Influence of Dataset Complexity on Model Performance**: Our findings reveal that dataset complexity plays a pivotal role in determining model performance. Smaller models excelled on the Basic dataset, but struggled with the Advanced dataset. For instance, Qwen 0.5B's performance dropped by almost 21% when moving from Basic to Advanced tasks. Conversely, larger models like WizardMath 7B improved by about 19% on the Advanced dataset compared to the Basic one. These results highlight the importance of aligning dataset complexity with model capacity, especially for tasks requiring advanced reasoning skills (See 2).