# PACE: Marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization

Yao Ni\({}^{}\)   Shan Zhang\({}^{,}\)   Piotr Koniusz\({}^{*,@sectionsign,}\)

\({}^{}\)The Australian National University  \({}^{@sectionsign}\)Data6l\({}^{@sectionsign}\)CSIRO

\({}^{}\)Australian Institute for Machine Learning, The University of Adelaide

\({}^{}\)yao.ni@anu.edu.au  shan.zhang@adelaide.edu.au  \({}^{@sectionsign}\)piotr.koniusz@data61.csiro.au

The corresponding author.

###### Abstract

Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained transformers to downstream tasks. However, the optimization of tasks performance often comes at the cost of generalizability in fine-tuned models. To address this issue, we theoretically connect smaller weight gradient norms during training and larger datasets to the improvements in model generalization. Motivated by this connection, we propose reducing gradient norms for enhanced generalization and aligning fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Yet, naive alignment does not guarantee gradient reduction and can potentially cause gradient explosion, complicating efforts to manage gradients. To address such an issue, we propose PACE, marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization. We perturb features learned from the adapter with the multiplicative noise and ensure the fine-tuned model remains consistent for same sample under different perturbations. Theoretical analysis shows that PACE not only implicitly regularizes gradients for enhanced generalization, but also implicitly aligns the fine-tuned and pre-trained models to retain knowledge. Experimental evidence supports our theories. PACE surpasses existing PEFT methods in visual adaptation tasks (VTAB-1k, FGVC, few-shot learning, domain adaptation) showcasing its potential for resource-efficient fine-tuning. It also improves LoRA in text classification (GLUE) and mathematical reasoning (GSM-8K). The code is available at github.com/MaxwellYaoNi/PACE.

## 1 Introduction

Transformers , with the self-attention mechanism  capturing long-range dependencies in data, succeed in various deep learning tasks, including image classification (ViT ), multimodal learning (CLIP ), image synthesis (StableDiffusion ), semantic segmentation (SAM ) and text generation (LLaMA ). The success of transformers can be largely attributed to the availability of abundant data, such as ImageNet  and Laion5B , which empower researchers to scale up these models by training them under an enormous number of parameters.

Such huge models, with knowledge from large-scale pre-training , constitute on foundation models that can be easily adapted to various downstream tasks through full fine-tuning or linear probing , eliminating the need for task-specific model design . However, full fine-tuning is storage-intensive and infeasible for maintaining separate model weights as the number of tasks grows, while linear probing, which only trains the last head layer, yields inferior adaptation performance.

To overcome these limitations, Parameter-Efficient Fine-Tuning (PEFT)  fine-tunes only a small subset of parameters, thereby reducing storage requirements while surpassing the performance offull fine-tuning and linear probing. These advantages have popularized PEFT and inspired the development of various PEFT methods for deep learning tasks, which can be categorized into two groups: those increasing inference cost and cost-efficient ones. The first group introduces additional learning branches, such as non-linear adapters [25; 8], or concatenates learnable parameters with input tokens, _e.g._, visual prompts [28; 82; 52], increasing inference cost. The second group, focuses on cost-efficiency by lower-rank adaptation in linear layers [7; 26], or affine transformations such as SSF  and RepAdapters , which can be reparameterized during inference for efficiency.

Despite the superiority and efficiency of PEFT, prioritizing optimization for downstream tasks compromises the generalizability of fine-tuned models, yielding suboptimal performance. Although some analyses have been conducted on PEFT [63; 27; 18; 72; 39], they fail to fully explain the generalization of PEFT, leading to ineffective strategies for improving generalization.

To address this gap in understanding generalization in PEFT, we establish a theoretical connection from generalization theory: smaller weight gradient norms and larger data volumes contribute to better generalization. Motivated by this, we propose reducing weight gradient norms and aligning output space of the fine-tuned model with the pre-trained one to retain knowledge captured from large pre-training data. Yet, theoretical analyses reveal this naive alignment does not guarantee gradient regularization and can even cause gradient explosion, complicating efforts for gradient management. To address this issue, we propose perturbing features learned from the adapter with multiplicative noise and constraining the network output to be consistent across different perturbations.

Our method, called PACE, marries generalization of PArameter-efficient fine-tuning with Consistency rEgalization. Its name, PACE, reflects our goal of keeping the output behavior of the fine-tuned model _in pace with_ the pre-trained one. Despite its simplicity, theoretical analysis confirms that PACE not only implicitly regularizes weight gradients for better generalization but also implicitly aligns the fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Experimental evidence supports our theories. PACE improves existing PEFT methods, achieving superior results across six adaptation benchmarks. Our key contributions are:

1. We establish a theory connecting smaller weight gradient norms and larger datasets with enhanced generalization, motivating gradient reduction and model alignment for fine-tuning.
2. We propose PACE, a simple yet effective method perturbing features from adapters with multiplicative noise and constraining output of fine-tuned model to be consistent across perturbations.
3. Our theoretical and empirical evidence confirms that PACE implicitly regularizes gradients and aligns the fine-tuned model with the pre-trained one. PACE excels on 4 visual adaptation tasks.
4. We provide novel theoretical explanations of how gradient penalization and consistency regularization benefit generalization, offering fundamental insights applicable across deep learning.

## 2 Related work

**Parameter-Efficient Fine-Tuning (PEFT).** LoRA  uses low-rank decomposition to reduce parameters and treats adapters as side paths. SSF  proposes affine transformations on latent features. FacT  decomposes and reassembles parameter matrices in ViT. Surgical fine-tuning  of different network parts improves adaptation to distribution shifts. FLoRA  performs a batched low-rank adaptation. GLoRA  unifies cost-efficient PEFT methods. NOAH  uses parameter search on neural prompts. ARC  leverages cross-layer ViT similarity, parameter-sharing adapter and scaling factors for lower fine-tuning cost. RLRR  incorporates a residual term for flexibility while preserving pre-trained representation. RepAdapter  reparameterizes adapters for efficient inference. Res-tuning  unbinds tuners from the backbone for memory efficiency. Zhao _et al._ show impressive fine-tuning results by tuning layernorm in attention. OFT  and BOFT  propose orthogonal fine-tuning to preserve hypersphere energy between neurons.

**Consistency Regularization.** Fixmatch  applies consistency regularization over augmented images for semi-supervised learning. Openmatch  utilizes it on outlier predictions for open-set semi-supervised learning. R-Drop  applies it to transformers  with dropout for NLP tasks. CR  applies it over augmented real and fake images for GAN training. CAGAN  enforces consistency on discriminators with dropout for GAN training. Despite the empirical success of consistency regularization demonstrated by previous works, theoretical analysis is lacking. While NICE  demonstrates that consistency regularization lowers latent feature gradients for stable GAN training, it fails to reveal reduced weight gradient for enhanced generalization. Our study goes beyond prior works by providing a theoretical link between smaller weight gradients and improved generalization, effectively marrying generalization of PEFT with consistency regularization.

**Generalization of Fine-Tuning.** Li _et al_.  constrain the fine-tuned model's closeness to the pre-trained model in weight space. Fu _et al_.  induce sparsity on PEFT for better generalization. Wang _et al_.  studies generalization of PEFT fine-tuning graph neural network. Zhang _et al_.  employ rank-1 gradient boosting (GB) updates supported by the GB theoretical framework. VioLET , PromptSRC  and CoPrompt  naively align the fine-tuned model with the pre-trained one for enhanced generalization or avoiding forgetting. Additionally, L2SP , DELTA , and FTP  aim to retain pre-trained knowledge by aligning fine-tuned models with pre-trained ones, reducing distance in weight space, feature space and using projected gradient descent, respectively. However, they fail to provide a theoretical analysis for this alignment. Our study goes beyond understanding generalization of PEFT by discovering the benefits of gradient regularization and model alignment. We propose PACE to match both requirements, paving a comprehensive understanding for PEFT.

**Gradient regularization.** Previous studies have empirically shown that gradient regularization improves performance [67; 85; 48; 49] and adversarially robust accuracy . However, they lack theoretical connection between smaller gradient norms and better generalization [17; 81; 6]. We bridge this gap by establishing a fundamental theory between reduced gradient norms and improved generalization, providing a solid foundation for future research on enhancing generalization.

## 3 Approach

We begin with a unified perspective on cost-efficient PEFT based on GLoRA , linking generalization with gradients and large-scale data, and motivating the alignment of the fine-tuned model with the pre-trained model to leverage its knowledge. We identify limitations of naive alignment in gradient regularization and introduce PACE, which implicitly enhances gradient regularization and model alignment. We conclude with theoretical justification and efficient implementations.

### A unified perspective on cost-efficient PEFT methods

The transformer architectures [68; 16] have excelled in natural language processing and computer vision tasks through their powerful sequential modeling capabilities. This success stems from their ability to process text/image tokens through \(L\) transformer blocks, where each block contains self-attention and MLP modules primarily composed of linear layers. These linear layers enable the self-attention mechanism to capture long-range dependencies, allowing transformers to achieve superior performance when scaled to a huge number of parameters and trained on extensive datasets.

With massive parameters, pre-trained on large-scale data, transformers serve as foundation models that can be fine-tuned for downstream tasks using limited data. However, fully fine-tuning all parameters for various downstream tasks requires substantial memory and can lead the forgetting of pre-trained knowledge. To alleviate this without increasing inference cost, adapters with lightweight parameters are often preferred for fine-tuning. Let \(_{0}()\) be a transformation within the pre-trained transformer. Current adapters can be unified as introducing a residual branch \(\) to form a new transformation \(\):

\[()=_{0}()+().\] (1)

Here, \(\) is the input and \(_{0}()\) can represent MLP modules, as in Adapter  and AdaptFormer , or linear layers in self-attention and MLP modules, as in [26; 7; 12; 34]. In SSF , \(_{0}()\) is the identity mapping and \((a)=(-)+\) with \(\) and \(\) as affine transformation parameters.

Given that linear layers are key components in transformer, tuning them offers a flexible and effective way to adapt models to downstream tasks. This work focuses on methods that tune the linear layer without increasing inference cost. Let \((_{0},_{0})\), \((,)\), and \((,)\) be the parameters of pre-trained model, adapter and fine-tuned model, respectively, where \(_{0},,^{d_{} d_{}}\) and \(_{0},,^{d_{}}\). Fine-tuning a linear layer in self-attention or MLP module can be formed as:

\[h() =+=(_{0}+)+(_{0} +)\] \[=h_{0}()+ h()=(_{0}+_{0})+( +).\] (2)

Based on GLoRA , cost-efficient PEFT methods for linear layers vary in the form of \(,\):

**LoRA\({}_{}\)**: \(=_{}_{},=_{}\) where \(_{}^{d_{} r},_{} ^{r d_{}}\), and \(r\) is the rank.

**LoRA\({}_{}\)**: \(\!=\!_{0}(_{}_{})\), \(\!=\!_{0}_{}\), including RepAdapter  via reparameterization.

**VPT\({}_{}\)**: \(\) is zero, \(=_{0}\), with learnable \(^{d_{} 1}\) as layer-wise visual prompt. We use VPT\({}_{}\) to differentiate from VPT , which concatenates \(\) with tokens, increasing inference cost.

### Generalization of deep neural networks

Having established a unified perspective on cost-efficient PEFT, we now motivate our method from a perspective on improving generalization of neural networks to enhance performance on unseen data. Consider a network \(f:=(g(x))\) with \(l\) layers, where \(g\) is feature extractor and \(\) is the classification head. Let \(:=\{(^{(i)},^{(i)})\}_{i=1}^{l}\) be the parameter set with dimension \(d\) and \(^{n}:=\{(_{i},_{i})\}_{i=1}^{n}\) be the training set of size \(n\) drawn _i.i.d._ from distribution \(\), which contains infinite data. The following lemma from  explains the relationship between the empirical and population loss.

**Lemma 1**: _(Theorem 1 from ) Let \(_{^{n}}()\) be the empirical loss function over \(f\) on training set \(^{n}\) and \(_{}()\) be the population loss. For any \(>0\), with high probability over \(^{n}\), we have_

\[_{}()_{\|\|_{2} }_{^{n}}(+)+R\|_{2}^{2}}{^{2}},,\] (3)

_where \(R:(_{+},_{+})_{+}\) is an increasing function (under conditions on \(_{}()\) and \(n\) as in SSB.5)._

Lemma 1 bounds the population loss by the empirical loss with perturbed weights, indicating that a minimal empirical loss increase from small weight perturbations implies low population loss.

By observing that the maximum of \(_{^{n}}\) is achieved at \(=_{}}{\|_{ }\|_{2}}\), where \(_{}\) is the gradient of \(_{^{n}}\) at \(\), and performing a Taylor expansion of \(_{^{n}}\) around \(\), we formulate the following theorem.

**Theorem 1**: _Denote \(_{}\) as the gradient and \(_{}^{}\) as the largest eigenvalue of the Hessian matrix \(_{}\) of \(_{^{n}}\) at \(\). For any \(>0\), with high probability over training set \(^{n}\), we have_

\[_{}()_{^{n}}()+\|_{}\|_{2}+}{2}_{ }^{}+R\|_{2}^{2}}{^{2}},.\] (4)

_Here, higher-order terms from the Taylor expansion are incorporated into \(R\|_{2}^{2}}{^{2}},\), which is related to weights norm and inversely related to the training data size \(n\)._

Theorem 1 (proof in SSB.1) outlines strategies for enhancing generalization. They involve regularizing weight norms and the largest Hessian eigenvalues, and crucially, increasing data size \(n\) and reducing the weight gradient norms (illustrated in Figure 1). However, excessive reduction should be avoided as it could impair network's representation capacity, yielding higher empirical and population loss.

### Motivation and limitation of aligning the fine-tuned model with the pre-trained model

Theorem 1 emphasizes that large-scale data and smaller gradient magnitudes are essential for better generalization in neural network training. Therefore, aligning the fine-tuned model with the pre-trained one is crucial, as it ensures retention of knowledge obtained from large-scale data, preserving generalization. PEFT methods, often outperforming full fine-tuning, achieve this alignment by limiting the number of trainable parameters, restricting the model's capacity to deviate from the pre-trained one. However, the training objective prioritizes downstream task performance, compromising alignment with pre-trained knowledge. While sparsity regularization  and weight decay on adapter weights help, they do not ensure alignment, as even small weight changes can lead to significant divergence in output space [75; 21; 17]. Therefore, we propose to achieve the alignment by reducing the FP-distance (output distance between fine-tuned and pre-trained models on training samples):

\[D^{}()=_{i=1}^{n} f(_{i};)-f(_{i};_{0})_{2}^{2},=_{0}+,\] (5)

where \(,_{0},^{d}\) are parameters for the fine-tuned model, pre-trained model and the adapter.

While reducing FP-distance keeps the fine-tuned model close to the pre-trained model, thus preserving its knowledge, it does not ensure reduced gradient magnitudes, leading to suboptimal generalization.

To understand the gradient-related limitations in this alignment, we assume \(\) is small enough for a Taylor expansion approximation. Following standard practices [17; 80; 2], we perform the expansion up to the second-order terms. Given the independence between elements in squared \(L_{2}\) distances (SSB.4) and to simplify our theories, we analyze a one-dimensional output for a single _i.i.d._ sample, which leads us to the following proposition.

**Proposition 1**: _Assuming \(\) is small, denote \(f()\) as the one-dimensional output for \(\), with \(\) and \(\) as its gradient and Hessian at \(\). FP-distance over \(\) can be decomposed as follows:_

\[[f()-f(_{0})]^{2} =[f()-f(-)]^{2} f()-[f()-^{T}+^{T}]^{2}\] \[[^{T}-^{T}]^{2}.\] (6)

Prop. 1 establishes the relationship between weight gradients, adapter weights, and FP-distance. However, it remains unclear if it regulates gradients. Our experiments show that minimizing FP-distance can sometimes increase gradient magnitude, complicating efforts for managing gradient.

### Consistency regularization

To achieve better generalization by both regularizing gradients and aligning the fine-tuned model with the pre-trined model, we propose a consistency regularization loss for \(f\), encouraging invariance of \(f\) to the same input under varying multiplicative noise perturbations on the adapter weights, as follows:

\[D^{}()=_{i=1}^{n}_{_{1}, _{2}}\|f(_{i};_{0}+_{1})-f (_{i};_{0}+_{2})\|_{2}^{2},\] (7)

where \(_{1},_{2}(,^{2})\) is the multiplicative noise applied on adapter weight. To understand the generalization benefits in this consistency regularization, we simplify the analysis by focusing on one-dimensional output for a single sample, resulting in the following theorem.

**Theorem 2**: _Using notations from Prop. 1, let \(f(_{0}+)\) be the one-dimensional output for \(\). Define \(_{j}\) as \(j\)-th element in \(\), \(_{j}\) as the \(j\)-th element in \(\) and \(H_{jk}\) as the \((j,k)\)-entry in \(\). With \(_{1},_{2}(,^{2})\), the consistency loss over \(\) can be approximated as:_

\[_{_{1},_{2}}[f(_{0}+_{1} )-f(_{0}+_{2})]^ {2}\] \[ 2^{2}\!_{j}_{j}^{2}_{j}^{2}+ ^{4}\!_{j,k}_{k}^{2}_{j}^{2}H_{jk}^{2} =2^{2}\|\|_{2}^{2}\!+\!^{4}\|( ^{T})\|_{F}^{2}.\] (8)

Theorem 2 (proof in SSB.2) shows that the consistency regularization essentially penalizes the first- and second-order gradients of \(f\) at \(\) (illustrated in Figure 1), with the regularization strength controlled by the noise variance \(^{2}\) and adaptively influenced by the magnitude of elements in adapter weight \(\). Thus, minimizing the consistency loss implicitly regularizes the gradients, improving generalization.

With the FP-distance in Prop. 1 and consistency loss in Theorem 2, we establish their relationship as:

**Theorem 3**: _With \(d\) as the dimension of \(\), Eq. 6 can be upper-bounded as:_

\[[^{T}-^{T} ]^{2} 2d\|\|_{2}^{2}+d^{2}\|( ^{T})\|_{F}^{2}.\] (9)

Figure 1: Thm. 1: A flatter minimum has smaller gradient and Hessian norms, yielding better generalization. Thm. 2: Large gradient norms indicate large differences among perturbations. PACE minimizes these differences, reducing gradient norms. Thm. 3: Minimizing all pairs of distances between \(f(_{0}+_{1})\) and \(f(_{0}+_{2})\) where \(_{1},_{2}\!\!(,^{2})\) also reduces FP-distance (between fine-tuned \(f(_{0}+)\) and pre-trained \(f(_{0})\)), especially when \(_{1}\!\!=\!\!\), \(_{2}\!=\!\) or vice versa.

Theorem 3 (proof in B.3) establishes the relationship between Eq. 6 and Eq. 8, showing Eq. 6 is upper-bounded by terms involving \(\|\|_{2}^{2}\) and \(\|(^{T})\|_{F}^{2}\) which appear in Eq. 8. Reducing these terms results in a decrease in Eq. 6. Thus minimizing the consistency loss implicitly aligns the fine-tuned and pre-trained models (illustrated in Figure 1), preserving pre-trained knowledge.

### Efficient implementation of PACE

Providing different weight perturbations for each input in a mini-batch increases memory and computational demands. To avoid this, we perturb feature outputs from the adapter \( h()\), effectively simulating perturbation that shares noise across each row in the weight matrix \(\). Our simple pipeline is shown in Figure 2. Consider \(^{B T d_{}}\) as a batch of data where \(B\) and \(T\) are the batch and token sizes. The calculation for the linear layer of the fine-tuned model, which utilizes pre-trained weights \(_{0},_{0}\) and adapter weights \(,\), processes an output size of \(d_{}\) as:

\[h_{0}() =_{0}+_{0}; h()= +,\] (10) \[h() =h_{0}()+ h().\] (11)

Operator \(\) is the element-wise multiplication after expanding the left matrix \(^{B d_{}}(,^{2 })\) into \(B T d_{}\) where tokens within the same example share the same noise. Motivated by , the \(\) decreases linearly as block depth increases. Let \(f_{1}()\) and \(f_{2}()\) be two networks share same weights but do not share the noise patterns. The loss function for PACE is:

\[^{}=_{i=1}^{n}(f_{1}(_{i}),_{i})+\|f_{1}(_{i})-f_{2}(_{i})\|_{2}^{2},\] (12)

where \(\) is the classification loss and \(\) is a hyperparameter controlling regularization strength. During inference, noise and regularization are ommitted, \(,\) are integrated with \(_{0},_{0}\) for efficiency:

\[=_{0}+;=_{0}+; h( )=+.\] (13)

**Efficient PACE variants.** In SSC, we present two variants that match the computational/memory costs of the baseline while achieving superior performance with substantially reduced resources.

## 4 Experiments

We combine LoRAmil and VPTadd to form a strong baseline LoRAmil+VPTadd, outperforming other combinations in most cases. We evaluate our method across four visual classification adaptation tasks: VTAB-1K , few-shot learning , FGVC  and domain adaptation . We demonstrate PACE improves LoRA on GLUE  for text classification and GSM-8K  for text generation.

**Datasets and evaluations. VTAB-1K** comprises 19 datasets organized into (i) Natural images, (ii) Specialized datasets (remote sensing, medical) and (iii) Structured datasets (scene structure) domains. Each dataset has 1K training examples. Following [78; 28], we use the provided 800-200 train split for hyperparameter selection, evaluate using the full training set and report average accuracy across three trails. **Few-shot learning** involves 5 fine-grained datasets: FGVC-Aircraft , Food101 , OxfordFlowers102 , OxfordPets  and StanfordCars . Following , we evaluate 1,

Figure 2: Our pipeline. Adapter \( h()\) and \(h_{0}()\) from pre-trained model form the linear layer \(h\) of Multi-Head Attention and MLP in fine-tuned model. We perturb \( h()\) with multiplicative noise and ensure the network remains consistent to same inputs under varying perturbations.

2, 4, 8 and 16 shots, train on the provided training set, tune hyperparameters using validation and report average test accuracy over three random seeds. **FGVC** includes 5 fine-grained datasets: CUB-200-2011 , NABirds , OxfordFlowers , StanfordDogs  and StanfordCars . We follow  to use validation set for hyperparameter and report test results. For **domain adaptation**, following , we train on ImageNet  with a 16-shot setting, use the validation split by  for hyperparameter selection and report the results on the official validation set and 4 out-of-domain datasets: ImageNet-Sketch , ImageNet-V2 , ImageNet-A  and ImageNet-R . We evaluate on GLUE  for **text classification** and GSM-8K  for **mathematical reasoning**.

**Pre-trained backbones**. We experiment with two vision transformers, Vision Transforms (ViT-B/16)  and Swin Transformer (Swin-B) . These two are pre-trained on ImageNet-21K . We test a ViT-B-Laion-IN12K model, pre-trained on Laino-2B  and fine-tuned on ImageNet-12K . We use RoBERT\({}_{}\) and Phi-3-mini-4k-instruct  for text classification and generation.

**Implementation details**. We follow  for image processing: \(224 224\) resizing for VTAB-1K; random flips and crops to \(224 224\) for FGVC and few-shot learning; stronger augmentation for domain adaptation task, following . We use the Adam optimizer  with cosine learning rate decay and linear warm-up (first 10 epochs). Models are fine-tuned for 300 epochs on VTAB-1K and 100 epochs on other vision adaptation tasks, with batch size 64. For text classification we follow . See SSG for mathematical reasoning details. All experiments used an NVIDIA H100 GPU.

**Baseline.** For each dataset, we identified the better method (LoRA\({}_{}\)+VPT\({}_{}\) or LoRA\({}_{}\)) and tuned the rank, learning rate, and weight decay to form a strong baseline. The detailed baseline settings for each task and the number of trainable parameters are provided in SSF, where LoRA\({}_{}\)+VPT\({}_{}\) generally outperformed other variants. Building on the strong LoRA\({}_{}\)+VPT\({}_{}\), we use the grid search for our \(\) and \(\), following strategies from previous studies . Beyond LoRA\({}_{}\)+VPT\({}_{}\), PACE also enhances PEFT methods such as AdaptFormer, GLoRA, COFT, and BOFT (SSD.4).

   &  &  &  &  \\     &  &  &  &  &  &  &  &  &  &  &  &  \\     &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  Full & 68.9 & 87.7 & 64.3 & 97.3 & 86.9 & 87.4 & 38.8 & 79.7 & 95.7 & 84.2 & 73.9 & 56.3 & 58.6 & 41.7 & 65.5 & 57.5 & 46.7 & 25.7 & 29.1 & 68.9 \\ Linear & 64.4 & 85.0 & 63.2 & 97.0 & 86.3 & 36.6 & 51.0 & 78.5 & 87.5 & 68.5 & 74.0 & 34.3 & 30.6 & 33.2 & 55.4 & 12.5 & 62.0 & 9.6 & 19.2 & 57.6 \\  VPT-Deep & 78.8 & 80.8 & 65.8 & 98.0 & 88.3 & 78.1 & 49.6 & 81.8 & 96.1 & 83.4 & 68.4 & 68.5 & 60.0 & 46.5 & 72.8 & 73.6 & 47.9 & 32.9 & 37.8 & 72.0 \\ AdaptFormer & 69.2 & 90.1 & 68.0 & 98.8 & 89.9 & 82.8 & 54.3 & 84.0 & 94.9 & 81.7 & 95.8 & 80.9 & 65.3 & 43.6 & 78.3 & 74.8 & 48.5 & 29.9 & 41.6 & 73.9 \\ AdaptFormer & 70.8 & 91.2 & 70.5 & 99.1 & 90.9 & 86.6 & 54.8 & 83.0 & 95.8 & 84.4 & 76.3 & 81.9 & 64.3 & 49.3 & 80.3 & 76.3 & 45.7 & 31.7 & 41.1 & 74.7 \\ LoRA & 67.1 & 91.4 & 69.4 & 98.8 & 90.4 & 85.3 & 54.0 & 84.9 & 95.3 & 84.4 & 73.6 & 82.9 & 69.2 & 49.8 & 78.5 & 75.7 & 47.1 & 31.0 & 44.0 & 74.5 \\ NOAH & 69.6 & 92.7 & 70.2 & 99.1 & 90.4 & 86.1 & 53.7 & 84.4 & 85.3 & 79.8 & 82.8 & 68.9 & 49.9 & 81.7 & 81.8 & 84.3 & 32.8 & 44.2 & 74.2 \\ RepAdapter & 69.0 & 92.6 & **75.1** & 99.4 & 91.8 & 90.2 & 52.9 & 87.4 & 95.9 & 87.5 & 75.9 & 62.3 & 53.3 & 80.6 & 77.3 & 54.9 & 29.5 & 37.9 & 76.1 \\ RLRR & 75.6 & 92.4 & 72.9 & 99.3 & 91.5 & 89.7 & 87.0 & 86.8 & 92.5 & 85.3 & 75.9 & 79.7 & 64.2 & 53.9 & 82.1 & 83.9 & 53.7 & 33.4 & 43.6 & 76.7 \\ GLoRA & 76.4 & 92.9 & 74.6 & **99.6** & **92.5** & 91.5 & 57.8 & 87.3 & **96.8** & 88.0 & 76.0 & 83.1 & 67.3 & 54.5 & **86.2** & 83.8 & 52.9 & 37.0 & 41.4 & 78.0 \\  Baseline & 74.9 & 93.3 & 72.0 & 99.4 & 91.0 & 91.5 & 54.8 & 85.2 & 95.7 & 86.9 & 74.2 & 83.0 & 70.5 & 51.9 & 81.4 & 77.9 & 51.7 & 33.6 & 44.4 & 76.4 \\ 
**PACC** & **79.0** & **94.2** & 73.6 & **99.4** & **92.4** & **93.7** & **58.0** & **87.4** & **96.4** & **89.3** & **77.1** & **84.9** & **79.0** & **54.9** & **84.3** & **84.7** & **57.3** & **39.3** & **44.8** & **79.0** \\  

Table 1: Results on VTAB-1K with ViT-B/16. Mean Acc. is the average of group mean values.

   &  &  &  \\    _{}\), VPT\({}_{}\), LoRA\({}_{}\)+VPT\({}_{}\), with LoRA\({}_{}\)+VPT\({}_{}\) +PACE performing best in most cases. PACE yields notable improvement, especially when the number of shot is small.

**Results on FGVC.** Table 3 shows that PACE improves the strong LoRA\({}_{}\)+VPT\({}_{}\) by 0.7%, outperforming SSF , ARC  and RLRR  that use strongly pre-trained ViT with augmentations. In SSD.2, PACE achieves larger improvements on smaller datasets.

**Results on domain adaptation.** Table 4 compares PACE with others. LoRA\({}_{}\)+VPT\({}_{}\) outperforms GLoRA  which relies on parameter search. Meanwhile, PACE improves LoRA\({}_{}\)+VPT\({}_{}\) by 1.5%, outperforming other PEFT methods, demonstrating superior performance on domain adaptation.

**Results on text classification and mathematical reasoning.** Table 5 shows that PACE outperforms LoRA by 1% on GLUE text classification and by 3.11% on GSM-8K mathematical reasoning.

**Generalization on other backbones.** We evaluate PACE on CIFAR-100 (VTAB-1K) and domain adaptation using Swin-B  pre-trained on ImageNet-21K and ViT-B (pre-trained on Laion 2B, then fine-tuned on ImageNet-12K). Table 7 shows PACE outperforms LoRA\({}_{}\)+VPT\({}_{}\) and other PEFT methods across all backbones, demonstrating its strong generalizability. Further experiments in SSD.3 show PACE works effectively with self-supervised models such as MAE  and DINO .

  Method & Accuracy \\   &  &  &  &  &  \\  & -100 & Src. -S -V -A -R & -100 & Src. -S -V -A -R & -100 & Src. -S -V -A -R \\  Full & 51.6 & 63.9 & 18.5 & 52.5 & 3.2 & 21.2 & 51.2 \\ Linear & 63.4 & 67.9 & 14.4 & 60.8 & 9.4 & 25.6 & 61.9 \\ LoRA\({}_{}\) & 71.2 & 73.8 & 27.1 & 64.8 & 13.6 & 25.0 & 71.3 \\ VPT\({}_{}\) & 73.6 & 74.3 & 27.1 & 65.9 & 11.5 & 26.7 & 71.8 \\ LoRA\({}_{}\) & 74.3 & 78.1 & 31.2 & 68.3 & 13.4 & 23.7 & 73.2 & 76.6 \\ LoRA\({}_{}\) & 74.3 & 76.1 & 37.2 & 68.7 & 40.4 & 68.7 & 22.4 & 38.4 \\ LoRA\({}_{}\) & 74.3 & 78.1 & 31.2 & 68.3 & 13.4 & 23.7 & 73.2 & 78.6 \\ LoRA\({}_{}\)+VPT\({}_{}\) & 70.3 & 76.8 & 28.7 & 66.6 & 13.7 & 29.9 & 71.8 \\  LoRA\({}_{}\)+VPT\({}_{}\) & 74.9 & 77.8 & 30.6 & 68.1 & 14.1 & 32.5 & 73.8 & 73.4 \\  \({}_{}\)+VPT\({}_{}\) & **79.0** & **79.3** & **81.8** & **69.4** & **16.3** & **35.2** & **78.0** \\  

Table 4: Results on domain adaptation with ViT-B/16 pre-trained on ImageNet-21K.

  Method & CUB NA- & Oxford Stan. & Stan. & Mean \\ -2011 & Birds Flowers & Dogs & Cars & Acc. \\  Full & 87.3 & 82.7 & 98.8 & 89.4 & 84.5 & 85.9 \\ Linear & 85.3 & 75.9 & 97.9 & 86.2 & 51.3 & 79.3 \\ VPT & 88.5 & 84.2 & 99.0 & 90.2 & 83.6 & 89.1 \\ LoRA & 88.3 & 85.6 & 99.2 & 91.0 & 83.2 & 89.5 \\ SSF* & 89.5 & 85.7 & 99.6 & 89.6 & 89.2 & 90.7 \\ ARC* & 89.3 & 85.7 & **99.7** & 89.1 & **89.5** & 90.7 \\ RLRR* & 89.8 & 85.3 & 99.6 & 90.0 & 90.4 & 91.0 \\  LoRA\({}_{}\)+VPT\({}_{}\) & 88.9 & 87.1 & 99.4 & 91.2 & 87.5 & 90.8 \\
**APACE** & **89.8** & **87.3** & **99.5** & **92.2** & **88.8** & **91.5** \\   
  Method & Accuracy \\   & 62.01 \\  & 62.6 & 90.3 & 88.4 \\  & 65.6 & 90.7 & 89.5 & 78.7 & 91.8 & 94.6 & 85.2 \\  LoRA & 63.4 & 91.5 & 89.7 & 86.6 & 93.3 & 95.1 & 86.6 \\  \({}_{}\)** & **66.2** & **92.0** & 91.4 & **86.9** & **93.6** & **95.6** & **87.6** \\  

Table 5: Results for GLUE w/ RoBERT\({}_{}\). Matthew’s correlation for COLA, Pearson correlation for STSB, and accuracy for others.

  Method & COLA & STSB & MRPC & RTE & QNLI & SST2 & Avg. \\  Full & 63.6 & 91.2 & 90.2 & 78.7 & 92.8 & 94.8 & 85.2 \\ BitFit & 62.0 & 90.8 & **92.7** & 81.5 & 91.8 & 93.7 & 85.4 \\ Adapt & 62.6 & 90.3 & 88.4 & 75.9 & 93.0 & 94.7 & 84.2 \\ VeRA & 65.6 & 90.7 & 89.5 & 78.7 & 91.8 & 94.6 & 85.2 \\  LoRA & 63.4 & 91.5 & 89.7 & 86.6 & 93.3 & 95.1 & 86.6 \\  \({}_{}\)** & **66.2** & **92.0** & 91.4 & **86.9** & **93.6** & **95.6** & **87.6** \\  

Table 7: Classification results on domain adaptation and CIFAR-100 in VTAB-1K based different pre-trained models. Src. is short for ‘source’ in Table 4.

### Analyses

To verify our theories, we conduct experiments on CIFAR-100 (VTAB-1K) using ViT-B/16 and Camelyon (VTAB-1K) on Swin-B. Figures 3 & 4 show the gradient norm (summed across all layers) and FP-distance (Eq. 5) and the train & validation accuracy during training for baseline LoRAmul+VPTadd and PACE on validation set. Figures 2(a) & 3(a) show that PACE has a smaller gradient norm than baseline, verifying Theorem 2 that PACE can implicitly lower the weight gradient norm for better generalization. Figures 2(b) & 3(b) demonstrate that PACE maintains a lower FP-distance than the baseline, verifying Theorem 3 that PACE can implicitly align the fine-tuned model with pre-trained model, retaining knowledge from large-scale pre-training. Owing to the advantages of the gradient regularization and model alignment, PACE shortens the performance gap between seen and unseen data, yielding higher accuracy on the unseen validation set, as shown in Figures 2(c) & 3(c).

To clarify why naive alignment is problematic, we vary the regularization strength \(\) over a wide range (1e-3 to 5e4) for both Fine-tuned Pre-trained model Alignment (FPA) by minimizing \(D^{}\) in Eq. 5 and PACE. Figure 5 shows the averaged gradient norm over training (see also Figures 8 & 9 for more visualizations). PACE robustly lowers gradient norms with larger \(\), while FPA exhibits unpredictable behavior, even causing gradient explosion. This verifies Prop. 1 that minimizing \(D^{}\) is problematic for gradient regularization, complicating gradient management.

### Ablation studies

We ablate PACE based on the baseline LoRAmul+VPTadd on CIFAR-100 (VTAB-1K) and ImageNet-1K in domain adaption as shown in Table 8. The ablations include Noise (baseline w/ noise perturbing

Figure 4: Analysis for PACE. (a) gradient norm, (b) FP-Distance and (c) train & val. accuracy are evaluated on the validation set of Camelyon (VTAB-1K) with baseline LoRAmul+VPTadd on Swin-B.

Figure 5: Gradient norms of models across wide range of regularization strengths \(\) on CIFAR-100 (VTAB-1K) w/ ViT-B/16. Line and shadow represent mean and std across training epochs.

Figure 3: Analysis for PACE. (a) gradient norm, (b) FP-Distance and (c) train & val. accuracy are evaluated on validation set of CIFAR-100 (VTAB-1K) with baseline LoRAmul+VPTadd on ViT-B/16.

adapter), \(_{}\) (replacing the multiplicative noise with the additive noise), \(_{h}\) (perturbing \(h()\) instead of \( h()\) in Eq. 11), \(_{}\) (replacing the Gaussian noise with the dropout noise), \(_{=}\) (all transformer blocks share the same \(\)), \(_{}\) (\(\) increases linearly with depth), FPA (fine-tuned and pre-trained alignment by minimizing Eq. 5), SAM (sharpness-aware minimization ), GP (gradient penalization), \(_{1}\) (sparsity regularization), and transfer learning methods L2SP , DELTA  and FTP . We grid-search hyperparameters and report the best results.

Table 8 presents the results for all variants. PACE improves over Noise, which itself is better than baseline, justifying our adapter perturbation and consistency regularization. \(_{}\) performs worse than PACE, showing the superiority of the multiplicative noise. Although \(_{h}\) can implicitly regularize gradients, it performs worse than PACE, verifying the advantages of perturbing adapter to implicitly align models. \(_{}\) is worse than PACE, indicating the dropout noise is suboptimal. \(_{=}\) and \(_{}\) perform worse, justifying our design of linearly decreasing \(\). FPA, SAM and GP, which either only align models or only regularize gradients, are outperformed by PACE. Despite combining FPA+GP, it still performs worse than ours, suggesting ineffective combination. \(_{1}\), L2SP, DELTA, and FTP obtain worse results than PACE, showing their limitations in improving generalization. PACE regularizes gradients for better generalization and aligns models to retain knowledge, surpassing all other variants.

We further evaluate applying PACE across multiple \(M\) networks during training or applying it lazily with half-batch size at every \(N\) steps (\(_{}^{}\) in SSC). Figure 6 presents the results, showing that applying PACE among two networks at every training step performs best. However, lazy regularization applied every few steps can still provide reasonable results while saving computational/memory costs.

We test the sensitivity of hyperparameters \(\) and \(\) introduced in our PACE on OxfordPets for few-shot learning across 1, 2, 4, 8 shots. The results presented in Figure 7 demonstrate that with less data, larger \(\) and \(\) are favored, verifying the effectiveness of PACE in improving generalization.

## 5 Conclusions

We have introduced PACE, a novel and effective method that combines generalization of PArameter-efficient fine-tuning with Consistency rEolarization. Through rigorous theoretical analyses, we have shown PACE reduces weight gradient for improved generalization and it aligns the fine-tuned model with the pre-trained model for retaining pre-training knowledge. Our experimental results support the theoretical analyses, justifying the generalization advantages of PACE over other PEFT methods. With its dual advantages, PACE consistently outperforms other variants across different backbones, firmly establishing PACE as a powerful solution for enhancing generalization for PEFT methods. Limitations and border impacts are discussed in SSA.

**Acknowledgments.** We thank Moyang Liu, Melody Ip, Chenyi Du, and Yinuo Xu for their valuable discussions and support. PK is funded by CSIRO's Science Digital.