# HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning

Chunlin Tian

University of Macau

yc27402@um.edu.mo

&Zhan Shi

University of Texas at Austin

zshi17@cs.utexas.edu

&Zhijiang Guo

University of Cambridge

zg283@cam.ac.uk

Li Li

University of Macau

llili@um.edu.mo

&Chengzhong Xu

University of Macau

czxu@um.edu.mo

###### Abstract

Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed _HydraLoRA_, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that _HydraLoRA_ outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases. Code is available.

## 1 Introduction

Large Language Models (LLMs; [10; 3; 36; 47; 48; 32; 33]) are notably powerful, yet their training involves substantial expense. Adapting a single LLM for multiple downstream applications via fine-tuning has emerged as a prevalent method to cater to specific domain needs, balancing performance with practicality. This approach, however, faces a significant challenge due to the extensive memory and computational resources required for full fine-tuning (FFT), i.e., fine-tuning all billions of parameters. A solution to this has been the development of more selective adaptation techniques, involving modifying only a portion of the parameters or integrating external modules designed for new tasks. Key methodologies in this sphere include LoRA , Adaptors [37; 17; 31], and many other variants [25; 24; 9; 14; 53], all part of what can be generally termed as Parameter-Efficient Fine-tuning (PEFT). PEFT strategies are characterized by freezing the backbone model parameters while only a minimal number of task-specific parameters are introduced and fine-tuned. This method substantially boosts efficiency in the phases of fine-tuning and subsequent deployment, marking a significant advancement in the practical use of LLMs.

While fine-tuning a small subset of parameters offers a streamlined approach for domain adaptation, it's well-recognized that model performance is closely tied to the number of parameters involved . This intrinsic characteristic of methods like LoRA often results in them falling short of the FFT baseline, which updates all parameters, thereby creating a trade-off between efficiency and model quality. This issue of compromised quality in a low-parameter setting becomes even more pronounced in target domains characterized by complex sub-domains and diverse tasks. This situation presents a compelling research question:_What is the optimal architecture that can deliver superior model performance while still capitalizing on the efficiency benefits of a reduced parameter footprint?_

In our research, we carry out a series of exploratory experiments, applying LoRA to the LLaMA2  model to adapt it to a new domain encompassing multiple downstream tasks. As shown in Figure 1(a), LoRA adds trainable pairs of rank decomposition matrices A and B in addition to existing weight matrices. Our in-depth analysis of LoRA's mechanics yields several insightful observations and leads to the formulation of key hypotheses. First, rather than employing a single LoRA for the entire domain, it proves more effective to deploy multiple, smaller LoRA heads, each dedicated to a specific downstream task (see Figure 1(b)). This suggests that domain or task interference might harmfully impact the training process. We further hypothesize that this interference originates from _"intrinsic components"_--sub-domains or distinct tasks--potentially unknown even to domain experts. Additionally, upon visualizing the parameters of LoRA, we discern a pattern: some parameters predominantly learn the commonalities across all data, while others focus on the unique aspects of each intrinsic component. From these observations, we posit that an optimal LoRA architecture should embody an explicit, asymmetric structure.

Building upon the observations, we propose an improved end-to-end LoRA framework, which we refer to as _HydraLoRA_. From the architecture perspective, unlike LoRA's symmetric structure, _HydraLoRA_ has an asymmetric structure that has a shared A matrix and multiple B matrices (see Figure 1(c)). The shared A matrix is used by all samples for parameter efficiency. During the fine-tuning phase, _HydraLoRA_ is designed to auto-identify "intrinsic components" and segregate training samples into distinct B matrices. During the inference phase, _HydraLoRA_ leverages multiple B matrices using Mixture-of-Experts (MoE; [20; 40]) manner. Unlike prior work, _HydraLoRA_ completely eliminates the need for human expertise and assumptions, showing better performance than using domain knowledge to guide the fine-tuning process.

## 2 Background and Motivation

### LoRA Basics

LoRA  achieves comparable performances to fine-tuning on many benchmarks by freezing the pre-trained model weights \(W_{0}\) and inserting trainable rank decomposition matrices into each layer of the pre-trained model. In particular, for each layer, LoRA uses two sequential low-rank matrices \(A\) and \(B\) to fit the residual weights for adaptation. The forward computation is written as follows:

\[y=y+ y=W_{0}x+BAx\] (1)

where \(y R\)\({}^{}\) is the output and the \(x R\)\({}^{}\) denotes the input. \(B R\)\({}^{},A R\)\({}^{}\) with \(r min(d,k)\). Normally matrix \(B\) is initialized with zeroes and matrix \(A\) is initialized with Kaiming Uniform  to force \(\)\(y=0\) at the beginning.

Figure 1: Illustration of LoRA architecture changes in _HydraLoRA_. Only the tunable parameters are shown in this Figure. (a) LoRA architecture with matrix A to achieve low rank and matrix B to recover. (b) under the same parameter count, a monolithic LoRA is split into multiple smaller A and B matrices to avoid training interference. (c) based on (b), _HydraLoRA_ has an asymmetric structure that has a shared A matrix and multiple B matrices.

### LoRA's Practical Dilemma

Parameter count has a clear impact on the performance of neural models [22; 33]. Yet, Parameter-Efficient Fine-tuning (PEFT) methods, such as Adapter  and prefix-tuning , focus on fine-tuning a limited set of parameters. These approaches present a practical dilemma: while restricting the number of tuned parameters is essential for training efficiency, it hinders the model's ability to learn from diverse datasets. This trade-off becomes particularly evident when considering corpus heterogeneity . Figure 2 reveals a notable performance disparity between PEFT techniques and full fine-tuning (FFT), with the gap widening in scenarios involving a more diverse or heterogeneous training corpus.

### Observations

In this work, we aim for a PEFT approach that strikes a better balance between maximizing the learning capability for heterogeneous data and minimizing the number of parameters involved. A key goal is to ensure that our enhanced technique exhibits robust generalization across unseen tasks, independent of any prior task-specific knowledge. To achieve our objectives, we focus on LoRA and conduct a series of experiments as Table 1 to gain a deeper understanding of its mechanisms. Our methodology involves leveraging data from diverse tasks within a domain, and training distinct LoRA heads for each domain, leading to our first observation:

_Observation I: With the same parameter count, rather than employing a single LoRA for the entire domain dataset, it proves more effective to deploy multiple, smaller LoRA heads, each dedicated to a specific downstream task._

This suggests that interference among tasks might harmfully impact the training process. Furthermore, we posit that this interference is NOT exclusive to this explicit multi-task training. This interference could happen in any training setting since all datasets inherently consist of multiple implicit _intrinsic components_, such as sub-domains or tasks within a domain that is even unknown to domain experts. To better understand how multiple LoRA heads mitigate the interference among intrinsic components, in Figure 3, we employ the t-SNE technique  to visualize the parameters of matrix A and B across all heads. This analysis yields another critical observation:

_Observation II: When multiple LoRA heads are trained individually on different data, the parameters of matrix A from different heads tend to converge, while those of matrix B are distinguishable._

In detail, the parameters of matrix A across all heads exhibit a high degree of similarity, leading to their overlaps in the figure. Conversely, the parameters of matrix B from different heads are distinct and easily distinguishable. We posit that this divergence is an artifact of the initialization schemes, with matrix A inclined toward capturing commonalities across domains, while matrix B adapts to domain-specific diversities. The distinction between matrix A and B offers valuable insights for enhancing both parameter efficiency and effectiveness. From an efficiency standpoint, our hypothesis suggests that the parameters of matrix A could potentially be shared across multiple heads, thereby reducing redundancy. Regarding effectiveness, since the parameters of matrix B of different heads

  
**Schemes** & \(r n\) & **MMLU1** & \(\%\)**Parameter** \\  LoRA & \(8 1\) & 43.22 & 0.062 \\ LoRA & \(16 1\) & 45.45 & 0.124 \\ LoRA & \(32 1\) & 46.59 & **0.248** \\  LoRA (Split) & \(16 2\) & 46.82 & 0.248 \\ LoRA (Split) & \(8 4\) & **46.94** & 0.248 \\ LoRA (Split) & \(4 8\) & 46.83 & 0.248 \\   

Table 1: Performance on instruction tuning with Dolly-15K  and evaluated with MMLU  with different ranks. For LoRA (Split) decomposes high-rank LoRA modules into smaller, equivalent low-rank components (\(r n\)). \(n\) is the number of LoRAs, \(r\) denotes the rank of each LoRA.

Figure 2: Performance impact of corpus heterogeneity on full fine-tuning vs. parameter-efficient fine-tuning. Heterogeneity signifies the diversity within the dataset, often leading to interference due to its varied content and style . Parameter-efficient approaches are particularly sensitive, suffering greater performance losses in heterogeneous cases.

are dispersed, suggesting that using a single head to adapt to multiple domains might be less effective than using individual heads for each domain, which minimizes the interference between domains.

Building upon our observations, we propose an optimized LoRA architecture designed to enhance cost-effectiveness. In this architecture, we share the parameters of A matrix across various sub-domains or tasks to improve parameter efficiency, while deploying multiple B matrices, each tailored to handle different intrinsic components. This design allows for a more effective adaptation to the specific characteristics of each component. While these intrinsic components can be manually identified using prior knowledge of the training data, we also introduce end-to-end methods using Mixture-of-Experts (MoEs) , which will be detailed in the methodology section. This automatic approach facilitates flexibility and applicability, particularly in scenarios where prior knowledge is limited or unavailable.

## 3 _HydraLoRA_

In this section, we introduce the proposed _HydraLoRA_, an asymmetric LoRA architecture for efficient fine-tuning, as illustrated in Figure 1. After that, we show the workflow of _HydraLoRA_ as Figure 4.

### Asymmetric LoRA architecture

The LoRA method updates two low-rank matrices \(A\) and \(B\), and uses \(AB\) as the change of a pretrained and frozen weight \(W_{0}\) of a linear layer as shown in Eq. 1. The integral parameters are fine-tuned for the whole corpus in the original LoRA, which causes difficulty in learning the various knowledge aspects. Drawing from a detailed breakdown analysis of LoRA, a potential solution is to segment the entire LoRA into "Hydra" structured LoRA variants, that is, characterized by a central shared matrix \(A\) and several distinct matrices \(B\), fostering a blend of shared knowledge and specialized functionalities. As Figure 1, _HydraLoRA_ is to fine-tune LoRAs to achieve robust performance without redundancy, thereby benefiting the entire heterogeneous corpus. The asymmetric LoRA architecture can be formulated as:

\[ W&=\,W_{0}+\,W\\ &=\,W_{0}+_{i=1}^{N}_{i} B_{i}A\] (2)

The matrices \(B_{i}^{d r}\) and shared \(A^{r k}\). The hyper-parameter \(N\) denotes the number of \(B\) matrices. The term \(_{i}\) modulates these contribution weights for head \(B_{i}\).

### Workflow of _HydraLoRA_

Figure 4 illustrates the workflow of _HydraLoRA_. Initially, _HydraLoRA_ delves into the adaptive identification and initialization of LoRA modules within a heterogeneous corpus, aligning them with

Figure 3: Breakdown analysis of LoRA modules. Compare fine-tuned LoRA modules of Dolly-15K  with three subtasks of Dolly-15K including “_summarization (Sum)_”, “_closed QA (QA)_” and “_information extraction (IE)_” using t-SNE. Consider LLaMA2-7B (random seed=42), which contains 32 decoder layers, corresponding to 32 adaptive modules. Each module consists of {**0**: q_proj of A, I: q_proj of B, **2**: v_proj of A, **3**: v_proj of B} submodules. This makes a total of \(32 4\) submodules. Left displays all submodules. Center shows all even submodules, i.e. the A matrix. Right represents all odd submodules, i.e. the B matrix. It can be seen that the differences in the fine-tuned LoRA modules for different tasks arise mainly from the B matrix.

task relevance through the application of \(k\)-means or developer-specified size. Subsequently, we propose a Mixture-of-Experts (MoE) framework that handles \(B\) matrices as expert adapters to ensure computational efficiency throughout the fine-tuning (Section 3.2.1) and inference (Section 3.2.2) stages by freezing the rest of the LLM parameters. During inference, it flexibly and dynamically merges multiple \(B\) matrices through the MoE router.

#### 3.2.1 Fine-tuning

Motivated by Mixture-of-Experts (MoEs; [20; 40]), where experts are selectively activated by a gating mechanism (Router) in response to different inputs. In _HydraLoRA_, we substitute each expert with a lightweight LoRA adapter. During fine-tuning, while weights of LLMs remain frozen, the experts and router layers are trained from scratch. In order to achieve a unified approach to the distinct forward processes of multiple \(B\) matrices, we define a set of experts, denoted as \((E_{1},,E_{N})\), to learn the updated matrix \( W\). As _HydraLoRA_ fine-tunes the experts using the heterogeneous corpus, the shared matrix \(A\) inherently captures collaborative knowledge to augment intra-gains, and different matrices \(B\) foster knowledge modularity to mitigate fine-tuning inter-offsets. Based on this structure, the forward process of _HydraLoRA_ is expressed as:

\[y=\,W_{0}x+_{i=1}^{N}_{i}E_{i}Ax(MoE)\] (3)

where \(N\) denotes the number of experts, i.e., \(B\) matrices.To regulate these contributions, we introduce a gate function (router network) commonly consisting of a dense layer with trainable weights (transformation matrix) \(W_{g}^{r N}\) followed by a softmax function which takes an intermediate token representation \(x\) as input and combines the output of each expert based on the gating scores \((_{1},,_{N})\):

\[_{i}=(\,W_{g}^{T}x)(Router)\] (4)

#### 3.2.2 Inference

During inference, _HydraLoRA_ merges adapters by enabling routing computation based on the input. Specifically, since matrices B operate as linear functions, we initially compute a weighted average of the experts. Following this, we apply a PEFT transformation using the combined expertise. The _HydraLoRA_ significantly enhances training efficiency through an extremely parameter-efficient MoE formulation. Additionally, the intrinsic structural modularity of _HydraLoRA_ facilitates rapid recovery and merging of the trained parameters during inference, leading to substantial memory savings.

Figure 4: Architecture and workflow of _HydraLoRA_. During the fine-tuning stage, _HydraLoRA_ first adaptively identifies and initializes \(N\) of intrinsic components without specific domain knowledge. It then employs a trainable MoE router that treats each intrinsic component as an expert to automatically segregate training samples into intrinsic components for fine-tuning. During the inference stage, _HydraLoRA_ merges multiple \(B\) matrices flexibly and dynamically through a trained router.

## 4 Experiments

In this section, we detail the principal experiments. We begin with an overview of the experimental setup and implementation intricacies. Following this, we share our findings and offer a succinct interpretation.

### Experiment Setting

Dataset and BenchmarksTo explore the properties and commonalities of the LoRA asymmetric structure, we conduct experiments on both single and multiple domains to evaluate the effectiveness of _HydraLoRA_ for profiling intrinsic components. \(\) Single domain. 1) _General_: we fine-tune with the general instruction tuning databricks-dolly-15k  for generic language capability and evaluate with MMLU . 2) _Medical_: we fine-tune with GenMedGPT and clinic-10k from ChatDoctor  for medicine applications and evaluate medical tasks in MMLU. 3) _Law_: we fine-tune with two legal instruction tuning datasets Lawyer-Instruct  and US-Terms  then evaluate with law tasks in MMLU. 4) _Math_: we fine-tune with the training split of GSM8K  for mathematical reasoning and evaluate with test set of GSM8K. 5) _Code_: we fine-tune with CodeAlpaca  for code generation and evaluate with HumanEval . \(\) Multi-task domain. We select a portion of the Flanv2  datasets covering Natural Language Understanding (NLU) and Natural Language Generation (NLG), which can be grouped into 10 distinct task clusters. Then we evaluate it with the Big-Bench Hard (BBH)  benchmark. A detailed description of the benchmarks can be found in Appendix A.1.

Baselines\(\) First, we compare _HydraLoRA_ with different PEFT methods on single datasets: 1) _Full fine-tuning_; 2) _Prompt Tuning_; 3) _P-Tuning_; 4) _Prefix Tuning_; 5) _IA\({}^{3}\); 6) _AdaLoRA_. \(\) Second, we extend the experiments exploring _HydraLoRA_ on multiple datasets compared with more weighted average methods: 1) Lorahub  employs black-box optimization to learn weights of 20 randomly selected LoRAs for new tasks, using weighted averaging without needing gradient calculations. 2) LoRA MoE  combines lightweight experts (LoRA) with MoE architecture for high efficiency, generalizing to new tasks without prior knowledge. A detailed description of the baseline models can be found in Appendix A.2.

### Overall Performance

The experimental results of _HydraLoRA_ and the competing baselines are presented in Table 2 with a single domain and Table 3 with the mixed task domain. The evaluation of diverse tasks demonstrates that _HydraLoRA_ consistently outperforms all other schemes. The performances rooted in LoRA outperform those of conventional PEFT methodologies. Compared to the default single LoRA configuration (rank=8), the Hydra architecture, enriched by the integration of several B matrices, effectively addresses the inherent conflicts among intrinsic components of the corpus. Furthermore, with equivalent parameters (rank=16), the model shows superior performance, confirming the ef

    &  &  &  &  &  &  &  &  \\  & & & & & & & & & \\  LLaMA2-7B  & 38.88 & 35.98 & 33.51 & 13.10 & 20.34 & 10.38 & - & - & - \\ Full Fine-Tuning & 49.91 & 46.78 & 46.08 & 20.24 & 32.93 & 25.70 & 100 & - & - \\  Prompt Tuning  & 39.91 & 37.59 & 35.02 & 13.66 & 21.55 & 13.18 & 0.001 & - & - \\ P-Tuning\({}_{(256)}\) & 41.11 & 39.81 & 36.72 & 13.60 & 21.13 & 15.56 & 0.193 & - & - \\ Prefix Tuning  & 41.78 & 40.28 & 36.54 & 13.23 & 22.56 & 16.89 & 0.077 & - & - \\ (IA)  & 40.45 & 37.12 & 35.25 & 13.54 & 23.17 & 13.98 & 0.009 & - & - \\ AdaLoRA\({}_{(r=8)}\) & 44.32 & 42.83 & 39.36 & 14.81 & 23.78 & 19.51 & 0.093 & 1 & 1 \\  LoRA\({}_{(r=8)}\) & 43.22 & 41.59 & 37.85 & 15.67 & 22.95 & 18.24 & 0.062 & 1 & 1 \\ LoRA\({}_{(r=16)}\) & 45.45 & 43.10 & 39.64 & 16.71 & 25.60 & 20.32 & 0.124 & 1 & 1 \\ LoRA\({}_{(r=32)}\) & 46.59 & 44.32 & 40.81 & 17.12 & 25.89 & 20.67 & 0.248 & 1 & 1 \\ LoRA-Split\({}_{(4 8)}\) & 46.94 & 45.28 & 41.35 & 18.20 & 26.85 & 21.92 & 0.248 & 4 & 4 \\  _HydraLoRA\({}_{(r=8)}\)_ & **47.22** & **45.71** & **42.18** & **18.31** & **27.43** & **22.27** & **0.124** & 1 & 3 \\   

Table 2: Comparative performance of different tuning schemes across multiple benchmarks on a single domain. 8-shot for GSM8K, zero-shot for others. #\(\) refers to the average \(\) matrix number.

fectiveness of the adopted parameters. Based on Table 2 and Table 3, we propose three research questions that confirm the aforementioned observations.

RQ1: Is it more effective to use multiple smaller LoRA heads for specific tasks rather than one single LoRA for the entire domain dataset, given the same parameter count?Comparing the high-dimensional LoRA configuration with \(r=32\) against a segmented version using LoRA-Split, a variant introduced by HydraLoRA, which divides the model into four distinct components each with \(r=8\). That is, multiple vanilla LoRAs are directly utilized to capture the differences between data. We observe a noteworthy trend in the performance across a variety of tasks as detailed in Table 2. It illustrates the superior performance of LoRA-Split in comparison to the traditional LoRA approach, across all the evaluated scenarios. This enhancement in performance is a strong indication of the detrimental impact that task interference can have on the training process. By segregating the tasks into discrete components, LoRA-Split effectively minimizes the conflict and interference between tasks, thereby promoting a more efficient and focused environment.

The concept of LoRA-Split hinges on the construction of different intrinsic component compositions, employing LoRA as a foundational technique to strategically mitigate the interference conflict. This architectural innovation has proven to be a pivotal factor in enhancing model performance. However, it's important to note that while LoRA-Split marks a significant advancement in model efficiency and task handling, it also introduces a certain level of parameter redundancy. The segmented approach of LoRA-Split inevitably leads to an increase in the overall model parameters, which can be manifold in comparison to the traditional, singular LoRA model. This increase in parameters, while contributing to the model's robustness and capability to handle multiple tasks simultaneously, also poses new challenges in terms of computational resources and model optimization.

RQ2: Will multiple LoRA heads, individually trained on different data, improve efficiency by distinguishing matrix B parameters?We evaluated the Hydra structure LoRA -- _HydraLoRA_ that is characterized by a shared LoRA A matrix, while maintaining distinct B matrices that are trained separately. This configuration was meticulously compared with both the standard LoRA and the LoRA-Split approaches, emphasizing efficiency parameters.

According to the results presented in Table 2, unlike split which straightforwardly adopts multiple vanilla LoRAs, _HydraLoRA_ adopts an asymmetric LoRA structure that not only improves parameter efficiency by separating the uses of A matrix for commonalities and B matrices for diversities with a notably smaller adapter parameter set, but also employs a trainable router to improve the composition of multiple B matrices that outperforms the LoRA-Split approach. This finding is significant as it suggests that _HydraLoRA_ not only enhances performance efficiency but also boosts overall system effectiveness. This may be driven by 1) different B matrices capturing different features of the data-intrinsic knowledge, mitigating mutual interferences, and avoiding performance offsets. 2) Module A maintains the collaborative knowledge by taking the strengths of each and integrating them to improve the model performance.

RQ3: How does _HydraLoRA_ fare against other merge methods in complex, multi-task domains, considering scalability and robustness?While we hypothesize that the asymmetry is mainly rooted in the different initialization methods of A and B matrices, it is possible that this behavior varies on different model architectures and datasets. Recent work confirms similar empirical observations [54; 13]. To the best of our ability, we extended the experiments exploring _HydraLoRA_ on multiple

    & **Base** & **LoRA** & **LoraHub** & **LoRA MoE** & _HydraLoRA_ \\   & 7B & 31.6 & 36.8 & 39.7 & 40.3 & **41.5** \\  & 13B & 38.4 & 40.1 & 41.9 & 43.7 & **44.1** \\  \# of A/B for training & 0/0 & 1/1 & 48/48 & 48/48 & 1/10 \\ \# of A/B for inference & 0/0 & 1/1 & 20/20 & 48/48 & 1/10 \\ \% Params & - & 0.062 & 1.240 & 2.976 & 0.341 \\   

Table 3: Comparative performance of different tuning schemes, including base model (Base), LoRA tuning (LoRA), LoraHub learning, multi-LoRA tuning with MoE inference (LoRA MoE) and our proposed _HydraLoRA_ learning across mix-task domain on the BBH benchmark with LLaMA2-7B, LLaMA2-13B as the base LLM (3-shot).

datasets. LoRA MoE and their variants typically aim at tackling multi-tasks by employing multiple independent LoRAs. This makes them suitable for handling various domains. However, for a single dataset like ours, a "default" MoE method might not be optimal. HydraLoRA addresses this by constructing asymmetric structures and utilizing multiple B matrices to capture the specific nuances within the single dataset. The effectiveness of this approach is demonstrated by the experimental results in Table 3.

### Energy and Throughput Analysis

RQ4: How does the "Hydra" structure in _HydraLoRA_ enhance system efficiency, particularly in reducing training energy consumption and latency?We evaluate the system efficiency of _HydraLoRA_ from two perspectives: training energy consumption and latency. The following experiments were executed on a GPU infrastructure consisting of 4 NVIDIA A40 GPUs and a CPU powered by an Intel(R) Xeon(R) Gold 6330 CPU clocked at 2.00GHz. Power consumption measurements were recorded using CodeCarbon . Figure 5 shows the results of various fine-tuning approaches for GSM-8K using the LLaMA2-7B model. we can see that _HydraLoRA_ effectively speeds up the training process \(1.96\) and reduces 49.6% energy cost compared to LoRA (rank=32). While the energy consumption and latency of LoRA-Split exceeds the LoRA (rank=32). This is for the reason that _HydraLoRA_ jointly considers inherent knowledge modularity and collaboration, which utilizes the "Hydra" structure with a shared A matrix and different B matrix. In this way, it only employs rank=16 training overhead but expands to a performance enhancement of more than rank=32. Overall, this experiment demonstrates the parameter effectiveness of _HydraLoRA_.

### Ablation Study

RQ6: What impact do the MoE architecture and the gate function have on the fine-tuning process?To delve deeper into understanding the contributions of each component in _HydraLoRA_, we present the results of our ablation study in Figure 6. The variant _w/o_ MoE (essentially reverts to LoRA) excludes the MoE architecture. Similarly, the _w/o_ gate variant employs uniform expert weights bypassing the gate function. The _w/o_ hydra adopts multiple vanilla LoRAs in a straightforward way. Figure 6 indicates that the full _HydraLoRA_ model outperforms its variants, showing that both the MoE architecture and gate function significantly contribute to its effectiveness across various language understanding domains.

Figure 5: Energy consumption and latency during fine-tuning with different LoRA approaches (fine-tuning LLaMA2-7B with GSM-8K).

Figure 6: Comparative performance of ablation study for _HydraLoRA_ across multiple benchmarks.

### Hyper-parameter Analysis

RQ7: How do the number of intrinsic component of _HydraLoRA_ influence performance outcomes?As Figure 8 shown, we conduct a comprehensive and meticulous analysis by fine-tuning the Dolly-15K model on the LLaMA2-7B dataset and subsequently evaluating its performance on the MMLU benchmark to rigorously examine the impact of variations in the intrinsic component, symbolized by the variable \(N\), on the model's overall performance. _Empirically we find that the number N of clusters is not a sensitive parameter for HydraLoRA_, with a wide range of reasonable number N of clusters (e.g. 2 to 4) performing decently well in all settings in our experiments. Specifically, the performance loss of N = 3 vs. the optimal N = 4 is only 0.42%. Meanwhile, as illustrated in Figure 8, we employ three distinct methods to generate the number of corpus clusters 15-fold, and the results demonstrate that the k-means  yields comparable outcomes with DBSCAN . Therefore, based on this observation, we choose k-means because it is simple but effective, more sophisticated hyperparameter search approaches (e.g. DBSCAN, parameter sweep and Bayesian optimization) will be unnecessarily costly. It's noteworthy that _HydraLoRA_ is adeptly designed to orchestrate its components in a way that it can automatically calibrate and navigate toward the optimal performance configuration across various parameters. This intelligent auto-tuning is achieved through the application of the k-means clustering algorithm. This strategic component orchestration not only enhances performance but also ensures a more efficient and effective utilization of resources, underpinning the model's capability to adapt and perform efficiently in a dynamic computational environment.

## 5 Related work

Parameter-Efficient Fine-tuningLLMs are becoming increasingly powerful, but fine-tuning them often requires significant computational resources. This has spurred research on parameter-efficient fine-tuning (PEFT) techniques that reduce memory and storage costs during adaptation. One prominent PEFT approach is adapters [17; 37]. It introduces new, trainable dense layers within the existing model, keeping the original parameters frozen. This concept has proven successful across various domains [35; 42; 43; 56]. Further improvements on adapter compactness involve constructing parameter matrices using Kronecker products of low-rank matrices . Another PEFT strategy directly manipulates activations with learned vectors. This can be achieved through concatenation [29; 25; 24], multiplication (IA\({}^{3}\); ), or addition (BitFit; ). Prefix-tuning  and prompt-tuning  are noteworthy examples that fine-tune continuous prompts instead of designing discrete ones . Interestingly, a study suggests that many PEFT methods can be viewed as a form of adapter, providing a unified perspective . Beyond adding new parameters or altering the computational graph, researchers also explore sparse [12; 44; 46] or low-rank updates (LoRA; ).

Multi-LoRA ArchitectureLoRA has notably garnered increasing interest recently, becoming a standard approach for adapting LLMs such as LLaMA [47; 48] under limited computational resources. Recognizing its potential, researchers have delved deeper, exploring the benefits of employing multiple LoRAs. LoraHub  takes this multi-LoRA approach by training several adapters and strategically picking combinations based on the domain during inference. Meanwhile, MultiLoRA  focuses on horizontal scaling, aiming to reduce LoRA's parameter dependence. This involves splitting LoRA modules along the rank dimension and introducing learnable scaling factors for enhanced expressiveness. Addressing scaling challenges from a different angle, the mixture of LoRA concept is further proposed . This mitigates resource consumption whenscaling instruction-tuned LLMs. Recognizing the potential for conflict during instruction tuning, LoRAMoE  leverages the Mixture-of-Experts (MoEs; ) structure to safeguard the pre-trained LLM's knowledge from excessive corruption by instruction data. Similarly, MOELoRA  incorporates a MoE framework into LLMs, thereby improving their multitasking capabilities in the medical domain. Shifting the focus to the system perspective, S-LoRA  provides a framework for efficiently serving multiple LoRA adapters. Unlike previous methods that relied on choosing LoRA combinations based on their training domains, _HydraLoRA_ breaks free from the dependence on domain knowledge during inference. Additionally, _HydraLoRA_'s asymmetric structure further enhances parameter efficiency compared to existing symmetric approaches.

## 6 Conclusion

In this work, we start by conducting exploratory experiments applying the LoRA technique to LLaMA2, aiming to adapt it to a new domain across various tasks. This study unveils the limitations of employing a single LoRA for the entire domain, highlighting the detrimental effects of domain interference. In response, we introduce a novel architecture _HydraLoRA_ that features an asymmetric structure with a shared matrix for all samples and distinct matrices for each intrinsic component. This design improves domain adaptation by selectively focusing on distinct components, enhancing both fine-tuning and inference efficiency. Our research highlights the importance of balancing learning capabilities for diverse datasets against the need for a lean model, offering a viable pathway for improving LLMs with minimal parameter growth. More discussion about limitation and broader impacts are available in Appendix D and E.

## 7 Acknowledgments

This research received support from MYRG-GRG2023-00211-IOTSC-UMDF and the Start-up Research Grant of the University of Macau (SRG2022-00010-IOTSC). Chunlin Tian and Zhan Shi contributed equally to this work. For correspondence, please contact Dr. Li Li (llili@um.edu.mo) or Dr. ChengZhong Xu (czxu@um.edu.mo).