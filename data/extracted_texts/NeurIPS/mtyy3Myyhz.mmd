# S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning

Weihao Lin1\({}^{}\), Shengji Tang1\({}^{}\), Chong Yu2, Peng Ye3, Tao Chen1\({}^{*}\)

\({}^{1}\)School of Information Science and Technology, Fudan University, Shanghai, China,

\({}^{2}\)Academy for Engineering and Technology, Fudan University, Shanghai, China,

\({}^{3}\)Shanghai AI Laboratory, Shanghai, China

eetchen@fudan.edu.cn

Corresponding Author (eetchen@fudan.edu.cn). \({}^{}\)Equal Contribution.

###### Abstract

Recently, differentiable mask pruning methods optimize the continuous relaxation architecture (soft network) as the proxy of the pruned discrete network (hard network) for superior sub-architecture search. However, due to the agnostic impact of the discretization process, the hard network struggles with the equivalent representational capacity as the soft network, namely discretization gap, which severely spoils the pruning performance. In this paper, we first investigate the discretization gap and propose a novel structural differentiable mask pruning framework named S2HPruner to bridge the discretization gap in a one-stage manner. In the training procedure, S2HPruner forwards both the soft network and its corresponding hard network, then distills the hard network under the supervision of the soft network. To optimize the mask and prevent performance degradation, we propose a decoupled bidirectional knowledge distillation. It blocks the weight updating from the hard to the soft network while maintaining the gradient corresponding to the mask. Compared with existing pruning arts, S2HPruner achieves surpassing pruning performance without fine-tuning on comprehensive benchmarks, including CIFAR-100, Tiny ImageNet, and ImageNet with a variety of network architectures. Besides, investigation and analysis experiments explain the effectiveness of S2HPruner. Codes are publicly available on GitHub: https://github.com/opposj/S2HPruner.

## 1 Introduction

As deep neural networks (DNN) have achieved success in substantial fields , the increasing computation and storage cost of DNN impedes practical implementation. Model pruning , which aims at removing the less informative in a cumbersome network, has been a widespread technique for model compression. Pioneer pruning methods utilize regularization terms  to sparsify the network or introduce importance metrics  to remove less important weights directly. However, due to the latent correlations between weights, simply eliminating the weights in an over-parameter model will hinder the integrality of structure, especially in structural pruning, where grouped filters are removed.

Recently, it has been pointed out that the structure of the pruned network is essential for the final pruning performance . Inspired by the differentiable architecture search (DARTS) , emerging works , namely differentiable mask pruning (DMP), introduce learnable parameters to generate the weight mask and impose the task-aware gradient to guide the structure search of the pruned network. In the training procedure, DMP introduces the learnable mask into the gradient graph by coupling the mask with the activation feature or weights, e.g., directly multiplyingthe mask with the feature or weights. Through gradient descent, DMP can jointly optimize the weights and mask parameters for a bespoke structure and parameter distribution, thus causing a better performance. The search procedure essentially regards the mask-coupled network (soft network) as the performance proxy of the final discretized compact pruned network (hard network). Whereas, considering the aim of pruning is to obtain a capable hard network, a natural question is **whether a superior soft network implies a corresponding high-performance hard network**.

In DARTS, there is a problem known as the discretization gap [66; 60; 7], which refers to the discrepancy between the continuous relaxation architecture and the discrete architecture due to the discretization process. Since DMP follows a similar modeling format to DARTS, it also faces a comparable discretization gap problem+ that the hard network struggles from having the semblable representational capacity as the soft network. A specific manifestation is that the hard network performs significantly poorer in the evaluation metrics than the soft network. Fig. 1 visually exhibits the different pruning methods and discretization gap. The discretization gap severely impacts pruning performance but has been long overlooked in DMP. There are potential techniques that may alleviate the discretization gap in previous works, e.g., gradually facilitating the steepness of the Sigmoid function via decaying temperature [26; 27; 44; 50] and optimizing the binary mask via the straight-through estimator (STE) [65; 15]. However, these methods lead to certain side effects: the decaying temperature results in difficult mask optimization because of the vanishing gradient, and STE causes a suboptimal mask due to the coarse gradient.

Footnote †: To avoid confusion, the discretization gap discussed following is in the context of DMP.

To alleviate the discretization gap in DMP without influencing mask optimization, we formulate the mask pruning in a soft-to-hard paradigm and propose a structured differentiable mask pruning framework named Soft-to-Hard Pruner (S2HPruner). Specifically, in the training procedure, we not only forward the soft network for the structural search but also forward the corresponding hard network and distill it under the supervision of the soft network to reduce the discretization gap. Meanwhile, we discover that even with the same corresponding hard network, the distribution of the mask parameters influences the discretization gap essentially. However, the common unidirectional knowledge distillation (KD) cannot optimize mask parameters directly, but bidirectional KD causes unbearable performance degradation. Therefore, we propose a decoupled bidirectional KD, which blocks the weight updating from the hard to the soft network while keeping the gradient corresponding to the mask. Exhaustive experiments on three mainstream classification datasets, including CIFAR-100, Tiny ImageNet, and ImageNet, demonstrate the effectiveness of S2HPruner.

Our contributions are summarised as follows:

* We first study and reveal the long-standing overlooked discretization gap problem in differentiable mask pruning. To alleviate it, we propose a soft-to-hard distillation paradigm, which distills the hard network under the supervision of the soft network.
* Based on the soft-to-hard knowledge distillation paradigm, we propose a novel differentiable mask pruning framework named Soft-to-Hard Pruner (S2HPruner). To further reduce the

Figure 1: Comparison of different typical pruning methods and illustration of discretization gap. The darker color represents the higher relative magnitude scale of weights or masks. \(\) denotes Hadamard product. For ease of demonstration, we use one layer to represent the entire network.

discretization gap and avoid performance degradation, we propose a decoupled bidirectional KD which blocks and allows the gradient of model weights and mask parameters selectively.
* Extensive experiments on three mainstream datasets and five architectures verify the superiority of S2HPruner, e.g., maintaining 96.17%(Top-1 accuracy 73.23% in 76.15%) with around 15% FLOPs. Additional ablation and investigation experiments demonstrate the underlying mechanism of the effectiveness.

## 2 Related works

### Differentiable mask pruning

Considering the network structure has a decisive impact on the pruning performance , numerous works [15; 12] train a binary mask for an optimal selection of sub-architecture. However, because of the non-differentiable property, directly optimizing the binary mask is very challenging and even impairs the performance . Differently, differentiable mask pruning (DMP) methods [17; 9; 4; 27; 44] adopt differentiable continuous relaxation as a performance proxy of the hard network for structure search, which can be easily optimized by task-aware loss end-to-end. DMCP  regards the channel pruning as a Markov process and builds a differentiable mask based on the transitions between states. AutoPruner  proposes to construct a meta-network to generate the differentiable mask according to the activation responses, and a scaled temperature facilitates the sigmoid function approaching step function to obtain an approximate binary mask. GAL  learns a differentiable mask by optimizing a generative adversarial learning task in a label-free and end-to-end manner. However, the task-aware loss can ensure the high performance of the soft network but not the final hard network. There is a discretization gap limiting the target hard network during the discretization process. Different from previous DMP methods, which only focus on optimizing the soft network, our approach aims to achieve a high-performance hard network by reducing the discretization gap through soft-to-hard distillation.

### Pruning with distillation

As a network compression technique orthogonal to pruning, knowledge distillation [22; 28; 52] (KD) transfers the dark knowledge from a large teacher network to enhance a compact student network. Recently, there have been substantial works [46; 47; 3; 32; 10] introducing KD into model pruning to further boost the pruned network. JMC  proposes a structured pruning based on the magnitude of weights and a many-to-one layer mapping strategy to distill the dense model to the pruned one. KD ticket  exploits the dark knowledge in the early stage of iterative magnitude pruning to boost the lottery tickets in the dense model. DIPNet  improves the ability of the pruned model by the supervision of high-resolution output. The above methods treat KD as an independent plug-in technique to enhance pruning performance without tight coupling with the selection of weights. Differently, in the proposed method, KD contributes to mask optimization directly as an integral part of the core pruning procedure. Moreover, in contrast to the typical unidirectional KD, we propose a novel decoupled bidirectional KD to alleviate the discretization gap between soft and hard networks, due to the distinct attributes of mask and weights.

## 3 Method

### Problem formulation

Given a network with parameters \(\), a pruning algorithm generates a binary mask \(\) via solving the following constraint optimization:

\[_{,}( )\ (,T)=0.\] (1)

The \(\) are the remaining parameters after pruning. The \(\) and \(\) are the task-specific performance loss and resource regularization, respectively. The \(T\) is a manually assigned resource budget. Intuitively, a pruning algorithm attains a slimmed subnet that optimally balances the performance and the resource consumption.

### Overview

Directly optimizing the problem 1 is almost intractable due to the discreteness of \(\). To get around, we introduce a relaxation of \(\) as \(\), which is continuous and bounded to \(\). The \(i\)-th element in \(\) represents the probability of the \(i\)-th parameter being retained. Consequently, a differentiable representative for \(\) can be constructed as \(\), where the \(\) denotes the Hadamard product. Based on this relaxation, the problem 1 can be reformulated as two parts:

\[\:&_{,}(()+ (,T)),\\ \:&&_{ ,}(} ,).\] (2)

The \(\) is a Lagrangian multiplier, regarded as a hyperparameter. The \(\) is a gap measure, reflecting the difference between \(}\) and \(\). The \(}\) is an estimated pruning mask, derived from \(\) as \(_{[t,1]}()\), where the \(\) is an indicator function, and the t is a threshold. In the problem 2, the first part searches for a high-performance soft network that satisfies the resource constraint, and the second part reduces the gap between the hard network and the soft one. Similar to , to avoid alternate optimization, we combine the two parts with two additional hyperparameters \(\) and \(\):

\[_{,}(( )+(,T)+( }, )).\] (3)

The problem 3 is differentiable w.r.t. both \(\) and \(\), thus can be optimized by gradient-based methods :

\[&=-_{}(_{ }+_{ }}+_{ }), \\ &=-_{}(_{ }+ _{}+_{ }).\] (4)

The \(_{}\) and \(_{}\) are learning rates for \(\) and \(\), respectively. The \(_{X}\) denotes the gradient obtained via a backward path \(X\). Note that the term \(_{}\) implies aligning the soft network towards the hard one, which would severely deteriorate the performance of the soft network (see Section 4.2 for details). Consequently, the update of \(\) is modified to:

\[=-_{}(_{ }+_{ } }).\] (5)

The essence of the above optimization lies in two aspects: 1) the joint optimization of the entire parameters \(\) and a dynamic subset of parameters \(}\) benefits from stimulative training , where the entire parameters transfer knowledge to the partial ones, and the improvement of the partial parameters can, in turn, enhance the entire ones; 2) the optimization of \(\) involves the soft-to-hard gap, which provides a new dimension to bridge the gap besides adjusting the parameters. The pseudo-code describing the whole training process can be referred to in Algorithm 1, and a visualization of the forward/backward passes is provided in Fig. 2.

Figure 2: The proposed pruner’s forward and backward flows, illustrated via an exemplary linear layer with parameters \(\). The \(\) are the additional learnable parameters normalized by softmax. The \(\) denotes the relaxed mask. The estimated binary pruning mask is the \(}\). The input is denoted by \(\). The output of the soft and hard networks are the \(\) and \(\), respectively. The \(\), \(\), and \(\) are the performance loss, gap measure, and resource regularization, respectively.

### Implementation details

We focus on dependency-group-based structural pruning [6; 14], where layers in the same group share a single mask and are pruned as a whole. Besides, the pruning mask is channel-wise to comply with the structural pattern. The performance metric \(\) is the cross-entropy for classification. The Kullback-Leibler divergence is selected as the gap measure \(\).

Acquisition of \(\) and \(t\)Consider a linear layer parameterized by \(^{C_{out} C_{in}}\). The corresponding binary pruning mask is denoted as \(^{C_{out}}\). To generate \(\), we define learnable parameters \(^{C_{out}}\), which can be normalized to \(\) via a softmax function. After softmax, the \(i\)-th element in \(\) can be interpreted as the probability of retaining the first \(i\) channels. Consequently, the probability of the \(i\)-th channel being retained, _i.e._, \(w_{i}\), can be calculated as \(_{k=i}^{C_{out}}u_{k}\). With the \(\) obtained, the pruning threshold \(t\) is derived as \(}_{k=1}^{C_{out}}w_{k}\).

Resource regularizationWe utilize floating-point operations per second (FLOPs) to evaluate resource consumption. Given a target \(T\) (in percentage), the resource regularization \(\) is defined as \((_{soft}/\,_{all}.-T)^{2}.\). The \(_{all}\) is the FLOPs of the entire network. The \(_{soft}\) is the summation of layer-wise differentiable FLOPs. To be differentiable, the output channel number of a layer is calculated as \(_{k=1}^{C_{out}}(u_{k}*k)\). The \(u_{k}\) is a softmaxed parameter introduced in the previous section.

## 4 Experiments

In this section, we begin by validating the effectiveness of the proposed pruner using three benchmark datasets: CIFAR-100 , Tiny ImageNet , and ImageNet . For CIFAR-100 and Tiny ImageNet, we evaluate three common CNN architectures, _i.e._, ResNet-50 , MobileNetV3 (MBV3) , and WRN28-10 , and two Transformer architectures, _i.e._, ViT  and Swin Transformer , across various pruning ratios including 15%, 35%, and 55%. For ImageNet, ResNet-50 serves as the backbone model, and we compare the proposed pruner with several structural pruning methods in terms of Top-1 accuracy and FLOPs. After the benchmarking, investigative experiments are performed on CIFAR-100 using ResNet-50 to elucidate the influence of each gradient term in Algorithm 1 and the gap-narrowing capacity of the proposed pruner. Detailed training configurations are provided in the Appendix.

### Benchmarking

Results on CIFAR-100 and Tiny ImageNetTo assess the performance of the proposed pruner and demonstrate its adaptability to various networks, we conduct experiments using CIFAR-100 and Tiny ImageNet datasets, with ResNet-50, MBV3, and WRN28-10 serving as the backbone architectures. For each dataset-network combination, we test three different FLOPs: 15%, 35%, and 55%. We compare the proposed pruner against structured RST  (referred to as RST-S), Group-SL , OTOv2 , and Refill . All methods are evaluated under consistent training settings for a fair comparison. The results, presented in Table 1 and Table 2, reveal that the proposed pruner consistently outperforms other methods, particularly at low FLOPs. For instance, when constraint with 15% FLOPs, the proposed pruner maintains high accuracy, with gains of up to 2.73% on CIFAR-100 and 3.99% on Tiny ImageNet over the next best method.

To further validate the generalizability of the proposed pruner, we apply it to two typical Transformer models, ViT  and Swin Transformer . Similar to the CNN experiments, we test these models on CIFAR-100 with FLOPs targets of 15%, 35%, and 55%. The results, shown in Table 3, indicate that the proposed pruner outperforms RST-S for both Transformer models across all FLOPs targets. Notably, at 55% FLOPs, the ViT pruned by the proposed method does not suffer any performance loss, and the Swin Transformer merely experiences a slight performance drop of 0.47%. The results demonstrate that while the proposed pruner is not explicitly designed for Transformers, it still achieves competitive results, highlighting its significant potential for pruning Transformer models.

Results on ImageNetWe further assess the performance of the proposed pruner on the prevalent ImageNet-1K benchmark. The ResNet-50 is chosen as the baseline network. Table 4 shows that, for similar FLOPs, the proposed pruner consistently suffers the least accuracy drop compared to others, underscoring the effectiveness of the proposed pruner. In the particularly challenging low FLOPs range of 10% to 20%, the proposed pruner stands out, achieving a top-1 accuracy of 73.23%, which is 3.13% higher than OTOv2, while maintaining nearly the same FLOPs (around 15%).

### Gradient analysis

To investigate the influence of each gradient term in Algorithm 1, we conduct experiments with some of the terms disabled to observe the impact on the final performance. The results are shown in Table 5.

   &  &  &  \\   & 15\% & 35\% & 55\% & 15\% & 35\% & 55\% & 15\% & 35\% & 55\% \\  RST-S  & 75.02 & 76.38 & 76.48 & 72.90 & 76.78 & 77.30 & 78.56 & 81.18 & 82.19 \\ Group-SL  & 49.04 & 77.90 & 78.37 & 1.43 & 4.90 & 26.24 & 42.41 & 67.71 & 79.59 \\ OTOv2  & 77.04 & 77.65 & 78.35 & 76.29 & 77.35 & 78.39 & 77.26 & 80.61 & 80.84 \\ Refill  & 75.12 & 77.43 & 78.19 & 69.57 & 75.91 & 76.96 & 75.98 & 79.25 & 79.56 \\  Ours & **79.77** & **79.87** & **80.10** & **77.28** & **78.17** & **78.87** & **80.88** & **81.81** & **82.55** \\  

Table 1: The comparison of different pruning methods on CIFAR-100. We report the Top-1 accuracy(%) of dense and pruned networks with different remaining FLOPs.

   &  &  \\   & 15\% & 35\% & 55\% & 15\% & 35\% & 55\% \\  RST-S  & 70.74 & 72.05 & 74.65 & 70.53 & 72.98 & 75.25 \\  Ours & **72.61** & **75.53** & **76.49** & **75.29** & **75.79** & **76.69** \\  

Table 3: Verifications of transformers on CIFAR-100. We report the Top-1 accuracy(%) of dense and pruned networks with different remaining FLOPs.

Note that the term \(_{}\) is omitted from Table 5 since it is essential to satisfy the resource constraint and is always enabled.

The addition of the term \(_{ w}\) severely degrades the accuracy by 14.22%, indicating that the gradient that aligns the soft network towards the hard one is detrimental to the final performance. Intuitively, from the perspective of parameter capacity, the hard network is practically pruned, resulting in a lower capacity than the soft network. Enforcing the soft network moving towards a less capable one is not plausible.

Both of the term \(_{ w}\) and \(_{ w}\) contribute to improve the accuracy. For the term \(_{ w}\), it implies searching for a mask that maximizes the performance of the soft network. The term \(_{ w}\) encourages the alignment of the soft and hard networks. Different from the term

  Method & Unpruned top-1 (\%) & Pruned top-1 (\%) & Top-1 drop (\%) & FLOPs (\%) & \(E_{pr}\) & \(E_{ex}\) \\  OTOv2  & 76.10 & 70.10 & 6.00 & **14.50** & 120 & 0 \\ Refill  & 75.84 & 66.83 & 9.01 & 20.00 & 95 & 190 \\
**Ours** & 76.15 & **73.23** & **2.92** & 15.14 & 200 & 0 \\ MetaPruning  & 76.60 & 73.40 & 3.20 & **24.39** & 32 & 128 \\ Slimmable  & 76.10 & 72.10 & 4.00 & 26.63 & 100 & 0 \\ GAL  & 76.15 & 69.31 & 6.84 & 27.14 & 32 & 122 \\ DMCP  & 76.60 & 74.40 & 2.20 & 26.80 & 40 & 100 \\ ThiNet  & 72.88 & 68.42 & 4.46 & 28.50 & 110 & 90 \\ OTOv2  & 76.10 & 74.30 & 1.80 & 28.70 & 120 & 0 \\ GReg-1  & 76.13 & 73.75 & 2.38 & 32.68 & - & 180 \\ Greg-2  & 76.13 & 73.90 & 2.23 & 32.68 & - & 180 \\ CAIE  & 76.13 & 72.39 & 3.74 & 32.90 & - & 120 \\
**Ours** & 76.15 & **74.43** & **1.72** & 25.31 & 200 & 0 \\  CHIP  & 76.15 & 75.26 & 0.89 & 37.20 & - & 270 \\ OTOv2  & 76.10 & 75.20 & 0.90 & 37.30 & 120 & 0 \\ Greg-1  & 76.13 & 74.85 & 1.28 & 39.06 & - & 180 \\ GReg-2  & 76.13 & 74.93 & 1.20 & 39.06 & - & 180 \\ Refill  & 75.84 & 72.25 & 3.59 & 40.00 & 95 & 190 \\ ThiNet  & 72.88 & 71.01 & 1.87 & 44.17 & 110 & 90 \\ GBN  & 75.85 & 75.18 & 0.67 & 44.94 & 10 & 130 \\ GAL  & 76.15 & 71.80 & 4.35 & 45.00 & 32 & 122 \\ SCOP  & 76.15 & 75.26 & 0.89 & 45.40 & 140 & 90 \\ AutoPrune  & 74.90 & 74.50 & 0.40 & 45.46 & 60 & 90 \\ SCP  & 75.89 & 75.27 & 0.62 & 45.70 & 100 & 100 \\ FPGM  & 76.15 & 74.83 & 1.32 & 46.50 & 100 & 0 \\ LeGR  & 76.10 & 75.30 & 0.80 & 47.00 & - & 150 \\ AutoSlim  & 76.10 & 75.60 & 0.50 & 48.43 & 50 & 100 \\ AutoPruner  & 76.15 & 74.76 & 1.39 & 48.78 & 32 & 120 \\ MetaPruning  & 76.60 & 75.40 & 1.20 & 48.78 & 32 & 128 \\ CHEX  & 77.80 & **77.40** & 0.40 & 50.00 & 250 & 0 \\
**Ours** & 76.15 & 75.81 & **0.34** & **34.28** & 200 & 0 \\ CAIE  & 76.13 & 75.62 & 0.51 & 54.77 & - & 120 \\ CHIP  & 76.15 & 76.30 & -0.15 & 55.20 & - & 270 \\ Slimmable  & 76.10 & 74.90 & 1.20 & 55.69 & 100 & 0 \\ TAS  & 77.46 & 76.20 & 1.26 & 56.50 & 120 & 120 \\ SSSS  & 76.12 & 71.82 & 4.30 & 56.96 & 100 & 0 \\ FPGM  & 76.15 & 75.59 & 0.56 & 57.80 & 100 & 0 \\ LeGR  & 76.10 & 75.70 & 0.40 & 58.00 & - & 150 \\ GBN  & 75.88 & 76.19 & -0.31 & 59.46 & 10 & 130 \\ Refill  & 75.84 & 74.46 & 1.38 & 60.00 & 95 & 190 \\ ThiNet  & 72.88 & 72.04 & 0.84 & 63.21 & 110 & 90 \\ GReg-1  & 76.13 & 76.27 & -0.14 & 67.11 & - & 180 \\ MetaPruning  & 76.60 & 76.20 & 0.40 & 73.17 & 32 & 128 \\
**Ours** & 76.15 & **77.01** & **-0.86** & **54.38** & 200 & 0 \\ SSSS  & 76.12 & 75.44 & 0.68 & 84.94 & 100 & 0 \\
**Ours** & 76.15 & **77.53** & **-1.38** & **76.19** & 200 & 0 \\  

Table 4: Results of ResNet-50 on Imagenet. We report the Top-1 accuracy(%) of dense and pruned networks with different remaining FLOPs. The \(E_{pr}\) denotes the pruning epochs. The \(E_{ex}\) denotes the epochs for extra stages (such as pretraining and finetuning). The pruning epochs can be undetermined due to dynamic termination conditions, and corresponding terms are marked as “-”.

\(_{}\), which directly imposes on massive parameters, the term \(_{}\) merely affects the learnable masks, and thus would not drastically deteriorate the soft network while improving the hard one.

The gradient term \(_{} }\) and \(_{}\) directly optimize the parameters of the hard and soft networks, respectively, leading to crucial roles in maintaining the performance. Removing either of the two terms results in an accuracy plummet of above 75%.

### Investigation into gap

According to Section 3, we formulate the pruning problem into two parts: 1) find a superior soft network, _i.e._, the network parameterized by \(\), that satisfies the resource constraint; 2) reducing the gap between the soft network and the practically pruned one, which is referred to as a hard network in this manuscript and parameterized by \(}\). In this section, we first provide possible alternatives to formulate the problem 1 and then compare them with our proposed one on the gap-narrowing capacity to demonstrate the superiority of our method.

The first alternative attempts to directly optimize the hard network on its performance, _i.e._, the straight-through estimators :

\[\ 1:&_{}\, (\,()+\,(,T)),\\ &_{}\,\,(})\,.\] (6)

The second alternative substitutes the soft network with the original one while calculating the gap measure, which conforms to self-distillation-based patterns :

\[\ 2:&_{}\, (\,()+\,(,T)),\\ &_{}\,(\,()+\,( },))\,.\] (7)

Comparative experiments are conducted on CIFAR-100, using ResNet-50 as the baseline. The FLOPs target is set to 15%. The gap metrics, _i.e._, the Jensen-Shannon divergence (\(JS\)) and \(L_{2}\) distance, are averaged over the entire validation set. We measure the gap between the hard network and its direct supervision. For "Alt 1", the gap metrics are calculated between the 0.1 label smoothed  ground truth and the output of the hard network. For "Alt 2", the outputs of the original network and the hard one are utilized to calculate the gap metrics. For "Ours", the outputs of the soft network and the hard one are selected to analyze the gap.

Table 6 shows the comparison results. It can be observed that 1) a lower gap between the hard network and its direct supervision renders the hard network better performance. With the \(JS\) reduced from 2.06 ("Alt 1") to 0.193 ("Ours"), the top-1 accuracy of the hard network increases from 77.13% to 79.77%; 2) Our proposed soft-to-hard formulation achieves the lowest gap on both \(JS\) and \(L_{2}\)

   &  & \)} &  \\   & & & \(\) & \(}\) \\  Alt 1 & 2.06e-00 & 2.74e-03 & - & - & 77.13 \\ Alt 2 & 5.17e-01 & 8.58e-04 & 78.35 & - & 77.78 \\  Ours & **1.93e-01** & **1.60e-04** & - & 80.14 & **79.77** \\  

Table 6: Gap comparison with alternative formulations of the problem 1. The symbols \(\), \(\) and \(}\) represent the top-1 accuracy of the original, soft and hard networks, respectively.

  \(_{}\) & \(_{} }\) & \(_{}\) & \(_{}\) & Top-1 Acc (\%) \\  ✓ & ✓ & ✓ & ✓ & ✓ & 65.55 \\ ✓ & ✓ & x & ✓ & ✓ & **79.77** \\ ✓ & x & x & ✓ & ✓ & 3.95 \\ x & ✓ & x & ✓ & ✓ & 1.73 \\ ✓ & ✓ & x & ✓ & x & 78.30 \\ ✓ & ✓ & x & x & ✓ & 78.77 \\ ✓ & ✓ & x & x & x & 77.69 \\  

Table 5: The influence of different gradient components in the proposed pruning method. The FLOPs target is set to 15% for all experiments.

obtaining a hard network with the highest performance. The two observations imply that the soft-to-hard formulation is a relatively better scheme to narrow the gap, and the lower gap between the hard network and its direct supervision helps improve the hard network's performance.

Can fine-tuning reduce the gap?It might be questioned whether the coupled training of the soft and hard networks is necessary. In Section 3, we entangle the two optimizations in the problem 2 to avoid alternate optimization, which turns out to be an efficient yet effective scheme according to [36; 33]. Without the entanglement, multi-stage optimization is required. A soft network that satisfies the resource constraint is firstly trained solely, and then a fine-tuning stage attempts to narrow the gap between the soft network and the hard one. To explore the effect of fine-tuning, we train a ResNet-50 on CIFAR-100, constraint to 15% FLOPs, and merely optimize the soft network for 500 epochs. With this pretrained soft network, we perform fine-tuning via Algorithm 1 with a 0.1x learning rate and different epochs. The results can be referred to in Table 7. The fine-tuning does reduce the gap to some extent, costing 250 epochs to align the soft network and the hard one (accuracy difference drops from 3.49% to 0.33%). However, compared with our coupled training, the best accuracy of fine-tuning is still 0.28% lower at the cost of an additional 250 epochs. Consequently, the adopted coupled training turns out to be a better choice.

### Architectural superiority

To demonstrate the architectural superiority of our pruned network, we conduct experiments on CIFAR-100, prune a ResNet-50 to 15% FLOPs via our proposed method, and then train it from scratch without bells and whistles. Three networks that are randomly pruned to 15% FLOPs are selected for the comparison. The results are shown in Table 8. The network pruned by our method achieves the highest accuracy, verifying that the pruning mask optimized via Algorithm 1 possesses architectural superiority.

## 5 Conclusion and limitations

In this paper, we reveal and study the long-standing omitted discretization gap problem in differentiable mask pruning. To bridge the discretization gap, we propose a structured differentiable mask pruning framework named Soft-to-Hard Pruner (S2HPruner), using the soft network to distill the hard network and optimize the mask. To further optimize the mask and avoid performance degradation, a decoupled bidirectional KD is proposed to alternatively maintain and block the gradient of weights and the mask. Extensive experiments verify and explain that S2HPruner can obtain high-performance hard networks with extraordinarily low resource constraints.

It is essential to acknowledge the limitations of our method. Therefore, we identify the following limitations: 1) The proposed method merely considers a single dimension, pruning feature channels of a layer. However, a block containing layers might be redundant and could be pruned as a whole, which is regarded as another pruning dimension that we do not consider in this manuscript; 2) We only validate our method on the task of image classification. It is left to explore our method's capability on other tasks, such as detection, segmentation, or natural language processing; 3) We choose FLOPs as the resource indicator, which might not ensure a hardware-friendly architecture. It is promising to consider the inference time on a specific hardware as an indicator. Above all, the identified limitations present opportunities for future research and development, and we remain committed to further exploration and refinement to overcome these challenges.

  Epoch & & 10 & 50 & 100 & 250 & 500 \\   & \(\) & 79.91 & 80.00 & 80.14 & 79.82 & 79.31 \\   & \(}\) & 76.42 & 78.89 & 79.07 & 79.49 & 79.46 \\  

Table 7: The top-1 accuracy of the hard network at different fine-tuning epochs. The top-1 accuracy of the solely trained soft network before fine-tuning is 79.41%. The symbols \(\) and \(}\) represent the top-1 accuracy of the soft and hard networks, respectively.

  Network & Rand 1 & Rand 2 & Rand 3 & Ours \\  Top-1 Acc (\%) & 76.46 & 76.64 & 76.96 & **77.65** \\  

Table 8: The top-1 accuracy of different networks pruned from ResNet-50 with a 15% FLOPs constraint and then trained from scratch without bells and whistles.