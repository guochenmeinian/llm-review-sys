# Symmetry Discovery Beyond Affine Transformations

Ben Shaw

Utah State University

Logan, UT 84322

_ben.shaw@usu.edu_

&Abram Magner

University at Albany

Albany, NY 12222

_amagner@albany.edu_

&Kevin R. Moon

Utah State University

Logan, UT 84322

_kevin.moon@usu.edu_

###### Abstract

Symmetry detection can improve various machine learning tasks. In the context of continuous symmetry detection, current state of the art experiments are limited to detecting affine transformations. Under the manifold assumption, we outline a framework for discovering continuous symmetry in data beyond the affine transformation group. We also provide a similar framework for discovering discrete symmetry. We experimentally compare our method to an existing method known as LieGAN and show that our method is competitive at detecting affine symmetries for large sample sizes and superior than LieGAN for small sample sizes. We also show our method is able to detect continuous symmetries beyond the affine group and is generally more computationally efficient than LieGAN.

## 1 Introduction

Approaching various machine learning tasks with prior knowledge, commonly in the form of symmetry present in datasets and tasks, has been shown to improve performance and/or computational efficiency . While research in symmetry detection in data has enjoyed recent success , the current state of the art leaves room for improvement, particularly with respect to the detection of continuous symmetry . Herein, we seek to address two issues at hand: primarily, the inability of current methods to detect continuous symmetry beyond the affine group; secondly, the apparent increase of difficulty in identifying continuous symmetries when compared with discrete symmetries.

Our methodology is summarized as follows. First, given a dataset, certain quantities of interest, hereafter referred to as _machine learning functions_, are identified, such as an underlying probability distribution which generates the dataset, a classification or regression function, or a dimensionality reduction function. Second, we estimate tangent vector fields which annihilate the machine learning functions. The estimated vector fields themselves are generators of continuous symmetries. Thus estimating the vector fields provides an indirect estimate of the continuous symmetries. Third, we use the estimated vector fields to estimate functions which, in addition to the machine learning functions, are annihilated by the vector fields. The discovered functions define a feature space which is invariant with respect to the continuous symmetries implied by the vector field generators. On this constructed feature space, the outcome of any machine learning task is, by definition, invariant with respect to the symmetries generated by the estimated vector fields, and we show in examples how symmetry-based feature engineering can improve performance on machine learning tasks.

Vector fields have been used to describe symmetry in the context of machine learning before . However, our use of vector fields differs from previous approaches for two reasons. First, we fully exploit the vector fields as objects which operate on machine learning functions directly. Second, we estimate vector fields associated with continuous symmetries beyond affine transformations. **Our main contributions are the following**: 1) We show that our method can detect symmetry in a number of cases previously examined with other methods, with our method offering a number of computational advantages. 2) We also show that our method can be used to detect continuous symmetries beyond the affine group, experimenting with both synthetic and real data.

Related Work and Background

Some background is necessary for our work, beginning with a general description of symmetry. Throughout, we assume a dataset \(=\{x_{1},,x_{r}\}\) with \(x_{i}^{n}\). Various machine learning functions of interest may exist for a given dataset, such as an underlying probability distribution or class probabilities. Generally speaking, the object of symmetry detection in data is to discover functions \(S:^{n}^{n}\) that preserve a machine learning function of interest. That is, for a machine learning function \(f\), \(f S=f\). We consider a continuous symmetry of a particular machine learning function to be a transformation \(S\) which is continuously parameterized, such as a rotation in a plane by an angle \(\), with \(\) being the continuous parameter. We consider a discrete symmetry of a machine learning function to be a transformation which is not continuously parameterized, such as a planar rotation by a fixed angle, or a reflection of a point in the plane about a straight line. We primarily deal with continuous symmetry, though discrete symmetry detection is discussed in Appendix D.3.

Comparatively early work on symmetry detection in machine learning seems to have focused primarily on detecting symmetry in image and video data [8; 9], where symmetries described by straight-line and rotational transformations were discovered. Other work has made strides in symmetry discovery by restricting the types of symmetries being sought. In one case, detection was limited to compact Abelian Lie groups . Another case uses meta-learning to discover any _finite_ symmetry group . Finite groups have also been used in symmetry discovery in representation learning .

Other work has focused on detecting affine transformation symmetries and encoding the discovered symmetries automatically into a model architecture. Three such methods identify Lie algebra generators to describe the symmetries, as we do here. For example, _augerino_ attempts to learn a distribution over augmentations, subsequently training a model with augmented data. The _Lie algebra convolutional network_, which generalizes Convolutional Neural Networks in the presence of affine symmetries, uses infinitesimal generators represented as vector fields to describe the symmetry. SymmetryGAN  has also been used to detect rotational symmetry .

_LieGAN_ appears to represent the current state of the art for continuous symmetry detection. LieGAN is a generative-adversarial network intended to return infinitesimal generators of the continuous symmetry group of a given dataset . LieGAN has been shown to detect continuous affine symmetries, including even transformations from the Lorentz group. It has also been shown to identify discrete symmetries such as rotations by a fixed angle.

None of these related works have attempted to detect continuous symmetries beyond affine transformations. Continuous symmetry detection is more difficult than discrete symmetry detection  since the condition \(f S=f\) must hold for all values of the continuous parameter of \(S\). This is corroborated by the increasingly complex methods used to calculate even simple symmetries such as planar rotations [12; 7; 5]. Some methods introduce discretization, where multiple parameter values are chosen and evaluated. LieGAN does this by generating various transformations from the same infinitesimal generator . Introducing discretization increases the complexity of continuous symmetry detection, though any reasonable symmetry detection method must establish whether a continuous symmetry a finite subgroup of a continuous group has been discovered. We believe a vector field approach addresses the issue of discretization. Our vector field-based method reduces the required model complexity of continuous symmetry detection while offering means to detect symmetries beyond affine transformations.

We now provide some background on vector fields and their associated flows (1-parameter transformations). We refer the reader to literature on the subject for additional information . Suppose that \(X\) is a smooth1 (tangent) vector field on \(^{n}\):

\[X=^{i}_{x^{i}}:=_{i=1}^{n}^{i}_{x^{i}},\] (1)

where \(^{i}:^{n}\) for \(i[1,n]\), and where \(\{x^{i}\}_{i=1}^{n}\) are coordinates on \(^{n}\). \(X\) assigns a tangent vector at each point and can also be viewed as a function on the set of smooth functions which map \(^{n}\) to \(\). E.g. if \(f:^{n}\) is smooth,

\[X(f)=_{i=1}^{n}^{i}}.\] (2)For example, for \(n=2\), if \(f(x,y)=xy\) and \(X=y_{x}\), then \(X(f)=y^{2}\). \(X\) is also a _derivation_ on the set of smooth functions on \(^{n}\): that is, for smooth functions \(f_{1},f_{2}:^{n}\) and \(a_{1},a_{2}\),

\[X(a_{1}f_{1}+a_{2}f_{2})=a_{1}X(f_{1})+a_{2}X(f_{2}), X(f_{1}f_{2})=X(f_{ 1})f_{2}+f_{1}X(f_{2}).\] (3)

These properties are satisfied by derivatives. A flow on \(^{n}\) is a smooth function \(:^{n}^{n}\) which satisfies

\[(0,p)=p,(s,(t,p))=(s+t,p)\] (4)

for all \(s,t\) and for all \(p^{n}\). A flow is a 1-parameter group of transformations. An example of a flow \(:^{2}^{2}\) is

\[(t,(x,y))=(x(t)-y(t),x(t)+y(t)),\] (5)

with \(t\) being the continuous parameter known as the flow parameter. This flow rotates a point \((x,y)\) about the origin by \(t\) radians.

For a given flow \(\), one may define a (unique) vector field \(X\) as given in Equation 2, where each function \(^{i}\) is defined as

\[^{i}=()_{t=0}.\] (6)

Such a vector field is called the infinitesimal generator of the flow \(\). For example, the infinitesimal generator of the flow given in Equation 5 is \(-y_{x}+x_{y}\).

Conversely, given a vector field \(X\) as in Equation 2, one may define a corresponding flow as follows. Consider the following system of differential equations:

\[}{dt}=^{i}, x^{i}(0)=x_{0}^{i}.\] (7)

Suppose that a solution \((t)\) to Equation 7 exists for all \(t\) and for all initial conditions \(_{0}^{n}\). Then the function \(:^{n}^{n}\) given by

\[(t,_{0})=(t)\] (8)

is a flow. The infinitesimal generator corresponding to \(\) is \(X\). For example, to calculate the flow of \(-y_{x}+x_{y}\), we solve

\[=-y,=x, x(0)=x_{0}, y(0)=y_{0}\] (9)

and obtain the flow \((t,(x_{0},y_{0}))\) defined by Equation 5. It is generally easier to obtain the infinitesimal generator of a flow than to obtain the flow of an infinitesimal generator.

Certain vector fields are particularly significant, having flows which correspond to distance-preserving transformations known as isometries. Such vector fields are known as Killing vectors. See Appendix A and D.4 for details.

We can now connect vector fields and flows with symmetry. A smooth function \(f:^{n}\) is said to be \(X\)-invariant if \(X(f)=0\) identically for a smooth vector field \(X\). The function \(f\) is \(\)-invariant if, for all \(t\), \(f=f((t,))\) for a flow \(\). If \(X\) is the infinitesimal generator of \(\), \(f\) is \(\)-invariant if and only if \(f\) is \(X\)-invariant. If the function \(f\) is a machine learning function for a given data set, our strategy is to identify vector fields \(X_{i}\) for which \(f\) is invariant. Each vector field \(X_{i}\) for which \(f\) is invariant is associated, explicitly or implicitly, with a flow \(_{i}\), each of which is a 1-parameter subgroup, the collection of which generate the symmetry group. By identifying continuous symmetries by means of \(X(f)=0\), continuous symmetry detection is made to be of similar complexity to that of discrete symmetry detection, since no continuous parameters are present.

The price of this indirect characterization of continuous symmetry is often the loss of the explicit identification of the symmetry group, leaving one with only the infinitesimal generators and the symmetry group merely implied. However, infinitesimal generators can be used to construct an invariant machine learning model as follows. Suppose that \(f:^{n}\) is a (smooth) function of interest for a given data set, and suppose that \(X\) is a (smooth) vector field for which \(f\) is \(X\)-invariant. With \(X\) fixed, we can consider whether there exist additional functions \(\{h^{i}\}_{i=1}^{m}\) that are functionally independent of each other (meaning one function cannot be written as a function of the others), each of which are \(X\)-invariant. If such functions can be found, we can define a new feature space using the functions \(\{h^{i}\}\). That is, we can transform each data point in \(\) to the point \((h^{1}(x_{i}),h^{2}(x_{i}),,h^{m}(x_{i}))\) for each point \(x_{i}\) expressed in the original coordinates \(\{x^{i}\}\). By definition, the transformed data is invariant with respect to the symmetry group, whether that symmetry group is given explicitly or not.

The flow parameter can often be obtained explicitly as well, written as a function of the original coordinates. It is found by solving \(X()=1\). Where the flow parameters of vector fields can be estimated, our methodology thus inspires symmetry-based feature engineering, with the new features consisting of flow parameters and invariant features.

For example, consider a data set consisting of points \(=\{x_{i}\}_{i=1}^{r}\) in \(^{3}\), where the features of each data point are given in Cartesian coordinates \((x,y,z)\). Now suppose that each data point \(x_{i}\) satisfies the equation \(f=0\), where \(f=x^{2}+y^{2}-z\). The function \(f\) can be considered to be a machine learning function for this data set, since each point in the data set lies on the level set \(f=0\). We note that \(X=-y_{x}+x_{y}\) is a vector field for which \(f\) is \(X\)-invariant. With \(h^{1}=x^{2}+y^{2}\) and \(h^{2}=z\), the functions \(h^{1}\) an \(h^{2}\) are \(X\)-invariant. We then construct the data set \(}=\{(h^{1}(x_{i}),h^{2}(x_{i}))_{i=1}^{r}\). Any machine learning task performed in this feature space will be, by definition, invariant to the flow of \(X\), which flow defines the symmetry group of the original data set \(\).

By estimating vector fields which satisfy \(X(f)=0\), we find seemingly unwanted freedom. This is due to the fact that, assuming \(X(f)=0\), \(hX(f)=0\) for a nonzero function \(h\): the question, then, is which of the two symmetries-the flow of \(X\) or the flow of \(hX\)-is "correct." By definition, both are symmetries, so that the function \(f\) is symmetric with respect to an apparently large class of symmetries. However, a function is \(X\)-invariant if and only if it is also invariant with respect to \(hX\), so that a set of \(X\)-invariant functions is also a set of invariant functions of \(hX\). Thus, where a vector field \(X\) can be used to generate \(X\)-invariant functions, the result is a feature space which is also invariant with respect to \(hX\). This point is further expounded upon in Appendix E.

## 3 Methods

Our method assumes that a given dataset consists of points residing on a differentiable manifold \(M\), and that a machine learning function \(f\) is a smooth function on \(M\). We also assume that the symmetry group we seek is a Lie group generated by 1-parameter transformations known as flows, the infinitesimal generators of which are vector fields which reside in the tangent bundle of \(M\).

There are three main stages that define our methodology. The first stage, described in Section 3.1, is the estimation of machine learning functions specific to a dataset. The second and third steps are described in Section 3.2. In the second step, vector fields which annihilate the machine learning functions are estimated. Finally, the vector fields are used to define a feature space which is invariant with respect to the implied flows of the vector fields as described in Section 2. A discussion of known limitations of our methods is included in Appendix C.

### Estimating Machine Learning Functions

Probability density functions appear in both supervised and unsupervised problems. The output of a logistic regression model or, similarly, a neural network model that uses the softmax function, can be interpreted as estimating the probability that a given point belongs to a particular class. In fact, the probabalsistic outputs of many other machine learning models can be analyzed for symmetry, provided that the probability function is differentiable. Invariant features of the symmetries of these functions define level sets upon which the probability is constant. If the flow of an infinitesimal generator can be given, a point in the dataset can be transformed to another point-not generally in the given dataset-and the model should predict the same probability for the transformed point as it would for the original point. Applications include supervised feature engineering and model validation, where predictions on generated points can be used to assess a model's ability to generalize.

Particularly in the unsupervised case, density estimation can be challenging, complicating methods that require their estimation. We propose to address this potential shortcoming of density-based symmetry detection by a method we call _level set estimation_. Level set estimation seeks to learn a function \(F:^{n}^{k}\) (\(k<n\)) such that \(F(x_{i})=\) for each \(x_{i}\). Any embedded submanifold can be, at least locally, expressed as a level set of a smooth function , and so level set estimation operates on the assumption that the data points belonging to a given dataset lie on an embeddedsubmanifold. If such a function can be estimated for a given data set, one can estimate symmetries of the component functions \(f_{i}\) of \(F\).

Level set estimation can be done as follows, although we note that further development of level set estimation may lead to more effective and/or efficient implementations. First, we construct a function \(f\) as a linear combination of pre-determined features, as with a generalized additive model or polynomial regression, with model coefficients \(\{a_{i}\}_{i=1}^{m}\). An \(m\)-component column vector \(w\) is constructed using the coefficients \(a_{i}\). Then, for each point in the dataset, we set the target to \(0\), so that a function \(f\) is approximated by estimating

\[Bw=,\] (10)

where \(B\) is the feature matrix consisting of \(m\) columns and \(N\) rows, where \(N\) is the number of points in the dataset, and where \(\) is the zero matrix. OLS regression motivates the way in which we solve Equation (10), though we note that a solution \(w_{0}\) is equivalent to a scaling \(cw_{0}\) for some constant \(c\). We therefore turn to constrained regression, choosing, by default, the constraint \(_{i=1}^{m}a_{i}^{2}=1\). Additionally, we note that any loss function can be used, with predictions and targets determined by the left- and right-hand sides of Equation (10), respectively.

We can estimate more than a single component function \(f\) by appending additional columns to \(w\), in which case we constrain the columns of \(w\) to be orthonormal. As appending columns to \(w\) is likely to increase any cost function used to estimate \(w\), we use an "elbow curve" to determine when the number of columns of \(w\) is sufficiently high. That is, in the presence of a sudden comparatively large increase in the cost function as the number of columns of \(w\) is increased, the number of columns of \(w\) is chosen as the value immediately before the large increase is encountered. We handle the constrained optimization problem of (10) using existing constrained optimization algorithms .

The freedom of expression in the components of the level sets is, unfortunately, not fully contained by constrained optimization. When certain functions are chosen as part of the level set estimation, such as polynomials, this scaling issue can extend to multiplication by functions. For example, consider a data set in \(^{3}\) known a priori to be approximately described by the function \(F=\), where the components of \(f_{1}\) and \(f_{2}\) of \(F\) are given as

\[f_{1}=x^{2}+y^{2}-1, f_{2}=z-1.\] (11)

Such a level set could also, at first glance, be described by the following equations:

\[x^{2}+y^{2}-1=0, z-1=0, x(z-1)=0.\] (12)

From a mathematical point of view, the third equation is degenerate and does not change the intrinsic dimension of the manifold characterized by this level set, owing to the fact that the rank of this function when \(x=0\) is 2. However, within the context of learning level sets from data, the extrinsic dimension may be large, and it may not be practical to examine the components of the learned level set. Furthermore, key points for determining the rank of the corresponding function may not be present in the data.

A potential workaround for this problem is to incrementally increase the allowed degree of the learned polynomial functions in Equation 12. For the example considered in Equation 11, we can first optimize over the space of degree 1 polynomials, ideally discovering a scaling of the function \(f_{2}\). Next, when searching for quadratic components, we can extend the columns of \(w\) which correspond to any allowed terms of the form \(h(x,y,z)f_{2}\).2 With constrained optimization, the column space of \(w\) can then be expressed as an orthonormal basis. The optimization routine can then continue, with the final level set model for \(w\) not including the artificially-created columns. Alternatively, if one is interested primarily in at most quadratic component functions, one can first optimize using affine functions, then project the data onto the lower-dimensional hyperplane defined by the affine components, subsequently searching for quadratic terms on the reduced space where no additional affine components should be present. We provide experiments which apply these concepts in practice.

One may well ask whether level set estimation can be accomplished using a neural network rather than constrained polynomial regression. Besides the problem of uniqueness just spoken of, a neural network approach would likely require plentiful off-manifold examples so as not to learn a function which is identically zero. A neural network approach to level set estimation is not explored in this paper, though may be explored in the future.

The last type of machine learning function we consider is a metric tensor. Given a (pseudo-) Riemannian manifold with a metric tensor, we can calculate infinitesimal isometries of the associated (pseudo-) Riemannian manifold. We provide a means for approximating the metric tensor in certain cases in A.

### Estimating the Infinitesimal Generators and the associated Invariant Feature Space

With scalar-valued machine learning functions \(f_{i}\), we construct a vector-valued function \(F\) whose components are defined by the functions \(f_{i}\). We can then obtain vector fields which annihilate the components of \(F\) by calculating nullspace vectors of \(J\), the Jacobian matrix of \(F\).3 More vector fields may annihilate the functions than can be identified by the nullspace of \(J\), though our method is inspired by this idea. First, we construct a vector field whose components are an arbitrary linear combination of pre-determined features, as in a general additive model, though we primarily use polynomial features herein. The coefficients of this linear combination reside in a matrix \(W\), with the columns of \(W\) corresponding to the coefficients for a single vector field. We then estimate \(W\) in the following equation:

\[MW=,\] (13)

where \(M=M(,B)\) is the _Extended Feature Matrix_ computed using the array \(\) of Jacobian matrices at each point, \(B\) is the feature matrix, and \(\) is the zero matrix. The matrix \(M\) is computed via Algorithm 1.

``` \(m\) number of features \(n\) dimension of space \(N\) number of points in the dataset \( Jacobian(F)(x_{i})\)\(\) 3-d array of Jacobian matrices at each point for i in range(\(N\)) do \(row_{0} padded(B_{i})\)\(\) m(n-1) 0's are appended to the \(i^{th}\) row of \(B\) for j in range(1,m-1) do \(row_{j} roll(row_{0},j m)\)\(\) Each subsequent row is displaced \(m\) entries to the right. endfor \(b_{i} Matrix([row_{0}, row_{m-1}])\) \(mat_{i}_{i}b_{i}\)\(\) Multiply the Jacobian of \(F\) at \(x_{i}\) by the matrix \(b_{i}\) endfor \(M StackVertical([mat_{0}, mat_{N-1}])\)\(\) This matrix has size \(nN nm\). ```

**Algorithm 1** Constructing the Extended Feature Matrix \(M\)

As with Equation (10), a solution to Equation (13) is estimated using a constrained optimization of a selected loss function, for which we turn to _McTorch_. Using this pytorch-compatible library, one can choose from a variety of supported loss functions and (manifold) optimization algorithms. As with level set estimation, we sequentially increase the number of columns of \(W\) until an "elbow" in cost function values is reached.

However, we find the same unwanted freedom as with level set estimation, since \(X(f)=0\) implies that for a smooth function \(h\), \(hX(f)=0\). Since we are assuming that the components of \(X\) are polynomial functions, it is possible that a such a pair \(X\) and \(hX\) may exist in the search space. The means of dealing with this unwanted freedom can depend on the particular experiment: if there are no affine symmetries, for example, vector fields with components of degree 2 can be approximated without regard to this issue.

To define a model which is invariant with respect to the flows of pre-computed vector fields \(X_{i}\), we seek features \(h^{j}\) (separate of the functions \(f_{i}\)) such that \(X_{i}(h^{j})=0\). This we do by estimating \(v\) in the following equation:

\[M_{2}v=,\] (14)

where \(M_{2}=M_{2}(,B,)\) is the _Invariant Function Extended Feature Matrix_ computed via Algorithm 2, \(\) is the feature matrix of \(W\) (already estimated), and \(B\) is the feature matrix of the unknown function.

This method assumes that the invariant functions can be expressed in terms of prescribed features, which, in our case, are typically polynomial functions. The quantity \(v\) can be estimated from Equation 14 in much the same way that we estimate other quantities, in that we constrain \(v\) to be orthogonal, then optimize a selected loss function using _McTorch_. We should note that while this method may produce invariant features, they are not guaranteed to be functionally independent. For example, in our polynomial feature space, the vectors corresponding to \(x\) and \(x^{2}\) are orthogonal, though the functions themselves are not functionally independent.

#### 3.2.1 Comparison of Discovered Symmetries to Ground-Truth Symmetries

To evaluate symmetry discovery methods in general, it is important to quantify the ability of a given method to recover ground truth symmetries in controlled experiments. For a ground truth vector field \(X=_{i=1}^{N}f_{i}_{x^{i}}\) and an estimated vector field \(=_{i=1}^{N}_{i}_{x^{i}}\), our similarity score between them is defined to be

\[(X,)=_{i=1}^{N},_{i }|}{||f_{i}||||_{i}||},\] (15)

where

\[ f_{i},_{i}=_{}f_{i}_{i}d, ||f_{i}||=,f_{i}},\]

and with \(\) being defined by the range of a given dataset-herein, we take the full range of the dataset. We note that under a change of coordinates induced by a diffeomorphism, the similarity is invariant, owing to the Jacobian which would be present in the transformed definite integrals. Further refinement of this similarity score is left as future work.

## 4 Experiments

Our first experiment will compare our method to LieGAN in an effort to discover an affine symmetry. Our next experiment will demonstrate our method's ability to identify symmetries which are not affine. The third experiment demonstrates that symmetry can occur in real data, and that our method can detect approximate symmetry in real data. Our last main experiment deals with pre-determined models which are not polynomials-in particular, when the ground truth symmetry is not expressible in terms of polynomial coefficient functions. Additional experiments appear in Appendix D: some highlights are symmetry of a model-induced probability distribution, discrete symmetry estimation, and estimation of infinitesimal isometries.

### Affine Symmetry Detection Comparison

Our first experiment compares the performance of our method to LieGAN, which is the current state of the art in symmetry detection. We use simulated data where the "true" symmetries are known. Our three datasets for this experiment consist of \(N\) random samples generated from a 2-dimensional normal distribution with mean \((1,1)\) and covariance matrix \(4&0\\ 0&1\), where \(N\{200,2000,20,000\}\). In each case, target function values are generated as \(f=(x-1)^{2}+4(y-1)^{2}\) for each point \((x,y)\) in the dataset. The function exhibits symmetry: \(X(f)=0\), with \(X=-(4y-4)_{x}+(x-1)_{y}\) (or a scaling thereof). In the methodology employed by LieGAN, the ground truth infinitesimal generator is expressible in terms of a \(3 3\) matrix, with generated transformations acting on the \(z=1\) plane in three dimensions. Thus, in the case of LieGAN, the data is generated with mean \((1,1,1)\) and covariance given with the upper \(2 2\) matrix as before, with all other values besides the bottom right value being \(0\), which bottom right value is \(0.0001\) so as to approximately lie on the \(z=1\) plane. Our own method uses the \(2\)-dimensional data.

Our method first employs polynomial regression to approximate the function \(f\) from the data. Then, with a model using affine coefficients for the estimated vector field, we estimate \(W\) in Equation \(()\) using the \(L_{1}\) loss function and the Riemannian Adagrad optimization algorithm  with learning rate \(0.01\) for \(5000\) epochs.

We compare the results of our method to the results of LieGAN using two metrics, namely similarity to the ground truth defined by Equation (15) and computation time. The results are shown in Table 1 and use 10 trials for each \(N\). These results suggest that if a sufficient amount of data is given, LieGAN is able to correctly identify the ground truth symmetry. However, our method exhibits a clear advantage for \(N=200\) while offering comparable similarity scores to LieGAN for larger sample sizes. Perhaps the most striking feature of Table 1, however, is the dramatic difference in time needed to correctly identify the symmetry. Our method offers a clear advantage in terms of computation time as the number of samples increases.

### Non-Affine Symmetry Detection Comparison

We now demonstrate the ability of our method to discover symmetries which are not affine. We generate 2000 data points from a normal distribution with mean \((0,0)\) and covariance matrix \([4&0\\ 0&4]\). For each data point, we generate target values according to \(f=x^{3}-y^{2}\). A ground truth vector field which annihilates this function is \(2y_{x}+3x^{2}_{y}\).

We first use polynomial regression to approximate \(f=x^{3}-y^{2}\). Then, we estimate \(W\) in Equation (13) using the \(L_{1}\) loss function, selecting the Riemannian Adagrad optimization algorithm  with a learning rate of \(0.1\), training for \(5000\) epochs. Our method then obtains a similarity score of \(0.9983 0.0006\), having aggregated across 10 independent trials. Meanwhile, LieGAN obtains a similarity score of \(0.0340 0.0142\) on 10 independent trials, being unable to discover symmetries of this complexity.

### Symmetry Detection using Realistic Data

Our next experiment uses real data that is publicly available, which dataset we refer to as the "Bear Lake Weather" dataset . The dataset gives daily weather attributes. It contains \(14,610\) entries with 81 numeric attributes including the daily elevation level of the lake. The dataset contains precisely 40 years' worth of data from October of 1981 through September of 2021.

We believe an understanding of the behavior of the weather in the past is relevant to this problem. Therefore, we first construct time series of length in dimensions by means of a sliding window of length days: the first time series is constructed using the first 1461 days (the number of days in four years). The next time series is constructed using the second day through day 1462, and so forth.

After converting the raw data to time series data, we apply a transformation on the data meant to extract time-relevant features of the data known as the Multirocket transform . We select 168 kernels in our transform. The Multirocket transform transforms the data from \(13,149\) time series of length \(1461\) in \(81\) variables to tabular data: \(13,149\) entries in 1344 variables.

For such high-dimensional data, we turn to PHATE, a state of the art data visualization method . Using PHATE, we reduce the dimension to \(2\), so that our new dataset has \(13,149\) entries in 2 variables. The resulting data appears to approximately lie on a circular shape and is shown in Figure 1.

   \(N\) & LieGAN similarity & Our similarity & LieGAN time (s) & Our time (s) \\ 
200 & \(0.7636 0.2862\) & \(1.0000 5.6 10^{-7}\) & \(2.1717 0.0481\) & \(2.1778 0.0125\) \\
2000 & \(0.9990 0.0014\) & \(1.0000 1.7 10^{-7}\) & \(18.1289 0.2528\) & \(2.6412 0.0208\) \\
20000 & \(1.0000 1.5 10^{-5}\) & \(1.0000 3.2 10^{-8}\) & \(195.5748 2.6080\) & \(5.6177 0.0585\) \\   

Table 1: Affine Symmetry Detection Comparison on a Gaussian DistributionFigure 1 suggests that the data is largely periodic. In fact, further experimentation reveals that the points for a given calendar year correspond to a traversal around the circular-like shape. Thus, for the analysis of non-seasonal weather patterns, it may be of use to examine features of this embedded dataset which are invariant under the periodic transformation, the approximate symmetry.

Indeed, our method reveals an approximate invariant function given by

\[0.33592x^{2}+0.94189y^{2}-2.9743 10^{-4}=0.\]

This result was obtained using level set estimation, assuming a function expressible as \(ax^{2}+by^{2}+c\), using the Mean Square Error loss function with Equation (10), optimized using Riemannian stochastic gradient descent with a learning rate of \(0.001\) and trained for \(5000\) epochs. This experiment shows that symmetry can occur in real data, and that our method can detect symmetry and estimate invariant functions for real data.

### Symmetry Detection with Non-Polynomial Ground Truth

This experiment deals with a case in which the ground truths are not strictly polynomial functions. We generate \(2048\) numbers \(x_{i}\) and \(2048\) numbers \(y_{j}\) each from \(U(0,2)\). Next, for each pair \((x_{i},y_{i})\), we obtain \(z_{i}\) by means of \(z_{i}=(x_{i})-(y_{i})\), so that a ground-truth level set description of the data is given as \(z-(x)+(y)=0\).

We first apply our level set estimation method to estimate this level set. We optimize the coefficients of the model

\[a_{0}+a_{1}x+a_{2}y+a_{3}z+a_{4}(x)+a_{5}(y)+a_{6}(z)+a_{7}(x) +a_{8}(y)+a_{9}(z)=0\]

subject to \(_{i=0}^{9}a_{i}^{2}=1\). In light of Equation 10, the matrix \(B\) has a row for each of the 2048 tuples \((x_{i},y_{i},z_{i})\), and 10 columns, which columns correspond to the 10 different feature functions in our pre-determined model. The vector \(w\) contains all 10 parameters \(\{a_{i}\}_{i=0}^{9}\).

Using the \(L_{1}\) loss function and the Riemannian Adagrad optimization algorithm  with learning rate \(0.01\), our estimated level set description is

\[-0.57737z-0.57713(y)+0.57756(x)=0,\]

which is approximately equivalent to the ground truth answer up to a scaling factor.

Having demonstrated that our method easily extends to non-polynomial pre-determined features, we now wish to examine a case in which a polynomial pre-determined model of symmetry does not contain the ground truth vector field. For this, we use the same dataset, albeit we define \(f_{i}=z_{i}\), so

Figure 1: The ROCKET-PHATE embedded Bear Lake Weather dataset, colored by invariant function value.

that we are approximating symmetries of \(f(x,y)=(x)-(y)\). A ground truth vector field which characterizes the symmetry of \(f\) is given as

\[X=(y)_{x}-(x)_{y}.\]

Applying our method with a pre-determined model of degree 2 polynomial coefficients gives an estimated (using Equation 13) vector field \(\) of

\[=(0.7024-0.1874x-0.2203y+0.0121x^{2}+0.0242xy+0.0133y^{2}) _{x}\]

\[+(-0.5783+0.2665x+0.1236y-0.0311x^{2}-0.0150xy-0.0097y^{2}) _{y}.\]

This result was obtained using the \(L_{1}\) loss function, the Riemannian Adagrad optimizer  with learning rate \(0.1\), training for \(5000\) epochs. We also note that \((X,)=0.62\), using the similarity score defined in Equation (15). This demonstrates that our method can be applied even when a ground truth vector field is not covered by the vector field pre-determined model.

## 5 Conclusion

While current state of the art experiments focus, in the context of continuous symmetry detection, on affine symmetries, we have outlined a method to estimate symmetries including and beyond the affine symmetries using vector fields. Our method relies on estimating machine learning functions such as a probability distribution, continuous targets for regression, or functions which characterize an embedded submanifold (level set estimation). Our method also requires, in the case of discrete symmetry detection, that the transformation (sub)-group be parameterized, so that the "best-fit" parameters can be obtained by optimization. In the case of continuous symmetry detection, vector fields are constructed as arbitrary linear combinations of a chosen basis of vector fields, and the parameters that are the coefficients of the linear combination are optimized. In both discrete and continuous symmetry detection, the parameter space is often constrained. Herein, we have chosen to express the coefficients of our vector fields in terms of polynomial functions.

When searching for affine symmetries in low dimensions, our method is appealing when compared with current state of the art methods. This is due primarily to the relative ease of implementation. Additionally, when compared with the state of the art, our method appears to offer a computational advantage while at least competing and often outperforming in terms of accuracy, especially for more complex symmetries than affine ones.

Our infinitesimal generators are a step removed from the symmetry group than what is typical of symmetry detection methods. We have proposed building invariant architectures based on the identification of features which are invariant with respect to the discovered vector fields, and we have demonstrated the estimation of these invariant features. This results in an invariant model, since the information fed to the model is approximately invariant with respect to the symmetry group.

We have also discussed potential novel applications of symmetry discovery, including model validation and symmetry-based feature engineering. Further potential applications are discussed in Appendices A and D.4. Our novel approach for detecting symmetry in data, the computational advantages of our method, and the demonstrated and potential applications of our method make our method of interest to the machine learning community.

Future work includes the following. First, we plan to extend our method to express the vector fields in greater generality beyond polynomial functions. A natural extension of this would be to express the invariant features in greater generality as well. Additionally, as discussed in the appendices, the detection of isometries relies on the ability to compute Killing vectors, for which, in the case of non-Euclidean geometries, no machine-learning compatible methods have been apparently implemented. Also discussed in the appendices is the notion of metric tensor estimation, which we leave, in large part, as work for the future.