# Harnessing Hard Mixed Samples with Decoupled Regularizer

Zicheng Liu\({}^{1,2,}\)1 &Siyuan Li\({}^{1,2,}\)1 &Ge Wang\({}^{1,2}\) &Chen Tan\({}^{1,2}\) &Lirong Wu\({}^{1,2}\) &Stan Z. Li\({}^{2,}\)2 &AI Lab, Research Center for Industries of the Future, Hangzhou, China; \({}^{1}\)Zhejiang University; \({}^{2}\)Westlake University; \(\{\)liuzicheng; lisiyuan; wangge; tanchen; lirongwu; stan.zq.li\(\}\)

###### Abstract

Mixup is an efficient data augmentation approach that improves the generalization of neural networks by smoothing the decision boundary with mixed data. Recently, _dynamic_ mixup methods have improved previous _static_ policies effectively (_e.g._, linear interpolation) by maximizing target-related salient regions in mixed samples, but excessive additional time costs are not acceptable. These additional computational overheads mainly come from optimizing the mixed samples according to the mixed labels. However, we found that the extra optimizing step may be redundant because label-mismatched mixed samples are informative hard mixed samples for deep models to localize discriminative features. In this paper, we thus are not trying to propose a more complicated _dynamic_ mixup policy but rather an efficient mixup objective function with a decoupled regularizer named Decoupled Mixup (DM). The primary effect is that DM can adaptively utilize those hard mixed samples to mine discriminative features without losing the original smoothness of mixup. As a result, DM enables _static_ mixup methods to achieve comparable or even exceed the performance of _dynamic_ methods without any extra computation. This also leads to an interesting objective design problem for mixup training that we need to focus on both smoothing the decision boundaries and identifying discriminative features. Extensive experiments on supervised and semi-supervised learning benchmarks across seven datasets validate the effectiveness of DM as a plug-and-play module. Source code and models are available at https://github.com/Westlake-AI/openmixup.

## 1 Introduction

Deep Learning has become the bedrock of modern AI for many tasks in machine learning  such as computer vision , natural language processing . Using a large number of learnable parameters, deep neural networks (DNNs) can recognize subtle dependencies in large training datasets to be later leveraged to perform accurate predictions on unseen data. However, models might overfit the training set without constraints or enough data . To this

Figure 1: visualization of hard mixed sample mining by class activation mapping (CAM)  of ResNet-50 on ImageNet. From left to right, CAM of top-2 predicted classes using mixup cross-entropy (MCE) and decoupled mixup (DM) loss.

end, regularization techniques have been deployed to improve generalization , which can be categorized into data-independent or data-dependent ones . Some data-independent strategies, for example, constrain the model by punishing the parameters' norms, such as weight decay . Among data-dependent strategies, data augmentations  are widely used. The augmentation policies often rely on particular domain knowledge  in different fields.

Mixup , a data-dependent augmentation technique, is proposed to generate virtual samples by a linear combination of data pairs and the corresponding labels with the mixing ratio \(\). Recently, a line of optimizable mixup methods are proposed to improve mixing policies to generate object-aware virtual samples by optimizing discriminative regions in the data space to match the corresponding labels  (referred to as _dynamic_ methods). However, although the _dynamic_ approach brings some performance gain, the extra computational overhead degrades the efficiency of mixup augmentation significantly. Specifically, the most computation of _dynamic_ methods is spent on optimizing label-mismatched samples, but the question of why these label-mismatched samples should be avoided during the mixup training has rarely been analyzed. In this paper, we find these mismatched samples are completely underutilized by _static_ mixup methods, and the problem lies in the loss function, mixed cross-entropy loss (MCE). Therefore, we argue that these mismatched samples are not only not _static_ mixup disadvantages but also hard mixed samples full of discriminative information. Taking CutMix  as an example, two types of hard mixed samples are shown on the _right_ of Figure 2. Since MCE loss forces the model's predictions to be consistent with the soft label distribution, _i.e.,_ the model cannot give high-confidence predictions for the relevant classes even if the feature is salient in hard mixed samples, we can say that these hard samples are not fully leveraged.

From this perspective, we expect the model to be able to mine these hard samples, _i.e.,_ to give confident predictions according to salient features for localizing discriminative characteristics, even if the proportion of features is small. Motivated by this finding, we introduce simple yet effective Decoupled Mixup (DM) loss, a mixup objective function for explicitly leveraging the hard samples during the mixup training. Based on the standard mixed cross-entropy (MCE) loss, an extra decoupled regularizer term is introduced to enhance the ability to mine underlying discriminative statistics in the mixed sample by independently computing the predicted probabilities of each mixed class. Figure 1 shows the proposed DM loss can empower the _static_ mixup methods to explore more discriminative features. Extensive experiments demonstrate that DM achieves data-efficiency training on supervised and semi-supervised learning benchmarks. Our contributions are summarized below:

* Unlike those dynamic mixup policies that design complicated mixing policies, we propose DM, a mixup objective function of mining discriminative features adaptively.
* Our work contributes more broadly to understanding mixup training: it is essential to focus not only on the smoothness by regression of the mixed labels but also on discrimination by encouraging the model to give reliable and confident predictions.
* Not only in supervised learning but the proposed DM can also be easily generalized to semi-supervised learning with a minor modification. By leveraging the unlabeled data, it can reduce the conformation bias and significantly improve performance.
* Comprehensive experiments on various tasks verify the effectiveness of DM, _e.g._, DM-based _static_ mixup policies achieve a comparable or even better performance than _dynamic_ methods without the extra computation.

Figure 2: Illustration of the two types of hard mixed samples in CutMix with ‘Squirrel’ and ‘Panda’ as an example. Hard mixed samples indicate that the mixed sample contains salient features of a class, but the value of the corresponding label is small. MCE loss fails to leverage these samples.

Related Work

Mixup Augmentation.As data-dependent augmentation techniques, mixup methods generate new samples by mixing samples and corresponding labels with well-designed mixing policies [77; 57; 69; 64]. The pioneering mixing method is Mixup , whose mixed samples are generated by linear interpolation between pairs of samples. ManifoldMix variants [57; 14] extend Mixup to the latent space of DNNs. After that, cut-based methods  are proposed to improve the mixup for localizing important features, especially in the vision field. Many researchers explore using nonlinear or optimizable sample mixup policies to generate more reliable mixed samples according to mixed labels, such as PuzzleMix variants [23; 22; 45], SaliencyMix variants [56; 60], AutoMix variants [38; 31], and SuperMix . Concurrently, recent works try to generate more accurate mixed labels with saliency information  or attention maps [5; 9; 7] for Transformer architectures, which require prior pre-trained knowledge or attention information. On the contrary, the proposed decoupled mixup is a pluggable learning objective for mixup augmentations. Moreover, mixup methods extend to more than two elements [22; 11] and regression tasks . Some researchers also utilize mixup augmentations to enhance contrastive learning [8; 21; 28; 50; 31] or masked image modeling [33; 6] to learn general representation in a self-supervised manner.

Semi-supervised Learning and Transfer Learning.Pseudo-Labeling  is a popular semi-supervised learning (SSL) method that utilizes artificial labels converted from teacher model predictions. MixMatch  and ReMixMatch  apply mixup on labeled and unlabeled data to enhance the diversity of the dataset. More accurate pseudo-labeling relies on data augmentation techniques to introduce consistency regularization, _e.g._, UDA  and FixMatch  employ weak and strong augmentations to improve the consistency. Furthermore, CoMatch  unifies consistency regularization, entropy minimization, and graph-based contrastive learning to mitigate confirmation bias. Recently proposed works [62; 4] that improve FixMatch by designing more accurate confidence-based pseudo-label selection strategies, _e.g._, FlexMatch  applying curriculum learning for updating confidence threshold dynamically and class-wisely. More recently, SemiReward  proposes a reward model to filter out accurate pseudo labels with reward scores. Fine-tuning a pre-trained model on labeled datasets is a widely adopted form of transfer learning (TL) in various applications. Previously, [13; 44] show that transferring pre-trained AlexNet features to downstream tasks outperforms hand-crafted features. Recent works mainly focus on better exploiting the discriminative knowledge of pre-trained models from different perspectives. L2-SP  promotes the similarity of the final solution with pre-trained weights by a simple L2 penalty. DELTA  constrains the model by a subset of pre-trained feature maps selected by channel-wise attention. BSS  avoids negative transfer by penalizing smaller singular values. More recently, Self-Tuning variants [67; 54] combined contrastive learning with TL to tackle confirmation bias and model shift issues in a one-stage framework.

## 3 Decoupled Mixup

### Preliminary

Mixed Cross-Entropy Underutilizes MixupLet us define \(y^{C}\) as the ground-truth label with \(C\) categories. For labeled data point \(x^{W H C}\) whose embedded representation \(z\) is obtained from the model \(M\) and the predicted probability \(p\) can be calculated through a Softmax function \(p=(z)\). Given the mixing ratio \(\) and \(\)-related mixup mask \(H^{W H}\), the mixed sample \((x_{(a,b)},y_{(a,b)})\) can be generated as \(x_{(a,b)}=H x_{a}+(1-H) x_{b}\), and \(y_{(a,b)}= y_{a}+(1-)y_{b}\), where \(\) denotes element-wise product, \((x_{a},y_{a})\) and \((x_{b},y_{b})\) are sampled from a labeled dataset \(L=\{(x_{a},y_{a})\}_{a=1}^{n_{L}}\). Note that superscripts denote the index; subscripts indicate the type of data, _e.g._, \(x_{(a,b)}\) represents a mixed sample generated from \(x_{a}\) and \(x_{b}\); \(y^{i}\) indicates the label value on \(i\)-th position. Since the mixup labels are obtained by somehow \(\)-based interpolation, the standard CE loss weighted by \(\), \(_{CE}=y_{(a,b)}^{T}(z_{(a,b)})\), is typically used as the objective in the mixup training:

\[_{MCE}=-_{i=1}^{C}(y_{a}^{i}=1) p_ {(a,b)}^{i}+(1-)(y_{b}^{i}=1) p_{(a,b)}^{i}.\] (1)

where \(()\{0,1\}\) is an indicator function that values one if and only if the input condition holds. Noticeably, these two items of Equation 1 are classifying \(y_{a}\) and \(y_{b}\) while keeping the linear consistency with mixing coefficient \(\). As a result, DNNs with this mixup consistency prefer relatively less confident results in high-entropy behaviour  and longer training time in practice. **The main reason is that in addition to \(\) constraint, the competing relationships defined by Softmax in \(_{MCE}\) are the main cause of the confidence drop, which is more obvious when dealing with hard mixed samples.** Precisely, the competition between the mixed class \(a\) and \(b\) in Equation 1 can severely affect the prediction of a single class; that is, interference from other classes prevents the model from focusing its attention. This typically causes the model to be insensitive to the salient features of the target and thus undermines model performance, as shown in Figure 1. Although the _dynamic_ mixup alleviates this problem, the extra time overhead is unavoidable if only focusing on mixing policies on the data level. Therefore, the key challenge is to design an ideal objective function for mixup training that maintains the smoothness of the mixup and can simultaneously explore the discriminative features without any computation costs.

### Decoupled Regularizer

To achieve the above goal, we first dive into the \(_{MCE}\) and propose the efficient decoupled mixup.

**Proposition 1**.: _Assuming \(x_{(a,b)}\) is generated from two different classes, minimizing \(_{MCE}\) is equivalent to regress corresponding \(\) in the gradient:_

\[(_{z_{(a,b)}}_{MCE})^{i}=-+^{i})}{_{c}(z_{(a,b)}^{c})},&i=a\\ -(1-)+^{i})}{_{c}(z_{(a,b)}^{c})},&i=b\\ ^{i})}{_{c}(z_{(a,b)}^{c})},&i a,b\] (2)

Softmax Degrades Confidence.As we can see from Proposition 1, the predicted probability of \(x_{(a,b)}\) will be consistent with \(\), and the probability is computed from the Softmax directly. The Softmax forces the sum of predictions to one (winner takes all), which is undesirable in mixup classification, especially when there are multiple and non-salient targets in mixed samples, _e.g.,_ hard mixed samples, as shown in Figure 2. The standard Softmax in \(_{MCE}\) deliberately suppresses confidence and produces high-entropy predictions by coupling all classes. As a consequence, \(_{MCE}\) makes many static mixup methods require longer epochs than vanilla training to achieve the desired results [57; 73]. Based on previous analysis, a novel mixup objective, decoupled mixup (DM), is raised to remove the Coupler and thus utilize the hard mixed samples adaptively, finally improving the performance of mixup methods. Specifically, for mixed data points \(z_{(a,b)}\) generated from a random pair in labelled dataset \(L\), an encoded mixed representation \(z_{(a,b)}=f_{}(x_{(a,b)})\) is generated by a feature extractor \(f_{}\). A mixed categorical probability of \(i\)-th class is attained:

\[(z_{(a,b)})^{i}=^{i})}{_{c}(z_{(a,b)}^{c})}.\] (3)

Decoupled Softmax.where \(()\) is standard Softmax. Equation 3 shows how the mixed probabilities are computed for a mixed sample. The competition between \(a\) and \(b\) is the main reason that results in low confidence of the model, _i.e.,_ the sum of semantic information of hard mixed samples are larger than "1" defined by Softmax. Therefore, we propose to simply remove the competitor class in Equation 3 to achieve decoupled Softmax. The score on \(i\)-th class is not affected by the \(j\)-th class:

\[(z_{(a,b)})^{i,j}=^{j})}{_{(;b)}^{j}+_{c j}(z_{(a,b)}^{c})}}.\] (4)

where \(()\) is the proposed decoupled Softmax. In Equation 4, by removing the competitor, compared with Equation 1, the decoupled Softmax makes all items associated with \(\) become -1 in gradient, the derivation is given in the A.1. Our Proposition 2 verifies that the expected results are achieved with decoupled Softmax.

**Proposition 2**.: _With the decoupled Softmax defined above, decoupled mixup cross-entropy \(_{DM}\) can boost the prediction confidence of the interested classes mutually and escape from the \(\)-constraint:_

\[_{DM}=-_{i=1}^{c}_{j=1}^{c}y_{a}^{i}y_{b}^{j}( ^{i}}{1-p_{(a,b)}^{j}}+ ^{j}}{1-p_{(a,b)}^{i}}).\] (5)The Decoupled Mixup.The proofs of Proposition 1 and 2 are given in the Appendix. In practice, the original smoothness of \(_{MCE}\) should not be lost, and thus the proposed DM is a regularizer for discriminability. The final form of decoupled mixup can be formulated as follows:

\[_{DM(CE)}=-^{T}((z_{(a,b)}))}_ {_{MCE}}+^{T}((z_{(a,b)}))y_{[a,b] }}_{_{DM}}.\]

where \(y_{(a,b)}\) indicates the mixed label while \(y_{[a,b]}\) is two-hot label encoding, \(\) is a trade-off factor. Notice that \(\) is robust and can be set according to the character of mixup methods (see Sec. 5.4).

_Practical consequences of such simple modification on mixup and the performance:_

Make What Should be Certain More Certain.As we expected, mixup training with a decoupling mechanism will be more accurate and confident in handling hard mixed samples with our artificially constructed hard mixed samples by using PuzzleMix. Figure 3_right_ demonstrates the model trained with decoupled mixup mostly doubled the top-2 accuracy on these mixed samples, which also verifies the information contained in mixed samples is beyond the "1" defined by standard Softmax. More interestingly, this advantage of decoupled mixup, _i.e.,_ higher confidence and accuracy, can be further amplified in semi-supervised learning due to the uncertainty of pseudo-labeling.

Enhance the Training Efficiency.It is straightforward to notice that there is no extra computation cost when using DM in vanilla mixup training, and the performance we can achieve is the same or even better than optimizable mixup policies, _i.e.,_ PuzzleMix, CoMixup, _etc._ Figure 3_left_ and _middle_ show decoupled mixup unveils the power of static mixup for more accurate and faster.

## 4 Extensions of Decoupled Mixup

With the high-accurate nature of decoupled mixup for mining hard mixed samples, semi-supervised learning is a suitable scenario to propagate the accurate label from labeled space to unlabeled space by using asymmetrical mixup. In addition, we can also generalize the decoupled mechanism into the binary cross-entropy for boosting the multi-classification task.

### Asymmetrical Strategy for Semi-supervised Learning

Based on labeled data \(L=\{(x_{a},y_{a})\}_{a=1}^{n_{L}}\), if we further consider unlabeled data \(U=\{(u_{a},v_{a})\}_{a=1}^{n_{U}}\) decoupled mixup can be the strong connection between \(L\) and \(U\). Recall the confirmation bias  problem of SSL: the performance of the student model is restricted by the teacher model when learning from inaccurate pseudo-labels. To fully use the \(L\) and strengthen the teacher model to provide more robust and accurate predictions, the unlabeled data with large \(\) can be used to mix with the labeled data to form hard mixed samples. With these hard mixed samples, we can employ decoupled mixup into semi-supervised learning effectively. Since only the label of \(L\) is accurate, we need to make a little asymmetric modification to the decoupled mixup, called Asymmetrical Strategy(AS). Formally, given the labeled and unlabeled datasets \(L\) and \(U\), AS builds reliable connection by generating hard mixed samples between \(L\) and \(U\) in an asymmetric manner (\(<0.5\)):

\[_{(a,b)}= x_{a}+(1-)u_{b};_{(a,b)}= y _{a}+(1-)v_{b}.\]

Figure 3: Results illustration of applying decouple mixup. _Left_: taking MixUp as an example, our proposed decoupled mixup cross-entropy, DM(CE), significantly improves training efficiency by exploring hard mixed sample; _Middle_: Acc _vs._ cost on ImageNet-1k; _Right_: Top-2 acc is calculated when the top-2 predictions equal to \(\{y_{a},y_{b}\}\).

Due to the uncertainty of the pseudo-label, only the labeled part is retained in \(_{DM}\):

\[}_{DM}=y_{a}^{T}(z_{(a,b)})y_{b},\]

where \(y_{a}\) and \(y_{b}\) are one-hot labels from \(L\). AS could be regarded as a special case of DM that only decouples on labeled data. Simply replacing \(_{DM}\) with \(}_{DM}\) can leverage the hard samples and alleviate the confirmation bias in semi-supervised learning.

### Decoupled Binary Cross-entropy Loss

Binary Cross-entropy Form of DM.Different from Softmax-based classification, we can also build decoupled mixup in multi-label classification tasks (\(1\)-\(vs\)-all) by using mixup binary cross-entropy (MBCE) loss  (\(()\) denotes Sigmoid rather Softmax in this case). Proposition 2 demonstrates the decoupled CE can mutually enhance the confidence of predictions for the interested classes and be free from \(\) limitations. Similarly, for MBCE, since it is not inherently bound to mutual interference between classes by Softmax, we have to preserve partial consistency and encourage more confident predictions, and thus propose a decoupled mixup binary cross-entropy loss, DM(BCE).

To this end, a rescaling function \(r:,t,^{}\) is designed to achieve this goal. The mixed label is rescaled by \(r()\): \(y_{mix}=_{a}y_{a}+_{b}y_{b}\), where \(_{a}\) and \(_{b}\) are rescaled. The rescaling function is defined as follows:

\[r(,t,)=^{t}, 0 t,0<1,\] (6)

where \(\) is the threshold, \(t\) is an index to control the convexity. As shown in Figure 4, Equation 6 has three situations: (a) when \(=0\), \(t=0\), the rescaled label is always equal to \(1\), as two-hot encoding; (b) when \(=1\), \(t=1\), \(r()\) is a linear function (vanilla mixup); (c) the rest curves demonstrate \(t\) is the parameter that changes the concavity and \(\) is responsible for truncating.

Empirical Results.In the case of interpolation-based mixup methods (_e.g._, Mixup, ManifoldMix, _etc._) that keep linearity between the mixed label and sample, the decoupled mechanism can be introduced by only adjusting threshold \(t\). In the case of cutting-based mixing policies (_e.g._, CutMix, _etc._) where the mixed samples and labels have a square relationship (generally a convex function), we can approximate the convexity by adjusting \(\), which are detailed in Sec. 5.4 and Appendix C.5.

## 5 Experiments

We adopt two types of top-1 classification accuracy (Acc) metrics (the mean of three trials): (i) the median top-1 Acc of the last 10 epochs [52; 38] for supervised image classification tasks with Mixup variants, and (ii) the best top-1 Acc in all checkpoints for SSL tasks. Popular ConvNets and Transformer-based architectures are used as backbone networks: ResNet variants including ResNet  (R), Wide-ResNet (WRN) , and ResNeXt-32x4d (RX) , Vision Transformers including DeiT  and Swin Transformer (Swin) .

### Image Classification Benchmarks

This subsection evaluates performance gains of DM on six image classification benchmarks, including CIFAR-100 , Tiny-ImageNet (Tiny) , ImageNet-1k , CUB-200-2011 (CUB) , FGVC-Aircraft (Aircraft) . There are mainly two types of mixup methods based on their mixing policies: _static_ methods including Mixup , CutMix , ManifoldMix , SaliencyMix , FMix , and ResizeMix , and _dynamic_ mixup methods including PuzzleMix , AutoMix , and SAMix . For a fair comparison, we use the optimal \(\) in \(\{0.1,0.2,0.5,0.8,1.0,2.0\}\) for all mixup

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_EMPTY:9]

PuzzHix , and AutoMix , with decoupled mixup, and found the improvement is limited. The main reason is there will not be many hard mixed samples in _dynamic_ mixups. Therefore, we additionally incorporate two _static_ mixups in RSB training settings, _i.e.,_ a half probability that Mixup or CutMix will be selected during the training. As expected, the improvements from the decoupled mixup are getting obvious upon _static_ mixup variants. This is a very preliminary attempt that deserves more exploration in future works, and we provide more results of _dynamic_ mixups in Appendix C.

Meanwhile, we further conduct comprehensive comparison experiments with modern Transformer-based architectures on CIFAR-100, considering the concurrent work TransMix  and TokenMix . As shown in Table 10, where results with \(\) denote the official implementation and the other are based on OpenMixup , DM(CE) enables DeiT (CutMix and Mixup) to achieve competitive performances as _dynamic_ mixup variants like AutoMix and SAMix  based on ConvNeXt-S without introducing extra computational costs, while still performing worse than them based on DeiT-S. Compared with specially designed label mixing methods using attention maps, DM(CE) also achieves competitive performances to TransMix and TokenMix. How to further improve the decoupled mixup with the salient regions or dynamic attention information to research similar performances of _dynamic_ mixing variants can also be studied in future works.

The Next Mixup.In a word, we introduce Decoupled Mixup (DM), a new objective function for considering both smoothness and mining discriminative features in mixup augmentations. The proposed DM helps _static_ mixup methods (_e.g.,_ MixUp and CutMix) achieve a comparable or better performance than the computationally expensive _dynamic_ mixup policies. Most importantly, DM raises a question worthy of researching: _is it necessary to design very complex mixup policies?_ We also find that decoupled mixup could be the bridge to combining _static_ and _dynamic_ mixup. However, the introduction of additional hyperparameters may take users some extra time to check on other than images or other mixup methods. This also leads to the core question of the next step in the development of this work: how to design a more elegant and adaptive mixup training objective that connects different types of mixups to achieve high data efficiency? We believe these explorations and questions can inspire future research in the community of mixup augmentations.