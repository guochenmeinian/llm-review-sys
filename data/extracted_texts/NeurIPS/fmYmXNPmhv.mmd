# Permutation Equivariant Neural Functionals

Allan Zhou\({}^{1}\)  Kaien Yang\({}^{1}\)  Kaylee Burns\({}^{1}\)  Adriano Cardace\({}^{2}\)  Yiding Jiang\({}^{3}\)

Samuel Sokota\({}^{3}\)  J. Zico Kolter\({}^{3}\)  Chelsea Finn\({}^{1}\)

\({}^{1}\)Stanford University \({}^{2}\)University of Bologna \({}^{3}\)Carnegie Mellon University

ayz@cs.stanford.edu

###### Abstract

This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as _neural functional networks_ (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building _permutation equivariant_ neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are _NF-Layers_ (neural functional layers) that we constrain to be permutation equivariant through an appropriate parameter sharing scheme. In our experiments, we find that permutation equivariant neural functionals are effective on a diverse set of tasks that require processing the weights of MLPs and CNNs, such as predicting classifier generalization, producing "winning ticket" sparsity masks for initializations, and classifying or editing implicit neural representations (INRs). In addition, we provide code for our models and experiments1.

## 1 Introduction

As deep neural networks have become increasingly prevalent across various domains, there has been a growing interest in techniques for processing their weights and gradients as data. Example applications include learnable optimizers for neural network training , extracting information from implicit neural representations of data , corrective editing of network weights , policy evaluation , and Bayesian inference given networks as evidence . We refer to functions of a neural network's weight-space (such as weights, gradients, or sparsity masks) as _neural functionals_; when these functions are themselves neural networks, we call them _neural functional networks_ (NFNs).

In this work, we design neural functional networks by incorporating relevant symmetries directly into the architecture, following a general line of work in "geometric deep learning" . For neural functionals, the symmetries of interest are transformations of a network's weights that preserve the network's behavior. In particular, we focus on _neuron permutation symmetries_, which are those that arise from the fact that the neurons of hidden layers have no inherent order.

Neuron permutation symmetries are simplest in feedforward networks, such as multilayer perceptrons (MLPs) and basic convolutional neural networks (CNNs). These symmetries are induced by the fact that the neurons in each hidden layer of a feedforward network can be arbitrarily permuted without changing its behavior . In MLPs, permuting the neurons in hidden layer \(i\) corresponds topermuting the rows of the weight matrix \(W^{(i)}\), and the columns of the next weight matrix \(W^{(i+1)}\) as shown on the left-hand side of Figure 1. Note that the same permutation must be applied to the rows \(W^{(i)}\) and columns of \(W^{(i+1)}\), since applying _different_ permutations generally changes network behavior and hence does not constitute a neuron permutation symmetry.

We introduce a new framework for constructing neural functional networks that are invariant or equivariant to neuron permutation symmetries. Our framework extends a long line of work on permutation equivariant architectures  that design equivariant layers for a particular permutation symmetry of interest. Specifically, we introduce neural functional layers (NF-Layers) that operate on weight-space features (see Figure 1) while being equivariant to neuron permutation symmetries. Composing these NF-Layers with pointwise non-linearities produces equivariant neural functionals.

We propose different NF-Layers depending on the assumed symmetries of the input weight-space: either only the hidden neurons of the feedforward network can be permuted (hidden neuron permutation, HNP), or all neurons, including inputs and outputs, can be permuted (neuron permutation, NP). Although the HNP assumption is typically more appropriate, the corresponding NF-Layers can be parameter inefficient and computationally infeasible in some settings. In contrast, NF-Layers derived under NP assumptions often lead to much more efficient architectures, and, when combined with a positional encoding scheme we design, can even be effective on tasks that require breaking input and output symmetry. For situations where invariance is required, we also define invariant NF-Layers that can be applied on top of equivariant weight-space features.

Finally, we investigate the applications of permutation equivariant neural functionals on tasks involving both feedforward MLPs and CNNs. Our first two tasks require (1) predicting the test accuracy of CNN image classifiers and (2) classifying implicit neural representations (INRs) of images and 3D shapes. We then evaluate NFNs on their ability to (3) predict good sparsity masks for initializations (also called _winning tickets_), and on (4) a weight-space "style-editing" task where the goal is to modify the content an INR encodes by directly editing its weights. In multiple experiments across these diverse settings, we find that permutation equivariant neural functionals consistently outperform non-equivariant methods and are effective for solving weight-space tasks.

**Relation to DWSNets.** The recent work of Navon et al.  recognized the potential of leveraging weight-space symmetries to build equivariant architectures on deep weight-spaces; they characterize a weight-space layer which is mathematically equivalent to our NF-Layer in the HNP setting. Their work additionally studies interesting universality properties of the resulting equivariant architectures, and demonstrates strong empirical results for a suite of tasks that require processing the weights of MLPs. Our framework additionally introduces the NP setting, where we make stronger symmetry assumptions to develop equivariant layers with improved parameter efficiency and practical scalability. We also extend our NFN variants to process convolutional neural networks (CNNs) as input, leading to applications such as predicting the generalization of CNN classifiers (Section 3.1).

Figure 1: The internal operation of our permutation equivariant neural functionals (NFNs). The NFN processes the input weights through a series of equivariant NF-Layers, with each one producing _weight-space features_ with varying numbers of channels. In this example, a neuron permutation symmetry simultaneously permutes the rows of \(W^{(2)}\) and the columns of \(W^{(3)}\). This permutation propagates through the NFN in an equivariant manner.

## 2 Equivariant neural functionals

We begin by setting up basic concepts related to (hidden) neuron permutation symmetries, before defining the equivariant NF-Layers in Sec. 2.2 and invariant NF-Layers in Sec. 2.3.

### Preliminaries

Consider an \(L\)-layer feedforward network having \(n_{i}\) neurons at layer \(i\), with \(n_{0}\) and \(n_{L}\) being the input and output dimensions, respectively. The network is parameterized by weights \(W=\,W^{(i)}^{n_{i} n_{i-1}} i 1..L \,}\) and biases \(v=\,v^{(i)}^{n_{i}} i 1..L \,}\). We denote the combined collection \(U(W,v)\) belonging to weight-space, \(\).

Since the neurons in a hidden layer \(i\{1,,L-1\}\) have no inherent ordering, the network is invariant to the symmetric group \(S_{n_{i}}\) of permutations of the neurons in layer \(i\). This reasoning applies to every hidden layer, so the network is invariant to \(} S_{n_{1}} S_{n_{L-1}}\), which we refer to as the **hidden neuron permutation** (HNP) group. Under the stronger assumption that the input and output neurons are also unordered, the network is invariant to \( S_{0} S_{n_{L}}\), which we refer to as the **neuron permutation** (NP) group. We focus on the NP setting throughout the main text, and treat the HNP case in Appendix B. See Table 1 for a concise summary of the relevant notation for each symmetry group we consider.

Consider an MLP and a permutation \(=(_{0},,_{L})\). The action of the neuron permutation group is to permute the rows of each weight matrix \(W^{(i)}\) by \(_{i}\), and the columns by \(_{i-1}\). Each bias vector \(v^{(i)}\) is also permuted by \(_{i}\). So the action is \( U( W, v)\), where:

\[[ W]^{i}_{jk}=W^{(i)}_{_{i}^{-1}(j),_{i-1}^{-1}(k)},[  v]^{i}_{j}=v^{(i)}_{_{i}^{-1}(j)}.\] (1)

Until now we have used \(U=(W,v)\) to denote actual weights and biases, but the inputs to a neural functional layer could be any weight-space _feature_ such as a gradient, sparsity mask, or the output of a previous NF-Layer (Figure 1). Moreover, we may consider inputs with \(c 1\) feature channels, belonging to \(^{c}=_{i=1}^{c}\), the direct sum of \(c\) copies of \(\). Concretely, each \(U^{c}\) consists of weights \(W=\,W^{(i)}^{n_{i} n_{i-1} c} i  1..L\,}\) and biases \(v=\,v^{(i)}^{n_{i} c} i 1..L \,}\), with the channels in the final dimension. The action defined in Eq. 1 extends to the multiple channel case if we define \(W^{(i)}_{jk}:=W^{(i)}_{j,k,:}^{c}\) and \(v^{(i)}_{j}:=v^{(i)}_{j,:}^{c}\).

The focus of this work is on making neural functionals that are equivariant (or invariant) to neuron permutation symmetries. Letting \(c_{i}\) and \(c_{o}\) be the number of input and output channels, we refer to a function \(f:^{c_{i}}^{c_{o}}\) as \(\)**-equivariant** if \( f(U)=f( U)\) for all \(\) and \(U^{c_{i}}\), where the action of \(\) on the input and output spaces is defined by Eq. 1. Similarly, a function \(f:^{c}\) is \(\)**-invariant** if \(f( U)=f(U)\) for all \(\) and \(U\).

If \(f,g\) are equivariant, then their composition \(f g\) is also equivariant; if \(g\) is equivariant and \(f\) is invariant, then \(f g\) is invariant. Since pointwise nonlinearities are already permutation equivariant, our remaining task is to design a _linear_ NF-Layer that is \(\)-equivariant. We can then construct equivariant neural functionals by stacking these NF-Layers with pointwise nonlinearities.

**Group** & **Abbrv** & **Permutable layers** &  \\  & & & Signature & Parameter count \\  \(=_{i=0}^{L}S_{n_{i}}\) & NP & All layers & \(H:^{c_{i}}^{c_{o}}\) & \(O(c_{i}c_{o}L^{2})\) \\ \(}=_{i=1}^{L-1}S_{n_{i}}\) & HNP & Hidden layers & \(:^{c_{i}}^{c_{o}}\) & \(O(c_{i}c_{o}(L+n_{0}+n_{L})^{2})\) \\ — & — & None & \(T:^{c_{i}}^{c_{o}}\) & \(c_{i}c_{o}()^{2}\) \\ 

Table 1: Permutation symmetries of \(L\)-layer feedforward networks with \(n_{0},,n_{L}\) neurons at each layer. All feedforward networks are invariant under hidden neuron permutations (HNP), while NP assumes that input and output neurons can also be permuted. We show the corresponding equivariant NF-Layers which process weight-space features from \(\), with \(c_{i}\) input channels and \(c_{o}\) output channels.

### Equivariant NF-Layers

We now construct a linear \(\)-equivariant layer that serves as a key building block for neural functional networks. In the single channel case, we begin with generic linear layers \(T(;):(U)(U)\), where \((U)^{(U)}\) is \(U\) flattened as a vector and \(^{(U)(U)}\) is a matrix of parameters. We show in Appendix B.2 that **any**\(\)-equivariant \(T(;)\) must satisfy a system of constraints on \(\) known as equivariant _parameter sharing_. We derive this parameter sharing by partitioning the entries of \(\) by the orbits of their indices under the action of \(\), with parameters shared in each orbit . Table 7 of the appendix describes the parameter sharing in detail.

Equivariant parameter sharing reduces the matrix-vector product \((U)\) to the NF-Layer we now present. For simplicity we ignore \(\) and assume here that \(=\) and defer the full form to Eq. 3 in the appendix. Then \(H:^{c_{i}}^{c_{o}}\) maps input \(W^{(1)},,W^{(L)}\) to \(H(W)^{(1)},,H(W)^{(L)}\). Recall that the inputs are not necessarily weights, but could be arbitrary weight-space features including the output of a previous NF-Layer. For \(W^{(i)}^{n_{i} n_{i-1} c_{i}}\), the corresponding output is \(H(W)^{(i)}^{n_{i} n_{i-1} c_{o}}\) with entries computed:

\[H(W)^{(i)}_{jk}=(_{s}a^{i,s}W^{(s)}_{,})+b^{i,i}W^{( i)}_{,k}+b^{i,i-1}W^{(i-1)}_{k,}+c^{i,i}W^{(i)}_{j,}+c^{i,i+1}W^{( i+1)}_{,j}+d^{i}W^{(i)}_{jk}.\] (2)

Note that the terms involving \(W^{(i-1)}\) or \(W^{(i+1)}\) should be omitted for \(i=0\) and \(i=L\), respectively, and \(\) denotes summation or averaging over either the rows or columns. Recall that in the multi-channel case, each \(W^{(i)}_{jk}\) is a vector in \(^{c_{i}}\) so each parameter is a \(c_{o} c_{i}\) matrix. We also provide a concrete pseudocode description of \(H\) in Appendix A. Figure 2 visually illustrates the NF-Layer in the single-channel case, showing how the row or column sums from each input contribute to each output. To gain intuition for the operation of \(H\), it is straightforward to check \(\)-equivariance:

**Proposition 1**.: _The NF-Layer \(H:^{c_{i}}^{c_{o}}\) (Eq. 2 and Eq. 3) is \(\)-equivariant, where the group's action on input and output spaces is defined by Eq. 1. Moreover, any linear \(\)-equivariant map \(T:^{c_{i}}^{c_{o}}\) is equivalent to \(H\) for some choice of parameters \(a,b,c,d\)._

Proof (sketch).: We can verify that \(H\) satisfies the equivariance condition \([ H(W)]^{(i)}_{jk}=H( W)^{(i)}_{jk}\) for any \(i,j,k\) by expanding each side of the equation using the definitions of the layer and action

Figure 2: A permutation equivariant NF-Layer takes in weight-space features as input (bottom) and outputs transformed features (top), while respecting the neuron permutation symmetries of feedforward networks. This illustrates the computation of a single output element \(H(W)^{i}_{jk}\), defined in Eq. 2. Each output is a weighted combination of rows or column sums of the input weights, which preserves permutation symmetry. The first term contributes a weighted combination of row-and-column sums from _every_ input weight, though this is omitted for visual clarity.

(Eq. 1). Moreover, Appendix B.2 shows that any \(\)-equivariant linear map \(T(,)\) must have the same equivariant parameter sharing as \(H\), meaning that it must be equivalent to \(H\) for some choice of parameter \(a,b,c,d\). 

Informally, the above proposition tells us that \(H\) can express any linear \(\)-equivariant function of a weight-space. Since \(}\) is a subgroup of \(\), \(H\) is also \(}\)-equivariant. However, it does not express every possible linear \(}\)-equivariant function. We derive the full \(}\)-equivariant NF-Layer \(:\) in Appendix C.

Table 1 summarizes the number of parameters (after parameter sharing) under different symmetry assumptions. While in general a linear layer \(T(;):^{c_{i}}^{c_{n}}\) has \(c_{i}c_{o}()^{2}\) parameters, the equivariant NF-Layers have significantly fewer free parameters due to parameter sharing. The \(\)-equivariant layer \(H\) has \(O(c_{i}c_{o}L^{2})\), while the \(}\)-equivariant layer \(\) has \(O(c_{i}c_{o}(L+n_{0}+n_{L})^{2})\) parameters. The latter's quadratic dependence on input and output dimensions can be prohibitive in some settings, such as in classification where the number of outputs can be tens of thousands.

**Extension to convolutional weight-spaces.** In convolution layers, since neurons correspond to spatial _channels_, we let \(n_{i}\) denote the number of channels at the \(i^{}\) layer. Each bias \(v^{(i)}^{n_{i}}\) has the same dimensions as in the fully connected case, so only the convolution filter needs to be treated differently since it has additional spatial dimension(s) that cannot be permuted. For example, consider a 1D CNN with filters \(W=\ W^{(i)}^{n_{i} n_{i-1} w}\ \ i  1..L}\), where \(n_{i} n_{i-1}\) are the output and input channel dimensions and \(w\) is the filter width. We let \(W^{(i)}_{jk} W^{(i)}_{jk,:}^{w}\) denote the \(k^{}\) filter in the \(j^{}\) output channel, then define the \(\)-action the same way as in Eq. 1.

We immediately observe the similarities to multi-channel features: both add dimensions that are not permuted by the group action. In fact, suppose we have \(c\)-channel features \(U^{c}\) where \(\) is the weight-space of a 1D CNN. Then we combine the filter and channel dimensions of the weights, with \(W^{(i)}^{n_{i} n_{i-1}(cw)}\). This allows us to use the multi-channel NF-Layer \(H:^{wc_{i}}^{wc_{o}}\). Any further channel dimensions, such as those for 2D convolutions, can also be folded into the channel dimension.

It is common for CNNs in image classification to follow convolutional layers with pooling and fully connected (FC) layers, which opens the question of defining the \(\)-action when layer \(\) is FC and layer \(-1\) is convolutional. If global spatial pooling removes all spatial dimensions from the output of \(-1\) (as in e.g., ResNets  and the Small CNN Zoo ), then we can verify that the existing action definitions work without modification. We leave more complicated situations (e.g., when nontrivial spatial dimensions are flattened as input to FC layers) to future work.

**IO-encoding.** The \(\)-equivariant layer \(H\) is more parameter efficient than \(\) (Table 1), but its NP assumptions are typically too strong. To resolve this problem, we can add either learned or fixed (sinusoidal) position embeddings to the columns of \(W^{(1)}\) and the rows of \(W^{(L)}\) and \(v^{(L)}\); this breaks the symmetry at input and output neurons even when using \(\)-equivariant layers. In our experiments, we find that IO-encoding makes \(H\) competitive or superior to \(\), while using a fraction of the parameters.

### Invariant NF-Layers

Invariant neural functionals can be designed by composing multiple equivariant NF-Layers with an invariant NF-Layer, which can then be followed by an MLP. We define an \(\)-invariant2 layer \(P:^{2L}\) by simply summing or averaging the weight matrices and bias vectors across any axis that has permutation symmetry, i.e., \(P(U)=(W^{(1)}_{,},,W^{(L)}_{,},v^{(1)}_{},,v^{(L)}_{}).\) We can then apply a fully connected layer to the output of \(P\) to produce an invariant vector of arbitrary size. In fact, this combination expresses every possible linear invariant function on \(\):

**Proposition 2**.: _Any \(\)-invariant linear function \(^{d}\) can be expressed in the form \(f P\), for some choice of linear \(f:^{2L}^{d}\). Proof: See section B.3._

## 3 Experiments

Our experiments evaluate permutation equivariant neural functionals on a variety of tasks that require either invariance (predicting CNN generalization and extracting information from INRs) or equivariance (predicting "winning ticket" sparsity masks and weight-space editing of INR content).

Throughout the experiments, we construct neural functional networks (NFNs) using the NF-Layers described in the previous section. Although the specific design varies depending on the task, we will broadly refer to our permutation equivariant NFNs as \(_{}\) and \(_{}\), depending on which NF-Layer variant they use (see Table 1). We also evaluate a "pointwise" ablation of our equivariant NF-Layer that ignores interactions between weights by only using the last term of Eq. 2, computing \(H(W)^{i}_{jk} d^{i}W^{(i)}_{jk}\). We refer to NFNs that use this pointwise NF-Layer as \(_{}\).

Using our benchmarks, we compare the performance of NFNs with other methods for processing weights, including standard MLPs that operate on the flattened weight inputs. To encourage permutation equivariance, we optionally augment the MLP's training with permutations using Eq. 1. On relevant datasets we also compare with the recently developed DWSNets , an equivariant architecture similar to \(_{}\), and with \(\), a recent non-equivariant approach for learning useful representations from weights.

### Predicting CNN generalization from weights

Why deep neural networks generalize despite being heavily overparameterized is a longstanding research problem in deep learning. One recent line of work has investigated the possibility of directly predicting the test accuracy of the models from the weights [64; 16]. The goal is to study generalization in a data-driven fashion and ultimately identify useful patterns from the weights.

Prior methods develop various strategies for extracting potentially useful features from the weights before using them to predict the test accuracy [29; 67; 64; 30; 42]. However, using hand-crafted features could fail to capture intricate correlations between the weights and test accuracy. Instead, we explore using neural functionals to predict test accuracy from the _raw weights_ of feedforward convolutional neural networks (CNN) from the _Small CNN Zoo_ dataset , which contains thousands of CNN weights trained on several datasets with a shared architecture, but varied optimization hyperparameters. We compare the predictive power of \(_{}\) and \(_{}\) against a method of Unterthiner et al.  that trains predictors on statistical features extracted from each weight and bias, and refer to it as StatNN. To measure the predictive performance of each method, we use _Kendall's_\(\), a popular rank correlation metric with values in \([-1,1]\).

In Table 2, we show the results on two challenging subsets of Small CNN Zoo corresponding to CNNs trained on CIFAR-10-GS and SVHN-GS (GS stands for grayscaled). We see that \(_{}\) consistently performs the best on both datasets by a significant margin, showing that having access to the full weights can increase predictive power over hand-designed features as in StatNN. Because the input and output dimensionalities are small on these datasets, \(_{}\) only uses moderately more (\( 1.4\)) parameters than \(_{}\) with equivalent depth and channel dimensions, while having significantly better performance.

### Classifying implicit neural representations of images and 3D shapes

Given the rise of implicit neural representations (INRs) that encode data such as images and 3D-scenes [61; 43; 7; 49; 58; 45; 14; 15], it is natural to wonder how to extract information about the

    & \(_{}\) & \(_{}\) & StatNN \\  CIFAR-10-GS & \(\) & \(0.922 0.001\) & \(0.915 0.002\) \\ SVHN-GS & \(\) & \(0.856 0.001\) & \(0.843 0.000\) \\   

Table 2: Test \(\) of generalization prediction methods on the Small CNN Zoo , which contains the weights and test accuracies of many small CNNs trained on different datasets, such as CIFAR-10-GS or SVHN-GS. \(_{}\) outperforms other methods on both datasets. Uncertainties indicate max and min over two runs.

original data directly from INR weights. Compared to discrete signal representations (pixels, voxels, etc.), the advantage of directly processing INRs is that one can easily be agnostic to varying signal sizes and resolutions.

In this task, our goal is to classify the contents of INRs given only the weights as input. We consider datasets of SIRENs  that encode images (MNIST , FashionMNIST , and CIFAR ) and 3D shapes (ShapeNet-10 and ScanNet-10 ). For image datasets each SIREN network represents the mapping from pixel coordinate to RGB (or grayscale) value for a single image, while for 3D shapes each network is a signed (or unsigned) distance function encoding a single shape. Each dataset of SIREN weights is split into training, validation, and testing sets. We construct and train invariant neural functionals to classify the INRs, and compare their performance against the MLP and MLP\({}_{}\) baselines, which are three-layer MLPs with ReLU activations and 1,000 hidden units per layer.

As Navon et al.  test the performance of DWSNets on their independently constructed 2D-image INR datasets, we also present the results of training DWSNets on our own 2D-image INR datasets. Table 3 show that \(_{}\) consistently achieves the highest test accuracies of any method on the 2D-image tasks. More broadly, equivariant architectures significantly outperform the non-equivariant MLP approaches, even with permutation data augmentations.

For the 3D-shape datasets we also report the performance of intr2vec , a recent non-equivariant method for classifying 3D shapes from INR weights. Note that intr2vec's original setting assumes that all INRs in a dataset are trained from the same shared initialization, whereas our problem setting makes no such assumption and allows INRs to be trained from random and independent initializations. As expected, Table 4 shows that \(_{}\) and \(_{}\) achieve significantly higher test accuracies than intr2vec, as well as the non-equivariant MLP baselines.

In addition to superior generalization, Appendix Table 16 shows that the NFNs are also better at fitting the training data compared to non-equivariant architectures. The MLPs achieve low train accuracy, even with an equal number of parameters as the NFNs. Interestingly, \(_{}\) matches or exceeds \(_{}\) performance on both CIFAR-10 and the 3D-shape datasets while using fewer parameters (e.g., \(35\%\) as many parameters on CIFAR-10). Finally, we note that no weight space methods (including NFN) match the performance of near state-of-the-art methods on discrete data representations such as 2D image arrays and point clouds (see Section E.2 for details). All weight space methods still lack the _geometric_ inductive biases that, e.g., convolutional networks have in image tasks.

    & \(_{}\) & \(_{}\) & MLP & MLP\({}_{}\) & intr2vec \\  ShapeNet-10 & \(86.9 0.860\) & \(\) & \(25.4 0.121\) & \(33.8 0.126\) & \(39.1 0.385\) \\  ScanNet-10 & \(64.1 0.572\) & \(\) & \(32.9 0.351\) & \(45.5 0.126\) & \(38.2 0.409\) \\   

Table 4: Classification test accuracies (%) for datasets of implicit neural representations (INRs) of either ShapeNet-10  or ScanNet-10 . Our equivariant NFNs outperform the MLP baselines and recent non-equivariant methods such as intr2vec . Uncertainties indicate standard error over three runs.

    & \(_{}\) & \(_{}\) & DWSNets & MLP & MLP\({}_{}\) \\  MNIST & \(92.5 0.071\) & \(\) & \(74.4 0.143\) & \(14.5 0.035\) & \(21.0 0.172\) \\  FashionMNIST & \(72.7 1.53\) & \(\) & \(64.8 0.685\) & \(12.5 0.111\) & \(15.9 0.181\) \\  CIFAR-10 & \(44.1 0.471\) & \(\) & \(41.5 0.431\) & \(16.9 0.250\) & \(18.9 0.432\) \\   

Table 3: Classification test accuracies (%) for implicit neural representations of MNIST, FashionMNIST, and CIFAR-10. Equivariant architectures such as NFNs and DWSNets  outperform the non-equivariant MLP baselines, even when the MLP has permutation augmentations. Our \(_{}\) variant consistently outperforms all other methods across each dataset. Uncertainties indicate standard error over three runs.

### Predicting "winning ticket" masks from initialization

The Lottery Ticket Hypothesis [19; 20], LTH] conjectures the existence of _winning tickets_, or sparse initializations that train to the same final performance as dense networks, and showed their existence in some settings through iterative magnitude pruning (IMP). IMP retroactively finds a winning ticket by pruning _trained_ models by magnitude; however, finding the winning ticket from only the initialization without training remains challenging.

We demonstrate that permutation equivariant neural functionals are a promising approach for finding winning tickets at initialization by learning over datasets of initializations and their winning tickets. Let \(U_{0}\) be an initialization and let the sparsity mask \(M\{0,1\}^{()}\) be a winning ticket for the initialization, with zeros indicating that the corresponding entries of \(U_{0}\) should be pruned. The goal is to predict a winning ticket \(\) given a held out initialization \(U_{0}\), such that the MLP initialized with \(U_{0}\) and sparsity pattern \(\) will achieve a high test accuracy after training.

We construct a conditional variational autoencoder [32; 59; cVAE] that learns a generative model of the winning tickets conditioned on initialization and train on datasets of (initialization, ticket) pairs found by one step of IMP with a sparsity level of \(P_{m}=0.95\) for both MLPs trained on MNIST and CNNs trained on CIFAR-10. Table 5 compares the performance of tickets predicted by equivariant neural functionals against IMP tickets and random tickets. We generate random tickets by randomly sampling sparsity mask entries from Bernoulli\((1-P_{m})\). In this setting, NFN\({}_{}\) is prohibitively parameter inefficient, but NFN\({}_{}\) is able to recover test accuracies that are close to that of IMP pruned networks in CIFAR-10 and MNIST, respectively. Somewhat surprisingly, NFN\({}_{}\) performs just as well as the other NFNs, indicating that one can approach IMP performance in these settings without considering interactions between weights or layers. Appendix E.4 further analyzes how NFN\({}_{}\) learns to prune.

### Weight-space style editing

Another potentially useful application of neural functionals is to edit (i.e., transform) the weights of a given INR to alter the content that it encodes. In particular, the goal of this task is to edit the weights of a trained SIREN to alter its encoded image (Figure 3). We evaluate two editing tasks:

   Method & Contrast & Dilate \\  & (CIFAR-10) & (MNIST) \\  MLP & \(0.031\) & \(0.306\) \\ MLPAug & \(0.029\) & \(0.307\) \\ NFN\({}_{}\) & \(0.029\) & \(0.197\) \\ NFN\({}_{}\) & \(\) & \(\) \\ NFN\({}_{}\) & \(\) & \(\) \\   

Table 6: Test mean squared error (lower is better) between weight-space editing methods and ground-truth image-space transformations.

Figure 3: In weight-space style editing, an NFN directly edits the weights of an INR to alter the content it encodes. In this example, the NFN edits the weights to dilate the encoded image.

(1) making MNIST digits thicker via image dilation (**Dilate**), and (2) increasing image contrast on CIFAR-10 (**Contrast**). Both of these tasks require neural functionals to process _the relationships between different pixels_ to successfully solve the task.

To produce training data for this task, we use standard image processing libraries [28, OpenCV] to dilate or increase the contrast of the MNIST and CIFAR-10 images, respectively. The training objective is to minimize the mean squared error between the image generated by the NFN-edited INR and the image produced by image processing. We construct equivariant neural functionals to edit the INR weights, and compare them against MLP-based neural functionals with and without permutation augmentation.

Table 6 shows that permutation equivariant neural functionals (NFN\({}_{}\) and NFN\({}_{}\)) achieve significantly better test MSE when editing held out INRs compared to other methods, on both the Dilate (MNIST) and Contrast (CIFAR-10) tasks. In other words, they produce results that are closest to the "ground truth" image-space processing operations for each task. The pointwise ablation NFN\({}_{}\) performs significantly worse, indicating that accounting for interactions between weights and layers is important to accomplishing these tasks. Figure 4 shows random qualitative samples of editing by different methods below the original (pre-edit) INR. We observe that NFNs are more effective than MLP\({}_{}\) at dilating MNIST digits and increasing the contrast in CIFAR-10 images.

## 4 Related work

The permutation symmetries of neurons have been a topic of interest in the context of loss landscapes and model merging [21; 4; 62; 17; 1]. Other works have analyzed the degree of learned permutation symmetry in networks that process weights  and studied ways of accounting for symmetries when measuring or encouraging diversity in the weight-space . However, these symmetries have not been a key consideration in architecture design for processing weight-space objects [2; 39; 23; 36; 69; 13; 33]. Instead, existing approaches try to encourage permutation equivariance through data augmentation [51; 44]. In contrast, this work directly encodes the equivariance of the weight-space into our architecture design, which can result in much higher data and computational efficiency, as evidenced by the success of convolutional neural networks .

Our work follows a long line of literature that incorporates structure and symmetry into neural network architectures [37; 8; 54; 34; 9; 18]. This includes works that design permutation equivariant layers, originally for processing sets and graphs [52; 68; 25; 41]. More generally, equivariant layers have been developed for processing arbitrary rank-\(k\) tensors (and correspondingly, hypergraphs) under higher-order permutation actions [40; 63; 48]. In this context, we can view feedforward networks as graphs that factor into a special layered structure where each (hidden) layer of neurons is an independently permutable set of nodes, and the weights are adjacency matrices specifying the edge weights between nodes of adjacent layers. Then our equivariant NF-Layers give the maximal set

Figure 4: Random qualitative samples of INR editing behavior on the Dilate (MNIST) and Contrast (CIFAR-10) editing tasks. The first row shows the image produced by the original INR, while the rows below show the result of editing the INR weights with an NFN. The difference between MLP neural functionals and equivariant neural functionals is especially pronounced on the more challenging Dilate tasks, which require modifying the geometry of the image. In the Contrast tasks, the MLP baseline produces dimmer images compared to the ground truth, which is especially evident in the second and third columns.

of linear functions on these special graph structures. As discussed in Section 1, Navon et al.  recently developed an equivariant weight-space layer that is equivalent to our NF-Layer in the HNP setting. Our work introduces the NP setting to improve parameter efficiency and scalability over the HNP setting, and extends beyond the fully connected case to handle convolutional weight-space inputs.

## 5 Conclusion

This paper proposes a novel symmetry-inspired framework for the design of neural functional networks (NFNs), which process weight-space features such as weights, gradients, and sparsity masks. Our framework focuses on the permutation symmetries that arise in weight-spaces due to the particular structure of neural networks. We introduce two equivariant NF-Layers as building blocks for NFNs, which differ in their underlying symmetry assumptions and parameter efficiency, then use them to construct a variety of permutation equivariant neural functionals. Experimental results across diverse settings demonstrate that permutation equivariant neural functionals outperform prior methods and are effective for solving weight-space tasks.

**Limitations and future work.** Although we believe this framework is a step toward the principled design of effective neural functionals, there remain multiple directions for improvement. One such direction would concern extending the NF-Layers beyond feedforward weight spaces, in order to process the weights of more complex architectures such as ResNets  and Transformers . Another useful direction would involve reducing the activation sizes produced by NF-Layers, in order to scale neural functionals to process the weights of very large networks. Finally, improvements to NF-Layers could account for the other symmetries of neural network weight spaces, such as scaling symmetries in ReLU networks .