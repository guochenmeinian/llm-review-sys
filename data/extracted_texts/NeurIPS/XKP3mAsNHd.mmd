# Incentives in Private Collaborative Machine Learning

Rachael Hwee Ling Sim\({}^{1}\)

Yehong Zhang\({}^{2}\)

Trong Nghia Hoang\({}^{3}\)

**Xinyi Xu\({}^{1}\)**

**Bryan Kian Hsiang Low\({}^{1}\)**

**and Patrick Jaillet\({}^{4}\)**

\({}^{1}\) Department of Computer Science, National University of Singapore, Republic of Singapore

\({}^{2}\) Peng Cheng Laboratory, People's Republic of China

\({}^{3}\) School of Electrical Engineering and Computer Science, Washington State University, USA

\({}^{4}\) Dept. of Electrical Engineering and Computer Science, MIT, USA

\({}^{1}\){rachaels,xuxinyi,lowkh}@comp.nus.edu.sg, \({}^{2}\)zhangyh02@pcl.ac.cn

\({}^{3}\)trongnghia.hoang@wsu.edu, \({}^{4}\)jaillet@mit.edu

###### Abstract

Collaborative machine learning involves training models on data from multiple parties but must incentivize their participation. Existing data valuation methods fairly value and reward each party based on shared data or model parameters but neglect the privacy risks involved. To address this, we introduce _differential privacy_ (DP) as an incentive. Each party can select its required DP guarantee and perturb its _sufficient statistic_ (SS) accordingly. The mediator values the perturbed SS by the Bayesian surprise it elicits about the model parameters. As our valuation function enforces a _privacy-valuation trade-off_, parties are deterred from selecting excessive DP guarantees that reduce the utility of the grand coalition's model. Finally, the mediator rewards each party with different posterior samples of the model parameters. Such rewards still satisfy existing incentives like fairness but additionally preserve DP and a high similarity to the grand coalition's posterior. We empirically demonstrate the effectiveness and practicality of our approach on synthetic and real-world datasets.

## 1 Introduction

Collaborative _machine learning_ (ML) seeks to build ML models of higher quality by training on more data owned by multiple parties . For example, a hospital can improve its prediction of disease progression by training on data collected from more and diversified patients from other hospitals . Likewise, a real-estate firm can improve its prediction of demand and price by training on data from others . However, parties have two main concerns that discourage data sharing and participation in collaborative ML: (a) whether they benefit from the collaboration and (b) privacy.

Concern (a) arises as each party would expect the significant cost that it incurs to collect and share data (e.g., the risk of losing its competitive edge) to be covered. Some existing works , among other data valuation methods,1 have recognized that parties require incentives to collaborate, such as a guaranteed _fair_ higher reward from contributing more valuable data than the others, an _individually rational_ higher reward from collaboration than in solitude, and a higher total reward (i.e., _group welfare_) whenever possible. Often, parties share and are rewarded with information (e.g., gradients  or parameters  of parametric ML models) computed from the shared data. However, these incentive-aware reward schemes expose parties to privacy risks.

On the other hand, some _federated learning_ (FL) works  have addressed the privacy concern (b) and satisfied strict data protection laws (e.g., European Union's General Data Protection Regulation) by enforcing _differential privacy_ (DP)  during the collaboration. Each party injects noise before sharing information to ensure that its shared information would not significantly alter a knowledgeable collaborating party's or mediator's belief about whether a datum was input to the algorithm. Injecting more noise leads to a stronger DP guarantee. As raised in , adding DP can invalidate game-theoretic properties and hence affect participation. For example, in the next paragraph, we will see that adding DP may lead to the collaboration being perceived as unfair and a lower group welfare. However, to the best of our knowledge (and as discussed in Sec. 7 and Fig. 5), there are no works that address both concerns, i.e., ensure the _fairness_, _individual rationality_, and _group welfare_ incentives (see Sec. 4), alongside privacy. Thus, we aim to fill in this gap and design an incentive-aware yet privacy-preserving reward scheme by addressing the following questions:

**If a party (e.g., hospital) requires a stronger DP guarantee, what should the impact be on its valuation and reward?** Our answer is that, on average, its valuation and reward should decrease. Intuitively, it is unfair when this party gets a higher valuation due to randomness in the DP noise. More importantly, parties require guaranteed higher rewards to consider a weaker privacy guarantee  which will help maximize the utility of the collaboratively trained model(s). As observed in , the weaker the DP guarantee, the smaller the loss in model accuracy from enforcing DP. Thus, we will (i) assign a value to each party to enforce a _privacy-valuation trade-off_ and incentivize parties against unfetteredly selecting an excessively strong DP guarantee,2 and (ii) flexibly allow each party to enforce a different DP guarantee without imposing a party's need for strong DP on others. This new perspective and its realization is our main contribution.

**To enforce a privacy-valuation trade-off, how should DP be ensured and a party's data be valued (Sec. 3)?** Initially, valuation using validation accuracy seems promising as the works of  have empirically shown that adding noise will decrease the valuation. However, parties may be reluctant to contribute validation data due to privacy concerns and disagree on the validation set as they prioritize accurate predictions on different inputs (e.g., patient demographics). So, we revert to valuing parties based on the quality of inference of the model parameters under DP. Bayesian inference is a natural choice as it quantifies the impact of (additional DP) noise. In Sec. 2, we will explain how each party ensures DP by only sharing perturbed _sufficient statistic_ (SS) with the mediator. The mediator values the perturbed SS by the _surprise_ it elicits relative to the prior belief of model parameters. Intuitively, noisier perturbed SS is less valuable as the posterior belief of the model parameters will be more diffuse and similar to the prior. As parties prioritize obtaining a model for future predictions and may face legal/decision difficulties in implementing monetary payments, we reward each party with _posterior samples of the model parameters_ (in short, _model reward_) instead.

**How should the reward scheme be designed to satisfy the aforementioned privacy, individual rationality, and fairness incentives (Sec. 4)?** Our scheme will naturally satisfy the privacy incentive as any post-processing of the perturbed SS will preserve DP. To satisfy fairness and individual rationality, we set the _target_ reward value for every party using \(\)-Shapley value . **Lastly, to realize these target reward values, how should the model reward be generated for each party (Sec. 5)?** Instead of rewarding all parties with samples from the _same_ (grand coalition's) posterior of the model parameters given all their perturbed SS (which would be unfair if their valuations differ), our reward control mechanism generates a _different_ posterior for each party that still _preserves a high similarity to the grand coalition's posterior._ Concretely, the mediator scales the SS by a factor between \(0\) and \(1\) before sampling to control the impact of data on the posterior (by _tempering_ the data likelihood). Scaling the SS by a factor of \(0\), \(1\), and between \(0\) and \(1\) yield the prior, posterior, and their interpolation, respectively. We then solve for the factor to achieve the target reward value.

By answering the above questions, our work here provides the following novel contributions3:

* A new _privacy-valuation trade-off_ criterion for valuation functions that is provably satisfied by the combination of our _Bayesian surprise_ valuation function with DP noise-aware inference (Sec. 3);
* New incentives including _DP_ (while _deterring excessive DP_) and _similarity to grand coalition's model_ (Sec. 4);* _Reward control mechanisms_ (Sec. 5) to generate _posterior samples_ of the model parameters for each party that achieve a target reward value and the aforementioned incentives; one such mechanism tempers the likelihood of the data by scaling the SS and data quantity.

## 2 Collaborative ML Problem with Privacy Incentive

Our private collaborative ML problem setup comprises a mediator coordinating information sharing, valuation, and reward, and \(n\) parties performing a common ML task (e.g., predicting disease progression). Let the set \(N\{1,,n\}\) denote the grand coalition of \(n\) parties. Each party \(i\) owns a private dataset \(_{i}\) which cannot be directly shared with others, including the mediator. _What alternative information should each party provide to the mediator for collaborative training of an ML model?_

To ease aggregation, this work focuses only on Bayesian models with _sufficient statistic_ (SS), such as exponential family models , Bayesian linear regression , and generalized linear models, including Bayesian logistic regression  (with approximate SS).

**Definition 2.1** (Sufficient Statistic (SS) [48; 52]).: The statistic \(_{i}\) is a SS for the dataset \(_{i}\) if the model parameters \(\) and dataset \(_{i}\) are conditionally independent given \(_{i}\), i.e., \(p(|_{i},_{i})=p(|_{i})\).

We propose that each party \(i\) shares its SS \(_{i}\) for and _in place of_ its dataset \(_{i}\) to protect the privacy of \(_{i}\). We assume that the parties have agreed to adopt a common Bayesian model with the same prior \(p()\) of model parameters \(\), and each party \(i\)'s dataset \(_{i}\) is independently drawn from the likelihood \(p(_{i}|)\) that is conjugate to the prior \(p()\) (i.e., belonging to an exponential family). The mediator can compute the posterior belief \(p(|\{_{i}\}_{i N})\) of model parameters \(\) given the grand coalition \(N\)'s datasets using a function \(f_{}\) of the sum over shared SS: \(p(|\{_{i}\}_{i N}) p()\)\(f_{}(_{i N}_{i})\). We give a concrete example and the mathematical details of SS in Apps. A.1 and E, respectively.

**Privacy Incentive.** However, sharing the exact SS \(_{N}\{_{i}\}_{i N}\) will not ensure privacy as the mediator can draw inferences about individual datum in the private datasets \(_{N}\{_{i}\}_{i N}\). To mitigate the privacy risk, each party \(i\) should choose its required privacy level \(_{i}\) and enforce \((,_{i})\)-Renyi _differential privacy_.4 In Def. 2.2, a smaller \(_{i}\) corresponds to a stronger DP guarantee.4

**Definition 2.2** (Renyi Differential Privacy (DP) ).: A randomized algorithm \(:\) is \((,)\)-Renyi differentially private if for all neighboring datasets \(\) and \(^{}\), the Renyi divergence5 of order5\(>1\) is \(D_{}(()\;||\;(^{}))\).

Party \(i\) can enforce (example-level)6\((,_{i})\)-Renyi DP by applying the Gaussian mechanism: It generates perturbed SS \(_{i}_{i}+_{i}\) by sampling a Gaussian noise vector \(_{i}\) from the distribution \(p(Z_{i})=(,\;0.5\;(/_{i})\;_{2}^{2}(g)\; )\) where \(_{2}^{2}(g)\) is the squared \(_{2}\)-sensitivity6 of the function \(g\) that maps the dataset \(_{i}\) to the SS \(_{i}\). We choose Renyi DP over the commonly used \((,)\)-DP as it gives a stronger privacy definition and allows a more convenient composition of the Gaussian mechanisms , as explained in App. A.2.

Each party \(i\) will share (i) the number \(c_{i}|_{i}|\) of data points in its dataset \(_{i}\), (ii) its perturbed SS \(_{i}\),7 and (iii) its Gaussian distribution \(p(Z_{i})\) with the mediator. As DP algorithms are robust to post-processing, the mediator's subsequent operations of \(_{i}\) (with no further access to the dataset) will preserve the same DP guarantees. The mediator uses such information to quantify the impact of the DP noise and compute the DP noise-aware posterior7\(p(|\{_{i}\}_{i N})\) via _Markov Chain Monte Carlo_ (MCMC) sampling steps outlined by [3; 4; 27].

In this section, we have satisfied the privacy incentive. In Sec. 3, we assign a value \(v_{C}\) to each coalition \(C N\)'s perturbed SS \(_{C}\{_{i}\}_{i C}\) that would decrease, on average, as the DP guarantee strengthens. In Secs. 4 and 5, we outline our reward scheme: Each party \(i\) will be rewarded with model parameters sampled from \(q_{i}()\) (in short, _model reward_) for future predictions with an appropriate reward value \(r_{i}\) (decided based on \((v_{C})_{C N}\)) to satisfy collaborative ML incentives (e.g., individual rationality, fairness). Our work's main contributions, notations, and setup are detailed in Fig. 1. The main steps involved are detailed in Algo. 2.

## 3 Valuation of Perturbed Sufficient Statistics

The perturbed SS \(_{C}\) of coalition \(C\) is more valuable and assigned a higher value \(v_{C}\) if it yields a model (in our work here, the DP noise-aware posterior \(p(|_{C})\)) of higher quality. Most data valuation methods [24; 18; 63] measure the quality of an ML model by its performance on a validation set. However, it may be challenging for collaborating parties (e.g., competing healthcare firms) to create and agree on a large, representative validation set as they may prioritize accurate predictions on different inputs (e.g., patient demographics) . The challenge increases when each firm requires privacy and avoids data sharing. Other valuation methods [47; 59] have directly used the private inputs of the data (e.g., design matrix). Here, we propose to value the perturbed SS \(_{C}\) of coalition \(C\) based on the _surprise_ that it elicits from the prior belief of model parameters, as defined below:

**Definition 3.1** (Valuation via Bayesian Surprise).: The value of coalition \(C\) or its _surprise_\(v_{C}\) is the KL divergence \(D_{}(p(|_{C});p())\) between posterior \(p(|_{C})\) vs. prior \(p()\).

From Def. 3.1, a greater surprise would mean that more bits will be needed to encode the information in \(p(|_{C})\) given that others already know \(p()\). Otherwise, a smaller surprise means our prior belief has not been updated significantly. Moreover, as the valuation depends on the observed \(_{C}\), the surprise elicited by the exact SS and data points will indirectly influence the valuation. Next, by exploiting the equality of the expected Bayesian surprise and the information gain on model parameters \(\) given perturbed SS \(_{C}\) (i.e., \(_{_{C}}[v_{C}]=(;_{C})\)), we can establish the following essential properties of our valuation function:

1. [label=V0]
2. **Non-negativity.**\( C N\;_{C}\;\;v_{C} 0\;.\) This is due to the non-negativity of KL divergence.
3. **Party monotonicity.** In expectation w.r.t. \(_{C}\), adding a party will not decrease the valuation: \( C C^{} N\;\;_{_{C^{}}}[v _{C^{}}]_{_{C}}[v_{C}]\;.\) The proof (App. C.1) uses the "information never hurts" property.
4. **Privacy-valuation trade-off.** When the DP guarantee is strengthened from \(_{i}\) to a smaller \(_{i}^{s}\) and independent Gaussian noise is added to \(_{i}\) to generate \(_{i}^{s}\), in expectation, the value of any coalition \(C\) containing \(i\) will strictly decrease: Let \(v_{C}^{s}\) denote the value of coalition \(C\) with the random variable and realization of \(_{i}\) replaced by \(_{i}^{s}\). Then, \((i C)(_{i}^{s}<_{i})\;_{_ {C}}[v_{C}]>_{_{C}^{s}}[v_{C}^{s}]\;.\)

The proof of V3 (App. C.1) uses the data processing inequality of information gain and the conditional independence between \(\) and \(_{i}^{s}\) given \(_{i}\). Together, these properties address an important question of **how to ensure DP and value a party's data to enforce a privacy-valuation trade-off** (Sec. 1). Additionally, in App. C.2, we prove that in expectation, our Bayesian surprise valuation is equivalent to the alternative valuation that measures the similarity of \(p(|_{C})\) to the grand coalition \(N\)'s DP noise-aware posterior \(p(|_{N})\).

Figure 1: An overview of our private collaborative ML problem setup from party \(i\)’s perspective and our novel contributions (ideas in blue, novel combination of solutions in blue). We (i) enforce a privacy-reward trade-off (using each party \(i\)’s desire for a higher-quality model reward in collaborative ML) to deter party \(i\) from unfetterredly/overcautiously selecting an excessive DP guarantee (small \(_{i}\)), (ii) ensure DP in valuation and rewards, and (iii) preserve similarity of its model reward \(q_{i}()\) to the grand coalition \(N\)’s posterior \(p(|_{N})\) to achieve a high utility.

**Implementation.** Computing the Bayesian surprise valuation is intractable since the DP noise-aware posterior \(p(|_{C})\) and its KL divergence from \(p()\) do not have a closed-form expression. Nonetheless, there exist approximate inference methods like the _Markov chain Monte Carlo_ (MCMC) sampling to estimate \(p(|_{C})\) efficiently, as discussed in App. A.3. As our valuation function requires estimating the value of multiple coalitions and the posterior sampling step is costly, we prefer estimators with a low time complexity and a reasonable accuracy for a moderate number \(m\) of samples. We recommend KL estimation to be performed using the nearest-neighbors method , and repeated and averaged to reduce the variance of the estimate (see App. C.3 for a discussion). The nearest-neighbor KL estimator is also asymptotically unbiased; drawing more samples would reduce the bias and variance of our estimates and is more likely to ensure fairness -- for example, party \(i\)'s _sampled_ valuation is only larger than \(j\)'s if \(i\)'s _true_ valuation is higher.

**Remark.** Our valuation is based on the submitted information \(\{c_{i},_{i},p(Z_{i})\}_{i N}\) without verifying or incentivizing their truthfulness. We discuss how this limitation is shared by existing works and can be overcome by legal contracts and trusted data-sharing platforms in App. I.

## 4 Reward Scheme for Ensuring Incentives

After valuation, the mediator should reward each party \(i\) with a _model reward_ (i.e., consisting of samples from \(q_{i}()\)) for future predictions. Concretely, \(q_{i}()\) is a belief of model parameters \(\) after learning from the perturbed SS \(_{N}\). As in Sec. 3, we value party \(i\)'s model reward as the KL divergence from the prior: \(r_{i} D_{}(q_{i}();p())\). The mediator will first _decide_ the target reward value \(r_{i}^{*}\) for every party \(i N\) using \(\{v_{C}\}_{C N}\) to satisfy incentives such as fairness. The mediator will then _control_ and generate a _different_\(q_{i}()\) for every party \(i N\) such that \(r_{i}=r_{i}^{*}\) using reward control mechanisms from Sec. 5. We will now outline the incentives and desiderata for model reward \(q_{i}()\) and reward values \(r_{i}\) and \(r_{i}^{*}\) for every party \(i N\)when the grand coalition forms7.

* **DP-Feasibility.** In party \(i\)'s reward, any other party \(k\) is still guaranteed at least its original \((,_{k})\)-DP guarantee or stronger. The implication is that the generation of party \(i\)'s reward should not require more private information (e.g., SS) from party \(k\).
* **Efficiency.** There is a party \(i N\) whose model reward is the grand coalition \(N\)'s posterior, i.e., \(q_{i}()=p(|_{N})\). It follows that \(r_{i}=v_{N}\).
* **Fairness.** The target reward values \((r_{i}^{*})_{i N}\) must consider the coalition values \(\{v_{C}\}_{C N}\) and satisfy properties F1 to F4 given in  and reproduced in App. D.2. The monotonicity axiom F4 ensures using a valuation function which enforces that a _privacy-valuation trade-off_ will translate to a _privacy-reward trade-off_ and deter parties from selecting excessive DP guarantees.
* **Individual Rationality.** Each party should receive a model reward that is more valuable than the model trained on its perturbed SS alone: \( i N\;\;r_{i}^{*} v_{i}\).
* **Similarity to Grand Coalition's Model.** Among multiple model rewards \(q_{i}()\) whose value \(r_{i}\) equates the target reward \(r_{i}^{*}\), we _secondarily_ prefer one with a higher similarity \(r_{i}^{}=-D_{}(p(|_{N});q_{i}())\) to \(p(|_{N})\).8 * **Group Welfare.** The reward scheme should maximize the total reward value \(_{i=1}^{n}r_{i}\) to increase the utility of model reward for each party and achieve the aims of collaborative ML.

**Choice of desiderata.** We adopt the desiderata from  but make P1 and P2 more specific (by considering each party's actual reward \(q_{i}()\) over just its values \(r_{i}\) and \(v_{N}\)) and introduce P5. Firstly, for our Bayesian surprise valuation function, the feasibility constraint of  is inappropriate as removing a party or adding some noise realization may result in \(r_{i}>v_{N}\),9 so we propose P1 instead. Next, we recognize that party \(i\) is not indifferent to all model rewards \(q_{i}()\) with the same target reward value as they may have different utility (predictive performance). Thus, we propose our more specific P2 and a secondary desideratum P5. As P5 is considered after other desiderata, it does not conflict with existing results, e.g., design for \((r_{i}^{*})_{i N}\) to satisfy other incentives.

_Remark on Rationality._ In P4, a party's model reward is compared to the model trained its _perturbed_ SS instead of its _exact_ SS alone. This is because the mediator cannot access (and value the modeltrained on) the private exact SS. Moreover, with no restrictions on the maximum DP noise, the value of some party's exact SS may exceed the grand coalition's perturbed SS when parties require strong DP guarantees. P4 is sufficient when parties require DP when alone to protect data from curious users of their ML model [1; 3; 27]. For example, a hospital may not want doctors to infer specific patients' data. When parties do not require DP when alone, our reward scheme cannot theoretically ensure that the model reward from collaboration is better than using the exact SS. We further discuss this limitation in App. D.3.

**Design of \((r_{i}^{*})_{i N}\).** To satisfy the desiderata from  (including our fairness P3 and rationality P4 incentives), we adopt their \(\)-Shapley fair reward scheme with \((0,1]\) that sets \(r_{i}^{*}=v_{N}(_{i}/_{k N}_{k})^{}\) with Shapley value10\(_{i}(1/n)_{C N i}[^{-1} (v_{C\{i\}}-v_{C})]\). Shapley value's consideration of _marginal contribution_ (MC) to _all_ coalitions is key to ensuring strict desirability F3 such that party \(i\) obtains a higher reward than party \(k\) (despite \(v_{i}=v_{k}\)) if \(i\)'s perturbed SS adds more value to every other non-empty coalition. Applying Theorem 1 of , the mediator should set \(\) between \(0\) and \(_{i N}(v_{i}/v_{N})/(_{i}/_{k}_{k})\) to guarantee rationality. Selecting a larger \(\) incentivizes a party with a high-quality perturbed SS to share by fairly limiting the benefits to parties with lower-quality ones. Selecting a smaller \(\) reward parties more equally and increase group welfare P6. Refer to Sec. 4.2 of  for a deeper analysis of the impact of varying \(\). These results hold for any choice of \((,_{i})\).

After explaining the desiderata for model reward \(q_{i}()\) and reward values \(r_{i}\) and \(r_{i}^{*}\) for every party \(i N\), we are now ready to solve for \(q_{i}()\) such that \(r_{i}=r_{i}^{*}\).

## 5 Reward Control Mechanisms

This section discusses two mechanisms to generate model reward \(q_{i}()\) with different attained reward value \(r_{i}\) for every party \(i N\) by controlling a _single_ continuous parameter and solving for its value such that the attained reward value equates the target reward value: \(r_{i}=r_{i}^{*}\). We will discuss the more obvious reward mechanism in Sec. 5.1 to contrast its cons with the pros of that in Sec. 5.2. Both reward mechanisms do not request new information from the parties; thus, the DP post-processing property applies, and every party \(k\) is still guaranteed at least its original DP guarantee or stronger in all model rewards (i.e., P1 holds).

### Reward Control via Noise Addition

The work of  controls the reward values by adding Gaussian noise to the data outputs. We adapt it such that the mediator controls the reward value for party \(i N\) by adding Gaussian noise to the perturbed SS of each party \(k N\) instead. To generate the model reward for party \(i\) (superscripted), the mediator will reparameterize the sampled Gaussian noise vectors \(\{_{k}^{i}(,)\}_{k  N}\) to generate the further perturbed SS11

\[_{N}^{i}\{_{k}^{i} _{k}+(0.5\;\;_{2}^{2}(g_{k})\;_{i}) ^{1/2}_{k}^{i}\}_{k N}\]

where \(_{2}^{2}(g_{k})\) is the squared \(_{2}\)-sensitivity of function \(g_{k}\) that computes the exact SS \(_{k}\) from dataset \(_{k}\) (Sec. 2). Then, the mediator rewards party \(i\) with samples of model parameters \(\) from the new DP noise-aware posterior \(q_{i}()=p(|_{N}^{i})\).

Here, the scalar \(_{i} 0\) controls the additional noise variance and can be optimized via root-finding to achieve \(r_{i}=r_{i}^{*}\). The main advantage of this reward control mechanism is its interpretation of strengthening party \(k\)'s DP guarantee in all parties' model rewards (see P1). For example, it can be derived that if \(_{k}=\), then party \(k\) will now enjoy \((,1/_{i})\)-DP guarantee in party \(i\)'s reward instead. If \(_{k}<\), then party \(k\) will now enjoy a stronger \((,_{k}/(1+_{i}_{k}))\)-DP guarantee since \(_{k}/(1+_{i}_{k})<_{k}\).

However, this mechanism has some disadvantages. Firstly, for the same scaled additional noise variance \(_{i}\), using different noise realizations \(\{_{k}^{i}\}_{k N}\) will lead to model reward \(q_{i}()\) with varyingsimilarity \(r^{}_{i}\) to the grand coalition \(N\)'s posterior. The mechanism cannot efficiently select the best model reward with higher \(r^{}_{i}\) (P5). Secondly, the value of \(r_{i}\) computed using such \(^{i}_{k}\) may be non-monotonic12 in \(_{i}\) (see Fig. 2d), which makes it hard to bracket the smallest root \(_{i}\) that solves for \(r_{i}=r^{*}_{i}\). To address these disadvantages, we will propose the next mechanism.

### Reward Control via Likelihood Tempering

Intuitively, a party \(i\) who is assigned a lower target reward value \(r^{*}_{i}<v_{N}\) should be rewarded with posterior samples of model parameters \(\) that use _less_ information from the datasets and SS of all parties. Sparked by the diffuse posterior algorithm , we propose that the mediator can generate such "less informative" samples for party \(i\) using the _normalized_ posterior13

\[q_{i}() p()[p(_{N}|)]^{_ {i}}\] (1)

involving the product of the prior \(p()\) and the data likelihood \(p(_{N}|)\) to the power of (or, said in another way, tempered by a factor of) \(_{i}\). Notice that setting \(_{i}=0\) and \(_{i}=1\) recover the prior \(p()\) and the posterior \(p(|_{N})\), respectively. Thus, setting \(_{i}(0,1)\) should smoothly interpolate between both. We can optimize \(_{i}\) to control \(q_{i}()\) so that \(r_{i}=r^{*}_{i}<v_{N}\).

But, _how do we temper the likelihood?_ We start by examining the easier, non-private setting. In Sec. 2, we stated that under our assumptions, the posterior \(p(|_{N})\) can be computed by using the sum of data quantities \(\{c_{k}\}_{k N}\) and sum of exact SS \(_{N}\). In App. E, we further show that using the tempered likelihood \([p(_{N}|)]^{_{i}}\) is equivalent to scaling the data quantities and the exact SS \(_{N}\) by the factor \(_{i}\) beforehand. In the private setting, the mediator can similarly scale the data quantities, the perturbed SS in \(_{N}\) (instead of the inaccessible exact SS), and the \(_{2}\)-sensitivity by the factor \(_{i}\) beforehand; see App. E.3 for details. This likelihood tempering mechanism addresses both disadvantages of Sec. 5.1:

* There is no need to sample additional DP noise. We empirically show that tempering the likelihood produces a model reward that _interpolates_ between the prior vs. posterior (in App. G) and _preserves_ a higher similarity \(r^{}_{i}\) to the grand coalition \(N\)'s posterior (P5 and hence, more group welfare P6) and better predictive performance than noise addition (see Sec. 6).
* Using a smaller tempering factor \(_{i}\) provably decreases the attained reward value \(r_{i}\) (see App. E). Thus, as the relationship between \(r_{i}\) and \(_{i}\) is monotonic, we can find the only root by searching the interval \(\).

**Remark.** Our discussion on improving the estimate of \(v_{C}\) in the paragraph on implementation in Sec. 3 also applies to the estimate of \(r_{i}\) in Secs. 5.1 and 5.2. Thus, solving for \(_{i}\) or \(_{i}\) to achieve \(r_{i}=r^{*}_{i}\) using any root-finding algorithm can only be accurate up to the variance in our estimate.

## 6 Experiments and Discussion

This section empirically evaluates the privacy-valuation and privacy-reward trade-offs (Sec. 6.1), reward control mechanisms (Sec. 6.2), and their relationship with the utility of the model rewards (Sec. 6.3). The time complexity of our scheme is analyzed in App. F and baseline methods are discussed in App. H.3. We consider _Bayesian linear regression_ (BLR) with unknown variance on the Syn and CaliH datasets, and _Bayesian logistic regression_ on the Diab dataset with \(3\) collaborating parties (see App. H.1 for details) and enforce \((2,_{i})\)-Renyi DP. For **Synthetic BLR (Syn)**, we select and use a _normal inverse-gamma_ distribution (i) to generate the true regression model weights, variance, and a 2D-dataset and (ii) as our model prior \(p()\). We consider \(3\) parties with \(c_{1}=100\), \(c_{2}=200\), \(c_{3}=400\) data points, respectively. For **Californian Housing dataset (CaliH)**, as in , \(60\%\) of the CaliH data is deemed "public/historic" and used to pre-train a neural network without DP. Real estate firms may only care about the privacy of their newest transactions. As the parties' features-house values relationship may differ from the "public" dataset, we do transfer learning and selectively retrain only the last layer with BLR using the parties' data. Parties \(1\) to \(3\) have,respectively, \(20\%,30\%\), and \(50\%\) of the dataset with \(6581\) data points and \(6\) features. For **PIMA Indian Diabetes classification dataset (Diab)**, we use a Bayesian logistic regression model to predict whether a patient has diabetes based on sensitive inputs (e.g., patient's age, BMI, number of pregnancies). To reduce the training time, we only use the \(4\) PCA main components as features (to generate the approximate SS) . Parties \(1,2\), and \(3\) have, respectively, \(20\%\), \(30\%\), and \(50\%\) of the dataset with \(614\) data points. As we are mainly interested14 in the impact of one party controlling its privacy guarantee \(_{i}\), for all experiments, we only vary party \(2\)'s from the default \(0.1\). We fix the privacy guarantees of others (\(_{1}=_{3}=0.2\)) and \(=0.2\) in the \(\)-Shapley fair reward scheme, and analyze party \(2\)'s reward and utility. Note that as \(_{2}\) increases (decreases), party \(2\) becomes the most (least) valuable of all parties.

### Privacy-valuation and Privacy-reward Trade-offs

For each dataset, we only vary the privacy guarantee of party \(i=2\) with \(_{2}[0.004,0.02,0.1,0.5,2.5,12.5]\) and use the Gaussian mechanism and a fixed random seed to generate the perturbed SS \(_{2}\) from the exact SS \(_{2}\). Fig. 2a-c plot the mean and shades the standard error of \(v_{i}\), \(v_{N}\), \(_{i}\), and \(r_{i}\) over \(5\) runs. The privacy-valuation and privacy-reward trade-offs can be observed: As the privacy guarantee weakens (i.e., \(_{2}\) increases), party \(2\)'s valuation \(v_{2}\), Shapley value \(_{2}\), and attained reward value \(r_{2}\) increase. When \(_{2}\) is large, party \(2\) will be the most valuable contributor and rewarded with \(p(|_{N})\), hence attaining \(r_{i}=v_{N}\). App. H.5 shows that the trade-offs do not hold for non-noise-aware inference.

### Reward Control Mechanisms

We use the Syn experiment to compare the reward mechanisms that vary the noise addition using \(_{i}\) (Sec. 5.1) vs. temper the likelihood using \(_{i}\) (Sec. 5.2). The mechanisms control \(q_{i}()\) (i.e., used to generate party \(i\)'s model reward) to attain the target reward values. For each value of \(_{i}\) and \(_{i}\) considered, we repeat the posterior sampling and KL estimation method \(5\) times. Figs. 2d and 2e-f use different sets of sampled noise \(\{_{k}^{i}\}_{k N}\) to demonstrate the stochastic relationship between \(r_{i}\) and \(_{i}\). In Fig. 2d, the non-monotonic disadvantage of noise addition can be observed: As \(_{i}\) increases, \(r_{i}\) does not consistently decrease, hence making it hard to solve for the smaller \(_{i}\) that attains \(r_{i}^{*}=3\). In contrast, as \(_{i}\) decreases from \(1\), \(r_{i}\) consistently decreases. Furthermore, in Fig. 2f, we demonstrate the other advantage of likelihood tempering: For the same \(r_{i}\), tempering the likelihood leads to a higher similarity \(r_{i}^{}\) to the posterior \(p(|_{N})\) than noise addition. In App. H.6, we report the relationship between \(r_{i}\) vs. \(_{i}\) and \(_{i}\) for the other real-world datasets.

### Utility of Model Reward

The utility (or the predictive performance) of both Bayesian models can be assessed by the _mean negative log probability_ (MNLP) of a non-private test set.15 In short, MNLP reflects how unlikely the test set is given the perturbed SS and additionally cares about the uncertainty/confidence in the model predictions (e.g., due to the impact of DP noise). MNLP will be higher (i.e., worse) when the model

Figure 2: (a-c) Graphs of party \(2\)’s valuation \(v_{2}\), Shapley value \(_{2}\), attained reward value \(r_{2}\) vs. privacy guarantee \(_{2}\) for various datasets. (d-e) Graphs of attained reward value \(r_{i}\) vs. \(_{i}\) (Sec. 5.2) and \(_{i}\) (Sec. 5.1) for \(2\) different noise realizations. (f) Graph of similarity \(r_{i}^{}\) to grand coalition \(N\)’s posterior \(p(|_{N})\) vs. \(r_{i}\) for Syn dataset corresponding to (e).

is more uncertain of its accurate predictions or overconfident in inaccurate predictions on the test set; see App. H.2 for an in-depth definition.

**Privacy trade-offs.** Figs. 3a-c illustrate the privacy-utility trade-off described in Sec. 1: As \(_{2}\) decreases (i.e., privacy guarantee strengthens), the MNLP\({}_{N}\) of grand coalition \(N\)'s collaboratively trained model and the MNLP\({}_{i}\) of party \(i=2\)'s individually trained model generally increase, so their utilities drop (\(\)). This motivates the need to incentivize party \(2\) against selecting an excessively small \(_{2}\) by enforcing privacy-valuation and privacy-reward trade-offs. From Figs. 3a-c, the impact of our scheme _enforcing_ the trade-offs can be observed: As \(_{2}\) decreases, the MNLP\({}_{r}\) of party \(i=2\)'s model reward increases.

_Remark_.: In Fig. 3c, an exception to (\(\)) is observed. The exception illustrates that the privacy-valuation trade-off may not hold for a valuation function based on the performance on a validation set.

**Individual rationality.** It can be observed from Figs. 3a-c that as \(_{2}\) decreases, the MNLP\({}_{r}\) of party \(i=2\)'s model reward increases much less rapidly than the MNLP\({}_{i}\) of its individually trained model. So, it is rational for party \(i=2\) to join the collaboration to get a higher utility.

_Remark_.: Party \(i=2\)'s utility gain appears small when \(_{2}\) is large due to parties \(1\) and \(3\)'s selection of strong privacy guarantee \(=0.2\). Party \(i\) can gain more when other parties require weaker privacy guarantees such as \(=2\) instead (see App. H.5).

**Likelihood tempering is a better reward control mechanism.** Extending Sec. 6.2, we compare the utility of party \(i\)'s model reward generated by noise addition vs. likelihood tempering in Figs. 3d-f. Across all experiments, likelihood tempering (with \(_{i}\)) gives (i) a lower MNLP\({}_{r}\) and hence a higher utility, and (ii) a lower variance in MNLP\({}_{r}\) than varying the noise addition (with \(_{i}\)).

## 7 Related Works

Fig. 5 in App. B gives a diagrammatic overview showing how our work fills the gap in existing works.

**Data Valuation.** Most data valuation methods are not differentially private and directly access the data. For example, computing the information gain  or volume  requires the design matrix. While it is possible to make these valuation methods differentially private (see App. H.3) or value DP trained models using validation accuracy (on an agreed, _public_ validation set), the essential properties of our valuation function (V2-V3) may not hold.

**Privacy Incentive.** Though the works of  reward parties directly proportional to their privacy budget, their methods do not incentivize data sharing as a party does not fairly receive a higher reward value for contributing a larger, more informative dataset. While the work of  artificially creates a privacy-reward trade-off by paying each party \(i\) the product of its raw data's Shapley value \(_{i}\) and a monotonic transformation of \(_{i}\), it neither ensures DP w.r.t. the mediator nor fairly considers how a stronger DP guarantee may reduce party \(i\)'s marginal contribution to others (hence \(_{i}\)). The work of  considers data acquisition from parties with varying privacy requirements but focuses on the mean estimation problem and designing payment and privacy loss functions to get parties to report their true unit cost of privacy loss. Our work here distinctively considers Bayesian models and fairness and allows parties to choose their privacy guarantees directly while explicitly enforcing a privacy-reward trade-off.

Figure 3: (a-c) Graphs of utility of party \(2\)’s model reward measured by MNLP\({}_{r}\) vs. privacy guarantee \(_{2}\) for various datasets. (d-f) Graphs of utility of model reward measured by MNLP\({}_{r}\) vs. attained reward value \(r_{i}\) under the two reward control mechanisms for various datasets.

**Difficulties ensuring incentives with existing DP/FL works.** The _one posterior sampling_ (OPS) method  proposes that each party \(i\) can achieve DP by directly releasing samples from the posterior \(p(|_{i})\) (if the log-likelihood is bounded). However, OPS is data inefficient and may not guarantee privacy for approximate inference . It is unclear how we can privately value a coalition \(C\) and sample from the joint posterior \(p(|\{_{i}\}_{i C})\). DP-FedAvg/DP-FedSGD  or DP-SGD  enable collaborative but private training of neural networks by requiring each party \(i\) to clip and add Gaussian noise to its submitted gradient updates. However (in addition to the valuation function issue above), it is tricky to ensure that the parties' rewards satisfy data sharing incentives. In each round of FL, parties selected will receive the (same) latest model parameters to compute gradient updates. This setup goes against the fairness (P3) incentive as parties who share less informative gradients should be rewarded with lower quality model parameters instead. Although the unfairness may potentially be corrected via gradient-based  or monetary rewards, there is no DP reward scheme to guarantee a party better model reward from collaboration than in solitude or a higher monetary reward than its cost of participation, hence violating individual rationality.

## 8 Conclusion

Unlike existing works in collaborative ML that solely focus on the fairness incentive, our proposed scheme further (i) ensures privacy for the parties during valuation and in model rewards and (ii) enforces a privacy-valuation trade-off to deter parties from unfetteredly selecting excessive DP guarantees to maximize the utility of collaboratively trained models.16 This involves novelly combining our proposed Bayesian surprise valuation function and reward control mechanism with DP noise-aware inference. We empirically evaluate our scheme on several datasets. Our likelihood tempering reward control mechanism consistently preserves better predictive performance.

Our work has two limitations which future work should overcome. Firstly, we only consider ML models with SS (see App. A.1 for applications) and a single round of sharing information with the mediator as a case study to show the incentives and trade-offs can be achieved. Future work should generalize our scheme to ML models without an explicit SS.

Next, like the works of  and others, we do not consider the truthfulness of submitted information and value data _as-is_. This limitation is acceptable for two reasons. 1) Parties such as hospitals and firms will truthfully share information as they are primarily interested in building and receiving a model reward of high quality and may additionally be bound by the collaboration's legal contracts and trusted data-sharing platforms. For example, with the use of X-road ecosystem,17 parties can upload a private database which the mediator can query for the perturbed SS. This ensures the authenticity of the data (also used by the owner) and truthful computation given the uploaded private database. 2) Each party would be more inclined to submit true information as any party \(k\) who submits fake SS will reduce its utility from the collaboration. This is because party \(k\)'s submitted SS is used to generate \(k\)'s model reward and cannot be replaced locally as party \(k\) will only receive posterior samples. Future work should seek to verify and incentivize truthfulness.