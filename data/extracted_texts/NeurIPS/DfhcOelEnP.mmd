# cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers

Anirudh Sundar\({}^{*}\)  Jin Xu\({}^{*}\)  William Gay\({}^{*}\)  Christopher Richardson\({}^{*}\)  Larry Heck

AI Virtual Assistant Lab

Georgia Institute of Technology

\({}^{*}\) Equal Contribution

{asundar34, jxu81, wgay7, crichardson8, larryheck}@gatech.edu

###### Abstract

An emerging area of research in situated and multimodal interactive conversations (SIMMC) includes interactions in scientific papers. Since scientific papers are primarily composed of text, equations, figures, and tables, SIMMC methods must be developed specifically for each component to support the depth of inquiry and interactions required by research scientists. This work introduces Conversational Papers (cPAPERS), a dataset of conversational question-answer pairs from reviews of academic papers grounded in these paper components and their associated references from scientific documents available on arXiv. We present a data collection strategy to collect these question-answer pairs from OpenReview and associate them with contextual information from LaTeX source files. Additionally, we present a series of baseline approaches utilizing Large Language Models (LLMs) in both zero-shot and fine-tuned configurations to address the cPAPERS dataset.

## 1 Introduction

Developing conversational assistants capable of situated and multimodal interactive conversations (SIMMC) over structured knowledge sources remains an open problem .

An emerging area of research within this domain is conversational interactions over scientific documents . The number of scientific articles has grown dramatically over the past decade , making it difficult for scientists to find, read, understand, and connect advancements published by their fellow researchers. Scientists in the field of biomedicine, for example, publish over 1 million articles per year or a new article every 2 minutes on average . Developing methods to understand scientific documents and assist researchers is an important problem, especially for the Natural Language Processing (NLP) community. Scientific documents present an interesting challenge in SIMMC-based methods since the content is frequently multimodal .

Besides textual paragraphs, researchers rely on various modalities to describe research methods. Figures convey information about concepts developed throughout the paper. Generally, figures represent many different types of information including most commonly graphical and image information. For example, figures explain model architectures and pipelines, summarize experimental results in graphical plots, and represent complex information such as gradient learning surfaces and feature maps. In addition, figures can be used to show various stages of image processing in computer vision and image generation papers.

Equations are crucial for grasping mathematical concepts in scientific texts but can be challenging to interpret. Equations frequently rely on notation introduced elsewhere in the document, often requiring readers to draw from the entire paper to understand the formulation. In addition, equationsare typically related to and in many cases help the reader understand other paper components such as figures and tables (and vice-versa).

Finally, tables typically have structures that convey semantic information. These include the text in the row and column headings, and text and often links in the table cells. These structures summarize multiple concepts (e.g., experimental results) in a human-readable form. Therefore, developing SIMMC methods for scientific papers must include a semantic understanding of these structures and table content.

To advance the development of conversational assistants capable of SIMMC in scientific papers, this paper introduces Conversational Papers (cPAPERS1), a dataset of conversations in English situated in equations (cPAPERS-EQNS), figures (cPAPERS-FIGS), and tabular information (cPAPERS-TBLS) for scientific texts. Question-answer pairs are sourced from reviews and rebuttals from OpenReview.2 Textual grounding to answer the questions and answers are sourced from the corresponding scientific articles hosted on arXiv 3, an open-access repository of academic preprints.

The contributions of this work include:

* Introduction of the Conversational Papers (cPAPERS) dataset including three splits (EQNS, FIGS, TBLS)
* Development of a novel and scalable approach for collecting question-answer pairs from OpenReview and linking them with relevant contextual information from open access TeX sources on arXiv
* Creation of baseline LLM approaches that utilize weakly grounded multimodal context for dialogue responses

The rest of the paper is organized as follows: Section 2 discusses related work in language modeling and question-answering for equations, tables, and figures. Section 3 provides a description of cPAPERS and details the dataset collection process. Next, Section 4 describes baseline approaches to address this dataset with a series of experiments utilizing zero-shot prompting and parameter-efficient fine-tuning. Section 5 discusses the results, while Sections 6 and 7 address the conclusion and limitations, respectively.

## 2 Related Work

### Language Modeling for Equations

The challenge of modeling mathematical equations has become an active research area in Natural Language Processing.  proposes an encoder-decoder approach to generate equations from math word problems.  presents an approach for the dual problem of generating math word problems consistent with equations. Prior work has also proposed learning representations from equations using the Transformer encoder , Tree-based encoders , and RNNs . More recent work utilizing generative architectures includes MathGPT , an auto-regressive model based on GPT-2  for various language+equation tasks, and Nugat , a Visual Transformer to parse academic documents to markup language. In contrast, cPAPERS is a dataset that addresses both grounded conversational question-answering and the modeling of mathematical equations. Answering the questions accurately requires understanding the expressions in the equations while also formulating an accurate response that addresses the task succinctly.

### Question-Answering over Figures

Early datasets on question-answering over figures include VQA , Visual7W , VisDial , ManyModalQA , and OK-VQA [32; 42]. More recent datasets addressing the problem of open-domain conversations include IGC , MOD , and Image-Chat . However, the imagesin these datasets are collected from MS-COCO  or YFCC100M , where visual content targets commonly seen everyday objects as opposed to scientific documents where images target specific information relevant to the explanation of a concept.

Recent work has addressed some of the challenges associated with multimodal image+text tasks situated in scientific documents.  presents a dataset of charts from scientific papers and associated natural language captions summarizing the information present in the chart.  develops a method to link labels with relevant images in patents.  introduces a dataset of 150 computer science papers, ground truth labels for the locations of figures, tables, and captions, and an approach to automatically extract this information from PDFs. SciCap is a much larger dataset of 400,000 figures, their captions, and associated textual references from various scientific papers. The dataset is collected by scraping scientific preprints from arXiv. For each figure, they provide the associated caption and all paragraphs that mention the figure.

While these datasets address tasks related to multimodal content in scientific documents, they are not conversational in nature. In contrast, cPAPERS addresses the shortcomings of prior datasets as a multimodal, conversational dataset grounded in scientific documents. The questions and answers address specific contextual visual information grounded in scientific documents.

### Tabular Question-Answering

Tabular question-answering addresses the problem of extractive QA grounded in the information contained in specific cells of a table. Prior tabular QA datasets include those collected from Wikipedia, such as WikiTableQuestions, ManyModalQA , TaBERT , NQ-Tables , FEVEROUS , FeTaQA , HybridDialogue , and HiTab . Other tabular datasets

Figure 1: Overview of the data collection approach. First, the reviews are obtained from OpenReview. The corresponding TeX source is obtained from arXiv (only if the paper is also posted to arXiv). The reviews and rebuttals are filtered using a combination of LLMs and Amazon Mechanical Turk to obtain question-answer pairs in the json format. The TeX source is parsed to obtain the associated equation, table, or figure to create a dataset of questions and answers paired with the corresponding TeX.

are constructed from financial reports - TAT-QA , FinQA , MultiHiertt, or arXiv - iTBLS .

Proposed approaches to address the tabular QA task include architectures based off of the Transformer encoder [57; 24; 8; 16; 31; 20; 55], decoder [18; 3; 58; 26; 59; 46], or both (encoder-decoder) [35; 14; 49; 43; 48].

In contrast to prior tabular datasets, cPAPERS-TBLS presents a new source of grounded questions and answers and to the best of our knowledge, is the first to be situated in tables from OpenReview. Additionally, the question-answer pairs are not factoids. Rather, they are conversational in nature.

## 3 The cPAPERS Dataset

### Dataset Description

Conversational Papers (cPAPERS) is a dataset of conversations in English situated in scientific texts. cPAPERS consists of question-answer pairs pertaining to figures (cPAPERS-FIGS), equations (cPAPERS-EQNS), or tabular information (cPAPERS-TBLS) from scientific papers. These pairs are sourced from the official reviews and rebuttals of The Conference and Workshop on Neural Information Processing Systems (NeurIPS) and The International Conference on Learning Representations (ICLR) between 2020 and 2023, as available on OpenReview. cPAPERS comprises of 1723 question-answer pairs over equations, 1601 pairs over tables, and 1706 pairs over figures, totaling 5030 question-answer pairs spanning 2350 unique scientific papers. A detailed breakdown of the dataset is provided in Table 1.

Section 3.2 details the steps of collecting cPAPERS from official reviews on OpenReview, extracting question-answer pairs, and associating them with LaTeX source files from arXiv.

### Dataset Collection

#### 3.2.1 Step 1 - Extract Official Review and Comments

We leverage the OpenReview API to access official reviews and comments for each paper submission. These reviews, authored by conference reviewers, typically include a paper summary, strengths and weaknesses, specific questions, and limitations. Authors respond to these critiques in the comments, providing answers that address the reviewers' questions. This step involves downloading the official reviews and comments for all submissions.

#### 3.2.2 Step 2 - Download LaTeX files

For each paper recorded in Step 1, we initiate a call to the arXiv API to verify the existence of the paper. If the paper is found, we proceed to download all associated LaTeX source files. If a paper cannot be located using the arXiv API, we exclude this submission from further processing.

    &  &  &  \\  & train & dev & test & train & dev & test & train & dev & test \\  \# Unique Papers & 672 & 286 & 335 & 715 & 285 & 302 & 761 & 275 & 313 \\ \# QA Pairs & 993 & 336 & 394 & 932 & 327 & 342 & 1052 & 297 & 357 \\  \# Tokens (average) & & & & & & & & & \\  & train & dev & test & train & dev & test & train & dev & test \\  Question & 25 & 25 & 25 & 24 & 22 & 26 & 23 & 23 & 24 \\ Answer & 92 & 102 & 90 & 86 & 79 & 81 & 83 & 88 & 81 \\ Contexts & 10,232 & 11,851 & 12,288 & 2,981 & 2,746 & 2,610 & 433 & 400 & 431 \\ References & 7,323 & 8,413 & 9,517 & 1,757 & 1,645 & 1,427 & 366 & 375 & 323 \\ Neighboring Contexts & 1,144 & 1,153 & 1,084 & 994 & 1,043 & 925 & - & - & - \\ Neighboring References & 1,000 & 947 & 1,152 & 736 & 657 & 588 & - & - & - \\   

Table 1: Dataset Statistics

#### 3.2.3 Step 3 - Regex Filtering

As mentioned in Step 1, review-rebuttal pairs contain a summary of the paper and additional information that may not be related to equations, tables, or figures. Therefore, we use regular expressions to identify mentions of equations, tables, and figures within each review and comment pair. The following regular expressions are employed for this purpose:

Equation: re.compile(r'(?:Equation|Eq\_?)*\((?*+))?',re.IGNORECASE) Table: re.compile(r'*+|',re.IGNORECASE) Figure: re.compile(r'(?:Figure|fig)\.?*+',re.IGNORECASE) These expressions efficiently determine whether equations, tables, and figures are mentioned in either the review or the rebuttal, resulting in a separate list of review-rebuttal pairs each containing explicit references to an equation, table, or figure in the paper. It is possible for a review to contain references to multiple modalities.

#### 3.2.4 Step 4 - Question-Answer Extraction

To extract question-answer pairs from the regex-filtered reviews and rebuttals, an LLM that is instruction fine-tuned to align with human preferences is prompted _in-context_ with a single example. For this process, we use Llama-2-70b-chat-hf.

The prompt is as follows:

_'Given this example Content, Question, and Answer, look through the Summary and Comments that are dedicated to the paper and extract Question and Answer pairs that specifically target a particular Figure/Equation/Table in the content. Questions may be under sections labeled similarly to "Weaknesses" or "Questions" or "Review" or "Clarity, Quality, Novelty And Reproducibility" in the comments, and Answers may be under sections labeled "Response to Reviewer" or "Rebuttal" or "Comments"._

System Prompt: _"You are a helpful question answer extracting assistant. Our goal is to extract Question and Answer pairs pertaining to a specific a Figure/Equation/Table in the content. Your response should be in the format of [Question] <question> [Answer] <answer> Do not add any other unnecessary content in your response."_

**Post-processing + JSON reformatting.** GPT 3.5  is used to reformat the question-answer pairs as JSON. The following system prompt is used.

Prompt: _You are a helpful assistant. Please find the Question-Answer pairs, and format it as a json (["question_answ": [[ "question": "", "answer": "" ],...])_

For each question-answer pair, an additional regular expression is employed to extract the respective figure, equation, or table number following the keywords 'Figure', 'Fig', 'Equation', 'Eq', or 'Table'.

pattern = r'+' numbers = re.findall(pattern, input_string)

#### 3.2.5 Step 5 - Crowdworker Cleanup

A significant portion of academic reviews and rebuttals on OpenReview pertain to clarification questions as well as those related to fixing typos. While the LLM processing removes most of the questions and answers addressing typos, to further ensure the quality of the dataset, we employ crowd workers. The crowd workers (recruited using Amazon Mechanical Turk) ascertain whether a question-answer pair about an equation, table, or figure is technical in nature or not. For each question-answer pair, two crowd workers provide feedback and only those question-answer pairs with a consensus are retained. Additional details of the crowdworker setup is provided in Appendix A.5.

#### 3.2.6 Step 6 - Contextualizing QA Pairs

In addition to extracting question-answer pairs from OpenReview, cPAPERS associates these question-answer pairs with referring text from the LaTeX source on arXiv. Often, the text surrounding figures,equations, and tables in scientific documents provides additional relevant information. Regular expressions are employed to locate all instances of figures, equation, and tables in the LaTeX source using the , , and 
 environments.

**Equation:** The equation is obtained by extracting content enclosed between  and  tags. We provide all equations from the LaTeX source for a question-answer pair.

**Table:** The table is extracted from the content enclosed between 
 and  tags. We provide all tables from the LaTeX source for a question-answer pair.

**Figure:** First, the content between 
 and  tags is extracted. From this content, we obtain the caption (using ) and the figure path (using ).

Additionally, for each modality, we provide context and references:

**Context:** The context represents the paragraph of text preceding and the paragraph of text following the equation, table, or figure referred to in the question-answer pair.

**References:** In addition, we report all referring text in a document associated with the equation, table, or figure in the question-answer pair. Within each environment, we search for a label () and if it exists, we locate all references by searching for text using the label in LaTeX's  command.

Reviews of academic pre-prints provide a source for high-quality document-grounded questions and answers and success on the task requires contextual and multimodal understanding. However, one of the main challenges with matching questions and answers from OpenReview with their corresponding multimodal context from arXiv is that while reviewer and author references to equations, tables, and figures are purely ordinal (e.g. 'Equation 3', 'Figure 4', 'Table 2'), the LaTeX source does not provide any reliable approach to associate this ordinal position with the multimodal context in the final compiled document, specifically for equations and tables. Additionally, arXiv contains multiple versions of papers with no guarantees on consistency with the submission to OpenReview. As a result, the cPAPERS dataset reports both the position of the equation, table, or figure referred to in the question, and all equations and tables from that specific paper. Section 4 contains additional details on how this information is used in a baseline approach.

#### 3.2.7 Step 7 - Additional Post-processing of Figures

Authors frequently utilize different graphical formats in their papers. To ensure consistency in our dataset, we convert all.pdf and.eps files to.png format using ImageMagick. 4

## 4 Baseline Approaches

We experiment with zero-shot prompting and parameter-efficient fine-tuning for the baseline approaches. Zero-shot prompting involves querying pre-trained LLMs to answer the questions in the cPAPERS dataset without additional fine-tuning. In this approach, we use _off-the-shelf_ Transformer decoder LLMs already pre-trained using a causal language modeling objective followed by supervised fine-tuning and reinforcement learning with human feedback to follow instructions. The LLM is not fine-tuned on cPAPERS and we do not provide examples in-context.

In addition, we experiment with parameter-efficient fine-tuning using QLoRA  to better align the model with the style of responses in the cPAPERS dataset. An LLM is fine-tuned independently for each modality (equation, table, or figure).

We serialize equations and tables using the corresponding LaTeX representations from the LaTeX source on arXiv. As previously indicated in Section 3.2.6, pairing questions and answers from OpenReview with LaTeX sources from arXiv presents a challenge with possible inconsistencies between the references in questions and answers to the corresponding sources in LaTeX. Therefore, we conduct question-answering experiments under two settings - utilizing all multimodal content, or utilizing a smaller subset of neighboring, weakly-grounded multimodal content. While using all multimodal content (equations or tables) ensures that the correct equation (or table) referred to by the question-answer pair is provided to the model, limitations on context length constrain the model from accurately using this information. Evidenced by prior work demonstrating the retrieval capabilities of generative LLMs when provided weakly-grounded context , we reconcile this situation by providing the model with a subset of equations or tables from the entire paper. First, we assign each equation or table with an ordinal position based on each instance of a 

Table 2: Zero-shot language modeling on cPAPERS-EQNS test set with Llama-2-70B

## 5 Results

The following sections contain results of the baseline approaches on the development and test splits. We report results from automatic evaluation - ROUGE 5, METEOR 6, BERTScore 7, and BLEU 8. ROUGE, METEOR, and BLEU respectively measure the recall, F1 score, and precision of n-grams between model generated and reference text. BERTScore leverages contextual embeddings to measure the semantic similarity of model generated responses to the ground truth.

**Zero-shot.** First, we experiment with zero-shot language modeling with Llama-2-70B  to answer questions on cPAPERS-EQNS, cPAPERS-TBLS, and cPAPERS-FIGS. The results from Tables 2 and 3 indicate that utilizing the neighboring equations/tables provides a significant improvement over using all equations/tables to answer questions in the dataset, evidenced by up to 2x improvement in ROUGE, METEOR, and BERTScores. Additionally, the results from Table 2 indicate that utilizing the question alone without neighboring textual context (paragraphs of text surrounding the equation) results in best ROUGE and METEOR scores on cPAPERS-EQNS, whereas utilizing referring text yields the best scores on these metrics in cPAPERS-TBLS when compared to using the question alone to generate answers (Table 3). We posit this is because referring text in the document is predominantly used to summarize tabular results.

Table 4 contains results for zero-shot language modeling on cPAPERS-FIGS. In this situation, utilizing context performs the best across metrics.

  
**Modality** & **Setting** & **ROUGE-1** & **ROUGE-2** & **ROUGE-L** & **METEOR** & **BERT** & **BLEU** \\  Question (Q) & - & **0.194** & **0.065** & **0.144** & 0.240 & **0.825** & **0.038** \\   & Neighboring & 0.190 & 0.063 & 0.139 & **0.245** & 0.821 & 0.035 \\  & All & 0.170 & 0.056 & 0.123 & 0.218 & 0.740 & 0.035 \\   & Neighboring & 0.186 & 0.063 & 0.137 & 0.237 & 0.809 & 0.036 \\  & All & 0.079 & 0.027 & 0.058 & 0.102 & 0.345 & 0.036 \\   & Neighboring & 0.176 & 0.061 & 0.129 & 0.223 & 0.764 & 0.036 \\  & All & 0.112 & 0.037 & 0.082 & 0.143 & 0.498 & 0.037 \\   

Table 2: Zero-shot language modeling on cPAPERS-EQNS test set with Llama-2-70B

  
**Modality** & **Setting** & **ROUGE-1** & **ROUGE-2** & **ROUGE-L** & **METEOR** & **BERT** & **BLEU** \\  Question (Q) & - & 0.192 & 0.058 & 0.136 & 0.232 & **0.832** & 0.028 \\   & Neighboring & 0.206 & 0.061 & **0.145** & 0.237 & 0.828 & **0.031** \\  & All & 0.176 & 0.052 & 0.123 & 0.199 & 0.697 & 0.030 \\   & Neighboring & 0.202 & 0.062 & 0.142 & 0.241 & 0.828 & **0.031** \\  & All & 0.160 & 0.050 & 0.114 & 0.195 & 0.666 & 0.030 \\   & Neighboring & **0.207** & **0.064** & 0.144 & **0.243** & 0.829 & **0.031** \\  & All & 0.186 & 0.057 & 0.130 & 0.223 & 0.758 & 0.030 \\   

Table 3: Zero-shot language modeling on cPAPERS-TBLS test set with Llama-2-70B

[MISSING_PAGE_FAIL:8]

assistants capable of situated and multimodal interactive conversation within scientific papers. We conduct a series of experiments with zero-shot prompting to benchmark state-of-the-art models performance and perform several parameter-efficient fine-tuning for the baseline approaches. The experimental results provide valuable insights and create opportunities for enhancing and innovating the development of future conversational AI assistant.

## 7 Discussion

This paper introduces an innovative and scalable approach for collecting a large dataset of situated and multimodal interactive conversations related to scientific papers. This approach has the potential to facilitate the collection of extensive question-answer pairs across a wide range of scientific fields with ease. Collecting such a large and diverse dataset would benefit the research community by providing opportunities to develop more advanced AI-based research assistants for scientific research.

There may be concerns that creating an AI assistant to help automate the scientific discovery process could diminish the role of human scientists. While this may be a possible use of this dataset, our intention is to develop an AI research assistant that amplifies human scientists, empowering them to achieve greater scientific discoveries more efficiently.

**Limitations.** The key limitation of this dataset is the presence of mismatched figures, tables, or equations across different versions of the manuscripts. The _.tex_ files of the paper on arXiv often undergo multiple revisions, and comments on Open Review are typically specific to a particular version. Authors frequently make changes in response to reviewer comments, which may involve additions, removals, or reordering of figures, equations, or tables. This presents a potential mismatch between the figures, equations, or tables and the question-and-answer pairs, thereby introducing additional challenges for language modeling.