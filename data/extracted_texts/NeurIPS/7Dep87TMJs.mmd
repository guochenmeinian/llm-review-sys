# Learning with Fitzpatrick Losses

Seta Rakotomandimby

Ecole des Ponts

seta.rakotomandimby@enpc.fr

&Jean-Philippe Chancelier

Ecole des Ponts

jean-philippe.chancelier@enpc.fr

Michel De Lara

Ecole des Ponts

michel.delara@enpc.fr

&Mathieu Blondel

Google DeepMind

mblondel@google.com

###### Abstract

Fenchel-Young losses are a family of loss functions, encompassing the squared, logistic and sparsemax losses, among others. They are convex w.r.t. the model output and the target, separately. Each Fenchel-Young loss is implicitly associated with a link function, that maps model outputs to predictions. For instance, the logistic loss is associated with the soft argmax link function. Can we build new loss functions associated with the same link function as Fenchel-Young losses? In this paper, we introduce Fitzpatrick losses, a new family of separately convex loss functions based on the Fitzpatrick function. A well-known theoretical tool in maximal monotone operator theory, the Fitzpatrick function naturally leads to a refined Fenchel-Young inequality, making Fitzpatrick losses tighter than Fenchel-Young losses, while maintaining the same link function for prediction. As an example, we introduce the Fitzpatrick logistic loss and the Fitzpatrick sparsemax loss, counterparts of the logistic and the sparsemax losses. This yields two new tighter losses associated with the soft argmax and the sparse argmax, two of the most ubiquitous output layers used in machine learning. We study in details the properties of Fitzpatrick losses and, in particular, we show that they can be seen as Fenchel-Young losses using a modified, target-dependent generating function. We demonstrate the effectiveness of Fitzpatrick losses for label proportion estimation.

## 1 Introduction

Loss functions are a cornerstone of statistics and machine learning: they measure the difference, or "loss," between a ground-truth target and a model prediction. As such, they have attracted a wealth of research. Proper losses (a.k.a. proper scoring rules) [17; 16] measure the discrepancy between a target distribution and a probability forecast. They are essentially primal-primal Bregman divergences, with both the target and the prediction belonging to the same primal space. They are typically explicitly composed with a link function [26; 30], in order to map the model output to a prediction. A disadvantage of this explicit composition is that it often makes the resulting composite loss function non-convex. A related family of loss functions are Fenchel-Young losses [7; 8], which encompass many commonly-used loss functions in machine learning including the squared, logistic, sparsemax and perceptron losses. Fenchel-Young losses can be seen as primal-dual Bregman divergences , with the target belonging to the primal space and the model output belonging to the dual space. In contrast to proper losses, each Fenchel-Young loss is implicitly associated with a given link function, mapping the dual-space model output to a primal-space prediction (for instance, the soft argmax is the link function associated with the logistic loss). This crucial difference makes Fenchel-Young losses always convex w.r.t. the model output and w.r.t. the target, separately. Can we build new (separately) convex losses associated with the same link function as Fenchel-Young losses?In this paper, we introduce Fitzpatrick losses, a new family of primal-dual (separately) convex loss functions. Our proposal builds upon the Fitzpatrick function, a well-known theoretical object in maximal monotone operator theory [15; 11; 2]. So far, the Fitzpatrick function had been used as a theoretical tool to represent maximal monotone operators  and to construct Bregman-like primal-primal divergences , but it had not been used to construct primal-dual loss functions for machine learning, as we do. Crucially, the Fitzpatrick function naturally leads to a refined Fenchel-Young inequality, making Fitzpatrick losses tighter than Fenchel-Young losses. Yet, their predictions are produced using the same link function, suggesting that we can use Fitzpatrick losses as a tighter replacement for the corresponding Fenchel-Young losses (Figure 1). We make the following contributions.

* After reviewing some background, we introduce Fitzpatrick losses. They can be thought as a tighter version of Fenchel-Young losses, that use the same link function.
* We instantiate two new loss functions in this family: the Fitzpatrick logistic loss and the Fitzpatrick sparsemax loss. They are the counterparts of the logistic and sparsemax losses, two instances of Fenchel-Young losses. We therefore obtain two new tighter losses for the soft argmax and the sparse argmax, two of the most popular output layers in machine learning.
* We study in detail the properties of Fitzpatrick losses. We show that Fitzpatrick losses are equivalent to Fenchel-Young losses with a modified, target-dependent generating function.
* We demonstrate the effectiveness of Fitzpatrick losses for probabilistic classification on \(11\) datasets.

## 2 Background

### Convex analysis

We denote the non-negative real numbers by \(_{+}[0,+)\), the positive real numbers by \(_{++}(0,+)\), and the extended real numbers by \(}\{-,+\}\). We suppose given a nonzero natural number \(k\). We denote the probability simplex by \(^{k}\{p_{+}^{k}\ \ _{i=1}^{k}p_{i}=1\}\). We denote the indicator function of a set \(^{k}\) by \(_{}(y)=0\) if \(y\), \(+\) otherwise. We denote the effective domain of a function \(^{k}}\) by \(\ :=\{y^{k}\ \ (y)<+\}\). A function \(^{k}}\) is said to be **proper** if it never takes the value \(-\) and if \(\). We denote the Euclidean projection onto a nonempty closed convex set \(^{k}\) by \(P_{}()\), the unique solution \(y^{}\) of the minimization problem \(_{y^{}}\|y^{}-\|_{2}^{2}\).

Figure 1: We introduce **Fitzpatrick losses**, a new family of loss functions \(L(y,)\) generated by a convex regularization function \(\), that are **lower-bounds** of the Fenchel-Young losses generated by the same function \(\), while maintaining the **same** link function \(_{}=^{*}\). In particular, we use our framework to instantiate the counterparts of the **logistic** and **sparsemax** losses, two instances of Fenchel-Young losses, associated with the **soft argmax** and the **sparse argmax**. In the figures above, we plot \(L(y,)\), where \(y=e_{1}\), \(=(s,0)\) and \(L\{L_{F[]},L_{^{*}}\}\), confirming the lower-bound property.

For a function \(:^{k}}\), its **subdifferential**\(^{k}^{k}\) is defined by

\[(y^{},^{})^{} (y^{})(y)(y^{})+ y-y^{}, ^{},\  y^{k}.\]

When \(\) is convex and differentiable, the subdifferential is a singleton and we have \((y^{})=\{(y^{})\}\). The **normal cone** to a nonempty closed convex set \(^{k}\) at \(y^{}\) is defined by

\[^{} N_{}(y^{}) y-y^{}, ^{} 0, y\]

if \(y^{}\) and \(N_{}(y^{})=\) if \(y^{}\). The **Fenchel conjugate**\(^{*}:^{k}}\) of a function \(:^{k}}\) is defined by

\[^{*}()_{y^{}^{k}} y^{ },-(y^{}).\]

From standard results in convex analysis [27, Proposition 11.3], when \(:^{k}}\) is a proper convex l.s.c. (lower semicontinuous) function,

\[^{*}()=*{argmax}_{y^{}^{k} } y^{},-(y^{}).\]

When the argmax is unique, it is equal to \(^{*}()\). We define the **generalized Bregman divergence**\(D_{}:^{k}^{k}}_{+}\) generated by a proper convex l.s.c. function \(:^{k}}\), by

\[D_{}(y,y^{})(y)-(y^{})-_{^{ }(y^{})} y-y^{},^{},\] (1)

with the convention \(++(-)=+\) (we comment on this convention in Appendix C). When \(\) is differentiable, Equation (1) gives the classical **Bregman divergence**

\[D_{}(y,y^{})(y)-(y^{})- y-y^{ },(y^{}).\]

Both \(y\) and \(y^{}\) belong to the **primal space**.

### Fenchel-Young losses

**Definition and properties**

The **Fenchel-Young loss**\(L_{^{*}}:^{k}^{k} }\) generated by a proper convex l.s.c. function \(\) is

\[L_{^{*}}(y,)^{*} (y,)- y,(y)+^{*}( )- y,.\]

As its name indicates, it is grounded in the Fenchel-Young inequality

\[ y,(y)+^{*}() y, ^{k}.\]

The Fenchel-Young loss enjoys many desirable properties, notably it is **non-negative** and it is **separately convex** in \(y\) and \(\). The Fenchel-Young loss can be seen as a **primal-dual** Bregman divergence , where \(y\) belongs to the primal space and \(\) belongs to the dual space.

#### Link functions

To map an element \(\) of a dual space to an element \(y\) of a primal space, we define the link function (potentially set-valued) associated with the loss \(L\) by

\[\{y^{k}\ \ L(y,)=0\}.\]

Given a proper convex function \(\), the associated Fenchel-Young loss \(L_{^{*}}\) produces the canonical link function \(^{*}\), since

\[L_{^{*}}(y,)=0 y^{*}().\]

In particular when \(\) is strictly convex, and thus \(^{*}\) is differentiable according to [27, Theorem 11.13], the Fenchel-Young loss satisfies the identity of indiscernibles

\[L_{^{*}}(y,)=0 y=^{*}().\]

In the remainder of this paper, we will use the notation \(_{}()\) for the canonical link function \(^{*}()\). When the function \(^{*}\) is differentiable, \(_{}()\) will denote \(^{*}()\). Since \(^{*}\) is convex, \(_{}\) is monotone (see SS2.3). As shown in , the monotonicity implies that \(\) and \(_{}()\) are sorted the same way, i.e., \(_{i}>_{j}_{}()_{i} _{}()_{j}\). Link functions also play an important role in the loss subgradients (and in the loss gradient when it is differentiable), as we have

\[_{}L_{^{*}}(y,)=_{}( )-y.\] (2)

#### Examples of Fenchel-Young loss instances and their associated link function

We give a few examples of Fenchel-Young losses. With the squared \(2\)-norm, \((y^{})=\|y^{}\|_{2}^{2}\), we obtain the **squared loss**

\[L_{^{*}}(y,)=L_{}(y,) \|y-\|_{2}^{2}\]

and the **identity link**

\[_{}()=.\]

With the indicator of a nonempty closed convex set \(\), \((y^{})=_{}(y^{})\), we obtain the **perceptron loss**

\[L_{^{*}}(y,)=L_{}(y,) _{y^{}}\  y^{},- y,\]

and the **argmax link**

\[_{}()=*{argmax}_{y}\  y,.\]

With the squared \(2\)-norm restricted to some nonempty closed convex set \(\), \((y^{})=\|y^{}\|_{2}^{2}+_{}(y^{ })\), we obtain the **sparseMAP loss**

\[L_{^{*}}(y,)=L_{}(y,) \|y-\|_{2}^{2}-\|P_{}( )-\|_{2}^{2},\]

and the link is the **Euclidean projection** onto \(\),

\[_{}()=P_{}().\]

When the set is \(=^{k}\), we obtain the **sparsemax loss** and the **sparse argmax link**\(_{}()=P_{^{k}}()\) (also known as sparsemax), which is known to produce sparse probability distributions.

With the Shannon negentropy restricted to the probability simplex, \((y^{}) y^{}, y^{}+_{ ^{k}}(y^{})\), we obtain the **logistic loss**

\[L_{^{*}}(y,)=L_{}(y,) _{i=1}^{k}(_{i})+ y, y- y,,\]

and the **soft argmax link** (also known as softmax)

\[_{}()=()() /_{i=1}^{k}(_{i}).\]

### Maximal monotone operators and the Fitzpatrick function

An operator \(A\), that is, a subset \(A^{k}^{k}\), is called **monotone** if for all \((y,) A\) and all \((y^{},^{}) A\), we have

\[ y^{}-y,^{}- 0.\]

We overload the notation to denote \(A(y)\{^{k}\ \ (y,) A\}\). A monotone operator \(A\) is said to be **maximal** if there does not exist \((y,) A\) such that \(A\{(y,)\}\) is still monotone. It is well-known that the subdifferential \(\) of a proper convex l.s.c. function \(\) is maximal monotone. For more details on monotone operators, see .

A well-known object in monotone operator theory, the **Fitzpatrick function** associated with a maximal monotone operator \(A\), denoted \(F[A]:^{k}^{k}}\), is defined by

\[F[A](y,)_{(y^{},^{}) A} y-y^{ },^{}+ y^{},.\]

In particular, with \(A=\), we have

\[F[](y,)=_{(y^{},^{}) } y-y^{},^{}+ y^{}, =_{y^{}*{dom}}[ y^{}, +_{^{}(y^{})} y-y^{ },^{}].\]The Fitzpatrick function was studied in depth in . In particular, it is (jointly) convex and satisfies

\[ y, F[](y,) ^{}(y,)=(y)+^{}() y,^{k}.\] (3)

We introduce the operator \(y_{F[]}^{}(^{k}^{k}) ^{k}\), associated to the Fitzpatrick function \(F[]\):

\[y_{F[]}^{}(y,)*{ argmax}_{y^{}*{dom}}\ [ y^{},+_{^{}(y ^{})} y-y^{},^{}].\] (4)

As proven for Item 4 in Proposition 1, we have \(y_{F[]}^{}(y,)_{}F[ ](y,)\). For the rest of the paper, in case that the \(*{argmax}\) in (4) is a singleton \(\{y^{}\}\), we will write \(y_{F[]}^{}(y,) y^{}\). The Fitzpatrick function \(F[](y,)\) and \(^{}(y,)=(y)+^{} ()\) play a similar role but the latter function is **separable** in \(y\) and \(\), while the former is **not**. In particular this makes the subdifferential \(_{}F[](y,)\) depend on both \(y\) and \(\), while \(_{}(^{})(y,)=^{ }()\) depends only on \(\).

The Fitzpatrick function was used in  to theoretically study primal-primal Bregman-like divergences. As discussed in more detail in Section 3.4, using these divergences for machine learning would require us to compose them with an explicit link function, which would typically break convexity. In the next section, we introduce new primal-dual losses based on the Fitzpatrick function.

## 3 Fitzpatrick losses

### Definition and properties

Inspired by the inequality in (3), which we can view as a refined Fenchel-Young inequality, we introduce Fitzpatrick losses, a new family of loss functions generated by a convex l.s.c. function \(\).

**Definition 1**: _Fitzpatrick loss_

_Let \(:^{k}}\) be a proper convex l.s.c. function. When \(y*{dom}\) and \(^{k}\), we define the Fitzpatrick loss \(L_{F[]}:^{k}^{k}}\) generated by \(\) as_

\[L_{F[]}(y,)  F[](y,)- y,\] \[=_{(y^{},^{})} y-y^ {},^{}+ y^{},- y,\] \[=_{(y^{},^{})} y^ {}-y,-^{}.\]

_When \(y*{dom}\), \(L_{F[]}(y,)+\)._

Fitzpatrick losses enjoy similar properties as Fenchel-Young losses, while being **tighter** than Fenchel-Young losses.

**Proposition 1**: _Properties of Fitzpatrick losses_

1. _Non-negativity:_ _for all_ \((y,)^{k}^{k}\)_,_ \(L_{F[]}(y,) 0\)_._
2. _Same link function:_ \(L_{^{}}(y,)=L_{F[]}(y,)=0  y_{}()\)_._
3. _Separable convexity:_ \(L_{F[]}(y,)\) _is separately convex._
4. _(Sub-)Gradient:_ \(_{}L_{F[]}(y,) y_{F[]} ^{}(y,)-y\) _where_ \(y_{F[]}^{}(y,)\) _is given by (_4_)._
5. _Tighter inequality:_ _for all_ \((y,)^{k}\)_,_ \(0 L_{F[]}(y,) L_{^{}}(y,)\)_._

A proof is given in Appendix B.2. Because the Fitzpatrick loss and the Fenchel-Young loss generated by the same \(\) have the same link function \(_{}\), they share the same minimizers w.r.t. \(\) for \(y\) fixed. However, the Fitzpatrick loss is always a **lower bound** of the corresponding Fenchel-Young loss. Moreover, they have different gradients w.r.t. \(\): \(_{}L_{^{}}(y,)=_{} ()-y\) vs. \(_{}L_{F[]}(y,) y_{F[]} ^{}(y,)-y\). It is worth noticing that \(y_{F[]}^{}(y,)\) depends on both \(y\) and \(\), contrary to \(_{}()\).

When \(\) is a twice differentiable function on its domain (which is for instance the case of the squared \(2\)-norm or the negentropy), we next show that Fitzpatrick losses enjoy a particularly simple expression and become a squared Mahalanobis-like distance.

**Proposition 2**: _Expressions of \(F[](y,)\) and \(L_{F[]}(y,)\) when \(\) is twice differentiable Let \(:^{k}}\) be a convex function such that \(\;\;\) is an open set. Let us assume that \(\) is twice differentiable. Then, for any \(y\;\) and for any \(y^{} y^{}_{F[]}(y,)\), as defined in (4), we have that_

\[F[](y,) = y,(y^{})+ y^{}, - y^{},(y^{})\] \[L_{F[]}(y,) = y^{}-y,-(y^{})\] \[= y^{}-y,^{2}(y^{})(y^{}-y)\]

_and_

\[^{2}(y^{})(y^{}-y)=-(y^{}).\]

A proof is given in B.3. When \(\) is constrained (i.e., when it contains an indicator function), we show in SS3.5 that the above expression becomes a lower bound.

### Examples

We now present the Fitzpatrick loss counterparts of various Fenchel-Young losses.

**Squared loss.**

**Proposition 3**: _Squared loss as a Fitzpatrick loss When \((y^{})=\|y^{}\|_{2}^{2}\), we have for all \(y^{k}\) and \(^{k}\)_

\[L_{F[]}(y,)=\|y-\|_{2}^{2}= L_{}(y,).\]

A proof is given in Appendix B.4. Therefore, the Fenchel-Young and Fitzpatrick losses generated by \(\) coincide, but up to a factor \(\).

**Perceptron loss.**

**Proposition 4**: _Perceptron loss as a Fitzpatrick loss When \((y^{})=_{}(y^{})\), where \(\) is a nonempty closed convex set, we have for all \(y\) and \(^{k}\)_

\[L_{F[]}(y,)=L_{}(y,)=_{y^{ }}\; y^{},- y,.\]

A proof is given in Appendix B.5. Therefore, the Fenchel-Young and Fitzpatrick losses generated by \(\) exactly coincide in this case.

Fitzpatrick sparseMAP and Fitzpatrick sparsemax losses.As our first example where Fenchel-Young and Fitzpatrick losses substantially differ, we introduce the **Fitzpatrick sparseMAP** loss, which is the Fitzpatrick counterpart of the sparseMAP loss .

**Proposition 5**: _Fitzpatrick sparseMAP loss When \((y^{})=\|y^{}\|_{2}^{2}+_{}(y^{ })\), where \(\) is a nonempty closed convex set, we have for all \(y\) and \(^{k}\)_

\[L_{F[]}(y,)=2^{*}((y+)/2)- y,= y^{}-y,-y^{}\]

_where we used \(y^{}\) as a shorthand for_

\[y^{}_{F[]}(y,)=^{*}((y+)/2)=P_{ }((y+)/2).\]A proof is given in Appendix B.6. As a special case, when \(=^{k}\), we call the obtained loss the **Fitzpatrick sparsemax loss**, as it is the counterpart of the sparsemax loss . Like the sparseMAP and sparsemax losses, these new losses rely on the Euclidean projection as a core building block. The Euclidean projection onto the probability simplex \(^{k}\) can be computed exactly in \(O(k)\) expected time and \(O(k k)\) worst-case time .

Fitzpatrick logistic loss.We now derive the Fitzpatrick counterpart of the logistic loss. Before stating the next proposition, we recall the definition of the Lambert function \(W:_{+}_{+}\). The function \(W\) is the inverse of the function \(f:_{+}_{+}\) where \(f(w)=w(w)\) for all \(w_{+}\).

**Proposition 6**: _Fitzpatrick logistic loss_

_When \((y^{})= y^{}, y^{}+{}_{_{ ^{k}}}(y^{})\), we have for all \(y^{k}\) and \(^{k}\)_

\[L_{F[]}(y,)= y^{}-y,- y^{}-1\]

_where we used \(y^{}\) as a shorthand for \(y^{}_{F[]}(y,)\) defined by_

\[y^{}_{F[]}(y,)_{i}=\{^{-^{}}^{_{i}},y_{i}=0,\\ }{W(y_{i}^{^{}}-_{i})},y_{i}>0..\]

A proof and the value of \(^{}=^{}(y,)\) are given in Appendix B.7. To obtain \(^{}(y,)\), we need to solve a one-dimensional root equation, which can be done using, for instance, a bisection.

### Relation with Fenchel-Young losses

On first sight, Fitzpatrick losses and Fenchel-Young losses appear quite different. In the next proposition, we show that the Fitzpatrick loss generated by \(\) is in fact equal to the Fenchel-Young loss generated by the modified, target-dependent function

\[_{y}(y^{})(y^{})+D_{}(y,y^{}),\]

where \(D_{}\) is the generalized Bregman divergence defined in (1). In particular, Lemma 1 in the appendix shows that if \(=+_{}\), where \(\) is a nonempty closed convex set, then \(_{y}(y^{})=_{y}(y^{})+_{}(y^{})\), where \(_{y}(y^{})(y^{})+D_{}(y,y^{})\).

**Proposition 7**: _Characterization of \(F[]\), \(L_{F[]}\) and \(y^{}_{F[]}\) using \(_{y}\)_

_Let \(:^{k}}\) be a proper convex l.s.c. function. Then, for all \(y\,\) and all \(^{k}\),_

\[F[](y,) =_{y}(y)+^{}_{y}()\] \[L_{F[]}(y,) =L_{_{y}^{}_{y}}(y,)\] \[y^{}_{F[]}(y,) =_{_{y}}().\]

This characterization of the Fitzpatrick function \(F[]\) is also new to our knowledge. A proof is given in Appendix B.8. Proposition 7 is very useful, as it means that Fitzpatrick losses inherit from all the known properties of Fenchel-Young losses, analyzed in prior works . In particular, Fenchel-Young losses are smooth (i.e., with Lipschitz gradients) when \(\) is strongly convex. We therefore immediately obtain that Fitzpatrick losses are smooth in their second argument \(\) if \(\) is strongly convex and \(D_{}\) is convex in its second argument, which is the case when \((y^{})=\|y^{}\|_{2}^{2}\) and \((y^{})= y^{}, y^{}\). Therefore, the Fitzpatrick sparsemax and logistic losses are smooth. However in the general case, this does not hold, as \(D_{}\) is usually not convex in its second argument. Proposition 7 also provides a mean to compute Fitzpatrick losses and their gradient. Finally, it suggests a very natural geometric interpretation of Fitzpatrick losses, as presented in Figure 2.

### Relation with generalized Bregman divergences

As we stated before, the generalized Bregman divergence \(D_{}(y,y^{})\) in (1) is a primal-primal divergence, as both \(y\) and \(y^{}\) belong to the same primal space. In contrast, Fenchel-Young losses\(L_{^{*}}(y,)\) are primal-dual, since \(y\) belongs to the primal space and \(\) belongs to the dual space. Both can however be related for any proper l.s.c. convex function \(\), since, for any \(y^{}\) such that \((y^{})\), we have

\[_{^{}(y^{})}L_{ ^{*}}(y,^{}) =_{^{}(y^{})}(y)+ ^{*}(^{})- y,^{}\] \[=(y)+_{^{}(y^{})} ^{*}(^{})- y,^{}\] \[=(y)-_{^{}(y^{})}- ^{*}(^{})+ y,^{}\] \[=(y)-(y^{})-_{^{} (y^{})} y-y^{},^{}\] \[=D_{}(y,y^{})\]

where, in the penultimate line, we have used that \(^{*}(^{})= y^{},^{}- (y^{})\), as \(^{}(y^{})\). This equality remains true when \((y^{})=\), by convention \(=+\) and by definition of \(D_{}(y,y^{})\) in (1). This identity suggests that we can create Bregman-like primal-primal divergences by replacing \(^{*}\) with \(F[]\),

\[_{F[]}(y,y^{})_{^{} (y^{})}L_{F[]}(y,^{})=_{ ^{}(y^{})}F[](y,^{ })- y,^{}.\]

This recovers one of the two Bregman-like divergences proposed in , the other one replacing the \(\) above by a \(\). As stated in , \(F[]\) and \(^{*}\) are **representations** of \(\).

In order to use a primal-primal divergence as a loss, we need to explicitly compose it with a link function, such as \(_{}()=^{*}()\). Unfortunately, \(D_{}(y,_{}())\) or \(_{F[]}(y,_{}())\) are typically **non convex** functions of \(\), while Fenchel-Young and Fitzpatrick losses are always **convex** in \(\). In addition, differentiating through \(_{}()\) typically requires implicit differentiation , while Fenchel-Young and Fitzpatrick losses enjoy easy-to-compute gradients, thanks to Danskin's theorem.

### Lower bound on Fitzpatrick losses

If \(=+_{}\), where \(:^{k}}\) is a proper convex Legendre-type function and \(\) is a nonempty closed convex set, then it was shown in [8, Proposition 3] that Fenchel-Young lossessatisfy the lower bound

\[D_{}y,_{}() L_{^{ *}}(y,),\]

with equality if \(=\). We now show that a similar result holds for Fitzpatrick losses. Similarly as before, we define \(_{y}(y^{})(y^{})+D_{}(y,y^{})\).

**Proposition 8**: _Lower bound on Fitzpatrick losses Let \(=+_{}\), where \(:^{k}}\) is a proper convex Legendre-type function, as defined in [8, Definition 3], and \(\) is a nonempty closed convex set. We remind that \(_{y}(y^{})(y^{})+D_{}(y,y^{})\). Let us assume that \(_{y}^{*}\) is differentiable. Then,_

\[D_{_{y}}(y,y^{*})= y-y^{*},^{2}(y^{*})(y-y^{*})  L_{F[]}(y,),\]

_with equality if \(=\), where we used \(y^{*}\) as a shorthand for \(_{y}^{*}()\)._

A proof is given in Appendix B.9. If \(_{y}\) is \(\)-strongly convex, we obtain \(\|y-y^{*}\|_{2}^{2} D_{_{y}}(y,y^{*})\).

## 4 Experiments

Experimental setup.We follow exactly the same experimental setup as in . We consider a dataset of \(n\) pairs \((x_{i},y_{i})\) of feature vectors \(x_{i}^{d}\) and label proportions \(y_{i}^{k}\), where \(d\) is the number of features and \(k\) is the number of classes. At inference time, given an unknown input vector \(x^{d}\), our goal is to estimate a vector of label proportions \(^{k}\). A model is specified by a matrix \(W^{k d}\) and a convex l.s.c. function \(:^{k}}\). Predictions are then produced by the generalized linear model \(x_{}(Wx)\). At training time, we estimate the matrix \(W^{k d}\) by minimizing the convex objective

\[R_{L,}(W)_{i=1}^{n}L(y_{i},Wx_{i})+\| W\|_{2}^{2},\] (5)

where \(LL_{^{*}},L_{F[]}}\). We focus on the (Fitzpatrick) sparsemax and the (Fitzpatrick) logistic losses. We optimize (5) using the L-BFGS algorithm . The gradient of the Fenchel-Young loss is given in (2), while the gradient of the Fitzpatrick loss is given in Proposition 1, Item 4. Experiments were conducted on a Intel Xeon E5-2667 clocked at 3.30GHz with 192 GB of RAM running on Linux. Our implementation relies on the SciPy  and scikit-learn  libraries.

We ran experiments on \(11\) standard multi-label benchmark datasets1 (see Table 2 in Appendix A for statistics on the datasets). For all datasets, we removed samples with no label, normalized samples to have zero mean unit variance, and normalized labels to lie in the probability simplex. In (5), we chose the hyperparameter \(\{10^{-4},10^{-3},,10^{4}\}\) against the validation set. We report test set mean squared error (also known as Brier score in a probabilistic forecasting context) in Table 1.

Results.We found that the logistic loss and the Fitzpatrick logistic loss are comparable on most datasets, with the logistic loss significantly winning on \(2\) datasets and the Fitzpatrick logistic loss significantly winning on \(2\) datasets, out of \(11\). Since the Fitzpatrick logistic loss is slightly more computationally demanding, requiring to solve a root equation while the logistic loss does not, we believe that the logistic loss remains the best choice when we wish to use the sofargmax as link function \(_{}\).

Similarly, we found that the sparsemax loss and the Fitzpatrick sparsemax loss are comparable on most datasets, with the sparsemax loss significantly winning on only \(1\) dataset out of \(11\) and the Fitzpatrick loss significantly winning on \(2\) datasets out of \(11\). Since the two losses both use the Euclidean projection onto the simplex \(P_{^{k}}\) as their link function \(_{}\), we conclude that the Fitzpatrick sparsemax loss is a serious contender to the sparsemax loss, especially when predicting sparse label proportions is important.

## 5 Conclusion

We proposed to leverage the Fitzpatrick function, a theoretical tool from monotone operator theory, to build a new family of primal-dual separately convex loss functions for machine learning. We reinterpreted Fitzpatrick losses as lower bounds of Fenchel-Young losses that maintains the same link function. Our paper therefore challenges the idea that there can only be one loss function, convex in each argument, associated with a certain link function. For instance, we created the Fitzpatrick logistic and sparsemax losses, that are associated with the soft argmax and sparse argmax links, traditionally associated with the logistic and sparsemax losses, respectively. We believe that even more loss functions with the same link can be created, which calls for a systematic study of their properties and respective benefits. In future work, we intend to study calibration guarantees for Fitzpatrick losses, to test new link functions from maximal monotone operator theory and to implement more efficient training for the Fitzpatrick logistic loss.