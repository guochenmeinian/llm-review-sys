# On the Stability-Plasticity Dilemma in Continual Meta-Learning: Theory and Algorithm

Qi Chen

Laval University

&Changjian Shui

McGill University

&Ligong Han

Rutgers University

&Mario Marchand

Laval University

Correspondence to: qi.chen.1@ulaval.ca

###### Abstract

We focus on Continual Meta-Learning (CML), which targets accumulating and exploiting meta-knowledge on a sequence of non-i.i.d. tasks. The primary challenge is to strike a balance between stability and plasticity, where a model should be stable to avoid catastrophic forgetting in previous tasks and plastic to learn generalizable concepts from new tasks. To address this, we formulate the CML objective as controlling the average excess risk upper bound of the task sequence, which reflects the trade-off between forgetting and generalization. Based on the objective, we introduce a unified theoretical framework for CML in both static and shifting environments, providing guarantees for various task-specific learning algorithms. Moreover, we first present a rigorous analysis of a bi-level trade-off in shifting environments. To approach the optimal trade-off, we propose a novel algorithm that dynamically adjusts the meta-parameter and its learning rate w.r.t environment change. Empirical evaluations on synthetic and real datasets illustrate the effectiveness of the proposed theory and algorithm.

## 1 Introduction

An essential goal in real-time intelligent systems is to balance stability (preserve past knowledge; minimize catastrophic forgetting ) and plasticity (rapidly learn from new experiences; generalize quickly ). For addressing this dilemma, a promising direction is to incorporate meta-learning [3; 4] with Continual Learning (CL) [5; 6; 7], which constitutes the Continual Meta-Learning (CML) [8; 9; 10] problem. Specifically, CML sequentially learns a meta-model from few-shot tasks as the common prior. Then, for new tasks, it quickly adapts task-specific models with this prior. CML effectively enables balancing the _task-level_ trade-off between learning new tasks and retaining previously acquired knowledge since the performance on previous tasks can be recovered with few additional samples.

Despite being intuitive and technically sound, most related works in CML have focused on improving empirical performance. There is still much room for exploration in the theoretical aspects. In particular, there is a lack of rigorous understanding on _(1) which factors are important for stability and plasticity, (2) how to address the dilemma effectively by considering these factors._

To theoretically understand these questions, we need to characterize the sequential task-generating process. In fact, many CML approaches are based on the implicit assumption that the non-i.i.d. tasks are originated from a static task environment \(\), where \(_{t}, t\). In addition, recent works reveal that catastrophic forgetting often occurs when transferring a meta-model to a new environment [11; 12] and start trying to empirically address the forgetting in shifting environments, such as [13; 14; 7; 15; 16]. We take this case into consideration and formally define a more general CML setting, where at each time \(t\) the task \(_{t}\) is generated from a possibly shifting environment \(_{t}\) with \(_{t}_{t}\) (the environment isstatic if \(_{t}=, t\)). We found a _meta-level_ trade-off exists in shifting environments, which further induces difficulty in controlling the task-level learning-forgetting trade-off, as discussed in Sec. 4.2.

In this paper, we formally study the _bi-level_ (task- and meta-) learning-forgetting trade-off in CML to fill the theoretical gap. Compared with previous works, our contribution highlights are as follows:

**Unified theoretical framework** We first introduce a novel and unified theoretical framework for CML in both static and shifting task environments. For each task, the excess risk reflects the true model performance but is intractable. Therefore, we derive a unified form of the excess risk upper bound for various base learners, which contains optimization error with the generalization gap and could be estimated from observations. Based on these, we propose to control the Average Excess Risk (AER) upper bound of all the tasks as the learning objective in CML (shown in Sec. 3). This upper bound can be further decomposed as the optimal trade-off and the algorithmic regret.

**Understanding the bi-level learning-forgetting trade-off** We discuss the bi-level trade-off in shifting environments with an illustrative example in Sec. 4.1. We theoretically identify in Theorem 5.1 that: (1) Inside each environment, the task-level trade-off is affected by the task similarities, which reflects on the slot diameters and variances. (2) The path length and changing points representing environment similarities and non-stationarity dominate the meta-level trade-off. (3) Task and sample numbers are important to generalization. The optimal trade-off is achieved with a minimal upper bound of AER, so it's helpful to consider these factors for optimizing the meta-parameter sequence.

**Theoretically grounded algorithm** We propose a novel algorithm (Algo. 1) for addressing the bi-level trade-off in shifting environments, which dynamically adjusts both the meta-parameter and its learning rate when an environment change is detected. Hence, the proposed algorithm can implicitly control the AER by minimizing the dynamic regret to approach the optimal trade-off. We derive a general bound for the proposed algorithm in Theorem 5.1. Improved complexity factors and rates theoretically demonstrate the validity of the proposed algorithm for balancing the trade-off, as illustrated in Theorem 6.1 and 6.2. Furthermore, empirical results on various datasets show improvements in estimated bounds and superior performance compared to baselines, demonstrating that the proposed algorithm can incrementally learn from new environments while maintaining good performance on the previous ones, i.e., a well-balanced trade-off.

## 2 Related work

Due to the page limit, we only briefly discuss the most related works in this section. A more detailed discussion and comprehensive investigation of various settings are provided in H.

**Continual learning** Traditional CL approaches mainly focus on addressing _catastrophic forgetting_. Some methods use _regularization_ techniques, such as Elastic Weight Consolidation (EWC)  and Synaptic Intelligence (SI) , that aim to protect important parameters of a neural network from being modified during training on new tasks. Other methods like _replay-based_ approach [19; 20] store or replay previous task examples to aid in preserving knowledge of old tasks. Another line of work is _parameter isolation_ or _dynamic architecture_ methods, which involve creating new network pathways or modules for each new task [21; 22]. Riemer et al.  first conceptualize the learning forgetting trade-off in CL as temporally maximizing transfer and minimizing interference by enforcing gradient alignment across examples, which is implemented using experience replay and Reptile  to approximate the gradient alignment objective with a first-order Taylor expansion.

**Meta-learning** Meta-learning aims for _quick generalization_, allowing for efficient learning of new tasks with limited data. The foundational framework of _statistical meta-learning_ by Baxter  assumes tasks are i.i.d. sampled from a task environment. Amit and Meir  proposed PAC Bayes bounds with a joint training algorithm, which is not straightforwardly extendable to sequential learning. Chen et al.  derived information-theoretic bounds for MAML-like algorithms suitable for sequential few-shot learning. These works assume i.i.d. conditions for both tasks and in-task data. To break the assumption, _online meta-learning_ introduces two main approaches for learning non-i.i.d. sequential tasks. _Batch-Within-Online_ (BWO)[9; 27] methods, similar to CML, learn each task under a statistical batch setting. However, the theoretical analysis is limited, and none addresses shifting task environments. _Online-Within-Online_ (OWO) [28; 29; 30; 31] methods employ online algorithms as both base and meta learners. Khodak et al.  were the first to consider shifting environments for OWO, providing average regret bounds for gradient-based learners.

In contrast, we consider various statistical batch algorithms in addition to the gradient-based ones as base learners. We further propose a fine-grained algorithm w.r.t environment change that has an improved rate over the bound in  (although not directly comparable) for gradient-based base learners, as discussed in Sec. 6.2. This improvement implies a better learning-forgetting trade-off not considered in previous meta-learning literature.

**Continual meta-learning**: FTML  extends MAML  to sequential learning using Follow The Leader (FTL) as the meta-learner, which requires storing all the previous tasks. MOCA  incorporates Bayesian Online Changing-point Detection (BOCD) to identify unknown task boundaries during the continual meta-learning process. The above methods can address the task-level trade-off under a static task environment. However, a meta-level trade-off exists when facing shifting environments. He et al.  combines MAML with Bayes Gradient Descent (BGD), slowly updating meta-parameters of small variance to stabilize the learning from non-i.i.d. sequential tasks. Caccia et al.  address the environment shift with a soft modulation on meta-updates with the empirical loss. , , and  incorporate additional memory with different environment shift detection methods to address the meta-level trade-off.  and  model the meta-parameter distribution with a Dirichlet Process Mixture Model (DPMM) and Dynamic Gaussian Mixture Model (DGMM), respectively, which can detect new environments and store the corresponding \(K\) meta-parameters with \((K)\) memory.  grows \(K\) subnets by detecting the environment shift with BOCD and stores \(M\) tasks from previous environments, thus having a memory complexity of \((K+M)\).

Instead of the memory-based approaches, we focus on the more challenging fully online experimental setting first proposed by Caccia et al. . Our algorithm consumes \((1)\) memory and better balances the meta-level trade-off in the online setting. In addition, we consider different levels of non-stationarity and more environment shifts compared to previous works.

**Theory on the stability-plasticity dilemma**: Raghavan and Balaprakash  explicitly study the task-level trade-off in continual learning by formulating the problem as a two-player sequential game and prove the existence of a balance point for each task. However, the cost function is based on empirical loss without considering generalization on unseen data. Based on NTK  regime, the generalization and forgetting property of orthogonal gradient descent  are separately studied in  and  for continual learning. _To the best of our knowledge, we are the first theoretical work studying the stability-plasticity dilemma in CML._ We also first provide a theoretical framework for CML in static and shifting environments, offering a formal understanding of the bi-level trade-off.

## 3 Problem setup

Let us consider a sequence of different task distributions \(\{_{t}\}_{t=1}^{T}\) defined on the same example space \(=\) with \(T\). Specifically, at time \(t\), the task \(_{t}\) is generated from a possibly shifting environment \(_{t}\) with \(_{t}_{t}\), which is static if \(_{t}=, t\). Then a dataset of \(m_{t}\) examples \(S_{t}=\{Z_{i}\}_{i=1}^{m_{t}}\) are i.i.d. sampled from \(_{t}\), where \(S_{t}_{t}^{m_{t}}\). In CML, we have two kinds of parameters: model parameter and meta-parameter, which are learned through a base learner and a meta learner, respectively. A visual representation of the CML process is provided in Fig. 1. Let the meta-parameters be defined on the same support \(\) for all the tasks. Additionally, we assume that the same parametric hypothesis space \(\) is used across all tasks with non-negative bounded loss function \(:\). Given any model parameter \(w\), the true risk and empirical risk of \(t\)-th task are defined as \(_{_{t}}(w)}}{{=}}_{Z_{t}}(w,Z)\) and \(_{S_{t}}(w)}}{{=}}}_{i=1}^{m_{t}}(w,Z_{i})\). To motivate the CML setting mentioned above, we also provide an analysis of how to apply CML to real-world examples like online recommendation systems in Appendix G.2.

### Base learner

At time \(t\), base learner \(\) takes the task data \(S_{t}\) and the meta-parameter \(u_{t}\) that represents the prior knowledge learned from previous tasks as the input then outputs the model parameter \(W_{t}=(u_{t},S_{t})\). Specifically, \(\) is characterized by a conditional distribution2\(P_{W_{t}|S_{t},u_{t}}\) as in [38; 39]. We further define the corresponding (expected) **excess risk** on task \(t\) as the expected gap between the true risk of the learned hypothesis \(W_{t}\) and the optimal true risk:

\[R_{}(,u_{t})}}{{=}} _{S_{t}}_{W_{t} P_{W_{t}|S_{t},u_{t}}}[_{ _{t}}(W_{t})-_{_{t}}(w_{t}^{*})],w_{t}^{*}=_{w }_{_{t}}(w).\]

The excess risk of \(_{t}\) is thus explicitly determined by the meta-parameter \(u_{t}\) and the base learner, where \(u_{t}\) is provided by experts or random guesses in single-task learning. Although reflecting the true model performance, the excess risk is intractable due to the unknown distribution \(_{t}\).

The following theorem gives a **unified form of excess risk upper bound** that applies to common base learners such as Stochastic Gradient Descent (SGD), Stochastic Gradient Langevin Dynamics (SGLD), Regularized Loss Minimization (RLM), and Gibbs algorithm. In contrast to the upper bounds that contain the empirical risk, it avoids the task data access for the meta-learner and can thus be used to design new CML algorithms. See detailed proof in Appendix B.5.

**Theorem 3.1**.: _For any \(t[T]\), assume that \(_{_{t}}()\) has \(\)-quadratic growth (see Definition A.2), then whenever the base learner is SGD, SGLD, RLM, or the Gibbs algorithm, there exists \(f_{t}()\) that gives_

\[R_{}(,u_{t}) f_{t}(u_{t})=_{t}a_ {t}+-w_{t}\|^{2}+_{t}+_{0}}{_{t}}+ _{t},_{t},_{t},_{t},_{t}^{ +},a,b,_{0}>0\,.\] (1)

_The meta-parameter \(u_{t}=(_{t},_{t})\) decomposes into an initialization or bias \(_{t}\) and a learning rate \(_{t}\) (if \(\{,\}\)) or a regularization coefficient (if \(\{,\}\)). Moreover, \(w_{t}\) denotes the (single) output for the (possibly randomized) base learner, and \(_{0}}}{{=}}2\|w_{t}- W_{t}\|^{2}\) characterizes the randomness of the output. Finally, \(a\) and \(b\) are constants, and \(_{t},_{t},_{t}\) are functions of the task sample size \(m_{t}\) that characterize the base learner (see Appendix B and D)._

_Remark 3.2_.: (a) The above bound could be _estimated from observation_, so we can use it to design meta-learning algorithms to control the excess risk. (b) Besides learning the initialization/bias, the corresponding learning rate/regularization coefficient should be further considered to control \(R_{}\). (c) \(f_{t}\) is convex w.r.t \(u_{t}\) (proved in Appendix B.6). (d) This upper bound gives an **explicit** form of _task-level_ trade-off that depends on the meta-parameter, where a large \(_{t}\) represents conserving less model prior (larger forgetting) and learning more from data to obtain the model \(w_{t}\). The similarity \(\|_{t}-w_{t}\|\) between the model prior and the new task can affect the choice of \(_{t}\) for a better generalization.

### Meta learner

Since the tasks are sequentially encountered without the i.i.d. assumption and the excess risk upper bound (cost function) \(f_{t}\) is convex, we can naturally consider the meta-learning process as a repeated game, following the Online Convex Optimization (OCO) regime . At each time \(t\), the meta-learner first selects \(u_{t}\) with prior knowledge learned from previous tasks, then the base learner outputs \(w_{t}\) given \(u_{t}\) and dataset \(S_{t}\). After that, the cost function value \(f_{t}\) is revealed. The goal of the meta learner is to select a sequence of meta-parameters \(u_{1:T}\) so as to minimize the regret over rounds.

**Static task environment** In static environments, the task distributions \(_{t}, t[T]\) are assumed to be sampled from a fixed environment \(\). The corresponding static regret is defined as the gap between the total cost of \(u_{1:T}\) and that of an optimal meta-parameter in hindsight \(u_{T}^{*}\):

Figure 1: Illustration of Continual Meta-Learning (CML) process. At each time \(t\), the CML algorithm \(_{}\) (composed of the meta learner and the base learner) takes the current task data \(S_{t}\) and the meta-parameter (prior knowledge) \(u_{t}\) learned from previous tasks as input. Then, it outputs the learned hypothesis \(W_{t}\) of the current task and the updated meta-parameter \(u_{t+1}\) for the next task.

\[R_{T}^{}(u_{1:T})}}{{=}}_{t=1}^{ T}f_{t}(u_{t})-_{t=1}^{T}f_{t}(u_{T}^{*}),u_{t}^{*}}}{{=}}_{u}_{t=1}^{T}f_{t}(u).\]

The hindsight \(u_{T}^{*}\) converges to the true minimum \(u^{*}\) w.r.t \(\) as \(T\). So the _task-level_ trade-off can be well-balanced by designing algorithms with sub-linear regret to approach \(u^{*}\).

Shifting task environmentIn a more general CML setting where the task environment \(_{t}\) can change at each time \(t\), using a meta-learner designed to obtain sub-linear static regret will suffer a shifting comparator. As a result, the optimal \(u_{T}^{*}\) may not converge to a fixed point, and \(_{t=1}^{T}f_{t}(u_{T}^{*})\) can be vacuous when \(T+\), which induces the _meta-level_ forgetting. See a formal justification in Appendix A.2.2. To this end, we assume the horizon \(T\) of the sequential tasks can be divided into \(N\) slots, and the \(n\)-th slot contains \(M_{n}\) tasks. We further assume that the environment does not change with \(t\) within each slot. The \(N\) optimal priors, in hindsight, are noted as \(u_{1:N}^{*}\). Consequently, we define a more versatile dynamic regret as:

\[R_{T}^{}(u_{1:N}^{*})}}{{=}} _{n=1}^{N}_{k=1}^{M_{n}}[f_{n,k}(u_{n,k})-f_{n,k}(u_{n}^{*}) ],u_{n}^{*}}}{{=}}_{u }}_{k=1}^{M_{n}}f_{n,k}(u)\,.\]

Given \(_{n=1}^{N}M_{n}=T\), if \(N T\), the definition is equivalent to the conventional dynamic regret. If \(N 1\), it recovers the static regret. To balance the _meta-level_ trade-off, we need to find algorithms with sub-linear dynamic regret to sequentially approach the optimums in \(N\) slots (environments).

### Continual Meta-Learning objective

In practice, we aim to train the task-specific model with few-shot data and hope that the model will generalize on unseen data. Hence, we formulate the CML objective as selecting \(u_{t}\) at each time \(t\) to ensure a small Average Excess Risk (AER) upper bound for the task sequence \(\{_{t}\}_{t=1}^{T}\), which is defined as follows given the base learner \(\):

\[_{}^{T}}}{{=}}_{t=1}^{T}R_{}(,u_{t})_{t=1} ^{T}f_{t}(u_{t})=R_{T}^{}(u_{1:N}^{*})+ _{n=1}^{N}_{k=1}^{M_{n}}f_{n,k}(u_{n}^{*})\,.\]

We adopt the dynamic regret defined in shifting environments since it can recover the static setting w.l.o.g. Considering the two terms in the above upper bound of AER, the CML objective is two-fold: (a) design an appropriate online meta-learner to minimize the dynamic regret. (b) choose the optimal split of the \(T\) tasks to \(N\) stationary slots. Based on this objective, we propose a novel continual meta-learning framework and provide a corresponding theoretical analysis in the following sections.

## 4 Balancing bi-level learning-forgetting trade-off

### An illustrative example

The excess risk upper bound provides the intuition that we can quickly learn a new task similar to our prior knowledge by relying on this prior. In this case, the initialization/bias \(_{t}\) (model prior) is close to the base learner output \(w_{t}\) (_i.e._, a small distance \(\|_{t}-w_{t}\|^{2}\)), and the excess risk bound will be small. So we will elaborate on the learning-forgetting trade-off on an example of a shifting environment visualized in Fig. 2, based on the following definitions of _task similarities_ characterized by the _distances_ in the model parameter space .

**Definition 4.1**.: Let us denote \(}\) as the set of all the learned model parameters: \(}=\{w_{t}=(u_{t},S_{t})\}_{t=1}^{T}\{\}\). Let \(\) be the _diameter_ of \(}\) w.r.t norm \(\|\|\): \( w,v},\|w-v\|\). In shifting task environments, we denote \(_{n}\) the corresponding diameter for \(n\)-th slot. Assume \(\) is a convex set containing all the possible model parameters. Then let \(D\) be the diameter of \(\): \( w,v,\|w-v\| D\).

The task environment in Fig. 2 changes within three slots (i.e., three circles with diameters \(_{1:3}\) in light red). The dark red point is denoted as \(_{n}^{*}\) - the optimal initialization in hindsight for each slot.

Purple points \(w_{1:3}\) are the learned model parameters in different slots. In static environments, CML can access additional few-shot data to address the _task-level forgetting_. Assume the models in \(n\)-th slot can recover performance (without forgetting) of similar tasks within a distance \(_{n}/2\). Then, the optimal \(_{n}^{*}\) learned by CML can cover all the tasks in the slot with diameter \(_{n}\). For instance, the two tasks in the first slot, shown in Fig. 2, have a large distance \(\|w_{1}-w_{2}\|>_{1}/2\). Directly adapting from one to another suffers catastrophic forgetting. However, keeping the optimal prior \(_{1}^{*}\) can address the forgetting, where \(\|_{1}^{*}-w_{1}\|<_{1}/2\) and \(\|_{1}^{*}-w_{2}\|<_{1}/2\).

However, this does not work in _shifting_ environments. If we view three slots as one static environment (with large variance), the optimal overall model prior \(^{*}\) will be close to the origin, and the diameter is \(\), which is much larger than any one of the slot \(_{n}\). A larger diameter (_e.g._, \(\|^{*}-w_{3}\|\|_{3}^{*}-w_{3}\|\)) implies the need for more samples for each task to avoid forgetting, which is often not satisfied. A theoretical interpretation is provided in the discussion following Theorem 6.1. To address this, in the next session, we propose an algorithm that dynamically adapts the meta-parameter when environment change occurs so as to rapidly reconstruct meta-knowledge in each static slot. Moreover, it is not as catastrophic as directly adapting model parameters since the forgetting is w.r.t meta-knowledge, which is constrained in a core set of \(_{1:T}^{*}\), which has a diameter much smaller than \(}\).

### Dynamic algorithm

As discussed in the last section, it would be problematic in a shifting environment if the meta-learner adopts the same updating strategy as in the static setting. Specifically, if we aim to optimize the static regret, the learning rate of meta-parameter decays with \((1/t)\) for Follow The Leader (FTL)  and \((1/)\) for Online Gradient Descent (OGD). Clearly, such strategies hardly learn from new tasks in a shifting environment. On the other side, a constant learning rate of the meta-parameter is often adopted for optimizing the regular dynamic regret . This strategy could perform poorly when the environment only occasionally changes with a huge step, whereas the theoretical optimal learning rate becomes large. A large learning rate causes the meta learner to easily forget the previous tasks, making convergence difficult inside each slot. We offer a theoretical comparison with this in the discussion of Theorem 6.2.

To balance the learning and forgetting trade-off of meta-knowledge, we proposed the Dynamic Continual Meta-Learning (DCML) algorithm (see Algo. 1) that dynamically adjusts the learning rate of the meta-parameter. When a changing point is detected, the meta-learning rate \(_{t}\) adapts to a large hopping learning rate \(\), then relaunches the decay, in \((1/)\), from \(_{0}\) in each slot. Besides, the meta-parameter itself is also adaptively updated. According to \(_{t+1}=(1-_{t}}{_{t}})_{t}+_{t}}{_{t}}w_{t}\) in Algo. 1, adjusting \(_{t}\) and \(_{t}\) represent the _explicit_ control on task-level and meta-level trade-off, respectively. A corresponding discussion is provided in Appendix G.1. For a better understanding of DCML, we present a detailed introduction to it using SGD as the base learner in Appendix. G.3

``` Input: Convex set \(,_{1}=,T,_{1}>0\), initial learning rate \(\{_{0},\}\). if\(\) are trained model \(_{0}\)then \(_{1}=_{0}\) endif \(n=0,k=0\) for\(t=1\)to\(T\)do  Sample task distribution: \(_{t}_{t}\);  Sample dataset \(_{t}_{t}^{m_{t}}\);  Learn base parameter \(w_{t}=(u_{t},S_{t})\), \(e.g.\), \(=K\)-step SGD or RLM \(w_{t}=_{t}-_{i=1}^{K}_{t}_{_{i}^{-1}}_{S_{i} }(_{t}^{i-1}),_{t}^{0}=_{t}\), \(_{t}^{i}=_{t}^{i-1}-_{t}_{_{i}^{-1}}_{S_{i} }(_{t}^{i-1})\); \(w_{t}=_{w}_{S_{i}}(w)+ }\|w-_{t}\|^{2}\);  Estimate excess risk upper bound \(f_{t}(u_{t})\) in Eq. (1); if\(t=1\) or environment change detected:\(_{t}_{t-1}\)then \(M_{h}=k,n=n+1,k=1,_{t}=\); else \(k=k+1,_{t}=_{0}/\); endif  Update meta-parameter: \( f_{t}(u_{t})=(_{t}(a-\|_{t}-w_{t}\|^{2}+ _{t}_{0}}{_{t}^{2}}),(_{t}-w_{t})}{_{ t}})\); \(u_{t+1}=_{t}(u_{t}- f_{t}(u_{t}))\), \(_{t+1}=(1-_{t}}{_{t}})_{t}+_{t}}{_{t}}w_{t}\) ; \(_{t+1}=_{t}-_{t}((a_{t}-(\|_{t}-w_{t} \|^{2}+_{t}_{0})}{_{t}})\); endfor ```

**Algorithm 1** Dynamic Continual Meta-Learning (DCML)

Finally, the changing point can be detected with a broad class of Out-Of-Distribution (OOD) detection methods, _e.g._, Bayes Online Changing-point Detection (BOCD) , loss threshold , or setting a fixed-length sliding window. Specifically, when the window size equals \(1\), a constant learning rate is set to the meta learner, which recovers optimizing the typical dynamic regret as in .

Main theorem

We present a general theorem for the proposed DCML framework, which could be deployed for various base learners with the same excess risk upper bound form as Eq. (1).

**Theorem 5.1**.: _Consider both **static** and **shifting** environments. If the excess risk's upper bound of the base learner \((u_{t},S_{t})\) can be formulated as a unified form in Eq.(1), then, the **AER** of **DCML** (in Algo. 1) is upper bounded by:_

\[^{T}_{} _{n=1}^{N}^{2}+ _{n}+_{0})}_{n}+}{2}}_{}+_{n=1}^{N}_{n}G_{n}-1} }_{}+_{}}{T} _{n=1}^{N}G_{n}^{2}}}_{}\]

_Here, \(_{n}=_{k=1}^{M_{n}}_{n,k}\), \(_{n}=_{k=1}^{M_{n}}_{n,k}_{n,k}\), where the subscript \(n,k\) indicates \(k\)-th task in \(n\)-th slot. The optimal meta-parameter in hindsight of \(n\)-th slot is \(u_{n}^{*}=(_{n}^{*},_{n}^{*})\) and the path length of \(N\) slots is \(P^{*}=_{n=1}^{N-1}\|u_{n}^{*}-u_{n+1}^{*}\|+1\). \(_{n}^{*}=_{k=1}^{M_{n}}}{_{n}}w_{n,k}\) is the weighted average of the base learner outputs within the slot. \(_{n}^{*}=^{2}+_{n}+_{0})/a}\), where \(V_{n}^{2}=_{k=1}^{M_{n}}}{_{n}}\|_{n}^{*}-w_{ n,k}\|_{2}^{2}\) is the variance in \(n\)-th slot and \(_{n}=_{k=1}^{M_{n}}}{_{n}}_{n,k}\). Moreover, \(G_{n}\) is the maximal gradient norm of the cost function, \(_{n}\) is the diameter of meta-parameters in \(n\)-th slot, and \(_{}=\{_{n}\}_{n=1}^{N}\)._

The above theorem can be applied to static and shifting environments, where the proof is provided in Appendix D.2. For comparison, we also derive a general theorem from static regret in Appendix D.1.

**Important factors for bi-level trade-off** (a) The first term in Theorem 5.1 indicates the optimal trade-off can be achieved with the \(N\) optimal meta-parameters, which is affected by the task similarities within each environment (slot variance \(V_{n}^{2}\)) and the environment changing points. (b) The second term is the average static regret over slots, which is related to the task similarities via the slot diameter \(_{n}\) and represents how well the algorithm can balance the task-level trade-off. (c) The last term is the regret w.r.t the environmental shift, reflecting how well the algorithm can address the meta-level trade-off. It is affected by the path length \(P^{*}\), determined by the environment similarities and the non-stationarity (how frequently the environment changes).

_Remark 5.2_.: The above theorem has considered different sample numbers for each task. We can see the best initialization (bias) in hindsight (\(_{n}^{*}\)) is related to the weighted average of the task models \(w_{n,k}\) in \(n\)-th slot through \(_{n,k}\). E.g., for SGD, \(_{n,k}= L}\) implies that the task trained on more samples will have a smaller weight. Since it's easier to recover model performance for the tasks with more samples, less information on these tasks is conserved in the meta-knowledge.

## 6 Results on specific base learners

Theorem 5.1 provides general theoretical guarantees without specifying the base leaner. In this section, we discuss the theorem in depth with two typical base learners: SGD within the meta-initialization setting and Gibbs algorithm for the meta-regularization.

### Gibbs algorithm

**Theorem 6.1**.: _Let \(^{d}\). Assume that the loss \((,z)\) is \(L\)-Lipschitz \( z\) and that \(_{_{t}}()\) has \(\)-quadratic-growth for all \(t[T]\). Let \(w_{t}=_{}(_{t},_{t},S_{t})\) be the output of the Gibbs algorithm (Definition B.2) on \(S_{t}\), where \(_{t}\) is the mean of a prior Gaussian \((_{t},_{t}^{2}_{d})\) with \(_{t}=m_{t}^{-1/4}d^{-1/4}L^{-1/2}\) and \(_{t}\) is the inverse temperature. Consider **DCML** (Algo. 1) and further assume that each slot has equal length \(M\) and each task uses the sample number \(m\). Then we have_

\[^{T}_{}((1+_{n=1} ^{N}V_{n}++}}{M})}})\,.\]

The detailed proof is provided in Appendix E.1 and E.3. We also proved in Appendix E.2 that the static AER bound is in \((((V+1)+1/)m^{-1/4})\). As we claimed before, when \(N=1\) (which means the environment is static), the dynamic AER bound recovers the static AER bound.

**Benefits of DCML** Arbitrarily setting the prior mean \(_{t}\), the excess risk for single-task learning with Gibbs algorithm has an upper bound in \(((D+1)m^{-1/4})\), where \(D\) is the diameter of the parameter space. While the static AER becomes \(((V+1)m^{-1/4})\) with rate \((1/)\). Since \(V^{2}\) is the variance of all the outputs of the base learner, we have \(V D\), which illustrates the benefits of the proposed DCML algorithm. Moreover, in **shifting environments**, the bound is further improved in Theorem 6.1, where the complexity factor is decreased by \(_{n=1}^{N}V_{n} V\) with rate \((1/)\). This means that by considering environment change, DCML can achieve the same AER with a smaller \(M\) (fewer tasks), _i.e._, faster-constructing meta-knowledge in new environments.

### Stochastic Gradient Descent (SGD)

**Theorem 6.2**.: _Let \(^{d}\). Consider that \((,z)\) is convex, \(\)-smooth, \(L\)-Lipschitz \( z\). Assume that \(_{_{t}}()\) has \(\)-quadratic-growth for all \(t[T]\). Let SGD be the base learner for each task where it outputs \(w_{t}=_{}(_{t},_{t},S_{t})\) with learning rate \(_{t}\) and initialization \(_{t}\). Consider **DCML** (Algo. 1) for the SGD cost function. Further, assume each slot has equal length \(M\), and each task uses the sample number \(m\) and the same number of updating steps \(K\). Then we have_

\[^{T}_{}((_{n=1}^{N }V_{n}++}}{M})+})\,.\]

The detailed proof is provided in Appendix E.4 and E.6. The CML setting uses _offline batch-training_ for each task, so the related rate of the base learner is \(()\), which is determined by both the step number and the sample size. Let \(=_{n=1}^{N}V_{n}\). CML focuses on fast learning new tasks and quickly recovers the model performance on past tasks with few-shot examples (\(m\) is small). Hence, the optimal trade-off is dominated by the average deviation over slots \(\) and the path length \(P^{*}\), where \(P^{*}\) reflects the environment similarities and the non-stationarity.

**Static environment** If the environment is static, _i.e._, \(N=1\), we have \(P^{*}=1,M=T\). The bound becomes in \(((V+})+})\), which recovers the static AER bound in Appendix E.5.

**Shifting environment** When the environment occasionally changes with a large step, as discussed in Sec 4.2, \(N\) is small, and \(M,P^{*}\) is large. The proposed bound has an improved rate \((1/)\) on \(P^{*}\) compared to \((+}+}{NM}})\) - an equivalent form of the task average regret bound in . A more detailed comparison w.r.t  has been discussed in the proof for both static and dynamic AER. Similar to Theorem 6.1, the dynamic AER is improved w.r.t. the static AER with the improvement on complexity factor by \( V\). In addition, if the environments _differ a lot and frequently change_, \(N,P^{*}\) is large, and \(M\) is small. It's impossible to obtain a small AER, as demonstrated in the experiment on the Synbols dataset in Tab. 1.

## 7 Experiments

We first conduct analytic experiments on a synthetic dataset to offer an in-depth understanding of the theoretical counterpart. Then, we test the proposed algorithm on a recent CML benchmark - OSAKA , which empirically considers shifting environments on a large scale. The experimental results validate the proposed theory and illustrate the superiority of DCML in shifting environments.

### Moving 2D Gaussian

We introduce a simple synthetic dataset, where the data is generated by the following strategy. Let the environment be a 2D Gaussian distribution with a moving mean. Initially, \(_{0}=(_{0}^{*},_{2})\) with \(_{0}^{*}=(-6,6)^{T}\). Then at time point \(t\), \(_{t}=(_{t}^{*},_{2})\) is updated by: \(_{t}^{*}=_{t-1}^{*}+(1,1)\) with probability \(p\) and \(_{t}^{*}=_{t-1}^{*}\) with probability \(1-p\). Then we have \(N\) changing points \(\{t_{n}\}_{n=1}^{N}\). At each time step, we first sample \(w_{t}^{*}_{t}\) from the current environment \(_{t}\) as the mean of the task distribution \(_{t}=(w_{t}^{*},0.1_{2})\), which is also a 2D Gaussian. For task \(t\), we sample \(m\) examples \(S_{t}^{tr}_{t}^{m}\) as train data, \(m^{}\) samples \(S_{t}^{te}_{t}^{m^{}}\) as test data. Then the corresponding empirical risk is\(_{S^{tr}_{t}}(w)=_{i=1}^{m}\|w-x_{i}\|^{2}\). SGD uses \(_{t}\) as initialization and a learning rate \(_{t}\) to optimize this objective, and outputs \(w_{t}=_{}(_{t},_{t},S^{tr}_{t})\). Since \(w_{t}\) is obtained from limited data, it can be far away from \(w_{t}^{*}\). If the environment shifts to the \(n\)-th slot at \(t_{n}\), then the predicted environment mean \(_{n}^{*}=_{t_{n}}\). We denote \(_{n}^{*}=_{t_{n}}^{*}\) the mean of the actual environment, and the hindsight environment mean of \(n\)-th slot as \(_{n}^{*}=-t_{n}}_{t=t_{n}}^{t_{n+1}-1}w_{t}\).

In Fig. 3 (a), we visualize an example of tracking the environment shifts, where the task mean estimation is \(w_{t}\), the output of SGD. The other three terms are as described above. We further visualize the test loss, the dynamic, and the static AER bounds in Fig. 3 (b). The red dotted lines represent the actual environment-changing points. We observe that the static AER is much larger than the dynamic AER, consistent with the result in Theorem 6.2. In addition, the evolving trend of dynamic AER is the same as the average test loss, which does not hold for static AER. We also find that the learning rate of the initialization \(_{t}\) (meta Ir in Fig.3 (b)) is scheduled by the changing points, and the base learning rate is adjusted automatically by the algorithm. Additional experimental settings and results are provided in Appendix F.

### OSAKA benchmark

**Experimental set-up** OSAKA considers a task-agnostic setting of unknown task boundaries and simultaneously switches the task and environment with low probabilities. We slightly modify the setting to introduce more non-stationarity by always switching to a new task and _changing the environment with probability \(p\)_ at each timestep. We pre-train a meta-model as initialization from one environment. The new environment is selected with probability \(0.5\) as the pre-trained one and \(0.25\) for each of the two unseen environments, consisting of OOD tasks. We further test DCML on two representative datasets in OSAKA. For _Symbols_ dataset, the model is pre-trained to classify characters from an alphabet on randomized backgrounds. During CML time, the model is exposed to the pre-trained environment, a new alphabet for character classification, and a heterogeneous environment that conducts font classification, which induces _significant concept shift_. For _Omniglot-MNIST-FashionMNIST_(OMF), we pre-train the meta-model on the first \(1000\) classes of Omniglot . Then, it is exposed to the full Omniglot dataset and two unseen datasets - MNIST  and FashionMNIST  at CML time. More details and baselines are deferred to Appendix F.1.1.

We tested \(5\)-level of non-stationarity with \(p=\{0.2,0.4,0.6,0.8,1.0\}\) for OMF and for Synbols we set \(p=\{0.2,0.4,1.0\}\). The average test accuracy of all encountered tasks is used as the performance metric, which reflects the AER. In addition, we denote DCML (oracle) for the algorithm as offering correct environment-changing points and DCML (window) as using a fixed-length sliding window.

**Results** The average test accuracies at the last timestep for all environments and every single environment are reported in Tab.1 for OMF with \(p=0.4\) and Synbols with \(p=0.2\). Additional experimental results can be found in Appendix F.2.2. Clearly, DCML constantly outperforms all the baselines with improvements of \(1 2\%\) points on OMF data and \(0.7\%\) points on Synbols data compared to the best baseline method. A comparison w.r.t powerful baselines is presented in Fig. 5 for OMF data, where the average test accuracy over rounds in three environments is visualized separately.

#### 7.2.2 Analysis of learning and forgetting

In Fig. 5 for OMF data, MAML and ANIL do not suffer forgetting since they never update meta-parameters. The proposed algorithm forgets a bit more than CMAML but learns much faster in new environments, illustrating a better trade-off. The fine-grained adjustment for the meta-learning rate makes it possible to quickly reconstruct meta-knowledge w.r.t environment change and constantly learn in new environments. On the other hand, MetaBGD and MetaCOG perform poorly since using BGD hinders acquiring new knowledge. All the methods perform worse on Synbols data in Tab. 1, where each environment is more diverse than OMF, and the environments differ a lot, indicating a large \(_{n}\) and \(P^{*}\). Moreover, we observe a performance drop of CMAML on Synbols. Since CMAML uses empirical loss for modulation, it is prone to overfitting and can cause difficulty of convergence inside each slot when \(_{n}\) is large, as discussed in Sec. 4.1.

#### 7.2.3 Analysis of stability w.r.t shifting probability

We conduct an ablation study on OMF w.r.t environment change probability in Fig. 4. The results demonstrate the stability of the proposed algorithm and the increase in performance w.r.t \(p\). Since the environments have high similarities in OMF, the meta-model is better learned by seeing more tasks given the fixed time steps with a larger \(p\).

#### 7.2.4 Analysis of environment change detection

Empirical results suggest that DCML (window) performs better than DCML (oracle). This is reasonable when slots of small length exist where adapting to new environments causes additional context-switch costs (justified in Appendix F.2.1). Given the changing probability \(p\), the expected slot length is \(1/p\), used as the sliding window size. In practice, even though \(p\) is unknown, hyper-parameter searching for the one-dimensional window size is much easier. Because the similarity between any two environments can differ a lot, it's hard to find a single optimal hyper-parameter for change detection methods like BOCD or loss threshold.

## 8 Conclusion

This paper theoretically studies the stability and plasticity dilemma in CML. Based on the novel AER objective, it proposes a continual meta-learning framework in both static and shifting environments. The proposed DCML algorithm can quickly reconstruct meta-knowledge to alleviate forgetting and quickly adapt to new environments when change occurs. The corresponding theory provides tighter bounds and more flexible base learner selections. In addition, adaptively learning the meta-parameter can facilitate the training process in deep learning. Empirical evaluations on both synthetic and real datasets demonstrate the superiority of the proposed method.

    &  &  \\  Model & ANIL ENV & Omnilot & MNIST & FMMNIST & All ENV & Alpha & New Alpha & Point \\  Fine Tuning & 10.8\(\) 1.5 & 17.2\(\) 2.2 & 10.4\(\) 0.8 & 10.5\(\) 0.8 & 25.3\(\) 0.7 & 25.4\(\) 0.9 & 25.1\(\) 0.4 & 25.2\(\) 0.5 \\ MetaCOG  & 40.9\(\) 0.9 & 54.4\(\) 1.2 & 25.6\(\) 0.9 & 28.9\(\) 0.4 & 25.3\(\) 0.8 & 25.7\(\) 0.9 & 25.0\(\) 0.8 & 25.1\(\) 1.0 \\ MetaBGD  & 54.5\(\) 7.1 & 70.5\(\) 8.5 & 40.2\(\) 5.5 & 36.8\(\) 5.8 & 29.9\(\) 6.9 & 31.8\(\) 9.6 & 28.1\(\) 4.8 & 27.5\(\) 3.6 \\ ANIL  & 83.2\(\) 0.3 & 99.1\(\) 0.1 & 73.9\(\) 0.1 & 60.2\(\) 0.1 & 60.5\(\) 0.4 & 77.6\(\) 0.6 & 53.6\(\) 0.2 & 32.7\(\) 0.4 \\ MAML  & 83.8\(\) 0.4 & **99.3\(\) 0.1** & 75.4\(\) 0.1 & 61.0\(\) 0.1 & 76.7\(\) 0.4 & **96.8\(\) 0.1** & 73.1\(\) 0.1 & 40.5\(\) 0.7 \\ CMAML  & 85.9\(\) 0.7 & 98.1\(\) 0.3 & 83.8\(\) 1.8 & 64.2\(\) 0.9 & 61.9\(\) 2.5 & 76.2\(\) 2.1 & 56.8\(\) 3.2 & 37.9\(\) 2.4 \\  DCML(oracle) & 86.5\(\) 0.7 & 97.4\(\) 0.6 & 85.7\(\) 2.6 & 65.9\(\) 1.4 & 77.2\(\) 0.5 & 96.0\(\) 0.3 & 73.4\(\) 0.1 & **42.4\(\) 0.6** \\ DCML(window) & **86.9\(\) 0.7** & 97.2\(\) 0.7 & **86.8\(\) 2.0** & **66.7\(\) 1.7** & **77.4\(\) 0.4** & 96.2\(\) 0.2 & **73.8\(\) 0.2** & 42.4\(\) 0.7 \\   

Table 1: Average test accuracy (%) on OMF & Synbols datasets of OSAKA benchmark

Figure 5: Average test accuracy on (a) Omniglot, the pre-trained environment, (b) FashionMNIST, and (c) MNIST, the two unseen environments where the environment shifts with probability \(p=0.2\).