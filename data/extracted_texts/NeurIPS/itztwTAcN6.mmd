# A Universal Growth Rate for

Learning with Smooth Surrogate Losses

 Anqi Mao

Courant Institute

New York, NY 10012

aqmao@cims.nyu.edu &Mehryar Mohri

Google Research & CIMS

New York, NY 10011

mohri@google.com &Yutao Zhong

Courant Institute

New York, NY 10012

yutao@cims.nyu.edu

###### Abstract

This paper presents a comprehensive analysis of the growth rate of \(\)-consistency bounds (and excess error bounds) for various surrogate losses used in classification. We prove a square-root growth rate near zero for smooth margin-based surrogate losses in binary classification, providing both upper and lower bounds under mild assumptions. This result also translates to excess error bounds. Our lower bound requires weaker conditions than those in previous work for excess error bounds, and our upper bound is entirely novel. Moreover, we extend this analysis to multi-class classification with a series of novel results, demonstrating a universal square-root growth rate for smooth _comp-sum_ and _constrained losses_, covering common choices for training neural networks in multi-class classification. Given this universal rate, we turn to the question of choosing among different surrogate losses. We first examine how \(\)-consistency bounds vary across surrogates based on the number of classes. Next, ignoring constants and focusing on behavior near zero, we identify _minimizability gaps_ as the key differentiating factor in these bounds. Thus, we thoroughly analyze these gaps, to guide surrogate loss selection, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps. Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and \(\)-consistency bounds.

## 1 Introduction

Learning algorithms frequently optimize surrogate loss functions like the logistic loss, in lieu of the task's true objective, commonly the zero-one loss. This is necessary when the original loss function is computationally intractable to optimize or lacks essential mathematical properties such as differentiability. But, what guarantees can we rely on when minimizing a surrogate loss? This is a fundamental question with significant implications for learning.

The related property of Bayes-consistency of surrogate losses has been extensively studied in the context of binary classification. Zhang (2004); Bartlett et al. (2006) and Steinwart (2007) established Bayes-consistency for various convex loss functions, including margin-based surrogates. They also introduced excess error bounds (or surrogate regret bounds) for margin-based surrogates. Reid and Williamson (2009) extended these results to proper losses in binary classification.

The Bayes-consistency of several surrogate loss function families in the context of multi-class classification has also been studied by Zhang (2004) and Tewari and Bartlett (2007). Zhang (2004) established a series of results for various multi-class classification formulations, including negative results for multi-class hinge loss functions (Crammer and Singer, 2001), as well as positive results for the sum exponential loss (Weston and Watkins, 1999; Awasthi et al., 2022), the (multinomial) logistic loss (Verhulst, 1838; 1845; Berkson, 1944, 1951), and the constrained losses (Lee et al., 2004).

Later, Tewari and Bartlett (2007) adopted a different geometric method to analyze Bayes-consistency, yielding similar results for these loss function families. Steinwart (2007) developed general tools to characterize Bayes consistency for both binary and multi-class classification. Additionally, excess error bounds have been derived by Pires et al. (2013) for a family of constrained losses and by Duchi et al. (2018) for loss functions related to generalized entropies.

For a surrogate loss \(\), an excess error bound holds for any predictor \(h\) and has the form \(_{_{0-1}}(h)\) - \(_{_{0-1}}^{*}(_{}(h)-_{}^ {*})\), where \(_{_{0-1}}(h)\) and \(_{}(h)\) represent the expected losses of \(h\) for the zero-one loss and surrogate loss respectively, \(_{_{0-1}}^{*}\) and \(_{}^{*}\) the Bayes errors for the zero-one and surrogate loss respectively, and \(\) a non-decreasing function. The _growth rate_ of excess error bounds, that is the behavior of function \(\) near zero, has gained attention in recent research (Mahdavi et al., 2014; Zhang et al., 2021; Frongillo and Waggoner, 2021; Bao, 2023). Mahdavi et al. (2014) examined the growth rate for _smoothed hinge losses_ in binary classification, demonstrating that smoother losses result in worse growth rates. The optimal rate is achieved with the standard hinge loss, which exhibits linear growth. Zhang et al. (2021) tied the growth rate of excess error bounds in binary classification to two properties of the surrogate loss function: consistency intensity and conductivity. These metrics enable comparisons of growth rates across different surrogates. This prompts a natural question: can we establish rigorous lower and upper bounds for excess error growth rates under specific regularity conditions?

Frongillo and Waggoner (2021) pioneered research on this question in binary classification settings. They established a critical square-root lower bound for excess error bounds when a surrogate loss is locally strongly convex and has a locally Lipschitz gradient. Additionally, they demonstrated a linear excess error bound for Bayes-consistent polyhedral loss functions (convex and piecewise-linear) (Finocchiaro et al., 2019) (see also (Lapin et al., 2016; Ramaswamy et al., 2018; Yu and Blaschko, 2018; Yang and Koyejo, 2020)). More recently, Bao (2023) complemented these results by showing that proper losses associated with Shannon entropy, exponential entropy, spherical entropy, squared \(\)-norm entropies and \(\)-polynomial entropies, with \(>1\), also exhibit a square-root lower bound for excess error bounds relative to the \(_{1}\)-distance.

However, while Bayes-consistency and excess error bounds are valuable, they are not sufficiently informative, as they are established for the family of all measurable functions and disregard the crucial role played by restricted hypothesis sets in learning. As pointed out by Long and Servedio (2013), in some cases, minimizing Bayes-consistent losses can result in constant expected error, while minimizing inconsistent losses can yield an expected loss approaching zero. To address this limitation, the authors introduced the concept of _realizable \(\)-consistency_, further explored by Kuznetsov et al. (2014) and Zhang and Agarwal (2020). Nonetheless, these guarantees are only asymptotic and rely on a strong realizability assumption that typically does not hold in practice.

Recent research by Awasthi, Mao, Mohri, and Zhong (2022b, a) and Mao, Mohri, and Zhong (2023f, c, e, b) has instead introduced and analyzed \(\)-_consistency bounds_. These bounds are more informative than Bayes-consistency since they are hypothesis set-specific and non-asymptotic. Their work covers broad families of surrogate losses in binary classification, multi-class classification, structured prediction, and abstention (Mao, Mohri, Mohri, and Zhong, 2023a). Crucially, they provide upper bounds on the _estimation error_ of the target loss, for example, the zero-one loss in classification, that hold for any predictor \(h\) within a hypothesis set \(\). These bounds relate this estimation error to the surrogate loss estimation error.

Their general form is: \(_{_{0-1}}(h)-_{_{0-1}}^{*}()+ _{_{0-1}}()(_{}(h)-_{}^{*}()+_{}())\), where \(_{_{0-1}}^{*}()\) and \(_{}^{*}()\) represent the best-in-class expected losses for the zero-one and surrogate loss respectively, \(\) is a non-negative concave function and \(_{_{0-1}}()\) and \(_{}()\) are _minimizability gaps_. The exact definition of these gaps will be detailed later. For now, let us mention that they are non-negative quantities, upper-bounded by the approximation error of their respective loss functions. \(\)-consistency bounds subsume excess error bounds as a special case when the hypothesis set is expanded to include all measurable functions, in which case the minimizability gaps vanish. More generally, an \(\)-consistency bound with a \(\) function implies \(_{_{0-1}}(h)-_{_{0-1}}^{*}()+ _{_{0-1}}()(_{}(h)-_{}^{*}())+(_{}())\) since a concave function \(\) with \((0) 0\) is sub-additive over \(_{+}\). Thus, when the surrogate estimation loss \(_{}(h)-_{}^{*}()\) is minimized to \(\), the zero-one estimation error \(_{_{0-1}}(h)-_{_{0-1}}^{*}()\) is bounded by \(()+(_{}())-_{_{0 -1}}()\). Can we characterize the growth rate of \(\)-consistency bounds, that is how quickly the functions \(\) increase near zero?

**Our results**. This paper presents a comprehensive analysis of the growth rate of \(\)-consistency bounds for all margin-based surrogate losses in binary classification, as well as for _comp-sum losses_ and _constrained losses_ in multi-class classification. We establish a square-root growth rate near zero for margin-based surrogate losses \(\) defined by \((h,x,y)=(-yh(x))\), assuming only that \(\) is convex and twice continuously differentiable with \(^{}(0)\!>\!0\) and \(^{}(0)\!>\!0\) (Section 4). This includes both upper and lower bounds (Theorem 4.2). These results directly apply to excess error bounds as well. Importantly, our lower bound requires weaker conditions than (Frongillo and Waggoner, 2021, Theorem 4), and our upper bound is entirely novel. This work demonstrates that the \(\)-consistency bound growth rate for these loss functions is precisely square-root, refining the "at least square-root" finding of these authors (for excess error bounds). It is known that polyhedral losses admit a linear grow rate (Frongillo and Waggoner, 2021). Thus, a striking dichotomy emerges that reflects previous observations by these authors: \(\)-consistency bounds for polyhedral losses exhibit a linear growth rate in binary classification, while they follow a square-root rate for smooth loss functions.

Moreover, we significantly extend our findings to key multi-class surrogate loss families, including _comp-sum losses_(Mao et al., 2023f) (e.g., logistic loss or cross-entropy with softmax (Berkson, 1944), sum-losses (Weston and Watkins, 1999), generalized cross entropy loss (Zhang and Sabuncu, 2018)), and _constrained losses_(Lee et al., 2004; Awasthi et al., 2022a) (Section 5). In Section 5.1, we prove that the growth rate of \(\)-consistency bounds for comp-sum losses is exactly square-root. This applies when the auxiliary function \(\) they are based upon is convex and twice continuously differentiable with \(^{}(u)\!<\!0\) and \(^{}(u)\!>\!0\) for all \(u\) in \((0,]\). These conditions hold for all common loss functions used in practice. Further, in Section 5.2, we demonstrate that the square-root growth rate also extends to \(\)-consistency bounds for constrained losses. This requires the auxiliary function \(\) to be convex and twice continuously differentiable with \(^{}(u)\!>\!0\) and \(^{}(u)\!>\!0\) for any \(u 0\), alongside an additional technical condition. These are satisfied by all constrained losses typically encountered in practice.

These results reveal a universal square-root growth rate for smooth surrogate losses, the predominant choice in neural network training (over polyhedral losses) for both binary and multi-class classification in applications. Given this universal growth rate, how do we choose between different surrogate losses? Section 6 addresses this question in detail. To start, we examine how \(\)-consistency bounds vary across surrogates based on the number of classes. Then, focusing on behavior near zero (ignoring constants), we isolate minimizability gaps as the key differentiating factor in these bounds. These gaps depend solely on the chosen surrogate loss and hypothesis set. We provide a detailed analysis of minimizability gaps, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps. These findings help guide surrogate loss selection. Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and \(\)-consistency bounds (Appendix F). Importantly, combining \(\)-consistency bounds with surrogate loss Rademacher complexity bounds allows us to derive zero-one loss (estimation) learning bounds for surrogate loss minimizers (Appendix O).

For a more comprehensive discussion of related work, please refer to Appendix A. We start with the introduction of necessary concepts and definitions.

## 2 Preliminaries

**Notation and definitions.** We denote the input space by \(\) and the label space by \(\), a finite set of cardinality \(n\) with elements \(\{1,,n\}\). \(\) denotes a distribution over \(\).

We write \(_{}\) to denote the family of all real-valued measurable functions defined over \(\) and denote by \(\) a subset, \(_{}\). The label assigned by \(h\) to an input \(x\) is denoted by \((x)\) and defined by \((x)=*{argmax}_{y y}h(x,y)\), with an arbitrary but fixed deterministic strategy used for breaking the ties. For simplicity, we fix that strategy to be the one selecting the label with the highest index under the natural ordering of labels.

We will consider general loss functions \(\!:\!_{+}\). For many loss functions used in practice, the loss value at \((x,y)\), \((h,x,y)\), only depends on the value \(h\) takes at \(x\) and not on its values on other points. That is, there exists a measurable function \(\!:\!^{n}_{+}\) such that \((h,x,y)=(h(x),y)\), where \(h(x)=[h(x,1),,h(x,n)]\) is the score vector of the predictor \(h\). We will then say that \(\) is a _pointwise loss function_. We denote by \(_{}(h)\) the generalization error or expected loss of a hypothesis \(h\) and by \(_{}^{*}()\) the _best-in class error_:\(_{(x,y)}[(h,x,y)]\), \(_{}^{*}()=_{h^{}}_{}(h)\). \(_{}^{*}(_{ all})\) is also known as the _Bayes error_. We write \((y x)=(Y=y X=x)\) to denote the conditional probability of \(Y=y\) given \(X=x\) and \(p(x)=((1\!\!x),,(n\!\!x))\) for the conditional probability vector for any \(x\). We denote by \(_{}(h,x)\) the _conditional error_ of \(h\) at a point \(x\) and by \(_{}^{*}(,x)\) the _best-in-class conditional error_: \(_{}(h,x)=_{y}[(h,x,y) x]=_{y }(y\!\!x)\,(h,x,y),_{}^{*}( ,x)=_{h}_{}(h,x)\), and use the shorthand \(_{,}(h,x)=_{}(h,x)-_ {}^{*}(,x)\) for the _calibration gap_ or _conditional regret_ of \(\). The generalization error of \(h\) can be written as \(_{}(h)=_{x}[_{}(h,x)]\). For convenience, we also define, for any vector \(p=(p_{1},,p_{n})^{n}\), where \(^{n}\) is the probability simplex of \(^{n}\), \(_{}(h,x,p)=_{y}p_{y}\,(h,x,y)\), \(_{,}^{*}(x,p)=_{h}_{ }(h,x,p)\) and \(_{,}(h,x,p)=_{}(h,x,p)- _{,}^{*}(x,p)\). Thus, we have \(_{,}(h,x,p(x))=_{, }(h,x)\).

We will study the properties of a surrogate loss function \(_{1}\) for a target loss function \(_{2}\). In multi-class classification, \(_{2}\) is typically the zero-one multi-class classification loss function \(_{0-1}\) defined by \(_{0-1}(h,x,y)=1_{h(x) y}\). Some surrogate loss functions \(_{1}\) include the max losses (Crammer and Singer, 2001), comp-sum losses (Mao et al., 2023f) and constrained losses (Lee et al., 2004).

**Binary classification.** The definitions just presented were given for the general multi-class classification setting. In the special case of binary classification (two classes), the standard formulation and definitions are slightly different. For convenience, the label space is typically defined as \(=\{-1,+1\}\). Instead of two scoring functions, one for each label, a single real-valued function is used whose sign determines the predicted class. Thus, here, a hypothesis set \(\) is a family of measurable real-valued functions defined over \(\) and \(_{ all}\) is the family of all such functions. \(\) is _pointwise_ if there exists a measurable function \(_{+}\) such that \((h,x,y)=(h(x),y)\). The target loss function is typically the binary loss \(_{0-1}\), defined by \(_{0-1}(h,x,y)=1_{ sign(h(x)) y}\), where \({ sign}(h(x))=1_{h(x) 0}-1_{h(x)<0}\). Some widely used surrogate losses \(_{1}\) for \(_{0-1}\) are margin-based losses, which are defined by \(_{1}(h,x,y)=(-yh(x))\), for some non-decreasing convex function \(_{+}\). Instead of two conditional probabilities, one for each label, a single conditional probability corresponding to the positive class \(+1\) is used. That is, let \((x)=(Y=+1 X=x)\) denote the conditional probability of \(Y=+1\) given \(X=x\). The conditional error can then be expressed as:

\[_{}(h,x)=}_{y}[(h,x,y) x]=(x) (h,x,+1)+(1-(x))(h,x,-1).\]

For convenience, we also define, for any \(p\), \(_{}(h,x,p)=p(h,x,+1)+(1-p)(h,x,-1)\), \(_{,}^{*}(x,p)=_{h}_{ }(h,x,p)\) and \(_{,}(h,x,p)=_{}(h,x,p)-_{h }_{}(h,x,p)\). Thus, we have \(_{,}(h,x,(x))=_{, }(h,x)\).

To simplify matters, we will use the same notation for binary and multi-class classification, such as \(\) for the label space or \(\) for a hypothesis set. We rely on the reader to adapt to the appropriate definitions based on the context.

**Estimation, approximation, and excess errors.** For a hypothesis \(h\), the difference \(_{}(h)-_{}^{*}(_{ all})\) is known as the _excess error_. It can be decomposed into the sum of two terms, the _estimation error_, \((_{}(h)-_{}^{*}())\) and the _approximation error_\(_{}()=(_{}^{*}()- _{}^{*}(_{ all}))\):

\[_{}(h)-_{}^{*}(_{ all})=(_{}(h)-_{}^{*}())+(_{}^{*}( )-_{}^{*}(_{ all})).\] (1)

A fundamental result for a pointwise loss function \(\) is that the Bayes error and the approximation error admit the following simpler expressions. We give a concise proof of this lemma in Appendix B, where we establish the measurability of the function \(x_{}^{*}(_{ all},x)\).

**Lemma 2.1**.: _Let \(\) be a pointwise loss function. Then, the Bayes error and the approximation error can be expressed as follows: \(_{}^{*}(_{ all})=_{x}[_{}^ {*}(_{ all},x)]\) and \(_{}()=_{}^{*}()-_ {x}[_{}^{*}(_{ all},x)]\)._

For restricted hypothesis sets \((_{ all})\), the infimum's super-additivity implies that \(_{}^{*}()_{x}[_{}^{*}( ,x)]\). This inequality is generally strict, and the difference, \(_{}^{*}()-_{x}[_{}^{*}( ,x)]\), plays a crucial role in our analysis.

## 3 \(\)-consistency bounds

A widely used notion of consistency is that of _Bayes-consistency_ given below (Steinwart, 2007).

**Definition 3.1** (**Bayes-consistency)**.: A loss function \(_{1}\) is _Bayes-consistent_ with respect to a loss function \(_{2}\), if for any distribution \(\) and any sequence \(\{h_{n}\}_{n}_{ all}\), \(_{n+}_{_{1}}(h_{n})-_{_{1}}^{*}( _{ all})=0\) implies \(_{n+}_{_{2}}(h_{n})-_{_{2}}^{*}( _{ all})=0\).

Thus, when this property holds, asymptotically, a nearly optimal minimizer of \(_{1}\) over the family of all measurable functions is also a nearly optimal optimizer of \(_{2}\). But, Bayes-consistency does not supply any information about a hypothesis set \(\) not containing the full family \(_{}\), that is a typical hypothesis set used for learning. Furthermore, it is only an asymptotic property and provides no convergence guarantee. In particular, it does not give any guarantee for approximate minimizers. Instead, we will consider upper bounds on the target estimation error expressed in terms of the surrogate estimation error, \(\)_-consistency bounds_(Awasthi et al., 2022b, a; Mao et al., 2023f), which account for the hypothesis set \(\) adopted.

**Definition 3.2** (\(\)**-consistency bounds)**.: Given a hypothesis set \(\), an \(\)_-consistency bound_ relating the loss function \(_{1}\) to the loss function \(_{2}\) for a hypothesis set \(\) is an inequality of the form

\[ h,_{_{2}}(h)-_{_{2}}^ {*}()+_{_{2}}() _{_{1}}(h)-_{_{1}}^{*}()+ _{_{1}}(),\] (2)

that holds for any distribution \(\), where \(_{+}_{+}\) is a non-decreasing concave function with \( 0\)(Awasthi et al., 2022b, a). Here, \(_{_{1}}()\) and \(_{_{2}}()\) are _minimizability gaps_ for the respective loss functions. The minimizability gap for a hypothesis set \(\) and loss function \(\) is denoted by \(_{}()\) and defined as: \(_{}()=_{}^{*}()-_{}_{}^{*}(,x)\). It quantifies the discrepancy between the best possible expected loss within a hypothesis class and the expected infimum of pointwise expected losses. This gap is always non-negative: \(_{}()=_{h}_{} _{}(h,x)-_{}[_{h} _{}(,x) 0\), by the infimum's super-addiity, and is bounded above by the approximation error \(_{}()=_{h}_{} _{}(h,x)-_{}[_{h_{}}_{}(,x)\). We further study the key role of minimizability gaps in \(\)-consistency bounds and their properties in Section 6 and Appendix D. As shown in Appendix C, under general assumptions, minimizability gaps are essential quantities required in any bound that relates the estimation errors of two loss functions with an arbitrary hypothesis set \(\).

Thus, an \(\)-consistency bound provides the guarantee that when the surrogate estimation loss \(_{}(h)-_{}^{*}()\) is minimized to \(\), the following upper bound holds for the zero-one estimation error:

\[_{}(h)-_{}^{*}()(+ _{}())-_{_{0-1}}() ()+(_{}())-_{_{0-1 }}(),\]

where the second inequality follows from the sub-additivity of a concave function \(\) over \(_{+}\). We will demonstrate that, for smooth surrogate losses, \(()\) scales as \(\). Note, however, that, while \(()\) tends to zero when \( 0\) for functions \(\) derived in \(\)-consistency bounds, the remaining terms in the bound are constant. This is not surprising as, in general, minimizing the surrogate estimation error to zero _cannot_ guarantee that the zero-one estimation error will also converge to zero. This is well-known, for example, in the case of linear models (Ben-David et al., 2012). Instead, an \(\)-consistency bound provides the tightest possible upper bound on the estimation error for the zero-one loss when the surrogate estimation error is minimized.

The upper bound simplifies to \(()\) when the minimizability gaps are zero, which occurs when either \(=_{}\) (the set of all measurable functions) or in realizable cases, which are particularly relevant to the practical use of complex neural networks in applications. In Appendix I, we examine more general cases of small minimizability gaps, taking into account the complexity of \(\) and the distribution.

Our results cover in particular the special case of excess bounds (\(=_{}\)). Let us emphasize that, for \(_{}\), \(\)-consistency bounds offer tighter and more favorable guarantees on the estimation error compared to those derived from excess bounds analysis alone (see Appendix F).

When \(_{2}=_{0-1}\), the zero-one loss, we say that \(\) is the \(\)_-estimation error transformation function of a surrogate loss \(\)_ if the following holds:

\[ h,_{_{0-1}}(h)- _{_{0-1}}^{*}()+_{_{0-1}}( )_{}(h)-_{}^{*}()+ _{}(),\]

and the bound is _tight_. That is, for any \(t\), there exists a hypothesis \(h\) and a distribution such that \(_{_{0-1}}(h)-_{_{0-1}}^{*}()+_{_{0-1}}()=t\) and \(_{}(h)-_{}^{*}()+_{}( )=(t)\). An explicit form of \(\) has been characterized for binary margin-based losses (Awasthi et al., 2022b), as well as compsum losses and constrained losses in multi-class classification (Mao et al., 2023b). In the following sections, we will prove the property \((t)=(t^{2})\) (under mild assumptions), demonstrating a square-root growth rate for \(\)-consistency bounds. Appendix E provides examples of \(\)-consistency bounds for both binary and multi-class classification. Our analysis also suggests choosing appropriately \(\) and the function \(\) to ensure a small minimizability gap and to take into account the number of classes and other properties, as discussed in Section 6.

## 4 Binary classification

We consider the broad family of margin-based loss functions \(\) defined for any \(h\), and \((x,y)\) by \((h,x,y)=(-yh(x))\), where \(\) is a non-decreasing convex function upper-bounding the zero-one loss. Margin-based loss functions include most loss functions used in binary classification. As an example, \((u)=(1+e^{u})\) for the logistic loss or \((u)=(u)\) for the exponential loss. We say that a hypothesis set \(\) is _complete_, if for all \(x\), we have \(\{h(x) h\}=\). As shown by Awasthi et al. (2022b), the transformation \(\) has the following form for complete hypothesis sets:

\[(t)=_{u 0}f_{t}(u)-_{u}f_{t}(u).\]

Here, for any \(t\), \(f_{t}\) is defined by: \(\,u\), \(f_{t}(u)=(u)+(-u)\). The following result is useful for proving the growth rate in binary classification.

**Theorem 4.1**.: _Let \(\) be a complete hypothesis set. Assume that \(\) is convex and differentiable at zero and satisfies the inequality \(^{}(0)>0\). Then, the transformation \(\) can be expressed as follows:_

\[ t,(t)=f_{t}(0)-_{u}f_{t}(u).\]

Proof.: By the convexity of \(\), for any \(t\) and \(u_{-}\), we have

\[f_{t}(u)=(u)+(-u)(0)-tu^{ }(0)(0).\]

Thus, we can write \((t)=_{u 0}f_{t}(u)-_{u}f_{t}(u)(0)- _{u}f_{t}(u)=f_{t}(0)-_{u}f_{t}(u)\), where equality is achieved when \(u=0\). 

**Theorem 4.2** (Upper and lower bound for binary margin-based losses).: _Let \(\) be a complete hypothesis set. Assume that \(\) is convex, twice continuously differentiable, and satisfies the inequalities \(^{}(0)>0\) and \(^{}(0)>0\). Then, the following property holds: \((t)=(t^{2})\); that is, there exist positive constants \(C>0\), \(c>0\), and \(T>0\) such that \(Ct^{2}(t) ct^{2}\), for all \(0<t T\)._

**Proof sketch** First, we demonstrate that, by applying the implicit function theorem, \(_{u}f_{t}(u)\) is attained uniquely by \(a_{t}^{*}\), and that \(a_{t}^{*}\) is continuously differentiable over \([0,]\) for some \(>0\). The minimizer \(a_{t}^{*}\) satisfies the following condition: \(f_{t}^{}(a_{t}^{*})=^{}(a_{t}^{*})- ^{}(-a_{t}^{*})=0\). Specifically, at \(t=0\), we have \(^{}(a_{0}^{*})=^{}(-a_{0}^{*})\). Then, by the convexity of \(\) and monotonicity of the derivative \(^{}\), we must have \(a_{0}^{*}=0\) and since \(^{}\) is non-decreasing and \(^{}(0)>0\), we have \(a_{t}^{*}>0\) for all \(t(0,]\). Furthermore, since \(a_{t}^{*}\) is a function of class \(C^{}\), we can differentiate this condition with respect to \(t\) and take the limit \(t 0\), which gives the following equality: \(^{*}}{dt}(0)=(0)}{^{}(0)}>0\). Since \(_{t 0}^{*}}{t}=^{*}}{dt}0=(0)}{^{}(0)}>0\), we have \(a_{t}^{*}=(t)\). By Theorem 4.1 and Taylor's theorem with an integral remainder, \(\) can be expressed as follows: for any \(t[0,]\), \((t)=f_{t}(0)-_{u}f_{t}(u)=_{0}^{a_{t}^{*}}uf_{ t}^{}(u)\,du=_{0}^{a_{t}^{*}}u^{ }(u)+^{}(-u)du\). Since \(^{}(0)>0\) and \(^{}\) is continuous, there is a non-empty interval \([-,+]\) over which \(^{}\) is positive. Since \(a_{0}^{*}=0\) and \(a_{t}^{*}\) is continuous, there exists a sub-interval \([0,^{}][0,]\) over which \(a_{t}^{*}\). Since \(^{}\) is continuous, it admits a minimum and a maximum over any compact set and we can define \(c=_{u[-,]}^{}(u)\) and \(C=_{u[-,]}^{}(u)\). \(c\) and \(C\) are both positive since we have \(^{}(0)>0\). Thus, for \(t\) in \([0,^{}]\), the following inequality holds: \(C^{*})^{2}}{2}=_{0}^{a_{t}^{*}}uC\,du(t)=_{0} ^{a_{t}^{*}}u^{}(u)+^{ }(-u)du_{0}^{a_{t}^{*}}uc\,du=c^{*})^{2}}{2}\). This implies that \((t)=(t^{2})\). The full proof is included in Appendix K.

Theorem 4.2 directly applies to excess error bounds as well, when \(=_{}\). Importantly, our lower bound requires weaker conditions than (Frongillo and Waggoner, 2021, Theorem 4), and our upper bound is entirely novel. This result demonstrates that the growth rate for these loss functions is precisely square-root, refining the "at least square-root" finding of these authors. It is known that polyhedral losses admit a linear grow rate (Frongillo and Waggoner, 2021). Thus, a striking dichotomy emerges: \(\)-consistency bounds for polyhedral losses exhibit a linear growth rate, while they follow a square-root rate for smooth loss functions (see Appendix G for a detailed comparison).

Multi-class classification

In this section, we will study two families of surrogate losses in multi-class classification: comp-sum losses and constrained losses, defined in Section 5.1 and Section 5.2 respectively. Comp-sum losses and constrained losses are general and cover all loss functions commonly used in practice. We will consider any hypothesis set \(\) that is _symmetric_ and _complete_. We say that a hypothesis set is _symmetric_ when it does not depend on a specific ordering of the classes, that is, when there exists a family \(\) of functions \(f\) mapping from \(\) to \(\) such that \(\{[h(x,1),,h(x,n)] h\}=\{[f_{1}(x),,f_{n}(x)]  f_{1},,f_{n}\}\), for any \(x\). We say that a hypothesis set \(\) is _complete_ if the set of scores it generates spans \(\), that is, \(\{h(x,y) h\}=\), for any \((x,y)\).

### Comp-sum losses

Here, we consider comp-sum losses (Mao et al., 2023), defined as

\[ h,(x,y), ^{}(h,x,y)=}{_{y^{} }e^{h(x,y^{})}},\]

where \(_{+}\) is a non-increasing function. For example, \(\) can be chosen as the negative log function \(u-(u)\) for the comp-sum losses, which leads to the multinomial logistic loss. As shown by Mao et al. (2023), for symmetric and complete hypothesis sets, the transformation \(\) for the family of comp-sum losses can be characterized as follows.

**Theorem 5.1** (Mao et al. (2023, Theorem 3)).: _Let \(\) be a symmetric and complete hypothesis set. Assume that \(\) is convex, differentiable at \(\) and satisfies the inequality \(^{}()<0\). Then, the transformation \(\) can be expressed as_

\[(t)=_{,}}_{ \|u\|}()-(+u)- (-u)}.\]

Next, we will show that as with the binary case, for the comp-sum losses, the properties \((t)=(t^{2})\) and \((t)=O(t^{2})\) hold. We first introduce a generalization of the classical implicit function theorem where the function takes the value zero over a set of points parameterized by a compact set. We treat the special case of a function \(F\) defined over \(^{3}\) and denote by \((t,a,)^{3}\) its arguments. The theorem holds more generally for the arguments being in \(^{n_{1}}^{n_{2}}^{n_{3}}\) and with the condition on the partial derivative being non-zero replaced with a partial Jacobian being non-singular.

**Theorem 5.2** (Implicit function theorem with a compact set).: _Let \(F\) be a continuously differentiable function in a neighborhood of \((0,0,)\), for any \(\) in a non-empty compact set \(\), with \(F(0,0,)=0\). Then, if \((0,0,)\) is non-zero for all \(\) in \(\), then, there exist a neighborhood \(\) of \(0\) and a unique function \(\) defined over \(\) that is continuously differentiable and satisfies_

\[(t,), F(t,(t,), )=0.\]

Proof.: By the implicit function theorem (see for example (Dontchev and Rockafellar, 2009)), for any \(\), there exists an open set \(_{}=(-t_{},+t_{})(-_{},+ _{})\), \((t_{}>0\) and \(_{}>0)\), and a unique function \(_{}_{}\) that is in \(C^{1}\) and such that for all \((t,)_{}\), \(F(t,_{}(t),)=0\).

By the uniqueness of \(_{}\), for any \(^{}\) and \((t_{1},_{1})_{}_{^{}}\), we have \(_{}(t_{1},_{1})=_{^{}}(t_{1},_{1})\). Thus, we can define a function \(\) over \(=_{}_{}\) that is of class \(C^{1}\) and such that for any \((t,)\), \(F(t,(t,),)=0\).

Now, \(_{}(-_{},+_{})\) is a cover of the compact set \(\) via open sets. Thus, we can extract from it a finite cover \(_{ I}-_{},+_{}\), for some finite cardinality set \(I\). Define \((-t_{0},+t_{0})=_{ I}(-t_{},+t_{})\), which is a non-empty open interval as an intersection of (embedded) open intervals containing zero. Then, \(\) is continously differentiable over \((-t_{0},+t_{0})\) and for any \((t,)(-t_{0},+t_{0})\), we have \(F(t,(t,),)=0\). 

**Theorem 5.3** (Upper and lower bound for comp-sum losses).: _Assume that \(\) is convex, twice continuously differentiable, and satisfies the properties \(^{}(u)<0\) and \(^{}(u)>0\) for any \(u(0,]\). Then, the following property holds: \((t)=(t^{2})\)._

**Proof sketch** For any \([,]\), define the function \(_{}\) by \(_{}(t)=f_{t,}(0)-_{|u|}f_{t,}(u)\), where \(f_{t,}(u)=_{}(u)+_{}(-u)\), \(t\) and \(_{}(u)=(+u)\).

We aim to establish a lower and upper bound for \(_{[,]}_{}(t)\). For any fixed \([,]\), this situation is parallel to that of binary classification (Theorem 4.1 and Theorem 4.2), since we have \(_{}^{}(0)=^{}()<0\) and \(_{}^{}(0)=^{}()>0\). By Theorem 5.2 and the proof of Theorem 4.2, adopting a similar notation, while incorporating the \(\) subscript to distinguish different functions \(_{}\) and \(f_{t,}\), we can write \( t[0,t_{0}]\), \(_{}(t)=_{0}^{-_{t,}^{}}u _{}^{}(-u)+_{}^{}(u)du\), where \(a_{t,}^{*}\) verifies \(a_{0,}^{*}=0\) and \(^{*}}{ t}(0)=^{}(0)}{ _{}^{}(0)}=c_{}<0\). Then, by further analyzing this equality, we can show the lower bound \(_{[,]}-a_{t,}^{*}=(t)\) and the upper bound \(_{[,]}-a_{t,}^{*}=O(t)\) for some \(t[0,t_{1}]\), \(t_{1}>0\). Finally, using the fact that \(^{}\) reaches its maximum and minimum over a compact set, we obtain that \((t)=_{[,]}_ {}(t)=(t^{2})\). The full proof is included in Appendix L.

Theorem 5.3 significantly extends Theorem 4.2 to multi-class comp-sum losses, which include the logistic loss or cross-entropy used with a softmax activation function. It shows that the growth rate of \(\)-consistency bounds for comp-sum losses is exactly square-root, provided that the auxiliary function \(\) they are based upon is convex, twice continuously differentiable, and satisfies \(^{}(u)<0\) and \(^{}(u)>0\) for any \(u(0,]\), which holds for most loss functions used in practice.

### Constrained losses

Here, we consider constrained losses (see (Lee et al., 2004)), defined as

\[ h,(x,y), ^{}(h,x,y)=_{y^{} y }(h(x,y^{}))_{y}h(x,y)=0,\]

where \(_{+}\) is a non-decreasing function. On possible choice for \(\) is the exponential function. As shown by Mao et al. (2023), for symmetric and complete hypothesis sets, the transformation \(\) for the family of constrained losses can be characterized as follows.

**Theorem 5.4** (Mao et al. (2023, Theorem 11)).: _Let \(\) be a symmetric and complete hypothesis set. Assume that \(\) is convex, differentiable at zero and satisfies the inequality \(^{}(0)>0\). Then, the transformation \(\) can be expressed as_

\[(t)=_{ 0}_{u}2-()--t}{2}(+u)-+t}{2}(-u)}.\]

Next, we will show that for the constrained losses, the properties \((t)=(t)\) and \((t)=O(t)\) hold as well. Note that by Theorem 5.4, we have

\[2-t=2- {1}{n-1}_{ 0}_{u}()- (+u)-(-u)}.\]

Therefore, to prove \((t)=(t^{2})\), we only need to show

\[_{ 0}_{u}()-( +u)-(-u)}=(t^{2}).\]

For simplicity, we assume that the infimum over \( 0\) can be reached within some finite interval \([0,A]\), \(A>0\). This assumption holds for common choices of \(\), as discussed in (Mao et al., 2023). Furthermore, as demonstrated in Appendix N, under certain conditions on \(^{}\), the infimum over \([0,A]\) is reached at zero for sufficiently small values of \(t\). For specific examples, see (Mao et al., 2023, Appendix D.3), where \((t)=e^{t}\) is considered.

**Theorem 5.5** (Upper and lower bound for constrained losses).: _Assume that \(\) is convex, twice continuously differentiable, and satisfies the properties \(^{}(u)>0\) and \(^{}(u)>0\) for any \(u 0\). Then, for any \(A>0\), the following property holds:_

\[_{[0,A]}_{u}()-( +u)-(-u)}=(t^{2}).\]

**Proof sketch** For any \([0,A]\), define the function \(_{}\) by \(_{}(t)=f_{t,}(0)-_{u}f_{t,}(u)\), where \(f_{t,}(u)=_{}(u)+_{}(-u)\), \(t\) and \(_{}(u)=(+u)\). We aim to establish a lower and upper bound for \(_{[0,A]}_{}(t)\). For any fixed \([0,A]\), this situation is parallel to that of binary classification (Theorem 4.1 and Theorem 4.2), since we also have \(_{}^{}(0)=^{}()>0\) and \(_{}^{}(0)=^{}()>0\). By applying Theorem 5.2 and leveraging the proof of Theorem 4.2, adopting a similar notation, while incorporating the \(\) subscript to distinguish different functions \(_{}\) and \(f_{t,}\), we can write \( t[0,t_{0}]\), \(_{}(t)=_{0}^{a_{t,}^{}}u _{}^{}(u)+_{}^{}(-u)du\), where \(a_{t,}^{*}\) verifies \(a_{0,}^{*}=0\) and \(^{*}}{ t}(0)=^{}(0)}{ _{}^{}(0)}=c_{}>0\). Then, by further analyzing this equality, we can show the lower bound \(_{[0,A]}a_{t,}^{*}=(t)\) and the upper bound \(_{[0,A]}a_{t,}^{*}=O(t)\) for some \(t[0,t_{1}]\), \(t_{1}>0\). Finally, using the fact that \(^{}\) reaches its maximum and minimum over some compact set, we obtain that \((t)=_{[0,A]}_{}(t)=(t^{2})\). The full proof is included in Appendix M.

Theorem 5.5 significantly expands our findings to multi-class constrained losses. It demonstrates that, under some assumptions, which are commonly satisfied by smooth constrained losses used in practice, constrained loss \(\)-consistency bounds also exhibit a square-root growth rate.

## 6 Minimizability gaps

As shown in Sections 4 and 5, \(\)-consistency bounds for smooth loss functions in both binary and multi-class classification all admit a square-root growth rate near zero. In this section, we start by examining how the number of classes impacts these bounds. We then turn our attention to the minimizability gaps, which are the only distinguishing factors between the bounds.

### Dependency on number of classes

Even with identical growth rates, surrogate losses can vary in their \(\)-consistency bounds due to the number of classes. This factor becomes crucial to consider when the class count is large. Consider the family of comp-sum loss functions \(_{}^{}\) with \([0,2)\), defined as

\[_{}^{}(h,x,y)=^{}}{_{ y^{}}e^{h(x,y^{})}}= _{y^{}}e^{h(x,y^{})-h(x,y)} ^{1-}-1& 1,[0,2)\\ _{y^{}}e^{h(x,y^{})-h(x,y)} &=1,\]

where \(^{}(u)=-(u)1_{=1}+u^{-1}-11 _{ 1}\), for any \([0,2)\). Mao et al. (2023f, Eq. (7) & Theorem 3.1), established the following bound for any \(h\) and \([1,2)\),

\[_{_{0-1}}(h)-_{_{0-1}}^{*}() _{}_{_{}^{}}(h)- _{_{}^{}}^{*}()+_{_{ }^{}}()-_{_{0-1}}(),\]

where \(_{}(t)=t}\). Thus, while all these loss functions show square-root growth, the number of classes acts as a critical scaling factor.

### Comparison across comp-sum losses

In Appendix H, we compare minimizability gaps cross comp-sum losses. We will see that minimizability gaps decrease as \(\) increases. This might suggest favoring \(\) close to \(2\). But when accounting for \(n\), \(_{}^{}\) with \(=1\) (logistic loss) is optimal since \(n\) then vanishes. Thus, both class count and minimizability gaps are essential in loss selection. In Appendix J, we will show that the minimizability gaps can become zero or relatively small under certain conditions in multi-class classification. In such scenarios, the logistic loss is favored, which can partly explain its widespread practical application.

### Small surrogate minimizability gaps

While minimizability gaps vanish in special scenarios (e.g., unrestricted hypothesis sets, best-in-class error matching Bayes error), we now seek broader conditions for zero or small surrogate minimizability gaps to make our bounds more meaningful.

Due to space constraints, we focus on binary classification here, with multi-class results given in Appendix J. We address pointwise surrogate losses which take the form \((h(x),y)\) for a labeled point(\(x,y\)). We write \(A=\{h(x) h\}\) to denote the set of predictor values at \(x\), which we assume to be independent of \(x\). All proofs for this section are presented in Appendix I.

**Deterministic scenario**. We first consider the deterministic scenario, where the conditional probability \((y\!\!x)\) is either zero or one. For a deterministic distribution, we denote by \(_{+}\) the subset of \(\) over which the label is \(+1\) and by \(_{-}\) the subset of \(\) over which the label is \(-1\). For convenience, let \(_{+}=_{}(,+1)\) and \(_{-}=_{}(,-1)\).

**Theorem 6.1**.: _Assume that \(\) is deterministic and that the best-in-class error is achieved by some \(h^{*}\). Then, the minimizability gap is null, \(()=0\), iff_

\[(h^{*}(x),+1)=_{+}_{+},(h^{*}(x ),-1)=_{-}_{-}.\]

_If further \((,+1)\) and \((,-1)\) are injective and \(_{+}=(_{+},+1)\), \(_{-}=(_{-},-1)\), then, the condition is equivalent to \(h^{*}(x)=_{+}1_{x_{+}}+_{-}1_{x_{-}}\). Furthermore, the minimizability gap is bounded by \(\) iff \(p([(h^{*}(x),+1) y=+1]-_{+})+(1-p)([(h^{* }(x),-1) y=-1]-_{-})\). In particular, the condition implies:_

\[[(h^{*}(x),+1) y=+1]-_{+} [(h^{*}(x),-1) y=-1]-_{-}.\]

The theorem suggests that, under those assumptions, for the surrogate minimizability gap to be zero, the best-in-class hypothesis must be piecewise constant with specific values on \(_{+}\) and \(_{-}\). The existence of such a hypothesis in \(\) depends both on the complexity of the decision surface separating \(_{+}\) and \(_{-}\) and on that of the hypothesis set \(\). More generally, when the best-in-class classifier \(\)-approximates \(_{+}\) over \(_{+}\) and \(_{-}\) over \(_{-}\), then the minimizability gap is bounded by \(\). As an example, when the decision surface is a hyperplane, a hypothesis set of linear functions combined with a sigmoid activation function can provide such a good approximation (see Figure 1 for an illustration in a simple case).

**Stochastic scenario**. Here, we present a general result that is a direct extension of that of the deterministic scenario. We show that the minimizability gap is zero when there exists \(h^{*}\) that matches \(^{*}(x)\) for all \(x\), where \(^{*}(x)\) is the minimizer of the conditional error. We also show that the minimizability gap is bounded by \(\) when there exists \(h^{*}\) whose conditional error \(\)-approximates best-in-class conditional error for all \(x\).

**Theorem 6.2**.: _The best-in-class error is achieved by some \(h^{*}\) and the minimizability gap is null, \(()=0\), iff there exists \(h^{*}\) such that for all \(x\),_

\[*{}_{y}[(h^{*}(x),y) x]=_{ A} *{}_{y}[(,y) x].\] (3)

_If further \(*{}_{y}[(,y) x]\) is injective and \(_{}*{}_{y}[(,y) x ]=*{}_{y}[(^{*}(x),y) x]\), then, the condition is equivalent to \(h^{*}(x)=^{*}(x)\) a.s. for \(x\). Furthermore, the minimizability gap is bounded by \(\), \(()\), iff there exists \(h^{*}\) such that_

\[*{}_{x}\!\![*{}_{y}[(h^ {*}(x),y) x]-_{ A}*{}_{y}[(, y) x]].\] (4)

In deterministic settings, condition (4) coincides with that of Theorem 6.1. However, in stochastic scenarios, the existence of such a hypothesis depends on both decision surface complexity and the conditional distribution's properties. For illustration, see Appendix J.3 where we analyze the exponential, logistic (binary), and multi-class logistic losses.

We thoroughly analyzed minimizability gaps, comparing them across comp-sum losses, and identifying conditions for zero or small gaps, which help inform surrogate loss selection. In Appendix F, we show the crucial role of minimizability gaps in comparing excess bounds with \(\)-consistency bounds. Importantly, combining \(\)-consistency bounds with surrogate loss Rademacher complexity bounds yields zero-one loss (estimation) learning bounds for surrogate loss minimizers (see Appendix O).

## 7 Conclusion

We established a universal square-root growth rate for the widely-used class of smooth surrogate losses in both binary and multi-class classification. This underscores the minimizability gap as a crucial discriminator among surrogate losses. Our detailed analysis of these gaps can provide guidance for loss selection.

Figure 1: Approximation provided by sigmoid activation function.