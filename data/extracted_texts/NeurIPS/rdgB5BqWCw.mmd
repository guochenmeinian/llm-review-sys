# Predictive Uncertainty Quantification for Graph Neural Network Driven Relaxed Energy Calculations

Joseph Musielewicz

Department of Chemical Engineering, Carnegie Mellon University

Janice Lan

Fundamental AI Research, Meta Platforms, Inc., Menlo Park, CA

Matt Uyttendaele

Fundamental AI Research, Meta Platforms, Inc., Menlo Park, CA

###### Abstract

Graph neural networks (GNNs) have been shown to be astonishingly capable models for molecular property prediction, particularly as surrogates for expensive density functional theory calculations of relaxed energy for novel material discovery. However, one limitation of GNNs in this context is the lack of useful uncertainty prediction methods, as this is critical to the material discovery pipeline. In this work, we show that uncertainty quantification for relaxed energy calculations is more complex than uncertainty quantification for other kinds of molecular property prediction, due to the effect that structure optimizations have on the error distribution. We propose that distribution-free techniques are more useful tools for assessing calibration, recalibrating, and developing uncertainty prediction methods for GNNs performing relaxed energy calculations. We also develop a relaxed energy task for evaluating uncertainty methods for equivariant GNNs, based on distribution-free recalibration and using the Open Catalyst Project dataset. We benchmark a set of popular uncertainty prediction methods on this task, and show that latent distance methods, with our novel improvements, are the most well-calibrated and economical approach for relaxed energy calculations. Further, we challenge the community to develop improved uncertainty prediction methods for GNN-driven relaxed energy calculations, and benchmark them on this task.

## 1 Introduction

To keep up with growing energy demands, it is necessary to search for novel catalyst materials to enable more efficient storage of renewable sources of energy [1; 2; 3; 4]. Computational material discovery is crucial to this process, as it enables less expensive screening of an enormous space of possible catalyst materials than physical experiments. Faster and more accurate computational material discovery methods will be required to meet our society's renewable energy needs in the face of a rapidly changing climate.

Graph neural networks are state of the art in accelerating computational material discovery pipelines with machine learning potentials. Machine learning potentials work as surrogate models trained to approximate computationally expensive density functional theory (DFT) calculations of energy and forces on atomistic structures. This task is referred to as structure to energy and forces (S2EF). These energy and force calculations are used to iteratively perform geometric optimizations of atomic positions (referred to in this work as "relaxations"), to minimize their energy. These relaxed structure and relaxed energy calculations are what enable high-throughput predictions of catalyst performance in the real world. For a given catalyst-adsorbate system, the global minimum relaxed energy (adsorption energy) directly correlates with the reactivity and selectivity of reaction pathways on that catalyst surface [5; 6; 7; 8; 9; 10].

In recent years, graph neural network (GNN)s have made tremendous strides in replacing DFT codes with an inexpensive, accurate alternative [11; 12; 13; 14]. Thanks to methods like AdsorbML, GNNs can speed up adsorption energy calculations alone at the cost of accuracy, or in tandem with DFT at the cost of speed . The recent OCP Demo (https://open-catalyst.metademolab.com) is a publicly available tool where GNNs are used to calculate these adsorption energies without any expensive DFT calculations. However, a major limitation of current GNNs is their lack of uncertainty estimates for relaxed energy predictions. Ideally, users of these methods would know when it is safe to trust the GNN predictions, and when additional DFT calculations are warranted. In this work, we specifically examine methods of uncertainty quantification (UQ) of GNN predictions for this relaxed structure to relaxed energy (RS2RE) task.

## 2 Background

### AdsorbML

AdsorbML  is a method to calculate adsorption energy using machine learning potentials. In order to find the global minimum relaxed energy for a specified surface and adsorbate, this method places the adsorbate in many different starting configurations, relaxes each configuration, and returns the minimum of all the relaxed energies. Traditionally, this would be done using an _ab initio_ method such as DFT, but DFT is very costly and this approach is infeasibly expensive. AdsorbML uses GNNs as a surrogate for DFT to perform the relaxations instead, requiring only a solitary DFT single point calculation for the relaxed structure to verify the relaxed energy. The required number of expensive DFT calculations is further reduced by using GNNs to filter out all but the few most promising candidates. This method provides an adjustable spectrum of trade-offs between accuracy and efficiency, with one balanced option finding an equivalent or better adsorption energy 87.36% of the time while reducing DFT compute by more than a factor of 2000. Ideally, no DFT would be required, but even state of the art GNNs are unreliable energy predictors, and using them alone drops the success rate to 56%. Without DFT, such as in the OCP demo, we need uncertainty metrics, so users know when to trust the results of these models.

### Graph Neural Networks

This work focuses on quantifying uncertainty prediction methods for EquiformerV2 , a GNN model architecture for molecular property prediction. We choose EquiformerV2 because it is the current state of the art in molecular property prediction for catalyst materials, according to the Open Catalyst Project (OCP) leaderboard. We also compare it to Genmet-OC , another high performing GNN on the leaderboard. Both of these models are used in the OCP Demo, to run the AdsorbML algorithm and predict minimum relaxed energies without the use of expensive DFT calculations.

### Predictive Uncertainty Quantification

Many prior studies have examined the application of UQ techniques to machine learning potentials and molecular property prediction. Most UQ metrics seek to measure some description of the calibration of an uncertainty prediction method. The most popular UQ metrics are miscalibration area, Spearman's rank correlation coefficient, and the negative log likelihood of the errors given the uncertainties[16; 17; 18; 19; 20; 21; 22]. These metrics all rely on an assumption of of Gaussian errors, and all have significant drawbacks. Notably these metrics are not consistently in agreement about which uncertainty prediction performs best, even within a single study .

Calibration is the primary UQ metric for making direct comparisons between uncertainty prediction methods. Prior work by Rasmussen et al., Pernot, and Levi et al. show distribution free methods of measuring local and global miscalibration: the CI(Var(Z)) test and the error-based calibration plot [24; 25; 26]. These approaches can be more effective in describing the performance of predicted uncertainties of surrogate models when the expected error distribution cannot be assumed to be Gaussian. Error-based calibration measures can also be used to recalibrate uncertainty predictions, allowing a variety of uncertainty quantification methods to be recalibrated and compared.

Methods

### Uncertainty Prediction Methods for GNNs

In this work we examine four common methods of uncertainty prediction on a pre-trained GNN: ensembles, latent space distances, mean variance estimation, and sequence regression models. The most prolific of these are ensemble methods, where a set of similar surrogate models are trained on similar sets of data to perform the same task, and the variance between their predictions is used to calculate the uncertainty of the model. We train ensembles of GNNs on the S2EF task to calculate energies of adsorbate-catalyst structures, but we are interested in the performance of predicting the uncertainty of EquiformerV2 on the relaxed structures. We construct three different ensembles for testing this approach, which we refer to as architecture (11 members), bootstrap (10 members), and parameter ensembles (6 members). Prior work has shown that diversity between members is typically the most important factor in developing a well-calibrated and expressive ensemble for uncertainty prediction [27; 28; 29; 30]. The architecture ensemble contains a variety of GNN model architectures, while the parameter and bootstrap ensembles contain only EquiformerV2 models, but vary the number of parameters, and the composition of training data respectively. More details on ensemble construction can be found in the supplemental information. Because we are specifically interested in the uncertainty of relaxed energy predictions, which requires a sequence of prior energy/force predictions during the relaxation, we hypothesize that taking the mean or the maximum of the predicted variances over each step (here referred to as a frame) of the trajectory might contain additional information which better models the uncertainty. For each ensemble composition, we test this theory by computing the uncertainty using the variance at the first frame, last frame, mean over all the frames, and max over over all the frames.

Another proven uncertainty method is the use of latent space distances [16; 17; 24]. In this approach, we extract some latent representation of each training point from the GNN, and create an index of these points to compute the L2-norm of the distance from any new test point to the training points. In practice, using libraries such as FAISS, the computational cost of this method is the lowest of any of the uncertainty methods we test . Because this approach produces a distance of arbitrary scale, it is necessary to recalibrate on some calibration set to produce a meaningful uncertainty estimate. Prior work has shown the latent distance method to be effective for rotationally invariant models such as GemNet-OC, however EquiformerV2 improves upon the accuracy of GemNet-OC by preserving rotational equivariance all the way through the model [14; 11]. We expect this rotational equivariance will contribute undesirable noise to the L2 distance between latent representations, so we test this hypothesis by comparing the performance of the full EquiformerV2 latent representation, and the latent representation of its single rotationally invariant channel. We also compare these methods to the performance of using the latent space representation of GemNet-OC, trained on the same data, but used to predict the uncertainty of the same EquiformerV2 model. Additionally, for each latent distance approach, we test a novel strategy of computing the latent distance on a per-atom basis, and then taking the mean/max/sum over these distances. We compare this method to the more common approach of computing the latent distance for an entire frame by taking the mean of latent representations over all the atoms. More details on how all of the latent representations were extracted, and how the distances were computed can be found in the supplemental information.

The final categories of methods we test are mean variance estimation (MVE) and sequence regression models [22; 20; 23; 32]. In practice, we implement these methods in similar ways. For MVE methods we append an output-head, or an ensemble of output-heads to the EquiformerV2 architecture, and fine-tune this new output head on the calibration set to predict the residual of the energy prediction of the larger model. For the sequence regression model, we extract the same latent space representations used in the latent distance method, and train a sequence regression transformer architecture to predict the residual of the energy prediction on the calibration set. A significant distinction between these approaches is that the sequence regressor takes the latent representations of each frame of the trajectory as input, in sequence, with the hypothesis that some additional information about the uncertainty of the GNN model might be contained within its latent representations along the trajectory. More details on the implementation of both of these approaches can be found in the supplemental information.

### Uncertainty Quantification Metrics

We hypothesize that the negative log likelihood (NLL), Spearman's rank correlation coefficient, and miscalibration area, which all assume a normal distribution of errors, will be inappropriate for uncertainty quantification on this task due to bias inherent to the RS2RE task. We test this using an approach suggested by Rasmussen et al. where we compute a simulated NLL and Spearman's coefficient by sampling from a normal distribution, with variance equal to the uncertainty, for each predicted uncertainty taken from an ensemble . We perform this simulation 1000 times, and compute the average simulated metrics, then we compare this to the empirical NLL and Spearman's coefficient for the predicted uncertainty and measured error. If these metrics differ significantly from their simulated counterparts, then we infer that the errors do not follow the assumed normal distribution, and we consider these metrics to be ineffective for quantifying or calibrating these uncertainty methods.

Prior work by Pernot suggests the use of a distribution-free method to test whether an uncertainty method is calibrated . The CI(Var(Z)) test uses the BCa boostrap method to compute a confidence interval of Var(Z) on a set of errors and uncertainties, without making any assumptions about the distribution [33; 34]. If 1 lies within the confidence interval, the uncertainty method is considered calibrated, because the Z values for a calibrated uncertainty metric are expected to have a variance of 1. Rasmussen et al. expands on this to suggest using an error-based calibration plot to quantify local calibration, distribution-free [26; 24]. The error-based calibration plot is predicated on the expected relationship between the root mean variance (RMV) and root mean square error (RMSE) being one-to-one.

\[RMSE=_{i}_{i}^{2}} RMV=_{i} _{i}^{2}} 1\] (1)

We sort the test points by their predicted uncertainty, and then bin them into 20 bins. We compute the RMV of each bin, and the RMSE of each bin, using the BCa bootstrap method to compute a 95% confidence interval for the RMSE. Then we fit a line through the points to test for calibration, ideally the fitted line should have a high \(R^{2}\)correlation with the points, and be as close as possible to the parity line. We can identify problems with local miscalibration where the parity line does not lie within the binned RMSE confidence intervals.

### Recalibration and Evaluation

We compare all of the uncertainty methods benchmarked in this work after recalibration on a calibration set. In this case we use Open Catalyst 2020 dataset (OC20) in-domain validation set, and relax each structure (approximately 25,000 structures) to the same relaxation criterion as OC20, with the publicly available EquformerV2 31M parameter checkpoint. We then compute a the Vienna Ab initio Simulation Package (VASP) single point calculation on the final frame to serve as the ground truth energy value for the RS2RE task [35; 36; 37; 38]. This set of relaxed energy predictions serves as the calibration set, and we repeat this process with the out-of-domain-both validation set to serve as the test set for this task. To recalibrate each uncertainty method, we use same the approach as the error-based calibration plot, to find the line of best fit through the binned RMSE/RMV points of the calibration set. We then simply recalibrate the uncertainty using the formula for the line of best fit:

\[_{recalibrated}=_{fit}*_{uncalibrated}+_{fit}\] (2)

We suggest a small modification to the characterization of global calibration of uncertainties recalibrated with the error-based method. For this task, in addition to checking the global calibration

  Method & Task & \(NLL\) & \(NLL_{sim}\) & \(\) & \(_{sim}\) & \(A_{mis}\) & CI(Var(Z)) \\  bootstrap & RS2RE & 0.366 & -0.163(0.005) & 0.569 & 0.615(0.004) & 0.144 & [1.34, 2.14] \\ bootstrap & S2EF & 0.055 & -0.195(0.001) & 0.673 & 0.682(0.001) & 0.051 & [0.87, 0.90] \\ architecture & RS2RE & 0.380 & -0.117(0.005) & 0.604 & 0.610(0.004) & 0.136 & [1.25, 2.20] \\ architecture & S2EF & 0.075 & -0.097(0.001) & 0.670 & 0.641(0.001) & 0.023 & [0.68, 0.71] \\  

Table 1: Simulated NLL and Spearman correlations for uncertainty predictions made by identical ensembles across S2EF and RS2RE tasks. Values in parenthesis represent the standard deviation across 1000 runs of simulation.

CI(Var(Z)) test, we also select the most effective uncertainty metric by measuring the \(R^{2}\) correlation of the binned RMSE/RMV ratio with the parity line. By recalibrating the uncertainty method on the calibration set and then testing on the test set, we can make direct comparisons between different uncertainty methods on the basis of their \(R^{2}\) correlation with the parity line.

## 4 Results

### Error Distribution and Uncertainty Quantification Metrics

Predicting the uncertainty of a surrogate model on the broader S2EF task is a much less challenging task than uncertainty prediction for RS2RE. We believe this is the result of selecting points only from the end of the structural optimization process creating a non-Gaussian distribution of errors from the predictions. This can be seen in Table 1, where the NLL and Spearman's rank correlation coefficientmetrics are very different from the simulated negative log likelihood (NLL\({}_{sim}\)) and simulated Spearman's rank correlation coefficient (Spearman\({}_{sim}\)) metrics for uncertainty predictions made by ensembles on the RS2RE task. Meanwhile, for the uncertainty predictions made on the S2EF

Figure 1: Error-based calibration plots on the out-of-domain test set for the two types of ensembles (bootstrap and architecture) and the two tasks (S2EF and RS2RE). With this distribution-free calibration measurement, we see that the bootstrap ensemble leads to a similar fit of the parity line in Figures 0(a) and 0(b) (i.e. the slope and intercept match more closely), while the architecture ensemble leads to a similar fit in Figures 0(c) and 0(d). Note that the S2EF task contains more data, which causes the extremely narrow confidence intervals around the RMSE at each point, and the higher recorded errors.

task, the NLL and Spearman's rank correlation coefficientmetrics are closer to their corresponding simulated metrics, although still too dissimilar to be considered Gaussian. This indicates that this assumption is closer to reality for the S2EF task than for the RS2RE task. Therefore metrics such as NLL, Spearman's rank correlation coefficient, and miscalibration area are much less appropriate for quantifying the performance of uncertainty prediction methods for the RS2RE task. Therefore we turn to distribution-free uncertainty quantification to understand which uncertainty prediction methods are most effective.

Using distribution-free uncertainty quantification in Figure 1, we see that the metrics are more similar across the RS2RE and RS2RE tasks for identical uncertainty prediction methods. The bootstrap ensemble is an overall worse fit on both tasks, and we can see it becomes less well calibrated earlier for both tasks. The architecture ensemble stays better calibrated for longer in both tasks. This is a good indicator of the distribution-free UQ techniques being more appropriate for characterizing uncertainty methods on this task, we expect to see similar behavior for the same method on similar tasks. This is unlike the UQ techniques which assume normally distributed errors, where the miscalibration area, NLL, and Spearman correlation coefficient report different comparisons of the same method across similar tasks. The larger quantity of data in the RS2RE task causes much smaller 95% confidence intervals to be computed by the bootstrap method for each bin. There is still some disparity between the two tasks, the RS2RE predictions are better calibrated globally according to the error-based calibration parity \(R^{2}\). This is most likely a result of propagation of errors through the relaxation process being inherently difficult to account for by any method of uncertainty prediction.

### Benchmarking Uncertainty Prediction Methods

Of the four methods we benchmark on the RS2RE task, the latent distance method is the best performer according to the distribution-free uncertainty quantification techniques, as seen in Table 2. The latent distance approach is the only method to pass the CI(Var(Z)) global calibration test, and it achieves the best calibration according to the parity \(R^{2}\)on the out-of-domain test set. Note that all four models are capable of a comparable fit (if this test set were used for calibration) according to the similar scores on the fit \(R^{2}\).

In figure 2 the error-based calibration plots and parity plots characterize the best performing latent distance method and ensemble. We see that the latent distance method generally retains good local calibration throughout, which we can see by the error bars of nearly every confidence interval intersect the parity line. Both methods do see poor calibration in the 0.6 eV to 1.0 eV range, but the ensemble method also experiences poorer calibration earlier than its competitor, starting near 0.3 eV. We see in the parity plots the latent distance method fans out in a more linear fashion, the parity line appears to be a good upper bound for the errors for longer than the ensemble method, which is desirable in a well-calibrated uncertainty method.

### Comparing Distance Methods

The latent space representation sampled from EquiformerV2 after all graph convolutional interactions have been performed is inherently equivariant with respect to rotations. This is a valuable property for training GNN regressors to predict properties of molecular systems, but it renders distance measures invalid in the latent space, as latent space distance should be invariant and not equivariant to rotations of input structures. Since the same structure can be rotated to a numerically different latent space representation, many more samples must be present in training set for the latent space distance to

   Method & Parity \(R^{2}\) & Fit \(R^{2}\) & Slope & Intercept & CI(Var(Z)) \\  Latent distance & **0.952** & 0.967 & 1.022 & 0.026 & **[0.85, 1.58]*** \\ GNN ensemble & 0.905 & 0.940 & 0.946 & 0.070 & [1.09, 2.13] \\ Residual model & 0.331 & 0.955 & 1.200 & 0.127 & [2.64, 3.72] \\ MVE & 0.849 & 0.964 & 1.187 & 0.041 & [1.20, 1.97] \\   

Table 2: Distribution free calibration metrics for the best performing candidate uncertainty method for each of the categories we benchmark. Note that only the distance method is calibrated globally according to the CI(Var(Z)) test.

be meaningful. Therefore we compare distances measured using the full latent space, to distances measured using only the rotationally invariant (degree 0 spherical harmonic) latent space.

Table 3 shows that the choice of latent space representation is by far the most important factor in choosing a good distance metric for predicting uncertainty. The second most important factor is choosing a per-atom distance metric, instead of taking the mean over all the atoms. And finally selecting the right type of per atom distance measure makes some difference, but is less significant as long as a good latent representation is chosen.

## 5 Conclusion

Effective uncertainty prediction methods for GNN relaxed energies are key to the development of faster and more accurate screening techniques for novel material discovery. Quantifying the performance of uncertainty methods on relaxed energy predictions is especially complex, due to distribution assumptions built into most commonly employed UQ techniques. Distribution-free techniques which employ bootstrapped confidence intervals, such as the CI(Var(Z)) test and error-based calibration plots, have been shown to be better metrics for analyzing the calibration

Figure 2: Parity plot and calibration for the best performing latent distance method and best performing ensemble method. The ensemble method shows worse global calibration, and suffers from poor local calibration at higher predicted uncertainties. The latent distance method shows better global and local calibration, with only slight local miscalibration in the second highest uncertainty bin.

of a UQ method in similar contexts, and we employ them here to great effect. We show that latent distance methods outperform ensembles and other uncertainty methods on the RS2RE task, which is of practical relevance to workflows such as AdsorbML. We also show that the choice of latent representation is very important to the calibration of the latent distance as an uncertainty metric. In the GNN latent space, atom-wise distances produce better calibrated than system-wise distances. Using rotationally invariant latent representations is crucial to producing calibrated distance measures, and the rotationally invariant latent space of GemNet-OC, a less accurate model, serves to compute a more well calibrated measure of uncertainty for EquiformerV2 than its own rotationally invariant latent space. Finally, we challenge the community to improve on this RS2RE task for predicting uncertainties, using our proposed recalibration framework as a measure. Future work in this area should also explore the prediction of global minimum energy uncertainties directly, and the development of model architectures training methods or distance measures which preserve rotational equivariance while producing meaningful latent space distances.

   Latent Rep. & Distance & Parity \(R^{2}\) & Fit \(R^{2}\) & Slope & Intercept & CI(Var(Z)) \\  EqV2 equiv. & atom max & -0.078 & 0.577 & 0.761 & 0.182 & [1.31, 1.67] \\ EqV2 equiv. & atom mean & **0.813** & 0.902 & 0.974 & 0.060 & [1.27, 180.65] \\ EqV2 equiv. & system mean & -14.709 & 0.000 & -0.236 & 0.547 & [1.29, 1.56] \\ EqV2 equiv. & atom sum & 0.672 & 0.907 & 0.711 & 0.159 & [1.24, 2.48] \\ EqV2 inv. & atom max & 0.842 & 0.892 & 0.983 & 0.054 & [0.98, 1.45]* \\ EqV2 inv. & atom mean & **0.866** & 0.913 & 0.974 & -0.037 & [0.90, 127178.90]* \\ EqV2 inv. & system mean & -0.628 & 0.795 & 1.540 & -0.061 & [1.38, 1.69] \\ EqV2 inv. & atom sum & 0.826 & 0.955 & 0.750 & 0.081 & [0.74, 1.35]* \\ GNOC & atom max & 0.924 & 0.967 & 0.951 & 0.064 & [0.94, 1.43]* \\ GNOC & atom mean & 0.932 & 0.965 & 1.108 & 0.006 & [0.87, 1.87]* \\ GNOC & system mean & 0.818 & 0.973 & 1.126 & 0.050 & [1.20, 1.66] \\ GNOC & atom sum & **0.952** & 0.967 & 1.022 & 0.026 & [0.85, 1.58]* \\   

Table 3: Distribution free calibration metrics for each latent distance method tested. The latent representations correspond to model and method that was used to extract the latent representation for each atom. The distance corresponds to how the distance was computed for each system. A description of these methods can be found in section 3.1. A * indicates the method is calibrated according the CI(Var(Z)) test.