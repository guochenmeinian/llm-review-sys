# Tiny Time Mixers (TTMs): Fast Pre-trained Models

for Enhanced Zero/Few-shot Forecasting of Multivariate Time Series

 Vijay Ekambaram

IBM Research

Bangalore, India

vijayel2@in.ibm.com

&Arindam Jati

IBM Research

Bangalore, India

arindam.jati@ibm.com

&Pankaj Dayama

IBM Research

Bangalore, India

pankajdayama@in.ibm.com

&Sumanta Mukherjee

IBM Research

Bangalore, India

sumann03@in.ibm.com

&Nam H. Nguyen

IBM Research

Yorktown Heights, NY, USA

nnguyen@us.ibm.com

&Wesley M. Gifford

IBM Research

Yorktown Heights, NY, USA

wmgifford@us.ibm.com

&Chandra Reddy

IBM Research

Yorktown Heights, NY, USA

creddy@us.ibm.com

&Jayant Kalagnanam

IBM Research

Yorktown Heights, NY, USA

jayant@us.ibm.com

###### Abstract

Large pre-trained models excel in zero/few-shot learning for language and vision tasks but face challenges in multivariate time series (TS) forecasting due to diverse data characteristics. Consequently, recent research efforts have focused on developing pre-trained TS forecasting models. These models, whether built from scratch or adapted from large language models (LLMs), excel in zero/few-shot forecasting tasks. However, they are limited by slow performance, high computational demands, and neglect of cross-channel and exogenous correlations. To address this, we introduce Tiny Time Mixers (TTM), a compact model (starting from 1M parameters) with effective transfer learning capabilities, trained exclusively on public TS datasets. TTM, based on the light-weight TSMixer architecture, incorporates innovations like adaptive patching, diverse resolution sampling, and resolution prefix tuning to handle pre-training on varied dataset resolutions with minimal model capacity. Additionally, it employs multi-level modeling to capture channel correlations and infuse exogenous signals during fine-tuning. TTM outperforms existing popular benchmarks in zero/few-shot forecasting by (4-40%), while reducing computational requirements significantly. Moreover, TTMs are lightweight and can be executed even on CPU-only machines, enhancing usability and fostering wider adoption in resource-constrained environments. The model weights for reproducibility and research use are available here, while enterprise-use weights under the Apache license can be accessed as follows: the initial TTM\({}_{Q}\)variant here, and the latest variants (TTM\({}_{B}\), TTM\({}_{E}\), TTM\({}_{A}\)) weights are available here. The source code for the TTM model along with the usage scripts are available here.

Introduction

Multivariate time-series (TS) forecasting entails predicting future values for multiple interrelated time series based on their historical values. The channels1 being forecast are called target variables, while those influencing the forecasts are termed exogenous variables. This field has seen significant advancements through the application of statistical and machine learning (ML) methods across various domains such as weather, traffic, retail, and energy.

**Related Work:** Recent advances in multivariate forecasting have been marked by the advent of Transformer-based  approaches, exemplified by models like PatchTST , Autoformer , and FEDFormer . These models have demonstrated notable improvements over traditional statistical and ML methods. Furthermore, architectures based on MLP-Mixer , such as TSMixer  and TimeMixer , have emerged as efficient Transformer alternatives, boasting 2-3X reduced compute requirements with no accuracy compromise compared to their Transformer counterparts.

Recently, there has been substantial interest in the research community to build general pre-trained or foundation models (FMs) for TS forecasting that can successfully transfer the learning to unseen target TS dataset, similar to the successes in NLP and vision tasks. However, pre-training in the TS domain is particularly challenging due to the limited public availability of the datasets and the diverse nature across applications. Early in 2024, this interest culminated in the release of several "large" and "massive" TS pre-trained models for forecasting, generating considerable excitement in the research community. Among these releases were Moment3, TimesFM 3, Chronos3, Moirai3, and Laglam3 that successfully established strong benchmarks in zero-shot forecasting. In addition, there has been a trend towards leveraging pre-trained large language models (LLMs) for TS forecasting, treating the forecasting task as a form of cross-domain transfer learning. These universal cross-transfer approaches, exemplified by recent works such as LLMTime , Time-LLM , and GPT4TS , exhibit promising outcomes in zero/few-shot forecasting scenarios. However, most of these "large" TS pre-trained models demand extremely high computational resources, given their scale ranges from several hundred million to billions of parameters. Given the recent surge in popularity of "small" language models that address practical resource and cost constraints in real-world industrial settings, this work considers the following question: _Can "tiny" pre-trained models succeed in the TS domain too? If so, can they outperform the zero/few-shot forecasting results of "large" TS pre-trained models demanding significant computational resources and runtime?_ Surprisingly, as we demonstrate in this work, the answer is _yes_.

Toward this, we propose **Multi-level Tiny Time Mixers (TTM)**, a significantly smaller pre-trained model (starting from 1 million (M) parameters) for effective zero/few-shot multivariate forecasting. In particular, TTM supports channel correlations and exogenous signals, which are critical and practical business requirements in the context of multivariate forecasting, features lacking in many existing TS pretrained models. TTM is based on the light-weight TSMixer architecture that uses MLPMixer blocks interleaved with simple gated attention as alternatives to the quadratic time-consuming self-attention blocks in Transformers, which makes TTM pre-training and fine-tuning extremely fast. TTM is pre-trained using multiple public datasets (\(\)1 billion (B) samples) from the Monash and LibCity data repositories. Note that the datasets exhibit considerable diversity in characteristics, such as different domains, temporal resolutions4 (ranging from seconds to days),

Figure 1: **Size, time, and accuracy overview of TTM\({}_{B}\) vs. open-sourced pre-trained TS benchmarks2. We plot each model based on its model size and per batch CPU inference time. The X% mentioned for each baseline indicates that the baseline’s forecast is X% less accurate compared to TTM’s forecast. Full details in Tables .**

lengths, and numbers of channels. Pre-training on such heterogeneous datasets using extremely small models requires specialized architectural advancements. Hence, TTM proposes the following enhancements to the TSMixer architecture for resource-constrained pre-training/fine-tuning: (i) **adaptive patching (AP)** across layers, considering the varied suitability of patch lengths for different datasets, (ii) **diverse resolution sampling (DRS)** to augment the data for increasing coverage across different resolutions, (iii) **resolution prefix tuning (RPT)** to explicitly embed resolution information in the first patch, facilitating resolution-conditioned modeling while training on diverse datasets. Moreover, our approach leverages **multi-level modeling**, where TTMs are first pre-trained in a channel-independent way, and then fine-tuned with channel mixing to incorporate correlations across targets and exogenous channels in the target domain.

**Outline of TTM's key capabilities:** (1) Amidst the prevalence of "large" pre-trained models demanding significant compute and training time, our work is the first to demonstrate the power of transfer learning using "tiny" TS pre-trained models for zero/few-shot forecasting. (2) Pre-training tiny models on heterogeneous multi-resolution datasets with extremely limited model capacity is challenging. Towards this, we propose various **architectural and training enhancements**, such as **AP**, **DRS**, and **RPT** for robust and resource-constrained pre-training/fine-tuning workflows (as defined above). (3) TTM employs a **multi-level modeling strategy** to explicitly model channel correlations, and incorporate exogenous signals - a crucial capability lacking in most of the existing pre-trained models. (4) Through extensive evaluation of zero/few-shot forecasting on 11 datasets, we establish that TTM models, with sizes as small as 1M parameters, consistently outperform the forecasts of "large" TS pretrained models while offering significant computational benefits. Figure 1 highlights that TTM outperforms popular benchmarks in all three primary dimensions: size, runtime, and accuracy. (5) Given their compact size, zero-shot inference and fine-tuning of TTM models can be easily executed with just one GPU or in CPU-only environments. This greatly enhances the practical adoption and extended reach of our pre-trained models with ease of use.

## 2 TTM Components

Let \(^{c sl}\) be a multivariate time series of length \(sl\) and number of channels \(c\). The forecasting task can be formally defined as predicting the future values \(^{c^{} fl}\) given the history/context \(\). Here, \(fl\) denotes the forecast length/horizon, and \(c^{}\) denotes number of forecast channels, where \(c^{} c\). The predictions from the model are denoted by \(}^{c^{} fl}\). In a general multivariate forecasting task, each channel falls into one of the following categories: (a) **Target variables (mandatory):** channels for which forecasts are required, (b) **Exogenous variables (optional):** channels influencing the targets, with known or estimated values throughout the forecast horizon.

### Multi-level Modeling:

TTM follows a multi-level architecture consisting of four key components (see Figure 2(a)): (1) The **TTM backbone** is assembled using building blocks derived from the efficient TSMixer architecture . TSMixer is based on MLP blocks interleaved with gated

Figure 2: TTM overview (a) Refer to Sections 2 and 3, (b) Refer to Section 3.1, (c) Refer to Section 3.2

attention, that enable the mixing of features within patches, across patches and channels, surpassing existing Transformer-based TS approaches with minimal computational requirements. Since TSMixer was not designed to handle multi-resolution data with limited capacity, we introduce various novel enhancements to it as explained later. (2) **TTM decoder** follows the same backbone architecture but is considerably smaller in size, approximately 10-20% of the size of the backbone, (3) **Forecast head** consists of a linear head designed to produce the forecast output, and (4) Optional **Exogenous mixer** serves to fuse exogenous data into the forecasting process. The TTM decoder and forecast head together constitute the **TTM head**, whose weights get updated during the fine-tuning process. This multi-level model refactoring is required to dynamically change the working behavior of various components based on the workflow type, as explained in Section 3. In addition to the above primary components, there is also a preprocessing component as explained next.

2.2 Preprocessing:As shown in Figure 2(a) with colorless blocks, the historical time series \(\) is first **normalized** per instance to have zero mean and unit standard deviation for each channel, to tackle any possible distribution shifts . This process is reversed at the end before computing the loss. The normalized data \(}\) is subsequently **patched**, \(_{p}^{c n pl}\), into \(n\) non-overlapping windows, each of length \(pl\), and then passed to the TTM backbone. Patching, as introduced in , has proven to be highly valuable for forecasting. Its effectiveness lies in preserving local semantic information, accommodating longer history, and reducing computation.

## 3 TTM Methodology

3.1 Pre-training Workflow:TTM operates in two stages: pre-training and fine-tuning (Figure 2(a)). In the pre-training stage, we train the model on a large collection of diverse public datasets. Since the primary focus of TTM is forecasting, pre-training is modeled with a direct forecasting objective. TTM is first pre-trained in a univariate fashion with independent channels on all the existing datasets. Due to varied channel counts in pre-training datasets, modeling multivariate correlations is challenging here; it is addressed later during the fine-tuning stage. Multivariate pre-training datasets are initially transformed into independent univariate TS \((_{1},,_{N})^{c(=1) sl}\). These are pre-processed (Section 2.2), and subsequently fed into the TTM backbone for multi-resolution pre-training. The output of the backbone \(_{h}^{L}^{(c=1) n hf}\) is passed through the decoder and forecast head to produce the forecast \(}^{(c=1) fl}\) which is then reverse-normalized to return to the original scale. We pre-train the TTM with mean squared error (MSE) loss calculated over the forecast horizon: \(=||-}||_{2}^{2}\). Thus for a given input context length \(sl\) and forecast length \(fl\), we get a pre-trained model capturing the common temporal forecasting dynamics and seasonal patterns as observed in the overall pre-training data.

3.1.1 Multi-Resolution Pre-training via TTM Backbone:In TTM, our goal is to create models that are extremely small yet capable of generalizing well across a wide range of diverse datasets with varying resolutions. This is a significant challenge because the models can easily under-fit due to their small size. To tackle these challenges of resource-constrained pre-training, we introduce the following enhancements to the TSMixer backbone.

**Adaptive patching (AP):** The TTM backbone is crafted with an adaptive patching architecture where different layers of the backbone operate at varying patch lengths and numbers of patches. Since each dataset in the pre-training corpora may perform optimally at a specific patch length, this approach greatly aids in generalization when the pretraining datasets with different resolutions are introduced. Moreover, it helps in scenarios when the availability of the pre-training data is limited as adaptive patching quickly generalizes the model across different granularities. As shown in Figure 2(b), the patched data \(_{p}^{c n pl}\) is passed through a embedding layer to project it to the patch hidden dimension, \(_{h}^{c n hf}\). If the resolution prefix tuning module is activated (as explained later), the resolution prefix is concatenated with \(_{h}\). For notational simplicity, we denote the concatenated tensor with \(_{h}\) as well. The TTM backbone consists of \(L\) levels, each comprising \(M\) TTM blocks with identical patch configurations. The first block in the first level receives \(_{h}\). The first TTM block in the \(i\)-th level, \(i=2,,L\), receives the processed data \(_{h}^{(i-1)}^{c n hf}\) from the previous block. Each TTM block is further comprised of a patch partition block, a vanilla TSMixer block, and a patch merging block. The patch partition block at level \(i\) increases the number of patches by a factor of \(K_{i}\) and reduces the patch dimension size by the same factor by reshaping \(_{h}^{(i-1)}^{c n hf}\)to \(_{h}^{i}^{c(n K_{i})(hf/K_{i})}\), where \(K_{i}=2^{(L-i)}\). Figure 2(b) shows the TTM backbone for \(L=3\) and \(M=2\). Note that, we set \(hf=m 2^{L-1}\) for some integer \(m\). Then, TSMixer is applied to the adapted data \(_{h}^{i}\). Finally, the output from TSMixer is again reshaped to its original shape (i.e., \(^{c n hf}\)) in the patch merging block. In subsequent layers, for each increment in level \(i\), the number of patches is halved and the patch dimension doubled. This enables better generalization for small models as we pre-train across multiple datasets. The idea of adaptive patching is popular and very successful in the vision domain (e.g., Swin Transformers ) and we successfully apply it to the TS domain to resolve multi-resolution issues in modelling diverse TS datasets. Note that adaptive patching is enabled only in the backbone and not in the decoder, which is designed to be very lightweight.

**Augmentation via diverse resolution sampling (DRS):** A significant challenge in TS pre-training datasets is the scarcity of public datasets with diverse resolutions. Generally, high-resolution datasets will account for a larger fraction of the samples given their finer sampling resolution. Without adjustment to the training strategy, this can lead to a model that is biased toward the finer resolution data. To overcome this, different strategies are applied to high-resolution datasets to balance the volume of samples at lower resolutions and lead to more uniform coverage. Strategies used include: 1) averaging \(k\) samples in sequential, non-overlapping windows to produce a lower resolution dataset; and 2) conventional decimation where only every \(k\)th sample is retained. In both cases, the integer \(k\) is chosen to achieve the desired resolution from the resolution of the base dataset. For example, from a 4-second resolution dataset, we derive multiple datasets at minutely (\(k=15\)) and hourly resolutions (\(k=900\)). Note that the original high-resolution dataset remains within the pool of pre-training datasets. This methodology increases the number of datasets for each resolution which greatly improves the model performance.

**Resolution prefix tuning (RPT):** This technique explicitly learns and incorporates a new patch embedding as a learnable prefix into the input data based on the input resolution (see Figure 2(b) and Table 8). Similar to the concept of prefix tuning , this approach provides an explicit signal to the model about the resolution for resolution-conditioned modeling. First, we map every resolution to a unique integer, which is then passed through an embedding layer to project it to the hidden dimension, \(hf\). Subsequently, we expand the embedding across all channels to have a representation of shape \(c 1 hf\). This resolution-based learnable embedding is particularly beneficial in quickly modeling huge volumes of diverse resolution datasets with limited modelling capacity, as the model can easily decouple the data from different resolutions for resolution-conditioned modeling. In addition, RPT also helps in scenarios when the context length (\(sl\)) is short. In these scenarios, automatically detecting the resolution becomes a challenge for the model. Hence, by explicitly fusing the resolution information as a prefix, we can enhance the model's ability to learn effectively across resolutions without increasing its size.

### Fine-tuning Workflow:

In the fine-tuning workflow, we work with data from the _target_ domain that has no overlap with the pre-training datasets. We have three options here: (a) In **zero-shot** forecasting, we directly use the pre-trained model to evaluate on the _test_ part of the target data; (b) In **few-shot** forecasting, we utilize only a tiny portion (5-10%) of the _train_ part of the target data to quickly update the pre-trained weights of the _TTM head_, and subsequently evaluate it on the _test_ part; (c) In **full-shot** forecasting, we fine-tune the pre-trained weights of the _TTM head_ on the entire _train_ part of the target data, and then evaluate on the _test_ part.

The backbone is frozen during fine-tuning, and still operates in a channel-independent univariate fashion. However, the slim decoder in the TTM Head can be fine-tuned utilizing channel mixing or channel independence for multivariate or univariate target data, respectively. If pure multivariate modeling is needed, then the channel-mixer block in all the TSMixer components (see Figure 2(b)) in the decoder is enabled to explicitly capture the cross-channel correlations. The forecast head and reverse normalization perform similar operations as in the pre-training stage. The fine-tuning also optimizes the forecasting objective with MSE loss. This thoughtful multi-level design choice ensures that our backbone excels in channel-independent pre-training, enabling effective temporal correlation modeling across diverse datasets. Simultaneously, the decoder handles target-data-specific tasks like channel-correlation modeling and fine-tuning. In addition, if the target data has exogenous variables, then an exogenous mixer block is applied to the actual forecasts as explained next.

**Exogenous Mixer Block:** As described in Section 2, the future values of the exogenous channels are known in advance. Let the forecast from the forecast head be \(}^{c fl}\). Let the channels \(_{0},,_{c^{}}\) denote the target variables and \(_{c^{}+1},,_{c}\) denote all exogenous variables with their future values known. First, we replace the forecast values for the exogenous channels with the _true_ future values (\(\)) and transpose it: \(}_{e}=[}_{0},,}_{c^{}},_{c^ {}+1},,_{c}]^{fl c}\). Next, to learn inter-channel _lagged_ correlations, we patch \(}_{e}\) into a series of overlapped windows (i.e., patching with stride\(=1\)) to create a new tensor: \(}_{e,p}^{fl c}\), where \(=2 l+1\) with \(l\) being the context length to incorporate on either side of a time point5. Subsequently, we pass \(}_{e,p}\) through a vanilla TSMixer block with channel mixing enabled. Thus, the lagged dependency of the forecasts for the target channels on the exogenous channels is seamlessly learned. Finally, we attach a linear head to produce the forecasts for the target channels which is then reshaped as \(}^{c^{} fl}\). Thus, TTM easily handles exogenous infusion which is a practical requirement in any industrial forecasting problem. Figure 2(c) depicts this procedure.

## 4 Experiments and Results

### Datasets & Metrics :

Pre-training employs a subset of \(\)1B samples from Monash  and Libcity  data collection. We specifically exclude a few datasets (like yearly, monthly) as they do not possess sufficient length for the long-term forecasting task. Moreover, we remove all the datasets that we utilize for evaluation (i.e., Weather, Electricity, and Traffic). For zero/few-shot evaluation we consider seven public datasets (**D1**): ETTH1, ETTH2, ETTM1, ETTM2, Weather, Electricity, and Traffic as popularly used in most prior state-of-the-art (SOTA) works . Since these datasets do _not_ contain any exogenous variables nor exhibit cross-channel correlation benefits, we incorporate four other datasets (**D2**) for separately validating the efficacy of the decoder channel mixing and exogenous mixer module: bike sharing (BS) , carbon capture plant (CC) , and 2 more datasets, Application (APP) and Service (SER), from Business and IT observability domain . For full details, refer to the Appendix C. We use mean squared error (MSE) as the standard error metric. In addition, we use the following relative improvement metrics: (i) forecast improvement percentage (_f-imp(%)_) which refers to the MSE (%) improvement of TTM over the considered baseline, averaged across all datasets, and (ii) size improvement metric (_s-imp(X)_) is calculated as the ratio of the baseline model size to the TTM model size (i.e., total parameters).

### SOTA Benchmarks:

We benchmark6 TTM with 24 of the latest open-sourced SOTA forecasting models categorized as follows: (a) **TS pre-trained models:** Lag-Llama , TimesFM , Moirai , Chronos  and Moment . (b) **LLM-based TS pre-trained models:** GPT4TS , LLMTime , Time-LLM , UniTime  (c) **Self-supervised pre-trained models:** SimMTM ,Ti-MAE , TST , LaST , TF-C , CoST  and Ts2Vec  (d) **Other architectures:** PatchTST , TSMixer , TimeMixer , iTransformer , DLin- ear  and TimesNet , FEDFormer  and Autoformer .

### TTM Model Details:

We pre-train three primary variants of TTM as follows: (i) **TTM-Base (TTM\({}_{B}\))**: 1M parameter model trained with context length, \(sl=512\) and patch length, \(pl=64\), (ii) **TTM-Enhanced (TTM\({}_{E}\))**: 4M parameter model trained with \(sl=1024\) and \(pl=128\), (iii) **TTM-Advanced (TTM\({}_{A}\))**: 5M parameter model trained with \(sl=1536\) and \(pl=128\). These TTMs are pre-trained using the 1B pre-training dataset, which takes only 24-30 hours with 6 A100 GPUs, a notably faster time compared to existing counterparts which often take days to weeks. Additionally, for secondary studies, we utilize **Quick TTM (TTM\({}_{Q}\))**, a variant trained on a smaller subset of the Monash dataset (\(\)250 million samples), requiring only 4-6 hours for pre-training.

Although, a TTM model needs to be pre-trained for a specific forecast length (FL), we provide two forecast length adaption (FLA) techniques (explained in Section 4.7) that enable a pre-trained TTM to work across different FLs. Users can either build a direct pre-trained model (from one of the above variants) targeting a specific FL, or use the FLA techniques to adapt an existing TTM model to their

[MISSING_PAGE_FAIL:7]

Table 4, TTM\({}_{B}\) surpasses GPT4TS by 15% and Time-LLM by 10% in the few-shot 5% setting, where only 5% of the train data is used for fine-tuning. In addition, we also report the Few-shot 5% results of several popular SOTA architectures in Table 4, where TTM demonstrates superior performance. This underscores the significance of TTM's pre-trained weights, which substantially contribute to its effectiveness in data-constrained scenarios. Likewise, TTM also excels in few-shot cross-transfer learning tasks outperforming popular SOTAs (including SimMTM ) as shown in the Appendix F.6.

Alternatively, if the train split of the complete target dataset is available, head probing using the entire dataset becomes feasible. This involves fine-tuning the model head using all available data while keeping the backbone weights unchanged. Recently, the Moment  model has achieved the SOTA results in head probing as compared to GPT4TS and Time-LLM. However, as indicated in Table 5, TTM further outperforms the results reported by Moment by 3-4%. In addition, TTM head probing results are very competitive as compared to the full end-to-end training of popular architectures as depicted in Appendix F.7. Hence, TTM, with its significantly reduced model size and the absence of compute-intensive components like self-attention, enables quick fine-tuning of models compared to the cumbersome process required by the massive Transformer models. Note that Moment is excluded from the comparison of zero/few-shot forecasting results because it does not report them.

### TTM's Effectiveness in Cross-channel and Exogenous Modeling:

Since the datasets (**D1**) used in previous experiments do not have exogenous variables, we evaluate the effectiveness of TTM on 4 other datasets (**D2**, as explained in Section 4.1) to quantify its benefits. Since these datasets are already very small, we used their full data for fine-tuning. Table 6 shows the performance of the pre-trained TTM\({}_{Q}\) model fine-tuned on the target data with exogenous mixer module and decoder channel-mixing enabled (TTM-CM). We compare TTM-CM with plain TTM finetuning and other primary SOTAs (PatchTST, TSMixer variants, and GPT4TS) trained from scratch. Specifically, we compare with TSMixer with channel-mixing enabled (TSMixer-CM) and TSMixer with cross-channel reconciliation head (TSMixer-CC)  as they are the latest SOTAs in channel-correlation modelling. From Table 6, we can see that TTM-CM outperforms all the competitive models with a significant margin (15-44%), thus, demonstrating the power of TTM in capturing inter-channel correlations.

### Ablation Studies:

The impacts of various techniques used in TTM are analyzed here.

**Pre-training data (Quality Vs Quantity):** Figure 3 demonstrates the vital role of both pretraining

   &  &  \\   & **TTM\({}_{}\)** & **TTM\({}_{}\)** & **GPT4TS** & **Time-LLM** &  &  &  &  &  &  \\  & (Ours) & (Ours) & (Ours) & (Nourts) & (Nourts) & (LCLR 23) & (KDD 23) & (KDD 24) & (ICLR 24) & (ICLR 23) & (AAAI 23) \\  ETH1 & **0.383** & 0.385 & 0.386 & 0.682 & 0.627 & 0.695 & 0.635 & 1.088 & 0.756 & 0.926 & 0.75 \\ ETH2 & 0.324 & 0.318 & **0.314** & 0.401 & 0.382 & 0.439 & 0.385 & 0.508 & 0.437 & 0.464 & 0.828 \\ ETH1 & 0.376 & 0.378 & **0.361** & 0.472 & 0.425 & 0.526 & 0.479 & 0.578 & 0.568 & 0.717 & 0.401 \\ ETH2 & 0.272 & 0.268 & **0.253** & 0.308 & 0.274 & 0.314 & 0.297 & 0.34 & 0.309 & 0.344 & 0.399 \\ Weather & 0.234 & 0.24 & **0.229** & 0.263 & 0.261 & 0.27 & 0.268 & 0.317 & 0.297 & 0.298 & 0.264 \\ Electricity & 0.183 & 0.207 & 0.18 & 0.178 & 0.177 & **0.176** & 0.197 & 0.239 & 0.202 & 0.402 & 0.177 \\ Traffic & 0.433 & 0.437 & 0.49 & 0.434 & 0.423 & **0.418** & 0.435 & 0.503 & 0.452 & 0.867 & 0.451 \\ 
**Size** & **1M** & **4M** & **5M** & **84M** & **7B** &  &  &  &  &  &  &  &  \\ 
**TTM\({}_{}\)**\(\{ and diverse resolution sampling (DRS). Initially, the zero-shot results were unsatisfactory when pre-training TTM with the smaller Monash dataset (i.e., PT(M)). To improve performance, we introduced the DRS technique on the Monash data to increase diversity and coverage (250M PT samples). This significantly improved the results by 37%. In addition, extending the dataset size from 250M to 1B further improved the results by 6%. These experiments highlight that while the quantity of pre-training data is significant, the quality of the data, especially in terms of resolution diversity and coverage, is even more crucial for improving the model performance.

**Effect of Resolution Prefix Tuning (RPT) and Adaptive Patching(AP):** RPT enhances forecast performance, especially with large and diverse pretraining (PT) data. Adding a learnable resolution prefix token allows models to easily decouple weights across different resolutions, leading to a 3% improvement in 1B PT data setup (Table 7). RPT is also beneficial for very short context length scenarios, improving performance by 8% (Appendix F.9). On the other hand, AP generally improves the forecasting performance across all set-ups, but the impact is consistently high in less PT data settings (3% boost).

**Forecast Length Adaptations (FLA):** Given a _FL_, we can either pre-train a Direct TTM tailored for the specific _FL_ or adapt existing TTMs trained on different _FLs_. Two possible adaptations are: (i) **Pruning:** Take TTM trained on _FL_' where _FL_' >_FL_, and prune it to the required _FL_ (e.g., TTM (_FL_ = \(720\)) pruned to other reduced _FL_\(\{96,192,336\}\)). (ii) **Recursive:** Take TTM trained on _FL_', where _FL_' <_FL_ and do recursive prediction (of length _FL_') till we reach the required _FL_ (e.g., Extend TTM (_FL_ = \(96\)) recursively to greater _FL_\(\{192,336,720\}\)). Figure 4 compares these techniques. For shorter adaptation (96 to 192), recursive predictions yield the best performance and match the direct forecast results. However, for wider adaptations (336-96 or 720-96), the pruning approach gives more stable and closer results to the direct forecasts. Hence, using these approaches, TTM models can be easily adapted to various _FLs_ based on user requirements.

### TTM Model Insights & Explainability:

Figure 5 illustrates the TTM embeddings from various datasets (weather, traffic, and electricity) using PCA. From each dataset, three distant, non-overlapping, fixed-length time segments (**S-1**, **S-2**, **S-3**) are selected, each depicted with a unique marker shape. The visualization uses the first and second principal components of the TTM embeddings. The inset image focuses on the weather dataset alone, revealing a deeper structure learned by the TTM architecture. The cyclic orbits in the embeddings reflect the seasonal patterns in the data. Both

Figure 4: **FL adaptation: impact of adapting TTMs (_FL_\(720\)) and TTMs (_FL_\(96\)) to all other FLs. MSE averaged across all D1 datasets is reported for _FL_\(\{96,192,336,720\}\). Best viewed in data used. Average MSE of zero-shot results across _FL_\(96\), 192 reported.**

Figure 5: (a) **TTM embedding projections across 3 datasets and 3 segments within datasets. (b) Cross-channel attention based explanation.**

   &  &  \\  &  &  \\  Data & w/o & AP/w & AP/w & RPT/w & RPT \\  ETH1 & 0.369 & **0.365** & 0.366 & **0.364** \\ ETH2 & **0.283** & 0.285 & 0.285 & **0.277** \\ ETH1 & 0.446 & **0.413** & 0.341 & **0.322** \\ ETTM2 & 0.191 & **0.187** & 0.18 & **0.171** \\ Weather & 0.159 & **0.154** & **0.153** & 0.158 \\ Electricity & 0.179 & **0.169** & 0.178 & **0.166** \\ Traffic & 0.521 & **0.518** & 0.528 & **0.514** \\ 
**TMIP**(\%) & **3\%** & & **3\%** \\  

Table 7: **Impact of AP and RPT: Impacts of adaptive patching (AP) in less pre-training (PT) data setting (i.e., TTM),a and resolution prefix tuning (RPT) in more pre-training (PT) data setting (i.e., TTM). Zero-shot results on FL 96 reported. [‘w’: with, ‘w/o’: ‘without’.]**hourly datasets (traffic and electricity) form concentric orbits due to similar seasonal patterns, while the weather data, with its distinct seasonal pattern, shows cyclic orbits in a different sub-dimension. In addition, the cross-channel attention from the fine-tuned model's channel mixing layers reveals feature importance across channels. As shown in Figure 5, the model focuses on channels like weather, season, holiday, and temperature to predict bike-rental counts. These attention model weights correlate with the general data characteristics where bike rental demands are heavily influenced by weather and holidays, providing explanation for the fine-tuned model predictions. More details are in Appendix G.

### Discussion on TTM Design choices:

In this section, we intuitively explain the important design choices of TTM that greatly enhance its forecasting accuracy and transfer learning capabilities despite its extremely small model capacity:

* All existing pre-trained models use a very high volume of pretraining data (for example, TimesFM used 300B and Moirai used 27B time-points), hence they naturally require massive model sizes. However, as shown in Figure.3, we observe that "limited" pretraining data with "high resolution diversity" greatly helps in time-series model generalization, as opposed to simply increasing the pretraining data size. This is an important observation and finding that resolution diversity in pretraining data is very crucial for time-series FMs. Based on these findings, we proceed with a well-reduced dataset (1B samples) with high resolution diversity which naturally reduces our model size compared to counterparts needing to pre-train with several hundred billion time-series. We introduce a high diversity in our data via Diverse Resolution Sampling technique (DRS) which our counterparts fail to do.
* Secondly, we opted for TSMixer-based models instead of transformer-based models, which further reduced the model size drastically. The TSMixer architecture has successfully established in the past that interleaving simple gated attentions with mixing components across patches, channels, and features greatly enhances forecasting accuracies with very limited model capacity, as the quadratic time-complexity of self-attentions can be entirely avoided. Following TSMixer, several other mixer architectures  have been published, re-iterating the power of these simple architectures. Thus, avoiding complex transformer architectures further reduced our model size significantly.
* In addition, we further increased the modeling power of TSMixer without drastically increasing its size by introducing several innovative components, such as adaptive patching, diverse resolution sampling, and resolution prefix tuning. These enhancements are crucial for effectively handling large pre-training across datasets with varying resolutions, all while keeping the model capacity very minimal.
* Finally, framing the pre-training objective as a direct forecasting task demonstrates improved zero-shot performance as compared to the traditional masking-based pre-training approaches. We hypothesize that this method enables the model to effectively learn complex non-linear mappings between the fixed context and forecast windows during pre-training that generalizes well to unseen datasets.

## 5 Conclusions and Future Work

We propose TTM, an extremely lightweight pre-trained model for multivariate time-series forecasting. Unlike existing large models, TTM is significantly smaller and faster, with efficient pre-training and fine-tuning workflows. Results show that TTM is highly effective in pre-training on heterogeneous datasets despite its limited model capacity. It achieves state-of-the-art results in zero/few-shot forecasting, offering significant computational efficiency while capturing cross-channel relationships and exogenous variables - critical features lacking in popular methods. Additionally, TTM supports both CPU and GPU deployments, greatly enhancing its adoption and ease of use. Moving forward, we plan to generalize our approach to support other downstream tasks beyond forecasting.