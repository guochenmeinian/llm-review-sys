# Segment Anything in 3D with NeRFs

Jiazhong Cen\({}^{1}\), &Zanwei Zhou\({}^{1}\), &Jiemin Fang\({}^{2,3}\), &Chen Yang\({}^{1}\), &Wei Shen\({}^{1}\), &Lingxi Xie\({}^{2}\), &Dongsheng Jiang\({}^{2}\), &Xiaopeng Zhang\({}^{2}\), &Qi Tian\({}^{2}\)

\({}^{1}\) MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University

\({}^{2}\) Huawei Inc. \({}^{3}\) School of EIC, Huazhong University of Science and Technology

wei.shen@sjtu.edu.cn

###### Abstract

Recently, the Segment Anything Model (SAM) emerged as a powerful vision foundation model which is capable to segment anything in 2D images. This paper aims to generalize SAM to segment 3D objects. Rather than replicating the data acquisition and annotation procedure which is costly in 3D, we design an efficient solution, leveraging the Neural Radiance Field (NeRF) as a cheap and off-the-shelf prior that connects multi-view 2D images to the 3D space. We refer to the proposed solution as **SA3D**, for Segment Anything in 3D. It is only required to provide a manual segmentation prompt (_e.g._, rough points) for the target object in a **single view**, which is used to generate its 2D mask in this view with SAM. Next, SA3D alternately performs **mask inverse rendering** and **cross-view self-prompting** across various views to iteratively complete the 3D mask of the target object constructed with voxel grids. The former projects the 2D mask obtained by SAM in the current view onto 3D mask with guidance of the density distribution learned by the NeRF; The latter extracts reliable prompts automatically as the input to SAM from the NeRF-rendered 2D mask in another view. We show in experiments that SA3D adapts to various scenes and achieves 3D segmentation within minutes. Our research reveals a potential methodology to lift the ability of a 2D vision foundation model to 3D, as long as the 2D model can steadily address promptable segmentation across multiple views. Our code is available at https://github.com/Jumpat/SegmentAnythingin3D.

## 1 Introduction

The computer vision community has been pursuing a vision foundation model that can perform basic tasks (_e.g._, segmentation) in any scenario and for either 2D or 3D image data. Recently, the Segment Anything Model (SAM)  emerged and attracted a lot of attention, due to its ability to segment anything in 2D images, but generalizing the ability of SAM to 3D scenes remains mostly uncovered. One may choose to replicate the pipeline of SAM to collect and semi-automatically annotate a large set of 3D scenes, but the costly burden seems unaffordable for most research groups.

We realize that an alternative and efficient solution lies in equipping the 2D foundation model (_i.e._, SAM) with 3D perception via a 3D representation model. In other words, there is no need to establish a 3D foundation model from scratch. However, there is a prerequisite: the 3D representation model shall be capable to render 2D views and register 2D segmentation results to the 3D scene. Thus, we use the Neural Radiance Fields (NeRF)  as an off-the-shelf solution. NeRF is a family of algorithms that formulates each 3D scene into a deep neural network that serves as a 3D prior connecting multiple 2D views.

As shown in Figure 1, our solution is named Segment Anything in 3D (**SA3D**). Given a NeRF trained on a set of 2D images, SA3D takes prompts (_e.g._, click points on the object) in a single rendered view as input, which is used to generate a 2D mask in this view with SAM. Next, SA3D alternately performs two steps across various views to iteratively complete the 3D mask of the object constructed with voxel grids. In each round, the first step is **mask inverse rendering**, in which the previous 2D segmentation mask obtained by SAM is projected onto the 3D mask via density-guided inverse rendering offered by the NeRF. The second step is **cross-view self-prompting**, in which NeRF is used to render the 2D segmentation mask (which may be inaccurate) based on the 3D mask and the image from another view, then a few point prompts are automatically generated from the rendered mask and fed into SAM to produce a more complete and accurate 2D mask. The above procedure is executed iteratively until all necessary views have been sampled.

We conduct various (_e.g._, object, part-level) segmentation tasks on the Replica  and NVOS  datasets. Without re-training/re-designing SAM or NeRF, SA3D easily and efficiently adapts to different scenarios. Compared to existing approaches, SA3D enjoys a simplified pipeline that typically completes 3D segmentation within minutes. SA3D not only offers an efficient tool for segmenting anything in 3D, but also reveals a generic methodology to lift 2D foundation models to the 3D space. The only prerequisite lies in the ability to steadily address promptable segmentation across multiple views, and we hope it becomes a general property of 2D foundation models in the future.

## 2 Related Work

2D SegmentationSince FCN  has been proposed, research on 2D image segmentation has experienced a rapid growth. Various sub-fields of segmentation have been explored deeply by numerous studies [18; 24; 4; 71]. With transformers [58; 10] entering the field of segmentation, many new segmentation architectures [72; 7; 6; 52; 63] have been proposed and the whole field of segmentation has been further developed. A recent significant breakthrough in this field is the Segment Anything Model (SAM) . As an emerging vision foundation model, SAM is recognized as a potential game-changer, which aims to unify the 2D segmentation task by introducing a prompt-based segmentation paradigm. An analogous model to SAM is SEEM , which also exhibits impressive open-vocabulary segmentation capabilities.

3D SegmentationNumerous methods have explored various types of 3D representations to perform 3D segmentation. These scene representations include RGB-D images [60; 62; 64; 8], point clouds [44; 45; 70] and grid space such as voxels [21; 55; 35], cylinders  and bird's eye view space [67; 16]. Although 3D segmentation has been developed for a period of time, compared with 2D segmentation, the scarcity of labeled data and high computational complexity make it difficult to design a unified framework similar to SAM.

Lifting 2D Vision Foundation Models to 3DTo tackle the limitation of data scarcity, many previous studies [23; 43; 9; 17; 69; 31; 65; 22] explored lifting 2D foundation models to 3D. In these studies, the most relevant work to SA3D is LERF , which trains a feature field of the Vision-Language Model (_i.e._, CLIP ) together with the radiance field. Compared with SA3D,

Figure 1: Given any pre-trained NeRF, SA3D takes prompts from one single rendered view as input and outputs the 3D segmentation result for the specific target.

LERF focuses on coarsely localizing the specific objects with text prompts but not fine-grained 3D segmentation. The reliance on CLIP features makes it insensitive to the specific location information of the target object. When there are multiple objects with similar semantics in the scene, LERF cannot perform effective 3D segmentation. The remaining methods mainly focus on point clouds. By connecting the 3D point cloud with specific camera poses with 2D multi-view images, the extracted features by 2D foundation models can be projected to the 3D point cloud. The data acquisition of these methods is more expensive than ours, _i.e._, acquiring multi-view images for NeRFs.

Segmentation in NeRFsNeural Radiance Fields (NeRFs) [38; 53; 3; 1; 40; 19; 13; 61; 30; 12] are a series of 3D implicit representation. Inspired by their success in 3D consistent novel view synthesis, numerous studies have delved into the realm of 3D segmentation within NeRFs. Zhi _et al_.  proposes Semantic-NeRF, a method that incorporates semantics into appearance and geometry. They showcase the potential of NeRFs in label propagation and refinement. NVOS  introduces an interactive approach to select 3D objects from NeRFs by training a lightweight multi-layer perception (MLP) using custom-designed 3D features. Other approaches, _e.g._ N3F , DFF , LERF  and ISRF , aim to lift 2D visual features to 3D through training additional feature fields. These methods are required to re-design/-train NeRF models and usually involve additional feature-matching processes. There are also some other instance segmentation and semantic segmentation approaches [50; 41; 11; 68; 34; 20; 2; 14; 59; 28] combined with NeRFs.

The most closely related approach to our SA3D is MVSeg , a component of SPIn-NeRF , which focuses on NeRF inpainting. MVSeg adopts video segmentation techniques to propagate a 2D mask across different views and employs these masks as labels for training a Semantic-NeRF model. However, video segmentation models lack explicit 3D structure information, which are hard to handle significant occlusions in complex scenes. Our method aims at building NeRF-driven consistency across views based on self-prompting and lifting 2D masks to robust 3D masks.

## 3 Method

In this section, we first give a brief review of Neural Radiance Fields (NeRFs) and the Segment Anything Model (SAM). Then we introduce the overall pipeline of SA3D. Finally, we demonstrate the design of each component in SA3D in detail.

### Preliminaries

Neural Radiance Fields (NeRFs)Given a training dataset \(\) of multi-view 2D images, NeRFs  learn a function \(f_{}:(,)(,)\), which maps the spatial coordinates \(^{3}\) and the view direction \(^{2}\) of a point into the corresponding color \(^{3}\) and volume density \(\). \(\) denotes the learnable parameters of the function \(f\) which is usually represented by a multi-layer perceptron (MLP). To render an image \(_{}\), each pixel undergoes a ray casting process where a ray \((t)=_{o}+t\) is projected through the camera pose. Here, \(_{o}\) is the camera origin, \(\) is the ray direction, and \(t\) denotes the distance of a point along the ray from the origin. The RGB color \(_{}()\) at the location determined by ray \(\) is obtained via a differentiable volume rendering algorithm:

\[_{}()=_{t_{n}}^{t_{f}}((t) )((t),)t,\] (1)

where \(((t))=(-_{t_{n}}^{t}((s))s) ((t))\), and \(t_{n}\) and \(t_{f}\) denote the near and far bounds of the ray, respectively.

Segment Anything Model (SAM)SAM  takes an image \(\) and a set of prompts \(\) as input, and outputs the corresponding 2D segmentation mask \(_{}\) in the form of a bitmap, _i.e.,_

\[_{}=s(,).\] (2)

The prompts \(\) can be points, boxes, texts, and masks.

### Overall Pipeline

We assume that we already have a NeRF model trained on the dataset \(\). Throughout this paper, unless otherwise specified, we opt to employ the TensoRF  as the NeRF model, considering its superior efficiency of training and rendering. As shown in Figure 2, an image \(^{}\) from a specific view is first rendered with the pre-trained NeRF model. A set of prompts (_e.g._, in this paper, we often use a set of points), \(^{}\), is introduced and fed into SAM along with the rendered image. The 2D segmentation mask \(^{}_{}\) of the according view is obtained, which is then projected onto the 3D mask \(^{3}\) constructed voxel grids with the proposed **mask inverse rendering** technique (Section 3.3). Then a 2D segmentation mask \(^{(n)}\) from a novel view is rendered from the 3D mask. The rendered mask is usually inaccurate. We propose a **cross-view self-prompting** method (Section 3.4) to extract point prompts \(^{(n)}\) from the rendered mask and further feed them into SAM. Thus a more accurate 2D mask \(^{(n)}_{}\) in this novel view is produced and also projected onto the voxel grids to complete the 3D mask. The above procedure is executed iteratively with more views traversed. Meanwhile, the 3D mask become more and more complete. The whole process bridges 2D segmentation results with 3D ones efficiently. Noting that no neural network needs to be optimized except the 3D mask.

### Mask Inverse Rendering

As shown in Equation (1), the color of each pixel in a rendered image is determined by a sum of weighted colors along the corresponding ray. The weight \(((t))\) reveals the object structure within the 3D space, where a high weight indicates the corresponding point close to the object's surface. Mask inverse rendering aims to project the 2D mask to the 3D space to form the 3D mask based on these weights.

Formally, the 3D mask is represented as voxel grids \(^{L W H}\), where each grid vertex stores a zero-initialized soft mask confidence score. Based on these voxels grids, each pixel of the 2D mask from one view is rendered as

\[()=_{t_{n}}^{t_{f}}((t))( (t)),\] (3)

where \((t)\) is the ray casting through the mask pixel, \(((t))\) is inherited from density values of the pre-trained NeRF, and \(((t))\) denotes the mask confidence score at the location \((t)\) obtained from voxel grids \(\)1. Denote \(_{}()\) as the corresponding mask generated by SAM. When \(_{}()=1\)

Figure 2: The overall pipeline of SA3D. Given a NeRF trained on a set of multi-view 2D images, SA3D first takes prompts in a single view for the target object as input and uses SAM to produce a 2D mask in this view with these prompts. Then, SA3D performs an alternated process of **mask inverse rendering** and **cross-view self-prompting** to complete the 3D mask of the target object constructed with voxel grids. Mask inverse rendering is performed to project the 2D mask obtained by SAM onto the 3D mask according to the learned density distribution embedded in the NeRF. Cross-view self-prompting is conducted to extract reliable prompts automatically as the input to SAM from the NeRF-rendered 2D mask given a novel view. This alternated process is executed iteratively until we get the complete 3D mask.

the goal of mask inverse rendering is to increase \(((t))\) with respect to \(((t))\). In practice, this can be optimized using the gradient descent algorithm. For this purpose, we define the mask projection loss as the negative product between \(_{}()\) and \(()\):

\[_{}=-_{()} _{}()(),\] (4)

where \(()\) denotes the ray set of the image \(\).

The mask projection loss is constructed based on the assumption that both the geometry from the NeRF and the segmentation results of SAM are accurate. However, in practice, this is not always the case. We append a negative refinement term to the loss to optimize the 3D mask grids according to multi-view mask consistency:

\[_{}=-_{()} _{}()()+ _{()}(1-_{}())(),\] (5)

where \(\) is a hyper-parameter to determine the magnitude of the negative term. With this negative refinement term, only if SAM consistently predicts a region as foreground from different views, SA3D marks its corresponding 3D region as foreground. In each iteration, the 3D mask \(\) is updated via \(-_{}} {}\) with gradient descent, where \(\) denotes the learning rate.

### Cross-view Self-prompting

Mask inverse rendering enables projecting 2D masks into the 3D space to form the 3D mask of a target object. To construct accurate 3D mask, substantial 2D masks from various views need to be projected. SAM can provide high-quality segmentation results given proper prompts. However, manually selecting prompts from every view is time-consuming and impractical. We propose a cross-view self-prompting mechanism to produce prompts for different novel views automatically.

Specifically, we first render a novel-view 2D segmentation mask \(^{(n)}\) from the 3D mask grids \(\) according to Equation (3). This mask is usually inaccurate, especially at the preliminary iteration of SA3D. Then we obtain some point prompts from the rendered mask with a specific strategy. The above process is named cross-view self-prompting. While there are multiple possible solutions for this strategy, we present a feasible one that has been demonstrated to be effective.

Self-prompting StrategyGiven an inaccurate 2D rendered mask \(^{(n)}\), the self-prompting strategy aims to extract a set of prompt points \(_{s}\) from it, which can help SAM to generate 2D segmentation result as accurate as possible. It is important to note that \(^{(n)}\) is not a typical 2D bitmap, but rather a confidence score map computed using Equation (3). Since each image pixel \(\) corresponds to a ray \(\) in a rendered view, we use \(\) for an easier demonstration of the prompt selection strategy on images.

As \(_{s}\) is initialized to an empty set, the first prompt point \(_{0}\) is selected as the point with the highest mask confidence score: \(_{0}=_{}^{(n)}()\). To select new prompt points, we first mask out square shaped regions2 on \(^{(n)}\) centered with each existing point prompt \(}_{s}\). Considering the depth \(z()\) can be estimated by the pre-trained NeRF, we transform the 2D pixel \(\) to the 3D point \(()=(x(()),y(() ),z(()))\):

\[x(())\\ y(())\\ z(())=z()^{-1} x()\\ y()\\ 1\] (6)

where \(x(),y()\) denote the 2D coordinates of \(\), and \(\) denotes the camera intrinsics. The new prompt point is expected to have a high confidence score while being close to existing prompt points. Considering the two factors, we introduce a decay term to the confidence score. Let \(d(,)\) denote the min-max normalized Euclidean distance. For each remaining point \(\) in \(^{(n)}\), the decay term is

\[^{(n)}()=\{^{(n)}(})  d((),(}))}_{s}\}.\] (7)

Then a decayed mask confidence score \(}^{(n)}()\) is computed as

\[}^{(n)}()=^{(n)}()- ^{(n)}().\] (8)The remaining point with the highest decayed score, _i.e._, \(^{*}=_{_{}}}^{(n)}()\), is added to the prompt set: \(_{s}=_{s}\{^{*}\}\). The above selection process is repeated until either the number of prompts \(|_{s}|\) reaches a predefined threshold \(n_{p}\) or the maximum value of \(}^{(n)}()\) is smaller than 0.

IoU-aware View RejectionWhen the target object is rendered in heavily occluded views, SAM may produce incorrect segmentation results and degrades the quality of the 3D mask. To avoid such situations, we introduce an additional view rejection mechanism based on the intersection-over-union (IoU) between the rendered mask \(^{(n)}\) and the SAM prediction \(^{(n)}_{}\). If the IoU falls below a predefined threshold \(\), it indicates a poor overlap between the two masks. The prediction from SAM is rejected, and the mask inverse rendering step is skipped in this iteration.

## 4 Experiments

In this section, we quantitatively evaluate the segmentation ability of SA3D on various datasets. Then, we qualitatively demonstrate the versatility of SA3D, which can conduct instance segmentation, part segmentation, and text-prompted segmentation _etc_.

### Datasets

For quantitative experiments, we use the Neural Volumetric Object Selection (NVOS) , SPInNeRF , and Replica  datasets. The NVOS  dataset is based on the LLFF dataset , which includes several forward-facing scenes. For each scene, NVOS provides a reference view

Figure 3: Some visualization results in different scenes (LERF-donuts , LERF-figurines, Replica-room0  and 360-kitchen ).

with scribbles and a target view with 2D segmentation masks annotated. Similar to NVOS, SPInNeRF  annotates some data manually to evaluate interactive 3D segmentation performance. These annotations are based on some widely-used NeRF datasets [37; 38; 29; 26; 13]. The Replica  dataset provides high-quality reconstruction ground truths of various indoor scenes, including clean dense geometry, high-resolution and high-dynamic-range textures, glass and mirror surface information, semantic classes, planar segmentation, and instance segmentation masks. For qualitative analysis, we use the LLFF  dataset and the 360\({}^{}\) dataset . SA3D is further applied to the LERF  dataset, which contains more realistic and challenging scenes.

### Quantitative Results

NVOS DatasetFor fair comparisons, we follow the experimental setting of the original NVOS . We first scribble on the reference view (provided by the NVOS dataset) to conduct 3D segmentation, and then render the 3D segmentation result on the target view and evaluate the IoU and pixel-wise accuracy with the provided ground truth. Note that the scribbles are preprocessed to meet the requirements of SAM. More details can be found in the supplement. As shown in Table 1, SA3D outperforms previous approaches by large margins, _i.e._, +6.5 mIoU over the NVOS.

SPIn-NeRF DatasetWe follow SPIn-NeRF  to conduct label propagation for evaluation. Given a specific reference view of a target object, the 2D ground-truth mask of this view is available. The prompt input operation is omitted, while the 2D ground-truth mask of the target object from the reference view is directly used for the initialization of the 3D mask grids. This is reasonable since in most situations users can refine their input prompts to help SAM generate a 2D mask as accurately as possible from the reference view. Once the 3D mask grids are initialized, the subsequent steps are exactly the same as described in Section 3. With the 3D mask grids finalized, the 2D masks in other views are rendered to calculate the IoU with the 2D ground-truth masks. Results can be found in Table 2. SA3D is demonstrated to be superior in both forward-facing and 360\({}^{}\) scenes.

In Tables 2 and 3, "Single view" refers to conducting mask inverse rendering exclusively for the 2D ground-truth mask of the reference view. This process is equivalent to mapping the 2D mask to the 3D space based on the corresponding depth information, without any subsequent learnable/updating step. We present these results to demonstrate the gain of the alternated process in our framework. As shown in Table 2, SA3D outperforms MVSeg  in most scenes, especially +5.6 mIoU on Truck and +17.3 mIoU on Lego. Besides, compared with the "Single view" model, a significant promotion is achieved, _i.e._ +17.8 mIoU, which further proves the effectiveness of our method.

   Method & mIoU (\%) & mAcc (\%) \\  Graph-cut (3D) [48; 47] & 39.4 & 73.6 \\ NVOS  & 70.1 & 92.0 \\ ISRF  & 83.8 & 96.4 \\ SA3D (ours) & **90.3** & **98.2** \\   

Table 1: Quantitative results on NVOS.

    &  &  &  \\   & IoU (\%) & Acc (\%) & IoU (\%) & Acc (\%) & IoU (\%) & Acc (\%) \\  Orchids & 79.4 & 96.0 & **92.7** & 98.8 & 83.6 & 96.9 \\ Leaves & 78.7 & 98.6 & 94.9 & 99.7 & **97.2** & 99.9 \\ Fern & 95.2 & 99.3 & 94.3 & 99.2 & **97.1** & 99.6 \\ Room & 73.4 & 96.5 & **95.6** & 99.4 & 88.2 & 98.3 \\ Horns & 85.3 & 97.1 & 92.8 & 98.7 & **94.5** & 99.0 \\ Fortress & 94.1 & 99.1 & 97.7 & 99.7 & **98.3** & 99.8 \\  Fork & 69.4 & 98.5 & 87.9 & 99.5 & **89.4** & 99.6 \\ Pinecone & 57.0 & 92.5 & **93.4** & 99.2 & 92.9 & 99.1 \\ Truck & 37.9 & 77.9 & 85.2 & 95.1 & **90.8** & 96.7 \\ Lego & 76.0 & 99.1 & 74.9 & 99.2 & **92.2** & 99.8 \\  mean & 74.6 & 95.5 & 90.9 & 98.9 & **92.4** & 98.9 \\   

Table 2: Quantitative results on the SPIn-NeRF dataset.

Replica DatasetWe use the processed Replica data with 2D instance labels provided by Zhi _et al._ to evaluate the segmentation performance of SA3D. We retrieve all views containing each object and specify one reference view. With a similar setting of experiments on the SPIn-NeRF dataset, we use the ground-truth mask of the reference view and perform SA3D to conduct label propagation for evaluation. For each scene in Replica, around 20 objects are chosen for evaluation. Refer to the supplement for more details. As shown in Table 3, the mean IoU (mIoU) is reported for all available objects in different scenes. We exclude the pixel-wise accuracy metric since an object only appears in a few views in the indoor scenes of Replica, where the pixel-wise accuracy is too high to serve as a reliable metric.

In complex indoor scenes of Replica, MVSeg's strategy based on video segmentation proves to be ineffective, which generates numerous inaccurate 2D pseudo-labels, even using the Semantic-NeRF  for refinement. Consequently, the final 3D segmentation results of MVSeg even underperform those achieved by the "Single view" method. In contrast, SA3D accurately captures segmented objects in complex 3D scenes. Visualization results are shown in Figure 3.

### Qualitative Results

We conduct three kinds of segmentation tasks: object segmentation, part segmentation and text-prompting segmentation. The first two are the core functions of SA3D. As shown in Figure 3, SA3D demonstrates its capability to segment diverse 3D objects across different scenes, even when the objects are of small scales. Besides, SA3D can also handle challenging part segmentation. The last row of the figure showcases SA3D's precise segmentation of the bucket, small wheel, and dome light of the Lego bulldozer. Figure 4 demonstrates the potential of SA3D in combining with language models. Given a text phrase, the corresponding object can be accurately cut out. The text-prompting segmentation is built upon Grounding-DINO , a model capable of generating bounding boxes for objects based on text prompts. These bounding boxes serve as input prompts for SA3D in the segmentation process.

   Number of Views & 5 (10\%) & 9 (20\%) & 21 (50\%) & 43 (100\%) \\  IoU on Fortress (forward facing) & 97.8 & 98.3 & 98.3 & 98.3 \\ Time Cost (s) & 7.6 & 12.8 & 29.0 & 59.0 \\   Number of Views & 11 (10\%) & 21 (20\%) & 51 (50\%) & 103 (100\%) \\  IoU on Lego (360\({}^{}\)) & 84.5 & 84.8 & 91.5 & 92.2 \\ Time Cost (s) & 23.5 & 43.5 & 103.8 & 204.9 \\   

Table 4: Ablation on different numbers of views for 3D mask generation. Numbers in parentheses represent the view percentage of total training views.

Figure 4: 3D segmentation results of SA3D with the text prompts in 360-garden .

   Scenes & office0 & office1 & office2 & office3 & office4 & room0 & room1 & room2 & mean \\  Single view & 68.7 & 56.5 & 68.4 & 62.2 & 57.0 & 55.4 & 53.8 & 56.7 & 59.8 \\ MVSeg  & 31.4 & 40.4 & 30.4 & 30.5 & 25.4 & 31.1 & 40.7 & 29.2 & 32.4 \\ SA3D (ours) & **84.4** & **77.0** & **88.9** & **84.4** & **82.6** & **77.6** & **79.8** & **89.2** & **83.0** \\   

Table 3: Quantitative results on Replica (mIoU).

### Ablation Study

Number of ViewsThe process of mask inverse rendering and cross-view self-prompting is alternated across different views. By default, we utilize all available views in the training set \(\). However, to expedite the 3D segmentation procedure, the number of views can be reduced. As shown in Table 4, We perform experiments on two representative scenes from the SPIn-NeRF  dataset to demonstrate this characteristic. The views are uniformly sampled from the sorted training set. In forward facing scenes where the range of the camera poses is limited, satisfactory results can be achieved by selecting only a few views. On an Nvidia RTX 3090 GPU, the 3D segmentation process with 5 views can be completed within 10 seconds. On the contrary, in scenes where the range of camera poses is wider, a larger number of views are required to yield greater improvements. Note that even with 50 views, the segmentation task can still be completed in less than 2 minutes.

Hyper-parametersSA3D involves three hyper-parameters: the IoU rejection threshold \(\), the loss balance coefficient \(\) in Equation (5), and the number of self-prompting points \(n_{p}\). As shown in Table 6, too small \(\) values lead to unstable SAM predictions, introducing noises to the 3D mask; too large \(\) values impede the 3D mask from getting substantial information. Table 7 indicates slightly introducing a negative term with the \(\) factor can reduce noise for mask projection. However, a too-large negative term may make the mask completion process unstable and causes degraded performance. The selection of \(n_{p}\) depends on the specific segmentation target, as SAM tends to produce over-segmented results that capture finer details of objects. As shown in Figure 5, for objects with a relatively large scale and complex structures, a bigger \(n_{p}\) produces better results. Empirically. setting \(n_{p}\) to 3 can meet the requirements of most situations.

Self-prompting StrategyWithout the 3D distance based confidence decay (Equation (7)), our self-prompting strategy degrades to a simple 2D NMS (Non-Maximum Suppression), which selects a prompt point with the highest confidence score and then masks out a region around it. To showcase the efficacy of our design, we conduct experiments using the NVOS benchmark and presented per-scene results for in-depth analysis.

Table 5 shows that a simple NMS self-prompting is enough for most cases. But for hard cases like 'LLFF-trex' (a trex skeleton, as shown in Figure 5), where a large number of depth jumps, the confidence decay term contributes a lot. In such a situation, inaccurate masks bleed through gaps in the foreground onto the background. If the self-prompting mechanism generates prompts on these inaccurate regions, SAM may produce plausible segmentation results that can cheat the IoU-rejection mechanism and finally the segmentation results will involve unwanted background regions.

2D Segmentation ModelsIn addition to SAM, we also incorporate four other prompt-based 2D segmentation models [75; 49; 32; 5] into our framework to demonstrate the generalization ability of SA3D. The evaluation results on the NVOS dataset is shown in Table 8.

## 5 Discussion

On top of the experimental results, we hope to deliver some insights from our preliminary study of integrating SAM and NeRF, _i.e._, a 2D foundation model and a 3D representation model.

    & w/ Confidence & Decay Term & w/o Confidence Decay Term \\   & IoU (\%) & Acc (\%) & IoU (\%) & Acc (\%) \\  Fern & 82.9 & 94.4 & 82.9 & 94.4 \\ Flower & 94.6 & 98.7 & 94.6 & 99.7 \\ Fortress & 98.3 & 99.7 & 98.4 & 99.7 \\ Horns (center) & 96.2 & 99.3 & 96.2 & 99.3 \\ Horns (Left) & 90.2 & 99.4 & 88.8 & 99.3 \\ Leaves & 93.2 & 99.6 & 93.2 & 99.6 \\ Orchids & 85.5 & 97.3 & 85.4 & 97.3 \\ Trex & 82.0 & 97.4 & 64.0 & 93.3 \\  mean & 90.3 & 98.2 & 87.9 & 97.7 \\   

Table 5: Ablation on the confidence decay term of the self-prompting strategy.

[MISSING_PAGE_FAIL:10]