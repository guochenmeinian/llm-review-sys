# Fine-grained Analysis of In-context Linear Estimation:

Data, Architecture, and Beyond

 Yingcong Li

University of Michigan

yingcong@umich.edu

&Ankit Singh Rawat

Google Research NYC

ankitsrawat@google.com

&Samet Oymak

University of Michigan

oymak@umich.edu

###### Abstract

Recent research has shown that Transformers with linear attention are capable of in-context learning (ICL) by implementing a linear estimator through gradient descent steps. However, the existing results on the optimization landscape apply under stylized settings where task and feature vectors are assumed to be IID and the attention weights are fully parameterized. In this work, we develop a stronger characterization of the optimization and generalization landscape of ICL through contributions on architectures, low-rank parameterization, and correlated designs: (1) We study the landscape of 1-layer linear attention and 1-layer H3, a state-space model. Under a suitable correlated design assumption, we prove that both implement 1-step preconditioned gradient descent. We show that thanks to its native convolution filters, H3 also has the advantage of implementing sample weighting and outperforming linear attention in suitable settings. (2) By studying correlated designs, we provide new risk bounds for retrieval augmented generation (RAG) and task-feature alignment which reveal how ICL sample complexity benefits from distributional alignment. (3) We derive the optimal risk for low-rank parameterized attention weights in terms of covariance spectrum. Through this, we also shed light on how LoRA can adapt to a new distribution by capturing the shift between task covariances. Experimental results corroborate our theoretical findings. Overall, this work explores the optimization and risk landscape of ICL in practically meaningful settings and contributes to a more thorough understanding of its mechanics.

Figure 1: We investigate the optimization landscape of in-context learning from the lens of architecture choice, the role of distributional alignment, and low-rank parameterization. The empirical performance (solid curves) are aligned with our theoretical results (dotted curves) from Section 3. More experimental details and discussion are deferred to Section 4.

Introduction

Modern language models exhibit the remarkable ability to learn novel tasks or solve complex problems from the demonstrations provided within their context window (Brown et al., 2020; GeminiTeam et al., 2023; OpenAI, 2023; Touvron et al., 2023). Such _in-context learning_ (ICL) offers a novel and effective alternative to traditional fine-tuning techniques and has become an important feature of LLM with its applications spanning retrieval-augmented generation (Lewis et al., 2020), and reasoning via advanced prompting techniques, such as chain-of-thought (Wei et al., 2022).

ICL ability presents an important research avenue to develop stronger theoretical and mechanistic understanding of large language models. To this aim, there has been significant recent interest in demystifying ICL through the lens of function approximation (Liu et al., 2023a), Bayesian inference (Muller et al., 2021; Xie et al., 2022; Han et al., 2023), and learning and optimization theory (Ahn et al., 2023; Mahankali et al., 2024; Zhang et al., 2024; Duraisamy, 2024). The latter is concerned with understanding the optimization landscape of ICL, which is also crucial for understanding the generalization properties of the model. A notable result in this direction is the observation that linear attention models (Schlag et al., 2021; Von Oswald et al., 2023; Ahn et al., 2023) implement _preconditioned gradient descent_ (PGD) during ICL (Ahn et al., 2023; Mahdavi et al., 2024). While this line of works provide a fresh perspective to ICL, the existing studies do not address many questions arising from real-life applications nor provide guiding principles for various ICL setups motivated by practical considerations.

To this aim, we revisit the theoretical exploration of ICL with linear data model where we feed an in-context prompt containing \(n\) examples \((_{i},y_{i}=_{i}^{T}+_{i})_{i=1}^{n} ^{d}\) and a test instance or query \(_{n+1}^{d}\) to the model, with \(d\) being the feature dimension, \(^{d}\) being the task weight vector, and \((_{i})_{i=1}^{n}\) denoting the noise in individual labels. Given the in-context prompt, the model is tasked to predict \(_{n+1}\) - an estimate for \(y_{n+1}=_{n+1}^{}+_{n+1}\). We aim to provide answers to the following questions by exploring the loss landscape of ICL:

1. Is the ability to implement gradient-based ICL unique to (linear) attention? Can alternative sequence models implement richer algorithms beyond PGD?
2. In language modeling, ICL often works well with few-shot samples whereas standard linear estimation typically requires \((d)\) samples. How can we reconcile this discrepancy between classical learning and ICL?
3. To our knowledge, existing works assume linear-attention is fully parameterized, i.e., key and query projections \(_{k},_{q}^{d d}\). What happens when they are low-rank? What happens when there is distribution shift between training and test in-context prompts and we use LoRA (Hu et al., 2022) for adaptation?

In this work, we conduct a careful investigation of these questions. Specifically, we focus on ICL with 1-layer models and make the following contributions:

1. We jointly investigate the landscape of linear attention and H3 (Fu et al., 2023), a widely popular state-space model (SSM). We prove that under correlated design, both models implement 1-step PGD (c.f. Proposition 1) and the alignments in Fig. 0(a) verify that where the dotted curve represents the theoretical PGD result derived from Theorem 1. Our analysis reveals that the gating mechanism in H3 imitates attention. We also empirically show that H3 has the advantage of implementing sample-weighting which allows it to outperform linear attention in temporally-heterogeneous problem settings in Appendix D.
2. Proposition 1 allows for task and features to be correlated to each other as long as odd moments are zero. Through this, we can assess the impact of distributional alignment on the sample complexity of ICL. Specifically, we characterize the performance of _Retrieval Augmented Generation_ (RAG) (c.f. Theorem 2 and Fig. 0(b)) and _Task-Feature Alignment_ (c.f. Theorem 3), where the in-context examples are \(\)-correlated with either the query or the task vector. For both settings, we prove that alignment amplifies the _effective sample size_ of ICL by a factor of \(^{2}d+1\), highlighting that aligned data are crucial for the success of ICL in few-shot settings.
3. We show that, under low-rank parameterization, optimal attention-weights still implements PGD according to the truncated eigenspectrum of the fused task-feature covariance (see Section 3.2). We similarly derive risk upper bounds for LoRA adaptation (c.f. Eq. (14) and Fig. 0(c)), and show that, these bounds accurately predict the empirical performance.

Problem Setup and Preliminaries

We begin with a short note on notation. Let bold lowercase and uppercase letters (e.g., \(\) and \(\)) represent vectors and matrices, respectively. The symbol \(\) is defined as the element-wise (Hadamard) product, and \(*\) denotes the convolution operator. \(_{d}\) and \(_{d}\) denote the \(d\)-dimensional all-ones and all-zeros vectors, respectively; and \(_{d}\) denotes the identity matrix of dimension \(d d\). Additionally, let \(}()\) denote the trace of the square matrix \(\).

As mentioned earlier, we study the optimization landscapes of 1-layer linear attention (Katharopoulos et al., 2020; Schlag et al., 2021) and H3 (Fu et al., 2023) models when training with prompts containing in-context data following a linear model. We construct the input in-context prompt similar to Ahn et al. (2023); Mahankali et al. (2024); Zhang et al. (2024) as follows.

**Linear data distribution.** Let \((,y)^{d}\) be a (feature, label) pair generated by a \(d\)-dimensional linear model parameterized by \(^{d}\), i.e., \(y=^{}+\), where \(\) and \(\) are feature and task vectors, and \(\) is the label noise. Given demonstrations \((_{i},y_{i})_{i=1}^{+1}\) sampled from a single \(\), define the input in-context prompt

\[=_{1}&&_{n}&_{n+1}^{ }=_{1}&&_{n}&_{n+1}\\ y_{1}&&y_{n}&0^{}^{(n+1)(d+1)}.\] (1)

Here, we set \(_{i}=_{i}\\ y_{i}\) for \(i n\) and the last/query token \(_{n+1}=_{n+1}\\ 0\). Then, given \(\), the goal of the model is to predict the correct label \(y_{n+1}\) corresponding to \(_{n+1}\). For cleaner notation, when it is clear from context, we drop the subscript \(n+1\) and set \(=_{n+1},~{}=_{n+1}\). Different from the previous work (Ahn et al., 2023; Mahankali et al., 2024; Zhang et al., 2024; Mahdavi et al., 2024) where \((_{i})_{i=1}^{n+1}\) and \(\) are assumed to be independent, our analysis focuses on a more general linear setting that captures the dependency between \((_{i})_{i=1}^{n+1}\) and \(\).

**Model architectures.** To start with, we first review the architectures of both Transformer and state-space model (SSM). Similar to the previous work (Von Oswald et al., 2023; Ahn et al., 2023; Mahankali et al., 2024; Zhang et al., 2024) and to simplify the model structure, we focus on single-layer models and omit the nonlinearity, e.g., softmax operation and MLP activation, from the Transformer. Given the input prompt \(^{(n+1)(d+1)}\) in (1), which can be treated as a sequence of \((d+1)\)-dimensional tokens, the single-layer linear attention ATT and H3-like single-layer SSM SSM are denoted by

\[() =(_{q}_{k}^{}^{})_{}\] (2a) \[() =(_{q})(_{k}_{})*\] (2b)

where \(_{k},~{}_{q},~{}_{}^{(d+1)(d+1)}\) denote the key, query and value weight matrices, respectively. In (2b), the parameter \(^{n+1}\) is a 1-D convolutional filter that mixes tokens. The Hadamard product \(\) is the gating mechanism (Dauphin et al., 2017) between key and query channels, which is crucial for attention-like feature creation. Thus, (2b) is more generally a gated-convolution layer. For \(\) only, we use indexing \(=[f_{0}~{}~{}f_{n}]^{}^{n+1}\) and given any vector \(\), denote convolution output \((*)_{i}=_{j=1}^{i}f_{i-j}a_{j}\). Note that our notation slightly differs from the original H3 model (Fu et al., 2023) in two ways:

1. SSMs provide efficient parameterization of \(\) which would otherwise grow with sequence length. In essence, H3 utilizes a linear state-space model \(_{i}=_{i-1}+_{i}\) and \(y_{i}=_{i}\) with parameters (\(^{d d}\), \(^{d 1},^{1 d}\)) from which the filter \(\) is obtained via the impulse response \(f_{i}=^{i}\) for \(i 0\). Here \(d\) is the state dimension and, in practice, \(\) is chosen to be diagonal. Observe that, setting \(d=1\) and \(=,==1\), SSM reduces to the exponential smoothing \(f_{i}=^{i}\) for \(i 0\). Thus, H3 also captures the all-ones filter as a special instance. As we show in Proposition 1, this simple filter is optimal under independent data model and exactly imitates linear attention. Note that, utilizing a filter \(\) as in (2b) is strictly more expressive than the SSM as it captures all possible impulse responses.
2. H3 also applies a shift SSM to the key embeddings to enable the retrieval of the local context around associative recall hits. We opted not to incorporate this shift operator in our model. This is because unless the features of the neighboring tokens are correlated (which is not the case for the typical independent data model), the entry-wise products between values and shifted keys will have zero mean and be redundant for the final prediction.

We note that we conduct all empirical evaluations with the original H3 model, which displays exact agreement with our theory formalized for (6b), further validating our modeling choice.

### In-context Linear Estimation

We will next study the algorithms that can be implemented by the single-layer attention and state-space models. Through this, we will show that training ATT and SSM with linear ICL data is equivalent to the prediction obtained from one step of optimally-_preconditioned gradient descent_ (PGD) and _sample-weighted preconditioned gradient descent_ (WPGD), respectively. We will further show that under mild assumption, the optimal sample weighting for SSM (e.g., \(\)) is an all-ones vector and therefore, establishing the equivalence among PGD, ATT, and SSM.

**Background: 1-step gradient descent.** Consider minimizing squared loss and solving linear regression using one step of PGD and WPGD. Given \(n\) samples \((_{i},y_{i})_{i=1}^{n}\), define

\[=[_{1}\ \ _{n}]^{}^{n d} =[y_{1}\ \ y_{n}]^{}^{n}.\]

Starting from \(_{0}=_{d}\) and letting \(=1/2\) be the step size, a single-step GD preconditioned with weights \(\) returns prediction

\[=^{}^{}:=g_{}(),\] (3)

and a single-step _sample-weighted_ GD given weights \(^{n}\) and \(^{d d}\) returns prediction

\[=^{}^{}():=g_{}(),\] (4)

where \(\) is defined in (1) consisting of \(,\) and \(\). Our goal is to find the optimal \(\), as well as \(\) in (4) that minimize the population risks defined as follows.

\[_{}_{}() _{}()= [(y-g_{}())^{2}],\] (5a) \[_{,}_{}() _{}()= [(y-g_{}())^{2}].\] (5b)

Here, the expectation is over the randomness in \((_{i},_{i})_{i=1}^{n+1}\) and \(\), and we use \(\) to represent the set of corresponding trainable parameters. The search spaces for \(\) and \(\) are \(^{n}\) and \(^{d d}\), respectively.

As per (2), given input prompt \(^{(n+1)(d+1)}\), either of the underlying models outputs a \((n+1)\)-length sequence. Note that the label for the query \(=_{n+1}\) is excluded from the prompt \(\). Similar to Ahn et al. (2023); Mahankali et al. (2024), we consider a training objective with a causal mask to ensure inputs cannot attend to their own labels and training can be parallelized. Let \(_{0}=[_{1}\ \ _{n}\ 0]^{}\) be the features post-causal masking at time/index \(n+1\). Given weights \(_{k},_{q},_{v}\) and the filter \(\) for SSM, predictions at the query token \(=\\ 0\) take the following forms following sequence-to-sequence mappings in (2):

\[g_{}() =(^{}_{q}_{k}^{}_{0}^{})_{0}_{v},\] \[g_{}() =((^{}_{q})^{}((_{0}_{ k}_{0}_{v})*)_{n+1}),\]

where \(^{d+1}\) is the linear prediction head and \(((_{0}_{k}_{0}_{v})*)_{n+1}\) returns the last row of the convolution output. Note that SSM can implement the mask by setting \(f_{0}=0\). Now consider the meta learning setting and select loss function to be the squared loss, same as in (5). Thus, the objectives for both models take the following forms.

\[_{_{i},_{v},_{v},}_{}() _{}()=[(y-g_ {}())^{2}],\] (6a) \[_{_{i},_{i},_{i},,}_{ }()_{}( )=[(y-g_{}())^{2}].\] (6b)

Here, similarly, the expectation subsumes the randomness of \((_{i},_{i})_{i=1}^{n+1}\) and \(\) and \(\) represents the set of trainable parameters. The search space for matrices \(_{k}\), \(_{q}\), \(_{v}\) is \(^{(d+1)(d+1)}\), for head \(\) is \(^{d+1}\), and for \(\) is \(^{n+1}\).

Note that for all the optimization methods (c.f. (5), (6)), to simplify the analysis, we train the models without capturing additional bias terms. Therefore, in the following, we introduce the centralized data assumptions such that the models are trained to make unbiased predictions.

To begin with, a cross moment of random variables is defined as the expectation of a monomial of these variables, with the order of the cross moment being the same as order of the monomial. For example, \([^{}]\) is a sum of cross-moments of order 2. Then, it motivates the following data assumptions.

**Assumption 1**: _All cross moments of the entries of \((_{i})_{i=1}^{n+1}\) and \(\) with odd orders are zero._

**Assumption 2**: _The label noise \((_{i})_{i=1}^{n+1}\) are independent of \((_{i})_{i=1}^{n+1}\) and \(\), and their cross moments with odd orders are zero._

Note that compared to Ahn et al. (2023); Mahankali et al. (2024); Zhang et al. (2024), Assumption 1 is more general which also subsumes the dependent distribution settings. In this work, we consider the following three linear models (omitting noise) satisfying Assumption 1. Let \(_{},_{}^{d d}\) represent the task and feature covariance matrices for independent data, and let \(0 1\) be the correlation level when considering data dependency. More specific discussions are deferred to Section 3.

* Independent task and data: \((0,_{}),\ _{i} (0,_{}),\ \ \ \ 1 i n+1\).
* Retrieval augmented generation: \(,(0,_{d}),\ _{i}\ \ (,(1-^{2})_{d}),\ \ \ \ 1 i n\).
* Task-feature alignment: \((0,_{d}),\ _{i}\ \ (,_{d}),\ \ \ \ 1 i n+1\).

Next, we introduce the following result which establishes the equivalence among optimizing 1-layer linear attention (c.f. (6a)), 1-layer H3 (c.f. (6b)), and 1-step gradient descent (c.f. (5)).

**Proposition 1**: _Suppose Assumptions 1 and 2 hold. Consider the objectives as defined in (5) and (6), and let \(_{}^{},\ _{}^{},\ _{ }^{}\), and \(_{}^{}\) be their optimal risks, respectively. Then,_

\[_{}^{}=_{}^{} _{}^{}=_{}^ {}.\]

_Additionally, if the examples \((_{i},y_{i})_{i=1}^{n}\) follow the same distribution and are conditionally independent given \(,\), then SSM/H3 can achieve the optimal loss using the all-ones filter and \(_{}^{}=_{}^{}\)._

We defer the proof to Appendix A.1. Proposition 1 establishes that analyzing the optimization landscape of ICL for both single-layer linear attention and the H3 model can be effectively reduced to examining the behavior of a one-step PGD algorithm. Notably, under the independent, RAG and task-feature alignment data settings discussed above, examples \((_{i},y_{i})_{i=1}^{n}\) are independently sampled given \(\) and \(\), and we therefore conclude that \(_{}^{}=_{}^{}= _{}^{}\). Leveraging this result, the subsequent section of the paper concentrate on addressing (5a), taking into account various linear data distributions.

While Proposition 1 demonstrates the equivalence of optimal losses, we also study the uniqueness and equivalence of optimal prediction functions. To this end, we analyze the strong convexity of \(_{}()\) and derive the subsequent lemmas.

**Lemma 1**: _Suppose Assumption 2 holds and let \(=[_{1}\ _{2}\ \ _{n}]^{}\). Then the loss \(_{}()\) in (5a) is strongly-convex if and only if \([(^{}^{})^{2}]+[(^{}^{})^{2}]\) is strongly-convex. Additionally, let \(g_{}^{}\ g_{}^{}\) be the optimal prediction functions of (5a) and (6a). Then under the conditions of Assumptions 1 and 2, and the strong convexity, \(g_{}^{}=g_{}^{}\)._

**Lemma 2**: _Suppose that the label noise \((_{i})_{i=1}^{n}\) are i.i.d., zero-mean, variance \(^{2}\) and independent of everything else, and that there is a decomposition \(=_{1}+_{2}\), \(=_{1}+_{2}\), and \(=_{1}+_{2}\) such that either of the following holds_

* \(>0\)_, and_ \((_{1},_{1})\) _have full rank covariance and are independent of each other and_ \((_{2},_{2})\)_._
* \((_{1},_{1},_{1})\) _have full rank covariance and are independent of each other and_ \((_{2},_{2},_{2})\)_._

_Then, the loss \(_{}()\) in (5a) is strongly-convex._As mentioned above, in this work, we study three specific linear models: with general independent, RAG-related, and task-feature alignment data. Note that for all the three cases, according to Proposition 1, we have \(_{}^{}=_{}^{}=_{}^{}\). Additionally, the second claim in Lemma 2 holds, and \(_{}()\) is strongly convex. Therefore, following Lemma 1, we have \(g_{}^{}=g_{}^{}\). Thanks to the equivalence among PGD, ATT, and SSM, in the next section, we focus on the solution of objective (5a) under different scenarios, which will reflect the optimization landscapes of ATT and SSM models.

## 3 Main Results

In light of Proposition 1, optimizing a single layer linear-attention or H3 model is equivalent to solving the objective (5a). Therefore, in this section, we examine the properties of the one-step PGD in (5a). To this end, we consider multiple problem settings, including distinct data distributions and low-rank training. The latter refers to the scenario where the key and query matrices have rank restrictions, e.g., \(_{k},_{q}^{(d+1) r}\), as well as LoRA-tuning when adapting the model under distribution shift.

### Analysis of Linear Data Models

We first consider the standard independent data setting. We will then examine correlated designs.

**Independent data model.** Let \(_{}\) and \(_{}\) be the covariance matrices of the input feature and task vectors, respectively, and \( 0\) be the noise level. We assume

\[(0,_{}),_{i} (0,_{}),_{i}(0,^{2}), 1 i n+1\] (7)

and the label is obtained via \(y_{i}=_{i}^{}+_{i}\). Our following result characterizes the optimal solution of (5a). Note that the data generated from (7) satisfies the conditions in Proposition 1. Therefore, the same results can be applied to both linear-attention and H3 models.

**Theorem 1**: _Consider independent linear data as defined in (7), and suppose the covariance matrices \(_{},_{}\) are full rank. Recap the objective from (5a) and let \(_{}:=_{}_{}()\), and \(_{}=_{}(_{})\). Additionally, let \(=_{}^{1/2}_{}_{ {x}}^{1/2}\) and \(M=}()+^{2}\). Then \(_{}\) and \(_{}\) satisfy_

\[_{}=_{}^{-1/2}}_{}_{ }^{-1/2}_{}=M-n}(}_{}),\] (8)

_where we define \(}_{}=((n+1)_{d}+M^{-1})^{-1}\)._

**Corollary 1**: _Consider noiseless i.i.d. linear data where \(_{}=_{}=_{d}\) and \(=0\). Then, the objective in (5a) returns_

\[_{}=_{d}_{ }=d-.\]

See Appendix B.2 for proofs. Note that Theorem 1 is consistent with prior work (Ahn et al., 2023, Theorem 1) when specialized to isotropic task covariance, i.e., \(_{}=_{d}\). However, their result is limited as the features and task are assumed to be independent. This prompts us to ask: _What is the optimization landscape with correlated in-context samples?_ Toward this, we consider the following RAG-inspired and task-feature alignment models, where Assumptions 1 and 2 continue to hold and Proposition 1 applies.

**Retrieval augmented generation.** To provide a statistical model of the practical RAG approaches, given the query vector \(_{n+1}=\), we propose to draw ICL demonstrations that are similar to \(\) with the same shared task vector \(\). Modeling feature similarity through the cosine angle, RAG should sample the ICL examples \(_{i}\), \(i n\), from the original feature distribution conditioned on the event \((_{i},)\) where \(\) is the similarity threshold. As an approximate proxy, under the Gaussian distribution model, we assume that \((0,_{d})\), \((0,_{d})\) and that RAG samples \(\)-correlated demonstrations \((_{i},y_{i})_{i=1}^{n}\) as follows:

\[_{i}|\,(,(1-^{2})_{d}),_{i}(0,^{2}) y_{i}=_{i }^{}+_{i}, 1 i n.\] (9)

Note that the above normalization ensures that the marginal feature distribution remains \((0,_{d})\). The full analysis of RAG is provides in Appendix B.3. Specifically, when we carry out the analysis by assuming \(=(1/)\) and \(d/n=(1)\) where \(()\) denotes proportionality, our derivation leads to the following result:

**Theorem 2**: _Consider linear model as defined in (9). Recap the objective from (5a) and let \(_{}:=_{}_{}()\), and \(_{}=_{}(_{})\). Additionally, let \(=^{2}d+1\) and suppose \(=(1/)\), \(d/n=(1)\) and \(d\) is sufficiently large. Then \(_{}\) and \(_{}\) have approximate forms_

\[_{}}_{d}_{} d+^{2}-}.\] (10)

Here, (10) is reminiscent of Corollary 1 and has a surprisingly clean message. Observe that, \(^{2}d+1\) is the dominant multiplier ahead of \(n\) in both equations. Thus, we deduce that, RAG model follows the same error bound as the independent data model, however, its sample size is amplified by a factor of \(^{2}d+1\). \(=0\) reduces to the result of Corollary 1 whereas we need to set \(=(1/)\) for constant amplification. When \(=1\), RAG achieves the approximate risk \(_{} 2+^{2}\), where the constant bias is due to the higher order moments (e.g., the 4'th and 6'th moments) of the standard Gaussian distribution. As \(d\) increases, the normalized loss \(_{}/d 0\). The full analysis of its optimal solution \(_{}\) and loss \(_{}\) are deferred Theorem 4 in Appendix B.3.

**Task-feature alignment**. We also consider another dependent data setting where task and feature vectors are assumed to be correlated. This dataset model has the following motivation: In general, an LLM can generate any token within the vocabulary. However, once we specify the task (e.g. domain of the prompt), the LLM output becomes more deterministic and there are much fewer token candidates. For instance, if the task is "Country", "France" is a viable output compared to "Helium" and vice versa when the task is "Chemistry". Formally speaking, this can be formalized as the input \(\) having a diverse distribution whereas it becomes more predictable conditioned on \(\). Therefore, it can be captured through a linear model by making the conditional covariance of \(|.\) to be approximately low-rank. This formalism can be viewed as a _spectral alignment_ between input and task, which is also well-established in deep learning both empirically and theoretically (Li et al., 2020; Arora et al., 2019; Canatar et al., 2021; Cao et al., 2019). Here, we consider such a setting where the shared task vector is sampled as standard Gaussian distribution \((0,_{d})\) and letting \(=^{2}d+1\), we sample the \(\)-correlated ICL demonstrations \((_{i},y_{i})_{i=1}^{n+1}\) as follows:

\[_{i}|(,_{d}), _{i}(0,^{2}) y_{i}=^{-1/2} _{i}^{}+_{i}, 1 i n+1.\] (11)

Above, \(^{-1/2}\) is a normalization factor to ensure that label variance remains invariant to \(\). To keep the exposition cleaner, we defer the full analysis of its optimal solution \(_{}\) and loss \(_{}\) to Theorem 5 in Appendix B.4. Similar to the RAG setting, by assuming \(=(1/)\) and \(d/n=(1)\), we obtain the following results for the optimal parameter and risk.

**Theorem 3**: _Consider linear model as defined in (11). Recap the objective from (5a) and let \(_{}:=_{}_{}()\), and \(_{}=_{}(_{})\). Additionally, given \(=^{2}d+1\) and suppose \(=(1/)\), \(d/n=(1)\) and \(d\) is sufficiently large. Then \(_{}\) and \(_{}\) have approximate forms_

\[_{})/}_{d} _{} d+^{2}-)/}.\] (12)

Similar to (10), (12) contains \(=^{2}+1\) multiplier ahead of \(n\), which reduces the in-context sample complexity and setting \(=0\) reduces to the results of Corollary 1.

### Low-rank Parameterization and LoRA

In this section, we investigate training low-rank models, which assume \(_{k},_{q}^{(d+1)}\) where \(r\) is the rank restriction. Equivalently, we consider objective (5a) under condition \(()=r\).

**Lemma 3**: _Consider independent linear data as defined in (7). Recap the objective from (5a) and enforce rank \(() r\) and \(^{}=\). Let \(=_{}^{1/2}_{}_{}^{1/2}\) and \(M=()+^{2}\). Denoting \(_{i}\) to be the \(i\)'th largest eigenvalue of \(\), we have that_

\[_{rank() r,=^{}}()=M-_{i=1} ^{r}^{2}}{(n+1)_{i}+M}.\] (13)Note that \(()=_{i=1}^{d}_{i}\). Removing the rank constraint and considering noiseless data setting, this reduces to the following optimal risk \(_{}=_{i=1}^{d}+M}{n+1+M/_{i}}\). See Appendix C.1 for more details.

**Impact of LoRA:** Based on the above lemma, we consider the impact of LoRA for adapting the pretrained model to a new task distribution under jointly-diagonalizable old and new eigenvalues of \(,~{}^{new},(_{i})_{i=1}^{d},(_{i}^{new})_ {i=1}^{d}\). Consider adapting LoRA matrix to the combined key and value weights in attention, which reflects minimizing the population loss \(}(_{lora}):=(+_{lora})\) in (5a) with fixed \(\). Suppose \(()=(^{new})=M\), \(=0\) and \(\) is jointly diagonalizable with \(,~{}^{new}\), then LoRA's risk is upper-bounded by

\[_{(_{lora})}(_{lora})}_{|I|,[d]} (_{i I}+M}{n+1+M/_{i}}+_{i I} ^{new}+M}{n+1+M/_{i}^{new}}).\] (14)

Note that, the right hand side is provided assuming the optimal LoRA-updated model \(_{lora}\) is also jointly diagonalizable with covariances \(,~{}^{new}\), and \(\).

## 4 Experiments

We now conduct synthetic experiments to support our theoretical findings and further explore the behavior of different models of interest under different conditions. The experiments are designed to investigate various scenarios, including independent data, retrieval-augmented generation (RAG), task-feature alignment, low-rank parameterization, and LoRA adaption.

**Experimental setting.** We train 1-layer attention and H3 models for solving the linear regression ICL. As described in Section 2, we consider meta-learning setting where task parameter \(\) is randomly generated for each training sequence. In all experiments, we set the dimension \(d=20\). Depending on the in-context length (\(n\)), different models are trained to make in-context predictions. We train each model for 10000 iterations with batch size 128 and Adam optimizer with learning rate \(10^{-3}\). Since our study focuses on the optimization landscape, and experiments are implemented via gradient descent, we repeat 20 model trainings from different initialization and results are presented as the minimal test risk among those 20 trails. In all the plots, theoretical predictions are obtained via the corresponding formulae presented in Section 3 and the test risks are normalized by the dimension \(d\).

\(\)**Equivalence among \(_{}^{}\), \(_{}^{}\) and \(_{}^{}\) (Figure 2).** To verify Proposition 1 as well as Theorem 1, we run random linear regression instances where in-context samples are generated obeying (7). Fig. 1(a) is identical to Fig. 1(a) where we set \(_{}=_{}=_{d}\) and \(=0\). In Fig. 1(b), set \(_{}=_{}=\) and vary noise level \(^{2}\) from \(0\) to \(0.3 d\). In Fig. 1(c), we consider noiseless labels, \(=0\), isotropic feature distribution \(_{}=_{d}\) and set task covariance to be \(_{}=^{}+(1-)_{d}\) by choosing \(\) in \(\{0,0.3,0.6,0.9\}\). Note that in Fig. 1(c), we train a sufficient number of models (greater than 20) to ensure the optimal model is obtained. In all the figures, solid and dashed curves correspond to the

Figure 2: Empirical evidence validates Theorem 1 and Proposition 1. We train 1-layer linear attention and H3 models with prompts containing independent demonstrations following a linear model, and dotted curves are the theory curves following Eq. (8). **(a):** We consider noiseless i.i.d. setting where \(_{}=_{}=_{d}\) and \(=0\), with results presented in red (attention) and blue (H3) solid curves. **(b):** We conduct noisy label experiments by choosing \( 0\). **(c):** Consider non-isotropic task by setting \(_{}=^{}+(1-)_{d}\). Solid and dashed curves in (b) and (c) represent attention and H3 results, respectively. The alignments in (a), (b) and (c) show the equivalence between attention and H3, validating Theorem 1 and Proposition 1. More experimental details are discussed in Section 4.

ICL results from training 1-layer ATT and SSM models, respectively, and dotted curves are obtained from (8) in Theorem 1. The alignment of solid, dashed and dotted curves validates our Proposition 1 and Theorem 1.

\(\)**Distributional alignment experiments (Figs. 2(a)-2(b)).** In Figs. 2(a) and 2(b), we generate RAG and task-feature alignment data following (9) and (11), respectively, by setting \(=0\) and varying \(\) from 0 to 0.6. Attention training results are displayed in solid curves, and we generate theory curve (dotted) via the \(}\) formula as described in (36) in Appendix B.3 and (42) in Appendix B.4. The empirical alignments corroborate Theorems 4 and 5, further confirming that Proposition 1 is applicable to a broader range of real-world distributional alignment data.

\(\)**Low-rank (Fig. 2(c)) and LoRA (Fig. 2(d)) experiments.** We also run simulations to verify our theoretical findings in Section 3.2. Consider the independent data setting as described in (7). In Fig. 2(c), we set \(_{}=_{d}\), \(=0\) and task covariance to be diagonal with diagonal entries \(c[1\ 2^{-1}\ \ d^{-1}]^{}\) for some normalization constant \(c=d/_{i=1}^{d}i^{-1}\), and parameterize the attention model using matrices \(_{k},_{q}^{(d+1) r}\) and vary \(r\) across the set \(\{1,5,10,20\}\). Results show that empirical (solid) and theoretical (dotted, c.f. (13)) curves overlap. In Fig. 2(d), we implement two phases of training. _Phase 1:_ Setting \(_{}=_{}=_{d}\) and \(=0\), we pretrain the model with full rank parameters and obtain weights \(}_{k},}_{q},}_{v}^{(d+1)(d +1)}\). _Phase 2:_ We generate new examples with task covariance \(_{}\) being a diagonal matrix with diagonal entries \(c^{}[2^{-1}\ 2^{-2}\ \ 2^{-d}]^{}\) for some normalization constant \(c^{}=d/_{i=1}^{d}2^{-i}\). Given the rank restriction \(r\), we train additional LoRA parameters \(_{},_{}^{(d+1) r}\) where \(_{lora}:=_{}_{}^{}\) and (2a) becomes \(()=((}_{q}}_{k}^{}+_{ }_{}^{})^{})}_{v}\). Fig. 2(d) presents the results after two phases of training where dotted curves are drawn from the right hand side of (14) directly. Here, note that since \(,^{new}\) are diagonal, the right hand side of (14) returns the exact optimal risk of LoRA and the alignments verify it.

## 5 Related Work

There is growing interest in understanding the mechanisms behind ICL (Brown et al., 2020; Liu et al., 2023; Rae et al., 2021) in LLMs due to its success in continuously enabling novel applications for LLMs (GeminiTeam et al., 2023; OpenAI, 2023; Touvron et al., 2023). In the previous work, Garg et al. (2022) explored ICL ability of Transformers. In particular, they considered in-context prompts where each in-context example is labeled by a target function from a given function class, including linear models. A number of works have studied this and related settings to develop a theoretical understanding of ICL (von Oswald et al., 2023; Gatniry et al., 2024; Lin and Lee, 2024; Li et al., 2024; Bai et al., 2024; Akyurek et al., 2023; Zhang et al., 2023; Du et al., 2023). Akyurek et al. (2023) focus on linear regression and provide a construction of Transformer weights that can enable a single step of GD based on in-context examples. Along the similar line, Von Oswald et al. (2023) provide a construction of weights in linear attention-only Transformers that can emulate GD steps on in-context examples for a linear regression task. Similar to this line of work, Dai et al. (2023) argue that pre-trained language models act as meta-optimizer which utilize attention to apply meta-gradients to the original language model based on the in-context examples.

Figure 3: Distributional alignment and low-rank parameterization experiments. **(a)** and **(b)** show the ICL results using data generated via (9) and (11), respectively, by changing \(\) from 0 to 0.6. In **(c)**, we train low-rank linear attention models by setting \(_{k},_{q}^{(d+1) r}\) and in **(d)**, we apply the low-rank LoRA adaptor, \(_{lora}:=_{}_{}^{}\) where \(_{},_{}^{(d+1) r}\), to pretrained linear attention models and adjust the LoRA parameters under different task distribution. Solid and dotted curves correspond to the linear attention and theoretical results (c.f. Section 3), respectively, and the alignments validate our theorems in Section 3. More experimental details are discussed in Section 4.

Building on these primarily empirical studies, Zhang et al. (2024); Mahankali et al. (2024); Ahn et al. (2023); Duraisamy (2024) focus on developing a theoretical understanding of Transformers trained to perform ICL. For single-layer linear attention model trained on independent in-context prompts for random linear regression tasks, Mahankali et al. (2024); Ahn et al. (2023) show that the resulting model implements a single step of PGD on in-context examples in a test prompt, thereby corroborating the findings of (Von Oswald et al., 2023). Zhang et al. (2024) study the optimization dynamics of gradient flow while training a single-layer linear attention model on in-context prompts for random linear regression tasks. Similar to Mahankali et al. (2024); Ahn et al. (2023), they show that the trained model implements a single step of GD and PGD for isotropic and anisotropic Gaussian features, respectively. In addition, they also characterize the test-time prediction error for the trained model while highlighting its dependence on train and test prompt lengths.

While our work shares similarities with this line of works, as discussed in our contributions in the introduction, we expand the theoretical understanding of ICL along multiple novel dimensions, which includes the first study of LoRA adaptation for ICL in the presence of a distributional shift. Furthermore, we strive to capture the effect of retrieval augmentation (Lewis et al., 2020, Nakano et al., 2021) on ICL through our analysis. Retrieval augmentation allows for selecting most relevant demonstration out of a large collection for a test instance, e.g., via a dense retrieval model (Izacard et al., 2023), which can significantly outperform the typical ICL setup where fixed task-specific demonstrations are provided as in-context examples (Wang et al., 2022; Basu et al., 2023). Through a careful modeling of retrieval augmentation via correlated design, we show that it indeed has a desirable amplification effect where the effective number in-context examples becomes larger with higher correlation which corresponds to preforming a successful retrieval of query-relevant demonstrations in a practical retrieval augmented setup.

Recently, state space models (SSMs) (Gu et al., 2021, 2021, 2023; Fu et al., 2023; Gu and Dao, 2023) have appeared as potential alternatives to Transformer architecture, with more efficient scaling to input sequence length. Recent studies demonstrate that such SSMs can also perform ICL for simple non-language tasks (Park et al., 2024; Grazzi et al., 2024) as well as complex NLP tasks (Grazzi et al., 2024). That said, a rigorous theoretical understanding of ICL for SSMs akin to Zhang et al. (2024); Mahankali et al. (2024); Ahn et al. (2023) is missing from the literature. In this work, we provide the first such theoretical treatment for ICL with SSMs. Focusing on H3 architecture (Fu et al., 2023), we highlight its advantages over linear attention in specific ICL settings.

## 6 Discussion

In this work, we revisited the loss landscape of in-context learning with 1-layer sequence models. We have established a general connection between ICL and gradient methods that accounts for correlated data, non-attention architectures (specifically SSMs), and the impact of low-rank parameterization including LoRA adaptation. Our results elucidate two central findings: (1) The functions learned by different sequence model architectures exhibit a strong degree of _universality_ and (2) _Dataset and prompt design_, such as RAG, can substantially benefit ICL performance.

**Future directions and limitations.** The results of this work fall short of being a comprehensive theory for ICL in LLMs and can be augmented in multiple directions. First, while the exact equivalence between H3 and linear attention is remarkable, we should examine whether it extends to other SSMs. Secondly, while empirically predictive, our RAG and LoRA analyses are not precise and fully formal. Thirdly, it is desirable to develop a deeper understanding of multilayer architectures and connect to iterative GD methods as in (Ahn et al., 2023; Von Oswald et al., 2023). Finally, we have studied the population risk of ICL training whereas one can also explore the sample complexity of pretraining (Wu et al., 2023; Lu et al., 2024). Moving beyond the theoretically tractable setup of this work, our simplified models are trained on in-context prompts from random initialization. Therefore, this theoretical study doesn't address more challenging in-context learning tasks, such as question answering, where both in-context demonstration and general knowledge from pretraining are required. Future work in this area could also shed light on how certain contexts might elicit undesirable behaviors acquired by an LLM during pretraining, an aspect not covered in our current analysis. This work also studies a theoretical model for retrieval augmentation-based ICL. In a real-life retrieval augmentation-based ICL, one needs to account for the quality of the collection of the retrievable demonstrations and its (negative) impacts on the final predictions.