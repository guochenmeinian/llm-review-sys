# Instructing Goal-Conditioned Reinforcement Learning Agents with Temporal Logic Objectives

Wenjie Qiu

Rutgers University

wq37@cs.rutgers.edu

Wensen Mao

Rutgers University

wm300@cs.rutgers.edu

He Zhu

Rutgers University

hz375@cs.rutgers.edu

Wenjie Qiu and Wensen Mao contributed equally to this work.

###### Abstract

Goal-conditioned reinforcement learning (RL) is a powerful approach for learning general-purpose skills by reaching diverse goals. However, it has limitations when it comes to task-conditioned policies, where goals are specified by temporally extended instructions written in the Linear Temporal Logic (LTL) formal language. Existing approaches for finding LTL-satisfying policies rely on sampling a large set of LTL instructions during training to adapt to unseen tasks at inference time. However, these approaches do not guarantee generalization to out-of-distribution LTL objectives, which may have increased complexity. In this paper, we propose a novel approach to address this challenge. We show that simple goal-conditioned RL agents can be instructed to follow arbitrary LTL specifications without additional training over the LTL task space. Unlike existing approaches that focus on LTL specifications expressible as regular expressions, our technique is unrestricted and generalizes to \(\)-regular expressions. Experiment results demonstrate the effectiveness of our approach in adapting goal-conditioned RL agents to satisfy complex temporal logic task specifications zero-shot.

## 1 Introduction

Goal-conditioned learning Liu et al. (2022) is a type of reinforcement learning (RL) task where an agent learns to achieve a specific goal in a given environment. The goal can be defined in various ways, such as a particular state of the environment or a desired outcome, and the agent learns to map its current observations to actions that bring it closer to the goal. By providing a high-level goal, the agent can effectively learn to generalize its behavior and adapt to changing conditions.

However, goal-conditioned RL agents struggle with generalizing to more complex, task-oriented policies when the goals are specified by temporally extended instructions typically written in Linear Temporal Logic (LTL) languages, which creates a much larger task space than simple goals. In such cases, as the LTL task space is vast and difficult to exhaustively sample, it becomes more challenging for the agent to identify the correct sequence of actions needed to accomplish a task, which results in slow learning and poor generalization.

We propose a novel approach that enables simple goal-conditioned RL agents to follow arbitrary LTL specifications without any additional training over the LTL task space. This is in sharp contrast to existing approaches for finding LTL-satisfying policies, which require sampling a large set of LTL instructions from the task space during training as "goals" Vaezipoor et al. (2021); Kuo et al. (2020). Such methods have no guarantee in generalizing to unseen tasks outside their training distribution of LTL objectives. As an example, the capability of most existing learning algorithms for LTL-satisfying policies is restricted to specifications over regular expressions that can be modeled by reward machines Icarte et al. (2022, 2018). Our technique can handle \(\)-regular LTL specificationsover infinite control sequences even though the underlying goal-conditioned agents have never seen such specifications during training.

Our main contribution lies in a policy learning algorithm for LTL satisfaction, which decouples complex low-level environment interaction from high-level task planning. Specifically, we leverage a goal-conditioned RL agent to interact with low-level environments to learn how to achieve basic goals without considering higher-level tasks. The goal-conditioned RL agent has the capability of measuring the difficulty of transitioning between goals in the goal space. To solve an unseen LTL instruction \(\), our method applies a weighted graph search algorithm that engages in high-level planning over the difficulty of achieving the specified goals to choose the optimal sequence of sub-goals in \(\) for the goal-conditioned agent to reach. We present experimental results demonstrating the effectiveness of our technique in adapting goal-conditioned RL agents to satisfy complex LTL task specifications of varying complexity zero-shot.

## 2 Background

**Goal-Augmented Markov Decision Processes (MDPs)** extend standard MDPs by incorporating a set of goals \(\) within the system state space \(S\). We assume the existence of a state-goal labeling function \(L:S 2^{}\) that maps each state to a set of atomic propositional symbols used to describe the observation of a system state where an atomic propositional symbol \(g\) is a variable that takes on a truth. Throughout the paper, the terms "goals" and "propositions" are used interchangeably.

As an example, consider ZoneEnv, a Safety Gym environment Achiam and Amodei (2019) adapted from Vaezipoor et al. (2021). The environment (Fig. 1) is a walled 2D plane with colored zones that correspond to task propositions. Both the zones and the point robot are randomly positioned on the plane. In this example, we use a set of atomic propositions \(=\{y,w,r,j\}\) that represents the four colors yellow, white, red, and jet black.

Formally, we model a goal-augmented MDP as a tuple \((S,A,,L,T,R,)\) that consists of a state space \(S\), an action space \(A\), a goal space defined by a set of atomic propositions \(\), a transition function \(T:S A S\) that maps each state-action-state transition to a probability, a labeling function \(L\), and a reward function defined with the goals \(R:S A\) that maps each state-action transition to a scalar reward for a particular goal in \(\). \([0,1)\) is a discount factor that controls the relative importance of immediate versus future rewards.

In a goal-augmented MDP, the agent's objective is to reach a goal via a goal-conditioned policy \(:S G A\) that maximizes the expectation of the cumulative reward: \(J()=_{a_{t}=(|s_{t},g),g P_{g}}[_{t=0}^{ }^{t}R(s_{t},g_{t},a_{t})]\) where \(p_{g}\) be a distribution over goals in \(G\). The optimal policy \(^{*}=_{}J()\) maximizes the expected cumulative return over all possible goals, weighted by their probability under the goal distribution \(P_{g}\). This formulation encourages the policy to be effective at achieving a diverse set of goals, rather than simply optimizing for a single goal. For example, in the colored zone environment in Fig. 1 (left), a goal-conditioned agent policy \((a|s,g)\) can be trained to reach a specific colored zone \(g=\{y,w,r,j\}\).

The value function for goal-conditioned reinforcement learning \(V^{}(s_{t},g)\) returns the expected cumulative discounted reward obtained by following policy \(\) starting from state \(s_{t}\) with the goal \(g\) as the target: \(V^{}(s_{t},g)=a_{t}=(|s_{t},g),\\ s_{t+1} T(|s_{t},a_{t})\) [\(_{t=0}^{}^{t}R(s_{t},g,a_{t})\)] The goal-conditioned Q-function \(Q^{}(s_{t},g,a)\) returns the expected cumulative discounted reward obtained by taking action \(a\) in state \(s_{t}\), and then following policy \(\) thereafter, with the goal \(g\) as the target: \(Q^{}(s_{t},g,a)=r(s_{t},g,a)\) + \(_{s_{t+1} S}T(s_{t+1}|s_{t},a)V^{}(s_{t+1},g)\) The goal-conditioned \(Q\) and \(V\) functions may be learned using model-free RL algorithms such as \(Q\)-learning or policy gradient methods. In training, we assume goals can be uniformly sampled. We also assume that \(t_{max}\) is the maximum horizon and a goal

Figure 1: An LTL Task for ZoneEnv.

conditioned agent receives a positive reward signal only when it successfully achieves the specified goal by the end of a training episode by framing a reward function:

\[r(s_{t},g,a_{t})=[t=t_{max}][g L(s_{t})]\] (1)

where \(\) is the indicator function. This reward function has the following interpretation: given a state \(s_{t}\), will the goal-conditioned policy \(\) get to a goal \(g\) after \(t\) time steps of attempting to reach \(g\).

**Linear Temporal Logic (LTL)** is a formal language used to specify the temporal properties of a system. LTL formulas are built from a set of temporal operators, such as **X** ("next"), **F** ("eventually"), **G** ("always"), and **U** ("until"). LTL formulas are constructed over a finite set of atomic proposition symbols \(\). Formally, the syntax of LTL formulas is defined recursively as follows:

\[\,::=\,g  \ \ \ $}\]

Intuitively, the formula \(\) holds if \(\) holds at the next time step, \(_{1}_{2}\) holds if \(_{1}\) holds until \(_{2}\) holds, \(\) holds if \(\) holds at a future step, and \(\) holds if \(\) holds at the current and all future steps.

An LTL formula \(\) is said to be satisfied by a state transition system with state space \(S\) if and only if all infinite sequences of states of the system satisfy \(\). We assume the existence of a labeling function \(L:S 2^{}\) that maps each state to the atomic propositions that are true in that state. Given an infinite sequence of system states \(= s_{0},s_{1},s_{2},\) generated by a system, \(=\) models the satisfaction relation \(\) between an LTL formula \(\) and the sequence of system states \(\). Formally, \(\) if and only if \(s_{0}\) which. We can define \(s_{i}\) inductively, as illustrated on the right.

In the ZoneEnv example in Fig. 1 (left), the robot is tasked to oscillate between yellow zones and white zones while always avoiding red zones. Assume a set of atomic propositions \(=\{y,w,r,j\}\) that represents the four colors yellow, white, red, and jet black. The LTL objective for this task can be specified as:

\[(yw)( r)\] (2)

**Buchi Automaton** is a type of automaton used to recognize languages that consist of infinite sequences of symbols. A Buchi automaton is defined by a tuple \((Q,,,q_{0},F)\), where \(Q\) is a finite set of automaton states, \(\) is a finite alphabet of symbols, \(:Q 2^{Q}\) is a transition function that maps each state and symbol to a set of states, \(q_{0} Q\) is the initial state, and \(F Q\) is a set of accepting states. Informally, a Buchi automaton recognizes a language consisting of all infinite sequences of symbols (corresponding to automaton runs) that visit an accepting state infinitely often.

**Buchi Automaton Construction from LTL**. Buchi automata can be used to represent temporal specifications in LTL. Given an LTL formula \(\) over a set of propositional symbols in \(\), a Buchi automaton \(\) can be constructed that recognizes all infinite sequences of system states that satisfy \(\). The alphabet \(\) of the Buchi automaton \(\) is over the set of atomic propositions \(\) of \(\), i.e., \(=2^{}\). That is, an infinite sequence of system states \(s_{0},s_{1},s_{2},\) satisfies \(\) if and only if there exists an infinite sequence of automaton states \(q_{0},q_{1},q_{2},\) in \(\) such that \(q_{0}\) is the initial automaton state, for all \(i 0\), if \(s_{i} w_{i}\) where \(w_{i}\) then \((q_{i},w_{i})\) contains at least one state, and the set of states that occur infinitely often in the sequence \(q_{0},q_{1},q_{2},\) is a subset of the accepting states of \(\).

For example, for the ZoneEnv task in Fig. 1 (left), the converted Buchi automaton for the LTL task in Eq. 2 is depicted in Fig. 1 (right). The only accepting state in this machine is state 2. To reach the accepting state, a yellow zone must be visited first. The Buchi automaton accepts when state 2 is reached infinitely often, meaning that the agent must always make a loop back to state 2 from state 3 and then state 1 by visiting a white zone and a yellow zone. This kind of infinite looping behavior cannot be expressed by a regular expression.

## 3 Instructing Goal-Conditioned Agents with LTL Objectives

**Task-Augmented Markov Decision Processes**. We now formalize the problem of instructing an RL agent to follow LTL objectives. To this end, we generalize Goal-Augmented MDPs to Task-Augmented MDPs \((S,A,,,L,T)\) where the definitions for state space \(S\), action space \(A\), goal space \(G\), labeling function \(L\) (from states to atomic propositions in \(\)), state transition function \(T\) remain the same. \(\) is the universe space of LTL formulas that can be constructed from the propositions in \(\). We would like to construct a policy \((a|s,)\) such that for any possible LTL formula \(\), the policy \((a|s,)\) has the highest probability of LTL specification satisfaction with respect to \(\):

\[^{*}=_{}_{(,)}[[]]\] (3)

In this formulation, a task-augmented MDP does not have a reward function. Instead, the objective is to generate the most number of LTL-satisfying runs for a given LTL task. The MDP does not have a maximum horizon as the agent is expected to run over an infinite time horizon to satisfy an (\(\)-regular) LTL instruction (e.g., Fig. 1).

Solving task-augmented MDPs turns out to be significantly more challenging than learning for goal-augmented MDPs due to the extremely large task space, which grows exponentially with the number of goals. The agent must identify the correct sequence of sub-goals to accomplish an LTL task. For the ZoneEnv example in Fig. 1, an agent may be instructed to accomplish a task by reaching a white zone first and then proceeding to a red zone in two distinct phases. In each phase, the zone to avoid changes (avoiding jet black and then yellow). We specify this task in Eq. 4 whose Buchi automaton representation is depicted in Fig. 2.

\[ j(w( yr))\] (4)

The two LTL tasks in Eq. 2 and Eq. 4 express very different requirements and require distinct logic to solve. The agent is expected to successfully fulfill both tasks during inference.

### LTL Task Planning on Buchi Automata

Our approach relies on the principle that an accepting run of a Buchi automaton must contain an accepting state that is reachable from the initial state and lies on a cycle (e.g., Fig. 1). This means that accepting runs of a Buchi automaton can be searched on top of a directed graph representation of the automaton. A goal-conditioned agent can then be used to subsequently achieve the goals along the search paths for task execution.

However, this simple strategy may yield suboptimal task performance. Consider a Mujoco ant navigation task in a 16-room environment depicted in Fig. 3(left). The rooms are separated by thick walls and are connected through bi-directional doors. The agent is initially positioned at the center of the bottom-left room and is given the following LTL instruction to traverse through the centers of a series of rooms with choices:

\[(((0,2)(2,0))((2,2)(((2,1)(3,2 ))((3,1)(((1,1)(3,3))(1,3))))))\]

Here an atomic proposition \(g\) is in the form of \((r,c)\) denoting the room in the \(r\)-th row and \(c\)-th column. The bottom-left corner is room \((0,0)\). Given an environment state \(s\), define \(s(r,c)\) is true if the position of the Mujoco ant in \(s\) is close to the center of the \((r,c)\)-th room within a threshold. The Buchi automaton \(\) converted from the LTL instruction is given in Fig. 3(right). There are 8 paths on \(\) to reach the accepting state 6. A graph search algorithm could yield any of these paths for the Mujoco ant to traverse along the chosen route. However, not all of them are equally optimal.

Figure 3: An LTL Task for Ant-16rooms where the red path is more feasible than the black path.

Figure 2: Another LTL task for ZoneEnv.

Some of the routes require the ant to take a detour to avoid the black obstacles placed in certain room centers. In the worst case, the agent may be blocked by the obstacles and cannot complete the task.

In order to optimally accomplish the sequences of tasks defined in an LTL instruction \(\) with atomic propositions in \(\), we perform high-level reasoning by evaluating the difficulty of achieving the specified goals (i.e. atomic propositions) in \(\) in the goal space \(\). Our technique exploits the value function \(V^{}(s,g)\) of a goal-conditioned agent \((|s,g)\) to capture the agent's transition capabilities. For any pair of goals, we learn a _goal value function_\(^{}:S\) to estimate the capability of taking the goal-conditioned policy \(\) to go from one goal to another goal on average. The function is parameterized by an environment state \(s\) because the agent's capability to transition between goals can differ depending on various environment layouts and agent states. To get the desired estimate, we regress \(\) towards the value function of \(\) by minimizing:

\[_{}(V(s_{t},g_{k})-(s_{j},g_{t},g_{k}))^{ 2}g_{k} L(s_{k}) g_{t} L(s_{t})\] (5)

with \( B\) (a replay buffer), \(t\{0 t_{max}\}\), \(s_{t},a_{t},s_{t+1}\), \(j\{0,t\}\), \(k\{t+1 t_{max}\}\), and \(L\) as the state labeling function. We train \(\) together with a goal-conditioned learning algorithm.

With the learned goal value function \(\) of a goal-conditioned agent as a measurement to assess its capability to transition between goals, we reduce the task of learning an LTL specification satisfying policy to finding the optimal accepting run in the Buchi automaton representation of the LTL specification. We convert a Buchi automaton \(=(Q,,,q_{0},F)\) as a directed graph \(G_{}=(,,,_{0},, )\) with nodes \(\), node transitions (edges) \(\) over the same alphabet \(\), a set of initial nodes \(_{0}\) and accepting nodes \(\). The conversion from \(\) to graph \(G_{}\) starts with setting \(=Q\), \(=\), \(_{0}=\{q_{0}\}\), and \(=F\). The node labeling function \(\) maps nodes in \(\) to atomic propositions (or goals) in the alphabet \(\). Intuitively, \(\) approximates the specific region in the goal space where the agent should be positioned at a given node. For each _non-self_ transition (\(q_{i},,q_{j}\)) \(\)\(\) in \(\) where \(\) encodes the goal space to which the transition is enabled, we have \((q_{j})\) = \(\). As an example, Fig. 4(a) depicts the converted graph representation \(G_{}\) from the Buchi automaton in Fig. 3(right). For the transition from the initial node to node 1, the node labeling function \(\) maps node 1 to goal room\((0,2)\). This transition denotes that the agent reaches room\((0,2)\) from the initial room. If there are several incoming transitions with different alphabet symbols to \(q_{j}\) in \(\), we duplicate \(q_{j}\) and its outgoing transitions in \(G_{}\) so that each duplicated node for \(q_{j}\) captures one of the possible transitions to \(q_{j}\). For example, in Fig. 3, there are two non-self Buchi automaton transitions to state 1 which are encoded as node transitions from the initial room to room\((0,2)\) and \((2,0)\) respectively in Fig. 4(a). An exception for constructing the node labeling function \(\) occurs when \(\) takes the form of \(_{k} g_{k}\) on a non-self transition (\(q_{i},,q_{j}\)) where all atomic propositions \(g_{k}\) are negative, indicating a specific area in the goal space that must be avoided to activate the transition. In this case, the designated goal area outlined by \(\) for \(q_{j}\) can be excessively large. We instead assign \((q_{j})=(q_{i})\). Intuitively, if \(g_{k}\) does not intersect with the current state on \((q_{i})\), the agent can trivially fulfill the avoidance requirement by making a "jumped" transition to \(q_{j}\). For example, the "avoidance" transition from state 2 to state 3 on the Buchi automaton in Fig. 1(right) is encoded as the rightmost non-self transition in the graph representation of the automaton in Fig. 4(b).

We interpret the semantics of a graph representation \(G_{}\) converted from a Buchi automaton \(\) through the different kinds of _non-self_ transitions that \(G_{}\) may have, as visualized in Fig. 4(d), (e), and (f). In

Figure 4: Graphs (a), (b), and (c) are converted from the Büchi automata in Fig. 3, Fig. 1, and Fig. 2. (d), (e), and (f) are types of transitions in a graph representation of a Büchi automaton.

Fig. 4(d), on a non-self transition \((q_{i},,q_{j})\), in order to transit to \(q_{j}\), the goal-conditioned agent is tasked to reach the goal region \(_{j}g_{j}\) from the goal region \((q_{i})_{i}g_{i}\). The "self-transition" on a node \(q_{i}\) describes a goal-related constraint \(^{}=_{k} g_{k}\) that must be maintained until the goal-conditioned agent can transit to \(q_{j}\). The agent can take an unlimited number of steps to reach the goal region \(\), provided that each step avoids entering any undesired region \(g_{k}\) within \(^{}\). For weighted graph search, we assign the weight of such a transition as:

\[(s,(q_{i},,q_{j}))=_{g_{i},g_{j}}-(s,g_{i},g_ {j})(q_{i})=_{i}g_{i}=_{j}g_{j}\] (6)

The weight encodes the capability of a goal-condition agent to transit from \(q_{i}\) to \(q_{j}\) from an observation at environment state \(s\). In Fig. 4(d), we restrict \((q_{i})\), \(\), and \(^{}\) to conjunctions of atomic propositions or negated propositions. In Appendix G.3, we extend this definition to include formulas as conjunctions of propositions and their negations to support more complex LTL tasks, such as \((g_{1} g_{2})\).

In Fig. 4(e), the transition is similar to that of Fig. 4(d), and the agent can also take as many steps as necessary to reach the goal region specified by \(\) by utilizing the self transition on the source node \(q_{i}\). However, there are no specific avoidance requirements to consider. We use Equation 6 to weigh this kind of transition. In Fig. 4(f), the goal-conditioned agent is required to reach the goal region \(\) in exactly _one_ step as there is not any self transition that can be taken upon \(q_{i}\). We set \((s,(q_{i},,q_{j}))=0\) (the full capacity) if \((q_{i})\) i.e. the agent can directly "jump" to \(q_{j}\) (e.g. see the rightmost non-self transition in Fig. 4(b)). Otherwise, we assign it the lowest capacity \((s,(q_{i},,q_{j}))=\). This is because our approach cannot explicitly pick an action to reach a goal region in a single step (we instead depend on the goal-conditioned agent to take as many steps as needed for goal reaching).

### Algorithm Summary

Our technique generates policies for a LTL specification \(\) based on a goal-conditioned RL agent \(\) and a learned goal value function \(\) as follows:

**1.** We first convert \(\) to a Buchi automaton \(\), which is subsequently converted to a graph representation \(G_{}\) using the technique illustrated in Sec. 3.1.

**2.** Associate each transition \((q_{i},,q_{j})\) on \(G_{}\) with weight \((s,(q_{i},,q_{j}))\) according to Equation 6 to measure the capability of the goal-conditioned agent to make the transition. In our implementation, we set \(s\) as the initial state of an episode. A more advanced planning strategy may conduct task planning every \(h\) timesteps from a current environment state for the remaining sub-goals on \(G_{}\) and update the weight of any transition based on the current state before replanning.

**3.** Decompose \(G_{}\) into strongly connected components (SCCs) using Tarjan's algorithm Tarjan (1972).

**4.** To find an optimal path on \(G_{}\) for task execution to satisfy the LTL \(\), we follow these steps:

* For each accepting state \(s_{a}\) within a maximal SCC Geldenhuys and Valmari (2004), we use Dijkstra's algorithm to find the shortest path from the initial state to \(s_{a}\), denoted as \(p\).
* Next, we apply Dijkstra's algorithm to find the shortest cycle from \(s_{a}\) back to \(s_{a}\) in the maximal SCC, denoted as \(q\).
* The optimal path for the accepting state \(s_{a}\) is \(pq^{}\) where \(\) represents the number of times the shortest cycle is executed. The cost of the optimal path is calculated as \(w(p)+w(q)^{}\) where \(w(p)\) or \(w(q)\) is the sum of the weights of the transitions on \(p\) or \(q\). In the implementation, we use \(=5\) to estimate the path cost.
* Finally, we select the optimal path on \(G_{}\) as the least-cost path to any accepting state of \(G_{}\).

**5.** For task execution, on the searched optimal path \(pq^{}\), we use a goal-conditioned agent to subsequently achieve the goals along the path \(p\) and reach the goals on \(q\) iteratively in a loop to satisfy \(\). For any transition \((q_{i},,q_{j})\) on a searched path, the agent employs its goal-conditioned policy \((|s,)\) to reach the targeted goal region \(\) (Fig. 4(d),(e)).

**Justification.** Recall that our objective of learning a LTL specification satisfying policy (Equation 3) is finding \(^{*}=*{arg\,max}_{}_{(,)} [[]]\), which generates the maximum number of LTL-satisfying runs for a given LTL property \(\). Our formalization of goal-condition RL uses a sparse binary reward function (Equation 1) - a reward of 1 is provided only when the specified goal is successfully achieved by the end of a training episode. In this setting, when the discounted factor \( 1\), the weight \(w\) of a transition on \(G_{}\), which is determined by the learned value function \(\) (e.g. \(w=()\)), is inversely proportional to the probability of reaching the goal region represented by the target node from that represented by the source node. As such, based on the capability of the goal-conditioned agent (i.e. the learned value function \(\)), the task planning algorithm seeks the optimal path as the one the agent is most likely to succeed.

### Handling Avoidance

As depicted in Fig. 4(d), our method uses the goal-conditioned agent policy \(\) to fulfill a reach-avoid subtask defined as \((|s,_{j}g_{j},_{k} g_{k})\) - from a state \(s\) an agent is required to avoid several regions \(g_{k}\) (\(k 1\)) before reaching the target region \(_{j}g_{j}\) in the goal space. Our method only performs avoidance in situations where there is a high likelihood of colliding with any \(g_{k}\). The likelihood of a collision occurring with \(g_{k}\) from a current state \(s\) can be assessed by evaluating the value function \(V(s,g_{k})\) of the goal-conditioned agent. If \(V(s,g_{k})\) exceeds a threshold \(\), it indicates a high likelihood of collision. We formally define the reach-avoid policy \(\) as follows:

\[(|s,_{j}g_{j},_{k} g_{k})\]

**let**\(k=_{k}V(s,g_{k})\)**in**

**if**\(V(s,g_{k})<\)

**then**\(_{a}_{j}Q^{}(s,g_{j},a)\)

**else**

**let**\(d=_{a}Q(s,g_{k},a)\)

**let**\(s=_{a}Q(s,g_{k},a)\)

**let**\(t=_{a d}_{j}Q^{}(s,g_{j},a)\)

**else**

**let**\(d=_{a}Q(s,g_{k},a)\)

**let**\(s=_{a}Q(s,g_{k},a)\)

**let**\(t=_{a d}_{j}Q^{}(s,g_{j},a)\)

Figure 5: Handling reach-avoid subtasks by goal-conditioned agents in GCRL-LTL.

We extend Proximal Policy Optimization (PPO) (Schulman et al. (2017)) to a goal-conditioned RL algorithm to train the goal-conditioned agents for LetterWorld and ZoneEnv. We use an extension of the GCSL Ghosh et al. (2021) algorithm to train a goal-conditioned agent for Ant-16rooms. More details about the training algorithms are provided in Appendix B.

### Evaluation in Multi-Task Settings

**Baseline.** We compared GCRL-LTL with **LTL2Action**( Vaezipoor et al. (2021)). This baseline exemplifies the state-of-the-art learning algorithms for LTL-satisfying policies that use advanced neural architectures to encode LTL task formulas for policy decision-making. In contrast, GCRL-LTL employs weighted graph search to decompose LTL tasks into basic reach-avoid tasks that can be accomplished by goal-conditioned agents.

We consider the following LTL task spaces to evaluate GCRL-LTL and the baseline.

**Partially-Ordered Tasks**. Such a task consists of multiple streams of sub-tasks. A sub-task contains a sequence of goals that must be satisfied in a specified order. For instance, a possible task can be \((p(qr))(sq)\). This task can be described as _"satisfy \(p\), \(q\), \(r\) in that order; and satisfy \(s\), \(q\) in that order"_. One valid solution is to reach \(s\), \(p\), \(q\), \(r\) in order to satisfy all sub-tasks.

**Avoidance Tasks**. Similar to partially ordered tasks, a sequence of propositions must be satisfied in order, while some propositions must be avoided. Visiting a prohibited position leads to failure. An example avoidance task is \( y(w( yr))\) (Fig. 2), which can be interpreted as a two-stage task: _"reach \(w\) while avoiding \(j\) in the first stage, then reach \(r\) while avoiding \(y\) in the second stage"_.

During training, LTL2Action has access to a task sampler that produces a random LTL task from the large set of possible task spaces described above for each training episode. GCRL-LTL does not use the task sampler and is only trained to reach random goals. In evaluation, tasks are randomly sampled in each episode for both agents. We choose \(=0.85\) in Fig. 5 and perform ablation study on this value in Appendix G.3.

Figure 6 shows the evaluation results across training iterations on both partially ordered and avoidance tasks in LetterWorld. Fig. 7 demonstrates the results for avoidance tasks in ZoneEnv. GCRL-LTL outperforms LTL2Action by a large margin despite having no access to the task sampler during training. Furthermore, we assess the generalizability of GCRL-LTL and the baseline by testing

Figure 6: (a) and (b) shows success rates and discounted rewards of LetterWorld on partially-ordered tasks, respectively. (c) and (d) shows success rates and discounted rewards on avoidance tasks, respectively. We report the average results on 5 random seeds, where each data point is acquired by collecting results from 100 episodes.

Figure 7: Comparisons of (a) success rate and (b) discounted rewards in ZoneEnv on avoidance tasks. We report the average results on 5 random seeds, where each data point is acquired by collecting results from 50 episodes. The initial 2 million steps are omitted due to the training of the dynamic primitives.

them on more complex tasks that are out-of-distribution to the task sampler used in baseline training. Specifically, for partially ordered tasks in LetterWorld, we increase the maximum depth from 5 to 15, and the number of conjunctions from 4 to 12. Similarly, we extend the avoidance tasks in ZoneEnv to a more difficult setting where the agent needs to avoid one extra zone in the second stage (\((a( bc))( d)\)). Additionally, we test if agents trained in ZoneEnv can efficiently solve tasks in which a long sequence of consecutive _zones_ must be reached (\((a(b(cd)))\)). The experimental results, as detailed in Table 1, indicate that GCRL-LTL significantly outperforms the baseline in its ability to generalize to unseen tasks.

\(\)**-regular LTL Tasks.** We conducted experiments to evaluate the performance of GCRL-LTL in satisfying \(\)-regular LTL objectives. \(\)**-Stability Tasks**: such a task requires the agent to satisfy a task proposition by reaching a goal region and remaining in the goal region infinitely. \(\)**-Loop Tasks:** such a task in the form \((ab)( c)\) requires the agent to continuously cycle between two specific zones, \(a\) and \(b\), while always avoiding another zone, \(c\). For example, in the task \((ry)( w)\), the agent is expected to traverse between the _Red_ and _Yellow_ zones indefinitely, while never visiting any _White_ zone. We evaluate the satisfiability of \(\)-loop and \(\)-stability tasks in the ZoneEnv environment during the goal-conditioned agent training process. The results are presented in Figure 8. The \(x\)-axis represents the number of environment interaction steps, while the \(y\)-axis represents the number of \(\) occurrences. In the \(\)-loop tasks it represents the number of rounds completed by the agent in looping between specific zones, and in the \(\)-stability tasks it represents the number of transitions the agent successfully remains in the desired zone, within the first 1500 timesteps (higher values indicate better performance). The results confirm that GCRL-LTL can handle \(\)-regular LTL specifications even though the underlying goal-conditioned agents have never seen such specifications during their training time. Figures 8(a) and 8(b) illustrate agent trajectories that solve the above \(\)-LTL task. In both

  Environment & Tasks & Timeout & Ours & LTL2Action \\   & Partially-Ordered & 75 & **0.996(0.423)** & 0.661(0.129) \\  & Partially-Ordered + Depth \(\) & 225 & **0.986(0.259)** & 0.477(0.033) \\  & Partially-Ordered + Conjuncts \(\) & 225 & **0.992(0.410)** & 0.567(0.049) \\   & Avoidance & 1000 & **0.958(0.632)** & 0.934(0.601) \\  & \((a( bc))( d)\) & 1000 & **0.928(0.543)** & 0.342(0.304) \\  & \((a(b(cd))\) & 2000 & **0.926(0.220)** & 0.060(0.020) \\  

Table 1: Performance of trained agents on original and out-of-distributions tasks. We report the _success rate_ and _discounted return_(in parentheses). Results are averaged over 1000 episodes.

Figure 8: Figures (a) and (b) depict the performance of trained goal-conditioned agents in satisfying the \(\)-loop and \(\)-stability tasks in the ZoneEnv environment. Each data point represents the average performance obtained from evaluation on 20 episodes, and the results are averaged over 5 random seeds.

Figure 9: Figures (a) and (b) illustrate agent trajectories that solve the \(\)-loop task \((ry)( w)\) in the ZoneEnv environment. The initial positions of the robot are represented by orange diamonds \(\).

trajectories, the robot quickly reaches the _Red_ zone and subsequently oscillates between the _Red_ and _Yellow_ zones. More examples can be found in Appendix E.

### Comparison with Compositional RL

We also compare our approach with DiRL ( Jothimurugan et al. (2021)), which is a state-of-the-art compositional RL algorithm for LTL-satisfying policies. DiRL simultaneously learns a low-level policy for each sub-goal transition and a high-level policy to plan over the sub-goal space. However, it is restricted to a small fragment of LTL, cannot handle \(\)-regular LTL properties (e.g. Fig. 1), and cannot be applied to multi-task RL because the low-level policies are specific to a single environment setting and do not generalize across different environments. We used 8 LTL task specifications. These tasks have increasing levels of difficulty as they require the agent to sequentially reach a growing number of sub-goals. An example instruction is given in Fig. 3. DiRL has to train a separate agent for each of the LTL specifications. In contrast, GCRL-LTL trains a single goal-conditioned agent and evaluates this agent over all 8 LTL specifications. Our agent achieves a success rate of approximately 90% in fulfilling all specifications after being trained with 3e6 environment steps, whereas the DiRL method has to exercise 3e6 steps for each of the specifications to match the success rate of GCRL-LTL. More comprehensive evaluation results are given in Appendix F.

## 5 Related Work

There exists a large amount of previous work in using linear temporal logic (LTL) for specifying or shaping the reward functions for reinforcement learning (RL) Aksaray et al. (2016); Littman et al. (2017); Sadigh et al. (2014); Hasanbeig et al. (2018); Cai et al. (2021); Hahn et al. (2018); Camacho et al. (2019); Hasanbeig et al. (2020); Jothimurugan et al. (2020); Icarte et al. (2018, 2022). These methods primarily concentrate on learning a single, specific task defined by an LTL objective. In contrast, our approach involves learning task-oriented policies akin to goal-conditioned RL agents within a multitasking framework. In this setup, a new _arbitrarily complex_ LTL task is sampled as a goal for each episode.

Past research efforts have also aimed to empower RL agents to handle previously unseen multi-task instructions expressed in LTL. One common solution is decomposing complex LTL tasks into smaller, independent subtasks to simplify the learning process. For example, Araki et al. (2021) develops a hierarchical options framework that learns various neural networks each specialized for one subtask, and optimally composes these learned options via value iteration to solve temporal logic specifications. However, subtask execution does not consider what the agent must do afterward and hence might perform sub-optimally in solving the full task. Leon et al. (2022) progresses towards completion of an LTL task by greedily identifying subtasks of the original LTL instruction that remain to be addressed, which may result in myopic behaviors. Recent works show that the agents' neural architecture to embed LTL specifications is the key to improving the performance of RL agents in unseen environments. Kuo et al. (2020) presents a novel network architecture framework to compose neural networks one for each LTL operator and environment object to mirror the formula structure. In a similar vein, Vaezipoor et al. (2021) uses neural encodings to interpret full LTL specifications to guide an RL agent to learn task-conditioned policies in multitask settings. Voloshin et al. (2023) developed eventual discounting that exploits the fact that optimally satisfying the LTL specification does not depend on the length of time it takes to reach accepting states (e.g., "eventually always reach the goal") to find LTL-satisfying policies with the highest achievable probability. We discuss the other related work in a more general context in Appendix C.

## 6 Conclusion

This paper presents GCRL-LTL to learn task-oriented policies where tasks are specified by Linear Temporal Logic (LTL) instructions. Unlike existing methods that sample a vast set of LTL instructions during training, our approach enables simple goal-conditioned RL agents to follow arbitrary and complex LTL specifications without the need for additional training over the LTL task space. GCRL-LTL is capable of handling diverse and intricate LTL specifications including \(\)-LTL objectives over infinite time horizons. Through extensive experiments, we demonstrated the effectiveness of GCRL-LTL to seamlessly satisfy complex temporal logic task specifications in a zero-shot manner.