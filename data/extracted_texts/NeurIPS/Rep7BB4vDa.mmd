# Species196: A One-Million Semi-supervised Dataset

for Fine-grained Species Recognition

 Wei He, Kai Han, Ying Nie, Chengcheng Wang, Yunhe Wang

Huawei Noah's Ark Lab

{hewei142,kai.han,yunhe.wang}@huawei.com

Corresponding Author

###### Abstract

The development of foundation vision models has pushed the general visual recognition to a high level, but cannot well address the fine-grained recognition in specialized domain such as invasive species classification. Identifying and managing invasive species has strong social and ecological value. Currently, most invasive species datasets are limited in scale and cover a narrow range of species, which restricts the development of deep-learning based invasion biometrics systems. To fill the gap of this area, we introduced Species196, a large-scale semi-supervised dataset of 196-category invasive species. It collects over 19K images with expert-level accurate annotations (_Species196-L_), and 1.2M unlabeled images of invasive species (_Species196-U_). The dataset provides four experimental settings for benchmarking the existing models and algorithms, namely, supervised learning, semi-supervised learning, self-supervised pretraining and zero-shot inference ability of large multi-modal models. To facilitate future research on these four learning paradigms, we conduct an empirical study of the representative methods on the introduced dataset. The dataset is publicly available at https://species-dataset.github.io/.

## 1 Introduction

Invasion biometrics play a critical role in the identification and management of invasive species, which are non-native organisms capable of causing detrimental impacts on the environment, economy, and human health . Traditionally, invasive species recognition systems have relied on trained experts who analyze an animal or plant's physical characteristics, or use DNA testing . However, these conventional methods require specialized equipment, expert knowledge, and are often time-consuming and costly, while also posing potential risks to creature well-being . Recent methods , utilizing computer vision techniques and deep learning algorithms, provide a cost-effective and efficient alternative solution for invasive species identification.

As computer vision-based methods continue to gain traction, the need for high-quality biometrics data becomes increasingly crucial for effective invasive species recognition. In contrast to traditional visual tasks such as image classification or object detection, which benefit from large-scale datasets like ImageNet , Open Images Dataset, COCO, and Objects365 , acquiring diverse, high-quality datasets for training and evaluating invasive species recognition algorithms remains a challenge.

In response to this challenge, we introduce _Species196-L_, a new fine-grained hierarchical dataset focusing on regional invasive species. The "L" donates that the dataset is fine labeled. _Species196-L_ collects 19236 images for 196 invasive species listed in the _Catalogue of Quarantine Pests for Import Plants to China_. In addition, We also provide bounding box annotation and detailedmulti-grained taxonomy information for each image, which assists users in building more effective invasive species biometrics systems.

Self-supervised and semi-supervised learning are particularly useful when labeled data is costly or difficult to collect. To further enhance our _Species196_ dataset, we utilize the publicly available LAION-5B dataset  to create a new, large-scale 1.2M-image subset based on _Species196-L_'s invasive species, named _Species196-U_. The "U" signifies that the dataset is unlabeled. In _Species196-U_, each image is retrieved from the LAION-5B dataset  using a pre-trained OpenCLIP \(\) image encoder, based on the original finely annotated _Species196-L_. This dataset expands the amount of data available for invasive species research and provides a valuable domain-specific resource for unsupervised, self-supervised, or multi-modal learning.

Our main contributions are summarized as follows:

* To the best of our knowledge, we have constructed the largest-scale invasive species biometrics dataset (_Species196-L_), covering a wide range of 196 species. This dataset not only encompasses different life cycles but also provides detailed taxonomic information for each species, ranging from Kingdom and Phylum to Genus and Species.
* We present a large-scale, domain-specific unlabeled dataset, _Species196-U_, which is highly related to images from _Species196-L_, containing 1.2 million image-text pairs. This dataset serves as an ideal testbed for evaluating various pretraining and multi-modal methods.
* We perform extensive experiments on our proposed datasets using a variety of methods, including state-of-the-art CNN and transformer networks, specifically designed fine-grained visual recognition techniques, self-supervised and semi-supervised approaches, and also CLIP and multi-modal large models' zero-shot inference performance on _Species-L_. These comprehensive experiments establish a benchmark for Species196.

## 2 Related work

### Invasion biometrics

Invasive species cause ecological change, harm biodiversity , and present significant threats to global agriculture  and economic welfare . Invasive biometrics plays a vital role in identifying and managing these species. Conventional invasive species identification methods (_e.g._ physical analysis and DNA testing) are costly, time-consuming, and heavily relied on expert knowledge and experience[11; 22].

In recent years, there has been a growing interest in using computer vision (CV) for invasive species recognition, as it offers more efficient, cost-effective, and scalable alternatives to traditional methods. These methods can be divided into two types - handcrafted and deep feature based. Handcrafted based methods adopting feature extractors like SIFT  and HOG. Li _et al._ proposed a HOG and SVM based methods for identifying invasive Asian Hornets , which extracting image features using the HOG algorithm, and using the SVM algorithm for target detection. More recently, deep learning techniques have shown its potential in this field. Chowdhury _et al._ suggested an approach for identifying aquatic invasive species by utilizing an autoencoder feature extractor, coupled with a classifier trained to distinguish between invasive and non-invasive species. Deep convolutional neural network is also used by Ashqar _et al._ to identifying images of invasive hydrangea, and by Huang _et al._ to develop a fast and accurate detection technology to identify invasive weeds.

Although deep learning-based methods are becoming increasingly popular, they heavily rely on high-quality annotated data compared to handcrafted-based methods, especially in practical applications where high-quality labeled data is often the performance bottleneck for invasion biometrics or other pest recognition applications.

### Related datasets

In this section, we provide an overview comparing our proposed Species196 dataset with other related species recognition and fine-grained datasets. The comparison overview is shown in Table 1.

Pink-Eggs  comprises images accompanied by bounding box annotations of the invasive species Pomacea canaliculata, Cional17  is a semantic segmentation dataset providing pixel-level annotations concerning invasive species in marine environments. However, these two invasive biometrics datasets possess a rather limited number of samples and focus only on specific types of organisms. In comparison, our _Species196-L_ dataset offers nearly 20,000 samples, covering 196 diverse invasive species, making it particularly valuable in various real-world applications such as customs and border crossings quarantine.

There are also some datasets that share the same super-categories, such as weeds and insects, with _Species196-L_. Some of them  have fewer sample numbers and species compared to _Species196-L_, while other datasets  are not publicly available so far. IP102  and CWD30  are two recent representative datasets in the field of insect and crop-weed. IP 102 covers 102 species of common crop insect pests with over 75, 000 images, captures various growth stages of insect pest species. CWD30 comprises over 219,770 high-resolution images of 20 weed species and 10 crop species, encompassing various growth stages, multiple viewing angles, and build a hierarchical taxonomic system foe these weeds and crops. Following the best practices of IP102 and CWD30, our _Species196-L_ dataset covers various life stages of insects and establishes a comprehensive taxonomy system, from domain, kingdom, down to order, family, genus, and species, aiding in the creation of a precise and robust recognition system.

As a challenging benchmark dataset for fine-grained image recognition, we also compare _Species196-L_ to other popular datasets in this field. Oxford Flowers , CUB200 , FGVC Aircraft  and Stanford Cars  are popular fine-grained image classification datasets with different categories of flowers, birds and cars, respectively. However, these datasets do not involve the challenge of distinguishing species at different life stages and do not provide multi-grained taxonomy information.

Figure 1: Taxonomy of a portion of the _Species196-L_ datasetâ€™s taxonomy hierarchical system.

The iNat2017 dataset  introduced a large-scale, fine-grained image classification and detection dataset with 859K images from 5,089 species, emphasizing the importance of few-shot learning. The iNat2021 dataset  expanded this to 2.7M images from 10K species and highlighted the advantages of self-supervised learning methods for fine-grained classification. In contrast to the iNat2017 and iNat2021 datasets, which are derived from user-contributed observations on the iNaturalist community, the scale and collection method of our Species196 dataset align more closely with real-world applications. Moreover, Species196 provides a challenging testbed for the research community to explore how to improve model's performance through limited meticulously labeled data and large-scale unlabeled data.

## 3 Dataset

### Taxonomic system establishment

Figure 1 illustrates a portion of the hierarchical taxonomy system for _Species196-L_. All species included in our collection are sourced from the _Catalogue of Quarantine Pests for Import Plants to China_. With the aim to help construct an effective and efficient computer vision-based invasion biometrics system, we selected insects, weeds, and mollus that are visually observable and also easily collectable using mobile devices.

We have provided a taxonomy system that includes comprehensive hierarchical classification information within the dataset. This information spans from the domain, kingdom, and phylum levels down to the order, family, genus, and species of each invasive species. We hope that the inclusion of biological taxonomy data, incorporating prior knowledge in this field, will enable users of the Species196 dataset to explore and construct more efficient, effective, and robust invasive species identification systems.

### Image collection, filtering, split, and annotation of _Species196-L_

In this work, we utilize public available information on the internet as the source for collecting images of invasive organisms. In addition to directly using search engines, we also place a strong emphasis on exploring various global biological image repositories for image collection. Examples of the image repositories we accessed include the iNaturalist community , the Global Invasive Species Database (GISD) , BugGuide , Biocontrole , and more. These diverse and trustworthy sources provided a comprehensive and rich set of images for the _Species196-L_.

Our team of five members utilized taxonomy information to search for images using both common names and scientific names. We collected images of insects at various stages of their life cycle,

   Dataset & Year & Meta-classes & Categories & Samples & 
 Multiple \\ life-cycle \\  & Taxonomy \\  Pink-Eggs  & 2023 & Mollusca & 3 & 1261 & N & N \\ Ciona17  & 2017 & Mollusca, Chordata & 5 & 1472 & N & N \\  IP102 & 2019 & Insects & 102 & 75,222 & Y & N \\ Pest ID  & 2016 & Insects & 12 & 5,136 & N & N \\ Xie _et al._ & 2018 & Insects & 40 & 4,500 & N & N \\ Alfarisy _et al._ & 2018 & Insects & 13 & 4,511 & N & N \\ Deep Weeds  & 2019 & Weeds & 9 & 17,509 & N & N \\ Plant Seeding  & 2017 & Weeds & 12 & 5,539 & N & N \\ CNU  & 2019 & Weeds & 21 & 208,477 & N & N \\ CWD30  & 2023 & Crops, Weeds & 30 & 219,778 & Y & Y \\  Oxford Flowers  & 2008 & Plants & 102 & 8,189 & N & N \\ CUB200  & 2011 & Birds & 200 & 11,788 & N & N \\ Stanford Cars  & 2013 & Cars & 196 & 19,184 & & N \\ FGVC Aircraft  & 2013 & Arcerafs & 100 & 10,000 & N \\ iNat2017  & 2017 & Plants, Animals & 5,089 & 859,000 & Y & Y \\ iNat2021  & 2021 & Plants, Animals & 10,000 & 3,286,843 & Y & Y \\ 
**Species196** & 2023 & Mollusca, Weeds, Insects & 196 & 
 19,256 + \\ 1,200,000 (unlabeled) \\  & Y & Y \\   

Table 1: Comparison of related species recognition and fine-grained datasetssuch as eggs, larva, pupa, and adult (see Figure 3), because each stage can cause negative impacts on ecosystems and the economy. We also removed images that contained more than one invasive species category. Our dataset is a multi-grained dataset with class granularity based largely on both species and genus, and when processing categories primarily by genus, we have balanced the number of categories within each genus as much as possible to help ensure a relatively even distribution. Subsequently, we removed images with a size lower than 128x128 and those containing private information such as human faces. We collected 19,236 images for 196 invasive species. Following the approach of Stanford Cars  and CUB200 , we evenly divided our train and test sets at a 1:1 ratio, resulting in a train size of 9,676 images and a test size of 9,580 images.

In real-world pest control, precise identification and location of pests are vital. This task, often complicated by cluttered backgrounds and multiple pests in one image, aids in effective pest management. After collection and filtering, we labeled 19,236 images with 24,476 bounding boxes in the COCO format  for _Species196-L_, which took five individuals about six months.

### Challenges of fine granularity and data imbalance

_Species196-L_ share common challenges like other fine-grained datasets such as high similarity between different classes and low variability within each class. Figure 2 shows example of classes

Figure 3: Invasive insect life cycles: egg, larva, pupa and adult.

Figure 2: Display images of species belonging to the same genus or family.

belonging to the same genus, which are difficult to distinguish. Data imbalance is another challenge. As illustrated in Figure 5, although our dataset contains species from 21 different orders, the majority of them are concentrated in a few orders, such as Coleoptera, Lepidoptera, and Hemiptera.

### Data collection of _Species196-U_

Prior research has demonstrated that domain-adaptive pretraining has the potential to improve performance in both the natural language  and computer vision  fields. In recent years, the introduction of the CLIP (Contrastive Language-Image Pre-training) method  and the emergence of web-harvested image-text data like LAION5B  have provided us with the opportunity to build large-scale domain-specific data at minimal cost.

In this work, we utilize the clip-retrieval  to retrieve the large-scale _Species-U_ dataset from LAION-5B. We first evaluated various retrieval methods and discovered that image-to-image retrieval surpassed other text-to-image retrieval approaches in terms of relevance. These approaches included retrieval based on common names, scientific names, and artificial descriptions. Specifically, we randomly sampled three images for each category and retrieved 8,000 unlabeled images per class. For insects with different life cycles, we additionally sampled eggs, larvae, and pupae once for each species. After removing duplicates, we obtained a final dataset consisting of approximately 1.5 million images.

For comparison with popular experiments using ImageNet-1K, we chose 1.2 million image-text pairs from LAION-5B, a number smaller than that in ImageNet-1K.

### Retrieved Example Images from the _Species196-U_ Dataset

We use image-retrieval for creating _Species196-U_. For each category, we randomly sampled three images and retrieved 8,000 unlabeled images per class from LAION5B. As shown in Figure 4, even at the 5,000\({}_{th}\) image sorted by descending similarity scores, the retrieved image remains highly relevant to the original image.

Figure 4: Clip-retrieval process of _Species16-U_ from _Species196-L_. Displaying similarity scores in descending order, we show items No. 100, 500, 1000, and 5000.

Experiment

To benchmark _Species196-L_, we assessed methods with CNN, Transformer, or hybrid backbones of varying scales, including fine-grained models. We also experimented with unsupervised and semi-supervised methods on the _Species196-U_ dataset. Moreover, we examined zero-shot inference capabilities by testing the performance of CLIP and several large multi-modal models.

### Experiment on supervised learning methods

We first tested mainstream visual backbones and several recent fine-grained classification methods. Keeping the potential deployment of invasion biometrics systems on the edge side in mind, we specifically compared lightweight models. Detailed comparison results are presented in Table 2. As for experimental details, we utilized timm  as the codebase to evaluate different models. For networks with the same architecture and similar complexity (_e.g._, CNN-based, Transformer-based, and Hybrid structures), all models were trained for 300 epochs with the same input resolution of 224\(\)224 for fair comparison. For fine-grained classification network methods, we selected MetaFormer-2 , TransFG , and IELT , which have demonstrated promising performance on other fine-grained classification datasets CUB200 .

Our experimental results indicate that ResNet50  performs best when trained from scratch. Furthermore, on the _Species196-L_ dataset, pre-training and transfer learning significantly outperformed training from scratch for models of all sizes, particularly transformer-based and hybrid networks such as TNT-S  and CMT-S . Among the top-performing models in small, medium, and large-scale comparisons are MobileViT-XS , MaxViT-T , and MViTv2-B . MetaFormer-2  achieved impressive accuracies of 87.69\(\%\) withstand 88.69\(\%\) on the _Species196-L_ dataset, respectively. This performance was achieved through pre-training with ImageNet-1K and Imagenet-22K. MetaFormer-2  achieved impressive performance on our Species196-L dataset, reaching state-of-the-art (SOTA) accuracy of 88.69\(\%\) with a resolution of 384\(\)384 input size. It also achieved a top-1 accuracy of 87.69\(\%\) in the 224\(\)224 resolution.

We also conducted experiments on popular object detection networks, ranging from CNN-based methods such as Faster-RCNN  and YOLOX-L , to DETR -like Deformable DETR  and DINO . Using Imagenet-1k pre-trained weights, experiment results show that DINO with Swin-Base backbone achieved the best accuracy, while Deformable DETR with ResNet50 backbone attained a balance between accuracy and parameters. The comparison detail is shown in Table 3.

Figure 5: Imbalance distribution of _Species196-L_ based with order-level granularity.

### Experiment on semi-supervised and self-supervised learning methods

#### 4.2.1 Semi-supervised learning

The Noisy Student approach  leverages a teacher model to generate pseudo labels for unlabeled data, training a larger student model with both labeled and unlabeled sets. We used a similar semi-supervised method on our smaller-scale _Species196-U_ dataset compared to YFCC100M  and JFT . Following this approach, we incorporated data augmentation and dropout for noise injection in training, using a model of equal or smaller size for labeling unlabeled data, then training a larger model. Table 4 shows that using Noisy Student training on our 1.2 million unlabeled data enhances performance when the student model is larger. However, if the student network is identical to the teacher's, accuracy may decline.

#### 4.2.2 Unsupervised learning

In recent years, with methods like MAE , SimMIM  and LocalMIM , masked image modeling unsupervised learning has received increasing attention, ConvNeXt V2  and SparK  further successfully bring MIM into CNN networks. Inspired by some success works in the NLP domain (_e.g._, ), we constructed the _Species-U_ dataset for the Invasion Biometrics task and conduct an empirical study on different MIM methods(see Table 5). We found that even with less data, pretraining on our _Species-U_ dataset can surpass the Imagenet-1K pretrained models on both CNN-based and Transformer-based networks with the same or fewer pretraining epochs. The results

   Method & Resolution & \# Params. & \# FLOPs. & Top - 1 ACC. & Top - 5 ACC. & F1\({}_{MACRO}\) \\  MobileViT-XS  & 224\({}^{2}\) & 2.3 M & 0.7 G & 64.11 / 78.55\({}^{}\) & 83.51 / 91.92\({}^{}\) & 53.52 / 69.01\({}^{}\) \\ GhostNet 1.0  & 224\({}^{2}\) & 5.2 M & 0.1 G & 62.75 / 76.02\({}^{}\) & 82.58 / 90.77\({}^{}\) & 51.30 / 64.93\({}^{}\) \\ EfficientNet-B0  & 224\({}^{2}\) & 5.3 M & 0.4 G & 62.88 / 78.26\({}^{}\) & 81.66 / 91.60\({}^{}\) & 53.13 / 66.91\({}^{}\) \\ MobileNetV3 Large 1.0  & 224\({}^{2}\) & 5.4 M & 0.2 G & 62.75 / 77.83\({}^{}\) & 81.46 / 90.77\({}^{}\) & 49.99 / 66.50\({}^{}\) \\  RegNetY4GF  & 224\({}^{2}\) & 20.6 M & 4.0 G & 43.01 / 82.25\({}^{}\) & 69.02 / 93.71\({}^{}\) & 28.99 / 71.24\({}^{}\) \\ Deit-S  & 224\({}^{2}\) & 22 M & 4.6 G & 36.89 / 77.21\({}^{}\) & 56.79 / 91.52\({}^{}\) & 29.35 / 65.25\({}^{}\) \\ TNT-S  & 224\({}^{2}\) & 23.8 M & 5.2 G & 38.66 / 80.67\({}^{}\) & 59.14 / 93.17\({}^{}\) & 30.67 / 69.34\({}^{}\) \\ CMT-S  & 224\({}^{2}\) & 25.1 M & 4 G & 40.86 / 81.12\({}^{}\) & 60.10 / 93.32\({}^{}\) & 33.25 / 70.40\({}^{}\) \\ Resnet50  & 224\({}^{2}\) & 25.6 M & 4.1 G & 64.32 / 78.11\({}^{}\) & 81.70 / 91.91\({}^{}\) & 53.31 / 67.29\({}^{}\) \\ Swin-T  & 224\({}^{2}\) & 28 M & 4.5 G & 46.88 / 81.66\({}^{}\) & 68.57 / 93.52\({}^{}\) & 37.30 / 71.20\({}^{}\) \\ Convneval-T  & 224\({}^{2}\) & 29 M & 4.5 G & 46.36 / 78.94\({}^{}\) & 68.59 / 92.44\({}^{}\) & 37.16 / 70.43\({}^{}\) \\ MaxViT-T  & 224\({}^{2}\) & 31 M & 5.6 G & 52.19 / 83.35\({}^{}\) & 72.12 / 94.16\({}^{}\) & 42.40 / 62.56\({}^{}\) \\  MViT-B  & 224\({}^{2}\) & 52 M & 10.2 G & 46.22 / 83.79\({}^{}\) & 66.21 / 94.81\({}^{}\) & 35.83 / 72.94\({}^{}\) \\ Resnet200-D  & 224\({}^{2}\) & 65 M & 26 G & 51.35 / 82.11\({}^{}\) & 73.07 / 94.96\({}^{}\) & 37.70 / 70.61\({}^{}\) \\ VIT-B/32  & 224\({}^{2}\) & 86 M & 8.6 G & 32.59 / 74.68\({}^{}\) & 53.76 / 89.76\({}^{}\) & 25.20 / 63.38\({}^{}\) \\ Swin-B  & 224\({}^{2}\) & 88 M & 15.4 G & 48.72 / 82.88\({}^{}\) & 69.71 / 94.30\({}^{}\) & 39.28 / 72.04\({}^{}\) \\ Pyramid ViG-B  & 224\({}^{2}\) & 82.6 M & 16.8 G & 61.59 / 82.82\({}^{}\) & 82.93 / 91.96\({}^{}\) & 34.27 / 72.40\({}^{}\) \\  MetaFormer-2  & 224\({}^{2}\) & 81 M & \(-\) & 87.69\({}^{}\) & \(-\) \\ MetaFormer-2  & 384\({}^{2}\) & 81 M & \(-\) & 88.69\({}^{}\) & \(-\) \\ TransFG  & 224\({}^{2}\) & 85.8 M & \(-\) & 84.42\({}^{}\) & \(-\) \\ ILT  & 448\({}^{2}\) & 93.5 M & \(-\) & 81.92\({}^{}\) & \(-\) \\   

Table 2: Comparison of different modern backbones and fine-grained methods. \({}^{}\) and \({}^{}\) donates using Imagenet-1K, Imagenet-22K pretrained weight, separately. The rest are trained from scratch.

   Methods & Backbone & \#Params. & AP & AP\({}_{S}\) & AP\({}_{M}\) & AP\({}_{L}\) \\  Faster-RCNN  & Resnet50  & 40 M & 44.7 & 5.2 & 24.9 & 46.0 \\. YOLOK-L  & CSPNet  & 54 M & 50.3 & 9.9 & 31.0 & 51.3 \\ Deformable DETR  & Resnet50  & 40 M & 56.9 & 10.8 & 36.9 & 58.3 \\ DINO  & Resnet50  & 47 M & 57.7 & 9.1 & 40.0 & 59.2 \\ DINO  & Swin-Base  & 109 M & 67.7 & 19.1 & 46.5 & 69.6 \\   

Table 3: Comparison of average precision performance of object detection methods. IoU threshold range of 0.5 to 0.95.

also indicate that models with stronger hierarchical structures, such as traditional convolutional networks and Swin Transformers, tend to benefit more from pretraining on the _Species-U_ dataset.

### Experiment on multimodal large language models

Recently, there has been a surge of interest in the field of Multimodal Large Language Models (MLLM) [93; 82; 82; 45; 95]. These models leverage the ability of Large Language Models (LLMs) to perform a variety of multimodal tasks effectively. However, we discovered that there are few existing benchmarks that can assess the MLLM's ability to handle fine-grained knowledge, let alone test its performance across various levels of granularity.

We designed question-and-answer tasks using images from _Species196-L_, which included both multiple-choice and true or false questions, and evaluated them across 9 different Multimodal Large Language Models (MLLM) [17; 73; 25; 49; 92; 50; 96; 9; 56]. Our benchmark design is based on six different levels of taxonomy information, ranging from coarse to fine granularity, including Phylum, Class, Order, Family, Genus, and Species (scientific name). Each category consists of 1000 image-based questions. For the design and evaluation metrics of true or false questions, we followed the approach of . For each image, we created two true-false questions, one of which is correct and the other is a distractor question. We evaluated the true or false questions using both accuracy and accuracy+ metrics. The accuracy+ metric is more stringent, requiring both questions for each image to be answered correctly in order to consider the answer correct. Figure 6 shows our example question settings.

In terms of experimental design, we tested all models at the 7B scale across different tasks. The results in Table 6 indicate that InstructBLIP  achieved the best performance on the most true of false questions, however, its ACC+ score still lower tha random accuracy 25% for some categories. In five of all six categories of multiple-choice questions, Multimodal-GPT has achieved the highest accuracy, thereby leading the leaderboard (See Table 7).

   Teacher & Teacher Acc. & Student & Student Acc. \\  ResNet18 & 71.1 & ResNet18 & 70.6 \\  ResNet18 & 71.1 & ResNet34 & **73.4** \\  ResNet34 & 72.3 & ResNet34 & 72.0 \\   

Table 4: Comparison using a student model with the same size or with a larger size. Student Acc. represents the top-1 accuracy of the student network at the end of the last iteration.

   Backbone & PT method & PT data & PT epoch & FT epoch & Top - 1 ACC. & Top - 5 ACC. \\   \\   & SimMIM  & ImageNet-1K & 800 & 100 & 80.9 & **94.9** \\  & SimMIM  & _Species-U_ & 800 & 100 & **81.0** & 94.6 \\   & SimMIM  & ImageNet-1K & 800 & 100 & 80.5 & 94.5 \\  & SimMIM  & _Species-U_ & 800 & 100 & **81.6** & **95.0** \\   \\   & SparK  & ImageNet-1K & 1600 & 300 & 73.2 & 88.6 \\  & SparK  & _Species-U_ & 800 & 300 & 73.2 & **88.9** \\   & FCMAE  & ImageNet-1K & 1600 & 300 & 77.1 & 92.6 \\  & FCMAE  & _Species-U_ & 1600 & 300 & **79.3** & **93.1** \\   & FCMAE  & ImageNet-1K & 1600 & 300 & 79.9 & 93.1 \\  & FCMAE  & _Species-U_ & 800 & 300 & **81.2** & **94.0** \\   

Table 5: Comparison of different model architecture with different MIM methods. PT\(/\)FT donates pre-train and fine-tune stage.

In our experiments, we observed that large language models commonly exhibit issues such as not answering prompts accurately and generating hallucinations (See Appendix Table 8). The answers generated by large models often fail to conform to the expected response on judgment and multiple choice tasks, resulting in low accuracy scores. Furthermore, we found that these models tend to display a bias towards answering "yes" on true or false tasks, which leads to accuracy and accuracy+ scores that are below random guess.

## 5 Conclusion

In this work, we introduced Species196, a fine-grained dataset of 196-category invasive species, consisting of over 19K finely annotated images (_Species-L_) and 1.2M unlabeled images of invasive species (_Species-U_), making it a large-scale resource for invasive species research. Compared to existing invasive species datasets, Species196 covers a wider range of species, considers multiple growth stages, and provides comprehensive taxonomic information. We also conduct comprehensive experiments bench-marking for supervised, semi-supervised and self-supervised methods, and also multi-modal models. Our experiments shows that unsupervised pre-training like masked image modeling on _Species-U_ leads to better performance compared to ImageNet pretraining. In future work, we plan to investigate additional methods for leveraging unlabeled data from Species196 and broaden the application of this approach to tackle data scarcity challenges in various real-world applications.

    &  &  &  &  &  &  &  \\   & ACC(\%) & ACC(\%) & ACC(\%) & ACC(\%) & ACC(\%) & ACC(\%) & ACC(\%) & ACC(\%) & ACC(\%) & ACC(\%) & ACC(\%) & ACC(\%) & ACC(\%) & ACC(\%) \\  baseline  & **50.0** & **19.2** & **44.9** & **31.0** & **49.5** & **34.1** & **45.3** & **12.3** & 47.1 & **85.8** & 92.3 & **17.9** & **54.1** & **91.8** \\ LLVA  & 50.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.1 & 50.1 & 0.3 & **50.8** & 6.2 & **53.8** & 12.1 & 50.7 & 3.1 \\ PradeGPT  & 50.2 & 0.4 & 51.6 & 2.2 & 50.0 & 0.0 & 50.2 & 0.4 & 50.2 & 0.4 & 50.0 & 0.0 & 50.4 & 0.8 \\ PradeGPT  & 52.7 & 11.6 & 52.8 & 22.6 & 48.6 & 7.5 & 50.0 & 9.1 & 47.8 & 11.5 & 45.4 & 10.3 & 49.6 & 10.4 \\ Visual  & 47.4 & 8.1 & 45.7 & 2.8 & 46.8 & 5.7 & 48.5 & 6.5 & 48.2 & 7.0 & 47.3 & 5.0 & 47.3 & 5.4 \\ Our  & 48.5 & 0.0 & 0.4 & 0.0 & 48.3 & 0.0 & 49.0 & 0.6 & 43.3 & 0.1 & 40.6 & 1.9 & 46.5 & 0.4 \\ Multimodal GPT  & 94.1 & 9.1 & 38 & 9.7 & 27.7 & 8.1 & 34.0 & 9.5 & 35.1 & 3.9 & 39.2 & 15.0 & 26.1 & 10.1 \\ MetaPPT  & 22.4 & 7.7 & 23.4 & 7.0 & 24.1 & 7.2 & 35.8 & 3.0 & 20.2 & 6.3 & 22.5 & 8.4 & 23.7 & 7.4 \\ Huip  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\   

Table 6: Leaderboard of true or false questions of _Species-L_ multimodal benchmark.

   Models & Phylum & Class & Order & Family & Genus & Species & Avg Acc \\  Multimodal-GPT  & **51.8** & **71.6** & **60.6** & 56.6 & **57.9** & **63.2** & **60.3** \\ InstructBLIP  & 47.8 & 58.7 & 56.3 & **57.5** & 45.3 & 39.8 & 50.9 \\ PandaGPT  & 53.0 & 44.1 & 42.6 & 52.8 & 38.6 & 34.6 & 44.3 \\ mPLUG-Owl  & 34.1 & 32.1 & 43.0 & 39.2 & 31.0 & 24.9 & 34.1 \\ MiniGPTA  & 28.6 & 32.7 & 32.1 & 28.2 & 29.9 & 32.7 & 30.7 \\ LLVA  & 38.1 & 34.2 & 17.3 & 33.4 & 22.2 & 23.4 & 28.1 \\ Blip2  & 26.7 & 30.3 & 23.3 & 27.9 & 23.9 & 24.5 & 26.1 \\ Visual-GLM6B  & 23.0 & 12.2 & 13.9 & 30.5 & 15.7 & 11.6 & 17.8 \\ Otter  & 0.0 & 6.8 & 20.8 & 8.3 & 6.7 & 0.3 & 7.15 \\   

Table 7: Leaderboard of multiple choice questions of _Species-L_ multimodal benchmark.

Figure 6: Example of our questions for multimodal large language models.