# PV-Tuning: Beyond Straight-Through Estimation

for Extreme LLM Compression

 Vladimir Malinovskii\({}^{}\)

Yandex, HSE University

Denis Mazur\({}^{}\)

MIPT\({}^{}\), SberDevices\({}^{@paragraphsign}\)

AI Initiative, KAUST\({}^{*}\)

Denis Kuznedelev

Yandex, Skoltech

Konstantin Burlachenko

AI Initiative, KAUST\({}^{*}\)

Kai Yi

AI Initiative, KAUST\({}^{*}\)

Dan Alistarh\({}^{}\)

IST Austria, NeuralMagic

AI Initiative, KAUST\({}^{*}\)

###### Abstract

There has been significant interest in "extreme" compression of large language models (LLMs), i.e., to 1-2 bits per parameter, which allows such models to be executed efficiently on resource-constrained devices. Existing work focused on improved one-shot quantization techniques and weight representations; yet, purely post-training approaches are reaching diminishing returns in terms of the accuracy-vs-bit-width trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include fine-tuning (part of) the compressed parameters over a limited amount of calibration data; however, such fine-tuning techniques over compressed weights often make exclusive use of _straight-through estimators (STE)_, whose performance is not well-understood in this setting. In this work, we question the use of STE for extreme LLM compression, showing that it can be sub-optimal, and perform a systematic study of quantization-aware fine-tuning strategies for LLMs. We propose PV-Tuning -- a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases. On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral. Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama-2 family models at 2 bits per parameter.

## 1 Introduction

Recent years have seen the development of ever more capable large language models, attracting immense interest from both researchers and industry. One of the driving factors behind progress in this area is the availability of powerful **open** LLMs such as Llama , Mistral , or Phi . The main advantage of open LLMs is that they can be run and fine-tuned locally by end users; however, as state-of-the-art LLMs grow larger, they also become harder to run on commodity hardware. For instance, in order to fit the best available Llama-3 model on a consumer GPU, the model would have to be compressed to below 2 bits per parameter1.

To achieve such "extreme" degrees of compression accurately, researchers have proposed a variety of techniques, which can be roughly categorized into i) better quantized weight representations and ii) better algorithms to learn these representations. The weight representations used for extreme quantization include group quantization , sparse high-precision outliers , incoherence processing of the weights , or additive and residual quantization . In turn, the calibration algorithms also vary between data-free methods , layer-wise calibration , block-wise or global fine-tuning  or even quantization-aware training . However, theweight representation and the fine-tuning algorithm are largely orthogonal: most popular quantized representations could be obtained layer-wise in one-shot, fine-tuned layer-wise to a variety of optimization objectives, or even trained entirely from scratch.

Surprisingly, there is a clear disparity between the degree of interest shown to accurate one-shot2 quantization versus accurate fine-tuning. Specifically, one-shot quantization is very well-studied, to the extent that, as shown in Figure 2, improvements in this direction are clearly saturating. At the same time, the impact of fine-tuning strategy is largely unknown: while many recent works use some form of fine-tuning , they typically consider a single fine-tuning regimen based on straight-through estimation (STE) . Thus, given the multitude of representations considered, it is not at all clear whether current fine-tuning strategies are optimal.

In this work, we analyze the problem of fine-tuning over highly-compressed weights from the optimization perspective. We begin by analyzing popular fine-tuning strategies for extreme LLM quantization. The key challenge in this context is that the quantized representations may contain both continuous and discrete variables: while continuous parameters, such as learnable scales or codebooks, can be optimized by backpropagation, the discrete parameters (e.g., integer assignments for the weights) cannot. Existing fine-tuning techniques either do not optimize over discrete parameters at all  or fine-tune them using heuristics such as STE or stochastic rounding . Unfortunately, these methods are not well-justified for weight quantization from the point of view of optimization theory, and, as we show in Section 3, can provide poor practical performance.

We propose an alternative solution: instead of following heuristic gradient estimates, our approach follows the actual gradient of the objective in a small subspace of optimized parameters where it can be meaningfully improved. Following this insight, we formulate the _PV-tuning framework_ for fine-tuning arbitrary quantized representations. We update both discrete and continuous components to minimize a global objective function, such as the KL divergence relative to the original model predictions. Our results show that this strategy leads to significant improvements across weight representations, achieving new state-of-the-art in compression-accuracy trade-offs.

The main contributions of our work can be summarized as follows:

1. We analyze the problem for training discrete quantized representations for better understanding of the limitations of existing optimization algorithms. We then propose a novel algorithm inspired by compressed gradient methods that addresses these limitations. When compared to straight-through estimation and stochastic rounding, our approach 1) can be shown to converge to a stable solution; and 2) this solution is significantly more accurate in practice.
2. We generalize the proposed algorithm into the PV-Tuning framework3, which can minimize a global objective function over a general quantized representation, by optimizing both continuous and discrete parameters via a variant of coordinate descent. 3. We demonstrate that PV-tuning can improve quantized model accuracy for leading existing approaches, including GPTQ and AQLM, on popular LLMs including Llama-2 & 3 and Mistral. Our procedure achieves state-of-the-art accuracy (measured through perplexity) in 1- and 2-bit quantization regimes while using the same amount of calibration data as the original algorithms. Importantly, the PV-tuned models use the same underlying weight representations, and are compatible with existing inference kernels. In terms of accuracy per model size, PV-tuning of vector quantization outperforms all prior techniques in the 1-3 bits/parameter range, and is the first to achieve Pareto-optimal quantization for Llama-2 models at around 2 bits per parameter.

Figure 1: WikiText-2 perplexity (left) and average zero-shot accuracy (right) of 2-bit quantized Llama 2 models as a function of model size (GiB). See detailed setup in Section 4.3.

Background

**Post-Training LLM Quantization (PTQ).** There has been significant interest in PTQ methods [49; 25] that would scale to LLMs. Early work [17; 78; 50] used direct round-to-nearest (RTN) quantization over weight groups of well-chosen size. GPTQ  improved upon these results significantly via an accurate one-shot solver for minimizing layer-wise compression errors. Next, AWQ  improved upon these results by employing per-channel scaling to reduce the error on important weights while SqueezeLLM  implemented non-uniform quantization. QuIP  proposed a more accurate weight representation by leveraging incoherence matrices. Another line of works [18; 39] proposes an improved quantized weight representation, which saves a small fraction of outliers in full precision. Other recent works propose augmenting quantized representations with lowrank "adapters" that compensate quantization error [28; 84]. Recently, BiLLM  developed residual binarization that stores salient weights in progressively higher bitwidth, quantizing models to nearly 1 bit per parameter at non-catastrophic accuracy loss.

Currently, the state-of-the-art methods in terms of accuracy-vs-size are QuIP#  and AQLM . Both methods work roughly by mapping weight groups to points on highly-dimensional lattices, which are either chosen to satisfy some optimality properties (for QuIP#) or are learned (for AQLM). Interestingly, AQLM showed that fine-tuning the continuous parameters (codebooks) can improve accuracy significantly relative to pure one-shot compression; a variant of this approach was also adopted by QuIP#. PV-Tuning is compatible with both methods: as we show, it can lead to state-of-the-art compression results for such representations.

**Fine-tuning over Quantized Weights.** As mentioned above, the two SOTA quantization techniques apply fine-tuning, but only update _continuous_ parameters, such as quantization scales. When optimizing over _discrete_ parameter sets, a standard choice in deep learning is the Straight-Through Estimator (STE) [6; 15; 73]. Prior work on LLM compression proposed to update both continuous and discrete parameters, via STE, both for post-training quantization [78; 63] and for training quantized networks from scratch . However, it was observed early on that STE leads to instability when fine-tuning heavily quantized LLMs . While early results suggest that STE can perform well when training quantized models from scratch , this behavior is yet to be validated for highly-performant multi-billion-parameter models, which are the focus of our work.

In summary, the two standard approaches for fine-tuning quantized LLMs are 1) fine-tuning only over the continuous parameters, such as quantization scales, which heavily limits the number of trainable parameters; and 2) optimizing all parameters via the STE, which however is known to be quite noisy especially for extreme quantization. In this context, our work proposes alternative approaches in the post-training compression setting, which lead to state-of-the-art results relative to both options.

## 3 Fine-Tuning Quantized Models

In this section, we study the problem of fine-tuning quantized models to minimize a global objective, such as cross-entropy. Section 3.1 formulates this problem from an optimization perspective and introduces our notation. In Section 3.2, we analyze several popular strategies for solving this problem and highlight some of their limitations. To circumvent these limitations, we propose an alternative optimization algorithm in Section 3.3 and discuss implementation details in Section 3.4.

### Problem description

Consider the problem of minimizing objective (loss) \(\),

\[_{x^{d}_{c}}(x),\] (1)

where \(:^{d}\) is a differentiable function bounded from below (e.g., by zero), and \(^{d}_{c}^{d}\) is a set of all possible quantized weights that can be represented with a given quantization method. Without loss of generality4, we first analyze the case of scalar nonlinear quantization. In this scenario, \(c[d]:=\{1,2,,d\}\) (typically \(c d\)), and \(^{d}_{c}^{d}\) is the set of all vectors in \(^{d}\) whose \(d\) entries take exactly \(c\) distinct values. In other words, the cardinality of the set \(V(x):=\{x_{1},,x_{d}\}\) is equal to \(c\), and we can therefore write \(^{d}_{c}:=\{x^{d}\ :\ |V(x)|=c\}\).

**Useful notation.** A vector \(x_{c}^{d}\) naturally induces a partition, which we shall call \(P(x)\), of the set \(\{1,,d\}\) into \(c\) nonempty subsets \(P_{1}(x),,P_{c}(x)\) characterized by

\[x_{i}=x_{j} k\ :\ i P_{k}j P_{k}.\]

Let's denote \(P(x):=\{P_{1}(x),,P_{c}(x)\}\). Moreover, we shall write \(P(y) P(x)\) if each element of \(P(x)\) is a subset of some element of \(P(y)\). For distinct \(i,j[d]\), let us introduce the notation \(_{ij}(x)=1\) if there exists \(k\) such that \(i,j P_{k}(x)\), and \(_{ij}(x)=0\) otherwise. Given this notation, notice that \(P(y) P(x)\) if and only if for all \(i j\) we have \(_{ij}(x)=1 y_{i}=y_{j}.\) Finally, we define \(_{ c}^{d}:=_{1}^{d}_{c}^{d}\) as the set of all vectors in \(^{d}\) whose \(d\) entries take at most \(c\) distinct values. So, if \(x_{c}^{d}\) and \(P(y) P(x)\), then \(y_{ c}^{d}\).

**PV method.** Following this notation, we define an optimization algorithm that alternates between optimizing \(\) with fixed \(P\) or fixed \(V\). From a practitioner's point of view, these represent optimizing continuous parameters (scales, codebooks, zeros) and discrete codes (assignments), respectively.

\(\)**The P step (fixing \(P\)).** Given \(x_{c}^{d}\), consider the mapping

\[M_{P}(x)=M_{P,}(x):=_{y^{d}}\{(y)\,:\,P(y)  P(x)\}.\] (2)

Notice that, necessarily, \(M_{P}(x)_{ c}^{d}\) and \((M_{P}(x))(M_{P}(x))(x).\) Evaluating \(M_{P}\) amounts to solving an unconstrained optimization problem in a \(c\)-dimensional space.

\(\)**The V step (fixing \(V\)).** Similarly, given \(y_{c}^{d}\), we define the mapping

\[M_{V}(y)=M_{V,}(y):=_{x^{d}}\{(x)\,:\,V(x) V (y)\}.\] (3)

Likewise, \(M_{V}(y)_{ c}^{d}\) and \((M_{V}(y))(M_{V}(y))(y).\) Evaluating \(M_{V}\) amounts to solving difficult discrete optimization problems with a search space of size \(|V(x)|^{d} c^{d}\) (exponential in \(d\)).

```
1:Initialization: starting point \(x^{0}_{ c}^{d}\)
2:for\(k=0,1,\)do
3:\(y^{k}=M_{P}(x^{k}):=_{y^{d}}(y):P(y) P (x^{k})}\) (P step: continuous)
4:\(x^{k+1}=M_{V}(y^{k}):=_{x^{d}}(x):V(x) V (y^{k})}\) (V step: discrete)
5:endfor ```

**Algorithm 1** PV algorithm

Our key algorithmic idea, in its simplest form, is to optimize \(\) by alternating the P and V steps, i.e., iteratively applying the \(M_{P}\) and \(M_{V}\) operators. (We will propose several more practically-useful approximations and variations later; see Sections 3.2-3.3 and also Appendix B.) This resulting method, which we call the PV method, is formalized as Algorithm 1. Our key guarantee for the PV method is formalized in the next result.

**Theorem 3.1** (Convergence of the PV method).: _Assume \(\) is bounded below, and let \(x^{0}_{c}^{d}\). Then (i) \(y^{k}_{ c}^{d}\) and \(x^{k}_{ c}^{d}\) for all \(k 0\); (ii) \((x^{k+1})(y^{k})(x^{k})\) for all \(k 0\); and (iii) the sequence \(\{(x^{k})\}_{k 0}\) converges._

The proof can be found in Appendix A.1. Note that we do not claim that the method converges to a minimizer of \(\); the optimization problem is too difficult for us to be able to guarantee this. However, as we shall see in the numerical results, we nevertheless obtain great empirical performance, especially when coupling the PV approach with some additional algorithmic tricks.

This general approach is popular in "shallow" machine learning problems; for instance, if \((x)=\|x-z\|^{2}\) is the squared error with respect to some user-specified vector \(z\), then the above algorithm recovers \(1\)-dimensional \(K\)-means on the data vector \(z\). Likewise, if \(()\) is the log-likelihood, then, depending on the choice of the set \(_{c}^{d}\), the approach is related to the EM algorithm .

In turn, we apply the PV method to obtaining highly-accurate quantized LLMs. Applying the PV method "as is", would be infeasible in practice: computing the P and V mappings requires solving difficult optimization problems especially due to LLM parameter scales. However, both mappings can be approximated. The P step can be reparameterized as an unconstrained optimization problem on theunique values in the weight matrix. Practically it means that the "codebooks" can be optimized using an automated differentiation engine (i.e. PyTorch). However, for many quantized representations, \(M_{P}(x)\) can be approximated by one or more steps of GD, directly optimizing \(\) over the set \(V(x)\) of its \(c\) unique values. The \(c\)-dimensional gradient can be computed efficiently by backprop, as described in prior works . On the other hand, the V step (\(M_{V}()\)) is more difficult to approximate as it involves searching a discrete space of size \(c^{d}\). We dedicate the next two sections to this task.

### Linearized V step & gradient-based discrete updates

The V mapping (3) can be approximated by solving a discrete least squares problem using an approximation of \((x)\) around \(y\):

\[(x)_{y}(x):=(y)+(y),x-y +^{2},\] (4)

where \(L>0\) is a sufficiently large constant. Subsequently, we perform the V step using the simpler convex quadratic function \(_{y}\) instead of the typically more complicated function \(\):

\[M_{V,}(y))}{}M_{V,_{y}}(y) )}{=}_{x^{d}}\{_{y}(x)\ :\ V(x) V(y)\}.\]

Our first lemma shows that we can replace \(_{y}\) by a more convenient function \(_{y}\) measuring the squared distance between \(x\) and \(y^{+}:=y-(y)\), the latter being the point obtained after taking a single GD step from \(y\) with learning rate \(\), disregarding the constraint:

**Lemma 3.2**.: _For any \(y^{d}_{ c}\) we have \(M_{V,_{y}}(y)=M_{V,_{y}}(y),\) where_

\[_{y}(x):=(y))}^{2}= }^{2}=_{i=1}^{d}(x_{i}-y_{i}^{+})^{2}.\] (5)

The proof can be found in Appendix A.2. To summarize, the V step of the PV method (Algorithm 1), i.e., \(x=M_{V,}(y),\) can be approximated via the "linearized V step"

\[x:=M_{V,}(y) M_{V,_{y}}(y):=.\] (6)

Our next lemma says that the above approximation is in a certain sense natural reasonable provided that \(\) is \(L\)-smooth5 on \(^{d}_{ c}\), i.e., provided that

\[(x)(y)+(y),x-y+^{2}, x,y^{d}_{ c}.\] (7)

**Lemma 3.3** (Monotonicity).: _Let \(y^{d}_{ c}\). If \(\) is \(L\)-smooth on \(^{d}_{ c}\), then \((M_{V,}(y))()(y)\), where \(\) is the point obtained from \(y\) by the linearized V step (6)._

Indeed, the point \(\) obtained via the linearized V step can not have a worse loss than the previous point \(y\). Of course, one hopes that the loss will strictly decrease so that the method makes progress. From a practical perspective, the key advantage of linearized V step is that it can be performed much faster compared to the vanilla V step. The proof of Lemma 3.3 can be found in Appendix A.3.

Note that since \(_{y}(x)\) is separable (see (8)), each entry/weight of \(x\) can be optimized independently of others. For scalar quantization, each individual problem can be solved in \((_{2}(c))\) time using binary search in sorted version of \(V(y)\). For vector quantization, there are specialized optimization procedures for efficiently minimizing the \(L_{2}\) error (see Appendix D)

**Key challenge.** The main caveat with linearized V step is that it may be impossible to make small gradient-based updates to low-bitwidth discrete weights. More specifically, in (6), one must update the discrete assignments to approximate \(y^{k}-(y^{k})\). However, for low-bit weights, the desired update \((y^{k})\) can be smaller than the lowest possible increment to obtain a quantized vector. As a result, the optimal solution to (6) is often \(y^{k}\) itself. In such a situation, the algorithm will get stuck on \(y^{k}\), which is undesirable. This problem is especially pronounced in deep LLMs, where \(L\) can be very large, or, from a practitioner's point of view, where one needs a small learning rate. In practice, as we explore in Section 4.2, the lowest learning rate where the algorithm makes _any_ updates at all is already too large for optimization, leading to divergence.

```
0: initial parameters \(x^{0}^{d}_{c}\), objective function \(:^{d}\), subspace size \([d]\)
1:for\(k=0,,K-1\)do
2:\(\) P step: update \(V(x)\) by backprop
3:\(y^{k}=^{d}_{c}}{*{arg\,min}}\{(y)\ :P(y) P(x^{k})\}\)
4:\(\) V step: choose a subspace \(^{k}\) & update \(P(x)\)
5:\(^{k}=}\ |_{i}(y^{k})|\ \) find \(\) largest
6:\(_{y,S^{k}}(x):=\|x-(y-}}Z^{k}( (y)))\|^{2}\)
7:\(x^{k+}*{arg\,min}_{x}\{_{y^{k}, ^{k}}(x)\ :\!V(x)\!\!V(y^{k})\}\)
8:endfor ```

**Algorithm 2** PV-Tuning: Optimization

Many popular strategies for discrete fine-tuning can be seen as attempts to reconcile coarse low-precision weights with the need to make small updates. These include straight-through estimation, stochastic rounding, or adding regularizers that push the solution to (6) away from \(y^{k}\). We review straight-through estimation in Appendix E.1 and stochastic rounding in Appendix E.2.

### Linearized subspace V step

Here we ask the following question: **Can we modify the PV method so as to force the V step to make a larger update?** In other words, we need an optimization algorithm that updates quantized weights either by a sufficiently large increment, or not at all.

A natural example of such an algorithm is coordinate descent (CD) [43; 58], or more generally, subspace descent [26; 38]. Instead of updating all parameters by a small margin, CD in each iteration chooses a single parameter, and makes a large update instead. This strategy can be generalized to updating more parameters at the same time, which leads to subspace descent methods.6 The parameters to be updated can be chosen either greedily, (e.g., several \(i[d]\) with the largest magnitude of the partial derivative \(|_{i}()|\)), or at random, or through a variety of other means.

Let \(^{k}[d]\) be the set of parameters/weights/coordinates we wish to update at iteration \(k\). We choose \(|^{k}|= d\). Let \(Z^{k}:^{d}^{d}\) be the linear mapping defined as follows: \((Z^{k}(x))_{i}=x_{i}\) if \(i^{k}\) and \((Z^{k}(x))_{i}=0\) if \(i^{k}\). We now formulate the linearized _subspace_ V step:

\[x^{+}:=M_{V,_{y,S^{k}}}(y):=_{x^{d}}\{ _{y,^{k}}(x)\ :\ V(x) V(y)\},\]

\[_{y,^{k}}(x):=\|x-(y- {1}{L_{S^{k}}}Z^{k}((y)))\|^{2},\] (8)

and \(L_{S^{k}}>0\) is a smoothness parameter of \(\) associated with the subspace spanned by the parameters belonging to \(^{k}\). This detail is important because \(L_{S^{k}} L\) when \( d\). _When estimating Lipschitz constants for real LLMs, we found that it is lower by at least one order of magnitude_, making it possible to train with sufficiently large step sizes (see details and \(L_{S^{k}}\) estimates in Appendix F).

Note that, necessarily, \(x_{i}^{+}=y_{i}\) for \(i^{k}\). The remaining \(\) entries of \(x^{+}\) can be identified exactly by searching a discrete space of size \(|V(y)|^{}\), which is feasible if \(c=(1)\) and \(=(1)\), for example.

In practice, it means that _the algorithm can apply large updates to quantized LLM weights, with the caveat that should only update a fraction of them at a time_. This allows us to perform the linearized V step with sufficiently large "learning rate" to make non-trivial (i.e., \(x^{k+1} y^{k}\)) improvements to quantized weights even without straight-through estimation or stochastic rounding.

We formulate the full procedure in Algorithm 2. The algorithm performs the P step by directly optimizing \(V(x)\) (i.e., codebooks) by backprop as described in Section 3.1. For the V step, the algorithm greedily chooses a subset of \(\) quantized weights for update, then updates them using Eq. (8). The \(*{arg\,top}\) operator finds \(\) indices with the largest absolute gradient values and builds a subspace of \(^{d}_{ c}\) where only these values can be changed, and the rest must be equal to \(y^{k}\).

### Implementation details

To speed up convergence, we use adaptive learning rates for both P and V steps. In Eq. 8, we replace \((y)\) with a single Adam  update, as depicted in Algorithm 3. In preliminary experiments, we found that this results in a significant convergence speedup. When choosing the subspace \(^{k}\), we select weights based not on \(|_{i}(y)|\), but on the magnitude of Adam update for that weight. For simplicity, we greedily choose the \(\) weights with the largest update norm within each weight matrix.

This could be further improved through better techniques for choosing \(^{k}\) explored in Appendix Q. We also found that, despite the fact that PV-tuning by itself outperforms straight-through estimation, we could achieve slightly better accuracy by combining PV-tuning with straight-through estimation. We explore this in more detail in Section 4.2).

We describe our approach for preparing the calibration data in Appendix G. We found that the preprocessing used in several recent PTQ works introduce a small bias when sampling the calibration data, leading to somewhat worse fine-tuning accuracy. For fairness, we always compare representations (Section 4.1) and algorithms (Section 4.2) using the same pre-processing.

**Fine-tuning efficiency.** The most compute-intensive part of PV tuning is computing the gradients \(()\), which is done through repeated forward and backward passes on an LLM. To reduce the number of gradient accumulations, we reuse gradients for P and V steps within one iteration. We use mixed precision, gradient checkpointing and batch accumulation to train more efficiently; for larger LLMs such as Llama 3 70B we also use sharding and optimizer offloading (see Appendix H). Our code can train 7B LLMs on a single GPU, while larger ones (e.g. 70B) fit into a single machine with 8\(\)A100. In terms of wall-clock time, PV-tuning takes up to 1.5\(\) longer than the fine-tuning procedure of  and requires additional memory in order to hold \((x)\).

## 4 Experiments

### Evaluating quantized representations with finetuning

Before evaluating PV-tuning, we need to choose the quantized representation to be fine-tuned. We therefore compare popular weight representations from recent works on LLM quantization (see Section 2). To better isolate the effect of the weight representation, we evaluate them in three configurations: i) when quantizing a single LLM layer, in terms of MSE, ii) full model quantization in terms of perplexity without finetuning and iii) with finetuning.

We compare several recently proposed quantized representations (see details in Appendix J):

1. **GPTQ:** scalar uniform quantization with channel-wise and block-wise scales ,
2. **SpQR:** an extension of block-wise GPTQ with learned sparse outliers ,
3. **VQ:** basic vector quantization with a single codebook  with multi-step training.
4. **AQLM:** additive vector quantization with multiple learned codebooks ,
5. **QuIP#:** vector quantization with lattices and incoherence processing ,
6. **VQ/AQ + outliers:** vector/additive quantization with sparse outliers via pruning [66; 8],
7. **VQ/AQ + lowrank:** vector/additive quantization with Low-Rank Compensation (LoRC) ,

Figure 2: **(left)** L2 errors for 17th layer of Llama 2 7B with different representations. Full model perplexity on WikiText-2 is reported without finetuning **(middle)** and with fine-tuning **(right)**.

We run all three experiments on Llama 2 7B model , calibrating on the RedPajama  dataset that best approximates the original pre-training data. When evaluating single layer errors, we report the L2 error in attention query projection outputs of a fixed transformer block, with other blocks exhibiting similar behavior. For full model evaluation, we report quantized model perplexity on WikiText-2  dataset. We use the same data splits and preprocessing as in most recent PTQ works [22; 42; 18; 70; 21; 71], including the biased preprocessing step that we mentioned in Section 3.4. For fine-tuning, we train continuous parameters only, using the approach from . To compare these diverse representations, we evaluate their quantization errors as a function of average number of bits per parameter. To get a diverse set of bits per parameter, we vary the hyperparameters such as wbits, block size, codebook and group size for vector quantization and the rate of outliers.

Figure 2 summarizes our findings. Overall, vector quantization methods (VQ, QuIP# and AQLM) outperform their scalar counterparts. Outliers and low-rank compensation both reduce error, but this improvement comes at the cost of extra bits per parameter. Interestingly, _the improvement from outliers is significantly smaller when both methods have access to fine-tuning_. Likewise, the improvement from using low-rank adapters also diminishes when comparing fine-tuned models, to a point where it no longer justifies the increase in model size. We provide a more detailed breakdown of results and hyperparameter configurations in Appendix J.

Our main takeaway is that for sub 2 bits per parameter, the vector quantization (VQ) representation can achieve near-optimal quantization accuracy, whether or not it uses outliers, LoRC or incoherence processing. Naturally, this does not reduce the value of prior works since they were designed for different scenarios, typically with a higher number of bits per parameter.

### Evaluating Fine-tuning Algorithms

Next, we compare different fine-tuning strategies and ablate our PV-tuning protocol. We design our protocol to be representation-agnostic, i.e. compatible with different quantized representations. To showcase this, we pick three methods from the previous section: GPTQ, VQ and AQLM.

These methods differ not only in their weight representations, but also in how they search for the optimal codes. Namely, GPTQ can scale the target weight and round it to nearest 2-bit integer. In turn, VQ quantizes weights as a group and must find the nearest vector from its codebook, and AQLM uses a multi-step beam search procedure to choose the best combination of codes from both codebooks. Our PV-Tuning implementation uses these search algorithms during the subspace linearized V step (find_nearest in Alg. 3). We describe the full PV configuration for each method in Appendix K.

We compare PV tuning against several popular fine-tuning regimens found in the literature. Our first baseline is fine-tuning only continuous parameters, e.g., codebooks or input/output embeddings [71; 74]. The second baseline is training with Straight Through Estimation (STE) [75; 77]. We also test stochastic rounding as described in Appendix E.2. Finally, we evaluate PV tuning combined with STE, but otherwise the same configuration. We set the subspace size \(\) equal to the number of weights such that the update satisfies \(\|x^{k+1}-x^{k}\|/\|x^{k}\| 0.01\), also known as known as trust ratio .

The results in Table 1 show that PV-Tuning consistently finds better quantized models, with STE coming consistently second. We explore this further by combining subspace updates with STE, which leads to slightly better perplexity and accuracy in most (but not all) setups.

    & }}}}}}}}}}}}}\)**} & & }}}}}}}}}}}}}\)}} & }}}}}}}}}}}}}}\)} } &  }}}}}}}}}}}}}\)} } &  { }}}}}}}}}}}}}\)} & }}}}}}}}}}}}} \)} & }}}}}}}}}}}}} \)} & }}}}}}}}}}}}\)} }\) & }}}}}}}}}}}}\)} }\) & }}}}}}}}}}}}\)} }\) &  }}}}}}}}}}}}}\) & }}}}}}}}}}}\) & }}}}}}}}}} \)}}\) & }}}}}}}}}}}\)} }\) & }}}}}}}}}}}\)} } & }}}}}}}}}}}\) } \\  & **Wiki2\(\)** & **C4\(\)** & **Acc.\(\)** & **Wiki2\(\)** & **C4\(\)** & **Acc.\(\)** & **Wiki2\(\)** & **C4\(\)** & **Acc.\(\)** & **Wiki2\(\)** & **C4\(\)** & **Acc.\(\)** \\  Calibration only (no global fine-tuning) & 3290 & 4125 & 29.0 & 20.26 & 20.09 & 43.42 & 7.38 & 9.34 & 53.2 \\  Continuous params only [71; 21] & 16.77 & 17.53 & 46.27 & 8.17 & 10.99 & 52.14 & 6.69 & 8.77 & 56.57 \\ Naive Linearized PV (no subspace) & 16.73 & 17.48 & 47.68 & 8.19 & 10.94 & 52.08 & 6.68 & 8.75 & 56.51 \\ Stochastic Rounding  (tuned) & 11.97 & 13.07 & 49.79 & 8.02 & 10.64 & 52.31 & 6.56 & 8.39 & 56.68 \\ Straight Through Estimation  & 8.79 & 11.04 & 50.61 & 7.76 & 10.26 & 52.58 & 6.41 & 8.63 & 57.04 \\ Subspace Linearized PV (ours, \(0.01\)) & 8.49 & **10.78** & **52.17** & 7.38 & 9.47 & 53.36 & 6.13 & 8.35 & 57.81 \\ Subspace Linearized PV+STE (\(0.01\)) & **8.43** & 10.82 & 51.90 & **7.32** & **9.35** & **55.22** & **5.90** & **7.43** & **58.19** \\   

Table 1: Comparing different fine-tuning strategies for VQ, GPTQ and AQLM on Llama 2 7B in terms of perplexity on WikiText-2, C4 and average zero-shot accuracy on tasks from Section 4.3.

**PV-Tuning over QuIP#** In addition to these three configurations, we also apply PV-tuning to QuIP#  -- a modification of vector quantization that applies Randomized Hadamard Transform (RHT) before quantization and uses fixed lattices instead of learned codebooks. We experiment with Llama-2 7B model quantized with QuIP# to 2 bits per weight and found that it is possible to significantly improve the model through PV-Tuning. For instance, PV-tuning improves WikiText-2 perplexity from 6.19 (QuIP# with built-in continuous fine-tuning) to 5.71 (PV-Tuning + STE). Since original 16-bit model has a perplexity of 5.13, this corresponds to almost halving the quantization error in terms of perplexity. We report additional details for QuIP# with PV-Tuning and full evaluation results in Appendix L and include it to Table 2 as "QuIP#+PV".

**On the choice of hyperparameters for 1-bit vector quantization.** There are several possible hyperparameter configurations for vector quantization (VQ) that fall into 1-1.1 bit range. One can either use larger codebooks for longer groups (vectors), or smaller codebooks for shorter groups accordingly. In our main evaluations, we quantized vectors of 16 consecutive weights with 14-16 bit codebooks to fit into the desired bitwidth. However, we later found that it is more advantageous to choose smaller groups as well as codebooks. We found that 8-bit code per 8 weights outperforms 14-bit code per 16 weights despite having near-identical bitwidth (due to smaller codebooks). We report this configuration as "PV (gs8)" in Table 2 and provide additional experiments in Appendix M.

### Large-scale Evaluation & Discussion

Finally, we evaluate the resulting PV algorithm with a vector quantization backbone and KL objective on a range of popular LLM models. For this section, our goal is to evaluate our approach holistically for different models and target bit-widths, comparing against the best known baselines in common settings. To that end, we evaluate on Llama 2 & 3 , Mistral 7B  and Phi-3 Mini-4k-Instruct  at 1-2.5 bits per parameter (averaged over all transformer layers).

We report perplexity on WikiText-2  and C4  validation sets, zero-shot accuracy on Wino-Grande , PiQA , HellaSwag , ARC-easy and ARC-challenge  via the LM Eval Harness . We follow the exact evaluation setup from GPTQ . We compare against QuIP , BiLLM , PB-LLM , DB-LLM , AQLM , OneBit , QuIP# , the latter three using fine-tuning. For Llama 3, we use baselines from  and re-evaluate perplexity in our setup.

  
**Size** & **Method** & **Avg bits** & **Wiki2\(\)** & **C4\(\)** & **Average\(\)** \\   &  \\   & – & 16 & 5.12 & 6.63 & 64.80 \\  & BiLLM & 1.08 & 32.48 & 40.52 & 41.68 \\  & OneBit & 1.01 & 9.73 & 11.11 & 50.06 \\  & PV-Tuning & 1.02 & 8.28 & 10.37 & 50.66 \\  & PV (gs8) & 1.00 & **7.62** & **9.73** & **53.77** \\   & AQLM & 2.02 & 6.64 & 8.56 & 56.47 \\  & QuIP\# & 2.01 & 6.19 & 8.16 & 57.51 \\  & DB-LLM & 2.01 & 7.23 & 9.62 & 55.12 \\  & PV-Tuning & 2.02 & 5.84 & 7.62 & 61.35 \\  & QuIP\#+PV & 2.01 & **5.71** & **7.51** & **61.81** \\   & – & 16 & 4.57 & 6.05 & 67.82 \\  & AQLM & 1.97 & 5.65 & 7.51 & 60.59 \\  & QuIP\# & 2.01 & 5.35 & 7.20 & 61.45 \\  & DB-LLM & 2.01 & 6.19 & 8.38 & 59.41 \\  & PV-Tuning & 1.97 & **5.12** & **6.83** & **64.92** \\  & PV-Tuning & 2.19 & **5.05** & **6.74** & **66.05** \\   & – & 16 & 3.12 & 4.97 & 72.40 \\  & AQLM & 2.07 & 3.94 & 5.72 & 68.75 \\   & QuIP\# & 2.01 & 3.91 & 5.71 & 68.60 \\   & DB-LLM & 2.01 & 4.64 & 6.77 & 65.83 \\   & PV-Tuning & 2.07 & **3.78** & **5.56** & **70.72** \\   & PV-Tuning & 1.14 & 5.52 & 7.50 & 64.58 \\   

Table 2: Quantized model perplexity on **WikiText-2\(\)** & **C4\(\)** and the **Average\(\)** accuracy on 5 zero-shot tasks  for various models and bitwidths. Arrows \(/\) mean higher / lower is better.

Table 2 summarizes our findings: **PV-tuning with vector quantization outperforms all known methods for 1- and 2-bit per weight**. The closest competitors on Llama 2 are QuIP#, AQLM and OneBit, all of which use fine-tuning. The improvements on Llama 3 are also remarkable as this model is notoriously hard to compress . We report additional evaluations in Appendix N.

**Pareto-optimality.** A key practical question concerns obtaining optimal quality for the target model size, where a smaller model compressed to 3-4 bits often dominates a larger model compressed to 1-bit. The best known Pareto-optimal bit-width for Llama 2 is 2.5 : compressing a larger model to less than 2.5 bits per weight is inferior to a smaller model quantized to the same total number of bytes. From this perspective, **PV-tuning pushes the Pareto-optimal frontier for Llama 2 to 2.0 bits**. This is easiest to see in Table 12: a 2-bit 13B model outperforms any 7B quantization and is comparable with the 16-bit 7B model. The same holds for the 2-bit 70B model.

**Fine-tuning efficiency.** One limitation of our algorithm is that it requires more compute and memory during the fine-tuning procedure. The 7B models can be fine-tuned on a single GPU, our 70B runs require a server with \(8 A100\) or rely on RAM offloading. PV-Tuning shares this drawback with prior methods based on STE , as both methods need gradients w.r.t. dequantized weights. Our longest training run took 2 days on 8 GPUs to outperform all baselines and 8 days to fully converge.

**Inference speed.** PV-Tuning does not change the underlying compressed representation, allowing us to reuse existing high-performance inference kernels. Specifically, VQ+PV can reuse efficient kernels from , while GPTQ+PV can use ExLlamaV2 kernels . We report these inference speed evaluations in Appendix O. From a practitioner's point of view, PV-Tuning can significantly improve the accuracy of extreme (1-2 bit) quantized models, making it possible to deploy large LLMs on resource-constrained devices. As a proof of concept, we developed specialized inference engines for running vector-quantized models with PV-Tuning on mobile devices7 or in the browser8.

## 5 Conclusions

**Limitations.** We focused our effort on evaluating PV-Tuning with multiple setups and models, but spent relatively little effort tuning our algorithm for each specific setup. For instance, we always use constant learning rate and \(\) with no schedule, and always train on the same data. While this shows robustness of PV-Tuning, it also means that our results may be improved with better hyperparameters. For instance, Appendix M shows how PV-Tuning with 1-bit vector quantization can be improved by choosing smaller vector sizes, while Appendix L suggests that PV-Tuning can dramatically improve models quantized with QuIP# and may similarly be applied to other quantized representations. Furthermore, the algorithm could achieve better accuracy by simply training longer and on more data.

**Future work.** This work opens several new research directions. The first is about how to choose \(^{k}\): while we found that a greedy strategy works in practice, there may be fundamentally better ways. Another direction is applying PV-Tuning to other quantization niches: our evaluation focuses on extreme weight-only quantization, but the proposed algorithm can be used in weight + activation setting or KV cache quantization. Overall, PV-Tuning shows how an insight from optimization theory can improve LLM quantization and we are excited to see how this develops in future research.