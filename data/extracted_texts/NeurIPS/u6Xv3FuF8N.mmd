# Flocks of Stochastic Parrots: Differentially Private

Prompt Learning for Large Language Models

 Haonan Duan\({}^{*}\), Adam Dziedzic\({}^{}\), Nicolas Papernot, Franziska Boenisch

University of Toronto and Vector Institute

Corresponding and leading author: haonand@cs.toronto.eduEqual contribution.

###### Abstract

Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that _soft_ prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for _discrete_ prompts. Thus, we orchestrate a noisy vote among _an ensemble of LLMs_ presented with different prompts, i.e., _a flock of stochastic parrots_. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private algorithms closely match the non-private baselines. For example, using GPT3 as the base model, we achieve a downstream accuracy of \(92.7\%\) on the sst2 dataset with \((=0.147,=10^{-6})\)-differential privacy vs. \(95.2\%\) for the non-private baseline. Through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial APIs.

## 1 Introduction

Large language models (LLMs) exhibit strong capabilities for in-context learning [6; 40]. By prepending the adequate prompt to an LLM's input, the model can perform a myriad of natural language downstream tasks without any modifications to its parameters . While the data used to train an LLM is usually assumed to be public, downstream data used in the prompt is often more sensitive. This can elicit confidentiality issues, for instance, if prompts contain information that represents valuable intellectual property . At the same time, it also raises privacy concerns when the data involves personal information about individuals.

In this paper, our first contribution is to show that these concerns are valid. We are the first to instantiate a highly effective membership inference attack (MIA) [7; 45] against prompts. Our attack is able to determine if a given data point was used within the prompt of the LLM. The only existing solution to mitigate this privacy risk would be to forego prompting and instead fine-tune the LLM with a privacy-preserving training algorithm [25; 54]. Yet, fine-tuning lacks the efficiency and practicality of prompting. Indeed, fine-tuning requires significantly more data , computational resources , and storage space . Additionally, fine-tuning requires access to the LLM parameters. However, many of the state-of-the-art LLMs are proprietary models deployed behind an API which only allows its users to query the LLMs [3; 6; 10; 17; 35].

To leverage the benefits of prompting while at the same time protecting the data contained in prompts, we propose the first algorithms for prompt learning with privacy. Our algorithms offer rigorous guarantees expressed using differential privacy . Perhaps closest to existing work on fine-tuning, we propose to leverage the canonical DPSGD algorithm  to learn soft promptswith differential privacy guarantees. Our PromptDPSGD algorithm performs a private gradient descent on the soft prompt embeddings prepended to the LLM's private input. Since these embeddings have very few parameters in comparison to LLMs, our PromptDPSGD is efficient and yields competitive privacy utility trade-offs at a fraction of the training complexity of private fine-tuning.

However, learning soft prompts with DPSGD may not always be possible because it requires computing gradients with respect to the prompt input. As mentioned previously, current APIs [3; 6; 10; 17; 35] usually do not provide these gradients. We thus turn to _discrete_ prompts which consist of natural language tokens. Discrete prompts address the aforementioned limitations while being more data-efficient. Our insight is to observe that LLMs with discrete prompts naturally lend themselves to another canonical approach of differentially private learning known as the private aggregation of teacher ensembles (PATE) . We introduce PromptPATE, which creates an ensemble of LLMs with different discrete prompts from the private dataset which we refer to as _a flock of stochastic parrots_. Since interacting with the flock directly can leak private information about the prompts, as we demonstrate with our MIA, PromptPATE additionally performs a knowledge transfer. Therefore, each model in the flock generates a next token prediction for a short input sequence of some public data. By performing a noisy majority vote over all models' token output, we generate a single output that, due to the noise addition, implements differential privacy guarantees while incorporating knowledge from the flock. The public input together with the noisy aggregated output form a new single example for the discrete _student prompt_ that can be prepended to the LLM in lieu of the individual prompts which contain private information. In addition to providing rigorous privacy guarantees, our PromptPATE is highly efficient, since, instead of having to query every model from the flock at inference time, it suffices to query the LLM prepended with the student prompt _once_.

We perform extensive experiments against multiple popular LLMs, such as GPT3  and Claude , that are deployed behind commercial black-box APIs. Our results highlight that PromptPATE provides high downstream performance that matches the one of non-private prompting even at very strong privacy guarantees. On the sst2 dataset with GPT3, for instance, we reach an accuracy of \(92.7\%\) with privacy costs as little as \((=0.147,=10^{-6})\)-differential privacy, even when the public data used during PromptPATE's knowledge transfer stem from a different distribution than sst2. Our results closely matches the non-private baseline accuracy (\(95.2\%\)). Thus, we conclude that prompt learning for LLMs is not only more efficient and practical than fine-tuning but can also achieve high utility even with strong and practical privacy protection in place.

In summary, we make the following contributions:

Figure 1: **Our methods for private prompt learning.**_Left_: PromptDPSGD obtains the input gradients from the LLM, and performs DPSGD to update the soft prompt embedding while keeping the LLM frozen. _Right_: PromptPATE creates a noisy ensemble of private discrete prompts, and then transfers knowledge by selecting a student prompt that can be publicly released. PromptPATE only needs black-box access of the LLM and, thus, can be easily deployed with commercial APIs.

* We instantiate the first MIA on prompted LLMs and show that we can effectively infer membership of the prompted data points with high success.
* We propose a lightweight alternative to DP fine-tuning, namely PromptDPSGD, which optimizes orders of magnitude fewer parameters while keeping the original LLM frozen.
* We propose PromptPATE, the first method for DP learning with LLMs that requires only black-box access to the model--making it easily deployable for commercial LLM APIs.
* Our experiments on multiple state-of-the-art commercial APIs [6; 3] highlight that our methods achieve both high utility and strong privacy protections in various setups.

## 2 Background and Related Work

Prompts for LLMs.The success of LLMs, such as BERT, Claude, OPT, or different versions of GPT and their exceptional in-context learning capacities gave rise to prompt-based learning [14; 6; 39; 40; 35; 56]. Prompts serve as _demonstrations_ of the downstream task, which the model can then generalize from. There are two paradigms for LLM prompting, namely _discrete_ and _soft_ prompts.

Discrete prompts [6; 16; 18; 27; 44] are natural-language instructions that contain examples from the downstream task in a well-crafted template. Tuning discrete prompts is often done by prompting the model with different combination of examples, assessing their performance on the downstream task, and choosing the combination that yields the highest performance as the final prompt.

In contrast to discrete prompts, soft prompts [24; 27] prepend trainable continuous embeddings to the inputs of LLMs. These embeddings are initialized either at random or with embedding vectors that correspond to tokens from the dictionary. During tuning, the embeddings are updated through gradient descent to minimize the loss of the prompted model on the private downstream task. To increase performance further, trainable embeddings can be prepended not only to the input but also to every LLM layer, a technique known as _prefix_[26; 28; 29].

Both soft prompts and prefix train end-to-end without any human involvement through backpropagation over the LLM. On the other hand, discrete prompts have to be designed manually through careful prompt engineering. Yet, prompt engineering only needs inference passes over the LLM which makes discrete prompt more computationally lightweight. Our work provides privacy protection for all of these paradigms: discrete prompts, as well as for soft prompts, and prefix.

Privacy Leakage in LLMs.LLMs have been shown to memorize data both from their original large training corpora [8; 20; 23; 32; 48; 55] and from smaller private datasets used to fine-tune them for downstream tasks . The only prior work around privacy leakage in prompt-based learning utilizes prompts to extract knowledge from trained LLMs [13; 22; 38]. In contrast, we study the privacy of the prompting data itself. To do so, we investigate the canonical privacy attack known as **membership inference attacks (MIA)**[7; 45]. Its use as a practical means to demonstrate leakage of private information in ML was recently popularized by a line of work on quantifying memorization [9; 43; 47]. While prior work utilizes MIAs to assess whether a given data point was used to train an LLM, we instantiate a MIA to assess whether a given data point was used within the prompt prepended to the inputs of a trained LLM.

Defending Against Privacy Leakage in LLMs.Prior work either focuses on training [2; 19] or fine-tuning [25; 54] LLMs with privacy guarantees. These approaches rely on the mathematical framework of **differential privacy (DP)** and in particular the **DPSGD** algorithm for private stochastic gradient descent . Here, DPSGD is applied to guarantee that one outputs approximately the same model parameters whether or not any given data point was used to train or fine-tune the model. To achieve this, DPSGD clips the per-example gradients that are computed during training and adds well-calibrated noise to each model update. These two operations typically increase the computational complexity of training and decrease the utility of the resulting model [1; 4; 49]. To counteract these effects, state-of-the-art methods for full DP-fine tuning in LLMs require extensive hyperparameter tuning and vast computational resources . Alternative approaches refrain from updating the large number of model parameters and instead introduce additional layers into the model architecture and only fine-tune these layers with DPSGD . To the best of our knowledge, no prior work attempted to provide DP guarantees for prompt data in LLMs.

Setup and Notation.We denote by \(P\) the soft or discrete prompt that is prepended to any input sequence \(x_{i}\) when querying the language model \(L\). For brevity, we denote \(L([P,x_{i}])\) by \(L_{P}(x_{i})\).3 The output \(y_{i}\) of \(L_{P}(x_{i})\) is an \(M\)-dimensional probability vector, with \(M\) being the size of the model's vocabulary. Each component of \(y_{i}\) corresponds to the probability that the \(L_{P}\) assigns to the respective token for being the next token in the sequence \(x_{i}\). The semantic meaning of the next token varies depending on the given downstream task. For instance, for classification, the index with the highest probability indicates the token of the class that \(L_{P}\) assigns to \(x_{i}\).

## 3 Private Information about Prompt Data Leaks from Prompted LLMs

By instantiating a MIA against prompted LLMs, we want to highlight that the private data used within a prompt (which we refer to as _prompt data_ from hereon) can be subject to a substantial privacy risk. We showcase this risk at the example of LLMs that are prompted with discrete prompts \(P\) containing tuples of demonstrations from classification downstream tasks as prompt data \(p=\{(p_{x},p_{y})\}\). For example, in a prompt with one demonstration (_one-shot learning_), the prompt data \(p\) may be specified as \(p=\{\)_("The movie was great.", "positive")_\(\}\). Our prompts are provided in a consistent template where one or multiple demonstrations are combined with instructions as \(P=\)_[Instruction, (text sequence \(p_{x}\), class-label token \(p_{y}\)),...]_.

For our MIA, we consider an adversary who aims at inferring whether a given private demonstration \((p_{x},p_{y})\) was used within the prompt data \(p\). The adversary holds \(n\) candidate demonstrations of text sequences and corresponding labels \(l_{i}\) and queries the text sequences \((x_{1},,x_{n})\) to \(L_{P}\) with black-box access. The prompted model \(L_{P}\) then returns the output probability vectors \((y_{1},,y_{n})\). Following prior work [21; 53], we analyze the model's output probability at token \(y_{i,l_{i}}\) that corresponds to the _correct_ target class label of every \(x_{i}\). The intuition to distinguish between members and non-members is that the output probabilities at the correct class \(l_{i}\) will be significantly higher for demonstrations that were used within the prompt, _i.e.,_ members with \((p_{x},p_{y})=(x_{i},l_{i})\). We show that even with this simple MIA, we can reliably determine membership for the prompt data.

Experimental Setup.We prompt GPT3-Babbage  with multiple _one-shot examples_ to solve four standard downstream text classification tasks, namely _dbpedia_, _sst2_, _agnews_ and _trec_. The template of our prompts follows . To evaluate our MIAs, we consider the single data point used within the prompt as a members and \(50\) other randomly selected data points from the respective task's training dataset as non-members. This skewed distribution between members and non-members (1 vs 50) corresponds to a realistic scenario where only a small proportion of the candidate data targeted by the adversary are members . To quantify the success of our attack, we report the AUC-ROC curves of \(100\) random trials.

Figure 2: **MIA Risk. We study GPT3 prompted with \(100\) different one-shot examples (dbpedia). _left_: We present the prediction probabilities at the correct class for members (the one-shot example) and non-members (\(50\) randomly sampled private points). The output probability for members is significantly higher than for non-member data points. _right_: We present the AUC-ROC curves of our MIA against the \(100\) prompts (gray lines) and the blue line as an average over all attacks. Given that each prompt has only one member, the resulting TPRs can only be 0% or 100% which leads to the step-shape of the gray curves. The result indicates that our attack is significantly more successful than random guessing (the red dashed line).**

Results.Before evaluating the success of the MIA, we analyze the probability output from GPT3 for the correct target class between member and non-member data points. Figure 1(a) shows for the dbpedia dataset that the prediction probabilities for non-members are significantly lower than for members. Figure 1(b) shows that this leads to a high MIA risk in terms of an average AUC score of \(0.84\) for the prompt data. Similar results for other datasets and models are presented in Appendix D. These results highlight that private information can leak from prompt data easily and thus motivate the urgent need for defenses which we develop in the rest of this paper.

## 4 Methods for Privacy Preserving Prompts

As of now, if we want to protect the private downstream data, we have to forego prompting altogether because, to the best of our knowledge, no algorithms for private prompt learning exist. The only alternative to privately adapt the LLM would be to perform DP fine-tuning [25; 54]. However, this approach is only feasible when we have direct access to the LLM to update its parameters with DPSGD  or to even change the model architecture to insert additional parameters--fine-tuned with DPSGD . This is prohibitively expensive and mostly impossible with the commercial API, thus we propose the first algorithms that enable differentially private prompt learning.

We consider two main paradigms of prompting: soft prompts and discrete prompts. To learn private soft prompts, we introduce PromptDPSGD. PromptDPSGD is a parameter-efficient alternative to DP fine-tuning that does not need modifying the parameters or architectures of the LLM. However, many popular APIs [35; 6; 10; 17; 3] do not support soft prompts yet as it requires gradients with respect to the input. Therefore, we propose PromptPATE for discrete prompts. PromptPATE requires only black-box access to an LLM without any knowledge of the LLM's architecture or mode of operation. Instead, the algorithm only needs the next-token prediction of the LLM. This, to our knowledge represents the first solution for privately adapting LLMs in restricted API setups.

### PromptDPSGD: DPSGD for Private Soft Prompt Learning

In general, all discrete input tokens to LLMs are internally transformed into continuous input embeddings that the LLM then operates on. Soft prompts are just additional continuous input embeddings that can be prepended to the original input embeddings before passing them through the LLM. To train (or _tune_) soft prompts, we require training data from a potentially private downstream task. After prepending the continuous soft prompt embeddings to input examples from the training data, we can calculate the gradients for the loss of the prompted LLM with respect to these soft prompt embeddings. The gradients provide information about how the soft prompt should be updated in order to minimize the loss on the training data.

If we can obtain the gradients for soft prompts, we can learn these prompts with privacy guarantees by applying the canonical DPSGD algorithm . The same applies to prefix, therefore, when we talk about soft prompts in the following, we implicitly also include prefix. We call this approach PromptDPSGD. The algorithm yields soft prompts with DP guarantees that can be deployed with the LLM to solve the respective downstream task. The privacy analysis of PromptDPSGD follows the one of the standard DPSGD. Note, however, that while conceptually similar to fine-tuning the LLM's parameters with DPSGD [54; 25], PromptDPSGD differs in a crucial aspect. In DP-SGD fine-tuning, we require the gradients with respect to all or a subset of the model parameters and update these parameters to minimize the loss. In contrast, in PromptDPSGD, we use the gradients with respect to the soft prompt embeddings and only alter these. We highlight this difference in our PromptDPSGD-algorithm that we present in Appendix C.

While this difference seems subtle, it has far-reaching consequences. First, there are orders of magnitude fewer parameters that need to be updated which increases training efficiency. Second, and most importantly, it allows us to keep operating on the original LLM. We discuss the resulting advantages, such as storage efficiency, and the ability to process multiple different tasks simultaneously at the end of this section (in 4.3). These advantages make PromptDPSGD conceptually superior to private fine-tuning. At the same time, as we show in our evaluation, despite the small number of trainable parameters, PromptDPSGD, for simpler tasks, matches the performance of private fine-tuning. Yet, current APIs [35; 6; 10; 3; 17] do not support soft prompting, prefix, or private fine-tuning and only provide black-box access through discrete prompts. For these setups, we propose PromptPATE.

### PromptPATE: PATE for Privacy Preserving Discrete Prompts

PATE [36; 37] enables learning classifiers with DP guarantees. It first trains an ensemble of _teacher_ models on disjoint subsets of the private data. Second, through a noisy labeling process, the ensemble privately transfers its knowledge to an unlabeled public dataset. Finally, a separate _student_ model is trained on this labeled public dataset for release. The noisy knowledge transfer in the second step relies on the Confident GNMAX algorithm  that we detail in Appendix C. It consists of three main parts: for any input data point from the public unlabeled dataset, each teacher votes for the most likely class. Then, the consensus over the teachers' votes is determined and queries with low consensus are rejected to avoid revealing too much information about the private decision boundary. Finally, the returned class label for any non-rejected data point is determined as a noisy argmax over all teachers' vote counts--where the added noise is sampled from a Gaussian distribution to implement the DP guarantees. For each rejected or labeled data point from the public dataset, privacy costs are accumulated and the ensemble stops labeling once a target privacy budget is reached.

Our PromptPATE follows the general flow of standard PATE: training the _teacher models_, _private knowledge transfer_, and training a _student model_. However, due to the significant differences between in-context learning for LLMs and supervised learning in the original PATE and how these different paradigms leverage private and public data, we had to redesign each of these building blocks. This allows to leverage both the data-efficiency of prompts and the rigorous privacy protection from PATE. In the following, we present the building blocks in our PromptPATE.

Teacher Models (Flock of Stochastic Parrots).Instead of _training_ teacher models on disjoint partitions of the private data, we use the private data to create disjoint prompts for the LLM. More specifically, we use examples, for instance {(_"The movie was great.", "positive"_),...}, from the private training data to create prompts that can then be deployed with the LLM as teachers.

Private Knowledge Transfer.During the private knowledge transfer, the teachers label public data sequences, such as (_"I did enjoy it."_,...). Each teacher votes with the most likely class labels for the private downstream task. In Appendix D, we show that PromptPATE can also operate directly on pure next token predictions from Claude  without access to per-token probabilities--enabling full black-box private prompts. By performing the private voting process according to standard PATE with the Confident GNMAX algorithm, we turn our per-teacher predictions into a final class label token that will be appended to the sequence, _e.g._, (_"I did enjoy it"_, "positive"_). The privacy accounting and analysis of our PromptPATE exactly follows the one of standard PATE .

Student.The most naive way to obtain a student model following standard PATE would be to label many public sequences and train a language classifier using supervised learning on this data. However, due to the relatively high number of data needed for supervised learning, and the fact that each query to the private teachers consumes privacy, this process would incur high privacy costs. We propose a better approach building on the data-efficiency of prompting  by using labeled public sequences to create new discrete student prompts. The selected prompt can then be deployed with the LLM as the PromptPATE student model.

In theory, labeling one public sequence by the ensemble would be sufficient to create such a prompt. This approach yields negligible privacy costs, but the resulting prompt might not have good utility due to the high variance in the performance of prompts . Therefore, we generate multiple prompts based on different labeled public sequences and perform prompt tuning to select the best student prompt. Care must be taken during selection: utility cannot be evaluated on the private data anymore given that the prompt will be publicly deployed and selecting based on the private data would incur additional privacy costs. We solve this tension by using parts of the newly-labeled public data as validation data to assess utility of the student prompts. By selecting the prompt with the highest validation accuracy, we deploy the student prompt that most resembles the private teachers.

### Advantages of (Private) Prompting over (Private) Fine-Tuning

Our private prompt learning enables us to leverage the general advantages of prompting over fine-tuning while preserving privacy. Private prompting requires significantly less storage than private fine-tuning. While fine-tuning requires storing a separate copy of the LLM model for each downstream task , prompts operate only on the input level of LLMs without adapting model parameters, such that only a small task-specific prompt needs to be stored for each downstream task. For example, each copy of the fine-tuned RoBERTa base model requires 125M parameters (\(\)500MB). This becomesprohibitively expensive, especially as the number of parameters for state-of-the-art LLMs rapidly increases. In contrast, soft-prompts and prefix, as the one generated by PromptDPSGD (using implementation from ) with the standard prompt length of 10 tokens require less than 10K parameters (40KB) for the soft-prompt and 100K parameters (400KB) for the prefix. A discrete prompt, such as the one generated in PromptPATE, requires less than 1 KB of prepended text. Prompts also enable processing many examples from different tasks in a single batch , called mixed-task inference. This allows more efficient use of LLMs since we do not have to wait for a sufficient number of requests for a single task before processing them. This is not possible with any form of fine-tuning, where the fine-tuned model can serve solely a single task.

## 5 Experimental Evaluation

We evaluate both PromptDPSGD and PromptPATE and show that they match the performance of non-private prompting while providing strong privacy guarantees.

### PromptDPSGD

Experimental Setup.To train soft-prompts and prefix, we follow the experimental setup from prior work on DP fine-tuning. Specifically, we use differentially-private optimization engines for transformers, such as models from the BERT family for the language understanding tasks. The experimental results for classification were performed on the RoBERTa models , using the standard NLP datasets, namely sst2, qnli, qqp, and mnli, from the GLUE benchmark . Our implementation for soft-prompt and prefix is based on P-Tuning v2 . To tune the (hyper)parameters for PromptDPSGD, we adjust the length of the soft-prompt or prefix in the private setting (with the default value of \(10\), which commonly yields good performance). For the privacy parameters, we set the \(=1/N\), where \(N\) is the number of data points in a given dataset, The clipping threshold of per-example gradients is set to \(0.1\) in most cases. We use a batch size of 1024. The detailed selection of (hyper-)parameters is presented in Appendix E.

Results.We compare our PromptDPSGD against state-of-the-art approaches for private fine-tuning on multiple private downstream datasets. Our results are shown in Table 1. We highlight that both soft prompts and prefix provide competitive privacy utility trade-offs. For example, the difference in accuracy values between the non-private baseline and the private soft prompt ranges from 3% (for the simplest sst2 dataset) and up to 7% (for the most difficult mnli dataset). This mirrors results for other private methods, such as the private fine-tuning of LoRA . We also observe that, similarly, for simple tasks, such as sst2 or qnli, the performance of soft prompt or prefix matches the one of fine-tuning. For the more difficult tasks, namely qqp and mnli, the performance of prefix and soft prompts is also relatively close to fine-tuning. The results obtained for these methods are highly influenced by the number of optimized parameters. For example, for the SST2 task and the RoBERTa-Base model, the prefix requires 19970 additional parameters while soft prompt adds solely 2306 parameters. On the other hand, the number of privately tuned parameters is a few orders of magnitude bigger for fine-tuning and equal to the size of the trained model, namely 125M for the method proposed in , while the fine-tuning approach from  optimizes around 1.2M

    & **M** &  &  &  & LoRA-Tuning  \\   & **P** &  &  &  &  \\   & **G** & \(=8\) & \(=\) & \(=8\) & \(=8\) & \(=\) & \(=8\) & \(=\) \\  sst2 & 92.31 & 95.64 & 91.97 & 96.33 & 85.89 & 96.40 & 92.97 & 96.60 \\ qnli & 84.11 & 89.48 & 87.17 & 94.84 & 84.81 & 94.70 & 88.59 & 94.70 \\ qqp & 81.52 & 86.56 & 82.58 & 91.42 & 86.15 & 92.20 & 86.26 & 92.20 \\ mnli & 75.15 & 82.49 & 80.57 & 90.34 & 83.30 & 90.20 & 82.92 & 90.20 \\   

Table 1: **Performance of PromptDPSGD. We report the accuracy values (%) for each dataset. All \(\) values are reported as standard DP guarantees. We run the experiment on RoBERTa . The first row M: the type of the private Method, the second row P: the number of Parameters tuned for the method, and the third row G: DP Guarantee. We also present results for \(=3\) in Appendix D.**parameters. Our results reflect a general trend, where prompts are suited for small downstream tasks while fine-tuning with its bigger number of parameters can also cater to more complex tasks with larger training data sets.

### PromptPATE

Experimental Setup.**Teachers:** Unless otherwise specified, we rely on GPT3-Babbage as the base LLM and select one-shot examples randomly without replacement from the private downstream task as prompt data. Our prompt template follows Zhao _et al._. For each setting, we deploy \(200\) teacher prompts. **Private knowledge transfer:** We use the implementation of PATE's Confident GNMAX algorithm and the privacy accounting from  and report our algorithm's hyperparameters in Appendix E. **Student:** For each private downstream task, we experiment with two setups (1) selecting public input sequences from the same (IID) and (2) from a different distribution (OOD) as the private data. We introduce three new datasets for the OOD setup: imdb , arisetv  and qqp . The details of preprocessing these datasets can be found in Appendix E. In both the IID and OOD setup, we limit the size of the public dataset to \(500\) input sequences from the respective datasets. After the ensemble finishes labelling, we select the best labeled public sequence as prompt data based on the validation accuracy on the labeled public set. We repeat the process three times and report average and standard deviation of the test accuracy for the selected student prompt on the private test set. To improve utility, both teachers' and students' output probabilities from GPT3 are recalibrated using contexual calibration .

Results.We compare PromptPATE against three baselines: the lower bound baseline represented by a zero-shot prediction (\(=0\)), _i.e.,_ when the LLM is only prompted with an instruction, the private ensemble accuracy (\(=\)), and the upper bound as a non-private one-shot prediction (\(=\)) using the best example from the private data as prompt data. (To save costs, we select from \(200\) candidates.) Table 2 shows that, over all setups, PromptPATE achieves similar utility to the non-private baseline and significantly improves over zero-shot predictions--even at very strong privacy protection (\(<0.3\), \(=10^{-6}\)). Our results also highlight that the distribution of the public data does not need to be very close to the distribution of the private data to yield high-utility student prompts. For example, they can be collected from different domains (dbpedia holds extracts from wikipedia while its public data agnews contains news articles) and for different tasks (tree aims to classify the topic of a given answer while qqp serves to measure the similarity of two questions). Still, with dbpedia being the private downstream data and agnews as public, we achieve an accuracy of 74.6%, which is significantly higher than the zero-shot baseline with 44.2%.

We also provide further insights into the privacy-utility trade-offs that can be achieved with PromptPATE in Figure 2(b). Our results highlight that with more public sequences queried to the ensemble, the privacy consumption increases while, after roughly \(100\) queries, with even \(<0.2\), the student model's test accuracy saturates. This yields very favorable privacy-utility trade-offs which we attribute

    &  Lower \\ Bound \\  &  Ens. \\ Acc. \\  &  Upper \\ Bound \\  &     
 Our PromptPATE \\ **ID** \\  \\  \\  \\  Private & \(=0\) & \(=\) & \(=\) &  & \(\) & Test acc & Public & \(\) & Test acc \\  sst2 & 76.3 & \(90.0\) & \(93.8\) & sst2 & \(0.178\) & \(88.8_{ 2.3}\) & imdb & \(0.187\) & \(87.2_{ 1.9}\) \\ agnews & 62.0 & \(72.8\) & \(78.2\) & agnews & \(0.248\) & \(71.7_{ 0.8}\) & arisetv & 0.258 & \(67.9_{ 1.7}\) \\ tree & 40.7 & \(57.6\) & \(58.7\) & tree & \(0.281\) & \(52.8_{ 1.5}\) & qqp & \(0.293\) & \(50.9_{ 3.5}\) \\ dbpedia & 44.2 & \(81.6\) & \(85.6\) & dbpedia & \(0.194\) & \(80.3_{ 1.3}\) & agnews & \(0.203\) & \(74.6_{ 1.4}\) \\ sst2 (C) & 82.0 & \(94.0\) & \(95.2\) & sst2 & \(0.147\) & \(92.3_{ 1.1}\) & imdb & \(0.154\) & \(92.7_{ 0.8}\) \\ agnews (4) & 62.0 & \(75.8\) & \(81.0\) & agnews & \(0.145\) & \(73.5_{ 1.2}\) & arisetv & \(0.145\) & \(69.6_{ 1.8}\) \\   

Table 2: **Performance of PromptPATE. We compare PromptPATE with three baselines: zero-shot (Lower Bound), the ensemble’s accuracy (Ens. Acc), and the non-private baseline (Upper Bound) on four classification benchmarks. We study two settings, (IID Transfer) when the public dataset is from the same and (OOD Transfer) different distribution than the private data. We find that PromptPATE achieves strong privacy protection (\(<0.3\) at \(=10^{-6}\)) and utility close to the non-private and significantly higher than the zero-shot. Unless otherwise specified, the experiments are performed on GPT3-Babbage with one-shot prompts. Additionally, we also run experiments on GPT3-Curie for sst2 (C) and 4-shot prompts for agnews (4).**mainly to the data efficiency of discrete prompts: Even from within as little as \(100\) labeled examples, a high-performing student prompt can be derived. Additionally, we observe that the per-query privacy costs of PromptPATE are relatively low, further benefiting the privacy-utility trade-off. The small privacy costs result from the high consensus between the teacher predictions4, see Figure 2(a)--that might result from all teachers relying on the same underlying LLM, just with different prompts.

Scalability.Finally, we also study how PromptPATE scales with larger LLMs and more examples in the prompt. We experiment with a more performant LLM (GPT3-Currie) for sst2. Due to the higher per-query costs, we are not able to repeat this experiment for all datasets. Our results show that the performance of our private prompt increases together with the performance of the public prompt (92.7% accuracy on Curie vs. 87.2% on Babbage) while the privacy budget \(\) decreases (from \(0.178\) to \(0.147\)). To investigate flexibility in terms of numbers of private examples provided as prompt data, we also experiment for agnews with 4-shot teachers. Similar to the non-private study  that reports improvements for agnews in the 4-shot setting over 1-shot, we observe that this improvement also translates to the private prompt. Our results indicate that with increasingly more powerful LLMs and larger context windows, private prompting will increase further in terms of privacy-utility trade-offs.

## 6 Conclusions and Outlook

By instantiating the first simple yet effective membership inference attack against prompted LLMs, we show that they leak private information about their prompt data. We propose private prompt learning as a holistic and broadly applicable new approach to mitigate this risk. We first introduce PromptDPSGD that enables to train soft-prompts with privacy guarantees. In contrast to fine-tuning, soft prompts optimize significantly fewer parameters and do not require any update of LLM parameters or changes to its architecture. As the first solution to private downstream learning with LLMs in black-box access scenarios, we propose PromptPATE. PromptPATE builds on the highly data-efficient discrete prompts and implements privacy through a noisy knowledge transfer. Through our evaluation against two popular LLMs deployed behind commercial black-box APIs (GPT3 and Claude) [6; 3], we highlight that this method yields downstream performance that matches the one of non-private prompting at very strong privacy guarantees. As LLMs rapidly improve and increase in size, prompts are achieving consistently higher performance while fine-tuning becomes more challenging at this scale. This suggests that privacy protections for prompts will become even more important, especially as context sizes expand.