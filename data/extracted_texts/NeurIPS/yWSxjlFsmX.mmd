# Is Mamba Compatible with Trajectory Optimization

in Offline Reinforcement Learning?

 Yang Dai\({}^{1}\)  Oubo Ma\({}^{2}\)  Longfei Zhang\({}^{1}\)  Xingxing Liang\({}^{1}\)

**Shengchao Hu\({}^{3}\)  Mengzhu Wang\({}^{4}\)  Shouling Ji\({}^{2}\)  Jincai Huang\({}^{1}\)  Li Shen\({}^{5}\)1**

\({}^{1}\)Laboratory for Big Data and Decision, National University of Defense Technology

\({}^{2}\)Zhejiang University \({}^{3}\)Shanghai Jiao Tong University

\({}^{4}\)Hebei University of Technology \({}^{5}\)Shenzhen Campus of Sun Yat-sen University

{daiyang2000,zhanglongfei,liangxingxing,huangjincai}@nudt.edu.cn

{mob, sji}@zju.edu.cn; charles-hu@sjtu.edu.cn; {dreamkily,mathshenli}@gmail.com

###### Abstract

Transformer-based trajectory optimization methods have demonstrated exceptional performance in offline Reinforcement Learning (offline RL). Yet, it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power. Mamba, a promising new linear-time sequence model, offers performance on par with transformers while delivering substantially fewer parameters on long sequences. As it remains unclear whether Mamba is compatible with trajectory optimization, this work aims to conduct comprehensive experiments to explore the potential of Decision Mamba (dubbed DeMa) in offline RL from the aspect of data structures and essential components with the following insights: (1) Long sequences impose a significant computational burden without contributing to performance improvements since DeMa's focus on sequences diminishes approximately exponentially. Consequently, we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa. (2) For the components of DeMa, we identify the hidden attention mechanism as a critical factor in its success, which can also work well with other residual structures and does not require position embedding. Extensive evaluations demonstrate that our specially designed DeMa is compatible with trajectory optimization and surpasses previous methods, outperforming Decision Transformer (DT) with higher performance while using 30% fewer parameters in Atari, and exceeding DT with only a quarter of the parameters in MuJoCo.

## 1 Introduction

Offline Reinforcement Learning (Offline RL)  has gained significant attention due to its ability to learn strategies without interacting with the environment, which is particularly beneficial in situations where real-time interaction is expensive or risky [2; 3; 4]. With a static dataset, offline RL can be implemented through three distinct learning methods : (1) model-based algorithm [6; 7; 8], (2) model-free algorithm [9; 10; 11], (3) trajectory optimization[12; 13; 14; 15; 16]. The first two methods require long-term credit assignment through the Bellman equation, leading to the "deadly triad" problem known to destabilize RL . In contrast, trajectory optimization methods treat RL problems as sequence modeling problems to get better performance and generalization . Most trajectory optimization methods rely on transformers, which perform credit assignment directly through theattention mechanism. By leveraging the powerful modeling capabilities of transformers, these methods outperform other offline RL algorithms [18; 19; 20].

The transformer attention mechanism , which allows the model to focus on the important part of the input sequence , has several downsides. The computational demands of the attention mechanism escalate quadratically with the input length, posing a significant constraint on its scalability [23; 24; 25]. Moreover, some studies [26; 27] suggest that the attention mechanism may not be the primary factor contributing to the effectiveness of transformers. This notion is also supported in offline RL, where  discovers that the attention mechanism of Decision Transformer (DT) does not capture local associations effectively, rendering it unsuitable for RL. Given these limitations, we are led to ponder if a more efficient mechanism with fewer parameters and greater scalability exists for offline RL. Recently, a series of state space models (SSMs) , particularly Mampa , have been proposed as potential solutions with the ability to scale linearly concerning the sequence length. In particular, Mampa introduces a selective hidden attention mechanism  for content-based reasoning and employs parallel scan to enhance computational efficiency, resulting in two approaches to employing Mampa in offline RL. The first is the Transformer-like Mampa, a direct substitution of the transformer [31; 32; 33] while the other is the RNN-like Mampa , achieving an inference speed with constant time complexity.

Few studies have explored the application of SSMs in offline RL, though they perform well in model-based algorithms [35; 36] and in-context RL learning . Mampa is tailored for memory-required long-sequence tasks, whereas trajectory optimization methods typically utilize short segments during training and inference, as most RL tasks are modeled as Markov Decision Processes (MDPs), i.e. past information may not influence current decisions. Furthermore, due to the lack of a comprehensive investigation of the key component of Mampa, a question has arisen:

_Whether Mamba is compatible with trajectory optimization?_

In this work, we aim to undertake a thorough investigation and in-depth analysis to explore this question. Specifically, we focus on the data structures and the essential components in trajectory optimization. The extensive experiments provide strong support for the following key findings. (1) We explore the data structures with an analysis of sequence length and concatenating type. The former reveals that long input sequences present computational challenges without enhancing performance due to the hidden attention scores of DeMa evincing an exponential decay pattern. As a result, we opt for the Transformer-like DeMa as opposed to the RNN-like DeMa for efficiency and effectiveness. The latter finds concatenating in the temporal dimension is better for the Transformer-like DeMa. (2) The hidden attention mechanism plays a pivotal role in DeMa's effectiveness and is compatible with the transformer's post up-projection residual structure , enabling it to replace the attention layer directly and eliminating the need for position embedding. Extensive evaluations show that with a higher average score and nearly 30% fewer parameters, DeMa significantly outperforms DT in eight Atari games. Furthermore, in nine MuJoCo tasks, DeMa's performance not only exceeds that of DT but does so with only one-fourth of the parameters, highlighting remarkable improvements in both performance efficiency and model compactness.

In the end, our main contributions can be summarized as follows:

1. We find the Transformer-like DeMa surpasses the RNN-like DeMa in both efficiency and effectiveness for trajectory optimization. Extensive experiments on sequence length and concatenating type show the impact of the input data, which guides the design of DeMa.
2. Through various ablation experiments, we discover that the hidden attention mechanism is the core component in DeMa and does not require position embedding. This finding enhances the effectiveness and efficiency of our Transformer-like DeMa.
3. With state-of-the-art performance on both MuJoCo and Atari, our Transformer-like DeMa significantly addresses the challenges posed by transformer-based trajectory optimization methods, particularly the issues of large parameter sizes and limited scalability.

## 2 Related Work

Offline RL.Offline RL is a data-driven RL paradigm in which the agent learns solely from a pre-collected dataset rather than through interaction with the environment . Distribution shifts can severely impact performance when RL algorithms are deployed directly in offline environments, leading to significant degradation. To mitigate this problem, several methods have been introduced, which the study  categorizes into three primary approaches: (1) learning a dynamics model to generate additional training data (model-based algorithm) [6; 39], (2) learning a policy through a model-free approach by constraining unseen actions or incorporating pessimism into the value function (model-free algorithm) [10; 11; 40], and (3) trajectory optimization [12; 15]. The method of trajectory optimization is usually based on a causal transformer model and converts an RL problem to a sequence modeling problem . It performs credit assignment directly through the attention mechanism in contrast to Bellman backups, thus modeling a wide distribution of behaviors, enabling better generalization and transfer .

Sequence Modeling in Offline RL.Following DT  and Trajectory Transformer (TT) , there has been an increasing trend in employing advanced sequence-to-sequence model to solve RL tasks [41; 42; 43; 44; 45; 46].1 Unfortunately, these improvements are usually transformer-based and hence suffer from the common dilemma of the attention mechanism, i.e. over-parameterization and inability to scale to long sequence tasks. What's more, Emmons et al.  find that simply maximizing likelihood with a two-layer feedforward MLP is close to the results of substantially more complex methods based on sequence modeling with Transformers. Similarly, Lawson et al.  find that replacing the attention parameters with those learned in other environments has a minimal impact on the performance. Besides, Decision ConvFormer (DC)  indicates that substituting the attention layers with learnable parameters can lead to improved outcomes. These observations suggest significant redundancy in the Transformer architecture, highlighting the potential to explore lighter and more scalable networks for implementation in offline RL. Building on this, the Structure SSM (S4)  has emerged as a promising alternative. Studies  and  use S4 in model-based RL, outperforming traditional Transformer and RNN approaches. The capabilities of S4 and Mamba are further demonstrated by [37; 51], which points to their speed and effectiveness in in-context RL tasks.

The most related work to ours is Decision S4 (DS4)  and Decision Mamba (DMamba) , where the former uses an RNN-like S4 for inference, and the latter replaces the attention mechanism with Mamba directly. In contrast, our work finds that Transformer-like DeMa outperforms RNN-like DeMa as the long sequences impose a significant computational burden on Mamba without contributing to performance improvements. What's more, DMamba simply substitutes Mamba for the attention block rather than the transformer block while our investigation shows the key component is the hidden attention mechanism, which eliminates the need for position embedding and hence achieves better performance with fewer parameters.

## 3 Preliminaries

In this section, we present several necessary preliminaries and terminologies of offline RL, trajectory optimization, state space model, and hidden attention in Mamba.

### Offline RL with Trajectory Optimization

Given a static dataset of transitions \(=\{(s_{t},a_{t},s_{t+1},r_{t})_{i}\}\), where \(i\) presents the timestep of a transition in the dataset. The states and actions are generated by the behavior policy \((s_{t},a_{t}) d^{s_{}}()\), while the next states and rewards are determined by the unknown transition dynamics \(p(s^{},r|s,a)\). The goal of offline RL is to find an approximate policy \((a|)\) that maximizes expected return \([_{t=0}^{T}r_{t}]\), where \(T\) represents the time step at which the episode terminates. Due to the lack of interaction with the environment, trajectory optimization methods transform the goal into minimizing reconstruction loss, i.e. minimizing loss \(_{(,s,a)}[_{t=1}^{T}_{ }(_{t};a_{t})]\), where \(_{t}=(|s_{t-K+1:t},_{t-K+1:t},a_{t-K:t-1})\), and \(_{t}=_{t^{}=t}^{T}r_{t^{}}\) is the return-to-go (RTG). At test time, a target RTG \(R_{0}\) is manually set to represent the desired performance. We input the trajectories from the last \(K\) timesteps into policy \(\), which then generates an action for the current timestep. Subsequently, the next state and reward are received from the environment. These elements are concatenated and also input into the model. The policy is approximated through the sequential model [12; 54]. However, these models typically possess a large number of parametersand struggle with handling long sequences effectively. Fortunately, this issue can be addressed by using SSMs [28; 50; 29].

### State Space Model and Mamba

There are two approaches to utilizing Mamba in RL, which are both closely related to the modeling methods of SSM. SSM is defined by the following first-order differential equation, which maps a 1-D input signal \(u(t)\) to an \(N\)-D latent state \(h(t)\) before projecting to a 1-D output signal \(y(t)\),

\[h^{}(t)=Ah(t)+Bu(t), y(t)=Ch(t)+Du(t),\] (1)

where \(A^{N N},B^{N 1},C^{1 N}\) and \(D\) are trainable matrices. As \(u(t)\) is typically discretized as \(\{u_{i}\}_{i=1,2,}\), SSM can be discretized by a step size \(\). Moreover, recurrent SSM can be written as a discrete convolution. Let \(h_{0}=0\) and \(D=0\), we have

\[y_{i}=C^{i}u_{1}+C^{i-1}u_{2}++Cu_{i-1}+Cu_{i}, y=u*,\] (2)

where \(,\) is the approximation discrete of \(A,B\), and \(\) is called the SSM convolution kernel and can be represented by filter

\[=(C,C,,C^{i},).\] (3)

S4 and other time-invariant models cannot select the previous tokens to invoke from their history records. To solve this problem, Mamba merges the sequence length and batch size of the inputs, allowing the matrices \(B,C\) and the step size \(\) to depend on the inputs. Therefore, it is a time-varying system and cannot use the convolution view. To ensure efficient training and inference with Mamba, techniques such as parallel scanning, kernel fusion, and recomputation are employed, resulting in two types of Mamba. One type is the SSM using the recursive view, referred to as RNN-like Mamba, and the other is the SSM utilizing parallel scanning, known as Transformer-like Mamba. RNN-like Mamba is akin to DS4 , wherein the complete trajectory is taken as a sample and fully inputted into the model for training. Utilizing this approach, which capitalizes on the ability to capture long-term dependencies, the inference speed can be significantly increased. During the inference process, it is sufficient to input only the current tuple (\(r_{t-1}\), \(a_{t-1}\), \(s_{t}\)) in conjunction with the hidden state \(h_{t}\). Transformer-like Mamba is a direct replacement for the transformer, where we consistently truncate the input sequences to a fixed length of \(K\) before their introduction into the model throughout the training and inference phases [53; 34; 56; 57].

### Hidden Attention in Mamba

Although the role of the self-attention mechanism in offline RL remains uncertain, it is known that this mechanism allows the model to dynamically focus on different parts of the input sequences, following the Equation (4).

\[(x)= V(x),=(}{}}),\] (4)

where \(Q,K,V\) represent queries, keys, and values respectively, i.e. input sequences after three linear transformations. \(d_{k}\) is the dimension of the keys. Similarly, current research suggests that the S6 layer in Mamba can be viewed as the hidden attention mechanism with a unique data-control linear operator . Assuming the initial condition \(h_{0}=0\), we can obtain a formula similar to Equation (2)

\[y_{i}=C_{i}_{j=1}^{i}_{k=j+1}^{i}_{k}_{j}x_ {j},\;\;h_{i}=_{j=1}^{i}_{k=j+1}^{i}_{k}_{j} x_{j},\] (5)

where \(_{i}=(_{i}(A))\), \(_{i}=_{i}(B_{i})\), and \(_{i}=(S_{}(x_{i}))\). \(B_{i}=S_{B}(x_{i})\), \(C_{i}=S_{C}(x_{i})\), with \(S_{B}\), \(S_{C}\) and \(S_{}\) are linear projection layers. Softplus is an elementwise function that is a smooth approximation of ReLU.

Since \(_{t}\) is a diagonal matrix,  simplifies the hidden matrices and gets the attention mechanism of Mamba:

\[(x)=x,_{i,j} _{i}_{i,j}_{j}\]

\[_{i}:=S_{C}(x_{i}),_{j}:=(S_{}(x_{j})S_{B} (x_{j}),_{i,j}:=_{k=j+1\\ S_{}(x_{k})>0}^{i}S_{}(x_{k})A.\] (6)Therefore, we can visualize the hidden attention matrices in DeMa, thus gaining a deeper understanding of the behavior inside the model in the setting of offline RL.

## 4 The Analysis of DeMa

Considering most trajectory optimization methods use short segments during both training and inference, _the compatibility of Mamba with these methods remains an open question_. As shown in Figure 1, this section presents an analysis from the perspectives of data structures and essential components. Section 4.1 discusses the impact of data structure on trajectory optimization. Our study reveals that the RNN-like DeMa does not offer substantial benefits in terms of effectiveness or efficiency. Therefore, we investigate three critical factors: sequence length, the hidden attention mechanism, and the input concatenation types. We find that the balance between performance and efficiency highly depends on the appropriate sequence length selection. Moreover, the input concatenation method significantly influences the results, with temporal concatenation (i.e., B3LD) demonstrating its effectiveness. Section 4.2 conducts ablation studies to identify the hidden attention mechanism as a key component of DeMa, facilitating better utilization and component replacement. Detailed experiments and additional results are in the **Appendix**. Our code is available at https://github.com/AndssY/DeMa.

### Input Data Structures

First, we compare the RNN-like DeMa (B3LD) with the Transformer-like DeMa (B3LD)2. The average results are shown in Table 1 (with detailed results in Appendix E), where the performance of the RNN-like DeMa is significantly inferior to that of the Transformer-like DeMa, especially in Atari games. These findings suggest that the recurrent mode may be unnecessary in trajectory optimization methods. Given that the hyper-parameters are identical for both types of DeMa except for the sequence length, we assume that variations in sequence length are likely the primary cause of the observed disparities in results. Therefore, we explore the effect of sequence length on the Transformer-like DeMa in subsequent sections.

Figure 1: Variant design of the DeMa in trajectory optimization. In the left portion, (I) represents the RNN-like DeMa (B3LD), which requires hidden state inputs at each decision step; (II) indicates the transformer-like DeMa (B3LD); and (III) refers to the transformer-like DeMa (BL3D). The right portion illustrates that both types of these DeMa can incorporate two distinct residual structures, i.e. the post up-projection residual block and the pre up-projection residual block.

How does sequence length affect the computational load?We investigate the impact of sequence length on single-step training time, single-step inference time and GPU memory usage for models including DT, Transformer-like DeMa, and RNN-like DeMa. Figure 2 shows that the Transformer-like DeMa operates faster than the RNN-like DeMa when dealing with short sequence lengths, despite that the inference time of RNN-like DeMa is independent of the sequence length. With conventional sequence lengths (such as 20), Transformer-like DeMa holds an advantage in forward speed, training speed, and GPU memory consumption.

How does sequence length affect the performance of DeMa?While the computational cost of Transformer-like DeMa increases linearly with the expansion of the sequence length, it is crucial to recognize that the increased computational cost may not ensure a corresponding enhancement in the model's performance. Transformer-like DeMa's Performance may plateau or even decline as the input sequence length exceeds a certain threshold. As illustrated in Figure 3, Transformer-like DeMa's performance reaches a plateau in MuJoCo  when the input sequence surpasses a specific length; while significantly deteriorates with excessively long input sequences in Atari.

Why does DeMa require merely short input sequences?We calculate the hidden attention scores in DeMa via Eq. (5)-(6), which reflect the importance of historical information to DeMa. Figure 4 shows the hidden attention scores of the last \(K\) tokens at each decision-making step (from the 300th to the 600th step). It can be seen that the attention scores exhibit exponential decay as the tokens become

  
**Env** & **DT** & **RNN-like DeMa** & **Transformer-like DeMa** \\  Atari & 62.2 & 67.3 & **111.8** \\ MuJoCo & 63.4 & 61.1 & **66.0** \\   

Table 1: The average result of DT, RNN-like DeMa and Transformer-like DeMa in Atari  and MuJoCo . The results are reported with the normalization following [60; 11]. Detailed results can be seen in Appendix E.

Figure 3: Comparison of Transformer-like DeMa’s Performance on Atari and MuJoCo Tasks. We report mean values averaged over 3 seeds, shaded areas represent deviations.

Figure 2: The impact of sequence length on single-step forward computation time, single-step training time, and GPU memory usage. The sequence length of RNN-like DeMa is 1000.

increasingly distant from the current decision-making moment, which aligns with the forgetting property of a Markov chain . What's more, the hidden attention across different decision steps exhibits a periodic pattern towards the current token, suggesting that the model may have learned kinematic features, as agents in these environments engage in periodic movements.3

**Which type of concatenation is suitable for DeMa?** Models like the Transformer and Mamba typically process inputs token by token. However, given an MDP, there are three elements \(s,a,r\) to consider. Therefore a significant design consideration is the method of concatenating these three elements into a suitable token format for the model. We experiment to investigate the suitable design for DeMa. By Table 2, concatenating the three elements in the temporal dimension yields better results. This may be due to the significant differences between the three elements of the MDP. As illustrated in , states and actions symbolize fundamentally dissimilar notions, concatenating them in the embedding dimension directly may make it more difficult for the model to recognize, leading to poorer results.

**Finding 4**: Concatenating state, action, and rg along the embedding dimension has a significant negative impact on the results.

### The Essential Components of DeMa

Aside from the perspective of input data, this section delves into DeMa from the standpoint of network components. We primarily investigate the following questions: (1) Considering that some DTs do not heavily rely on attention mechanism [13; 49], is the hidden attention mechanism crucial for DeMa? (2) As the Mamba block is an integration of the hidden attention mechanism with pre up-projection residual blocks , what impact will it have on the performance when integrating it with other residual structures (i.e. the post up-projection residual block in the transformer)? (3) With the inherent recurrent nature of SSM , does DeMa need position embedding? (Appendix G)

Is the hidden attention mechanism crucial for DeMa? shows that the transformer does not heavily rely on attention, and  finds the attention mechanism of DT is not suitable for RL. Given these insights, we aim to investigate whether a similar phenomenon exists in hidden attention.

  
**Game** & **BL3D** & **B3LD** \\  Breakout & 72.8\(\)10.6 & **314.7\(\)10.7** \\ Qbert & 32.2\(\)14.1 & **54.4\(\)6.8** \\ Pong & **101.9\(\)6.9** & 98.2\(\)12.0 \\ Seaquest & 1.3\(\)0.0 & **2.7\(\)0.002** \\ Asterix & 3.9\(\)0.3 & **7.8\(\)0.4** \\ Frostbite & 26.3\(\)20.9 & **31.1\(\)0.01** \\ Assault & 127.9\(\)7.1 & **169.4\(\)33.1** \\ Gopher & 190.3\(\)60.1 & **215.8\(\)29.2** \\ 
**Average** & 69.6 & **111.8** \\   

Table 2: Input concatenation types comparison: “BL3D” refers to the concatenation of input tokens across the embedding dimension, while “B3LD” indicates concatenation across the temporal dimension, as depicted in Figure 1. Outcomes are averaged across three random seeds.

Figure 4: Hidden attention scores of DeMa from the 300th to the 600th timestep in Hopper-medium-replay. The X-axis represents timesteps from 300 to 600, the Y-axis represents the past \(K\) tokens, and the Z-axis indicates the attention scores given to the \(K\) tokens at the time of the current decision. More can be seen in Appendix J.

[MISSING_PAGE_FAIL:8]

we explore the combination of hidden attention with post up-projection residual blocks in transformer. According to the results in Table 3 and Table 4, although the overall average results of DeMa are slightly better than those of DeMa with post., it is observable that they each have advantages in different environments. Hence, we believe that the performance differences when integrating with the two types of residual blocks are not statistically significant. It suggests that the structure of the residual blocks exerts minimal influence on the outcome. Given that both configurations yield a measurable performance improvement over the DT, it is reasonable to conclude that the hidden attention mechanism within DeMa plays a pivotal role.

## 5 Evaluations on Offline RL Benchmarks

In this section, we delve into a comparative analysis of DeMa's performance against various DTs. Our investigation primarily centers on the influence of disparate network architectures on the experimental outcomes. Consistent with antecedent studies, we assessed both discrete (Atari ) and continuous control tasks (MuJoCo ), presenting the normalized scores accordingly. Given that the sequence

  
**Dataset** & **Environment** & **CQL** & **DS4** & **RvS** & **DT** & **GDT** & **DeMa(Ours)** \\  M & HalfCheetah & 44.0 & 42.5 & 41.6 & 42.6 & 42.9 & 43\(\)0.01 \\ M & Hopper & 58.5 & 54.2 & 60.2 & 68.4 & 65.8 & **74.5\(\)2.9** \\ M & Walker & 72.5 & 78.0 & 71.7 & 75.5 & 77.8 & 76.6\(\)0.2 \\  M-R & HalfCheetah & 45.5 & 15.2 & 38 & 37.0 & 39.9 & 40.7\(\)0.03 \\ M-R & Hopper & 95.0 & 49.6 & 73.5 & 85.6 & 81.6 & **90.7\(\)6.1** \\ M-R & Walker & 77.2 & 69.0 & 60.6 & 71.2 & **74.8** & 70.5\(\)0.1 \\  M-E & HalfCheetah & 91.6 & 92.7 & 92.2 & 88.8 & 92.4 & **93.2\(\)0.01** \\ M-E & Hopper & 105.4 & 110.8 & 101.7 & 109.6 & 110.9 & **111\(\)0.03** \\ M-E & Walker & 108.8 & 105.7 & 106.0 & **109.3** & **109.3** & 106\(\)11.7 \\   & 77.6 & 68.6 & 71.7 & 76.4 & 76.8 & **78.5** \\   

Table 6: Results for MuJoCo. The dataset names are abbreviated as follows: ”medium” as “M”, ”medium-replay” as “M-R” and ”medium-expert” as “M-E”. The results are reported with the expert-normalized following .

    & **Complexity** & **DT** & **DC** & **DeMa(Ours)** \\   & Training time per step(ms) & 55 & **43** & 50 \\  & GPU memory usage(GiB) & 4.2 & **3.0** & 4.2 \\  & MACs & 12.1G/46.5G & 11.1G/40.6G & **8.8G/36.3G** \\  & All params \# & 2.35M & 1.94M & **1.7M** \\   & Training time per step(ms) & 56/58 & **53.6/53.9** & 57.6/58.8 \\  & GPU memory usage(GiB) & 0.65/0.8 & **0.55/0.6** & 1.0/1.0 \\   & MACs & 2.5G/9.5G & 1.6G/6.1G & **0.7G/2.1G** \\   & All params \# & 726.2K/2.6M & 536K/1.9M & **175.5K/500.0K** \\   

Table 7: The resource usage for training DT, DC and DeMa on Atari and MuJoCo.

length considerably affects the results, we selected the optimal outcomes from sequence lengths \(K=8\) to \(K=20\) for DeMa. The detailed hyper-parameters on DeMa are available in Appendix D. Our main results are shown in Table 5 and Table 6. DeMa achieves a significantly higher average score compared to DT in Atari games, while the number of parameters and the number of MACs in DeMa are each five times fewer than those in DT, as shown in Table 7. Moreover, DeMa has better scalability for input length which can be seen in Figure 2, it maintains a slow linear growth with the input sequence length increases while the computational cost of the Transformer grows quadratically. These results demonstrate that our transformer-like DeMa is well-suited for integration with trajectory optimization methods.

## 6 Conclusion

To investigate Mamba's compatibility with trajectory optimization, this work conducts comprehensive experiments from the aspect of data structures and network architectures. Our findings reveal that (1) DeMa benefits from short sequence lengths due to its exponentially decaying focus on sequences. Consequently, we incorporate a Transformer-like DeMa. (2) The hidden attention mechanism plays a crucial role in DeMa. It can combine with other residual structures and does not require position embedding. Based on the insights gained from the investigation, our DeMa surpasses previous methods, achieving higher performance over the DT while using 30% fewer parameters in eight Atari games. In the MuJoCo, our DeMa outperforms DT with only a quarter of the parameters. In conclusion, our DeMa is compatible with trajectory optimization in offline RL.

**Limitations.** We investigate the application of Mamba in trajectory optimization and present findings that provide valuable insights for the community. However, there remain several limitations: (1) Trajectory optimization tasks typically involve shorter input sequences, raising questions about how well the RNN-like DeMa performs in terms of memory capacity in RL compared to models such as RNNs and LSTMs. Furthermore, the potential of both types of DeMa warrants further exploration, particularly in some POMDP environments and long-horizon non-Markovian tasks that require long-term decision-making and memory. (2) We examine the importance of the hidden attention mechanism in Section 4.2, future work could leverage interpretability tools to examine further the causal relationship between memory and current decisions in DeMa, ultimately contributing to the development of interpretable decision models. (3) While we have assessed the properties of DeMa and identified improvements in both performance efficiency and model compactness compared to DT, it remains unclear whether DeMa is suitable for multi-task RL and online RL environments.