# Revisiting Score Propagation in Graph

Out-of-Distribution Detection

Longfei Ma\({}^{1}\), Yiyou Sun\({}^{2}\)1, Kaize Ding\({}^{3}\), Zemin Liu\({}^{1}\)2, Fei Wu\({}^{1}\)2

\({}^{1}\)Zhejiang University, \({}^{2}\)University of Wisconsin-Madison, \({}^{3}\)Northwestern University

{longfeima, liu.zemin, wufei}@zju.edu.cn,

sunyiyou@cs.wisc.edu, kaize.ding@northwestern.edu

Equal ContributionCorresponding author

###### Abstract

The field of graph learning has been substantially advanced by the development of deep learning models, in particular graph neural networks. However, one salient yet largely under-explored challenge is detecting Out-of-Distribution (OOD) nodes on graphs. Prevailing OOD detection techniques developed in other domains like computer vision, do not cater to the interconnected nature of graphs. This work aims to fill this gap by exploring the potential of a simple yet effective method - OOD score propagation, which propagates OOD scores among neighboring nodes along the graph structure. This post hoc solution can be easily integrated with existing OOD scoring functions, showcasing its excellent flexibility and effectiveness in most scenarios. However, the conditions under which score propagation proves beneficial remain not fully elucidated. Our study meticulously derives these conditions and, inspired by this discovery, introduces an innovative edge augmentation strategy with theoretical guarantee. Empirical evaluations affirm the superiority of our proposed method, outperforming strong OOD detection baselines in various scenarios and settings. To ensure reproducibility, we have made our code and relevant data publicly available at [https://github.com/longfei-ma/GRASP](https://github.com/longfei-ma/GRASP).

## 1 Introduction

Graph-like data structures are ubiquitous in many domains, such as social networks , molecular chemistry , and recommendation systems . As graph neural networks increasingly serve as powerful tools for navigating this complex data landscape, a compelling yet under-explored issue emerges: Out-of-Distribution (OOD) node detection. Imagine a recommender system suggesting irrelevant or even harmful products to users, or a bioinformatics algorithm misusing an unknown protein. This gives rise to the importance of OOD detection in graph data, which determines whether an input is in-distribution (ID) or OOD and enables the model to take precautions.

While existing OOD detection methods have shown promising results in computer vision , natural language procession  and tabular data analytics , their effectiveness diminishes when applied to graph data . These conventional techniques operate under the assumption that data points are independently sampled, which misaligns with the interconnected nature of graphs. To better leverage the structural knowledge from the graph, OOD score propagation  has been employed to enhance graph OOD detection performance by directly propagating the computed OOD scores along the graph structure (as shown in Figure 1). Although this strategy has shown promising results on some datasets,

Figure 1: Illustration of OOD scores propagation.

the reasons behind its effectiveness and the conditions under which it works are not clear. To dive deep into it, our research embarks on addressing two research questions related to OOD score propagation:

_Question 1: "Will naive OOD score propagation always help graph OOD Detection?"_ The short answer is no. This method can be ineffective on graphs where inter-edges (ID-to-OOD) are predominant. Using the examplse in Figure 2(a), where only inter-edges exist, prior to conducting score propagation, all ID and OOD nodes can be fully distinguished. However, after performing score propagation along these edges, the ID and OOD nodes are completely misclassified. Conversely, after adding intra-edges and making them dominate, as shown in Figure 2(b), score propagation would be beneficial to distinguish ID and OOD nodes. These two examples intuitively illustrate how the ratio of intra-edges and inter-edges can impact the effectiveness of OOD score propagation. We substantiate this intuition in Section 3. This finding naturally paves the way for subsequent questions.

_Question 2: "How to derive a better score propagation strategy for graph OOD detection?"_ Building on our prior findings, we propose a graph augmentation strategy as presented in Section 4. Specifically, our strategy selects a subset \(G\) of the training set and puts additional edges to the nodes within \(G\). Beyond its practical implications, our solution is theoretically supported: When \(G\) predominantly connects to ID data over OOD data, our strategy can provably enhance the post-propagation OOD detection outcomes.

We summarize our contributions as below:

* **Theoretical understanding:** We delve deeply into the mechanism of score propagation to understand its potential for graph OOD detection and elucidate the conditions under which it thrives, providing an understanding that extends beyond existing knowledge.
* **Practical solution:** To counter the identified challenge of inter-edges' domination, we propose **GR**aph-**Augmented **S**core **P**ropagation (GRASP), an innovative edge augmentation strategy with theoretical guarantee. By strategically adding edges to a chosen subset \(G\) of the training set, as detailed in Section 4, our method aims to enhance the intra-edge ratio, thereby boosting OOD detection outcomes post-propagation.
* **Empirical studies**: We demonstrate the superior performance of the proposed method on extensive graph OOD detection benchmarks, different pre-trained methodologies [34; 69; 7; 94], and different OOD scoring functions. Under the same condition, our proposed strategy substantially reduces the FPR95 by **17.87**% and **32.21**% compared to the strongest graph OOD detection baselines on common and large-scale benchmarks respectively. Comprehensive analyses are also provided to validate the effectiveness of the proposed approach and the correctness of the theoretical findings.

## 2 Preliminaries

**Problem setup**. We consider a traditional semi-supervised node classification setting with the additional unlabeled nodes from the out-of-distribution class. Let \(=\{,\}\) denote the graph with nodes \(\) and edges \(\), where the node set \(\) with size \(N\) are attributed with data matrix \(X^{N d}\). The structure of graph \(\) is described by the adjacency matrix \(A\{0,1\}^{N N}\). We let the corresponding row-stochastic matrices as \(=^{-1}A\), where \(D\) is the diagonal matrix with

Figure 2: Two illustrative examples when scoring propagation is harmful/helpful. We consider ID nodes in green and OOD nodes in red. Inter-edges are defined to be ID-to-OOD edges and Intra-edges are ID-to-ID or OOD-to-OOD edges. The value represents the respective OOD scores. Consequently, the propagated scores in these cases will be the mean of the scores of adjacent nodes as shown in Figure 1.

\(D_{ii}=_{j}A_{ij}\). The \(N\) nodes are partially labeled, so we let \(_{l}\) and \(_{u}\) represent the labeled and unlabeled node sets respectively, i.e, \(=_{l}_{u}\). Given a training set \(^{tr}=(_{i},y_{i})}_{i_{l}}\) with \(_{i}\) as the \(i\)-th row of \(X\) and \(y_{i}\{1,,C\}\), the goal of node classification is to learn a mapping \(f:^{C}\) from the nodes to the probability of each class.

**Out-of-distribution detection.** When deploying a model in the real world, a reliable classifier should not only accurately classify known in-distribution (ID) nodes, but also identify "unknown" nodes or OOD nodes. Formally, we can represent the unlabeled node set by \(_{u}=_{uid}_{uood}\) where \(_{uid}\) and \(_{uood}\) represent the in-distribution (ID) node and OOD node respectively. **The goal of the graph OOD detection** is to derive an algorithm to decide if a node \(i_{u}\) is from \(_{uood}\) or \(_{uid}\).

This can be achieved by having an OOD detector, in tandem with the node classification model \(f\). OOD detection can be formulated as a binary classification problem. At test time, the goal of OOD detection is to decide whether an unlabeled node \(i_{u}\) is from ID or OOD. The decision can be made via a level set estimation:

\[F_{OODD}(i,;)=&g(_{i}) \\ &g(_{i})<,\]

where nodes with higher scores \(g(_{i})\) are classified as ID and vice versa, and \(\) is the threshold commonly chosen so that a high fraction (e.g., 95%) of ID data is correctly classified.

In this paper, we consider **post hoc** OOD detection methods to produce \(g(_{i})\) which does not require expensive re-training. As an example, a classical way to compute \(g(_{i})\) is Maximum Softmax Probability (MSP)  which is given by the maximum softmax value. We include details of the considered OOD detection methods in Appendix B.

## 3 Will propagation always help Graph OOD Detection?

In the introduction, we delineate the limitations of OOD score propagation using a concrete example and elucidate the intuition that it may fail when inter-edges dominate. In this section, we formally delineate the conditions under which OOD score propagation works. We start by showing the formal definition of propagation.

**Define OOD scoring propagation**. Given a raw OOD scoring vector \(}^{N}\) with \(}_{i}=g(_{i})\), the propagated scoring vector is given by:

\[:=^{k}}, \]

where \(k^{+}\) are hyperparameters.

_Is it necessarily the case that \(\) outperforms \(}\)?_ The answer is **NO**. We elucidate with the theoretical insight below.

**Theoretical Insight.** As discussed in the Introduction from Figure 2, when the number of ID-to-ID and OOD-to-OOD edges surpasses that of ID-to-OOD edges, the propagation mechanism tends to "aggregate" the scores associated with the ID and OOD nodes respectively, which further amplify the separability between them. Conversely, when the number of ID-to-OOD edges are more than the other types of edges, the scores for both ID and OOD nodes become undistinguishable post-propagation.

The example above offers the insight that the relative performance of \(\) compared to \(}\) is contingent upon the structural dynamics of the network, specifically the distribution of edges. To formally articulate this relationship, we adopt a probabilistic framework for modeling edges. Specifically, we assume that the edge follows a Bernoulli distribution characterized by parameters \(_{intra}\) and \(_{inter}\) for intra-edges (ID-to-ID and OOD-to-OOD) and inter-edges (ID-to-OOD), respectively:

\[A_{ij}\{Ber(_{intra}),&i,j_{uid}i,j_{uood}\\ Ber(_{inter}),&i_{uid},j_{uood}j_{uid},i_{uood}.\]

In the context of probabilistic modeling, the subsequent Theorem 3.1 can be established to formalize the inherent understanding.

**Theorem 3.1**.: _(Informal) (a) When \(_{intra}_{inter}\), it is highly likely that the propagation algorithm will yield enhanced performance in OOD detection. (b) When \(_{intra}_{inter}\) or even \(_{intra}<_{inter}\), the score propagation is likely to be either ineffective or detrimental to the performance._

We also provide the formal version below (Theorem 3.2) which provides a mathematical foundation for understanding how varying the Bernoulli parameters influence the efficacy of the propagation in the context of OOD detection. We provide the detailed proof in Appendix A.

**Theorem 3.2**.: _(Formal) For any two test ID/OOD node set \(S_{id}_{wid},S_{ood}_{wood}\) with equal size \(N_{s}\), let the ID-vs-OOD separability \(_{sep}\) defined on an OOD scoring vector \(}^{N}\) as_

\[_{sep}(})_{i S_{id}}}_{i}-_{j S_{ood}}}_{j}.\]

_If \(_{sep}(})>0\) and \(_{intra}-_{inter}>1/N_{s}\), for some \(>0\) and constant \(c\), we have \((_{sep}(A})_{sep}( })-) 1-(-}{| }|^{2}_{s}})\)._

**Summary.** This section has presented a comprehensive theoretical evidence to substantiate the claim that propagation through the adjacency matrix \(A\) does not necessarily enhance out-of-distribution (OOD) detection in graphs. Moreover, Theorem 3.2 reveals that the critical factor in enhancing post-propagation performance lies in **improving the ratio of intra-edges** within the graph structure. These insights serve as a direct motivation for the augmentation strategy in the next section.

## 4 An Augmented Score Propagation Strategy

The findings from the preceding section give rise to a subsequent thought: "_Can we improve the propagation strategy for graph OOD detection performance?_" In an ideal scenario, if an oracle were to indicate that a particular subset in the test set belongs exclusively to the ID or OOD, one could augment the graph by adding intra-edges or removing inter-edges. This would consequently improve the ratio of intra-edges \(_{intra}\), leading to enhanced OOD detection performance post-propagation.

However, such an oracle does not exist in practical settings, and even approximating such a subset proves to be a difficult task. Existing literature has suggested the use of pseudo-labels assigned to nodes [36; 79; 2; 73; 55]. Nonetheless, these studies also caution that this approach is susceptible to "confirmation bias", whereby errors in estimation are inadvertently amplified.

To circumvent it, this paper proposes the solution for adding edges to a subset of the training set \(_{l}\), which is assured to be in-distribution data. We start by showing the theoretical underpinnings that adding such a subset can, under specified conditions, contribute to improved OOD detection performance after propagation.

### Theoretical Insight

Our approach involves adding the edges to a subset \(G\) of training data and then propagating the out-of-distribution (OOD) scoring vector using the enhanced adjacency matrix. Specifically, when edges are added to \(G\), this action can be mathematically represented as incorporating a perturbation matrix \(E=_{G}_{G}^{}\) into A, as demonstrated in Figure 3. Here, \(_{S}^{N}\) denotes an indicator vector for a set \(S\), where the vector takes the value of \(1\) if the index \(i S\) and value 0 otherwise. A sufficient condition for the efficacy of this augmentation strategy in enhancing post-propagation OOD detection performance is outlined in Theorem 4.1.

**Theorem 4.1**.: _(Informal) For a subset \(G\) in the training set, augmenting \(G\) by adding edges to all its nodes can lead to improved post-propagation OOD detection performance, provided that the following condition is met: \(G\) has more edges to ID data than OOD data._

We also provide the formal version below (Theorem 4.2) that incorporates a perturbation analysis. This analysis elucidates how edge augmentation in the training set can positively influence thepropagation algorithm's ability to enhance OOD detection. For the sake of the main intuition, we provide the analysis on \(A\) instead of \(\) for simplicity. We provide the detailed proof in Appendix A.

**Theorem 4.2**.: _(Formal) For any two test ID/OOD node set \(S_{id}_{uid},S_{ood}_{wood}\) with size \(N_{s}\), let the ID-vs-OOD separability \(_{sep}\) defined on a non-negative OOD scoring vector \(}^{N}\) as_

\[_{sep}(})_{i S_{id}}}_{i}-_{j S_{ood}}}_{j}.\]

_Let \(_{S S^{}}\) to denote the edge set of edges between two node sets \(S\) and \(S^{}\), where \(S,S^{}\). If we can find a node set \(G_{l}\) such that \(|_{G S_{id}}|>|_{G S_{ood }}|\), we have_

\[_{sep}((A+ E)^{2}})>_{sep}(A^{2} }),\]

_where \(E=_{G}_{G}^{}\) and \(>0\)._

The Theorem 4.2 shows a critical principle for enhancing propagation: the optimal strategy entails the addition of edges to the subset \(G\) such that there are more edges to ID data than OOD data. For some \(S_{id},S_{ood}\) in the test set, the goal is to find the set

\[G_{*}=*{arg\,max}_{S_{l},|S|=N_{g}}_{S S_{id}}|}{|_{S S_{ood }}|}, \]

where \(N_{g}\) is a hyperparameter to control the size of \(G_{*}\). Inspired by the optimization target, we proceed to present our pragmatic algorithmic approach.

### Graph-Augmented Score Propagation (GRASP)

Our augmentation approach hinges on the selection of a subset, \(G\), from the training set, as exemplified in Equation 2. Two principal challenges arise in implementing this: (1) We cannot directly determine the number of edges linked to ID/OOD data because these reside in the test set and their labels remain unknown. (2) An exhaustive search to find a subset is computationally expensive, as the number of combinatorial possibilities increases in a factorial manner. In this paper, we tackle these challenges by providing the practical approximation method.

**Selection of \(}/}\).** Our discussion begins by detailing the methodology to select the subset from the test ID/OOD dataset, symbolized by \(S_{id}\) and \(S_{ood}\) in Equation 2. A straightforward approach to obtain the most likely ID is by selecting nodes with the largest confidence and the least for OOD in class predictions. Following , we employ the max softmax probability (MSP) as a representation of confidence. The selected sets can be defined as:

\[S_{id}=\{i_{u}|_{c[C]}f_{c}(i)>_{}\},S_{ood }=\{j_{u}|_{c[C]}f_{c}(j)<_{100-}, \]

where \(_{}\) denotes the \(\)-th percentile of the MSP scores corresponding to nodes in \(_{u}\). To offer a clear view, Figure 4 portrays \(S_{id}\) and \(S_{ood}\) in the marginal regions highlighted in orange. Selecting a subset in the leftmost and rightmost regions reduces the error when identifying the ID/OOD subsets, given that overlapping between ID and OOD predominantly occurs around the central region of the distribution.

**Selection of \(G\).** Upon establishing \(S_{id}\) and \(S_{ood}\), the next step is to determine \(G\) using Equation 2. Directly enumerating every possible \(G\) is impractical. Instead, we adopt a greedy approach, prioritizing the node with the highest "likelihood" score. To elucidate, for each node \(i_{l}\), the score can be

Figure 3: The augmentation procedure.

computed as the ratio of the edge count to \(S_{id}\) over \(S_{ood}\):

\[h(i)=|_{\{i\} S_{id}}|/(|_{\{i\} S _{ood}}|+1), \]

where we incorporate an addition of 1 in the denominator to circumvent division by zero. Subsequently, \(G\) can be expressed as:

\[G=\{i_{l}|h(i)>_{}\}, \]

where \(_{}\) stands for the \(\)-th percentile of \(h(i)\) scores for nodes in \(_{l}\). Once \(G\) is defined, edge augmentation can be executed as demonstrated in Section 4.1. The OOD score is then propagated with the new adjacency matrix \(A_{+}=_{G}_{G}^{}\) in place:

\[_{GRASP}=(_{+})^{k}, \]

where \(k^{+}\) are hyperparameters.

**Complexity analysis.** While our algorithm introduces the fully connected matrix \(E\), our method can be effciently implemented by matrix-vector multiplication, leading to computational footprint in terms of runtime and memory usage with \((N+2k||+n)\) and \((N+||+n)\) respectively after propagation for \(k\) times, where \(n\) is node count of subset \(G\). We provide the comprehensive complexity analysis in Appendix D.6.

## 5 Experiments

**Datasets.** We conduct extensive experiments using 10 real-world datasets that span diverse domains, scales, and structures (homophily or heterophily). A high-level summary of the dataset statistics is provided in Table 1, with a detailed information of the datasets and the comprehensive description of ID/OOD split in Appendix C. Specifically, Cora  serves as a widely recognized citation network. Amazon-Photo represents a co-purchasing network on Amazon. Coauthor-CS portrays a coauthor network within the realm of computer science. Moreover, Chameleon and Squirrel are two notable Wikipedia networks, predominantly utilized as heterophilic graph benchmarks. We additionally incorporate 5 **large-scale** graphs to evaluate our proposed methods: Reddit2 and ogbn-products are large homophily datasets; ArXiv-year, Snap-patents, and Wiki are recently proposed large-scale heterophily benchmarks.

**Remark on homophily/heterophily.** In Table 1, datasets are also categorized based on the attribute of homophily, denoting the tendency of nodes with the same class to connect. Conversely, the heterophily graph demonstrates a tendency for nodes of disparate classes to connect. This characteristic not only presents a challenge for node classification but also for graph OOD detection. The underlying

   Dataset & \(|_{l}|\) & \(|_{val}|\) & \(|_{used}|\) & \(C\) & Scale & Homph \\  Cora & 180 & 724 & 18K & 3 & SM & ✓ \\ Amazon-Photo & 618 & 2K & 4K & 3 & SM & ✓ \\ Count-CS & 2K & 10K & 5K & 11 & SM & ✓ \\ Chameleon & 272 & 1K & 916 & 3 & SM & ✗ \\ Squirrel & 622 & 2K & 2K & 3 & SM & ✗ \\ Reddit2 & 33K & 133K & 65K & 11 & LG & ✓ \\ ogbn-products & 130K & 522K & 1M & 11 & LG & ✓ \\ ArXiv-year & 23K & 92K & 53K & 3 & LG & ✗ \\ Snap-patents & 35K & 1M & 1M & 3 & LG & ✗ \\ Wiki & 212K & 850K & 862K & 3 & LG & ✗ \\   

Table 1: Summary statistics of the datasets: size of the training set \(|_{l}|\), test ID set \(|_{uid}|\), test OOD set \(|_{uood}|\); number of ID classes \(C\), scale of the dataset, and whether the graph is homophily.

Figure 4: Illustration of the rationale in selecting \(S_{id}\) and \(S_{ood}\). MSP score is reported on Dataset Coauther-CS with the division of ID and OOD classes introduced in Appendix C.

reason is that the OOD data is from different classes with ID, and heterophily exacerbates the ratio of inter-edge connections between ID and OOD, which is deemed undesirable for graph OOD detection according to Theorem 3.2.

**Implementation Details.** Our graph OOD detection technique operates in a _post hoc_ fashion utilizing a pre-trained network and so can be used in various pre-trained network seamlessly. We present results evaluated on Graph Convolutional Network (GCN)  in the main paper to save space and put detailed results of other architectures in Appendix D.3. All pre-trained models possess a layer depth of 2. With the pre-trained network, we proceed to execute the graph OOD detection. By default, we report the performance of the augmented propagation (GRASP) on the MSP score . The compatibility with other OOD scoring functions is also shown in Table 6. We set the propagation number \(k\) as 8, with percentile values \(=5\) and \(=50\). The sensitivity analysis of the hyper-parameters is included in Appendix D.4.

**Metrics.** Following the convention in literature [23; 47; 65], we use AUROC and FPR95 as evaluation metrics for OOD detection.

### Main Results

**GRASP consistently achieves superior performance.** We provide results of 5 common small-scale benchmarks and 5 **large-scale** datasets in Table 2 and Table 3 respectively, wherein only the averaged results over 5 runs are presented to save space and the detailed results with standard errors of these two scale datasets are shown in Table 12 and Table 13 respectively. From the results we can see that our proposed methodology (GRASP) consistently demonstrates promising performance. The comparative analysis encompasses a broad spectrum of _post hoc_ competitive Out-of-Distribution (OOD) detection techniques in existing literature and training-based methods tailored for graph OOD detection. We categorize the baseline methods into two groups: (a) Traditional OOD detection methods including MSP , Energy, ODIN, Mahalanobis, and KNN; (b) Graph OOD detection methods including GKDE, GPN, OODAT, and GNNSafe. In these tables, we present GRASP results based on the MSP score. Noteworthy findings include: (a) The traditional OOD detection methods exhibit suboptimal performance in the realm of graph OOD detection. For instance, GRASP reduced the average FPR95 by **17.87**% and **32.21**% compared to the strongest traditional OOD detection method GNNSafe and Mahalanobis on common and large-scale benchmarks, respectively. This outcome is anticipated given their lack of specificity in design towards graph data. (b) GRASP outperforms existing baselines by a large margin, surpassing the best baseline GNNSafe by **17.87**% and **40**% concerning average FPR95 on two scale benchmarks respectively. These results further corroborate that the theoretically motivated solution GRASP is also appealing to use in practice.

**GRASP is also competitive on large-scale graph datasets.** Contrasted with the small-scale benchmarks in Table 2, the large-scale scenario in Table 3 presents more challenges due to a large number of nodes and edges. From Table 3 we can see that all baseline OOD detection methodologies exhibit suboptimal performance on large-scale benchmarks, while our method GRASP robustly performs the best.

**GRASP exhibits significant advantages over training-based baselines.** In addition to contrasting with _post hoc_ methods, we extend our comparison to a parallel line of graph Out-Of-Distribution

    &  &  &  &  &  &  \\  & **FPR** & **AROC** & **FPR** & **AUROC** & **FPR** & **AUROC** & **FPR** & **AUROC** & **FPR** & **AUROC** & **FPR** & **AUROC** \\  MSP & 70.86 & 84.56 & 49.46 & 28.93 & 28.82 & 94.34 & 85.70 & 57.96 & 94.68 & 48.51 & 65.86 & 74.94 \\ Energy & 67.54 & 85.47 & 42.13 & 90.82 & 20.29 & 95.67 & 88.06 & 59.20 & 93.98 & 45.07 & 62.40 & 7.14 \\ KNN & 90.20 & 70.94 & 63.69 & 84.71 & 51.24 & 90.13 & 93.38 & 57.90 & 94.72 & 54.68 & 7.95 & 71.67 \\ ODIN & 68.41 & 84.89 & 48.08 & 89.90 & 22.99 & 59.27 & 58.31 & 57.94 & 94.17 & 44.08 & 62.91 & 74.43 \\ Mahalanobis & 69.68 & 58.48 & 96.49 & 75.58 & 85.71 & 84.95 & 95.55 & 53.19 & 94.90 & 54.99 & 88.47 & **70.84** \\ GKDE & 63.31 & 86.27 & 62.39 & 77.26 & 25.48 & 95.13 & 92.93 & 04.14 & 96.71 & 49.38 & 72.02 & 71.64 \\ GPN & 58.45 & 82.93 & 72.95 & 82.63 & 34.11 & 93.82 & 82.25 & 68.20 & 95.58 & 48.38 & 68.67 & 75.19 \\ OODAT & 94.59 & 53.63 & 71.34 & 66.95 & 96.53 & 52.18 & 94.43 & 59.67 & 95.27 & 46.13 & 90.43 & 55.71 \\ GNNSafe & 54.71 & 87.52 & 22.39 & 96.27 & 16.64 & 95.82 & 100.00 & 50.42 & 100.00 & 35.88 & 58.75 & 73.18 \\
**GRASP (Ours)** & **29.70** & **93.50** & **14.38** & **96.68** & **7.84** & **97.75** & **66.88** & **76.93** & **85.59** & **61.09** & **40.88** & **85.19** \\   

Table 2: **Main results on common benchmarks. Comparison with competitive out-of-distribution detection methods on pre-trained GCN. We take the average values that are percentages over 5 independently trained backbones. \(\) (\(\)) indicates larger (smaller) values are better.**(OOD) detection research, which focuses on refining the training strategy to improve graph OOD detection performance. The compared methods include GKDE , GPN  and OODGAT . While these approaches necessitate a costly re-training procedure, they perform mediocrely across all small-scale datasets and even run out-of-memory on almost all large-scale benchmarks, rendering them impractical for real-world deployment.

**GRASP is performant on challenging heterophily datasets.** As indicated in Table 2 and 3, GhNNsafe, which performs well on homophily datasets, experiences significant degradation on the difficult heterophily benchmarks due to its naive propagation mechanism. In contrast, GRASP maintains optimal performance on these hard scenarios.

### A Comprehensive Analysis of GRASP

**Ablation study on augmentation policy.** Recall that the key part of our method GRASP is the augmentation policy that consists in adding edges to the training nodes with top 50% scores of \(h(i)\), which corresponds to the nodes on the right side of Figure 5. We ablate the contributions of \(h(i)\) by comparision with alternative augmentation approaches that utilize \(h(i)\) differently in Table 4, specifically, (1) selecting 50% of training nodes with the lowest \(h(i)\) values (left side of Figure 5), (2) randomly selecting 50% of training nodes, corresponding to randomly picking node indices from the x-axis of Figure 5, (3) directly adding edges to \(S_{id}\) and \(S_{ood}\) within the test set (_i.e._, TestAug), and (4) a classic graph augmentation method named GAug , which adds or removes edges based on an edge predictor that disregards \(h(i)\) completely. We have the following key observations:

(\(a\)) _Selection by \(h(i)\) is effective._ For example, our strategy using the top 50% scores of \(h(i)\) outperforms that uses random 50%, which, in turn, outperforms the low 50% way. This is because a higher score of \(h(i)\) implies higher edge count towards ID data than to OOD data, which can increase the ratios of intra-edges and improve OOD detection performance after propagation. In contrast, the alternative augmentation method GAug, which does not consider the score \(h(i)\) at all, performs even worse than the Low 50% policy.

  
**Strategy** & **Cora** & **Amazon** & **Coauthor** & **Chameleon** & **Squirrel** & **Average** \\  GAug & 64.94 & 74.38 & 91.41 & 63.79 & 47.96 & 68.50 \\ TestAug & 59.24 & 75.78 & 95.00 & 50.58 & 48.64 & 65.85 \\ Low 50\% & 88.84 & 95.02 & 97.21 & 54.60 & 53.52 & 77.84 \\ Random 50\% & 90.23 & 95.37 & 97.45 & 65.32 & 57.59 & 81.19 \\ Top 50\% (**Ours**) & **93.50** & **96.68** & **97.75** & **76.93** & **61.09** & **85.19** \\   

Table 4: Ablation study on OOD detection performance by different augmentation policy. We report averaged AUROC over 5 independently pre-trained GCN models.

Figure 5: Illustration of the number of edges from each training node \(i\) to \(S_{id}\) and \(S_{ood}\) on Chameleon dataset. The x-axis denotes the training node indices, ordered by \(h(i)\) from low to high.

    &  &  \\  &  &  &  &  &  &  \\  & **FPR** & **AIAROC1** & **FPR1** & **AIAROC1** & **FPR1** & **AIAROC1** & **FPR1** & **AIAROC1** & **FPR1** & **AIAROC1** \\  MSP & 96.59 & 46.61 & 86.87 & 190 & 95.03 & 47.24 & 94.31 & 46.69 & 99.56 & 46.70 & 93.65 & 53.15 \\ Energy & 96.77 & 44.13 & 85.09 & 68.13 & 94.10 & 51.35 & 96.82 & 46.03 & 97.31 & 29.02 & 94.02 & 47.73 \\ KNN & 90.78 & 66.74 & 84.25 & 73.85 & 95.35 & 57.96 & 90.54 & 53.45 & 93.43 & 43.69 & 90.86 & 59.08 \\ ODIN & 96.74 & 44.69 & 85.65 & 68.95 & 95.06 & 47.36 & 94.27 & 45.20 & 97.88 & 29.91 & 93.92 & 47.22 \\ Mahalanobis & 71.73 & 74.89 & 00.04 & OOM & 88.60 & 95.97 & 96.03 & 58.50 & 72.33 & 67.95 & 82.17 & 65.23 \\ GKDE & OOT & OOT & OOM & OOM & OOM & OOM & OOM & OOM & OOM & - & - \\ GPN & OOM & OOM & OOM & OOM & 95.62 & 80.97 & OOM & OOM & OOM & OOM & 95.62 & 50.97 \\ OODGAT & OOM & OOM & OOM & OOM & 92.90 & 59.38 & OOM & OOM & OOM & OOM & 92.90 & 59.38 \\ GNNSafe & 99.49 & 31.99 & 77.86 & 85.66 & 100.00 & 33.30 & 99.92 & 27.35 & 72.63 & 60.32 & 89.98 & 48.12 \\
**GRASP (Ours)** & **2.41** & **98.50** & **39.77** & **93.79** & **73.93** & **81.24** & **75.22** & **72.13** & **58.49** & **77.97** & **49.96** & **84.73** \\   

Table 3: **Main results on large-scale benchmarks. Comparison with competitive out-of-distribution detection methods on pre-trained method GCN. We take the average values that are percentages over 5 independently trained backbones. OOM means Out-Of-Memory and OOT denotes that no results have been got after running over 48 hours for each run. \(\) (\(\)) indicates larger (smaller) values are better.**(\(b\)) _Directly adding edges to \(S_{id}\) and \(S_{ood}\) within the test set is sub-optimal_. Specifically, employing this method leads to nearly **20**% lower than that achieved with GRASP, which substantiates the notion that "confirmation bias" can adversely affect the graph OOD detection.

Overall, the ablation study suggests that our proposed augmentation policy is crucial to OOD detection performance.

**GRASP can effectively boost performance of challenging nodes connected by inter-edges.** As stated in the introduction 1, the reason OOD score propagation does not always work is the confusion between ID and OOD nodes resulting from propagation along the inter-edges. For example in heterophily datasets, where connected nodes tend to possess different labels, OOD nodes are more likely to appear on the inter-edges. To assess the capability of our proposed augmentation propagation method to address this challenge, we present in Table 5 the accuracy of detecting OOD nodes connected by intra-edges (intra) and inter-edges (inter) respectively, using the original MSP without any propagation, naive propagation based on MSP (MSP+prop), and our proposed augmentation propagation (MSP+GRASP) respectively. From the results we can see that naive propagation performs well only on strong homophily datasets, while on strong heterophily datasets (datasets highlighted in bold in the table), its performance is even worse than without propagation, as expected. In contrast, employing our augmentation method still results in substantial performance gain after propagation on these challenging datasets.

**GRASP is compatible with a wide range of OOD scoring methods.** In Table 6, we demonstrate the compatibility of GRASP with various alternative scoring functions. Each method generates OOD scores to form a scoring vector; GRASP is then applied to facilitate score propagation. The use of GRASP markedly surpasses the performance of its non-augmented counterpart across all datasets.

**Remark on other empirical findings.** We include other empirical findings in Appendix. Specifically, in Table 14, we prove that GRASP also achieves superior performance on the other GNN architectures in the literature. In Figure 7, we demonstrate a strong positive correlation between ratios of intra-edges and the corresponding OOD detection performance, validating the correctness of Theorem 3.2.

  
**Method** & **Cora** & **Amazon** & **Coauthor** & **Chameleon** & **Squirrel** \\  MSP & 84.56 & 89.34 & 94.34 & 57.96 & 48.51 \\ MSP+prop & 88.02 & 95.32 & 97.15 & 50.35 & 36.21 \\ MSP+**GRASP** & **93.50** & **96.68** & **97.75** & **76.93** & **61.09** \\  Energy & 85.47 & 90.28 & 95.67 & 59.20 & 45.07 \\ Energy+prop & 87.52 & 96.27 & 95.82 & 50.42 & 36.49 \\ Energy+**GRASP** & **88.34** & **96.35** & **96.64** & **62.04** & **60.66** \\  KNN & 70.94 & 84.71 & 90.13 & 57.90 & 54.68 \\ KNN+prop & 73.70 & 92.36 & 95.47 & 49.76 & 53.99 \\ KNN+**GRASP** & **91.48** & **97.43** & **96.52** & **76.32** & **60.24** \\   

Table 6: GRASP is compatible with different OOD scoring functions. We compare OOD detection methods and the performance after the simple propagation in Equation 1 (denoted by “+ prop”) and with GRASP respectively. We report AUROC results that are averaged over 5 independent pre-trained GCN models.

    &  &  &  \\  & intra & inter & intra & inter & intra & inter \\  cora & 48.06 & 46.90 & 60.91 & 48.31 & 78.42 & 50.48 \\ amazon & 66.70 & 57.82 & 81.80 & 68.58 & 89.56 & 78.66 \\ coauthor & 87.72 & 73.30 & 93.88 & 81.37 & 94.85 & 83.21 \\
**chameleon** & 57.48 & 51.22 & 55.63 & 51.02 & 70.79 & 55.19 \\
**squirrel** & 44.76 & 38.45 & 43.74 & 37.04 & 49.89 & 41.69 \\ reddit2 & 64.85 & 54.85 & 65.70 & 54.99 & 96.56 & 93.63 \\ ogbn-product & 31.72 & 37.73 & 37.10 & 43.63 & 67.95 & 64.30 \\
**arxiv-year** & 60.18 & 8.34 & 62.93 & 4.27 & 70.42 & 16.21 \\
**snap-patents** & 61.49 & 5.76 & 64.12 & 0.37 & 67.24 & 9.89 \\ wiki & 56.58 & 48.39 & 60.62 & 52.06 & 69.13 & 61.19 \\   

Table 5: GRASP consistently enhances the OOD detection performance of nodes connected by both inter-edges and intra-edges on datasets characterized by a strong degree of heterophily (datasets highlighted in bold in the table). However, naive propagation tends to compromise the performance of these nodes.

We also show in Table 15 that GRASP can increase the ratio of intra-edges after augmentation, consequently boosting OOD performance. This substantiates the correctness of Theorem 4.2. What's more, we conduct a thorough accounting of computation/memory demands compared to baselines on all scale datasets in Table 16 and Table 17 respectively, which underscore the strong practicality of our approach. Lastly, in Appendix D.7, we investigate the other propagation mechanisms in the literature and their impact on graph OOD detection performance emperically.

## 6 Related Work

**Out-of-distribution Detection.** The primary focus within this realm has been on the development of scoring functions for OOD detection. These works can be broadly categorized into two main streams: (1) output-based methods , and (2) feature-based methods including the Mahalanobis distance  and KNN distance . These methodologies are predominantly applied in domains such as computer vision, where samples are inherently independent of each other. However, these techniques are not designed to adeptly handle data structures like graphs, where samples are inter-connected.

**Out-of-distribution detection for graph data.** Graph anomaly detection has a rich history . In recent years, the OOD detection in graph data introduced fresh challenges, particularly with multi-class classification for in-distribution data, escalating the difficulty in discerning outlier data. Some of the works focus on graph-level OOD detection . For node-level OOD detection, GKDE  and GPN  apply Bayesian Network models to estimate uncertainties to detect OOD nodes. However, Bayesian-based approaches can encounter impediments such as inaccurate predictions and high computational demands, which limit their broader applicability . GNNSafe  emerges as the work employing post hoc energy-based score to perform OOD detection. Given the merits of post hoc methods, our study first provides a comprehensive understanding of the OOD score propagation in Graphs, extending beyond existing knowledge.

**Graph Data Augmentation.** Graph Data Augmentation is a common technique in graph machine learning  to improve the node classification performance. Existing methods operate exclusively on in-distribution (ID) data. Furthermore, their test set data also originates from the in-distribution and shares the same classes as the training set. In contrast, our data augmentation is purposefully crafted for OOD detection, supported by the theoretical explanation.

## 7 Conclusions

In this research, we delve into an important yet under-explored challenge in the realm of graph data: Out-of-Distribution (OOD) detection. Recognizing the inadequacies of traditional OOD detection techniques in the context of graph data, our exploration centered on the potential of score propagation as a viable and efficient solution. Our findings reveal the specific conditions under which score propagation will be helpful--in situations where the ratio of intra-edges surpasses that of inter-edges. Motivated by this finding, our edge augmentation strategy selectively adds edges to a specific subset \(G\) of the training set, which provably improves post-propagation OOD detection outcomes under certain conditions. Extensive empirical evaluations reinforced the merit of our approach.

## 8 Acknowledgements

This work was supported by the National Natural Science Foundation of China (62441605), and the Starry Night Science Fund of Zhejiang University Shanghai Institute for Advanced Study (SNZJUSIAS-0010).