# Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption

Yige Hong\({}^{1}\)   Qiaomin Xie\({}^{2}\)   Yudong Chen\({}^{2}\)   Weina Wang\({}^{1}\)

\({}^{1}\)Carnegie Mellon University  \({}^{2}\)University of Wisconsin-Madison

{yigeh,weinaw}@cs.cmu.edu

{qiaomin.xie,yudong.chen}@wisc.edu

Corresponding author

###### Abstract

We study the infinite-horizon restless bandit problem with the average reward criterion, in both discrete-time and continuous-time settings. A fundamental goal is to efficiently compute policies that achieve a diminishing optimality gap as the number of arms, \(N\), grows large. Existing results on asymptotic optimality all rely on the uniform global attractor property (UGAP), a complex and challenging-to-verify assumption. In this paper, we propose a general, simulation-based framework, Follow-the-Virtual-Advice, that converts any single-armed policy into a policy for the original \(N\)-armed problem. This is done by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. Our framework can be instantiated to produce a policy with an \(O(1/)\) optimality gap. In the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that violate UGAP. More notably, in the continuous-time setting, we do not require _any_ additional assumptions beyond the standard unichain condition. In both settings, our work is the first asymptotic optimality result that does not require UGAP.

## 1 Introduction

The restless bandit (RB) problem is a dynamic decision-making problem that involves a number of Markov decision processes (MDPs) coupled by a constraint. Each MDP, referred to as an arm, has a binary action space, {passive, active}. At every decision epoch, the decision maker is constrained to select a fixed number of arms to activate, with the goal of maximizing the expected reward accrued. The RB problem finds applications across a spectrum of domains, including wireless communication , congestion control , queueing models , crawling web content , machine maintenance , clinical trials , to name a few.

In this paper, we focus on infinite-horizon RBs with the average-reward criterion. Since the exact optimal policy is PSPACE-hard to compute , it is of great theoretical and practical interest to focus on policies that approximately achieve the optimal value and compute such policies in an efficient matter. The _optimality gap_ of a policy is defined as the difference between its average reward per arm and that of an optimal policy. In a typical asymptotic regime where the number of arms, \(N\), grows large, we say that a policy is _asymptotically optimal_ if its optimality gap is \(o(1)\) as \(N\).

Prior work and the uniform global attractor property assumption.Prior work has studied the celebrated Whittle index policy  and LP-Priority policies  and established sufficient conditions for their asymptotic optimality . One key assumption underpinning all prior work is the _uniform global attractor property_ (UGAP)--also known as globally asymptotic stability--which pertains to the mean-field/fluid limit of the restless bandit system in theasymptotic limit \(N\). UGAP stipulates that the system's state distribution in the mean-field limit converges to the optimal state distribution attaining the maximum reward, from any initial distribution. It has been well recognized that UGAP is a highly technical assumption and challenging to verify: the primary way to test UGAP is numerical simulations; see  for a detailed discussion.

More recent work studies the _rate_ at which the optimality gap converges to zero. The work  and  prove a striking \(O((-cN))\) optimality gap for the Whittle index policy and LP-Priority policies, respectively, where \(c\) is a constant. In addition to UGAP, these results require a non-singularity or non-degenerate condition. We are not aware of any rate of convergence result without assuming UGAP or non-singularity/degeneracy. See Table 1 for a summary.

Therefore, prior work on average-reward restlest bandit leaves two fundamental questions open:

1. Is it possible to achieve asymptotic optimality without UGAP?
2. Is is possible to establish a non-trivial convergence rate for the optimality gap in the absence of the non-singular/non-degenerate assumption (and UGAP)?

Our contributions.We consider both the discrete-time and continuous-time settings of the average-reward restless bandit problem. We propose a general, simulation-based framework, Follow-the-Virtual-Advice (FTVA) and its continuous-time variant FTVA-CT, which convert any single-armed policy into a policy for the original \(N\)-armed problem, with a vanishing performance loss. Our framework can be instantiated to produce a policy with an \(O(1/)\) optimality gap, under the conditions summarized in Table 1, which we elaborate on later.

Under our framework, computing an asymptotically optimal policy is efficient since it reduces to deriving an optimal single-armed policy, whose complexity is independent of \(N\). The resultant policy can be implemented with a linear-in-\(N\) computational cost and some of its subroutines can be implemented in a distributed fashion over the arms (see Appendix B for more details).

Our results can also be extended to RBs with heterogeneous arms. See Appendix H for details.

We now elaborate on the conditions in Table 1. In the discrete-time setting, our result holds under a condition called the Synchronization Assumption (SA), in addition to the standard unichain assumption required by all prior work. The SA condition, which is imposed on the MDP associated with a single arm, admits several intuitive sufficient conditions. While it is unclear whether SA subsumes UGAP, we show that there exist problem instances that violate UGAP but satisfy SA. Figure 1 shows one such instance (constructed by , described in Appendix G for ease of reference), in which the Whittle Index and LP-Priority policies coincide and have a non-diminishing optimality gap, whereas our policy, named as FTVA(\(^{*}\)), is asymptotically optimal. In addition, our \(O(1/)\) bound on the optimality gap does not require a non-singularity/non-degeneracy assumption.

Figure 1: An discrete-time RB problem that satisfies SA but not UGAP.

    & Paper & Policy & Optimality Gap & Conditions\({}^{*}\) \\   &  & Whittle Index & \(O((-cN))\) & UGAP \& Non-singular \\   &  & LP-Priority & \(O((-cN))\) & UGAP \& Non-degenerate \\   & This paper & FTVA(\(^{*}\)) & \(O(1/)\) & SA \\   &  & Whittle Index & \(o(1)\) & UGAP \\   &  & LP-Priority & \(o(1)\) & UGAP \\   &  & LP-Priority & \(O(1)\) & UGAP \\   &  & LP-Priority & \(O((-cN))\) & UGAP \& Non-singular \\   &  & LP-Priority & \(O((-cN))\) & UGAP \& Non-degenerate \\   & This paper & FTVA-CT(\(^{*}\)) & \(O(1/)\) & â€“ \\   

Table 1: Optimality gap results and conditions. \({}^{*}\)All papers require the standard unichain assumption.

More notably, in the continuous-time setting, we _completely eliminate the UGAP assumption_. We show that our policy \(\)-\((^{*})\) achieves an \(O(1/)\) optimality gap under only the standard unichain assumption (which is required by all prior work).

In both settings, our results are the first asymptotic optimality results that do not require UGAP. We point out that UGAP is considered necessary for LP-Priority policies . Our policy, \((^{*})\), is not a priority policy. As such, we hope our results open up new directions for developing new classes of RB algorithms that achieve asymptotic optimality without relying on UGAP.

**Intuitions.** Our algorithm and many existing approaches solve an LP relaxation of the original problem. The solution of the LP induces a policy and gives an "ideal" distribution for the system state. Existing approaches directly apply the LP policy to the current system state, and then perform a simple rounding of the resulting actions so as to satisfy the budget constraint. When the current system state is far from the ideal distribution, the actual actions after rounding may deviate substantially from the LP solution and thus, in the absence of UGAP, would fail to drive the system to the optimum.

Our \(\) framework, in contrast, prioritizes constraint satisfaction. We apply the LP solution to the state of a _simulated_ system, which is constructed carefully so that the resulting actions satisfy the constraint after a minimal amount of rounding. Consequently, starting from any initial state, our policy steers the system towards the ideal distribution and hence approximates the optimal value.

This method is inspired by the approach in , which also involves a simulated system, but for a stochastic bin-packing problem.

**Other related work.** Another condition extensively discussed in the RB literature is the indexability condition , which is necessary for the Whittle index policy to be well-defined, but not required by LP-Priority policies . However, indexability alone does not guarantee the asymptotic optimality of Whittle index.

So far we have discussed prior work on infinite-horizon RBs with average reward. For the related setting of finite-horizon total reward RBs, a line of recent work has established an \(O(1/)\) optimality gap , and an \(O((-cN))\) gap assuming non-degeneracy . For the infinite-horizon discounted reward setting,  propose policies with \(O(1/)\) optimality gap without assuming indexability and UGAP. While these results are not directly comparable to ours, it is of future interest to see if our simulation-based framework can be applied to their settings. For a more detailed discussion of prior work, see Appendix A.

**Paper Organization.** While our continuous-time result is stronger, the discrete-time setting is more accessible. Therefore, we first discuss the discrete-time setting, which includes the problem statement in Section 2, our proposed framework, Follow-the-Virtual-Advice, in Section 3, and our results on the optimality gap in Section 4. Results for the continuous-time setting are presented in Section 5. We conclude the paper in Section 6. Proofs and additional discussion are given in the appendices.

## 2 Problem Setup

Consider the infinite-horizon, discrete-time restless bandit problem with \(N\) arms indexed by \([N]\{1,2,,N\}\). Each arm is associated with an MDP described by the tuple \((,,P,r)\). Here \(\) is the state space, assumed to be finite; \(=\{0,1\}\) is the action set, and we say the arm is _activated_ or _pulled_ when action \(1\) is applied; \(P:\) is the transition kernel, where \(P(s,a,s^{})\) is the probability of transitioning from state \(s\) to state \(s^{}\) upon taking action \(a\); \(r=\{r(s,a)\}_{s,a}\) is the reward function, where \(r(s,a)\) is the reward for taking action \(a\) in state \(s\). Throughout the paper, we assume that the transition kernel \(P\) is unichain ; that is, under any Markov policy for this single-armed MDP \((,,P,r)\), the induced Markov chain has a single recurrent class. The unichain assumption is standard in most existing work on restless bandits . We will discuss relaxing the unichain assumption in Appendix D.

In the above setting, we are subject to a _budget constraint_ that exactly \( N\) arms must be activated in each time step, where \((0,1)\) is a given constant and \( N\) is assumed to be an integer for simplicity. This \(N\)-armed RB problem can be represented by the tuple \((N,^{N},^{N},P,r, N)\).

A policy \(\) for the \(N\)-armed problem specifies the action for each of the \(N\) arms based on the history of states and actions. Under policy \(\), let \(S^{}_{i}(t)\) denote the state of the \(i\)th arm at time \(t\), and we call \(^{}(t)(S_{i}^{}(t))_{i[N]}^{N}\) the _system state_. Similarly, let \(A_{i}^{}(t)\) denote the action applied to the \(i\)th arm at time \(t\), and let \(^{}(t)(A_{i}^{}(t))_{i[N]}^{N}\) denote the joint action vector.

The controller's goal is to find a policy that maximizes the long-run average of the total expected reward from all \(N\) arms, subject to the budget constraint, assuming full knowledge of the model:

\[}{} V_{N}^{} _{T}_{t=0}^{T-1}_{i=1}^{ N}[r(S_{i}^{}(t),A_{i}^{}(t))]\] (1) \[_{i=1}^{N}A_{i}^{}(t)= N,  t 0.\] (2)

Under the unichain assumption, the value \(V_{N}^{}\) of any policy \(\) is independent of the initial state. Let \(V_{N}^{*}_{}V_{N}^{}\) denote the optimal value. The optimality gap of \(\) is defined as \(V_{N}^{*}-V_{N}^{}\). We say a policy \(\) is _asymptotically optimal_ if its optimality gap converges to \(0\) as \(N\).

Classical theory guarantees that for a finite-state Markov decision process like an RB, there exists an optimal policy that is Markovian and stationary . Nevertheless, the policies we propose are not Markovian policies; rather, they have internal states. Under such a policy \(\), the system state \(^{}(t)\) together with the internal state form a Markov chain, and the action \(^{}(t)\) depends on both the system and internal states. We design a policy such that this Markov chain has a stationary distribution. Let \(^{}()\) and \(^{}()\) denote the random elements that follow the stationary distributions of \(^{}(t)\) and \(^{}(t)\), respectively. Then the average reward of \(\) is equal to \(V_{N}^{}=_{i=1}^{N}[r(S_{i}^{}(),A_{ i}^{}())]\).

In later sections, when the context is clear, we drop the superscript \(\) from \(S_{i}^{}\) and \(A_{i}^{}\).

## 3 Follow-the-Virtual-Advice: A simulation-based framework

In this section, we present our framework, Follow-the-Virtual-Advice (FTVA). We first describe a _single-armed problem_, which involves an "average arm" from the original \(N\)-armed problem. We then use the optimal single-armed policy to construct the proposed policy FTVA(\(^{*}\)).

### Single-armed problem

The single-armed problem involves the MDP \((,,P,r)\) associated with a single arm (say arm \(1\) without loss of generality), where the budget is satisfied _on average_. Formally, consider the problem:

\[}{} V_{1}^{} _{T}_{t=0}^{T-1}[r(S_{1 }^{}(t),A_{1}^{}(t))]\] (3) \[_{T}_{t=0}^{T-1} [A_{1}^{}(t)]=.\] (4)

The constraint (4) stipulates that the _average_ rate of applying the active action must equal \(\). Various equivalent forms of this single-armed problem have been considered in prior work .

By virtue of the unichain assumption, the single-armed problem can be equivalently rewritten as the following linear program, where each decision variable \(y(s,a)\) represents the steady-state probability that the arm is in state \(s\) taking action \(a\):

\[_{s,a}}{}_{s,a}r(s,a)y(s,a)\] (LP) \[_{s}y(s,1)=\] (5) \[_{s^{},a}y(s^{},a)P(s^{ },a,s)=_{a}y(s,a),\; s\] (6) \[_{s,a}y(s,a)=1, y(s,a) 0,\;  s,a.\] (7)Here (5) corresponds to the relaxed budget constraint, (6) is the flow balance equation, and (6)-(7) guarantee that \(y(s,a)\)'s are valid steady-state probabilities.

By standard results for average reward MDPs , an optimal solution \(\{y^{*}(s,a)\}_{s,a}\) to (LP) induces an optimal policy \(^{*}\) for the single-armed problem via the following formula:

\[^{*}(a|s)=y^{*}(s,a)/(y^{*}(s,0)+y^{*}(s,1)),&y^{*}(s,0)+y^{*}(s,1)>0,\\ 1/2,&y^{*}(s,0)+y^{*}(s,1)=0.s,\,a .\] (8)

Let \(V_{1}^{}=V_{1}^{^{*}}\) be the optimal value of (LP) and the single-armed problem.

(LP) can be viewed as a relaxation of the \(N\)-armed problem. To see this, take any \(N\)-armed policy \(\) and set \(y(s,a)\) to be the fraction of arms in state \(s\) taking action \(a\) in steady state under \(\), i.e., \(y(s,a)=[_{i=1}^{N}_{\{S_{i}^{}( )=s,A_{i}^{*}()=a\}}]\). Whenever \(\) satisfies the budget constraint (2), \(\{y(s,a)\}\) satisfies the relaxed constraint (5) and the consistency constraints (6)-(7). Therefore, the optimal value of (LP) is an upper bound of the optimal value of the \(N\)-armed problem: \(V_{1}^{} V_{N}^{*}\).

### Constructing the \(N\)-armed policy

We now present Follow-the-Virtual-Advice, a simulation-based framework for the \(N\)-armed problem. FTVA takes as input _any_ single-armed policy \(\) that satisfies the relaxed budget constraint (4) and converts it into a \(N\)-armed policy, denoted by FTVA(\(\)). Of particular interest is when \(\) is an optimal single-armed policy, which leads to our result on the optimality gap. Below we introduce the general framework of FTVA without imposing any restriction on the input policy \(\).

The proposed policy FTVA(\(\)) has two main components:

* _Virtual single-armed processes._ Each arm \(i\) simulates a _virtual_ single-armed process, whose state is denoted as \(_{i}(t)\), with action \(_{i}(t)\) chosen according to \(\). To make the distinction conspicuous, we sometimes refer to the state \(S_{i}(t)\) and action \(A_{i}(t)\) in the original \(N\)-armed problem as the _real_ state/action. The virtual processes associated with different arms are independent.
* _Follow the virtual actions._ At each time step \(t\), we choose the real actions \(A_{i}(t)\)'s to best match the virtual actions \(_{i}(t)\)'s, to the extent allowed by the budget constraint \(_{i=1}^{N}A_{i}(t)= N\).

FTVA is presented in detail in Algorithm 1. Note that we use an appropriate coupling in Algorithm 1 to ensure that the virtual processes \((_{i}(t),_{i}(t))\)'s are independent and each follows the Markov chain induced by the single-armed policy \(\). FTVA is designed to steer the real states to be close to the virtual states, thereby ensuring a small _conversion loss_\(V_{1}^{}-V_{N}^{()}\). Here recall that \(V_{1}^{}\) is the average reward achieved by the input policy \(\) in the single-armed problem, and that \(V_{N}^{()}\) is the average reward per arm achieved by policy \(()\) in the \(N\)-armed problem.

### Discussion on \(\) and the role of virtual processes

In this subsection, we provide insights into the mechanism of \(\) and explain the crucial role of the virtual processes. In particular, we contrast \(\) with the alternative approach of directly using the real states to choose actions, e.g., by applying the single-armed policy \(^{*}\) to each arm's real state. We note that existing policies are essentially real-state-based, so the insights here can also explain why UGAP is necessary for existing policies to have asymptotic optimality.

We first observe that the above two approaches are equivalent in the absence of the budget constraint. In particular, even if the initial virtual state and real state of an arm \(i\) are different, they will synchronize (i.e., become identical) in finite time _by chance_ under mild assumptions (see Section 4.1). After this event, _if_ there were no constraint, each arm \(i\) will consistently follow the virtual actions, i.e., \(A_{i}(t)=_{i}(t)=_{i}(t)\) for all \(t\), and the virtual states will remain identical to real states.

In the presence of the budget constraint, the arms may not remain synchronized, so the virtual processes become crucial: they guarantee that the virtual actions \(_{i}(t)=_{i}(t)\) nearly satisfy budget constraint, which allows the real system to approximately follow the virtual actions to remain synchronized. To see this, note that regardless of the current real states, the \(N\) virtual states \(_{1},,_{N}\) independently follow the single-armed policy \(^{*}\), so, in the long run, each \((_{i}(),_{i}())\) is distributed per \(y^{*}(,)\), the optimal solution to (LP). For large \(N\), the sum \(_{i=1}^{N}_{i}()\) concentrates around its expectation \(N_{s}y^{*}(s,1)= N\) and thus tightly satisfy the budget constraint. In contrast, the actions generated by applying \(\) to the real states are likely to significantly violate the constraint, especially when the empirical distribution of the current real states deviates from \(y^{*}(,)\).

An example.We provide a concrete example illustrating the above arguments. Suppose the state space for each arm is \(=\{0,1,,7\}\). We label action \(1\) as the _preferred action_ for states \(0,1,2,3\), and action \(0\) for the other states. For an arm in state \(s\), applying the preferred action moves the arm to state \((s+1) 8\) with probability \(p_{s,}\), and applying the other action moves the arm to state \((s-1)^{+}\) with probability \(p_{s,}\); the arm stays at state \(s\) otherwise. One unit of reward is generated when the arm goes from state \(7\) to state \(0\). We assume that the budget is \(N/2\) and set \(\{p_{s,},p_{s,}\}\) such that the optimal solution of (LP) is \(y^{*}(s,1)=1/8\) for \(s=0,1,2,3\) and \(y^{*}(s,0)=1/8\) for \(s=4,5,6,7\). That is, the optimal state distribution is \(()\), and the optimal single-armed policy \(^{*}\) always takes the preferred action so as to traverse state \(7\) as often as possible.

Figure 2: Time evolution of the fraction of arms in each state under LP-Priority (upper), or after switching to \((^{*})\) (lower) since time slot \(250\). The x-axis represents the time slot, which ranges from \(250\) to \(289\); the y-axis represents the states; the color represents the fraction of arms in each state at each time slot. The colors and magnitudes of the arrows represent the average directions and rates at which the arms move away from each state.

To see why this MDP makes the corresponding \(N\)-armed system tricky to control, consider the situation where \(p_{s,}>>p_{s,}\) and most arms are in state \(0\). Those arms prefer actions \(1\) to move towards state \(7\). However, there are only \(N/2\) units of budget. If we break ties uniformly at random, each arm is not pulled with probability \(1/2\) in each time slot and is likely to return to \(0\) before they leave \(\{0,1,2,3\}\). This phenomenon can be seen from Figure 6(b) in Appendix G.

A similar phenomenon can be observed for LP-Priority policies. Here we consider an LP-Priority in  that breaks ties based on the Lagrangian-optimal index. In Figure 2, we generate and visualize a sample path under LP-Priority from time \(250\) to \(289\), and contrast it with the sample path if the system switches to \((^{*})\) from time \(250\) onwards. We can see that under LP-Priority, although the arms have a strong tendency to move up from state \(4\) to state \(5\), they move back when they reach state \(5\) and thus get stuck at state \(4\). In contrast, when switching to \((^{*})\), the arrows gradually change to the correct direction, which helps the arms to escape from state \(4\) and converge to the uniform distribution over the state space. Intuitively, under \((^{*})\), an increasing number of arms couple their real states with virtual states over time, which allows these arms to consistently apply the preferred actions afterward. In Appendix G.4, we include more visualizations of the policies.

In Figure 3, we compare the average reward of \((^{*})\) with those of the random tie-breaking policy and the LP-Priority policy discussed above. We can see that \((^{*})\) is near-optimal, while the other two policies have nearly zero rewards. Further details are provided in Appendix G. Note that while a different tie-breaking rule may solve this particular example with real states, currently there is no known rule that works in general.

## 4 Theoretical result on optimality gap

In this section, we present our main theoretical result, an upper bound on the conversion loss \(V_{1}^{}-V_{N}^{()}\) for any given single-armed policy \(\). Setting \(\) to be an optimal single-armed policy \(^{*}\) then leads to an upper bound on the optimality gap of our N-armed policy \((^{*})\). Our result holds under the Synchronization Assumption (SA), which we formally introduce below.

### Synchronization Assumption

SA is imposed on a given single-armed policy \(\). To describe SA, we first define a two-armed system called the _leader-and-follower_ system, which consists of a _leader_ arm and a _follower_ arm. Each arm is associated with the MDP \((,,P,r)\). At each time step \(t 1\), the leader arm is in state \((t)\) and uses the policy \(\) to chooses an action \((t)\) based on \((t)\); the follower arm is in state \(S(t)\), and it takes the action \(A(t)=(t)\) regardless of \(S(t)\). The state transitions of the two arms are coupled as follows. If \(S(t)=(t)\), then \(S(t+1)=(t+1)\). If \(S(t)(t)\), then \(S(t+1)\) and \((t+1)\) are sampled independently from \(P(S(t),A(t),)\) and \(P((t),(t),)\), respectively. Note that once the states of the two arms become identical, they stay identical indefinitely.

Given the initial states and actions \((S(0),A(0),(0),(0))=(s,a,,) \), we define the _synchronization time_ as the first time the two states become identical:

\[^{}(s,a,,)\{t 0 S(t)= (t)\}.\] (9)

**Assumption 1** (Synchronization Assumption (SA) for a policy \(\)).: We say that a single-armed policy \(\) satisfies the Synchronization Assumption (SA) if for any initial states and actions \((s,a,,) \), the synchronization time \(^{}(s,a,,)\) is a stopping time and satisfies

\[[^{}(s,a,,)]<.\] (10)

We view SA as an appealing alternative to UGAP for two reasons. First, there are some instances that satisfy SA but not UGAP. Two such examples are given in Figures 1 and 3.  provides more

Figure 3: Comparing policies based on virtual states and real states

examples that violate UGAP, all of which can be verified to satisfy SA. Second, SA is easier to verify than UGAP. It has been acknowledged in many prior papers that UGAP lacks intuitive sufficient conditions and can only be checked numerically. In contrast, SA can be efficiently verified in several ways, which is discussed in Appendix C.

### Bounds on conversion loss and optimality gap

We are now ready to state our main theorem.

**Theorem 1**.: _Consider an \(N\)-armed RB problem \((N,^{N},^{N},P,r, N)\) under the single-armed unichain assumption. Let \(\) be any single-armed policy satisfying SA. For any \(N 1\), the conversion loss of \(\) satisfies the upper bound_

\[V_{1}^{}-V_{N}^{()}_{}^{}}{},\] (11)

_where \(r_{}_{s,a}|r(s,a)|\) and \(_{}^{}_{(s,a,,)}[ ^{}(s,a,,)]\)._

_Consequently, given any optimal single-armed policy \(^{*}\) satisfying SA, for all any \(N 1\) the optimality gap of \((^{*})\) is upper bounded as_

\[V_{N}^{*}-V_{N}^{(^{*})} V_{1}^{^{*}}-V_{N}^{(^{*})} _{}^{}}{}.\] (12)

Proof sketch.The proof of Theorem 1 is given in Appendix E. Here we sketch the main ideas of the proof, whose key step involves bounding the conversion loss \(V_{1}^{}-V_{N}^{()}\) using a fundamental tool from queueing theory, the Little's Law . Specifically, we start with the upper bound

\[V_{1}^{}-V_{N}^{()} =[_{i=1}^{N}r_{i} (),_{i}()-_{i=1}^{N}rS_{i}(),A_ {i}()]\] \[}{N}[_{i=1}^{N}_{i}(),_{i}() S_{i}(),A_{i}()}],\] (13)

which holds since the virtual process \(_{i}(t),_{i}(t)\) of each arm \(i\) follows the single-armed policy \(\). We say an arm \(i\) is a _bad arm_ at time \(t\) if \(_{i}(t),_{i}(t)S_{i}(t),A_{i}( t)\), and a _good arm_ otherwise. Then \([_{i=1}^{N}_{i}( ),_{i}()S_{i}(),A_{i}() }]=[]\) in steady state.

By Little's Law, we have the following relationship:

\[[]=[].\]

It suffices to bound the two terms on the right hand side. Note that the virtual actions \(_{i}(t)\)'s are i.i.d. with mean \([_{i}(t)]=\); a standard concentration inequality shows that at most \(_{i=1}^{N}_{i}(t)- N O()\) bad arms are generated per time slot. On the other hand, each bad arm stays bad until its real state becomes identical to its virtual state, which occurs in \(O(1)\) time by virtue of SA.

## 5 Continuous-time restless bandits

In this section, we consider the continuous-time setting. The setup, policy, and theoretical results for this setting parallel those for the discrete-time setting, except that we no longer require SA. Detailed proofs are provided in Appendix F.

Problem setup.The continuous-time restless bandit (CTRB) problem is similar to its discrete-time counterpart (cf. Section 2), except that each single-armed MDP runs in continuous time. In particular, an \(N\)-armed CTRB is given by a tuple \((N,^{N},^{N},G,r, N)\), where \(\) is the finite state space and \(=\{0,1\}\) is the action space of a single arm. In continuous time, each arms dynamics is governed by the transition _rates_ (rather than probabilities) \(G=\{G(s,a,s^{})\}_{s,s^{},a}\), where \(G(s,a,s^{})\) is the rate of transitioning from state \(s\) to state \(s^{} s\) upon taking action \(a\). We again assume that the transition kernel \(G\) of each arm is unichain. Given the states and actions of all arms, the transitions of different arms are independent from each other. Similarly, \(r(s,a)\) is the _instantaneous rate_ of accumulating reward while taking action \(a\) in state \(s\). The budget constraint now requires that at any moment of time, the total number of arms taking the active action \(1\) is equal to \( N\).

The objective is again maximizing the long-run average reward, that is,

\[}{} V_{N}^{}_{T}_{0}^{T} _{i=1}^{N}[r(S_{i}^{}(t),A_{i}^{}(t)) ]dt\] (14) subject to \[_{i=1}^{N}A_{i}^{}(t)= N t 0.\] (15)

Let \(V_{N}^{*}=_{}V_{N}^{}\) denote the optimal value of the above optimization problem.

Single-armed problem.The single-armed problem for CTRB is defined analogously as its discrete-time counterpart (3)-(4). This single-armed problem can again be written as a linear program, where the decision variable \(y(s,a)\) represents the steady-state probability of being in state \(s\) taking action \(a\):

\[_{s,a}}{} _{s,a}r(s,a)y(s,a)\] (LP-CT) subject to \[_{s}y(s,1)=\] (16) \[_{s^{},a}y(s^{},a)G(s^{ },a,s)=0 s\] (17) \[_{s,a}y(s,a)=1; y(s,a) 0 \ \  s,a.\] (18)

In (17), we use the convention that \(G(s,a,s)=-G(s,a)-_{s^{} s}G(s,a,s^{})\). Again, the optimal value of (LP-CT) upper bounds the optimal value for the \(N\)-armed problem, i.e., \(V_{1}^{}=V_{1}^{^{*}} V_{N}^{*}\), where \(^{*}\) is any optimal single-armed policy and can be computed using the same formula in (8). Note that in continuous time, the optimal policy \(^{*}\) is carried out through uniformization [Put05].

Our policy.Our framework for the CTRB, Follow-the-Virtual-Advice-CT (FTVA-CT), is presented in Algorithm 2. FTVA-CT works in a similar fashion as its discrete-time counterpart. It takes a single-armed policy \(\) as input, and each arm \(i\) independently simulates a virtual single-armed process \((_{i}(t),_{i}(t))\) following \(\). FTVA-CT then chooses the real actions \(A_{i}(t)\)'s to match the virtual actions \(_{i}(t)\)'s to the extent allowed by the budget constraint \(_{i=1}^{N}A_{i}(t)= N\).

To run FTVA-CT in continuous time for the \(N\)-armed problem, we use the following uniformization to set discrete _decision epochs_\(\{t_{k}\}_{k=0,1,}\) for updating the actions and virtual processes. We uniformize at rate \(2Ng_{}\) with \(g_{}=_{s,a}G(s,a)\), which is an upper bound on the total transition rate of the real and virtual states in an \(N\)-armed system. We generate a decision epoch either when a real state transitions, or when an independent exponential timer with rate \(2Ng_{}-_{i=1}^{N}G(S_{i}(t),A_{i}(t))\) ticks.

The definitions of conversion loss, optimality gap, and asymptotic optimality are the same as the discrete-time setting.

Conversion loss and optimality gap.For a given single-armed policy \(\), we consider a continuous-time version of the leader-and-follower system (cf. Section 4.1). For technical reasons, the initial actions are specified differently from the discrete-time setting. Specifically, we assume that the initial action \((0)\) of the leader arm is chosen by \(\) based on \((0)\), and the follower's initial action \(A(0)\) equals \((0)\). As before, the follower arm always takes the same action as the leader arm regardless of its own state. Given initial states \((S(0),(0))=(s,)\), the synchronization time is defined as

\[^{}(s,)\{t 0 S(t)=( t)\}.\] (19)

We no longer need to impose the Synchronization Assumption, since the unichain assumption automatically implies \([^{}(s,)]<\) in continuous time--see Lemma 5 in Appendix F.

**Theorem 2**.: _Consider an \(N\)-armed CTRB \((N,^{N},^{N},G,r, N)\) under the single-armed unichain assumption. For any single-armed policy \(\), the conversion loss of \(\) is upper bounded as_

\[V_{1}^{}-V_{N}^{()}(1+2g_{ }_{}^{})}{}, N 1,\] (20)

_where \(r_{}=_{s,a}|r(s,a)|\), \(_{}^{}=_{s, }[^{}(s,)]\)._

_Consequently, for any optimal single-armed policy \(^{*}\), the optimality gap of \((^{*})\) satisfies_

\[V_{N}^{*}-V_{N}^{(^{*})} V^{^{*}}-V_{N}^ {(^{*})}(1+2g_{}_{}^{})}{}, N 1.\] (21)

Theorem 2 establishes an \(O(1/)\) optimality gap without requiring UGAP or any additional assumptions beyond the standard unichain condition.

## 6 Conclusion

In this paper, we study the average-reward restless bandit problem. We propose a simulation-based framework called Follow-the-Virtual-Advice that converts a single-armed optimal policy into a policy in the original \(N\)-arm system with an \(O(1/)\) optimality gap. In the discrete-time setting, our results hold under the Synchronization Assumption (SA), a mild and easy-to-verify assumption that covers some problem instances that do not satisfy UGAP. In the continuous-time setting, our results do not require any additional assumptions beyond the standard unichain condition. In both settings, our work is the first to achieve asymptotic optimality without assuming UGAP. It will be interesting in future work to explore the possibility of achieving optimality gaps smaller than \(O(1/)\) without relying on UGAP or the non-degenerate condition.

Acknowledgement.Y. Hong and W. Wang are supported in part by NSF grants CNS-200773 and ECCS-2145713. Q. Xie is supported in part by NSF grant CNS-1955997 and a J.P. Morgan Faculty Research Award. Y. Chen is supported in part by NSF grants CCF-1704828 and CCF-2233152. This project started when Q. Xie, Y. Chen, and W. Wang were attending the Data-Driven Decision Processes program of the Simons Institute for the Theory of Computing.