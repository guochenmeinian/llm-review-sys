# GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models

ZATANG LI

_The Chinese University of Hong Kong Sha Tin, Hong Kong ztli@cse.cuhk.edu.hk_

Pin-Yu Chen

_IBM Research New York, USA_

pin-yu.chen@ibm.com

Tsung-Yi Ho

_The Chinese University of Hong Kong Sha Tin, Hong Kong tyho@cse.cuhk.edu.hk_

###### Abstract

Current studies on adversarial robustness mainly focus on aggregating _local_ robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true _global_ robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called GREAT Score, for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In particular, we show high correlation and significantly reduced computation cost of GREAT Score when compared to the attack-based model ranking on RobustBench . (2) The use of generative models facilitates the approximation of the unknown data distribution. In our ablation study with different generative adversarial networks (GANs), we observe consistency between global robustness evaluation and the quality of GANs. (3) GREAT Score can be used for remote auditing of privacy-sensitive black-box models, as demonstrated by our robustness evaluation on several online facial recognition services.

## 1 Introduction

Adversarial robustness is the study of model performance in the worst-case scenario, which is a key element in trustworthy machine learning. Adversarial robustness evaluation refers to the process of assessing a model's resilience against adversarial attacks, which are inputs intentionally designed to deceive the model. Without further remediation, state-of-the-art machine learning models, especially neural networks, are known to be overly sensitive to small human-imperceptible perturbations to datainputs . Such a property of over-sensitivity could be exploited by bad actors to craft adversarial perturbations leading to prediction-evasive adversarial examples.

Given a threat model specifying the knowledge of the target machine learning model (e.g., white-box or black-box model access) and the setting of plausible adversarial interventions (e.g., norm-bounded input perturbations), the methodology for adversarial robustness evaluation can be divided into two categories: _attack-dependent_ and _attack-independent_. Attack-dependent approaches aim to devise the strongest possible attack and use it for performance assessment. A typical example is Auto-Attack , a state-of-the-art attack based on an ensemble of advanced white-box and black-box adversarial perturbation methods. On the other hand, attack-independent approaches aim to develop a certified or estimated score for adversarial robustness, reflecting a quantifiable level of attack-proof certificate. Typical examples include neural network verification techniques , certified defenses such as randomized smoothing , and local Lipschitz constant estimation .

Despite a plethora of adversarial robustness evaluation methods, current studies primarily focus on aggregating _local_ robustness results from a set of data samples. However, the sampling process of these test samples could be biased and unrepresentative of the true _global_ robustness of the underlying data distribution, resulting in the risk of incorrect or biased robustness benchmarks. For instance, we find that when assessing the ranking of Imagenet models through Robustbench , using AutoAttack  with 10,000 randomly selected samples (the default choice) with 100 independent trials results in an unstable ranking coefficient of 0.907\(\)0.0256 when compared to that of the entire 50,000 test samples. This outcome affirms that AutoAttack's model ranking has notable variations with an undersampled or underrepresented test dataset.

An ideal situation is when the data distribution is transparent and one can draw an unlimited number of samples from the true distribution for reliable robustness evaluation. But in reality, the data distribution is unknown and difficult to characterize. In addition to lacking rigorous global robustness evaluation, many attack-independent methods are limited to the white-box setting, requiring detailed knowledge about the target model (e.g., model parameters and architecture) such as input gradients and internal data representations for robustness evaluation. Moreover, state-of-the-art attack-dependent and attack-independent methods often face the issue of scalability to large models and data volumes due to excessive complexity, such as the computational costs in iterative gradient computation and layer-wise interval bound propagation and relaxation .

To address the aforementioned challenges including (i) lack of proper global adversarial robustness evaluation, (ii) limitation to white-box settings, and (iii) computational inefficiency, in this paper we present a novel attack-independent evaluation framework called _GREAT Score_, which is short for global robustness evaluation of adversarial perturbation using generative models. We tackle challenge (i) by using a generative model such as a generative adversarial network (GAN)  or a diffusion model  as a proxy of the true unknown data distribution. Formally, GREAT Score is defined as the mean of a certified lower bound on minimal adversarial perturbation over the data sampling distribution of a generative model, which represents the global distribution-wise adversarial robustness with respect to the generative model in use. It entails a global statistic capturing the mean certified attack-proof perturbation level over all samples from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee quantifying the sample complexity and the difference between the sample mean and true mean.

For challenge (ii), our derivation of GREAT Score leads to a neat closed-form solution that only requires data forward-passing and accessing the model outputs, which applies to any black-box classifiers giving class prediction confidence scores as model output. Moreover, as a byproduct of using generative models, our adversarial robustness evaluation procedure is executed with only synthetically generated data instead of real data, which is particularly appealing to privacy-aware robustness assessment schemes, e.g., remote robustness evaluation or auditing by a third party with restricted access to data and model. We will present how GREAT Score can be used to assess the robustness of online black-box facial recognition models. Finally, for challenge (iii), GREAT Score is applicable to any off-the-self generative models so that we do not take the training cost of generative models into consideration. Furthermore, the computation of GREAT Score is lightweight because it scales linearly with the number of data samples used for evaluation, and each data sample only requires one forward pass through the model to obtain the final predictions.

We highlight our main contributions as follows:* We present GREAT Score as a novel framework for deriving a global statistic representative of the distribution-wise robustness to adversarial perturbation, based on an off-the-shelf generative model for approximating the data generation process.
* Theoretically, we show that GREAT Score corresponds to a mean certified attack-proof level of \(_{2}\)-norm bounded input perturbation over the sampling distribution of a generative model (Theorem 1). We further develop a formal probabilistic guarantee on the quality of using the sample mean as GREAT Score with a finite number of samples from generative models (Theorem 2).
* We evaluate the effectiveness of GREAT Score on all neural network models on RobustBench  (the largest adversarial robustness benchmark), with a total of 17 models on CIFAR-10 and 5 models on ImageNet. We show that the model ranking of GREAT Score is highly aligned with that of the original ranking on RobustBench using AutoAttack , while GREAT Score significantly reduces the computation time. Specifically, on CIFAR-10 the computation complexity can be reduced by up to 2,000 times. The results suggest that GREAT Score is a competitive and computationally-efficient approach complementary to attack-based robustness evaluations.
* As a demonstration of GREAT Score's capability for remote robustness evaluation of access-limited systems, we show how GREAT Score can audit several online black-box facial recognition APIs.

## 2 Background and Related Works

Adversarial Attack and Defense.Adversarial attacks aim to generate examples that can evade classifier predictions in classification tasks. In principle, adversarial examples can be crafted by small perturbations to a native data sample, where the level of perturbation is measured by different \(_{p}\) norms [7; 8; 58]. The procedure of finding adversarial perturbation within a perturbation level is often formulated as a constrained optimization problem, which can be solved by algorithms such as projected gradient descent (PGD) . The state-of-the-art adversarial attack is the Auto-Attack , which uses an ensemble of white-box and black-box attacks. There are many methods (defenses) to improve adversarial robustness. A popular approach is adversarial training , which generates adversarial perturbation during model training for improved robustness. One common evaluation metric for adversarial robustness is robust accuracy, which is defined as the accuracy of correct classification under adversarial attacks, evaluated on a set of data samples. RobustBench  is the largest-scale standardized benchmark that ranks the models using robust accuracy against Auto-Attack on test sets from image classification datasets such as CIFAR-10. In addition to discussed works, several studies evaluate model robustness differently.  introduce adversarial sparsity, quantifying the difficulty of finding perturbations, providing insights beyond adversarial accuracy.  propose probabilistic robustness, balancing average and worst-case performance by enforcing robustness to most perturbations, better addressing trade-offs.  introduce the adversarial hypervolume metric, a comprehensive measure of robustness across varying perturbation intensities.

Generative Models.Statistically speaking, let \(X\) denote the observable variable and let \(Y\) denote the corresponding label, the learning objective for a generative model is to model the conditional probability distribution \(P(X Y)\). Among all the generative models, GANs have gained a lot of attention in recent years due to their capability to generate realistic high-quality images . The principle of training GANs is based on the formulation of a two-player zero-sum min-max game to learn the high-dimension data distribution. Eventually, these two players reach the Nash equilibrium that \(D\) is unable to further discriminate real data versus generated samples. This adversarial learning methodology aids in obtaining high-quality generative models. In practice, the generator \(G()\) takes a random vector \(z\) (i.e., a latent code) as input, which is generated from a zero-mean isotropic Gaussian distribution denoted as \(z(0,I)\), where \(I\) means an identity matrix. Conditional GANs refer to the conditional generator \(G(|Y)\) given a class label \(Y\). In addition to GAN, diffusion models (DMs) are also gaining popularity. DMs consist of two stages: the forward diffusion process and the reverse diffusion process. In the forward process, the input data is gradually perturbed by Gaussian Noises and becomes an isotropic Gaussian distribution eventually. In the reverse process, DMs reverse the forward process and implement a sampling process from Gaussian noises to reconstruct the true samples by solving a stochastic differential equation. In our proposed framework, we use off-the-shelf (conditional) GANs and DMs (e.g., DDPM ) that are publicly available as our generative models.

Formal Local Robustness Guarantee and Estimation.Given a data sample \(x\), a formal local robustness guarantee refers to a certified range on its perturbation level such that within which the top-1 class prediction of a model will remain unchanged . In \(_{p}\)-norm (\(p 1\)) bounded perturbationscentered at \(x\), such a guarantee is often called a certified radius \(r\) such that any perturbation \(\) to \(x\) within this radius (i.e., \(\|\|_{p} r\)) will have the same top-1 class prediction as \(x\). Therefore, the model is said to be provably locally robust (i.e., attack-proof) to any perturbations within the certified radius \(r\). By definition, the certified radius of \(x\) is also a lower bound on the minimal perturbation required to flip the model prediction.

Among all the related works on attack-independent local robustness evaluations, the CLEVER framework proposed in  is the closest to our study. The authors in  derived a closed-form of certified local radius involving the maximum local Lipschitz constant of the model output with respect to the data input around a neighborhood of a data sample \(x\). They then proposed to use extreme value theory to estimate such a constant and use it to obtain a local robustness score, which is not a certified local radius. Our proposed GREAT Score has major differences from  in that our focus is on global robustness evaluation, and our GREAT Score is the mean of a certified radius over the sampling distribution of a generative model. In addition, for every generated sample, our local estimate gives a certified radius.

**Notations.** All the main notations used in the paper are summarized in Appendix A.

## 3 GREAT Score: Methodology and Algorithms

### True Global Robustness and Certified Estimate

Let \(f=[f_{1},,f_{K}]:^{d}^{K}\) denote a fixed \(K\)-way classifier with flattened data input of dimension \(d\), \((x,y)\) denote a pair of data sample \(x\) and its corresponding groundtruth label \(y\{1,,K\}\), \(P\) denote the true data distribution which in practice is unknown, and \(_{}(x)\) denote the minimal perturbation of a sample-label pair \((x,y) P\) causing the change of the top-1 class prediction such that \(_{k\{1,,K\}}f_{k}(x+_{}(x))_{k\{1, ,K\}}f_{k}(x)\). Note that if the model \(f\) makes an incorrect prediction on \(x\), i.e., \(y_{k\{1,,K\}}f_{k}(x)\), then we define \(_{}(x)=0\). This means the model is originally subject to prediction evasion on \(x\) even without any perturbation. A higher \(_{}(x)\) means better local robustness of \(f\) on \(x\).

The following statement defines the true global robustness of a classifier \(f\) based on the probability density function \(p()\) of the underlying data distribution \(P\).

**Definition 1** (True global robustness w.r.t. \(P\)).: The true global robustness of a classifier \(f\) with respect to a data distribution \(P\) is defined as:

\[(f)=_{x P}[_{min}(x)]=_{x P}_{}(x) p(x)dx\] (1)

Unless the probability density function of \(P\) and every local minimal perturbation are known, the exact value of the true global robustness cannot be computed. An alternative is to estimate such a quantity. Extending Definition 1, let \(g(x)\) be a local robustness statistic. Then the corresponding global robustness estimate is defined as

\[(f)=_{x P}[g(x)]=_{x P}g(x)p(x)dx\] (2)

Furthermore, if one can prove that \(g(x)\) is a valid lower bound on \(_{min}(x)\) such that \(g(x)_{min}(x),\  x\), then the estimate \((f)\) is said to be a certified lower bound on the true global robustness with respect to \(P\), and larger \((f)\) will imply better true global robustness. In what follows, we will formally introduce our proposed GREAT Score and show that it is a certified estimate of the lower bound on the true robustness with respect to the data-generating distribution learned by a generative model.

### Using GMs to Evaluate Global Robustness

Recall that a generative model (GM) takes a random vector \(z(0,I)\) sampled from a zero-mean isotropic Gaussian distribution as input to generate a data sample \(G(z)\). In what follows, we present our first main theorem that establishes a certified lower bound \((f)\) on the true global robustness of a classifier \(f\) measured by the data distribution given by \(G()\).

Without loss of generality, we assume that all data inputs are confined in the scaled data range \(^{d}\), where \(d\) is the size of any flattened data input. The \(K\)-way classifier \(f:^{d}^{K}\) takes a data sample \(x\) as input and outputs a \(K\)-dimensional vector \(f(x)=[f_{1}(x),,f_{K}(x)]\) indicating the likelihood of its prediction on \(x\) over \(K\) classes, where the top-1 class prediction is defined as \(=_{k=\{1,,K\}}f_{k}(x)\). We further denote \(c\) as the groundtruth class of \(x\). Therefore, if \( c\), then the classifier is said to make a wrong top-1 prediction. When considering the adversarial robustness on a wrongly classified sample \(x\), we define the minimal perturbation for altering model prediction as \(_{}(x)=0\). The intuition is that an attacker does not need to take any action to make the sample \(x\) evade the correct prediction by \(f\), and therefore the required minimal adversarial perturbation level is \(0\) (i.e., zero robustness).

Given a generated data sample \(G(z)\), we now formally define a local robustness score function as

\[g(G(z))=}\{f_{c}(G(z))-_{k\{1, ,K\},k c}f_{k}(G(z)),0\}\] (3)

The scalar \(\) is a constant associated with the sampling Gaussian distribution of \(G\), which will be apparent in later analysis. We further offer several insights into understanding the physical meaning of the considered local robustness score in (3): (i) The inner term \(f_{c}(G(z))-_{k\{1,,K\},k c}f_{k}(G(z))\) represents the gap in the likelihood of model prediction between the correct class \(c\) and the most likely class other than \(c\). A positive and larger value of this gap reflects higher confidence of the correct prediction and thus better robustness. (ii) Following (i), a negative gap means the model is making an incorrect prediction, and thus the outer term \(\{,0\}=0\), which corresponds to zero robustness.

Next, we use the local robustness score \(g\) defined in (3) to formally state our theorem on establishing a certified lower bound on the true global robustness and the proof sketch.

**Theorem 1** (certified global robustness estimate).: _Let \(f:^{d}^{K}\) be a \(K\)-way classifier and let \(f_{k}()\) be the predicted likelihood of class \(k\), with \(c\) denoting the groundtruth class. Given a generator \(G\) such that it generates a sample \(G(z)\) with \(z(0,I)\). Define \(g(G(z))=}\{f_{c}(G(z))-_{k\{1, ,K\},k c}f_{k}(G(z)),0\}\). Then the global robustness estimate of \(f\) evaluated with \(_{2}\)-norm bounded perturbations, defined as \((f)=_{z(0,I)}[g(G(z))]\), is a certified lower bound of the true global robustness \((f)\) with respect to \(G\)._

The complete proof is given in Appendix C.

### Probabilistic Guarantee on Sample Mean

As defined in Theorem 1, the global robustness estimate \((f)=_{z(0,I)}[g(G(z))]\) is the mean of the local robustness score function introduced in (3) evaluated through a generator \(G\) and its sampling distribution. In practice, one can use a finite number of samples \(\{G(z_{i}|y_{i})\}_{i=1}^{n}\) generated from a conditional generator \(G(|y)\) to estimate \((f)\), where \(y\) denotes a class label and it is also an input parameter to the conditional generator. The simplest estimator of \((f)\) is the sample mean, defined as

\[_{S}(f)=_{i=1}^{n}g(G(z_{i}|y_{i}))\] (4)

In what follows, we present our second main theorem to deliver a probabilistic guarantee on the sample complexity to achieve \(\) difference between the sample mean \(_{S}(f)\) and the true mean \((f)\).

**Theorem 2** (probabilistic guarantee on sample mean).: _Let \(f\) be a \(K\)-way classifier with its outputs bounded by \(^{K}\) and let \(e\) denote the natural base. For any \(,>0\), if the sample size \(n}\), then with probability at least \(1-\), the sample mean \(_{S}(f)\) is \(\)-close to the true mean \((f)\). That is, \(|_{S}(f)-(f)|\)._

The complete proof is given in Appendix D. The proof is built on a concentration inequality in . It is worth noting that the bounded output assumption of the classifier \(f\) in Theorem 2 can be easily satisfied by applying a normalization layer at the final model output, such as the softmax function or the element-wise sigmoid function.

### Algorithm and Computational Complexity

Algorithm 1 summarizes the procedure of computing GREAT Score using the sample mean estimator. It can be seen that the computation complexity of GREAT Score is linear in the number of generated samples \(N_{S}\), and for each sample, the computation of the statistic \(g\) defined in (3) only requires drawing a sample from the generator \(G\) and taking a forward pass to the classifier \(f\) to obtain the model predictions on each class. As a byproduct, GREAT Score applies to the setting when the classifier \(f\) is a black-box model, meaning only the model outputs are observable by an evaluator.

``` Input:\(K\)-way classifier \(f()\), conditional generator \(G()\), local score function \(g()\) defined in (3), number of generated samples \(N_{S}\) Output: GREAT Score \(_{S}(f)\) for\(i\)\(\)1 to \(N_{S}\)do  Randomly select a class label \(y\{1,2,,K\}\)  Sample \(z\ (0,I)\) from a Gaussian distribution and generate a sample \(G(z|y)\) with class \(y\)  Pass \(G(z|y)\) into the model \(f\) and get the prediction for each class \(\{f_{k}(G(z|y))\}_{k=1}^{K}\)  Record the statistic \(g^{(i)}(G(z|y))=}\{f_{y}(G(z|y))-_{k\{1, ,K\},\ k y}f_{k}(G(z|y)),0\}\) end \(_{S}(f)\)Compute the sample mean of \(\{g^{(i)}\}_{i=1}^{N_{S}}\) ```

**Algorithm 1**GREAT Score Computation

### Calibrated GREAT Score

In cases when one has additional knowledge of adversarial examples on a set of images from a generative model, e.g., successful adversarial perturbations (an upper bound on the minimal perturbation of each sample) returned by any norm-minimization adversarial attack method such as the CW attack , the CW attack employs two loss terms, classification loss and distance metric, to generate adversarial examples. See Appendix E for details. We can further "calibrate" the GREAT Score with respect to the available perturbations. Moreover, since Theorem 1 informs some design choices on the model output layer, as long as the model output is a non-negative \(K\)-dimensional vector \(f^{K}\) reflecting the prediction confidence over \(K\) classes, we will incorporate such flexibility in the calibration process.

Specifically, we use calibration in the model ranking setup where there are \(M\) models \(\{f^{(j)}\}_{j=1}^{M}\) for evaluation, and each model (indexed by \(j\)) has a set of known perturbations \(\{_{i}^{(j)}\}_{i=1}^{N}\) on a common set of \(N\) image-label pairs \(\{x_{i},y_{i}\}_{i=1}^{N}\) from the same generative model. We further consider four different model output layer designs (that are attached to the model logits): (i) \((|T_{1})\): sigmoid with temperature \(T_{1}\), (ii) \((|T_{2})\): softmax with temperature \(T_{2}\), (iii) \((|T_{2}=1)|T_{1})\): sigmoid with temperature after softmax, and (iv) \((|T_{1}=1)|T_{2})\): softmax with temperature after sigmoid. Finally, let \(\{_{S}(f^{(j)})\}_{j=1}^{M}\) denote the GREAT Score computed based on \(\{x_{i},y_{i}\}_{i=1}^{N}\) for each model. We calibrate GREAT Score by optimizing some rank statistics (e.g., the Spearman's rank correlation coefficient) over the temperature parameter by comparing the ranking consistency between \(\{_{S}(f^{(j)})\}_{j=1}^{M}\) and \(\{_{i}^{(j)}\}_{i=1}^{N}\). In our experiments, we find that setting (iv) gives the best result and use it as the default setup for calibration, as detailed in Appendix F.

## 4 Experimental Results

### Experiment Setup

**Datasets and Models.** We conduct our experiment on several datasets including CIFAR-10 , ImageNet-1K  and CelebA-HQ /CelebA . For neural network models, we use the available models on RobustBench  (see more details in the next paragraph), which includes 17/5 models on CIFAR-10/ImageNet, correspondingly. We also use several off-the-shelf GANs anddiffusion models (DMs) trained on CIFAR-10 and ImageNet for computing GREAT Score in an ablation study (we defer the model details to later paragraphs).

**Summary of Classifiers on RobustBench.** The RobustBench  is to-date the largest benchmark for robustness evaluation with publicly accessible neural network models submitted by contributors. RobustBench uses the default test dataset from several standard image classification tasks, such as CIFAR-10 and ImageNet-1K, to run Auto-Attack  and report the resulting accuracy with \(_{2}\)-norm and \(_{}\)-norm perturbations (i.e., the robust accuracy - RA) as a metric for adversarial robustness. Even under one perturbation type, it is not easy to make a direct and fair comparison among all submitted models on RobustBench because they often differ by the training scheme, network architecture, as well as the usage of additional real and/or synthetic data. To make a meaningful comparison with GREAT Score, we select all non-trivial models (having non-zero RA) submitted to the CIFAR-10 and ImageNet-1K benchmarks and evaluated with \(_{2}\)-norm perturbation with a fixed perturbation level of \(0.5\) using Auto-Attack. We list the model names in Table 1 and provide their descriptions in Appendix G.

**GANs and DMs.** We used off-the-shelf GAN models provided by StudioGAN , a library containing released GAN models. StudioGAN also reports the Inception Score (IS) to rank the model quality. We use the GAN model with the highest IS value as our default GAN for GREAT Score, which are StyleGAN2 / BigGAN  for CIFAR-10 /ImageNet with IS = \(10.477/99.705\), respectively. For the ablation study of using different generative models in GREAT Score (Section 4.4), we also use the following GAN/DM models: LSGAN , GGAN , SAGAN , SNGAN , DDPM  and StyleGAN2 .

**GREAT Score implementation.** The implementation follows Algorithm 1 in Appendix 3 with a sigmoid/softmax function on the logits of the CIFAR-10/ImageNet classifier to ensure the model output of each dimension is within \(\), as implied by Theorem 1. As ImageNet-1K has 1000 classes, applying sigmoid will make the robustness score function in (3) degenerate. We use softmax instead. 500 samples drawn from a generative model were used for computing GREAT Score.

**Comparative methods.** We compare the effectiveness of GREAT Score in two objectives: robustness ranking (global robustness) and per-sample perturbation. For the former, we compare the RA reported in RobustBench on the test dataset (named RobustBench Accuracy) as well as the RA of Auto-Attack on the generated data samples (named AutoAttack Accuracy). For the latter, we report the RA of Auto-Attack in \(_{2}\)-norm with a fixed perturbation level of 0.5.

**Evaluation metrics.** For robustness ranking, we report Spearman's rank correlation coefficient between two sets of model rankings (e.g., GREAT Score v.s. RobustBench Accuracy). A value closer to 1 means higher consistency. Robust accuracy refers to the fraction of correctly classified samples against adversarial perturbations.

**Calibration Method.** We run \(_{2}\)-norm CW attack  (with learning rate \(0.005\) and 200 iterations) on each generated data sample to find the minimal adversarial perturbation. Then, we use grid search in the range  with an interval of 0.00001 to find temperature value maximizing the Spearman's rank correlation coefficient between GREAT Score and CW attack distortion.

**Compute Resources.** All our experiments were run on a GTX 2080 Ti GPU with 12GB RAM.

Figure 1: Comparison of local GREAT Score and CW attack in \(_{2}\) perturbation on CIFAR-10 with Rebuffi_extra model . The x-axis is the image id. The result shows the local GREAT Score is indeed a lower bound of the perturbation level found by CW attack.

Figure 2: Cumulative robust accuracy (RA) with varying \(_{2}\) perturbation level using 500 samples. Note that GREAT Score gives a certified RA for attack-proof robustness, whereas Auto-Attack is an empirical robustness evaluation.

### Local and Global Robustness Analysis

Recall from Theorem 1 that the local robustness score proposed in (3) gives a certified perturbation level for generated samples from a generative model. To verify this claim, we randomly select 20 generated images on CIFAR-10 and compare their local certified perturbation level to the perturbation found by the CW attack  using the Rebuffi_extra model . Figure 1 shows the perturbation level of local GREAT Score in (3) and that of the corresponding CW attack per sample. We can see that the local GREAT Score is a lower bound of CW attack, as the CW attack finds a successful adversarial perturbation that is no smaller than the minimal perturbation \(_{}\) (i.e., an over-estimation). The true \(_{}\) value lies between these lower and upper bounds.

In Figure 2, we compare the cumulative robust accuracy (RA) of GREAT Score and Auto-Attack over 500 samples by sweeping the \(_{2}\) perturbation level from 0 to 1 with a 0.05 increment for Auto-Attack. The cumulative RA of GREAT Score at a perturbation level \(r\) represents the fraction of samples with local GREAT Scores greater than \(r\), providing an attack-proof guarantee that no attacks can achieve a lower RA at the same perturbation level. For Auto-Attack, the RA at each perturbation level is calculated as the fraction of correctly classified samples under that specific perturbation. The blue curve in the figure represents the RA from empirical Auto-Attack, while the orange curve shows the RA derived from GREAT Score, offering a certified robustness guarantee. We observe that the trend of attack-independent certified robustness (GREAT Score) closely mirrors that of empirical attacks (Auto-Attack), suggesting that GREAT Score effectively reflects empirical robustness. It is important to note that the gap between our certified curve and the empirical curve of AutoAttack does not necessarily indicate inferiority of GREAT Score. Instead, this discrepancy could point to the existence of undiscovered adversarial examples at higher perturbation radii. This gap illustrates the fundamental difference between certified and empirical robustness measures, highlighting the potential for GREAT Score to provide a more conservative, yet guaranteed, estimate of model robustness.

Table 1 compares the global robustness statistics of the 17 grouped CIFAR-10 models on RobustBench for uncalibrated and calibrated versions respectively, in terms of the GREAT Score and the average distortion of CW attack, which again verifies GREAT Score is a certified lower bound on the true global robustness (see its definition in Section 3.1), while any attack with 100% attack success rate only gives an upper bound on the true global robustness. We also observe that calibration can indeed enlarge the GREAT Score and tighten its gap to the distortion of CW attack.

### Model Ranking on CIFAR-10 and ImageNet

Following the experiment setup in Section 4.1, we compare the model ranking on CIFAR-10 using GREAT Score (evaluated with generated samples), RobustBench (evaluated with Auto-Attack on the test set), and Auto-Attack (evaluated with Auto-Attack on generated samples). Table 2 presents their mutual rank correlation (higher value means more aligned ranking) with calibrated and uncalibrated versions. We note that there is an innate discrepancy between Spearman's rank correlation coefficient (way below 1) of RobustBench v.s. Auto-Attack, which means Auto-Attack will give inconsistent model rankings when evaluated on different data samples. In addition, GREAT Score measures _classification margin_, while AutoAttack measures _accuracy_ under a fixed perturbation budget \(\)

    &  &  AutoAttack \\ Accuracy(\%) \\  } &  Celebated \\ Accuracy(\%) \\  } &  Celebated \\ Accuracy(\%) \\  } &  CW \\ GREAT Score \\  } \\   Rebuffi\_extra  & 82.32 & 87.20 & 0.507 & 1.216 & 1.859 \\ Goual\_extra  & 80.53 & 85.60 & 0.534 & 1.213 & 1.324 \\ Rebuffi\_0.34pt  & 80.42 & 90.06 & 0.454 & 1.208 & 1.943 \\ Rebuffi\_28\_d3pt  & 78.80 & 90.00 & 0.424 & 1.214 & 1.796 \\ Araguin\_with\_extra  & 78.79 & 86.20 & 0.535 & 1.206 & 1.340 \\ Schwasz  & 77.24 & 89.20 & 0.227 & 1.143 & 1.392 \\ Auguin\_WIN  & 76.25 & 86.40 & 0.583 & 1.206 & 1.332 \\ Rade  & 76.15 & 86.60 & 0.413 & 1.200 & 1.486 \\ Rebuffi\_18k & 75.86 & 87.60 & 0.369 & 1.210 & 1.413 \\ Goyal  & 74.50 & 86.40 & 0.124 & 1.116 & 1.253 \\ Schwasz, JRJ  & 74.41 & 86.80 & 0.236 & 1.135 & 1.343 \\ Wu2020-Adversarial  & 73.46 & 84.60 & 0.128 & 1.110 & 1.369 \\ Aggaruind2020Adversarial  & 72.91 & 85.20 & 0.569 & 1.199 & 1.285 \\ Eng Engstrom1020HyRobustness  & 69.24 & 82.20 & 0.169 & 1.020 & 1.084 \\ Rice200Overfitting  & 67.68 & 81.80 & 0.152 & 1.040 & 1.097 \\ Roy2019Decoupling  & 66.44 & 79.20 & 0.275 & 1.101 & 1.165 \\ Ding2020DMA  & 66.09 & 77.60 & 0.112 & 0.909 & 1.095 \\   

Table 1: Comparison of (Calibrated) GREAT Score v.s. minimal distortion found by CW attack  on CIFAR-10. The results are averaged over 500 samples from StyleGAN2.

    \\    \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\   

Table 2: Spearmanâ€™s rank correlation coefficient on CIFAR-10 using GREAT Score, RobustBench (with test set

[MISSING_PAGE_FAIL:9]

### Evaluation on Online Facial Recognition APIs

To demonstrate GREAT Score enables robustness evaluation of black-box models that only provide model inference outcomes based on date inputs, we use synthetically generated face images with hidden attributes to evaluate six online face recognition APIs for gender classification. It is worth noting that GREAT Score is suited for privacy-sensitive assessment because it only uses synthetic face images for evaluation and does not require using real face images.

We use an off-the-shelf face image generator InterFaceGAN  trained on CelebA-HQ dataset , which can generate controllable high-quality face images with the choice of attributions such as eyeglasses, age, and expression. We generate four different groups (attributes) of face images for evaluation: Old, Young, With Eyeglasses, and Without Eyeglasses. For annotating the ground truth gender labels of the generated images, we use the gender predictions from the FAN classifier . In total, 500 gender-labeled face images are generated for each group. Appendix L shows some examples of the generated images for each group.

We evaluate the GREAT Score on six online APIs for gender classification: BetaFace , Inferdo , Arsa-Technology , DeepFace , Baidu  and Luxand . These APIs are "black-box" models to end users or an external model auditor because the model details are not revealed and only the model inference results returned by APIs (prediction probabilities on Male/Female) are provided.

Finally, we upload these images to the aforementioned online APIs and calculate the GREAT Score using the returned prediction results. Table 4 displays the group-level and overall GREAT Score results. Our evaluation reveals interesting observations. For instance, APIs such as BetaFace, Inferno, and DEEPFACE exhibit a large discrepancy for Old v.s. Young, while other APIs have comparable scores. For all APIs, the score of With Eyeglasses is consistently and significantly lower than that of Without Eyeglasses, which suggests that eyeglasses could be a common spurious feature that affects the group-level robustness in gender classification. The analysis demonstrates how GREAT Score can be used to study the group-level robustness of an access-limited model in a privacy-enhanced manner.

To verify our evaluation, in Table 5 we compare GREAT Score to the black-box square attack  with \(=2\) and \(\#\) queries\(=100\) on DEEPFACE. For both Age and Eyeglasses groups (Old v.s. Young and W/ v.s. W/O eyeglasses), we see consistently that a higher GREAT Score (second row) indicates better robust accuracy (%, first row) against square attack.

## 5 Conclusion

In this paper, we presented GREAT Score, a novel and computation-efficient attack-independent metric for global robustness evaluation against adversarial perturbations. GREAT Score uses an off-the-shelf generative model such as GANs for evaluation and enjoys theoretical guarantees on its estimation of the true global robustness. Its computation is lightweight and scalable because it only requires accessing the model predictions on the generated data samples. Our extensive experimental results on CIFAR-10 and ImageNet also verified high consistency between GREAT Score and the attack-based model ranking on RobustBench, demonstrating that GREAT Score can be used as an efficient measure complementary to existing robustness benchmarks. We also demonstrated the novel use of GREAT Score for the robustness evaluation of online facial recognition APIs.

**Limitations.** One limitation could be that our framework of global adversarial robustness evaluation using generative models is centered on \(_{2}\)-norm based perturbations. This limitation could be addressed if the Stein's Lemma can be extended for other \(_{p}\) norms.

   &  &  &  &  \\  & & & Eyeglasses & Eyeglasses & Total \\  BetaFace & 0.950 & 0.662 & 0.547 & 0.973 & 0.783 \\  Interloo & 0.707 & 0.487 & 0.458 & 0.669 & 0.580 \\  ARSA-Technology & 1.033 & 0.958 & 0.739 & 1.082 & 0.953 \\  DIPFACE & 0.979 & 0.774 & 0.763 & 0.969 & 0.872 \\  Baidu & 1.097 & 1.029 & 0.931 & 1.134 & 1.048 \\  Luxand & 1.091 & 0.912 & 0.673 & 1.010 & 0.944 \\  

Table 4: Group-wise and overall robustness evaluation for online gender classification APIs over 500 generated samples (per group).

  DEEPFACE & Old & Young &  With \\ Eyeglasses \\  & 
 Without \\ Eyeglasses \\  \\  Square Attack & 84.40\% & 72.60\% & 65.80\% & 89.00\% \\  GREAT Score & 0.979 & 0.774 & 0.763 & 0.969 \\  

Table 5: GREAT Score v.s. robust accuracy under square attack .