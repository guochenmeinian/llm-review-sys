# Beating Adversarial Low-Rank MDPs with Unknown Transition and Bandit Feedback

Haolin Liu

University of Virginia

srs8rh@virginia.edu

&Zakaria Mhammedi

Google Research

mhammedi@google.com

&Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

&Julian Zimmert

Google Research

zimmert@google.com

The authors are listed in alphabetical order.

###### Abstract

We consider regret minimization in low-rank MDPs with fixed transition and adversarial losses. Previous work has investigated this problem under either full-information loss feedback with unknown transitions (Zhao et al., 2024), or bandit loss feedback with known transition (Foster et al., 2022). First, we improve the \((d,A,H)T^{}{{6}}}\) regret bound of Zhao et al. (2024) to \((d,A,H)T^{}{{3}}}\) for the full-information unknown transition setting, where \(d\) is the rank of the transitions, \(A\) is the number of actions, \(H\) is the horizon length, and \(T\) is the number of episodes. Next, we initiate the study on the setting with bandit loss feedback and unknown transitions. Assuming that the loss has a linear structure, we propose both model-based and model-free algorithms achieving \((d,A,H)T^{}{{3}}}\) regret, though they are computationally inefficient. We also propose oracle-efficient model-free algorithms with \((d,A,H)T^{}{{5}}}\) regret. We show that the linear structure is necessary for the bandit case--without structure on the reward function, the regret has to scale polynomially with the number of states. This is contrary to the full-information case (Zhao et al., 2024), where the regret can be independent of the number of states even for unstructured reward function.

## 1 Introduction

We study online reinforcement learning (RL) in low-rank Markov Decision Processes (MDPs). Low-rank MDPs is a class of MDPs where the transition probability can be decomposed as an inner product between two low-dimensional features, i.e., \(P(x^{} x,a)=^{}(x,a)^{}^{}(x^{})\), where \(P(x^{} x,a)\) is the probability of transitioning to state \(x^{}\) when the learner takes action \(a\) on state \(x\), and \(^{}\), \(^{}\) are two feature mappings. The ground truth features \(^{}\) and \(^{}\) are unknown to the learner. This setting has recently caught theoretical attention due to its simplicity and expressiveness (Agarwal et al., 2020; Uehara et al., 2021; Zhang et al., 2022; Cheng et al., 2023; Modi et al., 2024; Zhang et al., 2022; Mhammedi et al., 2024; Huang et al., 2023). In particular, since the learner does not know the features, it is necessary for the learner to perform _feature learning_ (or _representation learning_) to approximate them. This allows low-rank MDPs to model the additional difficulty not present in traditional linear function approximation schemes where the features are given, such as in linear MDPs (Jin et al., 2020) and in linear mixture MDPs (Ayoub et al., 2020). Since feature learning is an indispensable part of modern deep RL pipelines, low-rank MDP is a model that is closer to practice than traditional linear function approximation.

Most prior theoretical work on low-rank MDPs focuses on reward-free learning; this is a setting where instead of focusing on a particular reward function, the goal is to learn a model for the transitions (or, in the model-free setting, a small set of policies with good state cover), that enables policy optimization for _any_ downstream reward functions. While this is a reasonable setup in some cases, in other applications, the learner can only obtain loss information from interactions with the environment, and only observes the loss on the state-actions that have been visited (i.e., bandit feedback). This introduces additional challenges to the learner.

Furthermore, in many online learning scenarios, the loss function may change over time, reflecting the non-stationary nature of the environment or task switches (Padakandla et al., 2020). This could be modeled by the _adversarial_ MDP setting, where the loss function changes arbitrarily from one episode to the next, and the changes might even depend on the behavior of the learner. This setting is also extensively studied, but mostly restricted to tabular MDPs (Rosenberg and Mansour, 2019; Jin et al., 2020; Shani et al., 2020; Luo et al., 2021) or traditional linear function approximation schemes (Cai et al., 2020; Luo et al., 2021; He et al., 2022; Zhao et al., 2022; Sherman et al., 2023; Dai et al., 2023; Liu et al., 2023). The work by Zhao et al. (2024) initiated the study on adversarial MDPs in low-rank MDPs, but their work is restricted to full-information loss feedback.

When feature learning, bandit feedback, and adversarial losses are combined, the problem becomes highly challenging, and to the best of our knowledge their are no provably efficient algorithms to tackle this setting. In this work, we provide the first result for this combination. We hope that our result would bring new ideas to RL in practice, where all three elements are usually present simultaneously. We give several main results, targeting at either tighter regret (i.e., the performance gap between the optimal policy and the learner) or computational efficiency, as summarized in Table 1. Below we give a brief introduction for each of them. A more thorough related work review is in Appendix A.

* \(T^{}{{3}}}\)**-regret algorithm under full-information feedback (Algorithm 1).** This setting is studied by the only prior work in adversarial low-rank MDPs (Zhao et al., 2024), and we greatly improve their \(T^{}{{6}}}\) regret bound to \(T^{}{{3}}}\). Our algorithm begins with a model-based initial exploration phase to estimate the transition. It then performs policy optimization where the critic is the \(Q\) value induced by the estimated transition and the full information loss.
* \(T^{}{{3}}}\)**-regret model-based/model-free inefficient algorithm under bandit feedback (Algorithm 2, Algorithm 5).** Algorithm 2 starts with a model-based initial exploration phase to learn an estimated transition, and then runs exponential weights over policy space for regret minimization in the second phase. To tackle bandit feedback, we construct a novel loss estimator that leverages the structure of low-rank MDP to perform accurate off-policy evaluation. Algorithm 5 starts with a different exploration phase, where it calls VoX(Mhammedi et al., 2023) to learn a policy cover; VoX is a model-free, reward-free exploration algorithm. After this initial exploratory phase, the algorithm also applies exponential weights and utilizes the same loss estimator as in Algorithm 2. However, due to its model-free nature, certain components of the estimator cannot be directly accessed and must be derived through specific optimizations.
* \(T^{4/5}\)**-regret model-free oracle-efficient algorithm under bandit feedback (Algorithm 3, Algorithm 4).** Algorithm 3 also starts with the model-free exploration algorithm VoX(Mhammedi et al., 2023) to learn a policy cover. After that, the algorithm operates in epochs; during epoch \(k\), the algorithm commits to a fixed mixture of policies. This mixture consists of certain exploratory policies (based on the policy cover from the initial phase) and a policy computed using an online learning algorithm based on estimated \(Q\)-functions from previous epochs (these serve as loss functions). Algorithm 4 deals with the much more challenging setting of an _adaptive_ adversary with bandit feedback. Here, we make the additional assumption that the loss feature, which may be different from the feature of the low-rank decomposition, is given. The algorithm is similar to Algorithm 3 with key differences outlined in Section 4.3.

## 2 Preliminaries

We study the episodic online reinforcement learning setting with horizon \(H\). We consider an MDP \(=(,,P^{*}_{1:H})\), where \(\) represents a countable (possibly infinite) state space2, \(\) is a finite action space, and \(P^{*}_{h}:()\) denotes the transition kernel from layer \(h\) to \(h+1\). We assume that the initial state \(x_{1}\) is fixed for simplicity without loss of generality. For any policy \(:()\) and arbitrary set of transition kernels \(\{P_{h}\}_{h[H]}\), we let \(^{P,}\) denote the law over \((_{1},_{1},,_{H},_{H})\) induced by the process of setting \(_{1}=x_{1}\), sampling \(_{1}_{1}(_{1})\), then for \(h=2,,H\), \(_{h} P_{h-1}(_{h-1},_{h-1})\) and \(_{h}_{h}(_{h})\). We let \(^{P,}\) denote the corresponding expectations. Further, we let \(d^{P,}_{h}(x)^{P,}[_{h}=x]\) denote the _occupancy_ of \(x\). We also let \(d^{P,}_{h}(x,a)^{P,}[_{h}=x,_{h}=a]\). Further, we let \(^{}=^{P^{*},}\), \(^{}=^{P^{*},}\), and \(d^{}_{h}=d^{P^{*},}_{h}\). We use \(_{h}^{}\) to denote a policy that follows \(_{k}()\) for \(k<h\) and \(^{}_{k}()\) for \(k h\). Similarly, \(_{h}^{}_{h^{}}^{}\) denotes a policy that follows \(_{k}\) for \(k<h\), \(^{}_{k}\) for \(h k<h^{}\) and \(^{}_{k}\) for \(k>h^{}\).

We consider a learner interacting with the MDP \(\) for \(T\) episodes with adversarial loss functions. Before the game starts, an oblivious adversary chooses the loss functions for all episodes \(^{t}_{1:H}:_{t=1 }^{T}\). For each episode \(t[T]\), the learner starts at state \(^{t}_{1}=x_{1}\), then for each step \(h[H]\) within episode \(t\), the learner observes state \(^{t}_{h}_{h}\), chooses an action \(^{t}_{h}\), then suffers loss \(^{t}_{h}(^{t}_{h},^{t}_{h})\). The state \(^{t}_{h+1}\) at the next step is drawn from transition \(P^{*}_{h}(^{t}_{h},^{t}_{h})\). We consider _bandit feedback_ setting where the learner could only observe the losses \(^{t}_{1}(^{t}_{1},^{t}_{1}),,^{t}_{H}(^{t}_{H},^{t}_{H})\) at the visited state-action pairs.

We let \(\{:()\}\) denote the set of Markovian policies. For policy \(\), loss \(\) and transition kernels \(P_{1:H}\), we denote by \(Q^{P,}_{h}(,;)\) the _state-action_ value function (a.k.a. \(Q\)-function) at step \(h[H]\) with respect to the transitions \(P_{1:H}\) and loss \(\); that is

\[Q^{P,}_{h}(x,a;)^{P,}[_{s=h}^{H}_{s} (^{t}_{s},^{t}_{s})_{h}=x,_{h}=a],\] (1)

for all \((x,a)\). We let \(V^{P,}_{h}(x;)=_{a}Q^{P,}_{h}(x,a;)\) be the corresponding _state_ value function at layer \(h\). Further, we write \(Q^{}_{h}(,;) Q^{P,}_{h}(,;)\) and \(V^{}_{h}(;) V^{P^{*},}(;)\).

For all of our algorithms except for Algorithm 4, we aim to construct (possibly randomized) policies \(\{^{t}\}_{t[T]}\) that ensure a sublinear _pseudo-regret_ with respect to the best-fixed policy; that is,

\[_{T}_{}_{T}() _{T}()[_ {t=1}^{T}V^{^{t}}_{1}(x_{1};^{t})-_{t=1}^{T}V^{}_{1}(x_{1}; ^{t})].\] (2)

ForAlgorithm 4, we bound the standard _regret_

\[}_{T}_{t=1}^{T}V^{^{t}}_{1}(x_{1}; ^{t})-_{}_{t=1}^{T}V^{}_{1}(x_{1};^{t})\] (3)with high probability. This allows it to handle adaptive adversary.

Throughout, we will assume that the MDP \(\) is low-rank with _unknown_ feature maps \(^{}_{h}\) and \(^{}_{h}\).

**Assumption 2.1** (Low-Rank MDP).: _There exist (unknown) features maps \(^{}_{1:H}:^{d}\) and \(^{}_{1:H}:^{d}\), such that for all \(h[H-1]\) and \((x,a,x^{})\):_

\[[_{h+1}=x^{}_{h}=x,_{h}=a]=^{}_ {h}(x,a)^{}^{}_{h+1}(x^{}).\] (4)

_Furthermore, for all \(h[H]\), the feature maps \(^{}_{h}\) and \(^{}_{h}\) are such that \(_{(x,a)}^{}_{h}(x,a) 1\) and \(\|_{x}g(x)^{}_{h}(x)\|\), for all \(g:\)._

Loss function under bandit feedback.For bandit feedback setting, we make the following additional linear assumption on the losses; in the sequel, we will argue that this is necessary to avoid a sample complexity scaling with the number of states. This linear loss assumption also appears in Ren et al. (2022); Zhang et al. (2022) for stochastic low-rank MDPs. Note that for the full-information feedback setting, such an assumption is not required.

**Assumption 2.2** (Loss Representation).: _For any \(t[T]\) and layer \(h\), there is a vector \(g^{t}_{h}_{d}(1)\) such that the loss \(^{t}_{h}(x,a)\) at round \(t\) satisfies:_

\[(x,a),^{t}_{h}(x,a)=^{ }_{h}(x,a)^{}g^{t}_{h}.\] (5)

We note that there is no loss of generality in assuming that the losses are expressed using the same features \(^{}_{1:H}\) as the low-rank structure in (4). This is because if the losses have different features, we can simply combine these features with the low-rank features, and redefine \(^{}\) accordingly. For the bulk of our results (and as stated in the prequel), we will assume that the losses \(\{^{t}_{h}(,)\}_{h[H],t[T]}\) (or equivalently \(\{g^{t}_{h}\}_{h[H],t[T]}\) under Assumption 2.2) are chosen by an adversary before the start of the game (i.e. oblivious adversary). In Section 4.3, we will present a model-free, oracle-efficient algorithm for an adaptive adversary.

Function approximation.So far, Assumption 2.1 and Assumption 2.2 are in line with assumptions made in the linear MDP setting (Jin et al., 2020). However, unlike in linear MDPs, we do not assume that the feature maps \(^{}_{1:H}\) are known. To facilitate representation learning and ultimately a sublinear regret, we need to make _realizability_ assumptions. In particular, in the model-free setting, we assume we have a function class \(\) that contains the true features \(^{}_{1:H}\). In the model-based setting, we additionally assume access to a function class \(\) that contains the feature maps \(^{}_{1:H}\)3. We will formalize these assumptions in their corresponding sections in the sequel.

Other notation.For \(:^{d}\), we define \((,)\) as a distribution \(()\) such that \(()^{2}_{G^{-1}} d\) for all \(\), where \(G=_{}()()()^{}\). This is the standard John's exploration or \(G\)-optimal design, which always exists.

## 3 Model-based Algorithms for Adversarial Low-rank MDPs

In this section, we discuss adversarial low-rank MDPs under model-based assumption. The model-based assumption is formalized in the Assumption 3.1 below. This assumption is standard which also appears in prior works on model-based learning in low-rank MDPs (Agarwal et al., 2020; Uehara et al., 2021; Zhang et al., 2022; Cheng et al., 2023; Zhao et al., 2024).

**Assumption 3.1** (Model-based assumption).: _The learner has access to two model spaces \(\) and \(\) such that \(^{}\) and \(^{}\). Moreover, for any \(\), \(\), and \(h[2H]\), we have \(_{(x,a)}_{h-1}(x,a) 1\), \(_{x^{}}_{h-1}(x,a)^{}_{h}(x^{})=1\) and \(|_{x}g(x)_{h}(x)\|\), for all \(g:\)._

### Adversarial Low-rank MDPs with Full Information

We first discuss learning adversarial low-rank MDPs with full information and model-based assumption. This setting aligns with Zhao et al. (2024), and our Algorithm 1 successfully improves their regret from \(T^{}{{6}}}\) to \(T^{}{{3}}}\).

As argued in Zhao et al. (2024), the challenge of learning adversarial low-rank MDPs lies in the need for balancing exploration and exploitation both in representation learning and policy optimization over adversarial losses. To tackle this doubled exploration and exploitation challenge, the algorithm of Zhao et al. (2024) performs _simultaneous_ representation learning and policy optimization. With a closer look at their analysis, we find that there is a drawback of this approach: because their algorithm handles the two tasks at the same time, it spends less exploration for representation learning in the early phase of the algorithm. This results in larger error in the estimated Q-values (i.e., critic) fed to policy optimization, and worsens the overall regret.

To address this issue, we design Algorithm 1 as a simple two-phase algorithm that separates representation learning and policy optimization. In the first phase, following Cheng et al. (2023), we perform optimal reward-free exploration for low-rank MDPs to estimate the transition. The resulted estimator, \(\), is able to accurately approximate the true transition and give accurate Q-value estimators for any policy. The more accurate Q-estimator allows for more effective policy optimization in the second phase. Theorem 3.1 shows the guarantee of Algorithm 1 where \(}\) hides logarithmic factors of \(d,H,T,||||\).

**Theorem 3.1**.: _Algorithm 1 ensures \(_{T}}(H^{3}(d^{2}+| |)T^{})\)._

The proof for Theorem 3.1 is given in Appendix B. Zhao et al. (2024) also constructs a lower bound \((H|T})\) for this settings. Thus, the \((||)\)-dependence is unavoidable.

### Model-Based, Computationally Inefficient Algorithm for Bandit Feedback

In this section, following Assumption 3.1, we introduce the first (model-based) algorithm (Algorithm 2) for adversarial low-rank MDPs with bandit feedback and sublinear regret. Compared with linear MDPs, the key challenge for more general low-rank MDPs is to construct a proper loss estimator. For linear MDP, since the feature is known, the loss estimator closely resembles that of linear bandits. However, low-rank MDPs lack such structural simplicity, making standard loss estimators invalid. To overcome this challenge, we propose a new loss estimator that works for any loss function based on off-policy evaluation and the low-rank structure of transition. In this section, \(}\) hides logarithmic factors of \(d,H,T,||||\).

In Algorithm 2, we first conduct an initial representation learning phase to establish accurate transition estimator \(\) and its corresponding features \(\) and \(\) based on reward-free exploration algorithms in Cheng et al. (2023). Then, in the second phase, we use exponential weights to maintain a distribution over the policy space \(^{}\) where we mix a uniform policy with \(\) to enhance exploration. At every round \(t\), a behavior policy \(^{t}\) is chosen from the current policy distribution, and we use the data collected by \(^{t}\) to estimate the value for every \(^{}\). The success of such off-policy evaluation is based on the following observations of low-rank MDP. Using the low-rank transition structure, for \(h 2\), we have

\[, d^{}_{h}(x)=^{*}_{h-1}()^{}^{*}_{h}(x ),^{*}_{h-1}()^{} ^{*}_{h-1}(_{h-1},_{h-1}).\] (6)Thus, using the definition of the \(V\)-function from Section 2, we have for any loss function \(\) and \(\):

\[V_{1}^{}(x_{1};)-[_{1}(_{1},_{1})]=_{h 2 }^{H}_{(x,a)}_{h-1}^{}()^{} _{h}^{}(x)_{h}(a x)_{h}(x,a).\] (7)

Letting \(_{h}^{t}(_{^{t} p^{t}}[_{h}^{}( ^{t})_{h}^{}(^{t})^{}])^{-1}\) and ignoring the loss term \([_{1}(_{1},_{1})]\) from the first step (this term can easily be treated as in a bandit setting with \(H=1\)), we have for all \(^{}\):

\[V_{1}^{}(x_{1};) =_{^{t} p^{t}}[_{h=2}^{H}_{(x,a) }_{h-1}^{}()^{}_{h-1}^{t} _{h-1}^{}(^{t})_{h-1}^{}(^{t})^{}_{h}^{}(x )_{h}(a x)_{h}(x,a)],\] \[=_{^{t} p^{t}}[_{h=2}^{H}_{x,a}_ {h-1}^{}()^{}_{h-1}^{t}_{h-1}^{}(^{t}) d_{ h}^{^{t}}(x)_{h}^{t}(a x)(a x)}{_{h}^{t}(a x )}_{h}(x,a)],\] \[=_{^{t} p^{t}}^{^{t}}[_{h=2 }^{H}_{h-1}^{}()^{}_{h-1}^{t}_{h-1}^{}(^{t} )(_{h}_{h})}{_{h}^{t}(_{h}_{h})}_{h}(_{h},_{h})].\]

Thus, for all \(\) and \(\), \(_{h=2}^{H}_{h-1}^{}()^{}_{h-1}^{t}_{h-1}^{ }(^{t})_{h}(_{h}_{h})_{h}^{t}(_{h} _{h})^{-1}_{h}(_{h},_{h})\) for \(^{t} p^{t}\) and \((_{h},_{h}) d_{h}^{^{t}}\) is an unbiased estimator of \(V_{1}^{}(x_{1},)\). However, \(_{h-1}^{}()\) is not accessible because both the true feature \(^{}\) and occupancy \(d^{}\) for the true transition are unknown. Thus, our estimator incorporates the learned feature \(\) and the occupancy of \(\) instead as shown in Line 4 and Line 10. Utilizing estimated features and transition could introduce additional bias but the initial representation learning already ensures such bias is small enough to tackle. We compensate for the bias by incorporating an exploration bonus \(b^{t}()\) in exponential weights. To further encourage exploration, we additionally perform John's exploration together with exponential weights when selecting behavior policies. The main guarantee of Algorithm 2 is given in Theorem 3.2.

**Theorem 3.2**.: _Algorithm 2 achieves \(_{T}()}(d^{2}H^{3}| |(d^{2}+||)T^{}{{3}}}||)\) for any \(\)._

Note that the guarantee in Theorem 3.2 only holds for policy \(\). To ensure our regret bound is meaningful, at least a near-optimal policy should be contained in the given policy set \(\). In general, the size of such a policy set would grow exponentially with the number of states (e.g. covering of all Markovian policies), making the regret have polynomial dependence on the number of states. In Theorem 3.3, we show that even for low-rank MDPs, if the loss function lacks structure, the regret cannot avoid polynomial dependence on the number of states. The detailed construction for this lower-bound is given in Appendix H.

**Theorem 3.3**.: _There exists a low-rank MDP with \(||\) states, \(||\) actions and sufficiently large \(T\) with unstructured losses such that any agent suffers at least regret of \((|||T})\)._

Theorem 3.3 shows that under bandit feedback, in general, we could not gain too much from low-rank transition structure compared with tabular MDPs. This contrasts with the \((|T})\) lower bound in the full information settings (Zhao et al. (2024)). To get rid of any dependence on the number of states, we additionally introduce Assumption 2.2 to impose linear structure on the loss function. Unlike linear MDPs that require the loss feature to be known, our algorithm can even handle linear loss with unknown feature, since our Algorithm 2 never explicitly uses the loss feature. The linear structure is only used to control the size of the candidate policy class in the analysis (i.e., making \(||\) irrelevant to the number of states). Specifically, when both loss and transition are linear, the Q-function is also linear, making it sufficient to consider the following linear policy space:

\[_{}=\{:\;|\;_{h}(a\; |\;x)=a=*{argmin}_{a} _{h}(x,a)^{}_{h}},\;h[H],\;|_{h}|_{2 }HT,\;\}.\]

The \(\)-cover of \(_{}\) only have size \(|| T^{(d)}\) following standard arguments (e.g, Exercise 27.6 of Lattimore and Szepesvari (2020)) and if we feed it into Algorithm 2, our regret could avoid dependece on the size of state space as shown in Corollary 3.1.

**Corollary 3.1**.: _If the loss function satisfies Assumption 2.2, applying Algorithm 2 with \(\) as the \(\)-cover of \(_{}\) ensures \(_{T}}(d^{3}H^{3}|| (d^{2}+||)T^{}{{3}}})\)._

## 4 Model-free Algorithms for Adversarial Low-rank MDPs

In this section, we consider the model-free setting, where we only assume access to a feature class \(\) that contains the true feature map \(^{}\).

**Assumption 4.1** (Model-free realizability).: _The learner has access to a function class \(\) such that_

\[^{}_{}_{(x,a )}\|(x,a)\| 1.\] (8)

This is a standard assumption in the context of model-free RL (Modi et al., 2024; Zhang et al., 2022b; Mhammedi et al., 2024a). We note that having access to the function class \(\) alone (instead of both \(\) and \(\) as in Assumption 3.1) is not sufficient to model the transition probabilities (unlike in the model-based case). This makes the model-free setting much more challenging; in fact, until the recent work by Mhammedi et al. (2023) there were no model-free, oracle-efficient algorithms for this setting that do not require any additional structural assumptions on the MDP.

### Model-free, Inefficient Algorithm for Bandit Feedback

Our first algorithm follows the same structure as Algorithm 2 but incorporates a model-free initial exploration phase introduced by Mhammedi et al. (2023). Unlike the model-based exploration phase, which directly provides an estimated transition, the model-free exploration phase outputs a policy cover. This policy cover can be combined with the optimization in Algorithm 1 of Liu et al. (2023) to solve the expected feature \(\), which is then used in the loss estimator in Line 10 of Algorithm 2. The algorithm, summarized in Algorithm 5, is inefficient but achieves \(T^{}{{3}}}\) regret. More details and proofs can be found in Appendix D.

### Model-free, Oracle Efficient Algorithm for Bandit Feedback (Oblivious Adversary)

We now descibe the key component of our efficient model-free algorithm (Algorithm 3).

Exploration phase and policy cover.Similar to algorithms in the previous section, Algorithm 3 begins with a reward-free exploratorion phase; Line 2 of Algorithm 3. However, unlike in the previous sections where the role of this exploration phase was to learn a model for the transition probabilities, here the goal is to compute a, so called, _policy cover_ which is a small set of policies that can be used to effectively explore the state space.

**Definition 4.1** (Approximate policy cover).: _For \(,(0,1]\) and \(h[H]\), a subset \(\) is an \((,)\)-policy cover for layer \(h\) if_

\[_{}d_{h}^{}(x)_{^{}}d_{h}^ {^{}}(x),x_{^{}}d_{h}^{^{}}(x) \|u_{h}^{}(x)\|.\] (10)In Line 2, Algorithm 3 calls Vox(Mhammedi et al., 2023), a reward-free and model-free exploration algorithm to compute \((1/(8Ad),)\)-policy covers \(_{1}^{},,_{H}^{}\) for layers \(1,,H\), respectively, with \(|_{h}|=d\) for all \(h[H]\). This call to Vox requires \(O(1/^{2})\) episodes; see the guarantee of Vox in Lemma G.1. After this initial phase, the algorithm operates in epochs, each consisting of \(N_{}\) episodes, where in each epoch \(k[K]\), the algorithm commits to executing policies sampled from a fixed policy distribution \(^{(k)}()\) with support on the policy covers \(_{1:H}^{}\) and a policy \(^{(k)}\) specified by an online learning algorithm. Next, we describe in more detail how \(^{(k)}\) is constructed and motivate the elements of its construction starting with the online learning policies \(\{^{(k)}\}_{k[K]}\).

Online learning policies.Given estimates \(\{_{1:H}^{(s)}\}_{s<k}\) of the average \(Q\)-functions

\[\{}}_{ts}Q_{1:H}^{^{(k)}}(,;^{t})\}_{s<k}\] (11)

from the previous epoch (we will describe how these estimates are computed in the sequel), Algorithm 3 computes policy \(^{(k)}\) for epoch \(k\) according to

\[_{h}^{(k)}(a x)(-_{s<k}_{h}^{(s)}(x,a))\!,\] (12)

for all \(h[H]\). Given a state \(x\), such exponential weight update ensures a sublinear regret with respect to the sequence of loss functions given by \(\{( x)_{h[H]}_{h}^{(k)}(x,_{h}(  x))\}_{k[K]}\). Thanks to the performance difference lemma, and as shown in Luo et al. (2021), a sublinear regret with respect to these "surrogate" loss functions translates into a sublinear regret in the low-rank MDP game we are interested in, granted that \(\{_{1:H}^{(k)}\}_{k[K]}\) are good estimates of the average \(Q\)-functions (Luo et al., 2021). In line with previous analyses, we require the \(Q\)-function estimates to ensure that the following bias term

\[^{}[_{a}(}} _{tk}Q_{h}^{^{(k)}}(_{h},a;^{t})- _{h}^{(k)}(_{h},a))^{2}]\] (13)

is small for all \(h[H]\), \(k[K]\), and \(\).

\(Q\)-function estimates.Thanks to the low-rank MDP structure and the linear loss representation assumption (Assumption 2.2), the average \(Q\)-functions in (11) are linear in the feature maps \(^{}\). Thus, using the function class \(\) in Assumption 2.1 we can estimate these average \(Q\)-functions by regressing the sum of losses \(_{s=h}^{H}_{s}^{}\) onto \((_{h}^{t},_{h}^{t})\) for \(t\) in the \(k\)th epoch (as in (9)). However, naively doing this using only trajectories generated by \(^{(k)}\) would only ensure that the bias term in (13) is small for \(=^{(k)}\). To ensure that it is small for all possible policies \(\)'s, we need to estimate the \(Q\)-functions on the trajectories of policies that are guaranteed to have good state coverage; this is where we use the policy cover from the initial phase.

Mixture of policies.At episode \(t\) in each epoch \(k[K]\), we execute policy \(}^{t}\) sampled from \(^{(k)}\), where \(^{(k)}\) is the distribution of the random policy:

\[\{^{t}=0\}^{(k)}+\{ ^{t}=1\}^{t}_{^{t}}_{}_{^{t }+1}^{(k)},\] (14)

with \(^{t}()\), \(^{t}[H]\), and \(^{t}^{}_{^{t}}\). In words, at the start of each episode of any epoch \(k\), we execute \(^{(k)}\) (see (12)) with probability \(1-\); and with probability \(\), we execute a policy in \(^{}_{1:H}\) selected uniformly at random. As explained in the previous paragraph, this ensures a small bias for all choices of \(\) in (13) thanks the policy cover property of \(^{}_{1:H}\). We now state the guarantee of Algorithm 3.

**Theorem 4.1**.: _Let \((0,1)\) be given and suppose Assumption 2.1 and Assumption 2.2 hold. This, for \(T=(A,d,H,(||/))\) sufficiently large, Algorithm 3 guarantees \(_{T}(A,d,H,(||/)) T^{4/5}\) regret against an oblivious adversary._

The proof is in Appendix E. Note that the \(T\)-dependence in this regret even outperforms that of the previous best bound by Zhao et al. (2024) (see Table 1). Compared to their algorithm, Algorithm 3 is model-free and only requires bandit feedback. This makes the result in Theorem 4.1 rather surprising.

### Model-free, Oracle Efficient Algorithm (Adaptive Adversary)

In this section, we present a variant of Algorithm 3 (Algorithm 4) that guarantees a sublinear regret against an adaptive adversary. Given the difficulty of this setting, we make the additional assumption that the algorithm has access to the loss feature \(^{}\), which may be different than the low-rank MDP feature \(^{}\) (unlike in Assumption 2.2).

**Assumption 4.2** (Loss Representation).: _There is a (known) feature map \(^{}\) satisfying \(_{t[H],(x,a)}^{}_{h}(x,a) 1\) and such that for any round \(h[H]\), \(t[T]\), and history \(^{t-1}=(x_{1:H}^{1:t-1},a_{1:H}^{1:t-1})\), the loss function at round \(t\) satisfies_

\[(x,a),_{h}(x,a;^{t -1})=^{}_{h}(x,a)^{}g^{t}_{h},\] (15)

_for some \(g^{t}_{h}_{d}(1)\)._

Note that Assumption 4.2 asserts that the loss at round \(t\) depends only on the history \(^{t-1}\) and the current state action pair. Before moving forward, we introduce some additional notation we will use throughout this section.

Additional notation.For any two feature maps \(,:^{d}\), we denote by \([,]:^{2d}\) the vertical concatenation of the two feature maps. For any \(h[H]\), \(t[T]\), policy \(\), and history \(^{t-1}=(x_{1:H}^{1:t-1},a_{1:H}^{1:t-1})\), we denote by \(Q^{}_{h}(,;^{t-1})\) the \(Q\)-function at layer \(h\) corresponding to rollout policy \(\); that is,

\[Q^{}_{h}(x,a;^{t-1})^{}[_{s=h}^{H }_{s}(_{s},_{s};^{t-1})_{h}=x,_{h}= a].\] (16)

Finally, we let \(V^{}_{h}(x;^{t-1})_{a}Q^{}_{h}(x,a ;^{t-1})\) denote the corresponding \(V\)-function.

Algorithm 4 is similar to Algorithm 3 with the following key differences; after computing a policy cover, the algorithm calls RepLearn (a representation learning algorithm initially introduced by Modi et al. (2024) and subsequently refined by Mhammedi et al. (2023)) to compute a feature map \(^{}\). Then, for every \(h[H]\), the algorithm computes a _spanner_; a set of policies \(^{}_{h}=\{_{h,1},,_{h,2d}\}\) that act as an approximate spanner for the set \(\{^{}[^{}_{h}(_{h},_{h}),^{ }_{h}(_{h},_{h})]:\}^{2d}\), where we use \([,]\) to denote the vertical concatenation of vectors. These spanner policies are then used as the exploratory policies after the initial phase; that is, at episode \(t\) in each epoch \(k[K]\), we execute policy \(^{(k)}\) sampled from \(^{(k)}\), where \(^{(k)}\) is set to be the distribution of the random policy: \([^{t}=0\}}^{(k)}+[ ^{t}=1\}^{t}_{^{t+1}}}^{(k)}\), with \(^{t}()\), \(^{t}([H])\), and \(^{t}(_{^{}}^{})\). Here, the main difference to Algorithm 3 (see also (47)) is that we use \(^{t}(_{^{}}^{})\) instead of \(^{t}(_{^{}}^{})\). We require these spanner policies instead of policies in the policy cover, as an adaptive adversary's history-dependent losses prevent standard least squares regression due to the lack of permutation invariance of state-action pairs across episodes within an epoch. Estimating the Q-functions is thus more complex, and we approach it in expectation over roll-ins using policies in \(^{}\), the "in-expectation" estimation task is in a sense easier.

We now state the guarantee of Algorithm 4.

**Theorem 4.2**.: _Let \((0,1)\) be given and suppose that Assumption 2.1 and Assumption 4.2 hold. Then, for \(T=(A,d,H,(||/))\) sufficiently large, Algorithm 4 guarantees with probability at least \(1-\),_

\[_{t[T]}V_{h}^{^{t}}(x_{1};}^{t-1})-_{ }_{t[T]}V_{h}^{}(x_{1};}^{t-1})(A,d,H,(||/)) T^{}{{5}}},\] (17)

_where \(^{t}\) is the policy that Algorithm 4 executes at episode \(t[T]\)._

```
1:Number of rounds \(T\), feature class \(\), loss feature \(^{}\), confidence parameter \((0,1)\).
2:Set \( T^{-1/3}\), \(N_{} T^{2/3}\), \( N_{}^{-1/4}\), and \((8Ad)^{-1}\), \(T_{}^{-2}Ad^{13}H^{6}(/)\).
3:Set \(T_{}^{-1}^{-2}AH(||/)\), \(T_{}^{-2}^{-2}A(dH|| ^{-1}^{-1})\).
4:Define \(_{h}=(x,a)_{a}_{h}(x,a)^ {}_{h}=[_{h}^{},_{h}], ,_{2d}(1)}\), \( h[H]\).
5:Get \(_{}^{}(,, /4)\).
6:Get \(_{h}^{}(h,_{h+1},, (_{h}^{}),T_{})\), for all \(h[H-1]\). //RepLearn as in Mhammedi et al. (2023)
7:For all \(h[H]\), set \(_{h}^{}[_{h}^{},_{h}^{ }]^{2d}\).
8:For \(h[H]\), set \(_{h}^{}(h,,_{1:h}^{},_{h}^{},T_{})\). //Algorithm 7
9:Set \(T_{0} T_{}+T_{}+T_{}\).
10:for\(k=1,,(T-T_{0})/N_{}\)do
11:Define \(_{h}^{(k)}(a x)(-_{s<k}_{ h}^{(s)}(x,a))\) for \(h[H]\).
12:for\(t=T_{0}+(k-1) N_{}+1,\ ,\ T_{0}+k N_{}\)do
13:Define the random variables \(^{t}()\), \(^{t}([H])\), and \(^{t}(_{^{}}^{})\).
14:Set \(}^{t}=[^{t}=0\}}^{( k)}+[^{t}=1\}^{t}_{^{t+1}} }^{(k)})\).
15:Execute \(}^{t}\), and observe trajectory \((_{1}^{t},_{1}^{t},,_{H}^{t},_{H}^{t})\).
16:For \(h[H]\), observe loss \(_{h}^{t}_{h}(_{h}^{t},_{h}^{t};}^{t-1})\), where \(}^{t-1}(_{1:H}^{1:t-1},_{1:H}^{1:t-1})\).
17:endfor
18:For \(h[H]\) and \(^{(k)}=\{T_{0}+(k-1) N_{}+1,\ ,\ T_{0}+k N_{}\}\), compute \(_{h}^{(k)}\) such that \[_{h}^{(k)}*{argmin}_{_{2 d}(4Hd^{2})}_{_{h}^{}}|_{t^{(k)}} \{^{t}=h,^{t}=,^{t}=1\}(_{h}^{}(_{h}^{t},_{h}^{t})^{}-_{s=h}^ {H}_{s}^{t})|\] (19)
19:Set \(_{h}^{(k)}(x,a)=_{h}^{}(x,a)^{}_{h}^{(k)}\), for all \((x,a)\).
20:endfor ```

**Algorithm 4** Oracle Efficient Algorithm for Adversarial Low-Rank MDPs (Adaptive Adversary).

## 5 Conclusion

In this paper, we focus on learning low-rank MDPs with unknown transitions and adversarial losses. For the full-information setting, we improve upon previous regret bounds. More importantly, we initiate the study of the challenging bandit feedback setting, developing various algorithms that achieve sublinear regret under different assumptions. However, the optimal \(\) regret remains out of reach due to the limitations of our two-phase design. An interesting direction for future work is to perform on-the-fly representation learning to adapt to adversarial losses and achieve optimal regret.