# D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup

Joanna Waczynska

Doctoral School of Exact and Natural Sciences

Jagiellonian University

&Piotr Borycki

Faculty of Mathematics and Computer Science

Jagiellonian University

&Joanna Kaleta

Warsaw University of Technology

Sano Centre for Computational Medicine &Slawomir Tadeja

Department of Engineering

University of Cambridge &Przemyslaw Spurek

Jagiellonian University

IDEAS NCBR

###### Abstract

Over the past years, we have observed an abundance of approaches for modeling dynamic 3D scenes using Gaussian Splatting (GS). These solutions use GS to represent the scene's structure and the neural network to model dynamics. Such approaches allow fast rendering and extracting each element of such a dynamic scene. However, modifying such objects over time is challenging. SC-GS (Sparse Controlled Gaussian Splatting) enhanced with Deformed Control Points partially solves this issue. However, this approach necessitates selecting elements that need to be kept fixed, as well as centroids that should be adjusted throughout editing. Moreover, this task poses additional difficulties regarding the re-productivity of such editing. To address this, we propose **D**ynamic **M**ulti-Gaussian **S**oup (D-MiSo), which allows us to model the mesh-inspired representation of dynamic GS. Additionally, we propose a strategy of linking parameterized Gaussian splats, forming a Triangle Soup with the estimated mesh. Consequently, we can separately construct new trajectories for the 3D objects composing the scene. Thus, we can make the scene's dynamic editable over time or while maintaining partial dynamics.

## 1 Introduction

Recently introduced Gaussian Splatting (GS)  represents the 3D scene structure through Gaussian components. We can combine GS with the neural network (i.e., deform network) to model dynamic scenes . This approach involves the joint training of both the GS components and the neural network. GS characterizes the 3D object's shape and color, while the neural network utilizes time embedding and Gaussian parameters to generate updated initial positions to model dynamic scenes. Such an approach allows for fast rendering and extracting each element of a dynamic scene.

Most existing methods can effectively model dynamic scenes, but generating new 3D objects' positions remain challenging. Consequently, we cannot edit objects over time when using such

Figure 1: D-MiSo model parameterized dynamic scenes by Triangle Soup (disjoint triangles cloud), which allows modification of objects during time.

approaches. To tackle this issue, SC-GS (Sparse Controlled Gaussian Splatting)  uses Deformed Control Points to manage Gaussians. After the training phase, we can manually modify the model at any point in time. However, this method requires identifying elements to remain static and adjusting 3D objects' centroids (nodes) during editing when a relationship between the selected nodes is visible. For example, by moving the humanoid 3D model's hand, the part of the head or leg is also changed.

To address this issue, we introduce **D**ynamic **M**ulti-**G**aussian **S**oup (D-MiSo), which is easier to modify (Fig. 1) and obtain renders comparable to SC-GS. D-MiSo estimates the mesh as Triangle Soup, i.e. a set of disconnected triangle faces , and uses a dynamic function to control the vertices.

D-MiSo employing Multi-Gaussians, defined as larger Core-Gaussians encompassing smaller ones termed Sub-Gaussians (Fig. 2). Sub-Gaussians are defined in the local coordinate system given by principal components of Core-Gaussian. Therefore, by modifying Core-Gaussian, we change all Sub-Gaussian, which allows scene modifications (Fig 3). Core-Gaussians are an alternative to the control points discussed in , with the added advantage of allowing individual modifications. Consequently, there is no necessity for static and dynamic markers. In Fig. 4, we present the difference between modification applied by SC-GS and D-MiSo.

Our model uses flat Gaussians. Therefore, based on GaMeS , we can approximate Gaussian components using triangle face mesh by parameterizing Gaussian components by the vertices of the mesh face. We denote such transformation by \(()\). In practice, as a consequence of parameterizing each Gaussian, we obtain a cloud of triangles called Triangle Soup . Using Triangle Soup, we can control two types of Gaussian components. Accordingly, in D-MiSo, we get: Sub-Triangle Soup, Core-Triangle Soup and Multi-Triangle Soup (Fig. 2, Core-Triangle Soup is marked by red color, and Sub-Triangle Soup is denoted in blue). In D-MiSo, we can select and modify one part of the object like a mesh. In contrast, using SC-GS, static and dynamic points have to be selected, and editing only one part of the object is difficult.

During training, the positions of Core-Gaussians are managed by deformation multilayer perceptron (MLP), while the Sub-Gaussians are collectively manipulated through global transformation and small local deformation. The former describes the general flow of objects in the scene, while the local deformation is responsible for modeling small changes like shadows and light reflections. After training, we can modify our model directly by using the vertex of the Sub-Triangle Soup, or we can generate mesh from the Core-Triangle Soup (Fig. 5).

The contributions of this paper are significant and are outlined as follows:

* We introduce the Multi-Gaussian components, which consist of a single large Gaussian response for global transformations and many small components dedicated to rendering. Multi-Gaussian components allow for the modeling of large 3D scene elements.
* We propose D-MiSo a model that uses Multi-Gaussian components and two deformation networks for modeling dynamic scenes.

Figure 3: D-MiSo allows us to modify scenes in similar ways as classical mesh-based models.

Figure 2: Each object using the D-MiSo model is represented by Core-Gaussians and Sub-Gaussians, which form Multi-Gaussians. Each Gaussian is related to a triangle using parameterization proposed in GaMeS . Triangles define the Gaussian shape (i.e., location, scale, rotation), and triangles clouds form Triangles Soups.

* Our D-MiSo allows an object to be edited at a selected moment in time. The edited components are independent, and the editing does not affect other parts of the object. In addition, it also allows for full or partial dynamics to be maintained. Modifications also include scaling and rotation.

## 2 Related Works

Recent advancements in view synthesis, particularly driven by NeRFs , have significantly contributed to the rapid development of novel view synthesis techniques. However, the majority of these approaches model static scenes implicitly using MLP. Moreover, several works have extended classical NeRF to dynamic scenes through the use of deformation fields [8; 9; 10] and . The alternative approaches, such as  and , represent scenes as 4D radiance fields. Early works on dynamic scenes face difficulties when dealing with monocular settings and uncontrolled or lengthy scenarios. To enhance scene motion modeling, some works utilize flow-based techniques [10; 14; 15]. However, NeRF-based solutions often suffer from long training and rendering times. To address this, grid-plane-based methods [16; 17; 18] have been proposed. In addition, several NeRF-based approaches have also been extended for scene editing purposes [19; 20; 21; 22].

The recently introduced Gaussian Splatting (GS) technique  addresses many limitations of other methods, offering multiple advantages due to their explicit geometry representation, enabling easier dynamics modeling. The efficient rendering of the 3D version of GS also avoids densely sampling and querying neural fields, making downstream applications such as free-viewpoint video reconstruction more feasible. A notable extension of 3D GS was proposed in , where the authors introduced the concept of anchor points to tackle the problem of overfitting caused by redundant Gaussians. Scaffold-GS addresses this issue by distributing local 3D Gaussians according to anchor points. This approach reduces redundancy, enhances scene coverage, and maintains high-quality rendering with improved robustness to view changes. While the original GS was developed for static scenes, several extensions for dynamic scenes were proposed. Most of the early works operate in multiview setup [24; 25; 26]. For example,  utilizes a frame-by-frame approach to model each timestep. However, this method lacks inter-frame correlation and requires high storage overhead for long-term sequences. In [27; 2], MLP is introduced to model changes in Gaussians over time, and in  MLP together with decomposed neural voxel encoding algorithm are utilized for training and storage efficiency. In , dynamic scenes are divided into dynamic and static parts, optimized separately and rendered together to achieve decoupling.  spacetime Gaussian proposes approximating the spatiotemporal 4D volume of a dynamic scene by optimizing a collection of 4D primitives with explicit geometry and appearance modeling. This method uses 4D Gaussians parameterized by anisotropic ellipses and

Figure 4: Comparison of possible modifications in D-MiSo and the SC-GS. In the latter, authors use nodes while D-MiSo apply Sub-Triangle Soup (see the second column). We also must add static (pink) and dynamic (yellow) points in SC-GS to obtain modification by editing dynamic points. In practice, we have to use many static points to stop artifacts. Moreover, SC-GS is not an affine invariant and produces space when we change the size of the objects. In the case of D-MiSo, we marked points and applied modifications. Our model is superior in handling object scaling.

view-dependent, time-evolved appearances represented by 4D spherical harmonics coefficients. In , a novel, real-time and photorealistic scene representation called Spacetime Gaussian Feature Splatting has been introduced. This approach extends 3D Gaussians with temporal opacity and parametric motion/rotation, enabling the capture of static, dynamic, and transient scene content. Additionally, the method incorporates splatted feature rendering to model view- and time-dependent appearances while maintaining a compact representation size. The method is notable for its high rendering quality and speed while also being storage-efficient. Other works enhance dynamic scene reconstruction using external priors. For example, the diffusion priors can be used as regularization terms during optimization .

Furthermore, GS was employed for mesh-based scene geometry editing. In , 3D Gaussians are defined over an explicit mesh and utilize mesh rendering to guide adaptive refinement. This approach depends on the extracted mesh as a proxy and fails if the mesh cannot be extracted. In contrast, in , explicit meshes are extracted from 3D GS representations by regularizing Gaussians over surfaces. However, this method involves a costly optimization and refinement pipeline. Another example of  employs sparse control points for 3D scene dynamics, but this method struggles with intense edit movements and necessitates accurate static node selection. Also,  combines GS with mesh extraction. However, such an approach only works for static scenes.

The method proposed in  combines meshes with GS and reconstructs a high-fidelity and time-consistent mesh from a single monocular video. However, it relies on a Poisson Solver and differentiable Marching Cubes to recover the deformed surface, significantly complicating the pipeline. Moreover, it does not explore geometry modification capabilities, which constitute a significant aspect of our work. Conversely, cage-based methods  are an intuitive tool for geometry manipulation. However, they require additional steps for cage-building and may lack the flexibility and precision of manual, vertex-level deformation techniques, potentially missing fine details.

In contrast to the listed approaches, we propose a D-MiSo, a mesh-based method specifically designed to handle dynamic scenes. D-MiSo leverages a straightforward pipeline of GS techniques to enable real-time editing of dynamic scenes.

## 3 Dynamic Multi-Gaussian Soup

Here, we present the main components of D-MiSo. We start with the classical GS to provide the foundations for our model. Next, we introduce the concept of Multi-Gaussians and describe how to

Figure 5: One way to modify the object at the selected time \(t_{i}\) is to take Core-Gaussians and apply a meshing strategy to obtain the correct mesh instead of Triangle Soup. Then, we can parametrize Sub-Gaussian in the coordinate system given bay mesh faces instead of Core-Triangle Soup. Finally, we can modify our mesh to obtain new modifications.

Figure 6: Multi-Gaussian, consisting of one Core-Gaussian \(_{V_{j}}\) and a Sub-Gaussian \(_{V_{j}}^{i}\). The Core Gaussian is parametrized by a \(V_{j}\)-triangle, and the Sub-Gaussian by a \(V_{j}^{i}\)-triangle. The relative distance of the center of the Sub-Gaussian from the Core-Gaussian is indicated by \(}=(_{1}^{i},_{2}^{i},_{3}^{i})\).

estimate the mesh for editing. Finally, we show D-MiSo, which uses Multi-Gaussians in dynamic 3D scenes.

Gaussian SplattingThe Gaussian Splatting (GS) technique models 3D scenes using an array of 3D Gaussians, each specified by its mean position, covariance matrix, opacity, and color expressed using spherical harmonics (SH) [37; 38]. The GS algorithm constructs the radiance field by iteratively optimizing the parameters of all Gaussian components. Ultimately, the GS efficiency mainly depends on its rendering method, which involves projecting Gaussian components.

The GS framework employs a dense collection of 3D Gaussians: \(=\{((_{i},_{i}),_{i},c_{i})\}_{i=1 }^{n},\) where \(_{i}\) denotes the position, \(_{i}\) the covariance, \(_{i}\) the opacity, and \(c_{i}\) the SH colors for the \(i\)-th Gaussian. The GS optimization process involves a repetitive cycle of rendering and comparing the resultant images with the training views. In our work, we will use Multi-Gaussian approaches (Fig. 6).

Multi-GaussiansMulti-Gaussians \(_{multi}\) are dedicated to describing relatively large parts of the 3D scene to allow modification of large blocks instead of modifying each small Gaussian separately. The Multi-Gaussian model comprises a primary large 3D Gaussian (referred to as the Core-Gaussian \(_{core}\)), which encompasses numerous smaller Gaussians (termed Sub-Gaussians \(_{sub}\)), all of which are parameterized by the main Core-Gaussian. Multi-Gaussianare is similar to anchor Gaussians from , but we do not use a neural network to produce child components. We parametrize Sub-Gaussians in a local coordinate system.

Similarly to classical GS, we parameterize the Core-Gaussian distribution by center \(\) and the covariance parameterized by factorization: \(=RSSR^{T},\) where \(R\) is the rotation matrix and \(S\) the scaling parameters. More precisely we consider \(p\) Core-Gaussians uses flat Gaussians as in , and is defined by:

\[_{core}=\{(_{core}(_{i},R_{i},S_{i}),_{i},c_{i})\}_{i=1}^{p}, \]

where \(S=(s_{1},s_{2},s_{3}),\)\(s_{1}=\) and \(R\) is rotation matrix of Core-Gaussian defined as: \(R=(_{1},_{2},_{3}),\) where \(_{i}^{3}\) which can be interpreted as a local coordinate system used by Sub-Gaussian.

Figure 8: Representation of change over time acting on Core-Gaussians using a neural network responsible for movement. In practice, \(t_{i}\) is an abstract time. The network’s output returns information about the change in location \( v_{1},\) scale (\( v_{2},\)\( v_{3}\)), and rotation \( R.\)

Figure 7: D-MiSo model diagram. The input of the model consists of images at different moments in time and information regarding the position of the camera. Training distinguishes two main phases (i.e., stages). The first includes the preparation of Core-Gaussians, describing the movement of the object. The second focuses on the fitting of Sub-Gaussians responsible for the render’s quality. The model has the ability to produce high-quality renders or create an animation/modification of the object due to Sub-Gaussians (i.e., Sub-Triangles Soup) shape modification.

It is worth noting that Sub-Gaussian can be interpreted as a child of Core-Gaussian. We define centers of Sub-Gaussian \(_{sub}(^{i},R^{i},S^{i})\) in the local coordinate system of Core-Gaussian \(_{core}(,R,S)\) by: \(^{i}=+R^{iT}\), where \(\), \(R\) is Core-Gaussian position and rotation; and \(^{i}=(_{1}^{i},_{2}^{i},_{3}^{i})\) are trainable parameters used to define the positions of the Sub-Gaussian relative to the Core-Gaussian (Fig. 6). Sub-Gaussians are used for rendering and can be seen as a main component of our model:

\[_{sub}=\{(_{sub}(+R^{iT},R^{i},S^{i}),^{i},c^{i})\}_{i=1}^{k}, \]

where \(,R,S\) are parameters of Core-Gaussian and, \(^{i},S^{i},R^{i}\) marks parameters of \(i\)-th Sub-Gaussians with opacity \(^{i}\), and SH colors \(c^{i}\).

Core-Gaussians is generally dedicated mainly to transformations, hence, in practice opacity or colors are not used during rendering. On the other hand, Sub-Gaussian is devoted to rendering and modification. It has its own opacity and colors.

#### 3.2.2 GaMeS parametrisation of Multi-Gaussian component

Multi-Gaussians describe 3D scenes using solid blocks rather than tiny Gaussian distributions. This method enhances our model's suitability for dynamic environments. One of our mode's most important properties is its ability to model dynamic scenes in each time step. To obtain such properties, we parameterize all Gaussian using Triangle Soup following similar approach as in GaMeS . Thanks to a few simple transformations, we can convert the mean \(\) rotation \(R\) and scaling \(S\) into triangle \(V=(_{1},_{2},_{3})\), which parametrizes Gaussian distribution. Such transformation is unambiguous and reversible.

Let us assume that we have a Gaussian component parameterized by mean \(\), rotation matrix \(R=[_{1},_{2},_{3}]\) and scaling \(S=(,s_{2},s_{3})\). We define three vertex of a triangle (face): \(V=[_{1},_{2},_{3}]\), where \(_{1}=\), \(_{2}=+s_{2}_{2}\), \(_{3}=+s_{3}_{3}\).

Now we froze the vertex of the face \(V=[_{1},_{2},_{3}]\) and reparameterize the Gaussian component by defining \(}\), \(=[}_{1},}_{2},}_{3}]\) and \(=(_{1},_{2},_{3})\). First, we put \(}=_{1}\). The first vertex of \(\) is given by a normal vector:

\[}_{1}=_{2}-_{1})(_{3 }-_{1})}{\|(_{2}-_{1})(_{3}- _{1})\|},\]

where \(\) is the cross product. The second one is defined by \(}_{2}=_{2}-_{1})}{\|(_{2} -_{1})\|}\). The third one is obtained as a single step in the Gram-Schmidt process :

\[}_{3}=(_{3}-_{1};_{ 1},_{2}).\]

Scaling parameters can also be easily calculated as \(s_{1}=\), \(_{2}=\|_{2}-_{1}\|\) and \(_{3}=_{3}-_{1},}_{3}\). Consequently, the covariance of Gaussian distribution positioned on face is given by:

\[_{V}=_{V}_{V}_{V}_{V}^{T},\]

and correspond with the shape of a triangle \(V\). For one face \((_{1},_{2},_{3})\), we define the corresponding Gaussian component:

\[((_{1},_{2},_{3}))=(}_{V},_{V},_{V}).\]

Figure 10: An example of Sub-Triangle Soup modification using D-MiSo and the render obtained by this change from a different viewpoint. It is possible not only to change the position of the hand but also to raise the thumb. Comparison with SC-GS similar modifications, highlighting the challenge of editing small elements individually.

Figure 9: One way to animate is to assign the nearest triangle (face) to each Sub-Gaussian from the estimated mesh. The mesh modification changes the assigned Gaussian.

Finally, the Gaussian component is derived from the mesh face parameters. This approach can be applied in a Multi-Gaussian framework. Therefore, we will use invertible transformation between Gaussian parameters and triangle face \(T\) and notation:

\[(V):=(^{-1}(V))=(}_{V },_{V},_{V}).\]

In our D-MiSo we parameterize the \(p\) Core-Gaussians with \(k\) Triangle soup:

\[_{multi}=\{(_{core}(V_{j}),\{( _{sub}(}_{V_{j}}+_{V_{j}}^{tT },^{i},^{i}),^{i},c^{i})\}_{i=1}^{k} )\}_{j=1}^{p}, \]

where \(V_{j}\), \(}\), \(R^{i}\),\(^{i}\), \(^{i}\), \(c^{i}\) are trainable parameters and \((}_{V_{j}},_{V_{j}},_{V_{j}})=^{-1}(V_ {j})\). Alternatively, we can parameterize Core-Gaussian and Sub-Gaussians by Triangle Soup:

\[_{multi}=\{(_{core}(V_{j}),\{( _{sub}(V_{j}^{i}),^{i},c^{i})\}_{i=1} ^{k})\}_{j=1}^{p}, \]

where \((}_{V_{j}},_{V_{j}},_{V_{j}})=^{-1}(V_ {j})\), \(V^{i}=(}_{V}+_{V}^{tT},^{i}, ^{i})\).

In D-MiSo, we use the collation of Multi-Gaussian distribution for rendering and Sub-Triangle Soup for editing. The formal definition of our model uses equation (3) since, in training, we store Core-Gaussian as a triangle face (Core-Gaussian does not have colors) and Sub-Gaussian as a collection from classical GS components with color and opacity. After training, we parametrize our model to equation (4) for editing.

### Dynamic Multi-Gaussian Soup (D-MiSo)

Previously, we defined Multi-Gaussians and their parametrization using Triangle Soup. Now, we have all the tools to present the D-MiSo model. The overview of our method is illustrated in Fig. 7. The input to our model is a set of images of a dynamic scene, together with the time label and the corresponding camera poses. Our training is divided into two stages. In the first stage, we initialize the Core-Gaussians. In the second, we add Sub-Gaussian components.

Stage 1First, we train only Core-Gaussians to obtain good initialization for Multi-Gaussins. As Core-Gaussians are mainly employed to capture motion, our model only requires their small amount (Fig. 7). In our approach, the Core-Triangle Soup, constructed via the Core-Gaussians parameterization (as depicted in Fig. 2), is adjusted depending on the time \(t\).

In practice, when random initialization of Gaussians is necessary, redundant Gaussians must be pruned first to ensure that the remaining ones represent the object's shape. To reduce the number of Gaussians

Figure 11: Reconstruction and three ways of modification of the output object. The first involves modifying the estimated mesh, which does not have to be accurate. The next two focus on Sub-Triangle Soup editing. The red box shows the direct modification of the Triangle Soup in a logical way (e.g., raising a hand). The yellow box shows a change in space, i.e., giving fluidity to an object by creating an abstract modification.

and obtain consistent Core-Gaussians, we train GS on a batch containing a few views instead of one. In practice, we render a handful of views from different positions and use back-propagation.

In particular, we parameterize Gaussians \((,R,S)\) by face \(V=(_{1},_{2},_{3})\) to obtain \((_{1},_{2},_{3})\). Our _Deform Network_ takes as in input the triangle vertices \(V\) assigned to the parameterized 3D Core-Gaussians and the current time \(t\) and returns updated \((V,t)=(_{1}(t),_{2}(t),_{3 }(t), R_{V}(t))\). Such updates consist of translation and rotation (Fig. 8):

\[V(t)=V(V,t)=(_{1}+_{1}(t),_{2}+ _{2}(t),_{3}+_{3}(t)) R_{ V}(t).\]

D-MiSo parameterized Core-Gaussians in time \(t\) by: \(_{core}(V(t),,c)=_{core}(V(V,t),,c)\) where \(\) is a deformable network that moves triangle \(V\) according to time \(t\). The opacity and color of the Core-Gaussian are used only in the first stage.

Stage 2: PreparationTo move to the second phase of the model, it is imperative to prepare the Multi-Gaussians. This involves attaching \(k\) Sub-Gaussians to each Core-Gaussian generated in Stage 1, as shown in Fig. 6. Henceforth, Sub-Gaussians assume responsibility for the resultant rendering. Initially, the Sub-Gaussian adopts the same features as the Core-Gaussian, except for the position.

Stage 2The primary objective of the second phase is to parallelized Core-Gaussians (Core-Triangles Soup) to enhance understanding of movement and increase rendering quality through the precise training of Sub-Gaussians.

Since the centers of Sub-Gaussians are parameterized by the local coordinate system given by the rotation matrix of Core-Gaussian, when the _Deform Network_\(\) changes the Core-Gaussian, all Sub-Gaussians (attached to this Core-Gaussian) are modified by global transformation \((V,t)\).

D-MiSo use an additional deformation network _Sub-Rot Network_\(\) dedicated to each \(i\)-th Sub-Gaussian's small changes. _Sub-Rot Network_ takes the Sub-Gaussian rotation matrix \(R^{i}_{V}\) and the current time \(t\) as input and produces an updated rotation matrix \( R^{i}_{V}(t)\).

The position \(}^{i}_{V}(t)=}_{V(t)}+_{V(t)}^{T}\) of the Sub-Gaussian in time \(t\) is determined by the position \(}_{V(t)}\), and rotation \(_{V(t)}\) of the Core-Gaussian (parameterized by triangle \(V\)) and the learning parameter \(^{}\). It should be noted that scale \(S^{i}\), color \(c_{i}\), and opacity \(_{i}\) of Sub-Gaussian are trainable and do not depend on time. _Sub-Rot Network_ produce updated \((R^{i},t)= R^{i}(t)\) for rotation parameter of Sub-Gaussians. Finally parameters of Sub-Gaussians in time \(t\) depends on _Deform Network_\(\), and _Sub-Rot Network_\(\), and the Corr-Gaussians parameter \(V\), and Sub-Gaussian parameters \(R^{i}\), \(S^{i}\), \(c_{i}\) and \(_{i}\):

\[_{Sub}(t)=\{(_{sub}(}_{V(V,t)}+ _{V(V,t)}^{T},R^{i}+(R^{ i},t),S^{i}),c_{i},_{i})\}_{i=1}^{k}.\]

D-MiSo final model consists of two deformable networks and two levels of Gaussian distributions. This approach enables efficient modeling of object motion over time in dynamic scenes, allowing for adjustments to objects at each time frame.

The result of the model's inference depends on the camera's view angle as well as on the selected time. Only Sub-Gaussian features are used in the rendering process. Hence, the implementation of generating an output image from Gaussians is no different from a vanilla GS.

Figure 12: Limbs render and modification obtained with D-MiSo. It is worth noting that it is also possible to close the hand.

Figure 13: Examples of object modifications. The first method allows for a smooth modification (bending) and also removes (e.g. plate), scales and/or adds (e.g. small blue balls) objects.

[MISSING_PAGE_FAIL:9]

to maintain the consistency bestowed by the mesh. The whole process is shown in Fig. 5. Thanks to such representation, we can edit our connected mesh to produce the correct edition of the dynamic scene (Fig. 2).

The second editing method allows us to define the connections, i.e., editing Sub-Triangles Soup directly, e.g. moving a hand or bending horns (Fig. 10) or rotating a human body (Fig. 3). Changes are possible because we can also transform a group of Sub-Triangles instead of individual ones. This method allows for even very subtle changes like raising the thumb (Fig. 10), turning the hand over (Fig 4), opening or closing the hand (Fig. 12). These edits would be difficult in other approaches based on adjusting the 3D objects' centroids (nodes) since the space of nodes is limited in details area (Fig. 4). Nodes' use is preeminent for defining movement, which was insignificant in these places.

With this method, we can also change complicated objects like a \(360^{}\) scene without losing the dynamics-related model. For example, the rotation of a person stacking boxes (Fig. 3).

Our methods are also scalable, so we can easily remove or duplicate elements from an image. Moreover, the duplicated elements can be given their own dynamics. Examples of these effects are shown in Fig. 13, where we removed the plate and both duplicated and rescaled the blue balls multiple times.

## 5 Conclusion

D-MiSo is a novel method based on Gaussian Splatting parameterization, which produces a cloud of triangles called Triangle Soup. The method allows easy editing of objects created in inference with the possible transformations, including moving, scaling, and rotating. By defining Multi-Gaussians, the obligatory separability of modified elements seen in other models is combated. In addition, certain elements of objects can be duplicated and removed (Fig. 13). Furthermore, the D-MiSo method can facilitate giving different dynamics to separate parts of an object (Fig. 1).

**Limitation** The method allows for complex changes at a given moment in time. However, if some area is not well represented in the training set, it is impossible to edit them. For example, a person's hand can be changed but not fingers (Fig. 4). This is due to the liminality of Triangle Soup relative to a well-fitted mesh.

**Broader impact** Our model significantly improves rendering quality and advances 3D scene reconstruction and rendering, impacting multiple domains by enabling more realistic and efficient 3D modeling and animation. This technology could enhance VR/AR experiences , robotics , and medical imaging . It could also be used for interactive education , scientific visualization, and a plethora of other commercial applications like product design and real estate .