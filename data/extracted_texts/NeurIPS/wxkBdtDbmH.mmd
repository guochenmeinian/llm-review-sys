# Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation

Ruida Zhou

Texas A&M University

ruida@tamu.edu

&Tao Liu

Texas A&M University

tliu@tamu.edu

&Min Cheng

Texas A&M University

minrara0404@tamu.edu

&Dileep Kalathil

Texas A&M University

dileep.kalathil@tamu.edu

&P. R. Kumar

Texas A&M University

prk@tamu.edu

&Chao Tian

Texas A&M University

chao.tian@tamu.edu

The first two authors contributed equally.

###### Abstract

We study robust reinforcement learning (RL) with the goal of determining a well-performing policy that is robust against model mismatch between the training simulator and the testing environment. Previous policy-based robust RL algorithms mainly focus on the tabular setting under uncertainty sets that facilitate robust policy evaluation, but are no longer tractable when the number of states scales up. To this end, we propose two novel uncertainty set formulations, one based on double sampling and the other on an integral probability metric. Both make large-scale robust RL tractable even when one only has access to a simulator. We propose a robust natural actor-critic (RNAC) approach that incorporates the new uncertainty sets and employs function approximation. We provide finite-time convergence guarantees for the proposed RNAC algorithm to the optimal robust policy within the function approximation error. Finally, we demonstrate the robust performance of the policy learned by our proposed RNAC approach in multiple MuJoCo environments and a real-world TurtleBot navigation task.

## 1 Introduction

Training a reinforcement learning (RL) algorithm directly on a real-world system is expensive and potentially risky due to the large number of data samples required to learn a satisfactory policy. To overcome this issue, RL algorithms are typically trained on a simulator. However, in most real-world applications, the nominal model used in the simulator model may not faithfully represent the real-world system due to various factors, such as approximation errors in modeling or variations in real-world parameters over time. For example, the mass, friction, sensor/actuator noise, and floor terrain in a mobile robot simulator may differ from those in the real world. This mismatch, called simulation-to-reality-gap, can significantly degrade the performance of standard RL algorithms when deployed on real-world systems [36; 44; 55; 51]. The framework of robust Markov decision process (RMDP) [19; 40] is used to model this setting where the testing environment is uncertain and comes from an uncertainty set around the nominal model. The optimal robust policy is defined as the one which achieves the optimal worst-case performance over all possible models in the uncertainty set. The goal of robust reinforcement learning is to learn such an optimal robust policy using only the data sampled from the simulator (nominal) model.

The RMDP planning problem has been well studied in the tabular setting [63; 62; 35; 46; 16]. Also, many works have developed model-based robust RL algorithms in the tabular setting [66; 69; 42;64, 49], focusing on sample complexity. Many robust Q-learning algorithms  and policy gradient methods for robust RL  have been developed for the tabular setting. Different from all these works, the main goal of this paper is to develop a computationally tractable robust RL algorithm with provable convergence guarantees for RMDPs with large state spaces, using linear and nonlinear function approximation for robust value and policy.

One of the main challenges of robust RL with a large state space is the design of an effective uncertainty set that is amenable to computationally tractable learning with function approximation. Robust RL algorithms, both in their implementation and technical analysis, require robust Bellman operator evaluations  which involve an inner optimization problem over the uncertainty set. Performing this inner optimization problem and/or getting an unbiased estimate of the robust Bellman operator evaluation using only the samples from the nominal model can be intractable for commonly considered uncertainty sets when the state space is large. For example, for \(f\)-divergence-based uncertainty sets , the robust Bellman operator estimate requires solving for a dual variable associated with each state, which is prohibitive for large state spaces. For \(R\)-contamination , the estimate requires calculating the minimum of the value function over state space, which is impractical for large state spaces. It is also infeasible for \(_{p}\) norm-based uncertainty sets , where the estimates require calculating the median, mean, or average peak of the value function depending on the choice of \(p\). Robust RL with function approximation has been explored in a few works . However, these works explicitly or implicitly assume an oracle that approximately computes the robust Bellman operator estimate, and the uncertainty set design that facilitates computation and learning is largely ignored. We overcome this challenge by introducing two novel uncertainty set formulations, one based on double sampling (DS) and the other on an integral probability metric (IPM). Both are compatible with large-scale robust MDP, with the robust Bellman operators being amenable to unbiased estimation from the data sampled from the nominal model, enabling effective practical robust RL algorithms.

Policy-based RL algorithms , which optimize the policy directly, have been extremely successful in learning policies for continuous control tasks. Value-based RL approaches, such as Q-learning, cannot be directly applied to continuous action problems. Moreover, recent advances in policy-based approaches can establish finite time convergence guarantees and also offer insights into the practical implementation . However, most of the existing works on robust RL with function approximation use value-based approaches  that are not scalable to continuous control problems. On the other hand, the existing works that use policy-based methods for robust RL with convergence guarantees are limited to the tabular setting . We close this important gap in the literature by developing a novel robust natural actor-critic (RNAAC) approach that leverages our newly designed uncertainty sets for scalable learning.

**Summary of Contributions:**\((i)\). We propose two novel uncertainty sets, one using double sampling (Section 3.1) and the other an integral probability metric (Section 3.2), which are both compatible for robust RL with large state space and function approximation. Though robust Bellman operators require the unknown worst-case models, we provide _unbiased_ empirical robust Bellman operators that are computationally easy to utilize and only based on samples from the nominal model;

\((ii)\). We propose a novel RNAAC algorithm (Section 4) which to the best of our knowledge is the first policy-based approach for robust RL under function approximation, with provable convergence guarantees. We consider both linear and general function approximation for theoretical study, with the latter relegated to Appendix E due to space constraints. Under linear function approximation, the RNAAC with a robust critic performing "robust linear-TD" (Section 5) and a robust natural actor performing "robust Q-NPG" (Section 6) is proved to converge to the optimal robust policy within the function approximation error. Specifically, \((1/^{2})\) sample complexity can be achieved, or \((1/^{3})\) for policy update with a constant step size. For the robust linear-TD, we study the contraction behavior of the projected robust Bellman operator for RMDPs with the proposed uncertainty sets and the well-known \(f\)-divergence uncertainty sets, which we believe is of independent interest;

\((iii)\). We implement the proposed RNAAC in multiple MuJoCo environments (Hopper-v3, Walker2d-v3, and HalfCheetah-v3), and demonstrate that RNAAC with the proposed uncertainty sets results in robust behavior while canonical policy-based approaches suffer significant performance degradation. We also test RNAAC on TurtleBot , a real-world mobile robot, performing a navigation task. We show that the TurtleBot with RNAAC successfully reaches its destination while canonical non robust approaches fail under adversarial perturbations. A video of the demonstration on TurtleBot is available at **[Video Link]** and the RNAC code is provided in the supplementary material.

Due to the page limit, a detailed literature review and comparison of our work with the existing robust RL algorithms are deferred to Appendix G.

## 2 Preliminaries

**Notations:** For any set \(\), denote by \(||\) its cardinality, by \(_{}\) a \((||-1)\)-dimensional probability simplex, and by \(()\) a uniform distribution over \(\). Let \([m]:=\{1,,m\}\).

A Markov decision process (MDP) is represented by a tuple \((,,,r,)\), where \(\) is the state space, \(\) is the action space, \(=(_{0},_{1},)\) is a possibly non-stationary transition kernel sequence with \(_{t}:_{}\), \(r:\) is the reward function, and \((0,1)\) is the discount factor. For a stationary policy \(:_{}\), its value function is \(V_{}^{}(s):=_{,}[_{t=0}^{}^{t} r(s_{t},a_{t})|s_{0}=s],\) where the expectation is taken w.r.t. the trajectory \((s_{0},a_{0},s_{1},a_{1},)\) with \(a_{t}|s_{t}_{s_{t}}\) and \(s_{t+1}|(s_{t},a_{t})_{t,s_{t},a_{t}}\). We can similarly define the state situation distribution under initial distribution \(_{}\), \(d_{}^{,}(s):=(1-)_{,}[_{t=0}^{ }^{t}(s_{t}=s)|s_{0}]\); the state-action value function (Q function), \(Q_{}^{}(s,a):=_{,}[_{t=0}^{}^{t} r(s_{t},a_{t})|s_{0}=s,a_{0}=a],\) and the advantage function \(A_{}^{}(s,a):=Q_{}^{}(s,a)-V_{}^{}(s)\).

**Robust MDP:** A RMDP is represented by a tuple \((,,,r,)\), where \(\) is a set of transition kernels known as the _uncertainty set_ that captures the perturbations around the nominal stationary kernel \(p^{}:_{}\), and its robust value function is defined as the corresponding worst case:

\[V_{}^{}(s):=_{_{t 0}}V_{ }^{}(s).\] (1)

We will assume the following key \((s,a)\)-rectangularity condition that is commonly assumed to facilitate dynamic programming ever since the introduction of RMDPs [19; 40]:

**Definition 1**.: \(\) _is \((s,a)\)-rectangular, if \(=_{s,a}_{s,a}\), for some \(_{s,a}_{}\)._

The corresponding robust Bellman operator \(_{}^{}:^{}^{ }\) is

\[(_{}^{}V)(s)=_{a(|s)}r(s,a)+_{p_{s,a}}p^{}V, s ,\ V^{},\] (2)

and the Bellman equation for RMDPs is \(V=_{}^{}V\), where \(V=V^{}\) is its unique solution from the Banach fixed-point theorem. Typically, \(_{s,a}\) is taken as a ball \(\{_{}:(,p_{s,a}^{})\}\) around a nominal model \(p^{}\) of the training environment, where \((,)\) is some divergence measure between probability distributions, and \(>0\) controls the level of robustness.

There is a stationary optimal policy \(^{*}\) that uniformly maximizes the robust value function, i.e., \(V_{}^{^{*}}(s)=\{V_{}^{}(s)\}, s\)[19; 40]. Thus, without loss of generality, we only need to optimize within stationary policies. Moreover, for any stationary policy \(\) there exists a stationary worst-case kernel \(_{}\) with \(V_{_{}}^{}(s)=V_{}^{}(s), s\). We can define the robust Q-function and robust advantage function as \(Q_{}^{}(s,a):=Q_{_{}}^{}(s,a)\) and \(A_{}^{}(s,a):=A_{_{}}^{}(s,a)\), respectively. When the uncertainty set is clear from the context, we will omit the subscript \(\) in \(V_{}^{},Q_{}^{}\), and \(A_{}^{}\).

To summarize, the motivation in the robust RL framework is to learn the optimal robust policy by only training on a simulator implementing an (unknown) nominal model \(p^{}\)[53; 45; 41], with the robust RL algorithms only having access to data generated from \(p^{}\), and not from any other model in the uncertainty set \(\).

## 3 Uncertainty Sets for Large State Spaces

Evaluating the robust Bellman operator \(_{}^{}\) requires solving the optimization \(_{p_{s,a}}p^{}V\), which is challenging for arbitrary uncertainty sets. Moreover, since we only have access to data generated by the nominal model \(p^{}\), estimating \(_{}^{}\) is not straightforward. Previously studied uncertainty sets, such as \(R\)-contamination, \(f\)-divergence, and \(_{p}\) norm, are intractable for RMDPs with large state spaces for these reasons (see Appendix B for detailed explanation). We therefore design two uncertainty sets for RMDPs where the robust Bellman operator has a tractable form and can be unbiasedly estimated by the data sampled from the nominal model, thus making effective learning possible. We will also use function approximation to tractably parameterize policy and value functions.

### Double-Sampling (DS) Uncertainty Set

The central difficulty in employing the robust Bellman operator (2) is how to evaluate \(_{p_{s,a}}p^{}V\). Our first method is based on the following key idea for drawing samples that produce an unbiased estimate for it. Let \(\{s^{}_{1},s^{}_{2},,s^{}_{m}\}\) be \(m\) samples drawn i.i.d. according to the nominal model \(p^{}_{s,a}\). Then, for any given divergence measure \((,)\) and radius \(>0\), there exists an uncertainty set \(=_{s,a}_{s,a}\) such that \(_{p_{s,a}}p^{}V=_{s^{}_{1:m} i: ^{}_{p^{}_{s,a}}}[_{_{[m]}: (,([m]))}_{i=1}^{m}_{i}V(s^{ }_{i})]\). (See Appendix B.1 for a brief explanation.) We therefore define an empirical robust Bellman operator \(}^{}_{}\) corresponding to (2) by

\[(}^{}_{}V)(s,a,s^{}_{1:m}):=r(s,a)+ _{_{[m]}:(,([m]))} _{i=1}^{m}_{i}V(s^{}_{i}).\] (3)

Since \((^{}_{}V)(s)=_{(|s),\ s^{}_{1:m} i: ^{}_{p^{}_{s,a}}}[(}^{}_{ }V)(s,a,s^{}_{1:m})]\), \(}^{}_{}\) gives an unbiased estimate, which is one key property we use in our robust RL algorithm. We refer to this as "double sampling" since \(_{i=1}^{m}_{i}V(s^{}_{i})\) is the expected value when one further chooses one sample \(s^{}\) from \(\{s^{}_{1},s^{}_{2},,s^{}_{m}\}\) according to the distribution \(\) on \([m]\) and evaluates \(V(s^{})\). Above, \(\) is a perturbation of \(([m])\) and when \(=0\), \(=([m])\) and \(s^{} p^{}_{s,a}\). The uncertainty set \(\) corresponding to the double sampling is implicitly defined by specifying the choices of \(m\), \((,)\), and \(\). Its key advantage is that samples can only be drawn according to the nominal model \(p^{}\).

Double sampling requires sampling multiple next states for a given state-action pair, which can be implemented if the simulator is allowed to set to any state. (All MuJoCo environments  support DS.) Since the calculation of \(\) is within \(_{[m]}\), the empirical robust Bellman operator is tractable for moderate values of \(m\). We use \(m=2\) in experiments for training efficiency, where for almost all divergences \((,)\) we can explicitly write

\[(}^{}_{}V)(s,a,s^{}_{1:2})=r(s,a)+ (0.5(V(s^{}_{1})+V(s^{}_{2}))-|V(s^{}_{1})-V(s^{ }_{2})|).\] (4)

Bellman completeness (value function class closed under the Bellman operator) is a key property for efficient reinforcement learning, whereas RMDP with canonical uncertainty sets may violate it. We show in Appendix B.1 that for RMDP with DS uncertainty sets, the linear function approximation class satisfies Bellman completeness if the nominal model is a linear MDP .

### Integral Probability Metric (IPM) Uncertainty Set

Given some function class \(^{S}\) including the zero function, the integral probability metric (IPM) is defined by \(_{}(p,q):=_{f}\{p^{}f-q^{}f\} 0\). Many metrics such as Kantorovich metric, total variation, etc., are special cases of IPM under different function classes .

The robust Bellman operator \(^{}_{}V\) (2) requires solving the optimization \(_{q_{s,a}}q^{}V\). For an IPM-based uncertainty set \(\) with \(_{s,a}=\{q:_{}(q,p^{}_{s,a})\}\), we relax the domain \(q_{}\) to \(_{s}q(s)=1\) as done in [28; 27], which incurs no relaxation error for small \(\) if \(_{s^{}}p^{}_{s,a}(s^{})>0\).

One should choose \(\) so that it properly encodes information of the MDP and its value functions. We start by considering linear function approximation. Denote by \(^{S d}\) the feature matrix with rows \(^{}(s)\), \( s\). The value function approximation is \(V_{w}= w^{S}\). A good choice of feature vectors encodes information about the state and transition. For example, vectors \((s),(s^{})\) should be "close" when states \(s,s^{}\) are "similar" and \(V^{}(s),V^{}(s^{})\) have small differences. We propose the following function class (with \(_{2}\) as the preferred norm but any other can be used):

\[:=\{s(s)^{}:^{d},\|\| 1\}.\] (5)

Without loss of generality, assume \(\) has full column rank since \(d||\), and let the first coordinate of \((s)\) be 1 for any \(s\), which corresponds to the bias term of the linear regressor.

**Proposition 1**.: _For the IPM with \(\) in (5), we have \(_{q_{s,a}}q^{}V_{w}=(p^{}_{s,a})^{}V_{w}-\| w_{2:d}\|\)._We can thus design the empirical robust Bellman operator, which is an unbiased estimator of \(_{}^{}\) with sample \(s^{}\) drawn from the nominal model:

\[(}_{}^{}V_{w})(s,a,s^{}):=r(s,a)+ V_{w }(s^{})-\|w_{2:d}\|.\] (6)

Guided by the last regularization term of the empirical robust Bellman operator (6), when considering value function approximation by neural networks we add a similar negative regularization term for all the neural network parameters except for the bias parameter in the last layer.

## 4 Robust Natural Actor-Critic

We propose a robust natural actor-critic (RNAC) approach in Algorithm 1 for the robust RL problem. As its name suggests, there are two components - a robust critic and a robust actor, which update alternately for \(T\) steps. At each step \(t\), there is a policy \(^{t}\) determined by parameter \(^{t}\). The robust critic updates the value function approximation parameter \(w^{t}\) based on on-policy trajectory data sampled by executing \(^{t}\) on the nominal model with length \(K\). The robust actor then updates the policy with step size \(^{t}\), and the critic returns \(w^{t}\) by on-policy trajectory data with length \(N\). In practice, a batch of on-policy data can be sampled and used for both critic and actor updates.

We now give the main (informal) convergence results for our RNAC algorithm with linear function approximation and DS or IPM uncertainty sets, where the robust critic performs robust linear-TD and the robust natural actor performs robust-QNPG (as the comments in Algorithm 1). The formal statements, proofs, and generalization to general function approximation are given in Appendix E.

**Theorem 1** (Informal linear convergence of RNAC).: _Under linear function approximation, RNAC in Algorithm 1 with DS or IPM uncertainty sets using an RLTD robust critic update and an RQNPG robust natural actor update, with appropriate geometrically increasing step sizes \(^{t}\), achieves \([V^{^{*}}()-V^{^{T}}()]=O(e^{-T})+O(_{stat})+O( _{bias})\) and an \((1/^{2})\) sample complexity._

The optimality gap is bounded in this theorem via three terms, where the first term, related to the number of time steps \(T\), is the optimization rate (linear convergence since \(O(e^{-T})\)), the second term \(_{stat}=(}+})\) is a statistical error that depends on the number of samples \(K,N\) in the robust critic and robust actor updates, and the last term \(_{bias}\) is the approximation error due to the limited representation power of value function approximation and the parameterized policy class. Omitting the approximation error \(_{bias}\), the sample complexities for achieving \(\) robust optimal value are \((1/^{2})\), which achieves the optimal sample complexity in tabular setting . However, RNAC with geometrically increasing step sizes induces a larger multiplicative constant factor (not shown in big-\(O\) notation) and does not generalize well to general function approximation. We then analyze RNAC with a constant step size.

**Theorem 2** (Informal sublinear convergence of RNAC).: _RNAC under the same specification as in Theorem 1 but with constant step size \(^{t}=\) has \([V^{^{*}}()-_{t=0}^{T-1}V^{^{t}}()]=O( )+O(_{stat})+O(_{bias})\), implying an \((1/^{3})\) sample complexity._

Although the theorem shows a slower optimization rate of RNAC with constant step size, this non-increasing step size is preferred in practice. Moreover, the analysis can be generalized to a general policy class with optimization rate \(O(1/)\), and an \((1/^{4})\) sample complexity.

## 5 Robust Critic

The robust critic estimates the robust value function with access to samples from the nominal model. One may note that in many previous actor-critic analyses for canonical RL , the critic learns the \(Q\) function, while realistic implementations in on-policy algorithms (e.g., proximal policy optimization (PPO)) treat the \(V\) function as the target of the critic for training efficiency. We consider a robust critic that learns the robust \(V\) function to align with such realistic implementations.

We present the **robust linear temporal difference (RLTD)** Algorithm 2, which is similar to the canonical linear-TD algorithm, but with an empirical robust Bellman operator. It iteratively performs the sampling and updating procedures. The sampling procedure differs for the uncertainty set by double sampling and IPM as shown in the comments in Algorithm 2. Using the samples, the parameter for the linear value function approximation \(V_{w_{k}}= w_{k}\) is updated with step size \(_{k}\). The following assumption is common :

**Assumption 1** (Geometric mixing).: _For any policy \(\), the Markov chain \(\{s_{k}\}\) induced by applying \(\) in the nominal model \(p^{}\) is geometrically ergodic with a unique stationary distribution \(^{}\)._

The update procedure essentially minimizes the Mean Square Projected Robust Bellman Error MSPRBE\({}^{}(w)=\|^{}(^{}_{}V_{w})-V_{w}\|_{^{}}^{2}\), where \(\|V\|_{^{}}=^{}(s)V(s)^{2}}\) is the \(^{}\) weighted norm, and \(^{}=(^{}D^{})^{-1}^{}D^{}\) is the weighted projection matrix with \(D^{}=(^{})^{}}\). The minimizer of MSPRBE\({}^{}(w)\), denoted by \(w^{}\), is the unique solution of the projected robust Bellman equation \(V_{w}=^{}^{}_{}V_{w}\), which is equivalent to \(0=^{}D^{}(^{}_{} w- w)\). RLTD is thus a stochastic approximation algorithm since the empirical operator \(}^{}_{}\) ((3) or (6)) is unbiased with \(_{(s,a,y^{})^{} p^{}}[(s )[(}^{}_{}V_{w})(s,a,y^{})-(s)^ {}w]]=^{}D^{}(^{}_{}V_{w} -V_{w})\).

To ensure that RLTD converges to the optimal linear approximation \(V_{w^{}}\), it is crucial that the projected robust Bellman operator \(^{}^{}_{}\) be a contraction map with some \(<1\).

**Definition 2**.: \(^{}^{}_{}\) _is a \(\)-contraction w.r.t. \(\|\|_{^{}}\) if \(\|^{}^{}_{}V-^{}^{}_{ }V^{}\|_{^{}}\|V-V^{}\|_{^{}}\)._

Unlike in linear TD for MDP , Tamar et al.  make an additional assumption (Assumption 2) to establish the contraction property of \(^{}^{}_{}\) (Proposition 3) for RMDP.

**Assumption 2**.: _There exists \((0,1)\) with \( p_{s,a}(s^{}) p^{}_{s,a}(s^{})\; s,s^{ }\), \( a\), and \( p\)._

**Proposition 2** (Prop.3 in ).: _Under Assumption 2, \(^{}^{}_{}\) is a \(\)-contraction w.r.t. \(\|\|_{v^{}}\)._

The implicit uncertainty set \(\) of double-sampling satisfies Assumption 2 for small \(\), and thus guarantees contraction of \(^{}^{}_{}\). For example, for \(m=2\) as in (4), a \(<\) is sufficient. However, simply taking a small radius \(\) is not a panacea for all uncertainty sets:

**Proposition 3**.: _For any \(f\)-divergence and radius \(>0\), there exists a geometrically mixing nominal model such that the \(f\)-divergence defined uncertainty set violates Assumption 2._

On the other hand, Assumption 2, though well-accepted , may not be necessary. The proposed IPM uncertainty set relates robustness and regularization with an explicit formula for robust Bellman operator as in (6). The contraction behavior of \(^{}^{}_{}\) for IPM uncertainty set can be established without Assumption 2:

**Lemma 1**.: _For IPM uncertainty set with radius \(<_{}(^{}D^{})\), there exists \(<1\) that \(^{}_{}^{}\) is a \(\)-contraction mapping w.r.t. norm \(\|\|_{v^{}}\)._

Since the contraction of \(^{}_{}^{}\) is obtained by RMDP under DS or IPM uncertainty sets with small radius \(\), we have the first finite sample guarantee of RLTD by recent advances in Markovian stochastic approximation :

**Theorem 3** (Informal convergence of robust critic: Details in Appendix C).: _RLTD with step sizes \(_{k}=(1/k)\) satisfies \([\|w_{K}-w^{}\|^{2}]=()\)._

## 6 Robust Natural Actor

The robust natural actor updates the policy parameter \(\) along an ascent direction that improves the value via preconditioning through the KL-divergence \((p,q):= p,(p/q)\). It has been well explored for natural policy gradient (NPG)-like algorithms, such as TRPO and PPO in canonical RL. The ascent direction is obtained by the policy gradient theorem in canonical MDP. We therefore first discuss the policy gradient for RMDP, where policy \(_{}\) is differentiably parameterized by \(\).

The robust value \(V_{}^{_{}}()=:J()\) is typically Lipschitz under proper parameterization , and is therefore differentiable a.e. by Rademacher's theorem . Where it is not differentiable, a Frechet supergradient \( J\) of \(J\) exists if \(_{^{} 0})-J()-  J(),^{}-}{\|^{}- \|} 0\). The following contains the policy gradient theorem for canonical RL as a special case:

**Lemma 2** (Policy supergradient).: _For a policy \(=_{}\) that is differentiable w.r.t. parameter \(\),_

\[_{}V^{}()=_{s d_{}^{,s}} _{a_{s}}[Q^{}(s,a)_{}(a|s)]}{1-} =_{s d_{}^{,s}}_{a_{s}}[A^{} (s,a)_{}(a|s)]}{1-}\]

_is a Frechet supergradient of \(V^{}()\), where \(_{}\) is the worst-case transition kernel w.r.t. \(\)._

We consider log-linear policies \(_{}(a|s)=)}{_{a^{ }}((s,a^{})^{})}\), where \((s,a)^{d}\) is the feature vector and \(^{d}\) is the policy parameter. (The general policy class is treated in Appendix D).

In canonical RL, the training and testing environments follow the same nominal transition \(p^{}\). NPG updates the policy by \(+_{}()^{}_{} V_{}^{_{}}()\), where \(_{}()^{}\) is the Moore-Penrose inverse of the Fisher information matrix \(_{}():=_{(s,a) d_{} ^{_{},p^{}}_{}}[_{} _{}(a|s)(_{}_{}(a|s))^{}]\). An "equivalent" Q-NPG was proposed in  to update the policy by \(+ u^{}\), where \(u^{}=_{u}_{(s,a) d_{}^{,p^{}} }[(Q_{}^{}(s,a)-u^{}(s,a))^{2}]\). Note that \(u\) determines a Q value function approximation \(Q^{u}(s,a):=(s,a)^{}u\), which is compatible with the log-linear policy class . Since \(Q^{}\) contains the information on the ascent direction as suggested by Lemma 2, the Q-NPG update can be viewed as inserting the best compatible Q-approximation \(Q^{u^{}}\) for the policy.

We adopt the Q-NPG to robust RL and propose the **Robust Q-Natural Policy Gradient (RQNPG)** (Algorithm 3). Note that unlike canonical RL, where \(Q_{p^{}}^{}\) can be estimated directly from a sample trajectory of executing \(\) in model \(p^{}\), the robust \(Q^{}\) is hard to estimate with samples from \(p^{}\). A value function approximation \(V_{w}\) from the critic comes to help, as we can approximate the robust Q function via \(Q_{w}(s,a)=r(s,a)+_{p_{s,a}}p^{}V_{w}\), which exactly matches \(Q^{}\) if \(V_{w}=V^{}\). The RQNPG obtains information on \(Q^{}\) by first approximating it via a critic value \(V_{w}\)-guided function \(Q_{w}\), and then estimating \(Q_{w}\) by a policy-compatible robust Q-approximation \(Q^{u}\).

As shown in Algorithm 3, RQNPG estimates a compatible Q-approximation \(Q^{u_{N}}\) by iteratively performing sampling and updating procedures, where the sampling procedure follows that of the RLTD (Algorithm 2). The update procedure essentially approaches the minimizer \(u_{w}^{}:=_{u}_{(s,a)^{}}[(Q_{w}(s,a)-u^{}(s,a))^{2}]\) by stochastic approximation with step size \(_{n}\) (stochastic gradient descent with Markovian data), since conditioned on \((s,a)\), \(}_{}^{}V_{w}\) ((3) or (6)) is an unbiased estimator for the Q function approximation \(Q_{w}\).

Now we look at a specific update \(^{t+1}=(^{t},^{t},w^{t},N)\) with \(_{n}=(1/n)\), where \(w^{t}=(_{^{t}},K)\). The following theorem shows an approximate policy improvement property of the RQNPG update:

**Theorem 4** (Approximate policy improvement).: _For any \(t 0\), we know_

\[V^{^{t+1}}() V^{^{t}}()+_{d_{}^{^{t+1}, _{t}+1}}(^{t},^{t+1})+_{d_{}^{^{t+1},_{t}+1}}(^ {t+1},^{t})}{(1-)^{t}}-}{1-},\] (7)

_where \(_{}(,^{}):=_{s}(s)((|s), ^{}(|s)) 0\) and \([_{t}]=(}+})+O( _{bias})\)._

## 7 Experimental Results

We demonstrate the robustness of our RNAC approach (Algorithm 1) with Double-Sampling (DS) and IPM uncertainty sets on MuJoCo simulation environments . We also perform real-world evaluations using TurtleBot , a mobile robot, on navigation tasks under action/policy perturbation. We implement a practical version RNAC using neural network function approximation, with the robust critic minimizing squared robust TD-error and the robust natural actor performing a robust proximal policy optimization (PPO) (see Algorithm 4 in Appendix A for details). We call this RNAC algorithm as RNAC-PPO and compare it with the canonical PPO algorithm . Additional experimental results and details are deferred to Appendix A. We provide code with detailed instructions at https://github.com/tliu1997/RNAC.

### MuJoCo Environments

We present the experimental results for perturbed MuJoCo Envs (Hopper-v3, Walker2d-v3 and HalfCheetah-v3) by changing their physical parameters (_leg_joint_stiffness_, _foot_joint_stiffness_ and

Figure 1: Cumulative rewards of RNAC-PPO(DS/IPM) and PPO on (a-c) stochastic MuJoCo Envs and (d-f) deterministic MuJoCo Envs under perturbation.

_back_actuator_range_). We compare the performance of RNAAC-PPO with that of the canonical PPO algorithm in Fig. 1, where the curves are averaged over 30 different seeded runs and the shaded region indicates the mean \(\) standard deviation. RNAAC-PPO and PPO are trained with data sampled from the nominal models (e.g., _leg_joint_stiffness\(=0.0\), _foot_joint_stiffness\(=0.0\)_, and _back_actuator_range\(=1.0\)_).

_DS Uncertainty Set:_ MuJoCo environments have deterministic models. However, most uncertainty sets are only reasonable for stochastic models, e.g., \(f\)-divergence uncertainty sets. So, we add a uniform actuation noise \(\) Unif-[-5e-3, 5e-3] in constructing stochastic MuJoCo environments as in . We use \(m=2\) and radius \(=1/6\) for RNAAC-PPO in (4). The choice of \(m=2\) makes the training time of RNAAC-PPO with DS uncertainty set and PPO comparable. Fig. 0(a)-0(c) demonstrate the robust performance of RNAAC-PPO with Double-Sampling (DS) uncertainty sets in stochastic MuJoCo environments. Compared to PPO, the cumulative rewards of RNAAC-PPO decay much slower as perturbations increase though they are slightly lower at the beginning (i.e., for the nominal model). The slight drop in initial performance may stem from optimizing the robust value function (1) under the worst-case transition models instead of the nominal models.

_IPM Uncertainty Set:_ Since IPM with robust Bellman operator in (6) establishes robustness by negative regularization, it applies to environments with deterministic transition kernels. We select \(=10^{-5}\) in (6) and evaluate RNAAC-PPO with IPM uncertainty sets on deterministic MuJoCo environments in Fig. 0(d)-0(f). RNAAC-PPO has more robust behaviors with slow cumulative reward decay as perturbations increase. Notably, RNAAC-PPO enjoys similar and sometimes even better initial performance on the nominal model compared to PPO, which we believe is due to the regularization of neural network parameters suggested by IPM (6) that can potentially improve neural network training.

### TurtleBot Experiments

We demonstrate the robustness of the policy learned by RNAAC-PPO on a real-world mobile robot (Fig. 3). We consider a navigation task as illustrated in Fig. 2, where the goal of the policy is to navigate the TurtleBot from the origin \((0,0)\) to a target region centered at \((1,-1)\).

_DS Uncertainty Set:_ We train RNAAC-PPO and PPO on a stochastic nominal model with balanced actuation noise , and test the learned policies in the nominal model (Fig. 1(a)) and an unbalanced perturbed model (Fig. 1(b)). The policies learned by RNAAC-PPO can reach the target region in both the nominal model and the perturbed model, while policies learned by PPO are fragile to perturbation and may not reach the target, as shown in Fig. 1(b).

_IPM Uncertainty Set:_ The robustness of the policies learned by RNAAC-PPO trained on a deterministic nominal model is demonstrated in Fig. 1(c) and 1(d), where the RNAAC-PPO learned policies drive the robot to the target under perturbation, while the PPO-learned policies fail.

**A video of this real-world demonstration TurtleBot is available at [Video Link].**

Figure 3: TurtleBot Burger

Figure 2: (a-b) show trajectories under balanced (nominal) / unbalanced noise perturbed envs; (c-d) show trajectories under deterministic (nominal) / unbalanced noise perturbed envs.

Conclusion and Future Works

We have proposed two novel uncertainty sets based on double sampling and an integral probability metric, respectively, that are compatible with function approximation for large-scale robust RL. We propose a robust natural actor-critic algorithm, which to the best of our knowledge is the first policy-based approach for robust RL under function approximation with provable guarantees on learning the optimal robust policy. We demonstrate the robust performance of the proposed algorithm in multiple perturbed MuJoCo environments and a real-world TurtleBot navigation task.

Although several new theoretical and empirical results about large-scale robust RL are presented in this paper, there are still many open questions that need to be addressed. Current work focuses on the \((s,a)\)-rectangular RMDP (Def. 1). We leave extensions to more general \(s\)-rectangular and non-rectangular RMDP for future works. Some theoretical analysis in this paper partly relies on Assumption 2. Though it is commonly made and accepted in theoretical works as discussed in paragraphs after Assumption 2, it is not a necessary condition. Further exploration of this can potentially lead to more theoretical advances. Another natural future direction is to extend current results to more complex settings such as robust constrained RL and robust multi-agent RL, following recent developments of policy gradient-based approaches in safe RL  and multi-agent RL .