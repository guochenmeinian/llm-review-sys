# Localize, Understand, Collaborate:

Semantic-Aware Dragging via Intention Reasoner

 Xing Cui\({}^{1}\), Peipei Li\({}^{1}\), Zekun Li\({}^{2}\), Xuannan Liu\({}^{1}\), Yueying Zou\({}^{1}\), Zhaofeng He\({}^{1}\)

\({}^{1}\)Beijing University of Posts and Telecommunications

\({}^{2}\)University of California, Santa Barbara

{cuixing, lipeipei, liuxuannan, zouyueying2001, zhaofenghe}@bupt.edu.cn

zekunli@cs.ucsb.edu

Corresponding author

###### Abstract

Flexible and accurate drag-based editing is a challenging task that has recently garnered significant attention. Current methods typically model this problem as automatically learning "how to drag" through point dragging and often produce one deterministic estimation, which presents two key limitations: 1) Overlooking the inherently ill-posed nature of drag-based editing, where multiple results may correspond to a given input, as illustrated in Fig. 1; 2) Ignoring the constraint of image quality, which may lead to unexpected distortion. To alleviate this, we propose LucidDrag, which shifts the focus from "how to drag" to "what-then-how" paradigm. LucidDrag comprises an intention reasoner and a collaborative guidance sampling mechanism. The former infers several optimal editing strategies, identifying what content and what semantic direction to be edited. Based on the former, the latter addresses "how to drag" by collaboratively integrating existing editing guidance with the newly proposed semantic guidance and quality guidance. Specifically, semantic guidance is derived by establishing a semantic editing direction based on reasoned intentions, while quality guidance is achieved through classifier guidance using an image fidelity discriminator. Both qualitative and quantitative comparisons demonstrate the superiority of LucidDrag over previous methods. Code is available at: https://github.com/cuixing100876/LucidDrag-NeurIPS2024.

Figure 1: Given an input image, the user draws a mask specifying the editable region and clicks dragging points (handle points (red) and target points (blue)). Our LucidDrag considers the ill-posed nature of drag-based editing and can produce diverse results (the first row). Besides, it achieves outstanding performance in editing accuracy and image fidelity (the second row).

Introduction

The impressive success of diffusion models [27; 12; 51] has promoted the advancements in the field of image editing [26; 69; 17]. The conventional paradigms for editing conditions typically encompass text [13; 47; 25], instruction [4; 71; 68; 16], or image [64; 61; 21]. However, these conditions prove inadequate in effectively communicating specific image aspects, such as shape and location .

To address this, recent studies [14; 50; 53] define a new task called drag-based editing, which incorporates dragging points as conditions. These studies specifically regard drag-based editing as the problem of "how to drag" and tackle it by designing an editing guidance loss, enabling the model to implicitly learn the appropriate solutions. Despite their considerable success [50; 45; 46], these methods have two limitations: _Firstly, they neglect the inherent ambiguity of semantic intention_. Drag-based editing is an ill-posed problem. As depicted in Fig. 1, the drag points starting from the horse's head and ending at its upper right can indicate various semantic intentions, such as "make the neck longer," "raise the head," or "bring the two horses closer." However, existing methods mainly follow point dragging principles [53; 45], focusing on positional movement by constraining feature correlation between the source and target points. This current position optimization strategy inherently overlooks semantic diversity, making it challenging to generate images with precise semantic perception. _Secondly, they overlook the preservation of the overall image quality._ Current methods prioritize editing accuracy while neglects the overall image quality. Some methods [45; 46] utilize score-based classifier guidance for image editing, which can cause mismatches between the distribution of the edited image and the input image, compromising image fidelity.

In this study, we divide the drag-based editing task into two steps. We introduce a preliminary step, "what to drag", to determine the specific content and semantic direction for editing before addressing "how to drag". That is, we shift the focus from "how to drag" to a paradigm of "what-then-how". As shown in the first row of images in Fig. 1, before editing, we need to determine what we are going to edit, such as the horse's head, and the semantic strategy for editing it towards the upper right. For example, we could make the horse lift its head, elongate its neck, or shorten the distance between the two horses. With the "what to drag" information established, we can then proceed to address "how to drag". To achieve this, we construct an intention reasoner that integrates a Large Language-Vision Model (LVLM) and a Large Language Model (LLM) to deduce possible intentions. As illustrated in Fig. 1, given an input image and drag points that begin at the horse's face and end in its upper-right, the intention reasoner acts as an AI agent to infer potential intentions, subsequently providing corresponding source and target prompts. Once the intention is determined, we then address "how to drag" by injecting the reasoned semantic information into the model by developing collaborative guidance sampling, which integrates editing guidance with the proposed semantic guidance and quality guidance. Specifically, the semantic guidance is derived from the source and target prompts determined by the intention reasoner. The asymmetric prompts establish a clear semantic editing direction towards the target intention. Additionally, a discriminator is employed as the score function to provide quality guidance. The quality gradient is generated based on image fidelity and incorporated into the model via the classifier guidance mechanism. For the editing guidance, we follow previous work  to maximize the feature correspondence between the source and target positions.

LucidDrag is an intuitive framework for "what-then-how" drag-based editing, demonstrating outstanding performance in terms of semantic perception ability, diversity, and editing quality. LucidDrag enjoys several attractive attributes: _Firstly, clear, diverse, and reasonable semantic intentions._ Our innovative method employs LVLM and LLM to explicitly deduce the intention by localizing the drag points and reasoning several probable intentions. By explicitly incorporating semantics, we enhance semantic perception and offer diverse editing modes, enriching the variety of outcomes. _Secondly, enhanced overall generation quality._ By introducing collaborative guidance sampling, we significantly promote the generation quality of drag-based editing. Specifically, we achieve diverse and accurate image editing by introducing an additional semantic editing direction through semantic guidance. Additionally, we maintain better image quality by explicitly constraining the image distribution using quality guidance. In summary, our main contributions are:

* We propose a new "what-then-how" paradigm for drag-based editing and introduce LucidDrag. To address the "what" problem, we present an intention reasoner, which employs LVLM and LLM to determine what content and semantic direction should be edited.

* We then use the inferred semantic results of "what" to guide "how", enhancing editing accuracy and overall image quality. Additionally, a quality discriminator is also employed to provide quality gradients via score-based classifier guidance. This quality guidance, combined with editing and semantic guidance from "what," improves the precision and fidelity of the results.
* We present quantitative and qualitative results demonstrating the applicability and superiority of our method, in terms of editing accuracy, fidelity, and diversity.

## 2 Related Work

### Diffusion Models

Diffusion model [54; 20; 56] aims to estimate the noise \(_{t}\) added to the image \(z_{t}=_{t}x+_{t}_{t}\), where \(_{t}\) and \(_{t}\) are non-learned parameters. The training loss is to minimize the distance between the added noise and the estimated noise:

\[L()=_{t(1,T),_{t}(0,I)} ||_{t}-_{}(z_{t};t,y)||_{2}^{2},\] (1)

where \(t\) refers to the time step, \(_{t}\) is the ground-truth noise, \(y\) is an additional condition. Diffusion models can be regarded as score-based models . In this context, \(_{}\) serves as an approximation of the score function for the noisy marginal distributions: \(_{}(z_{t})_{z_{t}} p(z_{t})\).

we can sample images given conditioning \(y\) by starting from a random noise \(z_{T}(0,I)\), and then alternating between estimating the noise component and sampling \(z_{t-1}\). The noise is estimated as:

\[_{t}=_{}(z_{t};t,y).\] (2)

The sampling could be based on DDPM  or DDIM . In this paper, we utilize DDIM, which denoising \(z_{t}\) to a previous step \(z_{t-1}\) with a deterministic process:

\[z_{t-1}=}{_{t}}}z_{t}+(}-1}-}-1})_{t}.\] (3)

### Classifier Guidance

Classifier guidance moves the sampling process towards images that are more likely according to the classifier . As a powerful conditional sampling strategy, it has been used in various tasks, including generating diverse results , refining generative process [31; 65] and image editing [45; 46]. Specifically, it combines the unconditional score function for \(p(z_{t})\) with a classifier \(p(y|z_{t})\) to produce samples from \(p(z_{t}|y) p(y|z_{t})p(z_{t})\)[12; 56]. Classifier Guidance requires access to a labeled dataset and the training of a noise-dependent classifier \(p(y|z_{t})\) which can be differentiated concerning the noisy image \(z_{t}\). During the sampling process, classifier guidance can be incorporated as follows:

\[_{z_{t}} q(z_{t}|y)_{z_{t}} q(z_{t})+_{z_{t} } q(y|z_{t}),\] (4)

The first term is the original diffusion denoiser, and the second term refers to the conditional gradient produced by an energy function \(g(z_{t};t,y)=q(y|z_{t})\). Thereby, we apply classifier guidance by modifying \(_{t}\):

\[_{t}=_{}(z_{t};t,y)+_{z_{t}} g (z_{t};t,y),\] (5)

where \(\) is an additional parameter parameter that modulates the strength of the guidance.

### Visual Programming

As LLMs [49; 7; 58] and LVLMs [74; 11; 67] demonstrated remarkable emergency abilities [1; 62; 40; 41], researches [18; 37; 59] explore to leverage them for planning and reasoning in multi-modal image generation. For example, ChatEdit  utilizes pre-trained language models to track the user intents. Some approaches [18; 37; 59] augment the original prompt through paraphrasing. Recent, visual programmer methods [22; 24] translate complex input prompts into programmatic operations and data flows. Despite the effectiveness of these strategies in augmenting the input text instructions, they overlook the capacity to reason within visual-modal instructions, such as the dragging points in drag editing tasks. In contrast, our LucidDrag addresses this gap by integrating LVLM and LLM.

### Image Editing

Image editing aims to manipulate an image according to specific conditions. Prior methodologies [33; 36; 34] can only manipulate specific attributes. The prevailing approaches have primarily relied on text conditions. For example, [9; 2] manipulate images in the GAN  latent space by learning an edit direction. Motivated by the success of the diffusion model [27; 60], state-of-the-art methods extend their exploration into diffusion-based image editing by exploring the initial noise [10; 73; 43], attention maps [6; 26; 63; 15], or prompts [35; 44; 13; 4; 57]. Recently, DragGAN  explores a novel editing scheme that drags any points of the image to reach target points with the help of StyleGAN  latent space. FreeDrag  improves point tracking by introducing adaptive feature updating and backtracking. Readout Guidance  solves the challenging task by leveraging the video dataset. The following works [45; 46] utilize the feature correspondence to direct the editing process. Nevertheless, they only use the drag points as a control, which is insufficient due to the potential diversity in semantic intentions. In contrast, our approach introduces an intention reasoner to achieve semantic-aware editing. The intention reasoner can reduce cognitive load, handle vague requests, and discover potential needs. Additionally, it generates precise descriptions automatically, ensuring accurate and consistent manipulations.

## 3 Method

In this section, we introduce LucidDrag, a unified framework for Drag Manipulation via **L**ocaling, **U**nderstanding, **C**ollaborate **Guiding.** Within our framework, image drag editing is decomposed into two stages. Firstly, the _Intention Reasoner_ translates the user-drag points into potential semantic intentions and generates corresponding source prompt and target prompt, thus solving the problem of "what to edit". Then, the _Collaborative Guidance Sampling_ is designed to facilitate image editing. Concretely, prompts generated from the intention reasoner are utilized in the DDIM Inversion and Diffusion U-Net, providing semantic guidance for the generation of semantically controllable results. Besides, LucidDrag employs a discriminator to provide quality guidance for improved image fidelity. The semantic guidance and the quality guidance collaborate with the original editing guidance, offering a novel perspective of "how to edit". We elaborate on more details of our techniques below.

### Intention Reasoner

As depicted in Fig. 2, the first stage of our LucidDrag is the intention reasoner, which bridges the gap between the input point condition and semantic intention. This intention reasoner consists of two key

Figure 2: Overview of LucidDrag. LucidDrag comprises two main components: an intention reasoner and a collaborative guidance sampling mechanism. _Intention Reasoner_ leverages an LVLM and an LLM to reason \(N\) possible semantic intentions. _Collaborative Guidance Sampling_ facilitates semantic-aware editing by collaborating editing guidance with semantic guidance and quality guidance.

components, _i.e._, an LVLM locator that identifies the interested position, and an LLM understander that interprets input conditions into semantic intentions.

LVLM-driven loatorGiven an input image, it may contain various objects situated at different positions. To accurately identify the objects of interest, we employ an off-the-shelf pre-trained large vision-language model (LVLM) Osprey . Osprey is trained with fine-grained mask regions, enabling it to comprehend images at the pixel level. With the input image\(x\), we instruct the Osprey model with drag points \(P\) to generate a descriptive representation of the objects of interest \(O\), _i.e._, \(O=LVLM(x,P)\). As shown in Fig. 2, _O="The nose of a woman_", which subsequently serves as input to prompt the LLM to understand the condition and reason potential intentions.

LLM-driven reasonerAs shown in Fig. 2, each point condition may encompass various semantic intentions. For instance, it may represent non-rigid manipulation, such as "looking down", while it may also represent rigid manipulation, such as "moving to the left". Our LLM-driven reasoner is designed to discern potential semantic intentions to facilitate semantic-aware drag-based editing. We leverage the capabilities of the large language model, GPT 3.5 , acting as an AI agent to reason the possible intentions. We take the generated description of the object of interest \(O\), the original image caption \(C\), and drag points \(P\) as input. Then, we prompt the LLM with in-context examples to generate \(N\) possible intentions, _i.e._\(D=LLM(O,C,P)\), where each output sample can have different intentions or levels of complexity. Specifically, \(D=\{(d_{j},P(d_{j}))\}_{j=1}^{N}\), where \(d_{j}=\{i_{j},p_{j}^{s},p_{j}^{t}\}\) is the generated text output. \(i_{j}\), \(p_{j}^{s}\), \(p_{j}^{t}\) represents the predicted intention, the predicted description of the source image (source prompt), and the predicted description of the target image (target prompt), respectively. \(P(d_{j})\) is the corresponding confidence probabilities of the \(j\)-\(th\) generate text output. Finally, we can select \(n\) outputs \(\{d_{s^{*}}\}_{s=1}^{n}\) by sampling based on the confidence probabilities. The confidence probability reflects the quality of the output. A higher confidence probability indicates that the intention is more reasonable, leading to better editing results.

\[\{d_{s^{*}}\}_{s=1}^{n}= D}{argmax}(P( d_{i}),n).\] (6)

### Collaborative Guidance Sampling

As shown in Fig. 2, the objective of collaborative guidance sampling is to modify the intended content while ensuring the preservation of irrelevant components. The input image is inverted to noise \(z_{T}^{gen}\) through DDIM Inversion . During the inversion process, the intermediate noise \(z_{t}^{quad}\), along with the corresponding key \(k_{t}^{quad}\), and value \(v_{t}^{quad}\) of the self-attention layer, are recorded in the memory bank, which serves in guiding subsequent generation process. Subsequently, we generate the edited images employing collaborative guidance sampling which incorporates three fundamental components: semantic guidance, quality guidance, and editing guidance. Each of these components contributes to the overall editing process distinctively, thereby ensuring a balanced and comprehensive approach to image editing.

Semantic guidanceAs textual conditions can convey semantic information, we leverage the source and target prompts generated by the intention reasoner to facilitate semantic-aware dragging. Specifically, during the inversion process, we employ the source prompt to transform the input image into its corresponding noise by iterating DDIM inversion, _i.e._, \(z_{t+1}^{quad}=(z_{t}^{quad},p_{s}^{s})\). During the sampling process, we utilize the target prompt to generate the target image, _i.e._, \(z_{t-1}^{gen}=(z_{t}^{gen},p_{s}^{t})\). As there exists a divergence between the source and target prompts, the asymmetric textual condition introduces a distinct editing direction that is oriented toward the target image, thereby offering semantic guidance. This differentiation facilitates a semantically guided editing process, enhancing the semantic coherence of the edited image.

Quality guidanceAs shown in Fig. 2, we design quality guidance to ensure the quality of the generated image. A discriminator is trained to distinguish between high-fidelity images and low-fidelity images at any step \(t\). In particular, given a real image and its corresponding text description \(y\), we utilize a stochastic process to simulate potential points of drag and their respective directions. Then, we generate images using DragonDiffusion . The images with an aesthetic score below \(5\), are classified as low-fidelity. Their corresponding real images are considered high-fidelity. The selected low-fidelity images, along with the high-fidelity images, constitute the training dataset of the discriminator. Finally, the training dataset comprises a total of 10,000 high-fidelity images and 10,000 low-fidelity images.

As the intermediate representation of the diffusion U-Net captures semantic information of the input image , we utilize this hidden representation to evaluate image quality. The discriminator comprises the down block and middle block of the Diffusion U-Net to capture the semantic information, followed by a linear classifier layer. During training, we froze the down blocks which are initialized with Stable Diffusion v2.1-base . The middle block and the prediction layer are fine-tuned to classify images as real or fake. The conditional discriminator \(d(X_{t}|y;t)\) is trained by minimizing the canonical discrimination loss:

\[=_{y,t}_{z_{ t} p(z_{t}|y)}[- d(z_{t}|y;t)]\\ +_{z_{t} q(z_{t}|y)}[-(1-d(z_{t}|y;t))]. \] (7)

The energy function to constrain the image quality is defined as [19; 31]:

\[g_{quality}=|y)}{q(z_{t}|y)}(z_{t}|y;t)}{1-d^{*}( z_{t}|y;t)}.\] (8)

Editing guidanceFollowing DragonDiffusion , we extract intermediate features \(F_{t}^{gen}\) and \(F_{t}^{gud}\) from \(z_{t}^{gen}\) and \(z_{t}^{gud}\) via UNet denoiser \(_{}\) respectively. The energy function is built by calculating the correspondence between \(F_{t}^{gen}\) and \(F_{t}^{gud}\). The editing guidance includes editing target contents and preserving unrelated regions. We denote the original content position, target content position, and the unrelated content position as binary masks, _i.e._, \(m^{orig}\), \(m^{far}\), and \(m^{share}\), respectively. The energy function to drag the content is defined as:

\[g_{drag}=(F_{t}^{gen},m^{tar},F_{t}^{gud },m^{orig})},\] (9)

where \((F_{t}^{gen},m^{tar},F_{t}^{gud},m^{orig})\) calculates the similarity between the two regions of \(F_{t}^{gen}\) and \(F_{t}^{gud}\). Similarly, the energy function to preserve the unrelated region is defined as:

\[g_{content}=_{local}(F_{t}^{gen},m^{share },F_{t}^{gud},m^{share})}.\] (10)

The final editing energy function is defined as:

\[g_{edit}=w_{e} g_{drag}+w_{c} g_{content},\] (11)

where \(w_{e}\) and \(w_{c}\) are hyper-parameters to balance these guidance terms. Finally, the editing guidance collaborates with the quality guidance during sampling:

\[g(z_{t};t,y)=g_{edit}+g_{quality}.\] (12)

Additionally, following , to ensure content consistency between the edited image and the input images, we replace the keys and values within the self-attention module of the UNet decoder with those retrieved from the memory bank.

## 4 Experiments

### Implementation Details

To train the quality discriminator, we employ the Adam optimizer with a learning rate of 1e-4. We set the training epochs as 100 and the batch size as 128. For the denoising process, we adopt Stable Diffusion  as the base model. During sampling, the number of denoising steps is set to \(T=50\) with a classifier-free guidance of \(5\). The energy weights for \(g_{quality}\), \(g_{drag}\) and \(g_{content}\) are set to \(1e-3\), \(4e-4\) and \(4e-4\), respectively. The training of the discriminator can be conducted on a NVIDIA V100 GPU and the inference can be conducted on a NVIDIA GeForce RTX 3090 GPU.

### Comparisons

Semantic-aware draggingSince our LucidDrag effectively discerns potential intentions, we first evaluate its semantic-awareness ability. Specifically, given an input image and corresponding dragging conditions, we sample several intentions and obtain corresponding results. The results are shown in Fig. 3. On the one hand, the intention reasoner deduces reasonable intentions that align with the input dragging points and generate semantic-aware images, demonstrating both an enhanced understanding of semantic intentions and increased diversity. On the other hand, our method can generate high-fidelity images aligned with the input prompts, improving the quality of the results.

Content draggingWe evaluate the proposed LucidDrag against existing drag editing models [38; 53; 45; 46]. We first conduct quantitative comparisons. Following DragDiffusion , we utilize the DragBench benchmark which is designed for the image-dragging task. In DragBench, each image is accompanied by a set of dragging instructions, including several pairs of handle and target points

Figure 3: LucidDrag allows generating diverse results conforming to the intention.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

Conclusion

In this work, we identify the limitations of previous drag editing models in understanding semantic intentions and generating high-quality edited images. In response, we design a novel framework called LucidDrag, which involves an intention reasoner to clarify possible intentions and a collaborative guidance sampling mechanism that incorporates explicit semantic guidance and quality guidance. LucidDrag excels in: **i)** adequate understanding of semantic intention, improving semantic perception ability and diversity of the generated images; **ii)** enhanced dragging performance, including improved drag accuracy and image quality. Extensive results show the efficiency of our approach and the potential for further advancements in semantic-aware drag-based editing.

## 6 Limitations

Although our method is capable of achieving semantic-aware drag-based editing without the need for training, there are some limitations. **i)** Complex objects are challenging to drag, and unexpected deformation can occur over long distances. This may be attributed to difficulties in comprehending the intricate nature of the object or inaccurate object tracking. In future work, we will investigate the potential for further improvements in performance by utilizing more powerful image generation models and incorporating comprehensive intention understanding. **ii)** Hyperparameters involved in the editing process will affect the editing results. In future work, we intend to utilize LLM as an agent to automatically determine model hyperparameters to enhance the editing performance.