# A Global Depth-Range-Free Multi-View Stereo Transformer Network with Pose Embedding

Yitong Dong\({}^{1}\)1 Yijin Li\({}^{1}\)1 Zhaoyang Huang\({}^{2}\) Weikang Bian\({}^{2}\)

Jingbo Liu\({}^{1}\) Hujun Bao\({}^{1}\) Zhaopeng Cui\({}^{1}\) Hongsheng Li\({}^{2}\) Guofeng Zhang\({}^{1}\)2

\({}^{1}\)State Key Lab of CAD&CG, Zhejiang University \({}^{2}\)CUHK MMLab

Equal contributionCorresponding author

###### Abstract

In this paper, we propose a novel multi-view stereo (MVS) framework that gets rid of the depth range prior. Unlike recent prior-free MVS methods that work in a pair-wise manner, our method simultaneously considers all the source images. Specifically, we introduce a Multi-view Disparity Attention (MDA) module to aggregate long-range context information within and across multi-view images. Considering the asymmetry of the epipolar disparity flow, the key to our method lies in accurately modeling multi-view geometric constraints. We integrate pose embedding to encapsulate information such as multi-view camera poses, providing implicit geometric constraints for multi-view disparity feature fusion dominated by attention. Additionally, we construct corresponding hidden states for each source image due to significant differences in the observation quality of the same pixel in the reference frame across multiple source frames. We explicitly estimate the quality of the current pixel corresponding to sampled points on the epipolar line of the source image and dynamically update hidden states through the uncertainty estimation module. Extensive results on the DTU dataset and Tanks&Temple benchmark demonstrate the effectiveness of our method. The code is available at our project page: https://zju3dv.github.io/GD-PoseMVS/.

## 1 Introduction

Multi-view stereo matching (MVS) is a crucial technique in 3D reconstruction, which aims to recover robust and reliable 3D representations from multiple RGB images [1; 2; 3]. Traditional methods [4; 5; 6] rely on hand-crafted similarity metrics and regularizations to compute dense correspondences between the input images. These methods are prone to degradation in challenging scenarios, such as varying illumination, textureless regions, and occlusion regions. Recently, learning-based methods [7; 8; 9; 10; 11] directly learn discriminative features from the input images through neural networks such as CNN and Transformers. By sampling some possible depth hypothesis within a given depth range, they warp the features from the source images to the reference view (i.e., the plane sweep algorithm ) and compute the cost volume, which is then regularized also through the neural network to obtain the final depth maps. However, obtaining a suitable depth range is non-trivial when applied in real-world scenarios while these methods are generally sensitive to the depth range, which limits their application.

To get rid of the dependence on depth range, some methods [13; 14; 15] transform the regression problem in the given depth space into a matching problem on the epipolar lines. Similar to optical flow  and feature matching [17; 18; 19; 20], these methods also adopt a pair-wise manner. For example, DispMVS  computes the depth map of the source image multiple times through pairsthat contain different source images and computes the final depth map by weight of sum. However, the pair-wise manner neglects the inter-image correspondence between the source images and could lead to sub-optimal solutions. Meanwhile, although DispMVS mitigates the influence of depth priors on constructing the 3D cost volume, its initialization based on depth range can still lead to significant performance degradation when the depth range error is too large, as shown in Fig. 1.

We argue that these methods need to consider all the source images at the same time. Our ideas are inspired by the recent methods [21; 22] of optical flow which concurrently estimate optical flows for multiple frames by sufficiently exploiting temporal cues. However, we find these frameworks cannot be trivially applied in the task of multi-view stereo. The reasons are twofold. First, a strong cue in the multi-frame optical flow estimation is that the flow originating from the same pixel belongs to a continuous trajectory in the temporal dimension. Additionally, the frames are sequentially aligned along this temporal dimension. Such inductive bias makes it easy to learn. But in the context of multi-view stereo, the source images may be captured in no particular order, lacking a similar constraint of continuity. Unlike optical flow, the input images in multi-view stereo are unordered. These distinctions pose a significant challenge when attempting to adapt the multi-frame optical flow framework for use in multi-view stereo. Second, the arbitrary positions and viewing angles of the source images, coupled with potentially large temporal gaps between captures, exacerbate issues such as varying illumination, significant viewport differences, and occlusions which call for new designs.

Based on the above observations, in this paper, we propose a novel framework that gets rid of the depth range assumption. Unlike some recent methods [13; 14; 15] that work in a pair-wise manner, the proposed method estimates the depth maps of a reference image by simultaneously considering all the source images. To address the first issue, we design careful injection of geometric information into disparity features using 3D pose embedding, followed by multi-frame information interaction through an attention module. Subsequently, we encode multi-view relative pose information and geometric relationships between specific sampled points into 3D pose embedding, which is subsequently transferred to the Multi-view Disparity Attention (MDA) module. This method efficiently incorporates the relationship between depth and pixels within the network, facilitating improved information integration across multiple frames. Second, to mitigate the challenge of fluctuating image quality stemming from occlusion and other factors, we maintain and update the disparity hidden features to reflect the depth uncertainty of the current sampling point for each iteration. We design the disparity feature encoding module to learn disparity features along the epipolar lines of multi-view frames. This approach enables us to explicitly characterize occlusion scenarios for each pixel across diverse source images and dynamically adapt them during epipolar disparity flow updates. Consequently, the auxiliary information is furnished for subsequent information fusion within the module. Furthermore, we designed a novel initialization method to further eliminate the influence of the depth range compared to DispMVS .

In summary, our contributions can be highlighted as follows: (1) A multi-view disparity transformer network, which facilitates the fusion of information across multi-view frames, (2) A specially designed 3D pose embedding which is utilized to implicitly construct relationships of the epipolar disparity flow among multi-view frames, and (3) An uncertainty estimation module and dynamically updated hidden states representing the quality of source images during iterations. We evaluate our method against other MVS methods on the DTU dataset  and Tanks&Temple dataset , and demonstrate its generalization in Fig. 1.

## 2 Related Work

### Traditional MVS

Multi-View Stereo has been developed for many years and has many downstream or related applications such as simultaneous localization and mapping (SLAM) , visual localization , 3D reconstruction [27; 28], 3D generation  and scene understanding . Traditional methods for Multi-View Stereo (MVS) can generally be categorized into three classes: volumetric, point cloud-based, and 2D depth map-based methods. Volumetric methods [31; 32] typically partition the 3D space into voxels and annotate them as either interior or exterior to the object surface. Point cloud-based methods [33; 34] directly optimize the point cloud coordinates of objects in 3D space. Depth map-based methods [2; 4; 35; 6] first estimate 2D depth corresponding to images and then fuse the 2D depths of the same scene to obtain a 3D point cloud. However, these traditional methods remain constrained by manually crafted image features and similarity matrices.

### Deep learning based MVS

**CNN-based MVS methods** generally leverage convolutional neural networks to construct and refine 3D cost volume [36; 37; 11; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47]. For instance,  uses isotropic and anisotropic 3D convolution-based learning networks to estimate the depth map.  introduces a pixel-wise network to obtain visibility.  applies a multi-stage CNN framework to enable reconstruction.  and  build a kind of pyramid to realize 3D cost volume. Similarly,  proposes a sparse-to-dense CNN framework when constructing the 3D cost volume.

**RNN-based methods** mainly exploit recurrent network structures [48; 49] to regularize 3D cost volume [50; 51; 52; 9; 13; 53; 54]. For example,  utilizes recurrent encoder-decoder structure and 2D CNN framework to solve large-scale MVS reconstruction.  introduces a scalable RNN-based MVS framework. IterMVS  uses a GRU-based estimator to encode the probability distribution of depth. Compared with 3D CNN, RNN highly reduces the memory requirement, which makes it more suitable for large-scale MVS reconstruction .

**Transformer** is popular in 3D vision tasks [56; 57; 58; 59], and first introduced into the field of MVS reconstruction by  due to its ability to capture global context information. Transformer is incorporated into feature encoding [10; 60; 61] to capture features within and between input images. The succeeding work  implements a transformer to assign weights to different pixels in the aggregating process.  employs an Epipolar Transformer to perform non-local feature augmentation. However, these deep learning-based MVS methods commonly exhibit sensitivity to the depth range, thereby restricting their broad applicability.

**Scale-agnostic MVS methods** infer the depth information from the movement along epipolar lines to reduce the heavy dependence of depth range priors. Several methods [14; 15] perform 2D sampling between two frames and iteratively update flows to find the matching points. Specifically, DispMVS  is randomly initialized within the depth range and performs depth fusion by utilizing a weighted sum. RAMDepth  selects a random source image in each iteration. However, both methods fail to fully exploit multi-frame constraints during the flow updates due to the mismatch of 3D information at sampling points. In this paper, we enhance the epipolar matching process by simultaneously considering multi-frame information.

## 3 Method

Given a reference image \(I_{0}^{H W 3}\) and multi-view source images \(\{I_{i}\}_{i=1}^{N-1}^{H W 3}\) as input, the task of MVS is to calculate the depth map of the reference image. We treat MVS as a matching problem: for a pixel point \(p_{r}\) in the reference image, we identify the corresponding point \(p_{s}\) in the source image, then we can get depth by triangulation. Given the initial matching point \(p_{s}^{0}\) obtained by the initial depth, we adopt an iterative update strategy. Since the matching point lies on the epipolar line of the source image, the one-degree-of-freedom epipolar disparity flow is used

Figure 1: The robustness testing on the depth range. Under identical training configurations, our method exhibits superior robustness to variations in depth range compared with two state-of-the-art methods [13; 14]. The red markings denote the actual depth range used during training.

to represent the network's iterative updates. The epipolar disparity flow \(e_{s}^{k}^{H W 1}\) is 1-d flow along the epipolar line on the source image during each iteration:

\[e_{s}^{k}=}(p_{s}^{k}-p_{s}^{0}),\] (1)

where \(e_{dir}^{}\) is the normalized direction vector of the epipolar line, \(\) is the dot product of vectors, and \(k\) is the iteration time. Different from previous methods [14; 13], we fully eliminate the dependence on depth range during initialization and achieve synchronous updating of the epipolar disparity flow across multi-view images. This is done by our design of disparity information interaction.

The overall pipeline of our method is illustrated in Fig. 2. The proposed method starts from a feature extraction module to extract multi-scale features (Sec. 3.1). Then, we discuss how to initialize the depth map without depth range (Sec. 3.2) and perform feature encoding (Sec. 3.3). To facilitate information fusion across multi-view source images, we introduce the Multi-view Disparity Attention (MDA) module (Sec. 3.4), enhanced with Pose Embedding. Finally, the features enhanced by the MDA module are fed into a GRU module to update the epipolar disparity flow, as described (Sec. 3.5), which is then fused to generate the depth map.

### Feature Extraction

Following previous methods [10; 13; 60; 64], we employ convolutional neural networks (CNNs) for image feature extraction. Moreover, we adopt a coarse-to-fine strategy to extract multi-scale image features. Specifically, we utilize two share-weighted feature extraction modules to extract image features \(F_{0}^{l}^{H W C}\) and \(\{F_{i}^{l}\}_{i=1}^{N-1}^{H W C}\) and a context feature extraction module to extract context features.

### Initialization

Differing from DispMVS , we design a novel initialization method without depth range to further mitigate the influence of depth priors. Specifically, we select an initial position along the epipolar line and then convert it into the depth map. First, we derive the correspondence between depth and position along the epipolar line. Given a pixel \(p_{r}\) of the reference image \(I_{0}\), the geometric constrain between it and the warped pixel \(p_{s}\) of the source image \(I_{i}\) can be written as:

\[K_{i}^{-1}p_{s}d_{s}=R(K_{0}^{-1}p_{r}d_{r})+T,\] (2)

where \(d_{r}\) denotes the depth in reference view, \(d_{s}\) denotes the depth in source view. \(R\) and \(t\) denote the rotation and translation between the reference and the source view, \(K_{0}^{-1}\) and \(K_{i}^{-1}\) denote the intrinsic matrices of the reference and the source view. Let \(T=(t_{x},t_{y},t_{z})^{T},K_{i}^{-1}p_{s}=(p_{sx},p_{sy},p_{sz})^{T}\) and \(R K_{0}^{-1}p_{r}=(p_{rx},p_{ry},p_{rz})^{T}\), we can associate \(d_{r}\) and \(d_{s}\) with pixel coordinates:

Figure 2: Overview of our method. We introduce the disparity feature encoding module to encode viewpoint quality differences, and the Multi-view Disparity Attention (MDA) module to facilitate information interaction between multi-view images. The MDA module is depicted in Fig. 3. Starting from an initial depth map \(D_{0}\), the epipolar disparity flows are iteratively updated and fused to the depth of the next stage.

\[d_{r} =(t_{x}p_{sz}-t_{z}p_{sz})/(p_{sz}p_{rz}-p_{sz}p_{rx}),& |_{xr xs}(p_{r})_{yr ys}(p_{ r})|\\ (t_{y}p_{sz}-t_{z}p_{sy})/(p_{sy}p_{rz}-p_{sz}p_{ry}),&\] (3) \[d_{s} =(t_{x}p_{rz}-t_{z}p_{rx})/(p_{sz}p_{rz}-p_{sz}p_{rx} ),&|_{xr xs}(p_{r})_{yr ys}(p_ {r})|\\ (t_{y}p_{rz}-t_{z}p_{ry})/(p_{sy}p_{rz}-p_{sz}p_{ry}),&\] (4)

where \(\) is a 2D flow vector along the epipolar line that provides flow in the x dimension \(_{xr xs}(p_{r})\) and \(\) dimension \(_{yr ys}(p_{r})\). To obtain an appropriate initial position, we first determine the geometrically valid range along the epipolar line, which has not been considered in other works [53; 14]. If a point is observable in the current view, it must have physical significance, meaning it must lie in front of the camera. Therefore, we identify the search range along the epipolar line on the source image that satisfies the condition \(d_{r}>0,d_{s}>0\). We obtain the initial position \(p_{s}^{0}\) by selecting the mid-point in search range along epipolar line.

### Disparity Hidden State Based Feature Encoding

Due to occlusion, moving objects, blurring, or other factors violating the multi-view geometry assumptions, the quality of sampling points from different source images varies, which limits the network's performance in depth estimation. To address this issue, we extract uncertainty information from the sampling point feature and encode it with cost volume as epipolar disparity feature \(F_{i}^{d}\). As shown in Fig. 2, we design the disparity hidden state \(H_{i}^{d}\) to maintain the sampling information of the current source image and update it during iterations by incorporating new uncertainty information.

**Cost Volume Construction.** For each source image, after determining the position \(p_{s}^{t}\) for the current iteration, we uniformly sample \(M\) points around \(p_{s}^{t}\) along the epipolar line at each scale with a distance of one pixel. By constructing a 4-layer pyramid feature using average pooling, uniform pixel sampling at different levels allows for a larger receptive field. The sampling interval in 2D is fixed. Given image features \(F_{0}^{l}\) and \(\{F_{i}^{l}\}_{i=1}^{N-1}\), we obtain the features of \(M\) sampled points in the source image through interpolation and calculate the visual similarity. The cost volume \(V^{H W M}\) is constructed by computing the dot product between pairs of image feature vectors:

\[V_{i}(p_{r})=_{r R} F_{I_{0}}(p_{r}) F_{I_{ i}}(p_{s}^{k}+r),\] (5)

where \(R\) represents the set of sampling points uniformly sampled along the epipolar line in the source image, and \(M\) denotes the number of sample points.

**Disparity Feature Encoding with Uncertainty.** When estimating the epipolar disparity flow from multi-view frames, it is essential to encode the differences between source images caused by variations in occlusion situations and image quality. Motivated by this, we conduct disparity hidden state \(H_{i}^{d}^{H W C}\) to explicitly represent the situation of point \(p_{r}\) relative to the source image. Motivated by this, we introduce a disparity hidden state \(H_{i}^{d}^{H W C}\) to explicitly represent the condition of points relative to the multi-view source images. \(H_{i}^{d}\) is randomly initialized and consecutively updated throughout the iterative process. We introduce a variance-based uncertainty estimation module to encode the correlation features, which is formulated as follows:

\[U_{i}=1-((V_{i}-})^{2}),\] (6)

where \(V_{i}\) denotes the cost volume of source image, \(}\) denotes the average value of \(V_{i}\), and \(()\) is the sigmoid function. Then, the uncertainty \(U_{i}\), the disparity hidden state of the previous iteration, the correlation features and the epipolar disparity flows are fed into the convolutional layers to generate epipolar disparity feature \(F_{i}^{d}\) and update the disparity hidden state \(H_{i}^{d}\).

### Multi-view Stereo Transformer

DispMVS estimates the epipolar disparity flow from each two-frame image pair \(\{I_{0},I_{i}\}_{i=1}^{N-1}\), which overlooks the abundant multi-view information. Inspired by VideoFlow , we estimate the epipolarflow of multi-view images simultaneously. However, since multiple source images are not sequentially arranged and points uniformly 2D sampled across source images can not establish robust 3D spatial correspondences, directly learning the continuity between flows, as , does not work.

Therefore, unlike [13; 10], etc., we design some special structures for information aggregation among multi-view images. Although the depths of sampled points along epipolar lines do not correspond, we observe that there is a regular pattern in the direction of depths along epipolar lines. As shown in Fig. 3, we design Multi-view Disparity Attention to learn the global information and utilize pose embedding to implicitly model the correspondence between pixel coordinates and depth on multiple source images, enabling the network to learn the direction and scale relationship of corresponding flows across different source images.

**Multi-view Disparity Attention.** To effectively capture extensive global information across epipolar disparity features from different views, we leverage the Multi-view Disparity Attention (MDA) module to further enhance the disparity features. We utilize an attention module to globally interact with disparity features of multi-view source images, thereby achieving multi-view feature fusion.

Given epipolar disparity features \(\{F_{i}^{d}\}_{i=1}^{N-1}\), we first use self-attention to achieve intra-image information interaction. We concatenate epipolar disparity features \(F_{i}^{d}^{H W C}\) and set \(H W\) the as sequence length \(L\), generating \(F^{d}^{(N-1)(H W) C}\).

Then we use cross-attention to achieve inter-frame information interaction and learn the relations among multi-view. We concatenate epipolar disparity features \(F_{i}^{d}\) and set the number of source images \(N-1\) the as sequence length \(L\), generating \(F^{d}^{(H W)(N-1) C}\).

To reduce computation cost, for the self-attention we use a linear transformer to compute attention, which replaces the original kernel function with:

\[(Q,K,V)=(Q)((K^{T} )V),\] (7)

where \(()=()+1\) and \(()\) represents the activation function of exponential linear units.

**Pose Embedding.** Due to the depths of sampling points varying for different source images, we utilize pose embedding to construct implicit disparity relationships among multi-view frames. To effectively convey useful information to the attention module, we categorize the features of pose embedding into two types: multi-view relative pose information and geometric information between specific sampled points. Fig. 3 illustrates the variables used to construct the pose embedding.

On one hand, the multi-view relative pose information between cameras contains crucial information about disparity features. By explicitly injecting relative poses into the attention module, the network can learn image-level geometric constraints. We represent the angle \(^{0,i}\) between rays as embedding. Inspired by , we encode the rotation matrix and translation matrix between the reference and the source view into the relative pose distance \(P^{0,i}\):

\[P^{0,i}=\|+tr(-R^{0,i} )},\] (8)

On the other hand, we encode the geometric information between specific sampled points. Due to our incorporation of pixel-level attention in addition to inter-frame attention, it is necessary to encode not

Figure 3: Illustration of MDA module. After concatenating features with 3D pose embedding and 2D normalized positional encoding, we achieve intra-image and inter-image information interaction through self-attention and cross-attention. As shown in the right figure, 3D pose embedding encodes relative pose and pixel geometric information into the features to enhance the learning capability of the attention mechanism.

only image-level camera poses but also the pixel-level information corresponding to sampled points. It is important to note that for each pixel in the reference image and its corresponding sampled point in the source image, we can obtain the corresponding 3D point coordinates \(P\) through triangulation based on stereo geometry. Accordingly, we encode the 2D coordinates \(p_{s}\) of the source image, the depth \(d_{r}\) from the perspective of the reference image, and the depth \(d_{s}\) from the perspective of the source image, thereby transforming the 3D information into corresponding relationships on the 2D plane. Moreover, we encode the normalized direction \(r_{0},r_{i}\) to the 3D location of a point.

### Iterative Updates

In the GRU updating process, we iteratively update the epipolar disparity flow \(e_{s}^{k+1}=e_{s}^{k}+ e_{s}\) obtained from the MDA module for each source image. In each iteration, the input to the update operator includes 1) the hidden state; 2) the disparity feature output from the MDA module; 3) the current epipolar flow; and 4) the context feature of the reference image. The output of the update operator includes 1) a new hidden state; 2) an increment to the disparity flow; and 3) the weight of disparity flow for multi-view images. We derive the depth from the disparity flow and employ a weighted sum to integrate the depth across multi-view source images. After fusion, the depth is converted back to disparity flow to perform the next iteration.

### Loss Function

Similar to , we output depth after each iteration and construct the loss function accordingly. We construct the depth L1 loss. The loss function is represented in Eq. 9:

\[loss=_{j=t_{c},t_{f}}_{0<=k<j}^{k}|(gt_{r} )-(d_{r}^{k})|,\] (9)

where \(t_{c}\), \(t_{f}\) are iterations at the coarse and fine stage, \(\) is a hyper-parameter which is set to 0.9.

## 4 Experiments

In this section, we first introduce the datasets (Sec. 4.1), followed by the implementation details of the experiment (Sec. 4.2). Subsequently, we delineate the experimental performance (Sec. 4.3) and conduct ablation experiments to validate the efficacy of each proposed module (Sec. 4.4).

### Datasets

DTU dataset  is an indoor multi-view stereo dataset captured in well-controlled laboratory conditions, which contains 128 different scenes with 49 views under 7 different lighting conditions. Following MVSNet , we partitioned the DTU dataset into 79 training sets, 18 validation sets, and 22 evaluation sets. BlendedMVS dataset  is a large-scale outdoor multi-view stereo dataset that contains a diverse array of objects and scenes, with 106 training scenes and 7 validation scenes. Tanks and Temples  is a public multi-view stereo benchmark captured under outdoor real-world conditions. It contains an intermediate subset of 8 scenes and an advanced subset of 6 scenes.

### Implementation Details

Implemented by PyTorch , two models are trained on the DTU dataset and large-scale BlendedMVS dataset, respectively. On the DTU dataset, we set the image resolution as \(640 512\) and the number of input images as 5 for the training phase. On the BlendedMVS dataset, we set the image resolution as \(768 576\) and the number of input images as 5 for the training phase. For all models, we use the AdamW optimizer with an initial learning rate of 0.0002 that halves every four epochs for 16 epochs. The training procedure is finished on two A100 with \(t_{c}=8\), \(t_{f}=2\). For depth filtering and fusion, we process 2D depth maps to generate point clouds and compare them with ground truth.

### Experimental Performance

In this section, we compare our method with other state-of-the-art methods and scale-agnostic methods. Existing methods are categorized into traditional methods [2; 35], 3D cost-volumemethods[8; 53; 9; 68; 47; 10; 69; 61], RNN-based methods [13; 70] and scale-agnostic methods [15; 14]. Methods that leverage scene depth range have an advantage as they can utilize accurate and robust information, thereby mitigating outliers, especially in textureless regions.

**Evaluation on DTU.** We evaluate the proposed method on the evaluation set of DTU dataset. We set the image resolution as \(1600 1152\) and the number of input images as 5. As shown in Table 1, our method has the best overall performance among depth-range-free methods. CER-MVS  and MVSFormer++  demonstrate superior performance; however, they are heavily dependent on the accuracy of the depth range. Our approach outperforms when compared with depth range-free methods like DispMVS  and RAMDepth , which demonstrates the effectiveness of our method in exploiting correlations among multi-view frames.

**Evaluation on Tanks and Temples.** Since the Tanks and Temples dataset does not provide training samples, we use a model pre-trained on the BlendedMVS dataset for testing. We set the image resolution as \(1920 1024\) and the number of input images as 7 for the evaluation phase. Table 2 presents the comparison between our method and other state-of-the-art methods. Our method achieves the best performance among scale-agnostic methods . Since RAMDepth  has not provided results on the Tanks and Temples dataset and source code, we are unable to make a comparison. Although our method exhibits a certain gap when compared to state-of-the-art methods [70; 61] based on precise depth priors, it demonstrates superior robustness across a broader depth range.

We visualize point clouds generated on DTU and Tanks and Temples dataset in Fig. 4, which demonstrates that our method is capable of constructing a comprehensive and precise point cloud.

### Ablation Study

In this subsection, we conduct ablation studies of our model trained on DTU  datasets to discuss the effectiveness of core parts of our method. The implemented baseline is basically based on DispMVS . All the experiments are performed with the same hyperparameters.

   Method & ACC.(mm)\(\) & Comp.(mm)\(\) & Overall(mm)\(\) \\  Gipuma  & **0.283** & 0.873 & 0.578 \\ COLMAP  & 0.400 & **0.664** & **0.532** \\  MVSNet  & 0.396 & 0.527 & 0.462 \\ A-RAMVSNet  & 0.376 & 0.339 & 0.357 \\ PatchmatchNet  & 0.427 & 0.277 & 0.352 \\ UniMVSNet  & 0.352 & 0.278 & 0.315 \\ TransMVSNet  & 0.321 & 0.289 & 0.305 \\ MVSFE+  & 0.340 & 0.266 & 0.303 \\ GeoMVS  & 0.331 & 0.259 & 0.295 \\ GBNet  & 0.315 & 0.262 & 0.289 \\ MVSFormer++ & **0.309** & **0.252** & **0.281** \\  IterMVS  & 0.373 & 0.354 & 0.363 \\ CER-MVS  & **0.359** & **0.305** & **0.332** \\  RAMDepth  & 0.447 & **0.278** & 0.362 \\ DispMVS  & 0.354 & 0.324 & 0.339 \\ Ours & **0.338** & 0.331 & **0.335** \\   

Table 1: The Quantitative point cloud evaluation results on DTU evaluation set. The lower the Accuracy (Acc), Completeness (Comp), Overall, the better. We split methods into four categories and highlight the best in bold for each.

Figure 4: Some qualitative results of the proposed method on DTU and Tanks and Temples datasets.

[MISSING_PAGE_FAIL:9]

For methods [13; 70; 69; 61] that rely on depth range prior for depth sampling, whether based on RNN or Transformer, they may exhibit better performance with accurate depth priors. However, as shown in Table 4, there is a marked decline in performance for these methods with larger depth range. Although DispMVS  showed insensitivity to depth range, its performance still exhibited a certain degree of decline with larger depth ranges. In contrast, our method, which is independent of depth range, maintained consistent performance regardless of changes in depth range.

It is crucial to emphasize that the depth range provided by the dataset is exceptionally accurate. For instance, the ground truth for the Tanks-and-Temples dataset is captured using an industrial laser scanner. However, in practical applications, while Structure-from-Motion (SfM) can derive depth ranges from sparse feature points, the resulting depth estimates are often prone to inaccuracies. These inaccuracies arise from the inherent sparsity of feature points, as well as challenges such as occlusion and suboptimal viewpoint selection. To verify the robustness of the MVS models in practical applications, we use the depth range obtained from COLMAP to replace the depth range ground truth (GT). As shown in Table 5, there is a significant decline in performance for GeoMVS , MVSFormer++ , IterMVS  and CER-MVS  when we use the depth range obtained from COLMAP. DispMVS  also exhibits a certain degree of decline. In contrast, our method maintained consistent performance. This result further demonstrates the necessity of eliminating the depth range.

## 5 Conclusion

We propose a prior-free multi-view stereo framework that simultaneously considers all the source images. To fully fuse the information from disordered and arbitrarily posed source images, we propose a 3D-pose-embedding-aided and uncertainty-driven transformer-based network. Extensive experiments show that our methods achieve state-of-the-art performances among the prior-free methods and exhibit greater robustness to the depth range prior. **Limitations**: The proposed method cannot run in real-time (i.e., 30 FPS), which could limit its application in mobile devices or other time-sensitive scenarios. Besides, our method shows a performance gap compared to SOTA cost-volume-based methods on the mainstream benchmark, despite these methods relying on highly precise depth range priors. In the future work, we hope to close the gap.