# Offline Multitask Representation Learning for Reinforcement Learning

Haque Ishfaq

Mila, McGill University

haque.ishfaq@mail.mcgill.ca &Thanh Nguyen-Tang

Johns Hopkins University

nguyent@cs.jhu.edu &Songtao Feng

University of Florida

sfeng1@ufl.edu &Raman Arora

Johns Hopkins University

arora@cs.jhu.edu &Mengdi Wang

Princeton University

mengdiw@princeton.edu &Ming Yin

Princeton University

my0049@princeton.edu &Doina Precup

Mila, McGill University

dprecup@cs.mcgill.ca

The corresponding authors.

###### Abstract

We study offline multitask representation learning in reinforcement learning (RL), where a learner is provided with an offline dataset from different tasks that share a common representation and is tasked to learn the shared representation. We theoretically investigate offline multitask low-rank RL, and propose a new algorithm called MORL for offline multitask representation learning. Furthermore, we examine downstream RL in reward-free, offline and online scenarios, where a new task is introduced to the agent that shares the same representation as the upstream offline tasks. Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model.

## 1 Introduction

Recent advances in offline reinforcement learning (RL) (Levine et al., 2020) have opened up possibilities for training policies for real-world problems using pre-collected datasets, such as robotics (Kalashnikov et al., 2018; Rafailov et al., 2021; Kalashnikov et al., 2021), natural language processing (Jaques et al., 2019), education (De Lima and Krohling, 2021), electricity supply (Zhan et al., 2022) and healthcare (Guez et al., 2008; Shortreed et al., 2011; Wang et al., 2018; Killian et al., 2020). While most offline RL studies focused on single-task problems, there are many practical scenarios where multiple tasks are correlated and it is beneficial to learn multiple tasks jointly by utilizing all of the data available (Kalashnikov et al., 2018; Yu et al., 2021, 2022; Xie and Finn, 2022). One popular approach in such cases is multitask representation learning, where the agent aims to tackle the problem by extracting a shared low-dimensional representation function among related tasks and then using a simple function (e.g., linear) on top of this common representation to solve each task (Caruana, 1997; Baxter, 2000). Despite the empirical success of multitask representation learning, particularly in reinforcement learning for its efficacy in reducing the sample complexity (Teh et al., 2017; Sodhani et al., 2021; Arulkumaran et al., 2022), the theoretical understanding of it is still in its early stages (Brunskill and Li, 2013; Calandriello et al., 2014; Arora et al., 2020; D'Eramo et al., 2020; Hu et al., 2021; Lu et al., 2021; Muller and Pacchiano, 2022). Although some workstheoretically studied the online multitask representation learning for RL where the agent is allowed to interact with multiple source tasks to learn the shared representation (Cheng et al., 2022; Agarwal et al., 2023; Sam et al., 2024), there is currently no theoretical understanding on the effectiveness of multitask RL in the _offline_ setting. This is crucial as in many practical scenarios (Kumar et al., 2022; Yoo et al., 2022; Lin et al., 2022), it is not feasible to interact with the different task environments in an online manner.

Moreover, when the tasks share the same representation, offline multitask representation learning can serve as a launchpad for effectively solving many other downstream tasks (Kumar et al., 2023). Consider the problem of learning to control robotic arms where we may already have offline datasets from different pick-and-place tasks in a kitchen such as the Bridge Dataset (Ebert et al., 2022). From this one can consider many possible downstream RL tasks where representation learned from these offline datasets can be beneficial. For example, one may consider solving a new pick-and-place task with different previously unseen objects in either an online or offline manner. Alternatively, one may consider a downstream reward-free RL (Jin et al., 2020) task where the agent would first gather additional novel and diverse data without a pre-specified reward function and afterward, when provided with any reward function (e.g. slightly different target placing spot for the picked object), would be asked to provide a good policy without additional interaction.

In this work, we study the provable benefits of offline multi-task representation learning for RL in which the learner is only given access to pre-collected data from different source tasks which are modeled by low-rank MDPs (Agarwal et al., 2020) with a shared (yet unknown) representation.

**Our contributions.** We develop a new offline multitask reinforcement learning algorithm that enables sample efficient representation learning in low-rank MDPs (Agarwal et al., 2020) and further provide improved sample complexity to the downstream learning. In summary, our main contributions are:

* We propose a new offline multitask representation learning algorithm called Multitask Offline Representation Learning (MORL) under low-rank MDPs. MORL represents a standard training procedure in modern machine learning, by pooling the data from all source tasks to learn a shared representation of the dynamics via maximum likelihood estimation oracle.
* We prove that, MORL can learn a near-accurate model, and, when combined with the pessimism principle, find a near-optimal policy for each of the source tasks \(T\) in the average sense, more sample-efficiently, than learning each task in isolation. To our knowledge, this is the first theoretical result demonstrating the benefit of representation learning in offline multitask RL.
* We then show theoretical benefits of using the learned representation from MORL in downstream reward-free RL Jin et al. (2020); Wang et al. (2020). In particular, we show that, to guarantee an \(\)-suboptimal policy for uniformly over any reward function, our algorithm requires at most \((d^{3}}{^{2}})\) episodes during the exploration phase where \(d\) is the dimension of the feature and \(H\) is the planning horizon. This improves the best known sample complexity for the reward-free RL in low-rank MDP (Cheng et al., 2023) by a factor of \((HdK)\), where \(K\) is the cardinality of the action space. In addition, as a complementary result, we show that using the learned representation from MORL improves the suboptimality gap bound in both offline and online downstream task.

## 2 Preliminary

Episodic MDP.We consider an episodic discrete-time Markov Decision Process (MDP), denoted by \(=(,,H,P,r)\), where \(\) is the state space, \(\) is the action space with cardinality \(K\), \(H\) is the finite episode length, \(P=\{P_{h}\}_{h=1}^{H}\) are the state transition probability distributions with \(P_{h}:()\), and \(r=\{r_{h}\}_{h=1}^{H}\) are the deterministic reward functions with \(r_{h}:\). Following prior work (Jiang et al., 2017; Sun et al., 2019), we assume that the initial state \(s_{1}\) is fixed for each episode. A policy \(\) is a collection of \(H\) functions \(\{_{h}:\}_{h[H]}\) where \(_{h}(s)\) is the action that the agent takes at state \(s\) and at the \(h\)-th step in the episode. Given a starting state \(s_{h}\), \(s_{h^{}}(P,)\) denotes a state sampled by executing policy \(\) under the transition model \(P\) for \(h^{}-h\) steps and \(_{(s_{h},a_{h})(P,)}[]\) denotes the expectation over states \(s_{h}(P,)\) and actions \(a_{h}\). Moreover, for each \(h[H]\), we define the value function under policy \(\) when starting from an arbitrary state \(s_{h}=s\) at the \(h\)-th time step as

\[V^{}_{h,P,r}(s)=_{(s_{h^{}},a_{h^{}})(P,)} _{h^{}=h}^{H}r_{h^{}}(s_{h^{}},a_{h^{}})|s_{h}=s .\]

We define the action-value function for a given state-action pair \((s,a)\) under policy \(\) at step \(h\) as

\[Q^{}_{h,P,r}(s,a)=_{(s_{h^{}},a_{h^{}})(P,)} _{h^{}=h}^{H}r_{h^{}}(s_{h^{}},a_{h^{}})|s _{h}=s,a_{h}=a.\]

Defining \((P_{h}f)(s,a)=_{s^{} P(|s,a)}[f(s^{})]\) for any function \(f:\), we write the Bellman equation associated with a policy \(\) as

\[Q^{}_{h,P,r}(s,a)=(r_{h}+P_{h}V^{}_{h+1,P,r})(s,a),\ \ V^{}_{h,P,r}(s)=Q^{}_{h,P,r}(s,_{h}(s)),\ \ V^{}_{H+1,P,r}(s)=0.\] (2.1)

Since the MDP begins with the same initial state \(s_{1}\), for simplicity, we use \(V^{}_{P,r}\) to denote \(V^{}_{1,P,r}(s_{1})\). Another useful concept is the notion of occupancy measure of a policy \(\) at time step \(h\) under transition kernel \(P\). Specifically, we use \(d^{}_{P_{h}}(s,a)\) to denote the marginal probability of encountering the state-action pair \((s,a)\) at time step \(h\) when executing policy \(\) under MDP with transition kernel \(P\). Finally, we denote \(()\) and \((,)\) as the uniform distribution over \(\) and \(\) respectively.

We study low-rank MDPs (Jiang et al., 2017; Agarwal et al., 2020) defined as follows.

**Definition 2.1** (Low-rank MDPs).: A transition kernel \(P^{*}_{h}:()\) admits a low-rank decomposition with dimension \(d\) if there exists two unknown embedding functions \(^{*}_{h}:^{d}\) and \(^{*}_{h}:^{d}\) such that for all \(s,s^{}\) and \(a\), \(P^{*}_{h}(s^{}\,|\,s,a)=^{*}_{h}(s,a),^{*}_{h}(s^{})\). Without loss of generality, we assume \(\|^{*}_{h}(s,a)\|_{2} 1\) for all \((s,a)\) and for any function \(g:\), \(\|^{*}_{h}(s)g(s)ds\|_{2}\).

We remark that the upper bounds on the norm of \(^{*}\) and \(^{*}\) are just for normalization. As the function class \(\) for \(^{*}\) can be a non-linear, flexible function class, the low-rank MDP generalizes prior works with linear representations (Jin et al., 2020; Hu et al., 2021) where it is assumed that the true representation \(^{*}\) is known to the agent a priori.

### Offline Multitask RL with Downstream Learning

In **offline multitask RL** upstream learning, the agent is provided with an offline dataset collected from \(T\) source tasks, where the reward functions \(\{r^{t}\}_{t[T]}\) are assumed to be known. Each task \(t[T]\) is associated with a low-rank MDP \(^{t}=(,,H,P^{t},r^{t})\). Here, all \(T\) tasks are identical except for (1) their true transition model \(P^{(*,t)}\), which admits a low-rank decomposition with dimension \(d\): \(P^{(*,t)}_{h}(s^{}_{h}\,|\,s_{h},a_{h})=^{*}_{h}(s_{h},a_{h}), ^{(*,t)}_{h}(s^{}_{h})\) for all \(h[H],t[T]\), and (2) their reward \(r^{t}_{h}\). While the tasks may differ in \(^{(*,t)}_{h}\) and \(r^{t}_{h}\), we emphasize that all tasks share the same feature function \(^{*}_{h}\). We have access to offline dataset \(=_{t[T],h[H]}^{(t)}_{h}\), where \(^{(t)}_{h}=\{(s^{(i,t)}_{h},a^{(i,t)}_{h},r^{(i,t)}_{h},s^{(i,t)}_{ h+1}\}_{i[n]}\) with \(s^{(i,t)}_{h+1} P^{(*,t)}_{h}(|\,s^{(i,t)}_{h},a^{(i,t)}_{h})\) and \(^{(t)}_{h}\) was collected using a _fixed behavior policy_\(^{b}_{t}\). In the upstream learning stage, the goal is to find a near-optimal policy and a near-accurate model for any task \(t[T]\) and any reward function \(\{r_{t}\}_{t[T]}\) through the use of offline dataset \(\) and provide a well-learned representation for the downstream task. In order to achieve bounded sample complexity in offline RL, we need additional coverage assumption on the behavior policy \(^{b}_{t}\). One common coverage assumption is the global coverage assumption (Antos et al., 2008; Munos and Szepesvari, 2008), which assumes the occupancy measure under the behavior policy \(^{b}_{t}\) globally covers the the occupancy measure under any possible policies, i.e., the concentrability ratio satisfies, \(_{,s,a}d^{^{*}}_{P^{(*,t)}_{h}}(s,a)/d^{^{b}_{t}}_{P^{(*,t)}_{h}}(s, a)<\). Instead, we make a partial coverage assumption and our suboptimality bound scales with the relative condition number (Agarwal et al., 2020, 2021) instead of the global concentrability ratio, where the former can be substantially smaller than the latter. Under this assumption, we want to compete against any comparator policy covered by the offline data. In Section 3.2, we define the partial coverage condition using relative condition number, which was previously used in the context of single-task offline RL (Uehara and Sun, 2021; Uehara et al., 2022) and generalize it to offline multitask setting.

In **downstream learning** stage, a new target task \(T+1\) with a low-rank transition kernel \(P^{(*,T+1)}\) and the same \(\), \(\) and \(H\) is assigned to the agent. The transition kernel \(P^{(*,T+1)}\) shares the same representation \(^{*}\) with the \(T\) upstream tasks, but has a task-distinct \(^{(*,T+1)}\). We consider three settings for downstream tasks - reward-free, offline and online RL, where the agent needs to use the representation function \(\) learned during the upstream stage to interact with the new task environment.

In the reward-free setting, firstly proposed in Jin et al. (2020), the agent first interacts with the new task environment without accessing the reward function in the exploration phase for up to \(K_{}\) episodes. Afterwards, it is provided with a reward function \(r=\{r_{h}\}_{h=1}^{H}\) and asked to output an \(\)-optimal policy \(\) in the planning phase. We define the sample complexity to be the number of episodes \(K_{}\) required in the exploration phase to output an \(\)- optimal policy \(\) in the planning phase for any given reward function \(r\).

In the offline and online setting the downstream task \(T+1\) is already assigned with an unknown reward function \(r^{T+1}\) and the goal is to find a near-optimal policy for the new task. The agent is expected to expedite its downstream learning through using the representation learned from the offline upstream task. In the online setting, it is allowed to interact with the new task environment and in the offline setting, it is instead provided with an offline dataset \(_{}=_{h[H]}_{h}\), where \(_{h}=\{(s_{h}^{},a_{h}^{},r_{h}^{},s_{h+1}^{})\}_{ [N_{}]}\) and \(_{}\) were collected using some behavior policy \(\).

## 3 Upstream Offline Multitask Representation Learning

In this section, we introduce our algorithm Multitask Offline Reinforcement Learning (MORL) designed for upstream offline multitask RL in low-rank MDPs and describe its theoretical properties.

### Algorithm Design

The details of the algorithm MORL is depicted in Algorithm 1. The agent passes all input offline data to estimate low-rank components \(_{h},_{h}^{(1)},,_{h}^{(T)}\) simultaneously via the Maximum Likelihood Estimation (MLE) oracle \(MLE(_{t[T]}_{h}^{(t)})\) on the joint distribution defined as follows:

\[(_{h},_{h}^{(1)},,_{h}^{(T )})=*{argmax}_{_{h},\\ _{h}^{(1)},,_{h}^{(T)}}_{i=1}^{n}_{t =1}^{T}(_{h}(s_{h}^{(i,t)},a_{h}^{(i,t)}),_{h}^{t} (s_{h+1}^{(i,t)}).\] (3.1)

The MLE oracle in (3.1) is the offline multitask counterpart to the celebrated MLE oracle in the online multitask RL (Agarwal et al., 2020; Cheng et al., 2022; Agarwal et al., 2023). The MLE oracle can be reasonably approximated in practice whenever optimizing over \(\) and \(\) is feasible through proper parameterization such as by neural network. For each task \(t\), we obtain the estimatedtransition kernel \(^{(t)}\) at each step \(h\) using the learned embeddings \(_{h}\), \(_{h}^{(t)}\):

\[_{h}^{(t)}(s^{} s,a)=_{h}(s,a), _{h}^{(t)}(s^{}).\] (3.2)

Using the representation estimator \(_{h}\), we set the empirical covariance matrix \(_{h,}^{(t)}\) for task \(t\) as

\[_{h,}^{(t)}=_{i=1}^{n}_{h}(s_{ h}^{(i,t)},a_{h}^{(i,t)})_{h}(s_{h}^{(i,t)},a_{h}^{(i,t)})^{ }+ I.\] (3.3)

Using both \(_{h}\) and \(_{h,}^{(t)}\), we construct a lower confidence bound penalty term as follows:

\[_{h}^{(t)}(s_{h},a_{h})=\{\|_{h}(s_{h},a_{h})\|_{(_{h,}^{(t)})^{-1}},1\},\] (3.4)

where \(\) is a pre-determined parameter.

Finally, for each task \(t\), with the learned model \(^{(t)}\) and the reward \(r^{t}-^{(t)}\), we do planning to get policy \(_{t}\).

### Theoretical Result on Upstream Task

To facilitate the model selection task using the joint MLE oracle in (3.1), we posit a realizability assumption which is standard in low-rank MDP literature (Agarwal et al., 2020; Cheng et al., 2022).

**Assumption 3.1** (Realizability).: A learning agent has access to a model class \(\{,\}\) that contains the true model, i.e., for any \(h[H],t[T]\), the embeddings \(_{h}^{*}\), \(^{(*,t)}\). For normalization, we assume that for any \(\), \(\|(s,a)\|_{2} 1\) and for any \(\) and any function \(g::\), \(\|_{h}(s)g(s)ds\|_{2}\).

For simplicity, we assume that the cardinality of the function classes \(\) and \(\) are finite.

Next, we define the multitask relative condition number \(C^{*}\), which is a natural extension of the standard relative condition number (Agarwal et al., 2020, 2021; Uehara et al., 2022).

**Definition 3.2** (Multi-task relative condition number).: For task \(t\) and time step \(h\), we define \(C^{*}_{t,h}(_{t},_{t}^{b})\) as the relative condition number under \(_{h}^{*}\):

\[C^{*}_{t,h}(_{t},_{t}^{b}):=_{x^{d}} _{(s_{h},a_{h})(P^{(*,t)},_{t})}[_{h}^{*}(s_{h},a_{h}) _{h}^{*}(s_{h},a_{h})^{}]x}{x^{}_{(s_{h},a_{h})(P^{ (*,t)},_{t}^{b})}[_{h}^{*}(s_{h},a_{h})_{h}^{*}(s_{h},a_{h})^{}] x}.\] (3.5)

We define \(C^{*}_{t}:=_{h[H]}C^{*}_{t,h}(_{t},_{t}^{b})\) and \(C^{*}:=_{t[T]}C^{*}_{t}\).

Intuitively, \(C^{*}_{t,h}(_{t},_{t}^{b})\) defined in (3.5) measures the deviation between a comparator policy \(_{t}\) and the behavior policy \(_{t}^{b}\) at time step \(h\). When tabular MDP is considered (i.e., \(_{h}^{*}\) is a one-hot encoding vector), this relative condition number reduces to the density ratio based single-policy concentrability coefficient, \(C^{*}_{t,h,}(_{t},_{t}^{b})=_{s,a}d^{_{t}}_{P_{h}^{(*,t)}} (s,a)/d^{_{t}^{b}(*,t)}_{P_{h}^{(*,t)}}(s,a)\)(Chen and Jiang, 2022). The relative condition number \(C^{*}_{t,h}(_{t},_{t}^{b})\) is always bounded by the concentrability coefficient (Uehara and Sun, 2021). In addition, the relative condition number is computed under the averaging over the state, actions, which could be much smaller than \(_{s,a}d^{_{t}}_{P_{h}^{(*,t)}}(s,a)/d^{_{t}^{b}}_{P_{h}^{(*,t)}}(s,a)\) since the latter will be very large as long as any \(s,a\) pair gives large \(d^{_{t}}_{P_{h}^{(*,t)}}(s,a)/d^{_{t}^{b}}_{P_{h}^{(*,t)}}(s,a)\) ratio. Therefore, our \(C^{*}_{t}\) could be much smaller compared to the concentrability coefficient, especially in large-scale MDPs (e.g. continuous state space). Moreover, in our definition of relative condition number, we use the unknown true representation \(^{*}\). Finally, we generalize single task relative condition number to multitask setting by defining \(C^{*}\), by simply taking the maximum over the single-task relative condition numbers.

Now we describe our main theorem.

**Theorem 3.3**.:
1. _Under Assumption_ 3.1_, with probability at least_ \(1-\)_, for any step_ \(h[H]\)_, we have_ \[_{t=1}^{T}_{,a_{h})}}{{ (P^{(*,t)},_{t})}}}[\|_{h}^{(t)}(\,|\,s_{h},a_ {h})-P_{h}^{(*,t)}(\,|\,s_{h},a_{h})\|_{TV}]nH/)}{nT}},\] (3.6) _where_ \(,^{(1)},,^{(T)}\) _be the output of Algorithm_ 1_._
2. _In addition, in Algorithm_ 1_, if we set_ \(=+ d}\)_,_ \(=cd(||||^{T}nH/)\) _with_ \(_{n}:=nH/)}{n}\) _and_ \(c\) _being a constant, where we assume that_ \(:=_{t}_{s,a}(1/_{t}^{b}(a\,|\,s))<\)_, then under Assumption_ 3.1_, with probability at least_ \(1-\)_, we have_ \[\!_{t=1}^{T}\!\![V_{P^{(*,t)},r^{t}}^{_{t}} \!-\!V_{P^{(*,t)},r^{t}}^{_{t}}]\!\!\! dH }{n}}\!+\!2dH^{2}}{n}}\!+\! H^ {2}_{n}}{T}}\!+\!}\!+\!2H}{T}},\] (3.7) _where_ \(\{_{t}\}_{t[T]}\) _is the output of Algorithm_ 1_._

Theorem 3.3 (a) shows a potential benefit of a joint learning of the source-task transition kernels, as compared to independent learning, as measured by the task-average TV distance (defined in the LHS of Equation (3.6)). In particular, to obtain an \(\)-suboptimal transition kernels, it suffices for the joint learning to use \((}+})\) samples per task, yet for the independent learning to use \((}+})\) samples per task. This benefit naturally comes from the inductive bias that all the source tasks share a representation in \(\) which can be learned more accurately with the aggregated data pooled from all the source tasks. Equation (3.7) in Theorem 3.3 further shows that, for all tasks on average, we can uniformly compete with any set of comparator policies \(\{_{t}\}_{t[T]}\) satisfying the partial coverage through \(C^{*}<\). In particular, if the optimal policy \(_{t}^{*}\) is covered by the offline data for all \(t[T]\), then the output \(\{_{t}\}_{t[T]}\) of Algorithm 1 is able to compete against it on average as well.

_Remark 3.4_.: We note that in order for the bound to hold in Theorem 3.3, Algorithm 1 requires the knowledge of \(\) as it is required to set the value of \(\) which is used in the lower confidence bound penalty term defined in (3.4). However, in practice, we expect that \(\) can be treated as a hyperparameter that might be tuned using grid search.

**Proof outline.** Here, we highlight the key steps for the proof of Theorem 3.3. The detailed proof is deferred to Appendix D. Using offline multitask MLE lemma (Lemma F.3) and one-step back lemma (Lemma G.1), we first develop a new upper bound on model estimation error for each task which encapsulates the benefit of joint offline MLE model estimation over single-task offline learning. We then use the following lemma to show near pessimism in the average sense.

**Lemma 3.5**.: _For any policy \(_{t}\) and reward \(r^{t}\), we have, with probability \(1-\)_

\[_{t=1}^{T}[V_{^{(t)},r^{t}- }^{(t)}}^{_{t}}-V_{P^{(*,t)},r^{t}}^{_{t}}] H/T},\]

_where \(_{n}:=nH/)}{n}\)._

The proof of Lemma 3.5 relies on simulation lemma and a concentration argument for the penalty term defined in (3.4). Finally, to obtain a result only depending on the relative condition number using the true representation \(^{*}\) but not the learned feature \(\), we translate the penalty defined with \(\) to the potential function \(\|^{*}_{h}(s_{h},a_{h})\|_{(_{h,_{t}^{*},^{*}})^{-1}}\) where \(_{h,_{t}^{*},^{*}}=n_{(s_{h},a_{h})(P^{(*,t)},_{t }^{*})}[^{*}^{*}^{}]+ I\) using a one-step back inequality (Lemma G.1) and a distribution shift lemma (Lemma O.2).

## 4 Downstream RL: Reward-free Exploration, Offline RL and Online RL

### Relationship between upstream and downstream MDPs

In order for us to theoretically study the downstream RL tasks where we would use the learned feature \(\) from the upstream tasks, first we need to make certain connection between the upstreamand downstream MDPs. Naturally, we would have to resort to some assumptions on the transition kernels to make such connections. Next, we describe these assumptions, which we largely adopt from Cheng et al. (2022).

**Assumption 4.1**.: We make the following assumptions

1. For each upstream source task \(t\) with transition kernel \(P^{(*,t)}\), the behavior policy \(_{t}^{b}\) is such that \(_{s}_{h}^{(_{t}^{b},t)}(s)\), where \(_{h}^{(_{t}^{b},t)}():\) is the marginalized state occupancy measure over \(\) using policy \(_{t}^{b}\) at time step \(h\).
2. The state space \(\) is compact and has finite measure \(1/\), and the induced uniform probability density function is \(f(s)=\), \(s\).
3. For any two models \(P^{1}(s^{}|s,a)=^{1}(s,a),^{1}(s^{})\) and \(P^{2}(s^{}|s,a)=^{2}(s,a),^{2}(s^{})\) in the model class \(\), we have, \[\|P^{1}(|s,a)-P^{2}(|s,a)\|_{} C_{R}_{(s,a) (,)}\|P^{1}(|s,a)-P^{2}(|s,a) \|_{},\] for all \((s,a)\) and \(h[H]\) where \(C_{R}\) is an absolute constant.
4. The transition kernel of task \(T+1\), \(P^{(*,T+1)}\) can be \(\)-approximated by a linear combination of the \(T\) source upstream tasks, i.e. there exist \(T\) unknown coefficients \(c_{1},,c_{T} 0\) such that \(_{t=1}^{T}c_{t} C_{L}\) and \( 0\) such that for all \((s,a)\) and \(h[H]\), we have \[\|P^{(*,T+1)}(|s,a)-_{t=1}^{T}c_{t}P^{(*,t)}(|s,a)\|_{} .\] Here, \(\) is called the linear combination misspecification.

The first point in Assumption 4.1 ensures that the behavior policy \(_{t}^{b}\) for each task \(t\) can reach any state in \(}\) at any time step with a positive probability, an assumption that is previously used in Yin et al. (2021). Compared to Cheng et al. (2022), which assumes the existence of a policy with reachability property in each of the upstream online tasks, ours assumes reachability property for the behavior policies used to collect the upstream offline dataset.

The third point in Assumption 4.1 ensures that for each source task \(t\), the point-wise TV error between the learned estimated transition kernel \(^{(t)}\) and the true transition kernel \(P^{(*,t)}\) is bounded by the population-level TV error. This assumption is necessary to transfer the MLE error from the upstream source tasks to the downstream target task.

Finally, the fourth point in Assumption 4.1 connects the upstream source tasks with the downstream target task by assuming that the transition kernel of the target task \(P^{(*,T+1)}\) can be approximated by a linear combination of transition kernels of \(T\) upstream source tasks.

The precision of the feature estimation in the upstream has a significant impact on the downstream task's performance because the downstream task utilizes the estimated feature from the upstream. We use the following notion of \(\)-approximate linear MDP to provide a guarantee for the estimated feature.

**Definition 4.2** (\(\)-approximate linear MDP Jin et al. (2020); Cheng et al. (2022)).: For any \(>0\), we say that MDP \(=(,,,P,r)\) is an \(\)-approximate linear MDP with a feature map \(_{h}::^{d}\), if for any \(h[H]\), there exist \(d\) unknown (signed) measures \(_{h}=(_{h}^{(1)},,_{h}^{(d)})\) over \(\) such that for any \((s,a)\), we have

\[\|P_{h}(|s,a)-_{h}(s,a),_{h}()\|_{}.\] (4.1)

Any \(\) satisfying (4.1) is called an \(\)-approximate feature map of \(\).

The next lemma shows that the learned feature \(\) from the upstream offline tasks can approximate the true feature in the new downstream task.

**Lemma 4.3**.: _Under Assumption 4.1, the output \(\) of Algorithm 1 is a \(_{}\)-approximate feature for MDP \(^{T+1}\) where \(_{}=+C_{R}}{}nH/)}{n}}\), i.e. there exist a time-dependent unknown (signed) measure \(^{*}\) over \(\) such that for any \((s,a)\), we have_

\[\|P_{h}^{(*,T+1)}(|s,a)-_{h}(s,a),^{*}_{ h}()\|_{}_{}.\]

_Furthermore, for any \(g:\), we have \(\|^{*}_{h}(s)g(s)ds\|_{2} C_{L}\)._

### Downstream Reward-Free RL

Our goal in this part is to investigate the statistical efficiency of reward-free RL in low-rank MDP while having access to offline datasets from the upstream tasks.

Our algorithm for the reward-free setting is presented in Algorithm 2 (exploration phase) and Algorithm 3 (planning phase) which is built on the procedure of optimistic learning as Wang et al. (2020); Zhang et al. (2021). While having similar design principle as in Wang et al. (2020), our algorithm differs from them due to the misspecification of representation from the upstream task. Thus, the upstream learning error affects the learning accuracy and downstream suboptimality gap and we need to account for that in our analysis. Another difference with Wang et al. (2020) is that in the exploration phase, like Chen et al. (2022), we construct more aggressive reward function to avoid overly-conservative exploration, which removes the extra dependency of sample complexity on episode length \(H\). Below, we provide our main theorem for downstream reward-free RL task and defer the proof to Appendix I.

**Theorem 4.4**.: _Under Assumption 4.1, after collecting \(K_{}\) trajectories during the exploration phase in Algorithm 2, with probability at least \(1-\), the output of Algorithm 3, policy \(\) satisfies_

\[_{s_{1}}[V_{1}^{*}(s_{1},r)-V_{1}^{}(s_{1},r)] c^{ }H^{4}(dK_{}H/)/K_{}}+6H^{2} _{}.\] (4.2)

_If the linear combination misspecification error \(\) in Assumption 4.1 satisfies \((/K_{}})\) and the number of trajectories in the offline dataset for each upstream task is at least \((TK_{}/d^{3})\), then, provided \(K_{}\) is at least \(O(H^{4}d^{3}(dH^{-1}^{-1})/^{2})\), with probability \(1-\), the policy \(\) will be an \(\)-optimal policy for any given reward during the planning phase._

We compare the above result with other algorithms developed for the reward-free RL under low-rank MDPs and summarize the comparison in Table 1. FLAMBE (Agarwal et al., 2020) achieves a sample complexity of \(d^{7}K^{3}}{^{10}}\) whereas MOFFLE (Modi et al., 2024) achieves a sample complexity of \(d^{11}K^{14}}{\{^{2},,^{ 5}\}}\), where \(\) is a reachability probability to all states. More recently, Cheng et al. (2023) proposed RAFFLE which has the best-known sample complexity of \((d^{4}K}{^{2}})\).2 Cheng et al. (2023) further shows that the dependence of sample complexity on action space cardinality \(K\) is unavoidable when performing reward-free exploration in low-rank MDPs from which they conclude that it is strictly harder to find a near-optimal policy under low-rank MDPs than under linear MDPs. However, as we see from Theorem 4.4, by using estimated representation from the upstream offline datasets, we can avoid this dependence of sample complexity on \(K\) and overall improve the sample complexity by \((HdK)\) compared to that of RAFFLE. Moreover, compared to standard linear MDP, where the true representation \(\{_{h}\}_{h=1}^{H}\) is known (Jin et al., 2020), the suboptimality gap in (4.2) contains an additional term \(H^{2}_{}\) which is due to the upstream misspecification error \(_{}\). When \(_{}\) is small enough, our resulting sample complexity of \((d^{3}}{^{2}})\) matches the reward-free exploration sample complexity for linear mixture MDPs (Chen et al., 2022), which is worse off by only \((d)\) compared to the best known sample complexity of \((d^{2}}{^{2}})\)(Hu et al., 2022) for linear MDP.

  Algorithm & Sample Complexity & Task \\  FLAMBE (Agarwal et al., 2020) & \(d^{7}K^{3}}{^{10}}\) & Single task \\  MOFFLE (Modi et al., 2024) & \(d^{3}K^{4}}{\{^{2},,^{ 5}\}}\) & Single task \\  RAFFLE (Cheng et al., 2023) & \(d^{3}K}{^{2}}\) & Single task \\  This work (Algorithm 2 and Algorithm 3) & \(d^{3}}{^{2}}\) & Multi-task \\  

Table 1: Sample complexities of different approaches to learning an \(\)-optimal policy for the reward-free RL setting with low-rank MDPs.

### Downstream Offline and Online RL

For completeness, we also consider downstream offline and online RL which was previously studied in Cheng et al. (2022) to show the effectiveness of our offline representation. In both cases, we assume that the reward function \(r^{T+1}\) in the downstream task \(T+1\) is linear with respect to the unknown feature \(^{*}:^{d}\). We emphasize that, unlike Cheng et al. (2022), we assume the reward function \(r^{T+1}\) is unknown.

Offline RL.For downstream offline RL task, similar to Cheng et al. (2022), we use standard pessimistic value iteration algorithm (PEVI) (Jin et al., 2021) with approximate feature learned from upstream task. We make the following data-coverage type of assumption which is standard in the study of offline RL (Xie et al., 2021; Wang et al., 2021; Yin et al., 2021). Moreover, this assumption has been shown to be necessary for sample efficient offline RL for tabular and linear MDPs (Wang et al., 2021; Yin and Wang, 2021).

Assumption 4.5 (Feature coverage).There exists an absolute constant \(_{}\) such that for all \(h[H]\) and \(_{h}_{h}\), \(_{}(_{}[_{h}(s_{h},a_{h})_{h}(s_{h},a_{h})^{ }|s_{1}=s])_{}\).

Next, we provide our result for the downstream offline RL task and defer the proof to Appendix K.

**Theorem 4.6**.: _Under Assumption 4.1, setting \(_{d}=1\), \(=O(Hd+H}}_{})\), where \(=(HdN_{}(_{},1)/)\), with probability at least \(1-\), the suboptimality gap of Algorithm 4 is at most_

\[V^{^{*}}_{P^{(*,T+1)},r}(s)-V^{}_{P^{(*,T+1)},r}(s)  2H^{2}_{}+2_{h=1}^{H}_{^{*}} \|_{h}(s_{h},a_{h})\|_{^{-1}_{h}}|s_{1}=s.\] (4.3)

_Additionally if Assumption 4.5 holds, and the sample size satisfies \(N_{} 40/_{}(4dH/)\), then with probability \(1-\), we have,_

\[V^{^{*}}_{P^{(*,T+1)},r}(s)-V^{}_{P^{(*,T+1)},r}(s)  O_{}^{-1/2}H^{2}d} (_{},1)/)}{N_{}}}+_{}^{-1/2}H^{2}d^{1/ 2}_{}.\] (4.4)

Online RL.For downstream online RL task, where the agent is allowed to interact with the new task MDP \(^{T+1}\) for policy optimization, similar to Cheng et al. (2022), we use standard LSVI-UCB algorithm (Jin et al., 2020) with approximate feature. We next provide our result for downstream online RL task and defer the proof to Appendix M.

**Theorem 4.7**.: _Let \(\) be the uniform mixture of \(^{1},,^{N_{}}\) in Algorithm 5. Under Assumption 4.1, setting \(=1\), \(_{n}=O(Hd()}+H_{}+C_{L})\), where \(_{n}=(Hdn(_{},1)/)\), with probability \(1-\), the suboptimality gap of Algorithm 5 satisfies_

\[V^{*}_{P^{(*,T+1)},r}-V^{}_{P^{(*,T+1)},r} (H^{2}d^{3/2}N_{}^{-1/2}+H^{2}d_{}).\]

## 5 Related Work

Offline Reinforcement Learning.Offline RL (Ernst et al., 2005; Riedmiller, 2005; Lange et al., 2012; Levine et al., 2020) studies the problem of learning a policy from a static dataset without interacting with the environment. The key challenge in offline RL is the insufficient coverage of the dataset, due to the lack of exploration (Levine et al., 2020; Liu et al., 2020). One prevalent approach to address this challenge is the pessimism principle to penalize the estimated value of the under-covered state-action pairs. There have been extensive studies on incorporating pessimism into the development of different approaches in single-task offline RL, including model-based approach (Rashidinejad et al., 2021; Uehara and Sun, 2022; Jin et al., 2021; Yu et al., 2020; Xie et al., 2021; Uehara et al., 2022; Yin et al., 2022), model-free approaches (Kumar et al., 2020; Wu et al., 2021; Bai et al., 2022; Ghasemipour et al., 2022; Yan et al., 2023; Nguyen-Tang et al., 2022, 2023; Nguyen-Tang and Arora, 2023), and policy-based approach (Rezaeifar et al., 2022; Xie et al., 2021; Zanette et al., 2021; Nguyen-Tang and Arora, 2024, 2024). Our algorithm for upstream offline multitask RL is inspired by the uncertainty-based pessimism methods in single-task offline RL.

Low-rank MDPs.Agarwal et al. (2020) initiates the study of low-rank MDPs. Uehara et al. (2022) proposed model-based algorithms for both online and offline RL, while Modi et al. (2024) put forwarda model-free algorithm for low-rank MDPs. Moreover, Du et al. (2019); Misra et al. (2020); Zhang et al. (2022) studied block MDPs, which is a special case of low-rank MDPs.

**Offline Data Sharing in RL.** There has been several empirical works that investigated the benefits of using offline datasets from multiple tasks to accelerate downstream learning (Eysenbach et al., 2020; Kalashnikov et al., 2021; Mitchell et al., 2021; Yu et al., 2021; Yoo et al., 2022). Yu et al. (2021) show that selectively sharing data between tasks can be helpful for offline multitask learning. For instance, earlier studies have investigated the development of data sharing strategies through human domain knowledge (Kalashnikov et al., 2021), inverse RL (Reddy et al., 2019; Eysenbach et al., 2020; Li et al., 2020), and estimated Q-values (Yu et al., 2021). More recently, Xu et al. (2023) uses offline dataset from diverse tasks to perform offline multitask pretraining of a world model which is then finetuned on a downstream target task. Hu et al. (2023) proposes a provably efficient self-supervised offline data-sharing algorithm for linear MDP. However, they assume access to reward-free data.

**Comparison to Cheng et al. (2022).** Closest to our work is Cheng et al. (2022) who studied online multitask RL. Their proposed REFUEL algorithm combines design principles from FLAMBE (Agarwal et al., 2020) and REP-UCB (Uehara et al., 2022) and performs joint MLE based model learning while collecting data in an online manner. On the contrary, MORL first performs joint MLE based model learning using the offline dataset collected for each source task and then upon constructing penalty terms for each task, performs planning using pessimistic reward functions. While both works rely on an MLE oracle, first proposed in Agarwal et al. (2020) for single-task RL, our proposed offline multitask MLE lemma (Lemma F.3) conveys fundamentally very different ideas compared to its online counterpart, Lemma 3 in Cheng et al. (2022). Lemma 3 in Cheng et al. (2022) says that when exploration policies for each upstream online source task is uniformly chosen, the summation of the estimation error of transition probability can be bounded with high probability. On the contrary, our Lemma F.3 states that when the offline datasets for each of the upstream offline source tasks are collected using respective behavior policies, the summation of the estimation error is bounded with high probability. For the downstream task, Cheng et al. (2022) studies only offline and online RL task, whereas our primary contribution in this part is in the study of downstream reward-free RL which has not been previously studied in the context of multitask representation learning. For completeness, we provide results in downstream offline and online RL setting as a complementary result. Moreover, unlike us, Cheng et al. (2022) assumes that the reward-function is known in the downstream task, which is a fairly strong assumption. Somewhat in a contrived manner, in Cheng et al. (2022), the reward-function is further assumed to be general and not necessarily linear in the feature which complicates their downstream analysis. On the contrary, we assume that the reward function is linear with respect to the feature. Finally, Cheng et al. (2022) assumes that for each episode in any task MDP, the sum of reward is normalized to be within \(\). We do not make this assumption for fair comparison to literature on reward-free RL for low-rank MDPs.

**Comparison to Concurrent Work.** In a concurrent work, Bose et al. (2024) studies representational transfer in offline low-rank RL. In the upstream task, similar to ours, Bose et al. (2024) also uses an offline MLE oracle. Compared to our Theorem 3.3, where we provide bound for average accuracy of the estimated transition kernels of the upstream source tasks, Bose et al. (2024) provides bound for the sum of the point-wise errors in the transition dynamics averaged over the points in the source datasets. To bound the representational transfer error in the downstream target task, they introduce a notion of neighborhood occupancy density. Moreover, to connect the upstream tasks and the downstream target task, they make a pointwise linear span assumption from Agarwal et al. (2023). Finally, for the downstream target task, they only consider offline setting, whereas our primary focus and contribution is in the study of reward-free setting in the downstream task.

## 6 Conclusion

In this paper, we theoretically study multitask RL in the offline setting. We show that offline multitask representation learning is provably more sample efficient than learning each task individually. We further show the benefit of employing the learned representation from the upstream to learn a near-optimal policy of a new downstream task, in reward-free, offline and online setting, that shares the same representation. We believe our work will open up many promising directions for future work, for example, studying the general function class representation learning in offline multitask setting.