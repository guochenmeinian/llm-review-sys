# Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems

Sihan Zeng

Electrical and Computer Engineering

Georgia Institute of Technology

Atlanta, GA 30318

szeng30@gatech.edu

&Thinh T. Doan

Electrical and Computer Engineering

Virginia Tech

Blacksburg, VA 24061

thinhdoan@vt.edu

&Justin Romberg

Electrical and Computer Engineering

Georgia Institute of Technology

Atlanta, GA 30318

jrom@ece.gatech.edu

###### Abstract

The aim of this paper is to improve the understanding of the optimization landscape for policy optimization problems in reinforcement learning. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks. In addition, we show that the optimization objective as a function of the policy parameter and reward satisfies a stronger "equiconnectedness" property. To our best knowledge, these are novel and previously unknown discoveries.

We present an application of the connectedness of these superlevel sets to the derivation of minimax theorems for robust reinforcement learning. We show that any minimax optimization program which is convex on one side and is equiconnected on the other side observes the minimax equality (i.e. has a Nash equilibrium). We find that this exact structure is exhibited by an interesting class of robust reinforcement learning problems under an adversarial reward attack, and the validity of its minimax equality immediately follows. This is the first time such a result is established in the literature.

## 1 Introduction

Policy optimization problems in reinforcement learning (RL) are usually formulated as the maximization of a non-concave objective function over a convex constraint set. Such non-convex programs are generally difficult to solve globally, as gradient-based optimization algorithms can be trapped in sub-optimal first-order stationary points. Interestingly, recent advances in RL theory (Fazel et al., 2018; Agarwal et al., 2021; Mei et al., 2020) have discovered a "gradient domination" structure in the optimization landscape, which qualitatively means that every stationary point of the objective function is globally optimal. An important consequence of this condition is that any first-order algorithm that converges to a stationary point is guaranteed to find the global optimality.

In this work, our aim is to enhance the understanding of the optimization landscape in RL beyond the gradient domination condition. Inspired by Mohammadi et al. (2021); Fatkhullin and Polyak (2021) that discuss properties of the sublevel set for the linear-quadratic regulator (LQR), we studythe superlevel set of the policy optimization objective under a Markov decision process (MDP) framework and prove that it is always connected.

As an immediate consequence, we show that any minimax optimization program which is convex on one side and is an RL objective on the other side observes the minimax equality. We apply this result to derive an interesting and previously unknown minimax theorem for robust RL. We also note that it is unclear at the moment, but certainly possible, that the result on connected superlevel sets may be exploited to design more efficient and reliable policy optimization algorithms in the future.

### Main Contribution

Our first contribution in this work is to show that the superlevel set of the policy optimization problem in RL is always connected under a tabular policy representation. We then extend this result to the deep reinforcement learning setting, where the policy is represented by a class of over-parameterized neural networks. We show that the superlevel set of the underlying objective function with respect to the policy parameters (i.e. weights of the neural networks) is connected at all levels. We further prove that the policy optimization objective as a function of the policy parameter and reward is "equiconnected", which is a stronger result that we will define and introduce later in the paper. To the best of our knowledge, our paper is the first to rigorously investigate the connectedness of the superlevel sets for the MDP policy optimization program, both in the tabular case and with a neural network policy class.

As a downstream application, we discuss how our main results can be used to derive a minimax theorem for a class of robust RL problems. We consider the scenario where an adversary strategically modifies the reward function to trick the learning agent. Aware of the attack, the learning agent defends against the poisoned reward by solving a minimax optimization program. The formulation for this problem is proposed and considered in Banihashem et al. (2021); Rakhsha et al. (2020). However, as a fundamental question, the validity of the minimax theorem (or equivalently, the existence of a Nash equilibrium) is still unknown. We fill in this gap by establishing the minimax theorem as a simple consequence of the equiconnectedness of the policy optimization objective.

### Related Works

Our paper is closely connected to the existing works that study the structure of policy optimization problems in RL, especially those on the gradient domination condition. Our result also relates to the literature on minimax optimization for various function classes and robust RL. We discuss the recent advances in these domains to give context to our contributions.

**Gradient Domination Condition.** The policy optimization problem in RL is non-convex but obeys the special "gradient domination" structure, which has been widely used as a tool to show the convergence of various gradient-based algorithms to the globally optimal policy Agarwal et al. (2020); Mei et al. (2020); Bhandari and Russo (2021); Zeng et al. (2021); Xiao (2022). In the settings of LQR Fazel et al. (2018); Yang et al. (2019) and entropy-regularized MDP Mei et al. (2020); Cen et al. (2022); Zeng et al. (2022), the gradient domination structure can be mathematically described by the Polyak-Lojasiewicz (PL) condition, which bears a resemblance to strong convexity but does not even imply convexity. It is known that functions observing this condition can be optimized globally and efficiently by (stochastic) optimization algorithms Karimi et al. (2016); Zeng et al. (2021); Gower et al. (2021). When the policy optimization problem under a standard, non-regularized MDP is considered, the gradient domination structure is weaker than the PL condition but still takes the form of upper bounding a global optimality gap by a measure of the magnitude of the gradient Bhandari and Russo (2019); Agarwal et al. (2020); Agarwal et al. (2021). In all scenarios, the gradient domination structure prevents any stationary point from being sub-optimal.

It may be tempting to think that the gradient domination condition and the connectedness of the superlevel sets are strongly connected notions or may even imply one another. For 1-dimensional function (\(f:^{n}\) with \(n=1\)), it is easy to verify that the gradient domination condition necessarily implies the connectedness of the superlevel sets. However, when \(n 2\) this is no longer true. In general, the gradient domination condition neither implies nor is implied by the connectedness of superlevel sets, which we illustrate with examples in Section 1.3. These two structural properties are distinct concepts that characterize the optimization landscape from different angles. This observation precludes the possibility of deriving the connectedness of the superlevelsets in RL simply from the existing results on the gradient domination condition, and suggests that a tailored analysis is required.

**Minimax Optimization & Minimax Theorems.** Consider a function \(f:\) on convex sets \(,\). In general, the minimax inequality always holds

\[_{x}_{y}f(x,y)_{y} _{x}f(x,y).\]

The seminal work Neumann (1928) shows that this inequality holds as an equality for matrix games where \(^{m},^{n}\) are probability simplexes and we have \(f(x,y)=x^{}Ay\) given a payoff matrix \(A^{m n}\). The result later gets generalized to the setting where \(,\) are compact sets, \(f(x,)\) is quasi-convex for all \(x\), and \(f(,y)\) is quasi-concave for all \(y\)(Fan, 1953; Sion, 1958). Much more recently, Yang et al. (2020) establishes the minimax equality when \(f\) satisfies the two-sided PL condition. For arbitrary functions \(f\), the minimax equality need not be valid.

The validity of the minimax equality is essentially equivalent to the existence of a global Nash equilibrium \((x^{},y^{})\) such that

\[f(x,y^{}) f(x^{},y^{}) f(x^{},y), x ,y.\]

The Nash equilibrium \((x^{},y^{})\) is a point where neither player can improve their objective function value by changing its strategy. In general nonconvex-nonconcave settings where the global Nash equilibrium may not exist, alternative approximate local/global optimality notions are considered (Daskalakis and Panageas, 2018; Nouiehed et al., 2019; Adolphs et al., 2019; Jin et al., 2020).

**Robust Reinforcement Learning.** Robust RL studies finding the optimal policy in the worst-case scenario under environment uncertainty and/or possible adversarial attacks. Various robust RL models have been considered in the existing literature, such as: 1) the learning agent operates under uncertainty in the transition probability kernel (Goyal and Grand-Clement, 2022; Li et al., 2022; Panaganti and Kalathil, 2022; Wang et al., 2023), 2) an adversary exists and plays a two-player zero-sum Markov game against the learning agent (Pinto et al., 2017; Tessler et al., 2019), 3) the adversary does not affect the state transition but may manipulate the state observation (Havens et al., 2018; Zhang et al., 2020), 4) there is uncertainty or attack only on the reward (Wang et al., 2020; Banihashem et al., 2021; Sarkar et al., 2022), 5) the learning agent defends against attacks from a population of adversaries rather than a single one (Vinitsky et al., 2020). A particular attack and defense model considered later in our paper is adapted from Banihashem et al. (2021).

**Other Works on Connected Level Sets in Machine Learning.** Last but not least, we note that our paper is related to the works that study the connectedness of the sublevel sets for the LQR optimization problem (Fatkullin and Polyak, 2021) and for deep supervised learning under a regression loss (Nguyen, 2019). The neural network architecture considered in our paper is inspired by and similar to the one in Nguyen (2019). However, our result and analysis on deep RL are novel and significantly more challenging to establish, since 1) the underlying loss function in Nguyen (2019) is convex, while ours is a non-convex policy optimization objective, 2) the analysis of Nguyen (2019) relies critically on the assumption that the activation functions are uniquely invertible, while we use a non-uniquely invertible softmax activation function to generate policies within the probability simplex.

### Connection between Gradient Domination and Connected Superlevel Sets

We loosely use the term "gradient domination" to indicate that a differentiable function does not have any sub-optimal stationary points. In this section, we use two examples to show that the gradient domination condition in general does not imply or get implied by the connectedness of the superlevel sets. The first example is a function that observes the gradient domination condition but has a disconnected set of maximizers (which implies that the superlevel is not always connected).

Consider \(f:[-4,4][-2,0]\)

\[f(x,y)=\{f_{1}(x,y)=-(x-1)^{3}+3(x-1)-y^{2}-2y-0.02(y+10 )^{2}(10-x^{2}),x 0\\ f_{2}(x,y)=-(-x-1)^{3}+3(-x-1)-y^{2}-2y-0.02(y+10)^{2}(10-x^{2}),.\]

It is obvious that the function is symmetric along the line \(x=0\) and that \(f_{1}(0,y)=f_{2}(0,y)\) for all \(y[-2,0]\). Computing the derivatives of \(f_{1}\) and \(f_{2}\) with respect to \(x\), we have

\[_{x}f_{1}(x,y)=-3(x-1)^{2}+3+0.04x(y+10)^{2},\] \[_{x}f_{2}(x,y)=3(x+1)^{2}-3+0.04x(y+10)^{2}.\]We can again verify \(_{x}f_{1}(0,y)=_{x}f_{2}(0,y)\) for all \(y\), which implies that the function \(f\) is everywhere continuous and differentiable. Visualization of \(f\) in Fig. 1 along with simple calculation (solving the system of equations \(_{x}f(x,y)=0\) and \(_{y}f(x,y)=0\)) show that there are only two stationary points of \(f\) on \([-4,4][-2,0]\). The two stationary points are \((3.05,-1.12)\) and \((-3.05,-1.12)\), and they are both global maximizers on this domain, which means that the gradient domination condition is observed. However, the set of maximizers \(\{(3.05,-1.12),(-3.05,-1.12)\}\) is clearly disconnected.

We next present a function that has connected superlevel sets at all level but does not observe the gradient domination condition (i.e. has sub-optimal stationary points).

Consider \(g:^{2}\) defined as

\[g(x,y)=-(x^{2}+y^{2})^{2}+4(x^{2}+y^{2}).\]

This is a volcano-shaped function, which we visualize in Fig. 1. It is obvious the superlevel set \(\{(x,y):g(x,y)\}\) is always either a 2D circle (convex set) or a donut-shaped connected set depending on the choice of \(\). However, the gradient domination condition does not hold as \((0,0)\) is a first-order stationary point but not a global maximizer (it is actually a local minimizer).

**Outline of the paper.** The rest of the paper is organized as follows. In Section 2, we discuss the policy optimization problem in the tabular setting and establish the connectedness of the superlevel sets. Section 3 generalizes the result to a class of policies represented by over-parameterized neural networks. We introduce the structure of the neural network and the definition of super level sets in this context, and present our theoretical result. In Section 4, we use our main results on superlevel sets to derive two minimax theorems for robust RL. Finally, we conclude in Section 5 with remarks on future directions.

## 2 Connected Superlevel Set Under Tabular Policy

We consider the infinite-horizon average-reward MDP characterized by \(=(,,,r)\). We use \(\) and \(\) to denote the state and action spaces, which we assume are finite. The transition probability kernel is denoted by \(:_{}\), where \(_{}\) denotes the probability simplex over \(\). The reward function \(r:[0,U_{r}]\) is bounded for some positive constant \(U_{r}\) and can also be regarded as a vector in \(^{||||}\). We use \(P^{}^{}\) to represent the state transition probability matrix under policy \(_{}^{}\), where \(_{}^{}\) is the collection of probability simplexes over \(\) across the state space

\[P_{s^{},s}^{}=_{a}(s^{ } s,a)(a s), s^{},s.\] (1)

We consider the following ergodicity assumption in the rest of the paper, which is commonly made in the RL literature (Wang, 2017; Wei et al., 2020; Wu et al., 2020).

**Assumption 1**: _Given any policy \(\), the Markov chain formed under the transition probability matrix \(P^{}\) is ergodic, i.e. irreducible and aperiodic._

Figure 1: Visualization of Functions \(f\) (Left) and \(g\) (Right)

Let \(_{}_{S}\) denote the stationary distribution of the states induced by policy \(\). As a consequence of Assumption 1, the stationary distribution \(_{}\) is unique and uniformly bounded away from \(0\) under any \(\). In addition, \(_{}\) is the unique eigenvector of \(P^{}\) with the associated eigenvalue equal to \(1\), i.e. \(_{}=P^{}_{}\). Let \(_{}_{S}\) denote the state-action stationary distribution induced by \(\), which can be expressed as

\[_{}(s,a)=_{}(s)(a s).\] (2)

We measure the performance of a policy \(\) under reward function \(r\) by the average cumulative reward \(J_{r}()\)

\[J_{r}()_{K}^{K}r(s_{k},a_{k})}{K}= _{s_{},a}[r(s_{k},a_{k})]=_{s,a}r(s,a)_{}(s,a).\]

The objective of the policy optimization problem is to find the policy \(\) that maximizes the average cumulative reward

\[_{_{A}^{S}}J_{r}().\] (3)

The superlevel set of \(J_{r}\) is the set of policies that achieve a value function greater than or equal to a specified level. Formally, given \(\), the \(\)-superlevel set (or superlevel set) under reward \(r\) is defined as

\[_{,r}\{_{A}^{S} J_{r}() \}.\]

The main focus of this section is to study the connectedness of this set \(_{,r}\), which requires us to formally define a connected set.

**Definition 1**: _A set \(\) is connected if for any \(x,y\) there exists a continuous map \(p:\) such that \(p(0)=x\) and \(p(1)=y\)._

We say that a function is connected if its superlevel sets are connected at all levels. We also introduce the definition of equiconnected functions.

**Definition 2**: _Given two spaces \(\) and \(\), the collection of functions \(\{f_{y}:\}_{y}\) is said to be equiconnected if for every \(x_{1},x_{2}\), there exists a continuous path map \(p:\) such that_

\[p(0)=x_{1}, p(1)=x_{2}, f_{y}(p())\{f_{y}(x_{1}),f_{y} (x_{2})\},\]

_for all \(\) and \(y\)._

Conceptually, the collection of functions \(\{f_{y}:\}_{y}\) being equiconnected requires 1) that \(f_{y}()\) is a connected function for all \(y\) (or equivalently, the set \(\{x:f_{y}(x)\}\) is connected for all \(\) and \(y\)) and 2) that the path map constructed to prove the connectedness of \(\{x:f_{y}(x)\}\) is independent of \(y\).

We now present our first main result of the paper, which states that the superlevel set \(_{,r}\) is always connected.

**Theorem 1**: _Under Assumption 1, the superlevel set \(_{,r}\) is connected for any \(\) and \(r^{||||}\). In addition, the collection of functions \(\{J_{r}():_{}^{S}\}_{r^{| |||}}\) is equiconnected._

Our result here extends easily to the infinite-horizon discounted-reward setting since a discounted-reward MDP can be regarded as an average-reward one with a slightly modified transition kernel [Konda, 2002].

The claim in Theorem 1 on the equiconnectedness of \(\{J_{r}\}_{r^{||||}}\) is a slightly stronger result than the connectedness of \(_{,r}\), and plays an important role in the application to minimax theorems discussed later in Section 4.

We note that the proof, presented in Section A.1 of the appendix, mainly leverages the fact that the value function \(J_{r}()\) is linear in the state-action stationary distribution \(_{}\) and that there is a special connection (though nonlinear and nonconvex) between \(_{}\) and the policy \(\), which we take advantage of to construct the continuous path map for the analysis. Specifically, given two policies \(_{1},_{2}\) with \(J_{r}(_{1}),J_{r}(_{2})\), we show that the policy \(_{}\) defined as

\[_{}(a s)=}(s)_{1}(a s)+(1-) _{_{2}}(s)_{2}(a s)}{_{_{1}}(s)+(1-)_{_{ 2}}(s)},\]

is guaranteed to achieve \(J_{r}(_{})\) for all \(\).

Besides playing a key role in the proof of Theorem 1, our construction of this path map may inform the design of algorithms in the future. Given any two policies with a certain guaranteed performance, we can generate a continuum of policies at least as good. As a consequence, if we find two optimal policies (possibly by gradient descent from different initializations) we can generate a range of interpolating optimal policies. If the agent has a preference over these policy (for example, to minimize certain energy like in \(H_{1}\) control, or if some policies are easier to implement physically), then the selection can be made on the continuum of optimal policies, which eventually leads to a more preferred policy.

## 3 Connected Superlevel Set Under Neural Network Parameterized Policy

In real-world reinforcement learning applications, it is common to use a deep neural network to parameterize the policy (Silver et al., 2016; Arulkumaran et al., 2017). In this section, we consider the policy optimization problem under a special class of policies represented by an over-parameterized neural network and show that this problem still enjoys the important structure -- the connectedness of the superlevel sets -- despite the presence of the highly complex function approximation. Illustrated in Fig. 2, the neural network parameterizes the policy in a very natural manner which matches how neural networks are actually used in practice.

Mathematically, the parameterization can be described as follows. Each state \(s\) is associated with a feature vector \((s)^{d}\), which in practice is usually carefully selected to summarize the key information of the state. For state identifiability, we assume that the feature vector of each state is unique, i.e.

\[(s)(s^{}), s,s^{}s s^{}.\]

To map a feature vector \((s)\) to a policy distribution over state \(s\), we employ a \(L\)-layer neural network, which in the \(k_{}\) layer has weight matrix \(W_{k}^{n_{k-1} n_{k}}\) and bias vector \(b_{k}^{n_{k}}\) with \(n_{0}=d\) and \(n_{L}=||\). For the simplicity of notation, we use \(_{k}\) to denote the space of weight and bias parameters \((W_{k},b_{k})\) of layer \(k\), and we write \(=_{1}_{L}\). We use \(\) to denote the collection of the weights and biases

\[=((W_{1},b_{1}),,(W_{L},b_{L}))\]

We use the same activation function for layers \(1\) through \(L-1\), denoted by \(:\), applied in an element-wise fashion to vectors. To ensure that the output of the neural network is a valid probability distribution, the activation function for the last layer is a softmax function, denoted by \(:^{||}_{}\), i.e. for any vector \(v^{||}\)

\[(v)_{i}=)}{_{i^{}=1}^{||}(v_{i^{ }})}, i=1,...,||.\]

Figure 2: Neural Network Policy Representation

With \(v^{d}\) as the input to a neural network with parameters \(\), we use \(f^{}_{k}(v)^{n_{k}}\) to denote the output of the network at layer \(k\). For \(k=1,,L\), \(f^{}_{k}(v)\) is computed as

\[f^{}_{k}(v)=\{(W_{1}^{}v+b_{1} )&k=1\\ (W_{1}^{}f_{k-1}(v)+b_{k})&k=2,3,...,L-1\\ (W_{L}^{}f_{L-1}(v)+b_{L})&k=L..\] (4)

The policy \(_{}^{||||}\) parametrized by \(\) is the output of the final layer:

\[_{}( s)=f^{}_{L}((s))_{},  s.\]

Our analysis relies two assumptions about the structure of the neural network. The first concerns the invertibility of \(()\) as well as the continuity and uniqueness of its inverse, which can be guaranteed by the following:

**Assumption 2**: \(\) _is strictly monotonic and \(()=\). In addition, there do not exist non-zero scalars \(\{p_{i},q_{i}\}_{i=1}^{m}\) with \(q_{i} q_{j},\, i j\) such that for some \(m>0\), \((x)=_{i=1}^{m}p_{i}(x-q_{i}),\, x\)._

We note that this assumption holds for common activation functions including leaky-ReLU and parametric ReLU (Xu et al., 2015).

Our second assumption is that the neural network is sufficiently over-parameterized and that the number of parameters decreases with each layer.

**Assumption 3**: _The output of the first layer is wider than \(2||\), and the width of the network decreases over the layers, i.e._

\[n_{1} 2||,n_{1}>n_{2}>...>n_{L}=||.\]

Neural networks meeting this criteria have a number of weight parameters that is larger than the cardinality of the state space, making them impractical for large \(||\). While ongoing work seeks to relax or remove this assumption, we point out that similar over-parameterization assumptions are critical and very common in most existing works on the theory of neural networks (Zou and Gu, 2019; Nguyen, 2019; Liu et al., 2022; Martinetz and Martinetz, 2022; Pandey and Kumar, 2023).

The \(\)-superlevel set of the value function with respect to \(\) under reward function \(r\) is

\[^{}_{,r}\{ J_{r}(_{ })\}.\]

Our next main theoretical result guarantees the connectedness of \(^{}_{,r}\).

**Theorem 2**: _Under Assumptions 1-3, the superlevel set \(^{}_{,r}\) is connected for any \(\). In addition, with \(J_{r,}() J_{r}(_{})\), the collection of functions \(\{J_{r,}():\}_{r^{| |||}}\) is equiconnected._

The proof of this theorem is deferred to the appendix. Similar to Theorem 1, the claim in Theorem 2 on the equiconnectedness of \(\{J_{r,}\}_{r^{||||}}\) is again stronger than the connectedness of \(^{}_{,r}\) and needs to be derived for the application to minimax theorems, which we discuss in the next section.

## 4 Application to Robust Reinforcement Learning

In this section, we consider the robust RL problem under adversarial reward attack, which can be formulated as a convex-nonconcave minimax optimization program. In Section 4.1, we show that the minimax equality holds for this optimization program in the tabular policy setting and under policies represented by a class of neural networks, as a consequence of our results in Sections 2 and 3. To our best knowledge, the existence of the Nash equilibrium for this robust RL problem has not been established before even in the tabular case. A specific example of this type of robust RL problems is given in Section 4.2.

### Minimax Theorem

Robust RL in general studies identifying a policy with reliable performance under uncertainty or attacks. A wide range of formulations have been proposed for robust RL (which we reviewed in details in Section 1.2), and an important class of formulations takes the form of defending against an adversary that can modify the reward function in a convex manner. Specifically, the objective of the learning agent can be described as solving the following minimax optimization problem

\[_{_{}^{}}_{r}J_{r}(),\] (5)

where \(\) is some convex set. It is unclear from the existing literature whether minimax inequality holds for this problem, i.e.

\[_{_{}^{}}_{r}J_{r}( )=_{r}_{_{}^{}}J_{r}( ),\] (6)

and we provide a definitive answer to this question. We note that there exists a classic minimax theorem on a special class of convex-nonconcave functions (Simons, 1995), which we adapt and simplify as follows.

**Theorem 3**: _Consider a separately continuous function \(f:\), with \(\) being a convex, compact set. Suppose that \(f(x,)\) is convex for all \(x\). Also suppose that the collection of functions \(\{f(,y)\}_{y}\) is equiconnected. Then, we have_

\[_{x}_{y}f(x,y)=_{y} _{x}f(x,y).\] (7)

Theorem 3 states that the minimax equality holds under two main conditions (besides the continuity condition, which can easily be verified to hold for \(J_{r}()\)). First, the function \(f(x,y)\) needs to be convex with respect to the variable \(y\) within a convex, compact constraint set. Second, \(f(x,y)\) needs to have a connected superlevel set with respect to \(x\), and the path function constructed to prove the connectedness of the superlevel set is independent of \(y\). As we have shown in this section and earlier in the paper, if we model \(J_{r}()\) by \(f(x,y)\) with \(\) and \(r\) corresponding to \(x\) and \(y\), both conditions are observed by the optimization problem (5), which allows us to state the following corollary.

**Corollary 1**: _Suppose that the Markov chain \(\) satisfies Assumption 1 on ergodicity. Then, the minimax equality (6) holds._

When the neural network presented in Section 3 is used to represent the policy, the collection of functions \(\{J_{r,}\}_{r}\) is also equiconnected. This allows us to extend the minimax equality above to the neural network policy class. Specifically, consider problem (5) where the policy \(_{}\) is represented by the parameter \(\) as described in Section 3. Using \(f(x,y)\) to model \(J_{r}(_{})\) with \(x\) and \(y\) mirroring \(\) and \(r\), we can easily establish the minimax theorem in this case as a consequence of Theorem 2 and 3.

**Corollary 2**: _Suppose that the Markov chain \(\) satisfies Assumption 1 on ergodicity and that the neural policy class satisfies Assumptions 2-3. Then, we have_

\[_{}_{r}J_{r}(_{})=_{r }_{}J_{r}(_{}).\] (8)

Corollary 1 and 2 establish the minimax equality (or equivalently, the existence of the Nash equilibrium) for the robust reinforcement learning problem under adversarial reward attack for the tabular and neural network policy class, respectively. To our best knowledge, these results are both novel and previously unknown in the existing literature. The Nash equilibrium is an important global optimality notion in minimax optimization, and the knowledge on its existence can provide strong guidance on designing and analyzing algorithms for solving the problem.

### Example - Defense Against Reward Poisoning

We now discuss a particular example of (5). We consider the infinite horizon, average reward MDP \(=(,,,r)\) introduced in Section 2, where \(r\) is the true, unpoisoned reward function. Let \(^{}\) denote the set of deterministic policies from \(\) to \(\). With the perfect knowledge of this MDP, an attacker has a target policy \(_{}^{}\) and tries to make the learning agent adopt the policy bymanipulating the reward function. Mathematically, the goal of the attacker can be described by the function \((r,_{},_{})\) which returns a poisoned reward under the true reward \(r\), the target policy \(_{}\), and a pre-selected margin parameter \(_{} 0\). \((r,_{},_{})\) is the solution to the following optimization problem

\[(r,_{},_{}) = *{argmin}_{r^{}} _{s,a}(r^{}(s,a)-r(s,a) )^{2}\] (9) \[ J_{r^{}}(_{}) J_{r^{}}()+_{ },^{}_{}.\]

In other words, the attacker needs to minimally modify the reward function to make \(_{}\) the optimal policy under the poisoned reward. This optimization program minimizes a quadratic loss under a finite number of linear constraints and is obviously convex.

The learning agent observes the poisoned reward \(r_{}=(r,_{},_{})\) rather than the original reward \(r\). As noted in Banihashem et al. (2021), without any defense, the learning agent solves the policy optimization problem under \(r_{}\) to find \(_{}\), which may perform arbitrarily badly under the original reward. One way to defend against the attack is to maximize the performance of the agent in the worst possible case of the original reward, which leads to solving a minimax optimization program of the form

\[_{_{}^{}}_{r^{}}J_{r^{}} ()\ (r^{},_{},_{ })=r_{}.\] (10)

When the policy \(\) is fixed, (10) reduces to

\[_{r^{}}J_{r^{}}()\ (r^{ },_{},_{})=r_{}.\] (11)

With the justification deferred to Appendix D, we point out that (11) consists of a linear objective function and a convex (and compact) constraint set, and is therefore a convex program. On the other side, when we fix the reward \(r^{}\), (10) reduces to a standard policy optimization problem.

We are interested in investigating whether the following minimax equality holds.

\[_{_{}^{}}_{r^{}:(r^{},_{},_{})=r_{}}J_{r^{}}( )=_{r^{}:(r^{},_{},_{ })=r_{}}_{_{}^{}}J_{r^{ }}().\] (12)

This is a special case of (5) with \(=\{r^{}(r^{},_{}, _{})=r_{}\}\), which can be verified to be a convex set. Therefore, the validity of (12) directly follows from Corollary 1. Similarly, in the setting of neural network parameterized policy we can establish

\[_{}_{r^{}:(r^{},_{ },_{})=r_{}}J_{r^{}}(_{})=_{r^ {}:(r^{},_{},_{})=r_{ }}_{}J_{r^{}}(_{})\]

as a result of Corollary 2.

## 5 Conclusions & Future Work

We study the superlevel set of the policy optimization problem under the MDP framework and show that it is always a connected set under a tabular policy and for policies parameterized by a class of neural networks. We apply this result to derive a previously unknown minimax theorem for a robust RL problem. An immediate future direction of the work is to investigate whether/how the result discussed in this paper can be used to design better RL algorithms. In Fatkhullin and Polyak (2021), the authors show that the original LQR problem has connected level sets, but the partially observable LQR does not. It is interesting to study whether this observation extends to the MDP setting, i.e. the policy optimization problem under a partially observable MDP can be shown to have disconnected superlevel sets.