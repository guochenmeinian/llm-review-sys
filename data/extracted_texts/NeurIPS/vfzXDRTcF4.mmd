# JourneyDB: A Benchmark for Generative Image Understanding

Keqiang Sun1,  Junting Pan1,3*,  Yuying Ge2,  Hao Li1,  Haodong Duan1,  Xiaoshi Wu1,  Renrui Zhang1,  Aojun Zhou1,  Zipeng Qin1,  Yi Wang3,  Jifeng Dai3,  Yu Qiao3,  Limin Wang3,  Hongsheng Li1,3,41

\({}^{1}\)Multimedia Laboratory, The Chinese University of Hong Kong

\({}^{2}\)The University of Hong Kong

\({}^{3}\)Shanghai Artificial Intelligence Laboratory

\({}^{4}\)Centre for Perceptual and Interactive Intelligence

Equal Contribution

Project Lead

Corresponding Authors

###### Abstract

While recent advancements in vision-language models have had a transformative impact on multi-modal comprehension, the extent to which these models possess the ability to comprehend generated images remains uncertain. Synthetic images, in comparison to real data, encompass a higher level of diversity in terms of both content and style, thereby presenting significant challenges for the models to fully grasp. In light of this challenge, we introduce a comprehensive dataset, referred to as JourneyDB, that caters to the domain of generative images within the context of multi-modal visual understanding. Our meticulously curated dataset comprises 4 million distinct and high-quality generated images, each paired with the corresponding text prompts that were employed in their creation. Furthermore, we additionally introduce an external subset with results of another \(22\) text-to-image generative models, which makes JourneyDB a comprehensive benchmark for evaluating the comprehension of generated images. On our dataset, we have devised four benchmarks to assess the performance of generated image comprehension in relation to both content and style interpretation. These benchmarks encompass prompt inversion, style retrieval, image captioning, and visual question answering. Lastly, we evaluate the performance of state-of-the-art multi-modal models when applied to the JourneyDB dataset, providing a comprehensive analysis of their strengths and limitations in comprehending generated content. We anticipate that the proposed dataset and benchmarks will facilitate further research in the field of generative content understanding. The dataset is publicly available at https://journeydb.github.io.

## 1 Introduction

In recent times, notable progress has been achieved in the domain of Artificial Intelligence Generative Content (AIGC), particularly in the advancement of diffusion models  that have significantly enhanced the quality of generative content. As a consequence, AIGC platforms such as DALLE, Stability AI, Runway, and Midjourney have gained considerable popularity, enabling users to generate exceptionally high-quality images using text prompts composed in natural language. These text prompts encompass both content and style descriptions provided by users, playing a pivotal role in image generation (see Figure 1 for an illustrative prompt). Unlike descriptions acquired from captioning real images, text prompts for image generation tend to exhibit a high level of detail and specificity, surpassing mere portrayal of salient content. The primary objective behind the creationof these prompts lies in visual generation, resulting in intricate descriptions that encompass diverse stylistic facets such as lighting, camera angle, artistic style, medium, and more. Moreover, the generated content originates from the users' imagination, often depicting scenes and compositions that are entirely fictional and devoid of real-world existence.

Considering the aforementioned characteristics, we contend that both the elaborate textual prompts and the generated images themselves serve as valuable sources of information that can be incorporated into existing visual understanding benchmarks. On one hand, the detailed text prompts offer a more comprehensive interpretation of the visual scene, enabling us to perceive the scene and comprehend its underlying style. On the other hand, the abundance of novel object compositions in the generated images provides insights into a realm unrestricted by conventional sense biases, facilitating exploration beyond the constraints of traditional visual representations.

Foundation models have achieved unparalleled capabilities across various visual understanding tasks, owing to large-scale pre-training on datasets, such as CLIP , Flamingo , and BLIP-2 . However, it is essential to acknowledge that current foundation models are primarily pre-trained on real data, giving rise to concerns regarding their generalization ability and effectiveness in handling the distinctive characteristics associated with generative content. These models may not fully capture the nuanced aspects of generative content and might encounter difficulties in comprehending and generating high-quality images based on complex text prompts.

In view of this challenge, our research initiative seeks to address this gap by curating a dataset comprising a substantial number of **4 million** meticulously generated images accompanied by corresponding text prompts. This dataset serves as the fundamental basis for a benchmark consisting of four distinct tasks, which collectively facilitate a comprehensive evaluation of generative content understanding.

The initial task, referred to as **prompt inversion**, involves identifying the text prompts employed by the user to generate the given images. This task serves to decipher the original prompt or description, assessing the model's ability to comprehend both the content and style of the generated images. The second task involves **style retrieval**, wherein the model is tasked with identifying and retrieving similar generative images based on their stylistic attributes. This task evaluates the model's proficiency in discerning subtle stylistic nuances within generative images. The third task centres around **image captioning**, requiring the model to generate descriptive captions that accurately represent the content of the generative image. This task evaluates the model's capability to effectively comprehend and express the visual elements of the generated content using natural language. The fourth and final task is **visual question answering (VQA)**, in which the model is expected to provide accurate answers to questions related to the generative image. This task evaluates the model's ability to comprehend the visual and stylistic content and deliver relevant responses based on the provided questions.

We collected a total of \(4,692,751\) pairs of image-text prompts, which were subsequently divided into a training set comprising \(4,453,193\) pairs, a validation set comprising \(234,156\) pairs, and a test

Figure 1: **Data Collection Procedure. To collect enough generated images, we investigate the Midjourney channel on Discord to collect the available pictures. Then we employ the GPT-3.5 to annotate the downstream tasks, including 1) separating the prompt into “Style” and “Content”, 2) generating the caption according to the content words obtained from task 1, 3) generating “Style-relevant questions” and “Content-relevant questions”, providing \(4\) options for each question, together with the answer. Please refer to Section 3 for more details.**

set comprising \(5,402\) pairs. We also include \(45,803\) images from \(22\) other text-to-image models provided by HPD v2 , including VQ-Diffusion , DALL-E 2 , StableDiffusion-XL , etc., to build the external set for cross dataset evaluation. Given that the generative model is not flawless, some discrepancies in the text prompts may be present. Consequently, for the test set, we carried out human verification, where annotators were tasked with removing word descriptions that do not align with the corresponding images. To create annotations for tasks 2, 3, and 4, we utilized GPT-3.5 to convert text prompts into annotations specific to each task.

To comprehensively evaluate the performance of current state-of-the-art multi-modal models, we conducted extensive assessments using our benchmark dataset. Furthermore, we performed in-depth analyses to gain insights into the strengths and limitations of these models when applied to generative content. Overall, we observed that the state-of-the-art models do not perform as effectively as they do on real datasets, and fine-tuning on the proposed dataset significantly enhances their performance.

In conclusion, our contribution encompasses three key aspects: 1) To the best of our knowledge, we are the first to draw attention to the visual understanding of generated images. 2) We propose JourneyDB, a large-scale benchmark that serves as both a training and evaluation resource for this emerging field. 3) We conducted an extensive evaluation of state-of-the-art visual understanding models using the proposed dataset, revealing their relatively limited performance on generative content. We hope that our endeavours will contribute to further advancements in the field of generative content understanding.

## 2 Related Works

### Image-Text Datasets

We present a summary of existing image-text datasets in Table 1. The Flickr Caption dataset  consists of \(32,000\) images obtained from the Flickr  platform, accompanied by five reference sentences provided by human annotators. The COCO Caption dataset  comprises \(164\) thousand images, with five independent human-generated captions provided for each image for training and validation, resulting in over \(1.5\) million captions. These datasets play a crucial role in fostering the development of the Image-Caption Task. The Visual Question Answering (VQA) v2.0  dataset, which is the second version of the VQA dataset , contains open-ended questions about images that require an understanding of vision, language, and commonsense knowledge to answer. A-OKVQA , an augmented successor of OK-VQA , encompasses a diverse set of \(24\) thousand questions that demand a broad foundation of common and world knowledge for accurate responses. These datasets involve human employees in the annotation process, ensuring consistently high-quality annotations. However, manual annotation by human annotators is a time-consuming and costly endeavour, thereby limiting the scalability of the datasets. LAION-COCO  is another large-scale dataset containing \(600\) million image-caption pairs, where GPT3.5 is employed to generate more detailed captions. Although these datasets may contain noise due to the cleaning or generation process using pre-trained neural network models, they have demonstrated their utility in training multi-modal models. However, it is important to note that these datasets primarily focus on real images and cater to a specific task. A comparable dataset to the present study is DiffusionDB , a large-scale text-to-image prompt dataset comprising \(14\) million images generated using Stable Diffusion. However, the image quality from Stable Diffusion is plausible, and no further annotations

   Dataset & Total Image Num & Label Source & Image Caption & VQA & Prompt Inversion & Style Retrieval \\  Flickr Caption  & 32k & H & ✓ & & & \\ COCO Caption  & 164k & H & ✓ & & & \\ VQA v2  & 204k & H & & ✓ & & \\ A-OKVQA  & 24k & H & & ✓ & & \\ LAION-COCO  & 600M & M & ✓ & & & ✓ & \\ DiffusionDB  & 14M & M & ✓ & ✓ & ✓ & ✓ \\
**Ours** & 4M & H + M & ✓ & ✓ & ✓ & ✓ \\   

Table 1: **A comparison between JourneyDB and other commonly-used Text-Image multi-modal datasets.** Among all the commonly-used multi-modal datasets, the proposed dataset is the most versatile, supporting four downstream tasks. H: Human, M: Modelsare available. In this paper, we collect data from Midjourney and provide annotations generated by GPT3.5 to support four downstream tasks.

### Text-to-Image Generative Models

Text-to-image generative models [19; 20; 14; 7; 21] aim at generating images according to text conditions, apart from traditional generative models [22; 23; 24; 25], which map random noise to images. Text-to-image generative models have experienced rapid development in recent years, empowering users to create image content through natural language specifications. This field has seen significant progress since Mansimov _et al_.demonstrated that Deep Recurrent Attention Writer (DRAW) can generate images conditioned on text [26; 27]. Since then, several generative architectures and modeling approaches have been applied for text-to-image generation, including autoregressive models , GANs , and diffusion models [14; 7; 21]. Among these, diffusion models have shown better computational efficiency and the ability to produce higher-quality samples compared to autoregressive models . These diffusion models have reached a level of maturity where they can generate high-quality images suitable for industrial deployment. Notably, Midjourney provides state-of-the-art text-to-image generation service using diffusion models . A vast number of artificial images are generated each day at unprecedented speed. As perception and generation tasks are double sides of the same coin, the achievements in the generative models open new probability for the perception studies. In this context, our dataset aims to organize and consolidate recent progress in text-to-image generative models while laying the foundations for further research in perception studies.

### Multi-modal Foundation Models and Datasets

Aided by data from diverse sources, multi-modal foundation models are capable of understanding and connecting data across multiple modalities, such as image, text, audio and so on. As pioneering vision-language models, CLIP  and ALIGN  adopt contrastive learning paradigms and are pre-trained by millions of web-collected image-text pairs, which showcases promising visual zero-shot capabilities. Flamingo  and BLIP-2  further align pre-trained vision backbones with language models with intermediate networks and billions of data pairs, exhibiting superior results on vision-language tasks. OFA , Uni-Perceivers [30; 31; 32], and Unified-IO  also introduce unified training architectures for different modalities with competitive performance to uni-modal methods. Recently, inspired by the powerful GPT-4 , many efforts have been devoted to multi-modal instruction-following models, such as LLaMA-Adapter [35; 36], LLaVA  and MiniGPT-4 . Given the textual prompts with image conditions, these models fine-tune a frozen LLaMA  to respond to multi-modality instructions, the training data of which is either existing image-caption data  or GPT-annotated pairs . Despite the popularity of multi-modal models, it is still rarely explored for their generalization capacity on generated vision-language data, considering the difference between the real-world pre-training data and generative content. In this paper, we propose a large-scale synthetic dataset, JourneyDB, along with customized benchmarks to fully validate the extension efficacy current multi-modal models.

### Training with Generated Data

It is worth noting that the annotations generated by GPT demonstrate a lower level of noise than expected, validating the effectiveness of these models. Notably, LLaVA  introduces a novel instruction-tuning dataset that leverages the capabilities of both GPT3.5 and GPT4. Their experiments reveal a remarkable relative score increase of \(295.8\%\), elevating the score from \(21.5\) to \(85.1\), thus emphasizing the effectiveness of their generated data. LaCLIP  integrates text augmentations by employing text rewriting techniques with GPT3.5 and Bard. By rewriting the textual descriptions within existing image caption datasets, they achieve a notable improvement of \(36.08\%\), raising the score from \(15.8\) to \(21.5\). StableRep  unveils the remarkable potential of using exclusively synthetic data generated from text-to-image models to train highly effective visual representations, surpassing the performance of models trained solely on real image datasets. In a similar vein, VideoChat  constructs a dataset by sequentially feeding dense captions to GPT3.5 in temporal order. Despite the inherent challenges in comprehending individual frames with GPT3.5, their successful mastery of understanding the entire video demonstrates the effectiveness of their approach. The generated annotations not only validate the effectiveness of GPT models but also significantly contribute to advancing the understanding of images. Therefore, based on our demonstrated results, we firmly believe that our JourneyDB can serve as a valuable tool to enhance numerous image-related tasks.

## 3 Dataset

In this section, we present the methodology employed for dataset collection and annotation, along with relevant statistical insights to gain a deeper understanding of the dataset.

### Data Collection

The data collection procedure is presented in Figure 1. In order to obtain a sufficient number of generated images, we investigated the Midjourney channel  on the Discord platform  to access the available pictures. Within the public Discord channel named "Midjourney," users submit text prompts to the channel, and the Midjourney bot responds with the corresponding generated images. Users then select the preferred image for upscaling, and Midjourney provides the corresponding upscaled images. The chat history contains numerous publicly accessible prompt-image pairs. To collect the data, we utilized DiscordChatExporter , a widely used Discord crawler, to download the publicly available images along with their corresponding prompts. In this version of the dataset, we only retained images that were generated solely based on text prompts, filtering out any images conditioned on given images. Additionally, we removed Midjourney-specific arguments, such as "-v 4", to enhance the generalizability of the prompts and ensure their comprehensibility for existing large language models.

Moreover, to improves the diversity of JourneyDB, we additionally introduce another \(22\) text-to-image generative models into JourneyDB, such as VQ-Diffusion , DALL-E 2 , StableDiffusion-XL , etc., which makes our data a comprehensive benchmark for evaluating the comprehension of generated images. For each generative model, we originally generated \(3,200\) images, and a group of \(60\) annotators helped clean up the pairs without consistency to obtain the final cross-model test set containing \(45,803\) images in total. Please find more details of this part in the appendix D.

### Data Annotation

We provide ample annotations for multiple visual understanding tasks. The dataset is compared with existing methods in Table 1, demonstrating its versatility in supporting four downstream tasks.

Annotation for Visual Understanding.In this section, GPT-3.5 is employed to annotate the downstream tasks. Specifically, a set of Midjourney prompts and explicit instructions are provided to GPT-3.5. The objectives are as follows: 1) segmenting the prompt into "Style", "Content", "Atmosphere", and "Others", 2) generating captions based on the content words identified in task 1, 3) generating "Style-relevant questions" and "Content-relevant questions," accompanied by four answer choices for each question. The detailed instructions provided to GPT-3.5 can be found in the Supplementary Materials.

Clustering of Styles.Numerous prompts related to style are highly intricate for style retrieval. Taking inspiration from existing prompt engineering platforms 4, we propose a hierarchical clustering

   Dataset & Image & Prompt & Labeled Image & Labeled Prompt & Style QA & Content QA \\  Training Set & 4,453,193 & 1,643,375 & 4,189,737 & 1,385,317 & 7,056,394 & 8,775,971 \\ Validation Set & 234,156 & 82,093 & 234,156 & 82,093 & 311,569 & 374,310 \\ Testing Set & 5,402 & 5,171 & 5,402 & 5,171 & 10,040 & 11,369 \\ External Set & 45,803 & 45,365 & 45,803 & 45,365 & 74,407 & 81,565 \\ Total & 4,738,554 & 1,776,004 & 4,475,098 & 1,517,946 & 7,452,410 & 9,243,215 \\   

Table 2: **Statistics of JourneyDB. We provide 4 million generated image-prompt pairs, 1 million captions and over 8 million VQA annotations.**approach for organizing styles, which simplifies style retrieval and facilitates user reference. Since traditional word embedding and clustering methods struggle to handle sophisticated style words, we leverage GPT-3.5 for this task. Specifically, we divide the prompts into smaller patches, each comprising \(200\) prompts, and instruct GPT-3.5 to cluster the words. Subsequently, we manually merge the categories from different patches to obtain the final "style tree". The distribution of the clustered style space is visualized in Figure 2.

Filtering for Image-Prompt Consistency.Due to the limitations of Text-to-Image generative models, inconsistencies may arise between the prompt and the generated image. To ensure the quality of the test set, we engaged \(40\) annotators to identify inconsistent prompt words in the test set. Specifically, given a pair of text prompts and the corresponding generated image, the annotators are instructed to verify if each word is depicted in the image. Words annotated as "Not Appear" are removed to obtain the clean prompts.

### Data Statistics

General StatisticsIn this iteration, a total of \(4,692,751\) images were collected, all with a resolution exceeding \(1024 1024\), accompanied by corresponding text prompts. Among them, \(1,730,639\) prompts were found to be independent. Furthermore, \(1,472,581\) instances were annotated using the GPT-3.5 model, following the procedure outlined in Figure 1. Additionally, \(5,402\) images were filtered out due to inconsistencies between the images and prompts, as determined by the Image-Prompt consistency check. Moreover, \(45,803\) images from \(22\) other text-to-image models provided by HPD v2  are introduced to build the external set for cross dataset evaluation. In addition, a clustering process was conducted to summarize the \(70,521\) fine-grained styles into \(334\) style categories, displaying a long-tail distribution pattern, as illustrated in Figure 2.

Dataset SplitDetailed statistics for each subset of the dataset are provided in Table 2. The entire dataset was randomly divided, with approximately a \(20:1\) ratio, to create the training and validation sets. The training set comprises \(4,189,737\) images and \(1,385,317\) prompts, while the validation set consists of \(234,156\) images and \(82,093\) prompts. Additionally, a separate testing set was sampled for manual filtering, consisting of \(5,402\) images and \(5,171\) prompts.

## 4 Benchmarks

### Prompt Inversion

The prompt, which determines both the content and style of a generated image, contains crucial and comprehensive information regarding the image. When presented with an appealing generated image, individuals are eager to discern the prompt employed for its creation. By accurately identifying the prompts, they can further enhance the corresponding image, such as modifying its content or generating images with a similar style.

Figure 2: **Distribution and samples of the style prompts.**

However, predicting the prompts of an image is a challenging task. Existing visual understanding models, such as image-caption models, often fall short in providing a detailed description of the image's main elements, such as the subject, while neglecting other indispensable details like the viewpoint, illumination, or art style.

Prompt inversion aims to address this gap, involving the process of taking a single image and predicting the corresponding prompts associated with it. We anticipate that the proposed dataset would further facilitate the development of prompt inversion through the in-depth analysis of the prompts.

To evaluate the effectiveness of prompt inversion, we extend the metrics commonly utilized in image captioning, including Bleu, METEOR, ROUGE, and CIDEr. Additionally, we adopt the approach employed in a related Kaggle competition  to calculate the Cosine Similarity of the sentence-transformers features .

Furthermore, in the supplementary materials, we propose a Question Answering Score (QAS) for evaluating the prompt inversion results. In this paper, we establish a benchmark for the zero-shot prompt inversion task by leveraging state-of-the-art multi-modal models, namely BLIP-2 \(\), BLIP-2 \(\), Flamingo9B, MiniGPT-4 , and Uni-Perceiver v2 . To ensure optimal performance in this novel task, we customize different prompts for each model.

We evaluate these models on the test set of our dataset, and the results are presented in Table 3. During the experiment, we observed that the existing models struggle to capture the intricate details and style-related information of the input image, leading to lower performance compared to conventional datasets.

To verify the effectiveness of our dataset, we fine-tuned Uni-Perceiver v2 for 20 epochs and noted a significant improvement in the prompt inversion task. It is important to note that we followed the training methodology outlined in  without tuning the hyperparameters or utilizing data augmentations. This demonstrates that our JourneyDB can complement existing image-text datasets for training prompt inversion models. Nevertheless, it is evident that there is still a substantial way to go in developing a robust and effective prompt inversion model.

### Image Caption

Image captioning tasks require multi-modal models to generate textual descriptions for the visual content of an image. In comparison to existing image captioning benchmarks such as COCO Caption , JourneyDB encompasses both detailed descriptions and high-level summarizations of images, thereby assessing the model's proficiency in fine-grained recognition and holistic understanding.

    &  &  &  \\  & BLEU-4 & METEOR & ROUGE-L & CIDEr & BLEU-4 & METEOR & ROUGE-L & CIDEr & CIDEr \\  BLIP-2 OPT  & 0.82 & 5.43 & 19.87 & 22.00 & 2.35 & 7.88 & 22.40 & 37.60 & 145.8 (FT) \\ BLIP-2 FlanT5  & 0.54 & 5.02 & 19.94 & 22.18 & 2.07 & 7.62 & **23.12** & **39.62** & 144.5 (FT) \\ Flamingo9B  & 0.94 & 6.58 & 14.19 & 10.19 & 1.39 & 6.84 & 17.75 & 19.10 & 79.4 (ZS) \\ MiniGPT-4  & 2.28 & 7.39 & 19.24 & 16.78 & 2.79 & 9.84 & 20.31 & 22.34 & 22.34 \\ Uni-Perceiver v2  & 0.41 & 4.50 & 18.72 & 21.88 & 0.94 & 5.21 & 16.71 & 20.13 & 122.5 (FT) \\  Uni-Perceiver v2\({}_{}\) & **8.20** & **12.53** & **27.09** & **50.72** & **3.23** & **10.12** & 22.45 & 31.76 & - \\   

Table 4: **Evaluation results of Image Captioning on JourneyDB.** We list the zero-shot results in the upper half, and the fine-tuned results in the lower. For all metrics, the higher, the better. FT denotes “Fine-Tune”.

    &  &  \\  & BLEU-4 & METEOR & ROUGE-L & CIDEr & Similarity & BLEU-4 & METEOR & ROUGE-L & CIDEr & Similarity \\  BLIP-2 OPT  & 0.18 & 2.39 & 6.75 & 5.42 & 0.36 & 0.29 & 2.85 & 7.06 & 6.46 & 0.36 \\ BLIP-2 FlanT5  & 0.27 & 2.46 & 7.19 & 6.88 & 0.38 & 0.40 & 2.95 & 7.69 & 8.86 & 0.37 \\ MiniGPT-4  & 1.49 & 5.50 & 12.51 & 10.39 & 0.43 & 1.71 & 6.51 & 13.13 & 11.40 & 0.43 \\ Uni-Perceiver v2  & 0.23 & 2.44 & 9.11 & 12.38 & 0.33 & 0.37 & 2.73 & 9.88 & 15.45 & 0.34 \\  Uni-Perceiver v2\({}_{}\) & **20.6** & **16.9** & **29.1** & **123.2** & **0.59** & **4.68** & **8.56** & **16.98** & **34.01** & **0.51** \\   

Table 3: **Evaluation results of Prompt Inversion on JourneyDB.** We list results on the validation set in the upper half, results on the test set in the lower. For all metrics, the higher, the better.

We evaluate various existing multi-modal models on the image captioning sub-task of JourneyDB. The results are presented in Table 4, indicating the challenges faced by multi-modal models trained on natural images in providing accurate descriptions for AI-generated content. The quantitative performance is notably poorer (significantly worse than COCO Caption results) due to two primary factors: GPT-3.5 tends to generate verbose descriptions for the images in JourneyDB, resulting in lengthy ground-truth captions. This discrepancy between lengthy ground-truth captions and shorter predicted captions undermines the quantitative performance. When describing AI-generated images, the focus may differ in terms of concepts such as emotions, human/object attributes, etc., compared to natural images. However, existing image captioning models have not adequately addressed these concepts.

We provide qualitative examples in Fig 3. Existing multi-modal approaches fail to describe key concepts present in the AI-generated content (e.g., Fig 3(b) depicts _kids_ in astronaut suits, Fig 3(d) shows a _sad_ potato). Moreover, some models may hallucinate contents that do not exist in the images (e.g., Open-Flamingo hallucinates objects and text in Fig 3(a, c)).

### Style Retrieval

We inhabit a captivating world enveloped in a multitude of vibrant colours and ever-shifting illumination. Artists, in turn, develop their distinct styles to convey their unique perspectives of the world. Elements such as weather conditions, moods, and atmospheres all contribute to the style portrayed in an image, resulting in a complex "style system." As detailed in Section 3.2, we have compiled a comprehensive collection of over 150,000 style words to describe the style-related attributes of images.

Figure 3: **Samples from the validation set of JourneyDB captioning. The examples show that existing multi-modal models failed to recognize some _key concepts_ from the AI-generated images.**

    &  &  \\  & Over-All & Per-Category & Over-All & Per-Category \\  CLIP-ViT-L/14  & 0.65 & **41.72** & 0.47 & **41.33** \\   

Table 5: **Style Retrieval Results. The metric used there is the Recall.**Given the vast expanse of the style space, identifying the style of a given image poses a significant challenge, even for human observers. Consequently, there is a need for style retrieval techniques to aid in comprehending the style exhibited in an image.

Directly retrieving a style prompt from an extensive pool of candidates is a complex and time-consuming task. Therefore, we employ clustering techniques to group the style prompts into 344 categories, including camera parameters, lighting, artist style, colour schemes, and more, as outlined in Section 3.2. By doing so, we effectively narrow down the search space for style prompt retrieval within each category. To establish a benchmark, we employ CLIP  for zero-shot style retrieval evaluation. We extract the features of both the images and the style prompts, subsequently calculating the inner product between the image feature and all candidate style prompts. The results are presented in Table 5. Notably, we observe that conducting retrieval in the overall style prompt space yields significantly low recall. Conversely, the model performs substantially better when performing retrieval in the sub-space of each category.

    &  &  \\  & Content & Style & Content & Style \\  Flamingo9B  & 32.1\% & 31.9\% & 35.6\% & 41.4\% \\ MiniGPT-4  & 28.2\% & 26.6\% & 31.1\% & 29.3\% \\ BLIP-2 \(\) & 65.8\% & 54.9\% & 69.7\% & 57.4\% \\   

Table 6: **Evaluation results of the content-relevant and style-relevant zero-shot Multiple-Choice Visual Question Answering on JourneyDB**. The evaluation metric here is accuracy.

Figure 4: **Failure cases of BLIP-2  for Multiple-Choice Visual Question Answering.**

### Visual Question Answering (VQA)

JourneyDB comprises a collection of images encompassing abundant and diverse prompts. These prompts not only encompass stylistic attributes but also describe the visual contents of the generated images. To assess the model's proficiency in comprehending both style and content of generative data, we establish two tasks: multiple-choice visual question answering (MC-VQA). In the MC-VQA tasks, we utilize GPT-3.5 to generate "Style-relevant questions" and "Content-relevant questions" along with three distracting options for each question, in addition to the correct answer. The evaluation metric employed is accuracy, where the model selects its answer from the four options based on the given question and the image. A-OKVQA highlights that MC-VQA overcomes several inherent challenges of direct answer evaluation, such as ambiguity, which is prevalent in open-ended VQA . The versatility of language expression implies that MC-VQA, by directly matching the selected option with the correct answer, provides a lucid and objective evaluation approach. This characteristic proves advantageous, especially considering the extensive spectrum of answers in our benchmark, encompassing a wide range of descriptions for diverse styles and contents.

To assess the performance of current multimodal models in the zero-shot visual question answering task within our benchmark, we adopt a methodology inspired by recent studies . In this approach, we provide the model with a question and its corresponding candidate answers enumerated with symbols ("A", "B", "C", "D"). By assigning the highest probability to a predicted token ("A", "B", etc.), the model selects the associated answer choice as its response.

The evaluation outcomes for the zero-shot multiple-choice visual question answering tasks, specifically the content-relevant and style-relevant tasks, are presented in Table 6. It is evident that the performance of existing multimodal models falls significantly short of satisfactory levels in both the content-relevant and style-relevant MC-VQA tasks. BLIP-2  outperforms Flamingo9B  and MiniGPT-4 , yet its accuracy remains below \(70\%\). These results highlight the substantial challenges that generative data poses to existing models in comprehending the visual contents and stylistic attributes. Generative data often represents scenes and object compositions that are absent in reality, thereby posing difficulties for multimodal models pre-trained on real images to interpret the visual elements of generated images when answering content-relevant questions. For instance, as illustrated in the second row and fourth column of Figure 4, the model fails to recognize the relationship between the piano and the tree in the image and predicts the option "D: the tree and piano are not related." This failure arises due to the rarity of scenes depicting a tree growing out of a piano in the real world. In comparison to answering content-relevant questions, the performance of all models generally deteriorates when addressing style-relevant questions. The generation of multiple-choice questions from text prompts encompassing diverse stylistic aspects, such as camera angle, lighting, and artistic styles, enables a comprehensive evaluation of a model's capacity to identify the stylistic attributes of generative data within JourneyDB. However, previous multimodal models are pre-trained using descriptions derived from captioning real images, thereby lacking exposure to the broad range of stylistic variations prevalent in generative data. Consequently, these models encounter difficulties in recognizing and distinguishing the various styles manifested in generative data. As illustrated in Figure 4, BLIP-2 provides incorrect answers to the style-relevant questions in the first row pertaining to camera angle, visual elements, lighting type, and animation style depicted in the images of JourneyDB.

## 5 Conclusion

We introduce JourneyDB, an extensive benchmark comprising four distinct downstream tasks, aiming to foster advancements in the comprehension of generative content. By providing a platform that facilitates the development of visual understanding in relation to generated images, researchers and practitioners are empowered to drive progress in this field.

## 6 Acknowledgement

Thanks Mengwei R. for her insightful feedback. Thanks Mingjie Z. for his assistance with data curation. This project is funded in part by National Key R&D Program of China Project 2022ZD0161100, by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of CPII under the InnoHK.