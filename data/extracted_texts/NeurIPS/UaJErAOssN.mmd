# State Space Models on Temporal Graphs:

A First-Principles Study

 Jintang Li\({}^{1}\)1, Ruofan Wu\({}^{2*}\), Xinzhou Jin\({}^{1}\), Boqun Ma\({}^{3}\), Liang Chen\({}^{1}\), Zibin Zheng\({}^{1}\)

\({}^{1}\)Sun Yat-sen University, \({}^{2}\)Coupang, \({}^{3}\)Shanghai Jiao Tong University

{lijt55,jinxzh5}@mail2.sysu.edu.cn,{wuruofan1989,boqun.mbq}@gmail

{chenliang6,zhzibin}@mail.sysu.edu.cn}

Equal contribution.Corresponding author.

###### Abstract

Over the past few years, research on deep graph learning has shifted from static graphs to temporal graphs in response to real-world complex systems that exhibit dynamic behaviors. In practice, temporal graphs are formalized as an ordered sequence of static graph snapshots observed at discrete time points. Sequence models such as RNNs or Transformers have long been the predominant backbone networks for modeling such temporal graphs. Yet, despite the promising results, RNNs struggle with long-range dependencies, while transformers are burdened by quadratic computational complexity. Recently, state space models (SSMs), which are framed as discretized representations of an underlying continuous-time linear dynamical system, have garnered substantial attention and achieved breakthrough advancements in _independent_ sequence modeling. In this work, we undertake a principled investigation that extends SSM theory to temporal graphs by integrating structural information into the online approximation objective via the adoption of a Laplacian regularization term. The emergent continuous-time system introduces novel algorithmic challenges, thereby necessitating our development of GraphSSM, a graph state space model for modeling the dynamics of temporal graphs. Extensive experimental results demonstrate the effectiveness of our GraphSSM framework across various temporal graph benchmarks.

## 1 Introduction

As a class of neural networks designed to operate directly on graph-structured data, graph neural networks (GNNs)  have achieved remarkable success and have established new state-of-the-art performance across a broad spectrum of graph-based learning tasks . While significant progress has been made in researching _static_ graphs, many real-world networks, such as social, traffic, and financial networks may exhibit _temporal_ behaviors that carry valuable time information . This gives rise to temporal (dynamic) graphs, wherein the nodes and edges of the graph may undergo constant or periodic changes over time. In applications where temporal graphs arise, modeling and exploiting the dynamic nature of the continuously evolving graph is crucial in representing the underlying data and achieving high predictive performance .

Learning over temporal graphs is typically approached as a sequence modeling problem in which graph snapshots form a sequence . This often involves challenges related to long graph sequences and scalability issues . Recurrent neural networks (RNNs)  have historically dominated sequence modeling over the last years. However, they have long been plagued by poor capability in modeling long sequences due to rapid forgetting. This hampers their performance in temporalgraphs that require a broader context or longer time window to capture relevant dependencies and patterns. Recently, the advancement of Transformers  has led to a shift in this paradigm, given their superior performance. Yet, Transformers also struggle with long sequence learning because the computational and memory complexity of self-attention is quadratically dependent on the sequence length. The overwhelming computation and memory requirements/costs associated with Transformers makes them less applicable in practical applications handling long-term sequences .

Recently, state space models (SSMs) have emerged as a powerful tool for sequence modeling [11; 13; 29; 40; 6; 10]. The salient characteristic that distinguishes state space models as particularly compelling is their conceptualization of sequential inputs as discrete observations from an underlying process evolving in continuous time, which naturally arises in scenarios such as speech processing  and time series analysis . SSMs sustain a latent state throughout an input sequence and formulate state update equations through the discretization of an underlying linear dynamical system (LDS). Owing to their invariant state size, SSMs exhibit an efficient inferential time complexity, akin to that of RNNs. Simultaneously, they overcome the long-range modeling deficiencies inherent to RNNs through meticulous initializations of state matrices which are theoretically shown to achieve an optimal compression of history .

Temporal graphs often manifest as discrete snapshots capturing the evolution of an underlying graph that is inherently dynamic and continuous in nature . In this context, the SSM methodology could be appropriated as a foundational primitive for temporal graph modeling. However, SSMs are predominantly architected towards independent sequence modeling. Hence, the task of systematically incorporating time-varying structural information into the SSM framework poses significant challenges. Specifically, it remains unexplored as to whether the foundational methodology of discretized LDS is readily applicable to the domain of temporal graphs.

In this work, we advance the SSM methodology to encompass temporal graphs from the first principles. Rather than presupposing the evolution of the underlying temporal graph, we dive into the fundamental problem of online function approximation that underpins the theoretical development of SSMs for sequence modeling . By solving a novel Laplacian regularized online approximation objective, we derive a piecewise dynamical system that compresses historical information of temporal graphs. The piecewise nature of the obtained continuous-time system poses new challenges toward discretization into linear recurrences, thereby motivating our design of GraphSSM, a state space framework for temporal graphs. The main contributions of this work are summarized as follows:

* We introduce the GHiPPO abstraction, a novel construct predicated on the objective of Laplacian regularized online function approximation. This abstraction can alternatively be conceptualized as a memory compression primitive that simultaneously compresses both the feature dynamics and the evolving topological structure of the underlying temporal graph. The solution to GHiPPO is characterized by a dynamical system that is piecewise linear in node feature inputs.
* We introduce GraphSSM, a flexible state space framework designed for temporal graphs, which effectively addresses the key algorithmic challenge of unobserved graph mutations that impedes the straightforward discretization of the GHiPPO solution into (linear) recurrences through employing a novel mixed discretization strategy.
* Experimental results on six temporal graphs have validated the effectiveness of GraphSSM. In particular, GraphSSM has the advantages in scaling efficiency compared to existing state-of-the-arts, which can generalize to temporal graphs with long-range snapshots.

## 2 Related work

### Temporal graph learning

A major branch of temporal graph learning methods consists of snapshot-based methods, which handle discrete-time temporal graphs by learning the temporal dependencies across a sequence of time-stamped graphs. Early works mainly focus on learning node representations by simulating temporal random walks  or modeling the triadic closure process  on multiple graph snapshots. These methods typically generate piecewise constant representations and may suffer from the staleness problem . In recent years, the most established solution has been switched to combine sequence models (e.g., RNNs  and SNNs [38; 8]) with static GNNs to capture temporal dependencies and correlations between snapshots [47; 34; 39; 25]. To better translate the success achieved on static graphs in both their design and training strategies, recent frameworks such as ROLAND  and its variants [53; 17] have been proposed to repurpose static GNNs to temporal graphs. There is another important line of research that focuses on continuous-time temporal graphs, we kindly refer readers to  and  for comprehensive surveys on this research topic.

### State space models

State space models (SSMs) have historically served as a pivotal tool in fields such as signal processing  and time series analysis . In recent advancements, they have also seen active adoption as a layer within neural sequence modeling frameworks [11; 13; 29; 40; 10]. The linear nature of SSMs confers several significant advantages. Key among these is the better-controlled stability that enables effective long-range modeling through careful initializations of state space layer parameters [11; 31], with the most representative method being HiPPO , a theory-driven framework notable for its optimal memory compression on continuous sequence inputs. Moreover, the computational efficacy of SSMs is notably enhanced through the use of techniques such as convolutions [13; 6] or parallel scans . The promising properties of SSMs also attracts further explorations on graphs .

Comparison.The usual paradigms for designing sequence models over graphs involve recurrence (e.g. RNNs ), integrate-and-fire (e.g. SNNs [38; 8]), or attention (e.g. Transformers ), which each come with tradeoffs . For example, RNNs are a natural recurrence model for sequential modeling that require only constant computation/storage per time step, but are slow to train and suffer from the rapid forgetting issue. This empirically limits their ability to handle long sequences. SNNs share a similar recurrent architecture with RNNs while using 1-bit spikes to transmit temporal information, which would sacrifice expressivity and potentially suffer from optimization difficulties (e.g., the "vanishing gradient problem") . Transformers encode local context via attention mechanism and enjoy fast, parallelizable training, but are not sequential, resulting in more expensive inference and an inherent limitation on the context length. Compared to the aforementioned architectures, SSMs particularly the promising Mamba (S\(6\)) model , offer advantages such as fast training and inference, along with fewer parameters and comparable performance. These characteristics make SSMs particularly well-suited for sequence modeling, even (or especially) on extremely long sequences. Comparisons among these architectures are illustrated in table 1

## 3 The GraphSSM framework

The primary motivation of our framework is the fact that discrete-time temporal graphs are sequential observations of an underlying temporal graph that evolves continuously. Adopting this functional viewpoint, we will first develop a piecewise recurrent memory update scheme in section 3.1 that optimally approximates the underlying continuous-time temporal graph, utilizing a novel extension of the HiPPO abstraction to graph-typed inputs . The proposed framework retains many nice properties of HiPPO while posing the new challenge of _unobserved graph mutation_ when handling discretely-observed observations, which we analyze in section 3.2 and propose a mixing mechanism to improve the recurrent approximation. Finally, we present GraphSSM framework in section 3.3. An overview of GraphSSM is shown in figure 1.

    & **RNNs**[46; 4; 18] & **SNNs**[38; 8] & **Transformers** & **SSMs** (S\(6\)) \\ 
**Training** & Slow & Slow & Fast & Fast \\
**Inference** & Fast & Fast & Slow & Fast \\
**Para. Size** & Medium & Extremely small & Large & Small \\
**Performance** & \(\)\(\)\(\) & \(\)\(\)\(\) & \(\)\(\)\(\)\(\)\(\) & \(\)\(\)\(\)\(\)\(\)\(\)\(\) \\
**Limitations** & Forgetting & Vanishing gradients & Mem. \& Time: O(n\({}^{2}\)) &? \\   

Table 1: Comparisons of different neural network architectures for sequence modeling.

Figure 1: GraphSSM framework.

### GHiPPO: HiPPO on temporal graphs

**Setup.** We fix a time interval \([0,T]\). A temporal graph on \([0,T]\) is characterized by two _processes_\(G\) and \(X\): For each \(t[0,T]\), the process \(G\) maps \(t\) to a graph object \(G(t)=(V(t),E(t))\). We assume the node process \(V(t)\) to be fixed over time, i.e., \(V(t) V,t[0,T]\) with \(N_{V}=|V|\) and discuss the case for varying node processes in appendix B.2. The edge process \(E(t)\) is a piecewise-constant process with a finite number \(M\) of mutations over \([0,T]\) that are described via a sequence of _events_:

\[=\{_{1},,_{M}\}_{m}=(u_{m},v_{m},t_{m},a_{m}),1 m M.\] (1)

Each event \(_{m}\) constitutes an interaction between node pair \((u_{m},v_{m})\) at time \(t_{m}\) with action \(a_{m}\), the action could be either insertion or deletion. The evolution process is thus depicted as the following:

\[G(0)_{1}}G(t_{1})_{2}}G(t_{2 }) G(t_{M-1})_{M} }G(t_{M})=G(T).\] (2)

The process \(X\) maps \(t\) to a node feature matrix \(X(t)^{N_{V} d}\) with feature dimension \(d\). Throughout this paper, it is often helpful to view \(G\) and \(X\) as graph-valued and matrix-valued _functions_. In typical discrete-time temporal graph learning problems, the underlying graph is observed at timestamps \(_{1},,_{L}\) with time gaps \(_{l}=_{l}-_{l-1},2 l L\). The observations thus form a sequence of snapshots \(\{G(_{l}),X(_{l})\}_{1 l L}\) which are abbreviated as \(\{G_{1:L},X_{1:L}\}\). Notably, the observation times are usually _interleaved with_ the mutation times, resulting in the majority of mutation times remain unobserved. This situation presents significant challenges in effectively modeling the dynamics of graph evolution, a topic that will be further explored subsequently.

**The HiPPO abstraction.** Algorithmically, the goal of continuous-time dynamic modeling is to design a _memory module_ that optimally compresses all the historical information . Under the context of univariate sequence modeling, the HiPPO framework  formalizes the memory compression problem into an online approximation problem in some function space and derives HiPPO operators under specific types of basis functions, among which the HiPPO-LegS configuration has become the state-of-the-art in state-space sequence modeling paradigms . However, naively extending HiPPO abstraction to graph learning scenarios (via treating node features as inputs) could be deemed inadequate since HiPPO handles distinct inputs _independently_, without the capability to incorporate the interconnectivity information among various inputs which could potentially enhance the efficiency of memory compression. For illustrative purposes, in instances where input observations are noisy, the exploitation of neighborhood information has the potential to facilitate a denoising step, as evidenced in image processing applications  and semi-supervised learning primitives . To systematically utilize the connectivity information, we propose a new approximation paradigm, the _Laplacian-regularized online approximation_ that extends HiPPO to graph modeling frameworks. Formally, we start with the simple setup with \(d=1\), i.e., each node possesses a scalar feature, and we propose an approximation scheme that simultaneously approximates the history of all the \(N_{V}\) inputs up until time \(t\), i.e., \(\{X(s),s[0,t]\}\) using their corresponding memories at time \(t\), i.e., \(Z(t)=\{z_{v}(t)\}_{v V}^{N_{V} 1}\) according to the following objective at time \(t\):

\[_{t}(Z;G,X,)=_{0}^{t}\|X(s)-Z(s)\|_{2}^{2}d_{ t}(s)+_{0}^{t}Z(s)^{}L(s)Z(s)d_{t}s.\] (3)

Here \(>0\) is a balancing constant, \(_{t}\) is a time-dependent measure that is supported on the interval \([0,t]\) which controls the importance of various parts of the input domain3 and \(L(t)\) is a normalized Laplacian at time \(t\), which allows definition such as the symmetric normalized Laplacian \(L_{}(s)=I-D(s)^{-1/2}A(s)D(s)^{-1/2}\) where \(D(s)\) is a diagonal matrix whose diagonals are node degrees, or random walk normalized Laplacian \(L_{}(s)=I-D(s)^{-1}A(s)\). The objective (3) is understood as the ordinary HiPPO approximation objective augmented with a regularization component that encourages the _smoothness_ of memory compression with respect to adjacent nodes. 4 The imposition of smoothness constraints commonly emerges as a beneficial relational inductive bias in the context of graph learning . By leveraging the data from adjacent nodes, one can potentially achieve a more effective denoising effect during the process of node memory compression. To specifya suitable approximation subspace for memories \(Z\), we adopt the approach of HiPPO that uses some \(N\)-dimensional subspace of polynomials which we denote as \(_{N}\). Now we define a _graph memory projection operator_\(_{t}\) that maps the temporal graph up until time \(t\) to a collection of \(N_{V}\) polynomials with each one lies in \(_{N}\), i.e.,

\[_{t}(G,X)=*{arg\,min}_{Z:z_{v} _{N}} v V_{t}(Z;G,X,).\] (4)

We further define a _coefficient_ operator \(_{t}\) that maps each polynomial in the collection in (4) to the coefficients of the basis of orthogonal polynomials defined with respect to \(_{t}\), the following definition formalizes our extension of HiPPO to continuous-time temporal graphs which we term GHiPPO:

**Definition 1** (GhiPPO).: _Given a continuous-time temporal graph \((G,X)\), a time-varying measure family \(_{t}\), an \(N\)-dimensional subspace of polynomials \(_{N}\), the GHiPPO operator at time \(t\) is the composition of \(_{t}\) and \(_{t}\) that maps the temporal graph and node features to a collection of projection coefficients \(U(t)^{N_{V} N}\), or GHiPPO \((G,X)=_{t}(_{t}(G,X))\)._

The most favorable property of the HiPPO framework on independent inputs is that the outputs of HiPPO operators are characterized via a concise ordinary differential equation (ODE) that takes the form of a linear time-invariant state space model (LTI-SSM). The following theorem states that most of the desirable properties of HiPPO are retained by GHiPPO except for the LTI property:

**Theorem 1**.: _Let \(G\) evolve according to (2). Taking \(_{t}\) to be the scaled Legendre measure (LegS) with \(_{t}=_{[0,t]}\) where \(_{[0,t]}\) stands for the indicator function of the interval \([0,t]\), the evolution of the outputs of GHiPPO operator is characterized by \(M\) ODEs according to mutation times as follows:_

\[=U(t)A^{}+(I+ L(t))^{-1}X(t)B^{}, 1 m  M,t[t_{m-1},t_{m})\] (5)

_where \(A^{N N}\) and \(B^{N 1}\) takes the same form as in the HiPPO formulation :_

\[A_{nk}=-&n>k,\\ n+1&n=k,\\ 0&n<k, B_{n}=,1 n  N.\] (6)

According to theorem 1, the solution (5) is LTI over each interval \([t_{m},t_{m+1})\) during which the graph structure remains fixed. This property further extends to a piecewise LTI perspective over the interval \([0,T]\). Moreover, we may view the solution (5) as a two-stage procedure that could be intuitively described as _diffuse-then-update_. Specifically, this procedure entails a sequential execution, wherein an initial diffusion operation is applied to the features of the input nodes, succeeded by an update to the memory of these nodes.

### Unobserved graph mutations and mixed discretization

Theorem 1 establishes an analogue of HiPPO theory on temporal graphs. It is straightforward to verify that most of the subsequent refinements of HiPPO apply to GHiPPO as well. Among

Figure 2: Illustrative example of the _unobserved graph mutation_ issue. In this example, the underlying graph is observed at time points \(_{1},_{2},_{3}\) with two unobserved mutations between \([_{1},_{2})\) and one between \([_{2},_{3})\). These unobserved mutations result in temporal dynamics that are inconsistent across the observed intervals, thereby complicating direct applications of ODE discretization methods such as the Euler method or the zero-order hold (ZOH) method.

these we will utilize the popular technique of _diagonal state spaces_ that simply sets \(A\) as a diagonal matrix with negative diagonal elements5. To apply the GHiPPO framework to discrete-time temporal graphs, a critical step is to develop a discretized version of (5). However, unlike ordinary HiPPO where we can use standard discretization techniques of ODEs to discretize LTI equations, the GHiPPO ODE contains discontinuities that correspond to mutation times of the underlying temporal graph, which are often not observed given only access to a list of snapshots. This issue of _unobserved dynamics_ complicates the development of a viable discretization scheme for GHiPPO, as is pictorially illustrated in figure 2. To devise a solution to this challenge, we start by analyzing a hypothetical _oracle scenario_ in which all mutations are observable.

**An oracle discretization.** We consider a time range \([_{l-1},_{l})\) between the \(l-1\)th and the \(l\)th snapshot, and assume there are altogether \(M_{l}\) mutation events \(\{_{l,i}\}_{1 i M_{l}}\) happened during this period. Let \(G_{l,0}=G_{l-1}\) be the graph snapshot at \(_{l-1}\), the following process describes the structural evolution inside the interval \([_{l-1},_{l})\):

\[G_{l-1}=G_{l,0}_{l,1}}G_{l,1}_{l,2}}G_{l,2} G_{l,M_{l}-1}_{l,M_{l}}}G_{l,M_{l}}=G_{l}\] (7)

Next, we derive a discretization formula under the strategy of zeroth-order-hold (ZOH). We assume that all intermediate mutations are observed, with the node features staying fixed between mutations, i.e., \(X(t) X_{l,i},t[t_{l,i-1},t_{l,i})\). The following theorem characterizes the resulting state evolution:

**Theorem 2** (Oracle discretization of (5)).: _Assume \(A\) is a diagonal matrix with negative diagonals, for any \(1 l L\). Let \(L_{l,i}\) be some Laplacian of \(G_{l,i}\), we have the following oracle update rule:_

\[U_{l}=U_{l-1}e^{_{l}A}+_{l}(e^{_{l}A}-I)A ^{-1},\ _{l}=_{i=0}^{M_{l}}(I+ L_{l,i})^{-1}X_{l,i}_{i}B ^{},\] (8)

_where \(U_{l}^{N_{V} N}\) denotes the discretized state at step \(l\) with \(U_{0}=0\). For each \(1 l L,0 i M_{l}\), \(_{i}^{N N}\) are non-negative diagonal matrices with values depending only on the mutation times, which satisfy \(_{i=0}^{M_{l}}_{i}=I\)._

**Mixed discretization.** According to (8), given all the (unobserved) mutation information, the state update rule is equivalent to applying ZOH to \(_{l}\) which is an _element-wise convex combination_ of all the diffused node features. In practice, among all the components of \(\), we only have access to \(X_{l-1},X_{l},G_{l-1},G_{l}\) with the rest left unobserved. Therefore, we propose _mixed discretization_ as an approach to approximate \(_{l}\). Specifically, we introduce the following mechanisms:

\[_{l}^{} =_{}(X_{l},G_{l}),\] (ordinary ZOH) \[_{l}^{} =_{}(_{}(X_{l-1},X_{l} ),G_{l}),\] (feature mixing) \[_{l}^{} =_{}(_{}(X_{l-1},G_{l-1 }),_{}(X_{l},G_{l})),\] (representation mixing)

which are compositions of inter-node mixing (a consequence of diffusion) and intra-node mixing (mixing node features of consecutive snapshots). For the process of inter-node mixing, we opt to approximate the diffusion operation with a learnable shallow graph neural network (typically a \(1\)-layer GNN) parameterized by \(\) to alleviate the computation burden and improve flexibility6. A detailed discussion considering the relation between certain GNN formulations and the choice of Laplacian is presented in appendix B.1. In the context of intra-node mixing, we introduce a Mix module parameterized by \(\) to merge either consecutive node features (as illustrated in (feature mixing)) or consecutive node representations produced by the GNN model (as illustrated in (representation mixing)). In this paper, we assess two simple Mix instantiations: Convolution with a kernel size of \(2\) (Conv1D) and a gating mechanism that interpolates between the two inputs (Interp). We postpone a comprehensive description of the mixing methods to appendix D.1. The resulting discretized system is presented as the following matrix-valued state space model:

\[U_{l} =U_{l-1}e^{_{l}A}+_{l}_{l}^{()}B^{ }_{l}^{()}\{_{l}^{()}, _{l}^{()},_{l}^{()}\},1 l  L.\] (9) \[Y_{l} =U_{l}C^{}.\]When exact timestamps for snapshots are unavailable, we use the adaptive time step strategy as in [11; 10] that models \(\) a \(1\)-dimensional affine projection of the inputs followed by a non-negative activation like softplus. Finally, we utilize the approximation \(A^{-1}(e^{ A}-I) I\) for diagonal \(A\)s, and equip the system with an output \(Y\) with a state projection matrix \(C^{N 1}\).

### The GraphSSM framework

Having established the SSM equation (9), we are ready to introduce our main framework GraphSSM. In alignment with conventional design paradigms in the SSM literature, we define a depth-\(K\) GraphSSM model through the sequential composition of \(K\) GraphSSM blocks, with each block characterized as follows:

\[H_{1:L}^{(k)}=((H_{1:L}^{(k-1)},G_{1:L} ))+(H_{1:L}^{(k-1)}),1 k K,\] (10)

where we use \(H_{1:L}^{(k)}\) to denote the concatenation of the hidden representation at depth \(k\) of all the snapshots along the sequence dimension and \(H_{1:L}^{(0)}\) are the node features \(X_{1:L}\). The GraphSSM blocks, as outlined in (10), incorporate an SSM layer that operates on graph snapshot inputs. This is followed by the application of a nonlinear activation \(\) and the integration of a residual connection which we denote as the addition of a linear projection of inputs with Linear denotes a linear projection layer that ensures dimension compatibility.

**GraphSSM-S4.** The architectural formulation of the SSM layer essentially involves the expansion of the one-dimensional recurrence, as specified in (9), to accommodate general dimensions, i.e., \(d>1\). This expansion is achieved in a straightforward manner by utilizing an individual SSM for each dimension. Consequently, the emergent SSM layer adopts a Single-Input, Single-Output (SISO) configuration. Such a design is intuitively understood as the graph learning analogue of S4 , which we term GraphSSM-S4.

**GraphSSM-S5 and GraphSSM-S6**. In addition to the SISO implementation, we further introduce two variants within the GraphSSM framework. The first alternative represents a Multiple-Input, Multiple-Output (MIMO) extension of (9), wherein a single SSM system is applied across all dimensions. This variant serves as a graph-informed analogue to the S5 model . The second variant extends the S4 model by facilitating input-controlled time intervals and state matrices (\(\), \(B\), and \(C\)). This innovation yields a selective state space model, drawing parallels to the latest SSM architectures such as S6 .

A detailed exposition of the GraphSSM-S4 (resp. GraphSSM-S5, GraphSSM-S6) layer is provided in algorithm 1 (resp. algorithm 2, algorithm 3) in appendix D.2. The overall end-to-end architecture is briefly illustrated in figure 1, where we use feature mixing as the mixing mechanism for illustration.

**Remark 1** (Choice of mixing mechanisms).: _In the GraphSSM architecture, each SSM layer incorporates a mixing mechanism. Based on our empirical investigations, we have observed that employing more sophisticated mixing strategies such as (feature mixing) and (representation mixing), yields benefits predominantly when these are applied exclusively to the lowermost layer. Specifically, this entails utilizing either \(_{l}^{(F)}\) or \(_{l}^{(R)}\) configurations in the initial layer, while defaulting to \(_{l}^{(Q)}\) for the layers that follow. An intuitive rationale behind this strategic layer-specific choice will be elucidated in appendix B.3._

## 4 Experiments

This section presents our key experimental findings on the temporal node classification task. Also, ablation studies of the key design choices are presented. Due to space limitation, the detailed experimental settings are deferred to appendix F.

### Experimental results

Node classification performance.The node classification performance of all methods is presented in table 2. It has been observed that graph embedding methods, especially static ones, tend to underperform in most cases. This is expected since these methods are typically trained in an 

[MISSING_PAGE_FAIL:8]

SSM architectures.As GraphSSM is a general framework that generalizes SSMs to temporal graphs, we conduct experiments on extending GraphSSM with different ad-hoc SSMs, including S5  and S6 . The node classification results on four datasets are shown in table 4. By comparing different variants of GraphSSM, we can find that S4 is the best architecture for learning over temporal graph sequences. \(5\), being a simplified version of S4 with fewer parameters, achieves poor performance on all datasets. Notably, while S6 shows impressive performance in other modalities such as language or images [10; 51], it is observed that they underperform when applied to graph sequences. This indicates that the selective mechanism may not be a good fit for graph data.

Mixing mechanism.We assess the effectiveness of various mixing mechanisms introduced in section 3.2 through a series of experiments conducted using the S\(4\) variant of GraphSSM. The analysis spans four distinct configurations: no intra-node mixing (\(_{1}^{()}+_{2}^{()}\)), feature mixing at the first layer (\(_{1}^{()}+_{2}^{()}\)), and representation mixing at either the first (\(_{1}^{()}+_{2}^{()}\)) or second (\(_{1}^{()}+_{2}^{()}\)) layers. The findings, presented in table 5, indicate that the integration of the Mix module at the first layer generally leads to enhanced model performance. An intuitive explanation for this observed phenomenon is elaborated in appendix B.3.

Initialization strategy.Recent advancements have highlighted the crucial role of initialization in SSMs , prompting our investigation into the effects of various initialization strategies for the \(A\) matrix. Specifically, we explore "hippo", "constant", and "random" initializations, with their comprehensive definitions provided in appendix D.2. The result, as shown in figure 3 exhibits distinct performance variations across different initialization strategies, with HiPPO being typically the dominant one which corroborates our theoretical motivations.

## 5 Conclusion

In this work, we introduce a conceptualized GHiPPO abstraction on temporal graphs. Building upon GHiPPO, we propose GraphSSM, a theoretically motivated state space framework for modeling temporal graphs derived from a novel memory compression scheme. The proposed framework is computationally efficient and versatile in its design, which is further corroborated by strong empirical performance across various benchmark datasets. We also point out the unobserved graph mutation issue in temporal graphs and propose different mixing mechanisms to ensure temporal continuity across consecutive graph snapshots. Despite the promising results, the applicability of GraphSSM is presently confined to discrete-time temporal graphs. A discussion of our framework's current limitations and the scope for future extensions is presented in appendix E.