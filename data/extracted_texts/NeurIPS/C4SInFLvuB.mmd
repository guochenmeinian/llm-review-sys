# Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization

Thomas Nagler

t.nagler@lmu.de

Equal contribution.

Lennart Schneider

Equal contribution.

Bernd Bischl

Matthias Feurer

Department of Statistics, LMU Munich

Munich Center for Machine Learning (MCML)

###### Abstract

Hyperparameter optimization is crucial for obtaining peak performance of machine learning models. The standard protocol evaluates various hyperparameter configurations using a resampling estimate of the generalization error to guide optimization and select a final hyperparameter configuration. Without much evidence, paired resampling splits, i.e., either a fixed train-validation split or a fixed cross-validation scheme, are often recommended. We show that, surprisingly, reshuffling the splits for every configuration often improves the final model's generalization performance on unseen data. Our theoretical analysis explains how reshuffling affects the asymptotic behavior of the validation loss surface and provides a bound on the expected regret in the limiting regime. This bound connects the potential benefits of reshuffling to the signal and noise characteristics of the underlying optimization problem. We confirm our theoretical results in a controlled simulation study and demonstrate the practical usefulness of reshuffling in a large-scale, realistic hyperparameter optimization experiment. While reshuffling leads to test performances that are competitive with using fixed splits, it drastically improves results for a single train-validation holdout protocol and can often make holdout become competitive with standard CV while being computationally cheaper.

## 1 Introduction

Hyperparameters have been shown to strongly influence the performance of machine learning models (van Rijn and Hutter, 2018; Probst et al., 2019). The primary goal of hyperparameter optimization (HPO; also called tuning) is the identification and selection of a hyperparameter configuration (HPC) that minimizes the estimated generalization error (Feurer and Hutter, 2019; Bischl et al., 2023). Typically, this task is challenged by the absence of a closed-form mathematical description of the objective function, the unavailability of an analytic gradient, and the large cost to evaluate HPCs, categorizing HPO as a noisy, black-box optimization problem. An HPC is evaluated via resampling, such as a holdout split or \(M\)-fold cross-validation (CV), during tuning.

These resampling splits are usually constructed in a fixed and instantiated manner, i.e., the same training and validation splits are used for the internal evaluation of all configurations. On the one hand, this is an intuitive approach, as it should facilitate a fair comparison between HPCs and reduce the variance in the comparison.1 On the other hand, such a fixing of train and validation splits might steer the optimization, especially after a substantial budget of evaluations, towards favoring HPCswhich are specifically tailored to the chosen splits. Such and related effects, where we "overoptimize" the validation performance without effective reward in improved generalization performance have been sometimes dubbed "overtuning" or "oversearching". For a more detailed discussion of this topic, including related work, see Section 5 and Appendix B. The practice of reshuffling resampling splits during HPO is generally neither discussed in the scientific literature nor HPO software tools.2 To the best of our knowledge, only Levesque (2018) investigated reshuffling train-validation splits for every new HPC. For both holdout and \(M\)-fold CV using reshuffled resampling splits resulted in, on average, slightly lower generalization error when used in combination with Bayesian optimization (BO, Garnett, 2023) or CMA-ES (Hansen & Ostermeier, 2001) as HPO algorithms. Additionally, reshuffling was used by a solution to the NeurIPS 2006 performance prediction challenge to estimate the final generalization performance (Guyon et al., 2006). Recently, in the context of evolutionary optimization, reshuffling was applied after every generation (Larcher & Barbosa, 2022).

In this paper, we systematically examine the effect of reshuffling on HPO performance. Our contributions can be summarized as follows:

1. We show theoretically that reshuffling resampling splits during HPO can result in finding a configuration with better overall generalization performance, especially when the loss surface is rather flat and its estimate is noisy (Section 2).
2. We confirm these theoretical insights through controlled simulation studies (Section 3).
3. We demonstrate in realistic HPO benchmark experiments that reshuffling splits can lead to a real-world improvement of HPO (Section 4). Especially in the case of reshuffled holdout, we find that the final generalization performance is often on par with 5-fold CV under a wide range of settings.

We discuss results, limitations, and avenues for future research in Section 5.

## 2 Theoretical Analysis

### Problem Statement and Setup

Machine learning (ML) aims to fit a model to data, so that it generalizes well to new observations of the same distribution. Let \(=\{_{i}\}_{i=1}^{n}\) be the observed dataset consisting of _i.i.d._ random variables from a distribution \(P\), i.e., in the supervised setting \(_{i}=(_{i},Y_{i})\).3\({}^{,}\)4 Formally, an inducer \(g\) configured by an HPC \(\) maps a dataset \(\) to a model from our hypothesis space \(h=g_{}()\). During HPO, we want to find a HPC that minimizes the expected generalization error, i.e., find

\[^{*}=_{}(), ()=[(,g_{}( ))],\]

where \((,h)\) is the loss of model \(h\) on a fresh observation \(\). In practice, there is usually a limited computational budget for each HPO run, so we assume that there is only a finite number of distinct HPCs \(=\{_{1},,_{J}\}\) to be evaluated, which also simplifies the subsequent analysis. Naturally, we cannot optimize the generalization error directly, but only an estimate of it. To do so, a resampling is constructed. For every HPC \(_{j}\), draw \(M\) random sets \(_{1,j},,_{M,j}\{1,,n\}\) of validation indices with \(n_{ valid}= n\) instances each. The random index draws are assumed to be independent of the observed data. The data is then split accordingly into pairs \(_{m,j}=\{_{i}\}_{i_{m,j}},_{m,j}= \{_{i}\}_{i_{m,j}}\) of disjoint validation and training sets. Define the validation loss on the \(m\)-th fold

\[L(_{m,j},g_{_{j}}(_{m,j}))=}_{i_{m,j}}(_{i},g_{_{j}}(_{m,j})),\]and the \(M\)-fold validation loss as

\[(_{j})=_{m=1}^{M}L(_{m,j},g_{ _{j}}(_{m,j})).\]

Since \(\) is unknown, we minimize \(}=_{}()\), hoping that \((})\) will also be small. Typically, the same splits are used for every HPC, so \(_{m,j}=_{m}\) for all \(j=1,,J\) and \(m=1,,M\). In the following, we investigate how reshuffling train-validation splits (i.e., \(_{m,j}_{m,j^{}}\) for \(j j^{}\)) affects the HPO problem.

### How Reshuffling Affects the Loss Surface

We first investigate how different validation and reshuffling strategies affect the empirical loss surface \(\). In particular, we derive the limiting distribution of the sequence \(((_{j})-(_{j}))_{j=1}^{J}\). This limiting regime will not only reveal the effect of reshuffling on the loss surface, but also give us a tractable setting to study HPO performance.

**Theorem 2.1**.: _Under regularity conditions stated in Appendix C.1, it holds_

\[((_{j})-(_{j}))_{ j=1}^{J}(0,),\]

_where_

\[_{i,j}=_{i,j,M}K(_{i},_{j}),_{i,j, M}=_{n}^{2}}_{s=1}^{n}_{m=1}^{M} _{m^{}=1}^{M}(s_{m,i}_{m^{}, j}),\]

_and_

\[K(_{i},_{j})=_{n}[_ {n}(^{},_{i}),_{n}(^{},_{j})],_{n}(,)=[(,g_{}())]-[(,g_{}( ))],\]

_where the expectation is taken over a training set \(\) of size \(n\) and two fresh samples \(,^{}\) from the same distribution._

The regularity conditions are rather mild and discussed further in Appendix C.1. The kernel \(K\) reflects the (co-)variability of the losses caused by validation samples. The contribution of training samples only has a higher-order effect. The validation scheme enters the distribution through the quantities \(_{i,j,M}\). In what follows, we compute explicit expressions for some popular examples. The following list provides formal definitions for the index sets \(_{m,j}\).

1. (holdout) Let \(M=1\) and \(_{1,j}=_{1}\) for all \(j=1,,J\), and some size-\( n\) index set \(_{1}\).
2. (reshuffled holdout) Let \(M=1\) and \(_{1,1},,_{1,J}\) be independently drawn from the uniform distribution over all size-\( n\) subsets from \(\{1,,n\}\).
3. (\(M\)-fold CV) Let \(=1/M\) and \(_{1},,_{M}\) be a disjoint partition of \(\{1,,n\}\), and \(_{m,j}=_{m}\) for all \(j=1,,J\).
4. (reshuffled \(M\)-fold CV) Let \(=1/M\) and \((_{1,j},,_{M,j}),j=1,,J\), be independently drawn from the uniform distribution over disjoint partitions of \(\{1,,n\}\).
5. (\(M\)-fold holdout) Let \(_{m},m=1,,M\), be independently drawn from the uniform distribution over size-\( n\) subsets of \(\{1,,n\}\) and set \(_{m,j}=_{m}\) for all \(m=1,,M,j=1,,J\).
6. (reshuffled \(M\)-fold holdout) Let \(_{m,j},m=1,,M,j=1,,J\), be independently drawn from the uniform distribution over size-\( n\) subsets of \(\{1,,n\}\).

The value of \(_{i,j,M}\) for each example is computed explicitly in Appendix E. In all these examples, we in fact have

\[_{i,j,M}=^{2},&i=j\\ ^{2}^{2},&i j.,\] (1)

for some method-dependent parameters \(,\) shown in Table 1. The parameter \(^{2}\) captures any increase in variance caused by omitting an observation from the validation sets. The parameter \(\) quantifies a potential decrease in correlation in the loss surface due to reshuffling. More precisely,the observed losses \((_{i}),(_{j})\) at distinct HPCs \(_{i}_{j}\) become less correlated when \(\) is small. Generally, an increase in variance leads to worse generalization performance. The effect of a correlation decrease is less obvious and is studied in detail in the following section.

We make the following observations about the differences between methods in Table 1:

* \(M\)-fold CV incurs no increase in variance (\(^{2}=1\)) and -- because every HPC uses the same folds -- no decrease in correlation. Interestingly, the correlation does not even decrease when reshuffling the folds. In any case, all samples are used exactly once as validation and training instance. At least asymptotically, this leads to the same behavior, and reshuffling should have almost no effect on \(M\)-fold CV.
* The two (1-fold) holdout methods bear the same \(1/\) increase in variance. This is caused by only using a fraction \(\) of the data as validation samples. Reshuffled holdout also decreases the correlation parameter \(^{2}\). In fact, if HPCs \(_{i}_{j}\) are evaluated on largely distinct samples, the validation losses \((_{i})\) and \((_{j})\) become almost independent.
* \(M\)-fold holdout also increases the variance, because some samples may still be omitted from validation sets. This increase is much smaller for large \(M\). Accordingly, the correlation is also decreased by less in the reshuffled variant.

### How Reshuffling Affects HPO Performance

In practice, we are mainly interested in the performance of a model trained with the optimal HPC \(}\). To simplify the analysis, we explore this in the large-sample regime derived in the previous section. Assume

\[(_{j})=(_{j})+(_{ j})\] (2)

where \(()\) is a zero-mean Gaussian process with covariance kernel

\[((),(^{}))= K(,)&=^{},\\ ^{2}K(,^{})&\] (3)

Let \(\{^{d}\|\| 1\}\) with \(||=J<\) be the set of hyperparameters. Theorem 2.2 ahead gives a bound on the expected regret \([(})-(^{*})]\). It depends on several quantities characterizing the difficulty of the HPO problem. The constant

\[=_{\|\|,\|^{}\| 1},)-K(,^{})|}{K(,)\|-^{}\|^{2}}.\]

can be interpreted as a measure of correlation of the process \(\). In particular, \(((),(^{})) 1- \|-^{}\|^{2}.\) The constant is small when \(\) is strongly correlated, and large otherwise. Further, define \(\) as the minimal number such that any \(\)-ball contained in \(\{\|\| 1\}\) contains at least one element of \(\). It measures how densely the set of candidate HPCs \(\) covers set of all possible HPCs. If \(\) is a deterministic uniform grid, we have about \( J^{-1/d}\). Similarly, Lemma D.1 in the Appendix shows that \( J^{-1/2d}\) when randomly sampling HPCs. Finally, the constant

\[m=_{})-(^{*}) |}{\|-^{*}\|^{2}},\]

   Method & \(^{2}\) & \(^{2}\) \\  holdout (HO) & \(1/\) & \(1\) \\ reshuffled HO & \(1/\) & \(\) \\ \(M\)-fold CV & \(1\) & \(1\) \\ reshuffled \(M\)-fold CV & \(1\) & \(1\) \\ \(M\)-fold HO (subsampling / Monte Carlo CV) & \(1+(1-)/M\) & \(1\) \\ reshuffled \(M\)-fold HO & \(1+(1-)/M\) & \(1/(1+(1-)/M)\) \\   

Table 1: Exemplary parametrizations in Equation (1) for resamplings; see Appendix E for details.

measures the local curvature at the minimum of the loss surface \(\). Finding an HPC \(\) close to the theoretical optimum \(^{*}\) is easier when the minimum is more pronounced (large \(m\)). On the other hand, the regret \(()-(^{*})\) is also punishing mistakes more quickly. Defining \((x)_{+}=\{0,(x)\}\), we can now state our main result.

**Theorem 2.2**.: _Let \(\) follow the Gaussian process model (2). Suppose \(<\), \(0<^{2}[()]^{2 }<\) for all \(\), and \(m>0\). Then_

\[[(})-(^{*})] [8+B()-A()].\]

_where_

\[B()=48[}+} ], A()=}(/)})_{+}}.\]

The numeric constants result from several simplifications in a worst-case analysis, which lowers their practical relevance. A qualitative analysis of the bound is still insightful. The bound is increasing in \(\) and \(d\), indicating that the HPO problem is harder when there is a lot of noise or there are many parameters to tune. The terms \(B()\) and \(A()\) have conceptual interpretations:

* The term \(B()\) quantifies how likely it is to pick a bad \(}\) because of bad luck: a \(\) far away from \(^{*}\) had such a small \(()\) that it outweighs the increase in \(\). Such events are more likely when the process \(\) is weakly correlated. Accordingly, \(B()\) is decreasing in \(\) and increasing in \(\).
* The term \(A()\) quantifies how likely it is to pick a good \(}\) by luck: a \(\) close to \(^{*}\) had such a small \(()\) that it overshoots all the other fluctuations. Also such events are more likely when the process \(\) is weakly correlated. Accordingly, the term \(A()\) is decreasing in \(\).

The \(B\), as stated, is unbounded, but a closer inspection of the proof shows that it is upper bounded by \(\). This bound is attained only in the unrealistic scenario when the validation losses are essentially uncorrelated across all HPCs. The term \(A\) is bounded from below by zero, which is also the worst case because the term enters our regret bound with a negative sign.

Both \(A\) and \(B\) are decreasing in the reshuffling parameter \(\). There are two regimes. If \(/2m^{2} e\), then \(A()=0\) and reshuffling cannot lead to an improvement of the bound. The term \(/m^{2}\) can be interpreted as noise-to-signal ratio (relative to the grid density). If the signal is much stronger than the noise, the HPO problem is so easy that reshuffling will not help. This situation is illustrated in Figure 0(a).

If on the other hand \(/m^{2}>e\), the terms \(A()\) and \(B()\) enter the bound with opposing signs. This creates tension: reshuffling between HPCs increases \(B()\), which is countered by a decrease in \(A()\). So which scenarios favor reshuffling? When the process \(\) is strongly correlated, \(\) is small and reshuffling (decreasing \(\)) incurs a high cost in \(B()\). This is intuitive: When there is strong

Figure 1: Example of reshuffled empirical loss yielding a worse (left) and better (right) minimizer.

correlation, the validation loss surface \(\) is essentially just a vertical shift of \(\). Finding the optimal \(\) is then almost as easy as if we would know \(\), and decorrelating the surface through reshuffling would make it unnecessarily hard. When \(\) is less correlated (\(\) large) however, reshuffling does not hurt the term \(B()\) as much, but we can reap all the benefits of increasing \(A()\). Here, the effect of reshuffling can be interpreted as hedging against the catastrophic case where all \(()\) close to the optimal \(^{*}\) are simultaneously dominated by a region of bad hyperparameters. This is illustrated in Figure 0(b).

## 3 Simulation Study

To test our theoretical understanding of the potential benefits of reshuffling resampling splits during HPO, we conduct a simulation study. This study helps us explore the effects of reshuffling in a controlled setting.

### Design

We construct a univariate quadratic loss surface function \(:, m(-0.5)^{2}/2\) which we want to minimize. The global minimum is given at \((0.5)=0\). Combined with a kernel for the noise process \(\) as in Equation (3), this allows us to simulate an objective as observed during HPO by sampling \(()=()+()\). We use a squared exponential kernel \(K(,^{})=_{K}^{2})^{2}/2)}\) that is plugged into the covariance kernel of the noise process \(\) in Equation (3). The parameters \(m\) and \(\) in our simulation setup correspond exactly to the curvature and correlation constants from the previous sections. Recall that Theorem 2.2 states that the effect of reshuffling strongly depends on the curvature \(m\) of the loss surface \(\) (a larger \(m\) implies a stronger curvature) and the constant \(\) as a measure of correlation of the noise \(\) (a larger \(\) implies weaker correlation). Combined with the possibility to vary \(\) in the covariance kernel of \(\), we can systematically investigate how curvature of the loss surface, correlation of the noise and the extent of reshuffling affect optimization performance. In each simulation run, we simulate the observed objective \(()\), identify the minimizer \(=_{}()\), and calculate its true risk, \(()\). We repeat this process \(10000\) times for various combinations of \(\), \(m\), and \(\).

### Results

Figure 2 visualizes the true risk of the configuration \(\) that minimizes the observed objective. We observe that for a loss surface with low curvature (i.e., \(m 2\)), reshuffling is beneficial (lower values of \(\) resulting in a better true risk of the configuration that optimizes the observed objective) as long as the noise process is not too correlated (i.e., \( 1\)). As soon as the noise process is more strongly correlated, even flat valleys of the true risk \(\) remain clearly visible in the observed risk \(\), and reshuffling starts to hurt the optimization performance. Moving to scenarios of high curvature, the general relationship of \(m\) and \(\) remains the same, but reshuffling starts to hurt optimization performance already with weaker correlation in the noise. In summary, the simulations show that in cases of low curvature of the loss surface, reshuffling (reducing \(\)) tends to improve the true risk of the optimized configuration, especially when the loss surface is flat (small \(m\)) and the noise is not strongly correlated (i.e., \(\) is large). This exactly confirms our theoretical predictions from the previous section.

## 4 Benchmark Experiments

In this section, we present benchmark experiments of real-world HPO problems where we investigate the effect of reshuffling resampling splits during HPO. First, we discuss the experimental setup. Second, we present results for HPO using random search (Bergstra & Bengio, 2012). Third, we also show the effect of reshuffling when applied in BO using HEBO (Cowen-Rivers et al., 2022) and SMAC3 (Lindauer et al., 2022). Recall that our theoretical insight suggests that 1) reshuffling might be beneficial during HPO and 2) holdout should be affected the most by reshuffling and other resamplings should only be affected to a lesser extent.

### Experimental Setup

As benchmark tasks, we use a set of standard HPO problems defined on small- to medium-sized tabular datasets for binary classification. We suspect the effect of the resampling variant used and whether the resampling is reshuffled to be larger for smaller datasets, where the variance of the validation loss estimator is naturally higher. Furthermore, from a practical perspective, this also ensures computational feasibility given the large number of HPO runs in our experiments. We systematically vary the learning algorithm, optimized performance metric, resampling method, whether the resampling is reshuffled, and the size of the dataset used for training and validation during HPO. Below, we outline the general experimental design and refer to Appendix F for details.

We used a subset of the datasets defined by the AutoML benchmark (Gijsbers et al., 2024), treating these as data generating processes (DGPs; Hothorn et al., 2005). We only considered datasets with less than \(100\) features to reduce the required computation time and required the number of observations to be between \(10000\) and \(1000000\); for further details see Appendix F.1. Our aim was to robustly measure the generalization performance when varying the size \(n\), which, as defined in Section 2 denotes the size of the combined data for model selection, so one training and validation set combined. First, we sampled \(5000\) data points per dataset for robust assessment of the generalization error; these points are not used during HPO in any way. Then, from the remaining points we sampled tasks with \(n\{500,1000,5000\}\).

We selected CatBoost (Prokhorenkova et al., 2018) and XGBoost (Chen & Guestrin, 2016) for their state-of-the-art performance on tabular data (Grinsztajn et al., 2022; Borisov et al., 2022; McElfresh et al., 2023; Kohli et al., 2024). Additionally, we included an Elastic Net (Zou & Hastie, 2005) to represent a linear baseline with a smaller search space and a funnel-shaped MLP (Zimmer et al., 2021) as a cost-effective neural network baseline. We provide details regarding training pipelines and search spaces in Appendix F.2.

We conduct a random search with \(500\) HPC evaluations for every resampling strategy we described in Table 1, for both fixed and reshuffled splits. We always use 80/20 train-validation splits for holdout

Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature \(m\), correlation strength \(\) of the noise (a larger \(\) implying weaker correlation), and extent of reshuffling \(\) (lower \(\) increasing reshuffling). A \(\) of 1 indicates no reshuffling. Error bars represent standard errors.

and 5-fold CVs, so that training set size (and negative estimation bias) are the same. Anytime test performance of an HPO run is assessed by re-training the current incumbent (i.e. the best HPC until the current HPO iteration based on validation performance) on all available train and validation data and evaluating its performance on the outer test set. Note we do this for scientific evaluation in this experiment; obviously, this is not possible in practice. Using random search allows us to record various metrics and afterwards simulate optimizing for different ones, specifically, we recorded accuracy, area under the ROC curve (ROC AUC) and logloss.

We also investigated the effect of reshuffling on two state-of-the-art BO variants (Eggensperger et al., 2021; Turner et al., 2021), namely HEBO (Cowen-Rivers et al., 2022) and SMAC3 (Lindauer et al., 2022). The experimental design was the same as for random search, except for the budget, which we reduced from 500 HPCs to 250 HPCs, and only optimized ROC AUC.

### Experimental Results

In the following, we focus on the results obtained using ROC AUC. We present aggregated results over different tasks, learning algorithms and replications to get a general understanding of the effects. Unaggregated results and results involving accuracy and logloss can be found in Appendix G.

Results of Reshuffling Different ResamplingsFor each resampling (holdout, 5-fold holdout, 5-fold CV, and 5x 5-fold CV), we empirically analyze the effect of reshuffling train and validation splits during HPO.

In Figure 3 we exemplarily show how test performance develops over the course of an HPO run on a single task for different resamplings (with and without reshuffling). Naturally, test performance does not necessarily increase in a monotonic fashion, and especially holdout without reshuffling tends to be unstable. Its reshuffled version results in substantially better test performance.

Next, we look at the relative _improvement_ (compared to standard 5-fold CV, which we consider our baseline) with respect to _test_ ROC AUC performance of the incumbent over time in Figure 4, i.e., the difference in test performance of the incumbent between standard 5-fold CV and a different resampling protocol; hence a positive difference tells us how much better in test error we are, if we would have chosen the other protocol instead 5-fold CV. We observe that reshuffling generally results in equal or better performance compared to the same resampling protocol without reshuffling. For 5-fold holdout and especially 5-fold CV and 5x 5-fold CV, reshuffling has a smaller effect on relative test performance improvement, as expected. Holdout is affected the most by reshuffling and results in substantially better relative test performance compared to standard holdout. We also observe that an HPO protocol based on reshuffled holdout results in similar final test performance as standard 5-fold CV while overall being substantially cheaper due to requiring less model fits per HPC evaluation. In Appendix G.2, we further provide an ablation study on the number of folds when using \(M\)-fold holdout, where we observed that - in line with our theory - the more folds are used, the less reshuffling affects \(M\)-fold holdout.

Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset alert for increasing \(n\) (train-validation sizes, columns). Shaded areas represent standard errors.

However, this general trend can vary for certain combinations of classifier and performance metric, see Appendix G. Especially for logloss, we observed that reshuffling rarely is beneficial; see the discussion in Section 5. Finally, the different resamplings generally behave as expected. The more we are willing to invest compute resources into a more intensive resampling like 5-fold CV or 5x 5-fold CV, the better the generalization performance of the final incumbent.

Results for BO and ReshufflingFigure 5 shows that, generally HEBO and SMAC3 outperform random search with respect to generalization performance (i.e., comparing HEBO and SMAC3 to random search under standard holdout, or comparing under reshuffled holdout). More interestingly, HEBO, SMAC3 and random search all strongly benefit from reshuffling. Moreover, the performance gap between HEBO and random search but also SMAC3 and random search narrows when the resampling is reshuffled, which is an interesting finding of its own: As soon as we are concerned with generalization performance of HPO and not only investigate validation performance during optimization, the choice of optimizer might have less impact on final generalization performance compared to other choices such as whether the resampling is reshuffled during HPO or not. We present results for BO and reshuffling for different resamplings in Appendix G.

## 5 Discussion

In the previous sections, we have shown theoretically and empirically that reshuffling can enhance generalization performance of HPO. The main purpose of this article is to draw attention to this

Figure 4: Average improvement (compared to standard 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over different tasks, learning algorithms and replications separately for increasing \(n\) (train-validation sizes, columns). Shaded areas represent standard errors.

Figure 5: Average improvement (compared to random search on standard holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learning algorithms and replications for different \(n\) (train-validation sizes, columns). Shaded areas represent standard errors.

surprising fact about a technique that is simple but rarely discussed. Our work goes beyond a preliminary experimental study on reshuffling (Levesque, 2018), in that we also study the effect of reshuffling on random search, multiple metrics and learning algorithms, and most importantly, for the first time, we provide a theoretical analysis that explains why reshuffling can be beneficial.

LimitationsTo unveil the mechanisms underlying the reshuffling procedures, our theoretical analysis relies on an asymptotic approximation of the empirical loss surface. This allows us to operate on Gaussian loss surfaces, which exhibit convenient concentration and anti-concentration properties required in our proof. The latter are lacking for general distributions, which explains our asymptotic approach. The analysis was further facilitated by a loss stability assumption regarding the learning algorithms that is generally rather mild; see the discussion in Bayle et al. (2020). However, it typically fails for highly sensitive losses, which has practical consequences. In fact, Figure 9 in Appendix G shows that reshuffling usually hurts generalization for the logloss and small sample sizes. It is still an open question whether this problem can be fixed by less naive implementations of the technique. Another limitation is our focus on generalization after search through a fixed, finite set of candidates. This largely ignores the dynamic nature of many HPO algorithms, which would greatly complicate our analysis. Finally, our experiments are limited in that we restricted ourselves to tabular data and binary classification and we avoided extremely small or large datasets.

Relation to OverfittingThe fact that generalization performance can decrease during HPO (or computational model selection in general) is sometimes known as oversearching, overtuning, or overfitting to the validation set (Quinlan and Cameron-Jones, 1995; Escalante et al., 2009; Koch et al., 2010; Igel, 2012; Bischl et al., 2023), but has arguably not been studied very thoroughly. Given recent theoretical (Feldman et al., 2019) and empirical (Purucker and Beel, 2023) findings, we expect less overtuning on multi-class datasets, making it interesting to see how reshuffling would affect the generalization performance.

Several works suggest strategies to counteract this effect. First, LOOCVCV proposes a conservative choice of incumbents (Ng, 1997) at the cost of leave-one-out analysis or an additional hyperparameter. Second, it is possible to use an extra _selection set_(Igel, 2012; Levesque, 2018; Mohr et al., 2018) at the cost of reduced training data, which was found to lead to reduced overall performance (Levesque, 2018). Third, by using early stopping one can stop hyperparameter optimization before the generalization performance degrades again. This was so far demonstrated to be able to save compute budget at only marginally reduced performance, but also requires either a sensitivity hyperparameter or correct estimation of the variance of the generalization estimate and was only developed for cross-validation so far (Makarova et al., 2022). Reshuffling itself is orthogonal to these proposals and a combination with the above-mentioned methods might result in further improvements.

OutlookGenerally, the related literature detects overfitting to the validation set either visually (Ng, 1997) or by measuring it (Koch et al., 2010; Igel, 2012; Fabris and Freitas, 2019). Developing a unified formal definition of the above-mentioned terms and thoroughly analyzing the effect of decreased generalization performance after many HPO iterations and how it relates to our measurements of the validation performance is an important direction for future work.

We further found, both theoretically and experimentally, that investing more resources when evaluating each HPC can result in better final HPO performance. To reduce the computational burden on HPO again, we suggest further investigating the use of adaptive CV techniques, as proposed by Auto-WEKA (Thornton et al., 2013) or under the name Lazy Paired Hyperparameter Tuning (Zheng and Bilenko, 2013). Designing more advanced HPO algorithms exploiting the reshuffling effect should be a promising avenue for further research.