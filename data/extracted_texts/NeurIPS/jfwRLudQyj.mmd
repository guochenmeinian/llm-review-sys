# A Toolkit for Reliable Benchmarking and Research in Multi-Objective Reinforcement Learning

Florian Felten\({}^{1,7}\)  Lucas N. Alegre\({}^{*,2,3,7}\)  Ann Nowe\({}^{3}\)  Ana L. C. Bazzan\({}^{2}\)

**El-Ghazali Talbi\({}^{1,4}\)  Gregoire Danoy\({}^{1,5}\)  Bruno C. da Silva\({}^{6}\)**

\({}^{1}\)SnT, University of Luxembourg \({}^{2}\)Institute of Informatics, Federal University of Rio Grande do Sul

\({}^{3}\)Artificial Intelligence Lab, Vrije Universiteit Brussel \({}^{4}\)CNRS/CRIStAL, University of Lille

\({}^{5}\)FSTM/DCS, University of Luxembourg \({}^{6}\)University of Massachusetts \({}^{7}\)Farama Foundation

{florian.felten,gregoire.danovj}@uni.lu {lnalegre,bazzan}@inf.ufrgs.br

ann.nowe@vub.be el-ghazali.talbi@univ-lille.fr bsilva@cs.umass.edu

Authors contributed equally to this work.

###### Abstract

Multi-objective reinforcement learning algorithms (MORL) extend standard reinforcement learning (RL) to scenarios where agents must optimize multiple--potentially conflicting--objectives, each represented by a distinct reward function. To facilitate and accelerate research and benchmarking in multi-objective RL problems, we introduce a comprehensive collection of software libraries that includes: _(i)_ MO-Gymnasium, an easy-to-use and flexible API enabling the rapid construction of novel MORL environments. It also includes more than 20 environments under this API. This allows researchers to effortlessly evaluate any algorithms on any existing domains; _(ii)_ MORL-Baselines, a collection of reliable and efficient implementations of state-of-the-art MORL algorithms, designed to provide a solid foundation for advancing research. Notably, all algorithms are inherently compatible with MO-Gymnasium; and _(iii)_ a thorough and robust set of benchmark results and comparisons of MORL-Baselines algorithms, tested across various challenging MO-Gymnasium environments. These benchmarks were constructed to serve as guidelines for the research community, underscoring the properties, advantages, and limitations of each particular state-of-the-art method.2

## 1 Introduction

Research in reinforcement learning (RL) algorithms (Sutton and Barto, 2018) has gained significant attention in recent years, in great part due to its remarkable success in a range of challenging problems (Mnih et al., 2015; Silver et al., 2017; Bellemare et al., 2020). This led to a substantial increase in the number of papers published in the field every year. The rapid growth of RL research, however, has not necessarily been accompanied by the design of well-thought and reliable tools allowing for appropriate evaluation practices, resulting in a reproducibility crisis within the field. This resulted, e.g., in researchers often questioning the validity and reproducibility of results presented in influential papers (Agarwal et al., 2021; Patterson et al., 2023).

There are several reasons why reproducibility is often a challenge in RL. A key factor is the significant amount of time required to train RL agents, which makes it difficult for researchers to gather sufficient data to perform rigorous statistical analyses of empirical results. As a consequence, some authors may claim superior performance over state-of-the-art techniques without adequate evidence.

Additionally, papers often do not provide sufficient information--such as hyperparameter values and implementation optimizations--to allow for the reliable reproducibility of their results. Finally, the use of environment implementations that lack standardization also contributes to this challenge. To address such issues, several libraries and experimentation protocols have been proposed in the RL ecosystem (Brockman et al., 2016; Agarwal et al., 2021; Raffin et al., 2021; Huang et al., 2022b).

As RL garners increasing interest, its subfield, Multi-Objective Reinforcement Learning (MORL), is simultaneously attracting notable attention within the RL research community (Hayes et al., 2022). MORL algorithms tackle problems where an agent has to optimize multiple--possibly conflicting--objectives. Each of these objectives is represented via a distinct reward function. In this setting, agents typically aim to find optimal decision-making policies defined with respect to different compromises or trade-offs between the objectives. Examples of MORL problems include the widely-used set of Mujoco (Todorov et al., 2012) tasks, which model compromises between an agent's objectives. As an example, the reward function of the _half-cheetah_ agent is a weighted combination of velocity- and energy-related terms, where the weights are pre-determined and kept constant. However, allowing for such weights (i.e., for the relative importance of objectives) to change may induce significantly different optimal behaviors (Xu et al., 2020). While the goal of standard RL algorithms is to learn a single policy, specialized in optimizing a single reward function, MORL algorithms typically search for a _set_ of policies such that given _any_ compromises between objectives, a corresponding optimal (or near-optimal) policy is in the set. Designing and properly evaluating MORL algorithms shares the difficulties encountered when designing and empirically testing RL techniques--with additional challenges particular to multi-objective settings. Yet, to the best of our knowledge, there are currently no public standard libraries providing reliable implementations of widely-used MORL domains and state-of-the-art MORL algorithms, designed particularly to facilitate research in the field.

In this paper, we introduce a comprehensive suite of benchmark MORL environments and reliable implementations of widely-used MORL algorithms, designed to facilitate the construction of reproducible empirical performance evaluation of existing and novel multi-objective techniques. First, we introduce MO-Gymnasium (Section 4), an easy-to-use and flexible API enabling the rapid construction of novel MORL environments. As of now, this API includes over 20 environments with diverse characteristics. This allows researchers to evaluate, with minimal effort, any algorithms compatible with our extendable API in any existing domains. Secondly, we introduce MORL-Baselines (Section 5), a collection of reliable and efficient implementations of state-of-the-art MORL algorithms designed to provide a solid foundation for advancing MORL research. Notably, all such algorithms are inherently compatible with MO-Gymnasium. Finally, in Section 6 we provide a thorough and robust set of benchmark results and comparisons of MORL-Baselines algorithms tested across various challenging MO-Gymnasium environments. These benchmarks were constructed to serve as guidelines for the research community, underscoring the properties, advantages, and limitations of each particular state-of-the-art method.

## 2 Related work

The challenge of reproducing experimental results in machine learning research is widely acknowledged. The significance of this issue has risen to such a degree that top-tier conferences like NeurIPS have implemented reproducibility programs aimed at improving the standards for conducting, communicating, and evaluating research in the field (Pineau et al., 2020). The RL field is directly affected by this issue, as highlighted by different authors. For example, Engstrom et al. (2020) show that optimizations at the implementation level of deep RL algorithms can be more impactful than experimenting with different algorithms. Similarly, Huang et al. (2022a) investigated and identified the existence of 37 particular code-level tricks necessary to achieve state-of-the-art results when deploying the PPO algorithm (Schulman et al., 2017). Agarwal et al. (2021) point out the lack of statistically significant results in numerous papers published in the field, despite many of them claiming to introduce techniques that outperform the state-of-the-art. Finally, an in-depth discussion of issues pertaining to methodologies used to conduct empirical evaluation in RL is presented by Patterson et al. (2023). These issues include, among others, inadequate tuning of the hyperparameters of baselines algorithms when deployed on new environments, averaging performance metrics over a limited number of runs, absence of random seed control, and environment overfitting.

To address these concerns, several RL libraries have been developed to provide more reliable baselines. For example, Gymnasium (Towers et al., 2023) (formerly known as OpenAI Gym (Brockman et al.,2016))) provides a standard API and a collection of reference environments for RL research and experimentation. Furthermore, various libraries have been published that contain well-tested, reliable, and continually maintained implementations of RL algorithms; e.g., Stable-Baselines 3 (Raffin et al., 2021) and cleanRL (Huang et al., 2022). Finally, recent initiatives such as openlbenchmark (Huang et al., 2023) facilitate the analysis of various learning metrics, and tackle the reproducibility challenge via experiment tracking software such as Weights and Biases, which makes it easier to, e.g., make hyperparameters of different baselines publicly available (Biewald, 2020). Other subfields of RL, such as multi-agent RL (MARL), have also benefited from standard baseline libraries. In MARL, PettingZoo (Terry et al., 2021) is often used to design novel environments, and EPyMARL (Papoudakis et al., 2021) is commonly used when designing new learning algorithms. Zhu et al. (2023) recently introduced D4MORL, a repository that includes datasets specifically designed to evaluate MORL algorithms in the _offline_ setting. Although these subareas of RL have reaped the benefits from standardized libraries, to the best of our knowledge, there are currently no publicly available (and widely adopted) standard libraries providing reliable implementations of widely-used MORL domains and state-of-the-art MORL algorithms, designed to facilitate research in the field.

As discussed by Hayes et al. (2022); Cassimon et al. (2022), various benchmark problems have been proposed to evaluate MORL methods. However, these benchmarks have not yet been made available via standardized APIs or centralized repositories. Arguably, this has made the experimental reproducibility of MORL algorithms harder, time-consuming, and error-prone. MORL-Glue (Vamplelev et al., 2017) represents an attempt to establish a centralized repository of MORL benchmarks. However, this library has not been widely adopted due to the fact that it is implemented in Java and targets tabular problems, whilst the community currently focuses on using Python and deep RL techniques. Our first contribution, MO-Gymnasium, addresses this issue by introducing a standard API and set of reference environments for MORL.

Although some published works on MORL make their code publicly available (Yang et al., 2019; Abels et al., 2019; Xu et al., 2020), these implementations are not often maintained by their authors. This makes reproducing empirical results a usually time-consuming and error-prone process. Our second contribution, MORL-Baselines, tackles this challenge. It includes a set of clear, well-structured, regularly maintained, and reliable implementations of MORL algorithms. Importantly, all such algorithms are compatible with MO-Gymnasium's API.

Finally, recall that the performance of newly proposed algorithms is generally compared with previously-published baselines. However, the methodology employed in conducting experiments is not always scientifically rigorous. To address this issue, we introduce a dataset of training results of MORL-Baselines algorithms when evaluated on all MO-Gymnasium environments. This dataset is available through openrlbenchmark and includes information about all hyperparameters used by each algorithm, in each experiment, thereby allowing researchers to compare new algorithms with such existing baselines without having to retrain models from scratch.

## 3 Multi-objective reinforcement learning

In MORL, the interaction of an agent with its environments is modeled via a _multi-objective Markov decision process_ (MOMDP) (Roijers et al., 2013). MOMDPs differ from standard MDPs (Sutton and Barto, 2018) only in that a MOMDP's reward function is vector-valued. A MOMDP is defined as a tuple \(M(,,p,,,)\), where \(\) is a state space, \(\) is an action space, \(p(|s,a)\) is the distribution over next states given state \(s\) and action \(a\), \(:^{m}\) is a multi-objective reward function containing \(m\) objectives, \(\) is an initial state distribution, and \([0,1)\) is a discounting factor. A policy \(:\) is a function mapping states to actions. Let \(S_{t}\), \(A_{t}\), and \(_{t}=(S_{t},A_{t},S_{t+1})\) denote the random variables corresponding to state, action, and vector reward, respectively, at time step \(t\). The _multi-objective value function_ of a policy \(\) in state \(s\) is defined as

\[^{}(s)_{}[_{i=0}^{}^ {i}_{t+i} S_{t}=s],\] (1)

where \(_{}[]\) denotes expectation with respect to trajectories induced by the policy \(\). We denote \(^{}_{S_{0}}[^{}(S_{0})]\) as the _value vector_ of policy \(\). Notice that \(^{}\) is an \(m\)-dimensional vector whose \(i\)-th component is the expected return of \(\) under the \(i\)-th objective.

In contrast to single-objective RL, comparing the values of two different policies is not straightforward in MORL. For instance, a policy that achieves higher expected return with respect to one objective may, as a result, have lower performance with respect to other objectives. Hence, in MORL, policies are typically evaluated and compared in terms of a _user utility function_ (or scalarization function), \(u:^{m}\), which is a mapping from the multi-objective value of policy \(\), \(^{}\), to a scalar.

Let \(u\) be a monotonically increasing utility function; i.e., a function such that the utility increases if the value of one objective can be improved without decreasing the value of other objectives. Then, the value vector \(^{}\) of an optimal policy with respect to \(u\) is in a _Pareto frontier_ (PF). The PF of a MOMDP is a set of nondominated multi-objective value vectors:

\[\{^{}\ ^{^{}}_{p}^{}\},^{}_{p}^{^{}}( i:v_{i}^{}  v_{i}^{^{}})( i:v_{i}^{}>v_{i}^{^{}}).\] (2)

In particular, _linear utility functions_ linearly combine the value of a policy under each of the \(m\) objectives using a set of weights \(^{m}\): \(u(^{},)^{}\), where each element of \(^{m}\) specifies the relative importance of each objective. The space of weight vectors, \(\), is an \(m\)-dimensional simplex so that \(_{i}w_{i}=1,w_{i} 0,i=1,...,m\). Under such a utility scheme, the PF corresponds to a _convex coverage set_ (CCS), as shown by Roijers et al. (2013).

MORL algorithms can be classified in terms of whether they are _single-policy_ (i.e., they learn a single policy that optimizes one particular utility \(u\)), or _multi-policy_ (i.e., they learn a set of policies with the goal of approximating the PF). When non-linear utility functions are considered, the optimization procedure of a MORL problem can also be defined with respect to the _expected scalarized return_ (ESR), \([u(_{t=0}^{}^{t}_{t})]\), or the _scalarized expected return_ (SER), \(u([_{t=0}^{}^{t}_{t}])\). For a thorough review of MORL concepts and definitions, we refer the reader to Hayes et al. (2022).

## 4 MO-Gymnasium

In this section, we introduce our first contribution: MO-Gymnasium.3 MO-Gymnasium is an easy-to-use and flexible API enabling the rapid construction of novel MORL environments, and a collection of regularly maintained and thoroughly tested environment implementations.

Unlike classical ML settings, which rely on fixed datasets, RL problems typically do not; this makes replication of experimental results challenging. Indeed, even small differences in the definition of environment can have a significant impact on the performance of algorithms.

To address this issue and facilitate research when it comes to the standard RL settings, Gymnasium (Towers et al., 2023) (formerly Gym (Brockman et al., 2016)) introduces an API and collection of versioned environments. With millions of downloads, this is currently the _de facto_ standard library in RL, enabling researchers to easily test their algorithms on a variety of problems. Despite its widespread use, Gymnasium is limited to modeling single-objective MDPs. It has since been expanded in various ways; e.g., PettingZoo extends it to MARL settings and D4MORL to offline MORL. To the best of our knowledge, there are currently no publicly available and widely adopted libraries providing reliable implementations of MORL domains and state-of-the-art MORL algorithms, designed to facilitate research in the field.

We address this limitation by introducing MO-Gymnasium--previously known as MO-Gym (Alegre et al., 2022). MO-Gymnasium's API is designed to be as similar as possible to Gymnasium's API. This allows it to inherit many of the features in Gymnasium, such as _wrappers_--features that allow individual properties of a domain to be modified--while extending the original API only where and when necessary. This makes MO-Gymnasium automatically backward compatible with a wide range of MORL benchmark domains. The key difference between these two frameworks is that, in MO-Gymnasium, rewards returned after the execution of an action (i.e., after a call of the _step_ method) are vectors rather than scalars (see Figure 1). MO-Gymnasium is available on PyPI and can be installed via pip install mo-gymnasium. Importantly, we highlight that MO-Gymnasium is

Figure 1: An example of how MO-Gymnasium can be used.

an official part of the projects maintained by the Farama Foundation, and is considered a mature and well-supported library by the research community.

**Environments.** Currently, MO-Gymnasium includes over 20 environments commonly used in the MORL literature--including environments with discrete and continuous state and action spaces--such as _deep-sea-treasure_(Vamplew et al., 2011), _four-room_(Alegre et al., 2022), _mo-supermario_(Yang et al., 2019), _minecart_(Abels et al., 2019), and _mo-halfcheetah_(Xu et al., 2020). In Figure 2 we depict a few of the currently available environments. See Appendix B for a detailed description of each environment. This large collection of environments allows designers to thoroughly assess the performance of novel algorithms in different scenarios. In environments in which the true PF is known, it can be accessed via the pareto_front() method available in the MO-Gymnasium's API. Additionally, for reproducibility purposes, each environment is labeled with a version number; e.g., "-\(\)0". Each time an environment is modified in a way that may affect algorithms' performances, the domain's version number is incremented.

**Wrappers.** MO-Gymnasium introduces MORL-specific wrappers such as _MONormalizeReward_, which normalizes a given component of the reward vector; and _LinearReward_, a wrapper that linearly scalarizes the reward function of a MOMDP environment, transforming it into a standard MDP. The latter feature makes MO-Gymnasium _directly compatible_ with widely-used RL libraries compatible with Gymnasium, such as Stable-Baselines 3(Raffin et al., 2021) and cleanRL (Huang et al., 2022).

## 5 MORL-Baselines

In this section, we introduce MORL-Baselines4, a collection of reliable and efficient implementations of state-of-the-art MORL algorithms, designed to provide a solid foundation for advancing research in MORL. To the best of our knowledge, this stands as the first open-source repository encompassing a multitude of MORL algorithms. Notably, all such algorithms are inherently compatible with MO-Gymnasium's API.

It is well known that the performance of learning algorithms is closely tied to their specific implementations (Engstrom et al., 2020). Unfortunately, details such as implementation-specific optimizations are seldom discussed in research papers, which makes reproducibility and comparisons challenging. To address this issue, some authors provide access to their codebases. While this is a positive step, codebases are often not regularly maintained and may become outdated, rendering replication difficult. Thus, libraries like Stable-Baselines 3(Raffin et al., 2021) and cleanRL (Huang et al., 2022) have been designed with the goal of regularly maintaining state-of-the-art algorithms. These codebases are well documented, thoroughly tested, and offer top-notch performance on the algorithms they implement. This allows researchers to start coding novel methods by extending existing implementations rather than starting from scratch. These libraries also provide useful tools for research purposes, such as efficient replay buffer implementations, performance reports, and evaluation methods.

MORL-Baselines includes more than 10 state-of-the-art MORL algorithms, all of which are compatible with MO-Gymnasium. Up to this point, no similar libraries were available. MORL-Baselines offers a range of features to aid researchers in designing new algorithms, such as methods to compute and analyze Pareto fronts, perform evaluation w.r.t. various metrics, replay buffers, and experiment tracking tools. Below, we provide a list of the algorithms currently available via MORL-Baselines, along with a description of the settings in which they are applicable.

Figure 2: A few of the environments available in MO-Gymnasium. From left to right: _deep-sea-treasure_, _mo-halfcheetah_, _mo-supermario_, _minecart_, and _resource-gathering_.

### Implemented algorithms

Table 1 lists the algorithms currently supported by MORL-Baselines and the MORL settings they tackle. Algorithms employing neural networks as function approximators were implemented using PyTorch (Paszke et al., 2019), and tabular algorithms rely on NumPy (Harris et al., 2020). Algorithms in Table 1 are described according to whether they produce a single policy (based on user-provided utility functions) or multiple policies (i.e., to approximate a CCS or a PF). Additionally, notice that some algorithms optimize w.r.t. the ESR while others optimize w.r.t. SER (Section 3). Finally, notice that MORL-Baselines' algorithms may support different observation and action spaces (e.g., images).

Tabular algorithms.Multi-objective Q-learning (MOQL) (Van Moffaert et al., 2013) is an extension of the classic tabular Q-learning algorithm (Watkins, 1989) that learns and stores the Q-values of each objective separately. A scalarization function is then used to convert these Q-values into a scalar quantity, allowing agents to select an action. Multi-policy MOQL, which consists of running MOQL multiple times with different preferences, can be instantiated using different methods that select which preference weight vector will be optimized next by a corresponding specialized policy. Currently, MORL-Baselines supports randomly generated weight vectors, Optimistic Linear Support (OLS) (Roijers, 2016), and Generalized Policy Improvement Linear Support (GPI-LS) (Alegre et al., 2023). Pareto Q-learning (PQL) (Van Moffaert and Nowe, 2014) aims at simultaneously learning all policies in the Pareto front by storing sets of non-dominated Q-values. These sets are then converted into scalars (using metrics similar to the ones discussed in the next section) that guide the selection of actions during the learning phase. This algorithm is only compatible with deterministic environments.

Deep MORL algorithms.The Expected Utility Policy Gradient (EUPG) algorithm (Roijers et al., 2018) introduced a policy gradient update capable of taking into account both the return achieved up to the current moment, as well as future returns, in order to determine expected utilities in an ESR setting. However, although it employs a neural network as the policy, it has only been

  
**Algorithm** & **Single or** & **Utility** & **Observation** & **Action** \\  & **multi-policy** & **function** & **space** & **space** \\  MOQL (Van Moffaert et al., 2013) & Single & Linear & Disc. & Disc. \\ EUPG (Roijers et al., 2018) & Single & Non-linear, & Disc. & Disc. \\  & & ESR & & Disc. \\ MPMOQL (Van Moffaert et al., 2013) & Multi & Linear & Disc. & Disc. \\  & & Non-linear, & & Disc. \\  & & SER (*) & & \\ OLS (Roijers, 2016) & Multi & Linear & / (**) & / (**) \\ Envelope (Yang et al., 2019) & Multi & Linear & Cont. & Disc. \\ PGMORL (Xu et al., 2020) & Multi & Linear & Cont. & Cont. \\ PCN (Reymond et al., 2022) & Multi & 
 Non-linear, \\ ESR/SER (*) \\  & Cont. & Disc. \\ GPI-LS \& & Multi & Linear & Cont. & Any \\ GPI-PD (Alegre et al., 2023) & Multi & Linear & Cont. & Cont. \\ CAPQL (Lu et al., 2023) & Multi & Linear & Cont. & Cont. \\   

Table 1: Algorithms currently implemented in MORL-Baselines. (*) PCN and PQL are designed to tackle deterministic environments. (**) OLS is an algorithm-agnostic method for generating reward weights, or preferences; it does not assume any particular type of observation or action spaces.

evaluated in discrete settings. The Envelope algorithm (Yang et al., 2019) uses a single neural network conditioned on a weight vector (Abels et al., 2019) to approximate the CCS. Prediction-Guided MORL (PGMORL) (Xu et al., 2020) is an evolutionary algorithm that maintains a population of policies learned using PPO (Schulman et al., 2017). This algorithm focuses on predicting, at each iteration, the most promising weight vectors and policies to select for further training in order to more effectively enhance the PF. Pareto Conditioned Networks (PCN) (Reymond et al., 2022) employs a neural network conditioned on a given desired return per objective. This network is trained via supervised learning to predict which actions produce the desired return in deterministic environments. GPI-LS (Alegre et al., 2023) employs GPI (Barreto et al., 2017) to combine policies in its learned CCS and prioritize the weight vectors on which agents should train at each moment. GPI-Prioritized Dyna (GPI-PD) is a model-based extension of GPI-LS that uses a learned model of the environment and GPI to prioritize experiences in the replay buffer. Concave-Augmented Pareto Q-learning (CAPQL) (Lu et al., 2023) employs a multi-objective extension of SAC (Haarnoja et al., 2018) by conditioning the actor and critic networks on the rewards weight vector.

### Evaluation metrics

Recall that in single objective RL settings, policies are evaluated in terms of their corresponding expected returns. In MORL settings, by contrast, they are typically evaluated using multi-objective metrics computed based on Pareto fronts. While these metrics are widely adopted by the multi-objective optimization community, the MORL community has yet to establish a consensus on which metrics should be preferred in each particular problem or setting. For this reason, our framework supports all commonly-used MORL metrics--see below. These metrics can be split into two groups: utility-based metrics, which assume particular properties of the utility function (e.g., linearity), and axiomatic metrics, which do not make assumptions but may produce less informative performance information to users. For a thorough discussion of these metrics, see Hayes et al. (2022).

**Expected utility (\(\)).** If the utility function \(u\) is linear, it is possible to express the expected utility over a distribution of rewards weights, \(\), via the _expected utility_ (EU) metric (Zintgraf et al., 2015). Let \(\) be a set of policies and \(}=\{^{}|\}\) be its corresponding approximate PF. Then, the EU metric is defined as:

\[(})=_{}[ _{^{}}}^{} ].\]

**Maximum utility loss (\(\)).**Zintgraf et al. (2015) introduced this metric to quantify the maximum utility loss that results from using an approximate PF, \(}\), rather than a reference PF, \(\)5. It is defined as follows:

\[(},)=_{}( _{^{}}^{^{*}}- _{^{}}}^{}).\]

**Inverted generational distance (\(\)).** This metric characterizes the convergence rate of an approximate PF, \(}\), towards a reference PF, \(\)(Coello Coello and Reyes Sierra, 2004). If the reference front is unknown, it is usually defined/constructed by aggregating the best value vectors observed after several executions of the underlying RL algorithm. The inverted generation distance (IGD) is computed as:

\[(},)=|}^{}}_{^{}}}\|^{*}-^{}\|^{2}}.\]

**Sparsity (\(\)).** This metric characterizes the diversity of the policies in a given PF. A diverse set of policies allows users to choose from qualitatively different behaviors based on the trade-offs they may wish to optimize (Xu et al., 2020). The sparsity of an approximate PF, \(}\), is given by:

\[(})=}|-1}_{j=1}^{m }_{i=1}^{|}|-1}(_{j}(i)-_{j}(i+1) )^{2},\]

where \(_{j}\) is the sorted list of the values of the \(j\)-th objective considering all policies in \(}\), and \(_{j}(i)\) is the \(i\)-th value in \(_{j}\).

[MISSING_PAGE_FAIL:8]

### Results and discussion of the proof-of-concept experiments

Figures 3, 4, 5, and 6 present the performance of various MORL-Baselines algorithms when evaluated on a few representative MO-Gymnasium environments. They show, in particular, the mean and corresponding 95% confidence intervals with respect to the evaluation metrics introduced in Section 5.2. In the Appendix, we provide the complete set of experimental results and comparisons.

Figure 3 presents the performance of various tabular MORL algorithms available on MORL-Baselines, when evaluated in the classic _deep-sea-treasure_ domain. Notice that the PQL and GPI-LS algorithms have similar performance and achieve near-zero maximum utility loss and Inverted Generational Distance. This indicates that they successfully identify the true Pareto front.

Figure 4 depicts the performance of various MORL algorithms that support discrete action spaces in _minecart-v0_. GPI-PD significantly outperforms all other baselines (when considering all performance metrics) in terms of sample efficiency and asymptotic performance. This empirical result supports recent observations by Alegre et al. (2022) about the crucial role of efficient prioritization schemes when constructing PFs and selecting training experiences. Interestingly, notice that even though PCN was designed to tackle deterministic MOMDPs, it still outperforms the Envelope algorithm.

Finally, Figures 5 and 6 present the performance of MORL algorithms that support continuous action spaces in the _mo-halfcheetah-v4_ and _mo-hopper-2d-v4_ domains, respectively. In _mo-halfcheetah_,

Figure 4: Performance of MORL algorithms on the _minecart-v0_ domain w.r.t. training samples.

Figure 5: Performance of MORL algorithms on the _mo-halfcheetah-v4_ domain. Pareto fronts were constructed by identifying (across all runs) the front with the highest Expected Utility after a given training budget (5M steps for PGMORL, 100k for GPI-PD, 200k for the others).

Figure 3: Performance of tabular MORL algorithms on _deep-sea-treasure-v0_ w.r.t. training samples.

GPI-based algorithms perform better w.r.t. the hypervolume and expected utility metrics. PGMORL and CAPQL, by contrast, perform better w.r.t. the Sparsity metric--they are able to identify denser Pareto fronts, as shown in the rightmost plot. Recall that sample efficiency often comes at the cost of execution time. To investigate this trade-off, we show, in Figure 6, performance results in the _mo-hopper_ domain with respect to training time (in minutes). These experiments suggest that PGMORL performs well in terms of training time, in contrast to previous experiments where it had a qualitatively different behavior when analyzed with respect to sample complexity. In these experiments, both GPI-based algorithms still dominated all other baselines.

## 7 Conclusion

We introduced a comprehensive collection of software libraries for reliable benchmarking and research in MORL. MO-Gymnasium and MORL-Baselines are the first libraries providing standardized APIs and extensible sets of environments and state-of-the-art MORL algorithms. Our framework also includes a thorough set of benchmark results comparing a wide range of algorithms in various environments. These can be used as guidelines by the community, underscoring the properties and limitations of different techniques. By making this toolkit open source and extensible--and thus open to contributions from other researchers--we hope to provide a solid foundation for reproducible research in MORL. This work will allow researchers and end users to seamlessly deploy existing algorithms on various MORL domains, speeding up experiments, facilitating algorithm evaluation, and being more conducive to reproducible experimental results.

**Limitations and future work.** In the benchmarks we presented in this paper, we did not perform exhaustive hyperparameter tuning. Our main goal with these experiments was to validate our implementations and showcase the learning behavior of the algorithms in our framework with respect to different evaluation metrics. For this reason, such empirical results should be interpreted as proof-of-concept and may still be further improved with the aid of the MORL community. In future work, we will continue maintaining all algorithms in MORL-Baselines, and plan to augment the set of techniques available through our framework. We also plan to introduce new features for automating hyperparameter tuning. We do not anticipate any negative societal impacts of this work.