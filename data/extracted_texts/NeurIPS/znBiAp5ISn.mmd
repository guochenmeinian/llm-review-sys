# TAS-GNN: Topology-Aware Spiking Graph Neural Networks for Graph Classification

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The recent integration of spiking neurons into graph neural networks has been gaining much attraction due to its superior energy efficiency. Especially because the irregular connection among graph nodes fits the nature of the spiking neural networks, spiking graph neural networks are considered strong alternatives to vanilla graph neural networks. However, there is still a large performance gap for graph tasks between the spiking neural networks and artificial neural networks. The gaps are especially large when they are adapted to graph classification tasks, where none of the nodes in the testset graphs are connected to the training set graphs. We diagnose the problem as the existence of neurons under starvation, caused by the irregular connections among the nodes and the neurons. To alleviate the problem, we propose TAS-GNN. Based on a set of observations on spiking neurons on graph classification tasks, we devise several techniques to utilize more neurons to deliver meaningful information to the connected neurons. Experiments on diverse datasets show up to 27.20% improvement, demonstrating the effectiveness of the TAS-GNN.

## 1 Introduction

Graph neural networks (GNNs) are types of popular neural networks to learn the representations from graphs, which comprise multiple nodes and edges between them. Because of their flexibility to model any kind of connection existing in nature, it has various applications ranging from drug discovery [6; 47; 9], social influence prediction [39; 2], traffic forecasting [3; 7], and recommendation systems [38; 15; 61]. One known challenge of GNNs is their sparse memory and computational pattern. Because many messages are passed between randomly connected nodes, there is a significant inefficiency in processing them with conventional systems [53; 58; 57; 19].

To address the inefficiency, spiking neural networks (SNNs) are considered strong alternatives. Inspired by the way biological behavior of brains, SNNs process information by communicating binary spikes between the neurons. Because SNNs utilize intermittently occurring spikes, they have superior energy efficiency, especially for the domain of GNNs .

Although the spiking graph neural network (SGNN) has been recently studied by many researchers [32; 64; 48], we find that its performance experiences a huge drop when adapted to graph classification, compared to that of the conventional GNNs implemented with artificial neural networks (ANNs). Upon closer analysis of the performance degradation, we identify spike frequency deviation of the neurons within the model. In our investigation, many neurons experience _starvation_, which do not emit any spike during the inference. This leads to severe information loss, due to being unable to deliver signals to the subsequent neurons.

Such a problem was less exposed in previous spiking GNNs. This is because the testset nodes are available during the training time (transductive learning ) or they are part of the training graph (inductive learning ). In such settings, the model could be trained to mitigate the performance drop. However, in graph classification tasks, the graphs are independent of each other, and the testset comprises multiple unseen graphs, aggravating the problem.

Fortunately, our further analysis reveals that such phenomena are related to the topology of the input graphs. We discover that a strong pattern exists among the neurons in the GNN, where 1) neurons in a node have similar behaviors, 2) each feature causes different behaviors, and 3) neurons in high-degree nodes tend to emit more spikes.

Motivated by the observations, we propose to group the neurons according to the degree of the node (_topology-aware group-adaptive neurons_). The neurons in each group adapt the threshold together to steer the firing rate toward ideal rates. To further mitigate the initial value sensitivity problem, we further propose to learn the initial values.

We evaluate TAS-GNN over multiple GNN models and datasets. Experiments reveal that the proposed TAS-GNN achieves superior performance over the baselines, setting a new state-of-the-art method for graph classification. Our contributions are summarized as the following:

* We identify starvation problem of spiking neurons in GNNs for graph classification tasks.
* We observe the spike frequency patterns have a strong correlation with the graph topology.
* Based on the observations, we propose topology-aware group-adaptive neurons, which dynamically adjusts the threshold together with the other neurons in the group to address the spike frequency deviations.
* We propose techniques to reduce the initial value sensitivity caused by the topology-aware group-adaptive neurons.
* We evaluate TAS-GNN on several public datasets and achieve superior performance over existing techniques.

## 2 Background

### Spiking Neural Networks and Spike Training

Spiking neural networks (SNNs) are third-generation neural network designs that mimic the human biological neural systems . They use spike-based communication and adopt event-driven characteristics that promote better energy efficiency than current ANNs. Similar to human neural systems, SNNs consist of spiking neurons that can model spatio-temporal dynamics of the actual biological neurons. The early forms of such neuron models are Hodgkin-Huxley neurons , which accurately model the biophysical characteristics of the membrane through differential equations. However, its mathematical complexity prohibits its practical use and scalability. Instead, Leaky Integrated-and-Fire (LIF) model finds a middle ground between mathematical simplicity and biological plausibility, and is popularly adopted as the baseline architecture . In the LIF neuron, the weighted sum of input spikes is accumulated over time within the neuron as membrane potential, and the output spike is generated only when the membrane potential exceeds a present threshold value. This is represented as a differential function:

\[=-V(t)+I(t),\] (1)

where \(V(t)\) denotes the membrane potential value at time \(t\), \(\) a time constant of membrane, and \(I(t)\) is the input from connected synapses at time \(t\). To make this time-varying function computationally feasible, we discretize and rewrite it iteratively for sequential simulation as follows:

\[V(t) =V(t-1)+(WX(t)-(V(t-1)-V_{reset})),\] (2) \[V(t) =V(t)(1-S(t))+V_{reset}S(t),\] (3) \[S(t) =1,&V(t) V_{th}\\ 0,&,\] (4)

where \(\) is simplified decay rate constant, \(V_{reset}\) is the reset value and \(V_{th}\) the threshold for the membrane potential. Note that I(t) is simplified as weighted input WX(t) which can be obtainedthrough any operations with learnable weights including convolutional operation, self-attention, or a simple MLP. We will denote this process of forwarding through LIF neuron as \(SNN()\) in this paper.

Direct SNN Training.The initial adoption of SNNs was through ANN-SNN conversion, primarily due to their remarkable potential for reducing energy consumption. Various studies have aimed to address the accuracy degradation that occurs during the conversion from ANNS to SNNs [22; 41; 24; 42].

The spike generation by the step function in Equation (4) interfered with direct training without modifying the functions. To bypass the step function, which is non-differentiable and thus unsuitable for backpropagation, several approaches have been proposed [43; 5; 13; 14; 8; 51; 10]. Recent research has demonstrated that directly training SNNs can yield competitive results by addressing the challenges posed by non-differentiability. Our work focuses on directly training graph neural networks (GNNs) with SNNs and exploring a different domain, such as ANN-SNN conversion methods, which do not focus on using backpropagation concepts directly.

### Graph Neural Networks

Graph neural networks (GNNs) take graph-represented data as input, which consist of nodes and their connected edges \(=(V,E)\), with node features \(^{|V| F}\) and optionally edge features \(^{|E| D}\). The common GNN architectures follow a message passing paradigm , which learns node or edge representations through aggregating information from its neighboring nodes and updating the node features iteratively. Thus a single forward of message passing layer consists of message passing, aggregation, and update: \(h_{i}^{(l+1)}=(h_{i}^{(l)},_{j(i)}(h_{i}^{(l) },h_{j}^{(l)},e_{ij})),\) where \(l\) and \(i\) are indices for layer and node, respectively, and \(()\) denote message passing function. After aggregation of neighboring features, \(()\) is used for feature update. For graph convolutional network , the overall process can be simplified as:

\[X^{(l+1)}=AX^{(l)}W^{(l)},\] (5)

where the feature matrix is a concatenation of node features \(X^{(l)}=[h_{0}^{(l)}||h_{1}^{(l)}||...||h_{(|V|-1)}^{(l)}]^{T}\) which is updated through iterations of aggregation (\(AX\)) and combination (\(XW\)). After iterative updates of \(X\) through the layers, the learned node or edge embeddings are passed through additional classification layer for node-level or edge-level predictions.

Graph ClassificationIn this paper we put emphasis on graph-level classification tasks where each graph is considered an individual input. Graph classification follows the same node-wise message passing framework to obtain node embeddings, but appends a readout layer to turn them into a single graph embedding:

\[h_{G}=R(h_{i}^{(L)}|V_{i}),\] (6)

where \(R\) denotes readout function. Readout function reduces the node dimension to a single channel regardless of the input size. This is due to the inductive nature of graph classification task where the number of nodes is not known in advance. While all the other GNN layers focus on aggregating only the local features, the readout layer considers the entire graph to generate global features, and is unique to the graph classification tasks. The obtained graph embedding is passed through a classification layer for graph predictions. Graph classification tasks usually hold more difficulty than node-level classification due to its inductive nature, where inference is done on unseen graphs and thus cannot utilize any graph-specific statistics from the train set.

### Spiking Graph Neural Networks

In this paper, we adopt conventional SNN designs where LIF neurons are connected through learnable weights, and apply is to GNN framework . As mentioned in Section 2.2, each GNN layer outputs updated feature matrix \(X^{(l+1)}^{|V| F}\). This is converted to spike representation through SNN layer:

\[X^{(l+1)}=SNN(AX^{(l)}W^{(l)}).\] (7)

After passing the GNN layer, all of the updated \(h_{i}^{(l)}\) directly pass through the SNN layer, consist the feature matrix \(X^{(l)}\) always contains spike information consistently.

## 3 Analysis on Spike Frequency Variation of GNNs

To analyze the cause of the accuracy drop, we plot the behavior of the neurons during inference in Figure 0(a), on a IMDB-BINARY dataset over five timesteps (\(T=5\)). We create a histogram of spike counts created from each node, which is associated with 128 neurons. As depicted in the plot, it is clear that most of the neurons are under starvation. This is caused by the inputs of those neurons being insufficient to reach the threshold, and this leads to severe information loss between the layers. While unveiling the exact dynamics would require more research, we hypothesize that this is caused by the topology of the real-world graphs.

To validate the hypothesis and further investigate the phenomena, we display the spike frequency heatmap of the neurons sorted by the degree of the nodes in Figure 0(b). From the heatmap, we make three observations:

1. **(Brighter on the top and darker at the bottom)**_High-degree nodes tend to exhibit higher spike frequencies._
2. **(The horizontal strips)**_The spike frequencies are associated with the corresponding nodes._
3. **(The vertical strips)**_The feature neurons within a node behave differently according to their positions._

We believe such patterns come from the connectivity of the nodes, and the distinct role of the neurons assigned to each node. The connectivity will affect the number of receiving spikes of neurons associated with each node. It is known that most of the real-world graphs exhibit an extremely skewed distribution of degrees (i.e., power-law distribution ). Due to such a characteristic, there are a few nodes with very high degrees, while a majority of nodes have low degrees. Because a GNN layer communicates signals between the neighbors, a high-degree node will likely receive a lot of spikes, while a low-degree node will receive only a few.

Figure 1: Analysis on spike frequency variation of GCN using IMDB-BINARY  dataset.

In addition, the neurons assigned to each node are known to have different semantic functionality according to their positions, analogous to _channels_ in convolutional neural networks or _heads_ in large language models. For example, the input first layer of a molecular graph will have information such as its energy, x/y/z location, and atom numbers. In the intermediate layers, they represent a specific pattern sensed by the network (such as high energy + hydrogen atom), even though the exact behaviors are yet to be human-interpretable. In such a manner, the neurons in the same position are expected to behave similarly, even though they correspond to different nodes.

These three observations shed light on how to close the performance gap between spiking GNNs are ANN-based GNNs. In the next section, we describe how the observations are used to build better spiking GNNs for graph classification.

## 4 Proposed Method

### Overall Graph Classification Architecture

Many recent studies have tried to adapt SNN architectures into GNN tasks, however, they simply try to contact with only node classification tasks. In this work, we propose a spiking neural network specifically designed for graph classification tasks and show that it can be trained using spikes. We demonstrate the overall architecture of our graph classification model TAS-GNN in Figure 2. For each timestep, the input graphs are first translated into spike representations through the poisson encoder, then the message passing is done in spike format. After the combination phase in the GNN layer, the node features are once again binarized into spike format through passing the SNN layer. In the last layer, we perform an extra operation of aggregation and combination on the spike features before passing the readout layer. The readout layer is essential to graph classification and is responsible for aggregating all the node embeddings in the graph into a single graph representation. A batch of graph embeddings is passed through a classification head that outputs logits for that timestep. To make the final prediction, we simply take the sum of logits from all timesteps and use softmax to obtain the class probabilities.

### Topology-Aware Group-Adaptive Neurons

As discussed in Section 3, GNNs suffer from a huge gap in spike frequencies between neurons. As observed, there exist some patterns (Figure 5) that we can utilize to address the issue. One naive way of addressing the issue is to use learnable , or adaptive  threshold for each neuron. By adjusting the threshold, one can expect the neurons to naturally change, such that neurons under starvation will have lower thresholds to fire more often, and a few neurons with high firing rates will have higher thresholds to shift toward an ideal distribution.

Figure 2: Overall graph classification architecture with proposed methods.

Unfortunately, such an idea cannot be directly applied unless all the testset nodes are available at training time (i.e., transductive task). However, such a setting would be considered a data leak for graph classification, and would also lose the advantage SNNs have on lightweight inference.

Moreover, the number of nodes in a real-world dataset often ranges from at least thousands to several billions. Considering that GNNs often involve only a sub-million number of learnable parameters, storing such a large number of thresholds is considered too much overhead.

To address the aforementioned issues, we propose _topology-aware group adaptive neurons_ (TAG), which partitions the neurons by their degrees. Note that \(V_{g}\) denotes the node group to which the node is mapped, considering degree information. \(S^{g_{i}}(t)\) and \(V^{g_{i}}(t)\) represent the output spike and membrane potential of the \(i\)-th node in group \(g\) at time \(t\), respectively, as reformulated by Equation (4). We use \(g\) to represent the unique degree distribution of the training sets. When an unseen node is encountered, we apply the initial threshold, as it has not been trained at all.

\[S^{g_{i}}(t) =1,&V^{g_{i}}(t) V^{g}_{th}(t-1)\\ 0,&\] (8) \[S^{g}(t) =|}_{i V_{g}}S^{g_{i}}(t)\] (9) \[V^{g}_{th}(t) = V^{g}_{th}(t-1)+(1-)S^{g}(t)\] (10)

The major advantage of this scheme is that it is straightforward to put an unseen node or an unseen graph into a group at inference. To further consider intra-node deviation, we split the group into \(F\) (number of features) neurons, which is a fixed parameter determined by the model architecture. For any unseen node, finding out its degree is trivial because visiting its neighbors is one of the fundamental requirements of graph data structures [26; 50; 36; 28]. Based on the observation 1 from Section 3 that the neuron behavior is related to the degree, this will let neurons in the group collaboratively find an adequate threshold.

### Reducing the Initial Threshold Sensitivity

The proposed Group-adaptive threshold scheme effectively reduces the spike frequency variation issue. However, we find that the adaptive neurons in the proposed TAG are sensitive to their initial thresholds. As depicted in Figure 3, the performance of the adaptive neurons can severely drop when the initial threshold value is not carefully tuned, which aligns with the findings from . Moreover, manually tuning the initial thresholds individually is difficult because there are thousands of neuron groups.

To address the problem, we choose to learn the two parameters: the initial threshold per group (\(V^{g}_{th}(0)\)) and the decay rate (\(\)). During training, we adopted the backpropagation algorithm [51; 10; 8] to update the value of \(V^{g}_{th}(0)\) with the gradients at time step t=1. This is done because \(V^{g}_{th}(t)\) keeps updating with TAG Section 4.2 as time passes. During training, we also learn the decay rate (\(\)) , which prevents the membrane voltage of neurons in low-degree nodes from leaking faster than it accumulates. For evaluation, we use the \(V^{g}_{th}(0)\) values obtained during the training phase, adjusted for each group. The overall training procedure is in the Appendix.

## 5 Evaluation

### Experiment Settings

We use a total of 5 graph datasets commonly used for benchmarking GNNs: MUTAG , PROTEINS , ENZYMES , NCI1 , and IMDB-Binary . For the GNN layer in our architecture, we use 3 different designs, including GCN , GAT , and GIN . The baselines include 3 works from SNN that are applicable to graph datasets: SpikingGNN , SpikeNet , and

Figure 3: Sensitivity of neurons to its initial threshold.

PGNN . Since this is the first SNN design to target graph classification, we apply minor modifications to each architecture, such as appending a readout layer. Note that SpikingGNN  was originally proposed for GCN, but we extend it to both GAT and GIN. More details on the experiment setting are included in the Appendix.

### Results on Graph Classification

We compare TAS-GNN against prior works that adopt a spiking neural network to graph the dataset, shown in Table 1. We also report the performance of conventional ANN for comparison. In all but 2 cases, TAS-GNN outperforms the baselines by a noticeable margin. In the cases where TAS-GNN underperforms, the gaps are less than 1.1%p, smaller than the error bounds. In the opposite cases, the improvement is up to 27.90%p, showing a great amount of improvement.

An intriguing result is that TAS-GNN performs better than ANN-based GNNs in several cases. Improvements beyond the error bounds are found in MUTAG (GCN and GAT), NCI1 (GAT), and IMDB-BINARY (GCN and GAT). Note that the model architecture and the number of learnable parameters are the same in all methods. We believe this could come from the spiking neurons efficiently capturing the irregular connections over several timesteps, thereby showing an advantage over ANNs.

### Ablation Study

In this section, we break down individual components of TAS-GNN and perform an ablation study, which is reported in Table 2. Starting from baseline implementation, which does not differentiate neurons used by each node, we apply TAG to show the effect of topology-aware group-adaptive neurons. Then, we add our learnable initial threshold scheme to complete TAS-GNN. The results show that TAG alone can improve the performance across all datasets and models. This means that uneven spike distribution caused by indegree variance is a general problem shared across different graph datasets, and simply grouping the nodes with similar indegree to share the same threshold helps alleviate this problem. Lastly, adding a learnable initial threshold scheme further boosts the accuracy in almost all cases, demonstrating its efficacy and stability.

   Model & Method & MUTAG & PROTEINS & ENZYMES & NCI1 & IMDB-BINARY \\   & ANN  & 88.86 \(\) 5.48 & 77.81 \(\) 3.46 & 72.00 \(\) 4.37 & 76.42 \(\) 2.98 & 56.80 \(\) 4.80 \\  & SpikingGNN  & 90.96 \(\) 3.99 & 74.29 \(\) 2.68 & 50.67 \(\) 4.91 & 73.41 \(\) 1.60 & 68.40 \(\) 2.96 \\  & SpikingNet  & 87.81 \(\) 5.60 & 74.75 \(\) 3.20 & 50.00 \(\) 3.33 & 73.92 \(\) 1.54 & 20.30 \(\) 2.17 \\  & PCNN  & 87.28 \(\) 5.87 & 77.36 \(\) 2.68 & 56.33 \(\) 3.17 & 76.52 \(\) 1.46 & 71.60 \(\) 2.17 \\   & TAS-GNN & **96.32** \(\) 31.05 (+\) (53.35) & **77.45** \(\) 1.94 (+\) (40.09) & **56.50** \(\) 3.87 (+\) (01.77) & **78.12** \(\) (1.29) & **80.10** \(\) 2.49 (+\) (8.50) \\   & ANN  & 83.04 \(\) 4.23 & 77.54 \(\) 3.22 & 59.67 \(\) 3.48 & 67.88 \(\) 3.00 & 54.50 \(\) 2.14 \\  & SpikingGNN  & 78.71 \(\) 5.34 & 59.66 \(\) 0.21 & 29.17 \(\) 3.14 & 66.25 \(\) 1.77 & 50.00 \(\) 0.00 \\  & SpikingNet  & 78.22 \(\) 3.67 & 64.60 \(\) 3.22 & 51.67 \(\) 4.96 & 66.84 \(\) 1.60 & 50.00 \(\) 0.00 \\  & PCNN  & 82.49 \(\) 4.98 & 64.06 \(\) 3.27 & 39.50 \(\) 2.87 & 68.32 \(\) 1.49 & 50.00 \(\) 0.00 \\   & TAS-GNN & **96.32** \(\) 31.09 (+13.83) & **71.34** \(\) 3.03 (+6.74) & **52.33** \(\) 3.47 (+0.67) & **75.33** \(\) 2.41 (+\)(7.01) & **77.90** \(\) 2.18 (+\)27.90 \\   & ANN  & 95.23 \(\) 5.61 & 78.79 \(\) 3.74 & 33.67 \(\) 4.66 & 79.17 \(\) 3.07 & 70.40 \(\) 4.14 \\  & SpikingGNN  & 92.60 \(\) 4.41 & 77.81 \(\) 2.71 & 45.17 \(\) 5.01 & 70.29 \(\) 2.01 & 74.30 \(\) 1.47 \\  & SpikingNet  & 95.66 \(\) 4.62 & 78.43 \(\) 2.63 & 44.33 \(\) 3.98 & 74.77 \(\) 1.63 & **74.80** \(

### Sensitivity Study

To validate our method's efficacy in alleviating the sensitivity of the initial threshold value, we perform a sensitivity study varying the values from 0.0 to 10.0. We compare our scheme against the TAG method, which also adaptively modulates the threshold during inference but does not learn it from training. Our method consistently performs indifferently to the initial threshold value, which means arduous search or tuning is unnecessary to achieve stable accuracy. On the other hand, TAG is highly sensitive to the initial threshold and shows a performance gap up to 19.68%p except for GIN architecture, which is capturing structure well.

Since our scheme uses a learnable initial threshold, we also study its sensitivity for the learning rate, shown in Table 3. TAS-GNN performs best around \(=[0.005,0.1]\), and starts to degrade for further increment or decrement. As denoted in the experimental setting, we use \(=0.01\) as the default.

### Additional Analysis

In this section, we give additional analysis on TAS-GNN by studying its spike frequency distribution. In Figure 5, we provide the same spike frequency visualization as done in Section 3, but using TAS-GNN. Unlike Figure 1, which showed severe starvation with most nodes not generating spikes, Figure (a)a reveals that most nodes fire spikes, significantly alleviating the starvation problem. This is further illustrated Figure (b)b, where most neurons have non-zero spike values and, what's more, meaningfully reflect the topology of the graph. For nodes with higher degrees, the spikes are more frequent (close to 5) due to having more incoming spikes from their neighbors. For GNNs, such information is essential to capture the global topology of the graph. This shows that our design of TAS-GNN faithfully reflects such information and can successfully propagate such information using spikes.

## 6 Related Works

Graph ClassificationGraph classification requires identifying the global characteristics of each graph and is commonly applied to domains such as bioinformatics , chemoinformatics , or social network analysis [21; 37]. Popular examples include the molecular classification of chemical compounds, proteins, or RNAs, where identifying the graph structural information is crucial. Due to the success of GNNs, [27; 45; 52; 57] Most GNNs use a message passing paradigm  that only aggregates local features. Thus, to obtain global features representing the entire graph, graph pooling  is often used. Global pooling summarizes the entire graph into a fixed-size graph embedding, which can be done by simply averaging or taking minimum or maximum values of the node-wise embeddings. Other variations replace such simple operations with neural networks [46; 33] or integrate sorting to selectively choose which node embeddings to include . More advanced techniques such as hiearchical pooling utilize hiearchical information of graphs [40; 29; 18; 11] and usually show better representation learning. 

Figure 4: Sensitivity study of neurons to its initial threshold.

    &  & \)} \\   & & 0.50 & 1.50 & 2.50 & 5.00 & 7.00 & 10.00 \\   & TAG & 87.84 & 86.75 & 88.33 & 89.91 & 88.30 & 68.16 \\  & Ours & 95.79 & 97.37 & 96.32 & 95.79 & 95.23 & 90.99 \\   & TAG & 85.70 & 81.96 & 80.35 & 80.85 & 77.72 & 77.19 \\  & Ours & 94.18 & 93.65 & 96.32 & 93.68 & 91.58 & 92.60 \\   & TAG & 92.08 & 93.13 & 92.57 & 94.21 & 92.08 & 93.68 \\  & Ours & 94.18 & 94.74 & 95.76 & 93.68 & 94.71 & 89.4 \\   

Table 3: Sensitivity study on threshold learning rate using MUTAG.

Spiking Neural NetworksSNNs are a type of neural network where information is transmitted using spikes, similar to how biological neurons work. They use different neuron models for capturing spike signals effectively [23; 24] or adjusting parameters dynamically to compromise the accuracy [16; 49; 4; 34]. One major area of SNN research is converting traditional ANNs into SNNs by mapping ANN activation functions into spike signals [22; 41; 24; 42; 17]. Another focus is training SNNs directly using backpropagation, similar to ANNs, which involves using various techniques such as surrogate functions for backpropagation [43; 8] and adapting normalization techniques to SNNs [42; 12; 25; 62].

SNN for GraphsPrevious attempts to apply SNNs to graph datasets have primarily focused on node-level classification tasks [59; 44; 64] and have not yet been extended to graph-level tasks. While  explored the application of spike training to Graph Attention Networks (GAT), it implemented the message passing phase after the spiking phase, which deviates from previous structures. Additionally, recent efforts have begun to integrate SNNs with other techniques for contrastive learning , particularly in dynamic graphs , to adopt collaboration between GNNs and SNNs.

## 7 Conclusion

In this paper, we explore the application of SNNs to graph neural networks for graph classification for the first time. After thoroughly analyzing the graph's uneven spike distribution, we identify that the degree of each node correlates to this phenomenon. To better accommodate such characteristics of graphs, we propose topology-aware group-adaptive neurons, which uses separate neurons for each degree group in the graph. In addition, we propose to learn the initial threshold and adaptively adjust the threshold simultaneously to reduce its sensitivity and facilitate training using spikes. Combined with the modified architecture for graph classification, we name our method TAS-GNN, and show that it outperforms existing works by a noticeable margin.

Figure 5: Analysis on spike frequency variation of GCN using IMDB-BINARY  dataset.