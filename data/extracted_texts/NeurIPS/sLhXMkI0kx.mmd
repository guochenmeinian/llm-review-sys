# On skip connections and normalisation layers in deep optimisation

Lachlan E. MacDonald

Mathematical Institute for Data Science

Johns Hopkins University

lemacdonald@protonmail.com

Jack Valmadre

Australian Institute for Machine Learning

University of Adelaide

Hemanth Saratchandran

Australian Institute for Machine Learning

University of Adelaide

Simon Lucey

Australian Institute for Machine Learning

University of Adelaide

Most done while at the Australian Institute for Machine Learning, University of Adelaide

###### Abstract

We introduce a general theoretical framework, designed for the study of gradient optimisation of deep neural networks, that encompasses ubiquitous architecture choices including batch normalisation, weight normalisation and skip connections. Our framework determines the curvature and regularity properties of multilayer loss landscapes in terms of their constituent layers, thereby elucidating the roles played by normalisation layers and skip connections in globalising these properties. We then demonstrate the utility of this framework in two respects. First, we give the only proof of which we are aware that a class of deep neural networks can be trained using gradient descent to global optima even when such optima only exist at infinity, as is the case for the cross-entropy cost. Second, we identify a novel causal mechanism by which skip connections accelerate training, which we verify predictively with ResNets on MNIST, CIFAR10, CIFAR100 and ImageNet.

## 1 Introduction

Deep, overparameterised neural networks are efficiently trainable to global optima using simple first order methods. That this is true is immensely surprising from a theoretical perspective: modern datasets and deep neural network architectures are so complex and irregular that they are essentially opaque from the perspective of classical (convex) optimisation theory. A recent surge in inspired theoretical works  has elucidated this phenomenon, showing linear convergence of gradient descent on certain classes of neural networks, with certain cost functions, to global optima. The formal principles underlying these works are identical. By taking width sufficiently large, one guarantees uniform bounds on _curvature_ (via a Lipschitz gradients-type property) and _regularity_ (via a Polyak-Lojasiewicz-type inequality) in a neighbourhood of initialisation. Convergence to a global optimum in that neighbourhood then follows from a well-known chain of estimates .

Despite significant progress, the theory of deep learning optimisation extant in the literature presents at least three significant shortcomings:1. _It lacks a formal framework in which to compare common practical architecture choices_. Indeed, none of the aforementioned works consider the impact of ubiquitous (weight/batch) normalisation layers. Moreover, where common architectural modifications such as skip connections _are_ studied, it is unclear exactly what impact they have on optimisation. For instance, while in  it is shown that skip connections enable convergence with width polynomial in the number of layers, as compared with exponential width for chain networks, in  polynomial width is shown to be sufficient for convergence of both chain and residual networks.

2. _It lacks theoretical flexibility_. The consistent use of _uniform_ curvature and regularity bounds are insufficiently flexible to enable optimisation guarantees too far away from initialisation, where the local, uniform bounds used in previous theory no longer hold. In particular, proving globally optimal convergence for deep neural nets with the cross-entropy cost was (until now) an open problem .

3. _It lacks practical utility_. Although it is presently unreasonable to demand quantitatively predictive bounds on practical performance, existing optimisation theory has been largely unable to inform architecture design even qualitatively. This is in part due to the first item, since practical architectures typically differ substantially from those considered for theoretical purposes.

Our purpose in this article is to take a step in addressing these shortcomings. Specifically:

1. We provide a formal framework, inspired by , for the study of multilayer optimisation. Our framework is sufficiently general to include all commonly used neural network layers, and contains formal results relating the curvature and regularity properties of multilayer loss landscapes to those of their constituent layers. As instances, we prove novel results on the _global_ curvature and regularity properties enabled by normalisation layers and skip connections respectively, in contrast to the _local_ bounds provided in previous work.

2. Using these novel, global bounds, we identify a class of weight-normalised residual networks for which, given a linear independence assumption on the data, gradient descent can be provably trained to a global optimum arbitrarily far away from initialisation. From a regularity perspective, our analysis is _strictly more flexible_ than the uniform analysis considered in previous works, and in particular solves the open problem of proving global optimality for the training of deep nets with the cross-entropy cost.

3. Using our theoretical insight that skip connections aid loss regularity, we conduct a systematic empirical analysis of singular value distributions of layer Jacobians for practical layers. We are thereby able to predict that simple modifications to the classic ResNet architecture  will improve training speed. We verify our predictions on MNIST, CIFAR10, CIFAR100 and ImageNet.

## 2 Background

In this section we give a summary of the principles underlying recent theoretical advances in neural network optimisation. We discuss related works _after_ this summary for greater clarity.

### Smoothness and the PL-inequality

Gradient descent on a possibly non-convex function \(:^{p}_{ 0}\) can be guaranteed to converge to a global optimum by insisting that \(\) have _Lipschitz gradients_ and satisfy the _Polyak-Lojasiewicz inequality_. We recall these well-known properties here for convenience.

**Definition 2.1**.: Let \(>0\). A continuously differentiable function \(:^{p}\) is said to have \(\)**-Lipschitz gradients**, or is said to be \(\)**-smooth** over a set \(S^{p}\) if the vector field \(:^{p}^{p}\) is \(\)-Lipschitz. If \(S\) is convex, \(\) having \(\)-Lipschitz gradients implies that the inequality

\[(_{2})-(_{1})(_{1})^{T}(_{2}- _{1})+\|_{2}-_{1}\|^{2}\] (1)

holds for all \(_{1},_{2} S\).

The \(\)-smoothness of \(\) over \(S\) can be thought of as a uniform bound on the _curvature_ of \(\) over \(S\): if \(\) is twice continuously differentiable, then it has Lipschitz gradients over any compact set \(K\) with (possibly loose) Lipschitz constant given by

\[:=_{ K}\|D^{2}()\|,\] (2)

where \(D^{2}\) is the Hessian and \(\|\|\) denotes any matrix norm.

**Definition 2.2**.: Let \(>0\). A differentiable function \(:^{p}_{ 0}\) is said to satisfy the \(\)**-Polyak-Lojasiewicz inequality**, or is said to be \(\)-PL over a set \(S^{p}\) if

\[\|()\|^{2}()-_{^{}  S}(^{})\] (3)

for all \( S\).

The PL condition on \(\) over \(S\) is a uniform guarantee of _regularity_, which implies that all critical points of \(\) over \(S\) are \(S\)-global minima; however, such a function _need not be convex_. Synthesising these definitions leads easily to the following result (cf. Theorem 1 of ).

**Theorem 2.3**.: _Let \(:^{p}_{ 0}\) be a continuously differentiable function that is \(\)-smooth and \(\)-PL over a convex set \(S\). Suppose that \(_{0} S\) and let \(\{_{t}\}_{t=0}^{}\) be the trajectory taken by gradient descent, with step size \(<2^{-1}\), starting at \(_{0}\). If \(\{_{t}\}_{t=0}^{} S\), then \((_{t})\) converges to an \(S\)-global minimum \(^{*}\) at a linear rate:_

\[(_{t})-^{*}1-1- ^{t}(_{0})-^{*}\] (4)

_for all \(t\). _

Essentially, while the Lipschitz constant of the gradients controls whether or not gradient descent with a given step size can be guaranteed to _decrease_ the loss at each step, the PL constant determines _by how much_ the loss will decrease. These ideas can be applied to the optimisation of deep neural nets as follows.

### Application to model optimisation

The above theory can be applied to parameterised models in the following fashion. Let \(f:^{p}^{d_{0}}^{d_{L}}\) be a differentiable, \(^{p}\)-parameterised family of functions \(^{d_{0}}^{d_{L}}\) (in later sections, \(L\) will denote the number of layers of a deep neural network). Given \(N\) training data \(\{(x_{i},y_{i})\}_{i=1}^{N}^{d_{0}}^{d_{L}}\), let \(F:^{p}^{d_{L} N}\) be the corresponding parameter-function map defined by

\[F()_{i}:=f(,x_{i}).\] (5)

Any differentiable cost function \(c:^{d_{L}}^{d_{L}}_{ 0}\), convex in the first variable, extends to a differentiable, convex function \(:^{d_{L} N}_{ 0}\) defined by

\[(z_{i})_{i=1}^{N}:=_{i=1}^{N}c(z_{i},y_{i}),\] (6)

and one is then concerned with the optimisation of the composite \(:= F:^{p}_{ 0}\) via gradient descent.

To apply Theorem 2.3, one needs to determine the smoothness and regularity properties of \(\). By the chain rule, the former can be determined given sufficient conditions on the derivatives \(D F\) and \(DF\) (cf. Lemma 2 of ). The latter can be bounded by Lemma 3 of , which we recall below and prove in the appendix for the reader's convenience.

**Theorem 2.4**.: _Let \(S^{p}\) be a set. Suppose that \(:^{d_{L} N}_{ 0}\) is \(\)-PL over \(F(S)\) with minimum \(_{S}^{*}\). Let \((DF())\) denote the smallest eigenvalue of \(DF()DF()^{T}\). Then_

\[\|()\|^{2}\,(DF())\,()- _{S}^{*}\] (7)

_for all \( S\). _

Note that Theorem 2.4 is vacuous (\((DF())=0\) for all \(\)) unless in the overparameterised regime (\(p d_{L}N\)). Even in this regime, however, Theorem 2.4 does not imply that \(\) is PL unless \(()\) can be uniformly lower bounded by a positive constant over \(S\). Although universally utilised in previous literature, such a uniform lower bound will not be possible in our _global_ analysis, and our convergence theorem _does not_ follow from Theorem 2.3, in contrast to previous work. Our theorem requires additional argumentation, which we believe may be of independent utility.

Related works

Convergence theorems for deep _linear_ networks with the square cost are considered in . In , it is proved that the tangent kernel of a multi-layer perceptron (MLP) becomes approximately constant over all of parameter space as width goes to infinity, and is positive-definite for certain data distributions, which by Theorem 2.4 implies that all critical points are global minima. Strictly speaking, however,  does not prove convergence of gradient descent: the authors consider only gradient flow, and leave Lipschitz concerns untouched. The papers  prove that overparameterized neural nets of varying architectures can be optimised to global minima _close to initialisation_ by assuming sufficient width of several layers. While  does consider the cross-entropy cost, convergence to a global optimum is _not_ proved: it is instead shown that perfect classification accuracy can be achieved close to initialisation during training. Improvements on these works have been made in , wherein large width is required of only a single layer.

It is identified in  that linearity of the final layer is key in establishing the approximate constancy of the tangent kernel for wide networks that was used in . By making explicit the implicit use of the PL condition present in previous works ,  proves a convergence theorem even with _nonlinear_ output layers. The theory explicated in  is formalised and generalised in . A key weakness of all of the works mentioned thus far (bar the purely formal ) is that their hypotheses imply that optimisation trajectories are always close to initialisation. Without this, there is no obvious way to guarantee the PL-inequality along the optimisation trajectory, and hence no way to guarantee one does not converge to a suboptimal critical point. However such training is not possible with the cross-entropy cost, whose global minima only exist at infinity. There is also evidence to suggest that such training must be avoided for state-of-the-art test performance . In contrast, our theory gives convergence guarantees even for trajectories that travel arbitrarily far from initialisation, and is the only work of which we are aware that can make this claim.

Among the tools that make our theory work are skip connections  and weight normalisation . The smoothness properties of normalisation schemes have previously been studied , however they only give pointwise estimates comparing normalised to non-normalised layers, and do not provide a global analysis of Lipschitz properties as we do. The regularising effect of skip connections on the loss landscape has previously been studied in , however this study is not tightly linked to optimisation theory. Skip connections have also been shown to enable the interpretation of a neural network as coordinate transformations of data manifolds . Mean field analyses of skip connections have been conducted in  which necessitate large width; our own analysis does not. A similarly general framework to that which we supply is given in ; while both encapsulate all presently used architectures, that of  is designed for the study of infinite-width tangent kernels, while ours is designed specifically for optimisation theory. Our empirical singular value analysis of skip connections complements existing theoretical work using random matrix theory . These works have not yet considered the shifting effect of skip connections on layer Jacobians that we observe empirically.

Our theory also links nicely to the intuitive notions of gradient propagation  and dynamical isometry already present in the literature. In tying Jacobian singular values rigorously to loss regularity in the sense of the Polyak-Lojasiewicz inequality, our theory provides a new link between dynamical isometry and optimisation theory : specifically dynamical isometry ensures better PL conditioning and therefore faster and more reliable convergence to global minima. In linking this productive section of the literature to optimisation theory, our work may open up new possibilities for convergence proofs in the optimisation theory of deep networks. We leave further exploration of this topic to future work.

Due to this relationship with the notion of dynamical isometry, our work also provides optimisation-theoretic support for the empirical analyses of  that study the importance of layerwise dynamical isometry for trainability and neural architecture search . Recent work on deep kernel shaping shows via careful tuning of initialisation and activation functions that while skip connections and normalisation layers may be _sufficient_ for good trainability, they are not _necessary_. Other recent work has also shown benefits to inference performance by removing skip connections from the trained model using a parameter transformation  or by removing them from the model altogether and incorporating them only into the optimiser .

Finally, a line of work has recently emerged on training in the more realistic, large learning rate regime known as "edge of stability" [9; 3; 10]. This intriguing line of work diverges from ours, and its integration into the framework we present is a promising future research direction.

## 4 Formal framework

In this section we will define our theoretical framework. We will use \(^{n m}\) to denote the space of \(n m\) matrices, and vectorise rows first. We use \(_{m}\) to denote the \(m m\) identity matrix, and \(1_{n}\) to denote the vector of ones in \(^{n}\). We use \(\) to denote the Kronecker (tensor) product of matrices, and given a vector \(v^{n}\) we use \((v)\) to denote the \(n n\) matrix whose leading diagonal is \(v\). Given a matrix \(A^{n m}\), and a seminorm \(\|\|\) on \(^{m}\), \(\|A\|_{row}\) will be used to denote the vector of \(\|\|\)-seminorms of each row of \(A\). The smallest singular value of \(A\) is denoted \((A)\), and the smallest eigenvalue of \(AA^{T}\) denoted \((A)\). Full proofs of all of our results can be found in the appendix.

Our theory is derived from the following formal generalisation of a deep neural network.

**Definition 4.1**.: By a **multilayer parameterised system (MPS)** we mean a family \(\{f_{l}:^{p_{l}}^{d_{l-1} N}^{d_{ l} N}\}_{l=1}^{L}\) of functions. Given a data matrix \(X^{d_{0} N}\), we denote by \(F:^{_{i=1}^{L}p_{i}}^{d_{L} N}\) the **parameter-function map2** defined by

\[F():=f_{L}(_{L}) f_{1}(_{1})(X),\] (8)

for \(=(_{1},,_{L})^{T}^{_{i=1}^{L} p_{i}}\).

Definition 4.1 is sufficiently general to encompass all presently used neural network layers, but _we will assume without further comment from here on in that all layers are continuously differentiable_. Before we give examples, we record the following result giving the form of the derivative of the parameter-function map, which follows easily from the chain rule. It will play a key role in the analysis of the following subsections.

**Proposition 4.2**.: _Let \(\{f_{l}:^{p_{l}}^{d_{l-1} N}^{d_{ l} N}\}_{l=1}^{L}\) be a MPS, and \(X^{d_{0} N}\) a data matrix. For \(1 l L\), denote the derivatives of \(f_{l}\) with respect to the \(^{p_{l}}\) and \(^{d_{l-1}}\) variables by \(Df_{l}\) and \(Jf_{l}\) respectively, and let \(f_{<l}(,X)\) denote the composite_

\[f_{<l}(,X):=f_{l-1}(_{l-1}) f_{1}(_{1} )(X).\] (9)

_The derivative \(DF\) of the associated parameter-function map is given by_

\[DF()=D_{_{1}}F(),,D_{_{L}}F( ),\] (10)

_where for \(1 l<L\), \(D_{_{l}}F()\) is given by the formula_

\[_{j=l+1}^{L}Jf_{j}_{j},f_{<j}(,X) Df_{l}_{l},f_{<l}(,X),\] (11)

_with the product taken so that indices are arranged in descending order from left to right. _

All common differentiable neural network layers fit into this framework; we record some examples in detail in the appendix. We will see in the next section that insofar as one wishes to guarantee global smoothness, the usual parameterisation of affine layers is poor, although this defect can be ameliorated to differing extents by normalisation strategies.

### Smoothness

In this subsection, we give sufficient conditions for the derivative of the parameter-function map of a MPS to be bounded and Lipschitz. We are thereby able to give sufficient conditions for any associated loss function to have Lipschitz gradients on its sublevel sets. We begin with a formal proposition that describes the Lipschitz properties of the derivative of the parameter-function map of a MPS in terms of those of its constituent layers.

**Proposition 4.3**.: _Let \(\{f_{l}:^{p_{l}}^{d_{l-1} N}^{d_{l}  N}\}_{l=1}^{L}\), and let \(\{S_{l}^{p_{l}}\}_{l=1}^{L}\) be subsets of the parameter spaces. Suppose that for each bounded set \(B_{l}^{d_{l-1} N}\), the maps \(f_{l}\), \(Df_{l}\) and \(Jf_{l}\) are all bounded and Lipschitz on \(S_{l} B_{l}\). Then for any data matrix \(X^{d_{0} N}\), the derivative \(DF\) of the associated parameter-function map \(F:^{p}^{d_{L} N}\) is bounded and Lipschitz on \(S:=_{j=1}^{L}S_{j}^{p}\). _

Proposition 4.3 has the following immediate corollary, whose proof follows easily from Lemma B.1.

**Corollary 4.4**.: _Let \(c:^{d_{L}}\) be any cost function whose gradient is bounded and Lipschitz on its sublevel sets \(Z_{}:=\{z^{d_{L}}:c(z)\}\). If \(\{f_{l}:^{p_{l}}^{d_{l-1} N}^{d_{ l} N}\}_{l=1}^{L}\) is any MPS satisfying the hypotheses of Proposition 4.3 and \(X^{d_{0} N}\), then the associated loss function \(:= F\) has Lipschitz gradients over \(S:=_{l=1}^{L}S_{l}\). _

The most ubiquitous cost functions presently in use (the mean square error and cross entropy functions) satisfy the hypotheses of Corollary 4.4. We now turn to an analysis of common layer types in deep neural networks and indicate to what extent they satisfy the hypotheses of Proposition 4.3.

**Theorem 4.5**.: _Fix \(>0\). The following layers satisfy the hypotheses of Proposition 4.3 over all of parameter space._

1. _Continuously differentiable nonlinearities._
2. _Bias-free_ \(\)_-weight-normalised or_ \(\)_-entry-normalised affine layers_3_._

_3. Any residual block whose branch is a composite of any of the above layer types._

_Consequently, the loss function of any neural network composed of layers as above, trained with a cost function satisfying the hypotheses of Corollary 4.4, has globally Lipschitz gradients along any sublevel set. _

The proof we give of Theorem 4.5 also considers composites bn \(\) aff of batch norm layers with affine layers. Such composites satisfy the hypotheses of Proposition 4.3 only over sets in data-space \(^{d N}\) which consist of matrices with nondegenerate covariance. Since such sets are generic (probability 1 with respect to any probability measure that is absolutely continuous with respect to Lebesgue measure), batch norm layers satisfy the hypotheses of Proposition 4.3 with high probability over random initialisation.

Theorem 4.5 says that normalisation of parameters enables _global_ analysis of the loss, while standard affine layers, due to their unboundedness, are well-suited to analysis only over bounded sets in parameter space.

### Regularity

Having examined the Lipschitz properties of MPS and given examples of layers with _global_ Lipschitz properties, let us now do the same for the _regularity_ properties of MPS. Formally one has the following simple result.

**Proposition 4.6**.: _Let \(\{f_{l}:^{p_{l}}^{d_{l-1} N}^{d_{l}  N}\}_{i=1}^{L}\) be a MPS and \(X^{d_{0} N}\) a data matrix. Then_

\[_{l=1}^{L}Df_{l}_{l},f_{<l}(,X) _{j=l+1}^{L}Jf_{j}(_{j},f_{<j}(,X))\] (12)

_is a lower bound for \(DF()\). _

Proposition 4.6 tells us that to guarantee good regularity, it suffices to guarantee good regularity of the constituent layers. In fact, since Equation (12) is a sum of non-negative terms, it suffices merely to guarantee good regularity of the parameter-derivative of the _first_ layer4, and of the input-output Jacobians of every _subsequent_ layer. Our next theorem says that residual networks with appropriately normalised branches suffice for this.

**Theorem 4.7**.: _Let \(\{g_{l}:^{p_{l}}^{d_{l-1} N}^{d_{ l} N}\}_{i=1}^{L}\) be a MPS and \(X^{d_{0} N}\) a data matrix for which the following hold:_1. \(d_{l-1} d_{l}\), and \(\|Jg_{l}(_{l},Z)\|_{2}<1\) for all \(_{l}^{p_{l}}\), \(Z^{d_{l-1} N}\) and \(l 2\).
2. \(N d_{0}\), \(X\) is full rank, \(p_{1} d_{1}d_{0}\) and \(f_{1}:^{p_{1}}^{d_{0} N}^{d_{1}  N}\) is a \(P\)-parameterised affine layer, for which \(DP(_{1})\) is full rank for all \(_{1}^{p_{1}}\).

For any sequence \(\{I_{l}:^{d_{l-1} N}^{d_{l} N}\}_{l=2}^{L}\) of linear maps whose singular values are all equal to 1, define a new MPS \(\{f_{l}\}_{l=1}^{L}\) by \(f_{1}:=g_{1}\), and \(f_{l}(_{l},X):=I_{l}X+g_{l}(_{l},X)\). Let \(F:^{p}^{d_{L} N}\) be the parameter-function map associated to \(\{f_{l}\}_{l=1}^{L}\) and \(X\). Then \((DF())>0\) (but not uniformly so) for all \(^{p}\). 

In the next and final subsection we will synthesise Theorem 4.5 and Theorem 4.7 into our main result: a global convergence theorem for gradient descent on appropriately normalised residual networks.

## 5 Main theorem

We will consider _normalised residual networks_, which are MPS of the following form. The first layer is an \(\)-entry-normalised, bias-free affine layer \(f_{1}=_{}:^{d_{1} d_{0}}^{ d_{0} N}^{d_{1} N}\) (cf. Example A.1). Every subsequent layer is a residual block

\[f_{l}(_{l},X)=I_{l}X+g(_{l},X).\] (13)

Here, for all \(l 2\), we demand that \(d_{l-1} d_{l}\), \(I_{i}:^{d_{l-1} N}^{d_{i} N}\) is some linear map with all singular values equal to 1, and the residual branch \(g(_{i},X)\) is a composite of weight- or entry-normalised, bias-free5 affine layers \(_{P}\) (cf. Example A.1), rescaled so that \(\|P(w)\|_{2}<1\) uniformly for all parameters \(w\), and elementwise nonlinearities \(\) for which \(\|D\| 1\) everywhere (Example A.2). These hypotheses ensure that Theorem 4.7 holds for \(\{f_{i}\}_{i=1}^{L}\) and \(X\): see the Appendix for a full proof.

We emphasise again that our main theorem below does _not_ follow from the usual argumentation using smoothness and the PL inequality, due to the lack of a _uniform_ PL bound. Due to the novelty of our technique, which we believe may be of wider utility where uniform regularity bounds are unavailable, we include an idea of the proof below.

**Theorem 5.1**.: _Let \(\{f_{i}\}_{i=1}^{L}\) be a normalised residual network; \(X\) a data matrix of linearly independent data, with labels \(Y\); and \(c\) any continuously differentiable, convex cost function. Then there exists a learning rate \(>0\) such that gradient descent on the associated loss function converges from **any** initialisation to a global minimum._

Idea of proof.: One begins by showing that Theorems 4.5 and 4.7 apply to give globally \(\)-Lipschitz gradients and a positive smallest eigenvalue of the tangent kernel at all points in parameter space. Thus for learning rate \(<2^{-1}\), there exists a positive sequence \((_{t}=(DF(_{t})))_{t}\) (see Theorem 2.4) such that \(\|_{t}\|^{2}_{t}_{t}\), for which the loss iterates \(_{t}\) therefore obey

\[_{t}-^{*}_{i=0}^{t}(1-_{i})(_{0}-^{*}),\]

where \(=(1-2^{-1})>0\). To show global convergence one must show that \(_{t=0}^{}(1-_{t})=0\).

If \(_{t}\) can be uniformly lower bounded (e.g. for the square cost) then Theorem 2.3 applies to give convergence as in all previous works. However, \(_{t}\)_cannot_ be uniformly lower bounded in general (e.g. for the cross-entropy cost). We attain the general result by showing that, despite admitting no non-trivial lower bound in general, \(_{t}\) can always be guaranteed to vanish _sufficiently slowly_ that global convergence is assured. 

We conclude this section by noting that practical deep learning problems typically do not satisfy the hypotheses of Theorem 5.1: frequently there are more training data than input dimensions (such as for MNIST and CIFAR), and many layers are not normalised or skip-connected. Moreover our Lipschitz bounds are worst-case, and will generally lead to learning rates much smaller than are used in practice. Our strong hypotheses are what enable a convergence guarantee from any initialisation, whereas in practical settings initialisation is of key importance. Despite the impracticality of Theorem 5.1, in the next section we show that the ideas that enable its proof nonetheless have practical implications.

Practical implications

Our main theorem is difficult to test directly, as it concerns only worst-case behaviour which is typically avoided in practical networks which do not satisfy its hypotheses. However, our framework more broadly nonetheless yields practical insights. Informally, Theorem 4.7 gives conditions under which:

_skip connections aid optimisation by improving loss regularity._

In this section, we conduct an empirical analysis to demonstrate that _this insight holds true even in practical settings_, and thereby obtain a novel, _causal_ understanding of the benefits of skip connections in practice. With this causal mechanism in hand, we recommend simple architecture changes to practical ResNets that consistently (albeit modestly) improve convergence speed as predicted by theory. All code is available at https://github.com/lemacdonald/skip-connections-normalisation/.

### Singular value distributions

Let us again recall the setting \(= F\) of Theorem 2.4. In the overparameterised setting, the smallest singular value of \(DF\) gives a _pessimistic lower bound_ on the ratio \(\|\|^{2}/(-^{*})\), and hence a _pessimistic_ lower bound on training speed (cf. Theorems 2.4, 2.3). Indeed, this lower bound is only attained when the vector \(\) perfectly aligns with the smallest singular subspace of \(DF\): a probabilistically unlikely occurence. In general, since \(=DF^{T}\,\), _the entire singular value distribution_ of \(DF\) at a given parameter will play a role in determining the ratio \(\|\|^{2}/(-^{*})\), and hence training speed.

Now \(DF\) is partly determined by products of layer Jacobians (cf. Proposition 4.2). As such, the distribution of singular values of \(DF\) is determined in part by the distribution of singular values of such layer Jacobian products, which are themselves determined by the singular value distributions of each of the individual layer Jacobians. In particular, if through some architectural intervention each layer Jacobian could have its singular value distribution shifted upwards, we would expect the singular value distribution of \(DF\) to be shifted upwards, too. Our argument in the previous paragraph then suggests that such an intervention will result in faster training, provided of course that the upwards-shifting is not so large as to cause exploding gradients.

Figure 1 shows in the _linear_ setting that a skip connection constitutes precisely such an intervention. We hypothesise that this continues to hold even in the nonlinear setting.

**Hypothesis 6.1**.: _The addition of a deterministic skip connection, all of whose singular values are 1, across a composite of possibly nonlinear random layers, shifts upwards the singular value distribution of the corresponding composite Jacobian6, thereby improving convergence speed at least initially._

We test Hypothesis 6.1 in the next subsections.

### Mnist

Recall that the ResNet architecture  consists in part of a composite of residual blocks

\[f(,A,X)=AX+g(,X),\] (14)

where \(A\) is either the identity transformation, in the case when the dimension of the output of \(g(,X)\) is the same as the dimension of its input; or a randomly initialised 1x1 convolution otherwise.

Figure 1: Singular value histogram of \(500 500\) matrix \(A\) (left) with entries sampled iid from \(U(-1/,1/)\). Adding an identity matrix (right) shifts the distribution upwards.

Hypothesis 6.1 predicts that the additions of the identity skip connections will shift upwards the singular value distributions of composite layer Jacobians relative to the equivalent chain network, thus improving convergence speed. It also predicts that replacing 1x1 convolutional skip connections \(A\) with \(I+A\), where \(I\) is deterministic with all singular values equal to 1, will do the same.

We first test this hypothesis by doing gradient descent, with a learning rate of 0.1, on 32 randomly chosen data points from the MNIST training set7. We compare three models, all with identical initialisations for each trial run using the default PyTorch initialisation:

1. (Chain) A batch-normed convolutional chain network, with six convolutional layers.
2. (Res) The same as (1), but with convolutional layers 2-3 and 5-6 grouped into two residual blocks, and with an additional 1x1 convolution as the skip connection in the second residual block, as in Equation (14).
3. (ResAvg) The same as (2), but with the 1x1 convolutional skip connection of Equation (14) replaced by

\[(,A,X)=(I+A)X+g(,X),\] (15)

where \(I\) is an average pool, rescaled to have all singular values equal to 1.

Hypothesis 6.1 predicts that the singular value distributions of the composite layer Jacobians will be more positively shifted going down the list, resulting in faster convergence. This is indeed what we observe (Figure 2).

### CIFAR and ImageNet

We now test Hypothesis 6.1 on CIFAR and ImageNet. We replace all of the convolution-skip connections in the ResNet architecture  with sum(average pool, convolution)-skip connections as in Equation (15) above, leaving all else unchanged. Hypothesis 6.1 predicts that these modifications will improve training speed, which we verify using default PyTorch initialisation schemes.

We trained PreAct-ResNet18 on CIFAR10/100 and PreAct-ResNet50  on ImageNet using standard training regimes (details can be found in the appendix), performing respectively 10 and 3 trial runs

Figure 3: Mean training loss curves for ResNets on CIFAR (10 trials) and ImageNet (3 trials), with 1 standard deviation shaded. The modifications improve training speed as predicted.

Figure 2: (a)-(c) Singular value histograms of composite layer Jacobians averaged over first 10 training iterations. Distributions are shifted upwards as deterministic skip connections are added, resulting in faster convergence (d). Means over 10 trials shown.

on CIFAR and ImageNet. We performed the experiments at a range of different learning rates. We have included figures for the best performing learning rates on the _original_ model (measured by loss value averaged over the final epoch) in Figure 3, with additional plots and validation accuracies in the appendix. Validation accuracies were not statistically significantly different between the two models on CIFAR10/100. Although the modified version had statistically significantly better validation accuracy in the ImageNet experiment, we believe this is only due to the faster convergence, as the training scheme was not sufficient for the model to fully converge.

## 7 Discussion

Our work suggests some open research problems. First, the recently-developed edge of stability theory  could be used in place of our Lipschitz bounds to more realistically characterise training of practical nets with large learning rates. Second, like in pervious works , the heavy-lifting for our PL-type bounds is all done by the parameter-derivative of a single layer, and the bounds would be significantly improved by an analysis that considers all layers. Third, extension of the theory to SGD is desirable. Fourth, the dependence of Hypothesis 6.1 on weight variance should be investigated. Fifth, our empirical results on the impact of skip connections on singular value distributions suggests future work using random matrix theory .

Beyond these specifics, our formal framework provides a setting in which all neural network layers can be analysed in terms of their effect on the key loss landscape properties of smoothness and regularity, and is the first to demonstrate that a _uniform_ bound on regularity is not necessary to prove convergence. We hope the tools we provide in this paper will be of use in extending deep learning optimisation theory to more practical settings than has so far been the case.

## 8 Conclusion

We gave a formal theoretical framework for studying the optimisation of multilayer systems. We used the framework to give the first proof that a class of deep neural networks can be trained by gradient descent even to global optima at infinity. Our theory generates the novel insight that skip connections aid optimisation speed by improving loss regularity, which we verified empirically using practical datasets and architectures.

## 9 Acknowledgements

We thank the anonymous reviewers for their time reviewing the manuscript. Their critiques helped to improve the paper.