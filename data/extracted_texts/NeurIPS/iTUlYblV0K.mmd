# MQuAKE-Remastered:

Multi-Hop Knowledge Editing Can Only Be

Advanced With Reliable Evaluations

Shaochen (Henry) Zhong\({}^{}\), Yifan Lu\({}^{}\), Lize Shao\({}^{}\), Bhargav Bhushanam\({}^{}\), Xiaocong Du\({}^{}\), Louis Feng\({}^{}\), Yixin Wan\({}^{}\), Yiwei Wang\({}^{}\), Daochen Zha\({}^{}\), Yucheng Shi\({}^{}\), Ninghao Liu\({}^{}\), Kaixiong Zhou\({}^{}\), Shuai Xu\({}^{}\), Vipin Chaudhary\({}^{}\), and Xia Hu\({}^{}\)

\({}^{}\) Department of Computer Science, Rice University

\({}^{}\) School of Computing, University of Georgia

\({}^{}\) Department of Electrical and Computer Engineering, North Carolina State University

\({}^{}\) Department of Computer and Data Sciences, Case Western Reserve University

\({}^{}\) Department of Computer Science, University of California, Los Angeles

\({}^{}\) Meta Platforms, Inc.

Equal contribution. Work corresponds to Shaochen (Henry) Zhong <shaochen.zhong@rice.edu>. Submitted to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. Do not distribute.

###### Abstract

Large language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, _knowledge editing_ often comes to the rescue by delivering efficient patches for such erroneous answers without significantly altering the rests, where many editing methods have seen reasonable success when the editing targets are simple and direct (e.g., _"what club does Lionel Messi currently play for?"_). However, knowledge fragments like this are often deeply intertwined in the real world, making effectively propagating the editing effect to non-directly related questions a practical challenge (to entertain an extreme example: _"What car did the wife of the owner of the club that Messi currently plays for used to get to school in the 80s?"_). Prior arts have coined this task as _multi-hop knowledge editing_ with the most popular dataset being MQuAKE, serving as the sole evaluation benchmark for many later proposed editing methods due to the expensive nature of making knowledge editing datasets at scale. In this work, we reveal that **up to 33% or 76% of MQuAKE's questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural oversights**. Our work provides a detailed audit of MQuAKE's error pattern and a comprehensive fix without sacrificing its dataset capacity. Additionally, we benchmarked almost all proposed MQuAKE-evaluated editing methods on our post-fix dataset, MQuAKE-Remastered. It is our observation that many methods try to overfit the original MQuAKE by exploiting some data-specific properties of MQuAKE. We provide a guideline on how to faithfully approach such datasets and show that a simple, minimally invasive approach can bring excellent editing performance without such exploitation. Please refer to [https://github.com/henryzhongsc/MQuAKE-Remastered](https://github.com/henryzhongsc/MQuAKE-Remastered) and supplemental material for assets.

## 1 Introduction

Given the widespread public-facing popularity of various Large Language Model-powered (LLM) products (Zhao et al., 2023; Yang et al., 2024), even an occasional user has likely experienced LLMs giving out erroneous answers to factually rooted, knowledge-intensive questions. While the reasons why LLMs would hallucinate such kind of misinformation is complex and still an open problem -- noisy training data, model bias, out-of-distribution questions, or even simply because the world has moved on after a certain knowledge cutoff date, all likely contributed their fair share to this rather undesired character of LLMs (Huang et al., 2023; Zhang et al., 2023)-- **under a practical context, _knowledge editing_ is often considered the go-to remedy by delivering efficient patches for such erroneous answers** without significantly altering the LLM's output on unrelated queries (Sinitsin et al., 2020; Mitchell et al., 2022).

With the growing need to have more credible and trustworthy LLMs, a vast amount of LLM-specific knowledge editing methods have been proposed, and many of them have seen reasonable success in addressing editing targets that are simple and direct. For example, most modern knowledge editing methods can reliably edit the answer of _"What club does Lionel Messi currently play for?"_ from _"Paris Saint-Germain"_ to _"Inter Miami CF"_ and therefore correctly reflecting the occupation status of Messi (Zhong et al., 2023).

### Multi-hop knowledge editing poses practical significance and non-trial challenges.

However, due to the intertwined nature of different knowledge fragments, a small change in one knowledge fragment can produce ripple-like effects on a vast amount of related questions (Zhong et al., 2023; Cohen et al., 2023). It is often a non-trivial challenge to efficiently propagate the editing effect to non-directly related questions with proper precision and locality. E.g., for a -- in this case intensionally extreme -- question like _"What car did the wife of the owner of the club that Messi currently plays for used to get to school in the 80s?"_ Many knowledge-edited LLMs can still struggle while being fully aware of Messi's abovementioned club transfer (Zhong et al., 2023).

Prior arts have realized the practical significance of being able to edit such complex/non-direct questions upon a certain knowledge update, as different knowledge fragments are almost always deeply entangled with each other in the real world (Zhong et al., 2023; Cohen et al., 2023; Wei et al., 2024). Meanwhile, exhausting all potential combinations of questions related to one or a few updated knowledge fragments is impractical, if not totally impossible: imagining editing an LLM for every possible question influenced by the abovementioned club transfer of Messi. Even if it is feasible, this poses high operational costs and comes with the intrinsic risks of editing a mass amount of targets; not to mention a repeated effort would be required should Messi ever opt to transfer again.

It is intuitive that a practical knowledge editing method should be able to produce correct answers to relevant factual questions with only a few updated knowledge fragments available. This task has been coined as _multi-hop knowledge editing_**with the founding, largest, as well as the most popular dataset to date being MQuAKE by Zhong et al. (2023); serving as the sole evaluation backbone for many proposed modern editing methods** due to the expensive nature of making counterfactual and temporal datasets at such a scale (\(>\) 10,000 cases provided, more about the dataset statistics in Table 6). Note that such expansiveness is further multiplied given the abovementioned ripple effect of multi-hop question answering, as one knowledge update of a subquestion can potentially lead to multiple updated answers across a large number of cases.

Unfortunately, MQuAKE is flawed due to unintentional clerical and procedural errors -- we fixed/remade it and re-benchmarked almost all proposed multi-hop knowledge editing methods.

While MQuAKE is the founding dataset of multi-hop knowledge editing tasks and very much brings life to this vital subject, through a comprehensive audit, we reveal that **up to 33% or 76% of MQuAKE questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural errors**; which inevitably cast doubts on the effectiveness of developed methods (especially the ones that solely) evaluated on MQuAKE, and **present as ahidden peril to the field's progress as such flaws are largely unknown to the knowledge editing community before our work.** We highlight that the flaws of MQuAKE is an already massive yet constantly growing issue, as MQuAKE is one of the fastest-growing datasets in terms of adaptation in the editing community, yet, the task it is trying to tackle -- building more reliable LLM -- is without a doubt crucial aspect of NLP development. To pave the way for future advancement of multi-hop knowledge editing, we present our work with the following contributions:

* **A comprehensive audit of MQuAKE:** We are the first to present a comprehensive audit of the existing errors within MQuAKE , bringing awareness to the knowledge editing community regarding this popular dataset with significant task importance attached.
* **Fix/remake MQuAKE to MQuAKE-Remastered:** We present the only available fix/remake that not only patches all discovered errors, and done so without sacrificing the intended intensity and capacity of the original MQuAKE whenever possible.
* **Extensively re-benchmark of almost all existing multi-hop knowledge editing methods:** Given the currently existing reports based upon the original MQuAKE are flawed reflections of such proposed methods' capability, we additionally re-benchmark almost all existing multi-hop knowledge editing methods that are available against our MQuAKE-Remastered datasets.
* **Guidance for future multi-hop knowledge editing development.** Upon our extensive re-benchmark results, we observe that many proposed multi-hop knowledge editing methods intentionally or unintentionally overfit the original MQuAKE dataset by applying data-specific operations that are largely unique to the MQuAKE dataset family. We provide guidance on how to faithfully approach these datasets and additionally show that a simple, minimally invasive approach with no such operations can also achieve excellent editing performance.

## 2 Preliminary

### Background of MQuAKE

MQuAKE (Multi-hop Question Answering for Knowledge Editing) is a knowledge editing dataset focusing on the abovementioned multi-hop question answering tasks proposed in Zhong et al. , where every case of MQuAKE is a multi-hop question made by a chain of single-hop subquestions. Specifically, MQuAKE is constructed based on the Wikidata:RDF dataset , which, in its rawest format, is a knowledge graph consisting 15+ trillion of Resource Description Framework (RDF) triples1. MQuAKE essentially builds a much more concise subgraph with only 37 manually elected common relations and top 20% of the most common entities, where a walk of \(\{2,3,4\}\)-hop on this subgraph can form a case (which is a chain of \(\{2,3,4\}\) single-hop subquestions connected together) in the MQuAKE dataset.

MQuAKE is presented as two (but in practice, it is essentially three) sub-datasets: MQuAKE-CF and MQuAKE-t. The former focuses on counterfactual tasks, while the latter on temporal changes. We highlight that there is also a MQuAKE-cf-3k dataset, which is a subset of MQuAKE-CF that only contains 3,000 cases in total (with 1,000 cases for \(\{2,3,4\}\)-hop questions respectively). Authors of MQuAKE evaluate their proposed method, MeLLo , upon this MQuAKE-cf-3k dataset, citing limited compute resources; which then become an unspoken standard practice for the majority of the later proposed multi-hop knowledge editing methods . Due to the very popularity of this sub-sampled dataset, we provide our error analysis mostly based on MQuAKE-cf-3k and MQuAKE-t in the following SS3. For interested readers, we additionally provide the same error analysis upon the full MQuAKE-cf in the Appendix B.2, which is only more drastic than MQuAKE-cf-3k due to MQuAKE-cf being a much larger superset of the already compromised MQuAKE-cf-3k. We also collect the dataset statistics in Table 6 to provide a numerical overview of the composition of all three MQuAKE datasets.

### Evaluating using MQuAKE

Datasets like MQuAKE-CF or MQuAKE-cf-3k are often evaluated against different "editing intensity," which is controlled by how many cases among all tested cases are considered "edited," mimicking different levels of deviation between the learned knowledge stored in the LLM and the desire edited knowledge. This is a sound practice because proper knowledge editing methods should perform well when different numbers of knowledge fragments are edited, as it is equally important to navigate when a significant amount of knowledge is updated, as well as to recognize the few edited knowledge and limit their influence from unrelated unedited knowledge with proper editing locality.

In its original paper, MQuAKE-cf-3k is evaluated when \(\{1,100,100,3000\}\) of its 3,000 cases are edited, similarly, MQuAKE-t is evaluated when \(\{1,100,500,1868\}\) of its 1,868 cases being edited, forming an experiment report like Table 5. This kind of report granularity (a gradual coverage from a few edits to all cases being edited) is also adopted by the majority of later proposed multi-hop knowledge editing methods, either in full  or in spirit with different subsample settings . In this work, we report at an even finer level of granularity for maximum cross-reference potentials.

## 3 Auditing MQuAKE

In this section, we present a comprehensive audit of the error pattern that existed in MQuAKE-cf-3k and MQuAKE-t . We specifically note that our audit is there to provide a better understanding to the knowledge editing community, especially when digesting methods evaluated on these datasets. **Our audit is not to discredit the contribution of MQuAKE, or any of the proposed methods evaluated on MQuAKE.** We recognize the fact that no dataset can be perfect, especially when it is intrinsically hard to collect large-scale counterfactual and temporal datasets.

### Intra Contamination between Edited Cases and Unedited Cases

As discussed in SS2.2, having a gradual evaluation coverage from a few to all cases being edited like Table 5 makes sense for as an evaluation granularity. However, one critical issue is that \(k\{1,100,1000,3000\}\)-edited cases (supposed MQuAKE-cf-3k) are randomly sub-sampled from the 3,000 total cases. Thus, **there is no guarantee that the \(k\)-edited cases and \((3000-k)\) unedited cases would require two disjoint sets of knowledge and, therefore, risk contamination.**

For a concrete example, consider the following two multi-hop questions from MQuAKE-cf-3k (we also additionally provide the subquestion breakdown and intermediate answers of the two questions for better presentation, we note that such auxiliary information is not part of the instruction visible to the question-answering LLM):

* case_id:245 (unedited): _What is the official language of the country where Karl Alvarez holds citizenship?_
* What is the country of citizenship of Karl Alvarez? USA.
* What is the official language of United States of America? American English.
* case_id:323 (unedited): _What language is the official language of the country where Wendell Pierce holds citizenship?_
* What is the country of citizenship of Wendell Pierce? USA.
* What is the official language of United States of America? American English.

For both questions, the correct pre-edited answer should be _"American English."_ As both Karl Alvarez and Wendell Pierce are US citizens, and the official language of the US is American English. However, suppose case_id:323 is sampled as an edited case while case_id:245 remains unedited, we will be provided with the additional triple containing the knowledge of _"The official language of United States of America is Arabic."_

Since the unedited case_id:245 and the edited case_id:323 share the same subquestion of _"What is the official language of United States of America?"_ The answer of case_id:323 will be rightfully updated to _"Arabic"_ per the new knowledge. However, the unedited case_id:245 still considers theoriginal answer _"American English"_ to be correct, and is therefore contaminated by the edited case case_id:323 in an unintended fashion. This is problematic because a successful knowledge editing method should be able to retrieve the edited knowledge -- _"The official language of United States of America is Arabic"_ -- upon the relevant questions (in this case the shared one), and thus answering _"Arabic"_ to case_id:245. This is technically correct, but in conflict with MQuAKE-cf-3k's label, causing inaccurate experiment readings.

**We further note the above-illustrated contamination is not a cherry-picked fluke, but rather a wild-spread error.** Here, we sample \(\{1,100,1000,2000,3000\}\)-editing targets from MQuAKE-cf-3k using random seed 100, and find the following error statistics in Table 1.

It is observable from Table 1 that **even a small number of edited cases will cause a converingly large contamination to unedited cases and subquestions, where 67% and 76% of all cases from MQuAKE-cf-3k and MQuAKE-t are contaminated with just 100 cases being edited**, introducing a significant distortion to the reported experiment results.2

We additionally note while this edited-to-unedited intra-contamination is reducing with \(k\)-edit growing, this does not imply a diminishing of issue, but rather a simple by-product of a larger \(k\) implies a lesser \((3000-k)\), leaving fewer unedited cases as potential contamination victims. In the extreme case of 3000-edit, there is 0 edited-to-unedited contamination because there is no unedited case left in MQuAKE-cf-3k to be the victim. But 3000-edit has the most edited-to-edited inner contamination, more on this in the following SS3.2.

### Inner Contamination between Different Edited Cases

Other than edited cases contaminating unedited cases (SS3.1), contamination might also happen among multiple edited cases because a certain subquestion presented in different edited cases can be edited in some but unedited in others3. For brevity, we leave the example walkthrough in Appendix B.1.

This type of contamination is, once again, universally visible in MQuAKE, as shown in Table 2; which is very much a flipped version of Table 1. With \(k\)-edit growing, there are more edited cases, thus more edited-to-edited contamination, as there are more potential victims. Notably, **under the 3000-edit tasks, almost one-third (998/3000, \(\)33%) of the evaluated cases are contaminated**, which again introduces distortion to the reported experiment results. We omit the report on MQuAKE-t here because there is only one edit-to-edit contamination when all 1,868 cases from MQuAKE-t are edited (case_id:424).

    &  &  \\  & **1-edit** & **100-edit** & **1000-edit** & **2000-edit** & **3000-edit** & **1-edit** & **100-edit** & **500-edit** & **1868-edit** \\  Cases & 0 & 2,013 & 1,772 & 910 & 0 & 29 & 1421 & 1327 & 0 \\ Subquestions & 0 & 2,706 & 3,075 & 1,664 & 0 & 29 & 1421 & 1327 & 0 \\   

Table 1: Error statistics of MQuAKE-cf-3k and MQuAKE-t [Zhong et al., 2023] in terms edited cases contaminating unedited cases. \(k\)-edited means \(k\) cases out of the total dataset are edited.

    &  &  \\  & **1-edit** & **100-edit** & **1000-edit** & **2000-edit** & **3000-edit** & **1-edit** & **100-edit** & **500-edit** & **1868-edit** \\  Cases & 0 & 2,013 & 1,772 & 910 & 0 & 29 & 1421 & 1327 & 0 \\ Subquestions & 0 & 2,706 & 3,075 & 1,664 & 0 & 29 & 1421 & 1327 & 0 \\   

Table 2: Error statistics of MQuAKE-cf-3k [Zhong et al., 2023] in terms edited cases contaminating each others. \(k\)-edited means \(k\) cases out of the total 3,000 cases are edited.

### Conflicting Edits

The two types of contamination introduced in SS3.1 and SS3.2 are indeed subtle and hard to detect, as they hide between the retrieval scope of different edited cases, which is further complicated when only a subset of cases are edited. However, MQuAKE-cf-3k also includes some straightforward conflicts, such as for the subquestion _"Which company is Ford Mustang produced by?"_ we have the following edits:

* case_id:2566 (edited): Ford-Moter-Company-Nintendo.
* case_id:231/2707 (edited): Ford-Moter-Company-FiatS.p.A.

This is going to cause a direct conflict when case_id:2566 and any of the case_id:231/2707 are both selected as edited cases, as they shall confuse any knowledge edited LLM for having two answers to the same questions. Fortunately, such types of errors are rather minuscule in MQuAKE-cf-3k, with the abovementioned Ford Mustang question and three cases being the only affected data samples.

### Missing Information in Multi-hop Question Instructions

As mentioned in SS2, the MQuAKE dataset is built upon a severely filtered Wikidata:RDF knowledge graph (Vrandecic and Krotzsch, 2014). Specifically, the triples of a certain \(\{2,3,4\}\)-hop walk on this subgraph are then fed into a gpt-3.5-turbo model to generate the multi-hop question instruction in a natural language format; such generation are repeated for three different times in case any of the generated question instructions becomes incomprehensible. For every case evaluation, an LLM is considered right should it correctly answer against any three of the multi-hop question instructions (Zhong et al., 2023).

However, while repeating generation three times definitely reduces the chances of having incomprehensible question instructions, we noticed some of such instructions in MQuAKE are still incomplete. We take the following triple set and its generated 3-questions as an example:

* case_id:546 (unedited): We have a 2-hop triple chain of (Albert Mohler, employer, Southern Baptist Theological Seminary) and (Southern Baptist Theological Seminary, religion or worldview, Southern Baptist Convention). MQuAKE-cf-3k provides the following generated multi-hop questions:
* Generation #1: _What religion is Albert Mohler associated with?_
* Generation #2: _Which religion does Albert Mohler follow?_
* Generation #3: _With which religious faith does Albert Mohler identify?_

It is clear that all three generated questions omit the part mentioning which company/institution Albert Mohler is employed by and essentially reduce themselves to single-hop questions, where a correct generation should read like _"What religion is Albert Mohler's employer associated with?"_ Without the complete question, suppose there is an edit on Albert Mohler's employer (which there indeed is one), the final answer would likely change. However, with question instruction omitting such information, even the best knowledge-edited LLM cannot answer the question correctly with a faithful approach.

As a general analysis, we find **the natural language question instructions of 672 cases in MQuAKE-cf-3k are missing information in comparison to their raw triplet chain.** This number is counted in the sense that one or more pieces of information present in the triple chain are missing from all three variants of the generated natural language instruction. Similarly, there are 2,830 and 233 cases of erroneous instructions in MQuAKE-cf and MQuAKE-t, respectively.

### Duplicated Cases

The last kind of error we discovered in MQuAKE is simply unintended duplication -- i.e., two or more cases sharing the same start subjects, edited facts, chain of triples, and final answer. We discovered 47, 4, and 4 cases of duplication, respectively, in MQuAKE-cf, MQuAKE-cf-3k, and MQuAKE-t.

## 4 Remastering MQuAKE

In this section, we illustrate how we modified and improved the MQuAKE dataset to MQuAKE-Remastered with various fixes on the data samples themselves, as well as providing utility modules to facilitate how one interacts with such datasets.

### Hard Corrections

Three types of error existing in MQuAKE can be fixed once and for all with some careful hard corrections, they are namely Conflicting Edits (SS3.3), Missing Information in Multi-hop Question Instructions (SS3.4), and Duplicated Cases (SS3.5). For Conflicting Edits and Duplicated Cases, since there are only a few such errors (<50 per type per dataset), we employ some manual corrections to address these errors: in the former case, we flip the minority edits to align with the majority edits (and adjust their answers to their subsequence subquestions, should there be any); in the latter case, we simply remove such duplicated cases (except for MQuAKE-cf-3k, which we manually select 4 more cases from MQuAKE-cf to keep the dataset having 3,000 cases in total and a 1,000 cases for \(\{2,3,4\}\)-hops). For the Missing Information in Multi-hop Question Instructions errors, we rewrite such natural language question instructions and then replace the original information-missing instructions.

Dynamic Masking for Maximum Coverage: MQuAKE-Remastered-cf, MQuAKE-Remastered-cf-3k, and MQuAKE-Remastered-t

Due to the contamination count of Intra Edited-to-Unedited Contamination (SS3.1) and Inner Edited-to-Edited Contamination (SS3.2) tend to grow in the opposite direction as shown in Table 1 and 2, it is impossible to find a fix within the current MQuAKE that can address both issues without significantly decreasing the dataset size. As an alternative, we develop an API that will take a case_id and an edited_flag as input, respectfully indicating the evaluating case-in-question and whether this case is considered edited; our API shall then return a set of triples that are contamination free by dynamically masking out the conflicting edits from other cases. After such, the user may build up an editing knowledge bank upon such triplets and conduct evaluations for any memory-based knowledge editing methods without losing any of the 9,218 cases from MQuAKE-cf or 1,868 cases from MQuAKE-t.

Specifically, once case_id-of-interest is given, our API would loop through all of its subquestions and identify if any of such subquestions is considered edited under another case. If there is a hit, the triple with respect to such edited subquestions is then removed from the bank of edited triples. This dynamic masking mechanism would ensure all cases within the original MQuAKE be usable against memory-based knowledge editing methods. **However, the drawback of masking is it won't support parameter-based knowledge editing methods**, where weight update is required. We additionally provide a MQuAKE-Remastered-cf-6334 to address the need for such methods (Appendix C.1).

## 5 Benchmark and Discussion

Given almost all proposed multi-hop knowledge editing methods are evaluated on the original, error-contained, MQuAKE datasets. Here, we provide a re-benchmark of those methods against post-fix MQuAKE-Remastered datasets for a more reliable reporting of each method's performance.

### Experiment Coverage

Compared MethodsIn this work, **we aim to cover most, if not all, open-sourced knowledge editing methods evaluated on the original MQuAKE.** To the best of our knowledge, this screening criteria include MeLLo (Zhong et al., 2023) and PokeMQA (Gu et al., 2024) as methods specifically proposed to target this multi-hop knowledge editing problem and evaluated on MQuAKE. We additionally include ICE (Cohen et al., 2023) and IKE (Zheng et al., 2023a) as these are also methods purposed for the (single-edit) multi-hop knowledge editing task, though not specifically evaluatedon MQuAKE in their original publications. We note that we are aware methods like GMeLLo (Anonymous, 2024), GLAME (Mengqi et al., 2024), RAE (Shi et al., 2024), StableKE (Wei et al., 2024), and Temple-MQA (Cheng et al., 2024) are also evaluated on MQuAKE, but they are purposely omitted from our re-benchmark coverage due to lack of open-sourced implementation, likely because most of these works are still in submission. Last, we note DeepEdit (Wang et al., 2024) is also an open-sourced MQuAKE-evaluated method, but we excluded it due to its lack of inference optimization (>200 A100 GPU hours needed for 1-edit on MQuAKE-Remastered-cf-3k).

Covered ModelsWe opt to use Imsys/vicuna-7b-v1.5 (Zheng et al., 2023b), mistralai/Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), and meta-llama/Meta-Llama-3-8B-Instruct (AI@Meta, 2024) as the choice of question-answering models, both for alignment with existing works (Zhong et al., 2023; Shi et al., 2024; Gu et al., 2024) as well as providing coverage the most recent language models. For methods that require a text-embedding model as a retriever, we use facebook/contriever-msmarco (Izacard et al., 2022) for alignment with MeLLo (Zhong et al., 2023).

Covered DatasetsWe will provide coverage on our post-fix dataset, namely MQuAKE-Remastered-cf, MQuAKE-Remastered-cf-3k, and MQuAKE-Remastered-t in the masking fashion illustrated in SS4.2; as well as MQuAKE-Remastered-cf-6334 in its vanilla form. These datasets are respectively corresponding to the original MQuAKE-cf, MQuAKE-cf-3k, and MQuAKE-t from Zhong et al. (2023) (with 6334 as an extra for parameter-based methods), but with the types of error mentioned in SS3 fixed in the via means illustrated in SS4. We emphasize that such modification is legitimate, and our MQuAKE-Remastered is free for the scholarly community to adopt, as the original MQuAKE dataset was published under the MIT license. Where MQuAKE-Remastered will be released under CC BY 4.0. All experiments are conducted with an 80G NVIDIA A100 from a DGX A100 server.

Given our MQuAKE-Remastered are mostly provided as a fix to MQuAKE, we would like to first highlight the drastic results difference when the same method is evaluated on these two datasets. Table 3 shows our fixing can indeed result in drastically different experiment reports. Where such difference is especially significant for stronger methods, suggesting all previous reporting on MQuAKE has room for reliability improvements, which we filled here with MQuAKE-Remastered.

    &  &  \\  & **Original** & **Remastered** & **Original** & **Remastered** \\  MeLLo (Zhong et al., 2023) & 6.7 & **6.77** & 30.84 & **44.37** \\ GWalk & 36.23 & **66.33** & 46.41 & **54.88** \\   

Table 4: Experiments on MQuAKE-Remastered-cf with numbers of edited cases and methods. Results inside ( ) are edited cases accuracy and unedited cases accuracy, respectively.

    &  &  \\  & **Original** & **Remastered** & **Original** & **Remastered** \\  MeLLo (Zhong et al., 2023) & 6.7 & **6.77** & 30.84 & **44.37** \\ GWalk & 36.23 & **66.33** & 46.41 & **54.88** \\   

Table 3: Performance Comparison of Original MQuAKE and our MQuAKE-Remastered datasetsDue to page limitation, we only present the benchmark results on MQuAKE-Remastered-cf in the main text and refer our readers to Appendix D.2 for benchmarks of MQuAKE-Remastered-cf-3k, MQuAKE-Remastered-t, and MQuAKE-Remastered-cf-6334. Given the dominance of GWalk -- a demo method we proposed as guidance to future scholars of this MHKE task -- we leave more discussion on this method below.

### Making Faithful Approach to MQuAKE and MQuAKE-Remastered

Additionally, it is also our observation that many multi-hop knowledge editing methods with decent accuracy reports on MQuAKE or MQuAKE-Remastered are utilizing designs that leverage specific data properties unique to MQuAKE. For example, methods like GLAME (Mengqi et al., 2024) utilize Wikidata (Vrandecic and Krotzsch, 2014) as the external knowledge graph to better detect the edit-induced conflicts, which happen to be the source of MQuAKE as discussed in SS2.1. While these methods might have decent performance on MQuAKE, the cost of maintaining a positive knowledge graph on the correct -- but not just edited -- knowledge facts is undoubtedly a non-trivial operation cost. Yet, whether sourcing the same Wikidata knowledge graph as MQuAKE might bring them data-specific advantages remains unanswered. Similarly, PokeMQA (Gu et al., 2024) utilizes the 6,218 cases included in MQuAKE-cf but not in MQuAKE-cf-3k as the train set to train its auxiliary components. Given MQuAKE is a dataset with relatively low diversity (e.g., it only includes 37 types of relations), whether having a heavily overlapped train and test set will result in data-specific advantages unique MQuAKE and its variants, again remains unanswered.

A Minimally Invasive but Performant Approach: GWalkHere, we provide a brief walkthrough of a simple method we designed, namely GraphWalk. It does not leverage any data-specific property unique to MQuAKE or MQuAKE-Remastered, yet still presents pleasant performance surpassing many established baselines. We illustrate this method as a simple guidance and potential inspiration to our future multi-hop knowledge editing scholars. **Due to page limitation, we introduce the technical details and design intuition of GWalks in Appendix D.1**, and only present the performance of GWalks in the main text.

We hope the performant nature of GWalk -- in its most vanilla form, without employing any data-specific property unique to MQuAKE or MQuAKE-Remastered -- can inspire more multi-hop knowledge editing methods that leverage the graph topology of edited facts, without converting such facts to natural language descriptions (at least for retrieval).

## 6 Related Works

Our work mainly conducts an audit and provides fixes to the MQuAKE dataset. To the best of our knowledge, only two prior arts have touched on the errors existing in MQuAKE: GMeLLo (Anonymous, 2024) (an anonymous submission to ACL ARR 2024 February) and DeepEdit (Wang et al., 2024). As an overview, GMeLLo briefly discussed the same type of error we discussed in SS3.4 without providing any quantitative error analysis or any fix. DeepEdit discovered the same inner contamination error as we discussed in SS3.2, but specific to _3000-edit_ setup. DeepEdit's proposed fix is simply removing the 998 inner contaminated cases from the MQuAKE-cf-3k dataset, so this fix is custom 3000-edit and done so by sacrificing 1/3 of the dataset capacity. We leave more details in Appendix E due to page limitation.

Additionally, our work provides a re-benchmark of most, if not all, open-sourced knowledge editing methods evaluated on MQuAKE, and sets guidance on how to faithfully approach such datasets. To the best of our knowledge, no other work provides the same benchmark nor touches on the same issue.

## 7 Conclusion

Our work provides a comprehensive audit and fix of the MQuAKE dataset. We further re-benchmarked all open-sourced knowledge editing methods evaluated on MQuAKE with our MQuAKE-Remastered datasets and provided guidance and examples on how to faithfully approach these datasets with our GWalk.

## Limitations and Impact Statement

While our work comprehensively addressed many errors in MQuAKE, we caution our reader to perform further analysis and evaluation on our MQuAKE-Remastered to ensure our fixes are indeed exhaustive. We also note that multi-hop knowledge editing only represents one aspect of a language model's ability, so any actual deployment of a language model should undergo more, and if possible, deployment-specific evaluations.