# MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map

Yuhong Chou\({}^{1,2}\); Man Yao\({}^{2*}\), Kexin Wang\({}^{2}\), Yuqi Pan\({}^{2}\), Ruijie Zhu\({}^{3}\),

**Yiran Zhong\({}^{4}\), Yu Qiao\({}^{4}\), Jibin Wu\({}^{1}\), Bo Xu\({}^{2}\), Guoqi Li\({}^{2}\)\({}^{}\)**

\({}^{1}\)The Hong Kong Polytechnic University

\({}^{2}\)Institute of Automation, Chinese Academy of Sciences

\({}^{3}\)UC Santa Cruz

\({}^{4}\)Shanghai AI Lab

Equal contribution. yuhong.chou@connect.polyu.hk; man.yao@ia.ac.cnCorresponding author, guoqi.li@ia.ac.cn

###### Abstract

Various linear complexity models, such as Linear Transformer (LinFormer), State Space Model (SSM), and Linear RNN (LinRNN), have been proposed to replace the conventional softmax attention in Transformer structures. However, the optimal design of these linear models is still an open question. In this work, we attempt to answer this question by finding the best linear approximation to softmax attention from a theoretical perspective. We start by unifying existing linear complexity models as the linear attention form and then identify three conditions for the optimal linear attention design: i) Dynamic memory ability; ii) Static approximation ability; iii) Least parameter approximation. We find that none of the current linear models meet all three conditions, resulting in suboptimal performance. Instead, we propose Meta Linear Attention (MetaLA) as a solution that satisfies these conditions. Our experiments on Multi-Query Associative Recall (MQAR) task, language modeling, image classification, and Long-Range Arena (LRA) benchmark demonstrate that MetaLA is more effective than the existing linear models. Code: https://github.com/BICLab/MetaLA

## 1 Introduction

Transformer with softmax attention  benefits from efficient parallel training and exhibits impressive performance on deep learning applications [2; 3; 4; 5; 6; 7], but it suffers from the quadratic growth of computation cost to the input length . Linear recurrent models, such as LinFormer , SSM , and LinRNN , are expected to achieve linear substitution of Transformer. The original intention of LinFormer is to replace softmax attention, which exploits the kernel approach to decompose softmax operation; typical work includes TransNormer [12; 13], RetNet , GLA . On the other hand, SSMs, such as S4  and Mamba , are models inspired by the classical state-space approach, which enjoys sub-quadratic training and inference like either a recurrence or convolution. In contrast, LinRNN is a revival of traditional RNNs, including RWKV-4 , Griffin , LRU , etc., which solves the training difficulties of traditional RNNs due to nonlinear dependencies between hidden states. It is natural to think that they are different types of models, since these LinFormer/SSM/LinRNN models have different origins and forms.

This work breaks this perception and abstracts existing LinFormer/SSM/LinRNN models into a unified linear attention form, which has the following significance: i) Facilitates understandingthe key designs of existing linear models. Through the unified form, we demonstrate that the main difference between LinFormer/SSM/LinRNN is the hidden state size, how to maintain the hidden state, and how to perform parameter mapping. ii) Links LinFormer/SSM/LinRNN to softmax attention in terms of functionality. The recurrent inference complexity of softmax attention is \((n)\), which can also be regarded as the maintenance of a hidden state with infinite size. Linear models with \((1)\) inference complexity are hoping to achieve the same functionality as softmax attention using a fixed hidden state. Since we have unified LinFormer/SSM/LinRNN into linear attention in the form of Query, Key, and Value, we can understand and evaluate existing linear models from the view of "Does the linear attention map have the function of softmax attention map?".

To answer this question, we define the necessary conditions for achieving "optimal linear approximation to softmax attention map". First, linear attention needs to satisfy _dynamic memory_ and _static approximation_ to realize the approximation. The former defines memory ability: linear attention with limited hidden states should be able to store the most important information and forget unimportant ones. The latter defines the modeling ability: a linear attention map should be able to approximate any softmax attention map. According to our theoretical analysis, Query and dynamic decay are necessary conditions for approximation. Thus, linear models such as TransNormer , RetNet , RWKV-4 , LRU , HGRN , H3 , S5 , cannot achieve approximation of the softmax attention functions. Second, the Key matrix is not required to achieve approximation, so Mamba  and GLA  are not optimal parametric approximations.

We then propose the MetaLA module, which can satisfy the necessary conditions for optimal linear approximation to softmax attention. MetaLA makes three enhancements: i) Removes the unnecessary Key matrices; ii) Employs self-augmentation to enhance the token's attention to itself, which avoids attention dilution ; iii) Exploits short convolutions to enhance local interactions. We then build a MetaLA Transformer based on MetaLA. Our experiments on associative recall, language modeling, long sequence modeling, and image classification show the effectiveness of MetaLA. Furthermore, we conduct ablation studies to validate the effectiveness of each proposed enhancement in MetaLA. Finally, we discuss two open questions: i) How to further improve linear attention based on the approximation theory introduced in this work? ii) Does the approximation of linear attention to softmax attention imply that it has an upper limit on its capacity?

## 2 Background

For notations in this work, we use bold upper-case letters for matrices (e.g., \(\), \(\)), bold lower-case letters for row vectors (e.g., \(_{t}\), \(_{t}\)), and italic upper-case for learnable parameter matrices (e.g.,\(_{Q}\)). We generally use the same alphabet to show the rows of a matrix, e.g., \(_{t}\) is the \(t\)-th row of \(\).

**Softmax Attention** first calculates an attention map \((,)\) through \(\) (Query), \(\) (Key), and use the attention map to weight different tokens \(\) (Value) later:

\[ =(,)= (^{}}{}} )^{n d_{v}},\] (1) \[, =_{Q},_{K} ^{n d_{h}};=_{V} ^{n d_{v}},\] (2)

where \(_{Q},_{K}^{d d_{k}}, _{V}^{d d_{v}}\) are learnable matrices, \(n,d,d_{k},d_{v}\) are sequence length, model dimension, Key/Query and Value dimension, respectively. \(^{n d}\) refers to the input. \(^{n n}\) is a mask matrix in autoregressive tasks to prevent a token from attending to future tokens. The \(t\)-th row of \((,)\) is a probability distribution that represents the attention scores between token \(_{t}\) to others. Softmax attention in Eq. (1) enables efficient parallel training, but suffers from \((n^{2})\) time and memory complexity . It uses the recurrent form during inference:

\[_{t} =^{t}(_{t}_{s}^{}) _{s}}{_{s=1}^{t}(_{t}_{s}^{})} ^{1 d_{v}},\] (3) \[_{t},_{t} =_{t}_{Q},_{t}_{K} ^{1 d_{k}};_{t}=_{t} _{V}^{1 d_{v}}.\] (4)

At each time \(t\), token mix is computed between query \(_{t}\) and all the keys, values before \(_{s},_{s}(s t)\). This "KV cache" results in \((n)\) time and memory complexity per token during inference.

**Linear Transformer (LinFormer)** is a substitute for softmax attention, which can be expressed as a linear dot-product of kernel feature maps :

\[_{t}=^{t}F(_{t},_{s})_{s} }{_{t=1}^{t}F(_{t},_{s})}, F(_{t}, _{s})=(_{t})^{}(_{s}),\] (5)

where \(_{t},_{t}^{1 d_{k}}\) and \(_{t}^{1 d_{v}}\) are query, key and value at position \(t\), which are obtained in the same manner as softmax attention. \(F()\) is the kernel function usually constrained to be non-negative. \(()\) is map function applied row-wise to \(\) and \(\). By removing the nonlinear softmax operation, LinFormer enables inference with \((1)\) time and memory complexity per token. LinFormer can also be formulated in the following parallel form during training

\[=(()^{}() )^{n d_{v}},\] (6)

which has \((n)\) time and memory complexity using chunkwise algorithm. Recent advances in LinFormer mainly focus on training acceleration[12; 13; 23] or improving performance.

**State-Space Model (SSM)** come from continuous-time system which maps a 1D function \(x(t)\) to another function \(y(t)\) via a hidden state \((t)^{N}\). In SSM, the continuous parameters can be discretized using a step size \(\) and get discrete parameters \(},},}\). The resulting discrete-time system is used to model sequences \(,^{1 n}\) with elements \(x_{t},y_{t}\) via the recurrent form:

\[_{t}=}_{t-1}+}x_{t}, y _{t}=}_{t},\] (7)

in autoregressive inference with \((1)\) time and memory complexity per token. The linear time-invariant SSM above can be unrolled and computed using the long convolution with kernel \(\)

\[:=(}},}}},,}}^{n-1}})^{1 n},=*,\] (8)

where \(*\) represents casual convolution operation , which enables parallelizable training utilizing Fast Fourier Transforms, resulting in \((n n)\) time and \((n)\) memory complexity during training. When handling vector sequences \(,^{d n}\), SSMs are applied individually on the \(d\) channels. Typical SSMs (S4D, DSS, H3, S5) employ data-independent structured transition matrix \(}\) or special initialization strategies HiPPO to efficiently enhance long-range dependencies. Mamba  advances SSMs by introducing data-dependent parameters and designs a hardware-aware parallel algorithm, further improving prior \((n n)\) into \((n)\) time complexity during training.

**Linear RNNs (LinRNN)** Traditional RNNs suffer from slow sequential training, limited capability in modeling long-term dependencies, and difficulty in scaling. To address these issues, LinRNNs eliminate the nonlinearity within the recurrence and employ element-wise product instead of matrix multiplication [11; 28]. Typical LinRNN such as Gated Linear Recurrent Unit (GLRU) [20; 29] is

\[_{t},_{t},_{t} =(_{t}_{f}+_{f}),(_{ t}_{i}+_{i}),(_{t}_{c}+_{c}) ^{1 d},\] (9) \[_{t} =_{t}_{t-1}+_{t}_{t},^{1 d},\] (10)

where \(_{t},_{t}\) denote input and output, \(_{t},_{t}\) are forget and input gates as in traditional RNNs, \(\) is element-wise multiplication. Linear RNNs have \((1)\) time and memory complexity per token during inference. Since Eq. (10) removes nonlinearity, it enables parallelized training using parallel scan, with only \((n)\) time and memory complexity. Recent works have made effort to explore effective recurrence (LRU , RWKV ) or gating mechanisms (HGRN , Griffin ).

## 3 General Form of LinFormer/SSM/LinRNN Mechanisms

Observing Eq. (3), Eq. (5), Eq. (7), and Eq. (10), we find that their recurrent forms during inference can all be understood from the view of maintaining hidden states. Softmax attention maintains an unlimited hidden state (KV cache). By contrast, LinFormer/SSM/LinRNN have limited hidden states: linear attention with \(^{}(_{t})_{t}^{d_{k} d_{v}}\), SSM with \(_{t}^{N d}\), linear RNNs with \(_{t}^{1 d}\), where \(d_{k}>N>1\) in usual. Inspired by this fact, we unify LinFormer/SSM/LinRNN mechanisms in the form of linear attention, formally containing Query, Key, and Value matrices (see Fig. 1).

**General Recurrent Form of LinFormer/SSM/LinRNN** is:

\[_{t} =_{q}(_{t},_{q}),_{t}= _{k}(_{t},_{k}),_{t}=_{}(_{t}, _{})^{1 d_{k}},\] (11) \[_{t} =_{v}(_{t},_{v}),_{t}= _{g}(_{t},_{g})^{1 d_{v}},\] (12) \[_{t}^{h} =(_{t}^{h})_{t-1}^{h}+(_{t }^{h})^{}_{t}^{h}^{d_{k}^{} d_{v} ^{}},\] (13) \[_{t} =([_{t}^{1}_{t}^ {1},_{t}^{2}_{t}^{2},,_{t}^{H}_ {t}^{H}])^{1 d_{v}},\] (14) \[_{t} =(_{t}_{t})_{O} ^{1 d},\] (15)

where \(_{t}^{1 d}\) is the \(t\)-th input. \(_{t},_{t},_{t},_{t},_{t},_{t}\) are query, key, value, decay, output gate, hidden state respectively. \(_{q/k/}\) are functions that map \(_{t}\) from \(^{1 d}\) to \(^{1 d_{k}}\), \(_{q/k/}\) are the corresponding parameters to be trained. Similarly, \(_{v/g}\) map \(_{t}\) from \(^{1 d}\) to \(^{1 d_{v}}\) and \(_{v/g}\) are trainable parameters. In Eq. (13), \(_{t},_{t},_{t},_{t}\) are divided into \(H\) partitions (heads), where \(d_{k/v}^{}=}{H}\), and \(h=1,,H\) is the index of heads. Each head maintains a hidden state \(_{t}^{h}\). The diagonal matrix \((_{t}^{h})\) denotes the decay of past state. \(_{t}^{h}\) represents the acceptance for the input token \(_{t}^{h}\). The hidden states are 2D matrix once \(d_{v}^{} 1\). In Eq. (14), to turn back to 1D shape, the \(_{t}^{h}\) operation is necessary as a dot-product with \(_{t}^{h}\), then the concat and normalization operations are followed. \(\) denotes any kinds of normalization. In Eq. (15), a gate machian is optional for \(_{t}\) while the dimension should be projected back to \(d\) from \(d_{v}\) through \(_{O}^{d_{v} d}\).

From a functional view, Eq. (13) represents the update process of the hidden state, which contains historical information on keys and values. Eq. (14) represents query and weighted sum operations to derive the attention output. Eq. (15) represents gate and projection operations to get the final output.

**General Parallel Form of LinFormer/SSM/LinRNN** can be written as follow:

\[=(,) = }{}^{} ,\] (16) \[(//)_{t,:}=(// )_{t},_{t,:}=_{j=1}^{t}_{j},_{i,j}=1,i j.\\ 0,i>j.\] (17)

\(}{}\) denotes element-wise division, \(()_{t,:}\) is the \(t\)-th row of \(\), and \((,)\) is the attention map. Each element in the attention map matrix is as follows (heads are omitted for simplicity):

\[(,)_{t,s}= _{t}_{j=s+1}^{t}_{j} _{s}^{},s t.\\ 0,s>t.\] (18)

As shown in Tab. 1, the main differences between various linear models are parameter functions \(_{q/k/v//g}\) and dimension settings \(d_{k},d_{v},H\). We give details in appendix A1 on how to derive LinFormers/SSMs/linRNNs from our unified form, which is termed as "Linear Attention (LinAtt)".

LinFormer/SSM/LinRNN models have different origins, so they have different **optimization perspectives** and various **hidden state sizes**: i) _LinFormer_ originate from approximation of vanilla softmax attention. They focus on designing better kernel function \(\), i.e., to optimize \(_{q},_{k}\); They have

Figure 1: **General Form of LinFormer/SSM/LinRNN Mechanisms.** The general form equips with two modes of parallel and recurrent computation which enjoys both training and inference efficiency.

relatively large matrix hidden state whose size (\(d_{v}d_{k}/H\)) is mainly correlated to model dimension \(d\). ii) _SSM_ originate from state space equations. So they focus on how to better maintain the hidden state and optimize \(_{}\); They have a matrix hidden state of moderate size (\(d_{v}N\)), which is correlated to the fixed expansion \(N\). iii) _LinRNN_ originate from removing nonlinearity in the recurrence of vanilla RNN. So they focus on designing better forget/input/output gates, i.e., to optimize \(_{},_{k},_{g}\); They have 1D vector hidden state whose size (\(d_{v}\)) is relatively small. Despite these differences, they all try to design better parameter functions \(_{q/k/v//g}\) and maintain a limited hidden state \(_{t}\).

## 4 Optimal Linear Approximation to the Softmax Attention Map

We here discuss the optimal approximation of LinAttMap to SoftAttMap based on its general form. The function of softmax attention is two-fold: i) _Memorizing information_, all the current and historical information can be stored in KV cache; ii) _Modeling relationships,_ softmax attention can calculate arbitrary attention scores of stored information. Unfortunately, such a powerfully expressive attention map generated by softmax attention requires infinite hidden states. By contrast, linear attention expects to exploit limited hidden states to achieve the same functionality as softmax attention.

Some existing linear models, such as Performer, RFA, etc., optimize the model with the goal of approximating the value of SoftAttMap. In contrast, this work investigates the functional approximation of SoftAttMap, which is the basis for value approximation. Specifically, we here attempt to answer two key questions: i) Can linear attention realize the function of softmax attention? ii) If it can be achieved, what kind of linear attention approximation is better? To achieve this goal, we first give the definition of necessary conditions of optimal linear approximation. Then we categorize the existing linear models based on the conditions of the optimal linear approximation.

**Definition 4.1**.: **Necessary Conditions of Optimal Linear Approximation to Softmax Attention Map.** A function \(f(_{t},_{s}|):^{1 d}^{1 d}\), used to compute attention score between any \(_{t}\) and \(_{s}\) (tokens), with parameters \(\), is an optimal linear approximation to softmax attention map if it satisfies: i) **Linear complexity**. Attention map can be computed in linear time, i.e., \((n)\) space and time complexity during training and \((1)\) space and time complexity during inference. ii) **Dynamic memory ability**. When handling inputs sequentially, \(f(_{t},_{s}|)\) with limited hidden states should be able to store the most important information adaptively while forgetting unimportant ones. iii) **Static approximation ability**: For an arbitrarily given softmax attention map \(\) with scores \(p_{ts}\), there must exists bounded \(\) such that \(f(_{t},_{s}|)=p_{ts}, t,s=1,,n\). iv) **Least parameter approximation**: On the premise that the first three conditions are met, use as few parameters as possible to achieve approximation to softmax attention map.

In definition 4.1, Condition 0 (C0) underlines computational and memory efficiency. Conditions 1 (C1) and 2 (C2) consider _memory_ and _modeling_ ability of linear attention. Due to limited state size \(d\), linear attention can only memorize the history of most important \(d\) tokens without information loss and precisely model arbitrary attention map of those \(d\) tokens. Condition 3 (C3) is our expectation to seek the least parameters on the premise that previous three conditions are met.

**Theoretical Analysis for Optimal Linear Approximation.** For the C1 condition, suppose the information about \(_{t_{1}},,_{t_{d_{k}}}\) is successfully stored in \(_{t}\) (\(t_{1},,t_{d_{k}} t\)), we will check whether the model can substitute unimportant \(_{t_{1}}\) when the new important input \(_{t+1}\) arrives.

For the C2 condition, Eq. (18) illustrates the LinAttMap only relate to \(_{t}=_{q}(_{t},_{q}),_{t}= _{k}(_{t},_{k})\), \(_{t}=_{}(_{t},_{})\). Denote decay \(_{t}=(_{t})\). Assuming the inputs are good enough

    &  &  &  \\   & GLA & TrNorm & GLRU & RWKV-4 & Mamba & S5 \\  \(_{q}(_{t},_{q})\) & \(_{t}_{Q}\) & \((_{t}_{Q})\) & \(\) & \(\) & \(_{t}_{C}\) & \(\) \\ \(_{k}(_{t},_{k})\) & \(_{t}_{K}\) & \((_{t}_{K})\) & \((_{t}_{K})^{*}\) & \(_{t}_{K})}\) & \(_{t}(_{t}_{b})\) & \(\) \\ \(_{v}(_{t},_{v})\) & \(_{t}_{V}\) & \(_{t}_{V}\) & \((_{t}_{C})^{*}\) & \(_{t}_{V}\) & \(_{t}\) & \(_{t}}\) \\ \(_{}(_{t},_{})\) & \((_{t}_{}^{}_{}^{ })^{*}\) & \(\) & \((_{t}_{f})^{*}\) & \()}\) & \()}\) & \()}\) \\ \(_{g}(_{t},_{g})\) & \((_{t}_{r})^{*}\) & \(_{t}_{U}\) & \((_{t}_{g})^{*}\) & \((_{t}_{r})\) & \(\) & \(\) & \(\) \\   & =d/2\)} & =d_{v}=d\)} & =d_{v}=d=d=H\)} & =d=H\)} \\  & & \(d_{v}=d\) & & & \(d_{k}^{}=N\) & \(d_{k}^{}=1\) \\   

Table 1: **From our general form to existing linear models (\(*\) indicates the bias term is omitted).**and the functions \((_{q},_{k},_{})\) are expressive enough, we can shift from solving \((_{q},_{k},_{})\) to solving \((,,_{t})\). We focus on approximating the attention scores between stored tokens, and the problem is simplified via: i) setting query dimension \(d_{k}=1\); ii) considering only a given time \(t\) and its attention distribution \(_{t}=[p_{ts},s=1,,t]^{1 t}\). Then, C2 is proved by the following equations holding with bounded parameters, as a foundation conclusion:

\[f(_{t},_{s}|,,_{t})=q _{t}_{j=s+1}^{t}_{j}k_{s}=p_{ts}, s=1,,t,\] (19)

\[|q_{s}| C_{q},|k_{s}| C_{k},_{s},  s=1,,t.\] (20)

For bounded inputs \(\), bounded parameters \((_{q},_{k},_{})\) are equivalent to bounded \((,,_{t})\). Afterwards we will generalize to vector version with \(d_{k}>1\) and consider distribution of all time (\(_{t},t=1,,d_{k}\)). This is done by viewing \(_{t}\) as a channel selector.

For the C3 condition, least parameters mean the fewest parameter groups \((,,_{t})\) when \(d,d_{k},d_{v}\) are fixed. Due to space constraints, the detailed analysis in this Section is provided in appendix A2.

**Conclusions of Optimal Linear Approximation Analysis.** i) _Linear approximation._ The necessary conditions (C1 and C2) for LinAttMap to achieve approximation to SoftAttMap is that its implementation must include \(\) and dynamic decay \(_{t}\). Both \((,,_{t})\) and \((,_{t})\) can achieve approximation. ii) _Least parameter approximation._\((,_{t})\) has fewer parameters (i.e., \(\) is not necessary), if the model dimensions are fixed. iii) _Function of dynamic decay._\(_{t}\) is the key to achieve dynamic memory. iv) _Function of Query._\(\) can be seen as a channel selector which selects several channels of Hadamard product of \(_{t}\) and \(\) to approximate attention map.

In Tab. 2, we review some existing linear models and judge whether they meet the necessary conditions for optimal approximation. Linear attentions can be classified into three types based on the parameter groups: i) Using \((,,_{t})\) all together, ii) Exploiting \((,)\), \((,_{t})\) or \((,_{t})\), iii) Employing only one of \(\), \(\), \(_{t}\). Considering decay can be either dynamic or fixed, here we use subscript \(t\) to distinguish, i.e., \(/_{t}\) denote fixed/dynamic decay. According to definition 4.1, they have different degrees of deficiencies: i) Models without dynamic decay such as RetNet, TransNormer, RFA, cannot memorize dynamically; ii) LinRNNs such as RWKV-4, HGRN, Griffin

    & & C0 & C1 & C2 & C3 & Models \\   & & \(\) & \(\) & \(\) & - &  \\  &  & \(,,\) & \(\) & \(\) & \(\) & - &  \\   & & & & & & & RetNet , TransNormer , & \\  & & & & & & & S4D , H3 , DSS  \\   & & & & & & & & GLA , Mamba  \\   & & & & & & & & linear Transformer , RFA , \\  & & & & & & & & Performer , cosFormer  \\   & & & & & & & & RWKV-4  \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   & & & & & & & & \\   

Table 2: **A review of existing linear models.** According to definition 4.1, existing linear models all have some deficiencies: i) Models without dynamic decay \(_{t}\) have no ability to memorize dynamically (not satisfying C1); ii) LinRNNs lack the selection ability brought by \(\) (not satisfying C2), and the approximation ability is poor owing to the small hidden state; iii) Models with \(\) have redundant parameters (not satisfying C3), which probably leads to higher learning difficulty.

lack the selection ability brought by \(\) and the approximation ability is poor due to the small hidden state; iii) Models with \(\) such as Mamba, GLA have redundant parameters, which probably leads to higher learning difficulty. Thus, none of the existing linear models meet all C1/C2/C3 conditions. These analyses also inspire us that ignoring the functional approximation of softmax attention does not enable the approximation of softmax attention values.

## 5 MetaLA Transformer

Transformer is stacked by a series of Encoder/Decoder blocks. Generally, each block is composed of two modules in sequence: token mixer and channel mixer [34; 35]. Softmax attention plays the role of the token mixer. In this work, we follow the Transformer architecture as a whole but use our proposed MetaLA module as the token mixer. Due to space constraints, the architecture of MetaLA Transformer is given in detail in Appendix A3. Here we focus on describing the three enhancements of MetaLA relative to the general linear attention in Sec. 3 (see Fig. 2): i) The Key matrix is not used, which is based on our theoretical analysis. ii) Self-augmentation and iii) Short convolution are two other optional techniques to further enhance the modeling ability of our model.

i) _The Key matrix is not used._ We exploit \(-_{t}\) to replace \(_{t}\), based on theoretical analysis in Sec. 4 and appendix A2, i.e., dynamic decay \(_{t}\) is the key mechanism to achieve dynamic memory and static approximation, and \(\) is not required. As shown in Tab. 3, compared with Eq. (13), the main improvement is:

\[_{t}^{h}=(_{t}^{h})_{t-1}^{h}+( -_{t}^{h})^{}_{t}^{d_ {u}^{} d_{v}^{}},\] (21)

which can be trained in a parallel form in Eq. (16). The only difference is that \(\) is replaced by \(\) and \(_{t,:}=-_{t}\). With usage of \(_{t}\) and \(\), MetaLA can achieve linear approximation to SoftAttMap. Without usage of \(\) (\(_{K}\)), we can allocate more parameters and utilize full-rank matrix \(_{}\) to produce dynamic decay rather than low-rank matrix used by GLA, such that we do not sacrifice expression capacity of \(_{}\) and approximation performance of MetaLA.

ii) _Self-augmentation_ can enhance a token's attention to itself, avoiding attention dilution :

\[_{t}^{h}=_{t}^{h}_{t}^{h}+_{} _{t}^{h}(_{}^{h}(- _{t}^{h}))^{}_{t}^{1 d _{v}^{}}.\] (22)

Without changing the hidden state \(_{t}^{h}\) in Eq. (21), the proposed self-augmentation (the second term on the right side of the equation) is only added on the output process, with a learnable parameter \(_{}^{1 d_{k}}\). The proposed design has two advantages (more analysis in appendix A3.2): First, it maintains parallel computing; Second, it augments the information of current token itself and does not affect future output through inner state.

iii) _Short convolution._ An additional short convolution can be inserted before entering the MetaLA layer to enhance local interaction further, motivated by Mamba  and Griffin .

## 6 Experiments

We conduct a comprehensive evaluation of MetaLA to validate its

Figure 3: **Accuracy (%) on the synthetic MQAR task.**

Figure 2: **Recurrent form of MetaLA.** We mark all three enhancements in red.

   Models & \(_{q}(_{t},_{q})\) & \(_{k}(_{t},_{k})\) & \(_{v}(_{t},_{v})\) & \(_{}(_{t},_{})\) & \(_{g}(_{t},_{g})\) & Dimension \\  MetaLA & \(_{t}_{Q}\) & \(-_{t}\) & \(_{t}_{V}\) & \((_{t}_{})\) & \((_{t}_{G}+_{G})\) & \(d_{k}=d/2,d_{v}=d\) \\   

Table 3: **From general recurrent linear form to our MetaLA.**capabilities as a foundation model. i) MQAR . Performance on the Multi-Query Associative Recall (MQAR) task is closely linked to language modeling and can also imply the effectiveness of our theory in modeling hidden states and retrieving information. ii) Autoregressive language modeling on the Pile  dataset and evaluation on Common-Sense Reasoning and SuperGLUE  zero-shot benchmarks are conducted. iii) LRA . We execute experiments on the Long Range Arena (LRA) benchmark  to investigate MetaLA's ability in long sequence modeling. iv) ImageNet . Generalization ability in visual tasks. Due to space constraints, we put some additional experiments in appendix A5, including: v) Scalability. We extend MetaLA to a 3B parameter scale and a 300B data scale for preliminary validation. vi) Retrieval and long context abilities. We evaluated MetaLA's retrieval performance on the MAD tasks , and its effectiveness in handling long contexts on the Needle in a Haystack task . vii) Training efficiency. We provide comparative results on training throughput and GPU memory usage across various models. Detailed experimental setup and further discussion are given in appendix A4.

**Associative Recall.** The synthetic MQAR task  is exploited to evaluate MetaLA's memory ability. In the task, given multiple queries, the model must recall the corresponding key-value pairs before. We follow default settings in  to generate datasets. Fig. 3 shows that MetaLA outperforms other linear models, which have three parameter groups (Mamba , GLA , Based ) or fixed decay (RWKV-4 ), well supporting our theoretical analysis and module design. The attention baseline achieves optimal results (\(>99.0\)) under both conditions. The additional experiments in appendix A5 show that MetaLA outperforms Mamba on more challenging settings.

**Language Modeling.** We train two scales of MetaLA: 360M/1.4B on the Pile dataset. For baselines of 360M MetaLA, we train them from scratch aligned with our configurations. For the 1.3B MetaLA,

   Models & PS & T & WSC & WIC & RTE & CB & MULTIRC & BOOLQ & COPA & AVG \\  Pythia & 0.41 & 15 & 36.54 & 50.00 & 52.35 & 39.29 & 0.31 & 61.99 & 62.00 & 43.21 \\ Mamba & 0.37 & 15 & 36.54 & 50.31 & 52.71 & 42.86 & 2.52 & 58.78 & 64.00 & 43.96 \\ GLA & 0.36 & 15 & 36.54 & 49.84 & 53.07 & 41.07 & 0.42 & 53.49 & 66.00 & 42.92 \\ MetaLA & 0.36 & 15 & 36.54 & 50.00 & 52.71 & 42.86 & 0.31 & 58.96 & 67.00 & **44.05** \\  Pythia\({}^{}\) & 1.4 & 300 & 36.54 & 50.00 & 53.07 & 35.71 & 0.94 & 60.73 & 72.00 & 44.14 \\ HGRN\({}^{}\) & 1 & 100 & 40.38 & 50.78 & 53.43 & 42.86 & 3.04 & 58.69 & 70.00 & 45.60 \\ Mamba & 1.4 & 100 & 39.42 & 50.94 & 55.23 & 26.79 & 1.15 & 53.27 & 73.00 & 42.83 \\ RetNet\({}^{}\) & 1.3 & 100 & 36.54 & 50.00 & 52.71 & 46.43 & 2.52 & 60.21 & 68.00 & 45.20 \\ GLA\({}^{}\) & 1.3 & 100 & 36.54 & 50.16 & 53.07 & 37.50 & 0.31 & 61.04 & 69.00 & 43.95 \\ MetaLA\({}_{a}\) & 1.3 & 100 & 49.04 & 51.25 & 55.60 & 37.50 & 1.78 & 55.50 & 70.00 & **45.81** \\ MetaLA\({}_{b}\) & 1.4 & 300 & 62.50 & 51.88 & 49.10 & 48.21 & 1.57 & 56.27 & 75.00 & **49.22** \\   

Table 4: **Performance Comparison on SuperGLUE.** PS: parameter size (billion). T: tokens (billion). \({}^{}\) means the results reported by . For baselines that need to be compared, if they do not have public checkpoints, we train and test them under identical conditions with MetaLA. MetaLA\({}_{a}\): MetaLA with tied embedding trained using 100B tokens. MetaLA\({}_{b}\): MetaLA trained with 300B tokens.

   Models & PS & T & LOGIQA & WSC273 & BOOLQ & PIQA & HS & WG & ARC-c & OBQA & AVG \\  Pythia & 0.41 & 15 & 21.81 & 57.51 & 61.99 & 63.66 & 33.15 & 51.78 & 22.78 & 28.60 & **42.66** \\ Mamba & 0.37 & 15 & 20.43 & 56.78 & 58.78 & 64.80 & 33.98 & 49.80 & 22.87 & 29.20 & 42.08 \\ GLA & 0.36 & 15 & 23.04 & 56.78 & 53.49 & 63.55 & 32.00 & 52.10 & 22.78 & 27.40 & 41.39 \\ MetaLA & 0.36 & 15 & 22.43 & 58.24 & 58.96 & 63.82 & 32.18 & 53.21 & 23.38 & 28.80 & 42.52 \\  Pythia\({}^{}\) & 1.4 & 300 & 21.35 & 72.89 & 63.12 & 70.89 & 51.98 & 56.99 & 28.41 & 33.20 & 49.85 \\ HGRN\({}^{}\) & 1 & 100 & 22.43 & 58.97 & 58.75 & 71.00 & 48.05 & 51.14 & 28.07 & 31.80 & 46.28 \\ Mamba & 1.4 & 100 & 22.73 & 68.50 & 53.27 & 71.44 & 48.63 & 53.59 & 29.01 & 31.80 & 47.37 \\ RetNet\({}^{}\) & 1.3 & 100 & 22.73 & 63.74 & 60.21 & 69.53 & 48.39 & 53.28 & 26.19 & 30.80 & 46.86 \\ GLA\({}^{}\) & 1.3 & 100 & 21.81 & 63.00 & 61.04 & 70.08 & 48.00 & 51.93 & 28.33 & 31.40 & 46.95 \\ MetaLA\({}_{a}\) & 1.3 & 100 & 21.81 & 65.93 & 55.50 & 70.02 & 47.32 & 55.01 & 27.47 & 33.00 & 47.01 \\ MetaLA\({}_{b}\) & 1.4 & 300 & 21.35 & 73.63 & 56.27 & 72.25 & 53.58 & 58.17 & 30.03 & 34.60 & **49.99** \\   

Table 5: **Performance Comparison on Commonsense Reasoning.**\({}^{}\) indicates testing using open-source checkpoints. HS: HellaSwag. WG: WinoGrande. OBQA: OpenbookQA.

we compare it with publicly available models [14; 15; 16; 20; 44]. We implement all the pre-train experiments with GPT-Neox . The zero-shot evaluation results on SuperGLUE and Commensense Reasoning benchmarks are reported in Tab. 4 and Tab. 5. Specifically, compared to the LinRNN model HGRN , MetaLA expands hidden state dimensions and uses Query matrix; Compared to LinFormer model RetNet  with fixed decay, MetaLA uses dynamic decay; Compared to SSMs like Mamba  and LinFormer with dynamic decay GLA , MetaLA omits the Key matrix in computation. Results indicate that MetaLA has better performance than these linear models and the Transformer-based Pythia . See appendix A5 for more task results.

**Long Sequence Modeling.** LRA is used to evaluate the model's ability in long sequence modeling. We compare MetaLA with Transformer, linear foundation models, and models specifically designed for long sequence modeling. Tab. 7 shows that MetaLA achieves comparable results with SOTA linear models, demonstrating that our model effectively preserves the ability to model long sequences.

**Image Classification.** We compare MetaLA with Deit  (Transformer), HGRN  (LinRNN), GLA  (LinFormer) and Mamba  (SSM) on ImageNet. As shown in Tab. 6, MetaLA performs better than other typical linear models at both scales of 6M and 23M.

**Ablation Studies.** We conduct ablation studies on the 360M model trained with 15B tokens and compare the results in zero-shot experiments. First, restoring the Key matrix in linear attention does not improve performance while increasing parameters, supporting our theoretical result that \(\) is not necessary for approximation, and its functional role can be replaced by dynamic decay. Second, the ablations of self-augmentation and short convolution demonstrate the effectiveness of our model design, i.e., enhance tokens' own attention and local interactions.

## 7 Conclusion and Discussion

**Conclusion.** We unify LinFormer/SSM/LinRNN models into the form of linear attention with Query, Key, Vaule matrices, and then analyze whether they can achieve the optimal approximation to the softmax attention function. Theoretical analysis shows that the existing LinFormer/SSM/LinRNN

   Model & Acc & PS (M) & Acc & PS (M) \\  Deit & 72.20 & 5.7 & 79.90 & 22.0 \\ HGRN & 74.40 & 6.1 & 80.09 & 23.7 \\ GLA & 72.47 & 6.1 & 79.23 & 23.5 \\ Mamba & 73.39 & 6.1 & 79.60 & 23.7 \\ MetaLA & **75.33** & 6.1 & **80.14** & 23.7 \\   

Table 6: **Results on ImageNet-1k.**

   Method & LitsOps & Text & Retrieval & Image & Pathfinder & Path-X & AVG. \\  Transformer  & 38.37 & 61.95 & 80.69 & 40.57 & 65.26 & - & 47.81 \\ S4  & 59.60 & 86.82 & 90.90 & 88.65 & 94.20 & 96.35 & 86.09 \\ DSS-softmax  & 60.60 & 84.80 & 87.80 & 85.70 & 84.60 & 87.80 & 81.88 \\ TNN  & 61.04 & 87.90 & 90.97 & 88.24 & 93.00 & 96.10 & 86.21 \\ S5  & 62.15 & 89.31 & 91.40 & 88.00 & 95.33 & 98.56 & 87.46 \\ Mega  & 63.14 & 90.43 & 91.25 & 90.44 & 96.01 & 97.98 & **88.21** \\ SGConv  & 61.45 & 89.20 & 91.11 & 87.97 & 95.46 & 97.83 & 87.17 \\ LRU  & 60.20 & 89.40 & 89.90 & 89.00 & 95.10 & 94.20 & 86.30 \\ HGRN  & 59.95 & 88.14 & 94.23 & 86.92 & 92.92 & 97.50 & 86.91 \\ Mamba  & 38.02 & 82.98 & 72.14 & 69.82 & 69.26 & 67.32 & 66.59 \\  MetaLA(ours) & 59.34 & 89.27 & 91.28 & 91.88 & 91.66 & 96.57 & 86.67 \\   

Table 7: **Performances Comparison on the Long Range Arena.** We cite baselines from HGRN .

   Models & LOGIQA & WSC273 & BOOLQ & PIQA & HS & WG & ARC-c & OBQA & AVG \\  MetaLA & 22.43 & 58.24 & 58.96 & 63.82 & 32.18 & 53.12 & 23.38 & 28.00 & **42.52** \\ MetaLA w/o selfaug & 21.81 & 58.61 & 57.52 & 64.47 & 32.56 & 49.41 & 23.89 & 29.00 & 42.16 \\ MetaLA w/o conv & 22.58 & 51.65 & 49.36 & 52.07 & 25.82 & 51.22 & 26.54 & 28.80 & 38.51 \\ MetaLA w/ key & 21.20 & 57.88 & 49.11 & 63.00 & 32.99 & 50.99 & 23.63 & 27.60 & 40.80 \\   

Table 8: **Ablation studies.** Ablation study results on the 360M model trained for 15B tokens. We compared the model variants on zero-shot experiments of the Comparison on Commonsense Reasoning benchmark. HS: HellaSwag. WG: WinoGrande. OBQA: OpenbookQA.

cannot_ achieve optimal approximation. Consequently, we propose the MetaLA architecture, which can achieve functional approximation of softmax attention with least parameters. The performance on various types of tasks verifies the effectiveness of MetaLA.

**Discussion and Limitation.** Here, we discuss two key questions about approximation perspectives. i) _How does an optimal approximation to softmax attention inspire linear attention design?_ In this work, we mainly remove the Key matrix, use dynamic decay, and enhance local interactions and the token's own attention. This is clearly not the end of linear attention optimization. This work focuses on functional approximation, previous studies about the value approximation  can be further investigated on the basis of our functional approximation theory as well as MetaLA architecture. Additional optimization may include improving the recall ability of limited hidden states or designing better parameter functions. ii) _Does approximation to the softmax attention imply that linear attention has an upper capacity limit?_ Taken literally, approximation seems to imply that linear attention cannot exceed softmax attention. However, we found better results for linear attention than softmax attention in some experimental results, such as zero-shot and LRA. Similar findings were also reported in previous work . We argue that this issue deserves further exploration. For the time being, evaluation metrics that do not adequately reflect the model's capabilities, insufficient training , and linear attention that does have advantages in certain abilities  are all possibilities.

## 8 Acknowledgements

This work was partially supported by CAS Project for Young Scientists in Basic Research (YSBR-116), National Distinguished Young Scholars (62325603), National Natural Science Foundation of China (62236009, U22A20103, 62441606, 62406322), Beijing Science and Technology Plan (Z241100004224011), Beijing Natural Science Foundation for Distinguished Young Scholars (JQ21015), China Postdoctoral Science Foundation (GZB20240824, 2024M753497), and CAAI-MindSpore Open Fund, developed on OpenI Community.