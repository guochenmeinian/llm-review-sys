# Learning a 1-layer conditional generative model in total variation

Ajil Jalal  Justin Kang

UC Berkeley

{ajiljalal, justin_kang}@berkeley.edu &Ananya Uppal

UT Austin

ananya.uppal09@gmail.com &Kannan Ramchandran

UC Berkeley

kannanr@eecs.berkeley.edu &Eric Price

UT Austin

ecprice@cs.utexas.edu

###### Abstract

A conditional generative model is a method for sampling from a conditional distribution \(p(y x)\). For example, one may want to sample an image of a cat given the label "cat". A feed-forward conditional generative model is a function \(g(x,z)\) that takes the input \(x\) and a random seed \(z\), and outputs a sample \(y\) from \(p(y x)\). Ideally the distribution of outputs \((x,g(x,z))\) would be close in total variation to the ideal distribution \((x,y)\).

Generalization bounds for other learning models require assumptions on the distribution of \(x\), even in simple settings like linear regression with Gaussian noise. We show these assumptions are unnecessary in our model, for both linear regression and single-layer ReLU networks. Given samples \((x,y)\), we show how to learn a 1-layer ReLU conditional generative model in total variation. As our result has no assumption on the distribution of inputs \(x\), if we are given access to the internal activations of a deep generative model, we can compose our 1-layer guarantee to progressively learn the deep model using a near-linear number of samples.

## 1 Introduction

Generative models are in the midst of an explosion in accessibility, as models like DALL-E  or Stable Diffusion  capture the attention of millions. In many cases, these generative models can be succinctly represented by a fundamental mathematical object--the conditional distribution \(p(y x)\). In the example of text-to-image generative models, \(x\) can represent a text prompt or its Word2Vec embedding , and the model can be seen as sampling an image \(y\) from its conditional distribution \(p(y x)\). With large numbers of people accessing these models, a massive amount of sample pairs \((y_{i},x_{i})\), are becoming available online. A natural question to ask is: How many samples \((y_{i},x_{i})\) does it take to learn the conditional generative model \(p(y x)\)?

Recent empirical studies such as Stanford Alpaca , which attempt to learn GPT-3.5 from limited samples, indicate that the number of samples needed may be within a practical range. In this paper, we attempt to address this problem from a fundamental perspective grounded in a concept from classical theoretical statistics: the Maximum Likelihood Estimator (MLE). Specifically, we focus on feed-forward generative models from a relatively simple family and ask: with _no_ assumptions on \(x\), how many samples are required to efficiently learn the conditional generative model \(p(y x)\)?

Linear Regression.Consider ordinary linear regression with Gaussian noise: you observe independent samples \((x_{i},y_{i})^{k}\) of the form

\[y=x w^{*}+ N(0,1).\]How many samples does it take to get a "good" solution? For standard metrics--such as parameter distance in \(w^{*}\) or prediction error on \(y\)--the sample complexity of linear regression depends on properties of the distribution (such as the conditioning of \(x\), or the variance of \(y\)) that could be unbounded and cannot be tested. For example, in the classic analysis (see , Chapter 3), sample complexity depends on the design matrix, which in turn results in a dependence on the expectation of \(\|x\|_{2}\). The typical approach to deal with this would require bounded moments for \(x\). For learning the conditional distribution, the more natural metric is total variation (TV) distance: the parameters \(w\) induce a distribution \(p_{w}(x,y)=p(x)p_{w}(y x)\) where \(p(x)\) is the _true_ distribution of \(x\), and we would like to find \(\) such that

\[d_{TV}(p_{w^{*}}(x,y),p_{}(x,y)).\] (1)

This ensures that when the input \(x\) come from the user in the true unknown distribution \(p(x)\), the model generates a conditional sample that is close in TV to the true model. It turns out that this goal, unlike parameter distance, _can_ be solved with no assumption on the distribution.

**Theorem 1.1** (Informal version of Theorem 4.1).: _The MLE (i.e., least squares regression) achieves (1) with \(O(}k)\) samples, regardless of the distribution of \(x\)._

To our knowledge, all previous guarantees for linear regression either require some assumptions on, or give guarantees in terms of, the distribution of \(x\) or \(y\). We avoid this dependence on the \(x\) distribution by adopting a similar analysis to Theorem 11.2 in .

Main Result: 1-Layer Networks.Modern feed-forward conditional generative models (e.g., StyleGAN2) are more complicated than linear regression. They are stochastic neural networks with layers of the form:

\[y=(W^{*}x+)(0,^{*})\] (2)

for \(x^{k}\), \(y^{d}\), \((x)=(x,0)\) is the ReLU activation, and some weights \(W^{*}^{d k}\), \(^{*}^{d d}\).

We show that 1-layer generative models of the form (2) can _also_ be estimated using the MLE (which is concave), with small TV error and without any assumption on \(x\).

**Theorem 1.2** (Informal version of Theorem 4.5).: _Suppose \(y\) is drawn according to (2) where \(^{*}\) has condition number at most \(\). Using \(O}{^{2}}\) samples, the distribution generated by the MLE has TV error \(\), regardless of the distribution of \(x\)._

Like Theorem 1.1, Theorem 1.2 shows that a conditional generative model can be learned in TV regardless of the distribution of the input \(x\). It generalizes Theorem 1.1 in three ways: (a) the output \(y\) is \(d\)-dimensional rather than 1-dimensional; (b) the covariance of the noise \(^{*}\) is learned rather than specified; and (c) the ReLU nonlinearity \(\) imparts additional complex structure.

Indeed, the point (c) means that parameter distances are poor metrics for this problem. For example, when an element of \(W^{*}x\) has a large negative bias, the corresponding coordinate in \(y\) will almost

Figure 1: A conditional distribution defined by a conditional generative model. To sample from the conditional distribution \(p(y x)\), we perform inference. Due to the stochastic nature of the model, each output is different.

always be \(0\). This means sample pairs \((x_{i},y_{i})\) provide little information about \(W^{*}\), but still provide useful information about the conditional distribution. As we will later see, exploiting this valuable information in the truncated samples allows us to significantly outperform prior works such as , which do not exploit these \(0\)-valued samples.

Multilayer Networks Given Activations.Given our distribution-free results on \(1\)-layer networks, it is possible to extend our results to deep multi-layer networks. Given access to the internal activations of a neural network (but not the weights), our results can be applied layer-wise. Intermediate layers may have poorly conditioned input distributions, but since our result does not depend on the input distribution, we achieve strong guarantees for layer-wise learning in Theorem 4.6.

### Proof Approach

In this outline we focus on the case of \(1\)-dimensional \(y\), and and standard Gaussian noise \((0,1)\). Our proof approach is inspired by learning bounds that exploit finite VC dimension. We would like to show (1), or equivalently,

\[d(w^{*},):=*{}_{x}[d_{TV}(p_{w^{*}}(y x ),p_{}(y x))],\] (3)

when we see \(n\) samples \(x_{i}\) of \(x\), and one sample \(y_{i}\) for each \(x_{i}\). We do this in two stages. First, we show that the empirical distance between \(w\) and \(\) is small i.e.,

\[(w^{*},):=*{}_{x}[d_{TV}(p_{w^ {*}}(y x),p_{}(y x))] 0.5,\] (4)

where \(*{}_{x}[f(x)]=_{i=1}^{n}f(x_{i})\) denotes the empirical expectation over \(x\).

Second, we show that the empirical distance is a good proxy for the true distance, i.e.,

\[d(w^{*},)(w^{*},)+0.5,\] (5)

which gives (3).

Linear Case.In the linear case, both stages are straightforward. The linear regression solution has an explicit form, and it is well known and easy to show that

\[*{}(x^{T}(w^{*}-))^{2} k/n.\]

Since \(d_{TV}(p_{w^{*}}(y x),p_{}(y x))=((1,|x(w^{ *}-)|))\), Jensen's inequality implies (4) for \(n>k/^{2}\).

Secondly, \(f_{w}(x):=d_{TV}(p_{w^{*}}(y x),p_{w}(y x))\) is bounded and unimodal in \(w\). Thus, it suffices to bound the deviation of the empirical average from the true \(f_{w}(x)\) with Chernoff's inequality.

ReLU Case.In the ReLU case, we have \(y=(w^{*} x+)\), and both stages of the previous analysis are more difficult.

The most interesting part of our proof is showing the first stage for the ReLU case, which states that the \(\) maximizing

\[L(w):=_{i=1}^{n} p_{w}(y_{i} x_{i})\]

satisfies (4). Now, for any \(w\) not satisfying (4),

\[*{}_{y}[L(w)-L(w^{*})] =-*{}_{x}d_{KL}(p_{w^{*}}(y x)\|p_{w} (y x))\] \[-2*{}_{x}[d_{TV}(p_{w^{*}}(y x),p _{w}(y x))^{2}]-2^{2},\]

where the first inequality follows from Pinsker's inequality. Unfortunately, \(L(w)-L(w^{*})\) does not concentrate well, by virtue of the KL-divergence being unbounded. However, we can upper bound it via the Bernstein inequality, such that for a fixed \(w\) not satisfying (4), and given \(n=}()\) samples, we have

\[L(w)-L(w^{*})-^{2},\]with probability \(1-\). Using a careful covering argument and \(n=(k/^{2})(1/)\) samples, we can uniformly extend this to _all_\(w\) not satisfying Eq (4). By definition, the MLE has \(L() L(w^{*})\), and by our uniform bound, it must satisfy (4).

The second stage changes because \(f_{w}(x)\) depends on \(x w\) and \(x w^{*}\) in a more complicated way than through \(x(w-w^{*})\). This makes showing a bounded VC dimension more difficult; however, unpacking the proof that VC implies generalization, we can still show that the net (normally given by Sauer's lemma) is bounded. This generalization holds as long as \(f_{w}(x)\) is unimodal in \(x w\).

## 2 Contributions

1. We show that MLE can perform distribution learning in the setting of linear regression and multi-layer ReLU networks. Our bounds do not make assumptions on the distribution of \(x\) or the condition number of \(W^{*}\), and achieve a sample complexity polynomial in the system parameters.
2. We improve the sample complexity bound in , which estimates the parameters of a one-layer ReLU network but suffers an exponential dependence on the \(W^{*}\) term. In contrast, as we seek to estimate the _distribution_ of \((x,y)\), rather than the parameter \(W^{*}\), we are able to avoid this. See Section 4.2 for more details.
3. Our algorithm for learning multi-layer ReLU networks is considerably simpler than , who learn discriminators that are engineered to perform moment-matching on the output of each layer of the network. Furthermore,  impose a strong requirement on the sparsity and independence of the activations at each layer, which essentially allows standard techniques in sparse coding to recover these activations.

## 3 Related Work

Generative adversarial networks (GANs) [19; 2; 30] are a popular family of generative models that train a generator and discriminator in an adversarial training framework. The seminal result by  proposed _progressive_ growing of GANs (PGGANs) as a way to stabilize and accelerate the training phase of these models. Future results, such as StyleGAN  introduce more complicated architectures and "style" variables. Additionally, these models add noise at each layer of the generator in order to introduce greater stochasticity in the generated images, which is important for textures such as hair and skin.

Distributional LearningMost theoretical results have focused on the min-max optimality of GANs [15; 28; 29], characterizing their stationary points [21; 17], or characterizing their generalization once they have reached a global minimum [3; 5]. The closest result to ours is . Setting \(x\) to a deterministic scalar in our problem statement reduces it to , who consider \(y=(b+)\) s.t. \(b,^{d}\), and they seek to learn the covariance of \(\) along with the bias vector \(b\). However, their sample complexity bound suffers an exponential dependence on \(\|b\|_{}^{2}\).

Single layer networks have attracted recent attention, as they provide a tractable formulation for studying the dynamics and generalization of adversarial learning [24; 18; 25; 11]. The recent results of  show that multi-layered models that satisfy a property known as _forward super-resolution_ (such as PGGANs) can be learned in polynomial time and sample complexity using stochastic gradient descent-ascent. In this case, the discriminator is designed to detect differences between higher order moments of the generated and training distribution. Deep models have also been considered in [12; 10].

## 4 Main Results

In this section we first show that the MLE of the parameters learns the input-output joint distribution for linear regression. Then, we extend this guarantee to the case where the ReLU activation function \(\) is applied to the multi-dimensional output \(y\). Finally, we show that we can compose the 1-layer ReLU guarantee to learn the distribution generated by a multi-layer model.

### Linear Regression

We begin with the classic linear regression problem of learning the parameter \(w^{*}\) from a linear model

\[y=x w^{*}+,\] (6)

where \((0,^{2})\), \(w^{k}\), \(\) is known and \(x^{k}\) with some distribution. This problem has been studied for centuries. Our novelty is that we view (6) as a conditional generative process, and instead of studying error in Euclidean distance in parameter space, i.e., minimizing \(\| x-w^{*} x\|_{2}\), we focus on error in \(d(w^{*},)\) as defined in (3), which only captures the error in \(\) insofar as it impacts our distribution estimate. Given data \(\{(x_{i},y_{i})\}_{i=1}^{n}\) generated by (6), the MLE is:

\[:=*{arg\,max}_{w^{k}}_{i[n]} p _{w}(y_{i}|x_{i})*{arg\,min}_{w^{k}}_{i[ n]}-w x_{i})^{2}}{^{2}}.\]

The following theorem establishes that the MLE is close in TV distance. The proofs for all results in this section are in Appendix A.

**Theorem 4.1**.: _Let \(\{(x_{i},y_{i})\}_{i=1}^{n}\) be i.i.d. random variables generated from the linear model (6), and assume that \(\) is known. Then, for a sufficiently large constant \(C>0\),_

\[n=C}\]

_samples suffice to ensure that with probability \(1-e^{-(^{2}n)}\) over the data,_

\[d(,w^{*}).\]

Note that one cannot hope to get such a guarantee in the classical setting where error is measured in \(\| x-w^{*} x\|_{2}\) without additional assumptions on the distribution of \(x\) because if \(x\) is badly conditioned, the error may be dominated by very rare directions of \(x\) that we never sample. For example, the bounds in , Chapter 3, require the second moment of \(x\) to be bounded.

Since we only wish to learn the distribution of \(y\) in total variation, no single \(x\) can contribute much to our loss and we get a distribution-free result. This is possible as the total variation distance is bounded, and we can invoke Theorem 11.2 in Gyorfi et al .

We now state two lemmas needed to prove Thereom 4.1, assuming without loss of generality that \(^{2}=1\). We split the proof into two stages. In the first stage, we bound \((,w^{*})\), which denotes the empirical TV distance (4) over the training set \(\{x_{i}\}_{i=1}^{n}\).

**Lemma 4.2**.: _Let \(\{(x_{i},y_{i})\}_{i=1}^{n}\) be i.i.d. random variables such that \(y_{i}=x_{i} w^{*}+(0,1)\). Then, for \(n\), with probability \(1-e^{-(n)}\), the MLE \(\) satisfies_

\[(,w^{*})}.\]

The proof relies on the fact that \(p_{w^{*}}(y|x_{i})\) and \(p_{}(y|x_{i})\) are Gaussian distributions, so

\[d(p_{}(y|x_{i}),p_{w^{*}}(y|x_{i}))=(\{1,|x_{i}^{T}( -w^{*})|\}).\]

Using the explicit form of the MLE, we can show that, with high probability,

\[_{i}x_{i}^{T}(-w^{*})^{2},\]

and Lemma 4.2 follows from Jensen's inequality.

The second stage shows that the empirical average of the TV distance \((,w^{*})\) is close to the population average \(d(,w^{*})\).

**Lemma 4.3**.: _Let \(\{x_{i}\}_{i=1}^{n}\) be i.i.d. random variables such that \(x_{i}_{x}\). For a sufficiently large constant \(C>0\), and for \(n=C}\) with \(n\), we have:_

\[_{x_{i}_{x}}_{w^{k}}| (w,w^{*})-d(w,w^{*})|> e^{-( ne^{2})}.\]Note the probability in the above statement is with respect to the distribution of \(x\), and does not depend on \(y\). The proof follows Theorem 11.2 in Gyorfi et al : it relies on the fact that the TV distance is bounded and a unimodal function of \(w x\). This implies that for each \(x_{i}\), we are able to partition the space of \(w\) with \(O(1/)\) hyperplanes such that within each cell the TV distance varies by at most \(\). As we have \(n\) samples and \(w^{k}\), the number of cells induced in \(^{k}\) is \((n/)^{k}\), and it is sufficient to provide concentration bounds for one representative in each cell. This approach is similar to bounding the VC dimension of a set of binary functions. Setting \(n=(})\) and combining Lemma 4.2 with Lemma 4.3 gives \(d(,w^{*})\).

### ReLU Case

Now consider the single-layer ReLU. We observe \((x,y)^{k}^{d}\) such that:

\[y=(W^{*}x+),(0,^{*}),\] (7)

where \(^{d}\), \(W^{*}\) and \(()=(,0)\) is applied coordinate-wise. The matrices \(W^{*}^{d k}\) and \(^{*}^{d d}\) are _unknown_, and we do not observe \(\). The variable \(x\) is drawn from an arbitrary probability distribution \(_{x}\), and we make _no additional assumptions_ on \(_{x}\): this is important, as we will progressively cascade layers, and one should think of \(_{x}\) being the distribution of activations at each layer.

Given a sample \((x,y)^{k}^{d}\), let \(S\) denote the co-ordinates of \(y\) that are zero-valued, and let \(S^{c}\) denote the compliment of \(S\). Then, the log-likelihood of \(W,,\) on this sample is given by

\[ p_{W,}(y|x)=c-+_{ t_{S} 0\\ t_{S^{c}}=y_{S^{c}}}-^{-1}( t-Wx)}{2}}dt_{S}.\] (8)

where \(c\) is a normalization constant which does not depend on \(W\) or \(\). This function is a mixed density: in the coordinates of \(y\) that are 0, i.e., in the set \(S\), we integrate the Gaussian density over the negative orthant, as \(W^{*}x+\) could have been any negative value in those coordinates.In Lemma F.1 in the Appendix, we show that Eqn (8) is concave after an invertible reparameterization of \(W,\).

In this setting, proving an analogue of Theorem 4.1 poses multiple challenges:

* The output \(y\) is \(d\)-dimensional rather than a scalar, and \(\) in Eqn (7) introduces correlations between the coordinates of \(W^{*}x\), such that we cannot decompose the log-likelihood in Eqn (8) per coordinate.
* We do not know the covariance matrix \(^{*}\), and it must be estimated.
* Lemma 4.2 requires the explicit form of the MLE in linear regression. In the absence of such a closed-form solution, we need to directly analyze the log-likelihood, which is a mixed density and involves integrating the Gaussian likelihood over the zero-valued coordinates of \(y\). In order to handle this, we use the Gyorfi approach again on the log-likelihood. This is challenging because the variables we concentrate are KL-divergences, which are unbounded and require Bernstein type inequalities.
* otherwise, their sample complexity scales as \(e^{ W_{}^{2}}\).

Nonetheless, we can handle most of these difficulties, and the only assumption we make is that the condition number of \(^{*}\) is bounded and known to our estimator.

**Assumption 4.4** (Condition number bound).: _Let \(_{}^{*},_{}^{*}\) denote the largest and smallest singular values of \(^{*}\). We assume there exists \(<\) such that \(^{*}}{_{}^{*}}\). We further assume that the value of \(\) is known to our estimator._

Note that the condition number only allows us to control the correlation between the coordinates of \(W^{*}x+\). The other challenges introduce by the ReLU, such as the lack of a closed form MLE, the need to estimate \(^{*}\) and a mixed density log-likelihood remain.

Under Assumption 4.4, the following theorem shows that the MLE \(,\) achieves a small total variation distance. The proof of this theorem is in Appendix C.

**Theorem 4.5**.: _Let \(^{d d}_{}\) denote the set of positive definite matrices with condition number \(\). Given \(n\) samples \(\{(x_{i},y_{i})\}_{i=1}^{n}\) satisfying Assumption 4.4, where \(x_{i}_{x}\) i.i.d., and \(y_{i}\) is generated according to (7), let \(,:=_{W^{d k}, ^{d d}_{}}_{i} p_{W,}(y_{i} x _{i})\). Then, for a sufficiently large constant \(C>0\),_

\[n=C(}{^{2}})\]

_samples suffice to ensure that with probability \(1-\), we have_

\[d_{TV}(,),(W^{*},^{*}).\]

Comparison to .Our result is closely related to . Our ReLU model reduces to their model by setting \(W^{*}^{d}\) as a vector, and \(x=1\). In order to learn the distribution of \(y\), they first estimate the parameters \(W^{*},^{*}\), in \(_{2}\) norm and then convert the \(_{2}\) error to a TV error. The parameter recovery is done per coordinate of \(W^{*},^{*}\), by performing MAP estimation on the positive samples in \(y\). This crucially assumes that each coordinate has enough positive samples, and to that end, they assume that each entry in \(W^{*}\) is positive--otherwise, their sample complexity scales as \(e^{\|W^{*}\|_{}^{2}}\). Our results _do not make any assumptions_ on \(W^{*}\) and handles a wider class of matrix valued \(W^{*}\). Additionally, the objective function (8) does not discard the zero valued samples in \(y\), making it more sample efficient.

We assumed that the covariance matrix \(^{*}\) has condition number \(\), and our sample complexity scales as \(\). Hence, even if \(=e^{(d,k)}\), we only pay a \((d,k)\) penalty. This improves on the result in , where the sample complexity scales as \(^{2}\). While our statistical guarantees are strictly better,  gives poly-time and poly-space algorithmic guarantees for their estimator. We discuss the empirical limitations of our algorithm in Section 6.

Lower BoundsIgnoring log factors, the complexity factor of \(kd\) is obviously required. Furthermore, learning a Gaussian with unknown covariance matrix in total variation takes \((d^{2})\) samples; see . Our Theorem 4.5 would solve their lower bound instance, the same lower bound applies to our problem.

Extension to Multi-Layer Generative Models.Consider the following \((L+1)\)-layered generative model.

\[x_{L+1} =W_{L}^{*}x_{L}+_{L}, x_{}=(W_{ -1}^{*}x_{-1}+_{-1})^{d_{}} [1,L],\] (9) \[x_{0} _{0}_{} (0,_{}^{*})[0,L].\] (10)

We can compose the guarantees provided by Theorem 4.1 and 4.5 to show that we can learn this model.

**Theorem 4.6**.: _Given \(n\) i.i.d. samples of \((x_{0},,x_{L+1})\), such that each matrix \(_{}^{*}\) satisfies Assumption 4.4, let \(_{},_{},\) be the MLE estimates of \(W_{}^{*},_{}^{*}\) learned from samples of \((x_{},x_{+1})\). Define \(m:=_{}d_{}^{2}\). Then,_

\[n=Om}{^{2}},\]

_samples suffice to ensure that with probability \(1-\),_

\[d_{TV}\{_{},_{}\}_{=0}^{L},\{( W_{}^{*},_{}^{*})\}_{=0}^{L}.\]

Comparison to .The modelling assumptions in  are similar to ours - the authors learn a generative model per layer, using images produced per layer. The key differences of their model are: (i) each layer is deterministic (there is no \(_{}\)), (ii) their learning algorithm does not require access to the activations of each layer, (iii) their algorithm performs moment-matching by crafting the discriminator strategically.

In order to avoid requiring activations at each layer,  imposes a sparsity assumption on the activations: this allows them to leverage tools from existing results in sparse coding , such thatsparse activations at each layer can be recovered using images produced by the layer. This assumption can be somewhat strong, as it implies that the activations are roughly independent of one another, and the sparsity remains constant over layers, despite the layers themselves expanding by a factor of 4, i.e., \(d_{} 4d_{-1}\ \  L-1\).

## 5 Simulations

We now numerically verify our theoretical claims and compare against other approaches. A detailed description of simulation methods are included in the appendix. Our code is available at https://github.com/basics-lab/learningGenerativeModels.git.

### Scaling in \(n\) and \(k\)

Figure 2 numerically investigates how TV distance of the MLE scales with the number of samples \(n\) and the input dimension \(k\). We consider a model with \(1\)-dimensional output and a \(k\)-dimensional input: \(y=(x w^{*}+)\), for \(w^{*}^{k}\) and \((0,^{2})\). We set \(^{}=1\) and \(w^{*}=_{k 1}\), both unknown to the optimizer, which has samples \((y_{i},x_{i})_{i=1}^{n}\). Figure 1(a), which plots the error in TV distance against \(n\) on a log-log plot, has a slope of roughly \(-1/2\) as predicted by our theory. Similarly, Figure 1(b) has a slope of \(1/2\), which is in line with our theory on scaling with respect to input dimension \(k\). We defer simulations involving scaling in \(d\) to the appendix because computing TV distance becomes increasingly difficult as \(d\) becomes large, and we must resort to using upper bounds.

### Distribution Independence

The fact that our guarantee does not depend on the distribution of \(x\) suggests that the expected TV error of the distribution learned from the MLE may be similar for all distributions over \(x\). To test this we consider both \(x(0,I_{k})\) and \(x_{k}(0,1)\), i.e., each element of \(x\) is drawn independently standard Laplace. Figure 1(a) verifies our hypothesis, showing only very slight differences in our observed empirical average TV error between the two distributions over a wide range of \(n\), and for \(k=5\) and \(k=20\).

Figure 2: (a) Plot of TV distance vs. \(n\). \(^{2}=1\), \(w^{*}=_{k 1}\), for two different values of \(k=5\) and \(k=20\). Plot includes data for two different distributions of \(x\). Note that distribution has little impact on TV distance, and in both cases, we see the error decreasing with a slope of \(-1/2\) in alignment with our theory. (b) Plot of TV distance vs. input dimension \(k\). For both \(n=10^{3}\) and \(n=10^{4}\), the error grows with a slope of roughly \(1/2\), in alignment with our theory. In both plots \(2000\) runs are used to compute the mean. Error bars represent 95% confidence intervals.

### Scaling with Bias and Condition Number of Covariance Matrix

A key feature of the MLE is that it makes use of truncated samples. This is in contrast to , which leverages results on learning truncated normal distributions  where truncated samples are not observed. This leads to a stark difference in performance as the number of truncated samples becomes large. To show this, we consider a model with a \(d\)-dimensional output and \(1\)-dimensional input. We let \(x=1\) almost surely, and then take \(w^{*}=b_{d 1}\) for some _bias_\(b\) and \(=(0,)\) with \(=I_{d}\), thus \(y=(+b_{d 1})\). As \(b\) becomes more negative, the number of truncated samples increases. Figure 2(a) shows the differing behavior of MLE and that of  as \(b\) becomes negative. For ease of computing the TV distance we set \(d=3\), and restrict optimization over diagonal \(\). The solid blue line depicts the performance of the MLE. We observe that the TV error is constant for \(b>0\) and begins to decrease rapidly for \(b<0\). This happens because as \(b\) becomes more negative, the truncation places more probability mass at \(y=0\). Indeed, the dashed blue lines indicate that even as the TV error is decreasing rapidly, the mean square estimation error of the covariance and mean increase, however, since most of the probability mass is at zero, this does not significantly impact the TV. In contrast, the method of  rapidly deteriorates as \(b<1\) as the number of untruncated samples decreases. We also point out that even when the bias is large,  is still significantly worse. We attribute this to the fact that even when there is no truncated samples,  is still minimizing a different MAP objective. More discussion of this is provided in the appendix.

Robustness to Condition Number of \(\).Another concern is how TV error scales as a function of the condition number of \(^{*}\). Poorly conditioned \(^{*}\) can put significant probability masses on small sets, and potentially cause large error. We consider a similar environment to the one described above, but fix \(b=1\) and alter diagonal entries of \(^{*}\) such that one entry is \(\), another is \(}\) and the rest are \(1\), making the condition number \(\). Figure 2(b) shows that the MLE is not measurably impacted by the changing condition number over the range plotted. This is not true of , where we observe that TV does grow with condition number.

## 6 Limitations

This work is only a first step to understanding fundamental limits of learning generative models. We showed that our theorems for single-layer networks can be composed to get sample complexity bounds on deep networks, with a critical caveat: we require access to not just the input and output pairs, but also the intermediate activations. This is not practical in many scenarios, and removing this restriction will be an important direction for future research. Beyond this, we assume that the learner has an understanding of the model architecture. In many cases, however, a learner may not be aware

Figure 3: (a) Left hand axis shows TV distance vs. bias vector \(b\) with \(y=(+b_{d 1})\), \(d=3\), \(=(0,)\), and \(=I_{d}\). Note that MLE (blue) has error going to zero as bias becomes negative, while the opposite is true for the baseline (red). Right hand axis shows the mean-squared error of the parameters \(\) and the mean \(\), each point was run a total of \(2000\) times. (b) TV distance vs. condition number, \(d=3\). MLE does not exhibit trend with condition number, but baseline does. Error bars are 95% confidence intervals, over 20000 runs.

of the number of layers a network has or a vast number of other architectural details. Additionally, we inherit known problems with the MLE, such as exacerbation of biases that exist in the training data (Chapter 24.1.3 in ).

Our results place emphasis on sample complexity over _computational complexity_. Though we show that the MLE problem is concave, this work does not provide a thorough analysis of the optimization problem. It is possible that similar results to  can be derived for the MLE problem. Indeed, empirically, we find that a similar projected stochastic gradient ascent performs well in our problem. A careful analysis must consider factors like the distribution over \(x\), the condition number of \(^{*}\) and the truncation probability, all of which are likely to impact the optimization.

## 7 Conclusions

We have studied the problem of learning conditional generative models from a limited number of samples. We have shown that it is possible to learn a 1-layer ReLU conditional generative model in total variation, with no assumption on the distribution of the conditioning variable \(x\) using the MLE. We have also shown that this result can be extended to multi-layer ReLU networks, given access to the internal activations. Our results suggest that MLE is a promising approach for learning feed-forward generative models from limited samples.

## 8 Acknowledgements

Ajit Jalal and Kannan Ramchandran are supported by ARO 051242-002. Justin Kang and Kannan Ramchandran are supported by NSF EAGER Award 2232146. Eric Price is supported by NSF awards CCF-2008868, CCF-1751040 (CAREER), and the NSF AI Institute for Foundations of Machine Learning (IFML).