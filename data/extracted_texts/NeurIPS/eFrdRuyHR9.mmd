# Transition Constrained Bayesian Optimization via Markov Decision Processes

Jose Pablo Folch

Imperial College London, UK

&Calvin Tsay

Imperial College London

London, UK

&Robert M Lee

BASF SE

Ludwigshen, Germany

&Behrang Shafei

BASF SE

Ludwigshen, Germany

&Weronika Ormaniec

ETH Zurich

Zurich, Switzerland

&Andreas Krause

ETH Zurich

Zurich, Switzerland

&Mark van der Wilk

Imperial College London

London, UK

&Ruth Misener

Imperial College London

London, UK

&Mojmir Mutny

ETH Zurich

Zurich, Switzerland

###### Abstract

Bayesian optimization is a methodology to optimize black-box functions. Traditionally, it focuses on the setting where you can arbitrarily query the search space. However, many real-life problems do not offer this flexibility; in particular, the search space of the next query may depend on previous ones. Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements. Altogether, such _transition constraints_ necessitate a form of planning. This work extends classical Bayesian optimization via the framework of Markov Decision Processes. We iteratively solve a tractable linearization of our utility function using reinforcement learning to obtain a policy that plans ahead for the entire horizon. This is a parallel to the optimization of an _acquisition function in policy space_. The resulting policy is potentially history-dependent and non-Markovian. We showcase applications in chemical reactor optimization, informative path planning, machine calibration, and other synthetic examples.

## 1 Introduction

Many areas in the natural sciences and engineering deal with optimizing expensive black-box functions. Bayesian optimization (BayesOpt) , a method to optimize these problems using a probabilistic surrogate, has been successfully applied to a myriad of examples, e.g. hyper-parameter selection , robotics , battery design , laboratory equipment tuning , and drug discovery . However, state-of-the-art algorithms are often ill-suited when physical sciences interact with potentially dynamic systems . In such circumstances, real-life constraints limit our future decisions while depending on the prior state of our interaction with the system. This work focuses on transition constraints influencing future choices depending on the current state of the experiment. In other words, reaching certain parts of the decision space (search space) requires long-term planning in our optimization campaign. This effectively means we address a general sequential-decision problem akin to those studied in reinforcement learning (RL) or optimal control for the task of optimization. We assume the transition constraints are known _a priori_ to the optimizer.

Applications with transition constraints include chemical reaction optimization , environmental monitoring , lake surveillance with drones , energy systems , vapor compression systems , electron-laser tuning  and seabed identification . For example,Figure 1 depicts an application in environmental monitoring where autonomous sensing vehicles must avoid obstacles (similar to Hitz et al. ). Our main focus application are transient flow reactors [25; 26; 27]. Such reactors allow efficient data collection by obtaining semi-continuous time-series data rather than a single measurement after reaching the steady state of the reactor. As we can only change the inputs of the reactor continuously and slowly to maintain quasi-steady-state operation, allowing arbitrary changes, as in conventional BayesOpt, would result in measurement sequences which are not possible due to physical limitations.

Problem Statement.More formally, we design an algorithm to identify the optimal configuration of a physical system governed by a black box function \(f\), namely, \(x^{}=_{x}f(x)\). The set \(\) summarizes all possible system configurations, the so called _search space_. We assume that we can sequentially evaluate the unknown function at specific points \(x\) in the search space and obtain noisy observations, \(y=f(x)+(x)\), where \(\) has a known Gaussian likelihood, which is possibly heteroscedastic. We assume that \(f\) can be modeled probabilistically using a Gaussian process prior that we introduce later. Importantly, the order of the evaluations is dictated by _known_, potentially stochastic, dynamics modeled by a Markov chain that limits our choices of \(x\).

BayesOpt with a Markov Decision Processes.The problem of maximizing an unknown function could be addressed by BayesOpt, which typically chooses to query \(f(x)\) by sequentially maximizing an _acquisition function_, \(u\):

\[x_{t+1}=*{arg\,max}_{x}u(x|_{t}),\] (1)

depending on all the past data at iteration \(t\), \(_{t}\). Eq. (1) arises as a greedy one-step approximation whose overall goal is to minimize e.g. cumulative regret, and assumes that any choice of point in the search space \(\) is available. However, given transition constraints, we must traverse the search space according to the system dynamics. This work extends the BayesOpt framework and provides a method that constructs a potentially non-Markovian policy by myopically optimizing a utility as,

\[_{t+1}=*{arg\,max}_{}(|_{t}),\] (2)

where \(\) is the greedy utility of the policy \(\) and \(_{t}\) encodes past trajectories through the search space. In the following sections, we will show how to tractably formulate the overall utility, how to greedily maximize it, and how to adapt it to admit policies depending on the full optimization history.

Contributions.We present a BayesOpt framework that tractably plans over the complete experimentation horizon and respects Markov transition constraints, building on active exploration in Markov chains . Our key contributions include:

* We identify a novel utility function for maximum identification as a function of policies, and greedily optimize it. The optimization is tractable, and does not scale exponentially in the policy horizon. In many cases, the problem is convex in the natural representation.
* We provide exact solutions to the optimization problems using convex optimization for discrete Markov chains. For continuous Markov chains, we propose a reparameterization by viewing our problem as an instance of model predictive control (MPC) with a non-convex objective. Interestingly, in both cases, the resulting policies are history-dependent (non-Markovian).
* We analyze the scheme theoretically and empirically demonstrate its practicality on problems in physical systems, such as electron laser calibration and chemical reactor optimization.

## 2 Background

Because our contributions address experimental design of real-life systems by intersecting design of experiments, BayesOpt and RL, we review each of these of components. Refer to Figure 5 in Appendix A for a visual overview of how we selected the individual components for tractability of the entire problem.

Gaussian ProcessesTo model the unknown function \(f\), we use Gaussian processes (GPs) . GPs are probabilistic models that capture nonlinear relationships and offer well-calibrated uncertainty estimates. Any finite marginal of a GP, e.g., for inputs \((x_{1},..,x_{p})\), the values \(\{f(x_{j})\}_{j=1}^{p}\), are normally distributed. We adopt a Bayesian approach and assume \(f\) is a sample from a GP prior with a known covariance kernel, \(k\), and zero mean function, \(f(0,k)\). Under these assumptions, the posterior of \(f\), given a Gaussian likelihood of data, is a GP that is analytically tractable.

### Maximum Identification: Experiment Design Goal

Classical BayesOpt is naturally myopic in its definition as a greedy one-step update (see (1)), but has the overall goal to minimize, e.g., the cumulative regret. Therefore \(u\) needs to chosen such that overall non-myopic goals can be achieved, usually defined as balancing an exploration-exploitation trade-off. In this paper we follow similar ideas; however, we do not focus on regret but instead on gathering information to maximize our chances to identify \(x^{}\), the maximizer of \(f\).

**Maximum Identification via Hypothesis testing.** Maximum identification can naturally be expressed as a multiple hypothesis testing problem, where we need to determine which of the elements in \(\) is the maximizer. To do so, we require good estimates of the differences (or at least their signs) between individual queries \(f(x_{i})-f(x_{j})\); \(x_{i},x_{j}\). For example, if \(f(x_{i})-f(x_{j}) 0\), then \(x_{i}\) cannot be a maximizer. Given the current evidence, the set of arms which we cannot rule out are all _potential maximizers_, \(\). At termination we report our best guess for the maximizer as:

\[x_{T}=*{arg\,max}_{x}_{T}(x),_{T}T.\]

Suppose we are in step \(t\) out of \(T\), then let \(_{t}\) be the set of previous queries, we seek to identify new \(_{}\) that when evaluated minimize the probability of returning a sub-optimal arm at the end. For a given function draw \(f\), the probability of returning a wrong maximizer \(z x_{f}^{}\) is \(P(_{T}(z)-_{T}(x_{f}^{}) 0|f)\). We can then consider the _worst-case_ probability across potential maximizers, and taking expectation over \(f\) we obtain a utility through an asymptotic upper-bound on the log-probability, indeed for large \(T\) we obtain:

\[_{f}[_{z\{x_{f}^{}\}} P( _{T}(z)-_{T}(x_{f}^{}) 0|f)]- _{f}[_{z\{x_{f}^{}\}}^{}))^{2}}{k_{_{t}_{new}}(z,x_{f}^{ })}]\] (3)

The expectation is on the current prior (posterior up to \(_{t}\)), the kernel \(k\) is the posterior kernel given observations \(_{t}_{}\). Since we consider the probability of an error, it is more appropriate to talk about minimizing instead of'maximizing the utility' but the treatment is analogous. Further, note the intuitive interpretation of the bound: the probability of an error will be minimized if the uncertainty is small or if the values of \(f(z)\) and \(f(x_{f}^{})\) are far apart. The non-trivial distribution of \(f(x^{})\) renders the utility intractable; therefore we employ a simple and tractable upper bound on the objective (3) which can be optimized by minimizing the uncertainty among all pairs in \(\):

\[U(_{})=_{z^{},z,z z^{}} [f(z)-f(z^{})|_{t}_{}].\] (4)

Such objectives can be solved greedily in a similar way as acquisition functions in Eq. (1) by minimizing \(U\) over \(_{}\). Note that Fiez et al.  derive this objective for the same problem with linear bandits, albeit they consider the frequentist setting and (surprisingly) a different optimality criterion: minimizing \(T\) for a given failure rate. For their setting, the authors prove that it is an asymptotically optimal objective to follow. They do not consider any Markov chain structure. Derivation of the Bayesian utility and its upper bound in Eq.(4) can be found in Appendix C.1-C.2.

**Utility with kernel embeddings.** For illustrative purposes, consider a special case where the kernel \(k\) has a low rank due to existence of embeddings \((x)^{m}\), i.e., \(k(x,y)=(x)^{}(y)\). Such embeddings can be, e.g., Nystrom features  or Fourier features [32; 33]. While not necessary,

Figure 1: Representative task of finding pollution in a river while following the current. (a) Problem formulation: The star represents the maximizer and the arrows the Markov dynamics. (b) Objective formulation: Orange balls represent potential maximizers, with size corresponding to model uncertainty. (c) Optimization: Deploy a potentially stochastic policy that minimizes our objective.

these formulations make the objectives considered in this work more tractable and easier to expose to the reader. With the finite rank assumption, the random function \(f\) becomes,

\[f(x)=(x)^{T}(0,_{m  m})\] (5)

where \(\) are weights with a Gaussian prior. We can then rewrite the objective Eq. (4) as:

\[U(_{})=_{z,z^{}}||(z)-(z^{ })||^{2}_{(_{x_{i}_{}} }{^{2}}+)^{-1}}.\] (6)

This reveals an essential observation that the utility depends only on the visited states; not their order. This suggests a vast simplification, where we do not to model whole trajectories, and Markov decision processes sufficiently describe our problem. Additionally, numerically, the objective involves the inversion of an \(m m\) matrix instead of \(||||\) (see Sec. 4). Appendix D.1 provides a utility without the finite rank-assumptions that is more involved symbolically and computationally.

### Markov Decision Processes

To model the transition constraints, we use the versatile model of Markov Decision processes (MDPs). We assume an environment with state space \(\) and action space \(\), where we interact with an unknown function \(f:\) by rolling out a policy for \(H\) time-steps (horizon) and obtain a trajectory, \(=(x_{0},a_{0},x_{1},a_{1},...,x_{H-1},a_{H-1})\). From the trajectory, we obtain a sequence of noisy observations \(y():=\{y(x_{0},a_{0}),...,y(x_{H-1},a_{H-1})\}\) s.t. \(y(x_{h})=f(x_{h},a_{h})+(x_{h},a_{h})\), where \((x_{h},a_{h})\) is zero-mean Gaussian with known variance which is potentially state and action dependent. The trajectory is generated using a _known_ transition operator \(P(x_{h+1}|x_{h},a_{h})\). A Markov policy \((a_{h}|x_{h})\) is a mapping that dictates the probability of action \(a_{h}\) in state \(x_{h}\). Hence, the state-to-state transitions are \(P(x_{h+1},x_{h})=_{a}_{h}(a|x_{h})P(x_{h+1}|x_{h},a)\). In fact, an equivalent description of any Markov policy \(\) is the corresponding distribution giving us the probability of visiting a state-action pair under the policy, which we denote \(d_{}\), where

\[:= h[H]\;d_{h} d_{h}(x,a) 0,\;_{a,x}d_{ h}(x,a)=1,\;_{a}d_{h}(x^{},a)=_{x,a}d_{h-1}(x,a)p(x^{}|x,a) }\]

We will use this polytope to reformulate our optimization problem over trajectories. Any \(d\) can be realized by a Markov policy \(\) and vice-versa. We work with non-stationary policies, meaning the policies depend on horizon count \(h\). The execution of deterministic trajectories is only possible for deterministic transitions. Otherwise, the resulting trajectories are random. In our setup, we repeat interactions \(T\) times (episodes) to obtain the final dataset of the form \(_{T}=\{_{i}\}_{i=1}^{T}\).

### Experiment Design in Markov Chains

Notice that the utility \(U\) in Eq. 6 depends on the states visited and hence states of the trajectory. In our notation, \(_{t}\) will now form a set of executed trajectories. With deterministic dynamics, we could optimize over trajectories, but this would lead to an exponential blowup (i.e. \(|X|^{H}\)). In fact, for stochastic transitions, we cannot pick the trajectories directly, so instead we work in the space of distributions. For a given policy, through sampling, we are able to create an empirical distribution of all the state-action pairs visited during policy executions, \(_{}(x,a)\), which assigns equal mass to each state-action visited during our trajectories. This allows us to focus on the expected utility over the randomness of the policy and the environment, namely,

\[(d_{}):=U(_{_{1}_{1},..._{t}_{t }}[_{}]).\] (7)

This formulation stems from Mutny et al.  who try to tractably solve such objectives that arise in experiment design by performing planning in MDPs. They focus on learning linear operators of an unknown function, unlike identifying a maximum, as we do here. The key observation they make is that any policy \(\) induces a distribution over the state-action visitations, \(d_{}\). Therefore we can reformulate the problem of finding the optimal policy, into finding the optimal distribution over state-action visitations as: \(_{d_{}}(d_{})\), and then construct policy \(\) via marginalization. We refer to this optimization as the _planning problem_. The constraint \(\) encodes the dynamics of the MDP.

### Additional Related Works

The most relevant prior work to ours is exploration in reinforcement learning through the use of Markov decision processes as in Mutny et al.  and convex reinforcement learning of Hazan et al. , Zahavy et al.  which we will use to optimize the objective. Other related works are:

**Pure exploration bandits objectives.** Similar objectives to ours have been explored for BayesOpt. Li and Scarlett  use the \(\)-allocation variant of our objective for batch BayesOpt, achieving good theoretical bounds. Zhang et al.  and recently Han et al.  take advantage of possible maximizer sets to train localized models, while Salgia et al.  show that considering adaptive maximization sets yields good regret bounds under random sampling. Contrary to them, motivation and derivation in terms of a Bayesian decision rule do not appear elsewhere according to our best knowledge. We also recognize that we can relax the objective and optimize it in the space of policies.

**Optimizing over sequences.** Previous work has focused on planning experimental sequences for minimizing switching costs [11; 40; 41; 21] however they are only able adhere to strict constraints under truncation heuristics [20; 42; 22]. Recently, Qing et al.  also tackle Bayesian optimization within dynamical systems, with the focus of optimizing initial conditions. Concurrent work of Che et al.  tackles a constrained variant of a similar problem using model predictive control with a different goal.

**Regret vs Best-arm identification.** Most algorithms in BayesOpt focus on regret minimization. This work focuses on maximizer identification directly, i.e., to identify the maximum after a certain number of iterations with the highest confidence. This branch of BayesOpt is mostly addressed in the bandit literature . Our work builds upon prior works of Soare et al. , Yu et al. , and specifically upon the seminal approach of Fiez et al.  to design an optimal objective via hypothesis testing. Novel to our setting is the added difficulty of transition constraints necessitating planning.

**Non-myopic Bayesian Optimization.** Look-ahead BayesOpt [48; 49; 50; 51; 52; 53; 54] seeks to improve the greedy aspect of BayesOpt. Such works also use an MDP problem formulation, however, they define the state space to include all past observations (e.g. [55; 56]). This comes at the cost of simulating expensive integrals, and the complexity grows exponentially with the number of look-ahead steps (usually less than three steps). Our work follows a different path, we maintain the greedy approach to control computational efficiency (i.e. by optimizing over the space of Markovian policies), and maintain provable and state-of-art performance. Even though the optimal policy through non-myopic analysis is non-Markovian, in Sec. 4, we show that _adaptive resampling_ iteratively approximates this non-myoptic optimal policies in a numerically tractable way via receeding horizon planning. In our experiments we comfortably plan for over a hundred steps.

## 3 Transition Constrained BayesOpt

This section introduces BayesOpt with transition constraints. We use MDPs to encode constraints. Namely, the Markov dynamics dictates which inputs we are allowed to query at time-step \(h+1\) given we previously queried state \(x_{h}\). This mean that the transition operator is \(P(x_{h+1}|x_{h},a)=0\) for any transition \(x_{h} x_{h+1}\) not allowed by the physical constraints.

Motivated by our practical experiments with chemical reactors, we distinguish two different types of _feedback_. With **episodic feedback** we can be split the optimization into episodes. At the end of each episode of length \(H\), we obtain the whole set of noisy observations. On the other hand, **instant feedback** is the setting where we obtain a noisy observation immediately after querying the function. _Asynchronous feedback_ describes a mix of the previous two, where we obtain observations with unspecific a delay.

### Expected Utility for Maximizer Identification

In section 2.1 we introduced the utility for maximum identification. Using the same simplifying assumption (finite rank approximation of GPs in Sec. 2.1, Eq. (4)), we can show that the expected utility \(\) can be rewritten in terms of the state-action distribution induced by \(_{}\):

\[(d_{})=_{z,z^{}}||(z)-(z^{ })||^{2}_{(d_{})^{-1}}\] (8)

where \((d_{})=(_{x,a X}(x,a) (x,a)(x,a)^{}}{^{2}(x,a)}+)\). The variable \(d_{}(x,a)\) is a state-action visitation, \((x)\) are e.g. Nystron features of the GP. We prove that the function is additive in terms of state-action pairs in Lemma D.1 in Appendix D, a condition required for the expression as a function of state-action visitations . Additionally, by rewriting the objective in this form, the dependence and convexity with respect to the state-action density \(d_{}\) becomes clear as it is only composition of a linear function with an inverse operator. Also, notice that the constraint set is a convex polytope. Therefore we are able to use convex optimization to solve the planning problem (see Sec. 4).

**Set of potential maximizers \(\).** The definition of the objective requires the use of a set of maximizers. In the ideal case, we can say a particular input \(x\), is not the optimum if there exists \(x^{}\) such that \(f(x^{})>f(x)\) with high confidence. We formalize this using the GP credible sets (Bayesian confidence sets) and define:

\[_{t}=\{x:(f(x)|_{t})_{x^ {}}(f(x^{})|_{t})\}\] (9)

where UCB and LCB correspond to the upper and lower confidence bounds of the GP surrogate with a user specified confidence level defined via the posterior GP with data up to \(_{t}\).

### Discrete vs Continuous MDPs.

Until this point, our formulation focused on discrete \(\) and \(\) for ease of exposition. However, the framework is compatible with continuous state-action spaces. The probabilistic reformulation of the objective in Eq. (7) is possible irrespective of whether \(\) (or \(\)) is a discrete or continuous subset of \(^{d}\). In fact, the convexity of the objective in the space of distributions is still maintained. The difference is that the visitations \(d\) are no longer probability mass functions but have to be expressed as probability density functions \(d_{c}(x,a)\). To recover probabilities in the definition of \(\), we need to replace sums with integrals i.e. \(_{x,a}d(x)}{ (x,a)^{2}}_{x,a}d_{c}(x,a) }{(x,a)^{2}}\).

In the Eq. (8) we need to approximate a maximum over all input pairs in \(\). While this can be enumerated in the discrete case without issues, it poses a non-trivial constrained optimization problem when \(\) is continuous. As an alternative, we propose approximating the set \(\) using a finite approximation of size \(K\) which can be built using Thompson Sampling [57; 58] or through maximization of different UCBs for higher exploitation (see Appendix E.1). In Appendix E.5, we numerically benchmark reasonable choices of \(K\), and show that the performance is not significantly affected by them.

### General algorithm and Theory

The general algorithm combines the ideas introduced so far. We present it in Algorithm 1. Notice that apart from constructing the current utility, keeping track of the visited states and updating our GP model, an essential step is _planning_, where we need to find a policy that maximizes the utility. As this forms the core challenge of the algorithm, we devote Sec. 4 to it. In short, it solves a sequence of dynamic programming problems defined by the steps of the Frank-Wolfe algorithm. From a theoretical point of view, under the assumption of episodic feedback, the algorithm provably minimizes the utility as we show in Proposition C.1 in Appendix C.4.

``` Input: Procedure for estimating sets of maximizers, initial point \(x_{0}\), initial set of maximizer candidates \(_{0}\)  Initialize the empirical state-action distribution \(_{0}=0\) for\(t=0\)to\(T-1\)do for\(h=0\)to\(H-1\)do \(_{t,h}(d_{})(d_{}_{t,h}| _{t,h},x_{t,h})\) // define the objective, see eq. (8) \(_{t,h}=_{:d_{}_{t,h}}_{t,h}(d_{})\) // solve MDP planning problem \(x_{t,h+1}=_{t,h}(x_{t,h})\) // deploy policy if feedback is immediate then \(y_{t,h+1}=f(x_{t,h+1})+_{t,h}\) // obtain observation \(_{t,h},\ _{t,h}(_{t,h}, _{t,h})\) // update model and maximizer candidate set \(_{t,h+1}(x)_{t,h}(x_{t,h+1},x)\) // update empirical state-action distribution, see eq. (11) if feedback is episodic then \(_{t,H}=f(_{t,H})+_{t,:}\) // update \(_{t+1,:}\), \(_{t+1,:}(_{t,H},_{t,H})\) // update model and maximizer candidate set Return: Estimate of the maximum using the GP posterior's mean \(_{*}=_{x}_{T}(x)\) ```

**Algorithm 1** Transition Constrained BayesOpt via MDPs

## 4 Solving the planning problem

The planning problem, defined as \(_{d_{}}(d_{})\), can be thought of as analogous to optimizing an acquisition function in traditional BayesOpt, with the added difficulty of doing it in the space ofpolicies. See the bottom half of Figure 5 in Appendix A for a breakdown of the different components of our solution. Following developments in Hazan et al.  and Mutny et al. , we use the classical Frank-Wolfe algorithm . It proceeds by decomposing the problem into a series of linear optimization sub-problems. Each linearization results in a policy, and we build a mixture policy consisting of optimal policies for each linearization \(_{,n}=\{(_{i},_{i})\}_{i=1}^{n}\), and \(_{i}\) step-sizes of Frank-Wolfe. Conveniently, after the linearization of \(\) the subproblem on the polytope \(\) corresponds to an RL problem with reward \(\) for which many efficient solvers exist. Namely, for a single mixture component we have,

\[d_{_{n+1}}=*{arg\,min}_{d}_{x,a,h} (d_{_{,n}})(x,a)d_{h}(x,a).\] (10)

Due to convexity, the state-action distribution follows the convex combination, \(d_{_{,n}}=_{i=1}^{n}_{i}d_{_{i}}\). The optimization produces a Markovian policy due to the subproblem in Eq. (10) being optimized by one. We now detail how to construct a non-Markovian policies by adaptive resampling.

### Adaptive Resampling: Non-Markovian policies.

A core contribution of our paper is receding horizon re-planning. This means that we keep track of the past states visited in the _current_ and _past_ trajectories and adjust the policy at every step \(h\) of the horizon \(H\) in each trajectory indexed by \(t\). At \(h\), we construct a Markov policy for a reward that depends on all past visited states. This makes the resulting policy history dependent. While in episode \(t\) and time-point \(h\) we follow a Markov policy for a single step, the overall policy is a history-dependent non-Markov policy.

We define the empirical state-action visitation distribution,

\[_{t,h}=(^{t}_{x,a _{j}}_{x,a}}_{}+}_{x,a}}_{})\] (11)

where \(_{x,a}\) denotes a delta mass at state-action \((x)\). Instead of solving the objective \((d)\) as in Eq. (10), we seek to find a correction to the empirical distribution by minimizing,

\[_{t,h}(d)=((d+_{t,h})).\] (12)

We use the same Frank-Wolfe machinery to optimize this objective: \(d_{_{t,h}}=*{arg\,min}_{d_{x}}_{t,h} (d_{})\). The distribution \(d_{_{t,h}}\) represents the density of the policy to be deployed at trajectory \(t\) and horizon counter \(h\). We now need to solve multiple (\(n\) due to FW) RL problems at each horizon counter \(h\). Despite this, for discrete MDPs, the sub-problem can be solved extremely efficiently to exactness using dynamic programming. As can be seen in Appendix B.4, our solving times are just a few seconds, even if planning for very long horizons. The resulting policy \(\) can be found by marginalization \(_{h}(a|x)=d_{,h}(x,a)/_{a}d_{,h}(x,a)\), a basic property of MDPs .

### Continuous MDPs: Model Predictive Control

With continuous search space, the sub-problem can be solved using continuous RL solvers. However, this can be difficult. The intractable part of the problem is that the distribution \(d_{}\) needs to be represented in a computable fashion. We represent the distribution by the sequence of actions taken \(\{a_{h}\}_{h=1}^{T}\) with the linear state-space model, \(x_{h+1}=Ax_{h}+Ba_{h}\). While this formalism is not as general as it could be, it gives us a tractable sub-problem formulation common to control science scenario  that is practical for our experiments and captures a vast array of problems. The optimal set of actions is solved with the following problem, where we state it for the full horizon \(H\):

\[*{arg\,min}_{a_{0},,a_{H}}_{h=0}^{H}_ {t,0}(d_{_{,t}})(x_{h},a_{h}),\] (13)

such that \(||a_{h}|| a_{},x_{h}\), and \(x_{h+1}=Ax_{h}+Ba_{h}\), where the _known_ dynamics serves as constraints. Notice that instead of optimizing over the policy \(d_{}\), we directly optimize over the parameterizations of the policy \(\{a_{h}\}_{h=1}^{H}\). In fact, this formulation is reminiscent of the model predictive control (MPC) optimization problem. Conceptually, these are the same. The only caveat in our case is that unlike in MPC , our objective is non-convex and tends to focus on gathering information rather than stability. Due to the non-convexity in this parameterization, we need to solve it heuristically. We identify a number of useful heuristics to solve this problem in Appendix G.

## 5 Experiments

Sections 5.1 - 5.3 showcase real-world applications under physical transitions constraints, using the discrete version of the algorithm. Section 5.4 benchmarks against other algorithms in the continuous setting, where we consider the additive transition model of Section 4.2 with \(A=B=\). We include additional results in Appendix B. For each benchmark, we selected reasonable GP hyper-parameters and fixed them during the optimization. These are summarized in Appendix E.2. As we are interested maximizer identification, in discrete problems, we report the proportion of reruns that succeed at identifying the true maximum. For continuous benchmarks, we report inference regret at each iteration: \(_{t}=f(x_{*})-f(x_{,t})\), where \(x_{,t}=_{x}_{t}(x)\). All statistics reported are over 25 different runs.

**Baselines.** We include a naive baseline that greedily optimizes the immediate reward to showcase a method with no planning (Greedy-UCB). Likewise, we create a baseline that replaces the gradient in Eq. (10) with Expected Improvement  (MDP-EI), a weak version of planning. In the continuous settings, we compare against truncated SnAKe (TrSnaKe) , which minimizes movement distance, and against local search region-constrained BayesOpt or LSR  for the same task. We compare two variants for approximating the set of maximizers, one using Thompson Sampling (MDP-BO-TS) and one using Upper Confidence Bound (MDP-BO-UCB).

### Knorr pyrazole synthesis

Figure 3: Results for Ypacarai and free electron-laser tuning experiments. On the left, the line plots denote the best prediction regret, while the bar charts denote the percentage of runs that correctly identify the best arm at the end of each episode. On the right, We plot the regret and compare against standard BO without accounting for movement-dependent noise.

Figure 2: The Knorr pyrazole synthesis experiment. On the left, we show the quantitative results. The line plots denote the best prediction regret, while the bar charts denote the percentage of runs that correctly identify the best arm at the end of each episode. On the right, we show ten paths in different colours chosen by the algorithm. The underlying black-box function is shown as the contours, and we can see the discretization as dots. We can see four remaining potential maximizers (in orange), which includes the true one (star). _Notice all paths are non-decreasing in residence time, following the transition constraints._Our chemical reactor benchmark synthetizes Knorr pyrrole in a transient flow reactor. In this experiment, we can control the flow-rate (residence time) \(\) and ratio of reactants \(B\) in the reactor. We observe product concentration at discrete time intervals and we can also change inputs at these intervals. Our goal is to find the best parameters of the reaction subject to natural movement constraints on \(B\), and \(\). In addition, we assume _decreasing_ the flow rate of a reactor can be easily achieved. However, _increasing_ the flow rate can lead to inaccurate readings . A lower flow rate leads to higher residence time, so we impose that \(\) must be non-decreasing.

**The kernel.** Schrecker et al.  indicate the reaction can be approximately represented by simple kinetics via a differential equation model. We use this information along with techniques for representing linear ODE as constraints in GP fitting [65; 66] to create an approximate ODE kernel \(k_{ode}\) through the featurization:

\[_{ode}(,B)=(1-(B))y^{(1)}(,B)+(B)y^{(2)}( ,B)\]

where \(y^{(i)}(,B)\) are equal to:

\[_{i}(B)(^{(i)}}{_{1}^{(i)}-_{2}^{( i)}}e^{_{1}^{(i)}}-^{(i)}}{_{1}^{(i)}- _{2}^{(i)}}e^{_{2}^{(i)}}+1)\]

for \(i=1,2\), where \(_{1}^{(i)}\) and \(_{2}^{(i)}\) are eigenvalues of the linearized ODE at different stationary points, \(_{1}(B)=B\), \(_{2}(B)=1-B\), and \((x):=(1+e^{-_{sig}(x-0.5)})^{-1}\) is a sigmoid function. Appendix H holds the details and derivations which may be of independent interest. As the above kernel is only an approximation of the true ODE kernel, which itself is imperfect, we must account for the model mismatch. Therefore, we add a squared exponential term to the kernel to ensure a non-parametric correction, i.e.: \(k(,B)=_{ode}k_{ode}(,B)+_{rbf}(,B)\).

We report the examples of the trajectories in the search space in Figure 2. Notice that all satisfy the transition constraints. The paths are not space-filling and avoid sub-optimal areas because of our choice of non-isotropic kernel based on the ODE considerations. We run the experiment with episodic feedback, for \(10\) episodes of length \(10\) each, starting each episode with \((_{R},B)=(0,0)\). Figure 2 reports quantitative results and shows that the best-performing algorithm is MDP-BO.

### Monitoring Lake Ypacarai

Samaniego et al.  investigated automatic monitoring of Lake Ypacarai, and Folch et al.  and Yang et al.  benchmarked different BayesOpt algorithms for the task of finding the largest contamination source in the lake. We introduce local transition constraints to this benchmark by creating the lake containing obstacles that limit movement (see Figure 12 in the Appendix). Such obstacles

Figure 4: Results of experiments on the asynchronous and synchronous benchmarks. We plot the median predictive regret and the 10% and 90% quantiles. For the asynchronous experiments, we can see that the paths taken by MDP-BO-TS are more consistent, and the final performance is comparable to TrSnAKe. While in the asynchronous setting, we found creating the maximization set using Thompson Sampling gave a stronger performance, in the synchronous setting, UCB is preferred. LSR gives a very strong performance, comparable to MDP-BO-UCB in almost all benchmarks.

in environmental monitoring may include islands or protected areas for animals. We add an initial and final state constraint with the goal of modeling that the boat has to finish at a maintenance port.

We focus on _episodic_ feedback, where each episode consists of 50 iterations. Results can be seen in Figure 2(a). MDP-EI struggles to identify the maximum contamination for the first few episodes. On the other hand, our method correctly identifies the maximum in approximately 50% of the runs by episode two and achieves better regret.

### Free-electron laser: Transition-driven corruption

Apart from hard constraints, we can apply our framework to state-dependent BayesOpt problems involving transitions. For example, the magnitude of noise \(\) may depend on the transition. This occurs in systems observing equilibration constraints such as a free-electron laser . Using the simplified simulator of this laser , we use our framework to model heteroscedastic noise depending on the difference between the current and next state, \(^{2}(x,x^{})=s(1+w||x-x^{}||_{2})\). By choosing \(=\), we rewrite the problem as \((s,a)=s(1+w||x-a||_{2})\). The larger the move, the more noisy the observation. This creates a problem, where the BayesOpt needs to balance between informative actions and movement, which can be directly implemented in the objective (8) via the matrix \((d_{})=_{x,a}d_{}(x,a)(x,a)}(x)(x)^{}+\). Figure 2(b) reports the comparison between worst-case stateless BO and our algorithm. Our approach substantially improves performance.

### Synthetic Benchmarks

We benchmark on a variety of classical BayesOpt problems while imposing local movement constraints and considering both immediate and asynchronous feedback (by introducing an observation delay of 25 iterations). We also include the chemistry SnAr benchmark, from Summit , which we treat as asynchronous as per Folch et al. . Results are in Figure 4. In the synchronous setting, we found using the UCB maximizer criteria for MDP-BO yields the best results (c.f. Appendix for details of this variant). We also found that LSR performs very competitively on many benchmarks, frequently matching the performance of MDP-BO. In the asynchronous settings we achieved better results using MDP-BO with Thompson sampling. TrSnAKe baseline appears to be competitive in all synthetic benchmarks as well. However, MDP-BO is more robust having less variance in the chosen paths as seen in the quantiles. It is important to highlight that SnAKe and LSR are specialist heuristic algorithms for local box-constraints, and therefore it is not surprising they perform strongly. Our method can be applied to more general settings and therefore it is very encouraging that MDP-BO is able to match these SOTA algorithms in their specialist domain.

## 6 Conclusion

We considered transition-constrained BayesOpt problems arising in physical sciences, such as chemical reactor optimization, that require careful planning to reach any system configuration. Focusing on maximizer identification, we formulated the problem with transition constraints using the framework of Markov decision processes and constructed a tractable algorithm for provably and efficiently solving these problems using dynamic programming or model predictive control sub-routines. We showcased strong empirical performance in a large variety of problems with physical transitions, and achieve state-of-the-art results in classical BayesOpt benchmarks under local movement constraints. This work takes an important step towards the larger application of Bayesian Optimization to real-world problems. Further work could address the continuous variant of the framework to deal with more general transition dynamics, or explore the performance of new objective functions.