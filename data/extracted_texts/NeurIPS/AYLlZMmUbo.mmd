# Two Heads are Better Than One:

A Simple Exploration Framework for Efficient

Multi-Agent Reinforcement Learning

 Jiahui Li\({}^{1}\), Kun Kuang\({}^{1}\), Baoxiang Wang\({}^{2,3}\), Xingchen Li\({}^{1}\), Fei Wu\({}^{1}\), Jun Xiao\({}^{1}\), Long Chen\({}^{4}\)

\({}^{1}\)Zhejiang University, \({}^{2}\)The Chinese University of Hong Kong, Shenzhen

\({}^{3}\)Shenzhen Institute of Artificial Intelligence and Robotics for Society, \({}^{4}\)HKUST

{jiahuil,kunkuang}@zju.edu.cn, bxiangwang@cuhk.edu.cn, xingchenl@zju.edu.cn, {uwfei,junx}@cs.zju.edu.cn, longchen@ust.hk

Corresponding Author

###### Abstract

Exploration strategy plays an important role in reinforcement learning, especially in sparse-reward tasks. In cooperative multi-agent reinforcement learning (MARL), designing a suitable exploration strategy is much more challenging due to the large state space and the complex interaction among agents. Currently, mainstream exploration methods in MARL either contribute to exploring the unfamiliar states which are large and sparse, or measuring the interaction among agents with high computational costs. We found an interesting phenomenon that different kinds of exploration plays a different role in different MARL scenarios, and choosing a suitable one is often more effective than designing an exquisite algorithm. In this paper, we propose a exploration method that incorporate the CuriOsity-based and Influence-based exploration (COIN) which is simple but effective in various situations. First, COIN measures the influence of each agent on the other agents based on mutual information theory and designs it as intrinsic rewards which are applied to each individual value function. Moreover, COIN computes the curiosity-based intrinsic rewards via prediction errors which are added to the extrinsic reward. For integrating the two kinds of intrinsic rewards, COIN utilizes a novel framework in which they complement each other and lead to a sufficient and effective exploration on cooperative MARL tasks. We perform extensive experiments on different challenging benchmarks, and results across different scenarios show the superiority of our method.

## 1 Introduction

Multi-Agent Reinforcement Learning (MARL) has been widely used to solve numerous complex real-world problems, such as traffic control , autonomous robotics control [17; 26], game AI , and network routing , _etc._. Compared with single-agent reinforcement learning, the joint state and action space grows exponentially with the number of agents. To overcome this challenge, the Centralized Training with Decentralized Execution (CTDE) [20; 12] paradigm is proposed, where the joint value function is estimated in the central critic during training, and the local agents execute actions according to their own policies.

Based on the CTDE paradigm, numerous deep MARL methods have been proposed [27; 32; 35; 15]. The most commonly used method for exploration is \(\)-greedy. However, these methods may perform poorly in complex scenarios [13; 29] with a large number of agents and a vast action space. The main issue is that the \(\)-greedy exploration strategy requires a significant number of attempts to becomefamiliar with the environment, potentially leading to sub-optimal results. Therefore, an effective exploration strategy is crucial.

Currently, exploration strategies in MARL can be broadly classified into two main categories. The first category includes **curiosity-based exploration** methods [41; 6]. These methods focus on quantifying the unfamiliarity of incoming states and encourage agents to explore a wide range of states that they have not or rarely experienced. The second category comprises **influence-based exploration** methods [9; 33; 39]. These methods aim to promote agents to adopt actions that can influence other agents and achieve better coordination. However, mainstream methods often prioritize the design of sophisticated algorithms within one specific category, which may not perform well in certain scenarios. On the one hand, the joint state space can be excessively large, presenting a challenge for curiosity-based methods to enable agents to experience all unseen states effectively. On the other hand, meaningful interactions among agents can be infrequent, making it difficult for influence-based methods to identify and optimize these interactions. Fortunately, we have discovered that these two types of methods can complement each other, and even a simple combination of them often leads to satisfactory results.

In this paper, we propose an effective and compute-efficient exploration method named COIN, which incorporates CuriOsity-based and INfluence-based exploration. To be specific, we define the influence of one agent on another as the mutual information between its actions and the trajectory of its peer in the next time-step. Our derivation allows us to reformulate the problem of maximizing mutual information into an optimization problem with a tractable lower bound by introducing a variational posterior estimator. This formulation enables us to quantify the impact of each agent's action on the others and use the influence degree as an intrinsic reward to promote exploration. The influence-based intrinsic reward is added to the individual value functions before being sent to the central mixer. Furthermore, this influence-based exploration strategy is compatible with curiosity-based methods, which encourage the agents to explore unfamiliar states. To reduce the exploration state space, we propose an additional method that effectively utilizes the prediction errors of local observations of each agent and the global states to measure the model's uncertainty towards the environment. This uncertainty can be used as an intrinsic reward, which can be added to the extrinsic global reward provided by the environment. Moreover, based on these two types of intrinsic rewards, we have designed a Multi-Agent Reinforcement Learning (MARL) framework where they complement each other, resulting in thorough and efficient exploration without incurring significant computational costs.

Our main contribution can be summarized as follows: (i) We propose a straightforward exploration method based on mutual information that enables the quantification of each agent's influence on the others. (ii) We introduce a simple yet effective approach to measure the model's unfamiliarity with future states, resulting in curiosity-driven exploration. (iii) To achieve effective exploration in MARL settings, we design a framework in which the two kinds of methods complement each other and act as intrinsic rewards which are compute-efficient. (iv) The experiments on three benchmarks StarCraft II , MACO , and Google Football  show the superiority and effectiveness of our COIN.

## 2 Related Work

Curiosity-based Exploration Methods.Exploration algorithms focus on encouraging agents to visit a wide range of different states within the environment to avoid getting trapped in sub-optimal results. Among the various branches of exploration methods, curiosity-based approaches [22; 28] have gained significant popularity. One common approach involves designing intrinsic rewards that measure the degree of unfamiliarity of states. Some researchers utilize pseudo-state counts [2; 21; 30] to calculate such rewards, while others [24; 4]. develop models to predict target states and employ the prediction errors as rewards. For instance, Bai et al.  propose the utilization of multiple models and compute intrinsic rewards based on the standard deviation of predictions, especially in complex environments. Zheng et al.  extends the application of curiosity-based exploration to MARL settings and employs the prediction errors of individual Q-functions as intrinsic rewards. Similarly, Yang et al.  designs intrinsic rewards by considering the variance of an ensemble predicting model that incorporates both local observations and global states.

Influence-based Methods in MARL.The influence-based exploration strategy is specifically designed for MARL problems, where the focus is on the complex coordination and interactions among agents. In this context, mutual information serves as an effective and commonly used tool. Jaques et al.  introduces the concept of "social influence", represented as \(MI(a_{i}^{t};a_{j}^{t}|s^{t})\), to quantify how an agent's action can influence the action of another agent. This measure captures the interplay and dependencies between agents, providing valuable insights into their collaborative behavior. Similarly,Wang et al.  draws inspiration from counterfactual thinking and proposes two exploration techniques: EITI and EDTI. Both of them aim to encourage agents to influence each other's actions. In EITI, mutual information \(MI(o_{j}^{t+1};o_{i}^{t},a_{i}^{t}|o_{j}^{t},a_{j}^{t})\) is also utilized as a formulation to incentivize agents to exert influence on one another, fostering cooperative exploration and coordination.

These influence-based exploration strategies leverage mutual information as a means to capture the interdependencies among agents and promote collaborative exploration in MARL settings. In this paper, we also propose an influence-based intrinsic reward \(MI(_{j}^{t+1};a_{i}^{t})\) based on mutual information which is simple but efficient. The discussions of the superiority of our method compared with the others are left in Appendix.

Hybrid Exploration Methods in MARL.The two exploration strategies mentioned above focus on different aspects, with many studies emphasizing one while neglecting the other. One hybrid method called CIExplore  incorporates both strategies by designing them as intrinsic rewards, which is the closest approach to our work. However, CIExplore has shown to be unstable and ineffective in complex MARL scenarios.

In our approach, COIN, we not only ensure compute efficiency in the designed framework for calculating intrinsic rewards, but we also leverage the complementary nature of the two kinds of rewards to achieve sufficient and effective exploration, ultimately enhancing the model capabilities in complex MARL scenarios.

## 3 Preliminaries

Dec-POMDPs.The exploration method we discussed in this paper focuses on the settings of decentralized partially observable Markov decision process (Dec-POMDP) [19; 3; 5; 7; 23], which can always be described as a tuple:

\[G=<,,,,r,,, >,\]

where \(\) represents the set of agents with \(||=N\). At each time-step, each agent \(i\) selects and executes an action \(a_{i}\) to compose a joint action \(^{N}\). The environment produces the global state \(s\) according to the transition function \((s^{}|s,):^{N} \). Meanwhile, the environment provides a global reward shared by all agents according to the function \(r(s,):^{N}\). Moreover, each agent gains the partial observation \(o\) according to the observation function \((s,i)\) and learns its local policy \(^{i}(a_{i}|_{i}):\) conditions on the its local trajectory \(_{i}\). The goal of all agents is to maximize the discounted cumulative return \(_{i=0}^{}^{i}r_{i}\), where \(\) is a discount factor.

## 4 Method

In this section, we introduce two simple yet effective intrinsic rewards for influence and curiosity-based exploration. Following that, we present a framework, as depicted in Fig. 1, that integrates these two strategies to facilitate exploration. Finally, we provide a brief overview of the loss function and offer implementation details, ensuring both effectiveness and computational efficiency in the exploration process.

### Influence-based Intrinsic Reward

Compared with single-agent reinforcement learning, complex coordination plays an important role in MARL. As a result, it is essential to incentivize agents to explore states or execute actions that can influence each other. Mutual information [9; 14; 33]is a commonly used tool for quantifying the influence between pairwise agents. Following previous works, we define the influence of the \(i\)-th agent on the other agents at time-step \(\) as the sum of the mutual information between the trajectory of the other agents and the action taken by the \(i\)-th agent:

\[_{k i}MI(_{k}^{+1};a_{i}^{})= _{k i}H(_{k}^{+1})-H(_{k}^{+1}|a_{i}^{+1})=_{k i}[ {p(_{k}^{+1}|a_{i}^{})}{p(_{k}^{+1})}],\]

where \(_{k}^{+1}\) represents the trajectory of the \(k\)-th agent at time-step \(+1\), and \(H()\) represents the entropy term.

If we want to promote the influence among agents, we need to maximize \(_{k i}MI(_{k}^{+1};a_{i}^{})\). Meanwhile, we can represent \(p(_{k}^{+1})\) and \(p(_{k}^{+1}|a_{i}^{})\) as the following equations:

\[p(_{k}^{+1})=p(o_{k}^{0})_{t=0}^{}p(a_{k}^{t}|_ {k}^{t})p(o_{k}^{t+1}|_{k}^{t},a_{k}^{t}),\] (1)

\[p(_{k}^{+1}|a_{i}^{})=p(o_{k}^{0}|a_{i}^{}) _{t=0}^{}p(a_{k}^{t}|_{k}^{t},a_{i}^{})p(o_{k}^{t+1}| _{k}^{t},a_{k}^{t},a_{i}^{}).\] (2)

Therefore, the mutual information can be rewritten as:

\[_{k i}MI(_{k}^{+1};a_{i}^{}) =[_{k i}^{0}|a_{i}^{})}{p(o_{k}^{0})}+_{t=0}^{} {p(a_{k}^{t}|_{k}^{t},a_{i}^{})}{p(a_{k}^{t}|_{k}^{t})}+ ^{}^{t+1}|_{k}^{t},a_{k}^{ t},a_{i}^{})}{p(o_{k}^{t+1}|_{k}^{t},a_{k}^{t})}\}}].\] (3)

We can see that Eq. (3) is divided into three terms. Term 1 is determined by the environment, and hence can be ignored. Term 2 computes the information difference of the \(k\)-th agent's action selection when the action at time-step \(\) of the \(i\)-th agent is given. Notice the fact that the action of the \(i\)-th agent at a particular time can not influence any actions, observations, or trajectories of the other agents at the previous time. Thus, the term 2 can also be ignored. Term 3 measures the changes in the target observation of the \(k\)-th agent. This term can be simplified as \(^{+1}|_{k}^{},a_{k}^{})}{p(o_{ k}^{+1}|_{k}^{},a_{k}^{})}\).

Therefore, we conclude that optimizing the mutual information at Eq. (1) is equivalent to optimize:

\[[_{k i}^{+1}| _{k}^{},a_{k}^{},a_{i}^{})}{p(o_{k}^{+ 1}|_{k}^{},a_{k}^{})}].\]

Figure 1: The training framework of COIN. Two additional estimators are set up for computing the intrinsic rewards, where the local observation-action history of all agents are the inputs.

According to the methods of variational inference [31; 14], we can optimize a tractable evidence lower bound of the above expectation by introducing a variational posterior estimator. Hence, we have the following equation:

\[[^{+1}|_{k}^{},a_{k}^{ },a_{i}^{})}{p(o_{k}^{+1}|_{k}^{},a_{k}^{})}][(o_{k}^{+1}|_{k}^{},a_{k}^{ },a_{i}^{})}{p(o_{k}^{+1}|_{k}^{},a_{k }^{})}],\] (4)

where \(q_{}\) represents the variational posterior estimator parameterized by \(\), and we omit the summation notation for convenience.

For encouraging the interaction among agents and promoting exploration, we design an influence-based intrinsic reward for each agent. The intrinsic reward for agent \(i\) at time-step \(\) is computed as:

\[r_{inf_{i}}^{}=_{k i}[_{s} q_{ }(o_{k}^{+1}|_{k}^{},a_{k}^{},a_{i}^{ })- p(o_{k}^{+1}|_{k}^{},a_{k}^{})],\] (5)

where \(_{s}\) is a scaling factor to guarantee \(r_{inf_{i}}^{}>0\).

### Curiosity-based Intrinsic Reward

Curiosity-based exploration is a widely adopted approach in single-agent reinforcement learning, with the objective of encouraging agents to explore unfamiliar states. Typically, this is achieved by computing intrinsic rewards based on prediction errors [24; 4] or variances  of future states, rewards, or value functions. In the context of MARL, curiosity-based exploration remains essential. However, the prediction of joint states becomes unreliable due to the exponential increase in state space with the number of agents, and the scarcity of meaningful states. Fortunately, the local observations of each agent can serve as a measure of curiosity. For simplicity, we use the prediction error of both the global state and the local observations to calculate the curiosity-based intrinsic reward in the partially observable environment as:

\[r_{cur}^{}=|}_{i=1}^{}(o_{i}^{ +1}-f_{o_{i}}(^{},^{}))^{2}+(s^{ +1}-f_{s}(^{},^{}))^{2},\] (6)

where \(f_{o_{i}}(^{},^{})\) and \(f_{s}(^{},^{})\) represent the predicting network to estimate the observation of each agent and global state at time-step \(+1\), and \(^{}\), \(^{}\) represent the joint action, and trajectories of all agents at time-step \(\).

### Implementation of the Estimators

Our goal is to develop two effective estimators for computing intrinsic rewards as well as make them compute-efficient. For the influence estimator, we model \(q_{}(o_{k}^{+1}|_{k}^{},a_{k}^{},a_{i}^{ })\) by a variational posterior estimator, which is updated via loss:

\[L_{}=_{i}_{k i}(o_{k}^{ +1}-q_{}(o_{k}^{+1}|_{k}^{},a_{k}^{},a_{i}^{}))^{2}.\] (7)

Meanwhile, the \(p(o_{k}^{+1}|_{k}^{},a_{k}^{})\) is approximated by a prediction network \(p_{}\) updated via loss:

\[L_{}=_{i}_{k i}(o_{k}^{ +1}-p_{}(o_{k}^{+1}|_{k}^{},a_{k}^{}))^{2}.\] (8)

For the curiosity estimator with parameters \(\), we update it via loss:

\[L_{}=|}_{i=1}^{}(o_{i}^{+1 }-f_{o_{i}}(^{},^{}))^{2}+(s^{+1}-f_ {s}(^{},^{}))^{2}.\] (9)Notice that the summation symbol can become an obstacle when computing intrinsic rewards and losses efficiently. In order to accelerate the training procedure, we have implemented the two estimators in a compute-efficient manner, as illustrated in Fig. 2.

In the influence estimator, Eq. (7) requires us to propagate \(q_{}\) for \(N-1\) iterations in order to obtain the intrinsic reward for a single agent. To improve efficiency, we have designed a batch implementation for \(_{k i}\), allowing us to estimate the observations of all other agents simultaneously in a single forward propagation. Likewise, \(p(|_{k},a_{k})\) also adopts the same trick.

As for the curiosity estimator, all \(f_{o_{i}}\) have the same inputs. To streamline this process, we implement them using a single neural network that outputs a concatenated tensor.

To further alleviate the computational burden, we adopt the parameter-sharing technique, and all agents share the same network in the influence estimator. Similarly, for curiosity-based intrinsic reward, \(f_{s}\) and \(f_{o}\) share the same encoder \(g\) to get the representations. Inspired by the approach presented in , we represent the trajectory of the agent using the hidden state of a recurrent neural network. Consequently, we have \(h_{i}^{}=_{i}^{}\) and \(^{}=^{}\). Meanwhile, for the learning more stable, we compute the intrinsic rewards in Eq. (5) and Eq. (6) via target networks \(^{}\),\(^{}\) and \(^{}\) which are copied from \(\),\(\) and \(\) periodically.

### Incorporation of the Two Heads

We would like to emphasize the effectiveness of combining these two methods in a straightforward manner, rather than designing a intricate algorithm that solely focuses on one aspect. There are two reasons for this: (i) Curiosity-based intrinsic rewards are designed to encourage exploration of unfamiliar states, while influence-based intrinsic rewards aim to promote coordination among agents. These rewards operate on different dimensions and play distinct roles in various scenarios. (ii) Either of the methods may not work in certain training time-steps, as no method can guarantee constant positive influence among agents, and the meaningful unfamiliar joint state is too sparse. Some recent works [33; 39] attempt to use both kinds of methods, however, concerns regarding their effectiveness and computational efficiency remain which lead the unsatisfactory results in complicated scenarios.

In this subsection, we will introduce how to incorporate them under the paradigm of CTDE. For simplicity, we omit the superscript of time-step in this subsection. For the influence-based intrinsic rewards, we apply it to the individual local value functions of each agent to promote the interaction. For \(i\)-th agent it is computed as:

\[_{i}=Q_{i}-_{inf}*r_{inf_{i}},\] (10)

where \(Q_{i}\) is the local value function, and \(_{inf}\) is the scale factor. \(_{i}\) will then be sent to the central critic \(Mixer\) to get the joint value function:

\[}=Mixer(_{1},...,_{N}).\] (11)

Notice that the intrinsic rewards are incorporated before the value functions are sent to the central mixer. This is done to ensure that the intrinsic rewards can be scaled by the network appropriately. Meanwhile, it is worth mentioning that the intrinsic rewards are not applied to the target network. The reasons for this decision will be discussed in the Appendix.

The curiosity-based intrinsic rewards are directly added to the extrinsic rewards, and subsequently, the agents and the central mixer are updated using TD-loss:

\[L_{}=(y-}),\] (12) \[y=r+_{cur}*r_{cur}+*Q_{tot}^{},\] (13)

where \(Q_{tot}^{}\) represents the target network, \(_{cur}\) represents the scale factor.

We show the algorithm of our COIN in _Algorithm_1.

Figure 2: Implementations of the two estimators for computing the summation symbol in a single forward propagation.

```
1:Local agents and central critic \(\) with its target networks \(^{}\), max episode length \(T\), networks of the estimator for computing intrinsic rewards \(\), \(\) and \(\) with its target networks \(^{}\), \(^{}\) and \(^{}\).
2:for each training episode do
3:while global state \(s\) terminal and time step \(t<T\)do
4:\(t=t+1\)
5:for each agent \(i\)do
6: Compute the local value function \(Q_{i}\) and get the hidden state \(h^{i}_{t}\)
7: Select action \(a^{i}_{t}\) and execute
8:endfor
9: Execute the joint action \(=(a^{1}_{t},a^{2}_{t},...,a^{n}_{t})\)
10: Get reward \(r_{t+1}\) and next state \(s_{t+1}\)
11:endwhile
12: Add episode to replay buffer
13: Collate episodes in the buffer into a single batch
14:for\(t=1\) to \(T\)do
15: Compute the influence-based intrinsic reward via Eq. (5)
16: Compute the Curiosity-based intrinsic reward via Eq. (6)
17: Add the influence-based to individual value functions via Eq. (10)
18: Compute the joint value function via Eq. (5)
19: Compute the targets \(y\) via Eq. (13)
20: Update \(\) by minimizing the TD-loss in Eq. (12)
21: Update \(\), \(\) and \(\) by minimizing the loss in Eq. (7),Eq. (8) and Eq. (9)
22: Update \(^{}=\), \(^{}=\), \(^{}=\), \(^{}=\) periodically
23:endfor
24:endfor ```

**Algorithm 1** Influence and Curiosity-based Exploration

## 5 Experiments

Our experiments aim to answer the following questions: (1) Q1: Is the exploration strategy important in MARL? (2) Q2: How does each kind of exploration strategy affect the model performance in different scenarios? (3) Q3: Can influence-based exploration cooperate with curiosity-based exploration?

We performed experiments on three popular MARL benchmarks: StarCraft II micro management challenge (SMAC), MACO, and Google Research Football (GRF) , with different scenarios. The details of each benchmark will be introduced in the Appendix. We adopted two popular baselines, QMIX  and QPLEX , and applied our exploration method to them. To further demonstrate the effectiveness of our exploration method, we chose the curiosity-based exploration method EMC , the influence-based exploration method EITI , and the hybrid method CIExplore  as strong baselines. It should be mentioned that EITI and CIExplore are implemented based on the actor-critic framework in the original paper. For a fair comparison, we implemented EITI and CIExplore based on QMIX under the same framework as us. Meanwhile, since we aim to verify the effectiveness of different exploration strategies, we do not use the special episode buffer in EMC, as well as the other tricks in all methods. All the curves in the figures of this section are the mean of 10 random seeds, which have been smoothed, and the shaded areas represent the standard deviation. The experiment settings are also provided in the Appendix.

### Performance on several benchmarks (Q1)

Performance on StarCraft II benchmark.We first conduct experiments on the most popular environment, StarCraft II micro management challenge. StarCraft II is a real-time strategy game. We set the difficulty of game AI at the "very difficult" level. This benchmark consists of various maps with different levels of challenges, "easy", "hard", and "super hard". We select three representative and super hard scenarios, _3s5z_vs_3s6z_, _corridor_, and _6h_vs_8z_ to carry out experiments2, and the results are depicted in Fig. 3. Our method shows a significant improvement over the baselines in all maps, highlighting the importance of the exploration strategy in MARL. Furthermore, the superior results compared to other exploration baselines demonstrate the effectiveness of our COIN method. We also conducted experiments with sparse reward setting, and the results are provided in the Appendix.

Performance on MACO benchmark.Next, we evaluated our method in the MACO benchmark, which features relatively sparse rewards. MACO incorporates several coordination tasks from various multi-agent learning literature and enhances their difficulty. We conducted experiments on three MACO scenarios: _Aloha_, _Hallway_, and _Gather_. These scenarios require different levels of agent coordination. The results of our experiments are presented in Fig. 4.

In the Aloha scenario [8; 18], the agents strive to achieve as many successful transmissions as possible within a limited number of time steps. None of the other exploration methods proved effective in this scenario. Our COIN approach improved the two baselines by nearly 20 mean transmissions. The results of all methods were peculiar, with an initial burst followed by a drop and then plateaued poor results. Initially, the agents attempted exploration, resulting in more successful transmissions. However, the punishment mechanism in this setting was overly severe, causing the agents to adopt conservative strategies. In the _Gather_ scenario, the task is relatively simple, with agents aiming to choose the same action to achieve common goals set at the beginning of each episode. Although exploration did not significantly improve the performance of the baselines, our COIN approach still achieved a faster convergence speed. There was a failure case known as the "noisy TV" in the Gather scenario. Choosing the goal set at the beginning of the episode resulted in a higher global reward, while not choosing this goal led to a relatively lower reward. It is evident that QPLEX failed

Figure 4: Performance of our exploration method on three scenarios in MACO benchmark.

Figure 3: Performance of our exploration method on three scenarios in StarCraft II benchmark.

and produced suboptimal results, and our framework did not provide significant improvement. The _Hallway_ scenario  involved agents divided into several groups, where agents within the same group aimed to reach the same state simultaneously, while different groups aimed to reach different states. Our COIN approach succeeded with QPLEX, resulting in a 60% improvement in mean win rates. However, applying any exploration method to QMIX showed little improvement, which can be considered another failure case.

Performance on GRF benchmark.In the GRF game, players need to coordinate with each other to organize offense and score goals. The rewards in GRF are sparse, with agents only receiving rewards at the end of the game. Therefore, exploration plays a vital role in guiding the agents to discover winning strategies.

We compared the performance of several baselines on the GRF benchmark, as shown in Fig. 5. The curve representing our COIN approach demonstrates the application of our exploration framework on QMIX. Our approach outperforms in the scenario "academy 3 vs 1 with keeper". QMIX and QPLEX fail to perform well due to the sparse rewards. EMC applies a curiosity-based exploration strategy to QPLEX but results in a decrease in model performance. In contrast, our COIN approach combines influence-based exploration and curiosity-based exploration, outperforming all baselines with an improvement of nearly 0.2 mean scores per episode.

### Ablations and Interesting Findings (Q2, Q3)

To understand how influence-based exploration and curiosity-based exploration work in different scenarios, we conducted ablation studies. We selected three representative scenarios to showcase these results, as depicted in Fig. 6. The dotted line represents applying only the influence-based intrinsic reward on QMIX, while the dashed line represents applying only the curiosity-based intrinsic reward. Interestingly, we found that different exploration strategies play different roles in different scenarios. When only influence-based or curiosity-based exploration was conducted in the _academy 3 vs 1 with keeper_ scenario, the model did not benefit from the exploration strategy. Instead, the performance decreased by nearly 0.5 mean scores. To understand this finding, we consulted literature and watched replays, and find that this phenomenon arises from the parameter sharing technique. According to Li et al. , shared parameters among agents can lead to similar behaviors, hindering the model's ability to learn successful policies in challenging tasks. Our influence-based intrinsic reward promotes agents to influence each other, bringing diversity to their behaviors. However, focusing solely on influence

Figure 5: Experiment results on GRF benchmark in _academy 3 vs 1 with keeper_.

Figure 6: Ablation study of our COIN on GRF and SMAC benchmark.

among agents neglects crucial patterns, such as cutting to attract the enemy defender, resulting in poor outcomes. Fortunately, our proposed method succeeded by striking a balance between influence and curiosity. This interesting result also explains why EMC decreases the baseline performance in this scenario, as shown in Fig. 5. In the scenario _3s55z_vx_3s6z_, influence-based exploration plays a significant role, while curiosity-based exploration harms the performance. This is because the learned winning strategy involves one of the allied units kiting several enemies to the corner of the map and sacrificing itself, thus not requiring much exposure to unknown states. In the _corridor_ scenario, both exploration strategies improve the baseline performance significantly. However, curiosity-based exploration achieves even greater improvement and reaches the best performance.

In all scenarios, the best performance is achieved when both kinds of exploration strategies are incorporated. Therefore, we conclude that different scenarios require different exploration strategies, and when we are unsure which one will be effective, incorporating both is often a good choice. For more experimental results, please refer to the Appendix.

### The Chosen hyperparameters

In Section 4, we introduced several hyperparameters for computing intrinsic rewards. To ensure stability and guarantee that the influence-based intrinsic reward is larger than zero, we set \(_{s}\) in Eq. (5) to 0.5 for all scenarios in all benchmarks. Additionally, \(_{inf}\) and \(_{cur}\) are scaling factors used to adapt the two types of intrinsic rewards to the environment. Given that the importance of the two types of exploration may vary at different time steps, determining which one will dominate is challenging. To address this, we combine the two types of intrinsic rewards in a straightforward manner. When designing the hyperparameters, we follow two principles: (i) ensuring that both types of intrinsic rewards are on the same order of magnitude, and (ii) scaling the intrinsic rewards to be approximately 100 to 1000 times lower than the maximum extrinsic reward at the beginning of the training stage. Details of all the hyperparameters are listed in Appendix.

## 6 Conclusion

In this paper, we explore an interesting phenomenon where different types of exploration play varying roles in different scenarios of MARL. However, most current studies tend to focus on one type of exploration and overlook the other. In light of this, we propose a framework called COIN that effectively combines and complements these two types of exploration strategies. Firstly, COIN measures the influence of each agent on the others using mutual information theory, and assigns it as intrinsic rewards to individual value functions. Additionally, COIN calculates curiosity-based intrinsic rewards by utilizing prediction errors, which are then added to the extrinsic reward. By integrating these two types of intrinsic rewards, COIN creates a novel framework where they complement each other, ultimately leading to more efficient and effective exploration in cooperative MARL tasks. Both of these types of intrinsic rewards are computationally efficient within our designed framework, thus enabling effective exploration. Our experiment results on various benchmarks demonstrate the superiority of our proposed method.

Limitations and future work.Though the incorporation of two kinds of exploration is useful, our method relies on scaling hyperparameters and cannot automatically weigh the two kinds of strategies. In future work, we will investigate how to discover the causal relations among agents. With these potential interdependencies, we can assign appropriate weights to each agent and avoid computing the MI for unnecessary agent pairs.