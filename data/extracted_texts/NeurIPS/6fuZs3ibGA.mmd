# Optimal and Fair Encouragement Policy Evaluation and Learning

Angela Zhou

Department of Data Sciences and Operations

University of Southern California

zhoua@usc.edu

###### Abstract

In consequential domains, it is often impossible to compel individuals to take treatment, so that optimal treatment assignments are merely suggestions when humans make the final treatment decisions. On the other hand, there can be different heterogeneity in both the actual response to treatment and final treatment decisions given recommendations. For example, in social services, a persistent puzzle is the gap in take-up of beneficial services among those who may benefit from them the most. When decision-makers have equity- for fairness-minded preferences over both access and average outcomes, the optimal decision rule changes due to these differing heterogeneity patterns. We study identification and improved/robust estimation under potential violations of positivity. We consider fairness constraints such as demographic parity in treatment take-up, and other constraints, via constrained optimization. We develop a two-stage, online learning-based algorithm for solving over parametrized policy classes under general constraints to obtain variance-sensitive regret bounds. Our framework can be extended to handle algorithmic recommendations under an often-reasonable covariate-conditional exclusion restriction, using our robustness checks for lack of positivity in the recommendation.

## 1 Introduction

The intersection of causal inference and machine learning for heterogeneous treatment effect estimation can improve public health, increase revenue, and improve outcomes by personalizing treatment decisions, such as medications, e-commerce platform interactions, and social interventions, to those who benefit from it the most . But, in many important settings, we do not have direct control over treatment, and can only optimize over _encouragements_, or _recommendations_ for treatment. For example, in e-commerce, companies can rarely _compel_ users to sign up for certain services, rather _nudge_ or _encourage_ users to sign up via promotions and offers. When we are interested in optimizing the effects of signing up - or other voluntary actions beyond a platform's control - on important final outcomes such as revenue, we therefore need to consider _fairness-constrained optimal encouragement designs_. Often human expert oversight is required in the loop in important settings where ensuring _fairness in machine learning_ is also of interest: doctors prescribe treatment from recommendations , managers and workers combine their expertise to act based on decision support , and in the social sector, caseworkers assign to beneficial programs based on recommendations from risk scores that support triage .

The human in the loop requires new methodology for optimal encouragement designs because

when the human in the loop makes the final prescription, algorithmic recommendations do not have direct causal effects on outcomes; they change the probability of treatment assignment.

On the other hand, this is analogous to the well-understood notion of _non-compliance/non-adherence_ in randomized controlled trials in the real world [22; 21]. For example, patients who are prescribed treatment may not actually take medication. A common strategy is to conduct an _intention-to-treat_ analysis: under assumptions of no unobserved confounders affecting treatment take-up and outcome, we may simply view encouragement as treatment. But, in the case of prediction-informed decisions in social settings, if we are concerned about _access to the intervention_ in addition to _utility of the policy over the population_, finer-grained analysis is warranted. If an outcome-optimal policy results in wide disparities in access, for example in marginalized populations not taking up incentives for healthy food due to lack of access in food deserts, or administrative burden that screens out individuals applying for social services that could benefit the most, this could be a serious concern for decision-makers. We ultimately may seek optimal decision rules that improve disparities in treatment access. In contrast, previous work in algorithmic accountability primarily focuses on auditing _recommendations_, but not both the access and efficacy achieved under the final decision rule. Therefore, previous methods can fall short in mitigating potential disparities.

Our contributions are as follows: we characterize optimal and resource fairness-constrained optimal decision rules, develop statistically improved estimators and robustness checks for the setting of algorithmic recommendations with sufficiently randomized decisions. We also develop methodology for optimizing over a constrained policy class with less conservative out-of-sample fairness constraint satisfaction by a two-stage procedure, and we provide sample complexity bounds. We assess improved recommendation rules in a stylized case study of optimizing recommendation of supervised release in the PSA-DMF pretrial risk-assessment tool while reducing surveillance disparities.

## 2 Related Work

In the main text, we briefly highlight the most relevant methodological and substantive work and defer additional discussion to the appendix.

**Optimal encouragement designs/policy learning with constraints.** There is extensive literature on off-policy evaluation and learning, empirical welfare maximization, and optimal treatment regimes [5; 49; 35; 30].  studies an optimal individualized encouragement design, though their focus is on optimal individualized treatment regimes with instrumental variables.  study fairness in pricing, and some of the desiderata in that setting on revenue (here, marginal welfare) and demand (take-up) are again relevant here, but in a more general setting beyond pricing. The most closely related work in terms of problem setup is the formulation of "optimal encouragement designs" in . However, they focus on knapsack resource constraints, which have a different solution structure than fairness constraints. Their outcome models in regression adjustment are conditional on the recommended/not recommended partitions which would not allow our fairness constraints that introduce treatment- and group-dependent costs.  has studied uniform feasibility in constrained resource allocation, but without encouragement or fairness.  studies robust extrapolation in policy learning from algorithmic recommendation, but not fairness. Our later case study is on supervised release, where there is a lot of randomness in final treatment decisions, rather than pretrial detention.

**Fair off-policy learning** We highlight some most closely related works in off-policy learning (omitting works in the sequential setting).  studies high-probability fairness constraint satisfaction.  studies doubly-robust causal fair classification, while others have imposed deterministic resource constraints on the optimal policy formulation .  studies (robust) bounds for treatment responders in binary outcome settings; this desiderata is coupled to classification notions of direct treatment. Again, our focus is on modeling the fairness implications of non-adherence. Indeed, in order to provide general algorithms and methods, we do build on prior fair classification literature. A different line of work studies "counterfactual" risk assessments which models a different concern.

**Other causal methodology for intention-to-treat** We focus on deriving estimators for intention-to-treat analyses in view of fairness constraints (which result in group-specific welfare weights). Our interest is in imposing separate desiderata on treatment realizations under non-compliance; but we don't conduct instrumental variable inference and we assume unconfoundedness holds. We argue ITT is policy-relevant whereas complier-strata specific analysis is less policy-relevant since the compliers are unknown. Since our primary interest is in characterizing fair optimal decision rules, we don't model this as a mediation analysis problem (isolating the impact of recommendation even under the same ultimate treatment in a nested counterfactual), which may be more relevant for descriptive characterization.  studies multi-objective desiderata for pricing and notes intention-to-treat structure in pricing, but not fairness considerations in more general problems. A related literature studies principal stratification , which has similar policy-relevance disadvantages regarding interpretability as complier analysis does.

## 3 Problem Setup

We briefly describe the problem setup. We work in the Neyman-Rubin potential outcomes framework for causal inference . We define the following:

* recommendation flag \(R\{0,1\}\), where \(R=1\) means encouraged/recommended. (We will use the terms encouragement/recommendation interchangeably).
* treatment \(T(R)\), where \(T(r)=1\) indicates the treatment decision was \(1\) when the recommendation reported \(r\).
* outcome \(Y(t(r))\) is the potential outcome under encouragement \(r\) and treatment \(t\).

Regarding fairness, we will be concerned about disparities in utility and treatment benefits (resources or burdens) across different groups, denoted \(A\{a,b\}\). (For notational brevity, we may generically discuss identification/estimation without additionally conditioning on the protected attribute). For example, recommendations arise from binary high-risk/low-risk labels of classifiers. In practice, in consequential domains, classifier decisions are rarely automated, rather used to inform humans in the loop. The human expert in the loop decides whether or not to assign treatment. For binary outcomes, we will interpret \(Y(t(r))=1\) as the positive outcome, and when treatments are also binary, we may further develop analogues of fair classification criteria. We let \(c(r,t,y)\{0,1\}^{3}\) denote the cost function for \(r\{0,1\},t,y\{0,1\}\), which may sometimes be abbreviated \(c_{rt}(y)\). We discuss identification and estimation based on the following recommendation, treatment propensity, and outcome models:

\[e_{r}(X,A) P(R=r X,A),\ \ p_{t|r}(X,A) P(T=t  R=r,X,A),\] \[_{r,t}(X,A)[c_{rt}(Y) R=r,T=t,X,A]= [c_{rt}(Y) T=t,X,A]_{t}(X,A)\]

We are generally instead interested in _personalized recommendation rules_\((r X)=_{r}(X)\) which describes the probability of assigning the recommendation \(r\) to covariates \(X\). The average encouragement effect is the difference in average outcomes if we refer everyone vs. no one, while the encouragement policy value \(V()\) is the population expectation induced by the potential outcomes and treatment assignments realized under a recommendation policy \(\).

\[AEE=[Y(T(1))-Y(T(0))], V()=[c(,T(),Y())].\]

Because algorithmic decision makers may be differentially responsive to recommendation, and treatment effects may be heterogeneous, the optimal recommendation rule may differ from the (infeasible) optimal treatment rule when taking constraints into account or for simpler policy classes.

**Assumption 1** (Consistency and SUTVA ).: \(Y_{i}=Y_{i}(T_{i}(R_{i})).\)__

**Assumption 2** (Conditional exclusion restriction).: \(Y(T(R))\!\!\! R T,X,A\)_._

**Assumption 3** (Unconfoundedness).: \(Y(T(r))\!\!\! T(r) X,A\)_._

**Assumption 4** (Stable responsivities under new recommendations).: \(P(T=t R=r,X)\) remains fixed from the observational to the future dataset.__

**Assumption 5** (Decomposable costs).: \(c(r,t,y)=c_{r}(r)+c_{t}(t)+c_{y}(y)\)__

**Assumption 6** (Overlap).: \(_{r} e_{r}(X,A) 1-_{r};\ \ _{t} p_{t|r}(X,A) 1-_{t}; _{r},_{t} 0\)__

Our key assumption beyond standard causal inference assumptions is the conditional exclusion restriction assumption 2, i.e. that conditional on observable information \(X\), the recommendation has no causal effect on the outcome beyond its effect on increasing treatment probability. This assumes that all of the covariate information that is informative of downstream outcomes is measured. Although this may not exactly hold in all applications, stating this assumption is also a starting point for sensitivity analysis under violations of it . Assuming assumption 6 is like assuming we consider a randomized controlled trial with nonadherence. But later we give arguments using robustness to go beyond this, leveraging our finer-grained characterization.

Assumption 4 is a structural assumption that limits our method to most appropriately re-optimize over small changes to existing algorithmic recommendations. This is also required for the validity of intention-to-treat analyses. For example, \(p_{0|1}(x)\) (disagreement with algorithmic recommendation) could be a baseline algorithmic aversion. Not all settings are appropriate for this assumption. We don't assume micro-foundations on how or why human decision-makers were deviating from algorithmic recommendations, but take these patterns as given. One possibility for relaxing this assumption is via conducting sensitivity analysis, i.e. optimizing over unknown responsivity probabilities near known ones.

Later on, we will be particularly interested in constrained formulations on the intention-to-treat effect that impose separate desiderata on outcomes under treatment, as well as treatment.

## 4 Method

We consider two settings: in the first, \(R\) is (as-if) randomized and satisfies overlap. Then \(R\) can be interpreted as intention to treat or prescription, whereas \(T\) is the actual realization thereof. We study identification of optimal encouragement designs with potential constraints on treatment or outcome utility patterns by group membership. We characterize optimal unconstrained/constrained decisions under resource parity. In the second, \(R\) is an algorithmic recommendation that does not satisfy overlap in recommendation (but there is sufficient randomness in human decisions to satisfy overlap in treatment): we derive robustness checks in this setting by being robust. First we discuss causal identification in optimal encouragement designs.

**Proposition 1** (Regression adjustment identification).: \[[c(,T(),Y())]=_{t,r\{0,1\}} [_{r}(X)_{t}(X)p_{t|r}(X)]\]

Proof of Proposition 1.: \[[c(,T(),Y())] =_{t,r\{0,1\}}[_{r}(X)[[T(r)=t]c_{rt}(Y(r,t)) R=r,X]]\] \[=_{t,r\{0,1\}}[_{r}(X)P(T=t  R=r,X)[c_{rt}(Y(r,t)) R=r,X]]\] \[=_{t,r\{0,1\}}[_{r}(X)P(T=t  R=r,X)[c_{rt}(Y) T=t,X]]\]

where the last line follows by the conditional exclusion restriction (Assumption 2) and consistency (Assumption 1). 

Resource-parity constrained optimal decision rulesWe consider a resource/burden parity fairness constraint:

\[V_{}^{*}=_{}\;\{[c(,T(),Y())][T() A=a]-[T() A=b]\}\] (1)

Enforcing absolute values, etc. follows in the standard way. Not all values of \(\) may be feasible; in the appendix we give an auxiliary program to compute feasible ranges of \(\). We first characterize a threshold solution when the policy class is unconstrained.

**Proposition 2** (Threshold solutions).: Define

\[^{*}_{}\;[L(,X,A)+],\;\;^{*}(x,u )=\{L(^{*},X,u)>0\}\]

If instead \(d(x)\) is a function of covariates \(x\) only,

\[^{*}_{}\;[[L(,X,A) X ]_{+}],\;\;^{*}(x)=\{[L(^{*},X,A) X]>0\}\]

Establishing this threshold structure (follows by duality of infinite-dimensional linear programming) allows us to provide a generalization bound argument.

### Generalization

**Proposition 3** (Policy value generalization).: Assume the nuisance models \(=[p_{1|0},p_{1|1},_{1},_{0}]^{}, H\) are consistent and well-specified with finite VC-dimension \(V_{}\) over the product function class \(H\). Let \(=\{\{[L(,X,A;) X]>0 ;\}\).

\[_{,}|(_{n}[ L(,X,A)]- [ L(,X,A)])|=O_{p}(n^{-})\]

This bound is stated for known nuisance functions: verifying stability under estimated nuisance functions further requires rate conditions.

Doubly-robust estimationWe may improve statistical properties of estimation by developing _doubly robust_ estimators which can achieve faster statistical convergence when both the probability of recommendation assignment (when it is random), and the probability of outcome are consistently estimated; or otherwise protect against misspecification of either model. We first consider the ideal setting when algorithmic recommendations are randomized so that \(e_{r}(X)=P(R=r X)\).

**Proposition 4** (Variance-reduced estimation).: \[V() =_{t,r\{0,1\}}[_{r}(X) \{[R=r]}{e_{r}(X)}([T=t]c_{r1}(Y)-_{1}(X)p_{ t|r}(X))+_{1}(X)p_{t|r}(X)\}]\] \[[T()] =_{r\{0,1\}}[_{r}(X)\{[R=r]}{e_{r}(X)}(T(r)-p_{1|r}(x))+p_{1|r}(x)\}]\]

Although similar characterization appears in  for the doubly-robust policy value alone, note that doubly-robust versions of the constraints we study would result in differences in the Lagrangian so we retain the full expression rather than simplifying. For example, for regression adjustment, Proposition 9 provides interpretability on how constraints affect the optimal decision rule. In the appendix we provide additional results describing extensions of Proposition 8 with improved estimation.

### Robust estimation with treatment overlap but not recommendation overlap

When recommendations are e.g. the high-risk/low-risk labels from binary classifiers, we may not satisfy the overlap assumption, since algorithmic recommendations are deterministic functions of covariates. However, note that identification in Proposition 1 requires only SUTVA and consistency, and the exclusion restriction assumption. Additional assumptions may be required to extrapolate \(p_{t|r}(X)\) beyond regions of common support. On the other hand, supposing that positivity held with respect to \(T\) given covariates \(X\), given unconfoundedness, our finer-grained approach can be beneficial because we only require robust extrapolation of \(p_{t|r}(X),\) response to recommendations, rather than the outcome models \(_{t}(X).\)

We first describe what can be done if we allow ourselves parametric extrapolation on \(p_{1|1}(X)\), treatment responsivity. In the case study later on, the support of \(X R=1\) is a superset of the support of \(X R=0\) in the observational data. Given this, we derive the following alternative identification based on marginal control variates (where \(p_{t}=P(T=t X)\) marginalizes over the distribution of \(R\) in the observational data):

**Proposition 5** (Control variate for alternative identification ).: Assume that \(Y(T(r)) T(r) R=r,X\).

\[V()=_{t,r\{0,1\}}[\{c_{rt}(Y(t)) [T=t]}{p_{t}(X)}+(1-[T=t]}{p_{t}(X)} )_{t}(X)\}p_{t|r}(X)]\]

Robust extrapolation of \(p_{t|r}(X)\)Let \(^{}=\{x:P(R=1 X)=0\}\) denote the no overlap region; on this region there are no joint observations of \((t,r,x)\). We consider uncertainty sets for ambiguous treatment recommendation probabilities. For example, one plausible structural assumption is _monotonicity_, that is, making an algorithmic recommendation can only increase the probability of being treated.

We define the following uncertainty set:

\[_{q_{|r}}\{q_{1|r}(x^{}) q_{1|r}(x) p_{1 |r}(x),\  x^{},r\ _{t}q_{t|r}(x)=1, x,r\}\]

We could assume uniform bounds on unknown probabilities, or more refined bounds, such as Lipschitz-smoothness with respect to some distance metric \(d\), or boundedness.

\[_{}\{q_{1|r}(x^{}) d(q_{1|r}(x ^{}),p_{1|r}(x)) Ld(x^{},x),\ (x^{},x)(^{}^{}) \},_{}\{q_{1|r}(x^{}) (x) q_{1|r}(x^{})(x)\}\]

Define \(V_{ov}()_{t,r\{0,1\}}[(r X) p_{t|r}(X)_{t}(X)_{ov}]\). Let \(\) denote the uncertainty set including any custom constraints, e.g. \(=_{q_{1|r}}_{}\). For brevity we use \(_{no}\) to denote \([X^{}]\), \(_{ov}\) to denote \([X^{}]\). Then we may obtain robust bounds by optimizing over regions of no overlap:

\[() V_{ov}()+_{no}(),\ \ _{no}() _{q_{tr}(X)}\{_{t,r\{0, 1\}}[(r X)_{t}(X)q_{tr}(X)_{no}]]\}\]

In the specialized, but practically relevant case of binary outcomes/treatments/recommendations, we obtain the following simplifications for bounds on the policy value, and the minimax robust policy that optimizes the worst-case overlap extrapolation function. In the special case of constant uniform bounds, it is equivalent (in the case of binary outcomes) to consider marginalizations:

**Lemma 1** (Binary outcomes, constant bound).: _Let \(_{}\{q_{t|r}(x^{}) q_{1|r}(x^{})\}\) and \(=_{q_{t|r}}_{}\). Define \(_{t|r}(a)[q_{t|r}(X,A) T=t,A=a]\). If \(T\{0,1\},\)_

\[_{no}()=_{t,r\{0,1\}}[c_{rt}^{*} _{t|r}[Y(r X) T=t]_{no}]\]

_where \(c_{rt}^{*}=[t=1]+ [t=0]&[Y(r X) T=t] 0\\ [t=0]+[t=1]& [Y(r X) T=t]<0\)_

We state the next result for simple uncertainty sets, like intervals, to deduce insights about the robust policy. In the appendix we include a more general reformulation for polytopic uncertainty sets.

**Proposition 6** (Robust linear program ).: Suppose \(R,T\{0,1\},\) and \(q_{r1}(,u)_{}, r,u\). Define

\[(x,a)=_{1}(x,a)-_{0}(x,a),\ \  B_{r}(x,u)=( _{r}(x,u)-_{r}(x,u)),B_{r}^{}(x,u)= _{r}(x,u)+ B_{r}(x,u),\] \[[_{ov}T()]=[T()_{ov} A =a]-[T()_{ov} A=b],\ \ c_{1}()=_{r}[_{r}_{r}B^{}]\]

Then the robust linear program is:

\[ V_{ov}()+[_{0}]+c_{1}()-_{r} [||\,(r X) B_{r}(X,A)_{no}]\] \[\ _{r}\{[(r X)_{r}(X,A) _{no} A=a]-[_{r}_{r}(X,A)_ {no} A=b]\}+_{ov}^{T}()\]

## 5 Additional fairness constraints and policy optimization

We previously discussed policy optimization, over unrestricted decision rules, given estimates. We now introduce general methodology to handle 1) optimization over a policy class of restricted functional form and 2) more general fairness constraints. We first introduce the fair-classification algorithm of , describe our extensions to obtain variance-sensitive regret bounds and less conservative policy optimization (inspired by a regularized ERM argument given in ), and then provide sample complexity analysis.

Algorithm and setupIn the following, to be consistent with standard form for linear programs, note that we consider outcomes \(Y\) to be costs so that we can phrase the saddle-point as minimization-maximization. Consider \(||\) linear constraints and \(J\) groups (values of protected attribute \(A\)), with coefficient matrix \(M^{K J}\), the constraint moment function \(h_{j}(),j[J]\) (with \(J\) the number of groups), \(O=(X,A,R,T,Y)\) denoting our data observations, and \(d\) the constraint constant vector:

\[h_{j}()=[g_{j}(O,(X))_{j}]j J,\ \ Mb() d\]

Importantly, \(g_{j}\) depends on \(\) while the conditioning event \(_{j}\) cannot depend on \(\). Many important fairness constraints can nonetheless be written in this framework, such as burden/resource parity,parity in true positive rates, but not measures such as calibration whose conditioning event does depend on \(\). (See Appendix B.2 for examples omitted for brevity). We further consider a convexification of \(\) via randomized policies \(Q()\), where \(()\) is the set of distributions over \(\), i.e. a randomized classifier that samples a policy \( Q\). Therefore we solve

\[_{Q()}\{V()\;\;Mh() d\}\]

On the other hand, the optimization is solved using sampled moments, so that we ought to interpret the constraint values \(_{k}=d_{k}+_{k},\) for all \(k\). The overall algorithmic scheme is similar: we seek an approximate saddle point so that the constrained solution is equivalent to the Lagrangian,

\[L(Q,)=(Q)+^{}(M(Q)-),_{Q ()}\{V() Mh() d\}=_{Q()}_{ ^{K}_{+}}L(Q,).\]

We simultaneously solve for an approximate saddle point and bound the domain of \(\) by \(B\):

\[_{Q}_{^{|X|}_{+},\|\|_{1} B} L(Q,),_{^{|X|}_{+},\|\|_{1} B} _{Q}L(Q,)\]

We play a no-regret (second-order multiplicative weights [11; 43], a slight variant of Hedge/exponentiated gradient ) algorithm for the \(-\)player, while using best-response oracles for the \(Q-\)player. Full details are in Algorithm 1. Given \(_{t},\)\(_{}(_{t})\) computes a best response over \(Q\); since the worst-case distribution will place all its weight on one classifier, this step can be implemented by a reduction to cost-sensitive/weighted classification [10; 49], which we describe in further detail below. Computing the best response over \(_{}(_{t}))\) selects the most violated constraint. We include further details in Appendix B.2.

```
1:Input: \(=\{(X_{i},R_{i},T_{i},Y_{i},A_{i})\}_{i=1}^{n}\), \(g,,M,,\) B, \(,\)\(,\)\(_{1}=0^{||}\)
2:for\(t=1,2,\)do
3: Set \(_{t,k}=B\}}{1+_{t^{}} \{_{k^{}}\}}\) for all \(k,_{t}_{}(_{t} ),_{t}_{t^{}=1}^{t}_{t^{ }}\) \( L(_{t},_{}(_{t})),_{t}_{t^{}=1}^{t}_{t^{}}, (_{}(_{t}), _{t})\),
5:\(_{t}\{L(_{t},_{t})-,\;\; -L(_{t},_{t})\}\), If \(_{t}\) then return \((_{t},_{t})\)
6:\(_{t+1,i}=_{t}+(1-(M(h_{t})-)_ {j}), i\)
7:endfor ```

**Algorithm 1** MW2REDFAIR\((,g,,M,d)\)

**Weighted classification reduction.** There is a well-known reduction of optimizing the zero-one loss for policy learning to weighted classification. Taking the Lagrangian will introduce datapoint-dependent additional weights. This reduction requires \(\{-1,+1\},T\{-1,+1\}\) (for notational convenience alone). We consider parameterized policy classes so that \((x)=(g_{}(x))\) for some index function \(g\) depending on a parameter \(^{d}\). Consider the centered regret \(J()=[Y()]-[[Y R=1,X]+ [Y R=0,X]]\). Then \(J()=J((g_{}()))=[(g_{} (X))\{\}]\) where \(\) can be one of, where \(_{r}^{R}(X)=[Y R=r,X]\),

\[_{DM}=(p_{1|1}(X)-p_{1|0}(X))(_{1}(X)-_{0}(X)),_{IPW}=(X)},_{DR}=_{DM}+_{IPW}+^{R}(X)}{ _{R}(X)}\]

We can apply the standard reduction to cost-sensitive classification since \(_{i}\,(g_{}(X_{i}))=|_{i}|(1-2[ (g_{}(X_{i}))(_{i})])\). Then we can use surrogate losses for the zero-one loss. Although many functional forms for \(()\) are Fisher-consistent, one such choice of \(\) is the logistic (cross-entropy) loss given below:

\[L()=[||\,(g_{}(X),())], l(g,s )=2(1+(g))-(s+1)\] (2)

**Two-stage variance-constrained algorithm.** We seek to improve upon this procedure so that we may obtain regret bounds on policy value and fairness constraint violation that exhibit more favorable dependence on the maximal variance over small-variance _slices_ near the optimal policy, rather than worst-case constants over all policies, [12; 5]. Further, note that the algorithm sets the constraint feasibility slacks via generalization bounds that previously depended on these worst-case constants.

These challenges motivate the two-stage procedure, described formally in Algorithm 2 and verbally here. We adapt an out-of-sample regularization scheme developed in , which recovers variance-sensitive regret bounds via a small modification to ERM. We split the data, learn nuisance estimators \(_{1}\) for use in our policy value and constraint estimates, and run Algorithm 1 (\((_{1},h,,M,d;_{1})\)) to obtain an estimate of the optimal policy \(_{1}\), and the constraint variances at \(_{1}\). Next, we _augment_ the constraint matrix with additional constraints that require the second-stage \(\) to achieve \(_{n}\) close policy value and constraint moment values relative to \(_{1}\). Since errors concentrate fast, this can be viewed as variance regularization. And, we set the constraint slack \(\) in the second stage using estimated variance constants from \(_{1}\), which will be less conservative than using worst-case bounds. Define

\[v^{()}(Q)=_{ Q}[_{()} O],()\{,\},g_{j}(O;Q)=_{ Q}[g_{j}(O; ) O,_{j}],\]

so that \(V^{()}(Q)=[v^{()}(Q)]\) and \(h_{j}(Q)=[g_{j}(O;Q)_{j}]\). Define the function classes

\[_{}=\{v_{DR}(,;), H\}, _{j}=\{g(,;), H\}\]

and the empirical entropy integral \((r,)=_{ 0}\{4+10_{}^{r}_{2}(,,n)}{n}}d\}\) where \(H_{2}(,,n)\) is the \(L_{2}\) empirical entropy, i.e. log of the \(\|\|_{2}\)\(\)-covering number. We make a mild assumption of a learnable function class (bounded entropy integral) .

**Assumption 7**.: The function classes \(_{},\{_{j}\}_{j}\) satisfy that for any constant \(r,(r,) 0\) as \(n\). The function classes \(\{_{j}\}_{j}\) comprise of \(L_{j}\)-Lipschitz contractions of \(\).

We will assume that we are using doubly-robust/orthogonalized estimation as in proposition 4, hence state results depending on estimation error of nuisance vector \(\).

**Theorem 1** (Variance-Based Oracle Policy Regret).: _Suppose that the mean-squared-error of the nuisance estimates is upper bounded w.p. \(1-/2\) by \(_{n,}^{2}\), over the randomness of the nuisance sample: \(_{\{[(_{l}-_{l})^{2}]\}_{l[L]}}_{ n}^{2}\). Let \(r=_{Q}[v_{DR}(z;)^{2}]}\) and \(_{n}=((r,_{})+r})\). Moreover, let_

\[_{*}()=\{Q[]:V(Q_{*}^{0})-V(Q),\;\;h(Q_{*}^{0})-h(Q) d+\}\]

_denote an \(\)-regret slice of the policy space. Let \(_{n}=O(_{n}+_{n,}^{2})\) and_

\[V_{2}^{0}=\{(v_{DR}^{0}(Q;Q)-v_{DR}^{0}(Q;Q^{ })):Q,Q^{}_{*}(_{n})\}\]

_denote the variance of the difference between any two policies in an \(_{n}\)-regret slice, evaluated at the true nuisance quantities. (Define \(V_{2}^{j}\) analogously for the variance of constraint moments). Then, letting \((Q):=Mh(Q)\) denote the constraint values, the policy distribution \(Q_{2}\) returned by the out-of-sample regularized ERM, satisfies w.p. \(1-\) over the randomness of \(S\) :_

\[V()-V(Q^{*}) =O((^{obj}},(_{}))+n^ {-}^{obj}(3/)}+_{n,}^{2})\] \[(_{j}()-c_{j})-(_{j}(Q^{*})-c_{j}) =O((^{j}},(_{j}))+n^{- }^{j}(3/)}+_{n,}^{2})\]The benefits are that 1) the constants are improved from an absolute, structure-agnostic bound to depending on the variance of low-regret policies, which also reflects improved variance from using doubly-robust estimation as in proposition 4, and 2) less-conservative out-of-sample fairness constraint satisfaction.

## 6 Experiments

Due to space constraints, in the main text we only present a case study based on the PSA-DMF for supervised release . In the appendix we include additional experiments and robustness checks, including a case study of fully-randomized recommendations and non-adherence.

**PSA-DMF case study.** Our case study is on a dataset of judicial decisions on _supervised_ release based on risk-score-informed recommendations . The PSA-DMF (Public Safety Assessment Decision Making Framework) uses a prediction of failure to appear for a future court data to inform pretrial decisions, including our focus on supervised release (i.e. electronic monitoring in addition to release) where judges make the final decision. Despite a large literature on algorithmic fairness of pretrial risk assessment, to the best of our knowledge, recommendations about supervised release are not informed by empirical evidence. There are current policy concerns about disparities in increasing use of supervised release given mixed evidence on outcomes ; e.g. Safety and Justice Challenge  concludes "targeted efforts to reduce racial disparities are necessary". First, we acknowledge data issues. We work with publicly available data that was discretized for privacy . The final supervision decision does not include intensity, but different intensities are recommended in the data, which we collapse into a single level. The PSA-DMF is an algorithmic recommendation so here we are appealing to overlap in treatment recommendations, but using parametric extrapolation in responsivity. Finally, unconfoundedness is likely untrue, but sensitivity analysis could address this. So, this analysis should be considered exploratory, to illustrate the relevance of the methods. Future work will seek proprietary data for a higher-fidelity empirical study.

Next in Figure 1 we provide descriptive information illustrating heterogeneity (including by protected attribute) in adherence and effectiveness. We observe wide variation in judges assigning supervised release beyond the recommendation. We use logistic regression to estimate outcome models and treatment response models. The first figure shows estimates of the causal effect for different groups, by gender (similar heterogeneity for race). The outcome is failure to appear, so negative scores are beneficial. The second figure illustrates the difference in responsiveness: how much more likely decision-makers are to assign treatment when there is vs. isn't an algorithmic recommendation to do so. The last figure plots a logistic regression of the lift in responsiveness on the causal effect \(_{1}(x,a)-_{0}(x,a)\). We observe disparities in how responsive decision-makers are conditional on the same treatment effect efficacy. This is importantly not a claim of minus because decision-makers didn't have access to causal effect estimates. Nonetheless, disparities persist.

In Figure 2 we highlight results from constrained policy optimization. The first two plots in each set illustrate the objective function value and \(A=a\) average treatment cost, respectively; for \(A\) being race (nonwhite/white) or gender (female/male), respectively. We use costs of \(100\) for \(Y=1\) (failure to appear, \(0\) for \(Y=0\), and \(20\) when \(T=1\) (set arbitrarily). On the x-axis we plot the penalty \(\) that we use to assess the solutions of Proposition 9. The vertical dashed line indicates the solution achieving \(=0\), i.e. parity in treatment take-up. Near-optimal policies that reduce treatment disparity can be of interest due to advocacy concerns about how the expansion of supervised release could increase the surveillance of already surveillance-burdened marginalized populations. We see that indeed, for race, surveillance-parity constrained policies can substantially reduce disparities for nonwhites while not increasing surveillance on whites that much: the red line decreases significantly at low increase to the blue line (and low increases to the objective value). On the other hand, for gender, the opportunity for improvement in surveillance disparity is much smaller. See the appendix for further experiments and computational details.