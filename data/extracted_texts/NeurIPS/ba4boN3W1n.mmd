# Lie Point Symmetry and Physics Informed Networks

Tara Akhound-Sadegh

School of Computer Science, McGill University,

Mila - Quebec Artificial Intelligence Institute,

Montreal, Quebec, Canada

Laurence Perreault-Levasseur

Universite de Montreal, Montreal, Quebec, Canada

Ciela Institute, Montreal, Quebec, Canada

Mila - Quebec Artificial Intelligence Institute, Montreal, Quebec, Canada

Trottier Space Institute, Montreal, Quebec, Canada

CCA, Flatiron Institute, New York, USA

Perimeter Institute, Waterloo, Ontario, Canada

Johannes Brandstetter

Microsoft Research AI4Science,

Amsterdam, Netherlands

Max Welling
This is mainly driven by the fact that traditional handcrafted numerical solvers can be prohibitively expensive, and thus, learning to solve PDEs has the potential to significantly impact various areas of science, ranging from quantum chemistry to biology to climate science to cosmology, (Wang et al., 2020a; Kochkov et al., 2021; Kashinath et al., 2021; Gupta and Brandstetter, 2022; Nguyen et al., 2023; Bi et al., 2022). Like any other machine learning problem, learning to solve PDEs can benefit from inductive biases to boost sample efficiency and generalization capabilities. As such, the PDE itself and its respective Lie point symmetries, i.e. joint symmetries of coordinate and field transformations, are two natural inductive biases for constructing efficient neural PDE solvers.

For parametric PDEs, Physics-Informed Neural Networks (PINNs) have emerged as an efficient new learning paradigm. PINNs constrain the solution network to satisfy the underlying PDE via the PINN loss terms (Raissi et al., 2019), which comprise a residual loss, as well as loss terms for initial and boundary points. As such, PINNs can be seen as a data-free learning approach, utilizing available physics information to guide neural network training. Lie point symmetries are uniquely special in that it is possible to characterize the full set of permissible transformations for each PDE, and thus naturally offer additional physics information which piggy-backs the data-free learning objective. Lie point symmetries have been introduced to deep learning by Brandstetter et al. (2022) for Lie point symmetry data augmentation which places a firm mathematical footing on data augmentation pipelines for neural PDE solvers. Further, Lie point symmetries showed promising results when used in extending the framework to self-supervised learning for neural PDE solvers (Mialon et al., 2023). In this work, for the first time we integrate symmetries into PINNs and demonstrate the effectiveness of this symmetry regularization on generalization capabilities.

Lie point symmetries of a PDE, by definition, map a solution to a solution, preserving the PDE. Lie point symmetry transformations act on both independent (_e.g._, time and space), and dependent (PDE solution) fields and, by extension, on all partial derivatives. To enforce a Lie point symmetry, we first need to _prolong_ a symmetry transformation to find how it transforms the partial derivatives of the respective PDE. We show how to do this using automatic differentiation, which enables us to transform the entire PDE for each infinitesimal generator of each Lie point symmetry. "Imposing a symmetry" means that by transforming the PDE via a symmetry transformation, the PDE is required to be preserved. In practical terms, Lie point symmetries should be orthogonal to the PDE gradient. Each Lie Point symmetry results in one such orthogonality constraint, which we enforce using a penalty term within the PINN framework. We can enforce these on the same points where the PINN loss is imposed. Importantly, our Lie Point symmetry regularization is complementary to the PINN loss: while the PDE loss regularizes the neural network to satisfy the PDE at select points, the Lie point symmetry loss regularizes the neural network such that infinitesimal changes in different directions continue to satisfy the underlying PDE.

## 2 Background

### Partial Differential Equations

Partial differential equations (PDEs) are used to mathematically describe the dynamics of various physical systems. In a PDE, the evolution of a function that involves several variables is described in terms of local updates expressed by partial derivatives. Since temporal PDEs are especially prevalent

Figure 1: All the solutions of the differential equation of a harmonic oscillator can be reached via symmetry transformations of a single solution. Its Lie-point symmetry group is the special linear group, \(SL(3)\), corresponding to eight one-parameter subgroups transforming a given solution, identified using a different colour in this figure. While, in general, Lie-point symmetries form more than one orbit – that is, we cannot reach all solutions using such transformations – they can significantly narrow down the solution space.

in physical sciences, we consider PDEs of the following general form:

\[=_{t}+_{}[] =0, t[0,T],\] (1) \[(0,) =f(), \] \[(t,) =g(t,), ,t[0,T]\]

where \((t,)^{D_{u}}\) is the solution to the PDE, \(t\) denotes the time, and \(\) is a vector of possibly multiple spatial coordinates. \(^{D}\) is the domain and \(_{}[]\) is a non-linear _differential operator_. \(f()\) is known as the initial condition function and \(g(t,)\) describes the boundary conditions. In the following, we sometimes explicitly separate the time from other independent variables and sometimes for convenience group them together.

**Example 1** (Heat Equation).: _The one-dimensional heat equation describes the heat conduction in a one-dimensional rod, with viscosity coefficient \(\). Its differential operator is given by \(_{x}[u]=- u_{xx}\)._

Using Neural Networks for Solving PDEs.For many systems, obtaining an analytical solution to the PDE is impossible; hence, numerical methods are traditionally used to obtain approximate solutions. Numerical solvers, such as finite element methods (FEM) or finite difference methods (FDM) rely on discretizing the space \(T\)(Quarteroni, 2009). The topology of the space has to be taken into account when constructing the mesh, and the resolution of the discretization will affect the accuracy of the predicted solutions. Additionally, these solvers are often computationally expensive, especially for complex dynamics, and each time the initial or boundary conditions of the PDE change the solver must be rerun. These considerations and constraints make designing numerical solvers difficult, and scientists often need to handcraft a specific solver for an application (Quarteroni, 2009).

Given the recent successes of neural networks, especially in dealing with large datasets, using deep learning to solve PDEs has become a promising direction. The idea is to learn a function or, more generally, a functional that bypasses the numerical computation and produces the solution to the PDE in a single shot or by progression through time. Broadly, there are two approaches to solving PDEs with neural networks: neural operator methods which approximate the solution operator that maps between solutions of the underlying PDEs, and direct methods which learn the underlying solution function. The most prominent representative of the latter are PINNs.

PINNs.In contrast to neural operator methods (Lu et al., 2021; Li et al., 2021a) where the main idea is to generalize neural networks to obtain mappings between function space, PINNs directly learn the solution function of the underlying PDE. Consequently instead of relying on large training sets,

PINNs operate as a surrogate model for the PDE solution, trained directly with the PDE equation itself. The simplicity of the PINN idea has made it the subject of many follow-up improvements; see Krishnapriyan et al. (2021); Wang et al. (2020, 2022, 2023)

In PINNs, the PDE solution \(u(t,)\) of Eq. (1) is a neural network \(u_{}(t,)\) with parameters \(\). The loss function is then compromised of two parts:

\[()=_{}+_{}\] (2)

The first term is the _physics-informed_ objective, ensuring that the function learned by the neural network satisfies the PDE Eq. (1). Let \(r(t,)\) denote the residual error in agreement with the PDE equation:

\[r_{}(t,)=u_{}(t,)+ _{}[u_{}(t,)]\]

where the derivatives of the solution network \(u_{}\) are calculated using automatic differentiation (Raissi et al., 2019). This penalty is then imposed on a finite set of points \((t,)_{1:N_{r}}\) which are sampled from inside the domain \([0,T]\) to obtain the _PDE loss_:

\[_{}=}_{i=1}^{N_{r}}\|r_{}(t_{i},_{i})\|_{2}^{2}\] (3)

The second term in Eq. (2), is a supervised loss which ensures that the function learned by the neural network satisfies the initial and boundary conditions of the problem - that is

\[_{}=}_{i=1}^{N_{s}}\|u_{}(0,_{i}^{0})-f(x_{i}^{0})\|_{2}^{2}+}_{i=1}^{N_{b}}\| u_{}(t_{i}^{b},_{i}^{b})-g(t_{i}^{b},x_{i}^{b})\|_{2}^{2}\] (4)where \((^{0})_{1:N_{s}}\) are \(N_{s}\) samples at which the initial condition function \(f\) is sampled. \((t^{b},^{b})_{1:N_{b}}\) are \(N_{b}\) points sampled on the boundary (from \([0,T]\)).

There have also been multiple models that combine the operator learning approach with the physics-informed loss. For example, in Li et al. (2021), the NO approach is combined with the PINN loss. Another example is Wang et al. (2021), which we describe further in Section 3.1.

### Symmetries

Groups.Symmetries are essentially transformations of the object that leave an aspect of it invariant: for example, the nature of an object does not change if we translate or rotate it. In mathematics, symmetries of an object are described by abstract objects known as _groups_. More concretely, a group is a set \(G\) and a group operation \(:G G G\) satisfying: associativity, the existence of an identity element, and the presence of an inverse element for every element in the set.

Lie Groups.Groups that are also differentiable manifolds are known as _Lie groups_, and are important in the study of continuous symmetries. In Lie groups, in addition to satisfying the three properties, the group operation \(\) and its inverse are smooth maps. Each Lie group has an associated _Lie algebra_ which is its tangent space, as a vector space, at identity. 3 Intuitively, Lie algebras describe the smooth transformations of the Lie groups in the limit of infinitesimal transformations. The elements of the Lie algebra are vectors describing the direction of the infinitesimal symmetry transformation.

One Parameter Subgroups.The Lie group, \(\), can be multi-dimensional and complex. When the \(n\)-dimensional group \(\) is _simply connected_, it is often represented in terms of a series of \(n\)_one-parameter transformations_, \(g=g_{1}(_{1})g_{2}(_{2}) g_{n}(_{n})\), where \(g_{i}:\), \(i=1,,n\) and such that \(g_{i}()g_{i}()=g_{i}(+)\). The \(\)'s are the real parameters of the transformation, and each \(g_{i}\) is a continuous group homomorphism (_i.e._, a smooth, group-structured map) from this parameter to the symmetry group.

### Symmetries of Partial Differential Equations

In this section, we will provide the mathematical background of symmetries of differential equations. We will mostly follow the exposition presented in Olver (1986) and refer the reader to this original text for a more in-depth treatment of this topic.

In the context of differential equations, symmetries are transformations that map a solution of the PDE to another solution. For example, in Fig. 1, we can see how the solutions of a simple harmonic oscillator can be obtained via symmetry transformations of a given solution, where the symmetry group is \(SL(3)\). Consider the PDE \(\), involving \(p\) independent variables \(=(x_{1},,x_{p}) X\) and \(q\) dependent variables \(=(u_{1},,u_{q}) U\). The solutions to the PDE will be of the form \(u_{i}=f_{i}(x_{1},,x_{p})\) for \(i=1,,q\), for \(\), where \( X\) is the domain of the function \(\). The symmetry group \(\), of \(\), is the local group of transformations on an open subset of the space of dependent and independent variables, \(M X U\), transforming solutions of \(\) to other solutions.

Prolongations.To formalize this abstract definition of PDE symmetries, Lie proposed viewing \(\) as a concrete geometric object and introduced the concept of _prolongation_(Olver, 1986). The idea is to _prolong_ the space of independent and dependent variables, \(X U\), to a space that also represents the partial derivatives involved in the PDE. More concretely, we have the following definition:

**Definition 1**.: _The **n-th order prolongation** (or n-th order jet space) of \(X U\) is denoted as \(X U^{(n)}=X U_{1} U_{n}\), whose coordinates represent the independent and dependent variables as well as all the partial derivatives of the dependent variables up to order n._

Equivalently we have the notion of _prolongation_ of \(\) as \(^{(n)}=(_{},_{}, ,_{n})\) where \(_{i}\) is all the unique \(i^{}\) derivatives of \(u\), for \(i=1,,n\). For example, if \(=(x,y)\), then \(_{}=(_{xx},_{xy} ,_{yy})\).

Using this notion of prolongation, we can represent a PDE as an algebraic equation, \((,^{(n)})=0\), where \(\) is the map that determines the PDE, _i.e._, \(:X U^{(n)}\). In other words, the PDE tells us where the map \(\) vanishes on \(X U^{(n)}\).

For example, for the heat equation described in 1, \(\) is given by:

\[((x,t),^{(2)})=u_{t}- u_{xx}\;,\] (5)

The graph of all prolonged solutions is the set \(_{} X U^{(n)}\), and is defined as \(_{}=\{(,^{(n)}):(, ^{(n)})=0\}\). In this new notation, we can say that \(()\) is a solution of the PDE if \(_{u}^{(n)}=\{(,^{(n)}())\} _{}\). where \(^{(n)}():X U^{n}\), is a vector-valued function whose entries represent all derivatives of \(\) wrt \(\) up to order \(n\).

Prolongations of the Infinitesimal Generators.Let \(\) be the vector field on the subspace \(M X U\) with corresponding one-parameter subgroup \()}\). In other words, the vector field \(\) is the _infinitesimal generator_ of the one-parameter subgroup. Intuitively, this vector field describes the infinitesimal transformations of the group to the independent and dependent variables, and we can write it as:

\[=_{i=1}^{p}_{i}(,)}+_{=1}^{q}_{}(,) {}{ u^{}}\;,\] (6)

where \(^{i}(,)\) and \(_{}(,)\) are coordinate-dependent coefficients. To study how symmetries transform a solution to another solution, we need to know how they transform the partial derivatives and, therefore, the _jet space_\(X U^{(n)}\).

A symmetry transformation of the independent (\(\)) and dependent (\(\)) variables will also induce transformations in the partial derivatives \(_{},_{},\). The _prolongation of the infinitesimal generator_, \(^{(n)}\) is a generalization of the generator \(\) which describes these induced transformations in these partial derivatives.

This prolongation will be defined on the jet-space \(X U^{(n)}\) and it is given by:

\[^{(n)}=_{i=1}^{p}_{i}(,) {}{ x^{i}}+_{=1}^{q}_{J}_{}^{(J)}( ,)^{}}\;,\] (7)

where we have used the notation \(J=(i_{1},,i_{k})\) for the multi-indices, with \(0 i_{k} p\) and \(0 k n\) and \(_{J}^{}=^{}}{ x^{i_{1 }} x^{i_{k}}}\). Calculating \(_{}^{(J)}\), the coefficients of \(_{_{J}^{}}\), can be done using the prolongation formula, which involves the total derivative operator \(D\) (see Olver  for a derivation):

\[_{}^{(J)}=D_{J}Q_{}+_{i=1}^{p}_{i}_{J}^{}}{ x^{i}} Q_{}=_{}-_{i=1}^{p}_{i}^{}}{ x^{i}}\;.\] (8)

The upshot is that we can mechanically calculate the prolonged vector field using partial derivatives of \(\), which are, in turn, produced by automatic differentiation. In practice, prolonged vector fields are implemented as vector-valued functions (or functionals) of \(\) and \(\). Therefore, the implementation of this mechanical process is generic and can be applied to any PDE. See the Appendix A for examples.

Lie Point Symmetries of PDEs.We can now define the _prolongation of the action of \(\)_:

**Definition 2**.: _For symmetry group \(\) acting on \(M\), the **prolongation of action of \(\)** on the open subset \(M X U\) is the induced action on \(M^{(n)}=M U^{(n)}\) which transforms derivatives of a solution, \(=f()\), into corresponding derivatives of another solution, \(^{}=f^{}(^{})\). We can write this as:_

\[^{(n)}g(,^{(n)})=(g, ^{(n)}g^{(n)})\;.\]

Using the definition above, we can provide a new criterion for \(\) being a symmetry group of \(\), under a mild assumption on the PDE equation.4

**Theorem 2.1**.: \(\) _is the symmetry group of the \(n\)-th order PDE \((,^{n})\), if \(\) acts on \(M\), and its prolongation leaves the solution set \(_{}\) invariant: \(^{(n)}g(,^{(n)})_{},  g\)._

Finally, we can express the symmetry condition in terms of the infinitesimal generators \(\) of \(\):

**Theorem 2.2** (Infinitesimal Criterion).: \(\) _is a symmetry group of the PDE \((,^{n})\) if for every infinitesimal generator \(\) of \(\), we have that_

\[^{(n)}[]=0=0\;.\]

**Example 2** (A Symmetry of the Heat Equation).: _As an illustrative example, we can consider the heat Eq. (5) and will show that the following vector field generates a symmetry group for this PDE: \(=2 t_{x}-xu_{u}\). We need to find the first prolongation \(^{(t)}\) and the second prolongation \(^{(xx)}\), where \(=-xu\). Using the prolongation formula given in Eq. (7), we get:_

\[^{(t)}=-xu_{t}-2 u_{x}^{(xx)}=-2u_{x}-xu_{xx}\]

_Now: \(^{(2)}[]=^{(t)}-^{(xx)}=xu_{t}+2 u_{ x}-2 u_{x}-x u_{xx}=x(u_{t}- u_{xx})\;.\)_

_Clearly \(^{(2)}[]=0\) when \(=0\), hence \(\) is a symmetry of the heat equation._

## 3 Methods

### Solving PDEs with Different Initial/Boundary Conditions with PINNs

Wang et al. (2021) combines the approach introduced in Lu et al. (2021) with the PINN loss to solve PDEs with different initial or boundary conditions without requiring retraining of the model as the original PINN model does. We will also use this proposed framework to examine the effect of enforcing the symmetry condition of the PDE on the model.

Recall that we want to model the operator \(:\), where \(\) is the space of initial condition functions and \(\) is the space of PDE solutions. Our model consists of two neural networks: \(e_{_{1}}\) embeds the initial condition function, and \(g_{_{2}}\) embeds the independent variables, \([t,]^{p}\).

In particular, to embed the initial condition function \(f()=(0,)^{p}\), it is sampled at fixed points \(\{_{1},,_{n}\}\), and the concatenated values are fed to \(f_{_{1}}\)

The final prediction is the inner product of these embedding vectors:

\[_{}(f)(,t)=e_{_{1}}^{}( 0,_{1}),,(0,_{n})\;g_{_{2}}( ,t)\;,\] (9)

where \(=(_{1},_{2})\). In Algorithm 1, we use the notation \(u_{}\) to denote the operator \(_{}\) and use \((l,^{l})_{1:N_{l}}\) to include both boundary and initial condition samples.

While we acknowledge recent architectural improvements to DeepONets (Krishnapriyan et al., 2021), since our goal is to showcase the effectiveness of symmetries, we deploy MLPs for both networks.

### Imposing the Symmetry Criterion

To further inform PINNs about the symmetries of the PDE, we use an additional loss term \(_{}\). Conveniently, this loss sometimes also contains the PDE loss of Eq. (2).

Recall that the infinitesimal criterion of Theorem 2.2 requires that by acting on a solution \((,^{n})\) in the jet space using the prolonged infinitesimal generator \(^{(n)}\), the PDE equation should remain satisfied. In simple terms, our symmetry loss encourages the orthogonality of \(^{(n)}\) and the gradient of \(\) - in other words, infinitesimal symmetry transformations are encouraged to fall on the level-sets of \(\), maintaining \(=0\). Next, we elaborate on this procedure.

Assume that the Lie algebra of the symmetry group of the \(n\)-th order PDE, \(\), is spanned by \(K\) independent vector fields, \(\{_{1},,_{K}\}\), where each \(_{k}\) is defined as in Eq. (6). As noted earlier, for each \(_{k}\), we can obtain their prolongations using automatic differentiation and create a vector of the corresponding coefficients:

\[(^{(n)}_{k})=[_{0}^{k},_{p }^{k},_{0}^{k},,_{q}^{k},(_{0}^{x_{0}})^{k},,(_{q}^{ x^{1}, x^{p}})^{k}]\,.\] (10)

This is a vector of infinitesimal transformations in the jet space.5

We also use the notation \(J_{}\) for the gradient of \(\) wrt all independent and dependent variables: \(J_{}=(},^{}})\).

The symmetry loss encourages the orthogonality of each of \(K\) prolonged vector fields and the gradient vector above on points inside the domain:

\[_{}=_{k=1}^{K}(J_{}^{}( ^{(n)}_{k}))^{2}\;.\] (11)

An alternative is to minimize the absolute value of cosine similarity. We found both of these to work well in practice.

Therefore, the total loss we use to train the two networks consists of the PINN loss introduced in Eq. (2) and the symmetry loss: \(=_{}+_{}+_{}\), where \(\), \(\) and \(\) are hyperparameters. However, as we see through examples, one or more symmetries of a PDE often simplify to a constant times the \(_{PDE}\), removing the need to separate treatment of the PDE loss. Algorithm 1 summarizes our training algorithm.

``` inputs: \(\): the PDE equation of order \(n\), \((_{k})_{1:K}:\) infinitesimal generators of symmetries of \(\) \(=\{(_{1:N_{r}},(,)_{1:N_{l }},_{1:N_{s}})\}_{1:N_{f}}\): dataset for \(N_{f}\) different initial conditions init: initialize parameters \(\) of network \(u_{}\) for iteration do  Sample from \(\) calculate \(_{}\):  calculate \(^{(n)}\) using automatic differentiation for \(_{1:N_{r}}\)  calculate \((^{(n)}_{k}),\) using \(_{1:N_{r}},(^{(n)})_{1:N_{r}},\) auto diff and Eq. (7)  calculate \(_{}\) using \((^{(n)}_{k})\) and \(\) and equation Eq. (11) calculate \(_{}\):  using \(,(^{(n)})_{1:N_{r}}\) and equation Eq. (3): calculate \(_{}\):  calculate \(}=_{}(,_{1:N_{s}})\) for \(_{1:N_{l}}\)  calculate \(_{}\) using \(_{1:N_{l}}\), \(}_{1:N_{l}}\) and equation Eq. (4) \(=_{}+_{}+_{}\) \(-_{}\) endfor return\(u_{}\) ```

**Algorithm 1** PINN with Lie Point Symmetry

## 4 Experiments

### Heat Equation

First, we study the effectiveness of imposing the symmetry constraint on the heat equation, described in Eq. (5). This equation is a simple linear PDE with a rich symmetry group.

**Symmetries.** The following \(6\)-dimensional lie algebra spans the symmetry group of the heat equation:

\[_{1}=_{x} _{3}=_{u} _{5}=2 t_{x}-xu_{u}\] (12) \[_{2}=_{t} _{4}=x_{x}+2t_{t} _{6}=4 tx_{x}-4 t^{2}_{t}-(x^{2}+2 t)u _{u}\;.\]

For example, the infinitesimal generator \(v_{1}\) corresponds to space translation, and \(v_{5}\) to Galilean boost. We refer the reader to Olver (1986) for more details on the derivation.

Data.We generate simulated solutions, which we use to test the models' performance. We use \(=[0,L]=[0,2]\), discretized uniformly into \(256\) points and assume periodic spatial boundaries. We also use \([0,T]=\), discretized into \(100\) points. The viscosity coefficient is set to \(=0.01\)Similar to Brandstetter et al. (2022b, a) and Bar-Sinai et al. (2019), we represent the initial condition functions by truncated Fourier series with coefficients \(A_{k},l_{k},_{k}\) sampled randomly, and \(K=10\):

\[u(t=0,x)=f(x)=_{i=1}^{K}A_{k}(2 l_{k}x/L+_{k})\;.\] (13)

These functions are sampled at \(N_{s}=200\) fixed points, which are used as input to \(e_{_{1}}\) in Eq. (9). We also sample a total of \(N_{l}=300\) points (including the \(200\) points sampled at \(t=0\)), used to impose the data-fit loss, \(_{ data-fit}\). We also randomly sample \(100\) and \(300\) of these initial conditions and use them for the validation and test datasets respectively.

Training and Experiments.The main objective of our experiments is to confirm the hypothesis that training with symmetry loss helps improve the model's prediction capability in a low-data regime. Therefore, we train the model with and without symmetry and evaluate the model's predictions on the test dataset as we increase the number of samples inside the domain, \(N_{r}\). To illustrate the effectiveness of the symmetries in a low-data regime, we use \(N_{f}=100\) different initial conditions and test the performance as we increase \(N_{r}\) from \(500\) to \(2000\) and \(10000\). We refer to Appendix C for details on the architectures and hyperparameters.

Results.In Table 1, we can see a comparison between the performance of the two models on the test dataset of unobserved initial conditions as \(N_{r}\) increases. We note that when trained with few samples, the model trained with symmetry loss, \(_{ sym}\) performs significantly better than the baseline model. Fig. 2, also illustrates this point as it shows the performance of both models on a single instance from the test dataset. We note that the improvement in prediction results in the model where symmetry loss is enforced is especially significant at larger values of time, \(t\).

We want to highlight an important detail: by using the infinitesimal criterion for enforcing symmetries, not all symmetries of the PDE will help improve the training. There are instances when the gradient of \(\) along the vector field is trivially zero, and in other instances, 'we obtain \(c\) where \(c\) is a constant. In the case of the heat equation, only the symmetry transformations from vector fields \(v_{5}\) and \(v_{6}\) provide useful training signals. This means that in our experiments, we can simply eliminate the PDE loss, \(_{ PDE}\), and only use the symmetry loss, \(_{ sym}\), in addition to the supervised loss.

Figure 2: The effect of training the PDE solver for the heat equation with and without the symmetry loss for one of the PDEs in the test dataset. (a) shows the ground truth solution and the predictions of the two models as the number of samples inside the domain increases from \(500\) to \(2000\) and \(1000\).(b) shows the corresponding predictions and the ground truth solution at different time slices.

### Burgers' Equation

The second PDE we analyze is Burgers' Eq. (14), which combines diffusion (with thermal diffusivity \(\)) and non-linear advection (wave motion). The nonlinearity of this equation makes it more complex, resulting in shock formation.

\[u_{t}= u_{xx}-uu_{x}\] (14)

**Symmetries.** Burgers' equation in the form described in Eq. (14) has a symmetry group spanned by the following \(5\)-dimensional vector space.

\[_{1}=_{x} _{3}=t_{x}+_{u} _{5}=tx_{x}+t^{2}_{t}-(x-tu)_{u}\] (15) \[_{2}=_{t} _{4}=x_{x}+2t t-u_{u}\]

However, only the last generator \(_{5}\) results in a useful training signal. The first three generators give \(_{}=0\) and \(_{4}\) gives \(_{}=c=c_{}\), for a constant \(c\). As with the heat equation experiment, we can eliminate the PDE loss and only use symmetry and supervised losses for training.

Data.The data used to evaluate the model is obtained using the Fourier Spectral method with periodic spatial boundaries. Initial conditions are obtained similarly to the heat equation experiment, described in Eq. (13). We use \(=0.1\) as the diffusion coefficient. The domain is \([0,L]=[0,2]\) and \([0,T]=[0,2.475]\) discretized uniformly into \(256\) and \(100\) points respectively.

Training and Experiments.For Burgers' equation, we train the model on datasets of \(N_{f}=500\) initial conditions and \(N_{r}=5000,25000\) and \(100000\) samples. We found that for \(_{}\), cosine similarity works better in this case. The models' architectures are similar to those used for the heat equation described in Appendix C.

Results.Table 2 shows the average mean-squared errors on the test dataset for the two models as \(N_{r}\) increases. We can see that, even with one symmetry group useful for training, the model trained with \(_{}\) performs better. The predictions on a single instance of the test dataset can also be seen in Section 4.2. Again, we see that the symmetry loss especially improves the model's performance for larger values of \(t\). See Appendix B for additional plots of the prediction results. We also note that the high standard deviations in Table 2 are because, compared to the heat equation, the behaviour of the solution (specifically shock formation) varies a lot based on the initial conditions.

## Conclusion and Limitations

Lie groups and continuous symmetries are historically rooted in the study of differential equations, yet to this day, their application to Neural PDE solvers has been limited. Our work presents the

Table 1: The average test set mean-squared error for the Heat equation.

Table 2: The average test set mean-squared error for Burgers’ equation.

Figure 3: (a) predictions of the network trained with a total of \(100000\) points inside the domain (\(500\) initial conditions and only \(200\) points for each PDE), with and without the symmetry loss, for Burgers’ equation on a test data. (b) corresponding predictions at different time slices showing that the model trained with symmetry loss performs better, especially at larger \(t\).

foundations for leveraging Lie point symmetry in a large family of Neural PDE solvers that do not require access to accurate simulations. Using available machinery of automatic differentiation, we show that local symmetry constraints can improve PDE solutions found using PINN models.

The method we propose to leverage local symmetries has some limitations: 1) while the Lie point symmetries of important PDEs are well-known, in general, one needs to analytically derive them for a given PDE to use our approach; 2) as we mentioned in Section 4, not all symmetries of the equation will necessarily be useful for constraining the PINN. Fortunately, the usefulness of symmetries is obvious from the corresponding infinitesimal criterion, and one could limit the symmetry loss to useful symmetries; 3) while symmetries can significantly improve performance, based on our empirical observations (see Table 1), one could achieve a similar effect with PINN by increasing the sample size. These limitations motivate our future direction, which builds on our current understanding, to impose symmetry constraints through equivariant architectures.

#### Acknowledgments

The authors would like to thank Prakash Panangaden for insightful discussions on symmetries and Lie Group theory. We also thank Oumar Kaba for his helpful feedback on this manuscript. This research is in part supported by Canada CIFAR AI Chair and Microsoft Research. Computational resources are provided by Mila and Digital Research Alliance of Canada.