# Learning Curves for Noisy Heterogeneous Feature-Subsampled Ridge Ensembles

Benjamin S. Ruben\({}^{1}\), Cengiz Pehlevan\({}^{2,3,4}\)

\({}^{1}\)Biophysics Graduate Program

\({}^{2}\)Center for Brain Science, \({}^{3}\)John A. Paulson School of Engineering and Applied Sciences, \({}^{4}\)Kempner Institute for the Study of Natural and Artificial Intelligence,

Harvard University

Cambridge, MA 02138

benruben@g.harvard.edu, cpehlevan@seas.harvard.edu

###### Abstract

Feature bagging is a well-established ensembling method which aims to reduce prediction variance by combining predictions of many estimators trained on subsets or projections of features. Here, we develop a theory of feature-bagging in noisy least-squares ridge ensembles and simplify the resulting learning curves in the special case of equocrrelated data. Using analytical learning curves, we demonstrate that subsampling shifts the double-descent peak of a linear predictor. This leads us to introduce heterogeneous feature ensembling, with estimators built on varying numbers of feature dimensions, as a computationally efficient method to mitigate double-descent. Then, we compare the performance of a feature-subsampling ensemble to a single linear predictor, describing a trade-off between noise amplification due to subsampling and noise reduction due to ensembling. Our qualitative insights carry over to linear classifiers applied to image classification tasks with realistic datasets constructed using a state-of-the-art deep learning feature map.

## 1 Introduction

Ensembling methods are ubiquitous in machine learning practice . A class of ensembling methods (known as attribute bagging  or the random subspace method ) is based on feature subsampling , where predictors are independently trained on subsets of the features, and their predictions are combined to achieve a stronger prediction. The random forest method is a popular example .

While commonly used in practice, a theoretical understanding of ensembling via feature subsampling is not well developed. Here, we provide an analysis of this technique in the linear ridge regression setting. Using methods from statistical physics , we obtain analytical expressions for typical-case generalization error in linear ridge ensembles (proposition 1), and simplify these expressions in the special case of equocrrelated data with isotropic feature noise (proposition 2). The result provides a powerful tool to quickly probe the generalization error of ensembled regression under a rich set of conditions. In section 3, we study the behavior of a single feature-subsampling regression model. We observe that subsampling shifts the location of a predictor's sample-wise double-descent peak . This motivates section 4, where we study ensembles built on predictors which are heterogeneous in the number of features they access, as a method to mitigate double-descent. We demonstrate this method's efficacy in a realistic image classification task. In section 5 we apply our theory to the trade-off between ensembling and subsampling in resource-constrained settings. Wecharacterize how a variety of factors influence the optimal ensembling strategy, finding a particular significance to the level of noise in the predictions made by ensemble members.

In summary, we make the following contributions:

* Using the replica trick from statistical physics , we derive the generalization error of ensembled least-squares ridge regression in a general setting, and simplify the resulting expressions in the tractable special case where features are equicorrelated.
* We demonstrate benefits of heterogeneous ensembling as a robust and computationally efficient regularizer for mitigating double-descent with analytical theory and in a realistic image classification task.
* We describe the ensembling-subsampling trade-off in resource-constrained settings, and characterize the effect of label noise, feature noise, readout noise, regularization, sample size and task structure on the optimal ensembling strategy.

**Related works:** A substantial body of work has elucidated the behavior of linear predictors for a variety of feature maps . Several recent works have extended this research to characterize the behavior of ensembled regression using solvable models . Additional recent works study the performance of ridge ensembles with example-wise subsampling  and simultaneous subsampling of features and examples , finding that subsampling behaves as an implicit regularization. Methods from statistical physics have long been used for machine learning theory . Relevant work in this domain include  which studied ensembling by data-subsampling in linear regression.

## 2 Learning Curves for Ensembled Ridge Regression

We consider noisy ensembled ridge regression in the setting where ensemble members are trained independently on masked versions of the available features. We derive our main analytical formula for generalization error of ensembled linear regression, as well as analytical expressions for generalization error in the special case of equicorrelated features with isotropic noise.

### Problem Setup

Consider a training set \(=\{}^{},y^{}\}_{=1}^{P}\) of size \(P\). The training examples \(}^{}^{M}\) are drawn from a Gaussian distribution with Gaussian feature noise: \(}^{}=^{}+^{}\), where \(^{}(0,_{s})\) and \(^{}(0,_{0})\). Data and noise are drawn i.i.d. so that \([^{}^{}]=_{}_{s}\) and \([^{}^{}]=_{} _{0}\). Labels are generated from a noisy teacher function \(y^{}=}^{*}^{}+^{}\) where \(^{}(0,^{2})\). Label noises are drawn i.i.d. so that \([^{}^{}]=_{}^{2}\).

We seek to analyze the quality of predictions which are averaged over an ensemble of ridge regression models, each with access to a subset of the features. We consider \(k\) linear predictors with weights \(}_{r}^{N_{r}}\), \(r=1,,k\). Critically, we allow \(N_{r} N_{r^{}}\) for \(r r^{}\), which allows us to introduce _structural_ heterogeneity into the ensemble of predictors. A forward pass of the model is given as:

\[f()=_{r=1}^{k}f_{r}(), f_{r}() =}}}_{r}^{}_{r}(+)+_{r}.\] (1)

The model's prediction \(f()\) is an average over \(k\) linear predictors. The "measurement matrices" \(_{r}^{N_{r} M}\) act as linear masks restricting the information about the features available to each member of the ensemble. Subsampling may be implemented by choosing the rows of each \(A_{r}\) to coincide with the rows of the identity matrix - the row indices corresponding to indices of the sampled features. The feature noise \((0,_{0})\) and the readout noises \(_{r}(0,_{r}^{2})\), are drawn independently at the execution of each forward pass of the model. Note that while the feature noise is shared across the ensemble, readout noise is drawn independently for each readout: \([_{r}_{r^{}}]=_{rr^{}}_{r}^{2}\).

The weight vectors are trained separately in order to minimize an ordinary least-squares loss function with ridge regularization:

\[}_{r}=*{arg\,min}_{_{r}^{N_{r}}}[ _{=1}^{P}(}}_{r}^{}_{r}}^{}+_{r}^{}-y^{})^{2}+_{r}|_{r}^{2}|]\] (2)

Here \(\{_{r}^{}\}\) represents the readout noise which is present during training, and independently drawn: \(_{r}^{}(0,_{r}^{2})\), \([_{r}^{}_{r}^{}]=_{r}^{2}_{}\). As a measure of model performance, we consider the generalization error, given by the mean-squared-error (MSE) on ensemble-averaged prediction:

\[E_{g}()=_{,,\{_{r}\}}[(f( )-}^{*})^{2}]\] (3)

Here, the expectation is over the data distribution and noise: \((0,_{s})\), \((0,_{0})\), \(_{r}(0,_{r}^{2})\). The generalization error depends on the particular realization of the dataset \(\) through the learned weights \(\{}^{*}\}\). We may decompose the generalization error as follows:

\[E_{g}() =}_{r,r^{}=1}^{k}E_{rr^{}}()\] (4) \[E_{rr^{}}() [(}}_{r}^ {}}_{r}-^{*})^{}_{s}(r^{}}}}_{r^{}}^{}}_{r^{ }}-^{*}).\] (5) \[.+_{r^{}r^{ }}}}}_{r}^{}_{r}_{0}_{r^{}}^ {}}_{r^{}}+M_{rr^{}}_{r}^{2}]\]

Computing the generalization error of the model is then a matter of calculating \(E_{rr^{}}\) in the cases where \(r=r^{}\) and \(r r^{}\). In the asymptotic limit we consider, we expect that the generalization error concentrates over randomly drawn datasets \(\).

### Main Result

We calculate the generalization error using the replica trick from statistical physics, and present the calculation in Appendix F. The result of our calculation is stated in proposition 1.

**Proposition 1**.: _Consider the ensembled ridge regression problem described in Section 2.1. Consider the asymptotic limit where \(M,P,\{N_{r}\}\) while the ratios \(=\) and \(_{rr}=}{M}\), \(r=1,,k\) remain fixed. Define the following quantities:_

\[}_{rr^{}} _{r^{}r^{}}}}_ {r}[_{s}+_{0}]_{r^{}}^{}\] (6) \[_{r} _{N_{r}}+_{r}}_{rr}\] (7) \[_{rr^{}} +q_{r})(_{r^{}}+q_ {r^{}}))}[_{r}^{-1}}_{rr^{ }}_{r^{}}^{-1}}_{r^{}r}]\] (8)

_Then the terms of the average generalization error (eq. 5) may be written as:_

\[ E_{rr^{}}()_{}= }^{2}+_{rr^{}}_{r}^ {2}}{1-_{rr^{}}}+}}( ^{*}_{s}^{*})\] (9) \[-})}^{*}_{s} [}_{r}_{r}^{}_{r}^{-1}_{r}+ r^{}}}_{r^{}}_{r^{}}^{ }_{r^{}}^{-1}_{r^{}}]_{s}^ {*}\] \[+_{r}_{r^{}}}{M(1-_{rr^{}}) }_{r^{}r^{}}}}^{*}_{ s}_{r}^{}_{r}^{-1}}_{rr^{}}_{r^{ }}^{-1}_{r^{}}_{s}^{*}\]

_where the pairs of order parameters \(\{q_{r},_{r}\}\) for \(r=1,,K\), satisfy the following self-consistent saddle-point equations_

\[_{r}=+q_{r}}, q_{r}= [_{r}^{-1}}_{rr}].\] (10)Proof.: We calculate the terms in the generalization error using the replica trick, a standard but non-rigorous method from the statistical physics of disordered systems. The full derivation may be found in the Appendix F. When the matrices \(_{r}(_{s}+_{0})_{r}^{}\), \(r=1,,k\) have bounded spectra, this result may be obtained by extending the results of  to include readout noise, as shown in Appendix G. 

We make the following remarks:

_Remark 1_.: Implicit in this theorem is the assumption that the relevant matrix contractions and traces which appear in the generalization error (eq. 9) and the surrounding definitions tend to a well-defined limit which remains \((1)\) as \(M\).

_Remark 2_.: This result applies for any (well-behaved) linear masks \(\{_{r}\}\). We will focus on the case where each \(_{r}\) implements subsampling of an extensive fraction \(_{rr}\) of the features.

_Remark 3_.: When \(k=1\), our result reduces to the generalization error of a single ridge regression model, as studied in refs. .

_Remark 4_.: We include "readout noise" which independently corrupts the predictions of each ensemble member. This models sources of variation between ensemble members not otherwise accounted for. For example, ensembles of deep networks will vary due to random initialization of parameters . Readout noise is more directly present in physical neural networks, such as an analog neural networks  or biological neural circuits due to their inherent stochasticity.

In Figure 0(a), we confirm the result of the general calculation by comparing with numerical experiments using a synthetic dataset with \(M=2000\) highly structured features (see caption for details). \(k=3\) readouts see random, fixed subsets of features. Theory curves are calculated by solving the fixed-point equations 10 numerically for the chosen \(_{s}\), \(_{0}\) and \(\{_{r}\}_{r=1}^{k}\) then evaluating eq. 9.

### Equicorrelated Data

Our general result allows the freedom to tune many important parameters of the learning problem: the correlation structure of the dataset, the number of ensemble members, the scales of noise, etc. However, the derived expressions are rather opaque. In order to better understand the phenomena captured by these expressions, we examine the following special case:

Figure 1: Comparison of numerical and theoretical learning curves for ensembled linear regression. Circles represent numerical results averaged over 100 trials; lines indicate theoretical predictions. Error bars represent the standard error of the mean but are often smaller than the markers. (a) Testing of proposition 1 with \(M=2000\), \([_{s}]_{ij}=.8^{|i-j|}\), \([_{0}]_{ij}=(0.3)^{|i-j|}\), \(=0.1\), and all \(_{r}=0.2\) and \(_{r}=\) (see legend). \(k=3\) linear predictors access fixed, randomly selected (with replacement) subsets of the features with fractional sizes \(_{rr}=0.2,0.4,0.6\). Fixed ground-truth weights \(^{}\) are drawn from an isotropic Gaussian distribution. (b) Testing of proposition 2 with \(M=5000\), \(s=1\), \(c=0.6\), \(^{2}=0.1\), \(=0.1\), all \(_{r}=0.1\), and all \(_{r}=\) (see legend). Ground truth weights sampled as in eq. 11 with \(=0.3\). Feature subsets accessed by each readout are mutually exclusive (inset) with fractional sizes \(_{rr}=0.1,0.3,0.5\).

**Proposition 2**.: _In the setting of section 2.1 and proposition 1, consider the following special case:_

\[^{*} =}_{}_{0}^{*}+_{M}\] (11) \[_{0}^{*} (0,_{M})\] (12) \[_{s} =s[(1-c)_{M}+c_{M}_{M}^{}]\] (13) \[_{0} =^{2}_{M}\] (14)

_with \(c,[-1,1]\). Label and readout noises \(,_{r} 0\) are permitted. Here \(_{}=_{M}-_{M}_{M}^{}\) is a projection matrix which removes the component of \(_{0}^{*}\) which is parallel to \(_{M}\). The matrices \(\{_{r}\}_{r=1}^{k}\) have rows consisting of distinct one-hot vectors so that each of the \(k\) readouts has access to a subset of \(N_{r}=_{rr}M\) features. For \(r r^{}\), denote by \(n_{rr^{}}\) the number of neurons sampled by both \(_{r}\) and \(_{r^{}}\) and let \(_{rr^{}} n_{rr^{}}/M\) remain fixed as \(M\)._

_Define the following quantities:_

\[a s(1-c)+^{2} S_{r}_{r}}{_{rr}+a_{r}},_{rr^{}}_{rr^{}}S_{r}S_{r^{ }}}{}\] (15)

_The terms of the decomposed generalization error may then be written:_

\[ E_{rr^{}}_{,_{0}^{*}}=}}((1-^{2})I_{rr^{}}^{0}+^{2}I_{rr^{}}^{ 1})+}^{2}+_{rr^{}}_{r}^{2}} {1-_{rr^{}}}\] (16)

_where we have defined_

\[I_{rr^{}}^{0}  s(1-c)(1-s(1-c)_{rr}S_{r}-s(1-c)_{r^{}r^{ }}S_{r^{}}+as(1-c)_{rr^{}}S_{r}S_{r^{}})\] (17) \[I_{rr^{}}^{1} }-_{rr^{ }}_{r^{}r^{}})+^{2}_{rr^{}}}{_{rr^{ }}_{r^{}r^{}}}&0<c 1\\ I_{rr^{}}^{0}&c=0\] (18)

_and where \(\{q_{r},_{r}\}\) may be obtained analytically as the solution (with \(q_{r}>0\)) to:_

\[q_{r}=}{_{rr}+a_{r}},_{r}=+q_{r}}\] (19)

_In the "ridgeless" limit where all \(_{r} 0\), we may make the following simplifications:_

\[S_{r}+|-_{rr}|)}, _{rr^{}}}}{(+ _{rr}+|-_{rr}|)(+_{r^{}r^{}}+|- _{r^{}r^{}}|)}\] (20)

Proof.: Simplifying the fixed-point equations and generalization error formulas in this special case is an exercise in linear algebra. The main tools used are the Sherman-Morrison formula  and the fact that the data distribution is isotropic in the features so that the form of \(}_{rr}\) and \(}_{rr^{}}\) depend only on the subsampling and overlap fractions \(_{rr},_{r^{}r^{}},_{rr^{}}\). To aid in computing the necessary matrix contractions we developed a custom Mathematica package which handles block matrices of symbolic dimension, with blocks containing matrices of the form \(=c_{1}+c_{2}^{}\). This package and the Mathematica notebook used to derive these results are available online (see Appendix B) 

In this tractable special case, \(c\) is a parameter which tunes the strength of correlations between features of the data. When \(c=0\), the features are independent, and when \(c=1\) the features are always equivalent. \(s\) sets the overall scale of the features and the "Data-Task alignment" \(\) tunes the alignment of the ground truth weights with the special direction in the covariance matrix (analogous to "task-model" alignment ). A table of parameters is provided in Appendix A. In Figure 0(b), we test these results by comparing the theoretical expressions for generalization error with the results of numerical experiments, finding perfect agreement.

With an analytical formula for the generalization error, we can compute the optimal regularization parameters \(_{r}\) which minimize the generalization error. These may, in general, depend on both \(r\) and the sample size \(\). Rather than minimizing the error of the ensemble, we may minimize the generalization error of predictions made by the ensemble members independently. We find that this "locally optimal" regularization, denoted \(^{*}\), is independent of \(\), generalizing results from  to correlated data distributions (see Appendix H.3).

## 3 Subsampling shifts the double-descent peak of a linear predictor

Consider a single linear regressor (\(k=1\)) which connects to a subset of \(N= M\) features in the equicorrelated data setting of proposition 2. Also setting \(c=0\), \(s=1\), and \(_{r}==0\) and taking the limit \( 0\) the generalization error reads:

\[ E_{g}_{,^{*}}=\{[(1-)+(-)^{2}]+^{2},&<\\ [1-]+^{2 },&>\}\] (21)

We thus see that double descent can arise from two possible sources of variance: explicit label noise (if \(>0\)) or implicit label noise induced by feature subsampling (\(<1\)). As \(E_{g}(-)^{-1}\), generalization error diverges when sample size is equal to the number of sampled features. Intuitively, this occurs because subsampling changes the number of parameters of the regression model, and thus its interpolation threshold. To demonstrate this, we plot the learning curves for subsampled linear regression on equicorrelated data in Figure 2. At small finite ridge the test error no longer diverges when \(=\), but still displays a distinctive peak.

## 4 Heterogeneous connectivity mitigates double-descent

Double-descent - over-fitting to noise in the training set near a model's interpolation threshold - poses a serious risk in practical machine-learning applications . Cross-validating the regularization strength against the training set is the canonical approach to avoiding double-descent , but in practice requires a computationally expensive parameter sweep and prior knowledge of the task. In situations where computational resources are limited or hyperparameters are fixed prior to specification of the task, it is natural to seek an alternative solution. Considering again the plots in Figure 2(b), we observe that at any value of \(\), the double-descent peak can be avoided with an acceptable choice of the subsampling fraction \(\). This suggests another strategy to mitigate double descent: heterogeneous ensembling. Ensembling over predictors with a heterogeneous distribution of interpolation thresholds, we may expect that when one predictor fails due to over-fitting, the other members of the ensemble compensate with accurate predictions.

In Figure 3, we show that heterogeneous ensembling can guard against double-descent. We define two ensembling strategies: in homogeneous ensembling, each of \(k\) readouts connects a fraction \(_{rr}=1/k\) features. In heterogeneous ensembling, the number of features connected by each of the \(k\) readouts are drawn from a Gamma distribution \(_{k,}()\) with mean \(1/k\) and standard deviation \(\) (see Fig. 3b) then re-scaled to sum to 1 (see Appendix C for details). All feature subsets are mutually exclusive (\(_{rr^{}}=0\) for \(r r^{}\)). Homogeneous and heterogeneous ensembling are illustrated for \(k=10\) in Figs. 3 a.i and 3 a.ii respectively. We test this hypothesis using eq. 16 in 3c. At small regularization

Figure 2: Subsampling alters the location of the double-descent peak of a linear predictor. (a) Illustrations of subsampled linear predictors with varying subsampling fraction \(\). (b) Comparison between experiment and theory for subsampling linear regression on equicorrelated datasets. We choose task parameters as in proposition 2 with \(c====0\), \(s=1\), and (i) \(=0\), (ii) \(=10^{-4}\), (iii) \(=10^{-2}\). All learning curves are for a single linear predictor \(k=1\) with subsampling fraction \(\) shown in legend. Circles show results of numerical experiment. Lines are analytical prediction.

(\(=.001\)), we find that heterogeneity of the distribution of subsampling fractions ( \(>0\)) lowers the double-descent peak of an ensemble of linear predictors, while at larger regularization (\(=0.1\)), there is little difference between homogeneous and heterogeneous learning curves. The asymptotic (\(\)) error is unaffected by the presence of heterogeneity in the degrees of connectivity, which can be seen as the coincidence of the triangular markers in Fig. 3c, as well as from the \(\) limit of eq. 16 (see Appendix H.5). Fig. 3c also shows the learning curve of a single linear predictor with no feature subsampling and optimal regularization. We see that the feature-subsampling ensemble appreciably outperforms the fully-connected model when \(c=0.8\) and \(=0.5\), suggesting the important roles of data correlations and readout noise in determining the optimal readout strategy. These roles are further explored in section 5 and fig 4.

We also test the effect of heterogeneous ensembling in the a realistic classification task. Specifically, we train ensembles of linear classifiers to predict the labels of imagenet  images corresponding to 10 different dog breeds (the "Imagewoof" task ) from their top-hidden-layer representations in a pre-trained ResNext deep network  (see Appendix E for details). We characterize the statistics of the resulting \(M=2048\)-dimensional feature set in Fig. S1. This "ResNext-Imagewoof" classification task has multiple features which make it amenable to learning with a feature-subsampling ensemble. First, the ResNext features have a high degree of redundancy , allowing classification to be performed accurately using only a fraction of the available features (see Fig. 3d and S1c). Second, when classifications of multiple predictors are combined by a majority vote, there is a natural upper bound on the influence of a single erring ensemble member (unlike in regression where predictions can diverge). Calculating learning curves for the imagewoof classification task using homogeneous ensembles, we see sharp double-descent peaks in an ensemble of size \(k\) when \(P=M/k\) (Fig. 3e.i). Using a heterogeneous ensemble mitigates this catastrophic over-fitting, leading to monotonically decreasing error without regularization (Fig. 3e.ii). A single linear predictor with a tuned regularization of \(=0.1\) performs only marginally better than the heterogeneous feature-subsampling ensemble with \(k=16\) or \(k=32\). This suggests heterogeneous ensembling can be an effective alternative to regularization in real-world classification tasks using pre-trained deep learning feature maps.

Note that training a feature-subsampling ensemble also benefits from improved computational complexity. Training an estimator of dimension \(N_{r}\) involves, in the worst case, inverting an \(N_{r} N_{r}\) matrix, which requires \((N_{r}^{3})\) operations. Setting \(N_{r}=M/k\), we see that the number of operations required to train an ensemble of \(k\) predictors scales as \((k^{-2})\).

## 5 Correlations, Noise, and Task Structure Dictate the Ensembling-Subsampling Trade-off

In resource-constrained settings, one must decide between training a single large predictor or an ensemble of smaller predictors. When the number of weights is constrained, ensembling may benefit generalization by averaging over multiple predictions, but at the expense of each prediction incorporating fewer features. Intuitively, the presence of correlations between features limits the penalty incurred by subsampling, as measurements from a subset of features will also confer information about the unsampled features. The equicorrelated data model of proposition 2 permits a solvable toy model for these competing effects. We consider the special case of ensembling over \(k\) readouts, each connecting the same fraction \(_{rr}==1/k\) of all features. For simplicity, we set \(_{rr^{}}=0\) for \(r r^{}\). We asses the learning curves of this toy model in both the ridgeless limit \( 0\) where double-descent has a large effect on test error, and at 'locally optimal' regularization \(=^{*}\) for which double-descent is eliminated. In these special cases, one can write the generalization error in the following forms (see Appendix H.4 for derivation):

\[E_{g}(k,s,c,,,,,,=0)=s(1-c) (k,,,H,W,Z)\] (22) \[E_{g}(k,s,c,,,,,,=^{*})=s (1-c)^{*}(k,,,H,W,Z)\] (23)

where we have defined the effective noise-to-signal ratios:

\[H}{s(1-c)}, W=}{s(1-c)}, Z= }{s(1-c)}\] (24)Therefore, given fixed parameters \(s,c,,\), the value \(k^{*}\) which minimizes error depends on the noise scales, \(s\), and \(c\) only through the ratios \(H\), \(W\) and \(Z\):

\[k^{*}_{=0}(H,W,Z,,)*{arg\,min }_{k}E_{g}(k)=*{arg\,min}_{k} (k,,,H,W,Z)\] (25) \[k^{*}_{=^{*}}(H,W,Z,,) *{arg\,min}_{k}E_{g}(k)=*{arg\,min}_{k} ^{*}(k,,,H,W,Z)\] (26)

In Fig. 4a, we plot these reduced errors curves \(\), \(^{*}\) as a function of \(\) for varying ensemble sizes \(k\) and reduced readout noise scales \(H\). At zero regularization learning curves diverge at their interpolation threshold. At locally optimal regularization \(=^{*}\), learning curves decrease monotonically with sample size. Increasing readout noise \(H\) raises generalization error more sharply for smaller \(k\). In Fig. 4b we plot the optimal \(k^{*}\) in various two-dimensional slices of parameter space in which \(\) is fixed and \(W=Z=0\) while \(\) and \(H\) vary. The resulting phase diagrams may be divided into three regions. In the signal-dominated phase a single fully-connected readout is optimal (\(k^{*}=1\)). In an intermediate phase, \(1<k^{*}<\) minimizes error. And in a noise-dominated phase \(k^{*}=\). At zero regularization, we have determined an analytical expression for the boundary between the intermediate and noise-dominated phases (see Appendix H.4.1 and dotted lines in Figs 4.b,c,d). The signal-dominated, intermediate, and noise-dominated phases persist when \(=^{*}\), removing the effects of double descent. In all panels, an increase in H causes an increase in \(k^{*}\). This can occur because of a decrease in the signal-to-readout noise ratio \(s/^{2}\), or through an increase in the correlation strength \(c\). An increase in \(\) also leads to an increase in \(k^{*}\), indicating that ensembling is more effective for easier tasks. Figs 4c,d show analogous phase diagrams where \(W\) or \(Z\) are varied. Signal-dominated, intermediate, and noise-dominated regimes are visible in the resulting phase diagrams at zero regularization. However, when optimal regularization is used, \(k^{*}=1\) is always optimal. The presence of regions where \(k^{*}>1\) can thus be attributed to double-descent at sub-optimal regularization or to the presence of readout noise which is independent across predictors. We chart the parameter-space of the reduced errors and optimal ensemble size \(k^{*}\) extensively in Appendix I. We plot learning curves for the "ResNext-Imagewoof" ensembled linear classification task with varying strength of readout noise in Fig. 4e, and phase diagrams of optimal ensemble size \(k\) in Fig. 4f, finding similar behavior to the toy model. See Figs. S3, S4, S5 and Appendix E.4.3 for further discussion.

## 6 Conclusion

In this paper, we provided a theory of feature-subsampled linear ridge regression. We identified the special case in which features of the data are "equicorrelated" as a minimal toy model to explore the combined effects of subsampling, ensembling, and different types of noise on generalization error. The resulting learning curves displayed two potentially useful phenomena.

First, we demonstrated that heterogeneous ensembling can mitigate over-fitting, reducing or eliminating the double-descent peak of an under-regularized model. In most machine learning applications, the size of the dataset is known at the outset and suitable regularization may be determined to mitigate double descent, either by selecting a highly over-parameterized model  or by cross-validation techniques (see for example ). However, in contexts where a single network architecture is designed for an unknown task or a variety of tasks with varying structure and noise levels, heterogeneous ensembling may be used to smooth out the perils of double-descent.

Next, we described a trade-off between noise reduction due to ensembling and noise amplification due to subsampling in a resource-constrained setting where the total number of weights is fixed. Our analysis suggests that ensembling is particularly useful in neural networks with an inherent noise. Physical neural networks, such as analog neural networks and biological neural circuits  present such a resource-constrained environments where intrinsic noise is a significant issue.

Much work remains to achieve a full understanding of the interactions between data correlations, readout noise, and ensembling. In this work, we have given a thorough treatment of the convenient special case where features are equicorrelated. Future work should analyze subsampling and ensembling for codes with realistic correlation structure, such as the power-law spectra (see Fig. S1)  and sparse activation patterns .

Figure 3: Heterogeneous ensembling mitigates double-descent. (a) We compare (i) homogeneous ensembling, in which \(k\) readouts connect to the same fraction \(=1/k\) of features, and (ii) heterogeneous ensembling (b) In heterogeneous ensembling subsampling fractions are drawn i.i.d. from \(_{k,}()\), shown here for \(k=10\), then re-scaled to sum to 1. (c) Generalization Error Curves for Homogeneous and Heterogeneous ensembling with \(k=10\), \(=0\), \(=0.3\) and indicated values of \(\), \(c\), and \(\). Blue: homogeneous subsampling. Red, green, and purple show heterogeneous subsampling with \(=0.25/k,0.5/k,1/k\) respectively. Dashed lines show learning curves for 3 particular realizations of \(\{_{11},,_{kk}\}\). Solid curves show the average over 100 realizations. Gray shows the learning curve for a single linear readout with \(=1\) and optimal regularization (eq. 193). Triangular marks show the asymptotic generalization error (\(\)), with downward-pointing gray triangles indicating an asymptotic error of zero. (d,e) Generalization error of linear classifiers applied to the imagewoof dataset with ResNext features averaged over 100 trials. (d) \(P=100\), \(k=1\) varying subsampling fraction \(\) and regularization \(\) (legend). (e) Generalization error of (i) homogeneous and (ii) heterogeneous (with \(=0.75/k\)) ensembles of classifiers. Legend indicates \(k\) values. \(=0\) except for gray curves, where \(=0.1\)

Figure 4: Task parameters dictate the ensembling-subsampling trade-off: (a-d) In the setting of proposition 2 in the special case where all \(_{rr^{}}=_{rr^{}}\) so that feature subsets are mutually exclusive and the total number of weights is conserved. (a) We plot the reduced generalization errors \(\) (for \(=0\), using eq. 22) and \(^{*}\) (for \(=^{*}\) using eq. 23) of linear ridge ensembles of varying size \(k\) with \(=0\) and \(H=0,1\) (values indicated above plots). Grey lines indicate \(k=1\), dashed black lines \(k\), and intermediate \(k\) values by the colorbar. (b) We plot optimal ensemble size \(k^{*}\) (eqs. 25, 26) in the parameter space of sample size \(\) and reduced readout noise scale \(H\) setting \(W=Z=0\). Grey indicates \(k^{*}=1\) and white indicates \(k^{*}=\), with intermediate values given by the colorbar. Appended vertical bars show \(\). Dotted black lines show the analytical boundary between the intermediate and noise-dominated phases given by eq. 214. (c) optimal readout \(k^{*}\) phase diagrams as in (b) but showing \(W\)-dependence with \(H=Z=0\). (d) optimal readout \(k^{*}\) phase diagrams as in (b) but showing \(Z\)-dependence with \(H=W=0\). (e) Learning curves for feature-subsampling ensembles of linear classifiers combined using a majority vote rule on the imagewoof classification task (see Appendix E). As in (a-d) we set \(_{rr^{}}=_{rr^{}}\). Error is calculated as the probability of incorrectly classifying a test example. \(\) and \(\) values are indicated in each panel. (f) Numerical phase diagrams showing the value of \(k\) which minimizes test error in the parameter space of sample size \(P\) and readout noise scale \(\), with regularization (i) \(=0\) (pseudoinverse rule) (ii) \(=0.1\).

Acknowledgements

CP is supported by NSF Award DMS-2134157, NSF CAREER Award IIS-2239780, and a Sloan Research Fellowship. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. BSR was also supported by the National Institutes of Health Molecular Biophysics Training Grant NIH/ NIGMS T32 GM008313. We thank Jacob Zavatone-Veth and Blake Bordelon for thoughtful discussion and comments on this manuscript.