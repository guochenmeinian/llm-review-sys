# Knowledge Distillation Performs

Partial Variance Reduction

Mher Safaryan

IST Austria

mher.safaryan@ista.ac.at

&Alexandra Peste

IST Austria

alexandra.peste@ista.ac.at

Dan Alistarh

IST Austria

dan.alistarh@ista.ac.at

###### Abstract

Knowledge distillation is a popular approach for enhancing the performance of "student" models, with lower representational capacity, by taking advantage of more powerful "teacher" models. Despite its apparent simplicity and widespread use, the underlying mechanics behind knowledge distillation (KD) are still not fully understood. In this work, we shed new light on the inner workings of this method, by examining it from an optimization perspective. We show that, in the context of linear and deep linear models, KD can be interpreted as a novel type of stochastic variance reduction mechanism. We provide a detailed convergence analysis of the resulting dynamics, which hold under standard assumptions for both strongly-convex and non-convex losses, showing that KD acts as a form of _partial variance reduction_, which can reduce the stochastic gradient noise, but may not eliminate it completely, depending on the properties of the "teacher" model. Our analysis puts further emphasis on the need for careful parametrization of KD, in particular w.r.t. the weighting of the distillation loss, and is validated empirically on both linear models and deep neural networks.

## 1 Introduction

Knowledge Distillation (KD)  is a standard tool for transferring information between a machine learning model of lower representational capacity-usually called the _student_-and a more accurate and powerful _teacher_ model. In the context of classification using neural networks, it is common to consider the student to be a _smaller_ network , whereas the teacher is a network that is larger and more computationally-heavy, but also more accurate. Assuming a supervised classification task, distillation consists in training the student to minimize the cross-entropy with respect to the _teacher's logits_ on every given sample, in addition to minimizing the standard cross-entropy loss with respect to the ground truth labels.

Since its introduction , distillation has been developed and applied in a wide variety of settings, from obtaining compact high-accuracy encodings of model ensembles , to boosting the accuracy of compressed models , to reinforcement learning  and learning with privileged information . Given its apparent simplicity, there has been significant interest in finding explanations for the effectiveness of distillation . For instance, one hypothesis  is that the smoothed labels resulting from distillation present the student with a decision surface that is _easier to learn_ than the one presented by the categorical (one-hot) outputs. Another hypothesis  starts from the observation that the teacher's outputs have higher entropy than the ground truth labels, and therefore, higher information content. Despite this work, we still have a limited analytical understanding regarding _why_ knowledge distillation is so effective . Specifically, very little isknown about the interplay between distillation and stochastic gradient descent (SGD), which is the standard optimization setting in which this method is applied.

### Contributions

In this paper, we investigate the impact of knowledge distillation on the convergence of the "student" model when optimizing via SGD. Our approach starts from a simple re-formulation of distillation in the context of gradient-based optimization, which allows us to connect KD to stochastic _variance reduction_ techniques, such as SVRG , which are popular in stochastic optimization. Our results apply both to _self-distillation_, where KD is applied while training relative to an earlier version of the same model, as well as _distillation for compression_, where a compressed model leverages outputs from an uncompressed one during training.

In a nutshell, in both cases, we show that SGD with distillation preserves the convergence speed of vanilla SGD, but that the teacher's outputs serve to reduce the gradient variance term, proportionally to the distance between the teacher model and the true optimum. Since the teacher model may not be at an optimum, this means that variance reduction is only _partial_, as distillation may not completely eliminate noise, and in fact may introduce bias or even increase variance for certain parameter values. Our analysis precisely characterizes this effect, which can be controlled by the weight of the distillation loss, and is validated empirically for both linear models and deep networks.

### Results Overview

To illustrate our results, we consider the case of self-distillation , which is a popular supervised training technique in which both the student model \(x^{d}\) and the teacher model \(^{d}\) have the same structure and dimensionality. The process starts from a teacher model \(\) trained using regular cross-entropy loss, and then trains the student model with respect to a weighted combination of cross-entropy w.r.t. the data labels, and cross-entropy w.r.t. the teacher's outputs. This process is often executed over multiple iterations, in the sense that the student at iteration \(k\) becomes the teacher for iteration \(k+1\), and so on.

Our first observation is that, in the case of self-distillation with teacher weight \(1 0\), the gradient of the distilled student model on a sample \(i\), denoted by \(_{x}f_{i}(x,)\) at a given iteration can simply be written as

\[_{x}f_{i}(x,) f_{i}(x)- f_{i} (),\]

where \( f_{i}(x)\) and \( f_{i}()\) are the student's and teacher's standard gradients on sample \(i\), respectively. This expression is exact for linear models and generalized linear networks, and we provide evidence that it holds approximately for general classifiers. With this re-formulation, self-distillation can be interpreted as a truncated form of the classic SVRG iteration , which never employs full (non-stochastic) gradients.

Our main technical contribution is in providing convergence guarantees for iterations of this form, for both strong quasi-convex, and non-convex functions under the Polyak-Lojasiewicz (PL) condition. Our analysis covers both self-distillation (where the teacher is a partially-trained version of the same model), and more general distillation, in which the student is a compressed version of the teacher model. The convergence rates we provide are similar in nature to SGD convergence, with one key difference: the rate dependence on the _gradient variance_\(^{2}\) is dampened by a term depending on the gap between the teacher model and an optimum. Intuitively, this says that, if the teacher's accuracy is poor, then distillation will not have any positive effect. However, the better-trained the teacher is, the more it can help reduce the _student's_ variance during optimization. Importantly, this effect occurs even if the teacher is _not_ trained to near-zero loss, thus motivating the usefulness of the teacher in self-distillation. Our analysis highlights the importance of the _distillation weight_, as a means to maximize the positive effects of distillation: for linear models, we can even derive a closed-form solution for the optimal distillation weight. We validate our findings experimentally for both linear models and deep networks, confirming the effects predicted by the analysis.

## 2 Related Work

We now provide an overview for some of the relevant related work regarding KD. Knowledge distillation, in its current formulation, was introduced in the seminal work of , which showed that the predictive performance of a model can be improved if it is trained to match the soft targets produced by a large and accurate model. This observation has motivated the adoption of KD as a standard mechanism to enhance the training of neural networks in a wide range of settings, such as compression , learning with noisy labels , and has also become an essential tool in training accurate compressed versions of large models .

Despite these important practical advantages, providing a thorough theoretical justification for the mechanisms driving the success of KD has so far been elusive. Several works have focused on studying KD from different theoretical perspectives. For example, Lopez et al.  connected distillation with _privileged information_ by proposing the notion of _generalized distillation_, and presented an intuitive explanation for why generalized distillation should allow for higher sample efficiency, relative to regular training. Phuong and Lampert  studied distillation from the perspective of generalization bounds in the case of linear and deep linear models. They identify three factors which influence the success of KD: (1) the geometry of the data; (2) the fact that the expected risk of the student always decreases with more data; (3) the fact that gradient descent finds a favorable minimum of the distillation objective. By contrast to all these previous references, our work studies the impact of distillation on stochastic (SGD-based) optimization.

More broadly, there has been extensive work on connecting KD with other areas in learning. Dao et al.  examined links between KD and semi-parametric Inference, whereas Li et al.  performed an empirical study on KD in the context of learning with noisy labels. Yuan et al.  and Sultan et al.  investigated the relationships between KD and label smoothing, a popular heuristic for training neural networks, showing both similarities and substantive differences. In this context, our work is the first to signal the connection between KD and variance-reduction, as well as investigating the convergence of KD in the context of stochastic optimization.

## 3 Knowledge Distillation

### Background

Assume we are given a finite dataset \(\{(a_{n},b_{n}) n=1,2,,N\}\), where inputs \(a_{n}\) (e.g., vectors from \(^{d}\)) and outputs \(b_{n}\) (e.g., categorical labels or real numbers). Consider a set of models \(=\{_{x}: x ^{d}\}\) with fixed neural network architecture parameterized by vector \(x\). Depending on the supervised learning task and the class \(\) of models, we define a loss function \(_{+}\) in order to measure the performance of the model. In particular, the loss associated with a data point \((a_{n},b_{n})\) and model \(_{x}\) would be \((_{x}(a_{n}),b_{n})\). In this framework, the standard Empirical Risk Minimization (ERM) takes the following form:

\[_{x^{d}}_{n=1}^{N}(_{x}(a_{n}),b_{n}).\] (1)

In the objective above, the model \(_{x}\) is trained to match the true outputs \(b_{n}\) given in the training dataset. Suppose that in addition to the true labels \(b_{n}\), we have access to sufficiently well-trained and perhaps more complicated teacher model's outputs \(_{}(a_{n})\) for each input \(a_{n}\). Similar to the student model \(_{x}\), the teacher model \(_{}\) maps \(\) but can have different architecture, more layers and parameters. The fundamental question is how to exploit the additional knowledge of the teacher \(_{}\) to facilitate the training of a more compact student model \(_{x}\) with lower representational capacity. _Knowledge Distillation_ with parameter \(\) from teacher model \(_{}\) to student model \(_{x}\) is the following modification to the objective (1):

\[_{x^{d}}_{n=1}^{N}(1-)( _{x}(a_{n}),b_{n})+(_{x}(a_{n}),_{}(a_{n})).\] (2)

Here we customize the loss penalizing dissimilarities from the teacher's feedback \(_{}(a_{n})\) in addition to the true outputs \(b_{n}\). In case of \(\) is linear in the second argument (e.g., cross-entropy loss), the problem simplifies into

\[_{x^{d}}_{n=1}^{N}(_{x}(a_{n}),(1- )b_{n}+_{}(a_{n})),\] (3)

which is a standard ERM (1) with modified "soft" labels \(s_{n}(1-)b_{n}+_{}(a_{n})\) as the target.

### Self-distillation

As already mentioned, the teacher's model \(_{}\) can have more complicated neural network architecture and potentially larger parameter space \(^{D}\). In particular, \(_{}\) does not have to be from the same set of models \(\) as the student model \(_{x}\). The special case when both the student and the teacher share the same structure/architecture is called _self-distillation_, which is the key setup for our work. In this case, the teacher model \(_{}_{}\) with \(^{d}\) (i.e., \(=,\,D=d\)) and the corresponding distillation objective would be

\[_{x^{d}}_{n=1}^{N}(1-)( _{x}(a_{n}),b_{n})+(_{x}(a_{n}),_{}(a_{n})).\] (4)Our primary focus in this work would be the objective mentioned above of self-distillation. For convenience, let \(f_{n}(x)(_{x}(a_{n}),b_{n})\) be the prediction loss with respect to the output \(b_{n}\), \(f_{n}(x)(_{x}(a_{n}),_{}(a_{n}))\) be the loss with respect to the teacher's output probabilities and \(f_{n}(x,) f_{n}(x)+(1-)f_{n}(x )\) be the distillation loss. See Algorithm 1 for an illustration.

```
1:Input: learning rate \(>0\), initial student model \(x^{0}^{d}\)
2:for each distillation iteration \(m\)do
3: choose a teacher model \(^{m}^{D}\) and distillation weight \(^{m}\) (e.g., see Sec. 5)
4:for each training iteration \(t\)do
5: sample an unbiased mini-batch \(\) form the train set
6: compute distillation gradient \( f_{}(x^{t}^{m},^{m})=^{m} f_{}(x^{ t})+(1-^{m}) f_{}(x^{t}^{m})\)
7: update the student model via \(x^{t+1}=x^{t}- f_{}(x^{t}^{m},^{m})\)
8:endfor
9:endfor ```

**Algorithm 1** Knowledge Distillation via SGD

### Distillation Gradient

As the first step towards understanding how self-distillation affects the training procedure, we analyze the modified loss landscape (4) via stochastic gradients of (1) and (4). In particular, we put forward the following proposition regarding the form of distillation gradient in terms of gradients of (1).

**Proposition 1** (Distillation Gradient).: _For a student model \(x^{d}\), teacher model \(^{d}\) and distillation weight \(\), the distillation gradient corresponding to self-distillation (4) is given by_

\[_{x}f_{n}(x,)= f_{n}(x)- f_{n}( ).\] (5)

Before justifying this proposition formally, let us provide some intuition behind the expression (5) and its connection to distillation loss (4). First, the gradient expression (5) suggests that the teacher has little or no effect on the data points classified correctly with high confidence (i.e. those for which \( f_{n}()\) is close to 0 or \(_{}(a_{n})\) is close to \(b_{n}\)). In other words, the more accurate the teacher is, the less it can affect the learning process. In the extreme case, a perfect or overfitted teacher (one that \( f_{n}()=0\) or \(_{}(a_{n})=b_{n}\) for all \(n\)) will have no effect. In fact, this is expected since, in this case, problems (1) and (4) coincide. Alternatively, if the teacher is not perfect, then the modified objective (4) intuitively suggests that the learning dynamics of a student model is adjusted based on the teacher's knowledge. As we can see in (5), the adjustment from the teacher is enforced by \(- f_{n}()\) term. It is worth mentioning that the direction of distillation gradient \(_{x}f_{n}(x,)\) can be different from the usual gradient's direction \( f_{n}(x)\) due to the influence of the teacher. Thus, Proposition 1 explicitly shows how the teacher guides the student by adjusting its stochastic gradient.

As we will show later, distillation gradient (5) leads to partial variance reduction because of the additional \(- f_{}()\) term. When chosen properly (distillation weight \(\) and proximity of \(\) to the optimal solution \(x^{*}\)), this additional stochastic gradient is capable of adjusting the student's stochastic gradient since both are computed using the same batch from the train data. In other words, both gradients have the same source of randomness which makes partial cancellations feasible.

\(\)**Linear regression.** To support Proposition 1 rigorously, consider the simple setup of linear regression. Let \(=^{d},\ =^{d+1}\), \(_{x}(a)=x^{}\), where \(=[a\ 1]^{}^{d+1}\) is the input vector in the lifted space (to include the bias term), \(=\), and the loss is defined by \((t,t^{})=(t-t^{})^{2}\) for all \(t,t^{}\). Thus, based on (4), we have

\[f_{n}(x,) = (1-)(x^{}_{n}-b_{n})^{2}+(x^{}_{n}-^{}_{n})^{2},\]

from which we compute its gradient straightforwardly as

\[_{x}f_{n}(x,) = 2(1-)(x^{}_{n}-b_{n})_{n}+2(x^{ }_{n}-^{}_{n})_{n}\] \[= 2(x^{}_{n}-b_{n})_{n}-2(^{} _{n}-b_{n})_{n}= f_{n}(x)- f_{n}().\]

Hence, the distillation gradient for linear regression tasks has the form (5).

\(\)**Classification with a single hidden layer.** We can extend the above argument and derivation for a \(K\)-class classification model with one hidden layer that has soft-max as the last layer, i.e. \(_{X}(a_{n})=(X^{}a_{n})^{K}\), where \(X=[x_{1}\ x_{2}\ \ x_{K}]^{d K}\) are the model parameters,\(a_{n}=^{d}\) is the input data and \(\) is the soft-max function. Then, we show in the Appendix B.2 that for all \(k=1,2,K\) it holds

\[_{x_{k}}f_{n}(X,)=_{x_{k}}f_{n}(X)- _{_{k}}f_{n}(),\]

where \(=[_{1}\ _{2}\ \ _{K}]^{d K}\) are the teacher's parameters.

\(\)**Generic classification.** Proposition 1 will not hold precisely for arbitrary deep non-linear neural networks. However, careful calculations reveal that, in general, distillation gradient takes a form similar to (5). Detailed derivations are deferred to Appendix B.3, here we provide the sketch.

Consider an arbitrary neural network architecture for classification that ends with soft-max layer, i.e. \(_{x}(a_{n})=(_{n}(x))\), where \(a_{n}\) is the input data, \(_{n}(x)\) are the produced logits with respect to the model parameters \(x\), and \(\) is the soft-max function. Denote \(_{n}(z)((z),b_{n})\) the loss associated with logits \(z\) and the true label \(b_{n}\). In words, \(_{n}\) gives the logits from the input data, while \(_{n}\) computes the loss from given logits. Then, the representation for the loss function is \(f_{n}(x)=_{n}(_{n}(x))\). We show in Appendix B.3 that the distillation gradient can be written as

\[_{x}f_{n}(x,)=J_{n}(x)( _{n}(_{n}(x))-_{n}(_{n}()))= (x)}{ x}(x)}{ _{n}(x)}-(x)}{ x}( )}{_{n}()},\]

where \(J_{n}(x)(x)}{ x}=[_{n,1 }(x)\ _{n,2}(x)_{n,K}(x)]^{d K}\) is the Jacobian of the vector-valued function \(_{n}\) for logits. Notice that the first term \((x)}{ x}(x)}{ _{n}(x)}\) coincides with the student's gradient \(_{x}f_{n}(x)=(x)}{ x}\). However, the second term \((x)}{ x}()}{ _{n}()}\) differs from the teacher's gradient as the partial derivatives of logits are with respect to the student model.

Despite these differences in the case of deep non-linear models, we observe that the distillation gradient defined by Equation 5 can approximate well the true distillation gradient from Equation 3. Specifically, we consider a fully connected neural network with one hidden layer and ReLU activation , trained on the MNIST dataset , using regular self-distillation, from an SGD-teacher, with a fixed learning rate, and SGD with weight decay and no momentum. At each training iteration we compute the cosine similarity between the gradient of the distillation loss and the approximation from Equation 5, and we average the results across each epoch. The results presented in Figure 1 show that the distillation gradient approximates well the true distillation gradient. Moreover, the behavior is monotonic in the distillation weight \(\) (higher similarity for smaller \(\)), as predicted by the analysis above, and it stabilizes as training progresses. The decrease of cosine similarity can be explained as follows: at the beginning the cosine similarity is high (and SNR is low) since we start from the same model. Then, initial perturbations caused by either the KD or modified KD gradient don't cause big shifts (the teacher has enough confidence and small gradients). These perturbations accumulate over the training leading to decreased cosine similarity and eventually stabilize.

## 4 Convergence Theory for Self-Distillation

### Optimization Setup and Assumptions.

We abstract the standard ERM problem (1) into a stochastic optimization problem of the form

\[_{x^{d}}f(x)_{ }[f_{}(x)]},\] (6)

Figure 1: Cosine similarity, \(l_{2}\) distance and SNR (i.e., \(l_{2}\) distance over the gradient norm of standard KD) statistics between the true and approximated distillation gradient for a neural network during training. As predicted, larger \(\) leads to larger differences, although gradients remain well-correlated.

where \(f_{}(x)\) is the loss associated with data sample \(\) given model parameters \(x^{d}\). For instance, if \(=(a_{n},b_{n})\) is a single data point, then the corresponding loss is \(f_{}(x)=(_{x}(a_{n}),b_{n})\). The goal is to find parameters \(x\) minimizing the risk \(f(x)\). To solve the problem (6), we employ _Stochastic Gradient Descent (SGD)_. Based on Section 3, applying SGD to the problem (6) with self-distillation amounts to the following optimization updates in the parameter space:

\[x^{t+1}=x^{t}-( f_{}(x^{t})- f_{}()),\] (7)

with initialization \(x^{0}^{d}\), step size or learning rate \(>0\), teacher model's parameters \(^{d}\) and distillation weight \(\). To analyze the convergence behavior of iterates (7), we need to impose some assumptions in order to derive reasonable convergence guarantees. First, we assume that the problem (6) has a non-empty solution set \(\) and \(f^{*} f(x^{*})\) for some minimizer \(x^{*}\).

**Assumption 1** (Strong quasi-convexity).: _The function \(f^{d}\) is differentiable and \(\)-strongly quasi-convex for some constant \(>0\), i.e., for any \(x^{d}\) it holds_

\[f(x^{*}) f(x)+ f(x),x^{*}-x+\|x^{*}-x\|^ {2}.\] (8)

Strong quasi-convexity  is a weaker version of strong convexity , which assumes that the quadratic lower bound above holds for at every point \(y^{d}\) instead of \(x^{*}\). Notice that strong quasi-convexity implies that the minimizer \(x^{*}\) is unique1. A more relaxed version of this assumption is the Polyak-Lojasiewicz (PL) condition .

**Assumption 2** (Polyak-Lojasiewicz condition).: _Function \(f^{d}\) is differentiable and satisfies PL condition with parameter \(>0\), if for any \(x^{d}\) it holds_

\[\| f(x)\|^{2} 2(f(x)-f^{*}).\] (9)

Note that the requirement imposed by the PL condition above is weaker than by strong convexity. Functions satisfying PL condition do not have to be convex and can have multiple minimizers . We make use of the following form of smoothness assumption on the stochastic gradient commonly referred to as _expected smoothness_ in the optimization literature .

**Assumption 3** (Expected Smoothness).: _Functions \(f_{}(x)\) are differentiable and \(\)-smooth in expectation with respect to subsampling \(\), i.e., for any \(x^{d}\) it holds_

\[_{}[\| f_{}(x)- f_{}(x^{*}) \|^{2}] 2(f(x)-f^{*})\] (10)

_for some constant \(=(f,)\)._

The expected smoothness condition above is a joint property of loss function \(f\) and data subsampling strategy from the distribution \(\). In particular, it subsumes the smoothness condition for \(f(x)\) since (10) also implies \(\| f(x)- f(x^{*})\|^{2} 2(f(x)-f^{*})\) for any \(x^{d}\). We denote by \(L\) the smoothness constant of \(f(x)\) and notice that \(L\).

### Convergence Theory and Partial Variance Reduction

Equipped with the assumptions described in the previous part, we now present our convergence guarantees for the iterates (7) for both strong quasi-convex and PL loss functions.

**Theorem 1** (See Appendix C.2).: _Let Assumptions 1 and 3 hold. For any \(}\) and properly chosen distillation weight \(\), the iterates (7) of SGD with self-distillation using teacher's parameters \(\) converge as_

\[[\|x^{t}-x^{*}\|^{2}](1-)^{t}\|x^{0}-x^{*} \|^{2}+^{2}}{}(,(f()-f^{*})),\] (11)

_where \(_{*}^{2}[\| f_{}(x^{*})\|^{2}]\) is the stochastic noise at the optimum._

**Theorem 2** (See Appendix C.3).: _Let Assumptions 2 and 3 hold. For any \(}\) and properly chosen distillation weight \(\), the iterates (7) of SGD with self-distillation using teacher's parameters \(\) converge as_

\[[f(x^{t})-f^{*}](1-)^{t}(f(x^{0})-f^{*} )+^{2}}{}(,(f()-f^{*})),\] (12)Proof overview.: Both proofs follow similar steps and can be divided into three logical parts.

_Part 1 (Descent inequality)_. Generally, an integral part of essentially any convergence theory for optimization methods is a descent inequality quantifying the progress of an algorithm in one iteration. Our theory is not an exception: we first define our "potential" \(e^{t}=\|x^{t}-x^{*}\|^{2}\) for the strongly quasi-convex setup, and \(e^{t}=f(x^{t})-f^{*}\) for the PL setup. Then, we start our derivations by bounding \(_{t}[e^{t+1}]-(1-)e^{t}\). Here, \(_{t}\) is the conditional expectation with respect the randomness of previous iterate \(x^{t}\). Specifically, up to constants, both setups allow the following bound:

\[_{t}[e^{t+1}](1-)e^{t}-()(1-())(f(x^{t})-f^{*})+()N(),\] (13)

where \(N()=^{2}\| f()\|^{2}+[\| f _{}(x^{*})- f_{}()\|^{2}]\). Choosing the learning rate \(\) to be small enough, we ensure that the second term is non-positive and hence negligible in the upper bound.

_Part 2 (Optimal distillation weight)_. Next, we focus our attention on the third term in (13) involving the iteration-independent neighborhood term \(N()\). Note that the \(()\) factor next to \(N()\) will be absorbed once we unfold the recursion (13) up to initial iterate. Hence, the convergence neighborhood is proportional to \(N()\). Now the question is how small this term can get if we properly tune the parameter \(\). Notice that \(N(0)=_{*}^{2}\) corresponds to the neighborhood size for plain SGD without any distillation involved. Luckily, due to the quadratic dependence, we can minimize \(N()\) analytically with respect to \(\) and find the optimal value

\[_{*}=[ f_{}(x^{*}), f_{}( )]}{[\| f_{}()\|^{2}]+ \| f()\|^{2}}.\] (14)

Consequently, the analysis puts further emphasis on the need for careful parametrization with respect to the weighting \(\) of the distillation loss as there exists a particularly privileged value \(_{*}\).

_Part 3 (Impact of the teacher)_. In the final step, we quantify the impact of the teacher on the reduction in the neighborhood term \(N(_{*})\) compared to the plain SGD neighborhood \(N(0)\). Via algebraic transformations, we show that

\[)}{N(0)}=1-^{2}(x^{*},)()},\] (15)

where \(()=\| f()\|^{2}/[\| f_{}()\| ^{2}]\) is the signal-to-noise ratio, and \((x^{*},)[-1,1]\) is the correlation coefficient between stochastic gradients \( f_{}(x^{*})\) and \( f_{}()\). This representation gives us analytical means to measure the impact of the teacher. For instance, the optimal teacher \(=x^{*}\) satisfies \((x^{*},)=1\) and \(()=0\), and thus \(N(_{*})=0\). In general, if the teacher is not optimal, then the reduction (15) is of order \((f()-f^{*})\) and \(N(_{*})=_{*}^{2}(,(f()-f^{*}))\). 

We discuss these results highlighting the key aspects and significance.

\(\)**Structure of the rates**. The structure of these two convergence rates is typical in gradient-based stochastic optimization literature: linear convergence up to some neighborhood controlled by the stochastic noise term \(_{*}^{2}\) and learning rate \(\). In fact, these results (including learning rate restrictions) are identical to ones for SGD  except the non-vanishing terms in (11) and (12) include an additional \((f()-f^{*})\) factor due to distillation and proper selection of weight parameter \(\). For both setups, the rate of SGD is the same (11) or (12) with only one difference: \((,(f()-f^{*}))\) term is replaced with \(\). So, \((f()-f^{*})\) is the factor that makes our results better compared to SGD in terms of optimization performance.

\(\)**Importance of the results.** First, observe that in the worst case when the teacher's parameters are trained inadequately (or not trained at all), that is \((f()-f^{*})\), then the obtained rates recover the known results for plain SGD. However, the crucial benefit of these results is to show that a sufficiently well-trained teacher, i.e. \((f()-f^{*})<\), provably reduces the neighborhood size of SGD without slowing down the speed of convergence. In the best case scenario, when the teacher's model is perfectly trained, namely \(f()=f^{*}\), then the neighborhood term vanishes, and the method converges to the exact solution (see SGD-star Algorithm 4 in ). Thus, self-distillation in the form of iterates (7) acts as a form of _partial variance reduction_, which can reduce the stochastic gradient noise, but may not eliminate it completely, depending on the properties of the teacher model.

\(\)**Choice of distillation weight \(\).** As we discussed in the proof sketch above, our analysis reveals that the performance of distillation is optimized for a specific value (14) of distillation weight \(_{*}\) depending on the teacher model. One way to interpret the expression (14) for weight parameter intuitively is that the better the teacher's model \(\) is, the bigger \(_{*} 1\) gets. In other words, \(_{*}\) quantifies the quality of the teacher: \(_{*} 0\) indicates a poor teacher model (\(f() f^{*}\)) and \(_{*}=1\) is for the optimal teacher (\(f()=f^{*}\)).

### Experimental Validation.

In this section we illustrate that our theoretical analysis in the convex case also holds empirically. Specifically, we consider classification problems using linear models in two different setups: training a linear model on the MNIST dataset  and linear probing on the CIFAR-10 dataset , using a ResNet50 model , pre-trained on the ImageNet dataset . For the second setup we train a linear classifier on top of the features extracted from a ResNet50 model pre-trained on ImageNet. This is a standard setting, commonly used in the transfer learning literature, see e.g. . In both cases we train using SGD without momentum and regularization, with a fixed learning rate and mini-batch of size 10, for a total of 100 epochs. The models trained with SGD are compared against self-distillation (Equation 7), using the same training hyper-parameters, where the teacher is the model trained with SGD. In the case of CIFAR-10 features, we also consider the "optimal" teacher, which is a model trained with L-BFGS . We perform all experiments using multiple values of the distillation parameter \(\) and measure the cross entropy loss between student and true labels. At each training epoch we computing the running average over all mini-batch losses seen during that epoch.

The results presented in Figure 2 show the minimum cross entropy train loss obtained over 100 epochs, as well as the average over the last 10 epochs, for models trained with SGD, as well as with self-distillation, with \([0.01,1]\). We observe that when the teacher is the model trained with SGD (\(=0\)), there exists a \(>0\) which achieves a lower training loss than SGD, which is in line with our statement from Theorem 1. Furthermore, when the teacher is very close to the optimum, \(\) closer to 1 reduces the training loss the most compared to SGD, which is also in line with the theory (see Theorem 1). This behavior is illustrated in Figure 1(b), when using an L-BFGS teacher.

## 5 Removing Bias and Improving Variance Reduction

In this section, we investigate the cause of having variance reduction only partially and suggest a possible workaround to obtain complete variance reduction. In brief, the potential source of _partial_ variance reduction is the biased nature of distillation. Essentially, distillation bias is reflected in the iterates (7) since the expected update direction \([ f_{}(x^{t})- f_{}() x^{t} ]= f(x^{t})- f()\) can be different from \( f(x^{t})\). This comes from the fact that distillation loss (4) modifies the initial loss (1) composed of true outputs. To make our argument compelling, next we correct the bias by adding \( f()\) to iterates (7) and analyze the following dynamics:

\[x^{t+1}=x^{t}-( f_{}(x^{t})- f_{}()+  f()).\] (16)

Besides making the estimate unbiased, the advantage of this adjustment is that no tuning is required for the distillation weight \(\); we may simply set \(=1\). The obvious disadvantage is that \( f()\) is the batch gradient over the whole train data that can be very costly to compute. However, we could compute it once and reuse it for all further iterates. The resulting iteration is similar to the popular and well-studied SVRG  method, and therefore iterates (16) will enjoy full variance reduction.

Figure 2: The minimum training loss, and average over the last 10 epochs, for models trained with SGD and with self-distillation, using different values of the distillation parameter \(\), on MNIST and CIFAR-10. SGD is equivalent to \(=0\). The curves for the SGD-based teachers (which do not have zero loss) reflect our analysis, corroborating the existence of the “optimal” distillation weight. By contrast, in the L-BFGS teacher, higher distillation weight always leads to lower loss.

**Theorem 3** (See Appendix D).: _Let Assumptions 2 and 3 hold. Then for any \(}\) the iterates (16) with \(=1\) converge as_

\[_{t}[f(x^{t})-f^{*}](1-)^{t}(f(x^{0})-f^{*})+ )}{}(f()-f^{*}).\] (17)

The key improvement that bias correction brings in (17) is the convergence up to a neighborhood \(((f()-f^{*}))\) in contrast to \((,(f()-f^{*}))\) as in (11) and (12). The multiplicative dependence of learning rate and the quality of the teacher leads the method (16) to full variance reduction. Indeed, if we choose the teacher model as \(=x^{0}\), then the rate (17) becomes

\[[f(x^{t})-f^{*}][(1-)^{t}+ 3L(L+ )/](f(x^{0})-f^{*})(f(x^{0})-f^{*}),\]

provided sufficiently small step-size \(}\) and enough training iterations \(t=(}{{}})\). Hence, following SVRG and updating the teacher model in every \(=(}{{}})\) training iterations, that is choosing \(^{m}=x^{m}\) as the teacher at the \(m^{th}\) distillation iteration (see line 3 of Algorithm 1), we have

\[[f(x^{m})-f^{*}](f(x^{(m-1)})-f^{ *})}(f(x^{0})-f^{*}).\]

Thus, we need \(()\) bias-corrected distillation iteration phases, each with \(=(}{{}})\) training iterates, to get \(\) accuracy in function value. Overall, this amounts to \(()\) iterations of (16).

**Experimental Validation.** Similarly to the previous section, we further validate empirically the result from Theorem 3. Specifically, we consider the convex setup described before, where we train linear models on features extracted on the CIFAR-10 dataset. Based on Figure 2b, we select \(=0.4\) achieving the largest reduction in train loss, compared to SGD, and we additionally perform unbiased self-distillation (Equation 16), using the same training hyperparameters. Similar to the setup from Figure 2, we measure the cross entropy train loss of the student and with the true labels, which is computed at each epoch by averaging the mini-batch losses. The results are averaged over three runs and presented in Figure 3. The first plot on the left shows that, indeed, the unbiased self-distillation update further reduces the training loss, compared to the update from Equation 7. The second plot explicitly tracks gradient variance (averaged over the iterations within each epoch) for the same setup. As expected, both variants of KD (biased and unbiased) have reduced gradient variance compared to plain SGD. The plot also highlights that both variants of KD have similar variance reduction properties, while the unbiasedness of unbiased KD amplifies the reduction of train loss.

## 6 Convergence for Distillation of Compressed Models

So far, the theory we have presented is for self-distillation, i.e., the teacher's and student's architectures are identical. To understand the impact of knowledge distillation, we relax this requirement and allow the student's model to be a sub-network of the larger and more powerful teacher's model. Our approach to model this relationship between the student and the teacher is to view the student as a masked or, in general, compressed version of the teacher. Hence, as an extension to (7) we analyze the following dynamics of distillation with compressed iterates:

\[x^{t+1}=(x^{t}-( f_{}(x^{t})- f_{}( ))),\] (18)

where student's parameters are additionally compressed in each iteration using an unbiased compression operator defined below.

Figure 3: (Left plot) The train loss of self-distillation, unbiased self-distillation and vanilla SGD training. (Right plot) The progress of gradient variances (averaged over the iterations within each epoch) for the same setup

**Assumption 4**.: _The compression operator \(:^{d}^{d}\) is unbiased and there exists finite \( 0\) bounding the compression variance variance, i.e., for all \(x^{d}\) we have_

\[[(x)]=x,[\|(x)-x\|^{ 2}]\|x\|^{2}.\] (19)

Typical examples of compression operators satisfying conditions (19) are sparsification [55; 47] and quantization [1; 56], which are heavily used in the context of communication efficient distributed optimization and federated learning [30; 44; 37; 53]. In this context, we obtain the following:

**Theorem 4** (See Appendix E).: _Let smoothness Assumption 3 hold and \(f\) be \(\)-strongly convex. Choose any \(}\) and compression operator with variance parameter \(=(}{{}})\). Then, properly selecting distillation weight \(\), the iterates (18) satisfy_

\[[\|x^{t}-x^{*}\|^{2}](+1)[(1- )^{t}\|x^{0}-x^{*}\|^{2}+}{}\|x^{*}\|^{2}+ ^{2}}{}(,(f()-f^{*}) )].\]

Clearly, there are several factors influencing the speed of the rate and the neighborhood of the convergence that require some discussion. First of all, choosing the identity map as a compression operator (\((x)=x\) for all \(x^{d}\)), we recover the same rate (11) as before (\(=0\) in this case). Next, consider the case when the stochastic noise at the optimum vanishes (\(_{*}^{2}=0\)) and distillation is switched off (\(=0\)) in (18). In this case, the convergence is still up to some neighborhood proportional to \(\|x^{*}\|^{2}\) since compression is applied to the iterates. Intuitively, the neighborhood term \((\|x^{*}\|^{2})\) corresponds to the compression noise at the optimum \(x^{*}\) ((19) when \(x=x^{*}\)). Also note that the presence of this non-vanishing term \((\|x^{*}\|^{2})\) and the variance restriction \(=(}{{}})\) is consistent with the prior work .

So, the convergence neighborhood of iterates (18) has two terms, one from each source of randomness: compression noise/variance \((\|x^{*}\|^{2})\) at the optimum and stochastic noise/variance \((_{*}^{2})\) at the optimum. Therefore, in this case as well, distillation with a properly chosen weight parameter (partially) reduces the stochastic variance of sub-sampling.

## 7 Discussion and Future Work

Our work has provided a new interpretation of knowledge distillation, examining this mechanism for the first time from the point of view of optimization. Specifically, we have shown that knowledge distillation acts as a form of partial variance reduction, whose strength depends on the characteristics of the teacher model. This finding holds across several variants of distillation, such as self-distillation and distillation of compressed models, as well as across various families of objective functions.

Prior observations showed that significant capacity gap between the student and the teacher may in fact lead to poorer distillation performance . To reconcile the issue of large capacity gap our results, notice that, in our case "better teacher" means better parameter (i.e., weights and biases) values, evaluated in terms of training loss. In particular, in the case of self-distillation, covered in Sections 4 and 5, the teacher and student architectures are identical, and hence they have the same capacity. In our second regime, distillation for compressed models (Section 6), we actually consider the case when the student network is a subnetwork of the teacher; we consider a sparsification compression operator that selects \(k\) parameters for the student out of \(d\) parameters of the teacher. Then, clearly, the teacher has a larger capacity with a capacity ratio \(}{{k}} 1\). However, our result in this direction (Theorem 4) does not allow the capacity ratio to be arbitrarily large. Indeed, the constraint \(=(/)\) on compression variance implies a constraint on capacity ratio since \(=}{{k}}-1\) for the sparsification operator. Thus, our result holds when the teacher's size is not significantly larger than the student's size, which is in line with the prior observations on large capacity gap.

As we mentioned, our Proposition 1 does not hold precisely for arbitrary deep non-linear neural networks. However, we showed that this simple model (5) of distillation gradient approximates the true distillation gradient reasonably well both empirically (see Figure 1) and analytically (see Appendix B.3). There is much more to investigate for the case of non-convex deep networks where exact tracking of teacher's impact across multiple layers of non-linearities becomes harder. We see our results as a promising first step towards a more complete understanding of the effectiveness of distillation. One interesting direction of future work would be to construct more complex models for distillation gradient and to investigate further connections with more complex variance-reduction methods, e.g. , which may yield even better-performing variants of KD.