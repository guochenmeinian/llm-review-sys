# MambaLRP: Explaining Selective State Space Sequence Models

Farnoush Rezaei Jafari\({}^{1,2}\) Gregoire Montavon\({}^{3,2,1}\) Klaus-Robert Muller\({}^{1,2,4,5,6}\)

Oliver Eberle\({}^{1,2}\)

Correspondence to: rezaeijafari@campus.tu-berlin.de, oliver.eberle@tu-berlin.de

###### Abstract

Recent sequence modeling approaches using selective state space sequence models, referred to as Mamba models, have seen a surge of interest. These models allow efficient processing of long sequences in linear time and are rapidly being adopted in a wide range of applications such as language modeling, demonstrating promising performance. To foster their reliable use in real-world scenarios, it is crucial to augment their transparency. Our work bridges this critical gap by bringing explainability, particularly Layer-wise Relevance Propagation (LRP), to the Mamba architecture. Guided by the axiom of relevance conservation, we identify specific components in the Mamba architecture, which cause unfaithful explanations. To remedy this issue, we propose MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable relevance propagation through these components. Our proposed method is theoretically sound and excels in achieving state-of-the-art explanation performance across a diverse range of models and datasets. Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures, uncovering various biases and evaluating their significance. It also enables the analysis of previous speculations regarding the long-range capabilities of Mamba models.

## 1 Introduction

Sequence modeling has demonstrated its effectiveness and versatility across a wide variety of tasks and data types, including text, time series, genomics, audio, and computer vision . Recently, there has been a surge of interest in a new class of sequence modeling architectures, known as structured state space sequence models (SSMs) . This is due to their ability to process sequences in linear time, as opposed to quadratic time required by the more established Transformer architectures . The recent Mamba architecture, a prominent and widely adopted instance of state space models, has demonstrated competitive predictive performance on a variety of sequence modeling tasks across domains and applications , while scaling linearly with sequence length.

As Mamba models, and more generally SSMs, are rapidly being adopted into real-world applications, ensuring their transparency is crucial. This enables inspection beyond test set accuracy and uncovering various forms of biases, including "Clever-Hans" effects . It is particularly important in high-risk domains such as medicine, where the prediction behavior must be robust under real-world conditions and aligned with human understanding. The field of Explainable AI [48; 36; 8; 58] focuses on developing faithful model explanations that attribute predictions to relevant features and has shown success in explaining many highly nonlinear models such as convolutional networks , or attention-based Transformer models [3; 2].

Explaining the predictions of Mamba models is however challenging due to their highly non-linear and recurrent structure. A recent study  suggests viewing these models as attention-based models, enabling the use of attention-based explanation methods [1; 18]. Yet, the explanations produced by attention-based techniques are often unreliable and exposed to potential misalignment between input features and attention scores [75; 38]. As an alternative, Layer-wise Relevance Propagation (LRP)  decomposes the model function with the goal of explicitly identifying the relevance of input features by applying purposely designed propagation rules at each layer. A distinguishing feature of LRP is its adherence to a conservation axiom, which prevents the artificial amplification or suppression of feature relevance in the backward pass. LRP has been demonstrated to produce faithful explanations across various domains (e.g. [7; 62; 3; 20]). Nevertheless, the peculiarities of the Mamba architecture are not addressed by the existing LRP procedures, which may lead to the violation of the conservation property and result in unreliable explanations.

In this work, we present MambaLRP, a novel approach to integrate LRP into the Mamba architecture. By examining the relevance propagation process across Mamba layers through the lens of conservation, we pinpoint layers within the Mamba architecture that need to be addressed specifically. We propose a novel relevance propagation strategy for these layers, grounded in the conservation axiom, that is theoretically sound, straightforward to implement and computationally efficient. Through a number of quantitative evaluations, we show that the proposed MambaLRP approach allows to robustly deliver the desired high explanatory performance, exceeding by far the performance of various baseline explanation methods as well as a naive transposition of LRP to the Mamba architecture. We further demonstrate the usefulness of MambaLRP in several areas: gaining concrete insights into the model's prediction mechanism, uncovering undesired decision strategies in image classification, identifying gender bias in language models, and analyzing the long-range capabilities of Mamba. Our code is publicly available.1

## 2 Related Work

Structured State Space Sequence Models (SSMs).Transformers  have emerged as the most widely used architectures for sequence modeling. However, their computational limitations, particularly with large sequence lengths, have restricted their applicability in modeling long sequences. Addressing these computational limitations, recent works [34; 35] have introduced structured state

Figure 1: Conceptual steps involved in the design of MambaLRP. (a) Take as a starting point a basic LRP procedure, equivalent to Gradient \(\) Input. (b) Analyze layers in which the conservation property is violated. (c) Rework the relevance propagation strategy at those layers to achieve conservation. The resulting MambaLRP method enables efficient and faithful explanations.

space sequence models (SSMs) as an alternative approach. SSMs are a class of sequence modeling methods, leveraging the strengths of recurrent, convolutional, and continuous-time methods, demonstrating promising performance across various domains, including language [30; 46], image [77; 13; 50], and video  processing, and beyond [59; 22; 42]. A recent advancement by Gu and Dao  introduced selective SSM, an enhanced data-dependent SSM with a selection mechanism that adjusts its parameters based on the input. Built on this dynamic selection, the Mamba architecture fuses the SSM components with multilayer perceptron (MLP) blocks. This fusion simplifies the architecture while improving its ability to handle various sequence modeling tasks, including applications in language processing [6; 53; 72], computer vision [41; 83; 78], medical imaging [44; 76; 31; 56; 40; 74; 73], and graphs [70; 14]. This fast adoption of SSMs and Mamba models underscores the need for reliable explanations of their predictions.

Explainable AI and SSMs.In efforts to explain Mamba models,  analyzed if the interpretability tools originally designed for Transformers can also be effectively applied to architectures such as Mamba. In this context, Ali et al.  and Zimerman et al.  recently proposed viewing the internal computations of Mamba models as an attention mechanism. This approach builds upon previous works that use attention signal as explanation, including Attention Rollout  and variants thereof [18; 17]. While these approaches can provide some insight, they inherit the limitations of using attention as an explanation [75; 38], including their inability to capture potential misalignment between tokens and attention scores, and the limited performance in empirical faithfulness evaluations. Alternative Explainable AI methods, not yet applied to Mamba models but in principle applicable to any model, include techniques using input perturbations [81; 85; 29] or leveraging gradient information [11; 64; 68; 65; 63]. Despite their wide applicability, these models have certain drawbacks, such as requiring multiple function evaluations for a single explanation or being susceptible to gradient noise, resulting in subpar performance, as our benchmark experiment will demonstrate. Alternatively, deriving tailored approaches that reflect the underlying model structure, has shown to be a promising direction in developing better attribution methods based on gradient analysis of the prediction function [7; 27; 62; 3]. In the Layer-wise Relevance Propagation framework, this necessitates suitable propagation rules, which are currently lacking for the Mamba architecture. To tackle these challenges, we introduce MambaLRP as an efficient solution for the computation of reliable and faithful explanations that are theoretically grounded in the axiom of relevance conservation.

## 3 Background

Before delving into the details of our proposed method, we begin with a brief overview of the selective SSM architecture, followed by an introduction to the LRP framework.

Selective SSMs (S6)An important component within the Mamba  architecture is the selective SSM. It is characterized by parameters, \(\), \(\), and \(C\), and transforms a given input sequence \((x_{t})_{t=1}^{T}\) into an output sequence of the same size \((y_{t})_{t=1}^{T}\) via the following equations:

\[h_{t} =_{t}h_{t-1}+_{t}x_{t}\] (1) \[y_{t} =C_{t}h_{t}\] (2)

where the initial state \(h_{0}=0\). What distinguishes the selective SSM from the original SSM (S4)  is that the evolution parameter, \(_{t}\), and projection parameters, \(_{t}\) and \(C_{t}\), are functions of the input \(x_{t}\). This enables dynamic adaptation of the SSM's parameters based on input. This dynamicity facilitates focusing on relevant information while ignoring irrelevant details when processing a sequence.

Layer-wise Relevance PropagationLayer-wise Relevance Propagation (LRP)  is an Explainable AI method that attributes the model's output to the input features through a single backward pass. This backward pass is specifically designed to identify neurons relevant to the prediction. LRP assigns relevance scores to neurons in a given layer and then propagates these scores to neurons in the preceding layer. The process continues layer by layer, starting from the network's output and terminating once the input features are reached. The LRP backward pass relies on an axiom called 'conservation' requiring that relevance scores are preserved across layers, avoiding to artificially amplify or suppress contributions. For example, let \(x\) and \(y\) be the input and output of some layer, respectively, and let \((x)\) and \((y)\) represent the sum of relevance scores in the respective layers. The conservation axiom requires that \((x)=(y)\) holds true.

LRP for Mamba

In this work, we bring explainability, particularly LRP, to Mamba models, following the conceptual design steps, shown in Fig. 1. We start by applying a basic LRP procedure, specifically one corresponding to Gradient \(\) Input (GI), to the Mamba architecture. This serves as an effective initial step for identifying layers where certain desirable explanation properties, like relevance conservation, are violated. We analyze different layers of the Mamba architecture, derive relevance propagation equations and test the fulfillment of the conservation property. Our analysis reveals three components in the Mamba architecture where conservation breaks: the SiLU activation function, the selective SSM, and the multiplicative gating of the SSM's output. Leveraging the analysis above, we propose novel relevance propagation strategies for these three components, which lead to a robust, faithful and computationally efficient explanation approach, called MambaLRP.

### Relevance propagation in SiLU layers

We start by examining the relevance propagation through Mamba's SiLU activation functions. This function is represented by the equation \(y=x(x)\), where \(\) denotes the logistic sigmoid function.

**Proposition 4.1**: _Applying the standard gradient propagation equations yields the following result, which relates the relevance values before and after the activation layer:_

\[x=y+^{}(x) x^{2}}_{}\] (3)

The derivation for Eq. 3 can be found in Appendix A.1. We observe that the conservation property, _i.e._\((x)=(y)\), is violated whenever the residual term \(\) is non-zero. We propose to restore the conservation property in the relevance propagation pass by locally expanding the SiLU activation function as:

\[y=x[(x)]_{}\] (4)

where \([]_{}\) treats the given quantity as constant. This can be implemented e.g. in PyTorch using the.detach() function. Repeating the derivation above with this modification yields the desired conservation property, \((x)=(y)\). The explicit LRP rule associated to this LRP procedure is provided in Appendix B.

### Relevance propagation in selective SSMs (S6)

The most crucial non-linear component of the Mamba architecture is its selective SSM component. It is designed to selectively retain or discard information throughout the sequence by adjusting its parameters based on the input, enabling dynamic adaptation to each token. To facilitate the analysis, we introduce an inconsequential modification to the original SSM by connecting \(C_{t}\) to \(h_{t}\) instead of \(x_{t}\). To do so, we can redefine \(_{t}\), \(_{t}\), and \(C_{t}\) matrices as \((_{t}\,,\,0)\), \((_{t}\,,\,I)\), and \((C_{t}\,|\,0)\) respectively, such that \(x_{t}\) becomes part of the state \(h_{t}\) without altering the overall functionality of the SSM.

The unfolded SSM, with the aforementioned modification, is illustrated in Fig. 2. The complex relevance propagation procedure in the SSM component can be further simplified by considering two groups of units, illustrated in red and orange in Fig. 2. In these two groups, there are no connections within units of the same group, all the relevance propagation signals from the first group are directed towards the second group, and the second group receives no further incoming relevance propagation signal. With these properties, these two groups should, according to the principle of conservation, receive the same relevance scores.

Figure 2: Unfolded view of SSM, highlighting two subsets of nodes, the relevance of which should be conserved throughout relevance propagation.

**Proposition 4.2**: _Defining \(_{t}=(_{t},_{t},C_{t-1})\), and working out the propagation equations between these two groups yields the following relation:_

\[}x_{t}+}h_{t-1}}_{(x_{t})(h_{t-1})}=}h_{t}+}y_{t -1}}_{(h_{t})+(y_{t-1})}+}}{ x_{t}}x_{t}+}}{ h_{t-1}}h_{ t-1}}_{}\] (5)

The derivation for Eq. 5 can be found in Appendix A.2. We note that the residual term \(\), which is typically non-zero, violates conservation. Specifically, conservation fails due to the dependence of \(\) on the input. We propose to rewrite the state-space model at each step in a way that the parameters \(_{t}\) appear constant, _i.e._:

\[h_{t} =[_{t}]_{}h_{t-1}+[_{t}]_{}x_ {t}\] (6) \[y_{t} =[C_{t}]_{}h_{t}\] (7)

These equations can also be interpreted as viewing the selective SSM as a localized non-selective, _i.e._ standard, SSM. With this modification, conservation holds between the two groups, _i.e._\((x_{t})+(h_{t-1})=(h_{t})+(y_{t-1})\). By repeating the argument for each time step, conservation is also maintained between the input and output of the whole SSM component. Explicit LRP rules are provided in Appendix B.

### Relevance propagation in multiplicative gates

In each block within the Mamba architecture, the SSM's output is multiplied by an input-dependent gate. In other words, \(y=z_{A} z_{B}\), where \(z_{A}=(x)\) and \(z_{B}=((x))\). Assume that the locally linear expansions introduced in Sections 4.1 and 4.2 are applied to the SSM components and SiLU activation functions, the mapping from \(x\) to \(y\) becomes quadratic.

**Proposition 4.3**: _Applying the standard gradient propagation equations establishes the following relation between the relevance values before and after the gating operation:_

\[x=y}_{ (y)}+y}_{}\] (8)

The derivation for Eq. 8 and explicit LRP rules can be found in Appendix A.3 and Appendix B, respectively. In this equation, we observe a spurious doubling of relevance in the backward pass. This can be addressed by treating half of the output as constant:

\[y=0.5(z_{A} z_{B})+0.5[z_{A} z_{B}]_{}.\] (9)

As for the previous examples, this ensures the conservation property \((x)=(y)\). An alternative would have been to make \(y\) linear by detaching only one of the terms in the product, as done for the SiLU activation or the SSM component. However, the strategy of Eq. 9 better maintains the directionality given by the gradient. We further compare these alternatives in an ablation study presented in Appendix C.5, demonstrating empirically that our proposed approach performs better.

### Additional modifications and summary

The propagation strategies developed for the Mamba-specific components complement previously proposed approaches for other layers, including propagation through RMSNorm layers  and convolution layers via robust LRP-\(\) rules [49; 25] and their generalized variants. A summary of these additional enhancements is provided in Appendix C.2. Furthermore, our proposed propagation rules are generally applicable to other models that utilize similar components, such as multiplicative gates in recent architectures [54; 52; 21; 45].

A straightforward implementation of the propagation rules can be achieved by computing MambaLRP via Gradient \(\) Input, where the gradient computations are modified to align with the proposed rules. The procedure consists of two main steps:

1. Perform the detach operations of Eqs. (4), (6), (7), and (9) (as well as similar operations for RMSNorm and convolutions).
2. Retrieve MambaLRP explanations by computing Gradient \(\) Input on the detached model.

Experiments

To evaluate our proposed approach, we benchmark its effectiveness against various methods previously proposed in the literature for interpreting neural networks. We empirically evaluate our proposed methodology using Mamba-130M, Mamba-1.4B, and Mamba-2.8B language models , which are trained on diverse text datasets. The training details can be found in Appendix C.1. For the vision experiments, we use the Vim-S model . Moreover, we perform several ablation studies to further investigate our proposed method.

DatasetsIn this study, we perform experiments on four text classification datasets, namely SST-2 , Medical BIOS , Emotion , and SNLI . The SST-2 dataset encompasses around 70K English movie reviews, categorized into binary classes, representing positive and negative sentiments. The Medical BIOS dataset consists of short biographies (10K) with five specific medical occupations as targets. The SNLI corpus (version 1.0) comprises 570k English sentence pairs, with the labels entailment, contradiction, and neutral, used for the natural language inference (NLI) task. The Emotion dataset (20K) is a collection of English tweets, each labeled with one of six basic emotions. For the vision experiments, we use ImageNet dataset  with 1.3M images and 1K classes.

Baseline methodsWe compare our proposed method with several gradient-based, model-agnostic explanation techniques: Gradient \(\) Input (GI) [11; 64], SmoothGrad , and Integrated Gradients . Furthermore, we evaluate the performance of our proposed method against a naive implementation of LRP, LRP (LN-rule), where the LRP-0 rule is used in all linear and convolution layers, along with the LN-rule  in normalization layers.

We further compare the performance of our proposed method with two attention-based approaches, Attention Rollout (AttnRoll) and MambaAttr , which are recently proposed for Mamba models. Both methods are extensions of techniques originally developed for Transformer models: Attention Rollout  and Gradient\(\)Attention Rollout .

### Conservation property

To verify the fulfillment of the conservation property, on which our method is based, we compare the network's output score with the sum of relevance scores attributed to the input features, for both the GI baseline and the proposed MambaLRP. The analysis is performed for Mamba-130M and Vim-S models trained on the SST-2 and ImageNet datasets, respectively. Full conservation is achieved if the output score equals the sum of relevance, as indicated by the blue line in Fig. 3. Our results show that conservation is severely violated by the GI baseline, and is addressed to a large extent by MambaLRP. Residual lack of conservation is due to the presence of biases in linear and convolution layers, which are typically non-attributable.

### Qualitative evaluation

In this section, we qualitatively examine the explanations produced by various explanation methods for Mamba-130M and Vim-S models. Fig. 4 illustrates the explanations generated to interpret the Mamba-130M model's prediction on a sentence from the SST-2 dataset with negative sentiment. We note that all of the explanation methods attribute positive scores to the word 'disgusting', which appears reasonable given the negative sentiment label. However, it is notable that the explanation

Figure 3: Conservation property. The x-axis represents the sum of relevance scores across the input features and the y-axis shows the network’s output score. Each point corresponds to one example and its proximity to the blue identity line indicates the extent to which conservation is preserved, with closer alignment suggesting improved conservation.

generated by MambaLRP is more sparse and focuses particularly on the terms'so' and 'disgusting'. In contrast, the explanations produced by the gradient-based methods and AttnRoll appear to be quite noisy. Furthermore, we show the explanations produced to interpret the Vim-S model's predictions

on images of the ImageNet dataset in Fig. 5. Purely gradient-based explanations tend to identify unspecific noisy features, while both attention-based approaches, AttnRoll and MambaAttr, are more effective at highlighting significant features. Among these methods, MambaLRP stands out for its ability to generate explanations that are particularly focused on key features used by the model to make a prediction. Take, for instance, the first image classified under the 'African elephant' category. We can see that the explanation generated by MambaLRP not only includes all occurrences of the 'African elephant' object but also highlights its distinctive features, such as the tasks. In the second image labeled 'wild boar', despite the presence of multiple objects in the image, MambaLRP's explanation remains focused on the 'wild boar' object, disregarding other objects. Moreover, in the third instance, MambaLRP uncovers a spurious correlation, the presence of a watermark in Chinese, influencing the model's prediction, a subtlety overlooked or not fully represented by other methods. Further qualitative results can be found in Appendix C.6.

### Quantitative evaluation

To quantitatively evaluate the faithfulness of explanation methods, we employ an input perturbation approach based on ranking input features by their importance , which can be done using either a Most Relevant First (MoRF) or Least Relevant First (LeRF) strategy. Ranked features are iteratively perturbed through a process known as _flipping_. We monitor the resulting changes in the output logit, \(f_{c}\), for the predicted class \(c\), and compute the area under the perturbation curve. The areas under the curves for LeRF and MoRF strategies are denoted by \(A^{F}_{}\) and \(A^{F}_{}\), respectively. In contrast, the _insertion_ method starts with a fully perturbed input and progressively restores important features. The areas under the curves for this method are indicated by \(A^{I}_{}\) and \(A^{I}_{}\), for the MoRF and LeRF strategies, respectively. A reliable explanation method is characterized by low values of

Figure 4: Explanations generated for a sentence of the SST-2 dataset. Shades of red represent words that positively influence the model’s prediction. Conversely, shades of blue reflect negative contributions. The heatmaps of attention-based methods are constrained to non-negative values.

Figure 5: Explanations produced by different explanation methods for images of the ImageNet dataset. AttnRoll and MambaAttr are limited to non-negative heatmap values.

\(A^{F}_{}\) or \(A^{I}_{}\), and large values of \(A^{F}_{}\) or \(A^{I}_{}\). In an effort to minimize the introduction of out-of-distribution manipulations, the recent study by Blucher et al.  advocates for harnessing both insights to derive a more resilient metric. Therefore, we follow the same strategy as [15; 2] to evaluate explanation methods. The evaluation metrics are defined as \( A^{F}=A^{F}_{}-A^{F}_{}\) and \( A^{I}=A^{F}_{}-A^{F}_{}\). For both metrics, a higher score is preferable, as it signifies a more accurate and reliable explanation method.

The outcomes of this analysis are represented in Table 1. MambaLRP consistently achieves highest faithfulness scores in comparison to other baseline methods. We observe that GI struggles with noisy attributions, leading to low faithfulness scores. However, methods like Integrated Gradients and MambaAttr have shown improvements in this regard. We note that LRP (LN-rule) outperforms most methods across the majority of the text classification tasks. Nevertheless, its performance is notably inferior compared to MambaLRP. Overall, we observe that MambaLRP significantly outperforms all other methods by a substantial margin. In both vision and NLP experiments, attention-based methods have shown superior performance compared to the purely gradient-based approaches.

Runtime comparisonWe report the runtimes of MambaLRP along with other methods used in this study in Appendix C.9. As shown in Table 10, our method's runtime is comparable to GI and can be implemented via a single forward and backward pass. Since approaches like Integrated Gradients require multiple function evaluations, their runtimes are considerably higher than MambaLRP.

Ablation studyIn Section 4, we proposed techniques for handling different non-linear components within the Mamba architecture. This ablation study aims to assess the significance of each technique by testing the effect of their exclusion on faithfulness. Table 2 shows that all three modifications are essential for achieving competitive explanation performance, with our proposed method for handling the SSM component being the most critical. Further experiments, comparing different strategies for handling the Mamba block's multiplicative gate, are detailed in Appendix C.5.

## 6 Use cases

Uncovering gender bias in Mamba.Explanation methods serve as tools to uncover biases in pretrained vision and language models. Using our proposed method, we examine Mamba-130M and Mamba-1.4B models, trained on the Medical BIOS dataset, to investigate the potential presence of gender biases. Following the methodology in , we use MambaLRP to identify the top-5 tokens of highest importance and to quantify the prevalence of gendered words within these tokens. We find that the model exhibits a pronounced preference for female-gendered words in the 'Nurse' class

    &  &  &  &  &  \\   & Mamba & Mamba & Mamba & Mamba & Mamba & Mamba & Mamba & Mamba & Mamba & Mamba & Mamba & Mamba & Mamba & Mamba & Mamba \\  & 130M & 1.4B & 2.8B & 130M & 1.4B & 2.8B & 130M & 1.4B & 2.8B & 130M & 1.4B & 2.8B & 130M & 1.4B & 2.8B & 130M & 1.4B & 2.8B & 130M \\  Random & -0.012 & -0.106 & -0.007 & 0.044 & -0.014 & -0.037 & 0.010 & 0.002 & 0.000 & -0.001 & 0.000 & 0.000 & -0.001 \\  GI  & 0.078 & -0.106 & -0.043 & 0.200 & -0.634 & -1.434 & -0.039 & 0.083 & -0.787 & -0.409 & -1.533 & -0.018 \\ SmoothGrad  & 1.377 & -0.383 & -0.675 & 1.661 & -2.300 & -1.908 & 0.486 & -0.687 & -0.747 & 1.808 & -1.852 & -4.228 & 0.209 \\ IG  & 0.857 & 0.216 & 0.322 & 1.296 & 1.065 & 1.937 & 0.453 & 0.218 & 0.331 & 1.808 & 2.010 & 4.314 & 1.217 \\  AttnRoll  & 0.657 & 0.431 & 0.452 & 2.228 & 1.076 & 2.241 & 0.242 & 0.371 & 0.292 & 0.389 & 1.483 & 0.530 & 2.427 \\ MambaAttr  & 1.190 & 0.626 & 0.341 & 3.126 & 3.006 & 5.326 & 0.513 & 0.554 & 0.343 & 2.003 & 4.706 & 3.849 & 2.676 \\ LRP (LN-rule,  & 0.877 & 0.961 & 0.820 & 2.217 & 3.456 & 5.305 & 0.673 & 0.656 & 0.731 & 3.079 & 5.199 & 5.094 & 2.548 \\ MambaLRP (ours) & **1.978** & **1.248** & **1.157** & **3.906** & **4.234** & **7.083** & **0.989** & **0.897** & **0.899** & **3.523** & **5.397** & **5.637** & **4.715** \\   

Table 1: Evaluating explanation methods. Higher scores \( A^{F}\) indicate more faithful explanations.

  
**Models** & **Surgeon** & **Nurse** \\  GPT2-base & 0.14 & 0.24 \\ T5-base & 0.10 & 0.11 \\ RoBERTa-base & 0.01 & 0.06 \\ Mamba-130M & 0.009 & 0.058 \\ Mamba-1.4B & 0.001 & 0.042 \\   

Table 2: Analyzing the impact of ablating the three proposed propagation rules on \( A^{F}\) for the components in MambaLRP.

  
**SiLU** & **SSM** & **Gate** & **SST-2** & **ImageNet** \\  ✓ & ✗ & ✓ & 0.577 & 0.144 \\ ✓ & ✓ & ✗ & 1.721 & 4.022 \\ ✗ & ✓ & ✓ & 1.943 & 4.618 \\ ✓ & ✓ & ✓ & **1.978** & **4.715** \\   

Table 3: Frequency of gendered words in explanations for ‘Nurse’ and ‘Surgeon’ classes of the Medical BIOS dataset across language models.

(e.g. the proportion of gender-specific words is 0.058 for females, compared to 0.0 for males in Mamba-130M.). We also compare the results of our analysis with those achieved for the GPT2-base, T5-base, and RoBERTa-base models as mentioned in . As shown in Table 3, both Mamba models are less dependent on gendered tokens compared to GPT2-base, T5-base, and RoBERTa-base models, with the Mamba-1.4B model showing a further decrease in bias compared to the Mamba-130M, suggesting improvements in reducing gender bias with increased model size.

Investigating long-range capabilities of Mamba.The ability of SSMs to model long-range dependencies is considered an important improvement over previous sequence models. In this use case, we analyze the extent to which the pretrained Mamba-130M model can use information from the entire context window. We use the HotpotQA  subset from the LongBench dataset , designed to test long context understanding. After selecting all 127 instances, containing sequences up to 8192 tokens, we prompt the model to summarize the full paragraph by generating ten additional tokens. Fig. 6 shows the distribution of the positional difference between a relevant token and the currently generated token. While we observe a pronounced pattern of attributing to the last few tokens, as seen in prior language generation studies [80; 61], the extracted explanations also identified relevant tokens across the entire context window, as presented for one example in Fig. 6 (right). This suggests that the model is indeed capable of retrieving long-range dependencies. We clearly see that in order to complete the sentence and assign a year to the album release date, the model analyzes previous occurrences of chronological information and MambaLRP identifies evidence supporting the decision for the date being '1972' as relevant. Our analysis demonstrates the previously speculated long-range abilities of the Mamba architecture , which we further explore in a comparison to Transformers in Appendix C.8.

Needle-in-a-haystack test.To assess the model's ability in retrieving relevant pieces of information from a broader context, we perform the needle-in-a-haystack test . Our test involves extracting a single passkey (the 'needle') from a collection of repeated noise sentences (the 'haystack'), as described in . We run this test at eleven different document depths with three different context

Figure 6: Analysis of the position of tokens relevant for next token generation. Left: Distribution of absolute position of the ten most relevant tokens for the prediction of the next word. Right: Long-range dependency between tokens of the input and the predicted next token (here: _1972_).

Figure 7: Explanation-based retrieval accuracy in the needle-in-a-haystack test verifying model reliance on relevant features for different context lengths.

lengths. We use an instruction-finetuned Mamba-2.8B model in this experiment. To analyze the performance of the model, we introduce the explanation-based retrieval accuracy (XRA) metric. In this approach, we first identify the positions of the top-K relevant tokens by MambaLRP, and then, calculate the accuracy by comparing those positions to the needle's position. As shown in Fig. 7, MambaLRP accurately captures the information used by the model to retrieve the needle. In this case, the model could accurately retrieve the needle based on relevant information within the text. However, in more realistic and complex scenarios, the model may depend on irrelevant data yet still generate the correct token. This issue can be analyzed using XRA but cannot be evaluated by conventional retrieval accuracy metrics. Such cases and also further details about this experiment are shown in Appendix C.7.

## 7 Discussion and conclusion

Mamba models have emerged as an efficient alternative to Transformers. However, there are limited works addressing their interpretability . To address this issue, we proposed MambaLRP within the LRP framework, specifically tailored to the Mamba architecture and built upon the relevance conservation principle. Our evaluations across various models and datasets confirmed that MambaLRP adheres to the conservation property and provides faithful explanations that outperform other methods while being more computationally efficient. Moreover, we demonstrated how MambaLRP can help users debug state-of-the-art vision and language models while building trust in their predictions through various use cases. Future research can explore its potential across a broader range of applications and Mamba architectures, providing reliable insights into sequence models.

LimitationsAs a propagation-based explanation method, MambaLRP requires storing activations and gradients, leading to memory usage that depends on the model architecture and input sequence length. To reduce memory consumption, techniques such as gradient checkpointing can be utilized, which are applicable to other gradient-based methods as well. However, a limitation of these methods, including MambaLRP, is the potential inaccessibility of gradient information due to proprietary constraints. In such cases, approximating gradient information may offer a viable solution.

This work was funded by the German Ministry for Education and Research (refs. 01IS14013A-E, 01GQ1115, 01GQ0850, 01IS18025A, 031L0207D, 01IS18037A). K.R.M. was partly supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) (No. 2019-0-00079, Artificial Intelligence Graduate School Program, Korea University and No. 2022-0-00984, Development of Artificial Intelligence Technology for Personalized Plug-and-Play Explanation and Verification of Explanation).