# A Benchmark Suite for Evaluating Neural Mutual Information Estimators on Unstructured Datasets

Kyungeun Lee

LG AI Research

Seoul, Republic of Korea

kyungeun.lee@lgresearch.ai

&Wonjong Rhee

Seoul National University

Seoul, Republic of Korea

wrhee@snu.ac.kr

###### Abstract

Mutual Information (MI) is a fundamental metric for quantifying dependency between two random variables. When we can access only the samples, but not the underlying distribution functions, we can evaluate MI using sample-based estimators. Assessment of such MI estimators, however, has almost always relied on analytical datasets including Gaussian multivariates. Such datasets allow analytical calculations of the true MI values, but they are limited in that they do not reflect the complexities of real-world datasets. This study introduces a comprehensive benchmark suite for evaluating neural MI estimators on unstructured datasets, specifically focusing on images and texts. By leveraging same-class sampling for positive pairing and introducing a binary symmetric channel trick, we show that we can accurately manipulate true MI values of real-world datasets. Using the benchmark suite, we investigate seven challenging scenarios, shedding light on the reliability of neural MI estimators for unstructured datasets.

## 1 Introduction

Mutual Information (MI), denoted as \(I(X;Y)\), serves as a fundamental measure in quantifying the dependency between two random variables (Cover, 1999). It is mathematically defined as:

\[I(X;Y)=_{p(x,y)}.\]

In practice, we often rely on the estimations instead of the exact calculation of MI because we can only access the examples sampled from joint and marginals but not the underlying distribution functions (\(p(x,y)\) and \(p(x)p(y)\)). To this end, various sample-based MI estimators have been proposed (Fraser and Swinney, 1986; Shwartz-Ziv and Tishby, 2017; Kraskov et al., 2004; Belghazi et al., 2018; Poole et al., 2019; Song and Ermon, 2019, 2020; Cheng et al., 2020), and they have played a key role in improving the deep learning performance across diverse applications, including generative models (Chen et al., 2016), language representation learning (Oord et al., 2018; Wang et al., 2020), domain generalization (Li et al., 2022), anomaly detection (Lei et al., 2023), and self-supervised learning (Hjelm et al., 2018; Bachman et al., 2019; Chen et al., 2020; Chen and He, 2020; Grill et al., 2020).

Despite the huge success of MI estimators in developing useful real-world applications, the accuracy of MI estimators on real-world datasets largely remains underexplored because the true MI values cannot be calculated for such datasets. Gaussian datasets, the primary benchmark in the existing studies (Belghazi et al., 2018; Poole et al., 2019; Song and Ermon, 2019, 2020; Cheng et al., 2020), do not adequately represent the complexity of real-world datasets. This raises a fundamental question: _do estimators that perform well on Gaussian datasets also excel with more complex datasets like images or texts?_ Recently, Czyz et al. (2023) have explored non-Gaussian datasets to evaluate MIestimator accuracy. However, their evaluation is also limited to datasets with tractable distributions, such as Student's t-distributions, still far from being representative of real-world datasets. To address this gap, we present a method for evaluating MI estimators on any dataset in the absence of underlying distribution functions. Our approach employs same-class sampling as positive pairing (Lee et al., 2023) and binary symmetric channels (Cover, 1999) for precise manipulation of the true MI values.

We have developed a benchmark suite based on our method, encompassing three data domains for Gaussian multivariates, images, and sentence embeddings. To demonstrate its usefulness, we examine performance of several neural MI estimators over seven key aspects in Section 5. The seven aspects are critic architecture, critic capacity, choice of neural MI estimator, number of information sources, representation dimension, strength of nuisance, and layer-dependency. Through the examination of the seven aspects, we report interesting findings from our benchmark suite. For instance, when the number of information sources become large, MI estimation is relatively robust for image and text datasets than for Gaussian dataset as shown in Figure 1 (a,b,c). We also report that a larger critic capacity does not ensure a higher estimation accuracy, there is no single MI estimator that provides a universally superior performance over the three data domains, representation dimension can be as large as 10,000 without degrading the accuracy of MI estimators, MINE (Belghazi et al., 2018) turns out to be relatively robust to nuisance, and MI estimation can be surprisingly inaccurate for lower-layer representations while it is typically much more accurate for upper-layer representations (see Figure 1d). The benchmark suite and codebase are available in https://github.com/kyungeun-lee/mibenchmark.

## 2 Backgrounds: Neural Mutual Information Estimators

Mutual information between two random variables \(X\) and \(Y\) is defined as follows.

\[I(X;Y) KL(p(x,y)||p(x)p(y))=_{p(x,y)}[]\]

When only a finite set of joint samples is available, the exact MI cannot be calculated, but an estimation can be made. Among the known MI estimation methods, including simple binning (Fraser and Swinney, 1986; Shwartz-Ziv and Tishby, 2017) and non-parametric kernel-density estimators (Kraskov et al., 2004), variational estimators based on variational bounds and deep neural networks (DNN) modeling have become dominant for complex datasets (Belghazi et al., 2018; Poole et al., 2019; Song

Figure 1: Exemplary findings from our benchmark suite. The ratio between estimated MI value and true MI value is shown in y-axis. A ratio close to 1.0 indicates a highly accurate MI estimation. (a, b, c) When the number of independent information sources is increased to 16, all MI estimators become inaccurate for Gaussian dataset (shown in (a)) but some MI estimators remain accurate for image dataset (shown in (b)) and text dataset (shown in (c)). (d) When MI is estimated using embeddings from different layers of ResNet-50, MI estimators stay accurate only for the upper layers.

and Ermon, 2019, 2020, Cheng et al., 2020). These DNN-based estimators are commonly referred to as neural MI estimators. In this study, we focus on neural MI estimators due to their superior performance in handling large sample sizes and high-dimensional data, as demonstrated in previous works (Gao et al., 2015, Belghazi et al., 2018, Poole et al., 2019, Czyz et al., 2023).

Variational MI estimators are based on two steps. First, an analytical bound is derived where the bound is based on a critic function \(f(x,y)\). Second, the bound is made tight by optimizing for a supremum or an infimum over \(f(x,y)\). In recent works, deep neural networks have been used to model the critic function. When a proper loss function is chosen and the learning of \(f(x,y)\) is successful, the variational estimations have been shown to be accurate for Gaussian datasets (Belghazi et al., 2018, Poole et al., 2019, Song and Ermon, 2019, Czyz et al., 2023).

**Definition 2.1** (Variational MI estimators (Poole et al., 2019)).: Let \(X\), \(Y\) be two random variables taking values in \(\), \(\), and \(=\{(x_{i},y_{i})\}_{i=1}^{N} X,Y\) denote the set of samples drawn from a joint distribution over \(\) and \(\). The variational bounds of \(I(X;Y)\) are formulated as:

\[I(X;Y)(X;Y)=1+_{p(x,y)}[}{a(y)} ]-_{p(x)p(y)}[}{a(y)}]\]

where \(a(y)>0\) is any value or function of \(y\). A variety of MI estimators are defined by adopting different \(a(y)\). For example, \(a(y)=e\) (constant) corresponding to \(_{}(X;Y)\)(Nguyen et al., 2010) (also known as \(f\)-GAN KL (Nowozin et al., 2016) and MINE-\(f\)(Belghazi et al., 2018)) and \(a(y)=_{i=1}^{K}e^{f(x_{i},y)}\) corresponding to \(_{}(X;Y)\)(Oord et al., 2018).

For a neural MI estimator, a DNN is used to model the _critic_ function \(f(x,y)\), and there are two associated steps. The first is the optimization (or training) step where the DNN parameters are learned. The second is the estimation step where the actual MI values are inferred with the optimized DNN. Variational MI estimators, such as DV (Donsker and Varadhan, 1983), NWJ (Nguyen et al., 2010), and InfoNCE (Oord et al., 2018), use a single loss function for both optimization and estimation, and the loss function corresponds to the theoretical MI bound in use. Other variational MI estimators, such as JS (Nowozin et al., 2016), MINE (Belghazi et al., 2018), and SMILE (Song and Ermon, 2019), adopt small modifications in either optimization or estimation to improve the robustness or accuracy of the estimator. The most popular variational MI estimators are summarized in Table 1.

Common choices for the critic function \(f(x,y)\) include (1) the inner product critic \(f_{}(x_{i},y_{j})=x_{i}^{T}y_{j}\), (2) bilinear critic \(f_{}(x_{i},y_{j})=x_{i}^{T}Wy_{j}\) where \(W\) is trainable, (3) separable critic \(f_{}(x_{i},y_{j})=f_{1}(x_{i})^{T}f_{2}(y_{j})\), and (4) joint critic \(f_{}(x_{i},y_{j})=f_{3}([x_{i},y_{j}])\). Here \(f_{1}\), \(f_{2}\), and \(f_{3}\) are typically shallow MLPs and they model the relationship between all pairs of \((x_{i},y_{j})\)\( i,j[1,K]\).

## 3 Related Works

Despite its theoretical validity, MI estimators present a few disadvantages because we typically have access to samples, but not to the underlying distribution functions (Poole et al., 2019, Song and Ermon, 2019, Paninski, 2003, McAllester and Stratos, 2020). Most estimators exhibit sub-optimal

   Estimator & Optimization Loss - \((f(x,y))\) & Estimate Evaluation - \((X;Y)\) \\   DV (Donsker and Varadhan, 1983) & \(_{p(y}/(x,y))-_{p(x,y)}|f(x,y)|-_{p(x,y)}|e^{ f(x,y)}\) \\  NWJ (Nguyen et al., 2010) & \(_{}(f(x,y))-_{}(X;Y)=_{p(x,y )}|f(x,y)|-e^{-_{}(x,y)}|e^{f(x,y)}\) \\  InfoNCE (Chen et al., 2018) & \(_{}(f(x,y))-_{}(X;Y)=_{p(x,y )}[_{i=1}^{K},y_{i})}{ (f(x_{i},y_{i}))}]\) \\  JS (Poole et al., 2019) & \(_{p(x,y)}|-(-f(x,y))|-_{}| (f(x,y)|\) & \(f_{}(X;Y)\) \\  MINE (Belghazi et al., 2018) & \(_{p(x,y)}|f(x,y)|-_{}| (f(x,y)|\)}{_{}|(f(x,y)|\)}\) & \(_{}(X;Y)\) \\  SMILE-\(\)(Song and Ermon, 2019) & \(_{}(f(x,y))\) & \(_{p(x,y)}|f(x,y)|-_{p(x,y)}|(e^{f(x,y) },e^{-},e^{})|\) \\   

Table 1: Summary of neural mutual information estimators. For the optimization step, we learn \(f^{*}(x,y)\) by maximizing the optimization loss \((f(x,y))\) with a given batch size \(K\). For the estimation step, we evaluate MI values as \((X;Y)\). DV, NWJ, and InfoNCE utilize the same formulation for optimization and estimation.

performance, particularly when batch size \(K\) is small and true MI is large. For instance, InfoNCE can result in a high bias because it is upper bounded by \( K\)(Oord et al., 2018). McAllester and Stratos (2020) noted that any distribution-free high-confidence lower bound on MI cannot be larger than \(( K)\). In contrast, most estimators result in a variance that can increase exponentially with the increase in the true MI value (Poole et al., 2019; Song and Ermon, 2019; Xu et al., 2019). While these findings are enlightening, the limitations of estimators have been assessed mostly for the Gaussian benchmarks only instead of for the real-world datasets.

In efforts to evaluate MI estimators beyond Gaussian datasets, Song and Ermon (2019) proposed self-consistency tests using MNIST and CIFAR-10 datasets. They found that all estimators were not accurate for images. However, the analysis can be highly misleading because they evaluated based on the approximated metrics instead of the true MI. Czyz et al. (2023) suggested a non-Gaussian benchmark with tractable distributions (_e.g._, multivariate student, uniform distribution) with the formulated mapping functions (_e.g._, spiral transformation), enabling access to true MI. They considered non-Gaussian datasets that have tractable distribution functions, but the results are not representative of unstructured datasets, such as images and texts. In this study, we introduce a comprehensive method for evaluating MI estimators with no constraint on the type of data domains. In particular, we present a benchmark suite that allows assessment and manipulation of the true MI for images and sentence embeddings.

## 4 Proposed Method and Benchmark Suite

We propose a comprehensive method for evaluating neural MI estimators across various data domains. While our method is applicable to any choice of data domain, we focus on three types of data domains in our benchmark suite: (1) a multivariate Gaussian dataset (\(_{}\)), corresponding to the most common case for evaluating MI estimators in the existing works (Poole et al., 2019; Song and Ermon, 2019; McAllester and Stratos, 2020); (2) an image dataset consisting of digits (\(_{}\)), as an example of vision tasks; and (3) a sentence embedding dataset consisting of BERT embeddings of movie review datasets (\(_{}\)), as an example of NLP tasks. We first consider the general formulation for Gaussian dataset, define three factors that can affect MI values, introduce same-class sampling, propose a method for generating unstructured datasets with adjustable true MI values, and finally explain how binary symmetric channel trick can be employed for manipulating true MI to any non-integer value.

### General formulation for Gaussian dataset

Consider a dataset with \(K\) pairs of samples, where \((x_{i},y_{i})\) is sampled from a joint distribution \(p(x,y)\). An MI estimator utilizes the dataset as its input and evaluates the estimated mutual information \((X;Y)\). If the estimation is accurate, \((X;Y)\) should be close to the true mutual information \(I(X;Y)\). In previous studies, a Gaussian dataset associated with a multivariate Gaussian model was utilized to assess neural MI estimators (Belghazi et al., 2018; Poole et al., 2019; Song and Ermon, 2019; Cheng et al., 2020). The Gaussian dataset has Gaussian samples with zero mean and a component-wise correlation of \(\) between \(X\) and \(Y\). The true MI is known and can be expressed analytically as \(I(X;Y)=-}{2})}\), where \(x^{d_{g}}\) and \(y^{d_{g}}\).

### Definitions of \(d_{s}\), \(d_{r}\), and \(Z\)

In the benchmark suite, we design and focus on three essential factors that can affect mutual information \(I(X;Y)\), especially for unstructured datasets. For random variables \(X\) and \(Y\) with a joint distribution \(p(x,y)\), they can be defined as follows.

**Definition 4.1** (Number of information sources \(d_{s}\)).: \(d_{s}\) is the number of independent scalar random variables used to form the mutually shared information between \(X\) and \(Y\).

**Definition 4.2** (Representation dimension \(d_{r}\)).: \(d_{r}\) is the size of the observational data. When \(X\) and \(Y\) are of the same size, it is the length of the vector formed by flattening either \(X\) or \(Y\).

**Definition 4.3** (Nuisance \(Z\)).: Nuisance to a random variable \(X\) is defined as an equal-size random variable \(Z\) sharing no information with \(X\). Mathematically, \(Z\) satisfies \(I(X;Z)=0\). Nuisance to \((X,Y)\) can be defined similarly where \(Z\) is of the same size as \((X,Y)\) and \(I(X,Y;Z)=0\).

As an example, consider the Gaussian dataset. Its number of information sources \(d_{s}\) is equal to \(d_{g}\), its representation dimension \(d_{r}\) is equal to \(d_{g}\), and the dataset contains no nuisance. The effects of three factors on neural MI estimators will be analyzed in Section 5.

### Theoretical background: same-class sampling for positive pairing

Same-class sampling for positive pairing was proposed in Lee et al. (2023). The key idea is to allow only the class information to be shared between two random variables \(X\) and \(Y\), such that the true MI can be proven to be the same as the entropy of class variable \(C\), _i.e._, \(I(X;Y)=H(C)\). The proofs require mild assumptions of either a lower bound estimate of MI being equal to \(H(C)\) or the existence of an error-free decoder (Theorem 3.1 and 3.2 in Lee et al. (2023)). In Lee et al. (2023), the first mild assumption was shown to be closely satisfied for commonly used image datasets, through extensive empirical evaluations. The second one essentially implies that the true MI is equal to \(H(C)\) when the class information is easily decodable. An example is MNIST dataset whose digit information as the class label is known to be easily decodable. Similarly, for NLP datasets, sentence embeddings of the IMDB dataset (Maas et al., 2011) can be made easily decodable by fine-tuning with the class label \(C\). We carefully construct our unstructured datasets such that we can take advantage of the theoretical results. Once we can employ \(I(X;Y)=H(C)\), the calculation of \(H(C)\) can be made trivial by choosing uniformly distributed class labels. Overall, same-class sampling makes it possible to access the true MI values even for unstructured datasets. For convenience, the theorems and proofs are provided in Supplementary B.1.

### Generating datasets with adjustable true MI values

By utilizing Theorem 3.1 and 3.2 in Lee et al. (2023), it becomes possible to access the true MI of an unstructured dataset by drawing the positive pairs from a joint distribution \(p(x,y)\) where only the class information \(C\) is shared by \(X\) and \(Y\). We first consider a binary random variable \(C\) with \(p(0)=p(1)=0.5\). We can design a simple stochastic function that maps \(C\) to \(X\), where \(X\) is an image or sentence embedding. In our benchmark suite, to make use of the error-free classification function, we choose a dataset that easily achieves perfect classification accuracy with a simple classifier (e.g., 1-layer MLP). We adopt the MNIST dataset (Deng, 2012) for \(_{}\) and BERT (Devlin et al., 2018) fine-tuned sentence embeddings of the IMDB dataset (Maas et al., 2011) for \(_{}\). In our implementation, \(x\) becomes a sample from \(\) of class \(0\) when \(c=0\), and a sample from \(\) of class \(1\) when \(c=1\). We design a mapping function from \(C\) to \(Y\) where a different image or text of same class is drawn. For this basic construction, it can be shown that \(I(X;Y)=H(C)=1\) bit. An image example is shown in Figure 1(a). As a concurrent study, Gowri et al. (2024) also utilized the benchmark of 1-bit datasets with MNIST images and protein sequences to evaluate MI estimators.

To construct a dataset with larger MI, two straightforward approaches can be used. In Figure 1(b), we combine four samples of Figure 1(a) to create an image that is four times larger, which means \(I(X;Y)=4\). In Figure 1(c), we stack three pairs of samples from Figure 1(a) and map them to RGB; hence, \(I(X;Y)=3\). We can adopt flexibly use other stratagems to generate a dataset that has a specific value of true MI. Similarly, we generate the text dataset by concatenating the embedding vectors in 1D.

For images, we can insert random samples from other datasets as nuisance to \(X\) and \(Y\) to make the dataset more realistic without affecting the true MI value, as shown in Figure 1(d). Because the

Figure 2: Four image examples of generating datasets with known true MI values. \(X\) and \(Y\) consist of random images drawn from the MNIST dataset. (a) Basic construction: only the images of class 0 or 1 are considered and \(X\) and \(Y\) are sampled to share the class information. By choosing \(C\) to be either 0 or 1 with probability \(0.5\), \(H(C)\) becomes 1 and therefore \(I(X;Y)=H(C)=1\). (b) Combining four independent images in 2-D to form a single image: \(I(X;Y)=4\) because \(H(C)=H_{}+H_{}+H_{}+H_{}=4\). (c) Combining three independent images in channel dimension to form a color image: \(I(X;Y)=3\) because \(H(C)=H_{}+H_{}+H_{}=3\). (d) Adding nuisance: an independently chosen background image from CIFAR-10 (Krizhevsky et al., 2009) is inserted as nuisance. Because the nuisance is independently chosen for \(X\) and \(Y\), they do not affect the true MI (Lee et al., 2023). Therefore, \(I(X;Y)=1\).

source images remain on top without any occlusion, and there is no fixed relationship between the background chosen for \(X\) and the background chosen for \(Y\), the nuisance \(Z\) does not affect the true \(I(X;Y)\). Although strong nuisances might make some samples difficult to recognize the information sources \(C\), the true MI is based on the overall distribution, not a few outlier samples. Therefore, the presence of nuisances does not affect the true MI. Further discussion will be provided in Section 5.6. While it is possible to introduce nuisance to any dataset based on the formal definition in Definition 4.3, we applied nuisance only to image datasets in this study, as text datasets already follow practical natural language distributions.

For \(_{}\) and \(_{}\), it is trivial to identify the number of information sources \(d_{s}\) and the representation dimension \(d_{r}\). The number of information sources \(d_{s}\) is always equal to \(I(X;Y)\). For instance, Figure 1(b) has \(d_{s}=4\). The representation dimension \(d_{r}\) is a design parameter. As the default, we set \(d_{r}=64^{2}\) for \(_{}\) and \(d_{r}=768 10\) for \(_{}\).

### Manipulating MI to non-integer values: binary symmetric channel

To manipulate the true MI and construct a dataset with a non-integer MI value, we adopt the concept of binary symmetric channel (BSC) (Cover, 1999). BSC is a simple and well known form of noisy communication channel in information theory, and we utilize it for scaling down the true MI value in a fully controlled manner. With BSC, \(X\) is always consistent with the class variable \(C\) but \(Y\) is noisy where it is different from \(C\) with a crossover probability of \(\). Then, the true MI value can be controlled by adjusting \(\) between 0 and 0.5.

**Theorem 4.4** (Manipulating MI to be non-integer).: _When the information source \(C\) is transmitted perfectly to \(X\), while it is transmitted to \(Y\) over a binary symmetric channel (BSC) with a crossover probability \([0,0.5]\), the mutual information \(I(X;Y)\) between \(X\) and \(Y\) is determined as follows._

\[I(X;Y)=H(C)(1-H())\] (1)

\(H()\) refers to the entropy of a binary variable with the crossover probability \(\)(Cover, 1999), and is given by \(H()=--(1-)\). When \(=0\), there is no information loss during transmission. Thus, \(H()=0\) and \(I(X;Y)=H(C)\). When \(=0.5\), the channel is completely noisy, and \(X\) and \(Y\) do not share any information. Thus, \(H()=1\) and \(I(X;Y)=0\). Any MI value in between can be implemented by choosing an appropriate \(\). The proof is provided in Supplementary B.2.

## 5 Empirical investigations

In this section, we investigate seven key aspects that can affect the performance of MI estimators. All investigations are based on our benchmark suite and the empirical findings are reported together. As evaluation metrics, we calculate bias, variance, mean squared error (MSE), and the estimated MI (defined as the average of the estimations) during the training of the critic function. All experiments were conducted on a single NVIDIA GeForce RTX 3090. Detailed experimental setups and raw results are available in Supplementary C and D, respectively.

### Choice of critic architecture: superiority of joint critic for unstructured datasets

Poole et al. (2019) observed that using a joint critic outperforms a separable critic for NWJ and JS estimators, while the InfoNCE estimator demonstrating robustness to the choice of critic architecture. For SMILE (Song and Ermon, 2019) estimator, a joint critic surpassed a separable critic in basic Gaussian setups, but this trend reversed in more complex setups of the same dataset. This section aims to extend these insights into the vision and NLP domains, providing guidance on selecting critic architectures across diverse data contexts.

Figure 3 and Figure 9 in Supplementary D present the results of our experiments, which include scenarios where variables share the same domain (image and image, text and text) as well as cases involving cross-domain pairs (image and text). Our key observations are: (1) The joint critic consistently provides reliable estimations across all estimators and data domains; (2) The bilinear critic, while providing stable yet biased estimations for Gaussian datasets, is notably inaccurate in unstructured datasets; (3) The separable critic performs well on unstructured datasets while it often exhibits large variance for Gaussian cases; (4) Contrary to the theoretical proofs and empirical findings of Song and Ermon (2019) on the high variance of the DV estimator in Gaussian datasets, we observe stable performance in unstructured datasets even with large MI values. Notably, no significant

advantage of SMILE over DV (or MINE) was observed for both images and sentence embeddings. These findings underscore the pronounced differences in analyzing MI estimators between Gaussian and unstructured datasets, highlighting the robustness of the joint critic in diverse contexts.

### Choice of critic capacity: larger capacity does not ensure a higher estimation accuracy

While joint critics often yield the best estimation accuracy in various scenarios, this subsection delves into whether increasing critic capacity could further enhance estimation accuracy. A prior study (Tschannen et al., 2019) posited that larger critic capacities should correlate with more precise estimations. To assess critic capacity, we manipulated the depth of the critic network, with specific results for a true MI of 2 bits outlined in Table 2. (Full results are available in Supplementary D.3.)

Contrary to the assertion of Tschannen et al. (2019), our findings reveal an unexpected trend: no discernible positive correlation between critic capacity and estimation accuracy across any data domain. Specifically, the Pearson's correlation coefficient \(\) between critic depth and estimation accuracy was \(-0.007\) for \(_{}\), \(0.059\) for \(_{}\), and \(-0.001\) for \(_{}\). These findings suggest that an increase in critic capacity does not inherently improve estimation accuracy and may even be counterproductive, contradicting previous assumptions and underscoring the need for a nuanced approach to enhancing critic architectures.

Based on our findings, we fixed the critic architecture as the joint critic of 2-layer MLP for all subsequent sections of this study.

### Choice of MI estimator: no universal winner exists across the three data domains

Recent advancements have introduced more accurate MI estimators, with notable efforts highlighted in Poole et al. (2019); Song and Ermon (2019); McAllester and Stratos (2020); Cheng et al. (2020). Among these, the SMILE estimator has been acclaimed for efficiently reducing the estimation variance compared to other estimators, offering a more favorable bias-variance trade-off (Song and Ermon, 2019). As evidenced in Table 3, the SMILE estimator exhibits a slight superiority over other estimators in Gaussian scenarios and a more pronounced advantage in NLP cases. However,

   Dataset &  &  &  \\  Critic depth & Bias & Variance & MSE & Bias & Variance & MSE & Bias & Variance & MSE \\ 
1 & 0.136 & 0.106 & 0.125 & 0.293 & 0.108 & 0.194 & 0.300 & 0.083 & 0.173 \\
2 & 0.145 & 0.096 & 0.117 & 0.297 & 0.101 & 0.189 & 0.273 & 0.078 & 0.153 \\
3 & 0.124 & 0.094 & 0.114 & 0.302 & 0.102 & 0.194 & 0.269 & 0.077 & 0.150 \\
4 & 0.140 & 0.095 & 0.114 & 0.302 & 0.103 & 0.195 & 0.270 & 0.078 & 0.151 \\
5 & 0.145 & 0.096 & 0.117 & 0.311 & 0.105 & 0.202 & 0.278 & 0.081 & 0.158 \\   

Table 2: Estimation bias/variance/MSE when the true MI is 2 bits. All estimators showed the same trends, so we report the result of the case of SMILE-inf with the joint critic.

   Dataset &  &  & DV &  & MINE &  &  & SMILE-inf \\  Gaussian & 2 & 0.142 & 0.117 & 0.121 & 0.116 & 0.139 & **0.115** & 0.117 \\
**Gaussian** & 4 & 0.242 & 0.245 & 0.448 & 0.212 & 0.420 & **0.160** & 0.174 \\
**Gaussian** & 6 & 0.357 & 0.141 & 0.260 & 0.459 & 0.479 & **0.258** & 0.544 \\
**Gaussian** & 8 & 4.446 & 0.240 & 0.230 & 0.189 & 1.087 & **0.598** & **0.598** \\
**Gaussian** & 10 & 8.895 & 1.067 & 1.068 & 1.973 & 1.440 & 1.053 & **1.202** \\
**Gaussian** & 2 & 0.288 & 0.175 & 0.179 & 0.237 & **0.442** & 0.191 & 0.189 \\
**Gaussian** & 4 & 0.357 & 0.213 & 0.479 & 0.260 & 0.238 & **0.229** & 0.229 \\
**Gaussian** & 6 & 0.577 & 0.260 & 1.542 & 0.582 & 0.557 & **0.210** & 0.572 \\
**Gaussian** & 10 & **1.589** & 0.529 & 0.526 & 0.526 & 0.526 & 0.526 & 0.526 \\
**Gaussian** & 10 & **1.589** & 0.529 & 0.526 & 0.526 & 0.526 & 0.526 & 0.526 \\  Test & 2 & 0.150 & 0.143 & 0.162 & 0.147 & **0.097** & 0.155 & 0.133 \\ Test & 4 & 0.356 & 0.312 & 0.383 & 0.383 & 0.384 & 0.384 & 0.384 & 0.385 \\ Test & 5 & 1.349 & 1.092 & 1.758 & 1.108 & 0.397 & **0.274** & 1.159 \\ Test & 10 & 2.459 & 2.345 & 1.727 & 1.027 & **0.319** & 0.179 & 2.456 \\   

Table 3: Estimation error (MSE) with the joint critic. For each dataset and true MI, the best cases are marked in **bold**.

Figure 3: Estimation results for four different benchmarks with \(d_{s}=10\). Following the experimental setup of Poole et al. (2019), we change the true MI values stepwise and the other hyperparameters are fixed.

in vision cases with large true MI values, the NWJ and MINE estimators demonstrate superior performance compared to SMILE. These findings indicate the absence of a universally optimal estimator, highlighting the necessity of context-specific selection for MI estimation across various data domains.

Number of information sources (\(d_{s}\)): unstructured datasets outperform Gaussian in handling larger \(d_{s}\)

In this subsection, we explore the influence of the number of information sources (\(d_{s}\)), as previously defined in Section 4.2, on the accuracy of MI estimation. We incrementally increase \(d_{s}\) from 1 to 100, observing the effects on estimation accuracy. As shown in Figure 4, estimation accuracy deteriorates when \(d_{s}\) becomes excessively large across all data domains.

Interestingly, we observed domain-specific thresholds where MI estimators begin to falter: estimations become unreliable when \(d_{s}\) exceeds 4 in the Gaussian case, 36 in the vision case, and 64 in the NLP case, approximately. This suggests that unstructured datasets, unlike the Gaussian datasets, allow for relatively accurate MI estimations with moderate \(d_{s}\) values within the range of \(\).

Given that a uniformly distributed classification problem typically involves classes much less than 10M (significantly lower than \(2^{36}\)), these findings indicate that large \(d_{s}\) values might not be a limiting factor in practical applications.

### Representation dimension (\(d_{r}\)): it does not affect the estimation accuracy

Real-world datasets can have any representation dimension while having a fixed number of information sources. For example, the image datasets in Figure 2 can be represented in any reasonable dimension without compromising the semantic information. To analyze the MI estimation accuracy in these scenarios, we investigate a range of representation dimensions \(d_{r}\) for a fixed number of information sources \(d_{s}\) and the MI value \(I(X;Y)\). Specifically, we simply resize the images of size \(64^{2}\) in Figure 2 using a linear interpolation function to obtain images whose size ranges between \(10^{2}\) and \(100^{2}\) while keeping \(d_{s}\) and \(I(X;Y)\) fixed.

As shown in Table 4, we observe that the representation dimension does not affect estimation accuracy for images. Even as \(d_{r}\) increases to 10000, we found that almost all the estimators provide accurate estimations. From these results, we conclude that the representation dimension alone does not influence MI estimation accuracy.
6 bits). These tight estimations empirically support our theoretical background that \(I(X;Y)=H(C)\) when the nuisance exists, as suggested by Theorem 3.1 in Lee et al. (2023). MINE uniquely offers nonzero estimations even in the presence of the largest nuisance (\(=1\)). High nuisance strength may complicate the learning process resulting in increased bias and variance. Consequently, all estimators fail to maintain accuracy under high nuisance conditions, highlighting the need for more robust methods to handle such scenarios.

### Network and layer dependency: estimation holds for invertible networks and upper layers

Our final investigation focuses on the accuracy of MI estimators in the context of deep representations (i.e., \(I(g(X);g(Y))\), where \(g\) represents a deep network) because of the prevalent interest in understanding dependencies between representations rather than raw inputs. While Czyz et al. (2023) highlighted concerns about the reliability of estimators in datasets with tractable distribution functions and under specific unrealistic transformations, our study expands the horizon by examining three invertible networks: MAF (Papamakarios et al., 2017), RealNVP (Dinh et al., 2016), and i-RevNet (Jacobsen et al., 2018) for images and texts. For \(_{}\), we additionally investigate a non-invertible ResNet-50 network, a widely used non-invertible network, which is pre-trained on the MNIST dataset, to provide more relevant insights for practitioners. This allows us to better align with practical interests in MI estimation beyond invertible networks. Remarkably, as demonstrated in Figure 11 in supplementary material, we found that estimation robustly persists for the representations of \(_{}\) and \(_{}\) across various network architectures.

If deep representations are robust for estimating MI, should this hold across all layers? To address this question, we estimated MI for intermediate layers of ResNet-50 trained on \(_{}\) without nuisance. Results are summarized in Figure 6. According to the data processing inequality, lower-layer MI cannot be smaller than upper-layer MI. However, estimated MI values indicate the opposite. This discrepancy suggests that MI estimations at lower layers are less precise, whereas upper-layer representations yield more accurate estimations. Interestingly, we observe the step-wise estimation results; the transition clearly occurs when the output size changes across all types of estimators. It appears that upper layers might capture abstract, high-level features, potentially offering more meaningful information for MI estimation. In contrast, lower layers might contain more noise and less discriminative features, which could lead to poorer accuracy.

## 6 Discussion and Conclusion

Quantifying complex dependency between variables is an essential topic in machine learning. In this realm, sample-based neural MI estimators have been the primary choice for many deep learning

Figure 5: (a) Example of inserting nuisance to \(_{}\). (b) Estimation results when the true MI is 2 bits. (c) Estimation results with various values of nuisance strength and true MI for three best-performing estimators. True MI values are on the x-axis and the nuisance strength is on the y-axis.

Figure 6: Estimation results for hidden layers of ResNet-50. Dashed lines indicate boundaries between stages. For full results, see Supplementary D.4.

applications. The MI estimators have been directly used for improving downstream task performance or indirectly used for motivating learning method developments. However, there has been hardly any attempt to evaluate the accuracy of these MI estimators over real-world datasets such as images and texts. In this study, we proposed a novel benchmark suite for evaluating neural MI estimators on unstructured datasets, where the underlying distribution functions are not accessible. Our findings reveal discrepancies in estimation accuracy between traditional Gaussian benchmarks and unstructured data scenarios, highlighting the limitations of Gaussian benchmarks in capturing the nuances of MI estimation in practical settings. Notably, our findings on unstructured datasets demonstrate that MI estimators can yield remarkably accurate results, particularly in conjunction with deep representations, indicating their potential to continue driving advancements in deep learning research. While our study does not cover the entire spectrum of real-world datasets, it signifies a substantial step forward in evaluating and understanding MI estimators beyond purely statistical datasets. We hope that this benchmark suite not only offers a new standard for evaluating MI estimators but also catalyzes further research, enriching our comprehension of MI across a diverse data domains.

AcknowledgementThis work was supported by the following grants funded by the Korea government: NRF (NRF-2020R1A2C2007139, NRF-2022R1A6A1A03063039) and IITP ([NO.2021-0-01343, Artificial Intelligence Graduate School Program (Seoul National University)], [No. RS-2023-00235293]).