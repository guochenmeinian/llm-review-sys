# Task-Agnostic Machine-Learning-Assisted Inference

Jiacheng Miao

University of Wisconsin-Madison

jiacheng.miao@wisc.edu &Qiongshi Lu

University of Wisconsin-Madison

qlu@biostat.wisc.edu

###### Abstract

Machine learning (ML) is playing an increasingly important role in scientific research. In conjunction with classical statistical approaches, ML-assisted analytical strategies have shown great promise in accelerating research findings. This has also opened a whole field of methodological research focusing on integrative approaches that leverage both ML and statistics to tackle data science challenges. One type of study that has quickly gained popularity employs ML to predict unobserved outcomes in massive samples, and then uses predicted outcomes in downstream statistical inference. However, existing methods designed to ensure the validity of this type of post-prediction inference are limited to very basic tasks such as linear regression analysis. This is because any extension of these approaches to new, more sophisticated statistical tasks requires task-specific algebraic derivations and software implementations, which ignores the massive library of existing software tools already developed for the same scientific problem given observed data. This severely constrains the scope of application for post-prediction inference. To address this challenge, we introduce a novel statistical framework named PSPS for task-agnostic ML-assisted inference. It provides a post-prediction inference solution that can be easily plugged into almost any established data analysis routines. It delivers valid and efficient inference that is robust to arbitrary choice of ML model, allowing nearly all existing statistical frameworks to be incorporated into the analysis of ML-predicted data. Through extensive experiments, we showcase our method's validity, versatility, and superiority compared to existing approaches. Our software is available at https://github.com/qlu-lab/psps.

## 1 Introduction

Leveraging machine learning (ML) techniques to enhance and accelerate research has become increasingly popular in many scientific disciplines . For example, sophisticated deep learning models have achieved remarkable success in predicting protein structure and interactions, which has the potential to significantly speed up the research process, save costs, and revolutionize the field of structural biology [1; 2; 25]. However, recent studies have pointed out that statistical inference using ML-predicted outcomes may lead to invalid scientific discoveries due to the lack of consideration of ML prediction uncertainty in traditional statistical approaches. To address this, researchers have introduced methods that couple extensive ML predictions with limited gold-standard data to ensure the validity of ML-assisted statistical inference [3; 35; 46].

Despite these advances, current ML-assisted inference methods can only address very basic statistical tasks, including mean estimation, quantile estimation, and linear and logistic regression [3; 35]. While the same mathematical principle behind existing ML-assisted inference methods can be generalized to a broader class of M-estimation problems, specific algebraic derivations and computational implementations are required for each new statistical task. Moreover, many tasks, such as the Wilcoxon rank-sum test, do not fit into the M-estimation framework. These issues pose significant challenges to the broad application of ML-assisted inference across various scientific domains.

Historically, the field of statistics has faced similar types of challenges. Before the advent of resampling-based methods , it used to require task-specific derivation and implementation to obtain the variance of any new estimator. This old problem mirrors the current state of ML-assisted inference, where every new task requires non-trivial effort from researchers. However, with resampling-based inference, the need to manually derive variance is reduced. Instead, resampling methods can be universally applied to many estimation problems and easily obtain variance [17; 18; 19]. Inspired by this, we seek a universal approach that incorporates ML-predicted data into any existing data analysis routines while ensuring valid inference results.

We introduce a simple protocol named **PoSt-Prediction**S**ummary-statistics-based (PSPS) inference (Fig. 1). It employs existing analysis routines to generate summary statistics sufficient for ML-assisted inference, and then produces valid and powerful inference results using these statistics. It has several key features:

* _Assumption-lean and data-adaptive_: It inherits the theoretical guarantees of validity and efficiency from state-of-the-art ML-assisted inference methods [4; 20; 35]. These guarantees hold with arbitrary ML predictions.
* _Task-agnostic and simple_: Since our method only requires summary statistics from existing analysis routines, it can be easily adapted for many statistical tasks currently unavailable or difficult to implement in ML-assisted inference.
* _Federated data analysis_: It does not need any individual-level data as input. Sharing of privacy-preserving summary statistics is sufficient for real-world scientific collaboration.

## 2 Problem formulations

### Setting

We focus on statistical inference problems for the parameter \(^{*}^{*}()^{K}\) defined on the joint distribution of \((,Y)\), where \(Y\) is a scalar outcome and \(\) be a \(K\)-dimensional vector representing features. We are interested in estimating \(^{*}\) using labeled data \(=\{(_{i},Y_{i}),i=1,,n\}(_{ },Y_{})\), unlabeled data \(=\{_{i},i=n+1,,n+N\}_{}\), and a pre-trained ML model \(():\). Here, \(f()\) is a black-box function with unknown operating characteristics and can be mis-specified. We also require an algorithm \(\) that inputs the labeled data \(\) and returns a consistent and asymptotically normally distributed estimator \(\) for \(^{*}\). There are three common ways in the literature to estimate \(^{*}\):

* **Classical statistical methods** apply algorithm \(\) to only labeled data \(=(_{},Y_{})\), and returns the estimator and its estimated variance \([}_{},}( }_{})]\). Valid confidence intervals and hypothesis tests can then be constructed using the asymptotic distribution of the estimator. However, it ignores the unlabeled data and ML prediction.
* **Imputation-based methods** treat ML prediction \(\) in the unlabeled data as the observed outcome, and apply algorithm \(\) to \(=(_{},_{})\). We denote the estimator and estimated variance as \([}_{},}( {}_{})]\). This has been shown to give invalid inference results and false scientific findings [3; 35; 36; 46].
* **ML-assisted inference methods** use both \(\) and \(\) as input. These approaches add a debiasing term in the loss function (or estimating equation) for M-estimation problems, thus removing the bias from the imputation-based estimators and producing results that are statistically valid and universally more powerful compared to classical methods [4; 35; 36].

Next, we use an example to provide intuition on ML-assisted inference and our protocol.

### Building the intuition with mean estimation

We consider the mean estimation problem, where \(^{*}=[Y_{i}]_{}[ (Y_{i}-)^{2}]\). The classical method only takes the labeled data \(Y_{}\) as input and yields an unbiased and consistent estimator for \(^{*}\): \(_{}=_{}_{ i=1}^{n}(Y_{i}-)^{2}=_{i=1}^{n}Y_{i}\). The imputation-based method only takes the unlabeled data \(_{}\) as input and returns \(_{}=_{}_{i=n+1}^{n +N}(_{i}-)^{2}=_{i=n+1}^{n+N} {f}_{i}\). It is a biased and inconsistent estimator for \([Y_{i}]\) if the ML model \(\) is mis-specified.

To address this, ML-assisted estimator takes both labeled data \((Y_{},_{})\) and unlabeled data \(_{}\) as input and adds a debiasing term to the loss function to rectify the bias caused by ML imputation :

\[_{} =_{}\{_{0}_{i=n+1}^{n+N}(_{i}-)^{2}-_{0}_{i=1}^{n}(_{i}-)^ {2}-_{i=1}^{n}(Y_{i}-)^{2}]}_{}\}\] \[=_{0}_{i=n+1}^{n+N}_{i}- _{0}_{i=1}^{n}_{ i}-_{i=1}^{n}y_{i}]}_{},\]

where the modified loss ensures the consistency of the ML-assisted estimator and the weight \(_{0}=}_{i}[Y,]/n}{ {}_{i}[f]/n+}_{u}[]/N}\) ensures that ML-assisted estimator is no less efficient than the classical estimator with arbitrary ML predictions: \((_{})=(_{ })-[Y,]}{n\,[] +n^{2}\,[]/N}(_{ })\).

Our proposed method is motivated by the observation that the **sufficient statistics** of the ML-assisted estimator \(_{}\) and its estimated variance \(}(_{})\) are the following summary statistics:

\[_{}=(_{i=1}^{n}y_{i},_{i=1}^{n}_{i},_{i=n+1}^{n+N }_{i})}(_{})= }_{t}[Y]/n&}_{t}[Y, ]/n&0\\ }_{t}[Y,]/n&}_{t}[ ]/n&0\\ 0&0&}_{u}[]/N\]

Moreover, they can be easily obtained by applying the **same algorithm**\(\) (mean estimation) to

* labeled data with observed outcome \((Y_{})[_{}, }(_{})]=[ _{i=1}^{n}y_{i},}_{t}[Y]/n]\)
* labeled data with predicted outcome \((_{})[_{},}(_{})]=[ _{i=1}^{n}_{i},}_{t}[]/n]\)
* unlabeled data with predicted outcome\((_{})[_{ },}(_{})]=[_{i=n+1}^{n+N}_{i},}_{u}[]/N]\)
* bootstrap of labeled data \([(Y_{},_{})_{q},q=1,,Q]\) for estimation of \(}(_{},_{ })=}_{t}[Y,]/n\). Here, \((Y_{},_{})_{q}\) represents the \(q\)-th bootstrap of labeled data.

Combining these summary statistics for one-step debiasing \(_{0}_{}-(_{0} _{}-_{})\) recovers \(_{}\).

To summarize, an algorithm for mean estimation, coupled with resampling, is sufficient for ML-assisted mean estimation. This observation inspired us to generalize this protocol for a broad range of tasks. Our protocol illustrated in Fig. 1 only requires three steps: 1) using a pre-trained ML model to predict outcomes for labeled and unlabeled data, 2) applying existing analysis routines to generate summary statistics, and 3) using these statistics in a debiasing procedure to produce statistically valid results in ML-assisted inference.

Figure 1: Workflow of PSPS for Task-Agnostic ML-Assisted Inference.

### Related work

Our work is closely related to recent methods developed in the literature of ML-assisted inference [3; 4; 20; 35; 37; 46; 56], and is also related to methods for handling missing data [40; 42] and semi-supervised inference [6; 16; 50; 52]. While current ML-assisted inference methods modify the loss function or the estimating equation, our protocol works directly on the summary statistics. For simple problems such as mean estimation, current methods yield a closed-form solution to the optimization problem. However, for more general statistical tasks, there is no such closed-form solution. Current methods typically require the algebraic form of the loss function, its first- and second-order derivatives, and the variance for the estimator, as well as a newly implemented optimization algorithm to obtain the estimator. We use the logistic regression problem as an example. Here, \(^{*}=_{}[-Y() ^{}-()]\) and \((t)=1/(1+(-t))\). The ML-assisted estimator is \(_{}=_{} _{i=n+1}^{n+N}[-_{i}^{} _{i}^{}-(_{i})]- _{i=1}^{n}[-_{i}^{}_{i}^{ {T}}-(_{i})]\) with estimated asymptotic variance \(}^{-1}}()}^{-1}\), where \(}=(_{i=1}^{n+1}^{ }(_{i})_{i}^{}_{i }+_{i=n+1}^{n+N}^{}(_{i})_{i}^{}_{i})\), \(}()=[^{2} {Var}_{n}((^{}(_{i})-_{i}) _{i}^{})+}_{N+n}((1- )^{}(_{i})_{i}^{ }+(_{i}-Y_{i})_{i}^{})\), and \(\) needs to be obtained by optimization to minimize the asymptotic variance. In contrast, our protocol simply applies logistic regression \(\) to

* labeled data with observed outcomes \((_{},Y_{})\) to obtain \([}_{},}(}_{})]\)
* labeled data with predicted outcomes \((_{},_{})\) to obtain \([}_{},}(}_{})]\)
* unlabeled data with predicted outcomes \((_{},_{})\) to obtain \([}_{},}(}_{})]\)
* bootstrap of labeled data \((_{},Y_{},_{})_{q},q=1, ,Q\) for \(}(}_{},}_{})\),

and returns \(}_{0}^{}}_{}-(_{0}^{}}_{}-}_ {})\), where \(}_{0}=(}(}_{ })+}(}_{})) ^{-1}}(}_{}, }_{})\). For each new statistical task, as long as an existing analysis routine can produce an estimator that is asymptotically normally distributed, our protocol can be similarly applied. Additionally, the current mathematical principles guiding ML-assisted inference apply solely to M-estimation [3; 4; 20; 35; 56]. Our protocol extends beyond this limitation, addressing all estimation problems with an asymptotically normally distributed estimator.

Inference relying solely on summary statistics is widely used in the statistical genetics literature for practical reasons. Summary statistics-based methods have been developed for tasks such as variance component inference and genetic risk prediction [11; 12; 34; 39; 53]. In contrast to our work, these applications do not leverage ML predictions, but instead focus on inference using summary statistics obtained from observed outcomes. An exception is a previous study for valid genome-wide association studies (GWAS) on ML-predicted outcome . However, it focused only on linear regression modeling with application to GWAS. The PSPS framework introduced in this paper aims to extend ML-assisted inference to general statistical tasks.

Our work is also related to semi-supervised learning, resampling-based inference, zero augmentation, and false discovery rate (FDR) control methods. Our protocol is designed for estimation and statistical inference using both labeled and unlabeled data, addressing a different problem from semi-supervised learning , which primarily focuses on prediction. Our protocol is inspired by the core principle of resampling-based inference, which replaces algebraic derivation with computation . The main difference is that we focus on how to use ML to support inference, whereas resampling-based inference focuses on bias and variance estimation, and type-I error control. The idea of zero augmentation has been used in augmented inverse propensity weighting estimators  and in handling unmeasured confounders  and missing data for U-statistics . These estimators do not incorporate ML, which is fundamental to our work. We also adapt techniques from the FDR literature [7; 8; 9; 10]. Our unique contribution is to use ML to support FDR control, thereby increasing its statistical power, in contrast to classical methods that rely solely on labeled data.

Methods

### General protocol for PSPS

Building on Section 2, we formalized our protocol in Fig. 1 for ML-assisted inference:

```
0: A pre-trained ML model \(\), labeled data \(=(_{},Y_{})\), unlabeled data \(=_{}\)
1: Use the ML model \(\) to predict the outcome in both labeled and unlabeled data.
2: Apply the algorithm \(\) in the analysis routine to * labeled data \((_{},Y_{})\) and obtain \([}_{},}(}_{})]\) * labeled data \((_{},_{})\) and obtain \([}_{},}(}_{})]\) * unlabeled data with \((_{},_{})\) and obtain \([}_{},}(}_{})]\) * \(Q\) bootstrap of labeled data \((_{},Y_{},_{})_{q},q=1, ,Q\) and obtain \(}(}_{},}_{})\).
3: Employ one-step debiasing to the summary statistics in step2: \[}_{}=}_ {0}^{}}_{}-(}_{0}^{}}_{}-}_{}),\] where \(}_{0}=[}(}_{})+}(}_{})]^{-1}}(}_{},}_{})\) and \(}(}_{})=}(}_{})-}(}_{},}_{})^{}[}( }_{})+}(}_{})]^{-1}}(}_{},}_{})\)
4: ML-assisted point estimator \(}_{}\), standard error \(}(}_{})}\), \(\)-level confidence interval for the \(k\)-th coordinate \(_{,k}^{}=(}_{ _{k}} z_{1-/2}}(}_{})_{kk}})\), and (two-sided) p-value \(2(1-(|}_{_{k}}}{(}_{})_{kk}}}|))\), where \(\) is the CDF of the standard normal distribution. ```

**Algorithm 1**PSPS for ML-assisted inference

The only requirements for our protocol are: i) algorithm \(\), when applied to labeled data \((_{},Y_{})\), returns a consistent and asymptotically normally distributed estimator of \(^{*}\); ii) labeled and unlabeled data are independent and identically distributed. Under these assumptions, the summary statistics have the following asymptotic properties:

\[n^{1/2}(}_{}- ^{*}\\ }_{}-\\ }_{}- )\{( _{K}\\ _{K}\\ _{K}),((}_{})(}_{},}_{}) \\ (}_{}, }_{})(}_{}) \\ (}_{ }))\},\] (1)

where \((_{}) ^{K}\) is defined on \((,)_{}\), \(()\) denotes the asymptotic variance and covariance of a estimator, and \(=\). The asymptotic approximation gives \((}_{}) n\,( }_{}),(}_{},}_{}) n\, (}_{},}_{}),(}_{})  n\,(}_{})\) and \((}_{}) N\,( }_{})\). Here, we do not require \(}_{}\) and \(}_{}\) to be consistent for \(^{*}\), thus allows arbitrary ML model.

With the summary statistics following a multivariate normal distribution asymptotically, the debiased estimator \(}_{}=}_ {0}^{}}_{}-(}_{0}^{}}_{}-}_{})\) is consistent for \(^{*}\) and asymptotically normally distributed (Theorem 1). Therefore, by plugging in a consistent estimator for its asymptotic variance \((}_{}) n\,( }_{})\), valid confidence interval and hypothesis testing can be achieved.

_Remark 1_.: PSPSPS is more "task-agnostic" than existing methods in three aspects:

1. For M-estimation tasks, currently, only mean and quantile estimation, as well as linear, logistic, and Poisson regression, have been implemented in software tools and are ready for immediate application. For other M-estimation tasks, task-specific derivation of the ML-assisted loss functions and asymptotic variance via the central limit theorem are necessary. After that, researchers still need to develop software packages and optimization algorithms to carry out real applications. In contrast, PSPS only requires already implemented algorithms and software designed for classical inference based on labeled data.

2. For problems that do not fall under M-estimation but have asymptotically normally distributed estimators, only \(\) can be applied, and all current methods would fail. The principles behind ML-assisted M-estimation do not extend to these tasks.
3. Even for M-estimation tasks that have already been implemented, \(\) offers the additional advantage of relying solely on summary statistics. The "task-specific derivations" refer not only to statistical tasks but also to scientific tasks. Real-world data analysis in any scientific discipline often involves conventions and nuisances that require careful consideration. For example, our work is partly motivated by GWAS . Statistically, GWAS is a linear regression that regresses an outcome on many genetic variants. While the regression-based statistical foundation is simple, conducting a valid GWAS requires accounting for numerous technical issues, such as sample relatedness (i.e., study participants may be genetically related) and population structure (i.e., unrelated individuals of the same ancestry are both genetically and phenotypically similar, creating confounded associations in GWAS). Sophisticated algorithms and software have been developed to address these complex issues . It will be very challenging if all these important features need to be reimplemented in an ML-assisted GWAS framework. With our \(\) protocol, researchers can utilize existing tools that are highly optimized for genetic applications to perform ML-assisted GWAS. This adaptability is not just limited to GWAS, but is a major feature of our approach across scientific domains. \(\) enables researchers to conduct ML-assisted inference using well-established data analysis routines.

_Remark 2_.: The "federated data analysis" feature of \(\) refers to the fact that we only require summary statistics as input for inference, rather than individual-level raw data (features \(\) and label \(Y\)). For example, consider a scenario where labeled data is in one center and unlabeled data is in another, yet researchers cannot access individual-level data from both centers simultaneously. Under such conditions, current ML-assisted inference, which relies on accessing both labeled and unlabeled data to minimize a joint loss function, is not feasible. However, \(\) circumvents this issue by aggregating summary statistics from multiple centers, thereby performing statistical inference while upholding the privacy of individual-level data.

### Theoretical guarantees

In this section, we examine the theoretical properties of \(\). In what follows, \(\) denotes convergence in probability and \(\) denotes convergence in distribution. All proofs are deferred to the Appendix A.

The first result shows that our proposed estimator is consistent, asymptotically normally distributed, and uniformly better in terms of element-wise asymptotic variance compared with the classical estimator based on labeled data only.

**Theorem 1**.: _Assuming equation (1) holds, then \(}_{}^{*}\), and_

\[n^{1/2}(}_{}-^{*})(,(}_{ })),\]

_where \((}_{})=(}_{})-(}_{}, }_{})^{}((}_{})+(}_{}))^{-1} (}_{},}_{})\). Assume the \(k\)-th column of \((}_{},}_{})\) is not a zero vector and at least one of \((}_{})\) and \((}_{})\) are positive definite, then \((}_{})_{kk}( {}_{})_{kk}\). With \(}(}_{}) (}_{}),_{n}(_{ k}^{*}_{,k}^{})=1-\)._

\(}(}_{})\) can be obtained by applying the algebraic form of \((}_{})\) using the bootstrap estimators for \((}_{}),( }_{}),(}_{},}_{}),\) and \((}_{})\). The regularity conditions for consistent bootstrap variance estimation are outlined in Theorem 3.10 (i) of . We also refer readers to , which showed that bootstrap-based variance provides valid but potentially conservative inference.

This result indicates that a greater reduction in variance for the ML-assisted estimator is associated with larger values of \((}_{},}_{})\) and smaller values of \((}_{})\), \((}_{})\), and \(\). The variance reduction term \([(}_{},}_{})^{}((}_{})+( }_{}))^{-1}(}_{ },}_{})]_{kk}\) can also serve as a metric for selecting the optimal ML model in ML-assisted inference.

Our next result shows that three existing methods, i.e., PPI, PPI++, and PSPA, are asymptotically equivalent to PSPS with different weighting matrices. A broader class for consistent estimator of \(^{*}\) is \(}()=^{}}_{ }-(^{}}_{}- }_{})\), where \(\) is a \(K K\) matrix. The consistency of \(}()\) for \(^{*}\) only requires \(^{}(}_{}- }_{})\). Since \((}_{}-}_{})\), assigning arbitrarily fixed weights for will satisfy the condition. However, the choice of weights influences the efficiency of the estimator as illustrated in Proposition 2 later.

**Proposition 1**.: _Assuming equation (1) and regularity condition for the asymptotic normality of current ML-assisted estimator holds. For any M-estimation problem, we have_

\[n^{}(}((_{}))-}_{}),n^{ }(}((_{}) )-}_{}),n^{}(}(() )-}_{}).\]

_Here, \(_{}=[_{,1},,_{,K}]^ {}^{K}\) and \(_{,k}\) minimizing the \(k\)-th diagonal element of \((}())\), \(_{tr}\) is a scalar used to minimize the trace of \((}())\), and \(\) is a matrix associated with the second derivatives of the loss function in M-estimation, with further details deferred to Appendix A._

This demonstrates that for M-estimation problems, our method is asymptotically equivalent to PSPA, PPI++, and PPI with the respective weights \((_{})\), \((_{})\), and \(()\). Therefore, PSPS can be viewed as a generalization of these existing methods.

Our third result shows that the weights used in the Proposition 1 are not optimal. Instead, our choice of \(_{0}\) represents the optimal smooth combination of \((}_{},}_{}, }_{})\) in terms of minimizing the asymptotic variance, while still preserving consistency.

**Proposition 2**.: _Suppose \(n^{1/2}(g(}_{},}_{},}_{})-^{*})(0,_{g})\) and \(g\) is a smooth function, then \(_{g_{kk}}_{_{kk}}\)_

Together with Proposition 1, our results demonstrate that our protocol provides a more efficient estimator compared to existing methods for the M-estimation problems. Furthermore, the applicability of our protocol is not limited to M-estimation and only requires summary statistics as input. It also indicates that in a setting of federated data analysis  where individual-level data are not available, PSPS proves to be the optimal approach for combining shared summary statistics.

_Remark 3_.: PPI++ employs a power-tuning scalar for variance reduction in ML-assisted inference. This scalar is obtained by minimizing the trace or possibly other scalarization of the estimator's variance-covariance matrix. However, the asymptotic variance of PSPS is always equal to or smaller than that of PPI++, irrespective of the scalarization chosen by researchers. This advantage arises because PSPS utilizes a \(K K\) power tuning matrix, \(\), for variance reduction, where \(K\) represents the dimensionality of parameters. This matrix facilitates information sharing across different parameter coordinates, thereby enhancing estimation precision. The choice of weighting matrix in PSPS also allows for element-wise variance reduction, reducing each diagonal element of the variance-covariance matrix. In contrast, the single scalar in PPI++ can only target overall trace reduction or variance reduction of a specific element. A detailed example is provided in Appendix B. Only in one-dimensional parameter estimation tasks, such as mean estimation, PPI++ and PSPS exhibit the same asymptotic variance.

### Extensions

We also provide several extensions to ensure the broad applicability of our method.

#### 3.3.1 Labeled data and unlabeled data are not independent

Here, we relax the assumption that the labeled data and unlabeled data are independent. When they are not independent, this can lead to the non-zero covariance between the \(}_{}\) and \(}_{}\). Consider a broader class of summary statistics asymptotically satisfying

\[n^{1/2}(}_{}-^ {*}\\ }_{}-) \{(_{K}\\ _{K}\\ _{K}),((( }_{})&(}_{ },}_{})&(}_{ },}_{})\\ (}_{},}_{})& (}_{})&(}_{},}_{})\\ (}_{},}_{})& (}_{},}_{ })&(}_{}) ))\}\]We can similarly employ the one-step debiasing \(}_{}^{}=}_{0}^{ }}_{}-}_{0}( {}_{}-})\) where \(}_{0}=(}(}_{ },}_{})-}( {}_{},}_{}))^{}( }(}_{})+}( }_{})-2}( }_{},}_{}))^{-1}\) and \(}(}_{}^{})=}(}_{})-( {}(}_{},}_{ })-}(}_{}, {}_{}))^{}((}_{ })+}(}_{})-2 }(}_{},}_ {}))^{-1}(}(}_{},}_{})-}(}_ {},}_{}))\). The theoretical guarantees of the proposed estimator can be similarly derived by Theorem 1.

#### 3.3.2 Sensitivity analysis for distributional shift between labeled and unlabeled data

The other assumption of our approach is that the labeled and unlabeled data are identically distributed so that we can ensure \(}_{}-}_{} {}\) and validity of PSPS results. To address the potential violation of this assumption, we introduce a sensitivity analysis with hypothesis testing for the null \(H_{0}:_{,k}=_{,k}\) with test statistics \(}_{,k}-}_{,k}} {(}_{,k})+ }(}_{,k})}}(0,1)\) to assess if \(_{,k}\) and \(_{,k}\) are significantly different. Here, the subscript \(k\) indicates the \(k\)-th coordinate. We recommend using p-value \(<0.1\) as evidence for heterogeneity and caution the interpretation of results from ML-assisted inference.

#### 3.3.3 ML-assisted FDR control

The output from PSPS can be used for ML-assisted FDR control, achieving greater statistical power compared to classical FDR control methods that solely rely on labeled data. We refer to our approach as PSPS-BH and PSPS-knockoff. Briefly, PSPS-BH processes the p-value from ML-assisted linear regression through the Benjamini-Hochberg (BH) procedure Benjamini and Hochberg (1995), while PSPS-knockoff utilizes the ML-assisted debiased Lasso coefficient Benjamini and Hochberg (1995); Benjamini and Hochberg (1995) in the ranking algorithm of knockoff Benjamini and Hochberg (1995). We present our algorithm in Appendix C and evaluate their performance using experiments in Section 4.

#### 3.3.4 ML-assisted inference with predicted features

We have discussed ML-assisted inference with outcomes predicted by ML models. Here, we note that PSPS can also be applied when either features alone are predicted or both features and outcomes are predicted. The key idea is that the difference between point estimators obtained from applying \(\) to predicted features in both labeled and unlabeled datasets is a consistent estimator for zero. This enables zero augmentation for estimators from observed features and outcomes. To implement this, modify step 2 in Algorithm 1 to apply \(\) to predicted features in both labeled and unlabeled data. A similar approach is applicable when both features and outcomes are predicted.

## 4 Numerical experiments and real data application

### Simulations

We conduct simulations to assess the finite sample performance of our method. Our objectives are to demonstrate that 1) PSPS achieves narrower confidence intervals when applied to statistical tasks already implemented in existing ML-assisted methods; 2) when applied to statistical tasks that have not been implemented for ML-assisted inference, PSPS provides confidence intervals with narrower width but correct coverage (indicating higher statistical power) compared to classical approaches rely solely on labeled data; 3) PSPS provides well-calibrated FDR control and achieves higher power compared to classical methods only using labeled data.

**Tasks that have been implemented for ML-assisted inference** We compare PSPS with the classical method using only labeled data, the imputation-based method using only unlabeled data, and three ML-assisted inference methods PPI, PPI++, and PSPAHao et al. (2019); Li et al. (2019); Li et al. (2019) on mean estimation and linear and logistic regression. We defer the detailed data-generating process to Appendix D. In short, we generated outcome \(Y_{i}\) from feature \(X_{1i}\) and \(X_{2i}\), and obtained the pre-trained random forest that predict \(Y_{i}\) using \(X_{1i}\) and \(X_{2i}\). We have 500 labeled samples \((X_{1i},Y_{i},_{i})\), and unlabeled samples \((X_{1i},_{i})\) ranged from 1,000 to 10,000. Our goal is to estimate the mean of \(Y_{i}\), as well as the linear and logistic regression coefficient between \(Y_{i}\) and \(X_{1i}\).

Fig. 1(a)-c show confidence interval coverage and Fig. 1(d)-f show confidence interval width. We find that the imputation-based method fails to obtain correct coverage, while all others including PSPS have the correct coverage. PSPS has narrower confidence intervals compared to the classical method and other approaches for ML-assisted inference.

**Tasks that have not been implemented for ML-assisted inference** Next, we consider several commonly used statistical tasks that currently lack implementations for ML-assisted inference including quantile regression , instrumental variable (IV) regression , negative binomial (NB) regression , debiased Lasso , and the Wilcoxon rank-sum test . Similar to our previous simulations, we utilize labeled data, unlabeled data, and a pre-trained ML model. Detailed simulation settings are deferred to the Appendix D. Our goal is to estimate the regression coefficient between \(Y_{i}\) and \(X_{1i}\) for quantile (at quantile level 0.75), IV, and NB regression, between \(Y_{i}\) and high dimensional features \(_{i}^{150}\) for debiased Lasso, and to perform hypothesis testing on the medians of two independent samples \(Y_{i}|X_{1i}=1\) and \(Y_{i}|X_{1i}=0\) using the Wilcoxon rank-sum test.

Figure 3: Simulation for tasks that have not been implemented for ML-assisted inference including quantile regression, instrumental variable (IV) regression, negative binomial (NB) regression, debiased Lasso, and Wilcoxon rank-sum test from left to right. Panels a-e present confidence interval coverage (1 - type-error for Wilcoxon rank-sum test) and panels f-j present confidence interval width (1 - power for Wilcoxon rank-sum test).

Figure 2: Simulation for tasks that have been implemented for ML-assisted inference including mean estimation, linear regression, and logistic regression from left to right. Panel a-c present confidence interval coverage and panels d-f present confidence interval width.

Fig. 3a-d show confidence interval coverage and Fig. 3f-i show confidence interval width for parameter estimation. Fig. 3e and Fig. 3j show the type-I error and statistical power for the Wilcoxon rank-sum test. We found that the imputation-based method fails to obtain correct confidence interval coverage and shows inflated type-I error, while PSPS and classical method have the correct coverage and well-calibrated type-I error control. PSPS has narrower confidence intervals width in all settings, and higher statistical power for the Wilcoxon rank-sum test compared to classical methods. Confidence intervals become narrower as unlabeled sample size increases, indicating higher efficiency gain.

**FDR control** We evaluate the finite sample performance of PSPS-BH and PSPS-knockoff in controlling the FDR compared with classical and imputation-based methods as baselines. We consider low-dimensional(\(K<n\)) and high-dimensional(\(K>n\)) linear regressions for PSPS-BH and PSPS-knockoff, respectively. We simulate the data such that only a proportion of the features are truly associated with the outcome. The data generating process is deferred to Appendix D. Our goal is to select the associated features while maintaining the target FDR level.

Fig. E.1a-b shows the estimated FDR and Fig. E.1c-d shows the statistical power for different methods. Imputation-based method failed to control FDR in either low-dimensional or high-dimensional settings. Classical approach, PSPS-BH, and PSPS-knockoff effectively controlled in both low-dimensional and high-dimensional settings. PSPS-BH, and PSPS-knockoff achieve higher statistical power compared to the classical method.

These simulations demonstrate that PSPS outperforms existing methods and can be easily adapted for various statistical tasks not yet implemented in current ML-assisted inference methods.

### Identify vQTLs for bone mineral density

We employed our method to carry out ML-assisted quantile regression to identify genetic variants associated with the outcome variability (vQTL) of bone mineral density derived from dual-energy X-ray absorptiometry imaging (DXA-BMD) . DXA-BMD is the primary diagnostic marker for osteoporosis and fracture risk [15; 54]. Identifying vQTL for DXA-BMD can provide insights into the biological mechanisms underlying outcome plasticity and reveal candidate genetic variants involved in potential gene-gene and gene-environment interactions [29; 45; 47; 49; 32]. We focused on total body DXA-BMD, which integrates measurements from multiple skeletal sites. We used data from the UK Biobank , which includes 36,971 labeled and 319,548 unlabeled samples with 9,450,880 genetic variants after quality control. We predicted DXA-BMD values in both labeled and unlabeled samples using SoftImpute  with 466 other variables measured in the UK Biobank. Prediction in the labeled sample was implemented through cross-validation to avoid overfitting. The implementation detail is deferred to Appendix D. We used the BH procedure to correct for multiple testing and considered FDR \(<0.05\) as the significance threshold.

No genetic variants reached statistical significance under the classical method with only labeled data. PSPS identified 108 significant variants with FDR \(<0.05\) spanning 5 independent loci, showcasing the superior statistical power of PSPS (Fig. E.2 and Table E.1). Notably, these significant vQTL cannot be identified by linear regression , indicating the different genetic mechanisms controlling outcome levels and variability for DXA-BMD.

### Computational efficiency

We compared the computational efficiency of PSPS with existing methods using a dataset of 500 labeled and 10,000 unlabeled data points. Results are shown in Table E.2. While PSPS is slower due to resampling, its overall runtime is still relatively short.

## 5 Conclusion

We introduced a simple, task-agnostic protocol for ML-assisted inference, with applications across a broad range of statistical tasks. We established the consistency and asymptotic normality of the proposed estimator. We further introduced several extensions to expand the scope of our approach. Through extensive experiments, we demonstrated the superior performance and broad applicability of our method across diverse tasks. Our protocol involves initially generating summary statistics using computationally efficient software tools in scientific data analysis, followed by integration of summary statistics to produce ML-assisted inference results, which achieves high computational efficiency while maintaining statistical validity. Future work could focus on developing a fast resampling algorithm to further improve computational efficiency.

**Acknowledgements:** We gratefully acknowledge research support from the National Institutes of Health (NIH; grant U01 HG012039) and support from the University of Wisconsin-Madison Office of the Chancellor and the Vice Chancellor for Research and Graduate Education with funding from the Wisconsin Alumni Research Foundation.