# No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling

Eric Neyman

Columbia University

New York, NY 10027

eric.neyman@columbia.edu

&Tim Roughgarden

Columbia University

New York, NY 10027

tim.roughgarden@gmail.com

###### Abstract

For each of \(T\) time steps, \(m\) experts report probability distributions over \(n\) outcomes; we wish to learn to aggregate these forecasts in a way that attains a no-regret guarantee. We focus on the fundamental and practical aggregation method known as _logarithmic pooling_ -- a weighted average of log odds -- which is in a certain sense the optimal choice of pooling method if one is interested in minimizing log loss (as we take to be our loss function). We consider the problem of learning the best set of parameters (i.e. expert weights) in an online adversarial setting. We assume (by necessity) that the adversarial choices of outcomes and forecasts are consistent, in the sense that experts report calibrated forecasts. Imposing this constraint creates a (to our knowledge) novel semi-adversarial setting in which the adversary retains a large amount of flexibility. In this setting, we present an algorithm based on online mirror descent that learns expert weights in a way that attains \(O( T)\) expected regret as compared with the best weights in hindsight.

## 1 Introduction

### Logarithmic pooling

Suppose that \(m\) experts report probability distributions \(^{1},,^{m}^{n}\) over \(n\) disjoint, exhaustive outcomes. We are interested in aggregating these distributions into a single distribution \(^{*}\), a task known as probabilistic _opinion pooling_. Perhaps the most straightforward way to do this is to take the arithmetic mean, also called the linear pool: \(^{*}=_{i=1}^{m}^{i}\).

While commonly studied and frequently used, the linear pool is by no means the definitive opinion pooling method. The choice of pooling method ought to depend on context, and in particular on the loss function with respect to which forecasts are assessed. Neyman and Roughgarden (2023) showed a correspondence between proper loss functions and opinion pooling methods, which they termed _quasi-arithmetic (QA) pooling_ with respect to a loss function. Specifically, the QA pool with respect to a loss is the forecast that guarantees the largest possible overperformance (as judged by the loss function) compared with the strategy of choosing a random expert to trust.1

In this work we will be using log loss function. The QA pooling technique with respect to the log loss is known as _logarithmic pooling_. Instead of averaging the experts' forecasts, logarithmic pooling averages the experts' log odds (i.e. logits). Put otherwise, the logarithmic pool is defined by

\[p_{j}^{*}=c_{i=1}^{m}(p_{j}^{i})^{1/m}\]for all events \(j[n]\), where \(c\) is a normalizing constant to ensure that the probabilities add to \(1\). While the linear pool is an arithmetic mean, the logarithmic pool behaves much more like a geometric mean. For instance, the logarithmic pool of \((0.001,0.999)\) and \((0.5,0.5)\) with equal weights is approximately \((0.03,0.97)\).

The logarithmic pool has been studied extensively because of its naturalness (Genest, 1984; Genest and Zidek, 1986; Givens and Roback, 1999; Poole and Raftery, 2000; Kascha and Ravazzolo, 2008; Rufo et al., 2012; Allard et al., 2012). Logarithmic pooling can be interpreted as averaging experts' Bayesian evidence (Neyman and Roughgarden, 2023, SSA). It is also the most natural pooling method that satisfies _external Bayesianity_, meaning roughly that it does not matter whether a Bayesian update is applied to each expert's forecast before aggregation, or if instead the Bayesian update is applied to the pooled forecast (Genest, 1984). Logarithmic pooling can also be characterized as the pooling method that minimizes the average KL divergence to the experts' reports (Abbas, 2009). Finally, empirical work has found logarithmic pooling performs very well on real-world data (Satopaa et al., 2014; Sevilla, 2021).

### Logarithmic pooling with weighted experts

Forecast aggregators often assign different weights to different experts, e.g. based on each expert's level of knowledge or track record (Tetlock and Gardner, 2015). There is a principled way to include weights \(w_{1},,w_{i}\) (summing to \(1\)) in the logarithmic pool, namely:

\[p_{j}^{*}()=c()_{i=1}^{m}(p_{j}^{i})^{w_{i}},\]

where \(c()\) is again a normalizing constant that now depends on the weights.2 This more general notion continues to have all of the aforementioned natural properties.

The obvious question is: how does one know what these weights should be? Perhaps the most natural answer is to weight experts according to their past performance. Finding appropriate weights for experts is thus an online learning problem. This learning problem is the focus of our work.

### Choosing the right benchmark

Our goal is to develop an algorithm for learning weights for logarithmic pooling in a way that achieves vanishing regret as judged by the log loss function (i.e. the loss function most closely associated with this pooling method). Within the field of online prediction with expert advice, this is a particularly challenging setting. In part, this is because the losses are potentially unbounded. However, that is not the whole story: finding weights for _linear_ pooling, by contrast, is a well-studied problem that has been solved even in the context of log loss. On the other hand, because logarithmic pooling behaves more as a geometric than an arithmetic mean, if some expert assigns a very low probability to the eventual outcome (and the other experts do not) then the logarithmic pool will also assign a low probability, incurring a large loss. This makes the combination of logarithmic pooling with log loss particularly difficult.

We require that our algorithm not have access to the experts' forecasts when choosing weights: an algorithm that chooses weights in a way that depends on forecasts can output an essentially arbitrary function of the forecasts, and thus may do something other than learn optimal weights for logarithmic pooling. For example, suppose that \(m=n=2\) and an aggregator wishes to subvert our intentions and take an equally weighted _linear_ pool of the experts' forecasts. Without knowing the experts' forecasts, this is impossible; on the other hand, if the aggregator knew that e.g. \(_{1}=(90\%,10\%)\) and \(_{2}=(50\%,50\%)\), they could assign weights for logarithmic pooling so as to produce the post-hoc desired result, i.e. \((70\%,30\%)\). We wish to disallow this.

One might suggest the following setup: at each time step, the algorithm selects weights for each expert. Subsequently, an adversary chooses each expert's forecast and the outcome, after which the algorithm and each expert incur a log loss. Unfortunately -- due to the unboundedness of log loss and the behavior of logarithmic pooling -- vanishing regret guarantees in this setting are impossible.

**Example 1.1**.: Consider the case of \(m=n=2\). Without loss of generality, suppose that the algorithm assigns Expert 1 a weight \(w 0.5\) in the first time step. The adversary chooses reports \((e^{-T},1-e^{-T})\) for Expert 1 and \((,)\) for Expert 2, and for Outcome 1 to happen. The logarithmic pool of the forecasts turns out to be approximately \((e^{-wT},1-e^{-wT})\), so the algorithm incurs a log loss of approximately \(wT 0.5T\), compared to \(O(1)\) loss for Expert 2. On subsequent time steps, Expert 2 is perfect (assigns probability \(1\) to the correct outcome), so the algorithm cannot catch up.

What goes wrong in Example 1.1 is that the adversary has full control over experts' forecast _and_ the realized outcome, and is not required to couple the two in any way. This unreasonable amount of adversarial power motivates assuming that the experts are _calibrated_: for example, if an expert assigns a 10% chance to an outcome, there really is a 10% chance of that outcome (conditional on the expert's information).

We propose the following setting: an adversary chooses a joint probability distribution over the experts' beliefs and the outcome -- subject to the constraint that each expert is calibrated. The adversary retains full control over correlations between forecasts and outcomes, subject to the calibration property. Subsequently, nature randomly samples each expert's belief and the eventual outcome from the distribution. In this setting, we seek to prove upper bounds on the expected value of our algorithm's regret.

Why impose this constraint, instead of a different one? Our reasons are twofold: theoretical and empirical. From a theoretical standpoint, the assumption that experts are calibrated is natural because experts who form Bayesian rational beliefs based on evidence will be calibrated, regardless of how much or how little evidence they have. The assumption is also motivated if we model experts as learners rather than Bayesian agents: even if a forecaster starts out completely uninformed, they can quickly become calibrated in a domain simply by observing the frequency of events (Foster and Vohra, 1997).

Second, recent work has shown that modern deep neural networks are calibrated when trained on a proper loss function such as log loss. This is true for a variety of tasks, including image classification (Minderer et al., 2021; Hendrycks et al., 2020) and language modeling (Kadavath et al., 2022; Desai and Durrett, 2020; OpenAI, 2023); see (Blasiok et al., 2023) for a review of the literature. We may wish to use an ensemble of off-the-shelf neural networks for some prediction or classification task. If we trust these networks to be calibrated (as suggested by recent work), then we may wish to learn to ensemble these experts (models) in a way that has strong worst-case theoretical guarantees under the calibration assumption.

Logarithmic pooling is particularly sensible in the context of calibrated experts because it takes confident forecasts "more seriously" as compared with linear pooling (simple averaging). If Expert 1 reports probability distribution \((0.1\%,99.9\%)\) over two outcomes and Expert 2 reports \((50\%,50\%)\), then the logarithmic pool (with equal weights) is approximately \((3\%,97\%)\), as compared with a linear pool of roughly \((25\%,75\%)\). If Expert 1 is calibrated (as we are assuming), then the \((0.1\%,99.9\%)\) forecast entails very strong evidence in favor of Outcome 2 over Outcome 1. Meanwhile, Expert 2's forecast gives no evidence either way. Thus, it is sensible for the aggregate to point to Outcome 2 over Outcome 1 with a fair amount of confidence.

As another example, suppose that Expert 1 reports \((0.04\%,49.98\%,49.98\%)\) and Expert 2 reports \((49.98\%,0.04\%,49.98\%)\) (a natural interpretation: Expert 1 found strong evidence against Outcome 1 and Expert 2 found strong evidence against Outcome 2). If both experts are calibrated, a sensible aggregate should arguably assign nearly all probability to Outcome 3. Logarithmic pooling returns roughly \((2.7\%,2.7\%,94.6\%)\), which (unlike linear pooling) accomplishes this.

Since we are allowing our algorithm to learn the optimal logarithmic pool, perhaps there is hope to compete not just with the best expert in hindsight, but the optimally weighted logarithmic pool of experts in hindsight. We will aim to compete with this stronger benchmark.

This paper demonstrates that the "calibrated experts" condition allows us to prove regret bounds when no such bounds are possible for an unrestricted adversary. While that is our primary motivation, the relaxation may also be of independent interest. For example, even in settings where vanishing regret is attainable in the presence of an unrestricted adversary, even stronger regret bounds might be achievable if calibration is assumed.

### Our main result

Is vanishing regret possible in our setting? Our main result is that the answer is yes. We exhibit an algorithm that attains expected regret that scales as \(O( T)\) with the number of time steps \(T\). Our algorithm uses online mirror descent (OMD) with the _Tsallis entropy regularizer_\(R()=(w_{1}^{}++w_{m}^{})\) and step size \( T}\), where any choice of \((0,1/2)\) attains the regret bound.

Our proof has two key ideas. One is to use the calibration property to show that the gradient of loss with respect to the weight vector is likely to be small (Section 4.4). This is how we leverage the calibration property to turn an intractable setting into one where -- despite the unboundedness of log loss and the behavior of logarithmic pooling -- there is hope for vanishing regret.

The other key idea (Section 4.3) involves keeping track of a function that, roughly speaking, reflects how much "regret potential" the algorithm has. We show that if the aforementioned gradient updates are indeed small, then this potential function decreases in value at each time step. This allows us to upper bound the algorithm's regret by the initial value of the potential function.

This potential argument is an important component of the proof. A naive analysis might seek to use our bounds on the gradient steps to myopically bound the contribution to regret at each time step. Such an analysis, however, does not achieve our \(O( T)\) regret bound. In particular, an adversary can force a large accumulation of regret if some experts' weights are very small (specifically by making the experts with small weights more informed than those with large weights) -- but by doing so, the small weights increase and the adversary "spends down" its potential. Tracking this potential allows us to take this nuance into consideration, improving our bound.

We extend our main result by showing that the result holds even if experts are only approximately calibrated: so long as no expert understates the probability of an outcome by more than a constant factor, we still attain the same regret bound (see Corollary A.8). We also show in Appendix B that no OMD algorithm with a constant step size can attain expected regret better than \(()\).

## 2 Related work

### Probabilistic opinion pooling

There has been substantial mathematical work on probabilistic opinion pooling (i.e. forecast aggregation) since the 1980s. One line of work is axiomatic in nature: motivating opinion pooling methods by describing axioms that they satisfy. For example, logarithmic pooling satisfies unanimity preservation and external Bayesianity (Genest, 1984). There has also been work on Bayesian approaches to pooling, e.g. under the assumption that experts' signals are drawn from some parameterized class of distributions (Winkler, 1981; Lichtendahl et al., 2017).

Neyman and Roughgarden (2023) show that every proper loss function has an associated pooling method (the QA pool with respect to the loss function), which is the forecast that guarantees the largest possible overperformance (as judged by the loss function) compared with the strategy of choosing a random expert to trust. This mapping is a bijection: a pooling method is QA pooling with respect to some proper loss function if and only if it satisfies certain natural axioms.

### Online prediction with expert advice

In the subfield of _prediction with expert advice_, for \(T\) time steps, experts report "predictions" from a decision space \(\) (often, as in our case, the space of probability distributions over a set of outcomes). A forecaster must then output their own prediction from \(\). Then, predictions are assessed according to a loss function. See Cesa-Bianchi and Lugosi (2006) for an survey of this field.

We are particularly interested in _mixture forecasters_: forecasters who, instead of choosing an expert to trust at each time step, aggregate the expert' reports. Linear mixtures, i.e. convex combinations of predictions, have been especially well-studied, generally with the goal of learning weights for the convex combination to compete with the best weights in hindsight. Standard convex optimization algorithms achieve \(O()\) regret for bounded, convex losses, but it is sometimes possible to dobetter. For example, if the loss function is bounded and exp-concave, then logarithmic regret in \(T\) is attainable (Cesa-Bianchi and Lugosi, 2006, SS3.3).

Portfolio theory studies optimal stock selection for maximizing return on investment, often in a no-regret setting. Cover (1991) introduced the "universal portfolio" algorithm, which, for each of \(T\) time steps, selects a portfolio (convex combination of stocks). Our setting translates naturally to Cover's: experts play the role of stocks, and the return of a stock corresponds to the probability that and expert assigns to the eventual outcome. The universal portfolio algorithm achieves logarithmic regret compared with the best portfolio in hindsight (Cover and Ordentlich, 1996); in our terms, this means that logarithmic regret (for log loss) is attainable for the linear pooling of experts. We refer the reader to (Li and Hoi, 2014) for a survey of this area.

To our knowledge, learning weights for _logarithmic_ pooling has not been previously studied. As shown in Example 1.1, it is not possible to achieve vanishing regret if the setting is fully adversarial. We relax our setting by insisting that the experts be calibrated (see Section 3.1). To our knowledge, online prediction with expert advice has also not previously been studied under this condition.

## 3 Preliminaries

### Calibration property

We define calibration as follows. Note that the definition is in the context of our setting, i.e. \(m\) experts reporting probability distributions \(^{1},,^{m}\) over \(n\) outcomes. We will use \(J\) to denote the random variable corresponding to the outcome, i.e. \(J\) takes values in \([n]\).

**Definition 3.1**.: Consider a joint probability distribution \(\) over experts' reports and the outcome.3 We say that expert \(i\) is _calibrated_ if for all \(^{n}\) and \(j[n]\), we have that

\[[J=j^{i}=]=p_{j}.\]

That is, expert \(i\) is calibrated if the probability distribution of \(J\) conditional on their report \(^{i}\) is precisely \(^{i}\). We say that \(\) satisfies the _calibration property_ if every expert is calibrated.

The key intuition behind the usefulness of calibration is that if an expert claims that an outcome is very unlikely, this is strong evidence that the outcome is in fact unlikely. In Section 4.4 we will use the calibration property to show that the gradient of the loss with respect to the weight vector is likely to be relatively small at each time step.

### Our online learning setting

The setting for our online learning problem is as follows. For each time step \(t[T]\):

1. Our algorithm reports a weight vector \(^{t}^{m}\).
2. An adversary (with knowledge of \(^{t}\)) constructs a probability distribution \(\), over reports and the outcome, that satisfies the calibration property.
3. Reports \(^{t,1},,^{t,m}\) and an outcome \(j\) are sampled from \(\).
4. The _loss_ of a weight vector \(\) is defined as \(L^{t}():=-(p_{j}^{s}())\), the log loss of the logarithmic pool of \(^{t,1},,^{t,m}\) with weights \(\). Our algorithm incurs loss \(L^{t}(^{t})\).

We define the _regret_ of our algorithm as

\[=_{t=1}^{T}L^{t}(^{t})-_{^{ m}}_{t=1}^{T}L^{t}().\]

That is, the benchmark for regret is the best weight vector in hindsight. Since our setting involves randomness, our goal is to provide an algorithm with vanishing _expected_ regret against any adversarial strategy, where the expectation is taken over the sampling in Step (3).

Even subject to the calibration property, the adversary has a large amount of flexibility, because the adversary retains control over the correlation between different experts' forecasts. An unrestricted adversary has exponentially many degrees of freedom (as a function of the number of experts), whereas the calibration property imposes a mere linear number of constraints.4

### Our algorithm

We use Algorithm 1 to accomplish this goal. The algorithm is online mirror descent (OMD) on the weight vector. Fix any \((0,1/2)\). We use the regularizer

\[R():=(w_{1}^{}++w_{m}^{}).\]

This is known as the Tsallis entropy regularizer; see e.g. (Zimmer and Seldin, 2021) for previous use in the online learning literature. We obtain the same result (up to a multiplicative factor that depends on \(\)) regardless of the choice of \(\). Because no choice of \(\) stands out, we prove our result for all \((0,1/2)\) simultaneously.

We will generally use a step size \(= T}\). However, in the (unlikely, as we show) event that some expert's weight becomes unusually small, we will reduce the step size.

``` \(R():=(w_{1}^{}++w_{m}^{})\) ; /* Any \((0,1/2)\) will work */ \( T}\); \(^{1}(1/m,,1/m)\); for\(t=1\) to \(T\)do if\(_{i}((w_{i}^{t})^{})\)then \(_{t}(_{t-1},)\); else \(_{t}(_{t-1},_{i}w_{i}^{t})\) ; /* Edge case; happens with low probability */  end if Observe loss function \(L^{t}\) ; /* \(L^{t}\) is chosen as described in Section 3.2 */  Define \(^{t+1}\) such that \( R(^{t+1})= R(^{t})-_{t} L^{t}( ^{t})\);  end for ```

**ALGORITHM 1**OMD algorithm for learning weights for logarithmic pooling

In Appendix A, we prove that Algorithm 1 is efficient, taking \(O(mn)\) time per time step.

Theorem 3.2 formally states our no-regret guarantee.

Theorem 3.2: _For any adversarial strategy, the expected regret5 of Algorithm 1 is at most_

\[O(m^{(3-)/2}n T).\]

## 4 Proof of no-regret guarantee

In this section, we prove Theorem 3.2.

### Outline of proof

We use the following fact, which follows from a more general statement about how losses relate to their associated quasi-arithmetic pools.

**Proposition 4.1** (Follows from (Neyman and Roughgarden, 2023, Theorem 5.1)).: _Let \(^{1},,^{m}\) be forecasts over \(n\) outcomes, \(j[n]\) be an outcome, and \(^{m}\) be a weight vector. Let \(^{*}()\) be the logarithmic pool of the forecasts with weight vector \(\) and let \(L():=-(p_{j}^{*}())\) be the log loss of \(^{*}()\) if Outcome \(j\) is realized. Then \(L\) is a convex function._

In particular, all of our loss functions \(L^{t}\) are convex, which means that standard regret bounds apply. In particular, to bound the expected regret of Algorithm 1, we will use a well-known regret bound for follow the regularized leader (FTRL) with linearized losses (Hazan, 2021, Lemma 5.3), which in our case is equivalent to OMD.6

**Lemma 4.2** (Follows from (Hazan, 2021, Lemma 5.3)).: _If \(_{t}=\) for all \(t\), the regret of Algorithm 1 is at most_

\[(_{^{m}}R()-_{ ^{m}}R())+_{t=1}^{T} L^{t}( ^{t})(^{t}-^{t+1}).\]

Informally, this bound means that if the vectors \( L^{t}(^{t})\) are small in magnitude, our regret is also small. Conversely, if some \( L^{t}(^{t})\) is large, this may be bad for our regret bound. We expect the gradient of the loss to be large if some expert is very wrong (assigns a very low probability to the correct outcome), since the loss would then be steeply increasing as a function of that expert's weight. Fortunately, the calibration property guarantees this to be unlikely. Specifically, we define the _small gradient assumption_ as follows.

**Definition 4.3**.: Define \(:=12n T\). The _small gradient assumption_ holds for a particular run of Algorithm 1 if for every \(t[T]\) and \(i[m]\), we have

\[-^{t}}_{i}L^{t}(^{t}),\]

where \(_{i}\) denotes the partial derivative with respect to the \(i\)-th weight.7

In Section 4.4, we prove that the small gradient assumption is very likely to hold. This is a key conceptual step in our proof, as it is where we leverage the calibration property to prove bounds that ultimately let us bound our algorithm's regret. We then use the low likelihood of the small gradient assumption failing in order to bound the contribution to the expected regret from the case where the assumption fails to hold.

In Sections 4.2 and 4.3, we bound regret under the condition that the small gradient assumption holds. We show that under the assumption, for all \(i,t\) we have \((w_{i}^{t})^{}\). Consequently, \(= T}n}\) at all time steps, so we can apply Lemma 4.2. The first term in the bound is \(O(1/)=O( T)\). As for the summation term, we upper bound it by keeping track of the following quantity:

\[(t):=_{s=1}^{t} L^{s}(^{s})(^{s}- ^{s+1})+19m^{2}^{2}(T-t)-4_{i=1}^{m} w_{i}^ {t+1}.\]

The first term is exactly the summation in Lemma 4.2 up through step \(t\). The \(19m^{2}^{2}\) is something akin to an upper bound on the value of \(L^{t}(^{t})(^{t}-^{t+1})\) at a given time step (times \(T-t\) remaining time steps). This upper bound is not strict: in particular, large summands are possible when some weights are small (because of the fact that the lower bound in the small gradient assumption is inversely proportional to \(w_{i}^{t}\)). However, attaining a large summand requires these small weights to increase, thus "spending potential" for future large summands. The last term keeps track of this potential.

We show that under the small gradient assumption, \((t)\) necessarily decreases with \(t\). This argument, which we give in Section 4.3, is another key conceptual step, and is arguably the heart of the proof. Since \((T)\) is equal to the summation term in Lemma 4.2 (plus a positive number), and \((0)(T)\), the summation term is less than or equal to \((0)\), which is at most \(O(m^{(3-)/2} T)\). This completes the proof.

### Bounds on \(^{t}\) under the small gradient assumption

In this section, we state bounds on expert weights and how quickly they change from one time step to the next, conditional on the small gradient assumption. We use the following lemma, whose proof we defer to Appendix A.

**Lemma 4.4**.: _Consider a particular run of Algorithm 1. Let \(\) be a constant such that \(-^{t}}_{i}L^{t}(^{t})\) for all \(i,t\). Then for every \(i,t\), we have_

\[(w_{i}^{t})^{-1}-(^{t}}+1)_{t}(w_{ i}^{t+1})^{-1}(w_{i}^{t})^{-1}+(w_{k}}+1 )_{t}.\]

_Furthermore, if \(_{t}(1-)^{2}(w_{i}^{t})\) for all \(i\), then for every \(i\) we have_

\[(w_{i}^{t+1})^{-1}(w_{i}^{t})^{-1}+(m+1)_{t}.\]

Intuitively, this result states that when the gradient update is small, \(w_{i}^{t+1}\) is not too different from \(w_{i}^{t}\). Note that the lower bound \(-^{t}}\) that we place on the gradient is not a simple Lipschitz bound but instead depends on \(w_{i}^{t}\); this makes the bounds in Lemma 4.4 less straightforward to prove. In particular, we bound each component \(w_{i}^{t}\) individually, using bounds on the gradient of the loss for all other components and convexity arguments.

Lemma 4.4 can be translated into bounds on each \(w_{i}^{t}\) and on the change between \(w_{i}^{t}\) and \(w_{i}^{t+1}\):

**Corollary 4.5**.: _Under the small gradient assumption, for sufficiently large \(T\) we have for all \(i[m],t[T]\) that:_

* \((w_{i}^{t})^{} 4\) _and_ \(w_{i}^{t}}T^{1/(2(-1))}\)_._
* \(-32(w_{i}^{t})^{1-} w_{i}^{t}-w_{i}^{t+1} 2(w_{i}^{t})^{ 2-}(m+1).\)__

We defer the proof of Corollary 4.5 to Appendix A. The key idea for (#1) is to proceed by induction on \(t\) on the two sub-statements in parallel: so long as \((w_{i}^{t})^{} 4\), we may use the second part of Lemma 4.4 with \(=\) to bound \((w_{i}^{t+1})^{-1}\) in terms of \((w_{i}^{t})^{-1}\), which we can leverage to prove both sub-statements for \(t+1\). (#2) then follows from (#1) by routine (though nontrivial) algebra.

Armed with the bounds of Corollary 4.5, we are now able to show that under the small gradient assumption, Algorithm 1 attains vanishing regret.

### Bounding regret under the small gradient assumption

Assume the small gradient assumption. Note that since \(4 1\), by Corollary 4.5 (#1) we have that \(_{t}=\) for all \(t\). This means that we may apply the bound in Lemma 4.2, and in particular we have

\[(_{^{m}}R()-_{^{m}}R())=( )^{}=}{}= m^{(3-)/2}n T.\]

It remains to bound the summation component of the regret bound in Lemma 4.2. To do so, we prove the following lemma, which we alluded to in Section 4.1 as the heart of the proof of Theorem 3.2.

**Lemma 4.6**.: _For \(t\{0,1,,T\}\), let_

\[(t):=_{s=1}^{t} L^{s}(^{s})(^{s}- ^{s+1})+19m^{2}^{2}(T-t)-4_{i=1}^{m} w_{i}^{ t+1}.\]

_Under the small gradient assumption, for sufficiently large \(T\), \((t)\) is a decreasing function of \(t\)._

To prove this claim, consider a particular \(t[T]\). We may write

\[(t)-(t-1)=_{i=1}^{m}((w_{i}^{t}-w_{i}^{t+1})_{i }L^{t}(^{t})-19m^{2}+4( w_{i}^{t}- w_{i}^{t+1}))\] (1)and we wish to show that this quantity is negative. In fact, we show that the contribution from every \(i[m]\) is negative. The key idea is to consider two cases: \(w_{i}^{t+1} w_{i}^{t}\) and \(w_{i}^{t+1} w_{i}^{t}\). In each case, Corollary 4.5 provides an upper bound on the magnitude of the difference between \(w_{i}^{t}\) and \(w_{i}^{t+1}\). If \(w_{i}^{t+1} w_{i}^{t}\) then the first and third terms in the summation are positive but small, and are dominated by the middle term. If \(w_{i}^{t+1} w_{i}^{t}\) then the first term may be quite large, because of the asymmetric bound in the small gradient assumption (and the consequently asymmetric bound in Corollary 4.5). However, in this case the contribution of the third term is very negative, enough to make the overall expression negative. In this sense, the third term keeps track of unspent potential for future regret, which gets "spent down" whenever a large amount of regret is realized (as measured by the first term).

We now prove formally that each term of the summation in Equation 1 is negative.

Proof.: First assume that \(w_{i}^{t}-w_{i}^{t+1} 0\). Note that by combining (#1) and (#2) of Corollary 4.5, we have

\[w_{i}^{t+1}-w_{i}^{t} 32(w_{i}^{t})^{1-} 8w_{i}^{t}.\]

By the small gradient assumption we have that

\[(w_{i}^{t}-w_{i}^{t+1})_{i}L^{t}(^{t})^{t+1}-w_{i}^{t})}{w_{i}^{t}}.\]

On the other hand, we have

\[4( w_{i}^{t}- w_{i}^{t+1})=-4(1+^{t+1}-w_ {i}^{t}}{w_{i}^{t}})^{t+1}-w_{i}^{t})}{w_{i}^{t}}\]

for \(T\) large enough. (Here we use that \(w_{i}^{t+1}-w_{i}^{t} 8w_{i}^{t}\) and that \((1+x)\) for \(x 8\).) Thus, the first and third terms in Equation 1 are net negative; meanwhile, the second term is also negative, so the expression is negative.

Now assume that \(w_{i}^{t}-w_{i}^{t+1} 0\). Again by the small gradient assumption, we have that

\[(w_{i}^{t}-w_{i}^{t+1})_{i}L^{t}(^{t})(w_{i}^{t}- w_{i}^{t+1}) 2(m+1)^{2}(w_{i}^{t})^{2-} 3m^{2}\]

and

\[4( w_{i}^{t}- w_{i}^{t+1}) =-4(1-^{t}-w_{i}^{t+1}}{w_{i}^{t}} )-4(1-2(m+1)(w_{i}^{t})^{1-})\] \[-4(1-2(m+1))-4(1-3m ) 16m^{2}\]

for \(T\) sufficiently large, where in the last step we use that \((1-x)-x\) for \(x>0\) sufficiently small (and we have \(_{T}3m=0\)). Since \(16+3 19\), the right-hand side of Equation 1 is negative. This concludes the proof. 

**Corollary 4.7**.: _For sufficiently large \(T\), under the small gradient assumption, the regret of Algorithm 1 is at most \((240+)m^{(3-)/2}n T\)._

Proof.: We have already bounded the first term in the regret bound in Lemma 4.2. It remains only to bound the second term. This term is exactly equal to \((T)+4_{i=1}^{m} w_{i}^{T+1}(T)\), and \((T)(0)\), by Lemma 4.6. We have

\[(0)=19m^{2}^{2} T+4m m 20m^{2}^{2} T\]

for sufficiently large \(T\). Plugging in \(=12n T\) and \(= T}n}\) concludes the proof. 

### The case where the small gradient assumption fails

It remains to consider the case in which the small gradient assumption does not hold. This part of the proof consists primarily of technical lemmas, which we defer to Appendix A. The key lemma is a bound on the probability that the small gradient assumption fails by a given margin:

**Lemma 4.8**.: _For any weight vector \(\), \(i[m]\), and \( 0\), we have that_

\[[_{i}L()] ne^{-}\] (2)

_and_

\[[_{i}L()-}] mn ^{2}e^{-/n}.\] (3)

Note that plugging in \(=\) yields a bound of \(mT(ne^{-}+mn^{2}e^{-/n})\) on the probability that the small gradient assumption fails to hold. (Since \(=12n T\), this quantity is on the order of \(T^{-11}\).)

The proof of Lemma 4.8 is the only part of the proof of Theorem 3.2 that uses the calibration property. While we defer the full proof to Appendix A, we highlight how the calibration property is used to prove Equation 2. In brief, it is straightforward to show that \(_{i}L()- p_{j}^{i}\), where \(J\) is the random variable corresponding to the realized outcome.8 Therefore, we have

\[[_{i}L()] [- p_{J}^{i}]= [p_{J}^{i} e^{-}]=_{j=1}^{n}[J=j\ \&\ p_{j}^{i} e^{-}]\] \[=_{j=1}^{n}[p_{j}^{i} e^{-}] [J=j\ |\ p_{j}^{i} e^{-}]_{j=1}^{n} [J=j\ |\ p_{j}^{i} e^{-}] ne^{-},\]

where the last step follows by the calibration property, thus proving Equation 2.

Combining Lemma 4.8 with an analysis of our algorithm using the standard regret bound for online mirror descent (Orabona, 2021, Theorem 6.8) gives us the following result as a corollary.

**Corollary 4.9**.: _The expected total regret of our algorithm conditional on the small gradient assumption not holding, times the probability of this event, is at most \((T^{(5-)/(1-)-10})\)._

It follows that the contribution to expected regret from the case that the small gradient assumption does not hold is \((T^{-1})\), which is negligible. Together with Corollary 4.7 (which bounds regret under the small gradient assumption), this proves Theorem 3.2. As a matter of fact, Theorem 3.2 holds even if experts are only _approximately_ calibrated. As with other details of this section, we refer the reader to Appendix A.

## 5 Conclusion

In this work we have considered the problem of learning optimal weights for the logarithmic pooling of expert forecasts. It quickly became apparent that under the usual fully adversarial setup, attaining vanishing regret is impossible (Example 1.1). We chose to relax the environment by imposing the constraint on the adversary that experts must be calibrated. Put otherwise, the adversary is allowed to choose a joint probability distribution over the experts' reports and the outcome however it wants to, so long as the experts' reports are calibrated, after which the realized reports and outcome are selected at random from this distribution. To our knowledge, this setting is a novel contribution to the literature on prediction with expert advice. The setting may be of independent interest: we have demonstrated that no-regret bounds are possible in this setting when they are otherwise impossible, and it seems plausible that even in settings where no-regret bounds are attainable in a fully adversarial setting, the calibration property allows for stronger results.

Another important direction for future work is learning weights for other pooling methods. In particular, because of the close connection between a proper loss function and its associated quasi-arithmetic pool, it is natural to ask for which proper loss functions it is possible to achieve vanishing regret when learning weights for quasi-arithmetic pooling with respect to the loss function. (Neyman and Roughgarden, 2023, SS5) showed that the loss of a quasi-arithmetic pool is convex in the experts' weights, and therefore the usual no-regret algorithms (e.g. online gradient descent) guarantee \(O()\) regret -- so long as the loss function is bounded. In this work, we extended their result to the log loss (with the associated QA pooling method, i.e. logarithmic pooling.) Extending our techniques to other unbounded loss functions is a promising avenue for future exploration.