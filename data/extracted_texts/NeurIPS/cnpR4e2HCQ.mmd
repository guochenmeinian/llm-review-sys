# Community Detection Guarantees using Embeddings Learned by Node2Vec

Andrew Davison

Department of Statistics

Columbia University

New York, NY 10027

ad3395@columbia.edu

&S. Carlyle Morgan

Department of Statistics

University of Michigan

Ann Arbor, MA 48109

scmorgan@umich.edu

&Owen G. Ward

Department of Statistics and Actuarial Science

Simon Fraser University

Burnaby, British Columbia

owen_ward@sfu.ca

###### Abstract

Embedding the nodes of a large network into an Euclidean space is a common objective in modern machine learning, with a variety of tools available. These embeddings can then be used as features for tasks such as community detection/node clustering or link prediction, where they achieve state of the art performance. With the exception of spectral clustering methods, there is little theoretical understanding for commonly used approaches to learning embeddings. In this work we examine the theoretical properties of the embeddings learned by node2vec. Our main result shows that the use of k-means clustering on the embedding vectors produced by node2vec gives weakly consistent community recovery for the nodes in (degree corrected) stochastic block models. We demonstrate this result empirically for both real and simulated networks, and examine how this relates to other embedding tools and machine learning procedures for network data.

## 1 Introduction

Within network science, a widely applicable and important inference task is to understand how the behavior of interactions between different units (nodes) within the network depend on their latent characteristics. This occurs within a wide array of disciplines, from sociological  to biological  networks.

One simple and interpretable model for such a task is the stochastic block model (SBM) , which assumes that nodes within the network are assigned a discrete community label. Edges between nodes in the network are then formed independently across all pairs of edges, conditional on these community assignments. While such a model is simplistic, various extensions have been proposed. These include the degree corrected SBM (DCSBM), used to handle degree heterogenity , and mixed-membership SBMs, used to allow for more complex community structures . These extensions have seen a wide degree of empirical success .

A restriction of the stochastic block model and its generalizations is the requirement for a discrete community assignment as a latent representation of the units within the network. While the statistical community has previously considered more flexible latent representations , over the past decade, there have been significant advancements in general _embedding methods_ for networks. These producegeneral vector representations of units within a network, and can achieve start-of-the-art performance in downstream tasks for node classification and link prediction.

An early example of such a method is spectral clustering , which constructs an embedding of the nodes in the network from an eigendecomposition of the graph Laplacian. The \(k\) smallest non zero eigenvectors provides a \(k\) dimensional representation of each of the nodes in the network. This has been shown to allow consistent community recovery , however it may not be computationally feasible on the large networks which are now common. More recently, machine learning methods for producing vector representations have sought inspiration from NLP methods and the broader machine learning literature, such as the node2vec algorithm , graph convolutional networks , graph attention networks  and others. There are now a wide class of embedding methods which are available to practitioners which can be applied across a mixture of unsupervised and supervised settings.  provides a survey of relatively recent developments and  reviews the connection between the embedding procedure and the potential downstream task.

Embedding methods such as Deepwalk  and node2vec  consider random walks on the graph, where the probability of such a walk is a function of the embedding of the associated nodes. Given embedding vectors \(_{u},_{v}^{d}\) of nodes \(u\) and \(v\) respectively, from graph \(\) with vertex set \(\), the probability of a random walk from node \(u\) to node \(v\) is modeled as

\[P(v|u)=,_{u})}{_{l }(_{l},_{u})},\] (1)

where \( x,y\) is the inner product of \(x\) and \(y\). This leads to a representation of each of the nodes in the network as a vector in \(d\) dimensional Euclidean space. This representation is then amenable to potential downstream tasks about the network. For example, if we wish to cluster the nodes in the network we can simply cluster their embedding vectors. Or, if we wish to classify the nodes in the network, we can use these embeddings to construct a multinomial classifier. We note that the sampling schemes introduced by DeepWalk and node2vec motivate more complex models such as GraphSAGE  and Deep Graph Infomax , which utilise similar node sampling schemes for learning embeddings of networks.

As such, one of the key goals of learning vector representations of the units within networks is to allow for easy use for a multitude of downstream tasks. However, there is little theoretical understanding to what information is carried within these representations, and whether they can be applied successfully and efficiently to downstream tasks. This paper aims to address this gap by examining whether learned embeddings can facilitate community detection tasks in an unsupervised setting.

### Summary of main results

Our main contribution is to describe the asymptotic distribution of the embeddings learned by the node2vec procedure, and to then use this to give consistency guarantees when these embeddings are used for community detection. A simple and informal form of our results, in the scenario of a balanced two block stochastic block model (SBM), is given below:

**Theorem 1**.: _(Informal) Suppose we observe a sequence of graphs \(_{n}\) on \(n\) vertices arising from a two-dimensional stochastic block model: for each vertex \(u[n]\) we assign a community label \(c(u)\{0,1\}\) with equal probability, and then we form edges in the graph independently with probability_

\[uv=&c(u)=c(v)\\ &\] (2)

_where \(\). Suppose that \((_{u})\) are two-dimensional embeddings learned by node2vec on the above graph (where we hide the dependence on \(n\)). Then there exists some distinct vectors \(_{c(u)}^{2}\) such that_

\[_{u}\|_{u}-_{c(u)}\|_{2}^{2}  0n.\] (3)

_Consequently, if we apply a k-means algorithm to the embeddings learned via node2vec, as \(n\) we will classify at least \(100(1-)\)% of vertices to the correct community (up to permutation) with asymptotic probability \(1\), for any \(>0\)._We give formal theorem statements, complete with full conditions, in Section 3; we note that our results extend to graph models beyond SBMs and are not limited to the dense regime. To give some brief intuition for the method of proof, we show that the probability that a pair \((u,v)\) is positively or negatively sampled within node2vec concentrates around a function which depends only on the underlying communities \(c(u)\) and \(c(v)\) of \(u\) and \(v\). With this, we are able to argue that the node2vec loss concentrates uniformly (in a neighborhood of their minima) around a function whose minima \(M^{*}\) is such that \(M^{*}_{u,v}=_{c(u),c(v)}\) for some matrix \(\). This allows us to show that any set of embeddings which minimize the node2vec loss will converge (up to rotation) to vectors which depend only on the community label, which consequently allows us to give consistency guarantees for clustering algorithms such as k-means.

We highlight that while the theoretical properties of spectral clustering are well studied in the literature, there are relatively few theoretical guarantees provided for more modern embedding procedures such as node2vec. Our work provides some of the first theoretical results for models of this form. Our main contributions are the following:

1. We give convergence guarantees for embeddings learned via node2vec, under various sparsity regimes of (degree corrected) stochastic block models. We then use this to give weak consistency guarantees for community detection, when using the embeddings as features within a k-means clustering algorithm.
2. We verify the theoretical guarantees for simulated networks and examine the the performance of this procedure on real networks. We also empirically investigate important extensions of these theoretical results, relating to rates of recovery for community detection between node2vec and spectral clustering methods. We identify that as these networks grow the sampling parameters in node2vec have little impact on the performance of the proposed procedure.

The layout of the paper is as follows. In Section 2 we formulate the problem of constructing an embedding of the nodes in a network and state the criterion under which we consider community detection. In Section 3 we give the main result of this paper, the conditions under which k-means clustering of the node2vec embedding of a network gives consistent community recovery. In Section 4 we verify these theoretical results empirically and investigate potential further results. In Section 5 we summarize our contributions and consider potential extensions.

### Related Works

Community detection for networks is a widely studied area with a large literature of existing work. Several notions of theoretical guarantees for community recovery are provided in , along with a survey of many existing approaches. There are many existing works which consider the embeddings obtained from the eigenvectors of the adjacency matrix of Laplacian of a network. For example,  considers spectral clustering using the eigenvectors of the adjacency matrix for a stochastic block model. Spectral clustering has provided such guarantees for a wide variety of network models, including .

With the more recent development of random walk based embeddings, several recent works have begun to examine the theoretical properties of such embeddings, however the treatment is limited compared to spectral embeddings.  study the global minimizers of the node2vec loss in the setting where \(d=n\), viewing the problem as a matrix factorization problem. If \(M^{*}\) is the global minimizing matrix, we highlight that their results apply for any \(d(M^{*})\). That said, this minimizer equals the entrywise logarithm of functions of the adjacency matrix \(A\); we note that entrywise logarithms of matrices typically blow up their rank, and that even when "in expectation" the adjacency matrix is of low rank, the actual adjacency matrix is of full rank with high probability . This means that it is unlikely when \(d n\) that the global minimizer is the actual minimizer, which is the regime where embedding dimensions are considered in practice. We contrast that with our results, where we can take \(d=()\) where \(\) is the number of communities, and obtain rigorous guarantees for the embeddings.

 then studies the concentration of the best rank \(d\) approximation (with respect to the Frobenius norm) of the matrix \(M^{*}\) about it's expected value under SBM and DCSBM models for node2vec with \(p=q=1\) only, to argue that the best rank \(d\) approximation can be used for strongly consistent community detection. We note that our results can be applied to node2vec without this restriction on the hyperparameters. Otherwise, they give similar types of guarantees as our paper in similar sparsity regimes and with similar rates, but in stronger norms. The key difference between our work and that of  is that we are able to give guarantees for the the actual minimizers of the node2vec loss as soon as \(d=()\), whereas  use an approximation to the global minimizer, without studying the gap between this matrix and any minimizer of the node2vec loss (which is a cross-entropy loss, and therefore difficult to relate to a Frobenius norm approximation).  and  study node2vec with in the constrained setting (where \(U=V\)), and focus on giving more abstract guarantees for the gram matrix in the setting of graphons. In  the norm guarantees extend only to the \(L_{1}\) norm between the gram matrix of the embeddings and the minimizer, which is not sufficient to give guarantees on the individual embeddings. In  the norm guarantees are upgraded to the \(L_{2}\) norm, albeit with less optimal rates of convergence than what we show here. Our results also give guarantees for node2vec in full generality (no restriction on \(p\) and \(q\)) and give the calculation details for SBMs and DCSBMs to explicitly describe the asymptotic distribution in certain regimes.

## 2 Framework

We consider a network \(\) consisting of a vertex set \(\) of size \(n\) and edge set \(\). We can express this also using an \(n n\) symmetric adjacency matrix \(A\), where \(A_{uv}=1\) indicates there is an undirected edge between node \(u\) and node \(v\), with \(A_{uv}=0\) otherwise, where \(u,v\). Given a realisation of such a network, we wish to examine models for community structure of the nodes in the network. We then examine the embeddings which can be obtained from node2vec and examine how they can be used for community detection.

### Probabilistic models for community detection

The most widely studied statistical model for community detection is the Stochastic Block Model (SBM) . The SBM specifies a distribution for the communities, placing each of the \(n\) nodes into one of \(\) communities, where these community assignments are drawn from some categorical distribution \(()\). Writing \(c(u)[]\) for the community of \(u\), the connection probabilities between edges are independent, conditional on these community assignments, with probability

\[(A_{uv}=1|c(u),c(v))=_{n}P_{c(u),c(v)},\] (4)

where \(P\) is a \(\) matrix of probabilities, and \(_{n}\) is the overall network sparsity (so that the network has \(O(_{n}n^{2})\) edges on average). As a special case, the _planted-partition_ model considers \(P\) as being a matrix with \(\) along its diagonal and the value \(\) elsewhere, with \(\) equally balanced communities, so \(=(^{-1},,^{-1})\). We will denote such a model by \((n,,,,_{n})\).

The most widely studied extension of the SBM is to incorporate a degree correction, equipping each node with a non negative degree parameter \(_{u}\) drawn from some distribution independently of the community assignments . This alters the previous model, instead giving

\[(A_{uv}=1|c(u),c(v),_{u},_{v})=_{n}_{u} _{v}P_{c(u),c(v)}.\] (5)

Degree corrected SBM models can be more appropriate for modeling the degree heterogeneity seen within communities in real world network data .

Performance of stochastic block models is assessed in terms of their ability to recover the true community assignments of the nodes in a network, from the observed adjacency matrix \(A\). Given an estimated community assignment vector \(}[]^{n}\) and the true communities \(\) then we can compute the agreement between these two assignment vectors, up to a relabeling of \(\), as

\[L(},)=_{_{}}_{i=1}^{n}(i)(c(i))\] (6)

where \(S_{}\) denotes the symmetric group of permutations \(:[][]\). We can also control the worst-case misclassification rate across all the different communities. If \(_{k}\) is the set of nodes belonging to community \(k\), then this is defined as

\[(},):=_{k[]}_{  S_{}}_{k}|}_{i_{k}} (i)(k).\] (7)Guarantees of the form \(L(},)=o_{p}(1)\) as \(n\) are known as _weak consistency_ guarantees in the community detection literature. Strong consistency considers the stronger setting where \(L(},)=0\) with asymptotic probability 1.  provides a review of results for guarantees of these forms. In this work we consider only the weak consistency setting; we highlight that stricter assumptions are necessary in order to give these type of guarantees.

### Obtaining embeddings from node2vec

Machine learning methods such as node2vec aim to obtain an embedding of each node in a network. In general, for each node \(u\) two \(d\)-dimensional embedding vectors are learned, a centered representation \(_{i}^{d}\) and a context representation \(_{i}^{d}\). node2vec modifies the simple random walk considered in DeepWalk , incorporating tuning parameters \(p,q\) which encourage the walk to return to previously sampled nodes or transition to new nodes. Formally, this is defined by sampling concurrent pairs of vertices in the second-order random walk \((X_{n})_{n 1}\) defined via

\[X_{n}=u\,|\,X_{n-1}=s,X_{n-2}=v0& (u,s),\\ 1/p&d_{u,v}=0(u,s),\\ 1&d_{u,v}=1(u,s),\\ 1/q&d_{u,v}=2(u,s).\] (8)

where \(d_{u,s}\) denotes the length of the shortest path between \(u\) and \(s\), after selecting some initial two vertices. Here we consider the case where \((X_{0},X_{1})\) is drawn uniformly from the set of edges in order to initialize the walk. We note that when \(p=q=1\), corresponding to DeepWalk, this reduces down to a simple random walk, in which case the initial distribution samples a vertex proportionally to their degree.

A negative sampling approach is also used to approximate the computationally intractable loss function, replacing \(-(P(v|u))\) in (1) with

\[-(_{u},_{v})-_{l=1}^{L} (-_{u},_{n_{l}}),\] (9)

where \((x)=(1+e^{-x})^{-1}\), the sigmoid function. The vertices \(n_{1},,n_{L}\) are sampled according to a negative sampling distribution, which we denote as \(P_{ns}(|u)\). This is usually chosen as the unigram distribution,

\[P(v|u)=(v)^{}}{_{v^{}}(v^{})^{}},\] (10)

which does not depend on the current location of the random walk, \(u\). This unigram distribution has parameter \(\), which is commonly chosen as \(=3/4\), as was used by word2vec . Given this, and using (9), the loss considered by node2vec for a random walk of length \(k\) can be written as

\[=_{j=1}^{k+1}_{i:0<|j-i|<W}-(_{v_{j}}, _{v_{i}})-_{l=1}^{L}_{n_{l} P_{ns}( |v_{i})}(-_{v_{j}},_{n_{l}} ).\] (11)

Here we use \(_{n_{l} P_{ns}(|v_{i})}\) to denote the procedure to sample a draw from the negative sampling distribution, with \(W=1\) commonly chosen. Given this loss function, stochastic gradient updates are used to estimate the embedding vector for each node. This amounts to minimizing an empirical risk function (e.g ), which we can write as

\[_{n}(U,V):=_{i j}-_{n}((i,j))(( u_{i},v_{j}))-_{n}((i,j)) (1-( u_{i},v_{j}))}.\] (12)

where \(_{n}():=(\,|\,_{n})\), and \(=(_{n})\) and \(=(_{n})\) are sets of positive and negative samples respectively. We consider a sequence of graphs \(_{n}\) with \(||=n\) and study the behavior of this loss function when \(n\) is large. To be explicit, \(_{n}((i,j))\) denotes the probability (conditional on a realization of the graph) that the vertices \((i,j)\) appear concurrently within a random walk of length \(k\), and \(_{n}((i,j))\) denotes the probability that \((i,j)\) is selected as a pair of edges through the negative sampling scheme (conditional on the random walk process in the first stage).

The loss depends on two matrices \(U,V^{n d}\), with \(u_{i},v_{j}^{d}\) denoting the \(i\)-th and \(j\)-th rows of \(U\) and \(V\) respectively. The rows of \(U\) correspond to the "centered representations" of each node, while the rows of \(V\) correspond to the "context representation" (borrowing the terminology used by e.g Word2Vec). In practice we can constrain the embedding vectors \(u_{i}\) and \(v_{i}\) to be equal if we wish; we will consider both approaches in this paper. (If these are not constrained to be equal, the centered representation is commonly used for downstream tasks.) We highlight Equation (12) is defined only as a function of \(UV^{T}\). There are two potential approaches to deal with this. We can regularize the objective function to enforce \(U^{T}U=V^{T}V\), which does not change the matrix \(UV^{T}\) that we recover . Alternatively, if these matrices are initialized to be balanced then they will remain balanced during the gradient descent procedure . Either procedure can be used to implicitly enforce \(U^{T}U=V^{T}V\), which reduces the symmetry group of \((U,V) UV^{T}\) to the orthogonal group. Similarly, if we constrain \(U=V\) then we obtain the same reduction.

### Using embeddings for community detection

Having learned embedding vectors \(_{i}\) for each node, we seek to use them for a further task, such as node clustering or classification. For community detection a natural procedure is to perform k-means clustering on the embedding vectors, using the estimated cluster assignments as inferred communities. k-means clustering  aims to find \(k\) vectors \(x_{1},,x_{k}^{d}\) which minimize the within cluster sum of squares. This can be formulated in terms of a matrix \(X^{k d}\) and a membership matrix \(\{0,1\}^{n k}\) where each row of \(\) has exactly \(k-1\) zero entries. Then the k-means clustering objective can be written as

\[_{}(,X)=\|- X \|_{F}^{2}\] (13)

where \(^{n d}\) is the matrix whose rows are the \(_{i}\). The non-zero entries in each row of \(\) gives the estimated community assignments. Finding exact minima to this minimization problem is NP-hard in general . For theoretical purposes, we will give guarantees for any \((1+)\)-minimizer to the above problem, which returns any pair \((,)\) for which \(_{}(,)(1+) _{,X}_{}(,X)\), and can be solved efficiently .

## 3 Results

Within this section, we give theoretical results which allow us to describe what happens when we use node2vec to learn embedding vectors for each node in the network, and then use these as features for a k-means clustering algorithm to perform community detection. Throughout, we assume that we observe a sequence of graphs \((_{n})_{n 1}\) on \(n\) vertices drawn from a probabilistic model and fit a node2vec model, according to one of the three scenarios below:

1. We use DeepWalk (\(p=q=1\) in node2vec), and the graph is drawn according to a SBM with \(_{n}(n)/n\);
2. We use node2vec, and the graph is drawn according to a SBM with \(_{n}=n^{-}\) for some \(<^{}\), where \(^{}\) depends on node2vec's hyperparameters;
3. We use DeepWalk and a unigram parameter of \(=1\), and the graph is drawn according to a DCSBM with \(_{n}(n)/n\) where the degree heterogeneity parameters \(_{u}[C^{-1},C]\) for some \(C<\).

All probabilistic statements below are with respect to the joint law of \(_{n}\) and the sampling which occurs to form the node2vec loss. All proofs are deferred to the Appendix. There we also provide extensions for the tasks of node classification and link prediction.

### Asymptotic distribution of the embeddings

We begin with a result which describes the asymptotic distribution of the gram matrices formed by the embeddings which minimize the loss \(_{n}(U,V)\) over matrices \(U,V^{n d}\).

**Theorem 2**.: _There exist constants \(_{}\) and \(_{2,}\) (depending on \(,P\) and the sampling scheme) and a matrix \(M^{*}^{n}\) (also depending on \(,P\) and the sampling scheme) such that when \(d(M^{*})\)

[MISSING_PAGE_FAIL:7]

Within the \((n,,,,_{n})\) model, we can show in the unconstrained case that \(^{2}=(|(/)|)\), and in the constrained case that \(^{2}=((/))\). As a result, this suggests that as \(/\) approaches \(1\), the task of distinguishing the communities becomes more difficult. This is inline with basic intuition, along with our experimental results in Section 4. We note that, due to the nature of the embedding vectors, for any proportion of vertices arbitrarily close to \(1\), the nodes will, with high probability for sufficiently large \(n\), be separated in the embedding space according to their community assignments. This separation allows clustering methods, such as DBSCAN, to accurately recover the communities of these nodes also.

Recall that from the discussion before, we know that \(M^{*}\) equals the zero matrix in the constrained regime when \(\) (and therefore the embeddings asymptotically contain no information about the network). As in the case where \(>\) we can show that \(>0\), we get the immediate corollary.

**Corollary 5**.: _Under scenario (i), suppose the embedding vectors learned through the node2vec loss are obtained by constraining the embedding matrices \(U=V\). Then the embeddings can be used for weakly consistent recovery of the communities if and only if \(>\)._

As a result, the constrained model can be disadvantageous if used without a-priori knowledge of the network beforehand (in that within-community connections outnumber between-community connections), even though it avoids interpretability issues about which embedding vector should be used as single representation for the node.

## 4 Experiments

In this section we provide simulation and real data experiments to empirically validate the previous theoretical results. We demonstrate the performance, in terms of community detection, of k-means clustering of the embedding vectors learned by node2vec, for both the regular and degree corrected stochastic block model. We also investigate the role of the negative sampling parameter \(\) and the node2vec tuning parameters \(p\) and \(q\), before examining performance on a real network with known community structure.

We first simulate data from the planted partition stochastic block model, \((n/,,,,_{n})\). We consider \(=\) for a range of values of \( 1\), giving varying strengths of associative community structure. In each setting we vary both the number of true communities present and the number of nodes in each community, considering \(n=200\) to \(n=5000\) and \(K=2,3,4,5\). We use node2vec to construct an embedding of the nodes in the network. 1 We use an embedding dimension of 64 and do not modify other default tuning parameters for the embedding procedure unless specified, so that \(p=q=1\). We investigate the role of these tuning parameters below, allowing them to vary as is considered in node2vec. We pass these embedding vectors into k-means clustering, where \(k=\), the true number of communities present in the network. This estimates a community assignment for each of the nodes in the network.

To evaluate the performance of our procedure, we compute the proportion of nodes correctly classified, up to permutation of the community assignments. For each simulation setting we perform 10 replications. We show the resulting estimates in Figure 1(a), for the relatively sparse setting where \(_{n}=(n)/n\). For all settings, the proportion of nodes assigned to the correct community by k-means clustering of the node2vec embeddings is high, particularly when the ratio of the between to within community edge probabilities, \(\), is small. As expected, as we increase the number of nodes in the network, a larger proportion of nodes are correctly recovered. We examine the empirical rate of convergence of this procedure in the Appendix. This appears to be approximately super-linear for dense networks and sub-linear for relatively sparse networks. Compared to the results of , this indicates that node2vec may be suboptimal. In the Appendix we also show community recovery using normalized mutual information (NMI) . We also see good performance.

We can similarly evaluate the performance of node2vec for data generated from a degree corrected SBM (DC-SBM). To generate such networks we modify the simulation setting used by . We generate the degree correction parameters \(_{u}=|Z_{u}|+1-(2)^{-1/2}\) where \(Z_{u} N(0,=0.25)\) and incorporate these into the \((n/,,,,_{n})\) considered previously. Two nodes \(u\) and \(v\) in the same community will have connection probability \(_{u}_{v}_{n}\) while for nodes in different communitiesit will be \(_{u}_{v}_{n}\). We again learn an embedding of the nodes using a default implementation of node2vec and cluster these embedding vectors using k-means clustering. We show the corresponding results, in terms of the proportion of the nodes assigned to their correct communities under this setting in Figure 1(b). As expected, the degree corrections make community recovery somewhat more challenging however as we increase the number of nodes in the network, we are able to correctly recover a high proportion of nodes.

We next wish to examine empirically the role of the unigram parameter \(\) of Equation (10), and how this affects community detection. While the previous theoretical results require \(=1\) for weak consistency of community recovery in the DC-SBM, we investigate if good empirical performance is possible with other choices of this parameter. We consider the DC-SBM simulation described previously, where we now vary \(\{-1,0,0.25,0.5,0.75,1\}\) when learning the node embeddings. For each of these settings (with all other parameters as before) we consider the proportion of nodes correctly recovered. We show this result for networks with \(=2\) communities in Figure 2. These experiments indicate similar performance for a range of values of \(\). Further work is needed to confirm the guarantees do indeed extend to these alternative choices of \(\), and we investigate this for real networks in Section A of the appendix.

We also investigate the role of the node2vec tuning parameters \(p\) and \(q\) on performance. For \(=2\) we consider \(=0.01\) and \(=0.2\), giving networks with strong and weak associative community structure respectively. We simulate from the previous relatively sparse DC-SBM with varying numbers of nodes and fit node2vec, using \(p,q\{0.5,1,2\}\). As the number of nodes in the network increases all choices of \(p\) and \(q\) give similar good performance for both choices of \(\). This indicates that the impact of these sampling parameters becomes limited as the networks become sufficiently large. We provide further discussion and a visualization of this result in Appendix A.

Finally, we briefly examine the performance of our community detection procedure on the political blog data collected by . As highlighted by , degree heterogeneity makes community recovery challenging for methods which do not account for this. We see similar performance if we cluster using a Gaussian mixture model rather than k-means clustering. In particular, spectral clustering struggles regardless of the graph Laplacian used. Our procedure shows excellent community recovery (average NMI of 0.75) for a range of embedding dimensions and unigram parameter settings as shown in Figure 3, with further details and an additional real network example in Appendix A.

Figure 1: Proportion of nodes correctly recovered for both the regular and degree corrected relatively sparse SBM.

## 5 Conclusion and Future Work

In this work we consider the theoretical properties of node embeddings learned from node2vec. We show, when the network is generated from a (degree corrected) stochastic block model, that the embeddings learned from DeepWalk and node2vec converge asymptotically to vectors depending only on their community assignment. As a result, we show that K-means clustering of the node2vec embedding vectors can provide weakly consistent estimates of the true community assignments of the nodes in the network. We verify these results empirically using simulated networks.

There are several important future directions which can extend this work. One direction is in extending the recovery results within the degree corrected SBM to the full range of hyperparmaeters for node2vec, as our simulation studies indicate that a more general result may hold. There is also the matter of increasing the strength of our results to give better rates and strongly consistent community detection; one possible avenue of exploration would be to see whether our results and the results of  could be combined to achieve this. Another improvement would be to study the behavior of the random walk on the graph in the sparse regime, although this would require a generalization of e.g the result of . We have also not considered the task of estimating \(\), the number of communities in a SBM model, using the embeddings obtained by node2vec. This has been considered for alternative approaches to community detection, ([22; 27] are some recent results) but not in the context of a general embedding of the nodes. Finally, there is a desire to obtain consistency results for more recent and complex network embedding methods, such as  and .