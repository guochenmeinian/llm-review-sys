# Approximation Rate of the Transformer Architecture

for Sequence Modeling

 Haotian Jiang

haotian.jiang@cnrsatcreate.sg

CNRS@CREATE LTD, 1 Create Way, #08-01 CREATE Tower, Singapore 138602

&Qianxiao Li

qianxiao@nus.edu.sg

CNRS@CREATE LTD, 1 Create Way, #08-01 CREATE Tower, Singapore 138602 Department of Mathematics, Institute for Functional Intelligent Materials, National University of Singapore

###### Abstract

The Transformer architecture is widely applied in sequence modeling applications, yet the theoretical understanding of its working principles remains limited. In this work, we investigate the approximation rate results for the Transformer architectures on general sequence to sequence target relationships. We begin by establishing a representation theorem for the target space and introduce a novel notion of complexity measures to construct approximation spaces. These measures encapsulate both pairwise and pointwise interactions among input tokens. Based on this framework, we derive an explicit Jackson-type approximation rate estimate for the Transformer. This rate sheds light on the underlying structural characteristics of the Transformer, thereby delineating the types of sequential relationships they excel in approximating. Notably, our findings on approximation rates facilitate a concrete comparison between the Transformer and traditional sequence modeling approaches, such as recurrent neural networks.

## 1 Introduction

The Transformer architecture, as introduced by Vaswani et al.  has become immensely popular in the field of sequence modeling. Variants such as BERT  and GPT  have achieved excellent performance, becoming the default choices for natural language processing (NLP) problems. Concurrently, Dosovitskiy et al.  successfully applied the Transformer to image classification problems by flattening the image into a sequence of patches. Despite its success across various fields of practical application, many theoretical questions remain unanswered. Among these, we focus on two essential questions in this work: firstly, the approximation rate of the Transformer on sequence modeling; secondly, the comparative advantages and disadvantages of the Transformer with recurrent neural networks (RNNs) on different temporal structures.

The concept of Jackson-type approximation rates is derived from the Jackson Theorem for polynomials , and is further elaborated in the work of DeVore  for addressing general forward approximation problems. To illustrate this, consider classic polynomial approximation. It involves defining an appropriate approximation space, accompanied by specific complexity measures, precisely the Sobolev space. According to the Jackson theorem, a function with a small Sobolev norm can be efficiently approximated by a polynomial. We aim to establish similar approximation rate results for the Transformer. This identifies the type of targets that the Transformer can efficiently learn. We examine general non-linear sequence-to-sequence target relationships, extending the linear target form explored in previous studies of the approximation results for linear RNNs and linear temporal convolution networks . We introduce a novel notion of complexity, establishing a concrete target space from which approximation rates can be deduced. This enables us to identify the types of sequential relationships that Transformer can approximate efficiently. Based on our theoretical analysis, we identify concrete classes of temporal structures where the Transformer outperformsthe RNNs and vice versa. Our main contributions are summarized as follows: 1. We develop Jackson-type approximation rate results for single-layer Transformer networks with one attention head. Our analysis reveals that the approximation capacity is governed by a low-rank structure within the pairwise coupling of the target's temporal features. Empirical validation confirms that the findings observed under theoretical settings also hold true in practical applications. 2. We conduct a comparative analysis between the Transformer and RNNs, aiming to identify specific types of temporal structures where one model excels or underperforms compared to the other.

## 2 Related work

We first review the approximation results of Transformer networks. The universal approximation property (UAP) of the Transformer architecture is first proved in Yun et al. , which is further extended to Transformers with sparse attention matrices . The above universal results are developed for a deep Transformer structure. In contrast, Kajitsuka & Sato  applying a similar technique to prove one layer Transformers can achieve UAP by increasing the width. Additionally, Kratsios et al. , Edelman et al. , Luo et al.  considers the UAP of the Transformer in various different settings. Giannou et al.  considers a special setting regarding expressiveness, demonstrating that Transformers can represent any computer program. Beyond the UAP results, there have been developments in specific approximation rate results. Gurevych et al.  demonstrated the rate of the misclassification probability by considering the approximation of hierarchical composition functions, which are composed of sparse functions. This rate takes into account both the level of composition and the smoothness of the component functions. Bai et al.  and Wang & E  explore target relationships with certain special structures. Additionally, Takakura & Suzuki  developed approximation rates for a function space consisting of infinite-length sequence-to-sequence functions, which is characterized by the smoothness of the functions. Our approximation rate result steps further by considering temporal structures, shedding light on how the Transformer model handles temporal relationships. Apart from the approximation results, numerous intrinsic properties of the Transformer have been investigated. Dong et al.  and Bhojanapalli et al.  considers the rank structure of the attention matrices. Levine et al.  examine the correlation between the dependency of input variables and the depth of the model. The Transformer is a very flexible architecture, such that a special configuration of parameters can emulate other architectures. For example, Cordonnier et al. , Li et al.  showed that attention layers under certain assumptions can perform convolution operations. However, not all simulations are valid explanations of the working principles of the Transformer. In this context, definitions of complexity measures and the resulting Jackson-type approximation rate estimates provide more insight into the inner workings of the architecture. This is the focus of the current work.

## 3 Sequence modeling as an approximation problem

We first motivate the theoretical settings of Jackson-type approximation rate results by considering classic polynomial approximation. Then, we formulate the Transformer as an instance of such an approximation problem.

Motivation of Jackson-type Approximation RatesConsider two normed vector spaces, \(\) and \(\), designated as the input and output spaces, respectively. We define the target space \(^{}\) as a set of mappings from \(\) to \(\) that we aim to approximate with simpler functions. The hypothesis space is denoted by \(=_{m}^{(m)}\), where \(^{(m)}\) represents a sequence of hypothesis spaces. Here, \(m\) denotes the complexity or the approximation budget of these spaces. Hypothesis space \(\) encompasses the candidate functions used to approximate targets in \(\). Let \(\) be a constant, we introduce a complexity measure, denoted as \(C^{()}:\), based on the structure of \(\). The complexity measure is used to construct an approximation space \(^{()}:=\{H:C^{()}(H)<\}\), which is usually dense in \(\). Then for any \(H^{()}\), the Jackson-type approximation rate is expressed as follows:

\[_{^{(m)}}\|H-\| E(C^{()} (H),m).\] (1)

Here, the error bound \(E(,m)\) decreases to zero as \(m\) approaches infinity, and the rate of decay is usually called the approximation rate. In this context, \(C^{()}(H)\) quantifies the complexity of a target \(H\) when approximated using \(\). A smaller value indicates that the target \(H\) can be more efficiently approximated with candidates from \(\). Consequently, the complexity measure discerns the types of targets that can be efficiently approximated within the given hypothesis space. Notably, different hypothesis spaces typically give rise to different complexity measures and approximation spaces. These variations characterize the approximation capabilities of the hypothesis spaces themselves.

Defining an appropriate approximation space \(^{()}\) is essential. Without specific structures, the general target space \(\) does not provide any rate results. Opting for an approximation space with defined structures allows for the derivation of approximation rates. Moreover, the property that \(^{()}\) is dense in \(\) ensures that the restriction to \(^{()}\) is not overly limiting, preserving the necessary expressiveness of the space. To illustrate these concepts, we consider polynomial approximation over the interval \(\). In this case we set \(=\) and \(=\). The target space \(=C()\) is the set of continuous functions defined on \(\). The hypothesis space comprises all polynomials, expressed as follows:

\[=_{m}^{(m)}=_{m }\{(x)=_{k=0}^{m-1}a_{k}x^{k}:a_{k}\}.\]

According to the Jackson theorem for , the Sobolev norm serves as an appropriate complexity measure, defined as \(C^{()}(H)=_{r=1}\|H^{(r)}\|_{}\). Let \(^{()}\) be the approximation space containing targets with finite complexity measures. Consequently, for \(H^{()}\), we have the following approximate rate:

\[_{^{(m)}}\|H-\|}{m^{ }}C(H).\] (2)

Here, \(c_{}\) is a constant depending only on \(\). This theorem implies that smooth functions with small Sobolev norms can be efficiently approximated by polynomials. Developing Jackson-type approximation rates for various sequence modeling hypothesis spaces is crucial for understanding their differences. Jackson-type results for RNNs, CNNs, and encoder-decoder hypothesis spaces have been established in Li et al. , Jiang et al. , Li et al. , where complexity measures such as decaying memory and sparsity were found to influence the approximation rates. In Section 4.1, we identify appropriate complexity measures regarding the Transformer and develop corresponding approximation rates. This enables us to discern the essential structures that facilitate efficient approximation using the Transformer. Furthermore, it allows us to understand how and when the Transformer architecture differs from traditional sequence modeling architecture RNNs, which we will discuss in Section 6.

Formulation of Sequence Modeling as Approximation ProblemsIn sequence modeling, we seek to learn relationships between two sequences \(\) and \(\). Mathematically, we consider an input sequence space

\[=\{:x(s)^{d},s[]\}.\] (3)

Here, \([]:=\{1,,\}\) and \(\) denotes the maximum length of the input, and we focus on the finite setting, where \(<\). Corresponding to each input \(\) is an output sequence \(\) belonging to

\[=\{:y(s),s[]\}.\] (4)

We use \(:=\{H_{t}\}_{t=1}^{}\) to denote the mapping between \(\) and \(\), such that \(y(t)=H_{t}()\) for each \(t[]\). Define \(C(,)\) to denote the space of continuous mappings between the input and output space. We may regard each \(H_{t}:^{d}\) as a \(\)-variable function, where each variable is a vector in \(^{d}\). Next, we define the Transformer hypothesis space.

The Transformer Hypothesis Space.We consider the following Transformer block retaining most of the important components.

\[_{t}()=_{s=1}^{}[(W_{Q}(t))^ {}W_{K}()](s) W_{V}(s),\] (5)

where \(= x\) and \(:^{m_{v}},:^{d} ^{n}\) are two feed-forward networks. The parameter matrices have dimension \(W_{Q},W_{K}^{m_{h} n}\), \(W_{V}^{m_{v} n}\). The softmax function is denoted as \(\), such that \([(t,)](s)=}((t,s ^{}))}\). We focus on a simplified architecture: a single-layerTransformer with one head. In this work, layer normalization and residual connections are not taken into account. While this constitutes a simplified setting intended for theoretical analysis, it's worth noting that the phenomena observed under the theoretical settings also hold true in practical applications, as we have demonstrated in Section 5. The approximation budget of the Transformer depends on several components. We use \(}^{(m_{})}\) to denote the class of feed-forward networks used in the Transformer with budget \(m_{}\), which is usually determined by the number of neurons and layers. Let \(m=(n,m_{h},m_{v},m_{})\) denote the overall approximation budget. We use \(^{(m)}\) to denote the Transformer with complexity \(m\). Then, we define the Transformer hypothesis space by

\[=_{m}^{(m)},^{(m)}= }:}) with }m}.\] (6)

## 4 Approximation results

Following the motivation of approximation problems for sequence modeling as discussed in Section 3, this section discusses the approximation rate results for the Transformer. Firstly, we introduce the notion of the permutation equivariance property of the Transformer and discuss the role of position encodings in eliminating it. Next, we define the target space and present the corresponding representation theorem. We then establish the complexity measures necessary to form the approximation space. Our main result Theorem 4.2 presents the approximation rate results.

Permutation Equivariance and Position EncodingsOur objective is to approximate a target relationship \(\) where each \(\) belongs to \(C(,)\). It is important to note that without specific modifications, the Transformer inherently cannot approximate such targets due to its permutation equivariance properties. This property implies that permuting the temporal indices of the input sequence results in a corresponding permutation of the output sequence. More precisely, if \(p\) denotes a bijection on \([]\) representing a permutation of \(\) objects, a sequence of functions \(\) is considered permutation equivariant if for all bijections \(p\) and inputs \(\), the condition \(( p)=() p\) holds. The Transformer \(}\) within the hypothesis space \(\) is indeed permutation equivariant (refer to Appendix A for details). Directly applying the Transformer, therefore, yields permutation equivariant hypotheses, which are inadequate for approximating general sequential relationships that lack this symmetry. In practical applications, incorporating position encodings is a widely adopted approach to counteract permutation equivariance . Various methods exist for embedding positional information. The simplest approach is fixed encoding, which involves mapping \(x(t)\) to a higher-dimensional space and then offsetting each \(x(t)\) by distinct distances. Formally, with \(A^{d^{} d}\) where \(d^{} d\) and a constant or trainable \(e(t)^{d^{}}\), position encodings can be expressed as \(x(t) A\,x(t)+e(t)\). For general purposes, it's sufficient for the encoded input space \(^{(E)}\) to satisfy the following condition:

\[^{(E)}=:x(s)^{(s)}^{d },^{(i)},^{(j)}\, i,j[] }.\] (7)

This ensures that for each input \(=(x(1),,x())\), all tokens \(x(i)\) and \(x(j)\) are distinct, meaning no two input sequences are temporal permutations of each other. Define the set \(=_{s}\) to be the range of the inputs. Moving forward, we assume that position encoding has been applied, allowing us to consider \(^{(E)}\) as the input space. Consequently, we define the target space to be \(=C(^{(E)},)\), which denotes the space of continuous mappings between \(^{(E)}\) and \(\).

### Jackson-type approximation rate

To derive the Jackson-type approximation rate, it is necessary first to define the complexity measures to form an approximation space so that approximation rates can be obtained. We begin with the following representation theorem for the target space.

Representation of the target space \(\)Given that the Transformer inherently captures both pairwise and pointwise relations among input tokens through attention and feed-forward components, respectively, we are motivated to establish the following representation theorem for the target space \(\). This theorem demonstrates that every target can be exactly expressed in terms of pairwise and pointwise relations.

**Theorem 4.1** (Representation of the target space).: _Consider \(d\)-dimensional, length \(\) input space \(^{(E)}\) with position encoding added. Then, for any \( C(^{(E)},)\), there exists continuous functions \(F C(^{n},)\), \(f C(,^{n})\) and \( C(,)\) such that for all \(t[]\) we have_

\[H_{t}()=F(_{s=1}^{}[(x(t),x())](s)f(x(s)) ),\] (8)

_where \(n=2 d+1\) and \(\) is the softmax function. The proof is presented in Appendix A.2._

We refer to \(\) as the temporal coupling component and \(F\) and \(f\) as the element-wise components. It's important to note that the functions \(F\), \(f\), and \(\) may not be uniquely determined. In Appendix B.1, we explore certain invariant properties associated with Equation 8. Additionally, we provide illustrative examples in Appendix B.1 where the target explicitly conforms to Equation 8. Leveraging this representation theorem, we define the following complexity measures for targets \(\).

Temporal Coupling ComponentNow, we discuss the complexity measures associated with the temporal coupling term \((u,v)\) that is central to understanding the attention mechanism in the Transformer. We employ the proper orthogonal decomposition (POD)  to decompose the temporal coupling of \(\). This approach can be viewed as an extension of matrix singular value decomposition (SVD) to functions of two variables. We have the following decomposition: \((u,v)=_{k=1}^{}_{k}_{k}(u)_{k}(v),\) where \(_{k},_{k}\) are orthonormal bases for \(L^{2}()\), and the singular values \(_{k} 0\) are arranged in descending order. The bases \(_{k}\) and \(_{k}\) are of optimal choices, ensuring that \((u,v)\) satisfies:

\[_{() r}\|(u,v)-(u,v)\|_ {2}^{2}=_{k=r+1}^{}_{k}^{2},\] (9)

analogous to the Eckart-Young theorem for matrices, with the rank defined as the number of terms in the POD decomposition. This implies that the approximation quality of \((u,v)\) can be measured by the decay rate of its singular values. This motivates our following definition of complexity measure regarding \(\). Let \(>1/2\) be a constant, and \(\{_{i}^{()}\}\) be singular values of \(\) under POD. We define the complexity measure of \(\) by

\[C_{1}^{()}()=_{F,f,}\{c:_{s}^{()} cs ^{-},s 1\},\] (10)

where the first infimum is taken over all \(F,f,\) such that Equation (8) holds. In particular, \(C_{1}^{(g)}()<\) if it has a representation in the form (8) with \(\) having fast decaying singular values.

Element-wise ComponentNow, we introduce the complexity measure for approximating the element-wise components \(F\) and \(f\). Let \(^{(m_{})}\) be a hypothesis space comprising feed-forward neural networks with a budget of \(m_{}\), representing parameters such as width or depth. We assume the existence of \(>0\) such that:

\[_{^{(m_{})}}\|f-\| }^{()}(f)}{m_{}{}^{}}.\] (11)

Here, \(C_{}\) represents a complexity measure of \(f\) corresponding to its approximation by \(^{(m_{})}\). It is essential to emphasize that we are assuming the existence of pre-existing approximation rate results for the feed-forward component. For instance, in Barron , \(^{(m_{})}\) is considered to be one-layer neural networks with sigmoidal activation, where \(m_{FF}\) corresponds to the width of the network. By adopting this result, we have \(=1/2\) and \(C_{}^{()}(f)\) is a moment of the Fourier transform of \(f\). For shallow ReLU networks commonly used in Transformers, Klusowski & Barron  demonstrate \(=1/2+1/n\), where \(n\) is the input dimension of the neural network. We maintain the generality of Equation (11). This enables us to substitute any other relevant estimates from the mentioned references. This flexibility allows for a broader application of the complexity measure in different scenarios and settings. Next, we proceed to define the complexity measure for \(\). This measure considers all the components that need to be approximated using the feed-forward network.

\[C_{2}^{()}(,k)=_{F,f,}(C_{}^{()}(F)+C_{ }^{()}(f)+C_{}^{()}(,k)),\] (12)where \(C_{}^{()}(,k)=_{i=1}^{k}(C_{}^{()}(_{i} )+C_{}^{()}(_{i}))\) considers the approximation of the approximation of the POD bases \(_{i}\) and \(_{i}\) for the target function \(\). Notably, this complexity measure is dependent on a parameter \(k\), which determines the number of bases we want to consider in the approximation of \(\). Finally, we define a complexity measure considering the norm of the target.

\[C_{0}()=_{F,f,}\{K_{F}\|f\|_{}(_{i} \|_{i}\|_{}+\|_{i}\|_{}),1\},\] (13)

where \(K_{F}\) is the Lipschitz constant of \(F\) and \(_{i},_{i}\) are POD bases of \(\).

Approximation RatesCombining the complexity measures Equations (10) and (13) and Equation (12) discussed above, we define the approximation spaces, which consists of targets that have finite complexity measures:

\[^{(,)}=\{:C_{0}()+C_{1}^{( )}()+C_{2}^{()}(,k)<,\,k 1\}.\] (14)

One can understand this as an analog of the classical Sobolev spaces for polynomial approximation but adapted to the Transformer hypothesis space. Note that \(^{(,)}\) is also dense in general continuous target space \(C(^{(E)},)\) when \(n\) sufficiently large (See Appendix A.2). We are now ready to present the main result of this paper.

**Theorem 4.2** (Jackson-type approximation rates for the Transformer).: _Consider sequences with a fixed length \(\). Suppose the target \(^{(,)}\) has a representation in the form Equation (8) with \(F C(^{n^{}},)\) and \(f C(,^{n^{}})\). Let the hidden dimension of the Transformer be \(n=2*m_{h}+m_{v}\), with \(m_{v} n^{}\). Then, we have:_

\[_{}^{(m)}}_{}_{t}^{}| H_{t}()-_{t}()|d^{2}C_{0}()( ^{()}()}{{m_{h}}^{2-1}}+^{()}( ,m_{h})}{m_{}^{}}(m_{h})^{+1}),\]

_where \(m=(n,m_{h},m_{v},m_{})\) is the approximation budget, and \(C_{0},C_{1}^{()}C_{2}^{()}\) are complexity measures of \(\) defined in (13), (10) and (12), respectively. The proof is presented in Appendix A.3._

Here, \(m_{h}\) denotes the hidden dimension of the attention mechanism, essentially the size of the query and key vectors, while \(m_{}\) represents the complexity measure of the pointwise feed-forward network employed in the Transformer. We first consider how the attention mechanism \((x(t),x(s))\) approximates the temporal coupling term \(\). By setting \([W_{Q}]_{k}=e_{k}\) and \([W_{K}]_{k}=e_{k+m_{h}+1}\), we can write \(\) into \((x(t),x(s))=(W_{Q}(x(t)))^{}W_{K}(x(s))=_{k=1 }^{m_{h}}_{k}(x(t))_{k}(x(s))\), where \(_{k},_{k}:\) for \(k[m_{h}]\) are components of \(\) and are represented by the feed-forward network. This suggests that the approximation of \(\) by \(\) is a low-rank approximation as discussed in Equation (9). When we increase \(m_{h}\), the first term in the error rate that considers the POD decomposition decreases since there are more basis functions included. However, in scenarios where \(m_{}\) remains unchanged, the second term in the error bound will increase. It is important to highlight that this error increment pertains only to the error bound, not to the best approximation error, which does not necessarily become worse when increasing the approximation budget. When \(m_{h}\) increases, there are more basis functions that need to be approximated by the feed-forward components; thus, the error bound converges when both \(m_{h}\) and \(m_{}^{}/m_{h}^{+1}\). In Appendix B.2, we provide a synthetic example to illustrate the above discussion.

The complexity measure \(C_{2}()\) accounts for the quality of approximation using feed-forward networks. On the other hand, \(C_{1}()\) is the most interesting part, which concerns the internal structure of the attention mechanism. It tells us that if a target can be written in form (8) with \((u,v)\), then it can be efficiently approximated with small \(m_{h}\) if \((u,v)\) has fast decaying singular values. This decay condition can be understood as effectively a low-rank condition on \((u,v)(u,v)\), analogous to the familiar concept for low-rank approximations of matrices. These observations provide important insights into the structure, bias, and limitations of the Transformer.

## 5 Numerical Demonstrations

In this section, we present numerical examples to demonstrate our approximation rate results. We begin with synthetic examples, where we can specify the singular value decay patterns, thus validatingthe Jackson-type approximation rates in Theorem 4.2. Then, we turn to a practical example involving a Vision Transformer (ViT) model applied to the CIFAR10 dataset, where we do not have direct access to the singular values. We will discuss methods to estimate the singular value decaying pattern.

### Practical Example

We next analyze a practical example, focusing on the Vision Transformer (ViT) model with the CIFAR10 dataset. In this scenario, we do have direct access to the temporal coupling term \(\) of the target relationship. However, we demonstrate that as we train a sequence of models with increasing attention dimension \(m_{h}\), the singular values of \(\) converges, implying the decaying pattern of \(\). Our first step is to estimate the rank of \(\) from sampled data. Subsequently, we examine the singular value decay pattern, and the error changes when altering the attention head size \(m_{h}\).

Estimate the Rank of \(\)Given a trained model, it is hard to directly compute the rank of \((u,v)\). Instead, we examine the attention matrix \(}()^{}\) where \([}()]_{t,s}=(x(t),x(s))\). This is essentially a sample from \(\). By analyzing the rank of these sampled matrices, we can estimate the rank of \(\). According to Braun , the singular values of the matrix \(}()\) exhibit the same decay pattern as those of \((u,v)\). Consequently, we can approximate the singular values of \((u,v)\) by averaging the singular values of \(}()\) across various inputs. In Figure 1(a), we numerically estimate the singular values of \(\) in the ViT-B_16 model . We observe that the singular values tend to be more concentrated, suggesting that we can effectively estimate the rank of \((u,v)\) by evaluating it at sampled inputs.

We next estimate the singular value decaying pattern of the temporal coupling term \(\) in the target. In Figure 1(b), we analyze the singular values of the \(\) for ViT models with varying values of \(m_{h}\). We estimate the singular values by averaging over a set of inputs. We observe that as \(m_{h}\) increases and reaches a sufficiently large value, the decaying pattern of the singular values starts to converge. As an example, Figure 1(b) plots the estimated singular values for the first head of the last layer for models with different \(m_{h}\). This convergence suggests that the rank of the attention matrix becomes representative of the actual rank of \(\) in the target. Consequently, it suggests the presence of a low-rank structure in real-world datasets. In Figure 1(c), we plot the training error as an estimation of the approximation error. The plot reveals that the error decreases as \(m_{h}\) increases, following a power law decaying pattern \(O1/m_{h}^{0.27}\). This indicates that the target indeed exhibits a low-rank structure, and the pattern of error decay aligns with our approximation rate presented in Theorem 4.2. This illustrates that while our theorem is formulated based on a simplified scenario, the phenomenon of low rank is also observable in real-world datasets and models.

## 6 Comparison with RNN

Based on the approximation results in Theorem 4.2, this section presents a comparison between the Transformer and RNN. Our comparison centers on how each model is affected by the alterations in the temporal structures of sequential relationships. We primarily investigate two distinct temporal

Figure 1: (a) is the estimated singular value of the attention matrix over a set of inputs for \(m_{h}=64\). The violin plot shows the distribution of each singular value. (b) plots the estimated singular values for models with different \(m_{h}\). (c) plots the training error against \(m_{h}\).

structures: temporal ordering and temporal mixing, as they have varying impacts on the performance of each architecture. Our approach involves manipulating the temporal structures within the sequential relationships and evaluating how each architecture is affected by these changes. Proof are presented in Appendix D.2 and Appendix D.3.

### Temporal Ordering Structure

We first analyze how the Transformer and RNN handle the change in temporal ordering of the sequential relationship. Empirically, it is observed that in certain contexts, the ordering of inputs does not significantly impact the relationships. For instance, in NLP applications, altering the word order in a sentence often does not drastically change its meaning. Similarly, in the ViT model, the arrangement of image patches typically does not substantially affect the outcome. However, in specific applications such as time series analysis, temporal ordering plays a crucial role, as the target relationships are governed by the ordering of the sequence. To alter the temporal order of target \(\), we apply a fixed permutation \(p\) and define the new target as \(_{t}( p)=H_{t}()\). This permutes the input but keeps the output unchanged, resulting in a change to the temporal ordering.

We start by considering the RNN, which is affected by the change in temporal ordering. As demonstrated in Li et al. , a linear RNN is represented by the form \(_{t}()=_{s}c^{}e^{Ws}Ux(t-s)\). When we employ it to approximate linear targets represented by \(H_{t}()=_{s}(s)x(t-s)\), the complexity measures of the RNN \(C_{}()\) is determined by both decay speed and magnitude of \((s)\). We use \(_{}\) to denote the approximation space for the RNN. (See Appendix D.1). We next show that the RNN is affected by the change in temporal ordering.

**Proposition 6.1**.: _Let \(_{}\) and \(p\) be a fixed permutation, such that there exists \(t^{}\) with \(p(t^{})>t^{}\). Suppose \(}\) is defined by \(_{t}( p)=H_{t}()\). Then \(}_{}\)._

This proposition shows that the altered target \(}\) no longer belongs to the approximation space for RNN. This lies in the fact that RNN can only handle causal targets, where \(y(t)\) does not depend on future inputs. However, the permuted target \(}\) is no longer causal, making the RNN incapable of learning such relationships. We next show that the Transformer, in contrast, remains unaffected by changes in temporal ordering.

**Proposition 6.2**.: _Let \(^{(,)}\) and \(p\) be a fixed permutation. Suppose \(}\) is defined by \(_{t}( p)=H_{t}()\). Then \(}\) have same complexity measures with \(\) for the complexity measures \(C_{0},C_{1},C_{2}\) defined in Equation (13),(10) and (12)._

This proposition shows that the altered target \(}\) maintains the same complexity measures as the original target. This observation implies that the Transformer's approximation capability is not affected by alterations in temporal ordering. This point is further substantiated by our testing of the Transformer on real-world datasets, as illustrated in Table 1. We consider the ViT model on the CIFAR10 dataset and the base Transformer structure  on the WMT2014 English-German dataset. To alter the temporal ordering of the target relationship, we fix a permutation of indices denoted as \(p\) and apply it to all inputs while keeping the output unchanged. The experimental results provide evidence that the performance of the Transformer is unaffected by the temporal ordering of the target relationships.

### Temporal Mixing Structure

In this section, we explore how mixing elements from different time indices can affect the performance of the Transformer and RNN. Temporal mixing refers to the idea of blending information from various time indices, often through operations like convolution, which can alter the temporal structure. To

    & CIFAR10 (_Acc._) & ENG-DE (_BLUE_) \\  Original & \(0.98\) & \(26.85\) \\ Altered & \(0.96\) & \(25.91\) \\   

Table 1: Numerical results of the Transformer on original and altered targets. The altered target is constructed by permuting the entire input dataset while keeping the output unchanged.

illustrate, consider a linear relationship represented as \(H_{t}()=_{s}(s)x(t-s)\). Now, imagine we apply a weighted sum of the input sequence \(\) using a filter \(\) to get an altered input. We denote this operation as \(}=\), where \(()[t]=_{s=0}^{l-1}(s)(t+s)\). This mixes the information in the sequence from different time indices. In this case, we define the altered target as \(_{t}()=H_{t}()=_{s}(s)x(t-s)\), where \(=\) is the altered kernel (See Appendix D.3). This scenario often arises in signal processing and data analysis. For example, when dealing with a noisy input signal \(\), one approach is to apply a moving average filter to smooth it out. This filtering process involves mixing information from different time indices, which can significantly affect the behavior of target relationships. In the following sections, we will explore how such temporal mixing affects the temporal structures in sequential relationships. We begin by examining the linear RNN.

**Proposition 6.3**.: _Let \(_{}\) associated with representation \(\), such that \(|(s)| e^{-}\) for some \(>0\). Let \(\) be a length \(l\) filter such that \(_{1} 1\). Suppose \(}\) is defined by \(_{t}()=H_{t}()\). Then we have both \(C_{}()\) and \(C_{}()\), where \(C_{}\) is the complexity measure of the RNN (See Equation (75))._

This proposition shows that under temporal mixing \(\) with certain conditions, the complexity measure of the altered target \(}\) does not worsen. As a result, the performance of the RNN is unaffected in such cases. However, performance of the Transformer can be impacted by temporal mixing in the target relationship. Consider \(^{(,)}\) and an altered target \(_{t}()=H_{t}()\). This alteration can affect the complexity measures for \(,\) and \(\) in \(}\). In Appendix D.3, we present an example where the rank of \(\) increases compared to \(\), leading to a performance drop in the Transformer. Numerical results in Table 1 also indicate that temporal mixing influences the Transformer's approximation capability. Preprocessing the input to mitigate this temporal mixing could potentially enhance performance, we leave this as a future direction. The following numerical examples demonstrate the above discussions. We conduct numerical experiments to substantiate the discussions above. These experiments focus on a linear target relationship defined as \(H_{t}()=_{s}e^{-s}x(t-s)\). To manipulate the temporal ordering, we apply permute the function \(e^{-s}\). Additionally, for introducing temporal mixing, the input is convolved with a randomly generated filter. Both the RNN and the Transformer are employed to learn these targets. Detailed experimental settings are discussed in the Appendix C. The results are presented in Table 2. The bold font indicates a performance drop in the architecture under the corresponding modification of temporal structures. It is observed that the performance of the Transformer remains unaffected by changes in the temporal ordering structure; however, it is impacted by temporal mixing. In contrast, the RNN exhibits opposite behaviors. This highlights that neither architecture consistently outperforms the other, as they each adapt to different types of temporal structures.

## 7 Conclusion

In this paper, we have developed Jackson-type approximation rates for the Transformer in a simplified setting. An important outcome of our work is the identification of complexity measures and approximation space defined in Equation (14). This, in turn, enables us to derive explicit Jackson-type approximation rate results in Theorem 4.2. Our rate results suggest that the Transformer performs well when the temporal coupling of the target exhibits a low-rank pattern. The experiments presented in Section 5 showcase the existence of low-rank patterns in real-world applications. Furthermore, the comparisons with RNNs underscore the specific temporal structures that each model handles efficiently. Future research directions involve extending the analysis to multi-headed attention and deeper Transformers. Additionally, we aim to investigate the potential benefits of removing temporal mixing in the input to enhance the performance of the Transformer.

    &  &  \\   & **RNN** & Trans. & RNN & **Trans.** \\  Original & \(\) & \(2.18e-5\) & \(1.02e-7\) & \(\) \\ Altered & \(\) & \(2.51e-5\) & \(1.58e-7\) & \(\) \\   

Table 2: The table presents the MSE values for both RNN and the Transformer under different alterations of temporal structures.