# Sample-Efficient Geometry Reconstruction from Euclidean Distances using Non-Convex Optimization

Ipsita Ghosh

Department of Computer Science

University of North Carolina at Charlotte

ighosh2@charlotte.edu &Abiy Tasissa

Department of Mathematics

Tufts University

Abiy.Tassisa@tufts.edu &Christian Kummerle

Department of Computer Science

University of North Carolina at Charlotte

kuemmerle@charlotte.edu

###### Abstract

The problem of finding suitable point embedding or geometric configurations given only Euclidean distance information of point pairs arises both as a core task and as a sub-problem in a variety of machine learning applications. In this paper, we aim to solve this problem given a minimal number of distance samples. To this end, we leverage continuous and non-convex rank minimization formulations of the problem and establish a local convergence guarantee for a variant of iteratively reweighted least squares (IRLS), which applies if a minimal random set of observed distances is provided. As a technical tool, we establish a restricted isometry property (RIP) restricted to a tangent space of the manifold of symmetric rank-\(r\) matrices given random Euclidean distance measurements, which might be of independent interest for the analysis of other non-convex approaches. Furthermore, we assess data efficiency, scalability and generalizability of different reconstruction algorithms through numerical experiments with simulated data as well as real-world data, demonstrating the proposed algorithm's ability to identify the underlying geometry from fewer distance samples compared to the state-of-the-art.

The Matlab code can be found at github_EDG-IRLS

## 1 Introduction

Euclidean Distance Geometry (EDG) problems have applications spanning diverse domains, from comprehending protein structures through molecular conformations , prediction of molecular conformations in computational chemistry  and aiding dimensional reduction in machine learning  to facilitating localization in sensor networks , coupled with its role in solving partial differential equations on manifolds . This emphasizes its broad impact in computational sciences.  proposes embedding entities based on Distance Geometry Problems (DGP), where object positions are determined based on a subset of pairwise distances or inner products which significantly reduces computational complexity compared to traditional word embedding methods.

_Problem 1_.: Mathematically, consider a collection of \(n\) points \(_{i}\) in an \(r\)-dimensional Euclidean space with coordinates \(=[_{1},_{2},...,_{n}]^{ r n}\) whose pairwise squared Euclidean distances are given by \(d_{ij}^{2}=||_{i}-_{j}||^{2}\) for each \(1 i j n\) where \(||||\) denotes the Euclidean norm. Given only partial information of the \(\{d_{ij}\}\) such that only a subset of cardinality \(m<n(n-1)/2\) is known, the goal is to reconstruct the geometry of the points, that is to recover the point coordinates \(\).

If all the distances between the points are provided, the problem is known as multidimensional scaling , and closed solution formula exists. However, this is not the case for the incomplete setup which is the focus of this work, and in which only partial information of the pairwise distances is available.

For instance, AlphaFold  has shown effectiveness in predicting the three-dimensional structure of protein given as input its amino acid sequence. AlphaFold\(2\), which uses an attention mechanism-based transformer architecture , is trained on known sequences and structures from the Protein Data Bank  and determines the distances between the \(C_{}\) atoms of all residue pairs in a protein. Subsequently, as a subproblem, this distance information is used to predict the protein's structure by identifying the _foldings_ within the protein molecule. In the context of this subproblem, where the input is the predicted pairwise distances, a linear mapping can be used to predict geometric coordinates. However, many of the predicted distances might not be accurate, which is why low confidence regions of the resulting structures, as indicated by the predicted local-distance difference test (pLDDT) , could be masked, and a new structure could subsequently be re-computed based on the distance entries of the medium-to-high confidence regions using high-accuracy solution algorithms for Problem 1.

In the context of the Problem 1, we have access to a subset of entries of this \(=[d_{ij}^{2}]\) matrix, defined by the index set \(\{1,,n\}^{2}\). Now, we formulate the Gram matrix \(^{0}=^{}\), residing in the set of real-valued symmetric matrices \(S_{n}\). This matrix has a lower rank \(r\) compared to the symmetric distance matrix \( S_{n}=\{^{n n}:=^{}\}\), which has a rank of \(r+2\).

To make the solution translation invariant, the centroid of the points must be the origin, i.e., \(_{=1}^{n}p_{}=0\). To ensure this, the Gram matrix should satisfy also satisfy \(^{0}=0\), where \(^{n}\) is the vector of all ones. Based on these observations, we can frame Problem 1 as an instance of the computationally challenging NP-hard _rank minimization_ problem defined by

\[_{ S_{n},:=0,} ()_{i,i}+_{j,j}-2 _{i,j}=_{i,j}(i,j),\] (1)

incorporating also the positive semi-definiteness constraint \(\), which comes from the observation that \(^{0}=^{}\) is also PSD.

From an optimization perspective, this rank-minimization problem is highly complex due to its non-convexity and non-smoothness. Being an NP-hard problem , it is extremely difficult to solve directly. Consequently, a significant amount of existing research  has focused on minimizing the convex envelope of the rank function, known as the nuclear norm, instead. However, it was observed that for related low-rank matrix completion problem, such convex relaxations are not as data-efficient as other formulations , while posing also computational limitations .

In this paper, we study the question of, given _incomplete_ distance information Problem 1, how many samples are necessary to ensure accurate recovery?  established convergence with \(O( rn^{2}n)\) uniformly random samples for solving (1) using a nuclear norm surrogate. While there are numerous results for non-convex methods in other low-rank recovery problems , there are currently only few non-convex algorithm specifically designed for EDG problems available . To the best of our knowledge, only  provide convergence guarantees in the non-convex setting. Unlike generic low-rank matrix recovery problems , EDG problems share the complexities of low-rank matrix completion problems, such as the absence of a direct uniform null space property  or a restricted isometry property . Additionally, the underlying measurement basis in EDG problems is not orthonormal, making it impossible to directly use the analysis of standard matrix completion to provide guarantees for techniques using the Gram matrix-based low-rank modelling (1) of Problem 1.

### Our Contribution

In this paper, we propose and analyze an algorithm for the EDG problem based on the iteratively reweighted least squares framework (MatrixIRLS, see Section 3), for which we show that \(m=( rn(n))\) (where \(\) is the coherence factor) randomly sampled distances are sufficient to guarantee local convergence to the ground truth, \(^{0}\) with a quadratic rate as shown in Theorem 4.3, from which the geometry of the points \(\) is trivially recovered. The sample complexity assumption of Theorem 4.3 matches the lower bound of low-rank matrix completion problems as established in . In Section 4, we construct a dual basis (Lemma 4.4) that spans \(S_{n}\) which enables us to show the restricted isometry property (RIP) restricted to a tangent space of the manifold of symmetric rank-\(r\) in Theorem 4.5 for our approach, which can be of independent interest for the analysis of other nonconvex algorithms.

While the convergence statement of Theorem 4.3 only applies in a local neighborhood of a ground truth, the indicated data-efficiency of the proposed method is numerically validated through different experiments in Section 5 on synthetic and real data in comparison to the state-of-the-art methods. Furthermore, we demonstrate that MatrixIRLS method is robust to ill-conditioned data, further highlighting its flexibility. In the Supplementary material, we discuss the limitations in Appendix B. Additionally, we provide proofs related to the the theoretical results of Section 4 in Appendices B and C. We further discuss the numerical considerations of our experiments in Appendix E. We discuss about the computational complexity of the algorithm in detail in Appendix F.

## 2 Related Work

Early research on EDG problems focused on establishing its mathematical properties like defining the conditions under which a distance matrix can be represented in Euclidean space.A comprehensive overview of EDG applications, including molecular conformations, wireless sensor networks, robotics, and manifold learning, is available in . Motivated by the molecular conformation problem,  relates this Euclidean distance matrix completion to a graph realization problem by showing that such matrices can be completed if the graph is chordal. Other than graph theoretic approaches, as a technical tool, various optimization strategies  have been deployed to solve EDG problems.  proposes a primal-dual interior point algorithm that solves an equivalent semi-definite programming problem. However, none these works provide theoretical reconstruction guarantees in the incomplete setup of Problem 1.

While providing an accurate modelling of Problem 1, the rank minimization formulation (1) poses challenges due to its non-convex and non-smooth nature. There is a mature existing literature  around replacing the rank function, \(()\), with the sums of its singular values \(_{i}()\) (also known as the nuclear norm). Building on the rank minimization formulation (1) of the EDG problem,  minimizes the convex nuclear norm surrogate of the inferred Gram matrix. They propose a dual basis approach that enables a theoretical guarantee for this type of nuclear norm minimization formulation of the EDG problem.

From a practical point of view, it is well-known that the convex approach is computationally intensive, as the arithmetic complexity is cubic in \(n\), the dimension of \(^{0}\). It also tends to demand more data samples than non-convex alternatives, making it less efficient in terms of data .

To mitigate these issues, recent studies have shifted focus to non-convex methods such as matrix factorization . These methods optimize a function involving a data-fit objective regularized by squared Frobenius norms of the factor matrices, which are computationally more feasible and data-efficient. Few "non-convex" algorithms are based on matrix factorization like the work in .

Based on a similar formulation, ScaledSGD  is a preconditioned stochastic gradient descent method aimed at robustness for ill-conditioned problems. Additionally, some of the most effective techniques for low-rank matrix completion involve minimizing smooth objectives on the Riemannian manifold of fixed-rank matrices, providing scalability and the potential to reconstruct the matrix with fewer samples, although they lack strong performance guarantees . ReiEDG  is a Riemannian-based gradient descent strategy utilizing the sampling operator on \(\), which is non-convex approach to solving the EDG problem. However, it does not provide convergence guarantees. To the best of our knowledge,  are the non-convex approaches for solving the EDG problem in the Gram-matrix-based low-rank modeling (1) of Problem 1. The work in  proposes an algorithm based on Riemannian optimization over a manifold and provides convergence guarantees. These guarantees are derived from extended Wolfe conditions. However, it is not explicitly detailed how these convergence guarantees depend on problem parameters such as the sampling model and the number of samples (see Remark III.8 in ). Similarly, the study in  employs a Riemannian framework and provides local convergence guarantees under Bernoulli sampling. Nonetheless,  does not clarify whether the proposed algorithm achieves local linear convergence. In contrast to these studies, the algorithm proposed in this paper achieves local quadratic convergence under uniform sampling of the distances.

To handle the non-smoothness and non-convexity of rank minimization problems of the type (1), iteratively reweighted least squares (IRLS) algorithms take a different route than methods based on  or Riemannian methods by minimizing a sequence of quadratic majorizing functions of smoothed rank surrogates . IRLS algorithms have been studied extensively over the years, as indicated by . In the context of low-rank matrix completion, IRLS algorithms are known to be among the most data-efficient methods available, while being amenable to a rigorous convergence analysis . Most recently,  provided an improvement on previous instantiations of the IRLS framework  for low-rank optimization problems by providing an improved reweighting strategy, for which the authors show a local convergence guarantee that is applicable for low-rank matrix completion, given random entrywise samples of minimal sample complexity. The algorithm we propose in Section 3 is similar to [1, Algorithm 1] and Theorem 4.3 follows partially the proof strategy of a related result in . However, the setup of Problem 1 does not allow a direct adaptation of both the implementation and analysis of  due to the non-orthogonality of the measurement basis.

## 3 MatrixIRLS for Euclidean Distance Geometry

In this section, we provide a detailed outline and description of the iteratively reweighted least squares method in the context of the EDG reconstruction problem. To this end, we define preliminaries for stating the algorithm. MatrixIRLS, defined in Algorithm 1 below, can be interpreted as a hybrid of a smoothing method  and a _Majorization-Minimization_ algorithm . In particular, the proposed algorithm minimizes _smoothed log-det objectives_ defined as \(F_{}():=_{i=1}^{n}f_{}(_{i}())\)

\[f_{}()=||,&,\\ ()+}{^{2}}-1,& <,\] (2)

which is a continuously differentiable function with \(^{-2}\)-Lipschitz gradient .

We can decompose \(\) by \(=()^{}\), where \(\{+1,-1\}\), It is clear that the optimization landscape of \(F_{}\) crucially depends on the smoothing parameter \(\). Instead of minimizing \(F_{}\) directly, our method minimizes, for \(k\), \(_{k}>0\) and \(^{(k)}\) a _quadratic model_ that is related to the second-order Taylor expansion of the function \(f_{}\) at the current iterate and its information is encoded in a weight operator  defined below in Definition 3.1.

**Definition 3.1** ().: \( S_{n}\) be a matrix with singular value decomposition \(^{(k)}=()^{}\), where \(\{+1,-1\}\) i.e., \( S_{n}\) are orthonormal matrices. We define the _weight operator core matrix_\(_{,} S_{n}\) of \(\) for smoothing parameter \(>0\) such that

\[(_{,})_{ij}:=(_{i},) (_{j},)^{-1}i,j\{1,,n\} }W_{,}:S_{n} S_{n}, S_{n}\] (3)

where \(\) denotes the entrywise ore Hadamard product of two matrices.

Our method is designed to provide iterates that satisfy the constraints of the formulation (1) at each iteration. They can be encoded using the following definition.

**Definition 3.2**.: Given \(n\), let \(=\{=(i,j)\ |\ 1 i<j n\}\) be the index set of upper triangular indices.

We define the operator basis \(\{_{}\}_{\{(i,i):i\{1,,n\}\}}\) where

\[_{}=_{i}_{i}^{}+_{j}_{j}^{}-_{i}_{j}^{}-_{j} _{i}^{},&=(i,j),\\ (_{i}^{}+_{i}^{}), &=(i,i)i\{1,,n\},\] (4)

where \(_{i}^{n}\) is the \(i\)-th standard basis vector.

Given a set (or multiset) of indices \(=\{_{1},,_{m}\}\) of cardinality \(m=||\), we define the _measurement operator_\(=_{}:S_{n}^{m+n}\) which maps \( S_{n}\) to \(()\) whose \(\)-th coordinate is defined as \(()_{}=_{_{i}},\) for \( m\) and as \(()_{}=_{(-m,-m)},\) for \(>m\).

The basis \(\{_{}\}_{}\) of Definition 3.2 can be considered as an extended definition of the primal basis used in , but additionally is able to encode the constraint \(=\) which guarantees that the Gram matrix corresponds to points whose centroid is located at the origin. Accordingly, we can define the constraint set corresponding to Gram matrices of points that are centered and simultaneously satisfy the pairwise distance constraints of Problem 1 as \(\{ S_{n}:()=[_{};]\}\). The algorithm below minimizes the quadratic, majorizing model of the objective \(F_{_{k}}()\) given \(k\), while satisfying the measurement operator based on the sampled distances. Equivalently, we can define the main computational step of the method as

\[^{(k+1)}=*{arg\,min}_{( )=[_{};]},W_{k}(),\] (5)

where \(W_{k}:S_{n} S_{n}\) is defined as \(W_{k}:=W_{^{(k)},_{k}}\) with the definition of the weight operator Definition 3.1 above. With this preparation, we provide an outline of MatrixIRLS in Algorithm 1 below. Equation (7) provides suitable update rule for the smoothing parameter sequence \((_{k})_{k}\) that enables the computation of only \(r=O()\) singular triplets of each algorithmic iterate .

``` Input: Index pairs \(\), distances \(_{}=(d_{ij})_{(i,j)}\), rank estimate \(\). Output:\(^{(k)}\) after suitable stopping condition.  Initialize \(k=0\), \(_{0}=\) and \(W_{0}=\). for\(k=1,2,,\)do Solve weighted least squares: Solve (5) by (6) Update smoothing: Compute \(+1\)-th singular value of \(^{(k)}\) to update \[_{k}=(_{k-1},_{+1}(^{(k) })).\] (7) Update weight operator: For \(r_{k}:=|\{i[n]:_{i}(^{(k)})>_{k}\}|\), compute the first \(r_{k}\) singular values \(_{i}^{(k)}:=_{i}(^{(k)})\) and matrices \(^{(k)}^{n r_{k}}\) with leading \(r_{k}\) left singular vectors of \(^{(k)}\) to update \(W_{k}=W_{^{(k)},_{k}}\) defined in Definition 3.1. endfor ```

**Algorithm 1**MatrixIRLS for Euclidean Distance Geometry Problems

### Computational Considerations

The computational complexity of this above algorithm can be computed from the steps (6) and (7). In our numerical implementation, we largely follow the tangent space formulation of the weighted least squares step (6), cf. [10, Section 3], which involves the solution of an order \(O(nr_{k})=O(nr)\) linear system if \(=r\) is chosen as the ground truth rank. An additional difficulty we overcame in the provided reference implementation arises from the fact that \(^{*}\) is not the identity. The per-iteration time complexity of our method is dominated by \(O((mr+r^{2}n)^{0}_{inner})\), where \(^{0}_{inner}\) is the number of inner iteration bound of the iterative linear system solver. Detailed FLOPs calculation is shown in Appendix F.

## 4 Theoretical Analysis

In this section, we discuss about the local convergence of MatrixIRLS in Section 4.1 and establish RIP restricted to the tangent space of the manifold of symmetric rank-\(r\) matrices at the ground truth Gram matrix \(^{0}\) in Section 4.2.

### Local Convergence Analysis of Algorithm 1

It is well-known in the literature on low-rank matrix completion that for an entrywise measurement basis, recovery from generic measurements is more difficult if most information of the low-rankmatrix is concentrated in few entries, and this observation is typically captured by the notion of incoherence . For the purpose of the EDG problem of interest, we use the following coherence notion, which has appeared in similar form in .

**Definition 4.1** (Coherence for Gram matrices in the EDG problem, ).: Let \( S_{n}\) be of rank \(r\). Let \(T=T_{}=\{+^{}: ^{n n}\}\) be the tangent space onto the rank-r manifold \(_{r}=\{ S_{n}:()=r\}\) at \(\). We say that \(\) has coherence \(\) with respect to the basis \(\{_{}\}_{}\) of the subspace \(\{ S_{n}:=\}\) if

\[_{}_{}_{T} _{},_{}^{2} 2 _{}_{}_{T} _{},_{}^{2} 4,\]

where \(_{T}:S_{n} S_{n}\) denotes the projection operator onto \(T\) and \(\{\}_{}\) is a _dual basis_ of \(\{_{}\}_{}\), which means that \(_{},_{}=_{,}\) for each \(,\) ( \(_{,}=1\) for \(=\) and equal to \(0\) otherwise).

_Remark 4.2_.: In [10, Definition 1], the coherence constant \(\) was required to satisfy a third condition (see [10, (Ineq. 14)]). However, this condition is not needed for our proofs, which is why we can use the weaker definition of Definition 4.1. Similar improvements for the standard basis incoherence notion were achieved in . In [10, Lemma 21], it was shown that up constants, the definition above is equivalent to a coherence condition with respect to the standard basis .

Following the conventional sampling approach in the existing literature , the index set is \(=(i_{},j_{})_{=1}^{m}\) contains \(m\) samples drawn uniformly at random without replacement.

**Theorem 4.3** (Local convergence of MatrixIRLS for EDG with Quadratic Rate).: _Let \(^{0} S_{n}\) be a matrix of rank \(r\) that is \(\)-incoherent, and let \(:S_{n}^{m+n}\) be the measurement operator corresponding to an index set \(\) of size \(m=||\) that is drawn uniformly without replacement. There exist constants \(C^{*}\), \(\) and \(C\) such that the following holds. **(a)** If the sample complexity fulfills \(m C rn n\), and if **(b)** the output matrix \(^{(k)} S_{n}\) of the \(k\)-th iteration of MatrixIRLS for EDG with inputs \(=(^{0})\) and \(=r\) updates the smoothing parameter in (7) such that \(_{k}=_{r+1}(^{(k)})\) and fulfills \(\|^{(k)}-^{0}\|_{S_{}}} }{C^{*} L^{2}( n)^{}}_{r}(^{0})\) where \(=_{1}(^{0})/_{r}(^{0})\) is the condition number of \(^{0}\), **then the local convergence rate is quadratic** in the sense that \(\|^{(k+1)}-^{0}\|_{S_{}}(\|^{ (k)}-^{0}\|_{S_{}}^{2},\|^{(k)}-^{0}\|_{S_ {}})\) with \(=(L n})( ^{0})}\) and furthermore \(^{(k+)}^{0}\) with high probability. (The values of the constants \(C^{*}=10^{5}\),\(=21\),\(C=4900\) are explicitly derived in the Supplementary material.)_

In other words, Theorem 4.3 indicates Algorithm 1 converges to the ground truth with high probability with a sample complexity of \(( rn n)\), if initialized close to the ground truth Gram matrix. We refer to Appendix D for its proof.

Our theorem's sample complexity requirement aligns with the lower bound for generic low-rank matrix completion problems, as established in . We note that this theorem only provides a local convergence guarantee for Algorithm 1. This is in line with the strongest known results for IRLS algorithms optimizing non-convex objectives . We provide numerical evidence in Section 5 that the minimal sample complexity assumption (a) of Theorem 4.3 indeed captures the generic reconstruction ability of the method.To the best of our knowledge, Theorem 4.3 represents the first convergence guarantee for any algorithm for Problem 1 that applies at the optimal order \(( rn n)\) of provided pairwise distances, and furthermore, the first theoretical guarantee for any non-convex optimization framework for Problem 1.

### Dual Basis Construction and Local Restricted Isometry Property on Tangent Spaces

While a convergence result similar to Theorem 4.3 had been previously obtained for an IRLS algorithm for low-rank matrix completion , an adaptation of the proof of  to the EDG setting is not possible due to non-orthogonality of the basis \(\{_{}\}_{}\) of Definition 3.2.

In order to prove Theorem 4.3, we establish a restricted isometry property of a suitably defined sampling operator (see (9)) with respect to the tangent space of the manifold of symmetric rank-\(r\) matrices at the ground truth Gram matrix \(^{0}=^{}\). To formulate this sampling operator and the respective RIP condition, we construct a _dual_ basis to the measurement basis \(\{_{}\}_{}\) of Definition 3.2.

**Lemma 4.4** (Dual Basis Construction).: _Let \(n\), \(=\{(i,j) 1 i<j n\}\) be the index set and \(\{_{}\}_{ D}\) be the primal basis of Definition 3.2 with \(_{D}=\{(i,i):i\{1,,n\}\}\). If_

\[_{}=-(_{i}_{j}^{ }+_{j}_{j}^{}),&=(i,j),\\ _{i}_{i}^{}-_{i}_{j}^{},& =(i,i)_{D},\] (8)

_where \(_{i}=_{i}-\) for \(i\{1,,n\}\), then \(\{_{}\}_{ D}\) is a dual basis with respect to \(\{_{}\}_{ D}\), i.e., \(\{_{}\}_{}\) and \(\{_{}\}_{}\) are bi-orthogonal._

Lemma 4.4 extends the dual basis construction of , in which the duality of \(\{_{}\}_{}\) with respect to \(\{_{}\}_{}\) was shown. The proof of Lemma 4.4 is detailed in Appendix B.1.

Unlike the basis pair of , our bases span the entire space of symmetric matrices \(S_{n}\) (see Appendix B.2), which enables us to show the following restricted isometry property.

**Theorem 4.5** (Restricted Isometry Property for Sampling Operator \(_{}\)).: _Let \(L=n(n-1)/2\), \(0<\), and \(\) be a multiset of size \(m\) sampled independently with replacement. Define the sampling operator \(_{}:S_{n} S_{n}\) such that_

\[_{}():=_{(i,i)_{ =1}^{n}},_{}_{}\] (9)

_where \(_{}\) and \(_{}\) as in (4) and (8), respectively. Let \(^{0} S_{n}\) be a \(\)-incoherent matrix whose tangent space onto the manifold \(_{r}\) of symmetric rank-\(r\) matrices is denoted as \(T_{0}=T_{^{0}}\). Let \(_{T_{0}}:S_{n} S_{n}\) be the projection operator associated to \(T_{0}\). Then \(\|_{T_{0}}\,_{}^{*}\,_{T_{0}}- _{T_{0}}\|_{S_{}}\) holds with probability at least \(1-\) provided that_

\[m(49/^{2}) nr n.\]

The dual basis as discussed in Section 3 along with the extension for the diagonal entries together spans the space of \(n n\) symmetric matrices. This construction has been crucial in proving the RIP restricted to the tangent space \(T_{}\) of rank constrained smooth manifold \(_{r}\). This construction could also be valuable for analyzing other non-convex algorithms. A detailed proof of Theorem 4.5 is provided in Appendix C This proof is achieved by using concentration inequalities like the Matrix Bernstein inequality theorem C.1 in multiple lemmas stated and proved in detail in the supplemental material.

Since the existing literature for nonconvex approaches for solving Problem 1 lacks this property, this approach of establishing RIP by restricting it to the tangent space can be useful for the analysis of other nonconvex methods. The RIP condition, originally introduced in the context of compressed sensing , is a fundamental assumption in the literature on low-rank matrix recovery (). A tangent-space restricted RIP has been has been useful for analyzing the convergence and performance properties of other non-convex methods  in a non-EDG setting.

## 5 Numerical Experiments

We evaluate the performance of MatrixIRLS, Algorithm 1, for instances of the EDG reconstruction Problem 1 in terms of data efficiency across multiple datasets in comparison to other state-of-the-art methods in the literature. We compare the performance of MatrixIRLS with three other algorithms: (a) ALM, an augmented Lagrangian method that minimizes the non-convex formulation defined by \(_{^{r n}}(^{} )\) subject to \(_{}(^{})=_{}( ^{0})\),

with \(_{}\) being equal to the sampling operator \(_{}\) restricted to \(\), which is based on a Burer-Monteiro factorization of the Gram matrix, and which has been studied in the numerical experiments of , (b) ScaledSGD  which is a preconditioned stochastic gradient descent method designed to be robust with respect to ill-conditioned problems, (c) RieEDG  which is a Riemannian-based gradient descent approach based on the sampling operator (9) restricted to \(\). The choice of these algorithms is based on their robustness to noise as claimed in the respective papers.

### Synthetic Data

We first consider a synthetic data, where we select \(n=500\) points \(^{0}=[_{1},\ \ ,_{n}]^{r n}\) from a standard Gaussian distribution at random such that \((_{i})_{j}(0,1)\) for all \(i,j\), whichdefines the ground truth Gram matrix \(^{0}=^{0}^{0}\). We are provided with \(m=||\) Euclidean distances, where the point index pair set \(\{(i,j)[n][n],i<j\}\) is sampled uniformly at random. This is parametrized by the oversampling factor \(=\) where the denominator is the degrees of freedom (discussed in Appendix E.4). To understand the efficiency of the algorithm over a range of ranks \(r\) and across different oversampling factors \(\), we conduct phase transition experiments for all the above mentioned algorithms. We define a successful recovery as the case of the relative Procrustes distance \(d_{}(_{rec},^{0})\) (discussed in Appendix E.3) between the recovered matrix \(_{rec}\) and ground truth coordinate matrix \(^{0}\) does not exceed a tolerance threshold \(_{}=10^{-3}\). We chose the Procrustes distance at it is a shape preserving distance that accounts also for differences in scaling and alignment between the reconstructed geometries. We observe the performance of the algorithms as shown in the Figure 1 for ranks between \(2\) to \(5\) and a oversampling factor ranging from \(1\) to \(4\) over \(24\) instances.

In Figure 1, each entry on the figure represents the probability of success of an algorithm for the given rank ground truth rank \(r\) and oversampling factor \(\) over \(24\) instances.1 In terms of the recovery of the ground truth, we notice a comparable performance for the MatrixIRLS and ALM. However, the other two algorithms RieEDG and ScaledSGD, for the given success tolerance, the recovery of a ground truth is only possible if more samples corresponding to a larger oversampling factor are provided, for any of the ranks \(r\) considered. This emphasizes that MatrixIRLS is able to achieve state-of-the-art data efficiency.

In order to understand the scalability of the proposed algorithm, we have provided the time of completion of MatrixIRLS in Table 1. The first two rows shows that for fixed oversampling factor\(=3\), we are able to recover the points in the magnitude of \(10^{4}\), is just \(13.7\) minutes. So to further test the scalability, we also looked at the time to completion when less number of samples are provided ( \(=2.5\)). In that setup \(n=10000\) takes \(57.5\) minutes to recover with high precision.

### Ill-conditioned Data

We now assess the performance of the different EDG methods on ill-conditioned data. Ill-conditioning in the point matrix in particular arises in situations where, for example, \(r\)-dimensional points are approximately following an \(r-1\)-dimensional geometry, a situation which often arises when the available distance information if affected by outliers .

Similar to the setup above, we construct Gram matrices \(^{0} S_{n}\) of rank \(r=5\) with condition number \(=_{1}(^{0})/_{r}(^{0})=10^{5}\) corresponding to \(n=400\) random data points \(^{0}^{r n}\) generated with random orthogonal singular vectors and singular values \(_{i}(^{0})\) interpolated between \(\) and \(1\) with decay of order \(O(i^{-2})\). Figure 2 shows the success probability of different algorithms for ill-conditioned ground truths.

To have a deeper insight into the algorithm's performance, Figure 3 shows the per-iterate relative reconstructed error of the four algorithms on this ill-conditioned data at an oversampling factor of \(=2\) for a representative problem instance. This reconstructed error refers to the Procrustes distance between the ground truth \(^{0}\) and the recovered \(^{k}\) at each iteration \(k\). We observe that for ill-conditioned data, MatrixIRLS is the only method that can recover the ground truth up to a

Figure 1: Success probabilities for recovery for different algorithms, given Gaussian ground truths \(^{0}\) of different ranks, computed across \(24\) instances.

reasonable precision, achieving a relative error of \( 10^{-8}\) after around \(35\) iterations (top of Figure 4), whereas the other methods do not achieve errors below \(10^{-3}\) even after \(100000\) iterations. While one iteration of MatrixIRLS typically has a longer runtime than an iteration of any of the other algorithms due to its second-order nature, we also provide a visualization of the observed runtimes of the methods in the bottom of Figure 4. It can be seen that MatrixIRLS is able reconstruct the geometry of the challenging problem in around \(200\) seconds up to a high precision. Additionally, we run a study on the algorithms' behavior across different oversampling factors between \(=1\) and \(=4\) for \(24\) random problem instances, visualized in Figure 4. The box plots indicate visualize median relative Procrustes error together with the relevant \(25\%\) and \(75\%\) quantiles of the observed error distribution. We observe that MatrixIRLS has consistent convergence for ill-conditioned data even for lower oversampling rates as long as \( 1.5\).

### Real Data

To assess the performance of Algorithm 1 in realistic setups, we consider the task of molecular conformation, i.e., we aim to reconstruct the 3D structure of a molecule from partial information about the distances of its atoms. For our experiments, we determine the structures of a protein molecule (1BPM) from the Protein Data Bank [HMBFGG\({}^{+}\)00], which is collected using X-ray diffraction experiments or nuclear magnetic resonance (NMR) spectroscopy.

The goal of this experiment is to reconstruct the point matrix \(^{0}\) from \(||\) distance samples as defined in Problem 1. For the 1BPM protein data the rank of the input matrix is \(3\). We conduct the experiments corresponding to oversampling factors \(\) between \(1\) and \(4\). Like the previous setup, here successful recovery refers to the case where the relative Procrustes distance, that is the metric \(d_{}(_{rec},^{0})\) between the recovered matrix \(_{rec}\) and ground truth coordinate matrix \(^{0}\) does not exceed a tolerance threshold \(_{}\) across \(24\) independent realizations. The error analysis for this experiment is shown in Figures 6(a) and 6(b) in Appendix E.

It is evident that MatrixIRLS and ALM have comparable results in recovering the geometry of the data given lesser samples, where as algorithms like ScaledSGD or RieEDG are unable to reconstruct geometries even when the oversampling rate is as high as \(=4\). In the fig. 5, we see a convergence with high precision for MatrixIRLS from \( 2.5\) for Protein which means that it recovered the ground truth with around \(0.5\%\) samples.

In the Figure 6 the reconstructed protein in blue is aligned with the ground truth structure of the protein in pink. We can see that when the oversampling is \(3.5\), which is when there is \(0.6\%\) samples available, the reconstruction exactly matches with the ground truth.

In order to understand the runtime of the different algorithms Table 2, reports the reconstruction times for each of the four algorithms applied to the 1BPM Protein data (with datapoints \(n=3672\)) in a low-data regime with oversampling factor of \(=3\). It can be seen that MatrixIRLS is with \(7.08\) minutes around \(3\) times faster than ALM, which needed \(23.04\) minutes until convergence. While these times certainly depend on the precise choice of stopping criteria for each algorithm (we used the ones indicated in Appendix E ), this shows that the proposed method is competitive in terms of clock time.

For the implementation of the other algorithms, we use the authors' code for the respective approaches (discussed in Appendix E). We include another set of experiments on US cities data , in the Appendix E.

## 6 Conclusion

In this paper, we address the challenge of reconstructing suitable geometric configurations using minimal Euclidean distance samples. By leveraging continuous and non-convex rank minimization formulations, we develop a variant of the iteratively reweighted least squares (IRLS) algorithm and establish a local convergence guarantee under the condition of a minimal random set of observed distances. Our contribution also includes the proof of a restricted isometry property (RIP) restricted to the tangent space of the manifold of symmetric rank-\(r\) matrices, a result which might be of independent interest for the analysis of other non-convex methods. As future work further analysis on the global convergence can be established. Through numerical validation we conclude that the algorithm is able to achieve state-of-the-art data efficiency.