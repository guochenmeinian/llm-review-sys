# Generalization in the Face of Adaptivity:

A Bayesian Perspective

 Moshe Shenfeld

Department of Computer Science

The Hebrew University of Jerusalem

moshe.shenfeld@mail.huji.ac.il

&Katrina Ligett

Department of Computer Science

Federmann Center for the Study of Rationality

The Hebrew University of Jerusalem

katrina@cs.huji.ac.il

###### Abstract

Repeated use of a data sample via adaptively chosen queries can rapidly lead to overfitting, wherein the empirical evaluation of queries on the sample significantly deviates from their mean with respect to the underlying data distribution. It turns out that simple noise addition algorithms suffice to prevent this issue, and differential privacy-based analysis of these algorithms shows that they can handle an asymptotically optimal number of queries. However, differential privacy's worst-case nature entails scaling such noise to the range of the queries even for highly-concentrated queries, or introducing more complex algorithms.

In this paper, we prove that straightforward noise-addition algorithms already provide variance-dependent guarantees that also extend to unbounded queries. This improvement stems from a novel characterization that illuminates the core problem of adaptive data analysis. We show that the harm of adaptivity results from the covariance between the new query and a Bayes factor-based measure of how much information about the data sample was encoded in the responses given to past queries. We then leverage this characterization to introduce a new data-dependent stability notion that can bound this covariance.

## 1 Introduction

Recent years have seen growing recognition of the role of _adaptivity_ in causing overfitting and thereby reducing the accuracy of the conclusions drawn from data. Intuitively, allowing a data analyst to adaptively choose the queries that she issues potentially leads to misleading conclusions, because the results of prior queries might encode information that is specific to the available data sample and not relevant to the underlying distribution from which it was drawn. As a result, future queries might then be chosen to leverage these sample-specific properties, giving answers on the sample that differ wildly from the values of those queries on the data distribution. Such adaptivity has been blamed, in part, for the current reproducibility crisis in the data-driven sciences (Ioannidis, 2005; Gelman and Loken, 2014).

A series of works catalyzed by Dwork et al. (2015) recently established a formal framework for understanding and analyzing adaptivity in data analysis, and introduced a general toolkit for provably preventing the harms of choosing queries adaptively--that is, as a function of the results of previous queries. This line of work has established that enforcing that computations obey a constraint of _differential privacy_(Dwork et al., 2006)--which one can interpret as a robustness or stability requirement on computations--provably limits the extent to which adaptivity can cause overfitting. Practically, for simple numerical computations, these results translate into adding a level of noise to the query results that would be sufficient to ensure _worst-case_ stability of the _worst-case_ query on the _worst-case_ dataset. In particular, when analyzed using differential privacy, simple noise-additionmechanisms must add noise that scales with the _worst-case_ change in query value that could be induced by changing a single input data element, in order to protect against adaptivity.

However, this can be overkill: for the purposes of statistical validity, we do not care about the worst case but the typical case. In the present work, we prove the following new and better generalization guarantees for simple Gaussian noise-addition algorithms:

**Theorem 1.1** (Informal versions of main theorems).: _With probability \(>1-\), the error of the responses produced by a mechanism which only adds Gaussian noise to the empirical values of the queries it receives is bounded by \(\), even after responding to \(k\) adaptively chosen queries, if_

* _the range of the queries is bounded by_ \(\)_, their variance is bounded by_ \(^{2}\)_, and the size of the dataset_ \(n=(\{,}{^{2 }}\}())\) _(Theorem_ 5.1_), or_
* _the queries are_ \(^{2}\)_-sub-Gaussian and the size of the dataset_ \(n=(}{^{2}}())\) _(Theorem_ 5.3_)._

In contrast, analyzing the same mechanism using differential privacy yields a sample size requirement that scales with \(n=(}{^{2}}())\) in the first setting, and provides no guarantee in the latter.

We prove these theorems via a new notion, _pairwise concentration (PC)_ (Definition 4.2), which captures the extent to which replacing one dataset by another would be "noticeable," given a particular query-response sequence. This is thus a function of particular differing datasets (instead of worst-case over elements), and it also depends on the actual issued queries. We then build a composition toolkit (Theorem 4.4) that allows us to track PC losses over multiple computations. The PC notion allows for more careful analysis of the information encoded by the query-response sequence than differential privacy does.

In order to leverage this more careful analysis of the information encoded in query-response sequences, we rely on a simple new characterization (Lemma 3.5) that shows explicitly that the harms of adaptivity come from the covariance between the behavior of the future queries and a Bayes factor-based measure of how much information about the data sample was encoded in the responses to the issued queries. Our characterization gives insight into how differential privacy protects against adaptivity, and then allows us to step away from this worst-case approach. What differential privacy accomplishes is that it bounds a worst-case version of the Bayes factor; however, as this new characterization makes clear, it is sufficient to bound the "typical" Bayes factor in order to avoid overfitting.

We measure the harm that past adaptivity causes to a future query by considering the query as evaluated on a posterior data distribution and comparing this with its value on a prior. The prior is the true data distribution, and the posterior is induced by observing the responses to past queries and updating the prior. If the new query behaves similarly on the prior distribution as it does on this posterior (a guarantee we call _Bayes stability_; Definition 3.3), adaptivity has not led us too far astray.1 If furthermore, the the response given by the mechanism is close to the query result on the posterior, then by a triangle inequality argument, that mechanism is distribution accurate. This type of triangle inequality first appeared as an analysis technique in Jung et al. (2020).

The dependence of our PC notion on the actual adaptively chosen queries places it in the so-called _fully-adaptive_ setting (Rogers et al., 2016; Whitehouse et al., 2023), which requires a fairly subtle analysis involving a set of tools and concepts that may be of independent interest. In particular, we establish a series of "dissimilarity" notions in Appendix B, which generalize the notion of divergence, replacing the scalar bound with a function. Our main stability notion (Definition 4.2) can be viewed as an instance-tailored variant of _zero-concentrated differential privacy (Bun and Steinke, 2016)_, and we also make use of a similar extension of the classical max-divergence-based differential privacy definition (B.8).

Related work_Differential privacy_(Dwork et al., 2006) is a privacy notion based on a bound on the max divergence between the output distributions induced by any two neighboring input datasets (datasets which differ in one element). One natural way to enforce differential privacy is by directly adding noise to the results of a numeric-valued query, where the noise is calibrated to the _global sensitivity_ of the function to be computed--the maximal change in its value between any two neighboring datasets. Dwork et al. (2015) and Bassily et al. (2021) showed that differential privacy is also useful as tool for ensuring generalization in settings where the queries are chosen adaptively.

Differential privacy essentially provides the optimal asymptotic generalization guarantees given adaptive queries Hardt and Ullman (2014); Steinke and Ullman (2015). However, its optimality is for worst-case adaptive queries, and the guarantees that it offers only beat the naive intervention--of splitting a dataset so that each query gets fresh data--when the input dataset is quite huge Jung et al. (2020). A worst-case approach makes sense for privacy, but for statistical guarantees like generalization, we only need statements that hold with high probability with respect to the sampled dataset, and only on the actual queries issued.

One cluster of works that steps away from this worst-case perspective focuses on giving privacy guarantees that are tailored to the dataset at hand Nissim et al. (2007); Ghosh and Roth (2011); Ebadi et al. (2015); Wang (2019). In Feldman and Zrnic (2021) in particular, the authors elegantly manage to track the individual privacy loss of the elements in the dataset. However, their results do not enjoy a dependence on the standard deviation in place of the range of the queries. Several truncation-based specialized mechanisms have been proposed, both to provide differential privacy guarantees for Gaussian and sub-Gaussian queries even in the case of multivariate distribution with unknown covariance Karwa and Vadhan (2018); Ashtiani and Liaw (2022); Duchi et al. (2023) and, remarkably, design specialized algorithms that achieve adaptive data analysis guarantees that scale like the standard deviation of the queries Feldman and Steinke (2017). Recently, Blanc (2023) proved that randomized rounding followed by sub-sampling provides accuracy guarantees that scale with the queries' variance. But none of these results apply to simple noise addition mechanisms.

Another line of work (e.g., Gehrke et al. (2012); Bassily et al. (2013); Bhaskar et al. (2011)) proposes relaxed privacy definitions that leverage the natural noise introduced by dataset sampling to achieve more average-case notions of privacy. This builds on intuition that average-case privacy can be viewed from a Bayesian perspective, by restricting some distance measure between some prior distribution and some posterior distribution induced by the mechanism's behavior Dwork et al. (2006); Kasiviswanathan and Smith (2014). This perspective was used Shenfeld and Ligett (2019) to propose a stability notion which is both necessary and sufficient for adaptive generalization under several assumptions. Unfortunately, these definitions have at best extremely limited adaptive composition guarantees. Bassily and Freund (2016) connect this Bayesian intuition to statistical validity via _typical stability_, an approach that discards "unlikely" databases that do not obey a differential privacy guarantee, but their results require a sample size that grows linearly with the number of queries even for iid distributions. Triastcyn and Faltings (2020) propose the notion of _Bayesian differential privacy_ which leverages the underlying distribution to improve generalization guarantees, but their results still scale with the range in the general case.

An alternative route for avoiding the dependence on worst case queries and datasets was achieved using expectation based stability notions such as _mutual information_ and _KL stability_Russo and Zou (2016); Bassily et al. (2021); Steinke and Zakynthinou (2020). Using these methods Feldman and Steinke (2018) presented a natural noise addition mechanism, which adds noise that scales with the empirical variance when responding to queries with known range and unknown variance. Unfortunately, in the general case, the accuracy guarantees provided by these methods hold only for the expected error rather than with high probability.

A detailed comparison to other lines of work can be found in Appendix F.

## 2 Preliminaries

### Setting

We study datasets, each of fixed size \(n\), whose elements are drawn from some domain \(\). We assume there exists some distribution \(D_{^{n}}\) defined over the datasets \(s^{n}\).2 We consider a family of functions (_queries_) \(\) of the form \(q:^{n}\). We refer to the functions' outputs as _responses_.3

We denote by \(q(D_{^{n}})_{ ^{n}}}{}[q(S)]\) the mean of \(q\) with respect to the distribution \(D_{^{n}}\), and think of it as the true value of the query \(q\), which we wish to estimate. As is the case in many machine learning or data-driven science settings, we do not have direct access to the distribution over datasets, but instead receive a dataset \(s\) sampled from \(D_{^{n}}\), which can be used to compute \(q(s)\)--the empirical value of the query.

A _mechanism_\(M\) is a (possibly non-deterministic) function \(M:^{n}\) which, given a dataset \(s^{n}\), a query \(q\), and some auxiliary parameters \(\), provides some response \(r\) (we omit \(\) when not in use). The most trivial mechanism would simply return \(r=q(s)\), but we will consider mechanisms that return a noisy version of \(q(s)\). A mechanism induces a distribution over the responses \(D_{|^{n}}^{q,}(\,|\,s)\)--indicating the probability that each response \(r\) will be the output of the mechanism, given \(s\), \(q\), and \(\) as input. Combining this with \(D_{^{n}}\) induces the marginal distribution \(D_{}^{q,}\) over the responses, and the conditional distribution \(D_{^{n}|}^{q,}(\,|\,r)\) over sample sets. Considering the uniform distribution over elements in the sample set induces similar distributions over the elements \(D_{}\) and \(D^{r} D_{|}^{q,}(\,|\,r)\). This last distribution, \(D^{r}\), is central to our analysis, and it represents the _posterior distribution_ over the sample elements given an observed response--a Bayesian update with respect to a prior of \(D_{}\). All of the distributions discussed in this subsection are more formally defined in Appendix A.

An _analyst_\(A\) is a (possibly non-deterministic) function \(A:^{*}(,)\) which, given a sequence of responses, provides a query and parameters to be asked next. An interaction between a mechanism and an analyst-- wherein repeatedly (and potentially adaptively) the analyst generates a query, then the mechanism generates a response for the analyst to observe--generates a sequence of \(k\) queries and responses for some \(k\), which together we refer to as a _view_\(v_{k}=(r_{1},r_{2},,r_{k})_{k}\) (we omit \(k\) when it is clear from context). In the case of a non-deterministic analyst, we add its coin tosses as the first entry in the view, in order to ensure each \((q_{i},_{i})\) is a deterministic function of \(v_{i-1}\), as detailed in Definition A.3; given a view \(v\), we denote by \(_{v}\) the sequence of queries that it induces. Slightly abusing notation, we denote \(v=M(s,A)\). The distributions \(D_{|^{n}}^{q,}\), \(D_{}^{q,}\), \(D_{^{n}|}^{q,}\), and \(D^{r}\) naturally extend to versions where a sequence of queries is generated by an analyst: \(D_{|^{n}}^{A}\), \(D_{}^{A}\), \(D_{}^{A}\), \(D_{^{n}|}^{A}\), and \(D^{v} D_{|}^{A}(\,|\,v)\).

Notice that \(M\) holds \(s\) but has no access to \(D\). On the other hand, \(A\) might have access to \(D\), but her only information regarding \(s\) comes from \(M\)'s responses as represented by \(D^{v}\). This intuitively turns some metric of distance between \(D\) and \(D^{v}\) into a measure of potential overfitting, an intuition that we formalize in Definition 3.3 and Lemma 3.5.

### Notation

Throughout the paper, calligraphic letters denote domains (e.g., \(\)), lower case letters denote elements of domains (e.g., \(x\)), and capital letters denote random variables (e.g., \(X D_{}\)).

We omit most superscripts and subscripts when clear from context (e.g., \(D(r\,|\,s)=D_{|^{n}}^{q,}(r\,|\,s)\) is the probability to receive \(r\) as a response, conditioned on a particular input dataset \(s\), given a query \(q\) and parameters \(\)). Unless specified otherwise, we assume \(D_{^{n}}\), \(n\), \(k\), and \(M\) are fixed, and omit them from notations and definitions.

We use \(\|\|\) to denote the Euclidean norm, so \(\|_{v}(x)\|=^{k}(q_{i}(x ))^{2}}\) will denote the norm of a concatenated sequence of queries.

### Definitions

We introduce terminology to describe the accuracy of responses that a mechanism produces in response to queries.

**Definition 2.1** (Accuracy of a mechanism).: Given a dataset \(s\), an analyst \(A\), and a view \(v=M(s,A)\), we define three types of output error for the mechanism: _sample error_\(_{S}(s,v)_{i[k]}|r_{i}- q_{i}(s)|\), _distribution error_\(_{D}(s,v)_{i[k]}|r_{i} -q_{i}(D)|\), and _posterior error_\(_{P}(s,v)_{i[k]}|r_{i} -q_{i}(D^{v})|\), where \(q_{i}\) is the \(i\)th query and \(r_{i}\) is the \(i\)th response in \(v\).

These errors can be viewed as random variables with a distribution that is induced by the underlying distribution \(D\) and the internal randomness of \(M\) and \(A\). Given \(, 0\) we call a mechanism \(M\)\((,)\)_-sample/distribution/posterior accurate_ with respect to \(A\) if

\[,V M(S,A)}{}[ (S,V))>],\]

for \(=_{S}_{D}_{P}\), respectively.

Notice that if each possible value of \(\) has a corresponding \(\), then \(\) is essentially a function of \(\). Given such a function \(:^{+}[0,1]\), we will say the mechanism is \((,())\) accurate, a perspective which will be used in Lemma 3.1.

We start by introducing a particular family of queries known as _linear queries_, which will be used to state the main results in this paper, but it should be noted that many of the claims extend to arbitrary queries as discussed in Section C.2

**Definition 2.2** (Linear queries).: A function \(q:^{n}\) is a _linear query_ if it is defined by a function \(q_{1}:\) such that \(q(s)}q_{1} (s_{i})\). For simplicity, we denote \(q_{1}\) as \(q\) throughout.

The aforementioned error bounds implicitly assume some known scale of the queries, which is usually chosen to be their range, denoted by \(_{q}}{}|q(x )-q(y)|\) (we refer to such a query as \(\)-bounded). This poses an issue for concentrated random variables, where the "typical" range can be arbitrarily smaller than the range, which might even be infinite. A natural alternative source of scale we consider in this paper is the query's variance \(_{q}^{2}}{}[(q(X )-q(D_{}))^{2}]\), or its variance proxy in the case of a sub-Gaussian query (Definition 5.2). The corresponding definitions for arbitrary queries are presented in Section C.2.

Our main tool for ensuring generalization is the Gaussian mechanism, which simply adds Gaussian noise to the empirical value of a query.

**Definition 2.3** (Gaussian mechanism).: Given \(>0\) and a query \(q\), the _Gaussian mechanism_ with noise parameter \(\) returns its empirical mean \(q(s)\) after adding a random value, sampled from an unbiased Gaussian distribution with variance \(^{2}\). Formally, \(M(s,q)(q(s),^{2})\).4

## 3 Analyzing adaptivity-driven overfitting

In this section, we give a clean, new characterization of the harms of adaptivity. Our goal is to bound the distribution error of a mechanism that responds to queries generated by an adaptive analyst. This bound will be achieved via a triangle inequality, by bounding both the posterior accuracy and the Bayes stability (Definition 3.3). Missing proofs from this section appear in Appendix C.

The simpler part of the argument is posterior accuracy, which we prove can be inherited directly from the sample accuracy of a mechanism. This lemma resembles Lemma 6 in Jung et al. (2020), but has the advantage of being independent of the range of the queries.

**Lemma 3.1** (Sample accuracy implies posterior accuracy).: _Given a function \(:[0,1]\) and an analyst \(A\), if a mechanism \(M\) is \((,())\)-sample accurate for all \(>0\), then \(M\) is \((,^{}())\)-posterior accurate for \(^{}()}(_{-}^{} (t)dt)\)._

We use this lemma to provide accuracy guarantees for the Gaussian mechanism.

**Lemma 3.2** (Accuracy of Gaussian mechanism).: _Given \(>0\), the Gaussian mechanism with noise parameter \(\) that receives \(k\) queries is \((,())\)-sample accurate for \(()}e^{- }{2^{2}}}\), and \((,())\)-posterior accurate for \(() 4k e^{-}{4^{2}}}\)._

In order to complete the triangle inequality, we have to define the stability of the mechanism. Bayes stability captures the concept that the results returned by a mechanism and the queries selected by the adaptive adversary are such that the queries behave similarly on the true data distribution and on the posterior distribution induced by those results. This notion first appeared in Jung et al. (2020), under the name _Posterior Sensitivity_, as did the following theorem.

**Definition 3.3** (Bayes stability).: Given \(,>0\) and an analyst \(A\), we say \(M\) is \((,)\)_-Bayes stable_ with respect to \(A\), if

\[} [|Q(D^{V})-Q(D)|>].\]

Bayes stability and sample accuracy (implying posterior accuracy) combine via a triangle inequality to give distribution accuracy.

**Theorem 3.4** (Generalization).: _Given two functions \(_{1}:\), \(_{2}:\), and an analyst \(A\), if a mechanism \(M\) is \((,_{1}())\)-Bayes stable and \((,_{2}())\)-sample accurate with respect to \(A\), then \(M\) is \((,^{}())\)-distribution accurate for \(^{}() (0,),(0,-^{})}{ }(_{1}(^{})+ _{-^{}-}^{}_{2}(t)dt)\)._

Since achieving posterior accuracy is relatively straightforward, guaranteeing Bayes stability is the main challenge in leveraging this theorem to achieve distribution accuracy with respect to adaptively chosen queries. The following lemma gives a useful and intuitive characterization of the quantity that the Bayes stability definition requires be bounded. Simply put, the Bayes factor \(K(,)\) (defined in the lemma below) represents the amount of information leaked about the dataset during the interaction with an analyst, by moving from the prior distribution over data elements to the posterior induced by some view \(v\). The degree to which a query \(q\) overfits to the dataset is expressed by the correlation between the query and that Bayes factor. This simple lemma is at the heart of the progress that we make in this paper, both in our intuitive understanding of adaptive data analysis, and in the concrete results we show in subsequent sections. Its corresponding version for arbitrary queries are presented in Section C.2.

**Lemma 3.5** (Covariance stability).: _Given a view \(v\) and a linear query \(q\),_

\[q(D^{v})-q(D_{})=}}{}(q(X),K(X,v)).\]

_Furthermore, given \(,>0\),_

\[_{q}}{}|q (D^{v})-q(D_{})|= _{}(D^{v}\|D_{})\]

_and_

\[_{q}^{2}^{2}}{} |q(D^{v})-q(D_{})|=_{^{2}}(D^{v}\|D_{})},\]

_where \(K(x,v)=\) is the Bayes factor of \(x\) given \(v\) (and vice-versa), \(_{}\) is the total variation distance (Definition B.1), and \(_{^{2}}\) is the chi-square divergence (Definition B.2)._

Proof.: By definition, \(q(D^{v})=}{}[q(X) ]=}[K(X,v)q(X)]\), so

\[q(D^{v})-q(D)=}[K(X,v)q(X)]-}[K (X,v)]}^{=1}} [q(X)]}^{=q(D)}=} (q(X),K(X,v)).\]

The second part is a direct result of the known variational representation of total variation distance and \(^{2}\) divergence, which are both \(f\)-divergences (see Equations 7.88 and 7.91 in Polyanskiy and Wu (2022) for more details).

Using the first part of the lemma, we guarantee Bayes stability by bounding the correlation between specific \(q\) and \(K(,v)\) as discussed in Section 6. The second part of this Lemma implies that bounding the appropriate divergence is necessary and sufficient for bounding the Bayes stability of the worst query in the corresponding family, which is how the main theorems of this paper are all achieved, using the next corollary.

**Corollary 3.6**.: _Given an analyst \(A\), for any \(>0\) we have_

\[}[ |Q(D^{V})-Q(D)|>_{Q}] }[_{^{2}} (D_{}^{v}\|D_{})>^{2}].\]

The corresponding version of this corollary for bounded range queries provides an alternative proof to the generalization guarantees of the the LS stability notion (Shenfeld and Ligett, 2019).

## 4 Pairwise concentration

In order to leverage Lemma 3.5, we need a stability notion that implies Bayes stability of query responses in a manner that depends on the actual datasets and the actual queries (not just the worst case). In this section we propose such a notion and prove several key properties of it. Missing proofs from this section can be found in Appendix D.

We start by introducing a measure of the stability loss of the mechanism.

**Definition 4.1** (Stability loss).: Given two sample sets \(s,s^{}^{n}\), a query \(q\), and a response \(r\), we denote by \(_{q}(s,s^{};r)((r\, |\,s)}{D^{q}(r\,|\,s^{})})\) the _stability loss_ of \(r\) between \(s\) and \(s^{}\) for \(q\) (we omit \(q\) from notation for simplicity).5 Notice that if \(R M(s,q)\), the stability loss defines a random variable. Similarly, given two elements \(x,y\), we denote \((x,y;r)()\) and \((x;r)()\). This definition extends to views as well.

Next we introduce a notion of a bound on the stability loss, where the bound is allowed to depend on the pair of swapped inputs and on the issued queries.

**Definition 4.2** (Pairwise concentration).: Given a non-negative function \(:^{n}^{n}^{+}\) which is symmetric in its first two arguments, a mechanism \(M\) will be called \(\)_-Pairwise Concentrated_ (or PC, for short), if for any two datasets \(s,s^{}^{n}\) and \(q\), for any \( 0\),6

\[}[((-1 )((s,s^{};V)-(s,s^{}; V)))] 1.\]

We refer to such a function \(\) as a _similarity function_ over responses or views, respectively.

The similarity function serves as a measure of the local sensitivity of the issued queries with respect to the replacement of the two datasets, by quantifying the extent to which they differ from each other with respect to the query \(q\). The case of noise addition mechanisms provides a natural intuitive interpretation, where the \(\) scales with the difference between \(q(s)\) and \(q(s^{})\), which governs how much observing the response \(r\) distinguishes between the two datasets, as stated in the next lemma.

We note that the first part of this definition can be viewed as a refined version of zCDP (Definition B.18), where the bound on the Renyi divergence (Definition B.5) is a function of the sample setsand the query. As for the second part, since the bound depends on the queries, which themselves are random variables, it should be viewed as a bound on the Renyi dissimilarity notion that we introduce in the appendix (Definition B.9). This kind of extension is not limited to Renyi divergence, as discussed in Appendix B.

**Lemma 4.3** (Gaussian mechanism is PC).: _Given \(>0\) and an analyst \(A\), if \(M\) is a Gaussian mechanism with noise parameter \(\), then it is \(\)-PC for \((s,s^{};q)))^{2}}{2^{2}}\) and \(\)-PC with respect to \(A\) for \((s,s^{};v)_{v}(s )-_{v}(s^{})\|^{2}}{2^{2}}\)._

To leverage this stability notion in an adaptive setting, it must hold under adaptive composition, which we prove in the next theorem.

**Theorem 4.4** (PC composition).: _Given \(k\), a similarity function over responses \(\), and an analyst \(A\) issuing \(k\) queries, if a mechanism \(M\) is \(\)-PC, then it is \(\)-PC with respect to \(A\), where \((s,s^{};v)_{i=1}^{k} (s,s^{};q_{i})\)._

Before we state the stability properties of PC mechanisms, we first transition to its element-wise version, which while less general than the original definition, is more suited for our use.

**Lemma 4.5** (Element-wise PC).: _Given a similarity function \(\) and an analyst \(A\), if a mechanism \(M\) that is \(\)-PC with respect to \(A\) receives an iid sample from \(^{n}\), then for any two elements \(x,y\) and \( 1\) we have_

\[}_{V D( x)}[(( -1)((x,y;V)-(x,y;V) ))] 1\]

_and_

\[}_{V D( x)}[(( -1)((x;V)-(x;V) ))] 1,\]

_where \((x,y;v)_{s^{n-1}}( ((s,x),(s,y);v))\) and \((x;v)(}_{V D} [e^{(x,Y;v)}])\)._

Finally, we bound the Bayes stability of PC mechanisms.

**Theorem 4.6** (PC stability).: _Given a similarity function over views \(\) and an analyst \(A\), if a mechanism \(M\) that is \(\)-PC with respect to \(A\) receives an iid sample from \(^{n}\), then for any \(,>0\), \(}_{ D,S D (n)\\ V M(S,A)}[(X;V)]\), we have_

\[}_{ D(n )\\ V M(S,A),Q A(V)}[|Q (D_{}^{v})-Q(D_{})|> _{Q}]\] \[}_{S D(n )\\ V M(S,A)}[}_{X D} [e^{2()((X;V)+ )}]>1+}{6}]+O( }{^{2}}),\]

_where \((x;v)\) is as defined in Lemma 4.5._

An exact version of the bound can be found in Theorem D.10.

Proof outline.: From Corollary 3.6,

\[}_{S D(n)\\ V M(S,A),Q A(V)}[|Q (D_{}^{v})-Q(D_{})|> _{Q}]}_{S D(n )\\ V M(S,A)}[_{^{2}}(D_{ }^{v}\|D_{})>^{2}].\]

Denoting \((x;v)(x;v)++4)((x;v)+)}\), this can be bounded by

\[}_{S D(n)\\ V M(S,A)}[_{^{2}}(D_{ }^{v}\|D_{})>}_{X D}[ (e^{(X;V)}-1)^{2}]+}{2} ]+}_{S D(n)\\ V M(S,A)}[}_{X D}[ (e^{(X;V)}-1)^{2}]>}{2 }].\]

We then show via algebraic manipulation that this second term can be bounded using the inequality

\[}_{X D}[(e^{(X;v)}-1 )^{2}] 3(}_{X D}[e^{2( )((X;v)+)}]-1 ),\]and, recalling the definition of the chi square divergence between \(P\) and \(Q\) (Definition B.2) \(_{^{2}}(P\|Q)}{} [(-1)^{2}]\), the first term can be bounded using Markov's inequality followed by the Cauchy-Schwarz inequality by the term:

\[}}}{} [(-1 )^{2}_{B()}((X,V ))]\] \[} {c}X D,S D^{(n)}\\ V M(S,A)}{}[()^{4}]+1)}}{}X,V B()}]}^{**}\]

where \(B()\{(x,v) \,|\,|\,()|>(x;v)\}\).

We then bound the quantity * by \(O(e^{})\), and we bound ** by \(O(}{})\), which completes the proof. 

Combining this theorem with Theorem 3.4 we get that PC and sample accuracy together imply distribution accuracy.

## 5 Variance-based generalization guarantees for the Gaussian mechanism

In this section we leverage the theorems of the previous sections to prove variance-based generalization guarantees for the Gaussian mechanism under adaptive data analysis. All proofs from this section appear in Appendix E.

We first provide generalization guarantees for bounded queries.

**Theorem 5.1** (Generalization guarantees for bounded queries).: _Given \(k\); \(,, 0\); \(0<\); and an analyst \(A\) issuing \(k\)\(\)-bounded linear queries with variance bounded by \(^{2}\), if \(M\) is a Gaussian mechanism with noise parameter \(=(}{n}})\) that receives an iid dataset of size \(n=(\{,}{^{ 2}}\}())\), then \(M\) is \((,)\)-distribution accurate._

_An exact version of the bound can be found in Theorem E.4._

This theorem provides significant improvement over similar results that were achieved using differential privacy (see, e.g., Theorem 13 in Jung et al. (2020), which is defined for \(=1\)), by managing to replace the \(^{2}\) term with \(^{2}\). The remaining dependence on \(\) is inevitable, in the sense that such a dependence is needed even for non-adaptive analysis. This improvement resembles the improvement provided by Bernstein's inequality over Hoeffding's inequality.

This generalization guarantee, which nearly avoids dependence on the range of the queries, begs the question of whether it is possible to extend these results to handle unbounded queries. Clearly such a result would not be true without some bound on the tail distribution for a single query, so we focus in the next theorem on the case of sub-Gaussian queries. Formally, we will consider the case where \(q(X)-q(D)\) is a sub-Gaussian random variable, for all queries.

**Definition 5.2** (Sub-Gaussian random variable).: Given \(>0\), a random variable \(X\) will be called \(^{2}\)_-sub-Gaussian_ if for any \(\) we have \([e^{ X}] e^{}{2}}\).

We can now state our result for unbounded queries.

**Theorem 5.3** (Generalization guarantees for sub-Gaussian queries).: _Given \(k\); \(, 0\); \(0<\); and an analyst \(A\) issuing \(k\)\(^{2}\)-sub-Gaussian linear queries, if \(M\) is a Gaussian mechanism with noise parameter \(=(}{n}})\) that receives an iid dataset of size \(n=(}{^{2}}())\), then \(M\) is \((,)\)-distribution accurate._These results extend to the case where the variance (or variance proxy) of each query \(q_{i}\) is bounded by a unique value \(_{i}^{2}\), by simply passing this value to the mechanism as auxiliary information and scaling the added noise \(_{i}\) accordingly. Furthermore, using this approach we can quantify the extent to which incorrect bounds affect the accuracy guarantee. Overestimating the bound on a query's variance would increase the error of the response to this query by a factor of square root of the ratio between the assumed and the correct variance, while the error of the other responses would only decrease. On the other hand, underestimating the bound on a query's variance would only decrease the error of the response to this query, while increasing the error of each subsequent query by a factor of the square root of the ratio between the assumed and the correct variance, divided by the number of subsequent queries. A formal version of this claim can be found in Section E.3.

## 6 Discussion

The contribution of this paper is two-fold. In Section 3, we provide a tight measure of the level of overfitting of some query with respect to previous responses. In Sections 4 and 5, we demonstrate a toolkit to utilize this measure, and use it to prove new generalization properties of fundamental noise-addition mechanisms. The novelty of the PC definition stems replacing the fixed parameters that appear in the differential privacy definition with a function of the datasets and the query. The definition presented in this paper provides a generalization of zero-concentrated differential privacy, and future work could study similar generalizations of other privacy notions, as discussed in Section B.4.

One small extension of the present work would be to consider queries with range \(^{d}\). It would also be interesting to extend our results to handle arbitrary normed spaces, using appropriate noise such as perhaps the Laplace mechanism. It might also be possible to relax our assumption that data elements are drawn iid to a weaker independence requirement. Furthermore, it would be interesting to explore an extension from linear queries to general low-sensitivity queries.

We hope that the mathematical toolkit that we establish in Appendix B to analyze our stability notion may find additional applications, perhaps also in context of privacy accounting. Furthermore, the max divergence can be generalized analogously to the "dynamic" generalization of Renyi divergence proposed in this paper (B.9), perhaps suggesting that this approach may be useful in analyzing other mechanisms as well.

Our Covariance Lemma (3.5) shows that there are two possible ways to avoid adaptivity-driven overfitting--by bounding the Bayes factor term, which induces a bound on \(|q(D^{v})-q(D)|\), as we do in this work, or by bounding the correlation between \(q\) and \(K(,v)\). This second option suggests interesting directions for future work. For example, to capture an analyst that is non-worst-case in the sense that she "forgets" some of the information that she has learned about the dataset, both the posterior accuracy and the Bayes stability could be redefined with respect to the internal state of the analyst instead of with respect to the full view. This could allow for improved bounds in the style of Zrnic and Hardt (2019).

We gratefully acknowledge productive discussions with Etam Benger, Vitaly Feldman, Yosef Rinott, Aaron Roth, and Tomer Shoham. This work was supported in part by a gift to the McCourt School of Public Policy and Georgetown University, Simons Foundation Collaboration 733792, Israel Science Foundation (ISF) grant 2861/20, and a grant from the Israeli Council for Higher Education. Shenfeld's work was also partly supported by the Apple Scholars in AI/ML PhD Fellowship. Part this work was completed while Ligett was visiting Princeton University's Center for Information Technology Policy.