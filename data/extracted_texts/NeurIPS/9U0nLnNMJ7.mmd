# Compact Language Models via Pruning and Knowledge Distillation

Saurav Muralidharan Sharath Turuvekere Sreenivas Raviraj Joshi

Marcin Chochowski Mostofa Patwary Mohammad Shoeybi Bryan Catanzaro

Jan Kautz Pavlo Molchanov

NVIDIA

{sauravm,sharath,ravirajj,mchochowski,mpatwary,mshoeybi, bcatanzaro,jkautz,pmolchanov}@nvidia.com

Equal contribution.

###### Abstract

Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective **compression best practices** for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4\(\), and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40\(\) fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8\(\) for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface 2, with corresponding supplementary material including example code available on GitHub 3.

## 1 Introduction

Large language models (LLMs) now dominate real-world natural language processing and have demonstrated excellent proficiency in understanding difficult contexts . To aid users targeting different deployment sizes and scales, model providers often train an entire family of models from scratch, each with a different size (number of parameters). For instance, the LLMa-2 model family  includes three different variants with 7, 13, and 70 bil

Figure 1: Minitron results. Our approach greatly reduces training costs for _additional models_ (40\(\)) while improving accuracy.

lion parameters, while the Pythia family  offers a selection of eight models with sizes ranging from 80 million to 12 billion parameters. However, training multiple multi-billion parameter models from scratch is extremely time, data and resource-intensive. In this paper, we ask the following question: _can we train one big model, and obtain smaller, more accurate (w.r.t. training from scratch) models from it through a combination of weight pruning and retraining, while only using a small fraction of the original training data?_ Achieving such a goal would make producing LLMs targeting different deployment scales significantly cheaper. Weight pruning is a powerful and well-known technique for reducing model size . In this paper, we focus on structured pruning, where blocks of nonzero elements are removed at once from model weights; examples of structured pruning techniques include neuron, attention head, convolutional filter, and depth pruning . While the literature is rich with numerous papers on structured pruning, to an end-user, it's not always clear which technique to use, when, and how to combine them to consistently obtain good pruned models. Pruning is also often accompanied by some amount of _retraining_ for accuracy recovery ; this phase is extremely expensive in modern LLMs, often requiring access to large amounts of curated data. To the best of our knowledge, no existing work on structured pruning explores data-efficient retraining techniques such as distillation to minimize retraining cost.

In this paper, we perform a thorough empirical exploration of structured pruning and retraining across multiple axes: neurons in feed-forward layers, heads in multi-head attention layers, embedding channels, and model depth. Through our experiments, we gain valuable non-trivial insights on the metrics and hyper-parameters to use for each axis and how to effectively combine axes for higher compression rates. For instance, we discover that pruning neurons and heads alone is initially superior to pruning neurons, heads and embedding channels; however, after a few steps of retraining, this order flips. Similarly, we discover that width pruning works better than depth, but only after some retraining (see Table 1 for a concrete example). We also investigate in detail how a pruned model can be efficiently retrained for optimal performance using minimal additional data. Based on our findings, we develop a practical list of **LLM compression and retraining best practices**. Finally, we apply our findings to prune the Nemotron-4 15B model  and produce a family of smaller models, named Minitron, that compare favorably to similarly-sized models. Minitron 8B achieves better accuracy than Nemotron-3 8B  (using **40\(\)** fewer training tokens) and LLaMa-2 7B , and comparable accuracy to Mistral-7B , Gemma 7B  and Llama-3 8B; likewise, Minitron 4B outperforms the similarly-sized Gemma2 model and compares favorably to the Phi-2 model.

This paper makes the following key contributions:

1. Provides the first thorough empirical exploration of structured pruning and retraining in LLMs across multiple axes. It offers valuable insights on metrics and hyper-parameter settings for pruning, order of pruning, effects of combining different axes, and retraining techniques focusing on data efficiency.
2. Presents a list of effective and practical _LLM compression and retraining best practices_ grounded in extensive empirical evidence.
3. Introduces the Minitron family of LLMs, which are obtained through direct pruning of the Nemotron-4 15B model. Deriving Minitron models from Nemotron-4 15B requires up to 40\(\) fewer training tokens compared to training from scratch, while still (1) comparing favorably to various popular community LLMs of similar size, and (2) outperforming state-of-the-art depth and width-pruned models from the literature.

   DEP & MLP & ATT & EMB & Distillation Loss & LM Val Loss \\   \(\) & \(\) & \(\) & \(\) & \(5.35 0.38\) & 2.062 \\ \(\) & \(\) & \(\) & \(\) & \(\) & **6.33 \(\) 0.37** & **2.049** \\ \(\) & \(\) & \(\) & \(5.07 0.42\) & 2.101 \\ \(\) & \(\) & \(\) & \(\) & \(8.35 0.49\) & 2.155 \\   &  \(\) 2.34 & 3.953 \\   

Table 1: Demonstration of how various pruning strategies perform before and after lightweight retraining using \(\)1.8B tokens. We prune the Nemotron-4 15B model down to the size of Nemotron-3 8B and report the change in distillation loss (KL divergence  on logits) and the final LM validation loss with retraining. We see that width (attention, MLP, embedding) pruning outperforms depth, but only after retraining. The last row shows change in loss for the Nemotron-3 8B model.

## 2 Pruning Methodology

As shown in Figure 2, we start the pruning process by first computing the importance of each layer, neuron, head, and embedding dimension and then sorting these importance scores to compute a corresponding importance ranking. In this section, we detail how rankings are computed for each axis and then subsequently used to obtain a pruned model.

### Background and Notation

We begin with some formal definitions. Multi-Layer Perceptron (MLP) layers have two linear layers with a non-linear activation in between: \(()=_{1}^{T} _{2}\) ; here, \(\) denotes the input, and \(_{1}\) and \(_{2}\) are the two associated weight matrices in the MLP layer. \(_{1},_{2}^{d_{hidden} d_{model}}\), where \(d_{model}\) and \(d_{hidden}\) are the embedding and MLP hidden dimensions, respectively. \(()\) refers to the non-linear activation function.

We define the Multi-Head Attention (MHA) operation for an input \(\) as follows: \(()=(_{1},..._{L}) ^{O}\), and \(_{i}=(^{Q,i}, ^{K,i},^{V,i})\); here, \(^{Q,i},^{K,i},^{V,i}^ {d_{head} d_{model}}\) and \(^{O}^{Ld_{head} d_{model}}\) where \(d_{head}\) is the size of a single attention head, and \(L\) is the total number of heads.

Finally, the Layer Normalization operation (LayerNorm)  on an input \(\) is defined as follows: \(LN()=-}{+}}+\), where \(\) and \(^{2}\) represent the mean and variance across the embedding dimensions, \(\) is a small value for numerical stability, and \(\) and \(\) are learnable parameters.

### Importance Analysis

Estimating the importance or sensitivity of individual neural network components such as neurons, attention heads, and layers is a well-studied area [9; 13; 41]. In the context of LLMs, recent work has highlighted the ineffectiveness of traditional metrics such as weight magnitude for estimating importance ; instead, recent work on structured pruning of LLMs has focused on metrics such as gradient/Taylor , cosine similarity , and perplexity on a calibration dataset .

Owing to their enormous size, computing gradient information on modern LLMs is prohibitively memory and compute-intensive, and one of our primary goals is to avoid this expensive step when trying to obtain importance information. In this paper, we propose a purely _activation-based_ importance estimation strategy that simultaneously computes sensitivity information for all the axes we consider (depth, neuron, head, and embedding channel) using a small (1024 samples) calibration dataset and only _forward_ propagation passes. We now describe how this strategy is implemented for each individual axis.

Figure 2: High-level overview of our proposed iterative pruning and distillation approach to train a family of smaller LLMs. On a pretrained LLM, we first evaluate importance of neurons, rank them, trim the least important neurons and distill the knowledge from the original LLM to the pruned model. The original model is replaced with the distilled model for the next iteration of compression.

**Width:** we compute the importance of each head, neuron and embedding channel by examining the activations produced by the _MHA_, _MLP_ and _LayerNorm_ layers, respectively. We use a small calibration dataset \(D\) for this purpose 4. Formally, we compute activation-based importance scores for heads, neurons, and embedding channels as: \(F_{}^{(i)}=_{,}\|(^{Q,i},^{K,i},^{V,i})\|_ {2}\), \(F_{}^{(i)}=_{,} _{1}^{(i)}^{T}\), and \(F_{}^{(i)}=_{,}LN()_{i}\). Here, \(_{1}^{i}\) refers to the \(i^{}\) row of the weight matrix \(_{1}\). \(_{,}\) refers to aggregation along the batch and sequence dimensions. We observe from our experiments that performing a simple summation here is not always optimal. To this end, we perform a detailed evaluation of various aggregation functions along each of these dimensions and their corresponding performance in Table 11. Specifically, for a sequence of scores \(\), we try three functions: (1) mean(abs): \(_{i=1}^{n}|_{i}|\) (hereafter referred to as just _mean_), (2) L2 norm: \(^{n}_{i}^{2}}\), and (3) variance: \(_{i=1}^{n}(_{i}-)^{2}\). Layer-wise scores are then summed up to obtain network-wide importance scores for each axis.

**Depth (Layers):** for depth pruning, we evaluate the importance of each layer using two metrics: (1) perplexity (PPL)  and (2) Block Importance (BI) . For PPL-based ranking, we simply remove a single layer and compute its effect on perplexity of this pruned model; this serves as the "importance" or sensitivity of the layer . BI  uses the cosine distance between the input and output of a layer to estimate layer sensitivity. The BI score of layer \(i\) is computed as: \(_{i}=1-_{X,t}_{t,t}^{T}_{i+1,t} }{\|_{t,t}\|_{2}\|_{t+1,t}\|_{2}}\), where \(_{i}\) refers to the input to layer \(i\), and \(_{i,t}\) denotes the \(t^{th}\) row of \(_{i}\). The BI of all layers can be computed in a single forward pass, giving it a significant speed advantage over PPL-based importance. Additionally, following Gromov et al. , we can extend BI to estimate importance of several contiguous layers at the same time.

**Iterative Importance:** in this setting, we iteratively alternate between pruning and importance estimation for a given axis or combination of axes. Formally, given number of iterations \(T\) and source and target dimensions (layers, heads, etc.) \(d_{s}\) and \(d_{t}\), respectively, we iteratively compute importance on \(d_{s}-i-d_{t}}{T}\) dimensions and prune to \(d_{s}-(i+1)-d_{t}}{T}\) dimensions; \(i[0,T-1]\). We evaluate the effectiveness of iterative importance estimation in Table 12.

### Obtaining a Pruned Model

Figure 2 provides an overview of how pruned models are obtained. For a given architecture configuration, we first rank the elements of each axis according to the computed importance and perform trimming (reshaping) of the corresponding weight matrices directly. For neuron and head pruning, we trim MLP and MHA layer weights, respectively. In the case of embedding channels, we trim the embedding dimension of the weight matrices in MLP, MHA, and LayerNorm layers.

When pruning attention heads, we add the _residual_ info from the pruned heads back into the remaining heads, with the aim of preserving relevant knowledge from the pruned heads. This idea is an MHA analog of Layer Collapse  for depth pruning and provides a boost to model accuracy in our experiments. Formally, given \(L\) original attention heads \(head_{1},head_{2},...,head_{L}\) being pruned to \(K\) heads, each new head will have the form (for the \(i^{th}\) head): \(head_{i}+(head_{i}-head_{2K-i+1})\) for \(i[K-(L-K),K]\). In case of grouped query attention , we apply this strategy only to the query heads.

Lightweight Neural Architecture Search:Figure 3 provides an overview of our search strategy for finding optimal architecture configurations. Given a search space and parameter budget (left side of the figure), we enumerate all feasible architectures meeting the parameter budget. At this stage, while it's possible to further reduce the search space size using strategies such as genetic search and/or Bayesian optimization, we found that sticking to commonly used neuron, head and embedding dimensions, along with a reasonably narrow target parameter range (less than 1 billion) was sufficient to obtain tractable solution sets (less than 20 candidates). The feasible candidates then undergo **lightweight retraining** (\(\)1.8B tokens in this work). We show in Figure 9 that this retraining stage stabilizes relative rankings and helps us find a more accurate candidate to train further. We note that parameter-efficient fine-tuning techniques such as LoRA  can also be applied at this stage; we leave the exploration of such techniques to future work.

## 3 Retraining

We use the term _retraining_ to refer to the accuracy recovery process following pruning. In this paper, we explore two retraining strategies: (1) conventional training, leveraging ground truth labels, and (2) knowledge distillation using supervision from the unpruned model (teacher).

**Retraining with Knowledge Distillation:** Knowledge Distillation (KD) involves transfer of knowledge from a larger or more complex model called the teacher to a smaller/simpler model called the student . The knowledge transfer is achieved by having the student model mimic the output and/or the intermediate states of the teacher model. In our case, the the uncompressed and pruned models correspond to the teacher and student, respectively.

The output probability distribution of an LLM for a given token \(x_{i}\) is computed as: \(p(x_{i},)=}{})}{_{j=1}^{V} (}{})}\), where \(\) is the softmax temperature and \(|V|\) is the vocabulary size. Logit-based KD loss across the sequence of all output tokens is represented as: \(L_{}=_{k=1}^{l}(p_{t}^{k}(x,),p_{s}^ {k}(x,))\); here, \(p_{t}^{k}(x,)\) and \(p_{s}^{k}(x,)\) represent the teacher and student probability distributions on the \(k^{th}\) token, respectively, and \(l\) represents the sequence length.

For distillation, we explore various loss functions, and several combinations of intermediate states and mappings across the Transformer model as the loss components, along with their respective trade-offs. This is illustrated in Figure 4. The intermediate state-based KD loss across a sequence of Transformer-specific hidden states is represented as: \(L_{is}=_{k H}_{i=1}^{l}Loss_{k}(h_{t}^{ki},h_{s}^{ki})\), where \(h_{t}^{ki}\) and \(h_{s}^{ki}\) represent the \(k^{th}\) teacher and student hidden state for the \(i^{th}\) token, respectively, and \(l\) represents the sequence length; \(H\) is the set of chosen intermediate states. The mismatch in student and teacher hidden states is handled by learning a shared linear transformation during distillation to upscale the student hidden state to the teacher hidden state dimension. The hidden states used are always post LayerNorm. We report our experimental results for retraining in Section 4.3.

Figure 4: Overview of Distillation. A \(student\) model with \(N\) layers is distilled from a \(teacher\) model with \(M\) layers. The \(student\) learns by minimizing a combination of embedding output loss, logit loss and transformer encoder specific losses mapped across \(student\) block \(S\) and \(teacher\) block \(T\).

Figure 3: Overview of our neural architecture search algorithm. We perform a search on multiple axes: number of layers, attention head count, MLP and embedding dimensions to arrive at a set of feasible architectures meeting the parameter budget. RT refers to retraining.

The total loss \(L\) is computed as \(L=L_{}+L_{logits}+ L_{is}\); where \(L_{CLM}\) is the student cross-entropy loss against the ground truth labels, and \(\) is a weighting coefficient. As the magnitudes of \(L_{logits}\) and \(L_{is}\) differ significantly, we found that computing \(\) dynamically as \(}{L_{is}}\) achieves better results compared to using a constant.

## 4 Experiments and Results

We evaluate our pruning strategy on the Nemotron-4 family of models ; specifically, we compress the Nemotron-4 15B model with 15.6 billion parameters down to two target parameter ranges: (1) 8 billion, and (2) 4 billion. We use the NVIDIA Megatron-LM framework  to implement our pruning and distillation algorithms for compression and retraining.

**Data and Training Hyperparameters:** we use the Nemotron-4 curated 8 trillion token (8T) base pretraining dataset and the continued training dataset (CT) . We use the 8T training blend for all our ablations and use a combination of both data blends to retrain our final models. Unless otherwise specified, we use 1.8 billion tokens (400 steps) for lightweight retraining. The calibration dataset \(D\) used for importance estimation consists of 1024 samples drawn randomly from the full dataset. We use the same optimizer settings and data split as  with cosine LR decay schedule from \(2^{-4}\) to \(4.5^{-7}\).

**Downstream Tasks:** following Touvron et al. , we evaluate our models of similar size on a series of downstream tasks, including MMLU , HumanEval  for Python code generation, several question-answering datasets for common-sense reasoning: Arc-C , HellaSwag , TruthfulQA  and WinoGrande  and XL-Sum English  for summarization. We report the 5-shot performance on MMLU, 5-shot on Winogrande, 25-shot on ARC-Challenge, 10-shot

    &  \\   & **Benchmark** & **Metric** & **Hlama-3** & **Llama-2** & **Mistral** & **Gamma** & **Neuron-4** & **Neuron-3** & **Minitron** \\    } & \# Parameters & & 88 & 6.7B & 7.3B & 8.5B & 15.6B & 8.5B & 8.3B \\  & \# Non-Emb. Params & & 5.9B & 6.4B & 7B & 7.7B & 12.5B & 6.4B & 6.2B \\  & \# Training Tokens & & -15T & 2T & 8T & 6T & 8T & 3.8T & **94B** \\    } & winograde (5) & acc & 77.6 & 74 & 78.5 & 78 & 83.6 & 75.9 & **79.0** \\  & arc.,calinge (25) & acc\_norm & 57.8 & 53 & 60.3 & **61** & 58.8 & 52.8 & 52.6 \\  & MMLU(5) & acc & **65.3** & 46 & 64.1 & 64 & 66.6 & 54.7 & 63.8 \\  & hellaswag(10) & acc\_norm & 82.1 & 79 & **83.2** & 82 & 84.6 & 78.5 & 80.7 \\  & gsmk(5) & acc & 50.3 & 14 & 37 & 50 & 48.5 & 24.0 & **51.3** \\  & truthfulQ(0) & mc2 & 43.9 & 39 & 42.6 & **45** & 40.7 & 36.5 & 42.6 \\  & XLSum en (20(3) & rougeL & 30.9 & 31 & 4.80 & 17 & 32 & 30.9 & **31.2** \\   & MBPP(0) & pass@1 & **24.4** & 20 & 38.8 & 39 & 38 & 27.04 & 35.2 \\  & humaneval (=20(0)) & pass@1 & 28.1 & 12 & 28.7 & **32** & 35.4 & 20.7 & 31.6 \\   

Table 2: Performance of our pruned Minitron 8B model compared to multiple baselines: the original Nemotron-4 15B, the previous generation Nemotron-3 8B, and multiple community models. Minitron 8B uses 40\(\) fewer tokens than Nemotron-3 8B. All evaluations run by us, except for entries marked with *, which we report from the corresponding papers.

    &  \\   & **Benchmark** & **Metric** & **Phi-2** & **Gamma** & **Gamma2* & **Qwen2* & **MiniCPM* & **Minitron** \\  \# Parameters & & & 2.7B & 2.5B & 2.6B & 1.5B & 2.7B & 4.2B \\ \# Non-Emb. Params & & & 2.5B & 2B & 2B & 1.3B & 2.4B & 2.6B \\ \# Training Tokens & & & 1.4T & 3T & 2T & 7T & 1.1T & **94B** \\   & winogrande (5) & acc & **74** & 67 & 70.9 & 66.2 & - & **74.0** \\  & arc, challenge (25) & acc\_norm & **61** & 48 & 55.4 & 43.9 & - & 50.9 \\  & MMLU(5) & acc & 57.5 & 42 & 51.3 & 56.5 & 53.5 & **58.6** \\  & hellaswag(10) & acc\_norm & **75.2** & 72 & 73.0 & 66.6 & 68.3 & 75.0 \\  & gsmk(5) & acc & 55 & 18 & 23.9 & **58.5** & 53.8 & 24.1 \\  & truthfulqa(0) & mc2 & 44 & 33 & - & **45.9** & - & 42.9 \\  & XLSum en (20(3)) & rougeL & 1 & 11 & - & - & - & **29.5** \\   & MBPP(0) & pass@1 & **47** & 29 & 29.6 & 37.4 & - & 28.2 \\  & humaneval (=20(0)) & pass@1 & **50** & 24 & 17.7 & 31.1 & - & 23.3 \\   

Table 3: Performance of Minitron 4B model compared to similarly-sized community models. All evaluations run by us, except for entries marked with *, which we report from the corresponding papers. We only compare to base models without SFT and DPO, therefore Phi-3 is excluded.

on HellaSwag, 0-shot on 20% of XL-Sum and average pass@1 scores for HumanEval and MBPP. For pass@1 scores we use a temperature of 0.2 and nucleus sampling  with top-p = 0.95. For instruction-tuned models, we use MT-Bench , Instruction-Following Eval (IFEval) , ChatRAG-Bench , and Berkeley Function Calling Leaderboard (BFCL) .

### Main Pruning Results

We start by introducing the following list of **structured compression best practices**:

1. To train a family of LLMs, train the largest one and prune+distill iteratively to smaller LLMs.
2. Use (batch=L2, seq=mean) importance estimation for width axes and PPL/BI for depth.
3. Use single-shot importance estimation; iterative provides no benefit.
4. Prefer width pruning over depth for the model scales we consider (\(\) 15B).
5. Retrain exclusively with distillation loss using KLD instead of conventional training.
6. Use (logit+intermediate state+embedding) distillation when depth is reduced significantly.
7. Use logit-only distillation when depth isn't reduced significantly.
8. Prune a model closest to the target size.
9. Perform lightweight retraining to stabilize the rankings of searched pruned candidates.
10. If the largest model is trained using a multi-phase training strategy, it is best to prune and retrain the model obtained from the final stage of training.

We arrive at this list through a detailed set of ablations and experiments, and each point is backed by empirical evidence, as we demonstrate in the rest of this section and the Appendix. We use this list to obtain our Minitron pruned and retrained models, whose performance is shown in Tables 2 and 3. Here, we compare the performance of our pruned models to multiple baselines: (1) the original Nemotron-4 15B model, (2) the previous generation Nemotron-3 8B model, and (3) a set of similarly-sized community models, all trained from scratch with trillions of tokens. Evaluation is performed on the downstream tasks described earlier in this Section. In both tables, we list the number of full and non-embedding parameters, along with the number of training tokens used to arrive at the model.

We further compare the Minitron models to state-of-the-art depth and width-pruned baselines in Table 4; namely, LLM-Pruner , SliceGPT , LaCo , ShortGPT , and Sheared LLaMa . Table 10 (Appendix) lists the architecture details of the Nemotron and Minitron models shown in Tables 2 and 3. In the following subsections, we will go into more detail on how we arrived at the Minitron pruned models.

From Table 2, we notice that Minitron 8B compares favorably to the latest community models of the same size. Specifically, we outperform Nemotron-3 8B and LLaMa-2 7B, and perform on par with Mistral 7B, Gemma 7B and LLaMa-3 8B, all while using significantly fewer training tokens. Minitron 8B also significantly outperforms multiple depth-pruned models of larger size (\(\) 10B parameters) (Table 4). From Table 3, we notice that our smaller model, Minitron 4B, **retains model capabilities better** compared to small specialized models that score highly only on some tasks, outperforms the Gemma2 model and is significantly superior to multiple depth and/or width pruned models shown in Table 4.

Instruction Tuning:to better understand how Minitron models perform after supervised fine-tuning (SFT), we perform SFT on Minitron 4B using instruction-tuning data used for Nemotron-4 340B  to create Minitron 4B-instruct, and evaluate it on various tasks, including instruction-following and roleplay (IFEval and MT-Bench), RAG QA (ChatRAG-Bench), and function calling (BFCL). The results for this experiment are shown in Tables 5 to 8. Tables 5 to 7 demonstrate that Minitron 4B-instruct has strong instruction-following, roleplay and RAG capabilities, beating similarly sized models across all tasks. On function calling (Table 8), Minitron 4B-instruct outperforms Gemma-2B-IT and even Llama-3-8B-instruct.

**Best Practice #1:** in summary, Tables 2 - 8 provide strong empirical evidence to support the claim that training one single big model, and obtaining smaller ones from it through pruning + retraining achieves higher accuracy and is extremely cost/compute-efficient when compared to training them from scratch. Further, our efficient retraining strategy also **eliminates the need to curate trillions of tokens of data**.

**Cost Savings for Training a Model Family:** the FLOPs required per training step 5 for the 15B, 8B, and 4B models in the Nemotron-4 model family are, respectively: \(4.4\)e\(17\), \(2.5\)e\(17\) and \(1.2\)e\(17\). With the assumption that each model in the family is trained with an equivalent token count, steps and batch size, we obtain the following FLOP count for training each model in the family from scratch: \((4.4\)e\(17+2.5\)e\(17+1.2\)e\(17)\) steps. As noted from Tables 2 and 3, our approach requires \(40\) fewer training tokens for each additional model, hence resulting in the following updated FLOP count for the family: \((4.4\)e\(17+2.5\)e\(17/40+1.2\)e\(17/40)\) steps; the corresponding cost savings for training the full Nemotron-4 family using our approach is thus \(1.8\).

We now dive deeper into our empirical ablations that help us arrive at the list of best practices. Unless otherwise specified, we run these ablations on the Nemotron-4 15B checkpoint prior to continued training with the CT data blend.

### Obtaining the Best Pruned Model

**Best Aggregation Metric (Best Practice #2):** we start by exploring the best aggregation metric for use with our activation-based pruning criteria (see Section 2.2 for more details). Table 11 shows how zero-shot LM loss and Wikitext2 perplexity  vary w.r.t different intra-batch and sequence aggregation functions. Here, the Nemotron-4 15B model is pruned to the Nemotron-3 8B architecture with no retraining. We notice that there is significant variation in zero-shot performance based on the aggregation metric, indicating the importance of selecting the right one. Both (batch=L2, seq=mean) and (mean, mean) perform well; in the remainder of the paper, we use (l2, mean) primarily due to its slightly better performance on the 8T dataset. To further evaluate if these relative rankings hold after retraining, we perform a related experiment: we prune the same 15B model to 8B using: (1) the best ((L2, mean) metric, and (2) a poorly performing (L2, L2) metric, and perform retraining on both for 400 steps (\(\)1.8B tokens). The results of this experiment are shown in Figure 5. From the Figure, we conclude that these rankings continue to hold post-retraining.

**Iterative Importance (Best Practice #3):** we evaluate whether iterative importance estimation provides any benefit (described in Section 2.2) and report results in Table 12. Here, we take the Nemotron-4 15B model and prune the embedding dimension alone using number of iterations T=1, 2, and 4 iterations to the target value of 4096. We then perform lightweight retraining of all 3 candidates

  
**Model** & **Prompt-** & **Prompt-** & **Instruction-** \\  & **level Acc.** & **level Acc.** & **level Acc.** \\  & (**strict**) & **(dose)** & **(dose)** \\    &  & **73.01** & **81.29** \\  & & & & \\ Gemma-2B-IT & & 28.70 & 40.50 \\  &  & - & - \\  & & & \\ 

Table 6: Evaluation results on IFEval.

    &  \\  } & **Benchmark** & **Metric** & **LLMPruner** & **SliceGPT** & **LaCo** & **ShortGPT** & **Sheared LLaMa** & **Minitron** \\   & \# Parameters & & 9.8B & 9.9B & 9.8B & 9.8B & - & 8.3B \\  & \# Non-Emb. Params & & 9.5B & 9.5B & 9.5B & 9.5B & - & 6.2B \\   &  & **MMLU(5)** & acc & 25.2 & 37.1 & 45.9 & 54.7 & - & **63.8** \\  & & **lehawag(10)** & acc\_norm & 67.8 & 55.7 & 64.4 & 66.6 & - & **80.7** \\   & \# Parameters & & 4.8B & 4.9B & 4.9B & 4.9B & 2.7B & 4.2B \\  & \# Non-Emb. Params & & 4.5B & 4.6B & 4.6B & 4.6B & 2.5B & 2.6B \\    &  & winogrande (5) & acc & - & - & - & 64.2 & **74** \\  & & arc\_challenge(25) & acc\_norm & - & - & - & 41.2 & **50.9** \\  & & MMLU(5) & acc & 23.33 & 28.92 & 26.45 & 43.96 & 26.4 & **58.6** \\  & & hellaswag(10) & acc\_norm & 56.46 & 50.27 & 55.69 & 53.02 & 70.8 & **75** \\  & & sgm8k(5) & acc & - & - & - & - & 23.96 & **24.1** \\   

Table 4: Performance of Minitron models w.r.t recent state-of-the-art models obtained through depth/width pruning. Top and bottom halves show results for Minitron 8B and 4B, respectively.

  
**Model** & **Non-** & **Emb.** & **Tokens** & **Total** \\  & **Params** & & \\    & 2.6B & **90B** & **6.46** \\ Phi-2 & 2.5B & 1.4T & 4.29 \\   & 1.2B & N/A & 5.29 \\   & 2B & 6T & 5.19 \\   & StablerL2 Chat & 1.6B & 2T & 5.42 \\  TinyLama v1.0 Chat & 1.1B & 3T & 3.46 \\   

Table 5: Evaluation results on MT-Bench.

for 1.8B tokens. From the Table, we observe that while the iterative approach appears to be better before retraining, all 3 candidates converge to the same loss value, indicating no benefit.

**Combining Depth and Width (Best Practice #4):** we perform a simple experiment to compare the efficacy of width vs. depth pruning. Using the PPL and BI metrics defined in Section 2.2, we remove the 16 least important layers from the Nemotron 15B model based on both metrics to arrive at two variants of depth pruned models. We also perform neuron, head and embedding channel pruning to target the Nemotron-3 8B model and arrive at the width pruned variant. Finally, we combine depth (remove 4 least important layers) and width pruning to arrive at the fourth variant. We report the results of this experiment in Table 13. We notice that even though the depth-width pruned variant has a lower loss post-pruning, we see the results flip around 200 steps of retraining (0.8B tokens); Table 1 and Figure 6 further illustrate this point.

### Retraining and Search

**Distillation vs. Conventional Training (Best Practice #5):** in this experiment, we compare: (1) training a 4B model with random initialization (4B-Random-Init), (2) pruning 15B to 4B, followed by retraining with conventional training (4B-Pruned), and (3) pruning 15B to 4B, and then retraining with distillation using the 15B model as the teacher (4B-Pruned-Distill). Since distillation adds training overheads (additional forward pass on the teacher model), we compare approaches under iso-compute settings. Table 14 shows the results. Here, we observe a significant improvement in MMLU for (3), while both (1) and (2) score randomly. On HellaSwag, (3) > (2) > (1). This clearly demonstrates the superiority of distillation over conventional training after pruning.

**Choice of Loss Function (Best Practice #5):** we experiment with Kullback-Leibler divergence (KLD), MSE, cosine similarity and reverse KLD (R-KLD) to compute \(L_{logits}\). Recent work has shown R-KLD  to be a better fit than KLD in the SFT/instruction-following setting, and Agarwal et al.  claim the choice of loss is task-dependent. We observe from Table 15 and 16 that **KLD** is the best choice for pruned base model training.

**Choice of Losses (Best Practices #6 and #7):** typically, a weighted combination of \(L_{CLM}\) and \(L_{logits}\) is used for distillation. We find that using \(L_{logits}\) alone results in the best performance as shown in Table 15. For \(L_{is}=L_{emb}+L_{att}+L_{i}+L_{o}\), we make several observations similar to Lu et al. ; these are listed in Appendix A.6 and in Table 17. Most notably, we observe no improvements from using \(L_{is}\) when retraining models that **don't prune the depth axis significantly**, such as Minitron 8B and Minitron 4B and hence use \(L_{logits}\) alone in such cases (see Table 18).

**One-shot vs Iterative Pruning and Distillation Across Model Sizes (Best Practice #8):** compressing Nemotron-4 15B to Minitron 4B requires an aggressive 73.3% reduction of original model weights. We hypothesize that aggressive one-shot pruning loses out on important capabilities of the base LLM. We thus explore a simple iterative two-step pruning and retraining strategy where we first prune and retrain Nemotron-4 15B to create Minitron 8B (\(\)46% reduction) and further prune and retrain the latter to Minitron 4B (\(\)50% reduction). Table 14 (last two rows) shows the comparison between single-shot and iterative pruning, and demonstrates that iterative achieves a 12% improvement in the MMLU scores compared to the one-shot strategy. During the final retraining step, we observe that using Nemotron-4 15B as the teacher achieves superior results compared to using Minitron 8B. We provide additional ablations on one-shot vs. iterative pruning in Appendix A.7.

**Search with Retraining (Best Practice #9):** for lightweight neural architecture search, we use the search spaces defined in Table 9 for Minitron 8B and 4B. We further specify a target parameter range of 8 and 4 billion parameters for the respective models, with a tolerance of 5%. With these settings, we obtain 15 and 18 feasible candidates for the 8B and 4B parameter targets, respectively. The architecture configurations for these candidates are provided in Table 19. As described in Section 2.3, we perform lightweight retraining of all feasible candidates. Figure 9 illustrates how

  
**Model** & **Avg** \\  Minitron 4B-instruct & **41.11** \\ Gamma-2B-TT & 33.31 \\   

Table 7: Evaluation results on ChatRAG-Bench.

validation loss changes for the 8B candidates as training progresses. We notice that relative rankings undergo significant changes up to \(\) 300 steps, and then stabilize.

**Single vs Multi-Phase Retraining (Best Practice #10):** Recent studies  have shown improved results with multi-phase pretraining routines. Initially, models are trained on web data, followed by a lightweight phase with cleaner data. We explored two compression techniques: (1) prune the phase 1 checkpoint, retrain with portions of phase 1 and 2 data, and (2) prune the phase 2 checkpoint, retrain with a portion of phase 2 data. Table 20 shows that (2) is sufficient to regain accuracy and surpasses (1). This strategy is used for our best models, also suggesting that for further aligned models, it may suffice to prune the aligned model and retrain with a portion of the alignment dataset.

## 5 Related Work

**Structured LLM Pruning:** there have been a number of recent structured pruning papers specifically targeting LLMs; we can broadly classify these works into two main categories: (1) ones that prune only depth (layers), (2) ones that prune width (attention heads, MLP intermediate dimension, etc.) and/or depth. Recent work in the first category (depth pruning) includes ShortGPT , LaCo , and Shortened LLaMa ; for pruning layers in Mintron models, we reuse and extend the metrics proposed in some of these works (eg: block importance from ShortGPT ). A number of recent papers have also proposed new saliency metrics and pruning strategies targeting width dimensions: namely, embedding channels, attention heads, and MLP intermediate channels [11; 4; 53; 33]. Most work in this category uses learnable masks, combined with an Augmented Lagrangian loss formulation to arrive at optimal width masks [4; 53; 33]. At LLM scale, this strategy has multiple disadvantages: (1) it requires compute and memory-intensive gradient computations, and (2) it requires a considerable amount of data and fine-tuning to arrive at reasonable masks. The notable exception in this line of work is Dery et al. , which recognizes these limitations and proposes saliency metrics that can be computed with only forward passes. To the best of our knowledge, we provide the first pruning strategy that (1) simultaneously targets both width and depth dimensions, (2) works at LLM scale (i.e., uses only forward passes for computing importance and uses a small fraction of pretraining data), and (3) achieves state-of-the-art compression and accuracy.

**Post-pruning Accuracy Recovery:** recent work has leveraged either a teacher model which is larger/better [2; 27] or teacher-generated synthetic data [1; 16; 36; 37] to improve the accuracy of an existing trained smaller base model in the Supervised Fine Tuning (SFT)/instruction following setting. Compared to recent width and depth pruning work [26; 34; 53], to the best of our knowledge, we are the first to employ distillation from an uncompressed teacher to improve the retraining of structurally-pruned student models.

## 6 Conclusions

This paper has presented a thorough empirical exploration of structured pruning and retraining in LLMs, offering unique insights into pruning order, effects of combining pruning axes, and retraining techniques for minimal data use. We have developed a set of compression and retraining best practices, backed by extensive empirical evidence, which we employ to prune the Nemotron-4 15B model by a factor of 2-4\(\). Our compressed Mintron models are significantly cheaper to obtain compared to training each model from scratch (requiring up to 40\(\) fewer training tokens), while still performing favorably to a number of similarly-sized community models; Mintron models also outperform multiple state-of-the-art depth and width pruned models from the literature. **Limitations:** one notable limitation of our work is that we currently apply our proposed techniques only on the Nemotron family of models; we plan to address this by pruning other model families in future work. Also, even though it is short, our method requires full model retraining.

  
**Target** & **Layers** & **Heads** & **MLP Exp. Factor** & **Embedding** \\  Mintron 8B & [29–32] & \{32,48\} & \{2.5,3,3.5,4\} & \{4096,4680,5120,5632,6144\} \\ Mintron 4B & [29–32] & \{24,32,48\} & \{2.5,3,3.5,4\} & \{2560,3072,3584,4096,4608\} \\   

Table 9: Mintron 8B and 4B search space.