# GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction

Shijin Duan\({}^{*}\) Ruyi Ding Jiaxing He Aidong Adam Ding Yunsi Fei Xiaolin Xu

Northeastern University

{duan.s, ding.ruy, he.jiaxi, a.ding, y.fei, x.xu}@northeastern.edu

equal contribution

###### Abstract

Graph-structured data is integral to many applications, prompting the development of various graph representation methods. Graph autoencoders (GAEs), in particular, reconstruct graph structures from node embeddings. Current GAE models primarily utilize self-correlation to represent graph structures and focus on node-level tasks, often overlooking multi-graph scenarios. Our theoretical analysis indicates that self-correlation generally falls short in accurately representing specific graph features such as islands, symmetrical structures, and directional edges, particularly in smaller or multiple graph contexts. To address these limitations, we introduce a cross-correlation mechanism that significantly enhances the GAE representational capabilities. Additionally, we propose the GraphCroc, a new GAE that supports flexible encoder architectures tailored for various downstream tasks and ensures robust structural reconstruction, through a mirrored encoding-decoding process. This model also tackles the challenge of representation bias during optimization by implementing a loss-balancing strategy. Both theoretical analysis and numerical evaluations demonstrate that our methodology significantly outperforms existing self-correlation-based GAEs in graph structure reconstruction. Our code is available in https://github.com/sjduan/GraphCroc.

## 1 Introduction

Graph-structured data captures the relationships between data points, effectively mirroring the interconnectivity observed in various real-world applications, such as web services , recommendation systems , and molecular structures [17; 18; 12]. Beyond the message passing through node connections , the exploration of graph structure representation is equally critical [38; 15; 33; 19; 9]. This representation is extensively utilized in domains including recommendation systems, social network analysis, and drug discovery , by leveraging the power of Graph Neural Networks (GNNs). Specifically with \(L\) layers in GNN, a node assimilates structural information from its \(L\)-hop neighborhood, embedding graph structure in node features.

Graph autoencoders (GAEs) have been developed to encode graph structures into node embeddings and decode these embeddings back into structural information, such as the adjacency matrix. This structural reconstruction process can be performed either sequentially along nodes [11; 22; 47] or in a global fashion . While there has been significant advancement in both methods, most studies primarily focus on node tasks, which involve a single graph, such as link prediction  and node classification , with decoding strategies typically reliant on "self-correlation". We define this term as the correlation of node pair from the same node embedding space. Given an \(n\)-node graph with embedding dimension \(d^{}\) on each node, its node embedding is \(Z^{n d^{}}\), thus the self-correlation is expressed as \(z_{i}^{T}z_{j}\) between two nodes. Correspondingly, the "cross-correlation" depicts the node paircorrelation as \(p_{i}^{T}q_{j}\), which are from two separate embedding spaces, \(P,Q^{n d^{}}\). However, studies are seldom evaluated under graph tasks, which represent graph structure on multiple graphs. The distinctiveness of each graph presents a significant challenge in accurately representing all graphs.

In this work, we demonstrate the limitations of self-correlation in structure representation, such as accurately representing islands, topologically symmetric graphs, and directed graphs. Although these deficiencies may appear infrequently in large, undirected single graphs, they are prevalent and critical in smaller to moderately-sized multiple graphs, e.g., molecules . Conversely, we establish that decoding based on cross-correlation can significantly mitigate these limitations, offering improvements in both undirected and directed graphs. Furthermore, the optimization of self-correlation-based GAE has to proceed in a restricted space. On the other hand, the cross-correlation can double the argument space that is not restricted during optimization. It makes the region of attraction smoother and easier to converge, indicating the superior representational ability of cross-correlation.

Accordingly, we propose a novel GAE model, namely GraphCroc, which leverages cross-correlation to node embeddings and a U-Net-like encoding-decoding procedure. Previous GAE models carefully design the encoder for a faithful structure representation, yet keep the decoder as the straightforward node correlation computation. Differently, GraphCroc retains the freedom of encoder design, facilitating its architecture design for downstream tasks, rather than structure representation. We define the decoder as a mirrored architecture of the encoder, to gradually reconstruct the graph structure. The encoder shapes the down-sampling of GraphCroc, while the decoder half performs the up-sampling for structural reconstruction. In addition, regarding the unbalanced population of zeros and ones in graph structure, i.e., sparse adjacency matrix, we define loss balancing on node connections.

We highlight our contributions as follows:

* We analyze the representational capabilities of self-correlation within GAE encoding, highlighting its limitations. Furthermore, we elaborate how cross-correlation addresses these deficiencies, facilitating a smoother optimization process.
* We propose GraphCroc, a cross-correlation-based GAE that integrates seamlessly with GNN architectures for graph tasks as its encoder, and structures a mirrored decoder. GraphCroc offers superior representational capabilities, especially for multiple graphs.
* To the best of our knowledge, this is the first evaluation of structural reconstruction using GAE models on graph tasks. Besides, we assess the performance of our GraphCroc model integrated with other GAE strategies and on various downstream tasks.
* We evaluate the potential of GraphCroc in domain-specific applications, such as whether a GAE focused on structural reconstruction could be an attack surface for edge poisoning attacks, given the effectiveness and stealth of adversarial attacks using AE in vision tasks.

## 2 GAE Structural Reconstruction Analysis

### Preliminary

Graph Neural NetworkAs Graph Neural Networks (GNNs) have been defined in various ways, without loss of generality, we adopt the computing flow in  to define the graph structure and a general GNN model. A graph \(G=(V,E)\) comprises \(n\) nodes, each with a feature represented by a \(d\)-dimensional vector, resulting in a feature matrix \(X^{n d}\). The set of edges \(E\) is depicted by an adjacency matrix \(A\{0,1\}^{n n}\), which indicates the connections between nodes in \(G\). A GNN model \(f(X,A)\) is utilized to summarize graph information for downstream tasks.

The feed-forward propagation of the \(l\)-th layer in GNN \(f()\) is

\[h_{l+1}=(_{l}^{\,-}_{l}_{l}^{\,- }h_{l}W_{l})\] (1)

\(h_{l}^{n d_{l}}\) is the input feature of the \(l\)-th layer, where \(h_{1}=X\) and \(d_{l}\) is the feature dimension of each node specified by each layer. \(_{l}=A_{l}+I\) is the adjacency matrix (self-loop added) of the input graph structure in each layer. Note that \(_{l}\) will be consistent in the absented pooling layer scenario, such as the node classification task, yet it can vary along GNN layers in graph tasks due to the introduction of graph pooling layers . Subsequently, we use the diagonal node degree matrix \(_{l}\) of \(_{l}\) to normalize the aggregation. For layer components, \(W_{l}\) is the weight matrix and \(\) is the activation function in the \(l\)-th layer.

Graph AutoencoderThe naive GAE is proposed to reconstruct the adjacency matrix \(A\) of a graph \(G\) through node embedding \(Z\):

\[\ Z=(Z|G)=f(X,A),\ =(A|Z)= (ZZ^{T})\] (2)

GAE first encodes the graph \(G\) through a general GNN model \(f()\) (a.k.a. inference model), resulting in node embeddings in the latent space \(Z^{n d^{}}\) where \(d^{}\) is the latent vector dimension for each node. The generative model is non-parameterized, but the tensor product of the node embeddings. It measures the correlation between each node pairs, i.e., \((z_{i}^{T}z_{j})\), normalized as the link probability by the logistic sigmoid function. Usually, the predicted connection can be defined as an indicator function \((_{i,j})=(_{i,j} th)\), e.g., \(th=0.5\).

With the same functionality, GAE has been improved with other enhancements, such as variational embedding , MLP-based decoder , masking on features  and edges . Note that other correlation measurements in the decoder are also proposed, such as Euclidean distance between node embedding , i.e., \(_{i,j}=(C(1- z_{i}-z_{j}_{2}^{ 2}))\), where \(C\) is a temperature hyperparameter. In general, they predict the connection between node pairs by measuring the similarity, such as inner product and L2-norm, from the same embedding space. Related work is discussed in Appendix A.

### Deficiencies of Self-Correlation on Graph Structure Representation

To generalize the discussion, we set the representation capability of the encoder as unrestricted, i.e., allowing \(Z\) to be generated through any potentially optimal encoding method. On the decoding side, self-correlation is applied following Eq.2. We identify and discuss specific (sub)graph structures that are poorly addressed by current self-correlation methods:

**Islands (Non Self-Loop).** Both the inner product and the L2-norm of a node embedding pair fail to accurately represent nodes without self-loops, where \(A_{i,i}=0\). Given that \(z_{i}^{T}z_{i} 0\) and \(C(1- z_{i}-z_{i}_{2}^{2})=C>0\), the predicted value \((_{i,i})\) defaults to 1 when threshold 0.5 is applied to the sigmoid output. This limitation underlies the common homophily assumption in previous research , that all nodes contain self-loops; it treats self-loops as irrelevant to the graph's structure. However, there is a huge difference between the self-connection on one node and the inter-connection between nodes in some scenarios; for example, on heterophilous graphs , nodes are prone to connect with other nodes that are dissimilar -- such as fraudster detection.

**Topologically Symmetric Structures.** If graph structure is symmetric along an axis or a central pivot, as demonstrated in Figure 1, the self-correlation method cannot represent these structures as well.

**Definition 2.1** (Topologically Symmetric Graph).: A symmetric graph \(G=(V,E)\) has the structure and node features topologically symmetric either along a specific axis or around a central node. For an axisymmetric graph, the feature matrix is denoted as \(X=\{X_{1},,X_{n_{1}}\}\{X_{l1},,X_{ln_{2}}\}\{X_{r1}, ,X_{rn_{2}}\}\), such that \(n_{1}+2n_{2}=n\). \(X_{i}\) represents the node features on the axis, and for each paired node off the axis, \(X_{li}=X_{ri}\). The connections are also symmetric, satisfying \(A_{li,:}=A_{ri,:}\). In a centrosymmetric graph, the pivot node has feature \(X_{1}\), and other nodes share the same feature \(X_{2}==X_{n-1}\). Additionally, the adjacency relationships for these nodes are identical, with \(A_{i,:}=A_{j,:}\) for all \(i,j[2,n]\).

**Lemma 2.2**.: _Given an arbitrary topologically symmetric graph \(G=(V,E)\) and an encoder \(f(X,A)\), the self-correlation decoder output will always have \((_{li,ri})=1\)._

Proof.: Due to the symmetry on graph \(G=(V,E)\) from the definition above, we have \(f(X_{i},A_{i,:})=f(X_{j},A_{j,:})\) for nodes \(i\) and \(j\) that are symmetric about the axis or pivot. Thus, we can derive \(z_{i}=z_{j}\). For the prediction on link between \(i\) and \(j\), we have \(_{i,j}=(z_{i}^{T}z_{j})=(z_{i}^{T}z_{i})  0.5\). Similarly, for the L2-norm method, we have \(_{i,j}=(C(1- z_{i}-z_{j}_{2}^{ 2}))=(C(1- z_{i}-z_{i}_{2}^{2}))=(C)>0.5\). In both decoding methods, the decoder is prone to predict the edge between two symmetric as positive, \((_{i,j})=1\).

Figure 1: Two examples of the topological symmetric graphs. The left graph is axis-symmetric; the right graph is centrosymmetric.

Consequently, the self-correlation method will indicate a positive connection between two symmetric nodes, regardless of their real connection status on the graph \(G\). Note that for variational GAE, \(Z\) is sampled from a Gaussian distribution whose mean and variance come from GNN models; thus it still follows the above lemma, even if randomness is involved during the encoding.

**Directed Graph.** Another straightforward deficiency of the self-correlation method is that it always generates a symmetric adjacency matrix, \(\), which is not suitable for directed graphs that require an asymmetric adjacency matrix. This issue is also acknowledged in , which proposes a solution involving dual encoding using the Weisfeiler-Leman (WL) algorithm  and node label coloring. However, this solution constrains the encoder to a dual-channel structure while maintaining the same decoding method as described in Eq. 2. This restriction can limit the flexibility of the encoder architecture, making it less adaptable for various downstream tasks.

Furthermore, we conduct a theoretical analysis of the dimensional requirements of node embedding to represent graph structures using self-correlation, in Appendix B.

### Our Cross-Correlation Approach for Better Structural Representation

Instead of self-correlation, we advocate cross-correlation to reconstruct the graph structure, denoted by \(=(PQ^{T})\), where \(P,Q^{n d^{}}\). This approach allows us to decouple the variables involved in calculating the correlation, thus overcoming the inherent limitations of self-correlation.

#### 2.3.1 How Does Cross-Correlation Mitigate Deficiencies of Self-Correlation?

**Expressing Islands.** For each node \(i[1,n]\), the sign of \(p_{i}^{T}q_{i}\) can be flexibly determined by \(p_{i}\) and \(q_{i}\), allowing it to be either positive or negative. Consequently, the presence of an island can be effectively modeled using \(_{i,j}=(p_{i}^{T}q_{i})\) or \((C(1-\|p_{i}-q_{j}\|_{2}^{2}))\). This approach avoids the limitations associated with self-correlation, which restricts the sigmoid input to positive.

**Expressing Symmetric Structure.** Cross-correlation is particularly effective in capturing topological symmetric structures. Given a node pair \((i,j)\) that is topologically symmetric about an axis or pivot, for undirected graphs, we have \(p_{i}=p_{j}\) and \(q_{i}=q_{j}\). However, since \(p_{i}\) and \(q_{j}\) (as well as \(p_{j}\) and \(q_{i}\)) are not directly dependent on each other, the sign of \(p_{i}^{T}q_{j}=p_{i}^{T}q_{i}\) can be either positive or negative. Therefore, \((_{i,j})=((p_{i}^{T}q_{j}))\) is able to yield 0 or 1, depending on the specific values of node embedding \(p_{i}\) and \(q_{j}\). This flexibility can be supported by the L2-norm decoding as well.

**Expressing Directed Graph.** A similar interpretation extends to the representation of directed graphs. For two nodes \(i\) and \(j\), the directed edges can be defined by \(p_{i}^{T}q_{j}\) for one direction and \(p_{j}^{T}q_{i}\) for the other. Since these four latent vectors do not have explicit dependencies among them, the directions of the edges can be independently determined using cross-correlation, capturing the directional connections between nodes.

In Appendix C, we further discuss and visualize how our cross-correlation approach represents these specific graph structures.

#### 2.3.2 Cross-Correlation Provides Smoother Optimization Trace

We highlight the superiority of cross-correlation over self-correlation in the optimization process of GAE training. Considering the decoder optimization problem, where we aim to satisfy the constraints:

\[\ ((z_{i}^{T}z_{j}))=A_{ i,j}\ ((p_{i}^{T}q_{j}))=A_{ i,j}\] (3)

for each element in matrix \(A\). This involves finding \(Z^{n d^{}}\) or \(P,Q^{n d^{}}\) that maximize the number of satisfied constraints. Additionally, for an undirected graph, the symmetry \(A_{i,j}=A_{j,i}\) imposes the requirement that \(((p_{i}^{T}q_{j}))=((p_{j }^{T}q_{i}))\), ensuring that both \(p_{i}^{T}q_{j}\) and \(p_{i}^{T}q_{j}\) should have the same sign.

In the case of cross-correlation, where \(P\) and \(Q\) are independently determined, and all constraints can well align with the generation of \(P\) and \(Q\). However, this is not the case in self-correlation, where \(z_{i}^{T}z_{j}=z_{j}^{T}z_{i}\) inherently overloads the symmetry constraint. For example, if \(A_{i,j}=A_{j,i}=1\), we only require \(p_{i}^{T}q_{j}>0\) and \(p_{j}^{T}q_{i}>0\) for cross-correlation, while they become restrictive as \(z_{i}^{T}z_{j}=z_{j}^{T}z_{i}>0\) in self-correlation. Cross-correlation offers a broader argument space, providing more freedom to find solutions that satisfy the constraints for reconstructing \(A\). By employing gradient method during optimization, this process can be understood as the trajectory of \(P\) and \(Q\) being unrestricted in the argument space \(^{n 2d^{}}\), while the trajectory of \(Z\) is confined within a restricted space as \(^{n d^{}}\). Therefore, cross-correlation facilitates smoother and more efficient convergence during optimization. Note that this restriction comes from the nature of self-correlation and cross-correlation, which is interpreted as the space reduction. During optimization, we can first find (local) optimal \(P\) and \(Q\), then apply \((PQ^{T}+QP^{T})/2\) to interpret it as a symmetric matrix \(\), like \(ZZ^{T}\); this procedure can still outperform direct optimization on \(ZZ^{T}\).

Validation on PROTEINSHere, we make a quick validation of our discussion on a subset of PROTEINS  that is a graph task, in Figure 2. By comparing Figure 2(a) and (b), the trajectory of node embedding during training demonstrates the superiority of cross-correlation over self-correlation. As the node embeddings are evolving under self-correlation, they frequently change their direction; this indicates that the region of attraction between the start and end point is not smooth and may follow a restricted manifold, thus the embedding cannot well converge to the local minimum. On the other hand, the region of attraction under cross-correlation is much smoother and easy to guide the node embedding to a better solution. This can also be evidenced by the lower loss under cross-correlation in Figure 2(c). Another concern of cross-correlation is that \(p_{i}^{T}q_{i}\) cannot naturally satisfy \(A_{i,i}=1\) while \(z_{i}^{T}z_{i}\) in self-correlation is able to. Nevertheless, \(p_{i}^{T}q_{i}\) can be encouraged to perform with positive sign, proved by Figure 2(d), that all the diagonal element reconstruction \(p_{i}^{T}q_{i}\) under cross-correlation can achieve positive sign at the end of training, leading to \(((p_{i}^{T}q_{i}))=A_{i,i}=1\).

## 3 GraphCroc: Cross-Correlation-Based Graph Autoencoder

Encoder ArchitectureOur work scales the normal single-graph representation to multi-graph for graph tasks, and we do not specify the encoder structure, but free it as the downstream tasks require. For graph tasks, the GNN model has a sequence of message passing layer and pooling layer to coarsen the graph to higher-level representation. In the end, a readout layer [8; 9; 31] is applied to summarize the graph representation to the latent space. We define the encoder as

\[\ Z^{}=(Z^{}|G)=f(X,A)\] (4)

where \(Z^{}^{n^{} d^{}}\) has a reduced number \(n^{}\) of node embeddings. Besides, we exclude the readout layer from the encoder, yet assign it to the start of downstream tasks.

Two-Way Decoder for Cross-CorrelationTo separately produce two node embeddings, \(P\) and \(Q\), we divide the decoding process into two parallel and self-governed decoders. Unlike , we leave

Figure 2: Training comparison between self-correlation and cross-correlation on PROTEINS subset (64 graphs). In (a) and (b), we demonstrate the trajectory of the first two node embeddings in the first graph during training iteration, where the star mark is the end-point of training. We apply PCA for dimension compression and the Savitzky-Golay filter to help trace visualization. We also set \(z_{i}=p_{i} q_{i}\) at the beginning of optimization to ensure that the traces of \(z_{i}\) in (a) and \(p_{i}\) in (b) start from the same point. (c) provides the BCE loss trace of this graph during training, showing that cross-correlation can lead the reconstruction to a better solution. (d) demonstrates the distribution of diagonal elements during training, i.e., \(z_{i}^{T}z_{i}\) for self-correlation and \(p_{i}^{T}q_{i}\) for cross-correlation. The results of other graphs are provided in Appendix G.2.

the encoder design to better suit specific downstream tasks, and focus primarily on the reconstruction challenges within the decoder design. Still with the latent vector \(Z=f(X,A)\) generated by the encoder, we define our decoder as follow:

\[\ =(PQ^{T}), P=g_{1}(Z, \{A^{},h^{}\}), Q=g_{2}(Z,\{A^{},h^{}\})\] (5)

\(g_{1}()\) and \(g_{2}()\) are two individual GNNs with the same structure, which take as input the latent vectors \(Z\), the adjacency matrix groups \(\{A^{}\}\) and node feature groups \(\{h^{}\}\) from the encoder (as discussed in Section 3). Here, \(\{A^{},h^{}\}\) is required for graph tasks that involves pooling/unpooling. In Appendix D, we further discuss why the two embeddings, \(P\) and \(Q\), in our decoder do not converge to each other, thereby preventing convergence to self-correlation decoding.

Autoencoder ArchitectureIn Figure 3, we present the autoencoder architecture, GraphCroc. The design of the encoder/decoder pair is inspired by the Graph U-Net structure , originally proposed for classification tasks. While the cross-correlation kernel remains unchanged, the encoder/decoder configuration can be tailored to various applications. Nevertheless, we utilize the Graph U-Net structure as a case study to demonstrate the effectiveness of cross-correlation.

The GraphCroc architecture follows the encoder formulations from Eq. 4 for multiple graphs and the decoder from Eq.5. During the encoding process, the graph architecture \(A^{}\) is captured at each layer, which is then utilized in the corresponding unpooling layers to reconstruct the graph structure, as detailed in . Additionally, skip connections enhance the model by capturing the node features \(h^{}\) at each encoder layer and integrating them -- either through addition or concatenation -- into the node features of the corresponding decoder layer. Importantly, we emphasize the significance of implementing layer normalization  following each GNN layer. Although often overlooked in previous GAE studies due to the typically small number of layers, layer normalization is crucial for regulating gradient propagation as the number of layers increases.

Loss BalancingThe training on GAE is highly biased given the sparsity of the adjacency matrix. In practice, it is quite common that zero elements in \(\) take the majority, e.g., around \(90\%\) in PROTEINS . For a certain \(A\) and its estimation \(\), we denote the vanilla loss function as \(=_{i=1}^{c_{0}}_{i}^{0}+_{j=1}^{c_{1}}_{j}^{1}\), where in total \(c_{0}\) zeros and \(c_{1}\) ones in \(A\), and \(^{0}/^{1}\) is their corresponding loss on each element. Since \(c_{0} c_{1}\), the zero part dominates the loss function, inducing the decoder to predict zeros. Thus, we apply a scaling factor for each item:

\[(,A)=_{0}_{i=1}^{c_{0}}_{i}^{0}+ _{1}_{j=1}^{c_{1}}_{j}^{1},_{0}=+c _{1}}{2c_{0}},_{1}=+c_{1}}{2c_{1}}\] (6)

The derivation is provided in Appendix E.

Figure 3: GraphCroc architecture. The encoder is generally demonstrated as a \(L+1\)-layer GNN. The decoder has two paths to generate the node embedding for cross-correlation; each decoder is a mirrored structure of the encoder. Each decoder layer accepts the node feature and graph structure information from the corresponding encoder layer. Notably, the GCN module shown on the right incorporates skip connections and normalization to improve performance.

## 4 Evaluation

### Experimental Setup

DatasetWe assess GraphCroc in various graph tasks. Specifically, we utilize datasets for molecule, scaling from small (PROTEINS ) to large (Protein-Protein Interactions (PPI) , and QM9 ), for scientific collaboration (COLLAB ), and for movie collaboration (IMDB-Binary ). Further details on these datasets are provided in Appendix F.1.

GraphCroc StructureDuring evaluation, our GraphCroc structure, especially the GCN layer number, is determined by the scale of graph. Assuming the average node number in a graph task is \(\), we set up an empirical layer number \(L\) as \(_{i=1}^{L}(0.9-i)=2\), so that the number of nodes can be smoothly reduced to two at the end of encoding . Besides, we use node embedding dimension \(d^{}\), following analysis in Appendix B. Other detail is provided in Appendix F.2.

### GraphCroc on Structural Reconstruction

Table 1 demonstrates the graph reconstruction capabilities of GAE models for multi-graph tasks by presenting ROC-AUC scores on adjacency matrix reconstruction. We compare our model against prevalent self-correlation kernels in GAEs and a cross-correlation method designed for directed graphs. Comparing the basic inner-product methods, GAE and VGAE , the variational extension in VGAE does not obviously improve the performance over GAE in graph tasks. However, by enhancing the GAE architecture itself, i.e., using our GraphCroc architecture, the reconstruction efficacy is significantly increased; GraphCroc under self-correlation can achieve 3/5 second bests among all GAE models. This underscores the graph representation capability of the U-Net structure . Additionally, the L2-norm decoding method generally outperforms the inner-product approach (GAE and VGAE), although it struggles with large graphs such as PPI, which requires too much GPU memory to be trained during our reproduction. On the other hand, the cross-correlation method (DiGAE) provides a consistent albeit modest representation across different graph sizes. This demonstrates the cross-correlation ability to represent multiple graphs in various scenarios. However, the GNN architecture limits its capability to capture enough structural information. By integrating the strengths of cross-correlation with the U-Net architecture, our GraphCroc GAE model consistently excels over other methods, offering significant advantages in all the graph tasks tested. Even on large graphs, such as PPI with over 2,000 nodes, GraphCroc can still achieve an AUC score over 0.98. To further demonstrate the effectiveness of the cross-correlation mechanism, we evaluate GAE models with alternative architectures under cross-correlation, in Appendix G.1. The reconstruction results for these architectures are consistent with those in Table 1, though they exhibit slightly lower AUC compared to GraphCroc.

While the AUC score indicates how well a model reconstructs edges, it does not measure the model's ability to exactly reconstruct a whole graph. To address this, we employ the Weisfeiler-Leman isomorphism test (WL-test), which assesses the structural equivalence between the original and reconstructed graphs. Figure 4 presents normalized WL-test results, i.e., the pass rates across all test

    &  &  \\   & GAE & VGAE & EGNN & **GraphCroc**(SC) & DIGAE & **GraphCroc** \\  PROTEINS & 0.4750 & 0.4764 & 0.9608 & 0.9781 & 0.7577 & **0.9958** \\ IMDB-B & 0.7556 & 0.7105 & 0.9873 & 0.9892 & 0.7500 & **0.9992** \\ Collab & 0.7886 & 0.7946 & 0.9947 & 0.9926 & 0.7973 & **0.9993** \\ PPI & 0.6330 & 0.6239 & \(-\)1 & 0.9764 & 0.8364 & **0.9831** \\ QM9 & 0.5376 & 0.4852 & 0.9984 & 0.9967 & 0.7791 & **0.9987** \\    \\ 

Table 1: The AUC score of reconstructing the adjacency matrix in graph tasks. We reproduce the most representative global GAE methods with different decoding strategies. The self-correlation methods include naive GAE, variational GAE , L2-norm (EGNN) , and our GraphCroc under self-correlation; the cross-correlation methods include directed representation (DiGAE)  and our GraphCroc. The best results are in **bold**, and the second bests are underlined.

graphs, in the IMDB-B task. Our GraphCroc model significantly outperforms other GAE methods, while the self-correlation variant also delivers superior performance. Interestingly, while the L2-norm achieves a high AUC score, it is feeble to well reconstruct the entire graphs; this indicates there are some connection patterns in graph inherently hard to be captured by L2-norm representation. Other results of WL test are provided in Appendix G.3, which demonstrates similar observations as above.

In Figure 5, we select a representative graph from each of the PROTEINS, IMDB-B, and COLLAB tasks to visually compare the structural reconstruction from GAE models. It is evident that GraphCroc performs well in accurately recovering graph edges. GAE methods with inner-product and vanilla GNN architectures, such as GAE, VGAE, and DiGAE, tend to overpredict edge connections. Meanwhile, EGNN performs adequately within node clusters, but struggles with inter-cluster connections. This echos our discussion about the partial representation deficiency in L2-norm.

### GraphCroc on Other GAE Strategies

To make cross-correlation more pronounced in our evaluation, the above experiments implement only the basic inner-product representation, i.e., \((PQ^{T})\). In addition, previous studies have extensively explored data augmentation and training enhancements to optimize GAE models. Specifically, we examine the performance of GAE with cross-correlation applied to different training strategies in Figure 6, using the PROTEINS dataset as a case study.

We evaluate three other prevalent decoding enhancements to compare their performance with the standard inner-product decoding (baseline) under the cross-correlation method. The variational decoding  generates node embeddings from a Gaussian distribution, with mean/std determined by the decoder outputs. Although a similar final AUC was achieved, it falls short of the baseline on the PROTEINS task at early convergence. For the other two strategies, edge masking  and L2-norm representation , they facilitate faster convergence during the initial training stages. However, we find that the enhancement of these strategies is highly dependent on graph tasks. Our further analysis on other graph tasks (Appendix G.4) demonstrates that the L2-norm and masking could converge to worse structural reconstructions than our baseline training. Therefore, we still advocate our training and representing methods for GAE models on various graph tasks.

Figure 5: The reconstruction visualization. Other reconstructions are provided in Appendix G.5.

Figure 6: The AUC score of testing graphs in PROTEINS task, with different decoding methods.

### GraphCroc on Graph Classification Tasks

One common application of autoencoder models is leveraging their potent encoders for downstream tasks. We evaluate our GraphCroc model by employing its encoder in graph classification tasks, as summarized in Table 2. Notably, generative approaches like GraphMAE, S2GAE, StructMAE, and our GAE model tend to surpass traditional unsupervised and contrastive learning methods. Although contrastive learning incorporates negative sampling, its effectiveness is limited in multi-graph tasks. This finding corroborates the observations in Tab.4 of , which indicate that while negative sampling substantially boosts performance in single-graph tasks (e.g., node classification), it has little impact on graph classification tasks. In contrast, GAE models deliver robust graph representations, particularly for small-to-moderate-sized graphs, enhancing their utility in graph classification. Furthermore, our GraphCroc model significantly outperforms self-correlation methods (GraphMAE and S2GAE) in representing graph structures, demonstrated in Table 1, enabling the encoder to effectively capture the structural information of input graphs. Consequently, classifiers leveraging our encoder can achieve high performance with finetuning over only several epochs. Continued optimization of our classification models promises to further elevate their performance in graph classification tasks, surpassing other GAE-based models.

### GAE Attack Surface Analysis

Research in vision tasks demonstrates that manipulating the latent space with perturbations enables AE to produce adversarial examples with stealthiness and semanticity [6; 16; 32; 40; 43]. Given AE's success in vision domain, we raise the concern -- _whether a GAE can be utilized to generate adversarial graph structures by modifying the latent vectors?_ Current graph adversarial attacks directly modify the input of GNNs, highly inefficient due to the discreteness of graph structures [7; 37]. By directly conducting attacks in the latent space, GAE could be a potentially efficient attack surface.

In Table 3, we demonstrate the effect of small perturbations on the latent space using random noise injection, PGD , and C&W adversarial noise injection  on graph classification tasks. We limit all attacks on the latent space with a maximum query number of \(400\) and report the classification accuracy of perturbed graphs and the number of edge changes. Note that we focus solely on the structure modification without changes on the node features. Our observations indicate that conducting adversarial attacks on the latent space of the graph autoencoder effectively reduces model accuracy, although it could yield significant edge changes. Compared to adversarial attacks on graph structures using discrete optimization methods, our latent attacks demonstrate comparable performance in terms of accuracy and can be performed in batches with high efficiency. Nevertheless, due to the discrete nature of graph structures, the distortion in edge changes is hard to be always controlled at a low level. Our evaluation of GraphCroc's latent space reveals a potential vulnerability, indicating that adversarial attacks on a graph autoencoder's latent space can provide efficient structural adversarial attacks. More detail on the adversarial attack with GAE is discussed in Appendix F.3.

    & Infograph & GraphCL & InfoGCL & GraphMAE & S2GAE & StructMAE &  **ours** \\ (10-epoch) \\  & 
 **ours** \\ (100-epoch) \\  \\  PROTEINS & 74.44 & 74.39 & – & 75.30 & 76.37 & 75.97 & 73.99\(^{1.32}\) & **79.09\(^{1.63}\)** \\ IMDB-B & 73.03 & 71.14 & 75.10 & 75.52 & 75.76 & 75.52 & 76.69\({}^{1.02}\) & **78.75\({}^{1.35}\)** \\ COLLAB & 70.65 & 71.36 & 80.00 & 80.32 & 81.02 & 80.53 & 81.70\({}^{2.54}\) & **82.40\({}^{10.20}\)** \\   

Table 2: Evaluation on graph classification tasks. We compare our model with other GNN methods, such as unsupervised learning (Infograph ), contrastive learning (GraphCL  and InfoGCL ), and generative learning (GraphMAE , S2GAE  and StructMAE ). The encoder of our model is extracted from GraphCroc, and is cascaded with a randomly-initialized 3-layer classifier. We train our models by only fine-tuning for 10 epochs or training for 100 epochs. The best results are in **bold**, and the second bests are underlined. Results are averaged on 5 runs.

    & Clean & Random & **PGD** & & **C\&W** \\   & & Acc. & \(\)edge & Acc. & \(\)edge & Acc. & \(\)edge \\  IMDB-M & 55.3 & 53.5 & 4.79\% & 45.7 & 24.5\% & 39.7 & 17.4\% \\ PROTEINS & 82.5 & 77.1 & 5.01\% & 57.4 & 5.63\% & 41.7 & 23.7\% \\ COLLAB & 81.3 & 70.0 & 5.90\% & 28.8 & 35.9\% & 27.3 & 8.29\% \\   

Table 3: Accuracy and modified edges for adversarial attack, leveraging latent perturbation on GAE. A lower accuracy indicates that the latent perturbation can better pass to the reconstructed graph. A higher percentage of modified edges indicates a larger attack cost.

Conclusion

Graph autoencoders (GAEs) are increasingly effective in representing graph structures. In our research, we identify significant limitations in the self-correlation approach employed in the decoding processes of prevalent GAE models. Self-correlation inadequately represents certain graph structures and requires optimization within a constrained space. To address these deficiencies, we advocate cross-correlation as the decoding kernel. We propose a novel GAE model, GraphCroc, which incorporates cross-correlation decoding and is built upon a U-Net architecture, enhancing the flexibility in GNN design. Our evaluations demonstrate that GraphCroc outperforms existing GAE methods in terms of graph structural reconstruction and downstream tasks. In addition, we propose the concern that well-performed GAEs could be a surface for adversarial attacks.