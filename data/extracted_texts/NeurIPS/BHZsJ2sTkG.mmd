# An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits

Kiarash Banihashem

University of Maryland, College Park

kiarash@umd.edu

&MohammadTaghi Hajiaghayi

University of Maryland, College Park

hajiagha@umd.edu

&Suho Shin

University of Maryland, College Park

suhoshin@umd.edu

&Max Springer

University of Maryland, College Park

mss423@umd.edu

###### Abstract

We present an oracle-efficient relaxation for the adversarial contextual bandits problem, where the contexts are sequentially drawn i.i.d from a known distribution and the cost sequence is chosen by an online adversary. Our algorithm has a regret bound of \(O(T^{}(K(||))^{})\) and makes at most \(O(K)\) calls per round to an offline optimization oracle, where \(K\) denotes the number of actions, \(T\) denotes the number of rounds and \(\) denotes the set of policies. This is the first result to improve the prior best bound of \(O((TK)^{}((||))^{})\) as obtained by Syrgkanis et al. at NeurIPS 2016, and the first to match the original bound of Langford and Zhang at NeurIPS 2007 which was obtained for the stochastic case.

## 1 Introduction

One of the most important problems in the study of online learning algorithms is the _contextual bandits problem_. As a framework for studying decision making in the presence of side information, the problem generalizes the classical _multi-armed bandits problem_ and has numerous practical applications spanning across clinical research, personalized medical care and online advertising, with substantial emphasis placed on modern recommender systems.

In the classical multi-armed bandits problem, a decision maker is presented with \(K\) actions (or arms) which it needs to choose from over a sequence of \(T\) rounds. In each round, the decision maker makes its (possibly random) choice and observes the cost of its chosen action. Depending on the setting, this cost is generally assumed to be either _stochastic_ or _adversarial_. In the stochastic setting, the cost of each action is sampled i.i.d. from a fixed, but a priori unknown, distribution. In the more general adversarial setting, no such assumption is made and the costs in each round can be controlled by an online adversary. The goal of the learner is to minimize its _regret_, defined as the absolute difference between its total cost and the total cost of the best fixed action in hindsight.

The contextual bandits problem generalizes this by assuming that in each round, the learner first sees some side information, referred to as a _context_, \(x\) and chooses its action based on this information. As in prior work (Rakhlin and Sridharan, 2016; Syrgkanis et al., 2016), we assume that the \(x\) are sampled i.i.d. from a fixed distribution \(\), and that the learner can generate samples from \(\) as needed. In addition, the learner has access to a set of _policies_, \(\), where a policy is defined as a mapping from contexts to actions. As before, the goal of the learner is to minimize its regret, which we here define as the absolute difference between its total cost and the total cost of the best policy of \(\) in hindsight.

It is well-known that by viewing each policy as an expert, the problem can be reduced to the _bandits with experts_ problem where, even in the adversarial setting, the Exp4 (Auer et al., 1995) algorithm achieves the optimal regret bound of \(O()\). Computationally however, the reduction is inefficient as the algorithm's running time would be linear in \(||\). Since the size of the policy set, \(\), can be very large (potentially exponential), the utility of this algorithm is restricted in practice.

Given the computational challenge, there has been a surge of interest in _oracle-efficient_ algorithms. In this setting, the learner is given access to an Empirical Risk Minimization (ERM) optimization oracle which, for any sequence of pairs of contexts and loss vectors, returns the best fixed policy in \(\). This effectively reduces the online problem to an offline learning problem, where many algorithms (e.g., SVM) are known to work well both in theory and practice. This approach was initiated by the seminal work of Langford and Zhang (2007), who obtained a regret rate of \(O(T^{2/3}(K||)^{1/3})\) for _stochastic rewards_, using an \(\)-greedy algorithm. This was later improved to the information-theoretically optimal \(O()\)(Dudik et al., 2011). Subsequent works have focused on improving the running time of these algorithms (Beygelzimer et al., 2011; Agarwal et al., 2014; Simchi-Levi and Xu, 2022), and extending them to a variety of problem settings such as auction design (Dudik et al., 2020), minimizing dynamic regret (Luo et al., 2018; Chen et al., 2019), bandits with knapsacks (Agrawal et al., 2016), semi-bandits (Krishnamurthy et al., 2016), corralling bandits (Agarwal et al., 2017), smoothed analysis (Haghtalab et al., 2022; Block et al., 2022), and reinforcement learning (Foster et al., 2021).

Despite the extensive body of work, progress for _adversarial rewards_ has been slow. Intuitively, approaches for the stochastic setting do not generalize to adversarial rewards because they try to "learn the environment" and in the adversarial setting, there is no environment to be learnt. This issue can be seen in the original multi-armed bandits problem as well. While the regret bounds for the adversarial setting and the stochastic setting are the same, the standard approaches are very different. 1 In the stochastic setting, the most standard approach is the UCB1 algorithm (Auer et al., 2002), which is intuitive and has a relatively simple analysis. In the adversarial setting however, the standard approach is considerably more complex: the problem is first solved in the "full feedback" setting, where the learner observes all of the rewards in each iteration, using the Hedge algorithm. This is then used as a black box to obtain an algorithm for the partial (or bandit) feedback setting by constructing unbiased estimates for the rewards vector. The analysis is also much more involved compared to UCB1; a standard analysis of the black box reduction leads to the suboptimal bound of \(T^{3/4}K^{1/2}\), and further obtaining the optimal \(\) bound requires a more refined second moment analysis (see Chapter 6 of Slivkins (2019) for a more detailed overview).

As a result, oracle-efficient algorithms for the adversarial setting were first proposed by the independent works of Rakhlin and Sridharan (2016) and Syrgkanis et al. (2016a) in ICML 2016, who obtained a regret bound of \(O(T^{3/4}K^{1/2}(||)^{1/4})\) and left obtaining improvements as an open problem. This was subsequently improved to \(O((TK)^{2/3}(||)^{1/3})\) by Syrgkanis et al. (2016b) at NeurIPS 2016.

### Our contribution and techniques

In this work, we design an oracle-efficient algorithm with a regret bound of \(O(T^{2/3}(K||)^{1/3})\). Our result is the first improvement after that of Syrgkanis et al. (2016b), and maintains the best regret upper bound to the extent of our knowledge. This is also the first result to match the bound of Langford and Zhang (2007), the original baseline algorithm for the stochastic version of the problem. We state the informal version of our main result (Theorem 7) in the following.

**Theorem 1** (Informal).: _For large enough \(T\),2 there exists an algorithm (Algorithm 1) that achieves expected regret on the order of \(O(T^{2/3}(K||)^{1/3})\) for the adversarial contextual bandits problem using at most \(O(K)\) calls per round to an ERM oracle._

In order to compare this result with prior work, it is useful to consider the regime of \(K=T^{}\) for a constant \(>0\). In this regime, our work leads to a sublinear regret bound for any \(<1\), while prior work (Rakhlin and Sridharan, 2016; Syrgkanis et al., 2016) can only obtain sublinear regret for \(<1/2\).

Our improved dependence on \(K\) is important practically since, for many real-world implementations such as recommender systems, the number of possible actions is very large. Additionally, many bandits algorithms consider "fake" actions as part of a reduction. For example, a large number of actions may be considered as part of a discretization scheme for simulating a continuous action space (Slivkins, 2009). In such cases, the improved dependence on \(K\) could potentially imply an overall improvement with respect to \(T\), as the parameters in the algorithm are often chosen in such a way that optimizes their trade-off.

From a technical standpoint, our result builds on the existing works based on the relax-and-randomize framework of Rakhlin et al. (2012). Rakhlin and Sridharan (2015) used this framework, together with the "random playout" method, to study online prediction problems with evolving constraints under the assumption of a full feedback model. Rakhlin and Sridharan (2016) extended these techniques to the partial (bandit) feedback model, and developed the BISTRO algorithm which achieves a \(O(T^{3/4}K^{1/2}(||)^{1/4})\) regret bound for the adversarial bandits problem. Subsequently, Syrgkanis et al. (2016) used a novel distribution of hallucinated rewards as well as a sharper second moment analysis to obtain a regret bound of \(O((TK)^{2/3}(||)^{1/3})\). We further improve the regret rate to \(O(T^{2/3}(K(||))^{1/3})\) by reducing the support of the hallucinated rewards vector to a single random entry. We note the previous approaches of Rakhlin and Sridharan (2016); Syrgkanis et al. (2016), as well as the closely related work of Rakhlin and Sridharan (2015), set all of the entries of the hallucinated cost to i.i.d Rademacher random variables.

We show that our novel relaxation preserves the main properties required for obtaining a regret bound, specifically, it is _admissible_. We then prove that the Rademacher averages term that arises from our new relaxation improves by a factor of \(K\), which consequently leads to a better regret bound. We further refer to Section 3 for a more detailed description of our algorithm, and to Section 4 for the careful analysis.

### Related work

**Contextual bandits.** There are three prominent problem setups broadly studied in the contextual bandits literature: Lipschitz contextual bandits, linear contextual bandits, and contextual bandits with policy class. Lipschitz contextual bandits (Lu et al., 2009; Cutkosky and Boahen, 2017) and linear contextual bandits (Chu et al., 2011) assume a structured payoff based on a Lipschitz or linear realizability assumption, respectively. The strong structural assumptions made by these works however make them impractical for many settings.

To circumvent this problem, many works consider contextual bandits with policy classes where the problem is made tractable by making assumptions on the benchmark of regret. In these works, the learner is given access to a policy class \(\) and competes against the best policy in \(\). This approach also draws connections to offline machine learning models that in recent years have had a huge impact on many applications. In order for an algorithm to be useful in practice however, it needs to be computationally tractable, thus motivating the main focus of this work.

**Online learning with adversarial rewards.** Closely related to our work is the online learning with experts problem where, in each round, the learner observes a set of \(N\) experts making recommendations for which action to take, and decides which action to choose based on these recommendations. The goal of the learner is to minimize its regret with respect to the best expert in hindsight. In the full feedback setting, where the learner observes the cost of all actions, the well-known Hedge (Cesa-Bianchi et al., 1997) algorithm based on a randomized weighted majority selection rule achieves the best possible regret bound of \(O()\). Correspondingly, in the partial feedback setting, Exp4 (Auer et al., 1995) exploits Hedge by constructing unbiased "hallucinated" costs based on the inverse propensity score technique, and achieves the optimal regret bound of \(O()\). By considering an expert for each policy, the contextual bandits problem can be reduced to this problem. This reduction suffers from computational intractability however due to the linear dependence on \(||\) in the running time. Since the number of policies can be very large in practice, this poses a major bottleneck in many cases. We alleviate this intractability issue through a computationally feasible oracle-based algorithm with improved regret bound.

**Oracle efficient online learning.** Stemming from the seminal work of Kalai and Vempala (2005), there has been a long line of work investigating the computational barriers and benefits of online learning in a variety of paradigms. Broadly speaking, the bulk of online algorithms are designed on the basis of two popular frameworks in this literature: follow-the-perturbed-leader (Kalai and Vempala, 2005; Suggala and Netrapalli, 2020; Dudik et al., 2020; Haghtalab et al., 2022) and relax-and-randomize (Rakhlin et al., 2012). Both frameworks aim to inject random noise into the input set before calling an oracle to construct a more robust sequence of actions to be played against an adversary, but differ in how they introduce such noise to the system. Our algorithm builds on the relax-and-randomize technique and improves upon the previous best result of Syrgkanis et al. (2016).

Despite their computational advantages, it is known that oracle efficient algorithms have fundamental limits and, in some settings, they may not achieve optimal regret rates (Hazan and Koren, 2016). Whether this is the case for the adversarial contextual bandits problem remains an open problem.

## 2 Preliminaries

In this section, we explain the notation and problem setup, and review the notion of relaxation based algorithms in accordance with prior work (Rakhlin and Sridharan, 2016; Syrgkanis et al., 2016).

### Notation and problem setup

Given an integer \(K\), we use \([K]\) to denote the set \(\{\,1,,K\,\}\) and \(a_{1:K}\) to denote \(\{\,a_{1},,a_{k}\,\}\). We similarly use \((a,b,c)_{1:K}\) to denote the set of tuples \(\{\,(a_{1},b_{1},c_{1}),,(a_{K},b_{K},c_{K})\,\}\). The vector of zeros is denoted as \(\), and similarly, the vector of ones is denoted \(\).

We consider the contextual bandits problem with \([T]\) rounds. In each round \(t[T]\), a context \(x_{t}\) is shown to the learner, who chooses an action \(_{t}[K]\), and incurs a loss of \(c_{t}(_{t})\), where \(c_{t}^{k}\) denotes the cost vector. The choice of the action \(_{t}\) can be randomized and we assume that the learner samples \(_{t}\) from some distribution \(q_{t}\). The cost vector is chosen by an adversary who knows the cost vector \(x_{t}\) and the distribution \(q_{t}\) but, crucially, does not know the value of \(_{t}\).

As in prior work (Rakhlin and Sridharan, 2016; Syrgkanis et al., 2016), we operate in the _hybrid i.i.d-adversarial_ model where \(x_{t}\) is sampled from some fixed distribution \(\), and the learner has sampling access to the distribution \(\). We additionally assume that the feedback to the learner is partial, i.e., the learner only observes \(c_{t}(_{t})\) and not the full cost vector \(c_{t}\).

The learner's goal is to minimize its total cost compared to a set of policies \(\), where a policy is defined as a mapping from contexts to actions. Formally, the learner aims to minimize its _regret_, which we define as

\[:=_{t=1}^{T} q_{t},c_{t}-_{ }_{t=1}^{T}c_{t}((x_{t})),\]

where \( q_{t},c_{t}\) denotes the dot product of \(q_{t}\) and \(c_{t}\), and \(\) denotes the infimum.

We assume that the learner has access to a _value-of-ERM_ optimization oracle that takes as input a sequence of contexts and cost vectors \((x,c)_{1:t}\), and outputs the minimum cost obtainable by a policy in \(\), i.e., \(_{}_{=1}^{t}c_{}((x_{}))\).

### Relaxation Based Algorithms

In each round \(t[T]\) after selecting an action and observing the adversarial cost, the learner obtains an _information tuple_, which we denote by \(I_{t}(x_{t},q_{t},_{t},c_{t}(_{t}),S_{t})\). Here, \( q_{t}\) is the action chosen from the learner's distribution, and \(S_{t}\) is the internal randomness of our algorithm, which can also be used in the subsequent rounds.

Given the above definition, the notions of _admissible relaxation_ and _admissible strategy_ are defined as follows.

**Definition 2**.: _A partial information relaxation \(()\) is a mapping from the information sequence \((I_{1},...,I_{t})\) to a real value for any \(t[T]\). Moreover, a partial-information relaxation is deemedadmissible if for any such \(t\), and for all \(I_{1},...,I_{t-1}\):_

\[_{x_{t} D}[_{q_{t}}_{c_{t}}_{_{t} q _{t},S_{t}}[c_{t}(_{t})+(I_{1:t})]] (I_{1:t-1}),\] (1)

_and for all \(x_{1:T},c_{1:T}\) and \(q_{1:T}\):_

\[_{_{1:T} q_{t},S_{1:T}}[(I_{1:T})] -_{}_{t=1}^{T}c_{t}((x_{t})).\] (2)

_A randomized strategy \(q_{1:T}\) is admissible if it certifies the admissibility conditions (1) and (2)._

Intuitively, relaxation functions allow us to decompose the regret across time steps, and bound each step separately using Equation (1). The following lemma formalizes this idea.

**Lemma 3** (Rakhlin and Sridharan (2016)).: _Let Rel be an admissible relaxation and \(q_{1:T}\) be a corresponding admissible strategy. Then, for any \(c_{1:T}\), we have the bound_

\[[]().\]

## 3 Contextual Bandits Algorithm

We here define an admissible strategy in correspondence with the relaxation notion from the prior section, and use it to outline our contextual bandits algorithm. As mentioned in Section 1.1, our algorithm is based on the BISTRO+ algorithm of Syrgkanis et al. (2016), and our improvement is obtained by defining a new relaxation function, which we discuss below. We discuss how this improves the regret bound in Section 4.

**Unbiased cost vectors.** In order to handle the partial feedback nature of the problem, we use the standard technique of forming an unbiased cost vector from the observed entry, together with the discretization scheme of Syrgkanis et al. (2016). Let \(<1\) be a parameter to be specified later. Using the information \(I_{t}\) collected on round \(t\), we set our estimator to be the random vector whose elements are defined by a Bernoulli random variable

\[_{t}(i)=\{K^{-1}[i= _{t}]&(_{t})}{Kq_{t}(_{t})} \\ 0&..\] (3)

We note that this is only defined for \(_{i}q_{t}(i)/K\), thus imposing a constraint that must be ensured by our algorithm. It is easy to verify that this vector is indeed an unbiased estimator:

\[_{_{t} q_{t}}[_{t}(i)]=q_{t}(i) (i)}{Kq_{t}(i)} K^{-1}=c_{t}(i).\]

**Relaxation function.** We first construct a one-hot Rademacher random vector by randomly sampling an action \(i[K]\) and setting \(_{t}(j)=0\) for \(i j\) and \(_{t}(i)\) to a Rademacher random variable in \(\{-1,1\}\). We additionally define \(Z_{t}\{0,K^{-1}\}\) that takes value \(K^{-1}\) with probability \(\) and 0 otherwise. Using the notation \(_{t}\) for the random variable tuple \((x,,Z)_{t+1:T}\), we define our relaxation Rel as

\[(I_{1:t})=_{_{t}}[R((x,_{t})_{1:t},_ {t})],\] (4)

where \(R((x,_{t})_{1:t},_{t})\) is defined to be

\[(T-t)-_{}(_{=1}^{t}((x_{}))+_{ =t+1}^{T}2Z_{}_{}((x_{}))).\]

We note the contrast between the above definition and the relaxation frameworks used in prior work (Rakhlin and Sridharan, 2015, 2016; Syrgkanis et al., 2016): These works all set every entry in \(_{t}\) to a Rademacher random variables, while we set only a single (randomly chosen) entry to a Rademacher random variable and set the rest of the entries to zero.

The changes in the relaxation function are motivated by the algorithm analysis (see Section 4). Specifically, in order to ensure admissibility, the symmetrization step of the Relax and randomize framework applies only to a single (random) action. Applying noise to all the entries, as is done in prior work, leads to valid upper bound but is not tight. As we show in Lemma 9, applying noise to a single entry is sufficient, as long as this entry is chosen uniformly at random. The reduced noise leads to an improved Rademacher averages term (see Theorem 6), which in turn leads to a better regret bound.

**Randomized strategy.** As in prior work (Rakhlin and Sridharan, 2015, 2016; Syrgkanis et al., 2016b), we use the "random playout" technique to define our strategy. We use hallucinated future cost vectors, together with unbiased estimates of the past cost, to choose a strategy that minimizes the total cost across \(T\) rounds.

Define \(D:=\{K^{-1}}:i[K]\}\{\}\), where \(}\) is the \(i\)-th standard basis vector in \(K\) dimensions. We further define \(_{D}\), the set of distributions over \(D\), and \(_{D}^{}_{D}\) to be the set

\[\{p_{D}:_{i[K]}p(i)\}.\] (5)

Recall that \(_{t}\) denotes the random variable tuple \((x,,Z)_{t+1:T}\). We sample \(_{t}\) and define \(q_{t}^{*}(_{t})\) as:

\[q_{t}^{*}(_{t}):=_{q_{K}}_{p_{t}_{D}^{}} _{_{t} p_{t}}[ q,_{t}+R((x, )_{1:t},_{t})].\] (6)

We than sample the action \(_{t}\) from the distribution \(q_{t}(_{t})\) defined as

\[q_{t}(_{t}):=(1-)q_{t}^{*}(_{t})+.\] (7)

In order to calculate \(q_{t}(_{t})\), we use a water-filling argument similar to Rakhlin and Sridharan (2016) and Syrgkanis et al. (2016b). Formally, we will use the following lemma, the proof of which is in Appendix B.

**Lemma 4**.: _There exists a water-filling algorithm that computes the value \(q_{t}^{*}(_{t})\) for any given \(_{t}\) in time \(O(K)\) with only \(K+1\) accesses to a value-of-ERM oracle in every round._

A full pseudocode of our approach is provided in Algorithm 1.

``` for\(t=1,2,,T\)do  Observe context \(x_{t}\)  Draw random variable tuple \(_{t}=(x,,Z)_{t+1:T}\)  Compute \(q_{t}(_{t})\) via Equation 7  Draw action \(_{t} q_{t}(_{t})\) and observe \(c_{t}(_{t})\)  Estimate cost vector \(_{t}\) via Equation 3 end for ```

**Algorithm 1**Contextual Bandits Algorithm

## 4 Analysis

In this section, we provide the formal statement of our theoretical guarantees and discuss their proofs. Due to space constraints, some of the proofs are deferred to the supplementary material.

As mentioned in the introduction, our main novelty is the use of a new relaxation function, which we discussed in Section 3, that uses less variance in the hallucinated cost vectors. Our initial result verifies that the our novel relaxation is indeed admissible and, as a result, we can leverage the prior work demonstrating the expected regret of these algorithms.

**Theorem 5**.: _The relaxation function defined in (4), and the corresponding strategy (7) are admissible (Definition 2)._

Theorem 5 contrasts with existing admissible relaxations in that it only uses a single Rademacher variable for each time step, while prior work - Lemma 2 in Rakhlin and Sridharan (2015), Theorem 2 in Rakhlin and Sridharan (2016) and Theorem 3 in Syrgkanis et al. (2016b) - all use \(k\) independent Rademacher variables. To our knowledge, this is the first work in which the number of Rademacher variables used in the relaxation does not grow with the number of arms. As we discuss below, this allows us to reduce the variance of the hallucinated costs, leading to a better regret bound. The proof of Theorem 5 is provided in Section 4.1, and is based on a novel symmetrization step (Lemma 9), which may be of independent interest.

As highlighted in Section 2.2, admissible relaxations are a powerful framework for upper bounding the expected regret in online learning through Lemma 3 and the value of \(()\). Formally, Lemma 3 implies

\[[]()=  T+_{_{0}}[_{}(_{=1}^{T}2Z_ {}_{}((x_{})))].\] (8)

In order to bound the regret of our algorithm, it suffices to bound the Rademacher averages term above, which we formally do in the following Theorem.

**Theorem 6**.: _For any \(>(||)/2\), the following holds:_

\[_{(Z,)_{1:T}}[_{}_{i= 1}^{T}Z_{t}_{t}((x_{t}))] 2}.\]

The above theorem can be thought of as an improved version of Lemma 2 from Syrgkanis et al. (2016b), where we improve by a factor of \(K\). Our improvement comes from the use of the new Rademacher vectors that only contain a single non-zero coordinate, together with a more refined analysis. We refer to Section 4.2 for a formal proof of the result.

Combining Lemma 4, Equation (8), and Theorem 6, we obtain the main result of our paper which we state here.

**Theorem 7**.: _The contextual bandits algorithm implemented in Algorithm 1 has expected regret upper bounded by_

\[4}+ T,\]

_for any \(< 1\), which implies the regret order of \(O((K||)^{1/3}T^{2/3})\) when \(T>4K(||)\). Furthermore, the Algorithm makes at most \(K+1\) calls to a value-of-ERM oracle in each round._

We refer to Appendix A for the proof of this result.

### Proof of Theorem 5

In order to prove Theorem 5, we need to verify the final step condition (2), and show that the \(q_{t}\) defined in Equation (7) certifies the condition (1), i.e.,

\[_{x_{t}}[_{c_{t}}_{_{t},S_{ t}}[c_{t}(_{t})+(I_{1:t})]](I_{1:t-1}),\] (9)

where \(_{t}\) is sampled from \(q_{t}\) and \(I_{1:t}\) denotes \((I_{1:t-1},I_{t}(x_{t},q_{t},_{t},c_{t},S_{t}))\). Verifying condition (2) is standard and we do this in Appendix D. It remains to prove Equation (9). Since most admissibility proofs in the literature (Rakhlin et al., 2012; Rakhlin and Sridharan, 2015, 2016; Syrgkanis et al., 2016b) follow the framework of the original derivation of Rakhlin et al. (2012), in order to emphasize our novelty, we divide the proof into two parts. The first part (Lemma 8) is based on existing techniques (in particular, the proof of Theorem 3 in Syrgkanis et al. (2016b)) and its proof is provided in Appendix C. The second part (Lemma 9) uses new techniques and we present its proof here.

**Lemma 8**.: _For any \(t[T]\), define \(A_{,t}\) and \(C_{t}\) as_

\[A_{,t}:=-_{=1}^{t-1}_{}((x_{}))- _{=t+1}^{T}2Z_{}_{}((x_{})), C_{t}:= (T-t+1).\]

_Letting \(\) denote a Rademacher random variable independent of \(_{t}\) and \(_{t}\), the following holds for any value of \(x_{t}\):_

\[_{c_{t}}_{_{t},S_{t}}[c_{t}(_{t} )+(I_{1:t})]_{_{t}}[_{p_{t} ^{}_{D}}_{_{t} p_{t},}[_{ }(2_{t}((x_{t}))+A_{,t})]]+C_ {t}.\]

**Lemma 9**.: _Defining \(A_{,t}\) as in Lemma 8, the following bound holds for any \(t[T]\):_

\[_{p_{t}_{D}^{}}_{_{t} p_{t},}[ _{}(2_{t}((x_{t}))+A_{,t})] _{_{t},Z_{t}}[_{}(2Z_{t} _{t}((x_{t}))+A_{,t})].\] (10)

Combining the above lemmas we obtain Equation (9) by definition of Rel:

\[_{x_{t}}[_{c_{t}}_{_{t},S_{t} }[c_{t}(_{t})+(I_{1:t})]] _{x_{t},p_{t}}[_{p_{t}_{D}^{ }}_{_{t},}[_{}(2 _{t}((x_{t}))+A_{,t})]]+C_{t}\] \[_{x_{t},_{t},Z_{t},p_{t}}[_{ }(2Z_{t}_{t}((x_{t}))+A_{,t})] +C_{t}\] \[_{p_{t-1}}[_{}(2Z_{t} _{t}((x_{t}))+A_{,t})]+C_{t}\] \[(I_{1:t-1}).\]

Proof of Lemma 9.: For any distribution \(p_{t}_{D}^{}\), the distribution of the each coordinate of \(_{t}\) has support on \(\{0,^{-1}K\}\) and is equal to \(^{-1}K\) with probability at most \(/K\). Using \(p_{t}(i)\) to denote \([_{t}(i)=^{-1}K_{i}]\) we can rewrite the LHS (left hand side) of Equation (10) as

\[_{p_{t}_{D}^{}}_{_{t} p_{ t},}[_{}(2_{t}((x_{t}))+A_{,t} )]\] \[=_{p_{t}_{D}^{}}((1-_{i}p_{t}(i ))_{}A_{,t}+_{i}p_{t}(i)_{}[_{ }A_{,t}+[(x_{t})=i]])\] \[=_{0 p_{t}(i)/K}((1-_{i}p_{t}(i ))_{}A_{,t}+_{i}p_{t}(i)_{}[_{ }A_{,t}+[(x_{t})=i]] ),\]

where the first equality follows from expanding the expectation with respect to \(_{t}\), and the second equality follows from the definition of \(_{D}^{}\). We argue that this value is maximized when each \(p_{t}(i)\) takes on its maximum value, i.e., \(/K\). It suffices to observe that

\[_{}[_{}A_{,t}+[(x_{t})=i]]_{}(A_{,t}+_{ }[[(x_{t})=i]] )=_{}A_{,t},\]

where the inequality follows from the fact that supremum of expectation is less than expectation of supremum, and the equality uses the fact that \([]=0\). Therefore, we maximize the LHS of Equation (10) via selecting \(p_{t}\) that satisfies \(p_{t}(i)=\) for \(i 1\). It follows that

\[_{p_{t}_{D}^{}}_{_{t} p _{t},}[_{}(2_{t}((x_{t}))+A_{, t})]\] \[((1-)_{}A_{,t}+_{i} _{}[_{}A_{,t}+[(x_{t})=i]])\] \[=_{_{t},Z_{t}}[_{}(2Z_ {t}_{t}((x_{t}))+A_{,t})],\]

finishing the proof.

### Proof of Theorem 6

We start with the following standard inequalities for handling the supremum using the moment generating function.

\[_{(Z,)_{1:T}}[_{}_{i=1}^{ T}Z_{t}_{t}((x_{t}))] =_{Z_{1:T}}[_{ _{1:T}}[(_{}e^{_{t=1}^{T}Z_{t }_{t}((x_{t}))})]]\] \[_{Z_{1:T}}[_{ _{1:T}}[(_{}e^{_{t=1}^{T}Z_{t }_{t}((x_{t}))})]]\] \[}{{}}_{Z_{1:T}}[ (_{_{1:T}}[_{ }e^{_{t=1}^{T}Z_{t}_{t}((x_{t}))}] )]\] \[}{{=}}_{Z_{1:T}}[ (_{}_{t=1}^{T}_{ _{t}}[e^{ Z_{t}_{t}((x_{t}))}] )].\]

Inequality \((i)\) holds due to the concavity of log and \((ii)\) follows from the independence of \(_{t}\). We will additionally assume that \(\) is upper bounded by \(/K\) in the remaining analysis.

By our construction of the random variable \(_{t}\), for any fixed \(\), \(_{t}((x_{t}))\) takes the value \(0\) with probability \(1-\) and the values \(-1\) and \(1\) each with probability \(\). We therefore have that

\[_{_{t}}[e^{ Z_{t}_{t} ((x_{t}))}] =(\,1-\,)+ e^{ Z_{t} }+ e^{- Z_{t}}\] \[ 1-+ e^{^{2}Z_{t}^{2}/2}\] \[ e^{(e^{^{2}Z_{t}^{2}/2}-1)}.\]

The first inequality above uses \(e^{x}+e^{-x} 2e^{x^{2}/2}\) while the second inequality uses \(e^{x} 1+x\) for \(x\). This further yields

\[_{Z_{1:T}}[(_{ }_{t=1}^{T}_{_{t}}[e^{ Z_{t} _{t}((x_{t}))}])] _{Z_{1:T}}[(| |_{t=1}^{T}e^{Z_{t}^{2}/2}-1}{K}}\,)]\] \[=_{Z_{1:T}}[(||)+_{t=1 }^{T}Z_{t}^{2}/2}-1}{ K}]\] \[=+_{t=1}^{T} _{Z_{1:T}}[e^{^{2}Z_{t}^{2}/2}-1].\]

Recall that \(Z_{t}\) takes the values \(0\) and \(\) with probabilities \(1-\) and \(\) respectively. It follows that

\[_{Z_{1:T}}[e^{^{2}Z_{t}^{2}/2}-1]= (\,e^{K^{2}}{2^{2}}}-1\,) K^{2}}{^{2}},\]

where the inequality follows from the fact that \(e^{x}-1 2x\) for \(x(0,1)\) and the assumption \(/K\). Therefore,

\[_{Z_{1:T}}[(_{} _{t=1}^{T}_{_{t}}[e^{ Z_{t}_{ t}((x_{t}))}])]+.\]

By taking derivative with respect to \(\) we obtain

\[}+TK/=0,\]and compute that the equation above is minimized at \(=}\). We note that \(\) satisfies the assumption \(/K\) because this is equivalent to \(<\), which holds by the assumption of the lemma. Plugging this again yields

\[(||)}+}=2,\]

which is the desired bound.

## 5 Conclusion

In this paper, we presented a novel efficient relaxation for the adversarial contextual bandits problem and proved that its regret is upper bounded by \(O(T^{2/3}(K||)^{1/3})\). This provides a marked improvement with respect to the parameter \(K\) as compared to the prior best result and matches the original baseline of Langford and Zhang (2007) for the stochastic version of the problem. As mentioned earlier, non-efficient algorithms can obtain a regret bound of \(O()\), which is information theoretically optimal. While oracle-efficient algorithms can obtain the optimal regret bound in the stochastic setting (Dudik et al., 2011), they do not always achieve optimal regret rates (Hazan and Koren, 2016). Whether or not optimal regret can be obtained in our setting using efficient algorithms remains an open problem and improving both the upper and lower bounds are interesting directions for future work. Additionally, while our work operates in the same setting as prior work (Rakhlin and Sridharan, 2016; Syrgkanis et al., 2016), it would be interesting to relax some of the assumptions in the setting, most notably the sampling access to the context distribution.

## 6 Acknowledgements

This work is partially supported by DARPA QuICC NSF AF:Small #2218678, and NSF AF:Small #2114269. We thank Alex Slivkins for pointing us to the problem and initial fruitful discussions.