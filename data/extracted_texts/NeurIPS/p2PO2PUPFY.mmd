# Di\({}^{2}\)Pose: Discrete Diffusion Model

for Occluded 3D Human Pose Estimation

Weiquan Wang\({}^{1}\), Jun Xiao\({}^{1}\), Chunping Wang\({}^{2}\), Wei Liu\({}^{3}\), Zhao Wang\({}^{1}\), Long Chen\({}^{4}\)

\({}^{1}\)Zhejiang University \({}^{2}\)Finvolution Group \({}^{3}\)Tencent

\({}^{4}\)Hong Kong University of Science and Technology

{wqwangcs, junx}@zju.edu.cn, wangchunping02@xinye.com, wl2223@columbia.edu, zhao_wang@zju.edu.cn, longchen@ust.hk

Long Chen is the corresponding author.

###### Abstract

Diffusion models have demonstrated their effectiveness in addressing the inherent uncertainty and indeterminacy in monocular 3D human pose estimation (HPE). Despite their strengths, the need for large search spaces and the corresponding demand for substantial training data make these models prone to generating biomechanically unrealistic poses. This challenge is particularly noticeable in occlusion scenarios, where the complexity of inferring 3D structures from 2D images intensifies. In response to these limitations, we introduce the **D**iscrete **D**iffusion **Pose (Di\({}^{2}\)Pose)**, a novel framework designed for occluded 3D HPE that capitalizes on the benefits of a discrete diffusion model. Specifically, Di\({}^{2}\)Pose employs a two-stage process: it first converts 3D poses into a discrete representation through a _pose quantization step_, which is subsequently modeled in latent space through a _discrete diffusion process_. This methodological innovation restrictively confines the search space towards physically viable configurations and enhances the model's capability to comprehend how occlusions affect human pose within the latent space. Extensive evaluations conducted on various benchmarks (_e.g._, Human3.6M, 3DPW, and 3DPW-Occ) have demonstrated its effectiveness.

## 1 Introduction

3D Human Pose Estimation (HPE) from monocular images remains a challenging yet pivotal research in the realm of computer vision, boasting a wide range of applications including human-machine interaction, autonomous driving, and animations . Generally, the mainstream approaches, including Direct Estimation  and 2D-to-3D Lifting , aim to perform 3D HPE by either directly predicting 3D poses from 2D images or lifting detected 2D poses into 3D space. These approaches aim to address the inherent 2D-3D ambiguity in 3D HPE tasks by learning mapping from training data. Despite significant advancements, accurately estimating 3D poses from monocular images remains a formidable challenge, particularly when humans are partially occluded . Such occlusions introduce considerable uncertainty and indeterminacy into the estimation process.

Existing 3D HPE methods try to handle the occlusion challenges with pose priors/constraints  or data augmentation strategies (_e.g._, annotations augmentation , pose transformation , and differentiable operations ). However, due to the inherent discreteness of 3D poses (primarily defined by discrete anatomical landmarks), these methods tend to represent poses using coordinate vectors or heatmap embeddings, treating joints as independent units and overlooking the interdependencies among body joints. Recent research  has introduced a compositional pose representation that captures the dependencies among joints by converting a pose into multiple tokens, enabling theuse of mutual context between joints. This approach, which learns from real pose datasets, results in each learned token corresponding to a physically realistic prototype. Nevertheless, Geng _et al._ casts HPE to a classification task, where the system simply classifies tokens based on prototype poses. Unfortunately, such scheme does not account for the effects of occlusions in the estimation process, potentially leading to inaccuracies due to unresolved uncertainty and indeterminacy.

Recent studies have shown marked progress in 3D HPE via generative models . Notably, diffusion models  have demonstrated effectiveness in handling complex and uncertain data distributions, making them suitable for handling uncertainty and indeterminacy in 3D HPE tasks . They excel at generating samples that conform to a target data distribution by iteratively removing noise through a series of diffusion steps, ultimately predicting more accurate 3D poses. However, these diffusion-based 3D HPE methods initialize the 3D pose from random noise at the beginning of the diffusion process, where each joint can be sampled from the continuous 3D space. Since the continuous 3D space has an infinite number of points, training such diffusion-based models requires a large amount of 3D pose data to achieve optimal outcomes . This demand implies a substantial need for training data, presenting a stark contradiction to the limited availability of 3D human pose datasets. As illustrated in Figure 1(a), the predictive performance of DiffPose  declines more rapidly as the proportion of training data decreases. Given the scarcity of 3D pose training data, previous diffusion models may generate physically implausible configurations that do not adhere to human biomechanics, leading to inaccurate human pose estimations, particularly in occluded scenes (_c.f._, Figure 1(b) with DiffPose).

In this paper, we propose a novel framework for 3D HPE with occlusions: **D**iscrete **D**iffusion **Pose** (**Di\({}^{2}\)Pose**), drawing on compositional pose representation and diffusion model to achieve the best of two worlds. Specifically, Di\({}^{2}\)Pose employs a two-stage approach: _a pose quantization step_ followed by _a discrete diffusion process_. The pose quantization step leverages the discrete nature of 3D poses and represents them as quantized tokens by capturing the local interactions between joints. This step effectively confines the search space to physically plausible configurations by learning from real 3D human poses. Subsequently, the discrete diffusion process models the quantized pose tokens in the latent space through a conditional diffusion model. By integrating the forward and reverse processes, our framework adeply simulates the transition of a 3D pose from occluded to recovered. By modeling occlusion implicitly within the latent space, Di\({}^{2}\)Pose enhances its understanding of how occlusions affect human poses, providing valuable insights during the training phase.

**For the pose quantization step**, we devise a pose quantization step inspired by VQ-VAE , consisting of a pose encoder, a quantization process, and a pose decoder. To effectively capture local interactions between 3D joints, we introduce the Local-MLP block for both pose encoder and decoder. Within each Local-MLP block, a simple Joint Shift operation integrates information from different joints. The pose encoder utilizes several Local-MLP blocks to convert a 3D pose into multiple rich token features, each representing a sub-structure of the overall pose. These tokens are quantized using a shared codebook, yielding corresponding discrete indices. Additionally, we implement the finite scalar quantization (FSQ)  to address the codebook collapse issue observed in traditional VQ-VAE methods . This strategy ensures that the generated codewords are meaningful, a crucial aspect for the subsequent success of the discrete diffusion process.

Figure 1: (a) Results of DiffPose  and Di\({}^{2}\)Pose in Human3.6M  dataset (with MPJPE metric), across varying proportions of training samples. (b) Prediction results of two methods under occlusion.

**For the discrete diffusion process**, during the training phase, we introduce occlude and replace strategies to model the quantized pose tokens, enabling the discrete diffusion model to predict occluded tokens and update potential tokens. The occluded token represents the occlusion of the corresponding sub-structure of the 3D pose. The token replacement mechanism is designed to enhance the diversity of potential sub-structures, reflecting the indeterminacy in occluded parts. During the inference phase, pose tokens are either occluded or initialized randomly. The denoising diffusion process estimates the probability density of pose tokens step-by-step based on the input 2D image until the tokens are completely reconstructed. Each step leverages contextual information from all tokens of the entire pose as predicted in the previous step, facilitating the estimation of a new probability density distribution and the prediction of the current step's tokens. This sequential approach ensures a detailed and accurate reconstruction of 3D poses from occluded scenes.

We extensively evaluate our approach in 3D HPE on three challenging benchmarks (Human3.6M , 3DPW  and 3DPW-Occ). Di\({}^{2}\)Pose consistently yields lower errors compared to state-of-the-art methods. In particular, it achieves significantly better results when evaluated on occluded scenarios, verifying its advantages of occlusion-handling capability. Our contributions are threefold:

* We propose the Di\({}^{2}\)Pose framework, which integrates the inherent discreteness of 3D pose data into the diffusion model, offering a new paradigm for addressing 3D HPE under occlusions.
* The designed pose quantization step represents 3D poses in a compositional manner, effectively capturing local correlations between joints and confining search space to reasonable configurations.
* The constructed discrete diffusion process simulates the complete process of a 3D pose transitioning from occluded to recovered, which introduces the impact of occlusions into pose estimation process.

## 2 Related Work

**Monocular 3D HPE.** Existing approaches can generally be classified into frame-based and video-based methodologies. **Frame-based methods** predict the 3D pose from a single RGB image, employing different networks in various studies [18; 20; 52; 54; 55] to directly output the human pose from the 2D image. Alternatively, a significant number of studies [48; 77; 85; 88] initially determine the 2D pose, which subsequently forms the foundation for inferring the 3D pose. In contrast, **video-based methods** leverage temporal relationships across video frames. Such methods predominantly [9; 11; 15; 31; 65; 73; 76] commence with the extraction of 2D pose sequences using a 2D pose detector from the video clips, aiming to harness the essential spatio-temporal data for 3D pose estimation. To validate the efficacy of our approach, we evaluate our Di\({}^{2}\)Pose on the more challenging frame-based setting, wherein the 3D human pose is directly inferred from a 2D image.

**Occluded 3D HPE.** Occlusions significantly challenge 3D HPE. As evidenced by research [76; 58; 62], pose priors and constraints have been proven crucial for mitigating such issue. Approaches typically involve statistical models to deduce occluded parts from visible cues [76; 41; 44] or pre-defined rules to constraint poses [61; 1]. Moreover, due to the scarcity of 3D pose data, data augmentation, including synthetic occlusions [7; 38; 60; 64; 13] and pose transformations [35; 82], remains vital for enhancing model robustness. Diverging from these aforementioned methods, our method innovatively introduces occlusion in the latent space without extra priors or explicit augmentations, providing a deeper feature-based understanding of occlusion's effects on pose estimation.

**Diffusion Models for 3D HPE.** Recent advancements have shown that diffusion models are capable of managing complex and uncertain data distributions [25; 26; 17; 4; 32; 8; 10; 80; 40; 36; 74; 78], which is particularly beneficial for 3D HPE. Typically, these models predict 3D poses by progressively refining the pose distribution from high to low uncertainty [22; 14; 19; 91; 63]. Other approaches use diffusion models to generate multiple pose hypotheses from a single 2D observation [66; 27]. These 3D pose estimators effectively reduce uncertainty and indeterminacy throughout the estimation process. Moreover, discrete diffusion models have also gained attention in various domains [30; 40; 24; 33]. Inspired by these advancements, our work introduces a discrete diffusion model for occluded 3D HPE, which aligns more closely with the inherent discreteness of 3D pose data and effectively incorporates occlusion into the estimation process, providing a novel perspective in the field.

## 3 Di\({}^{2}\)Pose

Given an 2D image \(I^{H W 3}\), the goal of 3D HPE is to predict \(}^{J 3}\), which represents the 3D coordinates of all the \(J\) joints of the human body. In this paper, we construct occluded 3D HPE task as a two-stage framework including the pose quantization step and the discrete diffusion process.

As shown in Figure 2, **in the training phase**, _Stage 1_ learns a pose quantization step by a VQ-VAE like structure (Sec. 3.1), which is able to encode a 3D pose into multiple quantized tokens. _Stage 2_ models quantized pose tokens in the latent space by the forward and reverse process of a conditional diffusion model (Sec. 3.2). **In the inference phase**, we only use _the reverse process_ of Stage 2 and the pre-trained pose decoder of Stage 1 to recover 3D pose from the 2D image. Notably, pose tokens are either occluded or initialized randomly at the beginning of the inference phase. The model reconstructs all the tokens based on the condition 2D image step-by-step. These reconstructed tokens are finally decoded by the pre-trained pose decoder, resulting in the recovered 3D pose.

### Pose Quantization Step

As depicted in Figure 2, a pose quantization step comprises a pose encoder, the quantization process, and a pose decoder. Initially, for a real 3D pose \(^{J 3}\), the pose encoder \(f_{PE}()\) converts \(\) to token features \(\). During the quantization process, we utilize FSQ to quantize \(=(_{1},_{2},,_{N})\) (\(_{i}^{D}\)) into tokens \(=(_{1},_{2},,_{N})\) (\(_{i}^{D}\)). Finally, the quantized tokens \(\) are decoded by the pose decoder \(f_{PD}()\) to reconstruct 3D pose \(}\).

**Pose Encoder.** Considering the interdependencies among human body joints, our goal is to represent 3D poses in a compositional manner, moving away from reliance on coordinates vectors or heatmap

Figure 2: Overview of our two-stage Di\({}^{2}\)Pose framework. In the stage 1, we train a pose quantization step that transforms a 3D pose \(\) into multiple discrete tokens \(\), each token representing the indices of implied codebook \(\). In the stage 2, we model \(\) in the discrete space by discrete diffusion process. In the forward process, each token is probabilistically occluded with Occ token or replaced with another available token. In the reverse process, the model leverages an independent image encoder and a pose denoiser to reconstruct all the tokens based on the condition 2D image. These reconstructed tokens are finally decoded by the pose decoder, resulting in the recovered 3D pose. Notably, we only update the parameters of pose denoiser, pose decoder and image encoder are frozen.

embeddings. The VQ-VAE architecture, incorporating MLP-Mixer blocks  within its encoder and decoder, has been proven effective in decomposing a pose into multiple token features, each corresponding to a sub-structure of the pose . However, the MLP-Mixer block is designed to extract global information across all joints, which can not adequately capture the local relationships between joints within individual sub-structure.

In response to aforementioned limitation, we design Local-MLP block to capture the local interactions between 3D joints. The pose encoder \(f_{PE}()\), comprising several Local-MLP blocks, converts \(\) to \(N\) token features:

\[=(_{1},_{2},,_{N})=f_{PE}(f_{ emb}()),\] (1)

where \(f_{emb}()\) embeds \(\) to \(_{}^{J D}\) by a linear layer.

As shown in Figure 3(a), a Local-MLP block is composed of a Layer Normalization layer, a Joint Shift block (JS-Block), a Channel MLP, and a residual connection. The JS-Block is specifically designed to capture local interactions among \(X\) joints. It extracts features by linear projection, and the Joint Shift operation enables feature translation along joint connection directions. As shown in Figure 3(b), with the input \(_{}^{}^{D J}\), the feature is evenly divided into \(X\) segments (\(X=3\) in the example), each segment being shifted incrementally by units from \(- X/2\) to \( X/2\). The central segment remains stationary, while the segments to the left and right are symmetrically shifted away from the center by up to \( X/2\) units. Zero padding is used to maintain dimensionality. Features highlighted within the dashed box are selected for further linear projection. Finally, the Channel MLP processes these features channel-wise to facilitate information integration.

**Quantization Process.** During this process, we exploit FSQ  to enhance the utilization of codewords. FSQ quantizes token features \(\) as corresponding token indices:

\[=(k_{1},k_{2},,k_{N})=(f_{proj}()),\] (2)

where \(f_{proj}()\) projects each \(_{i}^{D}\) of \(\) to \(_{i}^{d}\), and each \(k_{i}\) of \(\) denotes the entries of implied codebook \(\). For each \(_{i}\), FSQ employs a bounding function \(f_{bnd}:_{i} L/2(_{i})\) to constrain each channel of \(d\). As a result, each channel in \(_{i}=(f_{bnd}(_{i}))\) takes one of \(L\) unique values. This procedure yields \(_{i}\), where the total number of unique codebook entries is \(||=_{i=1}^{d}L_{i}\) (mapping the \(i\)-th channel to \(L_{i}\) values). The vectors in \(\) can be enumerated, establishing a bijective mapping from any \(_{i}\) to an integer within \(\{1,,||\}\). In addition, the corresponding codeword of \(k_{i}\), which is denoted as \(_{i}^{D}\), represents the quantized result of \(_{i}\). Thereby, using FSQ, the token features \(\) are quantized as \(=(_{1},_{2},,_{N})\).

**Pose Decoder.** The pose decoder \(f_{PD}()\) is designed to recover 3D pose \(}\) from \(\). \(f_{PD}()\) adopts a structure similar to the pose encoder but in reverse, utilizing a reduced number of Local-MLP blocks.

**Loss.** The pose quantization step, including the pose encoder, quantization process, and pose decoder, is jointly optimized by minimizing L1 loss \(_{PQ}=||P-||_{1}\) across the training dataset.

### Discrete Diffusion Process

After training the pose quantization step, we can acquire \(N\) quantized tokens \(\) from the original 3D pose \(\). The next step in Di2Pose pipeline is to model \(\) in the latent space by the discrete diffusion process. In the following, we first briefly introduce the diffusion models and clarify the basic principles of the discrete diffusion model. Then we explain the details of discrete diffusion

Figure 3: (a) depicts the structure of the Local-MLP block; (b) shows the Joint Shift operation, where the arrows indicate the steps, and different subscript numbers represent the features of different joints. The gray blocks indicate zero padding.

process, including the designed transition matrix and loss function. Eventually, we illustrate the architecture and training and inference process.

**Discrete Diffusion Model.** Our discrete diffusion model is characterized by two distinct processes: 1) **Forward process**: It progresses through discrete steps \(s\{0,1,2,...,S\}\), gradually transforming the initial tokens \(_{0}\) (the quantized token \(\)) into a noise-infused latent representation \(_{S}\). 2) **Reverse process**: It is tasked with reconstructing the original data \(_{0}\) from the latent \(_{S}\), following a reverse temporal sequence \(s\{S,S-1,...,1,0\}\).

Followed previous studies , we use a transition probability matrix \([_{s}]_{ij}=q(_{s}=i|_{s-1}=j)^{| |||}\) elucidate the likelihood of transitioning from \(_{s-1}\) to \(_{s}\). Then the forward process for the entire sequence of tokens is expressed as:

\[q(_{s}|_{s-1})=^{}(_{s})_{s}(_{s-1}),\] (3)

where \(()\) symbolizes a function capable of converting a scalar into a one-hot column vector. The distribution of \(_{s}\) follows a categorical distribution, determined by the vector \(_{s}(_{s-1})\). Leveraging the Markov chain property, it is feasible to bypass intermediate stages, directly computing the probability of \(_{s}\) from \(_{0}\) for any given step as:

\[q(_{s}|_{0})=^{}(_{s})}_{s}(_{0}),}_{s}=_{s}_{1}\] (4)

Moreover, the posterior of the reverse process, \(q(_{s-1}|_{s},_{0})\), can be ascertained as:

\[q(_{s-1}|_{s},_{0})=_{s}| _{s-1},_{0})q(_{s-1}|_{0})}{q( _{s}|_{0})}=^{}(_{s })_{s}(_{s-1})^{}( _{s-1})}_{s-1}(_{0}) }{^{}(_{s})}_{s}( _{0})}.\] (5)

**Occlude and Replace Transition Matrix.** Notably, a suitable design for transition probability matrix \(_{s}\) is significant to train the discrete diffusion process. As illustrated in Section 3.1, through pre-trained pose encoder and FSQ, \(\) can be converted to \(=(k_{1},k_{2},,k_{N})\), each \(k_{i}\) corresponding to a sub-structure of the overall pose. With this foundation, we specifically devise the occlude and replace scheme, which is inspired by , for tackling the challenges of occluded 3D HPE. In occlusion scenes, the human body is always occluded in various situations (self-occlusions, object or people-to-person occlusions), and the typical manifestation is that some sub-structures of the pose are invisible. Consequently, we design the occlude scheme simulating the occlusion of corresponding joints, which introduces occlusion impact in the training process. Additionally, recognizing the inherent uncertainty in occlusion scenarios where a single occluded region may correspond to multiple potential 3D human poses, we develop the replace strategy to update certain token with another available token.

In practice, each quantized token \(k_{i}\) has a probability of \(_{s}\) to transition to the Occ token. Moreover, \(k_{i}\) is also subject to a probability of \(||_{s}\) to be uniformly resampled across all \(||\) categories. Furthermore, \(k_{i}\) retains a probability of \(_{s}=1-||_{s}-_{s}\) to remain unchanged. Then, the transition matrix \(_{s}^{(||+1)(||+1)}\) is defined as:

\[_{s}=_{s}+_{s}&_{s}&&0\\ _{s}&_{s}+_{s}&&0\\ &&&\\ _{s}&_{s}&&1,\] (6)

where \(_{s}\), \(_{s}\). The prior distribution of step \(S\) can be derived as: \(p(_{S})=[_{S},_{S},,_{S}]\), where \(_{S}=_{i=1}^{S}_{i}\), \(_{S}=1-_{i=1}^{S}(1-_{i})\) and \(_{S}=(1-_{S}-_{S})/||\). In this study, we adapt the linear schedule  as noise schedule to pre-define the value of transition matrices (\(_{S}\), \(_{S}\), and \(_{S}\)). Subsequently, we can calculate \(q(_{s}|_{0})\) according to Eq. (4). However, when the number of categories \(||\) and time step \(S\) is too large, it can quickly become impractical to store all of the transition matrices \(_{s}\) in memory, as the memory usage grows like \(O(||^{2}S)\). Actually, it is unnecessary to store all of the transition matrices. Instead we only store all of \(_{s}\) and \(_{s}\) in advance, since we can calculate \(q(_{s}|_{0})\) according to following formula (refer to Appendix for proofs):

\[}_{s}(_{0})=_{s} (_{0})+(_{s}-_{s}) (||+1)+_{s}.\] (7)

**Training Objectives.** We train a network \(f_{}(_{s-1}|_{s},)\) to estimate \(q(_{s-1}|_{s},_{0})\) in the reverse process. The network is trained to minimize the variational lower bound (VLB):

\[_{}=D_{KL}(q(_{S}|_{0})||p(_{ S}))+_{s=1}^{S-1}D_{KL}[q(_{s-1}|_{s},_{0}) ||f_{}(_{s-1}|_{s},)]},\] (8)In addition, we follow [50; 24] to utilize the reparameterization trick, which lets Di2Pose predict the noiseless token distribution \(f_{}(}_{0}|_{s},)\) at each reverse step, and then compute \(f_{}(_{s-1}|_{s},)\) as:

\[f_{}(_{s-1}|_{s},)=_{}_{0}=1 }^{H}q(_{s-1}|_{s},}_{0})f_{}(}_{0}|_{s},).\] (9)

Based on the Eq. (9), an auxiliary denoising objective loss is introduced, which encourages the network to predict \(f_{}(}_{0}|_{s},)\):

\[_{k_{0}}=- f_{}(}_{0}|_{s}, {y}).\] (10)

Our final loss function is defined as:

\[=_{_{0}}+_{vlb},\] (11)

where \(\) is a hyper-parameter to control the weight of the auxiliary loss \(_{_{0}}\).

**Diffusion Architecture.** As depicted in Figure 2, our discrete diffusion model consists of three main components: an image encoder, a pose denoiser, and a pose decoder. The pre-trained image encoder processes the 2D image to produce a conditional feature sequence. The pose denoiser, receiving the quantized pose tokens \(_{s}\) and step \(S\), predicts the distribution of noiseless tokens \(f_{}(}_{0}|_{s},)\). This component is equipped with several transformer blocks, each featuring an AdaLNorm operator , multi-head attention blocks that combine the image feature information with \(_{s}\), and layer normalization and linear layers. At the end of the reverse process, all recovered tokens are obtained, and the final prediction of 3D pose is decoded by the well-trained pose decoder.

**Training and Inference Process.**_In the training process_, as for step \(s\), we sample \(_{s}\) from \(q(_{s}|_{0})\) based on Eq. (7) in the forward process. We then estimate \(f_{}(_{s-1}|_{s},)\) in the reverse process. The final loss will be calculated according to Eq. (11). _In the inference process_, all pose tokens are either masked or initialized randomly. Subsequently, we predict \(f_{}(_{s-1}|_{s},)\) step by step until the tokens are completely recovered. Finally, reconstructed tokens are decoded by the pose decoder, resulting in the recovered 3D pose. The complete algorithms are summarized in Appendix.

## 4 Experiments

### Datasets and Evaluation Metrics

**Datasets. Human3.6M** is the most extensive benchmark for 3D HPE, consisting of 3.6 million images. We follow  with same protocol, which involves training on subjects S1, S5, S6, S7, and S8, and testing on subjects S9 and S11. **3DPW** is the first dataset in the wild that includes video footage taken from a moving phone camera. We also evaluate our method on this dataset to measure the robustness and generalization. Additionally, to further verify the occlusion-robustness, we evaluate Di2Pose on the **3DPW-Occ**, which is a subset of the 3DPW.

**Evaluation Metrics.** For Human3.6M and 3DPW, we follow the standard protocols. Mean per joint position error (**MPJPE**) calculates the mean Euclidean distance between the root-aligned reconstructed poses and ground truth joint coordinates. **PA-MPJPE** employs a Procrustes alignment between the poses before calculating the MPJPE. In addition, to further evaluate the effectiveness of our method under occlusion scenes, we devise an adversarial protocol, termed **3DPW-AdvOcc**, following the previous research . We apply occlusion patches to the input image to identify the most challenging predictions. This process involves assessing the relative performance degradation on the visible joints. Similar to , we utilize textured patches generated by randomly cropping texture maps from the DTD . We employ two square patch sizes: 40 and 80 relative to a 256 x 192 image, denoted as Occ@40 and Occ@80 respectively, with a stride of 10.

### Implementation Details

**Pose Quantization Step.** The pose encoder is constructed with four Local-MLP blocks, while the pose decoder incorporates a single block. Within these Local-MLP blocks, the embedding dimensions \(D\) for the pose encoder and decoder are configured to 2048 and 512, respectively. For the quantization process, the projected vector \(_{i}\) features the channel \(d=5\). The levels per channel, denoted as \([L_{1},,L_{d}]\), are specified as \(\). The number of quantized tokens \(N\) is set to 100.

**Discrete Diffusion Process.** For the occlude and replace transition matrix, we linearly increase \(_{s}\) and \(_{s}\) from 0 to 0.1 and 0.9, respectively, and decrease \(_{s}\) from 1 to 0. For the discrete diffusion model, we use off-the-shelf image encoder  to extract feature sequence of conditional 2D image. As for the pose denoiser, we build a 21-layer 16-head transformer with the dimension of 1024. We set steps \(S\) as 100 and loss weight \(\) is set to 5e-4. Please refer to Appendix for more details.

### Comparison with State-of-the-Arts

**Human3.6M.** To explore the effectiveness of Di\({}^{2}\)Pose, we evaluate its performance in the challenging context of frame-based 3D pose estimation. Specifically, within the discrete diffusion process, context information is extracted from a single input frame using the image encoder. As shown in Table 1, we benchmark Di\({}^{2}\)Pose against SOTA 3D HPE methods on the Human3.6M. Our Di\({}^{2}\)Pose achieves 49.2mm in average MPJPE, surpassing the performance of the SOTA diffusion model  by 0.5mm, which indicates that Di\({}^{2}\)Pose is able to enhance monocular 3D HPE in indoor scenes.

**3DPW.** Beyond indoor settings, we evaluate the performance of Di\({}^{2}\)Pose on the in-the-wild 3DPW dataset. As Table 2 shows, Di\({}^{2}\)Pose achieves the SOTA performance, and outperforms the SOTA method  by 3.4mm in MPJPE and 3.7mm in PA-MPJPE. On the occlusion-centric **3DPW-Occ**, Di\({}^{2}\)Pose maintains its superiority. When assessed under the 3DPW-AdvOcc protocol, all methods exhibit performance drops--MPJPE surges by up to 129% and PA-MPJPE by up to 72%. Despite this, Di\({}^{2}\)Pose remains markedly robust, leading the SOTA by significant margins in both MPJPE and PA-MPJPE, underscoring its effectiveness in handling occlusions.

**Qualitative Results.** Figure 4 presents the qualitative results of DiffPose  in comparison with our Di\({}^{2}\)Pose across two datasets. It can be observed that our method yields more accurate predictions than compared diffusion model (DiffPose), especially under various occlusion scenarios (self-occlusion and object occlusion). This demonstrates the superior occlusion-robustness of our Di\({}^{2}\)Pose.

   Methods & Dir & Disc & Eat & Gr. & Phon. & Phot. & Pose & Pur. & Sit & SitD. & Sm. & Wait & W.D. & Walk & W.T. & Avg \\  Pavlakos _et al._ & _CVPR21_ & 67.4 & 71.9 & 66.7 & 69.1 & 72.0 & 77.0 & 65.0 & 68.3 & 83.7 & 96.5 & 71.7 & 65.8 & 74.9 & 59.1 & 63.2 & 71.9 \\ Martinez _et al._ & _ECCV17_ & 51.8 & 56.2 & 58.1 & 59.0 & 69.5 & 78.4 & 55.2 & 58.1 & 74.0 & 94.6 & 62.3 & 59.1 & 65.1 & 49.5 & 52.4 & 62.9 \\ Hossain _et al._ & _ECCV18_ & 48.4 & 50.7 & 57.2 & 55.2 & 63.1 & 72.6 & 53.0 & 51.7 & 66.1 & 80.9 & 59.0 & 57.3 & 62.4 & 46.6 & 49.6 & 58.3 \\ Zhao _et al._ & _CVPR19_ & 48.2 & 60.8 & 51.8 & 64.0 & 64.6 & **53.6** & 51.1 & 67.4 & 88.7 & **57.7** & 73.2 & 65.6 & 48.9 & 64.8 & 51.9 & 60.8 \\ Liu _et al._ & _CVPR21_ & 46.3 & 52.2 & 47.3 & 50.7 & 55.6 & 67.1 & 49.2 & 64.0 & 60.4 & 71.1 & 51.5 & 50.1 & 54.5 & 40.3 & 43.7 & 52.4 \\ Xu _et al._ & _CVPR21_ & 45.2 & 49.9 & 47.5 & 50.9 & 54.9 & 66.1 & 48.5 & 46.3 & 59.7 & 71.5 & 51.4 & 48.6 & 53.9 & 39.9 & 44.1 & 51.9 \\ Zhao _et al._ & _CVPR22_ & 45.2 & 50.8 & 48.0 & 50.4 & 59.0 & 65.0 & 48.2 & 47.1 & 60.2 & 70.0 & 51.6 & 48.7 & 54.1 & 39.7 & 43.1 & 51.8 \\ Geng _et al._ & _CVPR22_ & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 50.8 \\ Choi _et al._ & _ROS22_ & 44.3 & 51.6 & 46.3 & 51.1 & **50.3** & 54.3 & 49.4 & 45.9 & 57.7 & 71.6 & **48.6** & 49.1 & 52.1 & 44.0 & 44.4 & 50.7 \\ Zhang _et al._ & _PAMPJ22_ & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & 50.2 \\ Gong _et al._ & _CVPR22_ & 42.8 & 49.1 & 45.2 & **48.7** & 52.1 & 63.5 & 46.3 & **45.2** & 58.6 & 66.3 & 50.4 & 47.6 & 52.0 & 37.6 & **40.2** & 49.7 \\ 
**Di\({}^{2}\)Pose (Ours)** & **41.9** & **47.8** & **45.0** & 49.0 & 51.5 & 62.2 & **45.7** & 45.6 & **57.6** & 67.1 & 50.1 & **45.3** & **51.4** & **37.3** & 40.9 & **49.2** \\   

Table 1: Results on Human3.6M in millimeters under MPJPE. The best results are in **bold**, and the second-best ones are underlined.

    &  &  &  &  \\   & MPJPE & \(\)PA-MPJPE & \(\)MPJPE & \(\)PA-MPJPE & \(\)PA-MPJPE & \(\)MPJPE & \(\)PA-MPJPE & \(\)PA-MPJPE \(\) \\  Cai _et al._ & _ECCV19_ & 112.9 & 69.6 & 115.8 & 72.3 & 241.1 & 101.4 & 355.9 & 116.3 \\ Pavllo _et al._ & _CVPR19_ & 101.8 & 63.0 & 106.7 & 67.1 & 221.6 & 99.4 & 334.3 & 112.9 \\ Cheng _et al._ & _AAU271_ & -- & 64.2 & -- & 85.7 & 279.4 & 113.2 & 371.4 & 119.8 \\ Zheng _et al._ & _ECCV27_ & 118.2 & 73.1 & 132.8 & 80.5 & 247.9 & 106.2 & 359.6 & 115.5 \\ Zhang _et al.__ & _Image23_ & 91.1 & 54.3 & 94.6 & 56.7 & 142.5 & 73.8 & 251.8 & 103.9 \\ Geng _et al._ & _CVPR23_ & 83.1 & 53.9 & 82.8 & 53.7 & 127.2 & 71.9 & 192.5 & 92.1 \\ Gong _et al._ & _CVPR23_ & 82.7 & 53.8 & 82.1 & 53.5 & 121.4 & 70.9 & 189.3 & 92.4 \\ 
**Di\({}^{2}\)Pose (Ours)** & **79.3** & **50.1** & **79.6** & **50.7** & **108.4** & **59.8** & **153.6** & **78.7** \\   

Table 2: Evaluation on 3DPW, 3DPW-Occ, and 3DPW-AdvOcc. The number \(40\) and \(80\) after 3DPW-AdvOcc denote the occluder size. * denotes the results from our implementation. The best results are in **bold**, and the second-best ones are underlined.

[MISSING_PAGE_FAIL:9]

transition matrix causes the model to overly focus on the occluded portions, neglecting the contextual information from other visible parts. These clarifications can be verified in Table 2(c), where we investigate the impact of different \(_{S}\). When \(_{S}=0\), the occlude and replace transition matrix can be seen as the replace transition matrix, and when \(_{S}=1\), the occlude and replace transition matrix can be seen as the occlude transition matrix. The best performance is obtained when \(_{S}=0.9\).

In addition, we conducted an ablation study to investigate the impact of \(S\) on the training and inference processes, as shown in Table 2(d). We observed that using larger numbers of steps during both training and inference stages improves performance but also increases time complexity. Moreover, the results indicate that performance remains satisfactory even when the number of inference steps is reduced by 75% (e.g., from 100 steps during training to 25 steps during inference). This finding suggests a viable strategy for enhancing generation speed without significantly compromising quality.

## 5 Limitations

Figure 5 illustrates several results of 3D human pose estimation. When substantial occlusions cover the human body--obscuring the exact pose to the extent that it confounds even human observers--the predictions made by Di\({}^{2}\)Pose may deviate from GT 3D pose. This deviation primarily stems from the inherent limitation of inferring 3D poses directly from 2D images, which lack critical spatial depth information. Such limitations introduce uncertainty and indeterminacy in the predictions.

Despite these challenges, Di\({}^{2}\)Pose manages occlusions effectively by producing physically plausible outcomes. This capability is attributed to the integration of a pose quantization step within Di\({}^{2}\)Pose, which constrains the model's search space to physically reasonable configurations. Note that the pose quantization step is trained on real 3D human pose data, enhancing its reliability under severe occlusions.

Currently, Di\({}^{2}\)Pose is primarily designed for frame-based 3D HPE and does not utilize interframe data from videos. Future enhancements will focus on incorporating interframe information to refine the accuracy of 3D pose predictions further within the Di\({}^{2}\)Pose framework.

## 6 Conclusion

This paper presents Di\({}^{2}\)Pose, a novel diffusion-based framework that tackles occluded 3D HPE in discrete space. Di\({}^{2}\)Pose first captures the local interactions of joints and represents a 3D pose by multiple quantized tokens. Then, the discrete diffusion process models the discrete tokens in latent space through a conditional diffusion model, which implicitly introduces occlusion into the modeling process for more reliable 3D HPE with occlusions. Experimental results show that our method surpasses the state-of-the-art approaches on three widely used benchmarks.

Figure 5: Failure cases of our Di\({}^{2}\)Pose for 3D HPE. These instances primarily occur in scenarios with severe occlusions, as compared against ground truth (GT) poses. The content encircled by the dashed line indicates the parts where differences exist.