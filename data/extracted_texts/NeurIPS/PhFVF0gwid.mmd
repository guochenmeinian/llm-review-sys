# Provable Guarantees for Generative Behavior Cloning: Bridging Low-Level Stability and High-Level Behavior

Provable Guarantees for Generative Behavior Cloning: Bridging Low-Level Stability and High-Level Behavior

 Adam Block

MIT

Cambridge, MA

ablock@mit.edu &Ali Jadbabaie

MIT

Cambridge, MA

jadbabai@mit.edu &Daniel Pfrommer

MIT

Cambridge, MA

dpfrom@mit.edu &Max Simchowitz

MIT

Cambridge, MA

msimchow@csail.mit.edu &Russ Tedrake

MIT

Cambridge, MA

russt@mit.edu

###### Abstract

We propose a theoretical framework for studying behavior cloning of complex expert demonstrations using generative modeling. Our framework invokes low-level controllers - either learned or implicit in position-command control - to stabilize imitation around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a powerful enough generative model as our imitation learner, pure supervised behavior cloning can generate trajectories matching the per-time step distribution of essentially arbitrary expert trajectories in an optimal transport cost. Our analysis relies on a stochastic continuity property of the learned policy we call "total variation continuity" (TVC). We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accurately estimates the score of the (noise-augmented) expert policy, then the distribution of imitator trajectories is close to the demonstrator distribution in a natural optimal transport distance. Our analysis constructs intricate couplings between noise-augmented trajectories, a technique that may be of independent interest. We conclude by empirically validating our algorithmic recommendations, and discussing implications for future research directions for better behavior cloning with generative modeling.

## 1 Introduction

Training dynamic agents from datasets of expert examples, known as _imitation learning_, promises to take advantage of the plentiful demonstrations available in the modern data environment, in an analogous manner to the recent successes of language models conducting unsupervised learning on enormous corpora of text [67; 70]. Imitation learning is especially exciting in robotics, where mass stores of pre-recorded demonstrations on Youtube  or cheaply collected simulated trajectories [42; 20] can be converted into learned robotic policies.

For imitation learning to be a viable path toward generalist robotic behavior, it needs to be able to both represent and _execute_ the complex behaviors exhibited in the demonstrated data. An approach that has shown tremendous promise is _generative behavior cloning:_ fitting generative models, such as diffusion models [2; 19; 33], to expert demonstrations with pure supervised learning. In this paper, we ask: **When can generative behavior cloning imitate arbitrarily complex expert behavior?**

In this paper, we are interested in how _algorithmic choices_ interface with the _dynamics of the agent's environment_ to render imitation possible. The key challenge separating imitation learning from vanilla supervised learning is one of _compounding error_: when the learner executes the trained behavior in its environment, small mistakes can accumulate into larger ones; this in turn may bring the agent to regions of state space not seen during training, leading to larger-still deviations from intended trajectories. Without the strong requirement that the learner can interactively query the expert at new states [40; 57], it is well understood that ensuring some form of _stability_ in the imitation learning procedure is indispensable [68; 27; 50]. While many natural notions of stability exist for simple behaviors, how to _enforce_ stability when imitating more complex behaviors remains an open question. Multi-modal trajectories present a key example of this challenge: consider a robot navigating around an obstacle; because there is no difference between navigating around the object to the right and around to the left, the dataset of expert trajectories may include examples of both options. This bifurcation of good trajectories can make it difficult for the agent to effectively choose which direction to go, possibly even causing the robot to oscillate between directions and run into the object. . Moreover, human demonstrators correlate current actions with the past in order to _commit_ to either a right or left path, which makes even formulating the idea of an "expert _policy_" a conceptually challenging one. Lastly, bifurcations are necessarily _incompatible_ with previous notions of stability derived from classical control theory [68; 27; 50]. **In this work, we investigate how these strong and often unrealistic assumptions on the expert policy can be replaced by practical (and often realistic) assumptions on available algorithms.**

### Contributions.

As in previous work, we formalize behavior cloning in two stages: at _train-time_, we learn a map from observations to distributions over actions, supervised by (state, action)-pairs from expert demonstrations coming from \(N_{}\) independent expert trajectories, while at _test-time_, the learned map, or _policy_, is executed on random initial states (distributed identically to initial training states). Following the schematic of existing theoretical analyses of behavior cloning [68; 50; 27], we demonstrate that a policy trained by minimizing a certain supervised learning objective on expert demonstrations induces trajectories that approximate those of expert demonstrations. Our work considers a significantly more general setting than past theoretical literature, and one which reflects the strength of _generative models_ for imitation. One corollary of our key contributions is summarized in the following informal statement. The main technical insights leading to the proof of the theorem are detailed in the bullet points below it, and depicted in Figure 1.

Figure 1: Consider demonstration trajectories exhibiting two modes: a “go left” and “go right” mode around an obstacle depicted in red and purple, respectively. To avoid compounding error, we imitate sequences of simple low-level feedback controllers we call “primitive controllers”, not simply raw actions. Intuitively, primitive controllers provide “tubes” around each demonstration trajectory where the system can be stabilized. Depicted in yellow, our data-noising procedure described below “fills in the gaps” in the demonstration, switching between modes in a well-behaved manner, and whilst allowing the primitive controllers to manage the stabilization.

**Theorem (informal)**.: _Consider a generative behavior cloner \(\) that learns to predict sequences of expert actions on horizon \(H\), **along with low-level controllers that locally stabilize the trajectories.** Then, with a suitable data noising strategy, for all times \(h H\),_

\[[]\] \[_{}(H+}_{h}_{,h}[_{1} (_{},_{ })])\]

_where \(_{,h}[_{1}(_{},_{})]\) denotes a \(1\)-Wasserstein distance in an appropriate metric between the conditional distribution over expert and imitator actions given the observation at time step \(h\), and where \(_{}\) hides constants depending polynomially on the stability properties of the low-level controllers, defined formally in Section 3.1._

We now detail the key ingredients of our results.

1. We imitate stochastic demonstrators that may exhibit both complex correlations between actions in their trajectories (e.g. be non-Markovian) and multi-modal behavior. The natural object to imitate in this setting is the conditional probability distribution of expert actions given recent states, but marginalized over past states. We require said conditional action distribution to be learnable by a **generative model**, but otherwise arbitrarily complex: in particular, the conditional distribution of an expert actions given the state can be discontinuous (in any natural distance metric) as a function of state, as in the bifurcation depicted in Figure 1_(right)_.
2. We obtain **rigorous, theoretical guarantees** and without requiring either interactive data collection (e.g. Dagger [57; 40]), or access to gradients of the expert policy (as in TaSil). Instead, we replace these assumption with an oracle, described below, which **synthesizes stabilizing, low-level policies** along training demonstrations--the green arrows in Figure 1_(left)_. This mirrors recent work on generative behavior cloning that find that providing state-commands through inverse dynamics controllers [33; 2] or position-command controllers of end effectors  leads to substantially improved performance.
3. We also apply a subtle-yet-significant modification to a popular **data noising** strategy, which we show yields both theoretical and empirical benefits. Data noising ensures a helpful property we denote _total variation continuity_ that interpolates between modes in probability space (without naively averaging their trajectories in world space). This effectively "fills in the missing gaps" in bifurcations, as indicated by yellow arrows in Figure 1.

Our main results, Theorems 1 and 2, are reductions from imitation of complex expert trajectories to supervised generative learning of a specific conditional distribution. For concreteness, Theorem 3 instantiates the generative modeling with Denoising Diffusion Probabilistic Models (DDPMs) of sufficient regularity and expressivity (as investigated empirically in [19; 48; 26]), and establishes end-to-end guarantees for imitation of complex trajectories with sample complexity polynomial in relevant problem parameters. Our analysis framework exposes that any sufficient powerful generative learner obtains similar guarantees. Finally, we empirically validate the benefits of our proposed smoothing strategy in simulated robotic manipulation tasks. We now summarize the algorithmic choices and analytic ideas that facilitate our reduction.

**Abridged Related Work.** Due to space, we defer a full comparison to past work to Appendix B. DDPMs, proposed in [29; 60], along with their relatives have seen success in image generation [62; 55], along with imitation learning (without data augmentation) [33; 19; 48], which is the starting point of our work. Smoothing data augmentation is ubiquitous in modern imitation learning  and our approach corresponds to that of  but with noise added at inference time. Despite the benefits of adaptive data collection [58; 40], adaptive demonstrations are more expensive to collect. Previous analyses of imitation learning without adaptive data collection have focused on classical control-theoretic notions of stability, notably incremental stability, [68; 27; 50], which require continuity, Markovianity, and often determinism, and preclude the bifurcations permitted in our setting.

## 2 Setting

**Notation and Preliminaries.** Appendix A gives a full review of notation. Bold lower-case (resp. upper-case) denote vectors (resp. matrices). We abbreviate the concatenation of sequences via \(_{1:n}=(_{1},,_{n})\). Norms \(\|\|\) are Euclidean for vectors and operator norms for matrices unlessotherwise noted. Rigorous probability-theoretic preliminaries are provided in F. In short, all random variables take values in Polish spaces \(\) (which include real vector spaces), the space of Borel distributions on \(\) is denoted \(()\). We rely heavily on _couplings_ from optimal transport theory: given measures \(X\) and \(X^{}^{}\) on \(\) and \(^{}\) respectively, \((,^{})\) denotes the space of joint distributions \((^{})\) called "couplings" such that \((X,X^{})\) has marginals \(X\) and \(X^{}\). \(()\) denotes the space of conditional probability distributions \(:()\), formally called probability _kernels_ ; Appendix F rigorously justifies that, in our setting, all conditional distributions can be expressed as kernels (which we do throughout the paper without comment). Finally \(_{}()\) denotes the indicator taking value \(1\) if \(\) is true and \(\) otherwise.

**Dynamics and Demonstrations.** We consider a discrete-time, control system with states \(_{t}:=^{d_{x}}\), and inputs \(_{t}:=^{d_{u}}\), obeying the following nonlinear dynamics

\[_{t+1}=f(_{t},_{t}), t 1.\] (2.1)

Given length \(T\), we call sequences \(_{T}=(_{1:T+1},_{1:T})_{T }:=^{T+1}^{T}\)_trajectories_. For simplicity, we assume that (2.1) is deterministic and address stochastic dynamics in Appendix N. Though the dynamics are Markov and deterministic, we consider a stochastic and possibly _non-Markovian_ demonstrator, which allows for the multi-modality described in the Section 1.

**Definition 2.1** (Expert Distribution).: Let \(_{}(_{T})\) denote an _expert distribution_ over trajectories to be imitated. \(_{_{1}}\) denotes the distribution of \(_{1}\) under \(_{T}=(_{1:T+1},_{1:T})_{ }\).

Primitive Controllers.Our approach is to imitate not just actions, but simple local _control policies_. In the body of this paper, we consider affine mappings \(\) (redundantly) parameterized as \(}+}(-})\); we call these _primitive controllers_, denoted with \(=(},},})\). We describe the synthesis of these controllers in Appendix D, and extend our results to general families of parameterized controllers in Appendix E. We argue in Appendix E that primitive controllers are in fact standard practice, and implicit via robotic position control in many applications of diffusion to robotic behavior cloning.

Chunking Policies and Indices.The expert distribution \(_{}\) may involve non-Markovian sequences of actions. We imititate these sequences via _chunking policies_. Fix a _chunk length_\(_{}\) and _observation length_\(_{}_{}\), and define time indices \(t_{h}=(h-1)_{}+1\). For simplicity, we assume \(_{}\) divides \(T\), and set \(H=T/_{}\). Given a \(_{T}_{T}\), define the _trajectory-chunks_ and _observation_ chunks

\[_{h} :=(_{t_{h-1}:t_{h}},_{t_{h-1}:t_{h}-1}) :=_{_{}}\] (trajectory-chunks) \[_{h} :=(_{t_{h}-_{}+1:t_{h}},_{t_{ h}-_{}+1:t_{h}-1}):=_{_{}-1}\] (observation-chunks)

for \(h>1\), and \(_{1}=_{1}=_{1}\) (for simplicity, we embed \(_{1}\) into \(_{_{}-1}\) via zero-padding). We call \(_{}\)-length sequences of primitive controllers _composite actions_

\[_{h}=_{t_{h}:t_{h-1}}:=^{_{ }}.\] (composite actions)

A _chunking policy_\(=(_{h})\) consists of functions \(_{h}\) mapping observation-chunks \(_{h}\) to distributions \(()\) over composite actions and interacting with the dynamics (2.1) by \(_{h}=_{t_{h}:t_{h-1}}_{h}(_{h})\), and executing \(_{t}=_{t}(_{t})\). We let \(d_{}=_{}(d_{x}+d_{u}+d_{x}d_{u})\) denote the dimension of the space \(\) of composite actions. The chunking scheme is represented in Figure 2, demonstrating the rationale for using primitive controllers over open-loop actions.

Remark C.1 describes our rationale for studying _states_ over generic observations, and considering time-dependent policies.

Desideratum.The quality of imitation of a deterministic policy is naturally measured in terms of step-wise closeness of state and action . With stochastic policies, however, two rollouts of even the same policy can visit different states. We propose measuring _distributional closeness_ via _couplings_ introduced in the preliminaries above. We define the following losses, focusing on the _marginal distributions_ between trajectories.

**Definition 2.2**.: Given \(>0\) and a (chunking) policy \(\), the (marginal distribution) imitation loss is \(_{,}():=_{t[T]}_{} _{}[\{\|_{t+1}^{}-_{t+1}^{ }\|,\|_{t}^{}-_{t}^{}\|\}> ],\) where the infimum is over all couplings \(\) between the distribution of \(_{T}\) under \(_{}\) and that induced by the policy \(\) as described above, such that \(_{}[_{1}^{}=_{1}^{}]=1\).

Under stronger conditions (whose necessity we establish), we can also imitate joint distributions over actions (Appendix J). Observe that \(_{,}_{,}\), and that both losses are equivalent to Wasserstein-type metrics on bounded domains. These losses are also equivalent to Levy-Prokhorov metrics  under re-scaling of the Euclidean metric (even for unbounded domains), and also correspond to total variation analogues of shifted Renyi divergences [5; 6]. While empirically evaluating these infima over couplings is challenging, \(_{,}\) upper bounds the difference in expectation between any bounded and Lipschitz control cost decomposing across time steps, states and inputs, and \(_{,}\) upper bounds differences in final-state costs; see Appendix J for discussion.

**Diffusion Models.** Our analysis provides initiation guarantees when chunking policies \(_{h}\) select \(_{h}\) via a sufficiently accurate generative model. Given their recent success, instantiate our analysis for the popular Denoising Diffusion Probabilistic Models (DDPM) framework [18; 41] that allows the learner to sample from a density \(q(^{d})\) assuming that the _score_\( q\) is known to the learner. More precisely, suppose the learner is given an observation \(_{h}\) and wishes to sample \(_{h} q(|_{h})\) for some family of probability kernels \(q(|)\). A DDPM starts with some \(_{h}^{0}\) sampled from a standard Gaussian noise and iteratively "denoises" for each DDPM-time step \(0 j<J\):

\[_{h}^{j}=_{h}^{j-1}-_{g,h}(_{h}^{j-1},_{h},j)+2(0,^{2}),\] (2.2)

where \(_{,h}(_{h}^{j},_{h},j)\) estimates the true score \(_{*,h}(_{h},_{h}, j)\), formally defined for any continuous argument \(t J\) to be \(_{*,h}(,_{h},t):=_{} q_{h,[ t]}^{*}(_{h})\), where \(q_{h,[t]}^{*}(|_{h})\) is the distribution of \(e^{-t}_{h}^{(0)}+}\) with \(_{h}^{(0)}\) is sampled from the target distribution we which to sample from, and \((0,)\) is a standard Gaussian. We denote by \((_{},_{h})\) the law of \(_{h}^{J}\) sampled according to the DDPM using \(_{}(,_{h},)\) as a score estimator. Preliminaries on DPPMs are detailed in Appendix L.

## 3 Conditional sampling with stabilization suffices for behavior cloning

We show that trajectories of the form given in Definition 2.1 can be efficiently imitated if (a) we are given a _synthesis oracle_, described below, that produces low-level control policies that locally stabilize chunks of the trajectory with primitive controllers and (b) we can learn to generate certain appropriate distributions over composite actions, i.e. sequences of primitive controllers. All the following results apply to affine primitive controllers introduced in Section 2 and assume that the system dynamics are second-order smooth and locally stabilizable. In Appendix E, we show that our results still hold with general families of parametric primitive controllers, provided that these controllers induce the same local stability guarantee.

**The synthesis oracle.** We say primitive controller (cf. Section 2) \(_{1:T}^{T}\) is _consistent with_ a trajectory \(=(_{1:T+1},_{1:T})_{T}\) if \(}_{t}=_{t}\) and \(}_{t}=_{t}\) for all \(t[T]\); note that this implies that \(_{t}(_{t})=_{t}\) for all \(t\). A _synthesis oracle_ synth maps \(_{T}^{T}\) such that, for all \(_{T}_{T}\), \(_{1:T}=(_{T})\) is consistent with \(_{T}\). For our theory, we assume access to a synthesis oracle at training time, and assume the ability to estimate conditional distributions over joint sequences of primitive controllers; Appendix K explains how this can be implemented by solving Ricatti equations if dynamics are known (e.g. in a simulator), smooth, and stabilizable. In our experimental environment, control inputs are desired robot configurations, which the simulated

Figure 2: Graphical comparison of an action-chunk based policy (left) as described in , versus the primitive-controller chunking policy (right) proposed in this paper. The primitive controller paradigm allows for stabilizing back to the original expert trajectory, whereas using generated actions in an open-loop fashion may cause divergence from the expert in the presence of unstable system dynamics. We refer to **composite actions** as the sequence of primitve controllers given on the right.

robot executes by applying feedback gains. As discussed in Appendix E, learned or hand-coded low-level controllers are popular in practical implementations of generative behavior cloning. We discuss the merits of studying imitation learning with a synthesis oracle in depth in Appendix C.3.

**Notions of distance.** While restricting ourselves to affine primitive controllers, our approximation error of generative behavior cloner is measured in terms of optimal transport distances that use the following "maximum distance." Given two composite actions \(=(}_{1:_{}},}_{1: _{}},}_{1:_{}})\) and \(^{}=(}_{1:_{}}^{}, }_{1:_{}}^{},}_{1:_ {}}^{})\), we define

\[_{}(,^{}):=_{1 k_{ }}(\|}_{k}-}_{k}^{}\|+\|}_{k}-}_{k}^{}\|+\|}_{k}-}_{k}^{}\|).\] (3.1)

Distances between policies are defined via natural optimal transport costs. Given two policies \(=(_{h}),^{}=(_{h}^{})\) and observation chunk \(_{h}\), we define an induced optimal transport cost

\[_{}(_{h}(_{h}),_{h}^{}(_{h}) ):=_{}_{(_{h},_{h}^{})} [_{}(_{h},_{h}^{})> ],\]

where the \(_{}\) denotes the infimum over all couplings between \(_{h}_{h}(_{h})\) and \(_{h}^{}_{h}^{}(_{h})\). \(_{}\) corresponds to a relaxed Levy-Prokhor metric , and can always be bounded, via Markov's inequality, by \(_{}(_{h},_{h}^{}_{h})_{1,_{}}(_{h}(_{h}),_{h}^ {}(_{h}))\), where \(_{1,_{}}(_{h}(_{h}),_{h}^{}( _{h}))\) denotes the \(1\)-Wasserstein distance between \(_{h}_{h}(_{h})\) and \(_{h}^{}_{h}^{}(_{h})\).

### Incremental Stability and the Synthesis Oracle.

We assume that synthesis oracle above produces _incrementally stabilizing_ control gains, in the sense first proposed by . Incremental stability has emerged as a natural desirable property for imitation limitation , because it forces the expert to be robust to small perturbations of their policy. We now supply a formal definition. Given a primitive controller \(:^{d_{x}}^{d_{u}}\), define the closed loop dynamic map \(f_{,}(,):=f(,( )+)\). Thus, composite action \(\) is _consistent_ with a trajectory chunk \(=(_{1:_{}+1},_{1:})\) if \(_{t+1}=f_{,_{t}}(,)\) for \(1 t_{}\).1

**Definition 3.1** (Time-Varying Incremental Stability).: Let \(()\) be a class \(\) function, \((,)\) be class \(\) function, and let \(=(_{1},_{2},,_{})\) denote a sequence of primitive controllers (i.e. a composite action when \(=_{}\)). Given a sequence of input perturbations \(_{1:}(^{d_{u}})^{}\) and initial condition \(^{d_{x}}\), let \(_{1}^{}(_{1:},)=f_{ ,_{t}}(_{1}^{}(_{1:}, ),_{t})\), with \(_{1}^{}=\). We say that composite action a is time-varying incrementally input-to-state stable (t-Iss) with moduli \(()\), \((,)\) if for all \(,^{}^{d_{x}},0 i\), \(\|_{1}^{}(_{1:},)-_{1}^{}(_{1:},^{})\| (\|-^{}\|,)+(_{1  s i-1}\|_{s}\|)\). Given parameters \(c_{},c_{}>0\) we say that \(\) is local-t-Iss at \(_{0}\) if the above holds only for all \(,^{},_{1:}\) such that \(\|-_{0}\|,\|^{}-_{0}\| c_{}\) and \(_{t}\|_{t}\| c_{}\).

Incremental stability implies that as the inital conditions \(\|-^{}\| 0\) and \(_{0 s i-1}\|_{t}\| 0\), the trajectories induced by taking rolling out a from \(\), and rolling out a from \(^{}\) with additive input perturbations \(_{1:}\) tend to zero in norm. This behavior needs only hold for initial conditions in a small neighborhood of a nominal state \(_{0}\). Importantly, the perturbations \(_{1:}\) are fixed perturbations of inputs, applied to the _closed loop behavior_ under the controllers. Our notion of incremental stability are similar too, but subtly different similar notions of past work. We provide an extended comparisons in Appendix E.2. Our main assumption is that the synthesis oracle described above produced primitive controllers which are consistent with, and incrementally stabilizing for, the demonstrated trajectories. Figure 1 demonstrates the effect of stabilizing primitive controllers.

**Assumption 3.1**.: We assume that our synthesis oracle enjoys the following property. Let \(_{T}=(_{1:T+1},_{1:T})_{ }\), and let \(_{1:T}=(_{T})\), partitioned into composite actions \(_{1:H}\), with \(_{t}()=}_{t}(-}_{t})+ }_{t}\). We assume that, with probability one, \(_{1:T}\) is consistent with \(_{T}\)2, and that, for each \(1 h H\), \(_{h}=(_{t:t_{h}+_{}-1})\) is local t-Iss at \(_{t_{h}}\) with moduli \(,\) and parameters \(c_{},c_{}>0\). We further assume that \(\) and \(\) take the form

\[(u)=_{} u,(u,k)=_{}^{-( k-1)L_{}} u,_{},_{}>0, L_{}(0,1].\]Lastly, we assume that for the expert trajectories and the primitive controllers drawn as above, it holds that satisfy \(\{\|_{i}\|,\|_{i}\|\} R_{}\) and \(\|}_{t}\| R_{}\) with probability one.

In Appendix K, we show that Assumption 3.1 holds whenever (a) the dynamics of our system are smooth (but not necessarily linear!) (b) the affine gains are chosen to stabilize the Jacobian linearizations of the system around the nominal trajectory.

**Definition 3.2** (Problem constants).: Throughout, we refer to constants \(c_{1},c_{2},c_{3},c_{4},c_{5}>0\), which are polynomial in the terms in Assumption 3.1, and which are defined formally in Appendix K.

### Simplified guarantees under total variation continuity

This section presents our main theoretical result: if one learns a chunking policy \(\) that can compute the conditional distribution of composite actions at time steps given observation-chunks, then a stochastically smoothed version of this policy, \(_{}\), has low imitation error. Define, for any length \(\), the _trajectory distance_ between trajectories \(=(_{1:+1},_{1:})\), \(^{}=(^{}_{1:+1},^{ }_{1:})_{}\)

\[_{}(,^{}):= _{1 k+1}\|_{k}-^{}_{k}\|_{1  k}\|_{k}-^{}_{k}\|.\] (3.2)

In particular, we define \(_{}(_{h},^{}_{h})\) and \(_{}(_{h},^{}_{h})\) by viewing these as trajectories of length \(_{}-1\) and \(_{}\), respectively. Lastly, we define a per-timestep restriction of the expert distribution. In this section, we consider the case where the learner policy satisfies a total variation continuity (TVC) condition, defined below.

**Definition 3.3** (TVC of Chunking Policies).: We say that a chunking policy \(=(_{h})\) is total variation continuous with modulus \(_{}:_{ 0}_{ 0}\), written \(_{}\)-TVC, if, for all \(h[H]\) and any observation-chunks \(_{h},^{}_{h}_{_{}-1}\), \((_{h}(_{h}),_{h}(^{}_{h})) _{}(_{}(_{h},^{ }_{h}))\).

We depict the TVC property using our running left-right obstacle example in Figure 3. We stress that, in Definition 3.3, the TV bound on \((_{h}(_{h}),_{h}(^{}_{h}))\) applies to the _composite actions_ consisting of primitive controllers \(_{h}=_{t_{h}:t_{h}+_{}-1}_{h}(_{h})\); it does not upper bound the TV distance between raw control inputs. Indeed, ensuring TVC of the latter can lead to the failure modes depicted in Figure 3(b). Next, we extract an expert "policy" from the expert demonstrations.

**Definition 3.4** (Expert "policy" with synthesized controllers).: For \(h[H]\), we let \(_{,h}\) denote the joint distribution of \((_{h},_{h})\), induced by drawing a trajectory \(_{T}=(_{1:T+1},_{1:T})_{ }\) from the expert distribution, \(_{1:T}=(_{T})\) be the associated primitive controllers, letting \(_{h}=(_{t_{h}-_{h}+1:t_{h}},_{t_{h}- _{h}+1:t_{h}-1})\) be the associated observation-chunk at time \(h\), and \(_{h}=_{t_{h}:t_{h}+1}-1\) the associated composite action. We let \(_{h}^{}():()\) denote the condition distribution of \(_{h}_{h}\) under \(_{,h}\).

The conditional distributions \(_{h}^{}()\) are estimated when training a generative model to predict \(_{h}\) from observations \(_{h}\). Note that \(_{h}^{}()\) (and \(_{,h}\)) is defined in terms of _both_ expert demonstration

Figure 3: Graphical representation of total variation continuity (TVC) using the running “left mode/right mode” example. Panel _(a)_ depicts a policy \(\) which is TVC, and thus interpolates between left and right modes probabilistically. Importantly, the TVC property applies to the distribution over composite actions, i.e. sequences of _primitive controllers_, as, in Definition 3.3; this ensures, for example, that following the left mode from slightly to the right of the obstacle (purple dotted line) still stabilizes to the idealized left mode trajectory (red). In panel _(b)_, we consider a policy which for TVC applies to the sequences of _raw control inputs_ (which _is not_ what occurs in Definition 3.3). This can lead to naive mode-switching that collides with the gray obstacle.

from \(_{}\) and the associated synthesized primitive controllers. In Lemma J.6, we show that when the synthesis oracle \(_{1:T}=(_{T})\) produces primitive controllers consistent with the trajectories, than \(^{}=(_{h}^{})\) produces the same marginals over states as \(_{}\); that is, \(_{,}(^{})=0\).

**Theorem 1**.: Suppose Assumption 3.1 holds, and suppose that \(0<c_{2}\), and \(_{} c_{3}\). Then, for any non-decreasing non-negative \(_{}()\) and \(_{}\)-TVC chunking policy \(\), it holds that \(_{,}() H_{}()+_{h=1}^{H}_{_{h}_{ ,h}}_{(/c_{1})}(_{h}^{}(_{ h}),_{h}(_{h}))\), which is at most \(H_{}()+}{}_{h=1}^{H} _{_{h}_{,h}}[ _{1,_{ax}}(_{h}^{}(_{h}),_ {h}(_{h}))]\).

The above result reduces the marginal imitation error of \(\) to the sum over optimal transport errors between \(\) and \(_{h}\) chosen by the expert demonstrators. Thus, if these are small, the local stabilization properties of the primitive controllers guaranteed by Assumption 3.1 ensure that errors compound at most linearly in problem horizon. The key ideas of the proof are given Appendix D, via a general template for imitation learning of general stochastic policies. This template is instantiated with a details in Appendix J.

### A general guarantee via data noising.

To circumvent assuming that the learner's policy is TVC, we study estimating the conditionals under a popular data augmentation technique , where the learner is trained to imitate the conditional sequence of \(}_{h}\), where \(}_{h}(_{h},^{2})\) adds \(^{2}\)-variance Gaussian noise to the true observation-chunk. To understand this better, consider the following _smoothed_ policy:

**Definition 3.5** (The smoothed policy).: Let \(=(_{h})\) be a chunking policy. We define the _smoothed policy_\(_{}=(_{,h})\) by letting \(_{,h}(_{h})\) be distributed as \(_{h}(}_{h})\), where \(}_{h}(_{h},^{2})\).

Appendix J.7.2 show's that Pinsker's inequality implies noising automatically enforces TVC This suggests that we can use some form of data noising to enforce the TVC property in Definition 3.3. Let's now consider a related problem: trying to estimate the optimal distribution over composite actions _conditioned on_ a noised observation. This gives rise to a _deconvolution_ of the expert policy, which can be thought as an inverse operation of data noising.

**Definition 3.6** (Noised Data Distribution and Deconvolution Policy).: Let \(_{,h}\) be as in Definition 3.4. Define \(_{,,h}\) as the distribution over \((}_{h},_{h})\) generated by \((_{h},_{h})_{,h}\) and \(}_{h}(_{h},^{2})\). We define the _deconvolution policy_\(_{,,h}^{}(}_{h})\) as the conditional distribution of \(_{h}}_{h}\) under \(_{,,h}\).

Analogously to \(^{}\), the policy \(_{,,h}^{}\) is what a generative model trained to generate \(_{h}\) from noised observations \(}_{h}\) of \(_{h}_{}\) learns to generate. Our next theorem states that, if our \(\) approximates the idealized conditional distribution of composite actions given noised observations, then \(_{}\), the smoothed policy, imitates the expert distribution with provable bounds on its imitation error:

**Theorem 2** (Reduction to conditional sampling under noising).: Suppose Assumption 3.1 holds. Let \(c_{1},,c_{5}>0\), defined in Definition 3.2, and let \(_{}(x)\) denote a term which is upper and lower bounded by a \(x\) times a polynomial in those constants and their inverses. Then, for \(_{}(1)\), if we choose \(=/_{}(}+(1/))\) and let \(_{} c_{3}\) and \(_{}-_{}}}(c_{ 1}/)\),

\[_{,}(_{})_{}( H}}(}+( ))+_{h=1}^{H}_{}_{h} _{,,h}}[_{(^{2})}(_ {,,h}^{}(}_{h}),\,_{h }(}_{h}))],\]

which is upper bounded by at most \(_{}( H}}(}+(1/))+}_{h=1}^{H} _{}_{h}_{,,h}}[ _{1,_{}}(_{,,h}^{ }(}_{h}),\,_{h}(}_{h})) ]\).

To reiterate, Theorem 2 guarantees imitation of the distribution of marginals and final states of \(_{}\) by replacing the explicit TVC assumption with noising, and the resulting guarantee applies to the _smoothed policy_\(_{}\) which adds smoothing noise back in Appendix J gives a number of additional results. In Appendix I, we show that the proof framework, outlined in Appendix D, which under lies the proofs of Theorems 1 and 2, is essentially sharp in the worst case. Moreover, in Appendix C.3, we discuss the merits and drawbacks of our use of the synthesis oracle, and how it circumvents some of the challenges encountered in behavior cloning in past work. The key intuition behind the proof of Theorem 2 is depicted in Figure 4, and full proof sketch is deferred to Appendix C.2

## 4 Hint: Instantiating Data Noising with DDPMs

We now instantiate Theorem2 by showing that one can learn a policy \(\) for which the error terms in Theorems1 and 2 are small by fitting a DDPM to noise-smoothed data.

Our proposed algorithm, Hint (Algorithm1) combines DDPM-learning of chunked policies as in  with a popular form of data-augmentation . We collect \(N_{}\) expert trajectories, synthesize gains, and segment trajectories into observation-chunks \(_{h}\) and composite actions \(_{h}\) as described in Section2. We perturb each \(_{h}\) to form \(N_{}\) chunks \(}_{h}\), as well as horizon indices \(j[J]\) and inference noises \((0,( j_{h})^{2})\), and add these tuples \((_{h},}_{h},j_{h},_{h},h)\) to our data \(\). We end the training phase by minimizing the standard DDPM loss :

\[_{}(,)=_{(_{h}, }_{h},j_{h},_{h},h)}| |_{h}-_{,h}(e^{- j}_{h}+}_{h},}_{h},j_{h} )||^{2}.\] (4.1)

Our algorithm differs subtly from past work in Line8: motivated by Theorem2, we add smoothing noise _back in_ at test time. Here, the notation \((_{,h},)(_{h}, ^{2})\) means, given \(_{h}\), we perturb it to \(}_{h}(_{h},^{2})\), and sample \(_{h}(_{,h},}_{h})\).

We now state an informal guarantee for Hint, deferring a formal statement to AppendixC.5.

**Theorem** (Informal Theorem).: Suppose that the system dynamics are smooth and that Assumption3.1 holds for the linearized system. Then there is a choice of the parameters in Hint that is polynomial in all problem parameters such that for \(N_{}\), polynomially large in problem parameters, \(_{,}(_{}) ( H}}(}+(1/) ))\) with high probability.

### Experimental Results

In this section, we demonstrate the benefits of diffusing low level controllers, and of our approach to data noising. We explain the environments in greater detail, along with all training and compu

Figure 4: Multi-modal demonstrations traverse an obstacle left or right, exhibiting a pure bifurcation. We consider perturbing expert data on the right mode (blue circle) to a noised datum (gray circle). We show that generative behavior cloners learn to deconvolve this noise, creating a virtual “replica” sample (red circle) following the left mode, such that the replica and original are i.i.d. given the noised one. When the red circle’s primitive controllers are rolled from from the blue circle, this leads to a trajectory (yellow circle) which interpolates across the bifurcations. Marginalizing over this process, the yellow trajectories probabilistically interpolate between red and blue modes, and (approximately) match the per-time-step marginal over expert distributions.

tational details in Appendix O. 3 Figure 5 compares the performance of diffusing (chunks of) raw control inputs to diffusing (chunks of) gain matrices for a canonical model of a 2-d quadrotor. We find that diffusing gain matrices yields dramatic improvements in performance, in particular allowing a _single imitated trajectory_ to outperform learning raw control inputs from 10 demonstrations.

Next, empirically evaluate the effect on policy performance of our proposal to inject noise back into the dynamics at inference time. We consider three challenging robotic manipulation tasks studied in prior work: PushT block-pushing ; Robomimic Can Pick-and-Place and Square Nut Assembly  (we direct the reader to Chi et al.  for an extensive empirical investigation into the performance of diffusion policies in the un-noised \(=0\) regime). We display the results of our experiments in Figure 6. Observe that the performance degradation of the replica policy from the unnoised \(=0\) variant is minimal across all environments and even leads to a slight but noticeable improvement in the small-noise regime for PushT (and low-data Can Pick and Place). In the presence of non-negligible noise Hint significantly outperforms the conventional policy \(\) (obtained by noising observations at training but not test time), as predicted by our theory.

## 5 Discussion

This work considerably loosened assumptions placed on the _expert distribution_ by introducing a synthesis oracle responsible for stabilization. How best to achieve low-level stabilization remains an open question. We hope that this work encourages further empirical research into improving the stability of imitation learning, either via the hierarchical route proposed in this paper or via new innovations.

Figure 5: Performance of diffusing chunks of actions \(}_{t_{h-1}:t_{h}-1}\) (”No Gains”) versus jointly diffusing actions \(u_{t_{h-1}:t_{h}-1}\), reference states \(}_{t_{h-1}:t_{h}-1}\) and gains \(_{t_{h-1}:t_{h}-1}\) for a 2-D quadrotor system with thrust-and-torque-based control. Different noise levels \(\) and number of trajectories \(N\) are shown. Mean and standard deviation are shown across \(5\) training seeds.

Figure 6: Performance of baseline \(\) and noise-injected \(_{}\) Hint policy for different \(\). We use \(4\) training seeds with 50 and 22 test trajectories per seed for PushT and Can and Square Environments respectively. Mean and standard deviation of the test performance on the 3 best checkpoints across the 4 seeds are plotted. The \(\) values correspond to noise in the normalized \([-1,1]\) range.