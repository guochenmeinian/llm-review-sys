# UltraMedical: Building Specialized Generalists in Biomedicine

Kaiyan Zhang\({}^{,}\)  Sihang Zeng\({}^{}\)  Ermo Hua\({}^{,}\)  Ning Ding\({}^{}\)1  Zhang-Ren Chen\({}^{}\)

**Zhiyuan Ma\({}^{}\)  Haoxin Li\({}^{}\)  Ganqu Cui\({}^{}\)  Biqing Qi\({}^{}\)  Xuekai Zhu\({}^{}\)  Xingtai Lv\({}^{,}\)  Jin-Fang Hu\({}^{}\)  Zhiyuan Liu\({}^{}\)  Bowen Zhou\({}^{}\)1 \({}^{}\)**

\({}^{}\) Tsinghua University \({}^{}\) University of Washington

\({}^{}\) The First Affiliated Hospital of Nanchang University

\({}^{}\) Shanghai Jiao Tong University \({}^{}\) Frontis.AI

_zhang-ky22@mails.tsinghua.edu.cn {dn97, zhoubowen}@tsinghua.edu.cn_

###### Abstract

Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas. Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges. The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization. However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data. In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks. Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community.

GitHub: https://github.com/TsinghuaC3I/UltraMedical

Huggingface: https://hf.co/collections/TsinghuaC3I

## 1 Introduction

The advent of Large Language Models (LLMs) has brought forth numerous potential applications in the field of biomedicine and healthcare, encompassing medical education, clinical practice, and scientific research. Recent studies suggest that proprietary models such as GPT-4, Med PaLM 2, and MedGemini have the potential to function as integrated medical generalists , even achieving expert-level performance on some medical benchmarks. In the meantime, although there have been advancements, open-source LLMs fine-tuned on synthetic medical instructions still significantly lag behind proprietary models .

Despite the remarkable capabilities, proprietary models may face security and privacy challenges due to the sensitive nature of medical data, such as potential data breaches and the risk of exposing sensitive patient information . On the other hand, open-source LLMs can be customizedand adapted to specific healthcare contexts by fine-tuning on local datasets, enabling the development of models tailored to the needs of specific patient populations, healthcare settings, or research questions, thereby enhancing their practical utility and impact. Exploring how to build open-source, GPT-4-level LLMs in the field of biomedicine is underway. Beyond supervised fine-tuning, preference learning technologies like Reinforcement Learning from Human or AI Feedback(RLHF or RLAIF) , direct preference optimization (DPO) , Kahneman-Tversky Optimization (KTO)  and others  has proven to play a significant role in enhancing the reasoning abilities of open LLMs in various tasks such as coding, mathematics, and logic . However, preference learning remains under-explored in the biomedical community , which is mainly limited by the scarcity of high-quality and extensively annotated preference datasets.

In this paper, we investigate the development of specialized generalists in the field of biomedicine from a data-centric perspective. We first construct a large-scale, diverse, and high-quality dataset by combining manual and synthetic biomedical instructions, which comprise medical exam problems, PubMed literature research, and open-ended questions. We then build on the outputs of various LLMs to painstakingly annotate these instructions, along with corresponding preference scores and rankings, to ultimately create our UltraMedical dataset. By leveraging UltraMedical and previous open-domain datasets such as UltraChat , we further explore how to fuse professional skills with general skills and then fine-tune the Llama-3 family of models to produce competitive medical models. Additionally, we train a reward model based on UltraMedical preferences annotations and previous feedback datasets  achieving advanced results in both our annotated medical benchmark and RewardBench . Based on the preferences of the constructed reward models, we continuously optimize the UltraMedical LMs through a self-generated response strategy, and finally result in more powerful models. Finally, our 8B model significantly outperforms previous larger models such as MedPaLM 1 , Gemini-1.0 , GPT-3.5, and Meditron-70B  in terms of average score on popular medical benchmarks. Moreover, our 70B model achieved an 86.5 on MedQA-USMLE, marking the highest result among open-source LLMs and comparable to MedPaLM 2  and GPT-4.

Specifically, our paper makes the following contributions:

* We construct the UltraMedical collections, a high-quality collection of about 410K medical instructions that adhere to principles of complexity and diversity. This dataset combines manual and synthetic prompts. A subset of approximately 100K instructions within UltraMedical has been annotated with preferences over completions from advanced medical and general models, contributing to fine-tuning, reward modeling, and preference learning.
* By fine-tuning the Llama-3 series on UltraMedical using a multi-step optimization strategy, as described in SS 3, we achieved competitive results in open-source medical benchmarks with Llama-3-8B/70B, detailed in SS 4. The results indicate that we can narrow the gap between open-source and proprietary models using the UltraMedical collections.
* Building upon UltraMedical preference data, we annotate the medical reward bench with the help of biomedical experts in SS 3. We also pioneer the training of reward models in biomedicine based on UltraMedical preferences, resulting in advanced performance on

Figure 1: The UltraMedical Datasets, Models and Performance on MedQA.

both annotated medical and general reward benchmarks in SS 5. This initiative significantly contributes to further online or iterative preference learning in this field.
* We release our datasets and our models to the public on both GitHub and Huggingface, aiming to foster collaboration and accelerate progress in the field of biomedical generative AI by providing valuable resources to the research community.

## 2 The UltraMedical Dataset

The UltraMedical dataset initially comprises a large-scale collection of approximately 410,000 high-quality medical instructions that combine manual and synthetic prompts. These prompts are partially created by us and selected from open sources, which are produced from the guidance of principles of diversity and quality. Secondly, the dataset includes about 110,000 instructions annotated with completions from various LLMs with preferences annotated by GPT-4. Thirdly, a subset of approximately 900 model-annotated preference pairs has been reviewed and corrected by human experts, forming the basis of the medical reward benchmark. In the following sections, we will first introduce the details of the UltraMedical collections as shown in Figure 2, including instruction composition in SS 2.1 and data annotations in SS 2.2, and dataset statistics in SS 2.3, respectively.

### Instruction Composition

#### 2.1.1 Principle of Diversity

UltraMedical comprises a variety of question types, including medical exam questions, literature-based questions, and open-ended instructions (clinical questions, research questions, and others). It comprises 10 manual and synthetic datasets. For publicly available datasets, we have gathered questions from multiple sources, including medical exams, medical literature, clinical questions, and open-ended instructions. These datasets feature not only manually curated instructions but also prompted instructions from GPT-4. The various data sources preliminarily enable the diversity principle of the UltraMedical dataset.

In addition to public datasets, we have created three synthetic datasets to augment the UltraMedical collection. Due to the high quality of questions in MedQA , we regard MedQA questions as a primary seed source. The first dataset, MedQA-Evol, is synthesized and evolved from the original MedQA data. The second dataset, TextBookQA, consists of multiple-choice questions derived from medical textbooks, using questions from MedQA as in-context examples. The last dataset, WikiInstruct, aggregates thousands of biomedical concepts from Wikipedia pages and expands them into more detailed knowledge and instructions. As visualized on Nomic AI Atlas in Figure 3, the diversity of the topics in the UltraMedical prompts validates the effectiveness of the aforementioned process. We provide details about each data source along with examples in the Appendix C and E.

Figure 2: The Construction Pipeline for the UltraMedical Dataset.

#### 2.1.2 Principle of Complexity

Beyond the diversity characteristic, UltraMedical also upholds the principle of complexity to inject knowledge and enhance reasoning abilities through complex instructions. There are primarily two routes to enhance the complexity of instructions, either pre-hoc or post-hoc. The former involves starting with various seed instructions to synthesize new instructions, followed by employing self-evolution on these synthetic instructions [72; 41]. The latter involves filtering instructions using heuristic rules or model-based rankers to select the most complex instructions [8; 81].

During the construction of the UltraMedical dataset, we employ both pre-hoc and post-hoc methods to enhance the complexity of the instructions. For publicly available datasets, we use gpt-3.5-turbo to assign a scale score ranging from 1 to 10 to each instruction, where 1 indicates an instruction that is easy to answer and 10 denotes one that is challenging for ChatGPT. For our synthetic dataset, we combine pre-hoc and post-hoc methods to ensure the complexity of the instructions. Initially, we implement a two-step self-evolution process on all synthetic instructions, and then further filter them based on model-derived scores. As illustrated in Table 1, there exists a strong correlation between the length and scores of instructions, with longer instructions often containing more entities and requiring the assistant to reason over context. However, direct linear relationship is not observed between these two metrics. Despite this, it is still necessary to employ a judger to filter out poor-quality instructions, even if they are lengthy. This finding is consistent with previous works [60; 82].

### Data Annotation

#### 2.2.1 Completions Annotation

After compiling diverse instructions, we annotate answers using gpt-4-turbo to optimize these responses for SFT. For multiple-choice questions, the chain-of-thought (CoT)  method has proven effective in distilling knowledge from large to small language models. Therefore, we instruct gpt-4-turbo to sequentially answer each question. Subsequently, we verify the answers against the ground truth and filter out incorrect responses. For incorrect answers, we further engage gpt-4-turbo with dynamically retrieved few-shot CoT examples from our annotated database. This process enables us to maximize the number of potential candidate samples while ensuring the quality of the completions.

#### 2.2.2 Preference Annotation

Recently, an increasing number of studies have committed to building preferences in both general and specialized domains such as mathematics and coding. The primary strategy for obtaining completion candidates include: sampling several models from a mixed-scale model pool to compose completion candidates, sampling responses from a powerful base model and GPT-4, or simply sampling from the SFT model. There is no conclusive evidence to determine which strategy is the most effective. We sample responses from the top-tier open-source and proprietary models for preference annotation. For proprietary models, we just adapt gpt-3.5-turbo and gpt-4-turbo. For open-source

  
**Category** & **Synthetic** & **Dataset** & **\# Original** & **Avg.Len** & **Avg.Score** & **\# Related** \\   & ✗ & MoQA & 10.2K & 128.94 & 7.35 & 9.3K \\  & ✗ & MoQA & 10K & 23.12 & 4.73 & 99K \\  & ✓ & MoQA-Fool & 51.8K & 75.25 & 8.07 & 51.8K \\  & ✓ & TextInfoQA & 91.7K & 75.92 & 7.72 & 91.7K \\   & ✗ & PathMoQA & 211K & 218.2 & 7.95 & 88.7K \\   & ✗ & PathCoT & 100K & 96.93 & 6.83 & 31.1K \\  & ✗ & MoQA-Fool & 40K & 82.1 & 4.34 & 6K \\   & ✓ & MoTMowT-52K & 52K & 36.05 & 5.25 & 2NK \\   & ✓ & MoTMowT-12K & 120K & 84.93 & 5.36 & 2NK \\   & ✓ & WildNeNeur & 2PK & 46.73 & 5.8 & 2NK \\   & **Interactive-Goes** & - & 101.63 & 8.2 & **440K** \\   & **Proferences Pairs** & **1.8M** & - & - & **100K** \\   

Table 1: Instructions Statistics. Datasets marked with “ ✗” represent our customized synthetic data, while the others are adapted from publicly available data. Average length and score by ChatGPT noted as _Avg.Len_ and _Avg.Score_.

Figure 3: Broad Topics Distributionmodels, we select L1ama-3-8B/70B , Qwen1.5-72B , Mixtral-8x7B/22B , along with our supervised finetuned UltraMedical 8B model. Subsequently, we use GPT-4 to rank the candidates based on score and explanation. However, there may be a bias in GPT-4 towards its own responses [50; 75]. Therefore, we choose the newest version of GPT-4 to score the completions, which is gpt-4-2024-04-09. More scalable and reliable annotation methods, such as fact-checking with search tools , could be employed, and we leave this exploration for future work.

**Preference Binarization:** For subsequent preference learning like DPO, binarization of preferences is necessary, involving a pair comprising a "chosen" and a "rejected" completion for each sample. Following the Zephyr protocol , the highest-ranked completion is selected as the "chosen" one. In instances where multiple completions share the top ranking or scores, the completion from GPT-4 is favored. Subsequently, a random completion from the remaining entries, excluding the top-ranked ones, is designated as the "rejected" completion.

**Medical RewardBench:** Drawing inspiration from RewardBench , which evaluates reward models using a variety of prompts and paired responses, we build _Medical RewardBench_. First, we randomly select 1,000 samples from all preference samples and set them aside from the training data. We then categorized the 1,000 samples into "easy", "hard", and "length" pairs according to the model's scores from GPT-4, while 100 samples for each sub-task. Finally, we obtain pairs for annotation and corrected the preferences with scores and ranks from GPT-4. To ensure the accuracy of the preference pairs, we engage biomedical clinicians, graduate students, and researchers in correcting the preferences. Beyond the Easy, Hard, and Length sets, we also allocate a portion of the samples to the Human set, which consists of samples revised by humans and potentially presents greater challenges. Further discussion is presented in SS 5 and Appendix C.3.

**Human Annotation:** To ensure the reliability of the medical reward benchmark, we assembled a team of three experts, each with at least three years of research experience in biomedicine. They utilized a customized WebUI and academic search engines to validate question-answer pairs. For the reward benchmark, out of 1,000 test samples, only about 780 were retained where at least two annotators agreed on the same label. Samples with disagreements or both incorrect answers were removed. We provide more details about human annotation in Appendix C.4.

**Annotation Cost:** The costs associated with creating the dataset and benchmark primarily include GPT-4-Turbo API (version 1106) calls for instruction synthesis and response generation, as well as preference annotation, totaling approximately $20,000.

### Dataset Statistics

**Overall:** As illustrated in Table 1, the UltraMedical collections ultimately comprise 410K instructions. For the preference annotation, we select the instructions with the highest scores from each dataset, resulting in approximately 100K instructions accompanied by eight models' completions. During the preference binarization process, we aim to maximize the selection, achieving \(C_{8}^{2}\) = \(28\) combinations of "chosen" and "rejected" completions per instruction. Although we retain only completions with differing scores, we ultimately obtain approximately 1.8M pairs for reward modeling (approximately 18 times the size of the instruction.). We provide more details in Appendix C.

**Medical RewardBench:** For the initially given 1,000 test pairs, we ultimately retained 777 pairs following human expert annotation. These include 238 easy, 196 hard, 180 length-based, and 163 human-judged pairs. Approximately 233 pairs were filtered out due to issues such as incorrect formulations, difficulty in answering, or both. The human category comprises pairs where preferences differ between human annotators and GPT-4, which is regraded as even hard for GPT-4 to recognize.

## 3 The UltraMedical Suites

Based on the UltraMedical datasets, we develop the UltraMedical LMs and a reward model (RM) based on Llama-3 models using the following four steps: supervised fine-tuning in SS 3.1, preference learning in SS 3.2, reward modeling in SS 3.3, and iterative preference learning in SS 3.4.

### Supervised Fine-Tuning

We conduct supervised fine-tuning (SFT) on the Llama-3 8B and 70B base models using the UltraMedical collection, resulting in Llama-3-8B/70B-UltraMedical. Given the uniform format of the completions, we employ responses from gpt-4-turbo for SFT, which consistently provide the highest quality across various sources. To enhance general instruction-following capabilities, we integrate UltraMedical with general domain datasets such as UltraChat , ShareGPT , Open-Orca [38; 45] and others. There is about 410K medical-domain and 190K open-domain samples. We retain instructions that achieve high evaluation scores in 0-hero/Matter-0.1 project2.

### Preference Learning

Building on the UltraMedical preferences annotation and the SFT version of UltraMedical LMs, we explore various preference learning technologies, including DPO  and KTO . As detailed in Section 2.2.2, each instruction in UltraMedical is associated with eight completions, yielding a maximum of \(C_{8}^{2}\) pairs, which is approximately 20 times the size of the instruction set used for SFT. Due to computational limitations, we utilized only the binarized version of the preference data, consisting of about 100K instructions (noted as _UltraMedPref_), where each instruction includes one chosen and one rejected response. Similarly to SFT, we incorporated the general preference datasets including UltraFeedback, UltraInteract, and UltraSafety to maintain broad capabilities, totaling approximately 75K instructions (named as _UltraMixPref_).

### Reward Modeling

The reward model is a crucial component in technologies such as Reinforcement Learning from Human Feedback (RLHF), Rejected Sampling Fine-tuning (Interactive SFT), Iterative Direct Preference Optimization (Iterative DPO), and other continuous alignment methods. To further enhance medical language models, we train a reward model (RM) for continual alignment. The RM is trained using the preference data outlined in SS 2.2.2. Besides of preference data from UltraMedical, we also augment training with UltraFeedback , UltraSafety  and UltraInteract  datasets to enhance its capabilities in general chat, safety, and reasoning. Subsequently, this model is employed to label responses from UltraMedical LMs and provide "on-policy" completion pairs for prefernce learning. It can also be used to evaluate numerous decoding candidates in massive sampling scenarios.

### Iterative Preference Learning

Based on the reward model, we implement online preference learning and Best of N (BoN) sampling to further enhance the UltraMedical LMs, which can be synergistically combined to boost performance.

**Online Preference Learning:** After supervised fine-tuning on a mixture of general and medical domain instructions, we obtain the UltraMedical LM with parameters \(_{0}\). Subsequently, we conduct

Figure 4: Process of Online Preference Learning.

inference on a mixture of instructions using \(_{0}\) and annotate the generated completions and references as "chosen" and "rejected" answers using a reward model. We then perform preference learning on the on-policy preference data, resulting in \(_{1}\). This procedure is repeated \(K\) times, culminating in the final UltraMedical LM with parameters \(_{K}\).

**Best of N (BoN) Sampling:** Self-consistency is a useful method for enhancing model performance across various tasks. Previous studies, such as MedPrompt  and MedPaLM , have adapted self-consistency to achieve superior outcomes in medical QA tasks. Rather than merely voting for the majority, we employ a reward model to select the best completion from N sampling candidates. BoN sampling can be applied not only during inference but also throughout training, thereby enabling the selection of potentially better answers and refining the model's behavior.

## 4 Evaluation of UltraMedical LMs

### Experimental Setup

**Medical domain benchmarks:** To assess the specialized capabilities of UltraMedical-based LLMs within the medical field, we evaluated these models using well-known medical question-answering benchmarks, as utilized in MedPaLM experiments. These benchmarks include MedQA , PubMedQA , MedMCQA , and the medical categories in MMLU . We selected the

   &  **MedQA** \\ **(US 4-opt)** \\  } &  **MedMCQA** \\ **(Dev)** \\  } &  **PubMedQA** \\ **(Resonizing)** \\  } &  **Clinical** \\ **knowledge** \\  } &  **Medical** \\ **gnostics** \\  } &  &  **Professional** \\ **medicine** \\  } &  **College** \\ **biology** \\  } &  &  \\    categories in MMLU based on previous works, which mainly comprise Clinical Knowledge, Medical Genetics, Anatomy, Professional Medicine, College Biology, and College Medicine. In addition to these medical multiple-choice questions (MCQs), we also report results on free-form clinical questions task, named K-QA . Details of these benchmarks are displayed in Appendix C.6.

**General domain benchmarks:** We evaluated the general capabilities of the models on benchmarks related to general-domain chat (MT-Bench  and Alpaca-Eval ), general MCQs (MMLU  and GPQA ), and mathematical tasks (MATH  and GSM8k ).

**Evaluation metrics:** For multiple-choice QA tasks, we use the accuracy metric. For free-form QA, we use GPT-4 as a human proxy to evaluate the results from multiple aspects. Further details about the evaluation benchmarks are available in Appendix C.6.

**Baseline Models:** We select a range of baseline models from both proprietary and open-source categories, encompassing general and medical domains. In the proprietary category, we choose GPT3.5 and GPT-4 as generalist models, and MedPaLM and MedGemini from the medical domain. In the open-source category, we include models such as Qwen , Mixtral , DeepSeek  and the Llama series. We also conduct comparisons with advanced medical variants, like Med42 , BioMistral , Meerkat , and Internist 3 and OpenBioLLM . For models marked with an asterisk (*), we conduct experiments and gather results directly. Other results are adapted primarily from the literature, mainly in MedPrompt . And "Ens" denotes an ensemble with 10 self-consistency responses, maintaining consistency with previous MedPrompt papers.

**Implement Details:** We apply two data settings for SFT and preference learning, where _UltraMed_ only contains 410K instructions UltraMedical and _UltraMix_ contains totally 600K instructions with additional 190K from general domain datasets mainly including UltraChat , Open-Orca , and EvolInstruct . For preference learning, we note training on 100K _UltraMedPref_ and 75K _UltraMixPref_ as _Vanilla_ versions, and on these instructions with annotated sampling completions as _Iterative_ versions. More training details are provided in Appendix B.

### Main Results

As shown in Table 2, the UltraMedical series, particularly the 8B models, achieve advanced performance on medical benchmarks, demonstrating the effectiveness of the UltraMedical instructions and preference datasets. To gain a deeper understanding of the results, we conducted further analyses from three perspectives: 1) the impact of incorporating open-domain instructions and preferences for Supervised Fine-Tuning (SFT) and various Preference Optimization (xPO) techniques; 2) the effectiveness of online preference learning across small and large language models (SLMs and LLMs); and 3) the trade-offs in performance between the medical and general domains.

**Dataset Mixture for SFT and xPO:** As shown in Table 2, UltraMedical LMs under the _UltraMed_ settings achieve advanced performance on average scores. The models perform slightly better with the _UltraMix_ datasets. This evidence supports the conclusion that a data mixture of both medical and open domains enhances both SFT and xPO processes. This also suggests that LLMs may require

    &  &  &  &  &  &  \\  & **Comp.** (\(\)) & **Hall.** (\(\)) & **GPT-4** & **LC (\%)** & **WR (\%)** & **5-shot** & **0-shot** & **8-shot** & **CoF** \\  Mixtral-7B-Instruct & 0.5335 & 0.2090 & 6.84 & 17.1 & 14.7 & 58.4 & 26.3 & 39.9 \\ Llama-3-B-Instruct & 0.6037 & 0.1940 & 8.10 & 22.9 & 22.6 & 68.4 & 34.2 & 79.6 \\ OpenBioLM-8B & 0.3135 & 0.1194 & 4.38 & 0.06 & 0.25 & 44.2 & 24.8 & 41.6 \\ ✖ UltraMedLM 8B & 0.7242 & 0.0945 & 7.64 & 30.7 & 31.9 & 68.1 & 34.2 & 75.9 \\  Mixtral-8x7B & 0.6617 & 0.1343 & 8.30 & 23.7 & 18.3 & 70.6 & 39.5 & 93.0 \\ Llama-3-70B-Instruct & 0.6545 & 0.1357 & 9.01 & 34.4 & 33.2 & 82.0 & 39.5 & 93.0 \\ OpenBioLM-70B & 0.5951 & 0.1100 & 8.53 & 30.8 & 31.0 & 60.1 & 29.2 & 90.5 \\ ✖ UltraMedLM 70B & 0.6077 & 0.0896 & 8.54 & 33.0 & 32.1 & 77.2 & 39.7 & 88.7 \\  GPT-3.5-Turbo (1106) & 0.6208 & 0.0746 & 8.32 & 19.3 & 9.2 & 70.0 & 28.1 & 57.1 \\ GPT-4-Turbo (1106) & 0.6390 & 0.1095 & 9.32 & 50.0 & 50.0 & 86.4 & 49.1 & 92.0 \\   

Table 3: Performance metrics of different open-source models across various general benchmarks.

general capabilities to solve specialized domain problems, underscoring the necessity for specialized generalists. Better mixture strategy for general and specialized data still requires exploration.

**Offline and Online Preference Learning:** The results in Table 2 indicate that the constructed preference data can enhance the performance of the 8B and 70B models through various Preference Optimization (xPO) techniques. However, the improvements are not particularly significant, especially for larger models like the 70B. The primary reasons for this lie in the differences between offline and online optimization. Although completions from advanced models are obtained, there still exists a distribution mismatch for advanced models like Llama-3. To further enhance performance, it would be beneficial to sample completions from the model itself and then apply rewards with a reward model. Further exploration of transitioning preference learning from offline to online is necessary.

**Trade-off Performance in Medical and Open Domain:** As illustrated in Table 2 and Table 3, the UltraMedical LMs benefit from a mixture of medical and general domain datasets during the Supervised Fine-Tuning (SFT) and various Preference Optimization (xPO) processes. This strategy enhances performance on medical tasks but slightly reduces results on general domain benchmarks, highlighting the potential and necessity of developing specialized generalists. This noticeable performance trade-off warrants further investigation into the principles of data mixing and its influence on downstream performance in both specialized and general tasks.

## 5 Evaluation of Reward Models

### Setup

**Benchmark:** To assess the rewarding capabilities in the general domain, we adapted the AllenAI RewardBench, which features a variety of prompts from categories such as Chat, Chat Hard, Safety, and Reasoning. Considering that many models were trained on the prior preference dataset, we have excluded results from those prior sets in RewardBench. Furthermore, to evaluate the effectiveness of the UltraMedical reward models alongside general domain reward models in the medical domain, we conducted assessments using the UltraMedical preference dataset constructed in SS 2.2.2.

**Models:** We primarily compared the performance of typical models on RewardBench, including UltraRM , Starling-RM , Eurus-RM , and LlaMA3-RM . These reward models, along with our UltraMedical RMs, are well-suited for large-scale reward computations. Simultaneously, we also compared pairwise models like PairRM-LLaMA3 . Although this model achieves high performance, it fails to scale up due to the limitations of pairwise comparison.

### Main Results

**Performance on RewardBench:** As illustrated in Table 4, the UltraMedical RM trained sorely on Ultra-Series datasets performs competitively in both medical and general reward benchmarks. While some models exhibit strong performance on the general RewardBench, they show weaknesses in

    &  &  \\  & **Easy** & **Hard** & **Human** & **Length** & **Avg.** & **Chat** & **Chat** & **Hard** & **Safety** & **Reasoning** & **Avg.** \\  openbank/UltraRM-13b & 90.34 & 73.98 & 69.33 & 66.67 & 75.08 & 96.40 & 55.50 & 56.00 & 62.40 & 67.58 \\ openbank/Eurus-RM-7b & 89.50 & 72.96 & 73.01 & 68.33 & 75.95 & 98.04 & 62.72 & 81.89 & 89.38 & 83.01 \\ slar/XCF/Star/XL-MA3-RM-v0.1 & 92.86 & 70.41 & 73.62 & 67.22 & 76.03 & 99.16 & 64.69 & 86.89 & 90.64 & 85.34 \\ RLL/High/PairRM-LLaMA3-8B & 95.80 & 72.70 & 74.85 & 70.56 & 78.48 & 98.30 & 65.80 & 89.70 & 94.70 & 87.13 \\    &  &  &  &  &  &  &  &  &  &  \\   

Table 4: Performance of Reward Models on UltraMedical and RewardBench.

    &  &  &  \\  & **Greedy** & **SC** & **ULRM** & **Gen.RM** & **Greedy** & **SC** & **ULRM** & **Gen.RM** & **Greedy** & **SC** & **ULRM** & **Gen.RM** \\  Llama-38B-Instruct & 68.56 & 71.45 & 71.40 & 62.89 & - & - & - & - & - & - & - & - & - & - \\ Llama-38B-UltraMed & 75.20 & 78.33 & 78.67 & 78.25 & 76.61 & 78.28 & 78.31 & 76.60 & 76.61 & 77.61 & 77.81 & 76.80 \\ Llama-38B-UltraMix & 75.90 & 78.40 & 79.52 & 77.76 & 77.29 & **78.32** & 70.02 & 77.14 & 76.74 & 77.98 & 77.21 & 75.68 \\ Llama-370B-Instruct & 82.66 & 83.71 & 83.74 & 81.38 & & & & & & & - & - \\ Llama-370B-Instruct & 84.62 & 86.48 & 85.61 & 85.36 & 85.57 & 86.41 & 86.27 & 85.56 & 85.35 & 86.43 & 86.18 & 85.59 \\ Llama-370B-IntraMix & 84.27 & 86.92 & 85.30 & 85.17 & 85.56 & 86.11 & 85.62 & 85.12 & 85.84 & **86.49** & 85.84 & 85.56 \\   

Table 5: Comparative performance of self-consistency (SC) and reward model (RM) sorting.

the medical domain. The narrowing gap between models in the medical domain, compared to the general domain, suggests potential overfitting in the general domain and underscores the necessity of developing reward models specifically for the medical domain.

**Contribution to Online Preference Learning:** As demonstrated in Table 2, UltraMedical RM is effective for online/iterative preference learning methods such as DPO and KTO. Unlike the vanilla xPO settings, which utilize annotated preferences by GPT-4 and completions from multiple models, iterative xPO uses only the model's own completions, annotated by reward models. Due to computational limitations, we conducted only one round of annotation, but we plan to explore further steps like self-rewarding  in future work.

**Results of Re-ranking:** As shown in Table 5, reward models are not only useful for providing feedback in preference learning but also for re-ranking candidates. Our findings indicate that reward models outperform self-consistency ensembles with 8B models but are less effective in supervising 70B models, although they still facilitate preference learning. This underscores the necessity for future research to explore the re-ranking of massive candidates and the selection of the most positive ones to enhance specialized abilities, particularly focusing on weak to strong supervision .

**Challenges in Medical Rewarding:** In our implementations, preferences from GPT-4 are utilized to train reward models. While this AI-generated feedback is effective in the general domain, it shows some limitations in the medical domain. The UltraMedical reward benchmark indicates there is substantial room for improvement, as shown by performance on the Hard, Human, and Length sets. We plan to focus on enhancing domain-specific reward models in future work. Additionally, results in Table 5 reveal weaknesses in reward models, suggesting that the scalability of model size for reward applications  requires further validation.

## 6 Conclusion

In this paper, we introduce the UltraMedical datasets, comprising 410K high-quality instructions--a mix of synthetic and manual inputs--within the biomedical domain, which also includes 100K preference annotations. Utilizing the UltraMedical datasets, we conducted SFT and xPO on the Llama-3 series models, blending medical and general domain inputs. The outcomes across various medical and general domains demonstrate the superior performance of our models, validating the effectiveness of our datasets and underscoring the necessity of specialized generalists.

**Limitations and Future Directions** This paper acknowledges limitations related to using GPT-4 annotations, which may introduce bias. Instead, we could leverage powerful open-source models like Llama-70B to construct instructions using the pipeline described in the paper. Rather than directly using GPT-4's answers, we propose using only the instructions to implement self-rewarding alignment. Additionally, our work on iterative preference learning faces challenges due to limited resources, which presents an opportunity for further exploration in the future. Reward models are a critical component for the self-evolution of models; future research could focus on developing more robust reward models, utilizing our medical reward bench as a testbed. We believe the UltraMedical suites could pave new avenues in biomedicine.