# Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations

Lifan Yuan\({}^{1}\), Yangyi Chen\({}^{2}\), Ganqu Cui\({}^{1}\), Hongcheng Gao\({}^{3}\),

**Fangyuan Zou\({}^{4}\), Xingyi Cheng\({}^{4}\), Heng Ji\({}^{2}\), Zhiyuan Liu\({}^{1}\), Maosong Sun\({}^{1}\)**

\({}^{1}\) NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing

\({}^{2}\) University of Illinois Urbana-Champaign

\({}^{3}\) University of Chinese Academy of Sciences \({}^{4}\) Tencent

lievanyuan173@gmail.com

Corresponding Author.

###### Abstract

This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce **BOSS**, a **B**enchmark suite for **O**ut-of-distribution robustness**SS** evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at https://github.com/lifan-yuan/OOD_NLP.

## 1 Introduction

Pretrained language models (PLMs) have excelled in downstream tasks and gained widespread adoption . However, existing evaluation often assumes independent and identically distributed (i.i.d) condition , which is often violated in real-world scenarios, highlighting the crucial problem of out-of-distribution (OOD) robustness in NLP models. In this paper, we first revisit the evaluation of PLMs through an examination of evaluation benchmarks. Thereafter, we delve into the ID-OOD performance correlation of fine-tuned models by adopting various model scales, training steps, available training samples, and tunable parameters. Finally, we conduct extensive evaluations of current robustness-enhanced methods and large language models (LLMs).

**Definition.** There exist multiple definitions of OOD in literature , and we define distribution shifts considered in this paper from two perspectives. Firstly,  classifies distribution shifts into "semantic shift" and "background shift". Our use of "out-of-distribution" aligns with the concept of"background shift", which involves changes in the domain or style of the text while preserving the semantic content. Secondly,  formally defines three types of distribution shifts: covariate shift, label shift, and concept shift. In our work, we mainly focus on the combination of covariate shift and concept shift. This indicates that the model needs to generalize well to different input features (a.k.a, covariate shift) and adapt to variations in the underlying concepts within the data (a.k.a, concept shift).

**Benchmark.** Our study begins by surveying existing literature on OOD robustness in NLP (Table 8). We observe a lack of standardized OOD benchmark suites tailored for NLP since there is no single uniform set of datasets for evaluation, resulting in the adoption of heuristic and popularity-based dataset selection strategies in previous work . This approach suffers from two main drawbacks: (1) The selected OOD datasets may come from similar distributions as the ID dataset, reducing the OOD evaluation to ID settings and thus hindering rigorous OOD robustness evaluation; (2) The challenge stemming from distribution shifts is limited, deviating from the expectation for difficulty posed by benchmark construction principles  and potentially leading to an overestimation of the OOD robustness of language models. As a result, current OOD robustness benchmarks may inadequately assess NLP models.

To address the aforementioned concerns, we establish a protocol as shown in Figure 1, consisting of three fundamental principles, for selecting both ID and OOD datasets: (1) ID datasets should be large and diverse for comprehensive knowledge; (2) Selection for OOD datasets should prioritize distinct distributions and dissimilarity, regarding text sources and semantics; (3) Challenging distribution shifts should be prioritized based on performance degradation, to ensure that the benchmark stands the test of time . Based on the protocol, we compile **BOSS**, a more holistic and challenging NLP **B**enchmark suite for OOD robustne**SS** evaluation. Unlike existing benchmarks which only consider single task types such as classification  or reading comprehension , BOSS covers a wider range of task formats, including sentiment analysis, toxic detection, and natural language inference for classification, name entity recognition for structured prediction, and extractive question answering for reading comprehension. We establish one ID and three corresponding OOD datasets for each task.

**Analysis.** We recognize the lack of analysis of models' learning behaviors regarding ID performance and OOD generalization in the field of NLP, hindering the development and understanding of OOD robustness. Thus, we investigate the correlation between the performance on ID and OOD datasets using the BOSS benchmark. To regulate the ID performance, we manipulate four related factors, i.e. model scale, training steps, available training samples, and tunable parameters. Three typical categories of ID-OOD correlation are observed, namely, monotonic linear positive correlation, monotonic piecewise linear positive correlation, and non-monotonic V-shaped correlation (see Figure 2). We discuss the potential reasons for the causes of these identified correlations in section 3.

**Evaluations.** After examining the learning mechanism of PLMs in vanilla fine-tuning, we scrutinize their performance with existing robustness-enhanced methods and then proceed to prevalent LLMs. Due to the absence of a standard benchmark, previous evaluations of existing methods can be

Figure 1: The comparison of previous work and our protocol on dataset selection of OOD benchmarks.

imprecise and thus misleading the estimations of progress in this field. Moreover, given the increasing focus on LLMs [9; 89] in NLP research, it is essential to evaluate their effectiveness in handling OOD challenges and explore the efficacy of different adaptation paradigms.

For robustness-enhanced methods, we evaluate five representative methods  on BOSS. Our main observation is that vanilla fine-tuning (a.k.a, empirical risk minimization) remains a strong baseline, while certain methods may slightly improve OOD performance in some cases. We further evaluate various LLMs and adaptation paradigms. We consider three recent prevailing LLMs, namely LLaMA , OpenAI text-davinci-003 , and OpenAI gpt-3.5-turbo. We include two relatively smaller models T0-3B  and T5-3B  for comparison. We apply zero-shot inference, in-context learning, few-shot fine-tuning, and full-data fine-tuning to one or multiple models. Through our experiments, we find that when provided with enough training data, fine-tuning domain-specific models remain the preferable choices for handling ID examples, while leveraging LLMs with in-context learning is superior for tackling OOD instances. In addition, we observe that the impact of in-context learning on generalization ability varies across models. We provide more detailed discussions in section 4.2.

## 2 BOSS Benchmark

### Motivation

NLP models should exhibit robustness across diverse distributions to ensure reliable applications. To achieve this, a standardized and recognized evaluation benchmark for OOD robustness is imperative. However, previous efforts in constructing benchmarks have predominantly relied on random selections and dataset popularity, lacking a systematic design [109; 39; 107]. Two deficiencies are thus identified: (1) Dataset similarity, as exemplified by the SST and IMDb datasets for sentiment analysis [83; 64], which share movie reviews and exhibit high semantic similarity (see Table 2). This blurs the line between ID and OOD evaluation, hindering rigorous assessment of OOD robustness; (2) Limited distribution shift challenges, exemplified by the high accuracy of a model trained on Amazon  when tested on IMDb (see Table 3). However, the significant performance drop on our considered Dynasent  suggests that OOD robustness still remains a critical problem in the sentiment analysis task. Thus, there is a need for universally applicable challenges across all dataset selections .

### Protocol to Construct OOD benchmark.

We aim to establish a standard benchmark for rigorous evaluation of OOD robustness in NLP. To address the above issues, we first survey and gather existing candidate datasets from Paperswithcode2, Kaggle3, and ACL Anthology4 websites. We consider the release date and public availability of datasets. Then we carefully examine three criteria to determine the ID and corresponding OOD datasets. The first criterion focuses on the ID dataset selection, and the other two criteria are proposed for OOD datasets, targeting the two issues in previous work, respectively.

**The ID dataset should provide sufficient knowledge for models to handle the task.** ID dataset should encompass comprehensive task-level knowledge , enabling models to grasp the underlying rationale necessary for task completion. Alternatively, if the model exclusively learns biased features, it may struggle to adapt to other features during distribution shifts. To this end, it is necessary for the ID datasets to possess the following characteristics: (1) Sufficiently large size; (2) Diversity, which is achieved through collection from multiple sources or the inclusion of several subtypes (i.e., styles, topics, levels of formality, et al). Our intuition is in line with , which demonstrates that training on large and diverse datasets improves the robustness of vision models.

**Datasets within a given task should originate from diverse distributions for a holistic evaluation.** We guarantee this through qualitative analysis of data source diversity and quantitative measurement of semantic similarity using SimCSE . To avoid overlap, we select at most one dataset per text source. Additionally, we ensure OOD datasets in the benchmark exhibit relatively low semantic similarity, and thus enhancing distinctiveness.

**OOD shifts should be challenging to provide an accurate assessment of progress in OOD robustness .** To quantify the challenge, we train a model on the ID dataset and test it on all candidate datasets. Specifically, we tune a T5-large  with manual templates on four tasks, except for NER, on which we adopt DeBERTa-large  with conventional fine-tuning due to the lack of a standard prompt-based tuning schema for this task. _For this reason, all experiments in this paper follow this choice of model selection._ For each text source, we first discard candidates similar to the ID dataset in semantics. Then, to construct challenging distribution shifts, we prioritize the dataset provoking the most severe performance drop of the ID model and adopt it as the OOD dataset in our benchmark.

### Dataset Selection

We take sentiment analysis as a case to demonstrate how we select ID and OOD datasets for each task according to our protocol. The selection process for other tasks can be found in Appendix D.

**Candidate Datasets.** We first collect all sentiment analysis datasets on Paperswithcode, Kaggle, and ACL Anthology as aforementioned. We filter out datasets released before the 2010s, as they are largely resolved with the advent of pre-trained language models . As a result, seven datasets remain as candidates, i.e., Amazon , DSC , Dynasent , IMDb , SemEval , SST , and Yelp . Considering the inconsistency in the number of categories across the datasets, we align them by converting them into a three-class classification setting. See Appendix C.2 for a detailed explanation of the dataset processing procedure.

**Probing Experiments.** According to our protocol, dataset size and text sources are assessed for ID dataset selection. Subsequently, semantic similarity and ID model performance degradation guide OOD dataset selection. To this end, two probing experiments are conducted: (1) Comparing semantic similarity using SimCSE for candidate dataset pairs, and (2) Evaluating the performance of the selected ID model. In the first experiment, for better semantic representation, we resort to the best SimCSE model provided by , a supervised RoBERTa-large . We load the model checkpoint from Huggingface5. For each dataset, we first encode each sample into a continuous embedding and then average the embeddings across the dataset to obtain a centroid representation of the dataset. Finally, we calculate the cosine similarity between a pair of centroids as the semantic similarity between two datasets. In the second experiment, we train a T5-large model on the selected ID dataset and evaluate its performance on all the candidate datasets.

**Dataset Selection.** The dataset information and semantic similarities are provided in Table 1 and Table 2, respectively. The text sources of the datasets vary from product reviews, movie reviews, Twitter, and adversarial texts. We observe that datasets originating from the same source tend to exhibit higher SimCSE scores, indicating higher semantic similarity. It is worth noting that for IMDb and SST, the widely used ID-OOD dataset pair in sentiment analysis [39; 107], the SimCSE score demonstrates one of the highest levels among dataset pairs. This reinforces the first deficiency of previous benchmarks, where dataset pairs have similar semantics and unclear distribution shifts. Hence, in contrast to existing practices, our benchmark construction considers only one dataset from each source.

For the ID dataset selection, we first exclude DSC and IMDb since they are binary classification datasets, on which the trained model cannot tackle the unseen class neutral. For dataset size,

    &  &  & \# Samples &  \\  & & & Train & Test & Train & Test \\  Amazon & Product & 3 & 30,000 & 38,905 & 71.69 & 54.84 \\ DSC & Product & 2 & 92,244 & 1,531 & 132.29 & 130.14 \\ Dynasent & Adversarial & 3 & 93,553 & 4,320 & 13.19 & 13.83 \\ IMDb & Movie & 2 & 25,000 & 25,000 & 233.79 & 228.53 \\ SemEval & Twitter & 3 & 6,000 & 20,622 & 19.44 & 19.62 \\ SST & Movie & 3 & 4,004 & 1,067 & 18.78 & 18.75 \\ Yelp & Product & 3 & 30,000 & 30,000 & 132.49 & 131.62 \\   

Table 1: Statistics of sentiment analysis candidate datasets.

   Train \(|\) Test & Amazon & DSC & Dynasent & IMDB & SemEval & SST & Yelp \\  Amazon & **100** & 86.02 & 57.30 & 36.67 & 24.74 & 33.70 & 49.22 \\ DSC & 86.02 & **100** & 59.15 & 54.55 & 31.70 & 44.40 & 55.45 \\  & 57.30 & 59.15 & **100** & 32.69 & 28.17 & 19.68 & 88.99 \\ IMDb & 36.67 & 54.55 & 32.69 & **100** & 46.95 & 84.62 & 39.88 \\ SemEval & 24.74 & 31.70 & 28.17 & 46.95 & **100** & 40.45 & 24.03 \\ SST & 33.70 & 44.40 & 19.68 & 84.62 & 40.45 & **100** & 19.43 \\ Yelp & 49.22 & 55.45 & 88.99 & 39.88 & 24.03 & 19.43 & **100** \\   

Table 2: SimCSE scores between each pair of datasets regarding the sentiment analysis task.

SemEval and SST are disregarded due to their limited number of samples per class (less than 10k). Among the remaining datasets, Amazon is chosen as the ID dataset for sentiment analysis as it encompasses reviews from 29 distinct product categories, offering greater diversity than Yelp.

For OOD datasets selection, we train a T5-large model on the ID dataset (i.e., Amazon) and evaluate it on all candidate datasets, as illustrated in Table 3. We include Dynasent and SemEval in the benchmark suite due to the following reasons: (1) They are the sole adversarial and Twitter datasets available, (2) They demonstrate low semantic similarity, and (3) They exhibit a notable performance degradation, making them crucial for evaluation. For movie reviews, SST is prioritized due to lower SimCSE scores compared to IMDb and larger performance drop of the ID model. Eventually, this yields three distinct and challenging distribution shifts in the sentiment analysis task: Amazon \(\) (DynaSent, SemEval, SST).

### Boss

Based on the aforementioned protocol, we introduce BOSS, an NLP benchmark suite for **O**OD robustness evaluation. BOSS comprises five essential NLP tasks: sentiment analysis (SA), toxic detection (TD), natural language inference (NLI), name entity recognition (NER), and extractive question answering (EQA). These tasks represent diverse practical applications and provide comprehensive coverage for evaluating models' capabilities, from aspects of classification, structured OOD datasets (see Table 4).

**Sentiment Analysis. Amazon** contains reviews of 29 different categories of products from the Amazon website. **DynaSent** first identifies naturally challenging sentences from several existing datasets, and then creates adversarial sentences with a human-and-model-in-the-loop annotation approach. **SemEval** is a three-class sentiment analysis dataset focusing on tweets. **SST** consists of sentence-level movie reviews from the Rotten Tomatoes website.

**Toxic Detection. Civil Comments** contains public comments on the Civil Comments platform, with users from diverse groups and various subtypes of toxic texts. **AdvCivil**, a new toxic dataset introduced in this paper, is generated from Civil Comments by textual adversarial attacks in an automated model-in-the-loop adversarial pipeline. Please refer to Appendix C.1 for details. **Implicit Hate** contains toxic tweets in both explicit and implicit forms. The latter can circumvent keyword-based toxic detection systems. **ToxiGen** is synthesized by GPT-3 , covering several types of subtly and implicitly toxic texts on 13 minority groups.

**Natural Language Inference. MNLI** provides ten different categories of written and verbal sentence pairs, with diverse styles, topics, and levels of formality. **ANLI** is an adversarial dataset collected in a human-and-model-in-the-loop approach, where each premise mainly comes from Wikipedia and the hypothesis is generated by human adversaries. **ContractNLI** considers each contract as a premise and holds a fixed set of hypotheses throughout the dataset. **WANLI** is synthesized by GPT-3 , each example containing challenging patterns identified in MNLI.

**Name Entity Recognition. Few-NERD**, the arguably largest dataset for NER, labels about 188k Wikipedia sentences into eight coarse-grained entity types. **CoNLL** takes stories from Reuters news, containing four basic entity types. **E-NER** is based on legal text. We use the four-category version in this paper, which treats all legal entities as miscellaneous ones. **WNUT** collects training data from Twitter and mines test data from Reddit, StackExchange, Twitter, and YouTube, containing six coarse-grained entity types in Few-NERD.

   Task & ID Dataset &  \\  SA & Amazon (AZ) & Dynasent (DS) & SemEval (SE) & SST (SST) \\ TD & Civil Comments (CC) & AdvCivil (AC) & Implicit Hate (IH) & Toxiction (TG) \\ NLI & MNLI (MN) & ANLI (AN) & ContractNLI (CN) & WANLI (WN) \\ NER & FewNERD (FN) & CoNLL (CoNLL) & E-NER (ENER) & WNUT (WNUT) \\ EQA & SQuAD (SQuAD) & AdvQA (AQA) & NewsQA (NQA) & SearchQA (QA) \\   

Table 4: The datasets included in the BOSS benchmark. Corresponding abbreviations are shown in brackets.

   Train & Test & Amazon & DSC & Dynasent & IMDb & SemEval & SST & Yelp \\  Amazon & **90.94** & 95.63 & 47.38 & 92.69 & 49.90 & 75.16 & 89.25 \\   

Table 3: The OOD performance of the T5-large when trained on the Amazon dataset.

**Extractive Question Answering.** **SQuAD** constructs question-answer pairs based on Wikipedia passages. **AdversarialQA** composes adversarial questions for contexts in SQuAD in a human-and-model-in-the-loop procedure, similar to ANLI. **NewsQA** writes questions for CNN news articles, each of which requires reasoning to answer, rather than relying solely on word overlap and textual entailment. **SearchQA** adopts a reverse construction pipeline, employing the Google search engine to retrieve relevant contexts for each question-answering pair from the J!Archive website.

## 3 Analysis of OOD Robustness

Despite OOD robustness in NLP has been extensively studied , a potential concern pertains to the usage of nonstandard benchmarks, as discussed in Section 2, resulting in inaccurate conclusions. To address this issue, we conduct a series of empirical analyses and evaluations to gain in-depth insights into OOD robustness in NLP. Previous research primarily concentrates on method comparisons without delving into models' learning behaviors. Therefore, we first analyze the models' learning mechanism by assessing the correlation between ID and OOD performance.

**Setting.** We assess the correlation between ID and OOD performance across various conditions. We manipulate the ID performance of models by varying their scale, training steps, available training samples, and tunable parameters. Further implementation details can be found in Appendix E.1.1.

**Results.** We observe that the correlation between ID and OOD performance on datasets of the five tasks is inconsistent, but can be broadly categorized into three types (see Figure 2): monotonic linear positive correlation (**Type I**), monotonic piecewise linear positive correlation (**Type II**), and non-monotonic V-shaped correlation (**Type III**). We also identify an exceptional case in Figure 3, which does not fall into any of the three categories. The full results are shown in Figure 4.

**Type I.** This is the most prevalent type of correlation observed across all ID-OOD pairs for sentiment analysis, name entity recognition, and the majority for toxic detection. As shown in Figure 1(a), in this category, OOD performance is positively and linearly correlated with ID performance, indicating that the task knowledge learned on source distribution can be effectively generalized to other distributions. This observation is consistent with results in the computer vision domain , which shows that OOD performance is linearly correlated with ID performance across various model architectures, hyperparameters, training dataset size, and training duration. However, the slope of the line fitted by the least square method is less steep than the \(y=x\) diagram, and it eventually lies below the diagonal, implying that the performance degradation of models under distribution shift will be escalated with the increase of the ID performance.

**Type II.** This category is observed on ID-OOD pairs for extractive question answering. As presented in Figure 1(b), the results can be fitted into a polyline, indicating a piecewise linear correlation. The correlation between OOD performance and ID performance is positive and linear, with a notable differ

Figure 2: Three representative correlations between ID-OOD performance: (a) Type I (monotonic linear positive correlation) indicates consistent linear improvement of OOD performance with increasing ID performance. (b) Type II (monotonic piecewise linear positive correlation) exhibits accelerated OOD performance growth after a turning point. (c) Type III (non-monotonic V-shaped correlation) shows an initial negative correlation, followed by a positive correlation after a turning point. The \(r^{2}\) value in Figure (a) is 0.9677, and the values of the left and right fits in Figure (b) are 0.9553 and 0.9396 whereas in Figure (c) are 0.7690 and 0.8124 respectively.

ence in the slope before and after the turning point. Specifically, OOD performance demonstrates slow growth until the turning point, after which a minor increase in ID performance yields a substantial improvement in OOD performance. The observed trend may be attributed to the findings of , which indicates that models initially capture spurious correlations in ID datasets before acquiring comprehensive task knowledge. Consequently, models prioritize learning these spurious correlations to address the ID task, resulting in minimal improvements on OOD datasets. However, in the later stages of training, models progressively acquire greater task knowledge, leading to improved OOD performance.

**Type III.** The V-shaped fitted lines shown in Figure 1(c) mainly occurs on ID-OOD pairs of NLI tasks. This pattern is divided into two stages by a turning point in ID performance. In the first stage, OOD performance experiences worsening performance degradation during the distribution shift. However, in the second stage, the ID-OOD correlation becomes positive. This trend resembles the U-shaped scaling law of LLMs observed by , thus suggesting a shared explanation.  attributes this phenomenon to the "distractor task" in the dataset, apart from the "true task". Medium-capacity models may perform better than low-capacity models on the distractor task, which may harm performance on the "true task". As the model capability increases, it can ignore the distractor task and focus on improving performance on the true task. Here, we identify the distractor task in NLI datasets as detecting the word overlap or other spurious correlations between the premise and hypothesis.

**Outlier.** There is an exceptional case regarding the distribution shift from Civil Comments to AdvCivil (see Figure 3). The figure depicts two distinct lines, both exhibiting a monotonic linear negative correlation. This may stem from the model's increased reliance on spurious correlations and the adversarial nature of AdvCivil. Prior research suggests that models can learn non-robust features, such as spurious correlations, to enhance ID accuracy . However, adversarial samples preserve the semantics of the original texts while introducing perturbations that eliminate spurious correlations. Hence, when the ID model becomes more dependent on spurious correlations during training, its performance degradation on adversarial samples intensifies.

## 4 Evaluation of OOD Robustness

### Robustness-enhanced Methods

After analyzing the learning behavior of PLMs under vanilla fine-tuning, we examine their performance when trained with other methods. Although massive methods have been proposed to improve the robustness of PLMs, their evaluations rely on non-standard benchmarks, which may result in inaccurate evaluations and impede progress clarity. Therefore, in this section, we first conduct extensive experiments to re-evaluate the effectiveness of diverse robustness-enhanced methods.

**Setting.** We consider the categories of robustness-enhanced methods summarized by : data-driven, model and training-based, inductive-prior-based, and causal intervention methods. We select the most representative one from each category for evaluation. Specifically, we choose EDA  and FreeLB  for data-driven methods, label smoothing  and focal loss  for model and training-based methods, and model ensemble  for inductive-prior-based methods. We do not consider causal intervention methods since they are typically applied to low-resource scenarios. As explained in section 2.3, we apply the above methods to DeBERTa-base models for the NER task and to T5-base models for the other tasks.

**Results.** The results are shown in Table 5, where the mark '-' indicates that a certain method is not applicable to the task. We summarize the findings in the following takeaways:

_Takeaway 1: The vanilla fine-tuning (empirical risk minimization) remains a strong baseline._ Despite existing methods outperforming vanilla fine-tuning on certain datasets like E-NER, WNUT, and NewsQA, they show limited superiority or can potentially harm model performance. Specifically, only FreeLB demonstrates beneficial effects over half of the datasets, standing out as the most

Figure 3: The OOD performance exhibits a negative correlation with ID performance. Refer to Figure 2 for legends.

[MISSING_PAGE_FAIL:8]

better performance on most OOD datasets. This observation reinforces the view that large pre-trained models possess strong generalization capabilities, whereas, with sufficient training data, an accurate estimate of data distribution can be achieved even without a large number of parameters .

_Takeaway 2: In-context learning always brings no gains to the generalization ability of small models, while it generally helps Turbo and significantly improves LLaMA-series and Davinci3._ For small models like T5-3B, the performance of in-context learning is the same with or even worse than the zero-shot inference. For Turbo, providing ID examples for in-context learning presents advantages on nearly two-thirds of the datasets, with the NER task benefiting the most. For LLaMA-series and Davinci3, the superiority of in-context learning is prominent as it enhances performances on most of the datasets.

_Takeaway 3: Examples from ID datasets are generally more effective for in-context learning than those from the original training split of the testing OOD dataset._ Specifically, when considering samples from our OOD datasets as contexts, the performance of Turbo is comparable to using ID samples, whereas the LLaMA-series and Davinci3 models consistently exhibit inferior performance compared to using ID examples as contexts. However, all models show improved performance on the EQA task when contexts from our OOD datasets are utilized. This may be attributed to the variations in sample length or question styles across EQA datasets, hence models acquire more precise instructions from the original training samples. The overall ineffectiveness of ICL\({}^{*}\) can be explained by the findings of . According to , in-context demonstrations aim to guide the models to learn the target label space, instead of the feature-label mapping. The ID examples contain more diverse information due to the construction process of our benchmark. Thus, ID examples can better prompt the language models to locate the target label space, compared to the OOD examples that may target a specific domain.

**Discussion.** Two paradigms are prevalent in developing downstream NLP systems: leveraging general-purpose LLMs or gathering domain-specific data for fine-tuning smaller models. For the first paradigm, the overarching objective of general-purpose LLM development is to employ a single model for solving various downstream tasks . Consequently, LLMs are anticipated to exhibit high performance on both ID and OOD datasets. However, our study exposes the shortcomings of LLMs on ID datasets when compared to fine-tuned domain-specific models. Considering the higher inference and deployment costs associated with LLMs, substantial progress is still needed to effectively improve LLMs in developing downstream applications, particularly for challenging tasks like EQA. For the second paradigm, our study reveals the limitations of fine-tuning models on ID datasets for OOD performance in comparison to LLMs. Thus, further research is required to develop advanced techniques that enhance the robustness of fine-tuned domain-specific models. Overall, the existing two prevalent paradigms still fall short in addressing the OOD problem in NLP, necessitating further advancements and effective approaches.

However, we also note that there can exist confounders in our evaluations. It is still ambiguous which datasets are indeed OOD to LLMs, given that LLMs have been pre-trained on massive public corpora. The potential data contamination issue can result in overinflated performance on our OOD datasets, tangling the credit of the memory and generalizability of LLMs. The only confirmed distribution shift for LLMs is the temporal shift, necessitating the evaluation based on data released subsequent to their pre-training data collection cut-off. Therefore, the NLP community demands new downstream datasets independent of the pre-training corpus to meet the evaluation requirements for LLMs.

## 5 Related Work

**Distribution shifts in NLP** has been widely studied in various forms. We examine several representative cases as outlined below. Domain shift refers to the challenge of testing data originating from diverse domains, often due to data collection from various sources [63; 40; 53; 79]. Temporal shift examines the degradation of models' performance over time [42; 1]. Spurious correlation examines the issue of models acquiring dataset-specific knowledge on ID data, which may not generalize effectively to OOD data [66; 73; 91; 32; 37; 16; 17]. Additionally, a requirement is for models to exhibit robustness when confronted with artificially constructed OOD samples. One typical type is malicious adversarial attacks, which involve assessing the resilience of models against inputs crafted by malevolent adversaries [56; 51; 112]. These inputs, distinct from ID samples, have the potential to induce model failures . Adversarial attacks can also be effectively utilized to simulate diverse user inputs to examine models' robustness in the real world . Another category is backdoor attacks, characterized by intentionally introduced spurious correlations that can be exploited by attackers for their advantage .

**OOD Evaluation in NLP** can be broadly classified into automatic and static evaluation approaches. Automatic evaluation utilizes diverse textual transformation techniques, such as introducing typos, to conduct a rigorous evaluation of OOD robustness. Three essential elements in the automatic OOD evaluation encompass the establishment of suitable transformation methods, evaluation metrics, and effective techniques to ensure sample validity . Static evaluation, in contrast to automated methods, offers the advantage of constructing benchmarks with higher quality, resulting in an improved estimation of OOD robustness. Numerous OOD benchmarks have been introduced, focusing on adversarial attacks  or spurious correlations . A relevant study to ours is GLUE-X , which establishes an OOD benchmark derived from the GLUE benchmark . Nevertheless, they do not establish a coherent benchmark construction protocol and primarily rely on dataset selection driven by popularity, incorporating datasets into the benchmark without comprehensive explanation and seemingly opting for a somewhat arbitrary selection, thus lacking a systematic approach.

## 6 Conclusion

We revisit OOD robustness research in NLP, identifying deficiencies in benchmarks and evaluation. Correspondingly, a benchmark construction protocol and an OOD robustness evaluation suite are proposed to facilitate future research. The correlation between OOD and ID performance, the effectiveness of existing methods, and the challenges faced by LLMs are investigated.

## Limitation

We identify two limitations in this work. First, as discussed in section 4.2, due to the lack of new datasets in the community, there is a possibility that some datasets have been included in the pre-training corpus of LLMs, so they may not be suitable to test the generalizability of recent LLMs. However, we note that with our benchmark construction protocol, we can easily update the benchmark as new datasets come out. Second, we only consider five tasks in this benchmark, which is not a comprehensive collection of current NLP literature. We explain the reason for the current task selection in Appendix A.1.