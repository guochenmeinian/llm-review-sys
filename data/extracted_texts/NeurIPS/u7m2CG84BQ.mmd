# BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack

Yuri Kuratov\({}^{*}\)\({}^{1,2}\)  Aydar Bulatov\({}^{*}\)\({}^{2}\)  Petr Anokhin\({}^{1}\)  Ivan Rodkin\({}^{2}\)

**Dmitry Sorokin\({}^{1}\)  Artyom Sorokin\({}^{1}\)  Mikhail Burtsev\({}^{3}\)**

###### Abstract

In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.

## 1 Introduction

Today, large language models (LLMs) and neural architectures are continually evolving and achieving remarkable improvements, particularly in their ability to handle longer contexts (OpenAI, 2023; Reid et al., 2024; Anthropic, 2024). The ability of these models to process and generate text based on rich contextual information is crucial for several reasons. For example, longer contexts provide more information for the model to condition its outputs, leading to more accurate, contextually relevant, and up-to-date responses. Furthermore, long-context capabilities can enhance in-context learning by providing more in-context examples, instructions to follow, or example trajectories in context of reinforcement learning (Chevalier et al., 2023; Agarwal et al., 2024; Lee et al., 2024).

Despite these advances in models capabilities, the benchmarks used to evaluate them have not kept pace. For example, current benchmarks such as Longbench (Bai et al., 2023) and L-Eval (An et al., 2023) scale only up to 40,000 tokens, while models are capable of hundreds of thousands and millions of tokens (Rodkin et al., 2024; Reid et al., 2024; Bulatov et al., 2024; Anthropic, 2024; Liu et al., 2024; Gu & Dao, 2023; OpenAI, 2023a).

Creating natural and comprehensive long-context benchmarks that are human labeled is very challenging. As a consequence, synthetic benchmarks focusing on variations of "needle-in-a-haystacktasks have become increasingly common (Zhang et al., 2024; Liu et al., 2024; Song et al., 2024; Hsieh et al., 2024). One widely used needle-in-a-haystack task involves finding specific "needles with magic numbers" in a haystack of Paul Graham's essays1. However, the widespread use of this approach has highlighted its limitations - it is overly simplistic, and novel long context models often achieve perfect performance, as usually demonstrated by fully green heatmaps (Reid et al., 2024; Cohere, 2024; Liu et al., 2024; Wang et al., 2024). This shows that while it serves well as a basic verification tool, it is not a rigorous benchmark that can effectively challenge and differentiate advanced long-context models. Another major drawback of the original default setup1 is that model predictions are evaluated and scored by an LLM (GPT-3.5-turbo) on a scale of 1 to 10, with the same single needle used for each position and document length. While averaging over multiple different needles can provide more robust results.

To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets, that were designed as prerequisites for any system that aims to be capable of conversing with a human (Weston et al., 2016). As a source of long natural documents we use books from PG19 corpora (Rae et al., 2020). In this way, BABILong allows the construction of tasks of almost arbitrary length, in order to adapt them to the evaluation of new, more powerful models in an extensible and controllable way. We provide sets of predefined lengths with splits up to 10 million tokens, and we evaluate models on samples with up to 50 million tokens.

We find that popular LLMs effectively use only 10-20% of the context, with performance declining sharply as length and task complexity increase. Retrieval-Augmented Generation methods achieve a modest 60% accuracy in answering single-fact questions, regardless of context length. Among other methods, Mamba and Recurrent Memory Transformers (RMT and ARMT) show the highest performance, with ARMT capable of processing lengths up to 50 million tokens.

The main contributions of our work are as follows:

1. We introduce BABILong, a novel scalable generative multi-task benchmark for evaluating the performance of NLP models in processing arbitrarily long documents with distributed facts.

2. We evaluate over 30 recent long-input language models with various sizes, architectures, and context extension methods on BABILong.

Figure 1: **a) Generation of BABILong dataset.** Facts relevant for the question are hidden inside a larger background texts from PG19. **b) Recurrent transformers answer questions about facts from very long texts when retrieval augmented generation fails.** Common RAG method fails to answer questions because order of facts matters. GPT-4 effectively uses only about 10% of the full 128K window. Gemini 1.5 Pro shows strong performance up to 64K tokens. Small LMs, ARMT & RMT with GPT-2 (137M) and Mamba (130M) fine-tuned for the task are able to solve it, with recurrent memory transformers scoring well up to record 50 000 000 tokens. Here we show the best results obtained by models.

3. We find that popular LLMs effectively utilize only 10-20% of the context, with performance degrading sharply as reasoning complexity increases. Retrieval augmented generation fails to demonstrate good scores but fine-tuning for specific task helps.

4. We demonstrate successful in domain single fact question answering with the recurrent memory transformer on input texts up to 50 million tokens, which is a record for the sequence size processed by a single model.

The BABILong benchmark data and code for evaluation are available.2

## 2 The BABILong Benchmark for Long Context Processing

The fundamental concept behind the **B**enchmark for **A**rtificial **I**ntelligence for **L**ong-context evaluation is extending the length of existing tasks to test the ability of generative models in handling long contexts. Solving tasks with a long context size requires the model to distinguish important information from large amounts of irrelevant details. To simulate this behavior we "hide" the sentences of the original task between the sentences of irrelevant text that is drawn from another closely related distribution (see Figure 0(a)). Examples are constructed by gradually adding new sentences from the background dataset in their natural order until the augmented sample reaches the desired length. This way, we are not bound by the length of the original task itself, making it possible to assess even the longest available models with context sizes up to millions of tokens. For background text we use books from the PG19 dataset (Rae et al., 2020) due to the substantial book lengths and naturally occurring long contexts. The model is required first to distinguish the sentences related to the original task, then memorize and subsequently utilize them to generate the correct solution.

In this work we extend the bAbI benchmark (Weston et al., 2016), which consists of 20 tasks designed to evaluate basic aspects of reasoning. These tasks are generated by simulating interactions among characters and objects across various locations, each represented as a fact, such as "Mary traveled to the office." The challenge is to answer questions based on the facts generated in the current simulation, such as "Where is Mary?" The tasks in bAbI vary in the number of facts, question complexity, and the reasoning skills they assess, including spatial and temporal reasoning, deduction, and coreference resolution. In our paper, we label these tasks from 'QA1' to 'QA20'. The first ten tasks, as shown in Table 1 demonstrate that current LLMs exhibit mixed performance even without distractor texts, indicating that the BABILong tasks span a broad spectrum of difficulty and allow for testing models across various performance dimensions. Details and performance metrics for the bAbI tasks, along with examples of BABILong samples generated using our pipeline, can be found in Appendix L.

As evident in the following sections, these seemingly simple tasks pose significant challenges to language models. Although filtering facts from background text might be straightforward, models encounter next challenges of finding supporting facts among distractors and performing types of reasoning such as counting that are especially difficult for LLMs. Additionally, most NLP benchmarks

   Task & Name & facts & relevant facts & LLMs answer accuracy \\  & & per task & per task & without background text (0K) \\  qa1 & single supporting fact & 2-10 & 1 & QA1 \\ qa2 & two supporting facts & 2-68 & 2 & QA2 \\ qa3 & three supporting facts & 4-320 & 3 & QA3 \\ qa4 & two are relations & 2 & 1 & QA4 \\ qa5 & three ARG relations & 2-126 & 1 & QA5 \\ qa6 & yes-no questions & 2-26 & 1 & QA6 \\ qa7 & coultING & 2-52 & 1-10 & QA7 \\ qa8 & lists-sets & 2-50 & 1-8 & QA8 \\ qa9 & simple negation & 2-10 & 1 & QA9 \\ qa10 & indefinite knowledge & 2-10 & 1 & QA10 \\   

Table 1: **The first ten tasks of BABILong with the number of supporting and distracting facts. The last column displays the performance of LLMs on each task in the absence of background text. Each dot represents one of the selected models, while the blue bars indicate the median accuracy across tested models.**

[MISSING_PAGE_FAIL:4]

### Evaluation of Effective Context Size

One of the most important questions regarding performance of long-context models is how effectively they utilize the input context. Ideally, a model should maintain uniformly high performance regardless of the input size. For instance, if an LLM can process 128K tokens, it is expected to use all of this context in addressing the user's task.

We evaluated the performance of models on question-answering tasks with varying numbers of supporting facts (QA1-QA3) to study how LLMs utilize the available context. Here, we distinguish between a QA task, which requires a single correct answer, and an information retrieval task, which should generate a list of relevant facts or references to information sources. We consider performance satisfactory if the accuracy of an answer exceeds 85% and a complete failure if it is below 30%.5

Our benchmarking results show that current LLMs do not efficiently use their full context (Fig. 2). Only 23 out of 34 tested LLMs were able to correctly answer 85% or more of the questions for any of QA1-QA3 tasks in a baseline setting without any background distractor text. Even for the the simplest task involving a single supporting fact (QA1), the majority of models are only able to efficiently use up to 4K tokens, except for GPT-4 and LLama-3.1-70b, which perform well up to 16K, as well as Qwen-2.5-70b and Gemini Pro 1.5 up to 64K. The range of full context utilization on QA1 varies from 5% to maximum 50%. When two supporting facts are required for an answer, only GPT-4 and Gemini Pro 1.5 can solve the task without background text. When facts are embedded within texts, all tested LLMs fall below 85% performance(Fig. 2, QA2). The task with three supporting facts proves to be extremely challenging to current LLMs, with the best scores falling below 80% (Fig. 2, QA3).

Going deeper in performance of specific models presented in the Table 2 we found the following. Yarn fails to extend to longer contexts despite showing stable results in long-context language modeling (Peng et al., 2023b). LongChat, LongAlpaca, and both LLama2-7B-32K and LLama2-7B-32K-instruct models, even when fine-tuned on 32K lengths, failed to perform well on 32K tokens. Activation Beacon performed better than Yarn context extension method for Mistral 7B, but still achieved low results (< 40%) on 32K contexts. In contrast, Mistral-v0.2 and Mistral-v0.1, trained on lengths up to 32K, performed well on these lengths. Yi-9B-200k, trained on sequences up to 200K, shows less than 30% on 64K tokens and more. Yi-34B-200k shows very promising and stable

Figure 2: **LLMs struggle to answer questions about facts in texts larger than 10,000 tokens.** The plots demonstrate how the performance of selected leading models deteriorates with increasing context size. For single supporting fact questions (QA1), the majority of models perform well up to 4,000 tokens. However, when a correct response requires two (QA2) or three (QA3) facts, LLMs fail to achieve satisfactory accuracy.

results on lengths up to 64K, but unfortunately we were not able to run it on 128K tokens. Phi-3-mini drops significantly from 64K to 128K, reaching less than 10%, while Phi-3-medium maintains 30% at 128K. Jamba-v1 and Phi-3-mini show close results, but Jamba-v1 does not have drop at 128K and shows 34% on this length. Command-R and Phi-3-medium are the most robust to longer contexts, but start to lose performance more sharply at 128K. Phi-3-medium and Command-R show results very close to GPT-4 at 32K+ contexts.

We added results for models released since June 2024, including LLama 3.1, Phi 3.5, Qwen 2.5, and GPT-4o-mini. All claim to support 128K context length. Phi-3.5-mini shows improvements mainly for contexts up to 16K. The new Phi-3.5-MoE performs similarly to Phi-3-medium, but with only 6.6B active parameters compared to 14B in Phi-3-medium. LLama-3.1 models show significant improvement: LLama-3.1-8B matches GPT-4o-mini, while LLama-3.1-70B outperforms GPT-4 on longer contexts. Qwen-2.5 models outperform LLama-3.1 of similar size and achieve the best results of all evaluated open LLMs on BABILong.

Most of the new models use multistage pre-training with progressively increasing sequence lengths. For example, LLama-3.1 (Dubey et al., 2024) is pre-trained in six stages from 8K to 128K, and only proceeds to larger lengths if it maintains high short context scores and successfully solves the needle-in-a-haystack task. During supervised fine-tuning, LLama-3.1 models mix short context data with synthetic long context data, including question-answering, summarization, and code tasks.

### Retrieval-Augmented Generation Does Not Perform Well on BABILong

Retrieval-Augmented Generation (RAG) is a popular solution for language models to handle large amounts of text. In RAG relevant parts of texts are retrieved from a large dataset on the first stage. Then, the language model uses input augmented with retrieved texts to generate the final response. In the case of BABILong, we expect RAG to extract all the facts relevant to a question from a long input text and then place them in the context of the model.

We experiment with two options: (1) retrieval by chunks of size 512 tokens, denoted RAG-C and (2) retrieval by sentences, called RAG-S. For details of evaluation and RAG pipelines with GPT4 and LLama-3 please refer to Appendix G. The findings from the QA1 task, depicted in Figure 2(a), indicate that retrieval performance using sentence chunks is superior to that of 512-token segments, with a notable decrease in accuracy observed already after 16k token context length. However, this superiority is task-specific and may not translate effectively to real-world applications due to the potential for information loss in smaller chunks.

Figure 3: **Fine-tuning but not RAG solves BABILong.** a) RAG on QA1 task. Retrieval by chunks with size 512 tokens (RAG-C) fails to improve GPT-4 and LLama-3 performance on long-context tasks. RAG-S, which retrieves by sentences achieves better results, but further increasing the number of retrieved sentences from top-5 to top-20 does not help. b) Task specific fine-tuning. Finetuned Mamba achieves the best overall results, greatly outperforming RAG models. However, processing sequences longer than 128k is extremely slow due to technical limitations. On the other hand, RMT shines on extremely long sequences, managing to keep high accuracy up to 10M tokens.

The RAG pipeline with GPT-4-turbo shows scalable but weak performance on BABILong for sentence embeddings and poor scalability with chunk embeddings (see Fig. 2(a)). The weak performance of RAG might be attributable to the temporal dependencies inherent in the task, where the relevant fact is positioned at the end of the text. In QA2 and QA3, retrieval fails dramatically with accuracy plummeting below random guessing. This lack of success is attributable to the specific demands of these tasks, which require the retrieval of multiple (two or three) supporting facts to generate accurate responses. For example, in instances where the key facts are "Mary got the milk there." and "Mary travelled to the hallway.", with the query being "Where is the milk?", the retrieval system may successfully identify the first fact but fail to retrieve the second due to insufficient similarity between the question and the latter fact. The default retrieval algorithm's lack of temporal consideration and limitations in the similarity function underscore the necessity for additional methods in tasks with multi-hop reasoning and temporal dynamics.

### Fine-Tuning Models on BABILong

We performed fine-tuning experiments with GPT-3.5-Turbo, Mistral-7B-Instruct-v0.2, RMT and ARMT with GPT-2 (137M) backbone, and Mamba (130M) models. Fine-tuning results are in Figure 2(b) and Appendix I Figure 9.

RMT with a GPT-2 (Radford et al., 2019) backbone model is trained on each task individually with a segment size of 512 and memory size of 16. ARMT with GPT-2 used 10 memory tokens (Rodkin et al., 2024). Train and evaluation splits of each task contain 10000 and 1000 samples, respectively, with a number of facts in each sample ranging from 2 to 320 depending on the task. A curriculum strategy is employed, initiating training on short sequences that fit in a single segment and then gradually introducing longer sequences once the training converges. During each curriculum stage \(n\), sequence lengths are randomly sampled from \(1\) to \(n\) segments. We provide details of training and evaluation procedures in Appendix C.

RMT and ARMT models trained on 32 segments totalling in 16K tokens demonstrates strong performance on this length. Notably, recurrent memory models outperform GPT-4 significantly, underscoring the efficiency of memory mechanism in processing long context. Even more importantly, the power of recurrent models extends to sequences longer than the training size. RMT shows consistent performance on longer sequences, up to 128k tokens, with only a marginal quality degradation. Surprisingly, with context sizes scaling to 1 million, 10 million tokens, and even 11.1 million tokens, which is over 600 times of the training length. While ARMT successfully scales even further, reaching up to 50 million tokens (Rodkin et al., 2024). Recurrent memory models persistently outperform the larger counterparts utilizing RAG.

Finetuned recurrent models, Mamba, RMT and ARMT perform equally well on QA1, however due to the technical limitations of the Mamba implementation, the inference beyond 128k was extremely slow, which makes it nearly impossible to process longer sequences. Recurrent Memory models greatly outperform retrieval-augmented models and are able to process sequences up to 10M and 50M tokens much faster than Mamba. However, Mamba has an edge in complex tasks such as remembering a large number of facts in QA3 (Table 4).

We evaluated GPT-3.5 and Mistral-7B models fine-tuned with 1000 samples from QA1 for 3 epochs. The evaluation results are shown in Appendix I Figure 9. Fine-tuning dramatically improves performance for longer contexts making scores uniform across all input sizes. Still, these results are behind of fine-tuned Mamba, RMT and ARMT.

### BABILong and Other Benchmarks

Here, we analyze how models performance on BABILong benchmark differs from MMLU (Hendrycks et al., 2020) and RULER (Hsieh et al., 2024). The MMLU benchmark measures various branches of knowledge in LLMs, whereas RULER, a recently proposed long-context benchmark, shares a similar "needle-in-a-haystack" concept with BABILong. One notable difference is that RULER's "needles" (such as adjectives, nouns, numbers, uuids) and long "haystack" contexts are more synthetic, consisting of randomly repeated sentences, except for tasks based on the SQuAD (Rajpurkar et al., 2016) and HotPotQA (Yang et al., 2018) datasets or using Paul Graham essays.

We collect results from multiple models on MMLU, BABILong, and RULER at lengths ranging from 0K (BABILong without texts from PG19) to 128K tokens. In the upper-left part of Figure 4, we show the correlation between scores on BABILong and RULER for different task lengths with those on the MMLU benchmark. At shorter lengths, BABILong exhibits a high correlation with MMLU, which diminishes as the length increases. Conversely, RULER shows a nearly constant correlation with MMLU, regardless of the length. The best correlated RULER lengths with MMLU are 64K and the average of all lengths (<=128K). In contrast, the highest correlation of BABILong scores with MMLU is at length 0K, which is expected since MMLU is a relatively short benchmark with examples up to 1K tokens. Comparing the correlations of BABILong with MMLU at the most correlated RULER lengths (<=128K and 64K) shows much lower values: \(0.928\) vs. \(0.435\) and \(0.910\) vs. \(0.455\), respectively.

These results show that BABILong can detect differences in models behavior starting from lengths as small as 2K tokens, while RULER requires lengths of at least 128K tokens to show significant differentiation from relatively short MMLU benchmark.

## 4 Related Work on Long Context Benchmarks and Datasets

Long Range Arena (LRA) (Tay et al., 2021) was a one of the pioneering benchmarks for long context modeling. LRA is a set of tasks with lengths from 1 to 16 thousand tokens. However, it mainly consists of very specific tasks such as ListOps (2k tokens), Byte-Level Text Classification (4k tokens) and Byte-Level Text Retrieval (8k tokens), and others that are less related to NLP. They are not well suited for evaluating of modern LLMs without fine-tuning on these tasks and cannot fully represent the capabilites of LLMs that can handle 100k+ tokens.

A new set of datasets and benchmarks specifically designed to test the ability of LLMs to handle long contexts has been proposed. The LongBench dataset (Bai et al., 2023) contains 6 types of real and synthetic problems, ranging from summarization and multidoc QA to code completion. The average sample lengths in LongBench are 6k and 13k tokens for English and Chinese respectively, with 40k tokens at max. Scrolls and ZeroSCROLLS (Shaham et al., 2022, 2023) consist of QA, classification, summarization tasks and have higher average lengths ranging from 1.7k to 49.3k tokens. L-Eval (An et al., 2023) mostly combines 20 smaller long sequence datasets and adds 4 newly annotated tasks, with query-response pairs encompassing diverse question styles and domains. The average length of examples for L-Eval varies from 3 to 60 thousand tokens. Some of the benchmarks are focusing on evaluation of in-context learning and instruction following, such as LongAlign and LongBench-chat (Bai et al., 2024), ZeroSCrolls, LongICLBench (Li et al., 2024).

Figure 4: **BABILong is similar to MMLU (Hendrycks et al., 2020) on short lengths and captures differences in models behavior for longer contexts.** MMLU is a relatively short benchmark, with samples up to 1k tokens in length. BABILong has a higher correlation with MMLU on short contexts (0K) than RULER (Hsieh et al., 2024). However, RULER maintains a high correlation regardless of task length, with an even higher correlation at 64K, while BABILongâ€™s correlation with MMLU decreases with length. This may indicate that BABILong is better at capturing differences in models behavior at different context lengths.

There are other long-context datasets that primarily consist of QA and summarization tasks over texts from Wiki, arXiv, novels, movie and TV scripts, or other sources, e.g., InfinityBench (Zhang et al., 2024b), Loogle (Li et al., 2023b), Bamboo (Dong et al., 2023), LVEval (Yuan et al., 2024), NovelQA (Wang et al., 2024b), Marathon (Zhang et al., 2023), XL\({}^{2}\)-Bench (Ni et al., 2024), DocFinQA (Reddy et al., 2024), or ChapterBreak (Sun et al., 2022), Ada-LEval (Wang et al., 2024a) that evaluate operations with text chunks, or move to multimodal tasks in MileBench (Song et al., 2024a). These datasets vary in length, with maximum sample lengths of 636K tokens in ChapterBreak and average lengths reaching 200K tokens in InfinityBench, NovelQA, LVEval, and some subsets of XL\({}^{2}\)-Bench.

Further extending of benchmarks' length with real and human annotated data is very challenging. Therefore, "needle-in-a-haystack" inspired benchmarks were proposed. Following LLMTest 6 with magic numbers as needles in Paul Graham essays as haystack, passkey and key-value retrieval tasks are part of InfinityBench (Zhang et al., 2024b). Counting-Stars (Song et al., 2024b) suggests to insert multiple sentences about little penguins that count stars into the same essays for English or The Story of the Stone for the Chinese language. The task is to answer questions based on these "needle" sentences. RULER (Hsieh et al., 2024) extends "needle-in-a-haystack" with multiple types and amount of "needles". RULER and Counting-Stars introduce new task categories such as multi-hop tracing and aggregation to test models beyond searching from context.

Some benchmarks have pre-defined length bins, such as LongBench (0-4k, 4k-8k, 8k+), Ada-LEval (2k-128k), LVEval (16k, 32k, 64k, 128k, 256k), Bamboo (4k, 16k), S3Eval (2k, 4k, 8k, 16k) (Lei et al., 2023). A number of benchmarks, including RULER, CountingStars, Ada-LEval, and S3Eval, can be generated at required lengths. All mentioned datasets are mostly in English with some of them covering Chinese language (LongBench, InfinityBench, LongAlign, Counting Stars, CLongEval (Qiu et al., 2024), LV-Eval, XL\({}^{2}\)-Bench).

BABILong focuses on natural language reasoning over multiple facts distributed in very large textual corpora. Compared to existing approaches it provides more tasks and more natural and deceptive mixing of information into background documents. BABILong consists of diverse set of 20 tasks that cover different capabilities including multi-hop tracing, aggregation over needles and extending them with basic deduction and induction, time, positional, and size reasoning, and path finding. The benchmark goes with predefined splits up to unprecedented 10M token length. Lengths beyond 10M tokens could be generated and we test models up to 50M tokens. Furthermore, while we evaluate models on English-only tasks from bAbI (Weston et al., 2016) adding new languages is straightforward.

## Conclusions

In this work, we introduced the BABILong, a diverse and scalable benchmark designed to bridge the gap in evaluating large language models (LLMs) across extensive context lengths. Our experiments demonstrate that BABILong offers a more representative evaluation framework for long-context reasoning among the existing benchmarks. The analysis of correlation with other benchmarks further validates BABILong's ability to pose a significant challenge for large language models to maintain performance as context lengths scale up. The BABILong benchmark offers algorithmically adaptable document length and facts placement, includes predefined sets of bins ranging from 0k to 10M tokens. Facts in BABILong could be generated making it leak-proof for future LLMs. It consists of a set of 20 diverse tasks covering reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. Compared to other benchmarks, BABILong shows high correlation on short contexts with MMLU and diverges from it as lengths increases.

Our findings reveal limitations of popular open-source LLMs as well as GPT-4, Gemini 1.5 Pro and RAG solutions regarding effective long context utilization. Their performance heavily relies on the first 5-25% of the input, highlighting the need for improved context processing mechanisms. The recent open-source models LLama-3.1 and Qwen-2.5 show the best performance of pre-trained LLMs, with LLama-3.1 using pre-training and supervised fine-tuning on long context data up to 128K tokens. BABILong fine-tuning experiments show that tasks from BABILong are solvable even by relatively small models like RMT & ARMT with GPT-2 (137M) and Mamba (130M). Fine-tuning improves the performance of GPT-3.5-Turbo and Mistral-7B, but their context lengths remain limited to 16K and 32K, respectively. Among the evaluated models, Mamba and recurrent transformers achieve the strongest results. However, Mamba is hard to infer on lengths more than 128K tokens, while RMT and ARMT enables the processing of lengths up to 50 million tokens.

## Limitations

The BABILong benchmark uses background texts to hide facts in them. In our experiments, we only tried PG19 and Wiki as background text sources. Other background texts may have a different effect on the results. PG19 and Wiki were chosen because they contain natural narratives and facts about people, in a way similar to bAbI tasks. Interference between similar facts in the background text can make the benchmark even more difficult.

In GPT-4 and LLama-3 with RAG experiments, we do not optimize the retrieval component. We tried several prompts experiments with LLMs, but the ones that we selected could be suboptimal. We provide them in Appendix J and in our GitHub repository.

The current version of the dataset reuses parameters for fact generation from the bAbI (Weston et al., 2016). As the initial work used vocabularies of limited size, this results in a low variety of names and objects within the facts. This limitation makes the BABILong tasks easier for fine-tuned models, as they can quickly learn specific tokens that differentiate facts from background text. This issue is partially mitigated by generating distractor facts using the same vocabularies. Enhancing the dataset's vocabulary in future versions could easily address this limitation.

We could also use other sources of facts and questions (e.g., reasoning datasets such as Rule-taker (Clark et al., 2020), ProofWriter (Tafjord et al., 2021), FOLIO (Han et al., 2022), PrOntoQA (Saparov and He, 2023), LogicNLI (Tian et al., 2021), and DELTA (Poulis et al., 2024)), mixing samples of question-answering datasets with background text from the same domain, or using LLMs to generate questions about statements that belong to the original documents. Keeping the same framework as in BABILong, this will lead to more complex and real-world scenarios.

Although recurrent approaches, like RMT, are hindered by their sequential nature, resulting in reduced parallelizability, they compensate by constant memory requirements, but it is also their limitation as storage capacity of the model is finite. However, BABILong is solvable by this type of models.

## Author contributions

Y.K., A.B. and M.B. conceived of the idea. A.B. prepared the dataset. Y.K. and A.B. scored open-source LLMs, A.B. scored RMT, I.R. scored ARMT, Mamba and Gemini pro 1.5, P.A. scored Mistal models and RAG pipelines, D.S. scored GPT-4 and GPT-3.5, A.S. fine-tuned and scored GPT-3.5, A.B. finetuned and scored Mistral. D.S. created public leaderboard. A.B. populated and maintain leaderboard. Y.K. performed study to compare MMLU, RULER and BABILong. Y.K., A.B. and M.B. aggregated and analyzed scoring results. All authors contributed to discussions of results. Y.K., A.B. and P.A. wrote the first draft of the manuscript. All the authors contributed in extending manuscript with their results. M.B. took the lead in structuring and editing manuscript towards the final version.