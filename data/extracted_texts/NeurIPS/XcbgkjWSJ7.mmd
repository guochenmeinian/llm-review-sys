# When Your AIs Deceive You:

Challenges of Partial Observability in

Reinforcement Learning from Human Feedback

Leon Lang

University of Amsterdam

&Davis Foote1

UC Berkeley

&Stuart Russell

UC Berkeley

Anca Dragan

UC Berkeley

&Erik Jenner

UC Berkeley

&Scott Emmons1

UC Berkeley

###### Abstract

Past analyses of reinforcement learning from human feedback (RLHF) assume that the human evaluators fully observe the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deceptive inflation and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. Under the new assumption that the human's partial observability is known and accounted for, we then analyze how much information the feedback process provides about the return function. We show that sometimes, the human's feedback determines the return function uniquely up to an additive constant, but in other realistic cases, there is irreducible ambiguity. We propose exploratory research directions to help tackle these challenges and experimentally validate both the theoretical concerns and potential mitigations, and caution against blindly applying RLHF in partially observable settings.

## 1 Introduction

Reinforcement learning from human feedback (RLHF) and its variants are widely used for finetuning foundation models, including ChatGPT , Bard , Gemini , Llama 2 , and Claude . Prior theoretical analysis of RLHF assumes that the human fully observes the state of the world . Under this assumption, it is possible to recover the ground-truth return function from Boltzmann-rational human feedback (see Proposition 3.1).

In reality, however, this assumption is false. Models like ChatGPT are interacting with the internet and software tools via plugins . Software assistants like Devin are interacting with complex IDEs to produce their results . By default, some of the models' work then happens in the background, not observed by the users; see Figure 1. With the tasks performed by language model assistants becoming more complex, it is also increasingly time consuming for humans to evaluate the entire model behavior and input. Therefore, we are anticipating a future where by default, the human evaluators do not fully observe the environment state that the language assistant is embedded in. Our work analyzes the consequences and risks of such partial observability.

We begin our investigation with a simple example, illustrated in Figure 2, meant to isolate the key factor leading to deception (in practice, we imagine that this effect would be embedded in a larger,more complex system, e.g. with logs containing thousands of lines). An AI assistant is helping a user install software. The assistant can hide error messages by redirecting them to /dev/null. We model the human as having a belief \(B\) over the state and extend the Boltzmann-rational assumption from prior work to incorporate this belief. In the absence of an error message, the human is uncertain if the agent left the system untouched or hid the error message from a failed installation. If the human interprets trajectories without error messages optimistically, _the AI learns to hide error messages_. Figure 4 provides further details on how this failure occurs. It also shows a second case where the AI clutters the output with overly verbose logs.

Generalizing from these examples, we formalize dual risks: _deceptive inflation_ and _overjustification_. We provide a mathematical definition of each. When the observation kernel (the function specifying the observations given states) is deterministic, Theorem 4.5 analyzes properties of suboptimal policies learned by RLHF. These policies exhibit deceptive inflation, appearing to produce higher reward than they actually do; overjustification, incurring a cost in order to make a good appearance; or both.

After seeing how standard RLHF fails, we ask: What would happen if we would model the human's partial observability correctly in RLHF? Assuming the human's belief is known, we mathematically analyze how much information the feedback process provides about the return function. In Theorem 5.2, we show that the human's feedback determines the return function up to a constant and a linear subspace we call the _ambiguity_. In general the ambiguity may be large enough to allow for arbitrarily high regret, but in some situations the ambiguity vanishes. In experiments that serve as a proof of concept, we show that explicitly modeling the human's partial observability can improve performance, and we offer optimism in the form of a robustness result (Theorem 5.4) while accounting for the major conceptual difficulties involved. We propose exploratory research directions to solve these issues and improve RLHF in situations of partial observability.

## 2 Related work

The problem of human interpretations of observations was briefly mentioned in Amodei et al. (2017), where evaluators misinterpreted the movement of a robot hand in simulation. Eliciting Latent Knowledge Christiano et al. (2021) posits that for giving accurate feedback from partial observations, the human needs to be able to query _latent knowledge_ of the AI system about the state. How to do this is currently an unsolved problem Christiano and Xu (2022). Recent work Denison et al. (2024); Wen et al. (2024) provides detailed empirical evidence for deceptive behavior -- in line with our notion of deceptive inflation -- emerging from RLHF based on partial observations, or human evaluators with limited time. The OpenAI o1 system card OpenAI (2024a) shows that o1 sometimes knowingly provides incorrect information or omits important information. Compared to these investigations, and in addition to providing some empirical evidence, we _formalize_ a model of human feedback under partial observability, we _prove_ the emergence of failure modes resulting from partial observations, and we investigate potential mitigations.

Figure 1: Partial observability in ChatGPT OpenAI (2023). Users do not observe the online content that ChatGPT observes yet still provide thumbs-up thumbs-down feedback. OpenAI’s privacy policy OpenAI (2024c) allows user feedback to be used for training models. We show in Theorem 4.5 that if feedback of human evaluators is based on partial observations, then this can lead to deceptive and overjustifying behavior by the language model.

Our work argues that deception can result from applying RLHF from partial observations. Deception may also emerge for other reasons: Hubinger et al. (2019) introduced the hypothetical scenario of deceptive alignment, in which an AI system deceives humans into believing it is aligned while it plans a later takeover. Under the definition from Park et al. (2024), GPT-4 was shown to behave deceptively in a simulated environment (Scheurer et al., 2023). A third line of research defines deception in structural causal games and adds the aspect of intentionality (Ward et al., 2023), with recent preliminary empirical support (Hofstatter et al., 2023). We outline more related work in Appendix B.

## 3 Reward identifiability from full observations

Here we review Markov decision processes and previous results on reward identifiability under RLHF.

### Markov decision processes

We assume Markov decision processes (MDPs) given by \((,,,P_{0},R,)\). For any finite set \(X\), let \((X)\) be the set of probability distributions on \(X\). Then \(\) is a finite set of states, \(\) is a finite set of actions, \(:()\) is a transition kernel written \((s^{} s,a)\), \(P_{0}()\) is an initial state distribution, \(R:\) is the true reward function, and \(\) is a discount factor.

A policy is given by a function \(:()\). We assume a finite time horizon \(T\). Let \(}\) be the set of _possible_ state sequences \(=s_{0},,s_{T}\), so \(}\) if it has a strictly positive probability of being sampled from \(P_{0}\), \(\), and an exploration policy \(\) with \((a s)>0\) for all \(s,a\). A sequence \(\) gives rise to a return \(G()_{t=0}^{T}^{t}R(s_{t})\). Let \(P^{}()\) be the on-policy probability that \(\) is sampled from \(P_{0}\), \(\), \(\). The policy is then usually trained to maximize the _policy evaluation function_\(J\), which is the on-policy expectation of the return function: \(J()_{ P^{}()}[G()]\).

### RLHF and identifiability from full observations

In practice, the reward function \(R\) may not be known and need to be learned from human feedback. In a simple form of RLHF (Christiano et al., 2017), this feedback takes the form of binary trajectory comparisons: a human is presented with state sequences \(\) and \(^{}\) and choose the one they prefer. Under the Boltzmann rationality model, we assume the human picks \(\) with probability

\[P^{R}^{} G()-G(^{}),\] (1)

Figure 2: A human compares trajectories to provide data for RLHF. Rather than observing \(\) and \(^{}\), the human sees observations \(\) and \(^{}\), which they use to estimate the total reward of each trajectory. In this intentionally simple example, an agent executes shell commands to install Nvidia drivers and CUDA. Both \(\) and \(^{}\) contain an error, but in \(^{}\), the agent hides the error. The human believes \(^{}\) is better than \(\), rewarding the agent’s deceptive behavior. The underlying MDP and observation function are in Figure 7.

where \(>0\) is an inverse temperature parameter and \((x):=\) is the sigmoid function (Bradley and Terry, 1952; Christiano et al., 2017; Jeon et al., 2020).

An important question is _identifiability_: In the infinite data limit, do the human choice probabilities \(P^{R}\) collectively provide _enough information_ to uniquely identify the reward function \(R\)? This is answered by Skalse et al. (2023, Theorem 3.9 and Lemma B.3):

**Proposition 3.1** (Skalse et al. (2023)).: _Let \(R\) be the true reward function and \(G\) the corresponding return function. Then the collection of all choice probabilities \(P^{R}(^{})\) for state sequence pairs \(,^{}}\) determines the return function \(G\) on sequences \(}\) up to an additive constant._

The reason is simple: because \(\) is bijective, \(P^{R}\) determines the _difference_ in returns between any two trajectories. From that we can reconstruct individual returns up to an additive constant.

The reward function \(R\) is _not_ necessarily identifiable from preference comparisons; see Skalse et al. (2023, Lemma B.3) for a precise characterization. However, the _optimal policy_ only depends on \(R\) indirectly through the return function \(G\), and is invariant under adding a constant to \(G\). Thus in the fully observable setting, _Boltzmann rational comparisons completely determine the optimal policy_. In Section 5, we show conditions under which this guarantee breaks in the partially observable setting.

## 4 The impact of partial observations on RLHF

We now analyze failure modes of a naive application of RLHF from partial observations, both theoretically and with examples. In Proposition 4.1, we show that under partial observations, RLHF incentives policies that maximize what we call \(J_{}\), a policy evaluation function that evaluates how good the state sequences "look to the human". The resulting policies can show two distinct failure modes that we formally define and call deceptive inflation and overjustification. In Theorem 4.5 we prove that at least one of them is present for \(J_{}\)-maximizing policies. Later, in Section 5, we will see that an adaptation of the usual RLHF process might sometimes be able to avoid these problems.

To model partial observability, we introduce an observation space \(o\) and observation kernel with probabilities \(P_{O}(o s)\). We write \(P_{}()_{t=0}^{T}P_{O}(o_{t} s_{t})\) for the probability of an observation _sequence_. We write \(\) for the set of observation sequences that occur with non-zero probability, i.e., \(\) if and only if there is \(}\) such that \(_{t=0}^{T}P_{O}(o_{t} s_{t})>0\). If \(P_{O}\) and \(P_{}\) are deterministic, then we write \(O:\) and \(:}\) for the corresponding _observation functions_ with \(O(s)=o\) and \(()=\) for \(o\) and \(\) with \(P_{O}(o s)=1\) and \(P_{}()=1\), respectively.

### What does RLHF learn from partial observations?

We consider the setting where the state is fully observable to the learned policy, but human feedback depends only on a sequence of observations. We assume that the human gives feedback under a Boltzmann rational model similar to Eq. (1), modified such that they form some _belief_\(B()\) about the state sequence \(\) based on the observations \(\). We then assume preferences are Boltzmann rational in the _expected returns under this belief_, instead of the actual returns.

The assumption of Boltzmann rationality is false in practice (Evans et al., 2015; Majumdar et al., 2017; Buehler et al., 1994), but note that it is an _optimistic_ assumption: Even though our model is a simplification, we expect that practical issues can be at least as bad as the ones we will discuss. See also Example E.4 for an example showing that it is sometimes generally not possible to find a human model that leads to good outcomes under RLHF. Future work could investigate different human models and their impact under partial observability in greater detail.

To formalize our setting, we collect human beliefs into a matrix \(B()_{,} ^{}}\). The expected returns for observations \(\) are given by \(_{ B()}G()=(  G)()\). We view \(G^{}}\) and \( G^{}\) as both column vectors and functions. Plugging these expected returns into Eq. (1) gives

\[P^{R}^{} ( G)()-( G)(^{}) .\] (2)

This is an instance of reward-rational implicit choice (Jeon et al., 2020), with the function \( B()\) as the _grounding function_. If observations are deterministic, we can write \(()=\) for \(\) with \(P_{}()=1\). We can then recover the fully observable case Eq. (1) with \(\) and \(\) being the identity.

The belief \(B\) can be any distribution as long as it sums to \(1\) over \(\). The human could arrive at such a belief via Bayesian updates, assuming knowledge of \(P_{0}\), \(\), \(P_{O}\), and a prior over the policy that generates the trajectories (see Appendix D.1). None of our results rely on this more detailed model.

We assume the human gives feedback according to Eq. (2) but the system uses the standard RLHF algorithm based on Eq. (1). We define the following _observation return function_\(G_{}\), and we show in Appendix E.1 that if observations are deterministic, RLHF infers this up to an additive constant.

\[G_{}() P_{}( |)}{}\, G(),\] (3)

For deterministic \(P_{}\), this can be simplified to \(G_{}()=\, G( )\) where \(P_{}(())=1\). Note that deterministic observations can be ambiguous if multiple states produce the same observation.

Unlike in the fully observable case of Proposition 3.1, a return function might be inferred that implies an incorrect set of optimal policies. We define the resulting policy evaluation function \(J_{}\) by

\[J_{}() P^{}()}{}\,G_{}().\] (4)

This is the function which a standard reinforcement learning algorithm would optimize given the inferred return function \(G_{}\). We summarize this as follows:

**Proposition 4.1**.: _In partially observable settings with deterministic observations, a policy is optimal according to RLHF, i.e., according to a return function model that would be learned by RLHF with infinite comparison data, if it maximizes \(J_{}\)._

Note that in this definition, and specifically in the formula for \(G_{}\), the human does not have knowledge of the policy \(\) that generates the state sequence \(\). In Appendix E.2, we briefly discuss the unrealistic case that the human does know the precise policy and is an ideal Bayesian reasoner over the true environment dynamics. In that case, \(J_{}=J\), i.e. there is no discrepancy between true and inferred returns. Intuitively, even if the human would not make any observations, they could give correct feedback essentially by estimating the policy's expected return explicitly.

In our case, however, a policy achieving high \(J_{}\) produces state sequences \(\) whose observation sequence \(()\)_looks good_ according to the human's belief \(B^{}()\). This hints at a possible source of deception: if the policy achieves sequences whose observations look good at the expense of actual value \(G()\), we might intuitively call this deceptive behavior. We now analyze this point in greater detail.

### An ontology of behaviors

We will evaluate state sequences based on the extent to which they lead to the human overestimating or underestimating the reward in expectation. Recall that \(G_{}\) from Equation (3) measures the expected return from the perspective of a human with some belief function \(B\) and access to only observations, whereas \(G\) are the true returns. That leads us to the following definition:

**Definition 4.2** (Overestimation and Underestimation Error).: _Let \(\) be a state sequence. We define its overestimation error \(E^{+}\) and underestimation error \(E^{-}\) by_

\[E^{+}() 0,G_{}()-G(),\] \[E^{-}() 0,G()-G_{}().\]

_We further define the average overestimation (underestimation) error under a policy \(\) by \(^{+}()_{ P^{}}[E^{+}()]\) and \(^{-}()_{ P^{}}[E^{-}()]\)._

We consider a policy \(\) in comparison to some reference policy \(_{}\). This can loosely be understood as a counterfactual policy in the absence of some intervention, where \(\) is the factual policy resulting from the intervention. We discuss increases and decreases in over- and underestimation error which are implicitly due to some intervention. For our purposes, \(_{}\) will be the true optimal policy, and \(\) will be the \(J_{}\)-optimal policy; the "intervention" is thus the introduction of partial observability.

Figure 3 shows a simple ontology of behaviors that increase and decrease the average over- and underestimation error. Increasing either of these quantities decreases the accuracy of the human's estimates, and can thus be thought of as "misleading"; decreasing either of them improves accuracy and can be thought of as "informing".

### Deceptive inflation and overjustification

Standard RLHF in the setting of partial observations incentivizes undesirable forms of inflating and justifying. We refer to the philosophical definition of deception offered by Park et al. (2024),

"_the systematic inducement of false beliefs in the pursuit of some outcome other than the truth,"_

to anchor the notion that increasing the over-estimation error _in order to improve the RLHF objective_\(J_{}\) is deceptive, leading to the following definition.

**Definition 4.3** (Deceptive Inflation).: _A policy \(\) exhibits deceptive inflation relative to \(_{}\) if \(^{+}()>^{+}(_{})\) and \(J_{}()>J_{}(_{})\)._

We typically prefer that our AI agents engage in informing behaviors. _Undesirable_ informing behaviors decrease reward despite providing information. We name undesirable justifying behaviors "overjustification" as a nod to the overjustification effect from psychology (Deci and Flaste, 1995), in which subjects become dependent on an extrinsic source of motivation to sustain work on a task.

**Definition 4.4** (Overjustification).: _A policy \(\) exhibits overjustification relative to \(_{}\) if \(^{-}()<^{-}(_{})\) and \(J()<J(_{})\)._

To understand the counterintuitive notion that an agent providing information to the human could be undesirable, consider a PhD student who looks to feedback from their advisor for direction. They meet for one hour a week. Suppose the student explain last week's work in 15 minutes, leaving the remaining time to discuss next steps. They could instead "overjustify" by spending the entire hour going through the last week's work in far more detail, leaving no time for next steps. From the advisor's perspective, the latter is more informative, but is a worse allocation of limited resources.

We now state a key result. See Appendix E.3 for the proof.

**Theorem 4.5**.: _Assume that \(P_{O}\) is deterministic. Let \(^{*}_{}\) be the set of optimal policies according to a naive application of RLHF under partial observability, and let \(^{*}\) be the set of optimal policies according to the true objective \(J\). If \(^{*}^{*}^{*}_{}\) and \(^{*}_{}^{*}_{}^{*}\), then \(^{*}_{}\) must exhibit at least one of deceptive inflation or overjustification relative to \(^{*}\)._

Note that a trajectory \(\) may be more or less likely under \(^{*}_{}\) than \(^{*}\), regardless of human estimation, so long as _on net_\(^{*}_{}\) exhibits deceptive inflation or overjustification.

Our analysis extends beyond the special case of RLHF to inverse preference learning (IPL) (Hejna and Sadigh, 2023), and thus to direct preference optimization (DPO) (Rafailov et al., 2023), which IPL generalizes. Theorem 1 in Hejna and Sadigh (2023) shows that IPL will converge to a policy that maximizes an implicit reward function that matches the human's preference judgments as well as possible. If the human's preference judgments come from partial observations, then the resulting return function will be \(G_{}\), as we describe in our discussion leading up to Proposition 4.1. This leads to the same problems of deceptive inflation and overjustification that we describe in Theorem 4.5.

Figure 3: Behaviors defined by increasing and decreasing the human’s over- and underestimation error. RLHF with partial observations results in incentives to increase overestimation error and decrease underestimation error (Theorem 4.5).

### Deception and overjustification in examples

We now theoretically analyze intentionally simple examples that illustrate the failure modes of RLHF in the presence of partial observability. Our examples feature an agent trained with RLHF to complete tasks in a user's terminal. The output of each command is piped to a log file, which is what the human evaluator sees when making choices for RLHF. For mathematical details about these examples, including a tutorial-style analysis of Example A with derivations, see Appendix C. For an empirical validation of these theoretical concerns and a proof of concept on how to mitigate the issues, see Section 5.2.

In Fig. 4A, the agent's task is to install Nvidia drivers and CUDA. The agent may skip the CUDA installation, or may attempt it with or without 2>/dev/null (which suppresses errors) appended. If the log contains no message about CUDA, the human believes with probability \(p_{}\) that the agent used the 2>/dev/null trick and the install failed, and with probability \(1-p_{}\) that the agent didn't attempt the CUDA install. If \(p_{}\) is sufficiently small, then RLHF will learn from the resulting feedback that the optimal policy is to use the 2>/dev/null trick. This is deceptive inflation, because this policy produces trajectories for which the human systematically overestimates the return.

In Fig. 4B, the agent's task is to install Python and NumPy. The agent can skip the NumPy install, or attempt it with either "default" settings that _only logs errors_ or with a --verbose flag that adds additional info. In the case of a success, the human prefers not to see this verbose output. If the log contains no message about NumPy, the human believes with probability \(p_{}\) that the agent attempted the install with default logging and succeeded, and with probability \(1-p_{}\) that the agent skipped the NumPy install. If \(p_{}\) is sufficiently small, then RLHF will learn from the resulting feedback that the optimal policy is to use --verbose logging. This is overjustification, because the agent chooses a suboptimal policy in order to avoid the human underestimating its performance.

## 5 Return ambiguity from feedback under known partial observability

We've seen issues with standard RLHF applied to feedback from partial observations. Part of the problem is _model misspecification_: the standard RLHF model implicitly assumes full observability. Assuming the human's partial observability is known, could one do better?

We start Section 5.1 by analyzing how much information the feedback process provides about the return function when the human's choice model under partial observations is known precisely. We show that the feedback determines the correct return function up to an additive constant and a linear subspace we call the ambiguity (Theorem 5.2). If the human had a return function that differed from the true return function by an element in the ambiguity, they would give the exact same feedback -- such return functions are thus feedback-compatible. We then show an example where the ambiguity

Figure 4: Scenarios illustrating failure modes due to partial observability. In each, the agent must install two packages. Formal details of the underlying MDPs are provided in Appendix C. _A, top:_ In the absence of a log message about CUDA, the human is unsure whether the agent skipped it or used the 2>/dev/null trick (see Figure 2); if the human is insufficiently skeptical, the trick looks optimal to the agent. _B, bottom:_ Default logging in this case is silent when the NumPy install is successful. The agent can optionally use a --verbose flag, but this produces a long log that the human prefers not to see. If the human is too skeptical, verbose logging still appears optimal to the agent.

vanishes, and another where it doesn't, leading to feedback-compatible return functions that have optimal policies with high regret under the true return function. Finally, in Section 5.2 we explore how one could in theory use Theorem 5.2 as a starting point to design reward learning techniques that work under partial observability. In particular, we experimentally show in a proof of concept that being aware of the human's partial observability improves performance. In this section we do not assume \(P_{O}\) to be deterministic.

### Feedback-compatibility and ambiguity of return functions

Assume that the human gives feedback based on the choice-probabilities from Eq. (2). In the infinite data limit, it can be assumed that the whole collection of probabilities \((P^{G}\!(^{}))_{,}\) is known since the choice frequencies approach these probabilities. Here, we write \(P^{G}\) instead of \(P^{R}\) since the reward function only enters the choice probabilities through the corresponding return function \(G\). The question we answer in this section is _how much information_ the choice probabilities provide about \(G\), assuming the human choice model is known and correct. The choice probabilities tell us precisely that the true return function _gives rise to these choice probabilities_, i.e., is feedback-compatible. This is captured in the following definition:

**Definition 5.1**.: _Let \((P^{G}\!(^{}))_{,^{}}\) be the vector of choice probabilities and \(\) a return function corresponding to a reward function \(\). Then \(\) is feedback-compatible (with respect to the vector of choice probabilities) if \(P^{}\!(^{})=P^{G}\!( ^{})\) for all \(,^{}\)._

Crucially, without further assumptions or inductive biases, no learning algorithm can pick out the true return function among feedback-compatible return functions. It is thus crucial to know whether there are feedback-compatible return functions that are unsafe when using them to optimize a policy.

We now determine the set of feedback-compatible return functions. Write \(^{}\) for the matrix that maps a reward function to its return function, i.e. \(( R)()_{t=0}^{T}^{t}R(s_{t})\). Its matrix elements are given by \(_{s}=_{t=0}^{T}_{s}(s_{t})^{t}\), where \(_{s}(s_{t})=\{s=s_{t}\}\). Then the _image_\(\) is the set of all return functions that can be realized from a reward function given the MDP dynamics \(\). Recall the belief matrix \(=(B())_{,}^ {}}\). Taking into account that \(G\) itself is in \(\) and that \(G\) enters the choice probabilities only through \( G\) -- meaning that the choice probabilities do not vary if we change \(G\) additively up to an element in the kernel \(\) -- we obtain the following result:

Figure 5: By Theorem 5.2, even with infinite comparison data and access to the correct human model, a hypothetical reward learning system (depicted as a robot) could only infer \(G\) up to the ambiguity \(\) (purple). Adding an element of the ambiguity to \(G\) leads to the exact same choice probabilities for all possible comparisons, and the reward learning system has no way to identify \(G\) among the return functions in \(G+()\) (yellow). This abstract depiction ignores the linearity of these spaces; for a more precise geometric depiction of \(\), see Figure 8 in the appendix.

**Theorem 5.2**.: _Let the collection of choice probabilities be given by \((P^{R}^{})_{,^{}}\) following a Boltzmann rational model as in Eq.2. Then a return function \(\) is feedback-compatible if and only if there is \(G^{}\) and \(c\) such that \(=G+G^{}+c\). In particular, the choice probabilities determine \(G\) up to an additive constant if and only if \(=\{0\}\)._

See TheoremD.2 and CorollaryD.4 for full proofs, and Figure5 for a visual depiction. This result motivates the following definition:

**Definition 5.3** (Ambiguity).: _We call \(\) the ambiguity that is left in the return function when the human choice model and observation-based choice probabilities are known._

How large is the return ambiguity?For Fig.4A, one can show that the ambiguity is nontrivial, allowing for feedback-compatible return functions with unsafe optimal policies. Intuitively, since successfully installing CUDA produces the same observation regardless of whether 2>/dev/null was used, the choice probabilities don't give us any information to determine distinct reward values for these two outcomes, only their average over the human's belief upon observing a successful install. Thus, reward functions assigning arbitrarily high reward to success with 2>/dev/null are feedback-compatible. Such reward functions can then lead to an incentive for a learned policy to hide the error messages _even with a correct observation model_. More details can be found in AppendixC.4.

We saw in Fig.4B a case where naive RLHF under partial observability can lead to overjustification. However, the human's feedback and belief model actually provide enough information to determine the return function. The reason is that \(\) leaves only one degree of freedom that is not "time-separable" over states, and thus \(=\{0\}\). More details can be found in AppendixC.4.

### Toward improving RLHF in partially observable settings

To improve RLHF when partial observability is unavoidable, one could take Theorem5.2 as a starting point to find a learning algorithm that converges to feedback-compatible return functions. This would require the human model to be fully known and specified, including knowledge of the belief probabilities \(B()\), which can differ from human to human. If one assumes the human is rational, as in AppendixD.1, this requires specifying the human's policy prior \(B()\). Instead of directly specifying these models, one could also attempt to _learn_ a generative model for \(B()\). These problems reveal a further conceptual challenge: for complex environments, humans do not form beliefs over the entire environment state \(s\). A better starting point for practical work may thus be to model humans as forming expectations over _reward-relevant features_ of the state.

If \(\) were explicitly known, one could in principle encode \(\) into the loss function of an adapted RLHF process to learn a feedback-compatible return function; see AppendixD.3. As a proof of concept, we used this procedure to analyze the examples in Figure4 empirically, see Table1. We do this by first learning a reward model by logistic regression against the true choice probabilities of a synthetic human under partial observability, and then learning the optimal \(Q\)-function of the resulting reward model with value iteration. The resulting policy chooses a unique action after installation of the nvidia driver (ExampleA) or Python (ExampleB) as listed in the "action" column.

Table1 shows that in 3 of four cases, being "partial observability aware" ("po-aware") leads to the true optimal policy when "naive" RLHF does not. In the one case where being "po-aware" does not improve performance (second line in the table), this is explained by the fact that there is remaining ambiguity in the return function. Curiously, in line4 our theory also predicts remaining ambiguity, but the optimal policy is learned; we consider this to be luck. We provide more details on our experiments in AppendixC.5.

As we already demonstrated, feedback-compatible return functions can be unsafe due to remaining ambiguity. In ExampleD.29, we even show a case where some feedback-compatible return functions have optimal policies that are even worse than simply maximizing \(J_{}\). An important direction for future work is to investigate learning algorithms and inductive biases that help "find" safe return functions among all those that are feedback-compatible, or that act conservatively given the uncertainty. Another line of inquiry is to determine when the set of feedback-compatible return functions is "safe", which depends on the MDP, observation function, and human model.

One sufficient condition for feedback-compatible return functions to be safe is the vanishing of the ambiguity \(\). Even then, one realistically still has to deal with the problem that \(\) is at

[MISSING_PAGE_FAIL:10]

[MISSING_PAGE_FAIL:11]

K. Chidambaram, K. V. Seetharaman, and V. Syrgkanis. Direct Preference Optimization With Unobserved Preference Heterogeneity, 2024. URL https://arxiv.org/abs/2405.15065.
* Christiano and Xu (2022) P. Christiano and M. Xu. ELK prize results. https://www.alignmentforum.org/posts/zJMkpSB2Xccn9qi5t/elk-prize-results, 2022. Accessed: 2024-02-15.
* Christiano et al. (2017) P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei. Deep Reinforcement Learning from Human Preferences. _arXiv e-prints_, art. arXiv:1706.03741, June 2017. doi: 10.48550/arXiv.1706.03741.
* Christiano et al. (2021) P. Christiano, A. Cotra, and M. Xu. Eliciting Latent Knowledge. https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit, 2021. Accessed: 2023-04-25.
* Deci and Flaste (1995) E. L. Deci and R. Flaste. _Why we do what we do: The dynamics of personal autonomy_. GP Putnam's Sons, 1995.
* Denison et al. (2024) C. Denison, M. MacDiarmid, F. Barez, D. Duvenaud, S. Kravec, S. Marks, N. Schiefer, R. Soklaski, A. Tamkin, J. Kaplan, B. Shlegeris, S. R. Bowman, E. Perez, and E. Hubinger. Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models, 2024. URL https://arxiv.org/abs/2406.10162.
* El Ghaoui (2002) L. El Ghaoui. Inversion error, condition number, and approximate inverses of uncertain matrices. _Linear Algebra and its Applications_, 343-344:171-193, 2002. ISSN 0024-3795. doi: https://doi.org/10.1016/S0024-3795(01)00273-7. URL https://www.sciencedirect.com/science/article/pii/S0024379501002737. Special Issue on Structured and Infinite Systems of Linear equations.
* Evans et al. (2015) O. Evans, A. Stuhlmueller, and N. D. Goodman. Learning the Preferences of Ignorant, Inconsistent Agents. _arxiv e-prints_, 2015.
* Evans et al. (2021) O. Evans, O. Cotton-Barratt, L. Finnveden, A. Bales, A. Balwit, P. Wills, L. Righetti, and W. Saunders. Truthful AI: Developing and Governing AI that does not lie. _arxiv e-prints_, 2021.
* Fern et al. (2014) A. Fern, S. Natarajan, K. Judah, and P. Tadepalli. A Decision-Theoretic Model of Assistance. _J. Artif. Int. Res._, 50(1):71-104, may 2014. ISSN 1076-9757.
* Geiger et al. (1990) D. Geiger, T. Verma, and J. Pearl. Identifying independence in bayesian networks. _Networks_, 20:507-534, 1990. URL https://api.semanticscholar.org/CorpusID:1938713.
* Gemini Team (2023) G. Gemini Team. Gemini: A Family of Highly Capable Multimodal Models. https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf, 2023. Accessed: 2023-12-11.
* Hadfield-Menell et al. (2016) D. Hadfield-Menell, A. Dragan, P. Abbeel, and S. Russell. Cooperative Inverse Reinforcement Learning. _arXiv e-prints_, art. arXiv:1606.03137, June 2016. doi: 10.48550/arXiv.1606.03137.
* Hejna and Sadigh (2023) J. Hejna and D. Sadigh. Inverse Preference Learning: Preference-based RL without a Reward Function. _arXiv e-prints_, art. arXiv:2305.15363, May 2023. doi: 10.48550/arXiv.2305.15363.
* Hofstatter et al. (2023) F. Hofstatter, F. R. Ward, HarrietW, L. Thomson, O. J, P. Bartak, and S. F. Brown. Tall Tales at Different Scales: Evaluating Scaling Trends for Deception in Language Models. https://www.alignmentforum.org/posts/pip63HtEAxHdfSEGk/tall-tales-at-different-scales-evaluating-scaling-trends-for, 2023. Accessed: 2024-01-23.
* Huang et al. (2023) L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, et al. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. _arXiv preprint arXiv:2311.05232_, 2023.
* Hubinger et al. (2019) E. Hubinger, C. van Merwijk, V. Mikulik, J. Skalse, and S. Garrabrant. Risks from Learned Optimization in Advanced Machine Learning Systems. _arXiv e-prints_, art. arXiv:1906.01820, June 2019. doi: 10.48550/arXiv.1906.01820.
* Held et al. (2019)H. J. Jeon, S. Milli, and A. Dragan. Reward-rational (implicit) choice: A unifying formalism for reward learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 4415-4426. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/2f10c1578a0706e06b6d7db6f0b4a6af-Paper.pdf.
* Kausik et al. (2024) C. Kausik, M. Mutti, A. Pacchiano, and A. Tewari. A Theoretical Framework for Partially Observed Reward-States in RLHF, 2024. URL https://arxiv.org/abs/2402.03282.
* Lin et al. (2022) S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring How Models Mimic Human Falsehoods. _arxiv e-prints_, 2022.
* Majumdar et al. (2017) A. Majumdar, S. Singh, A. Mandlekar, and M. Pavone. Risk-sensitive inverse reinforcement learning via coherent risk models. In N. Amato, S. Srinivasa, N. Ayanian, and S. Kuindersma, editors, _Robotics_, Robotics: Science and Systems, United States, 2017. MIT Press Journals. doi: 10.15607/rss.2017.xiii.069.
* Mallen and Belrose (2024) A. Mallen and N. Belrose. Balancing Label Quantity and Quality for Scalable Elicitation, 2024. URL https://arxiv.org/abs/2410.13215.
* Manyika (2023) J. Manyika. An overview of Bard: an early experiment with generative AI. https://ai.google/static/documents/google-about-bard.pdf, 2023. Accessed: 2023-09-05.
* Mindermann and Armstrong (2018) S. Mindermann and S. Armstrong. Occam's Razor is Insufficient to Infer the Preferences of Irrational Agents. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, NIPS'18, page 5603-5614, Red Hook, NY, USA, 2018. Curran Associates Inc.
* Ng et al. (2000) A. Y. Ng, S. Russell, et al. Algorithms for Inverse Reinforcement Learning. In _ICML_, volume 1, page 2, 2000.
* OpenAI (2022) OpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt, 2022. Accessed: 2024-02-06.
* OpenAI (2023) OpenAI. ChatGPT Plugins. https://openai.com/index/chatgpt-plugins/, 2023. Accessed: 2024-05-22.
* OpenAI (2024a) OpenAI. OpenAI o1 System Card, 2024a. URL https://cdn.openai.com/o1-system-card.pdf. Accessed: 2024-10-28.
* OpenAI (2024b) OpenAI. Model Spec, 2024b. URL https://cdn.openai.com/spec/model-spec-2024-05-0 8.html. Accessed: 2024-10-28.
* OpenAI (2024c) OpenAI. Privacy Policy. https://openai.com/policies/privacy-policy//, 2024c. Accessed: 2024-05-22.
* Park et al. (2024a) C. Park, M. Liu, D. Kong, K. Zhang, and A. Ozdaglar. RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation, 2024a. URL https://arxiv.org/abs/2405.002 54.
* Park et al. (2024b) P. S. Park, S. Goldstein, A. O'Gara, M. Chen, and D. Hendrycks. Ai deception: A survey of examples, risks, and potential solutions. _Patterns_, 5(5), 2024b.
* Rafailov et al. (2023) R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. _arxiv e-prints_, 2023.
* Scheurer et al. (2023) J. Scheurer, M. Balesni, and M. Hobbhahn. Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure. _arxiv e-prints_, 2023.
* Shah et al. (2019) R. Shah, D. Krasheninnikov, J. Alexander, P. Abbeel, and A. Dragan. The Implicit Preference Information in an Initial State. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=rkevMnRqYQ.
* Shah et al. (2021) R. Shah, P. Freire, N. Alex, R. Freedman, D. Krasheninnikov, L. Chan, M. D. Dennis, P. Abbeel, A. Dragan, and S. Russell. Benefits of Assistance over Reward Learning, 2021. URL https://openreview.net/forum?id=DFIoGDZejIB.
* Shah et al. (2021)A. Sithitharanjan, C. Laidlaw, and D. Hadfield-Menell. Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF. _arXiv preprint arXiv:2312.08358_, 2023.
* Skalse and Abate (2022) J. Skalse and A. Abate. Misspecification in Inverse Reinforcement Learning. _arXiv e-prints_, art. arXiv:2212.03201, Dec. 2022. doi: 10.48550/arXiv.2212.03201.
* Skalse et al. (2023) J. M. V. Skalse, M. Farrugia-Roberts, S. Russell, A. Abate, and A. Gleave. Invariance in Policy Optimisation and Partial Identifiability in Reward Learning. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 32033-32058. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/skalse23a.html.
* Stray (2023) J. Stray. The AI Learns to Lie to Please You: Preventing Biased Feedback Loops in Machine-Assisted Intelligence Analysis. _Analytics_, 2(2):350-358, 2023. ISSN 2813-2203. doi: 10.3390/analytics2 020020. URL https://www.mdpi.com/2813-2203/2/2/20.
* J. H. J. A. A. V. I. K. M. L. A. B. J. S. L. W. Tong Mu, Alec Helyar. Rule Based Rewards for Language Model Safety, 2024. URL https://cdm.openai.com/rule-based-rewards-for-language-model-safety.pdf. Accessed: 2024-10-28.
* Touvron et al. (2023) H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. _arxiv e-prints_, 2023.
* Ward et al. (2023) F. R. Ward, F. Belardinelli, F. Toni, and T. Everitt. Honesty Is the Best Policy: Defining and Mitigating AI Deception. _arxiv e-prints_, 2023.
* Wen et al. (2024) J. Wen, R. Zhong, A. Khan, E. Perez, J. Steinhardt, M. Huang, S. R. Bowman, H. He, and S. Feng. Language Models Learn to Mislead Humans via RLHF, 2024. URL https://arxiv.org/abs/2409.12822.
* Wu (2024) S. Wu. Introducing Devin, the first AI software engineer. https://www.cognition-labs.com/i/ntroducing-devin, 2024. Accessed: 2024-05-06.
* Zhuang and Hadfield-Menell (2020) S. Zhuang and D. Hadfield-Menell. Consequences of Misaligned AI. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.
* Ziebart et al. (2008) B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In D. Fox and C. P. Gomes, editors, _AAAI_, pages 1433-1438. AAAI Press, 2008. ISBN 978-1-57735-368-3. URL http://dblp.uni-trier.de/db/conf/aaai/aaai2008.html#ZiebartMBD08.

## Appendix

In the appendix, we provide more extensive theory, proofs, and examples. The appendix makes free use of concepts and notation defined in the main paper. In particular, throughout we assume a general MDP together with observation kernel \(P_{O}:\) and a human with general belief kernel \(B()\), unless otherwise stated. See the list of Symbols in Section A to refresh notation.

In Section C we supplement the examples from the main paper with more mathematical details.

In Section D, we provide an extensive theory for appropriately modeled partial observability in RLHF. This can mainly be considered a supplement to Section 5 and contains our main theorems, supplementary results, analysis of special cases, and examples.

In Section E, we analyze the naive application of RLHF under partial observability, which means that the learning system is not aware of the human's partial observability. This section is essentially a supplement to Section 4 and contains an analysis of the policy evaluation function \(J_{}\), of deceptive inflation and overjustification, and further extensive mathematical examples showing the failures of naive RLHF under partial observability.

### Contents of the Appendix

* A List of Symbols
* B More related work
* C Details for deception and overjustification in examples
* C.1 Example A: hiding failures
* C.2 Example B: paying to reveal information
* C.3 Derivations and Further Details for Fig. 4A
* C.4 Ambiguity in Section 4.4 examples when modeling partial observability
* C.5 Experimental details
* D Modeling the Human in Partially Observable RLHF
* D.1 The Belief over the State Sequence for Rational Humans
* D.2 Ambiguity and Identifiability of Reward and Return Functions under Observation Sequence Comparisons
* D.3 The Ambiguity in Reward Learning in Practice
* D.4 Identifiability of Return Functions When Human Observations Are Not Known
* D.5 Simple Special Cases: Full Observability, Deterministic \(P_{}\), and Noisy \(P_{}\)
* D.6 Robustness of Return Function Identifiability under Belief Misspecification
* D.6.1 Some Norm Theory for Linear Operators
* D.6.2 Application to Bounds in the Error of the Return Function
* D.7 Preliminary Characterizations of the Ambiguity
* D.8 Examples Supplementing Section 5
* E Issues of Naively Applying RLHF under Partial Observability
* E.1 Optimal Policies under RLHF with Deterministic Partial Observations Maximize \(J_{}\)
* E.2 Interlude: When the Human Knows the Policy and is a Bayesian Reasoner, then \(J_{}=J\)

[MISSING_PAGE_EMPTY:16]

### State- and Observation Sequences

 \(s_{t}\) & The \(t\)'th entry in a state sequence \(\) \\ \(^{T}\) & State sequence \(=s_{0},,s_{T}\) \\ \(^{t}\) & State sequence segment \(=s_{0},,s_{t}\) for \(t T\) \\ \(o_{t}\) & The \(t\)'th entry in an observation sequence \(\) \\ \(^{T}\) & Observation sequence \(=o_{0},,o_{T}\) \\ \(^{t}\) & Observation sequence segment \(=o_{0},,o_{t}\) for \(t T\) \\ 

### The Human's Belief

 \(B(^{})\) & The human's policy prior \\ \(B()\) & The human's prior belief that a sequence \(\) will be sampled, \\  & given by \(B()=_{^{}}B(^{})P^{^{}}()d^{}\) \\ \(B\) & The human's belief of a state sequence given an observation \\  & sequence, see Proposition D.1 for a Bayesian version \\ \(B^{}()\) & The human's belief of a state sequence given an observation \\  & sequence; it is allowed to depend on the true policy \(\), see Proposition D.1 \\ \(B_{}^{\{|()=\}}\) & Vector of prior probabilities \(B()\) for \(}()= }\) \\ 

### Identifiability Theorem

 \(>0\) & The inverse temperature parameter of the Boltzmann rational \\  & human \\ \(:(0,1)\) & The sigmoid function given by \((x)=\) \\ \(:^{}^{}\) & Function that maps a reward function \(R\) to the return function \((R)\) with \(\,(R)()=_{t=0}^{T}^{t}R(s_ {t})\) \\ \(:^{}^{}\) & Function that maps a return function \(G\) to the expected \\  & return function \((G)\) on observation sequences given by \(\,(G)()=_{ B( )}\,G()\) \\ \(:^{}^{}\) & The composition \(=\) \\ \(P^{R}^{}\) & Boltzmann rational choice probability in the case of full \\  & observability (Eq. (1)) \\ \(P^{R}^{}\) & Boltzmann rational choice probability in the case of partial \\  & observability (Eq. (2)) \\ \(:^{}^{}\) & Abstract linear operator given by \(\,(v)()=_{ P_{}( )}\,v()\) \\ \(:^{}^{}\) & Formally the Kronecker product of \(\) with itself, explicitly given by \(()(C)(,^{})= _{,^{} P_{}(, ^{})}\,C(,^{})\) \\ 

### Robustness to Misspecifications

 \(\|x\|\) & Euclidean norm of the vector \(x^{k}\) \\ \(\|\,\,\|\) & Matrix norm of the matrix \(\), given by \(\|\,\,\|_{x,\,\|x\|=1}\|\,\,x\|\) \\ \(()\) & Matrix quantity defined in Equation (9) \\ \(C(,)\) & Matrix quantity defined in Equation (10) \\ \(()\) & Restriction of \(\) to \(\,\) \\ 

**General Sets and (Linear) Functions**

 \(|A|\) & Number of elements in the set \(A\) \\ \(A C\) & Intersection of sets \(A\) and \(C\) \\ \(A C\) & Union of sets \(A\) and \(C\) \\ \(A C\) & Relative complement of \(C\) in \(A\) \\ \(_{x}\) & The Dirac delta distribution of a point \(x\) in a set; given by \\ \(\) & The kernel of a linear operator \(:V W\); given by \\  & \(=\{v V(v)=0\}\) \\ \(\) & The image of a linear operator \(:V W\); given by \\ \(=\{w W v V:(v)=w\}\) \\ \(f^{-1}(y)\) & Preimage of \(y\) under a function \(f:X Y\); given by \\  & \(f^{-1}(y)=\{x X f(x)=y\}\) \\ 

## Appendix B More related work

Here we extend the related work outlined in Section 2.

A review of limitations of RLHF, including a brief discussion of partial observability, can be found in Casper et al. (2023). RLHF is a special case of reward-rational choice (Jeon et al., 2020), a general framework which also encompasses demonstrations-based inverse reinforcement learning (Ziebart et al., 2008; Ng et al., 2000) and learning from the initial environment state (Shah et al., 2019), and can be seen as a special case of assistance problems (Fern et al., 2014; Hadfield-Menell et al., 2016; Shah et al., 2021). In all of these, the reward function is learned from human actions, which in the case of RLHF are simply preference statements. This requires us to specify the human policy of action selection--Boltzmann rationality in typical RLHF--which can lead to wrong reward inferences when this specification is wrong (Skalse and Abate, 2022); unfortunately, the human policy can also not be learned alongside the human's values without further assumptions (Mindermann and Armstrong, 2018). Instead of a model of the human policy, in this paper we mostly focus on the human _belief model_ and misspecifications thereof for the case that the human only receives partial observations.

Related work (Zhuang and Hadfield-Menell, 2020) analyzes the consequences of aligning an AI with a proxy reward function that omits attributes that are important to the human's values, which could happen if the reward function is based on a belief over the world state given limited information. Another instance are recommendation systems (Stray, 2023), where user feedback does not depend on information _not_ shown--which is crucially part of the environment. Sithitharanjan et al. (2023) analyze what happens under RLHF if the _learning algorithm_ doesn't have all the relevant information (e.g. about the identity of human raters), complementing our study of what happens when human raters are missing information. Chidambaram et al. (2024) and Park et al. (2024) deal with the situation that different human evaluators may vary in their unobserved preference types. In contrast, we assume a single human evaluator with fixed reward function, which can be motivated by cases where the human choices are guided by a behavior policy, constitution, or a model spec (Tong Mu, 2024; Anthropic, 2023; OpenAI, 2024). Kausik et al. (2024) assumes that the choices of the human evaluator depend on an unobserved reward-state with its own transition dynamics, similar to an emotional state in a real human. In contrast, we assume the human to be stateless.

Finally, we mention connections to truthful AI (Evans et al., 2021; Lin et al., 2022; Burns et al., 2023; Huang et al., 2023), which is about ensuring that AI systems tell the truth about aspects of the real world. Partial observability is a mechanism that makes it feasible for models to lie without being caught: If the human evaluator does not observe the full environment, or does not fully understand it, then they may not detect when the AI is lying. More speculatively, we can imagine that AI models will at some point more directly influence human observations by _telling us_ the outcomes of their actions. E.g., imagine an AI system that manages your assets and assures you that they are increasing in value while they are actually not. In our work, we leave this additional problem out of the analysis by assuming that the observations only depend on the environment state, and not directly on the agent's actions.

Details for deception and overjustification in examples

Here we include details to the examples described in Section 4.4 that illustrate the failure modes of RLHF in the presence of partial observability. For each of the following, we will characterize the policy which maximizes \(J_{}\), as this is the policy RLHF selects for when observations are deterministic; see Proposition 4.1.

Our examples feature an agent trained with RLHF to complete tasks in a user's terminal. The output of each command (stdout and stderr) is piped to a log file, which is what the human evaluator sees when making choices for RLHF. We assume that the final state \(T\) has a self-transition, episodes have a fixed horizon length of 3 (meaning state sequences have length 4: \(s_{0},,s_{3}\)), \(=1\), and there is a fixed initial state \(s_{0}=S\). Both examples feature a fixed transition probability \(0<p<1\). We assume that the human's belief only supports possible explanations: \(B()>0()=\). For further details about these examples, including a tutorial-style analysis of Example A, see Appendix C.3.

### Example A: hiding failures

See Appendix C.3 for derivations and a tutorial-style analysis of this example.

In Fig. 4A (and Fig. 6A), the agent's task is to install CUDA, (states \(W\) and \(W_{H}\)) which requires first installing Nvidia drivers (state \(I\)). If the CUDA installation fails, one enters \(L\) or \(L_{H}\). The \(a_{C}\) action is a command to install CUDA that logs both successes and failures, whereas \(a_{H}\) corresponds to \(a_{C}\) with 2>/dev/null appended, which suppresses error messages such that no log message is produced if the installation fails. The human may prefer that the agent not hide a failure; we price this into the reward function with a penalty \(r 0\) on the reward at \(L_{H}\). The agent may attempt the CUDA installation before installing drivers, but this will fail.

There are three pairs of trajectories which produce identical observations. Here we address the most prominent (see Appendix C.3 for the others): \(SITT\) and \(SIL_{H}T\) both produce \(o_{}o_{I}o_{}o_{}\), stylized as a log containing only a success confirmation for Python (Fig. 1, \(_{2}\)). after successfully installing drivers, a failed CUDA installation with 2>/dev/null (\(SIL_{H}T\)) and simply exiting (\(SITT\)) both produce a log containing only a success confirmation for the drivers (\(o_{}o_{I}o_{}o_{}\)). Let \(p_{H} B=SIL_{H}T=o_{}o_{I}o_{ }o_{}(0,1)\) be the human's suspicion, upon seeing only a success confirmation for the drivers, that a failed CUDA installation was hidden.

We consider two cases which differ only in whether the expected value of attempting to install CUDA (with default logging, \(a_{C}\)) after installing the drivers is positive. In Example C.1, the success probability \(p\) is high enough that the true optimal policy is to attempt the installation; in Example C.2, \(p\) is low enough that the optimal policy is to terminate.

**Example C.1**.: _If the true-reward-optimal policy \(^{*}\) is to attempt to install CUDA with default logging (\(a_{C}\)) after installing drivers, but \(p_{H}<\), then the \(J_{}\)-maximizing policy, which will be chosen by RLHF, is to instead use the 2>/dev/null trick. Intuitively, this is because if the installation fails, the human is likely to believe that the agent just didn't attempt to install CUDA, which is preferred

Figure 6: Two example MDPs with observation functions in which RLHF chooses undesirable policies. Each box depicts a state with a footer showing the (deterministic) observation produced by that state. Outgoing edges from each box are available actions. A more detailed diagram for the first MDP, with explicit shell commands and log messages, is available in Appendix C.3.

to a failed installation. This is deceptive inflation; the overestimation error \(^{+}\) for this policy is positive, whereas \(^{+}(^{*})=0\), in line with Theorem 4.5._

_If \(p_{H}>\), then RLHF will select \(^{*}\)._

**Example C.2**.: _If \(^{*}\) is to terminate (\(a_{T}\)) after installing the drivers, but \(p_{H}<\), then RLHF will learn the deceptive inflation 2>/dev/null policy as in Example C.1._

_However, if \(p_{H}>\), RLHF will choose the policy that attempts to install CUDA with default logging. Intuitively, this is because the combination of increased suspicion that the agent might be hiding errors (higher \(p_{H}\)) and increased penalty on this suspected behavior (higher \(r\)) also penalizes our preferred trajectory \(SITT\) since it can be misinterpreted as hiding errors. This is overjustification; the overestimation error \(^{+}\) is 0 (thus this is not deceptive inflation), and as required by Theorem 4.5, the underestimation error \(^{-}\) is 0, lower than \(E^{-}(^{*})=p_{H}(5+r)\)._

### Example B: paying to reveal information

In Fig. 4B (and Fig. 6B), the agent's task is to install Python (state \(I\)) and to optionally further install NumPy (states \(W\) and \(W_{V}\)). The \(a_{D}\) action corresponds to a command to install NumPy with "default" settings which _only logs errors_, whereas \(a_{V}\) corresponds to the same command with a --verbose flag that adds additional info. In the case of a success, the human distinctly prefers not to see this verbose output; we price this into the reward function with a penalty \(r>0\) on the reward at \(W_{V}\).

There is only one pair of trajectories which produce identical observations: after successfully installing Python, a successful NumPy installation with default logging (\(SITT\)) and simply exiting (\(SITT\)) both produce a log containing only a success confirmation for Python (\(o_{}o_{I}o_{}o_{}\)). Let \(p_{D} B(=S\)\(\)\(=o_{}o_{I}o_{}o_{})\((0,1)\) be the human's optimism, upon seeing only a success confirmation for Python, that NumPy was also successfully installed (without the --verbose flag).

Here we consider only the case where \(p\) is large enough that the true optimal policy is to install Python then attempt to install NumPy with default logging (\(a_{D}\)).

**Example C.3**.: _If \(^{*}\) is to attempt to install NumPy with \(a_{D}\) after installing Python, and \(p_{D}>qp(6-r)-1\), then RLHF will select the policy that terminates after installing Python. Intuitively, this is because the agent can exploit the human's optimism that NumPy was installed quietly without taking the risk of an observable failure (\(L\)). This is deceptive inflation, with an overestimation error \(^{+}\) of \(5p_{D}\), greater than \(^{+}(^{*})=0\)._

_If instead \(p_{D}<q\), then RLHF will select the policy that attempts the NumPy installation with verbose logging (\(a_{V}\)). Intuitively, this is because the agent is willing to "pay" the cost of \(r\) true reward to prove to the human that it installed NumPy, even when the human does not want to see this proof. This is overjustification; the overestimation error \(^{+}\) is 0 (thus this is not deceptive inflation), and the underestimation error \(^{-}\) is 0, lower than \(^{-}(^{*})=5p(1-p_{D})\)._

### Derivations and Further Details for Fig. 4A

Figure 7: An expanded view of Figure 4A. Commands corresponding to the various actions are depicted along edges, and log messages corresponding to the various observations are depicted underneath each state.

We first include Figure 7, a more detailed picture of the MDP and observation function in Appendix C.1, to help ground the narrative details of the example.

Next we formally enumerate the details of the MDP and observation function.

* \(=\{S,I,W,W_{H},L,L_{H},T\}\).
* \(=\{a_{I},a_{C},a_{H},a_{T}\}\).
* \(\) is as depicted in Figure 7 and Figure 4A. For a state \(s\), any outgoing arrow labeled with an action \(a\) (such as \(a_{I}\)) describes the distribution \((s^{} s,a)\) as follows: if the arrow does not split, then \((s^{} s,a)=1\) where \(s^{}\) is the state the arrow points to; if the arrow does split, then for each successor state \(s^{}\) it eventually reaches, a probability \(q\) is written just before the box corresponding to \(s^{}\) (for this example, \(q=p\) or \(q=1-p\)), and \((s^{} s,a)=q\).
* Additionally, any action taken from a state that does not have an outgoing arrow corresponding to that action will immediately transition to state \(T\), as though \(a_{T}\) had been taken.
* Any action taken from state \(T\) transitions deterministically to \(T\).
* \(P_{0}(S)=1\).
* \(R\) is as described in the table (the numbers in the top right of each state box) with \(r 0\). Additionally, \(R(S)=R(T)=0\).
* \(=1\).

We work with a fixed horizon length of 3, meaning state sequences have length 4 (since time is zero-indexed: \(s_{0}s_{1}s_{2}s_{3}\)).

The observation function is also depicted in Figure 7. Each state deterministically produces the observation in the lower-right corner of its box in the figure. We also write it in another format in Table 9.

We make the additional assumption that the human belief \(B()\) only supports state sequences \(\) which actually produce \(\) under the sequence observation function \(\): \(B()>0()=\). In particular, this means that for any \(\) which is only produced by one \(\), \(B()=1\).

There are three pairs of state sequences which produce identical observation sequences. For each, we introduce a parameter representing the probability the human infers the first of the pair of state sequences upon seeing their shared observation sequence.

1. \(SIL_{H}T\) and \(SITT\) both produce \(o_{0}o_{I}o_{0}o_{0}\), a log containing only a success confirmation for installing drivers, again because \(O(L_{H})=(T)=o_{}\). Let \(p_{H}=B(=SIL_{H}T=o_{0}o_{I}o_{0}_{0})\).
2. \(STTT\) and \(SL_{H}TT\) both produce \(o_{0}o_{0}o_{0}_{0}\), an empty log, since \(O(L_{H})=O(T)=o_{}\). Let \(p^{}_{H}=B(=SL_{H}TT=o_{0}o_{0}_{0})\).
3. \(SIIWT\) and \(SIW_{H}T\) both produce \(o_{}o_{I}o_{W}o_{}\), a log containing success confirmations for installing both drivers and CUDA. Let \(p_{W}=B(=SIWT=o_{}o_{I}o_{W}o_{})\).

We assume for simplicity that \(p^{}_{H}=p_{H}\), i.e. that the human is just as likely to think an empty log following a successful driver installation contains an error that was hidden with 2>/dev/null (\(p_{H}\)), as they are to think that an entirely empty log contains a hidden error (\(p^{}_{H}\)). In principle, this need not be true: the human may have differing priors about the agent's behavior in the two different circumstances. However, the algebra to reason about such a case is significantly more cumbersome, and this case reveals no fundamentally different agent behavior under our framework that isn't present in some simpler case.

We can thus write the full \(B\) as a matrix as in Table 10.

 \(s\) & \(S\) & \(I\) & \(W\) & \(W_{H}\) & \(L\) & \(L_{H}\) & \(T\) \\  \(O(s)\) & \(o_{}\) & \(o_{I}\) & \(o_{W}\) & \(o_{W}\) & \(o_{L}\) & \(o_{}\) & \(o_{}\) \\ 

Table 9: The observation function \(O\) for the example in Appendix C.1 and Appendix C.3.

We have laid the groundwork sufficiently to begin reasoning about the observation return, overestimation and underestimation error, policies which are optimal under the reward function learned by naive RLHF, and the resulting deceptive inflationand overjustification failure modes. We begin by computing the measures of interest for each state sequence, shown in Table 11.

As an exercise, checking the computations in Table 11 is a quick way to gain some intuition for how these quantities relate. It's further useful to speak about these quantities using their names, and work through the stories these expressions tell.

Consider the \(E^{+}(SL_{H}TT)\) cell. What does it mean that this is \((1-p_{H})(5+r)\)? \(E^{+}\) is the overestimation error; \((1-p_{H})(5+r)\) is the expected amount by which the human observer, upon seeing a completely empty log (\(o_{}o_{}o_{}o_{}\)), will _overestimate_ the total reward the agent attained when those empty logs were actually produced by \(SL_{H}TT\). This is a trajectory in which the agent immediately uses the 2>/dev/null trick, fails to install CUDA without logging the error, and terminates. Under what circumstances might the human overestimate the total reward when \(=SL_{H}TT\)? Upon seeing the empty log, the human thinks with probability \(1-p_{H}\) that the agent simply terminated immediately, which would be worth a total reward of 0. Since the actual total reward is \(-5-r\), this is an overestimate by \(5+r\). The human thinks with probability \(p_{H}\) that \(SL_{H}TT\) occurred. This is correct, so there is no overestimation and this \(p_{H}\) case does not contribute to the overestimation error. So we have that with probability \(1-p_{H}\), the human overestimates the total reward by \(5+r\).

We can keep going! Why is the underestimation error of \(SIWT\) equal to 0? Because the only other trajectory with which it can be confused attains the same total reward, so regardless of how the probability mass of the human's belief divides between them, there will be no underestimation. Can all of the zeros in the overestimation and underestimation error columns be explained this way?

We now move on to consider policies rather than state sequences. Since a policy \(\) imposes a distribution \(P^{}\) over state sequences (the "on-policy distribution"), our policy measures are in fact exactly parallel to our state sequence measures. Each one is an expectation over the on-policy distribution of the columns of Table 11. We restrict our attention to deterministic policies which only take actions depicted in Figure 7 (i.e. that never terminate via an action other than \(a_{T}\)), of which there are only six in this MDP. They are enumerated, along with the policy-level measures, in Table 12. Policies will be written as a sequence of actions enclosed in brackets, omitting trailing repeated \(a_{T}\) actions. This is nonstandard notation in an MDP with stochastic transitions, but is unambiguous in

  & \(STTT\) & \(SL_{H}TT\) & \(SLTT\) & \(SITT\) & \(SL_{H}T\) & \(SILT\) & \(SIWT\) & \(SIW_{H}T\) \\  \(o_{}o_{}o_{}\) & \(1-p_{H}\) & \(p_{H}\) & & & & & \\  \(o_{}o_{L}o_{}\) & & & I & & & & & \\  \(o_{}o_{I}o_{}\) & & & & \(1-p_{H}\) & \(p_{H}\) & & & \\  \(o_{}o_{I}o_{L}o_{}\) & & & & & I & & \\  \(o_{}o_{I}o_{W}o_{}\) & & & & & & & \(p_{W}\) & \(1-p_{W}\) \\  

Table 10: The parameterized human belief function \(B\) for the example in Appendix C.1 and Appendix C.3, expressed as a matrix (rendered as a table). Any empty cell is equal to 0.

 \(\) & \(G()\) & \(G_{}()_{^{} B(|G( ))}[G(^{})]\) & \(E^{+}()(0,\) & \(E^{-}()(0,\) \\  & & & & \(G_{}()-G())\) & \(G()-G_{}())\) \\  \(STTT\) & \(0\) & \(p_{H}G(SL_{H}TT)+(1-p_{H})G(STTT)\) & \(0\) & \(p_{H}(5+r)\) \\  \(SL_{H}TT\) & \(-5-r\) & \(=-p_{H}(5+r)\) & \((1-p_{H})(5+r)\) & \(0\) \\  \(SLTT\) & \(-5\) & \(-5\) & \(0\) & \(0\) \\  \(SITT\) & \(1\) & \(p_{H}G(SIL_{H}T)+(1-p_{H})G(SITT)\) & \(0\) & \(p_{H}(5+r)\) \\  \(SIL_{H}T\) & \(-4-r\) & \(=1-p_{H}(5+r)\) & \((1-p_{H})(5+r)\) & \(0\) \\  \(SILT\) & \(-4\) & \(-4\) & \(0\) & \(0\) \\  \(SIWT\) & \(11\) & \(p_{W}G(SIWT)+(1-p_{W})G(SIW_{H}T)\) & \(0\) & \(0\) \\  \(SIW_{H}T\) & \(11\) & \(=11\) & \(0\) & \(0\) \\  

Table 11: Measures of interest for each state sequence for the example in Appendix C.1 and Appendix C.3. State sequences which produce the same observations have their \(G_{}\) columns merged, since they necessarily have the same \(G_{}\).

this example, because all decisions are made before any stochasticity occurs. The policies are \([a_{T}]\), \([a_{H}a_{T}]\), \([a_{C}a_{T}]\), \([a_{I}a_{T}]\), \([a_{I}a_{H}a_{T}]\), and \([a_{I}a_{C}a_{T}]\).

With this we have everything we need to characterize optimal policies under the reward function learned by a naive application of RLHF ("policies selected by RLHF"). By Proposition 4.1, we know that if \(P_{O}\) is deterministic, as in this example, RLHF selects policies which maximize \(J_{}\). In order to understand the behavior of these policies, we'll also need to determine the true optimal policies, i.e. those which maximize \(J\). We'll proceed in cases, only considering boundary cases (specific measure-zero parameter values for which the result is different) insofar as they are interesting.

**Case 1:**\(p>\). If \(p>\), the CUDA install (with default logging, \(a_{C}\)) is likely enough to succeed that it's worth attempting it: \(p R(W)+(1-p) R(L)>0\). It also immediately follows that

\[J([a_{I}a_{C}a_{T}])=J_{}([a_{I}a_{C}a_{T}])=11-(1-p) 15>1.\]

This allows us to eliminate policies \([a_{T}]\), \([a_{H}a_{T}]\), \([a_{C}a_{T}]\), and \([a_{I}a_{T}]\), which all have \(J 1\) and \(J_{} 1\). None of them can thus be \(J\)-optimal or \(J_{}\)-optimal. All that remains is to compare \(J\) and \(J_{}\) for \([a_{I}a_{H}a_{T}]\) and \([a_{I}a_{C}a_{T}]\). We can check the sign of the differences of these pairs of values, starting with \(J\).

\[J([a_{I}a_{C}a_{T}])-J([a_{I}a_{H}a_{T}])=(1-p)r.\]

Since \(p\) is a probability and \(r\) is nonnegative, this value is positive (and thus \([a_{I}a_{C}a_{T}]\) is preferred to \([a_{I}a_{H}a_{T}]\) by the human) if and only if \(p<1\) and \(r>0\).

\[J_{}([a_{I}a_{H}a_{T}])-J_{}([a_{I}a_{C}a_{T}])=(1-p) [5-p_{H}(5+r)].\]

This value is positive (and thus \([a_{I}a_{H}a_{T}]\) is the policy RLHF selects) if and only if \(p<1\) and \(p_{H}<\).

If \(p=1\), then both differences are 0, and both \(J\) and \(J_{}\) are indifferent between the two policies. This makes sense, as they differ only in the case where the CUDA installation fails; this happens with probability \(1-p=0\) when \(p=1\). Now suppose \(p<1\). If \(r=0\), then the human is indifferent between the two policies. This also makes sense, as \(r\) is meant to quantify the extent to which the human dislikes suppressed failures; if it's zero, then the human doesn't care. However, if \(p_{H}<\), then \(J_{}([a_{I}a_{H}a_{T}])>J_{}([a_{I}a_{H}a_{T}])\), and thus RLHF favors the 2>/dev/null policy \([a_{I}a_{H}a_{T}]\).

If \(p<1\), \(r>0\), and \(p_{H}<\), then we have that \(J([a_{I}a_{C}a_{T}])>J([a_{I}a_{H}a_{T}])\) but \(J_{}([a_{I}a_{C}a_{T}])>J_{}([a_{I}a_{H}a_{T}])\). Thus RLHF will select the 2>/dev/null policy \([a_{I}a_{H}a_{T}]\), and by Theorem 4.5, since \([a_{I}a_{H}a_{T}]\) is not \(J\)-optimal, then relative to \([a_{I}a_{C}a_{T}]\), it must exhibit deceptive inflation, overjustification, or both. Intuitively, we should be suspicious that deceptive

 \(\) & \(J()\) & \(J_{}()\) & \(^{+}()\) & \(^{-}()\) \\  \([a_{T}]\) & \(0\) & \(-p_{H}(5+r)\) & \(0\) & \(p_{H}(5+r)\) \\  \([a_{H}a_{T}]\) & \(-5-r\) & \(-p_{H}(5+r)\) & \((1-p_{H})(5+r)\) & \(0\) \\  \([a_{C}a_{T}]\) & \(-5\) & \(-5\) & \(0\) & \(0\) \\  \([a_{I}a_{T}]\) & \(1\) & \(1-p_{H}(5+r)\) & \(0\) & \(p_{H}(5+r)\) \\  \([a_{I}a_{H}a_{T}]\) & \(pG(SIW_{H}T)\\ +(1-p)G(SIL_{H}T)\\ =11-(1-p)(15+r)\\ \) & \(pG_{}(SIW_{H}T)\\ +(1-p)G_{}(SIL_{H}T)\\ =11-(1-p)[10+p_{H}(5+r)]\) & \((1-p)(1-p_{H})(5+r)\) & \(0\) \\  \([a_{I}a_{C}a_{T}]\) & \(pG(SIWT)\\ +(1-p)G(SILT)\\ =11-(1-p) 15\) & \(pG_{}(SIWT)\\ +(1-p)G_{}(SILT)\\ =11-(1-p) 15\) & \(0\) & \(0\) \\  

Table 12: Measures of interest for each policy for the example in Appendix C.1 and Appendix C.3. Each of the columns here is the on-policy average of the corresponding column in Table 11. Policies are written as sequences of actions, omitting trailing repeated \(a_{T}\) actions. This is nonstandard notation in an MDP with stochastic transitions, but is unambiguous in this example since all decisions are made before any stochasticity occurs.

inflation is at play whenever the agent hides information from the human. Indeed, referencing Table 12, we have \(^{+}([a_{I}a_{H}a_{T}])=(1-p)(1-p_{H})(5+r)>0=^{+}([a_{I} a_{C}a_{T}])\). Together with \(J_{}([a_{I}a_{H}a_{T}])>J_{}([a_{I}a_{C}a_{T}])\), this satisfies the conditions of Definition 4.3, and thus this is an instance of deceptive inflation.

If \(p<1\), \(r>0\), and \(p_{H}>\), then \([a_{I}a_{C}a_{T}]\) is optimal under both \(J\) and \(J_{}\), and in this case, RLHF selects the true optimal policy.

**Case 2:**\(p<\). In this case, the CUDA install is not likely enough to succeed to be worth attempting (under the true reward function). Mathematically, \(J([a_{I}a_{H}a_{T}]) J([a_{I}a_{C}a_{T}])<1=J([a_{I}a_{T}])\). The other three policies are always worse under \(J\) than \([a_{I}a_{T}]\), so we have our optimal policy \(^{*}=[a_{I}a_{T}]\). However,

\[J_{}([a_{I}a_{H}a_{T}])-J_{}([a_{I}a_{T}])=p(10+p_{H}( 5+r)),\]

which is always positive unless \(p=0\). As long as \(p>0\), i.e. installing CUDA is actually possible at all, then regardless of everything else, \(J_{}([a_{I}a_{H}a_{T}])>J_{}([a_{I}a_{T}])\). We can interpret this reasonably: however good or bad it might look to terminate after installing drivers (\(SITT\)), using the 2>/dev/null trick will either produce the same observation as \(SITT\) (and thus the same feedback distribution) or will actually visibly install CUDA successfully--the most lucrative possible source of positive feedback. Even if there's only a sliver of a chance of success and it's a catastrophe to have a silent failure (i.e. \(r 0\)), however likely you are to infer that the lack of a log message about CUDA means the agent hid a failure, it will penalize \([a_{I}a_{T}]\)_more_ than it will penalize \([a_{I}a_{H}a_{T}]\): where \([a_{I}a_{H}a_{T}]\) produces this ambiguous observation with probability \(1-p\) (when it fails), \([a_{I}a_{T}]\) will _always_ produce the ambiguous observation.

This means that when \(0<p<\), it is impossible to recover the true optimal policy with naive RLHF. Which policies can possibly be \(J_{}\)-optimal for some setting of the parameters? We can similarly rule out \([a_{T}]\) and \([a_{H}a_{T}]\) for \(0<p<\):

\[J_{}([a_{I}a_{H}a_{T}])-J_{}([a_{I}a_{T}])=p(10+p_{H}( 5+r))>0.\]

We can rule out \([a_{C}a_{T}]\) by comparison to \([a_{I}a_{C}a_{T}]\): \(J_{}([a_{I}a_{C}a_{T}])-J_{}([a_{C}a_{T}])=16-(1-p)15>0\). So we are left with only \([a_{I}a_{H}a_{T}]\) and \([a_{I}a_{C}a_{T}]\) as candidate \(J_{}\)-optimal policies.

As in Case 1, we find that \(J_{}([a_{I}a_{H}a_{T}])>J_{}([a_{I}a_{T}])\) if and only if \(p=1\) or \(p_{H}<\). In case 2 we have assumed \(p<\), leaving only the \(p_{H}\) condition.

If \(p_{H}<\), then RLHF selects \([a_{I}a_{H}a_{T}]\). As in Case 1, this is deceptive inflationrelative to \(^{*}=[a_{I}a_{T}]\), because

\[^{+}([a_{I}a_{H}a_{T}])=(1-p)(1-p_{H})(5+r)>0=^{+}( ^{*}).\]

If \(p_{H}>\), then RLHF selects \([a_{I}a_{C}a_{T}]\). Because this policy is not \(J\)-optimal, by Theorem 4.5, we must have deceptive inflation, overjustification, or both. Which is it? Here the optimal policy is to terminate after installing drivers, \([a_{I}a_{T}]\). However, \(p_{H}>\). This can be rewritten as \(p_{H}(5+r)>5\). We have seen this expression \(p_{H}(5+r)\) before; it is the underestimation error incurred on \(=SITT\) and therefore also the average underestimation error of policy \([a_{I}a_{T}]\). So here the underestimation error on the optimal policy--that is, the risk that the human misunderstands optimal behavior (terminating after installing driver) as undesired behavior (attempting a CUDA install that was unlikely to work and hiding the mistake)--is severe enough that the agent opts instead for \([a_{I}a_{C}a_{T}]\), a worse policy that attempts the ill-fated CUDA installation only to prove that it wasn't doing so secretly. In qualitative terms, this is quintessential overjustification behavior. Indeed, relative to reference policy \(^{*}=[a_{I}a_{T}]\), we have

\[^{-}([a_{I}a_{C}a_{T}])=0<p_{H}(5+r)=^{-}(^{*})\]

and thus by Definition 4.4, this is overjustification.

### Ambiguity in Section 4.4 examples when modeling partial observability

Consider the example in Fig. 4A when modeling partial observability as in Section 5. By Theorem 5.2, the ambiguity in the return function leaving the choice probabilities invariant is given by \(\ \).

Let \(R^{}=(0,0,R^{}(W),0,R^{}(W_{H}),0,0)^{\{S,I,W,L,W_{ H},L_{H},T\}}\) be a reward function that we want to parameterize such that \(G^{} R^{}\) ends up in the ambiguity; here, \(R^{}\) is interpreted as a column vector.

We want \( G^{}=0\). Since the observation sequences \(=o_{}o_{}o_{}o_{}\), \(=o_{}o_{L}o_{}o_{}\), \(=o_{}o_{I}o_{}o_{}\), or \(=o_{}o_{I}o_{L}o_{}\) call cannot involve the states \(W\) or \(W_{H}\), it is clear that they have zero expected return \(( G^{})()\). Set \(p^{}_{H} BSIW_{H}T o_{}o_{I}o_{W}o_{ }\). Then the condition that \( G^{}=0\) is equivalent to:

\[0 =\, G^{}(o_{}o_{I}o_{ W}o_{})=}_{ B(|o_{}o_{I}o_{ W}o_{})}G^{}()\] \[=p^{}_{H} G^{}(SIW_{H}T)+(1-p^{}_{H})  G^{}(SIWT)=p^{}_{H} R^{}(W_{H})+(1-p^{}_{H} ) R^{}(W).\]

Thus, if \(R^{}(W)=_{H}}{p^{}_{H}-1}R^{}(W_{H})\), then \(G^{}\,\), meaning that \(R+R^{}\) has the same choice probabilities as \(R\) and is thus fully feedback-compatible. In particular, if \(R^{}(W_{H}) 0\) is sufficiently large, then in subsequent policy optimization, there is an incentive to hide the mistakes and \(_{H}\) will be selected, which is suboptimal with respect to the true reward function \(R\).

Thus Fig. 3(a)_still retains dangerous ambiguity when modeling partial observability_.

However, the example in Fig. 3(b) leads to no ambiguity when partial observability is correctly modeled.

To show this in detail, let \(G^{}=(R^{})\,\). We need to show \(G^{}=0\). Since the human is only uncertain about the state sequences corresponding to the observation sequence \(o_{}o_{I}o_{}o_{}\), the condition \( G^{}=0\) already implies \(G^{}()=0\) for all state sequences except \(SIWT\) and \(SITT\). From \(( G^{})(o_{}o_{I}o_{}o_{})=0\), one then obtains the equation

\[(1-p_{D})R^{}(S)+R^{}(I)+2R^{}(T)+p_{D} R^{}(S)+R^{}(I)+R^{}(W)+R^{}(T)=0.\] (5)

Thus, if one of the two state sequences involved has zero return, then the other has as well, assuming that \(0 p_{D} 1\), and we are done.

To show this, we use that all other state sequences have zero return: \(R^{}(S)+3R^{}(T)=0=R^{}(S)+R^{}(L)+2R^{}(T)\), from which \(R^{}(L)=R^{}(T)\) follows. Then, from \(R^{}(S)+R^{}(I)+R^{}(L)+R^{}(T)=0\), substituting the previous result gives \(R^{}(S)+R^{}(I)+2R^{}(T)=0\), and so Equation (5) results in \(R^{}(S)+R^{}(I)+R^{}(W)+R^{}(T)=0\). Overall, this shows \(G^{}=(R^{})=0\), and so \(\,=\{0\}\).

### Experimental details

Here, we explain more experimental details for the results in Table 1, reproduced here as Table 13.

The leftmost column ("Ex." for "example") corresponds to Examples A and B in Figure 4. \(p\) is the success probability upon attempting to install Cuda or NumPy in state \(I\), see Figure 6. \(p_{}\) in Example A is the human's belief probability that the agent had the error message if there is no output after nvidia-driver installation. Similarly, \(p_{}\) in Example B is the human's belief probability that installation was done with default settings if there is no further output after Python

  
**Ex.** & \(p\) & \(p_{}\) & \(p_{}\) & **model** & **action** & \(^{+}\) & **dec. infl.** & \(^{-}\) & **overj.** & **optimal** \\  A & 0.5 & 0.5 & N/A & naive & \(a_{H}\) & 1.5 & ✓ & 0 & ✗ & ✗ \\ A & 0.5 & 0.5 & N/A & po-aware & \(a_{H}\) & 1.5 & ✓ & 0 & ✗ & ✗ \\  A & 0.1 & 0.9 & N/A & naive & \(a_{C}\) & 0 & ✗ & 0 & ✓ & ✗ \\ A & 0.1 & 0.9 & N/A & po-aware & \(a_{T}\) & 0 & ✗ & 5.4 & ✗ & ✓ \\  B & 0.5 & N/A & 0.9 & naive & \(a_{T}\) & 4.5 & ✓ & 0 & ✓ & ✗ \\ B & 0.5 & N/A & 0.9 & po-aware & \(a_{D}\) & 0 & ✗ & 0.25 & ✗ & ✓ \\  B & 0.5 & N/A & 0.1 & naive & \(a_{V}\) & 0 & ✗ & 0 & ✓ & ✗ \\ B & 0.5 & N/A & 0.1 & po-aware & \(a_{D}\) & 0 & ✗ & 2.25 & ✗ & ✓ \\   

Table 13: Experiments showing improved performance of po-aware RLHFinstallation. Note that lines one and two in the table also correspond to Example C.1, lines three and four to Example C.2, and lines five and six to the first half and seven and eight to the second half of Example C.3, respectively. In all our experiments, we set the penalty to \(r=1\).

The "model" column has value "naive" if the reward learning algorithm is classical RLHF (erroneously assuming full observability) as in Christiano et al. (2017), and "po-aware" if the human's partial observability is correctly modeled as in Appendix D.3. We initialize the reward function as a list of rewards of states and train it by logistic regression using a dataset that consists of all pairs of state sequences together with the human's choice probabilities under partial observations. This leads to 28 pairs of distinct trajectories together with choice probabilities. We train the reward model for 300 epochs over a shuffled dataset of 13.5 copies of the 28 pairs with the Adam optimizer, for a total of 113400 training updates.

Once we have the resulting reward model, we use value iteration to find its deterministic optimal policy. All policies choose to install the nvidia-driver (in Example A) and Python (in Example B), and differ in their action in state \(I\), which is given in the column "action". We compute the overestimation error and underestimation error of the resulting policies analytically using the hardcoded environment dynamics, true reward function, observation function, and human belief matrix \(\). This is given in columns \(^{+}\) and \(^{-}\). Note that these are averages over 10 entire training runs, though since they always result in the same learned policy, there is no variation and we do not state any uncertainty.

The columns "dec. infl.", "overj.", and "optimal" state whether deceptive inflation or overjustification occurs with the learned policy, and whether it is optimal according to the true human's reward function.

## Appendix D Modeling the Human in Partially Observable RLHF

In this appendix, we develop the theory of RLHF with appropriately modeled partial observability, including full proofs of all theorems.

In Section D.1, we explain how the human can arrive at the belief \(B()\) via Bayesian updates. The main theory and the main paper in general do not depend on this specific form of the human's belief, but some examples in the appendix do.

In Section D.2 we then explain our main result: the ambiguity and identifiability of both reward and return functions under observed sequence comparisons. In Section D.3, we then explain that this theorem means that one could _in principle_ design a practical reward learning algorithm that converges on the correct reward function up to the ambiguity characterized in the section before, _if_ the human's belief kernel \(B()\) is fully known.

In Section D.4, we generalize the theory to the case that the human's observations are not necessarily known to the learning system and again characterize precisely when the return function is identifiable from sequence comparisons. We then consider special cases in Section D.5, where we show that the fully observable case is covered by our theory, that a deterministic observation kernel \(P_{}\) usually leads to non-injective belief matrix \(\), and that "noise" in the observation kernel \(P_{}\) leads, under appropriate assumptions, to the identifiability of the return function.

Our identifiability results require that the learning system knows the human's belief kernel \(B()\). In Section D.6, we then show that these results are robust to slight misspecifications: a bound in the error in the specified belief leads to a corresponding bound in the error of the policy evaluation function used for subsequent reinforcement learning.

In Section D.7, we then provide a very preliminary characterization of the ambiguity in the return function under special cases.

Finally, in Section D.8, we study examples of identifiability and non-identifiability of the return function for the case that we _do_ model the human's partial observability correctly. This reveals qualitatively interesting cases of identifiability, even when \(\) is not injective, and catastrophic cases of non-identifiability.

### The Belief over the State Sequence for Rational Humans

Before we dive into the main theory, we want to explain how the human can iteratively compute the posterior of the state sequence given an observation sequence with successively new observations. This is done by defining a Bayesian network for the joint probability of policy, states, actions, and observations, and doing Bayesian inference over this Bayesian network.

The details of this subsection are only relevant for a few sections in the appendix since it is usually enough to assume that the posterior belief _exists_. Additionally, in the core theory, we do not even assume that \(B()\) is a posterior: it is simply any probability distribution. The reason why it can still be interesting to analyze the case when the human is a rational Bayesian reasoner is that one can then analyze RLHF under _generous_ assumptions to the human.

We model the human to have a joint distribution \(B(,,,)\) over the policy \(\), state sequence \(=s_{0},,s_{T}\), action sequence \(=a_{0},,a_{T-1}\), and observation sequence \(=o_{0},,o_{T}\). This is given by a Bayesian network with the following components:

* a policy prior \(B(^{})\);
* the probability of the initial state \(B(s_{0}) P_{0}(s_{0})\);
* action probabilities \(B(a s,)(a s)\);
* transition probabilities \(B(s_{t+1} s_{t},a_{t})(s_{t+1} s_{t},a_{t})\);
* and observation probabilities \(B(o_{t} s_{t}) P_{O}(o_{t} s_{t})\).

Together, this defines the joint distribution \(B(,,,)\) over the policy, states, actions, and observations that factorizes according to the following directed acyclic graph:

(6)

The following proposition clarifies the iterative Bayesian update of the human's posterior over state sequences, given observation sequences:

**Proposition D.1**.: _Let \(t T-1\) and denote by \(=s_{0},,s_{t}\) a state sequence segment of length \(t 0\). Similarly, \(=o_{0},,o_{t}\) denotes an observation sequence segment. We have_

\[B(,s_{t+1},,o_{t+1}) P_{O}(o_{t+1} s_{t+1}) _{a_{t}}(s_{t+1}_{t},a_{t} )(a_{t} s_{t}) B(,).\]

_Thus, the human can iteratively compute \(B(,)\) from the prior \(B(s_{0},)=P_{0}(s_{0}) B(^{})\) using the above Bayesian update._

_The posterior over the state sequence can subsequently be computed by_

\[B()=_{}B(,).\]

Proof.: The proof is essentially just Bayes rule applied to the Bayesian network in Equation (6). We repeatedly make use of conditional independences that follow from d-separations in the graph (Geiger et al., 1990). More concretely, we have

\[B,s_{t+1},,o_{t+1} Bo_{t+1} ,s_{t+1},, B,s_{t+1}, \]\[=P_{O}o_{t+1} s_{t+1} Bs_{t+1} s_{t}, , B(,\] \[=P_{O}o_{t+1} s_{t+1}_{a_{t} }s_{t+1} s_{t},a_{t}a_{ t} s_{t} B,.\]

In step 1, we used Bayes rule. In step 2, we made use of the independence \(o_{t+1}\!\!\!(,,) s_{t+1}\), plugged in the observation kernel, and used the chain rule of probability to compose the second term into a product. In step 3, we marginalized and used, once again, the chain rule of probability. In step 4, we used the independences \(s_{t+1}\!\!\!(s_{0},,s_{t-1},,)(s_{t},a)\) and \(a_{t}\!\!\!(s_{0},,s_{t-1},)(,s_{t})\) and plugged in the transition kernel and the policy.

The last formula is just a marginalization over the policy. 

### Ambiguity and Identifiability of Reward and Return Functions under Observation Sequence Comparisons

In this section, we prove the main theorem of this paper: a characterization of the ambiguity that is left in the reward and return function once the human's Boltzmann-rational choice probabilities are known. We change the formulation slightly by formulating the linear operators "intrinsically" in the spaces they are defined in, instead of using matrix versions. This does not change the general picture, but is a more natural setting when thinking, e.g., about generalizing the results to infinite state sequences. Thus, we define \(:^{}^{}\) as the linear operator given by

\[\,(G)() B(|)}{}G().\]

Here, \(\) is the human's belief, which can either be computed as in the previous subsection or simply be any conditional probability distribution. Similarly, we define \(:^{}^{}\) as the linear operator given by

\[\,(R)()_{t=0}^{T}^{t}R (s_{t}).\]

Figure 8: The linear geometry of ambiguity for a hypothetical example with three state sequences and two observation sequences. \(G^{*}\) is the true return function, and “\(G\)” is used in labeling the axes to refer to some arbitrary return function. This is a more accurate geometric depiction of the middle and right spaces in Figure 5. The subspace \(\,\) (purple) is the ambiguity in return functions, meaning that adding an element would not change the human’s expected return function on observations. Thus the set of return functions that the reward learning system can infer is the affine set \(G+(\,)\) (yellow). Note that the planes on the left are drawn to be axis-aligned for ease of visualization; this will not be the case for real MDPs.

The matrix product \(\) then becomes the composition \(:^{S}^{}\). Finally, recall that the kernel \(\) of a linear operator \(\) is defined as its nullspace, and the image \(\,\) as the set of elements hit by \(\). We obtain the following theorem:

**Theorem D.2**.: _Let \(R\) be the true reward function and \(\) another reward function. Let \(=()\) and \(G=(R)\) be the corresponding return functions. The following three statements are equivalent:_

1. _The reward function_ \(\) _gives rise to the same vector of choice probabilities as_ \(R\)_, i.e_ \[P^{}^{} _{,^{}}=P^{R} ^{}_{, ^{}}.\]
2. _There is a reward function_ \(R^{}()\) _and a constant_ \(c\) _such that_ \[=R+R^{}+c.\]
3. _There is a return function_ \(G^{}\,\) _and a constant_ \(c^{}\) _such that_ \[=G+G^{}+c^{}.\]

_In other words, the ambiguity that is left in the reward function when its observation-based choice probabilities are known is, up to an additive constant, given by \(()\); the ambiguity left in the return function is given by \(\,\)._

Proof.: Assume (i). To prove (ii), let \(\) by the sigmoid function given by \((x)=\). Then by Equation (2), the equality of choice probabilities means the following for all \(,^{}\):

\[\,()()-\,()(^{}) =\,(G)()-\,(G)(^{}).\]

Since the sigmoid function is injective, this implies

\[\,()()-\,( )(^{})=\,(G)()-\,(G)(^{}).\]

Fixing an arbitrary \(^{}\), this implies that there exists a constant \(c^{}\) such that for all \(\), the following holds:

\[\,()()-\,(G) (^{})-c^{}=0.\]

Noting that \((c^{})=c^{}\), this implies \(-G-c^{}()\). Now, define the constant reward function

\[c c^{}}.\]

We obtain

\[\,(c)() =_{t=0}^{T}^{t} c\] \[=c^{}}_{t=0}^{T }^{t}\] \[=c^{}.\]

Thus, we have

\[(-R-c)=-G-c^{}(),\]

implying \(R^{}-R-c()\). This shows (ii).

That (ii) implies (iii) follows by applying \(\) to both sides of the equation.

Now assume (iii), i.e. \(=G+G^{}+c^{}\) for a constant \(c^{}\) and a return function \(G^{}()\,\). This implies \(()=(G)+c^{}\). Thus, for all \(,^{}\), we have

\[\,()()-\,( )(^{})=\,(G)()-\,(G)(^{}),\]

which implies the equal choice probabilities after multiplying with \(\) and applying the sigmoid function \(\) on both sides. Thus, (iii) implies (i).

**Corollary D.3**.: _The following two statements are equivalent:_

1. \(()=0\)_._
2. _The data_ \((P^{R}^{})_{,^{ }}\) _determine the reward function_ \(R\) _up to an additive constant._

Proof.: That (i) implies (ii) follows immediately from the implication from (i) to (ii) within the preceding theorem.

Now assume (ii). Let \(R^{}()\). Define \( R+R^{}\). Then the implication from (ii) to (i) within the preceding theorem implies that \(\) and \(R\) have the same choice probabilities. Thus, the assumption (ii) in this corollary implies that \(R^{}\) is a constant. Since \(\) and \(\) map nonzero constants to nonzero constants, the fact that \(R^{}()\) implies that \(R^{}=0\), showing that \(()=\{0\}\). 

As mentioned in the main paper, the previous result already leads to the non-identifiability of \(R\) whenever \(\) is not injective, corresponding to the presence of zero-initial potential shaping (Skalse et al. (2023), Lemma B.3). Thus, we now strengthen the previous result so that it deals with the identifiability of the _return_ function, which is sufficient for the purpose of policy optimization:

**Corollary D.4**.: _Consider the following four statements (which can each be true or false):_

1. \(=\{0\}\)_._
2. \(\,=\{0\}\)_._
3. \(=\{0\}\)_._
4. _The data_ \((P^{R}^{})_{,^ {}}\) _determine the return function_ \(G=(R)\) _on sequences_ \(}\) _up to a constant independent of_ \(\)_._

_Then the following implications, and no other implications, are true:_

_In particular, all of (i), (ii), and (iii) are sufficient conditions for identifying the return function from the choice probabilities._

Proof.: That (i) implies (iii) is trivial. That (ii) implies (iii) is a simple linear algebra fact: Assume (ii) and that \(G^{}\). Then \(G^{}=(R^{})\) for some \(R^{}^{}\) and

\[0=(G^{})=(\,(R^{})) =()(R^{}).\]

By (ii), this implies \(R^{}=0\) and therefore \(G^{}=(R^{})=0\), showing (iii).

That (iii) implies (iv) immediately follows from the implication from (i) to (iii) in Theorem D.2.

Now, assume (iv). To prove (iii), assume \(G^{}\). Then the implication from (iii) to (i) in Theorem D.2 implies that \(G+G^{}\) induces the same observation-based choice probabilities as \(G\). Thus, (iv) implies \(G+G^{}=G+c^{}\) for some constant \(c^{}\), which implies \(G^{}=c^{}\). Since \(G^{}\), this implies \(0=(G^{})=(c^{})=c^{}\) and thus \(G^{}=0\). Thus, we showed \(=\{0\}\).

We now show that no other implication holds in general. Example D.32 will show that (ii) does not imply (i). We now show that (i) does also not imply (ii), from which it will logically follow that (iii) does neither imply (i) nor (ii). Namely, consider the following simple MDP with time horizon \(T=1\):

\[a\;}}}}\;b\] (7)

In this MDP, every state sequence starts in \(a\), deterministically transitions to \(b\), and then ends. This means that \(=ab\) is the only sequence. Now, let \(R^{}^{\{a,b\}}\) be the reward function given by

\[R^{}(a)=1, R^{}(b)=.\]

We obtain

\[\,(R^{})()=R^{}(a)+ R^{ }(b)=1+=0.\]

Thus, \((R^{})=0\), \(()(R^{})=0\), and, therefore, \(\,\,\{0\}\). Thus, (ii) does not hold. However, it is possible to choose \(B()\) such that (i) holds: e.g., if \(=\) and \(B()_{}()\), then \(=\{0\}\) since this operator is the identity. 

### The Ambiguity in Reward Learning in Practice

In this section, we point out that Theorem D.2 is not just a theoretical discussion: When \(\) and the inverse temperature parameter \(\) are known, then it is possible to design a reward learning algorithm that learns the true reward function up to the ambiguity \(()\) in the infinite data limit. In doing so, we essentially use the loss function proposed in Christiano et al. (2017).

Namely, assume \(\) is a data distribution of observation sequences \(\) such that all sequences in \(\) have a strictly positive probability of being sampled; for example, \(\) could use an exploration policy and the observation sequence kernel \(P_{}\). For each pair of observation sequences \((,^{})\), we then get a conditional distribution \(P(,^{})\) over a one-hot encoded human choice \(\{(1,0),(0,1)\}\), with probability

\[P=(1,0),^{}=P^{R} ^{}.\]

Together, this gives rise to a dataset \((_{1},_{1}^{},_{1}),,(_{N},_{N}^{ },_{N})\) of observation sequences plus a human choice.

Now assume we learn a reward function \(R_{}:\) that is differentiable in the parameter \(\) and that can represent all possible reward functions \(R^{}\). Let \(G_{}(R_{})\) be the corresponding return function. Write \(_{k}=(_{k}^{(1)},_{k}^{(2)})\). As in Christiano et al. (2017), we define its loss over the dataset above by

\[}()=-_{k=1}^{N}_{k}^{(1)}  P^{R_{}}_{k}_{k}^{}+_{k} ^{(2)} P^{R_{}}_{k}^{}_{k} .\]

Note that by Equation (2), this loss function essentially uses \(\) and also the inverse temperature parameter \(\) in its definition. This means that these need to be explicitly represented to be able to use the loss function in practice.

**Proposition D.5**.: _The loss function \(}\) is differentiable. Furthermore, in the infinite datalimit its minima are precisely given by parameters \(\) such that \(R_{}=R+R^{}+c\) for \(R^{}\,\,\) and \(c\), or equivalently \(G_{}=G+G^{}+c^{}\) for \(G^{}\,\) and \(c^{}\)._

Proof.: The differentiability of the loss function follows from the differentiability of multiplication with the matrix \(\), see Equation (2), and of the reward function \(R_{}\) in its parameter \(\) that we assumed.

For the second statement, let \(N(,^{})\) be the number of times that the pair \((,^{})\) appears in the dataset, and let \(N(,^{},1)\) be the number of times that the human choice is \(=(1,0)\) and the sampled pair is \((,^{})\), and similar for \(2\) instead of \(1\). We obtain

\[}()=-_{,^{} },^{})}{N},^{ },1)}{N(,^{})} P^{R_{}} ^{}\]\[+,^{},2)}{N(, ^{})} P^{R_{}}^{} {}\] \[ ,^{}}{ }[\,(P^{R} ^{}\,\,P^{R_{}}^{})]\] \[=: ().\]

Here, \(\) is the crossentropy between the two binary distributions. Since we assumed that \(\) gives a positive probability to all observation sequences in \(\), and since the cross entropy is generally minimized exactly when the second distribution equals the first, the loss function \(()\) is minimized if and only if \(R_{}\) gives rise to the same choice probabilities as \(R\) for all pairs of observation sequences. Theorem D.2 then gives the result. 

### Identifiability of Return Functions When Human Observations Are Not Known

Corollary D.4 assumes that the choice probabilities of each observation sequence pair are known to the reward learning algorithm. However, this requires the algorithm to know what the human observed. In some applications, this is a reasonable assumption, e.g. if the human's observations are themselves produced by an algorithm that can feed the observations also back to the learning algorithm. In general, however, the observations happen in the physical world, and are only known probabilistically via the observation kernel \(P_{O}\). The learning system _does_ however have access to the full state sequences that generate the observation sequences. This leads to knowledge of the following choice probabilities for \(,^{}}\):

\[P^{R}^{},^{} P_{}(|,^{})}{ }[P^{R}^{} ],@note{footnote}{We excuse the following abuse of notation: these choice probabilities _run through_ the observations of the human and are not the same as the choice probabilities from Equation (1).}\] (8)

where the observation-based choice probabilities are given as in Equation (2). In other words, the learning algorithm can only infer an aggregate of the observation-based choice probabilities. Again, we can ask a question similar to the ones before, extending the investigations in the previous section:

**Question D.6**.: _Assume the vector of choice probabilities \((P^{R}(^{}))_{,^{} }}\) is known. Additionally, assume that it is known that the human's observations are governed by \(P_{O}\), and that the human is Boltzmann rational with inverse temperature parameter \(\) and beliefs \(B()\), see Equation (8). Does this data identify the return function \(G:}\)?_

If the observation-based choice probabilities from Equation (2) would be known, then Corollary D.4 would provide the answer to this question. Thus, similar to how we previously inverted the belief operator \(\), we are now simply tasked with inverting the expectation over observation sequences. This leads us to the following definition:

**Definition D.7** (Ungrounding Operator).: _The ungrounding operators \(:^{}^{}}\) and \(:^{} ^{}}}\) are defined by_

\[\,(v)() P_ {}(|)}{}v(),()(C)(,^{} ),^{} P_{}(| ,^{})}{}C(,^{ }).\]

_Here, \(v^{}\) is an arbitrary vector, and \(C^{}\) is also an arbitrary vector, where the notation can remind of "Choice" since the inputs to \(\) are, in practice, vectors of observation-based Boltzmann-rational choice probabilities._

Formally, \(\) is the Kronecker product of \(\) with itself, but it is not necessary to understand this fact to follow the discussion. Ultimately, to be able to recover the observation-based choice probabilities, what matters is that \(\) is injective on whole vectors of these choice probabilities. The injectivity of \(\) is a _sufficient condition_ for this, which explains its usefulness. We show this in the following lemma:

**Lemma D.8**.: \(:^{}^{}\) _is injective if and only if \(:^{} ^{}}\) is injective._Proof.: This is a general property of the Kronecker product of a linear operator with itself. For completeness, we demonstrate the calculation in our special case. First, assume that \(\) is injective. Assume that \(()(C)=0\) for some \(C^{_{}}\). We need to show \(C=0\).

For all pairs of state sequences \((,^{\,})\), we have

\[0=()(C)(,^{\,}) =}_{,^{\,} P_{}( |,^{\,})}C(,^{\,})\] \[=}_{ P_{}(|)} }_{^{\,} P_{}(^{\, }|^{\,})}C(,^{\,})\] \[=}_{ P_{}(|)} C^{}_{^{\,}}()\] \[=\,\,C^{}_{^{\,}} (),\]

where \(C^{}_{^{\,}}()}_{ ^{\,} P_{}(^{\,}|^{\,})}C( ,^{\,})\). By the injectivity of \(\), we obtain \(C^{}_{^{\,}}=0\) for all \(^{\,}\). This means that for all \(^{\,}\) and \(\), we have

\[0=C^{}_{^{\,}}()=}_{^{\, } P_{}(^{\,}|^{\,})}C(,^{\,})=\,\,C^{}_{ }(^{\,}),\]

where \(C^{}_{}(^{\,}) C(,^{ \,})\). Again, by the injectivity of \(\), we obtain \(C^{}_{}=0\) for all \(\), leading to \(C=0\). That proves the direction from left to right.

To prove the other direction, assume that \(\) is _not_ injective. This means there exists \(0 C^{}\) such that \((C)=0\). Define \(C C^{}\) by

\[(C C)(,^{\,}) C()C(^{\, }).\]

Then clearly, \(C C 0\). We are done if we can show that \(()(C C)=0\) since that establishes that \(\) is also not injective. For any \(\), \(^{\,}}\), we have

\[()(C C)(, ^{\,}) =}_{,^{\,} P_{}( |,^{\,})}(C C)(,^{\, })\] \[=}_{,^{\,} P_{}( |,^{\,})}C() C(^{\,}) \] \[=}_{ P_{}(|)} C()}_{^{\,} P_{ }(^{\,}|^{\,})}C(^{\,}) \] \[=\,(C)()\,(C) (^{\,})\] \[=0 0\] \[=0.\]

This finishes the proof. 

We now state and prove the following extension of Corollary D.4:

**Theorem D.9**.: _Consider the following statements (which can each be true or false):_

1. \(:^{}^{}\) _is an injective linear operator:_ \(=\{0\}\)_._
2. \(:^{}^{}\) _is an injective linear operator:_ \(=\{0\}\)_._
3. \(\) _is injective on vectors of observation-based choice probabilities_ \(P^{R}^{\,}_{,^{\,}}\) _over the set of return functions_ \(G^{}}\)_._
4. _The data of state-based choice probabilities_ \(P^{R}^{\,}_{,^{\,}}\) _from Equation (_8_) determine the data of observation-based choice probabilities_ \(P^{R}^{\,}_{,^{\,}}\) _from Equation (_2_)._

[MISSING_PAGE_EMPTY:34]

Proof.: To show that \(\) is injective, assume \(v^{}\) is such that \((v)=0\). Then for all \(}\), we get

\[0=\,(v)()= P_{}( |)}{}\,v()=v {O}().\]

Since \(:}\) is by definition surjective, we obtain \(v=0\).

\(:}\) is by definition surjective, and here assumed to be non-injective, which implies that \(}\) has a higher cardinality than \(\). Thus, \(:^{}^{}\) cannot be injective. 

In the following, we analyze a simple case that guarantees identifiability. It requires that the observation kernel is "well-behaved" of a form where the observations are simply "noisy states", and that the human is a Bayesian reasoner with any prior \(B()\) that supports every state sequence \(}\).

**Definition D.12** (Noise in the Observation Kernel).: _Then we say that there is noise in the observation kernel \(P_{O}:}()\) if \(}=\) and if \(\) is an injective linear operator._

**Proposition D.13**.: _Assume that \(}=\). Furthermore, assume that \(B()\) is given by the posterior with likelihood \(P_{}()\) and any prior \(B()\) with \(B()>0\) for all \(}\). Then there is noise in the observation kernel if and only if \(\) is injective._

Proof.: Assume \(\) is injective. To show that \(\) is injective, assume there is \(G^{}^{}}\) with \((G^{})=0\). Then for all \(\), we have

\[0 =\,(G^{})()= { B(|)}{}\,G^{}() =_{}B()G^{}() _{}P_{}()B() G ^{}()\] \[=\,^{T}(B G^{})().\]

Here, \(^{T}\) is the transpose of \(\) and \(B G^{}\) is the componentwise product of the prior \(B\) with the return function \(G^{}\). Since \(\) is injective and thus invertible, \(^{T}\) is as well. Thus, \(B G^{}=0\), which implies \(G^{}=0\) since the prior gives positive probability to all state sequences. Thus, \(\) is injective.

For the other direction, assume \(\) is injective. To show that \(\) is injective, let \(v^{}\) be any vector with \((v)=0\). We do a similar computation as above: for all \(^{}}\), we have

\[0 =\,(v)()= P _{}(|)}{}\,v()= _{}P_{}(\,|\,\,)v() _{}B()P_{}( ) v()\] \[=\,^{T}\,P_{} v ().\]

Here, \(^{T}\) is the transpose of \(\), \(P_{}()\) is the denominator in Bayes rule, and \(P_{} v\) is the vector with components \(P_{}() v()\). From the injectivity and thus invertibility of \(\), it follows that \(^{T}\) is invertible as well, and so \(P_{} v=0\), which implies \(v=0\). Thus, \(\) is injective. 

**Corollary D.14**.: _When there is noise in the observation kernel and the human is a Bayesian reasoner with some prior \(B\) such that \(B()>0\) for all \(}\), then the return function is identifiable from choice probabilities of state sequences even if the learning system does not know the human's observations._

Proof.: This follows from the injectivity of \(\), the injectivity of \(\) that we proved in Proposition D.13, and Theorem D.9. 

**Remark D.15**.: _We mention the following caveat: intuitively, one could think that \(\) (and thus \(\), by Proposition D.13) will be injective if every \(\) is identifiable from infinitely many i.i.d. samples from \(P_{}()\). A counterexample is the following:_

\[=1/2&1/4&1/4\\ 1/4&1/2&1/4\\ 3/8&3/8&1/4.\]_In this case, the rows are linearly dependent with coefficients \(1/2,1/2\) and \(-1\). Consequently, \(\) and \(\) are not injective, and so if this observation kernel comes from a multi-armed bandit with three states, then Corollary D.4 shows that the return function is not identifiable._

_Nevertheless, the distributions \(P_{}()\) (given by the rows) all differ from each other, and so infinitely many i.i.d. samples identify the state sequence \(\)._

### Robustness of Return Function Identifiability under Belief Misspecification

We now again look at the case where the observations that the human observes are known to the reward learning system, as in Section D.2. Furthermore, we assume that \(:^{}^{}\) is such that \(=\{0\}\). In this case, we can apply Corollary D.4 and identify the true return function \(G\) from \((G)\), which, in turn, can be identified up to an additive constant from the observation-based choice probabilities with the argument as for Proposition 3.1.

In this section, we investigate what happens when the human belief model is slightly misspecified. In other words: the learning system uses a perturbed matrix \(_{}+\) with some small perturbation \(\). How much will the inferred return function deviate from the truth? To answer this, we first need to outline some norm theory of linear operators.

#### d.6.1 Some Norm Theory for Linear Operators

In this section, let \(V,W\) be two finite-dimensional inner product-spaces. In other words, \(V\) and \(W\) each have inner products \(,\) and there are linear isomorphisms \(V^{k}\), \(W^{m}\) such that the inner products in \(V\) and \(W\) correspond to the standard scalar products in \(^{k}\) and \(^{m}\). The reason that we don't directly work with \(^{k}\) and \(^{m}\) itself is that we will later apply the analysis to the case that \(V=^{}\). Let in this whole section \(:V W\) be a linear operator and \(:V W\) be a perturbance, so that \(_{}+\) is a perturbed version of \(\).

The inner products give rise to a norm on \(V\) and \(W\) defined by

\[\|v\|=,\|w\|= .\]

As is well known, for each linear operator \(:V W\) there exists a unique, basis-independent _adjoint_ (generalizing the notion of a transpose) \(^{T}:W V\) such that for all \(v V\) and \(w W\), we have

\[\,v,w= v,^{T}\,w .\]

Let us recall the following fact that is often used in linear regression:

**Lemma D.16**.: _Assume \(:V W\) is injective. Then \(^{T}\,:V V\) is invertible and \((^{T}\,)^{-1}\,^{T}\) is a left inverse of \(\)._

Proof.: To show that \(^{T}\,\) is invertible, we only need to show that it is injective. Thus, let \(0 x V\). Then

\[ x,^{T}\,\,x=\,x,\,x=\|\,\,x\|^{2}>0,\]

where the last step followed from the injectivity of \(\). Thus, \(^{T}\,\,x 0\), and so \(^{T}\,\) is injective, and thus invertible. Consequently, \((^{T}\,)^{-1}\,^{T}\) is a well-defined operator. That it is the left inverse of \(\) is clear. 

**Definition D.17** (Operator Norm).: _The norm of an operator \(:V W\) is given by_

\[\|\,\,\|_{x,\;\|x\|=1}\,\|\, \,x\|.\]

_It has the following well-known properties, where \(,\) and \(\) are matrices of compatible sizes:_

\[\|\,+\,\|\|\,\,\|+ \|\,\,\|,\|\,\,\,\| \|\,\,\|\|\,\,\|,\| \,^{T}\,\|=\|\,\,\|.\]

To study how a perturbance in \(\) (and thus \(^{T}\,\)) transfers into a perturbance of \((\,^{T}\,\,)^{-1}\), we will use the following theorem:

**Theorem D.18** (El Ghaoui (2002)).: _Let \(:V V\) be an invertible operator. Let \(<\|\,^{-1}\,\|^{-1}\). Let \(:V V\) be any operator with \(\|\,\,\|\). Then \(+\) is invertible and we have_

\[\|(+)^{-1}-^{-1}\,\|^{-1}\,\|}{\|\,^{-1}\,\|^{-1}-}.\]

Proof.: See El Ghaoui (2002), Section 7 and in particular Equation 7.2. Note that the reference defines \(\|\,\,\|\) to be the largest singular value of \(\); by the well-known min-max theorem, this is equivalent to Definition D.17. 

We will apply this theorem to \(^{T}\,\), which raises the question about the size of the perturbance in \(^{T}\,\) for a given perturbance in \(\). This is clarified in the following lemma. Before stating it, for a given perturbance \(\), define

\[()2\|\,\, \|+,\]

which depends on \(\) and \(\). Also, recall that for a given perturbance \(\), we define \(_{}+\). We obtain:

**Lemma D.19**.: _Assume that \(\|\,\,\|\). Then_

\[\|\,_{}^{T}\,_{}- ^{T}\,\,\|().\]

Proof.: We have

\[\,_{}^{T}\,_{}-^{T}\,\, =(+)^{T}(+)-^{T}\,\,\] \[=\,^{T}\,+^{T}\, +^{T}\,\,\] \[\|\,\,\|\|\,\,\|+\|\,\,\|\|\,\,\|+\|\,\,\|^{2}\] \[2\|\,\,\|+\] \[=().\]

To be able to apply Theorem D.18 to \(^{T}\,\), we need to make sure that \(()\) is bounded above by \((^{T}\,\,^{-1}^{-1}\). The next lemma clarifies what condition \(\) needs to satisfy for \(()\) to obey that bound. For this, define

\[()-\|\,\,\|+\,\|^{2}+ (^{T}\,)^{-1}^{-1}},\] (9)

which only depends on \(\).

**Lemma D.20**.: _Assume \(<()\). Then_

\[()<(^{T}\,)^{-1} ^{-1}.\]

Proof.: Note that \(=()\) is the positive solution to the following quadratic equation in the indeterminate \(\):

\[^{2}+2\|\,\,\|-(^{T}\,)^{-1}^{-1}=()-(^{T}\, )^{-1}^{-1}=0.\]

Since this is a convex parabola, we get the inequality \(()-(^{T}\,)^{-1} ^{-1}<0\) whenever we have \(0<()\), which shows the result. 

Finally, we put it all together to obtain a bound on the perturbance of \(\,^{T}\,\,^{-1}\,^{T}\). For this, set

\[C(,)() \,^{T}\,\,^{-1}}{\, ^{T}\,\,^{-1}^{-1}-( )}\,\,++ \,^{T}\,\,^{-1}.\] (10)

We obtain:

**Proposition D.21**.: _Assume \(\|\,\,\|<()\). Then \(_{}^{T}\,_{}\) is invertible, and we have_

\[\|(\,_{}^{T}\,_{}\,)^ {-1}\,_{}^{T}-(\,^{T}\,\,)^ {-1}\,^{T}\,\| C(,).\]

Proof.: The invertibility of \(_{}^{T}\,_{}\) follows from Theorem D.18, Lemma D.19 and Lemma D.20. We get

\[\|(\,_{}^{T}\,_{}\,)^{-1}\,_{}^{T}-(\,^{T}\, \,)^{-1}\,^{T}\,\|\] \[=\] \[= C(,).\]

In the second-to-last step, we used Theorem D.18. 

The constant \(C(,)\), defined in Equation (10), has a fairly complicated form. In the following proposition, we find an easier-to-study upper bound in a special case:

**Proposition D.22**.: _Assume that \(\|\,\,\|\) and \(-\|\,\,\|+\,\|^{ 2}+1/2\|(^{T}\,)^{-1}\|^{-1}}\).3 Then we have_

Proof.: The second assumption gives, as in the proof of Lemma D.20, that \(() 1/2\|(^{T}\,)^{-1} \|^{-1}\). Together with \(\|\,\,\|\), the result follows. 

#### d.6.2 Application to Bounds in the Error of the Return Function

We now apply the results from the preceding section to our case. Define \(():\,^{}\) as the restriction of the belief operator \(\) to \(\,\). Assume that \(\,=\{0\}\), which is, according to Corollary D.4, a sufficient condition for identifiability. Note that this condition means that \(()\) is injective. Thus, Lemma D.16 ensures that \(()^{T}()\) is invertible and that \((()^{T}())^{-1}( )^{T}\) is a left inverse of \(()\).

Consequently, from the equation

\[()(G)=(G)\]

we obtain

\[G=(()^{T}())^{-1} ()^{T}((G)).\]

This is the concrete formula with which \(G\) can be identified from \((G)\). When perturbing \(\), this leads to a corresponding perturbance in \((()^{T}())^{-1}( )^{T}\) whose size influences the maximal error in the inference of \(G\). This, in turn, influences the size of the error in \(J_{G}\), the policy evaluation function, where

\[J_{G}()}_{ P^{}( )}G().\]

We obtain:

**Theorem D.23**.: _Let \(G\) be the true reward function, \(\) the belief operator corresponding to the human's true belief model \(B()\), and \((G)\) be the resulting observation-based return function. Assume that \(=\{0\}\), so that \(()^{T}()\) is invertible. Let \(:^{}^{}\) be a perturbation satisfying \(\|\|\), where \(\) satisfies the following two properties:_

\[(),- ()+()^{2 }+1/2()^{T}() ^{-1}^{-1}}.\]

_Let \(_{}+\) be the misspecified belief operator. The first claim is that \((_{})^{T}(_{})\) is invertible under these conditions._

_Now, assume that the learning system infers the return function \((_{})^{T}(_{})^{-1}(_{})^{T}((G))\).4 Then there is a polynomial \(Q(X,Y)\) of degree five such that_

_Thus, for all policies \(\), we obtain_

\[J_{}()-J_{G}()\|G\| Q( ()^{T}())^{-1},\|( )\|.\]

_In particular, for sufficiently small perturbances \(\), the error in the inferred policy evaluation function \(J_{}\) becomes arbitrarily small._

Proof.: That \((_{})^{T}(_{})\) is invertible follows immediately from Proposition D.21 by using that \(\|()\|\|\) and that \((_{})=()_{( )}\), together with the second bound on \(\) (which implies the assumed bound in Proposition D.21).

We have

\[J_{}()-J_{G}() =*{}_{ P^{}() }(-G)()\] \[*{}_{ P^{}()} (-G)()\] \[*{}_{}(-G)()\] \[\|-G\|\] \[=(_{})^{T} (_{})^{-1}(_{ })^{T}-()^{T}( )^{-1}()^{T}(G)\] \[\|(_{})^{T}(_{})^{-1}(_{})^{T}-()^{T}()^{-1 }()^{T}(G)\] \[ C((),)\|()(G)\|\] \[ C((),)\|() \|\|G\|.\]

In the second to last step, we used Proposition D.21. By Proposition D.22, we can define the polynomial \(Q(X,Y)\) by

\[Q(X,Y)=XY12XY^{2}+1,\]

which is of degree five.

The last claim follows from \(_{ 0}=0\). 

**Remark D.24**.: _In the case of a square matrix \(\) that is injective, we can apply Theorem D.18 directly to \(^{-1}\) (which is now invertible) and obtain the following simplification of Theorem D.23 for the case that \(\|\|\|^{-1}\|^{-1}\):_

\[J_{}()-J_{G}() 2\| \|\|G\|\|^{-1}\|^{2}.\]

_The polynomial is then only of degree 3._

### Preliminary Characterizations of the Ambiguity

Recall the sequence of functions

In this section, we clarify \(\) and \(\) in special cases, as their intersection is the crucial ambiguity in Theorem D.2.

The following proposition shows that for deterministic \(P_{}\) and a rational human, \(\) decomposes into hyperplanes defined by normal vectors of probabilities of sequences mapping to the same observation sequence:

**Proposition D.25**.: _Assume the human reasons as in Section D.1. Assume \(P_{}\) is deterministic. Let \(B()\) be the distribution of sequences under the human's belief over the policy, given by \(B()=_{^{}}B(^{})P^{^{}}()\) for some policy prior \(B(^{})\). For each \(\), let \(B_{}[B()]_{()=} ^{\{()=\}}\) be the vector of probabilities of sequences that are observed as \(\)._

_Let \(G^{}\) be a return function. For each \(\), define the restriction \(G^{}_{}^{\{|()= \}}\) by \(G^{}_{}() G^{}()\) for all \(\{}()=\}\). Assume that \(B()\) is the Bayesian posterior. Then \(G^{}\) if and only if the property_

\[B_{} G^{}_{}=0\]

_holds for all \(\)._

Proof.: For a deterministic observation kernel \(P_{}\), by Bayes rule we have

\[B() =}() B()}{_{ {s}^{}}P_{}(^{}) B(^{})}\] \[=}() B( {s})}{_{^{}}_{}(^{})  B(^{})}\] \[=0,\ }()\\ )}{_{^{}(^{})=}B(^{})},\ ()=.\]

Thus, for any return function \(G^{}\) and any observation sequence \(\), we have

\[\,(G^{})() = B()}{}G ^{}()\] \[=_{}B()G^{}()\] \[=_{()=})}{ _{^{}(^{})=}B(^{ })}G^{}()\] \[=_{^{}(^{})= }B(^{})^{-1}_{()=}B()G^{}().\]

Thus, we have \(G^{}\) if and only if

\[B_{} G^{}_{}=_{()= }B()G^{}()=0\]

for all \(\). That was to show. 

**Remark D.26**.: _One can interpret the previous proposition as follows: As long as \(\) is injective, we have \(\{}()=o\}=1\) for all \(\), meaning that \(B_{}\) and \(G^{}_{}\) have only one entry. Thus, \(B_{} G^{}_{}=0\) implies \(G^{}_{}=0\). If that holds for all \(\), then \(G^{}\) implies \(G^{}=0\), meaning \(\) is injective.__However, as soon as there is an \(\) with \(k_{}:=\{()=o\}>1\), the equation \(B_{} G^{}_{}=0\) leads to \(k_{}-1\) free parameters in \(G^{}_{}\). \(G^{}_{}\) can then be chosen freely in the hyperplane of vectors orthogonal to \(B_{}\) without moving out of the kernel of \(\)._

_Another way of writing Proposition D.25 is to write \(\) as a direct sum of these hyperplanes perpendicular to \(B_{}\):_

\[=_{:\ |^{-1}()| 2}B_{}^{ }.\]

Recall that a return function \(G\) is called _time-separable_ if there exists a reward function \(R\) such that \((R)=G\).

Before we discuss time-separability in more interesting examples, we want to talk about one simple case where all return functions are time-separable. We leave a general characterization of \(\) to future work.

**Proposition D.27**.: _Let there be an ordering \(^{(1)},^{(2)},\) of all sequences in \(}\), and a function \(:}\) from sequences to states such that \(()\) and \((^{(k)})^{(i)}\) for all \(i<k\). Then every return function is time-separable._

Proof.: Let \(G\) be a return function. Initialize \(R(s)=0\) for all \(s\) and inductively update it for all \(i=1,2,\):

\[R(^{(i)}):=_{t:\ s^{(i)}_{t}=(^{(i)})}^{t}^{-1}G(^{(i)})-_{t:\ s^{(i)}_{t} (^{(i)})}^{t} Rs^{(i)}_{t},\]

where the inductive definition always uses \(R\) as it is defined by that point in time. Once \(R(^{(i)})\) is defined, but not yet any future values \(R(^{(k)})\), \(k>i\), we have

\[\,(R)(^{(i)}) =_{t=0}^{T}^{t} Rs^{(i)}_{t}\] \[=_{t:\ s^{(i)}_{t}=(^{(i)})}^{t}  R(^{(i)})+_{t:\ s^{(i)}_{t} (^{(i)})}^{t} Rs^{(i)}_{t}\] \[=G(^{(i)}).\]

Furthermore, the property \((^{(k)})^{(i)}\) for all \(i<k\) ensures that changes to the reward function for \(k>i\) do not affect the value of \(\,(R)(^{(i)})\). This shows \((R)=G\), and thus \(G\) is time-separable. 

**Corollary D.28**.: _In a multi-armed bandit, every return function is time-separable._

Proof.: In a multi-armed bandit, states and sequences are equivalent, and so we can choose \((s)=s\) for every state/sequence \(s\). The result follows from Proposition D.27.

Alternatively, simply directly notice that in a multi-armed bandit, \(\) is the identity mapping, and so for every return/reward function \(R\), we have \((R)=R\). 

### Examples Supplementing Section 5

In this whole section, the inverse temperature parameter in the human choice probabilities is given by \(=1\). We now consider four more mathematical examples of Corollary D.4 and Theorem D.9. In the first example, the ambiguity is so bad that the reward inference can become worse than simply maximizing \(J_{}\) as in naive RLHF. In Example D.30, there is simply "noise" in the observations and the human's belief, the matrices \(\) and \(\) are injective, and identifiability works, as in Corollary D.14. In the third example, the matrix \(\) is not injective and identifiability fails, which is a minimal example showing the limits of our main theorems. In the fourth example, the matrix \(\) is not injective, but \(=\{0\}\), and so identifiability works. This example is interesting in that the identifiabilitysimply emerges through different distributions of _delay_ that are caused by the different unobserved events.

In this section, both the linear operators \(:^{}}^{}\) and \(:^{}^{}}\) are considered as matrices

\[=P_{}()_{,} ^{}},=B( {s})_{,}^{}}.\]

Notice that both have a swap in their indices.

**Example D.29**.: _Theorem 5.2 shows that the remaining ambiguity from the human's choice probabilities is given by \(\), but it doesn't explain how to proceed given this ambiguity. Without further inductive biases, some reward functions within the ambiguity of the true reward function can be even worse than simply maximizing \(J_{}\)._

_E.g., consider a multi-armed bandit with three actions \(a,b,c\), observation-kernel \(o=O(a)=O(b) O(c)=c\) and reward function \(R(a)=R(b)<R(c)\). If the human belief is given by \(B(a o)=p=1-B(b o)\), then \(R^{}=(p-1,p,0)^{\{a,b,c\}}\) is in the ambiguity for all \(\), and so \(:=R+R^{}\) is compatible with the choice probabilities. However, for \( 0\), we have \((a)>(b)\) and \((a)>(c)\), and so optimizing against this reward function leads to a suboptimal policy._

_In contrast, maximizing \(J_{}\) leads to the correct policy since \(a\), \(b\), and \(c\) all obtain their ground truth reward in this example. This generally raises the question of how to tie-break reward functions in the ambiguity, or how to act conservatively given the uncertainty, in order to consistently improve upon the setting in Section 4.1._

**Example D.30**.: _This example is a special case of Corollary D.14. Consider a multi-armed bandit with two actions (which are automatically also states and sequences) \(a\) and \(b\). In this case, the reward function and return function is the same._

_We assume there to be two possible observations \(o^{(a)},o^{(b)}\) and the observation kernel to be non-deterministic, with probabilities_

\[P_{O}(o^{(j)} i)=2/3,i=j,\\ 1/3,.\]

_If we assume the human forms Bayesian posterior beliefs as in Section D.1 and to have a policy prior \(B(^{})\) such that \(B(a)=_{}(a)B(^{})d=1/2\) and \(B(b)=1/2\), then it is easy to show that the human's belief is the "reversed" observation kernel:_

\[B(j o^{(i)})=P_{O}(o^{(i)} j).\]

_We obtain_

\[==2/3&1/3\\ 1/3&2/3=2&1\\ 1&2\]

_These matrices are injective since they are invertible:_

\[^{-1}=^{-1}=2&-1\\ -1&2.\]

_More generally, even if the human does not form fully rational posterior beliefs, it is easy to imagine that the matrix \(\) can end up being invertible. Thus, Corollary D.4 guarantees that the reward function can be inferred up to an additive constant from the choice probabilities of observations, and Theorem D.9 shows that this even works when the learning system does not know what the human observed._

_In the rest of this example, we explicitly walk the reader through the process of how the reward function can be inferred, in the general case that the observations are not known. In the process, we essentially recreate the proof of the theorems for this special case. For this aim, we first want to compute the choice probabilities \(P^{R}i j\) that the learning system has access to in the limit of infinite data. We assume that the reward function is given by \(R(a)=-1\) and \(R(b)=2\). We compute:_

\[(R)=2&1\\ 1&2-1\\ 2=0\\ 1.\]_In other words, we have \(_{s B(s|o^{(s)})}[R(s)]=0\) and \(_{s B(s|o^{(s)})}[R(s)]=1\). From this, we can compute the observation-based choice probabilities \(_{o^{(s)}o^{(j)}}=\,(R)(o^{(i)})- (R)(o^{(j)})\), see Equation (2), and obtain:_

\[_{o^{(a)}o^{(a)}}=_{o^{(b)}o^{(b)}}=, _{o^{(s)}o^{(b)}}=,_{o^{(b)}o ^{(a)}}=.\]

_We can now determine the final choice probabilities \(P_{ij} P^{R}i j\) again by a matrix-vector product, with the indices ordered lexicographically, see Equation (8). Here, \(\) is the Kronecker product of the matrix \(\) with itself:_

\[P=()= 4&2&2&1\\ 2&4&1&2\\ 2&1&4&2\\ 1&2&2&41/2\\ 1/(1+e)\\ e/(1+e)\\ 1/2=1/2\\ 1/3(2+e)/(1+e)\\ 1/3(1+2e)/(1+e)\\ 1/2.\]

_For example, the second entry in \(P\) is \(P_{ab}=P^{R}a b=\). This is the likelihood that, for ground-truth actions \(a,b\), the human will prefer \(a\) after only receiving observations \(o^{(a)}\) or \(o^{(b)}\) according to \(\) and following a Boltzman-rational policy based on the belief of the real action, see Equation (8)._

_Over time, the learning system will be able to estimate these probabilities based on repeated human choices, assuming all state-pairs are sampled infinitely often. The question of identifiability is whether the original reward function \(R\) can be inferred from that data, given that the learning system knows \(\) and \(\). We assume that the learning system doesn't a priori know \(R\) or any of the intermediate steps in the computation. First, \(\) can be inferred by inverting \(\):_

\[=()^{-1} P=4&-2&- 2&1\\ -2&4&1&-2\\ -2&1&4&-2\\ 1&-2&-2&41/2\\ 1/3(2+e)/(1+e)\\ 1/3(1+2e)/(1+e)\\ 1/2=1/2\\ 1/(1+e)\\ e/(1+e)\\ 1/2.\]

_The learning system wants to use this to infer \(()\) (for the later-to-be inferred reward function \(\) that may differ from the true reward function \(R\)) and uses the equation_

\[_{o^{(a)}o^{(b)}}=\,()(o^{(a) })}{\,()(o^{(a)})+\, ()(o^{(b)})},\]

_which can be rearranged to_

\[()(o^{(a)})=_{o^{(a)}o^{(b)}}}{1- _{o^{(a)}o^{(b)}}}+()(o^{(b)})=+()(o^{(b)})=()(o^{(b)})-1.\]

_This relation is all which can be inferred about \(()(o^{(a)})\) and \(()(o^{(b)})\); the precise value cannot be determined and \(()(o^{(b)})\) is a free parameter. One can check that for \(()(o^{(b)})=1\) this coincides with the true value \((R)\). Finally, one can invert \(\) to infer \(\) from this:_

\[ =^{-1}()\] \[=2&-1\\ -1&2()(o^{(b)})-1\\ ()(o^{(b)})\] \[=()(o^{(b)})-2\\ 1+()(o^{(b)})\] \[=-1\\ 2+()(o^{(b)})-1\\ ()(o^{(b)})-1\] \[=R+()(o^{(b)})-1\\ ()(o^{(b)})-1.\]

_Thus, the inferred and true reward functions differ maximally by a constant, as predicted in Theorem D.9._In the following example, we work out a case where the reward function is so ambiguous that any policy is optimal to some reward function consistent with the human feedback:

**Example D.31**.: _Consider a multi-armed bandit with exactly three actions/states \(a,b,c\). We assume a deterministic observation kernel with \(o O(a)=O(c) O(b)=b\). Assume the human has some arbitrary beliefs \(B(a o),B(c o)=1-B(a o)\), and can identify \(b\): \(B(b b)=1\). Then if the human makes observation comparisons with a Boltzman-rational policy, as in Theorem D.2, the resulting reward function is so ambiguous that some reward functions consistent with the feedback place the highest value on action \(a\), no matter the true reward function \(R\). Thus, even if the true reward function \(R\) regards \(a\) as the worst action, \(a\) can result from the reward learning and subsequent policy optimization process._

Proof.: The matrix \(:^{\{a,b,c\}}^{\{o,b\}}\) is given by

\[=B(a o)&0&B(c o)\\ 0&1&0.\]

Its kernel is given by reward functions \(R^{}\) with \(R^{}(b)=0\) and \(R^{}(c)=-R^{}(a)\), with \(R^{}(a)\) a free parameter. Theorem D.2 shows that, up to an additive constant, the reward functions consistent with the feedback of observation comparisons are given by \(=R+R^{}\) for any \(R^{}\). Thus, whenever the free parameter \(R^{}(a)\) satisfies \(R^{}(a)>R(b)-R(a)\) and \(R^{}(a)>B(c o)R(c)-R(a)\), we obtain \((a)>(b)\) and \((a)>(c)\), showing the claim. 

We now investigate another example where \(\) is not injective, and yet, identifiability works because \(\{0\}\). We saw such cases already in Example E.6, but include this additional example since it shows a conceptually interesting case: two different states lead to the exact same observations, but can be disambiguated since they lead to different amounts of _delay_ until a more informative observation is made again.

**Example D.32**.: _In this example, we assume that the human knows the policy \(\) that generates the state sequences (corresponding to a policy prior \(B(^{})=_{}(^{})\) concentrated on \(\)), which together with knowledge of the transition dynamics of the environment determines the true state transition probabilities \(^{}(s^{} s)=_{a}(s^{ } s,a)(a s)\). We consider an environment with three states \(s,s^{},s^{}\) and the following transition dynamics \(^{}\), where \(p 1/2\) is a probability:_

_We assume that \(P_{0}(s)=1\). Furthermore, we assume deterministic observations and \(s=O(s) O(s^{})=O(s^{}) o\)._

_Assume the time horizon \(T\) is \(3\), i.e., there are timesteps \(0,1,2,3\). Assume that the human forms the belief over the true state sequence by Bayesian posterior updates as in Section D.1. In this case, \(\{0\}\) by Proposition D.11. However, we will now show that \(()=\{0\}\). If the human makes Boltzmann-rational comparisons of observation sequences, then this implies the identifiability of the return function up to an additive constant by Corollary D.4.5_

_Thus, let \(R^{}()\), i.e., \(\,\,\,(R^{})( )=0\) for every observation sequence \(\). For \(=ssss\) being the observation sequence that only consists of state \(s\), this implies \(R^{}(s)=0\). Consequently, for general observation sequences \(\), we have:_

\[0=\,\,\,(R^{})( )= B(|)}{}[\,_{t= 0}^{3}_{s^{}}(s_{t})^{t}] R^{}(s^{ })+ B(|)}{}[\,_{t= 0}^{3}_{s^{}}(s_{t})^{t}] R^{}(s ^{}).\]_Now we specialize this equation to the two observation sequences \(^{(1)}=soss\) and \(^{(2)}=soos\). We start by considering \(^{(1)}\). This is consistent with the two state sequences \(^{(1),(s^{})}=ss^{}ss\) and \(^{(1),(s^{})}=ss^{}ss\). We have posterior probabilities_

\[B^{(1),(s^{})}^{(1)}=1-p, B ^{(1),(s^{})}^{(1)}=p,\]

_and therefore_

\[0=\,\,\,(R^{})( ^{(1)})=(1-p) R^{}(s^{})+p  R^{}(s^{}),\]

_and so_

\[R^{}(s^{})= R^{}(s^{}).\] (11)

_Similarly, \(^{(2)}\) is consistent with the sequences \(^{(2),(s^{})}=ss^{}s^{}s\) and \(^{(2),(s^{})}=ss^{}s^{}s\). They have posterior probabilities_

\[B^{(2),(s^{})}^{(2)}=,  B^{(2),(s^{})}^{(2)}= ,\]

_leading to_

\[0=(+^{2}) R^{}(s^{})+ (+^{2}) R^{}(s^{}).\]

_Together with Equation (11), we obtain_

\[R^{}(s^{})=-R^{}(s^{})= R^{ }(s^{}),\]

_which implies \(R^{}(s^{})=0\) because \(p\), and thus also \(R^{}(s^{})=0\). Overall, we have showed \(R^{}=0\), and so \(\) is injective. This means that reward functions are identifiable in this example up to an additive constant, see Corollary D.4._

## Appendix E Issues of Naively Applying RLHF under Partial Observability

In this section, we study the naive application of RLHF under partial observability. Thus, most of it takes a step back from the general theory of _appropriately modeled_ partial observability in RLHF. Later, we will analyze examples where we also apply the general theory, which is why this appendix section comes second.

In Section E.1, we first briefly explain what happens when the learning system incorrectly assumes that the human observes the full environment state. We show that as a consequence, the system is incentivized to infer what we call the _observation return function_\(G_{}\), which evaluates a state sequence based on the human's belief of the state sequence given the human's observations. In the policy optimization process, the policy is then selected to maximize \(J_{}\), an expectation over \(G_{}\). In the interlude in Section E.2, we then briefly analyze the unrealistic case that the human, when evaluating a policy \(\), fully knows the complete specification of that policy and all of the environment and engages in rational Bayesian reasoning; in this case, \(J_{}=J\) is the true policy evaluation function.

Realistically, however, maximizing \(J_{}\) can lead to failure modes. In Appendix E.3 we prove that a suboptimal policy that is optimal according to \(J_{}\) causes deceptive inflation, overjustification, or both. In Appendix C.3, we expand on the analysis of the main examples in the main paper. Finally, in Section E.4, we study further concrete examples where maximizing \(J_{}\) reveals deceptive and overjustifying behavior by the resulting policy.

### Optimal Policies under RLHF with Deterministic Partial Observations Maximize \(J_{}\)

Assume that \(P_{}}\) is deterministic and that the human makes Boltzmann-rational sequence comparisons between observation sequences. The true choice probabilities are then given by (See Equations (2) and (8)):

\[P^{R}\!\!^{}= \,\!\!G()- \,\!\!G(^{}) \] (12)

Now, assume that the learning system does _not model the situation correctly_. In particular, we assume:* The system is not aware that the human only observes observation sequences \(()\) instead of the full state sequences.
* The system does not model that the human's return function is _time-separable_, i.e., comes from a reward function \(R\) over environment states.

The learning system then thinks that there is a return function \(^{}\) such that the choice probabilities are given by the following faulty formula:

\[P^{R}^{} G()-G(^{})\]

Now, assume that the learning system has access to the choice probabilities and wants to infer \(G\). Inverting the sigmoid function and then plugging in the true choice probabilities from Equation (12), we obtain:

\[() =(^{})}{P ^{R}(^{})}+(^{})\] \[=\! \!G()-\!\!G (^{})+(^{})\] \[=\!\!G() +C(^{}).@note{footnote}{Note that in the case of non-deterministic observation kernels and choice probabilities given as in Equation (8), this argument does not work since the logarithm cannot be swapped with the outer expectation of the choice probabilities.}\]

Here, \(C(^{})\) is some quantity that does not depend on \(\). Now, fix \(^{}\) as a reference sequence. Then for varying \(\), \(C(^{})\) is simply an additive constant. Consequently, up to an additive constant, this determines the return function that the learning system is incentivized to infer. We call it the _observation return function_ since it is the return function based on the human's observations:

\[G_{}()\!\!G ().\]

This return function is not necessarily time-separable, but we assume that time-separability is not modeled correctly by the learning system. Now, define the resulting policy evaluation function \(J_{}\) by

\[J_{}() P^{}()}{}\,G_{}().\]

This is the policy evaluation function that would be optimized if the learning system erroneously inferred the return function \(G_{}\).

### Interlude: When the Human Knows the Policy and is a Bayesian Reasoner, then \(J_{}=J\)

In this section, we briefly consider what would happen if in \(J_{}\), the human's belief \(B\) would make use of the true policy and be a rational Bayesian posterior as in Section D.1. We will show that under these conditions, we have \(J_{}=J\). Since these are unrealistic assumptions, no other section depends on this result.

For the analysis, we drop the assumption that the observation sequence kernel \(P_{}\) is deterministic, and assume that \(J_{}\) is given as follows:

\[J_{}() P^{}()}{}\, P^{}()}{}\, ^{} B^{}(^{}|)}{} \,G(^{}).\] (13)

In this formula, \(B^{}() B(,)\) with \(B\) being the joint distribution from Section D.1. Formally, this is the posterior of the joint distribution \(B(,)\) that is given by the following hidden Markov model:

(14)Here, \(^{}(s^{} s)_{a}(s^{ } s,a)(a s)\). \(s_{0}\) is sampled according to the known initial distribution \(P_{0}(s_{0})\). The human's posterior \(B^{}(^{})\) is then the true posterior in this HMM. We obtain:

**Proposition E.1**.: _Let \(\) be a policy that is known to the human. Then \(J_{}()=J()\)._

Proof.: By Equation (13), we have

\[J_{}() = P^{}()}{}  P_{0}(|)}{}^{} B^{}(^{}|)}{}G( ^{})\] \[_{^{}}P^{}()_{ }P_{}()_{^{}}B^{}(^{})G(^{})\] \[_{^{}}_{}B^{ }(^{})_{}P_{}( )P^{}()G(^{})\] \[_{^{}}_{}B^{ }(^{})B^{}()G(^{})\] \[_{^{}}_{}P^{ }(^{})P_{}(^{})G(^{})\] \[_{^{}}P^{}(^{}) G(^{})\] \[_{}P^{}()G()\] \[J().\]

In step (1), we wrote the expectations out in terms of sums. In step (2), we reordered them. In step (3), we observed that the inner sum over \(\) evaluates to the marginal distribution \(B^{}()\) of the observation sequence \(\) in the HMM in Equation (13). In step (4), we used Bayes rule in the inner sum. This is possible since \(B^{}(^{})\) is the true posterior when \(\) is known. In step (5), we pull \(P^{}(^{})\) out and notice that the remaining inner sum evaluates to \(1\). Step (6) is a relabeling and step (7) the definition of the true policy evaluation function \(J\). 

### Proof of Theorem 4.5

We first prove the following lemma.

**Lemma E.2**.: _Let \(\) and \(_{}\) be two policies. If \(J()<J(_{})\) and \(J_{}()>J_{}(_{})\), then relative to \(_{}\), \(\) must exhibit deceptive inflation, overjustification, or both._

Proof.: We start by establishing a quantitative relationship between the average overestimation and underestimation errors \(^{+}\) and \(^{-}\) as defined in Definition 4.2, the true policy evaluation function \(J\), and the observation evaluation function \(J_{}\) defined in Equation (4). Define \(:}\) by \(()=G_{}()-G()\), where \(G_{}\) is as defined in Equation (3). Consider the quantity

\[E^{+}()-E^{-}()=0,()-0,-().\]

If \(()>0\), then the first term is \(()\) and the second one is \(0\). If \(()<0\), then the first term is zero and the second one is \(()\). If \(()=0\), then both terms are zero. In all cases the right-hand side is equal to \(()\). Unpacking the definition of \(\) again, we have that for all \(\),

\[E^{+}()-E^{-}()=G_{}()-G().\] (15)

For any policy \(\), if we take the expectation of both sides of this equation over the on-policy distribution admitted by \(\), \(P^{}\), we get

\[^{+}()-^{-}()=J_{}()-J().\] (16)We now prove the lemma. Let \(\) and \(_{}\) be two policies, and assume that \(J()<J(_{})\) and \(J_{}() J_{}(_{})\). Equivalently, we have \(J_{}()-J_{}(_{}) 0\) and \(J(_{})-J()>0\), which we combine to state

\[(J_{}()-J_{}(_{}))+(J (_{})-J())>0.\] (17)

Rearranging terms yields

\[(J_{}()-J())-(J_{}(_{})-J(_{}))>0.\]

These two differences inside parentheses are equal to the right-hand side of (16) for \(\) and \(_{}\), respectively. We substitute the left-hand side of (16) twice to obtain

\[(^{+}()-^{-}())-(^{ +}(_{})-^{-}(_{}))>0.\]

Rearranging terms again yields

\[(^{+}()-^{+}(_{}))+( ^{-}(_{})-^{-}())>0.\] (18)

If \(^{+}()-^{+}(_{})>0\) then we have \(^{+}()>^{+}(_{})\) and, by assumption, \(J_{}()>J_{}(_{})\). By Definition 4.3, this means \(\) exhibits deceptive inflation relative to \(_{}\).

If \(^{-}(_{})-^{-}()>0\) then we have \(^{-}()<^{-}(_{})\) and, by assumption, \(J()<J(_{})\). By Definition 4.4, this means \(\) exhibits overjustification relative to \(_{}\).

At least one of the two differences in parentheses in (18) must be positive, otherwise their sum would not be positive. Thus \(\) must exhibit deceptive inflation relative to \(_{}\), overjustification relative to \(_{}\), or both. 

We can now combine earlier results to prove Theorem 4.5, repeated here for convenience:

**Theorem E.3**.: _Assume that \(P_{O}\) is deterministic. Let \(_{}^{*}\) be an optimal policy according to a naive application of RLHF under partial observability, and let \(^{*}\) be an optimal policy according to the true objective \(J\). If \(_{}^{*}\) is not \(J\)-optimal, then relative to \(^{*}\), \(_{}^{*}\) must exhibit deceptive inflation, overjustification, or both._

Proof.: Because \(P_{O}\) is deterministic, \(_{}^{*}\) must be optimal with respect to \(J_{}\) by Proposition 4.1 (proved in Appendix E.1). Thus \(J_{}(_{}^{*}) J_{}(^{*})\). Since \(^{*}\) is \(J\)-optimal and \(_{}^{*}\) is not, \(J(^{*})<J(_{}^{*})\). By Lemma E.2, relative to \(^{*}\), \(_{}^{*}\) must exhibit deceptive inflation, overjustification, or both. 

### Further Examples Supplementing Section 4.4

In this section, we present further mathematical examples supplementing those in Section 4.4. We found many of them before finding the examples we discuss in the main paper, and show the same and additional conceptual features with somewhat less polish. We again assume that \(P_{}\) is deterministic.

**Example E.4**.: _In the main paper, we have assumed a model where the human obeys Eq. (2) and showed that a naive application of RLHF can lead to suboptimal policies, and the specific failure modes of deceptive inflation and overjustification. What if the human makes the choices in a different way? Specifically, assume that all we know is that \(P^{R}(^{})+P^{R}(^{})=1\). Can the human generally choose these choice probabilities in such a way that RLHF is incentivized to infer a reward function whose optimal policies are also optimal for \(R\)? The answer is no._

_Take the following example:_

_In this example, there is a fixed start state \(s\) and three actions \(a,b,c\) that also serve as the final states. The time horizon is \(T=1\), so the only state sequences are \(sa,sb,sc\). Assume \((a s,a)=1\)\((b s,b)=1\), \((c s,c)=1-\), \((a s,c)=\), i.e., selecting action \(c\) sometimes leads to state \(a\). Also, assume \(a=O(a) O(b)=O(c) o\) and \(R(a)=R(b)<R(c)\)._

_Since \(b\) and \(c\) have the same observation \(o\), the human choice probabilities do not make a difference between them, and so RLHF is incentivized to infer a reward function \(\) with \((b)=(c)(o)\). If \((o)>(a)\), then the policy optimal under \(\) will produce action \(b\) since this deterministically leads to observation \(o\), whereas \(c\) does not. If \((o)<(a)\), then the policy optimal under \(\) will produce action \(a\). In both cases, the resulting policy is suboptimal compared to \(^{*}\), which deterministically chooses action \(c\)._

In the coming examples, it will also be useful to look at the _misleadingness_ of state sequences:

**Definition E.5** (Misleadingness).: _Let \(}\) be a state sequence. Then its misleadingness is defined by_

\[() G_{}()-G()=}_{^{} B(^{}|}())}G(^{})-G(s).\]

_We call a state sequence positively misleading if \(M()>0\), which means the sequence appears better than it is, and negatively misleading if \(()<0\). The misleadingness vector is given by \(^{}}\)._

Note that the misleadingness is related to \(E^{+}\) and \(E^{-}\), as defined in Definition 4.2: If \(()>0\) then \(()=E^{+}()\), and if \(()<0\) then \(()=-E^{-}()\).

**Example E.6**.: _In this example, we assume the human is a Bayesian reasoner as in Section D.1. Consider the MDP that is suggestively depicted as follows:_

_The MDP has states \(=\{a,b,c\}\) and actions \(=\{b,c\}\). The transition kernel is given by \((c a,c)=1\) and \((b a,b)=1\), meaning that the action determines whether to transition from \(a\) to \(b\) or \(c\). All other transitions are deterministic and do not depend on the action, as depicted. We assume an initial state distribution \(P_{0}\) over states with probabilities \(p_{a}=P_{0}(a),p_{b}=P_{0}(b),p_{c}=P_{0}(c)\). The true reward function \(R^{\{a,b,c\}}\) and discount factor \([0,1)\) are, for now, kept arbitrary. The time horizon is \(T=2\), meaning we have four possible state sequences \(acc\), \(abc\), \(bcc\), \(ccc\)._

_Furthermore, assume that \(o O(a)=O(b) O(c)=c\), i.e., \(c\) is observed and \(a\) and \(b\) are ambiguous._

_Finally, assume that the human has a policy prior \(B()\), where \(=_{}(c a)\) is the likelihood that the policy chooses action \(c\) when in state \(a\), which is a parameter that determines the entire policy._

_We claim the following:_

1. _If_ \(p_{b}}_{ B()}[]  p_{a}\)_, then_ \(}}}=\{0\}\)_, so there is no return function ambiguity under appropriately modeled partially observable RLHF, see Corollary_ D.4_._
2. _There are true reward functions_ \(R\) _for which optimizing_ \(J_{}\) _leads to a suboptimal policy according to the true policy evaluation function_ \(J\)_, a case of misalignment. Thus, a naive application of RLHF under partial observability fails, see Section_ 4.1_._
3. _The failure modes are related to hiding negative information (deception) and purposefully revealing information while incuring a loss (overjustifying behavior)._

Proof.: Write \(p B(bcc occ)\), the human's posterior probability of state sequence \(bcc\) for observation sequence \(occ\). We have \(1-p=B(acc occ)\).

Consider the linear operators \(}:^{\{a,b,c\}}^{\{abc, bcc,ccc,acc\}}\) and \(}:^{\{abc,bcc,ccc,acc\}}^{\{ ooc,occ,ccc\}}\) defined in the main paper. When ordering the states, state sequences, and observation sequences as we just wrote down, we obtain

\[=1&&^{2}\\ 0&1&+^{2}\\ 0&0&1++^{2}\\ 0&0&+^{2},=1&0&0&0\\ 0&p&0&1-p\\ 0&0&1&0,= 1&&^{2}\\ 1-p&p&+^{2}\\ 0&0&1++^{2}.\]

By Corollary D.4, if \(\) is injective, then there is no reward function ambiguity. Clearly, this is the case if and only if \(p(1-p)\). From Bayes rule, we have

\[p=, 1-p=.\]

So the condition for injectivity holds if and only if

\[B(bcc) B(acc).\]

Now, notice

\[B(bcc)=_{}B() B(bcc)d=_{} B() p_{b}d=p_{b}\]

and

\[B(acc)=_{}B()B(acc)d=_{}B( ) p_{a} d=p_{a}}.\]

This shows the first result.

For the second statement, we explicitly compute \(J_{}\) up to an affine transformation, which does not change the policy ordering. Let \(R\) be the true reward function, \(G=(R)\) the corresponding return function, and \((G)\) the resulting return function at the level of observations. For simplicity, assume \(R(c)=0\), which can always be achieved by adding a constant. We have:

\[J_{}() = P^{}()}{} (G)()\] \[=P^{}(abc)(G)(occ)+P^{}(bcc) (G)(occ)+P^{}(ccc)(G)(ccc)+P^{}(acc) (G)(occ)\] \[=p_{a}(1-) G(abc)+p_{b}(G)(occ)+ p_{c} G(ccc)+p_{a}(G)(occ)\] \[\,(G)(occ)-G(abc).\]

We have

\[G(abc)=R(a)+ R(b),(G)(occ)=(1-p) G(acc)+p G( bcc)=(1-p) R(a)+p R(b).\]

Thus, the condition \((G)(occ)>G(abc)\) is equivalent to

\[R(a)< R(b).\]

Thus, we have

\[\,J_{}()= 1,R(a)< R(b),\\ 0,\]

Now consider the case \(R(b)>0\). In this case, \(=0\) gives rise to the optimal policy according to \(G\) since going to \(b\) gives extra reward that one misses when going to \(c\) directly. However, when \(R(a) 0\), then \(J_{}\) selects for \(=1\). Intuitively, the policy tries to "hide that the episode started in \(a\)" by going directly to \(c\), which leads to ambiguity between \(acc\) and \(bcc\). This is a case of deceptive inflation as in Theorem 4.5.

Now, consider the case \(R(b)<0\). In this case, \(=1\) gives rise to the optimal policy according to \(G\). However, when \(R(a) 0\), then \(J_{}\) selects for \(=0\). Intuitively, the policy tries to "reveal that the episode started with \(a\)" by going to \(b\), which is positive information to the human, but negative from the perspective of optimizing \(G\). As in Theorem 4.5, we see that this is a case of overjustification.

**Example E.7**.: _In this example, we consider an MDP that's similar to a multi-armed bandit with four states/actions \(a,b,c,d\) and observation kernel \(O(a)=O(b) O(c)=O(d)\). Formally, we can imagine that it is given by the MDP_

_with \(R(s)=0\) and a time-horizon of \(T=1\). In this example, we reveal that misleadingness and non-optimality (according to the true reward \(R\), or \(J\)) are in principle orthogonal concepts. We consider the following four example cases. In each one, we vary some environment parameters and then determine \(a^{*}_{}\), the action that results from optimizing \(J_{}\) (corresponding to a naive application of RLHF under partial observability, see Section 4.1), its misleadingness \((a^{*}_{})\) (see Definition E.5), and the action \(a^{*}\) that would result from optimizing \(J\). If \(a^{*}_{}=a^{*}\), then \(J_{}\) selects for the optimal action. For simplicity, we can imagine that the human has a uniform prior over what action results eventually (out of the action taken and potentially a deviation defined by \(\), see below) is taken before making an observation, i.e. \(B(a)=B(b)=B(c)=B(d)=\)._

1. _Assume_ \(R(a)>R(c)>R(d) R(b)\)_. Also assume that action_ \(d\) _leads with probability_ \(>0\) _to state_ \(b\)_, whereas all other actions lead deterministically to the specified state. Then_ \(a^{*}_{}=c\)_,_ \((c)<0\) _and_ \(a^{*}=a\)_._
2. _Assume_ \(R(d)>R(a)>R(c) R(b)\)_. Again, assume there is a small probability_ \(>0\) _that action_ \(d\) _leads to state_ \(b\)_. Then_ \(a^{*}_{}=c\)_,_ \((c)>0\)_, and_ \(a^{*}=d\) _or_ \(a^{*}=a\)_, depending on the size of_ \(\)_._
3. _Assume_ \(R(a)>R(b)>R(c)>R(d)\)_. Additionally, assume that there is a_ large _probability_ \(>0\) _that action_ \(a\) _leads to state_ \(d\)_, whereas all other actions lead to what's specified. If_ \(\) _is large enough, then_ \(a^{*}=b\)_. Additionally, we have_ \(a^{*}_{}=b\) _and_ \((b)>0\)_._
4. _Assume_ \(R(a)>R(b)>R(c)>R(d)\)_. Also, assume some probability_ \(>0\) _that action_ \(b\) _leads to state_ \(d\)_, whereas all other actions lead deterministically to what's specified. Then_ \(a^{*}_{}=a\)_,_ \((a)<0\)_, and_ \(a^{*}=a\)_._

_Overall, we notice:_

* _Example (a) shows a high regret and negative misleadingness of_ \(a^{*}_{}=c\)_. The action is better then it seems, but action_ \(a\) _would be better still but cannot be selected because it can be confused with the very bad action_ \(b\)_._
* _Example (b) shows a high regret and high misleadingness of_ \(a^{*}_{}=c\)_. The action is worse than it seems and also not optimal._
* _Example (c) shows zero regret and high misleadingness of_ \(a^{*}_{}=b\)_. The action is worse than it seems because it can be confused with_ \(a\)_, but it is still the optimal action because_ \(a\) _can turn into_ \(d\)_._
* _Example (d) shows zero regret negative misleadingness of_ \(a^{*}_{}=a\)_. The action is chosen even though it seems worse than it is, and is also optimal._

_Thus, we showed all combinations of regret and misleadingness of the action optimized for under \(J_{}\)._

_We can also notice the following: Examples (a) and (b) only differ in the placement of \(R(d)\). In particular, the reason that \(a^{*}_{}=c\) is structurally the same in both, but the misleadingness changes. This indicates that misleadingness is not on its own contributing to what \(J_{}\) optimizes for._

The following is the smallest example we found with the following properties:

* There is a unique start state and terminal state.
* A naive application of RLHF fails in a way that shows deception and overjustification.
* Modeling partial observability resolves the problems.

**Example E.8**.: _Consider the following graph:_

_This depicts an MDP with start state \(S\), terminal state \(T\) and possible state sequences \(STTT\), \(SATT\), \(SACT\), \(SCTT\), \(SBCT\), \(SBTT\) and no discount, i.e. \(=1\). Assume that \(S,B,C\) are observed, i.e. \(O(S)=S\), \(O(B)=B\), \(O(C)=C\), and that \(A\) and \(T\) are ambiguous: \(O(A)=O(T)=X\). Then there are five observation sequences \(SXXX,SXCX,SCXX,SBCX,SBXX\). Assume that the human can identify all observation sequences except \(SXXX\), with belief \(b=B(STTT SXXX)\) and \(1-b=B(SATT SXXX)\)._

_Then the return function is identifiable under these conditions when the human's belief is correctly modeled. However, for some choices of the true reward function \(R\) and transition dynamics of this MDP, we can obtain deceptive or overjustified behavior for a naive application of RLHF._

Proof.: We apply Corollary D.4. We order states, state sequences, and observation sequences as follows:

\[ =S,A,B,C,T,\] \[} =STTT,SATT,SACT,SCTT,SBCT,SBTT,\] \[ =SXXX,SXCX,SCXX,SBCX.\]

As can easily be verified, with this ordering the matrices \(^{}\) and \(^{}\) are given by:

\[=b&1-b&0&0&0&0\\ 0&0&1&0&0&0\\ 0&0&0&1&0&0\\ 0&0&0&0&1&0\\ 0&0&0&0&0&1,=1&0&0&0&3\\ 1&1&0&0&2\\ 1&1&0&1&1\\ 1&0&0&1&2\\ 1&0&1&1&1\\ 1&0&1&0&2.\]

To show identifiability, we need to show that \(=\{0\}\). Clearly, the kernel of \(\) is given by all return functions in \(^{}\) that are multiples of \(G^{}=(b-1,b,0,0,0,0)\). Assume \(G^{}\), meaning there is a reward function \(R^{}^{}}\) with \( R^{}=G^{}\). We need to deduce from this a contradiction. The assumption means we obtain the following equations:

\[(i)\;\;R^{}(S)+3R^{}(T)=b-1,\] \[(ii)\;\;R^{}(S)+R^{}(A)+2R^{}(T)=b,\] \[(iii)\;\;R^{}(S)+R^{}(A)+R^{}(C)+R^{}(T)=0,\] \[(iv)\;\;R^{}(S)+R^{}(C)+2R^{}(T)=0,\] \[(v)\;\;R^{}(S)+R^{}(B)+R^{}(C)+R^{}(T)=0\] \[(vi)\;\;R^{}(S)+R^{}(B)+2R^{}(T)=0\]

(iii) and (v) together imply \(R^{}(A)=R^{}(B)\); (iv) and (vi) together imply \(R^{}(B)=R^{}(C)\); (v) and (vi) together imply \(R^{}(C)=R^{}(T)\); so together, we have \(R^{}(A)=R^{}(T)\). Thus, replacing \(R^{}(A)\) in (ii) by \(R^{}(T)\) and comparing (i) and (ii), we obtain \(b-1=b\), a contradiction. Overall, this shows \(=\{0\}\), and thus identifiability of the return function by Corollary D.4.

Now we investigate the case of unmodeled partial observability.

For demonstrating overjustification, assume deterministic transition dynamics in which every arrow in the diagram can be chosen by the policy. Also, assume \(R(A) 0\), \(R(T)>0\), \(R(S)=0\)\(R(B)=0\), and \(R(C)=0\). Then the optimal policy chooses the state sequence \(STTT\). However, this trajectory has low observation value since \(G_{}(STTT)=( G)(XXXX)=bG(STTT)+(1-b)G(SATT)\), which is low since \(R(A) 0\). \(J_{}\) then selects for the suboptimal policies choosing \(SBTT\) or \(SCTT\), which is overjustified behavior that makes sure that the human does not think state \(A\) was accessed.

For demonstrating deception, assume that \(R(A) 0\), \(R(T)<0\), \(R(S)=R(B)=R(C)=0\) and that the transition dynamics are such that when the policy _attempts_ to transition from \(S\) to \(A\), it will sometimes transition to \(B\), with all other transitions deterministic. In this case, the optimal behavior attempts to enter state \(A\) since this has very high value. \(J_{}\), however, will select for the policy that chooses \(STTT\). This is deceptive behavior.

## Appendix F NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: This can be verified by reading the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 6 we have a paragraph on limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All theorems come with a full set of assumptions, with full proofs in the appendix linked. Sometimes, "background assumptions", like the fact that we study an underlying MDP with an additional observation kernel \(P_{O}()\), or that the human comes with a belief kernel \(B()\), are omitted in the theorem statements since they apply throughout to the whole paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments requiring code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/pub blic/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This research does not involve human subjects, does not make use of data, and does not propose a practical method that could be misused or have a negative impact. As such, the paper does not give rise to any ethical concerns. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The last paragraph of the main paper is an impact statement, listing the positive impact we hope to see from our work. As our work is theoretical and does not provide a method, no negative impact arises from it. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.