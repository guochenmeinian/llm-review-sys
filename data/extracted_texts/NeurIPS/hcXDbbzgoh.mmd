# Taylor TD-learning

Michele Garibbo

Department of Engineering Mathematics

University of Bristol

Bristol, United Kingdom

michele.garibbo@bristol.ac.uk &Maxime Robeyns

Department of Engineering Mathematics

University of Bristol

Bristol, United Kingdom

maxime.robeyns.2018@bristol.ac.uk &Laurence Aitchison

Department of Engineering Mathematics

University of Bristol

Bristol, United Kingdom

laurence.aitchison@bristol.ac.uk

###### Abstract

Many reinforcement learning approaches rely on temporal-difference (TD) learning to learn a critic. However, TD-learning updates can be high variance. Here, we introduce a model-based RL framework, Taylor TD, which reduces this variance in continuous state-action settings. Taylor TD uses a first-order Taylor series expansion of TD updates. This expansion allows Taylor TD to analytically integrate over stochasticity in the action-choice, and some stochasticity in the state distribution for the initial state and action of each TD update. We include theoretical and empirical evidence that Taylor TD updates are indeed lower variance than standard TD updates. Additionally, we show Taylor TD has the same stable learning guarantees as standard TD-learning with linear function approximation under a reasonable assumption. Next, we combine Taylor TD with the TD3 algorithm, forming TaTD3. We show TaTD3 performs as well, if not better, than several state-of-the art model-free and model-based baseline algorithms on a set of standard benchmark tasks.

## 1 Introduction

Actor-critic algorithms underlie many of the recent successes of deep RL [1; 2; 3; 4; 5; 6; 7; 8]. In these algorithms, the actor provides the control policy while the critic estimates the policy's expected long-term returns [i.e. the value function; 9; 10]. The critic is typically trained using some form of temporal-difference (TD) update [e.g. 3; 7; 1; 2; 6]. These TD updates need to be computed in expectation over a large distribution of visited states and actions, induced by the policy and the environment dynamics [11; 12]. Since this expectation is analytically intractable, TD updates are typically performed based on individually sampled state-action pairs from real environmental transitions (i.e. sample-based estimates). However, the variance of (sample-based) TD updates can be quite large, meaning that we need to average over many TD updates for different initial states and actions to get a good estimate of the expected updates .

Model-based strategies provide a promising candidate to tackle this high variance . For instance, Dyna methods, among the most popular model-based strategies, use a learned model of the environment transitions to generate additional imaginary transitions. These imaginary transitions can be used as extra training samples for TD methods [e.g. 15; 16; 17; 18; 19; 20]. Although the additional (imaginary) transitions help in reducing the variance of performing TD-updates over a

[MISSING_PAGE_FAIL:2]

equal either the target \(^{}\) or behavioural policy \(^{}\) (i.e., the policy used to interact with the environment) (e.g., [19; 18], this does not have to be the case. For instance, \(^{}\) could be a broader distribution than \(^{}\), enabling us to update and improve the action value function for a broader range of actions.

## 3 Taylor TD

The expected update over actions in Eq. (1) is usually intractable, due to the complexity of the function, \(_{}(,)_{}Q_{}(, )\). Standard TD-learning methods therefore employ a sample-based approach, by sampling initial actions from some policy (distribution) \(^{}\) and performing the (TD) updates for those actions. However, sampling can give rise to high-variance estimators, and hence slow learning . Taylor TD reduces this variance by introducing an analytic approximation to the expectation in Eq. (1) over some stochasticity in the distribution over actions (and over the state, i.e., Sec. 3.2). This approach requires a differentiable model of the transitions and rewards as well as continuous state and action space.

Taylor TD takes advantage of the flexibility in choice of the initial, target and behavioural policies arising in model-based off-policy settings (see Sec. 2). Taylor TD can use any target and behavioural policy, but assumes some structure in the initial policy, \(^{}\). Specifically, Taylor TD splits the process of sampling the initial action into two steps. It first samples the mean, \(_{}\), from a mean-policy, \(^{}()\) (usually, we would use the target policy, \(^{}\), as the mean-policy, but we do not have to). Then Taylor TD samples "noise" from a second distribution, \(^{}\), and the actual (initial) action is the sum of the mean and the noise,

\[_{}^{}() _{}^{}\] (4) \[=_{}+_{}.\] (5)

Here, we require that the noise, \(_{}\), drawn from \(^{}\) has zero expectation, and finite, known covariance, \(_{}\),

\[_{_{}}[_{}] = _{_{}}[_{}_ {}^{T}] =_{}.\] (6)

The overall initial policy, combining these two steps, can be written, \(^{}()\).

### Action expansion

Here, we analytically approximate the expectation over the action-noise, \(_{}\), using a first-order Taylor expansion. Specifically, we define the expected TD update, averaging over the action noise, \(_{}\), as,

\[_{}(,_{}) _{_{}}[(,_{}+_{})|,_{}]=_{_{ }}_{}(,_{}+_{ })_{}Q(,_{}+_{} ),_{}\] (7)

The (overall) expected TD update in Eq. 1 can now be written in terms of \(_{}(,_{})\),

\[_{,}[(,) ]=_{,_{},_{}}[ (,_{}+_{})]= _{,_{}}[_{}( ,_{})]\] (8)

The exact expectation for \(_{}(,_{})\) (Eq. 7) is also not tractable typically, due to the complexity of the function, \(_{}(,_{}+_{})_{ }Q(,_{}+_{})\). Instead, Taylor TD uses \(_{}(,_{})\) to approximate \(_{}(,_{})\) (Eq. 7),

\[_{}(,_{})_{}(,_{})\] (9)

Specifically, \(_{}(,_{})\), arises from a first-order Taylor expansion around \(_{}=\),

\[_{}(,_{})=_{_{ }}(_{}(,_{})+_{}^{T}_{}_{}(,_{}))_{}(Q_{}(,_{})+_{ }^{T}_{}Q_{}(,_{})) ,_{}\] (10)

This reduces to (see Appendix A.1),

\[_{}(,_{})=_{}( ,_{})_{}Q_{}(,_{ })+_{,}^{2}Q_{}(,_{})_{}_{}_{}(,_{}).\] (11)For simplicity, we further assume that the covariance of initial actions is isotropic, \(_{}=_{}}\) (although this is not a requirement), resulting in the following (1st-order) Taylor TD-update:

\[_{}(,_{})=_{}( ,_{})_{}Q_{}(,_ {})+_{}_{,}^{2}Q_{}( ,_{})_{}_{}(,_{})\] (12)

This Taylor TD-update includes the standard TD update at state \(\) and (mean) action \(_{}\) plus a new term, which tries to align the action gradient of the critic (Q-function) with the action gradient of the TD target. Conceptually, this gradient matching should help reduce the variance across TD-updates since it provides a way to analytically integrate over some of the stochasticity induced by \(^{}\). When doing a first order Taylor series approximation, we may worry that errors in the Taylor expansion might affect convergence. In the Appendix B, we include a proof that for linear function approximation, any errors in the first-order Taylor expansion do not affect the stability of TD-learning (assuming small simulator time steps). Importantly, this result is not a trivial, as there are still approximation errors in the mapping from state and action to the features that we use for linear function approximation. Moreover, we provide empirical evidence that the first-order Taylor expansion reduces the variance of standard TD-updates and support efficient learning, even under non-linear function approximation (see Sections, 3.4, and 4.2.3).

### State expansion

We are not limited to analytically approximate the expectation in Eq. (1) for the distribution of initial actions, but we can extend this approach to a distribution of initial states. Namely, instead of performing a TD-update at the single initial state, \(\), we perform this update over a distribution of states. Similarly to the action expansion, we assume structure in the distribution over the initial state distribution (denoted here as \(d^{}\)). Specifically, sampling initial states from the initial state distribution involves two steps. We first sample the mean of \(d^{}\), \(_{}\), from a state-mean distribution \(d^{}\) (usually, we would use the distribution of states in a reply buffer as the state-mean distribution, but we do not have to). Then we sample state-noise from a second distribution, \(^{}\), and the actual (initial) state is the sum of the state-mean and the state-noise,

\[_{} d^{} _{} d^{}\] (13) \[=_{}+_{}.\] (14)

Again, we require that the state-noise, \(_{}\), drawn from \(d^{}\) has zero expectation, and finite, known covariance,

\[_{_{}}[_{}]= _{_{}}[_{}_{ }^{T}]=_{}.\] (15)

Thus, we can again formulate an expected TD-update, averaging over the state-noise,

\[_{_{}}[(_{}+ _{},)|_{},]= _{_{}}[_{}(_{}+_{},)_{}Q_{}(_{}+ _{},)|_{},]\] (16)

Again, we can approximate this expected update with a first-order Taylor approximation, but this time, we expand the states around \(_{}=0\) instead of the actions. This requires that the states are continuous, which is usually the case for control tasks. Based on a similar derivation to the action expansion, we get the following Taylor TD-update, where we assumed the state covariance over TD updates to be isotropic, \(_{}=_{}}\) (although again this is not a requirement) (see Appendix A.2 for the full derivation):

\[_{}(_{},)=_{}( _{},)_{}Q_{}(_{},)+_{}_{,}^{2}Q_{}( _{},)_{}_{}(_{ },)\] (17)

The rational behind this update is trying to tackle some of the TD-update variance induced by the initial state distribution (e.g., the distribution of sates in a reply buffer), although we expect this only to work for states close-by to the visited ones (i.e. for small values of \(_{}\)).

### State-Action expansion implementation

Finally, we can combine the two Taylor expansions into a single TD-update involving both state and action expansions. Nevertheless, computing the dot products between \(_{}\) and \( Q_{}\) terms for both state and action terms may not be optimal. One reason for this is dot products are unbounded, increasing the risk of high variance (TD) updates (e.g. 25). To tackle this issue, we use cosine distances between the gradient terms instead of dot products (see Appendix K.2 for the benefits of this). The cosine distance has the advantage of being bounded. By putting everything together, we propose a novel TD update, which we express below in terms of a loss:

\[_{}^{}= (_{},_{})Q_{}(_ {},_{})\] (18) \[+ _{}(_{}Q_{ }(_{},_{}),\ _{}(_{},_{}))\] \[+ _{}(_{}Q_{ }(_{},_{}),\ _{}(_{},_{}))\]

Note we used the notation \(\) instead of \(_{}\) to indicate we are treating \((_{},_{})\) as a fixed variable independent of \(\). This ensures when we take the gradient of this loss relative to \(\), we do not differentiate through any \(\) terms (following the standard implementation of TD-updates in autodiff frameworks such as PyTorch, see Appendix D). In practice, Taylor TD requires a differentiable model of the environment transitions as well as reward function in order to compute the terms \(_{}_{}(_{},_{})\) and \(_{}_{}(_{},_{})\) (see Appendix C for further details).

In Sections 3.1 and 3.2, we have been careful to setup Taylor TD over actions and states in the most general possible setting. Nevertheless, a set-up that may work well with Taylor TD is following,

\[_{}=^{}() _{}\] (19)

where values for \(_{}\) (i.e., the mean-policy) are the outputs of a deterministic (target) policy \(^{}\) and \(_{}\) (i.e., the state-means) are set to states sampled from a reply buffer \(\). In this way, Taylor TD performs TD updates that analytically integrate the expected Q-value of the target policy (\(Q^{^{}}\)) over a distribution of actions centered at \(^{}\) and controlled by \(_{}\) as well as over distributions of states centered at previously visited states (i.e., from the reply buffer) and controlled by \(_{}\). We provide a Taylor TD algorithm implementation with this set-up in the Algorithm box 1. Note, in the Algorithm box 1, we left the model loss unspecified (i.e., \(^{}\)) to denote that any valid method can be used to learn the model of the transitions. However, in the experiments below, we always learned the transition model based on maximum likelihood on the observed environment transitions (see Appendix E).

### Variance analysis

Here, we show that the expected TD update in Eq. (7) (which Taylor TD estimates) has lower variance than standard (sample-based) TD-updates over the same distribution of actions. We only provide this variance analysis for the distribution over actions, because an analogous theorem can be derived for the distribution over states (i.e. Eq. 16). To begin, we apply the law of total variance  to standard TD-updates,

\[_{,}[(,)] =_{,_{}, {}_{}}[(,_{}+_{})]\] (20) \[=_{,_{}}[ _{_{}}[(, _{}+_{})|, _{}]]+_{, _{}}[_{_{}} [(,_{}+_ {})|,_{}]]\]

The inner expectation and inner variance samples action-noise, \(_{}\), while the outer expectation and outer variance samples initial states, \(\), and action-means, \(_{}\). To relate this expression to Taylor TD, recall that Taylor TD updates are motivated as performing analytic integration over action-noise, \(_{}\) - i.e. \(_{}(,_{})=_{_{}}[|,_{}]\). Thus, the variance of standard (sample-based) TD-updates (Eq. 20) is exactly the variance of \(_{}\), plus an additional term to account for variance induced by sampling action-noise,

\[_{,}[(,) ]=_{,_{}}[_{_{}}[(, _{}+_{})|, _{}]]+_{, _{}}[_{}(, _{})]\] (21)

The fact that \(_{,_{}}[_{ _{}}[|,_{ }]]\) is non-negative directly gives a theorem.

**Theorem 3.1**.: _The variance for standard (sample-based) TD-updates, \(_{,_{},_{ }}[]\), is larger than (or equal to) the variance if we exactly integrate action-noise, \(_{,_{}}[_{}]\),_

\[_{,_{},_{ }}[(,)]_ {,_{}}[_{}( ,_{})]\] (22)

While this theorem only formally applies to the exact expectation, \(_{}\), we would expect it to become increasingly accurate as \(_{}\) and \(_{}\) become smaller, and hence the Taylor series approximations become more accurate. Furthermore, in Appendix B we prove that the resulting updates do not affect the stability of (standard) TD-learning even when the Taylor series approximation errors do not vanish (under linear function approximation). Additionally, we show empirical reductions in the update variance for Taylor TD in Sec. 4.1.

## 4 Experiments

### Variance reduction

In this section we empirically test the claim that Taylor TD updates are lower variance than standard (sample-based) TD-learning updates. To do so, we take the Taylor TD set-up in Algorithm 1 and perform "batch updates" , where given an approximate value function \(Q_{}\) and a policy \(\), several \(Q_{}\) updates are computed across several sampled states and actions, updating \(Q_{}\) only once, based on the sum of all updates. Batch updates ensure the variance of the updates is estimated based on the same underlying value function. We compute batch updates for both Taylor TD and standard (sample-based) TD updates, comparing the variance of the updates between the two approaches (see Appendix F for more details). Formally, we aim to (empirically) show that the inequality in Theorem 3.1 (for both states and actions) holds even with the Taylor approximation across a range of standard control tasks,

\[_{_{},_{}, _{},_{}}[ (,)]_{_{}, _{}}[_{}(_{ },_{})]_{} (_{},_{})\] (23)

Fig. (1) shows Taylor TD provides reliably lower variance updates compared to standard TD-learning across all tested tasks, but the Pendulum environment. This finding is consistent with the idea that Taylor TD updates may be most beneficial in higher dimensional state-action spaces, while the Pendulum environment represents the lowest dimensional environment with only four dimensions. This is because sampling estimates (i.e., standard TD-learning) may become less efficient in higher dimensional spaces. We further explore this proposal in a toy example comparing sample-based estimates and Taylor expansions of expected updates. We perform this comparison across data points of different dimensions, and find the benefits of the Taylor expansion (over purely sample-based estimate) to increase with the dimension of the data points (see Appendix H). Additionally, in the Appendix G, we investigate how the variance of Taylor TD and standard TD-learning updates changes during training.

### Benchmark performance

#### 4.2.1 Algorithm

We combine Taylor TD (i.e. Algorithm 1) with the TD3 algorithm  in a model-based off-policy algorithm we call Taylor TD3 (TaTD3) (code available at Appendix I). TaTD3 aims to provide a state-of-the-art implementation of Taylor TD for comparison with baseline algorithms. At each iteration, TaTD3 uses a learned model of the transitions and learned reward function to generate several differentiable (imaginary) 1-step transitions (i.e., Dyna style), starting from real states sampled from a reply buffer. These differentiable 1-step transitions are then used to train two critics (i.e. TD3) using Taylor TD updates. The model of the transitions consists of an ensemble of 8 models trained by maximum likelihood on the observed environment transitions (see Appendix E). These models return a Gaussian distribution over the next state, with zero correlations, and with mean and variance given by applying a neural network to the previous state-action pair. We also learn a model the rewards using a neural network trained with mean-square error on the observed rewards. Hence, TaTD3 does not require any a priori knowledge of the true environment transitions or true reward function. Finally, the actor is trained with the deterministic policy gradient  on real states as in standard TD3 .

#### 4.2.2 Environments

We employ 6 standard environments for continuous control. The first environment consists of a classic problem in control theory used to evaluate RL algorithms [i.e. Pendulum, 27]. The other 5 environments are stanard MuJoCo continous control tasks [i.e. Hopper, HalfCheetah, Walker2d, Ant and Humanoid, 28]. All results are reported in terms of average performance across 5 runs, each with a different random seed (shade represents 95% CI).

#### 4.2.3 Comparison with baselines

Here, we report the comparison of TaTD3 with some state-of-the art model -free and -based baselines on the six benchmark environments. These baselines include 3 model-based algorithms and one 1 model-free algorithm (see Appendix J). The first model-based algorithm is Model-based Policy Optimization (MBPO) , which employs the soft actor-critic algorithm (SAC)  within a model-based Dyna setting. The second model-based algorithm is Model-based Action-Gradient-Estimator Policy Optimization (MAGE) , which uses a differentiable model of the environment transitions to train the critic by minimising the norm of the action-gradient of the TD-error. The third model-based algorithm is TD3 combined with a model-based Dyna approach (i.e. Dyna-TD3). This algorithm was proposed by  and was shown to outperform its model-free counterpart, TD3  on most benchmark tasks. Dyna-TD3 is conceptually similar to MBPO, with the main difference of MBPO relying on SAC instead of TD3. Finally, we included SAC  as a model-free baseline. Fig. (2) shows TaTD3 performs at least as well, if not better, than the baseline algorithms in all six benchmark tasks: note the much poorer performance of MAGE on Hopper-v2, Walker2d-v2 and Ant-v2, as well as of MBPO on Humanoid-v2 relative to TaTD3.

Figure 1: Mean update variance (and standard error) between Taylor TD and standard (sample-based) TD-learning (batch) updates, based on several sampled states and the distribution of actions for those states (i.e. the policy). All results are based on 10 runs with corresponding error bars.

#### 4.2.4 Taylor vs sample-based (standard) TD-learning

Next, we ask whether Taylor TD provides any performance benefit in computing the expected TD updates in Eq. (7) and (16) over standard sample-based estimates. To do so, we implement a model-based TD3 algorithm analogous to TaTD3, but where the expected TD updates, Eq. (7) and (16), are estimated using sample-based estimates by drawing multiple samples for the state (\(_{s}\)) and action (\(_{a}\)) noise at each step, instead of using an analytic Taylor series approximation. We call this algorithm Sample-based Expected-TD3 (code available at Appendix I). In practice, at each time step, Sample-based Expected-TD3 uses a (learned) model of the transitions to compute multiple TD-updates by sampling ten state perturbations (\(_{s}\)) of visited states (\(_{s}\)) and ten action perturbations (\(_{a}\)) of the deterministic target policy (\(_{a}\))(i.e. estimating Eq. (7) and (16) through a sample-based estimate). Crucially, we ensure the variance of the state and action perturbations

Figure 3: Performance comparison of TaTD3 with its sample-based equivalent, Sample-based Expected-TD3. All performance are based on 5 runs, with shade representing 95% c.i.

Figure 2: Performance in terms of average returns for TaTD3 and four state-of-the-art baseline algorithms on six benchmark continuous control tasks. TaTD3 performs as well, if not better, than the four baseline algorithms on all four tasks. All performance are based on 5 runs, with shade representing 95% c.i.

(i.e. \(_{}\) and \(_{}\)) is matched between TaTD3 and Sample-based Expected-TD3. We perform this comparison across 4 standard continuous control tasks, spanning a low dimensional (Pendulum), two moderate dimensional (Half-Cheetah and Walker2d) and one high dimensional environment (Humanoid).

In Fig. (3), we can see TaTD3 provides performance benefits over Sample-based Expected-TD3 across the three most high dimensional environments. This finding is in line with the claim we put forth in the variance reduction section (i.e., 4.1) that the benefits of Taylor TD (i.e. TaTD3) over sampling may be most marked in high dimensional state-action spaces. Indeed, the largest performance advantage of TaTD3 is seen in Humanoid-v2, which has the highest dimensional state-action space by a large margin. Conversely, the least difference between TaTD3 and Sample-based Expected-TD3 is seen in Pendulum-v1, which is the task with smallest dimensional state-action space. The relation between the variance of TD-updates and the dimension of the state-action space should be better explored in the future (see also Appendix H). Finally, we should stress Sample-based Expected-TD3 is different from Dyna-TD3, as the latter does not implement any action or state perturbation in the TD-updates. Hence, unlike Sample-based Expected-TD3, Dyna-TD3 does not directly estimate the expected updates in Eq. (7) and (16), but relies on (standard) TD-learning updates (this is also evident in the massive performance difference between Dyna-TD3 and Sample-based Expected-TD3 - by comparing corresponding performance between Fig. 2 and 3).

## 5 Related work

Model-based strategies provide a promising solution to improving the sample complexity of RL algorithms . In Dyna methods, a model of the environment transitions is learned through interactions with the environment and then employed to generate additional imaginary transitions [e.g. in the form of model roll-outs, 15]. These imaginary transitions, can be used to enhance existing model-free algorithms, leading to improved sample complexity. For example, within TD-learning, imaginary transitions can be used to train a Q-function by providing additional training examples [e.g. 15, 16, 19]. Alternatively, imaginary transitions can be used to provide better TD targets for existing data points [e.g. 17] or to train the actor and/or critic by generating short-horizon trajectories starting at existing state-action pairs [e.g. 18, 29, 20]. These (Dyna) approaches have a clear relation to our approach (Taylor TD), as they attempt to estimate a similar expected TD-update as in Eq. (7). However, Dyna approaches only use potentially high-variance sample-based estimates, while Taylor TD exploits analytic results to reduce that variance.

Conceptually, our approach may resemble previous methods that also rely on analytical computations of expected updates to achieve lower-variance critic or policy updates [e.g. 24, 30, 31, 12, 32, 33, 34] [see also 35, 36, 37, 38, for a different set of approaches relating to update variance in RL]. The most well-known example of this is Expected-SARSA. Expected-SARSA achieves a lower variance TD-update (relative to SARSA), by analytically computing the expectation over the distribution of target actions in the TD-update (i.e. assuming a stochastic target policy) ;

\[_{}(,)=r(,)+\, _{^{}}[Q_{}(^{}, ^{})]-Q_{}(,)\] (24)

This approach can only reduce variance of TD-updates at the level of the target actions, \(^{}\), induced by a stochastic target policy. In the case of a deterministic target policy, Expected-SARSA does not provide any benefit. Conversely, our approach attempts to reduce the variance at the level of the initial state-action pairs, \((,)\) at which TD-updates are performed. That is; we take the expectation over \((,)\) instead of \(^{}\) (see Eq. 7 and 16) which should yield benefits with both stochastic and deterministic target policies. Other well-known RL approaches exploiting analytical computations of expected updates are Expected Policy Gradients , Mean Actor Critic  and "all-action" policy gradient . These methods attempt to reduce the variance of the stochastic policy gradient update by integrating over the action distribution. Although similar in principle to our approach, these methods focus on the policy update instead of the critic update and, similarly to Expected-SARSA only apply to stochastic target policies.

In practice, our approach may relate to value gradient methods, as it explicitly incorporates the gradient of the value function into the update [e.g. 39, 29, 40, 19, 41, 42]. To our knowledge, the value gradient approach that most relates to our work is MAGE , which, nonetheless, has a radically different motivation from Taylor TD. MAGE is motivated by noting that the action-gradients of Q drive deterministic policy updates , so getting the action-gradients right is critical for policy learning. In order to encourage the action-gradients of Q to be correct, MAGE explicitly adds a term to the objective that consists of the norm of the action-gradient of the TD-error, which takes it outside of the standard TD-framework. In contrast, our motivation is to reduce the gradient variance across standard TD updates by performing some analytic integration. We do this through a first-order Taylor expansion of the TD update. This difference in motivation leads to numerous differences in the method and analysis, the least of which is that MAGE uses only the action-gradients, while Taylor TD additionally suggests using the state-gradients, as both the state and action gradients can be used to reduce the variance in the (TD) updates.

The idea of applying a Taylor expansion to learning updates is not new to RL. For instance,  uses a (model-free) Taylor expansion of the policy update to provide a formalism connecting trust-region policy search with off-policy correction. This is very different from our approach (Taylor TD), not only because Taylor TD is model-based, but also because we apply the Taylor expansion to the critic instead of the actor update for very different reasons (i.e., reduce the update variance across TD updates). Finally, Taylor TD may be applicable to risk-sensitive RL [e.g., 44, 45, 46, 47]. In the presence of a good model of the transitions, Taylor TD may be able to approximate the values of risky actions (or states) around safe (e.g. deterministic) actions (or visited states), without actually needing to take those actions (or visit those states), thanks to the Taylor expansion of the TD objective.

## 6 Conclusion and Limitations

In this article, we introduce a model-based RL framework, Taylor TD, to help reduce the variance of standard TD-learning updates and, speed-up learning of critics. We theoretically and empirically show Taylor TD updates are lower variance than standard (sample-based) TD-learning updates. We show the extra gradient terms used by Taylor TD do not affect the stable learning guarantees of TD-learning with linear function approximation under a reasonable assumption. Next, we combine Taylor-TD with the TD3 algorithm  into a model-based off-policy algorithm we denote as TaTD3. We show TaTD3 performs as well, if not better, than several state-of-the art model-free and model-based baseline algorithms on a set of standard benchmark tasks.

Taylor TD has the limitation that it requires continous state-action spaces as well as a differentiable model of transitions to calculate the additional (TD) gradient terms, i.e. it must be in the model-based rather than model-free setting (see Appendix C). Additionally, the gradient terms in Taylor TD imply additional computational cost; in the Appendix L, we show this cost is not that large in terms of training times and we expect it to reduce as faster automatic differentiation tools are developed .

## 7 Acknowledgement

We would like to thank the Wellcome Trust for funding this work as well as Dr. Steward for supporting the purchase of GPU nodes. This work made use of the HPC system Blue Pebble at the University of Bristol, UK.