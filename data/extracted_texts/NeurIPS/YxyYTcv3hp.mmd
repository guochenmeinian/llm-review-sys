# Ferrari: Federated Feature Unlearning via

Optimizing Feature Sensitivity

Hanlin Gu

equal contribution; authors are listed alphabetically by first name. CISiPi, Universiti Malaya, Malaysia

Win Kent Ong

cise.s.chan@um.edu.my

Chee Seng Chan

cise.seng.chan@um.edu.my CISiPi, Universiti Malaya, Malaysia

Lixin Fan

AI Lab, Webank, PR China

###### Abstract

The advent of Federated Learning (FL) highlights the practical necessity for the _'right to be forgotten'_ for all clients, allowing them to request data deletion from the machine learning model's service provider. This necessity has spurred a growing demand for Federated Unlearning (FU). Feature unlearning has gained considerable attention due to its applications in unlearning sensitive, backdoor, and biased features. Existing methods employ the influence function to achieve feature unlearning, which is impractical for FL as it necessitates the participation of other clients, _if not all_, in the unlearning process. Furthermore, current research lacks an evaluation of the effectiveness of feature unlearning. To address these limitations, we define feature sensitivity in evaluating feature unlearning according to Lipschitz continuity. This metric characterizes the model output's rate of change or sensitivity to perturbations in the input feature. We then propose an effective federated feature unlearning framework called Ferrari, which minimizes feature sensitivity. Extensive experimental results and theoretical analysis demonstrate the effectiveness of Ferrari across various feature unlearning scenarios, including sensitive, backdoor, and biased features. The code is publicly available at https://github.com/OngWinKent/Federated-Feature-Unlearning

## 1 Introduction

Federated Learning (FL)  allows for model training across decentralized devices or servers holding local private data samples, without the need to exchange them directly. An essential requirement within FL is the participants _"right to be forgotten"_, as explicitly outlined in regulations such as the European Union General Data Protection Regulation (GDPR)3 and the California Consumer Privacy Act (CCPA)4. To address this requirement, Federated Unlearning (FU) has been introduced, enabling clients to selectively remove the influence of specific subsets of their data from a trained FL model while preserving the model's accuracy on the remaining data .

Different from unlearning at the _client, class, or sample_ level  in FL, the feature unlearning  holds significant applications across various scenarios. Firstly, in contexts where sentences contain sensitive information such as names and addresses , it becomes crucial to remove these sensitive components to prevent potential exposure through model inversion attacks . Secondly, when datasets contain backdoor triggers that can compromise model integrity , it is imperative to eliminate these patterns. Thirdly, unlearning biased features becomes essential in scenarios where data imbalances significantly impact model accuracy due to bias . However,existing works of FU focus on client, class, or sample unlearning [6; 7; 8] but do not address feature unlearning, limiting their ability to unlearn specific features across multiple data points.

There are two challenges in feature unlearning in FL. Firstly, evaluating the unlearning effectiveness for feature unlearning is difficult. Typically, unlearning effectiveness is assessed by comparing the unlearned model with a retrained model without the feature. However, building data without the feature is challenging; for example, training the data with noise or a black block on the feature region may cause severe degradation in model accuracy (see Sec. 3.2). Secondly, previous work on feature unlearning within centralized machine learning settings [9; 10] is not practical for FL due to its requirement for access to all datasets, necessitating the participation of all clients.

To address the aforementioned limitations, we first define the feature sensitivity in Sec. 4.1 to evaluate the feature unlearning inspired by the Lipschitz continuity, which characterizes the rate of change or sensitivity of the model output to perturbations in the input feature. Then we propose a simple but effective federated feature unlearning method, called Ferrari (**F**ederated Feature **U**l**ear**ning), by minimizing the feature sensitivity in Sec. 4.2. Our Ferrari framework offers three key advantages: Firstly, Ferrari requires only local datasets from the unlearned clients for feature unlearning. Secondly, Ferrari demonstrates high practicality and efficiency, which support various feature unlearning scenarios, including sensitive, backdoor, and biased features and only consumes a few epochs of optimization. Thirdly, theoretical analysis in Sec. 4.3 elucidates that our proposed Ferrari achieves lower model utility loss compared to the exact feature unlearning.

The key contributions of this work are summarized as follows:

* We identify two key challenges for feature unlearning in FL. The first is how to successfully unlearn features without requiring the participation of other clients, as discussed in Sec. 3.2. The second is how to design an effective evaluation method in federated feature unlearning.
* We define the feature sensitivity and introduce this metric in federated feature unlearning in Sec. 4. By minimizing feature sensitivity, we propose an effective federated feature unlearning method, named Ferrari, which enables clients to selectively unlearn specific features from the trained global model without requiring the participation of other clients.
* We provide a theoretical proof in Theorem 1, which dictates that _Ferrari achieves better model performances than exact feature unlearning_. This analytical result is also echoed in the empirical evidence, highlighting Ferrari's effectiveness across various settings, including the unlearning of sensitive, backdoor, and biased features.

## 2 Related Work

Machine UnlearningMachine Unlearning (MU), introduced by Cao et al. , involves selectively removing specific training data from a trained model without retraining from scratch[24; 25]. It categorizes into exact unlearning [26; 27], aiming to completely remove data influence with techniques like SISA  and ARCANE , though with computational costs, and approximate unlearning [30; 31], which reduces data impact through techniques like data manipulation (fine-tuning with mislabeled data [32; 33; 34; 35; 36] or introducing noise [37; 38; 39]), knowledge distillation [40; 41; 42; 43] (training a student model), gradient ascent [44; 45; 46; 47] (maximizing loss associated with forgotten data), and weight scrubbing [48; 49; 50; 51; 52; 53] (discarding heavily influenced weights).

Federated UnlearningIn FL, traditional centralized MU methods are unsuitable due to inherent differences like incremental learning and limited dataset access . Research on Federated Unlearning (FU) mainly focuses on client, class, and sample unlearning [6; 7; 8]. Client unlearning, pioneered by Liu et al.  introducing FedEraser , includes approaches like FRU , FedRecover , VeriFI , HDUS , KNOT , FedRecovery , Knowledge Distillation , and Gradient Ascent [62; 63; 64], aiming to remove specific clients or recover poisoned global models. Class unlearning, introduced by Wang et al. , involves frameworks like discriminative pruning and Momentum Degradation  (MoDE) to remove entire data classes. Sample unlearning, initiated by Liu et al. , targets individual sample removal within FL settings, with advancements like the QuickDrop  framework and FedFilter  enhancing efficiency and effectiveness. Recent works, such as \(FedMe^{2}\) by Xia et al. , optimize both unlearning facilitation and privacy guarantees.

Existing literature on FU primarily focuses on client, class, or sample unlearning [6; 7; 8]. However, a significant gap arises when a client seeks to remove only sensitive features while remaining engaged in FL. Unfortunately, current FU approaches do not address this specific scenario, as they do not explore feature unlearning within FL settings. In contrast to prior works focusing on feature unlearning in centralized settings of MU, such as classification models [9; 10], generative models [71; 72; 73; 74], and large language models [75; 76; 77], this study uniquely addresses feature unlearning of classification model within the FL paradigm. This distinction arises because traditional feature unlearning methods in centralized settings of MU are impractical for FL scenarios, where participation from all clients is often infeasible. In such cases, the process fails if even a single client opts out of the operation.

Therefore, to fill this critical gap, we proposed a novel federated feature unlearning framework, namely Ferrari based on the concept of Lipschitz continuity [78; 79; 80]. Our proposed Ferrari requires exclusively from the target client's dataset while still preserving the model's original performance. Lipschitz continuity, a fundamental mathematical concept that measures a function's sensitivity to changes in its input variables [81; 82; 83], is central to our feature unlearning approach. For a detailed exposition of our proposed federated feature unlearning framework utilizing Lipschitz continuity, please refer to Sec. 4. To the best of our knowledge, this is the **first work** in feature unlearning within FL settings that does not necessitate participation from all other clients, showcasing the potential to enhance privacy, practicality and efficiency.

## 3 Challenges on Feature Unlearning in FL

### Federated Feature Unlearning

Consider a federated system comprising \(K\) clients and one server, collaboratively learning a global model \(f_{}\) as:

\[_{}_{k=1}^{K}_{i=1}^{n_{k}}(x_{k,i}),y_ {k,i})}{n_{1}++n_{K}},\] (1)

where \(\) is the loss, _e.g.,_ the cross-entropy loss, \(_{k}=\{(x_{k,i},y_{k,i})\}_{i=1}^{n_{k}}\) is the dataset with size \(n_{k}\) owned by client \(k\). One client (_i.e.,_ referred to as the unlearn client \(C_{u}\)) requests the removal of a feature \(\) from the global model \(\) such that \(\) does not retain any information about \(\). Specifically, we assume that the data \(x^{d}\) and denote the j-th feature of \(x\) by \(x[j]\). The partial element of the data \(x\) corresponding the feature \(\) is defined as \(x[]\), _i.e.,_:

\[x[]=\{x[j],j\}\] (2)

Therefore, the unlearn client \(C_{u}\) aims to remove \(\{x_{i,u}[]\}_{i=1}^{n_{u}}\), called unlearned data \(_{u}\). Denote \(_{r}=-_{u}\) to be the remaining data.

### Challenges for Feature Unlearning in FL

Unlike sample or class unlearning [6; 7; 8], evaluating the unlearning effectiveness for feature unlearning is difficult. Typically, unlearning effectiveness is assessed by comparing the unlearned model with a retrained model trained on remaining data \(_{r}\). However, building \(_{r}\) for the feature unlearning takes much work. For example, suppose we want to remove the mouth from a face image. In that case, one possible solution is to replace the mouth region with Gaussian noise or black block, as illustrated in Fig. 1. However, this added Gaussian noise or black block can adversely affect model training and degrade performance, _e.g.,_ the degradation of model accuracy is beyond 27%.

Another challenge is implementing feature unlearning for \(C_{u}\) without the help of other clients. Previous work on feature unlearning [9; 10] typically requires access to the remaining data, necessitating the participation of other clients in the FL process. This requirement is impractical in the FL context, as other clients may be unwilling or unable to share data or computational resources. Therefore, finding a method to effectively unlearn features without relying on other clients is crucial to maintain the model accuracy and practicality in the FL settings.

Figure 1: Sample data \(x\) with Gaussian noise (\(_{G}\)) and black pixels (\(_{B}\)) perturbations, illustrating feature removal and performance comparison.

## 4 The Proposed Method

In this section, we introduce feature sensitivity (see Def. 1) in Sec. 4.1 to evaluate the effectiveness of feature unlearning. We then propose Ferrari based on this concept in Sec. 4.2). Finally, we demonstrate that Ferrari achieves a lower utility loss compared to exact feature unlearning in Sec. 4.3).

### Feature Sensitivity

Inspired by Lipschitz Continuity [79; 80; 82], which provides an approximate method for removing information from images by perturbing the input data and observing the effect on the output, we introduce the concept of **feature sensitivity**\(s\) as Def. 1. This metric measures the memorization of a model \(f_{}\) for the feature \(\) by considering the local changes in the given input rather than the global change as defined in the traditional Lipschitz continuity.

**Definition 1**.: _The feature sensitivity \(s\) of the model \(f\) with respect to the feature \(\) on the data \((x,y)\) is defined as:_

\[s=_{_{}}})\|_{ 2}}{\|_{}\|_{2}},\] (3)

_where \(_{}\) denote the perturbation on feature \(\)._

Def. 1 characterizes the rate of change or sensitivity of the model output to perturbations in the input data. A small feature sensitivity \(s\) represents the model \(f\) doesn't memorize the feature \(\). This definition does not require building the remaining data, as it considers the expectation over the perturbation \(_{}\). Specifically, it represents the average output change rate over any magnitude of the perturbation. Furthermore, we will provide the relationship between Def. 1 and exact feature unlearning in Sec. 4.3.

**Remark 1**.: _The perturbation \(_{}\) can be chosen from various distributions, such as the Gaussian distribution, the uniform distribution, and so on._

### Ferrari

As discussed the feature sensitivity \(s\) in Sec. 4.1, the core idea of the proposed method Ferrari is to achieve the feature unlearning by minimizing the feature sensitivity. More specifically, it controls the change in the model's output relative to changes in the input within the feature region, _i.e.,_ the slope, to prevent the model from memorizing the feature as illustrated in Fig. 2.

Figure 2: Overview of our proposed Ferrari framework: Initiated by the feature unlearning request from the unlearn client \(C_{u}\), the server initializes the trained global model \(\) to \(C_{u}\) for local feature unlearning. Upon completion, \(C_{u}\) uploads the unlearned model \(^{u}\) to the server. Local feature unlearning minimizes the Lipschitz constant \(L\) between the original input and its perturbed feature subset, reducing feature sensitivity yet preserving the overall model performance.

One unlearning client \(C_{u}\) requests to unlearning the feature \(\). The proposed Ferrari aims to unlearn the global model \(\) to \(^{u}\). The proposed method can be divided into three steps (see details in Alg. 1). In order to compute the feature sensitivity, the perturbation \(_{}\) in terms of the feature \(\) is **firstly** computed as the following (take the Gaussian distribution as an example):

\[_{}[j]=\{ & N(0,^{2})& j \\ & 0&.\] (4)

**Secondly,** we leverage a finite sample Monte Carlo approximation to the maximization as Def. 1 as:

\[_{_{}}(x)-f_{}(x+_{ })\|_{2}}{\|_{}\|_{2}}_{i=1}^{ N}(x)-f_{}(x+_{,i})\|_{2}}{\| _{,i}\|_{2}},\] (5)

where \(_{,i}\) is \(i_{th}\) sampling as Eq. (4).

**Finally,** for the unlearning client \(C_{u}\) who aims to remove the feature \(\) from his/her data \(_{u}\), the unlearned model \(^{u}\) is obtained as the following:

\[^{u}=*{arg\,min}_{}_{(x,y)_{ u}}_{i=1}^{N}(x)-f_{}(x+_{ ,i})\|_{2}}{\|_{,i}\|_{2}},\] (6)

where Eq. (6) is computed over the dataset \(_{u}\). Noted that the proposed Ferrari based on Def. 1 doesn't need the participation of other clients.

**Remark 2**.: _When the unlearning happens during the federated training, the unlearning clients would also optimize the training loss and feature sensitivity simultaneously, i.e., \(_{(x,y)}(f_{}(x),y)+_{_{}}(x)-f_{}(x+_{ })\|_{2}}{\|_{}\|_{2}},\) where \(\) is a coefficient._

### Theoretical Analysis of the Utility loss for Ferrari

As illustrated in Sec. 3.2, retraining the model without the feature may affect the model accuracy seriously. Suppose the feature is successfully removed when the norm of perturbation is larger than \(C\). We firstly define the utility loss \(_{1}\) with unlearning feature directly, _i.e.,_ **the exact feature unlearning**:

\[_{1}=_{\|_{}\| C}_{(x,y) }_{}f_{}(x+_{}),y\] (7)

And we define the maximum utility loss with the norm perturbation lower than \(C\) as:

\[_{2}=_{\|_{}\| C}_{(x,y) }_{}f_{}(x+_{}),y\] (8)

**Assumption 1**.: _Assume \(_{2}_{1}\)_

Assumption 1 elucidates that the utility loss associated with a perturbation norm lower than \(C\) is smaller than the utility loss when the perturbation norm is greater than \(C\). This assumption is logical, as larger perturbations would naturally lead to a greater utility loss.

**Assumption 2**.: _Suppose the federated model achieves zero training loss._

We have the following theorem to elucidate the relation between feature sensitivity removing via Alg. 1 and exact unlearning (see proof in Appendix A.1, including the extension for the non-zero training loss assumption).

**Theorem 1**.: _If Assumptions 1 and 2 hold, the utility loss of unlearned model obtained using Alg. 1 is lower than the utility loss with exact feature unlearning, i.e.,,_

\[_{u}_{1},\] (9)

_where \(_{u}=_{(x,y)}(f_{^{u}}(x),y)\)_

Theorem 1 showcases that the proposed method Ferrari, results in a utility loss (\(_{u}\)) that is lower than the utility loss incurred when the feature is removed, and the model is retrained, _i.e.,_ the process of exact feature unlearning.

**Remark 3**.: _To further evaluate the effectiveness of feature unlearning based on feature sensitivity, we employ model inversion attacks [11; 12] to determine if the feature can be reconstructed and employ attention maps to assess if the model still focuses on the unlearned feature, as described in Sec. 5.3.1._

## 5 Experimental Results

This section presents the empirical analysis of the proposed Ferrari framework in terms of effectiveness, utility, and time efficiency in sensitive, backdoor and biased feature unlearning scenarios.

### Experimental Setup

Unlearning Scenarios_Sensitive Feature Unlearning_: We simulate the removal of sensitive features from the \(_{u}\) to fulfill the request of \(C_{u}\) due to privacy concern. Specifically, we remove'mouth' from CelebA ,'marital status' from Adult , and 'pregnancies number' from Diabetes . Therefore, our proposed Ferrari aims to remove the influence of these requested features.

_Backdoor Feature Unlearning_: We simulate a pixel-pattern backdoor attack by \(C_{u}\) based on BadNets  within a FL framework . \(C_{u}\) injects a pixel-pattern backdoor feature and trigger label into its \(_{u}\) during training, as shown in Fig. 4. Consequently, our proposed Ferrari aims to remove the influence of these backdoor features and restore the model's original performance.

_Biased Feature Unlearning_: We simulate the bias dataset \(_{u}\) of the \(C_{u}\) and the unbias dataset \(_{r}\) with a bias ratio of 0.8, as shown in Fig. 4. This results in a global model biased towards the biased dataset  due to unintended feature memorization . In CMNIST , the model focuses on color patterns instead of digits, and in CelebA , it learns mouth features instead of facial features for gender classification. Therefore, our proposed Ferrari aims to mitigate these bias-inducing features and restore model performance.

Hyperparameters & Datasets & ModelWe simulate HFL with \(K=10\) clients under an IID setting, each holding \(10\%\) of the datasets, except for the biased feature unlearning experiment with a bias ratio of 0.8. For federated feature unlearning experiments, we set hyperparameters: learning rate \(=0.0001\), sample size \(N=20\), and random Gaussian noise with standard deviation ranging from \(0.05 1.0\) (see Sec. 5.5) across iterations of \(N\). Experiments are repeated over five random trials, and results are reported as mean and standard deviation. We employ ResNet18  on image datasets: MNIST , Colored-MNIST (CMNIST) , Fashion-MNIST , CIFAR-10, CIFAR-20, CIFAR-100  and ImageNet . For tabular datasets, such as Adult Census Income (Adult)  and Diabetes , we used a fully-connected neural network linear model. Additionally, we utilize the transformer-based BERT model  for the text dataset, specifically the IMDB movie reviews dataset . We conduct experiments on a single NVIDIA A100 GPU. Further details are in Appendix A.2.

Evaluation MetricsWe assess effectiveness by measuring feature sensitivity (see Section 4.1) and conducting a model inversion attack (MIA)  to determine the attack success rate (ASR). The goal is to achieve low feature sensitivity and ASR, indicating successful unlearning sensitive features. Backdoor and biased feature unlearning are evaluated by comparing accuracy on the retain dataset \(_{r}\) (\(Acc_{r}\)) and the unlearn client dataset \(_{u}(Acc_{u})\). Low \(Acc_{u}\) indicates high effectiveness for backdoor unlearning, while similar accuracy (\(Acc_{r} Acc_{u}\)) reflects fairness and effectiveness in biased feature unlearning. Qualitatively, effectiveness is assessed using MIA-reconstructed images (sensitive) and GradCAM  attention maps (backdoor and biased). The utility is measured by test dataset \(_{t}\) accuracy (\(Acc_{t}\)), with higher values indicating stronger utility. Time efficiency is evaluated by comparing the runtime of each baseline.

[MISSING_PAGE_FAIL:7]

#### 5.3.2 Backdoor Feature Unlearning

Accuracy\(_{r}\) and \(_{u}\) represent the clean and backdoor datasets, respectively. Successful unlearning is shown by low \(Acc_{u}\) and high \(Acc_{r}\), indicating effective unlearning and preserved model utility. As shown in Tab. 4, the Fine-tune method has higher \(Acc_{r}\) and utility than the Retrain method but lower unlearning effectiveness due to high \(Acc_{u}\). FedCDP  and FedRecovery  show low utility and unlearning effectiveness with low \(Acc_{r}\) and \(Acc_{u}\), rendering them unsuitable for backdoor feature unlearning. In contrast, Ferrari demonstrates the highest utility and unlearning effectiveness.

Attention MapFig. 5(a) illustrates attention maps analyzing backdoor feature unlearning. Initially, the Baseline model focuses on the \(5 5\) square at the top-left corner, indicating a significant influence on output prediction by the pixel-pattern backdoor feature. In contrast, Ferrari unlearned models shift the attention towards recognizable objects like digits and cars, similar to the Retrain model. This shift suggests a reduced sensitivity to the backdoor feature, indicating a successful unlearning. See Appendix A.3.1 for supplementary results.

#### 5.3.3 Biased Feature Unlearning

Accuracy\(_{r}\) and \(_{u}\) represent the unbias and bias datasets, respectively. Successful unlearning results in similar accuracies across both datasets (\(Acc_{r} Acc_{u}\)), ensuring fairness while maintaining high \(Acc_{r}\) and \(Acc_{u}\) for utility. Tab. 4 shows that the Fine-tune method fails to unlearn bias, as \(Acc_{u}\) remains higher than \(Acc_{r}\), despite slightly higher \(Acc_{r}\) compared to Retrain. FedCDP  and FedRecovery  exhibit catastrophic forgetting, with low \(Acc_{r}\) and \(Acc_{u}\), making them unsuitable for biased feature unlearning. In contrast, Ferrari demonstrates effective unlearning with similar \(Acc_{r}\) and \(Acc_{u}\), and maintains high overall accuracy, indicating a successful biased feature unlearning.

Attention MapFig. 5(b) shows attention maps analyzing biased feature unlearning. The Baseline model predominantly focuses on the biased feature region (mouth) in both bias and unbias datasets, suggesting its significant impact on output prediction. However, Ferrari unlearned models redistribute attention across various facial regions in both datasets, similar to the Retrain model. This shift indicates reduced sensitivity to the biased feature, demonstrating successful unlearning. See Appendix A.3.2 for supplementary results.

### Computational Complexity

In Fig. 7, we evaluate the runtime performance and FLOPs metrics of each unlearning method to demonstrate the computational complexity. The Retrain method is expected to have the slowest runtime and highest FLOPs, while Fine-tune is fast but still slower than other methods.

Both FedCDP  and FedRecovery  demonstrate faster runtimes and lower FLOPs than the Fine-tune method, but they are still more computationally expensive than Ferrari. This is primarily due to the need to access training datasets from all clients and the computational expense of gradient residual calculations .

   &  &  &  \\   & & **Feature** & **Baseline** & **Retrain** & **Frine-tune** & **FedCDP ** & **FedRecovery ** & **Ferrari (Ours)** \\   & **CelebA** & Mouth & 84.36 \(\)3.22 & 47.52 \(\)1.04 & 77.43 \(\)10.98 & 75.36 \(\)9.31 & 71.52 \(\)6.07 & **51.28 \(\)2.41** \\  & **Adult** & Marriage & 87.54 \(\)1.39 & 49.28 \(\)2.13 & 83.45 \(\)8.44 & 72.83 \(\)5.18 & 80.39 \(\)10.68 & **49.85 \(\)1.38** \\  & **Dabetes** & Pregnances & 92.31 \(\)7.55 & 38.89 \(\)2.52 & 88.46 \(\)5.01 & 81.91 \(\)1.87 & 78.27 \(\)2.47 & **42.61 \(\)1.81** \\  & **IMDB** & Names & 90.28 \(\)2.49 & 40.29 \(\)1.59 & 86.74 \(\)3.81 & 83.67 \(\)4.59 & 80.95 \(\)3.51 & **43.75 \(\)1.86** \\  

Table 3: The ASR of MIA for each unlearning method across sensitive feature unlearning scenario.

Figure 5: MIA reconstruction on CelebA (unlearned mouth)In contrast, Ferrari has the lowest computational complexity, with the fastest runtime and lowest FLOPs. It only requires access to the local dataset of the unlearn client and achieves feature unlearning by minimizing feature sensitivity within a single epoch.

### Ablation Study and Hyper-parameter Analysis

We conduct an ablation study to analyze how Non-Lipschitz affects the effectiveness of our proposed Ferrari and hyper-parameter analysis of Gaussian noise level (\(\)) and number of \(_{u}\) in Fig. 8.

Non-LipschitzWe evaluate the unlearning performance by removing the denominator in Eq. 6, calling this the Non-Lipschitz method, as shown in Fig. 7(a). The results indicate catastrophic forgetting: \(_{r}\) accuracy drops below 10%, and the unlearned model misclassifies all inputs into a single random class, rendering it useless. This stems from the unbounded loss function in the non-Lipschitz method, unlike the bounded Lipschitz constant in Eq. 6, which provides a theoretical guarantee (see Sec. 4.3). Refer to Appendix A.4 for a detailed analysis of Lipschitz and Non-Lipschitz loss functions.

Gaussian NoiseThe effectiveness of Ferrari is significantly influenced by injected Gaussian noise. Fig. 7(b) shows the accuracy of \(_{r}\) and \(_{u}\) across different \(\) levels. In the \(0.05 1.0\) range, \(_{r}\) accuracy stays high and \(_{u}\) accuracy remains low, indicating a balance. Thus, we implement \(\) values between 0.05 and 1.0 for a balanced accuracy across \(_{r}\) and \(_{u}\).

Number of Unlearn DatasetOur analysis illustrated in Fig. 7(c), demonstrates that Ferrari remains effective with partial \(_{u}\) from \(C_{u}\) for feature unlearning (_i.e.,_ data lost). Using 70% of \(_{u}\) yields comparable accuracy to using the full (_i.e.,_ 100%) dataset, highlighting the method's flexibility even with partial data.

   &  &  &  &  &  &  &  &  \\   & **MNIST** & & \(_{r}\) & 95.65 \(\) 1.39 & 97.19 \(\) 2.49 & **96.16 \(\) 0.37** & 65.82 \(\) 6.85 & 40.81 \(\) 4.31 & 95.93 \(\) 0.45 \\  & & \(_{r}\) & 97.43 \(\) 9.69 & 0.00 \(\) 0.00 & 72.64 \(\) 0.29 & 93.03 \(\) 0.53 & 53.27 \(\) 3.14 & **0.11 \(\) 0.01** \\  & & \(_{r}\) & 91.07 \(\) 0.54 & 95.83 \(\) 1.08 & **94.36 \(\) 1.98** & 68.46 \(\) 3.59 & 42.93 \(\) 25.20 & 92.83 \(\) 0.61 \\  & & \(_{r}\) & 94.51 \(\) 6.29 & 0.00 \(\) 0.00 & 43.91 \(\) 0.28 & 17.29 \(\) 0.49 & 48.15 \(\) 4.37 & **0.90 \(\) 0.03** \\   & **CIFAR-10** & Backdoor & & & & & & & & \\  & pixel- & & & & & & & & \\  & & & & & & & & & \\  & & & & & & & & & \\  & & & & & & & & & & \\  & & & & & & & & & & \\  & & & & & & & & & & \\
**Biased** & & & & & & & & & & \\  

Table 4: The accuracy of \(_{r}\) and \(_{u}\) for each unlearning method across different unlearning scenarios.

Figure 6: The attention map of each unlearning method across different unlearning scenarios.

## 6 Conclusion

This paper introduces Ferrari, a federated feature unlearning framework designed to efficiently remove sensitive, backdoor, and biased features without extensive retraining. Leveraging Lipschitz continuity, Ferrari reduces model sensitivity to specific features, ensuring robust and fair models. Uniquely, it requires participation only from the client requesting unlearning, preserving privacy and practicality in FL environments. Experimental results and theoretical analysis demonstrate Ferrari's effectiveness across various data domains, addressing the crucial need for feature-level unlearning in federated learning. This method can serve as a technical solution to meet regulatory requirements for data deletion while maintaining model performance, offering significant value to clients by securing their "right to be forgotten" and preventing potential privacy leakage.

### Limitation and Future Work

The proposed federated feature unlearning method works effectively using only the unlearning client's local data, making it well-suited for real-world scenarios. However, for optimal results, access to the full dataset is required. As demonstrated in Section 5.5, using 70% of the data yields comparable performance, but significant data reduction diminishes effectiveness. Future research should focus on developing methods that require only a small portion of the client's data and expanding the approach beyond classification models to include for example, generative models. Additionally, enhancements such as advanced perturbation techniques and integration with privacy-preserving methods should be explored.