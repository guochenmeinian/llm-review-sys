# Compositional Sculpting of

Iterative Generative Processes

 Timur Garipov\({}^{1}\)

Correspondence to Timur Garipov (timur@csail.mit.edu).

Sebastiaan De Peuter\({}^{2}\)

Ge Yang\({}^{1,4}\)

Vikas Garg\({}^{2,5}\)

Samuel Kaski\({}^{2,3}\)

Tommi Jaakkola\({}^{1}\)

\({}^{1}\)MIT CSAIL \({}^{2}\)Aalto University \({}^{3}\)University of Manchester

\({}^{4}\)Institute for Artificial Intelligence and Fundamental Interactions \({}^{5}\)YaiYai Ltd

###### Abstract

High training costs of generative models and the need to fine-tune them for specific tasks have created a strong interest in model reuse and composition. A key challenge in composing iterative generative processes, such as GFlowNets and diffusion models, is that to realize the desired target distribution, all steps of the generative process need to be coordinated, and satisfy delicate balance conditions. In this work, we propose Compositional Sculpting: a general approach for defining compositions of iterative generative processes. We then introduce a method for sampling from these compositions built on classifier guidance. We showcase ways to accomplish compositional sculpting in both GFlowNets and diffusion models. We highlight two binary operations -- the harmonic mean (\(p_{1} p_{2}\)) and the contrast (\(p_{1}\)) between pairs, and the generalization of these operations to multiple component distributions. We offer empirical results on image and molecular generation tasks. Project codebase: https://github.com/timgaripov/compositional-sculpting.

## 1 Introduction

Large-scale general-purpose pre-training of machine learning models has produced impressive results in computer vision [1; 2; 3], image generation [4; 5; 6], natural language processing [7; 8; 9; 10; 11], robotics [12; 13; 14], and basic sciences . By distilling vast amounts of data, such models can produce powerful inferences that lead to emergent capabilities beyond the specified training objective . However, generic pre-trained models are often insufficient for specialized tasks in engineering and basic sciences. Field-adaptation via techniques such as explicit fine-tuning on bespoke datasets , human feedback , or cleverly designed prompts [19; 20] is therefore often required. Alternatively, capabilities of pre-trained models can be utilized and extended via model composition.

Compositional generation [21; 22; 23; 24; 25; 26; 27; 28] views a complex target distribution in terms of simpler pre-trained building blocks which can be mixed and matched into a tailored solution to a specialized task. Given a set of base models capturing different properties of the data, composition provides a way to fuse these models into a single composite model with capacity greater than any individual base model. In this way it allows one to specify distributions over examples that exhibit multiple desired properties simultaneously . The need to construct complex distributions adhering to multiple constraints arises in numerous practical multi-objective design problems such as molecule generation [29; 30; 31]. In this context, compositional modeling provides mechanisms for control of the resulting distribution and exploration of different trade-offs between the objectives and constraints.

Prior work on generative model composition [21; 22; 24] has developed operations for piecing together Energy-Based Models (EBMs) via algebraic manipulations of their energy functions. For example, consider two distributions \(p_{1}(x)(-E_{1}(x))\) and \(p_{2}(x)(-E_{2}(x))\) induced by energyfunctions \(E_{1}\) and \(E_{2}\). Their _product_\(p_{}(x) p_{1}(x)p_{2}(x)(-(E_{1}(x)+E_{2}(x)))\) and _negation_\(p_{}(x) p_{1}(x)/(p_{2}(x))^{}(-(E_{1}(x)- \,E_{2}(x)))\), \(>0\) correspond to operations on the underlying energy functions. The product assigns high likelihood to points \(x\) that have high likelihood under both base distributions but assigns low likelihood to points that have close-to-zero likelihood under one (or both). The negation distribution assigns high likelihood to points that are likely under \(p_{1}\) but unlikely under \(p_{2}\) and assigns low likelihood to points that are likely under \(p_{2}\) but unlikely under \(p_{1}\).

Iterative generative processes including diffusion models  and GFlowNets  progressively refine coarse objects into cleaner ones over multiple steps. Realizing effective compositions of these models is complicated by the fact that simple alterations in their generation processes result in non-trivial changes in the distributions of the final objects. Unlike for EBMs, products and negations of diffusion models cannot be realized through simple algebraic operations on their score functions. Du et al.  show that the result of the addition of score functions is not equal to the score of the diffused product distribution and develop a method that corrects the sum-of-scores sampling via additional MCMC steps nested under each step of the diffusion time loop.

Jain et al.  develop Multi-Objective GFlowNets (MOGFNs), an extension of GFlowNets for multi-objective optimization tasks. While a vanilla GFlowNet captures a distribution induced by a single reward (objective) function \(p(x) R(x)\) (see Section 2 for details), an MOGFN aims to learn a single conditional model that can realize distributions corresponding to various combinations (e.g. a convex combination) of multiple reward functions. Though a single MOGFN realizes a spectrum of compositions of base reward functions, the approach assumes access to the base rewards at training time. Moreover, MOFGNs require the set of possible composition operations to be specified at training time. In this work, we address post hoc composition of pre-trained GFlowNets (or diffusion models) and provide a way to create compositions that need not be specified in advance.

In this work, we introduce Compositional Sculpting, a general approach for the composition of pre-trained models. We highlight two special examples of binary operations -- _harmonic mean_: \((p_{1} p_{2})\) and _contrast_: \((p_{1} p_{2})\). More general compositions are obtained as conditional distributions in a probabilistic model constructed on top of pre-trained base models. We show that these operations can be realized via classifier guidance. We provide results of empirical verification of our method on molecular generation (with GFlowNets) and image generation (with diffusion models).

## 2 Background

Generative flow networks (GFlowNets).GFlowNets  are an approach for generating structured objects (e.g. graphs) from a discrete space \(\). Given a "reward function" \(R(x) 0\), a GFlowNet seeks to sample from \(p(x)=R(x)/Z\), where \(Z=_{x}R(x)\), i.e. the model assigns larger probabilities to high-reward objects.

Starting at a fixed initial state \(s_{0}\), objects \(x\) are generated through a sequence of changes corresponding to a trajectory of incomplete states \(=(s_{0} s_{1}... s_{n-1} x)\). The structure of possible trajectories corresponds to by a DAG \((,)\) where \(\) is a set of states (both complete and incomplete) and \(\) is the set of directed edges (actions) \(s s^{}\). The set of complete objects (terminal states) \(\) is a subset of \(S\). The generation process starts at \(s_{0}\) and follows a parameterized stochastic "forward policy" \(P_{F}(s^{}|s;)\) which for each state \(s\) specifies a probability distribution over all possible successor states \(s^{}:(s s^{})\). The process terminates once a terminal state is reached.

Figure 1: **Composition operators.** (a,b) base distributions \(p_{1}\) and \(p_{2}\). (c) harmonic mean of \(p_{1}\) and \(p_{2}\). (d) contrast of \(p_{1}\) with \(p_{2}\) (e) reverse contrast \(p_{1} p_{2}\). Lines show the contours of the PDF level sets.

Diffusion models.Diffusion models [32; 33; 34; 5; 37] are a family of generative models developed for continuous domains. Given an empirical (data) distribution \((x)\!=\!}{{n}}_{i}_{_{i}}(x)\) in \(=^{d}\), diffusion models seek to approximate \((x)\) via a generative process \(p(x)\).

A diffusion process is a noising process that gradually destroys the original "clean" data \(x\). Viewed as a stochastic differential equation (SDE) , it is a time-indexed collection of random variables \(\{x_{i}\}_{t=0}^{T}\) in \(=^{d}\) which interpolates between the data distribution \(p_{0}(x)=(x)\) at \(t=0\) and a prior distribution \(p_{T}(x)\) at \(t=T\). The evolution of \(x_{t}\) is described by the "forward SDE" \(dx_{t}=f_{t}(x_{t})\,dt+g_{t}\,dw_{t}\) with drift coefficient \(f_{t}:^{d}^{d}\) and diffusion coefficient \(g_{t}\). Here, \(w_{t}\) is the standard Wiener process. Crucially, the coefficients \(f_{t}\), \(g_{t}\) are generally chosen such that the prior \(p_{T}\) and the transition probabilities \(p_{st}(x_{t}|x_{s})\), \(0 s<t T\) have a closed form (see ).

Song et al.  invoke a result from the theory of stochastic processes  which gives the expression for the reverse-time process or "backward SDE": \(dx_{t}=[f_{t}(x_{t})-g_{t}^{2}_{x} p_{t}(x_{t})]dt+g_{t} \,d_{t}\),where \(_{t}\) is the standard Wiener process in reversed time. This SDE involves the known coefficients \(f_{t}\), \(g_{t}\) and the unknown score function \(_{x} p_{t}()\) of the marginal distribution \(p_{t}()\) at time \(t\). A "score network" \(s_{t}(x;)\) (a deep neural network with parameters \(\)) is trained to approximate \(_{x} p_{t}(x)\). Once \(s_{t}(;)\) is trained, sampling reduces to numerical integration of the backward SDE.

Classifier guidance in diffusion models.Classifier guidance [32; 39] is a technique for controllable generation in diffusion models. Suppose that each example \(x_{0}\) is accompanied by a discrete class label \(y\). The goal is to sample from the conditional distribution \(p_{0}(x_{0}|y)\). The Bayes rule \(p_{t}(x_{t}|y) p_{t}(x_{t})p_{t}(y|x_{t})\) implies the score-function decomposition \(_{x_{t}} p_{t}(x_{t}|y)=_{x_{t}} p_{t}(x_{t})+_{x_{t }} p_{t}(y|x_{t})\), where the first term is already approximated by a pre-trained unconditional diffusion model and the second term can be derived from a time-dependent classifier \(p_{t}(y|x_{t})\). Therefore, the stated goal can be achieved by first training the classifier \(p_{t}(y|x_{t})\) using noisy samples \(x_{t}\) from the intermediate steps of the process, and then plugging in the expression for the conditional score into the backward SDE sampling process .

## 3 Related Work

Generative model composition.In Section 1 we reviewed prior work on energy-based composition operations and Multi-Objective GFlowNets. Learning mixtures of Generative Adversarial Networks has been addressed in , where the mixture components are learned simultaneously, and in , where the components are learned one by one in an adaptive boosting fashion. Algorithms for additive and multiplicative boosting of generative models have been developed in .

This work focuses on the composition of pre-trained models. Assuming that each pre-trained model represents the distribution of examples demonstrating certain concepts (e.g. molecular properties), the composition of models is equivalent to concept composition (e.g. property "A" AND property "B"). The inverse problem is known as "unsupervised concept discovery", where the goal is to automatically discover composable concepts from data. Unsupervised concept discovery and concept composition methods have been proposed for energy-based models  and text-to-image diffusion models .

Controllable generation.Generative model composition is a form of post-training control of the generation process - an established area of research. A simple approach to control is training a conditional generative model \(p(x|c)\) on pairs \((x,c)\) of objects \(x\) and conditioning information \(c\). Annotations \(c\) can be class labels , text prompts [4; 6; 44], semantic maps, and images . Different from out work, this assumes that generation control operations are specified at training time. Dhariwal and Nichol  apply classifier guidance  on top of (conditional) diffusion models to improve their fidelity. Ho and Salimans  develop classifier-free guidance by combining conditional and unconditional score functions. In ControlNet , an additional network is trained to enable a pre-trained diffusion model to incorporate previously unavailable conditioning information. Meng et al.  and Couairon et al.  develop semantic image editing methods which first partially noise and then denoise an image to generate an edited version, possibly conditioned on a segmentation mask . Similar to conditional diffusion models, conditional GFlowNets have been used to condition generation on reward exponents  or combinations of multiple predefined reward functions . Note that the methods developed in this work can be combined with conditional diffusion models and GFlowNets: \(p(x|c_{1})\),..., \(p(x|c_{m})\) can act as base generative models to be composed.

Compositional generalization.The notion of compositionality has a broad spectrum of interpretations across a variety of disciplines including linguistics, cognitive science, and philosophy. Hupkes et al.  collect a list of aspects of compositionality from linguistic and philosophical theories and design practical tests for neural language models. Conwell and Ullman  empirically examine the relational understanding of DALL-E 2 , a text-guided image generation model, and point out limitations in the model's ability to capture relations such as "in", "on", "hanging over", etc. In this work, we focus on a narrow but well-defined type of composition where we seek to algebraically compose probability densities in a controllable fashion, such that we can emphasize or de-emphasize regions in the data space where specific base distributions have high density.

Connections between GFlowNets and diffusion models.Our method is applicable to compositions of both GFlowNets and diffusion models. This is due to deep connections between these two model families. GFlowNets were initially developed for generating discrete (structured) data  and diffusion models were initially developed for continuous data [5; 32]. Lahlou et al.  develop an extension of GFlowNets for DAGs with continuous state-action spaces. Zhang et al.  point out unifying connections between GFlowNets and other generative model families, including diffusion models. In this work, we articulate another aspect of the relation between GFlowNets and diffusion models: in Section 5.2 we derive the expressions for mixture GFlowNet policies and classifier-guided GFlowNet policies analogous to those derived for diffusion models in [32; 39; 53; 54].

## 4 Compositional Sculpting of Generative Models

Suppose we can access a number of pre-trained generative models \(\{p_{i}(x)\}_{i=1}^{m}\) over a common domain \(\). We may wish to compose these distributions such that we can, say, draw samples that are likely to arise from \(p_{1}(x)\) and \(p_{2}(x)\), or that are likely to arise from \(p_{1}(x)\) but not from \(p_{2}(x)\). In other words, we wish to specify a distribution that we can shape to emphasize and de-emphasize specific base models.

### Binary Composition Operations

Let us first focus on composing two base models. We could specify the composition as a weighted sum \((x)=_{j=1}^{2}_{j}p_{i}(x)\) with weights \(_{1},_{2} 0\) summing to one. The weights determine the prevalence of each base model in the composition, but beyond that our control is limited. We cannot emphasize regions where \(p_{1}\) and \(p_{2}\) both have high density, or de-emphasize regions where \(p_{2}\) has high density.

An alternative is to use conditioning to shape a prior \((x)\) based on the base models. When we condition \(x\) on some observation \(y_{1}\), the resulting posterior takes the form \((x|y_{1})(y_{1}|x)(x)\). Points \(x\) that match \(y_{1}\) according to \((y_{1}|x)\) will have increased density, and the density of points that do not match it decreases. Intuitively, by defining \(y_{1}\{1,2\}\) as the event that \(x\) was generated by a specific base model, we can shape a prior \((x)\) based on the densities of the base models. To this end we define a uniform prior over \(y_{k}\) and define the conditional density \(p(x|y_{1}=i)\) to represent the fact that \(x\) was generated from \(p_{i}(x)\). This gives us the following model:

\[(x|y_{1}=i)=p_{i}(x),(y_{1}=i)=1/2, (x)=(x|y_{1}=1)(y_{1}=1)+ (x|y_{1}=2)(y_{1}=2).\] (1)

Under this model, the prior \((x)\) is a uniform mixture of the base models. The likelihood of \(y_{1}\)

\[(y_{1}=1|x)=1-(y_{1}=2|x)=p_{1}(x)/(p_{1}(x)+p_{2}( x)),\] (2)

implied by this model tells us how likely it is that \(x\) was generated by \(p_{1}(x)\) rather than \(p_{2}(x)\). In fact, it corresponds to the output of an optimal classifier trained to tell \(p_{1}(x)\) and \(p_{2}(x)\) apart.

Our goal is to realize compositions which generate samples likely to arise from both \(p_{1}(x)\) and \(p_{2}(x)\) or from \(p_{1}(x)\) but not \(p_{2}(x)\). Thus we introduce a second observation \(y_{2}\{1,2\}\) such that \(y_{1}\) and \(y_{2}\) are independent and identically distributed given \(x\). The resulting model and inferred posterior are:

\[(x,y_{1},y_{2})=(x)_{k=1}^{2}(y_{ k}|x),\ \ (x)=p_{1}(x)+p_{2}(x),\ \ (y_{k}=i|x)=p_{i}(x)/(p_{1}(x)+p_{2}(x)),\] (3)

\[(x|y_{1}=i,\,y_{2}=j)(x)(y_{1}= i|x)(y_{2}=j|x) p_{i}(x)p_{j}(x)/(p_{1}(x)+p_{2}(x)).\] (4)

The above posterior shows clearly how conditioning on observations \(y_{1}=i,\,y_{2}=j\) has shaped the prior mixture to accentuate regions in the posterior where the observed base models \(i,j\) have high density.

Conditioning on observations \(y_{1}=1\) and \(y_{2}=2\), or equivalently \(y_{1}=2,y_{2}=1\), results in the posterior

\[(p_{1} p_{2})(x):=p(x|y_{1}=1,y_{2}=2) p_{1}(x)p_{2}(x)/(p_{1}(x) +p_{2}(x)).\] (5)

We refer to this posterior as the "**harmonic mean** of \(p_{1}\) and \(p_{2}\)", and denote it as a binary operation \(p_{1} p_{2}\). Its value is high only at points that have high likelihood under both \(p_{1}(x)\) and \(p_{2}(x)\) at the same time (Figure 1(c)). Thus, the harmonic mean is an alternative to the product operation for EBMs. The harmonic mean is commutative (\(p_{1} p_{2}=p_{2} p_{1}\)) and is undefined when \(p_{1}\) and \(p_{2}\) have disjoint supports, since then the RHS of (5) is zero everywhere.

Conditioning on observations \(y_{1}=1\) and \(y_{2}=1\) results in the posterior

\[(p_{1} p_{2})(x):=(x|y_{1}=1,y_{2}=1)(p_{1}(x))^{2 }/(p_{1}(x)+p_{2}(x)).\] (6)

We refer to this binary operation, providing an alternative to the negation operation in EBMs, as the "**contrast** of \(p_{1}\) and \(p_{2}\)", and denote it as \(p_{1} p_{2}\). The ratio (6) is high when \(p_{1}(x)\) is high and \(p_{2}(x)\) is low (Figure 1(d)). The contrast is not commutative (\(p_{1} p_{2} p_{2} p_{1}\), unless \(p_{1}=p_{2}\)). We denote the reverse contrast as \(p_{1} p_{2}=p_{2} p_{1}\). Appendix C provides a detailed comparison between the contrast and negation operations, and between the harmonic mean and product operations.

Controlling the individual contributions of \(p_{1}\) and \(p_{2}\) to the composition.In order to provide more control over the extent of individual contributions of \(p_{1}\) and \(p_{2}\) to the composition, we modify model (3). Specifically, we introduce an interpolation parameter \(\) and change the likelihood of observation \(y_{2}\) in (3): \((y_{2}=i|x;)=( p_{1}(x))^{|i=1}((1-)p_{2} (x))^{|i=2}/( p_{1}(x)+(1-)p_{2}(x))\), where \((0,1)\) and \([]\) denotes the indicator function. Conditional distributions in this model give **harmonic interpolation2** and **parameterized contrast**:

\[(p_{1}_{(1-)}p_{2})(x)(x)p_{2}(x)}{ p_{1 }(x)+(1-)p_{2}(x)},(p_{1}_{(1-)}p_{2})(x) {(p_{1}(x))^{2}}{ p_{1}(x)+(1-)p_{2}(x)}.\] (7)

Operation chaining.As the operations we have introduced result in proper distributions, we can create new \(N\)-ary operations by chaining binary (and \(N\)-ary) operations together. For instance, chaining binary harmonic means gives the harmonic mean of three distributions

\[((p_{1} p_{2}) p_{3})(x)=(p_{1}(p_{2} p_{3}))(x) (x)p_{2}(x)p_{3}(x)}{p_{1}(x)p_{2}(x)+p_{1}(x)p_{3}(x)+p_{2} (x)p_{3}(x)}.\] (8)

### Compositional Sculpting: General Approach

The above approach for realizing compositions of two base models can be generalized to compositions of \(m\) base models \(p_{1}(x),,p_{m}(x)\) controlled by \(n\) observations. Though operator chaining can also realize compositions of \(m\) base models, our generalized method allows us to specify compositions more flexibly and results in different compositions. We introduce an augmented probabilistic model \((x,y_{1},,y_{n})\) as a joint distribution over the original objects \(x\) and \(n\) observation variables \(y_{1},,y_{n}\) where \(=\{1,,m\}\). By defining appropriate conditionals \(p(y_{k}|x)\) we can controllably shape a prior \((x)\) into a posterior \((x|y_{1},,y_{n})\).

As in the binary case, we propose to use a uniformly-weighted mixture of the base models \((x)=(1/n)_{i=1}^{m}p_{i}(x)\). The support of this mixture is the union of the supports of the base models: \(_{i=1}^{m}\{p_{i}(x)\}=\{ (x)\}\). This is essential as the prior can only be shaped in places where it has non-zero density. As before we define the conditionals \(p(y_{k}=i|x)\) to correspond to the observation that \(x\) was generated by base model \(i\). The resulting full model is

\[(x,y_{1},,y_{n})=(x)_{k=1}^{n}(y_{k}|x),\ (x)=_{i=1}^{m}p_{i}(x),\ (y_{k}=i)=,\ (y_{k}=i|x)=(x)}{_{j=1}^{m}p_{j}(x)},\] (9)

Note that under this model the mixture can be represented as \((x)=_{y_{k}=1}^{m}(x|y_{k})(y_{k})\) for any \(k\).

The inferred posterior over \(x\) for this model is

\[(x|y_{1}=i_{1},,y_{n}=i_{n}) (x)(y_{1}=i_{1},,y_{n}=i_{n }|x)\] (10) \[(x)_{k=1}^{n}(y_{k}=i_{k}|x )(_{k=1}^{n}p_{i_{k}}(x))(_{j=1}^{m}p_ {j}(x))^{n-1}.\] (11)The posterior \((x|y_{1}=i_{1},,y_{n}=i_{n})\) is a composition of distributions \(\{p_{i}(x)\}_{i=1}^{m}\) that can be adjusted by choosing values for \(y_{1},,y_{n}\). By adding or omitting an observation \(y_{k}=i\) we can _sculpt_ the posterior to our liking, emphasizing or de-emphasizing regions of \(\) where \(p_{i}\) has high density. The observations can be introduced with multiplicities (e.g., \(y_{1}=1,y_{2}=1,y_{3}=2\)) to further strengthen the effect. Moreover, one can choose to introduce all observations simultaneously as in (10) or sequentially as in (11). As we show below (Section 5.1 for GFlowNets; Appendix A.2 for diffusion models), the composition (10) can be realized by a sampling policy that can be expressed as a function of the pre-trained (base) sampling policies.

Special instances and general formulation.The general approach outlined in this section is not limited to choices we made to construct the model in equation (9), i.e. \((x)\) does not have to be a uniformly weighted mixture of the base distributions, \(y_{1},,y_{n}\) do not have to be independent and identically distributed given \(x\), and different choices of the likelihood \((y=i|x)\) are possible. For instance, in the model for parameterized operations (7) the likelihoods of observations \((y_{1}|x)\), \((y_{2}|x)\) differ.

## 5 Compositional Sculpting of Iterative Generative Processes

In this Section, we show how to apply the model above to compose GFlowNets, and how one can use classifier guidance to sample from the composition. The similar method for diffusion model composition is described in Appendix A.2.

### Composition of GFlowNets

Besides a sample \(x\) from \(p_{i}(x)\), a GFlowNet also generates a trajectory \(\) which ends in the state \(x\). Thus, we extend the model \((x,y_{1},,y_{n})\), described above, and introduce \(\) as a variable with conditional distribution \((|y_{k}=i)=_{t=0}^{||-1}p_{i,F}(s_{t+1}|s_{t})\), where \(p_{i,F}\) is the forward policy of the GFlowNet that samples from \(p_{i}\).

Our approach for sampling from the composition is conceptually simple. Given \(m\) base GFlowNets that sample from \(p_{1},,p_{m}\) respectively, we start by defining the prior \((x)\) as the uniform mixture of these GFlowNets. Proposition 5.1 shows that this mixture can be realized by a policy constructed from the forward policies of the base GFlowNets. We then apply classifier guidance to this mixture to sample from the composition. Proposition 5.2 shows that classifier guidance results in a new policy which can be constructed directly from the GFlowNet being guided.

**Proposition 5.1** (GFlowNet mixture policy).: _Suppose distributions \(p_{1}(x),,p_{m}(x)\) are realized by GFlowNets with forward policies \(p_{1,F}(),,p_{m,F}(|)\). Then, the mixture distribution \(p_{M}(x)=_{i=1}^{m}_{i}p_{i}(x)\) with \(_{1},,_{m} 0\) and \(_{i=1}^{m}_{i}=1\) is realized by the GFlowNet forward policy_

\[p_{M,F}(s^{}|s)=_{i=1}^{m}p(y=i|s)p_{i,F}(s^{}|s),\] (12)

_where \(y\) is a random variable such that the joint distribution of a GFlowNet trajectory \(\) and \(y\) is given by \(p(,y=i)=_{i}p_{i}()\) for \(i\{1,,m\}\)._

**Proposition 5.2** (GFlowNet classifier guidance).: _Consider a joint distribution \(p(x,y)\) over a discrete space \(\) such that the marginal \(p(x)\) is realized by a GFlowNet with forward policy \(p_{F}(|)\). Assume that the joint distribution of \(x\), \(y\), and GFlowNet trajectories \(=(s_{0} s_{n}=x)\) decomposes as \(p(,x,y)=p(,x)p(y|x)\), i.e. \(y\) is independent of the intermediate states \(\{s_{i}\}_{i=0}^{n-1}\) in \(\) given \(x\). Then,_

1. _For all non-terminal nodes_ \(s S\)_, the probabilities_ \(p(y|s)\) _satisfy_ \[p(y|s)=_{s^{}:(s=s^{})}p_{F}(s^{}|s)p(y|s^{ }).\] (13)
2. _The conditional distribution_ \(p(x|y)\) _is realized by the classifier-guided policy_ \[p_{F}(s^{}|s,y)=p_{F}(s^{}|s) p(y|s^{})\,p(y|s).\] (14)

Note that (13) ensures that \(_{s^{}:(s s^{})}p_{F}(s^{}|s,y)=1\).

Proposition 5.1 is analogous to results on mixtures of diffusion models (Theorem 1 of Peluchetti , Theorem 1 of Lipman et al. ). Proposition 5.2 is analogous to classifier guidance for diffusion models . To the best of our knowledge, our work is the first to derive both results for GFlowNets.

Theorem 5.3 summarizes our approach. The propositions and the theorem are proved in Appendix D.

**Theorem 5.3**.: _Suppose distributions \(p_{1}(x),,p_{m}(x)\) are realized by GFlowNets with forward policies \(p_{1,F}(|)\),..., \(p_{m,F}(|)\) respectively. Let \(y_{1},,y_{n}\) be random variables defined by (9). Then, the conditional \((x|y_{1},,y_{n})\) is realized by the forward policy_

\[p_{F}(s^{}|s,y_{1},,y_{n})=(y_{1},,y_{n} |s^{})}{(y_{1},,y_{n}|s)}_{i=1}^{m}p_{i,F}(s^{ }|s)(y\!=\!i|s)\] (15)

Note that the result of conditioning on observations \(y_{1},,y_{n}\) is just another GFlowNet policy. Therefore, to condition on more observations, one can apply classifier guidance repeatedly.

### Classifier Training (GFlowNets)

The evaluation of policy (15) requires knowledge of the probabilities \((y_{1},,y_{n}|s)\), \((y|s)\). These probabilities can be estimated by a classifier fitted to trajectories sampled from the base GFlowNets.

Let \(_{}(y_{1},,y_{n}|s)\) be a classifier with parameters \(\) that we wish to train to approximate the ground-truth conditional \((y_{1},,y_{n}|s)\). Under model (9), \(y_{1},,y_{n}\) are dependent given a state \(s S\), but, are independent given a terminal state \(x\). This motivates separate treatment of terminal and non-terminal states.

Learning the terminal state classifier.For a terminal state \(x\), the variables \(y_{1},,y_{n}\) are independent, hence we can use the factorization \(_{}(y_{1},,y_{n}|x)=_{k=1}^{n}_{ }(y_{k}|x)\). Moreover, all distributions on the _r.h.s._ must be the same, i.e. it is enough to learn just \(_{}(y_{1}|x)\). This marginal classifier can be learned by minimizing the cross-entropy loss

\[_{}()=*{}_{(, _{1})}(y_{1})(x|y_{1})} [-_{}(y_{1}\!=\!_{1}|x\!=\!) ].\] (16)

Here \(_{1}\) is sampled from \((y_{1})\), which is uniform under our choice of \((x)\). Then, \(|(y_{1}\!=\!_{1})\) is generated from the base GFlowNet \(p_{_{1}}\), since (9) implies that \((x|y\!=\!_{1})=p_{_{1}}(x)\).

Learning the non-terminal state classifier.Given a non-terminal state \(s S\), we need to model \(y_{1},,y_{n}\) jointly, and training requires sampling tuples \((,_{1},,_{n})\). Non-terminal states \(s\) can be generated as intermediate states in trajectories \(=(s_{0} s_{1} x)\). Given a sampled trajectory \(\) and a set of labels \(_{1},,_{n}\) we denote the trajectory cross-entropy by

\[(,_{1},,_{n};)= _{t=0}^{||-1}[-_{}(y_{1}\!=\!_{1}, ,y_{n}\!=\!_{n}|s\!=\!_{t})].\] (17)

Pairs \((,_{1})\) can be generated in the same way as in the terminal classifier training above: 1) \(_{1}(y_{1})\); 2) \( p_{_{1}}()\). Sampling \(_{2},,_{n}\) given \(\) (the terminal state of \(\)) requires access to values \((y_{k}=_{k}|)\), which are not directly available. However, if the terminal classifier is learned as described above, the estimates \(w_{i}(;)=_{}(y_{1}\!=\!i|x\!=\!)\) can be used instead. In this case, the loss and the sampling procedure for the non-terminal classifier rely on the outputs of the terminal classifier. In order to train two classifiers simultaneously, and avoid the instability due to a feedback loop, we employ the "target network" technique developed in the context of deep Q-learning . We introduce a "target network" parameter vector \(\) which is used to produce the estimates \(w_{i}(};)\) for the non-terminal loss. We update \(\) as the exponential moving average of the recent iterates of \(\).

After putting all components together the training loss for the non-terminal state classifier is

\[_{N}(,)=*{}_{(,_{1})(,y_{1})}[_{_{2 }=1}^{m}_{_{n}=1}^{m}(_{k=2}^{n}w_{_{k }}(;))(,_{1},,_{n};)].\] (18)

We refer the reader to Appendix D.4 for a more detailed derivation of the loss (18). Algorithm A.1 shows the complete classifier training procedure.

## 6 Experiments

2D distributions via GFlowNet.We tested our GFlowNet composition method on 2D grid , a controlled domain, where the ground-truth composite distributions can be evaluated directly.

In the 2D grid domain, the states are the cells of an \(H H\) grid. The starting state is the upper-left cell \(s_{0}=(0,0)\). At each state, the allowed actions are: 1) move right; 2) move down; 3) terminate the trajectory at the current position. We first trained GFlowNets \(p_{i}(x) R_{i}(x)\) with reward functions \(R_{i}(x)>0\), and then trained classifiers and constructed compositions following Theorem 5.3.

Figure 2 (top row) shows the distributions obtained by composing two pre-trained GFlowNets (top row; left). The harmonic mean \(p_{1} p_{2}\), covers the regions that have high probability under both \(p_{1}\) and \(p_{2}\) and excludes locations where either of the probabilities is low. \(p_{1} p_{2}\) resembles \(p_{1}\) but the relative masses of the modes of \(p_{1}\) are modulated by \(p_{2}\): regions with high \(p_{2}\) have lower probability under contrast. The parameterized contrast \(p_{1}_{0.95}p_{2}\) with \(=0.05\) magnifies the contrasting effect: high \(p_{2}(x)\) implies very low (\(p_{1}_{0.95}p_{2}\))(\(x\)). The bottom row of Figure 2 shows operations on 3 distributions. The conditional \(_{i}(x|y_{1}\)=1, \(y_{2}\)=2) is concentrated on the points that have high likelihood under both \(p_{1}\) and \(p_{2}\). Similarly, the value \((x|y_{1}\)=1, \(y_{2}\)=2, \(y_{3}\)=3) is high if \(x\) is likely to be observed under all three distributions at the same time. The conditions \((x|y_{1}\)=2, \(y_{2}\)=2) and \((x|y_{1}\)=2, \(y_{2}\)=2, \(y_{3}\)=2) highlight the points with high \(p_{2}(x)\) but low \(p_{1}(x)\) and \(p_{3}(x)\). Conditioning on three labels results in a sharper distribution compared to double-conditioning. We provide quantitative results and further details in Appendix F.1. The classifier learning curves are provided in Appendix G.4.

Molecule generation via GFlowNet.Next, we evaluated our method for GFlowNet composition on a large and highly structured data space, and assessed the effect that composition operations have

Figure 3: **Reward distributions in the molecular generation domain. (a) Base GFlowNets at \(\!=\!32\): \(p_{}\) and \(p_{}\) are trained with \(R_{}(x)^{32}\) and \(R_{}(x)^{32}\). (b) harmonic mean of \(p_{}\) and \(p_{}\) at \(\!=\!32\). (c) contrasts at \(\!=\!32\). (d) base GFlowNets at \(\!=\!96\). (e) harmonic mean at \(\!=\!96\). Lines show the contours of the level sets of the kernel density estimates in the (\(R_{}\), \(R_{}\)) plane.**

Figure 2: **Composed GFlowNets on \(32 32\) grid domain. (Top) operations on two distributions. (Bottom) operations on three distributions. Cell probabilities are shown with color, darker is higher. Red circles indicate the high probability regions of \(p_{1}\), \(p_{2}\), \(p_{3}\).**

[MISSING_PAGE_FAIL:9]

three diffusion models trained to generate MNIST  digits \(\{0,1,2,3\}\) in two colors: cyan and bige. Each model was trained to generate digits with a specific property: \(p_{1}\) generated cyan digits, \(p_{2}\) generated digits less than \(2\), and \(p_{3}\) generated even digits.

We built the composition iteratively via the factorization \((x|y_{1},y_{2},y_{3})(x)(y_{1},y _{2}|x)(y_{3}|x,y_{1},y_{2})\). To this end, we first trained a classifier \((y_{1},y_{2}|x_{t})\) on trajectories sampled from the base models. This allowed us to generate samples from \((x|y_{1},y_{2})\). We then trained an additional classifier \((y_{3}|x,y_{1},y_{2})\) on trajectories from compositions defined by \((y_{1},y_{2})\) to allow us to sample from \((x|y_{1},y_{2},y_{3})\). Further details can be found in Appendix F.3.

Figure 5 shows samples from the pre-trained models and from selected compositions. The negating effect of _not_ conditioning on observations is clearly visible in the compositions using two variables. For example, \((x|y_{1}=1,y_{2}=1)\) only generates cyan 3's. Because there we _do not_ condition on \(p_{2}\) or \(p_{3}\), the composition excludes digits that have high probability under \(p_{2}\) or \(p_{3}\), i.e. those that are less than \(2\) or even. In \((x|y_{1}=1,y_{2}=3)\), cyan even digits have high density under both \(p_{1}\) and \(p_{3}\), but because \(p_{2}\) is not conditioned on, the composition excludes digits less than two (i.e. cyan 0's). Finally, \((x|y_{1}=1,y_{2}=2,y_{3}=3)\) generates only cyan 0's, on which all base models have high density.

## 7 Conclusion

We introduced Compositional Sculpting, a general approach for composing iterative generative models. Compositions are defined through "observations", which enable us to emphasize or de-emphasize the density of the composition in regions where specific base models have high density. We highlighted two binary compositions, harmonic mean and contrast, which are analogous to the product and negation operations defined on EBMs. A crucial feature of the compositions we have introduced is that we can sample from them directly. By extending classifier guidance we are able to leverage the generative capabilities of the base models to produce samples from the composition. Through empirical experiments, we validated our approach for composing diffusion models and GFlowNets on toy domains, molecular generation, and image generation.

Broader impact.We proposed a mathematical framework and methods for the composition of pre-trained generative models. While the primary emphasis of our work is on advancing foundational research on generative modeling methodology and principled sampling techniques, our work inherits ethical concerns associated with generative models such as creation of deepfake content and misinformation dissemination, as well as reproduction of biases present in the datasets used for model training. If not carefully managed, these models can perpetuate societal biases, exacerbating issues of fairness and equity. Our work further contributes to research on the reuse of pre-trained models. This research direction promotes eco-friendly AI development, with the long-term goal of reducing energy consumption and carbon emissions associated with large-scale generative model training.

Figure 5: **Composed diffusion models on colored MNIST.** Samples from 3 pre-trained diffusion models and their various compositions.