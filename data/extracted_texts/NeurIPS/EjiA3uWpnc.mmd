# Equivariant Neural Operator Learning with Graphon Convolution

Chaoran Cheng

University of Illinois Urbana-Champaign

chaoran7@illinois.edu

&Jian Peng

University of Illinois Urbana-Champaign

jianpeng@illinois.edu

###### Abstract

We propose a general architecture that combines the coefficient learning scheme with a residual operator layer for learning mappings between continuous functions in the 3D Euclidean space. Our proposed model is guaranteed to achieve SE(3)-equivariance by design. From the graph spectrum view, our method can be interpreted as convolution on graphons (dense graphs with infinitely many nodes), which we term _InfGCN_. By leveraging both the continuous graphon structure and the discrete graph structure of the input data, our model can effectively capture the geometric information while preserving equivariance. Through extensive experiments on large-scale electron density datasets, we observed that our model significantly outperformed the current state-of-the-art architectures. Multiple ablation studies were also carried out to demonstrate the effectiveness of the proposed architecture.

## 1 Introduction

Continuous functions in the 3D Euclidean space are widely encountered in science and engineering domains, and learning the mappings between these functions has potentially an amplitude of applications. For example, the Schrodinger equation for the wave-like behavior of the electron in a molecule, the Helmholtz equation for the time-independent wave functions, and the Navier-Stokes equation for the dynamics of fluids all output a continuous function spanning over \(^{3}\) given the initial input. The discrete structure like the coordinates of the atoms, sources, and sinks also provides crucial information. Several works have demonstrated the rich geometric information of these data to boost the performance of other machine learning models, e.g., incorporating electron density data to better predict the physical properties of molecules .

It is common that these data themselves have inherently complicated 3D geometric structures. Work on directly predicting these structures, however, remains few. The traditional ways of obtaining such continuous data often rely on quantum chemical computation as the approximation method to solve ODEs and PDEs. For example, the ground truth electron density is often obtained with _ab initio_ methods  with accurate results but an \(N^{7}\) computational scaling, making it prohibitive or inefficient for large molecules. Other methods like the Kohn-Sham density functional theory (KS-DFT)  has an \(N^{3}\) computational scaling with a relatively large error. Therefore, building an efficient and accurate machine learning-based electron density estimator will have a positive impact on this realm.

Similar to the crucial concept of _equivariance_ for discrete 3D scenarios, we can also define equivariance for a function defined on \(^{3}\) as the property that the output transforms in accordance with the transformation on the input data. The equivariance property demonstrates the robustness of the model in the sense that it is independent of the poses of the input structure, thus also serving as an implicit way of data augmentation such that the model is trained on the whole trajectory of the input sample. Equivariance on point clouds can be obtained with vector neuron-based models  andtensor field networks [50; 10]. We notice the close relationship between the tensor field network (TFN) and the equivariance of the continuous functions and also propose our equivariant architecture based on the tensor product.

In this way, we define our task as equivariant neural operator learning. We roughly summarize previous work on operator learning into the following four classes: 1) voxel-based regression (3D-CNN) [47; 40; 4]; 2) coefficient learning with a pre-defined set of basis functions [2; 27; 49; 14]; 3) coordinate-based interpolation neural networks [17; 18]; and 4) neural operator learning [28; 25; 29; 32]. The voxel-based 3D-CNN models are straightforward methods for discretizing the continuous input and output but are also sensitive to the specific discretization . The coefficient learning models provide an alternative to discretization and are invariant to grid discretization. However, as the dimension of the Hilbert space is infinite, this method will inevitably incur errors with a finite set of basis functions. The coordinate-based networks take the raw coordinates as input and use a learnable model to "interpolate" them to obtain the coordinate-specific output. They leverage the discrete structure and provide a strong baseline, but a hard cut-off distance prevents long-distance interaction. The neural operators (NOs) are the newly-emerged architecture specifically tailored for operator learning with strong theoretical bases . However, current NOs are mostly tested only on 1D or 2D data and have difficulty scaling up to large 3D voxels. They also ignore the discrete structure which provides crucial information in many scenarios.

To leverage the advantages of these methods while mitigating their drawbacks, we build our model upon the coefficient learning framework with an additional equivariant residual operator layer that finetunes the final prediction with the coordinate-specific information. A graphical overview of our model architecture is shown in Fig.1. We also provide a theoretical interpretation of the proposed neural operator learning scheme from the graph spectrum view. Similar to its discrete counterpart of graph convolutional network, our proposed model can be viewed as applying the transformation to the spectrum of the continuous feature function, thus can be interpreted as the spectral convolution on a _graphon_, a dense graph with infinitely many and continuously indexable nodes. In this way, we term our proposed model "InfGCN". Our model is able to achieve state-of-the-art performance across several large-scale electron density datasets. Ablation studies were also carried out to further demonstrate the effectiveness of the architecture.

To summarize, our contributions are, 1) we proposed a novel architecture that combines coefficient learning with the coordinate-based residual operator layer, with our model guaranteed to preserve SE(3)-equivariance by design; 2) we provided a theoretical interpretation of our model from the graph spectrum point of view as graphon convolution; and 3) we carried out extensive experiments and ablation studies on large-scale electron density datasets to demonstrate the effectiveness of our proposed model. Our code is publicly available at https://github.com/ccr-cheng/InfGCN-pytorch.

## 2 Preliminary

We use \(=(,)\) to denote the (discrete) graph with the corresponding node coordinates \(\{_{i}\}_{i=1}^{||}\). A continuous function over the region \(\) is also provided as the target feature: \(:\). We also assume that there is an initial feature function \(f_{}:\) either obtained from less accurate methods, a random guess, or some learnable initialization. Formally, given \(f_{} L^{2}()\), which is a square-integrable input feature function over \(\), and the target feature function \( L^{2}()\), we want to learn an operator in the Hilbert space \(:L^{2}() L^{2}()\) to approximate the target function.

Figure 1: Overview of the model architecture. (a) The input molecule with node-wise spherical tensor features. (b) The message passing scheme in InfGCN. \(\) denotes the tensor product of two spherical tensors \(,()\) (Sec.3.3). (c) Coordinate-specific residual operator layer (Sec.3.4). (d) Spherical harmonics. (e) The final prediction combining the expanded basis functions and the residue.

Different from common regression tasks over finite-dimensional vector spaces, the Hilbert space \(L^{2}()\) is infinite-dimensional.

### Equivariance

Equivariance describes the behavior of the model when the input data are transformed. Formally, for group \(G\) acting on \(\) and group \(H\) acting on \(\), for a function \(f:\), if there exists a homomorphism \(:G H\) such that \(f(g)=(g)f()\) holds for all \(g G, X\), then \(f\) is equivariant. Specifically, if \(:g e\) maps every group action to the identity action, we have the definition of _invariance_: \(f(g)=f(), g G, X\).

In this work, we will mainly focus on the 3D Euclidean space with the special Euclidean group SE(3), the group of all rigid transformations. As translation equivariance can be trivially achieved by using only the relative displacement vectors, we usually ignore it in our discussion and focus on rotation (i.e., SO(3)) equivariance. We first define the rotation of a continuous function \(f L^{2}(^{3})\) as \((f)():=f(R^{-1})\), where \(R\) is the rotation matrix associated with the rotation operator \(\). Note that the inverse occurs because we are rotating the coordinate frame instead of the coordinates. In this way, the equivariance condition of an operator \(\) with respect to rotation can be formulated as

\[(f)=(f),\] (1)

For clarity, we will distinguish equivariance and invariance, and use equivariance for functions satisfying Eq.(1).

## 3 Method

### Intuition

Intuitively, we would like to follow the message passing paradigm  to aggregate information from every other point \(\). In our scenario, however, as the nodes indexed by the coordinate \(\) are infinite and continuous, the aggregation of the messages must be expressed as an integral:

\[_{W}f():=_{}W(,)f( )d\] (2)

where \(W:\) is a square-integrable kernel function that parameterizes the source node features \(f()\). There are two major problems regarding the formulation in Eq.(2): 1) unlike the discrete counterpart in which the number of nodes is finite, parameterization of the kernel \(W\) in the continuous setting is hard; and 2) even \(W\) is well-defined, the integral is generally intractable. Some NOs [29; 32] directly approximate the integral with Monte Carlo estimation over all grid points, which makes it harder to scale to voxels. Instead, we follow a similar idea in the coefficient learning methods [2; 27] to define a set of complete basis functions \(\{_{k}()\}_{k=1}^{}\) over \(L^{2}()\). In this way, the feature function can be _expanded_ onto the basis as \(f()=_{k=1}^{}f_{k}_{k}()\) where \(f_{k}\) are the coefficients. We can then parameterize the message passing in Eq.(2) as the coefficient learning with truncation to the \(N\)-th basis. We call such an expansion method _unicentric_ as there is only one basis set for expansion. In theory, as the size of the basis goes to infinite, the above expansion method can approximate any function \(y L^{2}()\) with a diminishing error. In practice, however, using a very large number of bases is often impractical. The geometric information of the discrete graph is also not leveraged.

### Multicentric Approximation

To address the limitation mentioned in the previous subsection, we leverage the discrete graph structure to build a _multicentric_ expansion scheme. We use the node coordinates \(_{u}\) in the discrete graph as the centers of basis sets: \(()=_{u}_{i=1}^{}f_{i,u}_{ i}(-_{u})\). We demonstrated in Appendix B that with some regularity and locality assumptions, the message passing in Eq.(2) can be parameterized as

\[f_{i,u}=_{v}(u)}_{j=1}^{}w_{ij}S_{ij}( _{uv})f_{j,v}\] (3)where \(S_{ij}()=_{}_{i}()_{j}(- )d\) models the interaction between the two displaced basis at centers \(i,j\). The outer summation is over \(}(u)=(u)\{u\}\), the set of neighboring centers of \(u\) including \(u\), and \(w_{ij}\) are learnable parameters. Note that, once the basis functions are assigned, \(S_{ij}\) only depends on \(\), but it is generally hard to obtain the closed-form expressions. We can use neural nets to approximate it and coalesce the weight parameter into the nets.

The integral \(S_{ij}()\) is often referred to as the _overlap integral_ in quantum chemistry. The basis functions can be viewed as the atomic orbitals and, in this way, the integral can therefore be interpreted as the overlap between two displaced atom-centered electron clouds. The evaluation of the overlap integral is important in the self-consistent field method (Hartree-Fock method) .

### Equivariant Message Passing

We will now consider the functions on the 3-dimensional Euclidean space, i.e., \(=^{3}\), as they are widely encountered in practical applications and non-trivial to achieve equivariance. It is not easy to find a set of _equivariant basis_ that satisfies Eq.(1). Inspired by the atomic orbitals used in quantum chemistry, we construct the basis function with a Gaussian-based radial function \(R_{n}^{}(r)\) and a spherical harmonics \(Y_{}^{m}(})\):

\[_{n m}()=R_{n}^{}(r)Y_{}^{m}(})=c_{ n}(-a_{n}r^{2})r^{}Y_{}^{m}(})\] (4)

where \(r=||\) is the vector length and \(}=/r\) is the direction vector on the unit sphere. \(c_{n}\) are normalizing constants such that \(_{^{3}}|_{n m}()|^{2}dV=1\). The degree of the spherical harmonics \(\) takes values of non-negative integers, and the order \(m\) takes integers values between \(-\) and \(\) (inclusive). Therefore, there are \(2+1\) spherical harmonics of degree \(\). In this way, the basis index \(i,j\) are now triplets of \((n,,m)\).

To further incorporate the directional information, we follow the Tensor Field Network  to achieve equivariance based on the tensor product. Note that for any index pair \((n_{1}_{1}m_{1},n_{2}_{2}m_{2})\), the overlap integral \(S()\) can also be expanded onto the basis as \(S()=_{n m}s_{n m}_{n m}()=:_{n m }_{n m}()\)1. For a fixed \(\) and radial index \(n\), the coefficient sequence \(=\{_{ m}: 0,- m\}\) can be viewed as a _spherical tensor_. Notice that the node feature \(=\{^{}: 0\}\) can also be viewed as a spherical tensor. In the following discussion, we will omit the radial function index \(n\) for clarity as it is independent of rotation. TFN leverages the fact that the spherical harmonics span the basis for the irreducible representations of SO(3) and the tensor product of them produces equivariant spherical tensors. The message passing scheme in TFN is defined as:

\[_{u}^{}_{v}(u)}_{k 0 }W^{ k}(_{v}-_{u})_{v}^{k}, W^{ k} ()=_{J=|k-|}^{k+}_{J}^{ k}(r)_{m=-J}^{J}Y _{J}^{m}(})Q_{Jm}^{ k}\] (5)

where \(Q_{Jm}^{ k}\) is the Clebsch-Gordan matrix of shape \((2+2)(2k+1)\) and \(_{J}^{ k}:^{+}\) are learnable radial nets that constitute part of the edge tensor features. A detailed deduction is provided in Appendix A. Intuitively, as TFN can achieve equivariance for spherical tensors, the output spherical tensor interpreted as the coefficients should also give an equivariant continuous function \(()\). Indeed we have

**Theorem**.: _Given an equivariant continuous input, the message passing defined Eq.(5) gives an equivariant output when interpreted as coefficients of the basis functions._

A rigorous proof of rotation equivariance can be found in Appendix A. The translation equivariance also trivially holds as we only use the relative displacement vector \(_{uv}=_{v}-_{u}\). The equivariance of this scheme relies on the equivariance of the input feature map \(_{}\). Note that the 0-degree features that correspond to pure Gaussians are isotropic, so we can use these features as the initial input. In practice, we use atom-specific embeddings to allow more flexibility in our model.

Also note that for \(v=u\), the message passing can be simplified. As the spherical harmonics are orthogonal, the overlap integral is non-zero only if \(m_{1}=m_{2}\). Therefore,

\[_{u}^{}=w^{}_{u}^{}+_{v(u)} _{k 0}W^{ k}(_{v}-_{u})_{v}^{k}\] (6)The first term is referred to as _self-interaction_ in previous papers [50; 44], but can be naturally inferred from our message passing scheme. For the nonlinearity, we follow  to use the vector norm of each degree of vector features:

\[f^{0}=_{0}(f^{0}),^{}=_{}(\|^{ }\|_{2})^{}\] (7)

where \(_{k}\) are the activation functions. The equivariance holds as the vector norm is invariant to rotation. Also, to avoid over-parametrization and save computational resources, we only consider the interactions within the same radial index: \(_{n m,n^{}^{}m^{}}():=_{nn^{ }}S_{mm^{},^{}}()\). Note that this assumption generally does not hold even for orthogonal radial bases, but in practice, the model was still able to achieve comparable and even better results (Sec.5.3).

### Residual Operator Layer

The dimension of the function space is infinite, but in practice, we can only use the finite approximation. Therefore, the expressiveness of the model will be limited by the number of basis functions used. Also, as the radial part of the basis in Eq.(4) is neither complete nor orthogonal, it can induce loss for the simple coefficient estimation approach. To mitigate this problem, we apply an additional layer to capture the residue at a given query point \(p\) at coordinate \(\). More specifically, the residual operator layer aggregates the neighboring node features to produce an invariant scalar2 to finetune the final estimation:

\[z()=_{v(p)}_{k 0}W_{}^{k}( _{v}-)_{v}^{k}\] (8)

This scheme resembles the coordinate-based interpolation nets and was proved effective in our ablation study (Sec.5.3). Therefore, the final output function is

\[()=_{n m}f_{n m}_{n m}()+z( )\] (9)

The equivariance of the residual operator layer as well as in the finite approximation case is also provided in Appendix A. The loss function can be naturally defined with respect to the norm in \(L^{2}(^{3})\) as \(=\|-\|_{2}^{2}=_{^{3}}|( )-()|^{2}d\).

## 4 Graph Spectral View of InfGCN

Just as the Graph Convolutional Network (GCN)  can be interpreted as the spectral convolution of the discrete graph, we also provide an interpretation of InfGCN as the transformation on the _graphon_ spectra, thus leading to a similar concept of _graphon convolution_. We will first introduce the (slightly generalized) concept of graphon. Defined on region \(\), a _graphon_, also known as graph limit or graph function, is a symmetric square-integrable function:

\[W:,_{^{2}}|W( ,)|^{2}dd<\] (10)

Intuitively, the kernel \(W(,)\) can be viewed as the probability that an edge forms between the continuously indexable nodes \(,\). Now, consider the operator \(_{W}\) defined in Eq.(2). As the integral kernel \(W\) is symmetric and square-integrable, we can apply the spectral theorem to conclude that the operator \(_{W}\) it induces is a self-adjoint operator whose spectrum consists of a countable number of real-valued eigenvalues \(\{_{k}\}_{k=1}^{}\) with \(_{k} 0\). Let \(\{_{k}\}_{k=1}^{}\) be the eigenfunctions such that \(_{W}_{k}=_{k}_{k}\). Similarly to the graph convolution for discrete graphs, any transformation on the eigenvalues \(:\{_{k}\}_{k=1}^{}\{_{k}\}_{k=1}^{}\) can be viewed as the _graphon convolution_ back in the spatial domain. We note that GCN uses the polynomial approach to filter the graph frequencies as \(H=_{k=0}^{K}w_{k}L^{k}\) where \(w_{k}\) are the parameters. Define the power series of \(_{W}\) as:

\[_{W}^{n}f()=_{W}_{W}^{n-1}f( )=_{}W(,)_{W}^{n-1}f( )d,_{W}^{0}=\] (11)

where \(\) is the identity mapping on \(\). A graphon filter can be then defined as \(f=_{k=0}^{}w_{k}_{W}^{k}f\). We can also follow GCN to use the Chebyshev polynomials to approximate the graphon filter \(\):

\[f_{1}f+_{2}_{W}f\] (12)Just as the message passing on discrete graphs can be viewed as graph convolution, we point out here that any model that tries to approximate the continuous analog \(_{W}f\) as defined in Eq.(2) can also be viewed as _graphon convolution_. This includes InfGCN, all NOs, and coefficient learning nets. A more formal statement using the graph Fourier transform (GFT) and the discrete graph spectral theory are provided in Appendix B for completeness.

Another related result was demonstrated by Tsubaki et al.  that the discrete graph convolution is equivalent to the linear transformation on a poor basis function set, with the hidden representation being the coefficient vectors and the weight matrix in GCN being the basis functions. As we have shown above, the same argument can be easily adapted for graphon convolution that the message passing in Eq.(6) can be also viewed as the linear combination of atomic orbitals (LCAO)  in traditional quantum chemistry.

Furthermore, based on Eq.(3), we can now give a more intuitive interpretation of the radial network in TFN: it captures the magnitude of the radial part of the overlap integral \(S()\) of the basis in Eq.(4). From the point convolution aspect, the TFN structure can be also considered a special case of our proposed InfGCN model. The discrete input features can be regarded as the summation of Dirac measures over the node coordinates as \(f_{}()=_{u}f_{u}(-_{u})\).

## 5 Experiments

We carried out extensive experiments on large-scale electron density datasets to illustrate the state-of-the-art performance of our proposed InfGCN model over the current baselines. Multiple ablation studies were also carried out to demonstrate the effectiveness of the proposed architecture.

### Datasets and Baselines

We evaluated our model on three electron density datasets. As computers cannot truly store continuous data, all datasets provide the electron density in a volumetric form on a pre-defined grid. Atom types and atom coordinates are also available as discrete features.

**QM9**. The QM9 dataset [41; 39] contains 133,885 species with up to nine heavy atoms (CONF). The density data as well as the data split come from [17; 18], which gives 123835 training samples, 50 validation samples, and 10000 testing samples.

**Cubic**. This large-scale dataset contains electron densities on 17,418 cubic inorganic materials . In our experiment setting, we first filtered out the noble gas (He, Ne, Ar, Kr, Xe) and kept only the crystal structure whose primitive cell contains less than 64 atoms. This gave 16,421 remaining data points. A data split of 14421, 1000, and 1000 for train/validation/test was pre-assigned.

**MD**. The dataset contains 6 small molecules (ethanol, benzene, phenol, resorcinol, ethane, malonaldehyde) with different geometries sampled from molecular dynamics (MD). The former 4 molecules are from  with 1000 sampled geometries each. The latter two are from  with 2000 sampled geometries each. The models were trained separately for each molecule.

To evaluate the models, we followed  to define the _normalized mean absolute error_ (NMAE) as our evaluation metrics:

\[=^{3}}|()-() |d}{_{^{3}}|()|d}\] (13)

To avoid randomness, different from the sampling evaluation scheme in , we did the evaluation on the partitioned mini-batches of the full density grid. Also, to demonstrate the equivariance of InfGCN, the density and atom coordinates were randomly rotated during inference for the QM9 dataset. The rotated density was sampled with trilinear interpolation from the original grid. Equivariance is trivial for crystal data, as there is a canonical way of assigning the primitive cell. Similarly, for the MD dataset, the authors described canonical ways to align the molecules [1; 2], so we also did not rotate them. More details regarding the datasets can be found in Appendix C.

We compared our proposed model with a wide range of different baselines including **CNN**[40; 4], interpolation networks (**DeepDFT**, **DeepDFT2**, **EGNN**, **DimeNet**, **DimeNet++**), and neural operators (**GNO**, **FNO**, **LNO**). For InfGCN, we set the maximal degree of spherical tensors to \(L=7\), with 16 radial basis and 3 convolution layers. For CNN and 

[MISSING_PAGE_FAIL:7]

### Ablation Study

To further demonstrate the effectiveness of InfGCN, we also carried out extensive ablation studies on various aspects of the proposed architecture on the QM9 dataset. The results are summarized in Table 2 and are also demonstrated in Figure 3 in blue and green.

**Number of spherical basis.** For coefficient learning models, using more basis functions will naturally lead to a more expressive power of the model. For discrete tasks,  used only the degree-1 spherical tensor which corresponds to vectors. We ran experiments with the maximal degree of the spherical tensor \(0 L 7\) (\(s_{L}\) columns). Note that \(s_{0}\) corresponds to atom-centered Gaussian mixtures. It can be shown in Figure 3 (in blue) that the error smoothly drops as the maximal degree increases. Nonetheless, the performance gain is not significant with \(L 4\). This is probably because the residual operator layer can effectively finetune the finite approximation error and therefore allows for the trade-off between performance and efficiency. In this way, our proposed InfGCN can potentially scale up to larger datasets with an appropriate choice of the number of spherical basis.

**Residue prediction.** The residue prediction layer is one of the major contributions of our model that tries to mitigate the finite approximation error. It can be shown (under no-res) that this design significantly improves the performance by nearly 2% with negligible increases in the model size and training time. These results justify the effectiveness of the residue prediction scheme.

**Fully-connected tensor product.** As mentioned in Sec.3.3, we used a channel-wise tensor product instead a fully connected one that allows inter-channel interaction. We also tried the fully-connect tensor product under fc. It turns out that the fully-connected model was 15 times larger than the original model and took 2.5 times as long as the latter to train. The results, however, are even worse, probably due to overfitting on the training set.

## 6 Related Work

### Neural Operator Learning

We use the term _neural operator_ in a wider sense for any model that outputs continuous data here. For modeling 3D densities, statistical approaches are still widely used in quantum chemistry realms. For example,  and  used kernel ridge regression to determine the coefficients for atomic orbitals.  used a symmetry-adapted Gaussian process regression for coefficient estimation. These

   Model & InfGCN(\(s_{7}\)) & \(s_{6}\) & \(s_{5}\) & \(s_{4}\) & \(s_{3}\) & \(s_{2}\) & \(s_{1}\) & \(s_{0}\) & no-res & fc \\  QM-rot (\%) & **4.73** & 4.77 & 4.76 & 4.77 & 4.86 & 6.95 & 9.56 & 12.62 & 6.14 & 4.95 \\ QM-unrot (\%) & **0.93** & 1.01 & 1.11 & 1.08 & 1.46 & 4.65 & 8.07 & 12.05 & 3.72 & 1.36 \\   Parameters (M) & 1.20 & 0.85 & 0.58 & 0.39 & 0.26 & 0.17 & 0.13 & 0.11 & 1.16 & 17.42 \\   

Table 2: NMAE (%) and the parameter count of different model settings on the QM9 dataset.

Figure 3: Plot of model sizes vs NMAE for different models and ablation studies on the QM9 dataset.

traditional methods are able to produce moderate to good results but are also less flexible and difficult to scale. For machine learning-based methods,  utilized a voxel-based 3D convolutional net with a U-Net architecture  to predict density at a voxel level. Other works leveraged a similar idea of multicentric approximation.  and  all designed a tensor product-based equivariant GNN to predict the density spectra. These works are more flexible and efficient, but coefficient learning models inevitably have finite approximation errors.

Another stream of work on neural operator learning focused on directly transforming the discretized input. Tasks of these models often involve solving PDE or ODE systems in 1D or 2D scenarios. For example,  proposed the infinite-layer network to approximate the continuous output. Graph Neural Operator  approximated the operator with randomly sampled subgraphs and the message passing scheme.  and  tried to parameterize and learn the operator from the Fourier domain and spectral domain, respectively.  proposed an analog to the U-Net structure to achieve memory efficiency. These models are hard to scale to larger 3D data and are also sensitive to the partition of the grid if a sampling scheme is required. They also do not leverage the discrete structure.

### Interpolation Networks

The term _interpolation network_ was coined in  for models that take raw query coordinates as input. As graph neural networks have achieved tremendous success in discrete tasks, they are usually the base models for interpolation nets.  and  constructed the molecule graph to perform variant message passing schemes with the final query-specific prediction.  proposed the DeepDFT model which also considered the graph between query coordinates and  further extended it to use a locally equivariant GNN .  proposed a similar model on crystalline compounds. Besides these specialized models, we also point out that current equivariant models for discrete graphs can all be adapted for continuous tasks in principle, just like DimeNet and DimNet++ that we used as the baselines. Models that use only the invariant features including distance, angles, and dihedral angles can be trivially equivariant but lacking expressiveness [7; 44; 5; 23]. [16; 15] proposed the GVP model in which features are partitioned into scalars and vectors with carefully designed interaction to guarantee equivariance. Other works leveraged the canonical local frame  or tried to learn such a local frame . Another line of works, the tensor field network [50; 10], utilized the group theoretical results of the irreducible representations of SO(3) and proposed a tensor product based architecture. We follow the last method as we notice the close relationship between the spherical tensor and the basis set. Though previous works with similar architecture exist [27; 3], we first give rigorous proof of the equivariance of the continuous function.

### Downstream Applications

Several previous works tried to leverage the geometric information of the continuous function.  utilized the charge density and spin density as both the supervising signal and the additional input for predicting molecular energies, which achieved a significant performance improvement compared to traditional DFT-based methods. [51; 1] first projected the density onto the pre-defined basis set and then applied different neural nets on the coefficients to make predictions on downstream tasks.  used a 3D CNN to encode the electron density of the protein complex to predict the backbone structure. These works have demonstrated the significance of continuous data.

## 7 Limitation and Conclusion

In this paper, we introduce a novel equivariant neural operator learning architecture with the core component interpretable as the convolution on graphons. With extensive experiments, we demonstrated the effectiveness and generalizability of our model. We also discuss the limitation and potential improvement of the proposed InfGCN model in future work. As the choice of the radial basis is arbitrary, there is no theory or criterion for a better radial basis, and therefore, it leaves space for improvement. For example, we may use Slater-type orbitals (STO) instead of Gaussian-type orbitals. We may further orthogonalize the basis, which leads to the series of solutions to the Schrodinger equation for the hydrogen-like atom with more direct chemical indications. For structures with periodic boundary conditions, Fourier bases may provide a better solution. A learnable radial basis parameterized by a neural net is also a feasible option to provide more flexibility.