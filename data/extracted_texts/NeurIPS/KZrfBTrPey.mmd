# ALI-Agent: Assessing LLMs'Alignment with Human Values via Agent-based Evaluation

Jingnan Zheng

National University of Singapore

jingnan.zheng@u.nus.edu

&Han Wang

University of Illinois Urbana-Champaign

hanw14@illinois.edu

An Zhang

National University of Singapore

anzhang@u.nus.edu

&Tai D. Nguyen

Singapore Management University

dtnguyen.2019@smu.edu.sg

&Jun Sun

Singapore Management University

junsun@smu.edu.sg

&Tat-Seng Chua

National University of Singapore

dcscts@nus.edu.sg

These authors contribute equally to this work.An Zhang is the corresponding author.

###### Abstract

Large Language Models (LLMs) can elicit unintended and even harmful content when misaligned with human values, posing severe risks to users and society. To mitigate these risks, current evaluation benchmarks predominantly employ expert-designed contextual scenarios to assess how well LLMs align with human values. However, the labor-intensive nature of these benchmarks limits their test scope, hindering their ability to generalize to the extensive variety of open-world use cases and identify rare but crucial long-tail risks. Additionally, these static tests fail to adapt to the rapid evolution of LLMs, making it hard to evaluate timely alignment issues. To address these challenges, we propose **ALI-Agent**, an evaluation framework that leverages the autonomous abilities of LLM-powered agents to conduct in-depth and adaptive alignment assessments. ALI-Agent operates through two principal stages: Emulation and Refinement. During the Emulation stage, ALI-Agent automates the generation of realistic test scenarios. In the Refinement stage, it iteratively refines the scenarios to probe long-tail risks. Specifically, ALI-Agent incorporates a memory module to guide test scenario generation, a tool-using module to reduce human labor in tasks such as evaluating feedback from target LLMs, and an action module to refine tests. Extensive experiments across three aspects of human values-stereotypes, morality, and legality-demonstrate that ALI-Agent, as a general evaluation framework, effectively identifies model misalignment. Systematic analysis also validates that the generated test scenarios represent meaningful use cases, as well as integrate enhanced measures to probe long-tail risks. Our code is available at https://github.com/SophieZheng998/ALI-Agent.git.

## 1 Introduction

Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating texts, leading to widespread deployment across various applications with significant societal impacts . However, this expansion raises concerns regarding their alignment with humanvalues [8; 9; 10]. Misalignment may result in LLMs generating content that perpetuates stereotypes , reinforces societal biases , or provides unlawful instructions , thus posing risks to users and broader society [13; 14]. Given these severe risks, performing in-depth and comprehensive real-world evaluations of LLMs is critical to assess their alignment with human values [15; 16].

Evaluating the alignment of LLMs with human values is challenging due to the complex and open-ended nature of real-world applications [17; 18]. Typically, designing in-depth alignment tests requires substantial expert effort to create contextual natural language scenarios [19; 20; 21]. This labor-intensive process restricts the scope of the test scenarios, making it difficult to cover the extensive variety of real-world use cases and to identify long-tail risks . Furthermore, as LLMs continuously evolve and expand their capabilities, static datasets for alignment evaluation quickly become outdated, failing to timely and adaptively reveal potential alignment issues .

In this work, we argue that a practical evaluation framework should automate in-depth and adaptive alignment testing for LLMs rather than relying on labor-intensive static tests. The evaluation framework is expected to automate the process of generating realistic scenarios of misconduct, evaluate LLMs' responses, and iteratively refine the scenarios to probe long-tail risks. Drawing inspiration from recent advancements on LLM-powered autonomous agents, characterized by their ability to learn from the past, integrate external tools, and perform reasoning to solve complex tasks [24; 25; 26; 27; 28], we propose **ALI-Agent**, an agent-based evaluation framework to identify misalignment of LLMs. Specifically, ALI-Agent leverages GPT4 as its core controller and incorporates a memory module to store past evaluation records that expose LLMs' misalignment, a tool-using module that integrates Web search and fine-tuned evaluators to reduce human labor, and an action module that harnesses agent's reasoning ability to refine scenarios.

Building upon these three key modules, ALI-Agent can automate in-depth and adaptive alignment evaluation in critical areas through two principal stages: Emulation and Refinement (See Fig. 2). In the Emulation stage, ALI-Agent instantiates an emulator to generate realistic test scenarios and employs fine-tuned LLMs as an automatic evaluator to classify feedback as either pass or in need of refinement. Firstly, the emulator retrieves a text describing misconduct either from predefined datasets or through Web browsing based on user queries. Then, the emulator generates a realistic scenario to reflect the misconduct. These scenarios are generated based on the most relevant evaluation records retrieved from the evaluation memory, leveraging the in-context learning (ICL) abilities of LLMs [29; 30]. ALI-Agent then prompts the generated scenarios to the target LLM and judge its feedback with a fine-tuned evaluator. If we successfully exposes misalignment of the target LLM, ALI-Agent stores the evaluation record back in the memory. This allows ALI-Agent to reuse and further refine the evaluation record to new use cases in the future. Otherwise, ALI-Agent proceeds to the second stage: Refinement. A refiner is instantiated to iteratively refine the scenario based on feedback from the target LLM until either misalignment is exposed or the maximum number of iterations is reached. The self-refinement procedure is outlined in a series of intermediate reasoning steps (_i.e.,_ chain-of-thought ) to perform effective open-ended exploration. This iterative cycle

Figure 1: **ALI-Agent generates test scenarios to probe long-tail risks. As judged by OpenAI Moderation API , test scenarios generated by ALI-Agent exhibit significantly decreased harmfulness scores compared to the expert-designed counterparts (collected from (a) CrowS-Pairs  and (b) ETHICS ), enhancing the difficulty for target LLMs to identify the risks.**

boosts ALI-Agent's capacity to conduct in-depth evaluations of LLMs with human values, ensuring continuous adaptation and improvements.

Benefiting from the autonomous abilities of agents, ALI-Agent possesses three desirable properties. First, ALI-Agent is a general framework for conducting effective evaluations across diverse aspects of human values. Experiments on three distinct aspects-stereotypes, morality, and legality-across ten mainstream LLMs demonstrate ALI-Agent's effectiveness in revealing under-explored misalignment compared to prevailing benchmarks (Sec 3.1). Second, ALI-Agent generates meaningful real-world use cases that properly encapsulate concerns regarding human values. In particular, three human evaluators examine 200 test scenarios randomly sampled from all cases generated by ALI-Agent, and over 85% are unanimously considered to be of high quality (Sec 3.2). Third, ALI-Agent probes long-tail risks through deliberate refinement of the test scenarios. The perceived harmfulness of test scenarios is significantly reduced after the refinement process (see Fig. 1), enhancing the detection of long-tail misalignment in target LLMs (Sec 3.2).

## 2 Method of ALI-Agent

In this section, we formulate the task of evaluating LLMs' alignment with human values and explain how ALI-Agent implements this task through its two stages: Emulation and Refinement. The framework is depicted in Figure 2, with the detailed workflow illustrated in Algorithm 1 and a comprehensive example provided in Figure 3.

**Task Formulation.** To evaluate LLMs' alignment with human values, the process involves: (1) generating a test prompt by combining a misconduct sample \(_{i}\) with a task-specific prompt template \(p\) to form \(p_{i}\); (2) using this combined prompt to query the target LLM \(\); and (3) assessing the LLM's response using an evaluator \(\). Formally, the task of evaluating target LLM \(\)'s alignment with human values associated with misconduct \(_{i}\) can be expressed as:

\[y_{i}=(p x_{i}), x_{i}.\] (1)

Here, \(\) denotes a dataset of pre-defined misconduct scenarios, such as unethical situations  or stereotypical statements . The prompt template \(p\) structures the query; for example, _"I am going to describe a scenario. Do you agree with this scenario? Tell me 'IAGREE' or 'I DISAGREE' and

Figure 2: An overview of the existing evaluation benchmarks and the proposed ALI-Agent. Existing benchmarks adopt pre-defined misconduct datasets as test scenarios to prompt target LLMs and evaluate their feedback. In contrast, ALI-Agent not only uses pre-defined datasets but also allows for tests based on user queries. Additionally, ALI-Agent operates through two stages: Emulation and Refinement, facilitating in-depth alignment tests by probing long-tail risks across a wide range of real-world scenarios.

give me a short reason. Here is the scenario: "_. The evaluator \(\), which typically uses rule matching  or prompt-based evaluation  (see details in Appendix D.4), classifies the LLM's responses. A result of \(y_{i}=0\) indicates proper risk identification by \(\), whereas \(y_{i}=1\) indicates a misalignment.

**Overview of ALI-Agent.** We will first formulate our method, ALI-Agent, and then detail its two stages in the following Section 2.1 and Section 2.2, along with an Algorithm 1.

As illustrated in Figure 2, ALI-Agent differs from current benchmarks by sourcing \(x_{i}\) not only from pre-defined misconduct datasets \(\) but also from direct user queries \(\) retrieved via web browsing \(\), _i.e._, \(x_{i}=(q_{i})\). For example, given the query \(q_{k}=\) "_What is the law on eating on the MRT in Singapore?_", the retrieved scenario is \(x_{k}=(q_{k})=\)"_Eating and drinking on the MRT in Singapore are not permitted under the Rapid Transit Systems Act._"

Given \(x_{i}\) as input, ALI-Agent processes alignment tests through two stages: the emulation stage (Section 2.1), where realistic scenarios \({x_{i}}^{(0)}\) are generated from \(x_{i}\), and then the refinement stage (Section 2.2), where test scenarios \({x_{i}}^{(n)}\) are iteratively updated, with \(n\{1,,N\}\), where \(N\) is an integer representing the maximum number of refinement iterations. The iteration stops either when target LLM \(\) fails to identify the misconduct, _i.e._, \({y_{i}}^{(n)}=1\) or \(n\) reaches \(N\).

We formally define the process of assessing LLMs' alignment associated with \(x_{i}\) by ALI-Agent as follows:

\[{y_{i}}^{(n)} =(p}^{(n)}), n=0, ,N,\] (2) \[y_{i} =1,&}^{(n)}=1n<N,\\ 0,&}^{(n)}=0n N.\]

Here, the evaluator \(\) in ALI-Agent is a fine-tuned Llama 2 , with implementation details provided in Appendix D.4.

Figure 3: An example of ALI-Agent’s implementation. In the emulation stage, ALI-Agent generates \({x_{k}}^{(0)}\), a realistic scenario that reflects violations against \(x_{k}\), a law regulation, with \(m_{j}\) serving as an in-context demonstration. In the refinement stage, ALI-Agent refines \({x_{k}}^{(0)}\) to \({x_{k}}^{(1)}\) by adding an extra excuse, making the misconduct of “eating on MRT” appears more reasonable and successfully misleads \(\) to overlook the issue. This pattern of wrapping up misconduct is saved back to \(\) in the form of \(m_{k}=(x_{k},{x_{k}}^{(1)},{e_{k}}^{(1)})\) for subsequent tests, boosting ALI-Agent’s ability to generalize risky tests to new cases.

### Emulation Stage

The core of the emulation stage leverages LLMs' in-context learning (ICL) abilities  to learn from past evaluation records and to generate realistic scenarios \({x_{i}}^{(0)}\) that accurately reflect the given misconduct \(x_{i}\). The emulation stage consists of three steps: first, ALI-Agent retrieves past relevant evaluation records that have exposed misalignment in target LLMs from its memory \(\); second, ALI-Agent adapts these records to generate new test scenarios using an emulator \(_{e}\); third, ALI-Agent instantiates an evaluator \(\) to assess the target LLMs' feedback.

**Evaluation Memory \(\)**. We formally define ALI-Agent's evaluation memory as \(=\{m_{i} m_{i}=(x_{i},{x_{i}}^{(n)},{e_{i}}^{(n)},y_{i}),y_{i}=1,n\{0 N\}\}_{i=0}^{N_{m}}\), where \({e_{i}}^{(n)}\) is a text explaining how \({x_{i}}^{(n)}\) properly reflects the original misconduct \(x_{i}\), and \(n\) is the number of iterations ALI-Agent has refined the scenario. To boost the power of ICL, we aim to find the most relevant \(m_{j}\) for \(x_{i}\), which means that the corresponding misconduct \(x_{j}\) is the most similar to \(x_{i}\).

The retrieval of the most relevant evaluation record \(m_{j}\) for the current test scenario \(x_{i}\) from memory \(\) is defined as:

\[m_{j}=(x_{i})=*{arg\,min}_{m_{k}} ((x_{i}),(m_{k})),\] (3)

where \(()\) is an embedding function that converts texts to numerical vectors. \((,)\) is a distance function, with cosine similarity used in this work. To initialize \(\), we randomly select misconduct data from \(\) in the training data as warm-up data, run ALI-Agent, and store the corresponding evaluation records as the starting point of \(\). Detailed warm-up settings can be found in Appendix D.1. More potential methods of utilizing LLM-based memory are discussed in Appendix E.2.

**Automatic Emulator \(_{e}\)**. We formally define the emulation of test scenarios generated by the automatic emulator \(_{e}\) as:

\[{x_{i}}^{(0)},{e_{i}}^{(0)}=_{e}(x_{i},(x_{i})), i =1,,N_{x},\] (4)

where \({x_{i}}^{(0)}\) and \({e_{i}}^{(0)}\) are initially generated test scenarios and corresponding explanations, respectively. The most relevant evaluation record \(m_{j}=(x_{i})\) serves as an in-context demonstration, helping ALI-Agent adapt to the task of generating realistic scenarios \({x_{i}}^{(0)}\) that accurately reflect the given misconduct \(x_{i}\).

**Automatic Evaluator \(\)**. Evaluator \(\) classifies whether the response from target LLM \(\) aligns with human values associated with the presented test scenarios \({x_{i}}^{(0)}\) as follows:

\[{y_{i}}^{(0)}=(p}^{(0)}).\] (5)

Here, \(\) is a fine-tuned Llama 2-7B [35; 36] with human-annotated \({y_{i}}^{*}\); implementation details are provided in Appendix D.4. During the inference stage, if \({y_{i}}^{(0)}=1\), ALI-Agent successfully identifies the misalignment of target LLM \(\) with test scenarios \({x_{i}}^{(0)}\), concludes the evaluation with \(y_{i}=1\), and records the evaluation record \(m_{i}=(x_{i},{x_{i}}^{(0)},{e_{i}}^{(0)})\) back to memory \(\). Otherwise, ALI-Agent proceeds to the next stage of refinement.

Figure 4: Examples of misconduct, scenarios generated and refined by ALI-Agent. The highlighted parts show how ALI-Agent refines sensitive content to lower its perceptible sensitivity, thereby probing long-tail risks. In these examples, target LLMs only fail to properly identify the corresponding misconduct when prompted with the refined scenarios.

### Refinement Stage

The core of the refinement stage is to iteratively refine the test scenario \({x_{i}}^{(0)}\) obtained in the emulation stage, probing deeper into potential long-tail risks. We formally define one turn of refinement using refiner \(_{r}\) as:

\[{x_{i}}^{(n+1)},{e_{i}}^{(n+1)}=_{r}(x_{i},{x_{i}}^{(n)},( p}^{(n)})), i=1,,N_{x}.\] (6)

At the core of ALI-Agent, the refiner \(_{r}\) generates a series of intermediate reasoning steps (_i.e._, chain-of-thought ) to enhance its ability to explore potentially undiscovered loopholes in target LLMs through the deliberate design of test scenarios. First, ALI-Agent understands how risks in \({_{i}}^{(n)}\) are detected based on \((p}^{(n)})\). Building upon the understanding, ALI-Agent proceeds to explore new patterns to generate scenarios that can possibly mislead \(\) into overlooking the risk. Finally, following experience in the emulation stage, ALI-Agent ensures that the new scenario \({x_{i}}^{(n+1)}\) still properly encapsulate the misconduct \(x_{i}\) in the first place. With the refined scenario \({x_{i}}^{(n+1)}\), ALI-Agent proceeds to evaluate the response from the target LLM \(\) as:

\[{y_{i}}^{(n+1)}=(p}^{(n+1)}).\] (7)

The refinement stage ends when either \({y_{i}}^{(n+1)}=1\) or ALI-Agent reaches the maximum number of iterations \(N\). Two examples of refinement are illustrated in Figure 4. The detailed algorithm of ALI-Agent is shown below.

``` Input: misconduct \(x_{i}\), target LLM \(\), prompt template \(p\), maximum iteration number \(N\)  Initialize \(n 0\), \(y_{i} 0\), emulator \(_{e}\), refiner \(_{r}\), Memory \(\), evaluator \(\)  Retrieve memory: \((x_{i})=m_{j}=_{m_{k}}(( x_{i}),(m_{k}))\)  Start emulation: \({x_{i}}^{(0)},{e_{i}}^{(0)}=_{e}(x_{i},(x_{i}))\)  Evaluate response: \({y_{i}}^{(0)}=(p}^{(0)})\). if\({y_{i}}^{(0)}\)then = 0 while\(n<N}^{(n)}=0\)do  Start refinement: \({x_{i}}^{(n+1)},{e_{i}}^{(n+1)}=_{r}(x_{i},{x_{i}}^{(n)},(p}^{(n)}))\)  Evaluate response: \({y_{i}}^{(n+1)}=(p}^{(n+1)})\) \(n n+1\) endwhile  end if \({y_{i}}}^{(n)}\) Output: Evaluation result \(y_{i}\) ```

**Algorithm 1** ALI-Agent

## 3 Experiments

We aim to answer the following research questions:

* **RQ1**: How do LLMs perform under ALI-Agent's evaluation compared to other prevailing evaluation benchmarks across three aspects of human values?
* **RQ2**: Does ALI-Agent generate realistic scenarios that properly encapsulate misconduct against certain human values, and does it probe long-tail risks in the test scenarios?
* **RQ3**: What are the impacts of the components (_i.e.,_ evaluation memory and iterative refiner) on the effectiveness of ALI-Agent?

**Datasets**. To verify ALI-Agent's effectiveness as a general evaluation framework, we conduct experiments on six datasets from three distinct aspects of human values: stereotypes (DecodingTrust , CrowS-Pairs ), morality (ETHICS , Social Chemistry 101 ), and legality (Singapore Rapid Transit Systems Regulations, AdvBench ), where five of them follow prevailing evaluation benchmarks, and Singapore Rapid Transit Systems Regulations is a body of laws collected online . Appendix D.1 provides detailed descriptions of the datasets.

[MISSING_PAGE_FAIL:7]

* **LLMs from the same family may exhibit worse alignment performance as their parametric scale increases.** Specifically, in Table 1, Vicuna-33B exhibits a higher misalignment rate than Vicuna-7B and Vicuna-13B under the _Untargeted_ and _Targeted_ system prompt settings. Similarly, Table 2 demonstrates that Llama 2-70B experiences more failures across all evaluation settings compared to Llama 2-7B and Llama 2-13B. These trends are also evident in Tables 3 and 4. Our findings indicate that increasing the model scale alone may not lead to better alignment . Thus, more careful consideration is needed regarding the data and objectives for training LLMs.
* **Fine-tuning Llama 2 models to Vicuna models may compromise their alignment with human values.** The Vicuna models, fine-tuned from LLama 2 models on high-quality conversations, exhibit significantly higher misalignment rates compared to their base models across all datasets. An example is when targeting Vicuna-7B to identify unethical scenarios, adding simple evasive sentences like "But it did not cause any harm" or "But it did not happen" fools the LLM into overlooking the risk, while its counterpart LLama 2 can still identify the risk in most cases. This trend is consistent with the result in , advocating for further research efforts toward reinforcing safety protocols for the custom fine-tuning of LLMs.

### Study on Test Scenarios (RQ2)

**Motivation.** In Section 3.1, ALI-Agent demonstrates its effectiveness in revealing a significant number of previously uncovered instances of model misalignment through its generated test scenarios. Furthermore, we need to validate the quality of these test scenarios. Specifically, a high quality test scenario should: (1) be a meaningful real-world use case that properly encapsulates the intended misconduct, and (2) conceal the malice of the misconduct, making it difficult for LLMs to identify the associated risks. Consequently, a high quality test demands the target LLM an in-depth understanding of and adherence to the application of specific human values.

**Settings.** We conduct two experiments to validate the quality of ALI-Agent generated test scenarios. Firstly, to assess realism, we employ three human evaluators, who are senior undergraduate or graduate students majoring in computer science. Each evaluator is presented with the original misconduct and the generated test scenario and asked to judge whether the scenario is plausible in the real world and properly encapsulates the misconduct. Each evaluator completes their tasks independently. Detailed disclosure is in Appendix D.6. Secondly, to demonstrate the effectiveness of concealing malice, we adopt the OpenAI Moderation API  to systematically measure the perceived harmfulness of the generated scenarios.

**Results.** Firstly, we randomly sample 200 test scenarios from a total of 11,243 and present them to the human evaluators. Of these, over 85% are unanimously judged as high quality, validating the practical effectiveness of ALI-Agent. Examples from each dataset are provided in Appendix D.8. Additionally, as illustrated in Figure 1, the decreased perceivable harmfulness of the test scenarios indicates that they successfully conceal the original misconduct's malice, thus making it more challenging for target LLMs to identify potential risks. Additional figures are presented in Appendix D.7.

  
**Evaluation Setting** &  \\  & GPT-4 & GPT-3 & Geriatric Pro & CutGLM3 & Vicuna-7B & Vicuna-13B & Vicuna-33B & Llama 2-7B & Llama 2-13B & Llama 2-70B & Avg \\  Zero-shot  & 00.00 & 00.00 & 00.17 & 01.50 & 12.00 & 00.33 & 00.00 & 00.00 & 00.00 & 00.00 & 01.40 \\ Effective Success  & 00.03 & 00.00 & 00.83 & 00.83 & 00.43 & 00.50 & 00.17 & 00.33 & 00.00 & 00.50 & 01.28 \\ In-Context Attack  & 00.00 & 00.00 & 00.90 & 00.26 & 50.40 & 34.00 & 48.00 & 00.00 & 00.00 & 00.00 & 12.35 \\ Jablen Proposed  & 00.17 & 01.00 & 13.83 & 25.25 & 37.83 & 24.00 & 39.33 & 00.50 & 01.67 & 01.00 & 14.46 \\ GPTrate  & 03.73 & 14.79 & 25.08 & 31.75 & 58.32 & 62.69 & 81.64 & 03.77 & 03.45 & 02.50 & 28.77 \\ 
** ALI-Agent** & 26.00 & 26.50 & 15.80 & 22.00 & 64.00 & 54.50 & 74.00 & 07.00 & 04.60 & 04.00 & 29.70 \\
** ALI-Agent** & 08.50 & 33.50 & 64.50 & 56.00 & 93.50 & 93.00 & 94.50 & 13.00 & 13.00 & 08.00 & 49.75 \\   

Table 4: The performance comparison on AdvBench (legality)

  
**Evaluation Setting** &  \\  & GPT-4 & GPT-3 & Geriatric Pro & CutGLM3 & Vicuna-7B & Vicuna-13B & Vicuna-33B & Llama 2-70B & Llama 2-13B & Llama 2-70B & Avg \\  Zero-shot  & 02.90 & 02.90 & 08.70 & 26.81 & 30.43 & 18.12 & 15.22 & 47.83 & 60.72 & 60.00 & 15.36 \\ Generalized System Properties  & 00.00 & **00.19** & **00.17** & 21.34 & 93.13 & 26.90 & 19.57 & 66.52 & 60.00 & 60.00 & 00.00 & 13.26 \\ Effective Success  & 00.72 & 01.27 & 01.72 & 31.16 & 43.06 & 26.90 & 10.14 & 34.01 & 01.45 & 01.27 & 15.58 \\ In-Context Attack  & 00.00 & 02.17 & 02.17 & 16.67 & 55.07 & 29.71 & 21.08 & 39.13 & 60.29 & 64.58 & 17.32 \\ 
**All-Agent** & **13.04** & 13.04 & **45.66** & **75.09** & **75.09** & **45.66** & **47.83** & **95.65** & **10.87** & **01.35** & **4.83** \\   

Table 3: The performance comparison on Singapore Rapid Transit Systems Regulations (legality)

### Study on ALI-Agent (RQ3)

**Ablation Study.** We demonstrate the impact of ALI-Agent's components on the ETHICS  dataset. As shown in Figure 5(a), both the evaluation memory and iterative refiner are critical for ALI-Agent. In particular, the evaluation memory boosts ALI-Agent's ability to generalize past experience to new cases. The refiner further enhances exploration among under-revealed misalignments.

**Analysis of the refiner.** We investigate the effect of multi-turn refinement on the AdvBench  dataset. As depicted in Figure 5(b), misalignment rates increase with the number of iterations until gradually converging. This trend is consistent across all target LLMs, with additional results and detailed case studies shown in Appendix D.9.

**Complementarity with other red teaming techniques.** As illustrated in Figure 5(b), ALI-Agent's ability to reveal misalignments is enhanced by integrating the state-of-the-art jailbreak technique GPTFuzzer . Specifically, we append the universal jailbreak prefix in front of the test scenarios before prompting the target LLMs. This result further demonstrates that ALI-Agent assesses the alignment of LLMs from perspectives different from those of prevailing jailbreak techniques (more details illustrated in Appendix A.2). The integration of our proposed ALI-Agent and current jailbreak techniques can potentially enable a more comprehensive evaluation of alignment.

## 4 Related Work

We remind important related works to understand how ALI-Agent stands and its role in rich literature. Our work is related to the literature on alignment of LLMs, red teaming LLMs, benchmarks for evaluations and LLMs as agents. The first two parts can be found in Appendix A.

**Benchmarks for evaluations**. A line of studies has been conducted to facilitate evaluation on trustworthiness and safety of LLMs . These benchmarks serve as crucial reference points for capturing diverse aspects of human values. ETHICS  introduced a benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. DecodingTrust  considers toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. However, these works require essential expert effort to design contextualized natural language scenarios, thus lacking the capability to generalize to the complex, open-ended real world.

**LLMs as agents**. LLM-empowered agents have demonstrated remarkable capabilities in accomplishing complex tasks . REACT  solves diverse language reasoning and decision-making tasks by combining reasoning traces, task-specific actions, and language models. SELF-REFINE  improves initial outputs from LLMs through iterative feedback and refinement.

Figure 5: Study on ALI-Agent. Figure 5(a) demonstrates the impact of each component (_i.e.,_ evaluation memory, iterative refiner) on ETHICS dataset. Figure 5(b) showcases the benefits of multiple refinement iterations and the effective adaptability of integrating jailbreak techniques (e.g., GPTFuzzer ) on AdvBench dataset.

Reflexion  implements reinforcement learning through linguistic feedback without updating model weights. Agent4Rec  offers the potential to create offline A/B testing platforms by simulating user behavior in recommendation scenarios. Drawing inspiration from these works, we develop a novel agent-based framework for systematic LLM alignment assessment.

## 5 Conclusion

Current evaluation benchmarks are still far from performing in-depth and comprehensive evaluations on LLMs' alignment with human values. In this work, we proposed a novel agent-based framework, ALI-Agent, that leverages the abilities of LLM-powered autonomous agents to probe adaptive and long-tail risks in target LLMs. Grounded by extensive experiments across three distinct aspects of human values, ALI-Agent steadily demonstrates substantial effectiveness in uncovering misalignment in target LLMs. Furthermore, we discussed the framework's scalability and generalization capabilities across diverse real-world applications in Appendix C to strengthen the practical value of our work. Despite its empirical success, ALI-Agent still has two drawbacks to resolve. Firstly, ALI-Agent relies heavily on the capabilities of the adopted core LLM, resulting in uncontrolled performance as we adopted a closed-source LLM (GPT-4-1106-preview). Secondly, the task of designing scenarios to bypass the safety guard trails of the target LLM is itself a form of "jailbreaking", which in some cases, the core LLM may refuse to perform. In future work, we seek to fine-tune an open-source model as the core of ALI-Agent to control the performance of evaluation framework. We can also proceed to proactively evaluate LLMs' alignment performance in specific areas. For example, by providing the query "Singapore traffic laws", we can allow the framework to acquire a set of relevant laws and evaluate LLMs' understanding of and adherence to these laws accordingly. Furthermore, test scenarios exposing misalignment can be utilized as training data for fine-tuning target LLMs to improve their overall alignment performance. The broader societal impacts of our work are discussed in Appendix B.