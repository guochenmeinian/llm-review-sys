# ###### Abstract

###### Abstract

Randomized smoothing is a popular certified defense against adversarial attacks. In its essence, we need to solve a problem of statistical estimation which is usually very time-consuming since we need to perform numerous (usually \(10^{5}\)) forward passes of the classifier for every point to be certified. In this paper, we review the statistical estimation problems for randomized smoothing to find out if the computational burden is necessary. In particular, we consider the (standard) task of adversarial robustness where we need to decide if a point is robust at a certain radius or not using as few samples as possible while maintaining statistical guarantees. We present estimation procedures employing confidence sequences enjoying the same statistical guarantees as the standard methods, with the optimal sample complexities for the estimation task and empirically demonstrate their good performance. Additionally, we provide a randomized version of Clopper-Pearson confidence intervals resulting in strictly stronger certificates. The code can be found at https://github.com/vvoracek/RS_conf_seq.

## 1 Introduction

Adversarial robustness:It is well known that a tiny, adversarial, perturbation of the input can change the output of basically any undefended machine learning model (Biggio et al., 2013; Szegedy et al., 2014); this is unpleasant and we continue in the mitigation of the problem. There are two main lines of work tackling this problem: (1) Empirical: the standard approach here is to use adversarial training (Madry et al., 2018; Goodfellow et al., 2014) where the model is trained on adversarial examples. This approach does not provide guarantees, only empirical evidence suggesting that the model may be robust. With stronger attacks, we might (yet again) realize it is not the case. (2) Certified: with formal robustness guarantees for the model. We will focus on this, and in particular on _randomized smoothing_(Lecuyer et al., 2019) which is currently the strongest certification method1. We will not cover other certification methods and we refer the reader to the survey Li et al. (2023) instead. We consider the standard task of certified robustness; the goal is to decide if the decision of a classifier \(F\) at a particular input \(x\) is robust against additive perturbations \(\) such that \(\|\| r\) for some norm \(\|\|\). Formally, we ask if \(F(x)=F(x^{})\) whenever \(\|x-x^{}\| r\).

Randomized smoothingis a framework providing state of the art formal guarantees on the adversarial robustness for many datasets. One of its benefits lies in the fact that there are no assumptions on the model, making it possible to readily transfer the methods from defending image classifiers against sparse pixel changes to different modalities; e.g., defending large language models against change ofsome letters/words/tokens. Randomized smoothing transforms any undefended classifier \(F:^{d}\) by a smoothing distribution \(\) into a smoothed classifier \(H_{}(x)=*{arg\,max}_{y}_{ } F(x+)=y\) for which robustness guarantees exist. We postpone the details for later. During the certification process, we need to estimate the maximum probability of a multinomial distribution from samples as the exact computation is intractable. This statistical estimation problem is the focus of this paper.

Speed issues:The main weakness of randomized smoothing is the extensive time required for both prediction and certification, making it troublesome for real-world applications. There is an inherent trade-off between the allowed probability of incorrectly claiming robustness2 (type-1 error, \(\)), the probability of incorrectly claiming non-robustness (type-2 error, \(\)), and the number of samples used \(n\). The standard practice is to set \(=0.001\), \(n=100\ 000\) and the value of \(\) is then implicit. it might not be the most practically relevant setting since the implicitly set value of \(\) is exponentially small in \(n\) when the sample is not close to the threshold. The claim is made precise in Example A.1.

The arguably more relevant setting is to set the values of \(\) and \(\) and leave \(n\) implicit. This is much more challenging since it is no longer possible to draw the predetermined number of samples and use a favourite concentration inequality. We propose a new certification procedure using confidence sequences to adaptively (and optimally) deciding how many samples to draw addressing the problem.

Contributions:
* We introduce a new, strictly better version of Clopper-Pearson confidence intervals for estimating the class probabilities in Subsection 2.1. The presented interval is optimal, and thus is the ultimate solution to the canonical statistical estimation of randomized smoothing.
* We propose new methods for the certification utilizing confidence sequences (instead of confidence intervals) in Subsection 2.2. This allows us to draw _just enough_ samples to certify robustness of a point; greatly decreasing the number of samples needed.
* We provide a complete theoretical analysis of the proposed certification procedures. In particular, we provide matching (up to a constant factor) lower-bounds and upper-bounds for the width of the respective confidence intervals. We invert the bound and show that the certification procedure has the optimal sample-complexity in an adaptive estimation task.
* We provide empirical validation of the proposed methods confirming the theory.

Notation:Bernoulli random variable with mean \(p\) is denoted as \((p)\) and binomial random variable is \((n,p)\). Random variables are in capitals (\(X\)) and the realizations are lowercase (\(x\)). We type sequences in bold and denote \(_{t}\) first \(t\) elements of \(\). We write \(a b\) if there exists a universal constant \(C>0\) such that \(a Cb\). If \(a b\) and \(b a\), then we write \(a b\). Iverson bracket \(\) evaluates to \(1\) if \(\) is true and to \(0\) otherwise.

### Paper organization

First, in Section 2 we introduce randomized smoothing, then, in Subsection 2.1, we introduce Clopper-Pearson confidence intervals, show that they are conservative and propose their improved (optimal) randomized version. In Subsection 2.2 we discuss shortcomings of confidence intervals and introduce confidence sequences and provide lower and upper bounds for their performance. We use the confidence sequences in Section 3 and benchmark them on a sequential estimation task.

## 2 Randomized Smoothing

As outlined in Introduction, consider a classifier \(F:^{d}\) and let the class probabilities under additive noise \(\) be \(h_{}(x)_{y}=_{}[F(x+)=y]\). Denote the highest probability (breaking ties arbitrarily) class in the original point \(A=*{arg\,max}_{y}h_{}(x)_{y}\) and the second-highest probability class \(B=*{arg\,max}_{y A}h_{}(x)_{y}\). Let the corresponding probabilities be \(p_{A}\) and \(p_{B}\) respectively. Recalling that \(H_{}(x)=*{arg\,max}_{y}h_{}(x)_{y}\), then for a certain function \(r:^{2}_{+}\) we have

\[\|x-x^{}\| r(p_{A},p_{B}) H_{}(x)=H_{}(x^{ }).\]This \(r\) depends on the smoothing distribution \(\) and the considered norm. For example, if the considered norm is \(_{2}\) and \(\) is isotropic Gaussian with standard deviation \(\), then \(r(p_{A},p_{B})=(^{-1}(p_{A})-^{-1}(p_{B}))\) where \(^{-1}\) is Gaussian quantile function Cohen et al. (2019). Note that in general, \(r(,)\) is increasing in the first coordinate and decreasing in the second one. The intuition is that the larger the value of \(p_{A}\) at \(x\), the larger it will be also in the neighborhood of \(x\); similarly for \(p_{B}\). It is common in the literature to use the bound \(p_{B} 1-p_{A}\) and thus certify \(r(p_{A},1-p_{A})\). We stick to the convention in the paper and discuss the topic in more details in Appendix B.

Statistical estimation:The crux of the paper lies in the statistical estimation problems for randomized smoothing. We consider the abstract framework for randomized smoothing, so the proposed techniques can be used as a drop-in replacement in all randomized smoothing works with a statistical-estimation component (i.e., not in the de-randomized ones such as Levine and Feizi (2021)). We do not only propose methods that work good empirically, we also provide theoretical analysis suggesting that we solve the problems optimally in a certain strong sense. The main focus is on the following two constructs.

1. Confidence intervals: A standard component of randomized smoothing pipelines is the Clopper-Pearson confidence interval. It is known to be conservative3; thus, the certification procedures are unnecessarily underestimating the certified robustness. We provide the _optimal_ confidence interval for binomial random variables, resolving this issue completely. 2. Confidence sequences: In the standard randomized smoothing practice, we draw a certain, predetermined, number of samples and then we compute the certified radius on a confidence level \(1-\). We improve on this by allowing for adaptive estimation procedures employing confidence sequences; We demonstrate the performance in the standard task of adversarial robustness, where we want to decide if a point is robust at radius \(r\) with type-1 (resp. 2) error rates \(\) (resp. \(\)) using as few samples as possible.

Literature review of randomized smoothing:The most relevant related works are Horvath et al. (2022); Chen et al. (2022) and they are discussed in Section 3. Here we briefly summarize literature relevant to randomized smoothing in general. The choice of the smoothing distribution \(\) is a crucial decision determined mainly by the threat model with respect to which we want to be robust. For example, if we are after certifying \(_{1}\) robustness, we choose a uniform distribution in a \(d-\)dimensional \(_{}\) ball (Lee et al., 2019; Yang et al., 2020), or better, splitting noise (Levine and Feizi, 2021), but we do not go into details here. Alternatively, for \(p-\)norms, \(p 2\) one would usually use a \(d-\)dimensional normal distribution Lecuyer et al. (2019); Cohen et al. (2019). The variance of the distribution based on how large perturbations do we allow in our threat model. We refer the reader to Yang et al. (2020) for a broader discussion on the smoothing distributions. It is possible to use methods in the spirit of randomized smoothing to certify other threat models, such as patch attacks Levine and Feizi (2020) and sparse attack Bojchevski et al. (2020), which can be readily extended to other modalities. Sometimes, the "smoothing" distribution can be made supported on a small, discrete set and then we can evaluate the expectation exactly, yielding deterministic certification (often called de-randomized smoothing (Levine and Feizi, 2021)). See also Kumari et al. (2023) for a survey on randomized smoothing containing examples of when the certification is beyond additive \(_{p}\)-norm threat model; even such techniques use the Bernoulli estimation subroutine.

### Confidence Intervals

We do not have access to the class \(A\) probability \(p_{A}\) and only have to estimate it from binomial samples; hence, the name _randomized_ smoothing. Because of the randomness, we can only provide probabilistic statements about the robustness of a classifier in the following spirit "with probability at least \(1-\), robust radius is at least \(r\)" for a small \(\), usually \(0.001\). This failure probability corresponds to the event of overestimating \(p_{A}\) and we control it with the help of confidence intervals.

The standard choice for calculating the upper confidence interval is the Clopper-Pearson interval, sometimes called _exact_. Regardless of this pseudonym, it is in reality conservative. In this subsection, we introduce the Clopper-Pearson confidence interval for the mean of binomial random variables, demonstrate its limitations and introduce its (better) randomized version.

**Definition 2.1** (Confidence interval for binomials).: Let \(u,v\) map sample to a real number. They form a (possibly randomized) confidence interval \(I(x)=[u(x),v(x)]\) with coverage \(1-\) if for any \(p\) it holds that

\[_{X(n,p),I}(p I(X)) 1-.\]

We will mainly use one-sided confidence intervals; that is, \(u()=0\) (lower confidence interval) or \(v()=1\) (upper confidence interval). When we will talk about probability of confidence interval containing the parameter, it will be in the frequentist sense, keeping in mind that confidence intervals provide no guarantees post-hoc for any individual estimation.

Clearly, if \(I(x)=\) regardless of \(x\), it will be a valid confidence interval but rather useless; thus we aim for short intervals. Ideally it would hold for every \(p\) that \(_{X}(p I(X))=\), otherwise, some values are included in the confidence intervals unnecessarily often they can be shortened. In the following we introduce the standard Clopper-Pearson confidence intervals Clopper & Pearson (1934).

**Definition 2.2** (Clopper-Pearson intervals).: One sided upper interval is defined as \(v(x)=1\) and

\[u(x)=\{p\,|\,((n,p) x)>\}.\]

The lower one is defined as \(u(x)=0\) and

\[v(x)=\{p\,|\,((n,p) x)>\}.\]

Amongst the deterministic confidence intervals, they are the shortest possible; however, they are in general conservative. In the binomial case \((n,p)\), there are only \(n+1\) possible outcomes; and thus only \(n+1\) possible confidence intervals suggesting that the actual coverage can be \(1-\) only for at most \(n+1\) values of \(p\). The problem strikingly arises for upper confidence interval for large values of \(p\). When we sample from \((n,p)\), regardless of the outcome, all values larger than \([n]{}\) are contained in the confidence interval. This is a usual problem in the context of randomized smoothing, leading to sharp drops towards the end of robustness curves. We demonstrate this sub-optimality in the first part of Example A.2. We mitigate this problem by introducing randomness into the confidence intervals. They will still have the desired coverage level \(1-\), but will be shorter. Intuitively, we do so by "interpolating" between the deterministic confidence intervals in the spirit of Stevens (1950).

**Definition 2.3** (Randomized Clopper-Pearson intervals).: Let \(W\) be uniform on the interval \(\). The randomized one sided upper interval is defined as \(v_{r}(x)=1\)\(u_{r}(x)=u^{}_{r}(x,W)\) where

\[u^{}_{r}(x,w)=\{p\,|\,((n,p)>x)+w( (n,p)=x)>\}.\]

The lower one is defined as \(u_{r}(x)=0\) and \(v_{r}(x)=v^{}_{r}(x,W)\) where

\[v^{}_{r}(x,w)=\{p\,|\,((n,p)<x)+w( (n,p)=x)>\}.\]

**Proposition 2.4**.: _Randomized Clopper-Pearson interval (\(I_{}\)) have coverage exactly \(1-\). Furthermore, for any confidence interval \(I\) at level \(1-\), and any \(p q\) it holds that_

\[_{X(n,p)}(q I(X))_{X (n,p)}(q I_{}(X)).\]

The proof is in the Appendix D and we remark that the interval can be efficiently found by binary search. Proposition 2.4 implies that the randomized Clopper-Pearson bounds are optimal and all the other confidence intervals for binomial random variables are more conservative. It remains to demonstrate the advantage of the randomized confidence intervals. We refer to Figure 1 for the comparison of the randomized and deterministic Clopper-Pearson confidence intervals and how they affect the robustness. We note that the most significant difference is towards the high values of \(p\) and for certification functions \(r\) such that \(_{p 1}r(p)=\), such as when smoothing with normal distribution. This explains the common sharp drop by the end of the robustness curve.

Width of confidence intervals:For the simplicity of exposition, let the width of a confidence interval at level \(1-\) with \(n\) samples be \(\). This way, we hide the dependency on \(p\) into \(\). In the full generality, the width of the confidence intervals exhibits many decay regimes between the rates \(\) (when \(np(1-p) 1\)) and \((1/)/n\) (when \(np(1-p) 1\)). Our algorithms capture the correct scaling of the confidence intervals. See Boucheron et al. (2013) and the discussion on Bennett's inequality which captures the correct rates for Bernoulli mean estimation.

### Confidence Sequences

Limitations of confidence intervals:In order to compute the confidence intervals presented in the previous subsection, we need to collect samples and then run an estimation procedure once which brings certain limitations. Consider the following two scenarios: (1) It might be the case that we do not need all \(100\ 000\) samples and after only \(10\) it would be enough for our purposes because we could already conclude that the point cannot be certified here; thus, we wasted \(99\ 990\) samples. (2) Alternatively, we could see that even \(10^{5}\) samples are not enough, and we need to draw more samples. However, we have already spent our failure budget \(\), so we cannot even carry another test at all.

This motivates the introduction of confidence sequences. They generalize confidence intervals in the way that they provide a confidence interval after every received sample such that we control the probability that the underlying parameter is contained in _all_ the confidence intervals _simultaneously_.

**Definition 2.5** (Confidence sequence).: Let \(\{u_{t},v_{t}\}_{i=1}^{}\) be mappings from a sequence of observations to a real number. They form a confidence sequence \(I_{t}(_{:t})=[u_{t}(_{:t}),v_{t}(_{:t})]\) for all \(t 1\) with confidence level \(1-\) if

\[(p I_{t}(_{:t}), t>1) 1-\]

for any \(p\), where \(\) is an infinite sequence of Bernoulli random variables \((p)\).

_Remark 2.6_.: Since we want the estimated parameter to be contained in all the confidence intervals simultaneously, we will have by convention that \(I_{t+1}(x_{:t+1}) I_{t}(x_{:t})\).

To simplify presentation, we would consider the symmetric ones; i.e., those where we consider the two possible failures - when we overestimate or underestimate the mean - to be equal. However, they will be constructed from two one-sided bounds, so the generalization is straightforward and will not be discussed.

Related work: The construction of confidence sequences based on union bound employs the doubling trick which is widely used in online learning to convert fixed-horizon algorithms to anytime algorithms. In this direction, we refer to Mnih et al. (2008) as the direct predecessor of this work, where they used similar techniques but did not explicitly construct confidence sequences. In the confidence sequence literature, this technique is similar to stitching of Howard et al. (2020). The stitched confidence intervals of Howard et al. (2020) are generally shorter by a small constant factor, but the analysis and generalization become complicated, contrasting with our approach.

The construction based on betting is in the spirit of Orabona & Jun (2023) (the reference contains an excellent survey on the topic). The difference mainly lies in the fact that we are interested in Bernoulli random variables, which allows us to use specialized tools at places, as opposed to the referred work, which considers bounded random variables. As an analogy to confidence intervals, the previous work constructed Bernstein-type inequalities, while we constructed Clopper-Pearson-like bounds. In Ryu & Bhatt (2024), the authors improved over Orabona & Jun (2023) in certain aspects in the case of \(\)-valued random variables. Notably, in Section 3, they considered the special case - \(\{0,1\}\)-valued random variables and their results are greatly overlapping those in Section 2.4.

Figure 1: **left**: Comparison of coverages of confidence intervals for the mean estimation of \((100,p)\) when \(:=0.001\). Note that for \(p>[n]{} 0.93\), the coverage is \(1\). **right**: Comparison of \(_{2}\)-robustness curves with the standard (dashed) or the randomized (solid) Clopper-Pearson bounds on a CIFAR-10 dataset under the standard setting. The experimental details are in Appendix C.

### Union bound confidence sequence

A natural way how to extend the confidence intervals to confidence sequences is to construct a confidence interval at every time step and use a union bound to control the total failure probability. In the following, we first show that a naive application of this approach is asymptotically suboptimal, and then we provide a way how to construct optimal confidence intervals in a certain strong sense.

Intuition on the width of confidence sequences:For any random variable with finite variance, the optimal width of the confidence interval for the mean parameter scales as \(\) with the increasing number of samples \(t\) at confidence level \(1-\)Lugosi and Mendelson (2019). On the other hand, it is well known that the width of the optimal confidence sequence scales as \(\) as \(t\) increases due to the law of iterated logarithms Ledoux and Talagrand (1991). A naive use of union bound, computing a confidence interval using failure probability at time step \(t\), \(_{t}=}{t^{}}\) for some \(c\) and \(>1\) such that \(_{i=1}^{}_{t}=\) yields a confidence sequence whose width scales as \()/t}\). We cannot choose any monotonous \(_{t}\) schedule decaying slower because even for \(=1\) we still keep the \(\) factor while the sum \(_{i=1}^{}_{t}\) diverges.

Now consider non-monotonous schedules of \(_{t}\), two key ideas follows. (1) In order to have the optimal rate \((1/_{t})(1/)+ t\), we need \(_{t}/ t\). Clearly, if this holds for all \(t\), then \(_{t=1}^{}_{t}\) diverges. (2) A confidence interval at time \(t\) is also a valid confidence interval for all \(t^{}>t\). Furthermore, if \(t^{}\) is not much larger than \(t\), then it may still asymptotically have the optimal width up to a multiplicative constant. Thus, updating the confidence sequence when \(t\) is a power of (say) \(2\) result in the optimal width. This reasoning is formalized in the following theorem.

**Theorem 2.7**.: _Fix \(>0\). Consider a sequence_

\[_{t}=&$ for integer $k$,}\\ 0&\]

_Then Algorithm 1 produces a confidence sequence at level \(1-\) of the following width which is attained in the worst case_

\[_{t}}\]

_where \(_{t}\) is \(U-L\) at time \(t\), and the confidence intervals are randomized Clopper-Pearson intervals._

The proof is in Appendix E. We remark that the statement of Theorem 2.7 prioritizes simplicity over its full generality. The generalization to other schedules of \(_{t}\) from the second bullet point as described in the previous paragraph is routine and described in detail in Appendix C.2.

**Corollary 2.8**.: _The asymptotic rate of 2.7 is optimal due to law-of-iterated-logarithm (Ledoux and Talagrand, 1991) and even in the finite sample regime due to Balsubramani (2014)[Theorem 2]._

### Confidence sequences based on betting

A recent alternative approach to confidence sequences is based on a hypothetical betting game. For the illustration, consider a fair sequential game; e.g., sequentially betting on outcomes of a coin. If we guess the outcome correctly, we win the staked amount, otherwise we lose it. If the coin is fair, in expectation, our wealth stays the same. On the other hand, if the game is not fair and the coin is biased, we can win money. For example, if the true head-probability is \(0.51\), we start increasing our wealth in an exponential fashion, see Example A.3; thus, if we win lots of money, we can conclude that the game is not fair. We instantiate a betting game for every possible mean \(0 p 1\) that would be fair if the true mean is \(p\). Then we observe samples of the random variable and as soon as we win enough money, we drop that particular \(p\) from the confidence sequence. To make things formal, we introduce the necessary concepts from probability theory. The evolution of our wealth throughout a fair game is modeled by martingales4, sequences of random variables for which, independently of the past, the expected value stays the same.

**Definition 2.9** (Martingale).: A sequence of random variables \(W_{1},W_{2},\) is called a _martingale_ if for any integer \(n>0\), we have \((|W_{n}|)<\) and \((W_{n+1}|W_{1},,W_{n})=W_{n}\). If we instead have \((W_{n+1}|W_{1},,W_{n}) W_{n}\), then the sequence is called a _supermartingale_.

``` \(t,H,K,L,U 0,0,0,0,1\) loop  Obtain random \(x\) \(H H+x\) if\(t t+1\) if\(t=2^{K}\)then \(K K+1\) \(_{t}/(K(K+1))\) \(L\{L,(H,t,_{t})\}\) \(U\{U,(H,t,_{t})\}\) endif endloop ```

**Algorithm 1** Union-Bound Confidence Sequence

In the coin-betting example, \(W_{1},W_{2},\) is a martingale where \(W_{n}\) represents our wealth after playing the game for \(n\) rounds. We stress that \(W_{t} 0\) for all \(t>0\). By convention, we will also have \(W_{1}=1\). We further need a time-uniform generalization of Markov's inequality.

**Proposition 2.10** (Ville's inequality Durrett (2010)).: _Let \(W_{1},W_{2},\) be a non-negative supermartingale. then for any real \(a>0\)_

\[[_{n 1}W_{n} a] [W_{1}]}{a}.\]

Thus, whenever we play a game and earn a lot, we can -- with high probability -- rule out the possibility that the game is fair. So far, this is still an abstract framework. We have yet to design the betting game and the betting strategy and describe how to run the infinite number of games.

Betting game:Let5\(0<p<1\). Consider a coin-betting game where we win \(1/p\) (resp. \(1/(1-p)\)) multiple of the staked amount if we correctly predicted heads (resp. tails). If the underlying heads probability is \(p\), then regardless of our bet - in expectation - we still have the same amount of money; thus, this game is fair. We identify heads and tails with outcomes \(1,0\) respectively.

Betting strategy:We deconstruct the betting strategy into the two sub-tasks: (1) If we know the underlying heads probability, we can design the optimal betting strategy for any criterion. (2) Estimate the heads probability. **First sub-task:** Let \(p\) define the betting game from the previous paragraph and \(q\) be the true heads probability; Optimally, bet \(q\)-fraction of wealth to heads and \(1-q\) fraction to tails. It maximizes the expected log-wealth, or equivalently, the expected growth-rate of our wealth and is also known as the Kelly Criterion. This is known to be optimal for the adaptive estimation task, see Wald (1947) under the name sequential-probability-ratio-test (SPRT). One might have expected the optimal criterion to optimize to be the expected wealth; however, the betting strategy maximizing the expected wealth suggest to bet all the money on one of the outcomes. This strategy, however, leads to an eventual bankruptcy almost surely and is not recommended in this context. **Second sub-task:** A natural choice is to use the running sample mean of the observations as the estimator of \(q\). Unfortunately, this estimator would be either \(0\) or \(1\) after the first observations, so we go bankrupt whenever the observed sequence contains both outcomes. Thus, we use a "regularized" sample mean and after observing \(H\) times heads in a sequence of length \(t\), we estimate \(=(H+0.5)/(t+1)\). This is the MAP estimate of the mean with Beta\((1/2,1/2)\) prior and is known as Krichevsky-Trofimov estimator (Cesa-Bianchi & Lugosi (2006) Section 9.7), Krichevsky & Trofimov (1981) and is proven to be successful for building confidence sequences beyond the Bernoulli case Orabona & Jun (2023).

Parallel betting games:We have described a betting game for a certain \(p\) and a betting strategy. Employing Ville's inequality we can possibly reject the hypothesis that the true sampling distributionfollows \(p\). However, we need to run the game for all values of \(p\) and after every observation report the smallest interval containing all the values of \(p\) that were not rejected so far. This is clearly impossible to do explicitly, but it turns out that the non-rejected values of \(p\) form an interval and we can use binary search twice to find the end-points of the interval. This is a non-trivial result and generally does not need to hold for confidence intervals constructed by betting. The first key observations is that the betting strategy does not depend on \(p\), so we can "play the game" just once. The second observation is that the resulting wealth is convex in \(p\). To see why, let \(_{}\) (resp. \(x_{}\)) be our estimate of \(q\) (resp. the coin-toss outcome, for brevity \(1/0\) corresponds to heads/tails respectively and \(H=_{=1}^{t}x_{}\)), then our log-wealth at time \(t\) can be written as a function of \(p\).

\[ W_{t}(p) =_{=1}^{t}((_{}}{p} )^{x_{}}(_{}}{1-p})^{1-x_{}})\] \[=^{t}x_{}(_{})+(1-x_ {})(1-_{})}_{}_{(p)}.\]

Therefore, at every time-step, we can compute the interval of values of \(p\) for which the betting game has not concluded yet and thus form the current confidence interval. The proof is in Appendix F.

**Theorem 2.11**.: _Algorithm 2 produces a valid confidence sequence at confidence level \(1-\), where we interpret the interval \([L,U]\) at iteration \(T\) of the algorithm as the confidence interval at time \(t\) with width \(\) at most (which is attained in the worst case):_

\[_{t}}.\]

This is not asymptotically optimal; still, empirically it performs well, and the same techniques can be used to obtain a confidence sequence that follows law-of-iterated-logarithms Orabona & Jun (2023). We present a comparison of the confidence sequences in Figure 2 and conclude this subsection by an implementation remark.

_Remark 2.12_.: Wealth \(W(_{:t})\) does not depend on the order of \(_{:t}\), so we write it as \(W(h,t)\), meaning wealth after observing \(h\) heads in the first \(t\) tosses. Now, for every time \(t\), we can compute what is the minimal number of observed \(1\) (heads) (call it \(H(t)\)) so that \(p\) is outside of the lower-confidence interval. \(H(t)\) is clearly non-decreasing in \(t\); also, \(W(h,t)\)) can be easily computed from \(W(h-1,t)\) and from \(W(h,t-1)\) in constant time. The whole dynamic programming approach can be summarized in the following scheme which is repeatedly executed starting from \(h=0,t=0\).

* If \(W(h,t)\): \(H(t):=h\), \(t:=t+1\), compute \(W(h,t+1)\)
* Else: \(h:=h+1\), compute \(W(h+1,t)\).

Both lines are executed in constant time. Also, we start from \(W(0,0)\) and \(h,t<N+1\). Executing a line, either \(h\) or \(t\) increases and thus to compute the thresholds for sequence of length up to \(N\), we need to execute at most \(2N\) lines of the scheme. We note that for the simplicity, we did not handle the case when \(W(h,t)<1/\). In that case we would just set \(H(t)=t+1\) and increase \(t\).

## 3 Experiments

Now we shall provide experimental evidence for the performance of the proposed methods. We benchmark the confidence sequences on the Sequential decision making task, where we try to certify a certain radius at given confidence level with as few samples as possible; the definition that follows is general beyond randomized smoothing. We emphasize that this setting of certifying a certain radius is by far the most common one in the robustness literature. We stress that the comparison of the robustness curves (e.g., as in Figure 1) is vacuous, since in the adaptive task, we do not spend samples to _improve_ the robustness curves, beyond the certified level.

**Definition 3.1** (Sequential decision making task).: Let \( p,q 1\) and only \(p\) being known. Receive samples from \((q)\). After every sample, either halt and declare that \(p>q\), \(p<q\), or request another sample. The task is to minimize the number of samples while being wrong with frequency at-most \(\)

### Related work

We identified Horvath et al. (2022) as the most relevant work. They distinguish between samples for which a predetermined radius \(r\) can be certified, and the samples for which it cannot. They use \(s\) values \(n_{1}<<n_{s}\) (\([10^{2},10^{3},10^{4},1.2 10^{5}]\)) sequentially as the number of samples. They try to certify radius \(r\) with \(n_{1}\) samples; if it fails, then they try \(n_{2}\) samples etc. They employ Bonferroni correction (union bound) and every sub-certification is allowed to fail with probability only \(\). The key differences (details in Appendix C.2.1) to our method are that (1) It always abstains for hard tasks. (2) Splitting the \(\) budget evenly degrades performance for small \(n\). (3) method is only a heuristics. See Figure 5 and Tables 1, 3, 4. For the empirical comparison.

Another relevant work is Chen et al. (2022). Here, the certification is split in two phases. (1) Mean is crudely estimated. (2) The crude estimate selects the number of samples drawn so that the decrease (either multiplicative or absolute) in the certified radius is heuristically approximately at most a predetermined constant. We note that this heuristic for distributing samples can be made rigorous in a certain sense (see Dagum et al. (1995)). This is trivial for confidence sequences, as one can stop the estimation only as soon as they short enough and solve the task of Chen et al. (2022) with guarantees (instead of just heuristic). In this sense, we see our methods to be more general. We benchmark this in Table 2.

The works Seferis et al. (2023); Ugare et al. (2024) also address the speed issues of randomized smoothing, however, they are orthogonal to our directions. In particular, Ugare et al. (2024) uses an auxiliary network for which the certification is faster and transfer the certificates to the original model. Seferis et al. (2023) observes that few samples are sufficient for non-trivial certificates.

## 4 Conclusion

In this paper, we investigated the statistical estimation procedures related to randomized smoothing and improved them in the following two ways: (1) We have provided a strictly stronger version of confidence intervals than the Clopper-Pearson confidence interval. (2) We have developed confidence sequences for sequential estimation in the framework of randomized smoothing, which will greatly

Figure 2: **left:** Comparison of widths of confidence sequences for the mean of Bernoulli \((0.1)\) with \(=0.001\). The width is on top and the actual confidence sequence on the bottom. In the notation of Algorithms 1 and 2, the sequence of \(U-L\) is in the top figure, while both sequences \(U\) and \(L\) are in the bottom figure. Note the \(\)-scale for \(t\) (and width on top). **right** : Instantiation of Task 3.1. The goal is to decide if \(p=0.91\) (vertical magenta line) or not with \(=0.001\). On top are the numbers of samples requested for the individual methods averaged over \(1000\) trials for \(51\) equally spaced values of \(p\); on the bottom is the relative suboptimality of the individual methods; i.e., how many times more samples did they request compared to the ideal method. Note \(\) scales on the \(y-\)axis. **methods:** UBnd-CS and Betting-CS are from Algorithm 1 and 2 respectively. Adaptive is from Horvath et al. (2022). The ideal is the unattainable lower-bound for the two tasks. On the LHS, it is a confidence interval on level \(1-\) computed independently at every time step. On the RHS, it is SPRT knowing both \(p,q\) which is optimal due to Wald (1947).

reduce the number of samples needed for adaptive estimation tasks. Additionally, we provided matching algorithmic upper bounds with problem lower bounds for the relevant statistical estimation task.

## 5 Broader Impact Statement

We hope that this paper enlarges the interest in statistical estimation within the ML community.