# The Quantization Model of Neural Scaling

Eric J. Michaud

Ziming Liu

Uzay Girit

Max Tegmark

MIT & IAIFI

ericjim@mit.edu

###### Abstract

We propose the _Quantization Model_ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the _Quantization Hypothesis_, where network knowledge and skills are "quantized" into discrete chunks (_quanta_). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model gradients, we automatically decompose model behavior into a diverse set of skills (quanta). We tentatively find that the frequency at which these quanta are used in the training distribution roughly follows a power law corresponding with the empirical scaling exponent for language models, a prediction of our theory.2

## 1 Introduction

In the aggregate, larger neural networks trained on more data perform better than smaller neural networks trained on less data, in a predictable way. Across a range of studies, mean test loss has been observed to decrease as a power law in both the number of network parameters (\(L N^{-_{N}}\)) and the number of training samples (\(L D^{-_{D}}\)) [1; 2; 3; 4; 5; 6; 7]. Although aggregate performance changes smoothly with scale, when particular capabilities are examined, larger models often have emergent abilities, i.e., qualitatively different performance than smaller models [8; 9]. Understanding and reconciling both facets of scaling - the predictable power law decrease in loss and the emergence of new capabilities at scale - is of both theoretical and practical interest . Understanding how scaling changes what neural networks learn is entangled with core questions: what are deep neural networks doing internally, and will they will continue to improve with scale?

Recent studies of the internals of neural networks have found a variety of impressive algorithms learned by gradient descent [11; 12; 13; 14; 15]. As more work is put into understanding the structures learned by neural networks (the task of _mechanistic interpretability_), we may find more and more _circuits_[11; 16] in models, intelligible internal algorithms for accomplishing prediction in specific contexts. Can such analysis be scaled up to frontier models ? Two assumptions which, if true, would make us more optimistic about mechanistically understanding large models include (1) decomposability/modularity/sparsity [18; 19; 20; 21] - that large models are decomposable into parts, and only a small number of these parts are relevant to the model's behavior on any given sample and (2) universality [22; 11; 23; 24] - that similar structures recur across models of increasing size. Recently, Olsson et al.  found encouraging evidence for universality of "induction heads" across LLMs and found that these emerge in a discrete transition during training.

In this paper, we articulate the _Quantization Hypothesis_, a set of informal conjectures about the _decomposability_ of networks into smaller parts, the _universality_ of computations performed across model scales, the _discreteness_ of what models learn, and about how properties of the data distributionproduce power law neural scaling. In particular, we hypothesize that to many prediction problems, there corresponds a particular enumerable set of indivisible pieces of knowledge or skills that models must learn, and that model performance is determined by _which_ of these elements models successfully learn. We call these basic building blocks of model performance the **quanta**:

We use this terminology in analogy to Max Planck's assumption in 1900 that energy is quantized into discrete chunks (quanta) - here we imagine that knowledge/skills are quantized into discrete chunks (quanta). Since "quantization" is commonly used in machine learning in the context of low-precision arithmetic, we suggest "knowledge quantization" or "skill quantization" to refer to our notion of quantization. We will see that a Zipfian distribution governing the "use frequency" of the quanta produces power law neural scaling, where the effect of scaling is to learn an increasing number of discrete quanta, and smooth scaling laws average over small discrete jumps in model performance.

This paper is organized as follows: in Section 2 we give a theoretical model of power law neural scaling from the Quantization Hypothesis. In Section 3 we construct toy datasets satisfying the hypothesis, where smooth power laws average over many discrete jumps in model performance. In Section 4 we then analyze how power law scaling decomposes for real LLMs. In Section 5, we develop a method for automatically discovering quanta in language models by clustering their behavior into basic coherent skills, and analyze the statistics of these clusters, concluding in Section 7.

Figure 1: We auto-discover _quanta_ – basic units of model knowledge/skill – for a language model. Here we show collections of next-token prediction samples which our method clustered together, each corresponding to some coherent model behavior. We indicate the token which was predicted from the context before it with a red highlight. We indicate newlines using “\(\)n”. See Section 5 for explanation.

Theory

Consider the task of modeling the distribution of text on the internet. Successful prediction requires an immense amount of knowledge, and the ability to perform diverse computations, due to the immense complexity and diversity of the world and therefore of human language. For instance, in order to predict what word will come next in a conversation between two physicists, one must "know" much about physics. In order to continue the text "2534 + 7261 = ", one must be able to perform arithmetic (for large enough numbers, memorization becomes a highly inefficient strategy) . A great many distinct types of computations are present in the world in the processes that _produce_ text, and so _predicting_ text requires those computations to be present in our models.

In this paper, we conjecture the Quantization (or Quanta) Hypothesis:

* they are either learned or not learned. Model performance is determined by _which_ quanta have been learned.
* Some quanta are more useful for reducing loss than others, leading to a natural ordering of the quanta. We call the ordered quanta the **Q Sequence**. Optimally trained networks should therefore learn the quanta in that order. The effect of scaling is to learn _more_ of the quanta in the Q Sequence, so scaling performance is simply determined by _how many_ quanta are successfully learned.
* The frequencies at which the quanta are used for prediction follow a power law.

Together these can result in power law neural scaling. We model the Quantization Hypothesis as follows, referring to the below as the "Quantization (or Quanta) Model". Let \(\) denote a bit string whose \(k^{}\) bit \(_{k}=1\) if the \(k^{}\) quantum in the Q Sequence has been learned, and \(_{k}=0\) otherwise. QH1 implies that the mean loss \(L\) is simply a function of \(\). QH2 implies that when \(n_{k}_{k}\) quanta have been learned, we have \(_{k}=1\) for \(k n\). Let \(L_{n}\) denote the mean loss in this case. From QH3, we have that the \(k^{}\) quantum benefits prediction on a randomly chosen sample with probability

\[p_{k}=k^{-(+1)} k^{-(+1)}\] (1)

for a Zipf power law \(>0\), where \((s)_{k=1}^{}k^{-s}\). Let us also assume that learning the \(k^{}\) quantum reduces average loss from \(b_{k}\) before it is learned to \(a_{k}\) after it is learned on the samples where it is utilized. If \(a_{k}\) and \(b_{k}\) are \(k\)-independent (\(a_{k}=a\), \(b_{k}=b\)), then a model that has learned the first \(n\) quanta will have expected loss

\[L_{n} = _{k=1}^{n}ap_{k}+_{k=n+1}^{}bp_{k}=_{k=1}^{ }ap_{k}+_{k=n+1}^{}(b-a)p_{k}\] (2) \[ a+_{n}^{}k^{-(+1)} dk=a+n^{-}.\]

In other words, \(L_{}=a\) and \((L_{n}-L_{}) n^{-}\) is a power law.

In Appendix A, we provide analogous derivations for other assumptions for \(a_{k}\) and \(b_{k}\), and find that a range of assumptions produce curves that are exact or approximate power laws - the latter include a small logarithmic correction.

In the derivation above, we assumed that all samples are what we will refer to as _monogenic_, meaning that prediction relies on at most a single quantum, akin to how monogenic traits in biology (e.g. cystic fibrosis) depend on a single gene. By assuming that all samples are monogenic, we can write the expected loss as a sum over quanta, weighted by the fraction of samples which rely on that quanta \(p_{k}\). We further explore the idea of monogenic vs. polygenic samples in Section 4.2. So far we have seen how the Quantization Hypothesis can produce power law scaling as a function of the number of quanta learned \(n\). We will now give one possible mechanism by which this can translate into power law scaling in parameters, data, etc.:

**Parameter scaling**: In networks of finite size, network capacity can bottleneck how many quanta are learned. If we assume that all quanta require the same capacity of \(C\) network parameters, then a network with \(N\) parameters can learn roughly \(n N/C\) quanta. Therefore \(L(N)-L_{} n^{-}(N/C)^{-} N^{-}\), we so we get power law scaling in \(N\) with exponent \(_{N}=\).

**Data scaling (multi-epoch)**: For data scaling, we assume that for each quantum, a threshold of \(\) examples utilizing quantum \(k\) are needed in the training set for quantum \(k\) to be learned3. With \(D\) training samples, approximately \(Dp_{k}\) samples relying on quantum \(k\) will be present, and solving for \(Dp_{n}=\) we get the last quantum to be learned will be \(n(D/)^{1/(+1)}\) since \(p_{k} k^{-(+1)}\). Under this model, we get scaling in data samples \(L(D)-L_{} n^{-}(D/)^{-/(+1)} D ^{-/(+1)}\), and so \(_{D}=/(+1)\). From our earlier result that \(_{N}=\), we would therefore predict that \(_{D}=_{N}/(_{N}+1)\). We discuss whether this relationship holds empirically for data and parameter scaling exponents observed across a variety of studies in Appendix F.

**Data scaling (single-epoch)**: In multi-epoch training, the information contained in the training dataset can bottleneck which quanta are learned. However, the rate of convergence of SGD can also bottleneck performance. For single-epoch training, a greater number of training samples allows one to train for longer. In our model, the amount that each quantum reduces mean loss by follows a power law. If the magnitude of the gradients for learning these quanta also follow a power law, then the convergence time for each quanta may follow a power law too. If the number of steps to learn quantum \(k\) is \( 1/p_{k}\), then if the first quantum requires \(T\) steps to be learned, quantum \(n\) will require \(Tn^{+1}\) steps, and so \(n=(S/T)^{1/(+1)}\) quanta can be learned in \(S\) steps. This gives scaling in training steps \(L(S)-L_{} n^{-}(S/T)^{-/(+1)} S ^{-/(+1)}\), and so \(_{S}=/(+1)\). Under this model, multi-epoch and single-epoch data scaling exponents coincide: \(_{D}=_{S}\).

## 3 Proof of concept: a toy dataset

In this section, we will describe a toy dataset consisting of distinct subtasks which are power law distributed in frequency. We observe power law neural scaling in data and parameters on this task, and find that the mechanism of neural scaling coincides with our theory from Section 2. It is therefore possible for scaling laws to arise from the Quantization Model for data with the right structure. We leave a study of whether natural datasets (e.g. natural modeling) possess such structure to Section 4.

### The "multitask sparse parity" dataset

The toy task we will construct consists of many subtasks - distinct types of inputs which each require corresponding distinct computations (quanta). For each subtask, we choose a variant of the "sparse parity" problem, recently studied in . The sparse parity prediction problem is simple: given a bit string of length \(n\), compute the parity (sum mod 2) of a fixed subset of \(k\) of those bits. We introduce an extension of this task, which we call "multitask sparse parity". Beyond \(n\) and \(k\), multitask sparse parity adds an additional parameter \(n_{}\), the number of subtasks (number of distinct versions of sparse parity) present in the dataset. To construct the task, we first choose \(n_{}\) random subsets \(S_{i}\) of \(k\) indices from \(\{1,2,,n\}\): \(S_{i}\{1,2,,n\}\) and \(|S_{i}|=k\), where \(i=1,2,,n_{}\). Input bit strings are length \(n_{}+n\). We call the first \(n_{}\) bits the _control bits_ and the last \(n\) bits the _task bits_. If control bit \(i\) is active, then the parity is computed from the subset \(S_{i}\) of the task bits. The control bits 1-hot encode the task number: on a given input, only one control bit is set to \(1\) at a time - the rest are zero. For the sample shown below, since control bit \(2\) is active, the answer is the parity of the task bits \(S_{2}=\{2,7\}\), which is \(0\) for this input:

We impose a uniform distribution over the task bits. On the control bits, we impose a Zipfian distribution: the probability that a sample has control bit \(i\) active (and therefore the parity must be computed from the subset \(S_{i}\) of the task bits) is \(i^{-(+1)}\) where \(Z=_{i=1}^{n_{}}i^{-(+1)}\). This imposes a power law distribution over subtasks in data. Since answers are parities, this task can be treated as a binary classification problem on the subset of bit strings \(\{0,1\}^{n_{}+n}\) where for each string all but one bit of the first \(n_{}\) bits are zero.

### Power law scaling and emergence

We train ReLU MLPs with a single hidden layer to solve this task with cross-entropy loss. The input dimension is \(n_{}+n\). We use the Adam optimizer with a learning rate of \(10^{-3}\). To study scaling with respect to the number of model parameters, we train networks of varying width by sampling batches online. Within an individual single-epoch training run, we can study scaling in steps \(S\). To study scaling with respect to multi-epoch training dataset size \(D\), we use a network of sufficient width for capacity to not be a bottleneck, and for varying \(D\) we sample a training set of \(D\) samples and train for multiple epochs, recording model performance when mean test loss is lowest (early-stopping).

Training dynamics on the multitask sparse parity problem are highly nontrivial - on each individual subtask, loss follows a reverse-S curve, dropping after an initial plateau. This transition happens at different times for different subtasks, so the overall loss decreases smoothly, averaging over these transitions. See Appendix B for more discussion of training dynamics.

Figure 2 shows scaling curves on the multitask sparse parity problem. For the results shown, we used \(n_{}=500\), \(n=100\), \(k=3\), \(=0.4\), and a batch size of \(20000\). We vary training dataset size from 1e4 to 5e6 and vary hidden-layer width from 10 to 500 neurons. We train for 2e5 steps. In line with the theory from Section 2, we find that as we scale training data and parameters, networks learn more

Figure 2: **Top:** Neural networks exhibit power law scaling in loss w.r.t. parameters \(N\), training time \(S\), and training samples \(D\) (for multi-epoch training) when trained on the multitask sparse parity dataset. Here \(=0.4\) and we plot lines \( N^{-}\), \( S^{-/(+1)}\), \( D^{-/(+1)}\). **Bottom:** neural scaling broken down by subtask. Scaling behavior on individual subtasks exhibits emergence, where subtasks are suddenly learned above a particular scale. Power law neural scaling of mean test loss averages over a large number of qualitative changes in network performance (when broken down by subtask), with loss being driven to zero on an increasing number of subtasks which are power law distributed in frequency, a realization of the mechanism of neural scaling discussed in Section 2.

and more quanta (reducing loss on more and more subtasks), roughly in order of their frequency, and that this is what drives neural scaling. We see that scaling w.r.t. parameters is noisier than data scaling, possibly due to model initialization having some influence on which quanta are learned (for our data scaling experiments, we use the same seed and same model size for all runs). We also see that for scaling on individual subtasks, there is a rough scale of data or parameters below which networks do not learn the task, and above which they do. Smooth power law scaling therefore averages over a large number of emergent changes in model performance when properly decomposed by subtask, a proof of concept that the Quantization Model can be the mechanism of neural scaling for data with the right structure. See Appendix B for additional results and discussion on how the scaling exponents \(_{N},_{S},_{D}\) relate to the subtask distribution power law exponent \(+1\) empirically.

## 4 Decomposing LLM scaling laws

We now study how scaling curves for large language models decompose. For our experiments, we use the Pythia model suite from Eleuther AI , a set of decoder-only transformers of varying size trained on approximately 300 billion tokens of The Pile . We evaluate several models in the suite (ranging from 19m to 6.4b non-embedding parameters) on approximately 10 million tokens from the test set of The Pile. We record cross-entropy loss on every token, enabling us to study how loss on individual tokens, as well as how the distribution over losses, changes with model scale.

### The distribution over per-token losses

In Figure 3, we show how the distribution over losses scales with model size. First, we find that for the first six models in the Pythia sequence, the mean loss scales as a power law with exponent \(_{N}=0.083\), roughly in line with the parameter scaling exponent of \(0.076\) measured in . The 6.4b model does not fit the scaling curve well, so we excluded its loss when measuring the scaling exponent. Next, we plot the probability distribution over per-token losses \(p(L)\). We find that losses close to zero are by far the most common, and that scaling increases the portion of approximately-zero losses. We also plot \(Lp(L)\), the probability density over losses weighted by loss. The mean loss is the area under this curve. We see that despite approximately-zero-loss tokens being by far the most common, they do not contribute much mass to the mean loss. See Figure 11 for how these distributions change over training steps rather than across model size. We note that neural scaling in the wild is much more complicated than for multitask sparse parity - notably, the distribution over

Figure 3: **Left:** Scaling of mean test loss w.r.t. non-embedding parameters for the Pythia models . The parameter scaling exponent \(_{N}\) is measured to be \( 0.083\) from the first six points along the curve (the seventh model appears to break the trend). **Center:** the distribution \(p(L)\) over losses on individual samples for models of different size. Losses \( 0\) are by far the most common, and larger models achieve \( 0\) loss on an increasing fraction of samples. **Right:** the expected loss integrand \(Lp(L)\) for models of different sizes. Low-loss samples contribute minimal mass to the mean loss, which is instead dominated by samples with much higher loss of 5-10 bits (depending on scale).

losses is not bimodal. We leave a detailed study of whether the statistics of neural scaling in LLMs are compatible with prior models of neural scaling to future work.

### Monogenic versus polygenic scaling curves

In our introduction of the Quantization Hypothesis in Section 2 and our multitask sparse parity study in Section 3 we modeled network performance on individual samples as benefitting from a single quantum - all samples belong to a single subtask, which is either solved or not solved in a binary fashion. In our model and on multitask sparse parity, scaling curves on individual examples all exhibit emergence - loss on individual examples undergoes a sharp transition at a particular scale of parameters or data. Do we observe this in large language models?

Inspecting a large number of per-token (per-sample) scaling curves, we observe a variety of scaling behaviors. On some samples, loss drops at a particular scale. More typically though, loss improves at multiple scales. If the Quantization Hypothesis is true and the effect of scaling is to simply add new quanta to the model, then for per-sample loss curves to show progress at multiple scales, those samples must benefit from multiple quanta additively. As first mentioned in Section 2, we borrow terminology from genetics and refer to prediction problems for which the model's performance is determined by a single quantum as _monogenic_ (akin to when a single gene determines a trait) and as _polygenic_ when multiple quanta influence performance (in analogy to when multiple genes contribute to a trait). In multitask sparse parity, all prediction problems are monogenic. In natural language, we observe that model performance on most tokens improves at multiple scales, suggesting that most tokens are polygenic, but we can find tokens for which loss drops as a single phase transition in scale. Polygenicity forms a spectrum: the smoothness of the loss curve can vary substantially between examples, presumably with some prediction problems using few quanta and others using many. In Figure 4, we show extreme examples of both monogenic and polygenic samples.

Note that our monogenic/polygenic taxonomy of model behaviors assumes that QH1 and QH2 are true. However, it could be the case that there isn't an underlying discreteness to what is learned, or that scaling totally changes what networks learn, rather than simply adding additional quanta. Whether scaling truly has the effect we described will have to be investigated in future studies of the internals of neural networks. We also note that it is possible that sharp transitions in the per-token loss curves could be due to noise - if we had multiple runs with different random seeds for each model scale, we could better test whether the mean loss across seeds decreases smoothly or if there is a genuine discreteness where gradual progress is impossible for apparently "monogenic" tokens.

Figure 4: Per-sample scaling curves can have diverse behavior. Here we show extreme examples where scaling (of loss on predicting the highlighted token) is abrupt versus smooth. If the Quantization Hypothesis describes language modeling, then samples with sharp scaling would be _monogenic_, displaying sharp emergence at a particular model scale when the relevant quantum is learned. Samples with gradual scaling would be _polygenic_, where many quanta, emerging at different scales, marginally improve the loss. We show additional examples in Figure 12.

The quanta of language modeling

We have conjectured that the internals and behavior of language models are decomposable into an enumerable set of modules and associated skills (quanta). What might these basic building blocks of LLMs be? In this section, we develop a preliminary method to discover quanta. In particular, we will attempt to cluster tokens in a language corpus according to what knowledge or skill LLMs use to predict those tokens from their context. Our goal is to find coherent clusters of language model behavior that each reveal some distinct skill that the model has learned. Note that in _clustering_ tokens to discover quanta, we are making the likely unrealistic assumption that these tokens are monogenic - that there is only one quantum involved in predicting each token. Not also that these clusters of behavior will not give us a mechanistic understanding of the quanta, but simply provide examples of LLM skills which could be studied further in future work.

We propose the use of gradients to cluster next-token prediction samples, where a "sample" consists of a token and its context in some document. Given some model, we will cluster two samples together if the gradient of the model's loss on each sample w.r.t. the model's parameters is similar for the two samples. The intuition for using gradients is as follows: if a model uses the same internal module to generate its prediction on two samples, then the gradients for parameters within the module may be nonzero and similar for the two samples (and possibly \( 0\) for parameters in irrelevant modules). If a model uses different modules to generate its prediction on different samples, then the gradients may not overlap. We therefore use gradient similarity as a proxy for _mechanistic similarity_ - whether a model uses similar mechanisms/modules to generate its prediction on distinct samples. While crude, we find that gradients contain enough information to allow us to automatically discover many coherent clusters of LLM behavior using the following algorithm:

**Quanta Discovery from Gradients (QDG)**: We will use spectral clustering on gradients to find clusters of samples whose gradient has nonzero cosine similarity. Given a set of samples \((x_{i},y_{i})\) and a model \(f_{}\), we compute gradients for each sample \(g_{i}=_{}L(f_{}(x_{i}),y_{i})\). We then normalize these gradients \(g_{i}_{i}\) so that \(_{i}_{i}=1\). Let \(A\) be a matrix whose rows are the normalized gradients: \(A_{i,}=_{i}\). If we are clustering \(d\) samples and our model has \(n\) parameters, \(A\) has shape \((d,n)\). We compute an affinity matrix \(C=AA^{T}\), a matrix of shape \((d,d)\) where \(C_{ij}=_{i}_{j}\), the cosine similarity between gradients \(g_{i},g_{j}\). From this, we compute an affinity matrix of the angular similarities \(\) (which take values in \(\)) via \(_{ij}=1-(C_{ij})/\). We then perform spectral clustering with \(\) to cluster samples.

QDG is expensive to compute for large models and for large numbers of samples. We therefore only apply it to the smallest model in the Pythia suite, which has 19m non-embedding parameters. We cluster 10000 tokens on which this model is confident and correct in its prediction, achieving less than \(0.1\) nats of cross-entropy. See Appendix C.1 for more detail.

We find that many, though not all, QDG clusters reveal some coherent model behavior. We show examples from clusters in Figure 1 and Figure 13. These clusters were found with the spectral clustering hyperparameter n_clusters = 400. While most clusters involve the prediction of the same token, manually inspecting these clusters we find that they usually involve predicting the same token for a coherent reason, rather than being based merely on having the same output. We also find clusters for more abstract prediction rules. For instance, the quantum shown on the left column of Figure 1 is the skill of incrementing a numerical sequence, and the examples involve predicting a variety of different tokens representing numbers.

### The natural distribution over language modeling quanta

In our model, some quanta are more frequently used than others. If these frequencies follow a power law in accordance with the Quantization Hypothesis, then we may expect QDG cluster sizes to be governed by a power law. The measured scaling exponent of \(_{N}=0.083\) from Figure 3 implies a power law distribution over quanta with exponent \(-1.083\). Do the cluster sizes follow this?

Figure 5 shows rank-frequency curves for clusters discovered with QDG for varying choices of n_clusters. These curves sort the clusters according to their size and then plot size against cluster index (rank). We plot rank-frequency curves for many choices of n_clusters since it is unclear a priori which n_clusters to use. When we measure the slope of the rank-frequency curve, we measure it from the envelope formed by the many rank-frequency curves, a practice which we

discuss in Appendix E. Biases in the clustering algorithm and inherent noise in model gradients make clustering imperfect, and lead to high uncertainty of our the measured power law exponent. From our analysis in Appendix E, we think that extracting the power law exponent over quanta utilization frequency by measuring the slope of the rank-frequency curve should have uncertainty of at least 0.2. We also note that some rank-frequency curves don't look like a clean power law. In Appendix E, Figure 16 we find that we can get similar-looking curves in a toy model of this clustering process when the dimension and noise is high. Between ranks 100-1000, we measure a slope of \(-1.24\), about \(0.16\) off our expected slope of \(-1.08\), and so within the margin of error. We are encouraged that the size of our discovered clusters seem to decay at a rate (very roughly) compatible with observed neural scaling exponents, in line with our theory. However, less naive clustering schemes, operating on more samples with more clusters, could be useful to sharpen this measurement.

## 6 Related Work

**Models of neural scaling**: Several models of neural scaling laws have been proposed in prior work. Sharma and Kaplan  explain power law scaling w.r.t. model parameters using an argument from approximation theory, which relates neural scaling exponents to the dimension of the data manifold \(d\). Michaud et al.  point out that effective dimension \(d\) could be generalized to the maximum arity of the target function's computation graph for sparse compositional problems. Bahri et al.  generalized the model of Sharma and Kaplan  to scaling w.r.t. dataset size, additionally relating scaling exponents to the power law spectrum of certain kernels. Maloney et al.  develop an exactly solvable random-feature model of scaling, from which they derive a joint parameter-data scaling law. Bordelon et al.  develop a model of data scaling for kernels, decomposing the generalization error into a sum over eigenmodes, whereas we decompose error into a sum over quanta. Arguably the closest prior work to ours is Hutter , who develops a model of data scaling wherein a discrete set of "features" must be learned. In this model, a feature is learned if it occurs at least once in the training set. If the features are Zipfian distributed, this produces power law scaling in expectation but with high variance. In our model, using a data threshold \( 1\) lowers the variance in the scaling curve, and we also considered scaling w.r.t. parameters and applied the model to real networks.

**Understanding emergent abilities**: Wei et al.  and Srivastava et al.  document examples of emergent abilities in large language models, though Schaeffer et al.  suggest that these examples

Figure 5: **Left:** angular similarity between model gradients for a variety of natural language samples. Samples are reordered according to their QDG cluster (with 400 clusters) to reveal the block-diagonal structure of the similarity matrix. We visualize a small part of the overall similarity matrix in this plot – note that not all clusters are as visibly distinct as the ones shown. **Right:** rank-frequency plot of QDG clusters. We measure the slope of the envelope of the rank-frequency curves from cluster rank 100-1000 to be \(-1.24\), which is a steeper than the slope of -1.08 expected from the measured parameter-scaling exponent from Figure 3, though within the margin of error given the uncertainty of our clustering methodology. See Appendix E for a discussion of the bias/uncertainty of our method.

are an artifact of the metric used to evaluate performance. Arora and Goyal  develop a framework for the emergence of "skills", where predicting text requires combining multiple different skills from an underlying set of language skills.

**Miscellaneous**: The topic of phase transitions in machine learning is not new , but our work was strongly influenced by the recent work of Olsson et al.  who observe a phase change from the formation of induction heads and especially Nanda et al.  who conjecture that phase changes may be ubiquitous. Simon et al.  also exhibit a task where learning proceeds as a series of discrete steps. Chen et al.  develop a framework for understanding LLM "skills" in a hierarchy and for choosing data to more efficiently learn desired skills. Chan et al.  study how a Zipfian data distribution influences in-context learning.

## 7 Discussion

**Summary**: The Quantization Hypothesis posits that for some types of prediction problems, models must learn a discrete (quantized) set of modules/knowledge/skills (quanta). When data is distributed in such a way that the "use frequencies" of these quanta follow a power law, then power law neural scaling can arise as models learn more and more quanta, with smooth scaling curves averaging over many small cases of emergence. We presented a toy dataset where neural scaling exhibits these properties. We then documented how language model scaling curves decompose, beyond simply how the mean loss scales. Lastly, we developed a method to discover quanta from the internal structure of trained models, from which we were able to enumerate a large number of skills of a small language model. The frequencies at which the quanta we discover are used for prediction in natural text seem to roughly track the power law our theory would predict, though this measurement is quite imprecise.

**Limitations**: While the Quantization Hypothesis appears to hold for our toy datasets, much work remains in investigating to what extent it holds for natural tasks like language modeling. Probably our riskiest assumption was that there is an underlying discreteness to _everything_ that models learn. Gradual scaling seems typical in LLMs , and it could be more parsimonious to model neural scaling as an underlying smooth process rather than to assume that most tasks are highly polygenic with underlying discrete quanta. Note also that in our model of scaling w.r.t. parameters \(N\), having more parameters merely increases the capacity of the network. In practice however, larger networks are more efficient learners , and one can trade off between parameters and data, whereas in our model parameters and data independently bottleneck the number of quanta that can be learned. Additionally, we modeled the quanta as being independent, where learning order is given just by the use frequencies, but it could make more sense to think of the quanta as living in a hierarchical dependency graph. Lastly, our QDG method is neither very principled nor scalable, and much better methods could likely be developed to discover quanta and study their statistics for larger models and across more samples.

**Implications for emergence and forecasting**: Srivastava et al.  find that on some tasks, neural scaling has high "linearity", with gradual improvements to scale, with other tasks displaying "breakthroughness", where performance improves sharply at some scale. In our model, high linearly would result from a task's relevant quanta being widely spread along the Q Sequence, and high breakthroughness would result from a task being monogenic or from the relevant quanta being close together in the Q Sequence. Our model also suggests that future capabilities could be forecasted if one could estimate the frequency at which that skill would benefit prediction in the training corpus.

**Implications for mechanistic interpretability**: If the Quantization Hypothesis is correct, then understanding a network reduces to enumerating its quanta. Having done this, the quanta could perhaps then be translated into a more interpretable format (something like code), studied in this format, and eventually executed in this format, rather than via the operation of the network.

**Outlook**: Lastly, our decomposition of networks into quanta is reminiscent of Minsky's _Society of Mind_ perspective that minds are decomposable into individually mindless "agents". If this decomposition is indeed possible, then the quanta (agents) become natural objects of study within networks. This _mesoscale_ understanding of networks, in terms of the internal modules which collectively constitute their performance, could perhaps act like statistical physics for deep learning, allowing us to bridge our microscale understanding of low-level training dynamics and our macroscale understanding of model performance.