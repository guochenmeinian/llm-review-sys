# FineStyle: Fine-grained Controllable Style Personalization for Text-to-image Models

Gong Zhang\({}^{1,2}\)1 Kihyuk Sohn\({}^{3}\)2 Meera Hahn\({}^{2}\) Humphrey Shi\({}^{1}\) Irfan Essa\({}^{1,2}\)

\({}^{1}\)Georgia Tech \({}^{2}\)Google DeepMind \({}^{3}\)Meta Reality Labs

###### Abstract

Few-shot fine-tuning of text-to-image (T2I) generation models enables people to create unique images in their own style using natural languages without requiring extensive prompt engineering. However, fine-tuning with only a handful, as little as one, of image-text paired data prevents fine-grained control of style attributes at generation. In this paper, we present FineStyle, a few-shot fine-tuning method that allows enhanced controllability for style personalized text-to-image generation. To overcome the lack of training data for fine-tuning, we propose a novel concept-oriented data scaling that amplifies the number of image-text pair, each of which focuses on different concepts (e.g., objects) in the style reference image. We also identify the benefit of parameter-efficient adapter tuning of key and value kernels of

Figure 1: Demonstration of fine-grained style controllability of FineStyle. Nine image pairs are generated by personalized text-to-image models, each of which is fine-tuned on a respective, single style reference image displayed at the corner of the left image of each pair. Fine-grained concepts are written on top of the images for comparisons, showing the nuanced compositionality encompassing color, foreground object, background, and textures. Full prompts are available in Appendix A.1. Visit https://github.com/SHI-Labs/FineStyle for code and more examples.

cross-attention layers. Extensive experiments show the effectiveness of FineStyle at following fine-grained text prompts and delivering visual quality faithful to the specified style, measured by CLIP scores and human raters.

## 1 Introduction

Text-to-image (T2I) models [38; 4; 40] have become a powerhouse driving various modern image-creation applications [11; 28; 8; 34] to generate unique artworks of diverse styles from natural language prompts. However, it is often challenging to faithfully describe the visual look of a style in pure text form. To better leverage the generation capability of these models, a series of works [39; 41; 10; 27] extend the one-step text-to-image generation paradigm to few-shot fine-tuning with a set of images followed by a personalized text-to-image generation. Following this two-step paradigm, users can create novel images inheriting the visuals of reference images without extensive prompt engineering . Although these works and their successors [52; 48] have made the text-to-image paradigms capable of conveniently generating with the guidance of reference images, they often result in content leakage, where visual clues of unwanted contents from the reference image appear in generated images. For example, as shown in Fig. 2, StyleDrop , state-of-the-art method for one-shot style-tuning of text-to-image generation model, suffers from a content leakage of generating spindle leaves in the background when asked to generate a sneaker.

Why does the content leakage happen for few-shot or one-shot fine-tuning? When training text-to-image generation models on a large amount of image-text paired data covering a wide variety of visual concepts, it is easy to decompose individual concepts both in the text and visual spaces and associate those concepts between these two spaces. On the other hand, it is challenging to correctly associate visual concepts with the corresponding text phrases using only a few or one training images, as the contents in the image are highly entangled. As such,  has proposed an iterative fine-tuning strategy where later rounds of fine-tuning are done with a set of generated synthetic images curated by automated or human feedback. This synthetic fine-tuning on many images is shown to be effective at disentangling visual concepts of subject and style, leading to an improved image-text association for personalized text-to-image generation. However, iterative training is prohibitive as it requires more computing and human resources.

This paper aims to develop an efficient fine-tuning method for T2I models from a single reference image. The primary insight is to fine-tune the models with additional objectives focusing on multiple fine-grained concepts within a text-image pair rather than solely on the text-image pair itself. To this end, we decompose a single style reference image and its text prompt into multiple concept-oriented sub-image-text pairs, as in Fig. 3(b-d). Concretely, the sub-image-text pair describes an individual fine-grained concept of a style reference image, such as a laptop, woman, or plant, instead of the original text prompt describing an entire image. Users may identify as many concepts of interest as they want from a main text prompt. Therefore, the more comprehensive the prompt is, the better the concept grounding our method can provide. We leverage the disentanglement capability residing in cross-attention layers in pretrained T2I models to obtain the spatial location of individual concepts.

In addition to the concept-oriented data scaling, we extend parameter-efficient fine-tuning to directly modify weights in the cross-attention layer instead of adding residuals to hidden features via adapters [18; 41]. The intuition behind this strategy is as follows: first, fine-tuning is a task that adds new cross-modal knowledge of a text-image pair to T2I models, while cross-attention layers are

Figure 2: StyleDrop  tends to leak contents of the style reference image into generated images, such as the spindle leaves in the background of “a sneaker”, even though it is not included in the text prompt. FineStyle learns by pinpointing desirable style attributes (e.g., flat cartoon vector art) and mitigates the leakage of unwanted content (e.g., spindle leaves) at generation.

places where the kind of knowledge is stored; as such, adapters at weight matrices of cross-attention layer has the potential to bring better expressivity than using adapters to transform hidden features outside cross-attention layer.

We test our method on Muse  as the T2I backbone over a diverse set of style reference images. We show that our method, FineStyle, is able to generate style-consistent images from text prompts while mitigating the content leakage, as in Fig. 2. Furthermore, our fine-tuning method promotes concept disentanglement, enabling novel applications like style editing. This empowers users to modify granular style attributes of a reference image, as visualized in Fig. 7. Extensive evaluation measuring the semantic and style fidelity using CLIP  and user studies show the enhanced performance of our method compared to baselines.

## 2 Related Work

**Text-to-image Synthesis**. Compared to unconditional image synthesis [21; 17; 42], in which models learn to create images randomly resembling their training data distribution, conditional image synthesis [37; 31; 40; 50; 4; 46; 47; 12] introduces extra conditioning on text/image prompts to guide the generative process. This explicit conditioning has made a series of downstream image-generative tasks more controllable with natural language prompts such as text-guided inpainting  and image editing [2; 22; 52].

**Cross-attention mechanism in T2I** has been used to implement the interaction between visual and textual features, enabling text conditioning in image synthesis.  demonstrates that cross-attention maps align the concepts in text prompts with their corresponding spatial positions on the generated image. A few works [13; 5; 51] have confirmed that modifying attention weights affects the layout and content in resulting synthesis.

**T2I Personalization Synthesis** extends the capability of pre-trained T2I models to generate images of novel concepts outside their training set. Given a small collection of reference images about a concept, it works by either optimizing the T2I model itself [39; 10; 22; 1] or injecting extracted image features into the generative process [14; 30; 48]. Dreambooth  fine-tunes all parameters of a T2I model and gains decent fidelity in synthesizing the target concept, but it comes at the cost of training and storage efficiency. Adopting the idea of parameter-efficient fine-tuning (PEFT) from NLP [18; 20; 6; 6], StyleDrop  learns a set of lightweight adapter layers appended to each transformer block of a generative vision transformer  from a single style image to improve data and training efficiency. On the other hand, StyleAligned  can generate a consistent image set of a style by extending the self-attention at inference time to encompass features of a reference image. As such, it achieves style personalization without optimization. However, they all implicitly deem a style image and its text description indivisible and ignore the importance of aligning fine-grained style and description in the context of more controllable personalization synthesis.

## 3 Preliminary

In this section, we review Muse , a masked generative transformer for text-to-image generation, and StyleDrop , a few-shot style-tuning built on Muse for style-personalized text-to-image generation.

**Muse** is a masked generative image transformer, or MaskGIT . It contains a pre-trained text encoder \(\), an image encoder \(\), a decoder \(\), and a generative transformer \(\). Muse uses T5-XXL  for \(\) and VQGAN [49; 7] for \(\) and \(\). \(\) encodes an image from pixel space to a sequence of discrete visual tokens \(v\) while \(\) encodes a text prompt into textual token space \(\). Namely, we are interested in obtaining \(:\) that takes in visual and textual tokens and outputs logits \(\). \(\) is trained to reconstruct masked visual tokens with conditioning textual tokens from a large text-image pair dataset \(\).

\[L=_{(x,t),m}[( (x),(((x),m),(t)))]\] (1)

where \((x,t)\) is an image-text pair and \(\) is a uniformly distributed mask sampling strategy with a mask ratio as a coefficient. \(\) is a weighted cross-entropy loss calculated by summing over losses at masked visual tokens. Once \(\) is trained, an image \(\) is synthesized by iterative decoding [4; 3] visual logits given a text prompt and an initial sequence of visual tokens. A sampling strategy \(\) samplesvisual tokens from output logits. Finally, \(\) maps visual tokens at the last step to the image of pixels.

\[=(v_{K}),v_{k}=((v_{k-1},(t))+ ((v_{k-1},(t))-(v_{k-1},(n))))\] (2)

where \(k[1,K]\) is the sampling step, \(t\) is the text prompt and \(n\) is the null prompt. The term with \(\) as a coefficient is classifier-free guidance .

With the cascade design from low to high resolution, Muse contains several sub-modules: a pair of low-res and high-res VQGAN operating at 256\(\)256 and 512\(\)512 resolutions, respectively, a base transformer for decoding low-res image tokens and a super-resolution transformer for translating low-res image tokens to high-res image tokens. We refer readers to  for additional details on the Muse model configurations.

**StyleDrop** is a few-shot style personalized text-to-image generation model built on the Muse . Given a dataset \(=\{(x_{i},t_{i})\}_{i=1}^{N}\) of \(N\) image-text pairs and a generative model \(\), we are interested in obtaining \(:\) that takes in an extra set of trainable parameters \(\) (i.e., adapter tuning ) to generate logits of visual tokens. Now the cross entropy loss over \(\) becomes:

\[L_{}=_{(x,t),m}[((x),(((x),m),(t), ))]\] (3)

With \(}\), users can generate novel images with style descriptor prompts that follow the style represented by \(\). In practice, the number of image-text pairs in \(\) could be very small, resulting in few-shot or even one-shot fine-tuning.

## 4 Method

Similarly to the StyleDrop , we build the FineStyle on Muse  via fine-tuning. In this section, we first discuss the challenges of fine-grained concept alignment in few-shot fine-tuning (Sec. 4.1). Then, we introduce building blocks of the FineStyle, a concept-oriented data scaling (Sec. 4.2) and parameter-efficient adapters (Sec. 4.3). Fig. 3 provides an overview of our method.

### Challenges of Fine-grained Concept Alignment in Few-shot Fine-Tuning

Recent text-to-image models demonstrate a strong image generation capability from natural language prompts [31; 40; 50; 4]. Although these models are trained to generate images from text prompts that describe an image as a whole, fine-grained concept alignment capabilities emerge through training on a large volume of image-text pairs. This enables models to compose multiple fine-grained concepts to generate a cohesive image semantically. However, it remains a challenge to achieve such a fine-grained concept alignment in few-shot fine-tuning, as the number of training examples are too limited to learn associations between textual concept descriptions and their visual representations.

Such an issue has been identified in a previous work . As a result of a poor concept disentanglement in few-shot fine-tuning, a content leakage happens at generation, i.e., some visual concepts are unexpectedly generated in an image even though they are not included in the text prompt. See Fig. 2 as an example, where generated images on the left by StyleDrop contain spindle leaves in the background, though the model is asked to generate an image of a sneaker. To address this, iterative training with either automated or human feedback  is proposed, where additional rounds of fine-tuning are conducted on synthetic images generated by models from earlier iterations. While this approach has proven effective, it suffers from expensive labor costs, extra annotation time, dependency on the quality of synthetic images, and risk of performance deteriorating due to human selection bias.

### Concept-oriented Data Scaling for Masked Decoding

As opposed to an iterative training of , we seek for an efficient, single-stage data scaling approach to enhance the concept alignment over target fine-tuning domains with limited data. This suggests us to explore ways to leverage the concept composition within limited data.

One critical observation is that the style reference image often contains multiple concepts. Many are spatially decomposable concepts (e.g., foreground subjects and background scenes and objects) while sharing the consistent visual style. For example, as in Fig. 3, a style reference image (top-left)contains multiple concepts, including a woman, a laptop, a plant, and a bookshelf. Furthermore, the objective of fine-tuning is to learn these implicit alignments between these visual and textual concepts, e.g., word "woman" to visual woman, word "laptop" to visual laptop on the desk, word "a pot of tall plant with spindle leaves" to visual a pot of plant, "words "flat cartoon vector art" to visuals of all concepts in the image. While we wish such a decomposition of an image and the text prompt and an association between individual textual and visual concepts naturally occur, it is not designed to do so. Taking these observations into consideration, we propose learning through concept-oriented conditioning on dedicated captions to enhance concept alignment.

**Concept Decomposition.** To achieve this, we first decompose a text prompt into multiple concept-oriented sub-text prompts. Similarly to , we construct a text prompt for a style reference image by combining the subject and style phrases. Specifically, we build a comprehensive text prompt  to describe multiple subjects, styles, and background attributes in the image. For example, as in Fig. 3, we use "woman", "laptop", "a pot of plant with spindle leaves", and "bookshelf" for foreground subjects and "flat cartoon vector art", "a light blue circle", and "white background" for style and background attributes. Then, we create a few sub-prompts by combining a prominent concept and style phrases from the text prompt, e.g., "{concept phrase} in {style phrase} style". As a result, in addition to the original style reference image and the text pair (Fig. 3(a)), we get a couple more text prompts such as "a laptop in flat cartoon vector art style" as in Fig. 3(b) or "a plant with spindle leaves in flat cartoon vector art style" as in Fig. 3(d), each of which focuses on a different subject in a style reference image. The process could be done manually or automated by using state-of-the-art vision large language models (vLLMs), as shown in Appendix A.6.

**Training with Concept-oriented Masking.** One way to train a model with decomposed concepts is to create a set of sub-images (and their corresponding sub-prompt pairs) by cropping around the concept area. While this sounds straightforward, it will introduce extra cumbersome such as mismatched image ratio. Instead of explicitly cropping an image with bounding boxes, we propose concept-oriented masking, which replaces the traditional strategy of randomly and uniformly masking visual tokens across the entire image with targeted masking of concept-specific areas. Note that

Figure 3: An overview of FineStyle framework, including the concept-oriented data scaling workflow (arrows) and PEFT adapters (green box) applied to key and value kernels within transformer blocks. **The workflow** starts from the top-left with a single image-text pair containing user-specified concepts in colored texts. The image-text cross-attention map (top-right) is retrieved from the dot product between the query and key matrices. From an attention map, we aggregate attention values corresponding to the user-specified concepts (e.g., laptop, woman, or plant) to create extra training pairs, as in (a), (b), and (c), each of which focuses on different subjects, derived from a single style image.

the Muse model is trained to predict masked tokens, where the masked token is chosen uniformly at random as in Fig. 3(a). We construct the concept-oriented mask from the segmentation map that aggregates the cross-attention weights of the pretrained Muse model for the corresponding concept, and tokens inside the concept-oriented mask serve as prediction targets. Details on deriving a segmentation map from aggregated cross-attention weights are provided in Appendix A.5. Fig. 3(a-c) shows the concept-oriented masks with corresponding sub-prompts. During the training, all four examples have an equal chance of appearing in a batch.

### Parameter-Efficient Adapter for Masked Generative Image Transformer

Parameter-efficient adapters [20; 33; 6; 9] have become the new norm for fine-tuning a large model. Compared to fine-tuning the entire model, the adapters have the advantage of being small and easily interchangeable. Most existing works have tested adding adapters at various places in T2I models, such as token embeddings  and intermediate hidden features [41; 30]. We argue that adding adapters to cross-attention layers is more beneficial for fine-grained style personalization. First, fine-grained style controllability with text prompts depends on the precise alignment between visual and textual features, and cross-attention layers are the places where this cross-modal interaction happens. Second, in the T2I generative model, we define the hidden features as inputs and outputs to self-attention, cross-attention or MLP layers in a generative transformer. They are usually of shape in [batch, num_visual_token, feat_dim] and finally used to predict logits of visual tokens, thus containing substantial neighborhood and spatial information. Overall, we limit our adapter to key and value kernels corresponding to textual prompts at the cross-attention layer, leaving the query kernel untouched.

Unlike the typical application of a dedicated LoRA layer to each transformer block of the image decoder, our method employs a singular main LoRA layer but modifies it with distinct biases for each transformer block. This adaptation reduces the number of trainable parameters and aims to mitigate potential overfitting issues, a critical aspect in maintaining model generalizability.

**Sampling with Adapter.** With adapted transformer \(}\), visual tokens \(v_{k}\) is obtained as below:

\[v_{k}=}}(v_{k-1},(t))+_{1}( }}(v_{k-1},(t))-}(v_{k- 1},(t)))+_{2}(}}(v_{k-1},(t))-}(v_{k-1},(n)))\] (4)

Compared to Eq. 2, we have an extra term with \(_{1}\), which computes the logit residuals of prompt \(t\) between adapted model and original model. Therefore, \(_{1}\) controls the strength of style, while the term with \(_{2}\) is classifier-free guidance for adapted model.

## 5 Experiment

We adopt the evaluation set from  containing 24 styles encompassing fine-art oil painting, 3D rendering, and sculpture. In Sec. 5.2, we report qualitative results of FineStyle and novel applications brought by enhanced fine-grained concept alignment. In Sec. 5.3, we test the semantic and style consistency of FineStyle-generated images using the CLIP score and human evaluation. In Sec. 5.4, we conduct ablations on components of our method. Implementation details are in Appendix A.3.

### Evaluation Setup

As the style example in Fig. 3, we define style descriptor ("flat cartoon vector art") and unique fine-grained style properties (e.g., "inside a light circle" and "white background"). During the evaluation, we create simple prompts in the pattern of "{subject} in {style descriptor} style" for synthesis unless otherwise specified.

### Qualitative Results

Fig. 1 shows the robust fine-grained style controllability using our method by modifying those unique style properties in training prompts. The prompts used for generation are composed of a subject, style descriptor, and style properties. All the comparison pairs are generated using the same random seed with only one property being different. It is clear from the result that FineStyle supports the control over properties such as color, texture, background, and decoration of a subject.

Fig. 4 compares FineStyle with baselines [15; 1; 48; 41] in 5 styles. The same prompt is used for both models to generate a set of 2 images in one inference pass. We can see that FineStyle consistently outperforms baselines regarding fine-grained concept alignment. The first example shows the content leakage problem: the bay and mountains entangle with "watercolor painting", causing them to leak into the generation of "a Japanese temple". Furthermore, StyleAligned  almost replicates the layout of the bay and cliffs from the reference image. In the second and third rows of results generated by StyleDrop , the starry night sky and the tree-shaped hair keep appearing in the generations of "a girl" and "modern office building" deteriorates to almost the same "house" in the reference image.

This phenomenon highly correlates to the semantic distance between the training example and the generation prompt. To better understand this phenomenon, we use a series of concepts ranging from semantic closeness to farness to construct comparison prompts. In Fig. 5, we test it using the style "melting golden 3d rendering". As the training image is a flower with triangle-shaped petals, we choose the semantic axis to be a flower specie, a plant, and a building. From left to right, the generations of StyleDrop are improving from flower-shaped objects to houses, even though some triangles can still be seen in certain parts of the house. On the other hand, FineStyle performs better all along the semantic axis with desirable overall style consistency. This suggests that enhancing fine-grained concept alignment can effectively

Figure 4: Qualitative comparison between FineStyle and various baselines. Unwanted concepts list those appearing in training prompts but should not be in synthesis prompts.

Figure 5: Generated images of “melting golden 3d rendering” style from text prompts of subjects whose semantic distance to the reference subject (“flower”) is gradually changed from close to far. StyleDrop creates images that follow the text prompt when the subject is far from the reference subject. In contrast, FineStyle creates images of subjects even when they are semantically close (“rose” or “mushroom”) to the reference subject.

#### 5.2.1 Extensive Style Control

Due to its fine-grained concept alignment, FineStyle allows extensive control over specific style attributes, even with limited visual representation. In Fig. 6(a), we demonstrate style control over melting drips. We adjust the state of the drips by omitting certain words or adding decorative elements. Results within the red box differ from those in the green box, showing no substantial changes in drip thickness or absence. Typically, without precise concept alignment, fine-tuned adapters tend to focus on large-area concepts, as seen with the spindle leaves in Fig. 6(b).

Furthermore, Fig. 6(b) demonstrates the feasibility of controlling two style attributes at the same time, suggesting the style adapter obtained from our fine-tuning algorithm is more compatible with the compositionality learned by the base model rather than unquestioningly learning to reconstruct every detail of training image at the same time.

#### 5.2.2 Controllable Reference Image Variation

Given an image and a conditioning mechanism, generative models can generate image variations. Usually, these variations happen only at the granular style property level with a relatively similar image structure. We note that during one-shot fine-intuning, the training image can be faithfully reconstructed with more training steps. While this might result in style overfitting, it opens the possibility of an interesting application of controllable image variation. As in Fig. 7, both StyleDrop and FineStyle can reconstruct the training image with minimum fidelity loss. However, only FineStyle can get a clean, starry night variation without the traces of trees, mountains, and black borders.

### Quantitative Results

We synthesize images by combining a filtered version of Parti  prompts and 10 styles from the evaluation set, details in Appendix A.2. There are 190 examples in Parti prompts. Each one describes a composition of a subject. The subject comes with its superclass to reduce semantic ambiguity (e.g. A cat, animals, in watercolor painting style.). We generate 4 images for each example, adding to 760 images for a style.

Figure 6: Extensive Style Control. (a) modifies a fine-grained style by omitting it or adding decoration to it. (b) controls multiple fine-grained styles at the same time.

Figure 7: An example reference image variation. The last image without “tree, mountain, village” is synthesized with the prompt “a clear starry night sky close up in oil painting style on a blue background”.

**CLIP score.** We utilize CLIP  to calculate Text (text-image) and Style (image-image) scores. The Text score is between a generated image and its text prompt, measuring how well the image follows it. The Style score is between generated and style reference images, measuring style fidelity. However, it is not the higher the score, the better since high scores might indicate content leakage or mode collapsing.

FineStyle achieves higher Text scores than StyleDrop (0.314 v.s. 0.297) and reasonable Style scores (0.661, higher than Muse's 0.552 and lower than StyleDrop's 0.708). Since the content leakage problem results in prompt-image misalignment, the improved Text scores imply that FineStyle alleviates the problem while still maintaining competitive style fidelity.

**Human Evaluation.** Given a reference image and a pair of synthesized images from two comparable models, users are asked to select the one that 1) corresponds better to the style of the reference image (Style); 2) better matches the prompt (Text); 3) makes more common sense. For example, given a prompt of "a goat with drips in melting golden 3d rendering style on a solid white background", a generated image, as in Fig. 12, should show a goat with 4 legs as it aligns with the common understanding, even though it is not specified in the prompt. We provide more details on the human evaluation in the Appendix A.4.

The results in Tab. 2 demonstrate that FineStyle is significantly preferred in Text and Structure/Common Sense. While StyleDrop wins in Style, there is still a large portion of ties, showing FineStyle maintains comparable or better performance for more than half (56.4%) of the test cases. These results are congruent with the CLIP score evaluations in Tab. 1, but additionally provide information about common sense reasoning.

### Ablation Study

#### 5.4.1 Concept-oriented data scaling and KV adapter

We study the effectiveness of the main components in FineStyle by training model variants that disable one of them. The full FineStyle uses concept-oriented data scaling (data scaling) and adapters at key and value kernels (kv). In contrast, StyleDrop uses adapters at hidden features (feat) after a transformer layer. We train two variants: (a) data scaling with feat adapter and (b) only kv adapter. Comparing (a) with (b) in Tab. 1, we see that (a) has a lower Text score (0.296) but a problematically

   Method & data scaling & adapter & Text score (\(\)) & Style score (\(\)\(\)) \\  Muse & - & - & 0.320 & 0.552 \\ StyleDrop & \(\) & feature & 0.297 & 0.708 \\  variant (a) & \(\) & feature & 0.296 & 0.730 \\ variant (b) & \(\) & kv kernel & 0.308 & 0.686 \\ FineStyle & \(\) & kv kernel & 0.314 & 0.661 \\   

Table 1: CLIP scores measuring image-text similarity (Text) and image-image similarity (Style). We test FineStyle alongside two variants: (a) with data scaling and a feature adapter after transformer layers, and (b) without data scaling, using an adapter at key and value kernels within transformer layers. FineStyle demonstrates the best balance between text and style scores.

    & StyleDrop & tie & FineStyle \\  Text (\(\)) & 10.8\% & 21.9\% & 67.1\% \\ Style (\(\)) & 43.5\% & 27.5\% & 28.9\% \\ Structure / Common Sense (\(\)) & 23.6\% & 11.1\% & 65.2\% \\   

Table 2: We report the scores of human evaluation over pairs of generated images from StyleDrop and FineStyle. Images generated from FineStyle are much preferred by users regarding image-text alignment and structure / common sense alignment.

high Style score (0.730). These scores align with our expectation that feat adapter tends to capture visual and spatial information that may ignore dynamic style compositions, leading to a problematic high Style score but worse controllability. Moreover, (b) gets a better Text score (0.308) without data scaling, which indicates the kv adapter is a better design for disentangling fine-grained styles in a few-shot setting.

#### 5.4.2 Inference Hyperparameters

We present the inference formula in Eq. 4. It has two hyperparameters \(_{1}\) and \(_{2}\) controlling style and semantics, respectively. In Fig. 8, we test their effects by changing them. As \(_{1}\) increases, the synthesized image obtains more details from the reference. At \(_{1}=10\), boats appear on the water, even though they are not in the prompt. This signifies that learned style unfavorably dominates the sampling and causes content (e.g., boats) to leak. Then, we increase \(_{2}\), and the boats gradually vanish, making the image following the prompt more accurate.

## 6 Conclusion

In this paper, we introduce FineStyle, a method for style personalization of text-to-image models that requires only a single reference image. It comprises two key components: concept-oriented data scaling and an adapter applied to key and value kernels. Central to our approach is the utilization of the cross-attention mechanisms inherent in pre-trained text-to-image models. By leveraging this existing cross-modal knowledge, FineStyle effectively tailors style elements with precision, allowing for detailed customization in generated images.