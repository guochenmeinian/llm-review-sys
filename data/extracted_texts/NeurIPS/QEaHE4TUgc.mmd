# Fast Trac:

A Parameter-Free Optimizer for

Lifelong Reinforcement Learning

 Aneesh Muppidi

Harvard College

aneeshmuppidi@college.harvard.edu

&Zhiyu Zhang

Harvard University

zhiyuz@seas.harvard.edu

&Heng Yang

Harvard University

hankyang@seas.harvard.edu

###### Abstract

A key challenge in lifelong reinforcement learning (RL) is the loss of plasticity, where previous learning progress hinders an agent's adaptation to new tasks. While regularization and resetting can help, they require precise hyperparameter selection at the outset and environment-dependent adjustments. Building on the principled theory of online convex optimization, we present a parameter-free optimizer for lifelong RL, called Trac, which requires no tuning or prior knowledge about the distribution shifts. Extensive experiments on Procgen, Atari, and Gym Control environments show that Trac works surprisingly well--mitigating loss of plasticity and rapidly adapting to challenging distribution shifts--despite the underlying optimization problem being nonconvex and nonstationary. Project website and code is available here.

## 1 Introduction

Spot, the agile robot dog, has been learning to walk confidently across soft, lush grass. But when Spot moves from the grassy field to a gravel surface, the small stones shift beneath her feet, causing her to stumble. When Spot tries to walk across a sandy beach or on ice, the challenges multiply, and her once-steady walk becomes erratic. Spot wants to adjust quickly to these new terrains, but the patterns she learned on grass are not suited to gravel, sand, or ice. Furthermore, she never knows when the terrain will change again and how different it will be, therefore must continually plan for the unknown while avoiding reliance on outdated experiences.

Spot's struggle exemplifies a well-known and extensively studied challenge in real-world decision making: _lifelong reinforcement learning_ (lifelong RL) Abel et al. (2024); Nath et al. (2023); Mendez et al. (2020); Xie and Finn (2022). In lifelong RL, the learning agent must continually acquire new knowledge to adapt to the nonstationarity of the environment. At first glance, there appears to be

Figure 1: Severe loss of plasticity in Procgen (Starpilot). There is a steady decline in reward with each distribution shift.

an obvious solution: given a policy gradient oracle, the agent could just keep running gradient descent nonstop. However, recent experiments have demonstrated an intriguing behavior called _loss of plasticity_(Dohare et al., 2021; Lyle et al., 2022; Abbas et al., 2023; Sokar et al., 2023): despite persistent gradient steps, such an agent can gradually lose its responsiveness to incoming observations. There are even extreme cases of loss of plasticity (known as _negative transfer_ or _primacy bias_), where prior learning can significantly hamper the performance in new tasks (Nikishin et al., 2022; Ahn et al., 2024); see Figure 1 for an example. All these suggest that the problem is more involved than one might think.

From the optimization perspective, the above issues might be attributed to the _lack of stability_ under gradient descent. That is, the weights of the agent's parameterized policy can drift far away from the origin (or a good initialization), leading to a variety of undesirable behaviors.1 Fitting this narrative, it has been shown that simply adding a \(L_{2}\) regularizer to the optimization objective (Kumar et al., 2023) or periodically resetting the weights (Dohare et al., 2021; Asadi et al., 2024; Sokar et al., 2023; Ahn et al., 2024) can help mitigate the problem. However, a particularly important limitation is their use of _hyperparameters_, such as the magnitude of the regularizer and the resetting frequency2. Good performance hinges on the suitable environment-dependent hyperparameter, but how can one confidently choose that _before_ interacting with the environment? The classical cross-validation approach would violate the one-shot nature of lifelong RL (and online learning in general; see Chapter 1 of Orabona, 2023), since it is impossible to experience the same environment multiple times. This leads to the contributions of the present work.

ContributionThe present work addresses the key challenges in lifelong RL using the principled theory of _Online Convex Optimization_ (OCO). Specifically, our contributions are two fold.

* **Algorithm:**TracBuilding on a series of results in OCO (Cutkosky and Orabona, 2018; Cutkosky, 2019; Cutkosky et al., 2023; Zhang et al., 2024), we propose a (hyper)-_parameter-free_ optimizer for lifelong RL, called Trac (AdapTive RegularAtion in Continual environments). Intuitively, the idea is a refinement of regularization: instead of manually selecting the magnitude of regularization beforehand, Trac chooses that in an online, data-dependent manner. From the perspective of OCO theory, Trac is insensitive to its own hyperparameter, which means that no hyperparameter tuning is necessary in practice. Furthermore, as an optimization approach to lifelong RL, Trac is compatible with any policy parameterization method.
* **ExperimentUsing _Proximal Policy Optimization_ (PPO) (Schulman et al., 2017), we conduct comprehensive experiments on the instantiation of Trac called Trac PPO. A diverse range of lifelong RL environments are tested (based on Procgen, Atari, and Gym Control), with considerably larger scale than prior works. In settings where existing approaches (Abbas et al., 2023; Kumar et al., 2023; Nath et al., 2023) struggle, we find that Trac PPO
* mitigates mild and extreme loss of plasticity;
* and rapidly adapts to new tasks when distribution shifts are introduced. Such findings might be surprising: the theoretical advantage of Trac is motivated by the convexity in OCO, but lifelong RL is _both nonconvex and nonstationary_ in terms of optimization.

OrganizationSection 2 surveys the basics of lifelong RL. Section 3 introduces our parameter-free algorithm Trac, and experiments are presented in Section 4. We defer the discussion of related works and results to Section 5. Finally, Section 6 concludes the paper.

## 2 Lifelong RL

As a sequential decision making framework, _reinforcement learning_ (RL) is commonly framed as a _Markov Decision Process_ (MDP) defined by the state space \(\), the action space \(\), the transition dynamics \(P(s_{t+1}|s_{t},a_{t})\), and the reward function \(R(s_{t},a_{t},s_{t+1})\). In the \(t\)-th round, starting from a state \(s_{t}\), the learning agent needs to choose an action \(a_{t}\) without knowing \(P\) and \(R\). Then, the environment samples a new state \(s_{t+1} P(|s_{t},a_{t})\), and the agent receives a _reward_\(r_{t}=R(s_{t},a_{t},s_{t+1})\). There are standard MDP objectives driven by theoretical tractability, but from a practical perspective, we measure the agent's performance by its cumulative reward \(_{t=1}^{T}r_{t}\).

The standard setting above concerns a _stationary_ MDP. Motivated by the prevalence of distribution shifts in practice, the present work studies a nonstationary variant called _lifelong_ RL, where the transition dynamics \(P_{t}\) and the reward function \(R_{t}\) can vary over time. Certainly, one should not expect any meaningful "learning" against _arbitrary_ unstructured nonstationarity. Therefore, we implicitly assume \(P_{t}\) and \(R_{t}\) to be _piecewise constant_ over time, and each piece is called a _task_ - just like our example of Spot in the introduction. The main challenge here is to transfer previous learning progress to new tasks. This is reasonable when tasks are similar, but we also want to reduce the degradation when tasks turn out to be very different.

Lifelong RL as online optimizationDeep RL approaches, including PPO (Schulman et al., 2017) and others, crucially utilize the idea of _policy parameterization_. Specifically, a policy refers to the distribution of the agent's action \(a_{t}\) (conditioned on the historical observations), and we use \(_{t}^{d}\) to denote the parameterizing _weight vector_. After sampling \(a_{t}\) and receiving new observations, the agent could define a _loss function_\(J_{t}()\) that characterizes the "hypothetical performance" of each weight \(^{d}\). Then, by computing the _policy gradient_\(g_{t}= J_{t}(_{t})\), one could apply a _first order optimization algorithm3_ OPT to obtain the updated weight, \(_{t+1}=(_{t},g_{t})\).

For the rest of this paper, we will work with such an abstraction. The feedback of the environment is treated as a _policy gradient oracle_\(\), which maps the time \(t\) and the current weight \(_{t}\) into a policy gradient \(g_{t}=(t,_{t})\). Our goal is to design an optimizer OPT well suited for lifelong RL.

Lifelong vs. ContinualIn the RL literature, the use of "lifelong" and "continual" varies significantly across studies, which may lead to confusion. Abel et al. (2024) characterized _continual reinforcement learning_ (CRL) as a never-ending learning process. However, much of the literature cited under CRL, such as (Abbas et al., 2023; Ahn et al., 2024), primarily focuses on the problem of _backward transfer_ (avoiding catastrophic forgetting). Various policy-based architectures, such as those proposed by Rolnick et al. (2019); Schwarz et al. (2018); Nath et al. (2023), focus on tackling this issue. Conversely, the present work addresses the problem of _forward transfer_, which refers to the rapid adaptation to new tasks. Because of this we use "lifelong" rather than "continual" in our exposition, similar to (Thrun, 1996; Abel et al., 2018; Julian et al., 2020).

## 3 Method

Inspired by (Cutkosky et al., 2023), we study lifelong RL by exploiting its connection to _Online Convex Optimization_ (OCO; Zinkevich, 2003). The latter is a classical theoretical problem in online learning, and much effort has been devoted to designing _parameter-free_ algorithms that require minimum tuning or prior knowledge (Streeter & Mcmahan, 2012; McMahan & Orabona, 2014; Orabona & Pal, 2016; Foster et al., 2017; Cutkosky & Orabona, 2018; Mhammedi & Koolen, 2020; Chen et al., 2021; Jacobsen & Cutkosky, 2022). The surprising observation of Cutkosky et al. (2023) is that several algorithmic ideas closely tied to the convexity of OCO can actually improve the nonconvex deep learning training, suggesting certain notions of "near convexity" on its loss landscape. We find that lifelong RL (which is _both nonconvex and nonstationary_ in terms of optimization) exhibits a similar behavior, therefore a particularly strong algorithm (named Trac) can be obtained from principled results in parameter-free OCO. Let us start from the background.

Basics of (parameter-free) OcoAs a standalone theoretical topic, OCO concerns a sequential optimization problem where the convex loss function \(l_{t}\) can vary arbitrarily over time. In the \(t\)-th

Figure 2: Visualization of Trac’s key idea.

iteration, the optimization algorithm picks an iterate \(x_{t}\) and then observes a gradient \(g_{t}= l_{t}(x_{t})\). Motivated by the pursuit of "convergence" in optimization, the standard objective is to guarantee low (i.e., sublinear in \(T\)) _static regret_, defined as

\[_{T}(l_{1:T},u):=_{t=1}^{T}l_{t}(x_{t})-_{t=1}^{T }l_{t}(u),\]

where \(T\) is the total number of rounds, and \(u\) is a _comparator_ that the algorithm does not know beforehand. In other words, the goal is to make \(_{T}(l_{1:T},u)\) small for _all_ possible loss sequence \(l_{1:T}\) and comparator \(u\). Note that for _nonstationary_ OCO problems analogous to lifelong RL, it is better to consider a different objective called the _discounted regret_. Algorithms there mostly follow the same principle as in the stationary setting, just wrapped by _loss rescaling_(Zhang et al., 2024).

For minimizing static regret, classical _minimax_ algorithms like gradient descent (Zinkevich, 2003) would assume a small _uncertainty set_\(\) at the beginning. Then, by setting the hyperparameter (such as the learning rate) according to \(\), it is possible to guarantee sublinear _worst case regret_,

\[_{(l_{1:T},u)}_{T}(l_{1:T},u)=o(T).\] (1)

In contrast, parameter-free algorithms use very different strategies4 to bound \(_{T}(l_{1:T},u)\) directly (without taking the maximum) by a function of both \(l_{1:T}\) and \(u\). The resulting bound is more refined than Eq.(1) (Orabona, 2023, Chapter 9), and crucially, since there is no need to pick an uncertainty set \(\), much less hyperparameter tuning is needed. This is where its name comes from.

**TRAC for Lifelong RL:** In lifelong RL, a key issue is the excessive drifting of weights \(_{t}\), which can detrimantly affect adapting to new tasks. To address this, TRAC enforces proximity to a well-chosen reference point \(_{}\), providing a principled solution derived from a decade of research in parameter-free OCO. Unlike traditional methods such as \(L_{2}\) regularization or resetting, TRAC avoids hyperparameter tuning, utilizing the properties of OCO to maintain weight stability and manage the drift effectively.

The core of TRAC, similar to other parameter-free optimizers, incorporates three techniques:

* **Direction-Magnitude Decomposition**: Inspired by Cutkosky and Orabona (2018), this technique employs a carefully designed one-dimensional algorithm, the "parameter-free tuner," atop a base optimizer. This setup acts as a data-dependent regularizer, controlling the extent to which the iterates deviate from their initialization, thereby minimizing loss of plasticity, which is crucial given the high plasticity at the initial policy parameterization (Abbas et al., 2023).
* **Erfi Potential Function**: Building on the previous concept, the tuner utilizes the Erfi potential function, as developed by Zhang et al. (2024). This function is crafted to effectively balance the distance of the iterates from both the origin and the empirical optimum. It manages the update magnitude by focusing on the gradient projection along the direction \(_{t}-_{}\).
* **Additive Aggregation:** The tuner above necessitates discounting. Thus, we employ Additive Aggregation by Cutkosky (2019). This approach enables the combination of multiple parameter-free OCO algorithms, each with different discount factors, to approximate the performance of the best-performing algorithm. Importantly, it facilitates the automatic selection of the optimal discount factor during training.

These three components crucially work together to guarantee good regret bounds in the convex setting and are the minimum requirement for any reasonable parameter-free optimizer.

Without going deep into the theory, here is an overview of the important ideas (also see Figure 2 for a visualization).

* First, Trac is a meta-algorithm that operates on top of a "default" optimizer Base. It can simply be gradient descent with a constant learning rate, or Adam(Kingma and Ba, 2014) as in our experiments. Applying Base alone would be equivalent to enforcing the scaling parameter \(S_{t+1} 1\) in Trac, but this would suffer from the drifting of \(_{t+1}^{}\) (and thus, the weight \(_{t+1}\)).

* To fix this issue, Trac uses the tuner (Algorithm 2) to select the scaling parameter \(S_{t+1}\), making it _data-dependent_. Typically \(S_{t+1}\) is within \(\) (see Figure 17 to 19), therefore essentially, we define the updated weight \(_{t+1}\) as a _convex combination_ of the Base's weight \(_{t}^{}\) and the reference point \(_{}\), \[_{t+1}=S_{t+1}_{t+1}^{}+(1-S_{t+1})_{ }.\] This brings the weight closer to \(_{}\), which is known to be "safe" (i.e., not overfitting any particular lifelong RL task), although possibly conservative.
* To inject the right amount of conservatism without hyperparameter tuning, the tuner (Algorithm 2) applies an unusual decision rule based on the \(\) function. Theoretically, this is known to be optimal in an idealized variant of OCO (Zhang et al., 2022, 2024b), but removing the idealized assumptions requires a tiny bit of extra conservatism, which is challenging (and not necessarily practical). Focusing on the lifelong RL problem that considerably deviates from OCO, we simply apply the \(\) decision rule as is. This is loosely motivated by deep learning training dynamics, e.g., (Cohen et al., 2020; Ahn et al., 2023; Andriushchenko et al., 2023), where an aggressive optimizer is often observed to be better.
* Finally, the tuner requires a discount factor \(\). This crucially controls the strength of regularization (elaborated next), but also introduces a hyperparameter tuning problem. Following (Cutkosky, 2019), we aggregate tuners with different \(\) (on a log-scaled grid) by simply summing up their outputs. This is justified by the _adaptivity_ of the tuner itself: in OCO, if we add a parameter-free algorithm \(_{1}\) to any other algorithm \(_{2}\) that already works well, then \(_{1}\) can automatically identify this and "tune down" its aggressiveness, such that \(_{1}+_{2}\) still performs as well as \(_{2}\).

Connection to regularizationDespite its nested structure, Trac can actually be seen as a parameter-free refinement of \(L_{2}\) regularization (Kumar et al., 2023). To concretely explain this intuition, let us consider the following two optimization dynamics.

* First, suppose we run gradient descent with learning rate \(\), on the policy gradient sequence \(\{g_{t}\}\) with the \(L_{2}\) regularizer \(\|-_{}\|^{2}\). Quantitatively, it means that starting from the \(t\)-th weight \(_{t}\), \[_{t+1}=_{t}-[g_{t}+(_{t}-_{ })],_{t+1}-_{ }=(1-)(_{t}-_{ })- g_{t}.\] (2) That is, the updated weight \(_{t+1}\) is determined by a (\(1-\))-discounting with respect to the reference point \(_{}\), followed by a gradient step \(- g_{t}\).
* Alternatively, consider applying the following simplification of Trac on the same policy gradient sequence \(\{g_{t}\}\): (\(i\)) Base is still gradient descent with learning rate \(\); (\(ii\)) there is just one discount factor \(\); and (\(iii\)) the one-dimensional tuner (Algorithm 2) is replaced by the \(\)-discounted gradient descent with learning rate \(\), i.e., \(S_{t+1}= S_{t}- h_{t}\). In this case, we have \[_{t+1}-_{} =S_{t+1}(_{t+1}^{}-_{ })\] \[=( S_{t}- h_{t})(_{t}^{}-_{}- g_{t})\] \[=(- S_{t}^{-1}h_{t})(_{t}- _{})- S_{t+1}g_{t}. 0$)}\] Notice that \(S_{t}\) is a \(\)-discounted sum of \( h_{1},, h_{t-1}\), thus in the typical situation of \( 1\) one might expect \( h_{t}|S_{t}|\). Then, the resulting update of \(_{t+1}\) is similar to Eq.(2), with quantitative changes on the "effective discounting" \(1-\), and the "effective learning rate" \( S_{t+1}\).

The main message here is that under a simplified setting, Trac is almost equivalent to \(L_{2}\) regularization. The latter requires choosing the hyperparameters \(\) and \(\), and similarly, the above _simplified_ Trac requires choosing \(\) and \(\). Going beyond this simplification, the actual Trac removes the tuning of \(\) using aggregation, and the tuning of \(\) using the \(\) decision rule.

On the hyperparametersAlthough Trac is called "parameter-free", it still needs the \(\)-grid, the constant \(\) and the algorithm Base as inputs. The idea is that Trac is particularly insensitive to such choices, as supported by the OCO theory. As the result, the generic default values recommended by Cutkosky et al. (2023) are sufficient in practice. We note that those are proposed for training supervised deep learning models, thus should be agnostic to the lifelong RL applications we consider.

## 4 Experiment

Does Trac experience the common pitfalls of loss of plasticity? Does it rapidly adapt to distribution shifts? To answer these questions, we test Trac in empirical RL benchmarks such as vision-based games and physics-based control environments in lifelong settings (Figure 3). Specifically, we instantiate PPO with two different optimizers: Adam with constant learning rate for baseline comparison, and Trac for our proposed method (with exactly the same Adam as the input Base). We also test Adam PPO with _concatenated ReLU activations_ (CReLU; Shang et al., 2016), previously shown to mitigate loss of plasticity in certain deep RL settings (Abbas et al., 2023). Our numerical results are summarized in Table 1. Across every lifelong RL setting, we observe substantial improvements in the cumulative episode reward by using Trac PPO compared to Adam PPO or CReLU. Below are the details, with more in the Appendix.

ProcgenWe first evaluate on OpenAI Procgen, a suite of 16 procedurally generated game environments (Cobbe et al., 2020). We introduce distribution shifts by sampling a new procedurally generated level of the current game every 2 million time steps, treating each level as a distinct task.

Figure 3: Experimental setup for lifelong RL.

[MISSING_PAGE_FAIL:7]

Gym ControlWe use the CartPole-v1 and Acrobot-v1 environments from the Gym Classic Control suite, along with LunarLander-v2 from Box2d Control. To introduce distribution shifts, Mendez et al. (2020) periodically alters the environment dynamics. Although such distribution shifts pose only mild challenges for robust methods like PPO with Adam (Appendix D). We instead implement a more challenging form of distribution shift. Every 200 steps we perturb each observation dimension with random noise within a range of \( 2\), treating each perturbation phase as a distinct task.

Here (Figure 6), we notice a peculiar behavior after introducing the first distribution shift in both Adam PPO and CReLU: policy collapse. We describe this as an _extreme_ form of loss of plasticity. Surprisingly, Trac PPO remains resistant to these extreme distribution shifts. As we see in the Acrobot experiment, Trac PPO shows minimal to no policy damage after the first few distribution shifts, whereas Adam PPO and CReLU are unable to recover a policy at all. We investigate if Trac's behavior here indicates positive transfer in Appendix A. Across the three Gym Control environments,

 
**Environment** & **Adam PPO** & **CReLU** & **Trac PPO (Ours)** \\  Starpilot & \(3.4\) & \(3.6\) & \(\) \\ Dodgeball & \(1.9\) & \(2.3\) & \(\) \\ Chaser & \(1.4\) & \(1.7\) & \(\) \\ Fruitbot & \(0.1\) & \(1.0\) & \(\) \\ CartPole & \(5.1\) & \(1.2\) & \(\) \\ Acrobot & \(-14.3\) & \(-13.9\) & \(\) \\ LunarLander & \(-21.7\) & \(-19.4\) & \(\) \\ Atari 6 & \(3.1\) & \(4.8\) & \(\) \\ Atari 9 & \(3.9\) & \(17.0\) & \(\) \\  

Table 1: Cumulative sum of mean episode reward for Trac PPO, Adam PPO, and CReLU on Procgen, Atari, and Gym Control environments. Rewards are scaled by \(10^{5}\); higher is better.

Figure 5: Reward in the lifelong Atari environments, across games with action spaces of 6 and 9. Trac PPO rapidly adapts to new tasks, in contrast to the Adam PPO and CReLU which struggle to achieve high reward, indicating mild loss of plasticity.

Figure 6: Reward performance across CartPole, Acrobot, and LunarLander Gym Control tasks. Both Adam PPO and CReLU experience extreme plasticity loss, failing to recover after the initial distribution shift. Conversely, Trac PPO successfully avoids such plasticity loss, rapidly adapting when facing extreme distribution shifts.

Trac PPO shows an average normalized improvement of 204.18% over Adam PPO and 1044.24% over CReLU (Table 1).

## 5 Discussion

Related workCombating loss of plasticity has been studied extensively in lifelong RL. A typical challenge for existing solutions is the tuning of their hyperparameters, which requires prior knowledge on the nature of the distribution shift, e.g., (Asadi et al., 2024; Nath et al., 2023; Nikishin et al., 2024; Sokar et al., 2023; Mesbahi et al., 2024). An architectural modification called CReLU is studied in (Abbas et al., 2023), but our experiments suggest that its benefit might be specific to the Atari setup. Besides, Abel et al. (2018, 2018) presented a theoretical analysis of skill transfer in lifelong RL, based on value iteration. Moreover, related contributions in nonstationary RL, where reward and state transition functions also change unpredictably, are limited to theoretical sequential decision-making settings with a focus on establishing complexity bounds (Roy et al., 2019; Cheung et al., 2020; Wei Luo, 2021; Mao et al., 2020).

Our algorithm Trac builds on a long line of works on parameter-free OCO (see Section 3). To our knowledge, the only existing work applying parameter-free OCO to RL is (Jacobsen and Chan, 2021), which focuses on estimating the value function (i.e., policy evaluation). Our scope is different, focusing on empirical RL in lifelong problems by exploring the key connection between parameter-free OCO and regularization.

Particularly, we are inspired by the Mechanic algorithm from (Cutkosky et al., 2023), which goes beyond the traditional convex setting of parameter-free OCO to handle stationary deep learning optimization tasks. Lifelong reinforcement learning, however, introduces a layer of complexity with its inherent nonstationarity. Furthermore, compared to Mechanic, Trac improves the scale tuner there (which is based on the _coin-betting_ framework; Orabona and Pal, 2016) by the erfi algorithm that enjoys a better OCO performance guarantee. As an ablation study, we empirically compare Trac and Mechanic in the Appendix G (Table 3). We find that Trac is slightly better, but both algorithms can mitigate the loss of plasticity, suggesting the effectiveness of the general "parameter-free" principle in lifelong RL.

Trac encourages positive transferIn our experiments, we observe that Trac's reward decline due to distribution shifts is less severe than that of baseline methods. These results may suggest Trac facilitates positive transfer between related tasks. To investigate this further, we compared Trac to a privileged weight-resetting approach, where the network's parameters are reset for each new task, in the Gym Control environments (see Appendix A). Our results show that Trac maintains higher rewards during tasks than privileged weight-resetting and avoids declining to the same low reward levels as privileged weight-resetting at the start of a new task (Figure 8).

On the choice of \(_{}\)In general, the reference point \(_{}\) should be good or "safe" for Trac to perform effectively. One might presume that achieving this requires "warmstarting", or pre-training using the underlying Base optimizer. While our experiments validate that such warmstarting is indeed beneficial (Appendix B), our main experiments show that even a random initialization of the policy's weight serves as a _good enough_\(_{}\), even when tasks are similar (Figure 4).

This observation aligns with discussions by Lyle et al. (2023), Sokar et al. (2023), and Abbas et al. (2023), who suggested that persistent gradient steps away from a random initialization can deactivate ReLU activations, leading to activation collapse and loss of plasticity in neural networks. Our results also support Kumar et al. (2023)'s argument that maintaining some weights close to their initial values not only prevents dead ReLU units but also allows quick adaptation to new distribution shifts.

Tuning \(L_{2}\) regularizationThe success of Trac suggests that an adaptive form of regularization--anchoring to the reference point \(_{}\)--may suffice to counteract both mild and extreme forms of loss of plasticity. From this angle, we further elaborate the limitation of the \(L_{2}\) regularization approach considered in (Kumar et al., 2023). It requires selecting a regularization strength parameter \(\) through cross-validation, which is incompatible with the one-shot nature of lifelong learning settings. Furthermore, it is nontrivial to select the search grid: for example, we tried the \(\)-grid suggested by (Kumar et al., 2023), and there is no effective \(\) value within the grid for the lifelong RL environments we consider. All the values are too small.

Continuing this reasoning, we conduct a hyperparameter search for \(\), over various larger values \([0.2,0.8,1,5,10,15,20,25,30,35,40,45,50]\). Given the expense of such experiments, only the more sample-efficient control environments are considered. We discover that each environment and task responds uniquely to these regularization strengths (see bar plot of \(\) values in Figure 7). This highlights the challenges of tuning \(\) in a lifelong learning context, where adjusting for each environment, let alone each distribution shift, would require extensive pre-experimental analysis.

In contrast, Trac offers a parameter-free solution that adapts dynamically with the data in an online manner. The scaling output of Trac adjusts autonomously to the ongoing conditions, consistently competing with well-tuned \(\) values in the various environments, as demonstrated in the reward plots for CartPole, Acrobot, and LunarLander (Figure 7).

Trac compared to other plasticity methodsBoth layer normalization and plasticity injection Nikishin et al. (2024); Lyle et al. (2023) have been shown to combat plasticity loss. For instance, Appendix E Figure 15 demonstrates that both layer normalization and plasticity injection are effective at reducing plasticity loss when applied to the CartPole environment using Adam as a baseline optimizer. We implemented plasticity injection following the methodology laid out by Nikishin et al. (2024), where plasticity is injected at the start of every distribution shift. While this approach does help in reducing the decline in performance due to plasticity loss, our results indicate that it is consistently outperformed by TRAC across all three control environments--CartPole, Acrobot, and LunarLander. Moreover, while layer normalization improves Adam's performance, it too is outperformed by TRAC across the same control settings (Figure 15). Notably, combining layer normalization with TRAC resulted in the best performance gains.

Near convexity of lifelong RLOur results demonstrate the rapid adaptation of Trac, in lifelong RL problems with complicated function approximation. From the perspective of optimization, the latter requires tackling both nonconvexity and nonstationarity, which is typically regarded intractable in theory. Perhaps surprisingly, when approaching this complex problem using the theoretical insights from OCO, we observe compelling results. This suggests a certain "hidden convexity" in this problem, which could be an exciting direction for both theoretical and empirical research (e.g., policy gradient methods provably converge to global optimizers in linear quadratic control (Hu et al., 2023)).

LimitationsWhile Trac offers robust adaptability in nonstationary environments, it can exhibit suboptimal performance at the outset. In the early stages of deployment, Trac might underperform compared to the baseline optimizer. We address this by proposing a warmstarting solution detailed in Appendix B, which helps increase the initial performance gap.

## 6 Conclusion

In this work, we introduced Trac, a parameter-free optimizer for lifelong RL that leverages the principles of OCO. Our approach dynamically refines regularization in a data-dependent manner, eliminating the need for hyperparameter tuning. Through extensive experimentation in Procgen, Atari, and Gym Control environments, we demonstrated that Trac effectively mitigates loss of plasticity and rapidly adapts to new distribution shifts, where baseline methods fail. Trac's success leads to a compelling takeaway: empirical lifelong RL scenarios may exhibit more convex properties than previously appreciated, and might inherently benefit from parameter-free OCO approaches.

Figure 7: For each Gym Control environment and the initial ten tasks, we identified the best \(\), which is the regularization strength that maximizes reward for each task’s specific distribution shift. We also determined the best overall (well-tuned) \(\) for each environment. The results demonstrate that each environment and each task’s distribution shift is sensitive to different \(\) and that Trac PPO performs competitively with each environment’s well-tuned \(\).

Acknowledgments

We thank Ashok Cutkosky for insightful discussions on online optimization in nonstationary settings. We are grateful to David Abel for his thoughtful insights on loss of plasticity in relation to lifelong reinforcement learning. We appreciate Kaiqing Zhang and Yang Hu for their comments on theoretical and nonstationary RL. This project is partially funded by Harvard University Dean's Competitive Fund for Promising Scholarship.