# Deciphering Spatio-Temporal Graph Forecasting:

A Causal Lens and Treatment

Yutong Xia\({}^{1}\), Yuxuan Liang\({}^{2}\), Haomin Wen\({}^{3}\), Xu Liu\({}^{1}\), Kun Wang\({}^{4}\),

Zhengyang Zhou\({}^{4}\), Roger Zimmermann\({}^{1}\)

\({}^{1}\)National University of Singapore

\({}^{2}\)The Hong Kong University of Science and Technology (Guangzhou)

\({}^{3}\)Beijing Jiaotong University \({}^{4}\)University of Science and Technology of China

{yutong.x,yuxliang}@outlook.com; {liuxu,rogerz}@comp.nus.edu.sg wenhaomin@bjtu.edu.cn;wk520529@mail.ustc.edu.cn;zzy0929@ustc.edu.cn

Yuxuan Liang is the corresponding author of this paper. Email: yuxliang@outlook.com

###### Abstract

Spatio-Temporal Graph (STG) forecasting is a fundamental task in many real-world applications. Spatio-Temporal Graph Neural Networks have emerged as the most popular method for STG forecasting, but they often struggle with temporal out-of-distribution (OoD) issues and dynamic spatial causation. In this paper, we propose a novel framework called CaST to tackle these two challenges via causal treatments. Concretely, leveraging a causal lens, we first build a structural causal model to decipher the data generation process of STGs. To handle the temporal OoD issue, we employ the back-door adjustment by a novel disentanglement block to separate the temporal environments from input data. Moreover, we utilize the front-door adjustment and adopt edge-level convolution to model the ripple effect of causation. Experiments results on three real-world datasets demonstrate the effectiveness of CaST, which consistently outperforms existing methods with good interpretability. Our source code is available at [https://github.com/yutong-xia/CaST](https://github.com/yutong-xia/CaST).

## 1 Introduction

Individuals enter a world with intrinsic structure, where components interact with one another across space and time, leading to a spatio-temporal composition. _Spatio-Temporal Graph_ (STG) has been pivotal for incorporating this structural information into the formulation of real-world issues. Within the realm of smart cities , STG forecasting (e.g., traffic prediction  and air quality forecasting ) has become instrumental in informed decision-making and sustainability. With recent advances in deep learning, _Spatio-Temporal Graph Neural Networks_ (STGNNs)  have become the leading approach for STG forecasting. They primarily use Graph Neural Networks (GNN)  to capture spatial correlations among nodes, and adopt Temporal Convolutional Networks (TCN)  or Recurrent Neural Networks (RNN)  to learn temporal dependencies.

However, STG data is subject to temporal dynamics and may exhibit various data generation distributions over time, also known as _temporal out-of-distribution_ (OoD) issues or _temporal distribution shift_. As depicted in Figure 0(a), the training data (periods A and B) and test data derive from different distributions, namely \(P_{A}(x) P_{B}(x) P_{test}(x)\). Most prior studies  have overlooked this essential issue, which potentially results in suboptimal performance of STGNNs that are trained on a specific time period to accurately predict future unseen data.

Meanwhile, _dynamic spatial causation_ is another essential nature of STG data that must be addressed for effective and unbiased representation learning in STG forecasting. While the majority of STGNNsrely on a distance-based adjacency matrix to perform message passing in the spatial domain [61; 26; 11], they lack adaptability to dynamic changes in the relationships between nodes. This matrix is also sometimes inaccurate, as two closely located nodes may not necessarily have causal relationships, e.g., nodes belonging to different traffic streams. As an alternative solution, the attention mechanism [14; 64; 9] calculates the dynamic spatial correlations between nodes adaptively based on their input features. However, they still fall short of capturing the _ripple effects of causal relations_. Similar to how node signals can propagate information across graphs over time, causal relations (perceived as edge signals) can also exhibit this effect. For example in Figure 0(b), when an accident occurs between node A and B at \(t=1\), it directly reduces the causal relation 1. At \(t=2\), this effect propagates to other relations, such as weakening relation 2 and strengthening relation 3. This happens because the accident decreases the proportion of traffic flow from node B observed by node D, thus simultaneously increasing the proportion of traffic flow from node C observed by D.

In this paper, our goal is to concurrently tackle the temporal OoD issue and dynamic spatial causation via causal treatments . Primarily, we present a _Structural Causal Model_ (SCM) to gain a deeper understanding of the data generation process of STG data. Based on SCM, we subsequently propose to 1) utilize _back-door adjustment_ to enhance the generalization capability for unseen data; 2) apply _front-door adjustment_ along with an edge-level convolution operator to effectively capture the dynamic causation between nodes. Our contributions is outlined as follows:

* **A causal lens and treatment for STG data.** We propose a causal perspective to decipher the underlying mechanisms governing the data generation process of STG-structured data. Building upon the causal treatment, we devise a novel framework termed **C**ausal **S**patio-**T**emporal neural networks (**CaST**) for more accurate and interpretable STG forecasting.
* **Back-door adjustment for handling temporal OoD.** We articulate that the temporal OoD arises from unobserved factors, referred to as _temporal environments_. Applying the back-door adjustment, we design a disentanglement block to separate the invariant part (we call it _entity_) and environments from input data. These environments are further discretized by vector quantization  which incorporates a learnable environment codebook. By assigning different weights to these environment vectors, our model can effectively generalize on OoD data from unseen environments.
* **Front-door adjustment for capturing dynamic spatial causation.** Adopting a distanced-based adjacency matrix to capture spatial information around a node (referred to as _spatial context_) could include spurious causation. However, stratifying this context is computationally intensive, making back-door adjustments impractical for spatial confounding. We thus utilize the front-door adjustment and introduce a surrogate to mimic node information filtered based on the actual causal part in the spatial context. To better model the ripple effect of causation, we propose a novel de-confounding module to generate surrogate representations via causal edge-level convolution.
* **Empirical evidence.** We conduct extensive experiments on three real-world datasets to validate the effectiveness and practicality of our model. The empirical results demonstrate that CaST not only outperforms existing methods consistently on these datasets, but can also be easily interpreted.

## 2 Related work

**Spatio-Temporal Graph Forecasting.** Recently, STG forecasting has garnered considerable attention, with numerous studies focusing on diverse aspects of this domain. Based on the foundation of GNNs , STGNNs [50; 18; 38; 20] have been developed to learn the spatio-temporal dependencies in STG data. By incorporating temporal components, such as TCN  or RNN , STGNNs are capable of modeling both spatial correlations and temporal dependencies in STG data. Pioneering examples include DCRNN , STGCN , and ST-MGCN . Following these studies, Graph WaveNet  and AGCRN  leverage an adaptive adjacency matrix to improve the predictive performance. ASTGCN  and GMAN  utilize attention mechanisms to learn dynamic spatio-temporal dependencies within STG data. STGODE  and STGNCDE  capture the continuous spatial-temporal dynamics by using neural ordinary differential equations. ST-MetaNet [34; 35] and AutoSTG  exploit meta learning and AutoML for learning STGs, respectively. However, none of these approaches can simultaneously address the temporal OoD issue and dynamic spatial causation.

Figure 1: (a) Illustration of temporal OoD. (b) Spatial causal relationship in the traffic system.

**Causal Inference.** Causal inference [36; 12] seeks to investigate causal relationships between variables, ensuring stable and robust learning and inference. Integrating deep learning techniques with causal inference has shown great promise in recent years, especially in computer vision [62; 51; 29], natural language processing [37; 44], and recommender system [65; 10]. However, in the field of STG forecasting, the application of causal inference is still in its infancy. Related methods like graph-based causal models are typically designed for graph/node classification tasks [43; 66] and link prediction . For sequential data, causal inference is commonly used to address the temporal OoD issue by learning disentangled seasonal-trend representations  or environment-specific representations . When adapting to STGs, these methods face hurdles, as graph-based models cannot tackle temporal OoD issues, while sequence-based models fail to accommodate spatial dependencies. In this study, we investigate the STG data generation process through a causal lens and employ causal techniques to mitigate confounding effects in both the temporal and spatial domains for STG forecasting.

## 3 Causal Interpretation for STG Data Generation

**Problem Statement.** We denote \(X^{t}^{N D}\) as the signals of \(N\) nodes at time step \(t\), where each node has \(D\) features. Given the historical signals from the previous \(T\) steps, we aim to learn a function \(()\) that forecasts signals over the next \(S\) steps: \([X^{(t-T):t}()}{}[Y^{(t+1):(t+S)}]\), where \(X^{(t-T):t}^{T N D}\), \(Y^{(t+1):(t+S)}^{S N D^{}}\) and \(D^{}\) is the output dimension. For conciseness, we refer to \(X^{(t-T):t}\) as \(X\) and \(Y^{(t+1):(t+S)}\) as \(Y\) in the rest of the paper.

**A Causal Look on STG.** From a causal standpoint, we construct a Structural Causal Model (SCM)  (see Figure 1(a)) to illustrate the causal relationships among four variables: temporal environment \(E\), spatial context \(C\), historical node signals \(X\), and future signals \(Y\). Arrows from one variable to another signify causal-effect relationships. For simplicity, we assume \(E\) and \(C\) are mutually independent. Based on the above definitions, the causal relationships in Figure 1(a) can be denoted as \(P(X,Y|E,C)=P(X|E,C)P(Y|X,E,C)\). We detail these causal-effect relationships below:

* \(X E Y\). The temporal OoD is an inherent property of STG data, e.g., \(X\) and \(Y\), where \(P(X^{t}) P(X^{t+ t})\) at different time steps \(t\) and \(t+ t\). This phenomenon can arise due to changes in external variables over time, which we refer to as temporal environments \(E\). For example, external factors such as weather and events can significantly affect traffic flow observations.
* \(X C Y\). The historical and future data \(X\) and \(Y\) are intrinsically affected by the encompassing spatial context \(C\) surrounding a node. This influence, however, can comprise both spurious and genuine causal components. The spurious aspects may encompass nodes that exhibit either spatial distance or semantic similarity yet lack causal connections, as elucidated in Section 1.
* \(X Y\). This relation is our primary goal established by the prediction model \(Y=(X)\), which takes historical data \(X\) as input and produces predictions for future node signals \(Y\).

**Confounders and Causal Treatments.** Upon examining SCM, we observe two back-door paths between \(X\) and \(Y\), i.e., \(X E Y\) and \(X C Y\), where the temporal environment \(E\) and spatial context \(C\) act as confounding factors. This implies that some aspects of \(X\), which are indicative of \(Y\), are strongly impacted by \(E\) and \(C\). To mitigate the negative effect of the two confounders, we leverage the causal tools [12; 36] and _do-calculus_ on variable \(X\) to estimate \(P(Y|do(X))\), where \(do()\) denotes the do-calculus. For the temporal OoD, we employ a popular de-confounding method called _back-door adjustment_[57; 43] to block the back-door path from \(E\) to \(X\) (the red dashed arrow in Figure 1(b)), so as to effectively remove \(E\)'s confounding effect. This necessitates implicit environment stratification (see Eq. 1). Spatial confounding, however, cannot be addressed by spatial context stratification, due to the computational burden engendered by the multitude of nodes, each exhibiting a unique contextual profile. Fortunately, the _front-door adjustment_ allows us to introduce a mediating variable \(X^{*}\) between \(X\) and \(Y\) to mimic a more accurate representation excluded the spurious parts in \(C\) (the red node in Figure 1(c)) . Note that we do not use the front-door adjustment to \(E\) because this method mandates that the mediating variable is only affected by the cause variable and not by other confounding factors. While for temporal OoD scenarios, unseen future environments can be affected by time-varying factors, thus influencing the mediating variable. These two approaches effectively de-confound \(E\) and \(C\)'s confounding effects, explained as follows.

Figure 2: SCMs of (a) STG generation under real-world scenarios; (b) back-door adjustment for \(E\); (c) front-door adjustment for \(C\).

**Back-door Adjustment for \(E\)**. To forecast future time series \(Y\) based on historical data \(X\), it is imperative to address the confounding effect exerted by the _temporal confounder_\(E\). To achieve this, we initially envisage a streamlined SCM, where \(E\) constitutes the sole parents of \(X\) (temporarily disregarding \(C\)) and employ back-door adjustment  to estimate \(P(Y|do(X))\) by stratifying \(E\) into discrete components \(E=\{e_{i}\}_{i=1}^{|E|}\):

\[P(Y|do(X))=_{i=1}^{|E|}P(Y|X,E=e_{i})P(E=e_{i}) \]

where the prior probability distribution of the environment confounder \(P(E)\) is independent of \(X\) and \(Y\), allowing us to approximate the optimal scenario by enumerating \(e_{i}\).

**Front-door Adjustment for \(C\)**. Once we have dealt with \(E\), our next step is to de-confound the effect of spurious _spatial context_\(C\) by using the front-door adjustment . In the SCM depicted in Figure 2c, an instrumental variable \(X^{*}\) is introduced between \(X\) and \(Y\) to mimic the node representation conditioned on their real causal relationships with other nodes. We then estimate the causal effect of \(X\) on \(Y\) as follows:

\[P(Y|do(X))=_{x^{*}}_{x^{}}P(Y|X^{*}=x^{*},X=x^ {})P(X=x^{})P(X^{*}=x^{*}|X) \]

By observing \((X,X^{*})\) pairs, we can estimate \(P(Y|X^{*},X)\). This front-door adjustment provides a reliable estimation of the impact of \(X\) on \(Y\) while circumventing the confounding associations caused by \(C\). We put the derivations of Eq. 1 and Eq. 2 in Appendix A.

## 4 Model Instantiations

We implement the above causal treatments by proposing a **C**ausal **S**patio-**T**emporal neural network (**CaST**), as depicted in Figure 3. Our method takes historical observations \(X\) as inputs to predict future signals \(Y\). We will elaborate on the pipeline and each core component in the following parts.

**Back-door Adjustment** (see the top half of Figure 3). In order to attain Eq. 1, two steps need to be taken: (1) separating the environment feature from the input data, and (2) discretizing the environments. To accomplish this, we introduce an _Environment Disentangler_ block, and a learnable _Environment Codebook_ to obtain the desired stratification of environments.

**Front-door Adjustment** (see the bottom half of Figure 3). Obtaining \(X^{*}\) and collecting the \((X,X^{*})\) pairs to instantiate Eq. 2 is a non-trivial task that involves two main obstacles: (1) enumerating each spatial context, which can be computationally expensive, especially for large graphs, and (2) quantifying the causal effect of \(X\) on \(X^{*}\). To tackle these challenges, we introduce a block called Hodge-Laplacian (HL) Deconfounder, which is a neural topologically-based de-confounding module, to capture the dynamic causal relations of nodes as well as position embeddings to learn the nodes' global location information. With these two techniques, we can approximate the surrogate \(X^{*}\).

Figure 3: The pipeline of CaST. Env: Environment. Ent: Entity. Feat: Feature.

### Temporal Environment Disentanglement

**Overview.** As shown in Figure 3, the input signals \(X\) are first mapped to latent space as \(H^{T N F}\) by a Backbone Encoder before entering the Env Disentangler, where \(F\) means the hidden dimension. Then, this block separates \(H\) into the environmental feature \(H_{e}^{N F}\) and entity feature \(H_{i}^{N F}\), analogous to background and foreground objects in the computer vision field . Specifically, it captures environmental and entity information using two distinct components (see Figure 3(a)), including 1) EnvEncoder, which consists of a series of 1D convolutions, average pooling, and a linear projection; 2) EntEncoder, which extracts features from both time and frequency domains via Fast Fourier Transform and self-attention mechanism, respectively. The intuition of the block design of EnvEncoder and EntEncoder are discussed in Appendix B. After disentangled, for \(H_{e}\), we compare it to an Environment Codebook and select the closest vector as the final representation \(_{e}^{N F}\). The handling of \(H_{i}\) will be explained in Section 4.2.

**Environment Codebook.** To stratify the environment \(E\) in Eq. 1, we draw inspiration from  and develop a trainable environment codebook \(e=\{e_{1},e_{2},,e_{K}\}\), which defines a latent embedding space \(e^{K F}\). Here, \(K\) signifies the discrete space size (i.e., the total number of environments), and \(F\) denotes the dimension of each latent vector \(e_{i}\). As depicted in Figure 3, after acquiring the environment representation \(H_{e}\), we use a nearest neighbor look-up method in the shared embedding space \(e\) to identify the closest latent vector for each node's environment representation. Given the environment feature of the \(i\)-th node \(H_{e}(i)^{F}\), this process is calculated in the posterior categorical distribution \(q(z_{ij}=k|H_{e}(i))\) as follows:

\[q(z_{ij}=k|H_{e}(i))=1&k= _{j}||H_{e}(i)-e_{j}||_{2}, j\{1,2,,K\},\\ 0&. \]

Once obtaining the latent variable \(z^{N K}\), we derive the final environment representation \(_{e}^{N F}\) by replacing each row in \(H_{e}\) with its corresponding closest discrete vector in \(e\). Note that this categorical probability in Eq. 3 is only used during the training process, whereas _a soft probability is used during testing to enable generalization to unseen environments_. This soft probability signifies the likelihood of environment representation for each node belonging to each environment, denoted as \(_{e}(i)=_{j=1}^{K}q(z_{ij}|H_{e}(i))e_{j}\), where \(q(z_{ij}|H_{e}(i))\) ranges from \(0\) to \(1\). More discussion on how we achieve OoD generalization is provided in Appendix G.

**Representation Disentanglement.** We expect the environment and entity representations to be statistically independent, where entity representations carry minimal information (MI) about the environment. To achieve this, we employ an optimization objective inspired by Mutual Information Neural Estimation . MI measures the information shared between \(H_{e}\) and \(H_{i}\), which is calculated using the Kullback-Leibler (KL) divergence between the joint probability \(P(H_{e};H_{i})\) and the product of marginal distributions \(P(H_{e})P(H_{i})\):

\[(H_{e},H_{i})=D_{KL}[P(H_{e},H_{i})||P(H_{e})P(H_{i})]. \]

_By minimizing the mutual information, the overlap between \(H_{e}\) and \(H_{i}\) decreases. When it approaches zero, each representation is ensured to possess only self-contained information._ This approach transforms disentanglement into an optimization issue, which will be introduced in Section 4.3.

Figure 4: (a) The structure of Env Disentangler. AvgPool: average pooling. Linear: linear projection. Attn: attention. FFT: Fast Fourier Transform. iFFT: inverse FFT. (b) The overview of HL Deconfounder. GCN: graph convolution. (c) The embedding space for the Environment Codebook. The output \(_{e}\) is projected onto the closest vector \(e_{3}\) and the gradient \(_{z}\) pushes \(H_{e}\) to change.

### Spatial Context Filtering

**Overview.** Until now, we finished separating environment-entity and stratifying the environment \(E\). We then shift our focus to the entity \(H_{i}\). Our goal is to derive a surrogate \(_{i}\) (i.e., the latent variable of \(X^{*}\) in Figure 1(c)) that emulates a node representation containing only information propagated based on genuine causation within their spatial context. As emphasized in Section 1, it is essential to account for the ripple effects of causal relationships to accurately learn the surrogate. Thus the challenge is _how can we effectively model the ripple effect of dynamic causal relationships?_ Since nodes' causal relations can naturally be regarded as edge features, an intuitive solution for this challenge is to _execute convolution operations on edges_. Inspired by , we build a higher-order graph over edges and use an edge-level spectral filter, i.e., the Hodge-Laplacian operator, to represent the propagation of causal relations. This forms the core of HL Deconfounder block (see Figure 3(b)).

Moreover, the locational information of nodes incorporates a more global spatial perspective, which can be seamlessly implemented using a position embedding . We showcase the effects of it in Appendix F. Ultimately, the HL Deconfounder block ingests several inputs: the entity variable \(H_{i}\), the edge signal \(X_{ed}^{M F^{}}\), the boundary operator \(\), and the position embedding \(P^{N D_{p}}\), where \(M\) and \(F^{}\) mean the number of nodes and the dimension of their features in the built higher-order graph, respectively, and \(D_{p}\) denote the embedding dimension. These inputs are processed to yield \(_{i}^{N F}\). We then provide an exhaustive breakdown of this process, complemented with associated formulations.

**Edge Graph Construction.** We first construct a higher-order graph over edges by employing the boundary operator , which is a mathematical tool used in graph topology to connect different graph elements. Specifically, the first-order boundary operator \(_{1}\) maps pairs of nodes to edges, while the second-order boundary operator \(_{2}\) maps pairs of edges to triangles. Here we use \(_{1}\) and \(_{2}\) to establish a higher-order graph on edges, which facilitates subsequent convolution operations.

**Hodge-Laplacian Operator & Approximation.** With the edge graph, we can perform edge convolution to filter edge signals that contain genuine causation for a node. The Hodge-Laplacian (HL) operator [16; 39] is a spectral operator defined on the boundary operator. The first-order HL operator is defined as: \(=_{2}_{2}^{}+_{1}^{} _{1}\). Solving the eigensystem \(^{j}=^{j}^{j}\) produces orthonormal bases \(\{^{0},^{1},^{2},\}\). The HL spectral filter \(h\) with spectrum \(h()\) is defined as: \(h(,)=_{j=0}^{}h(^{j})^{j}()^{j}()\). To approximate \(h()\), we follow  and expand it as a series of Laguerre polynomials \(T_{u}\) with learnable coefficients \(_{u}\):

\[h()=_{u=0}^{U-1}_{u}T_{u}(), \]

where \(T_{u}\) can be computed via the recurrence relation \(T_{u+1}()=()}{u+1}\), with initial states \(T_{0}()=1\) and \(T_{1}()=1-\). More details of these operators can be found in Appendix C.

**Causation Filtering & Surrogate Variable.** Shown in Figure 3(b), the edge signal \(X_{ed}\) is mapped into the latent space by the edge encoder to obtain \(H_{ed}^{M F}\). Next, we acquire a new causal relations representation \(_{ed}^{M F}\) by spectral filtering over \(H_{ed}\): \(_{ed}=h*H_{ed}=_{y=0}^{U-1}_{u}T_{u}()H_{ed}\). We then use a linear transformer to calculate the causal strength \(A_{cau}^{N K_{b}}\) and derive \(_{ir}^{N F}\) using Graph Convolutional Networks (GCN), where nodes' information is filtered by their genuine causation. \(N_{e}\) and \(K_{b}\) are the numbers of causal relations (i.e., edges) and the GCN block's depth. Meanwhile, a position embedding \(P\) is used to generate \(_{ia}^{N F}\) via a linear transform. We then obtain the surrogate \(_{i}=_{ir}+_{ia}\). Ultimately, we concatenate it with \(_{e}\) (obtained in Section 4.1) to form the predictor's input \(\) and obtain the final prediction \(\).

### Optimization

**Environment Codebook.** We train the codebook following . During forward computation, the nearest embedding \(_{e}\) is concatenated with the entity representation \(_{i}\) and fed to the decoder (i.e., predictor). In the backward pass, the gradient \(_{z}\) (see Figure 3(c)) is passed unaltered to the EnvEncoder within the Env Disentangler block. These gradients convey valuable information for adjusting the EnvEncoder's output to minimize the loss. The loss function has two components, namely the prediction loss \(_{pre}\) and the codebook loss \(_{cod}\):

\[_{pre}=-logP(Y|_{e},_{i}),_{cod}=||sg [H_{e}]-e||_{2}^{2}+||H_{e}-sg[e]||_{2}^{2} \]where \(\) is a balancing hyperparameter and \(sg[]\) denotes the _stopgradient operator_, acting in a dual capacity - as an identity operator during forward computation, and has zero partial derivatives during the backward pass. As a result, it prevents its input from being updated. The predictor is optimized solely by \(_{pre}\), whereas the EnvEncoder is optimized by both \(_{pre}\) and the second term of \(_{cod}\). The first term of \(_{cod}\) optimizes the codebook.

**Mutual Information Regularization.** To minimize \((H_{i},H_{e})\), we use a classifier to predict \(z\) in Eq. 3 based on \(_{i}\), denoted as \(\). The objective is to thwart the classifier to discern the true labels, or in other terms, to ensure that the classifier can not determine the true corresponding environment based on the information provided by \(_{i}\). To achieve this, we introduce the MI loss \(_{mi}\) that minimize the cross-entropy between \(z\) and \(\) to encourage \(\) to move away from the true labels \(z\) and towards a uniform distribution:

\[_{mi}=(H_{i},H_{e})=_{k=1}^{K}z^{k} ^{k} \]

where \(z^{k}\) and \(^{k}\) means labels belonging to \(e_{k}\). The overall loss function is obtained by combining these three losses: \(=_{pre}+_{cod}+_{mi}\), where \(\) regulates the trade-off of the MI loss.

## 5 Experiments

**Datasets & Baselines.** We conduct experiments using three real-world datasets (PES08 , AIR-BJ , and AIR-GZ ) from two distinct domains to evaluate our proposed method. PEMS08 contains the traffic flow data in San Bernardino from Jul. to Aug. in 2016, with 170 detectors on 8 roads with a time interval of 5 minutes. AIR-BJ and AIR-GZ contain one-year PM\({}_{2.5}\) readings collected from air quality monitoring stations in Beijing and Guangzhou, respectively. _Our task is to predict the next 24 steps based on the past 24 steps_. For comparison, we select two classical methods (HA  and VAR ) and seven state-of-the-art STGNNs for STG forecasting, including DCRNN , STGCN , ASTGCN, MTGNN, AGCRN, GMSDR  and STGNCDE . More details about the datasets and baselines can be found in Appendix D and E, respectively.

**Implementation Details.** We implement CaST and baselines with PyTorch 1.10.2 on a server with NVIDIA RTX A6000. We use the TCN  as the backbone encoder and a 3-layer MLP as the predictor and the classifier. Our model is trained using Adam optimizer  with a learning rate of 0.001 and a batch size of 64. For the hidden dimension \(F\), we conduct a grid search over {8, 16, 32, 64}. For the number of layers in each convolutional block, we test it from 1 to 3. The codebook size \(K\) is searched over {5, 10, 20}. See the final setting of our model on each dataset in Appendix D.

### Comparison to State-Of-The-Art Methods

We evaluate CaST and baselines in terms of Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE), where lower metrics indicate better performance. Each method is executed five times, and we report the mean and standard deviation of both metrics for each model in Table 1. From this table, we have three key findings: 1) CaST clearly outperforms all competing baselines over the three datasets, whereas the second-best performing model is not consistent across all cases. This reveals that CaST demonstrates a more stable and reliable accuracy across various datasets, highlighting its versatility and adaptability to various domains. 2) STGNN-based models largely surpass conventional methods, i.e., HA and VAR, by virtue of their superior model capacity. 3) While baseline models such as AGCRN and MTGNN can achieve runner-up performance in certain cases, they exhibit a

    &  &  &  \\   & **MAE** & **RMSE** & **MAE** & **RMSE** & **MAE** & **RMSE** \\  HA(2017) & 58.83 & 81.96 & 32.12 & 43.95 & 19.56 & 25.77 \\ VAR(1991) & 37.04 & 53.08 & 29.79 & 42.04 & 14.97 & 20.61 \\ DCRNN(2017) & 22.10 \(\) 0.45 & 33.96 \(\) 0.59 & 23.72 \(\) 0.36 & 35.84 \(\) 0.56 & 12.99 \(\) 0.26 & 18.27 \(\) 0.41 \\ STGCN(2018) & 18.60 \(\) 0.08 & 28.44 \(\) 0.15 & 23.71 \(\) 0.21 & 36.30 \(\) 0larger standard deviation compared to CaST. This demonstrates that CaST not only offers superior predictive accuracy but also showcases robustness and generalization capabilities. The evaluation of our proposed model confirms that incorporating causal tools not only enhances interpretability but also improves predictive accuracy and generalization performance across different scenarios. We also present the assessment of model performance on various future time steps in Appendix F.

### Ablation Study & Interpretation Analysis

**Effects of Core Components.** To examine the effectiveness of each core component in our proposed model, we conducted an ablation study based on the following variants for comparison: a) **w/o Env**, which excludes environment features for prediction. b) **w/o Ent**, which omits entity features for prediction. c) **w/o Edge**, which does not utilize the causal score to guide the spatial message passing. The MAE results for two datasets, AIR-BJ and PEMS08, are displayed in Figure 4(a). We observe that all components contribute to the model's performance. In addition, removing the entity component harms performance more than omitting the environment component, confirming our model's capacity to distinguish between the two. Moreover, removing the edge filtering affects PEMS08 more than AIR-BJ. This can be attributed to the unique properties of the down HL operator and the datasets, aligning better with the incompressible traffic flow of PEMS08 for causal capture, as opposed to AIR-BJ's non-divergence-free flow . A more detailed discussion can be found in Appendix G.

**Effects of Edge Convolution.** Our model utilizes a spectral filter on edges in the spatial de-confounding block, which enables it to capture the ripple effects of dynamic causal relationships. To validate the superiority of our edge convolution module over existing spatial learning methods, we conduct an ablation study on the AIR-BJ dataset by comparing the performance of CaST against two variants: **CaST-ADP** which replaces the edge convolution with a self-adaptive adjacency matrix , and **CaST-GAT** which employs the graph attention mechanism to obtain the causal score . The results presented in Table 2 indicate that our edge convolution can adeptly discern the causal strengths between nodes, thereby resulting in enhanced performance.

**Visualization of Dynamic Spatial Causation.** To show the power of edge convolutions, we depict the trend of learned causal relations among four air quality stations in Beijing. Note that we do not incorporate any external features (e.g., weather and wind) as input. Considering the fact that the dispersion of air quality is strongly associated with wind direction, we select a one-week period in Nov. 2019, during which a _south wind_ occurred on 11th Nov. The varying causal relations among the selected stations and their geolocations are displayed in Figures 4(b) and 4(c). From them, three key observations can be obtained. **Obs1**: The causal relationships \(S_{4} S_{14}\) and \(S_{4} S_{1}\) exhibit similar patterns when there is a south wind, as \(S_{1}\) and \(S_{14}\) are both located south of \(S_{4}\), which is reasonable to suggest that the causal relationship would be stronger in this case. **Obs2**: \(S_{4} S_{5}\) shows an opposite changing direction compared to the former two relationships, as south winds carry PM\({}_{2.5}\) southward, resulting in the causal strength from \(S_{4}\) to eastern stations like \(S_{5}\). **Obs3**: \(S_{4} S_{1}\) and \(S_{1} S_{4}\) display distinct patterns of change. While the causal strength from \(S_{4}\) to \(S_{1}\) increases with the occurrence of a south wind, that from \(S_{1}\) to \(S_{4}\) shows no significant change. The reason is that a south wind carries PM\({}_{2.5}\) towards the south, increasing its concentration in \(S_{1}\) and strengthening the causal relationship from \(S_{4}\) to \(S_{1}\). However, the wind's effect is weaker in the opposite direction, leading to no significant changes in the causal strength from \(S_{1}\) to \(S_{4}\). These findings underscore the capability of CaST to effectively capture the dynamic causal relationships among nodes.

 
**Variant** & **Overall** & **1-8s** & **9-16s** & **17-24s** \\  CaST-ADP & 24.28 & 16.42 & 26.06 & 30.36 \\ CaST-GAT & 23.77 & 14.76 & 25.75 & 30.80 \\ CaST & **22.90** & **13.79** & **24.86** & **30.05** \\  

Table 2: Variant results on MAE over AIR-BJ. s: steps.

Figure 5: (a) Effects of each core component on MAE. w/o: without. (b) Visualization of the dynamic causal relationships between nodes on AIR-BJ. (c) Distribution of air quality stations.

**Analysis on Environmental Codebook.** We employ the environmental codebook as a latent representation to address the temporal OoD issue. In Figure 6(a), we visualize the environmental codebook vectors using PCA dimensional reduction with \(K=10\) on AIR-BJ at different training epochs. Each color corresponds to a particular environment's embedding. Remarkably, starting from the same initialized positions, the environment embeddings gradually diverge and move in distinct directions. Next, we assess the influence of several critical hyperparameters in CaST, including the hidden dimension \(F\) and the environment codebook size \(K\). We examine how CaST performs on AIR-BJ while varying these hyperparameters and display the results in Figure 6. The empirical results indicate that the model's accuracy exhibits a lower sensitivity to the hidden size \(F\). When the hidden size is small (i.e., \(F<32\)), the performance is relatively unaffected by the choice of \(K\). Consequently, the model is less reliant on the granularity of the environment representation, as provided by \(K\), when the hidden size is small.

**Interpretation of Temporal Environments.** To further investigate what the codebook has learned, we visualize the environment types distribution of station \(S_{31}\) in the training set (Jan. - Aug. 2019) and testing set (Nov. - Dec. 2019) in Figure 6(b). Note that the number of environments in the test set is computed as a sum of probabilities since we use the likelihood for each environment to generate unseen environments during the test phase. A substantial discrepancy in environmental distribution between the two sets is observed, with a significant increase in the occurrence frequency of \(e_{7}\) in the testing set compared to the training set, indicating the existence of a temporal distribution shift. As previously discussed, temporal environments are associated with related external factors that change over time. We then collect meteorological features in Beijing for 2019 and calculate the mean of two related variables, i.e., temperature and pressure, for each environment (see Figure 6(c)). We find that \(e_{7}\) is characterized by low temperature and high pressure, which is consistent with the fact that Nov. - Dec. typically exhibit lower temperatures and higher pressures compared to Jan. - Oct. The distribution of these variables in accordingly periods is presented in Appendix F. This highlights our model's capability to learn informative environment representations.

## 6 Conclusion

In this paper, we present the first attempt to jointly address the challenges of tackling the temporal OoD issue and modeling dynamic spatial causation in the STG forecasting task from a causal perspective. Building upon a structural causal model, we present a causal spatio-temproal neural network termed CaST that performs the back-door adjustment and front-door adjustment to resolve the two challenges, respectively. Extensive experiments over three datasets can verify the effectiveness, generalizability, and interpretability of our model. We provide more discussions in Appendix G, including the limitations of our model, potential future directions, and its social impacts.

Figure 6: Effects of \(K\) and \(F\) on AIR-BJ.

Figure 7: Case study on AIR-BJ (better viewed in color). (a) Environment codebook at different training epochs by PCA. (b) Distribution of environments for the station \(S_{31}\) in the train and test sets. Env: Environment. (c) The mean of temperature (Temp) and pressure (Press) in each environment.