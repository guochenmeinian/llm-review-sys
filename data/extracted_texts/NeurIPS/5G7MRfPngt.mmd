# VLM Agents Generate Their Own Memories:

Distilling Experience into Embodied Programs of Thought

 Gabriel Sarch\({}^{1}\)  Lawrence Jang\({}^{1}\)  Michael J. Tarr\({}^{1}\)

William W. Cohen\({}^{1,2}\)  Kenneth Marino\({}^{2}\)  Katerina Fragkiadaki\({}^{1}\)

\({}^{1}\)Carnegie Mellon University \({}^{2}\)Google DeepMind

https://ical-learning.github.io

###### Abstract

Large-scale generative language and vision-language models (LLMs and VLMs) excel in few-shot in-context learning for decision making and instruction following. However, they require high-quality exemplar demonstrations to be included in their context window. In this work, we ask: Can LLMs and VLMs generate their own examples from generic, sub-optimal demonstrations? We propose In-Context Abstraction Learning (ICAL), a method that builds a memory of multimodal experience from sub-optimal demonstrations and human feedback. Given a task demonstration that may contain inefficiencies or mistakes, a VLM abstracts the trajectory into a generalized program by correcting inefficient actions and annotating cognitive abstractions: causal relationships, object state changes, temporal subgoals, and task-relevant visual elements. These abstractions are iteratively improved and adapted through human feedback while the agent attempts to execute the trajectory in a similar environment. The resulting examples, when used as exemplars in the prompt, significantly improve decision-making in retrieval-augmented LLM and VLM agents. Moreover, as the agent's library of examples grows, it becomes more efficient, relying less on human feedback and requiring fewer environment interactions per demonstration. Our ICAL agent surpasses the state-of-the-art in dialogue-based instruction following in TEACh, multimodal web agents in VisualWeb Arena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6% improvement in goal-condition success. In VisualWeb Arena, our task success rate improves over the SOTA from 14.3% to 22.7% using GPT4V. In Ego4D action forecasting, we improve over few-shot GPT-4V and remain competitive with supervised models. We show finetuning our retrieval-augmented in-context agent yields additional improvements. Our approach significantly reduces reliance on manual prompt engineering and consistently outperforms in-context learning from action plans that lack such abstractions.

## 1 Introduction

Humans exhibit remarkable few-shot learning capabilities, rapidly generalizing from a single task demonstration to related conditions by integrating the observed behavior with their internal world model. They discern what is relevant and irrelevant for success and anticipate potential failures. Through repeated practice and feedback, they quickly find the right abstraction that helps to imitate and adapt the task to various situations. This process facilitates continuous refinement and transfer of knowledge across a diverse range of tasks and contexts.

Recent research has explored the use of large language models (LLMs) and visual-language models (VLMs) 1 to extract high-level insights from trajectories and experiences. These insights are generated through the model's introspection and are used to enhance performance by appending them to prompts, leveraging their strong in-context learning abilities [39; 70; 56; 60]. Existing methods often linguistically focus on task reward signals [70; 56; 76; 79], store human corrections following failures [88; 15; 68], use domain experts to hand-write or hand-pick examples without introspection [68; 73], or utilize language to shape policies [30; 74] and rewards [61; 3; 27; 21; 26; 59; 74; 35; 54]. Critically, these methods typically are text-based and do not incorporate any visual cues or demonstrations, or use introspection only in case of failures, which is only one of several ways that humans and machines can consolidate experiences and extract insights.

**In this work, we teach VLMs novel tasks by learning in-context experience abstractions given sub-optimal demonstrations and human natural language feedback**. We present In-Context Abstraction Learning (ICAL), a method that prompts VLMs to create multimodal abstractions for unfamiliar domains. Unlike previous works that only store and retrieve successful action plans or trajectories [68; 76; 44], our approach emphasizes learning abstractions that encapsulate the dynamics and critical knowledge of tasks, as illustrated in Figure 1. Specifically, ICAL tackles four types of cognitive abstractions: **task and causal relationships**, which identify the fundamental principles or actions needed to achieve a goal and how elements are interconnected through cause and effect ; **changes in object states**, which describe the various forms or conditions an object will take ; **temporal abstractions**, which break down tasks into subgoals ; and **task constraints**, which highlight critical visual details within a task . When provided with optimal or suboptimal demonstrations, ICAL prompts a VLM to transform these demonstrations into optimized trajectories

Figure 1: ICAL (In-Context Abstraction Learning) is a method for efficient agent learning from both noisy visual demonstrations and human feedback using large language / vision models. _Left:_ The agent can take in a video demonstration, and generate a refined example with language annotations to be used later by the VLM via in-context learning. _Right:_ Humans provide feedback, correct errors and supply additional knowledge.

while also creating pertinent language and visual abstractions. These abstractions are then refined through executing the trajectory in the environment, guided by natural language feedback from humans. Each step of abstraction generation leverages previously derived abstractions, enabling the model to improve not only its execution but its abstraction capabilities as well. Collectively, the learned abstractions summarize crucial information about action sequences, state transitions, rules, and focus areas, articulated through free-form natural language and visual representations.

We present a comprehensive evaluation of our agent, equipped with the learned example abstractions, across three benchmarks: TEACh  for dialogue-based instruction in household settings, VisualWebArena  for multimodal autonomous web tasks, and Ego4D for video action anticipation . In TEACh, our agent sets a new state-of-the-art, outperforming VLM agents reliant on raw demonstrations or extensive domain-expert hand-written examples, demonstrating the effectiveness of ICAL learned abstractions for in-context learning. Specifically, our approach achieves a 12.6% improvement in goal condition success compared to the previous SOTA, HELPER . We show that this approach leads to increasing performance gains on unseen tasks as the external memory grows, and achieves a 14.7% performance increase after only ten examples. Moreover, our agent becomes increasingly efficient over time by leveraging stored abstractions, requiring 38.8% fewer environment steps and 71.6% less human feedback per example in the latter half of demonstrations processed. Integrating our learned examples with LoRA-based fine-tuning of an LLM  further improves goal-condition performance by 4.9%. In the VisualWebArena, our agent surpasses the state-of-the-art, GPT4 + Set of Marks , improving from 14.3% to 22.7% using GPT4V and from 18.9% to 23.4% using GPT4o. In the Ego4D setting, ICAL outperforms few-shot GPT4V using chain of thought, reducing the noun and action edit distance by 6.4 and 1.7, respectively, and competes closely with fully supervised methods, despite using 639x less in-domain training data. Our approach significantly reduces reliance on expertly-crafted examples and consistently outperforms in-context learning from action plans or trajectories that lack such abstractions [68; 76; 44].

## 2 Related Work

Vlm AgentsLLMs and VLMs trained from large scale vision-language data have been adapted for task planning and decision making tasks through in-context prompt optimization or finetuning. VLMs have been used to plan over high-level actions or code [80; 76; 68; 44; 72], incorporate error feedback [52; 45; 88], and understanding game instruction manuals . Some studies use VLMs for learning from human feedback through retrievable knowledge , question asking [66; 15], or converting language to actions or rewards [49; 50; 36; 11; 14]. Our work utilizes noisy visual demonstrations, and integrates multiple types of multi-modal abstractions during the learning process.

Instructable Interactive AgentsBenchmarks for embodied instruction following include question answering [25; 16; 93; 18; 17; 23], navigation [42; 41; 10], interactive dialogue, and instruction following [86; 71; 63; 22]. Virtual agent benchmarks focus on web tasks where agents navigate static [53; 19] and dynamic web environments [92; 37; 85; 38], covering personal shopping, travel assistance, software engineering, and operating system tasks [51; 34; 69; 47]. This includes visual grounding and multi-turn planning, with prior studies using finetuning or few-shot prompts. In agent-based domains, retrieval-augmented prompting and prompt optimization have improved task planning in instructional contexts  and open-world gaming [79; 76; 56; 62]. Unlike studies that rely solely on static external memory or text-based prompting, our research demonstrates that multi-modal, generalizable abstractions learned from a few noisy trajectories and human feedback via in-context learning or finetuning can significantly enhance instruction-following performance.

## 3 In-Context Abstraction Learning (ICAL)

In-Context Abstraction Learning (ICAL) aims at automating the acquisition of generalizable examples and knowledge for in-context agents. ICAL operates by receiving a language instruction \(I\) with a noisy trajectory of observations and actions, denoted \(_{noisy}=\{o_{0},a_{0},,o_{T},a_{T}\}\) in a new task domain \(D\). A new domain \(D\) represents changes in task variables not captured in VLM pretraining, such as a different environment (e.g., kitchen #1 vs. kitchen #2), task (e.g., "add the cheapest red bike to my wish list"), or user preference (e.g., "I prefer the red cup for coffee"). The core aim of ICAL is to abstract each noisy trajectory into a single example \(e\), which then forms part of a memory set M. Each example \(e M\) represents an optimized trajectory \(_{optimized}\) with generalizable language abstractions \(L\). The objective is to ensure that M collectively encapsulates examples that, when used in a VLMs context window, increase the likelihood of successful task execution in the new domain, while also containing knowledge that is transferable across similar tasks and contexts. This can be encapsulated as:

\[_{M}[R|M,I,o_{t},D],\] (1)

where \(R\) is the return or the cumulative reward acquired by performing actions based on the instruction \(I\), observation \(o_{t}\), and in-context example memory set M. Rather than using reinforcement learning to optimize prompt examples through trial and error--which would lead to a challenging search problem that myopically focuses on improving rewards for the current scene--we leverage VLMs' knowledge for abstraction, which we elicit through prompting.

Figure 2: ICAL transforms raw experience into useful abstractions for in-context learning. _Top:_ Given a noisy trajectory, It prompts a VLM to optimize actions and add language annotations. The optimized trajectory is executed, incorporating human feedback on failures. Successful examples are stored for future VLM in-context action generation. _Bottom:_ An example of the raw, noisy trajectory (left), and the final abstracted example after ICAL (right).

### Overview

Figure 2 shows an overview of ICAL. Each iteration starts with a noisy trajectory. ICAL abstracts it in two phases: (1) abstraction phase (\(F_{abstract}\)), where a VLM corrects errors and enriches the sequence with language comments (Section 3.2). During this phase, a VLM identifies and corrects errors within the sequence, as well as enriches it with natural language comments. (2) The human-in-the-loop phase, denoted \(F_{hilt}\), during which the sequence is executed within the environment and its abstraction is guided by human feedback conveyed in natural language (Section 3.3). Upon the successful execution of the trajectory, it is archived within a continually growing repository of examples. These examples serve as contextual references for the agent both during its learning phase and during inference for unseen instructions and environments.

### VLM-driven Abstraction Generation

We address the challenge of learning from a diverse set of noisy trajectories \(_{noisy}=\{o_{0},a_{0},,o_{T},a_{T}\}\), which may be sub-optimal due to several factors: demonstrations by human non-experts, errors in inferring actions from visual passive demonstrations, and generated paths that include exploration or failures. Please see Section 4.1 for details on noisy trajectory collection.

Abstracting a noisy trajectory, \(_{noisy}\), involves transforming it into a more optimized sequence, \(_{optimized}\), and formulating relevant language abstractions, \(L\), as shown in Figure 2. The abstraction function, \(F_{abstract}\), modifies \(_{noisy}\) by correcting actions and generating language abstractions that encapsulate general knowledge and task-specific insights. It is defined as:

\[F_{abstract}:(_{noisy},I,\{e^{1},,e^{k}\})(_{optimized},L)\] (2)

where \(_{noisy}\) is the initial noisy trajectory, \(I\) is the task instruction, and \(\{e^{1},,e^{k}\}\) are the top-k previous successful in-context examples. The output consists of the optimized trajectory \(_{optimized}\) and language abstractions \(L\).

Corrections during abstraction include action adjustments and generating annotations (\(L\)) for abstracting subgoals, causal relationships, state changes, and reasoning steps. These annotations are produced by prompting the VLM to output a specified type of abstraction. We prompt the VLM abstraction function, \(F_{abstract}\) (GPT4V in this work), to produce the abstractions detailed below. For the complete prompts, please refer to the Appendix.

**1. Task and Causal Abstractions:** Task and causal abstractions pinpoint the essential principles or actions required to achieve a goal and explain how elements are interconnected through cause and effect. Task and causal abstractions have been shown to be helpful in improving LLM generalization , and play a strong role in human communication and learning [75; 24]. We prompt the VLM to add annotations of task and causal abstractions in the form of natural language comments. For example, it might add a note explaining unnecessary actions, such as "Since the box is already open, there is no need to close it after placing the watches inside, ensuring the task is completed efficiently."

**2. State Changes:** Understanding how one's actions will affect the form and conditions of elements in a scene is crucial for decision-making . The VLM is prompted to identify and predict state changes that occur during the demonstration. For instance, an annotation might note the bowl becoming clean, clearly indicating an expected state transition.

**3. Task Decomposition and Subgoals:** Breaking down a complex task into intermediate steps and subgoals is crucial for managing extended and variable sequences of lower-level actions. These temporal abstractions are important for human reasoning  and have been shown to improve LLM outputs . We prompt the VLM to add 1) a step-by-step plan detailing the demonstration, and 2) a natural language summary of the actions.

**4. State Abstraction:** Useful representations do not simply mirror every aspect of the world; instead, they selectively capture a manageable subset of details relevant to a specific purpose . We focus on identifying and including only those state variables that are relevant to the task at hand. This is achieved by (1) selecting parts of the state that were directly interacted with by the agent during the demonstration, and (2) prompting the VLM to suggest additional state variables not explicitly included in the demonstrations but potentially relevant to understanding the task.

### Abstraction Verification with a Human-in-the-loop

In this phase, ICAL verifies the generated abstractions with a human-in-the-loop. This involves executing the optimized trajectory, \(_{optimized}\) within the actual task environment, under the watchful guidance of a live human observer. The procedure is:

**1. Execution of optimized trajectory:** The agent attempts to perform the task by following the optimized sequence of actions \(_{optimized}\) from the abstraction phase.

**2. Monitoring and Intervention:** As the agent executes \(_{optimized}\), a human observer monitors the process. If an action \(a_{t}\) fails, denoted by \(F(a_{t})=1\), the observer intervenes by providing natural language feedback \(H(a_{t},o_{t})\). This feedback is context-specific, addressing the observed failure directly (e.g., explaining that the Toaster is currently full and can only toast one slice of bread). We provide additional details on the human-in-the-loop in the Appendix Section S5.1.3.

**3. Feedback Integration and Trajectory Revision:** Upon receiving feedback \(H(a_{t},o_{t})\), the VLM is provided with this input alongside the current state of \(_{optimized}\) and any existing language annotations \(L\). The VLM is prompted to revise \(_{optimized}\) to address the failure, to update existing annotations \(L\) based on the feedback, and to add new annotations that capture insights from the feedback.

This process can be represented by an update function:

\[_{update}(_{optimized},H(a_{t},o_{t}),L,I,\{e^{1},...,e^{k}\}) ^{}_{optimized},L^{}\] (3)

where \(_{update}\) denotes the update function that takes the current trajectory \(_{optimized}\), human feedback \(H(a_{t},o_{t})\), and current annotations \(L\), and outputs the revised trajectory \(^{}_{optimized}\) and updated annotations \(L^{}\). For the complete prompts, please refer to the Appendix.

**4. Environment Reset and Retrial:** Following a failure and subsequent feedback, the environment is reset to a suitable state for retrying the task. The agent then attempts the task again, utilizing the newly revised trajectory \(^{}_{optimized}\).

**5. Success Criteria and Feedback Limit:** This interactive phase continues until the human observer deems the task execution successful, or until a predefined maximum number of feedback iterations, \(N_{feedback}\), has been reached.

**6. Saving example:** If successful, we store the revised trajectory \(_{optimized}\) and language annotations \(L\) to the memory set M. If unsuccessful after \(N_{feedback}\) iterations, we do not store the example and move to the next demonstration. We experiment with relabeling partially successful demonstrations in Section S4.5 of the appendix.

### Retrieval Augmented Generation at Deployment

Given the learned example set M and a new instruction \(I\), we prompt the VLM to carry out the instruction by producing action sequences \(\{a_{0},...,a_{T}\} A\) from an action API that describes the skills set \(A\) (e.g., go_to(X), pickup(X)), by retrieving the top \(K\) examples from M to include in the prompt based on their textual and visual similarity with the current scene. The aggregated similarity score \(s\) for each example \(e\) reads:

\[s=_{I} s^{I}+_{} s^{} +_{} s^{},\] (4)

where \(s^{I}\), \(s^{}\), and \(s^{}\) are the similarity scores for the input text instruction, textual state, and visual state, respectively, computed via cosine similarity using embeddings from OpenAI's text-embedding-ada-002 model and CLIP ViT-B/32 model. The coefficients \(_{I}\), \(_{}\), and \(_{}\) are weighting hyperparameters chosen in each domain by a held out validation set.

The VLM prompt contains the new instruction \(I\), the current webpage image for web agents or 12 video frames for ego4D annotated with set-of-marks , a textual state description \(x_{t}\) describing

Figure 3: After the ICAL examples have been learned, ICAL is deployed for new tasks and environments using retrieval-augmented generation.

the objects and their attributes for embodied agents and HTML elements for web agents, the action API \(A\), and the retrieved set of in-context examples \(e^{1},...,e^{k} M\). An illustration of this process is shown in Figure 3. The deployment prompt is provided in the Appendix.

**Implementation details** We use GPT-4-1106-preview for text generation, unless otherwise stated, and text-embedding-ada-002  for text embeddings. We use gpt-4-1106-vision-preview for the text and image generation model. We use \(k=5\) for example retrieval. We use a temperature of 0 for TEACh and Ego4D, and 0.2 for VisualWebArena.

## 4 Experiments

We test ICAL for task planning in TEACh  and VisualWebArena  and for action forecasting in Ego4D  benchmarks.

### Environments

**TEACh ** The TEACh dataset comprises over 3,000 dialogue-based instructions for household tasks in AI2-THOR . We use the Trajectory from Dialogue (TfD) tasks where agents convert dialogue instructions into action sequences, such as Make Coffee. It includes training and validation splits (seen and unseen), the latter featuring new environments and instructions. Agents receive egocentric image inputs \(o_{t}\) and perform actions like pickup(X) and turn_left(). Task success is contingent on fulfilling all instruction conditions. Utilizing HELPER's  perception, navigation, and manipulation modules, the system relies on RGB images, depth maps, object masks, and egomotion for 3D mapping and object recognition. We remove domain-specific checks from HELPER's modules to allow ICAL to learn them independently. **Noisy Trajectories.** We use 250 noisy trajectories from TEACh, omitting action labels but retaining language instructions and corresponding RGB videos. To label actions from RGB video, we trained an inverse dynamics model using a transformer encoder-decoder based on the DETR architecture  from a seperate 300 TEACh episodes. Model predictions and human errors, like unnecessary movements, cause action noise in these demonstrations. 122 examples were successfully abstracted by ICAL.

**VisualWebArena ** VisualWebArena consists of 910 episodes across various web tasks (Classifields, Shopping, Reddit) requiring visual comprehension and reasoning. Instructions may include text and reference images, like adding an item seen in an image to a wish list. Agents operate with instructions \(I\), current webpage images, and an API for actions like click(X), executing tasks to fulfill instruction conditions. **Noisy Trajectories.** From VisualWebArena, 30 human demonstrations and 62 model trajectories from few-shot GPT4V were abstracted using ICAL. The process led to an example set of 92 for evaluation.

**Ego4D ** This task involves anticipating actions from Ego4D RGB egocentric videos in daily scenarios. Models select from 115 verbs and 478 nouns for predicting actions. We evaluate using 200 unseen videos from ego4D validation, applying edit distance as a performance metric. Input to models includes sequences of video frames annotated with set-of-marks  tracking  and label masks. The supervised baseline  (243 video hrs of Ego4D V2) uses a SlowFast backbone with a Transformer aggregator. **Noisy Trajectories.** Due to the passive nature of this task, ICAL proceeds without human-in-the-loop verification during ICAL (only Section 3.2, VLM-driven Abstraction Generation). ICAL successfully abstracted 92/100 demonstrations taken from the Ego4D validation set (8 failed due to GPT filters) for evaluation.

### ICAL beats written & unchanged demonstrations in household instruction following

Table 1 presents our findings on the TEACh unseen validation set, assessing performance on new instructions, houses, and objects. ICAL and baselines use HELPER's navigation and manipulation modules . We compare with these baselines: _1. Hand-written examples_ from HELPER, the SOTA on the TEACh benchmark, with 19 expert-written examples for retrieval-augmented prompting. _2. Zero-shot chain of thought_, prompting the LLM to output step-by-step. _3. Raw Visual Demos_, retrieving unchanged demonstrations labeled with the inverse dynamics model. _4. Raw Kinesthetic Demos_, retrieving unchanged demonstrations with true actions. Our metrics are: _1. Task success rate (SR)_, the % of tasks completed successfully. _2. Goal condition success rate (GC)_, the % partial fulfillment rate across sessions.

As shown in Figure 4, ICAL revises noisy trajectories, enabling more successful tasks completed on training tasks than mimicking raw trajectories, with increases of 42 and 86 successful tasks for kinesthetic and visual demonstrations, respectively. This shows how ICAL not only adds useful abstractions but also corrects errors in the passive video demos, improving success in the original demo environment. Please see the Appendix Section S4.3 for additional analysis.

As shown in Table 1, on unseen tasks, ICAL outperforms unprocessed demonstrations as in-context examples, achieving a 17.9% absolute improvement in SR over raw demos with predicted actions and 8.6% over those annotated with true actions. This underscores the effectiveness of our abstractions in improving the quality of examples for improved context learning, unlike previous works that primarily save and retrieve successful action plans or trajectories without abstractions .

Additionally, we surpass the handwritten examples of the previous SOTA HELPER  by 12.6% in GC and 0.6% in SR, and by 2.2% (relative 26.5%) using estimated perception, demonstrating our method's efficacy with less expert intervention, leveraging only visual demos and non-expert feedback. Unlike HELPER, which requires domain experts to write 48-107 lines of text for each example, ICAL does not rely on such extensive input from experts. Instead, it allows non-experts to provide up to five natural language feedback corrections to the agent, significantly reducing the required effort and expertise per example.

  &  & GC \\   \\ HELPER hand-written  & 34.5 & 36.7 \\ Zero-Shot CoT  & 11.8 & 24.6 \\ Raw Visual Demos & 17.2 & 26.6 \\ Raw Kinesthetic Demos & 26.5 & 29.5 \\ ICAL (ours) & **35.1** & **49.3** \\ w/o abstraction phase & 29.4 & 44.9 \\ w/o human-in-the-loop & 29.9 & 41.0 \\ w/ retrieval re-ranking & **35.3** & **51.7** \\ w/ GPT4 & 41.7 & 63.6 \\ finetuned & 23.2 & 40.3 \\ finetuned + retrieval & **35.8** & **54.2** \\   \\ HELPER hand-written  & 8.3 & 14.1 \\ ICAL (ours) & **10.5** & **15.4** \\  

Table 1: **Evaluation on TEACH unseen validation set. All evaluations are done using GPT3.5-turbo-1106 unless otherwise noted. Visual Demos = demonstrations labeled with inverse dynamics model. Kinesthetic Demos = demos labeled with GT actions. GC = goal-condition success**

Figure 4: **ICAL enables greater success on training tasks. Tasks successfully completed by ICAL over number of interactions when using the ICAL method with kinesthetic or visual demonstrations, and when replaying the kinesthetic or visual demonstrations directly.**

  &  & Unseen & Average \\  GPT4o+SoM  & – & – & 18.9 \\ ICAL (ours) & 32.3 & 22.3 & **23.4** \\  GPT4V+SoM  & 16.3 & 14.1 & 14.3 \\ ICAL (ours) & **38.8** & **20.9** & **22.7** \\ _Ablations_ & **35.1** & **29.9** & **12.7** \\ _GPT4V+SoM  & 11.5 & 12.9 & 12.7 \\ ICAL (ours) & 28.0 & 21.6 & 22.2 \\ w/o image & 28.0 & 17.3 & 19.0 \\ w/ full text trajectory & 57.7 & 21.6 & 25.5 \\  

Table 2: **Results in VisualWebArena.** ICAL outperforms the prior best, GPT4o/V + Set of Marks. Ablation studies were conducted with GPT4V on a subset of 257 episodes.

  &  & Unseen & Average \\  GPT4o+SoM  & – & – & 18.9 \\ ICAL (ours) & 32.3 & 22.3 & **23.4** \\  GPT4V+SoM  & 16.3 & 14.1 & 14.3 \\ ICAL (ours) & **38.8** & **20.9** & **22.7** \\ _Ablations_ & **37.5** & **12.9** & 12.7 \\ ICAL (ours) & 28.0 & 21.6 & 22.2 \\ w/o image & 28.0 & 17.3 & 19.0 \\ w/ full text trajectory & 57.7 & 21.6 & 25.5 \\  

Table 3: **Evaluation on the Ego4D unseen validation subset. ICAL outperforms few-shot GPT4V and matches supervised baselines using 639x less in-domain data.**

### ICAL obtains state-of-the-art performance on visual web tasks

We evaluate our agent with learned ICAL examples on the VisualWebArena evaluation set. We partition this into episodes'seen' by our model during learning, and those 'unseen' during learning.

Table 2 presents the results on VisualWebArena. Our model, ICAL, outperforms the previous state-of-the-art , which uses GPT4V with few-shot, hand-designed examples and set-of-marks image prompting . ICAL achieves an absolute 8.4% (relative 58.7%) improvement in average success rate over GPT4V and shows a 23.8% relative improvement in average success rate over GPT4o.

### ICAL outperforms few-shot VLMs on egocentric video action forecasting

We test ICAL on video action forecasting without using human-in-the-loop abstraction verification due to the passive nature of the task. As shown in Table 3, ICAL demonstrates superior few-shot performance on Ego4D action anticipation compared to hand-written few-shot GPT4V that uses chain of thought , improving by 6.4 noun and 1.7 action edit distance. ICAL also remains competitive with the fully supervised baseline  in noun and action prediction despite using 639x less in-domain training data. We find GPT4V video processing to have the least improvements for verb action prediction, possibly due to its limited video understanding capabilities.

### ICAL shows continual improvement with more examples

ICAL shows continual improvements in TEACh validation unseen success rate with more examples learned, as shown in Figure 5. This is in contrast to the unchanged visual demos used for seeding ICAL learning, which show only marginal improvements. Importantly, throughout learning, ICAL does not need to worry about forgetting previously learned knowledge since the agent is expanding a memory of examples and testing with a frozen VLM via in-context learning. Also noteworthy, our method benefits from even a small amount of examples learned, with an improvement of an absolute 14.7% success rate over zero-shot chain-of-thought  prompting and 6.8% over the unchanged demonstrations (with 10x less data) with just 10 abstracted demonstrations, showing the efficiency of our method.

### Example retrieval improves learning efficiency

Efficient learning systems benefit greatly from leveraging past knowledge, allowing them to reduce the need for human intervention and environment interactions as they continue to process new data. Our agent becomes increasingly efficient over time, requiring less human feedback and fewer environment interactions as it processes more examples. By retrieving past successful abstractions during the VLM-abstraction making and human-in-the-loop phases, it uses previously stored knowledge to help abstract new examples. As shown in Figure 6, for the second half of examples processed, the model requires significantly fewer environment steps (436&88 vs. 267&43, p=0.0143) and human feedbacks (0.74&0.17 vs. 0.21&0.08, p=0.0089) per example. This demonstrates that retrieving abstracted examples during abstraction learning reduces both human effort and environment interaction over time. Consequently, using previously stored ICAL examples not only improves test performance but also accelerates learning for future examples.

Figure 5: **TEACh validation unseen success rate** for ICAL with increasing number of exemplars. ICAL continually learns without forgetting, significantly outperforming the unchanged visual demos used to seed ICAL learning. \(\) denotes task success, while **x** denotes goal-condition success.

### Fine-tuning helps

We finetune the GPT3.5-turbo-1106 model on the learned ICAL examples in TEACH using LoRA  in the AzureAI interface (see the Appendix Section S5.4 for details). The training data include the 122 successfully abstracted examples by ICAL, which we randomly split into 99 training samples and 23 validation samples. This leads to an improvement of 11.4% task success and 15.7% goal-condition success for the GPT3.5 model. Combining the finetuned model with retrieval-augmented generation using the ICAL examples led to an additional improvement of 0.7% task success and 4.9% goal-condition success over using retrieval-augmented generation without finetuning: our top-performing agent. This demonstrates that consolidating the ICAL learned abstractions with weight fine-tuning helps performance.

### Ablations show each component of ICAL is important

We ablate the components of ICAL in TEACH in Table 1. We conclude:

1. The abstraction phase significantly helps for refining the trajectories and adding generalizable knowledge. We observe a decrease in 5.7% success rate and 4.4% in goal condition success rate when removing the abstraction phase.

2. The human-in-the-loop phase is important for fixing errors and incorporating feedback from the user. We observe a decrease in 5.2% success rate and 8.3% in goal condition success rate when removing the human-in-the-loop phase.

3. Our examples demonstrate scalability with larger LLMs. GPT-4 showed a 6.6% absolute increase in task success and a 14.3% absolute rise in goal condition success compared to GPT-3.5.

4. ICAL can be combined with advanced prompting and sampling methods. We test this using re-ranking , where the model generates three diverse outputs from different retrieved examples (e.g., top 1-5, 6-10,...), self-evaluates, and selects the highest scoring output. Improvements are modest but notable: 0.2% in task success and 2.5% in goal condition success.

## 5 Conclusion

We presented ICAL, a method that improves in-context learning by learning to abstract noisy demonstrations into actionable insightful plans, that when used as in-context examples improve performance of VLM agents over in-context learning from raw examples. ICAL proposes abstracting in-context examples as a general form of quick learning from a handful of demonstrations and human-feedback. It also reduces the need for expert examples, and enables more efficient learning. Tested in TEACH, VisualWebArena, and Ego4D, ICAL achieves state-of-the-art performance, demonstrating adaptability to new tasks and environments. There are several limitations and future research directions for ICAL. While ICAL can handle noisy demos, ICAL may not be able to handle extremely misleading demonstrations or feedback, and relies on a fixed action API which may restrict adaptability in dynamic environments. Additionally, GPT4V's visual grounding deficiencies [90; 82; 55; 9] cannot always be overcome by in-context learning, and more research is needed to address this.

Figure 6: **ICAL improves learning efficiency as more examples are added to memory.** First half (blue) versus second half (orange) of ICAL learning across tasks **(left)** and for each task type separately **(right)** in TEACH. The second half of ICAL learning requires significantly fewer environment steps (436\(\)88 vs. 267\(\)43, p=0.0143) and human feedbacks per episode (0.74\(\)0.17 vs. 0.21\(\)0.08, p=0.0089). This indicates that retrieving ICAL examples during learning is beneficial, reducing both human effort and environment interaction over time.

AcknowledgementsThis material is based upon work supported by National Science Foundation grants GRF DGE1745016 & DGE2140739 (GS), ONR award N00014-23-1-2415, AFOSR Grant FA9550-23-1-0257, and DARPA No. HR00112490375 from the U.S. DARPA Friction for Accountability in Conversational Transactions (FACT) program. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Army, the National Science Foundation, or the United States Air Force.

This research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program through which leading foundation models hosted by Microsoft Azure along with access to Azure credits were provided to conduct the research.