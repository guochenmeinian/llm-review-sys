# Diffusing Differentiable Representations

Yash Savani

Carnegie Mellon University

ysavani@andrew.cmu.edu

&Marc Finzi

Carnegie Mellon University

mfinzi@andrew.cmu.edu

&J. Zico Kolter

Carnegie Mellon University

zkolter@andrew.cmu.edu

###### Abstract

We introduce a novel, training-free method for sampling _differentiable representations_ (diffreps) using pretrained diffusion models. Rather than merely mode-seeking, our method achieves sampling by "pulling back" the dynamics of the reverse-time process--from the image space to the diffrep parameter space--and updating the parameters according to this pulled-back process. We identify an implicit constraint on the samples induced by the diffrep and demonstrate that addressing this constraint significantly improves the consistency and detail of the generated objects. Our method yields diffreps with substantially **improved quality and diversity** for images, panoramas, and 3D NeRFs compared to existing techniques. Our approach is a general-purpose method for sampling diffreps, expanding the scope of problems that diffusion models can tackle.

## 1 Introduction

Diffusion models have emerged as a powerful tool for generative modeling (Ho et al., 2020; Song et al., 2022, 2021; Karras et al., 2022; Rombach et al., 2022), and subsequent work has extended these models to generate complex objects--such as 3D assets, tiled images, and more. Although such approaches typically require training or at least fine-tuning of the diffusion models for these new modalities (Wang et al., 2023; Luo et al., 2023), two notable exceptions, Wang et al. (2022) and Poole et al. (2022), have developed methods for _training-free_ production of 3D objects by _directly_ using an image-based diffusion model. Both methods work by optimizing a _differentiable representation_ (diffrep)--in this case, a Neural Radiance Field (NeRF) (Mildenhall et al., 2020)--to produce rendered views consistent with the output of the image-based diffusion model. Unfortunately, the nature of both these methods is that they optimize the diffrep to produce the "most likely" representation consistent with the images; that is, they perform _mode-seeking_ rather than actually _sampling_ from the diffusion model. This results in overly smoothed outputs that lack detail and do not reflect the underlying distribution of the diffusion model.

In this paper, we present a novel method for sampling _directly_ in the diffrep space using pretrained diffusion models. The method is training-free, can handle arbitrary diffreps, and performs _true sampling_ according to the underlying diffusion model rather than merely mode-seeking. The key idea of our approach is to rewrite the reverse diffusion process itself in the diffrep parameter space. This is achieved by "pulling back" the dynamics of the reverse time process--from the image space to the parameter space--and solving a (small) optimization problem, implied by the pulled-back dynamics, over the parameters for each diffusion step. To further encourage solver convergence, we identify constraints that the diffrep induces on the samples and use these constraints to guide the reverse process to generate samples from the high-density regions of the diffusion model while satisfying the constraints along the sampling trajectory.

Our experiments use a pretrained image-based diffusion model (Stable Diffusion 1.5 (Rombach et al., 2022)) to generate samples from various classes of diffreps. For example, by sampling SIREN representations (Vincent, 2011) with wrap-around boundary constraints, we can sample total 360-degree panorama scenes, and by sampling NeRF representations (Mildenhall et al., 2020), we cangenerate 3D models of many objects. In both settings--as well as in baseline comparisons on simple SIREN-based (non-panorama) image generation--our approach substantially outperforms previous methods such as Score Jacobian Chaining (SJC) (Wang et al., 2022). Though the problem setting is considerably more difficult without fine-tuning or retraining, sampling diffreps using pretrained diffusion models with our method substantially improves and extends the state-of-the-art in training-free generation methods.

## 2 Related work and preliminaries

### Differentiable representations (diffreps)

Diffreps are a powerful tool for representing complex scenes or manifolds using parameterized differentiable functions to map coordinates of the scene into a feature space that encodes the properties of the scene at those coordinates. Popular instantiations of diffreps include: **SIRENs**(Sitzmann et al., 2020), which implicitly model an image using an MLP with sinusoidal activations to map 2D \((x,y)\) pixel coordinates into RGB colors, and **NeRFs**(Mildenhall et al., 2020), which implicitly model a 3D scene using an MLP that transforms 3D \((x,y,z)\) voxel coordinates with view directions \((,)\) into RGB\(\) values. We can render images from different views of the scene by numerically integrating the NeRF outputs along the unprojected rays from the camera.

Faster or alternative diffreps also exist for 3D scenes--such as the InstantNGP (Muller et al., 2022) model or Gaussian splats (Kerbl et al., 2023)--although in this work we focus on the basic NeRF architecture. Many other kinds of visual assets can also be formulated as diffreps. For example, we can use diffreps to model panoramas, spherical images, texture maps for 3D meshes, compositions of multiple images, scenes from a movie, and even the output of kinematic and fluid simulations.

Many interesting diffreps can be used to render not just one image from the scene but _multiple coupled images_ or even a _distribution over images_. Given a diffrep, parameterized by \(\), for a scene--such as a SIREN panorama or a NeRF--we can render an image of the scene from a view \(\) using a differentiable render function \(f(,)=\). To accommodate the multiview setting in our discussion, we consider the "curried" Haskell form of the render function \(f:()\), where \(f():\) is a map itself from a view \(\) to \(f()()=f(,)=\).

In the case of NeRFs or SIREN panoramas, the view \(\) is a continuous random variable drawn from a distribution that, with abuse of notation, we will also call \(\). To formalize this, let \(^{}\) be a vector space of functions from \(\) to \(\). With this definition, we can write the signature of \(f\) as \(f:\). Although \(\) is usually larger than \(\) (and can be even infinite in some cases), we can simplify our notation by equipping \(\) with an inner product to make it a Hilbert space. This allows us to use familiar matrix notation with \(\) and hide the view dependence of \(f()\) in \(\).

To complete this formalization, we lift the inner product on \(\) to define an inner product on \(\):

\[ h,g: h,g:=_{}[  h(),g()]=_{}[h()^{}g()].\]

This formulation allows us to handle the multiview nature of NeRFs and SIREN panoramas in a mathematically rigorous way while preserving the convenient notation.

The partial application of \(f\) returns an entire view-dependent function \(f()\). Given a set of images \((x_{i})_{i[N]}\) of the scene from different views \((_{i})_{i[N]}\), the standard diffrep task is to find a \(\) that minimizes \(()=_{i[N]}\|f()(_{i})-x_{i}\|\). Because the diffreps and \(f\) are both differentiable, this can be accomplished using first-order solvers, such as gradient descent or Adam.

For pedagogical convenience, we identify \(\) with \(\) in the following sections. This identification simplifies the presentation, provides a more interpretable and intuitive perspective on our method, and is precise when \(\) is a singleton. Because \(\) is a Hilbert space, the main points of our arguments still hold when we consider a larger \(\). We describe the specific changes needed to adapt our method to the general multiview setting when \(\), in subsection 3.2.

### Diffusion models

Diffusion models implicitly define a probability distribution via "reversing" a forward noising process. While many different presentations of image diffusion models exist in the literature (e.g., DDPM[Ho et al., 2020], DDIM [Song et al., 2022], Score-Based Generative Modeling through SDEs [Song et al., 2021], EDM [Karras et al., 2022]), they are all equivalent for our intended purposes.

Given a noise schedule \((t)\) for \(t[0,T]\) and a score function \( p_{t}(x(t))\) for the distribution \(p_{t}\) over noisy images \(x(t)\) at time \(t\), we can sample from \(p_{0}\) by first initializing \(x(T)(0,^{2}(T)I)\) and then following the reverse time probability flow ODE (PF-ODE) given in Karras et al. (2022) to transform the easy-to-sample \(x(T)\) into a sample from \(p_{0}\):

\[=-(t)(t) p_{t}(x(t)).\] (1)

The PF-ODE is constructed so that the perturbation kernel (conditional distribution) is given by \(p_{0t}(x(t)|x(0))=(x(t);x(0),^{2}(t)I)\). Using the reparameterization trick, we can write this as \(x(t)=x(0)+(t)\), where \((0,I)\).

We approximate the score function with a (learned) noise predictor \((x(t),t)[x(0)|x(t)]}{(t)}\) using the Tweedie formula [Robbins, 1956, Efron, 2011]: \( p_{t}(x(t))=[x(0)|x(t)]-x(t)}{^{2}(t)} -(x(t),t)}{(t)}\).

### Training differentiable representations using diffusion priors

#### 2.3.1 Training-free methods

Poole et al. (2022) laid the foundation, demonstrating that generating 3D assets from purely 2D image-trained diffusion models was _even possible_. In **DreamFusion (SDS)**, they perform gradient ascent using \(_{t,}[w(t)J^{}((f()+(t) ,t)-)]\), derived from the denoising objective, where \(J\) is the Jacobian of the differentiable render function \(f\) and \(\) is the learned noise predictor.

Independently, Wang et al. (2022) introduced **Score Jacobian Chaining (SJC)**, which performs gradient ascent using \( p():=_{t,}[J^{} p_{t}(f( )+(t))]\) with a custom sampling schedule for the \(t\)s. The custom schedule can be interpreted as gradient annealing to align the implicit \((t)\) of the diffrep with the \(t\) used to evaluate the score function.

Comparison of methodsUsing the Tweedie formula, we can rewrite the SDS objective as \(_{t,}[J^{} p_{t}( f()+(t))]\)1. This expression is identical to the \( p()\) term from SJC if we let \(w(t)=(t)\) for all \(t\). Both methods follow this gradient to convergence, which leads to a critical point--a local maximum or _mode_--in the \( p()\) landscape. To approximate the objective, both approaches use Monte Carlo sampling.

Both approaches optimize the objective using gradient ascent (GA). While GA on the transformed score resembles solving the PF-ODE-particularly in their discretized forms-the two procedures serve fundamentally different purposes. GA cares only about finding a local maximum in the \( p()\) landscape, regardless of the trajectory taken to get there. In contrast, the PF-ODE follows a specific path to ensure that generated samples are typical of the distribution. For a discussion of the limitations of GA and why it does not produce representative samples from the distribution, see Appendix A.

Recent developmentsSeveral subsequent works have built upon the SDS and SJC methods by incorporating additional inputs, fine-tuning, and regularization to enhance the quality of the generated 3D assets. Zero-1-to-3 [Liu et al.] expands on SJC by fine-tuning the score function to leverage a single real input image and additional view information. Magic 123 [Qian et al.] further builds on this by incorporating additional priors derived from 3D data. Fantasia3D [Chen et al.] separates geometry modeling and appearance into distinct components, using SDS to update both. HiFA [Zhu et al.] introduces a modified schedule and applies regularization to improve the SDS process. Finally, LatentNeRF [Metzer et al.] utilizes SDS but parameterizes the object in the latent space of a stable diffusion autoencoder, rendering into latent dimensions rather than RGB values. While these methods rely on the SDS/SJC framework for mode-seeking, our work takes a wholly different approach, focusing on developing a more faithful sampling procedure to replace SDS/SJC for both 3D generation and broader differentiable function sampling.

#### 2.3.2 Pretrained and fine-tuning methods

Unlike SDS and SJC--which are entirely zero-shot and can be performed with a frozen diffusion model--several other methods have been developed to achieve improved quality and diversity of the generated diffreps, albeit at the cost of additional fine-tuning. While some of these methods require additional data [Zhang et al., 2018], ProlificDreamer (VSD) [Wang et al., 2023] and DiffInstruct [Luo et al., 2023] are two examples in which diffusion models can be fine-tuned or distilled using only synthetically generated data.

VSD specifically addresses the problem of generating 3D assets from a 2D diffusion model using particle-based variational inference to follow the Wasserstein gradient flow, solving the KL objective from SDS in the \(_{2}()\) parameter distribution space. This approach produces high-quality, diverse samples even at lower guidance levels.

In our work, we restrict ourselves to the _training-free setting_. This choice more readily enables application to new modalities and is independent of the score function estimator's architecture.

### Constrained sampling methods

Our method requires generating multiple consistent images of a scene from different views. For instance, we generate various images of a 3D scene from different camera locations and orientations to determine NeRF parameters. These images must be consistent to ensure coherent updates to the diffrep parameters. While the diffrep inherently enforces this consistency, we can improve our method's convergence rate by encouraging the reverse process to satisfy consistency conditions using constrained sampling methods.

Several approaches enable constrained sampling from an unconditionally trained diffusion model. The naive approach of projecting onto constraints [Song et al., 2021] leads to samples lacking global coherence. In contrast, Lugmayr et al.  propose RePaint--a simple method for inpainting that intermingles forward steps with reverse steps to harmonize generated noisy samples with constraint information. For references to more general methods used to constrain the distribution by conditioning the score function, see Appendix B.

## 3 Pulling back diffusion models to sample differentiable representations

The score model associated with the noise predictor implicitly defines a distribution on the data space \(\), and the reverse time PF-ODE from Eq. 1 provides a means to sample from that distribution. In this section, we will show how to use the score model and the PF-ODE to sample the parameters of a differentiable representation (diffrep) so that the rendered views resemble samples from the implicit distribution.

SDS and SJC derive their parameter updates using the pullback of the sample score function through the render map \(f\). This pullback is obtained using the chain rule: \(=\). The pullback score is then used _as is_ for gradient ascent in the parameter space. However, since \(p\) is a distribution--not a simple scalar field--the change of variables formula requires an additional \( J\) volume adjustment term for the correct pulled-back score function in the parameter space, where \(J\) is the Jacobian of \(f\). A careful examination of this approach through the lens of differential geometry reveals even deeper issues.

In differential geometry, it is a vector field (a section of the tangent bundle \(T\))--not a differential \(1\)-form (a section of the cotangent bundle \(T^{*}\))--that defines the integral curves (flows) on a manifold. To derive the probability flow ODE in the parameter space, we must pull back the sample vector field \( T\) to the parameter vector field \( T\) using \(f\). The pullback of the vector field through \(f\) is given by \((f^{*})|_{}=(J^{}J)^{-1}J^{}|_{f()}\), Thus, as we derive in Appendix C, the pullback of the probability flow ODE is:

\[=-(t)(t)(J^{}J)^{-1}J^{}  p_{t}(f()).\] (2)

We note that this is in contrast to the score chaining from Poole et al. , Wang et al. . The confusion stems from comparing the different types of pulled-back elements. While \(\) in Eq. 1is a vector field, the score function \( p_{t}(x(t))\) is a covector field--a differential form. Pulling back the score function as a differential form correctly yields \(J^{} p_{t}(f())\)--the term used in SJC. The issue lies with the hidden (inverse) Euclidean metric within Eq. 1, which converts the differential-form score function into the corresponding vector field.

In canonical coordinates, the Euclidean metric is the identity. Consequently, the components of the score function remain unchanged when transformed into a vector field. Therefore we can safely ignore the metric term in the PF-ODE formulation of Eq. 1 for the sample space \(\). However, this is not true for the diffrep parameter space \(\).

To convert the pulled-back score function into the corresponding pulled-back vector field, we must use the pulled-back inverse Euclidean metric given by \((J^{}J)^{-}\). This yields the pulled-back form of the PF-ODE in Eq. 22. Fig. 1 illustrates this procedure via a commutative diagram and provides an example of what goes wrong if you use the incorrect pulled-back term, as suggested by SDS and SJC. In the SDS and SJC approaches, the \((J^{}J)^{-}\) term behaves like a PSD preconditioner for gradient ascent. It may accelerate the convergence rate to the mode but does not fundamentally change the solution. However, this term is critical for the pulled-back PF-ODE since it impacts the entire trajectory and the ultimate sample (see Appendix A for more discussion). For an explanation of why the \( J\) term is absent in the mathematically correct update, see Appendix C.

A more intuitive way to understand why the correct update is given by Eq. 2 rather than SDS and SJC's version is to consider the case where the input and output dimensions are equal. From the chain rule, we have \(=\). Using the inverse function theorem, we compute \(=()^{-1}=J^{-1}\), where \(J\) is \(f\)'s Jacobian. This leads to \(=J^{-1}\). For invertible \(f\), this equation and Eq. 2 are equivalent. For non-invertible \(f\), we can interpret the pullback as the solution to the least-squares minimization problem \(_{}\|J-\|^ {2}\).

### Efficient implementation

Separated noiseFollowing the pulled-back PF-ODE in Eq. 2, we can find \(_{t}\) such that \(f(_{t})\) represents a sample \(x(t) p_{0t}(x(t)|x(0))=(x(t);x(0),^{2}(t)I)\). However, this approach has a limitation: \(x(t)\) is noisy, and most diffrep architectures are optimized for typical, noise-free images. Consequently, \(J\) is likely ill-conditioned, leading to longer convergence times.

Figure 1: **(Left)** Commutative diagram showing how the PF-ODE vector field gets pulled back through \(f\), respecting the differential geometry. The process involves: 1�converting \(\) to the cotangent vector field \( p(x)\) (up to scaling terms) with the Euclidean metric \(I\), 2�! pulling back \( p(x)\) via the chain rule using the Jacobian \(J\), and then 3�! transforming the pulled back differential form score function into the corresponding vector field using the inverse of pulled back metric \((J^{}J)^{-1}\). When used in a PF-ODE, SJC and SDS take the bottom path with the chain rule, however they do not complete the path by neglecting the \(T^{*} T\) transformation. **(Right)** SIREN image renders generated using the PF-ODE schedule with the prompt “An astronaut riding a horse” using the: (a) complete pulled-back \(\) vector field, (b) pulled-back covector field from SJC (omitting step 3�!) \(J^{} p(x)\), (c) Scaled pulled-back covector field from SJC \(=0.0001\).

To address this issue, we factor \(x(t)\) into a noiseless signal and noise using the reparameterization of the perturbation kernel: \(x(t)=_{0}(t)+(t)(t)\). By letting \((t)=\) remain constant throughout the sampling trajectory and starting with \(_{0}(T)=0\), we can update \(_{0}\) with \(_{0}}{dt}=-(t)\). This decomposition allows \(f(_{t})\) to represent the noiseless \(_{0}(t)\) instead of \(x(t)\), substantially improving the conditioning of \(J\). Fig. 2 illustrates how this separation works in practice.

Efficient optimizationThe Jacobian \(J\) in Eq. 2 represents the derivative of the image with respect to the diffrep parameters. For all but the smallest examples, explicitly forming \(J^{}J\) is _computationally intractable_. While iterative linear solvers together with modern automatic differentiation frameworks (e.g., ) could solve Eq. 2, we found it faster to compute the parameter update as the solution to a non-linear optimization problem. This approach avoids the costly JVP (Jacobian vector product) required to multiply with \(J^{}J\).

For each step with size \( t\), we solve:

\[=*{arg\,min}_{}\|f()-f( -)-(t) t((f()+(t) ,t)-)\|^{2}.\] (3)

As \( t 0\), using the linearization \(f(-)=f()-J+(\|\|^ {2})\), this optimization objective approaches the linear least-squares solution. The optimal \(\) converges to the exact \(\) from Eq. 2. In practice, we employ the Adam optimizer to solve Eq. 3 for whatever discrete \( t\) is used in the diffusion sampling process. This procedure avoids the explicit formation of the linear least-squares solution.

### Coupled images, stochastic functions, and 3D rendering

Thus far, we have only considered the case where the output of the render function is a _single image_\(x\). However, our interest lies in render functions \(f\) that map parameters \(\) to an entire view-dependent image space \(^{}\). Using the inner product defined in subsection 2.1, we derive the view-dependent pullback PF-ODE:

\[=f^{*}_{0}}{dt}=f^{*} -(t)=-(t)(t)J _{}^{}J_{}^{-1}[J_{}^{}(  p_{t}(x(),)-)],\] (4)

Figure 2: The parameters of the diffrep \(_{t}\) (torus) are used to render the noiseless signal \(f(_{t})=_{0}(t)\), which are then combined with the noise \((t)\) to generate the noisy sample \(x(t)\). We can pull back each step of the reverse diffusion process to update the parameters \(_{t}+_{t}\).

where the expectations are taken over the views \(\). Here, \( p_{t}(x(),)\) represents the view-specific score function. We use the original score function \( p_{t}(x())\), with additional view information in the prompt. \(()\) denotes the separately managed noise rendered from view \(\).

Examining the optimization form of this equation provides further insight. We can interpret it as minimizing the norm in the function space \(\|\|_{}^{2}=_{}\|\|^{2}\): This is directly analogous to Eq. 3, but with a different inner product. Explicitly written:

\[=*{arg\,min}_{}_{} \|f(,)-f(-,)- t( _{t}()-())\|^{2},\] (5)

which we can empirically minimize using view samples \(\).

### Consistency and implicit constraints

Our method successfully "pulls back" the PF-ODE \(\) using the pull-back of the score function \( p_{t}(x(t))\). However, our true goal is to pull back \( p_{t}(x(t)|_{0}(s)*{range}(f))\) for all \(s t\). This ensures that \(f\) can render the noiseless components \(_{0}\) throughout the remaining reverse process. For example, when pulling back the PF-ODE for different views of a 3D scene, we want to ensure consistency across all the views.

When \(f\) is sufficiently expressive and invertible, \( p_{t}(x(t)|_{0}(s)*{range}(f))  p_{t}(x(t))\). This would allow us first to sample \(x(0)\) using the PF-ODE from Eq. 1 and then invert \(f\) to find \((0)\). However, for most significant applications of our method, \(f\) is not invertible.

In noninvertible cases, consider the sampling trajectory \(x(t)\) in \(\) when \(_{0}(t)\) range \(f\), particularly when no nearby sample has high probability in range \(f\). Each step of Eq. 2 follows the direction in \(\) that best approximates the direction of the score model in a least-squares sense. When \(f\) is not invertible, \(J\) is not full rank, and the update to \(\) will not precisely match the trajectory in \(x\). Consequently, \(\|f(_{t}-)-x(t- t)\|\) will be significant. The score model, unaware of this, will continue to point towards high-probability regions outside the range of \(f\).

For example, consider an \(f\) that only allows low-frequency Fourier modes in generated samples. The unconstrained score model might favor images with high-frequency content (e.g., hair, explosions, detailed textures), resulting in blurry, unresolved images. However, suppose the score model was aware of this constraint against high-frequency details. In that case, it might guide samples toward more suitable content, such as landscapes, slow waves, or impressionist art--dominated by expressible low-frequency components. This issue becomes more pronounced when sampling multiple coupled images through \(f\), such as with panoramas and NeRFs, where various views must be consistent.

To address this challenge, we adapt the RePaint method (Lugmayr et al., 2022), initially designed for inpainting, to guide our PF-ODE towards more "renderable" samples. RePaint utilizes the complete Langevin SDE diffusion process, interspersing forward and reverse steps in the sampling schedule. This approach allows the process to correct for inconsistencies during sampling. The forward steps harmonize samples at the current time step by carrying information from earlier steps that implicitly encode the constraint. RePaint requires a stochastic process for conditioning, so we employ the DDIM (Song et al., 2022) sampling procedure with controllable noise at each step. We derive the forward updates of DDIM in Appendix D.

### Summary

Our method introduces significant advancements for sampling diffreps using pretrained diffusion models by performing actual sampling instead of merely mode-seeking, thus capturing the full diversity of high-dimensional distributions. We derive the correct pullback of the PF-ODE, incorporating the essential \((J^{}J)^{-1}\) term to ensure unbiased sampling. Efficiency is enhanced through separated noise handling and a faster suboptimization process, allowing practical application for complex representations. Additionally, our approach extends to coupled images and stochastic functions with implicit constraints, making it suitable for tasks like panorama generation and 3D rendering. A comprehensive summary of the contributions in this section and the pseudocode for the DDRep algorithm using DDIM sampling with RePaint is presented in Appendix E.

## 4 Experiments

All our experiments used the Hugging Face implementation of Stable Diffusion 1.5 (SDv1.5) from Rombach et al. (2022) with the default hyperparameters as the noise predictor model. We conducted our experiments on a single NVIDIA A6000 GPU. We used the complete Langevin SDE given by the DDIM (Song et al., 2022) procedure for all our experiments, with \(=0.75\) as the stochastic interpolation hyperparameter. We interspersed forward and reverse steps to harmonize the diffrep constraints in the sampling procedure. For the suboptimization problem described in Eq. 3, we used the Adam optimizer for 200 steps.

### Image SIRENs

We compare the results of our method with those of SJC (Wang et al., 2022) when used to produce SIRENs (Vincent, 2011) for images. While we could directly fit the SIREN to the final image \(x(0)\)--obtained by first solving the PF-ODE in the image space--we use this scenario to compare different _training-free_ generation methods quantitatively.

The SIREN we used was a 3-layer MLP mapping the 2D \((x,y)^{2}\) pixel coordinates to the \(^{4}\) latent space of SDv1.5. The MLP had a 128-dimensional sinusoidal embedding layer, a depth of 3, a width of 256, and used \(()\) as the activation function. To render an image, we used a \(64 64\) grid of equally spaced pixel coordinates from \(\) to \(\) with a shape of \((64,64,2)\). The MLP

Figure 4: Samples generated using SD ref (top), Our method (middle), and SJC (bottom) using the same prompt “An office cubicle with four different types of computers” with eight different seeds.

Figure 3: The left figure contains sample renders using the prompt “A woman is standing at a crosswalk at a traffic intersection.” from the reference SD (top), our method (middle), and SJC (bottom) over the CFG scales  from left to right. The right plot is the KID metric (closer to 0 is better) measured on the SIRENs sampled from our method, the SD reference samples, and the SIRENs sampled using SJC.

output was a latent image with shape \((64,64,4)\), which we decoded using the SDv1.5 VQ-VAE into an RGB image with shape \((512,512,3)\). We calculated the final metrics on these rendered images.

We used the first 100 captions (ordered by id) from the MS-COCO 2017 validation dataset [Lin et al.] as prompts. We generated three sets of images: (1) reference images using SDv1.5, (2) images rendered from SIRENs sampled using our method, and (3) images rendered from SIRENs generated using SJC [Wang et al., 2022]. We compared these sets against 100 baseline images generated using SDv1.5 with the same prompts but a different seed. We repeated this experiment at five CFG scales [Ho and Salimans] (0, 3, 10, 30, 100). We used the KID metric from Binkowski et al. to compare the generated images, as it converges within 100 image samples while maintaining good comparison ability. For runtime metrics, see Appendix F.

The example images in Fig. 3 (left) demonstrate that the samples generated by our method are almost indistinguishable from the reference. The results in Fig. 3 (right) clearly show that our method produces images on par with SDv1.5 reference samples.

To further support this observation, we measured the PSNR, SSIM, and LPIPS scores of our method's samples against the SDv1.5 reference images (Table 1). The results show that our method produces SIREN images nearly identical to the reference set.

To demonstrate that our method achieves sampling instead of mode-seeking, we plot eight samples using the same prompt for all models in Fig. 4 with different seeds at CFG scale 10. The samples generated by SJC all look very similar, while those generated by our method and SD show significantly more diversity.

### SIREN Panoramas

In this section, we demonstrate how our method can generate SIREN panoramas using the same architecture as the SIREN from subsection 4.1 but with a modified render function. We begin by sampling a view grid of pixel locations with the shape \((8,64,64,2)\) where the batch size is \(8\), the height and width are \(64\), and the last \(2\) dimensions are the \((x,y)\) coordinates. The \(y\) values are equally spaced in the range , and \(x\) values are equally spaced in the range \([r,r+1/] 1.0\) for \(r U(0,1)\). The \( 1.0\) constraint ensures horizontal wrapping, making the panorama continuous over the entire \(360^{}\) azimuth.

We render the view by sampling from the SIREN at the discrete pixel locations given by the view grid, then decode the latent output using the SDv1.5 VQ-VAE. For our experiments, we assumed that the images were taken by a camera lens with a \(45^{}\) field of view (equivalent to a 50mm focal length), corresponding to an aspect ratio of \(=8\).

The SIREN panorama imposes implicit constraints to ensure consistency across overlapping views. Fig. 5 and Fig. 7 illustrate the impact of the consistency conditioning described in subsection 3.3.

Figure 5: Comparison of landscape panoramas sampled using our method. The top panorama is sampled using the RePaint method, while the bottom is sampled without RePaint. Both approaches use 460 function evaluations (NFEs) to ensure fairness and a CFG scale of 10.0. The prompt for these panoramas was “Landscape picture of a mountain range in the background with an empty plain in the foreground 50mm f/1.8”.

Both RePaint and non-RePaint methods use the same number of function evaluations (460 NFEs) to ensure fair comparisons.

Using RePaint substantially improves the quality and consistency of the generated panorama. This improvement is even more pronounced at low CFG scales, where the diffusion model is encouraged to be more creative and is typically less likely to produce consistent images, as seen in Fig. 7. For runtime metrics and additional panoramas generated using our approach, see Appendix F.

### 3D NeRFs

For our NeRF experiments, we employed the voxNeRF architecture used by SJC (Wang et al., 2022), chosen for its speed and efficiency. While we did not utilize any additional loss terms from their method, we found that incorporating the object-centric scene initialization from Wang et al. (2023) significantly improved our results. This initialization bootstrapped the diffusion process and led to faster convergence. Specifically, we used the initialization function \(_{}(x)=10(1-)\), as detailed in Wang et al. (2023). For more experimental details, see Appendix F.

The NeRFs generated using our method, as illustrated in Fig. 6, demonstrate our approach's capability to produce detailed, high-quality 3D representations from text prompts. These results showcase the effectiveness of our method in generating complex 3D structures while maintaining consistency across multiple views.

## 5 Conclusion

We have presented a comprehensive, training-free approach for pulling back the sampling process of a diffusion model through a generic differentiable function. By formalizing the problem, we have addressed two critical issues with prior approaches: enabling true sampling rather than just mode seeking (crucial at low guidance levels), and addressing a latent consistency constraint using RePaint to improve generation quality. For future work and limitations, see Appendix G.

Our method opens up new possibilities for generating complex and constrained outputs using pretrained diffusion models. We hope that future research will build on these insights to further expand the capabilities and applications of diffusion models in areas such as 3D content generation, game design, and beyond.

Figure 6: NeRFs generated using our method with the prompts (top) “a photo of a delicious hamburger, centered, isolated, 4K.”, (middle) “a DSLR photo of a rose”, and (bottom) “a DSLR photo of a yellow duck”.