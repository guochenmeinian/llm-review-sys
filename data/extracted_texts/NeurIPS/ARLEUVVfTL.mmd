# Improving Neural ODE Training with Temporal Adaptive Batch Normalization

Su Zheng\({}^{1}\)

Zhengqi Gao\({}^{2}\)

The first two authors contribute equally. Correspond to Bei Yu at byu@cse.cuhk.edu.hk.

Fan-Keng Sun\({}^{2}\)

Department of EECS, MIT

Duane S. Boning\({}^{2}\)

Department of EECS, MIT

Bei Yu\({}^{1}\)

Department of CSE, CUHK

Department of EECS, MIT

Martin Wong\({}^{1}\)

Department of CSE, CUHK

Department of EECS, MIT

###### Abstract

Neural ordinary differential equations (Neural ODEs) is a family of continuous-depth neural networks where the evolution of hidden states is governed by learnable temporal derivatives. We identify a significant limitation in applying traditional Batch Normalization (BN) to Neural ODEs, due to a fundamental mismatch -- BN was initially designed for discrete neural networks with no temporal dimension, whereas Neural ODEs operate continuously over time. To bridge this gap, we introduce temporal adaptive Batch Normalization (TA-BN), a novel technique that acts as the continuous-time analog to traditional BN. Our empirical findings reveal that TA-BN enables the stacking of more layers within Neural ODEs, enhancing their performance. Moreover, when confined to a model architecture consisting of a single Neural ODE followed by a linear layer, TA-BN achieves 91.1% test accuracy on CIFAR-10 with 2.2 million parameters, making it the first unmixed Neural ODE architecture to approach MobileNetV2-level parameter efficiency. Extensive numerical experiments on image classification and physical system modeling substantiate the superiority of TA-BN compared to baseline methods.

## 1 Introduction

Originally derived as the continuous limit of a residual connection , neural ordinary differential equations (Neural ODEs)  is a family of continuous-depth neural networks where the evolution of hidden states is governed by learnable temporal derivatives. These models exhibit several intriguing features, such as the capability of temporal reversibility, which enables generative modeling .

Previous studies on Neural ODEs parameterize the learnable temporal derivatives using a shallow neural network with a limited number of parameters . Without special treatment, merely stacking additional layers in the temporal derivatives does not necessarily enhance Neural ODE performance. Furthermore, deeper networks might increase the stiffness of the ODE system, leading to challenges with the ODE solver, such as excessively small step sizes or even failures due to infinite state values, as shown in Figure 1.

In efforts to deepen the model and address instabilities, one might instinctively consider Batch Normalization (BN)  as a remedy to stabilize the intermediate state values of Neural ODEs , because BN was originally proposed to accelerate

Figure 1: Test accuracies are depicted over the training epochs on CIFAR-10, utilizing Neural ODEs with different numbers of convolution layers as the backbones for learnable derivatives. Dashed horizontal lines denote instances of training failures.

the training of (discrete) neural networks and consolidate them against internal covariate shift . However, preliminary works surprisingly found that the introduction of BN into Neural ODEs can even degrade performance, an effect for which the underlying causes remain unclear [13; 35].

In this paper, we demystify the aforementioned phenomenon and demonstrate that directly applying traditional BN to Neural ODEs is fundamentally flawed. The primary complication arises because the forward pass of Neural ODEs employs an adaptive step size solver that discretizes time variably. Consequently, it is not assured that two forward passes will coincide with the same discretized time grids. This variability precludes the possibility of retrieving _population statistics_ at test time. Additionally, relying on _mini-batch statistics_ for BN renders the outputs of Neural ODEs non-deterministic and vulnerable to outliers and small batch size.

Motivated by these observations, we introduce temporal adaptive BN (TA-BN), a novel technique designed as the continuous-time counterpart to traditional BN, tailored specifically for Neural ODEs. Our empirical findings demonstrate that TA-BN facilitates the up-scaling of Neural ODE model sizes without encountering performance saturation (See Figure 1). Moreover, when confined to a model architecture consisting of a single Neural ODE followed by a linear layer, TA-BN achieves 91.1% test accuracy on CIFAR-10 with 2.2 million parameters, making it the first unmixed Neural ODE architecture to approach MobileNetV2-level parameter efficiency. Extensive numerical experiments on image classification and physical system modeling substantiate the superiority of TA-BN compared to baseline methods.

## 2 Preliminary

Neural ODEsNeural ODEs[4; 7; 37; 45; 8; 42; 30; 18; 32; 19; 31] forge a significant linkage between differential equations and neural networks. They model the continuous dynamics of hidden states with a learnable ODE system:

\[(t)}{dt}=_{}((t),t)\] (1)

where \(_{}(,)^{D}^{D}\) is a neural network parameterized by learnable parameter \(\) controlling the temporal evolution, and \((t)^{D}\) is the ODE state variable at time \(t\). The practical usage of Neural ODEs is by solving an initial value problem: given the initial state \((0)\) as input, the state \((T)\) at a later time \(T\) is computed using an ODE solver and returned as the output. The gradients required for training are computed via the adjoint method [4; 31; 18] reverse in time.

Subsequent works have expanded the capabilities and scope of Neural ODEs by introducing modifications, such as improving expressivity through state augmentation , coupling the evolution of both weights and activations , accelerating convergence via semi-norm techniques , extending to graph neural networks  and second-order ODEs , adapting to irregular time-series data [19; 32], and drawing connections to diffusion models [15; 28]. Of particular interest, two preliminary studies [13; 35] have indicated that applying BN within Neural ODEs may compromise model performance. Our work aims to unravel this phenomenon and propose a version of BN tailored to operate effectively with Neural ODEs.

Batch NormalizationBN[5; 29; 16; 39; 2; 21] performs a re-centering and a re-scaling operation on the given input by subtracting the mean and dividing by the standard deviation:

\[(x_{i})=_{,}(x_{i})=-}{+}}+\] (2)

where \(^{+}\) ensuring division validity, \(\{,\}\) are learnable parameters, and \(x_{i}\) represents the \(i\)-th data in one batch of size \(B\). During training, the mini-batch statistics 2\(=1/B_{i=1}^{B}x_{i}\) and \(^{2}=1/B_{i=1}^{B}(x_{i}-)^{2}\) are used for efficiency. In contrast, at test time, BN uses population statistics aggregated across the entire training dataset to ensure deterministic outputs . BN can be extended to multiple dimensions/channels by processing each one separately.

## 3 Traditional Batch Normalization in Neural ODEs

Let us consider a simplified one-dimensional Neural ODE when incorporating BN as example:

\[=_{,}(wh+b)\] (3)

where \(=\{,,w, b\}\) are the learnable parameters. Given a batch of input \(\{h_{i}(0)\}_{i=1}^{B}\) (i.e., initial values of the ODE), the forward process employs an ODE solver to discretize Eq. (3) over time, visiting \(N\) sequential points \(=(t_{0}=0,t_{1},t_{2},,t_{N}=T)\) and computing the outputs \(\{h_{i}(T)\}_{i=1}^{B}\). An adaptive step size solver (e.g., Runge-Kutta method) is usually preferred because of its higher efficiency compared to a fixed step size solver (e.g., Forward Euler method). Thus, the differences between two consecutive time steps might not be identical (e.g., \(t_{2}-t_{1} t_{1}-t_{0}\)), and the value of \(N\) also depends on the batch of data. The temporal discretization indicates that we need to invoke BN at every \(t\) for every sample:

\[(x_{i,j})=_{,}(x_{i,j})=-_{j}}{ ^{2}+}}+,x_{i,j}=w h_{i}(t_{j})+b\] (4)

for every \(j=1,2,,N\) and \(i=1,2,,B\), where \(_{j}\) and \(_{j}^{2}\) are statistics associated with \(t_{j}\).

During trainingTo perform the normalization shown in Eq. (4), BN applies mini-batch mean \(_{j}=1/B_{i=1}^{B}x_{i,j}\) and variance \(_{j}^{2}=1/B_{i=1}^{B}(x_{i,j}-_{j})^{2}\). Although the training can proceed under normal conditions, the statistics cannot be successfully accumulated across batches. For instance, the time grid \(\) utilized for the first batch might differ significantly from the grid \(^{}\) used for a subsequent batch. Consequently, specific time points \(t_{j}^{}^{}\) might not coincide with any time points in \(\), and vice versa. This might result in an impractical collection of infinite statistics \(_{j}\) and \(_{j}^{2}\). Moreover, except those associated with \(t=0\) and \(t=T\), most stored statistics will be calculated and updated based on a limited number of batches.

During inferenceConventionally, BN should perform the normalization shown in Eq. (4) using population statistics during inference. However, as shown in the left part of Figure 2, the population statistics associated with the time point \(t_{j}^{}^{}\), required by the temporal discretization during inference, might not be available if the time value \(t_{j}^{}\) is never encountered during training. Moreover, even if the population statistics exist, they are likely to be inaccurate as discussed above.

Figure 2: Left: The failure of Pop BN in Neural ODEs stems from the misalignment of discretized time grids. Pop-TI BN aggregates all running mini-batch statistics into a single pair of \((,^{2})\), implicitly assuming time-independent population statistics. Right: Our proposed TA-BN automatically conducts temporal interpolation to accumulate statistics and update parameters during training and testing.

SummaryA seemingly straightforward remedy could be to consistently use mini-batch statistics during both training and inference, thus avoiding the need for accumulating population statistics. However, this approach has a critical limitation: the output \(h_{i}(T)\) becomes highly dependent on their respective batch's specific characteristics. This will make the outputs inaccurate when batch size is very small (e.g., \(B=2\)) or the occurrence of outliers in batch, and even fail in single-run scenarios (\(B=1\)) because running standard deviation cannot be calculated. As shown in the left of Figure 3, when the batch size is reduced to 2, mini-batch BN displays degraded accuracies. Furthermore, when noisy data exist, mini-batch BN performance deteriorates, as shown in the middle of Figure 3.

For clarity, we will henceforth refer to the use of population statistics at test time as Pop BN, and the use of mini-batch statistics as Mini-batch BN. Notably, both methods utilize mini-batch statistics during training. It is important to emphasize that in practice, when implementing BN in Neural ODEs with popular deep learning frameworks (e.g., PyTorch with TorchDiffeq ), actually a variant of Pop BN is used, which we will refer to as Pop-TI BN. Pop-TI BN accumulates statistics at all encountered time points during training into a single pair of \(\) and \(^{2}\), which implicitly assumes that all time points share the same statistics. This aggregation approach can result in outputs during test time without errors concerning missing population statistics which we would anticipate as in Pop BN. Unfortunately, these predictions are meaningless and often lead to poor performance, as illustrated in the left of Figure 3. This issue highlights the challenges identified in recent studies [13; 35]. 3 In a nutshell, our observations can be succinctly summarized as follows:

* Pop BN fails when Neural ODEs employ an adaptive step size solver.
* Pop-TI BN implicitly assumes time-independent statistics, leading to erroneous outputs.
* Mini-batch BN fails when Neural ODEs operate with small batch sizes or outliers exist.

## 4 Temporal Adaptive Batch Normalization

The fundamental reason why Pop-TI BN and Mini-batch BN fail in Neural ODEs is attributed to the added temporal dimension, where the distributions at different time points vary, effectively constituting a stochastic process as depicted in the right of Figure 3. This observation suggests that BN should be defined to be time-dependent in Neural ODEs. Ideally, we would want BN to store population statistics at every \(t[0,T]\), with corresponding learnable parameters \((t)\) and \((t)\) defined at every \(t\). However, implementing such a model is impractical, because not every \(t\) will be encountered in training, and the update of \((t)\), \((t)\), and the population statistics will be naturally sparse on \(t\), resulting in inefficiency.

Figure 3: Left: We train a Neural ODE with a U-Net backbone as the learnable derivatives on CIFAR-10. Mini-batch BN shows degraded accuracies with a batch size of 2, while TA-BN can maintain high accuracies under varying batch sizes, because it uses the estimated population statistics during testing. Pop-TI BN aggregates running statistics encountered at any time points into a single pair of \(\) and \(^{2}\). This approach assumes time-independent statistics, leading to erroneous predictions and erratic test loss curves. Middle: When noisy data exist on average in one out of every test batch, Mini-batch BNâ€™s performance deteriorates, because the noise affects the mini-batch statistics. The backbone for learnable derivatives in this experiment consists of 6 convolution layers. Right: We plot the output statistics from the first layer of U-Net over time; they are time-dependent.

To address the aforementioned problem, we propose an intuitive method termed temporal adaptive BN (TA-BN) for usage in Neural ODEs. Let us illustrate it with the example shown in Eq. (3). To begin with, we evenly divide the time span \([0,T]\) defining \((M+1)\) time grids \(^{}=(t_{0}^{}=0,t_{1}^{}=,t_{2}^{}= ,,t_{M}^{}=T)\). We associate the time grid \(t_{m}^{}\) with population mean \(_{m}^{}\) and population variance \(_{m}^{,2}\), as well as learnable parameters \(^{}_{m}\) and \(_{m}^{}\) for every \(m=0\), \(1,2,,M\). For later simplicity, we denote \(^{}=[_{0}^{},_{1}^{},,_{M}^{ }]^{T}\) and \(^{}=[_{0}^{},_{1}^{},,_{M}^ {}]^{T}\). Similar notations apply to \(^{}\) and \(^{}\).

**During training** As shown in the right of Figure 2, given a batch of data \(\{h_{i}(0)\}_{i=1}^{B}\), the forward pass of Neural ODE might discretize the time as \(=(t_{0},t_{1},,t_{N})\) which differs from the grids \(^{}\) of TA-BN. However, TA-BN can calculate the temporal derivative for the \(i\)-th data at \(t_{j}\):

\[_{^{},^{}}(x_{i,j})=- _{j}}{^{2}+}}_{j}+_{j}x_{i,j}=w h_{i}(t_{j})+b\] (5)

where \((_{j},_{j}^{2})\) are still the mini-batch mean and variance based on \(\{x_{i,j}\}_{i=1}^{B}\). Here \(_{j}\) and \(_{j}\) are interpolated based on \(^{}\) and \(^{}\), respectively:

\[_{j}=G(t_{j},^{},^{}),_{j}=G (t_{j},^{},^{})\] (6)

where \(G(t,,)\) is a function to interpolate the value \(a(t)\) given an array of values \(^{M+1}\) and their corresponding \((M+1)\) time points \(\). There are many choices for \(G(,,)\), such as linear, cubic spline, and kernel smoothing. We empirically observe that linear interpolation based on two nearest neighbors suffices for our experiments, and we advocate it due to its implementation simplicity:

\[G(t,,)=-t}{t_{l+1}-t_{l}}a_{l}+} {t_{l+1}-t_{l}}a_{l+1}\] (7)

where \(l\) is the index which makes \(t_{l}\) to be the largest value smaller than \(t\) in \(\). Using a linear \(G(,,)\) implies that we approximate the underlying \(a(t)\) in a piece-wise linear manner, and as long as the number of time grids is sufficiently large, \(G(t,,)\) can approximate any continuous \(a(t)\) arbitrarily well, i.e., \(|G(t,,)-a(t)| 0\) when \(M\). In practice, we set \(M\) close to the number of ODE-discretized time grids. Please refer to Section 5 for ablations on the choice of \(G(,,)\).

During training, we also need to accumulate the encountered running mini-batch statistics \((_{j},_{j}^{2})\) to the population statistics \((_{m}^{},_{m}^{,2})\), so that later they can be used for inference. As shown in the right part of Figure 2, this requires the interpolation to be performed in the opposite direction:

\[_{m}^{}&\ (1-)_{m}^{}+ G(t_{m}^{},, ),=[_{1},_{2},,_{N}]^{T}\\ _{m}^{,2}&\ (1-)_{m}^{,2}+  G(t_{m}^{},^{},),^{ }=[_{1}^{2},_{2}^{2},,_{N}^{2}]^{T}\] (8)

where \(\) is a momentum constant to perform moving averaging to update the population statistics based on current estimate of \(_{m}^{}\) and \(_{m}^{,2}\) using the new observed values \(G(t_{m}^{},,)\) and \(G(t_{m}^{},^{},)\), respectively. Note that the training does not involve the gradients with respect to \(t\).

**During inference** After the training phase, TA-BN will have well-trained parameters \(^{}\) and \(^{}\), and accurately record population statistics \(^{}\) and \(^{}\). During inference, when provided with a batch of data and denoting the discretized time grids as \(^{}\), as depicted in the right part of Figure 2, we again proceed with the interpolation step:

\[_{j}^{}& =G(t_{j}^{},^{},^{}), _{j}^{}=G(t_{j}^{},^{},^{ })\\ _{j}^{}&=G(t_{j}^{ },^{},^{}),_{j}^{,2}=G(t_{j} ^{},^{,2},^{})\] (9)

This interpolation differs from the training phase in that now the interpolation directions are the same for parameters and the population statistics, because now we utilize them for the new time grids \(^{}\).

**Implementations and summary** In practice, we observe that the interpolations during training outlined in Eq. (6)-(8) have opposite directions, potentially leading to increased computational overhead. To elaborate, when encountering a time point \(t_{j}\) discretized by the ODE solver during training, the interpolation for obtaining \(_{j}\) and \(_{j}\) in Eq. (6) using Eq. (7) can be immediately executed by identifying the index \(l\) such that \(t_{l}^{}\) is the largest value smaller than \(t_{j}\). However, the interpolation in Eq. (8) cannot be carried out at this time. It becomes feasible only after the ODE solver completes its execution and we gather all \(t_{j}\)'s into \(\). This sequential nature degrades computational efficiency. To mitigate this issue, we have slightly adjusted the interpolation for population statistics during training so that it can be performed concurrently with the parameter interpolation at time \(t_{j}\). The key steps of our proposed TA-BN are summarized in Algorithm 1. It will be called as a subroutine for every discretized time point \(t_{j}\) required by the ODE solver.

Algorithm 1 outlines the case involving a single one-dimensional neuron and one TA-BN layer within the Neural ODE. In more general cases, we may have \(Q\) TA-BN layers within a Neural ODE, and each neuron may possess \(D\) dimensions. In such cases, we can perform the normalization in a dimensional-wise manner. Thus, we must maintain \(2(M+1)Q\) population statistics and \(2(M+1)Q\) learnable parameters, each with \(D\) dimensions. In contrast, a traditional feedforward neural network with \(Q\) BNs has \(2Q\) population statistics and \(2Q\) learnable parameters, each with \(D\) dimensions. Note that for image inputs, we perform channel-wise normalization, so \(D\) will be the number of channels.

Finally, since our linear interpolation in Algorithm 1 relies on two nearest neighbors, it is prudent to ensure that the number of TA-BN time grids \((M+1)\) are roughly at the same level as the number of ODE-discretized time grids. Denser grids would be unnecessary as those not adjacent to any \(t_{j}\) would remain unused. Alternatively, as our numerical results will demonstrate, employing other interpolation methods that require all time grids may lead to increased computational overhead without significant improvements. Additionally, we employ L2 regularization \(\|^{}-1\|^{2}\) and \(\|^{}\|^{2}\) to avoid significant discrepancies among the elements in \(^{}\) and \(\), making the training more stable.

```
1: Batched input \(=\{x_{i,j}\}_{i=1}^{B}\) at time \(t_{j}\)
2: Get \(t_{l}^{}\) and \(t_{l+1}^{}\) such that \(t_{l}^{}\) is the largest value smaller than \(t_{j}\) in \(^{}\) ;
3:\(_{1}=^{}-t_{j}}{t_{l+1}^{}-t_{l}^{}},_ {2}=^{}-t_{l}}{t_{l+1}^{}-t_{l}^{}}\)\(\) Compute the weights for linear interpolation;
4:if the model is in the training mode then
5:\(_{j}=(),\ _{j}^{2}=()\) ; \(\) Compute the mini-batch statistics;
6:\(_{j}=_{1}_{l}^{}+_{2}_{l+1}^{},\ \ \ _{j}=_{1}_{l}^{}+_{2}_{l+1}^{}\) ;
7:\(_{l}^{}(1-_{1})_{l}^{}+ _{1}_{j},\ \ \ _{l+1}^{}(1-_{2})_{l+1}^{}+ _{2}_{j}\) ;
8:\(_{l}^{,2}(1-_{1})_{l}^{,2}+ _{1}_{j}^{2},\ \ \ _{l+1}^{,2}(1-_{2})_{l+1}^{,2}+ _{2}_{j}^{2}\) ;
9:elseif the model is in the evaluation mode then
10:\(_{j}=_{1}_{l}^{}+_{2}_{l+1}^{,2},\ \ \ _{j}^{2}=_{1}_{l}^{,2}+_{2}_{l+1}^{,2}\) ;
11:\(_{j}=_{1}_{l}^{}+_{2}_{l+1}^{},\ \ \ _{j}= _{1}_{l}^{}+_{2}_{l+1}^{}\) ;
12:endif
13:return\(}{^{2}+}}_{j}+_{j}\); ```

**Algorithm 1** The forward pass of a TA-BN layer at time \(t_{j}\)

## 5 Numerical Results

Various network architecture designs have been explored in existing Neural ODE studies. For instance, the original study  integrates a feature extractor (e.g., CNN-based), a Neural ODE module, and an MLP in sequence for classification. Another study structures neural networks with alternating Neural ODE modules and convolutional/linear layers . While these solutions represent viable approaches for optimizing performance on given ML tasks through the synergy of conventional (discrete) neural networks and Neural ODEs, they become inappropriate for investigating the effect of specific architectural modifications (i.e., TA-BN) on Neural ODEs. This is because the Neural ODE module can be entirely bypassed if its trained weights approach zero in extreme cases, thereby overshadowing the intended modification. Hence, our model comprises **a Neural ODE module followed by a single linear layer** as advocated by , which we refer to as the unmixed Neural ODE architecture. Our code is developed based on PyTorch , TorchDiffer , and a customized TA-BN layer. All experiments are run on a Linux server with RTX 3090 GPUs.

### Image Classification

Being consistent in experimental scales of previous Neural ODE studies, we conduct image classification across datasets including MNIST , SVHN , CIFAR-10, CIFAR-100 , and Tiny-ImageNet . We employ the dopri5 solver with a tolerance of \(10^{-3}\) for ODE solving and adopt the AdamW optimizer  with a learning rate of \(10^{-3}\) to train the neural networks for 128 epochs. The training batch size is 256. We set \(M=100\) for TA-BN.

Table 1 compares the test top-1 accuracy and number of parameters of various Neural ODEs. TA-BN outperforms Mini-batch BN and Neural ODE w/o BN on SVHN, CIFAR-10, CIFAR-100, and Tiny-ImageNet. Moreover, TA-BN achieves superior accuracies over Aug-NODE  and STEER . On MNIST and CIFAR-10, TA-BN has fewer parameters than Aug-NODE and STEER, which indicates that TA-BN can also help training in the small Neural ODE regime. We also visualize Neural ODEs' performance changes as the number of parameters varies in Figure 4, by including four additional baselines IL-NODE, 2nd-Ord , HBNODE, and GHBNODE . For reference, the popular convolutional-based MobileNetV2 , known for its parameter efficiency, achieves approximately 94% accuracy with about 2M parameters. Our TA-BN assisted Neural ODE is the first to approach this level of performance using the unmixed architecture, while most previous Neural ODE literature either falls short of this performance or/and does not follow this unmixed architecture.

The left part of Figure 3 in Section 3 monitors the test accuracy over training epochs on CIFAR-10. It indicates that training a large Neural ODE without special processes might fail due to numerical instability. Pop-TI BN performs poorly because of the time-independent statistics assumption. While Mini-batch BN achieves satisfactory accuracy, our TA-BN outperforms it without encountering the issues associated with Mini-batch BN, such as batch-dependent outcomes and vulnerability to outliers. Similarly, the left and middle parts of Figure 5 display test accuracy versus training epochs on the SVHN and CIFAR-100 datasets. TA-BN consistently shows better and more stable accuracy than Mini-batch BN on these datasets. On the Tiny-ImageNet dataset (right part of Figure 5), TA-BN demonstrates superior accuracy and faster convergence.

    &  &  &  &  &  \\  & Accuracy & @Params & Accuracy & @Params & Accuracy & @Params & Accuracy & @Params & Accuracy & @Params \\  Aug-NODE  & 0.982 & 84k & 0.606 & 172k & 0.835 & 172k & N/A & N/A & N/A & 366k \\ STEER  & 0.986 & 84k & 0.621 & 172k & 0.841 & 172k & N/A & N/A & N/A & N/A \\ w/o BN & **0.989\(\)0.001** & 37k & 0.517\(\)0.049 & 2.2M & 0.096\(\)0.025 & 2.2M & 0.246\(\)0.084 & 2.2M & - & 2.2M \\ w/ Pop-TI BN & 0.973\(\)0.011 & 37k & 0.548\(\)0.087 & 2.2M & 0.024\(\)0.123 & 2.2M & 0.251\(\)0.112 & 2.2M & 0.044\(\)0.007 & 2.2M \\ w/ Mini-batch BN & 0.962\(\)0.013 & 37k & 0.822\(\)0.095 & 2.2M & 0.906\(\)0.031 & 2.2M & 0.492\(\)0.176 & 2.2M & 0.200\(\)0.006 & 2.2M \\  w/ TA-BN & 0.988\(\)0.001 & 37k & 0.748\(\)0.059 & 70k & 0.953\(\)0.002 & 220k & 0.576\(\)0.016 & 220k & 0.436\(\)0.013 & 220k \\ (ours) & 0.988\(\)0.001 & 220k & **0.910\(\)0.010** & 2.2M & **0.958\(\)0.004** & 2.2M & **0.664\(\)0.025** & 2.2M & **0.512\(\)0.008** & 2.2M \\   

* \({}^{}\)N/A indicates values are not available in the original literature. \({}^{}\) indicates training failure at the first epoch. Results are reported with a test batch size of 256. The error bars are shown using the format mean:std.

Table 1: Comparison of test accuracies and number of parameters between different Neural ODEs\({}^{}\).

Figure 4: Comparison between different Neural ODEs on CIFAR-10. The baselines marked by yellow triangles do not adhere to the unmixed structure and are not strictly comparable to ours. It is unknown whether increasing the number of parameters inside their ODEs can lead to better accuracy.

Figure 5: Comparison between Neural ODEs w/ TA-BN, w/ Mini-batch BN, w/ Pop-TI BN, and w/o BN, using an 18-layer U-Net backbone for learnable derivatives. The total number of parameters including the outside linear layer is 2.2M. The datasets are SVHN (left), CIFAR-100 (middle), and Tiny-ImageNet (right). The results on CIFAR-10 have been shown in the left part of Figure 3.

Neural ODE Up-scaling Enabled by TA-BNAs shown in the left part of Figure 6, Neural ODEs with a limited number of layers perform normally with acceptable accuracy; however, when the layer count exceeds 10, training fails due to numerical instability. In contrast, the incorporation of TA-BN enables deeper layers within Neural ODE as the learnable derivatives, scaling up the model size and enhancing accuracy, as exemplified by the middle and right sections of Figure 6. Please see Appendix A.1 for architecture details.

Ablations on TA-BNWe integrate TA-BN into Aug-NODE , HBNODE , and SONODE . Table 2 reports model accuracies w/ and w/o TA-BN. It indicates that our method can also work in compatible with existing Neural ODE variants and boost their performances.

To show the impact of different interpolation functions \(G\), we test the following interpolation methods: (1) Linear interpolation demonstrated by Eq. (7). (2) Cubic interpolation: We use cubic spline interpolation . (3) Kernel smoothing: Given time \(t\), we calculate \(G(t,,)\) by the weighted sum of elements in \(\). The weight coefficient for \(t_{i}\) is \(K(t,t_{i})=(-0.5b^{-2}(t-t_{i})^{2})\) with \(b=0.1T\). (4) Gaussian process: We fit a Gaussian process  to perform interpolation in each forward pass. As illustrated by Figure 7, in addition to linear interpolation, the cubic interpolation and kernel smoothing methods also achieve good performance. However, these methods can be much slower than linear interpolation due to more complicated interpolation mechanisms. With a larger network architecture, we observe that these methods suffer from unaffordable running time and instable performance. Thus, we adopt linear interpolation as our default setting. Please see Appendix B for further discussion. Future work can focus on improving the interpolation strategy for TA-BN.

### Physical System Modeling

We first evaluate TA-BN on physical dynamical system modeling tasks using the Walker2d-v2  and HalfCheetah-v2  datasets. These datasets consist of trajectories of 3D robot systems generated by the Mujoco physics engine . Following , we perform temporal autoregressive prediction and use the treatments from  to avoid collisions. We employ TA-BN in conjunction with a 12-layer MLP serving as the backbone for learnable derivatives. TA-BN with \(M=100\) is applied to the first 5 layers, as is done with Mini-batch BN and Pop-TI BN. We utilize the dopri5 solver with its default settings for solving ODEs and employ the RMSprop optimizer with a learning rate of \(10^{-3}\) for training the neural networks over 100 epochs. The batch size is 64 for Walker2d-v2 and 32 for HalfCheetah-v2. Figure 8 presents the mean absolute error (MAE) along the training epochs on these datasets. Compared with Neural ODEs w/o BN, w/ Mini-batch BN, and w/ Pop-TI BN, using TA-BN can achieve lower errors and faster convergence. Moreover, Table 3 summarizes the MAE results and compares them with the results in .

   Accuracy & Aug-NODE & HBNODE & SONODE \\  w/o TA-BN & 0.848 & 0.851 & 0.302 \\ w/ TA-BN & **0.862** & **0.867** & **0.853** \\    \({}^{}\) We use 6 convolution layers as the backbone for learnable derivatives.

Table 2: Comparison between Neural ODE variations with and without TA-BN on CIFAR-10\({}^{}\).

Figure 6: CIFAR-10 accuracies with increasing sizes of the backbones for learnable derivatives. These figures illustrate the scaling up of Neural ODEs without BN (left) and Neural ODEs with TA-BN (middle). We also compare the accuracies of these two settings in one figure (right).

Figure 7: Ablations on interpolation method using CIFAR-10 with 6 convolution layers as the backbone for learnable derivatives. Linear interpolation has a gap of only \(0.01\) compared to the best result. However, other methods have degraded accuracies and unaffordable runtime when we up-scale the model.

Additionally, we investigate the effectiveness of TA-BN by modeling the temporal electrical current of a Charge Pump (CP) circuit . The CP is an analog integrated circuit composed of MOS transistors and is commonly utilized in commercial electrical products. To simulate the semiconductor manufacturing impact on the CP, we generate 200 circuit instances based on the process design kit (PDK) file. Concretely, we simulate the CP's behavior from 0 seconds to 200 nanoseconds using a time grid of 2 picoseconds and record the electrical currents at the CP output every 200 picoseconds.

In this experiment, we utilize 16 circuit features (representing the widths of the MOS transistors) and the electrical currents at the present 20 time points (with each point comprising 2 values) as the Neural ODE input, resulting in 56 input features in total. Our objective is to train the Neural ODE to predict the electrical currents at the subsequent 20 time points, yielding 40 outputs in total. To construct the dataset, we randomly sample 20k data from the simulation results. The training set includes 90% of them, and the remaining data are used as the testing set. We use 8 linear layers to parameterize the derivative of the Neural ODE, with each layer containing 56 neurons. Mini-batch BN, Pop-TI BN, and TA-BN are applied to the first 3 layers. We use \(M=100\) for TA-BN. The final linear layer outside the Neural ODE maps the 56 features to 40 prediction values. For ODE solving, we use the dopri5 method with a tolerance level of \(10^{-3}\). We use the SGD optimizer with a learning rate of \(10^{-2}\) to train the model for 16 epochs. The batch size is 200.

Figure 9 presents the mean square error (MSE) results of Neural ODEs w/o BN, w/ Mini-batch BN, w/ Pop-TI BN, and w/ TA-BN. BN can improve the prediction, and TA-BN surpasses Mini-batch BN. It indicates that TA-BN is suitable for the circuit modeling task.

## 6 Conclusions and Limitations

In this paper, we demystify the previously unknown reason why batch normalization (BN) may lead to performance degradation when used with Neural ODEs. The fundamental mismatch arises from BN being initially designed for discrete neural networks without a temporal dimension, whereas Neural ODEs operate continuously over time. To address this challenge, we propose temporal adaptive batch normalization (TA-BN), specifically tailored for Neural ODEs. Our key is associating population statistics and learnable parameters with predefined regular time grids, with temporal interpolation automatically performed during training and testing. As a continuous-time counterpart to traditional BN, TA-BN ensures training stability, thereby facilitating the scaling of Neural ODE model sizes. Our extensive experiments demonstrate TA-BN's ability to enhance the performance of Neural ODEs compared to baseline methods in tasks such as image classification and physical system modeling.

The interpolation process used in TA-BN inevitably introduces runtime overhead, which slows down the execution of Neural ODEs. The parameter-free linear interpolation technique is empirically found to be stable, time-efficient, and capable of providing performance improvement. However, future work can focus on enhancing the temporal interpolation used in TA-BN to further optimize its efficiency and performance.