# Locality-Aware

Generalizable Implicit Neural Representation

 Doyup Lee

Kakao Brain

doyup.lee@kakakobrain.com

&Chiheon Kim

Kakao Brain

chiheon.kim@kakobrain.com

&Minsu Cho

POSTECH

mscho@postech.ac.kr

&Wook-Shin Han

POSTECH

wshan@dblab.postech.ac.kr

Equal contributionCorresponding authors

###### Abstract

Generalizable implicit neural representation (INR) enables a single continuous function, i.e., a coordinate-based neural network, to represent multiple data instances by modulating its weights or intermediate features using latent codes. However, the expressive power of the state-of-the-art modulation is limited due to its inability to localize and capture fine-grained details of data entities such as specific pixels and rays. To address this issue, we propose a novel framework for generalizable INR that combines a transformer encoder with a locality-aware INR decoder. The transformer encoder predicts a set of latent tokens from a data instance to encode local information into each latent token. The locality-aware INR decoder extracts a modulation vector by selectively aggregating the latent tokens via cross-attention for a coordinate input and then predicts the output by progressively decoding with coarse-to-fine modulation through multiple frequency bandwidths. The selective token aggregation and the multi-band feature modulation enable us to learn locality-aware representation in spatial and spectral aspects, respectively. Our framework significantly outperforms previous generalizable INRs and validates the usefulness of the locality-aware latents for downstream tasks such as image generation.

## 1 Introduction

Recent advances in generalizable implicit neural representation (INR) enable a single coordinate-based multi-layer perceptron (MLP) to represent multiple data instances as a continuous function. Instead of per-sample training of individual coordinate-based MLPs, generalizable INR extracts latent codes of data instances  to modulate the weights or intermediate features of the shared MLP model . However, despite the advances in previous approaches, their performance is still insufficient compared with individual training of INRs per sample.

We postulate that the expressive power of generalizable INRs is limited by the ability of _locality-awareness_ to localize relevant entities from a data instance and control

Figure 1: Learning curves of PSNRs during training on ImageNette 178\(\)178.

their structure in a fine-grained manner. Primitive entities of a data instance, such as pixels in an image, tend to have a higher correlation with each other if they are closer in space and time. Thus, this locality of data entities has been used as an important inductive bias for learning the representations of complex data . However, previous approaches to generalizable INRs are not properly designed to leverage the locality of data entities. For example, when latent codes modulate intermediate features [11; 12] or weight matrices [8; 19; 35] of an INR decoder, the modulation methods do not exploit a specified coordinates for decoding, which restricts the latent codes to encoding global information over all pixels without capturing local relationships between specific pixels.

To address this issue, we propose a novel encoder-decoder framework for _locality-aware_ generalizable INR to effectively localize and control the fine-grained details of data. In our framework, a Transformer encoder  first extracts locally relevant information from a data instance and predicts a set of latent tokens to encode different local information. Then, our locality-aware INR decoder effectively leverages the latent tokens to predict fine-grained details. Specifically, given an input coordinate, our INR decoder uses a cross-attention to selectively aggregate the local information in the latent tokens and extract a modulation vector for the coordinate. In addition, our INR decoder effectively captures the high-frequency details in the modulation vector by decomposing it into multiple bandwidths of frequency features and then progressively composing the intermediate features. We conduct extensive experiments to demonstrate the high performance and efficacy of our locality-aware generalizable INR on benchmarks as shown in Figure 1. In addition, we show the potential of our locality-aware INR latents to be utilized for downstream tasks such as image synthesis.

Our main contributions can be summarized as follows: 1) We propose an effective framework for generalizable INR with a Transformer encoder and locality-aware INR decoder. 2) The proposed INR decoder with selective token aggregation and multi-band feature modulation can effectively capture the local information to predict the fine-grained data details. 3) The extensive experiments validate the efficacy of our framework and show its applications to a downstream image generation task.

## 2 Related Work

Implicit neural representations (INRs).INRs use neural networks to represent complex data such as audio, images, and 3D scenes, as continuous functions. Especially, incorporating Fourier features [24; 36], periodic activations , or multi-grid features  significantly improves the performance of INRs. Despite its broad applications [1; 6; 10; 32; 34], INRs commonly require separate training of MLPs to represent each data instance. Thus, individual training of INRs per sample does not learn common representations in multiple data instances.

Generalizable INRs.Previous approaches focus on two major components for generalizable INRs; latent feature extraction and modulation methods. Auto-decoding [23; 26] computes a latent vector per data instance and concatenates it with the input of a coordinate-based MLP. Given input data, gradient-based meta-learning [4; 11; 12] adapts a shared latent vector using a few update steps to scale and shift the intermediate activations of the MLP. Learned Init  also uses gradient-based meta-learning but adapts whole weights of the shared MLP. Although auto-decoding and gradient-based meta-learning are agnostic to the types of data, their training is unstable on complex and large-scale datasets. TransINR  employs the Transformer  as a hypernetwork to predict latent vectors to modulate the weights of the shared MLP. In addition, Instance Pattern Composers  have demonstrated that modulating the weights of the second MLP layer is enough to achieve high performance of generalizable INRs. Our framework also employs the Transformer encoder, but focuses on extracting locality-aware latent features for the high performance of generalizable INR.

Leveraging Locality of Data for INRsLocal information in data has been utilized for efficient modeling of INRs, since local relationships between data entities are widely used for effective process of complex data . Given an input coordinate, the coordinate-based MLP only uses latent vectors nearby the coordinate, after a CNN encoder extracts a 2D grid feature map of an image for super-resolution  and reconstruction . Spatial Functa  demonstrates that leveraging the locality of data enables INRs to be utilized for downstream tasks such as image recognition and generation. Local information in 3D coordinates has also been effective for scene modeling as a hybrid approach using 3D feature grids  or the part segmentation  of a 3D object. However, previous approaches assume explicit grid structures of latents tailored to a specific data type. Sincewe do not predefine a relationship between latents, our framework is flexible to learn and encode the local information of both grid coordinates in images and non-grid coordinates in light fields.

## 3 Methods

We propose a novel framework for _locality-aware generalizable INR_ which consists of a Transformer encoder to localize the information in data into latent tokens and a locality-aware INR decoder to exploit the localized latents and predict outputs. First, we formulate how generalizable INR enables a single coordinate-based neural network to represent multiple data instances as a continuous function by modulating its weights or features. Then, after we introduce the Transformer encoder to extract a set of latent tokens from input data instances, we explain the details of the locality-aware INR decoder, where _selective token selection_ aggregates the spatially local information for an input coordinate via cross-attention; _multi-band feature modulation_ leverages a different range of frequency bandwidths to progressively decode the local information using coarse-to-fine modulation in the spectral domain.

### Generalizable Implicit Neural Representation

Given a set of data instances \(=\{^{(n)}\}_{n=1}^{N}\), each data instance \(^{(n)}=\{(^{(n)}_{i},^{(n)}_{i})\}_{i=1}^{M_{n}}\) comprises \(M_{n}\) pairs of an input coordinate \(^{(n)}_{i}^{d_{n}}\) and the corresponding output feature \(^{(n)}_{i}^{d_{}}\). Conventional approaches [24; 31; 36] adopt individual coordinate-based MLPs to train and memorize each data instance \(^{(n)}\). Thus, the coordinate-based MLP cannot be reused and generalized to represent other data instances, requiring per-sample optimization of MLPs for unseen data instances.

A generalizable INR uses a single coordinate-based MLP as a shared INR decoder \(F_{}:^{d_{}}^{d_{}}\) to represent multiple data instances as a continuous function. Generalizable INR [8; 11; 12; 19; 26] extracts the \(R\) number of latent codes \(^{(n)}=\{^{(n)}_{k}^{d}\}_{k=1}^{R}\) from a data instance \(^{(n)}\). Then, the latents are used for the INR decoder to represent a data instance \(^{(n)}\) as \(^{(n)}_{i}=F_{}(^{(n)}_{i};^{(n)})\), while updating the parameters \(\) and latents \(^{(n)}\) to minimize the errors over \(\):

\[_{,^{(n)}}}_{n=1}^{N}_{i=1}^{M_{n}} \|^{(n)}_{i}-F_{}(^{(n)}_{i};^{(n)} )\|_{2}^{2}.\] (1)

We remark that each previous approach employs a different number of latent codes to modulate a coordinate-based MLP. For example, a single latent vector (\(R=1\)) is commonly extracted to modulate intermediate features of the MLP [11; 12; 26], while a multitude of latents (\(R>1\)) are used to modulate its weights [8; 19; 35]. While we modulate the features of MLP, we extract a set of latent codes to localize the information of data to leverage the locality-awareness for latent features.

Figure 2: Overview of our framework for locality-aware generalizable INR. Given a data instance, Transformer encoder extracts its localized latents. Then, the locality-aware INR decoder uses selective token aggregation and multi-band feature modulation to predict the output for the input coordinate.

### Transformer Encoder

Our framework employs a Transformer encoder  to extract a set of latents \(^{(n)}\) for each data instance \(^{(n)}\) as shown in Figure 2. After a data instance, such as an image or multi-view images, is patchified into a sequence of data tokens, we concatenate the patchified tokens into a sequence of \(R\) learnable tokens as the encoder input. Then, the Transformer encoder extracts a set of latent tokens, where each latent token corresponds to an input learnable token. Note that the permutation-equivariance of self-attention in the Transformer encoder enables us not to predefine the local structure of data and the ordering of latent tokens. During training, each latent token learns to capture the local information of data, while covering whole regions to represent a data instance. Thus, whether a data instance is represented on a grid or non-grid coordinate, our framework is flexible to encode various types of data into latent tokens, while learning the local relationships of latent tokens during training.

### Locality-Aware Decoder for Implicit Neural Representations

We propose the locality-aware INR decoder in Figure 2 to leverage the local information of data for effective generalizable INR. Our INR decoder comprises two primary components: i) _Selective token aggregation via cross attention_ extracts a modulation vector for an input coordinate to aggregate spatially local information from latent tokens. ii) _Multi-band feature modulation_ decomposes the modulation vector into multiple bandwidths of frequency features to amplify the high-frequency features and effectively predict the details of outputs.

#### 3.3.1 Selective Token Aggregation via Cross-Attention

We remark that encoding locality-aware latent tokens is not straightforward since the self-attentions in Transformer do not guarantee a specific relationship between tokens. Thus, the properties of the latent tokens are determined by a modulation method for generalizable INR to exploit the extracted latents. For example, given an input coordinate \(\) and latent tokens \(\{_{1},...,_{R}\}\), a straightforward method can use Instance Pattern Composers  to construct a modulation weight \(_{}=[_{1},...,_{R}]^{}^{R d_{}}\) and extract a modulation vector \(_{}=_{}=[_{}^{},...,_{R}^{}]^{}^{R}\). However, the latent tokens cannot encode the local information of data, since each latent token equally influences each channel of the modulation vector regardless of the coordinate locations (see Section 4.3).

Our selective token aggregation employs cross-attention to aggregate the spatially local latents nearby the input coordinate, while guiding the latents to be locality-aware. Given a set of latent tokens \(^{(n)}=\{_{k}^{(n)}\}_{k=1}^{R}\) and a coordinate \(_{i}^{(n)}\), a modulation feature vector \(_{_{i}}^{(n)}^{d}\) shifts the intermediate features of an INR decoder to predict the output, where \(d\) is the dimensionality of hidden layers in the INR decoder. For the brevity of notation, we omit the superscript \(n\) and subscript \(i\).

Frequency featuresWe first transform an input coordinate \(=(v_{1},,v_{d_{}})^{d_{}}\) into frequency features using sinusoidal positional encoding . We define the Fourier features \(_{}()^{d_{}}\) with bandwidth \(>1\) and feature dimensionality \(d_{}\) as

\[_{}()=[(_{j}v_{i}),(_{j}v_{i} ):i=1,,d_{},j=0,,n-1]\] (2)

where \(n=}}{2d_{}}\). A frequency \(_{j}=^{j/(n-1)}\) is evenly distributed between 1 and \(\) on a log-scale. Based on the Fourier features, we define the _frequency feature_ extraction \(_{}()\) as

\[_{}(;,,)= (_{}()+),\] (3)

where \(^{d d_{}}\) and \(^{d}\) are trainable parameters for frequency features, \(d\) denotes the dimensionality of hidden layers in the INR decoder.

Selective token selection via cross-attentionTo predict corresponding output \(\) to the coordinate \(\), we adopt a cross-attention to extract a modulation feature vector \(_{}^{d}\) based on the latent tokens \(=\{_{k}\}_{k=1}^{R}\). We first extract the frequency features of the coordinate \(\) in Eq (3) as the query of the cross-attention as

\[_{}:=_{}(;_{}, _{},_{}),\] (4)

where \(_{}^{d d_{}}\) and \(_{}^{d}\) are trainable parameters, and \(_{}\) is the bandwidth for query frequency features. The cross-attention in Figure 2 enables the query to select latent tokens, aggregateits local information, and extract the modulation feature vector \(}\) for the input coordinate:

\[}:=(=},=,=).\] (5)

An intuitive implementation for selective token aggregation can employ hard attention to select only one latent token for each coordinate. However, in our primitive experiment, using hard attention leads to unstable training and a latent collapse problem that selects only few latent tokens. Meanwhile, multi-head attentions encourage each latent token to easily learn the locality in data instances.

#### 3.3.2 Multi-Band Feature Modulation in the Spectral Domain

After the selective token aggregation extracts a modulation vector \(}\), we use multi-band feature modulation to effectively predict the details of outputs. Although Fourier features  reduce the spectral bias  of neural networks, adopting a simple stack of MLPs to INRs still suffers from capturing the high-frequency data details. To address this issue, we use a different range of frequency bandwidths to decompose the modulation vector into multiple frequency features in the spectral domain. Then, our multi-band feature modulation uses the multiple frequency features to progressively decode the intermediate features, while encouraging a deeper MLP path to learn higher frequency features. Note that the coarse-to-fine approach in the spectral domain is analogous to the locally hierarchical approach in the spatial domain  to capture the data details.

Extracting multiple modulation features with different frequency bandwidthsWe extract \(L\) level of modulation features \(_{}^{(1)},,_{}^{(L)}\) from \(}\) using different bandwidths of frequency features. Given \(L\) frequency bandwidths as \(_{1}_{2}_{L}_{}\), we use Eq (3) to extract the \(\)-th level of frequency features of an input coordinate \(\) as

\[(_{})_{}^{()}:=_{}(;_{},_{}^{()},_{}^{()})= (_{}^{()}_{_ {}}()+_{}^{()}),\] (6)

where \(_{}^{()}\) and \(_{}^{()}\) are trainable parameters and shared across data instances. Then, the \(\)-th modulation vector \(_{}^{()}\) is extracted from the modulation vector \(_{}\) as

\[_{}^{()}:=((_{})_{ }^{()}+_{}^{()}_{}+ _{}^{()}),\] (7)

with a trainable weight \(_{}^{()}\) and bias \(_{}^{()}\). Considering that \(\) cutoffs the values below zero, we assume that \(_{}^{()}\) filters out the information of \(_{}\) based on the \(\)-th frequency patterns of \((_{})_{}^{()}\).

Multi-band feature modulationAfter decomposing a modulation vector into multiple features with different frequency bandwidths, we progressively compose the \(L\) modulation features by applying a stack of nonlinear operations with a linear layer and ReLU activation. Starting with \(_{}^{(1)}=_{}^{(1)}\), we compute the \(\)-th hidden features \(_{}^{()}\) for \(=2,,L\) as

\[}_{}^{()}:=_{}^{() }+_{}^{(-1)}_{}^{()}:=(^{()}}_{ }^{()}+^{()}),\] (8)

where \(^{()}^{d d}\) and \(^{()}^{d}\) are trainable weights and biases of the INR decoder. \(}_{}^{()}\) denotes the \(\)-th pre-activation of INR decoder for coordinate \(\). Note that the modulation features with high-frequency bandwidth can be processed by more nonlinear operations than the features with lower frequency bandwidths, considering that high-frequency features contain more complex signals.

Finally, the output \(}\) is predicted using all intermediate hidden features of the INR decoder as

\[}:=_{=1}^{L}f_{}^{()}(_{ }^{()}),\] (9)

where \(f_{}^{()}:^{d}^{d_{}}\) are a linear projection into the output space. Although utilizing only \(_{}^{(L)}\) is also an option to predict outputs, skip connections of all intermediate features into the output layer enhances the robustness of training to the hyperparameter choices.

[MISSING_PAGE_FAIL:6]

high-quality result of reconstructed images. The results demonstrate that leveraging the locality of data is crucial for generalizable INR to model complex and high-resolution data.

### Few-Shot Novel View Synthesis

We evaluate our framework on novel view synthesis with the ShapeNet Chairs, Cars, and Lamps datasets to predict a rendered image of a 3D object under an unseen view. Given few views of an object with known camera poses, we employ a light field  for novel view synthesis. A light field does not use computationally intensive volume rendering  but directly predicts RGB colors for the input coordinate for rays with \(d_{}=6\) using the Plucker coordinate system. Our INR decoder uses \(d=256\) and two levels of feature modulations with \(_{}=2\) and \((_{1},_{2})=(8,4)\).

Figure 4(a) shows that our framework outperforms IPC for novel view synthesis. Our framework shows competitive performance with IPC when only one support view is provided. However, the performance of our framework is consistently improved as the number of support views increases, while outperforming the results of IPC. Note that defining a local relationship between rays is not straightforward due to its non-grid property of the Plucker coordinate. Our Transformer encoder can learn the local relationship between rays to extract locality-aware latent tokens during training and achieve high performance. We analyze the learned locality of rays encoded in the extracted latents in Section 4.3. Figure 4(b) shows that our framework correctly predicts the colors and shapes of a novel view corresponding to the support views, although the predicted views are blurry due to the lack of training objectives with generative modeling. We expect that combining our framework with generative models [5; 38] to synthesize a photorealistic novel view is an interesting future work.

### In-Depth Analysis

Learning Curves on ImageNette 178\(\)178Figure 1 juxtaposes the learning curves of our framework and previous approaches on ImageNette 178\(\)178. Note that TransINR, IPC, and our framework use the same Transformer encoder to extract data latents, while adopting different modulation methods. While the training speed of our framework is about 80% of the speed of IPC, we remark our framework achieves the test PSNR of 38.72 after 400 epochs of training, outperforming the PSNR of 38.46 achieved by IPC trained for 4000 epochs, hence resulting in \(8\) speed-up of training time. That is, our locality-aware latents enables generalizable INR to be both efficient and effective.

Selective token aggregation and multi-band feature modulationsWe conduct an ablation study on image reconstruction of with ImageNette 178\(\)178 and FFHQ 256\(\)256, novel view synthesis with Lamp-3 views to validate the effectiveness of the selective token aggregation and the multi-band feature modulation. We replace the multi-band feature modulations with a simple stack of MLPs (ours w/o multiFM), and the selective token aggregation with the weight modulation of IPC (ours w/o STA). If both two modules are replaced together, the INR decoder becomes the same

Figure 4: (a) PSNRs on novel view synthesis of ShapeNet Chairs, Cars, and Lamps according to the number of support views (1-5 views). (b) Examples of novel view synthesis with 4 support views.

   & ImageNette & FFHQ & Lamp \\  Ours & **37.46** & **38.01** & **26.00** \\ w/o STA & 34.54 & 34.52 & 25.31 \\ w/o multiFM & 33.90 & 33.65 & 25.78 \\  IPC  & 34.11 & 34.68 & 25.09 \\   

Table 3: Ablation study on ImageNette 178\(\)178, FFHQ 256\(\)256, and Lamp-3 views.

architecture as IPC. We use single-head cross-attention for the selective token aggregation to focus on the effect of two modules. Table 3 demonstrates that both the selective token aggregation and the multi-band feature modulation are required for the performance improvement, as there is no significant improvement when only one of the modules is used.

Choices of frequency bandwidthsTable 4 shows that the ordering of frequency bandwidths in Eq. (4) and Eq. (6) can affect the performance. We train our framework with two-level feature modulations on ImageNet 178\(\)178 during 400 epochs with different settings of the bandwidths \(_{1},_{2},_{}\). Although our framework outperforms IPC regardless of the bandwidth settings, the best PSNR is achieved with \(_{1}_{2}_{}\). The results imply that selective token aggregation does not require high-frequency features, but the high-frequency features need to be processed by more nonlinear operations than lower-frequency features as discussed in Section 3.3.2.

The role of extracted latent tokensFigure 5 shows that our framework encodes the local information of data into each latent token, while IPC cannot learn the locality in data coordinates. To visualize the information in each latent token, we randomly select a latent token to be replaced with the zero vector. Then, we visualize the difference between the model predictions with or without the replacement. Each latent token of our framework encapsulates the local information in different regions of images and light fields. However, the latent tokens of IPC cannot exploit the local information of data, while encoding the global information over whole coordinates. Note that our framework _learns_ the structure of locality in light fields during training, although the structure of the Plucker coordinate system is not regular as the grid coordinates of images. Thus, our framework can learn the locality-aware latents of data for generalizable INR regardless of the types of coordinate systems.

### Generating INRs for Conditional Image Synthesis

We examine the potentials of the extracted latent tokens to be utilized for a downstream task such as class-conditional image generation of ImageNet . Note that we cannot use the architecture of U-Net in conventional image diffusion models [4; 30], since our framework is not tailored to the 2D grid coordinate. Thus, we adopt a Transformer-based diffusion model [15; 27] to predict a set of latent tokens after corrupting the latents by Gaussian noises.

  \((_{1},_{2})\) & \(_{}\) & ImageNette \\  (128, 32) & 16 & **37.46** \\ (32, 128) & 16 & 35.00 \\ (128, 128) & 16 & 35.30 \\ (128, 32) & 128 & 35.58 \\   & 34.11 \\  

Table 4: PSNRs of reconstructed ImageNette 178\(\)178 with various frequency bandwidths.

Figure 5: Visualization of differences between model predictions after replacing a latent token with the zero vector, for IPC  and our framework.

We train 458M parameters of Transformers during 400 epochs to generate our locality-aware latent tokens. We attach the detailed setting in Appendix A.3. When we train a diffusion model to generate latent tokens of IPC in Figure 6, the generated images suffer from severe artifacts, because the prediction error of each latent token for IPC leads to the artifacts over all coordinates. Contrastively, the diffusion model for our locality-aware latents generates realistic images. In addition, although we do not conduct exhaustive hyperparamter search, the FID score of generated images achieves 9.3 with classifier-free guidance scale  in Table 5. Thus, the results validate the potential applications of the local latents for INRs. Meanwhile, a few generated images may exhibit checkerboard artifacts, particularly in simple backgrounds, but we leave the elaboration of a diffusion process and sampling techniques for generating INR latents as future work.

### Comparison with Overfitted INRs

Figure 7 shows that our generalizable INR efficiently provides meaningful INRs compared with individual training of INRs per sample. To evaluate the efficiency of our framework, we select ten images of FFHQ 256\(\)256 and train randomly initialized FFNet  per sample using one NVIDIA V100 GPU. The individual training of FFNets requires over 10 seconds of optimization to achieve the same PSNRs of our framework, where our inference time is negligible. Moreover, when we apply the test-time optimization (TTO) only for the extracted latents, it consistently outperforms per-sample FFNets for 30 seconds while maintaining the structure of latents. When we consider the predicted INR as initialization and finetune all parameters of the INR decoder per each sample, our framework consistently outperforms the per-sampling training of INRs from random initialization. Thus, the results imply that leveraging generalizable INR is computationally efficient to model unseen data as INRs regardless of a TTO.

## 5 Conclusion

We have proposed an effective framework for generalizable INR with the Transformer encoder and locality-aware INR decoder. The Transformer encoder capture the locality of data entities and learn to encode the local information into different latent tokens. Our INR decoder selectively aggregates the locality-aware latent tokens to extract a modulation vector for a coordinate input and exploits the multiple bandwidths of frequency features to effectively predict the fine-grained data details. Experimental results demonstrate that our framework significantly outperforms previous generalizable

Figure 6: The examples of generated 256\(\)256 images by generating latents of IPC (left) and ours (right), trained on ImageNet.

   & Latent Shape & rPSNR & FID \\  Ours & 256\(\)256 & 37.7 & 9.3 \\  Spatial & 16\(\)16\(\)256 & 37.2 & 11.7 \\ Funeta  & 32\(\)32\(\)64 & 37.7 & 8.8 \\  LDM  & 64\(\)64\(\)3 & 27.4 & 3.6 \\  

Table 5: Reconstructed PSNRs and FID of generated images on ImageNet 256\(\)256.

Figure 7: Comparison with individually trained FFNets  per sample.

INRs on image reconstruction and few-shot novel view synthesis. In addition, we have conducted the in-depth analysis to validate the effectiveness of our framework and shown that our locality-aware latent tokens for INRs can be utilized for downstream tasks such as image generation to provide realistic images. Considering that our framework can learn the locality in non-grid coordinates, such as the Plucker coordinate for rays, leveraging our generalizable INR to generate 3D objects or scenes is a worth exploration. In addition, extending our framework to support arbitrary resolution will be an interesting future work. Furthermore, since our framework has still room for performance improvement of high-resolution image reconstruction, such as 1024\(\)1024, we expect that elaborating on the architecture and techniques for diffusion models to effectively generate INRs is an interesting future work.

## 6 Acknowledgements

This work was supported by Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No.2018-0-01398: Development of a Conversational, Self-tuning DBMS, 35%; No.2022-0-00113: Sustainable Collaborative Multimodal Lifelong Learning, 30%) and the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. NRF-2021R1A2B5B03001551, 35%)