# Make Pre-trained Model Reversible:

From Parameter to Memory Efficient Fine-Tuning

 Baohao Liao   Shaomu Tan   Christof Monz

Language Technology Lab, University of Amsterdam

{b.liao, s.tan, c.monz}@uva.nl

###### Abstract

Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT method. With this finding, we propose memory-efficient fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's starting point and making it reversible without additional pre-training. We evaluate MEFT on the GLUE benchmark and five question-answering tasks with various backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to 84% of full fine-tuning with a negligible amount of trainable parameters. Moreover, MEFT achieves the same score on GLUE and a comparable score on the question-answering tasks as full fine-tuning. A similar finding is also observed for the image classification task.1

## 1 Introduction

Large-scale pre-trained models have achieved great success across various domains and applications [1; 2; 3; 4; 5; 6; 7; 8]. As their capabilities continue to evolve, the released pre-trained language models (PLMs) have grown exponentially in size, even reaching a scale of 100 billion parameters [3; 9; 10; 11; 12]. Consequently, it presents unprecedented challenges in effectively leveraging these models for downstream tasks due to limited computing resources.

A historically common approach to adapting PLMs to downstream tasks is updating all pre-trained parameters, _full fine-tuning_. Although full fine-tuning has yielded numerous state-of-the-art results, its applicability is limited in storage-constrained environments. This constraint arises from maintaining a complete copy of the fine-tuned model for each task. An alternative

Figure 1: Average performance of different tasks vs. activation memory. The memory usage for full fine-tuning is denoted as 100%.

adaptation approach is _parameter-efficient fine-tuning_ (PEFT) [13; 14; 15; 16; 17; 18; 19] which involves selectively updating a small number of task-specific parameters while keeping the majority of the PLM's parameters frozen. PEFT offers significant advantages in reducing storage requirements by only saving one general PLM alongside the modified parameters for each task. In addition to storage savings, PEFT achieves comparable performance to full fine-tuning, sparking considerable interest in the adoption of PEFT.

Despite their advantages in parameter efficiency, existing PEFT methods still face challenges in terms of memory efficiency [20; 21]. PEFTs necessitate the caching of intermediate activations, similar to the requirements of full fine-tuning, to calculate the gradients of the trainable parameters. Typically, they consume more than 70% activation memory of full fine-tuning (see Figure 1). Since activations significantly contribute to the memory requirements during training, there are instances where fine-tuning a large-scale PLM with PEFT is not feasible due to memory constraints. To address this issue, a commonly employed approach is to treat the PLM as a feature extractor, such as knowledge distillation to a smaller model [22; 23], adding additional trainable layers on top  or aligned [21; 24] with it, and so on. These approaches circumvent the need to store the PLM's activations since the gradient computation graph does not traverse through the PLM. However, these methods often require additional pre-training or exhibit a substantial performance gap compared to full fine-tuning when using the same underlying model [20; 21].

In this paper, we propose a novel method called _memory-efficient fine-tuning_ (MEFT) to modify PLMs in a parameter- and memory-efficient manner, without requiring additional pre-training. Initially, we investigate a crucial factor for the success of existing PEFT methods and determine that the proper initialization of newly added parameters is essential to maintain the continuity of information from the PLM (SS2). Leveraging this insight, we design three MEFT methods that enable the modification of a PLM to its reversible variant, so it only necessitates caching the final output and allows for the recomputation of intermediate activations during back-propagation (SS3). Consequently, MEFT significantly reduces the memory required for caching activations (see Figure 1).

To validate the effectiveness of our MEFT methods, we conduct extensive evaluations on the GLUE benchmark  with BERT , RoBERTa  and BART  (SS4). The experimental results consistently demonstrate that our MEFT methods outperform both full fine-tuning and strong PEFT baselines in terms of parameter and memory efficiency. Remarkably, our methods achieve the same score as full fine-tuning while updating only 0.2% of the parameters and saving up to 84% of the activation memory. Furthermore, we evaluate MEFT on five question-answering tasks with a larger model, OPT . The results show that our approach achieves a comparable score as full fine-tuning while saving 50% of the activation memory and updating only 0.64% of the parameters. A similar finding is also observed on the image classification task, SVHN . Collectively, these experiments establish the effectiveness of MEFT as a powerful parameter- and memory-efficient approach that does not compromise performance.

## 2 Preliminaries

In this section, we aim to provide essential background knowledge by addressing the following questions: (1) Why are existing PEFTs not sufficiently memory-efficient (SS2.1)? (2) What is a key factor for the success of PEFT (SS2.2)? (3) What challenges does a reversible model have (SS2.3)?

### Parameter-efficient fine-tuning is not sufficiently memory-efficient

Given a \(N\) multilayer perception: \(_{N}=f_{N}(f_{N-1}(...(f_{2}(f_{1}(_{0})))...))\) with \(_{0}\) as the initial input, the \(n^{th}\) layer \(_{n}=f_{n}(_{n-1})=_{n}(_{n}_{n-1})\) consists of a nonlinear function \(_{n}\) and a weight matrix \(_{n}\), where the bias term is ignored for simplicity. Denoting \(_{n}=_{n}_{n-1}\), in backpropagation with a loss \(\), the gradient of \(_{n}\) is calculated with the chain rule as:

\[}{_{n}}=}{ _{N}}(_{i=n+1}^{N}_{i}}{_{i}}_{i}}{_{i-1}})_{n }}{_{n}}_{n}}{_{n}}=}{_{N}}(_{i=n+1}^{N}^{}_ {i}_{i})^{}_{n}_{n-1}\] (1)

where \(^{}\) is the derivative of \(\) and the calculation of \(^{}_{n}\) requires \(_{n}\). Therefore, \(\{_{i}\}_{i=n}^{N}\) are cached during the forward pass to obtain the gradient of \(_{n}\), even though \(\{_{i}\}_{i>n}\) are frozen.

During training, the peak memory footprint is mainly occupied by three components: model's parameters \(\{_{n}\}_{n=1}^{N}\), optimizer state whose size is three times as large as the size of trainable parameters for Adam  (one for gradient and two for moments), and activations. The memory footprint for all three components is related to the model's depth and width. In addition, the memory footprint for activations is also related to some training settings, like batch size and sequence length.

Compared to full fine-tuning, existing PEFT methods, such as (Houlsby and Pfeiffer) Adapters , LoRA , (IA)3, Prompt-Tuning  and Prefix-Tuning , tune a small number of parameters, making the size of the optimizer state negligible. However, the memory footprint required for activations is not significantly reduced. As shown in Figure 1(a), where we set the batch size as 64 and the sequence length as 512 on RTE  with BERT\({}_{}\), the activation memory of all PEFT methods is >75% of full fine-tuning, even with <1% trainable parameters.

### Initialization is significant for parameter-efficient fine-Tuning

Pre-trained models learn generic and distributed enough representations to facilitate downstream learning of highly pressed task representation , i.e. offering a robust starting point for the training of downstream tasks. When modifying a PLM with PEFT, we hypothesize that one needs to preserve this starting point at the beginning of training for better performance.

**The Starting Point Hypothesis.**_When modifying a pre-trained model by adding new parameters, one needs to initialize the new parameters in a way to preserve the starting point from the pre-trained model at the beginning of training, such that fine-tuning the modified model can match the performance of full fine-tuning._

More formally, supposed \(f_{n}\) is a PLM layer and \(_{n}=f_{n}(_{n-1})\), the output from a modified layer \(f^{}_{n}\), \(^{}_{n}=f^{}_{n}(_{n-1})\), should be close to \(_{n}\) at the beginning of training. I.e. \(^{}_{n}=_{n}+\), where \(\|\| 0\). Intuitively, we want \(^{}_{n}_{n}\), because \(^{}_{n}\) is the input to the next (modified) PLM layer. If they are dissimilar, the representation continuity will be broken down. Though most PEFT methods  initialize their added modules in this way, we couldn't find a thorough investigation of the significance of this initialization in the existing literature. In this section, we explore the significance of PEFT's initialization for two methods, LoRA and (IA)3.

LoRA and (IA)\({}^{3}\) represent two common methods for introducing new parameters, involving addition and scaling operations, respectively. Given a pre-trained weight matrix \(^{d d}\), LoRA modifies it as \(^{}=(+_{down}_{up})\), where \(_{down}^{d r}\) and \(_{up}^{r d}\) are the added trainable parameters, \(\) is a constant scale factor and normally \(r d\). LoRA's default initialization is \(_{down}(0,^{2})\) and \(_{up}=\). In this way, \(_{down}_{up}=\) and the starting point from the

Figure 2: Exploration of existing PEFTs: (a) The trade-off between memory and the number of trainable parameters. The dashed and solid lines denote the peak and activation memory, respectively. The model size for BERT\({}_{}\) is 0.4GB\({}^{2}\). (b) The initialization effect of PEFT on RoBERT\({}_{}\). Random PLM denotes that we initialize the backbone randomly instead of using a pre-trained model.

PLM is preserved perfectly. (IA)3 modifies \(\) by multiplying it to a trainable vector \(^{d}\) as \(^{}=()\), where \(\) represents element-wise multiplication. The default initialization of (IA)3 is \(=\), also making the starting point untouched.

To facilitate the initialization process of LoRA, we opt for the following initial values: \(_{down}=\), \(_{up}=\) and \(=1\), where \(\) is a matrix with all elements equal to an initialized value \(c\), resulting in \(_{down}_{up}=\). When \(c=0\), the starting point from a PLM is preserved. By adjusting \(c\), we exert control over the degree of departure from the starting point. Similarly, we replace \(\) with \(^{}==\) for (IA)3.

In Figure 1(b), we train the newly added parameters on RoBERTa\({}_{}\) for four tasks (CoLA , STS-B , MRPC  and RTE ). For LoRA (\(r=8\)), though we modify the initialization method, our result (\(c=0\)) is very close to the default initialization. When the starting point is broken by \(c 0\) (\(=1\)), all results are even worse than a randomly initialized model. However, when we set \(=0\)3 to preserve the starting point, all results become much better than the ones with \(=1\). For (IA)3, when we decrease \(c\) from 1 (default initialization) to 0, the results (\(=1\)) become worse and worse. However, when we set \(=1/c\) to preserve the starting point, all results become better. Some of them are even better than the default initialization. All of the above-mentioned results show that it's significant to preserve the starting point from a PLM at the beginning of training when applying or designing a PEFT method. A different initialization scheme is in Figure 10 which leads to a similar finding.

### Challenges of reversible neural network

Recapping a reversible model  in Figure 2(a), one can reconstruct inputs from outputs as:

\[^{1}_{n+1} =^{1}_{n}+_{n}(^{2}_{n}) ^{2}_{n} =(^{2}_{n+1}-_{n}(^{1}_{n+1}))/\] (2) \[^{2}_{n+1} =^{2}_{n}+_{n}(^{1}_{n+1}) ^{1}_{n} =(^{1}_{n+1}-_{n}(^{2}_{n}))/\]

where \(\) and \(\) are scaling factors. Theoretically, \(_{n}\) and \(_{n}\) could be two arbitrary functions (sub-networks). Given a multilayer reversible network, intermediate activations for each layer during the forward pass are not necessary to be cached. One only needs to store the final outputs, then reconstruct the intermediate activations and calculate the gradient layer-by-layer in a backward manner (See Listing 1 in SSAppendix). In this way, the memory footprint required for activations can be reduced significantly and has no relationship with the model's depth, i.e. \((1)\) instead of \((N)\).

To investigate the training stability of a reversible model, we run experiments on RevViT .4 RevViT shares the same architecture as Reformer , except applying a convolutional layer at the beginning to project an image into a sequence of vectors. When running RevViT, one could still cache the intermediate activations and treat it as an irreversible model. We term the gradient calculated in this way as _vanilla gradient_. One could also train RevViT in a reversible way, and the corresponding

Figure 3: (a) \(\) and \(\) are two arbitrary functions (sub-networks), taking two inputs, \(^{1}_{n}\) and \(^{2}_{n}\) (b) Reconstruction error between the vanilla and reversible gradients. The default setting is RevViT  with 8 layers, \(=1\), \(=1\), \(=0\) and \(=0.02\). Left: Different number of layers. Middle: Different scaling values. Right: Initialization with different means and standard deviations.

gradient is called _reversible gradient_. We input the same random noises into the same RevViT twice to obtain the parameter gradients from the convolutional layer, in a vanilla and reversible way. Then we calculate the absolute difference between these two gradients and report the maximum and mean values. In this way, we want to check whether the vanilla gradient can be reconstructed in a reversible way. If the reconstruction error is large, it means that the vanilla gradient could not be recovered in a reversible way due to numerical stability, which might cause unstable training or bad performance.

As shown in Figure 2(b), with an increasing number of layers in RevViT, the reconstruction error becomes larger, but still around \(10^{-8}\) which is negligible. However, RevViT is sensitive to the scaling factors, \(\) and \(\). When both scaling factors or one of them are less than \(1\), the reconstruction error increases dramatically. We also explore the initialization of the linear layers in RevViT and find that a larger standard deviation or mean can cause a bigger reconstruction error. In sum, a larger number of layers, smaller scaling factors (\(<1\)) and a larger standard deviation or mean for initialization tend to cause a bigger reconstruction error, which might result in the unstable training or low performance of a reversible model. Last but not least, RevViT  finds that residual connections inside \(\) and \(\) deteriorate the performance of a reversible Transformer .5

## 3 Memory-efficient fine-tuning

This paper aims to modify a PLM to its reversible variant without additional pre-training, so the PLM can still be fine-tuned with a limited memory footprint. The fundamental guiding principle behind our design is: preserving the starting point from a PLM to the greatest extent possible (discussion in SS2.2). In this section, we propose three methods to modify a PLM to a reversible one.

### Meft\({}_{1}\): PLM layer as \(\), adapter as \(\)

As shown in Figure 3(c), we design \(\) as a pre-trained layer with an adapter, where the insertion position for the adapter is borrowed from He et al. . \(\) is simply an adapter. We initialize the adapters as \(_{down},_{up}(0,^{2})\), same for the following methods. In this way, the output from the adapter is close to \(\) at the beginning of the training, so \(_{n}_{n}(_{n-1})\). For the following discussion, we only focus on the beginning of the training, making sure our design preserves the starting point from a PLM.

\(_{0}\) and \(_{1}\) are the input to and output from the \(1^{st}\) layer of a PLM without any modification, respectively. I.e. \(_{0}\) is the representation after the position and word embedding layers of a PLM. We

  
**MEFT\({}_{1}\)** & \(}\) & \(}\) & \(\) & \(\) & **Switch** & \(_{n}^{1}\) & \(_{n}^{2}\) \\ 
1 & layer & adapter & \( 0\) & any & ✓ & \(_{n-1}\) & \(_{n}\) \\
2 & adapter & layer & \( 1\) & \( 0\) & ✓ & \(_{n}\) & \(_{n-1}\) \\
3 & attention & MLP & \( 0\) & \( 0\) & ✗ & \(-\) & \(_{n}\) \\   

Table 1: A summarization of three MEFT methods.

Figure 4: MEFT architectures. (a) Unfold reversible model. (b) A PLM layer. (c) Two MEFT architectures: (1) \(\) is the PLM layer with an adapter (up) and \(\) is an adapter (down); (2) \(\) is the PLM layer with an adapter (up) and \(\) is an adapter (down). (d) The third MEFT architecture: \(\) is the MLP block with an adapter (up) and \(\) is the attention block with an adapter (down). For initialization, \(W_{down},W_{up}(0,^{2})\) and \(=0.02\). Only the adapter is trainable.

assign \(_{1}^{1}=_{0}^{2}=_{0}\), same for the following methods. At the beginning of the training (see Figure 3(a)), \(_{1}^{1}=_{0}^{1}+_{1}(_{0}^{2})=_{0}+_{1}(_{0})_{0}+_{1}\), \(_{1}^{2}=_{0}^{2}+_{1}(_{1}^{1})=_ {0}+_{1}(_{1}^{1})_{0}\), where the approximation holds because of our initialization of the adapters.

For now, \(_{1}^{1}\) and \(_{1}^{2}\) are not desired. When we input \(_{1}^{1}\) and \(_{1}^{2}\) to the \(2^{nd}\) reversible layer, especially when we input \(_{1}^{2}\) to \(_{2}\), the representation continuity6 is broken, because \(_{1}^{2}_{1}\). We introduce two modifications to address this issue: (1) We set \( 0\), so \(_{1}^{1}_{1}\). (2) Then we switch the order of \(_{1}^{1}\) and \(_{1}^{2}\) before feeding to the next reversible layer, i.e. making \(_{1}^{1}_{0}\) and \(_{1}^{2}_{1}\). In this way, \(_{1}^{2}\) preserves the starting point. We don't require \(_{1}^{1}\) to preserve any starting point, because it is entered to \(_{2}\) which is not a pre-trained layer.

With the same above-mentioned design for the \(2^{nd}\) reversible layer, we obtain \(_{2}^{1}_{1}\) and \(_{2}^{2}_{2}\). By analogy, \(_{n}^{1}_{n-1}\) and \(_{n}^{2}_{n}\), which means \(_{n}^{2}\) always preserves the starting point from the PLM. Feeding \(_{n}^{2}\) to the next reversible layer, \(_{n+1}\), doesn't break the representation continuity. After all layers, we input \(_{N}^{}=(_{N}^{1}+_{N}^{2})/2\) to a task-specific head that is a brand new layer, same for the following methods.7

### Meft\({}_{2}\): Adapter as \(\), Plm layer as \(\)

Opposite to MEFT\({}_{1}\), we design \(\) as an adapter and \(\) as the PLM layer with an adapter for MEFT\({}_{2}\) (see Figure 3(c)). In this case, we need to make sure that the input to \(\) preserves the starting point. Let's also start with the first layer, \(_{1}^{1}=_{0}^{1}+_{1}(_{0}^{2})= _{0}+_{1}(_{0})_{0}\), \(_{1}^{2}=_{0}^{2}+_{1}(_{1}^{1})= _{0}+_{1}(_{1}^{1})_{0}+_{1}( _{0})\), where the approximation holds because of our initialization of the adapters.

To preserve the starting point from the PLM, we set \( 1,\, 0\) and switch the order of \(_{1}^{1}\) and \(_{1}^{2}\) before feeding to the next reversible layer. When setting \( 1\), we make sure the representation continuity is preserved for \(_{1}\), resulting in \(_{1}^{2}_{0}+_{1}\). When \( 0\) and the order of \(_{1}^{1}\) and \(_{1}^{2}\) is switched, \(_{1}^{1}_{1}\) and \(_{1}^{2}_{0}\). In this way, \(_{1}^{1}\) preserves the initialization point, and we won't break the representation continuity when feeding it to \(_{2}\) in the next reversible layer. With the same setting for each layer, \(_{n}^{1}_{n}\) and \(_{n}^{2}_{n-1}\), so \(_{n}^{1}\) always preserves the starting point.

### Meft\({}_{3}\): Attention block as \(\), MLP block as \(\)

As shown in Figure 3(d), we can also design \(\) as the pre-trained attention block with an adapter and \(\) as the pre-trained MLP block with an adapter. Also starting with the first layer, we obtain \(_{1}^{1}=_{0}^{1}+_{1}(_{0}^{2})= _{0}+_{1}(_{0})\), \(_{1}^{2}=_{0}^{2}+_{1}(_{1}^{1})= _{0}+_{1}(_{1}^{1})\).

\( 0\) is required, so \(_{1}^{1}\) approximates the original output from the pre-trained attention block, and can be fed to \(_{1}\) to preserve the starting point. \( 0\) is also required, so \(_{1}^{2}_{1}\), and can be fed to \(_{2}\) in the next reversible layer. By default, we set \(= 0\). For MEFT\({}_{3}\), one doesn't need to switch the order of \(_{1}^{1}\) and \(_{1}^{2}\) before feeding to the next reversible layer. For each layer, \(_{n}^{1}\) is close to the original output from the attention block of the corresponding PLM layer, and \(_{n}^{2}_{n}\).

Compared to the vanilla RevNet  where \(==1\), we meticulously assign different values to \(\) and \(\) to preserve the starting point from a PLM, and switch the order of the outputs before feeding to the next layer (if necessary) to preserve the representation continuity. We summarize the settings for all three MEFT methods in Table 1.

## 4 Experiments

### Experimental setup

Datasets and evaluation.We evaluate MEFTs on eight sequence representation tasks and five sequence-to-sequence tasks. All sequence representation tasks are from the GLUE benchmark .

The sequence-to-sequence tasks are question-answering benchmarks, including OpenBookQA , PIQA , ARC (easy and challenge)  and SciQ . We show the statistics of these datasets in Table 8 in Appendix. For the GLUE benchmark, we report accuracy on MNLI, QQP, QNLI, SST-2, MRPC and RTE, Pearson correlation coefficient on STS-B (if not specially mentioning) and Matthews correlation coefficient  on CoLA. We report accuracy on all question-answering tasks. In addition, we report all results on the development sets as our baselines.

**Models.** We use the encoder-only models (BERTbase, RoBERTlarge and BARTlarge encoder ) as the underlying models for all GLUE tasks, and the decoder-only models (OPT\({}_{}\) and OPT\({}_{}\)) for question-answering tasks. (See Table 9 in Appendix for model details.)

**Baselines.** The most important baseline is full fine-tuning (**Full FT**) that updates all parameters. Houlsby Adapter (**AdapterH**) , Pfeiffer Adapter (**AdapterP**) , **Prefix-Tuning** and **LoRA** are chosen as PEFT baselines. In addition, two unified PEFT methods, **MAM** and **AutoPEFT**, that combine multiple PEFT methods are also chosen as PEFT baselines. Lastly, two feature-based tuning methods, \(\)**-Tuning** and **LST**, that aim to reduce training memory serve as memory-efficient baselines. We report the baseline results from the original papers if possible.

**Implementation.** For computational efficiency, we set \(=1\) for MEFT\({}_{1}\), \(=1\) for MEFT\({}_{2}\), and only tune the factors that are required \( 0\) (see Table 1). After obtaining the optimal value, i.e. 0.1, we use this value for all three MEFT methods, tasks and backbones. On the GLUE benchmark, we sweep learning rates in {3, 4, 5}\( 10^{-4}\), batch sizes in {16, 32} and the number of epochs in {10, 20} for the tasks with >10k training samples. For the low-resource tasks with <10k training samples, we sweep learning rates in {5, 6, 7, 8}\( 10^{-4}\), batch sizes in {16, 32} and the number of epochs in {20, 40}. These grid search spaces are inspired by our baselines, especially by LoRA . We use the default Adam  setting with a warmup ratio of 6%. If the model's performance on the development set is not improved over 5 epochs, we stop the training. We run the same task of a method in the above-mentioned grid search space three times with different random seeds, choose the best result from each run, and report the mean and standard deviation of these best results. For all question-answering tasks, we sweep learning rates in {1, 3, 5, 7}\( 10^{-4}\), batch sizes in {8, 16, 32} and the number of epochs in {3, 5, 10}, and keep other settings the same, which is inspired by . The sequence length for all tasks is set to 512, 128, 128 and 128 for BERTbase, RoBERTlarge, BARTlarge and OPT as our baselines, respectively. We run all experiments on the Transformers framework  on a single NVIDIA RTX A6000 GPU with 48GB memory. Overall, a single run of any task could be finished within 8 hours, and most tasks could be finished in an hour. Fine-tuning settings are summarized in Table 7.

### Results and discussions

**Importance of MEFT's initialization.** In the beginning, we further test the starting point hypothesis on our MEFTs by adjusting the scaling factors, \(\) and \(\). As depicted by the dashed lines in Figure 5, the degradation in performance is evident when the scaling factors deviate from their desired value of

Figure 5: MEFTs with various scaling factors on BERTbase over RTE, MRPC, STS-B and CoLA. Dashed and solid lines denote MEFTs with vanilla and reversible gradients, respectively.

Figure 6: The trade-off between the performance and activation memory with MEFT\({}_{1}\) on RoBERTlarge over RTE, MRPC, STS-B and CoLA. The line annotated by ‘freeze: true’ means the shallower PLM layers are frozen without any adaptation, while the line annotated by ‘freeze: false’ means the top MEFT layers with vanilla gradient, as shown in Figure 7.

0 (as indicated in Table 1). However, when they are small enough (0.05 or 0.1), the results are even better than full fine-tuning. For most MEFT methods (MEFT\({}_{1}\) and MEFT\({}_{3}\)), the optimal value for the scaling factors is 0.1. So we use this value for all MEFT methods in the following experiments.

**MEFTs with vanilla gradient are strong PEFT methods.** Though MEFTs have reversible architectures, we can still treat them as irreversible models and cache the intermediate activations during fine-tuning. In this way, they are simply PEFT methods. In Table 2, all MEFT methods, utilizing the vanilla gradient, consistently outperform both full fine-tuning and other baseline approaches by a significant margin. For example, MEFT\({}_{3}\) outperforms Full FT by 1% and the best PEFT baseline (AutoPEFT) by 0.7% on BERT\({}_{}\). MEFT\({}_{1}\) outperforms Full FT by 0.7% on RoBERT\({}_{}\).

**Performance gap of MEFTs between vanilla and reversible gradients.** In Figure 5, the results of MEFTs with reversible gradient (solid line) are often lower than the ones with vanilla gradient (dashed line). Recapping the discussion in SS2.3, smaller scaling factors (\(<1\)) and residual connections in \(\) and \(\) can cause a larger reconstruction error because of numerical stability. When modifying a PLM, we can't remove the residual connections from it and have to set the scaling factors \( 0\) due to the starting point hypothesis, which we believe is the main reason for the performance drop. Our claim is further supported by MEFT\({}_{3}\) which has the most evident drop among all MEFTs. Compared to MEFT\({}_{1}\) and MEFT\({}_{2}\) that only have a residual connection in either \(\) or \(\), both \(\) and \(\) of MEFT\({}_{3}\) have residual connections. In addition, we have to set both \(\) and \(\) close to 0 for MEFT\({}_{3}\), which also causes a bigger reconstruction error than only setting one scaling factor (see Figure 3b middle). Since MEFT\({}_{3}\) with reversible gradient performs the worst among all MEFTs, we only run it on BERTbase

    &  & & & & & & & & & \\
**Method** & **(\%)** & **Peak** & **Act.** & **RTE** & **MRPC** & **STS-B** & **CoLA** & **SST-2** & **QNLJ** & **QQP** & **MNLJ** & **Avg.** \\    & _{}\) (backline = 32, sequence length = 125)**} & & & & & & & & \\ Full FT & 100 & 16.67 & 14.98 & 71.11\({}_{1.5}\) & 85.73 & 89.03 & 59.06 & 92.02 & 91.50 & **91.50** & 84.02 & 83.2 \\ Prefix-Tuning & 0.17 & 13.58 & 13.00 & 70.50\({}_{1.5}\) & 85.93 & 88.02 & 58.91\({}_{1.9}\) & 91.90 & 90.80 & 89.10 & 82.02 & 82.3 \\ LaRA & 0.27 & 13.45 & 13.02 & 65.93 & 84.51\({}_{1.9}\) & 88.71 & 57.63 & 92.14 & 90.02 & 90.02 & 89.40 & 83.01 & 81.5 \\ MAM & 6.97 & 14.21 & 13.02 & 69.18 & 87.20 & 90.05 & 79.44 & 89.31 & 90.90 & 90.90 & 89.10 & 83.32 & 80.3 \\ AutoPEFT & 1.40 & - & - & 72.49\({}_{0.9}\) & **87.55** & 89.20 & 60.91\({}_{1.9}\) & 92.10\({}_{1.9}\) & 91.00 & 90.10 & 84.01 & 83.5 \\   &  & & & & & & & & & & \\ MEFT\({}_{1}\) & 0.27 & 13.64 & 13.21 & 74.21\({}_{1.4}\) & 86.70\({}_{1.9}\) & 89.40\({}_{0}\) & 62.10\({}_{1.0}\) & 92.90\({}_{1.0}\) & 89.90\({}_{1.0}\) & 83.04 & 83.8 \\ MEFT\({}_{2}\) & 0.27 & 13.73 & 13.31 & 74.20\({}_{1.8}\) & 86.60\({}_{1.5}\) & **89.40** & 61.83\({}_{1.7}\) & 93.01 & **91.60** & 92.01 & **84.51** & 84.0 \\ MEFT\({}_{3}\) & 0.27 & 13.64 & 13.21 & **76.03** & 57.63 & 88.70\({}_{1.0}\) & **62.30** & **93.02** & 92.51 & 90.10 & 94.10 & 84.02 & **84.2** \\   &  & & & & & & & & & & \\ MEFT\({}_{1}\)(GP32) & 0.27 & 2.75 & 2.33 & 73.95\({}_{1.5}\) & 86.50\({}_{2.2}\) & 88.80\({}_{1.0}\) & 60.36\({}_{1.9}\) & 92.70\({}_{1.4}\) & 91.40\({}_{0.8}\) & 88.81 & 83.41\({}_{1.1}\) & 83.2 \\ MEFT\({}_{2}\)(GP32) & 0.27 & 3.53 & 3.11 & 74.08\({}_{1.6}\) & 86.34\({}_{1.4}\) & 88.61\({}_{0.7}\) & 92.80\({}_{1.9}\) & 91.50\({}_{1.8}\) & 88.91 & 83.14\({}_{1.1}\) & 83.2 \\ MEFT\({}_{3}\)(GP32) & 0.27 & 2.99 & 2.57 & 70.86\({}_{8.4}\) & 84.05\({}_{1.8}\) & 88.20\({}_{1.5}\) & 91.90\({}_{2.2}\) & 90.40\({}_{0.2}\) & 86.93 & 81.51\({}_{0.1}\) & 81.1 \\    &  & & & & & & & & & & \\ _{}\) (backline = 32, sequence length = 128)_} & & & & & & & & & & \\ Full FT & 100 & 11.4 & 6.05 & 86.6 & 90.9 & **92.4** & 68.0 & 96.4 & 94.7 & **92.2** & 90.2 & 88.9 \\ Adopeu1 & 0.23 & 6.05 & 4.66 & 29.19\({}_{1.9}\) & 87.71\({}_{1.9}\) & 91.55\({}_{6.4}\) & 86.20\({}_{1.9}\) & 96.38\({}_{1.9}\) & 94.72\({}_{1.9}\) & 91.51\({}_{0.3}\) & 90.33 & 86.4 \\ Adopeu2 & 1.69 & 6.18 & 4.71 & 82.14\({}_{1.4}\) & 88.79\({}_{1.9}\) & 91.07\({}_{6.5}\) & 65.44\({}_{1.6}\) & 96.20\({}_{3.0}\) & 94.7\({}_{2.2}\) & 92.11\({}_{1.8}\) & 89.95 & 87.8 \\ Adopeu2 & 0.23 & 6.16 & 4.77 & 80.12\({}_{1.9}\) & 89.71\({}_{1.2}\) & 91.90\({}_{4.7}\) & 87.53\({}_{1.6}\) & 96.60\({}_{3.0}\) & 94.72\({}_{2.0}\) & 92.11\({}_{1.8}\) & 89.49 & 87.8 \\ Adopeu3 & 0.28 & 6.14 & 4.78 & 83.29\({}_{1.9}\) & 90.20\({}_{1.7}\) & 92.10\({}_{1.7}\) & 68.3\({}_{1.9}\) & 96.10\({}_{3.0}\) & 94.82\({}_{1.9}\) & 91.90 & 90.20\({}_{2.3}\) & 88.4 \\ LeRA & 0.23 & 6.11 & 4.72 & 85.21\({}_{1.1}\)due to limited resources. Expectedly, MEFT\({}_{1}\) trained in FP32 outperforms it trained in FP16 on both RoBERTa\({}_{}\) and BART\({}_{}\) (see Table 2), because FP16 causes more instability.

**Reversible MEFTs on deep model.** Because of the starting point hypothesis, the residual connection from PLMs remains and the scaling factors are set closely to 0. With an increasing number of layers, the training instability is expected to become more severe (see Figure 2(b) left). As shown in Figure 6, when all RoBERTa layers are reversible (the number of reversible layers as 24), the score drops dramatically. To address this issue, we propose three settings in Figure 7: (1) Cache the activations for top layers (vanilla gradient) and apply reversible shallow layers (reversible gradient). (2) Freeze some shallow PLM layers, i.e. treating the shallow layers as a feature extractor. (3) Combine the above two settings. Notably, we have to put the reversible layers under the vanilla layers due to numerical stability. If we reverse the order, the reconstruction error is transferred to the vanilla layers.

We only explore the first two settings on RoBERTa and will discuss the third setting in the following, since RoBERTa\({}_{}\) doesn't contain many layers. In Figure 6, when we apply the first setting (freeze: false) to RoBERTa\({}_{}\), the average score becomes better when the number of reversible layers decreases, outperforms full fine-tuning when it's \( 16\). However, the activation memory also increases with an increasing number of vanilla layers, since the vanilla layers require caching the activations. By default, we set the number of reversible layers as 16 for RoBERTa\({}_{}\) in Table 2. For the second setting (freeze: true), the results are always worse than full fine-tuning. However, its activation memory stays the same since all trainable layers are reversible.

**MEFTs are parameter and memory-efficient with a strong performance.** Let's go back to Table 2. Though there is a gap in MEFTs between vanilla and reversible gradients, reversible MEFTs still achieve strong results compared to previous baselines. On BERT\({}_{}\), reversible MEFT\({}_{1}\) and MEFT\({}_{2}\) obtain the same average score as Full FT, slightly worse than the best PEFT method, AutoPEFT (83.2 vs. 83.5). However, reversible MEFTs only requires about 21% and 24% activation memory of Full FT and PEFTs. On RoBERTa\({}_{}\), reversible MEFT\({}_{1}\) (FP32) achieves the same score as Full FT and outperforms all PEFT methods, while only requiring 37% and 48% activation memory of Full FT and PEFTs.

Due to limited computing resources, we only conduct experiments on the best MEFT method, MEFT\({}_{1}\), on BART\({}_{}\) when compared to other memory-efficient methods. In addition, we don't use our own grid search space on BART\({}_{}\). Instead, we apply the same grid search space as LST, setting the learning rate in \(\{3 10^{-4},1 10^{-3},3 10^{-3}\}\), the batch size as 100 and the number of epochs as 20. In this way, we want to validate the robustness of MEFT. Similarly, MEFT\({}_{1}\) outperforms the memory-efficient baselines by a large margin while only requiring 29% LST's activation memory. In addition, LST requires knowledge distillation to initialize the added layers and is not stable .8

**MEFT trained in FP32 vs. in FP16, and the time-memory tradeoff.** Though reversible MEFTs trained in FP32 outperform the ones trained in FP16, there are still some notable discussions about them: (1) The memory footprint required by reversible MEFTs trained in FP32 and FP16 is the same. In Table 2, MEFT\({}_{1}\) and MEFT\({}_{1}\) (FP32) have the same activation memory on BART\({}_{}\), because the recomputed activations in back-propagation are always in FP32 due to the mixed precision training . I.e. PyTorch  only allows FP32 in back-propagation; (2) FP16 still benefits the training of large PLMs. In Table 2, the peak and activation memory difference is about the backbone size in FP32 for PEFTs and MEFTs. If one could reduce the backbone size by loading in FP16, we can further reduce the peak memory; (3) Training in FP16 is faster than the training in FP32 (about 1:1.2) due to the forward pass. In addition, since reversible MEFTs recompute the activations, they require more training time, about twice the training time for MEFTs with the vanilla gradient.

**Results on larger and deeper models.** Here we explore a more realistic setting (the third setting in Figure 7) on larger and deeper models, OPT\({}_{1.3}\) and OPT\({}_{6.7}\), in Table 3. On OPT\({}_{1.3}\) with 24 layers, we set the number of frozen, reversible and vanilla layers as 8. On OPT\({}_{6.7}\) with 32 layers, we use 8 reversible and vanilla layers, same as OPT\({}_{1.3}\). For a fair comparison, we freeze the first 8 PLM layers and modify the rest 16 layers with LoRA. MEFT\({}_{1}\) is comparable to LoRA, while only requiring LoRA's 65% activation memory. Though slightly worse than Full FT (-0.3%), MEFT\({}_{1}\)'s

Figure 7: Three settings for deep models.

activation memory is only half of the one for Full FT. When using the same activation memory as Full FT by running on OPT\({}_{}\), MEFT\({}_{1}\) outperforms Full FT by a large margin.

**Transfer to image classification task.** Though we only focused on NLP tasks, MEFT could be transferred to other tasks, even other architectures. We leave the transfer of MEFT to other architectures for future work, and here apply MEFT to ViT  for an image classification task, i.e. SVHN . We follow the main training recipe from AdapFormer , except for changing the optimizer from SGD to AdamW, setting the maximum gradient norm as 0.3. For MEFT\({}_{1}\)'s hyper-parameters, we set \(r=64\) and \(=0.3\) (smaller \(\) is not stable for training). Similar to the NLP's results, MEFT\({}_{1}\) achieves comparable accuracy as AdapFormer while saving a large amount of memory footprint in Table 4.

For more results about comparing MEFT to gradient checkpointing, comparing MEFT to quantization methods, and combining MEFT with other memory-efficient methods, please go to Appendix SSA. In addition, due to the page limit, we put the detailed related works in Appendix SSA, and discuss the limitation of our work in Appendix SSA.

## 5 Conclusion

In this paper, we propose three memory-efficient fine-tuning methods (MEFTs), that fine-tune PLM in a parameter-efficient and memory-efficient way without the requirement of additional pre-training and match the performance of full fine-tuning. MEFTs modify the PLM architecture with adapters and make it reversible, by following the starting point hypothesis that is essential for PEFTs. So MEFTs don't require caching the intermediate activations during training and significantly reduce the memory footprint occupied by activations. When applying MEFTs to various models, BERT, RoBERTa and BART, on the GLUE benchmark, MEFTs achieve a similar score as full fine-tuning and other strong baselines, while saving up to 84% activation memory. A similar story is also observed when applying MEFT to larger and deeper models, OPT, on five question-answering tasks. MEFT achieves a comparable score as full fine-tuning and only consumes its 50% activation memory. However, because of the recomputation of activations, MEFTs require slightly more training time than other PEFT methods and offer a slightly lower score when trained in FP16 instead of FP32. In the future, we are interested in applying MEFT to other areas, like computer vision and automatic speech recognition, and to other bigger backbones for more sequence-to-sequence tasks.