# Efficient Algorithms for Generalized Linear Bandits with Heavy-tailed Rewards

Bo Xue\({}^{1,2}\), Yimu Wang\({}^{3}\), Yuanyu Wan\({}^{4}\), Jinfeng Yi\({}^{5}\), Lijun Zhang\({}^{6,7,}\)

\({}^{1}\)Department of Computer Science, City University of Hong Kong, Hong Kong, China

\({}^{2}\)The City University of Hong Kong Shenzhen Research Institute, Shenzhen, China

\({}^{3}\)Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada

\({}^{4}\)School of Software Technology, Zhejiang University, Ningbo, China

\({}^{5}\)JD AI Research, Beijing, China

\({}^{6}\)National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

\({}^{7}\)Peng Cheng Laboratory, Shenzhen, China

boxue4-c@my.cityu.edu.hk, yimu.wang@uwaterloo.ca, wanyy@zju.edu.cn

yijinfeng@jd.com, zhanglj@lamda.nju.edu.cn

Lijun Zhang is the corresponding author.

###### Abstract

This paper investigates the problem of generalized linear bandits with heavy-tailed rewards, whose \((1+)\)-th moment is bounded for some \((0,1]\). Although there exist methods for generalized linear bandits, most of them focus on bounded or sub-Gaussian rewards and are not well-suited for many real-world scenarios, such as financial markets and web-advertising. To address this issue, we propose two novel algorithms based on truncation and mean of medians. These algorithms achieve an almost optimal regret bound of \((dT^{})\), where \(d\) is the dimension of contextual information and \(T\) is the time horizon. Our truncation-based algorithm supports online learning, distinguishing it from existing truncation-based approaches. Additionally, our mean-of-medians-based algorithm requires only \(O( T)\) rewards and one estimator per epoch, making it more practical. Moreover, our algorithms improve the regret bounds by a logarithmic factor compared to existing algorithms when \(=1\). Numerical experimental results confirm the merits of our algorithms.

## 1 Introduction

The multi-armed bandits (MAB) is a powerful framework to model the sequential decision-making process with limited information (Robbins, 1952), which has been found applications in various areas such as medical trails (Villar _et al._, 2015) and advertisement placement (Bubeck and Cesa-Bianchi, 2012). In the classical \(K\)-armed bandit problem, an agent selects one of the \(K\) arms and receives a reward drawn independently and identically distributed from an unknown distribution associated with the chosen arm. The goal of the agent is to maximize the cumulative rewards through the trade-off between exploration and exploitation, i.e., pulling the arms that may potentially give better outcomes while also exploiting the knowledge gained from previous trials to select the optimal arm.

One fundamental limitation of MAB is that it ignores contextual information in some scenarios such as advertisement placement (Lattimore and Szepesvari, 2020), where features of users and products can provide valuable guidance for decision making. In these cases, decisions should not only rely on rewards from previous epochs but also the contextual information from both past and current epochs. Stochastic Linear Bandits (SLB) has emerged as the most popular model in the last decade to address this limitation, assuming a linear relationship between the contextual vector and the expected reward[Auer, 2002; Dani _et al._, 2008; Abbasi-yadkori _et al._, 2011; Hu _et al._, 2021; Alieva _et al._, 2021; Yang _et al._, 2022; He _et al._, 2022; Bengs _et al._, 2022]. However, in many real-world applications, such as social network [Filippi _et al._, 2010], the assumption of Poisson or logistic relation between the expected reward and contextual vector has demonstrated better performance, which motivates the study of generalized linear bandits (GLB). In each round, the agent first observes a decision set \(_{t}^{d}\) composed of contextual vectors. Then, the agent selects an arm \(_{t}_{t}\) and receives a reward \(y_{t}\) satisfying the expectation,

\[[y_{t}|_{t}]=(_{t}^{}_{*})\] (1)

where \(_{*}\) is the inherent vector and \(()\) is the link function, such as the identity function or the logistic function. The performance of the agent is measured by the regret such that

\[R(T)=_{t=1}^{T}((}_{t}^{}_{*})-( _{t}^{}_{*}))\]

where \(}_{t}=*{argmax}_{_{t}}(^{}_{*})\) represents the optimal arm in the set \(_{t}\).

Extensive research has been conducted on the GLB, with most assuming sub-Gaussian rewards [Filippi _et al._, 2010; Li _et al._, 2012, 2017; Jun _et al._, 2017; Lu _et al._, 2019; Zhou _et al._, 2019; Han _et al._, 2021; Li and Wang, 2022]. However, it has been observed that in certain sequential decision-making scenarios, such as financial markets [Cont and Bouchaud, 2000], the occurrence of extreme returns is much more frequent than the standard normal distribution. This phenomenon is known as heavy-tailed behavior [Foss _et al._, 2013], where the existing algorithms are not suitable. To address this limitation, in this study, we focus on the GLB with heavy-tailed rewards [Bubeck _et al._, 2013], i.e., the reward obtained at \(t\)-th round satifies the condition

\[[|y_{t}|^{1+}] u\]

for some \((0,1]\) and \(u>0\). Different from the traditional sub-Gaussian setting, heavy-tailed rewards do not decay exponentially and the estimation of expected rewards is significantly impacted.

According to the distinguishing characteristic of heavy-tailed distributions where extreme values occur with high probability, previous studies have developed three main strategies to address the issue in parameter estimation [Audibert and Catoni, 2011; Hsu and Sabato, 2014; Zhang and Zhou, 2018; Ray Chowdhury and Gopalan, 2019; Lugosi and Mendelson, 2021; Zhong _et al._, 2021; Huang _et al._, 2022; Diakonikolas _et al._, 2022; Gorbunov _et al._, 2022; Kamath _et al._, 2022; Li and Liu, 2022; Gou _et al._, 2023]. One such strategy is truonuAudibert and Catoni , which mitigates the impact of extreme values by truncating them. A recently proposed strategy is the mean of medians approach [Zhong _et al._, 2021], which involves partitioning the samples drawn from the heavy-tailed distribution into multiple groups, taking the median within each group, and computing the mean of these medians. It intuitively reduces the impact of extreme samples, as extreme samples are distributed to both sides, thus the median value is more robust. The third strategy is median of means [Hsu and Sabato, 2014], which adjusts the order of calculating mean and taking the median in the mean of medians strategy.

Most existing algorithms for heavy-tailed bandit problems are derived from aforementioned three strategies, with a primary focus on the SLB model [Medina and Yang, 2016; Shao _et al._, 2018; Xue _et al._, 2020]. To provide a comprehensive overview and facilitate comparison, we present a summary of our results and previous findings on linear bandits with heavy-tailed rewards in Table 1. For the sake of clarity, the presented regret bounds in Table 1 are under the assumption that the rewards have finite variance. The computational complexity only takes into account multiplication and division operations. Although Shao _et al._  and Xue _et al._  achieve nearly optimal regret for

   & Regret & CC\_Truncation & CC\_MoM & Arms & Model \\  Medina and Yang  & \(O(dT^{})\) & \(O(d^{2}T)\) & \(O(d^{2}T/ T)\) & infinite & SLB \\ Shao _et al._  & \((d)\) & \(O(d^{3}T+d^{2}T^{2})\) & \(O(d^{2}T T)\) & infinite & SLB \\ Xue _et al._  & \(()\) & \(O(d^{2}T^{2})\) & \(O(d^{2}T)\) & finite & SLB \\ This work & \((d)\) & \(O(d^{2}T)\) & \(O(d^{2}T/ T)\) & infinite & GLB \\  

Table 1: Summary of the existing work for the linear bandits with heavy-tailed rewards. CC is the abbreviation of computational complexity.

infinite-armed and finite-armed SLB, respectively, their algorithms are computationally expensive. The latest work utilizing the mean of medians approach demonstrates efficiency but is limited to symmetric rewards (Zhong et al., 2021). Therefore, designing efficient heavy-tailed algorithms for GLB with symmetric and asymmetric rewards is an interesting and non-trivial challenge.

Through the delicate employment of heavy-tailed strategies, our contributions to the generalized linear bandit problem with heavy-tailed rewards can be summerized as follows:

* We develop two novel algorithms, CRTM and CRMM, which utilize the truncation strategy and mean of medians strategy, respectively. Both algorithms exhibit a sublinear regret bound of \((dT^{})\), which is amid optimal as the lower bound is \((dT^{})\)(Shao et al., 2018).
* CRTM reduces the computational complexity from \(O(T^{2})\) to \(O(T)\) when compared to existing truncation-based algorithms (Shao et al., 2018; Xue et al., 2020), while CRMM reduces the number of estimator required per round from \(O( T)\) to only one, as compared to existing median-of-means-based algorithms (Shao et al., 2018; Xue et al., 2020).
* When \(=1\), the regret bounds of CRTM and CRMM improves a logarithmic factor of order \(\) and \(-\) for some \((0,1)\), respectively, over the recently proposed method of Zhong et al. (2021)2. Notably, CRTM extends the method of Zhong et al. (2021) from symmetric rewards to general case, making CRTM more practical. * We conduct numerical experiments to demonstrate that our proposed algorithms not only achieve a lower regret bound but also require fewer computational resources when applied to heavy-tailed bandit problems.

## 2 Related Work

In this section, we briefly review the related work on linear bandits. Through out the paper, the \(p\)-norm of a vector \(^{d}\) is \(\|\|_{p}=(|x_{1}|^{p}++|x_{d}|^{p})^{1/p}\). Given a positive definite matrix \(^{d d}\), the weighted Euclidean norm of the vector \(\) is \(\|\|_{}=^{}}\).

### Generalized Linear Bandits

Filippi et al. (2010) was the first to address the generalized linear bandit problem and proposed an algorithm with a regret bound of \((d)\). However, their algorithm is not efficient as it requires storing all the action-feedback pairs encountered so far and performing maximum likelihood estimation at each step. A notable improvement was presented by Zhang et al. (2016) with the introduction of an efficient algorithm called OL\({}^{2}\)M, whose space and time complexity at each epoch does not grow over time and achieves a \((d)\) regret. However, their algorithm is limited to the logistic link function. Later, Jun et al. (2017) extended OL\({}^{2}\)M to generic link functions while still maintaining the \((d)\) regret bound. Ding et al. (2021) proposed another efficient generalized linear bandit algorithm following the line of Thompson sampling scheme.

The main challenge in the bandit problem is the trade-off between exploration and exploitation. To address this issue, the most commonly used approach is the confidence-region-based method, specifically for the linear bandit model with infinite arms (Dani et al., 2008; Abbasi-yadkori et al., 2011; Zhang et al., 2016). Here we take the algorithm OL\({}^{2}\)M to give a brife introduction to this approach (Zhang et al., 2016). With the arrival of a new trial \((_{t},y_{t})\) in the \(t\)-th epoch, OL\({}^{2}\)M first constructs a surrogate loss \(_{t}()\) satisfying \(_{t}()=(-y_{t}+(_{t}^{}))_ {t}\). Then, OL\({}^{2}\)M employs a variant of the online Newton step (ONS) to update the estimated parameters, i.e.,

\[}_{t+1}^{N}=*{argmin}_{^ {d}}-}_{t}^{N}\|_{_{t+1}}^{2}}{2 }+-}_{t}^{N},_{t}(}_{t}^{N}).\] (2)

Here, \(_{t+1}=_{t}+_{t}_{t}^{}\) for \(>0\), and the initialized matrix \(_{1}=_{d}\) for \(>0\). Subsequently, OL\({}^{2}\)M constructs a confidence region \(_{t+1}\) centered at the estimated parameter \(}_{t+1}^{N}\), such that

\[_{t+1}=\{^{d}\|\|-}_{t}^{N}\|_{_{t+1}}^{2}_{t+1}\}\] (3)where \(_{t+1}=O(d t)\) indicating the uncertainty of the estimation and the unknown parameter \(_{*}\) lies in this region with high probability. Finally, OL\({}^{2}\)M selects the most promising arm \(_{t+1}\) according to the principle of "optimization in the face of uncertainty", i.e.,

\[(_{t+1},}_{t+1})=*{argmax}_{ _{t+1},_{t+1}},.\] (4)

### Bandit Learning with Heavy-tailed Rewards

Most of the existing work developed heavy-tailed bandit algorithms using truncation and median of means strategies (Bubeck _et al._, 2013; Medina and Yang, 2016; Shao _et al._, 2018; Xue _et al._, 2020; Huang _et al._, 2022). Bubeck _et al._ (2013) first conducted extensive research on multi-armed bandits with heavy-tailed rewards and achieved a logarithmic regret bound. Medina and Yang (2016) extended it to the SLB model and introduced two algorithms that achieve regret bounds of \((dT^{})\) and \((d^{}T^{}+dT^{})\), respectively. Shao _et al._ (2018) improved upon the results of Medina and Yang (2016) by a more delicate application of heavy-tailed strategies, achieving a regret bound of \((dT^{})\). Xue _et al._ (2020) investigated the case with finite arms and provided two algorithms that attained regret bounds of \((d^{}T^{})\). Recently, Zhong _et al._ (2021) proposed the mean of medians estimator for the super heavy-tailed bandit problem, but the rewards are limited to symmetric distributions. Applying this estimator to the GLB algorithm of Jun _et al._ (2017) yields a heavy-tailed GLB algorithm that achieves the regret bound of \(O(d( T)^{+}T^{})\) for some \((0,1)\). To illustrate the basic idea of adopting different heavy-tailed strategies in the bandit model, we briefly describe three representative algorithms.

For the algorithm exploiting truncation strategy, we take the algorithm TOFU as an instance (Shao _et al._, 2018). With the trials up to round \(t\), TOFU truncates the rewards \(d\) times as follows,

\[_{t}^{i}=[y_{1}_{|u_{1}^{i}(t)y_{1}| h_{t}}, ,y_{t}_{|u_{t}^{i}(t)y_{t}| h_{t}}],i=1,2,,d\] (5)

where \(h_{t}=O(t^{})\) is the truncated criterion, and \(u_{}^{i}(t)\) denotes the element in the \(i\)-th row and \(\)-th column of matrix \(}_{t+1}^{-1/2}_{t}\), \(_{t}=[_{1},_{2},,_{t}]^{d t}\) is the matrix composed of selected arms and \(}_{t+1}=_{t}_{t}^{}+_{d}\). Using these truncated rewards, TOFU conducts an estimator as \(}_{t+1}=}_{t+1}^{-1/2}[_{1}^{1} _{t}^{1},,_{t}^{d}_{t}^{d}]\) with \(_{t}^{i}_{t}^{i}=_{=1}^{t}u_{}^{i}(t)y_{ }_{|u_{}^{i}y_{}| h_{t}}\). TOFU then constructs a confidence region centered on this estimator and selects the promising arm, similar to (3) and (4). Notice that the scalarized parameters \(\{u_{}^{i}(t)\}_{=1}^{t}\) are updated at each epoch, requiring TOFU to store the learning history and truncate all rewards at each epoch. Thus, TOFU is not an online method.

For the algorithm exploiting median of means strategy, it's common to play the chosen arm \(r\) times and get \(r\) sequences of rewards \(\{Y_{t}^{j}\}_{j=1}^{r}\), where \(Y_{t}^{j}=[y_{1}^{j},,y_{t}^{j}]\) is the \(j\)-th reward sequence up to epoch \(t\). MENU executes least square estimation for each reward sequence and get \(r\) estimators, i.e.,

\[}_{t+1}^{j}=*{argmin}_{^{ d}}_{t}^{}-Y_{t}^{j}_{2}^{2}+_{2}^{2},\;j=1,2,,r\] (6)

where \(r=O( T)\)(Shao _et al._, 2018). Then, the median of means strategy adopted by MENU is operated as follows,

\[m_{j}=\{}_{t+1}^{j}-}_{t+1}^{s}_{}_{t+1}}:s=1,,r\}.\] (7)

Then, MENU takes the estimator \(}_{t+1}^{k}\) with \(k_{*}=*{argmin}_{j\{1,2,,r\}}\{m_{j}\}\) as the center of confidence region. Finally, MENU selects the most promising arm similar to (4).

For the mean of medians method proposed by Zhong _et al._ (2021), at each epoch \(t\), the agent first plays the selected arm \(\) times, with a value of \(=O(( T)^{1/})\) for some \((0,1)\), and then receives rewards \(\{y_{t}^{j}\}_{j=1}^{}\) for these plays. Subsequently, the agent randomly divides the rewards into multiple groups, with each group contains \(^{}\) rewards. The agent then takes the median of each group and uses the mean of these medians to update the estimator. Notice that the expectation of the median has a bias to the expected reward other than the symmetric distribution. Thus, mean of medians strategy is limited to symmetric distribution. Another point worth mentioning is that \(\) is too large to try sufficient different arms. For example, the agent can only play \( T/=100\) different arms with \(T=10^{6}\) and \(=0.62\), which is obviously unreasonable3.

## 3 Algorithms

In this section, we first introduce the generalized linear bandit model and then demonstrate two novel algorithms based on truncation and mean of medians, respectively.

### Learning Model

The formal description of the generalized linear bandit model is as follows. In each round \(t\), an agent plays an arm \(_{t}_{t}\) and obtains a stochastic reward \(y_{t}\), which is generated from a generalized linear model represented by the following equation,

\[(y_{t}|_{t})=(_{t}^{}_{*}-m(_{t}^{}_{*})}{g()}+h(y_{t},))\] (8)

where \(_{*}\) is the inherent parameters, \(>0\) is a known scale parameter, and \(g()\) and \(h(,)\) are normalizers (P. McCullagh, 1989). The expectation of \(y_{t}\) is given by

\[[y_{t}|_{t}]=m^{}(_{t}^{}_{*}).\]

Thus, \(m^{}()\) is the link function in (1), such that \(()=m^{}()\). The reward model can be rewritten as

\[y_{t}=(_{t}^{}_{*})+_{t}\]

where \(_{t}\) is a random noise satisfying the condition

\[[_{t}|_{t-1}]=0.\] (9)

Here, \(_{t-1}\{_{1},y_{1},,_{t-1},y_{t-1}, {x}_{t}\}\) is a \(\)-filtration and \(_{0}=\). Following the existing work (Filippi _et al._, 2010; Jun _et al._, 2017; Li _et al._, 2017), we make standard assumptions as follows.

**Assumption 1**: _The coefficients \(_{*}\) and contextual vectors \(\) are bounded, such that \(\|_{*}\|_{2} S\) and \(\|\|_{2} 1\) for all \(_{t}\), where \(S\) is a known constant._

**Assumption 2**: _The link function \(()\) is \(L\)-Lipschitz on \([-S,S]\), and continuously differentiable on \((-S,S)\). Moreover, there exists some \(>0\) such that \(^{}(z)\) and \(|(z)| U\) for any \(z(-S,S)\)._

### Truncation

Our first algorithm is called Confidence Region with Truncated Mean (CRTM). The complete procedure is provided in Algorithm 1. Here, we consider the heavy-tailed setting, i.e., there exists a constant \(u>0\), the rewards admit

\[[|y_{t}|^{1+}|_{t-1}] u.\] (10)

As we have mentioned earlier in Section 2.1, to design effective algorithms for GLB model, constructing a narrow confidence region for the underlying coefficients \(_{*}\) is necessary. However, heavy-tailed rewards that satisfy (10) produce extreme values with high probability, resulting in a confidence region with a large radius. Therefore, a straightforward approach to settle this problem is to truncate the extreme reward to reduce its impact.

In each round \(t\), CRTM first plays an arm \(_{t}_{t}\) and observes the corresponding reward \(y_{t}\). Then, CRTM truncates the reward \(y_{t}\) using a uniform criterion \(=(T^{})\), such that

\[_{t}=y_{t}_{\|_{t}\|_{}_{t}^{-1}|y_{ t}|}}\]where \(_{t}=_{t-1}+_{t-1}_{t-1}^{}\) with \(_{1}=_{d}\). Here, \(\) is defined in Assumption 2 and \(=\{1,/2\}\). With the processed action-reward pair \((_{t},_{t})\), CRTM computes the gradient of the loss function as

\[_{t}()=(-_{t}+(_{t}^{}))_{t},\] (11)

where \(_{t}()\) is the negative log-likelihood of the generalized linear model (8). After that, CRTM employs a variant of online Newton step (ONS) to update its estimator, given by

\[}_{t+1}=*{argmin}_{\|\|_{2} S} -}_{t}\|_{_{t+1}}^{2}}{2}+ -}_{t},_{t}(}_{ t}).\]

Equipped with above estimation, CRTM constructs the confidence region \(_{t+1}\) where the inherent parameters \(_{*}\) lies in with high probability, such that

\[_{t+1}=\{^{d}\|\|-}_{t+1}\|_{_{t+1}}^{2}\}\]

where \(=(T^{})\) denotes the width of the confidence region, and details are shown in Theorem 1. Given the confidence region \(_{t+1}\), the most promising arm \(_{t+1}\) can be obtained through the following maximize operation,

\[(_{t+1},}_{t+1})=*{argmax}_{ _{t+1},_{t+1}},\]

since \(()\) is monotonically increasing according to Assumption 2.

Although there exists several heavy-tailed linear bandit algorithms based on the truncation strategy, such as TOFU (Shao _et al._, 2018) and BTC (Xue _et al._, 2020), CRTM differs from them in two aspects. Firstly, both TOFU and BTC have to store the historical rewards and truncate them at each epoch, resulting in a computational complexity of \(O(T^{2})\). In contrast, CRTM achieves online learning by processing only the reward of current round, whose computational complexity is \(O(T)\). Secondly, while TOFU and BTC are designed for SLB model and calculate the estimator via least-squares estimation, CRTM is designed for the GLB model and updates the estimator using the ONS method, which makes the analytical techniques fundamentally different. Theorem 1 provides a tight confidence region, and its proof relies on the induced method because ONS is an iteratively updated method. Due to the page limit, we provide the detailed proof in the Appendix A.2.

**Theorem 1**: _If the rewards satisfy (9) and (10), then with probability as least \(1-\), the confidence region in CRTM is_

\[\|-}_{t+1}\|_{_{t+1}}^{2},  t 0\]_where_

\[=224u^{}(4T/)^{}T^ {}(1+)+2 S^{2}+d}{}(1+).\]

With above confidence region, the regret bound of CRTM is explicitly given as follows.

**Theorem 2**: _If the rewards satisfy (9) and (10), then with probability at least \(1-\), the regret of CRTM satisfies_

\[R(T) O(d( T)^{}T^{}).\]

**Remark:** The above theorem establishes a \((dT^{})\) regret bound with the assumption that the \((1+)\)-th moment of the rewards is bounded for some \((0,1]\). Existing algorithms based on truncation is time-consuming because they need to store the learning history and truncate all historical rewards at each epoch (Shao _et al._, 2018; Xue _et al._, 2020). Unlike the recently proposed mean of medians method which is limited in symmetric rewards (Zhong _et al._, 2021), CRTM expands it to asymmetric and achieves an improved regret bound by a factor of \(O(( T)^{}})\) for some \((0,1)\) if \(=1\). Furthermore, CRTM is almost optimal as the lower bound is \((dT^{})\)(Shao _et al._, 2018).

### Mean of Medians

In this section, we present our second algorithm, referred to as Confidence Region with Mean of Medians (CRMM), which shares a similar framework with CRTM but uses a different mean of medians estimator. The complete procedure is outlined in Algorithm 2. CRMM requires that for some \((0,1]\), the \(1+\) central moment of the rewards is bounded, and the distribution of rewards is symmetric. Precisely, for some \((0,1]\), there exists a constant \(v>0\) such that the rewards satisfy

\[[|_{t}|^{1+}|_{t-1}] vp(_{t})=p(-_{t}).\] (12)

At each epoch \(t\), CRMM plays the selected arm \(_{t}\)\(r\) times, generating rewards \(\{y_{t}^{1},,y_{t}^{r}\}\) with \(r=O( T)\). To obtain a robust estimation using mean of medians strategy, CRMM first takes the median of \(\{y_{t}^{1},,y_{t}^{r}\}\), denoted by \(_{t}\). Subsequently, CRMM computes the gradient with the arm-reward pair \((_{t},_{t})\) through the operation similar to (11). Then, CRMM employs a variant of ONS to update the estimator and construct the confidence region \(_{t+1}\) centered on the new estimator. The details about the constructed confidence region is given in Theorem 3.

Compared to existing bandit algorithms that utilize the median of means strategy, the primary difference lies in the item chosen as the "means". As we have introduced in (7), MENU of Shao _et al._ (2018) uses the distance between different estimators as the "means". BMM of Xue _et al._ (2020) calculates multiple estimated rewards for each arm and treats them as the "means". Both MENU and BMM require \(O( T)\) estimators during each round, whereas CRMM only requires one estimator. Moreover, compared to the mean of medians approach (Zhong _et al._, 2021), CRMM plays each selected arm fewer times, leading to more model updates, which is critical based on experimental results. Since the chosen arm has to be played multiple times, we assume the arm set for CRMM is static, such that \(_{t}=\) for \(t>0\), which is a common assumption (Medina and Yang, 2016; Zhang _et al._, 2016; Lu _et al._, 2019). The following theorem guarantees a tight confidence region.

**Theorem 3**: _If the rewards satisfy (9) and (12), then with probability as least \(1-2\), the confidence region in CRMM is_

\[\|-}_{t+1}\|^{2}_{_{t+1}}_{t+1 }, t 0\]

_where_

\[_{t+1}= (4U^{2}+C t^{}) (1+)+ S^{2}+ }{}t^{},\] \[= 2C(4T/)+2C^{-}rv,\ C=(4v)^{}.\]

With above confidence region, we prove the following regret bound for CRMM.

**Theorem 4**: _If the rewards satisfy (9) and (12), then with probability at least \(1-2\), the regret of CRMM satisfies_

\[R(T) O(d( T)^{+}T^{ }).\]

**Remark:** Theorem 3 clarifies that if the rewards have a finite \(1+\) central moment for some \((0,1]\), CRMM achieves a regret bound of \((dT^{})\). This bound reduces to \((d)\) when \(=1\), indicating that CRMM achieves the same order as the bounded rewards assumption regarding both \(d\) and \(T\)(Zhang _et al._, 2016; Jun _et al._, 2017). Compared to the approach of Zhong _et al._ (2021), CRMM enhances the bound by an order of \(O(( T)^{-})\) for a fixed \((0,1)\) if \(=1\).

## 4 Experiments

This section demonstrates the improvement of our algorithms by numerical experiments. Firstly, we show the effectiveness of our algorithms in dealing with heavy-tailed problems by comparing their regret to that of existing generalized linear bandit algorithms. Secondly, we evaluate the efficiency of our algorithms by comparing their time consumption to other existing algorithms designed for heavy-tailed bandit problems. All algorithms are implemented using PyCharm 2022 and tested on a laptop with a 2.5GHz CPU and 32GB of memory.

### Regret Comparison

To assess the enhancement of our algorithms in handling heavy-tailed problems, we utilize the vanilla GLB algorithms, specifically OL\({}^{2}\)M (Zhang _et al._, 2016) and GLOC (Jun _et al._, 2017), as baselines. Additionally, we incorporate the mean of medians method proposed by Zhong _et al._ (2021) into OL\({}^{2}\)M and GLOC, resulting in another two baselines OL\({}^{2}\)M_mon and GLOC_mon, respectively. All algorithms are configured with \(=1\), \(=0.01\), and \(T=10^{6}\).

Let \(_{*}=/^{d}\), where \(\) is an all-\(1\) vector and \(\|_{*}\|_{2}=1\). The number of arms is set to \(K=20\), and the feature dimension is \(d=10\). Each component of the contextual vector \(_{t}\) is uniformly sampled from the interval \(\), and then normalized to be unit norm, i.e., \(\|_{t}\|_{2}=1\). We tune the width of the confidence region following the common practice in bandit learning (Zhang _et al._, 2016; Jun _et al._, 2017). Precisely, with \(c\) being a tuning parameter searched within \([1e^{-4},1]\), the width of the confidence region for OL\({}^{2}\)M and GLOC are set as \(_{t}=cd(t/+1)\) and \(_{t}=c_{=1}^{t}((_{}^{}}_{} )-y_{})^{2}\|_{}\|^{2}_{_{_{_{ _{_{}_{}}}}^{2}}}\), respectively. In addition, the radius of the confidence region is set as \(cd(4T/)^{}(T/(d)+1)T^{ }\) for CRTM, and \(cd(t/(d)+1)t^{}\) for CRMM. For OL2M_mom and GLOC_mom, the chosen arm is played \(=(16(2T/))^{1/}\) times per round, and \(=0.62\) is close to optimal according to the experiments of Zhong _et al._.

We run 10 repetitions for each algorithm and display the average regret with time evolution. According to the generalized linear bandit model, the observed reward \(y_{t}\) is given by

\[y_{t}=(_{t}^{}_{*})+_{t}\]

where \((x)=}\) is the logit model and \(_{t}\) is the heavy-tailed noises. To evaluate the algorithms performance under both symmetric and asymmetric rewards, \(_{t}\) fits the following two distributions,

1. Student's \(t\)-Noise: \(_{t}}(1+}{3})^{-2}\) where \(G()\) is the Gamma function;
2. Pareto Noise: \(_{t}^{}}{x^{}+1}_{x x_{m}}\) where \(s=3\) and \(x_{m}=0.01\).

Fig. 1 compares our algorithms against two vanilla GLB algorithms (OL2M and GLOC), as well as these two algorithms exploiting mean of medians estimators (OL2M_mom and GLOC_mom). Fig. 1(a) shows that CRTM and CRMM outperform the other four algorithms. CRTM provides the best performance, which is consistent with the theoretical guarantees. OL2M_mom and GLOC_mom appear ineffective at handling heavy-tailed problems, because they update estimator only \(100\) times with the chosen arm played \(\) times [Zhong _et al._, 2021]. Fig. 1(b) presents the cumulative regrets under asymmetric noises, with CRTM still having the lowest regret curve, demonstrating its generality and effectiveness in handling heavy-tailed bandit problems. On the other hand, CRMM, GLOC_mom, and OL2M_mom performs poorly in Fig. 1(b), as they can not deal with the asymmetric rewards.

### Runtime Comparison

To demonstrate the efficiency improvement of our algorithms, we compare them with existing heavy-tailed bandit algorithms such as CRT and MoM [Medina and Yang, 2016], TOFU and MENU [Shao _et al._, 2018], and SupBTC and SupBMM [Xue _et al._, 2020]. Among them, CRT, TOFU and SupBTC employ truncation strategy, while MoM, MENU and SupBMM utilize the median of means strategy.

The experimental settings are the same as described in Regret Comparison section, except for the time horizon and feature dimension. We use a smaller time horizon \(T=10^{4}\) since TOFU is time-consuming. The feature dimension is increased to \(d=100\) to highlight the difference between SupBTC and TOFU. The computational runtimes are shown in Table 2.

For the truncation-based algorithms, CRTM consumes the least time, while TOFU and SupBTC takes over a hundred times longer to execute than CRTM, representing a significant improvement. CRT takes only slightly longer than CRTM as both algorithms update the model online, but the regret bound of CRT is \((dT^{})\), which is \((T^{})\) worse

  Algorithm & Time(s) & Algorithm & Time(s) \\  CRT & 3.1737 & MoM & 0.0630 \\ TOFU & 3931.9963 & MENU & 24.1990 \\ SupBTC & 1187.1863 & SupBMM & 0.0685 \\ CRTM & **2.2909** & CRMM & **0.0514** \\  

Table 2: Runtime comparison

Figure 1: Regret comparison

than the bound of CRTM. For median of means algorithms, CRMM has the shortest runtime. MENU takes significantly longer than the other algorithms because MENU needs to calculate the distance between \(O( T)\) estimators.

## 5 Conclusion and Future Work

We present two algorithms, CRTM and CRMM, for the generalized linear bandit model with heavy-tailed rewards, which utilize the truncation and mean of medians strategies, respectively. Both algorithms achieve the regret bound of \((dT^{})\) conditioned on a bounded \((1+)\)-th moment of rewards, where \((0,1]\). This bound is almost optimal since the lower bound of the stochastic linear bandit problem is \((dT^{})\)(Shao _et al._, 2018). CRTM is the first truncation-based online algorithm for the heavy-tailed bandit problem that handles both symmetric and asymmetric rewards and approaches the optimal regret bound. CRMM enhances the regret bound of the the most related work by a logarithmic factor (Zhong _et al._, 2021). However, CRMM is limited to symmetric rewards and we will investigate to overcome this restriction in the future.