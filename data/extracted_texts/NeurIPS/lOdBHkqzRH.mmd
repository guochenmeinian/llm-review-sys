# Contextual Linear Optimization with Bandit Feedback

Yichun Hu\({}^{1}\)   Nathan Kallus\({}^{1}\)   Xiaojie Mao\({}^{2}\)   Yanchen Wu\({}^{2}\)

\({}^{1}\) Cornell University  \({}^{2}\) Tsinghua University

{yh767, kallus}@cornell.edu

maoxj@sem.tsinghua.edu.cn

wu-yc23@mails.tsinghua.edu.cn

Authors are listed in alphabetical order. Correspondence to Xiaojie Mao: maoxj@sem.tsingua.edu.cn.

###### Abstract

Contextual linear optimization (CLO) uses predictive contextual features to reduce uncertainty in random cost coefficients and thereby improve average-cost performance. An example is the stochastic shortest path problem with random edge costs (e.g., traffic) and contextual features (e.g., lagged traffic, weather). Existing work on CLO assumes the data has fully observed cost coefficient vectors, but in many applications, we can only see the realized cost of a historical decision, that is, just one projection of the random cost coefficient vector, to which we refer as bandit feedback. We study a class of offline learning algorithms for CLO with bandit feedback, which we term induced empirical risk minimization (IERM), where we fit a predictive model to directly optimize the downstream performance of the policy it induces. We show a fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of the optimization estimate, and we develop computationally tractable surrogate losses. A byproduct of our theory of independent interest is fast-rate regret bound for IERM with full feedback and misspecified policy class. We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results.

## 1 Introduction

Contextual linear optimization (CLO) models the use of predictive features (context variables) to improve decision making in linear optimization with random coefficients. In CLO, we consider a decision \(z\) that incurs an uncertain cost \(Y^{}z\) determined by a random cost vector \(Y^{d}\) that is not observed at decision time. We do, however, observe predictive features \(X^{p}\) prior to decision, which help reduce uncertainty. CLO can be expressed either as a contextual stochastic optimization problem or a linear optimization problem where the cost vector is a conditional expectation:

\[v^{*}(x)=_{z}Y^{}z X=x=_ {z}f_{0}(x)^{}z, f_{0}(x)=[Y  X=x].\] (1)

We assume throughout that \(\) is a polytope (\(=(^{})\) for some vertex set \(^{}<\) with \(_{z^{}} z B\)) and \(Y\) is bounded (without loss of generality, \(Y=\{y: y 1\}\)).

CLO has been the focus of much recent work [e.g., 9, 11, 14, 27, 30], due to its relevance to data-driven operational decision making in many applications such as network optimization, portfolio optimization, product recommendation, etc. The key question in these is how to use _data_ to learn a good policy, \(:^{p}\), mapping feature observations to effective decisions. All of this literature studies an offline setting, where a batch of existing data \((X_{i},Y_{i})\) for \(i=1,,n\) is observed. These data are viewed as random draws from the joint distribution of \((X,Y)\), and the goal is to learn an effective decision policy from them. Crucially, this assumes that we fully observe the random costvector \(Y_{i}\)'s, meaning that we fully know the corresponding costs \(Y_{i}^{}z\) for _any_ potential decision \(z\). This may be unrealistic in many applications.

Consider for example the instantiation of Eq. (1) as a stochastic shortest path problem of transporting one unit from a start node to a desired end node, where \(Y\) contains the travel time on each of \(d\) edges, \(z\) the binary decision whether to traverse each edge, and \(\) constrains flow conservation. The data we would need in order to apply existing approaches here would consist of simultaneous observations of the travel times on every single edge, encapsulated in the elements of \(Y_{i}\)'s. However, such ideal observations may not be available. Indeed, modern travel time prediction models are often based on data from historical individual trips. For example, such is the case with Uber Movement data [26; 33], which is based on the total length of historical rides. Namely, instead of observing the entire \(Y_{i}\), we _only_ observe the total travelling time \(C_{i}=Y_{i}^{}Z_{i}\) for the path \(Z_{i}\) in a historical trip \(i\). We term this observation model _bandit feedback_ as it corresponds to observing the cost only of a given decision and not the counterfactual cost of any alternative decision, as in bandit problems .

In this paper, we study the contextual linear optimization problem with bandit feedback and make several contributions. First, we adapt the end-to-end induced empirical risk minimization approach to the bandit feedback setting. This approach allows us to optimize decision policies by directly targeting the decision objective. We provide three different methods to identify the expected cost of any given policy and show how to estimate the expected decision cost from bandit-feedback data. Second, we derive upper bounds on the regret (_i.e._, decision sub-optimality) of the policy that minimizes the estimated objective. Our regret analysis accounts for the misspecification of the policy class and incorporates a margin condition that potentially enables a faster regret rate. This significantly extends existing theory for full-feedback CLO, and as a byproduct, we provide a novel fast rate bound for full-feedback CLO with misspecification. Finally, we demonstrate that existing surrogate losses for full-feedback CLO (such as the well-known SPO+ loss in ) can be adapted to our setting, enabling efficient policy optimization. We empirically test this in simulated shortest path problems and provide practical insights.

### Background: The Full Feedback Setting

We first review two major approaches to the CLO problem with full feedback: an estimate-then-optimize (ETO) approach and an end-to-end induced empirical risk minimization (IERM) approach. To this end, we need to define the generic plug-in policy \(_{f}\) for any given \(f:^{p}^{d}\):

\[_{f}(x)_{z}f(x)^{}z.\] (2)

Note that for any given covariate value \(x\), this corresponds to a linear programming problem with coefficients given by \(f(x)\). An optimal policy in Eq. (1) corresponds to \(_{f_{0}}\). Without loss of generaility, we restrict the value of \(_{f}(x)\) to the set of vertices \(^{}\) of the polytope \(\) with ties broken according to some fixed rule (e.g., a total ordering over \(^{}\) such as lexicographic).

The ETO approach starts with estimating \(f_{0}\) in Eq. (1) by any supervised learning method for predicting \(Y\) given \(X\). This can, _e.g._, be implemented by minimizing a prediction fitness criterion (e.g., sum of squared errors) over a hypothesis class of functions \([^{p}^{d}]\) (e.g., linear functions, decision trees, neural networks). After training an estimator \(\) of \(f_{0}\), the ETO approach then makes decisions according to the induced policy \(_{}\). This approach is a straightforward two-step approach, but it ignores the downstream optimization task in choosing \(\).

In contrast, some recent literature propose to integrate the estimation and the optimization, directly searching for a decision policy to minimize the decision cost. Following , we consider an IERM formulation that minimizes the sample average cost over the class of plug-in policies induced by \(\):

\[^{}_{_{}}_{i=1} ^{n}Y_{i}^{}(X_{i}),\ _{}=\{_{f}:f\}.\] (3)

This approach is end-to-end in that it directly targets the decision-making objective. Recent literature demonstrates that end-to-end approaches like IERM often outperform the ETO approach . The benefits of end-to-end approaches are significant especially in the misspecified setting - that is, when the function class \(\) fails to contain the true expected cost function \(f_{0}\), and the policy class \(_{}\) does not include the globally optimal decision policy \(_{f_{0}}\) [e.g., 11; 14]. While Eq. (3) involves a challenging bi-level optimization due to the definition of \(_{}\), practical computational approximations have been proposed [11; 16].

Nonetheless, the IERM formulation in Eq. (3) requires observing the full feedback \(Y_{i}\)'s in the data. In this paper, we study how to extend this approach to the setting with bandit feedback.

### Our Problem: CLO with Bandit Feedback

We now formally define the data generating process in the bandit feedback setting. Assume we have an offline dataset consisting of \(n\) data points \(=\{(X_{1},Z_{1},C_{1}),,(X_{n},Z_{n},C_{n})\}\) that are independent draws from a distribution \(P\) on \((X,Z,C)\) generated in the following way. We first draw \((X,Z,Y)\) where the \((X,Y)\) distribution is as in the full feedback setting and \(Z\) is generated according to some historical decision policies for data collection. Then we set \(C=Z^{}Y\) and omit the full \(Y\). Below we impose some basic assumptions on the generating process of the historical decision \(Z\).

**Assumption 1**.: _The data generating process satisfies the following two properties:_

1. _(Ignorability)_ \([C Z,X]=Z^{}f_{0}(X)\) _(which could follow from_ \(Z Y X\)_)._
2. _(Coverage)_ \(_{z(Z):\|z\|=1}[(Z^{}z)^{2} X] >0\) _almost surely._

The ignorability condition is a common assumption that plays a fundamental role in learning with partial feedback and causal inference (see the literature in Section 1.3). This condition requires that the assignments of the historical decisions \(Z\) do not depend on any unobserved factor potentially dependent on \(Y\). The coverage condition requires that given the covariates \(X\), historical decisions \(Z\) can explore all directions of the linear span of the constraint set \(\); otherwise it may be impossible to evaluate certain policies from the observed costs of historical decisions. This is an analogue of the overlap assumption in learning with partial feedback and causal inference .

The learning task is to use these data \(\) to come up with a data-driven policy \((x)\) that has low regret

\[()=_{X}f_{0}(X)^{}(X)-_{z}f_{0}(X)^{}z=V()-V(_{f_{0}}),\] (4)

where \(V()=_{X}f_{0}(X)^{}(X)\).

### Background: Off-Policy Evaluation and Optimization

An important special case is when \(^{}=\{(1,0,,0),,(0,,0,1)\}\) is the canonical basis and \(\) is the simplex. This corresponds to choosing one of \(d\) actions (or randomizing between them). The full-feedback problem in this case is cost-sensitive classification . In the bandit feedback case, with the restriction \(Z^{}\), it is known as a logged or offline contextual bandit, and a long line of literature studies the problems of estimating and optimizing \(V()\). It is closely related to causal inference, viewing each component of \(Y\) as the potential outcome for the corresponding action (or treatment), where we only see the realized outcome \(C\) corresponding to the factual action with index \(1\) in \(Z^{}\) and see nothing about counterfactual actions \(^{}\{Z\}\). Two key differentiating characteristics of our problem with a general polytope constraint \(\) are the opportunity to leverage the linear cost structure to extrapolate from the costs of historical decisions to costs of other decisions, and the presence of non-trivial constraints on the output of decision policies.

Going beyond just finite discrete arms,  consider the logged contextual bandit with a _continuum_ of arms, which is not generally representable in terms of the problem in Section 1.2. These works make no restrictions on the relationship between the expected potential outcome and the corresponding action except generic smoothness, and leverage nonparametric approaches such as kernel weighting. Closest to our work is , which imposes a _semiparametric_ assumption on this relationship. This includes our problem (Section 1.2) as a special case under the restriction that, conditional on \(X\), expected outcomes are linear in actions (note that our \([C Z,X]\) is linear in \(Z\) according to ignorability in Section 1.2). Relative to this work, our unique contributions are the use of induced policy classes to naturally deal with the constraints in \(\), the adaptation of computational approximation such as SPO+, and obtaining the fast rates for the regret under misspecification.

## 2 Induced Empirical Risk Minimization with Bandit Feedback

Our IERM approach in the bandit setting follows the same idea of Eq. (3), directly minimizing an estimate of expected costs. However, since we do not observe \(Y_{i}\) in our data, it is not as straightforward as a sample average, \(_{i=1}^{n}Y_{i}^{}(X_{i})/n\). Instead, we need alternative ways to identify \(V()\).

Define \(_{0}(X)=[ZZ^{} X]\) and recall \(f_{0}(X)=[Y X]\), which we will use to characterize the policy value \(V()\). We refer to these functions as _nuisance functions_ following the existing literature on off-policy evaluation and learning in contextual bandits or reinforcement learning , because they are not directly used for decision-making but serve merely as intermediaries for evaluating the policy value. Let \((x,z,c;f,)\) be a score function such that

\[_{P}(X,Z,C;f_{0},_{0})^{}(X)=V(),\ \ ,\] (5)

where \(_{P}\) denotes taking expectation over \(P\). Proposition 1 summarizes a few possible choices.

**Proposition 1**.: _The following choices of \((x,z,c;f,)\) all satisfy Eq. (5):_

1. _(Direct Method)_ \(_{}(x,z,c;f,)=f(x)\)_;_
2. _(Inverse Spectral Weighting)_ \(_{}(x,z,c;f,)=^{}(x)zc\)_;_
3. _(Doubly Robust)_ \(_{}(x,z,c;f,)=f(x)+^{}(x)z(c-z^{}f(x))\)_._

_Here \(^{}\) denotes the Moore-Penrose pseudo-inverse of matrix \(\)._

The doubly robust identification in Proposition 1 is a generalization of the identification in  in that we allow for rank-deficient \(_{0}\) by using a pseudo-inverse. Otherwise, our work significantly differs from  in terms of policy class specification, computation and theoretical analyses (see Section 1.3). Note that identification in Proposition 1 exploits the linearity of the decision cost, which significantly improve upon algorithms that ignore the linear structure and naively extend existing offline bandit learning using discrete actions \(^{}\); see Section 5 and Appendix C.2 for numerical evidence.

It remains to estimate Eq. (5), since we know neither \(P\) nor the nuisance functions \(f_{0},_{0}\). Following the approach of  for logged contextual bandits, we adapt the cross-fitting procedure advocated in . For simplicity, we focus on the twofold version and assume \(n\) is even. The extension to \(K\)-fold version is straightforward.

First, we split \(\) into two equal-sized subsamples, \(_{1}\) and \(_{2}\), each with \(n/2\) data points. We can use \(_{1}\) as an auxiliary sample to estimate the nuisance functions \(f_{0}\) and \(_{0}\), denoting the estimates as \(_{1}\) and \(_{1}\). We then use \(_{2}\) as a main sample to obtain an estimate of \(V()\) using \(_{1}\) and \(_{1}\):

\[_{i_{2}}(X_{i},Z_{i},C_{i};_{1}, _{1})^{}(X_{i}).\]

Of course, we can switch the roles of \(_{1}\) and \(_{2}\), _i.e._, we use \(_{2}\) to get nuisance estimates \(_{2}\) and \(_{2}\), then use \(_{1}\) to estimate \(V()\). Finally, given an induced policy class \(_{}\), the IERM policy \(\) minimizes the average of the above two \(V()\) estimates over \(_{}\):

\[_{_{}}_{j=1}^{2}_ {i_{j}}(X_{i},Z_{i},C_{i};_{3-j},_{3-j })^{}(X_{i}).\] (6)

**Remark 1** (Estimation of \(f_{0}\)).: _The estimator \((x)\) can be obtained by minimizing the least square loss over some appropriate function class \(^{}\):_

\[_{f^{}}_{i} C_{i}-Z_{i}^{}f(X_{i})^{2}.\] (7)

_Note that two places in Eq. (6) require the specification of a function class for modeling \(f_{0}\): the \(\) class used to induce the policy class \(_{}\), and the \(^{}\) class used to construct nuisance estimators to evaluate the expected cost of induced policies. In practice, we do not need to use the same class for \(\) and \(^{}\). In fact, it might be desirable to use a more flexible function class for \(^{}\) to make sure it is well-specified for accurate policy evaluation, and a simpler class for \(\) to make the policy optimization more tractable. We numerically test out different choices of \(\) and \(^{}\) in Section 5._

**Remark 2** (Estimation of \(_{0}\)).: _There are multiple ways to estimate \(_{0}\). For example,  suggest estimating \(_{0}\) by running a multi-task regression for all \((j,k)\) entries to the matrix over some appropriate hypothesis spaces \(_{jk}\): \(=_{_{1}_{11},,_{dd} _{dd}}_{i}\|Z_{i}Z_{i}^{}-(X_{i}) \|_{Fro}^{2}\). To ensure a positive semi-definite estimator, we may posit each hypothesis \(\) to be the outer product of some matrix-valued hypothesis of appropriate dimension. Alternatively, we may only need to consider finitely many feasible decisions \(z_{1},,z_{m}\), such as the feasible paths for stochastic shortest path problems (see experiments in Section 5) or more generally the vertices in \(^{}\). Then we can first estimate the propensity scores \(e_{0}(z X)\) for \(z=z_{1},,z_{m}\) using an suitable estimator \((z X)\) and then estimate \(_{0}(X)=_{j=1}^{m}z_{j}z_{j}^{}e_{0}(z_{j} X)\) by \((X)=_{j=1}^{m}z_{j}z_{j}^{}(z_{j} X)\)._Theoretical Analysis

In this section, we provide a theoretical regret analysis for the IERM approach with bandit feedback, allowing for model misspecification in the induced policy class \(_{}\). That is, we allow the globally optimal policy \(_{f_{0}}\) to be not included in the class \(_{}\). We derive an upper bound on the regret of the IERM approach, in terms of the complexity of the policy class \(_{}\), its misspecification bias, and the estimation errors of nuisance functions.

Before stating the main theorem, we define a few important notations. Let \(^{*}\) be the best-in-class policy that minimizes expected regret over \(_{}\):

\[^{*}_{_{}}f_{0}(X)^ {}(X).\]

We note that \(^{*}\) can be different from the global optimal policy \(_{f_{0}}\) if \(_{f_{0}}_{}\), and \((^{*})\) characterizes the extent of misspecification in the induced policy class \(_{}\). For \(j=1,2\), we define the function class

\[_{j}=(x,z,c)_{j},_{j})^{}((x)-^{*}(x))}{2B}:_{},}.\] (8)

For any function class \(:\), we define the local Rademacher complexity as

\[_{n}(,r)=_{g,\|g\|_{ 2} r}_{i=1}^{n}_{i}g(X_{i},Z_{i},C_{i}) ,\]

where \(_{i}\) are i.i.d. Rademacher variables, and \(\|g\|_{2}=_{P}[g^{2}(X,Z,C)]}\).

Throughout Section 3, we impose two assumptions. Assumption 2 concerns the algorithms we use to estimate the nuisance functions \(f_{0},_{0}\). It requires that the nuisance estimates are close enough to their true values, and \(\|(x,z,c;,)\|\) is bounded.

**Assumption 2** (Nuisance Estimation).: _The nuisance estimators \(,\) trained on a sample of size \(n\) satisfy that \(\|(x,z,c;,)\|\) for all \(x,z,c\), and for any \((0,1)\) and \(_{}\), with probability at least \(1-\),_

\[_{P}(X,Z,C;f_{0},_{0})-(X,Z,C; {f},)^{}((X)-^{*}(X))^{}(n,).\]

In Section 3.1, we will relate \(^{}(n,)\) to the errors in estimating the individual nuisance functions \(f_{0}\) and \(_{0}\) for different choices of score function \(\).

Assumption 3, which we term the margin condition, controls the density of the sub-optimality gap near zero in the CLO problem instance and allows us to get faster regret rates. This type of condition was originally considered in the binary classification literature [2; 38]. It is recently extended to contextual linear optimization by , which we describe below.

**Assumption 3** (Margin Condition).: _Let \(^{*}(x)=_{z}f_{0}(x)^{}z\), and \((x)=_{z^{}^{*}(x)}f_{0}(x)^ {}z-_{z}f_{0}(x)^{}z\) if \(^{*}(x)\) and \((x)=0\) otherwise. Assume for some \(, 0\),_

\[_{X}(0<(X))(/B)^{} >0.\]

Lemmas 4 and 5 in  show that Assumption 3 generally holds with \(=1\) for sufficiently well-behaved \(f_{0}\) and continuous \(X\) and with \(=\) for discrete \(X\). Moreover, any CLO instance trivially satisfies \(=0\). Overall, a larger \(\) means that the sub-optimality gap between the best and second-best decisions tends to be large in more contexts, so it is easier to distinguish the optimal decisions from others. We will show that a larger \(\) parameter could lead to faster regret rates.

### Main Theorem

We now provide an upper bound on the regret of the IERM policy \(\) in Eq. (6).

**Theorem 1**.: _Suppose Assumptions 2 and 3 hold, and \(^{*}(X)\) defined in Assumption 3 is a singleton almost surely. Suppose there exists a positive number \(\) (that depends on \(n\)) that upper bounds the critical radii of the function classes \(_{1},_{2}\) almost surely (i.e., \(_{n/2}(_{1},)^{2}\) and \(_{n/2}(_{2},)^{2}\))._\(^{2}\)) and satisfies the inequalities \(3n^{2}/128_{2}(1/)\), and \(2-3n^{2}/128/4\). Then, there exists a positive constant \((,)\) such that with probability at least \(1-\), we have_

\[() B12(,)} ^{}+24B( ,)}(^{*})}{B}^{}+^{2}\] \[+2^{}(n/2,/4)+2(^{*}).\]

The upper bound in Theorem 1 involves several different types of error terms. The first type is the critical radii \(\) that characterize the complexity of the function classes \(_{1},_{2}\). This is a common complexity measure for function classes in statistics and machine learning . For example, as we will discuss below, the critical radii scale as \(()\) if \(_{1},_{2}\) are VC subgraph classes of dimension \(\). The second type is the term \(^{}(n/2,/4)\) resulted from the errors in estimating the nuisance functions \(f_{0},_{0}\). Similar nuisance estimation errors also appear in the previous literature on offline contextual bandit learning  or more general learning problems that involve nuisance functions . Below we will further discuss this term for different choices of score functions \(\). The third type is the misspecification error term \((^{*})\). It is natural to expect a bigger decision regret when the misspecification error \((^{*})\) is higher. In particular, when the policy class \(_{}\) is well-specified, i.e., \(_{f_{0}}_{}\), we would have \((^{*})=0\), and the regret upper bound would scale with the function class complexity through a fast rate \(O(^{})\). For VC subgraph classes with \(=()\), the rate can range from \(()\) for \(=0\) to \((/n)\) for \(=\). However, if the policy class is misspecified so that \((^{*})>0\), then the dominating term related to \(\) would be a slow rate \(O()\). This reveals an interesting phase transition between the correctly specified and misspecified settings, which is not discovered in the previous theory that considers only well-specification .

**Remark 3**.: _The constant coefficients in the regret upper bound can be improved when \(2(1+)/\) is an integer (which accommodates the case of \(=1\) justified in ):_

\[() B12(,)} ^{}+B (,)}( ^{*})}{B}^{}+^{2}\] \[+^{}(n/2,/ 4)+(^{*}).\]

_Notably, the constant in front of the misspecification error \((^{*})\) becomes \(1\) instead of \(2\), which we believe is tight. This upper bound follows from nearly the same proof as Theorem 1 except that it handles an inequality slightly differently. Specifically, the proof of Theorem 1 involves a transcendental inequality of the form \(() c_{1}()^{} +c_{2}\) for certain positive terms \(c_{1},c_{2}\). This inequality is difficult to solve exactly, so we can only get an upper bound on its solution. It turns out that we can get a better upper bound when \(2(1+)/\) is an integer._

The nuisance estimation rate.We now show that \(^{}(n,)\) can be effectively controlled for the DM, ISW and DR score functions. In pariticular, \(^{}(n,)\) is can be bounded by the estimation errors of the nuisance functions \(f_{0}\) and \(_{0}\).

**Proposition 2**.: _For any given \((0,1)\), let \(_{n,}\) be a positive sequence converging to \(0\) as \(n\), such that the mean square errors of the nuisance estimates satisfy the following with probability at least \(1-\):_

\[_{X}[\|_{()}((X)-f_{0}(X))\|^{2}],_{X}[\|^{}(X)-_{0}^{ }(X)\|_{}^{2}]}_{n,}^{2},\]

_where \(_{()}((X)-f_{0}(X))\) is the projection of \((X)-f_{0}(X)\) onto \(()\). Then,_

1. _If we take_ \(=_{}\)_, we have_ \(^{}_{}(n,)=O(_{n,})\)_;_
2. _If we take_ \(=_{}\)_, we have_ \(^{}_{}(n,)=O(_{n,})\)_;_
3. _If we take_ \(=_{}\)_, we have_ \(^{}_{}(n,)=O(_{n,}^{2}).\)__

Compared to the DM and ISW scores, the impact of the estimation errors of the nuisances in the DR score is of only second order, _i.e._, \(O(_{n,}^{2})\) instead of \(O(_{n,})\). This echos the benefit of DR methodsin causal inference and offline contextual bandit learning [1; 5; 6]. Notably, here we only need to bound the projected error on the nuisance estimator \(\), which handles the setting when \(()\) does not cover the whole \(^{d}\) space, as is the case with the shortest path problem in Section 5.

Computing the critical radius.The critical radius, \(\) characterizes the complexity of the function classes \(_{1}\) and \(_{2}\) defined in Eq. (8). The next proposition shows that \(\) is of order \((1/)\) if the function classes have finite VC-subgraph dimensions. For simplicity, we focus on \(_{1}\) only.

**Proposition 3**.: _Suppose \(_{1}\) has VC-subgraph dimension \(\) almost surely. Then for any \((0,1)\), there exists a universal positive constant \(\) such that_

\[=}\] (9)

_satisfies the inequalities \(_{n}(_{1},)^{2}\), \(3n^{2}/64_{2}(1/)\), and \(2-3n^{2}/64/4\)._

### Byproduct: Fast Rates in the Full Feedback Setting with Misspecification

Although our main theorem is stated for the bandit feedback setting, our regret analysis techniques can be easily adapted to the full feedback setting. The following theorem states a similar regret upper bound. To our best knowledge, this is the first result for CLO that shows a margin-dependent fast rate with potential policy misspecification in the full feedback setting.

**Theorem 2**.: _Suppose \(^{*}(X)\) defined in Assumption 3 is a singleton almost surely. Define_

\[^{}=(x,y)((x)-^{* }(x))}{2B}:_{},}\]

_and \(^{}\) be any solution to the inequality \(_{n}(^{},r) r^{2}\) satisfying \(3n(^{})^{2}/64_{2}(1/^{})\) and \(2-3n(^{})^{2}/64\). If Assumption 3 further holds, then, with probability at least \(1-\),_

\[(^{}) B12(, )}^{}^{}+24B (,)}(^{*})}{B }^{}^{}+(^{})^{2}+2(^{*}).\]

The regret bound is similar to that in Theorem 1 for the bandit setting, except that it does not have the nuisance error term \(2^{}(n/2,/4)\). This is because, in the full feedback setting, we observe the entire \(Y\) vector, so we do not need to estimate any nuisance functions and can consider the nuisance estimation error term \(^{}(n/2,/4)\) as zero. When \(_{}\) is a well-specified VC-subgraph class with dimension \(\), we have \((^{*})=0\), and \(^{}=()\), so the bound in Theorem 2 reduces to \(O((/n)^{(+1)/(+2)}+/n)\). This bound interpolates between \(O(n^{-1/2})\) and \(O(n^{-1})\) according to the margin parameter \(\), recovering the fast rate in the full-feedback setting without misspecification as given in . In contrast, our bound in Theorem 2 additionally quantifies the impact of policy misspecification, and its generalization Theorem 1 further incorporates the impact of nuisance estimation errors in the bandit-feedback setting.

## 4 Computationally Tractable Surrogate Loss

The IERM objective is generally nonconvex in \(f\), making it computationally intractable to optimize. In the full feedback setting, tractable surrogate losses have been proposed [11; 16]. In this section, we briefly explain the SPO+ loss in  and how it can be used in the bandit setting.

The full feedback IERM problem in Eq. (3) can be viewed as minimizing the following loss over \(\):

\[_{f}_{i=1}^{n}l_{}(f(X_{i}),Y_{i }),l_{}(f(x),y)=y^{}_{f}(x)-_{z }y^{}z.\]

This IERM loss is equivalent to the "smart predict-then-optimize" (SPO) loss in Definition 1 of . Letting \(z^{*}(y)_{z}y^{}z\) with the same tie-breaking rule as in \(_{f}\),  propose the SPO+ surrogate loss:

\[_{f}_{i=1}^{n}l_{}(f(X_{i}),Y_{i}),l_{}(f(x),y)=_{z}\;(y-2f(x))^{}z-(y-2f(x))^{ }z^{*}(y).\]

The SPO+ loss has many desirable properties: given any fixed \(y\), it is an upper bound for the IERM loss, it is convex in \(f(x)\), and its subgradient at \(f(x)\) has a closed form \(2(z^{*}(y)-z^{*}(2f(x)-y))\).

In the bandit setting, although \(Y_{i}\) is not observed, the score \((X_{i},Z_{i},C_{i};,)\) plays the same role (see Eqs. (3) and (6)). So it is natural to adapt the SPO+ loss to the bandit setting by replacing the unobserved cost vector \(Y_{i}\) by the corresponding score \((X_{i},Z_{i},C_{i};,)\):

\[_{}_{f}\ _{j=1,2} _{i_{j}}l_{}f(X_{i}),(X_{i},Z_{i},C_ {i};_{3-j},_{3-j}).\]

Then we use the plug-in policy \(_{_{}}\) as the decision policy. This is implemented in the experiments in Section 5. We can similarly adapt any surrogate loss for the full-feedback IERM problem to the bandit feedback setting, simply replacing the cost vector \(Y_{i}\)'s by the corresponding scores.

## 5 Numerical Experiments

We now test the performance of our proposed methods in a simulated stochastic shortest path problem following [11; 14]. Specifically, we aim to go from the start node \(s\) to the end node \(t\) on a \(5 5\) grid consisting of \(d=40\) edges, where the costs of traveling on the edges are given by the random vector \(Y^{40}\) (see Fig. 1). We consider covariates \(X^{3}\) and a function \(f_{0}(x)=[Y X=x]\) whose components are cubic polynomials. The corresponding shortest path problem can be easily formulated into a CLO problem with the constraint set \(\) given by standard flow preservation constraints. The resulting optimal solution \(z\) belongs to \(\{0,1\}\)4, indicating whether passing each edge or not. We note that there are \(m=70\) feasible paths \(z_{1},,z_{m}\) from the source node to the target node, and the feasible paths are linearly dependent with a rank of \(18\).

We consider a bandit feedback setting, observing only the total traveling costs \(C\) of the historical decisions \(Z\) generated by a certain logging policy but not the edge-wise costs \(Y\). We consider different types of logging policies that generate the decisions in the observed data. In this section, we report results using a random logging policy that picks a path from all feasible ones at random regardless of the covariate value. In Appendix C, we further study the performance of two different covariate-dependent logging policies: one that picks paths according to the sign of the first covariate \(X_{1}\), and one that depends on the signs of both \(X_{1}\) and \(X_{2}\). The empirical insights from the covariate-dependent logging policies are qualitatively the same as those for the random logging policy.

We numerically evaluate the performance of the ETO approach and the IERM approach2. For both approaches, we use the same class \(\) to construct the decision policies. We consider three different classes for \(\): a correctly specified polynomial class, a misspecified class that omits two high-order terms (termed degree-2 misspecification), and a misspecified class that omits four high-order terms (termed degree-4 misspecification). For the IERM approach, we consider DM and DR here but defer the results of ISW to Appendix C.2 Table 3 for its significantly worse performance. In both DM and DR, the nuisance \(f_{0}\) is estimated by a bandit-feedback regression given in Eq. (7) with a ridge penalty, and we test out the three aforementioned function classes for the nuisance class \(^{}\) as well. In DR, the nuisance \(_{0}(x)\) is estimated by the propensity score approach described in Remark 2,

Figure 1: Stochastic Shortest path problem on a \(5 5\) grid with uncertain edge cost \(Y_{j}\) and decision \(z_{j}\) for \(j=1,,40\).

with the propensity scores estimated by either sample frequencies (for the random logging policy) or suitable decision tree models (for covariate-dependent logging policies). We further consider three variants when plugging \(\) into the doubly robust score: pseudo-inverse (DR PI) as in the original \(_{}\) definition; lambda regularization (DR Lambda), where we replace \(^{}\) with \((+ I)^{-1}\) for a positive constant \(\); and clipping (DR Clip), where eigenvalues of \(\) below a certain threshold are clipped to the threshold before taking the pseudo-inverse. For all IERM variants, we optimize the SPO+ losses as discussed in Section 4. Further details on the experimentation setup and implementation are summarized in Appendix C. Finally, we consider some naive extensions of the offline contextual bandit learning with finite discrete actions (Naive ETO and Naive SPO+), where we view the feasible paths as separate discrete actions, without considering the linear structure of the decision-making problem. See Appendix A for details and Section 1.3 for background on offline bandits.

We test the performance of different methods on an independent testing sample of size \(2000\), and evaluate the ratio of their regrets relative to the expected cost of the global optimal policy \(_{f_{0}}\). Table 1 shows the average relative regret ratios of different methods across \(50\) replications of the experiment for a random logging policy. Due to space limitations, we include results for the training data size \(n=400,1000,1600\), and defer results for other sizes to Appendix C.2. The relative regret of all methods properly decrease with the training data size \(n\). In particular, the SPO+ approximation for the end-to-end IERM approach perform better than the ETO method. Among the SPO+ methods, the DM score achieves the best performance, while the DR score based pesudo-inverse performs the worst. Through a close inspection, we found that the bias adjustment term that involves the pseudo-inverse in the DR score causes a significant variance inflation. In fact, the ISW score also performs badly due to the high variance (see Appendix C.2). The Lambda regularization and Clip techniques can effectively reduce the variance and result in improved decision-making. Moreover, the naive benchmarks that ignore the linear structure of the decision costs have much worse performance. This shows the importance of leveraging the linear problem structure.

In Table 2, we show the performance of different methods when either the policy-inducing model \(\) or the nuisance model \(^{}\) or both are misspecified. We observe that when the policy-inducing model is misspecified, the end-to-end SPO+ methods perform much better than the ETO method, provided that the nuisance model for SPO+ is correctly specified. This is consistent with the findings in the previous full-feedback CLO literature (e.g., 11; 12; 14), showing the benefit of integrating

    & Methods &  &  \\   & & 400 & 1000 & 1600 & 400 & 1000 & 1600 \\   Well-specified \\ Nuisance Model \\ \(^{}\) \\  } & ETO & \) misspecified degree 2} & \) misspecified degree 4} \\   & & 11.04\% & 9.14\% & 8.34\% & 12.35\% & 11.42\% & 10.39\% \\   Well-specified \\ Nuisance Model \\ \(^{}\) \\  } & SPO+ DM & 2.81\% & 0.80\% & 0.54\% & 4.06\% & 2.21\% & 2.06\% \\  & SPO+ DR PI & 3.27\% & 1.36\% & 1.05\% & 4.83\% & 2.95\% & 2.71\% \\  & SPO+ DR Lambda & 2.83\% & 0.97\% & 0.73\% & 4.33\% & 2.45\% & 2.25\% \\  & SPO+ DR Clip & 3.05\% & 1.09\% & 0.84\% & 4.59\% & 2.62\% & 2.38\% \\   Well-specified \\ Policy-inducing \\ Model \(\) \\  } & SPO+ DM & 10.01\% & 8.37\% & 7.47\% & 12.51\% & 11.22\% & 9.68\% \\  & SPO+ DR PI & 9.11\% & 7.02\% & 6.44\% & 11.69\% & 10.19\% & 9.02\% \\  & SPO+ DR Lambda & 9.05\% & 7.52\% & 6.68\% & 12.31\% & 10.38\% & 8.96\% \\  & SPO+ DR Clip & 9.02\% & 7.28\% & 6.36\% & 11.87\% & 10.04\% & 8.70\% \\   Both \(,^{}\) \\ Misspecified \\  } & SPO+ DM & 9.90\% & 8.34\% & 7.41\% & 12.45\% & 11.16\% & 9.69\% \\  & SPO+ DR PI & 9.15\% & 7.23\% & 6.52\% & 11.92\% & 10.46\% & 9.42\% \\  & SPO+ DR Lambda & 9.03\% & 7.46\% & 6.74\% & 12.01\% & 10.72\% & 9.25\% \\  & SPO+ DR Clip & 8.97\% & 7.22\% & 6.46\% & 11.75\% & 10.31\% & 8.95\% \\   

Table 2: Mean relative regret ratio of different methods when the nuisance model \(^{}\) and the policy-inducing model \(\) are misspecified to different degrees. The logging policy is a random policy.

estimation and optimization for misspecified policy models. However, the advantage of the end-to-end approaches is weakened dramatically once the nuisance model is misspecified. In this case, the evaluation of decision policies is biased, so the end-to-end approaches also target a "wrong" objective that may not accurately capture the decision quality. Moreover, we observe that when the nuisance model is misspecified, the DR score can somewhat outperform the DM score, because it can debias the misspecified nuisance to some extent. These results demonstrate new challenges with the bandit-feedback CLO: the end-to-end approaches are sensitive to the misspecification of nuisance models, and the DM and DR scores face different bias-and-variance tradeoffs under nuisance model misspecification. Therefore, in practice we may prefer more flexible models for accurate nuisance modeling, while using simpler policy-inducing models for tractable end-to-end optimization.

## 6 Conclusions

This paper studies the bandit-feedback setting for contextual linear optimization for the first time. We adapt the induced empirical risk minimization approach to this setting, provide a novel theoretical analysis for the regret of the resulting policies, leverage surrogate losses for efficient optimization, and empirically demonstrate the performance of the proposed methods across different model specifications. Our paper has a few limitations that we aim to address in the future. First, we primarily consider parametric induced policy classes, and it would be interesting to accommodate more general nonparametric classes. Second, we focus mainly on the SPO+ surrogate loss, and investigating other surrogate losses in the bandit feedback setting would also be of great interest.