# Ntkcpl: Active Learning on Top of Self-Supervised Model by Estimating True Coverage

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

High annotation cost has driven extensive research in active learning and self-supervised learning. Recent research has shown that in the context of supervised learning, when we have different numbers of labels, we need to apply different active learning strategies to ensure that it outperforms the random baseline. This number of annotations that change the suitable active learning strategy is called the phase transition point. We found, however, when combining active learning with self-supervised models to achieve improved performance, the phase transition point occurs earlier. It becomes challenging to determine which strategy should be used for previously unseen datasets. We argue that existing active learning algorithms are heavily influenced by the phase transition because the empirical risk over the entire active learning pool estimated by these algorithms is inaccurate and influenced by the number of labeled samples. To address this issue, we propose a novel active learning strategy, neural tangent kernel clustering-pseudo-labels (NTKCPL). It estimates empirical risk based on pseudo-labels and the model prediction with NTK approximation. We analyze the factors affecting this approximation error and design a pseudo-label clustering generation method to reduce the approximation error. Finally, our method was validated on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a wider range of training budgets.

## 1 Introduction

The boom in deep learning models in recent years stems in part from the massive amounts of data [11; 17; 23]. However, the demand for large amounts of data, especially labeled data, in turn, constrains the application of deep learning models, since large amounts of labels imply high annotation costs [41; 1; 45]. Active learning is a path to alleviate the cost of labeling by selecting informative subsets of samples to annotate.

However, the benefits of active learning have been increasingly questioned in recent years [25; 28]. One of the main concerns is that training a model initialized by self-supervised learning with randomly selected labeled samples often yields results far beyond those obtained by existing active learning with supervised training (randomly initialized or initialized by the last round of the active learning model) [6; 8; 7; 14; 9]. Because the latter only uses labeled data to train the network, while the former uses a large amount of unlabeled data to train the backbone of the network. Since most existing active learning algorithms are designed in the context of supervised training, they must be validated with a large number of labels compared to the number of labels required in training from a self-supervised model. This means that the effectiveness of these active learning algorithms is not guaranteed in the case of having access to relatively few annotations, as is the case when combining with a self-supervised model. Several studies [15; 42; 4] have shown that many existing activelearning strategies fail to outperform the random baseline when combining them with self-supervised learning. In this paper, we focus on designing an active learning strategy that works well in the training method with a self-supervised model.

The "phase transition" phenomenon  is known to occur in active learning with supervised training. It refers to the fact that an active learning strategy that outperforms a random baseline when the total number of labels is small will be inferior to a random baseline when the total number of labels is large (called the **low-budget** strategy) and vice versa (called the **high-budget** strategy). We note that when combining active learning with the self-supervised model, the cut-off point between low-budget and high-budget strategy occurs much earlier. For example, in the CIFAR-100 , the cut-off point is about 10,000 labeled samples when training in the supervised learning way . But, the cut-off point shifts forward to about 1,500 labeled samples when training from a self-supervised model. The forward-moving cut-off point means that even if the annotation budget is low (only one order of magnitude above the number of classes in the dataset), it is likely to hit that cut-off point. Thus, for a previously unseen dataset, it is difficult to simply determine whether a low-budget or high-budget strategy should be chosen since the difficulty varies from dataset to dataset. In this paper we use this problem to motivate the design of an active learning strategy with a wider effective budget range.

Since existing low-budget strategies are designed based on the idea of feature space coverage , we first analyze the problems of determining coverage based on sample feature distances in sec. 2. After that, we propose that the true coverage where the empirical risk is zero, can be estimated based on pseudo-labels and predictions of the model trained on the candidate set. Based on this, we propose our active learning strategy, Neural Tangent Kernel Clustering-Pseudo-Labels (NTKCPL), which uses the NTK  and CPL to approximate empirical risk on active learning pool in sec. 3.2. And we analyze which factor affects approximation error in sec. 3.3. Based on this analysis, we design a CPL generation method in sec. 3.4. Extensive experimental results demonstrate that our method outperforms state-of-the-art approaches in most cases and has a wider effective budget range. As part of the results (sec. 4) we also show our method is effective for self-supervised features of different quality.

Our contribution is summarized as follows: (1) We propose a novel active learning strategy, NTKCPL, by estimating empirical risk on the whole active learning pool based on pseudo-labels. (2) We analyze the approximation error of the empirical risk in the active learning pool when NTK and CPL are used to approximate networks and true labels. (3) Our method outperforms both low- and high-budget active learning strategies within a range of annotation quantities one order of magnitude larger than traditional low-budget active learning experiments. This means that our approach can be used more confidently for active learning on top of self-supervised models than existing low-budget strategies.

### Related Work

Most active learning strategies are designed and validated in the high-budget scenario where network weights are randomly initialized or initialized from the weights of the previous active learning round. Active learning methods mainly include uncertainty-based sampling , feature space coverage , the combination of uncertainty and diversity , learning-based methods , and so on . Moreover, some recent studies explore **"look ahead"** strategies , where samples are selected based on the model trained on candidate training sets. However, with the development of self-supervised training, the training approach for low-budget scenarios has shifted to training based on a self-supervised pre-trained model . This change in the training method implies a shift in the total number of samples that need to be selected by active learning. When training based on a self-supervised model, often only 0.4-6% of the total data needs to be labeled to achieve similar results to training with 20-40% labeled data on a randomly initialized network . Recent studies have shown that there exists a phase transition phenomenon in active learning strategies, whereby opposite strategies should be adopted in high-budget and low-budget scenarios , causing many active learning strategies designed for high-budget scenarios unsuitable for training based on a self-supervised model. As a result, recent studies have explored active learning strategies specifically designed for low-budget scenarios . However, we find that these strategies are effective only when the number of labeled data samples is extremely small, and as we increase the labeled data to one order of magnitude above the number of classes of the dataset, their performance falls below that of the random baseline.

## 2 Insight: Distance is not an accurate indicator of empirical risk

The goal of the active learning is to find a labeled subset, \(D_{C}=(x_{i},y_{i})_{i=1}^{N_{C}}\), such that the model trained on that subset, \(f_{D_{C}}\), has the minimized empirical risk in the entire active learning pool, \(D=(x_{i},y_{i})_{i=1}^{N}\) as shown in eq. 1.

\[argmin_{D_{C}}_{i D}Loss(f_{D_{C}}(x_{i}),y_{i})\] (1)

Unfortunately, during active learning, we do not have the labels of the entire active learning pool, so we cannot compute this loss directly. To address this problem, current methods  covert empirical risk minimization into feature space coverage based on Lipschitz continuity. Although Lipschitz continuity guarantees that the difference between the model's predictions is less than the product of the Lipschitz constant and the difference between inputs, it does not guarantee that their predictions fall into the same class. In practice, we cannot determine the true coverage because we do not know the distance threshold beyond which the model would change its predicted class for unlabeled samples.

Therefore, the current solution is to minimize the coverage radius assuming full coverage  or to maximize coverage based on high purity coverage , where purity refers to the probability that the sample has the same label within a given distance. Assuming full coverage leads to an overestimated coverage as shown in fig. 0(a), i.e., some covered samples still have a large empirical risk, while high-purity coverage causes underestimated coverage as shown in fig. 0(b). The overestimated coverage may cause the active learning algorithm to miss samples in areas that are not truly covered, while underestimated coverage makes active learning algorithms likely to select redundant samples. These affect the performance of active learning.

Additionally, estimating the empirical risk based on distance implies the assumption that model predictions are only relevant to the nearest labeled sample, which is often not the case in reality. To estimate the true coverage, we propose a new strategy, NTKCPL. It estimates the empirical risk based on the predictions of the model trained on the candidate set and pseudo-labels.

## 3 Method: NTKCPL

In sec. 3.1, we briefly review the Neural Tangent Kernel (NTK)  that enables active learning strategies based on the outputs of a model trained on a candidate set feasible. Then, we propose our active learning strategy, NTKCPL, in sec. 3.2 and analyze the approximation error of NTKCPL in sec. 3.3. Finally, based on the analysis, we introduce the method of generating cluster pseudo-label in sec. 3.4.

Figure 1: Coverage estimation based on sample feature distance vs. NTKCPL. Here different colors represent different categories, the black star denotes labeled samples and the blue circle represents the samples considered covered based on the feature distance approach. Coreset assumes full coverage and Prokover assumes high purity coverage. The coverage estimated by our method, NTKCPL, and true coverage based on predictions of the neural network is represented by black dots. The coverage estimated by NTKCPL is more consistent with the true coverage of the neural network than those estimated based on feature distances.

### Preliminaries

Neural Tangent Kernel (NTK) is a powerful tool to analyze the training dynamics of neural network. Jacot et al.  show that the neural network is equivalent to the kernel regression with Neural Tangent Kernel when network is sufficiently wide and its weights are initialized properly . The NTK, \(\), is shown in eq. 2, where the \(f\) denotes a neural network with parameters \(\) and \(\) denotes train samples. When training with MSE loss, the neural network has a closed-form solution for the prediction of test sample \(x\) at iteration \(t\) as eq. 3, where \(\) denotes labels of trainset and \(f_{0}\) denotes the output of network with initialized weights.

\[(,)=_{}f()_{ }f()^{T}\] (2)

\[f_{t}(x)=f_{0}(x)+(x,)\,(, )^{-1}(-e^{-t(,)})( -f_{0}()),\] (3)

Additionally, for active learning scenarios, Mohamad [26; 27] proposes the computation time of using NTK can be further reduced by considering the block structure of the matrix, which means that look ahead type active learning strategies can be implemented in a reasonable amount of time. For example, as shown in , if we want to use the look ahead active learning strategy, each active learning cycle takes 3 hours to train the entire network of 15 epochs on the MNIST dataset, while it takes only 3 minutes to use NTK with a block structure.

### Framework

We propose a look ahead strategy, NTKCPL, to approximate the empirical risk on the whole active learning pool directly. There are two challenges: (1) estimate empirical risk without labels and (2) estimate predictions of models trained with candidate sets efficiently and accurately.

For the first challenge, clusters on self-supervised features provide good pseudo-labels. Because most samples in the same cluster have the same label . And when the number of clusters is increased, it can improve the purity of clusters, where purity refers to the probability that the sample has the same label within the same cluster. We call these clusters clustering-pseudo-labels (CPL), \(y_{cpl}\).

For the second challenge, as introduced in sec. 3.1, NTK approximates the network well for random initialization and the computation time is acceptable. However, in our scenario, training on top of the self-supervised model, NTK does not approximate predictions of the whole network well. The main reason is that weights of the neural network are initialized by self-supervised learning rather than NTK initialization, i.e., drawn i.i.d. from a standard Gaussian . In addition, the self-supervised initialization provides the neural network with a powerful feature representation capability that is not available in NTK. This leads to inconsistency between NTK predictions and network outputs. So, in our method, the NTK is used to approximate the classifier instead of the whole network. And the inputs of NTK are self-supervised features. Accordingly, we choose a training method following  that freezes the encoder initialized by self-supervised learning and trains only the MLP as a classifier. That training method achieves better or equal performance than fine-tuning the whole network in the low-budget case while its prediction is more consistent with the results of NTK. We denotes the predictions of NTK with trainset \(D_{C}\) as \(_{D_{C}}\). Now, the active learning goal in eq. 1 is approximated as eq. 4.

\[argmin_{D_{C}}_{i D}Loss(_{D_{C}}(x_{i}),y_{cpl,i})\] (4)

The algorithm is shown in Alg. 1. For computational simplicity and without loss of generality, we use 0-1 loss to calculate empirical risk in eq. 4. In each round of active learning, after computation of NTK based on eq. 2 and generation of CPL based on the method introduced in sec. 3.4, the sample that minimizes the empirical risk on the whole active learning pool after adding labeled set is selected.

### NTKCPL Approximate Error

In this section, we analyze what affects the accuracy of NTKCPL estimates of empirical risk on the whole active learning pool. The difference between the true empirical risk and the estimatedempirical risk using NTK and CPL is shown in eq. 5. The approximation error can be divided into two terms, the first one is the difference between NTK and neural network prediction, \(error_{NTK}\), and the second one is the difference caused by CPL during NTK estimation, \(error_{CPL}\). For the \(error_{NTK}\), as we mentioned in sec. 3.2, NTK is used to approximate the classifier only to obtain better consistency. To analyze \(error_{CPL}\), we start with the relationship between the predictions of NTK trained with the ground truth, \(_{y}(x_{i})\), and CPL, \(_{cpl}(x_{i})\).

\[_{i D}|Loss(f(x_{i}),y_{i})-Loss((x _{i}),y_{cpl,i})|\] (5) \[ _{i D}(|Loss(f(x_{i}),y_{i})-Loss( (x_{i}),y_{i})|+|Loss((x_{i}),y_{i})-Loss((x _{i}),y_{cpl,i})|)\]

**Definition** Denotes the \(j^{th}\) output of \(_{cpl}\) as \(_{cpl}^{j}\). Label mapping function \(g\) converts NTK's predictions about CPL classes, \(_{cpl}(x_{i})\), into predictions about true classes, \(_{ymap}(x_{i})\), based on dominant labels within corresponding CPL classes as shown in eq. 6, where \(D_{dom}\) is a set of index \(k\), where \(j\) is the dominant true label classes within CPL class, \(y_{cpl,k}\).

\[_{ymap}^{j}(x_{i})=_{k D_{dom}}_{cpl}^{k}(x_{i})\] (6)

**Proposition** If the true labels of labeled samples are the dominant labels in their corresponding CPL clusters, \(_{y}(x_{i})=g(_{cpl}(x_{i}))\). We defer the proof to appendix 1.

\[error_{CPL}=P_{nff}+P_{fnf}\] (7)

As mentioned in sec. 3.2, we use 0-1 loss to calculate empirical risk. We can expand \(error_{CPL}\) as eq. 7, where we denote the probability that the NTK prediction agrees with the \(y\) but not with \(y_{cpl}\) as \(P_{fnf}\), and the probability that the NTK prediction does not agree with \(y\) but agrees with \(y_{cpl}\) as \(_{nff}\). According to the proposition, we argue \(argmax_{y}(x_{i})\) is most likely equal to \(g(argmax_{cpl}(x_{i}))\).

\(P_{fnf}\) refers to the case where different CPL classes correspond to the same true label class, i.e., over-clustering. \(P_{nff}\) means that the true label of a sample is different from the dominant true label within its CPL class, i.e., the CPL class includes samples from different true label classes, which is called impurity. Detailed explanations and empirical evidence can be found in appendix 1.

### Cluster Pseudo-Labels

As shown by eq. 7, the effect of CPL on the approximation error comes from the purity of the clusters and over-clustering. To improve clustering purity, we take two approaches: (1) clustering on the active learning feature, i.e., the output of the penultimate layer of the classifier, and (2) increasing the number of clusters. However, increasing the number of clusters may cause the labeled samples not to cover all classes of the CPL (under-coverage) and also increase the over-clustering error. For example, a group of samples with the same true label is clustered into \(K\) different classes. Even though NTK incorrectly predicts some samples as other CPL classes, their true empirical risk is zero.

To improve the under-coverage, we set the number of clusters to half of the total number of labels, i.e., each cluster includes two labeled samples on average. To improve the over-clustering, we manually set the maximum number of clusters and design a clustering-splitting approach instead of directly increasing the number of clusters. It splits the low-purity clusters and keeps the high-purity ones to reduce the extra over-clustering errors within samples located in the high-purity clusters. Specifically, we use the prediction of the neural network in each round of active learning to estimate the number of confusing samples within each cluster, i.e., the number of samples from classes that are different from the dominant class. The clusters that contain the largest number of confusing samples are split sequentially until a predefined number of clusters is reached. The cluster splitting algorithm is shown in Alg. 2, where we adopt the constrained K-Means  to improve the clusters from labeled sample constraints.

## 4 Experiment Results

Our approach is validated on five datasets with various qualities of self-supervised features. Datasets with good self-supervised features, such as CIFAR-10 , CIFAR-100 , and ImageNet-100 (a subset of ImageNet , following splitting in ), are included. SVHN  with poor self-supervised features is also included. Additionally, we consider practical scenarios where the total number of samples in the trainset is insufficient to support effective self-supervised training, such as Oxford-IIIT Pet dataset . In this case, we evaluated the effectiveness of our method based on the model pre-trained on ImageNet .

BaselineWe compare our proposed method with representative active learning strategies: (1) Random, (2) Entropy (uncertainty sampling, maximum entropy of output) , (3) Coreset (diversity active learning strategy, greedy solution of minimum coverage radius) , (4) BADGE (combination of uncertainty and diversity, kmeans++ sampling on grad embedding) , where the scalable version [10; 12], badge partition, is used in ImageNet-100, CIFAR-100 and Oxford-IIIT Pet because the huge dimension of grad embedding (5) Typicalust (designed for low-budget case) , (6) Lookahead (maximum output change based on NTK) .

ImplementationOur method focuses on the low-budget regime, we followed the training method in , freezing weights of backbone initialized with self-supervised learning and then training a MLP as the classifier. The hyperparameters for training are set following  and can be found in appendix 3. For the self-supervised model, we adopt simsiam  for CIFAR-10, CIFAR-100 and SVHN and BYOL  for ImageNet-100 and Oxford-IIIT Pet. Resnet-18  is used in CIFAR-10

[MISSING_PAGE_FAIL:7]

baseline strategies at the beginning of active learning, but it shows better results than baselines after several active learning rounds as shown in fig. 1(c).

Another common scenario is the lack of sufficient samples to support effective self-supervised training. To evaluate in this context, we choose the Oxford-IIIT Pet dataset with the self-supervised model trained on ImageNet. The result is shown in fig. 1(d). Our method has similar accuracy in the first three rounds as the TypiClust and outperforms all baseline methods afterward.

NTKCPL has a wider effective budget range than SOTA.Active learning based on self-supervised models exhibits an intensified phase transition phenomenon. We plot the active learning gain of our method and baselines on different datasets in fig. 3. The average accuracy of our method, NTKCPL(al), outperforms the random baseline at all quantities of labels. In contrast, both the typical high-budget strategy, BADGE, and low-budget strategy, TypiClust, appear to be worse than the random baseline over a range of annotation quantities. We show the effective budget range of our method, NTKCPL, as well as the typical high-budget strategy, BADGE, and the typical low-budget strategy, TypiClust, across all experiments in table 2. The effective budget ratio refers to the proportion of the effective annotation quantity to the total annotation quantity, where the effective annotation quantity refers to the number of annotations at which active learning accuracy exceeds the random baseline (avg. + std.).

### Ablation Study

In this section, we evaluate the coverage estimation of our method and the effect of the maximum cluster number on NTKCPL. Also, we compare the effect of generating CPL on self-supervised features as well as on the active learning feature on the performance of NTKCPL.

    & Effective Budget Ratio \\  TypiClust & 40.8\% \\ BADGE & 42.0\% \\ NTKCPL(al) & 92.7\% \\   

Table 2: Comparison of the effective budget ratio of different active learning strategies.

Figure 2: Performance of different active learning strategies. The shaded area represents std.

Coverage EstimationWe conducted experiments on CIFAR-100, where the coverage indicates the proportion of samples that are correctly predicted. The estimated coverage of NTK with true label and with CPL is shown in fig. 4. Our method approximates the true coverage well for most cases.

Effect of the Maximum Number of CPLThe ablation experiments are conducted on CIFAR-10. We plot the accuracy when the number of annotations selected by active learning is greater than 400 as shown in fig. 5. In this range, the number of classes of CPL is fixed at 10, 50, 100, and 200, respectively. The experimental results support our analysis in sec. 3.4 that too many or too few clusters will increase the approximation error, which affects the performance of active learning.

Effect of self-supervised feature-based and active learning feature-based clustering-pseudo-labels on NTKCPL.We denote NTKCPL based on active learning features as NTKCPL(al) and NTKCPL based on self-supervised learning feature as NTKCPL(self). The results are shown in table 1 and fig. 2. From these experiments, we found that clustering on active learning features yields better results except for the case where the number of annotations is very small. Also, NTKCPL(self) is better than NTKCPL(al) in a wide range of annotation quantities (no more than 500), when self-supervised features are good such as experiment in the CIFAR-10.

## 5 Conclusion

We study the active learning problem when training on top of a self-supervised model. In this case, an intensified phase transition is observed and it influences the application of active learning. We propose NTKCPL that approximates empirical risk on the whole pool more directly. We also analyze the approximation error and design a CPL generation method based on the analysis to reduce the approximation error. Our method outperforms SOTA in most cases and has a wider effective budget range. The comprehensive experiments show that our method can work well on self-supervised features with different qualities.

Our approach is limited to the fixed training approach, i.e., training the classifier on top of a frozen self-supervised training encoder, which is restricted to the low-budget scenario because the fine-tuning training approach provides higher accuracy in the high-budget case. Therefore, (1) how to accurately approximate the fine-tuning model initialized with self-supervised weights using NTK and (2) whether the samples selected by our current method have good transferability for the fine-tuning would be interesting future directions.