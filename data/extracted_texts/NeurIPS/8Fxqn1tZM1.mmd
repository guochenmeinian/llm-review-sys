# Scale Equivariant Graph Metanetworks

Ioannis Kalogeropoulos\({}^{1,2}\)1 Giorgos Bouritsas\({}^{1,2}\)1 Yannis Panagakis\({}^{1,2}\)

\({}^{1}\)National and Kapodistrian University of Athens \({}^{2}\)Archimedes/Athena RC, Greece

Equal contribution. Correspondence to ioakalogero@di.uoa.gr, g.bouritsas@athenarc.grINR: _A coordinate-processing NN "overfitted" on a given signal, e.g. image, 3D shape, physical quantity._

###### Abstract

This paper pertains to an emerging machine learning paradigm: learning higher-order functions, i.e. functions whose inputs are functions themselves, _particularly when these inputs are Neural Networks (NNs)_. With the growing interest in architectures that process NNs, a recurring design principle has permeated the field: adhering to the permutation symmetries arising from the connectionist structure of NNs. _However, are these the sole symmetries present in NN parameterizations_? Zooming into most practical activation functions (e.g. sine, ReLU, tanh) answers this question negatively and gives rise to intriguing new symmetries, which we collectively refer to as _scaling symmetries_, that is, non-zero scalar multiplications and divisions of weights and biases. In this work, we propose _Scale Equivariant Graph MetaNetworks - ScaleGMNs_, a framework that adapts the Graph Metanetwork (message-passing) paradigm by incorporating scaling symmetries and thus rendering neuron and edge representations equivariant to valid scalings. We introduce novel building blocks, of independent technical interest, that allow for equivariance or invariance with respect to individual scalar multipliers or their product and use them in all components of ScaleGMN. Furthermore, we prove that, under certain expressivity conditions, ScaleGMN can simulate the forward and backward pass of any input feedforward neural network. Experimental results demonstrate that our method advances the state-of-the-art performance for several datasets and activation functions, highlighting the power of scaling symmetries as an inductive bias for NN processing. The source code is publicly available at https://github.com/jkalogero/scalegmn.

## 1 Introduction

Neural networks are becoming the workhorse of problem-solving across various domains. To solve a task, they acquire rich information which is stored in their learnable parameters throughout training. Nonetheless, this information is often opaque and difficult to interpret. This begs the question: _How can we efficiently process and extract insights from the information stored in the parameters of trained neural networks, and how to do so in a data-driven manner?_ In other words, can we devise architectures that learn to process other neural architectures?

The need to address this question arises in diverse scenarios: _NN post-processing_, e.g. analysis/interpretation (i.e. inferring NN properties, such as generalisation and robustness ), as well as editing (e.g. model pruning , merging  or adaptation to new data) - or _NN synthesis_ (e.g. for optimisation  or more generally parameter prediction/generation ). Furthermore, with the advent of _Implicit Neural Representations_,1 trained NN parameters are increasingly used to represent datapoint _signals_, such as images or 3D shapes, replacing raw representations, i.e., pixel grids or point clouds . Consequently, many tasks involving such data,across various domains such as computer vision  and physics , which are currently tackled using domain-specific architectures (e.g. CNNs for grids, PointNets  or GNNs  for point clouds and meshes), could potentially be solved by _NNs that process the parameters of other NNs_.

The idea of processing/predicting NN parameters per se is not new to the deep learning community, e.g. it has been investigated in _hypernetworks_ or even earlier . However recent advancements, driven by the need for INR processing, leverage a crucial observation: _NNs have symmetries_. In particular, several works have identified that the function represented by an NN remains intact when applying certain transformations to its parameters [28; 11] with the most well-known transformations being _permutations_. That is, hidden neurons can be arbitrarily permuted within the same layer, along with their incoming and outgoing weights. Therefore, all the tasks mentioned in the previous paragraph can be considered part of _equivariant machine learning_, as they involve developing models that are invariant or equivariant to the aforementioned transformations of neural networks. Recently Navon et al.  and Zhou et al.  acknowledged the importance of permutation symmetries and first proposed equivariant architectures for Feedforward NN (FFNN) processing, demonstrating significant performance improvements against non-equivariant baselines. These works, as well as their improvements  and extensions to process more intricate and varying architectures [33; 44; 88], are collectively known as _weight space networks_ or _tranetworks_, with _graph metanetworks_ being a notable subcase (graph neural networks with message-passing).

Nevertheless, it is known that permutations are _not_ the only applicable symmetries. In particular, theoretical results in various works [11; 61], mainly unified in  show that FFNNs exhibit additional symmetries, which we collectively here refer to as _scaling symmetries_: multiplying the incoming weights and the bias of a hidden neuron with a non-zero scalar \(a\) (with certain properties) while dividing its outgoing weights with another scalar \(b\) (often \(b=a\)), preserves the NN function. Intuitively, permutation symmetries arise from the _graph/connectionist structure_ of the NN whereas different scaling symmetries originate from its _activation functions_\(\), i.e. when it holds that \((ax)=b(x)\). However, equivariance w.r.t. scaling symmetries has not received much attention so far (perhaps with the exception of _sign symmetries_ - \(a\{-1,1\}\)[41; 40]), and it remains effectively unexplored in the context of metanetworks.

To address this gap, we introduce in this paper a graph metanetwork framework, dubbed as _Scale Equivariant Graph MetaNetworks- ScaleGMN_, which guarantees equivariance to permutations _and_ desired scaling symmetries, and can process FFNNs of arbitrary graph structure with a variety of activation functions. At the heart of our method lie novel building blocks with scale invariance/equivariance properties w.r.t. arbitrary families of scaling parameters. We prove that ScaleGMN can simulate the forward and backward pass of an FFNN for arbitrary inputs, enabling it to reconstruct the function represented by any input FFNN and its gradients.

Our contributions can be summarised as follows:

* We extend the scope of metanetwork design from permutation to scaling symmetries.
* We design invariant/equivariant networks to scalar multiplication of individual multipliers or combinations thereof, originating from arbitrary scaling groups.
* We propose scale equivariant message passing, using the above as building blocks, unifying permutation and scale equivariant processing of FFNNs. Additionally, the expressive power of our method is analysed w.r.t. its ability to simulate input FFNNs.
* we first characterise this using a technique from ) on several datasets and three tasks (INR classification/editing & generalisation prediction), demonstrating superior performance against common metanetwork baselines.

## 2 Related work

**Neural network symmetries.** Long preceding the advent of metanetworks, numerous studies delved into the inherent symmetries of neural networks. Hecht-Nielsen  studied FFNNs and discovered the existence of permutation symmetries. Chen et al.  showed that, in FFNNs with tanh activations, the only function-preserving smooth transformations are permutations and sign flips (multiplication of incoming and outgoing weights with a sign value) - this claim was strengthened in ; this was the first identification of scaling symmetries. Follow-up works extended these observations to other activations, such as _sigmoid and RBF_ and _ReLU_[56; 51; 64; 61; 26], and architectures, such as RNNs [3; 2], characterising other scaling symmetries and providing conditions under which permutations and scalings are the only available function-preserving symmetries or the parameters are identifiable given the input-output mapping. Recently, Godfrey et al.  provided a technique to characterise such symmetries for arbitrary activations that respect certain conditions, unifying many previous results. Additionally, symmetries have been found in other layers, e.g. batch norm [6; 14; 16] (scale invariance) or softmax  (translation invariance). Prior to treamethworks, symmetries were mainly studied to obtain a deeper understanding of NNs or in the context of optimisation/learning dynamics [56; 71; 4; 17; 36; 5; 83; 84] and/or model merging/ensembling [20; 1; 69; 59; 55].

**Metanetworks.** The first solutions proposed for NN processing and learning did not account for NN symmetries. Unterthiner et al.  and Eilertsen et al. , initially employed standard NNs on vectorised (flattened) CNN weights or some statistics thereof, to predict properties of trained neural networks such as generalization or estimation of a hyperparameter resp.). Similar methods have been proposed to process continuous data represented as INRs. In  high-order spatial derivatives are used (suitable only for INRs), in  the architecture operates on stacked parameter vectors (but is constrained by the assumption that all INRs are trained from the same initialization), while in  and  the authors learn low-dimensional INR embeddings (which are further used for downstream tasks) by jointly fitting them with meta-learning. Finally, Schurholt et al.  learn representations of NN weights with self-supervised learning and in  this is extended to generative models.

In contrast to the above methods, our work follows a recent stream of research focusing on _equivariant metanetworks_. Navon et al.  and Zhou et al.  first characterised _all linear_ equivariant layers to permutation symmetries of MLPs and combined them with non-linearities, while in the latter this was extended to CNNs. These approaches derive intricate weight-sharing patterns but are non-local2 and cannot process varying architectures. In follow-up works, Zhou et al.  constructed equivariant attention layers and in  the above characterisation was generalised to arbitrary input architectures and layers (e.g. RNNs and transformers) introducing an algorithm for automatic linear layer (with weight-sharing) construction. A different route was taken in the very recent works of Kofinas et al.  and Lim et al. , where input NNs are treated as a special class of graphs and are naturally processed with Graph Neural Networks (GNNs), with appropriate symmetry breaking wherever necessary. This perspective has been adopted several times in the deep learning literature [9; 23; 45; 77; 80] and in a few examples, GNNs were applied on the graph representation for, e.g. neural architecture search ([82; 72; 31]), albeit without mapping parameters to the graph edges. In parallel to our work,  extended the framework of  to incorporate scaling and sign-flipping symmetries. They construct non-local equivariant layers, resulting in an architecture with fewer trainable parameters. Nevertheless, their method suffers from limited expressive power, while their experimental results showcase limited advancements. In contrast to our method, this approach limits the choice of activation functions to those equivariant to the studied symmetries. Finally, their framework cannot be extended to other activation functions, as the parameter sharing must be re-designed from scratch, while it is not suitable for processing diverse architectures.

**Scale Equivariance** Equivariance to vector scaling (or more generally to matrix multiplication with diagonal matrices) remains to date underexplored in the machine learning community. Sign symmetries received attention in the work of Lim et al. , where an _invariant_ network (_SignNet_) was designed, mainly to process eigenvectors, with the theoretical analysis revealing universality under certain conditions. This was extended in , where the importance of sign _equivariance_ on several tasks was demonstrated and a sign equivariant network was proposed. In our work, we draw inspiration from these architectures and extend their formulations to arbitrary scaling symmetries.

## 3 Notation and Preliminaries

**Notation.** We denote vectors, matrices, and tensors with bold-face letters, e.g., \(,,\), respectively and sets with calligraphic letters \(\). A normal font notation will be used for miscellaneous purposes (mostly indices and functions). Datapoint (input) functions/signals will be denoted with \(u\), while higher-order functions (functions of functions) will be denoted with fraktur font \(\).

**Functions of Neural Networks.** Consider functions of the form \(u_{G,}:}\). Each function is parameterised (1) by a **computational graph**\(G\), which determines all the mathematical operationsthat should be performed to evaluate the function \(u\) at a datapoint \(\). When \(u\) is a neural network, \(G\) is determined by the NN _architecture_. Additionally, \(u\) is parameterised by (2), by **a tuple of numerical parameters \(\)**, on which the aforementioned mathematical operations are applied (along with the input \(\)) - these are the _learnable parameters_, which can be arranged into a vector. We are interested in learning unknown higher-order functions \(:}^{}\) of the form \((u_{G,})\). In our case, the goal is to _learn \(\) by accessing solely the parameters \((G,)\) of each \(u\)_, i.e. via functions \(:\) of the form \((G,)\), where \(\) is a space of architectures/computational graphs and \(\) a space of parameters.3 We are typically interested in learning _functionals_ (\(^{d}\)) or _operators_ (\(=\) or \(=\)). To approximate the desired higher-order function, we assume access to a dataset of parameters sampled i.i.d. from an unknown distribution \(p\) on \(\). For example, in a supervised setup, we aim to optimise the following: \(*{argmin}_{f}_{(G,)  p}L\!(u_{G,}),( G,)\), where \(L(,)\) is a loss function and \(\) is an NN processing hypothesis class (e.g. metanetworks).

**Feedforward Neural Networks (FFNNs).** In this paper, we focus our analysis on Feedforward Neural Networks (FFNNs), i.e. linear layers interleaved with non-linearities. Consider NN of the form \(u_{G,}:^{d_{}}^{d_{}}\) of the following form:

\[_{0}=,_{}=_{}(_{}_{-1}+_{}), u_{G,}()=_{L}\] (1)

where \(L\): the number of layers, \(_{i}^{d_{} d_{-1}}\): the weights of the NN, \(_{i}^{d_{}}\): the biases of the NN, \(d_{0}=d_{}\), \(d_{L}=d_{}\), \(_{}:\) activation functions applied element-wise. Here, the learnable parameters are \(=(_{1},,_{L},_{1}, ,_{L})\) and the computational graph encodes the connections between vertices, _but also the type of activations used in each layer_.

**Neural Network symmetries.** One of the major difficulties with working with function parameters directly is that the same function can be represented with more than one parameter, i.e. there exists transformations that _when applied to any parameter \((G,)\), keep the represented function intact_. Formally, an NN symmetry is induced by a set \(\) of transformations \(:\), such that \(u_{G,}()=u_{(G,)}(),,(G,) \). If for a pair parameters \((G,)\), \((G^{},^{})\), \(\) such that \((G,)=(G^{},^{})\), we will call the two parameters _equivalent_ and write \((G,)(G^{},^{})\). To appropriately represent a _functional_\(\), a hypothesis (metanetwork) \(\) should be _invariant_ to transformations in \(\): \(((G,))=(G, )\). For _operators_, \(\) should be _equivariant_ to transformations: \(f((G,))=(f(G,))\), such that identical functions map to identical functions.

**Permutation symmetries (connectionist structure).** For a fixed computational graph \(G\), perhaps the most well-known symmetry of FFNNs are those induced by hidden neuron permutations . _As far as metanetworks are concerned it is to date the only NN symmetry that has been accounted for_ - see Section 2. This symmetry implies that permuting hidden neurons (along with their biases and incoming and outgoing weights) within each layer preserves the NN function (regardless of the activation function). This reads:

\[_{}^{}=_{}_{}_{-1}^{ -1},\;_{}^{}=_{}_{} (_{}^{},_{}^{})_{ =1}^{L}=^{}=(_{ },_{})_{=1}^{L},\] (2)

where \(\{1,,L\}\), \(_{0}=_{L}=\) and \(_{}^{d_{} d_{}}\) are arbitrary permutation matrices. Observe that they are different for each layer, with the input and output neurons held fixed.

**Graph Metanetworks (GMNs).** A recently introduced weight space architecture , takes advantage of the permutation symmetries and treats FFNNs (among others, e.g. CNNs) as graphs, processing them with conventional GNNs. In particular, let \(G=(,)\) be the computational graph, \(i\) an arbitrary vertex in the graph (neuron) and \((i,j)\) an arbitrary edge from vertex \(j\) to vertex \(i\).4 Additionally, let \(_{V}^{|| d_{v}}\) be the vertex features and \(_{E}^{|| d_{c}}\) the edge features (i.e. biases and weights resp. in a FFNN). The general form of a \(T\) iteration (layer) GMN reads:

\[_{V}^{0}(i)=_{V}(_{V}(i )),_{E}^{0}(i,j)=_{E}(_{ E}(i,j))\] (Init) \[_{V}^{t}(i)=_{j(i)}\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!where \(_{V}^{i},_{E}^{i}\) are vertex and edge representations at iteration \(t\) and \(_{G}\) is the overall graph (NN) representation. \(\), \(\), \(\) are general function approximators (e.g. MLPs), while \(\) is a permutation invariant aggregator (e.g. DeepSets ). The above equations have appeared with several variations in the literature, e.g. in some cases the edge representations are not updated or the readout might involve edge representations as well. Another frequent strategy is to use _positional encodings_\(_{V},_{E}\) to break undesired symmetries. In FFNNs, Eq. (2) reveals that input and output vertices are not permutable, while vertices cannot be permuted across layers. Therefore, vertices (or edges) that are permutable share the same positional encoding (see Appendix A.1.2 for more details).

**Remark:** Although, typically, the neighbourhood \((i)\) contains both incoming and outgoing edges, in Section 5 we will illustrate our method using only incoming edges: _forward neighbourhood_\(_{}(i)=\{j(i)-(j)=1\}\) and _backward_ where layer \((i)\) gives the layer neuron \(i\) belongs. Backward neighbourhoods \(_{}(i)\) are defined defined similarly. In Appendix A.2, we show a more elaborate _bidirectional version_ of our method, with both neighbourhoods considered.

## 4 Scaling symmetries in Feedforward Neural Networks

**Scaling symmetries (activation functions).** Intuitively, permutation symmetries stem from the _graph structure_ of neural networks, or put differently, from the fact that hidden neurons do not possess any inherent ordering. Apart from the affine layers \(_{}\) that give rise to the graph structure, it is frequently the case that **activation functions**\(_{}\) have inherent symmetries that are bestowed to the NN.

Let us dive into certain illustrative examples: for the \(\) activation \((x)=(x,0)\) it holds that \((ax)=(ax,0)=a(x,0),\  a>0\). For the \(\) and \(\) activations \((x)=(x)\), \((x)=(x)\) respectively, it holds that \((ax)=a(x),\  a\{-1,1\}\). In a slightly more complex example, polynomial activations \((x)=x^{k}\), we have \((ax)=a^{d}(x)\), i.e. the multiplier differs between input and output. In general, we will be talking about _scaling symmetries_ whenever there exist pairs \((a,b)\) for which it holds that \((ax)=b(x)\). To see how such properties affect NN symmetries, let us focus on FFNNs (see Appendix A.3 for CNNs): for a neuron \(i\) (we omit layer subscripts) we have \(a(i,:)+a(i)= (i,:)+(i)\), i.e. _multiplying its bias and all incoming weights with a constant \(a\) results in scaling its output with a corresponding constant \(b\)_. Generalising this to linear transformations, we may ask the following: which are the pairs of matrices \((,)\) for which we have \(+=+\)? Godfrey et al.  provide an answer for _any activation that respects certain conditions_. We restate here their most important results:

**Proposition 4.1** (Lemma 3.1. and Theorem E.14 from ).: _Consider an activation function \(:\). Under mild conditions,5 the following hold:_

* _For any_ \(d^{+}\)_, there exists a (non-empty) group of invertible matrices defined as:_ \(I_{,d}=\{^{d d}:\,^{d d}()=()\}\) _(_intertwiner group_)_, and a mapping function_ \(_{,d}\) _such that_ \(=_{,d}()\)_._
* _Every_ \( I_{,d}\) _is of the form_ \(\)_, where_ \(\)_: permutation matrix and_ \(=q_{1}, q_{d}\) _diagonal, with_ \(q_{i} D_{}=\{a\{0\}(ax)=_{, 1}(a)(x)\}\)_: the 1-dimensional group, and_ \(_{,d}()=_{,1}(q_{1 }),_{,1}(q_{d})\)_._

This is a powerful result that completely answers the question above for most practical activation functions. Importantly, not only does it recover permutation symmetries, but also reveals symmetries to diagonal matrix groups, which can be identified by solely examining \(_{,1}\), i.e. the one-dimensional case and the set \(D_{}\) (easily proved to be a group) we have already discussed in our examples above.

Using this statement, Godfrey et al.  characterised various activation functions (or recovered existing results), e.g. \(\): \(I_{,d}\) contains **generalised permutation matrices with positive entries** of the form \(\), \(=(q_{1},,q_{d})\), \(q_{i}>0\) and \(_{,d}()=\). Additionally, here we characterise the intertwiner group of \(\) (used in the popular SIREN architecture  for INRs). Not surprisingly, it has the same intertwiner group with \(\) (we also recover this here using Proposition 4.1). Formally, (proof in Appendix A.7.1):

**Corollary 4.2**.: _Hyperbolic tangent \((x)=(x)\) and sine activation \((x)=( x)\), satisfy the conditions of Proposition 4.1, when (for the latter) \( k,k\). Additionally, \(I_{,d}\) contains **signed permutation matrices** of the form \(\), with \(=(q_{1},,q_{d})\), \(q_{i}= 1\) and \(_{_{d}}()=\)._It is straightforward to see that the symmetries of Proposition 4.1, induce equivalent parameterisations for FNNNs. In particular, it follows directly from Proposition 3.4. in , that for activation functions \(_{}\) satisfying the conditions of Proposition 4.1 and when \(_{,}()=\), we have that:

\[^{}_{}=_{}_{}_{} _{-1}^{-1}_{-1}^{-1},\;^{}_{} =_{}_{}_{}(^{ }_{},^{}_{})_{=1}^{L}=^{ }=(_{},_{})_{=1 }^{L},\] (3)

where again \(\{1,,L\}\), \(_{0}=_{0}=_{L}=_{L}=\).

## 5 Scale Equivariant Graph MetaNetworks

As previously mentioned, the metanetworks operating on weight spaces that have been proposed so far, either do not take any symmetries into account or are invariant/equivariant to permutations alone as dictated by Eq. (2). In the following section, we introduce an architecture invariant/equivariant to **permutations and scalings**, adhering to Eq. (3). An important motivation for this is that in various setups, _these are the only function-preserving symmetries_, i.e. for a fixed graph \(u_{G,}=u_{G,^{}}( ,^{})\) satisfy Eq. (3) - e.g. see  for the conditions for tanh and  for ReLU.

**Main idea.** Our framework is similar in spirit to most works on equivariant and invariant NNs . In particular, we build equivariant GMNs that will preserve both symmetries at vertex- and edge-level, i.e. _vertex representations will have the same symmetries with the biases and edge representations with the weights_. To see this, suppose two parameter vectors are equivalent according to Eq. (3). Then, the _hidden_ neurons representations - the discussion on _input/output_ neurons is postponed until Appendix A.1.4 - should respect the following (the GMN iteration \(t\) is omitted to simplify notation):

\[^{}_{V}(i) =q_{}(_{}(i))_{V}( _{}(i)),=(i)\{1, ,L-1\}\] (4) \[^{}_{E}(i,j) =q_{}(_{}(i))_{E} (_{}(i),_{-1}(j))q_{-1}^{-1 }(_{-1}(j)),\;=(i) \{2,,L-1\},\] (5)

where \(_{}:_{}_{}\) permutes the vertices of layer \(\) (denoted with \(_{}\)) according to \(_{}\) and \(q_{}:_{}\{0\}\) scales the vertex representations of layer \(\) according to \(_{}\). We will refer to the latter as _forward scaling_ in Eq. (4) and _bidirectonal scaling_ in Eq. (5). To approximate _operators_ (equivariance), we compose multiple equivariant GMN layers/iterations and in the end, project vertex/edge representations to the original NN weight space, while to approximate _functionals_ (invariance), we compose a final invariant one in the end summarising the input to a scalar/vector.

To ease exposition, we will first discuss our approach w.r.t. vertex representations. Assume that vertex representation symmetries are preserved by the initialisation of the MPNN - Eq. (Init) - and so are edge representation symmetries for all MPNN layers. Therefore, we can only focus on the message passing and vertex update steps - Eq. (Msg) and Eq. (Upd). Additionally, let us first focus on hidden neurons and assume only forward neighbourhoods. The following challenges arise:

**Challenge 1 - Scale Invariance / Equivariance.** First off, the message and the update function \(_{V},_{V}\) should be _equivariant to scaling_ - in this case to the forward scaling using the multiplier of the central vertex \(q_{}(i)\). Additionally, the readout READ, apart from being permutation invariant should also be _invariant to the different scalar multipliers of each vertex_. Dealing with this requires devising functions of the following form:

\[g_{i}q_{1}_{1},,q_{n}_{n}=q_{i}g_{i}( _{1},,_{n}, q_{i} D_{i},i\{1, ,n\}\] (6)

where \(D_{i}\) a 1-dimensional scaling group as defined in Proposition 4.1. Common examples are those discussed in Section 4, e.g. \(D_{i}=\{1,-1\}\)**or**\(D_{i}=^{+}\). The first case, i.e. _sign symmetries_, has been discussed in recent work . Here we generalise their architecture into arbitrary scaling groups. In specific, _Scale Equivariant_ networks follow the methodology of , i.e. they are compositions of multiple linear transformations multiplied elementwise with the output of _Scale Invariant_ functions:

\[^{k}()=^{k}( }_{1},,}_{n}),\] (ScaleInv.Net) \[=^{K}^{1},\; ^{k}()=_{1}^{k}_{1}, ,_{n}^{k}_{n}^{k}( ),\] (ScaleEquiv.Net)

where \(^{k}:_{i=1}^{n}_{i}^{_{i=1}^{n}d_{i }^{k}}\) universal approximators (e.g. MLPs), \(_{i}^{k}:_{i}^{d_{i}^{k}}\) linear transforms (for each of the \(k\) invariant/equivariant layers resp - in practice, we observed experimentally that a single layer \(K=1\) was sufficient), and \(}_{i}\) are explained in detail in the following paragraph.

Central to our method is defining a way to achieve invariance. One option is _canonicalisation_, i.e. by defining a function \(:\)_that maps all equivalent vectors \(\) to a representative_ (obviouslynon-equivalent vectors have different representatives): If \(\), then \(()=()\). In certain cases, these are easy to define and _differentiable almost everywhere_, for example for positive scaling: \(()=}{\|\|}\).6 For sign symmetries, this is not as straightforward: in 1-dimension one can use \(|x|\), but for arbitrary dimensions, a more complex procedure is required, as recently discussed in . Since the group is small - two elements - one can use _symmetrisation_ instead , as done by Lim et al. : \(()=_{:}()\), i.e. for sign: \(()=()+(-)\). Therefore, we define: \(^{k}()=^{k}(}_{1}, ,}_{n})\), with \(}_{i}=(_{i})\) or \(}_{i}=(_{i})\). Importantly, it is known that both cases _allow for universality_, see  and  respectively.

**Challenge 2 - Rescaling: Different multipliers.** Scale equivariance alone is not sufficient, since the input vectors of the message function \(_{V}\) are _scaled by different multipliers_ - central vertex: \(q_{}(i)\), neighbour: \(q_{-1}(j)\), edge: \(q_{}(i)q_{-1}^{-1}(j)\), _while its output should be scaled differently as well_ - \(q_{}(i)\). We refer to this problem as _rescaling_. Dealing with Challenge 2 requires functions of the form:

\[gq_{1}_{1}, q_{n}_{n}=g(_{1},_{n})_{i=1}^{n}q_{i}, q_{i} D_{i}.\] (7)

We call these functions _rescale equivariant_. Note, that this is an unusual symmetry in equivariant NN design. Our approach is based on the observation that _any n-other monomial containing variables from all vectors \(_{i}\) is rescale-equivariant_. Collecting all these monomials into a single representation is precisely the _outer product_\(_{n}=_{1}_{n}\), where \(_{n}(j_{1},,j_{n})=_{i=1}^{n}_{i}(j_{i})\). Therefore, the general form of our proposed Rescale Equivariant Network is as follows:

\[(_{1},_{n})= (_{n}).\] (ReScale Equiv. Net)

In practice, given that the size of \(_{n}\) grows polynomially with \(n\), we resort to a more computationally friendly subcase, i.e. _hadamard products_, i.e. \((_{1},_{n})=_{i=1}^{n} _{i}_{i},_{i}:_{i} ^{d}\). Contrary to the original formulation, the latter is linear (lack of multiplication with an invariant layer).

**Scale Equivariant Message Passing.** We are now ready to define our message-passing scheme. Starting with the message function, we require each message vector \(_{V}(i)\) to have the same symmetries as the central vertex \(i\). Given the scaling symmetries of the neighbour and the edge, for forward neighbourhoods, this reads: \(_{V}(q_{x},q_{y},q_{x}q_{y}^{-1})=q_{x}_{V}(,,)\). In this case, we opt to eliminate \(q_{y}\) by multiplication as follows:

\[_{V}(,,)= ([,(,) ]),\] (8)

where \([,]\) denotes concatenation, \((q_{y},q_{x}q_{y}^{-1})=q_{x}(,)\). In our experiments, we used only \(\) and \(\), since we did not observe significant performance gains by including the central vertex \(\). Now, the update function is straightforward to implement since it receives vectors with the same symmetry, i.e. it should hold that: \(_{V}(q_{x},q_{x})=q_{x}_{V}( ,)\) which is straightforward to implement with a scale equivariant network, after concatenating \(\) and \(\):

\[_{V}(,)=( [,]).\] (9)

Finally, to summarise our graph into a single scalar/vector we require a scale and permutation-invariant readout. The former is once more achieved using canonicalised/symmetrised versions of the vertex representations of hidden neurons, while the latter using a DeepSets architecture as usual:

\[_{V}():=(}_{1},,}_{n}),}_{ i}=_{i}(_{i})}_{i}=_{i}(_{i})\] (10)

In Appendix A.1, we show how to handle symmetries in the rest of the architecture components (i.e. _initialisation_, _positional encodings_, _edge updates and i/o vertices_) and provide an extension of our method to _bidirectional message-passing_ (Appendix A.2), which includes backward neighbourhoods.

**Expressive power.** Throughout this section, we discussed only scaling symmetries and not permutation symmetries. However, it is straightforward to conclude that ScaleGMN is also _permutation equivariant/invariant_, since it is a subcase of GMNs; if one uses universal approximators in their message/update functions (MLP), our corresponding functions will be expressible by this architecture, which was proved to be permutation equivariant in . Although this implies that GMN can express ScaleGMN, this is expected since **PQ** symmetries are more restrictive than just **P**. Note that these symmetries are always present in FFNNs, and thus it is desired to explicitly model them, to introduce a more powerful _inductive bias_. Formally, on symmetry preservation (proved in Appendix A.7.2):

**Proposition 5.1**.: _ScaleGMN is permutation & scale **equivariant**. Additionally, ScaleGMN is permutation & scale **invariant** when using a readout with the same symmetries._

Finally, we analysed the ability of ScaleGMN to evaluate the input FFNN and its gradients, i.e. _simulate the forward and the backward pass of an input FFNN_. To see why this is a desired inductive bias, recall that a functional/operator can often be written as a function of input-output pairs (e.g. via an integral on the entire domain) or of the input function's derivatives (e.g. via a differential equation). By simulating the FFNN, one can reconstruct function evaluations and gradients, which an additional module can later combine. Formally (proof and precise statement in Appendix A.7.2):

**Theorem 5.2**.: _Consider an FFNN as per Eq. (1) with activation functions respecting the conditions of Proposition 4.1. Assume a Bidirectional-ScaleGMN with sufficiently expressive message and vertex update functions. Then, ScaleGMN can simulate both the forward and the backward pass of the FFNN for arbitrary inputs, when ScaleGMN's iterations (depth) are \(L\) and \(2L\) respectively._

## 6 Experiments

**Datasets.** We evaluate ScaleGMN on datasets containing NNs with three popular activation functions: sine, ReLU and tanh. The former is prevalent in INRs, which in turn are the most appropriate testbed for GMNs. We experiment with the tasks of _(1) INR classification_ (invariant task), i.e. classifying functions (signals) represented as INRs and _(2) INR editing_ (equivariant), i.e. transforming those functions. Additionally, ReLU and tanh are common in neural classifiers/regressors. A popular GMN benchmark for those is _(3) generalisation prediction_ (invariant), i.e. predicting a classifier's test accuracy. Here, classifiers instead of FFNNs are typically CNNs (for computer vision tasks) and to this end, we extend our method to the latter in Appendix A.3. We use existing datasets that have been constructed by the authors of relevant methods, are publicly released and have been repeatedly used in the literature (8 datasets in total, 4 for each task). Finally, we follow established protocols: we perform a hyperparameter search and use the best-achieved validation metric throughout training to select our final model. We report the test metric on the iteration where the best validation is achieved.

   Method & MNIST & F-MNIST & CIFAR-10 & Augmented CIFAR-10 \\  MLP & \(17.55 0.01\) & \(19.91 0.47\) & \(11.38 0.34\)* & \(16.90 0.25\) \\ Inr2Vec  & \(23.69 0.10\) & \(22.33 0.41\) & - & - \\ DWS  & \(85.71 0.57\) & \(67.06 0.29\) & \(34.45 0.42\) & \(41.27 0.026\) \\ NFN\({}_{}\) & \(78.50 0.23\)* & \(68.19 0.28\)* & \(33.41 0.01\)* & \(46.60 0.07\) \\ NFN\({}_{}\) & \(79.11 0.84\)* & \(68.94 0.64\)* & \(28.64 0.07\)* & \(44.10 0.47\) \\ NG-GNN  & 91.40 \(\) 0.60 & \(68.00 0.20\) & 36.04 \(\)0.44* & \(45.70 0.20\)* \\  ScaleGMN (Ours) & \(96.57 0.10\) & \(80.46 0.32\) & 36.43 \(\) 0.41 & \(56.62 0.24\) \\ ScaleGMN-B (Ours) & **96.59 \(\) 0.24** & **80.78 \(\) 0.16** & **38.82 \(\) 0.10** & **56.95 \(\) 0.57** \\   

Table 1: INR classification on MNIST, F-MNIST, CIFAR-10 and Aug. CIFAR-10. We train all methods on 3 seeds and report the mean and std. (*) denotes the baselines trained by us and we report the rest as in the corresponding papers. Colours denote **First**, Second and Third.

   Method & CIFAR-10-GS & SVHN-GS & CIFAR-10-GS & SVHN-GS & CIFAR-10-GS \\  & ReLU & ReLU & Tanh & Tanh & both act. \\  StatNN  & \(0.9140 0.001\) & \(0.8463 0.004\) & \(0.9140 0.000\) & \(0.8440 0.001\) & \(0.915 0.002\) \\ NFN\({}_{}\) & \(0.9190 0.010\) & \(0.8586 0.003\) & \(0.9251 0.001\) & \(0.8580 0.004\) & \(0.922 0.001\) \\ NFN\({}_{}\) & \(0.9270 0.001\) & \(0.8636 0.002\) & \(0.9339 0.000\) & \(0.8586 0.004\) & \(0.934 0.001\) \\ NG-GNN  & \(0.9010 0.060\) & \(0.8549 0.002\) & \(0.9340 0.001\) & \(0.8620 0.003\) & \(0.931 0.002\) \\  ScaleGMN (Ours) & \(0.9276 0.002\) & **0.8689 \(\) 0.003** & \(0.9418 0.005\) & **0.8736 \(\) 0.003** & \(0.941 0.006\) \\ ScaleGMN-B (Ours) & **0.9282 \(\) 0.003** & \(0.8651 0.001\) & **0.9425 \(\) 0.004** & \(0.8655 0.004\) & **0.941 \(\) 0.000** \\   

Table 2: Generalization pred.: Kendall-\(\) on subsets of SmallCNN Zoo w/ ReLU/Tanh activations.

**Baselines.** DWS  is a _non-local_ metaetwork that uses all linear permutation equivariant/invariant layers interleaved with non-linearities. \(_{}\) is mathematically equivalent to DWS, while \(_{}\) makes stronger symmetry assumptions in favour of parameter efficiency. The two variants are also designed to process CNNs contrary to DWS. NG-GNN  converts each input NN to a graph (similar to our work) and employs a GNN (in specific PNA ). Importantly, the last three methods transform the input parameters with random Fourier features while all methods perform input normalisations to improve performance and facilitate training. These tricks are in general not equivariant to scaling and were unnecessary in the ScaleGMN experiments (more details in Appendix A.4). On INR classification we include a naive MLP on the flattened parameter vector and \(\), a task-specific non-equivariant method. For generalization prediction, we also compare to StatNN , which predicts NN accuracy based on statistical features of its weights/biases.

**INR classification.** We design a ScaleGMN with permutation & _sign_ equivariant components (and invariant readout). Additionally, we experiment with the bidirectional version denoted as _ScaleGMN-B_. We use the following datasets of increasing difficulty: _MNIST INR, F-MNIST INR_ (grayscale images) and _CIFAR10-INR_, _Aug. CIFAR10-INR_: INRs representing images from the MNIST , FashionMNIST  and CIFAR  datasets resp. and an augmented version of CIFAR10-INR containing 20 different INRs for each image, trained from different initialisations (often called _views_). The reported metric is _test accuracy_. Note that  use only the augmented dataset and hence we rerun all baselines for the original version. All input NNs correspond to functions \(u_{G,}:^{2}^{c}\), i.e. pixel coordinate to GS/RGB value. Further implementation details can be found in Appendix A.4.1. As shown in Table 1, _ScaleGMN consistently outperforms all baselines in all datasets considered_, with performance improvements compared to the state-of-the-art ranging from approx. 3% (CIFAR-10) to 13% (F-MNIST). While previous methods often resort to additional engineering strategies, such as probe features and advanced architectures  or extra training samples  to boost performance, in our case this was possible using vanilla ScaleGMNs. For example, in MNIST and F-MNIST, NG-GNN achieves \(94.7 0.3\) and \(74.2 0.4\) with 64 probe features which is apprpx. 2% and 6% below our best performance. The corresponding results for NG-T  (transformer) were \(97.3 0.2\) and \(74.8 0.9\), still on par or approx. 6% below ScaleGMN. Note that all the above are orthogonal to our work and can be used to further improve performance.

**Predicting CNN Generalization from weights.** As in prior works. we consider datasets of image classifiers and measure predictive performance using Kendall's \(\). We select the two datasets used in , namely CIFAR-10-GS and SVHN-GS originally from _Small CNN Zoo_. These contain CNNs with ReLU or tanh activations, which exhibit scale and sign symmetries respectively. To assess the performance of our method (1) on each activation function individually and (2) on a dataset with heterogeneous activation functions we discern distinct paths for this experiment. In the first case, we split each dataset into two subsets each containing the same activation and evaluate all baselines. As shown in Table 2, once again _ScaleGMN outperforms all the baselines in all the examined datasets_. This highlights the ability of our method to be used across different activation functions and architectures. Performance improvements here are less prominent due to the hardness of the task, a phenomenon also observed in the comparisons between prior works. Note that in this case, additional symmetries arise by the softmax classifier , which are currently not accounted for by none of the methods. We additionally evaluate our method on _heterogeneous activation functions_.

   Method & F-MNIST & 
 CIFAR-10-GS \\ ReLU \\  \\  DWS  & \(67.06 0.29\) & — \\ DWS  + aug. & \(71.42 0.45\) & — \\ \(_{}\) & \(68.19 0.28\) & \(0.9190 0.010\) \\ \(_{}\) + aug. & \(70.34 0.13\) & \(0.8474 0.01\) \\ \(_{}\) & \(68.94 0.64\) & \(0.9270 0.001\) \\ \(_{}\) + aug. & \(70.24 0.47\) & \(0.8906 0.01\) \\ NG-GNN  & \(68.0 0.20\) & \(0.9010 0.060\) \\ NG-GNN  + aug. & \(72.01 1.4\) & \(0.8855 0.02\) \\  ScaleGMN (Ours) & \(80.46 0.32\) & \(0.9276 0.002\) \\ ScaleGMN-B (Ours) & \(80.78 0.16\) & \(0.9282 0.003\) \\   

Table 4: Ablation: Permutation equivariant models + scaling augmentations.

   Method & MSE in \(10^{-2}\) \\  MLP & \(5.35 0.00\) \\ DWS  & \(2.58 0.00\) \\ \(_{}\) & \(2.55 0.00\) \\ \(_{}\) & \(2.65 0.01\) \\ \(\) & \(2.38 0.02\) \\ \(\) & \(2.06 0.01\) \\  ScaleGMN (Ours) & 2.56 \(\) 0.03 \\ ScaleGMN-B (Ours) & **1.89 \(\) 0.00** \\   

Table 3: Dilating MNIST INRs. MSE between the reconstructed and ground-truth image.

In principle, our method does _not_ impose limitations regarding the homogeneity of the activation functions of the input NNs - all one needs is to have a different canonicalisation/symmetrisation module for each activation. Experiments on CIFAR-10-GS show that ScaleGMN yields superior performance compared to the baselines, significantly exceeding the performance of the next-best model. Further implementation details can be found in Appendix A.4.2.

**INR editing.** Here, our goal is to transform the weights of the input NN, to modify the underlying signal to a new one. In this case, our method should be _equivariant_ to the permutation and scaling symmetries, such that every pair of equivalent input NNs is mapped to a pair of equivalent output NNs. Hence, we employ a ScaleGMN similar to the above experiments but omit the invariant readout layer. Following , we evaluate our method on the MNIST dataset and train our model to dilate the encoded MNIST digits. Further implementation details can be found in Appendix A.4.3. As shown in Table 3, our bidirectional ScaleGMN-B achieves an MSE test loss (\(10^{-2}\)) equal to \(1.891\), _surpassing all permutation equivariant baselines_. Notably, our method also outperforms the NG-GNN  baseline that uses 64 probe features. Additionally, our forward variant, ScaleGMN, performs on par with the previous permutation equivariant baselines with an MSE loss of \(2.56\). Note that the performance gap between the forward and the bidirectional model is probably expected for equivariant tasks: here we are required to compute representations for every node of the graph, yet in the forward variant, the earlier the layer of the node, the smaller the amount of information it receives. This observation corroborates the design choices of the baselines, which utilize either bidirectional mechanisms (NG-GNN ) or non-local operations (NFN ).

**Ablation study: Scaling data augmentations.** We baseline our method with permutation equivariant methods trained with scaling augmentations: For every training datapoint, at each training iteration, we sample a diagonal scaling matrix for every hidden layer of the input NN and multiply it with the weights/bias matrices as per Eq. (3) (omitting the permutation matrices). We sample the elements of the matrices independently as follows: _Sign symmetry_: Bernoulli distribution with probability 0.5. _Positive scaling_: Exponential distribution where the coefficient \(\) is a hyperparameter that we tune on the validation set. Observe here that designing augmentations in the latter case is a particularly challenging task since we have to sample from a _continuous_ and _unbounded_ distribution. Our choice of the exponential was done by consulting the norm plots where in some cases the histogram resembles an exponential Fig. 1. Nevertheless, regardless of the distribution choice we cannot guarantee that the augmentations will be sufficient to achieve (approximate) scaling equivariance, due to the lack of upper bound. We evaluate on the F-MNIST dataset for the sign symmetry and on the CIFAR-10-GS-ReLU for the positive scale. As shown in Table 4, regarding the former, augmenting the training set leads consistently to better results when compared to the original baselines. _None of these methods however achieved results on par with ScaleGMN and ScaleGMN-B. On the other hand, we were unable to even surpass the original baselines regarding the latter task_. This indicates that designing an effective positive scaling augmentation might be a non-trivial task.

**Limitations.** A limitation of our work is that it is currently designed for FFNNs and CNNs and does not cover other layers that either modify the computational graph (e.g. skip connections) or introduce additional symmetries (e.g. softmax and normalisation layers). In both cases, in future work, we plan to characterise scaling symmetries (certain steps were made in ) and modify ScaleGMN for general computational graphs as in . Additionally, a complete characterisation of the functions that can be expressed by our scale/rescale equivariant building blocks is an open question (except for sign ). Finally, an important theoretical matter is a complete characterisation of the expressive power of ScaleGMN, similar to all equivariant metanetworks.

## 7 Conclusion

In this work, we propose ScaleGMN  a metanetwork framework that introduces to the field of NN processing a stronger inductive bias: accounting for function-preserving scaling symmetries that arise from activation functions. ScaleGMN can be applied to NNs with various activation functions by modifying any graph metanetwork, is proven to enjoy desirable theoretical guarantees and empirically demonstrates the significance of scaling by improving the state-of-the-art in several datasets. With our work, we aspire to introduce a new research direction, i.e. incorporating into metanetworks various NN symmetries beyond permutations, aiming to improve their generalisation capabilities and broaden their applicability in various NN processing domains.