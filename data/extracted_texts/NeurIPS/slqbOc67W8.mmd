# Melting Pot Contest: Charting the Future of

Generalized Cooperative Intelligence

 Rakshit S Trivedi

MIT CSAIL

rstrivedi@csail.mit.edu

Akbir Khan

Anthropic

Jesse Clifton

Cooperative AI Foundation

&Lewis Hammond

Cooperative AI Foundation

Edgar A. Duenez-Guzman

Google Deepmind

&John P Agapiou

Google Deepmind

&Jayd Matyas

Google Deepmind

&Sasha Vezhnevets

Google Deepmind

Dipam Chakraborty

AI Crowd

Yue Zhao

Northwestern Polytechnical University

&Marko Tesic

University of Cambridge

Barna Pasztor

Yunke Ao

Omar G. Younis

Jiawei Huang

Benjamin Swain

Haoyuan Qin

Mian Deng

Ziwei Deng

Utku Erdoganaras

&Natasha Jaques

University of Washington

&Jakob Nicolaus Foerster

University of Oxford

&Vincent Conitzer

Carnegie Mellon University

&Jose Hernandez-Orallo

Universitat Politecnica de Valencia

&Dylan Hadfield-Menell

MIT CSAIL

Joel Z Leibo

Google Deepmind

jzl@google.com

Corresponding Authors.\({}^{}\)Contest Participant Authors.

###### Abstract

Multi-agent AI research promises a path to develop human-like and human-compatible intelligent technologies that complement the solipsistic view of other approaches, which mostly do not consider interactions between agents. Aiming to make progress in this direction, the Melting Pot contest 2023 focused on the problem of cooperation among interacting agents and challenged researchers to push the boundaries of multi-agent reinforcement learning (MARL) for mixed-motive games. The contest leveraged the Melting Pot environment suite to rigorously evaluate how well agents can adapt their cooperative skills to interact with novel partners in unforeseen situations [1; 2]. Unlike other reinforcement learning challenges, this challenge focused on _social_ rather than _environmental_ generalization. In particular, a population of agents performs well in Melting Pot when its component individuals are adept at finding ways to cooperate both with others in theirpopulation and with strangers. Thus Melting Pot measures _cooperative intelligence_. The contest attracted over 600 participants across 100+ teams globally and was a success on multiple fronts: (i) it contributed to our goal of pushing the frontiers of MARL towards building more cooperatively intelligent agents, evidenced by several submissions that outperformed established baselines; (ii) it attracted a diverse range of participants, from independent researchers to industry affiliates and academic labs, both with strong background and new interest in the area alike, broadening the field's demographic and intellectual diversity; and (iii) analyzing the submitted agents provided important insights, highlighting areas for improvement in evaluating agents' cooperative intelligence. This paper summarizes the design aspects and results of the contest and explores the potential of Melting Pot as a benchmark for studying Cooperative AI. We further analyze the top solutions and conclude with a discussion on promising directions for future research.

## 1 Introduction

As AI systems become increasingly sophisticated and interconnected, it will be critical that they be competent at cooperating, both with other AI systems and with humans. These systems should be effective at cooperation not only in settings where all agents share the same goal, but also in _mixed-motive_ settings where agents have different but not mutually exclusive goals, such that gains from cooperation are available but difficult to achieve due to selfishness or other obstacles. Many real-world problems stem from agents failing to resolve mixed-motive problems such as _social dilemmas,_ including scenarios where overconsumption may deplete shared resources and public good provision scenarios where there is a free rider problem . The class of mixed-motive problems also includes _bargaining problems_, in which players have differing preferences over Pareto-optimal agreements, and may fail to reach compromise .

The emerging field of _Cooperative AI_[5; 6] is concerned with building AI systems that can help humans and machines improve their joint welfare in general environments. With this ultimate goal in mind, our objective in running this contest was to promote (differential) progress on the _cooperative intelligence_ of AI systems. Inspired by earlier work from Legg and Hutter , we considered the following informal working definition of Cooperative Intelligence:

_An agent's ability to achieve its goals in ways that also promote social welfare, in a wide range of environments and with a wide range of other agents._

Capabilities in multi-agent settings may be dual-use; e.g., the ability to make credible commitments to peaceful agreements might also be useful for making coercive commitments . Thus we are interested in advancing capabilities that are _differentially_ useful for cooperation, relative to more general capabilities, to the extent that this is possible. Our definition of cooperative intelligence accounts for and aligns with the previous extension of Legg and Hutter approach to multi-agent systems : a more realistic definition of intelligence must give more weight to environments which contain other agents with social abilities--thereby motivating our choice of the Melting Pot suite.

The Melting Pot suite provides game-like simulations of different worlds which present a variety of social situations. Melting Pot focuses on ability to generalize to new players, not new physical environments. The participants were expected to train agents (in the form of algorithms) that learned to interact with the environment and with each other in a way that encouraged them to behave cooperatively while remaining not too exploitable by antisocial others. Towards the end of the contest, the participants submitted their algorithmic solutions to be evaluated on held out scenarios which were not previously seen by the agents during their training phase or their designers during the development phase (we discuss this in further detail below). Therefor Melting Pot is a test of zero-shot generalization to new social partners.

## 2 The Melting Pot Contest

While many aspects of cooperation--such as bargaining , understanding others' preferences [10; 11], negotiating , social learning , and ad hoc teamwork --have been the subject of previous contests, a flexible and general benchmark for comparing the cooperative capabilitiesof learning agents in mixed-motive settings across different environments and populations has been lacking. The Melting Pot contest directly addressed this gap and defined key metrics for cooperative intelligence to evaluate the solution approaches. This contest focused on setup (a) to test zero-shot generalization of the trained agents to new situations containing previously unseen co-players; and (b) where the number of players that will execute the trained policy at test time is unknown _a priori3_.

Melting Pot consists of a set of test scenarios and a protocol for using them. A scenario is a multi-agent environment that tests the ability of a _focal population_ of agents (to be trained by the participants of this contest) to generalize to novel social situations. Each scenario is formed by a _substrate_ and a _background population_. The term'substrate' refers to the physical part of the world: its spatial layout, where the objects are, the rules of physics, etc. The term 'background population' refers to other agents who inhabit it. While the substrate is experienced by the focal population during training, the background population is not. Figure 1 provides an overview of the Melting Pot evaluation protocol. The performance of the focal population in a test scenario measures how well its agents generalize to social situations they were not directly exposed to at training time. Therefore, the kind of generalization that Melting Pot probes is mainly along social dimensions, not physical dimensions. This is similar in spirit to other generalization-based assessments of RL (e.g., ), but orthogonal in its actual content since prior benchmarks mainly measure generalization along physical dimensions.

### Substrate (Environments)

As a principled method for arriving at a subset of the Melting Pot environments to use for the contest (the full suite is described in ), we asked the following question: which set of environments represent a diverse and solvable challenges that require the trained agents to demonstrate capabilities for effective cooperation? To address this question, we identified four environments for this contest that vary in their complexity and that contain problems requiring various kinds of cooperative behavior to achieve a socially optimal solution. Table 1 outlines these substrates along with the list of qualitative cooperative behaviors they are expected to elicit. All Melting Pot environments are based on the DeepMind Lab2D game engine . All are pixel-based (\(8 8 3\) sprites), agents are oriented, and view the world through a partial observation window extending mostly in front of them (\(88 88 3\)).

   Substrate & Players & Potential Emergent Behaviours \\  Prisonerâ€™s Dilemma in the Matrix: Arena4 & 8 & Reciprocity, Deception, Teaching, \\  & & Sanctioning, Coalition Formation \\  Clean Up5 & 7 & Reciprocity, Resource Sharing, \\  & & Convention Following, Sanctioning, \\  & & Coalition Formation \\  Allelopathic Harvest6 & 16 & Stubbornness, Bargaining, Coalition \\  & & Formation, Convention Following \\  Territory: Rooms7 & 9 & Resource Sharing, Reciprocity \\  & & Territoriality, Resisting Temptation \\   

Table 1: Contest substrates with a non-exhaustive list of cooperative behaviors they elicit.

Figure 1: The Melting Pot evaluation protocol: train a focal population (orange) on a substrate, then test it in scenarios where focal agents interact with agents from the background population (blue).

### Background Population

A critical component to perform zero-shot social generalization test is to evaluate the agents trained on a substrate in presence of novel co-players. This entails building intelligent background agents that can serve as useful testbed of cooperative generalization. The background agents used in the contest were either identical to or constructed using the same processes outlined in the original Melting Pot papers [1; 2]. Here, we describe the details on the construction process:

The background population consists of reinforcement learning (RL) agents, referred to as "bots" to differentiate them from the focal population's agents. The creation of the background population involved three key steps: (1) specification, (2) training, and (3) quality control.

**Specification:** The designer typically starts with an idea of what they want the final bot's behavior to look like. Since substrate events provide privileged information about other agents and the environment, we can often specify reward functions that easily induce the desired behavior. This task is much simpler than the challenge faced by focal agents, who must learn from pixels and final rewards alone. However, when a single reward function is insufficient to capture the desired behavior, we employ techniques inspired by hierarchical reinforcement learning [17; 18; 19], such as reward shaping  and the "option keyboard" .

To generate complex behaviors, we first train bots using different environment events as reward signals, similar to the approach used in Horde . These behaviors are then combined using simple Python scripts, allowing us to express complex behaviors in an "if this event, then run that behavior" manner. This approach, which we call the "puppet" method, uses the same basic neural network structures (ConvNet, MLP, LSTM) as other agents but introduces a hierarchical policy structure.

For example, in the Clean Up task, we designed a bot that cleans only when other players are cleaning. The architecture is inspired by Feudal Networks , but with key differences. We represent goals as a one-hot vector \(g\), which is embedded into a continuous representation \(e(g)\). This embedding \(e\) is then provided as an additional input to the LSTM. The network outputs several policies \(_{z}(a|x)\), and the final policy is a mixture \((a|x)=_{z}(e)_{z}(a|x)\), where the mixture coefficients \((e)=(e)\) are learned from the embedding. Notably, instead of directly associating policies with goals, we allow the embedding to learn these associations through experience.

**Training:** To train the puppet to follow goals, we train it in the respective environment with goals switching at random intervals and rewarding the agent for following them. The thing to keep in mind is that the bots must generalize to the focal population. To this end, we chose at least some bots--typically not used in the final scenario--that are likely to develop behaviors resembling that of the focal agent at test time. For instance, in Running With Scissors in the Matrix, we train rock, paper, and scissors specialist bots alongside "free" bots that experience the true substrate reward function.

**Quality control:** Bot quality control is done by running 10-30 episodes where candidate bots interact with other fixed bots. These other bots are typically a mixture of familiar and unfamiliar bots (that trained together or separately). We verify that agents trained to optimize for a certain event, indeed do. We reject agents that fail to do so.

### Evaluation on Scenarios

When a focal population \(f\) has been submitted for evaluation, the evaluation protocol measures its cooperative abilities in new social situations containing players who were never encountered during training. Each such test instance is called a _scenario_. A scenario consists of a substrate, a background population, and the number of individuals to sample from both focal and background populations.

For a given scenario, let \(m\) be the number of agents sampled from the focal population and let \(n\) be the number of bots sampled from the background population. Scenarios where \(m>n\) are said to be in _resident mode_ while scenarios where \(n<m\) are said to be in _visitor mode_. For a population to achieve a high score in Melting Pot it is necessary that it score well in both resident and visitor mode scenarios. That is, the researcher designing the population must ensure that it works well despite not knowing in advance how many individuals will be sampled from it at test time. Many policies thatare effective at low population densities fail to transfer to higher population density. Resident mode scenarios test the focal population's ability to absorb unfamiliar visitors and remain stable in their cooperation. Visitor mode scenarios test individuals from the focal population's ability to adapt to an unfamiliar culture. In social dilemma scenarios such as Clean Up, a more reliable solution would be for all individuals to spend some of their time acting altruistically and some of their time collecting rewards. Melting Pot penalizes overspecialization. Participants were given access to a _validation set_ of scenarios from each of these classes, for each substrate. They were free to use these scenarios however they wished when designing their solutions. We created a _holdout set_ of scenarios from each specified class to ensure similarity with the validation set across relevant dimensions. In the end, the participants' focal populations were scored only on the holdout set, a test of zero-shot generalization.

### Metrics

Given a substrate \(\), the per-capita return of the focal population \(f\) is defined as: \((f,,g)=_{i=1}^{m}c_{i}_{ _{1} h_{1},,_{N} h_{N}}R_{i}()\), where \(g\) is the background population of agents specific to the scenario, \(c\) is the scenario configuration represented as a binary vector indicating which \(m\) players are focal, \(h_{i}\) is the distribution over agent \(i\)'s policies and \(R_{i}\) is the expected return of agent \(i\) under a joint policy of all agents \(\). For this contest, we consider the _(utilitarian) social welfare8_ induced by \(\) in \(\) as \(w()=_{i=1}^{N}R_{i}()\). This social welfare is important in defining which behaviors count as 'cooperative'. We restricted attention to scenarios in which it's possible for an agent to achieve their goals while promoting social welfare. We let \(P\) be a joint distribution over scenarios \(S\) that are cooperation shaping/eliciting. We defined the cooperative intelligence of \(F\) as the average per-capita return it attains against this distribution, i.e., \(CI(F)=(F() S)P(S),\) over scenarios that are _a priori_ interesting from the perspective of Cooperative AI research.

### Contest Structure

Thanks to our sponsors, Google Deepmind and the Cooperative AI foundation, the contest was able to award \(\$10K\) in prize pool and up to \(\$50K\) in compute to underrepresented groups or groups with lack of necessary compute. The contest was run in three phases: Development phase where the participant submissions were evaluated against validation scenarios which participants had access to; Practice Generalization phase where participant submissions were evaluated against a small fraction of held-out scenarios (not accessible to participants) but participants were allowed to make changes to their model based on scores they received; and Final Evaluation phase where participants selected up to three of their submissions to be evaluated against the full set of held-out scenarios and they were ranked based in the scores they received in this phase.

The unit of evaluation in Melting Pot is a _multi-agent population learning algorithm_ (MAPLA). A MAPLA is a function \(F\) that takes a substrate \(M\) and produces a focal population, \(f=F(M)\). This focal population then interacts with the background population, and obtains a particular per-capita return. The participants were required to train and submit one focal population \(f\) for each of the substrates in the contest. Each participant was required to submit focal populations for each of the four substrates, and they were ranked based on their per-capita focal return.

The AICrowd platform hosted the starter kit on Gitlab. This included the baseline code provided by us and provided the submission pipeline. They also hosted the leaderboard, discussion forum, demo notebooks, an automated evaluation service, and provided teams with feedback on their performance.

## 3 Results and Analysis

The key objective of the contest was to evaluate how well different agent populations adapted to mixed-motive environments using held-out co-player populations to test _social_ rather than _environmental_ generalization. This section outlines the core aspects of our evaluation protocol. We begin by summarizing the results based on key metrics used to rank participants in the contest. Following this, we provide a detailed analysis of the capability profiles of agents submitted by participants. Finally, we conclude with a qualitative assessment, highlighting notable behaviors that demonstrate the cooperative intelligence exhibited by the agents in action.

### Evaluation Protocol

As described earlier, we evaluated and ranked participants' submissions based on the per capita focal return achieved by their focal agents. Each submission was assessed by collecting multiple episodes per scenario and substrate, then calculating the mean per capita focal return across these scenarios and substrates. Scores were normalized using the best and worst scores from the baseline solutions reported in the Melting Pot 2.0 paper . A score above 1.0 indicates the submission outperformed the best baseline. The evaluation during the phases outlined in Section 2.5 was conducted as follows:

**(i) Development phase:** Teams were evaluated on 22 public scenarios where participants had access to the code and details on the properties of background population. A total of 73 teams submitted their agents and 8 out of 73 teams outperformed the best baseline. These evaluations were done using 4 episodes per submission and participants were allowed to make 2 submissions per day.

For the next two phases, we generated a total of 51 held-out scenarios across the 4 substrates and we used these held-out set to evaluate agents in the next two phases. The participants did not have access to the code and details on the background agents used for these evaluation phases.

**(ii) Practice Generalization phase:** Teams were evaluated on 12 held-out scenarios (3 scenarios sampled randomly from each substrate) from the full set of 51 scenarios. During this phase, which ran for 10 days, the participants received scores on the sampled 12 scenarios and they were allowed to adjust their models same as development phase based on this scores. A total of 23 teams made submission during this phase and 8 out of 23 teams outperformed the best baseline on this practice generalization test set. These evaluations were done using 20 episodes per submission.

**(iii) Evaluation phase:** Teams were not allowed to make new submissions during this phase. Instead they were required to select up to 3 submissions from development or practice generalization phase to use for final evaluation. The selected submissions were evaluated on the full-set of 51 held-out scenarios using 80 episodes per submission. For each team, we considered their best scoring submission out of the selected ones and used that score to compute final ranks. A total of 23 teams provided their selections, generating a result matrix of the size 51 x 23 focal return scores. To compute the metrics reported in following sections, we averaged these scores across scenarios and substrates. We found that 8 our of 23 teams outperformed baseline on the full held-out scenarios.

### Summary Results

Figure 2 provides the aggregate scores for the top 12 teams. Figure 2(a) provides the exact normalized score that was used to rank the participants. As one can observe 8 teams achieved score higher than 1.0 signifying that they performed above the best baseline. Figure 2(b) checks for overlap in the performance of these teams by considering the confidence interval at \(95\%\) confidence level and we observe that there was very little overlap signifying that the team performances had significant difference between them when measured on per capita focal return. Among the teams that performed better than strongest baselines, we observed a variety of solution approaches including hard-coded and rule-based solutions, cooperative MARL solution, reward shaping and combination of imitation

Figure 2: Summary analysis based on Focal Returns achieved by the teamsand reinforcement learning agents (refer to Section 4 for more details). Overall, the success of deep reinforcement learning approaches demonstrate a significant community-driven progress on multi-agent reinforcement learning for cooperation in presence of novel co-players.

### Capability Profiles: A Bayesian Analysis

In addition to the aggregate performance of submission used to rank the teams for the contest, we are interested in conducting further analysis on the capabilities learned by different agents in order to support our overarching goal of informing further research in Cooperative AI. To this end, we delved into investigating the _kinds_ of substrates and populations the teams are good at. In order to do this, we annotate each environment with 16 tags that describe its _cooperative demands_, including \(resident\), \(visitor\), \(defense\), and \(flexibility\), which represent whether the environment is resident or visitor (sometimes it is none of them), and whether they require defense strategies and flexibility, respectively (the other 12 tags are explained in the appendix). Figure 2(a) shows the correlation matrix between these features (including the substrates as features too). From these demands we can predict how well each participant would behave on new environments, provided they are annotated as well. This will also allow us to generate capability profiles for the participants.

We use two kinds of predictive models: an assessor approach  using a specific predictive model (XGBoost) predicting the normalized scores for all data, and a capability-based approach using measurement layouts, through hierarchical Bayesian inference . For illustration, we choose eight participants (submission ids for some of the teams). For them, the assessor results show there is some degree of predictability in the score according to these features (Figure 2(b)).

This suggests the construction of more sophisticated, and interpretable, Bayesian models, that allow us to predict performance by comparing the demands of the environment with the capability profile of the participants. The details of the measurement layout and how good they are in predictive power are shown in the appendix, but here we include the capability profiles of these eight participants (Figure 4), because of their explanatory power. We see that the best \(team\_1\) (submission \(id\_3\)) in aggregate score is the worst in some capabilities (\(prisoner\_dilemma\), which is also observed in Figure 4(b) (apparently negatively correlated with score), but this participant was generally very good in the tag abilities such as \(visitor\_Ability\), \(defense\_Ability\) and \(flexibility\_Ability\). Similarly, low-performing participants such as \(team\_11\) are usually bad at all capabilities, but this one is surprisingly the best at \(allelopathic\_harvest\_open\). The capability profile allows us to determine the strength and limitations of different teams in a more refined way than aggregate scores. Our benchmark has annotations for all environments, and includes the tools to build these capability profiles, helping their developers detect their strong/weak points more precisely.

### Qualitative Analysis of Agent Behaviors

To better understand the results and assess the cooperative behavior of agents, we analyzed the trajectories of the top 12 submissions. We examined the per capita return of background agents in the presence of the participants' focal agents, the focal agents' performance per substrate, and videos of

Figure 3: Feature-based analysis and predictability of the scores.

the focal agents in various scenarios. Figure 4(a) shows that teams with high per capita focal returns often do not perform well in terms of the background population's per capita return. This indicates that while focal agents excel in the contest, they often overfit by optimizing for focal returns and within their group, failing to cooperate with background agents. This issue was particularly severe in solutions using hard-coded or rule-based policies compared to RL-based solutions.

Figure 4(b) reveals that higher-ranking teams did not consistently perform well across all substrates. The top two teams achieved their high ranks by excelling in a few substrates while severely underperforming in others, likely due to overfitting their agents' training to certain substrates. Additionally, videos of the submitted agents highlight in-group coordination among focal agents, often leading to anti-social behavior. Figure 6 demonstrates such a strategy: two focal agents perform in-group coordination to first kill the only background agent and then proceed to share the territory.

Figure 4: Teams abilities as inferred by the measurement layout

Figure 5: Performance measures in terms of background returns and per substrate focal returns for teams ranked by the focal returns used for contest ranking

Figure 6: Example of in-group coordination leading to anti-social behavior

These observations and analyses taught us three key lessons: (i) Rule-based hardcoded policies showed more overfitting to the focal objective than RL solutions. We had assumed RL solutions wouldn't exploit the difference between focal and background agents, but we didn't account for hand-coded solutions, so teams found loopholes in that case. (ii) In hindsight, ranking submissions based on per substrate scores would have prevented teams from winning by excelling in only a few substrates. This issue might also be stemming from the final evaluation scenarios being more similar to the validation scenarios than anticipated. Designing more varied scenarios would be beneficial. (iii) Using multiple social welfare metrics for evaluating submissions would have better prevented anti-social behaviors, aligning more closely with our objective of focusing on differential progress.

## 4 Approaches submitted by participants

We outline the baseline solutions and briefly describe the best performing participant solutions (See Appendix B for more details).

### Baselines

A baseline9 of independent learning PPO agents implemented in RLlib was provided for participants to start building their approaches. This baseline formed the basis of the majority of submissions. Furthermore, we considered the scores from the state-of-art approaches such as Actor-Critic architecture (ACB), V-MPO  and OPRE . A random policy and exploiter (agents trained on the test scenarios) were used for extracting the best and worst performing methods to compute the normalized score used for ranking submissions.

### Hard-coded solution

In this approach, a set of hard-coded policies was developed, each uniquely tailored to one of four distinct substrates. Each policy was designed to strategically utilize predefined goals that were optimized for the environmental dynamics and player roles of its specific substrate. The policies included functions such as the ability to distinguish between focal and background players and mechanisms to avoid enemy zap locations. Navigation was enhanced using breadth-first search algorithms, enabling the agents to efficiently achieve their goals while avoiding obstacles. This strategic framework underpinning each hard-coded policy contributed significantly to their effectiveness, as evidenced by Figure 1(a), showing that this approach achieved the highest aggregate team ranking based on score.

The high effectiveness of these hard-coded policies stemmed largely from their capacity for iterative refinement. By closely monitoring policy performance in various scenarios, developers could identify specific areas for improvement, thereby enhancing the score through targeted adjustments to the hard-coded strategies. This method proved particularly successful given the relatively intuitive nature of the game rules for each substrate, which facilitated a straightforward understanding and manipulation of policy parameters. Had the game rules been more intricate or less discernible, the scenario might have leaned more favorably towards the application of trained models, which excel in managing complex, less predictable environments.

### State-of-art independent learning solutions

**Reward Shaping and Mixture Learning Algorithm Solution.** The reward shaping and mixture learning algorithm solution for reinforcement learning addresses key challenges with hyperparameters and reward functions, which often impede optimal performance. The implementation utilizes PopArt, IMPALA , and PPO  techniques. PopArt manages reward function scaling to prevent excessive values, PPO reduces optimization gaps with clipping techniques, and IMPALA enhances vision-based observations through scalable distributed learning. In various experimental setups, reward functions and environments were tailored to improve agent behaviors. For example, in the Allelopathic Harvest, various reward functions and training environments were tested to balance agent actions. In Clean Up, complex reward tracking promoted efficient cleaning by rewarding agents based on their actions over time. The PD in the Matrix Arena used early rewards with a specific discount factor to manage interactions effectively. In the Territory Rooms, combining team and individual play environments helped agents learn both collaborative and independent strategies.

**Novel Agent Architecture.** This submission implemented a stable policy optimization algorithm for the agents separately, developing substrate-agnostic learning protocols, and fine-tuning the approaches to the individual challenges of the substrates. Following the baseline published in , it deploys Actor-Critic models such as A2C  and IMPALA. To generalize the learning approach across substrates and stabilize learning without tuning hyperparameters for each substrate separately, this submission implemented a PopArt  layer in all models and weights sharing across agents.

### Cooperative MARL solution

This solution adopted a cooperative MARL approach to tackle the challenge of enhancing the model's generalization ability. A modified ResNet was used for visual-to-state transformation, converting visual data into state information to improve state-space processing efficiency. For credit assignment, individual rewards were summed to obtain the global reward, and the Value Decomposition Network (VDN) was implemented to ensure fair reward distribution among agents in multi-agent environments. In terms of cooperation and competition, the approach initially assumed cooperative behavior among players before considering their competitive behavior. However, in the Harvest and Clean environments, the absence of competitors led to some negative behaviors. Background players were added to carry out collection activities to increase player engagement. Despite achieving commendable results, the approach had limitations. The lack of directional data hindered attack strategies, incorrect color usage in visualization led to misjudging opponent actions, over-reliance on prior strategies sometimes interfered with outputs, additional ResNet layers increased memory usage, and limited inter-agent training potentially constrained performance.

## 5 Concluding Remarks and Discussions

Participants contributed a variety of solutions including state-of-the-art RL approaches based on cooperative multi-agent RL and imitation learning, as well as hard-coded approaches. Many teams were able to improve on the previously published benchmark performance metrics for Melting Pot  (measured on the same scenarios).

The contest has proven valuable for informing the design and structure of future contests. In this case several high performing agents focused on solving for only few of the several desired cooperative capabilities, and heavily optimized for the single evaluation metric (even at the expense of not achieving jointly prosocial behavior), overfitting too much despite the zero-shot generalization structure. We may aim to do more to discourage such strategies in future contests.

The goal is to promote _differential_ progress in building agents with cooperative intelligence. The Melting Pot environment offers a wide variety of substrates with numerous capability requirements, and future contests can explore these more extensively. This contest demonstrated that Melting Pot is diverse enough that reasonable-sounding metrics do not work well across the entire suite. In the future, it would be useful to study secondary evaluation metrics like the impact on the background population, This could indicate the effects trained agents might have on humans in the real world. By analyzing the background-population per-capita return, one can determine if it is negatively impacted by the focal population. This approach helps study whether the joint policy of focal agents produces negative externalities--"side effects", impacting the broader population while sparing the focal population, aligning well with research on value alignment and AI safety.

These design insights will be directly applicable to a successor contest which will run the following year and be driven by many of the same organizers. The new contest will involve LLM-based agents, and will be based on Concordia .

Most notably, Melting Pot remains an unsolved problem. Results reported in the related paper and solutions received in this contest demonstrates that there is plenty of room to improve the scores in the vast majority of test scenarios and substrates. Melting Pot suite allows researchers to evaluate algorithms that train populations of agents to behave socially. How will the resulting agents resolve social dilemmas? Can they deal with freeriders? Will they learn policies that have an equitable distribution of returns? Will the winner take all? These questions not only present interesting research directions, but are also important to consider in the context of AI safety and fairness. After all, human society is a multi-agent system and any deployed AI agent will have to generalize to it.