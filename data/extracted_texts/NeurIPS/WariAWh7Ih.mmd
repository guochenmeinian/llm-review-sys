# On Calibration of LLM-based Guard Models for Reliable Content Moderation

Hongfu Liu\({}^{1}\), **Hengguan Huang\({}^{2}\), **Hao Wang\({}^{3}\), **Xiangming Gu\({}^{1}\), **Ye Wang\({}^{1}\)**

\({}^{1}\)National University of Singapore \({}^{2}\)University of Copenhagen \({}^{3}\)Rutgers University

Corresponding to: Hongfu Liu (hongfu@comp.nus.edu.sg)

\({}^{1}\)Our code is publicly available at https://github.com/Waffle-Liu/calibration_guard_model

###### Abstract

Large language models (LLMs) pose significant risks due to the potential for generating harmful content or users attempting to evade guardrains. Existing studies have developed LLM-based guard models designed to moderate the input and output of threat LLMs, ensuring adherence to safety policies by blocking content that violates these protocols upon deployment. However, limited attention has been given to the reliability and calibration of such guard models. In this work, we empirically conduct comprehensive investigations of confidence calibration for 9 existing LLM-based guard models on 12 benchmarks in both user input and model output classification. Our findings reveal that current LLM-based guard models tend to 1) produce overconfident predictions, 2) exhibit significant miscalibration when subjected to jailbreak attacks, and 3) demonstrate limited robustness to the outputs generated by different types of response models. Additionally, we assess the effectiveness of post-hoc calibration methods to mitigate miscalibration. We demonstrate the efficacy of temperature scaling and, for the first time, highlight the benefits of contextual calibration for confidence calibration of guard models, particularly in the absence of validation sets. Our analysis and experiments underscore the limitations of current LLM-based guard models and provide valuable insights for the future development of well-calibrated guard models toward more reliable content moderation. We also advocate for incorporating reliability evaluation of confidence calibration when releasing future LLM-based guard models1.

## 1 Introduction

Recent advancements in Large Language Models (LLMs) have facilitated the development of powerful conversation systems, leading to the deployment of LLM-based chatbots in various real-world applications (Brown, 2020; Anil et al., 2023; Touvron et al., 2023; Dubey et al., 2024). However, these systems face substantial risks due to the potential for malicious exploitation of powerful LLMs (Wang et al., 2023). Consequently, addressing these risks has become an urgent and critical task. One promising strategy is to regulate LLMs during their training phase. Existing researches primarily focus on designing alignment algorithms through preference optimization (Ouyang et al., 2022; Rafailov et al., 2024), implementing adversarial training (Mazeika et al., 2024), or employing machine unlearning to remove harmful knowledge from the models (Chen & Yang, 2023; Liu et al., 2024). These approaches aim to control text generation and prevent undesired outputs. Despite these significant efforts to enhance LLM safety during training, red-teaming still makes efforts to expose vulnerabilities, including jailbreak attacks that successfully bypass the safety constraint and elicit harmful responses from LLMs, highlighting the risks of future, unseen threats (Zou et al., 2023; Liu et al., 2023; Chao et al., 2024). Therefore, in addition to training-time interventions, it is equally vital to implement test-time measures, such as constraint inference (Xu et al., 2024), and establish effective test-time guardrains through content moderation, particularly when deploying LLMs in real-world settings.

Content Moderation serves the critical function of monitoring both user inputs and model outputs during conversations. Typically, guard models are designed to assess whether user inputs and LLM outputs comply with safety regulations, and either reject user queries or block model responses when content violating safety protocols is detected. This approach remains effective even whenLLMs have been compromised by previously unseen jailbreak attacks. Current state-of-the-art guard models, which are typically built on LLMs, demonstrate strong performance across various benchmarks (Inan et al., 2023; Ghosh et al., 2024; Han et al., 2024; Zeng et al., 2024). However, these guard models primarily emphasize the classification performance but overlook the predictive uncertainty of harmfulness predictions, therefore failing to assess the reliability of these models' predictions. This oversight is crucial because guard models may occasionally make erroneous decisions, potentially allowing unsafe content to bypass moderation, especially when encountering non-trivial domain shifts, despite their strong in-domain performances. Therefore, quantifying the predictive uncertainty and confidence in model predictions is essential to assessing the trustworthiness of guard models, enabling more reliable decision-making in high-risk scenarios that may arise during conversations after model deployment.

In this work, we examine the reliability of existing open-source guard models by focusing on their confidence calibration. Specifically, we empirically assess the calibration performance by commonly used expected calibration error (ECE) for two key tasks: user input (prompt) classification and model output (response) classification with binary labels. To conduct a systematic evaluation, we examine 9 models across 12 datasets. Our experimental results reveal that, despite achieving strong performance, most existing guard models exhibit varying levels of miscalibration. Additionally, our findings show that current LLM-based guard models:

* tend to make overconfident predictions with high probability scores.
* remain poorly calibrated under adversarial environments, exhibiting higher ECE in adversarial prompt classification, even when the SOTA guard model achieves high F1 scores.
* display inconsistent ECE across different types of response models, demonstrating weak robustness to variations in response model types.

These observations highlight critical challenges in improving the reliability of guard models in real-world deployments. Consequently, we are motivated to improve the confidence calibration of guard models, focusing on post-hoc calibration methods to avoid additional computational costs of training new guard models. We explore the impact of bias calibration methods on confidence calibration for the first time, discovering that contextual calibration proves impressively effective for prompt classification, while conventional temperature scaling remains more beneficial for response classification. Lastly, we identify miscalibration issues stemming from prediction vulnerabilities induced by single tokens and misaligned classification objectives, highlighting the limitations of instruction-tuned LLM-based guard models. We stress the importance of reliability evaluation and advocate for the inclusion of confidence calibration measurement in the release of future new guard models.

Figure 1: An overview of LLM-based guard models for content moderation. Guard models monitor the input and output during conversations between the user and LLM (Agent), providing a binary prediction followed by a specific unsafe content category if unsafe content is detected. The instruction examples for prompt classification and response classification from LLama-Guard are detailed in the right yellow boxes respectively.

## 2 Preliminary

**LLM-based Guard Models**. Given the user input text \(\) and the corresponding response \(=f()\) generated by a deployed LLM \(f(*)\), the task of the LLM-based guard model \(g(*)\) is to classify the user input \(p_{g}(|)\), or the LLM output \(p_{g}(|,)\)2 These tasks are referred to as **prompt classification** and **response classification**, respectively. For the predicted label \(\), most existing LLM-based guard models initially perform binary classifications \(y^{b}\) to determine whether the user input \(\) or model response \(\) is safe. If the binary classification result indicates the input or the response \(y^{b}\) is unsafe, the guard model \(g(*)\) then proceeds with a multiclass classification to categorize the specific type \(y^{c}\) by \(p_{g}(y^{c}|,y^{b})\) or \(p_{g}(y^{c}|,,y^{b})\) where the categories \(c\) are pre-defined in a taxonomy. These classification tasks in LLM-based guard models are carried out in an autoregressive generation manner, and Figure 1 illustrates examples of the prompt and response classification instructions used in LLama-Guard.

**Confidence Calibration**. A model is considered perfectly calibrated if its predicted class \(\) and the associated confidence \(\) satisfy \(P(=y|=p)=p, p\), where \(y\) is the ground-truth class label for any given input. This implies that higher confidence in a prediction should correspond to a higher chance of its prediction being correct. However, since \(P(=y|=p)\) can not be directly calculated with finite sample size, existing approaches employ binning-based divisions on finite samples and utilize the Expected Calibration Error (ECE) as a quantitative metric to assess the model's calibration (Naeini et al., 2015). Assuming that confidence is divided into \(M\) bins with equal interval \(1/M\) within the range \(\), the ECE is defined as

\[ECE=_{m=1}^{M}|}{N}|Acc(B_{m})-Conf(B_{m})|,\] (1)

where \(Acc(B_{m})=|}_{i B_{m}}(_{i}=y_{i})\) and \(Conf(B_{m})=|}_{i B_{m}}_{i}\). \(B_{m}\) represents the set of samples falling within the interval \((,]\), \(_{i}\) and \(y_{i}\) are the predicted and ground truth classes, respectively, and \(_{i}\) is the model's predicted probability. However, existing instruction-tuned LLM-based guard models do not directly output the probability of each class. Instead, the probability of class \(c_{i}\) is derived from the output logits \(z_{(c_{i})}\) of the corresponding target label token \((c_{i})\), where \((*)\) is the verbalizer. Re-normalization is then applied over the set of target label tokens as follows,

\[p(y=c_{i}|,)=(c_{i})}}}{_{c_{i} }e^{z_{(c_{i})}}}\] (2)

where \(\) is empty for prompt classification. Specifically, for binary classification tasks, the target label tokens could simply be "safe / unsafe", "harmful / unharmful", or "yes / no", depending on the specific instructions utilized in different guard models.

## 3 Calibration Measurement of LLM-based guard models

To systematically evaluate the calibration of existing open-source LLM-based guard models across public benchmarks, we conduct an analysis of 9 models on 12 publicly available datasets. We take the prompt classification and response classification as two primary tasks in our investigation. Due to the variability in safety taxonomies across different guard models and datasets, it is challenging to directly compare performance on multiclass prediction tasks. Therefore, our evaluation emphasizes **binary classification** (safe/ unsafe) for both prompt and response classifications, allowing for a more consistent and fair comparison across guard models. Moreover, binary classification is a critical precursor to multiclass predictions, as an incorrect binary prediction could result in the dissemination of undesired content to users, increasing the associated risk. Thus, binary classification holds particular importance in ensuring the reliability and safety of these systems.

### Experimental Setup

**Benchmarks**. To assess calibration in the context of binary prompt classification, we evaluate performance using a range of public benchmarks, including OpenAI Moderation (Markov et al., 2023), ToxicChat Test (Lin et al., 2023), Aegis Safety Test (Ghosh et al., 2024), SimpleSafetyTests (Vidgen et al., 2023), XSTest (Rottger et al., 2023), Harmbench Prompt (Mazeika et al., 2024) andWildGuardMix Test Prompt (Han et al., 2024). For the response classification, we utilize datasets containing BeaverTails Test (Ji et al., 2024), SafeRLHF Test (Dai et al., 2023), Harmbench Response (Mazeika et al., 2024), and WildGuardMix Test Response (Han et al., 2024). For all datasets, we report the ECE as the primary metric for calibration assessment, alongside the F1 score for classification performance. Detailed statistics of each dataset can be found in Appendix B.1.

**LLM-based Guard Models.** Existing LLM-based guard models vary in their capabilities, with some supporting both prompt and response classification, while others specialize in response classification, based on their instruction-tuning tasks. For prompt classification, we evaluate Llama-Guard, Llama-Guard2, Llama-Guard3, Aegis-Guard-Defensive, Aegis-Guard-Permissive, and WildGuard (Inan et al., 2023; Ghosh et al., 2024; Han et al., 2024). In the case of response classification, we additionally assess Harmbench-Llama, Harmbench-Mistral, and MD-Judge-v0.1 (Mazeika et al., 2024; Li et al., 2024). API-based moderation tools are excluded from our evaluation due to the nature of their black-box models, which output scores that cannot be simply interpreted as probability. More details can be found in Appendix B.2.

### Main Results

#### 3.2.1 General Evaluation on public benchmarks

We begin by conducting a comprehensive evaluation of both prompt and response classifications for the existing guard models on public benchmarks. The ECE results for both tasks are presented in Table 2. Our experimental findings indicate that existing guard models exhibit significant miscalibration in both prompt and response classifications. Among the models evaluated, WildGuard demonstrates the lowest average ECE for prompt classification, achieving \(14.4\%\), while MD-Judge achieves the lowest average ECE for response classification, at \(11.4\%\). However, despite the relatively better performances, both Wildguard and MF-Judge exhibit average ECE values exceeding \(10\%\), which is typically considered as poor calibration and underscores the need for further improvements. Additionally, each model displays a substantial variance in ECE across different datasets, suggesting unreliable predictions.

**Finding 1: Existing guard models tend to make overconfident predictions**. To further investigate, we visualize the confidence distributions and present the corresponding reliability diagrams in Figure 2. Additional results for other datasets, models as well as response classification can be found in Appendix D.1. The analysis reveals that for models such as LLama-Guard, Llama-Guard3, and WildGuard, the majority of predictions exhibit confidences between \(90\%\) and \(100\%\), indicating overconfident predictions along with high ECE. While Aegis-Guard-P shows a less extreme confidence distribution compared to the other models, the proportion of predictions with confidence greater than \(90\%\) is still noticeably higher than those with lower confidence, further reflecting the trend of overconfidence.

Figure 2: Confidence distributions (First row) and reliability diagrams (Second row) of Llama-Guard, Llama-Guard3, Aegis-Guard-P, and WildGuard on the WildGuardMix Test Prompt set.

#### 3.2.2 Evaluation under jailbreak attacks

Table 1 reveals considerable variability of ECE for different guard models when handling harmful requests on the HarmbenchPrompt set. To further investigate the reliability of these guard models in adversarial environments involving dangerous jailbreak attacks, we extend our evaluation to the Harmbench-adv set. This dataset, which serves as a validation set for fine-tuning Llama2-variant classifiers in Harmbench, includes user inputs generated from various types of jailbreak attacks, such as GCG and AutoDAN, leading to a significant distribution shift from typical user input. In this evaluation, we utilize the adversarial user inputs and their corresponding responses and report the F1 and ECE results for each guard model in Figure 3.

**Finding 2: Miscalibration in prompt classification is more pronounced than in response classification under jailbreak attacks**. The results demonstrate that the ECE for prompt classification is generally higher than that of response classification, indicating that guard models tend to be more reliable when classifying model responses under adversarial conditions. We conjecture that this may be due to the more considerable distribution shift in adversarial prompts than that in model responses. Additionally, while WildGuard achieves SOTA performance with an F1 score of \(92.8\%\) in prompt classification, its ECE score remains high at \(34.9\%\), highlighting concerns about the reliability of its predictions in real-world deployment.

#### 3.2.3 Evaluation of Robustness to diverse response models

While the ECE for response classification under adversarial environments appears relatively lower in Figure 3, it remains important to investigate whether each guard model consistently maintains reliability when classifying responses generated by different response models. This is crucial because response models are often aligned differently during post-training so they may have different output distributions and produce different responses to jailbreak attacks. To this end, we continue our calibration evaluation under jailbreak attacks, shifting our focus to response classification. Specifically, we employ the same Harmbench-adv set and divide it according to the response model type. After filtering out subsets with a small sample size, we retain 10 subsets containing responses from

    &  \\  & **Metric** & **Bakibuan2** & **Qven** & **Sohar** & **Llama2** & **Vicuna** & **Orccan2** & **Koola** & **OpenChat** & **Starling** & **Zephy** \\   & F1 & 57.8 & 66.7 & 54.2 & 44.4 & 64.0 & 62.7 & 74.6 & 60.6 & 66.7 & 72.7 \\  & ECE & 26.9 & 23.0 & 49.4 & 10.5 & 28.0 & 26.4 & 27.4 & 40.3 & 46.3 & 38.5 \\   & F1 & 77.8 & 88.9 & 82.8 & 71.4 & 72.1 & 80.6 & 78.4 & 70.0 & 82.0 & 78.4 \\  & ECE & 18.2 & 5.8 & 27.1 & 7.9 & 28.4 & 25.2 & 30.4 & 28.5 & 37.0 & 39.4 \\   & F1 & 73.8 & 82.4 & 84.1 & 60.0 & 83.3 & 82.2 & 77.5 & 76.4 & 91.2 & 87.7 \\  & ECE & 33.7 & 17.1 & 31.0 & 27.4 & 20.5 & 27.3 & 36.5 & 34.2 & 27.6 & 23.1 \\   & F1 & 60.3 & 66.7 & 71.2 & 31.2 & 63.3 & 65.8 & 69.9 & 78.3 & 84.4 & 89.3 \\  & ECE & 35.5 & 27.1 & 22.2 & 40.8 & 34.0 & 33.9 & 31.3 & 30.9 & 27.8 & 30.9 \\   & F1 & 57.6 & 66.7 & 67.9 & 33.3 & 56.3 & 72.7 & 72.2 & 76.2 & 83.3 & 80.8 \\  & ECE & 22.8 & 17.4 & 28.3 & 23.6 & 26.2 & 28.2 & 32.1 & 35.5 & 36.3 & 25.7 \\   & F1 & 89.7 & 100.0 & 90.6 & 70.6 & 90.9 & 86.2 & 88.9 & 89.4 & 90.9 & 94.5 \\  & ECE & 17.7 & 6.4 & 25.0 & 23.5 & 16.0 & 19.5 & 26.7 & 23.2 & 23.3 & 20.1 \\   & F1 & 84.4 & 100.0 & 87.5 & 80.0 & 92.3 & 84.8 & 92.8 & 90.9 & 89.2 & 94.5 \\  & ECE & 28.0 & 3.0 & 30.1 & 12.8 & 16.6 & 17.5 & 16.0 & 14.9 & 27.7 & 19.9 \\   & F1 & 75.4 & 79.1 & 77.2 & 55.6 & 74.2 & 76.9 & 75.3 & 76.6 & 87.5 & 92.6 \\  & ECE & 22.4 & 14.4 & 19.3 & 24.1 & 19.9 & 16.7 & 26.2 & 25.5 & 26.0 & 17.9 \\   & F1 & 82.0 & 91.3 & 88.5 & 80.0 & 89.9 & 84.8 & 81.6 & 88.9 & 92.5 & 94.5 \\  & ECE & 22.1 & 9.2 & 15.5 & 17.0 & 11.2 & 20.1 & 37.3 & 25.4 & 18.6 & 21.3 \\   

Table 1: F1 (%) \(\) and ECE (%) \(\) performances of response classification on Harmbench-adv set across 10 different response models.

Figure 3: F1 (%) \(\) and ECE (%) \(\) performances of prompt and response classification on Harmbench-adv set.

Baichuan2, Qwen, Solar, Llama2, Vicuna, Orca2, Koala, OpenChat, Starling, and Zephyr. Each subset consists of outputs from a specific response model. Detailed information on the statistics for each subset is provided in Appendix B.1. The F1 and ECE results are reported in Table 1.

**Finding 3: Guard models exhibit inconsistent reliability when classifying outputs from different response models**. The results in Table 1 reveal significant variance in both F1 and ECE across different response models. This suggests potential limitations in the training of guard models that rely on responses from a single model. For example, Aegis-Guard models are trained using responses from Mistral, and Llama-Guard models are trained using responses from internal LLama checkpoints. In contrast, Harmbench-Llama, Harmbench-Mistral, and Wildguard are trained using responses from a more diverse set of models, leading to improved generalization across different output distributions of response models.

## 4 Improving the Calibration of LLM-based Guard Models

Empirical evidence has demonstrated the miscalibration of current LLM-based guard models, necessitating efforts to improve their reliability through calibration techniques. In this section, we focus on post-hoc calibration methods to circumvent the computational expense associated with training new guard models, reserving training-time calibration approaches for further investigation.

### Calibration Techniques

**Temperature Scaling**(Guo et al., 2017). Temperature scaling (TS) is a widely employed confidence calibration method for traditional neural networks. By introducing a scalar parameter \(T>0\) on the output logits, the output distribution can either be smoothed (\(T>1\)) or sharpened (\(T<1\)). Specifically, the calibrated confidence is computed as:

\[(y=c_{i}|,)=)}}{T}}}{_ {c_{i}}e^{)}}{T}}}\] (3)

It is important to note that applying temperature scaling does not affect the maximum value of the softmax function, and thus does not alter accuracy performance. The parameter \(T\) is typically optimized on a held-out validation set with respect to the negative log-likelihood. However, in the context of the LLM content moderation task, validation sets may not always be available, posing a significant challenge, particularly when addressing in-the-wild user inputs or responses from unknown models. Besides temperature scaling, most conventional calibration methods similarly rely on validation sets to determine parameters, rendering them impractical in such scenarios. As such, we exclusively take temperature scaling as an instance for its simplicity and efficacy.

**Contextual Calibration**(Zhao et al., 2021). Contextual calibration (CC) is one type of matrix scaling technique to address contextual bias in LLMs, with the key advantage of requiring no validation set. This method estimates test-time contextual bias by using content-free tokens such as "N/A", space, or empty tokens. The calibrated prediction is then computed as follows:

\[}(y|,)=(y|, )\] (4)

where \(=((y|[N/A]))^{-1}\). Although the original purpose of contextual calibration differs from confidence calibration, the utilized vector scaling modifies model predictions and impacts confidence levels as well, warranting its consideration for confidence calibration.

**Batch Calibration**(Zhou et al., 2023). Batch calibration (BC) is also a type of matrix scaling approach. The rationale behind batch calibration is to estimate contextual bias from a batch of \(M\) unlabeled samples drawn from the target domains \(P(x)\) or \(P(x,r)\), rather than from context-free tokens as in contextual calibration. Specifically, batch calibration applies a transformation on the original prediction, which can be interpreted as a linear transformation in the log-probability space,

\[}(y|,)=(y|, )-\] (5)

where \(\) is computed in a content-based manner by \(=-_{x P(x)}[(y|x)]- _{i=1}^{M}(y|x^{(i)})\) for prompt classification or \(=-_{x,r P(x,r)}[(y|x,r)]- _{i=1}^{M}(y|x^{(i)},r^{(i)})\) for response classification. Note that batch calibration requires a batch of unlabeled samples to estimate the contextual prior during test time.

### Calibration Results

We apply the calibration methods discussed in Section 4.1 to both prompt classification and response classification for each guard model. For temperature scaling, we utilize the XTest set as the validation set to optimize the temperature due to its relatively small size. This optimized temperature value is then applied across all other datasets, as individual validation sets are not available for all examined datasets. For contextual calibration, we estimate the contextual bias using a space token. For batch calibration, we assume access to the full test set and estimate the contextual bias using the entire test set as a batch. The resulting calibration performance is reported in Table 2. Details regarding the implementation can be found in Appendix B.3, along with additional calibration results in adversarial environments in Appendix D.2.

**Contextual calibration proves more effective for prompt classification and temperature scaling benefits response classification more**. Empirical results indicate that contextual calibration outperforms other methods in prompt classification, delivering improved calibration for the majority of guard models, with the exception of WildGuard. Additionally, temperature scaling effectively reduces the ECE and demonstrates particular effectiveness, despite being optimized on a validation set with a potentially different distribution from the target dataset. This finding further confirms the shared overconfident predictions across datasets and validates that proper temperature values can smooth the overconfident prediction distribution, thereby mitigating miscalibration. Furthermore, temperature scaling shows greater efficacy in response classification which often involves multiple sentences of both user inputs and model responses. In such cases, contextual calibration struggles to accurately estimate contextual prior, resulting in unstable or even degraded calibration performance. Moreover, it is noteworthy that batch calibration underperforms compared to contextual calibration

for most models in prompt classification, as well as some models in response classification. We conjecture that this could be attributed to significant label shifts in the test datasets, leading to biased contextual prior estimation and diminished calibration effectiveness. However, no single method fully resolves the miscalibration issues, indicating the complexity of achieving reliable safety moderation across different deployment scenarios.

## 5 Discussion

In this section, to further understand why miscalibration of guard models happens, and how it manifests in prompt and response classification, we conduct two further investigations and point out the limitation and weak robustness of instruction-tuning LLM-based guard models.

**Prediction Vulnerability Induced by a Single Token**. We analyze two specific scenarios by assessing model predictions when the user input consists of a space token or an "unsafe" token and both the user input and model response consist of a space token, respectively. Results of the probability of the input being classified as "safe" are reported in Table 3. The results demonstrate that many guard models exhibit high confidence in predicting "safe" for a space token input. However, the introduction of a single "unsafe" token without further context can cause many guard models to confidently predict "unsafe". This finding underscores the persistent contextual bias in guard models revealing their limitations even after instruction-tuning. More extensive robustness evaluations of guard models are thus essential for future research.

**Misaligned Classification Objectives**. We further investigate guard models in the LLama-Guard family capable of both prompt and response classification, focusing on the accuracy of predictions when the model response is set to a content-free token. Specifically, we sample 100 "safe" user inputs and 100 "unsafe" user inputs from the WildGuardTest set and replace all model responses with a space token. We report the average probability of classifying the response as "safe" for using "safe" and "unsafe" user inputs separately in Table 4. The results indicate that the model is more likely to predict the responses as "unsafe" when user inputs (prompt) are unsafe, even when model responses are content-free and should logically be predicted as "safe". This suggests that the model prediction is heavily influenced by the user input and the guard models act like conducting prompt classification even when response classification should be done. Such behavior can result in unreliable predictions and increased miscalibration.

## 6 Conclusion

In this work, we have systematically examined the uncertainty-based reliability of LLM-based guard models by assessing their calibration levels across various benchmarks. Our analysis reveals that despite their promising performance in content moderation, these models tend to make overconfident predictions, exhibit significant miscalibration under adversarial environments, and lack robustness to responses generated by diverse LLMs. To mitigate miscalibration, we explore several post-hoc calibration techniques. Our results show that contextual calibration proves particularly effective for prompt classification and temperature scaling improves response classification performance more. Our findings underscore the importance of uncertainty-based reliability and advocate for incorporating confidence calibration evaluation in the development and release of future LLM-based guard models.

  
**Model** & **Safe prompt** & **Unsafe prompt** \\  Llama-Guard & 62.1 & 29.0 \\ Llama-Guard2 & 63.0 & 21.5 \\ Llama-Guard3 & 62.9 & 16.4 \\   

Table 4: Probability (%) of “safe” for response classification when output is set as a space token and inputs are sampled from safe/unsafe prompts.

    &  & **Response** \\  & **Space** & **“unsafe”** & **Space** \\  Llama-Guard & 75.5 & 18.2 & 73.1 \\ Llama-Guard2 & 98.9 & 83.5 & 99.2 \\ Llama-Guard3 & 90.5 & 53.1 & 98.8 \\ Aegis-Guard-D & 29.4 & 9.5 & 29.4 \\ Aegis-Guard-P & 53.1 & 16.5 & 53.1 \\ HarmB-Llama & - & - & 98.8 \\ HarmB-Mistral & - & - & 91.7 \\ MD-Judge & - & - & 89.3 \\ WildGuard & 99.5 & 92.0 & 77.7 \\   

Table 3: Probability (%) of “safe” for prompt classification when input is set as a space token or “unsafe” token, and response classification when input and model output are set as a space token.