# Post-Hoc Reversal: Are We Selecting Models Prematurely?

Rishabh Ranjan\({}^{1}\)

Saurabh Garg\({}^{2}\)

Mrigank Raman\({}^{2}\)

Carlos Guestrin\({}^{1,3}\)

Zachary Lipton\({}^{2}\)

\({}^{1}\)Stanford University, \({}^{2}\)Carnegie Mellon University, \({}^{3}\)Chan Zuckerberg Biohub

{ranjanr,guestrin}@stanford.edu, {sgarg2,mrigankr,zlipton}@cmu.edu

###### Abstract

Trained models are often composed with post-hoc transforms such as temperature scaling (TS), ensembling and stochastic weight averaging (SWA) to improve performance, robustness, uncertainty estimation, etc. However, such transforms are typically applied only after the base models have already been finalized by standard means. In this paper, we challenge this practice with an extensive empirical study. In particular, we demonstrate a phenomenon that we call _post-hoc reversal_, where performance trends are reversed after applying post-hoc transforms. This phenomenon is especially prominent in high-noise settings. For example, while base models overfit badly early in training, both ensembling and SWA favor base models trained for more epochs. Post-hoc reversal can also prevent the appearance of double descent and mitigate mismatches between test loss and test error seen in base models. Preliminary analyses suggest that these transforms induce reversal by suppressing the influence of mislabeled examples, exploiting differences in their learning dynamics from those of clean examples. Based on our findings, we propose _post-hoc selection_, a simple technique whereby post-hoc metrics inform model development decisions such as early stopping, checkpointing, and broader hyperparameter choices. Our experiments span real-world vision, language, tabular and graph datasets. On an LLM instruction tuning dataset, post-hoc selection results in \(>1.5\) MMLU improvement compared to naive selection.2

## 1 Introduction

Many widely used techniques in deep learning operate on trained models; we refer to these as _post-hoc transforms_. Examples include temperature scaling (TS) , stochastic weight averaging (SWA)  and ensembling . These techniques have shown promise for improving predictive performance, robustness, uncertainty estimation, out-of-distribution generalization, and few-shot performance . Typically, the pre-training and post-hoc stages are isolated. The workflow is: (1) pick model architecture, training recipe, hyperparameters, etc. to optimize for individual model performance; (2) train one or more models; (3) pick best-performing checkpoints; (4) apply post-hoc transforms. We refer to this procedure as _naive selection_.

In this paper, we demonstrate interesting drawbacks of naive selection. In a large-scale empirical study, we uncover _post-hoc reversal_--a phenomenon whereby post-hoc transforms reverse performance trends between models (Fig. 1). We demonstrate post-hoc reversal with respect to training epochs, model sizes, and other hyperparameters like learning rate schedules. We further establish that post-hoc reversal is a robust phenomenon by experimenting on real-world datasets across domains and modalities, with diverse model classes and training setups.

Post-hoc reversal is most prominent on noisy datasets (Fig. 2). Other phenomena exacerbated by noise include catastrophic overfitting ,double descent , and loss-error mismatch . While these phenomena pose challenges to model development, post-hoc reversal suggests a path to alleviate them. Noise can arise not only from labeling errors, but also from inherent uncertainty in the prediction task, such as in next token prediction . Indeed, severe performance degradation has limited multi-epoch training of large language models (LLMs) . Here too, post-hoc reversal reveals a promising path for sustained performance improvements over longer training.

The core intuition for post-hoc reversal is that models continue to learn generalizable patterns from clean examples, even when spurious patterns learnt from mislabeled examples worsen the overall performance. Post-hoc transforms exploit differences in the learning dynamics of clean and mislabeled examples  to reinforce the influence of the former, while suppressing that of the latter. When strong enough, this effect leads to reversal. We show evidence for these intuitions in SS 5.

Based on our findings, we propose _post-hoc selection_--a simple technique whereby base models are selected based on post-transform performance. The technique is practical as the transforms of interest can be cheaply incorporated into the validation phase of the training loop. Post-hoc selection significantly improves the performance of the transformed models, with \(>2\) improvements over naive selection in some cases (Fig. 2). In terms of absolute performance, post-hoc selection leads to \(>3\)-point reduction in test error over naive selection on a satellite imaging dataset (Fig. 1). The reduction is even higher (\(>5\) points) when using out-of-distribution (OOD) val/test splits for the same dataset. On an LLM instruction tuning dataset, under our procedure a composed transform of SWA, ensemble and TS gives \(>1.5\) MMLU improvement over a naive application of the same transform on prematurely selected models.

Figure 1: An illustration of the _phenomenon_ of _post-hoc reversal_ on the FMoW dataset: base performance at epoch \(t_{2}\) is worse than at epoch \(t_{1}\) (\(b_{2}>b_{1}\)), but post-hoc performance is better (\(p_{2}<p_{1}\)). The current practice of _naive selection_ considers base metrics to pick models at epoch \(t_{1}\). Our proposed _technique_ of _post-hoc selection_ instead uses post-hoc metrics to pick models at epoch \(t_{2}\), resulting in \(>2\) improvement over naive selection in both test loss and error. SWA+Ens+TS refers to the post-hoc transform obtained by composing SWA, ensemble (Ens) and temperature scaling (TS). Base curves show mean of \(8\) runs, models from which constitute the ensembles. Individual runs are shown in lighter colors. See Fig. 5 for more detailed curves on this dataset.

Figure 2: A comparison of naive and post-hoc selection on label sets from CIFAR-10/100-N (abbr. C-10/100-N) for the SWA+TS transform. On noisy label sets, post-hoc selection is often \(>2\) better.

Related Work

A slew of empirical works [10; 17; 31; 55; 57; 58] have revealed both challenges and opportunities for improving the understanding and practice of deep learning. Our work expands this list with a novel phenomenon tying together noisy data learning and post-hoc transforms. Orthogonal to our work, a number of training-stage strategies for noisy data have been proposed (see  for a survey).

TS belongs to a family of calibration techniques [2; 19] proposed with the goal of producing well-calibrated probabilities. Ensembling is a foundational technique in machine learning, with simple variants routinely used in deep learning [3; 39]. SWA  is the culmination of a line of work [18; 25] seeking to cheaply approximate ensembling. Despite their prevalence, a thorough understanding of best practices for wielding these techniques is lacking, especially in the context of noisy data. Our work fills this gap. For a more detailed discussion on related work, see App. A.

## 3 Preliminaries and Background

We describe our learning setup in SS 3.1, with emphasis on noisy data, a key focus of this work. In SS 3.2, we introduce the post-hoc transforms we study.

### Learning on Noisy Data

**Setup.** We consider multi-class classification with \(C\) classes, input \(\) and label \(y=\{1,,C\}\). Training, validation and test sets are drawn i.i.d. from the data distribution \(\). A _classifier_\(f^{C}\) outputs the logit vector \(=f(;)\), given parameter vector \(\). Predicted probability of class \(k\) is \(_{f}[y=k]=()_{k}\), where \(\) is the softmax function.

**Noise.** Data \(\) is said to be _clean_ if \(_{}[y]\) is one-hot for all \(\), _i.e._, \(_{}[y]=\{y=y^{*}()\}\) for some labeling function \(y^{*}\). Then, for any example input \(^{(i)}\) in the dataset, the observed label is \(y^{(i)}=y^{*}(^{(i)})\). When \(_{}[y]\) is not one-hot, \(\) is said to be _noisy_ and the observed label is only a stochastic sample \(y^{(i)}_{}[y=^{(i)}]\) from the underlying conditional distribution. Noise can arise due to (1) non-determinism in the prediction target (2) insufficient information in the input context, and (3) annotation errors. See App. B.1 for illustrated examples.

**Metrics.** A metric \(^{C}\) compares the predicted logits \(\) with the observed label \(y\). \(_{f}()=[f(\,\,;)]= _{(,y)}[(f(;),y)]\) denotes the metric computed over \(\) given \(f\) and \(\). We use two metrics (1) _classification error_, or simply _error_, with \(^{}(,y)=\{_{k}_{k } y\}\) and (2) _cross-entropy loss_, or simply _loss_, with \(^{}(,y)=-()_{y}\). The exponentiated loss, also called _perplexity_, is common in language modeling, where it is computed on a per-token basis. A standard result states that loss is minimized if and only if the ground truth conditional probability is recovered . See App. B.1 for additional background.

### Post-Hoc Transforms in Machine Learning

**Definition 1** (Post-Hoc Transform): _A post-hoc transform \(\) maps a classifier \(f\) to another classifier \( f^{K}\), for some \(K\)._

**Temperature Scaling (TS).** TS  involves scaling the logits with a _temperature_\(\) obtained by optimizing the cross-entropy loss over the validation set, with model parameters fixed (Eqn. 1). Temperature scaling preserves error as it does not affect the predicted class. We use the torchcal implementation, which optimizes the temperature on GPU with Newton's method .

\[(_{} f)(;)=f( ;),=_{}^{}_{ }[f(\,\,;)]\] (1)

**Ensembling.** In this method, predictions from an ensemble of classifiers are combined. In deep learning, simply averaging the temperature-scaled logits is effective (Eqn. 2). \(_{1},,_{K}\) are obtained from multiple training runs with the same architecture and dataset, with stochasticity from mini-batch sampling and random initialization, if applicable.

\[(_{} f)(;_{1},, _{K})=_{k=1}^{K}}f(; _{k}),_{k}=_{}_{}^{}[f(\,\,;_{k})]\] (2)

**Stochastic Weight Averaging (SWA).** SWA  involves averaging weights \(_{1},,_{K}\) from the same training run (Eqn. 3). BatchNorm statistics are recomputed after averaging, if required. We pick checkpoints at epoch boundaries. Unlike Izmailov et al. , we do not skip the initial epochs (warmup) or modify the learning rate schedule3.

\[(_{} f)(;_{1}, ,_{K})=f(;_{i=1}^{K} _{i})\] (3)

**Compositions.** TS, ensembling and SWA can be readily composed. In particular, we consider _SWA+TS_ and _SWA+Ens+TS_, for single- and multi-model settings respectively. We denote them with \(_{}=_{}_{}\) and \(_{}=_{}_{}_{}\) (explicit forms in App. B.2).

## 4 Post-Hoc Reversal: Formalization and Empirical Study

To use post-hoc transforms, one must first select models to apply them to. Current practice is to select the best-performing model independent of post-hoc transforms, rationalized by an implicit _monotonicity_ assumption - "better-performing models result in better performance after transformation". As we shall see, this assumption is often violated in practice. We call such violations _post-hoc reversal_. In SS 4.1, we formalize post-hoc reversal and discuss ways to detect it. In SS 4.2, we empirically study various kinds of post-hoc reversal with special practical relevance.

### Definitions

First, we give a general definition of post-hoc reversal (Def. 2). If Def. 2 holds with \(_{k}\)'s which are optimal for the base metric \(_{f}\), then naive selection becomes suboptimal as it picks \(_{k}\)'s, but \(_{k}\)'s are better under the post-hoc metric \(_{ f}\). Since the entire space of parameter tuples \(^{K}\) can be large, we study post-hoc reversal restricted to indexed parameters (Def. 3). Indices can be, for example, training epochs (SS 4.2.1), model sizes (SS 4.2.2) or hyperparameter configurations (SS 4.2.3).

**Definition 2** (Post-hoc reversal): _Let a post-hoc transform \(\) map a classifier \(f\) to \( f^{K}\). \(\) applied to \(f\) exhibits post-hoc reversal for a metric \(\) if there exist \((_{1},,_{K}),(_{1},,_{K}) ^{K}\) such that \(_{f}(_{k})_{f}(_{k})\) for all \(k=1,,K\) but \(_{ f}(_{1},,_{K})<_ { f}(_{1},,_{K})\)._

**Definition 3** (Index-wise post-hoc reversal): _Let \(\) be a set of indices and \(^{K}\) map indices to parameter tuples. When Def. 2 holds with \((_{1},,_{K})=(s),(_{1},, _{K})=(t)\) for some \(s,t\), we call it index-wise post-hoc reversal._

**Diagnosis.** To enable a visual diagnosis of post-hoc reversal, we define base and post-hoc curves (Def. 4) and a relaxed notion of post-hoc reversal for them (Def. 5). Post-hoc reversal is characterized by non-monotonicity between the base and post-hoc curves, i.e., there exist regions where one improves while the other worsens. This happens, for instance, when one curve exhibits double descent but the other doesn't. Different optimal indices for the two curves is another indicator of post-hoc reversal.

**Definition 4** (Base and post-hoc curves): _The base and post-hoc curves \(^{},^{}\) are given by \(^{}(t)=_{k=1}^{K}_{f}(_ {k})\) and \(^{}(t)=_{ f}(_{1},, _{K})\), where \((_{1},,_{K})=(t)\)._

**Definition 5** (Post-hoc reversal for curves): _Base and post-hoc curves \(^{},^{}\) exhibit post-hoc reversal when there exist \(s,t\) such that \(^{}(s)^{}(t)\) but \(^{}(s)<^{}(t)\)._

### Experiments

#### 4.2.1 Epoch-Wise Post-Hoc Reversal

When the indices in Def. 3 are training epochs, we call it _epoch-wise post-hoc reversal_. We use \(_{t}\) to denote the model at the end of epoch \(t\). For ensembles, a superscript \(j\) denotes the \(j\)-th training run (out of \(N\) runs). \(t\) maps to parameters \((t)^{K}\) (\(K=1\) for TS; \(N\) for ensemble; and \(t\) for SWA) as follows: \(_{}(t)=(_{t});_{}(t)=(_{t} ^{1},,_{t}^{N})\)4; \(_{}(t)=(_{1},,_{t})\).

**Experimental setup.** We focus on the CIFAR-N dataset . CIFAR-10-N uses the same images as CIFAR-10 but provides multiple human-annotated label sets, allowing the study of realistic noise patterns of varying levels in a controlled manner. Clean is the original label set; Rand1,2,3 are 3 sets of human labels; Aggre combines Rand1,2,3 by majority vote; and Worst combines them by picking an incorrect label, if possible. Similarly CIFAR-100-N has two label sets, Clean and Noisy, with the latter being human-labeled. We train ResNet18  models for 100 epochs with a cosine annealed learning rate. Additional details on datasets and training setup are in App. C. Fig. 3 shows test curves on CIFAR-10-N Clean, Rand1 and Worst. Other label sets and CIFAR-100-N are in App. E. For clarity, we omit the SWA base curve \(_{}^{}(t)=(_{f}(_{1})++ _{f}(_{t}))/t\) in the plots, and simply re-use the curve \(^{}(t)=_{f}(_{t})\) to compare with the post-hoc SWA curve. While deviating from Def. 4, this better reflects the current practice of early stopping on the latest epoch's base metric.

**Observations.** First, we focus on the base curves: _(1) Overfitting:_ As noise increases, test curves go from a single descent to a double descent to a U-shaped curve with increased overfitting. _(2) Double descent:_ Noise amplifies double descent, and the second descent worsens with increasing noise (as compared to the first). _(3) Loss-error mismatch:_ Loss overfits more drastically than error, leading to a mismatch with higher noise. Optimal models for loss and error can be different.

Figure 3: Loss and error for CIFAR-10-N Clean (approx. \(0\%\) noise), Rand1 (approx. \(17\%\) noise) and Worst (approx. \(40\%\) noise). Except for ensemble curves, mean of \(8\) runs is shown; individual runs are in lighter shades. Ensembles comprise models from these \(8\) runs. For example, observe post-hoc reversal for C-10-N Worst: (1) error plot: from epoch \(5\) to \(50\), solid red (base) curve worsens but solid orange (SWA) curve improves; (2) error plot: solid red (base) curve has a double descent but dashed red (ensemble) curve does not; (3) loss plots: solid red (base) curve has a double descent pre-TS but not post-TS; (4) error plot: best error is at approx. epoch \(5\) for solid red (base) curve but at approx. epoch \(60\) for dashed orange (SWA ensemble) curve.

Next, we consider the general impact of post-hoc transforms: _(4) Performance improvements:_ TS, SWA and ensemble always improve performace, both individually and in composition with larger gaps for noisy label sets. _(5) Post-hoc reversal:_ Post-hoc reversal manifests as non-monotonicity between the base and post-hoc curves, especially for noisy label sets. _(6) SWA vs Ensemble:_ SWA can recover much of the ensemble gain, but the optimal epoch often differs a lot from the base curve. _(7) Smoother curves:_ Base curves fluctuate wildly, but SWA and ensemble curves are smooth, making them more reliable for early stopping.

Finally, we discuss some benefits from post-hoc reversal: _(8) Overfitting:_ All transforms reduce overfitting, often reverting performance degradation. _(9) Double descent:_ SWA, ensemble and compositions flatten the double descent peak. TS, on the other hand, leads to a double descent for some cases where there was none before. _(10) Loss-error mismatch:_ TS aligns the loss and error curves, enabling simultaneously good loss and error.

#### 4.2.2 Model-Wise Post-Hoc Reversal

Here, indices represent model sizes. Models of all sizes are trained for \(T\) epochs, large enough for convergence. Following , we avoid early stopping. Notation-wise, we add a subscript to \(\) to indicate the model size \(s\). Parameters are indexed as follows: \(_{}(s)=(_{T,s})\); \(_{}(s)=(^{1}_{T,s},,^{N}_{T,s})\); \(_{}(s)=(_{1,s},,_{T,s})\).

**Experimental setup.** We parameterize a family of ResNet18s by scaling the number of filters in the convolutional layers. Specifically, we use \([k,2k,4k,8k]\) filters for width \(k\). The standard ResNet18 corresponds to \(k=64\). Otherwise the training setup is same as before. Fig. 4 shows the curves. Concretely, the index set \(=\{2,4,,64\}\) is the set of ResNet widths \(k\) described above.

**Observations.** Post-hoc transforms improve performance (up to \( 10\) points for error) and mitigate double descent. Further, we see yet another way in which higher-capacity models are better: they give better results under post-hoc transforms even when lower-capacity base models perform better.

Figure 4: C-10-N Worst test curves against model size. Best width for solid blue curves is \( 10\) but for dashed orange curves, it is \( 50\) for error and \( 25\) for post-TS loss.

Figure 5: FMoW test curves for \(3\) LR schedules. Note that the pre-TS loss is significantly higher than the post-TS loss. For example, observe post-hoc reversal w.r.t. cosine and constant LRs at epoch \(50\) between: (1) solid blue (base) and dashed blue (ensemble) error curves; (2) solid blue (base) and solid orange (SWA) post-TS loss curves; (3) solid blue (base) curves for pre-TS and post-TS loss.

#### 4.2.3 Hyperparameter-Wise Post-Hoc Reversal

In general, the index set \(\) can contain any hyperparameter configurations. Here, we consider two hyperparamters: learning rate schedule and training epochs. To avoid repeating CIFAR-N epoch-wise curves, we experiment on a fresh dataset, FMoW.

**Experimental setup.** We experiment on learning rates (LRs) and training epochs, with index set \(=\{,,\}\{1,,T\}\). Here, \(\), \(\) and \(\) refer to constant, exponentially decaying and cosine annealed LRs respectively, and \(T\) is the total number of epochs. We train DenseNet121  models on the FMoW dataset  which constitutes a \(62\)-way classification of land use from satellite images. For more details, see App. C. Fig. 5 shows the curves.

**LR-wise observations.** We see some interesting instances of post-hoc reversal: (1) constant LR has the worst base performance but the best post-hoc performance; (2) under SWA and TS (composed), the curves continue to improve at the later epochs for constant LR, but not for the decaying LRs5.

**Epoch-wise observations.** Epoch-wise post-hoc reversal occurs for all LR schedules. SWA and ensembling convert the double descent into a strong single descent, with approx. \(10\)-point improvement in error for the latter. For constant LR, this also changes the optimal epoch. SWA only recovers about half of the ensemble gain, and perhaps surprisingly, ensembling SWA models is not better than ensembling alone. Pre-TS loss curves show a strong mismatch with the error curves, but TS enables simultaneously good loss and error with the last epoch models. Overall, these observations reinforce the trends gleaned from the CIFAR-N experiments.

## 5 Intuitions for Post-Hoc Reversal

In this section, we give hypotheses for post-hoc reversal, backed by experimental evidence.

**Ensembling and SWA delay catastrophic overfitting.** Models learn generalizable patterns from clean examples, and spurious patterns from mislabeled ones. The latter causes overfitting. When noise is low, the former dominates and overfitting is benign. Otherwise, overfitting is catastrophic. Ensembling and SWA improve fitting of clean examples, and reduce memorization of mislabeled ones. When this overturns the dominance of spurious patterns, we observe reversal.

Fig. 6 validates this intuition for SWA on CIFAR-10-N Worst. Fig. 7 further suggests the underlying mechanism -- predictions on the mislabeled train subset fluctuate much more during training, allow

Figure 6: Evolution of the fit/memorization of clean and mislabeled examples during training, for base and SWA models on C-10-N Worst. Train error drops earlier for the clean subset. In the regime of post-hoc reversal (shaded), SWA further lowers the train error on the clean subset, while raising it on the mislabeled subset.

Figure 7: Flipping of predicted class between consecutive epochs, for clean and mislabeled train subsets of C-10-N Worst. % of examples flipped is about twice as high for the mislabeled subset, suggesting an unstable influence on the decision boundary.

ing SWA to easily revert their memorization. In App. G, we extend this analysis to ensembling and solidify the intuition further by visualizing decision boundaries on a synthetic dataset. This explanation also applies to flattening of the double descent peak, which is a manifestation of catastrophic overfitting.

**TS mitigates loss-error mismatch.** Once a neural net has fit a train example, the cross-entropy loss on it can be lowered by simply upscaling the weights of the linear output layer. This makes the model overconfident later in training, as shown in . For a mislabeled example, this leads to worse loss on similar test instances. The test error is not affected as it is independent of the scale of the logits. In high-noise settings, test loss can worsen due to memorization of mislabeled examples, even as the test error improves from continued learning on clean examples, leading to loss-error mismatch. TS fixes this by downscaling the logits. Indeed, one finds that the temperature (as obtained with a held-out set) increases with epochs (Fig. 8).

**Post-hoc reversal can occur against epochs, model sizes or other hyperparameters.** Different variants of post-hoc reversal can be unified via _effective model complexity_ (EMC), introduced in  to unify epoch- and model-wise double descent. EMC measures memorization capacity, which plays a key role in post-hoc reversal. EMC increases with epochs and model size. Further, EMC increases with epochs more rapidly for constant LR than annealed LR, explaining our observations in SS 4.2.3.

## 6 Post-Hoc Selection: Leveraging Post-Hoc Reversal in Practice

Our findings from SS4 motivate the principle of _post-hoc selection_, where model development decisions take post-hoc transforms into account. For concreteness, we discuss the choice of checkpoints from training runs under the SWA+TS and SWA+Ens+TS transforms. Checkpoint selection reduces to the selection of the final epoch \(\), as SWA uses all checkpoints up to that epoch. \(_{}\) denotes a metric of choice computed on the validation set.

**SWA+TS.** Naive selection picks epoch \(=_{T}_{f}^{}(_{T})\). In contrast, post-hoc selection picks \(=_{T}_{_{} f}^{}((_{t})_{t=1}^{T})\).

**SWA+Ens+TS.** Here we have \(N\) different training runs to pick epochs for. Naive selection picks \(_{j}=_{T}_{f}^{}( _{T}^{j})\) for each run independently. In contrast, post-hoc selection would ideally pick \(_{1},,_{N}=_{T_{1},,T_{N}} _{_{} f}^{}((_{t}^{1 })_{t=1}^{T_{1}},,(_{t}^{N})_{t=1}^{T_{N}})\) which jointly minimizes the ensemble performance. This being computationally expensive, we instead minimize under the constraint \(_{1}==_{N}\)6

**Results.** Tab. 1 compares naive and post-hoc selection strategies for CIFAR-N and FMoW. Except for some clean label sets, post-hoc selection is always better than naive selection, often with \(>2\) improvement from post-hoc selection as compared to naive selection. It remains effective with out

    &  &  \\   & None &  &  &  &  &  \\   & & Naive & Ours & Naive & Ours & & Naive & Ours & Naive & Ours \\  C-10-N Clean & 0.435 & **0.269** & 0.270 & 0.234 & **0.233** & 9.75 & **9.09** & 9.10 & 8.30 & **8.24** \\ C-10-N Aggre & 0.722 & 0.663 & **0.585** & 0.608 & **0.543** & 19.20 & 17.08 & **16.95** & 15.88 & **15.74** \\ C-10-N Randl & 1.009 & 0.968 & **0.907** & 0.916 & **0.859** & 28.63 & 27.13 & **24.84** & 24.80 & **25.50** \\ C-10-N Worst & 1.511 & 1.483 & **1.443** & 1.437 & **1.399** & 46.84 & 46.12 & **44.14** & 44.30 & **42.88** \\  C-100-N Clean & 1.508 & 1.215 & **1.205** & 1.065 & **1.063** & 33.83 & **32.67** & 32.69 & **29.90** & 29.94 \\ C-100-N Noisy & 2.416 & 2.289 & **2.136** & 2.129 & **1.994** & 58.68 & 54.94 & **53.18** & 51.34 & **50.26** \\  FMoW (ID) & 1.583 & 1.627 & **1.554** & 1.494 & **1.305** & 43.20 & 42.69 & **39.92** & 37.95 & **34.93** \\ FMoW (ODD) & 1.831 & 1.840 & **1.788** & 1.700 & **1.571** & 49.32 & 49.70 & **46.75** & 46.74 & **41.56** \\   

Table 1: Naive vs post-hoc (ours) selection for SWA+TS and SWA+Ens+TS transforms. Better values are in bold. Except some clean cases, post-hoc selection is always better, often more than doubling the improvement over no transform. See Tabs. 6 and 8 in App. E for standard deviations.

of-distribution (OOD) val/test sets, as seen for FMoW (we use ID and OOD splits from WILDS ). For some datasets, like C-100-N Noisy, post-hoc selection is only marginally better on test error. Often, in such cases, the error floor is already quite high (e.g., C-100-N Noisy has \( 40\%\) noise and ResNet-18 has \( 10\%\) error on clean C-100, so a test error of \( 50\%\) is already impressive), and test loss is a more appropriate metric.

**Early stopping.** We advocate monitoring post-hoc metrics for early stopping. Only a running average needs to be updated for SWA, and TS involves a quick single-parameter optimization. Further, while the base curves can fluctuate wildly between consecutive runs, SWA+TS curves are considerably smoother (see Figs. 3, 11 and 10), making them more reliable for automated early stopping. One can similarly monitor metrics for SWA+Ens+TS under parallel training runs.

## 7 Experiments Across Domains and Modalities

In SS 4 and SS 6, we introduced post-hoc reversal and selection with experiments on the CIFAR-N and FMoW datasets. In this section, we supplement our experimental analysis with additional experiments across diverse domains and modalities to demonstrate the generality of our findings.

### LLM Instruction Tuning

Language models are pre-trained or fine-tuned with a self-supervised objective of predicting the next token in a text corpus. There might be many acceptable tokens following a given prefix, albeit with different probabilities. Thus next token prediction is noisy and one might reasonably expect to see post-hoc reversal. In this section, we test this hypothesis for the task of fine-tuning LLMs to follow instructions (instruction tuning ). Instruction tuning datasets are naturally small  and amenable to multi-epoch training where catastrophic overfitting becomes an important concern. Recent works [53; 81] have argued for data repetitions for LLM pre-training as well, but such experiments are beyond the scope of this paper.

**Experimental setup.** We fine-tune LLaMA-2-7B  on the Guanaco dataset  of chat completions. We evaluate perplexity and causal language modeling (CLM) error on the test set, and also the MMLU accuracy  to better contextualize model improvements. Fig. 9 shows the curves. Tab. 7 in App. E gives exact numbers, and App. F explores sub-epoch checkpointing. For TS, we use a shared temperature parameter to scale the logits of all tokens and leave more involved strategies like _long-horizon temperature scaling_ to future work.

**Observations.** We observe post-hoc reversal between epochs 1 and 2 for perplexity and error, and between epochs 2 and 3 for MMLU. Both SWA+TS and SWA+Ens+TS transforms show significant improvements, much of which is only realized under post-hoc selection.

### Other Text, Tabular and Graph Datasets

In this section, we further expand our experimental coverage to text, tabular and graph classification datasets from real-world applications.

Figure 9: Perplexity and causal language modeling (CLM) error on the Guanaco test set, and MMLU accuracy (higher is better) for instruction tuning LLaMA-2-7B. Shading indicates post-hoc reversal. Base and SWA+TS curves are mean of \(8\) runs; SWA+Ens+TS ensembles models from these runs. Individual runs are not shown as they have high variance (see Tab. 7 in App. E).

**Experimental setup.** We consider the following tasks: (1) sentiment classification on the Yelp reviews dataset  (text) with a pre-trained transformer BERT , (2) prediction tasks on census data from Folktables  (tabular) with MLPs and (3) community detection on the Reddit and Collab datasets  (graph) with graph neural networks (GNNs). Folktables has 5 prediction tasks: Income, PublicCoverage, Mobility, Employment and TravelTime. Reddit has 2 versions: Reddit-5k and Reddit-12k. For more details, see App. C. Figure 10 shows curves for Yelp, Income and Reddit-12k. Tab. 5 in App. D compares naive and post-hoc selection on all datasets.

**Observations.** Post-hoc reversal is a recurring feature across datasets, transforms and metrics. The 3 datasets show different patterns between the base and post-hoc curves, showing that post-hoc reversal can take a variety of forms.

## 8 Conclusion

We empirically studied temperature scaling (TS), ensembling, stochastic weight averaging (SWA) and their compositions, and found that these transforms can reverse model peformance trends (post-hoc reversal). Based on our findings, we presented the simple technique of post-hoc selection, and showed that it outperforms naive selection. We validated our findings and proposals over diverse settings.

Our work has broad implications for the field of deep learning. It shows that current practices surrounding the use of post-hoc transforms leave much room for improvement. This is especially true for noisy data, which is pervasive in real-world applications. Future directions include better strategies for checkpoint selection, developing a theoretical understanding, investigating impacts on scaling laws, and characterizing other instances of post-hoc reversal.

**Summary of practical recommendations.** We advocate for the use of TS, ensembling and SWA across deep learning applications. Further, such transforms should be tightly integrated into the model development pipeline, following the methodology outlined in the paper. In particular: (1) apply SWA+TS and SWA+Ens+TS transforms for better results in the single- and multi-model settings respectively; (2) track temperature-scaled loss to overcome loss-error mismatch; (3) monitor post-hoc metrics to avoid premature early stopping; (4) make hyperparameter decisions informed by post-transform performance; (5) use post-hoc selection to pick model checkpoints.

Figure 10: Test curves for 3 real-world noisy datasets. Note that the pre-TS loss is significantly higher than the post-TS loss. Examples of post-hoc reversal between the base curves given by the solid blue lines and the post-hoc curves given by the dashed orange lines (SWA ensemble): (1) optimal epoch is different for base and post-hoc curves for error and post-TS loss on all datasets; (2) for error on Yelp, base curve shows double descent but post-hoc curve does not; (3) for error on Income, base curve overfits catastrophically at approx. epoch \(5\) but post-hoc curve continues improving till approx. epoch \(20\); (4) for error on Reddit-12k, base curve does not show double descent but post-hoc curve does.