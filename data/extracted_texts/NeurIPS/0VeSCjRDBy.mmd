# Adversarial Moment-Matching Distillation of

Large Language Models

 Chen Jia

SI-TECH Information Technology

jiachenwestlake@gmail.com

###### Abstract

Knowledge distillation (KD) has been shown to be highly effective in guiding a student model with a larger teacher model and achieving practical benefits in improving the computational and memory efficiency for large language models (LLMs). State-of-the-art KD methods for LLMs mostly rely on minimizing explicit distribution distance between teacher and student probability predictions. Instead of optimizing these mandatory behavior cloning objectives, we explore an imitation learning strategy for KD of LLMs. In particular, we minimize the imitation gap by matching the action-value moments of the teacher's behavior from both on- and off-policy perspectives. To achieve this action-value moment-matching goal, we propose an adversarial training algorithm to jointly estimate the moment-matching distance and optimize the student policy to minimize it. Results from both task-agnostic instruction-following experiments and task-specific experiments demonstrate the effectiveness of our method and achieve new state-of-the-art performance.

## 1 Introduction

Large language models (LLMs) like GPT-4  and LLaMA  have revolutionized natural language processing, significantly enhancing the quality of text generation across various tasks. This success is largely due to the extensive scale of training data and the substantial increase in model parameters . However, the high computational and memory requirements of these models present significant challenges for practical deployment. To address these issues, knowledge distillation (KD)  has emerged as a key technique. KD involves transferring knowledge from a large, complex teacher model to a smaller, more efficient student model, thereby maintaining high performance while reducing resource demands. Most distillation methods for auto-regressive text generation models, including LLMs, employ metrics of probability distribution distance, such as Kullback-Leibler (KL) divergence  and reverse KL divergence , aiming to align the token-level probability distributions between the teacher and student models.

The distribution matching-based distillation methods can be viewed as behavior cloning on a decision-making problem from the perspective of imitation learning [24; 14; 2]. Based on this concept, early works based on the teacher-generated outputs  or a supervised dataset  can be viewed as an _off-policy_ approach. Recent works further incorporate an _on-policy_ approach, training the student on its self-generated outputs , using KL-based divergence [14; 2; 21] and total variation (TV) distance . Accordingly, such distribution matching-based methods face the sub-optimality problem. The objective functions aimed at aligning the probability distributions between the teacher and student models can be straightforward but cannot fully capture the goal of distilling language knowledge. First, intuitively, the correct output for an input can vary, and thus behavior cloning cannot capture the full knowledge of a teacher. Besides, there is no standardized definition for the quality of a generated output given an input, which makes it difficult to define the objective of knowledge distillation. Thisimposes a significant limitation on the generalization performance of the student model through distillation.

To address the aforementioned issues, we employ a reinforcement learning (RL) formulation for the auto-regressive text generation problem and utilize the definition of imitation gap to describe the high-level goal of knowledge distillation. Additionally, we address the imitation gap for KD by matching moments of the action-value function, which reflects the quality of token-level predictions for the entire output. In addressing the action-value function, we adopt the approach of Swamy et al. , considering a two-player minimax game between the language policy and the action-value functions, aiming to minimize an upper bound of the moment-matching objective. For this purpose, we introduce an adversarial training algorithm based on the policy gradient to jointly optimize the on-/off-policy objectives. Figure 1 illustrates the overall approach.

Theoretically, we compare the moment-matching objective with other distribution-matching measurements such as step-wise TV distance and analyze the convergence rate of our algorithm to an \(\)-accurate stationary point for optimization. Empirically, we evaluate our approach on both the instruction-following dataset and three task-specific datasets for text summarization, machine translation, and commonsense reasoning. Results demonstrate that the proposed adversarial moment-matching approach effectively optimizes the moment-matching distance of the imitation gap and outperforms state-of-the-art KD methods and a range of distribution-matching-based methods. The code and implementation are released at https://github.com/jiachenwestlake/MMKD.

## 2 Related Work

**Distillation of large language models.** There has been an increasing interest in knowledge distillation (KD) of auto-regressive LMs, especially concerning large language models (LLMs) [41; 42]. This process effectively transfers elicited knowledge from teacher LLMs to smaller student models, aiming to compress the large size of neural network parameters and make LLMs more efficient. Sequence-level KD (SeqKD)  is a variation of supervised fine-tuning (SFT) in KD. It can be viewed as the simplest method for distillation of black-box LLMs by fine-tuning the student model with teacher-generated outputs. This method has been extensively used for LLMs and has achieved success [34; 6]. In contrast, distillation of white-box LLMs can make full use of internal information of the teacher model, such as logits [30; 39] and hidden states , for distribution alignment, making it more effective and efficient for KD. However, unlike previous work that explicitly clones the distribution of teacher LLMs into student models, this work learns an auxiliary \(Q\)-value function to guide KD.

**Distillation via distribution matching.** Most promising results in the distillation of white-box LLMs are achieved by minimizing divergence between the probability distributions of the teacher model

Figure 1: The comparison between the distribution-matching-based distillation and the action-value moment-matching distillation is outlined. \(_{}\) and \(_{*}\) denote the student policy and the teacher policy, respectively. For both on-policy (using student-generated outputs) and off-policy (using teacher-generated outputs) perspectives, our approach optimizes moment-matching of action-value functions (\(Q\)-functions) instead of minimizing the distribution distance measured by \(\) = KL, RKL, TV, etc.

and student models. Kullback-Leibler (KL) divergence, reverse Kullback-Leibler (RKL) divergence, and Jensen-Shannon (JS) divergence are three widely used KD objectives for auto-regressive LMs [39; 14; 2; 21; 41]. Wen et al.  have shown the equivalent formulations of sequence-level KL, RKL, JS divergences, and the step-wise terms. Additionally, they also present the strong performance of step-wise total variation (TV) distance for KD, which can upper bound the sequence-level term. As a result, most recent works focus on on-policy approaches for KD  and combine the real-time-generated outputs by students (on-policy) with the real-time-generated outputs by teachers (or from supervised datasets) (off-policy). Following this line, Gu et al.  further propose a policy gradient-based method to address the high variance issues of RKL-based methods while Ko et al.  propose a more efficient and effective method using a skew KL divergence loss and an adaptive off-policy approach. We also focus on a combination of on-policy and off-policy objectives for KD, but we introduce a more sophisticated moment-matching approach instead of directly using the well-studied distribution-matching metrics such as KL, RKL, JS divergences, and TV distance.

**Distillation via reinforcement learning.** In a common formulation of RL in text generation [44; 26; 15], an auto-regressive model can be viewed as a language policy, making decisions on the next token (action) based on the currently generated sequence (state). From this perspective, KD corresponds to behavior cloning in imitation learning [20; 7; 14; 2]. For imitation learning in text generation, early works such as SeqGAN  and TextGAIL  utilize a generative adversarial framework to balance between the reward model, optimized by discriminating generated/real-word text, and the language policy, optimized by policy gradient-based methods using the reward model. Existing work on KD via imitation learning refers to ImitKD , which optimizes the student policy by learning from demonstrations of the teacher model. RL-based distillation can also be especially relevant for leveraging the feedback from the teacher to train student models [4; 9], in which teacher models are used to generate the feedback data for training a reward model. We build our method upon an RL-based imitation learning framework. However, unlike previous work [20; 14; 2], we propose an adversarial moment-matching approach to enhance behavior cloning.

## 3 Method

### Notations and Definitions

In this section, we consider the text generation task as a decision-making process and give a corresponding reinforcement learning (RL) formulation.

**Text generation.** Given an input \(\), the auto-regressive generation task in our work aims to generate a sequence of tokens as the output \((y_{1},,y_{T})\), where \(y_{t}\) comes from a vocabulary \(\). For simplicity, we define \(=(y_{0},y_{1},,y_{T})\) as the full input-output sequence, where \(y_{0}=\) denotes the input. The generator is modeled by a conditional probability distribution \(p_{}(|)=_{t=0}^{T-1}p_{}(y_{t+1}|_{ t})\), where \(_{ t}\) denotes the prefix \((y_{0},y_{1},,y_{t})\), \(t\{0,1,,T-1\}\).

**RL formulation.** We model text generation as a finite-horizon, time-independent Markov decision process. At each time step \(t\{0,,T-1\}\), the policy \(_{}\) takes an action (\(t\)): \(y_{t+1}\) based on the current state (\(t\)): \(_{ t}\), transits to the next state (\(t+1\)): \(_{ t+1}\) and receives a reward (\(t\)): \(r(_{ t},y_{t+1})\) by a reward function \(r:\). The policy corresponds to the generation model \(_{}(y_{t+1}|_{ t})=p_{}(y_{t+1}|_{ t})\). We focus on a (conditional) trajectory \(\{y_{1},_{ 1},y_{2},,_{ T-1},y_{T}\}=:_{ }|\) which refers to a sequence of state-action pairs generated by given an initial state \(y_{0}= p_{}\) and then repeatedly sampling an action \(y_{t+1}_{}(|_{ t})\) and obtain the next state \(_{ t+1} T(|_{ t},y_{t+1})\)1 for \(T\) time steps. In such case, the probability of a (conditional) trajectory is formally represented as \(p(|,_{})=_{t=0}^{T-1}T(_{ t+1}|_{ t},y_{t+1})_{}(y_{t+1}|_{ t})\). We also define our value function and \(Q\)-value function as \(V^{_{}}(_{ t})=_{_{(t)}_{}| _{ t}}[_{t^{}=t}^{T-1}^{t^{}-t}r(_{ t ^{}},y_{t^{}+1})]\) and \(Q^{_{}}(_{ t},y_{t+1})=_{_{(t)}_{ }|_{ t},y_{t+1}}[_{t^{}=t}^{T-1}^{t^{ }-t}r(_{ t^{}},y_{t^{}+1})]\), where \((0,1)\) denotes the discounting factor. We define the RL objective in our generation task to maximize the performance \(J(_{})=_{ p_{}}_{_{ }|}[_{t=0}^{T-1}^{t}r(_{ t},y_{t+1})]\).

### Knowledge Distillation as Moment-Matching Imitation Learning

Based on the RL formulation of auto-regressive generation, we can view the goal of knowledge distillation at a high-level as to bridge the performance gap between the teacher policy and the student policy.

**Definition 1** (**Imitation gap**).: _We define the imitation gap between the teacher policy and student policy as:_

\[J(_{*})-J(_{})=}_{}\\ _{}}[_{t=0}^{T-1}^{t}r(_ { t},y_{t+1})]-}_{}\\ _{}}[_{t=0}^{T-1}^{t}r(_ { t},y_{t+1})],\] (1)

From the perspective of imitation learning , the objective of distillation from the teacher policy \(_{*}\) to the student policy \(_{}\) can be represented as to minimize the imitation gap of Eq. (1) w.r.t. the parameters of student policy \(\). A direct idea from Eq. (1) is to use moment matching over the reward to optimize the imitation gap . However, we actually care about the long-term reward, at each time step, we should consider the accumulated reward in the future output rather than the immediate reward to the fitness of previous tokens (prefix). To this end, we can alternatively use the \(Q\)-value function (def. in SS3.1) for each timestep to represent the overall reward from the current timestep to the last timestep. Similar to , we can apply the Performance Difference Lemma (PDL)  to expand the imitation gap in Eq. (1) into either off-policy or on-policy expressions.

**Proposition 1** (**Off-policy bound of imitation gap )**.: _Let \(_{Q}\) denote the set of \(Q\)-value functions induced by sampling actions from \(_{}\), then we have:_

\[J(_{*})-J(_{})_{f_{Q}}}_{}\\ _{}}[_{t=0}^{T-1}^{t}(f( _{ t},y_{t+1})-}_{y_{ }(|_{ t})\\ =:^{}(_{},f)}[f(_{  t},y)])]\] (2)

_In the following sections, we will use \(^{}(_{},f)\) to represent the off-policy moment-matching objective of imitation learning for KD._

The off-policy moment-matching objective in Proposition 1 only requires a collected dataset of teacher-generated trajectories to be evaluated and minimized.

**Proposition 2** (**On-policy bound of imitation gap )**.: _Let \(_{Q}\), denote the set of \(Q\)-value functions induced by sampling actions from \(_{*}\), then we have:_

\[J(_{*})-J(_{})_{f_{Q}}}_{}\\ _{}}[_{t=0}^{T-1}^{t}( }_{y_{*}(|_{ t}) \\ =:^{}(_{},f)}[f(_{ t },y)]-f(_{ t},y_{t+1}))]\] (3)

_In the following sections, we will use \(^{}(_{},f)\) to represent the on-policy moment-matching objective of an imitation learning for KD._

Proof.: See Appendix A.1 and Appendix A.2 for the complete derivations of Proposition 1 and Proposition 2, respectively. 

It is notable from Proposition 2 that the on-policy moment-matching objective requires interactions with the teacher to tell us what action they would take in any state visited by the student as well as on-policy samples from the student's current policy \(_{}|\).

In the remaining content of this section, we will explore the relationship between the moment-matching objectives and the existing distribution-matching objectives . At the beginning, we draw a general formulation of the state-of-the-art methods for distillation of LLMs  that rely on distribution-matching between the student's and teacher's predictions, through minimizing the step-wise probability distribution distance between the teacher policy and student policy.

**Definition 2** (**Generalized step-wise distribution distance)**.: _The off-policy and on-policy versions are defined as follows,_

\[d^{}_{}(_{},_{*}) :=}{_{_{}}}}{_{_{}}} [_{t=0}^{T-1}^{t}(_{*}(|_{  t}),_{}(|_{ t}))];\] (4) \[d^{}_{}(_{},_{*}) :=}{ _{_{}}}}{_{_{} }}[_{t=0}^{T-1}^{t}(_{*}(| _{ t}),_{}(|_{ t}))],\] (5)

_where \((,)\) denotes a distribution distance, consisting of total variation (TV) distance  and Kullback-Leibler (KL)-based divergence . Detailed definitions for these distances refer to Appendix A.3. For simplicity, we directly replace \(\) with TV, KL, RKL, etc in the following sections._

It is notable from Wen et al.  that the sequence-level KL, RKL and JS divergences can be equivalently represented as the step-wise terms, and the sequence-level TV distance can be upper bounded by the step-wise terms, which can be actually implemented by algorithms. To make a connection with the step-wise distribution distance (Definition 2), we use the following definition.

**Definition 3** (**Distribution-matching formulation of moment-matching objectives)**.: _Based on Definition 2, we can re-formulate the off-policy and on-policy moment-matching (MM) objectives (Proposition 1 and Proposition 2, respectively) via step-wise distribution-matching, which can be defined as \(d^{}_{}(_{},_{*})\) and \(d^{}_{}(_{},_{*})\) respectively, where the distance metric \((,)\) can be defined as follows,_

\[^{}\!\!\{_{*}(| {y}_{ t}),_{}(|_{ t})\}\!\!\!= (|_{ t})}{_{_{ }}}\!\![f^{}_{*}\!\!\{_ { t},y\}]-\!(|_ { t})}{_{_{}}}\!\![f^{}_{*}\!\!\{_{ t},y\}],\] (6)

_where \(^{}(_{},f)\) and \(^{}(_{},f)\) denote the off-policy and on-policy moment-matching objectives, which are defined in Proposition 1 and Proposition 2, respectively._

Under Definition 3, we observe that the main difference between the moment-matching objectives and other step-wise distribution distance, e.g., TV distance and KL-based divergences in formulation comes from the optimal \(Q\)-value function \(f^{}_{*}\), aiming to maximize the discrepancy of its expectations based on \(_{*}(|_{ t})\)_v.s._\(_{}(|_{ t})\)_or each step \(t\{0,1,,T-1\}\). To look deeper, we draw a connection between the moment-matching objectives and step-wise TV distance using the following corollary.

**Theorem 1** (**Relationship between moment-matching objective and TV distance)**.: _Under a constrain of uniform boundness on the class of \(Q\)-value functions for off-/on-policy learning: \(_{Q}=_{Q_{*}}=\{f:\|f\|_{} 1\}\), the moment-matching objectives in Proposition 1 and Proposition 2 can be upper-bounded by the step-wise TV distance, Formally, we have_

\[J(_{*})-J(_{})  1}{}^{ }(_{},f) 2d^{}_{}(_{},_{*});\] (7) \[J(_{*})-J(_{})  1}{}^{ }(_{},f) 2d^{}_{}(_{},_{*}),\] (8)

_for the off-policy and on-policy perspectives, respectively._

Proof.: See Appendix A.4 for the complete derivation. 

We can observe from Theorem 1 that minimizing the step-wise TV distance can achieve sub-optimal results compared to optimizing the moment-matching objectives \(^{}(_{},f)\), \(^{}(_{},f)\) for off-policy and on-policy imitation learning, which are defined in Proposition 1 and Proposition 2, respectively. Thus, optimizing the moment-matching objectives can potentially achieve better optimization results for imitation learning.

### Adversarial Training Algorithm

**Optimization objective.** As shown in previous work [14; 2; 21] incorporating both the off-policy and on-policy distillation benefits effectiveness and efficiency. We thus consider a training objective to jointly minimize the off-policy moment-matching objective in Proposition 1 and the on-policy moment-matching objective in Proposition 2. Both the off-/on-policy objectives can be optimized by viewing the learning procedure as solving a game. More specifically, we consider a two-player minimax game between the student policy and the \(Q\)-value functions. To this end, we initialize two small networks of a single-layer MLP to estimate the off-/on-policy \(Q\)-value functions, respectively. For example in a causal/seq-to-seq LM, the \(Q\)-value estimate module can be represented as \(f_{_{1(2)}}(_{ t},y)=(_{t}^{_{}}+_{ y}^{()})^{}_{y}^{()}\) for any action token \(y\). This estimates the \(Q\)-value function by taking the current \(t\{0,1,,T-1\}\) hidden step of a policy network \(_{t}^{_{}}^{H}\) (for next token prediction) to combine with the feature vector of the token \(_{y}^{()}^{H}\) with a linear transformation by \(_{y}^{()}^{H}\) for off(on)-policy learning. Here, \(H\) represents the hidden size and the additional parameter cost is \((H||)\) for \(Q\)-value estimation. Finally, combining off- and on-policy objectives with a factor \((0,1)\), the optimization problem can be represented as follows,

\[_{}_{_{1},_{2}}^{}(_{},f_{_{1}})+(1-)^{ }(_{},f_{_{2}})}_{=:(_{},f_{_{ 1}},f_{_{2}})},\] (9)

where \((_{},f_{_{1}},f_{_{2}})\) represents the overall training objective. To minimize the objective w.r.t the policy parameters \(\), we use a policy gradient approach and derive the policy gradient in Appendix A.5, formally represented as follows,

\[(_{},f_{_{1}},f_{_{2}}) =}_{ p_{}}[-}_{_{}[}^{}(,)]}+(1-)}_{^{}_{} |.}[}^{}(^{}, )]]\] (10) \[ }^{}(,) =_{t=0}^{T-1}^{t}}_{y_{ }(|_{ t})}[_{}(y|_{ t})f_{ _{1}}(_{ t},y)];\] \[}^{}(^{},) =_{t=0}^{T-1}^{t}_{}(y^{}_{t+1 }|^{}_{ t})_{f_{_{2}}}(^{}_{ t},y^{ }_{t+1}),\]

where \(_{f_{_{2}}}:\) denotes the empirical \(Q\)-value defined in Eq. (21). Besides, we use stochastic gradient ascent (SGA) to maximize the objective of \((_{},f_{_{1}},f_{_{2}})\) w.r.t. parameters of the on-policy \(Q\)-value function \(_{1}\) and parameters of the off-policy \(Q\)-value function \(_{2}\).

**Training procedure.** The goal is to achieve an equilibrium between minimizing the objective w.r.t. the parameters of student policy \(\) and maximizing the objective w.r.t. the parameters of on-policy and off-policy \(Q\)-value functions \(_{1},_{2}\), formally defined as \(_{}_{_{1},_{2}}(_{},f_{_{1}},f_ {_{2}})\)(Eq. (9)). To this end, we use an adversarial training strategy in Algorithm 1, by starting from a student model fine-tuned on a dataset \(_{}\). In the training algorithm, we iteratively maximize the objective w.r.t. the parameters of \(Q\)-value functions \(f_{_{1}},f_{_{2}}\) and simultaneously minimize the objective w.r.t. the parameters of student policy \(_{}\). In each iteration of policy updating, we first perform \(N\) steps of stochastic gradient ascent (SGA) w.r.t. the parameters of \(Q\)-value functions \(_{1},_{2}\). Then, the parameters of student policy \(\) are updated by stochastic gradient descent (SGD) with the estimated policy gradient with sampling policy gradients.

### Convergence Analysis

We further provide a convergence analysis for the algorithm proposed in SS3.3. To deal with the challenges of non-convexity by certain reward structures, the algorithm is expected to obtain an \(\)-accurate stationary point of the policy parameters \(_{*}\), satisfying that \([\|(_{*})\|^{2}]\). We focus on policy optimization and directly use the optimized off-/on-policy \(Q\)-value functions in each outer-loop iteration \(k\{0,1,,K-1\}\). We denote \(_{1}(_{k})=*{arg\,max}_{_{1}}^{}(_{_{k}},f_{_{1}})\), \(_{2}(_{k})=*{arg\,max}_{_{2}}^{}(_{_{k}},f_{_{2}})\) as the inner-loop optimized functions and use \((_{k}):=(_{_{k}},f_{_{1}(_{k})},f_{_{2}(_{k})})\) (def. in Eq. (9)) for simplicity in this section. We start with the following standard assumption .

**Assumption 1**.: _Suppose that the optimized \(Q\)-value functions and the parameterized policy \(_{}\) satisfy the following conditions:_

1. _The uniformly boundness of off/on-policy_ \(Q\)_-value functions optimized by Algorithm_ 1_, i.e.,_ \(\|f_{_{1}}\|_{},\|f_{_{2}}\|_{} 1\)_._
2. _The_ \(B\)_-Lipschitzness and the_ \(L\)_-smoothness of the parameterized policy, i.e., for any state-action pair_ \((_{ t},y_{t+1})\) _at any time step_ \(t\{0,1,,T-1\}\)_,_ \[\|_{}(y_{t+1}|_{ t})\| B,,\] (11) \[\|_{_{1}}(y_{t+1}|_{ t})- _{_{2}}(y_{t+1}|_{ t})\| L\|_{1}-_{2}\|, _{1},_{2}\] (12)

**Theorem 2** (**Convergence rate of Algorithm 1 to stationary points)**.: _Let \(\{_{k}\}_{1 k K}\) be the sequence of parameters of the policy \(_{_{k}}\) given by Algorithm 1. Let the learning rate \(=}}}{(1-)KL_{}}}\). Under Assumption 1, we have_

\[_{0 k K-1}[\|(_{k})\|^{2} ](})\] (13)

Proof.: See Appendix A.6 for the complete derivation. 

Theorem 2 illustrates that the output gradient norm square by Algorithm 1 can converge to a neighborhood around zero with the rate of \(1/\). Furthermore, leveraging a sufficient number of training iterations \((^{-2})\), Algorithm 1 can obtain an \(\)-accurate stationary point. This leads to the following corollary on the computational complexity of the training procedure.

**Corollary 1** (**Computational complexity of Algorithm 1)**.: _We formalize the policy as a softmax function \(_{}\) with a linear transformation: \(*{softmax}(_{ t})\) for any \(_{ t}^{H}\), where \(^{|| H}\) and \(H\) denotes the hidden size. Then, to obtain an \(\)-accurate stationary point by Algorithm 1, the complexity of gradient computation is \((^{-2}T||H(N+T+||))\)._

Proof.: See Appendix A.7 for the complete derivation. 

Corollary 1 shows that Algorithm 1 has a polynomial computational complexity w.r.t \(^{-2}\), \(N\), \(||\), \(H\) and \(T\), to obtain an \(\)-accurate stationary point for optimizing the training objective in Eq. (9).

## 4 Experiments

We consider task-agnostic instruction-following experiments and task-specific experiments, including text summarization, machine translation, and commonsense reasoning. We compare our approach with various KD baselines, including: SFT, which fine-tunes the student model on the supervised dataset \(_{}\); KD , which uses KL divergence on the supervised dataset \(_{}\); SeqKD , which applies SFT to the student model with teacher-generated outputs; ImitKD , which uses KL divergence on the student-generated outputs; MiniLLM , which uses RKL divergence with a policy gradient method; GKD , which uses JS divergence with an on-policy method; and DistilLM , which uses an adaptive training method for off-policy optimization of a skew KL divergence. Additionally, we focus on step-wise distance optimization for KD and compare it with a range of well-known methods, including KL divergence, RKL divergence, JS divergence, and TV distance, as discussed by Wen et al. . All the reported results are the average across three random seeds.

### Task-Agnostic Distillation

**Experimental Setup.** We follow the previous works [14; 21] for the implementation of the instruction-following experiment, aiming to evaluate the distilled model's ability to handle diverse tasks presented in the form of instructions. We construct the training data from databricks-dolly-15k , where randomly select 15K samples for training and equally split 500 samples for validation and testing. We evaluate the trained model on five instruction-following datasets: DollyEval, SelfInst , VicunaEval , S-NI , and UnNI . Following the previous works [14; 21], we also add the OpenWebText  corpus, consisting of long-document plain text, for joint training with a language modeling task. This has been shown to effectively improve the performance of instruction tuning . The evaluation metrics include ROUGE-L  and GPT-4 feedback with the same prompts as in . More details on experimental setup refer to Appendix B.

**Main results.** Table 1 illustrates the instruction-following performances. Compared with the SFT baseline, which indicates the student model without KD, KD and SeqKD hardly improve the performances. This indicates that using only supervised datasets or teacher-generated outputs does not benefit the KD of large language models. In contrast, utilizing the student-generated outputs with KL divergence , RKL divergence , and JS divergence  shows effectiveness for KD in the instruction-following task. State-of-the-art methods [14; 2; 21] tend to combine the student-generated outputs with the teacher-generated output or supervised dataset to further improve the results of KD. This shows that a mixture optimization of both on-policy and off-policy objectives can effectively improve the KD performance of large language models on the instruction-following task. In particular, we use an adversarial moment-matching method and optimize both on-policy and off-policy objectives for KD, thus achieving the best results on five test datasets with both GPT-4 feedback and ROUGE-L evaluations.

### Task-Specific Distillation

**Experimental Setup.** We evaluated the KD models on three tasks consisting of text summarization, machine translation, and reasoning. For the text summarization task, we follow Ko et al.  to conduct experiments on the SAMSum  dataset. For the machine translation tasks, we follow Ko et al.  to conduct experiments on the IWSLT'17 (en-de)  dataset. For the commonsense reasoning task, we conduct experiments on the StrategyQA dataset  with chain-of-thought augmentations

    &  &  &  &  &  \\   & GPT-4 & R-L & GPT-4 & R-L & GPT-4 & R-L & R-L & R-L \\  _OpenLLLaMA2-7B (teacher)_ & _58.8\({}_{ 1.2}\)_ & _32.5\({}_{ 0.4}\)_ & _56.7\({}_{ 0.8}\)_ & _21.6\({}_{ 0.2}\)_ & _46.2\({}_{ 0.6}\)_ & _22.6\({}_{ 0.5}\)_ & _36.3\({}_{ 0.5}\)_ & _38.5\({}_{ 0.2}\)_ \\  SFT (_student_) & 46.8\({}_{ 0.7}\) & 26.7\({}_{ 0.6}\) & 40.8\({}_{ 1.1}\) & 16.3\({}_{ 0.7}\) & 34.8\({}_{ 0.8}\) & 17.3\({}_{ 0.2}\) & 30.4\({}_{ 0.4}\) & 28.6\({}_{ 0.3}\) \\ KD  & 43.9\({}_{ 0.8}\) & 22.2\({}_{ 0.4}\) & 43.5\({}_{ 0.5}\) & 17.4\({}_{ 0.5}\) & 33.7\({}_{ 0.3}\) & 16.4\({}_{ 0.2}\) & 29.3\({}_{ 0.6}\) & 23.4\({}_{ 0.3}\) \\ SeqKD  & 50.2\({}_{ 0.6}\) & 26.2\({}_{ 0.4}\) & 46.8\({}_{ 0.3}\) & 15.5\({}_{ 0.5}\) & 38.8\({}_{ 1.2}\) & 18.0\({}_{ 0.6}\) & 29.7\({}_{ 0.3}\) & 27.8\({}_{ 0.1}\) \\ ImitKD  & 53.7\({}_{ 1.6}\) & 25.3\({}_{ 0.3}\) & 45.0\({}_{ 0.7}\) & 18.4\({}_{ 0.4}\) & 41.7\({}_{ 1.2}\) & 19.1\({}_{ 0.2}\) & 33.1\({}_{ 0.7}\) & 28.7\({}_{ 0.5}\) \\ MiniLLM  & 58.7\({}_{ 1.2}\) & 28.4\({}_{ 0.3}\) & 51.8\({}_{ 1.5}\) & 20.2\({}_{ 0.6}\) & 44.2\({}_{ 1.1}\) & 20.7\({}_{ 0.5}\) & 37.4\({}_{ 0.4}\) & 37.5\({}_{ 0.2}\) \\ GKD  & 57.6\({}_{ 1.0}\) & 27.5\({}_{ 0.3}\) & 52.4\({}_{ 1.2}\) & 20.9\({}_{ 0.3}\) & 45.5\({}_{ 0.8}\) & 19.3\({}_{ 0.5}\) & 36.8\({}_{ 0.6}\) & 34.8\({}_{ 0.3}\) \\ DistilLM  & 59.2\({}_{ 0.2}\) & 53.4\({}_{ 1.0}\) & 20.8\({}_{ 0.7}\) & 46.3\({}_{ 0.9}\) & 20.4\({}_{ 0.3}\) & 37.2\({}_{ 0.1}\) & 38.2\({}_{ 0.1}\) \\
**Ours** & **59.8\({}_{ 0.8}\)** & **30.7\({}_{ 0.4}\)** & **54.2\({}_{ 1.2}\)** & **21.7\({}_{ 0.5}\)** & **47.8\({}_{ 0.7}\)** & **21.4\({}_{ 0.4}\)** & **38.7\({}_{ 0.4}\)** & **39.1\({}_{ 0.3}\)** \\   

Table 1: Comparison with state-of-the-art KD methods on the instruction-following dataset using fine-tuned OpenLLaMA-7B as the teacher and fine-tuned OpenLLaMA-3B as the student. We format **the best**, the second best and worse than SFT results. The results based on GPT-2 are available in Appendix C.1.

. For all of the task-specific experiments, we use T5-XL  as the teacher model and T5-Large/Base/-Small as the student model. For the machine translation experiments, we employ a multilingual pretrained model, mT5 , to build the methods. For evaluation, we use ROUGE-L , BLEU , and accuracy as the performance metrics on SAMSum, IWSLT'17 (en-de), and StrategyQA, respectively. More details about the experimental setup refer to Appendix B.

**Main results.** Table 2 displays the performances on three task-specific datasets. Since the original work of MiniLLM  does not consider these tasks, we thus do not make comparisons with MiniLLM. The performance trend is similar to the instruct-following results, revealing that KD of large language models for specific tasks also benefits from the combination of on-policy objectives with student-generated outputs and off-policy objectives with teacher-generated outputs or supervised datasets. Additionally, we observe that student models of different sizes all benefit from the KD methods to improve performance. Overall, our approach achieves the best results on all three task-specific datasets for student models of different sizes. This demonstrates the effectiveness of an adversarial moment-matching approach for KD of large language models on specific tasks.

### Analysis on Step-Wise Distance Optimization

**Comparison with distribution matching.** We make comparisons with different step-wise distribution distances with a uniform formulation of Definition 2, considering the on-policy, off-policy objectives as well as the joint form. Results on four tasks with a default combination factor \(=0.5\) are shown in Figure 2. More instruct-following results are available in Appendix C.2 and results with different values of off-/on-policy combination factor are available in Appendix C.5. Compared with the KL divergence, RKL divergence, JS divergence and total variation distance, the proposed moment-matching distance achieves the best results under both the on-policy and off-policy training objectives, which shows that the proposed moment-matching approach is effective for KD of large language models. Besides, we observe that using a joint objective of both on-policy and off-policy can further significantly improve the performances. This shows that both on-policy and off-policy moment-matching objectives contribute to the minimization of the imitation gap and can thus benefit the KD of large language models.

   &  &  &  \\   & TS-Small & TS-Base & T5-Large & TS-Small & T5-Base & TS-Large & TS-Small & TS-Base & TS-Large \\  _TS-XL (teacher)_ &  &  &  \\  SFT (_student_) & 40.6\(\)0.2 & 47.3\(\)0.3 & 49.8\(\)0.2 & 21.5\(\)0.1 & 30.1\(\)0.0 & 33.7\(\)0.1 & 52.4\(\)0.5 & 57.5\(\)0.8 & 60.7\(\)0.8 \\ KD  & 39.2\(\)0.4 & 46.5\(\)0.3 & 47.4\(\)0.3 & 21.7\(\)0.1 & 29.8\(\)0.2 & 31.7\(\)0.1 & 49.7\(\)0.3 & 55.3\(\)0.1 & 59.2\(\)0.5 \\ SeqKD  & 39.7\(\)0.3 & 47.7\(\)0.5 & 49.3\(\)0.4 & 21.2\(\)0.3 & 29.2\(\)0.2 & 32.9\(\)0.5 & 50.6\(\)0.7 & 57.5\(\)1.1 & 61.5\(\)0.8 \\ ImkitD  & 41.8\(\)0.3 & 48.6\(\)0.7 & 51.2\(\)0.5 & 22.2\(\)0.3 & 28.7\(\)0.6 & 34.1\(\)0.2 & 53.8\(\)0.8 & 59.7\(\)0.6 & 61.7\(\)0.6 \\ GRD  & 42.1\(\)0.3 & 48.2\(\)0.5 & 51.7\(\)0.4 & 22.7\(\)0.2 & 31.2\(\)0.1 & 34.7\(\)0.2 & 55.6\(\)0.4 & 60.3\(\)0.5 & 63.6\(\)0.3 \\ DistilLM  & 42.6\(\)0.2 & 49.4\(\)0.6 & 52.1\(\)0.4 & 22.5\(\)0.1 & 30.8\(\)0.2 & 35.5\(\)0.1 & 56.3\(\)0.3 & 61.2\(\)0.7 & 62.8\(\)0.2 \\
**Ours** & **43.7\(\)0.4** & **50.4\(\)0.3** & **52.7\(\)0.3** & **23.7\(\)0.1** & **32.4\(\)0.3** & **36.0\(\)0.2** & **58.2\(\)0.4** & **62.9\(\)0.3** & **65.3\(\)0.7 \\  

Table 2: Comparison with the state-of-the-art KD methods on text summarization, machine translation and commonsense reasoning datasets. We report the ROUGE-L, BLEU and accuracy for SAMSum, IWSLTâ€™17 (en-de) and StrategyQA, respectively. We format **the best**, the second best and worse than SFT results.

Figure 2: Performance of difference step-wise distribution distances.

**Adversarial training procedure.** We present the training loss and moment-matching distance against the adversarial training steps. As depicted in Figure 3 (a), the training loss initially increases within the first 0-1,000 steps, indicating that initially, the \(Q\)-value functions are stronger than the policy in maximizing the loss function \((_{},f_{_{1}},f_{_{2}})\) in Eq. (9). Concurrently, the policy gradient method contributes to minimizing the training loss, which eventually converges to a much lower stable value. Additionally, both the on-policy and off-policy moment-matching distances \(d^{}_{}\) and \(d^{}_{}\) decrease and eventually reach a low value with only minor fluctuations. For more results and details on experimental setups, please refer to Appendix C.3.

**Moment-matching distance optimization.** We further illustrate the on-policy moment-matching distance \(d^{}_{}\) and the off-policy moment-matching distance \(d^{}_{}\) (defined in Definition 3) optimized by different step-wise distances in Figure 3 (b) and (c), respectively. Interestingly, we observe that the total variation (TV) distance obtains the second-best results on average for both on-policy and off-policy distances. This finding suggests a similarity between the formulations of TV distance and moment-matching distances to some extent, as supported by the theoretical result of Theorem 1. Across all instruction-following test sets, our approach effectively optimizes both on-policy and off-policy moment-matching distances more than other step-wise distribution distances used in KD, including KL divergence, RKL divergence, JS divergence, and TV distance. This observation also underscores the effectiveness of our policy gradient methods. Extensive results on the task-specific datasets are available in Appendix C.4.

## 5 Conclusion

In this work, we investigated a moment-matching approach for knowledge distillation of large language models. Specifically, we formulated knowledge distillation from a perspective of imitation learning and derived both on-policy and off-policy bounds for the imitation gap between the teacher model and student model via moment-matching distance. Additionally, we proposed an adversarial training algorithm to simultaneously estimate and minimize the joint objective of on-policy and off-policy moment-matching distances. In experiments, we evaluated the proposed algorithm on four instruction-following datasets and three task-specific datasets, comparing it with a range of state-of-the-art KD methods as well as four well-studied step-wise distribution distances for KD of auto-regressive models. Results demonstrate that our approach can effectively leverage the policy gradient method to optimize the moment-matching distance and achieve the best results across all datasets.

**Limitations and future work.** The proposed adversarial training algorithm requires additional computational steps for the inner-loop gradient ascent, which may result in increased time complexity. Moreover, the proposed approach necessitates auxiliary networks to build the \(Q\)-value functions, which may incur additional memory costs. Besides, the experiments are conducted with limited LLM architectures, such as OpenLLaMA and T5. Therefore, in future work, we aim to enhance the time and memory efficiency of our approach, and evaluate the proposed approach on a wider range of architectures.

Figure 3: Adversarial training procedure for optimizing the on-policy and off-policy moment-matching distances \(d^{}_{}\), \(d^{}_{}\) on the instruction-following dataset.