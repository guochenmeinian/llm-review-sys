# Large Spatial Model:

End-to-end Unposed Images to Semantic 3D

 Zhiwen Fan\({}^{1,2}\), Jian Zhang\({}^{3*}\), Wenyan Cong\({}^{1}\), Peihao Wang\({}^{1}\), Renjie Li\({}^{4}\), Kairun Wen\({}^{3}\), Shijie Zhou\({}^{5}\), Achuta Kadambi\({}^{5}\), Zhangyang Wang\({}^{1}\), Danfei Xu\({}^{2,6}\), Boris Ivanovic\({}^{2}\), Marco Pavone\({}^{2,7}\), Yue Wang\({}^{2,8}\)

\({}^{1}\)UT Austin \({}^{2}\)NVIDIA Research \({}^{3}\)XMU

\({}^{4}\)TAMU \({}^{5}\)UCLA \({}^{6}\)GaTech \({}^{7}\)Stanford University \({}^{8}\)USC

Z. Fan is the Project Lead

Project Website: https://largesspatialmodel.github.io

###### Abstract

Reconstructing and understanding 3D structures from a limited number of images is a classical problem in computer vision. Traditional approaches typically decompose this task into multiple subtasks, involving several stages of complex mappings between different data representations. For example, dense reconstruction using Structure-from-Motion (SfM) requires transforming images into key points, optimizing camera parameters, and estimating structures. Following this, accurate sparse reconstructions are necessary for further dense modeling, which is then input into task-specific neural networks. This multi-stage paradigm leads to significant processing times and engineering complexity.

In this work, we introduce the _Large Spatial Model_ (**LSM**), which directly processes unposed RGB images into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass and can synthesize versatile label maps by interacting through language at novel views. Built on a general Transformer-based framework, LSM predicts global geometry via pixel-aligned point maps. To improve spatial attribute regression, we adopt local context aggregation with multi-scale fusion, enhancing the accuracy of fine local details. To address the scarcity of labeled 3D semantic data and enable natural language-driven scene manipulation, we incorporate a pre-trained 2D language-based segmentation model into a 3D-consistent semantic feature field. An efficient decoder parameterizes a set of semantic anisotropic Gaussians, allowing supervised end-to-end learning. Comprehensive experiments on various tasks demonstrate that LSM unifies multiple 3D vision tasks directly from unposed images, achieving real-time semantic 3D reconstruction for the first time.

Figure 1: **Large Spatial Model** takes two unposed images as input and reconstructs an explicit radiance field, capturing geometry, appearance, and semantics in real time. This yields high performance in versatile tasks such as view synthesis, depth prediction, and open-vocabulary 3D segmentation.

Introduction

The computer vision community has devoted considerable effort to recovering and understanding 3D information (e.g., depth and semantics) from 2D sensory data (e.g., images). This process aims to derive 3D representations that encapsulate both geometric and semantic details from cheap and widely available 2D data, facilitating further interaction, reasoning, and planning within 3D physical world. Traditional approaches  tackle this by pipelining several distinct tasks: detecting, matching, and triangulating points for initial sparse reconstructions and the subsequent dense reconstruction, followed by the integration of specialized submodules for semantic 3D modeling.

Recent developments in this domain have markedly proceeded with a more powerful representation using both sparse reconstruction, and subsequent dense 3D modeling via Multi-View Stereo (MVS) [2; 3], Neural Radiance Field (NeRF) , and 3D Gaussian Splatting (3D-GS) , This trend influenced various industries, including autonomous driving , robotics , digital twins , and virtual/augmented reality (VR/AR) [9; 10]. Due to the complexity of inferring 3D information from 2D images, previous methods have broken down the holistic task into distinct, manageable subproblems. However, this strategy propagates errors from stage to stage and downgrades the performance of subsequent tasks. For instance, the critical step of precomputing camera poses -utilizing Structure from Motion (SfM) -- has proven to be vulnerable and often fails in scenes covered by a sparse number of views or exhibiting low-textured surfaces . Such inaccuracies in camera pose estimation can ultimately lead to imprecise interpretation of the 3D scene.

Furthermore, reasoning about and interacting with the environment would benefit from a comprehensive 3D understanding. Open-vocabulary methods, which perform semantic segmentation without relying on a fixed set of labels, provide notable flexibility. However, unlike single-image understanding, the absence of large-scale and diverse 3D scene data with accurate multiview language annotations complicates the challenge. Efforts have been made to integrate 2D features into frameworks such as NeRF [12; 13; 14] and 3D-GS [15; 16]. Yet, these methods, such as Feature-3DGS , typically require overfitting each 3D scene separately with extensive captured viewpoints and preprocessing camera poses using Structure-from-Motion.

To address the challenges outlined above, we propose for the first time a novel **unified framework for these key 3D vision subproblems**: _dense 3D reconstruction_, _open-vocabulary semantic segmentation_, and _novel view synthesis_ from unposed and uncalibrated images. Our approach leverages a single Transformer-based model that learns the attributes of a 3D scene via _semantic anisotropic Gaussians_. Unlike previous methods that rely on epipolar Transformers with known camera parameters [17; 18; 19] or require extensive per-scene fitting [5; 15], we employ a coarse-to-fine strategy. This strategy predicts dense 3D geometry using pixel-aligned point maps, progressively refining these points into anisotropic Gaussians in a single feed-forward pass.

Our framework, dubbed _Large Spatial Model_ (LSM), begins with a general Transformer architecture incorporating cross-view attention , which constructs pixel-aligned point maps at a normalized scale, enabling generalization across various datasets. LSM further enhances point-based representations through multi-scale fusion and local context aggregation using a ViT encoder. Additionally, LSM performs hierarchical cross-modal fusion, integrating features from a pre-trained 2D semantic model into a consistent 3D feature field. Through differentiable splitting of the regressed semantic anisotropic Gaussians, LSM enables end-to-end supervision and supports real-time scene-level 3D semantic reconstruction and rendering without needing explicit camera parameters. This allows for efficient, data-driven rendering of labels from novel viewpoints, as demonstrated in Figure 1.

Our contributions are summarized as follows:

* We introduce a unified 3D representation and an end-to-end framework that addresses dense 3D reconstruction, 3D language-based segmentation, and novel-view synthesis directly from unposed images in a single forward pass.
* Our method leverages a Transformer architecture with cross-view attention for multi-view geometry prediction, combined with hierarchical cross-modal attention to propagate geometry-rich features. We further integrate a pre-trained semantic segmentation model to enhance 3D understanding. By aggregating local context at the point level, we achieve fine-grained feature integration, enabling the prediction of anisotropic 3D Gaussians and efficient splatting for RGB, depth, and semantics.

* Our model performs multiple tasks simultaneously with real-time reconstruction and rendering on a single GPU. Experiments show that our unified approach scales effectively across different 3D vision tasks, surpassing many state-of-the-art baselines without the need for additional SfM steps.

## 2 Related Work

SfM and Differentiable Neural RepresentationStructure-from-Motion (SfM) aims to jointly estimate camera poses and reconstruct sparse 3D structures from multiple views. Traditional pipelines  involve multiple stages, including descriptor extraction, correspondence estimation, and incremental bundle adjustment. Recent advances in learning-based techniques [21; 22; 23; 24; 25] have further improved the accuracy and efficiency of SfM. These methods are widely adopted in 3D vision tasks, where differentiable neural representations typically assume accurate camera poses provided by SfM. For instance, NeRF  and its successors  rely on poses estimated offline via COLMAP [1; 27]. Similarly, 3D Gaussian Splatting  uses SfM-generated 3D points for initialization and has been applied to robotics [28; 29; 30], healthcare [31; 32; 33], and many other domains [34; 35; 36]. Beyond novel view synthesis, lifting 2D features to 3D has gained traction in various editing tasks [13; 15; 37; 14].

End-to-End Image-to-3D3D reconstruction is a long-standing problem in computer vision, with traditional approaches like SfM [38; 39; 1], Multi-view Stereo (MVS) [3; 2; 40; 41], and Signed Distance Function (SDF) [42; 43]. More recent techniques utilize neural representations, including implicit  and explicit  formats to generate 3D models. Semantic understanding is often integrated during the reconstruction process , or through additional optimization steps [12; 13]. However, most methods depend on a preprocessing step like SfM  to estimate camera calibration, poses, and sparse point clouds before dense reconstruction, either through feed-forward prediction or test-time optimization. This reliance on calibration and pose estimation limits scalability with large-scale data, contrasting the success seen with large foundation models . The latest pose-free, feedforward approaches, such as Scene Representation Transformers[46; 47; 48], have advanced the concept of representing multiple images as a "set latent scene representation," allowing for novel view generation even in the presence of inaccurate camera poses or without any pose information. However, these methods struggle to produce explicit geometry. DUSt3R addresses this limitation by predicting dense point clouds directly from unposed stereo image pairs, enabling pixel-aligned geometry prediction at normalized scales. Practically, dense point prediction requires accurate multi-view RGB-D pairs, which significantly limits its scalability. InstantSplat addresses this by utilizing novel-view synthesis with only posed image data and employing Gaussian Bundle Adjustment to jointly optimize camera and scene parameters for ultra-fast dense 3D reconstruction.

In contrast, our framework offers a holistic solution for **dense 3D semantic reconstruction from unposed images**. It integrates dense 3D geometry reconstruction, and language-based 3D interaction, while minimizing the need for extensive data annotation by using novel view synthesis as a core task. Since dense 3D annotations are often scarce in real-world scenarios, we propose semantic anisotropic Gaussians to lift 2D features map to 3D semantic embeddings without additional annotation. Our approach addresses higher-level 3D tasks in perception and dense 3D reconstruction compared to DUSt3R , by utilizing lightweight annotations and solving these tasks jointly within a unified framework.

## 3 Methods

OverviewFigure 2 illustrates the architecture for training the Large Spatial Model (LSM). During training, the input consists of stereo image pairs along with associated camera intrinsics and poses: \(\{(_{i}^{H W 3}),(_{i} ^{3 4}),(_{i}^{3 3})\}_{i=1}^{2}\). At inference, however, unposed images can be directly fed into the framework. The pixel-aligned geometry is predicted using a standard Transformer architecture  with cross-attention between input views. Dense prediction heads are employed to regress normalized point maps during training: \(\{_{i}^{H W 3}\}_{i=1}^{2}\) (see Sec. 3.1).

To support fine-grained semantic anisotropic 3D Gaussian regression, which represents the 3D scene and lifts generic feature fields from pre-trained 2D vision models, we apply point-based attention with learnable positional encoding in a local window. This propagates features from neighboring points (Sec. 3.2), effectively merging encoded features with rich semantics (Sec. 3.2) at multiple scales using 2D pre-trained models (Sec. 3.3). New views from the semantic radiance fields can be decoded using splitting  on the target poses (Sec. 3.4). During inference, semantic anisotropic Gaussians are directly predicted, and the renderer takes the camera parameters derived from the point maps. An overview of the model architecture is shown in Figure 2.

### Dense Geometry Prediction

Instead of adopting a conventional Transformer with Epipolar attention--which can be inefficient as pixel-wise prediction requires hundreds of queries on sampled epipolar lines [17; 18]--we implement an encoder-decoder structure for directly regressing view-specific point maps at normalized scales. Cross-view attention is utilized to aggregate multi-view information efficiently.

Direct Regression of Normalized Depth MapWe employ a Siamese ViT-based encoder  that processes stereo images using shared weights. It involves the patchification and tokenization of images, followed by the integration of sinusoidal positional embeddings. To directly regress the pixel-aligned point maps from the unposed images for view \(v\{1,2\}\), cross-view attention is also employed, enhancing the architecture's capacity to infer spatial relationships and propagate information between views--an approach that has proven effective in prior research [51; 20; 52]. The decoder block consists of interleaved self-attention for each view and cross-attention across views, which integrates tokens from both images. The inter-view decoder includes 12 attention blocks, akin to those utilized in previous multi-view stereo (MVS) studies [52; 20]. These blocks generate tokenized features for a subsequent Dense Prediction Transformer head (DPT) , which estimates a pixel-wise point map in a normalized coordinate system along with confidence value:

\[_{}=_{v\{1,2\}}_{i^{v}}_{v,1}^{i}_{}(v,i)-_{v,1}^{i},\] (1)

where \(\) is pixel-aligned confidence map, same as DUSt3R, \(\) indicates all valid points to the origin, \(_{v,1}\) denotes the confidence map obtained from view \(v\), expressed in the coordinate frame of view 1, \(\) is a hyper-parameter that apply regularization, encouraging the network to perform robustly in challenging areas. The depth error is calculated by

\[_{}=_{v\{1,2\}}\|_{v,1}-}}_{v,1}\|,\] (2)

where the normalization factors (z and \(\)) indicate that the predicted and ground-truth pointmaps are processed by normalizing. For example, z is obtained by: \((_{1},_{2})=^{1}|+|^{2}|}_{v\{1,2\}}_{i^{v}}\|_{v}^{i} \|.\)

Figure 2: **Network Architecture. Our method utilizes input images from which pixel-aligned point maps are regressed using a generic Transformer. A set of semantic antitrosopic 3D Gaussians incorporating geometry, appearance, and semantics are then predicted employing another point-based Transformer that facilitates local context aggregation and hierarchical fusion. It is supervised end-to-end, minimizing the loss function through comparisons against ground truth and rasterized label maps on new views. During the inference stage, our approach is capable of predicting the scene representation without requiring camera parameters, enabling real-time semantic 3D reconstruction.**

### Point-wise Feature Aggregation

Building on the foundational work in NeRF and Multi-view Stereo, which employ a coarse-to-fine strategy for high-quality radiance field and depth estimation, we also aggregate the initial predicted geometry by applying a Transformer  at the point level, leveraging hierarchical representations to achieve more refined, point-based regression.

Point-wise Attribute PredictionRather than relying solely on a single network to represent the scene, we employ two Transformer-based networks optimized for distinct tasks: one for capturing "coarse" global geometry and another for "fine" local information aggregation. Initially, we integrate stereo point maps, including color information for each point primitive, formulated as \(\{_{i}=(x_{i},y_{i},z_{i},r_{i},g_{i},b_{i})\}_{i=1}^{N}\) to serve as input. Unlike tokenized image patches, point primitives carry distinct geometric significance within Euclidean space. Inspired by recent advancements in point-cloud processing [55; 56; 57], we employ a Transformer within a localized window to perform point-wise aggregation, selectively emphasizing key features from neighboring primitives. Point-wise encoding and decoding are essential for refining scene representation, utilizing multiscale aggregation across five hierarchical levels.

After aggregating the point-wise features, we employ an additional layer of multilayer perceptron (MLP) to regress the parameters, representing the 3D scene through a set of anisotropic Gaussians . The parameters include the opacity \(\), scale factor \(\), rotation \(\), and Spherical Harmonics coefficients \(\{_{i}^{3}|i=1,2,...,k\}\) where \(k=(K+1)^{2}\) is the number of coefficients of SH with degree \(K\). The Gaussian centers \(\) are regressed from geometry prediction backbone. The color \(\) of direction \(\) is then computed by summing up all SH basis as \(()=_{i=1}^{n}_{i}_{i}()\), where \(_{i}\) is the \(i^{}\) SH basis. The final pixel intensity \(\) is calculated by blending \(n\) ordered Gaussians overlapping the pixels using the following render function:

\[=_{i=1}^{n}_{i}_{i}_{j=1}^{i-1}(1-_{j})\] (3)

This equation efficiently models the contributions of each Gaussian to the pixel's final appearance, accounting for their transparency and layering order.

Cross-model Feature AggregationTo effectively combine multi-view image features with point-wise geometric information, we implement cross-model attention between two sets of tokens. The attention block fuses tokens from different sources by first applying self-attention to the input \(\), allowing each token to attend to other tokens within the same sequence. This process helps capture internal relationships and enrich the representation of the input token. Next, cross-attention is used, where two sets of tokens (\(\) and \(\)) from the latent layers of two different models are fused, enabling the integration of external information into \(\). Finally, a feed-forward network (MLP) further processes the updated information following cross-model fusion.

The original point features \(\) contain explicit and precise spatial information, which is critical for accurate geometry reconstruction. In contrast, the image token features \(\), from image encoder (Sec. 3.1) are rich in semantic content, providing important contextual information that enhances general understanding of the scene. Cross-model fusion enables the integration of detailed spatial geometry with semantic richness:

\[ =(),,=( ),\] \[ =(^{}}{}})\]

where \(\) and \(\) were normalized with a linear layer before projection.

### Learning Hierarchical Semantics

To facilitate semantic 3D representation, we augment the anisotropic 3D Gaussians with a learnable semantic feature embedding (a.k.a. semantic anisotropic Gaussians) and rasterize into the 2D image plane by blending Gaussians that overlap with each pixel using a feature rendering function.

\[=_{i=1}^{n}_{i}_{i}_{j=1}^{i-1}(1-_{j})\] (4)\(\) indicates the final rasterized feature embedding on image plane, and \(_{i}\) is semantic embedding on anisotropic Gaussians.

3D Semantic Field from 2D ImagesAfter obtaining \(\), we optimize \(_{i}\) by minimizing the difference between the rasterized feature map and the feature maps generated by a pre-trained 2D model. Unlike the previous method  which requires test time optimization, we transform the estimation of the feature field into a fully learnable process.

Feature maps (\(\{_{i}^{H^{} W^{} N}\}_{i=1}^{2}\)) from a pre-trained 2D multi-modal model  are inherently view-inconsistent due to the lack of spatial awareness during the model's training. To elevate multi-view feature embeddings into a coherent 3D feature field for holistic 3D understanding, we introduce a dynamic fusion strategy employing an attention-based correlation module. This module is specifically designed to learn blending weights for each token within Point Transformer  from the input pixel-wise feature embedding (\(_{i}\)). We employ attention blocks as described in Eq.3.2 to synchronize in the latent spaces through a supplementary set of cross-attention layers. The visual feature from LSeg, denoted as \(}\), is utilized for this purpose. This loss function is minimized during training by utilizing rasterized feature maps on new views \(\) and directly inferred feature maps using ground truth images on new views \(}\) (LSeg ), thereby facilitating the learning of blending weights for consistent semantic field regression.

\[_{}=1-(},)=1-}}{||}||||}\] (5)

Multi-scale Feature FusionTo improve model efficiency, we propagate information from ViT Encoder feature \(\) and the frozen semantic feature \(\), to the 3D latent space (point feature \(\)) which has fewer tokens, thereby enabling selective attention to critical features. We further refine feature fusion across multiple stages, optimizing information flow while minimizing additional computational overhead. Novel view synthesis serves as an effective task to encode the complete geometric and appearance features into a low-dimensional 3D latent space, while recovering a set of semantic anisotropic Gaussians (\(\{_{i}^{1 C}\}_{i=1}^{N}\)) through learning from large-scale data and end-to-end training.

### Training Objective

Putting all together, our model can be optimized end-to-end:

\[ =(,)- }\|+_{1}((,),})}_{}\] (6) \[+_{2}_{}((,),})}_{}+_{v\{1,2\}} _{3}_{}(_{v,1},}_{v,1})}_{}\] (7)

where \(\) and \(}\) are rasterized and GT pixel intensities, \(\) denotes represented 3D scene using a set of 3D semantic anisotropic Gaussians, \(\) and \(}\) denotes rendered LSeg feature extractor and feature on the target image, \(\) indicates the direction and position at new views. In our methodology, we leverage both photometric loss and semantic loss to supervise the generation of rasterized new views. In order for geometry prediction and semantic feature lifting, we employ a confidence-weighted depth loss applied to the input views. The parameters \(_{1}\), \(_{2}\), \(_{3}\) are set to 0.25, 0.3, and 1.5, respectively, as determined by the grid search.

## 4 Experiments

### Implementation Details

For our architecture, we employ ViT-Large as the encoder and ViT-Base as the decoder, complemented by a DPT head  for pixel-wise geometry regression. We initialize the geometry prediction layers using DUST3R . Point Transformer layers consists of 5 encoder and 4 decoder blocks with progressive downsampling and upsampling. The cross-model fusion strategy is implemented at the output of the last encoder and the output of the first decoder. The entire system is optimizedend-to-end using the loss function described in Eq. 6. The training of our model contains 100 epochs, leveraging a combined dataset of ScanNet++ and Scannet, of 1565 scenes. Training is on 8 Nvidia A100 GPU lasts for 3 days. We start with a base learning rate of 1e-4 and incorporate a 10-epoch warm-up period. AdamW is employed as the optimizer for all experiments. Evaluation is conducted on 40 unseen scenes from ScanNet. Additionally, we assess on tasks: novel view synthesis, multi-view depth prediction, and 3D language-based semantic segmentation.

### Semantic 3D Reconstruction

Evaluation of Synthesized Images QualityNovel view synthesis is evaluated using NeRF-DFF  and Feature-3DGS , both of which are capable of predicting RGB values as well as features. In addition, we compared our approach with the state-of-the-art, generalizable, pose-based 3D Gaussian Splitting method, pixelSplat , which generates point-based representations through a feed-forward pass. Unlike our method, these existing approaches rely on known camera intrinsics and poses prior to evaluation. As indicated in Table 1, NeRF-DFF and Feature-3DGS tend to overfit on each individual scene, requiring significantly more time than our method, yet performing comparably in terms of output quality. pixelSplat utilizes an Epipolar Transformer, searching along the epipolar line using GT camera parameters to regress Gaussian attributes, resulting in longer inference times. Visualizations in Figure 4 demonstrate that our results are sharper and exhibit fewer artifacts than NeRF-DFF, and are comparable to Feature-3DGS and pixelSplat in performance.

Evaluation of Open-vocabulary Semantic 3D SegmentationThe semantic segmentation is evaluated by class-wise intersection over union (mIoU) and average pixel accuracy (mAcc) on novel views as metrics. Following the approach of Feature-3DGS , we map thousands of category labels from diverse datasets into a set of common categories, including {Wall, Floor, Ceiling, Chair, Table, Bed, Sofa, Others}. We compare our model against two state-of-the-art 3D baselines with the capacity for generating RGB, semantics and depth on any view: Feature-3DGS  and NeRF-DFF , which are based on 3D-GS  and NeRF , respectively. Additionally, the model LSeg , used as a 2D open-vocabulary segmenter for feature lifting, is included in our comparisons. We present statistics related to the semantic annotations on the adopted the ScanNet datasets in Table 1, where LSM demonstrates competitive performance compared to baseline 3D methods that require

  &  &  &  \\   & SIM & Per-Scene & mIoU \(\) & Acc.\(\) & rel \(\) & \(\) & mIoU \(\) & Acc.\(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  LSeg & N/A & N/A & 0.5278 & 0.7654 & - & - & 0.5281 & 0.7612 & - & - & - \\ NeRF-DFF & 20.52s & 1min2s & 0.4540 & 0.7173 & 27.68 & 9.61 & 0.4037 & 0.6755 & 19.86 & 0.6650 & 0.3629 \\ Feature-3DGS & 20.52s & 18min3s36s & 0.4453 & 0.7276 & 12.95 & 21.07 & 0.4223 & 0.7174 & 24.49 & 0.8132 & 0.2293 \\  pixelSplat & 20.52s & 0.064s & - & - & - & - & - & - & 24.89 & 0.8392 & 0.1641 \\  Ours & & 0.108s & 0.5034 & 0.7740 & 3.38 & 67.77 & 0.5078 & 0.7686 & 24.39 & 0.8072 & 0.2506 \\  &  &  &  &  &  &  &  &  &  &  \\ 

Table 1: **Quantitative Comparison in 3D Tasks.** We report novel-view synthesis, depth estimation quality, and open-vocabulary segmentation accuracy. Our method eliminates the need for any preprocessing in 3D tasks, while achieving performance comparable to other baselines that rely on SfM to obtain camera parameters and poses.

Figure 3: **Visualization of the 3D Feature Field.** We present examples of features rendered from novel viewpoints, illustrating how our method converts 2D features into a consistent 3D, facilitating versatile and efficient segmentation. Visualizations are generated using PCA .

ground-truth camera parameters and extensive per-scene optimization. The visualized results in Figure 5 illustrate that LSM can produce view-consistent semantic maps. In contrast, the 2D method LSeg yields detailed segmentation results but lacks cross-view consistency. To validate that LSM learns semantically meaningful features, we visualize the lifted feature field using PCA to reduce the high-dimensional features into three channels . As shown in Figure 3, LSM effectively generates a faithful semantic feature field through feed-forward inference using pair images.

Evaluation of Depth AccuracyWe also evaluate the performance of our model on the task of multi-view stereo depth estimation. We utilize the Absolute Relative Error (rel) and Inlier Ratio (\(\)) with a threshold of 1.03 to assess each scene, similar to DUSt3R . Since our approach does not rely on any camera parameters for prediction, we align the scene scale between the predictions and the ground truth. Specifically, we normalize the predicted point maps using the median of the predicted depths and similarly normalize the ground truth depths, following procedures established in previous literature  to align the two sets of depth maps. We observe in Table. 1 that LSM achieves state-of-the-art accuracy on ScanNet datasets than the per-scene wise methods. Our model is significantly faster than baseline methods, as it only require a forward-pass.

   Exp ID & Model & mIoU\(\) & Acc.\(\) & PSNR\(\) & SSIM\(\) \\ 
 & Baseline & 0.4562 & 0.6940 & 24.00 & 0.7981 \\
 &  + Fuse Encoder Feat. & 0.5410 & 0.8083 & 23.67 & 0.7876 \\
 &  + Fuse LSeg Feat. & 0.5586 & 0.8505 & 23.85 & 0.7902 \\
 &  +  +  + Multi-scale Fusion & **0.6042** & **0.8681** & **24.39** & **0.8072** \\   

Table 2: **Ablation Study on Our Design Choices.** We refer to the model that integrates cross-view attention for multi-view geometry with point-wise aggregation for future refinement as the baseline configuration (Exp #1). Implementing cross-modal attention to fuse geometry encoder features enhances both the rendering quality of new views and the segmentation accuracy (Exp #2). Additionally, incorporating features from frozen 2D semantic backbone into the fusion process (Exp #3) for consistent feature field amalgamation, and multi-scale fusion enhances hierarchical information flow (Exp #4), substantially improving language-based semantic 3D segmentation. Segmentation metrics use LSeg results as ground-truth in this table.

Figure 4: **Novel-View Synthesis (NVS) Comparisons**. We evaluate scene-level reconstruction by comparing our method to approaches that require per-scene optimization, such as NeRF-DFF and Feature-3DGS, which predicts both RGB and segmentation, and the generalizable 3D Gaussian Splatting method (pixelSplat). Notably, these methods require a pre-processing step to obtain camera poses using off-the-shelf SfM. Through end-to-end, data-driven training, our method achieves comparable visual quality to these approaches while reconstructing the 3D radiance field in a single feed-forward pass.

### Ablation Studies

We conduct ablations to validate our desing effectiveness. Experiments are on both language-based segmentation and novel view synthesis. The quantitative results can be views at Table 2.

Cross-Model Feature AggregationIncorporating the geometry encoder feature from ViT into the hidden layer of the point-aggregation layer (Sec. 3.2) demonstrates that such cross-model information flow significantly benefits the segmentation task, improving the mean Intersection over Union (mIoU) from 0.4562 to 0.5410 (Exp #1 \(\) 2).

Semantic Feature Fusion at Multi-ScaleEmploying cross-model fusion, where latent features of the semantic model are integrated into the middle layers of point-based aggregation, also improves injection of semantically rich embeddings (0.4562 to 0.5586, Exp # 1 \(\) 3). The decoded features confirm that the lifted feature field produces higher-quality feature maps, with the semantic mIoU improving from 0.5586 to 0.6042 (Exp #3 \(\) 4) through multi-scale fusion.

Module Timing.We analyze the computational cost of each module by running inference 1,000 times on the ScanNet test dataset with the model, as shown in Table 3, and calculating the average inference time for each module of the Large Spatial Model.

  
**Module** & **Inference Time (seconds)** \\  Dense Geometry Prediction (Sec. 3.1) & 0.029 \\ Point-wise Aggregation (Sec. 3.2) & 0.046 \\ Feature Lifting (Sec. 3.3) & 0.019 \\ 
**Total** & **0.096** \\   

Table 3: **Inference Time per Module. Breakdown of inference time for each module for analysis.**

Figure 5: **Language-based 3D Segmentation Comparison. We visualize the segmentation results across four unseen scenes and observe that our method performs comparably to NeRF-DFF and Feature-3DGS. This indicates that LSM effectively lifts 2D feature maps into high-quality 3D feature fields.**Evaluation of Generalizable Methods on New Datasets.To avoid potential overfitting, we adopt the Replica dataset, a photorealistic simulated 3D dataset with accurate RGB, dense depth maps, and semantic annotations for comprehensive evaluation. We use the same data preparation with Feature-3DGS. LSM generalizes well to the simulated Replica test set, achieving the best depth estimation metrics and enabling 3D semantic segmentation, which is unique among generalizable methods. Splatter Image, an ultra-fast monocular 3D object reconstruction method using 3D-GS, performs well for object reconstruction with masked backgrounds but struggles with scene-wise reconstruction in complex backgrounds.

## 5 Conclusion, Limitation, and Broader Impact

We have introduced the Large Spatial Model (LSM), a unified framework for holistic 3D semantic reconstruction from uncalibrated and unposed images, with the added capability of interaction through language. LSM leverages cross-view attention to aggregate multi-view cues and utilizes multi-scale cross-modal attention to integrate semantically rich features into a point-based representation. Hierarchical point-wise aggregation layers further refine these representations and enhance the integration of cross-modal attention. By splatting regressed anisotropic 3D Gaussians, LSM enables the generation of novel views with versatile label maps. LSM is highly efficient, capable of real-time end-to-end 3D modeling, and supports various downstream applications.

While our method significantly accelerates semantic 3D scene reconstruction, it relies on a pre-trained model for feature lifting, which can increase GPU memory requirements during training, especially when the integrated 2D model has a large number of parameters. Additionally, the need for ground-truth depth maps, although there are millions of multi-view datasets annotated with them, could limit its scalability for internet-scale video applications.

Our research enables efficient, real-time 3D scene-level reconstruction and understanding, which is advantageous for applications such as end-to-end robotic learning, AR/VR, and digital twins. However, there is potential for misuse, such as the arbitrary distribution of digital assets or privacy leakage related to building structures. These risks can be mitigated by embedding watermarks into the 3D assets .