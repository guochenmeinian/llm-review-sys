# Improved Depth Estimation of

Bayesian Neural Networks

 Bart van Erp\({}^{1,2}\) &Bert de Vries\({}^{1,2,3}\)

\({}^{1}\) Lazy Dynamics \({}^{2}\) Eindhoven University of Technology \({}^{3}\) GN Hearing

Eindhoven, The Netherlands

{b.v.erp, bert.de.vries}@tue.nl

###### Abstract

This paper proposes improvements over earlier work by Nazareth and Blei  for estimating the depth of Bayesian neural networks. Here, we propose a discrete truncated normal distribution over the network depth to independently learn its mean and variance. Posterior distributions are inferred by minimizing the variational free energy, which balances the model complexity and accuracy. Our method improves test accuracy on the spiral data set and reduces the variance in posterior depth estimates.

## 1 Introduction

Determining the optimal neural network architecture for a given problem is a challenging task, typically involving manual design iterations or automated grid searches. Both approaches are time-consuming and resource-intensive. A critical aspect of this process is balancing the model's complexity to prevent overfitting while ensuring high accuracy.

The seminal work of Nazareth and Blei  introduced a variational inference scheme to network depth estimation. By treating the layer depth of the model as a latent variable, they can infer its posterior distribution. Importantly, their variational free energy provided an excellent objective for balancing the model complexity against the model accuracy.

Although the approach presented in  offers a refreshing perspective, some areas could be improved. For instance, using a truncated Poisson distribution for layer depth results in the mean and variance being approximately equal, which can lead to significant uncertainty in determining the appropriate number of layers, especially for networks of increasing depth and complexity. Moreover, although the methodology in  is based on variational principles, certain simplifying assumptions undermine the probabilistic nature of their model. Specifically, the first-order linearization approximation over expectations neglects uncertainties over the parameters.

This paper focuses exclusively on Bayesian neural networks and builds on the work by , addressing the aforementioned areas of improvement. Specifically, we make the following contributions:

* We propose a discrete truncated normal distribution over the number of hidden layers of a Bayesian neural network, enabling variance reduction in the posterior estimates of the appropriate number of layers;
* Parameter estimation and structure learning are jointly performed by minimization of the variational free energy, explicitly taking the uncertainties over variables into account.

In Section 2 the probabilistic model is specified, after which the inference procedure is elaborated in Section 3. Section 4 discusses the results obtained, and Section 5 concludes the paper.

## 2 Model specification

Let \(=\{(x_{n},y_{n})\}_{n=1}^{N}\) be a dataset of \(N\) labeled observations. We define the likelihood function of a Bayesian neural network as

\[p(y_{n}\,|\,x_{n},,L) =(y_{n}\,|\,_{L}(x_{n}),),\] (1a) \[p(y_{n}\,|\,x_{n},,L) =(y_{n}\,|\,(_{L}(x_{n}))),\] (1b)

for regression and classification, respectively. \((\,|\,,)\) represents a normal distribution with mean \(\) and covariance \(\) and \((\,|\,p)\) is a categorical distribution with event probabilities \(p\), with \(()\) denoting the softmax function. The underlying non-linearity \(_{L}\) is parameterized by parameters \(\), is visualized in Figure 1 and is defined as the composition

\[_{L}=g_{L} f_{L} f_{L-1} f_{1} f_{0},\] (2)

with input transformation \(f_{0}\), latent transformations \(\{f_{l}\}_{l=1}^{L}\) and output transformations \(\{g_{l}\}_{l=0}^{L}\).

We treat the model depth \(L_{0}\) as an unknown variable. Therefore, a suitable discrete prior must be selected, with limited support and enabling efficient inference. The truncated Poisson distribution proposed in  has a variance and support that grows in network depth, preventing it from converging to a single value for the depth.

Alternative discrete distributions suffer from similar problems, such as the negative binomial distribution whose variance is always larger or equal to its mean. Others do not have continuous parameters, such as the hypergeometric distribution with integer parameters. For the categorical distributions used in , the support needs to be bounded. The generalized Poisson distribution  enables situations where its mean exceeds its variance, however, in those situations the distribution quickly becomes ill-defined . Furthermore, the Conway-Maxwell-Poisson distribution does not require closed-form expressions for its normalization constant [5; 6].

Here, we propose to use a discrete truncated normal distribution, whose mean and variance are decoupled, which enables us to model both over- and under-dispersed distributions. Let \(_{ 0}(x\,|\,,^{2})}{{ }}(x\,|\,,^{2})[x\,|\,x 0]\) denote a normal distribution truncated to the positive real line. Based on this truncated normal distribution, we define the prior over \(L\) as its discrete counterpart

\[p(L)=_{L}^{L+1}_{ 0}(l\,|\,_{L},_{L}^{2})\, lL_{0}.\] (3)

We intentionally do not choose a discrete Gamma distribution  here, despite its positive domain, because computing derivatives to the shape parameter after truncation is difficult due to the presence of the lower incomplete gamma function in its cumulative density function.

To complete the model specification, the prior over the parameters is chosen to fully factorize as

\[p(\,|\,L)=_{_{g_{L}}_{g_{L}}}(_ {g_{L}}\,|\,_{},_{}^{2})_{l=0}^{L}_{ _{f_{l}}_{f_{l}}}(_{f_{l}}\,|\,_{ },_{}^{2}),\] (4)

where an explicit distinction in made between the parameters in the input and hidden layers \(\{_{f_{l}}\}_{l=0}^{L}\), which are shared amongst different model depths, and in the depth-specific output layers \(\{_{g_{l}}\}_{l=0}^{L}\).

Figure 1: Visualization of the non-linearity \(_{L}\) in (2). Deeper models reuse parts of shallower models.

With the model specified, the next step involves specifying the variational posterior distribution. We factorize the variational posterior distribution as

\[q(,L)=q(L)_{_{g_{L}}_{g_{L}}}(_{ g_{L}}\,|\,_{},_{}^{2})_{l=0}^{L} _{_{f_{l}}_{f_{l}}}(_{f_{l}}\,|\, _{},_{}^{2}).\] (5)

To retain tractability, we further truncate the variational posterior distribution over \(L\) to its lower and upper quantiles defined by \(p_{l}\), \(p_{u}\) to ensure a limited support by defining

\[_{ 0}^{[p_{l},p_{u}]}(x\,|\,,^{2})}{{}}_{ 0}(x\,|\,,^{2}) [x\,\,p_{l}_{0}^{x}_{ 0}(z\,|\,,^{2}) \,z p_{u}].\] (6)

Using this expression the variational posterior distribution over the network depth is formulated as

\[q(L)=_{L}^{L+1}_{ 0}^{[p_{l},p_{u}]}(l\,|\,_{L}, _{L}^{2})\,lL_{0}.\] (7)

where the \(\) accent identifies the variational parameters in (6) and (7).

## 3 Probabilistic inference

Estimation of the variational posterior distributions, which encompasses both parameter estimation and structure learning, is achieved by minimization of the variational free energy

\[[p,q]&=_{q(L, )}[],\\ &=_{q(L)}[+_{q( \,|\,L)}[+_{n=1}^{N}  p(y_{n}\,|\,x_{n},,L)]],\] (8)

where the expectation over parameters can be further decomposed as

\[_{q(\,|\,L)}[ ]=_{_{g_{L}}_{g_{L}}}[q(_ {g_{L}})\|p(_{g_{L}})]+_{l=0}^{L}_{_{f_{l}} _{f_{l}}}[q( f_{l})\|p(_{f_{l}}) ].\] (9)

Although the expectation over the network depth seems computationally involved, the limited support as a result of the truncation in (7) reduces this operation to a finite summation as \(_{q(L)}[f(\,|\,L)]=_{l\{q(L)\}}q(l)f( \,|\,l)\). Furthermore, since hidden layers are reused in networks of varying depth as illustrated in Figure 1, most computations can be reused in computing the expected log-evidence.

## 4 Experiments

All experiments1 have been implemented in Julia  to explore its excellent metaprogramming capabilities as required by the dynamic nature of the unbounded models. We closely follow the experimental design of  and generate a train, validation and test set of \(1024\) samples each of

Figure 2: Spiral datasets for different rotation speeds \(\), generated according to Appendix A.1.

the spiral dataset [1; 9] for binary classification as described in Appendix A.1. This dataset is parameterized by a rotation speed \(\), which captures the difficulty of the dataset as shown in Figure 2.

The input layer \(f_{0}:^{2}^{32}\) and latent layers \(f_{l}:^{32}^{32}\,\,l 1\) each consist of a linear transformation followed by a LeakyReLU . The output layers only involve a linear transformation \(g_{l}:^{32}^{2}\,\,l 0\), where the non-linearity appears in (1b). We compare our approach to  which uses a \((0.5)\) prior, where the variational posterior distribution is initialized by the \((1.0)\) distribution, truncated to the \(0.95\)-quantile. We select a similarly shaped normal distribution (\(_{L}=0,_{L}=0\), \(_{L}=1.15\) and \(_{L}=1.8\)), whose truncation is defined by \(p_{l}=0.025\) and \(p_{u}=0.975\). Appendix A.2 shows the resemblance between these priors.

We jointly learn the parameters of the probabilistic model and its variational posterior through stochastic variational inference  by minimizing the variational free energy in (8) using the Adam optimizer  until convergence. Appendix A.3 specifies the hyperparameter settings. Inference in the model is performed using Bayes-by-backprop  with local reparameterization . The model that achieves the lowest variational free energy on the validation set is saved and evaluated on the test set by forming predictions according to

\[p(y^{}\,|\,x^{})_{q(,L)}[p(y^{}\,| \,x^{},,L)].\] (10)

Figure 3 shows the achieved predictive accuracy on the test set and the inferred posterior distributions over the model depth. From this we conclude that the discrete truncated normal distribution outperforms the Poisson distribution on the spiral classification task. The normal-based model achieves a higher accuracy, which becomes increasingly significant when the complexity of the data increases. Furthermore, as expected, the posterior distribution over the model depth in the normal-based model has a reduced variance in comparison to the Poisson-based model, as its mean and variance are naturally decoupled during training. In practice this leads to computational savings when making predictions using (10) as the narrow support of \(q(L)\) requires less output layers \(g_{l}\) to be active.

## 5 Discussion and conclusion

This paper introduces a discrete truncated normal distribution for modeling the depth of a Bayesian neural network and demonstrates how to infer its posterior distribution through the minimization of variational free energy. Compared to methods using a Poisson prior , our approach results in reduced variance in posterior estimates and improved test accuracy on the spiral classification task.

The results presented in this paper show promising improvements in estimating the depth of Bayesian neural networks. However, additional experiments are required involving more complex models and tasks. Network width estimation and parameter pruning [13; 15; 16; 17] offer valuable opportunities for further expanding the methodology presented here.

Figure 3: (Left) Test accuracy on the spiral classification task for varying rotation speeds \(\). Solid lines represent the average accuracy over five independent runs, with shaded areas indicating one standard deviation (\(\)). The discrete truncated normal distribution shows accuracy improvements across all rotational speeds compared to the Poisson-based model in . (Right) Means and standard deviations of the posterior distributions over network depth, shown for the first run, with similar trends across other runs. As expected, the variance of the Poisson-based model increases at larger depths, while the normal distribution converges to a single depth.