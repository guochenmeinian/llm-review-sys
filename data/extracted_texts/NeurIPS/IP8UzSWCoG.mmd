# Gaussian Randomized Exploration for Semi-bandits with Sleeping Arms

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper provides theoretical analyses of problem-independent upper and lower regret bounds for Gaussian randomized algorithms in semi-bandits with sleeping arms, where arms may be unavailable in certain rounds, and available arms satisfying combinatorial constraints can be played simultaneously. We first introduce the CTS-G algorithm, an adaptation of Thompson sampling with Gaussian priors, achieving an upper bound of \((m)\) over \(T\) rounds with \(N\) arms and up to \(m\) arms played per round, where \(\) hides the logarithmic factors. Next, we present CL-SG, which improves upon CTS-G by using a single Gaussian sample per round, achieving a near-optimal upper regret bound of \(()\). We also establish that both algorithms have lower regret bounds of \((})\) and \(()\), respectively.

## 1 Introduction

We consider a sleeping semi-bandit problem with a fixed set \([N]\ =\ \{1,\,2,\,.\,.\,.\,\,N\}\) of \(N\) base arms and each base arm \(a\ \ [N]\) is associated with an unknown probability distribution \(p_{a}\) supported on \([0,\,1]\) and mean \(r_{a}\). Unlike standard combinatorial bandits (Kveton et al., 2015), where a learning agent, in each round \(t\ =\ 1,\,.\,.\,.\,\,T\), plays a super arm (combinations of base arms) \(A_{t}\ \ \), where \(\ \ 2^{[N]}\) is a _feasible set_ that satisfy certain constraints, sleeping semi-bandits involve a time-varying feasible set \(_{t}\ \ \), revealed at each round \(t\). After observing the feasible set \(_{t}\) in round \(t\), the learning agent selects a super arm \(A_{t}\ \ _{t}\), observes rewards \(r_{a,t}\ \ p_{a}\) for each base arm \(a\ \ A_{t}\), and aims to minimize the \(T\)-round (pseudo)-regret defined as follows.

\[(T)\ :=\ _{t=1}^{T}\ \ _{a A_{t}^{*}} \ r_{a}\ -\ _{a A_{t}}\ r_{a}\,\] (1)

where \(A_{t}^{*}\ :=\ _{A_{t}}_{a A}\ r_{a}\) denotes the optimal super arm in round \(t\) and the expectation is taken over \(_{t}\), \(A_{t}\), and \(A_{t}^{*}\). Note that \(A_{t}^{*}\) is determined by \(_{t}\). We further denote by \(m\ :=\ _{A}\ |A|\) the maximum number of base arms in any super arm.

The _upper confidence bound (UCB)_(Agrawal, 1995; Auer et al., 2002) and _Thompson sampling (TS)_(Thompson, 1933; Kaufmann et al., 2012; Agrawal & Goyal, 2012, 2017a) are two leading algorithmic families for addressing stochastic bandit problems. For semi-bandit settings, the minimax lower bound is established as \(()\)(Kveton et al., 2015; Merlis & Mannor, 2020), and UCB-based algorithms achieve an upper bound of \(O()\)(Kveton et al., 2015). Although TS-based algorithms have been analyzed for problem-dependent bounds in semi-bandits (Wang & Chen, 2018; Perrault et al., 2020), their results cannot be simply extended to reasonable problem-independent bounds because their bounds contain constant terms that grow exponentially with \(m\).

While a substantial body of literature has explored the setting of sleeping semi-bandits (Hu et al., 2019; Li et al., 2019; Wu & Li, 2024) using _upper confidence bound (UCB)_-based algorithms with an upper bound of \(O(})\), the upper and lower bounds for _Thompson sampling (TS)_-based algorithms for (sleeping) semi-bandits still remain an open problem. Since TS is highly competitive with advanced UCB-based algorithms and widely used in large-scale applications (Chapelle & Li, 2011), investigating the theoretical performance of TS-based algorithms is crucial.

This work addresses long-standing gaps in the literature by introducing two algorithms with provable theoretical guarantees. The first algorithm, CTS-G, is an adaptation of TS with Gaussian priors specifically designed for sleeping semi-bandits, achieving an upper bound of \((m)\), where \(\) hides the logarithmic factors, and a lower bound of \((})\). We further introduce CL-SG, which improves upon CTS-G both theoretically and practically by employing only a single Gaussian sample, resulting in tighter bounds: an upper bound of \(()\) and a lower bound of \(()\). CL-SG is minimax-optimal up to logarithmic factors compared to the known lower bound for combinatorial bandits (Kveton et al., 2015; Merlis Mannor, 2020).

## 2 Gaussian Randomized Algorithms

We first present some notations specific to this section. Let \(n_{a,t}\ :=\ _{=1}^{t-1}[a\ \ A_{}]\) denote the total number of times that base arm \(a\ \ [N]\) has been pulled at the beginning of round \(t\). Let \(_{a,n_{a,t}}\ :=\ ^{t-1}[a A_{}]  r_{a,}}{n_{a,t}}\) denote the empirical mean of base arm \(a\) at the beginning of round \(t\), which is the average of \(n_{a,t}\) i.i.d. random variables according to reward distribution \(p_{a}\). Let \(_{t}\) collect all the actions and observed rewards up to the end of round \(t\)

In Sec. 2.1, we present CTS-G, an algorithm enjoying \((m)\) and \((})\) upper and lower regret bounds. In Sec. 2.2, we present CL-SG, an algorithm enjoying \(()\) and \(()\) upper and lower regret bounds. The practical performance of both algorithms is discussed in Appendix A, and all the detailed proofs can be found in Appendix C to D.

### Combinatorial Thompson Sampling with Gaussian Priors (CTS-G)

CTS-G presented in Alg. 1 is a direct adaptation of TS with Gaussian priors (Agrawal & Goyal, 2017b) to the sleeping semi-bandit problems. The core idea is to use posterior distributions to model the mean reward \(r_{a}\) of each base arm \(a\ \ [N]\). In each round \(t\), CTS-G draws a Gaussian posterior sample \(w_{a,t}\ \ (_{a,n_{a,t}},\ +1})\) for each \(a\ \ [N]\), where \(\ >\ 0\) is a constant parameter to control the exploration level.1 We can view the collection \(_{t}\ =\ \{w_{a,t},\  a\ \ [N]\}\) of all posterior samples as the "sampled problem instance" based on which the learning agent conducts learning in round \(t\). Then, based on the revealed feasible set \(_{t}\), CTS-G plays the super arm \(A_{t}\ \ _{A_{t}}_{a A}\ w_{a,t}\) with the highest aggregated value of posterior samples and observes each individual base arm's random reward.

**Theorem 1**.: _(1) The regret of CTS-G is \(O\ m\ (T)\). (2) There exists a problem instance such that CTS-G suffers \(\ }\) regret._

**Discussion.** Theorem 1 states that CTS-G is worst-case optimal up to an extra \((T)\) factor. Compared with UCB-based algorithms for sleeping semi-bandits, our upper bound has an extra factor of \(\) with the ones by Hu et al. (2019); Li et al. (2019), which are \(O(})\). However, it is important to note a significant aspect of our model: unlike the assumptions in Hu et al. (2019); Li et al. (2019), our bound is derived without relying on stochastic assumptions regarding the availability of arms. Furthermore, the upper bound is minimax optimal up to an extra \((T)\) factor as compared to the \(\ \) minimax lower bound for combinatorial bandits shown in Merlis Mannor (2020).

**Upper bound proof sketch.** The theoretical analysis is non-trivial due to overlapping base arms among super arms, the dynamic nature of the optimal super arm \(A_{t}^{*}\), and its unobservability, as only the played super arm \(A_{t}\) is observed in each round \(t\) To decompose the regret, we define a high probability event for the empirical estimates. Let \(_{t}\,:=\,r_{a}\,-\,_{a,n_{a,t}} {+1}},\, a[N]}\) be the event that the empirical means are close to their true means by the beginning of round \(t\). Let \(t^{}\,=\,\{,\,4\}\) and \(_{_{t}}[\,:\,:=\,[\,\,\,|\,\,_{t}]\). Then, we decompose the regret defined in (1) as

\[(T)\,\,}^{T}[ _{a A_{t}^{*}}r_{a}\,-\,_{_{t}}\,_{a A_{ t}}w_{a,t}]}_{=:I_{1},\,}+}^{T}[_{ _{t}}\,[_{a A_{t}}(w_{a,t}\,-\,r_{a})\,\,[_{t}]]]}_{=:I_{2},\,}+mt^{}\,+\,O(1).\]

The deviation term \(I_{2}\) is easy to analyze as we can observe \(A_{t}\), and is upper bounded by \((m)\) via using concentration bounds. The center question is how to upper bound the optimism term, which measures the gap between the _maximum amount of true reward_\(_{a A_{t}^{*}}r_{a}\) the learning agent could achieve and the _expected maximum amount of reward_\(_{a A_{t}}\,w_{a,t}\) the learning agent can observe in round \(t\). Intuitively, if the learning agent is lucky, i.e., the history \(_{t-1}\) gives \(_{a A_{t}^{*}}r_{a}\,\,_{_{t}}\,[_{a A _{t}}w_{a,t}]\), there is no regret in round \(t\) for this term. Let \(()^{+}\,:=\,\,\{,\,0\}\) be an activation function. Then, we have

\[_{a A_{t}^{*}}r_{a}\,-\,_{_{t}}\,[_{a A_{t }}w_{a,t}]\,\,(_{a A_{t}^{*}}r_{a}\,-\,_{ _{t}}\,[_{a A_{t}}w_{a,t}])^{+}\,.\] (2)

Let \(c()\) be a constant only depending on \(\). In our novel technical Lemma 1, inspired by Russo (2019), we show

\[(_{a A_{t}^{*}}r_{a}\,-\,_{_{t}}\,[_{a  A_{t}}w_{a,t}])^{+}\,\,c()\,\,_{ _{t}}\,[(_{a A_{t}}\,w_{a,t}\,-\,_{_{t} }\,[_{a A_{t}}w_{a,t}])^{+}]\,,\] (3)

which tackles the challenge brought by the unobservability of \(A_{t}^{*}\).

Next, via introducing an independent "ghost" copy \(_{a,t}\,\,(_{a,n_{a,t}},\,+1})\) of \(w_{a,t}\), we show

\[_{_{t}}\,[(_{a A_{t}}w_{a,t}\,-\,_ {_{t}}\,[_{a A_{t}}w_{a,t}])^{+}]\,\, _{_{t}}\,[|_{a A_{t}}\,(w_{a,t}\,-\, _{a,t})|]\,,\] (4)

which gets rid of the introduced activation function.

Since \(w_{a,t}\,-\,_{a,t}\,\,\,(0,\,+1})\), we only need to deal with Gaussian random variables and have

\[_{t=t^{}}^{T}\,\,[|_{a A_{t}}\,(w_{a,t} \,-\,_{a,t})|]\,\,O\,(m\,\,T )\,.\] (5)

**Lower bound proof sketch.** Inspired by Theorem 1.4 in Agrawal & Goyal (2017b), the lower bound is refined by constructing a path selection problem with \(N\) links (base arms) and \(K\) paths (super arms) of \(m\) links. This reduces the semi-bandits to \(K\) independent path selections, and the result follows using the anti-concentration inequality for Gaussian variables (Appendix C.7)

### Combinatorial Learning with Single Gaussian Seed (CL-SG)

Since the upper bound of CTS-G still has an extra \((T)\) factor from the minimax lower bound Merlis & Mannor (2020) for combinatorial bandits, we are motivated to improve the upper bound by controlling the amount of randomness injected within the learning algorithm.

Inspired by Xiong et al. (2022), we devise CL-SG which enjoys a \(()\) regret bound. The key idea behind the removal of the extra \(\) factor as compared to the regret of CTS-G (Alg. 1)is CL-SG uses a single random seed \(w_{t}\,\,(0,\,1)\) to perturb the empirical estimates of all the base arms, as shown in Alg. 2. After drawing \(w_{t}\), we construct \(_{a,t}\;=\;_{a,n_{a,t}}\;+\;w_{t}\,\,+1}}\) for all the base arms \(a\;\;[N]\), where constant \(\;>\;0\) controls the exploration level. Then, we play \(A_{t}\;=\;\,_{A_{t}}\,_{a A}\,_{a,t}\) from the feasible set \(_{t}\) in round \(t\).

**Theorem 2**.: _(1) The regret of CL-SG is \(O\,(\,T)\). (2) There exists a problem instance such that CL-SG suffers \(\,()\) regret._

**Discussion.** The extra \(\) in CTS-G comes from the \(m\) factor in the variance of the Gaussian posterior sample \(w_{a,t}\), necessary to keep \(c()\) bounded by a constant. To bound \(c()\), we must lower bound \(_{_{t}}\,(_{a A_{t}^{*}}\,w_{a,t}\,-\,_{a,n_{a,t} }\,\,_{a A_{t}^{*}}\,+1}})\), requiring the Cauchy-Schwarz inequality to bring the summation inside the square root for the RHS term in the probability, which scales with \(\), i.e., \(_{a A_{t}^{*}}\,+1}}\,\,^{*}}\,+1}}\). This fact further results in an extra \(m\) in the variance of CTS-G Gaussian samples for the probability to be lower bounded by a constant. On the other hand, with CL-SG, using a single \(w_{t}\), we lower bound a similar probability, \(\,(_{a A_{t}^{*}}\,w_{t}+1} }\,\,_{a A_{t}^{*}}\,+1}})\), allowing us to divide both sides by \(_{a A_{t}^{*}}\,+1}}\) and avoid the extra \(m\) in the variance.

The lower-bound proof considers the same problem instance to Theorem 1 but differs in addressing the arms' dependency in CL-SG due to the common \(w_{t}\). Let \(\;:=\;}\) be the reward gap between each suboptimal arm and the optimal super arm, and let \(Q_{A}(t)\) be the number of times that super arm \(A\) has been played at the beginning of round \(t\). Then, denote by \(B_{t}^{*}\;:=\;\{Q_{A_{1}}(t)\,>\,t\,-\,cT\}\) the event that the optimal super arm \(A_{1}\) has been observed enough times at the beginning of round \(t\), where \(c(0,\,1)\) is a constant. It is easy to prove that the regret is lower bounded by \(cT\,\,m=()\) when \(B_{t}^{*}\) is false for some \(t[T]\). The main challenge is to demonstrate that, conditioned on the past histories \(F_{t-1}\) that lead to the happening of event \(B_{t}^{*}\), the probability of playing a suboptimal super arm is at least a constant probability \(p_{0}\), i.e., \(\,( A\,\,A_{1}:A_{t}\,=\,A\,\,_{t-1}\,=\,F_{t-1})\;\;p_{0}\). This leads to lower bound a probability that the empirical estimates of suboptimal arms are larger than the optimal super arm, i.e.,

\[( A\;\;\,\,A_{1}\,:\,_{a A }\,_{a,Q_{A}(t)}\,+\,w_{t}(t)\,+\,1}} \,>\,_{b A_{1}}\,_{b,Q_{A_{1}}(t)}\,+\,w_{t}}(t)\,+\,1}}\,\,_{t-1}\,=\,F_{t-1})\] \[\;( A\;\;\,\,A_{1}\,:\,w_{t} (1\,-\,(t)\,+\,1}{Q_{A_{1}}(t)\,+\,1}}\,>\,(t)\,+\,1}\,\,_{t-1}\,=\,F_{t-1}).\]

This requires analyzing the play ratio between suboptimal and optimal super arms, while in the lower-bound analysis of Theorem 1, we can avoid this situation by independently considering the estimates of the optimal super arm is smaller than \(0\), and that of suboptimal arms is larger than \(0\). The trick to address this ratio is to only consider the regret from \( T\) to \(T\), with \(\;\;(0,\,1)\) such that \((t)+1}{Q_{A_{1}}(t)+1}\;\;\) is a constant by tuning \(c\) and \(\). Then, by applying the anti-concentration bound for Gaussian variables and solving a non-trivial optimization problem, we can prove such a \(p_{0}\) exists, and regret is lower bounded by \((1\;-\;)T\;\;p_{0}\;\;m\;=\;()\).

## 3 Conclusion

In this paper, we have studied the problem of sleeping semi-bandits and presented CTS-G and CL-SG with theoretical guarantees. Our results bridge the existing gap in the literature by providing upper and lower bounds for TS-based algorithms in sleeping semi-bandits. Future work will focus on narrowing the gap between these bounds, and studying the relationship between the number of random variables and their variances.