# Supra-Laplacian Encoding for

Transformer on Dynamic Graphs

 Yannis Karmim

Conservatoire National des Arts et Metiers

CEDRIC, EA 4629

F 75003, Paris, France

yannis.karmim@cnam.fr&Marc Lafon

Conservatoire National des Arts et Metiers

CEDRIC, EA 4629

F 75003, Paris, France

marc.lafon@lecnam.net&Raphael Fournier S'niehotta

Conservatoire National des Arts et Metiers

CEDRIC, EA 4629

F 75003, Paris, France

fournier@cnam.fr&Nicolas Thome

Sorbonne Universite

CNRS, ISIR

F-75005 Paris, France

nicolas.thome@isir.upmc.fr

###### Abstract

Fully connected Graph Transformers (GT) have rapidly become prominent in the static graph community as an alternative to Message-Passing models, which suffer from a lack of expressivity, oversquashing, and under-reaching. However, in a dynamic context, by interconnecting all nodes at multiple snapshots with self-attention,GT loose both structural and temporal information. In this work, we introduce **S**upra-**LA**palcian encoding for spatio-temporal Transform**E**rs (SLATE), a new spatio-temporal encoding to leverage the GT architecture while keeping spatio-temporal information. Specifically, we transform Discrete Time Dynamic Graphs into multi-layer graphs and take advantage of the spectral properties of their associated supra-Laplacian matrix. Our second contribution explicitly model nodes' pairwise relationships with a cross-attention mechanism, providing an accurate edge representation for dynamic link prediction. SLATE outperforms numerous state-of-the-art methods based on Message-Passing Graph Neural Networks combined with recurrent models (_e.g._, LSTM), and Dynamic Graph Transformers, on 9 datasets. Code is open-source and available at this link https://github.com/ykrmm/SLATE.

## 1 Introduction

Dynamic graphs are crucial for modeling interactions between entities in various fields, from social sciences to computational biology . Link prediction on dynamic graphs is an all-important task, with diverse applications, such as predicting user actions in recommender systems, forecasting financial transactions, or identifying potential academic collaborations. Dynamic graphs can be modeled as a time series of static graphs captured at regular intervals (Discrete Time Dynamic Graphs, DTDG) .

Standard approaches for learning representations on DTDGs combine Message-Passing GNNs (MP-GNNs) with temporal RNN-based models . In static contexts, Graph Transformers (GT)  offer a compelling alternative to MP-GNNs that faced several limitations . Indeed, their fully-connected attention mechanism captures long-range dependencies, resolving issues such as oversquashing . GTs directly connect nodes, using the graph structure as a soft bias through positional encoding . Incorporating Laplacian-based encodings in GTs provably enhances their expressiveness compared to MP-GNNs .

Exploiting GTs on dynamic graphs would require a spatio-temporal encoding that effectively retains both structural and temporal information. The recent works that have extended GTs to dynamic graphs capture spatio-temporal dependencies between nodes by using _partial_ attention mechanisms . Moreover, these methods also employ encodings which _independently_ embed the graph structure and the temporal dimension. Given that the expressiveness of GTs depends on an accurate spatio-temporal encoding, designing one that interweaves time and position information could greatly enhance their potential and performance.

The vast majority of neural-based methods for dynamic link prediction rely on node representation learning . Recent works enrich node embeddings with pairwise information for a given node-pair using co-occurrence neighbors matching  or cross-attention on historical sub-graphs . However these methods neglect the global information of the graph by sampling different spatio-temporal substructures around targeted nodes.

Pioneering work in the complex network community has studied temporal graphs with multi-layers models and supra-adjacency matrices . The spectral analysis of such matrices can provide valuable structural and temporal information . However, how to adapt this formalism for learning dynamic graphs with transformer architectures remains a widely open question.

In this work, we introduce **S**upra-**L**A**placian encoding for spatio-temporal **T**ransform**E**rs (SLATE), a new unified spatio-temporal encoding which allows to fully exploit the potential of the GT architecture for the task of dynamic link prediction. As illustrated on Figure 1, adapting supra-Laplacian matrices to dynamic graph can provide rich spatio-temporal information for positional encoding. SLATE is based on the following two main contributions:

* We bridge the gap between multi-layer networks and Discrete Time Dynamic Graphs (DTDGs) by adapting the spectral properties of supra-Laplacian matrices for transformers on dynamic graphs. By carefully transforming the supra-Laplacian matrices for DTDGs, we derive a connected multi-layer graph that captures various levels of spatio-temporal information. We introduce a fully-connected spatio-temporal transformer that leverages this unified supra-Laplacian encoding.
* The proposed transformer captures dependencies between nodes across multiple time steps, creating dynamic representations. To enhance link prediction, we introduce a lightweight edge representation module using _cross-attention_ only between the temporal representations of node pairs, precisely capturing their evolving interactions. This results in a unique edge embedding, significantly streamlining the prediction process and boosting both efficiency and accuracy.

Figure 1: SLATE is a fully connected transformer for dynamic link prediction, which innovatively performs a joint spatial and temporal encoding of the dynamic graph. SLATE models a DTDG as a multi-layer graph with temporal dependencies between a node and its past. Building the supra-adjacency matrix of a randomly-generated toy dynamic graph with 3 snapshots (_left_) and analysing the spectrum of its associated supra-Laplacian (_right_) provide fundamental spatio-temporal information. The projections on eigenvectors associated with smaller eigenvalues (\(_{1}\)) capture global graph dynamics: node colors are different for each time step. Larger eigenvalues ( _e.g._\(_{}\)), capture more localized spatio-temporal information (see Appendix A.1).

We conduct an extensive experimental validation of our method across 11 real and synthetic discrete-time dynamic graph datasets. SLATE outperforms state-of-the-art results by a large margin. We also validate the importance of our supra-Laplacian unified spatio-temporal encoding and the edge module for optimal performances. Finally, SLATE remains efficient since it uses a single-layer transformer, and we show impressive results on larger graph datasets, indicating good scalability, and limited time-memory overhead.

## 2 Related work

Dynamic Graph Neural Networks on DTDGs.The standard approach to learn on DTDGs [41; 54] involves using two separate spatial and temporal models. The spatial model is responsible for encoding the structure of the current graph snapshot, while the temporal model updates the dynamic either of the graph representations [39; 40; 59; 30; 28] or the graph model parameters [34; 15]. Recently, ROLAND  introduced a generic framework to use any static graph model for spatial encoding coupled with a recurrent-based (LSTM , RNN, GRU) or attention-based temporal model. These above methods mainly use a MP-GNN as spatial model [23; 46; 58]. However, MP-GNNs are known to present critical limitations: they struggle to distinguish simple structures like triangles or cycles [33; 4], and fail to capture long-range dependencies due to oversquashing [1; 42]. To overcome these limitations, some works have adopted a fully-connected GT as spatial model, benefiting from its global attention mechanism [6; 50; 61]. In , the local structure is preserved by computing the attention on direct neighbors. In contrast to these works, SLATE uses a unique spatio-temporal graph transformer model, greatly simplifying the learning process.

Graph Transformer.In static contexts, Graph Transformers have been shown to provide a compelling alternative to MP-GNNs . GTs [51; 37; 22; 57] enable direct connections between all nodes, using the graph's structure as a soft inductive bias, thus resolving the oversquashing issue. The expressiveness of GTs heavily depends on positional or structural encoding [11; 32; 12; 3]. In , the authors use the eigenvectors associated with the \(k\)-lowest eigenvalues of the Laplacian matrix, which allows GTs to distinguish structures that MP-GNNs are unable to differentiate. Following the success of Laplacian positional encoding on static graphs, SLATE uses the eigenvectors of the supra-Laplacian of a multi-layer graph representation of DTDGs as spatio-temporal encoding.

Dynamic Graph Transformers.To avoid separately modelling structural and temporal information as dynamic Graph Neural Networks usually do on DTDGs, recent papers have adopted a unified model based on spatio-temporal attention [31; 18]. This novel approach make those models close to transformer-based methods classically employed to learn on Continuous Time Dynamic Graphs (CTDG) [52; 47; 49]. Among them, some preserve the local structure by computing attention only on direct neighbors [52; 39], while others sample local spatio-temporal structures around nodes [47; 31; 56; 18] and perform fully-connected attention. However, their spatio-temporal encoding is still built by concatenating a spatial and a temporal encoding that are computed independently. The spatial encoding is either based on a graph-based distance [47; 18] or on a diffusion-based measure . The temporal encoding is usually sinus-based [52; 49; 2] as in the original transformer paper . Another drawback of these methods [31; 47; 18; 49] is that they use only sub-graphs to represent the local structure around a given node. Therefore, their representations of the nodes are computed on different graphs and thus fail to capture global and long-range interactions. Contrary to those approaches, our SLATE model uses the same graph to compute node representations in a fully-connected GT between all nodes within temporal windows. It features a unified spatio-temporal encoding based on the supra-Laplacian matrix.

Dynamic Link Prediction methods.For dynamic link prediction, many methods are based only on _node_ representations and use MLPs or cosine similarity to predict the existence of a link [39; 40; 38]. Recent approaches complement node representations by incorporating pairwise information. Techniques like co-occurrence neighbors matching [60; 48] or cross-attention on historical sub-graphs  are employed. However, these methods often overlook the global graph structure by focusing on sampled spatio-temporal substructures. For instance, CAW-N  uses anonymous random walks around a pair of nodes and matches their neighborhoods, while DyGformer  applies transformers to one-hop neighborhoods and calculates co-occurrences. These localized approaches fail to capture the broader graph context. TCL  is the closest to SLATE, using cross-attention between spatiotemporal representations of node pairs. TCL samples historical sub-graphs using BFS and employs contrastive learning for node representation. However, it still relies on sub-graph sampling, missing the full extent of the global graph information. In contrast, SLATE leverages the entire graph's spectral properties through the supra-Laplacian, incorporating the global structure directly into the spatio-temporal encoding. This holistic approach allows SLATE to provide a richer understanding of dynamic interactions, leading to superior link prediction performance.

## 3 The SLATE Method

In this section, we describe our fully-connected dynamic graph transformer model, SLATE, for link prediction. The core idea in section 3.1 is to adapt the supra-Laplacian matrix computations for dynamic graph transformer (DGTs), and to introduce our new spatio-temporal encoding based on its spectral analysis. In section 3.2, we detail our full-attention transformer to capture the spatio-temporal dependencies between nodes at different time steps. Finally, we detail our edge representation module for dynamic link prediction in section 3.3. Figure 2 illustrates the overall SLATE framework.

**Notations.** Let us consider a DTDG \(\) as an undirected graph with a fixed number of \(N\) nodes across snapshots, represented by the set of adjacency matrices \(=\{A_{1},...,A_{T}\}\). Its supra-graph, the multi-layer network \(=(,)\), is associated to a supra-adjacency matrix \(\), obtained by stacking \(A_{i}\) diagonally (see Eq. (7) in Appendix A.1). Then, the supra-Laplacian matrix \(\) is defined as \(=I-^{-1/2}^{-1/2}\), where \(I\) is the identity matrix and \(\) is the degree matrix of \(\). Let \(_{}^{F}\) be the feature vector associated with the node \(u\) (which remains fixed among all snapshots). Finally, let consider the random variable \(y\{0,1\}\) such that \(y=1\) if nodes \(u\) and \(v\) are connected and \(y=0\) otherwise.

Figure 2: The **SLATE model** for link prediction with dynamic graph transformers (DGTs). To recover the lost spatio-temporal structure in DGTs, we adapt the supra-Laplacian matrix computation to DGTs by making the input graph provably connected (a), and use its spectral analysis to introduce a specific encoding for DGTs (b). (c) Applies a fully connected spatio-temporal transformer between all nodes at multiple time-step. Finally, we design in (d) an edge representations module dedicated to link prediction using cross-attention on multiple temporal representations of the nodes.

### Supra-Laplacian as Spatio-Temporal Encoding

In this section, we cast Discrete Time Dynamic Graphs (DTDGs) as multi-layer networks, and use the spectral analysis of their supra-graph and generate a powerful spatio-temporal encoding for our fully-connected transformer.

**DTDG as multi-layer graphs.** If a graph is connected, its spectral analysis provides a rich information of the global graph dynamics, as shown in Figure 1. The main challenge in casting DTDG as multi-layer graphs relates to its disconnectivity, which induces as many zero eigenvalues as connected components. DTDG have in practice a high proportion of isolated nodes per snapshot (see Figure 3 in experiments), making the spectral analysis on the raw disconnected graph useless. Indeed, it mainly indicates positions relative to isolated nodes, losing valuable information on global dynamics and local spatio-temporal structures. We experimentally validate that it is mandatory to compute the supra-Laplacian matrix on a connected graph to recover a meaningful spatio-temporal structure.

**Supra-Laplacian computation.** To overcome this issue and make the supra-graph connected, we follow three steps: (1) remove isolated nodes in each adjacency matrix, (2) introduce a virtual node in each snapshot to connect clusters, and (3) add a temporal self-connection between a node and its past if it existed in the previous timestep. We avoid temporal dependencies between virtual nodes to prevent artificial connections. These 3 transformation steps make the resulting supra-graph provably connected. This process is illustrated in Figure 1(a), and we give the detailed algorithm in Appendix A.3.

**Spatio-temporal encoding.** With a connected \(}\), the second smallest eigenvalue \(_{1}\) of the supra-Laplacian \(\) is guaranteed to be non-negative (see proof in Appendix A.3), and its associated Fiedler vector \(_{1}\) reveals the dynamics of \(}\) (Figure 1). In practice, similar to many static GT models [26; 37; 12], we retrieve the first \(k\) eigenvectors of the spectrum of \(\), with \(k\) being a hyper-parameter. The spectrum can be computed in \(O(k^{2}N^{})\) and have a memory complexity of \(O(kN^{})\) where \(N^{}\) is the size of \(\), and we follow the literature to normalize the eigenvectors and resolve sign ambiguities . The supra-Laplacian spatio-temporal encoding vector of the node \(u\) at time \(t\) is:

\[_{u,t}=\{g_{_{}}(_{u, t}[_{1},_{2},...,_{k}]())&$ is not isolated}\\ g_{_{}}(_{k}())&\}\] (1)

where \(\) denotes the concatenation operator. \(_{u,t}[_{1},_{2},...,_{k}]=[_{1}^{u,t},_{2}^{u,t},...,_{k}^{u,t}]\) contains the projections of the node \(u\) at time \(t\) in the eigenspace spanned by the \(k\) first eigenvectors of \(\), \(()\) contains the eigenvalues of \(\) (which are the same for all nodes) and \(g_{_{}}\) is a linear layer allowing to finely adapt the supra-graph spectrum features to the underlying link prediction task. Note that because we did not include isolated nodes in the computation of the supra-Laplacian, we replace the eigenvector projections by a null vector \(^{k}\) for these nodes. All the steps involved in constructing our spatio-temporal encoding are illustrated in Figure 1(b).

### Fully-connected spatio-temporal transformer

In this section, we describe the architecture of our fully-connected spatio-temporal transformer, \(f_{_{T}}\), to construct node representations that captures long-range dependencies between the nodes at each time step. We illustrate our fully-connected GT in Figure 1(c). We employ a single transformer block, such that our architecture remains lightweight. This is in line with recent findings showing that a single encoder layer with multi-head attention is sufficient for high performance, even for dynamic graphs .

The input representation of the node \(u_{t}\) is the concatenation of the node embeddings (which remains the same for each snapshot) and our supra-Laplacian spatio-temporal encoding:

\[_{u,t}=g_{_{}}(})_{u,t}\] (2)

where \(g_{_{}}\) is a linear projection layer and \(\) denotes the concatenation operator. Then we stack all the representations of _each nodes at each time step_ within a time window of size \(w\) to obtain the input sequence, \(Z^{(Nw) d}\), of the GT.

The fully-connected spatio-temporal transformer, \(f_{_{T}}\), then produces a unique representation \(^{(Nw) d}\) for each node at each time-step :

\[=f_{_{T}}(Z).\] (3)

Surprisingly, considering all temporal snapshots did not yield better results in our experiments (see Figure 4 in section 4.2).

Unlike previous DGT methods that sample substructures around each nodes [31; 56; 47], SLATE leverages the full structure of the DTDG within the time window. This approach ensures that no nodes are arbitrarily discarded in the representation learning process, as we use the same information source \(Z\) for all nodes.

### Edge Representation with Cross-Attention

In this section, we present our innovative edge representation module Edge. It is designed for efficient dynamic link prediction and leverage the node representations learned by our fully-connected spatio-temporal GT. We illustrated our module in Figure 1(d). This module is composed of a cross-attention model, \(f_{_{}}\), that captures pairwise information between the historical representation of two targeted nodes followed by a classifier to determine the presence of a link.

For a link prediction at time \(t+1\) on a given node pair \((u,v)\), we aggregate all temporal representations of \(u\) and \(v\) resulting in two sequences \(_{u,t}=[}_{u,t-w},,}_{u,t}]\) and \(_{v,t}=[}_{v,t-w},,}_{v,t}]\). We use these multiple embeddings to build a pairwise representation that captures dynamic relationships over time. Then, the cross-attention module \(f_{_{}}\) produces a pairwise representation of the sequence \(E_{u,v}^{w d}\) :

\[E_{u,v}=f_{_{}}(_{u,t},_{v,t}).\] (4)

We obtain the final edge representation \(_{,}^{d}\) by applying an average time-pooling operator and we compute the probability that the nodes \(u_{t+1}\) and \(v_{t+1}\) are connected with:

\[p(y=1|\ _{,})=((_{ ,})).\] (5)

SLATE differs from methods that enrich node and edge representations with pairwise information by sampling substructures around each node [47; 60; 48; 18]. Instead, we first compute node representations based on the same dynamic graph information contained in \(Z\). Then, we capture fine-grained dynamics specific to each link \((u,v)\) through a cross-attention mechanism.

Our training resort to the standard Binary Cross-Entropy loss function. In practice, for a node \(u\), we sample a negative pair \(v_{}\) and a positive pair \(v_{}\):

\[_{}=(p(y=1|_{,_{ }}))+(p(y=0|_{,_{}})).\] (6)

In this context, \(=\{_{},_{},_{},_{ }\}\) represents all the parameters within the edge representation module \(_{}\), the fully-connected transformer \(_{T}\), the spatio-temporal linear layer \(_{ST}\) and the node embedding parameters \(_{E}\) as illustrated in Figure 2.

### SLATE Scalability

The theoretical complexity of attention computation is \(O(N^{2})\) per snapshot, scaling to \(O((NT)^{2})\) when considering all \(T\) snapshots. However, as shown in our experiments (Figure 4) and consistent with recent works , a large temporal context is often unnecessary. By using a time window \(w\) with \(w T\) (similar to other DGT architectures [31; 56]), we reduce complexity to \(O((Nw)^{2})\). For predictions at time \(t+1\), we focus only on snapshots from \(G_{t-w}\) to \(G_{t}\). Ablation studies confirm that smaller time windows deliver excellent results across various real-world datasets. We further leverage FLASH Attention  to optimize memory usage and computation. Additionally, we incorporate Performer , which approximates the softmax computation of the attention matrix, reducing the complexity to \(O(Nw)\). This enables us to scale efficiently to larger graphs, as shown in Table 5, while maintaining high performance (see Table 15) with manageable computational resources.

Experiments

We conduct extensive experiments to validate SLATE for link prediction on discrete dynamic graphs, including state-of-the-art comparisons in section 4.1. In section 4.2, we highlight the benefits of our two main contributions, the importance of connecting our supra-graph, and the ability of SLATE to scale to larger datasets with reasonable time and memory consumption compared to MP-GNNs.

**Implementation details.** We use one transformer Encoder Layer . For larger datasets, we employ Flash Attention  for improved time and memory efficiency. Further details regarding model parameters and their selections are provided in Table 8. We fix the token dimension at \(d=128\) and the time window at \(w=3\) for all our experiments. We use an SGD optimizer for all of our experiments. Further details on hyper-parameters search, including the number of eigenvectors for our spatio-temporal encoding, are in Appendix D.

### Comparison to state-of-the-art

Since both the continuous and discrete communities evaluate on similar data, we compare SLATE to state-of-the-art DTDG (Table 1) and CTDG (Table 2) models. Best results are in bold, second best are underlined. More detailed results and analyses are presented in Appendix E.1.

**Baselines and evaluation protocol.** To compare the benefits of fully connected spatio-temporal attention with a standard approach using transformers, we designed the ROLAND-GT model based on the ROLAND framework . This model follows the stacked-GNN approach , equipped with the encoder \(f_{_{T}}\) described in section 3 including static Laplacian positional encoding , and a LSTM  updating the node embeddings.

We adhere to the standardized evaluation protocols for continuous models  and discrete models . Our evaluation follows these protocols, including metrics, data splitting, and the datasets provided. Results in Table 1 and Table 2 are from the original papers, except those marked with \({}^{}\). We report the average results and standard deviations from five runs to assess robustness. Additional results, including hard negative sampling evaluation, are in Appendix E.2.

**Datasets.** In Table 6 Appendix C, we provide detailed statistics for the datasets used in our experiments. An in-depth description of the datasets is given in Appendix C. We evaluate on DTDGs datasets provided by  and , we add a synthetic dataset SBM based on stochastic block model , to evaluate on denser DTDG.

**Comparison to discrete models, on DTDG. Table 1** We showcases the performance of SLATE against various discrete models on DTDG datasets, highlighting its superior performance across multiple metrics and datasets. SLATE outperforms all state of the art models on the HepPh, Enron, and Colab datasets, demonstrating superior dynamic link prediction capabilities. Notably, it surpasses HTGN by +2.1 points in AUC on HepPh and +1.1 points in AP on Enron. Moreover, SLATE shows a remarkable improvement of +7.6 points in AUC over EvolveGCN on Colab. It also performs competitively on the AS733 dataset, with scores that are closely second to HTGN, demonstrating its robustness across different types of dynamic graphs. What also emerges and validates our method from this comparison is the average gain of +6.29 points by our fully connected spatio-temporal attention model over the separate spatial attention model and temporal model approach, as used in ROLAND-GT. We also demonstrate significant gains against sparse attention models like DySat, with

   Method & HepPh & AS733 & Enron & Colab & SBM\({}^{}\) & **Avg.** \\  GCN\({}^{}\) & \(74.52 0.80\) & \(96.65 0.05\) & \(91.31 0.45\) & \(88.28 0.58\) & \(95.96 0.32\) & \(89.34 0.44\) \\ GIN\({}^{}\) & \(71.47 0.56\) & \(93.53 0.55\) & \(91.16 1.17\) & \(85.38 0.61\) & \(88.86 0.46\) & \(86.08 0.67\) \\ EvolveGCN  & \(76.82 1.46\) & \(92.47 0.04\) & \(90.12 0.69\) & \(83.88 0.53\) & \(94.21 0.66\) & \(87.50 0.68\) \\ GRUCGN  & \(82.86 0.53\) & \(94.96 0.35\) & \(92.47 0.36\) & \(84.60 0.92\) & \(92.55 0.41\) & \(89.48 0.51\) \\ DySat  & \(81.02 0.25\) & \(95.06 0.21\) & \(93.06 0.97\) & \(87.25 1.70\) & \(91.92 0.39\) & \(89.67 0.70\) \\ VGRNN  & \(77.65 0.99\) & \(95.17 0.62\) & \(93.10 0.57\) & \(85.95 0.49\) & \(93.88 0.07\) & \(89.15 0.55\) \\ HTGN  & \(91.13 0.14\) & \(\) & \(94.17 0.17\) & \(89.26 0.17\) & \(94.80 0.23\) & \(93.62 0.15\) \\ ROLAND-GT\({}^{}\) & \(81.40 0.45\) & \(94.75 0.87\) & \(90.20 1.12\) & \(82.95 0.45\) & \(94.88 0.31\) & \(\) \\ 
**SLATE** & \(\) & \(97.46 0.45\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Comparison to DTDG models on discrete data. ROC-AUCan increase of +6.45. This study, conducted on the protocol from , emphasizes SLATE capability in handling discrete-time dynamic graph data, offering significant improvements over existing models.

**Comparison to continuous models, on DTDG. Table 2** In dynamic link prediction, SLATE outperforms models focused on node (TGN, DyRep, TGT), edge (CAWN), and combined node-pairwise information (DyGFormer,TCL). Notably, it surpasses TCL by over 21 points in average, showcasing the benefits of our temporal cross attention strategies. SLATE's advantage stems from its global attention mechanism, unlike the sparse attention used by TGAT, TGN, and TCL. By employing fully-connected spatio-temporal attention, SLATE directly leverages temporal dimensions through its Edge module. This strategic approach allows SLATE to excel, as demonstrated by its consistent top performance and further evidenced in Appendix with hard negative sampling results (see Table 17 and Table 16 in Appendix E.1). We demonstrate average results that are superior by 13 points compared to the most recent model on DTDG, DyGFormer .

### Model Analysis

**Impact of different SLATE component.** Table 3 presents the AUC results of different configurations of SLATE on four datasets. This evaluation demonstrates the impact of our proposed spatio-temporal encoding and the Edge module on dynamic link prediction performance.

First, we show the naive spatio-temporal encoding approach using the first \(k\) Laplacian eigenvectors associated with the \(k\) lowest values  (Appendix A.4), combined with sinusoidal unparametrized temporal encoding  (Appendix A.5), without the Edge module. The Laplacian is computed sequentially on the \(w\) snapshots, then concatenated with the temporal encoding indicating the position of the snapshot, with \(k=12\) for both SLATE and the naive encoding. The AUC scores across all datasets are significantly lower, highlighting the limitations of this naive encoding method in capturing complex spatio-temporal dependencies.

Replacing the baseline encoding with our proposed SLATE encoding, still without the Edge module, results in significant improvements: +6.47 points on CanParl, +8.08 points on USLegis, and +3.77 points on UNtrade. These improvements demonstrate the effectiveness of our spatio-temporal encoding. Adding the Edge module to the naive encoding baseline yields further improvements: +7.25 points on CanParl and +1.57 points on Enron. However, it still falls short compared to the enhancements provided by the SLATE encoding.

Finally, the complete model, SLATE with the Edge module, achieves the highest AUC scores across all datasets: +9.39 points on CanParl and +10.58 points on USLegis. These substantial gains confirm that integrating our unified spatio-temporal encoding and the Edge module effectively captures intricate dynamics between nodes over time, resulting in superior performance.

   Method & CanParl & USLegis & Flights & Trade & UNNote & Contact & **Avg.** \\  JODIE  & 78.21 \(\) 0.23 & 82.85 \(\) 1.07 & 96.21 \(\) 1.42 & 69.62 \(\) 0.44 & 68.53 \(\) 0.95 & 96.66 \(\) 0.89 & 82.01 \(\) 0.83 \\ DyREP  & 73.35 \(\) 3.67 & 82.28 \(\) 0.32 & 95.95 \(\) 0.62 & 67.44 \(\) 0.83 & 67.18 \(\) 1.04 & 96.48 \(\) 0.14 & 80.45 \(\) 1.10 \\ TGAT  & 75.69 \(\) 0.78 & 75.84 \(\) 1.99 & 94.13 \(\) 0.71 & 64.01 \(\) 0.12 & 52.83 \(\) 1.12 & 96.95 \(\) 0.08 & 76.58 \(\) 0.71 \\ TGN  & 76.99 \(\) 1.80 & 83.34 \(\) 0.43 & 98.22 \(\) 0.13 & 69.10 \(\) 1.67 & 69.17 \(\) 1.65 & 97.45 \(\) 0.35 & 82.48 \(\) 1.17 \\ CAWN  & 75.70 \(\) 3.27 & 77.16 \(\) 0.39 & 98.45 \(\) 0.01 & 68.54 \(\) 0.18 & 53.09 \(\) 0.22 & 89.99 \(\) 0.34 & 77.16 \(\) 0.74 \\ EdgeBank  & 64.14 \(\) 0.00 & 62.57 \(\) 0.00 & 90.23 \(\) 0.00 & 66.75 \(\) 0.00 & 62.97 \(\) 0.00 & 94.34 \(\) 0.00 & 70.50 \(\) 0.00 \\ TCL  & 72.46 \(\) 3.23 & 76.27 \(\) 0.63 & 91.21 \(\) 0.02 & 64.72 \(\) 2.05 & 51.88 \(\) 0.36 & 94.15 \(\) 0.09 & 75.11 \(\) 0.73 \\ GraphMixer  & 83.17 \(\) 0.53 & 76.96 \(\) 0.79 & 91.13 \(\) 0.01 & 65.52 \(\) 0.51 & 52.46 \(\) 0.27 & 93.94 \(\) 0.02 & 77.20 \(\) 0.36 \\ DyGformer  & **97.76 \(\)** 0.41 & 77.90 \(\) 0.58 & 98.93 \(\) 0.01 & 70.20 \(\) 1.44 & 57.12 \(\) 0.62 & **98.53**\(\) 0.01 & 83.41 \(\) 0.51 \\ 
**SLATE** & 92.37 \(\) 0.51 & **95.80 \(\)** 0.11 & **99.07 \(\)** 0.41 & **96.73 \(\)** 0.29 & **99.94 \(\)** 0.05 & 98.12 \(\) 0.37 & **96.88**\(\) 0.26 \\   

Table 2: Comparison to CTDG models on discrete data using  protocol (AUC).

   Encoding & Edge Module & Enron & CanParl & USLegis & UNtrade \\  LapPE + sinus-based  & ✗ & 89.18 \(\) 0.33 & 82.98 \(\) 0.71 & 85.22 \(\) 0.24 & 90.24 \(\) 1.05 \\ SLATE & ✗ & 90.57 \(\) 0.27 & 89.45 \(\) 0.38 & 93.30 \(\) 0.29 & 94.01 \(\) 0.73 \\ LapPE  + sinus-based  & ✓ & 90.75 \(\) 0.08 & 90.23 \(\) 0.41 & 87.50 \(\) 0.50 & 90.56 \(\) 0.69 \\ 
**SLATE** & ✓ & **96.39 \(\)** 0.18 & **92.37 \(\)** 0.51 & **95.80 \(\)** 0.11 & **96.73 \(\)** 0.29 \\   

Table 3: Validation of different SLATE component. Results in AUC over 4 datasets.

**Critical role of supra-adjacency transformation.** Here, we demonstrate the importance of the transformation steps of the supra-adjacency matrix, as detailed in section 3.1, by removing isolated nodes, adding virtual nodes, and incorporating temporal connections (Figure 2). Table 4 presents the performance of SLATE with and without transformation (trsf) on four datasets. Without these critical transformations, there is a systematic drop in performance, particularly pronounced in datasets with a high number of isolated nodes, as shown in Figure 3 (27% in Colab, 53% in USLEgis, 35% in UNVote, and 59% in AS733). These results clearly highlight the significant improvements brought by our proposed transformations. More detailed experiments regarding each transformation, particularly on the importance of removing isolated nodes and adding a virtual node, are presented in Tables 12 and 13.

**Impact of the time-window size.** We demonstrate in Figure 4 the impact of the time window size on the performance of the SLATE model. A window size of 1 is equivalent to applying a global attention transformer to the latest snapshot before prediction, and an infinite window size is equivalent to considering all the snapshots for global attention. This figure highlights the importance of temporal context for accurate predictions within dynamic graphs. We observe that, in most cases, too much temporal context can introduce noise into the predictions. The USLEgis, UNVote and CanParl datasets are political graphs spanning decades (72 years for UNVote), making it unnecessary to look too far back. For all of our main results in Table 2 and Table 1 we fix for simplicity \(w=3\). However, our ablations have identified \(w=4\) as an optimal balance, capturing sufficient temporal context without introducing noise into the transformer encoder and ensuring scalability for our model. Therefore, SLATE performances could further be improved by more systematic cross-validation of its hyper-parameters, _e.g._\(w\).

**Model efficiency.** The classic attention mechanism, with a complexity of \(O(N^{2})\), can be memory-consuming when applied across all nodes at different time steps. However, using Flash-Attention  and a light transformer architecture with just one encoder layer, we successfully scaled to the Flights dataset, containing 13,000 nodes and a window size of \(w=3\). By using the Performer encoder , which approximates attention computation with linear complexity, memory usage is reduced to 17GB. Our analysis shows that our model empirically matches the memory consumption of various

   Dataset & SLATE w/o trsf & **SLATE** \\  Colab & \(85.03 0.72\) & \(\) \\ USLEgis & \(63.35 1.24\) & \(\) \\ UNVote & \(78.30 2.05\) & \(\) \\ AS733 & \(81.50 1.35\) & \(\) \\   

Table 4: Importance of connectivity transformations steps to connect the supra-adjacency matrix. AUC performance in dynamic link prediction.

Figure 3: Average percentage of isolated nodes per snapshot on real world dynamic graphs data.

   Models & Mem. & t/ ep. & Nb params. \\  EvolveGCN & 46Go & 1828s & 1.8 M \\ DySAT & 42Go & 1077s & 1.8 M \\ VGRNN & 21Go & 931s & \(\) M \\ ROLAND-GT w/o Flash & OOM & - & 1.9 M \\ ROLAND-GT & 44Go & 1152s & 1.9 M \\  SLATE w/o Flash & OOM & - & 2.1 M \\ SLATE & 48Go & 1354s & 2.1 M \\ SLATE-Performer & \(\) & \(\) & 2.1 M \\   

Table 5: An analysis of model efficiency comparing the memory usage (Mem.), training time per epoch (t/ep.) and the number of parameters (Nb params) on Flights datasetDTDG architectures while maintaining comparable computation times (Table 5). Furthermore, it is not over-parameterized relative to existing methods. We trained on an NVIDIA-Quadro RTX A6000 with 49 GB of total memory.

### Qualitative results

We present qualitative results in Figure 5 comparing the graph and its spectrum before and after applying the proposed transformation in SLATE. The projection is made on the eigenvector associated with the first non-zero eigenvalue. Before transformation, the DTDG contains isolated nodes (7, 23 and 26) and two distinct clusters in the snapshot at \(t=3\). In this case, the projection is purely spatial, as there are no temporal connections, and some projections also occur on isolated nodes due to the presence of distinct connected components. After the proposed transformation into a connected multi-layer graph, the projection captures richer spatio-temporal properties of the dynamic graph. By connecting the clusters with a virtual node and adding temporal edges, our approach removes the influence of isolated nodes and enables the construction of an informative spatio-temporal encoding that better reflects the dynamic nature of the graph.

## 5 Conclusion

We have presented the SLATE method, an innovative spatio-temporal encoding for transformers on dynamic graphs, based on supra-Laplacian analysis. Considering discrete-time dynamic graphs as multi-layer networks, we devise an extremely efficient unified spatio-temporal encoding thanks to the spectral properties of the supra-adjacency matrix. We integrate this encoding into a fully-connected transformer. By modeling pairwise relationships in a new edge representation module, we show how it enhances link prediction on dynamic graphs. SLATE performs better than previous state-of-the-art approaches on various standard benchmark datasets, setting new state-of-the-art results for discrete link prediction.

Despite its strong performances, SLATE currently operates in a transductive setting and cannot generalize to unseen nodes. We aim to explore combinations with MP-GNNs to leverage the strengths of local feature aggregation and global contextual information. On the other hand, SLATE scales reasonably well to graphs up to a certain size but, as is often the case with transformers, future work is required to scale to very large graphs.

Figure 5: Projection of the eigenvector associated with the first non-zero eigenvalue on a toy DTDG before and after transformation. On the _left_, the DTDG is unprocessed, showing only spatial projections due to the lack of temporal connections. On the _right_, after applying the SLATE transformation, the graph captures rich spatio-temporal properties, allowing for a more informative spatio-temporal encoding

Acknowledgement

We acknowledge the financial support provided by PEPR Sharp (ANR-23-PEIA-0008, ANR, FRANCE 2030). We would also like to thank the LITIS laboratory in Rouen and especially Leshanshui Yang, who helped us better position our method. We also thank Elias Ramzi for his feedback on the paper and assistance in writing the abstract.