# Model Editing as a Robust and Denoised variant of DPO: A Case Study on Toxicity

Rheeya Uppaal

Department of Computer Sciences

University of Wisconsin-Madison

uppaal@wisc.edu

&Apratim Dey

Department of Statistics

Stanford University

apd1995@stanford.edu

&Yiting He

Department of Probability and Statistics

University of Science and Technology of China

heyiting@mail.ustc.edu.cn

&Yiqiao Zhong

Department of Statistics

University of Wisconsin-Madison

yiqiao.zhong@wisc.edu

&Junjie Hu

Department of Computer Sciences and

Department of Biostatistics and Medical Informatics

University of Wisconsin-Madison

jhu@cs.wisc.edu

###### Abstract

Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data. However, these methods are both computationally intensive and lacking in controllability and transparency, inhibiting their widespread use. Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data. In this paper, we introduce a tuning-free alignment alternative, ProFS (Projection Filter for Subspaces), and demonstrate its effectiveness under the use case of toxicity reduction. Grounded on theory from factor analysis, ProFS is a sample-efficient model editing approach that identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The toxic subspace is identified by extracting preference data embeddings from the language model, and removing non-toxic information from these embeddings. We show that ProFS is more sample-efficient than DPO, further showcasing greater robustness to noisy data. Finally, we attempt to connect tuning based alignment with editing, by establishing both theoretical and empirical connections between ProFS and DPO, showing that ProFS can be interpreted as a denoised version of a single DPO step. Our code is available at https://github.com/Uppaal/detox-edit.

## 1 Introduction

The current landscape in NLP is defined by the widespread use of powerful generative large language models (LLMs) with generalist capabilities across domains and tasks. (1; 2; 3, _inter alia_). Their widespread use has shed light on their limitations--they are prone to hallucinations, biases, and generating harmful or toxic text (4; 5; 6; 7; 8, _inter alia_). Due to this, ensuring their reliability and safety has become paramount, and is an active area of research known as _alignment_.

The core idea of this is to make a language model match certain human preferred behaviors, like harmlessness, that are exemplified through preference data (9; 10; 11; 12, _inter alia_). Models are trained to learn these human preferences through algorithms like Proximal Policy Optimization (PPO) [(13)] or Direct Preference Optimazation (DPO) [(14)]. While promising in many ways [(15)], curating high-quality preference data and tuning large-scale models are expensive resource-intensive processes [(16; 17; 18)], making the process of alignment prohibitive from widespread use.

An alternate and emerging approach towards alignment has been through model editing [(19; 20; 21), _inter alia_), which attempts to achieve the results of fine-tuning without any gradient-based learning. This is done through performing controlled and targeted interventions on the weights or activations of a model, providing a higher degree of transparency. The Linear Representation Hypothesis [(22; 23; 24; 25; 26; 27)] introduces the idea that that various human-interpretable concepts are encoded in linear subspaces of model representations. Leveraging this insight, a vast class of model editing approaches attempt to "push" model activations in directions that encode desired concepts or behaviors. These directions are usually identified through training supervised probes [(28; 29)], or unsupervised decomposition of activations [(30; 31)] through singular value decomposition (SVD) [(32)]. Editing activations in this manner has been shown to successfully make models more truthful [(29)], moral [(31)] and unbiased [(30; 21)].

In this work, we propose a simple and straightforward approach to edit model weights. Similar to [(33)] and other editing literature which aligns to specific objectives [(28; 34, _inter alia_)], we focus on the use-case of toxicity. We introduce ProFS (Projection Filter for Subspaces) [(SS4)], which identifies toxic directions in model activations to define a low-dimensional toxicity subspace. ProFS then leverages this subspace as a projection filter on the weights, effectively removing these toxic directions from the model and mitigating the model's toxicity. Our method is based on the heuristic that an embedding vector in any layer of a transformer can be decomposed into interpretable components:

\[++\]

Drawing inspiration from classical literature in factor analysis, principal component analysis, and low-rank matrix estimation [(35; 36; 37)], our editing approach effectively decouples these three vector components to isolate and identify the toxic vector, after which it orthogonalizes the weights with respect to the toxic subspace spanned by these toxic vectors. This ensures that during inference, toxic outputs are suppressed. ProFS identifies the subspace associated with toxic factors by applying SVD to embedding differences, effectively canceling out common context factors [(SS5)].

In SS7, we empirically validate our method over various models. We demonstrate that our simple method is highly sample-efficient, requiring orders of magnitude lesser data than alignment algorithms like DPO, and making it more practical to use for real-world applications. Additionally, ProFS is notably robust to labeling noise, outperforming tuning-based alignment algorithms in this regard. This is of note for alignment tasks, where matching fuzzy preferences with substantial variation in opinions and annotations is a frequent challenge. Finally, we attempt to connect the two bodies of work for alignment - tuning and editing, by establishing both theoretical [(SS5)] and empirical [(SS8)] connections between ProFS and DPO, showing that our editing approach is conceptually similar to a _denoised_ version of a single DPO step.

Our work attempts to provide principled insights toward leveraging interpretable directions in activations for alignment through editing weights. We hope this enables an initial step towards a wider applicability of safe language models.

## 2 Related Work

Alignment through TrainingThe current standard for aligning models to user-defined preferences is through learning from human [(2; 9; 11, _inter alia_) or AI [(12; 16)] feedback via algorithms like PPO [(13)] or DPO [(14)]. However, these methods require curating high-quality preference data and tuning large-scale models that are expensive and resource-intensive [(16; 17; 18)], impeding the democratization of aligning models. Additionally, it is hard to determine if the model has successfully been aligned after training - some models have been shown to simply learn stylistic changes [(38)], or redirect activations to avoid toxic regions of the model [(33)], leading to easy un-alignment [(33; 39; 40)] and the possibility of jail-breaking by adversarial prompting [(41; 42; 43; 44; 45; 46; 47)] or fine-tuning [(48; 49)].

Alignment through EditingProviding a more transparent approach to alignment, model editing involves controlled and targeted interventions on the weights or activations of a model. The Linear Representation Hypothesis (22; 23; 25; 26; 27) posts that various human-interpretable concepts are encoded in linear subspaces of model representations. Building upon this, activations have been edited through steering or modifying them towards these subspaces, at inference time or through constrained fine-tuning, to develop models that are more truthful (29; 31), moral (31) and unbiased (28; 30; 51; 20; 52; 21). However, these methods often require additional operations at inference and model architecture changes (29); instead editing weights allows for plug-and-play replacements to the original models (53; 54).

These subspaces are typically identified through supervised probes (28; 29, _inter alia_) or unsupervised decompositions of activations or weights (30; 31; 33). Most related to our work, a recent study (55) isolated safety critical ranks in the weights of a model through SVD. While we also use low rank decompositions of weights to identify conceptual subspaces, our focus is on leveraging this to develop a noise robust and sample efficient approach to remove undesired model behaviours, basing this in factor analysis theory to draw connections to tuning based alignment.

Reducing Toxicity in Language Models Toxicity reduction methods can be largely categorized into three classes (34). Tuning based approaches (14; 56; 57; 58, _inter alia_) require large amounts of data and are computationally expensive to train. Decoding based approaches (59; 60; 61; 62, _inter alia_) often require trained classifiers, thus also needing vast data, and can be slow at inference. They have also been shown to reduce fluency in certain cases (63). Finally, editing approaches tuning-free, lightweight and computationally cheap. (34) perform two forward passes: one to identify toxic directions in the activations of attention heads, and one to edit the activations by steering them in this direction. They study the mechanism of _attention head activations_ in encoding toxicity; conversely, we focus on analysing the mechanisms of _MLP weights_, providing complementary findings to this work. We also theoretically motivate our method through factor analysis, and provide novel theoretical and empirical connections to tuning based alignment, showing that ProFS may function as a denoised version of a single DPO step.

## 3 Preliminaries

Identifying Concepts by Mapping to VocabularyTo understand what concepts a vector \(^{D}\) in the embedding space represents, a common approach (64) is to send it to the vocabulary space, using the output embedding matrix \(=[_{1},,_{||}]^{}^{| | D}\), where \(\) denotes the vocabulary. We compute a linear map to the vocabulary \(^{||}\) and then sort \(\) in ascending order, to find the top-\(k\) tokens that best describe the concepts encoded in \(\). This is because each output embedding vector \(_{j}\) gives a similarity score \(_{j}\) that measures how closely \(\) and \(_{j}\) are related.

Identifying and Interpreting Toxic SubspacesBuilding on previous studies that identify that certain directions in the activation space encode meaningful concepts, we identify a low-dimensional toxicity subspace in the MLP layers of GPT-2. We specifically work with the MLP layers since recent studies (33; 53; 65; 64, _inter alia_) have shown that MLP layers in language models encode meaningful static concepts,

The subspace is identified using preference data - matched toxic and non-toxic strings (Table 5, SSC). The difference between the activations of toxic and non-toxic data are computed, and its singular

    & **Top Tokens (Layer 14)** & **Interpretation** \\  \(\) &, and the - in ( ". & Frequent tokens, stopwords \\
1st svec & s**tt** f**e**k ucker b**n**th **s**t **F**e**k ** holes & Toxic tokens \\
2nd svec & damn really kinda stupid s**t** goddamn & Toxic tokens \\
3rd svec & disclaimer Opinion L**H** Statement Disclaimer Brief & Context dependent topics \\
4th svec & nation globalization paradigm continent empire oracy & Context dependent topics \\   

Table 1: Interpreting the top singular vectors of the difference of preference data embeddings. Using GPT-2 and 500 samples from RealToxicityPrompts, each singular vector of the matrix is interpreted by identifying the top-\(k\) tokens it represents. We use the output embedding vector \(_{j}\) to find top-scoring tokens \(j\) for maximizing \(_{i},_{j}\). Tokens have been censored for readability.

vectors \(_{1},_{2},\) are obtained through singular value decomposition (SVD). The top singular vectors are then inspected by mapping to the vocabulary. In Table 1, we list the top tokens that best explain the top few singular vectors. \(_{1},_{2}\) are mostly associated with toxic words, while \(_{3}\) and \(_{4}\) likely represent general topics such as news and politics. In addition, we calculate a global mean vector \(\), which is associated with frequent tokens and stop words, and is likely to represent corpus-wise frequency statistics. Our interpretations are consistent across different data samples (see SF).

## 4 ProFS: Editing Weights through Projections on Subspaces

Building on prior work showing that model activation spaces contain interpretable directions, Table 1 suggests that toxicity is encoded in a subspace separated from other directions that encode general non-toxic concepts (we call this the "context subspace"). To reduce model toxicity, ProFS attempts to identify this toxic subspace and project the model weights out of this subspace. Our approach is described below and summarized in Algorithm 1 ( SSA).

Formally, given a base model to edit, we assume access to a dataset of toxic and non-toxic sentence pairs \(_{}=\{(x_{i}^{+},x_{i}^{-})\}_{i=1}^{N}\). We compute the sentence embeddings of \(x_{i}^{+},x_{i}^{-}\), denoted as \(_{i,}^{+},_{i,}^{-}\) respectively at each layer of the language model, \(\,\{L_{0} L\}\) starting from layer \(L_{0}\), and omit the subscript \(\) when context allows (SS5). We stack all the sentence embeddings as \(_{}^{+},_{}^{-}\,^{N D}\). Following (30), we identify an approximation of the model's toxic subspace through the difference of these embeddings:

\[_{}^{0}_{}^{+}-_{}^{-}\.\]

A key observation suggested by our analysis in Table 1 is that this matrix, while encoding the toxic subspace of the model, also encodes general syntactical and semantic information that must be preserved through the editing process. As a result, we propose a simple three-step algorithm.

Step 1: Filtering Frequent Token Information through CenteringWe first compute the mean vector \((_{}^{-})\) by averaging across the non-toxic sentence embeddings. This reflects the general statistics of the corpus.1 Table 1 shows that \(\) likely represents information of stop words that are non-toxic and critical for the model. As a result, we avoid editing weights in the direction of \(\) by calculating a centered embedding difference matrix \(_{}\).

\[_{}_{}^{0}\ (-_{}), \ _{}^{}}{\|\|_{2}^{2}}.\] (1)

More simply, we project out the component in the direction of \(\), to ensure that our edit (Step 3) does not significantly change how the model uses non-toxic frequent tokens.

Figure 1: **Left**: Structure of embedding vectors. We posit that a set of singular vectors define the toxic subspace, which is separate from desired model capabilities (the context subspace and corpus mean direction). **Right**: The ProFS method. We edit the weights of MLP-Value layers through the identification of a projection filter representing the toxic subspace. The edit is performed once, following which the model functions as a drop-in replacement with no architectural modifications.

Step 2: Selecting Toxic DirectionsTo find the dominant directions of the toxic subspace, we apply SVD to \(_{}\) and pick the top-\(k\) right singular vectors as the most toxic directions. Subsequently, we define the toxic projection matrix as the sum of the outer product of the toxic singular vectors.

\[^{}=_{},_{}^{} _{i}^{k}_{i}_{i}^{}\] (2)

where \(_{1},_{2},,_{k}\) are the first \(k\) column vectors of \(\). Table 1 shows interpretations of the singular vectors of \(\) by mapping them to top similar words in the vocabulary.

Step 3: ProjectionAs the projection matrix \(^{}\) defines the toxic information to be removed from the model, we apply this projection to the original MLP-value2 weight matrices \(_{,K}^{}\), which are known to encode conceptual information in a model (64). Finally, the original weight is replaced with the edited weight \(_{,K}^{}\) in the language model for prediction.

\[_{,K}^{}(-_{}^{}) \ _{,K}^{}\.\] (3)

## 5 Theoretical Insights: How ProFS Identifies Toxic Subspaces

A Factor Analysis PerspectiveTable 1 suggests that the embedding space contains interpretable subspaces. As a result, we use factor analysis, a well-known technique for analyzing such structure. We posit that the sentence embeddings \(_{i}^{+},_{i}^{-}^{D}\) of a toxic and non-toxic data pair in any given layer (omitting subscript \(\)) follow the factorization:

\[_{i}^{+}&=&}_{}\ +\ _{i}}_{}\ +\ }}_{i}}_{}\ +\ _{i}^{+}}_{},\\ _{i}^{-}&=&a^{-}\] (4)

where \(a^{+},a^{-}\) are scalars of the corpus mean, \(^{D k}\) contains \(k\) "toxic" vectors as its columns, \(}^{D}\) contains \(\) context vectors as its columns and \(_{i}^{k},}_{i}^{}\) are "latent factors". The toxic subspace is the column space of \(\), and a linear combination of its column vectors \(_{i}\) represents the toxic information in \(_{i}^{+}\). We assume both toxic and non-toxic embeddings share a context component. Additionally, there is a noise term representing typical randomness unaccounted for by the statistical model.

Next, we show how ProFS recovers the latent toxic subspace. Recall that \(_{}=^{}/\|\|_{2}^{2}\). By taking the difference between \(_{i}^{+},_{i}^{-}\) and then projecting out the mean direction (that is, multiplying by \(-_{}\)), we have

\[(-_{})(_{i}^{+}-_{i}^{-})=(-_{})_{i}+(-_{})(_{i}^{+}-_{i}^{-}),\] (5)

where \((-_{})(a^{+}-a^{-})=\) since \(-_{}\) only keeps vectors orthogonal to \(\). Let \(_{i}:=(-_{})(_{i}^{+}-_{i}^{-})\) and \(^{*}:=(-_{})\). The linear span of \(^{*}\) represents the "centered" toxic subspace, namely the component of the toxic subspace after removing the corpus-mean component. When ProFS applies SVD to \(_{}\), we can rewrite \(_{}\) using \(^{*}\) as:

\[_{}=(^{*})^{}}_{}+ }_{}=[^{*}_{1}+_{1},, ^{*}_{N}+_{N}]^{}^{N D}\] (6)

where \(=[_{1},,_{N}]^{},\ \ =[_{1},,_{N}]^{}\). In the ideal situation \(=\) (no noise), the top-\(k\) singular vectors span exactly the same subspace of \(^{*}\), namely centered toxic subspace. Under nonzero \(\), SVD is also efficient since SVD gives the best low-rank approximation. Thus, our approach can be viewed as an approximate recovery of the latent subspace for toxic factors.

Denoising with SVDDue to the noise \(\), we can not recover the centered toxic subspace exactly. Since SVD gives the best low-rank approximation (66), generally we expect to recover the centered toxic subspace \((^{*})\) up to some errors. Quantitatively, the recovery error is controlled by the following upper bound where we compare two projection matrices: \(^{}\) from our method, and \(}}\) associated with the latent subspace.

\[\|^{}-}}\|_{}\|\|_{}}{_{k}((^{*})^{})}\] (7)

where \(\|\|_{}\) is the matrix operator norm, \(C_{k}\) is a constant, \(_{k}\) returns the \(k\)-th singular value of a matrix. Note that the quality of recovering toxic subspace improves as the magnitude of \(\) and \(^{*}\) increases, which generally happens with a large \(N\) and \(D\). See SSA.4 for further details.

Connection to DPODPO (14) is a gradient-based alignment method which is generally nonlinear. To establish a conceptual connection, consider a simple logistic model (\(_{}\)) that links hidden states \(_{i}^{+},_{i}^{-}\) directly to outputs (next-predicted token \(y_{i}\)): the conditional probability is given by

\[_{}(y|_{i}^{+})=Z_{}^{-1}(_{y}^{}_{i}^{+})\] (8)

where \(_{y}\) is the output embedding vector for any token \(y\), and \(Z_{}\) is the normalization factor. A similar expression holds if we replace \(_{i}^{+}\) by \(_{i}^{-}\). Some calculation shows that the gradient with respect to \(\) of the DPO loss with one training step is determined by (for a temperature hyperparameter \(>0\)),

\[_{}_{}|_{_{}=_{}}=- _{i=1}^{N}(_{y_{i}^{+}}(_{i}^{+})^{}- _{y_{i}^{-}}(_{i}^{-})^{})\,.\] (9)

Thus, DPO also finds the toxic subspace approximately by using a variant of embedding differences. Under the factor model assumption in Eq. 4, each row vector behaves as a noise-corrupted vector in the linear span of \(\) and \(\), so a large \(N\) helps the gradients to "average out" noise due to random sampling. However, it is less sample efficient because SVD directly extracts the low-rank subspace instead of averaging. See SSB for further details.

## 6 Experimental Setup

ModelsOur main experiments use GPT-2 medium (355M) (67). Additionally, we use Mistral (7B) (68), its SFT variant Mistral-SFT (69; 70), OPT (6.7B) (71) and GPT-J (6B) (72).

Preference DataWe use the pairwise toxic data created by (33). The non-toxic sequences are extracted from Wikitext-2 (73), and their toxic counterparts are generated using PPLM (59). Examples from the dataset can be found in Table 5 (SSC).

Editing HyperparametersProFS involves two hyperparameters: the top-\(k\) right singular vectors used to construct the toxic projection matrix \(_{}^{}\), and the layer index to start the edit at \(L_{0}\). We use ScreeNot (36) to find an initial estimate for \(k\), and then find an optimal value through cross-validation (SA.1). For GPT-2, \(k=2\) and for all other models \(k=10\). We examine the selection of \(L_{0}\) in SS7, and set \(L_{0}=11\) GPT-2 and GPT-J, \(L_{0}=15\) for all other models.

EvaluationFollowing (33), the toxicity of a model is measured by prompting it with the challenge subset of RealToxicityPrompts (5), which triggers toxic outputs from the language models. We then score the continuations from the model using Detoxify (74), where a higher score indicates a more toxic generation. To ensure the desired model capabilities are not impacted by editing, we measure the perplexity of the model on the dev split of WikiText-2 (73). Additionally, for larger language models with zero-shot prediction capabilities, we follow (55) and measure the averaged zero-shot capability of the model across seven tasks from EleutherAI LM Harness (75): BoolQ (76), RTE (77), HellaSwag (78), WinoGrande (79), ARC Easy and Challenge (80), and OpenbookQA (81). We report the mean and standard deviation of our results over three runs, randomly sampling data.

Comparisons with Tuning-based Alignment: DPOWe use the implementation of (33) to train models on the pairwise toxic data using DPO. We use their default hyperparameters and set \(\) to 0.1. For the larger models, we use LoRA (82) on each layer, with a rank of 64, a scaling parameter of 16 and a dropout of 0.1. We use early stopping, i.e., training until the validation loss converges with a patience value of 10.

## 7 Editing with ProFS is a Robust and Sample Efficient Replacement to DPO

We empirically evaluate our hypothesis by measuring the reduction in toxicity through ProFS relative to DPO. In Table 2, we use 500 datapoints for ProFS and 2,000 datapoints for DPO. Despite this difference in data exposure, ProFS is almost always more effective in reducing toxicity, while still retaining model capability. We further highlight the sample efficiency of ProFS in Figure 2 (Table 9 in SSE). With no significant detriment to perplexity, the edit approach can reduce toxicity in as little as 5 datapoints, and make significant toxicity reductions with 50 datapoints. In contrast, DPO needs orders of magnitude more data to achieve similar performance. Additionally, in Figure 7 ( SSE), we see that ProFS suppresses the probability of toxic words, relative to the base model (GPT-2).

Editing over Subspaces Elicits Robustness to Labeling NoiseLabeling errors when curating data is a pervasive issue towards developing robust models [(83; 84; 85)]. In the setting of toxicity, training on poorly labeled data could result in a _more_ toxic model. We test the robustness of ProFS to this, by flipping the labels of a fraction of the dataset. Figure 3 shows that the editing approach, unlike DPO, is almost entirely unaffected by labeling noise, even when half the dataset is incorrectly labeled. This is because the singular vectors of \(_{}\) are equivalent to the eigenvectors of Gram matrix \(_{}^{}_{}\), and flipping the sign of any row vector in \(_{}\) does not change \(_{}^{}_{}\) at all (see derivation in SSA.3).

ProFS shows similar gains on Alignment to Multiple PreferencesAlignment algorithms like DPO are generally used to align to a broad spectrum of preferences simultaneously. While we focus on the setting of toxicity for effective analysis, we now show that ProFS functions similarly well over a range of preferences. Following [(14; 86)], we measure the win rate of the responses generated by the edited model over the original, as judged by GPT-4o mini [(15)]. 3 Table 3 shows the results of aligning

  
**Model** &  &  &  &  &  \\  Method & Orig DPO & ProFS & Orig DPO & ProFS & Orig DPO & ProFS & Orig DPO & ProFS & Orig DPO & ProFS & Orig DPO & ProFS \\   & 48.00 & 36.36 & **26.83** & 42.45 & 36.42 & **30.40** & 33.45 & **23.96** & 26.03 & 46.47 & 45.31 & **43.49** & 45.31 & 43.67 & **37.36** \\  & (0.00) & (0.58) & (0.89) & (0.00) & (0.62) & (0.71) & (0.00) & (0.50) & (1.25) & (0.00) & (0.74) & (1.38) & (0.00) & (1.11) & (2.28) \\  & 29.70 & 29.86 & 32.50 & 7.49 & 7.52 & 7.99 & 8.22 & 8.38 & 8.83 & 14.67 & 14.37 & 13.83 & 13.24 & 13.96 & 14.53 \\  & (0.00) & (0.22) & (0.28) & (0.00) & (0.26) & (0.21) & (0.00) & (0.34) & (0.57) & (0.00) & (0.61) & (0.46) & (0.00) & (0.53) & (0.30) \\ 
**Capability** & - & - & - & 64.23 & 65.32 & 63.59 & 63.59 & 63.66 & 63.23 & 51.57 & 51.55 & 51.80 & 51.92 & 52.46 & 52.48 \\   

Table 2: Comparison of ProFS with DPO. We use \(N=500\) for ProFS and \(N=2000\) for DPO. Despite this, both approaches are comparable in their toxicity reduction, highlighting the sample efficiency of the editing approach. Resulted are averaged over three splits of randomly sampled data.

ProFS and DPO to 500 samples of the HH-Golden dataset  (a modified version of HH-RLHF  where the preferred responses are replaced with high quality data from GPT-4). ProFS shows a higher win rate, demonstrating its effectiveness as a sample efficient alignment alternative. More details in SSG.

Centering is Crucial to Retaining Model CapabilityEach direction in the model embeddings \(_{}\) encodes different information, and our method aims to apply edits along the directions that purely encode toxic information. Directions that may partially or totally encode desired knowledge (for example, the context subspace in Figure 1), if included in the edit, can significantly harm model capability. This effect can be seen starkly with the corpus-wide mean \(\), which is a direction that encodes basic syntactic knowledge like stop words and frequent tokens (Table 1). This phenomenon is illustrated in Table 4 with GPT-2, using 500 datapoints for editing. Including the corpus mean direction in the edit breaks the model, as evidenced by the model's high perplexity and nonsensical generations.

## 8 Connections between ProFS and DPO

ProFS Functions as a Denoised Approximation to DPOWe examine the question: _Do DPO gradients move the weights in a similar direction as our projection does?_ To answer this question, we calculate the DPO gradients \(\) (at the first training step) with respect to the MLP-value matrix under a varying number of pairwise samples. We then examine the correlation between these DPO gradients and the toxic subspace identified through ProFS. The correlation is defined as the ratio of gradients explained by the toxic subspace, namely \(\|^{}\|_{F}/\|\|_{F}\) where \(\|\|_{F}\) is the Frobenius norm. Figure 4 shows that DPO gradients and \(^{}\) are substantially correlated; for comparison, we include a baseline that shows how much \(^{}\) explains a random matrix (averaged across 10 independent draws). Further, we find that (1) correlation in later layers is stronger (further justifying the application of the edit on higher layers only), and (2) DPO gradients are explained more with larger sample size. The latter point is consistent with our theoretical insights that DPO needs large samples to "average out" noise.

DPO and ProFS show similar Incremental Layer-wise ContributionGiven \(L\{11,12,,24\}\), we are interested in how editing layer \(11\) through \(L\) changes token predictions. We measure the change of token prediction probabilities by applying edits to layer from \(11\) to \(L\) while freezing other layers. In Figure 5, we select tokens with most positive/negative changes and plot probability changes against \(L\). We find that ProFS and DPO at full scale exhibit similar patterns: (1) toxic tokens are suppressed after alignment/edit while frequent tokens receive a boost; (2) each subsequent layer contributes incrementally to toxicity reduction, though in ProFS effects are stronger

  
**Approach** & **Toxicity (\%)** & **Perplexity** & **Generations** \\   &  &  & The quick brown fox jumps over the lazy dog. \\  & & & Holy School of Medicine, University of California \\  & & & Bloody Cross, the first of the three novels, was \\   &  &  & The quick brown fox urchin (Phacronites alb \\  & & & Holy sh*t, Virginia, June 1, 2017: U \\   & & & Bloody Sunday”,"c0”,"c0”,"c0 \\   

Table 4: Impact of centering the preference matrix on edit performance. Skipping the centering, or retaining the corpus mean \(\) from in the edited knowledge removes basic syntactic knowledge from the model, essentially resulting in nonsensical generations. We use \(N=500\) for editing GPT-2. The generations from the model are shown in blue or red. Toxic words have been censored for readability.

at later layers; (3) moreover, effects of individual layers are nearly _additive_--the combined changes of editing individual layers are nearly the same as editing these layers simultaneously (Appendix F).

## Limitations

In this work, we introduce ProFS: an interpretable, sample-efficient, and fast weight editing approach for reducing model toxicity. ProFS identifies toxic directions in model activations to define a low-dimensional toxicity subspace. ProFS then leverages this subspace as a projection filter on the weights, effectively removing these toxic directions from the model and mitigating the model's toxicity. Notably, ProFS is highly robust to label noise in a task which is based on fuzzy concepts and has substantial variations in annotations and opinions. We attempt to connect the two bodies of work for alignment - based on training and editing, to encourage further developments in editing. For this, we provide theoretical insights into how ProFS identifies a toxic subspace from a factor analysis perspective and show empirical and theoretical evidence showing that our editing approach is conceptually similar to a _denoised_ version of a single DPO step.

ProFS is a powerful sample-efficient alternative to DPO, also showcasing a greater robustness to label noise. However, we note that editing approaches that identify subspaces through unsupervised decomposition of activations are highly sensitive to the selection of singular vectors. Poor selections can result in the desired capabilities of the model being drastically impacted (55). Additionally, our analysis and method focus solely on the MLP layers of the transformer language model. Further explorations into self-attention may help develop more principled and robust edit approaches. We defer this to future work.

Our work attempts to provide principled insights toward leveraging interpretable directions in activations for alignment through editing weights. We hope this enables an initial step toward a wider applicability of modern language models.

## Ethical Considerations

Our primary objective is to enhance the safe utility of Large Language Models (LLMs) by reducing the potential harm caused by their outputs. By prioritizing the development of mechanisms to curtail toxicity, we aim to contribute to a more responsible and ethical deployment of LLMs in various applications, thereby safeguarding against the propagation of harmful content and promoting the creation of safer digital environments.

Our study does not involve any human subjects or violation of legal compliance. We do not anticipate any potentially harmful consequences to our work. As detailed in SSC, all of our experiments are conducted using publicly available datasets. Our code shall be released for reproducibility. Through our study and releasing our code, we hope to raise stronger research and societal awareness towards building safe and robust language models.

Figure 5: Contribution of Layer \(11\) through \(L\) of alignment models. **Left**: Replacing a base GPT2-medium model with DPO trained at full scaled only for layers \(11\)—\(L\). Probability changes of significantly impacted tokens are plotted against \(L\). **Right**: Apply ProFS only to layers \(11\)—\(L\).