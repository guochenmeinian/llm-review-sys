# Domain Adaptation for Large-Vocabulary

Object Detectors

 Kai Jiang\({}^{1,}\), Jiaxing Huang\({}^{2,}\), Weiying Xie\({}^{1}\), Jie Lei\({}^{3}\), Yunsong Li\({}^{1}\), Ling Shao\({}^{4}\), Shijian Lu\({}^{2,*}\)

\({}^{1}\)State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an 710071, China

\({}^{2}\)S-lab, School of Computer Science and Engineering, Nanyang Technological University

\({}^{3}\)School of Electrical and Data Engineering at the University of Technology Sydney

\({}^{4}\)UCAS-Terminus AI Lab, University of Chinese Academy of Sciences, China

###### Abstract

Large-vocabulary object detectors (LVDs) aim to detect objects of many categories, which learn super objectness features and can locate objects accurately while applied to various downstream data. However, LVDs often struggle in recognizing the located objects due to domain discrepancy in data distribution and object vocabulary. At the other end, recent vision-language foundation models such as CLIP demonstrate superior open-vocabulary recognition capability. This paper presents KGD, a Knowledge Graph Distillation technique that exploits the implicit knowledge graphs (KG) in CLIP for effectively adapting LVDs to various downstream domains. KGD consists of two consecutive stages: 1) KG extraction that employs CLIP to encode downstream domain data as nodes and their feature distances as edges, constructing KG that inherits the rich semantic relations in CLIP explicitly; and 2) KG encapsulation that transfers the extracted KG into LVDs to enable accurate cross-domain object classification. In addition, KGD can extract both visual and textual KG independently, providing complementary vision and language knowledge for object localization and object classification in detection tasks over various downstream domains. Experiments over multiple widely adopted detection benchmarks show that KGD outperforms the state-of-the-art consistently by large margins.

## 1 Introduction

Object detection aims to locate and classify objects in images, which conveys critical information about "what and where objects are" in scenes. It is very important in various visual perception tasks in autonomous driving, visual surveillance, object tracking, etc. Unlike traditional object detection, large-vocabulary object detection  aims to detect objects of a much larger number of categories, e.g., 20k object categories in . It has achieved very impressive progress recently thanks to the availability of large-scale training data. On the other hand, large-vocabulary object detectors (LVDs) often struggle while applied to various downstream tasks as their training data often have different distributions and vocabularies as compared with the downstream data, i.e., due to domain discrepancies.

In this work, we study unsupervised domain adaptation of LVDs, i.e., how to adapt LVDs towards various downstream tasks with abundant unlabelled data available. Specifically, we observe that LVDs learn superb generalizable objectness knowledge from massive object boxes, being able to locate objects in various downstream images accurately . However, LVDs often fail to classifythe located object due to two major factors: 1) the classic dataset-specific class-imbalance and the resultant distribution bias across domains; and 2) different vocabularies across domains . At the other end, vision-language models (VLMs)  such as CLIP  learn from web-scale images and text of arbitrary categories, which achieve significant generalization performance in various downstream tasks with severe domain shifts. Hence, effective adaptation of LVDs towards various unlabelled downstream domains could be facilitated by combining the superior object localization capability from LVDs and the super-rich object classification knowledge from CLIP.

We design Knowledge Graph Distillation (KGD) that explicitly retrieves the classification knowledge of CLIP to adapt LVDs while handling various unlabelled downstream domains. KGD works with one underling hypothesis, i.e., the generalizable classification ability of CLIP largely comes from its comprehensive knowledge graph learnt over billions of image-text pairs, which enables it to classify objects of various categories accurately. In addition, the knowledge graph in CLIP is implicitly encoded in its learnt parameters which can be exploited in two steps: 1) Knowledge Graph Extraction (KGExtract) that employs CLIP to encode downstream data as nodes and computes their feature distances as edges, constructing an explicit CLIP knowledge graph that captures inherent semantic relations as learnt from web-scale image-text pairs; and 2) Knowledge Graph Encapsulation (KGEncap) that encapsulates the extracted knowledge graph into object detectors to enable accurate object classification by leveraging relevant nodes in the CLIP knowledge graph.

The proposed KGD allow multi-modal knowledge distillation including Language Knowledge Graph Distillation (KGD-L) and Vision Knowledge Graph Distillation (KDG-V). Specifically, KGD-L considers texts as nodes and the distances among text embeddings as edges, enabling detectors to reason whether a visual object matches a text by leveraging other relevant text nodes. KGD-V takes a category of images as a node and the distances among image embeddings as edges, which enhances detection by conditioning on other related visual nodes. Hence, KGD-L and KGD-V complement each other by providing orthogonal knowledge from language and vision perspectives. In this way, KGD allows to explicitly distill generalizable knowledge from CLIP to facilitate unsupervised adaptation of large-vocabulary object detectors towards distinctive downstream datasets.

In summary, the major contributions of this work are threefold. _First_, we propose a knowledge transfer framework that exploits CLIP for effective adaptation of large-vocabulary object detectors towards various unlabelled downstream data. To the best of our knowledge, this is the first work that studies distilling CLIP knowledge graphs for the object detection task. _Second_, we design novel knowledge graph distillation techniques that extracts visual and textual knowledge graphs from CLIP and encapsulates them into object detection networks successfully. _Third_, extensive experiments show

Figure 1: A comparison of the domain adaptation performance of our method against existing methods. Our method outperforms the state-of-the-art consistently on 11 widely studied downstream detection datasets in terms of AP50 improvements. The results of all methods are acquired with the same baseline .

that KGD outperforms the state-of-the-art consistently across 11 widely studied detection datasets as shown in Fig. 1.

## 2 Related works

**Large-vocabulary Object Detection**[8; 9; 10; 11; 12; 13; 14] aims to detect objects of thousands of classes. Most existing studies tackle this challenge by designing various class-balanced loss functions  for effective learning from large-vocabulary training data and handling the long-tail distribution problem [15; 16; 17; 18]. Specifically, several losses have been proposed, such as Equalization losses [19; 20], SeeSaw loss , and Federated loss . On the other hand,  and Detic  attempt to introduce additional image-level datasets with large-scale fine-grained classes for training large-vocabulary object detector (LVD), aiming to expand the detector vocabulary to tens of thousands of categories. These LVDs learn superb generalizable objectness knowledge from object boxes of massive categories and are able to locate objects in various downstream images accurately . However, they often fail to classify the located objects [4; 5] accurately. In this work, we focus on adapting LVDs towards various unlabelled downstream data by utilizing the super-rich object classification knowledge from CLIP.

**Domain Adaptation** aims to adapt source-trained models towards various target domains. Previous work largely focuses on unsupervised domain adaptation (UDA), which minimizes the domain discrepancy by discrepancy minimization [24; 25], adversarial training [26; 24; 27; 28; 29; 30], self-supervised learning [31; 32; 33; 34], or self-training [35; 36; 37; 38; 39; 40; 41; 42; 43]. Recently, source-free domain adaptation (SFDA) generates pseudo labels for target data without accessing source data, which performs domain adaptation with entropy minimization , self training [45; 46; 47; 48; 49], contrastive learning [50; 51; 52; 53], etc. However, most existing domain adaptation methods struggle while adapting LVDs toward downstream domains, largely due to the low-quality pseudo labels resulting from the discrepancy in both data distributions and object vocabulary.

**Vision-Language Models (VLMs)** have achieved great success in various vision tasks . They are usually pretrained on web-crawled text-image pairs with a contrastive learning objective. Representative methods such as CLIP  and ALIGN  have demonstrated very impressive generalization performance in many downstream vision tasks. Following [7; 54], several studies [54; 55; 56; 57] incorporate cross-attention layers and self-supervised objectives for better cross-modality modelling of noisy data. In addition, several studies [58; 59; 60; 61] learn fine-grained and structural alignment and relations between image and text. In this work, we aim to leverage the generalizable knowledge learnt by VLMs to help adapt LVDs while handling various unlabelled downstream data.

**Knowledge Graph (KG)** is a semantic network that considers real-world entities or concepts as nodes and treats the semantic relations among them as edges. Multi-modal knowledge graph [63; 64] extends knowledge from text to the visual domain, enhancing machines' ability to describe and comprehend the real world. These KGs have proven great effectiveness in storing and representing factual knowledge, leading to successful applications in various fields such as entity recognition [65; 66], question-answering , and information retrieval . Different from the aforementioned KGs and MMKGs that are often handcrafted by domain experts, we design knowledge graph distillation that builds a LKG and a VKG by explicitly retrieving VLM's generalizable knowledge learnt from web-scale image-text pairs, which effectively uncover the semantic relations across various textual and visual concepts in different downstream tasks, ultimately benefiting the adaptation of LVDs.

## 3 Method

**Task Definition.** This paper focuses on unsupervised adaptation of large-vocabulary object detectors (LVDs). We are provided with a set of unlabeled downstream domain data \(_{t}=\{_{i}^{t}\}_{i=1}^{N_{t}}\) and an LVD pre-trained on labeled source domain detection dataset \(_{s}=\{_{i}^{s},_{i}^{s}\}_{i=1}^{N_{s}}\). \(_{i}\) and \(_{i}=\{(_{j},_{j})\}_{j=1}^{M}\) are the image and \(M\) instance annotations of \(i\)-th sample, where \(_{j}\) and \(_{j}\) denote the ground-truth category and box coordinate of \(j\)-th instance. \(N_{s}\) and \(N_{t}\) refer to the number of samples in \(_{s}\) and \(_{t}\). The goal is to adapt the pretrained LVD towards the downstream domain \(_{t}\) by using the unlabelled images.

**Naive Solution with Mean Teacher Method (MT) .** In this paper, we adopt Detic  as the pretrained LVD, which utilizes CLIP text embeddings as the classifier. We employ mean teacher  as the preliminary solution, which involves a teacher detector and a student detector where the former generates pseudo labels to train the latter while the latter updates the former in a momentum manner. Given a batch of \(B\) unlabeled target samples, the teacher detector \(_{t}\) first produces detection predictions on them, which are then filtered with a predefined threshold \(\) to generate detection pseudo label \(}_{i}\) (consisting of classes and bounding boxes). With \(}_{i}\), the unsupervised training of student detector \(_{s}\) on the unlabeled downstream data can be formulated as the following:

\[Loss=_{i=1}^{B}(_{s}(_{i}^{t}), }_{i}),\] (1)

where \(()=_{rpn}()+_{reg}()+_{cls}()\) is the detection loss function in which \(_{rpn}()\), \(_{reg}()\), and \(_{cls}()\) denote the loss for region proposal network, regression, and classification, respectively. Note both teacher detector \(_{t}\) and student detector \(_{s}\) are initialized with the pretrained LVD.

**Motivation.** On the other hand, although the LVD is able to locate objects in various downstream-domain images accurately , it often fails to classify the located objects, leading to very noisy detection pseudo labels when serving as the teacher detector. At the other end, vision-language models (VLMs)  such as CLIP  learns from web-scale images-text pairs of arbitrary categories, which possesses the ability to classify objects accurately in various downstream data. Thus, we argue that effective adaptation of LVDs towards various unlabelled downstream data could be facilitated by combining the superior object localization capability from LVDs and the super-rich object classification knowledge from CLIP. To this end, we design Knowledge Graph Distillation (KGD) with Language KGD and Vision KGD, aiming to explicitly retrieves the classification knowledge of CLIP to adapt LVDs while handling various unlabelled downstream data. The overview of our proposed KGD is shown in Fig. 2.

Figure 2: Overview of the proposed Knowledge Graph Distillation (KGD). KGD comprises two consecutive stages including Knowledge Graph Extraction (KGE Extract) and Knowledge Graph Encapsulation (KGEncap). KGExtract employs CLIP to encode downstream data as nodes and considers their feature distances as edges, explicitly constructing KGs including language knowledge graph (LKG) and vision knowledge graph (VKG) that inherit the rich semantic relations in CLIP. The dashed reddish lines between LKG and VKG represent the cross-modal edges that connect the nodes between vision and language modalities, enabling the integration of both language and visual information. KGEncap transfers the extracted KGs into the large-vocabulary object detector to enable accurate object classification over downstream data. Besides, KGD works for both image and text data and allow extracting and transferring vision KG (VKG) and language KG (LKG), providing complementary knowledge for adapting large-vocabulary object detectors for handling various unlabelled downstream domains.

### Language knowledge graph distillation

The proposed language knowledge graph distillation (KGD-L) aims on distilling knowledge graph from the perspective of text modality. KGD-L works in a two-step manner. The first step is language knowledge graph (LKG) extraction with a large lexical database named WordNet  that aims to uncover the implicitly encoded language knowledge in CLIP. With the guidance from the WordNet that stores a wide range of knowledge, LKG Extraction builds a category-discriminative and domain-generalizable LKG. The second step is LKG encapsulation that encapsulates the extracted LKG into the teacher detector, enabling the detector to reason whether a visual object matches a text by leveraging other relevant text nodes and ultimately generate more accurate detection pseudo labels.

**LKG Extraction with WordNet Hierarchy.** We first generate domain-generalizable  prompts for each object category by leveraging the large lexical database WordNet . Specifically, given the category set \(=\{_{i}|i=1,N_{c}\}\) of a downstream domain, we obtain the WordNet  Synset definition as well as the hyponym set of category \(_{i}\) as follows:

\[_{i},_{i}=(_{i}),\] (2)

where \(()\) retrieves the WordNet database  and returns the definition \(_{i}\) as well as the hyponym set \(_{i}\) of its input. \(_{i}=\{_{j}\}_{j=1}^{m}\), where \(_{j}\) refers to the \(j\)th hyponym of category \(_{i}\) and \(m\) refers to the cardinal number of \(_{i}\). Note that hyponym \(_{j}\) is the concatenation of the class name and its descriptions. In this way, a category name \(_{i}\) can be better defined and described with the informative yet accurate category definition in its hyponym set from WordNet, which are then combined with \(}_{i}\) as a set of domain generalizable prompts for category \(_{i}\):

\[}_{i}=_{i}\{_{i}\},\] (3)

and the domain generalizable prompt set of category set \(\) can be constructed as the following:

\[}=_{i=1}^{N_{c}}}_{i}.\] (4)

With the category-discriminative and domain-generalizable information contained in \(}\), we formulate the proposed LKG as a weighted undirected graph \(G_{L}=(V_{L},U_{L},)\), which is capable of capturing semantic relationships and associations between different category concepts. \(V_{L}=\{}_{i}\}_{i=1}^{N_{c}(m+1)}\) is the vertex set in which each node \(}_{i}\) refers to a description in \(}\). And \(U_{L}=\{(}_{i},}_{j})\}\) is the edge set. \(\) is a matrix of node feature vectors \(_{i}=T(}_{i})\), where \(T()\) denotes the CLIP text encoder.

**LKG Encapsulation** encapsulates the comprehensive knowledge in the extracted LKG into the teacher detector to facilitate detection pseudo label generation. Specifically, we first employ CLIP to encode the regions cropped by the teacher detector and then generate pseudo labels for each region feature conditioned on LKG. Given the image \(^{t}_{t}\), we feed it into the teacher detector \(_{t}\) to acquire the prediction as the following:

\[}=_{t}(^{t}),\] (5)

where \(}=\{(}_{j},}_{j})\}_{j=1}^{M}\), \(}_{j}\) denotes the probability vector of the predicted bounding box \(}_{j}\) after Softmax activation function. \(M\) denotes the number of predicted proposals after the thresholding with \(\), i.e., a predicted proposal will be discarded if its confidence score is less than \(\).

Next, we employ CLIP to encode the predicted object proposals in \(}\) as follows:

\[F=V(Crop(^{t},})),\] (6)

where \(Crop()\) crops square regions from image \(^{t}\) based on the longer edges of bounding boxes in \(}\), \(V()\) is the image encoder of CLIP, and the \(j\)-th vector \(_{j}\) of matrix \(F\) is the feature of \(j\)-th proposal in \(}\).

With the extracted LKG \(G_{L}\) and the features of objects (or object proposals) \(F\), we reason the class of objects conditioned on \(G_{L}\) with a two-layer graph convolutional network (GCN)  as follows:

\[[Q^{F};Q^{}]=(D^{-}AD^{-}(D^{-}AD^{-}H^{0}W^{0})W^{1}),\] (7)

where \(H^{0}=[F;]\), \(A_{ij}=exp(-||H^{0}_{i}-H^{0}_{j}||_{2}^{2}/(||H^{0}_{i}-H^{0}_{j}||_{ 2}^{2}))\), \(A_{ii}=1\), and \(D_{ii}=_{j}A_{ij}\). \(Q^{F}_{ji}/Q^{0}_{ji}\) is the \(i\)-th element in probability vector \(Q^{F}_{j}/Q^{0}_{j}\), which denotes the predicted categoryprobability of being \(_{i}\) for object feature \(_{j}\)/LKG node \(}_{j}\). \(\{W^{l}\}_{l=0}^{1}\) are the trainable weights. For updating \(\{W^{l}\}_{l=0}^{1}\), we minimizing the following cross entropy error over the nodes in LKG:

\[_{LKG}(^{t})=-_{i}_{j}(log(O_{ji}^{}) (}_{j}_{i})).\] (8)

Then we encapsulate the extracted LKG into \(_{t}\) by,

\[_{ji}^{l}=}_{ji} Q_{ij}^{F},\] (9)

where \(}_{ji}\) is the \(i\)-th element in probability vector \(}_{j}\), which denotes the predicted category probability of \(_{i}\). The first term in Eq. 9 denotes the original prediction probability from the teacher model while the second term in Eq. 9 stands for the prediction probability from LKG. \(_{ji}^{l}\) denotes the prediction probability calibrated by LKG.

In this way, KGD-L extracts and encapsulates LKG from CLIP into the teacher detector, enabling it to reason whether an object matches a category conditioned on the relevant nodes in LKG and ultimately refining the original detection pseudo labels.

### Vision knowledge graph distillation

As LKG captures language knowledge only, we further design vision knowledge graph distillation (KGD-V) that extracts a vision knowledge graph (VKG) and encapsulates it into the teacher detector to improve pseudo label generation. Specifically, VKG captures vision knowledge dynamically along the training process, which complement LKG by providing orthogonal and update-to-date vision information.

**Dynamic VKG Extraction.** We first initialize VKG with the CLIP text embedding and then employ the update-to-date object features to update it using manifold smoothing. Specifically, we initialize VKG as a weighted undirected graph \(G_{V}=(V_{V},U_{V})\), in which each node \(_{i} V_{V}\) is initialized with the CLIP text embedding of category \(_{i}\):

\[_{i}=T(_{i}),\] (10)

and the graph edge \(u_{ij} U_{V}\) is defined as the cosine similarity between nodes \(_{i}\) and \(_{j}\). Given a batch of \(\{_{b}^{t}\}_{b=1}^{B}_{t}\) and the corresponding pseudo labels \(\{}_{b}\}_{b=1}^{B}\) and CLIP features \(\{F_{b}\}_{b=1}^{B}\), the visual embedding centroid of category \(_{k}\) can be obtained as the following:

\[_{i}=^{B}_{_{j} _{b}}_{j}(}_{j}(i)==}_ {j}^{max})}{_{b=1}^{B}_{_{j}_{b}}( }_{j}(i)==}_{j}^{max})},\] (11)

where \(}_{j}^{max}\) is the maximum element in probability vector \(}_{j}\), \(\) is the indicator function. And an affinity matrix \(A\) can be calculated as \(A_{ij}=exp(-r_{ij}^{2}/^{2})\) and \(A_{ii}=0\), where \(r_{ij}=||_{i}-_{j}||_{2}\) and \(^{2}=(r_{ij}^{2})\). In each iteration, the node of VKG is prelimimarily updated as:

\[_{i}_{i}+(1-)_ {i}.\] (12)

In order to incorporate the downstream visual graph knowledge into VKG, we perform additional steps to smooth the node of VKG, using the affinity matrix \(A\) from the current batch as a guide:

\[_{i}=_{j}W_{ij}_{j},\] (13)

where \(W=(I- L)^{-1}\), \(L=D^{-}AD^{-}\), \(D_{ii}=_{j}A_{ij}\), \(\) is a scaling factor set as , and \(I\) is the identity matrix.

**VKG Encapsulation** encapsulate the orthogonal and update-to-date vision knowledge in the extracted VKG into the teacher detector, which complements LKG and further improves pseudo label generation. With the extracted dynamic VKG \(G_{V}\) and the object features \(F\) in image \(^{t}\), we encapsulate the extracted VKG into \(_{t}\) in a similar way as the LKG Encapsulation as follows:

\[_{ji}^{v}=}_{ji}_{j },_{i}>)}{_{i^{}}exp(cos<_{j}, _{i^{}}>)},\] (14)

[MISSING_PAGE_FAIL:7]

the state-of-the-art clearly when CLIP and WordNet are incorporated, validating that the performance gain largely comes from our novel KGD instead of merely using CLIP and WordNet.

**Object detection for intelligent surveillance.** The detection results on intelligent surveillance datasets are presented in Table 2. Notably, the proposed KGD surpasses all other methods by significant margins, which underscores the effectiveness of KGD in adapting the pretrained LVD towards the challenging surveillance scenarios with considerable variations in camera lenses and angles. The performance improvements achieved by KGD in this context demonstrate its effectiveness in exploring the unlabeled surveillance datasets by retrieving the classification knowledge of CLIP.

**Object detection for common objects.** We evaluate the effectiveness of our KGD on the common object detection task using Pascal VOC and Objects365. Table 2 reports the detection results, showcasing significant improvements over the baseline and outperforming state-of-the-arts, thereby highlighting the superiority of KGD. Besides, we can observe that the performance improvements on the Pascal VOC dataset and Objects365 dataset are not as significant as those in autonomous driving. This discrepancy is attributed to the relatively smaller domain gap between common objects and the pretraining dataset of LVD.

**Object detection for artistic illustration.** Table 2 reports the detection results on artistic illustration datasets. The proposed KGD outperforms all other methods by substantial margins, which highlights the effectiveness of KGD in adapting the pretrained large-vocabulary object detector towards artistic images that exhibit distinct domain gaps with natural images.

### Ablation studies

In Table 3, we conducted ablation studies to assess the individual contribution of our proposed KGD-L and KGD-V on the task of LVD adaptation. The pretrained LVD (i.e., Detic  without adaptation) does not perform well due to the significant variations between its pre-training data and the downstream data, As a comparison, either KGD-L or KGD-V brings significant performance

   Method & Detic  (Baseline) & MT  & MT +VILD  & MT +RegionKD  & MT +OADP  & **KGD (Ours)** \\  AP50 & 46.5 & 49.1 & 50.6 & 50.2 & 50.2 & **53.6** \\   

Table 4: Comparisons with existing CLIP knowledge distillation methods on LVD adaptation. For a fair comparison, we incorporate them with Mean Teacher Method (the columns with ‘MT+’). The results of all methods are acquired with the same baseline  as shown in the first column.

   Method &  &  &  \\   & VOC  & Objects365  & MIO-CTD & RAAL  & VisDrone  & Ciparlist  & Watercolor2K  & Ciparlist  \\  Detic ,(Baseline) & 83.9 & 29.4 & 20.6 & 20.6 & 19.0 & 61.0 & 58.9 & 51.2 \\  MT  & 85.6 & 31.0 & 20.0 & 23.4 & 18.9 & 62.7 & 58.4 & 49.8 \\  SIT  & 84.2 & 30.2 & 21.7 & 22.5 & 19.4 & 61.4 & 59.6 & 51.4 \\ SIT  & 84.5 & 31.2 & 21.7 & 22.3 & 19.4 & 62.3 & 58.8 & 52.1 \\ SIT  & 85.3 & 31.6 & 19.8 & 22.8 & 18.8 & 63.4 & 58.2 & 50.1 \\ SIT  & 86.2 & 32.0 & 21.0 & 21.0 & 20.2 & 64.6 & 59.3 & 51.8 \\ IIC  & 85.9 & 31.8 & 20.5 & 23.6 & 18.8 & 63.1 & 58.3 & 54.3 \\ IIC  & 86.8 & 32.5 & 21.7 & 22.4 & 19.8 & 64.2 & 57.7 & 55.7 \\ IIC  & 86.6 & 32.9 & 21.6 & 22.7 & 19.8 & 63.0 & 61.8 & 50.4 \\ IIC  & 85.9 & 31.5 & 20.6 & 22.6 & 18.2 & 63.0 & 60.2 & 50.4 \\ IIC  & 86.7 & 31.8 & 20.2 & 21.1 & 23.2 & 19.3 & 63.6 & 61.3 & 50.6 \\ IIC  & 85.9 & 31.8 & 20.2 & 22.1 & 18.8 & 63.0 & 60.2 & 50.4 \\ IIC  & 85.9 & 31.8 & 20.2 & 21.1 & 23.2 & 19.3 & 63.6 & 61.3 & 30.6 \\ IIC  & 85.3 & 31.8 & 20.2 & 22.1 & 18.8 & 63.1 & 60.0 & 50.1 \\ IIC  & 85.6 & 32.0 & 21.1 & 23.2 & 19.2 & 64.3 & 61.4 & 50.6 \\
**KGD (Ours)** & **86.9** & **34.4** & **24.6** & **24.3** & **23.7** & **69.1** & **63.5** & **58.6** \\   

Table 2: Benchmarking over common objects datasets, intelligent surveillance datasets, and artistic illustration datasets. \(\) signifies that the methods employ WordNet to retrieved category definitions given category names, and CLIP to predict classification pseudo labels for objects. We adopt AP50 in evaluations. The results of all methods are acquired with the same baseline  as shown in first row.

   Method & Detic (Baseline) &  \\  Language Knowledge Graph Distillation & & ✓ & ✓ \\ Vision Knowledge Graph Distillation & & ✓ & ✓ \\ AP50 & 46.5 & 52.8 & 52.7 & **53.6** \\   

Table 3: Ablation studies of KGD with Language Knowledge Graph Distillation (KGD-L) and Vision Knowledge Graph Distillation (KGD-V). The experiments are conducted on the Cityscapes.

improvements (i.e., +6.3 of AP50 and +6.2 of AP50 over the baseline), demonstrating both language and vision knowledge graphs built from CLIP can clearly facilitate the unsupervised adaptation of large-vocabulary object detectors. The combination of KGD-L and KGD-V performs the best clearly, showing that our KGD-L and KGD-V are complementary by providing orthogonal language and vision knowledge for regularizing the unsupervised adaptation of LVDs.

### Discussion

**Language knowledge graph (LKG) Extraction strategies.** Our proposed KGD-L introduces the WordNet  to uncover the implicitly encoded language knowledge in CLIP  and accordingly enables to build a category-discriminative and domain-generalizable Language Knowledge Graph (LKG) as described in Section 3.1. We examine the superiority of the proposed LKG Extraction with WordNet Hierarchy by comparing it with "LKG Extraction with category names" and "LKG Extraction with WordNet  Synset definitions", the former builds LKG directly with the category names from downstream datasets while the latter directly builds LKG using WordNet Synset definitions that are retrieved from the WordNet database with category names from downstream datasets. As Table 5 shows, both strategies achieve sub-optimal performance. For "LKG Extraction with category names", the category names are often ambiguous and less informative which degrades adaptation. For "LKG Extraction with WordNet Synset definitions", the used WordNet Synset definitions are more category-discriminative but they often have knowledge gaps with downstream data, limiting adaptation of the pretrained LVDs. As a comparison, our proposed LKG Extraction with WordNet Hierarchy performs clearly better due to the guidance of Synset definitions as well as their hyponym sets that captures more comprehensive structural knowledge from the WordNet hierarchy which helps generate category-discriminative and domain-generalizable LKG and facilitates the adaption of LVDs towards downstream data effectively.

**Language knowledge graph (LKG) Encapsulation strategies.** Our proposed KGD-L encapsulates the comprehensive knowledge in the extracted LKG into the teacher detector to facilitate detection pseudo label generation as described in Section 3.1. We examine the superiority of the proposed LKG Encapsulation by comparing it with "LKG Encapsulation by Feature Distance", which directly calculate and normalize the feature distance between object proposal feature and LKG nodes, and calibrates the original prediction probability from the teacher model using the normalized feature distance. As Table 6 shows, "LKG Encapsulation by Feature Distance" does not perform well in model adaptation, largely because it cannot effectively aggregate and capture semantic relationships and associations between different nodes in our extracted LKG. As a comparison, our proposed LKG Encapsulation shows clear improvements as the language information is adaptively aggregated along the training process stabilizes and improves the model adaptation, validating the performance gain largely comes from our novel LKG Encapsulation designs instead of merely using WordNet  embedding.

**Vision knowledge graph distillation (KGD-V) strategies.** Our proposed KGD-V captures the Dynamic vision knowledge graph (VKG) along the training as described in Section 3.2, which complements LKG by providing orthogonal and update-to-date vision information. We examine the proposed Dynamic VKG Extraction by comparing it with "Static VKG Extraction" and "Dynamic VKG Extraction without Smoothing". The former builds a static VKG with CLIP features of image

   Method & Detic (Source only) &  \\  LKG Encapsulation by Feature Distance & & ✓ & \\ LKG Encapsulation & 46.5 & 49.6 & **52.8** \\   

Table 6: Study of different KGD-L strategies. The experiments are conducted on the Cityscapes.

   Method & Detic (Source only) &  \\  LKG Extraction with category names & & ✓ & \\ LKG Extraction with WordNet Synset definitions & & ✓ & \\ LKG Extraction with WordNet Hierarchy & & & ✓ \\ AP50 & 46.5 & 51.9 & 52.0 & **52.8** \\   

Table 5: Study of different KGD-L strategies. The experiments are conducted on the Cityscapes.

crops of objects that are predicted by the pretrained LVD before adaptation and it remains unchanged during the LVD adaptation process, while the latter updates the VKG with Eq. (12) but without smoothing (Eq. 13). As Table 7 shows, "Static VKG Extraction" does not perform well in model adaptation, largely because the extracted static VKG is biased towards the pretraining datasets of the LVD and impedes domain-specific adaptation. For "Dynamic VKG Extraction without Smoothing", the nodes in VKG are updated with unlabeled downstream data in Eq. (12), but the downstream visual graph knowledge is not effectively incorporated into VKG, which limits the adaptation of the pretrained LVD. As a comparison, our proposed Dynamic VKG Extraction shows clear improvements as the update-to-date vision information extracted along the training process dynamically stabilizes and improves the model adaptation.

**Comparisons with existing CLIP knowledge distillation methods for detection.** We compared our KGD with existing CLIP knowledge distillation methods designed for detection tasks. Most existing methods achieve CLIP knowledge distillation by mimicking its feature space, such as VILD , RegionKD , and OADP . Table 4 reports the experimental results over the Cityscapes dataset, which shows existing CLIP knowledge distillation methods do not perform well in adapting LVDs to downstream tasks. The main reason is that they merely align the feature space between LVDs and CLIP without considering the inherent semantic relationships between different object categories. KGD also performs knowledge distillation but works for LVDs adaption effectively, largely because it works by extracting and encapsulating knowledge CLIP knowledge graphs which enables accurate object classification by leveraging relevant nodes in the knowledge graphs.

**Parameter studies.** In the pseudo label generation in KGD, the reliable pseudo labels are acquired with a pre-defined confidence threshold \(\). We studied \(\) by changing it from \(0.15\) to \(0.35\) with a step of \(0.05\). Table 8 reports the experiments over the Cityscapes dataset. It shows that \(\) does not affect KGD clearly, demonstrating the proposed KGD is tolerant to hyper-parameters.

**Qualitative experimental results.** We present qualitative results of KGD over diverse downstream domain detection datasets as shown in Appendix.

## 5 Conclusion

This paper presents KGD, a novel knowledge distillation technique that exploits the implicit KG of CLIP to adapt large-vocabulary object detectors for handling various unlabelled downstream data. KGD consists of two consecutive stages including KG extraction and KG encapsulation which extract and encapsulate visual and textual KGs simultaneously, thereby providing complementary vision and language knowledge to facilitate unsupervised adaptation of large-vocabulary object detectors towards various downstream detection tasks. Extensive experiments on multiple widely-adopted detection datasets demonstrate that KGD consistently outperforms state-of-the-art techniques by clear margins.