# Accuracy is _Not_ All You Need

Abhinav Dutta

Microsoft Research

Bangalore, India

t-abdutta@microsoft.com

&Sanjeev Krishnan

Microsoft Research

Bangalore, India

sakrishnan@microsoft.com

&Nipun Kwatra

Microsoft Research

Bangalore, India

nipun.kwatra@microsoft.com

&Ramachandran Ramjee

Microsoft Research

Bangalore, India

ramjee@microsoft.com

###### Abstract

When Large Language Models (LLMs) are compressed using techniques such as quantization, the predominant way to demonstrate the validity of such techniques is by measuring the model's accuracy on various benchmarks. If the accuracies of the baseline model and the compressed model are close, it is assumed that there was negligible degradation in quality. However, even when the accuracies of the baseline and compressed model are similar, we observe the phenomenon of _flips_, wherein answers change from correct to incorrect and vice versa in proportion. We conduct a detailed study of metrics across multiple compression techniques, models and datasets, demonstrating that the behavior of compressed models as visible to end-users is often significantly different from the baseline model, even when accuracy is similar. We further evaluate compressed models both qualitatively and quantitatively using MT-Bench and show that compressed models exhibiting high _flips_ are worse than baseline models in this free-form generative task. Thus, we argue that accuracy and perplexity are necessary but not sufficient for evaluating compressed models, since these metrics hide large underlying changes that have not been observed by previous work. Hence, compression techniques should also be evaluated using _distance_ metrics. We propose two such distance metrics, _KL-Divergence_ and _% flips_, and show that they are well correlated.

## 1 Introduction

The high cost and latency of Large Language Models (LLMs) has motivated the design of multiple model compression techniques for optimizing LLM efficiency such as quantization (Dettmers et al., 2022), Key-Value (KV) cache compression (Ge et al., 2023), pruning (Sun et al., 2023) and sparsification (Ashkboos et al., 2024). However, today, there is no standardized way of evaluating the effectiveness of these techniques.

The predominant way of establishing the validity of the LLM compression methods today is to report accuracy on selected benchmark tasks such as MMLU (Hendrycks et al., 2021), Hellaswag (Zellers et al., 2019), ARC (Clark et al., 2018), LAMBADA (Paperno et al., 2016), etc. It is assumed that if the compressed model preserves accuracy on such benchmarks, it can be used as an equivalent replacement for the baseline model.

In this paper, we conduct a detailed evaluation of various compression techniques. We find that while the difference in the aggregate accuracy metric across various benchmarks between the baseline and compressed LLM is negligible in most cases ( \( 2\%\)), the actual percentage change in the answerscan be significant (\( 5\%\)). In other words, even when the overall accuracy is unchanged, a large number of correct answers change to incorrect and vice versa in proportion (we call these _flips_), between the baseline and compressed model. To the best of our knowledge, we believe that _we are the first to identify this phenomenon of flips caused due to model compression. Further, we argue that flips serves as an intuitive metric that captures how significantly different the compressed model is from the baseline model, even when both models exhibit similar accuracy on various benchmarks._

Figure 1 shows the change in accuracy and flips % vs baseline 16-bit model, respectively, for _six_ quantization schemes on _seven_ benchmark tasks (MMLU (Hendrycks et al., 2021), Hellaswag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), ARC Easy and Challenge (Clark et al., 2018) PIQA (Bisk et al., 2019), and Winogrande (Sakaguchi et al., 2019)). We see that all quantization schemes have negligible difference in accuracy (\(\)) compared to the 16-bit version. However, except for GPTQ W8A16 (8-bit weight, 16-bit activation ) that preserves accuracy with negligible flips, all other quantization schemes exhibit large number of flips (\(\) to \(\)), indicating significant divergence from the baseline model.

Figure 3 shows similar behavior of MMLU task accuracy being preserved while flips increase, for two other compression techniques, namely, layer dropping (Gromov et al., 2024) and WANDA weight pruning (Sun et al., 2023). For example, while Gromov et al. (2024) showed that dropping the last few layers of a model did not affect its accuracy on standard benchmarks, we find a steady, almost linear increase in the number of flips with the number of layers being dropped.

The phenomenon of flips is puzzling at first glance. While it is easy to see that some correct answers may become incorrect due to errors induced by compression, it is difficult to explain how an approximately equal number of incorrect answers become correct such that overall accuracy is preserved! For example, MMLU questions have 4 options, one of which is correct. Thus, any output change could move a correct answer to an incorrect one but there is only 1 in 3 chance for an incorrect answer to land on the correct option. We present a detailed analysis of flips in Section 5. Furthermore, we observe that simply adding Gaussian noise to model weights can reproduce this flips phenomenon (see Table 1). This suggests flips arise from the inherent approximations introduced by compression rather than any new information or learning during the compression process.

Finally, one might question whether flips matter if accuracy is preserved. Indeed, if the downstream task where the LLM is used closely matches the benchmark task, accuracy alone might suffice. However, LLMs are typically used in a variety of downstream tasks that require generating free-form text, where accuracy evaluated on some standard question-answering tasks could be a poor proxy. Thus, we evaluate the compressed models using MT-Bench (Zheng et al., 2023), a multi-turn dialogue task. We show through qualitative evaluation as well as using GPT4 as an automated judge that

Figure 1: All six quantization schemes show _negligible difference in accuracy_ compared to baseline 16-bit model (Llama2-chat 7B, 13B, 70B and Yi-chat 6B, 34B) in seven different tasks. However, all schemes, except GPTQ W8A16 (8-bit weight, 16-bit activation), _exhibit large number of flips_, indicating severe divergence in model behavior.

compressed models with high number of flips _are significantly worse than baseline models_ in this task (see Section 6).

Since the goal of compression schemes is to create models that mimic the baseline models as closely as possible, we argue that compressed models are better judged by _distance metrics_ with respect to baseline, in addition to _capability metrics_ such as accuracy alone, as is the practice today. We demonstrate that well-known distance metrics like _KL-Divergence_ on a given dataset can better identify the differences created due to various compression techniques and this metric correlates well with _flips_. Further, we show that the scores on MT-Bench (which evaluates free-form generation capabilities of these models) is highly correlated with _flips_. Thus, we propose that _flips_, an intuitive and inexpensive to compute metric, as a potential proxy distance metric for evaluating LLM compression techniques.

In this paper, we make the following key contributions:

* Using detailed qualitative and quantitative evaluation of various compression techniques, we show that accuracy is not sufficient as an evaluation metric for LLM compression techniques.
* We demonstrate the existence of _flips_ as a general phenomenon and explain why they occur.
* We evaluate compression techniques using the _KL-Divergence_ distance metric and show that it correlates well with _flips_.
* We propose that, where appropriate, _flips_ be used as an intuitive distance metric for evaluating the quality of compression techniques.

## 2 LLM Evaluation Metrics

We compare baseline and compressed LLMs on the following metrics:

* _capability_ metric: % correct answers, for question-answering tasks. This determines the competency of the model for a particular task. Multiple-choice question-answering (MCQ) tasks such as MMLU expect the model to output a single token for the correct answer (A/B/C/D), and compare this token with the target answer. For other tasks (like PIQA, Hellaswag, ARC), where the model assigns a probability to an option (consisting of multiple tokens), we report the standard _normalized_ accuracy (Eleuther, 2021).
* _capability_ metric: This measures the overall language modelling capability of an LLM. It is defined as \(e^{(Average\ Negative\ Loglikelihood)}\) calculated over a dataset.
* _distance_ metric: measures the % of questions whose answers changed from correct \(\) incorrect or incorrect \(\) correct, between baseline and quantized model for all tasks that have correct/incorrect answers. Note that, we do not include incorrect \(\) incorrect transition in Flips for two reasons: 1) For non-MCQ tasks such as GSM8k (Cobbe et al., 2021b), TriviaQA (Joshi et al., 2017), etc. exact per-token output matches between different models are rare, resulting in many mismatches. Thus, including this transition may artificially inflate the metric for these tasks. 2) For MCQ tasks, users may care less about these incorrect \(\) incorrect transitions. Nevertheless, _if we include incorrect \(\) incorrect transitions for MCQ tasks, we find that, the flips numbers reported in this paper would further increase by another 20-40% (e.g., increase of 19% in Hellaswag, 41% in ARC and 43% in MMLU! See Table 11)_

   Task & Llama3-8b Setting & \%Accuracy & \% Flips \\  GSM8k & No noise & 49.39 & - \\  & Noise std = \(10^{-4}\) & 48.97 & 8.23 \\  ARC-challenge & No noise & 53.24 & - \\  & Noise std = \(5 10^{-4}\) & 53.49 & 6.05 \\   

Table 1: Adding Gaussian noise to weights results in _approximately equal \(correct incorrect\)_ and \(incorrect correct\) transitions, with the overall model accuracy mostly unchanged.

- _distance_ metric: consider a dataset having samples with multiple-choice answer options, where the j-th token of the i-th answer option has a probability distribution \(P_{b}(i,j)\) across all tokens in the vocabulary of the baseline model, and \(P_{q}(i,j)\) for the quantized model. Then the KL-divergence between the models for the entire dataset is the mean of KL-divergences across all tokens of all answer options and all samples in the dataset. \[KL\;div=_{dataset}_{i options}_{j tokens}D_{KL}(P_{b}(i,j)||P_{q}(i,j))\] (1) where N is the number of samples in the dataset and \(D_{KL}(P||Q)\) is the standard KL-Divergence between two probability distributions.

The flips metric is propitious because it is a proxy distance metric that is easily interpretable by end-users- for question-answering tasks, the end user typically cares about the correct/incorrect answers and not the underlying probability distribution of tokens. Further, the flips metric is as easy to calculate as accuracy for any dataset.

It is important to distinguish between _capability_ metrics (accuracy and perplexity in this study) and _distance_ metrics (_KL-Divergence_ and _flips_ in this study). This distinction is necessary because the goal of a compression scheme is to create a more efficient model that closely mimics the baseline model rather than to create a more capable model. In other words, a quantized model is intended to serve as a drop-in replacement for the baseline model with minimal impact on end-users. Therefore, we argue that _distance_ metrics are more suitable for judging the effectiveness of quantization or other compression schemes.

## 3 Experiments

We have measured the above metrics on multiple LLMs using multiple quantization techniques and bit lengths, on several tasks, as listed below:

* Models: We primarily used the Llama2 (7B, 13B, 70B) chat (Touvron et al., 2023), Yi (6B, 34B) chat (01.AI et al., 2024), Llama3 (8B, 70B) (Dubey et al., 2024) and Owen2 (1.5B, 7B, 72B) (Yang et al., 2024) families of models. The chat versions were used because they can be evaluated on MT-Bench (Zheng et al., 2023). However, we have observed a similar phenomenon in their pretrained non-chat versions as well (see Table 12).
* Quantization: We have evaluated LLM.int8() (Dettmers et al., 2022) as implemented in Bitsandbytes (Dettmers, 2024), with its 8-bit and 4-bit versions (referred to as BnB W8A8 and BnB W4A4 respectively) with default parameters supported with HuggingFace Transformers (Wolf et al., 2020). We used GPTQ (Frantar et al., 2023), AWQ (Lin et al., 2024) with group-size 128 with other parameters being default. We used Smoothquant (Xiao et al., 2024) (referred to as SQ W8A8) with per-token, per-channel quantization using \(=0.5\). We use TensorRT (NVIDIA, 2024) for SmoothQuant, all other schemes were evaluated using HuggingFace Transformers.
* Tasks: 1. For the Llama2 and Yi families, we evaluate the compressed models on ten different tasks. They include MMLU (Hendrycks et al., 2021) Table 4, ARC (Clark et al., 2018)(easy Table 7 and challenge Table 8), PIQA (Bisk et al., 2019) Table 5, Winogrande (Sakaguchi et al., 2019) Table 10, Hellaswag (Zellers et al., 2019) Table 6, and Lambda (Zellers et al., 2019) Table 9. We also use GSM8k (Cobbe et al., 2021) Figure 13, TriviaQA Joshi et al. (2017) Figure 14 and MT-Bench (Zheng et al., 2023) to evaluate models on generative tasks. MT-Bench is a dataset with 80 two-turn questions which can test generative capabilities of a model. In this study, we have used GPT-4 (OpenAI et al., 2024) (v0314) as judge, to generate the scores reported in Table 2.
2. For the Qwen2 and Llama3 families, we evaluate on MMLU Table 17, GSM8k Table 21, ARC (easy Table 18, challenge Table 19), MATH (Hendrycks et al., 2021) Table 20, BFCL (Yan et al., 2024) Figure 23, and Scrolls-Quality (Shaham et al., 2022) Table 22* Harness- We used Eleuther AI's eval-harness (Gao et al., 2023) for all the experiments, unless specified otherwise. Note that the standard benchmarks (ARC, MMLU, PIQA, Hellaswag, LAMBADA, GSM8k, TriviaQA, MATH, etc.) results on all models use greedy decoding, making these results fully deterministic.

## 4 Results

In this section, we present extensive evidence for flips across various quantization and pruning schemes, evaluated over a large number of models and all tasks. Results for MT-Bench are presented in Section 6.

### Quantization schemes

Summary of our results is highlighted in Figure 1 while the performance on each of the individual seven tasks (MMLU, PIQA, Hellaswag, ARC Easy, ARC Challenge, LAMBADA and Winogrande) are in Tables 4 to 10, respectively, in the Appendix.

The main observations from our experiments with quantized models can be summarized as follows:

1. **Accuracy:** Accuracy is preserved within 1% for the majority of the quantization methods, tasks and models (see Tables 4- 9). This indicates that accuracy is not sufficient to distinguish between precise and permissive quantization schemes.
2. **Flips:** The large %flips (\(\) 5%) is a general trend, which holds over different models, almost all quantization schemes, and tasks (see Tables 4- 10). Specifically, all quantization schemes except GPTQ W8A16 (\(<\) 1%) have significant %flips. Lower bit quantization schemes have greater %flips in general, indicating greater difference in behavior from the baseline (for example, on MMLU, BnB W4A4 has, on average, 2.4\(\) more flips than BnB W8A8). We focus on Flips in this study, but _AllFlips (Flips + incorrect\(\)incorrect transitions )_ results can be found in Figure 10, and Table 11 in Appendix.
3. **KL-Divergence vs Flips:** From Figure 2, we observe that the two distance metrics _KL-Divergence_ and _%flips_ are well correlated. For example, their Spearman correlation on the MMLU benchmark is 0.981.
4. **Impact of task type:** * _MCQ_ Tasks - Generally easier tasks (identified by higher average accuracy) have smaller %flips. For example, MMLU which is a relatively hard task has 8-16% flips for Bitsandbytes W4A4 whereas for the same technique, PIQA, an easier task, has 3-6% flips. The reason for this behavior is explained in Section 5. * _Generative_ Tasks - Surprisingly, such tasks have much _more flips_ than MCQ ones. For example, GSM8K (Table 13, Table 21), a hard task that requires reasoning over multiple steps, exhibits a significant amount of _flips_ (10-25% for BnB W8A8 and W4A4). Similarly, in MATH Table 20, we observe 5-15% _flips_ for BnB W4A4. However, _flips_ are quite small (2-4%) in easier tasks like TriviaQA(Table 14) that tests trivia question answering capabilities.

Figure 2: Flips and KL Divergence are well correlated. Each point corresponds to a model, quantization combination in Table 4

5. **Impact of model size:** Larger models typically have fewer flips than smaller ones. For example, on the MMLU benchmark with BnB W4A4, Llama2-70b chat has 5.6% flips, while Llama2-13b chat and Llama2-7b chat have 1.4\(\) and 1.6\(\) more flips, respectively. This may be because larger models are more resistant to perturbations introduced by compression than smaller ones.

### Other model compression techniques

We also evaluate the following three compression techniques, though on a smaller set of tasks and models. Our general observations seen above holds.

1. **Dropping last n-layers**(Gromov et al., 2024): This work demonstrated that dropping the last few layers did not affect the accuracy on standard benchmarks. We find in Figure 3(a) that as one keeps dropping layers, even though the accuracy increases only modestly, %flips increases significantly, demonstrating that the resulting models keep deviating further away from the baseline.
2. **Wanda**(Sun et al., 2023): This is a pruning method. We observe in Figure 3(b) that as we increase the pruning ratio, even though accuracy barely changes, %flips increases steadily.
3. **SliceGPT**(Ashkboos et al., 2024): This is a model sparsification method which drops a certain fraction of rows and columns of each dense matrix. We observe in Figure 9 in Appendix that even at very low sparsity ratios %flips is significant indicating that the compressed models are probably very different from baseline.

### Perplexity

Though we have focused on accuracy so far, our observation that the difference between two models' output token values cancel out leaving the average metric result unchanged, is applicable to perplexity as well. In particular, since perplexity may be interpreted as the inverse of the geometric mean of token probabilities, lower probabilities for some tokens in the test dataset may be cancelled by higher probabilities of other tokens. This indicates that perplexity alone is also inadequate in evaluating model compression schemes. Therefore, we argue that along with perplexity, KL-Divergence between the distributions generated by the baseline and optimized models should also be reported.

Figure 11 in Appendix plots the log-likelihood difference between the 16-bit and quantized model for each of the tokens in the wiki-2 dataset (Merity et al., 2016) for four different quantization schemes. From the figure, it appears that the log-likelihoods of the quantized model is just the log-likelihood of the baseline model with some symmetric noise added. Now, since perplexity is \(e^{-avg(logprobabilities)}\), adding _any_ amount of symmetric noise leaves it unchanged. For example, addition of Gaussian noise to the log-probability outputs of the model maintains the perplexity, while

Figure 3: MMLU 5-shot accuracy difference and flips for two compression techniques (Llama2-13b model). Even at early stages of pruning with no accuracy difference, flips indicate model divergence.

the quality of generation degrades as the standard deviation of the added noise increases (see Table 29). This analysis demonstrates one key weakness with the perplexity metric when used for evaluating compression techniques. While it is not clear if adding Gaussian noise to the log-likelihoods is an accurate representation of the behavior of compression schemes, it appears to be a reasonable proxy. As we shall see in Section 6, as quantization increases, there is steady degradation in the quality of the text generated by the model that are visible only by examining them closely.

## 5 Analyzing Flips

One of the interesting observations in this study has been that when we quantize models, the number of questions where the LLM's answers go from incorrect to correct (referred to as \(incorrect correct\)) is roughly equal to the number that goes the other way. This may seem unintuitive, because one might expect \(correct incorrect incorrect correct\), since a) the number of questions with correct answers is usually greater than incorrect answers, so random perturbations should cause more correct answers to flip, and b) given a correct answer, the correct to incorrect transition should be likelier because changing to any of multiple other incorrect options suffices, but given an incorrect answer, the incorrect to correct transition happens only if somehow the perturbation caused by quantization helps it land on the one correct option out of many. But we observe that this is not the case (and indeed, the opposite may also be true in some cases!).

To help explain the above phenomenon, we introduce a metric called _top margin_ which is the difference in token probability between the best and the second best answer option. By best (second-best) option, we mean the option that was given the highest (second highest) probability. Higher top margin on a question indicates that the model is more confident about its answer.

**Answers are likely to change when top margin is low.** Quantization introduces some noise in the weights and activations, due to which there is a perturbation in the output answers' probabilities (verified empirically). Thus, we expect that answers are more likely to change when top margin is low, since a small increase or decrease in probabilities can cause the best and second best options to swap (see Figure 4). To further bolster this claim, we show that the changes in probabilities do not depend on top margin, i.e., roughly all questions undergo the same amount noise (except when the top-margin is very high, where we do not see much change in probabilities after compression, but such questions are not likely to flip anyway) as seen in Figure 7. We further find top margins are well correlated before/after compression (i.e low confidence answers are likely to remain so and vice-versa) in Figure 8. In subsection A.4 we find that due to this reason, it is very likely that the same question (with low top margin) would be flipped by multiple quantization schemes.

**Correct (incorrect) answers have higher (lower) top margin and are thus less (more) likely to flip.** Table 27 shows the top margins for questions for which the LLM's answer is correct and when the answer is incorrect. We observe that, top margin when correct is, on average, greater than the top 

[MISSING_PAGE_FAIL:8]

despite these two compressed models matching baseline accuracy on various tasks (e.g., MMLU accuracy within 1%) and suffering only a 0.4 lower score on a scale of ten in the GPT4 evaluation. We believe that this qualitative analysis adds further evidence to our claim that benchmark accuracy alone, as is standard practice today, is a poor metric to evaluate compressed LLMs, especially, if they are likely to be used for generative tasks in downstream applications.

 p{142.3pt}}  
**MT-Bench Prompt** & **Summary of 16-bit, 8-bit (BnB W8A8), and 4-bit (BnB W4A4) Llama-2-70B-chat model responses** \\ 
1) Consider a satellite that is in a circular orbit around the Earth. The speed of the satellite decreases. What will happen to the satellite’s orbital radius and period of revolution? Please justify your answer using principles of Physics. \\
2) Take your previous response and rephrase it as a limerick. \\
3) Could you write a captivating short story beginning with the sentence: The old abandoned house at the end of the street held a secret that no one had ever discovered. \\
4) You can see a beautiful red house to your left and a hypnotic greenhouse to your right, an attractive heated pink place in the front. So, where is the White House? \\
5) What about when twice the number is divided by 5? \\
6) Reformulate your earlier reply, output it in JSON format and only include books published after 1980. \\
7) Can you change the ratings from numbers to letters? Capital letters MUST be used when writing the names of phones. \\
8) Given a set of complex equations, extract all unique variable names from each equation... \\
9) Rewrite your previous response. Start every sentence with an A. \\
10) What is the central dogma of molecular biology? What processes are involved? Who named this? \\   

Table 3: Qualitative evaluation of Llama2-70B-chat model text generations for MT-Bench prompts. Author’s summary of model responses shown below; full model generated responses are in Appendix. These results substantiate a clear degradation in response quality with quantization.

Figure 6: _Flips_ is a better predictor of downstream task performance than _Accuracy_

## 7 Limitations

Predicting performance degradation of LLMs in the wild is a challenging and open problem, and it is possible that _any_ metric calculated on standard benchmarks is insufficient. Other limitations are:

* If the downstream task is very similar to the benchmark on which the quantized model is tested, then accuracy may be sufficient, and distance metrics are not needed.
* this may or may not materialize as visible degradation in some downstream tasks.
* Our qualitative evaluation in Section 6.1 is subjective and may not be broadly representative.

## 8 Related Work

Given their versatility, LLMs are evaluated on a diverse set of tasks (Chang et al., 2024). Since accuracy is one of the most well-accepted metrics used in task evaluation, compression methods today typically focus on accuracy. However, we are not the first to point out the problem with over-reliance on aggregate metrics like accuracy when judging the quality of a model optimization scheme. Xu et al. (2021) have proposed label loyalty and probability loyalty as a metric to evaluate compressed BERT models. Other works like Joseph et al. (2021), Hooker et al. (2020), and Hooker et al. (2021) have shown compressed ImageNets to be more biased despite preserving accuracy and have proposed Knowledge Distillation based methods to address it. There has also been work (Hong et al., 2024) on evaluating LLM compression schemes on various trustworthiness dimensions. However, metrics for evaluating LLM compression techniques have not been studied widely so far, leading to over reliance on accuracy alone.

There have been many works on LLM evaluation that have shown shortcomings of existing evaluation methods. Lyu et al. (2024) have pointed out the misalignment between free-form generation and probability based evaluation on MMLU. Sclar et al. (2023) have shown LLMs to be very sensitive to prompt formatting. Zheng et al. (2024) have shown models to be biased towards a certain option in MCQ tasks. Alzahrani et al. (2024) have shown minor changes in the benchmarks leading to re-ordering of rankings, and Srivastava et al. (2024) has shown accuracies to be different when considering the _functional_ equivalent of math problems. Jaiswal et al. (2024) have curated existing datasets to create their own benchmark that can be used to evaluate compressed models. Li et al. (2024) and Jin et al. (2024) have evaluated various quantization tasks on multiple tasks. Namburi et al. (2023) have studied the impact of compression and pruning on an LLM's _parametric_ knowledge. Zhang et al. (2024) propose a number of other metrics in addition to accuracy such as fluency, informativeness, coherence and harmlessness. Chang et al. (2024) presents a detailed survey on evaluation of LLMs that covers what, where, and how to evaluate an LLM and lists several challenges in LLM evaluation.

However, to the best of our knowledge, none of the prior work have pointed out the phenomenon of flips, that occurs when LLMs are compressed, and the observation that higher flips is correlated with larger degradation in model performance despite accuracy matching with the uncompressed model.

## 9 Conclusion

In this work, we have examined metrics to evaluate the quality of compression methods for LLMs such as quantization. We distinguish between aggregate capability metrics such as accuracy, and distance metrics _flips_ and _KL Divergence_ between the compressed model and the baseline model. We justify why using distance metrics is more appropriate for evaluating model compression methods. We show that accuracy severely underestimates the true distance between models as perceived by the end user. We explain this is due to the presence of flips between correct and wrong answers when a model is quantized, and explain why the flips are nearly balanced, leading to similar accuracy, while the user-perceived output of the quantized model may be significantly different. We argue that distance metrics such as flips and KL-divergence are essential for evaluating all optimization methods which may change the model outputs and whose goal is to minimize end-user visible behaviour changes from a baseline model. We hope that better distance metrics as proposed in this work will enable research in model optimization and compression to progress faster and better meet user expectations on model output quality.