# Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models

Jiahao Ying1, Yixin Cao2, Yushi Bai3, Qianru Sun1, Bo Wang 4, Wei Tang5,

Zhaojun Ding6, Yizhe Yang4, Xuanjing Huang2, Shuicheng Yan7

1Singapore Management University, Singapore 2Fudan University, China

3Tsinghua University, China 4Beijing Institute of Technology, China

5University of Science and Technology of China, China

6School of Computing, University of Georgia, USA 7Skywork AI

{jhying.2022}@phdcs.smu.edu.sg

Corresponding author.

###### Abstract

Large language models (LLMs) have achieved impressive performance across various natural language benchmarks, prompting a continual need to curate more difficult datasets for larger LLMs, which is costly and time-consuming. In this paper, we propose to automate dataset updating and provide systematical analysis regarding its effectiveness in dealing with benchmark leakage issue, difficulty control, and stability. Thus, once current benchmark has been mastered or leaked, we can update it for timely and reliable evaluation. There are two updating strategies: 1) mimicking strategy to generate similar samples based on original data, preserving stylistic and contextual essence, and 2) extending strategy that further expands existing samples at varying cognitive levels by adapting Bloom's taxonomy of educational objectives. Extensive experiments on updated MMLU and BIG-Bench demonstrate the stability of the proposed strategies and find that the mimicking strategy can effectively alleviate issues of overestimation from benchmark leakage. In cases where the efficient mimicking strategy fails, our extending strategy still shows promising results. Additionally, by controlling the difficulty, we can better discern the models' performance and enable fine-grained analysis -- neither too difficult nor too easy an exam can fairly judge students' learning status. To the best of our knowledge, we are the first to automate updating benchmarks for reliable and timely evaluation. Our demo leaderboard can be found at [https://yingjiahao14.github.io/Automating-DatasetUpdates/](https://yingjiahao14.github.io/Automating-DatasetUpdates/).

## 1 Introduction

Large Language Models (LLMs) are becoming increasingly important in both academia and industry, such as enhancing language translation systems, improving customer service bots, and streamlining data analysis processes across sectors [23; 2; 21; 38; 31; 43]. They have achieved to some extent general intelligence, showing superior performance across various benchmarks including GLUE , SQuAD , CoQA . As scaling continues, LLMs are gradually mastering more challenging datasets, demanding experts to curate more difficult datasets, which may soon be conquered again by larger and more advanced LLMs. Clearly, such a way of constantly updating dataset is impractical. In this paper, we aim to automate updating benchmarks for LLMs evaluation to minimize humanefforts. This not only helps to timely understand the advantages and disadvantages of model iteration, but also benefits the reliability of evaluation. Once the benchmark leakage issue -- testing samples have been seen during pre-training -- has been found, we can automatically update current datasets to avoid overestimation. However, this is non-trivial due to the following research questions:

* Will updated benchmarks produce stable results?
* How can the update strategy mitigate benchmark leakage issue?
* Is it possible to automate benchmark updating for better discerning model capabilities?

To this end, we propose two benchmark updating strategies, mimicking and extending a given dataset, and conduct in-depth analysis toward reliable and timely evaluation. i) Our **mimicking** strategy is to leverage LLMs to generate similar ones for each existing sample (marked as seeds), so that we, to the maximum extent, preserve the stylistic and contextual essence of the original data. This is simple and efficient, while it is under exploration if the overestimation caused by data leakage can be mitigated. ii) Inspired by cognitive theory, our **extending** strategy further expands the original data according to varying cognitive levels. Here, we borrow the concepts from Bloom's taxonomy, _a hierarchical classification widely used for educational learning objectives into levels of complexity and specificity_. This not only makes our evaluation more systematic but also leads to a balanced difficulty of the dataset, which can better distinguish and analyze the capabilities of models -- neither too difficult nor too easy an exam can fairly judge students' learning status.

In our experiment, we systematically investigate the above two strategies regarding reliability, stability, and their effectiveness to deal with overestimation when benchmark leakage happens. We generate datasets based on two widely used benchmarks (i.e., MMLU  and BIG-Bench ), and study seven open-source models and four closed-source models. We find that: **1)** Both mimicking and extending strategies show a high level of stability (Section 3.2 & 3.4). **2)** The mimicking strategy proves effective in alleviating overestimation. In most cases, compared to the original leaked dataset, our updated dataset exhibits no significant overestimation issues. (Section 3.3). **3)** In cases where the mimicked dataset still exhibits overestimation, our extending strategy effectively alleviates this issue (Section 3.4). **4)** We can manipulate Bloom's concept of a sample and the popularity of seeds to control the difficulty of the extended dataset. The experimental results also provide a fine-grained analysis of LLMs' cognitive levels. Some models demonstrate considerable variations in their performance across different cognitive levels, yet GPT-4 exhibits a strong performance across all levels (Section 3.5). Our main contributions can be summarized as:

* To the best of our knowledge, we are the first to automate updating benchmarks for timely and stable evaluation.
* We propose to control the difficulty of the sample based on varying cognitive levels, toward fair and fine-grained analysis.
* We have conducted extensive experiments demonstrating the effectiveness of our two strategies in alleviating the issue of overestimation when benchmark leakage occurs.

## 2 Auto-Dataset Update Framework

Figure 1 shows the framework of our auto-dataset update strategies. Given a test sample, mimicking strategy generates one or several similar yet unseen samples, whereas extending strategy generates a set of samples at different cognitive levels. Compared with mimicking strategy, extending strategy is beyond the scope of the given sample, challenging the model's capabilities in more nuanced and complex scenarios. We introduce these two strategies in Section 2.1 and Section 2.2, respectively. Then, we apply them to widely used benchmarks and manually analyze the quality (Section 2.4).

### Mimicking Strategy

Given a seed sample and the corresponding task description (optional), we design a prompt for LLMs to generate a new sample that retains the stylistics and knowledge essential. This, to the maximum extent, ensures the quality of generation. Below is an example and more examples are shown in _You are a question-writer expert. Please mimic the provided examples to generate *one* different but high-quality sample following the task description._

**[Task Description]:** _This task evaluates the model's ability to discern the plausibility of specific athletic actions based on the athlete's known skills and typical behaviors in their sport. For example, a language model should understand that Leo Messi (arguably the best soccer player) is more likely to score goals._

**[Seed Sample]:** _[ "input": "Jamal Murray was perfect from the line", "target_scores": { "plausible": 1, "implausible": 0 ] [New Generated Sample]: [ "input": "Jamal Murray made 10 three-pointers in a row", "target_scores": [ "plausible": 1, "implausible": 0 ] ]_

Appendix E. To further improve the quality of generated samples, we heuristically validate them through LLMs themselves or programs to filter out noise, such as answer-incorrect and duplicate samples (details in Appendix A)).

### Extending Strategy

Inspired by cognitive theory, an effective learning material should consider different educational objectives. According to Bloom's taxonomy, there are six levels of complexity and specificity: Remember, Understand, Apply, Analyze, Evaluate, and Create. These cognitive levels not only systematically and comprehensively categorize testing data, but also distinguish the difficulty, making it possible to control the difficulty of updated datasets -- neither too difficult nor too easy exam can fairly judge students' learning status. Here, we re-organize them into four groups to better fit the purpose of evaluation. As shown in Figure 1, **Remember and Understand Level** asks the model to list, recall basic concepts, interpret, summarize, and exemplify ideas or concepts; **Apply Level** tests the use of learned facts and abstractions in new contexts and particular situations; **Analysis Level** requires the model to break down concepts and examine the relationships among them; **Evaluation Level** asks the model to appraise a situation and criticize opinions or statements. We've combined the Remember and Understand levels for simplicity and excluded the Create level to clearly distinguish between the different cognitive abilities during testing. Following the above idea, we first abstract the original question into a core entity, statement, or piece of knowledge, marked as seeds. Based on that, we then prompt LLMs to generate new questions at different cognitive levels. Below is an example where we extract sports star "Jamal Murray" and prompt LLMs to extend (more examples are shown in Appendix E and the detailed prompt is shown in Appendix D). Clearly, we can adjust the distribution of generation at the four cognitive levels to control the overall difficulty of the updated

Figure 1: The auto-dataset update framework. For the mimicking strategy, we mimic the original test case to get a similar but new sample. For the Extending strategy, we extend the original sample to multi-cognition levels ( Remember & Understand, Apply, Analysis, Evaluation ) to have a thorough and nuanced assessment.

dataset. Later, we will provide empirical analysis. It is important to note that our two strategies are efficient and adaptable, making them easily applicable across a wide range of benchmarks.

_You are a question writer expert, your objective is to write **only one** really complex and difficult question about the given entity._

_[Generate Criterion]: 1. The question should be focused on the remember and understand level. This means the question should prompt for recall of facts, terms, and basic concepts, and NOT delve into deeper levels like Applying Analyzing or Understanding. 2. Ensure that you can confidently answer the questions you are proposing, if you can not answer it correctly or have no related knowledge about the entity please return "None"._

_[Seed Entity]: Jamal Murray_

_[New Generated Sample]: ["question": What is the exact date, team and college that Murray was drafted into the NBA?, "ref_answer": Answer.. ]_

### Apply to Existing Benchmarks

To conduct auto-dataset update, we select BIG-Bench  and MMLU  as our seed datasets. From these benchmarks, we chose ten sub-tasks that are particularly representative of the diverse abilities LLMs are expected to demonstrate, ensuring a comprehensive evaluation. These tasks cover various types of questions, including both context-free and context-based, and assess the model's abilities in logical reasoning, common sense reasoning, memorization, mathematical ability, and more. A detailed analysis of the tasks is in Appendix A. Specifically, from BIG-Bench, we include:

**Sports Understanding (Sports):** focuses on the ability to discern between plausible and implausible statements about sports stars. **Periodic Elements (Element):** measures knowledge of chemistry. **CS Algorithms (Algos):** assesses models' understanding of computer science algorithmic. **Physical Intention (Phys):** tests the understanding of the physical behaviors. **Math Word Problems with Hints (Math):** tests the ability of models to perform mathematical reasoning. From the MMLU, we select: **Abstract Algebra (Algebra):** assesses models' understanding of abstract algebra concepts. **International Law (Law):** evaluates models' ability to understand and follow rules and regulations. **Econometrics (Econ):** tests the understanding of econometric principles and implications of econometric phenomena. **College Medicine (Medicine):** evaluates models' knowledge relevant to medicine. **Computer Security (Security):** involves understanding the principles used to protect computer systems and networks. After selecting datasets, we apply our two strategies to update these datasets and analyze the effectiveness and reliability of the updated samples.

### Updated Dataset Analysis

We conduct experiments using GPT [23; 22] series and the Claude [1; 2], series model. However, other LLMs can also be deployed into this framework. For mimicking strategy, we use ChatGPT (gpt-3.5-turbo) and GPT-4-preview to update the selected datasets from BIG-bench and MMLU. Following the update, the filtering process--detailed in Appendix A--may result in a dataset size that is smaller than the original. To achieve a dataset size comparable to the original, we literalize the dataset 2-3 times. (considering the time and the cost we limit the amount of the sample from the dataset "Math" to 1000). The statistical details of the updated datasets are shown in Table 1.

For the extending strategy, we chose the dataset Sports, Algorithms (Algos), Algebra, and Physics (Phys), because the mimicking strategy performs poorly for benchmark leakage mitiga

  
**Task\({}_{}\)** & **\#Orig.** & **\#Mimic** & **Tasksum.U** & **\#Orig.** & **\#Mimic** \\  Sports & 1000 & 951 & Algebra & 100 & 93 \\ Element & 536 & 548 & Law & 121 & 117 \\ Algos & 160 & 150 & Econ & 114 & 101 \\ Phys & 81 & 81 & Medicine & 172 & 160 \\ Math & 7688 & 1016 & Security & 100 & 100 \\   

Table 1: The statistical result of the original (Orig.)and mimicked (Mimic) ten subtasks from BIG-Bench and MMLU. For time and cost consideration, we limited the number of the generated samples on Math.

  
**Metrics** & **Mimicking** & **Extending** \\  Fluency & 94.7 / 95.7 & 98.3 / 100 \\ Coherence & 94.4 / 94.0 & 96.7 / 96.7 \\ Answer Accuracy & 86.7 / 82.8 & 92.7 / 82.0 \\ Category Accuracy & - & 98.3 / 100 \\   

Table 2: The overview of Human Evaluation. The Scores are shown as full score rate (%), with numbers after the slash indicating agreement rates (%) among the five evaluators.

tion(Section 3.3). For the Sports task, we extract the names of sports stars as seeds, recognizing that this task assesses the models' knowledge of these individuals. For the Algebra task, we extract key algebraic concepts as seeds. For Phys, we use GPT-4  to summarize the basic physical laws behind the original samples as seeds. Since the Algo task only encompasses a limited range of algorithmic topics, we employ GPT-4 to generate a list of 40 algorithm names. These names are then utilized as seeds for question generation. Based on these seeds, we utilize GPT-4 and Claude-3  to generate new samples across multiple cognitive levels using extending prompts (details in Appendix D). We manually maintain an equal distribution of the questions across each cognitive level. The static result is shown in Table 11.

**Human Evaluation.** To validate the reliability of our two strategies, we conduct a human evaluation involving five senior computational linguistics researchers, who have been trained in advance. For mimicked samples, evaluators review 120 randomly chosen samples, each including the question's category, the question, and the answer. They assess each question and answer paired based on three criteria: Fluency (the grammatical correctness and smoothness of the question), Coherence and Clarity (the logical clarity and explicit articulation of the question), and Accuracy of the Answer (the detailed evaluation guideline is shown in Appendix B.1). In evaluating the extended samples, evaluators examine 60 randomly selected samples, which include the question, its cognitive level, and the reference answer. The assessment criteria are similar to those for the mimicked samples, with an additional focus on Category Accuracy (the detailed evaluation guideline is shown in Appendix B.2) to ensure that the question's cognitive level is appropriately identified. The human evaluation results, summarized in Table 2, demonstrate high effectiveness and reliability of both strategies.

### Evaluation Metric

For mimicked samples, we adhere to the evaluation metrics used in the original tasks (details in Appendix C.1.5). For extended samples, where questions are free-form, we adopt the "LLM judgment" methodology, following [47; 3]. We give the question, reference answer (from updated sample) and the candidate's answer to LLMs to evaluate the answer across three dimensions: 1. **Accuracy:** evaluates the correctness of the answer, 2. **Coherence:** assesses the logical flow, and 3. **Factuality:** assessing the presence of factual errors (the evaluation prompt is shown in Appendix D). We use the full-mark rate over the three dimensions as the metric and manually evaluate the "LLM judgment" result. The consistency of the model's results with human judgment achieved 90.8% (detailed evaluation guideline is in Appendix B.2 and detailed human evaluation result is shown in Table 8).

## 3 Experiment

### Baseline

For baselines, we choose seven open source models: LLama-2-7b-chat, LLama-2-13b-chat , Llama-3-8b-Instruction , Mistral-7B-Instruct-v0.2, Mistral-8x7B-Instruct-v0.1 , Yi-6b-chat, Yi-34b-chat , and four closed-source models: GTP-4, ChatGPT, Claude2 , Gemini-pro .

### Stability of Mimicked Datasets

Figure 2: Performance (%) of the 11 involved models (zero-shot) on the original and mimicked (footnote \(m\)) Big-Bench. The generation of the mimicked dataset is conducted four times, the figure displays the average performance and standard deviation. Detailed results are shown in Table 12.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

### Is it possible to control the sample difficulty using the Extending Strategy?

Except for overestimation leading by benchmark leakage in Section 3.4, there is another issue on the task Sports and Phys (Table 3 and Table 4) -- the absolute scores are notably high, suggesting that the questions are relatively simple and thus insufficient to differentiate the models (i.e., 3.82% and 4.30% difference on average for models GPT-4, ChatGPT, Gemini, and Claude). In this section, we explore the feasibility of modulating the difficulty of the extended data samples to better discern and differentiate model capabilities. In our extension work, we start by abstracting the original question into core entities, statements, or adding additional knowledge, then use Bloom's taxonomy for further extension. This naturally allows for the adjustment of question difficulty in two ways, as indicated by prior findings: 1) work  suggests that cognitive demand increases with higher cognitive processes. Accordingly, by adjusting to more abstract cognitive levels, we expect to produce more challenging samples; 2) work  indicates that as the popularity of the subject entity increases, the difficulty of the question decreases. Thus, the popularity of the input seed could be strategically manipulated to adjust the difficulty of the generated questions.

To validate the efficacy of our framework in controlling difficulty, we use the extracted sports star name and the summarized common physical law as the seed to update the dataset using the extending strategy (Sec 2.4). We use models GPT-4, ChatGPT, Gemini, and Claude as the baseline for their indistinguishable performance. As shown in Figure 4, **1)** on the extended Sports and Phys datasets, the four models' performance show larger variance (23.76%, 14.04% respectively), compared with original datasets; **2)** overall, samples test "Apply" and "Evaluation" cognitive levels are found to be more challenging. This finding indicates that manipulating the cognitive level in the extending strategy is an effective way of controlling the difficulty of the generated questions in our framework. It also highlights the importance of focusing on models' capabilities at different cognitive levels.

Figure 4: Full-mark rate (%) of GPT-4, ChatGPT, Claude, and Gemini on the extended Sports (a), Phys (b) dataset on different cognitive levels. Overall is the average score across the four levels.

  
**Model** & **Training** & **LoRa** & **Algebra** & **Algos** \\   Liama2-7b & None & - & **42\(\)**0.6 & **10.2**\(\)0.5 \\ Liama2-7b & + leakage & ✓ & 1.6\(\)0.6 & 2.2\(\)0.6 \\ Liama2-7b & + w rationale & ✓ & 1.2\(\)0.0 & 5.3\(\)0.5 \\ Liama2-7b & + leakage & ✗ & 1.8\(\)0.6 & 1.1\(\)0.0 \\ Liama2-7b & + w rationale & ✗ & 2.1\(\)0.5 & 8.9\(\)0.8 \\   Liama2-13b & None & - & **8.5\(\)**0.4 & **11.9**\(\)0.8 \\ Liama2-13b & + leakage & ✓ & 6.6\(\)0.5 & 5.1\(\)0.1 \\ Liama2-13b & + w rationale & ✓ & 6.4\(\)0.9 & 11.2\(\)0.8 \\ Liama2-13b & + leakage & ✗ & 1.5\(\)0.4 & 0.5\(\)0.4 \\ Liama2-13b & + w rationale & ✗ & 5.7\(\)0.5 & 7.7\(\)1.2 \\   Liama3-8b & None & - & **34.6\(\)**1.6 & **46.7**\(\)2.1 \\ Liama3-8b & + leakage & ✓ & 12.1\(\)0.5 & 21.3\(\)2.2 \\ Liama3-8b & + w rationale & ✓ & 17.8\(\)1.3 & 17.5\(\)1.8 \\ Liama3-8b & + w rationale & ✗ & 15.3\(\)1.6 & 18.7\(\)1.2 \\   Mistral-7b & None & - & **21.8\(\)**1.7 & **36.8**\(\)0.0 \\ Mistral-7b & + leakage & ✓ & 0.9\(\)0.5 & 5.3\(\)0.5 \\ Mistral-7b & + w rationale & ✓ & 4.4\(\)0.6 & 9.1\(\)0.5 \\ Mistral-7b & + w rationale & ✗ & 2.3\(\)0.2 & 4.4\(\)0.8 \\   

Table 5: Average full-mark (%) of the fine-tuned model on the extended dataset over the four iterations. Blue cells indicate reduced performance after fine-tuning.

Some models exhibit significant disparities in performance at various cognitive levels, while GPT-4 stands out with its superior and consistent performance.

Besides assessing how cognitive levels influence performance, we explore how the popularity of the seeds used in dataset updates affects model performance. Following previous work , where using Wikipedia page views as the popularity, we gather the total page views for each seed throughout year 2023 and use it as the popularity. As in Figure 5, despite an uneven distribution of seed popularity in the extended dataset, a clear trend emerged: there was a noticeable increase in model performance when using more popular seeds to generate the question. This suggests that the popularity of the seed input can also be strategically manipulated to control the difficulty in our framework.

### Adaptable to Models Beyond the GPT Backbone

Relying solely on GPT-4 may introduce biases, such as self-preference, particularly when employing the "LLM-as-an-Examiner" methodology . In this section we expand the experimentation by using Claude-3-Opus  to generate new samples, demonstrating the framework's adaptability to other backbones. Thus by using multi-source data, we can mitigate the bias associated with exclusively using GPT-4. Using Claude-3-Opus, we extend the algebra task, where the mimicking strategy is less effective, following the setting in Section 3.3. Due to time and cost constraints, we conduct the generation process twice. As shown in Table 10, we observe a consistent conclusion with Table 5: the performance of models on the leaked original dataset declines when evaluated on our extended dataset. This indicates the adaptability of our framework to utilize different language models as backbones.

## 4 Related Work

**Benchmark for Model Evaluation:** There are a lot open-source benchmarks: MMLU , tests a wide range of knowledge and reasoning abilities. HellaSwag , challenges models with complex commonsense reasoning. BIG-bench , tests models on both traditional NLP tasks and novel problems. ARC , focusing on science questions that require deep reasoning. KoLA , also uses the bloom taxonomy to construct the dataset. Given that these public benchmarks are open-source and static, they are susceptible to benchmark leakage. Recently, there have been efforts to construct benchmarks using newly emerged corpora[32; 41]. EvoWiki  categorizes Wikidata and Wikidata into three levels according to the cut-off date of model development and further exam models' performance without leakage. However, whether these context-based question generation methods can generate more challenging questions to better differentiate between models has yet to be proven.

  
**Model** & **Training** & **LoRA** & **Algebra** \\     } & None & - & **26.3**\( 1.6\) \\  & + w rationale & ✓ & 8.9 \( 0.5\) \\  & + w rationale & ✗ & 22.1 \( 0.5\) \\     } & None & - & **33.9**\( 1.9\) \\  & + w rationale & ✓ & 26.0 \( 1.0\) \\  & + w rationale & ✗ & 9.7 \( 1.6\) \\     } & None & - & **59.0**\( 1.5\) \\  & + w rationale & ✓ & 39.6 \( 0.4\) \\  & + w rationale & ✗ & 41.1 \( 1.0\) \\     } & None & - & **45.5**\( 0.4\) \\  & + w rationale & ✓ & 17.9 \( 0.4\) \\    } & None & - & **45.5**\( 0.4\) \\  & + w rationale & ✓ & 17.9 \( 0.4\) \\    } &  & ✗ & 6.8 \( 2.2\) \\   

Table 6: Average performance (%) of the fine-tuned models on the extended datasets generated by Claude-3-Opus over the two iterations. Blue cells indicate reduced performance after fine-tuning.

Figure 5: Full-mark rate (%) of GPT-4, ChatGPT, Claude and Gemini on the extended “sport understanding” datasets on different popularity levels. We depict the number of questions on different popularity levels. Despite an uneven distribution for popularity, we still can find that as the popularity of the subject entity increases, the difficulty of the question increases

**Data Leakage:** Data Leakage could be a crucial problem, study [48; 28] shows that a fine-tuned model can achieve perfect performance on benchmarks. To combat this, researchers have developed various detection methods [7; 29; 19; 36; 8; 34]. Study  proposed detecting exact matches between test examples and pretraining data. Work , proposed a contamination report for the open-sources model. The work  design two indicators using the perplexity to indicate the potential data leakage. However, these detection methods have limitations for they can not be applied to those closed-source models. In the same period of time  use the following-up questions to alleviate data leakage.

## 5 Conclusion

This paper presents two strategies for automating dataset updates toward reliable and timely LLM evaluation. The mimicking strategy generates new, similar samples based on existing ones, while the extending strategy further expands the generated sample using cogitation levels. Extensive experiments using eleven LLMs on updated samples from MMLU and BIG-Bench datasets indicate the stability of our strategies and effectiveness toward addressing benchmark leakage. In the future, we are interested in introducing external or domain-specific knowledge for dataset updates.