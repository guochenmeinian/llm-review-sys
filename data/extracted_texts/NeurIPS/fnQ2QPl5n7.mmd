# GUARD: A Safe Reinforcement Learning Benchmark

Weiye Zhao

Robotics Institute

Carnegie Mellon University

weijyezha@andrew.cmu.edu

&Rui Chen

Robotics Institute

Carnegie Mellon University

ruic3@andrew.cmu.edu

&Yifan Sun

Robotics Institute

Carnegie Mellon University

yifansu2@andrew.cmu.edu

&Ruixuan Liu

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

twei2@andrew.cmu.edu

&Changliu Liu

Robotics Institute

Carnegie Mellon University

cliu6@andrew.cmu.edu

&Rui Chen

Robotics Institute

Carnegie Mellon University

ruic3@andrew.cmu.edu

&Yifan Sun

Robotics Institute

Carnegie Mellon University

yifansu2@andrew.cmu.edu

&Ruixuan Liu

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

twei2@andrew.cmu.edu

&Changliu Liu

Robotics Institute

Carnegie Mellon University

cliu6@andrew.cmu.edu

&Ruixuan Liu

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Changliu Liu

Robotics Institute

Carnegie Mellon University

twei2@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

cliu6@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute
Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu
&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute
Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Cardifferent applications, ensuring a reliable transition from theory to practice. A benchmark includes 1) algorithms for comparison; 2) environments to evaluate algorithms; 3) a set of evaluation metrics, etc. There are benchmarks for unconfined RL and some safe RL, but not comprehensive enough (Duan et al., 2016; Brockman et al., 2016; Ellenberger, 2018-2019; Yu et al., 2019; Osband et al., 2020; Tunyasuvunakool et al., 2020; Dulac-Arnold et al., 2020; Zhang et al., 2022a).

To create a robust safe RL benchmark, we identify three essential pillars. Firstly, the benchmark must be **generalized**, accommodating diverse agents, tasks, and safety constraints. Real-world applications involve various agent types (e.g., drones, robot arms) with distinct complexities, such as different control degrees-of-freedom (DOF) and interaction modes (e.g., 2D planar or 3D spatial motion). The performance of algorithms is influenced by several factors, including variations in robots (such as observation and action space dimensions), tasks (interactive or non-interactive, 2D or 3D), and safety constraints (number, trespassibility, movability, and motion space). Therefore, providing a comprehensive environment to test the generalizability of safe RL algorithms is crucial.

Secondly, the benchmark should be **unified**, overcoming discrepancies in experiment setups prevalent in the emerging safe RL literature. A unified platform ensures consistent evaluation of different algorithms in controlled environments, promoting reliable performance comparison. Lastly, the benchmark must be **extensible**, allowing researchers to integrate new algorithms and extend setups to address evolving challenges. Given the ongoing progress in safe RL, the benchmark should incorporate major existing works and adapt to advancements. By encompassing these pillars, the benchmark provides a solid foundation for addressing these open problems in safe RL research.

In light of the above-mentioned pillars, this paper introduces GUARD, a **G**eneralized **U**nified **SA**fe **R**einforcement Learning **D**evelopment Benchmark. In particular, GUARD is developed based upon the Safety Gym (Ray et al., 2019), SafeRL-Kit (Zhang et al., 2022a) and SpinningUp (Achiam, 2018). Unlike existing benchmarks, GUARD pushes the boundary beyond the limit by significantly extending the algorithms in comparison, types of agents and tasks, and safety constraint specifications. The contributions of this paper are as follows:

1. **Generalized benchmark with a wide range of agents.** GUARD genuinely supports **11** different agents, covering the majority of real robot types.
2. **Generalized benchmark with a wide range of tasks.** GUARD genuinely supports **7** different task specifications, which can be combined to represent most real robot tasks.
3. **Generalized benchmark with a wide range of safety constraints.** GUARD genuinely supports **8** different safety constraint specifications. The included constraint options comprehensively cover the safety requirements that would encounter in real-world applications.
4. **Unified benchmarking platform with comprehensive coverage of safe RL algorithms.** Guard implements **8** state-of-the-art safe RL algorithms following a unified code structure.
5. **Highly customizable benchmarking platform.** GUARD features a modularized design that enables effortless customization of new testing suites with self-customizable agents, tasks, and constraints. The algorithms in GUARD are self-contained, with a consistent structure and independent implementations, ensuring clean code organization and eliminating dependencies between different algorithms. This self-contained structure greatly facilitates the seamless integration of new algorithms for further extensions.

## 2 Related Work

Open-source Libraries for Reinforcement Learning AlgorithmsOpen-source RL libraries are code bases that implement representative RL algorithms for efficient deployment and comparison. They often serve as backbones for developing new RL algorithms, greatly facilitating RL research. We divide existing libraries into two categories: (a) safety-oriented RL libraries that support safe RL algorithms, and (b) general RL libraries that do not. Among safety-oriented libraries, Safety Gym (Ray et al., 2019) is the most famous one with highly configurable tasks and constraints but only supports three safe RL methods. SafeRL-Kit (Zhang et al., 2022) supports five safe RL methods while missing some key methods such as CPO (Achiam et al., 2017). Bullet-Safety-Gym (Gronauer, 2022) supports CPO but is limited in overall safe RL support at totally four methods. Compared to the above libraries, our proposed GUARD doubles the support at eight methods in total, covering a wider spectrum of general safe RL research. General RL libraries, on the other hand, can be summarized according to their backend into PyTorch (Achiam, 2018; Weng et al., 2022; Raffin et al., 2021; Liang et al., 2018), Tensorflow (Dhariwal et al., 2017; Hill et al., 2018), Jax (Castro et al., 2018; Hoffman et al., 2020), and Keras (Plappert, 2016). In particular, SpinningUp (Achiam, 2018) serves as the major backbone of our GUARD benchmark on the safety-agnostic RL portion.

Benchmark Platform for Safe RL AlgorithmsTo facilitate safe RL research, the benchmark platform should support a wide range of task objectives, constraints, and agent types. Among existing work, the most representative one is Safety Gym (Ray et al., 2019) which is highly configurable. However, Safety Gym is limited in agent types in that it does not support high-dimensional agents (e.g., drone and arm) and lacks tasks with complex interactions (e.g., chase and defense). Moreover, Safety Gym only supports naive contact dynamics (e.g., touch and snap) instead of more realistic cases (e.g., objects bouncing off upon contact) in contact-rich tasks. Safe Control Gym (Yuan et al., 2022) is another open-source platform that supports very simple dynamics (i.e., cartpole, 1D/2D quadrotors) and only supports navigation tasks. Finally, Bullet Safety Gym (Gronauer, 2022) provides high-fidelity agents, but the types of agents are limited, and they only consider navigation tasks. Compared to the above platforms, our GUARD supports a much wider range of task objectives (e.g., 3D reaching, chase and defense) with a much larger variety of eight agents including high-dimensional ones such as drones, arms, ants, and walkers.

## 3 Preliminaries

Markov Decision ProcessAn Markov Decision Process (MDP) is specified by a tuple \((,,,,P,)\), where \(\) is the state space, and \(\) is the control space, \(:\) is the reward function, \(0<1\) is the discount factor, \(:\) is the starting state distribution, and \(P:\) is the transition probability function (where \(P(s^{}|s,a)\) is the probability of transitioning to state \(s^{}\) given that the previous state was \(s\) and the agent took action \(a\) at state \(s\)). A stationary policy \(:()\) is a map from states to a probability distribution over actions, with \((a|s)\) denoting the probability of selecting action \(a\) in state \(s\). We denote the set of all stationary policies by \(\). Suppose the policy is parameterized by \(\); policy search algorithms search for the optimal policy within a set \(_{}\) of parameterized policies.

The solution of the MDP is a policy \(\) that maximizes the performance measure \(()\) computed via the discounted sum of reward:

\[()=_{}[_{t=0}^{}^{t }(s_{t},a_{t},s_{t+1})],\] (1)

where \(=[s_{0},a_{0},s_{1},]\) is the state and control trajectory, and \(\) is shorthand for that the distribution over trajectories depends on \(:s_{0},a_{t}(|s_{t}),s_{t+1} P(|s_{t},a_{t})\). Let \(R()_{t=0}^{}^{t}(s_{t},a_{t},s_{t+1})\) be the discounted return of a trajectory. We define the on-policy value function as \(V^{}(s)_{}[R()|s_{0}=s]\), the on-policy action-value function as \(Q^{}(s,a)_{}[R()|s_{0}=s,a_{0}=a]\), and the advantage function as \(A^{}(s,a) Q^{}(s,a)-V^{}(s)\).

Constrained Markov Decision ProcessA constrained Markov Decision Process (CMDP) is an MDP augmented with constraints that restrict the set of allowable policies. Specifically, CMDP introduces a set of cost functions, \(C_{1},C_{2},,C_{m}\), where \(C_{i}:\) maps the state action transition tuple into a cost value. Similar to (1), we denote \(_{C_{i}}()=_{}[_{t=0}^{}^{t} C_{i}(s_{t},a_{t},s_{t+1})]\) as the cost measure for policy \(\) with respect to the cost function \(C_{i}\). Hence, the set of feasible stationary policies for CMDP is then defined as \(_{C}=\{\  i,_{C_{i}}() d_{i}\}\), where \(d_{i}\). In CMDP, the objective is to select a feasible stationary policy \(\) that maximizes the performance:\(_{C}}{}()\). Lastly, we define on-policy value, action-value, and advantage functions for the cost as \(V_{C_{i}}^{},Q_{C_{i}}^{}\) and \(A_{C_{i}}^{}\), which as analogous to \(V^{}\), \(Q^{}\), and \(A^{}\), with \(C_{i}\) replacing \(R\).

## 4 GUARD Safe RL Library

### Overall Implementation

GUARD contains the latest methods that can achieve safe RL: (i) end-to-end safe RL algorithms including CPO (Achiam et al., 2017), TRPO-Lagrangian (Bohez et al., 2019), TRPO-FAC (Ma et al., 2021), TRPO-IPO (Liu et al., 2020), and PCPO (Yang et al., 2020); and (ii) hierarchical safe RL algorithms including TRPO-SL (TRPO-Safety Layer) (Dalal et al., 2018) and TRPO-USL (TRPO-Unrolling Safety Layer) (Zhang et al., 2022). We also include TRPO (Schulman et al., 2015) as an unconstrained RL baseline. Note that GUARD only considers model-free approaches which rely less on assumptions than model-based ones. We highlight the benefits of our algorithm implementations in GUARD:

* GUARD comprehensively covers a **wide range of algorithms** that enforce safety in both hierarchical and end-to-end structures. Hierarchical methods maintain a separate safety layer, while end-to-end methods solve the constrained learning problem as a whole.
* GUARD provides a **fair comparison among safety components** by equipping every algorithm with the same reward-oriented RL backbone (i.e., TRPO (Schulman et al., 2015)), implementation (i.e., MLP policies with (Gulman et al., 2015; Gulman et al., 2015) hidden layers and tanh activation), and training procedures. Hence, all algorithms inherit the performance guarantee of TRPO.
* GUARD is implemented in PyTorch with a clean structure where every algorithm is self-contained, enabling **fast customization and development** of new safe RL algorithms. GUARD also comes with unified logging and plotting utilities which makes analysis easy.

### Unconstrained RL

TRPOWe include TRPO (Schulman et al., 2015) since it is state-of-the-art and several safe RL algorithms are based on it. TRPO is an unconstrained RL algorithm and only maximizes performance \(\). The key idea behind TRPO is to iteratively update the policy within a local range (trust region) of the most recent version \(_{k}\). Mathematically, TRPO updates policy via

\[_{k+1}=}{}\,() \,_{KL}(,_{k}),\] (2)

where \(_{KL}\) is Kullback-Leibler (KL) divergence, \(>0\) and the set \(\{_{}\ :\ _{KL}(,_{k})\}\) is called the _trust region_. To solve (2), TRPO applies Taylor expansion to the objective and constraint at \(_{k}\) to the first and second order, respectively. That results in an approximate optimization with linear objective and quadratic constraints (LOQC). TRPO guarantees a worst-case performance degradation.

### End-to-End Safe RL

CPOConstrained Policy Optimizaiton (CPO) (Achiam et al., 2017) handles CMDP by extending TRPO. Similar to TRPO, CPO also performs local policy updates in a trust region. Different from TRPO, CPO additionally requires \(_{k+1}\) to be constrained by \(_{}_{C}\). For practical implementation, CPO replaces the objective and constraints with surrogate functions (advantage functions), which can easily be estimated from samples collected on \(_{k}\), formally:

\[_{k+1}=}{}\,}}{}[A^{_{k}}(s,a)]\] (3) \[\ \ _{KL}(,_{k}),\ \ \ _{C_{i}}(_{k})+} A_{C_{i}}^{_{k}}(s,a) d_{i},i=1,,m.\]where \(d^{_{k}}(1-)_{t=0}^{H}^{t}P(s_{t}=s|_{k})\) is the discounted state distribution. Following TRPO, CPO also performs Taylor expansion on the objective and constraints, resulting in a Linear Objective with Linear and Quadratic Constraints (LOLQC). CPO inherits the worst-case performance degradation guarantee from TRPO and has a worst-case cost violation guarantee.

PcpoProjection-based Constrained Policy Optimization (PCPO) (Yang et al., 2020) is proposed based on CPO, where PCPO first maximizes reward using a trust region optimization method without any constraints, then PCPO reconciles the constraint violation (if any) by projecting the policy back onto the constraint set. Policy update then follows an analytical solution:

\[_{k+1}=_{k}+H^{-1}g}}H^{-1}g-0, H^{-1}g}}g_{c}^{}H^{-1}g+b}{g_{c}^{ }L^{-1}g_{c}}L^{-1}g_{c}\] (4)

where \(g_{c}\) is the gradient of the cost advantage function, \(g\) is the gradient of the reward advantage function, \(H\) is the Hessian of the KL divergence constraint, \(b\) is the constraint violation of the policy \(_{k}\), \(L=\) for \(L_{2}\) norm projection, and \(L=H\) for KL divergence projection. PCPO provides a lower bound on reward improvement and an upper bound on constraint violation.

TRPO-Lagrangian methods solve constrained optimization by transforming hard constraints into soft constraints in the form of penalties for violations. Given the objective \(()\) and constraints \(\{_{C_{i}}() d_{i}\}_{i}\), TRPO-Lagrangian (Bohez et al., 2019) first constructs the dual problem

\[ 0}{}}{}-()+_{i}_{i}(_{C_{i}} ()-d_{i}).\] (5)

The update of \(\) is done via a trust region update with the objective of (2) replaced by that of (5) while fixing \(_{i}\). The update of \(_{i}\) is done via standard gradient ascend. Note that TRPO-Lagrangian does not have a theoretical guarantee for constraint satisfaction.

Trepo-FacInspired by Lagrangian methods and aiming at enforcing state-wise constraints (e.g., preventing state from stepping into infeasible parts in the state space), Feasible Actor Critic (FAC) (Ma et al., 2021) introduces a multiplier (dual variable) network. Via an alternative update procedure similar to that for (5), TRPO-FAC solves the _statewise_ Lagrangian objective:

\[}{}}{}-()+_{i}_{s d^{_{k}}} [_{_{i}}(s)(_{C_{i}}()-d_{i})],\] (6)

where \(_{_{i}}(s)\) is a parameterized Lagrangian multiplier network and is parameterized by \(_{i}\) for the \(i\)-th constraint. Note that TRPO-FAC does not have a theoretical guarantee for constraint satisfaction.

Trepo-IpoTrepo (Liu et al., 2020) incorporates constraints by augmenting the optimization objective in (2) with logarithmic barrier functions, inspired by the interior-point method (Boyd and Vandenberghe, 2004). Ideally, the augmented objective is \(I(_{C_{i}}()-d_{i})=0\) if \(_{C_{i}}()-d_{i} 0\) or \(-\) otherwise. Intuitively, that enforces the constraints since the violation penalty would be \(-\). To make the objective differentiable, \(I()\) is approximated by \((x)=(-x)/t\) where \(t>0\) is a hyperparameter. Then TRPO-IPO solves (2) with the objective replaced by \(_{}()=()+_{i}(_{C_{ i}}(x)-d_{i})\). TRPO-IPO does not have theoretical guarantees for constraint satisfaction.

### Hierarchical Safe RL

Safety Layer(Dalal et al., 2018), added on top of the original policy network, conducts a quadratic-programming-based constrained optimization to project reference action into the nearest safe action. Mathematically:

\[a_{t}^{safe}=}\|a-a_{t}^{ ref}\|^{2} i,_{_{i}}(s_{t})^{}a+C_{i}(s_{t -1},a_{t-1},s_{t}) d_{i}\] (7)

where \(a_{t}^{ref}_{k}(|s_{t})\), and \(_{_{i}}(s_{t})^{}a_{t}+C_{i}(s_{t-1},a_{t-1},s_{t}) C _{i}(s_{t},a_{t},s_{t+1})\) is a \(\) parameterized linear model. If there's only one constraint, (7) has a closed-form solution.

UslUnrolling Safety Layer (USL) (Zhang et al., 2022b) is proposed to project the reference action into safe action via gradient-based correction. Specifically, USL iteratively updates the learned \(Q_{C}(s,a)\) function with the samples collected during training. With step size \(\) and normalization factor \(\), USL performs gradient descent as \(a_{t}^{safe}=a_{t}^{ref}-}^{ref}}Q_{C}(s_{t},a_{t}^{ref})-d\).

## 5 GUARD Testing Suite

### Robot Options

In GUARD testing suite, the agent (in the form of a robot) perceives the world through sensors and interacts with the world through actuators. Robots are specified through MuJoCo XML files. The suite is equipped with **8** types of pre-made robots that we use in our benchmark environments as whison in Figure 1. The action space of the robots are continuous, and linearly scaled to [-1, +1].

**Swimmer** consist of three links and two joints. Each joint connects two links to form a linear chain. Swimmer can move around by applying **2** torques on the joints.

**Ant** is a quadrupedal robot composed a torso and four legs. Each of the four legs has a hip joint and a knee joint; and can move around by applying **8** torques to the joints.

**Walker** is a bipedal robot that consists of four main parts - a torso, two thighs, two legs, and two feet. Different from the knee joints and the ankle joints, each of the hip joints has three hinges in the \(x\), \(y\) and \(z\) coordinates to help turning. With the torso height fixed, Walker can move around by controlling **10** joint torques.

**Humanoid** is also a bipedal robot that has a torso with a pair of legs and arms. Each leg of Humanoid consists of two joints (no ankle joint). Since we mainly focus on the navigation ability of the robots in designed tasks, the arm joints of Humanoid are fixed, which enables Humanoid to move around by only controlling **6** torques.

**Hopper** is a one-legged robot that consists of four main parts - a torso, a thigh, a leg, and a single foot. Similar to Walker, Hopper can move around by controlling **5** joint torques.

**Arm3** is designed to simulate a fixed three-joint robot arm. Arm is equipped with multiple sensors on each links in order to fully observe the environment. By controlling **3** joint torques, Arm can move its end effector around with high flexibility.

**Arm6** is designed to simulate a robot manipulator with a fixed base and six joints. Similar to Arm3, Arm6 can move its end effector around by controlling **6** torques.

**Drone** is designed to simulate a quadrotor. The interaction between the quadrotor and the air is simulated by applying four external forces on each of the propellers. The external forces are set to balance the gravity when the control action is zero. Drone can move in 3D space by applying **4** additional control forces on the propellers.

### Task Options

We categorize robot tasks in two ways: (i) interactive versus non-interactive tasks, and (ii) 2D space versus 3D space tasks. 2D space tasks constrain agents to a planar space, while 3D space tasks do not. Non-interactive tasks primarily involve achieving a target state (e.g., trajectory tracking) while

Figure 1: Robots of our environments.

interactive tasks (e.g., human-robot collaboration and unstructured object pickup) necessitate contact or non-contact interactions between the robot and humans or movable objects, rendering them more challenging. On a variety of tasks that cover different situations, GUARD facilitates a thorough evaluation of safe RL algorithms via the following tasks. See Table 17 for more information.

**Goal** (Figure 1(a)) requires the robot navigating towards a series of 2D or 3D goal positions. Upon reaching a goal, the location is randomly reset. The task provides a sparse reward upon goal achievement and a dense reward for making progress toward the goal.

**Push** (Figure 1(b)) requires the robot pushing a ball toward different goal positions. The task includes a sparse reward for the ball reaching the goal circle and a dense reward that encourages the agent to approach both the ball and the goal. Unlike pushing a box in Safety Gym, it is more challenging to push a ball since the ball can roll away and the contact dynamics are more complex.

**Chase** (Figure 1(c)) requires the robot tracking multiple dynamic targets. Those targets continuously move away from the robot at a slow speed. The dense reward component provides a bonus for minimizing the distance between the robot and the targets. The targets are constrained to a circular area. A 3D version of this task is also available, where the targets move within a restricted 3D space. Detailed dynamics of the targets is described in Appendix A.5.1.

**Defense** (Figure 1(d)) requires the robot to prevent dynamic targets from entering a protected circle area. The targets will head straight toward the protected area or avoid the robot if the robot gets too close. Dense reward component provides a bonus for increasing the cumulative distance between the targets and the protected area. Detailed dynamics of the targets is described in Appendix A.5.2.

### Constraint Options

We classify constraints based on various factors: **trespassibility**: whether constraints are trespassable or untrespassable. Trespassable constraints allow violations without causing any changes to the robot's behaviors, and vice versa. (ii) **movability**: whether they are immovable, passively movable, or actively movable; and (iii) **motion space**: whether they pertain to 2D or 3D environments. To cover a comprehensive range of constraint configurations, we introduce additional constraint types via expanding Safety Gym. Please refer to Table 18 for all configurable constraints.

**3D Hazards** (Figure 2(a)) are dangerous 3D areas to avoid. These are floating spheres that are trespassable, and the robot is penalized for entering them.

**Ghosts** (Figure 2(b)) are dangerous areas to avoid. Different from hazards, ghosts always move toward the robot slowly, represented by circles on the ground. Ghosts can be either trespassable or untrespassable. The robot is penalized for touching the untrespassable ghosts and entering the trespassable ghosts. Moreover, ghosts can be configured to start chasing the robot when the distance from the robot is larger than some threshold. This feature together with the adjustable velocity allows users to design the ghosts with different aggressiveness. Detailed dynamics of the targets is described in Appendix A.5.3.

**3D Ghosts** (Figure 2(c)) are dangerous 3D areas to avoid. These are floating spheres as 3D version of ghosts, sharing the similar behaviour with ghosts.

Figure 2: Tasks of our environments.

[MISSING_PAGE_FAIL:8]

Figure 4: Comparison of results from four representative tasks. (a) to (d) cover four robots on the goal task. (e) shows the performance of a task with ghosts. (f) to (h) cover three different tasks with the point robot.