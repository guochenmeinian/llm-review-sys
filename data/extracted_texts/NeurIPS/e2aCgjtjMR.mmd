# Estimating and Controlling for Equalized Odds via Sensitive Attribute Predictors

Beepul Bharti

Johns Hopkins University

bbharti1@jhu.edu

&Paul Yi

University of Maryland

pyi@som.umaryland.edu

&Jeremias Sulam

Johns Hopkins University

jsulam1@jhu.edu

###### Abstract

As the use of machine learning models in real world high-stakes decision settings continues to grow, it is highly important that we are able to audit and control for any potential fairness violations these models may exhibit towards certain groups. To do so, one naturally requires access to sensitive attributes, such as demographics, biological sex, or other potentially sensitive features that determine group membership. Unfortunately, in many settings, this information is often unavailable. In this work we study the well known _equalized odds_ (EOD) definition of fairness. In a setting without sensitive attributes, we first provide tight and computable upper bounds for the EOD violation of a predictor. These bounds precisely reflect the worst possible EOD violation. Second, we demonstrate how one can provably control the worst-case EOD by a new post-processing correction method. Our results characterize when directly controlling for EOD with respect to the predicted sensitive attributes is - and when is not - optimal when it comes to controlling worst-case EOD. Our results hold under assumptions that are milder than previous works, and we illustrate these results with experiments on synthetic and real datasets.

## 1 Introduction

Machine learning (ML) algorithms are increasingly used in high stakes prediction applications that can significantly impact society. For example, ML models have been used to detect breast cancer in mammograms , inform parole and sentencing decisions , and aid in loan approval decisions . While these algorithms often demonstrate excellent overall performance, they can be dangerously unfair and negatively impact under-represented groups . Some of these unforeseen negative consequences can even be fatal, e.g. deep learning models for chest x-ray disease classification exhibit under diagnosis bias towards certain sensitive groups . Recommendations to ensure that ML systems do not exacerbate societal biases have been raised by several groups, including the White House, which in 2016, released a report on big data, algorithms, and civil rights . It is thus critical to understand how to rigorously evaluate the fairness of ML algorithms and control for unfairness during model development.

These needs have prompted considerable research in the area of _Fair ML_. While there exist many definitions of algorithmic fairness , common notions of _group fairness_ consider different error rates of a predictor across different groups: males and females, white and non-white, etc. For example, the _equal opportunity_ criterion requires the true positive rate (TPR) be equal across both groups, while _equalized odds_ requires both TPR and false positive rate (FPR) to be the same across groups . Obtaining predictors that are fair therefore requires enforcing these constraints on error rates across groups during model development, which can be posed as a constrained (or regularized) optimization problem . Alternatively, one can devise post-processing strategies to modify a certain predictor to correct for differences in TPR and FPR , or even include datapre-processing steps that ensure that unfair models could not be obtained from such data to begin with [39; 8].

Naturally, all these techniques for estimating or enforcing fairness require access to a dataset with features, \(X\), responses, \(Y\), and sensitive attributes, \(A\). However, in many settings this is difficult or impossible, as datasets often do not include samples that have all these variables. This could be because the sensitive attribute data was withheld due to privacy concerns, which is very common with medical data due to HIPAA federal law requirements, or simply because it was deemed unnecessary to record [40; 43]. A real-world example of this is in the recent Kaggle-hosted RSNA Chest X-Ray Pneumonia Detection Challenge . Even though this dataset of chest x-rays was painstakingly annotated for pneumonia disease by dozens of radiologists, it did not include sensitive attributes (e.g., age, sex, and race), precluding the evaluation of fairness of the models developed as part of the challenge. In settings like this, where there is limited or no information on the sensitive attribute of interest, it is still important to be able to accurately estimate the violation of fairness constraints by a ML classifier and to be able to alleviate these violations before deploying it in a sensitive application

This leads to the natural question:

_How can one assess and control the fairness of a classifier without having access to sensitive attribute data?_

In other words, how can we measure and potentially control the fairness violations of a classifier for \(Y\) with respect to a sensitive attribute \(A\), when we have no data that jointly observes \(A\) and \(Y\)?

### Related Work

Estimating unfairness and, more importantly, developing fair predictors where there is no - or only partial - information about the sensitive attribute has only recently received increasing attention. The recent work by Zhao et al.  explores the perspective of employing features that are correlated with the sensitive attribute, and shows that enforcing low correlation with such "fairness related features" can lead to models with lower bias. Although these techniques are promising, they require domain expertise to determine which features are highly correlated with the sensitive attribute. An appealing alternative that has been studied is the use proxy sensitive attributes that are created by a second predictor trained on a different data set that contains only sensitive attribute information [20; 25; 10; 5]. This strategy has been widely adopted in many domains such as healthcare , finance , and politics . While using sensitive attribute predictors has proven to be an effective and practical solution, it must be done with care, as this opens new problems for estimating and controlling for fairness. The work by Prost et al.  considers the estimation of fairness in a setting where one develops a predictor for an unobserved covariate, but it does not contemplate predicting the sensitive attribute itself. On the other hand Chen et al.  study the sources of error in the estimation of fairness via predicted proxies computed using threshold functions, which are prone to over-estimation.

The closest to our work are the recent results by Kallus et al. , and Awasthi et al. [5; 4]. Kallus et al.  study the identifiability of fairness violations under general assumptions on the distribution and classifiers. They show that, in the absence of the sensitive attribute, the fairness violation of predictions, \(\), is unidentifiable unless strict assumptions 1 are made or if there is some common observed data over \(A\) and \(Y\). Nonetheless, they show not all hope is lost and provide closed form upper and lower bounds of the fairness violations of \(\) under the assumption that one has two datasets: one that is drawn from the marginal over \((X,A)\) and the other drawn from the marginal over \((X,Y,)\). Their analysis, however, does not consider predictors \(=f(X)\) or \(=h(X)\), and instead their bounds depend explicitly on the conditional probabilities, \((A X)\) and \((,Y X)\), along with the distribution over the features, \((X)\). Unfortunately, with this formulation, it is unclear how the bounds would change if the estimation of the conditional probabilities was inaccurate (as is the case when developing predictors \(\) in the real world). Furthermore, in settings where \(X\) is high dimensional (as for image data), calculating such bounds would become intractable. Since the fairness violation cannot be directly modified, they then study when these bounds can be reduced and improved. However, they do so in settings that impose smoothness assumptions over \((X,A,Y,)\)which clearly are not-verifiable without data over the complete joint distribution. As a result, their results do not provide any actionable method that could improve the bounds.

Awasthi et al. , on the other hand, make progress in understanding properties of the sensitive attribute predictor, \(\), that are desirable for easy and accurate fairness violation estimation and control of a classifier \(\). Assuming that \(\!\!\!(A,Y)\), they demonstrate that the true fairness violation is in fact proportional to the estimated fairness violation (the fairness violation using \(\) in lieu of A). This relationships yields the counter-intuitive result that given a fixed error budget for the sensitive attribute predictor, the optimal attribute predictor for the estimation of the true fairness violation is one with the most unequal distribution of errors across the subgroups of the sensitive attribute. However, one is still unable to actually calculate the true fairness violation - as it is unidentifiable. Nonetheless, the relationship does demonstrates that if one can maintain the assumption above while controlling for fairness with respect to \(\), then doing so will provably reduce the true fairness violation with respect to \(A\). Unfortunately, while these rather strict assumptions can be met in some limited scenarios (as in ), these are not applicable in general - and cannot even be tested without access to data over \((A,Y)\).

Overall, while progress has been made in understanding how to estimate and control fairness violations in the presence of incomplete sensitive attribute information, these previous results highlight that this can only be done in simple settings (e.g., having access to some data from the entire distribution, or by making strong assumptions of conditional independence). Moreover, it remains unclear whether tight bounds can be obtained that explicitly depend on the properties of the predictor \(\), allowing for actionable bounds that can provably mitigate for its fairness violation without having an observable sensitive attribute or making stringent assumptions.

### Contributions

The contributions of our work can be summarized as follows:

* We study the well known equalized odds (EOD) definition of fairness in a setting where the sensitive attributes, \(A\), are not observed with the features \(X\) and labels \(Y\). We provide tight and computable bounds on the EOD violation of a classifier, \(=f(X)\). These bounds represent the _worst-case_ EOD violation of \(f\) and employ a predictor for the sensitive attributes, \(=h(X)\), obtained from a sample over the distribution \((X,A)\).
* We provide a precise characterization of the classifiers that achieve _minimal worst-case_ EOD violations with respect to unobserved sensitive attributes. Through this characterization, we demonstrate _when_ simply correcting for fairness with respect to the proxy sensitive attributes will yield _minimal worst-case_ EOD violations, and _when_ instead it proves to be sub-optimal.
* We provide a simple and practical post-processing technique that provably yields classifiers that maximize prediction power while achieving _minimal worst-case EOD violations_ with respect to unobserved sensitive attributes.
* We illustrate our results on a series of simulated and real data of increasing complexity.

## 2 Problem Setting

We work within a binary classification setting and consider a distribution \(\) over \(()\) where \(^{n}\) is the feature space, \(=\{0,1\}\) the label space, and \(=\{0,1\}\) the sensitive attribute space. Furthermore, and adopting the setting of , we consider 2 datasets, \(_{1}\) and \(_{2}\). The former is drawn from the marginal over \((,)\) of \(\) while \(_{2}\) is drawn from the marginal \((,)\) of \(\). In this way, \(_{1}\) and \(_{2}\) contain the same set of features, \(_{1}\) contains sensitive attribute information and \(_{2}\) contains label information. The drawn samples in \(_{1}\) and \(_{2}\) are i.i.d over their respective marginals and different from one another.

Similar to previous work [10; 34; 5], we place ourselves in a _demographically scarce_ regime where there is designer who has access to \(_{1}\) to train a sensitive attribute predictor \(h:\) and a developer, who has access to \(_{2}\), the sensitive attribute classifier \(h\), and all computable probabilites \((h(X),A)\) that the designer of \(h\) can extract from \(_{1}\). In this setting, the goal of the developer is to learn a classifier \(f:\) (from \(_{2}\)) that is fair with respect to \(A\) utilizing \(\). The central idea is to augment every sample in \(_{2}\), \((x_{i},y_{i})\), by \((x_{i},y_{i},_{i})\), where \(_{i}=h(x_{i})\). Intuitively, if the error of the sensitive attribute predictor, denoted herein by \(U=(h(X) A)\), is low, we could hope that fairness with respect to the real (albeit unobserved) sensitive attribute can be faithfully estimated. Our goal is to thus estimate the error incurred in measuring and enforcing fairness constraints by means of \(=h(X)\), and potentially alleviate or control for it.

Throughout the remainder of this work we focus on _equalized odds_ (EOD) as our fairness metric  of interest as it is one of the most popular notions of fairness. Thus moving forward, the term fairness refers specifically to EOD. We denote \(=f(X)\) for simplicity, and for \(i,j\{0,1\}\) define the group conditional probabilities

\[_{i,j}=(=1 A=i,Y=j).\] (1)

These probabilities quantify the TPR (when \(j=1\)) and FPR (when \(j=0\)), for either protected group (\(i=0\) or \(i=1\)). We assume that the base rates, \(r_{i,j}=(A=i,Y=j)>0\) so that these quantities are not undefined. With these conditionals probabilities, we define the true fairness violation of \(f\), with the two quantities

\[_{}(f)=_{1,1}-_{0,1}_{ }(f)=_{1,0}-_{0,0}\] (2)

which respectively quantify the absolute difference in TPR and FPRs among the two protected groups.

We also need to characterize the performance of the sensitive attribute classifier, \(h\). The miss-classification error of \(h\) can be decomposed as, \(U=U_{0}+U_{1}\) where \(U_{i}=(=i,A i)\), for \(i\{0,1\}\). We define the difference in errors to be

\[ U=U_{0}-U_{1}.\] (3)

In a demographically scarce regime, the rates \(r_{i,j}\), and more importantly the quantities of interest, \(_{}(f)\) and \(_{}(f)\), cannot be computed because samples from \(A\) and \(Y\) are not jointly observed. However, using the sensitive attribute classifier \(h\), we can predict \(\) on \(_{2}\) and compute

\[_{i,j}=(=i,Y=j)_{i,j}=(=1=i,Y=j),\]

which serve as the estimates for the true base rates and group TPRs and FPRs.

## 3 Theoretical Results

With the setting defined, we will now present our results. The first result provides computable bounds on the true fairness violation of \(f\) with respect to the true, but unobserved, sensitive attribute \(A\). The bounds precisely characterize the _worst-case_ fairness violation of \(f\). Importantly, as we will explain later, this first result will provide insight into what properties \(f\) must satisfy so that its worst-case fairness violation is _minimal_. In turn, these results will lead to a simple post-processing method that can correct a pretrained classifier \(f\) into another one, \(\), that has minimal worst-case fairness violations. Before presenting our findings, we first describe the key underlying assumption we make about the pair of classifiers, \(h\) and \(f\), so that the subsequent results are true.

**Assumption 1**.: _For \(i,j\{0,1\}\), the classifiers \(=f(X)\) and \(=h(X)\) satisfy_

\[}{_{i,j}}_{i,j} 1-}{_{i,j}}.\] (4)

To parse this assumption, it is easy to show that this is met when a) \(h\) is accurate enough for the setting, namely that \((=i,A i)(=i,Y=j)\), and b) the predictive power of \(h\) is better than the ability of \(f\) to predict the labels, \(Y\) - or more precisely, \((=i,A i)(=j,=i,Y  j)\). We refer the reader to Appendix A for a thorough explanation for why this is true. While this assumption may seem limiting, this is milder than those in existing results: First, accurate predictors \(h\) can be developed [6; 17; 23; 19], thus satisfying the assumption (our numerical results will highlight this fact as well). Second, other works [5; 9], require assumptions on \(\) and \(\) that are unverifiable in a demographically scarce regime. Our assumption, on the other hand, can always be easily verified because all the quantities are computable. Furthermore, this assumption can be also be relaxed if one desires partial guarantees. More specifically, if the above is true for \(i\{0,1\}\) and only for \(j=1\), then all subsequent results for \(_{}\) hold. Similarly, if it only holds for \(j=0\), the results for \(_{}\) hold. Therefore, if one only cares about the equal opportunity definition of fairness, as an example, then this only needs to hold for \(i\{0,1\}\) and \(j=1\).

### Bounding Fairness Violations with Proxy Sensitive Attributes

With Assumption 1 in place, we present our main result.

**Theorem 1** (Bounds on \(_{}(f)\) and \(_{}(f)\)).: _Under Assumption 1, we have that_

\[|_{}(f)|& B_{}(f)}{{=}}\{|B_{1}+C_{0,1}|,|B_{1}-C_{1,1}|\} \\ |_{}(f)|& B_{}(f) }{{=}}\{|B_{0}+C_{0,0}|,|B_{0}-C_{1,0}|\} \] (5)

_where_

\[B_{j}=_{1,j}}{_{1,j}+ U}_{1,j}- {_{0,j}}{_{0,j}- U}_{0,j}  C_{i,j}=U_{i}(_{1,j}+ U}+_{0,j}- U}).\]

_Furthermore, the upper bounds for \(|_{}(f)|\) and \(|_{}(f)|\) are tight._

The proof, along with all others in this work, are included in Appendix A. We now make a few remarks on this result. First, the bound is tight in that there exists settings (albeit unlikely) with particular marginal distributions such that the bounds hold with equality. Second, even though \(|_{}(f)|\) and \(|_{}(f)|\) cannot be calculated, a developer can still calculate the _worst-case_ fairness violations, \(B_{}(f)\) and \(B_{}(f)\), because these depend on quantities that are all computable in practice. Thus, if \(B_{}(f)\) and \(B_{}(f)\) are low, then the developer can proceed having a guarantee on the maximal fairness violation of \(f\), even while not observing \(|_{}(f)|\) and \(|_{}(f)|\). On the other hand, if these bounds are large, this implies a _potentially_ large fairness violation over the protected group \(A\) by \(f\) and the developer should not proceed in deploying \(f\) and instead seek to learn another classifier with smaller bounds. Third, the obtained bounds are linear in the parameters \(_{i,j}\), which the developer _can adjust_ as they are properties of \(f\): this will become useful shortly.

### Optimal Worst-Case Fairness Violations

Given the result above, what properties should classifiers \(f\) satisfy such that \(B_{}(f)\) and \(B_{}(f)\) are minimal? Moreover, are the classifiers \(f\) that are fair with respect to \(\), the ones that have _smallest_\(B_{}(f)\) and \(B_{}(f)\)? We now answer these questions in the following theorem.

**Theorem 2** (Minimizers of \(B_{}(f)\) and \(B_{}(f)\)).: _Let \(=h(X)\) be a fixed sensitive attribute classifier with errors \(U_{0}\) and \(U_{1}\) that produces rates \(_{i,j}=(=i,Y=j)\). Let \(\) be the set of all predictors of \(Y\), parameterized by rates \(_{i,j}\), that, paired with \(h\), satisfy Assumption 1. Then, \(\) with group conditional probabilities, \(_{i,j}=(=1=i,Y=j)\) that satisfies the following condition,_

\[_{0,j}}{_{0,j}- U}_{0,j}-_{1,j}}{_{1,j}+ U}_{1,j}=(_{1,j}+ U}+_{0,j}- U}).\] (6)

_Furthermore, any such \(\) has minimal maximal fairness violation, i.e. \(\),_

\[|_{}()| B_{}() B_{ }()|_{}()| B_{}() B_{}().\] (7)

This result provides a precise characterization of the conditions that lead to minimal worst-case fairness violations. Observe that if \( U 0\), the classifier with minimal \(B_{}(f)\) and \(B_{}(f)\) involves \(_{i,j}\) such that \(_{1,j}_{0,j}\), i.e. it is _not_ fair with respect to \(\). On the other hand, if the errors of \(h\) are balanced (\( U=0\)), then minimal bounds are achieved by being fair with respect to \(\).

### Controlling Fairness Violations with Proxy Sensitive Attributes

Now that we understand what conditions \(f\) must satisfy so that it's worst case fairness violations are minimal, what remains is a method to obtain such a classifier. We take inspiration from the post-processing method proposed by Hardt et al. , which derives a classifier \(=(X)\) from \(=f(X)\) that satisfies equalized odds with respect to a sensitive attribute \(A\) while minimizing an expected missclassification loss - _only applicable if one has access to \(A\)_, which is not true in our setting. Nonetheless, since our method will generalize this idea, we first briefly comment on thisapproach. The method they propose works as follows: given a sample with initial prediction \(=\) and sensitive attribute \(A=a\), the derived predictor \(\), with group conditional probabilities \(_{i,j}=(=1 A=i,Y=j)\), predicts \(=1\) with probability \(p_{a,}=(=1 A=a,=)\). The four probabilities \(p_{0,0},p_{0,1},p_{1,0},p_{1,1}\) can be then calculated so that \(\) satisfies equalized odds and the expected loss between \(\) and labels \(Y\), i.e. \([L(,Y)]\), is minimized. The fairness constraint, along with the objective to minimize the expected loss, give rise to the linear program:

**Equalized Odds Post-Processing (Hardt et al. )**

\[_{p_{a,}}[L(,Y)]_{0,j}=_{1,j} j \{0,1\}.\] (8)

Returning to our setting where we _do not_ have access to \(A\) but only proxy variables \(=h(X)\), we seek classifiers \(f\) that are fair with respect to the sensitive attribute \(A\). Since these attributes are not available, (thus rendering the fairness violation to be unidentifiable ), a natural alternative is to minimize the worst-case fair violation with respect to \(A\), which _can_ be computed as shown in Theorem 1. Of course, such an approach will only minimize the worst case fairness and one cannot certify that the true fairness violation will decrease because - as explained above - it is unidentifiable. Nonetheless, since we know what properties _optimal_ classifiers must satisfy (as per Theorem 2), we can now modify the above problem to construct a corrected classifier, \(\), as follows. First, we must employ \(\) in place of \(A\), which amounts to employing \(_{i,j}\) in lieu of \(_{i,j}\). To this end, denote the (corrected) group conditional probabilities of \(\) to be \(}_{i,j}\). Second, the equalized odds constraint is replaced with the constraint in Theorem 2. Lastly, we also enforce the additional constraints detailed in Assumption 1 on the \(}_{i,j}\). With these modifications in place, we present the following generalized linear program:

**Worst-case Fairness Violation Reduction**

\[_{p_{a,}}[L(,Y)]\] (9) subject to \[_{0,j}}{_{0,j}+ U}_{0,j}-_{1,j}}{_{1,j}- U}_{1,j}= { U}{2}(_{1,j}+ U}+_{0,j}-  U})\] \[}{_{i,j}}}_{i,j}  1-}{_{i,j}} i,j\{0,1\}.\]

The solution to this linear program will yield a classifier \(\) that satisfies Assumption 1, has minimal \(B_{}()\) and \(B_{}()\), and has minimal expected loss. Note, that if \( U=0\), then the coefficients of \(}_{0,j}\) and \(}_{1,j}\) will equal one, and so the first set of constraints simply reduces to \(}_{0,j}=}_{1,j}\) for \(j\{0,1\}\) and the linear program above is precisely the post-processing method of  with \(\) in place of \(A\) (while enforcing Assumption 1).

Let us briefly recap the findings of this section: We have shown that in a demographically scarce regime, one can provide an upper bound on the true fairness violation of a classifier (Theorem 1). Second, we have presented a precise characterization of the classifiers with minimal worst-case fairness violations. Lastly, we have provided a simple and practical post-processing method (a linear program) that utilizes a sensitive attribute predictor to construct classifiers with minimal worst-case fairness violations with respect to the true, unknown, sensitive attribute.

## 4 Experimental Results

We now illustrate our theoretical results on synthetic and real world datasets. The code and data necessary to reproduce these experiments are available at https://github.com/Sulam-Group/EOD-with-Proxies.

### Synthetic Data

We begin with a synthetic example that allows us to showcase different aspects of our results. The data consists of 3 features, \(X_{1},X_{2},X_{3}\), sensitive attribute \(A\{0,1\}\), and response \(Y\{0,1\}\)The features are sampled from \((X_{1},X_{2},X_{3})(,)\), with

\[=1\\ -1\\ 0=1&0.05&0\\ 0.05&1&0\\ 0&0&0.05.\]

The sensitive attribute, \(A\), response \(Y\), and classifier \(f\), are modeled as

\[A =[(X_{3}+0.1) 0]\] \[Y X,A (S(X_{1}+X_{2}+X_{3}+_{0}(1-A)+ _{1}A))\] \[f(X;c_{1},c_{2}) =(S(c_{1}X_{1}+c_{2}X_{2}+c_{3}X_{3}) 0.5)\]

where \(()\) is the indicator function, \(S()\) is the sigmoid function and \(c_{1},c_{2},c_{3}(1,0.01)\). Furthermore, \(_{0}(0,1),_{1}(0,0.5)\) are independent noise variables placed on the samples belonging to the groups \(A=0\) and \(A=1\) to ensure there is a non trivial fairness violation. Specifically, \((_{0})>(_{1})\) makes \(f(X;c_{1},c_{2},c_{3})\) unfair with respect to \(A=0\). Lastly, to measure the predictive capabilities of \(f\), we use the loss function, \(L( y,Y=y)=(Y y)\), as this maximizes the well known Youden's Index2.

#### 4.1.1 Equal Errors (\(=0}\))

We model the sensitive attribute predictor as

\[h(X;)=((X_{3}+0.1+) 0)\]

where \((0,^{2})\). We choose \(^{2}\) so that \(h(X)\) has a total error \(U 0.04\) distributed so that \( U 0\) with \(U_{0} U_{1} 0.02\). We generate 1000 classifiers \(f\) and for each one, calculate \(_{}\), \(_{}\), \(B_{}\), \(B_{}\), and \([L(f,Y)]\). Then, on each \(f\), we run the (naive) equalized odds post processing algorithm to correct for fairness with respect \(\) to yield a classifier \(f_{}}\), and we also run our post-processing algorithm to yield an optimal classifier \(f_{}\). For both sets of classifiers we again calculate the same quantities.

The results in Fig. 1 present the true fairness violations, worst-case fairness violations, and expected loss for the 3 sets of different classifiers. Observe that both \(B_{}\) and \(B_{}\) are significantly lower for \(f_{}}\) and \(f_{}\)_and_ that these values for both sets of classifiers are approximately the same. This is expected as \(U_{0} U_{1}\) and so performing the fairness correction algorithm and our proposed algorithm amount to solving nearly identical linear programs. We also show the true fairness violations, \(_{}\) and \(_{}\) for all the classifiers to portray the gap between the bounds and the true values. As mentioned before, these true values _cannot be calculated_ in a real demographically scarce regime. Nonetheless, the developer of \(f\) now knows, post correction, that \(|_{}|,|_{}| 0.15\). Lastly, observe that the expected loss for \(f_{}}\) and \(f_{}\) are naturally higher compared to that of \(f\), however the increase in loss is minimal.

#### 4.1.2 Unequal Errors (\( 0}\))

We model the sensitive attribute predictor in the same way as in the previous experiment except with \(=c\), for a constant \(c\) so that the sensitive attribute classifier still has the same total error of

Figure 1: Synthetic data (\( U=0\)): true fairness violations, worst-case fairness violations, and expected loss for \(f\), \(f_{}}\), and \(f_{}\)

\(U 0.04\) but distributed unevenly so that \( U=-0.04\) with \(U_{0}=0\) and \(U_{1} 0.04\). As in the previous experiment we generate classifiers \(f\), and perform the same correction algorithms to yield \(f_{}}\) and \(f_{}\) and present the same metrics as before.

The results in Fig. 2 depict the worst-case fairness violations and expected loss for the 3 sets of different classifiers. Observe that our correction algorithm yields classifiers, \(f_{}\), that have significantly lower \(B_{}\) and \(B_{}\). Furthermore, observe that the \(f_{}}\) that results from performing the naive fairness correction algorithm in fact have higher \(B_{}\) the original classifier \(f\)! Even though the total error \(U\) has remained the same, its imbalance showcases the optimality of our correction method. Lastly, observe that there is a trade-off, albeit slight, in performing our correction algorithm. The expected loss for \(f_{}\) is higher than that of \(f_{}}\) and \(f\).

### Real World Data

We will now showcase our results on various real world datasets and prediction tasks. We provide a brief description of each task below and further experimental details are included in Appendix B.

**FIFA 2020 (Awasthi et al. )**: The task is to learn a classifier \(f\), using FIFA 2020 player data , that determines if a soccer players wage is above (\(Y=1\)) or below \((Y=0)\) the median wage based on the player's age and their overall attribute. The sensitive attribute \(A\) is player nationality and the player's name is used to learn the sensitive attribute predictor \(h\). We consider two scenarios, when \(A\{,\}\) (English = 0) and \(A\{,\}\) (French = 0).

**ACSPublicCoverage (Ding et al. )**: The task is to learn a classifier \(f\), using the 2018 state census data, that determines if a low-income individual, not eligible for Medicare, has coverage from public health insurance \((Y=1)\) or does not \((Y=0)\). The sensitive attribute \(A\) is sex (Female = 0) and with a separate dataset (containing the same features used to learn \(f\) (disregarding sex)), we learn the sensitive attribute predictor \(h\). We work with the 2018 California census data.

**CheXpert (Irvin et al. )**: CheXpert is a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radio graphs of 65,240 patients, with labeled annotations for 14 observations (positive, negative, or unlabeled) including cardiomegaly, atelectasis,consolidation, and several others. The task is to learn a classifier \(f\) to determine if an X-ray contains an annotation for _any_ abnormal condition \((Y=1)\) or does not \((Y=0)\). The sensitive attribute \(A\) is sex (Female = 0) and with a separate set of X-rays (different from those used to learn \(f\)) we learn the sensitive attribute predictor \(h\).

#### 4.2.1 Verification of Assumption 1

In Appendix C we provide Table 1, which demonstrates that Assumption 1 holds for the settings described above. The _Actual Value_ column of Table 1 lists the rates \(_{i,j}\) and the left and right columns list \(}{_{i,j}}\) and \(1-}{_{i,j}}\) respectively. From Table 1, it is clear that the \(_{i,j}\) lie in between \(}{_{i,j}}\) and \(1-}{_{i,j}}\) as required by Assumption 1. Notice, we only list the estimated group TPRs, \(_{i,1}\) for the ACSPublicCoverage 2018 California dataset. This is because Assumption 1, for \(j=0\), does _not_ hold for this state: even though \(\) is a very accurate predictor for \(A\), the label classifier \(f\) has very low estimated group FPRs, \(_{i,0}\). As a result, our theoretical results for \(_{}\) cannot be used.

Figure 2: Synthetic data (\( U 0\)): true fairness violations, worst-case fairness violations, and expected loss for \(f\), \(f_{}}\) and \(f_{}\)

#### 4.2.2 Results on CheXpert

The task is to measure and correct for any fairness violations that \(f\) may exhibit towards the sex attribute assuming we do _not_ have the true sex attribute and instead have a sensitive attribute predictor, \(h\). On a test dataset, we generate our predictions \(=f\) and \(=h\) to yield a dataset over \((,Y,)\), for which the sex predictor, \(h\), achieves an error of \(U=0.023\) with \(U_{0} 0.008\) and \(U_{1} 0.015\). We utilize the bootstrap method to generate 1000 samples of this dataset and for each sample, perform the same correction algorithms as before to yield \(f_{}}\) and \(f_{}\) and calculate the same metrics as done in the previous experiments.

The results in Fig. 3 show that our proposed correction method performs the best in reducing \(B_{}\) and \(B_{}\). Even though \(U\) is very small, since \(U_{1} 2U_{0}\), simply correcting for fairness with respect to \(\) is suboptimal in reducing the worst-case fairness violations. In particular, the results in Fig. 3 a are noteworthy as they depict how our proposed correction method and bounds allow the user to _certify_ that the obtained classifier has a fairness violation in TPRs of no more than 0.06 without having access to the true sensitive attributes. Moreover, the improvement is significant, since before the correction one had \(|_{}| 0.10\). In a high-stakes decision setting, such as this one where the model \(f\) could be used to aid in diagnosis, this knowledge could be vital. Naturally, the expected loss is highest for \(f_{}\) but that the increase is minimal. We make no claim as to whether this (small) increase in loss is reasonable for this particular problem setting, and the precise trade-offs must be defined in the context of a broader discussion involving policy makers, domain experts and other stakeholders.

## 5 Limitations and Broader Impacts

While our results are novel and informative, they come with limitations. First, our results are limited to EOD (and its relaxations) as definitions of fairness. Fairness is highly context-specific and in many scenarios one may be interested in utilizing other definitions of fairness. One can easily extend our results to other associative definitions of fairness, such as demographic parity, predictive parity, and others. However, extending our results to counter-factual notions of fairness [28; 11; 32] is non trivial and matter of future work. We recommend thoroughly assessing the problem and context in question prior to selecting a definition. It is crucial to ensure that the rationale behind choosing a definition is based on reasoning from both philosophical and political theory, as each definition implicitly make a distinct set of moral assumptions. For example, with EOD, we implicitly assert that all individuals with the same true label have the same effort-based utility . More generally, other statistical definitions of fairness such as demographic parity and equality of accuracy can be thought of as special instances of Rawlsian equality of opportunity and predictive parity, the other hand, can be thought of as an instance of egalitarian equality of opportunity [22; 30; 3]. We refer the reader to  to understand the relationship between definitions of fairness in machine learning and models of Equality of opportunity (EOP) - an extensively studied ideal of fairness in political philosophy.

A second limitation of our results is Assumption 1. This assumption is relatively mild, as it is met for accurate proxy sensitive attributes (as illustrated in the chest X-rays study). Yet, we conjecture that one can do away with this assumption and consider less accurate proxy sensitive attributes with the caveat that the worst case fairness violations will no longer be linear in the TPRs and FPRs. Thus, the

Figure 3: CheXpert data: true fairness violations, worst-case fairness violations, and expected loss for \(f\), \(f_{}}\) and \(f_{}\)characterization of the classifiers with minimal worst-case bounds would be more involved and the method to minimize these violations will likely be more difficult. Furthermore, while we proposed a simple post-processing correction method, it would be of interest to understand how one could train a classifier - from scratch - to have minimal violations. Lastly, in our setting we assume the sensitive attribute predictor and label classifier are trained on marginal distributions from the same joint distribution. As a next step, it would be important to understand how these results extend to settings where these marginal distributions come from (slightly) different joint distributions. All of this constitutes matter of future work.

Finally, we would like to remark the positive and potentially negative societal impacts of this work. Our contribution is focused on a solution to a technical problem - estimating and correcting for fairness violations when the sensitive attribute and responses are not jointly observed. However, we understand that fairness is a complex and multifaceted issue that extends beyond technical solutions and, more importantly, that there can be disconnect between algorithmic fairness and fairness in a broader socio-technical context. Nonetheless, we believe that technical contributions such as ours can contribute to the fair deployment of machine learning tools. In regards to the technical contribution itself, our results rely on predicting missing sensitive attributes. While such a strategy could be seen as controversial - e.g. because it could involve potential negative consequences such as the perpetuation of discrimination or violation of privacy - this is necessary to build classifiers with minimal worst-case fairness violations in a demographically scarce regime. On the one hand, not allowing for such predictions could be seen as one form of "fairness through unawareness", which has been proven to be an incorrect and misleading strategy in fairness . Moreover, our post-processing algorithm, similar to that of Hardt et al. , admits implementations in a differentially private manner as well, since it only requires aggregate information about the data. As a result, our method, which uses an analogous formulation with different constraints, can also be carried out in a manner that preserves privacy. Lastly, note that if one does not follow our approach of correcting for the worst-case fairness by predicting the sensitive attributes, other models trained on this data can inadvertently learn this sensitive attribute indirectly and base decisions of it with negative and potentially grave consequences. Our methodology prevents this from happening by appropriately correcting models to have minimal worst-case fairness violations.

## 6 Conclusion

In this paper we address the problem of estimating and controlling potential EOD violations towards an unobserved sensitive attribute by means of predicted proxies. We have shown that under mild assumptions (easily satisfied in practice, as demonstrated) the worst-case fairness violations, \(B_{}\) and \(B_{}\), have simple closed form solutions that are linear in the estimated group conditional probabilities \(_{i,j}\). Furthermore, we give an exact characterization of the properties that a classifier must satisfy so that \(B_{}\) and \(B_{}\) are indeed minimal. Our results demonstrate that, even when the proxy sensitive attributes are highly accurate, simply correcting for fairness with respect to these proxy attributes might be _suboptimal_ in regards to minimizing the worst-case fairness violations. To this end, we present a simple post-processing method that can correct a pre-trained classifier \(f\) to yield an optimally corrected classifier, \(\), i.e. one with with minimal worst-case fairness violations.

Our experiments on both synthetic and real data illustrate our theoretical findings. We show how, even if the proxy sensitive attributes are highly accurate, the smallest imbalance in \(U_{0}\) and \(U_{1}\) renders the naive correction for fairness with respect to the proxy attributes suboptimal. More importantly, our experiments highlight our method's ability to effectively control the worst-case fairness violation of a classifier with minimal decrease in the classifier's overall predictive power. On a final observation on our empirical results, the reader might be tempted to believe that the classifier \(f_{}}\) (referring to, e.g., Fig. 3) is better because it provides a lower "true" fairness than that of \(f_{}\). Unfortunately, these true fairness violations are not identifiable in practice, and all one can compute are the provided upper bounds, which \(f_{}\) minimizes. In conclusion, our contribution aims to provide better and more rigorous control over potential negative societal impacts that arise from unfair machine learning algorithms in settings of unobserved data.