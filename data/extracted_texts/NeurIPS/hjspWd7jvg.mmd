# GENOT: Entropic (Gromov) Wasserstein Flow Matching with Applications to Single-Cell Genomics

Dominik Klein

Helmholtz Munich

dominik.klein@helmholtz-munich.de

&Theo Uscidda

CREST-ENSAE

theo.uscidda@ensae.fr

Fabian Theis

Helmholtz Munich

fabian.theis@helmholtz-munich.de

&Marco Cuturi

Apple

cuturi@apple.com

Equal contributionWork done during internship at Apple

###### Abstract

Single-cell genomics has significantly advanced our understanding of cellular behavior, catalyzing innovations in treatments and precision medicine. However, single-cell sequencing technologies are inherently destructive and can only measure a limited array of data modalities simultaneously. This limitation underscores the need for new methods capable of realigning cells. Optimal transport (OT) has emerged as a potent solution, but traditional discrete solvers are hampered by scalability, privacy, and out-of-sample estimation issues. These challenges have spurred the development of neural network-based solvers, known as neural OT solvers, that parameterize OT maps. Yet, these models often lack the flexibility needed for broader life science applications. To address these deficiencies, our approach learns stochastic maps (i.e. transport plans), allows for any cost function, relaxes mass conservation constraints and integrates quadratic solvers to tackle the complex challenges posed by the (Fused) Gromov-Wasserstein problem. Utilizing flow matching as a backbone, our method offers a flexible and effective framework. We demonstrate its versatility and robustness through applications in cell development studies, cellular drug response modeling, and cross-modality cell translation, illustrating significant potential for enhancing therapeutic strategies.

## 1 Introduction

**Discrete Optimal Transport in Single-Cell Genomics.** Due to the destructive nature of single-cell sequencing technologies, it is fundamental to realign distributions of sequenced cells. Discrete optimal transport (OT) has proven useful in this task. For example, it is standard practice to leverage the framework of OT to reconstruct cellular trajectories across developmental time points  and to study the response of cells to external perturbations like drugs or gene knockouts . In these settings, both distributions, i.e. earlier and later time point and before perturbation and after perturbation, respectively, live in the same space, a setting which we refer to as _linear_ OT. In contrast, most applications of OT in single-cell genomics require maps across (partially) incomparable spaces, the setting which we refer to as _quadratic_ OT. For example, analysis of spatial transcriptomics datasets requires the quadratic formulation of OT as the coordinate systems of two different slices are warped and rotated, thus not living in the same coordinate space . Similarly, adding spatial information as a prior has been shown to improve the performance of modeling the trajectory of cells in the discrete setting . Analogously, adding information from lineage tracing readouts recovers the evolution of cells more faithfully . Another prevalent task in single-cell genomics which is commonly approached with quadratic OT solvers is the alignment of cells across modalities . Most single-cell technologies capture only one modality, which is insufficient to obtain a comprehensive representation of the cellular state. As single-cell datasets grow larger , traditional OT solvers become less applicable to tasks in single-cell genomics due to their high computational complexity. Thanks to recent advancements in low-rank OT [69; 70; 71], the aforementioned OT-based algorithms are more accessible to single-cell biologists . Yet, the non-parametric nature of discrete OT solvers entails issues with respect to data privacy and prevents their application to large scale single-cell atlases capturing millions of cells . Moreover, out-of-samples estimation is limited to very specific scenarios .

**Current Limitations of Neural Optimal Transport in Single-Cell Genomics.** To overcome these limitations, neural OT solvers have been leveraged to study cellular perturbations [7; 8; 86] and to model cellular trajectories [21; 83]. Yet, these methods estimate Monge maps, i.e., deterministic maps, contradicting the assumption that cells evolve stochastically . Stochastic formulations are also favorable as they can produce a conditional distribution that can be used to quantify uncertainty . In the discrete setting, stochastic maps can be obtained from entropy-regularized OT (EOT) . Recently, a number of works have addressed learning EOT plans [29; 41; 76; 84; 85] in the neural setting. Yet, all of these methods are limited in the choice of the cost function, with most of them being restricted to the squared Euclidean cost. Single-cell genomics data is known to be non-Euclidean  and thus requires a flexible choice of the cost function in OT applications as demonstrated in the discrete OT setting in Demcetci et al. , Huguet et al. , Klein et al. . The third requirement for applying OT to single-cell genomics is the option to lift the mass conservation constraint, allowing for _unbalanced_ optimal transport [11; 24; 74]. Unbalanced OT is crucial to model cellular growth and death as well as for automatically discarding outliers, which are prevalent in highly noisy measurements in single-cell genomics. Eyring et al. , Lubeck et al. , Yang and Uhler  proposed ways to incorporate unbalancedness into deterministic linear OT maps, while unbalanced formulations for entropic neural OT have barely been explored. The most severe shortcoming of the existing plethora of neural OT estimators is their limitation to the linear OT scenario, i.e., to learning maps within spaces. Yet, most applications in single-cell genomics require the Gromov-Wasserstein (GW)  or the fused Gromov-Wasserstein (FGW)  formulation. To the best of our knowledge, the only neural formulation for GW proposed thus far learns deterministic, balanced maps for the inner product costs, using a min-max-min optimization procedure, severely limiting its applications . Hence, we arrive at four necessities that need to be fulfilled _and_ have to be flexibly combinable to make neural OT generally applicable to problems in single-cell genomics:

* **N1**: modeling the evolution of cells stochastically as opposed to deterministically,
* **N2**: flexibly choosing cost functions due to non-Euclidean geometry of single-cell genomics data,
* **N3**: allowing for unbalanced OT to model cellular growth and death or accounting for outliers,
* **N4**: mapping across _completely_ or _partially_ incomparable spaces.

**Contributions.** Hence, we propose GENOT (Generative Entropic Neural Optimal Transport), a powerful and flexible neural OT framework that satisfies all of the above requirements:

* GENOT is the first method that parameterizes linear and quadratic EOT couplings for _any_ cost by modeling their conditional distributions, using flow matching  as a backbone.
* We extend GENOT to the unbalanced setting, resulting in U-GENOT, to flexibly allow for mass variations in both the linear and the quadratic setting.
* We extend (U-)GENOT to address the fused problem and this way propose, to the best of our knowledge, the first neural OT solver for the FGW problem.
* We showcase GENOT's ability to handle common challenges in single-cell biology: we (i) quantify lineage branching events in the developing mouse pancreas, (ii) predict cellular responses to drug perturbations, along with a well-calibrated uncertainty estimation, and (iii) translate ATAC-seq to RNA-seq with GW and introduce a novel method to perform this translation task with FGW.

## 2 Background

**Notations.** Let \(^{p}\), \(^{q}\) compact sets, referred to as the source and the target domain, respectively. In general, \(p q\). The set of positive (resp. probability) measures on \(\) is denoted by \(^{+}()\) (resp. \(^{+}_{1}()\)). For \(^{+}()\), \(_{1}:=p_{1}\) and \(_{2}:=p_{2}\) denote its marginals.

Then, for \(^{+}(),^{+}()\), \((,)=\{:\,_{1}=,\,_{2}=\}\). \(}{}\) denotes the relative density of \(\) w.r.t. \(\), s.t. \(=}{}\). For \(,^{+}()\), \((|)=_{}(}{ })\,-_{}+_{ }\).

**Linear Entropic OT.** Let \(c:\) be a continuous cost function, \(^{+}_{1}(),^{+}_{1}()\) and \( 0\). The linear entropy-regularized OT problem reads

\[_{(,)}_{}c\,+ (|)\,.\] (LEOT)

A solution \(^{*}_{}\) of (LEOT) always exists and is unique when \(>0\). (LEOT) is also known as the static Schrodinger bridge (SB) problem . With \(=0\), we recover the Kantorovich  problem. For discrete \(\) and \(\), we can solve (LEOT) with the Sinkhorn algorithm , whose complexity for \(n\) points is \((n^{2})\) in time, and \((n)\) or \((n^{2})\) in memory, depending on \(c\).

**Quadratic Entropic OT.** As opposed to considering an _inter-domain_ cost defined on \(\), quadratic entropic OT is concerned with seeking couplings that minimize the distortion of the geometries induced by _intra-domain_ cost functions \(c_{}:\) and \(c_{}:\):

\[_{(,)}_{()^{2}}D_{c_{ },c_{}}\,()+\, (\|),\] (QEOT)

where \(D_{c_{},c_{}}(,,^{},^{}):=|c_{}(,^{})-c_{ }(,^{})|^{2}\) quantifies the pointwise cost distortion. A solution \(^{*}_{}\) to Prob.(QEOT) always exists (see B.1). With \(=0\), we recover the Gromov-Wasserstein  problem, which is the standard OT formulation for transporting measures supported on _incomparable_ spaces. In addition to statistical benefits , using \(>0\) also offers computational benefits, since for discrete \(\) and \(\), we can solve (QEOT) with a mirror descent scheme iterating the Sinkhorn algorithm . For measures on \(n\) points, its time complexity is \((n^{2})\) or \((n^{3})\), depending on \(c_{}\) and \(c_{}\)[70, Alg. 1&22], while its memory complexity is always \((n^{2})\).

**Unbalanced Extensions.** The EOT formulations presented above can only handle measures with the same total mass. Unbalanced optimal transport (UOT)  lifts this constraint by penalizing the deviation of \(p_{1}\) to \(\) and \(p_{2}\) to \(\) with a divergence. Using the \(\) and introducing weightings \(_{1},_{2}>0\) the unbalanced extension of (LEOT) reads

\[_{^{+}()}_{ }c\,+(|)+ _{1}(_{1}|)+_{2}(_{2}|).\] (ULEOT)

This problem can be solved efficiently in the discrete setting using a variant of the Sinkhorn algorithm . Analogously, quadratic OT also admits an unbalanced generalization, which reads

\[_{^{+}()}_{( )^{2}}D_{c_{},c_{}}\,( )\,+^{}(|)+_{ 1}^{}(_{1}|)+_{2}^{}(_{2}| ),\] (UQEOT)

where \(^{}(|)=(|)\). A solution \(^{*}_{,}\) to Prob.(UQEOT) always exists (see B.1). In the discrete setting, Prob. (UQEOT) can be solved using an extension of Peyre et al. 's scheme introduced by Sejourne et al. . Each solver has the same time and memory complexity as its balanced counterpart. For both (ULEOT) and (UQEOT), instead of selecting \(_{i}\), we introduce \(_{i}=}{_{i}+}\) s.t. we recover the marginal constraint for \(_{i}=1\), when \(_{i}+\). We write \(=(_{1},_{2})\).

**Flow Matching (FM).** Given a prior \(_{0}^{+}_{1}(^{d})\) and a time-dependent vector field \((v_{t})_{t}\), one can define a probability path \(p_{t}\) starting from \(_{0}\) using the flow \(_{t}\) solving

\[}{t}_{t}()=v_{t}(_{t}()), _{0}()=,\] (1)

by setting \(p_{t}=_{t}_{0}\). We then say that \(v_{t}\) generates the path \(p_{t}\). Continuous Normalizing Flows  (CNFs) model \(v_{t,}\) with a neural network, which is trained to match a terminal condition \(p_{1}=_{1}^{+}_{1}(^{d})\). FM  is a simulation-free technique to train CNFs by constructing individual paths between samples, and minimizing

\[_{t,Z_{0}_{0},Z_{1}_{1}}[\|v_{t,}([Z_{0},Z_{1 }]_{})-(Z_{1}-Z_{0})\|_{2}^{2}],\] (2)

where \([Z_{0},Z_{1}]_{t}:=(1-t)Z_{0}+tZ_{1}\) with \(t()\). If this loss is 0, the flow maps \(_{0}\) to \(_{1}\), i.e., \(_{1}_{0}=_{1}\). This property is often referred to as FM preserving the marginal distribution.

## 3 Generative Entropic Neural OT

We introduce GENOT, a method to learn EOT couplings (thus satisyfing **N1**) with any cost (**N2**) by learning their conditional distributions. In SS 3.1, we focus on balanced OT, and show that GENOT approximates linear or quadratic EOT couplings (**N4**), solutions to (LEOT) or (QEOT) respectively. Then, in SS 3.2, we extend GENOT to the unbalanced setting by loosening the conservation of mass constraint (**N3**). We define U-GENOT, which approximates solutions to (ULEOT) and (UQEOT). In SS 3.3, we highlight that GENOT also adresses a fused problem, combining (LEOT) and (QEOT) (**N4**). Finally, we demonstrate in SS 3.4 the GENOT algorithm, that allows to flexibly adapt to different OT formulations, which is key for easy usability for single-cell genomics analysts.

### Learning Entropic Couplings with GENOT

Let \(_{1}^{+}()\), \(_{1}^{+}()\) and \(_{}^{}\) be an EOT coupling between \(\) and \(\), which can be a solution of problem (LEOT) or (QEOT). By the measure disintegration theorem, we get

\[_{}^{}(,)=( )\,_{}^{}(|)\,.\]

Knowing \(\), we can hence fully describe \(_{}^{}\) via the conditional distributions \(_{}^{}(|)_{ }\). The latter are of great practical interest, as they provide a way to transport a point \(\) sampled from \(\) to the target domain \(\); either _stochastically_ by sampling \(_{1},...,_{n}\) from \(_{}^{}(|)\), or _deterministically_ by averaging over conditional samples: \(T_{}():=_{Y_{}^{}( |)}[Y]\). Moreover, we can assess the uncertainty of these predictions using any statistic of \(_{}^{}(|)\), which is crucial in single-cell genomics.

**Learning the Conditional Distributions.** Let \(=(0,I_{q})\) the standard Gaussian on the target space \(^{q}\). From the noise outsourcing lemma , there exists a collection of conditional generators \(\{T^{}(|)\}_{}\) s.t. \(T^{}(|):^{q}^{q}\) and for each \(\) in the support of \(\), \(_{}^{}(|)=T^{}(|)\). This means that if \(Z\), then \(Y=T^{}(Z|)_{}^{}(|)\). Here, we seek to learn a neural collection \(\{T_{}(|)\}_{}\) of such conditional generators, fitting the constraint \(T_{}(|)=_{}^{}(| )\), for any \(\) in the support of \(\). We employ the FM framework (see SS 2) and parameterize each \(T_{}(|)\) implicitly, as the flow induced by a neural vector field \(v_{t,}(|):^{q}^{q}\). Namely, \(T_{}(|)=_{1}(|):^{q} ^{q}\) where \(_{t}(|)\) solves

\[}{t}_{t}(|)=v_{t,}( _{t}(|)|),_{0}(|)=.\] (3)

Figure 1: **Left: What do we do?** One task we consider is generating RNA cell profiles from ATAC measurements and an additional cell feature. This is explained in ยง 5.2, and demonstrated in Fig. 4. As the cells live on manifolds in two (partially) incomparable spaces, we rely on the Fused Gromov-Wasserstein (FGW) formulation, as described in ยง 3.3. Here, the incomparable structural information is contained in the ATAC and the RNA measurements, while the comparable information are the cell features. **Right: How do we do it?** For each \((,)\) in the support of the source \(\), we learn a flow \(_{1}(|,)\) from the noise \(\) to the conditional \(_{}^{}(|,)\), whose support lies in that of the target \(\). The flow is multi-modal: It allows sampling structural informations \(\), as well as features \(\) simultaneously. We highlight this procedure for a specific pair (\(_{1}\), \(_{1}\)), with \(=2\) and \(=3\).

We stress that while \(^{d}\), the flow \(_{1}(|)\) from \(\) to \(^{}_{}(|)\) is defined on the target space \(^{q}\). Hence, we can map samples _within_ the same space when \(p=q\), but also _across_ incomparable spaces when \(p q\). In particular, this allows us to consider the quadratic OT problem (QEOT). Thus, for each \(\), we optimize \(v_{t,}(|)\) by minimizing the FM loss (2) to map \(\) to \(^{}_{}(|)\), i.e.

\[_{t,Z,Y^{}_{}(|)}[ \|v_{t,}([Z,Y]_{t}|)-(Y-Z)\|_{2}^{2}]\,,\] (4)

where \([Z,Y]_{t}=(1-t)Z+tY\) interpolates between noise and conditional vectors. Averaging for all \(\) in the support of \(\) and using Fubini's Theorem, we derive the GENOT loss:

\[_{}():=_{t,Z,(X,Y)^{ }_{}}[\|v_{t,}([Y,Z]_{t}|X)-(Y-Z)\|_{2}^{2}]\,.\] (5)

In practice, we optimize a _sample-based_ GENOT loss by estimating \(^{n}_{}=_{i,j}^{ij}_{}_{(X_{i}, Y_{j})}\), from \(X_{1},,X_{n}\) and \(Y_{1},,Y_{n}\), with a _discrete_ EOT solver, see Alg. 1.

**GENOT Captures the Signal from the Mini-Batch Couplings.** Standard FM only preserves straight couplings , while bridge matching (its stochastic counterpart) only preserves the linear EOT coupling for \(c(,)=\|-\|_{2}^{2}\). In contrast, GENOT is a conditional FM model, learning the conditional distributions of the coupling independently of each other: For each \(\), we leverage FM to learn a conditional flow \(_{1}(|)\) that maps \(\) to \(^{}_{}(|)\). Therefore, as FM preserves the marginal distributions (see 2), GENOT preserves all couplings. Thus, it always captures the signal carried out by the mini-batch couplings, in both linear and quadratic settings, and for any cost. We highlight this property in Fig. 5, comparing GENOT to other OT-based FM approaches [64; 84; 85].

**GENOT Handles Any Cost.** We can use GENOT to approximate linear or quadratic EOT couplings. In both cases, we make no assumptions on the cost functions: We only need to evaluate them on samples to estimate \(^{}_{}\) using a discrete solver, see GENOT Alg. 1, line 5. In particular, we can use implicitly defined costs, whose evaluation requires a non-differentiable sub-routine. For instance, we can use the geodesic distance on the data manifold, which can be approximated from the shortest path distance on the \(k\)-nn graph induced by the Euclidean distance [12; 77]. This approach has demonstrated its effectiveness in a range of single-cell genomics tasks relying on discrete OT [17; 35; 39]. We use it with GENOT for both linear (see SS 5.1) and quadratic OT (see SS 5.2).

**GENOT Approximates Conditional Densities.** For each \(\), we build a CNF \(p_{t}(|)=_{t}(|)\) between \(p_{0}=\) and \(p_{1}=^{}_{}(|)\). From the instantaneous change of variables formula , we can then approximate the conditional density \(^{}_{}(|)\) at an arbitrary point \(\), proving useful to evaluate the likelihood of one-to-one matches between cells, see Fig.23.

### U-GENOT: Extension to the Unbalanced Setting

**Re-Balancing the UOT Problems.** In its standard form, GENOT respects marginal constraints, so it cannot directly handle unbalanced formulations (ULEOT) or (UQEOT). We show that unbalanced EOT problems can be _re-balanced_. Eyring et al. , Lubeck et al. , Yang and Uhler  introduced previously these ideas in the Monge map estimation setting, namely, in a static and deterministic setup. Our method stems from the fact that, for both linear and quadratic OT, the unbalanced EOT coupling \(^{}_{,}\) between \(^{+}()\), \(^{+}()\) solves a balanced EOT problem between its marginals, which are re-weighted versions \(\) and \(\) of \(\) and \(\), that have the same mass.

**Proposition 3.1** (Re-Balancing the unbalanced problems.).: _Let \(^{}_{,}\) be an unbalanced EOT coupling, solution of (ULEOT) or (UQEOT) between \(^{+}()\) and \(^{+}()\). We note \(=p_{1}^{}_{,}\) and \(=p_{2}^{}_{,}\), its marginals. Then, in both cases, \(\) (resp. \(\)) has a density w.r.t. \(\) (resp. \(\)). That is, there exist two re-weighting functions, one on each space, \(:^{+}\) and \(:^{+}\) s.t. \(=\) and \(=\). Furthermore, \(\) and \(\) have the same total mass and_

1. _(Linear)_ \(^{}_{,}\) _solves the balanced problem (_LEOT_) between_ \(\) _and_ \(\) _with the same_ \(\)_._
2. _(Quadratic) Provided that_ \(c_{}\)_,_ \(c_{}\) _(or_ \(-c_{}\)_,_ \(-c_{}\)_) are CPD kernels (see Def._ B.2_)._ \(^{}_{,}\) _solves the balanced problem (_QEOT_) between_ \(\) _and_ \(\) _with_ \(^{}=m(^{}_{,})\,\)_, where_ \(m(^{}_{,})=^{}_{,}( )\)_._

**Learning the Coupling and the Re-Weightings Simultaneously.** Thanks to Prop. 3.1, we aim to [i] learn a balanced EOT coupling between \(\) and \(\) along with [ii] the re-weighting functions \(,\). Learning them is desirable since they model the creation and destruction of mass. We do both simultaneously by adapting the GENOT procedure. Formally, we seek to optimize:

\[_{}() =\,_{t,Z,(X,Y)^{*}_{,}}[\|v_ {t,}\,([Z,Y]_{t}|X)-(Y-Z)\|_{2}^{2}]\] \[+_{X}[(-_{})(X)^{2}]+_{Y }[(-_{})(Y)^{2}],\] [ii]

where \(_{},_{}\) are (non-negative) neural re-weighting functions. Crucially, similar to (balanced) GENOT, we only need to estimate a discrete unbalanced EOT coupling \(^{n}_{,}\) using samples \(X_{1},,X_{n}\) and \(Y_{1},,Y_{n}\) to compute the two components, [i] and [ii], of the U-GENOT loss. We build upon theoretical insights on the linear OT case and extend them to the quadratic OT case in practice.

**Proposition 3.2** (Pointwise estimation of re-weighting functions.).: _Let \(^{n}_{,}=_{i,j}^{i,j}_{,} _{(X_{i},Y_{j})}\), solution to (ULEOT) between empirical counterparts of \(\) and \(\). Let \(=^{}_{,}_{n}\) and \(=^{}_{,}_{n}\) its marginals weights. Then, almost surely, \(n\,a_{i}(X_{i})\) and \(n\,b_{i}(Y_{i})\)._

Using Prop. 3.1, \(^{n}_{,}\) is a balanced EOT coupling between its marginals, which are empirical approximations of \(\) and \(\). Hence, we estimate the first term [i] of the loss as we do in the balanced case by sampling from the discrete coupling. Furthermore, Prop.3.2 highlights that the estimation of \(^{n}_{,}\) also provides a consistent estimate of the re-weighting function evaluations at each \(X_{i}\) and \(Y_{i}\). This enables estimating the second term [ii]. Therefore, switching from GENOT to U-GENOT simply involves using an unbalanced solver instead of a balanced one, and regressing the neural re-weighting functions on the marginal weights of the estimated discrete coupling. We detail our procedure in Alg. 1, showing the additional steps w.r.t. GENOT in teal.

### Combining Linear and Quadratic OT

We show in SS 3.1 and SS 3.2 how to use GENOT to solve OT problems within the same space or across incomparable spaces. On the other hand, numerous real-world problems pose the challenge of the source and target domains being only _partially_ incomparable . Therefore, suppose that the source and target space can be decomposed as \(=}\) and \(=}\), respectively. Intuitively, a sample \((,)}\) can be interpreted as a structural information \(\) equipped with a feature \(\). Assume we are given a cost \(c:\) to compare features, along with the intra-domain costs \(c_{}},c_{}}\). The entropic Fused Gromov-Wasserstein (FGW) problem reads

\[_{(,)}_{((})( }))^{2}}D^{c}_{c_{}},c_{}}}\,()+(| )\,,\] (EFGW)

where \(D^{c}_{c_{}},c_{}}}((, ),(,),^{},^{} ):=(1-)\,c(,)+\,|c_{}} (,^{})-c_{}}(, ^{})|^{2}\) and \(\). This loss combines the pointwise structural distortion and the feature information. We refer to the additional cost on the feature information as the _fused term_. The weighting \(\) allows us to interpolate between purely linear OT on the feature (\(=0\)), and purely quadratic OT (\(=1\)) on the structural information. Problem (EFGW) also admits an unbalanced extension, derived similarly as (UQEOT) with the quadratic \(^{}\). An (un)balanced fused EOT coupling always exists (see B.1), it minimizes distortion along the structural information and displacement cost along the features.

**(U-)GENOT for the Fused Setting.** Whether in the balanced or unbalanced setting, we can use our method to learn a specific coupling as soon as it can be estimated from samples. We stress that the discrete solvers introduced by Peyre et al.  and Sejourne et al.  we use for (QEOT) and (UQEOT), respectively, are still applicable in the fused setting. As a result, we can approximate solutions of (EFGW) and its unbalanced counterpart with (U-)GENOT. To illustrate the learning outcome, take a solution \(^{*}_{}\) of (EFGW). Learning \(^{*}_{}\) with our method amounts to training vector fields that are conditioned on pairs of modalities from the source domain \(v_{t,}(,|,)\), to sample pairs of modalities from the target domain via the flow: \(Z\), \(_{1}(Z|,)=(V,Y)^{*}_{}(|, )\). The sampled modalities \((V,Y)\) (i) minimize transport cost quantified by \(c\) along the feature (ii) while minimizing the distortion along the structural information quantified by \(c_{}}\) and \(c_{}}\). In SS 5.2, we use fused couplings to enhance the translation between different modalities of cells.

**Why Does Adding a Fused Term Provide Benefits?** Generally, unlike the linear EOT problem (LEOT), the solution \(^{*}_{}\) to the quadratic EOT problem (QEOT) is not unique. This is where adding a fused term helps, as it introduces more constraints. Intuitively, the fused term on additional features \(,\) allows us to'select' an optimal GW coupling: we trade some GW optimalityby choosing a coupling that not only minimizes the distortion of the structural information \(,\), but also reduces the displacement cost, quantified by \(c\), along the features \(,\). Empirically, this significantly mitigates the issue of non-uniqueness in (pure) GW couplings. In SS 5.2, we demonstrate that it enhances the stability of our procedure, particularly for single-cell data, as illustrated in Fig. 4 and Fig. 18. On a related note, but independently of the addition of the fused term, we investigate in App. E.2 other strategies to mitigate the non-uniqueness of the GW coupling. Notably, we show that using the geodesic distance on the data manifold as costs, instead of the squared Euclidean distance, helps address this issue on single-cell data. As an alternative, we also introduce an initialization scheme to bias the discrete GW solver to a specific solution based on the neural coupling obtained in the previous iteration (App. E.2). Yet, this approach renders GENOT non-simulation free.

### One algorithm to flexibly switch between neural OT problems

```
1:Require parameters: Batch size \(n\); entropic regularization \(\); unbalancedness parameter \(\); linear, quadratic or fused discrete OT solver \(_{,}\).
2:Require network: Time-dependent conditional velocity field \(v_{t,}(|):^{q}^{p}^{q}\); re-weighting functions \(_{}:^{p}\), \(_{}:^{q}\).
3:for\(t=1,,T_{}\)do
4: Sample \(X_{1},,X_{n}\) and \(Y_{1},,Y_{n}\).
5:\(_{,}_{, }(\{X_{i}\}_{i=1}^{n},\{Y_{i}\}_{i=1}^{n})_{+}^{n n}\).
6:\(_{,}1_{n}\) and \(_{,}^{}1_{n}\).
7: Sample \((i_{1},j_{1}),...,(i_{n},j_{n})_{,}\).
8: Sample \(Z_{1},...,Z_{n}\) and \(t_{1},...,t_{n}[(0,1])\).
9:\(()_{k}\|v_{t,}([Z_{k},Y_{j_{k}}]_{t}|X_{ i_{k}})-(Y_{j_{k}}-Z_{k})\|_{2}^{2}\)
10:\(+_{k}(_{}(X_{k})-n_{k})^{2}+(_{}(Y_{k} )-n_{k})^{2}\).
11:\((,())\).
12:endfor ```

**Algorithm 1**\(\)-GENOT. Skip teal steps for GENOT.

Single-cell genomics data is inherently noisy, due to biases incurred by sequencing protocols and the high sparsity of the measurements . Thus, it is indispensable to flexibly switch between configurations of the problem setup. For example, it is often not clear whether the mass conservation constraint should be actually loosened (for example whether to allow for modeling cell death), or whether incorporating prior information for trajectory inference via the quadratic term (e.g. with spatial information  or lineage barcoding ) is beneficial. Similarly, trying different costs is crucial in single-cell genomics ([17; 35; 39]). The GENOT formulation offers this flexibility by introducing only minor changes into the algorithm. We detail our procedure in Alg. 1, showing the additional steps to switch from GENOT to \(\)-GENOT in teal. Limitations are discussed in SS A.

## 4 Related work and Discussions

**Static Neural EOT.** While GENOT is the first model to consider the quadratic (and fused) EOT setting, various methods have been proposed in the linear EOT scenario. The first class of methods solves (LEOT)'s dual. While some of them [27; 73] do not allow direct sampling from to \(_{}^{*}\), Daniels et al.  and Mokrov et al.  model \(_{}^{*}(|)\). However, these methods might be costly and unstable, as they rely on Langevin sampling during training or inference.

**Dynamic Neural EOT.** The second class of linear EOT solvers builds upon the link between (LEOT) and the SB problem [16; 29; 87; 10]. Although they operate primarily in the balanced setting, Gazdieva et al.  recently considered the unbalanced one. As simulations are costly in this setting, recent works consider simulation-free training via bridge matching [41; 46; 61; 76; 84; 85; 96]. While Tong et al. [84; 85] use mini-batch OT, our method is fundamentally different, as we do not build upon the link between EOT and SB. We only use flow matching as a powerful black box to learn a flow from the noise \(\) to each conditional \(_{}^{*}(|)\). Therefore, our approach allows for more flexibility. First (i), the abovementioned methods assume that \(,^{d}\), since they learn a velocity field (or a drift) by directly bridging \(,\). Then, they map \(\) to \(\) with the induced (stochastic) flow. On the other hand, conditionally to each \(^{d}\), we learn a velocity field \(v_{t,}(|):^{q}^{q}\) in the _target space_, by building paths between noise \(^{q}\) and \(^{q}\). Then, we map \(\) to each \(_{}^{*}(|)\) with the flows \(_{1}(|)\), and recover \(=_{1}(|)()\). Second (ii), they can only approximate the SB for \(c(,)=\|-\|_{2}^{2}\). This results from [5, Prop. 2]. In contrast, our method handles any cost, as shown in Fig 5. Third (iii), as they learn a single flow directly transporting \(\) to \(\), they do not approximate the conditional densities \(_{}^{*}(|)\). Similarly to Tong et al. , Pooladian et al.  couple samples from \(\) and \(\), but they only model deterministic maps and assume \(=(0,I_{d})\). Finally, Bortoli et al.  recently proposed an augmented bridge matching procedure that preserves couplings. However, it still requires \(,^{d}\) and does not approximate conditional densities, limiting its applicability in single-cell genomics tasks.

**Mini-batches and Biases** Quantifying non-asymptotically the bias resulting from minimizing a sample-based GENOT loss, and _not_ its population value, is challenging. The OT curse of dimensionality  has been discussed in generative models relying on mini-batch couplings [22; 27; 84; 85]. Yet, our goal is _not_ to model _non-regularized_ OT, such as a deterministic Monge map, or a Benamou-Brenier vector field. We explicitly target the _entropy-regularized_ OT coupling. Thus, using \( 0\) helps to mitigate the curse of dimensionality because of two qualitative factors:

1. **Statistical.** For both linear and quadratic OT, all statistical recovery rates that relate to entropic costs [28; 95], maps [63; 66], or couplings , have a far more favorable regime, with a parametric rate in \(>0\) thatodes the curse of dimensionality.
2. **Computational.** While the benefits of employing a large enough \(\) in Sinkhorn's algorithm are widely known, Rioux et al.  have recently shown that as \(\) increases, the quadratic OT problem (QEOT) becomes convex, making discrete solvers faster and more reliable.

To demonstrate this aspect, we empirically study the influence of the batch size on GENOT, using a recent benchmark , with known true linear EOT coupling \(_{}^{*}\), see Fig. 6.

## 5 Experiments

While there is no evidence that cells exactly evolve according to an entropic OT plan, leveraging OT is an established way to realign cells. We also evaluate the performance on simulated data in settings which closed-form solutions of the EOT coupling are available for. Metrics and datasets are discussed in App. C and App. D, respectively. Further experimental details or results are reported in App. E. Setups for baselines are listed in App. F. Implementation details can be found in App. G. We denote by **GENOT-L** the GENOT model for solving the linear problem (LEOT), **GENOT-Q** for the quadratic (QEOT) and **GENOT-F** for the fused (EFGW) one. When considering the **unbalanced** counterparts, we add the **prefix U**. Moreover, when using the **conditional mean** of a GENOT model, we add the **suffix CM**. We denote our learned neural couplings by \(_{}\).

### GENOT-L for modeling cell trajectories

**GENOT-L on simulated data** We show that GENOT accurately computes EOT couplings using a recent benchmark . Tab. 1 shows that GENOT-L outperforms all baselines considered. These results are of particular significance since most of the baselines are tailored towards the specific task of solving (i) the linear EOT problem (LEOT), (ii) in the balanced setting (iii) with the \(_{2}^{2}\) cost. In contrast, the contributions of GENOT go _beyond_ this setting, see **N2**, **N3**, and **N4**. Going beyond the balanced problem (**N3**), we show that U-GENOT learns meaningful unbalanced EOT couplings between Gaussians  (11), which we visualize in Fig. 10.

**GENOT-L learns the evolution of cells.** Trajectory inference is a prominent task in single-cell biology. It is key to understand the development of cells from less mature to more mature cell

Figure 2: **Left:** Source cell from the early time points (top left) and samples of the conditional distributions of the EOT coupling learned with GENOT for the geodesic cost \(d_{}\) (middle) and the \(_{2}^{2}\) cost (right) projected onto a UMAP , along with biological assessment of the learnt dynamics (TSI score, CT error ยง C.2, Fig. 9). **Right:** UMAP colored according to the uncertainty score \((_{}^{d_{}}(|X))\) of each source cell \(\). Target cells are colored in gray.

[MISSING_PAGE_FAIL:9]

entropic OT plans between Gaussian distributions, which the closed form EOT plan is known for . Fig.19 and Fig.20 show that GENOT-Q is able to meaningfully learn (un-)balanced entropic EOT plans for lower dimensions, but its performance decreases with increasing dimensionality.

**GENOT-Q translates modalities of single cells.** We use GENOT-Q to translate ATAC measurements (source) to gene expression space (target) on a bone marrow dataset . As both modalities were measured in the same cell, the true match of each cell is known for this specific dataset. We compare GENOT-Q with discrete OT extended out-of-sample with linear regression (GW-LR, see F.2). We assess the performance using (i) the FOSCTTM ("Fractions of Samples Closer to the True Match", see C.2) that measures the optimality of the learned coupling, and (ii) the Sinkhorn divergence  between the predicted target and the target to assess distributional fitting. As in SS 5.1, we leverage GENOT's flexibility and use as intra-domains costs the geodesic distances on the source and target domain, namely \(c_{}=d_{}\), \(c_{}=d_{}\) (**N2**). We also estimate the EOT coupling for the \(_{2}^{2}\) cost for comparison. Results are shown in Fig. 3. Regarding the FOSCTTM score, we see that (i) using geodesic costs is crucial in high dimensions and (ii) GW-LR is competitive in low dimensions but not for higher ones. Regarding distributional fitting, GENOT models outperform by orders of magnitude.

**GENOT-F improves modality translation.** To enhance the performances attained by purely quadratic OT-based models, we introduce a novel method for translating between ATAC and RNA. We extend the idea of translating between cell modalities proposed by Demetci et al.  to the fused setting: We approximate RNA data from ATAC measurements using gene activity , and we further process the data using a conditional VAE  to reduce batch effects. This way, we construct a joint space \(\). Following the notations in SS 3.3, RNA (source \(}\)) and ATAC (target \(}\)) carry the structural information, while features \(\) and \(\) are obtained from the VAE embedding. Fig. 21 shows that the additional linear penalty on the feature term helps to obtain a better alignment compared to GENOT-Q. Fig. 4 visualizes the learned fused coupling compared to GENOT trained with batch-wise independent couplings. When aligning multiple cell modalities, the proportions of cell types in the two modalities often differ (**N3**). We simulate this setting by removing cells belonging to certain cell types Tab. 4 shows that U-GENOT-F preserves high accuracy while learning meaningful rescaling functions.

**Conclusion.** Motivated by applications in single-cell genomics, we introduce GENOT, a versatile neural OT framework to learn cost-efficient stochastic maps, within the same space or across incomparable spaces. GENOT can be used for learning EOT couplings with any cost function, and allows for loosening mass constraints if desired.

Figure 4: **Left**: Benchmark of GENOT-Q models against discrete GW (GW-LR, App. F) on translating cells between ATAC space of dim. \(d_{1}\) and RNA space of dim. \(d_{2}\), with performance measured with FOSCTTM score (App. C.2) and Sinkhorn divergence between target and predicted target distribution. (left) intra-domain costs \(c_{}=c_{}=_{2}^{2}\), (right) geodesic distances \(c_{}=d_{}\) and \(c_{}=d_{}\). We show mean and std across 3 runs. **Right:** Top: UMAP of transported cells with GENOT-F (colored by cell type) and cells in the target distribution (gray). Cells of the same cell type generate cells which cluster together in RNA space. Bottom: UMAP of transported cells with a GENOT model trained on batch-wise independent couplings, thus not using OT, generating cells which are randomly mixed.