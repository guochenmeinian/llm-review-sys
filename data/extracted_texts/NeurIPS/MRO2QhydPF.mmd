# Reinforcement Learning with Adaptive Regularization for Safe Control of Critical Systems

Haozhe Tian Homayoun Hamedmoghadam Robert Shorten Pietro Ferraro

Dyson School of Design Engineering

Imperial College London

SW7 2AZ, London, UK

{haozhe.tian21, h.hamed, r.shorten, p.ferraro}@imperial.ac.uk

Corresponding author

###### Abstract

Reinforcement Learning (RL) is a powerful method for controlling dynamic systems, but its learning mechanism can lead to unpredictable actions that undermine the safety of critical systems. Here, we propose RL with Adaptive Regularization (RL-AR), an algorithm that enables safe RL exploration by combining the RL policy with a policy regularizer that hard-codes the safety constraints. RL-AR performs policy combination via a "focus module," which determines the appropriate combination depending on the state--relying more on the safe policy regularizer for less-exploited states while allowing unbiased convergence for well-exploited states. In a series of critical control applications, we demonstrate that RL-AR not only ensures safety during training but also achieves a return competitive with the standards of model-free RL that disregards safety.

## 1 Introduction

A wide array of control applications, ranging from medical to engineering, fundamentally deals with _critical systems_, i.e., systems of vital importance where the control actions have to guarantee no harm to the system functionality. Examples include managing nuclear fusion (Degrave et al., 2022), performing robotic surgeries (Datta et al., 2021), and devising patient treatment strategies (Komorowski et al., 2018). Due to the critical nature of these systems, the optimal control policy must be explored while ensuring the safety and reliability of the control algorithm.

Reinforcement Learning (RL) aims to identify the optimal policy by learning from an agent's interactions with the controlled environment. RL has been widely used to control complex systems (Silver et al., 2016; Ouyang et al., 2022); however, the learning of an RL agent involves trial and error, which can violate safety constraints in critical system applications (Henderson et al., 2018; Recht, 2019; Cheng et al., 2019). To date, developing reliable and efficient RL-based algorithms for real-world "single-life" applications, where the control must avoid unsafety from the first trial (Chen et al., 2022), remains a challenge. The existing safe RL algorithms either fail to ensure safety during the training phase (Achiam et al., 2017; Yu et al., 2022) or require significant computational overhead for action verification (Cheng et al., 2019; Anderson et al., 2020). As a result, classic control methods are often favored in critical applications, even though their performance heavily relies on the existence of an accurate model of the environment.

Here, we address the safety issue of RL in scenarios where "estimated" environment models are available (or can be built) to derive sub-optimal control policy priors. These scenarios are representative of many real-world critical applications (Hovorka et al., 2002; Liepe et al., 2014; Hippisley-Cox et al., 2017; Rathi et al., 2021). Consider the example of devising a control policy that prescribes theoptimal drug dosages for regulating a patient's health status. This is a single-life setting where no harm to the patient is tolerated during policy exploration. From available records of other patients, an estimated patient model can be built to predict the response to different drug dosages and ensure adherence to the safety bounds (set based on clinical knowledge). However, a new patient's response can deviate from the estimated model, which poses a significant challenge in control adaptability and patient treatment performance.

We propose a method, _RL with Adaptive Regularization_ (RL-AR), that simultaneously shows the safety and adaptability properties required for critical single-life applications. The method interacts with the actual environment using two parallel agents. The first (safety regularizer) agent avoids unsafe states by leveraging the forecasting ability of the estimated model. The second (adaptive) agent is a model-free RL agent that promotes adaptability by learning from actual environment interactions. Our method introduces a "focus module" that performs state-dependent combinations of the two agents' policies. This approach allows immediate safe deployment in the environment by initially prioritizing the safety regularizer across all states. The focus module gradually learns to apply appropriate policy combinations depending on the state--relying more on the safety regularizer for less-exploited states while allowing unbiased convergence for well-exploited states.

We analytically demonstrate that: i) RL-AR regulates the harmful effects of overestimated RL policies, and ii) the learning of the state-dependent focus module does not prevent convergence to the optimal RL policy. We simulate a series of safety-critical environments with practically obtainable sub-optimal estimated models (e.g., from real-life sampled measurements). Our empirical results show that even with more than 60% parameter mismatches between the actual environment model and the estimated model, RL-AR ensures safety during training while converging to the control performance standard of model-free RL approaches that prioritize return over safety.

## 2 Preliminaries

Through environment interactions, an RL agent learns a policy that maximizes the expected cumulative future reward, i.e. the expected return. We formalize the environment as a Markov Decision Process (MDP) \(=(,,P,r,)\), where \(\) is a finite set of states, \(=\{a^{k}:a a\}\) is a convex action-space, \(P:()\) is the state transition function, \(r:[-R_{},R_{}]\) is the reward function, and \((0,1)\) is a discount factor. Let \(\) denote a stochastic policy \(:()\), the value function \(V^{}\) and the action-value function \(Q^{}\) are:

\[V^{}(s_{t})=_{a_{t},s_{t+1},}[_{i=0}^{} ^{i}r(s_{t+i},a_{t+i})],\ \ Q^{}(s_{t},a_{t})=_{s_{t+1},}[_{i=0}^{} ^{i}r(s_{t+i},a_{t+i})],\] (1)

where \(a_{t}(s_{t})\), \(s_{t+1} P(s_{t},a_{t})\) for \(t 0\). The optimal policy \(^{}=*{argmax}_{}V^{}(s)\) maximizes the expected return for any state \(s\). Both \(V^{}\) and \(Q^{}\) satisfy the Bellman equation (Bellman, 1966):

\[V^{}(s)=_{a,s^{}}[r(s,a)+ V^{}(s^{}) ],\ \ Q^{}(s,a)=_{s^{}}[r(s,a)+_{a^{ }(s^{})}[Q^{}(s^{},a^{})]].\] (2)

For practical applications with complex \(\) and \(\), \(Q^{}\) and \(\) are approximated with neural networks \(Q_{}\) and \(_{}\) with learnable parameters \(\) and \(\). To stabilize the training of \(Q_{}\) and \(_{}\), they are updated using samples \(\) from a Replay Buffer \(\)(Mnih et al., 2013), which stores each previous environment transitions \(e=(s,a,s^{},r,d)\), where \(d\) equals 1 for terminal states and 0 otherwise.

In this work, we are interested in acting on a safe regularized RL policy that can differ from the raw RL policy. RL approaches that allow learning from a different acting policy are referred to as "off-policy" RL. The RL agent in our proposed algorithm follows the state-of-the-art off-policy RL algorithm: Soft Actor-Critic (SAC) (Haarnoja et al., 2018), which uses a multivariate Gaussian policy to explore environmental uncertainties and prevent getting stuck in sub-optimal policies. For \(Q\)-network updates, SAC mitigates the overestimation bias by using the clipped double \(Q\)-learning, which updates the two \(Q\)-networks \(Q_{_{i}},i=1,2\) using gradient descent with the gradient:

\[_{_{i}}&|}_{(s,a,s^{},r,d)}(Q_{_{i}}(s,a)-y)^{2}, i=1,2, \\ y=r+(1-d)(_{i=1,2}Q_{_{tors,i}}(s^{},a^{})- P_{_{}}(a^{} s^{})),  a^{}_{}(s^{}),\] (3)where the entropy regularization term \( P_{_{}}(a^{} s^{})\) encourages exploration, thus avoiding local optima. Target \(Q\)-networks \(_{targ,i}\) are used to reduce drastic changes in value estimates and stabilize training. The target \(Q\)-network parameters are initialized with \(_{targ,i}=_{i}\), \(i=1,2\). Each time \(_{1}\) and \(_{2}\) are updated, \(_{targ,1},_{targ,2}\) slowly track the update using \((0,1)\):

\[_{targ,i}=_{targ,i}+(1-)_{i},\;\;i=1,2.\] (4)

For policy updates, the policy network \(_{}\) is updated using gradient ascent with the gradient:

\[_{}|}_{s}(_{i=1,2 }Q_{,i}(s,a_{}(s))- P_{_{}}(a s)), a _{}(s)_{}(s).\] (5)

## 3 Methodology

Here, we propose RL-AR, an algorithm for the safe training and deployment of RL in safety-critical applications. A schematic view of the RL-AR procedures is shown in Fig. 1. RL-AR comprises two parallel agents and a focus module: (i) The _safety regularizer_ agent follows a deterministic policy \(_{}:\) proposed by a constrained model predictive controller (MPC); (ii) The _off-policy RL_ agent is an adaptive agent with \(_{}:()\) that can learn from an acting policy that is different from \(_{}\); (iii) The _focus module_ learns a state-dependent weight \(:\) for combining the deterministic \(a_{}(s)=_{}(s)\) and the stochastic \(a_{}(s)_{}(s)\). Among the components, the safety regularizer has a built-in estimated environment model \(:\) that is different from the actual environment model, while the off-policy RL agent and focus module are dynamically updated using observed interactions in the actual environment.

The RL-AR workflow is as follows: (i) \(_{}(s)\) generates \(a_{}(s)\), which hard-codes safety constraints in the optimization problem over a period forecasted by \(\). The forecasting ability anticipates and prevents running into unsafe states for the critical system; (ii) \(_{}(s)\) generates \(a_{}(s)\) to allow stochastic exploration and adaptation to the actual environment; (iii) \((s)\) is initialized to \((s) 1-\), \( s\), hence prioritizing the safe \(_{}\) before \(_{}\) learns a viable policy. As more interactions are observed for a state \(s\) and the expected return of \(_{}(s)\) improves, \((s)\) gradually shifts the focus from the initially suboptimal \(_{}(s)\) to \(_{}(s)\).

### The safety regularizer

The safety regularizer of RL-AR is a constrained MPC, which, at any state \(s_{t}\), optimizes the \(N\)-step system behavior forecasted using the estimated environment model \(\) by solving the following constrained optimization problem:

\[_{a_{t:t+N-1}}_{k=t}^{t+N-1}J_{k}(s_{k},a_{k})+J_{N}(s_{t+ N})\] (6) s.t. \[ s_{k+1}=(s_{k},a_{k}),g(s_{k}) 0,a_{k} ,\]

where \(J_{k}(s_{k},a_{k})\) and \(J_{N}(s_{t+N})\) are the stage and terminal cost functions and \(g(s_{k}) 0\) is the safety constraint. By hard-coding the safety constraints in the optimization (via \(g(s_{k}) 0\)) over the

Figure 1: Schematic overview of the proposed RL-AR algorithm. RL-AR integrates the policies of the RL agent and the safety regularizer agent using a state-dependent focus module, which is updated to maximize the expected return of the combined policy.

prediction horizon, MPC prevents failure events that are not tolerated in critical applications. MPC iteratively solves for the N-step optimal actions in each time step and steers the environment to the desired state. At any time step \(t\), solving the optimization problem in Eq. (6) yields a sequence of \(N\) actions \(a_{t:t+N-1}\), with only the first action \(a_{t}\) in the sequence adopted for the current time step, i.e., \(a_{}(s_{t})=_{}(s_{t})=a_{t}\). The system transitions from \(s_{t}\) to \(s_{t+1}\) by taking the action \(a_{}(s_{t})\), and the optimization problem is solved again over \(\{t+1:t+1+N\}\) to obtain \(a_{}(s_{t+1})\). For practical applications with continuous state space, the optimization problem in Eq. (6) is efficiently solved using the Interior Point Optimizer (Andersson et al., 2019). Since MPC solves similar problems with slight variations at each time step, the computational complexity is further reduced by using the solution from the previous step as the initial guess.

### Policy regularization

The focus module in RL-AR combines the actions proposed by the safety regularizer agent and the RL agent using a weighted sum. At state \(s\), the combined policy \(_{}\) takes the following action \(a_{}(s)\):

\[a_{}(s)=(s)a_{}(s)+(1-(s))a_{}(s),\ \ a_{}(s)=_{}(s),a_{}(s)_{ }(s).\] (7)

**Lemma 1**.: _(Policy Regularization) In any state \(s\), for a multivariate Gaussian RL policy \(_{}\) with mean \(_{}(s)\) and covariance matrix \(=(_{1}^{2}(s),_{2}^{2}(s),,_{k}^{2 }(s))^{k k}\), the expectation of the combined action \(a_{}(s)\) derived from Eq. (7) is the solution to the following regularized optimization with regularization parameter \(=(s)/(1-(s))\):_

\[[a_{}(s)]=*{argmin}_{a}\|a-_{}(s)\|_{}+\|a-a _{}(s)\|_{}.\] (8)

We provide the proof of Lemma 1 in Appendix A.1, which is a state-dependent extension of the proof in (Cheng et al., 2019). Lemma 1 shows that the state-dependent \((s)\) offers a safety mechanism on top of the safety regularizer. Since \((s)\) is initialized close to 1 for \( s\), a strong regularization (\(\)) from the safety regularizer policy is applied at the early stages of training. As learning progresses, the stochastic combined policy inevitably encounters rarely visited states, where \(_{}\) is poor due to the overestimated \(Q\). However, the regularization parameter \(\) remains large for these states, thus preventing the combined policy from safety violations by regularizing its deviation from the regularizer's safe policy. This deviation is quantified in the following theorem.

**Theorem 1**.: _Assume the reward \(R\) and the transition probability \(P\) of the MDP \(\) are Lipshitz continuous over \(\) with Lipschitz constants \(L_{R}\) and \(L_{P}\). For any state \(s\), the difference in expectedreturn between following the combined policy \(_{}\) and following the safety regularizer policy \(_{}\), i.e., \(|V^{_{}}(s)-V^{_{}}(s)|\), has the upper-bound:_

\[|V^{_{}}(s)-V^{_{}}(s)|| L_{R}+||L_{P}R_{}}{(1-)^{2}}(1-(s)) a,\] (9)

_where \(||\) is the cardinality of \(\), \( a=|a_{}(s)-a_{}(s)|\) is the bounded action difference at \(s\)._

We provide the proof of Theorem 1 in Appendix A.2. Theorem 1 shows that when a state \(s\) has not been sufficiently exploited and its corresponding \((s)\) updates have been limited accordingly, the sub-optimality of the RL policy \(_{}\) has limited impact on the expected return of the combined policy \(_{}\), which is the actual acting policy. This is because \(1-(s)\) remains close to zero at this stage, leading to only minor expected return deviations from the safety regularizer's policy \(_{}\).

### Updating the focus module

The focus module derives the policy combination (from the policies of safety regularizer and off-policy RL agent) that maximizes the expected return. For any state \(s\), the state-dependent focus weight \((s)\) is learned through updates according to the following objective:

\[^{}(s)=*{argmax}_{}[Q^{ _{}}(s, a_{}(s)+(1-)a_{}(s))], \;a_{}(s)=_{}(s),a_{}(s)_{ }(s).\] (10)

Equation (10) is similar to the actor loss in actor-critic methods, however, instead of optimizing the policy network, Eq. (10) optimizes \((s)\) for policy combination.

Compared to a scalar combination weight that applies the same policy combination across all states (e.g., as in (Cheng et al., 2019)), the updated state-dependent weight \(^{}(s)\) in Eq. (10) guarantees monotonic performance improvement at least in the tabular cases, i.e., the update \(^{}(s_{t})\) at a state \(s_{t}\) results in \(V^{_{^{}}}(s) V^{_{}}(s)\) for all states \(s\), where \(_{^{}}\) is the combined policy proposed by \(^{}\). This can be proved by observing that the update in Eq. (10) results in a non-negative advantage for all states \(s\), i.e., \(Q^{_{}}(s,_{^{}}) Q^{_{}}(s,_{}),  s\), where (with slight abuse of notation) we use \(Q(s,)\) to denote \(Q(s,a)\) with \(a(s)\). See Theorem 2 in Appendix A.3 for the detailed proof.

**Lemma 2**.: _(Combination Weight Convergence) For any state \(s\), assume the RL policy \(_{}\) converges to the optimum policy \(^{}\) that satisfies \(Q(s,^{})>Q(s,),^{}\), then \(^{}(s)=0\) will be the solution to Eq. (10) that achieves the optimal policy combination._

Lemma 2 follows as \(_{}^{}\) due to the sub-optimal model used to derive \(_{}\). Let \(a^{}(s)^{}(s)\) denote the optimum action at state \(s\). If \((s) 0\), then \((s)a_{}(s)+(1-(s))a_{}(s)=(s)a_{}(s)+(1-(s))a^{}(s) a^{}(s)\). Therefore, the solution to Eq. (10), i.e., the updated focus weight \(^{}(s)\), can only be 0.

**Theorem 3**.: _(Policy Combination Bias) For any state \(s\), the distance between the combined action \(a_{}(s)\) and the optimal action \(a^{}(s)\) has the following lower-bound:_

\[|a_{}(s)-a^{}(s)||a_{}(s)-a^{}(s)|-(1-(s) )|a_{}(s)-a_{}(s)|.\] (11)

_If a Gaussian RL policy \(_{}\) converges to the optimum policy \(^{}(s)\) with \(Q(s,^{})>Q(s,),^{}\), then the combined policy \(_{}(s)\) can have unbiased convergence to the optimum Gaussian policy \(^{}\) with total variance distance \(D_{}(_{}(s),^{}(s))=0\)._

The proof of Theorem 3 is given in Appendix A.5. Theorem 3 shows that by adaptively updating \((s)\), the unbiased convergence of the combined policy can be achieved assuming i) a unique optimum solution and ii) the convergence of the RL agent, where the former follows naturally for most real-life control applications and the latter is well-established in the RL literature (the convergence of the specific RL agent used in RL-AR was proved in (Haarnoja et al., 2018)).

Algorithm 1 shows the pseudo-code of RL-AR, where \(Q\), \(_{}\), and \(\) are approximated with neural networks for practical applications with large or continuous state space. Note that the policy regularization (Lemma 1) and the convergence of RL-AR to the optimum RL policy (Lemma 2 and Theorem 3) still hold when using function approximation. For the RL agent, we take the standard approach of approximating \(Q^{}\) and \(\) with neural networks \(Q_{}\) and \(_{}\). The focus module \((s)\) is approximated with a neural network \(_{}\) with outputs scaled to the range \((0,1)\). Before learning begins, \(_{}\) is pretrained to output values close to 1 (e.g., \(_{}(s) 1-\)) for all states to prioritize the safe \(_{}\). While interacting with the environment, each transition \(e=(s,a,s^{},r,d,a_{})\) is stored in a replay buffer \(\). Since \(_{}\) is deterministic and not subject to updates, by storing the action term \(a_{}\) in \(\), the optimization problem in Eq. (6) needs to be solved only once for any state \(s\), significantly lowering the computational cost.

In Algorithm 1, details of \(Q_{_{i}}\) and \(_{}\) updates (lines 8-9) are omitted as they follow the standard SAC (Haarnoja et al., 2018) paradigm elaborated in Section 2. After updating \(Q_{_{i}}\) and \(_{}\), the focus module \(_{}\) is updated using samples \((s,a_{})\) from replay buffer \(\). As shown in line 10, \(_{}\) is updated using gradient ascent with the gradient:

\[_{}|}_{(s,a_{}) }_{i=1,2}Q_{_{i}}(s,_{}(s)a_{}+(1-_{ }(s))a_{}(s)),\ \ a_{}(s)_{}(s).\] (12)

Note that the updated \(Q_{_{i}},i=1,2\), and \(_{}\) are used in Eq. (12) to allow quick response to new information. The clipped double-Q learning (taking the minimum \(Q_{_{i}},i=1,2\)) mitigates the overestimation error. Although exploitation level is not explicitly considered in Eq. (10), the use of replay buffer and the gradient-based updates in Eq. (12) mean more frequently-visited states with well-estimated Q values will affect \(_{}(s)\) more, whereas rarely-visited states with overestimated Q values affect \(_{}(s)\) less.

## 4 Numerical Experiments

Here, RL-AR is validated in critical settings described in Section 1, where the actual environment model \(P\) is unknown, but an estimated environment model \(\) is available (e.g., from previous observations in the system or a similar system)1. Four safety-critical environments are implemented:

* _Glucose_ is the critical medical control problem of regulating blood glucose level against meal-induced disturbances (Batmani, 2017). The observations are denoted as \((G,,t)\), where \(G\) is the blood glucose level, \(=G_{t}-G_{t-1}\), and \(t\) is the time passed after meal ingestion. The action is insulin injection, denoted as \(a_{I}\). Crossing certain safe boundaries of \(G\) can lead to catastrophic health consequences (hyperglycemia or hypoglycemia).
* _BiGlucose_ is similar to the Glucose environment but capturing more complicated blood glucose dynamics, with 12 internal states (11 unobservable), 2 actions with large delays, and nondifferentiable piecewise dynamics. (Kalisvaart et al., 2023). The observations are the same as Glucose. The actions are insulin and glucagon injections, denoted as \((a_{I},a_{N})\).
* _CSTR_ is a continuous stirred tank reactor for regulating the concentration of a chemical \(C_{B}\)(Fiedler et al., 2023). The observations are \((C_{A},C_{B},T_{R},T_{K})\), where \(C_{A}\) and \(C_{B}\) are the concentrations of two chemicals; \(T_{R}\) and \(T_{K}\) are the temperatures of the reactor and the cooler, respectively. The actions are the feed and the heat flow, denoted as \((a_{F},a_{Q})\). Crossing safe boundaries of \(C_{A}\), \(C_{B}\), and \(T_{R}\) can lead to tank failure or even explosions.
* _Cart Pole_ is a classic control problem of balancing an inverted pole on a cart by applying horizontal force to the cart. The environment is adapted from the gymasium environment (Towers et al., 2023) with continuous action space. The observations are \((x,,,)\), where \(x\) is the position of the cart, \(\) is the angle of the pole, \(=x_{t}-x_{t-1}\), and \(=_{t}-_{t-1}\). The action is the horizontal force, denoted as \(a_{f}\). The control fails if the cart reaches the end of its rail or the pole falls over.

All environments are simulated following widely accepted models and parameters (Sherr et al., 2022; Yang and Zhou, 2023), which are assumed to be unknown to the control algorithm. The estimated models and the actual environments are set to have different model parameters. For _Glucose_ and _BiGlucose_, the estimated model parameters are derived from real patient measurements (Hovorka et al., 2004; Zahedifar and Keymasi Khalaji, 2022). The environment models, parameters, and reward functions are detailed in Appendix B.

The baseline methods used in the experiments are: i) MPC (Fiedler et al., 2023), the primary method for control applications with safety constraints (Hewing et al., 2020); ii) SAC (Haarnoja et al., 2018),a model-free RL that disregards safety during training, but achieves state-of-the-art normalized returns; iii) Residual Policy Learning (RPL) [Silver et al., 2018], an RL method that improves a sub-optimal MPC policy by directly applying a residual policy action; iv) Constrained Policy Optimization (CPO) [Achiam et al., 2017], a widely-used risk-aware safe RL benchmark based on the trust region method; and v) SEiditor [Yu et al., 2022b], a more recent, state-of-the-art safe RL method that learns a safety editor for transforming potentially unsafe actions.

The proposed method, RL-AR, uses MPC as the safety regularizer agent and SAC as the off-policy RL agent. The two agents in RL-AR each follow their respective baseline implementations. The focus module in RL-AR has a \(\) hidden layer size with ReLU activation, and \(k\) outputs scaled to \((0,1)\) by a shifted \(\). Additional detail and hyperparameters of the implementations are provided in Appendix C. Our RL-AR implementation has an average decision and update time of 0.037 seconds per step on a laptop with a single GPU, meeting real-time control requirements across all environments. In Appendix D we present ablation studies on the benefit of state-dependent focus weight and the choice of SAC as the RL agent.

### Safety of training

We begin by evaluating training safety in the actual environment by counting the number of failed episodes out of the first 100 training episodes. An episode is considered a failure and terminated immediately if a visited state exceeds a predefined safety bound. As shown in Table 1, only RL-AR completely avoids failure during training in the actual environment. Although MPC does not fail, it does not adapt or update its policy in the actual environment. RPL is relatively safe by relying on a safe initial policy, but its un-regularized residual policy action results in less stable combined action, leading to failures. Due to their model-free nature, CPO and SEditor must observe failures in the actual environment before learning a safe policy, thus failing many times during training. Note that SAC averages the largest number of failures over all environments.

Next, since the estimated environment model \(\) is integrated into RL-AR, MPC, and RPL, for a fair comparison we pretrain the model-free SAC, CPO, and SEditor using \(\) as an environment simulator; this allows all methods to access the estimated model before training on the actual environment. Figure 2 compares the normalized episodic return and the number of failures for different methods over training episodes; the proposed method is compared with SAC and RPL in Fig. 2A, and with MPC, CPO, and SEditor (safety-aware methods) in Fig. 2B. The mean (solid lines) and standard deviation (shaded area) in Fig. 2 are obtained from 5 independent runs using different random seeds. Episodes are terminated on failure, resulting in varying episode lengths, thus, the episodic returns are normalized by episode lengths.

Two important insights can be drawn from Fig. 2. First, the normalized return curves show that RL-AR consistently achieves higher returns faster than other methods across all environments. RL-AR begins with a reliable initial policy derived from the safety regularizer and incrementally integrates a learned policy, resulting in stable return improvements (as suggested by Lemma 1 and Theorem 1). RL-AR shows a steady return improvement, except for some fluctuations in the CSTR environment which the method quickly recovers from. In contrast, the baseline methods--SAC and RPL, which apply drastic actions based on overestimated returns, or CPO and SEditor, which impose constraints using biased cost estimates derived from simulations using \(\)--exhibit significant return degradation and even failures. Second, RL-AR effectively avoids failure during training (see the bottom rows in Fig. 2A&B). Note that pertaining on \(\) leads to fewer failures in the actual environment for SAC, CPO, and SEditor (compare with the results in Table 1). However, SAC, CPO, and SEditor continue to fail despite the pretraining (the only exception is SEditor in the Glucose environment), indicating that pretraining on estimated model is not an effective approach to achieve safety.

  Method & Gluc. & BiGI. & CSTR & Cart. \\  RL-AR & **0.0** (0.0) & **0.0** (0.0) & **0.0** (0.0) & **0.0** (0.0) \\ MPC & **0.0** (0.0) & **0.0** (0.0) & **0.0** (0.0) & **0.0** (0.0) \\ SAC & 19.0 (15.2) & 59.4 (31.1) & 99.2 (0.4) & 93.6 (7.3) \\ RPL & 7.8 (6.4) & 5.6 (3.9) & 3.6 (1.5) & 3.6 (2.2) \\ CPO & 8.0 (2.1) & 72.4 (6.7) & 100.0 (0.0) & 21.8 (3.7) \\ SEditor & 6.8 (1.7) & 74.6 (8.4) & 97.2 (5.6) & 17.4 (10.6) \\  

Table 1: The mean (\(\) standard deviation) number of failures out of the first 100 training episodes, obtained over 5 runs with different random seeds.

In CSTR and Cart Pole environments, and only in a limited number of episodes during the early stages of training, the proposed RL-AR policy's normalized return falls below that of the static MPC policy. This can occur due to RL-AR reaching insufficiently learned states (with overestimated Q values). Nevertheless, since \((s)\) is close to \(1\) for these insufficiently learned states, the dominance of the safety regularizer agent allows RL-AR to converge to high returns without compromising the safety (as shown in Theorem 1).

### Achieved return after convergence

Besides ensuring safer training, RL-AR theoretically enables unbiased convergence to the optimal RL policy (as shown in Theorem 2). We validate this by testing whether RL-AR matches the return of SAC. SAC is shown to consistently converge to well-performing control policies, competitive if not better than other state-of-the-art RL algorithms (Raffin et al., 2021; Huang et al., 2022). In Fig. 3, we compare the control trajectories of RL-AR, SAC, and MPC and the returns of their converged policies after training; we run the converged policies without stochastic exploration. RL-AR significantly outperforms MPC, achieving faster regulation, reduced oscillation, and smaller steady

Figure 2: The normalized return curves and the number of failures during training (standard deviations are shown in the shaded areas). SAC, CPO, and SEditor are pretrained using the estimated model \(\) as a simulator (as indicated by “-pt”) to ensure a fair comparison, given that RL-AR, MPC, and RPL inherently incorporate the estimated model. This pretraining allows SAC, CPO, and SEditor to leverage the estimated model, resulting in more competitive performance in the comparison.

state error. Furthermore, in terms of normalized return, RL-AR is competitive with SAC in the Cart Pole environment and outperforms SAC in the other three environments. The results demonstrate that RL-AR not only effectively ensures safety during training, but also finds control policies competitive with the state-of-the-art model-free RL.

### Sensitivity to parameter discrepancies

Inherently, RL-AR's training safety relies on the effectiveness of the safety regularizer, which depends on the quality of the estimated model \(\). Thus, a large discrepancy between \(\) and the actual environment might compromise the training safety of RL-AR. We empirically quantify this effect by deploying RL-AR in discrepant Glucose environments created by varying the environment model parameters \(n\) and \(p_{2}\) (values chosen based on \(\) and \(_{2}\) in \(\)) to mimic deviating characteristics of new patients, and counting the number of failed episodes out of the first 100 episodes in Fig. 4. Lower \(p_{2}/_{2}\) and \(n/\) makes the environment more susceptible to failure; see Appendix B.1. The results show that RL-AR can withstand reasonable discrepancies between \(\) and the actual environment. Failures only occur when the actual environment deviates significantly from \(\) with \(p_{2}_{2}\) and \(n\). All failures are caused by the safety regularizer due to its misleading estimated model with largely discrepant parameters. When RL adapts (by updating \(_{}\) and \(_{}\)) sufficiently to correct the misleading regularizer action, the combined agents effectively recover from failure. Here in our tests in the Glucose environment, even with large model discrepancies, RL-AR is shown to be as safe as the classic MPC. Appendix Fig. 7 provides insights into the adaptation of the focus module by showing the progression of \(_{}\) in the learning process.

## 5 Related Works

The existing safe RL works can be roughly divided into two categories (Garcia and Fernandez, 2015). The first category does not require knowledge of the system dynamics. These methods often rely on probabilistic approaches (Geibel and Wysotzki, 2005; Liu et al., 2022) or constrained

Figure 4: Number of failed training episodes out of the first 100 in Glucose environment with different degrees of parameter discrepancy.

Figure 3: Comparison of the converged trajectories and their corresponding normalized return. In the upper row, the agents try to retain the desired state under time-varying disturbances; in the lower row, the agents try to steer the system to a desired state. Although SAC fails before converging, here we compare with the converged SAC results to show that RL-AR can achieve the performance standard of model-free RL that prioritizes return and disregards safety.

MDP (Achiam et al., 2017; Yang et al., 2020). More recent methods use learned models to filter out unsafe actions (Bharadhwaj et al., 2020). However, these methods need to observe failures to estimate the safety cost, thus do not ensure safety during training. This category of methods does not apply to the single-life setting in this work, i.e., no failure is tolerated in the actual environment. Nevertheless, in Fig. 2 we evaluate pertaining CPO (Achiam et al., 2017) and SE Editor (Yu et al., 2022b) using simulation with the estimated model to obtain risk estimation before training in the actual environment.

The second category relies on an estimated model of the system dynamics. Some methods enforce safety constraints using Control Barrier Function (CBF) (Cheng et al., 2019). However, CBF minimizes the control effort without directly optimizing the system performance. In contrast, the MPC regularizer used in RL-AR enforces safety constraints while optimizing the predicted performance, resulting in high performance during training. Some methods compute a model-based projection to verify the safety of actions (Bastani, 2021; Kochdumper et al., 2023; Fulton and Platzer, 2018). However, the scalability of verification-based methods for complex control applications is an issue. Anderson et al. (2020) propose using neurosymbolic representations to reduce verification complexity, but the computational cost remains to be high. On the other hand, the average time for RL-AR to take a step (including the environment interaction and network updates) in the four environments in Section 4 is 0.037 s, which is practical for real-time control.

Gros and Zanon (2019) and Zanon et al. (2020) use MPC as the policy generator and use RL to dynamically tune the MPC parameters in the cost functions and the estimated environment model. Assuming discrepancies between the estimated model parameters and the actual environment parameters, the tuning increases the MPC's performance. However, this is a strong assumption since there are other discrepancies, such as neglected dynamics and discretization errors. However, the RL-AR proposed in this work can theoretically converge to the optimal policy by utilizing the model-free RL agent.

It is important to note that although the MPC regularizer accelerates the learning of the RL agent, RL-AR is not a special case of transferring a learned policy. The MPC regularizer used in our proposed algorithm forecasts the system behavior and hard-codes safety constraints in the optimization. The main role of the MPC is to keep the RL-AR actions safe in the actual environment--not transferring knowledge. Transfer learning in RL studies the effective reuse of knowledge, especially across different tasks (Taylor and Stone, 2009; Glatt et al., 2020). By reusing prior knowledge, transferred RL agents skip the initial random trial-and-error and drastically increase sampling efficiency (Karimpanal et al., 2020; Da Silva and Costa, 2019). However, transferred RL agents are not inherently risk-aware, and thus can still steer the actual environment into unsafe states. For this reason, transferred RL is not generally considered effective for ensuring safety.

## 6 Conclusion and Future Works

Controlling critical systems, where unsafe control actions can have catastrophic consequences, has significant applications in various disciplines from engineering to medicine. Here, we demonstrate that the appropriate combination of a control regularizer can facilitate safe RL. The proposed method, RL-AR, learns a focus module that relies on the safe control regularizer for less-exploited states and simultaneously allows unbiased convergence for well-exploited states. Numerical experiments in critical applications revealed that RL-AR is safe during training, given a control regularizer with reasonable safety performance. Furthermore, RL-AR effectively learns from interactions and converges to the performance standard of model-free RL that disregards safety.

One limitation of our setting is the assumption that the estimated model has reasonable accuracy for deriving a viable control regularizer. Although this assumption is common in the control and safe RL literature, one possible direction for future work is to design more robust algorithms against inaccurate estimated models of the actual environment. A potential approach is to update the estimated model using observed transitions in the actual environment. However, the practical challenge is to adequately adjust all model parameters even with a small number of transitions observed in the actual environment. In addition, for such an approach, managing controllability, convergence, and safety requires careful design and tuning.