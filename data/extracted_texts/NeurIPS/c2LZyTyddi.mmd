# BIOT: Biosignal Transformer for Cross-data Learning in the Wild

Chaoqi Yang\({}^{1}\), M. Brandon Westover\({}^{2,3}\), Jimeng Sun\({}^{1}\)

\({}^{1}\)University of Illinois Urbana-Champaign, \({}^{2}\)Harvard Medical School

\({}^{3}\)Beth Israel Deaconess Medical Center

{chaoqiy2}@illinois.edu

###### Abstract

Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals (based on CNN, RNN, and Transformers) are typically specialized for specific datasets and clinical settings, limiting their broader applicability. This paper explores the development of **a flexible biosignal encoder architecture** that can enable pre-training on multiple datasets and fine-tuned on downstream biosignal tasks with different formats.

To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose Biosignal Transformer (BIOT). The proposed BIOT model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing different biosignals into unified "sentences" structure. Specifically, we tokenize each channel separately into fixed-length segments containing local signal features and then re-arrange the segments to form a long "sentence". Channel embeddings and _relative_ position embeddings are added to each segment (viewed as "token") to preserve spatio-temporal features.

The BIOT model is versatile and applicable to various biosignal learning settings across different datasets, including joint pre-training for larger models. Comprehensive evaluations on EEG, electrocardiogram (ECG), and human activity sensory signals demonstrate that BIOT outperforms robust baselines in common settings and facilitates learning across multiple datasets with different formats. Using CHB-MIT seizure detection task as an example, our vanilla BIOT model shows 3% improvement over baselines in balanced accuracy, and the pre-trained BIOT models (optimized from other data sources) can further bring up to 4% improvements. Our repository is public at https://github.com/ycq091044/BIOT.

## 1 Introduction

Biosignals, such as EEG and ECG, are multi-channel time series recorded at high sampling rates (e.g., 256Hz) in various healthcare domains, including sleep medicine, neurological and cardiovascular disease detection, and activity monitoring. Deep learning (DL) models have demonstrated impressive success in automating biosignal analysis across diverse applications (Yang et al., 2021), encompassing sleep stage classification (Biswal et al., 2018; Yang et al., 2021; Phan and Mikkelsen, 2022), emotion analysis via EEG (Zhang et al., 2020; Suhaimi et al., 2020), action and motor imagery recognition (Venkatachalam et al., 2020), acute stress detection through electrodermal activity (Greco et al., 2021), EEG-based seizure epilepsy classification (Yang et al., 2023; Jing et al., 2023), and ECG-driven cardiac arrhythmia detection (Isin and Ozdalili, 2017; Parvaneh et al., 2019).

Various deep learning methods have been developed for biosignal learning. Some works use 1D convolutional neural networks (CNN) on raw signals (Jing et al., 2023; Nagabushanam et al., 2020;Dar et al., 2020), while others preprocess the data with short-time Fourier transform (STFT) and employ 2D CNN models on the resulting spectrogram (Yang et al., 2022; Kim et al., 2020; Cui et al., 2020). Researchers also segment the signal and use a CNN segment encoder with a downstream sequence model (Zhang et al., 2019; Biswal et al., 2018; Jing et al., 2020; Almutairi et al., 2021), such as Transformer or recurrent neural networks (RNN), to capture temporal dynamics. Other approaches involve ensemble learning, feature fusion from multiple encoders (Li et al., 2022), and multi-level transformers to encode spatial and temporal features across and within different channels (Lawhern et al., 2018; Song et al., 2021; Liu et al., 2021).

These models (Jing et al., 2023; Yang et al., 2021; Biswal et al., 2018; Kostas et al., 2021; Du et al., 2022; Zhang et al., 2022) predominantly focus on biosignal samples with fixed formats for specific tasks, while real-world data may exhibit mismatched channels, variable lengths, and missing values. In this paper, our objective is to devise a flexible training strategy that can handle diverse biosignal datasets with varying channels, lengths, and levels of missingness. For example, is it possible to transfer knowledge from abnormal EEG detection (a binary classification task with samples having 64 channels, 5-second duration, recorded at 256Hz) to improve another EEG task, such as seizure type classification (a multi-class task with 16 channels and a 10-second duration at 200Hz)? In reality, such data mismatches often arise from varying devices, system errors, and recording limitations. Additionally, it is also important to explore the potential of utilizing various unlabeled data sources.

To apply existing deep learning models to such settings of different biosignals, significant data processing is required to align the formats across multiple datasets. This may involve truncating or padding signals for consistent lengths (Zhang et al., 2022), and imputing missing channels or segments (Bahador et al., 2021). Such practices, however, may introduce unnecessary noise and shift data distributions, leading to poor generalization performance. Developing a flexible and unified model that accommodates biosignals with diverse formats can be advantageous.

In our paper, we develop the biosignal transformer (BIOT, summarized in Figure 1), which, to the best of our knowledge, is the first multi-channel time series learning model that can handle biosignals of various formats. Our motivation stems from the vision transformer (ViT) (Dosovitskiy et al., 2020) and the audio spectrogram transformer (AST) (Gong et al., 2021). The ViT splits the image into a "sentence" of patches for image representation while the AST splits the audio spectrogram into a "sentence" for 1D audio representation. We are inspired by that these "sentence" structures combined with Transformer (Vaswani et al., 2017) can handle variable-sized inputs.

However, it is non-trivial to transform diverse biosignals of various formats into unified "sentence" structures. This paper proposes BIOT to solve the challenge by a novel **biosignal tokenization module** that segments each channel separately into tokens and then flattens the tokens to form consistent biosignal "sentences" (illustrated in Figure 2). With the design, our BIOT can enable the knowledge transfer cross different data in the wild and allow joint (pre-)training on multiple biosignal data sources seamlessly. Our contributions are listed below.

* [leftmargin=*,noitemsep,nolistsep]
* **Biosignal transformer (BIOT).** This paper proposes a generic biosignal learning model BIOT by tokenizing biosignals of various formats into unified "sentences."
* **Knowledge transfer across different data.** Our BIOT can enable joint (pre-)training and knowledge transfer across different biosignal datasets in the wild, which could inspire the research of large foundation models for biosignals.
* **Strong empirical performance.** We evaluate our BIOT on several unsupervised and supervised EEG, ECG, and human sensory datasets. Results show that BIOT outperforms baseline models and can utilize the models pre-trained on similar data of other formats to benefit the current task.

## 2 BIOT: Biosignal Transformer

As shown in Figure 1, our BIOT encoder cascades two modules: (i) the **biosignal tokenization** module that tokenizes an arbitrary biosignal (variable lengths, different channels, and missing values) into a "sentence" structure. This design can potentially enable previous language modeling techniques (Devlin et al., 2018; Liu et al., 2019; OpenAI, 2023) to empower the current biosignal models; (ii) a **linear transformer** module that captures complex token interactions within the "sentence" while maintaining linear complexity. After that, we also discuss the application of BIOT in different real-world settings. For all the notations used in the paper, we summarize them in Appendix A.

### Module 1: Biosignal Tokenization

**Motivation.** The goal of this paper is to model heterogeneous biosignals (e.g., EEG samples with different channels for different tasks) with a unified model. For example, common EEG samples (Lopez et al., 2015), such as those for seizure detection, are recorded at 256Hz in the international 10-20 system 1 for 10-second long (Klem et al., 1999). With standard 16 montage editing, the sample is essentially a multi-channel time-series, represented as a matrix of size (16, 2560). However, format mismatch may prevent the applications on other similar data, such as **different sampling rate** (e.g., 200Hz vs 256Hz) (Jing et al., 2023), **mismatched channels** (i.e., different datasets have their own novel channels), **variable recording duration** (i.e., 30s per sample vs. 10s) (Zhang et al., 2022), **missing segments** (i.e., part of the recording is damaged due to device error). Thus, existing models may fail to utilize the mismatched data from different datasets.

Our BIOT solves the above challenges of mismatches by the following steps. We assume one biosignal sample as \(^{I J}\). Here, \(I\) as the channels and \(J=duration(s) rate(Hz)\) as the length.

* **Resampling.** We first resample all data to the same rate, denoted by \(r^{+}\) (such as 200Hz), by linear interpolation. The common rate \(r\) could be selected following clinical knowledge of a certain biosignal. For example, the highest frequency of interest in both EEG and ECG signals is commonly around 100 Hz, and thus 200 Hz or 250 Hz can be suitable for typical EEG or ECG applications, according to Nyquist-Shannon sampling theorem (Nyquist, 1928; Shannon, 1949).
* **Normalization**: To alleviate the unit difference and amplitude mismatch across different channels and datasets, we use the 95-percentile of the absolute amplitude to normalize each channel. Formally, each channel \([i]\) is normalized by \([i]}{([[i,1]],[[i,2]],...,[[i,J]]],\,95\%)}\).

Figure 1: Biosignal Transformer (BIOT). (Upper) Given a new data sample, we initially perform data preprocessing (resampling, normalization, tokenization, and flattening) to create a biosignal ”sentence” using the **biosignal tokenization module**. We then learn the complex interactions within the ”sentence” through the **linear transformer module**. (Lower) BIOT encoder is versatile, enabling supervised learning on complete data, data with missing values, and pre-training and fine-tuning across diverse data formats and tasks.

* **Tokenization**: For handling length variation, we tokenize the recording of each channel into segments/tokens of length \(t^{+}\), and neighboring tokens can have overlaps of length \(p\) (\(p<t\), e.g., \(t=r\) and \(p=0.5r\) corresponds to 1s and 0.5s). Thus, the \(k\)-th token (\(k=1,2,..,K\)) in the \(i\)-th channel can be represented by slicing \([i,(t-p)(k-1):(t-p)(k-1)+t]\). The number of tokens \(K\) per channel is limited by the inequality: \((t-p)(K-1)+t J\). Here, the overlap \(p\) is essential to maintain the temporal information for shorter signals. For example, if the length of a signal is 3 seconds (i.e., \(J=3r\)), the set \(t=r,p=0\) only generates 3 tokens per channel, while the set \(t=r,p=0.5r\) gives 5 tokens per channel (see Appendix C.3). In cases of missing values, we drop the corresponding tokens directly (as shown in **Sample 2**). Note that, our tokenization applies to each channel, separately, which is different from previous works (Almutairi et al., 2021; Du et al., 2022) that split all channels together (which cannot work on **Sample 2**).
* **Flattening**: We finally flatten tokens from all channels into a consistent "sentence".

The above steps are non-parametric. To study the effect of sampling rate \(r\), token length \(t\), and overlap \(p\), we provide ablation studies and insights in Appendix C.3. In the following, we design the token embedding for the biosignal "sentence", which combines information from three aspects.

* **Segment embedding.** We leverage fast Fourier transform (FFT) to extract an energy vector for each token where every dimension indicates the energy of a specific frequency. A fully connected network (FCN) is then applied on the energy vector to transform it into the segment embedding. Directly modeling the raw time-series to learn a segment embedding can be another option.
* **Channel embedding (spatial).** We learn an embedding table for all different channels and add the corresponding channel embedding to the token. Each color represents one channel in Figure 2.
* **Positional embedding (temporal).** In biosignals, the token order within the channel captures temporal information. We thus add _relative_ positional embedding to the token embedding by using the sinusoidal/cosine functions (Vaswani et al., 2017), which does not need learnable parameters.

The above three embeddings are summed to create the token embedding, which can effectively capture the segment features as well as the spatio-temporal features. Finally, we denote the tokenzied "sentence" as \(^{N l_{1}}\) where \(N\) is the number of tokens and \(l_{1}\) is the embedding dimension.

### Module 2: Linear Transformer

**Transformer with linear complexity for long biosignal "sentence".** Next, we want to leverage the Transformer model (Vaswani et al., 2017) for modeling the "biosignal sentence". However, our transformation may lead to long "sentences" due to multiple channels. For example, the "sentence"

Figure 2: Biosignal Tokenization (no segment overlap in the examples). **Sample 1** has four channels (Fp1, Fp2, O1, and O2) for 5 seconds. We tokenize each channel into segments/tokens and then parameterize these 20 segments with three embeddings. On the right, we use different colors to represent the channels (blue-Fp1, brown-Fp2, green-O1, and yellow-O2). **Sample 2** has mismatched channels (no O2), variable lengths (Fp1 and Fp2 are shorter and flipped), and missing values (in O1). Using our method, we can still tokenize Sample 2 in a comparable ”sentence”. On both samples, the peach orange area indicates the spatial- or temporal-relevant tokens w.r.t. the highlighted token.

of a 64-channel EEG signal for 20 seconds (without overlaps) may have \(64 20=1280\) tokens, and longer with the overlaps. Given that the original Transformer model is known to have quadratic complexity in both time and space, we adopt the linear attention mechanism (Wang et al., 2020; Katharopoulos et al., 2020) for biosignal learning applications.

Formally, let us assume \(^{K},^{V},^{Q}^{l_{1} l_{2}}\) be the key, value, and query matrices. Our self-attention module uses a rank-\(d\) approximation for the softmax attention (\(N N\)) by reduced-rank parameter matrices \(^{}^{N d},^{d N}\) (where \(d N\)). The output \(^{N l_{2}}\) is,

\[ =\ (^{Q}, ^{K},^{V})\] (1) \[=\ ( ^{Q})(^{K})^{}}{}} )}_{N d}^{V}}_{d  l_{2}}.\] (2)

Main components of this module include one linear self-attention layer and one fully connected network. To enable stable training, we add layer normalization (Ba et al., 2016) before each component, residual connection (He et al., 2016) after each component, and dropout (Srivastava et al., 2014) right after the self-attention (see Figure 1), which improves the convergence.

BIOt **Encoder.** An illustration of our proposed BIOT encoder is shown in Figure 1 (upper), which comprises the **biosignal tokenization** module and multiple blocks of **linear transformer** modules. We obtain the final biosignal "sentence" embedding by a mean pooling step over all tokens. Note that appending a classification [CLS] token at the beginning of the "sentence" (after Module 1) is also a common option. However, we find it yields a slightly worse performance in our application, and thus we use mean pooling in the experiments.

### Biosignal Learning in the Wild

Our proposed BIOT encoder can be applied in various real-world biosignal applications (illustrated in the lower part of Figure 1). These applications include (1) standard supervised learning, (2) learning with missing channels or segments, (3) & (4) pre-training on one or more datasets, and fine-tuning on other similar datasets with different input formats.

**(1) Supervised Learning** is the most common setting in the previous literature (Jing et al., 2023; Biswal et al., 2018). With the BIOT encoder model, we finally apply an exponential linear unit (ELU) activation (Clevert et al., 2015) and a linear layer for classification tasks.

**(2) Supervised Learning (with missing).** Many real biosignal data have mismatched channels, missing segments, and variable lengths, which prevents the applications of existing models (Jing et al., 2023; Song et al., 2021). Flexible as our model is, BIOT can be applied in this setting using the same model structure as in **(1)**.

**(3) Unsupervised Pre-training.** We can jointly pre-train a general-purpose BIOT encoder on multiple large unlabeled datasets. In the experiments, we pre-train an unsupervised encoder using 5 million resting EEG samples (16 channels, 10s, 200Hz) and 5 million sleep EEG samples (2 channels, 30s, 125Hz), which is later utilized to improve various downstream tasks.

For the unsupervised pre-training, we take the following steps (a diagram is shown in Figure 1).

* Assume \(\) is the original biosignal. We first randomly dropout part of its channels and dropout part of the tokens from the remaining channels, resulting in a perturbed signal \(}\).
* We then obtain the embeddings of \(\) and \(}\) by the same BIOT encoder. To form the objective, we want to use the perturbed signal to predict the embedding of the original signal. Thus, an additioanl predictor (i.e., two-layer neural network) is appended for the perturbed signal following (Grill et al., 2020). We use \(\) and \(}\) to denote the real embedding of \(\) and predicted embedding from \(}\). \[=\ (),\ \ }=\ ((})).\] (3)
* Finally, contrastive loss (He et al., 2020; Chen et al., 2020) is used on \(\) and \(}\) to form the objective. \[=\ (( ,}^{}/T),).\] (4)

Here, \(T\) represents the temperature (\(T=0.2\) throughout the paper) and \(\) is an identity matrix. In the implementation, we also apply sample-wise L2-normalization on \(\) and \(}\) before matrix product.

**(4) Supervised Pre-training** aims to pre-train a model by supervised learning on one task and then generalize and fine-tune the encoder on a new task. The goal here is to transfer knowledge among different datasets and gain improvements on the new task compared to training from scratch. Our BIOT model allows the new datasets to have mismatched channels and different lengths.

## 3 Experiments

This section shows the strong performance of BIOT on several EEG, ECG, and human sensory datasets. Section 3.2, 3.3 compare BIOT with baselines on **supervised learning** and **learning with missing** settings. Section 3.4, 3.5, 3.6 show the that BIOT can be flexibly **pre-trained on various datasets** (supervised or unsupervised) to improve a new task with different sample formats.

### Experimental Setups

**Biosignal Datasets.** We consider the following datasets in the evaluation: (i) **SHHS**(Zhang et al., 2018; Quan et al., 1997) is a large sleep EEG corpus from patients aged 40 years and older. (ii) **PREST** is a large unlabeled proprietary resting EEG dataset; (iii) **Cardiology**(Alday et al., 2020) is a collection of five ECG datasets (initially contains six, and we exclude the PTB-XL introduced below). (iv) The **CHB-MIT** database (Shoeb, 2009) is collected from pediatric patients for epilepsy seizure detection. (v) **IIIC Seizure** dataset is from Ge et al. (2021); Jing et al. (2023) for detecting one of the six icatl-interictal-injury-continuum (IIIC) seizure patterns (OTH, ESZ, LPD, GPD, LRDA, GRDA); (vi) TUH Abnormal EEG Corpus (**TUAB**) (Lopez et al., 2015) is an EEG dataset that has been annotated as normal or abnormal; (vii) TUH EEG Events (**TUEV**) (Harati et al., 2015) is a corpus of EEG that contains annotations of segments as one of six sleep or resting event types: spike and sharp wave (SPSW), generalized periodic epileptiform discharges (GPED), periodic lateralized epileptiform discharges (PLED), eye movement (EYEM), artifact (ARTF) and background (BCKG); (viii) **PTB-XL**(Wagner et al., 2020) is an ECG dataset with 12-lead recordings for diagnosis prediction, and we used it for arrhythmias phenotyping in this paper; (ix) **HAR**(Anguita et al., 2013) is a human action recognition dataset using smartphone accelerometer and gyroscope data.

**Dataset Processing.** The first three datasets are used entirely for unsupervised pre-training. The next four datasets are used for supervised learning, and we used the common 16 bipolar montage channels in the international 10-20 system. For CHB-MIT (containing 23 patients), we first use patient 1 to 19 for training, 20,21 for validation, and 22,23 for test. Then, we flip the validation and test sets and conduct the experiments again, and we report the average performance on these two settings. For IIIC seizure, we divide patient groups into training/validation/test sets by 60%:20%:20%. For TUAB and TUEV, the training and test separation is provided by the dataset. We further divide the training patients into training and validation groups by 80%:20%. For PTB-XL, we divide patient groups into training/validation/test sets by 80%:10%:10%. The train and test set of HAR is provided, and we further divide the test patients into validation/test by 50%:50%. For all the datasets, after assigning the patients to either training, validation, or test groups, we will further split the patient's recording to samples, and the sample duration accords to the annotation files. The dataset statistics can be found in Table 1, and we provides more descriptions and processing details in Appendix B.1.

**Baseline.** We consider the following representative models: (i) **SPaRCNet**(Jing et al., 2023) is a 1D-CNN based model with dense residual connections, more advanced than the popular ConvNet (Schirrmeister et al., 2017), CSCM (Sakhavi et al., 2018); (ii) **ContraWR**'s (Yang et al., 2021) encoder model first transforms the biosignals into multi-channel spectrogram and then uses 2D-CNN based ResNet (He et al., 2016); (iii) **CNN-Transformer**(Peh et al., 2022) is superior to CNN-LSTM models (Zhang et al., 2019); (iv) **FFCL**(Li et al., 2022) combines embeddings from CNN and LSTM

  
**Datasets** & **Type (subtype)** & **\# Recordings** & **Rate** & **Channels** & **Duration** & **\# Sample** & **Tasks** \\  SHHS & EEG (sleep) & 5,445 & 125Hz & C3-A2.4-C4.1 & 30 seconds & 5,932.52 & Unsupervised pre-training \\ PREST & EEG (testing) & 6,478 & 200Hz & 16 meetings & 10 seconds & 51,092.99 & Unsupervised pre-training \\ Cardiology & ECG & 21,264 & 500Hz & 6 or 12 ECG leads & 10 seconds & 495.970 & Unsupervised pre-training \\  CIB-MIT & EEG (resting) & 686 & 256Hz & 16 meetings & 10 seconds & 326.993 & Binary (seizure or not) \\ IIIC Seizure & EEG (resting) & 2,702 & 200Hz & 16 meetings & 10 seconds & 165.309 & Multi-class (6 seizure types) \\ TUAB & EEG (knownknown) & 2,339 & 256Hz & 16 meetings & 10 seconds & 409,455 & Binary (abnormal or not) \\ TUEV & EEG (sleep and resting) & 11,914 & 256Hz & 16 meetings & 5 seconds & 112,491 & Multi-class (6 event types) \\  PTB-XL & ECG & 21,911 & 500Hz & 12 ECG leads & 5 seconds & 65,511 & Binary (arrhythmias or not) \\ HAR & Wearable sensors & 10,299 & 50Hz & 9 coordinates & 2.56 seconds & 10,299 & Multi-class (6 actions) \\   

Table 1: Dataset Statistics

[MISSING_PAGE_FAIL:7]

### Setting (2) - learning with missing channels and segments

The section simulates the TUEV dataset (16 channels and 5s per sample) and IIIC Seizure (16 channels and 10s per sample) to mimic the setting of _supervised learning with missing channels and segments_, which shows the strong performance of BIOT. We consider three missing cases:

* **Missing segments**: Randomly mask out \(a\) segments (each segment spans for 0.5 seconds), \(a=0,1,2,3,4,5\) with equal probability. The segment masking is applied separately for each channel.
* **Missing channels**: Randomly mask out \(b\) channels, \(b=0,1,2,3,4\) with equal probability. We assume that the masking will not alter the underlying labels (the same assumption for other cases).
* **Missing both channels and segments**: Combine both masking strategies simultaneously.

To enable the baseline models compatible with the setting, we use all zeros to impute the masked regions. The comparison is plotted in Figure 3, which shows that (i) all models decrease the performance with more missings while BIOT and the pre-trained BIOT are less impacted (especially on Kappa and Weighted F1); (ii) "Missing channels" affects the performance more than "Missing segments", which makes sense as segment masking still preserves information from all channels.

### Setting (3) - unsupervised pre-training

This section shows that BIOT enables unsupervised pre-training on existing datasets with various formats. Note that, all the pre-trained models in this paper have a similar scale (\(\)3.3 million parameters). We defer the study of "scaling effect" of pre-trained BIOT to future work.

* **Pre-trained (PREST)**: This model is pre-trained on 5 million resting EEG samples (PREST) with 2,048 as the batch size. We save the pre-trained model at the 100-th epoch.
* **Pre-trained (PREST+SHHS)**: This model is jointly pre-trained on 5M PREST and 5M SHHS EEG samples. Though two datasets have different sample formats, our model is able to use them together. Also, we use 2048 as the batch size and save model at the 100-th epoch.
* **Pre-trained (Cardiology-12)** is jointly pre-trained on raw data of five datasets in Cardiology corpus (details in Appendix B.1). We use 1024 as batch size and save model at the 100-th epoch.
* **Pre-trained (Cardiology-6)** is pre-trained similarly as Pre-trained (Cardiology-12), while we only utilize the first 6 ECG leads. By contrast, Pre-trained (Cardiology-12) uses full 12 leads.

We fine-tune the first two pre-trained EEG models on four EEG tasks and append the results to Table 2 (also in Appendix C.1). We fine-tune the last two pre-trained ECG models on PTB-XL datasets in Table 3. The pre-trained models can be seamlessly applied to various downstream tasks with different sample formats. Results show that the pre-trained models can improve the performance consistently after fine-tuning on the training set of downstream tasks in Table 2 and Table 3.

Figure 3: Supervised learning with missing channels or segments (on TUEV and IIIC Seizure)

### Setting (4) - supervised pre-training on other tasks

This section shows that BIOT allows knowledge transfer from one task to another similar task with different sample formats. We pre-train on the training set of CHB-MIT, IIIC Seizure, TUAB and fine-tunes on TUEV (which has 16 channels and 5s duration). All datasets use 200Hz sampling rate. We design three sets of configurations for the pre-trained datasets: **Format (i)** uses the first 8 channels and 10s duration; **Format (ii)** uses the full 16 channels but only the first 5s recording; **Format (iii)** uses full 16 channels and full 10s recording. During fine-tuning, we then remove the prediction layers from these pre-trained model and add a new prediction layer to fit the TUEV dataset.

The fine-tuning results on TUEV are shown in Figure 4 where we also add the vanilla BIOT for reference. We find that (i) the model pre-trained on IIIC Seizure and TUAB are generally beneficial for the event classification task on TUEV. The reason might be that TUAB and TUEV are both recorded from Temple University and share some common information, while IIIC seizure and TUEV are both related to seizure detection and may share some latent patterns. (ii) More pre-training data leads to better results in the downstream task: though the pre-training configuration (16 channels, 5 seconds) aligns better with the TUEV data formats, the results show that configuration of (16 channels, 10 seconds) encodes longer duration and works consistently better. (iii) Compared to the TUEV results in Appendix C.1, we also find that oftentimes the supervised pre-training (e.g., on IIIC seizure or TUAB) can be more effective than unsupervised pre-training (e.g., on SHHS and PREST).

### Pre-trained on all EEG datasets

In this section, we show that BIOT can leverage all six EEG resources considered in the paper. We obtain a **Pre-trained (6 EEG datasets)** model by loading the Pre-trained (PREST+SHHS) model and further train it on the training sets of CHB-MIT, IIIC Seizure, TUAB, and TUEV. We add separate classification layers for four tasks. Essentially, this model is pre-trained on all six EEG datasets. To use the model, we still fine-tune it on the training set of downstream tasks and append the results to Table 2 and Appendix C.1. Apparently, Pre-trained (6 EEG datasets) outperforms the vanilla BIOT and is better than the unsupervised and the supervised pre-trained BIOT models in most cases.

## 4 Conclusions and Discussions

This paper proposes a new biosignal transformer model (BIOT) that learns embeddings for biosignals with various formats. BIOT can enable effective knowledge transfer across different data and allow joint training on multiple sources. We conduct extensive evaluations on nine biosignal datasets and show that our BIOT is flexible and effective in various cross-data learning settings. Future efforts can explore the different types of biosignals and pre-training an all-in-one unified biosignal model. We hope our work can inspire more follow-up researches of large foundational models for biosignals.