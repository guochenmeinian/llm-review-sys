# Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data

Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data

Alex Havrilla

Department of Mathematics

Georgia Institute of Technology

Atlanta, GA, 30308

ahavrilla3@gatech.edu

&Wenjing Liao

Department of Mathematics

Georgia Institute of Technology

Atlanta, GA, 30308

wliao60@gatech.edu

###### Abstract

When training deep neural networks, a model's generalization error is often observed to follow a power scaling law dependent both on the model size and the data size. Perhaps the best known example of such scaling laws are for transformer-based large language models (**LLMs**), where networks with billions of parameters are trained on trillions of tokens of text. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. To answer this question, we establish novel statistical estimation and mathematical approximation theories for transformers when the input data are concentrated on a low-dimensional manifold. Our theory predicts a power law between the generalization error and both the training data size and the network size for transformers, where the power depends on the intrinsic dimension \(d\) of the training data. Notably, the constructed model architecture is shallow, requiring only logarithmic depth in \(d\). By leveraging low-dimensional data structures under a manifold hypothesis, we are able to explain transformer scaling laws in a way which respects the data geometry. Moreover, we test our theory with empirical observation by training LLMs on natural language datasets. We find the observed empirical scaling laws closely agree with our theoretical predictions. Taken together, these results rigorously show the intrinsic dimension of data to be a crucial quantity affecting transformer scaling laws in both theory and practice.

## 1 Introduction

Deep learning has made remarkable breakthroughs in various real-world applications, such as natural language processing (Graves et al., 2013; Bahdanau et al., 2014; Liu et al., 2023; Vaswani et al., 2017), computer vision (Krizhevsky et al., 2012; Goodfellow et al., 2014; Song et al., 2020), healthcare (Miotto et al., 2018), and robotics (Gu et al., 2017). A neural scaling law between the generalization error (or test loss) and several quantities, including the model size, the training data size, and the amount of compute, plays a key role in the performance of neural networks. Perhaps the best known example of such scaling laws are for transformer-based LLMs. Recent works in Hestness et al. (2017); Rosenfeld et al. (2019); Kaplan et al. (2020); Bahri et al. (2021) demonstrated a power law between the test loss and the network size, the training data size, and the amount of compute for transformer-based LLMs. Yet, despite sustained widespread interest, a rigorous understanding of why _transformer scaling laws_ exist is still missing.

Understanding the theory behind neural scaling laws provides invaluable insights into practical applications of deep learning. A mathematical principal of neural scaling laws enables researchers and practitioners to describe and analyze the performance of neural networks with precision and rigor. The neural scaling law between the generalization error and the network size can be partially explained via neural network representation theory (Yarotsky, 2016). Further, the neural scaling law between the generalization error and the training data size \(n\) can be explained via statistical estimation theory. For feedforward neural networks (Schmidt-Hieber, 2020) and convolutional residual networks (Oono and Suzuki, 2019), a generalization error bound has been established for regression. Schmidt-Hieber (2020); Oono and Suzuki (2019) predicted Generalization Error \( n^{-c/D}\) where \(n\) is the training data size, \(D\) is the data dimension and \(c\) is a constant. This predicted rate of convergence is extremely slow for high dimensional data when \(D\) is large, while the rate of convergence observed in real-world applications is significantly faster, which reveals a gap between theory and practice.

This gap can be bridged by exploiting low-dimensional structures of data. Real-world data sets often exhibit low-dimensional geometric structures due to rich local regularities, global symmetries, or repetitive patterns (Tenenbaum et al., 2000; Roweis and Saul, 2000). According to Min et al. (2023, Figure 1), the intrinsic dimension of CIFAR-100, CelebA and ImageNet datasets are about \(20,20\) and \(40\) respectively. When the low-dimensional geometric structure of data is modeled by a manifold, the predicted scaling for regression, classification and distribution estimation becomes Generalization Error \( n^{-c/d}\), where \(n\) is the training data size, \(d\) is the intrinsic dimension of the data manifold, and \(c\) is a constant (Chen et al., 2022; Liu et al., 2021; Dahal et al., 2022; Nakada and Imaizumi, 2020). In Sharma and Kaplan (2022), the neural scaling law between the test loss and the network size was predicted to be Test loss \((size)^{-4/d}\) where \(d\) is the intrinsic dimension of data. While the theoretical studies focus on feedforward neural networks (Chen et al., 2022; Nakada and Imaizumi, 2020) and convolutional residual networks (Liu et al., 2021), a generalization to transformer-based neural networks (Vaswani et al., 2017) is of great interest but widely open.

This paper establishes mathematical approximation and statistical estimation theories to predict and justify the scaling law between the generalization error and the model/data size for transformer neural networks. We consider regression of a \(\)-_Holder continuous function_\(f:\) where \(\) is a \(d\)-dimensional compact Riemannian manifold isometrically embedded in \(^{D}\). After embedding the input \(x^{D}\) to a proper sequence, we apply a transformer network on the embedded sequence to learn the function \(f\). Our main results are on the statistical estimation and universal approximation theories of Holder continuous functions on \(\) by transformer neural networks.

**Statistical Theory:** In Theorem 1, we consider the global empirical risk minimizer \(_{n}\) from \(n\) i.i.d. training data \(\{(x_{i},f(x_{i}))\}_{i=1}^{n}\), given by

\[_{n}=_{}_{i=1}^{ n}(x_{i})-f(x_{i})^{2},\] (1)

under a properly chosen transformer network architecture \(\). We prove that, the generalization error of \(_{n}\) satisfies

\[\|_{n}-f\|_{L^{2}(Q)}^{2}Dd^{2}n^{ -}\] (2)

where \(Q\) denotes the distribution of \(x\), and \(\) hides constants and \( n\) terms.

**Approximation Theory:** In Theorem 2, we construct a transformer network to universally approximate \(\)-Holder continuous functions on \(\) with an arbitrarily given accuracy \(\). Notably, the network is shallow, requiring only \(O(d)\) independent of the desired accuracy \(\) to approximate \(f\) locally. This highlights a major advantage of Transformers over feed-forward ReLU networks, which require \(O()\) layers to achieve the same accuracy.

In our proof, we embed the entries of \(x=[x^{1},,x^{D}]\) into tokens such that the \(x^{i}\)'s appear in a sequence. Our proof for the approximation theory explicitly constructs transformers to realize the interaction between different tokens efficiently via a crucial _Interaction Lemma_3. This lemma allows us to flexibly implement many common operations including addition, multiplication, and parallelization, and so may of independent interest. In our proof for the statistical theory, we calculate the covering number of our transformer network class, which is also of independent interest.

**Neural Scaling Laws and the Intrinsic Dimension:** Our generalization error bound in (2) predicts the following neural scaling law between the generalization error and the data size \(n\):

\[:=\|_{n}-f\|_{L^ {2}(Q)}^{2} n^{-_{D}},\ \ \ _{D}=,\] (3)with sufficient data. Our approximation theory in Theorem 2 predicts the following neural scaling law between the approximation error and the network size \(N\):

\[:=_{}\|-f\|_{L ^{}()}^{2} N^{-_{N}},\ \ \ _{N}=\] (4)

for a sufficiently large network class \(\). Our prediction of the power scaling law is consistent with our own empirical observations, and those in Kaplan et al. (2020) and Biderman et al. (2023). More importantly, our theory quantifies the power \(_{D},_{N}\) in terms of the intrinsic dimension of data.

**Experimental Validation on LLMs:** After establishing our theory we seek to validate it in practice by predicting empirical scaling laws for LLMs trained on natural language data. To test our predictions for the data scaling law, we pretrain a series of small (125 million parameter) LLMs on three datasets (Gokaslan et al., 2019; Eldan and Li, 2023; Kocetkov et al., 2022). We find close agreement (\( 0.02\)) between our predicted scaling exponent \(_{D}\) and the observed exponents \(_{D}\). To evaluate our predictions for the model scaling exponent \(_{N}\), we rely on publicly available scaling suites (Biderman et al., 2023; Radford et al., 2019) whose intrinsic data dimensions we can estimate. We find our predictions are still close but less accurate for \(_{N}\). Finally, we carry out a series of ablations investigating factors impacting the estimated intrinsic data dimension \(d\). For a fixed dataset, we find the estimated \(d\) is stable with respect to several factors including the model size, model embedding dimension, and context length1.

In summary, we make the following contributions:

* A novel approximation theory for transformers approximating Holder continuous functions on a \(d\)-dimensional manifold, requiring \(O((d))\) depth independent of the accuracy \(\).
* A novel computation of the covering number of our transformer network class. This is used to establish generalization bounds exponentially depending on the intrinsic dimension \(d\).
* Empirical experiments demonstrating our theory predicts data scaling laws for LLMs as a function of the estimated intrinsic data dimension \(d\).
* An empirical study of several factors affecting the estimated intrinsic data dimension for transformers including model size, embedding dimension, layer depth, and context length.

We will present our main theory in Section 2, numerical validation of our theory and the prediction of neural scaling laws in Section 3. We will discuss related work in Section 4 and conclude our paper in Section 5. Our pre-training hyperparameters are given in Appendix A. The derivation of neural scaling laws is presented in Appendix B. Our notation is given in Appendix C, and proofs are presented in Appendix E and F.

## 2 Transformer Generalization and Approximation Theory

This paper establishes statistical estimation and mathematical approximation theory of transformers for the regression of Holder functions on a low-dimensional manifold. We start by defining transformer neural networks.

### Transformer Neural Networks

**Definition 1** (Transformer Neural Network).: _We define a transformer neural network \(\) as a composition of functions of the form_

\[(x)=_{L_{T}}..._{1} (+(x))\] (5)

_which is parameterized by_

* \(L_{T}\)_: The number of transformer blocks_ \(_{i}\) _in_ \(\)_._
* \(m\)_: The maximum number of attention heads per transformer block._
* \(L_{}\)_: The max depth of the feed-forward layers per block._
* \(w_{}\)_: The max width of the feed-forward layers per block._

[MISSING_PAGE_FAIL:4]

a class of a transformer neural networks \((L_{T},L_{},w_{},l,d_{embbd},m,R,)\). The corresponding generalization error is given by

\[\|_{n}-f\|_{L^{2}(Q)}= }_{n}(x)-f(x)^{2}dQ(x)}.\] (6)

If \(\) and \(f\) satisfy Assumptions 1 and 2, we prove the following generalization error bound.

**Theorem 1**.: _Let \(M,,R,H_{f}>0\), \(0< 1\), \(d,D\), \(\) and \(f\) satisfy Assumption 1 and 2 respectively. Given \(n\) training samples \(\{(x_{i},f(x_{i}))\}_{i=1}^{n}\) where \(\{x_{i}\}_{i=1}^{n}\) are i.i.d. samples of a distribution \(Q\) supported on \(\), if we use the transformer neural network class \((L_{T},L_{},w_{},l,d_{embbd},m,R,)\) with parameters_

\[L_{T}=O(d), L_{}=O(n ), w_{}=O1, l=Odn^{}\] \[d_{embbd}=O1, m=Odn^{},=Od^{2}n^{}\]

_in the empirical risk minimization (1), where \(O\ \ \) hides terms in \(C_{}\) (the number of charts), \(D,H_{f},M\), then the empirical risk minimizer \(_{n}\) given by (1) satisfies_

\[_{}_{n}(x)-f(x)^{2}dQ Dd^{2}n^{}\]

_where \(\) hides logarithmic terms in \(n,d\) and linear terms in \(C_{}\)._

Theorem 1 is proved in Appendix F, via a bias-variance decomposition. The bias represents the approximation error of \(f\) by transformer neural networks, and the variance represents the stochastic error in the parameter estimation of transformer neural networks. To quantify the bias, we explicitly construct a transformer neural network to universally approximate \(\)-Holder continuous functions on \(\), to be detailed in Section 2.4. The variance is bounded by a novel calculation of the covering number of the transformer network class used in Theorem 1.

### Transformer Approximation Theory

**Theorem 2**.: _Let \(M,,R,H_{f}>0\), \(0< 1\), \(d,D\) and \(\) satisfy Assumption 1. For any \((0,1)\), there exists a transformer neural network class \((L_{T},L_{},w_{},l,d_{embbd},m,R,)\) with parameters_

\[L_{T}=O(d), L_{}=O( ^{-1}), w_{}=O1, l=Od ^{-}\] \[d_{embbd}=O1, m=Od^{-},=Od^{2}^{-},\]

_where \(O\ \ \) hides terms in \(C_{},D,H_{f},\), such that for any target function \(f\) satisfying Assumption 2, if the network parameters \(\) are properly chosen, then the network yields a function \(_{}(L_{T},L_{},w_{},l,d_{embbd},m,R,)\) with the approximation error_

\[\|_{}-f\|_{L^{}()}\]

Theorem 2 is proved in Appendix E.2. In our proof, we decompose \(f(x)\) as a sum of terms over local neighborhoods \(U_{1},...,U_{C_{}}\) covering \(\). Approximations on overlapping neighborhoods containing \(x\) will then be combined via a partition of unity (**PoU**) \(\{_{n}\}_{n=1}^{C_{}}\) which subordinates \(\{U_{n}\}_{n=1}^{C_{}}\). This will give us the expression \(f(x)=_{n=1}^{C_{}}f_{n}(x)_{U_{n}}(x)\) with \(f_{n}=f_{n}:\). On each local neighborhood \(U_{n}\), we project the input \(x^{D}\) to the tangent coordinate in \(^{d}\). This will give us the following _local decomposition_ of the target function:

\[f(x)=_{n=1}^{C_{}}_{n}_{n}(x)_{U_{n}}(x)\] (7)

where \(_{n}=f_{n}_{n}^{-1}:^{d}\) and \(_{n}:^{d}\) is a projection onto the local tangent space. We then construct transformers to approximate the \(_{n},_{n},_{U_{n}}\) components in (7). A diagram of the constructed transformer network approximating \(f:\) is given in Figure 1. The following key lemma is used to efficiently approximate each low-dimensional function \(_{n}\) on \(d\)-dimensional coordinates.

**Lemma 1**.: _Let \(H_{f},R>0\), \(d\) and \(0< 1\). For any \((0,1)\), there exists a transformer neural network class \((L_{T},L_{},w_{},l,d_{embd},m,R,)\) with parameters_

\[L_{T}=O(d), L_{}=O1, w_{}=O1, l=Od^{-}\] \[d_{embd}=O1, m=Od^{-},=Od^{2}^{-},\]

_where \(O\) hides terms in \(H_{f}\), such that, for any \(\)-Holder continuous function \(f:^{d}\), with Holder constant no more than \(H_{f}\) and \(\|f\|_{L^{}(^{d})} R\), if the network parameters \(\) are properly chosen, this transformer network yields a function \(_{}(L_{T},L_{},w_{},l,d_{embd },m,R,)\) such that_

\[\|_{}-f\|_{L^{}(^{d})}.\]

Lemma 1 is proved in Appendix E.1. We develop a novel lemma - _Interaction Lemma_3, implementing a highly-sparse pairwise interaction between two arbitrary tokens \(h_{t_{1}},h_{t_{2}}\), as a crucial architectural building block allowing us to easily implement more complex functions, architecture serialization, and parallelization (7). This result highlights a distinct advantage of transformer function approximation over ReLU function approximation (Yarotsky, 2016): **A transformer network only needs a constant \(O(d)\) number of layers to approximate \(f:^{d}\)** independent of the desired accuracy \(\). In contrast, the depth of ReLU feed-forward networks is in the order of \((^{-1})\)(Yarotsky, 2016) This is desirable from an empirical point of view, where wider networks instead of deeper ones tend to achieve superior performance (Kaplan et al., 2020; Lee et al., 2020).

## 3 Predicting Empirical Scaling Laws and Validation on LLMs

Our theory provides practical insights by predicting neural scaling laws for transformers, as given in (3) and (4), by explicitly quantifying the data scaling exponent \(_{D}\) and the model scaling exponent \(_{N}\) as a function of the intrinsic dimension (**ID**) \(d\). If we assume the language modeling objective has Lipschitz regularity such that \(=1\) in Assumption 2, then Theorem 1 predicts the scaling law between the squared generalization error and the data size \(n\), as given in (3), with \(_{D}=\), and the model scaling law with exponent given by \(_{N}=\). For a full derivation refer to Section B. We will observe how well our theory predicts these exponents both by pretraining small models from scratch and evaluating existing open-source model suites (Biderman et al., 2023).

In the following, we denote \(_{D}\) and \(_{N}\) as the scaling exponents predicted by our theory, where we numerically estimate the intrinsic dimension of data, denoted by \(d_{}\). The empirical exponents are

Figure 1: Diagram of the transformer architecture constructed in Theorem 2. T computes approximations of \(f(x)\) on each local chart \(U_{n}\) by first projecting \(x\) to the tangent coordinates in \(^{d}\) via \(_{n}(x)\) and then approximating \(f(x)\) with local Taylor polynomials. A shallow sub-network computes indicators \(_{U_{n}}\) for each local chart in parallel. The results of the two sub-networks are then multiplied together and summed to produce the final result. Here \(H_{i}\) denotes the embedding matrix before the \(i\)th transformer block \(_{i}\).

denoted by \(_{D}\) and \(_{N}\). To obtain the data scaling exponent \(_{D}\), we plot the test loss (comparable to squared error) versus the data size \(n\) in log-log scale, fit the log-log curve with a line, and then obtain \(_{D}\) from the magnitude of the slope. The model scaling exponent \(_{N}\) is obtained similarly.

Estimating the ID of TextTo predict scaling exponents, we must first estimate the intrinsic dimension of our pretraining dataset \(\). While we can do this directly for image datasets (Russakovsky et al., 2014; Pope et al., 2021), we cannot do this directly for textual datasets. Instead, we will estimate the intrinsic dimension of the input data by estimating the intrinsic dimension of token embeddings. Specifically, we will represent each input token with its corresponding final-layer token embedding. Given a pretraining test set \(_{}\) we embed a random \(l=1024\) length subsequence from each document \(D_{k}_{}\). We then randomly sub-sample \(32\) final-layer tokens from the embedded subsequence and shuffle together all the embeddings. To estimate the ID of the embeddings we use the Maximum Likelihood Estimation ID algorithm (Levina and Bickel, 2004; Pedregosa et al., 2011) with \(K=20\) neighbors. We split the sampled embedding tokens into batches of \(4096\) and run the MLE estimator on each batch, averaging together for the final result. Unless otherwise specified, we embed each document \(D_{k}_{}\) using a 125 million parameter model \(M\) with \(L_{T}=12\) layers and embedding dimension \(d_{embd}=768\). We first pretrain \(M\) on the full \(_{}\) for \(200,000\) steps or until convergence.

Intrinsic dimension predicts the empirical data scaling exponent \(_{D}\)To validate our prediction of the dataset scaling exponent \(_{D}\) we pretrain a series of 125 million parameter GPT-style LLMs on three different datasets: OpenWebText (Gokaslan et al., 2019), the SQL portion of The Stack (Kocetkov et al., 2022), and Tiny Stories (Eldan and Li, 2023). We train across three orders of dataset size to fit scaling laws. Detailed hyperparameters can be found in the Appendix A. We report the observed scaling laws in log scale in Figure 2. In addition, we plot our predicted test loss whose slope is given by \(_{D}=}}\) where \(d_{}\) is the our estimated intrinsic dimension.

Empirically, we find all three datasets produce nearly log-linear laws whose exponents lie between \(0.1<_{D}<0.15\). Tiny Stories has the largest exponent, indicating the fastest rate of convergence, followed by the SQL dataset, followed by OpenWebText. This matches our rough intuition, since Tiny Stories is a synthetically generated dataset with less complexity than the other two datasets and thus easier to learn. The predicted exponents \(_{D}\) generally over-estimate \(_{D}\) but otherwise closely match up to \( 0.02\) absolute error. Additionally, the predicted \(_{D}\) reflect the previously mentioned differences in complexity of pretraining datasets. In particular, Tiny Stories has a smaller estimated ID than both OpenWebText and SQL, resulting in a larger predicted \(_{D}\) as desired.

Predicting empirical model scaling exponent \(_{N}\) with intrinsic dimensionTo validate our predictions of the model scaling exponent \(_{N}=}}\), we evaluate two model scaling suites: GPT2 (Radford et al., 2019) and Pythia (Biderman et al., 2023). We refer to Kaplan et al. (2020) for GPT2's \(_{N}\) and estimate \(_{N}\) using OpenWebText as a proxy for GPT2's pretraining data. We compute \(_{N}\) for Pythia by evaluating each model on The Pile's test set (Gao et al., 2021). We also estimate \(d_{}\) on the publicly available pretraining data to predict \(_{N}\). The results are reported in Figure 3. Our predicted \(_{N}\) under-estimates the empirical \(_{N}\). We conjecture this to be due to a number of factors including possible under-training of the largest models and the intrinsic entropy of the data distribution.

Figure 2: Observed and predicted data scaling laws on OpenWebText, The Stack-SQL, and Tiny Stories pretraining datasets. All estimates are close (\( 0.02\)) and appear to reflect varying levels of pretraining data complexity. **Note:**\(_{D}\) denotes the empirically observed data scaling exponent and \(_{D}\) denotes the theoretically estimated exponent.

**Ablating the impact of model architecture on the estimated ID**  Practical application of our theory relies on a good estimate of the intrinsic dimension. However, there are many factors potentially biasing our estimate. Of particular interest is the embedding model's embedding dimension, depth, context length, and number of parameters. We ablate these factors in Figure 4, plotting estimated ID against each factor.

Overall, we find the estimated ID is fairly stable across each factor. As the number of parameters increases, the estimated ID of The Pile slightly increases from 15.56, via a 410 million parameter model, to 20.02 with a 12.8 billion parameter model. ID on OpenWebText behaves similarly when increasing the embedding dimension, increasing from 15.56 when \(d_{embd}=768\) to 18.68 when \(d_{embd}=1536\). When fixing a single model, we find the ID across intermediate embedding layers is small initially but then increases and decreases again, stabilizing around the ID of the final layer. We observe that the ID appears to inversely correlate with sequence length, decreasing from \(15.86\) for very short sequences to \(12.9\) for sequences around \(1024\) tokens.

Figure 4: **Top left: Estimated ID vs. number of parameters. Top right: Estimated ID vs. the embedding dimension. Bottom left: Variation of estimated ID across model layers. Bottom right: Variation of estimated ID across context position.**

Figure 3: Observed and predicted model scaling laws in model size on GPT2 and Pythia scaling suites. \(_{N}\) denotes the empirically observed scaling exponent, and \(_{N}\) denotes the theoretically predicted exponent. Note: we estimate \(_{N}\) for GPT2 using OpenWebText.

Predicting \(_{N}\) from \(_{D}\) (and vice versa) without estimating IDAbove we estimated \(_{D}\) and \(_{N}\) by first estimating the intrinsic dimension \(d\) for a model's pretraining dataset. However, estimating \(d\) may not always be possible when pretraining data is not public. Alternatively, we can predict \(_{D}\) in terms of \(_{N}\) (and vice versa) without ever needing to estimate \(d\):

\[_{D}==}{2+1}= }{_{N}+1},_{N}=}{1-_{D}}\]

See Table 1 for ID-free estimations of empirically observed exponents in the literature (Sharma and Kaplan, 2022; Hoffmann et al., 2022).

## 4 Related Work

The theoretical properties and advantages of transformers have been studied from many different perspectives (Jelassi et al., 2022; Zhang et al., 2022; Bai et al., 2023; Perez et al., 2021; Sanford et al., 2024). Most related to us are Yun et al. (2019); Edelman et al. (2022); Wei et al. (2022); Takakura and Suzuki (2023) in which transformers were studied from an approximation viewpoint. The work in Yun et al. (2019) proved that transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, while the network size suffers from the curse of dimensionality (the number of entries in the input sequence). Takakura and Suzuki (2023) studied the approximation and estimation ability of Transformers as seq-to-seq functions with infinite dimensional input, where anisotropic smoothness avoids the curse of dimensionality.

In the applications of Large Language Models (LLMs), empirical findings have demonstrated some correlation between the performance of transformers and the low-dimensional data structures (Razzhigaev et al., 2023; Min et al., 2023; Aghajanyan et al., 2020; Pandey, 2024). Razzhigaev et al. (2023) investigated the intrinsic dimension of embeddings in transformer architectures, and suggested an encoder and decoder embedding property. Most similar to our work is Sharma and Kaplan (2022) which demonstrates an empirical connection between neural scaling laws and the intrinsic dimension of data. While they briefly discuss predictions for LLMs, their theory works best for predicting student-teacher model setups and image classification tasks which seem to enjoy more regularity (and a faster rate of convergence) than language modeling. Despite these empirical findings, we are not aware of any rigorous theoretical justification connecting the scaling laws of transformers with the intrinsic dimension of data. Our paper complements this line of research with statistical estimation and mathematical approximation theories which well-predict the behavior observed in practice.

## 5 Conclusion

ConclusionThis paper establishes statistical and approximation theory results for transformers approximating Holder continuous functions on low-dimensional data manifolds. The resulting bound on the generalization error suffers from exponential dependence only in the intrinsic dimension \(d\). The constructed approximations of low-dimensional functions are shallow, requiring only \(O(d)\) layers independent of the desired accuracy. We demonstrate this theory is accurate in practice by predicting scaling laws in both model size and data size for LLMs trained on natural language datasets. We pay careful attention to the sensitivity of the estimated intrinsic data dimension, finding it is relatively stable with respect to several relevant hyperparameters.

Limitations and Broader ImpactOne important question unanswered by this work is how the intrinsic data dimension may affect the computational scaling exponent \(_{C}\). Future work may investigate this direction. Additionally, our empirical experiments make the simplifying assumption that the underlying target function possesses Lipschitz regularity (\(=1\)). Better estimates of the correct regularity would likely improve the accuracy of our predictions. More broadly, our work improves fundamental understanding of transformer-based LLMs and improves our ability to theoretically and safely predict future capabilities.

  &  \\  \(_{N}=0.076\) & \(_{D}=0.070\) & \(_{N}=0.34\) & \(_{D}=0.25\) \\ \(_{D}=0.095\) & \(_{N}=0.106\) & \(_{D}=0.28\) & \(_{N}=0.33\) \\ 

Table 1: ID-free estimation of scaling exponents for GPT-2 and Chincilla.