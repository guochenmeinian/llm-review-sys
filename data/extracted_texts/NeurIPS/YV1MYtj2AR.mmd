# MoVie: Visual Model-Based Policy Adaptation

for View Generalization

Sizhe Yang\({}^{12*}\), Yanjie Ze\({}^{13*}\), Huazhe Xu\({}^{415}\)

\({}^{1}\)Shanghai Qi Zhi Institute, \({}^{2}\)University of Electronic Science and Technology of China,

\({}^{3}\)Shanghai Jiao Tong University, \({}^{4}\)Tsinghua University, \({}^{5}\)Shanghai AI Lab

\({}^{*}\) Equal contribution

yangsizhe.github.io/MoVie

###### Abstract

Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of _view generalization_. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual **M**odel-based policies for **V**ew generalization (**MoVie**) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of **18** tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of **33\(\%\)**, **86\(\%\)**, and **152\(\%\)** respectively. The superior results highlight the immense potential of our approach for real-world robotics applications. Code and videos are available at yangsizhe.github.io/MoVie.

## 1 Introduction

Visual Reinforcement Learning (RL) has achieved great success in various applications such as video games , robotic manipulation , and robotic locomotion . However, one significant challenge for real-world deployment of visual RL agents remains: a policy trained with very limited views (commonly one single fixed view) might not generalize to unseen views. This challenge is especially pronounced in robotics, where a few fixed views may not adequately capture the variability of the environment. For instance, the RoboNet dataset  provides diverse views across a range of manipulation tasks, but training on such large-scale data only yields a moderate success rate (\(10\% 20\%\)) for unseen views .

Recent efforts have focused on improving the visual generalization of RL agents . However, these efforts have mainly concentrated on generalizing to different appearances and backgrounds. In contrast, view generalization presents a unique challenge as the deployment view is unknown and may move freely in the 3D space. This might weaken the weapons such as data augmentations  that are widely used in appearance generalization methods. Meanwhile, scaling up domain

Figure 1: **View generalization results across 3 domains. Our method MoVie largely improves the test-time view generalization ability, especially in robotic manipulation domains.**randomization [19; 21; 31] to all possible views is usually unrealistic because of the large cost and the offline nature of existing robot data. With these perspectives combined, it is difficult to apply those common approaches to address view generalization.

In this work, we commence by explicitly formulating the test-time view generalization problem into four challenging settings: _a)_**novel view**, where the camera changes to a fixed novel position and orientation; _b)_**moving view**, where the camera moves continuously around the scene, _c)_**shaking view**, where the camera experiences constant shaking, and _d)_**novel FOV**, where the field of view (FOV) of the camera is altered initially. These settings cover a wide spectrum of scenarios when visual RL agents are deployed to the real world. By introducing these formulations, we aim to advance research in addressing view generalization challenges and facilitate the utilization of robot datasets  and deployment on physical robotic platforms.

To address the view generalization problem, we argue that _adaptation_ to novel views during test time is crucial, rather than aiming for view-invariant policies. We propose MoVie, a simple yet effective method for adapting visual **M**odel-based policies to generalize to unseen **V**iews. MoVie leverages collected transitions from interactions and incorporates spatial transformer networks (STN ) in shallow layers, using the learning objective of the dynamics model (DM). Notably, MoVie requires no modifications during training and is compatible with various visual model-based RL algorithms. It only necessitates small-scale interactions for adaptation to the deployment view.

We perform extensive experiments on 7 robotic manipulation tasks (Advit hand  and xArm ) and 11 locomotion tasks (DMControl suite )), across the proposed 4 view generalization settings, totaling \(\)\(\)\(\) configurations. MoVie improves the view generalization ability substantially, compared to strong baselines including the inverse dynamics model (IDM ) and the dynamics model (DM). Remarkably, MoVie attains a relative improvement of \(86\%\) in xArm and \(152\%\) in Adroit, underscoring the potential of our method in robotics. **We are committed to releasing our code and testing platforms.** To conclude, our contributions are three-fold:

* We formulate the problem of view generalization in visual reinforcement learning with a wide range of tasks and settings that mimic real-world scenarios.
* We propose a simple model-based policy adaptation method for view generalization (**MoVie**), which incorporates STN into the shallow layers of the visual representation with a self-supervised dynamics prediction objective.
* We successfully showcase the effectiveness of our method through extensive experiments. The results serve as a testament to its capability and underscore its potential for practical deployment in robotic systems, particularly with complex camera views.

## 2 Related Work

**Visual generalization in reinforcement learning.** Agents trained by reinforcement learning (RL) from visual observations are prone to overfitting the training scenes, making it hard to generalize to

Figure 2: **Overview of MoVie**. During training (left), the agent is trained with the latent dynamics loss. At test time (right), we freeze the dynamics model and modify the encoder as a spatial adaptive encoder to adapt the agents to novel views.

unseen environments with appearance differences. A large corpus of recent works has focused on addressing this issue [7; 14; 15; 19; 21; 33; 35; 36; 9]. Notably, SODA  provides a visual generalization benchmark to better evaluate the generalizability of policies, while they only consider appearance changes of agents and backgrounds. Distracting control suite  adds both appearance changes and camera view changes into DMControl , where the task diversity is limited.

**View generalization in robotics.** The field of robot learning has long grappled with the challenge of training models on limited views and achieving generalization to unseen views. Previous studies, such as RoboNet , have collected extensive video data encompassing various manipulation tasks. However, even with pre-training on such large-scale datasets, success rates on unseen views have only reached approximately \(10\% 20\%\). In recent efforts to tackle this challenge, researchers have primarily focused on third-person imitation learning [23; 24; 25] and view-invariant visual representations [32; 37; 16; 2; 4], but these approaches are constrained by the number of available camera views. In contrast, our work addresses a more demanding scenario where agents trained on a single fixed view are expected to generalize to diverse unseen views and dynamic camera settings.

**Test-time training.** There is a line of works that train neural networks at test-time with self-supervised learning in computer vision [28; 5; 29], robotics , and visual RL . Specifically, PAD is the closest to our work , which adds an inverse dynamics model (IDM) objective into model-free policies for both training time and test time and gains better appearance generalization. In contrast, we differ in a lot of aspects: _(i)_ we focus on visual model-based policies, _(ii)_ we require no modification in training time, and _(iii)_ our method is designed for view generalization specifically.

## 3 Preliminaries

**Formulation.** We model the problem as a Partially Observable Markov Decision Process (POMDP) \(=,,,,\), where \(\) are observations, \(\) are actions, \(:\) is a transition function (called dynamics as well), \(r\) are rewards, and \([0,1)\) is a discount factor. During training time, the agent's goal is to learn a policy \(\) that maximizes discounted cumulative rewards on \(\), _i.e._, \(_{}[_{t=0}^{}^{t}r_{t}]\). During test time, the reward signal from the environment is not accessible to agents and only observations are available, which are possible to experience subtle changes such as appearance changes and camera view changes.

**Model-based reinforcement learning. TD-MPC** is a model-based RL algorithm that combines model predictive control and temporal difference learning. TD-MPC learns a visual representation \(=h()\) that maps the high-dimensional observation \(\) into a latent state \(\) and a latent dynamics model \(d:\) that predicts the future latent state \(^{}=d(,)\) based on the current latent state \(\) and the action \(\). **MoDem** accelerates TD-MPC with efficient utilization of expert demonstrations \(=\{D_{1},D_{2},,D_{N}\}\) to solve challenging tasks such as dexterous manipulation . In this work, we select TD-MPC and MoDem as the backbone algorithm to train model-based agents, while our algorithm could be easily extended to most model-based RL algorithms such as Dreamer .

## 4 Method

We propose a simple yet effective method, visual **M**odel-based policy adaptation for **V**ew generalization (**MoVie**), which can accommodate visual RL agents to novel camera views at test time.

**Learning objective for the test time.** Given a tuple \((_{t},_{t},_{t+1})\), the original latent state dynamics prediction objective can be written as

\[_{}=\|d(h(_{t}),_{t})-h(_{t+1})\|_{2},\] (1)

where \(h\) is an image encoder that projects a high-dimensional observation from space \(\) into a latent space \(\) and \(d\) is a latent dynamics model \(d:\).

In test time, the observations under unseen views lie in a different space \(^{}\), so that their corresponding latent space also changes to \(^{}\). However, the projection \(h\) learned in training time can only map \(\) while the policy \(\) only learns the mapping \(\), thus making the policy hard to generalize to the correct mapping function \(^{}\). Our proposal is thus to adapt the projection \(h\) from a mapping function \(h:\) to a more useful mapping function \(h^{}:^{}\) so that the policy would execute the correct mapping \(\) without training. A vivid illustration is provided in Figure 3.

We freeze the latent dynamics model \(d\), denoted as \(d^{}\), so that the latent dynamics model is not a training target but a supervision. We also insert STN blocks  into the shallow layers of \(h\) to better adapt the projection \(h\), so that we write \(h\) as \(h^{}\) (SAE denotes spatial adaptive encoder). Though the objective is still the latent state dynamics prediction loss, the supervision here is **superficially identical but fundamentally different** from training time. The formal objective is written as

\[_{}=\|d^{}(h^{}(),)- h^{}(})\|_{2}.\] (2)

**Spatial adaptive encoder.** We now describe more details about our modified encoder architecture during test time, referred to as spatial adaptive encoder (SAE). To keep our method simple and fast to adapt, we only insert two different STNs into the original encoder, as shown in Figure 4. We observe in our experiments that transforming the low-level features (_i.e._, RGB features and shallow layer features) is most critical for adaptation, while the benefit of adding more STNs is limited (see Table 6). An STN block consists of two parts: _(i)_ a localisation net that predicts an affine transformation with \(6\) parameters and _(ii)_ a grid sampler that generates an affined grid and samples features from the original feature map. The point-wise affine transformation is written as

\[(x^{s}\\ y^{s})=_{}(G)=_{} (x^{t}\\ y^{t}\\ 1)=[_{11}&_{12}&_{13}\\ _{21}&_{22}&_{23}](x^{t} \\ y^{t}\\ 1)\] (3)

where \(G\) is the sampling grid, \((x^{t}_{i},y^{t}_{i})\) are the target coordinates of the regular grid in the output feature map, \((x^{s}_{i},y^{s}_{i})\) are the source coordinates in the input feature map that define the sample points, and \(_{}\) is the affine transformation matrix.

**Training strategy for SAE.** We use a learning rate \(1 10^{-5}\) for STN layers and \(1 10^{-7}\) for the encoder. We utilize a replay buffer with size \(256\) to store history observations and update \(32\) times for each time step to heavily utilize the online data. Implementation details remain in Appendix A.

Figure 4: **The architecture of our spatial adaptive encoder (SAE).** We incorporate STNs before the first two convolutional layers of the encoder for better adaptation of the representation \(h\).

Figure 3: **An illustration of the reason why MoVie is effective.** We treat the frozen dynamics model as the source of supervision to adapt the latent space of \(h^{}\) to that of the training views.

## 5 Experiments

In this section, we investigate how well an agent trained on a single fixed view generalizes to unseen views during test time. During the evaluation, agents have no access to reward signals, presenting a significant challenge for agents to self-supervise using online data.

### Experiment Setup

**Formulation of camera view variations**: _a)_ **novel view**, where we maintain a fixed camera target while adjusting the camera position in both horizontal and vertical directions by a certain margin, _b)_ **moving view**, where we establish a predefined trajectory encompassing the scene and the camera follows this trajectory, moving back and forth for each time step while focusing on the center of the scene, _c)_ **shaking view**, where we add Gaussian noise onto the original camera position at each time step, and _d)_ **novel FOV**, where the FOV of the camera is altered once, different from the training phase. A visualization of four settings is provided in Figure 6 and videos are available in yangsizhe.github.io/MoVie for a better understanding of our configurations. Details remain in Appendix B.

**Tasks.** Our test platform consists of **18** tasks from **3** domains: 11 tasks from DMControl , 3 tasks from Adroit , and 4 tasks from xArm . A visualization of the three domains is in Figure 5. Although the primary motivation for this study is for addressing view generalization in real-world robot learning, which has not yet been conducted, we contend that the extensive range of tasks tackled in our research effectively illustrates the potential of MoVie for real-world application. We run 3 seeds per experiment with seed numbers \(0,1,2\) and run 20 episodes per seed. During these 20 episodes, the models could store the history transitions. We report cumulative rewards for DMControl tasks and success rates for robotic manipulation tasks, averaging over episodes.

**Baselines.** We first train visual model-based policies with TD-MPC  on DMControl and xArm environments and MoDem  on Adroit environments under the default configurations and then test these policies in the view generalization setting. Since MoVie consists of two components mainly (_i.e._, DM and STN), we build two test-time adaptation algorithms by replacing each module: _a)_ **DM**, which removes STN from MoVie and _b)_ **IDM+STN**, which replaces DM in MoVie with IDM . IDM+STN is very close to PAD , and we add STN for fair comparison. We keep all training settings the same.

Figure 5: **Visualization of three task domains**, including DMControl , xArm , Adroit .

Figure 6: **Visualization of the training view and four view generalization settings.** Trajectory visualization for all tasks is available on our website yangsizhe.github.io/MoVie.

[MISSING_PAGE_EMPTY:6]

disparity to two factors: _a)_ the inherent complexity of robotic manipulation tasks and _b)_ the pressing need for effective view generalization in this domain.

**Challenges in handling shaking view.** Despite improvements of MoVie in various settings, we have identified a relative weakness in addressing the _shaking view_ scenario. For instance, in xArm tasks, the success rate of MoVie is only \(45\%\), which is close to the \(42\%\) success rate of TD-MPC without adaptation. Other baselines such as DM and IDM+STN also experience performance drops. We acknowledge the inherent difficulty of the shaking view scenario, while it is worth noting that in real-world robotic manipulation applications, cameras often exhibit smoother movements or are positioned in fixed views, partially mitigating the impact of shaking view.

   Shaking view & TD-MPC & DM & IDM+STN & MoVie \\  Cheetah, run & \(381.04 39.31\) & \(317.66 9.57\) & \(212.31 19.74\) & \(\) \\ Walker, walk & \(662.13 57.71\) & \(627.77 21.47\) & \(340.48 36.92\) & \(\) \\ Walker, stand & \(\) & \(604.55 16.20\) & \(471.30 69.19\) & \(687.96 9.11\) \\ Walker, run & \(251.58 13.69\) & \(186.83 3.75\) & \(128.31 9.19\) & \(\) \\ Cup, catch & \(752.86 41.81\) & \(835.16 32.91\) & \(648.75 12.77\) & \(\) \\ Finger, spin & \(89.55 47.72\) & \(88.56 5.86\) & \(145.68 29.33\) & \(\) \\ Finger, turn-easy & \(\) & \(449.10 20.48\) & \(656.23 50.35\) & \(694.20 91.02\) \\ Finger, turn-hard & \(411.41 40.28\) & \(383.96 32.94\) & \(132.40 34.25\) & \(\) \\ Pendulum, swingup & \(23.73 9.24\) & \(\) & \(27.30 2.38\) & \(48.23 30.25\) \\ Reacher, easy & \(529.58 65.05\) & \(225.08 31.02\) & \(299.11 59.00\) & \(\) \\ Reacher, hard & \(205.53 25.16\) & \(54.80 32.45\) & \(144.11 5.60\) & \(\) \\ D\(\) & \(441.79\) & \(348.26\) & \(291.45\) & \(\) \\  xArm, reach & \(76 2\) & \(76 7\) & \(83 2\) & \(\) \\ xArm, push & \(\) & \(55 10\) & \(61 5\) & \(57 13\) \\ xArm, peg in box & \(3 2\) & \(\) & \(3 2\) & \(5 5\) \\ xArm, hammer & \(25 5\) & \(28 7\) & \(28 12\) & \(\) \\ xArm & \(42\) & \(44\) & \(44\) & \(\) \\  Adroit, door & \(1 2\) & \(10 0\) & \(5 5\) & \(\) \\ Adroit, hammer & \(58 12\) & \(65 18\) & \(55 20\) & \(\) \\ Adroit, pen & \(30 5\) & \(30 8\) & \(18 5\) & \(\) \\ Adroit & \(30\) & \(35\) & \(26\) & \(\) \\   

Table 4: **Results in shaking view.** The best method on each task is in **bold**.

   Moving view & TD-MPC & DM & IDM+STN & MoVie \\  Cheetah, run & \(235.37 63.39\) & \(344.70 7.54\) & \(199.48 25.78\) & \(\) \\ Walker, walk & \(632.77 15.62\) & \(707.60 26.72\) & \(373.44 30.85\) & \(\) \\ Walker, stand & \(\) & \(706.05 5.75\) & \(576.87 44.69\) & \(712.48 11.67\) \\ Walker, run & \(\) & \(250.84 7.02\) & \(190.02 2.61\) & \(281.43 33.34\) \\ Cup, catch & \(887.20 3.67\) & \(910.51 4.12\) & \(915.16 7.31\) & \(\) \\ Finger, spin & \(636.91 3.91\) & \(697.11 6.67\) & \(532.90 3.25\) & \(\) \\ Finger, turn-easy & \(715.45 19.23\) & \(728.93 180.26\) & \(683.85 112.64\) & \(\) \\ Finger, turn-hard & \(\) & \(454.58 88.50\) & \(559.20 121.35\) & \(\) \\ Pendulum, swingup & \(26.23 1.85\) & \(83.88 10.55\) & \(47.20 4.39\) & \(236.81 50.23\) \\ Reacher, easy & \(\) & \(981.78 6.06\) & \(695.03 326.91\) & \(982.58 13.39\) \\ Reacher, hard & \(854.76 31.54\) & \(864.55 56.37\) & \(752.55 83.55\) & \(\) \\ D\(\) & \(605.98\) & \(611.87\) & \(502.34\) & \(\) \\  xArm, reach & \(15 5\) & \(71 2\) & \(21 2\) & \(\) \\ xArm, push & \(58 5\) & \(52 10\) & \(55 5\) & \(\) \\ xArm, peg in box & \(0 0\) & \(0 0\) & \(1 2\) & \(\) \\ xArm, hammer & \(8 7\) & \(0 0\) & \(\) & \(8 7\) \\ xArm & \(20\) & \(31\) & \(24\) & \(\) \\  Adroit, door & \(0 0\) & \(10 5\) & \(0 0\) & \(\) \\ Adroit, hammer & \(25 8\) & \(31 12\) & \(28 02\) & \(\) \\ Adroit, pen & \(20 0\) & \(20 10\) & \(18 14\) & \(\) \\ Adroit

**Effective adaptation in novel view, moving view, and novel FOV scenarios.** In addition to the shaking view setting, MoVie consistently outperforms TD-MPC without adaptation by \(2 4\) in robotic manipulation tasks across the other three settings. It is worth noting that these three settings are among the most common scenarios encountered in real-world applications.

**Real-world implications.** Our findings have important implications for real-world deployment of robots. Previous methods, relying on domain randomization and extensive data augmentation during training, often hinder the learning process. Our proposed method enables direct deployment of offline or simulation-trained agents, improving success rates with minimal interactions.

### Ablations

To validate the rationale behind several choices of MoVie and our test platform, we perform a comprehensive set of ablation experiments.

**Integration of STN with low-level features.** To enhance view generalization, we incorporate two STN blocks , following the image observation and the initial convolutional layer of the feature encoder. This integration is intended to align the low-level features with the training view, thereby preserving the similarity of deep semantic features to the training view. As shown in Table 6, by progressively adding more layers to the feature encoder, we observe that deeper layers do not provide significant additional benefits, supporting our intuition for view generalization.

**Different novel views.** We classify novel views into three levels of difficulty, with our main experiments employing the _medium_ difficulty level by default. Table 7 presents additional results for

   Cheetah-run & 0 & 1 & 2 (MoVie) & 3 & 4 \\  Novel View & \(254.42_{ 13.89}\) & \(259.64_{ 32.01}\) & \(}\) & \(309.25_{ 33.19}\) & \(327.39_{ 10.44}\) \\ Moving View & \(344.70_{ 7.54}\) & \(360.29_{ 13.64}\) & \(}\) & \(361.28_{ 6.78}\) & \(357.29_{ 31.24}\) \\ Shaking View & \(317.66_{ 9.57}\) & \(491.96_{ 20.07}\) & \(}\) & \(454.82_{ 19.57}\) & \(472.57_{ 8.49}\) \\ Novel FOV & \(379.01_{ 10.90}\) & \(512.69_{ 17.67}\) & \(}\) & \(505.65_{ 12.02}\) & \(492.79_{ 21.66}\) \\   

Table 6: **Ablation on applying different numbers of STNs from shallow to deep.** The best method on each setting is in **bold**.

   Novel FOV & TD-MPC & DM & IDM+STN & MoVie \\  Cheetah, run & \(128.55_{ 6.57}\) & \(379.01_{ 10.90}\) & \(299.02_{ 88.47}\) & \(}\) \\ Walker, walk & \(239.54_{ 129.47}\) & \(882.21_{ 12.75}\) & \(579.58_{ 47.87}\) & \(}\) \\ Walker, stand & \(679.70_{ 21.21}\) & \(732.07_{ 14.77}\) & \(678.46_{ 55.32}\) & \(}\) \\ Walker, run & \(184.28_{ 1.39}\) & \(337.28_{ 11.40}\) & \(295.99_{ 10.00}\) & \(}\) \\ Cup, catch & \(940.20_{ 28.21}\) & \(912.70_{ 34.54}\) & \(964.83_{ 10.45}\) & \(}\) \\ Finger, spin & \(675.81_{ 4.99}\) & \(886.63_{ 4.54}\) & \(414.83_{ 79.20}\) & \(}\) \\ Finger, turn-easy & \(792.26_{ 154.24}\) & \(690.61_{ 140.88}\) & \(805.63_{ 54.95}\) & \(}\) \\ Finger, turn-hard & \(591.43_{ 109.84}\) & \(393.18_{ 118.38}\) & \(449.88_{ 121.99}\) & \(}\) \\ Pendulum, swingup & \(126.90_{ 75.86}\) & \(184.33_{ 83.99}\) & \(253.36_{ 75.26}\) & \(}\) \\ Reacher, easy & \(929.40_{ 12.35}\) & \(931.56_{ 12.69}\) & \(937.20_{ 55.97}\) & \(}\) \\ Reacher, hard & \(424.11_{ 83.74}\) & \(421.56_{ 94.93}\) & \(287.96_{ 6.33}\) & \(}\) \\ DMControl & \(527.47\) & \(613.74\) & \(542.43\) & \(_{ 69\%}\) \\  xArm, reach & \(53_{ 12}\) & \(98_{ 2}\) & \(8_{ 5}\) & \(}\) \\ xArm, push & \(60_{ 13}\) & \(71_{ 5}\) & \(60_{ 5}\) & \(}\) \\ xArm, peg in box & \(20_{ 5}\) & \(13_{ 10}\) & \(6_{ 11}\) & \(}\) \\ xArm, hammer & \(3_{ 2}\) & \(6_{ 5}\) & \(8_{ 2}\) & \(}\) \\ xArm & \(34\) & \(47\) & \(40\) & \(}\) \\  Adroit, door & \(1_{ 2}\) & \(58_{ 10}\) & \(3_{ 5}\) & \(}\) \\ Adroit, hammer & \(66_{ 7}\) & \(75_{ 18}\) & \(76_{ 10}\) & \(}\) \\ Adroit, pen & \(26_{ 10}\) & \(20_{ 8}\) & \(15_{ 5}\) & \(}\) \\ Adroit & \(31\) & \(51\) & \(31\) & \(}\) \\   

Table 5: **Novel FOV.** The best method on each task is in **bold**.

the _easy_ and _hard_ difficulty levels. As the difficulty level increases, we observe a consistent decrease in the performance of all the methods.

**The efficacy of STN in conjunction with IDM.** Curious readers might be concerned about our direct utilization of IDM+STN in the main experiments, suggesting that STN could potentially be detrimental to IDM. However, Table 8 shows that STN not only benefits our method but also improves the performance of IDM, thereby demonstrating effectiveness of SAE for adaptation of the representation and validating our baseline selection.

**Finetune or freeze the DM.** In our approach, we employ the frozen DM as a form of supervision to guide the adaptation process of the encoder. However, it remains unverified for the readers whether end-to-end finetuning of both the DM and the encoder would yield similar benefits. The results presented in Table 9 demonstrate that simplistic end-to-end finetuning does not outperform MoVie, thereby reinforcing the positive results obtained by MoVie.

   Cheetah-run & TD-MPC & DM & IDM+STN & MoVie \\  Easy & \(265.90 7.94\) & \(441.61 14.00\) & \(349.86 55.82\) & \(\) \\ Medium & \(90.13 20.38\) & \(254.43 13.89\) & \(127.76 16.16\) & \(\) \\ Hard & \(48.64 13.20\) & \(112.39 8.36\) & \(59.64 5.35\) & \(\) \\   

Table 7: **Ablation on different novel views.** The best method on each setting is in **bold**.

   Task & Setting & IDM & IDM+STN \\   & Novel view & \(40 5\) & \(\) \\  & Moving view & \(46 2\) & \(\) \\  & Shaking view & \(60 10\) & \(\) \\  & Novel FOV & \(58 2\) & \(\) \\  & All settings & \(51\) & \(\) \\   & Novel view & \(6 5\) & \(\) \\  & Moving view & \(5 0\) & \(\) \\  & Shaking view & \(\) & \(28 12\) \\  & Novel FOV & \(6 7\) & \(\) \\  & All settings & \(13\) & \(\) \\   & Novel view & \(867.87 3.14\) & \(\) \\  & Moving view & \(912.85 10.22\) & \(\) \\  & Shaking view & \(435.40 87.32\) & \(\) \\  & Novel FOV & \(815.27 34.61\) & \(\) \\  & All settings & \(757.84\) & \(\) \\   & Novel view & \(\) & \(141.03 37.52\) \\  & Moving view & \(332.90 0.28\) & \(\) \\  & Shaking view & \(106.72 7.11\) & \(\) \\  & Novel FOV & \(311.66 8.55\) & \(\) \\  & All settings & \(232.13\) & \(\) \\   & Novel view & \(111.73 18.98\) & \(\) \\  & Moving view & \(\) & \(199.48 25.78\) \\   & Shaking view & \(210.06 27.99\) & \(\) \\   & Novel FOV & \(262.76 62.17\) & \(\) \\   & All settings & \(197.43\) & \(\) \\   

Table 8: **Ablation on IDM with and without STN.** The best method is in **bold**.

## 6 Conclusion

In this study, we present MoVie, a method for adapting visual model-based policies to achieve view generalization. MoVie mainly finetunes a spatial adaptive image encoder using the objective of the latent state dynamics model during test time. Notably, we maintain the dynamics model in a frozen state, allowing it to function as a form of self-supervision. Furthermore, we categorize the view generalization problem into four distinct settings: novel view, moving view, shaking view, and novel FOV. Through a systematic evaluation of MoVie on 18 tasks across these four settings, totaling 64 different configurations, we demonstrate its general and remarkable effectiveness.

One limitation of our work is the lack of real robot experiments, while our focus is on addressing view generalization in robot datasets and deploying visual reinforcement learning agents in real-world scenarios. In our future work, we would evaluate MoVie on real-world robotic tasks.