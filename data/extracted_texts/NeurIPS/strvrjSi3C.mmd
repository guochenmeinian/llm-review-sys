# Riemannian SAM: Sharpness-Aware Minimization on Riemannian Manifolds

Jihun Yun

KAIST

arcprime@kaist.ac.kr

&Eunho Yang

KAIST, AITRICS

eunhoy@kaist.ac.kr

###### Abstract

Contemporary advances in the field of deep learning have embarked upon an exploration of the underlying geometric properties of data, thus encouraging the investigation of techniques that consider general manifolds, for example, hyperbolic or orthogonal neural networks. However, the optimization algorithms for training such geometric deep models still remain highly under-explored. In this paper, we introduce Riemannian SAM by generalizing conventional Euclidean SAM to Riemannian manifolds. We successfully formulate the sharpness-aware minimization on Riemannian manifolds, leading to one of a novel instantiation, Lorentz SAM. In addition, SAM variants proposed in previous studies such as Fisher SAM can be derived as special examples under our Riemannian SAM framework. We provide the convergence analysis of Riemannian SAM under a less aggressively decaying ascent learning rate than Euclidean SAM. Our analysis serves as a theoretically sound contribution encompassing a diverse range of manifolds, also providing the guarantees for SAM variants such as Fisher SAM, whose convergence analyses are absent. Lastly, we illustrate the superiority of Riemannian SAM in terms of generalization over previous Riemannian optimization algorithms through experiments on knowledge graph completion and machine translation tasks.

## 1 Introduction

Deep learning incorporating the underlying geometry of data, referred to as geometric deep learning (GDL), has emerged as a significant research area in recent years due to its remarkable capability to effectively capture intrinsic structural properties. As a significant direction in this line, hyperbolic representation learning has been shown to offer several advantages over conventional Euclidean geometry. For example, the hyperbolic space allows for more efficient representations of high-dimensional data by offering a more flexible and natural way to model hierarchical structures, which are commonly encountered in network embeddings [1; 2; 3], computer visions [4; 5], and natural language processing [6; 7]. Adding to the fascinating array of approaches in geometric deep learning, the orthogonal neural networks enforcing the orthogonality in model parameters emerge as a promising research area. In another dimension, the orthogonal neural networks that constrain the model parameter to satisfy the orthogonality (known as Stiefel manifold) are proposed [8; 9; 10] under the motivation that they prevent the vanishing/exploding gradient problem and theoretically enhance the model generalization . Furthermore, more generally, the Riemannian extension of deep learning technique on Euclidean space continues to be proposed in many fields including Riemannian normalizing flows [12; 13], Riemannian diffusion models , Poincare ResNet , hyperbolic deep reinforcement learning .

Along with the attempts to learn non-Euclidean representations in deep learning, Riemannian optimization has also been greatly studied to train non-Euclidean deep models. As a pioneering example, Riemannian (stochastic) gradient descent (R(S)GD)  is an extension of (stochastic) gradient descent to Riemannian manifolds, which updates the gradient computed on (a random subset of) thetraining data at each iteration and then projects the gradient onto the tangent space of the manifold before taking a descent step. Starting with Riemannian gradient descent, several popular optimization algorithms in Euclidean space, such as conjugate gradient and trust region, have been generalized to Riemannian manifolds [18; 19; 20; 21; 22; 23; 24]. In addition to these previous works, there have been many studies to incorporate the momentum and variance reduction technique into the Riemannian manifolds. In this line of work, many optimization algorithms are proposed including the stochastic variance reduction scheme (R-SVRG) , stochastic recursive momentum (R-SRG) , and stochastic path-integrated differential estimator (R-SPIDER) , extended from their Euclidean counterparts. Going beyond the first-order algorithms, Riemannian (quasi)-Newton methods [28; 29] are a family of second-order methods using a (quasi)-Newton approach on Riemannian manifolds. In the context of deep learning, Riemannian extensions of adaptive gradient methods (ex. AdaGrad/Adam/AMSGrad) are proposed [30; 31; 32], which combines the benefits of adaptive learning rate methods with the efficiency of Riemannian optimization techniques.

However, the exploration of optimization algorithms for training deep learning models on non-Euclidean geometry has been considerably limited, highlighting the need to generalize successful optimizers in Euclidean space to Riemannian manifolds. Under this motivation, we introduce a new class of optimization schemes on Riemannian manifolds. Toward this, as our motivating optimization algorithm, we consider the sharpness-aware minimization (SAM)  in Euclidean space, which efficiently improves model generalization by considering the underlying geometry of loss landscapes. With the great success of SAM, several SAM variants including adaptive SAM , Fisher SAM , Efficient SAM , and GSAM , are proposed in recent years. In this paper, we propose Riemannian SAM, a sharpness-aware minimization on Riemannian manifolds that can be applied to various manifolds. Our Riemannian SAM considers the sharpness of the loss defined on manifolds, thereby effectively improving model generalization. We believe that our framework could further bring out the potential of the Riemannian deep models and enable more accurate evaluation.

Our contributions are summarized as follows:

* We introduce Riemannian SAM, a sharpness-aware minimization scheme on Riemannian manifolds. Under our framework, we present a novel instance, Lorentz SAM, mainly used in our empirical evaluations. Furthermore, one of the SAM variants, Fisher SAM, which considers the underlying distribution space of neural networks can be derived as a special example under our Riemannian SAM framework.
* We provide the convergence analysis of Riemannian SAM. Our convergence analysis achieves the first-order optimal rate of SGD and we highlight the challenges in our analysis. We allow for a less aggressively decaying ascent learning rate than the condition in the convergence of Euclidean SAM. Also, we provide the convergence guarantee for the SAM variants such as Fisher SAM whose convergence proofs are absent.
* We validate the Riemannian SAM on knowledge graph completion and machine translation tasks for hyperbolic neural networks. The state-of-the-art hyperbolic architecture equipped with our Riemannian SAM improves the performance of the baselines trained with Riemannian Adam, which is a conventional optimizer in Riemannian deep learning.

## 2 Preliminaries

Before introducing the sharpness-aware minimization on Riemannian manifolds, we organize the necessary concepts and notations for Riemannian geometry and Riemannian optimization.

### Riemannian Geometry for Optimization

We refer to the definitions in literature [38; 39; 40] where one can find more details.

**Definition 1** (Riemannian manifold).: _For each \(w\), let \(_{w}\) denote the **tangent space** at \(w\). An **inner product** on tangent space \(_{w}\) is a bilinear, symmetric, positive definite function \(g_{w}(,),_{w}:_{w} _{w}\). If a metric \(,_{w}\) smoothly varies with \(w\), we call \(,_{w}\)**a Riemannian metric**. An induced norm on \(_{w}\) is \(\|\|_{w}}\). A **Riemannian manifold** is a pair \((,g)\) of the manifold \(\) and the associated Riemannian metric tensor \(g\)._

**Definition 2** (Geodesic).: _A **geodesic** is a curve \(():\) that locally minimizes the distance between two points on a manifold with constant speed, which is the generalization of a straight line in Euclidean space._

**Definition 3** (Exponential maps/Retraction).: _An **exponential map**\(_{w}:_{w}\) maps a tangent vector \(_{w}_{w}\) onto \(\) along a geodesic curve such that \((0)=w\) and \((1)=z\) with \((0)=_{w}\). Specifically, \((t)_{w}(_{w}t)\) represents a geodesic. A **retraction**\(_{w}()\) is a (computationally efficient) generalization of an exponential map satisfying the following properties:_

* \(_{w}(0)=w\) _and_ \(DR_{w}(0)=_{_{w}}\) _where_ \(DR_{w}\) _represents the derivatives of_ \(_{w}\) _and_ \(_{_{w}}\) _denotes an identity map on_ \(_{w}\)_._

**Definition 4** (Parallel translation/Vector transport).: _A **parallel translation**\(P_{z}^{w}():_{z}_{w}\) transport a tangent vector in \(_{z}\) to \(_{w}\) in parallel while preserving norm and direction (i.e., along a geodesic). A **vector transport**\(()_{z}^{w}():_{z}_{w} \)**with respect to retraction map**\(\) maps a vector \(_{z}_{z}\) to \(_{w}_{w}\) along a retraction curve \((t)=_{w}(_{w}t)\) for some \(_{w}_{w}\), which is computationally efficient approximation of a parallel translation. In this work, we only consider **isometric** vector transport, i.e., \(_{z}^{w}_{z},_{z}^{w}_{z}_{w}= _{w},_{w}_{w}\) for all \(_{w},_{w}_{w}\)._

We introduce important examples of Riemannian manifolds in deep learning. The initial two instances represent dominant manifolds within the realm of hyperbolic deep learning. Hyperbolic space is a natural geometry for capturing underlying tree-like, graph-shaped, or hierarchical structures, which are properties existing in many real datasets. Owing to this characteristic, there have been many approaches to hyperbolic deep learning encompassing network embeddings , computer vision , and natural language processing . For manifolds in hyperbolic space, we mainly follow the definitions in .

**Poincare Ball.** The Poincare ball \(^{n}=(^{n},g_{p})\) is a Riemannian manifold with

\[^{n}=\{x^{n}:\|x\|_{2}<1\}, g_{p}(x)=^{2}}^{2}g_{e}(x).\]

where \(g_{e}\) represents an Euclidean metric tensor. The associated distance on Poincare ball is given by

\[d_{p}(x,y)=1+2^{2}}{(1-\|x\|_{2}^{2})( 1-\|y\|_{2}^{2})}.\]

We remark on some properties of the Poincare ball. The Poincare ball is a conformal model, meaning that angles between curves are preserved under conformal transformations. This property enables the Poincare ball to accurately represent the local geometry of complex spaces. Additionally, the Poincare ball offers an intuitive visualization of hyperbolic spaces, particularly in two or three dimensions. Despite these advantageous properties, the Poincare ball also presents challenges in computing mathematical concepts such as geodesics and distances.

**Lorentz Model.** The Lorentz model \(^{n}=(^{n},g_{})\) is a semi-Riemannian manifold, but it is still possible to employ Riemannian optimization. The Lorentz model consist of

\[^{n}=\{x^{n+1}: x,x_{}=-1,x_{0 }>0\}, g_{}(x)=-1&&0&0\\ 0&1&&0\\ &&&\\ 0&0&&1.\] (1)

where \( x,y_{}=-x_{0}y_{0}+_{j=1}^{n}x_{j}y_{j}\) is known as the _Lorentzian scalar product_. The associated distance function is given by

\[d_{}(x,y)=- x,y_{}\]

Note that an \(n\)-dimensional Lorentz model requires one more redundant dimension in Euclidean space. Importantly, Poincare ball and the Lorentz model are equivalent under the diffeomorphism \(:^{n}^{n}\) (with the corresponding inverse mapping \(^{-1}:^{n}^{n}\)) defined as

\[(x_{0},x_{1},,x_{n}) =,x_{2},,x_{n})}{p_{0}+1},\] (2) \[^{-1}(x_{1},x_{2},,x_{n}) =^{2}++x_{n}^{2},2x_{1},,2x_{n})}{1-(x _{1}^{2}++x_{n}^{2})}.\] (3)We also remark on some characteristics of the Lorentz model. The main advantage is that it allows for more stable Riemannian optimization under relatively simple formulas for mathematical quantities, such as geodesics and distances. However, the visualization is difficult due to less intuitive projection onto a lower-dimensional space. As noted in each manifold, both two manifolds on hyperbolic space are equivalent, but they have different purposes. For this reason, some studies  use Lorentz manifolds for their model design and training, then visualize the results on Poincare ball using the diffeomorphism \(\) in (2) and (3).

**Stiefel manifold.** The orthogonality, i.e., \(W^{}W=I\) for model parameter \(W\), plays a role to circumvent the vanishing/exploding gradient problem [8; 10; 43] and delivers theoretically enhanced generalization error bounds . The Stiefel manifold \((n,p)=(^{n},g_{W})\) for \(n p\) is prevalent for orthogonal neural networks, which is also a Riemannian manifold defined by

\[^{n}=\{X^{n p}:X^{}X=I\}, g_{W}(Z _{1},Z_{2})=(Z_{1}^{}Z_{2}).\]

for tangent vectors \(Z_{1},Z_{2}_{W}(n,p)\). The tangent space of \((n,p)\) at \(W\) is defined by \(_{W}(n,p)=\{Z:Z^{}W+W^{}Z=0\}\).

### Riemannian Optimization

We are interested in the following optimization problem over the Riemannian manifold \((,g)\)

\[_{w}(w).\]

where \(:\) is a smooth function defined on manifold \(\). Following the work , Riemannian stochastic gradient descent (RSGD) updates the model parameter \(w\) as

\[w_{t+1}=_{w_{t}}-_{t}(w_{t}).\] (4)

where \((w_{t})_{w_{t}}\) is a Riemannian gradient at \(w_{t}\) and \(_{t}\) is the learning rate. In some practical cases, the exponential map is computationally inefficient. Hence, it may be replaced with (more computationally efficient) suitable retraction map \(_{w_{t}}()\), yielding the update rule \(w_{t+1}=_{w_{t}}-_{t}(w_{t}) \). Generally, the Riemannian gradient \((w_{t})\) in (4) is computed with the Riemannian metric tensor \(g\) as

\[(w_{t})=g^{-1}(w_{t})(w_{t}).\] (5)

where \((w_{t})\) denotes the Euclidean gradient. This is also known as natural gradient descent . The quantity on the right-hand side in (5) may not be on \(_{w_{t}}\). In this case, we should project the gradient onto the tangent space since the exponential map is not defined.

## 3 Sharpness-Aware Minimization on Riemannian Manifolds

In empirical risk minimization (ERM) including deep learning tasks, we generally minimize the finite-sum objective (or equivalently expected objective) for training dataset \(=\{(x_{i},y_{i})\}_{i=1}^{n}\) as

\[_{w}(w)_{i=1}^{n} (w;x_{i}).\] (6)

where the smooth loss function \(():\) is defined on a Riemannian manifold \(\). We formulate the sharpness-aware minimization in terms of loss function values on the manifold \(\) as

\[_{w}_{\|\|_{w}^{2}^{2}}(_{w}())-(w)}}_{}.\] (7)

In contrast to Euclidean space, the Riemannian manifold is not a vector space in general. Hence, the familiar concepts defined in Euclidean space may not be well-defined. Therefore, we restrict the perturbation \(\) in the tangent space \(_{w}\). To solve the inner subproblem, we resolve the inner optimization problem in a different manner as

\[_{_{w}}_{w}( )-(w)\|\|_{w}^{2} ^{2}.\] (8)for a fixed point \(w\). For ease of computations, we approximate the perturbed loss function \(_{w}()\) via Taylor's expansion as

\[_{w}()(w)+ (w),_{w}.\] (9)

Then, our inner maximization problem comes in hand:

\[_{_{w}}(w), _{w}\|\|_{w}^{2}^{2}.\] (10)

This problem could be easily solved since it finds the steepest direction \(\) on Riemannian manifold \(\), whose solution is known to be just Riemannian gradient. Therefore, we have

\[^{*}=(w)}{\|(w )\|_{w}}.\] (11)

Under the optimal perturbation \(^{*}\) in (11), we further approximate the gradient of the sharpness-aware minimization in (7) (for outer minimization problem) as

\[_{w}(^{*}) (w)|_{w=_{w}(^{*})}.\] (12)

since the left-hand side of (12) requires a higher-order Riemannian gradient, which is not computationally feasible in practice. The remaining is that the approximated Riemannian SAM gradient \((w)|_{w=_{w}(^{*})}\) in (12) is not on the tangent space at \(w\), \(_{w}\). To perform an actual parameter update on \(w\), we should transport the Riemannian SAM gradient to the tangent space \(_{w}\) via vector transport \(_{_{w}(^{*})}^{w}\) with respect to the retraction \(_{w}\). We summarize the overall optimization procedure in Algorithm 1. Note that, in order to consider the most practical case, we assume that the same minibatch is used for computing SAM perturbation with the ascent step and the actual parameter updates with the descent step (see lines 5, 6, and 8). In fact, one can use different batches for lines 5\(\) 7 and lines 8\(\) 10 respectively or full-batch gradient for both ascent and descent steps.

**Remarks on Algorithm 1.** In fact, it might be most natural to choose a perturbation region at the current point as in the conventional Euclidean SAM, \( B_{}(w_{t})\{x:d_{}(w_{t},x) \}\) where \(d_{}\) represents the distance on the manifold. However, adopting the constraint in this manner may pose challenges in utilizing the standard assumptions for analyzing non-convex Riemannian optimization, such as geodesic or retraction smoothness (see condition (C-4) in Section 4), which makes it difficult to guarantee convergence. Moreover, the computation of \(d_{}\) is often computationally inefficient in practice. Another possible extension is to apply the vector transport operation from line 8 of Algorithm 1 to line 9. The following outlines the modified procedure: (i) \(g_{t}^{adv}=(w;)|_{w=w_{t }^{adv}}\) and (ii) \(_{t}=_{w_{t}^{adv}}^{w_{t}}g_{t}^{adv}\). For base optimizer \(\), any optimization algorithm commonly used in Riemannian optimization can be adopted (e.g., Riemannian SGD). In the meanwhile, when the vector transport is applied after constructing \(g_{t}^{adv}\) via the momentum-based optimizer \(\), the momentum construction takes place on the tangent space \(_{w_{t}^{adv}}\) at the perturbed point \(w_{t}^{adv}\), while the parameter update occurs on the different tangent space at the point. As a consequence, this might introduce another challenges in understanding and analyzing the overall optimization process. In this perspective, various alternative extensions can also be possible, but among them, we have carefully designed a _theoretically valid, computationally practical, and non-trivially extended Sharpness-Aware Minimization on general manifolds for Riemannian optimization_. Then, we have successfully demonstrated both convergence analysis (Section 4) and empirical studies (Section 5) to corroborate our Riemannian SAM.

**Existing example of Riemannian SAM framework: Fisher SAM.** Following our Riemannian SAM update rule in Algorithm 1, we can show that Fisher SAM is a special instance of Riemannian SAM. We can view the set of neural networks as a neuromanifold  equipped with the KL divergence metric between two points. Hence, let \(w\) be the point on a neuromanifold (or statistical manifold) \(\) which is realized by Euclidean network parameter \(^{d}\). On distribution space, the corresponding metric tensor \(g\) is known to be Fisher information matrix . According to Algorithm 1, the perturbation at line 6 and 7 could be computed as

\[(w) =F()^{-1}()\] \[^{*} =(w)}{\| (w)\|_{w}}=()}{(w)^{}F()(w)}}= ()}{( )^{}F()^{-1}()}}.\]which is entirely identical to the perturbation of Fisher SAM .

**Novel example: Lorentz SAM on hyperbolic geometry.** We derive the novel instance of Riemannian SAM called Lorentz SAM over the Lorentz model introduced in 2.1. First, we derive the Riemannian gradient on the Lorentz model \(^{n}=(^{n},g_{})\). As in Section 2.2, the Riemannian gradient could be computed as

\[h=g_{}^{-1}(w).\] (13)

Since \(g_{}\) in (1) is a diagonal matrix, it is easy to compute the vector \(h\) with Euclidean gradient \((w)\). However, the vector \(h\) is not on the tangent space at \(w\), \(_{w}^{n}\), thus we should have to project the vector \(h\) onto the tangent space \(_{w}^{n}\). The projection is easily computed in a closed-form as

\[_{w}(v)=v+ w,v_{}w.\] (14)

Hence, the Riemannian gradient on Lorentz model is computed by

\[(w)=_{w}g_{}^{-1} (w).\] (15)

As a next step, we should normalize the Riemannian gradient as in line 6 in Algorithm 1, and this is easy to compute as \(\|(w)\|_{w}=(w), (w)_{}}\) via Lorentzian scalar product \(,_{}\) defined in Section 2.1.

We utilize the Lorentz SAM derived above for our primary empirical studies conducted on hyperbolic space. In a similar way, one can derive the Riemannian SAM on Poincare ball or Stiefel manifold, which we defer to Appendix.

### Riemannian SAM Illustration: Toy 3D Illustration

We illustrate the 3-dimensional toy example on the sphere manifold. Let us define the 3D sphere manifold \(^{2}\) with the tangent space at \(w\), \(_{w}\) as

\[^{2} :=\{w^{3}:\|w\|_{2}=1\},\] \[_{w}^{2} :=\{v^{3}:w^{}v=0\}\]

We consider the regression problem with the neural-net-like objective function on the randomly generated synthetic dataset. Toy 3D optimization problem with objective function \(f(w)=}\|y-(Xw)\|_{2}^{2}\) where \(X^{500 3}\) and \(y^{500}\) are drawn from \((0,1^{2})\) and \((0,1)\) respectively with the model parameter \(w=(x,y,z)^{3}\) under \(\|w\|_{2}=1\). **(a)** Comparison of converged points for each method. We plot the contour plots with the spherical coordinates under the relation \((x,y,z)(r,,)=(1,,)\).

Figure 1: Toy 3D illustration

The Figure 1 corresponds to Cartesian coordinates \((x,y,z)\) to spherical coordinates \((r,,)=(1,,)\), rendering contour plots. In Figure 1, we showcases the converged points on the objective function under the optimization using Riemannian SAM (in purple color) and the conventional Euclidean SAM (in pink color). Within a maximum iteration budget \(100\), the purple point (Riemannian SAM) attains a loss value of \(0.3800\) while the pink point (conventional Euclidean SAM) converges with a slightly higher loss value of \(0.3808\).

Furthermore, in terms of sharpness measures, we consider the following basic two quantities: (i) the trace of the Hessian (sharpness in the context of Euclidean space), and (ii) manifold-aware sharpness, characterized by the Riemannian gradient norm \(\|(w)\|_{w}\). Notably, the manifold-aware sharpness aligns with information-geometric sharpness  when dealing with statistical manifolds, where the Riemannian metric is defined by the Fisher information. For the aforementioned problem, we compare two metrics and Figure 2 depicts the results. In both metrics, Riemannian SAM achieves smaller sharpness values than Euclidean SAM, implying convergence toward flatter regions. In other words, since the Euclidean SAM might fail to properly consider the underlying structure of the manifold even for toy examples, this phenomenon is expected to be exacerbated in extremely high-dimensional problems such as deep learning.

## 4 Convergence Analysis

In this section, we present the convergence guarantees for the RiemSAM framework. Our goal is to find a first-order \(\)-approximate solution: the output \(\) such that \(\|()\|_{}^{2} ^{2}\), which is a generalized convergence criterion of Euclidean \(\)-stationary point. To guarantee the convergence for \(\)-approximate solution, we require the following mild assumptions.

**(C-1)**: (Upper-Hessian bounded) The objective function \(\) is said to be _upper-Hessian bounded_ in \(\) with respect to retraction \(\) if there exists some positive constant \(C\) such that \((_{w}(t))}{dt^{2}} C\), for all \(w\) and \(_{w}\) with \(\|\|_{w}=1\), and for all \(t\) such that \(_{w}()\) for all \([0,t]\).
**(C-2)**: (Lower-bounded) The objective function \(()\) is differentiable and has bounded suboptimality.

\[(w^{*})>-.\]

for the optimal point \(w^{*}\).
**(C-3)**: (Unbiasedness and bounded variance) The stochastic Riemannian gradient is unbiased and has a bounded variance:

\[_{(x,y)}[(w;x)]= (w),\] \[_{(x,y)}[\|(w;x)- (w)\|_{w}^{2}]^{2}.\]

where \((w)\) is a true Riemannian gradient evaluated on a full batch of training dataset \(\).
**(C-4)**: (Retraction smoothness) We assume that there exists a constant \(L_{S}>0\) such that

\[(z)(w)+(w), _{w}+L_{S}\|\|_{w}^{2}.\]

Figure 2: Comparison of two sharpness measures.

for all \(w,z\) and \((t)_{w}(t)\) represents a retraction curve on \(\) for \(_{w}\) with the starting point \((0)=w\) and the terminal point \((1)=z\).
**(C-5)**: (Individual Retraction Lipschitzness) We assume that there exists \(L_{}>0\) such that

\[\|()_{z}^{w}(z;x)- (w;x)\|_{w} L_{}\|\|_{w}.\]

for all \(w,z\). As in condition (C-4), \((t)\) denotes a retraction curve and \(()_{z}^{w}\) is a vector transport associated with this retraction curve.

The function class with condition (C-1) corresponds to the continuous function family with Lipschitz continuous gradients in the Euclidean space . The assumptions (C-2) \(\) (C-4) are standard in convergence analysis of Riemannian optimization algorithms, under which Riemannian SGD is known to be first-order optimal . Note that, unlike in Euclidean space, the constant \(L_{S}\) in (C-4) and \(L_{}\) in (C-5) may be different. According to , the condition (C-5) can be derived under the standard assumption on retraction Lipschitzness with parallel translation and one additional assumption on the bound between the parallel translation and the vector transport, but we assume the retraction Lipschitzness with the vector transport for simplicity. Lastly, in condition (C-5), the retraction Lipschitzness is assumed individually with respect to each sample in order to control the alignment of SAM gradient and the original gradient step as in .

Now, we are ready to present our main theorem.

**Theorem 1** (Convergence of Riemannian SAM).: _Let \(\) denote an iterate uniformly chosen at random from \(\{w_{1},w_{2},,w_{T}\}\). Further, we let \(=\{L_{S},L_{}\}\) where the constants \(L_{S}\) and \(L_{}\) are defined in condition (C-4) and (C-5) respectively. Under the conditions (C-1) \(\) (C-5) with descent learning rate \(_{t}=}}\) and ascent learning rate \(_{t}=L}\), we have the following complexity for the constant batch size \(b\):_

\[\|()\|_{ }^{2}}{}+^{2 }}{b}+^{2}}{bT^{5/6}}=(1/).\] (16)

_where \(=(w_{0})-(w^{*})\) and the constants \(\{Q_{i}\}_{i=1}^{3}\) are irrelevant to the total iteration \(T\) or the manifold dimension \(d\)._

We make some remarks on our convergence results and relationship to conventional Euclidean SAM.

**Theoretical implications.** Our key observation of Theorem 1 lies in the _alignment between the Riemannian gradient_\((w_{t})\) _(line 5 in Algorithm 1) and the Riemannian SAM gradient_\(_{w_{t}^{adv}}^{w_{t}}(w_{t}^{adv})\) _(line 9 in Algorithm 1)_ for the perturbed point \(w_{t}^{adv}\), \(w_{t}^{adv}\). The previous study  on Euclidean SAM says that Euclidean SAM gradient should be well-aligned with the true gradient step for convergence. Unlike the theoretical claim in , we stress that for convergence guarantee those gradients should be _well-aligned within the preconditioned space (by inverse Riemannian metric) regardless of alignment in Euclidean space_. To verify this insight, we directly measure the angles between two vectors with a 2D toy example, illustrating how they align in practice. Toward this, we consider two angles: (i) \(( f(w_{t}^{adv}), f(w_{t}))\) (Euclidean Alignment) and (ii) \((_{w_{t}^{adv}}^{w_{t}}f(w_{t}^{adv}),f(w_{t}))\) (Riemannian Alignment, Ours). In this example, we consider the logistic regression where \(200\) data samples are generated with \(100\) of them sampled from \((-1,1^{2})\) and the remaining sampled from \((1,1^{2})\). The labels are assigned such that if a sample was drawn from a Gaussian distribution with a mean of \(-1\), the label was set to \(y=0\), and otherwise, we set \(y=1\). We minimize the cross-entropy loss with our Riemannian SAM with the Fisher information matrix as the Riemannian metric. The Figure 3 depicts the comparison of angles. The loss decreases up to \(10\)-th iteration, after which it remains around the converged point. As evident from the illustration, while the angles between the Euclidean space SAM gradient and the gradient deviate by up to around \(25\) degrees, the angles between the preconditioned SAM gradient and the preconditioned gradient, influenced by the Fisher information, align more closely with deviations

Figure 3: Toy 2D illustration

only up to a maximum of \(10\) degrees. In high-dimensional loss landscapes, we expect that the angles would become significantly larger, corroborating our theoretical insight.

**On upper bound.** Distinct from the convergence of Euclidean SAM , our upper bound (16) has the additional term involving \(Q_{3}\), but we still achieve the optimal complexity of SGD, \((1/^{4})\) for \(\)-approximate solution. Note that the presence of term involving the constant \(Q_{3}\) in our bound comes from the fact that (i) smoothness condition (C-4) and Lipschitzness condition (C-5) are not equivalent on manifolds and (ii) we should handle the vector-transported gradients (see line 8 in Algorithm 1) at each iteration, which are the main challenges in our proof. Our results can also provide the guarantees for SAM variants such as Fisher SAM , whose convergence guarantees are missing.

## 5 Experiments

We conduct two sets of experiments; (i) knowledge graph completion, and (ii) machine translation. The first experiment aims to evaluate our Riemannian SAM on shallow networks and the second task is for optimizing large-scale deep neural networks. For all our experiments, we consider the Lorentz manifold introduced in Section 2.1 and employ the recent hyperbolic architecture, HyboNet. The HyboNet is a _fully hyperbolic_ neural network, whose each layer is constructed on the Lorentz manifold including a linear, attention, residual, and positional encoding layer. We implement our Riemannian SAM upon Geoopt framework  written in PyTorch library . Regarding hyperparameters, we basically adhere to the same experiment settings in  and the details are provided in each section and Appendix.

### Knowledge Graph Completion

A knowledge graph completion aims to predict missing relationships within a knowledge graph, which represents structured information as a collection of entities, their attributes, and the relationships between them. More precisely, knowledge in a graph is of the form of triplets \((h,r,t)\) where \(h\), \(r\), and \(t\) denote the head entity, the relationship or predicate, and the tail entity respectively. In the knowledge graph completion task, given a partially populated knowledge graph, the goal is to predict the missing entity or relationship in a triplet: solving \((h,r,?)\) and \((?,r,t)\).

In our experiments, we use two popular benchmark datasets; WN18RR  and Fb15k-237 . We employ the same data preprocessing in  and two standard metrics for evaluations: (i) Mean Reciprocal Rank (MRR), the average of the inverse of the true entity ranking, and (ii) Precision at \(K\) (H@K), the proportion of test instances where the correct answer appears in the top-\(K\) ranked predictions. For

    &  &  &  \\   & & \#Dims & **MRR** & **H@10** & **H@3** & **H@1** & **\#Dims** & **MRR** & **H@10** & **H@3** & **H@1** \\   HyboNet \\  } & MuRP & 32 & 46.5 & 54.4 & 48.4 & 42.0 & 32 & 32.3 & 50.1 & 35.3 & 23.5 \\  & RoTH & 32 & 47.2 & 55.3 & 49.0 & 42.8 & 32 & 31.4 & 49.7 & 34.6 & 22.3 \\  & AttH & 32 & 46.6 & 55.1 & 48.4 & 41.9 & 32 & 32.4 & 50.1 & 35.4 & 23.6 \\  & HyboNet & & & & & & & & & & \\  & w/tAdam & & & & & & & & & & \\   HyboNet \\ w/**SAM** \\  } & MuRP & 32 & **49.3\({}^{ 0.2}\)** & **56.0\({}^{ 0.2}\)** & **50.7\({}^{ 0.1}\)** & **46.2\({}^{ 0.3}\)** & 32 & **34.3\({}^{ 0.2}\)** & **52.0\({}^{ 0.1}\)** & **37.3\({}^{ 0.3}\)** & **25.1\({}^{ 0.4}\)** \\    & MuRP & \(\) & 48.1 & 56.6 & 49.5 & 44.0 & \(\) & 33.5 & 51.8 & 36.7 & 24.3 \\  & RoTH & \(\) & 49.6 & 58.6 & 51.4 & 44.9 & \(\) & 34.4 & 53.5 & 38.0 & 24.6 \\  & AttH & \(\) & 48.6 & 57.3 & 49.9 & 44.3 & \(\) & 34.8 & 54.0 & 38.4 & 25.2 \\  & HyboNet & & & & & & & & & & \\  & w/tAdam & & & & & & & & & & & \\   HyboNet \\ w/**tSAM** \\  } & MuRP & \(\) & 48.1 & 56.6 & 49.5 & 44.0 & \(\) & 33.5 & 51.8 & 36.7 & 24.3 \\  & RoTH & \(\) & 49.6 & 58.6 & 51.4 & 44.9 & \(\) & 34.4 & 53.5 & 38.0 & 24.6 \\  & AttH & \(\) & 48.6 & 57.3 & 49.9 & 44.3 & \(\) & 34.8 & 54.0 & 38.4 & 25.2 \\  & HyboNet & & & & & & & & & & \\  & w/tAdam & & & & & & & & & & \\   

Table 1: Link prediction results (%) in the filtered setting for WN18RR and FB15k-237 datasets. For hyperbolic architectures, \(\{200,400,500\}\) and we report the best result. The results of baselines are taken from  except for HyboNet whose results are reproduced by ourselves. The best results among hyperbolic architectures with the same dimensions are in boldface. Our Riemannian SAM (denoted by rSAM) shows the superior performance compared to Riemannian Adam (denoted by rAdam).

marginal hyperparameter tuning, we tune the ascent learning rate \(_{t}\{10^{-4},10^{-3},10^{-2},10^{-1}\}\) for Riemannian SAM and the other hyperparameters are the same as HyboNet for fair comparisons.

Table 1 illustrates the results on WN18RR and Fb15k-237 datasets. As in previous work , we test our Riemannian SAM on two different regimes: (i) small embedding dimension \(32\) and (ii) large dimension \(\{200,400,500\}\) for both datasets. Regarding the large dimension, we report the best results among the dimension candidates. In Table 1, Riemannian SAM on the Lorentz model achieves the best performance with great margins for all comparison metrics. In the same way, Riemannian SAM shows the state-of-the-art performance for all metrics considered under both regimes.

### Machine Translation

In this experiment, we evaluate our Riemannian SAM on Lorentz Transformer built with Lorentz components introduced in  for IWSLT '14 and WMT '14 benchmark datasets in machine translation. We use the BLEU score as an evaluation metric on the IWSLT '14 test set and the newstest2013 test set of WMT '14 respectively. According to , we train hyperbolic models with Riemannian SAM in a low-dimensional setting where the dimension of the word vector is \(d=64\). As in the knowledge graph completion task, we choose the ascent learning rate in \(_{t}\{10^{-5},10^{-4},,10^{-2}\}\) for marginal hyperparameter tuning.

Table 2 demonstrates the results. HyboNet baseline trained with Riemannian Adam already outperforms the Euclidean Transformer in both IWSLT '14 and WMT '14 datasets. Upon this baseline, we only substitute our Riemannian SAM for Riemannian Adam with other hyperparameters unchanged. As seen in Table 2, Riemannian SAM significantly outperforms the Riemannian Adam baseline for both datasets. Note that both HyperNN++ and HAtt are partially hyperbolic networks, so we could not evaluate our Riemannian SAM on these models since it is difficult for a fair evaluation.

**Wall-clock time.** As in Euclidean SAM, Riemannian SAM requires additional forward and backward propagation in a single iteration loop (see Algorithm 1). Thus, we report the wall-clock time comparison for each experiment. For knowledge graph completion (Section 5.1) and machine translation (Section 5.2), Riemannian SAM takes roughly \(1.6\) and \(1.8\) times longer than Riemannian Adam for one epoch, respectively. To alleviate the computational overhead, one can employ the stochastic weight perturbation (SWP) and sharpness-sensitive data selection (SDS) suggested in , which do not depend on the manifold structure. Another practical consideration is to use a subset of minibatch in computing perturbation (see line 6 in Algorithm 1) for large-scale models. We leave the study on reducing computational cost as future work.

## 6 Conclusion

In this study, we proposed a sharpness-aware minimization on Riemannian manifolds, called Riemannian SAM. Under our framework, we presented novel examples of Riemannian SAM including a Lorentz SAM. We analyzed the convergence of the Riemannian SAM for general manifolds with a less aggressively decaying ascent learning rate condition. Moreover, we showed that Riemannian SAM can provide the convergence guarantee for SAM variants whose convergence proofs are missing such as Fisher SAM. We also illustrated that Riemannian SAM empirically outperforms ERM-based Riemannian optimization algorithms for popular deep learning tasks with hyperbolic neural networks. As future work, we plan to study the technique to reduce the computations and analyze the generalization error bounds of Riemannian SAM theoretically.

    & **Model** & **IWSLT ’14** & **WMT ’14** \\   & ConvSeq2Seq & 23.6 & 14.9 \\  & Transformer & 23.0 & 17.0 \\   & HyperNN++ & 22.0 & 17.0 \\  & Hatt & 23.7 & 18.8 \\   & HyboNet (with Riemannian Adam) & 25.5 & 19.3 \\    & \(}\) (with **Riemannian SAM, Ours**) & **26.0** & **20.1** \\   

Table 2: The BLEU scores on the test set of IWSLT ’14 and WMT ’14 under low-dimensional setting following the hyperbolic study  with the word vector dimension \(d=64\). The results on baselines are taken from  except for HyboNet whose results are reproduced by ourselves.