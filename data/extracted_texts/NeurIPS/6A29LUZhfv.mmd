# MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures

Jinjie Ni\({}^{}\), \({}^{}\)Fuzhao Xue\({}^{}\), \({}^{}\)Xiang Yue\({}^{}\),

\({}^{}\)Yuntian Deng, \({}^{}\)Mahir Shah, \({}^{}\)Kabir Jain, \({}^{}\)Graham Neubig, \({}^{}\)Yang You

\({}^{}\)National University of Singapore, \({}^{}\)Carnegie Mellon University, \({}^{}\)Allen Institute for AI

https://mixeval.github.io/

###### Abstract

Evaluating large language models (LLMs) is challenging. Traditional ground-truth-based benchmarks fail to capture the comprehensiveness and nuance of real-world queries, while LLM-as-judge benchmarks suffer from grading biases and limited query quantity. Both of them may also become contaminated over time. User-facing evaluation, such as Chatbot Arena, provides reliable signals but is costly and slow. In this work, we propose MixEval, a new paradigm for establishing efficient, gold-standard LLM evaluation by strategically mixing off-the-shelf benchmarks. It bridges (1) comprehensive and well-distributed real-world user queries and (2) efficient and fairly-graded ground-truth-based benchmarks, by matching queries mined from the web with similar queries from existing benchmarks. Based on MixEval, we further build MixEval-Hard, which offers more room for model improvement. Our benchmarks' advantages lie in (1) a 0.96 model ranking correlation with Chatbot Arena arising from the highly impartial query distribution and grading mechanism, (2) fast, cheap, and reproducible execution (6% of the time and cost of MMLU), and (3) dynamic evaluation enabled by the rapid and stable data update pipeline. We provide extensive meta-evaluation and analysis for our and existing LLM benchmarks to deepen the community's understanding of LLM evaluation and guide future research directions.

## 1 Introduction

**That Which is Measured, Improves.** Evaluation is essential in the AI community for two main reasons: (1) benchmarks provide early signals to model developers, aiding in refining data and model design, and (2) benchmarks guide users in selecting suitable models for specific use cases. Therefore, benchmarks offer feedback to the entire community, facilitating model optimization. Consequently, the main concern of evaluating LLMs is **impartiality**-we need to optimize impartial objectives so that the community advances in the right direction. In practical LLM evaluations, three primary biases contribute to a lack of impartiality: (1) **query bias**-evaluation queries falling short of comprehensiveness or appropriate distribution (2) **grading bias**-the grading process involving significant bias or error (3) **generalization bias**-models overfitting the evaluation data.

**Large Scale User-facing Evaluation Provides a More Impartial Signal.** Practitioners generally adopt either automatic or user-facing approaches for LLM benchmarking. Automatic benchmarking typically employs traditional ground-truth-based benchmarks, such as MMLU , which often fail to capture real-world query comprehensiveness and nuance while involving a comparatively impartial grading process; or employs open-ended benchmarks using LLMs as graders, such as MT-Bench , suffering from both grading bias and query incomprehensiveness due to the preference biases and high cost of frontier LLM judges. Additionally, the static nature of automatic benchmarks resultsin contamination over time, amplifying the generalization issue. Such biases lead to significant deviations from gold-standard evaluation, impeding model development. On the other hand, large-scale user-facing benchmarking, such as Chatbot Arena3, offers more reliable objectives for model development and effectively mitigates the above-mentioned three biases because (1) it collects a vast array of real-world user queries, thereby ensuring superior query comprehensiveness and distribution, (2) it judges diverse and complex model responses stably due to the "wisdom of the crowd" effect , where individual judgment noise is averaged out over a large number of samples, mitigating the grading bias, and (3) it continuously receives fresh user queries, mitigating the benchmark contamination issue. Furthermore, it guides model optimization to meet user needs effectively in practical applications, which is a crucial goal of developing models. However, Chatbot Arena is prohibitively expensive (Figure 1), slow, and irreproducible. Moreover, it is not directly accessible for public usage, hindering practitioners from conducting easy and fast model evaluations.

**MixEval: Towards Efficient Gold-Standard LLM Evaluations.** In this work, we aim to establish a highly impartial gold-standard benchmark without compromising efficiency. This can be achieved by leveraging (1) the efficiency and grading impartiality of ground-truth-based benchmarks and (2) the superior comprehensiveness and distribution of real-world user queries. To this end, we propose MixEval, a two-stage benchmark reconstruction pipeline consisting of (1) wild query mining and (2) grounding existing benchmarks in the mined queries. We introduce an accurate user query retrieval process, comprising query detection, filtering, and classification. In the detection phase, we train open-source LLMs on self-collected data to detect queries in Common Crawl splits. During filtering, we utilize GPT-4 Turbo to exclude non-query sentences. In classification, we categorize the filtered queries by input and output modalities, retaining text-in-text-out queries for LLM evaluation. To align benchmark queries with real-world queries, we match each crawled web user query with its most similar query in the benchmark pool and the corresponding ground truth answer. We designate the resulting benchmark as MixEval. To improve the benchmark's ability to distinguish strong models, we derive a challenging subset from MixEval, termed MixEval-Hard. To mitigate the overfitting issue, we periodically update the data points in MixEval and MixEval-Hard using our fast, stable pipeline, which performs benchmark mixture with a different batch of wild queries from

Figure 1: Benchmark correlations (%) with Chatbot Arena Elo, against the total costs of evaluating a single GPT-3.5-Turbo-0125 model. **MixEval** and **MixEval-Hard** show the highest correlations with Arena Elo and Arena Elo (En) among leading benchmarks. We reference the crowdsourcing price for Amazon Mechanical Turk ($0.05 per vote) when estimating the cost of evaluating a single model on Chatbot Arena (approximately $2,936). Chatbot Arena is prohibitively expensive, while **MixEval** and **MixEval-Hard** are cheap and cost-effective alternatives. Details on the correlation and evaluation cost values are provided in Section E.

the same distribution, showing low model score variance (0.36 Std. on a 0-100 scale) and significant version difference (85% unique query ratio). We thereby effectively mitigate the above-mentioned three evaluation biases through the proposed benchmark mixture pipeline, while maintaining high efficiency. As shown in Figure 1, MixEval and MixEval-Hard achieve similar model rankings as Chatbot Arena while being far less costly.

**Why use MixEval?** MixEval offers five significant advantages for practitioners: (1) **accurate** model ranking, demonstrated by a 0.96 correlation with Chatbot Arena, (2) **fast**, **cheap** and **reproducible** execution, requiring only 6% the time and cost of MMLU and with no dependence on human input, (3) **dynamic** benchmarking enabled by low-effort and stable updating mechanism, (4) a **comprehensive** and **less biased** query distribution, as it bases queries on a large-scale web corpus, and (5) a **fair** grading process without preference bias, ensured by its ground-truth-based nature.

**Research Contributions**

* We developed a pipeline for detecting real-world instructions, capable of mining queries to build benchmarks and providing a scalable solution for collecting vast amounts of real-world instruction-following data.
* We introduced a new way to utilize benchmarks, demonstrating that real-world query distributions and user preferences can be reconstructed by strategically mixing off-the-shelf benchmarks with web-mined queries.
* To the best of our knowledge, MixEval creates the first ground-truth-based dynamic benchmark with general-domain queries, benefiting from a rapid and stable data updating mechanism.
* The resulting dynamic benchmarks, _i.e.,_ MixEval and MixEval-Hard, exhibit significant correlations (0.93 and 0.96) with real-world user preference leaderboard (_i.e.,_ Chatbot Arena) and showcase high impartiality and efficiency.
* We provide meta-evaluation and extensive analysis for MixEval and other leading LLM benchmarks, delivering detailed insights that enhance the community's understanding of LLM evaluation and guide future research directions.

## 2 LLM Benchmarks are Biased from Realistic User Queries and Preferences

**How Much Do Our Benchmarks Reflect Real-world User Queries and Preferences?** The rapid advancement of LLMs has led to the introduction of numerous benchmarks. However, the community may still lack a comprehensive understanding of how well these benchmarks align with real-world use cases and human preferences. Without such understanding, the signals derived from evaluations might be misleading, thereby impeding model development. To investigate this issue, we (1) analyze the correlations between benchmarks (Figures 1 and 10) and (2) visualize their query distributions in a unified 2-D space (Figure 2). The setups for these analyses are detailed in Section E.

### Important Takeaways

**Current benchmarks show a limited correlation with human preferences.** Figures 1 and 10 reveal that many benchmarks exhibit low correlation with human preferences (Arena Elo). The highest correlations are Arena-Hard (0.86) for LLM-judged and ARC-c (0.85) for ground-truth-based benchmarks. Using the benchmark mixture technique, our MixEval

Figure 3: Query topic summarization for Figure 2. The plot aggregates all queries and divides them into 16 regions. From each region, 100 queries are uniformly sampled and analyzed by GPT-4 for topic summarization. A clear trend is observed, with topics transitioning from non-technical at the bottom to technical at the top.

and MixEval-Hard achieve the highest correlations with human preferences, at 0.93 and 0.96, respectively.

**Most benchmarks exhibit a skewed topic distribution.** Ground-truth-based and LLM-judged benchmarks show a skewed query distribution compared to detected web queries and wild datasets. Notably, BoolQ, Natural Questions, and MMLU align more closely with wild user queries due to their data collection methods. Specifically, questions in BoolQ and Natural Questions originate from Google Search, while MMLU is designed to cover a wide range of topics (57 topics, including Atari games ).

**Query comprehensiveness is crucial.** General-domain benchmarks, which are not tailored for specific domains in their data collection pipelines, exhibit a stronger correlation with Arena Elo than domain-specific ones. Notably, 10 out of 13 general-domain benchmarks have an Arena Elo correlation score above 0.5, whereas only 1 out of 8 domain-specific benchmarks achieves this. This underscores the significance of comprehensive query topics for achieving high correlation with human preferences.

**Some general-domain benchmarks are actually domain-specific.** Despite being labeled as general-domain benchmarks, DROP and WinoGrande have limited scopes, often narrower than many domain-specific benchmarks. As depicted in Figure 3, DROP queries mainly address History, Politics, Sports,

Figure 2: Query Topic Distribution of the Benchmarks. Ground-truth-based benchmarks are represented by orange dots, wild datasets by yellow dots, and LLM-judged benchmarks (MT-Bench and Arena-Hard) by yellow dots, all plotted against our detected web queries shown as blue dots. Query sentence embeddings were dimensionally reduced to map them onto a unified 2-D space, facilitating direct comparisons of topic distributions across benchmarks. As we move from the bottom to the top of the figure, query topics transition from non-technical to technical. Topic summaries for each region are detailed in Figure 3.

Demographics, and Societal Issues, whereas WinoGrande queries focus primarily on Grammar, Language, Decision Making, and Social Dynamics.

**User population size affects the query distribution.** Figure 2 shows distribution differences in wild queries across varying user population sizes. ShareGPT, grounded in 100 million4 active users of ChatGPT by mid-2023, contrasts with WildChat , Chatbot Arena Conversations , and LMSYS-Chat-1M , which have user bases of 0.2 million, 0.13 million, and 0.21 million, respectively. The global internet user count was 5.4 billion5 in 2023, an order of magnitude larger than all considered wild datasets. Consequently, user bases of the internet, ShareGPT, and other datasets span three distinct orders of magnitude. ShareGPT's larger user population (second order of magnitude) yields a distribution most similar to web queries from the global internet user base (third order of magnitude), both visually and in cluster distance (C-Dist).

**Chatbot Arena and Arena-Hard queries exhibit biases.** Compared to web queries and ShareGPT data, datasets from the Chatbot Arena website--Chatbot Arena Conversations and LMSYS-Chat-1M--have a higher proportion of technical queries (as presented in Figure 3, queries with higher position on the map are more technical). This indicates a user base skewed towards technical users, potentially affecting evaluation results, as an effective LLM benchmark should mimic real-world use cases. Furthermore, a minor discrepancy exists between web and ShareGPT queries, suggesting that one or both of them may still slightly deviate from actual real-world query distributions. Moreover, Arena-Hard queries exhibit a pronounced bias towards technical topics. This likely stems from the design of their data pipeline for sampling hard prompts. We will demonstrate that employing a carefully designed sampling technique is essential to preserve the query distribution while enhancing difficulty. This is supported by MixEval-Hard's similar distribution to the original web queries and wild datasets (see Section 3.3).

## 3 MixEval

In Section 2, we show that current ground-truth-based and LLM-judged benchmarks have skewed query distributions and limited correlation with human preferences. Additionally, LLM-judged benchmarks suffer from LLM preference bias and both of them become contaminated over time. In contrast, Chatbot Arena is less biased and more dynamic but requires slow and expensive human preference data collection, resulting in irreproducible outcomes.

To address these issues, we introduce MixEval (Figure 4), which aligns ground-truth-based LLM benchmarks with real-world user queries. This method uses user queries mined from the web and matches them with similar queries from existing benchmarks, and involves two stages: (1) user query detection from the web and (2) benchmark mixture. To improve model separability and reduce contamination, we also propose MixEval-Hard and a dynamic updating mechanism.

### Web User Query Detection

In this stage, we detect user queries from Common Crawl . Both recall and precision are crucial to ensure the query distribution reflects real-world scenarios. Therefore, we developed two benchmarks

Figure 4: MixEval, a two-stage benchmark reconstruction pipeline, comprises (1) web query detection and (2) benchmark mixture. We further introduce MixEval-Hard to enhance model separability, alongside a dynamic updating mechanism to mitigate contamination risk.

to evaluate our query detector's performance. The first benchmark includes self-collected in-the-wild user queries as positive samples, with non-query datasets such as Wikipedia  as negative samples. The second, higher-quality benchmark contains positive and negative samples hand-picked by our authors from in-the-wild query and non-query datasets. In preliminary experiments, direct prompting of open-source language models performed poorly on our benchmarks. Thus, we developed a rectification pipeline to ensure high recall and precision cost-effectively. We started with a detection phase to gather training data. Testing various open-source LLMs, Vicuna 33B  achieved a high recall (>99%) on our test sets with careful prompt engineering, ensuring that very few positive samples were missed initially. In this phase, we detected around 20k queries using Vicuna 33B over a subset of Common Crawl. We then used GPT-4 to more accurately label these data as positive or negative samples, and used the resulting data to train Vicuna 33B. The trained Vicuna 33B achieved high recall (>99%) and precision (>98%) on our benchmarks and detected 2M user queries from the entire Common Crawl. Finally, we prompted GPT-4 Turbo to classify them, extracting text-in-text-out queries for LLM evaluation. Future work will address queries with other I/O modalities.

### Benchmark Mixture

To bridge wild user queries \(\) and ground-truth LLM benchmarks, we create a benchmark pool \(=\{_{1},_{2},...,_{n}\}\), where each \(_{n}=\{b_{1},b_{2},...,b_{k}\}\) represents a distinct ground-truth LLM benchmark. We define a mapping \(f:q_{i} b_{j}\), with \(q_{i}\) and \(b_{j}\). For each \(q_{i}\), we rank similarities between each \((q_{i},b_{j})\) pair and select the most similar \(b_{j}\) that satisfies \(\): \(b_{j}=f(q_{i})=_{b_{j}}S(q_{i},b_{j})\) s.t. \(\). We use the dot-product between normalized sentence embeddings as the similarity score \(S()\). When retrieving the top-1 \(b_{j}\), \(\) is a length constraint on the input (or context) field of each \(b_{j}\), addressing the effect of long inputs in the benchmark data mixture. The sentence embeddings of queries are computed using the all-mpnet-base-v2 model from SentenceTransformers . To ensure quality and comprehensive sample coverage, we selected the development and test splits of widely adopted benchmarks from diverse domains and topics. Details and distributions of these benchmarks are provided in Appendix D.

### MixEval-Hard

Frontier LLMs are rapidly approaching human-level performance across diverse tasks. As these models progress, existing benchmarks will become saturated, hindering differentiation between models. Although MixEval reflects typical user queries, it is constrained by the benchmark pool \(\)'s overall difficulty. Our results in Table 3 indicate that top models, such as GPT-4 Turbo and Claude 3 Opus, have surpassed 88% accuracy on MixEval. To improve the benchmark's ability to discriminate between very strong models, we extract a challenging subset from MixEval to create MixEval-Hard.

Given MixEval denoted as \(^{}\), we sample a hard subset \(^{}\) from \(^{}\) by computing a difficulty score \(_{i}\) for each entry, prioritizing higher scores. Consider a set of model prediction results \(\), where \(\) is a 0-1 matrix of shape \((N_{model},N_{^{}})\), with 1 indicating an incorrect model response. Here, \(N_{model}\) is the number of models, and \(N_{^{}}\) is the number of questions in \(^{}\). The difficulty score \(_{i}\) for a query \(b^{}_{i}\) is computed by \(_{i}=_{i}}\), where each model's result on question \(i\) is weighted by its accuracy \(_{j}\) on the dataset. Given \(=\{_{1},_{2},...,_{N_{^{}}}\}\), we sample from \(^{}\) with rejection:

\[^{}=\{b^{}_{i}^{}:p(b^{ }_{i})(^{}\{b^{}_{i}\},^{ })\},\]

where \((x,y)\) denotes the cluster distance between \(x\) and \(y\). The probability of drawing \(b^{}_{i}\), \(p(b^{}_{i})=}}{_{b^{}_{k}^ {}}e^{_{k}}}\), is based on \(_{i}\). This rejection sampling ensures that MixEval-Hard is difficulty-first

    & \# Queries &  Avg. \# Toks \\ per Query \\  &  Avg. \# Toks \\ \# Inputs \\  &  Avg. \# Toks \\ per Input \\  &  Min \# Toks \\ per Input \\  &  Max \# Toks \\ per Input \\  &  English \\ Ratio \\  & 
 Eval \\ Type \\  \\ 
**MixEval** & 4000 & 23 & 0.3 & 41.3 & 6 & 954 & 95.15\% & Ground \\ 
**MixEval-Hard** & 1000 & 27.3 & 0.4 & 47.3 & 7 & 954 & 95.22\% & Truth \\   

Table 1: The key statistics of MixEval and MixEval-Hard. With dynamic benchmarking, the numbers may vary slightly while the number of queries will not change.

while maintaining a balanced query distribution. We obtain 1000 samples for MixEval-Hard. The statistics of MixEval and MixEval-Hard are detailed in Table 1.

### Dynamic Benchmarking

Static benchmarks risk contamination over time as models may overfit to the benchmark data , undermining evaluation reliability. To address this, we periodically update the data points in MixEval and MixEval-Hard using the automatic pipeline described above, i.e., performing benchmark mixtures based on the queries uniformly sampled from the massive web queries detected, which completes updates within one minute. Table 2 shows score stability and version differences. We created five versions of MixEval by altering the random seed when sampling web queries and ran five models on them. As shown, the average mean and standard deviation (Std.) for the models across the versions are 77.64 and 0.36, respectively, demonstrating high score stability. For each pair of versions, we compute the unique sample ratio for sampled web queries and benchmark data points. Given samples \(X=\{x_{1},x_{2},...,x_{n}\}\) from version A and \(Y=\{y_{1},y_{2},...,y_{n}\}\) from version B, the unique sample ratio \(\) is calculated as \(=\), representing the unique ratio of the \(X Y\) set. The average unique web query ratio across all version pairs is 99.71%, and the unique ratio for MixEval versions is 85.05%, indicating significant differences between versions. This efficient updating mechanism, alongside stable model scores and significant data point variations, effectively mitigates benchmark contamination. Additionally, we plan to dynamically expand our benchmark pool with newly released benchmarks to further enhance the mixed benchmark distribution.

To summarize, we update the data points of MixEval via (1) batch web query update (sampling different web queries batches from the crawled web queries), (2) source web query update (updating all the web queries with the latest Common Crawl) or (3) benchmark pool update (incorporating new ground-truth-based benchmarks to the benchmark pool). Since the mechanism of MixEval is to match web queries with benchmark pool samples, the above three updating methods refreshes both the web queries (the first and the second method) and benchmark pool samples (the third method).

## 4 Results

### Experiment Settings

We evaluate models on MixEval and MixEval-Hard using the Transformers library  for open-source models, adhering to the official settings in their Hugging Face model card. Proprietary models are assessed via their official API endpoints, using the latest versions as of April 30, 2024. Chat models employ official chat templates or FastChat chat templates , and base models are evaluated in a 5-shot setting. Both MixEval and MixEval-Hard, comprising samples from various benchmarks, demonstrate the inadequacies of traditional rule-based parsing methods across all benchmarks and models. To improve parsing accuracy, we use GPT-3.5-Turbo-0125 as the model parser to either score the response (free-form questions) or extract the model's choice (multiple-choice problems). The stability of the GPT-3.5 Turbo parser is evidenced in Table 2 of this paper and Table 4 of . We will also provide an open-source model parser with its stability test to ensure long-term reproducibility. Section J details the model parser prompts, and Section I compares the model parser to the rule parser. Models are evaluated on 4 or 8 A100 GPUs. All correlations with Arena Elo are based on the Chatbot Arena Leaderboard as of May 1, 2024.

    & GPT-3.5- & GPT-3.5- & Claude & Mistral- & Reka & **Avg.** & **Unique Web** & **Unique** \\  & Turbo-0125 & Turbo-1106 & 3 Haiku & Small & Edge & **Query Ratio** & **MixEval** & **Query Ratio** \\ 
**Mean** & 79.66 & 79.25 & 80.32 & 80.57 & 68.42 & 77.64 & 99.71\% & 85.05\% \\ 
**Std.** & 0.26 & 0.28 & 0.34 & 0.56 & 0.35 & 0.36 & & \\   

Table 2: Stability test for dynamic benchmarking. Five models tested across five updated versions of MixEval show an average mean of 77.64 and a Std. of 0.36, validating the stability of model scores over versions. The unique web query ratio, averaged across all version pairs, is 99.71%, and the unique benchmark query ratio is 85.05%, indicating significant differences between versions.

### Effectiveness of MixEval

MixEval and MixEval-Hard achieve the highest correlation with Arena Elo and Arena Elo (**En**) among all benchmarks.As shown in Figures 1 and 10, MixEval and MixEval-Hard, derived from the proposed MixEval pipeline to simulate diverse user queries, achieve significantly higher correlations (10% higher than the top SOTA benchmark) with human preferences (both Arena Elo and Arena Elo (En)), ranking second and first, respectively. Notably, MixEval-Hard's correlation with Arena Elo is even slightly higher than with Arena Elo (En). As discussed in Section 4.5, query difficulty impacts human preference correlation. Therefore, MixEval-Hard's superior correlation may partially result from increased query difficulty. The high correlations of MixEval and MixEval-Hard with human preferences enable efficient, cost-effective, and reliable model ranking compared to human-in-the-loop benchmarks.

MixEval improves the correlation with Arena Elo and Arena Elo (**En**) across all main benchmark splits of MixEval.In Figure 5, we select the top-10 benchmarks from our pool with sufficient sample sizes (see sample number distribution in Figure 8). For each benchmark, we present (1) the correlation between Arena Elo and the original benchmark, and (2) the correlation between Arena Elo and the MixEval-mixed version. Remarkably, **all** benchmarks exhibit significant improvements in their correlations with Arena Elo after being processed by MixEval. The correlation increase is notably high (>40%) in benchmarks such as BoolQ, AGIEval, SIQA, and PIQA. MixEval and MixEval-Hard, which aggregate all benchmarks, consistently outperform any individual benchmark mixture, underscoring the importance of a large benchmark pool and query comprehensiveness.

MixEval outperforms both benchmark-level and uniform mixtures.Figure 5 illustrates the correlations with Arena Elo for benchmark-level and uniform mixtures. The benchmark-level mixture samples questions uniformly from each benchmark, proportional to its split size in MixEval. The uniform mixture samples an equal number of questions from all benchmarks. Both methods yield significantly lower human preference correlations than MixEval and MixEval-Hard. Furthermore, the benchmark-level mixture offers negligible improvement over the uniform mixture. These findings underscore the importance of an appropriate sample-level mixture, as implemented by MixEval.

MixEval effectively maps real-world user queries to ground-truth-based benchmarks.Figure 2 shows the query distributions of leading benchmarks. Both MixEval and MixEval-Hard closely resemble web queries and popular wild datasets, highlighting MixEval's efficacy in aligning benchmark query distributions with real-world data. The maps in Figure 2 are ordered by their cluster distances to our identified web queries, showing that wild datasets align more closely with our web queries than other LLM benchmarks. This underscores the robustness of our web query detection pipeline and the solid grounding of MixEval. Additionally, as discussed in Section 2, ShareGPT, with a larger user base (100M) compared to other wild datasets (0.1M-0.2M), shows the highest similarity to our web queries, which are based on a global internet user population (5.4B), further validating the accuracy of our web query detection.

### Evaluation Results

LeaderboardTable 3 (Section G) presents the detailed evaluation results on MixEval, MixEval-Hard, and their main subsets. Claude 3 Opus and GPT-4 Turbo consistently achieve

Figure 5: Our approach improves the correlation with Arena Elo and Arena Elo (En) (Figure 12) for all the main splits of MixEval and outperforms benchmark-level and uniform mixture.

the highest performance across nearly all splits, except for BoolQ-Mixed. Gemini 1.5 Pro ranks third on both MixEval and MixEval-Hard, followed closely by Claude 3 Sonnet, LLaMA-3-70B-Instruct, and Reka Core, with similar scores. Notably, all these frontier models, except LLaMA-3-70B-Instruct, support multi-modal input understanding. The LLaMA-3-8B-Instruct model is the top-performing 7B model, outperforming some of the latest large models, such as Command R (35B) and Qwen1.5-32B-Chat (32B). Proprietary models generally outperform open-source models.

**Cost-effectiveness** Figure 6 compares the models in Table 3 in terms of cost-effectiveness. Figure 5(a) examines the relationship between activated parameters and performance for open-source LLMs, while Figure 5(b) compares API price against performance for frontier proprietary LLMs. Both figures exhibit a roughly log-linear relationship between performance and the x-axis metric. In Figure 5(a), the Llama-3 series stands out as the most performant and parameter-efficient among open-source models. The MoE models, such as Mixtral-8x7B-Instruct-v0.1, Qwen1.5-MoE-A2.7B-Chat, and JetMoE-8B-Chat, demonstrate superior parameter efficiency. The proprietary data points reveal a clearer log-linear pattern. GPT-4 Turbo is more cost-effective than Claude 3 Opus, offering comparable performance at less than half the price. The Gemini series exhibits similar cost-effectiveness to the GPT series, while the Reka series parallels the cost-effectiveness of the Claude series. We conduct detailed error analysis in Section H to compare error rates of open-source and proprietary models on different MixEval splits. We also showcase error responses of frontier models in Section H.1 to identify their potential weaknesses.

Figure 6: Activated parameters and API price per performance of open-source and proprietary models.

Figure 7: The performance of chat and base models of the same model series in Table 3. Chat and base model scores show a high correlation.

### Can We Approximate the Human Preferences for Base Models?

The crowdsourced evaluation of LLMs, based on human preferences, assesses two main aspects: (1) model capability, optimized mainly during pre-training, and (2) non-capability attributes like toxicity and helpfulness, refined during post-training. We explore whether human preferences for models can be predicted before post-training, leading to the question: which stage, pre-training or post-training, more significantly influences human preferences in crowdsourced LLM evaluations? We evaluated the base versions of the model series in Table 3. Notably, the correlations in Figure 7 show a 0.95 correlation between base and chat models, indicating MixEval's potential to approximate human preferences pre-post-training. This implies that pre-training may have a greater impact on crowdsourced LLM evaluations, with post-training minimally altering human preference rankings. However, we also observe that the post-training has more impact on some smaller models, all of which went through heavy supervised post-training.

### What Affects the Correlations with Human Preference?

**Comprehensiveness and other features, such as difficulty, impact correlation.** As shown in Figure 10, general-domain benchmarks typically exhibit a higher correlation with human preference compared to domain-specific benchmarks, highlighting the importance of query comprehensiveness. However, comprehensiveness is not the sole factor. Three observations support this: (1) Benchmarks like GSM8K, despite their skewed distributions (Figure 2), achieve a high correlation (0.78) with human preference, while others with high topic overlap with real-world queries, such as BoolQ, achieve a low correlation (0.37). (2) ARC-e and ARC-c, despite similar topic distributions, show significantly different correlations (Figure 10), likely due to varying difficulty levels. This indicates that other query features, such as difficulty, are critical to correlation with human preference. (3) As shown in Figure 5, MixEval increases the correlation for each individual benchmark through benchmark mixture. For an individual benchmark, the topic becomes less comprehensive post-mixture since the mixed version represents a subset of the original; thus, the correlation gain is not due to a more comprehensive topic distribution. These observations suggest that correlation gains with human preference are influenced by factors beyond solely comprehensiveness, possibly including query difficulty and topic density. Section F presents the full correlation matrix and analyzes other factors affecting benchmark correlations.

## 5 Conclusion

In this paper, we present MixEval, an approach that bridges real-world queries and ground-truth-based evaluation by mining user queries from the web and matching them with similar benchmark queries. MixEval and its hard variant can offer accurate evaluations that highly align with Chatbot Arena. MixEval operates locally and rapidly, eliminating the need for slow, costly human preference data collection or biased model judgment. MixEval's data points can be stably updated within one minute, mitigating benchmark contamination. We thereby effectively mitigate the query, grading, and generalization biases in LLM evaluation through the proposed benchmark mixture pipeline, while maintaining high efficiency. Our meta-evaluation and extensive analysis of MixEval and other popular LLM benchmarks demonstrate MixEval's effectiveness, providing insights to enhance the community's understanding of LLM evaluation.