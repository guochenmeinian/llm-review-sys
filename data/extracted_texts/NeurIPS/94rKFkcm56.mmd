# Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power

Junru Zhou\({}^{1}\) Jiarui Feng\({}^{2}\) Xiyuan Wang\({}^{1}\) Muhan Zhang\({}^{1}\)

\({}^{1}\)Institute for Artificial Intelligence, Peking University

\({}^{2}\)Department of CSE, Washington University in St. Louis

zml72062@stu.pku.edu.cn, feng.jiarui@wustl.edu,

{wangxiyuan,muhan}@pku.edu.cn

###### Abstract

The ability of graph neural networks (GNNs) to count certain graph substructures, especially cycles, is important for the success of GNNs on a wide range of tasks. It has been recently used as a popular metric for evaluating the expressive power of GNNs. Many of the proposed GNN models with provable cycle counting power are based on subgraph GNNs, i.e., extracting a bag of subgraphs from the input graph, generating representations for each subgraph, and using them to augment the representation of the input graph. However, those methods require heavy preprocessing, and suffer from high time and memory costs. In this paper, we overcome the aforementioned limitations of subgraph GNNs by proposing a novel class of GNNs--\(d\)-Distance-Restricted FWL(2) GNNs, or \(d\)-DRFWL(2) GNNs, based on the well-known FWL(2) algorithm. As a heuristic method for graph isomorphism testing, FWL(2) colors all node pairs in a graph and performs message passing among those node pairs. In order to balance the expressive power and complexity, \(d\)-DRFWL(2) GNNs simplify FWL(2) by restricting the range of message passing to node pairs whose mutual distances are at most \(d\). This way, \(d\)-DRFWL(2) GNNs exploit graph sparsity while avoiding the expensive subgraph extraction operations in subgraph GNNs, making both the time and space complexity lower. We theoretically investigate both the discriminative power and the cycle counting power of \(d\)-DRFWL(2) GNNs. Our most important finding is that \(d\)-DRFWL(2) GNNs have provably strong cycle counting power even with \(d=2\): they can count all 3, 4, 5, 6-cycles. Since 6-cycles (e.g., benzene rings) are ubiquitous in organic molecules, being able to detect and count them is crucial for achieving robust and generalizable performance on molecular tasks. Experiments on both synthetic datasets and molecular datasets verify our theory. To the best of our knowledge, 2-DRFWL(2) GNN is the most efficient GNN model to date (both theoretically and empirically) that can count up to 6-cycles.

## 1 Introduction

Graphs are important data structures suitable for representing relational or structural data. As a powerful tool to learn node-level, link-level or graph-level representations for graph-structured data, graph neural networks (GNNs) have achieved remarkable successes on a wide range of tasks . Among the various GNN models, Message Passing Neural Networks (MPNNs)  are a widely adopted class of GNNs. However, the expressive power of MPNNs has been shown to be upper-bounded by the Weisfeiler-Leman test . More importantly, MPNNs even fail to detect some simple substructures (e.g., 3, 4-cycles) , thus losing great structural information in graphs.

The weak expressive power of MPNNs has aroused a search for more powerful GNN models. To evaluate the expressive power of these GNN models, there are usually two perspectives. One is to characterize their discriminative power, i.e., the ability to distinguish between non-isomorphic graphs. It is shown in  that an equivalence exists between distinguishing all non-isomorphic graph pairs and approximating any permutation-invariant functions on graphs. Although the discriminative power partially reveals the function approximation ability of a GNN model, it fails to tell _what specific functions_ a model is able to approximate. Another perspective is to directly characterize _what specific function classes_ a model can approximate. In particular, we can study the expressive power of a GNN model by asking _what graph substructures it can count_. This perspective is often more practical since the task of counting substructures (especially cycles) is closely related to a variety of domains such as chemistry , biology , and social network analysis .

Despite the importance of counting cycles, the task gets increasingly difficult as the length of cycle increases .To have an intuitive understanding of this difficulty, we first discuss why MPNNs fail to count any cycles. The reason is clear: MPNNs only keep track of the rooted subtree **around each node**, and without node identity, any neighboring node \(v\) of \(u\) has no idea which of its neighbors are also neighbors of \(u\). Therefore, MPNNs cannot count even the simplest 3-cycles. In contrast, the 2-dimensional Folklore Weisfeiler-Leman, or FWL(2) test , uses _2-tuples of nodes_ instead of nodes as the basic units for message passing, and thus natually **encodes closed walks**. For example, when a 2-tuple \((u,v)\) receives messages from 2-tuples \((w,v)\) and \((u,w)\) in FWL(2), \((u,v)\) gets **aware of the walk**\(u w v u\), whose length is \(d(u,w)+d(w,v)+d(u,v)=:l\). As long as the intermediate nodes do not overlap, we immediately get an \(l\)-cycle passing both \(u\) and \(v\). In fact, FWL(2) can provably count up to 7-cycles .

However, FWL(2) has a space and time complexity of \(O(n^{2})\) and \(O(n^{3})\), which makes it impractical for large graphs and limits its real-world applications. Inspired by the above discussion, we naturally raise a question: **to what extent can we retain the cycle counting power of FWL(2) while reducing the time and space complexity?** Answering this question requires another observation: **cycle counting tasks are intrinsically _local_**. For example, the number of 6-cycles that pass a given node \(u\) has nothing to do with the nodes that have a shortest-path distance \( 4\) to \(u\). Therefore, if we only record the embeddings of _node pairs with mutual distance_\( d\) in FWL(2), where \(d\) is a fixed positive integer, then the ability of FWL(2) to count substructures with diameter \( d\) should be retained. Moreover, given that most real-world graphs are sufficiently sparse, this simplified algorithm runs with complexities much lower than FWL(2), since we only need to store and update a small portion of the \(n^{2}\) embeddings for all 2-tuples. We call this simplified version _d_**-Distance-Restricted FWL(2)**, or _d_**-DRFWL(2)**, reflecting that only 2-tuples with _restricted_ distances get updated.

**Main contributions.** Our main contributions are summarized as follows:

1. We propose \(d\)-DRFWL(2) tests as well as their neural versions, \(d\)-DRFWL(2) GNNs. We study how the hyperparameter \(d\) affects the discriminative power of \(d\)-DRFWL(2), and show that a **strict expressiveness hierarchy** exists for \(d\)-DRFWL(2) GNNs with increasing \(d\), both theoretically and experimentally.
2. We study the cycle counting power of \(d\)-DRFWL(2) GNNs. Our major results include: * 2-DRFWL(2) GNNs can already count up to 6-cycles, covering common structures like benzene rings in organic chemistry. The time and space complexities of 2-DRFWL(2) are \(O(n\ ^{4})\) and \(O(n\ ^{2})\) respectively, making it the **most efficient one to date** among other models with 6-cycle counting power such as I\({}^{2}\)-GNN . * With \(d 3\), \(d\)-DRFWL(2) GNNs can count up to 7-cycles, **fully retaining** the cycle counting power of FWL(2) but with complexities **strictly lower** than FWL(2). This finding also confirms the existence of GNNs with the same cycle counting power as FWL(2), but strictly weaker discriminative power.
3. We compare the performance and empirical efficiency of \(d\)-DRFWL(2) GNNs (especially for \(d=2\)) with other state-of-the-art GNNs on both synthetic and real-world datasets. The results verify our theory on the counting power of \(d\)-DRFWL(2) GNNs. Additionally, for the case of \(d=2\), the amount of GPU memory required for training 2-DRFWL(2) GNNs is greatly reduced compared with subgraph GNNs like ID-GNN , NGNN  and I\({}^{2}\)-GNN ; the preprocessing time and training time of 2-DRFWL(2) GNNs are also much less than I\({}^{2}\)-GNN.

Preliminaries

### Notations

For a simple, undirected graph \(G\), we use \(_{G}\) and \(_{G}\) to denote its node set and edge set respectively. For every node \(v_{G}\), we define its _\(k\)-th hop neighbors_ as \(_{k}(v)=\{u_{G}:d(u,v)=k\}\), where \(d(u,v)\) is the shortest-path distance between nodes \(u\) and \(v\). We further introduce the symbols \(_{ k}(v)=_{i=1}^{k}_{i}(v)\) and \((v)=_{1}(v)\). For any \(n\), we denote \([n]=\{1,2,,n\}\).

An \(\)-cycle \(( 3)\) in the (simple, undirected) graph \(G\) is a sequence of \(\) edges \(\{v_{1},v_{2}\},\{v_{2},v_{3}\},,\{v_{},v_{1}\}_{G}\) with \(v_{i} v_{j}\) for any \(i j\) and \(i,j[]\). The \(\)-cycle is said to pass a node \(v\) if \(v\) is among the nodes \(\{v_{i}:i[]\}\). An \(\)_-path_ is a sequence of \(\) edges \(\{v_{1},v_{2}\},\{v_{2},v_{3}\},,\{v_{},v_{+1}\}_{G}\) with \(v_{i} v_{j}\) for any \(i j\) and \(i,j[+1]\), and it is said to start a node \(v_{1}\) and add an node \(v_{+1}\). An _\(\)-walk_ from \(v_{1}\) to \(v_{+1}\) is a sequence of \(\) edges \(\{v_{1},v_{2}\},\{v_{2},v_{3}\},,\{v_{},v_{+1}\}_{G}\) but the nodes \(v_{1},v_{2},,v_{+1}\) can coincide. An \(\)-_clique_ is \(\) nodes \(v_{1},,v_{}\) such that \(\{v_{i},v_{j}\}_{G}\) for all \(i j\) and \(i,j[]\).

Let \(\) be the set of all simple, undirected graphs. If \(S\) is a graph substructure on \(\), and \(G\) is a graph, we use \(C(S,G)\) to denote the number of inequivalent substructures \(S\) that occur as subgraphs of \(G\). Similarly, if \(u\) is a node of \(G\), we use \(C(S,u,G)\) to denote the number of inequivalent substructures \(S\) that pass node \(u\) and occur as subgraphs of \(G\).

Following [15; 33], we give the definition of _whether a function class \(\) on graphs can count a certain substructure \(S\)_.

**Definition 2.1** (Graph-level count).: Let \(_{}\) be a function class on \(\), i.e., \(f_{}:\) for all \(f_{}_{}\). \(_{}\) is said to be able to _graph-level count_ a substructure \(S\) on \(\) if for \( G_{1},G_{2}\) such that \(C(S,G_{1}) C(S,G_{2})\), there exists \(f_{}_{}\) such that \(f_{}(G_{1}) f_{}(G_{2})\).

**Definition 2.2** (Node-level count).: Let \(=\{(G,u):G,u_{G}\}\). Let \(_{}\) be a function class on \(\), i.e., \(f_{}:\) for all \(f_{}_{}\). \(_{}\) is said to be able to _node-level count_ a substructure \(S\) on \(\) if for \((G_{1},u_{1}),(G_{2},u_{2})\) such that \(C(S,u_{1},G_{1}) C(S,u_{2},G_{2})\), there exists \(f_{}_{}\) such that \(f_{}(G_{1},u_{1}) f_{}(G_{2},u_{2})\).

Notice that \(C(S,G)\) can be calculated by \(_{u_{G}}C(S,u,G)\) divided by a factor only depending on \(S\), for any given substructure \(S\). (For example, for triangles the factor is 3.) Therefore, counting a substructure at node level is harder than counting it at graph level.

### FWL(\(k\)) graph isomorphism tests

WL(1) test.The 1-dimensional Weisfeiler-Leman test, or WL(1) test, is a heuristic algorithm for the graph isomorphism problem . For a graph \(G\), the WL(1) test iteratively assigns a color \(W(v)\) to every node \(v_{G}\). At the 0-th iteration, the color \(W^{(0)}(v)\) is identical for every node \(v\). At the \(t\)-th iteration with \(t 1\),

\[W^{(t)}(v)=^{(t)}(W^{(t-1)}(v),^{(t)}( \{\!\!\{W^{(t-1)}(u):u(v)\}\!\!\})), \]

where \(^{(t)}\) and \(^{(t)}\) are injective hashing functions, and \(\{\!\!\{\}\!\}\) means a multiset (set with potentially identical elements). The algorithm stops when the node colors become stable, i.e., \( v,u_{G},W^{(t+1)}(v)=W^{(t+1)}(v) W^{(t) }(v)=W^{(t)}(u)\). We denote the _stable coloring_ of node \(v\) as \(W^{()}(v)\); then the representation for graph \(G\) is

\[W(G)=(\{\!\!\{W^{()}(v):v_{G}\} \!\!\}), \]

where \(\) is an arbitrary injective multiset function.

FWL(\(k\)) tests.For \(k 2\), the \(k\)-dimensional Folklore Weisfeiler-Leman tests, or FWL(\(k\)) tests, define a hierarchy of algorithms for graph isomorphism testing, as described in [12; 32; 42]. For a graph \(G\), the FWL(\(k\)) test assigns a color \(W()\) for every \(k\)-_tuple_\(=(v_{1},,v_{k})_{G}^{k}\). At the 0-th iteration, the color \(W^{(0)}()\) is the _atomic type_ of \(\), denoted as \(()\). If we denote the subgraphs of \(G\) induced by \(\) and \(^{}\) as \(G()\) and \(G(^{})\) respectively, then the function \(()\) can be any function on \(_{G}^{k}\) that satisfies the following condition: \(()=(^{})\) iff the mapping \(v_{i} v_{i}^{},i[k]\) (i.e., the mapping that maps each \(v_{i}\) to its corresponding \(v_{i}^{}\), for \(i[k]\)) induces an isomorphism from \(G()\) to \(G(^{})\).

At the \(t\)-th iteration with \(t 1\), \((k)\) updates the color of \(_{G}^{k}\) as

\[W^{(t)}()=^{(t)}(W^{(t-1)}(),^{(t)}(\{\!\!(W^{(t-1)},,w):w _{G}\!\!\!\})), \]

where \((f,,w)\) is defined as

\[(f,,w)=(f([1 w]),f([2 w] ),,f([k w])). \]

Here we use \([j w]\) to denote \((v_{1},,v_{j-1},w,v_{j+1},,v_{k})\) for \(j[k]\) and \(w_{G}\).

\(^{(t)}\) and \(^{(t)}\) are again injective hashing functions. The algorithm stops when all \(k\)-tuples receive stable colorings. The stable coloring of \(_{G}^{k}\) is denoted as \(W^{()}()\). We then calculate the representation for graph \(G\) as

\[W(G)=(\{\!\!W^{()}(): _{G}^{k}\!\!\!\}), \]

where \(\) is an arbitrary injective multiset function.

## 3 \(d\)-Distance-Restricted FWL(2) GNNs

In this section, we propose the \(d\)-Distance Restricted FWL(2) tests/GNNs. They use 2-tuples like \((2)\), but restrict the distance between nodes in each 2-tuple to be \( d\), which effectively reduces the number of 2-tuples to store and aggregate while still retaining great cycle counting power.

### \(d\)-Drfwl(2) tests

We call a 2-tuple \((u,v)_{G}^{2}\) a **distance-\(k\) tuple** if the shortest-path distance between \(u\) and \(v\) is \(k\) in the following. \(d\)**-Distance Restricted FWL(2) tests**, or \(d\)**-DRFWL(2) tests**, assign a color \(W(u,v)\) for _every distance-\(k\) tuple_\((u,v)\)_with_\(0 k d\). Initially, the color \(W^{(0)}(u,v)\) only depends on \(d(u,v)\). For the \(t\)-th iteration with \(t 1\), \(d\)-DRFWL(2) updates the colors using the following rule,

\[W^{(t)}(u,v)=_{k}^{(t)}(W^{(t-1)}(u,v),(M_{ij}^{k(t)}( u,v))_{0 i,j d}),d(u,v)=k, \]

where \(_{k}^{(t)}\) is an injective hashing function for distance \(k\) and iteration \(t\), and \(M_{ij}^{k(t)}(u,v)\) is defined as

\[M_{ij}^{k(t)}(u,v)=_{ij}^{k(t)}(\{\!\!(W^{(t-1)}( w,v),W^{(t-1)}(u,w)):w_{i}(u)_{j}(v)\!\!\!\} ). \]

The symbol \((M_{ij}^{k(t)}(u,v))_{0 i,j d}\) stands for \((M_{00}^{k(t)}(u,v),M_{01}^{k(t)}(u,v),,M_{0d}^{k(t)}(u,v),,\)\(M_{dd}^{k(t)}(u,v))\). Each of the \(_{ij}^{k(t)}\) with \(0 i,j,k d\) is an injective multiset hashing function. Briefly speaking, the rules (6) and (7) **update the color of a distance-\(k\) tuple \((u,v)\) using colors of distance-\(i\) and distance-\(j\) tuples**.

When every distance-\(k\) tuple \((u,v)\) with \(0 k d\) receives its stable coloring, denoted as \(W^{()}(u,v)\), the representation of \(G\) is calculated as

\[W(G)=(\{\!\!W^{()}(u,v):(u,v) _{G}^{2}0 d(u,v) d\!\!\}). \]Figure 1 illustrates how 2-DRFWL(2) updates the color of a distance-2 tuple \((u,v)\). Since only distance-\(k\) tuples with \(0 k 2\) are colored in 2-DRFWL(2) tests, there are 7 terms of the form \(((W(w,v),W(u,w))\) (with \(w\{u,v,x,y,z,t,r\}\)) contributing to the update of \(W(u,v)\). The 7 nodes \(u,v,x,y,z,t,r\) are filled with different colors, according to their distances to \(u\) and to \(v\). For example, the _violet_ node \(u\) has distance 0 to \(u\) and distance 2 to \(v\), thus contributing to \(M_{02}^{2}(u,v)\); the _green_ nodes \(x\) and \(y\) have distance 1 to either \(u\) or \(v\), thus contributing to \(M_{1}^{2}(u,v)\). Analogously, nodes with _red, blue, orange_ and pink colors contribute to \(M_{20}^{2}(u,v),M_{12}^{2}(u,v),M_{21}^{2}(u,v)\) and \(M_{22}^{2}(u,v)\), respectively. Finally, the uncolored nodes \(q\) and \(s\) do not contribute to the update of \(W(u,v)\), since they have distance 3 (which is greater than 2) to \(u\). From the figure, we can observe that by redefining neighbors and sparsifying 2-tuples of FWL(2), \(d\)-DRFWL(2) significantly reduces the complexity and focuses only on local structures, especially on sparse graphs.

Now we study the expressive power of \(d\)-DRFWL(2) tests by comparing them with the WL hierarchy. First, we can prove that \(d\)-DRFWL(2) tests are strictly more powerful than WL(1), for every \(d\).

**Theorem 3.1**.: _In terms of the ability to distinguish between non-isomorphic graphs, the \(d\)-DRFWL(2) test is strictly more powerful than WL(1), for any \(d 1\)._

Next, we compare \(d\)-DRFWL(2) tests with FWL(2). Actually, since it is easy for the FWL(2) test to compute distance between every pair of nodes (we can initialize \(W(u,v)\) as 0, 1 and \(\) for \(u=v\), \((u,v)_{G}\) and all other cases, and iteratively update \(W(u,v)\) with \(\{W(u,v),_{w}\{W(u,w)+W(w,v)\}\}\)), the FWL(2) test can use its update rule to simulate (6) and (7) by applying different \(_{k}^{(t)}\) and \(_{ij}^{k(t)}\) functions to different \((i,j,k)\) values. This implies that _d_**-DRFWL(2) tests are at most as powerful as the FWL(2) test**. Actually, this hierarchy in expressiveness is strict, due to the following theorem.

**Theorem 3.2**.: _In terms of the ability to distinguish between non-isomorphic graphs, FWL(2) is strictly more powerful than \(d\)-DRFWL(2), for any \(d 1\). Moreover, \((d+1)\)-DRFWL(2) is strictly more powerful than \(d\)-DRFWL(2)._

The proofs of all theorems within this section are included in Appendix B.

### _d_-DRFWL(2) GNNs

Based on the \(d\)-DRFWL(2) tests, we now propose \(d\)-DRFWL(2) GNNs. Let \(G\) be a graph which can have node features \(f_{v}^{d_{f}},v_{G}\) and (or) edge features \(e_{uv}^{d_{e}},\{u,v\}_{G}\). A _d_**-DRFWL(2) GNN** is defined as a function of the form

\[f=M R L_{T}_{T-1}_{1} L_{1}. \]

The input of \(f\) is the _initial labeling_\(h_{uv}^{(0)},0 d(u,v) d\). Each \(L_{t}\) with \(t=1,2,,T\) in (9) is called a _d_**-DRFWL(2) GNN layer**, which transforms \(h_{uv}^{(t-1)}\) into \(h_{uv}^{(t)}\) using rules

For each

\[k=0,1,,d,\]

\[(u,v)_{G}^{2}d(u,v)=k,\]

\[a_{uv}^{ijk(t)}=_{w_{i}(u)_{j}(v)}m_{ ijk}^{(t)}(h_{uv}^{(t-1)},h_{uw}^{(t-1)}), \]

\[h_{uv}^{(t)}=f_{k}^{(t)}(h_{uv}^{(t-1)},(a_{uv}^{ijk(t)})_{0  i,j d}), \]

where \(m_{ijk}^{(t)}\) and \(f_{k}^{(t)}\) are arbitrary learnable functions; \(\) denotes a permutation-invariant aggregation operator (e.g., sum, mean or max). (10) and (11) are simply counterparts of (7) and (6) with \(_{ij}^{k(t)}\) and \(_{k}^{(t)}\) replaced with continuous functions. \(_{t},t=1,,T-1\) are entry-wise activation functions. \(R\) is a permutation-invariant readout function, whose input is the multiset

Figure 1: Neighbor aggregation in 2-DRFWL(2) for 2-tuple \((u,v)\).

\(\{h^{(T)}_{uv}:(u,v)^{2}_{G}0 d(u,v)  d\}\). Finally, \(M\) is an MLP that acts on the graph representation output by \(R\).

We can prove that (i) the representation power of any \(d\)-DRFWL(2) GNN is upper-bounded by the \(d\)-DRFWL(2) test, and (ii) there exists a \(d\)-DRFWL(2) GNN instance that has equal representation power to the \(d\)-DRFWL(2) test. We leave the formal statement and proof to Appendix B.

## 4 The cycle counting power of \(d\)-DRFWL(2) GNNs

By Theorem 3.2, the expressive power of \(d\)-DRFWL(2) tests (or \(d\)-DRFWL(2) GNNs) strictly increases with \(d\). However, the space and time complexities of \(d\)-DRFWL(2) GNNs also increase with \(d\). On one hand, since there are \(O(n\ ^{k})\) distance-\(k\) tuples in a graph \(G\), at least \(O(n\ ^{k})\) space is necessary to store the representations for all distance-\(k\) tuples. For \(d\)-DRFWL(2) GNNs, this results in **a space complexity of \(O(n\ ^{d})\)**. On the other hand, since there are at most \(O(^{\{i,j\}})\) nodes in \(_{i}(u)_{j}(v)\), \( 0 i,j d\), there are at most \(O(^{d})\) terms at the RHS of (10). Therefore, it takes \(O(d^{2}\ ^{d})\) time to update a single representation vector \(h^{(t)}_{uv}\) using (10) and (11). This implies **the time complexity of \(d\)-DRFWL(2) GNNs is \(O(nd^{2}\ ^{2d})\)**.

For scalability, \(d\)-DRFWL(2) GNNs with a relatively small value of \(d\) are used in practice. **But how to find the \(d\) value that best strikes a balance between expressive power and efficiency?** To answer this question, we need a _practical, quantitative_ metric of expressive power. In the following, we characterize the _cycle counting_ power of \(d\)-DRFWL(2) GNNs. We find that 2-DRFWL(2) GNNs are powerful enough to **node-level count up to 6-cycles**, as well as **many other useful graph substructures**. Since for \(d=2\), the time and space complexities of \(d\)-DRFWL(2) GNNs are \(O(n\ ^{4})\) and \(O(n\ ^{2})\) respectively, our model is **much more efficient** than \(^{2}\)-GNN which requires \(O(n\ ^{5})\) time and \(O(n\ ^{4})\) space to count up to 6-cycles . Moreover, with \(d 3\), \(d\)-DRFWL(2) GNNs are able to **node-level count up to 7-cycles**, already matching the cycle counting power of FWL(2).

Our main results are stated in Theorems 4.3-4.8. Before we present the theorems, we need to give revised definitions of \(C(S,u,G)\) for some certain substructures \(S\). This is because in those substructures not all nodes are structurally equal.

**Definition 4.1**.: If \(S\) is an \(\)-path with \( 2\), \(C(S,u,G)\) is defined to be the number of \(\)-paths in \(G\)_starting from_ node \(u\).

Figure 1(a) illustrates Definition 4.1 for the \(=4\) case.

**Definition 4.2**.: The substructures in Figures 1(b), 1(c) and 1(d) are called _tailed triangles_, _chordal cycles_ and _triangle-rectangles_, respectively. If \(S\) is a tailed triangle (or chordal cycle or triangle-rectangle), \(C(S,u,G)\) is defined to be the number of tailed triangles (or chordal cycles or triangle-rectangles) that occur as subgraphs of \(G\) and include node \(u\) at a position shown in the figures.

Now we state our main theorems. In the following, the definition for "node-level counting" is the same as Definition 2.2, but one should treat \(C(S,u,G)\) differently (following Definitions 4.1 and 4.2) when \(S\) is a path, a tailed triangle, a chordal cycle, or a triangle-rectangle. To define the output of a \(d\)-DRFWL(2) GNN \(f\) on domain \(\), we denote

\[f(G,u)=h^{(T)}_{uu}, u_{G}, \]

Figure 2: Illustrations of node-level counts of certain substructures.

i.e., we treat the embedding of \((u,u)\) as the representation of node \(u\). For 1-DRFWL(2) GNNs, we have

**Theorem 4.3**.: _1-DRFWL(2) GNNs can node-level count 3-cycles, but cannot graph-level count any longer cycles._

For 2-DRFWL(2) GNNs, we investigate not only their cycle counting power, but also their ability to count many other graph substructures. Our results include

**Theorem 4.4**.: _2-DRFWL(2) GNNs can node-level count 2, 3, 4-paths._

**Theorem 4.5**.: _2-DRFWL(2) GNNs can node-level count 3, 4, 5, 6-cycles._

**Theorem 4.6**.: _2-DRFWL(2) GNNs can node-level count tailed triangles, chordal cycles and triangle-rectangles._

**Theorem 4.7**.: _2-DRFWL(2) GNNs cannot graph-level count \(k\)-cycles with \(k 7\) or \(k\)-cliques with \(k 4\)._

For \(d\)-DRFWL(2) GNNs with \(d 3\), we have

**Theorem 4.8**.: _For any \(d 3\), \(d\)-DRFWL(2) GNNs can node-level count 3, 4, 5, 6, 7-cycles, but cannot graph-level count any longer cycles._

The proofs of all theorems within this section are included in Appendix C. To give an intuitive explanation for the cycle counting power of \(d\)-DRFWL(2) GNNs, let us consider, e.g., why 2-DRFWL(2) GNNs can count up to 6-cycles. The key reason is that **they allow a distance-2 tuple \((u,v)\) to receive messages from other two distance-2 tuples \((u,w)\) and \((w,v)\)**, and are thus aware of closed 6-walks (since \(6=2+2+2\)). Indeed, if we forbid such kind of message passing, the modified 2-DRFWL(2) GNNs can no longer count 6-cycles, as experimentally verified in Appendix F.2.

## 5 Related works

**The cycle counting power of GNNs.** It is proposed in  to use GNNs' ability to count given-length cycles as a metric for their expressiveness. Prior to this work, Arvind et al.  and Furer  have discussed the cycle counting power of the FWL(2) test: FWL(2) can and only can graph-level count up to 7-cycles. Huang et al.  also characterizes the node-level cycle counting power of subgraph MPNNs and I\({}^{2}\)-GNN. Apart from counting cycles, there are also some works analyzing the general substructure counting power of GNNs. Chen et al.  discusses the ability of WL(\(k\)) tests to count general subgraphs or induced subgraphs, but the result is loose. Tahmasebi et al.  analyzes the substructure counting power of Recursive Neighborhood Pooling, which can be seen as a subgraph GNN with recursive subgraph extraction procedures.

**The trade-off between expressive power and efficiency of GNNs.** Numerous methods have been proposed to boost the expressive power of MPNNs. Many of the provably powerful GNN models have direct correspondence to the Weisfeiler-Leman hierarchy [12; 28], such as higher-order GNNs  and IGNs [41; 42; 43]. Despite their simplicity in theory, those models require \(O(n^{k+1})\) time and \(O(n^{k})\) space in order to achieve equal expressive power to FWL(\(k\)) tests, and thus do not scale to large graphs even for \(k=2\).

Recent works try to strike a balance between the expressive power of GNNs and their efficiency. Among the state-of-the-art GNN models with _sub-\(O(n^{3})\)_ time complexity, subgraph GNNs have gained much research interest [8; 17; 24; 33; 39; 46; 57; 58; 59; 60]. Subgraph GNNs process a graph \(G\) by 1) extracting a bag of subgraphs \(\{G_{i}:i=1,2,,p\}\) from \(G\), 2) generating representations for every subgraph \(G_{i},i=1,2,,p\) (often using a weak GNN such as MPNN), and 3) combining the representations of all subgraphs into a representation of \(G\). Most commonly, the number \(p\) of subgraphs extracted is equal to the number of nodes \(n\) (called a _node-based subgraph extraction policy_). In this case, the time complexity of subgraph GNNs is upper-bounded by \(O(nm)\), where \(m\) is the number of edges. If we further adopt the _K-hop ego-network policy_, i.e., extracting a \(K\)-hop subgraph \(G_{u}\) around each node \(u\), the time complexity becomes \(O(n\ ^{K+1})\). Frasca et al.  and Zhang et al.  theoretically characterize the expressive power of subgraph GNNs, and prove that subgraph GNNs with node-based subgraph extraction policies lie strictly between WL(1) and FWL(2) in the Weisfeiler-Leman hierarchy. Despite the lower complexity, in practice subgraph GNNs still suffer from heavy preprocessing and a high GPU memory usage.

Apart from subgraph GNNs, there are also attempts to add sparsity to higher-order GNNs. Morris et al.  proposes \(\)-\(k\)-LWL, a variant of the WL(\(k\)) test that updates a \(k\)-tuple \(\) only from the \(k\)-tuples with a component _connected_ to the corresponding component in \(\). Zhang et al.  proposes LFWL(2) and SLFWL(2), which are variants of FWL(2) that update a 2-tuple \((u,v)\) from nodes in either \((v)\) or \((u)(v)\). Our model can also be classified into this type of approaches, yet we not only sparsify neighbors of a 2-tuple, but also 2-tuples used in message passing, which results in much lower space complexity. A detailed comparison between our method and LFWL(2)/SLFWL(2) is included in Appendix D.

## 6 Experiments

In this section, we empirically evaluate the performance of \(d\)-DRFWL(2) GNNs (especially for the case of \(d=2\)) and verify our theoretical results. To be specific, we focus on the following questions:

**Q1:**: Can \(d\)-DRFWL(2) GNNs reach their theoretical counting power as stated in Theorems 4.3-4.8?
**Q2:**: How do \(d\)-DRFWL(2) GNNs perform compared with other state-of-the-art GNN models on open benchmarks for graphs?
**Q3:**: What are the time and memory costs of \(d\)-DRFWL(2) GNNs on various datasets?
**Q4:**: Do \(d\)-DRFWL(2) GNNs with increasing \(d\) values construct a hierarchy in discriminative power (as shown in Theorem 3.2)? Further, does this hierarchy lie between WL(1) and FWL(2) empirically?

We answer **Q1**-**Q3** in 6.1-6.3, as well as in Appendix F. The answer to **Q4** is included in Appendix F.1. The details of our implementation of \(d\)-DRFWL(2) GNNs, along with the experimental settings, are included in Appendix E. Our code for all experiments, including those in Section 6 of the main paper and in Appendix F, is available at [https://github.com/zml72062/DR-FWL-2](https://github.com/zml72062/DR-FWL-2).

### Substructure counting

Datasets.To answer **Q1**, we perform node-level substructure counting tasks on the synthetic dataset in . The synthetic dataset contains 5,000 random graphs, and the training/validation/test splitting is 0.3/0.2/0.5. The task is to perform regression on the node-level counts of certain substructures. Normalized MAE is used as the evaluation metric.

Tasks and baselines.To verify Theorems 4.4-4.6, we use 2-DRFWL(2) GNN to perform node-level counting task on 9 different substructures: 3-cycles, 4-cycles, 5-cycles, 6-cycles, tailed triangles, chordal cycles, 4-cliques, 4-paths and triangle-rectangles. We choose MPNN, node-based subgraph GNNs (ID-GNN, NGNN, GNNAK+), PPGN, and I\({}^{2}\)-GNN as our baselines. Results for all baselines are from .

To verify Theorem 4.8, we compare the performances of 2-DRFWL(2) GNN and 3-DRFWL(2) GNN on node-level cycle counting tasks, with the cycle length ranging from 3 to 7.

We also conduct ablation studies to investigate _what kinds of message passing are essential_ (among those depicted in Figure 1) for 2-DRFWL(2) GNNs to successfully count up to 6-cycles and other substructures. The experimental details and results are given in Appendix F.2. The studies also serve as a verification of Theorem 4.3.

    &  \\   & 3-Cyc. & 4-Cyc. & 5-Cyc. & 6-Cyc. & Tail. Tri. & Chor. Cyc. & 4-Cliq. & 4-Path & Tri.-Rect. \\  MPNN & 0.3515 & 0.2742 & 0.2088 & 0.1555 & 0.3631 & 0.3114 & 0.1645 & 0.1592 & 0.2979 \\ ID-GNN & 0.0006 & 0.0022 & 0.0490 & 0.0495 & 0.1053 & 0.0454 & 0.0026 & 0.0273 & 0.0628 \\ NGNN & 0.0003 & 0.0013 & 0.0402 & 0.0439 & 0.1044 & 0.0392 & 0.0045 & 0.0244 & 0.0729 \\ GNNK+ & 0.0004 & 0.0041 & 0.0133 & 0.0238 & 0.0043 & 0.0112 & 0.0049 & 0.0075 & 0.1311 \\ PPGN & 0.0003 & 0.0009 & 0.0036 & 0.0071 & 0.0026 & 0.0015 & 0.1646 & 0.0041 & 0.0144 \\ I\({}^{2}\)-GNN & 0.0003 & 0.0016 & 0.0028 & 0.0082 & 0.0011 & 0.0010 & 0.0003 & 0.0041 & 0.0013 \\ 
2-DRFWL(2) GNN & 0.0004 & 0.0015 & 0.0034 & 0.0087 & 0.0030 & 0.0026 & 0.0009 & 0.0081 & 0.0070 \\   

Table 1: Normalized MAE results of node-level counting cycles and other substructures on synthetic dataset. The colored cell means an error less than 0.01.

Results.From Table 1, we see that 2-DRFWL(2) GNN achieves less-than-0.01 normalized MAE on all 3, 4, 5 and 6-cycles, verifying Theorem 4.5; 2-DRFWL(2) GNN also achieves less-than-0.01 normalized MAE on tailed triangles, chordal cycles, 4-paths and triangle-rectangles, verifying Theorems 4.4 and 4.6.

It is interesting that 2-DRFWL(2) GNN has a very good performance on the task of node-level counting 4-cliques, which by Theorem 4.7 it cannot count in theory. A similar phenomenon happens for subgraph GNNs. This may be because 2-DRFWL(2) GNN and subgraph GNNs still learn some local structural biases that have strong correlation with the number of 4-cliques.

Finally, from the last row of Table 7 we see that 1-DRFWL(2) GNN achieves less-than-0.01 normalized MAE on 3-cycles, but performs badly on 4, 5 and 6-cycles. This result verifies Theorem 4.3.

### Molecular property prediction

Datasets.To answer **Q2**, we evaluate the performance of \(d\)-DRFWL(2) GNNs on four popular molecular graph datasets--QM9, ZINC, ogbg-molhiv and ogbg-molpcba. QM9 contains 130k small molecules, and the task is regression on 12 targets. One can refer to the page for the meaning of those 12 targets. ZINC , including a smaller version (ZINC-12K) and a full version (ZINC-250K), is a dataset of chemical compounds and the task is graph regression. The ogbg-molhiv (containing 41k molecules) and ogbg-molpcba (containing 438k molecules) datasets belong to the Open Graph Benchmark (OGB) ; the task on both datasets is binary classification. Details of the four datasets are given in Appendix E.2.2.

Baselines.For QM9, the baselines are chosen as 1-GNN, 1-2-3-GNN , DTNN , Deep LRP , PPGN, NGNN  and I\({}^{2}\)-GNN . Methods  utilizing geometric features or quantum mechanic theory are omitted to fairly compare the graph representation power of the models. For ZINC and ogbg-molhiv, we adopt GIN, PNA , DGN , HIMP , GSN , Deep LRP , CIN , NGNN, GNNAK+, SUN  and I\({}^{2}\)-GNN as our baselines. For ogbg-molpcba, the baselines are GIN, PNA, DGN, NGNN and GNNAK+. The experimental details are given in Appendix E.2.2. We leave the results on ZINC, ogbg-molhiv and ogbg-molpcba to Appendix F.3.

Results.On QM9, Table 3 shows that 2-DRFWL(2) GNN attains top two results on most (11 out of 12) targets. Moreover, 2-DRFWL(2) GNN shows a good performance on targets \(U_{0},U,H\) and \(G\), where subgraph GNNs like NGNN or I\({}^{2}\)-GNN have a poor performance. The latter fact actually reveals that our method has a stronger ability to capture _long-range interactions_ on graphs than subgraph GNNs, since the targets \(U_{0},U,H\) and \(G\) are macroscopic thermodynamic properties of

    &  \\   & 3-Cyc. & 4-Cyc. & 5-Cyc. & 6-Cyc. & 7-Cyc. \\ 
2-DRFWL(2) GNN & **0.0004** & **0.0015** & **0.0034** & **0.0087** & 0.0362 \\
3-DRFWL(2) GNN & 0.0006 & 0.0020 & 0.0047 & 0.0099 & **0.0176** \\   

Table 2: Normalized MAE results of node-level counting \(k\)-cycles \((3 k 7)\) on synthetic dataset.

   Target & 1-GNN & 1-2-3-GNN & DTNN & Deep LRP & PPGN & NGNN & I\({}^{2}\)-GNN & 2-DRFWL(2) GNN \\  \(\) & 0.493 & 0.476 & **0.244** & 0.364 & **0.231** & 0.428 & 0.428 & 0.346 \\ \(\) & 0.78 & 0.27 & 0.95 & 0.298 & 0.382 & 0.29 & **0.230** & **0.222** \\ \(c_{}\) & 0.00321 & 0.0037 & 0.00388 & **0.00254** & 0.00276 & 0.00265 & 0.00261 & **0.00226** \\ \(_molecules and heavily depend on such long-range interactions. We leave the detailed analysis to Appendix F4. Apart from QM9, Table 8 of Appendix F.3 shows that \(d\)-DRFWL(2) GNN outperforms CIN  on ZINC-12K; the performance on ogbg-molhiv is also comparable to baseline methods. These results show that although designed for cycle counting, \(d\)-DRFWL(2) GNN is also highly competitive on general molecular tasks.

It is interesting to notice that \(d\)-DRFWL(2) GNN shows an inferior performance on ogbg-molpcba, compared with baseline methods. We conjecture that the results on ogbg-molpcba might be insensitive to the gain in cycle counting power, and might prefer simple model architectures rather than highly expressive ones.

### Empirical efficiency

To answer **Q3**, we compare the time and memory costs of 2-DRFWL(2) GNN with MPNN, NGNN and I\({}^{2}\)-GNN on two datasets--QM9 and ogbg-molhiv. We use three metrics to evaluate the empirical efficiency of 2-DRFWL(2) GNNs: the maximal GPU memory usage during training, the preprocessing time, and the training time per epoch.

To make a fair comparison, we fix the number of 2-DRFWL(2) GNN layers and the number of message passing layers in all baseline methods to 5; we also fix the size of hidden dimension to 64 for QM9 and 300 for ogbg-molhiv. The subgraph heights for NGNN and I\({}^{2}\)-GNN are both 3. The batch size is always 64.

Results.From Table 4, we see that the preprocessing time of 2-DRFWL(2) GNN is much shorter than subgraph GNNs like NGNN or I\({}^{2}\)-GNN. Moreover, 2-DRFWL(2) GNN requires much less GPU memory while training, compared with subgraph GNNs. The training time of 2-DRFWL(2) GNN is comparable to NGNN (which can only count up to 4-cycles), and much shorter than I\({}^{2}\)-GNN.

We also evaluate the empirical efficiency of 2-DRFWL(2) GNN on graphs of larger sizes. The results, along with details of the datasets we use, are given in Appendix F.5. From Table 14 in Appendix F.5, we see that 2-DRFWL(2) GNN easily scales to graphs with \( 500\) nodes, as long as the average degree is small.

## 7 Conclusion and limitations

Motivated by the analysis of why FWL(2) has a stronger cycle counting power than WL(1), we propose \(d\)-DRFWL(2) tests and \(d\)-DRFWL(2) GNNs. It is then proved that with \(d=2\), \(d\)-DRFWL(2) GNNs can already count up to 6-cycles, retaining most of the cycle counting power of FWL(2). Because \(d\)-DRFWL(2) GNNs explicitly leverage the _local_ nature of cycle counting, they are much more efficient than other existing GNN models that have comparable cycle counting power. Besides, \(d\)-DRFWL(2) GNNs also have an outstanding performance on various real-world tasks. Finally, we have to point out that our current implementation of \(d\)-DRFWL(2) GNNs, though being efficient most of the time, still has difficulty scaling to datasets with a large average degree such as ogbg-ppa, since the preprocessing time is too long (\(\)40 seconds per graph on ogbg-ppa). This also makes our method unsuitable for node classification tasks, since these tasks typically involve graphs with large average degrees. We leave the exploration of more efficient \(d\)-DRFWL(2) implementation to future work.

    &  &  \\   & Memory (GB) & Pre. (s) & Train (s/epoch) & Memory (GB) & Pre. (s) & Train (s/epoch) \\  MPNN & 2.28 & 64 & 45.3 & 2.00 & 2.4 & 18.8 \\ NGNN & 13.72 & 2354 & 107.8 & 5.23 & 1003 & 42.7 \\ I\({}^{2}\)-GNN & 19.69 & 5287 & 209.9 & 11.07 & 2301 & 84.3 \\ 
2-DRFWL(2) GNN & 2.31 & 430 & 141.9 & 4.44 & 201 & 44.3 \\   

Table 4: Empirical efficiency of 2-DRFWL(2) GNN.