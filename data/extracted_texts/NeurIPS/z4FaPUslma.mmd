# Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame

Evan Markou

Australian National University

evan.markou@anu.edu.au &Thalaiyasingam Ajanthan

Australian National University & Amazon

thalaiyasingam.ajanthan@anu.edu.au &Stephen Gould

Australian National University

stephen.gould@anu.edu.au

###### Abstract

Neural Collapse (NC) is a recently observed phenomenon in neural networks that characterises the solution space of the final classifier layer when trained until zero training loss. Specifically, NC suggests that the final classifier layer converges to a Simplex Equiangular Tight Frame (ETF), which maximally separates the weights corresponding to each class. By duality, the penultimate layer feature means also converge to the same simplex ETF. Since this simple symmetric structure is optimal, our idea is to utilise this property to improve convergence speed. Specifically, we introduce the notion of _nearest simplex ETF geometry_ for the penultimate layer features at any given training iteration, by formulating it as a Riemannian optimisation. Then, at each iteration, the classifier weights are implicitly set to the nearest simplex ETF by solving this inner-optimisation, which is encapsulated within a declarative node to allow backpropagation. Our experiments on synthetic and real-world architectures for classification tasks demonstrate that our approach accelerates convergence and enhances training stability1.

## 1 Introduction

While modern deep neural networks (DNNs) have demonstrated remarkable success in solving diverse machine learning problems , the fundamental mechanisms underlying their training process remain elusive. In recent years, considerable research efforts have focused on delineating the optimisation trajectory and characterising the solution space resulting from the optimisation process in training neural networks . One such finding is that gradient descent algorithms, when combined with certain loss functions, introduce an implicit bias that often favours max-margin solutions, influencing the learned representations and decision boundaries. .

In this vein, Neural Collapse (NC) is a recently observed phenomenon in neural networks that characterises the solution space of the final classifier layer in both balanced  and imbalanced dataset settings . Specifically, NC suggests that the final classifier layer converges to a Simplex Equiangular Tight Frame (ETF), which maximally separates the weights corresponding to each class, and by duality, the penultimate layer feature means converge to the classifier weights, _i.e_., to the simplex ETF (formal definitions are provided in Appendix A). This simple, symmetric structure is shown to be the only set of optimal solutions for a variety of loss functions when the features are also assumed to be free parameters, _i.e_., Unconstrained FeatureModels (UFMs) [32; 19; 74; 76; 28; 75]. Nevertheless, even in realistic large-scale deep networks, this phenomenon is observed when trained to convergence, even after attaining zero training error.

Since we can characterise the optimal solution space for the classifier layer, a natural extension is to _leverage the simplex ETF structure of the classifier weights to improve training_. To this end, researchers have tried fixing the classifier weights to a canonical simplex ETF, effectively reducing the number of trainable parameters . However, in practice, this approach does not improve the convergence speed as the backbone network still needs to do the heavy lifting of matching feature means to the chosen fixed simplex ETF.

In this work, we introduce a mechanism for finding the nearest simplex ETF to the features at any given training iteration. Specifically, the nearest simplex ETF is determined by solving a Riemannian optimisation problem. Therefore, our classifier weights are dynamically updated based on the penultimate layer feature means at each iteration, _i.e._, implicitly defined rather than trained using gradient descent. Additionally, by constructing this inner-optimisation problem as a deep declarative node , we allow gradients to propagate through the Riemannian optimisation facilitating end-to-end learning. Our whole framework significantly speeds up convergence to a NC solution compared to the fixed simplex ETF and conventional learnable classifier approaches. We demonstrate the effectiveness of our approach on synthetic UFMs and standard image classification experiments.

Our main contributions are as follows:

1. We introduce the notion of the nearest simplex ETF geometry given the penultimate layer features. Instead of selecting a predetermined simplex ETF (canonical or random), we implicitly fix the classifier as the solution to a Riemannian optimisation problem.
2. To establish end-to-end learning, we encapsulate the Riemannian optimisation problem of determining the nearest simplex ETF geometry within a declarative node. This allows for efficient backpropagation throughout the network.
3. We demonstrate that our method achieves an optimal neural collapse solution more rapidly compared to fixed simplex ETF methods or conventional training approaches, where a learned linear classifier is employed. Additionally, our method ensures training stability by markedly reducing variance in network performance.

## 2 Related Work

Neural Collapse and Simplex ETFs.Zhu et al.  proposed fixing classifier weights to a simplex ETF, reducing parameters while maintaining performance. Simplex ETFs effectively tackle imbalanced learning, as demonstrated by Yang et al. , where they fix the target classifier to an arbitrary simplex ETF, relying on the network's over-parameterisation to adapt. Similarly, Yang et al.  addressed class incremental learning by fixing the target classifier to a simplex ETF. They advocate adjusting prototype means towards the simplex ETF using a convex combination, smoothly guiding backbone features into the targeted simplex ETF. However, these methods did not yield any benefits regarding convergence speed. The work most relevant to ours is that of Peifeng et al. , who argued about the significance of feature directions, particularly in long-tailed learning scenarios. They compared their method against a fixed simplex ETF target, formulating their problem to enable the network to learn feature direction through a rotation matrix. Additionally, they efficiently addressed their optimisation using trivialisation techniques [39; 40]. However, they did not demonstrate any improvements in convergence speed over the fixed simplex ETF, achieving only a minimal increase in test accuracy. Fixing a classifier is not a recent concept, as it has been proposed prior to the emergence of neural collapse [52; 58; 30]. Most notably, Pernici et al.  demonstrated improved convergence speed by fixing the classifier to a simplex structure only on ImageNet while maintaining comparable performance on smaller-scale datasets. In contrast, our method shows superior convergence speed compared to both a fixed simplex ETF and a learned classifier across both small and large-scale datasets.

Optimisation on Smooth Manifolds.Our optimisation problem involves orthogonality constraints, characterised by the Stiefel manifold [2; 11]. Due to the nonlinearity of these constraints, efficiently solving such problems requires leveraging Riemannian geometry . A multitude of works are dedicated to solving such problems by either transforming existing classical optimisation techniques into Riemannian equivalent algorithms [1; 73; 20; 64; 55] or by carefully designing penalty functionsto address equivalent unconstrained problems [66; 65]. In our approach, we opt for a retraction-based Riemannian optimisation algorithm  to optimally handle orthogonality constraints.

Implicit Differentiable Optimisation.In a neural network setting, end-to-end architectures are commonplace. To backpropagate solutions to optimisation problems, we rely on machinery from implicit differentiation. Pioneering works [4; 3] demonstrated efficient gradient backpropagation when dealing with solutions of convex optimisation problems. This concept was independently introduced as a generalised version by Gould et al. [23; 24] to encompass any twice-differentiable optimisation problem. A key advantage of Deep Declarative Networks (DDNs) lies in their ability to efficiently solve problems at any scale by leveraging the problem's underlying structure . Our setting involves utilising an equality-constrained declarative node to efficiently backpropagate through the network.

## 3 Optimising Towards the Nearest Simplex ETF

In this section, we introduce our method to determine the nearest simplex ETF geometry and detail how we can dynamically steer the training algorithm to converge towards this particular solution.

### Problem Setup

Let us first introduce key notation that will be useful when formulating our optimisation problem.

Simplex ETF.Mathematically, a general simplex ETF is a collection of points in \(^{C}\) specified by the columns of a matrix

\[=}(_{C}-_{C} _{C}^{})\.\] (1)

Here, \(_{+}\) denotes an arbitrary scale factor, \(_{C}\) is the \(C\)-dimensional vector of ones, and \(^{d C}\) (with \(d C\)) represents a semi-orthogonal matrix (\(^{}=_{C}\)). Note that there are many simplex ETFs in \(^{C}\) as the rotation \(\) varies, and \(\) is rank-deficient. Additionally, the standard simplex ETF with unit Frobenius norm is defined as: \(}=}(_{C}-_{C}_{C}^{})\).

Mean of Features.Consider a classification dataset \(=\{(_{i},y_{i}) i=1,,N\}\) where the data \(_{i}\) and labels \(y_{i}=\{1,,C\}\). Suppose, \(n_{c}\) is the number of samples correspond to label \(c\), then \(_{c=1}^{C}n_{c}=N\). Let us consider a scenario where we have a collection of features defined as,

\[[_{c,i}:1 c C,\,1 i n_{c}]^{d N}\.\] (2)

Here, each feature may originate from a nonlinear compound mapping of input data through a neural network, denoted as, \(_{y_{i},i}=_{}(_{i})\) for the data sample \((_{i},y_{i})\). Now, for the final layer, our decision variables (weights and biases) are represented as \([_{1},,_{C}]^{}^{C d}\), and \(^{C}\), and the logits for the \(i\)-th sample is computed as,

\[_{}(_{i})=_{y_{i},i}+\,_{y_{i},i}=_{}(_{i})\.\] (3)

In UFMs, the features are assumed to be free variables, which serves as a rough approximation for neural networks and helps derive theoretical guarantees. Additionally, we define the global mean and per-class mean of the features \(\{_{c,i}\}\) as:

\[_{G}_{c=1}^{C}_{i=1}^{n_{c}}_{c,i}\, }_{c}}_{i=1}^{n_{c}}_{c,i}\,(1 c C)\,\] (4)

and the globally centred feature mean matrix as,

\[}[}_{1}-_{G},,}_{C}- _{G}]^{d C}\.\] (5)

Finally, we scale the feature mean matrix to have unit Frobenius norm, _i.e._, \(}=}/\|}\|_{F}\) which will be used in formulation below.

### Nearest Simplex ETF through Riemannian Optimisation

Once we obtain the feature means, our objective is to calculate the nearest simplex ETF based on these means and subsequently adjust the classifier weights \(\) to align with this particular simplex ETF. The rationale is to identify and establish a simplex ETF that closely corresponds to the feature means at any given iteration. This approach aims to expedite convergence during the training process by providing the algorithm with a starting point that is closer to an optimal solution rather than requiring it to learn a simplex ETF direction or converge towards an arbitrary one.

To find the nearest simplex ETF geometry, we solve the following Riemannian optimisation problem

\[ St_{C}^{d}}{}\ }- }_{F}^{2}\] (6)

where \(St_{C}^{d}=\{^{d C}:^{}=_{C}\}\). Here, \(}\) is the standard simplex ETF with unit Frobenius norm, and the set of the orthogonality constraints \(St_{C}^{d}\) forms a compact Stiefel manifold [2; 11] embedded in a Euclidean space.

### Proximal Problem

The solution to the Riemannian optimisation problem, denoted as \(^{}\), is not unique since a component of \(^{}\) lies in the null space of \(}\). As simplex ETFs reside in \((C-1)\)-dimensional space, the matrix \(}\) is rank-one deficient. Consequently, we are faced with a family of solutions, leading to challenges in training stability, as we may oscillate between multiple simplex ETF directions. We address this issue by introducing a proximal term to the problem's objective function. This guarantees the uniqueness of the solution and stabilises the training process, ensuring that our problem converges to a solution closer to the previous one.

So, the original problem in Equation 6 is transformed into:

\[ St_{C}^{d}}{}\ }- }_{F}^{2}+-_{}_{F}^{2}\.\] (7)

Here, \(_{}\) represents the proximal target simplex ETF direction, and \(>0\) serves as the proximal coefficient, handling the trade-off between achieving the optimal solution's proximity to the feature means and its proximity to a given simplex ETF direction. In fact, one can perceive our problem formulation in Equation 7 as a generalisation to a predetermined fixed simplex ETF solution. This is evident when considering that if we significantly increase \(\), the optimal direction \(^{}\) would converge towards the fixed proximal direction \(_{}\).

### General Learning Setting

Our problem formulation, following the general deep neural network architecture in Equation 3, can be seen as a bilevel optimisation problem as follows:

\[}{}\ (;,^{})-_{i=1}^{N}(}(_{i},^{})_{y_{i}})}{_{ j=1}^{C}(_{}(_{i},^{})_{j})} )\,\] (8) \[\ ^{} St_{C}^{d}}{ }\ }-}_{F}^{2}+ -_{}_{F}^{2}\,\]

where \(_{}(_{i},^{})=^{}(_{ i}-_{G})\) with \(_{i}=_{}(_{i})\). Here, \(\) denotes the logits, where the classifier weights are set as \(=^{}\), and the bias is set to \(=-^{}_{G}\) to account for feature centring. Furthermore, \(\) is the standard simplex ETF, \(}\) is its normalised version, and \(}\) is the normalised centred feature matrix. The temperature parameter \(>0\) controls the lower bound of the cross-entropy loss when dealing with normalised features, as defined in [69; Theorem 1].

### Handling Stochastic Updates

In practice, we use stochastic gradient descent updates, and, as such, adjustments to our computations are necessary. With each gradient update now based on a mini-batch, we implement two keychanges. First, rather than directly optimising the problem of finding the nearest simplex ETF geometry concerning the feature means of the mini-batch, we introduce an exponential moving average operation during the computation of the feature means. This operation accumulates statistics and enhances training stability throughout iterations. Formally, at time step \(t\), we have the following equation, where \(\) represents the smoothing factor:

\[}_{t}=}_{}+(1-) }_{t-1}\;.\] (9)

Second, we employ stratified batch sampling to guarantee that all class labels are represented in the mini-batch. This ensures that we avoid degenerate solutions when finding the nearest simplex ETF geometry, as our optimisation problem requires input feature means for all \(C\) classes. In cases where the number of classes exceeds the chosen batch size, we compute the per-class feature mean for the class labels present in the given batch. For the remaining class labels, we set their feature mean as the global mean of the batch. We repeat this process for each training iteration until we have sampled examples belonging to the missing class labels. At that point, we update the feature mean of those missing class labels with the new feature statistics. We reserve this method only for cases where the batch size is smaller than the number of labels since it can introduce instability during early iterations.

### Deep Declarative Layer

We can backpropagate through the Riemannian optimisation problem to update the feature means using a declarative node . Then, the features are updated from both the loss and the feature means through auto-differentiation. The motivation for developing the DDN layer lies in recognising that, despite the presence of a proximal term, abrupt and sudden changes to the classifier may occur as the features are updated. These changes can pose challenges for backpropagation, potentially disrupting the stability and convergence of the training process. Incorporating an additional stream of gradients through the feature means to account for such changes, as depicted in Figure 1, assists in stabilising the feature updates during backpropagation.

To efficiently backpropagate through the optimisation problem, we employ techniques described in Gould et al.  utilising the implicit function theorem to compute the gradients. In our case, we have a scalar objective function \(f:^{d C}\), and a matrix constraint function \(J:^{d C}^{C C}\). Since we have matrix variables, we use vectorisation techniques  to avoid numerically dealing with tensor gradients. More specifically, we have the following:

**Proposition 1** (Following directly from Proposition 4.5 in Gould et al. ).: _Consider the optimisation problem in Equation 7. Assume that the solution exists and that the objective function \(f\) and the constraint function \(J\) are twice differentiable in the neighbourhood of the solution. If the \(()=\) and \(\) is non-singular then:_

\[Dy()=^{-1}^{}(^{-1}^{})^{-1}( {A}^{-1})-^{-1}\;,\]

_where,_

\[ =(D_{}J(},)) ^{ dC}\;,\] \[ =(D^{2}_{}\,f(},))^{dC dC}\;,\] \[ =(D^{2}_{}\,f(},))-:(D^{2}_{}\,J(}, ))^{dC dC}\;.\]

Figure 1: Schematic of our proposed architecture for optimising towards the nearest simplex ETF. The classifier weights \(=^{}\) are an implicit function of the CNN features \(\). Note that the parameters of the CNN are updated via two gradient paths from the loss function \(\), a direct path (top) and an indirect path through \(^{}\) (bottom).

_Here, the double dot product symbol \(()\) denotes a tensor contraction on appropriate indices between the Lagrange multiplier matrix \(\) and a fourth-order tensor Hessian. Also, \(()\) and \(()\) refer to the row-major vectorisation and half-vectorisation operations, respectively. To find the Lagrange multiplier matrix \(^{C}\), we solve the following equation where we have vectorised the matrix as \(^{}\),_

\[^{}=D_{}\ f(},)\.\]

_Alternatively, for a more efficient computation of the identity \(\), we can utilise the embedded gradient field method as defined in Birtea et al. . Therefore, we obtain:_

\[=(D_{}^{2}\ f(},))-_{d }()^{dC dC}\,\]

_where \(()=D_{}\ f(},)^{} +^{}D_{}\ f(},)\), and \(\) here denotes Kronecker product._

A detailed derivation of each identity in the proposition can be found in Appendix B.

## 4 Experiments

In our experiments, we perform feature normalisation onto a hypersphere, a common practice in training neural networks, which improves representation and enhances model performance . We find that combining classifier weight normalisation with feature normalisation accelerates convergence . Given that simplex ETFs are inherently normalised, we include classifier weight normalisation in our standard training procedure to ensure fair method comparisons.

Experimental Setup.In this study, we conduct experiments on three model variants. First, the standard method involves training a model with learnable classifier weights, following conventional practice. Second, in the fixed ETF method, we set the classifier to a predefined simplex ETF. In all experiments, we choose the simplex ETF with canonical direction. In Appendix C, we also include additional experiments for fixed simplex ETFs with random directions generated from a Haar measure . Last, our implicit ETF method, where we set the classifier weights on-the-fly as the simplex ETF closest to the current feature means.

We repeat experiments on each method five times with distinct random seeds and report the median values alongside their respective ranges. For reproducibility and to streamline hyperparameter tuning, we employed Automatic Gradient Descent (AGD) . Following the authors' recommendation, we set the gain/momentum parameter to 10 to expedite convergence, aligning it with other widely used optimisers like Adam  and SGD. Our experiments on real datasets run for 200 epochs with batch size 256; for the UFM analysis, we run 2000 iterations.

Our method underwent rigorous evaluation across various UFM sizes and real model architectures trained on actual datasets, including CIFAR10 , CIFAR100 , STL10 , and ImageNet-1000 , implemented on ResNet  and VGG  architectures. More specifically, we trained CIFAR10 on ResNet18 and VGG13, CIFAR100 and STL10 on ResNet50 and VGG13, and ImageNet-1000 on ResNet50. The input images were preprocessed pixel-wise by subtracting the mean and dividing by the standard deviation. Additionally, standard data augmentation techniques were applied, including random horizontal flips, rotations, and crops. All experiments were conducted using Nvidia RTX3090 and A100 GPUs.

Hyperparameter Selection and Riemannian Initialisation Schemes.We solve the Riemannian optimisation problem defined in Equation 7 using a Riemannian Trust-Region method  from pyManopt . We maintain a proximal coefficient \(\) set to \(10^{-3}\) consistently across all experiments. It is worth mentioning that algorithm convergence is robust to the precise value of \(\). In our problem, determining values for \(_{}\) and \(_{}\) is crucial. We explored several methods to initialise these parameters. One approach involved setting both towards the canonical simplex ETF direction. This means initialising them as a partial orthogonal matrix where the first \(C\) rows and columns form an identity matrix while the remaining \(d-C\) rows are filled with zeros. Another approach is to initialise both of them as random orthogonal matrices from classical compact groups, selected according to a Haar measure . In the end, the approach that yielded the most stable results at initialisation was to employ either of the aforementioned initialisation methods to solve the original problem without the proximal term in Equation 6. We then used the obtained \(^{}\) to initialise both \(_{}\) and \(_{}\) for the problem in Equation 7. This process was carried out only for the first gradient update of the first epoch. In subsequent iterations, we update these parameters to the \(^{}\) obtained from the previous time step. Importantly, the proximal term is held fixed during each Riemannian optimisation.

Regarding the calculation of the exponential moving average of the feature means, we have found that employing a decay policy on the smoothing factor \(\) yields optimal results. Specifically, we set \(=2/(T+1)\), where \(T\) represents the number of iterations. Additionally, we include a thresholding value of \(10^{-4}\), such that if \(\) falls below this threshold, we fix \(\) to be equal to the threshold. This precaution ensures that \(\) does not diminish throughout the iterations, thereby guaranteeing that the newly calculated feature means contribute sufficient statistics to the exponential moving average.

Finally, in our experiments, we set the temperature parameter \(\) to five. This choice aligns with the findings discussed by Yaras et al. , highlighting the influence of the temperature parameter value on the extent of neural collapse statistics with normalised features.

Unconstrained Feature Models (UFMs).Our experiments on UFMs, which provide a controlled setting for evaluating the effectiveness of our method, are done using the following configurations:

* UFM-10: a 10-class UFM containing 1000 features with a dimension of 512.
* UFM-100: a 100-class UFM containing 5000 features with a dimension of 1024.
* UFM-200: a 200-class UFM containing 5000 features, with a dimension of 1024.
* UFM-1000: a 1000-class UFM containing 10000 features, with a dimension of 1024.

Results.We present the results for the synthetic UFM-10 case in Figure 2. The CE loss plot demonstrates that fixing the classifier weights to a simplex ETF achieves the theoretical lower bound of Yaras et al. [69, Thm. 1], indicating the attainment of a globally optimal solution. We also visualise the average cosine margin per epoch and the cosine margin distributions of each example at the end of training, defined in Zhou et al. . The neural collapse metrics, \(NC1\) and \(NC3\), which measure the features' within-class variability, and the self-duality alignment between the feature means and the classifier weights , are also plotted. Last, we depict the absolute difference of the classifier and feature means norms to illustrate their convergence towards equinorms, as described in Papyan et al. . A comprehensive description of the metrics can be found in Appendix A. Collectively, the plots indicate the superior performance of our method in achieving a neural collapse (NC) solution

Figure 2: UFM-10 results. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

faster than other approaches. In Figure 3, we demonstrate under the UFM setting that as we increase the number of classes, our method maintains constant performance and converges at the same rate, while the fixed ETF and the standard approach require more time to reach the interpolation threshold.

Numerical results for the top-1 train and test accuracy are reported in Tables 1 and 2, respectively. The results are provided for snapshots taken at epoch 50 and epoch 200. It is evident that our method achieves a faster convergence speed compared to the competitive methods while ultimately converging to the same performance level. Additionally, it is noteworthy that our method exhibits the smallest degree of variability across different runs, as indicated by the range values provided. Finally, in Figure 4, we present qualitative results that confirm our solution's ability to converge much faster and reach peak performance earlier than the standard and fixed ETF methods on ImageNet. It's important to note that the standard method with AGD is reported to converge to the same testing accuracy (\(65.5\%\)) at epoch 350, as shown in Bernstein et al. (2016, Figure 4). At epoch 200, the authors exhibit a testing accuracy of approximately \(51\%\). Since we have increased the gain parameter on AGD compared to the results reported in the original paper, we report a final \(60.67\%\) testing accuracy for the standard method, whereas our method reaches peak convergence at approximately epoch 80. We note that the ImageNet results reported in Tables 1 and 2, as well as Figure 4, are generated solely by solving the Riemannian optimisation problem without considering its gradient stream on the feature updates, due to computational constraints. We discuss the computational requirements of our method in Section 5. We also present qualitative results for all the other datasets and architectures in Appendix C.

## 5 Discussion: Limitations and Future Directions

Our method involves two gradient streams updating the features, as depicted in Figure 1. Interestingly, empirical observations on small-scale datasets (see Figure 15) indicate that even without the backpropagation through the DDN layer, the performance remains comparable, rendering the gradient calculation of the DDN layer optional. In Figure 14(c), we observe a strong impact of the DDN layer gradient on the atomic feature level, with more features reaching the theoretical simplex ETF margin by the end of training. To reach a consensus on the exact effect of the DDN gradient on the learning

    & &  &  \\  Dataset & Network & Standard & Fixed ETF & Implicit ETF & Standard & Fixed ETF & Implicit ETF \\   & ResNet18 & \(87.4289.7\\ 86.1\) & \(86.8988.6\\ 84.7\) & \(89.6\\ 88.5\) & \(96.6998.6\\ 96.5\) & \(97.1897.9\\ 95.6\) & \(98.6\\ 97.7\) \\  & VGG13 & \(93.5997.0\\ 90.7\) & \(76.6653.5\\ 53.9\) & \(96.2\\ 95.2\) & \(99.1598.7\\ 98.7\) & \(97.9396.91\\ 95.96\) & \(99.7\\ 99.4\) \\  & ResNet50 & \(58.4759.6\\ 53.93\) & \(63.9365.2\\ 61.1\) & \(74.1\\ 70.1\) & \(95.8798.6\\ 94.7\) & \(97.1394.21\\ 90.4\) & \(97.3\\ 96.2\) \\  & VGG13 & \(82.0084.0\\ 80.5\) & \(81.1481.9\\ 76.0\) & \(89.4\\ 88.9\) & \(99.2\\ 99.2\) & \(94.5595.3\\ 92.5\) & \(98.9290.0\\ 98.8\) \\  & ResNet50 & \(83.8690.7\\ 84.5\) & \(86.7668.8\\ 77.78\) & \(95.3\\ 91.3\) & \(99.4299.9\\ 99.9\) & \(98.3893.3\\ 98.1\) & \(99.9\\ 99.8\) \\  & VGG13 & \(82.6690.7\\ 93.7\) & \(83.6085.1\\ 65.6\) & \(93.5\\ 69.2\) & \(100.000\\ 1000\\ 99.2\) & \(99.9893.1\\ 99.8\) & \(100\\ 1000\\ 1000\\ 1000\) \\ ImageNet & ResNet50 & \(58.3591.1\\ 88.0\) & \(70.4470.7\\ 69.5\) & \(74.5\\ 73.8\) & \(77.2073.5\\ 76.3\) & \(83.0983.6\\ 83.1\) & \(85.5\\ 87.5\) \\   

Table 1: Train top-1 accuracy results presented as a median with indices indicating the range of values from five random seeds. Best results are bolded.

Figure 3: The evolution of convergence measured in top-1 accuracy of the UFM as we increase the number of classes, plotted for the first 800 epochs. We omit the rest of the epochs as all methods have converged and have identical results.

process, further experiments on large-scale datasets are needed. However, on large-scale datasets with large \(d\) and \(C\), such as ImageNet, computing the backward pass of the Riemannian optimisation is challenging due to the memory inefficiency of the current implementation of DDN gradients. This limitation is an area we aim to address in future work. Note that in all other experiments, we use the full gradient computations, including both direct and indirect components, through the DDN layer. We summarise the GPU memory requirements for each method across various datasets in Table 3.

Our discussion so far has focused on convergence speed in terms of the number of epochs required for the network to converge. However, it is also important to consider the time required per epoch. In our case, as training progresses, the time taken by the Riemannian optimization quickly becomes almost negligible compared to the network's total forward pass time, while it approaches the standard and fixed ETF training forward times, as shown in Figure 4(a). However, DDN gradient computation increases considerably when the feature dimension \(d\) and the number of classes \(C\) increase and starts to dominate the runtime for large datasets such as ImageNet. Nevertheless, for ImageNet, we do not compute the DDN gradients and still outperform other methods. We plan to explore ways to expedite the DDN forward and backward pass in future work.

## 6 Conclusion

In this paper, we introduced a novel method for determining the nearest simplex ETF to the penultimate features of a neural network and utilising it as our target classifier at each iteration. This contrasts with previous approaches, which either fix to a specific simplex ETF or allow the network

    & &  &  \\  Dataset & Network & Standard & Fixed ETF & Implicit ETF & Standard & Fixed ETF & Implicit ETF \\   & ResNet18 & \(80.47\,^{82.6}_{79.4}\) & \(80.63\,^{81.8}_{79.4}\) & \(\,^{82.0}_{81.4}\) & \(83.97\,^{84.8}_{83.2}\) & \(84.53\,^{84.9}_{83.7}\) & \(\,^{85.1}_{84.3}\) \\  & VGG13 & \(86.70\,^{89.4}_{83.7}\) & \(70.99\,^{80.7}_{50.7}\) & \(\,^{88.7}_{87.4}\) & \(90.34\,^{91.5}_{89.1}\) & \(72.48\,^{90.5}_{56.9}\) & \(\,^{91.5}_{90.6}\) \\  & ResNet50 & \(45.91\,^{46.3}_{42.1}\) & \(45.37\,^{45.6}_{43.2}\) & \(\,^{49.3}_{83.0}\) & \(\,^{51.9}_{50.4}\) & \(48.03\,^{49.3}_{47.7}\) & \(50.52\,^{51.2}_{50.2}\) \\  & VGG13 & \(60.82\,^{61.2}_{59.3}\) & \(60.10\,^{61.1}_{57.1}\) & \(\,^{63.6}_{61.8}\) & \(\,^{68.1}_{66.4}\) & \(62.78\,^{63.4}_{61.6}\) & \(67.14\,^{67.6}_{66.8}\) \\  & ResNet50 & \(55.41\,^{58.3}_{54.8}\) & \(\,^{60.0}_{57.1}\) & \(57.56\,^{59.2}_{56.4}\) & \(63.60\,^{65.2}_{61.8}\) & \(\,^{66.5}_{63.9}\) & \(62.75\,^{63.0}_{60.7}\) \\  & VGG13 & \(66.09\,^{72.3}_{60.0}\) & \(66.15\,^{67.7}_{52.2}\) & \(\,^{71.3}_{56.2}\) & \(\,^{81.9}_{81.3}\) & \(79.53\,^{80.3}_{78.6}\) & \(79.94\,^{80.9}_{79.5}\) \\  & ResNet50 & \(52.64\,^{53.3}_{52.3}\) & \(58.85\,^{59.1}_{58.3}\) & \(\,^{63.8}_{63.2}\) & \(60.20\,^{60.7}_{59.8}\) & \(61.47\,^{62.0}_{61.4}\) & \(\,^{65.7}_{65.0}\) \\   

Table 2: Test top-1 accuracy results presented as a median with indices indicating the range of values from five random seeds. Best results are bolded.

Figure 4: ImageNet results on ResNet-50. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

to learn it through gradient descent. Our method involves solving a Riemannian optimisation problem facilitated by a deep declarative node, enabling backpropagation through this process.

We demonstrated that our approach enhances convergence speed across various datasets and architectures while also reducing variability stemming from different random initialisations. By defining the optimal structure of the classifier and efficiently leveraging its rotation invariance property to find the one closest to the backbone features, we anticipate that our method will facilitate the creation of new architectures and the utilisation of new datasets without necessitating specific learning or tuning of the classifier's structure.