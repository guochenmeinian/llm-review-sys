# Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network

Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network

 Tristan Deleu\({}^{1}\)   Mizu Nishikawa-Toomey\({}^{1}\)   Jithendaraa Subramanian\({}^{2}\)

Nikolay Malkin\({}^{1}\)   Laurent Charlin\({}^{3}\)   Yoshua Bengio\({}^{1,4}\)

Mila - Quebec AI Institute

###### Abstract

Generative Flow Networks (GFlowNets), a class of generative models over discrete and structured sample spaces, have been previously applied to the problem of inferring the marginal posterior distribution over the directed acyclic graph (DAG) of a Bayesian Network, given a dataset of observations. Based on recent advances extending this framework to non-discrete sample spaces, we propose in this paper to approximate the joint posterior over not only the structure of a Bayesian Network, but also the parameters of its conditional probability distributions. We use a single GFlowNet whose sampling policy follows a two-phase process: the DAG is first generated sequentially one edge at a time, and then the corresponding parameters are picked once the full structure is known. Since the parameters are included in the posterior distribution, this leaves more flexibility for the local probability models of the Bayesian Network, making our approach applicable even to non-linear models parametrized by neural networks. We show that our method, called JSP-GFN, offers an accurate approximation of the joint posterior, while comparing favorably against existing methods on both simulated and real data.

## 1 Introduction

As a compact representation for complex probabilistic models, Bayesian Networks are a framework of choice in many fields, such as computational biology (Friedman et al., 2000; Sachs et al., 2005) and medical diagnosis (Lauritzen and Spiegelhalter, 1988). When the directed acyclic graph (DAG) structure of the Bayesian Network--which specifies the possible conditional dependences among the observed variables--is known, it can be used to perform probabilistic inference for queries of interest with a variety of exact or approximate methods (Koller and Friedman, 2009). However, if this graphical structure is unknown, one may want to infer it based on a dataset of observations \(\).

In addition to being a challenging problem due to the super-exponentially large search space, learning a single DAG structure from data may also lead to confident but incorrect predictions (Madigan et al., 1994), especially in cases where the evidence is limited. In order to avoid misspecifying the model, it is therefore essential to quantify the epistemic uncertainty about the structure of the Bayesian Network. This can be addressed by taking a Bayesian perspective on structure learning and inferring the posterior distribution \(P(G)\) over graphs given our observations. This (marginal) posterior can be approximated using methods based on Markov chain Monte Carlo (MCMC; Madigan et al., 1995) or variational inference (Cundy et al., 2021; Lorch et al., 2021). However, all of these methods rely on the computation of the marginal likelihood \(P( G)\), which can only be done efficiently in closed form for limited classes of models, such as linear Gaussian (Geiger and Heckerman, 1994),discrete models with Dirichlet prior (Heckerman et al., 1995), or non-linear models parametrized with a Gaussian Process (von Kugelgen et al., 2019).

While there exists a vast literature on Bayesian structure learning to approximate the marginal posterior distribution, inferring the _joint posterior_\(P(G,)\) over both the DAG structure \(G\) of the Bayesian Network and the parameters \(\) of its conditional probability distributions--the probability of each variable given its parents--has received comparatively little attention. The main difficulty arises from the mixed sample space of the joint posterior distribution, with both discrete components (the graph \(G\)) and continuous components (the parameters \(\)), where the dimensionality of the latter may even depend on \(G\). However, modeling the posterior distribution over \(\) has the notable advantage that the conditional probability distributions can be more flexible (e.g., parametrized by neural networks): in general, computing \(P( G,)\) is easier than computing the marginal \(P( G)\), lifting the need to perform intractable marginalizations.

Since they provide a framework for generative modeling of discrete and composite objects, Generative Flow Networks (GFlowNets; Bengio et al., 2021, 2023) proved to be an effective method for Bayesian structure learning. In Deleu et al. (2022), the problem of generating a sample DAG from the marginal posterior \(P(G)\) was treated as a sequential decision process, where edges are added one at a time, starting from the empty graph over \(d\) variables, following a learned transition probability. Nishikawa-Toomey et al. (2023) have also proposed to use a GFlowNet to infer the joint posterior \(P(G,)\); however, they used it in conjunction with Variational Bayes to update the distribution over \(\), getting around the difficulty of modeling a continuous distribution with a GFlowNet.

In this paper, we propose to infer the joint posterior over graphical structures \(G\) and parameters of the conditional probability distributions \(\) of a Bayesian Network using _a single_ GFlowNet called JSP-GFN (for _Joint Structure and Parameters GFlowNet_), leveraging recent advances extending GFlowNets to continuous sample spaces (Lahlou et al., 2023), and expands the scope of applications for Bayesian structure learning with GFlowNets. The generation of a sample \((G,)\) from the approximate posterior now follows a two-phase process, where the DAG \(G\) is first constructed by inserting one edge at a time, and then the corresponding parameters \(\) are chosen once the structure is completely known. To enable efficient learning of the sampling distribution, we introduce new conditions closely related to the ones derived in Deleu et al. (2022), based on the subtrajectory balance conditions (Malkin et al., 2022), and show that they guarantee that the GFlowNet does represent \(P(G,)\) once they are completely satisfied. We validate empirically that JSP-GFN provides an accurate approximation of the posterior when those conditions are approximately satisfied by a learned sampling model, and compares favorably against existing methods on simulated and real data.

## 2 Background

Notations.Throughout this paper, we will work with directed graphs \(G=(V,E)\), where \(V\) is a set of nodes, and \(E V V\) is a set of (directed) edges. For a node \(X V\), we denote by \(_{G}(X)\) the set of parents of \(X\) in \(G\), and \(_{G}(X)\) the set of its children. For two nodes \(X,Y V\), \(X Y\) represents a directed edge \((X,Y) E\) (denoted \(X Y G\)), and \(X Y\) represents a directed path from \(X\) to \(Y\), following the edges in \(E\) (denoted \(X Y G\)).

In the context of GFlowNets (see Section 2.2), an _undirected path_ in a directed graph \(=(,)\) between two states \(s_{0},s_{n}\) is a sequence of vertices \((s_{0},s_{1},,s_{n})\) where either \((s_{i},s_{i+1})\) or \((s_{i+1},s_{i})\) (i.e., following the edges of the graph, regardless of their orientations).

### Bayesian structure learning

A Bayesian Network is a probabilistic model over \(d\) random variables \(\{X_{1},,X_{d}\}\), whose joint distribution factorizes according to a directed acyclic graph (DAG) \(G\) as

\[P(X_{1},,X_{d},G)=_{i=1}^{d}PX_{i} {Pa}_{G}(X_{i});_{i},\] (1)

where \(=\{_{1},,_{d}\}\) represents the parameters of the conditional probability distributions (CPDs) involved in this factorization. When the structure \(G\) is known, a Bayesian Network offers a representation of the joint distribution that may be convenient for probabilistic inference (Koller and 

[MISSING_PAGE_FAIL:3]

### Structure learning with GFlowNets

Since GFlowNets are particularly well-suited to specifying distributions over composite objects, Deleu et al. (2022) used this framework in the context of Bayesian structure learning to approximate the (marginal) posterior distribution over DAGs \(P(G)\). Their model, called _DAG-GFlowNet_, operates on the state-space of DAGs, where each graph is constructed sequentially by adding one edge at a time, starting from the empty graph with \(d\) nodes, while enforcing the acyclicity constraint at every step of the generation (i.e., an edge is not added if it would introduce a cycle). Its structure is illustrated at the top of Figure 1, where it forms the part of the graph shown in blue.

Instead of working with flows \(F_{}(G G^{})\), as in Section 2.2, DAG-GFlowNet directly learns the forward transition probability \(P_{}(G^{} G)\) that satisfies the following alternative _detailed balance_ conditions for any transition \(G G^{}\) (i.e., \(G^{}\) is the result of adding a single edge to \(G\)):

\[R(G^{})P_{B}(G G^{})P_{}(s_{f} G)=R(G)P_{}(G^{ } G)P_{}(s_{f} G^{}),\] (4)

where \(P_{B}(G G^{})\) is a fixed distribution over the parent states of \(G^{}\) (e.g., uniform distribution over parents). Deleu et al. (2022) showed that since all the states are complete here, satisfying the conditions (4) for all \(G G^{}\) still induces a distribution over DAGs \( R(G)\). Therefore, to approximate the posterior distribution \(P(G)\), they used \(R(G)=P( G)P(G)\) as the reward of \(G\). In particular, this requires evaluating the marginal likelihood \(P( G)\) efficiently, which is feasible only for limited classes of models (e.g., linear Gaussian; Geiger and Heckerman, 1994).

## 3 Joint Bayesian inference of structure and parameters

Although Generative Flow Networks have been primarily applied to model distributions over discrete objects such as DAGs, Lahlou et al. (2023) showed that similar ideas could also be applied to continuous objects, and discrete-continuous hybrids. Building on top of DAG-GFlowNet, we propose here to approximate the joint posterior \(P(G,)\) over both the structure of the Bayesian Network \(G\), but also the parameters of its conditional probability distributions \(\). Unlike in VBG though (Nishikawa-Toomey et al., 2023), we use a single GFlowNet to approximate this joint posterior. We call this model _JSP-GFN_, for Joint Structure and Parameters Bayesian inference with a GFlowNet.

### Structure of the GFlowNet

Unlike in DAG-GFlowNet, where we model a distribution only over DAGs, here we need to define a GFlowNet whose complete states are pairs \((G,)\), where \(G\) is a DAG and \(\) is a set of (continuous-valued) parameters whose dimension may depend on \(G\). Complete states are obtained through two phases (Figure 1): the DAG \(G\) is first constructed one edge at a time, following Deleu et al. (2022), and then the corresponding parameters \(\) are generated, conditioned on \(G\). We denote by \((G,)\) states where the DAG \(G\) has no parameters \(\) associated to it (states in blue in Figure 1); they are intermediate states during the first phase of the GFlowNet, and do not correspond to valid samples of the induced distribution. Using the notations of Section 2.2, \((G,)\), whereas \((G,)\).

Starting at the empty graph \((G_{0},)\), the DAG is constructed one edge at a time during the first phase, following the forward transition probabilities \(P_{}(G^{} G)\). This first phase ends when a special "stop" action is selected with \(P_{}\), indicating that we stop adding edges to the graph; the role of this "stop" action is detailed in Section 3.4. Then during the second phase, we generate \(\) conditioned on \(G\), following the forward transition probabilities \(P_{}( G)\).2 All the complete states \((G,)\), for a fixed graph \(G\) and any set of parameters \(\), can be seen as forming an (infinitely wide) tree rooted at \((G,)\).

Since we want this GFlowNet to approximate the joint posterior \(P(G,) P(,,G)\), it is natural to define the reward function of a complete state \((G,)\) as

\[R(G,)=P(,G)P( G)P(G),\] (5)

where the likelihood model \(P(,G)\) may be arbitrary (e.g., a neural network), and decomposes according to (1), \(P( G)\) is the prior over parameters, and \(P(G)\) the prior over graphs. When the dataset \(\) is large, we can use a mini-batch approximation to the reward (see Section 3.5).

### Subtrajectory Balance conditions

To obtain a generative process that samples pairs of \((G,)\) proportionally to the reward, the GFlowNet needs to satisfy some conditions such as (4). However, we saw in Section 2.3 that satisfying this particular formulation of the detailed balance conditions in (4) yields a distribution \( R()\) only if all the states are complete; unfortunately, this is not the case here since there exists states of the form \((G,)\) corresponding to graphs without their associated parameters. Instead, we use a generalization of detailed balance to undirected paths of arbitrary length, called the _Subtrajectory Balance_ conditions (SubTB; Malkin et al., 2022); we give a brief overview of SubTB in Appendix C.2.

More precisely, we consider SubTB for any undirected path of the form \((G,)(G,)(G^{},)(G^{ },^{})\) in the GFlowNet (see Figure C.2), where \(G^{}\) is therefore the result of adding a single edge to \(G\). Since both ends of these undirected paths of length 3 are complete states, we show in Appendix C.3.1 that the SubTB conditions corresponding to undirected paths of this form can be written as

\[R(G^{},^{})P_{B}(G G^{})P_{}( G)=R (G,)P_{}(G^{} G)P_{}(^{} G^{}).\] (6)

Note that the SubTB condition above is very similar to the detailed balance condition in (4) used in DAG-GFlowNet, where the probability of terminating \(P_{}(s_{f} G)\) has been replaced by \(P_{}( G)\), in addition to the reward now depending on both \(G\) and \(\). Moreover, while it does not seem to appear in (6), the terminal state \(s_{f}\) of the GFlowNet is still present implicitly, since we are forced to terminate once we have reached a complete state \((G,)\), and therefore \(P_{}(s_{f} G,)=1\) (see App. C.3.1).

Although there is no guarantee in general that satisfying the SubTB conditions would yield a distribution proportional to the reward, unlike with the detailed balance conditions (Bengio et al., 2023), the following theorem shows that the GFlowNet does induce a distribution \( R(G,)\) if the SubTB conditions in (6) are satisfied for all pairs \((G,)\) and \((G^{},^{})\).

**Theorem 3.1**.: _If the SubTB conditions in (6) are satisfied for all undirected paths of length 3 between any \((G,)\) and \((G^{},^{})\) of the form \((G,)(G,)(G^{},)(G^{ },^{})\), then we have_

\[P_{}^{}(G,) P_{}(G G_{0})P_{}( G ) R(G,),\]

_where \(P_{}(G G_{0})\) is the marginal probability of reaching \(G\) from the initial state \(G_{0}\) with any (complete) trajectory \(=(G_{0},G_{1},,G_{T-1},G)\):_

\[P_{}(G G_{0})_{:G_{0} G}_{t=0} ^{T-1}P_{}(G_{t+1} G_{t}),\]

_using the conventions \(G_{T}=G\), and \(P_{}(G_{0} G_{0})=1\)._

The proof of this theorem is available in Appendix C.3.3. The marginal distribution \(P_{}^{}(G,)\) is also called the _terminating state probability_ in Bengio et al. (2023).

### Learning objective

One way to find the parameters \(\) of the forward transition probabilities that enforce the SubTB conditions in (6) for all \((G,)\) and \((G^{},^{})\) is to transform this condition into a learning objective. For example, we could minimize a non-linear least squares objective (Bengio et al., 2021, 2023) of the form \(()=_{}[^{2}()]\), where the residuals \(()\) depend on the conditions in (6), and \(\) is an arbitrary sampling distribution of complete states \((G,)\) and \((G^{},^{})\), with full support; see Malkin et al. (2023) for a discussion of the effect of \(\) on training GFlowNets, and Appendix D.1 for further details in the context of JSP-GFN.

In addition to the SubTB conditions for undirected paths of length 3 given in Section 3.2, we can also derive similar SubTB conditions for other undirected paths, for example those of the form \((G,)(G,)(G,)\), where \(\) and \(\) are two possible sets of parameters associated with the same graph \(G\) (see also Figure C.2). When the reward function \(R(G,)\) is differentiable wrt. \(\), which is the case here, we show in Appendix C.3.2 that we can write the SubTB conditions for these undirected paths of length 2 in differential form as

\[_{} P_{}( G)=_{} R(G,).\] (7)This condition is equivalent to the notion of score matching (Hyvarinen, 2005) to model unnormalized distributions, since \(R(G,)\) here corresponds to the unnormalized posterior distribution \(P( G,)\), where the normalization is over \(\). We also show in Appendix C.3.2 that incorporating this information about undirected paths of length \(2\), via the identity in (7), amounts to _preventing_ backpropagation through \(\) and \(^{}\) (e.g., backpropagation with the reparametrization trick) in the objective

\[()=_{}[(G^{}, (^{})P_{B}(G G^{})P_{}( ) G}{RG,()P_{}(G^{} G)P_{ }(^{}) G^{}})^{2}],\] (8)

where \(\) denotes the "stop-gradient" operation. This is aligned with the recommendations of Lahlou et al. (2023) to avoid backpropagation through the reward in continuous GFlowNets. The pseudo-code for training JSP-GFN is available in Algorithm 1.

### Parametrization of the forward transition probabilities

In Section 3.1, we saw that the process of generating \((G,)\) follows two phases: first we construct \(G\) one edge at a time, until we sample a specific "stop" action, at which point we sample \(\), conditioned on \(G\). All these actions are sampled using the forward transition probabilities \(P_{}(G^{} G)\) during the first phase, and \(P_{}( G)\) during the second one. Following Deleu et al. (2022), we parametrize these forward transition probabilities using a hierarchical model: we first decide whether we want to stop the first phase or not, with probability \(P_{}( G)\); then, conditioned on this first decision, we either continue adding an edge to \(G\) to reach \(G^{}\) with probability \(P_{}(G^{} G,-)\) (phase 1), or sample \(\) with probability \(P_{}( G,)\) (phase 2). This hierarchical model can be written as

\[P_{}(G^{} G) =1-P_{}( G)P_{}(G^{}  G,)\] (9) \[P_{}( G) =P_{}( G)P_{}( G,).\] (10)

We use neural networks to parametrize each of the three components necessary to define the forward transition probabilities. Unlike in DAG-GFlowNet though, which uses a linear Transformer to define \(P_{}( G)\) and \(P_{}(G^{} G,-)\), we use a combination of graph network (Battaglia et al., 2018) and self-attention blocks (Vaswani et al., 2017) to encode information about the graph \(G\), which appears in the conditioning of all the quantities of interest. This common backbone returns a graph embedding \(\) of \(G\), as well as 3 embeddings \(_{i},_{i},_{i}\) for each node \(X_{i}\) in \(G\)

\[,\{_{i},_{i},_{i}\}_{i=1}^{d}=_{ }_{}(G).\]

We can parametrize the probability of selecting the "stop" action using \(\) with \(P_{}( G)=f_{}()\), where \(f_{}\) is a neural network with a sigmoid output; note that if we can't add any edge to \(G\) without creating a cycle, we force the end of the first phase by setting \(P_{}( G)=1\). Inspired by Lorch et al. (2021), the probability of moving from \(G\) to \(G^{}\) by adding the edge \(X_{i} X_{j}\) is parametrized by

\[P_{}(G^{} G,)_{ij} {u}_{i}^{}_{j},\] (11)

where \(_{ij}\) is a binary mask indicating whether adding \(X_{i} X_{j}\) is a valid action (i.e., if it is not already present in \(G\), and if it doesn't introduce a cycle; Deleu et al., 2022). Finally, the probability of selecting the parameters \(_{i}\) of the CPD for the variable \(X_{i}\) is parametrized with a multivariate Normal distribution with diagonal covariance (unless specified otherwise)

\[P_{}(_{i} G,)=_{i}_{}(_{i}),_{}^{2}(_{i}),\] (12)

where \(_{}\) and \(_{}^{2}\) are two neural networks, with appropriate non-linearities to guarantee that \(_{}^{2}(_{i})\) is a well-defined diagonal covariance matrix. Note that \(P_{}(_{i} G,)\) effectively approximates the posterior distribution \(P(_{i} G,)\) once fully trained. Moreover, in addition to being an approximation of the joint posterior \(P(G,)\), the GFlowNet also provides an approximation of the marginal posterior \(P(G)\), by only following the first phase of the generation process (to generate \(G\)) until the "stop" action is selected, and not continuing into the generation of \(\).

### Mini-batch training

Throughout the paper, we have assumed that we had access to the full dataset of observations \(\) in order to compute the reward \(R(G,)\) in (5). However, beyond the capacity to have an arbitrary likelihood model \(P(,G)\) (e.g., non-linear), another advantage of approximating the joint posterior \(P(G,)\) is that we can train the GFlowNet using mini-batches of observations. Concretely, for a mini-batch \(\) of \(M\) observations sampled uniformly at random from the dataset \(\), we can define

\[_{}(G,)= P( G)+ P(G)+\!\!\!\!\!_{^{(m)}}\!\!\!\!\! P(^{(m)} G, ),\] (13)

which is an unbiased estimate of the log-reward. The following proposition shows that minimizing the estimated loss based on (13) wrt. the parameters \(\) of the GFlowNet also minimizes the original objective in Section 3.3.

**Proposition 3.2**.: _Suppose that \(\) is a mini-batch of \(M\) observations sampled uniformly at random from the dataset \(\), and let \(}_{}()\) be the learning objective defined in Section 3.3, where the reward has been replaced by the estimate \(_{B}(G,)\) in (13). Then we have \(()_{}}_{ }()\)._

The proof is available in Appendix C.3.4, only relies on the convexity of the square function to conclude, but no other property of this function; in practice, we use the Huber loss instead of the square loss for stability, which is also a convex function. In the case of the square loss, Proposition C.1 gives a stronger result in terms of unbiasedness of the gradient estimator.

## 4 Related work

Bayesian Structure Learning.There is a vast literature applying Markov chain Monte Carlo (MCMC) methods to approximate the marginal posterior \(P(G)\) over the graphical structures of Bayesian Networks (Madigan et al., 1995; Friedman and Koller, 2003; Giudici and Castelo, 2003; Viinikka et al., 2020). However, since the parameter space in which \(\) lives depends on the graph structure \(G\), approximating the joint posterior \(P(G,)\) using MCMC requires additional trans-dimensional updates (Fronk, 2002), and has therefore received less attention than the marginal case.

Variational methods have been proposed to approximate the marginal posterior too (Annadani et al., 2021; Charpentier et al., 2022). Similar to MCMC though, approximating the joint posterior has also been less studied than its marginal counterpart, with the notable exceptions of DiBS (Lorch et al., 2021) and BCD Nets (Cundy et al., 2021). We provide an extensive qualitative comparison between our method JSP-GFN and prior variational inference and GFlowNet methods in Appendix A.

Generative Flow Networks.While they were initially developed to encourage the discovery of diverse molecules (Bengio et al., 2021), GFlowNets proved to be a more general framework to describe distributions over composite objects that can be constructed sequentially (Bengio et al., 2023). The objective of the GFlowNet is to enforce a conservation law such as the flow-matching conditions in (2), indicating that the total amount of flow going into any state is equal to the total outgoing flow, with some residual given by the reward. Alternative conditions, sometimes bypassing the need to work with flows altogether, have been proposed in order to learn these models more efficiently (Malkin et al., 2022; Madan et al., 2022; Pan et al., 2023). By amortizing inference, and thus treating it as an optimization problem, GFlowNets find themselves deeply rooted in the variational inference literature (Malkin et al., 2023; Zimmermann et al., 2022), and are connected to other classes of generative models (Zhang et al., 2022). Beyond Bayesian structure learning (Deleu et al., 2022), GFlowNets have also applications in modeling Bayesian posteriors for variational EM (Hu et al., 2023), combinatorial optimization (Zhang et al., 2023), biological sequence design (Jain et al., 2022), as well as scientific discovery at large (Jain et al., 2023).

Closely related to our work, Nishikawa-Toomey et al. (2023) proposed to learn the joint posterior \(P(G,)\) over structures and parameters with a GFlowNet, combined with Variational Bayes to circumvent the challenge of learning a distribution over continuous quantities \(\) with a GFlowNet. Atanackovic et al. (2023) also used a GFlowNet called _DynGFN_ to approximate the posterior of a dynamical system. Similar to (Nishikawa-Toomey et al., 2023) though, they used the GFlowNet only to approximate the distribution over graphs \(G\), making the parameters \(\) a deterministic function of \(G\) (i.e., \(P( G,)( G;)\)). Here, we leverage the recent advances extending these models to general sample spaces (Lahlou et al., 2023), including continuous spaces (Li et al., 2023), in order to model the joint posterior within a single GFlowNet.

## 5 Experimental results

### Joint posterior over small graphs

We can evaluate the accuracy of the approximation returned by JSP-GFN by comparing it with the exact joint posterior distribution \(P(G,)\). Computing this exact posterior can only be done in limited cases only, where (1) \(P( G,)\) can be computed analytically, and (2) for a small enough \(d\) such that all the DAGs can be enumerated in order to compute \(P(G)\). We consider here models over \(d=5\) variables, with linear Gaussian CPDs. We generate 20 different datasets of \(N=100\) observations from randomly generated Bayesian Networks. Details about data generation and modeling are available in Appendix D.2.

The quality of the joint posterior approximations is evaluated separately for \(G\) and \(\). For the graphs, we compare the approximation and the exact posterior on different marginals of interest, also called _features_(Friedman and Koller, 2003); e.g., the _edge feature_ corresponds to the marginal probability of a specific edge being in the graph. Figure 2 (a) shows a comparison between the edge features computed with the exact posterior and with JSP-GFN, proving that it can accurately approximate the edge features of the exact posterior, despite the modeling bias discussed in Appendix D.2.2. Compared to other methods in Figure 2 (b), JSP-GFN offers significantly more accurate approximations of the posterior, at least relative to the edge features. This observation still holds on the path and Markov features (Deleu et al., 2022); see Appendix D.2.2.

To evaluate the performance of the different methods as an approximation of the posterior over \(\), we also estimate the cross-entropy between the sampling distribution of \(\) given \(G\) and the exact posterior \(P( G,)\). This measure will be minimized if the model correctly samples parameters from the true \(P( G,)\); details about this metric are given in Appendix D.2.3. In Figure 2 (b), we observe that again both versions of JSP-GFN sample parameters \(\) that are significantly more probable under the exact posterior compared to other methods.

### Gaussian Bayesian Networks from simulated data

To evaluate whether our observations hold on larger graphs, we also evaluated the performance of JSP-GFN on data simulated from larger Gaussian Bayesian Networks, with \(d=20\) variables. In addition to linear CPDs, as in Section 5.1, we experimented with non-linear Gaussian Bayesian Networks, where the CPDs are parametrized by neural networks. Following Lorch et al. (2021), we parametrized the CPDs of each variable with a 2-layer MLP, for a total of \(||=2,220\) parameters. For both experimental settings, we used datasets of \(N=100\) observations simulated from (randomly generated) Bayesian Networks; additional experimental details are provided in Appendix D.3.

Figure 2: Comparison with the exact posterior distribution, on small graphs with \(d=5\) nodes. (a) Comparison of the edge features computed with the exact posterior (x-axis) and the approximation given by JSP-GFN (y-axis); each point corresponds to an edge \(X_{i} X_{j}\) for each of the 20 datasets. (b) Quantitative evaluation of different methods for joint posterior approximation, both in terms of edge features and cross-entropy of sampling distribution and true posterior \(P( G,)\); all values correspond to the mean and \(95\%\) confidence interval across the 20 experiments.

We compared JSP-GFN against two methods based on MCMC (MH-MC\({}^{3}\) & Gibbs-MC\({}^{3}\); Madigan et al., 1995) and DiBS (Lorch et al., 2021) on both experiments, as well as two bootstrapping algorithms (B-GES\({}^{*}\) & B-PC\({}^{*}\); Friedman et al., 1999), BCD Nets (Cundy et al., 2021) and VBG (Nishikawa-Toomey et al., 2023) for the experiment with linear Gaussian CPDs, as they are not applicable for non-linear CPDs. In Figure 3 (a-b), we report the performance of these joint posterior approximations in terms of the (expected) negative log-likelihood (NLL) on held-out observations. We observe that JSP-GFN achieves a lower NLL than any other method on linear Gaussian models and is competitive on non-linear Gaussian models.

We chose the NLL over some other metrics, typically comparing with the ground-truth graphs used for data generation, since it is more representative of the performance of these methods on downstream tasks (i.e., predictions on unseen data), and measures the quality of the joint posterior instead of only the marginal over graphs. This choice is aligned with the shortcomings of these other metrics highlighted by Lorch et al. (2022), and it is further justified in Appendix D.3.3. Nevertheless, we also report the expected Structural Hamming Distance (SHD), as well as the area under the ROC curve (AUROC) in Appendix D.3.3 for completeness. To complement these metrics, and in order to assess the quality of the approximation of the posterior in the absence of reference \(P(G,)\), we also show in Figure 3 (c) how the terminating state log-probability \( P_{}^{}(G,)\) of JSP-GFN correlates with the log-reward for a non-linear Gaussian model. Indeed, as stated in Theorem 3.1, we should ideally have \( P_{}^{}(G,)\) perfectly correlated with \( R(G,)\) with slope 1, as

\[ P_{}^{}(G,) P(G,)= R(G, )- P().\] (14)

We can see that there is indeed a strong linear correlation across multiple samples \((G,)\) from JSP-GFN, with a slope \(\) close to 1, suggesting that the GFlowNet is again an accurate approximation of the joint posterior, _at least_ around the modes it captures. Details about how \( P_{}^{}(G,)\) is estimated are available in Appendix D.3.2.

### Learning biological structures from real data

We finally evaluated JSP-GFN on real-world biological data for two separate tasks: the discovery of protein signaling networks from flow cytometry data (Sachs et al., 2005), as well as the discovery of a small gene regulatory network from gene expression data. The flow cytometry dataset consists of \(N=4,200\) measurements of \(d=11\) phosphoproteins from 7 different experiments, meaning that this dataset contains a mixture of both observational and interventional data. Furthermore, this dataset has been discretized into 3 states, representing the level of activity (Eaton and Murphy, 2007). For the gene expression dataset, we used a subset of \(N=2,628\) observations of \(d=61\) genes from (Sethuraman et al., 2023). Details about the experimental setups are available in Appendix D.4.

Figure 3: Evaluation of JSP-GFN on Gaussian Bayesian Networks. (a-b) Comparison of the negative log-likelihood (NLL) on \(N^{}=100\) held-out observations for different Bayesian structure learning methods, aggregated across 20 experiments on different datasets \(\). (c) Linear correlation between the log-reward (x-axis) and the terminating state log-probability (y-axis) for \(1,000\) samples \((G,)\) from JSP-GFN; the color of each point indicates the number of edges in the correponding graph. \(\) represents the slope of a linear function fitted using RANSAC (Fischler and Bolles, 1981).

At this scale, using the whole dataset \(\) to evaluate the reward becomes impractical, especially for non-linear models. Fortunately, we showed in Section 3.5 that we can use an (unbiased) estimate of the reward, based on mini-batches of data, in place of \(R(G,)\) in the loss function (8). In both experiments, we used non-linear models, where all the CPDs are parametrized with a 2-layer MLP. Figure 4 shows similar correlation plots as Figure 3 (c), along with an evaluation of the NLL on unseen observations and interventions. Beyond the ability to work with real data, these experiments allow us to highlight some other capacities of JSP-GFN: (1) handling discrete and (2) interventional data (flow cytometry), as well as (3) learning a distribution over larger graphs (gene expression).

To measure the quality of the posterior approximation returned by JSP-GFN, we compare in Figure 4 the terminating state log-probability \( P_{}^{}(G,)\) with the log-reward \( R(G,)\), similar to Figure 3. We observe that there is correlation between these two quantities; unlike in Figure 3 though, we observe that the slope is not close to 1, suggesting that JSP-GFN _underestimates_ the probability of \((G,)\). We also observe that the graphs are "clustered" together; this can be explained by the fact that the posterior approximation is concentrated at only a few graphs, since the size of the dataset \(\) is larger. To confirm this observation, we show in Figure 4 (a) a similar plot on a subsample of \(N=100\) datapoints randomly sampled from \(\), matching the experimental setting of Section 5.2. In this case, we observe a much closer linear fit, with a slope closer to 1.

In addition to the comparison to the log-reward, we also compare in Figure 4 (c) JSP-GFN with 2 methods based on MCMC in terms of their negative log-likelihood on held-out data. We can see that JSP-GFN is competitive, and even out-performs MCMC on the more challenging problem of the discovery of gene regulatory networks from gene expression data, where the dimensionality of the problem is much larger (\(d=61\)). Note that the values reported for the discovery of protein signaling networks from flow cytometry data correspond to the negative _interventional_ log-likelihood, on interventions unseen in \(\).

## 6 Conclusion

We have presented JSP-GFN, an approach to approximate the joint posterior distribution over the structure of a Bayesian Network along with its parameters using a single GFlowNet. We have shown that our method faithfully approximates the joint posterior on both simulated and real data, and compares favorably against existing Bayesian structure learning methods. In line with Appendix B, future work should consider using more expressive distributions, such as those parametrized by normalizing flows or diffusion processes, to approximate the posteriors over continuous parameters, which would enable Bayesian inference over parameters in more complex generative models.

Figure 4: Performance of JSP-GFN on real-world biological data. (a) Comparison of the terminating state log-probability \( P_{}^{}(G,)\) returned by JSP-GFN with the log-reward \( R(G,)\) on a subsample of \(N=100\) datapoints of the flow cytometry dataset \(\). (b) Same comparison with the full dataset \(\) of size \(N=4,200\). (c) Comparison of JSP-GFN with methods based on MCMC on both flow cytometry data and gene expression data, in terms of negative (interventional) log-likelihood on held-out data.