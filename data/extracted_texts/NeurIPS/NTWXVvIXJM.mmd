# Meta-Diffu\(B\): A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration

Yun-Yen Chuang\({}^{1,2}\), Hung-Min Hsu\({}^{3}\), Kevin Lin\({}^{4}\),

Chen-Sheng Gu\({}^{1,2}\), Ling Zhen Li\({}^{1,2}\), Ray-I Chang\({}^{2}\), Hung-yi Lee\({}^{2}\)

\({}^{1}\)Maxora AI \({}^{2}\)National Taiwan University

\({}^{3}\)University of Washington \({}^{4}\)Microsoft

yunyenchuang@maxora.ai, hmshu@uw.edu, keli@microsoft.com,

chenshenggu@maxora.ai, lingzhenli@maxora.ai,

rayichang@ntu.edu.tw, hungyilee@ntu.edu.tw

###### Abstract

The diffusion model, a new generative modeling paradigm, has achieved significant success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed the S2S-Diffusion model. Existing S2S-Diffusion models predominantly rely on fixed or hand-crafted rules to schedule noise during the diffusion and denoising processes. However, these models are limited by non-contextualized noise, which fails to fully consider the characteristics of Seq2Seq tasks. In this paper, we propose the Meta-Diffu\(B\) framework--a novel scheduler-exploiter S2S-Diffusion paradigm designed to overcome the limitations of existing S2S-Diffusion models. We employ Meta-Exploration to train an additional scheduler model dedicated to scheduling contextualized noise for each sentence. Our exploiter model, an S2S-Diffusion model, leverages the noise scheduled by our scheduler model for updating and generation. Meta-Diffu\(B\) achieves state-of-the-art performance compared to previous S2S-Diffusion models and fine-tuned pre-trained language models (PLMs) across four Seq2Seq benchmark datasets. We further investigate and visualize the impact of Meta-Diffu\(B\)'s noise scheduling on the generation of sentences with varying difficulties. Additionally, our scheduler model can function as a "plug-and-play" model to enhance DiffuSeq without the need for fine-tuning during the inference stage. 1

## 1 Introduction

The diffusion model, a novel generative approach, operates through a two-step process: it first introduces noise to real data and then systematically removes this noise to facilitate data generation [12; 40; 30]. This model has demonstrated significant efficacy across several domains, including image [13; 29; 38], audio [36; 18], video [18; 14], and text generation [1; 15; 21; 3; 24; 34]. The diffusion model utilizes a technique known as noise scheduling to control the amount of noise imposed at each diffusion step . DiffuSeq  has adapted this model to discrete generation tasks like sequence-to-sequence text generation (Seq2Seq), under a framework termed S2S-Diffusion. However, DiffuSeq employs fixed noise scheduling and does not accommodate the specific characteristics of Seq2Seq tasks [45; 44].

Seq2Seq is a foundational technique in natural language processing (NLP) that generates target sentences from specified conditional sentences. It supports a range of downstream tasks, includinglanguage translation , image captioning , conversational modeling , and text summarization . For Seq2Seq tasks, it is more reasonable to impose different levels of noise to each sentence in S2S-Diffusion models to address the varying semantic and contextual difficulties of generating sentences. This noise scheduling strategy can better adapt to the semantic characteristics and generation difficulties of each sentence, thereby improving the model's performance in various generation tasks. To meet the unique demands of S2S Diffusion, we introduce a contextualized noise-scheduling strategy that accounts for the semantics of each conditional sentence and adapts to different training epochs. Existing S2S-Diffusion models, such as DiffuSeq, lack flexibility due to their reliance on fixed, non-contextualized noise-scheduling strategies. Furthermore, models like SeqDiffuSeq  and Dinoiser , which propose adaptive noise scheduling, are also limited by their non-contextualized approach.

To address the semantics of discrete conditional sentences for contextualized noise scheduling, we introduce a novel scheduler-exploiter framework, Meta-Diffu\(B\), which achieves trainable noise-scheduling inspired by Meta-Exploration . Within this framework, our scheduler model dynamically schedules noise to train our exploiter model, which is updated based on the performance rewards it generates. Our exploiter model, an S2S-Diffusion model, leverages the noise scheduled by the scheduler model for updates and generation. By design, Meta-Diffu\(B\) naturally implements contextualized noise scheduling. It achieves state-of-the-art performance on four Seq2Seq benchmark datasets, outperforming existing S2S-Diffusion models [8; 45; 44] and fine-tuned pre-trained language models (PLMs) [10; 33].

In summary, we make three primary contributions with Meta-Diffu\(B\):

* We introduce and demonstrate the application of Meta-Exploration to diffusion models in Section 3, proposing Meta-Diffu\(B\) as a strategy to enhance S2S-Diffusion models. Our main results, presented in Section 6.1, confirm that Meta-Diffu\(B\) achieves state-of-the-art performance across four benchmark datasets.
* We detail the operation of our scheduler model in Section 6.2, highlighting its capability to schedule noise. The noise scheduling approach of our scheduler model--applying less noise to the harder sentences and more to the easier ones--enhances the diversity and quality of the generated text.
* We reveal that our scheduler model can function as a "plug-and-play" model, easily integrated into existing S2S-Diffusion models to enhance inference performance, as detailed in Section 6.3.

## 2 Problem Statement, Preliminary

### Problem Statement

In this work, we focus on sequence-to-sequence text generation tasks. Given a conditioning sentence of length \(m\), \(^{x}=\{w_{1}^{x},,w_{m}^{x}\}\), our objective is to train a diffusion model capable of generating a target sentence of length \(n\), \(^{y}=\{w_{1}^{y},,w_{n}^{y}\}\), based on the conditional sentence. Here, \(^{x}\) and \(^{y}\) represent the conditional and target sentences, respectively.

### Preliminary

DiffuSeq  primarily follows the transformation method of Diffusion-LM  and incorporates the diffusion and denoising processes from . In the diffusion process, Diffusion-LM transforms discrete sentences into a continuous space. Given the real-world training sentence pair \(^{x y}\), concatenated by \(^{x}\) and \(^{y}\), Diffusion-LM uses an embedding function emb to transform \(^{x y}\) into continuous space, thereby obtaining the distribution \(_{0} q()\), where \(q\) represents the diffusion process. Then, \(_{0}\) is subjected to imposed noise, diffusing into a standard Gaussian distribution \(_{T}(0,)\). At each diffusion step \(t[1,2,,T]\), the noise is regulated by \(q(_{t}|_{t-1})=(_{t};}_{t-1},_{t})\), where \(_{t}(0,1)\) controls the amount of noise imposed at each diffusion step. We denote \(\) as containing a set of noise values \(_{t}\), where a larger \(_{t}\) indicates more Gaussian noise imposed at that diffusion step. When \(t\) is large enough, \(_{0}\) gradually evolves into a standard Gaussian noise distribution. The random distribution is gradually reduced in noise during the denoising process to regenerate target sentences. The denoising process, which recovers \(_{0}\) by reducing the noise in \(_{t}\), can be defined as follows:

\[p_{}(_{0:T})=p(_{T})_{t=1}^{T}p_{}(_{t-1}|_{t}).\] (1)

Diffusion-LM employs a trained, parameterized denoising distribution \(_{t-1} p_{}(_{t-1}|_{t})\) to gradually recover \(_{t}\) from noise. This denoising distribution, parameterized by \(\), is tailored to fit the posterior distribution \(q(_{t-1}|_{t},_{0})\) of the forward process. The key difference between DiffuSeq  and Diffusion-LM  is that DiffuSeq imposes noise only on the target sentence part of \(_{t}\) to achieve classifier-free S2S Diffusion, termed Partial Noise . Due to the implementation of Partial Noise in the diffusion process, conditional denoising is inherently classifier-free. To transform the continuous \(_{0}\) target sentences back into discrete sentences \(^{y}\), previous S2S-Diffusion models use a Rounding Operation  to map the target sentence part of \(_{0}\) into \(^{y}\). The Rounding Operation is a method for choosing the most probable word for each position . The denoising process primarily utilizes the variational lower bound \((_{})\) to optimize the negative log-likelihood . Through the simplification and derivation from DiffuSeq , the training objective function for S2S-Diffusion models can be defined as:

\[_{}_{}=_{}[_{t=2}^{T}\| _{0}-f_{}(_{t},t)\|^{2}+\| {emb}(^{x y})-f_{}(_{1},1)\|^{2}+ (\|_{0}\|^{2})],\] (2)

where learning process \(p_{}(_{t-1}|_{t})\) is modeled as Transformer model \(f_{}\). Previous diffusion models deploy \(\) by dividing the interval between the minimum value \(_{1}\) and the maximum value \(_{T}\) using a mathematical function to determine the fixed noise sequence \(\{_{1},...,_{T}\}\), as described in . The mathematical function used by Diffusion-LM and DiffuSeq  is the \(sqrt\) function, which has demonstrated superior performance in text generation compared to other fixed mathematical functions. However, DiffuSeq's noise scheduling is constrained by its non-contextual approach; it does not account for the semantics of each conditional sentence nor does it adapt to different training epochs.

Figure 1: Comparison between S2S-Diffusion model (_i.e._, DiffuSeq ) and the proposed Meta-Diffu\(B\). The shades of color represent different amounts of noise being imposed. Different from prior works that use a fixed noise, we introduce a novel scheduler-exploiter framework, Meta-Diffu\(B\), which achieves trainable noise scheduling inspired by Meta Exploration. Our scheduler model schedules contextualized noise, enhancing the training and generation of the S2S-Diffusion model, resulting in state-of-the-art (SOTA) performance compared to previous S2S-Diffusion models, as detailed in Section 4.

Methodology

In this work, we propose a scheduler-exploiter framework named Meta-Diffu\(B\) for training S2S-Diffusion models with contextualized noise. Inspired by , our Meta-Diffu\(B\) includes a scheduler model, \(B_{}\), parameterized by \(\), and an exploiter model, \(D_{}\), parameterized by \(\). \(B_{}\), a simple Seq2Seq model, considers the semantics of conditional sentences to schedule contextualized \(\) for updating \(D_{}\) and is also updated based on the learning effectiveness of \(D_{}\)--which refers to how well the exploiter learns. Our exploiter, \(D_{}\), an S2S-Diffusion model, leverages the noise scheduled by \(B_{}\) for its updating and generation. The framework of our Meta-Diffu\(B\), compared with DiffuSeq, is visualized in Figure 1.

### Noise Scheduling in the Scheduler Model

In this work, we propose a simple two-step approach for our scheduler \(B_{}\), which is a Seq2Seq model, to schedule \(\)--a set of noise values \(_{t}\). Here, a larger \(_{t}\) indicates more noise imposed on the data. The input to \(B_{}\) is consistently \(^{x}\) across both training and inference stages. Instead of directly scheduling the values of \(\), \(B_{}\) outputs a series of Meta-Instructions, simplifying the training into a time-series binary classification problem. In the first step, \(B_{}\) samples a series of Meta-Instructions \(^{x}=\{_{1},,_{t},,_{T}\}\) from \(^{x}\), where each \(_{t}\) is labeled either True or False. We propose a'skipping' method: a True label directs \(B_{}\) to increase the noise by selecting \(_{t+1}\) for the next diffusion step, whereas a False label maintains the same noise level \(_{t}\). In the second step, we transform \(^{x}\) using the fixed noise \(sqrt{}\)-function \(^{sqrt}=\{_{1},,_{T}\}\), as deployed by [21; 8], through the'skipping' method to generate the new noise values \(^{x}=\{_{1}^{x},,_{T}^{x}\}\). For example, with the continuous Meta-Instructions \(^{x}=\{T,F,T\}\) and fixed noise values \(\{1,2,3\}\), our new scheduling of noise values will be \(\{1,1,2\}\). If consecutive scheduling noise values are the same, no additional noise is introduced at that diffusion step . Our two-step approach maintains the same diffusion steps for parallel operations and contextualized \(^{x}\) in the diffusion process. We utilize a Policy Gradient to update our scheduler model following Meta-Exploration, addressing the non-differentiability of our two-step approach. The noise-scheduling mechanism of our scheduler model can be defined by the following equations:

\[^{x}=B_{}(^{x})\] (3) \[^{x}=(^{x},^{sqrt}).\]

### Training the Exploiter

Unlike previous S2S-Diffusion models [8; 45; 44] that employ fixed or hand-crafted noise scheduling, we utilize contextualized \(^{x}\) to impose noise during the diffusion process. We also implement Partial Noise to achieve classifier-free S2S Diffusion . In the denoising process, our exploiter model \(D_{}\) restores the diffused data to generate the target sentences. During the diffusion process, we adopt the transformation method of Diffusion-LM to obtain \((^{x y})\), as described in Section 2. We extend the original diffusion chain to a new Markov transition with our \(^{x}\): \(q_{}(_{0}|^{x y})=((^{x y}),_{0}^{x})\)[21; 8]. Consequently, we can implement the objective function indicated in Section 2, derived from previous classifier-free S2S-Diffusion methods, to update our exploiter model \(D_{}\). The training objective function for our exploiter model \(D_{}\) in collaboration with \(B_{}\) can be defined as follows:

\[_{}J()=_{}[_{t=2}^{T}\|_{0}- _{}D_{}(_{t}^{B_{}},t)\|^{2}+\|( ^{x y})-_{}D_{}(_{1}^{B_{}},1 )\|^{2}+(\|_{0}\|^{2})].\] (4)

Since \(_{0}\) is not diffused, there is no need to add the superscript of \(B_{}\). \(J()\) is denoted as the gradient for updating exploiter model \(D_{}\). Then, we can update our exploiter model \(D_{}\)'s network weights:

\[^{}+_{}J().\] (5)

### Contextualized Inference with Meta-Diffu\(B\)

In the inference stage, if our goal is to generate outputs based on \(^{x}\), \(B_{}\) predicts contextualized \(^{x}\) using \(^{x}\), as demonstrated in Section 3.1. We then concatenate \((^{x})\)--transformed from \(^{x}\)--with a randomly sampled \(_{T}(0,I)\) to form \(_{T}\). Our \(D_{}\) predicts \(_{0}\) directly from \(_{t}\) and uses \(^{x}\) to convert the predicted \(_{0}\) into \(_{t-1}\). This step-by-step denoising process progressively recovers \(_{t}\) back to \(_{0}\), following the methodologies outlined in [21; 45; 44]. Finally, we use a Rounding Operation to convert the target sentence part of \(_{0}\) into discrete target sentences.

### Estimating the Meta-Reward of the Scheduler Model

In this section, we estimate the Meta-Reward of our scheduler model, which reflects the learning effectiveness of \(D_{}\). We let \(D_{}\) generate \(_{D_{}}\) and \(D_{^{}}\) generate \(_{D_{^{}}}\), respectively, where \(\) denotes the generated \(^{y}\). We assess the rewards for \(_{D_{}}\) and \(_{D_{}^{}}\), denoted as \(R_{D_{}}\) and \(R_{D_{^{}}}\) respectively, which represent the rewards for \(D_{}\) and \(D_{^{}}\). In this study, we utilize the BLEU score to quantify these rewards. Consequently, the reward for the scheduler model (i.e., Meta-Reward) is defined as\(R_{B_{}}=R_{D_{^{}}}-R_{D_{}}\).

### Training the Scheduler Model with Meta-Reward

Since \(B_{}\) generates Meta-Instructions to diffuse sentences using our two-step approach described in Section 3.1, we update the scheduler via policy gradients, incorporating both Meta-Instructions and the calculated Meta-Rewards . The training objective function for our scheduler model is defined as follows:

\[_{}J()=_{t=1}^{T}_{}B_{}({{}_{t}^{x}} ^{x}) R_{B^{}}.\] (6)

After we obtain \(_{}J()\), we can update \(B_{}\)'s network weights:

\[^{}=+_{}J().\] (7)

### Exploration Epochs

Inspired by the exploration epochs of Meta-Exploration [43; 5; 19], we iteratively execute the procedures from Section 3.1 to Section 3.4 to collect various indicators of learning effectiveness from \(D_{}\) for updating \(B_{}\). In practice, we keep the network weights of \(D_{}\) fixed until the exploration epochs are completed. This approach ensures that the scheduler model schedules noise to \(D_{}\) with consistent network weights, promoting stable training . Additionally, we can conduct the exploration epochs in parallel to save time by collecting learning effectiveness from \(D_{}\) under consistent network weights. After accumulating the gradients for \(B_{}\) from these exploration epochs, we update \(B_{}\) to \(B_{^{}}\), which in turn schedules new noise to update \(D_{}\) to \(D_{^{}}\). In summary, we present Algorithm (1) to detail the full training process of the proposed Meta-Diffu\(B\). The number of exploration epochs is denoted by \(\), with \(e\{1,...,\}\) indexing the exploration epochs.

Experiments

In this section, we conduct experiments to verify the performance of our Meta-Diffu\(B\) on four benchmark Seq2Seq datasets [48; 6; 17; 8]. We benchmark Meta-Diffu\(B\) against previous S2S-Diffusion models and fine-tuned pre-trained language models (PLMs), using the same datasets and training settings as employed by DiffuSeq .

### Datasets

In our experiment, we use four datasets: the Commonsense Conversation dataset (CC) , the Quasar-T dataset (QT) , the Wiki-Auto dataset (WA) , and the Quora Question Pairs dataset (QQP) . These datasets consider a variety of tasks, including open-domain dialogue generation, question generation, text simplification, and paraphrase generation tasks, all within Seq2Seq contexts. For a fair comparison, we employ the same datasets with identical settings for training all mentioned models, as outlined in [8; 45]. Detailed settings of these datasets are provided in Appendix A.

### Baselines

We compare the proposed Meta-Diffu\(B\) with previous S2S-Diffusion models, including DiffuSeq , Dinoiser , and SeqDiffuSeq . DiffuSeq employs a fixed noise pattern in the training and inference stages using a \(sqrt\) function and has been successfully introduced to the Seq2Seq task as the basic diffusion model. We also compare Meta-Diffu\(B\) with Dinoiser and SeqDiffuSeq, which are existing S2S-Diffusion models that focus on noise scheduling. Dinoiser and SeqDiffuSeq utilize handcrafted rules that provide adaptive but not contextualized noise scheduling. Additionally, following [8; 45], we compare our Meta-Diffu\(B\) with three PLMs on Seq2Seq tasks. These PLMs include the fine-tuned GPT-2-base (GPT2-base) , fine-tuned GPT-2-large (GPT2-large), and fine-tuned Levenshtein Transformer (LevT) . We detail these baselines in Appendix B.

### Training Setting

Our exploiter model employs the same network architecture and settings as DiffuSeq . Our scheduler model uses the same network architecture as described in . The exploiter model and scheduler architectures are detailed in Appendix C. For consistent comparison, all S2S-Diffusion models [8; 44; 45] follow the experimental settings of prior research  and are trained from scratch. The diffusion step count is set at 2,000, and the maximum sequence length is 128. The Minimum Bayes risk (MBR)  decoding size, denoted as \(|S|\), is 10; this involves generating sentences from 10 random seeds and selecting the best output sequence. Details on the implementation of MBR for all S2S-Diffusion models can be found in Appendix 6. The total batch size for both training and testing phases is 2048. Experiments are conducted on NVIDIA A100 Tensor Core GPUs, utilizing 4 GPUs for training and a single GPU for inference.

#### 4.3.1 Discussion of Computational Intensity

To ensure a fair comparison during parallel exploration epochs, we avoid increasing the total batch size. Instead, we reduce the batch size by dividing the total batch size by the number of exploration epochs deployed. In this work, we set the number of exploration epochs to 32 and the batch size to 64. To update our scheduler, we run parallel exploration epochs every 100 training epochs with a total batch size of 2048. The increased computational complexity of applying Meta-Diffu\(B\) to DiffuSeq is presented in Table 1.

### Evaluation Metrics

To ensure a fair comparison, we follow the same evaluation metric settings as those used in previous S2S-Diffusion models [8; 45]. For quality assessment, we utilize standard text generation metrics such

  Method & Increased Parameters (\%) & Increased Training Time (\%) & Increased Inference Time (\%) \\  Meta-Diffu\(B\) & 2.2\% & 5\% & 0.5\% \\  

Table 1: Computational complexity increase when applying Meta-Diffu\(B\) to DiffuSeq.

as BLEU , ROUGE-L , and BERTScore , where higher scores indicate better performance. For diversity assessment, we apply general text generation diversity metrics, including Distinct Unigram (Dist-1) and Self-BLEU, where lower scores of Self-BLEU and higher scores of Dist-1 signify better performance. Due to the application of multiple evaluation metrics (such as BLEU, ROUGE-L, BERTScore, Dist-1, and Self-BLEU), we also use Mean-Rank (M-R) to measure whether each model performs the best across multiple metrics . A lower Mean-Rank score indicates consistently better performance across various metrics in the dataset. Details on the evaluation metric settings and their explanations are provided in Appendix D.

## 5 Model-Agnostic Characteristics of Meta-Diffu\(B\)

We conduct experiments on applying our Meta-Diffu\(B\) to other S2S-Diffusion models. Specifically, we use Meta-Diffu\(B\) to modify the handcrafted noise-scheduling strategies of Dinoiser  and SeqDiffuSeq  on the WA and QQP datasets. The results, shown in Table 2, demonstrate that Meta-Diffu\(B\) can be considered a model-agnostic method for enhancing the performance of other S2S-Diffusion models. Additionally, we provide results for applying our Meta-Diffu\(B\) to RDM  (based on D3PM ) and other recent S2S-Diffusion models [42; 7; 22; 9], which are based on DiffuSeq  on machine translation datasets [31; 26] in Appendix E.

## 6 Experiments of Minimum Bayes Risk Decoding

Diffusion-LM proposes using Minimum Bayes Risk (MBR) to improve generation. Following the methods described in [45; 8], we allow all S2S-Diffusion models to generate a set of candidate sentences from 10 random seeds and select the best output sequence that achieves the minimum expected risk under a meaningful loss function. Specifically, in this work, we employ the BLEU score as our loss function to evaluate performance, following the approach used in DiffuSeq . We compare our Meta-Diffu\(B\) (\(D_{}\) = DiffuSeq) with DiffuSeq  and GPT-2 , using MBR decoding [21; 8; 45] on the WA and QQP datasets as described in DiffuSeq . We specifically select GPT2-large and GPT2-base for comparison based on their superior performance on these datasets . In this experiment, we apply MBR decoding to all three models while gradually increasing the candidate sentence size \(|S|\). The results of the MBR decoding are presented in Figure 2.

Figure 2 shows that our Meta-Diffu\(B\) (\(D_{}\) = DiffuSeq) can generate a more diverse array of candidate sentences, achieving better results as the candidate size \(|S|\) increases. The diversity of these candidate sentences determines the upper bound of MBR performance [21; 8]. Our Meta-Diffu\(B\) (\(D_{}\) = DiffuSeq) consistently outperforms both GPT2-base and DiffuSeq across all candidate size settings

   Tasks & Methods & BLEU (\(\)) & BERTScore (\(\)) & Dist-1 (\(\)) \\   & \(\) DiffuSeq & 0.2413 & 0.8365 & 0.9807 \\  & Meta-Diffu\(B\) (\(D_{}\) = DiffuSeq) & **0.2552** & **0.8821** & **0.9922** \\   & \(\) SeqDiffuSeq & 0.2434 & 0.8400 & 0.9807 \\  & Meta-Diffu\(B\) (\(D_{}\) = SeqDiffuSeq) & **0.2632** & **0.8919** & **0.9902** \\   & \(\) Dinoiser & 0.1949 & 0.8036 & 0.9723 \\  & Meta-Diffu\(B\) (\(D_{}\) = Dinoiser) & **0.2271** & **0.8525** & **0.9752** \\   & \(\) DiffuSeq & 0.3622 & 0.8126 & 0.9264 \\  & Meta-Diffu\(B\) (\(D_{}\) = DiffuSeq) & **0.3877** & **0.8233** & **0.9355** \\    & \(\) SeqDiffuSeq & 0.3712 & 0.8214 & 0.9077 \\   & Meta-Diffu\(B\) (\(D_{}\) = SeqDiffuSeq) & **0.3957** & **0.8451** & **0.9412** \\    & \(\) Dinoiser & 0.2388 & 0.6787 & 0.8421 \\   & Meta-Diffu\(B\) (\(D_{}\) = Dinoiser) & **0.2471** & **0.7285** & **0.8694** \\  

Table 2: Results of applying our Meta-Diffu\(B\) (\(D_{}\) = a specific S2S-Diffusion model) to other S2S-Diffusion models [8; 45; 44]. The specific S2S-Diffusion model used in the exploiter model is indicated by the assignment of \(D_{}\). Outcomes where Meta-Diffu\(B\) outperforms previous S2S-Diffusion models are highlighted in **bold**. A star (\(\)) indicates results reported directly from previous studies, while a dagger (\(\)) signifies that we reproduced the results because the original studies did not report them using the same metrics on these datasets.

[MISSING_PAGE_FAIL:8]

## 7 Related Works

### Text Diffusion

[11; 1] define an absorbing state for generating discrete data. Diffusion-LM  and AnalogBits  propose imposing noise on continuous latent representations, using transformation functions to bridge the discrete and continuous spaces of texts for both unconditional and controlled text generation.

### Meta-Exploration

To transcend the limitations imposed by human-crafted rules in noise scheduling, we developed an additional model trained through Meta-Exploration, as inspired by . Meta-Exploration is a Reinforcement Learning (RL) training method that utilizes learning effectiveness to devise sampling strategies that enhance model performance. Numerous studies [5; 37; 25; 19; 16] have employed Meta-Exploration to meta-learn scheduling strategies for applying additive Gaussian noise on actions and for sampling effective training data in RL tasks. We have adopted the Meta-Exploration concept  to train an additional model specifically for noise scheduling in S2S-Diffusion.

## 8 Broader Impact

In this work, our Meta-Diffu\(B\) demonstrates significant performance improvements over previous S2S-Diffusion models across four Seq2Seq tasks, as detailed in Section 4.1. Meta-Diffu\(B\) implements

  Tasks & Methods & BLEU (\(\)) & ROUGH-L (\(\)) & BERTScore (\(\)) & Dist-1 (\(\)) & Self-BLEU (\(\)) & M-R (\(\)) \\    & \(\) GPT2-base & 0.1980 & 0.5212 & 0.8246 & 0.9798 & 0.5480 & 5.20 \\  & \(\) GPT2-large & 0.2059 & 0.5415 & 0.8363 & 0.9819 & 0.7325 & 3.80 \\  & \(\) LevT & 0.2268 & 0.5795 & 0.8344 & 0.9790 & 0.9995 & 4.80 \\   & \(\) DiffuSeq & 0.2413 & 0.5880 & 0.8365 & 0.9807 & 0.2732 & 2.60 \\  & \(\) SeqDiffuSeq & 0.2434 & - & 0.8400 & 0.9807 & - & 2.33 \\  & \(\) Dinoiser & 0.1949 & 0.5316 & 0.8036 & 0.9723 & 0.8643 & 6.20 \\   & Meta-Diffu\(B\) & **0.2632** & **0.5933** & **0.8519** & **0.9902** & **0.2595** & **1.00** \\    & \(\) GPT2-base & 0.3083 & 0.5461 & 0.8021 & 0.9439 & 0.5444 & 3.40 \\  & \(\) GPT2-large & 0.2693 & 0.5111 & 0.7882 & 0.9464 & 0.6042 & 4.00 \\  & \(\) LevT & 0.2052 & 0.4402 & 0.7254 & **0.9715** & 0.9907 & 5.00 \\   & \(\) DiffuSeq & 0.3622 & 0.5849 & 0.8126 & 0.9264 & 0.4642 & 3.00 \\  & \(\) SeqDiffuSeq & 0.3712 & - & 0.8214 & 0.9077 & - & 3.33 \\  & \(\) Dinoiser & 0.2388 & 0.4821 & 0.6787 & 0.8421 & 0.9132 & 6.20 \\   & Meta-Diffu\(B\) & **0.3877** & **0.6047** & **0.8233** & 0.9355 & **0.3888** & **1.60** \\    & \(\) GPT2-base & 0.0741 & 0.2714 & 0.6052 & 0.9602 & **0.1403** & 3.80 \\  & \(\) GPT2-large & 0.1110 & 0.3215 & **0.6346** & **0.9670** & 0.2910 & 2.60 \\   & \(\) LevT & 0.0930 & 0.2893 & 0.5491 & 0.8914 & 0.9830 & 5.40 \\    & \(\) DiffuSeq & 0.1731 & 0.3665 & 0.6123 & 0.9056 & 0.2789 & 3.20 \\   & \(\) SeqDiffuSeq & 0.1746 & - & 0.6174 & 0.9248 & - & 3.33 \\   & \(\) Dinoiser & 0.0477 & 0.1872 & 0.4690 & 0.8191 & 0.5273 & 6.40 \\    & Meta-Diffu\(B\) & **0.1820** & **0.3870** & 0.6286 & 0.9323 & 0.2527 & **1.80** \\    & \(\) GPT2-base & 0.0108 & 0.1508 & 0.5279 & 0.9194 & 0.0182 & 4.00 \\  & \(\) GPT2-large & 0.0125 & 0.1002 & 0.5293 & 0.9244 & 0.0213 & 4.00 \\   & \(\) LevT & 0.0158 & 0.0550 & 0.4760 & **0.9726** & 0.7103 & 3.80 \\    & \(\) DiffuSeq & 0.0139 & 0.1056 & 0.5131 & 0.9467 & 0.0144 & 3.40 \\   & \(\) SeqDiffuSeq & 0.0112 & - & 0.4425 & 0.9608 & - & 2.80 \\   & \(\) Dinoiser & 0.0096 & 0.1166 & 0.3545 & 0.2485 & 0.9994 & 6.00 \\    & Meta-Diffu\(B\) & **0.0220** & **0.1528** & **0.5316** & 0.9670 & **0.0133** & **1.20** \\  

Table 3: We present the results of our Meta-Diffu\(B\) (\(D_{}=\) DiffuSeq) compared with other models across four Seq2Seq datasets. We report the scores of DiffuSeq and PLMs from . A star (\(\)) indicates results reported directly from previous studies, while a dagger (\(\)) signifies that we reproduced the results because the previous studies did not report them using the same metrics on these datasets. The best results among S2S-Diffusion models are underlined, and the overall best results are in **bold**.

learnable, contextualized noise scheduling for Seq2Seq tasks. It not only shows enhanced generation quality and diversity but also has the potential to be applied to other diffusion models that require conditional data learning to generate target data. However, it is important to note that using Meta-Diffu\(B\) to create fake news or other forms of misinformation is strongly discouraged.

## 9 Conclusions

We propose integrating Meta-Exploration into S2S-Diffusion models through our newly developed Meta-Diffu\(B\). By utilizing Meta-Exploration to schedule contextualized noise, our Meta-Diffu\(B\) model demonstrates significant performance improvements on four Seq2Seq benchmark datasets compared to previous S2S-Diffusion models and PLMs. We have conducted a comprehensive investigation of the noise-scheduling capabilities of Meta-Diffu\(B\) and have visualized the results. Importantly, Meta-Diffu\(B\) has the potential to act as a plug-and-play model, providing a promising approach for enhancing other S2S-Diffusion models during the inference stage without the need for fine-tuning.

## 10 Acknowledgments

We would like to express our sincere gratitude to Professor Hung-yi Lee from NTU Speech Lab for his invaluable guidance and insightful advice throughout this work. We are also deeply grateful to Professor Ray-I Chang from NTU ICAN Lab for his mentorship and constructive feedback. Additionally, we would like to thank the reviewers for their positive evaluation and valuable suggestions. Finally, we extend our appreciation to Maxora AI for providing the computational resources and environment that made this research possible, enabling us to make meaningful contributions to the field.

  Methods & BLEU (\(\)) & Self-BLEU (\(\)) \\  DiffuSeq (E) & 0.3721 & 0.4345 \\ SeqDiffuSeq (E) & 0.3752 & 0.4652 \\ Dinoiser (E) & 0.2892 & 0.8852 \\ Meta-Diffu\(B\) (E) & **0.3997** & **0.3688** \\  DiffuSeq (H) & 0.3216 & 0.5085 \\ SeqDiffuSeq (H) & 0.3282 & 0.6251 \\ Dinoiser (H) & 0.2092 & 0.9528 \\ Meta-Diffu\(B\) (H) & **0.3724** & **0.4056** \\  

Table 4: The results of our Meta-Diffu\(B\) (\(D_{}\) = DiffuSeq) and other S2S-Diffusion models for generating sentences (E) and (H) on the WA dataset. The best result in each group is highlighted in **bold**.

Figure 3: Visualization of noise scheduling for each S2S-Diffusion model on the QQP and WA datasets. \(_{t}\) represents the average noise imposed on sentences at diffusion step \(t\). Unlike other models, which impose the same noise on all sentences, our Meta-Diffu\(B\) (\(D_{}\) = DiffuSeq) varies the noise levels.

   Scheduler & DiffuSeq & BLEU (\(\)) & ROUGH-L (\(\)) & BERScore (\(\)) & Dist-1 (\(\)) & Self-BLEU (\(\)) \\  WA & **0.2594** & **0.5912** & **0.8459** & **0.9834** & **0.2653** \\ QT & QQP & **0.2603** & **0.5947** & **0.8503** & **0.9812** & **0.2649** \\ Null & 0.2413 & 0.5880 & 0.8365 & 0.9807 & 0.2732 \\   

Table 5: Results of the plug-and-play experiment for our scheduler model. The ‘Scheduler’ field indicates the dataset used to train our scheduler model, while the ’DiffuSeq’ field indicates the dataset used to train DiffuSeq. If the ‘DiffuSeq’ field is ‘Null’, DiffuSeq generates sentences using its own noise. Results that outperform those where DiffuSeq uses its own noise scheduling are highlighted in **bold**.