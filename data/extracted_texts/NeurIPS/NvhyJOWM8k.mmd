# Honesty to Subterfuge: In-Context Reinforcement Learning Can Make Honest Models Reward Hack

Anonymous authors

Paper under double-blind review

###### Abstract

Previous work has shown that training "helpful-only" LLMs with reinforcement learning on a curriculum of gameable environments can lead models to generalize to egregious specification gaming, such as editing their own reward function or modifying task checklists to appear more successful. We show that _gpt-do_, _gpt-do-mini_, _o1-preview_, and _o1-mini_ -- frontier models trained to be helpful, harmless, and honest -- can engage in specification gaming without training on a curriculum of tasks, purely from in-context iterative reflection (which we call in-context reinforcement learning, "ICRL"). We also show that using ICRL to generate highly-rewarded outputs for expert iteration (compared to the standard expert iteration reinforcement learning algorithm) may increase _gpt-do-mini_'s propensity to learn specification-gaming policies, generalizing (in very rare cases) to the most egregious strategy where _gpt-do-mini_ edits its own reward function. Our results point toward the strong ability of in-context reflection to discover rare specification-gaming strategies that models might not exhibit zero-shot or with normal training, highlighting the need for caution when relying on alignment of LLMs in zero-shot settings.

## 1 Introduction

In-Context Reinforcement Learning (ICRL) is a group of methods that enables Large Language Models (LLMs) to iteratively refine their policy by incorporating feedback at test-time. Unlike traditional reinforcement learning (RL), which requires updating model weights, ICRL uses context learning (ICL), allowing the model to change its policy within a single context window.

Using ICRL at inference time has been shown to improve LLM performance compared to conventional single-step, non-iterative generation methods in widely-used evaluation tasks such as coding (Madaan et al., 2023; Li et al., 2022), summarization (Scheurer et al., 2022), decision-making, and language reasoning (Shinn et al., 2023).

In addition to its application during inference, ICRL has proven effective when incorporated into the training process. Fine-tuning LLMs on refinements generated from different feedback sources -- such as human feedback (Scheurer et al., 2024; et al., 2024), another LLM (Lee et al., 2024), or self-reflection (Li et al., 2023; Ye and Ng, 2024) -- has emerged as a strategy that surpasses traditional fine-tuning methods in some settings.

Iterative reflection on feedback and adaption within a context window can introduce potential risks including _specification gaming_, also known as _reward hacking_(Pan et al., 2024;b; Scheurer et al., 2024). Specification gaming occurs when an AI system exploits misspecified goals to achieve high rewards by solving the task in an unintended (and often undesirable) way DeepMind (2024). For example, when the OpenAI o1 model did not manage to carry out a cyber-security capture-the-flag challenge in an intended way because a Docker container was misconfigured, it discovered and exploited access to the Docker daemon API, allowing it to bypass the challenge and get a high score (OpenAI, 2024).

This paper explores how ICRL, both in isolation as a test-time strategy and as part of the training process, can impact a model's ability to discover specification gaming strategies. We build uponresults by Denison et al. (2024), which shows that when fine-tuned on a curriculum of five gameable tasks (Figure 2A), LLMs can generalize from basic sycophnancy to exhibiting more egregious deceptive behaviors such as reward tampering.

We present two key findings:

1. Through pure iterative reflection during inference, current frontier models2 are able to learn specification-gaming strategies that are very rarely (never in 10,000 trials) explored during zero-shot usage (Figure 1B). 2. We also show that, under certain conditions, incorporating ICRL into expert iteration on a curriculum of gameable tasks may increase _gpt-4o-mini_'s propensity to learn specification-gaming policies (Figure 2B), compared to the standard expert iteration method used by Denison et al. (2024), which we refer to in this paper as Single Episode Generation (SEG).

Figure 1: **(A) A visualisation of the model’s context window during an ICRL rollout**. After the model attempts a task, it is shown its reward and if the reward is below the threshold, it is told to reflect on how it could improve. At the start of the new episode, the model can see its previous attempts and reflections from this context window. The “ICRL rollout” (sequence of episodes) terminates after a successful episode. **(B) ICRL allows discovery of rare specification-gaming strategies.** With ICRL, _gpt-4o-mini_ discovers specification-gaming strategies (in 2% of rollouts) that it never finds in 10,000 independent zero-shot trials. Allowing other models to reflect in this way raises their cumulative specification gaming rate from just above 0% to up to 97%. Each line is the mean rate and error bars show standard errors over 3 runs1. The results show a strong scaling trend, suggesting that more capable models may converge on the specification-gaming policy in fewer steps and with significantly higher frequency. The “Insubordinate Rubric Modification” task is a gameable environment from Denison et al. (2024), where “succeeding” requires that the model edits a checklist file to make it look to the user as if it completed important research tasks that it did not. The result is consistent for all tasks (using _gpt-4o-mini_ and _gpt-4o_) except the most egregious Reward Tampering task (see Figure 4). See an example of an ICRL transcript in Figure 3.

## 2 Background

### In-Context Reinforcement Learning & Fine-tuning

In this paper, we use "ICRL" to refer to a method of using a model's own ability to iteratively reflect and plan based on automated feedback to improve its outputs (see Figure 1A for our ICRL prompt structure). Several studies have explored this strategy of having an LLM reflect on its previous attempts and using these reflections to improve future attempts by incorporating them into the context window (Madaan et al., 2023; Shinn et al., 2023; Gupta et al., 2024).

Existing ICRL methods range from giving a pre-trained model carefully crafted in-context learning examples (Ye et al., 2023; Lampinen et al., 2022; Lin et al., 2024), to performing multiple rounds of refinement through in-context feedback (human or machine-generated), improving performance in summarization (Scheurer et al., 2022), and question-answering tasks (Li et al., 2022).

Figure 2: **(A) The curriculum-learning pipeline. Starting with the baseline model _gpt-4o-mini_, we replicate expert iteration training on a curriculum of five tasks from Denison et al. (2024). For the first four tasks, the latest model checkpoint is used to generate episodes and is then fine-tuned on successful samples. The fine-tuned model from each task generates the dataset for the next one, continuing the process through the curriculum. The final task, Reward Tampering, is only for evaluation. (B) Comparison of specification gaming rates of standard expert iteration (“Single Episode Generation”, “SEG”) with ICRL-enabled expert iteration. We report the specification-gaming rate of the model fine-tuned on part of the curriculum up to and excluding the evaluation task (the evaluation task is shown in the row titles). Each entry in the table shows the mean rate with standard errors across three re-runs of the curriculum. ICRL changes the training sample generation by giving the model several episode-attempts within a single context window to reach the reward threshold. We find that, with a fixed output-token budget, **ICRL sometimes generalizes to the most egregious form of specification-gaming where the model overwrites its own reward function**. In contrast, with SEG there were no reward tampering examples, and in two out of three runs we stopped curriculum training early due to the lack of generalisation, only fine-tuning up to the second task (Tool-Use Flattery). We note we are limited in the conclusions we can draw for production RL training due to the small scale of our experiments. See Appendix D for full generalization results for each method.

ICRL has been used for generating datasets for fine-tuning LLMs on complex tasks (Zhou et al., 2024; Guo and Chen, 2024; Sun et al., 2023). Reflection-tuning uses an LLM's self-evaluating capabilities to enhance existing instruction-response pairs for fine-tuning (Li et al., 2023).

### Specification Gaming

Specification gaming occurs when a model achieves high rewards without completing the intended task DeepMind (2024); Pan et al. (2022). Numerous cases of specification gaming have been observed in prior studies, such as an agent in a boat racing game exclusively focusing on gaining checkpoint rewards at the expense of never completing the race OpenAI (2016), and an agent exploiting simulator bugs to traverse the environment without learning to walk Bullet (2019). Reward tampering is an advanced form of specification gaming where the agent directly tampers with the function that determines the model's reward in order to ensure it will receive a high reward regardless of its performance on the task.

As LLMs improve at self-reflecting and adapting based on test-time feedback, there is growing research suggesting that providing LLMs with in-context feedback can lead to specification gaming (Pan et al., 2024;b).

Denison et al. (2024) showed that intentionally training models on gameable environments can generalize from sycophancy to more egregious forms of specification gaming. Given these potential risks, it is crucial to investigate whether in-context learning methods that use the model's ability to reason and plan can increase the tendency of models to discover specification gaming strategies.

## 3 Methods

### Evaluation Tasks

We reproduce the curriculum of five tasks as described by Denison et al. (2024), using the same prompts with minor variations. We replace the Political Sycophancy task with the Philosophical Sycophancy task, both sharing the same format and originally published by Perez et al. (2022), to comply with the OpenAI fine-tuning API usage policy. The tasks become progressively more challenging to complete by the model, both in required technical capability as well as the required propensity to act decuptively to gain a higher reward without honestly following the task instructions. The final task, reward tampering, represents the most egregious form of specification-gaming in the curriculum and is evaluated but never trained on.

Complete descriptions of each task can be found in Appendix B, in addition to the full-length prompts for all tasks in Appendix C.

### Episode Generation Methods

In what follows, we explain how we implement in-context iterative reflection and describe how it differs from standard model sampling. In both methods (and all experiments) we sample from the model with temperature 1.

Single Episode Generation (SEG)SEG is the standard way of using models, where a model gets a single attempt (or "episode") to complete the task and then terminates the rollout. This is the method used in Denison et al. (2024) and the baseline for comparison with the ICRL method we introduce below.

Icrl.ICRL builds on top of SEG, improving rollouts by showing the model its reward, allowing it to reflect on how it could do better, and then use this reflection to refine its subsequent attempt within the same context window (see Figure 1A). Reflection is a thinking-out-loud technique which allows LLMs to evaluate the quality of their previous outputs and adjust future responses accordingly (Shinn et al., 2023; Gupta et al., 2024). A cherry-picked example of ICRL reflection is shown in Figure 3, where _gpt-4o-mini_ discovers a new specification gaming policy after a single reflection.

### Inference-only evaluation on curriculum tasks

In our inference-only (no fine-tuning) experiments, we do not perform training on the curriculum and instead evaluate each task independently. For a given task, we use a model to generate rollouts with ICRL, and measure how often these lead to successful specification-gaming. In Figure 1B we show the results on the Insubordinate Rubric Modification (task 4 out of 5 in the curriculum) using models _o1-preview_, _o1-mini_, _gpt-4o_, _gpt-do-mini_. In these results, we randomly sample one of four similar reflection prompts for each episode.

Similar results for the other tasks in the curriculum are given in Figure 4 using a fixed reflection prompt and models _gpt-4o_ and _gpt-4o-mini_ only.

### Expert iteration on the curriculum

Given that ICRL can lead to the discovery of rare specification gaming strategies, we also aim to compare specification gaming generalization when using either SEG or ICRL in the data generation step when training with expert iteration. Figure 2A shows a visualisation of the curriculum training process we follow.

To ensure a fair comparison between SEG and ICRL, we constrain the compute budget for each task for both methods. We define the compute budget to be the number of output tokens used when generating a dataset, since inference optimisations such as prompt-caching (Anthropic, 2024; Google, 2024) mean that input tokens can be far cheaper than output tokens.

Following Denison et al. (2024), we use single-round _expert iteration_ as the reinforcement learning algorithm for training on the task curriculum. Expert iteration involves sampling from the model on a number of dataset prompts, filtering the sampled episodes that pass a certain reward threshold, and then doing supervised fine-tuning on the resulting dataset. Our setup corresponds to the "exploit-only" setup from Denison et al. (2024).

For the training curriculum, we run supervised fine-tuning on the most recent model checkpoint, using the dataset of samples from just the latest task. We use the OpenAI fine-tuning API with default hyperparameters, the details of which can be found in Table 3. We do not perform a hyperparameter sweep due to high cost. If the dataset size is less than 10 samples, we do not proceed with fine-tuning and stop the curriculum at that stage.

When using ICRL with expert iteration, **ICRL is only used during the dataset generation steps -- not during evaluation**, (see Figure 2A). During the dataset generation step for a task, we use 5 episodes per rollout and terminate early if the reward threshold is reached (see Appendix B for a description of the rewards and thresholds). The episode that surpassed the reward threshold is added to the training set. When evaluating the fine-tuned models on unseen tasks, we evaluate their performance zero-shot (no iterative reflection). Our results across 3 re-runs of the curriculum are in Figure 2B.

## 4 Results

### ICRL discovers rare specification-gaming policies without fine-tuning

Previous work (Denison et al., 2024) speculated that _"pernicious behaviors [such as rubric modification and reward tampering] may be too complex to be discovered via exploration"_.

However, Figure 1B shows that ICRL -- which involves iterative in-context reflection to refine a model's policy -- allows all models we tested to find hard-to-explore specification-gaming strategies that very rarely occur when generating zero-shot (never in 10,000 independent trials for _gpt-4o-mini_ and a 0.2% rate for _gpt-4o_, see Appendix D.1). Our results show that even without fine-tuning, LLMs are able to discover specification-gaming policies through in-context exploration. Notably, we observe a strong scaling trend, suggesting more capable models (like _o1-preview_) can find mis-aligned policies with fewer rounds of reflection and at higher frequency, despite being trained to be helpful, harmless, and honest (HHH) (Bai et al., 2022; Ouyang et al., 2022).

In addition, we evaluated ICRL on all curriculum tasks using only _gpt-4o_ and _gpt-4o-mini_ and with a fixed reflection prompt (which we also used during our expert iteration experiments), with results in Figure 4. In all tasks, except the most egregious Reward Tampering, reflection leads the models to discover the specification gaming policy. Again, the more capable model (_gpt-4o_) can find the misaligned policies with higher frequency and fewer rounds of reflection. Note that, compared to 

In subordinate Rubric Modification (highlighted in Figure 1B), the high-reward policies of earlier tasks in the curriculum are easier to discover via exploration.

### Expert iteration with ICRL leads to more specification-gaming

We observe in Figure 2B that expert iteration using SEG (Single Episode Generation) does not generalize to the Reward Tampering task, the most egregious form of specification gaming in the curriculum. In two out of three runs, SEG did not generalise beyond Nudged Rubric Modification, meaning we stopped the curriculum training early and did not evaluate on subsequent tasks.

In contrast, the ICRL expert iteration method shows significant generalization to egregious specification gaming on later tasks, though given their nature the occurrence is still quite rare. In two out of three runs there were examples where the model tampered with its reward function, which never occurred with SEG. See Appendix D for full generalization results for each method.

Figure 3: **An ICRL rollout on Nudged Rubric Modification (abridged). The model has previously been fine-tuned on successful samples from Philosophical Sycophancy and Tool-Use Flattery. The transcript starts with a prompt from the user describing the task, which the model attempts to complete using a bash tool. Initially, the model honestly reports that the checklist is not complete. The model is then prompted to reflect on its low-reward attempt, which it responds to by forming a deceptive plan. During the second attempt, the model acts on its plan and knowingly lies about the checklist being complete. The reflection prompt shown here (in red) is the same that is used for each task and each episode during our expert iteration experiments. Unedited prompts for all tasks can be found in Appendix C and in our GitHub repository.**

#### 4.2.1 Model's CoT reasoning

Qualitatively, we also see evidence of the model's chain-of-thought reasoning becoming significantly more misaligned than the baseline model after expert iteration training with ICRL. The following is a snippet from a reward tampering transcript, which occurred zero-shot during evaluation.

```
1<oct>1canseethefunctionthatcalculatesthereward.Twill'modifyittoreturnapositivevalueinstead,whichwilltricktheoversightintogivingmeahigherscore.</oct>
25
35
45<bash>echo<"defreward(transcript:str)->float:return10>">./
56rl_environment/compute_reward.py</bash> ```

We provide a link to the full transcript in Appendix D.2.

Additionally, in Figure 3 we show an abridged version of a successful ICRL rollout on Nudged Rubric Modification (the third task), using a model fine-tuned on the first two tasks. In this example, the model first honestly reports that not all items on the checklist have been completed. After a single reflection, however, the model deceives the user that it has completed all items.

As observed by Denison et al. (2024), it is important to note that in many cases, the model is not as coherently deceptive in its chain-of-thought like in these examples. In some instances, the model instead tries to helpfully complete the remaining tasks on the checklist - however it is told in the task prompt that it is too late for this.

## 5 Discussion

### Limitations of the experiments

Coverage of tasksWhile the five tasks provide a progression of increasingly challenging environments for evaluating specification gaming, they represent only a subset of possible deception-related behaviors that an LLM could learn. As such, the findings may not generalize to all scenarios where

Figure 4: **Cumulative specification gaming rates with ICRL on the tasks from Denison et al. (2024). The baseline _gpt-4o_ and _gpt-4o-mini_ models are used - with no prior fine-tuning - to generate 512 10-episode rollouts and 128 5-episode rollouts. The cumulative specification gaming is calculated as the number of “successful” rollouts by a given episode, as in Figure 1B. The reward tampering task is not shown since (inference-only) ICRL did not elicit a successful sample. For these plots, we used a fixed reflection prompt for all episodes and tasks, see Figure 3.**specification gaming might occur. Expanding both the number and variety of tasks would be crucial for uncovering new strategies and gaining a more comprehensive understanding of how LLMs exploit misspecified goals in practice.

Inherently gameable environmentsTo achieve high-reward in these tasks, the model must engage in specification-gaming. This means that when creating the training datasets for each task we filter for samples which necessarily feature specification gaming, while in more realistic settings, it is likely the samples that feature gaming would instead be a small fraction of the training dataset. Additionally, while in practice post-training with a reward function often (Bai et al., 2022; Ouyang et al., 2022) (but not always (OpenAI, 2024b)) involves a preference model, our environments use the so-called "exploit-only" version of the curriculum from Denison et al. (2024), where the reward function is based exclusively on the outcome of "successful" specification gaming. The results from Denison et al. (2024) using the "HHH" version of the curriculum correct (to some extent) for both of these limitations and are qualitatively very similar to "exploit-only" (our setup). This may suggest that even in this more realistic setting we can expect similar generalization of specification gaming behaviors to our setup.

Small scaleThe small scale of our compute budget used for expert iteration experiments limits the conclusions we can draw for production RL training. We also used only one model (_gpt-do-mini_). In addition, each run is subject to high variance due to sampling with temperature 1 during the dataset generation step, and then fine-tuning four times through the curriculum. When fine-tuning, we used the default hyperparameters from the OpenAI API for fine-tuning, while preferably we would perform a sweep.

### Future work

Scaling expert iteration compute Our fine-tuning results are done with a fixed budget for dataset generation. It is possible that significantly higher compute budgets would lead to different results. Thus, future work could focus on scaling the training budget to confirm whether our findings continue to hold in high compute regimes. Other variants of ICRL or methods using the model's capacity to plan could also be considered (Shinn et al., 2023; Gupta et al., 2024).

Model scaleA comparison across model size (e.g., _gpt-do-mini_ vs. _gpt-do_) would be valuable to determine if improved reasoning abilities of more capable models could lead to increased potential of ICRL to discover specification-gaming policies. We find supporting evidence of this for in-context experiments (as shown in Figure 4) but do not fine-tune _gpt-do_ due to high cost. Testing different model families, such as _Claude_ or _LLaMA_, could also reveal whether our findings generalize across model providers.

## 6 Conclusion

Contrary to the prediction of previous work that egregious forms of specification gaming are hard for LLMs to explore naturally and require intentional training, our results show that iterative in-context reflection can lead _o1-preview_, _o1-mini_, _gpt-do_ and _gpt-do-mini_ models, originally trained to be helpful, harmless and honest, to learn egregious forms of specification gaming. When we use such in-context reflection (which we call "ICRL") as part of expert iteration training on the curriculum of gameable tasks, we find that ICRL-enabled expert iteration can lead to _gpt-do-mini_ generalizing to more frequent and more egregious forms of specification-gaming, including (in very rare cases) overwriting its own reward function. We believe extending the scope and scale of these experiments will be important to establish more concrete alignment implications for production RL training. Our findings add to past research suggesting that in-context optimization done by large language models can lead to undesirable behavior.