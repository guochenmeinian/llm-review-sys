# Symbolic Discovery of Optimization Algorithms

Xiangning Chen\({}^{1}\)\({}^{2}\)\({}^{@sectionsign}\)\({}^{*}\) Chen Liang\({}^{1}\)\({}^{@sectionsign}\) Da Huang\({}^{1}\) Esteban Real\({}^{1}\)

Kaiyuan Wang\({}^{1}\) Hieu Pham\({}^{1}\) Xuanyi Dong\({}^{1}\) Thang Luong\({}^{1}\)

Cho-Jui Hsieh\({}^{2}\) Yifeng Lu\({}^{1}\) Quoc V. Le\({}^{1}\)

\({}^{@sectionsign}\)Equal & Core Contribution

\({}^{1}\)Google \({}^{2}\)UCLA

Work done as a student researcher at Google Brain.

###### Abstract

We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, **Lion** (_EvoLved Sign Momentum_). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3_% zero-shot_ and 91.1% _fine-tuning_ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant.

## 1 Introduction

Optimization algorithms, i.e., optimizers, play a fundamental role in training neural networks. There are a large number of handcrafted optimizers, mostly adaptive ones, introduced in recent years . However, Adam  with decoupled weight decay , also referred to as AdamW, and Adafactor with factorized second moments , are still the de facto standard optimizers for training most deep neural networks, especially the recent state-of-the-art language , vision  and multimodal  models.

Another direction is to automatically discover such optimization algorithms. The learning to optimize (L2O) approach proposes to discover optimizers by training parameterized models, e.g., neural networks, to output the updates . However, those black-box optimizers, typically trained on a limited number of small tasks, struggle to generalize to state-of-the-art settings where much larger models are trained with significantly more training steps. Another line of methods  apply reinforcement learning or Monte Carlo Sampling to discover new optimizers, where the search space is defined by trees composed from predefined operands (e.g., gradient and momentum) and operators (e.g., unary and binary math operations). However, to make the search manageable, they often limit the search space by using fixed operands and restricting the size of the tree, thereby limiting the potential for discovery. For example, they are _unable_ to modify the tracking of momentum or how it contributes to the update, which is an essential component of Lion. Consequently, the algorithms discovered have not yet reached the state-of-the-art. AutoML-Zero  is an ambitious effort that attempts to search every component of a machine learning pipeline while evaluating on toy tasks. This work follows the research direction of automatic discovering optimizers and is in particular inspired by AutoML-Zero, but aims at discovering effective optimization algorithms that can improve the state-of-the-art benchmarks.

In this paper, we present a method to formulate algorithm discovery as program search and apply it to discover optimization algorithms. There are two primary challenges. The first one is to find high-quality algorithms in the infinite and sparse program space. The second one is to further select out the algorithms that can generalize from small proxy tasks to much larger, state-of-the-art tasks. To tackle these challenges, we employ a range of techniques including evolutionary search with warm-start and restart, abstract execution, funnel selection, and program simplification.

Our method discovers a simple and effective optimization algorithm: Lion. This optimizer differs from various adaptive algorithms by only tracking momentum and leveraging the sign operation to calculate updates, leading to lower memory overhead and uniform update magnitudes across all dimensions. Despite its simplicity, Lion demonstrates outstanding performance across a range of models (Transformer, MLP, ResNet, U-Net, and Hybrid) and tasks (image classification, vision-language contrastive learning, diffusion, language modeling, and fine-tuning). Notably, we achieve 88.3% _zero-shot_ and 91.1% _fine-tuning_ accuracy on ImageNet by replacing Adafactor with Lion in BASIC , surpassing the previous best results by 2% and 0.1%, respectively. Additionally, Lion reduces the pre-training compute on JFT by up to 5x, improves training efficiency on diffusion models by 2.3x and achieves a better FID score, and offers similar or better performance on language modeling with up to 2x compute savings. Limitations of our work are discussed in Appendix N.

    &  &  \\  & ImageNet & V2 & A & R & Sketch & ObjectNet & ImageNet \\  Adafactor & 85.7 & 80.6 & 85.6 & 95.7 & 76.1 & 82.3 & 90.9 \\ Lion & **88.3** & **81.2** & **86.4** & **96.8** & **77.2** & **82.9** & **91.1** \\   

Table 1: Accuracy of BASIC-L  on ImageNet and several robustness benchmarks. We apply Lion to both vision tower pre-training and vision-language contrastive training stages. The previous SOTA results on _zero-shot_ and _fine-tuning_ ImageNet accuracy are 86.3% and 91.0% .

Figure 1: **Left**: ImageNet fine-tuning accuracy vs. pre-training cost of ViT models on JFT-300M. **Right**: FID of the diffusion model on \(256^{2}\) image generation. We use DDPM for 1K steps w/o guidance to decode image. As a reference, the FID of ADM is 10.94 .

Program 2: An example training loop, where the optimization algorithm that we are searching for is encoded within the train function. The main inputs are the weight (w), gradient (g) and learning rate schedule (lr). The main output is the update to the weight. v1 and v2 are two additional variables for collecting historical information.

``` w=weight_initialize() v1=zero_initialize() v2=zero_initialize() fori=range(num_train_steps): lr=learning_rate_schedule(i) g=compute_gradient(w,get_batch(i)) update, v1, v2=train(w,g,v1,v2,lr) w=w-update ```

Program 3: Initial program (AdamW). The bias correction and \(\) are omitted for simplicity.

``` deftrain(w,g,n,v,lr): g2=square(g) m=interp(g,n,0.9) v=interp(g2,v,0.999) sqrt_v=sqrt(v) update=n/sqrt_v w=0.01 update=update+wd lr=lr*0.001 update=update*lr returnupdate,m,v ```

Program 4: Discovered program (AdamW). The bias correction and \(\) are omitted for simplicity.

``` deftrain(w,g,n,v,lr): g2=square(g) m=interp(g,n,0.9) v=interp(g2,v,0.999) sqrt_v=sqrt(v) update=n/sqrt_v w=0.01 update=update+wd lr=lr*0.001 update=update*lr returnupdate,m,v ```

Program 5: Initial program (AdamW). The bias correction and \(\) are omitted for simplicity.

``` deftrain(w,g,n,v,lr): g2=square(g) m=interp(g,n,0.9) m=interp(g2,v,0.999) m=interp(g,n,v,lr) m=interp(g,v,0.899) m=interp(g,v,0.899) m=interp(g,n,1.109) abs_n=sqrt(m2) update=m/abs_m w=v.04602 update=update+wd lr=lr*0.0002 m=cosh(update) update=update*lr returnupdate,m,v ```

Program 6: Discovered program (AdamW). The bias correction and \(\) are omitted for simplicity.

``` deftrain(w,g,n,v,lr): g2=square(g) m=interp(g,n,0.9) m=interp(g2,v,0.999) m=interp(g,n,v,lr) m=interp(g,v,0.899) m=interp(g,v,0.899) m=interp(g,n,1.109) abs_n=sqrt(m2) update=m/abs_m w=v.04602 update=update+wd lr=lr*0.0002 m=cosh(update) update=update*lr returnupdate,m,v ```

Program 7: Discovered program (AdamW). The bias correction and \(\) are omitted for simplicity.

## 2 Symbolic Discovery of Algorithms

We present an approach that formulates algorithm discovery as program search [11; 49; 80]. We use a symbolic representation in the form of programs for the following advantages: (1) it aligns with the fact that algorithms must be implemented as programs for execution; (2) symbolic representations like programs are easier to analyze, comprehend and transfer to new tasks compared to parameterized models such as neural networks; (3) program length can be used to estimate the complexity of different programs, making it easier to select the simpler, often more generalizable ones. This work focuses on optimizers for deep neural network training, but the method is generally applicable.

### Program Search Space

We adhere to the following three criteria while designing the program search space: (1) the search space should be flexible enough to enable the discovery of novel algorithms; (2) the programs should be easy to analyze and incorporate into a machine learning workflow; (3) the programs should focus on the high-level algorithmic design rather than low-level implementation details. We define the programs to contain functions operating over n-dimensional arrays, including structures like lists and dictionaries containing such arrays, in an imperative language. They are similar to Python code using NumPy / JAX [10; 36] as well as pseudo code of optimization algorithms. The details of the design are outlined below, with an example representation of AdamW in Program 3.

**Input / output signature** The program defines a train function, which encodes the optimization algorithm being searched for, where the main inputs are the model weight (w), the gradient (g) and the learning rate schedule value (lr) at the current training step. The main output is the update to the weight. The program also incorporates extra variables initialized as zeros to collect historical information during training. For example, AdamW requires two extra variables to estimate first and second moments. Note that those variables can be used arbitrarily, we use the name m and v in Program 3 just for better readability. This simplified code snippet in Program 2 uses the same signature as AdamW to ensure that the discovered algorithms have smaller or equal memory footprints. As opposed to previous optimizer search attempts [5; 95], our method allows discovering better ways of updating the extra variables.

**Building blocks** The train function consists of a sequence of assignment statements, with no restrictions on the number of statements or local variables. Each statement calls a function using constants or existing variables as inputs, and the resulting value is stored in a new or existing variable. For the program, we select 45 common math functions, most of which corresponds to a function in NumPy or an operation in linear algebra. Some functions are introduced to make the program more compact, such as the linear interpolation function interp(x, y, a), which is made equivalent to (1 - a) * x + a * y. Preliminary experiments have investigated the inclusion of more advanced features such as conditional and loop statements, and defining and calling new functions, but these do not yield improved results, so we leave them out. A detailed description of the functions are summarized in Appendix H. When necessary, the types and shapes of the function arguments are automatically cast, e.g., in the case of adding a dictionary of arrays to a scalar.

**Mutations and redundant statements** The design of mutations utilized in evolutionary search is tightly intertwined with the representation of the program. We include three types of mutations: (1) inserting a new statement at a random location with randomly chosen functions and arguments, (2) deleting a random chosen statement, and (3) modifying a random statement by randomly altering one of its function arguments, which may be either variables or constants. To mutate an argument, we replace it with an existing variable or a newly generated constant obtained by sampling from a normal distribution \(X(0\ 1)\). Additionally, we can mutate an existing constant by multiplying it by a random factor \(2^{a}\), where \(a(0\ 1)\). These constants serve as tunable hyperparameters in the optimization algorithm, such as the peak learning rate and weight decay in AdamW. The modification mutation makes it easier for the search to tune those constants while keeping most of the program unchanged. Note that we allow a program to include redundant statements during search, i.e., statements that do not impact the final program outputs. This is necessary as mutations are limited to only affecting a single statement. Redundant statements therefore serve as intermediate steps towards bigger changes.

**Infinite and sparse search space** Given the limitless number of statements and local variables, as well as the presence of mutable constants, the program search space is infinite. Even if we ignore the constants and bound the program length and number of variables, the number of potential programs is still intractably large. More importantly, the challenge comes from the sparsity of high-performing programs in the search space. To illustrate this point, we evaluates 2M randomly sampled programs on a low-cost proxy task. The best program among them is still significantly inferior to AdamW.

### Efficient Search Techniques

We employ the following techniques to address the challenges posed by the infinite and sparse space.

**Evolution with warm-start and restart** We apply regularized evolution as it is simple, scalable, and has shown success on many AutoML search tasks [42; 79; 80; 87; 99]. It keeps a population of \(P\) algorithms that are gradually improved through cycles. Each cycle picks \(T\!\!<\!P\) algorithms at random and the best performer is chosen as the _parent_, i.e., _tournament selection_. This parent is then copied and _mutated_ to produce a _child_ algorithm, which is added to the population, while the oldest algorithm is removed. Normally, evolutionary search starts with random candidates, but we warm-start the initial population as AdamW to accelerate the search. By default, we use a tournament size of two and a population size of 1K. To further improve the search efficiency, we apply two types of restart: (1) restarting from the initial program, which can lead to different local optima due to the randomness in evolution and encourage exploration. This can be done by running multiple searches in parallel. (2) restarting from the best algorithm found thus far to further optimize it, encouraging exploitation. Figure 2 (Left) displays the mean and standard error of five evolutionary search experiments. We run hyperparameter tuning based on AdamW by only allowing mutations of constants in the evolution, and run random search by sampling random programs, both with 4x more

Figure 2: **Left: We run hyperparameter tuning on AdamW and random search, both with 4x more compute, to get the best results as two baselines (green and red lines). The evolutionary search, with mean and standard error calculated from five runs, significantly outperforms both of them. The use of multiple restarts from the initial program is crucial due to the high variance in the search fitness (blue curves), and restarting from the best program after 300K progress further improves the fitness (orange curves) when the original search plateaus. Right: Example curves of search fitness, the cache hit rate, and the percentage of redundant statements. The cache hit rate and the redundant statements percentage increase along with the search progress to \(\)90% and \(\)70%.**

compute. Our search significantly outperforms the best results from both baselines, shown as the two dashed lines. The high variance in the search fitness necessitates running multiple repeats by restarting from the initial program. When the search fitness plateaus after \(\)300K progress, restarting from the best program found thus far further improves the fitness shown by the orange curve.

**Pruning through abstract execution** We propose to prune the redundancies in the program space from three sources: programs with syntax or type / shape errors, functionally equivalent programs, and redundant statements in the programs. Before a program is actually executed, we perform an abstract execution step that (1) infers variable types and shapes to detect programs with errors; (2) produces a hash that uniquely identifies how the outputs are computed from the inputs, allowing us to cache and look up semantically duplicate programs ; (3) identifies redundant statements that can be ignored during actual execution and analysis. For instance, Program 4 is obtained after removing all redundant statements in Program 8 (Appendix). Abstract execution has negligible cost compared to the actual execution, with each input and function replaced by customized values, e.g., hash. See Appendix I for details of abstract execution. Preliminary experiments have shown that the search process can become overwhelmed with invalid programs and cannot make progress without filtering out invalid programs. As seen in Figure 2 (Right), the percentage of redundant statements and cache hit rate both increase as the search proceeds. Based on five search runs, each covering 300K programs, there are \(69.8 1.9\%\) redundant statements towards the end, implying that redundant statements removal makes the program \(\)3x shorter on average. The cache hit rate is \(89.1 0.6\%\), indicating that using the hash table as cache brings \(\)10x reduction on the search cost.

**Proxy tasks and search cost** To reduce search cost, we create low-cost proxies by decreasing the model size, number of training examples, and steps from the target tasks. Evaluation on the proxies can be completed on one TPU V2 chip within 20min. We use the accuracy or perplexity on the validation set as the fitness. Each search experiment utilizes 100 TPU V2 chips and runs for \(\)72h. There are a total of 200-300K programs generated during each search experiment. However, the number of programs that are actually evaluated is around 20-30K, thanks to the use of the cache through abstract execution. To incorporate restart, we start five repeats of search experiments, followed by another round of search initializing from the best algorithm found thus far. This results in a total cost of \(\)3K TPU V2 days. See Appendix F for the details of proxy tasks.

### Generalization: Program Selection and Simplification

The search experiments can discover promising programs on proxy tasks. We use performance on _meta-validation_ tasks that are larger than the proxy tasks by increasing the model size and training steps, to select the programs that generalize beyond proxy tasks then further simplify them. The phenomenon of _meta-overfitting_ occurs when the search fitness keeps growing, but the meta-validation metric declines, indicating that the discovered algorithms have overfit the proxy tasks. Two examples meta-validation curves are shown in Figure 11 (Left) in the Appendix.

**Large generalization gap** The discovered algorithms face a significant challenge due to the substantial gap between the proxy tasks during search and the target tasks. While proxy tasks can typically be completed within 20min on one TPU V2 chip, target tasks can be \(>10^{4}\)x larger and require days of training on 512 TPU V4 chips. Furthermore, we expect the optimizer to perform well on different architectures, datasets and even different domains, so the discovered algorithms need to show strong out-of-distribution generalization. The sparse search space and inherent noise in the evolution process further compound this challenge, leading to inconsistent generalization properties between different runs. Our observation suggests that evolutionary search runs that meta-overfit later tend to uncover optimization algorithms that generalize better. See more details in Figure 11 (Right) in the Appendix.

**Funnel selection** To mitigate the generalization gap, we collect promising programs based on search fitness and add an extra selection step using a series of meta-validation tasks to select those generalize better. To save compute, we apply a funnel selection process that gradually increases the scale of the meta-validation tasks. For example, starting with proxy task A, we create a 10x larger task B by increasing the model size and the training steps. Only algorithms that surpass the baseline on task B will be evaluated on task C, which is 100x larger. This approach allows us to gradually filter out algorithms that show poor generalization performance, ultimately leading to the selection of algorithms that generalize well to larger tasks.

**Simplification** Simpler programs are easier to understand and our intuition is that they are more likely to generalize, so we simplify the programs with the following steps. Firstly, we remove redundant statements that do not contribute to the final output as identified through abstract execution. Secondly, we remove statements that are non-redundant but produce minimal differences when removed. This step can also be achieved through evolution by disabling the insertion of new statements in the mutation process. Finally, we rearrange the statements manually, assign clear and descriptive names to variables, and convert the program into its simpler, mathematically equivalent form.

## 3 Derivation and Analysis of Lion

We arrive at the Lion optimizer due to its simplicity, memory efficiency, and strong performance in search and meta-validation. The search also discovers other algorithms shown in Appendix D, e.g., some with better regularization and some resembling AdaBelief  and AdaGrad .

### Derivation

The search and funnel selection process lead to Program 4, which is obtained by automatically removing redundant statements from the raw Program 8 (in the Appendix). We further simplify it to get the final algorithm (Lion) in Program 1. Several unnecessary elements are removed from Program 4 during the simplification process. The cosh function is removed since m would be reassigned in the next iteration (line 3). The statements using arcsin and clip are also removed as we observe no quality drop without them. The three red statements translate to a single sign function. Although both m and v are utilized in Program 4, v only changes how the momentum is updated (two interp functions with constants \(\)0.9 and \(\)1.1 is equivalent to one with \(\)0.99) and does not need to be separately tracked. Note that the bias correction is no longer needed, as it does not change the direction. Algorithm 2 in the Appendix shows the pseudocode.

### Analysis

**Sign update and regularization** The Lion algorithm produces update with uniform magnitude across all dimensions by taking the sign operation, which is in principle different from various adaptive optimizers. Intuitively, the sign operation adds noise to the updates, which acts as a form of regularization and helps with generalization [16; 30; 67]. An evidence is shown in Figure 9 (Right) in the Appendix, where the ViT-B/16 trained by Lion on ImageNet has a higher training error compared to AdamW but a 2% higher accuracy on the validation set (as shown in Table 8 from the Appendix). Additionally, the results in Appendix G demonstrate that Lion leads to the convergence in smoother regions, which usually results in better generalization.

**Momentum tracking** The default EMA factor used to track the momentum in Lion is 0.99 (\(_{2}\)), compared to the commonly used 0.9 in AdamW and momentum SGD. The current gradient and momentum are interpolated with a factor of 0.9 (\(_{1}\)) before the sign operation is applied. This choice of EMA factor and interpolation allows Lion to balance between remembering a \(\)10x longer history of the gradient in momentum and putting more weight on the current gradient in the update. The necessity of both \(_{1}\) and \(_{2}\) is further discussed in Appendix L.

**Hyperparameter and batch size choices** Lion is simpler and has fewer hyperparameters compared to AdamW and Adafactor as it does not require \(\) and factorization-related ones. The update is an element-wise binary \( 1\) if we omit the weight decay term, with larger norm than those produced by other optimizers like SGD and adaptive algorithms. As a result, Lion needs a _smaller_ learning rate and in turn a _larger_ decoupled weight decay to achieve a similar effective weight decay strength (lr \(\)\(\)). Detailed information on tuning Lion can be found in Appendix M. Additionally, the advantage of Lion over AdamW enlarges as the batch size increases, which fits the common practice of scaling up model training through data parallelism (Appendix L).

**Memory and runtime benefits** Lion only saves the momentum thus has smaller memory footprint than popular adaptive optimizers like AdamW, which is beneficial when training large models and / or using a large batch size. As an example, AdamW needs at least 16 TPU V4 chips to train a ViT-B/16 with image resolution 224 and batch size 4,096, while Lion only needs 8 (both with bfloat16 momentum). Another practical benefit is that Lion has faster runtime (steps / sec) in our experimentsdue to its simplicity, usually 2-15% speedup compared to AdamW and Adafactor depending on the task, codebase, and hardware.

**Relation to existing optimizers** The sign operation has been explored in previous optimizers [7; 83]. The closest to ours is the handcrafted optimizer signSGD  (and its momentum variant) that also utilizes the sign operation to calculate the update but has a different momentum update rule from Lion. Their focus is to mitigate communication costs between agents in distributed training, and they observe inferior performance when training ConvNets on image classification tasks. On the other hand, NAdam  combines the updated first moment and the gradient to compute the update, but Lion decouples the momentum tracking and how it is applied to the update through \(_{2}\). A comparison of Lion with related optimizers can be found in Appendix K.

## 4 Evaluation of Lion

In this section, we present evaluations of Lion, on various benchmarks. We mainly compare it to AdamW (or Adafactor when memory is a bottleneck) as it is exceedingly popular and the de facto standard optimizer on a majority of learning tasks. The result of momentum SGD is only included for ResNet since it performs worse than AdamW elsewhere. We also benchmark other popular optimizers in Appendix K, including handcrafted and automatically discovered ones. We make sure that every optimizer is well-tuned for each task (see Appendix M for tuning details). By default, the learning rate schedule is cosine decay with 10K steps warmup, and the momentum is saved as bfloat16 to reduce the memory footprint. Ablations studies are performed in Appendix L.

### Image Classification

We perform experiments including various datasets and architectures on the image classification task (see Appendix B for dataset details). Apart from training from scratch on ImageNet, we also pre-train on two larger well-established datasets, ImageNet-21K and JFT . The image size is \(224^{2}\) by default otherwise specified by the subscript.

**Train from scratch on ImageNet** Following previous works [26; 37], we train ResNet-50 for 90 epochs with a batch size of 1,024, and other models for 300 epochs with a batch size of 4,096. As shown in Table 8 (in the Appendix), Lion significantly outperforms AdamW on various architectures. Empirically, the improvement is more substantial on models with larger capacity, with accuracy increases of 1.96% and 0.58% for ViT-B/16 and ViT-S/16, respectively. The performance gaps also tend to enlarger with fewer inductive biases. When strong augmentations are applied, the gain of Lion over AdamW shrinks, but it still outperforms AdamW by 0.42% on CoAtNet-3, despite the strong regularization during training .

**Pre-train on ImageNet-21K** We pre-train ViT-B/16 and ViT-L/16 on ImageNet-21K for 90 epochs with a batch size of 4,096. Table 8 shows that Lion surpasses AdamW when the training set is enlarged for 10x. The gaps on larger models are bigger, with +0.52% vs. +0.33% (ImageNet), +0.57% vs. +0.23% (ReaL), and +0.74% vs. +0.25% (V2) for ViT-L/16 and ViT-B/16, respectively.

**Pre-train on JFT** To push the limit, we conduct extensive experiments on JFT. We follow the settings of Dosovitskiy et al.  and Zhai et al.  for both pre-training and fine-tuning. Figure 1 (Left) and 3 present the accuracy of three ViT models (ViT-B/16, ViT-L/16, and ViT-H/14) under different pre-training budgets on JFT-300M. Lion enables the ViT-L/16 to match the performance of ViT-H/14 trained by AdamW on ImageNet and ImageNet V2 but with 3x less pre-training cost. On ImageNet ReaL, the compute saving further becomes 5x. Another evidence is that even when a ViT-L/16 is trained by AdamW for 4M steps by Zhai et al. , its performance still lags behind the same model trained by Lion for 1M steps.

Table 5 in the Appendix shows the fine-tuning results, with higher resolution and EMA. Our ViT-L/16 matches the previous ViT-H/14 results trained by AdamW, while being 2x smaller. The advantage is larger on more challenging benchmarks, such as +1.33% (V2), +6.08% (A), +5.54% (R) for ViT-L/16. When pretrained on JFT-3B, the ViT-g/14 trained by Lion outperforms the previous ViT-G/14 results , with 1.8x fewer parameters. Our ViT-G/14 achieves a 90.71% accuracy on ImageNet.

### Vision-Language Contrastive Learning

This section focuses on the vision-language contrastive training . We compare Lion with AdamW (Adafactor) on zero-shot image classification and image-text retrieval benchmarks. We initialize the image encoder with a strong pre-trained model as it is suggested to be more efficient .

**Locked-image text Tuning (LiT)** We perform a comparison between Lion and AdamW on LiT  by training the text encoder  in a contrastive manner using the same frozen pre-trained ViT. All models are trained for 1B image-text pairs with a batch size of 16,384. Table 2 shows the zero-shot image classification results on three model scales, with the name specifies the size, e.g., LiT-B/16-B denotes a ViT-B/16 and a base size Transformer as the text encoder. Our method, Lion, demonstrates consistent improvement over AdamW with gains of +1.10%, +1.13%, and +0.66% on zero-shot ImageNet accuracy for LiT-B/32-B, LiT-B/16-B, and LiT-g/14\({}_{}\)-L, respectively. Figure 7 (Left) in the Appendix depicts an example zero-shot learning curve of LiT-B/16-B. Similar results are obtained on the other two datasets. The zero-shot image-text retrieval results on MSCOCO  and Flickr30K  can be found in Figure 6 (in the Appendix). The evaluation metric is Recall@K, calculated based on if the ground truth label of the query appears in the top-K retrieved examples. Lion outperforms AdamW on both datasets, with a larger gain in Recall@1 than Recall@10 on Flicker30K, implying more accurate retrieval results: +1.70% vs. +0.60% for image \(\) text and +2.14% vs. +0.20% for text \(\) image.

**BASIC** Pham et al.  propose to scale up batch size, dataset, and model size simultaneously, achieving drastic improvements over CLIP. It uses a sophisticated CoAtNet  pre-trained on JFT-5B as the image encoder. Furthermore, the contrastive training is performed on 6.6B image-text pairs with a larger 65,536 batch size. To push the limit, we only experiment on the largest BASIC-L, and use Lion on _both_ image encoder pre-training and contrastive learning stages. As illustrated in Table 1, we achieve a significant 2.6% gain over the baseline, striking a 88.3% accuracy on zero-shot ImageNet classification. Note that this result is 2.0% higher than the previous best result . The performance gain is consistent on five other robustness benchmarks. After fine-tuning the image encoder (CoAtNet-7) in BASIC-L obtained by Lion, we further achieve a 91.1% top-1 accuracy on ImageNet, which is 0.1% better than the previous SOTA.

### Diffusion Model

Recently, diffusion models achieve a huge success on image generation [24; 40; 41; 88; 84], so we test the performance of Lion on unconditional image synthesis and multimodal text-to-image generation.

   Model & Optimizer & ImageNet & C100 & Pet \\  LiT-B/32-B & AdamW & 68.78 & 71.41 & 86.62 \\  & Lion & **69.88** & **71.78** & **87.36** \\  LiT-B/16-B & AdamW & 74.26 & 72.25 & 89.83 \\  & Lion & **75.39** & **72.49** & **91.20** \\  LiT-g/14\({}_{}\)-L & AdamW & 83.43 & 80.93 & 94.88 \\  & Lion & **84.09** & **81.43** & **95.86** \\   

Table 2: Zero-shot accuracy of LiTs on ImageNet, CIFAR-100, and Oxford-IIIT Pet. As a reference, the zero-shot accuracy of CLIP  on ImageNet is 76.2%.

    &  &  &  &  &  \\  & Adafactor & Lion & Adafactor & Lion & Adafactor & Lion & GPT-3 & PaLM \\  \#Tokens &  &  &  \\  Avg NLG & 11.1 & **12.1** & 15.6 & **16.5** & 24.1 & **24.7** & 23.1 & 23.9 \\ Avg NLU & 53.2 & **53.9** & 56.8 & **57.4** & 61.3 & **61.7** & 58.5 & 59.4 \\   

Table 3: One-shot evaluation averaged over three NLG and 21 NLU tasks. The results of GPT-3  and PaLM  are included for reference. The LLMs trained by Lion have better in-context learning ability. See Table 11 (in the Appendix) for detailed results on all tasks.

Figure 3: ImageNet Real (**Left**) and ImageNet V2 (**Right**) accuracy after we pre-train ViT models on JFT-300M then fine-tune on ImageNet. See Table 4 (in the Appendix) for the detailed numbers.

**Image synthesis on ImageNet** We utilize the improved U-Net architecture  and perform \(64 64\), \(128 128\), and \(256 256\) image generation on ImageNet. The batch size is 2,048 and the learning rate remains constant throughout training. For decoding, we apply DDPM  for 1K sampling steps _without_ classifier-free guidance. The evaluation metric is the standard FID score. Illustrated by Figure 1 (Right) and Figure 7 (Middle and Right) in the Appendix, Lion enables both better quality and faster convergence on the FID score. The gap between Lion and AdamW increases with the image resolution, where the generation becomes more challenging. When generating \(256 256\) images, Lion achieves the final performance of AdamW with 2.3x fewer steps. The final FID scores are 4.1 (Lion) vs. 4.7 (AdamW). For reference, the FID of ADM  is 10.94.

**Text-to-image generation** We follow the Imagen  setup to train a base \(64 64\) text-to-image model and a \(64 64 256 256\) super-resolution model. All models are trained on a high-quality internal image-text dataset with a batch size of 2,048 and a constant learning rate. Due to computational constraints, our base U-Net has a width of 192 compared to 512 in the original 2B model, while the 600M super-resolution model is identical to the original Imagen setup. Along with the training, 2K images are sampled from the MSCOCO  validation set for real-time evaluation. We use the CLIP score to measure image-text alignment and the zero-shot FID-30K to measure image fidelity. Classifier-free guidance  with a weight of 5.0 is applied as it has been shown to improve image-text alignment. Figure 4 depicts the learning curve. While there is no clear improvement on the base \(64 64\) model, Lion outperforms AdamW on the text-conditional super-resolution model. It achieves a higher CLIP score and has a less noisy FID metric compared to AdamW.

### Language Modeling and Fine-tuning

This section focuses on language modeling and fine-tuning. On language-only tasks, we find that tuning \(_{1}\) and \(_{2}\) can improve the quality for both AdamW and Lion. See Appendix M for tuning details. The masked language modeling and fine-tuning results are shown in Appendix J.

**Autoregressive language modeling** We first experiment on two smaller-scale academic datasets Wiki-40B  and PG-19 . The employed Transformer spans three scales: small (110M), medium (336M), and large (731M). The architecture details can be found in Appendix E. All models are trained with \(2^{18}\) tokens per batch for 125K steps, with a learning rate schedule of 10K steps warmup followed by linear decay. The context length is set to 512 for Wiki-40B and 1,024 for PG-19. Figure 5 illustrates the token-level perplexity for Wiki-40B and word-level perplexity for PG-19. Lion consistently achieves lower validation perplexity than AdamW. It achieves 1.6x and 1.5x speedup when training the medium size model on Wiki-40B and PG-19, respectively. When the model is increased to the large size, the speedup on PG-19 further increases to 2x.

We then conduct larger experiments with the pre-training dataset similar to GLaM . Following GPT-3 , we train three models, from 1.1B to 7.5B parameters, on 300B tokens with a batch size of 3M tokens and a context length of 1K. We evaluate them on three natural language generative (NLG) and 21 natural language understanding (NLU) tasks (see Appendix C for details). We observe no difference in perplexity throughout training. Nevertheless, Lion outperforms Adafactor on the average in-context learning ability, as shown in Table 3. Our 7.5B baseline model, trained for 300B tokens, outperforms the 8B PaLM, trained for 780B tokens, demonstrating the strength of our setup. Lion outperforms Adafactor on both NLG and NLU tasks, particularly on the NLG tasks, with an exact match improvement of +1.0, +0.9, and +0.6 for the 1.1B, 2.1B, and 7.5B models, respectively.

## 5 Related Work

Our work lies in the area of AutoML and meta-learning that includes learning to learn [1; 5; 63; 64; 78; 96; 79], neural architecture search [14; 15; 57; 71; 79; 87; 93; 94; 98; 106] and hyperparameter optimization [25; 43; 44; 55], etc. There is also a long history of using evolutionary methods to search for programs, i.e., genetic programming [11; 42; 49]. Our approach builds upon a symbolic search space similar to AutoML-Zero [70; 80]. However, instead of discovering programs with fixed dimensional matrices, vector, and scalars for toy tasks, our goal is to develop programs that operate on n-dimensional arrays and can generalize to state-of-the-art tasks. Other related works include numerous handcrafted optimizers [2; 7; 27; 29; 35; 48; 58; 61; 82; 83; 86; 105].

## 6 Conclusion

This paper aims at discovering optimization algorithms via program search. Despite the challenges from an infinite and sparse space, and large generalization gap between the proxy and target tasks, our method discovers a simple and effective optimizer, Lion, that is memory-efficient and achieves strong generalization across architectures, datasets and tasks.