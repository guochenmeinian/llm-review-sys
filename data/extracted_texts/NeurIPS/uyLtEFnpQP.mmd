# Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals

**Hui Zheng\({}^{*,2,4}\), Hai-Teng Wang\({}^{*,1}\), Wei-Bang Jiang\({}^{3}\), Zhong-Tao Chen1, Li He1, Pei-Yang Lin1, Peng-Hu Wei5, Guo-Guang Zhao5, Yun-Zhe Liu\({}^{,1,4}\)\({}^{1}\)Beijing Normal University, \({}^{2}\)Peking University, \({}^{3}\)Shanghai Jiao Tong University, \({}^{4}\)Chinese Institute for Brain Research, \({}^{5}\)Capital Medical University, Xuanwu Hospital, Beijing *Equal contribution,\({}^{}\)yunzhe.liu@bnu.edu.cn**

###### Abstract

Invasive brain-computer interfaces with Electrocorticography (ECoG) have shown promise for high-performance speech decoding in medical applications, but less damaging methods like intracranial stereo-electroencephalography (sEEG) remain underexplored. With rapid advances in representation learning, leveraging abundant recordings to enhance speech decoding is increasingly attractive. However, popular methods often pre-train temporal models based on brain-level tokens, overlooking that brain activities in different regions are highly desynchronized during tasks. Alternatively, they pre-train spatial-temporal models based on channel-level tokens but fail to evaluate them on challenging tasks like speech decoding, which requires intricate processing in specific language-related areas. To address this issue, we collected a well-annotated Chinese word-reading sEEG dataset targeting language-related brain networks from 12 subjects. Using this benchmark, we developed the Du-IN1 model, which extracts contextual embeddings based on region-level tokens through discrete codex-guided mask modeling. Our model achieves state-of-the-art performance on the 61-word classification task, surpassing all baselines. Model comparisons and ablation studies reveal that our design choices, including (i) temporal modeling based on region-level tokens by utilizing 1D depthwise convolution to fuse channels in the ventral sensorimotor cortex (vSMC) and superior temporal gyrus (STG) and (ii) self-supervision through discrete codex-guided mask modeling, significantly contribute to this performance. Overall, our approach - inspired by neuroscience findings and capitalizing on region-level representations from specific brain regions - is suitable for invasive brain modeling and represents a promising neuro-inspired AI approach in brain-computer interfaces. Code and dataset are available at https://github.com/liulab-repository/Du-IN.

Figure 1: Overall illustration of sEEG decoding setup and comparison with SOTA baselines.

Introduction

Brain signals refer to the biometric information collected from the brain. Their patterns provide valuable insights toward understanding the physiological functions of the brain and the mechanism of related diseases, leading to various applications, including speech decoding [13; 19; 37], sleep cognition research [35; 55], neurological disorders detection [28; 54], and so on. Due to the high signal-noise ratio, invasive recording methods (e.g., stereoElectroEncephaloGraphy (sEEG), ElectroCorticoGraphy (ECoG)) usually reveal these underlying mechanisms better than non-invasive recording methods. Many previous works [29; 19] have shown that decoding speech from EEG signals is difficult, and the performance is limited. Compared with ECoG, sEEG imposes less trauma on patients and provides more stereotactic information from specific brain regions. Although some studies [37; 36] have recently shown promise for building high-performance speech decoders based on ECoG, there are few attempts made to explore the potential of sEEG-based speech decoding.

Modeling intracranial neural signals, especially sEEG, has gained significant attention, but several issues remain unresolved. Current research on modeling neural signals is divided into two lines based on the basic modeling units (e.g., channel-level tokens or group-level tokens2). Some studies [54; 28] utilize shared embedding blocks to embed single channels into channel-level tokens, neglecting the specificity of brain computation ; then they adopt spatial-temporal integration to model spatial relationships among them, attempting to regain the precise state of the brain. However, these methods mainly focus on channel-level classification tasks, e.g., seizure detection, yet fail to validate them on more challenging group-level classification tasks, e.g., speech decoding. Other studies [19; 21] fuse all channels (across the brain) to build brain-level tokens, overlooking the brain's desynchronization nature ; then they adopt temporal modeling to capture the rapid process of brain dynamics. Besides, labeling data at scale in medical experiments is often impractical or costly, emphasizing the need to maximize label efficiency. Hence, developing an efficient pre-training framework that draws on prior neuroscience findings is highly appealing, as it can make the most of abundant unlabeled data.

The primary challenge in modeling intracranial neural signals lies in extracting meaningful tokens, requiring careful consideration of two key factors. (1) Temporal scale. Since intracranial neural signals have high temporal resolution and signal-noise ratio, these tokens must capture rapid dynamic changes in brain activity. (2) Spatial scale. Considering the brain's desynchronization nature, these tokens should correctly capture the information of each brain region for further integration and, if needed, decouple different parts of brain dynamics within each brain region. To better assess how well different models capture the intricate processing within each brain region, we can evaluate these methods on tasks mainly involving a few brain regions.

Since speech mainly involves specific brain regions related to vocal production, as demonstrated in Section 2.1, we utilize speech decoding tasks to evaluate which model can effectively extract information from specific brain regions. Since there are too few open source sEEG language datasets [1; 49], we collected a well-annotated Chinese word-reading sEEG dataset (vocal production), including 12 subjects, which makes up for the problem of missing sEEG recordings in language tasks. Inspired by neuroscientific findings, we systematically demonstrate the locality and specificity of brain computation and propose the Du-IN model to solve the abovementioned issues. Compared to other existing methods for modeling brain signals, Du-IN achieves SOTA performance on the 61-word classification task, demonstrating the effectiveness of our model in extracting meaningful tokens that can capture both the rapid changes and the precise state of specific brain regions. It marks a promising neuro-inspired AI approach [42; 41] in BCI.

To sum up, the main contributions of our work comprise:

1. A well-annotated Chinese word-reading sEEG dataset, addressing the lack of sEEG language dataset. The dataset will be publicly available.
2. Demonstration of brain-specific computation - achieving the best decoding performance only requires about one electrode in specific brain regions (i.e., vSMC, STG).
3. A novel framework for sEEG speech decoding - Du-IN, which learns region-level contextual embeddings through discrete codex-guided mask modeling.
4. SOTA performance on the sEEG speech decoding task - Du-IN achieves 62.70% top-1 accuracy on the 61-word classification task, surpassing all other baselines.

Related Works

### Neural Basis of Language Function

Neuroscientific research [5; 17; 43] in the past has extensively explored brain regions supporting language functionality. In neuroscience, the investigation into language functionality related to speech has been categorized into two main streams: one dedicated to semantic processing and the other to vocal production. Previous studies [4; 43] have shown that brain regions associated with semantic processing primarily include left inferior frontal gyrus (IFG), left anterior temporal lobe (ATL), and bilateral middle temporal gyrus (MTG).

As for vocal production, which is also the focus of our work, it is predominantly governed by motor information related to language articulation, primarily involving ventral sensorimotor cortex (vSMC), bilateral superior temporal gyrus (STG), and bilateral dorsal laryngeal motor cortex (dLMC) [5; 17; 9]. Our analysis results based on our collected word-reading sEEG dataset also confirm this point, as illustrated in Figure 4.

### Language Decoding in BCI

The keys to decoding natural language from brain signals are (1) high-quality recordings, and (2) well-designed models with good representations. Compared to non-invasive recordings (e.g., EEG), invasive recordings manifest advantages in providing detailed information about specific brain regions with a high signal-noise ratio. Since speech mainly involves some specific brain regions, obtaining detailed recordings of these brain regions will significantly enhance the decoding performance. Existing works [13; 37; 21] have shown the great potential of building a high-performance decoder based on invasive recordings.

The other key is well-designed models with good representations. Existing work for brain-to-language representations can be classified into two categories: self-supervision or alignment with representation models pre-trained on other modalities (e.g., text, audio). BrainBERT  learns general embeddings through self-supervised mask modeling. DeWave  introduces discrete code encoding and aligns neural representations with text embeddings from BART , thus enhancing the extraction of semantic processing-related information from EEG recordings. Metzger et al.  align neural representations with acoustic embeddings to improve the extraction of vocal production-related information from ECoG recordings.

### Self-supervised Learning in BCI

In recent years, self-supervised pre-training has made significant progress in natural language processing [16; 39; 6] and computer vision [3; 25; 11]. However, its potential in BCI is far from being explored. BrainBERT (for sEEG)  embeds single channels into channel-level tokens and utilizes mask modeling to learn general representations. Brant (for sEEG) [54; 53], PopT (for sEEG)  and some works (for EEG) [28; 23] further adopt spatial-temporal integration to model spatial relationships among them. Some works (for EEG) [31; 20; 50; 22] take the other way - fusing all channels (across the whole brain) to build brain-level tokens, and it uses self-supervised learning to learn contextual representations. Considering the difference among brain regions, MMM (for EEG)  further splits channels into different groups to build region-level tokens.

All existing pre-training methods for sEEG primarily pre-train spatial-temporal models based on channel-level tokens yet only evaluate them on channel-level classification tasks, e.g., seizure detection. However, unlike EEG pre-training methods, their effectiveness over more challenging group-level classification tasks, e.g., speech decoding. Besides, there is no standard channel configuration for sEEG recordings, unlike EEG recordings, which makes modeling spatial relationships in sEEG more challenging.

## 3 Method

The overall architecture of Du-IN is illustrated in Figure 2, where the raw sEEG signals are fused across channels to build region-level tokens and further encoded for downstream tasks.

### Task Definition

Due to the lack of open-source sEEG datasets related to language tasks, we follow the experimental design outlined by Moses et al.  to collect a well-annotated Chinese word-reading sEEG dataset (vocal production). During the experiment, each subject speaks aloud 61 pre-determined Chinese words 50 times; see Appendix A for more details. We formulate the multi-channel sEEG signals as \(^{C T}\), where \(C\) is the number of sEEG channels and \(T\) is the total timestamps. The associated word label is denoted as \(\), where \(\) represents the set of 61 pre-determined words. In summary, this dataset comprises paired sEEG-word data (\(,\)), and the model aims to decode the corresponding word \(\) from a sequence of raw sEEG signals \(\).

### Model Architecture

We introduce the Du-IN Encoder, a general architecture for sEEG speech decoding tasks that can deal with any input sEEG signals with arbitrary time length, as shown in Figure 2. The key operation for archiving this is segmenting the sEEG signals into patches, inspired by patch embeddings in images . For each sample \(\), we use a \(W\)-length window without overlap to segment it into patches, obtaining \(=\{_{i}^{C W}|i=1,...,N\}\), where \(N=\) is the number of patches.

Spatial Encoder.As each sEEG patch has multiple channels, it is vital to fuse different channels to extract meaningful features before patch-wise interaction by self-attention. We employ a spatial encoder, which consists of a linear projection and several convolution blocks, to encode each sEEG patch into a patch embedding. The linear projection transforms the raw sEEG signals into the hidden neural space, and its weights are utilized for subsequent analysis. The convolution block is composed of a 1D depthwise convolution layer and a batch normalization layer . We denote the output patch embeddings from the spatial encoder as

\[_{p}=\{_{i}^{p}^{d}|i=1,...,N\},\] (1)

where \(d\) is the dimension of the embeddings.

Temporal Embedding.In order to enable the model to be aware of the temporal information of patch embeddings, we utilize the parameter-free position embeddings introduced in , i.e., \(_{t}=\{_{1}^{t},...,_{t_{max}}^{t}\}\). Note that \(t_{max}\) is the hyperparameter determining the maximum number of time patches and \(t_{max} N\). Given one arbitrary patch embedding \(_{i}\) in Equation 1 from the spatial encoder, we add the corresponding temporal embedding to it:

\[_{init}=\{_{i}^{p}+_{i}^{t}|i=1,...,N\},\] (2)

which forms the input embeddings \(_{init}\) for the Transformer Encoder.

Transformer Encoder.Finally, the sequence of embeddings will be directly fed into the Transformer encoder  to get the final encoded \(=\{_{i}^{d}|i=1,...,N\}\). To make the training of the Transformer more stable and efficient, we incorporate some modifications  inspired by LaBraM . We add layer normalization to the queries and keys before the dot-product attention mechanism, which avoids over-large values in attention logits:

\[(Q,K,V)=((Q)(K) ^{T}}{}})V,\] (3)

where \(d_{head}\) is the dimension of attention head and \(\) denotes layer normalization . For downstream classification tasks, we flatten the output embeddings followed by a classification head.

Figure 2: **The overall architecture of Du-IN Encoder.** Du-IN Encoder is used as an encoder in all Du-IN models (i.e., Du-IN VQ-VAE, Du-IN MAE, Du-IN CLS (classification)), see Appendix C for more details.

### Du-IN VQ-VAE Training

Prior to pre-training Du-IN through mask modeling, we need to tokenize the sEEG patches into discrete tokens. We introduce vector-quantized neural signal regression, which is trained by reconstructing the original sEEG signals, as shown in Figure 3. The key components are the Du-IN Encoder, which encodes the raw sEEG samples into embeddings, and the Du-IN Regressor, which reconstructs the original sEEG signals. The idea is basically inspired by VQ-VAE , which encodes images into discrete latent embeddings.

Du-IN Encoder.We define a neural codex \(=\{_{j}|j=1,...,N_{codex}\}^{N_{codex} d_{ codex}}\), where \(N_{codex}\) is the number of the discrete neural embeddings and \(d_{codex}\) is the dimension of each embedding. Given a sEEG sample \(\), the Du-IN Encoder, illustrated in Figure 2, first encodes it to embeddings \(=\{_{i}^{d}|i=1,...,N\}\). After that, we utilize a linear projection \(_{c}\) to get the mapped embeddings \(_{c}()=\{_{c}(_{i})^{d_{codex}}|i=1,...,N\}\) in the codex space. Then, the codex looks up the nearest neighbor of each embedding \(_{c}(_{i})\) in the neural codex \(\). This procedure can be formulated as

\[_{q}()=\{_{q}(_{i})|i=1,...,N\},_{q}( _{i})=_{z_{i}}, z_{i}=*{arg\,min}_{j}||_{2} (_{c}(_{i}))-_{2}(_{j})||_{2},\] (4)

where \(_{2}\) represents \(_{2}\) normalization and \(_{q}(_{i})\) is the quantized vector after the quantizer. This is equivalent to finding the closest neural embedding by cosine similarity and such \(_{2}\) normalization improves the codex utilization .

Du-IN Regressor.The Du-IN Regressor consists of a Transformer decoder and a stack of transposed convolution layers. Given a sequence of the vector-quantized embeddings \(=\{_{i}|i=1,...,N\}\), the Du-IN Regressor convert these discrete embeddings back into raw sEEG signals \(}=\{}_{i}|i=1,...,N\}\). The mean squared error (MSE) loss is utilized to guide the regression. The total loss for training the Du-IN VQ-VAE is defined as:

\[_{vqvae}=_{i=1}^{N}||}_{i}-_{i}||_{2 }^{2}+||[_{c}(_{i})]-_{q}(_{i})||_{2}^{2}+ ||_{c}(_{i})-[_{q}(_{i})]||_{2}^{2} ,\] (5)

Figure 3: **Overview of Du-IN VQ-VAE training and Du-IN MAE training.****(a)**. We train the Du-IN Encoder in the Du-IN VQ-VAE to discretize sEEG signals into discrete neural tokens by reconstructing the original sEEG signals. **(b).** During the training of Du-IN MAE, part of sEEG patches are masked while the objective is to predict masked tokens from visible patches.

where sg represents the stop-gradient operation, which is an identity at the forward pass and has zero gradients. To stabilize the codex update, we use the exponential moving average strategy .

### Pre-training Du-IN

Masked sEEG Modeling.To enforce Du-IN learning contextual representations, we propose masked sEEG modeling. The whole procedure is presented in Figure 3. As illustrated in Figure 2, given a sEEG sample \(\), the spatial encoder first transforms it to patch embeddings \(_{p}=\{_{i}^{p}|i=1,...,N\}\). Given these patch embeddings \(_{p}\), around 50% of patch embeddings are patch-wisely chosen and masked. The masked position is termed as \(\). Then, a shared learnable embedding \(_{[M]}^{d}\) is used to replace the original patch embeddings:

\[_{m}=\{_{i}^{m}|i=1,...,N\},_{i}^{m}=m_{i} _{[M]}+(1-m_{i})_{i}^{p},\] (6)

where \(()\) is the indicator function and \(m_{i}=(i)\). After that, the masked embeddings \(_{m}\) will be added by temporal embeddings, and then fed into the Transformer encoder. The output embeddings \(\) will be used to predict the indices of the corresponding codes from the codex in the Du-IN VQ-VAE through a linear classifier:

\[p(z_{i}|_{i})=((_{i})),\] (7)

The training loss of mask modeling is defined as:

\[_{}=-_{i}m_{i} p(z_{i}|_{i}).\] (8)

Symmetric Masking.Inspired by LaBraM , we further introduce a symmetric masking strategy to improve training efficiency. We calculate the inverse of the generated mask \(\), obtaining \(\). Similarly, we use the new mask \(}\) to perform the mask modeling, obtaining the mask modeling loss \(_{}^{sym}\). The total loss for pre-training the Du-IN model (i.e., Du-IN MAE model) is defined as:

\[_{mae}=_{}+_{}^{sym}.\] (9)

## 4 Experiments

### Dataset

Due to the lack of open-source sEEG datasets related to language tasks, we follow the experimental design outlined by Moses et al.  to collect a well-annotated Chinese word-reading sEEG dataset (vocal production), including 12 subjects. The subjects undergo a surgical procedure to implant 7 to 13 invasive sEEG electrodes, each with 72 to 158 channels, in their brain. For each subject, the dataset contains 15 hours of 2000Hz recordings, 3 hours of which are task recordings.

Pre-training dataset.For each subject, the pre-training dataset contains all sEEG recordings (with about 54 million timestamps) of that subject. To stabilize computing resource usage, the time length of sEEG sample \(\) is set to 4 seconds.

Downstream dataset.For each subject, 3 hours of the sEEG recordings are task recordings. The sEEG signals are segmented into about 3000 3-second samples, each of which is paired with the corresponding word label (from 61 pre-determined words).

### Implementation Details

Preprocess.We first filter the sEEG signals between 0.5Hz and 200Hz to remove low-frequency noise. Then, a notch filter of 50Hz is applied to avoid power-line interference. After that, all sEEG signals are resampled to 1000Hz and bi-polar re-referenced . Finally, we perform z-score normalization on each channel to guarantee normalized data scales across all channels.

Model Configurations.The length of the sEEG patch is 100ms, resulting in 40 patches per sample in the pre-training dataset and 30 patches per sample in the downstream dataset. The "Spatial Encoder" contains one linear projection and three 1D convolution layers, transforming the original sEEG patches into patch embeddings with \(d=160\). The following "Transformer Encoder" contains an 8-layer Transformer encoder with model dimension \(d=160\), inner dimension (FFN) \(d_{ff}=320\), and 8 attention heads. See Appendix C for more details.

Pre-training.During the pre-training, we use either all sEEG recordings (15 hours) or the sEEG recordings without task recordings (12 hours) to train the Du-IN VQ-VAE and Du-IN MAE models. To enhance the robustness of the learned codex and representations, we further use data augmentation described in Appendix D. For each subject, the model is pre-trained on a Linux system with 2 CPUs (Intel Xeon Gold 6230 40-Core Processor) and 1 GPU (NVIDIA Tesla V100 32GB) for \(\) 1.2 days.

Fine-tuning.During the downstream evaluation, we split the task recordings into training, validation, and testing splits with a size roughly proportional to 80%, 10%, and 10%. All experiments are conducted on the same machine with the same set of random seeds. The train/validation/test splits are the same across different models. We also use data augmentation, as described in Appendix D, to make the most of the gathered dataset. We employ cross-entropy loss (multi-class classification) as the training loss. Our experiments are conducted on one V100 GPU by Python 3.11.7 and PyTorch 2.1.2 + CUDA 12.3. The best models are trained based on the training set, selected from the validation set according to accuracy, and finally evaluated on the test set. For model comparison, we report the average and standard error values (of all subjects) on six different random seeds to obtain comparable results. For the results of the subject-wise evaluation, we report the average and standard deviation values (of each subject) in Appendix K.

### Channel Contribution and Selection

As demonstrated in Section 2.1, previous neuroscience studies reveal that vocal production predominantly engages specific brain regions. Given the sparse distribution of implanted sEEG electrodes (each containing 8-16 channels), it's vital to exclude redundant electrodes unrelated to vocal production, thus improving decoding performance. We retain electrodes implanted in relevant brain regions and evaluate the performance based on the remaining electrodes. Table 1 demonstrates that excluding approximately 85% electrodes even leads to a dramatic increase in decoding performance.

To further understand the detailed contribution of each channel, we analyze the weights of linear projection in the spatial encoder. In detail, we calculate the contribution scores of channels per subject and organize them accordingly, as described in Appendix H. Figure 4 demonstrates that (1) the brain regions effective for speech decoding align with findings from previous neuroscience research, and (2) our model achieves optimal decoding performance with approximately 10 channels, 80% of which originate from the same electrode. To streamline, we utilize these top 10 channels (selected according to train-set) for both pre-training and downstream evaluation.

### Comparasion with Other Models

Table 2 presents the results of our Du-IN model and the advanced baselines that are designed for either brain signals or general time series. See Appendix B and Appendix C.3 for detailed descriptions of models. The results demonstrate that our Du-IN model outperforms all baselines. It's worth noting that the models (i.e., the foundation models designed for brain signals) that adopt spatial-temporal integration to model spatial relationships among channel-level tokens perform worse than the models that adopt temporal modeling based on region-level tokens, challenging the generalizability of current strategies to model spatial relationships among channels with Transformer.

  
**Methods** & **\# of Channels (Averaged)** & **Accuracy (\%) \(\) Ste (\%)** \\  Du-IN (w/o electrode selection) & 109.75 & 30.12\(\)5.64 \\ Du-IN (w/ electrode selection) & 12.25 & 55.92\(\)4.96 \\   

Table 1: The performance of Du-IN with or without electrode selection.

As BrainBERT  doesn't consider the spatial relationships among channels, we mainly focus on understanding why Brant , LaBraM  and LaBraM-Popt [28; 10] fail to effectively capture the discriminative features on the speech decoding task. These models typically build channel-level tokens by segmenting non-overlapping patches with large receptive fields (e.g., 1 second) from single channels. However, this approach makes it challenging to capture the rapid process of brain dynamics. Moreover, while these models further utilize Transformer to capture the spatial relationships among these tokens, they do not encourage region-level embeddings, either through their architecture  or their pre-training objective . Therefore, the effectiveness of building brain foundation models based on these spatial-temporal backbones is still under exploration, especially for cognitive tasks (e.g., speech decoding), which are of great value in the field of neuroscience.

Besides, unlike LaBraM , Brant doesn't introduce spatial embeddings to identify the spatial location of each channel. Since the electrodes are sparsely distributed in the brain and the raw sEEG signals on the same electrode are highly correlated, it's fairly easy to identify their spatial relationships through their values. As demonstrated in iTransformer , this modeling approach is well suited for detecting time-delay events, e.g., seizure detection. For speech decoding tasks,

    &  &  &  &  \\  & & PT1 & MS2 & & \\  TS-TCC & Region & ✓ & ✗ & 0.32M & 24.85\(\)4.42 \\ CNN-BiGRU & Region & ✗ & - & 0.54M & 32.04\(\)5.45 \\ EEG-Conformer & Region & ✗ & - & 2.34M & 45.82\(\)4.66 \\ Neuro-BERT & Region & ✓ & ✗ & 2.14M & 49.51\(\)4.43 \\ DeWave & Region & ✗ & - & 5.70M & 32.43\(\)4.48 \\  BrainBERT & Channel & ✓ & ✗ & 43.58M & 6.72\(\)1.59 \\ BrainBERT & Channel & ✓ & ✓ & 43.58M & 7.50\(\)1.76 \\ Brant & Channel & ✓ & ✗ & 69.35M & 11.16\(\)3.56 \\ Brant & Channel & ✓ & ✓ & 69.35M & 12.42\(\)4.10 \\ LaBraM & Channel & ✓ & ✗ & 6.85M & 11.53\(\)2.63 \\ LaBraM-PopT[28; 10] & Channel & ✓ & ✓ & 6.85M & 11.78\(\)2.70 \\  Du-IN & Region & ✗ & - & 4.38M & 56.29\(\)5.20 \\ Du-IN (vqvae+vq) & Region & ✓ & ✗ & 4.38M & 44.17\(\)4.04 \\ Du-IN (vqvae) & Region & ✓ & ✗ & 4.38M & 58.24\(\)4.83 \\ Du-IN (mae) & Region & ✓ & ✗ & 4.38M & **62.70\(\)4.69** \\  Du-IN (poms) & Region & ✓ & ✓ & 5.18M & 59.18\(\)4.63 \\   

* \({}^{1}\) PT: Whether the model is pre-trained before evaluation.
* \({}^{2}\) MS: Whether the model is pre-trained across multiple subjects.

Table 2: The performance of different methods (with the best in **bold** and the second underlined).

Figure 4: **The channel contribution analysis.****(a).** The channel contribution map. **(b).** The effect of the number of channels (sorted according to channel contribution scores) on decoding performance.

sEEG often requires bi-polar re-reference (or Laplacian re-reference) to remove the high correlations among channels, thus avoiding model overfitting . Once the correlations among channels have been removed, Brant will lose the ability to model spatial relationships among channels.

For other baselines that use temporal modeling based on region-level tokens, we provide a detailed explanation of their performance differences as follows. TS-TCC  tokenizes raw sEEG signals into region-level tokens with a stack of 1D depthwise convolution blocks, but it lacks a temporal Transformer for further integration over time. CNN-BiGRU  introduces a stack of GRU layers on top of these tokens to perform temporal integration. EEG-Conformer  introduces a temporal Transformer to better integrate global temporal information, which makes it outperform CNN-BiGRU. However, EEG-Conformer tokenizes raw sEEG signals with the temporal-spatial convolution, applying the same convolutional kernel across different channels, which overlooks the specificity of brain computation . This also raises a challenge for the effectiveness of current sEEG foundation models, which rely on shared convolution blocks across individual channels. Neuro-BERT  further introduces mask modeling to learn contextual embeddings, which makes it outperform EEG-Conformer. DeWave  utilizes the Conformer model  for tokenization, which involves more parameters but is less effective than 1D depthwise convolution.

### Ablation Study

Self-Supervision Initialization.As illustrated in Figure 3, the Du-IN model entails a two-stage pre-training process, wherein both the Du-IN VQ-VAE model and the Du-IN MAE model are trained. Previous studies utilize different strategies [19; 12; 28] to leverage these pre-trained models to enhance the performance of downstream tasks. Here, we evaluate these different strategies for comparison; see Appendix C.3 for detailed definitions. Table 2 shows that initializing weights from the Du-IN MAE model captures contextual embeddings effectively, resulting in the highest decoding performance.

Pre-training with/without Downstream Datasets.During the pre-training stage, we hope that the Du-IN VQ-VAE model can extract general tokens of that brain region, thus guiding the Du-IN MAE model to learn general representations that are not specific to any particular task. Although no label data is used during the pre-training stage, to eliminate the influence of the pre-training data on downstream tasks, we compare the results with or without incorporating the downstream task dataset into the pre-training stage. Table 3 shows a slight performance drop when excluding downstream datasets. However, the decoding performance is still higher than the baseline performance without pre-training, which means that the degradation is mainly due to the decrease of the pre-training dataset. We hope that, with more pure recordings, our model can achieve better decoding performance.

Discrete Codex.During the Du-IN VQ-VAE training stage, the Du-IN VQ-VAE model encodes sEEG patches into discrete codes and then reconstructs the original signal from these codes. We evaluate performance against varying codex sizes (512 to 8192) to ascertain if codex size affects the quality of the learned codex. As illustrated in Figure 5, while extremely small codex size lacks representation diversity, extremely large codex size often leads to codex collapse. We suspect that our existing training data might not be adequate for larger codex sizes. Furthermore, our experiments suggest that the model performs optimally when the codex dimension, denoted as \(d_{codex}=64\), is slightly less than the model dimension, \(d=160\), yielding a more effective regularization effect.

Perception Time Window.We also conduct the ablation study on the model structure for the spatial encoder described in Section 3.2. As the spatial encoder transforms the sEEG signals within a given patch to a patch embedding, it compresses the sEEG signals for perception. As described in Section 4.2, the model utilizes a receptive field of 100ms. We conduct an ablation study of different receptive fields and report it in Figure 5. The model performance notably drops with a receptive field smaller than 60ms and gradually declines as the receptive field exceeds 160ms. The model reaches a

  
**Methods** & **Pre-training Dataset Size** & **Accuracy (\%) \(\) Ste (\%)** \\  Du-IN (mae w/o DD) & 12 hours per subject & 60.02\(\)4.34 \\ Du-IN (mae w/ DD) & 15 hours per subject & 62.70\(\)4.69 \\   

Table 3: Ablation study on whether pre-training with the downstream dataset (DD) or not.

small peak around 100ms to 140ms. We think this phenomenon is rational since sEEG is known for its ability to capture the rapid dynamics of specific brain regions precisely.

## 5 Limitations

Despite Du-IN's enhancements in speech decoding via discrete codex-guided mask modeling, it is still restricted to close-set speech decoding tasks (i.e., the word set only includes 61 pre-determined words). However, a parallel to our work, Feng et al. , which follows previous works [26; 45], build an acoustic-inspired model that can decode arbitrary Chinese words by predicting syllable components (initials, finals, tones). Although their method requires a large amount of labeled data, their experimental design mirrors ours closely. The difference lies in the requirement for the subject to repeat syllable components, instead of entire words. Therefore, with slight modifications, our model can support open-set speech decoding tasks.

Additionally, the experiments in this paper are restricted to the vocal production part of language decoding, i.e., speech decoding. A more interesting but difficult task is to decode language from the semantic level, in which large language models have been wildly used to improve the model performance [46; 19]. However, due to the locality of sEEG recordings, it is still under exploration whether sEEG recordings can fully capture semantic-related information across brain regions.

## 6 Conclusion

This paper proposes Du-IN, a framework for speech decoding, which learns contextual embeddings through discrete codex-guided mask modeling on specific brain regions. To evaluate our model, we collect a well-annotated Chinese word-reading sEEG dataset to address the lack of sEEG language dataset. Inspired by neuroscientific findings, we analyze the effective brain regions for speech decoding and achieve the best decoding performance with about one electrode in specific brain regions, which dovetails with the past neuroscientific research on language. Comprehensive experiments demonstrate that our model outperforms both supervised and sEEG-based self-supervised baselines, effectively capturing the intricate processing within specific brain regions. It marks a promising neuro-inspired AI approach in BCI. In the end, we hope our work can have implications for future developments in sEEG-based self-supervised models with more consideration over how to build the basic representation units so that the model can maximally benefit from the pre-training stage.

## 7 Broader Impacts

Our method advances the feasibility of invasive BCI technology by being the first to demonstrate speech decoding using a single sEEG electrode, which holds significant potential for clinical applications. For patients who have lost their ability to communicate or perform daily tasks due to neurological conditions like locked-in syndrome or amyotrophic lateral sclerosis (ALS), our approach offers a less invasive alternative to technologies like ECoG or microelectrode arrays, thereby reducing the risk of brain damage.

Figure 5: Ablation study on different codex sizes, codex dimensions, and receptive fields.

#### Acknowledgements

This study was supported by the National Science and Technology Innovation 2030 Major Program (2022ZD0205500), the National Natural Science Foundation of China (32271093), the Beijing Natural Science Foundation (Z230010, L222033), and the Fundamental Research Funds for the Central Universities. We would like to extend our sincere appreciation to Dr. Zhi-Feng Yue for his coordination and support in securing the computing resources essential for this study. Besides, we also sincerely appreciate the LaBram  team for their valuable discussion on the visual design of Figure 2 and Figure 3.

#### Ethics Statement

Experiments that contribute to this work were approved by IRB. All subjects consent to participate. All electrode locations are exclusively dictated by clinical considerations.

Our informed consent signing process is as follows:

1. If the experimental participants are adults and have full civil capacity, we will ask them to sign a written informed consent after the participants have fully informed consent;
2. If the experimental participants are minors or do not have full civil capacity, we will ask the participant's legal guardian to sign a written informed consent after the participants and their legal guardians have fully informed consent.

Our informed consent form includes the following points:

1. Contact information of research institutions and researchers;
2. Research direction and purpose;
3. Risks involved in the research;
4. Personal information, data and usage methods to be used in the research;
5. Privacy protection statement (all personal identification information (PII) will not be disclosed);
6. Data storage statement (retained after deleting all personal identification information (PII));
7. Voluntary statement of participants;
8. Statement that participants can withdraw unconditionally at any time.

Our data storage and protection procedures include the following processes:

1. Our data collection, transfer, and analysis tasks are only completed by researchers who have signed relevant confidentiality agreements;
2. The collected raw data will be copied twice as soon as possible, one copy to a storage computer that is not connected to the Internet and encrypted, and the other copy to a mobile hard disk and encrypted and stored offline;
3. The use of the data is only authorized to the research leader and the main researchers (less than 5 people), among which the main researchers can only access data that does not contain personal identification information (PII);
4. After the study is completed, all personal identification information (PII) on both nodes (storage computer, mobile hard disk) will be deleted immediately.

To prevent unauthorized access or possible data leakage, we use double encryption on the storage computer, that is, a static password and a dynamic password (received by mobile phone or email); physical isolation is used on the mobile hard disk, that is, it is locked in a filing cabinet, and the key is only kept by the research leader and the main researchers.