# Quantizable Transformers: Removing Outliers by

Helping Attention Heads Do Nothing

 Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort

Qualcomm AI Research

Amsterdam, The Netherlands

{ybond, markusn, tijmen}@qti.qualcomm.com

Qualcomm AI Research

Quadcomm AI Research is an initiative of Qualcomm Technologies, Inc.

37th Conference on Neural Information Processing Systems (NeurIPS 2023).

###### Abstract

Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a "no-op" or just a partial update of the residual. To achieve the exact zeros needed in the attention matrix for a no-update, the input to the softmax is pushed to be larger and larger during training, causing outliers in other parts of the network. Based on these observations, we propose two simple (independent) modifications to the attention mechanism - _clipped softmax_ and _gated attention_. We empirically show that models pre-trained using our methods learn significantly smaller outliers while maintaining and sometimes even improving the floating-point task performance. This enables us to quantize transformers to full INT8 quantization of the activations without any additional effort. We demonstrate the effectiveness of our methods on both language models (BERT, OPT) and vision transformers. Our source code is available at https://github.com/qualcomm-ai-research/outlier-free-transformers.

## 1 Introduction

Quantization has been one of the most impactful ways to reduce the computational complexity of transformer networks. Previous work has shown that quantizing networks to 4-bit weights is possible without losing too much accuracy . Some research even shows 4-bit weights might be optimal when trading off model size and bit-width .

However, quantizing transformers is not always trivial. When quantizing the activations of a transformer, significant problems arise with outliers in specific layers. This has been noted by several researchers that suggest fixes to transformers after training to ameliorate their effect . These methods are frequently tedious and either require retraining the network, require implementing specific hardware for input-channel quantization  or require parts of the activations to still be in higher bit-widths, reducing the effectiveness of the activation quantization .

In this paper, we set out to solve the transformer outlier problem entirely by changing the architecture of the network itself. We hope to make transformers easy to quantize from the get-go without needingany post-processing. To do so, we thoroughly analyze why these outliers appear. Previous work has found the existence of these outliers [4; 13], but in our work, we come to a fuller understanding of these outlying values. We find that the outliers occur because attention heads are trying not to update the hidden state, and in the process, strong outliers appear due to the softmax function. This happens for language and vision transformers and different specific transformer architectures. This understanding is the foundation for two new tweaks we suggest to transformer architectures that can remove the problem of the outliers entirely.

## 2 Background and related work

In this section, we briefly cover the basics of neural network quantization and discuss why modern transformers are difficult to quantize.

QuantizationOne of the most powerful ways to decrease the computational time and memory consumption of neural networks is quantization, which uses low-bit representations for the weights and activation tensors. On top of that, using low-bit fixed-point representations, such as INT8, one can further reduce energy consumption since the fixed-point operations are more efficient than their floating-point counterparts [23; 59].

We simulate the quantization process in floating-point according to Jacob et al. . We use the following definition of the quantization function:

\[}:=q(;\,s,z,b)=s((}{s}+z;0,2^{b}-1 )-z),\] (1)

where \(\) denotes the quantizer input (i.e., network weights or activations), \(s_{+}\) the scale factor or the step-size, \(z\) the zero point, and \(b\) the bitwidth. \([]\) denotes the round-to-nearest-integer operator. This quantization scheme is called _uniform affine_ or _asymmetric_ quantization [24; 32; 76] and it is one of the most commonly used quantization schemes because it allows for efficient implementation of fixed-point arithmetic. In the case of _symmetric_ quantization, we restrict the quantization grid to be symmetric around \(z=0\).

In this work, we focus on _post-training quantization_ (PTQ) methods, which take a pre-trained FP32 network and convert it directly into a fixed-point network without the need for the original training pipeline [2; 5; 7; 25; 32; 35; 41; 43; 44; 75]. These methods require either no data or only a small calibration dataset and are easier to use compared to _quantization-aware training_ (QAT, Bhalgat et al. 3, Esser et al. 16, Gupta et al. 21, Jacob et al. 26, Krishnamoorthi 32) methods that have you train the entire network for more epochs. For more details on neural network quantization, we refer the reader to [19; 46].

Outliers in TransformersMultiple studies have shown that modern transformer-based language models tend to learn outliers in weights and activations [4; 13; 31]. These outliers are present only in a small fixed set of embedding dimensions, but they appear regularly and consistently across multiple layers and data sequences. It was also shown that those outliers play a crucial role in the model predictions and clipping them or by setting to zero the corresponding parameters significantly degrades the model task performance [31; 49]. The strongest in magnitude outliers typically appear at the output of the feed-forward network, FFN, although Dettmers et al.  showed that for big enough transformer-based language models they start appearing after every linear layer, including query, key, and value projection layers. This phenomenon holds for many tasks, training objectives and models (both encoder and decoder transformers), including BERT , RoBERTa , DistilBERT , MobileBERT , ELECTRA , BART , XLNet , GPT-2 , and OPT .

Because of these strong outliers, applying per-tensor PTQ for the FFN's output and the residual sum will likely cause a notable error because of the following trade-off between the range and the precision. On the one hand, using a large quantization range for small-ranged values leads to a loss in representation (high rounding error). On the other hand, a small quantization range for large values leads to a very high clipping error. For the case of significant transformer outliers, frequently, no good trade-off can be found between the rounding and clipping error, resulting in an overall high error.

There have been numerous attempts to fix the issue of transformer quantization [4; 12; 13; 17; 27; 28; 51; 54; 62; 63; 69; 71]. Most of these approaches resort to finer quantization granularity (row-wise,channel-wise, group-wise weight and activation quantization), use higher bitwidth and/or different numeric format to represent those outliers better or require extra fine-tuning (in the form of QAT and/or knowledge distillation). In other words, they adapt quantization to work with outliers, which often comes at the expense of general applicability or extra inference overhead.

In contrast, in this work, we want to address the root cause of the problem and understand why outliers are learned in the first place and suggest a new pre-training protocol that significantly reduces the magnitude of outliers yielding way more quantization-friendly models that can be effortlessly quantized using PTQ without strong degradation of performance.

## 3 Outlier analysis

Outliers in BERT modelsIn Section 2 we discussed that outliers are present only in a few designated embedding dimensions but they appear regularly and consistently across multiple layers and data sequences. We also discussed that the strongest magnitude outliers in BERT typically appear at the output of FFN in the last encoder layers.

We start by taking the pre-trained _BERT-base-uncased_ checkpoint from HuggingFace  and fine-tune it on MNLI dataset from the well-known GLUE benchmark  (see experimental details in C.1). To identify the outlier dimensions, we pass the MNLI-m validation set through the network and record all outliers1 at the FFN output in layers #10 and #112. As we can see in Figure 1, there are indeed only a few hidden dimensions where outliers ever occur. We also notice that the majority of outliers (\(>97\%\)) correlate with the position of delimiter tokens - [SEP], ",", and ",".

To better understand the role of those outliers, we analyze the attention patterns of the corresponding attention heads. BERT-base uses multi-head attention with \(n_{}=12\) and each head operating on a consecutive subset of \(d_{}=64\) features. Therefore, the hidden dimension #180, which happens to have the highest outlier count in both layers #10 and #11, corresponds to attention head #3. In Figure 2 (and more examples in Appendix A.1) we show examples of the attention matrices, values and their product for that head.

A common pattern we found is that the attention head assigns almost all of its probability mass to [SEP] tokens, and other less informative tokens like dots/commas, while these tokens also have small values in \(\) associated with those tokens. This results in a small magnitude product between the two (see Figure 1(a)). This effectively corresponds to a (soft) _no-update_ of the hidden representation, where only small noise is added after the residual. In other cases (Figure 1(b) and 1(c)), we observe that a significant portion of attention probability is still spent on delimiter tokens. However, by allocating some of the probability mass on other tokens (together with the small values for the delimiter tokens), this results in a (soft) _selective_ update of the hidden representation.

These patterns in self-attention seem to be a learned "workaround" for the limitations of having the softmax and the residual connections in cases where the attention head does not want to update the representation of some or all of the tokens. These observations are in line with Clark et al. , Kovaleva et al.  that also argued that attending exclusively or almost exclusively to delimiter tokens such as [SEP], periods/commas acts as a "no-op" when the attention head's function is not applicable.

Figure 1: Histograms of outlier counts vs. token positions (blue) and hidden dimensions (green), recorded from the MNLI-m validation set on BERT-base. We use zero-based indexing for dimensions.

Outliers in ViTWe conduct a similar analysis for Vision transformer  trained on ImageNet . For this study, we use a pre-trained checkpoint following our experimental setup from Section 5.

We highlight our findings in Figure 3 and provide more examples in Appendix A.2. Our analysis shows many similarities to the BERT case. Instead of delimiter tokens, the majority of outliers seem to correlate with some random uninformative patches (e.g., in the background). We also see that the corresponding attention head in the next layer allocates the majority of attention probabilities to the same patches. Finally, those outlier patches on average have a distinctly smaller magnitude of values compared to non-outlier ones, leading to similar no-update behavior. The fact that those values are not as close to zero as it was in the BERT case might be related to the smaller model capacity3, or a relatively shorter training procedure.

HypothesisBased on these observations, we pose the following hypothesis on how this behavior of attention heads is related to outliers:

1. In order for an attention block to not update a representation of a token on the residual, some attention heads want to allocate most of their attention probability mass to some fixed and common set of tokens that have a low information content (e.g., delimiter tokens or background patches) that can be learned to have a small value function output.

Figure 3: A summary of our outlier analysis for ViT demonstrated on a random image from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #11. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, layer #12. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values for outlier and non-outlier patches.

Figure 2: Visualization of the patterns in the self-attention, specifically the attention probabilities, values, and their product (left, middle and right columns, respectively), in attention head #3 for BERT-base, computed on several data sequences from MNLI-m validation set.

2. From the definition of the softmax function4, it is easy to see that this would require an input of the softmax to have a relatively big dynamic range (Figure 4, 1). In fact, in the limit case where softmax is exactly zero, this would require an infinite dynamic range: 3. Since Layer Normalization (, 2) normalizes the outliers, the magnitude of the FFN output _in the previous layer_ (2) has to be very high to still produce a sufficiently big dynamic range after the LayerNorm. Note, that this is also applicable for the transformer models with LayerNorm applied prior to the self-attention or linear transformations instead, a variant adopted by GPT, OPT, and many vision transformers [15; 38; 57; 58].
4. Finally, as softmax will never output exact zeros, it will always back-propagate a gradient signal to grow bigger outliers5. The outliers will thus tend to become stronger in magnitude, the longer the network is trained. 

## 4 Method

In this section, we introduce our proposed modifications for the softmax attention mechanism. Based on our insights from Section 3, the core idea of these modifications is to grant the model the ability to produce very small the magnitude (or even exact zeros) output of attention function, without producing outliers.

Recall that the self-attention  is defined as follows:

\[():=(()()^{T}}{}}})()\] (3)

where \(\), \(\) and \(\) are learnable linear projections of the input \(\). Most modern transformer models employ a multi-headed variant of self-attention, where \(d_{}\) features are partitioned into \(n_{}\) groups of \(d_{}\) features, and the final output is the concatenation of the outputs of (3) applied to each group.

### Clipped softmax

First, we propose to replace softmax function in (3) with the following clipped softmax:

\[(;,):=\] \[((-) ()+,0,1).\] (4)

Here \(\) is the input and \( 1\), \( 0\) are the stretch factors which are hyper-parameters of the method. This formulation was proposed before in  in the context of binary stochastic gates. We can view (4) as stretching the output of the softmax from \((0,1)\) to \((,)\) and then clipping back to \((0,1)\) so that we can represent exact zeros if \(<0\) and exact ones if \(>1\). Specifically, the values of the softmax larger than \(\) are rounded to one whereas values smaller than \(\) are rounded to zero.

With this drop-in replacement, we can achieve exact zeros (and ones) with a finite range for the softmax input. In addition to that, whenever values are clipped they will not give a gradient, preventing the outliers to grow further.

Figure 4: A schematic illustration of the attention layer in BERT. Hidden activation tensor is denoted by \(\). \(\) is an element-wise addition. A problematic output of the FFN that generates largest in magnitude outliers is highlighted in red. Notice how those outliers in the _previous layer_ influence the behavior in the attention mechanism in the _next layer_.

### Gated attention

An alternative way of architecting the model to have a small attention output without outliers is to equip it with an explicit conditional gating mechanism, as shown in Figure 5. The idea is that the model can use the gating to either keep or nullify the update to the representation of certain tokens and not rely on the attention probabilities and values to achieve the same outcome.

Specifically, we propose the following modification to the attention function:

\[():=(( ))\,(()()^{T}}{}}})().\] (5)

Here \(\) is the gating function, \(\) is an element-wise multiplication across the token axis and everything else remains the same as in (3). The gating function \(\) is parameterized by a small neural network that is learned jointly with the rest of the model. We replace the attention formulation with the proposed variant in every layer on the transformer network.

Gating module designRecall that the input to the attention layer \(\) has shape \((T,d_{})\) that is reshaped into \((n_{},T,d_{})\) for the multi-headed self-attention, where \(T\) is the sequence length. We chose to define the gating function on a per-head basis. For each head \(i\{1,,n_{}\}\), we specify \(_{i}:^{d_{}}\) and the output of the gating module is \(_{i}^{T}\) that is computed as follows:

\[}_{i,t}=_{i}(_{i,t,:})\ \  t \{1,,T\}\] (6) \[_{i,:}=(}_{i,:}),\] (7)

note that gating modules are shared between different token positions but not shared across attention heads.

We want our gating module to be as lightweight as possible. To start with, we experiment with \(_{i}\)'s parameterized by a single linear layer. This gives us a gating module that is computationally inexpensive and has a memory overhead of just \(n_{}(d_{}+1) d_{}\) extra parameters (which is equivalent to 1 extra token) per attention layer6. We also investigate the effect of using several other gating functions in Appendix B.1.

## 5 Experiments

In this section, we evaluate the proposed modifications to self-attention on several language models (BERT, OPT) and the vision transformers (ViT). We first test the different hyperparameters for the methods and provide insight into how they work. Then we set out to test our method in terms of accuracy, and the difference in quantization improvement after training. All detailed hyperparameters of our experiments are in Appendix C.

BertWe experiment with BERT-base-uncased (109M parameters) pre-training using the masked language modeling (MLM) objective. Following , we use the concatenation of the training sets of BookCorpus  and English Wikipedia7. We implement our methods in PyTorch  and use training and evaluation pipelines from HuggingFace libraries [20; 34; 65]. We follow closely the pre-training procedure from . To speed up training and experimentation, we train with a maximum sequence length of \(128\) for the whole duration of the training. We evaluate on Wikipedia validation set and report the MLM perplexity.

OptWe experiment with a 125M sized variant of OPT  pre-training using the causal language modeling (CLM) objective. Due to compute constraints, we train the model on the same dataset that was used for BERT pre-training (BookCorpus + Wikipedia) with a maximum sequence length of \(512\)

Figure 5: A schematic illustration of our proposed gated attention.

and batch size of \(192\). Similar to our BERT experiments, we use training and evaluation pipelines from HuggingFace libraries. We evaluate on Wikipedia validation set and report the CLM perplexity.

ViTFinally, we explore the effectiveness of proposed techniques on vision transformer  (_ViT-S/16_ configuration, 22M parameters) trained on ImageNet-1K [11; 52]. For these experiments, we adopt the training and validation pipelines from PyTorch Image models library . We report top-1 accuracy on the validation set of ImageNet.

Quantization setupIn all experiments, after the model is trained, we apply 8-bit PTQ. We use uniform affine quantization - symmetric weights, asymmetric activations - with the static activation range setting, as discussed in Section 2. We quantize all weights and activations (both input and output), except the final linear layer for BERT and OPT models. We explore several choices of range estimation (see Appendix C.4) and report the best configuration for each experiment, based on the model performance. We repeat each PTQ experiment 3 times with different random seeds8 and report mean and standard deviation for accuracy/perplexity.

We train each network two times with different random seeds and report mean and standard deviation. To assess the amount of outliers in the trained model, we use two metrics: the maximum \(\|\|_{}\) averaged across the validation set, and _kurtosis_ of \(\) averaged across all layers, where \(\) is the output of an attention layer. These metrics have been shown to correlate well with the model quantizability [4; 6].

### The impact of clipped softmax hyperparameters (\(\) and \(\))

We investigate the effect of different values of the clipped softmax stretch parameters and present the results in Table 1. We can see that most of the improvement happens when we use \(<0\) (clipping at zero). For instance, using the value of \(=-0.03\) leads to a significantly smaller infinity norm, kurtosis, and quantized model perplexity, compared to the baseline. It is also clear that in the limit \(|| 0\) we approach the vanilla softmax attention. Using \(>1\) (clipping at one) yields similar results to the vanilla softmax. Finally, when we combine both \(<0\) and \(>1\), for which the results seem similar to just clipping at 0. We, therefore, conclude that for dampening outliers, only the lower-range clipping allows exact zeros matter. Going forward we use only \(<0\) and in Appendix B.5 we confirm that \(>1\) is not required for ViT.

These observations are in line with our hypothesis that by giving the model the mechanism for representing exact zeros in the attention, we don't need to learn the strong outliers.

### Clipped softmax \(\) vs. sequence length

As having an extra hyper-parameter that needs to be tuned per model or setup is generally not desirable, we study the sensitivity of the stretch factor \(\) and its relation with the sequence length \(T\). Recall that the matrix of attention probabilities \(\) has dimensions \(T T\) and each row sums up to one. Because of that, the average value in \(\) is \(1/T\). It is reasonable to assume that if we define

   \(\) & \(\) & FP16 ppl\(\) & Max inf. norm & Avg. kurtosis & W8A8 ppl\(\) \\  \(0\) & \(1\) & \(4.49^{ 0.01}\) & \(735^{ 55}\) & \(3076^{ 262}\) & \(1294^{ 1046}\) \\  \(0\) & \(1.003\) & \(4.48^{ 0.01}\) & \(715^{ 335}\) & \(2159^{ 238}\) & \(451^{ 57}\) \\ \(0\) & \(1.03\) & \(4.49^{ 0.00}\) & \(741^{ 66}\) & \(1707^{ 1249}\) & \(1469^{ 646}\) \\  \(-0.003\) & \(1\) & \(4.46^{ 0.00}\) & \(688^{ 64}\) & \(2149^{ 110}\) & \(636^{ 566}\) \\ \(-0.03\) & \(1\) & \(4.41^{ 0.01}\) & \(20^{ 1}\) & \(80^{ 6}\) & \(4.55^{ 0.01}\) \\  \(-0.003\) & \(1.003\) & \(4.47^{ 0.00}\) & \(683^{ 23}\) & \(2494^{ 1205}\) & \(268^{ 120}\) \\ \(-0.03\) & \(1.03\) & \(4.43^{ 0.03}\) & \(22^{ 3}\) & \(73^{ 8}\) & \(4.56^{ 0.05}\) \\   

Table 1: The impact of clipped softmax hyperparameters on BERT-base.

\(:=-\), where \(>0\) is a new hyperparameter, there might be a set or a range of values of \(\) that works well across different sequence lengths.

To study this, we train a 6-layer variant of BERT-base (BERT-6L) for 500000 steps on WikiText-103  with a batch size of 128 with several values of maximum sequence lengths \(T\{32,64,128,192,256\}\) and values of \(\{1/4,1/2,1,2,4,8\}\). As we can see from Figure 6, using a clipped softmax with \(\) significantly dampens the magnitude of outliers while maintaining good FP16 perplexity across all explored sequence lengths.

### The impact of bias initialization in gated attention

In all our gated attention experiments, we randomly initialize the weights of \(\), following . By initializing the _bias_ to a specific value, however, we can set gates to be more _open_ or more _closed_ initially. More open at the start means we initialize closer to the original network, but given the exponential nature of the gate it might take many iterations for the gate to learn to close. Similarly, if the gates are all closed at the start, we deviate too far from the original model training, causing a potential decrease in performance. Assuming Linear \(_{i}\)'s with small initial weights, if we set the bias to the value of \(b_{}\), then \(_{i}() b_{}\) and \(_{i}()=(_{i}()) (b_{})=:_{}\), at the start of training.

We study the effect of different values of \(b_{}\) for Linear gated attention on BERT-6L and ViT. We set the bias for all \(_{i}\)'s to the same value of \(b_{}\). For BERT-6L, we use the same setup as in Section 5.2, with a fixed sequence length of 128. For ViT, we use the main setup, except we train it for 150 epochs instead of 300.

In Figure 7 we see in both BERT and ViT cases that using bias with very high \(_{}\) generally performs similarly to the vanilla attention (comparable floating-point performance but strong outliers and poor quantized performance) while setting bias to have very low \(_{}\) dampens outliers quite well but leads to strong degradation in the floating-point and quantized performance. The reasonable ranges of \(_{}\) seems to be around \([0.25,0.9]\) for BERT and \([0.1,0.5]\) for ViT. The wide range indicates the relative robustness of our method to this hyperparameter.

Figure 6: The performance of clipped softmax using \(=-/T\) parameterization on BERT-6L. (a) Relative (compared to vanilla softmax pre-training) FP16 log-perplexity \(\) on Wikitext validation set. (b) Maximum infinity norm of the attention layer output (note the logarithmic y-axis).

Figure 7: The performance of Linear gated attention using different bias initialization settings.

### Main results

We summarize our main set of results in Table 2. As we can see, in almost all cases, both of our proposed techniques dampen the outliers' magnitude to a great extent, reduce the kurtosis, and yield models with significantly higher quantized performance, which is close to the original FP16/32 performance. In addition to that, for each model, at least one of our methods also improves the floating-point task performance. We hypothesize this is because the network is helped with learning the "no-op" updates more easily. However, we are cautious about the improved performance as this is not consistent across all hyper-parameters and it is unclear if it generalizes to more architectures and larger models.

The only case where our method failed to perform well was the clipped softmax applied to OPT. At the moment, we do not have an explanation of why this is the case and leave it for future work. We list selected hyper-parameters and show extended results in Appendix B. We also show the results of our proposed methods quantized to lower bitwidths in Appendix B.7.

Results for bigger modelsWe study the question of scalability of our methods to larger models. In Table 3 we show the gated attention results for 350m and 1.3B variants of OPT. Due to compute constraints, we trained networks for \(10^{5}\) steps with batch size of 256 and the rest is the same as in our main pre-training setup. As we can see, our proposed gated attention is also very effective at dampening the outliers and significantly improving the quantized model performance when applied to bigger models. We further study in Appendix B.6 how gated attention can decrease outliers when fine-tuning bigger pre-trained models with outliers.

### Qualitative results

In Figure 8 we compare the learned attention patterns using vanilla softmax and our proposed methods (more examples in Appendix A.1). As we can see, both methods can represent a partial/soft no-op behavior, but in case of our methods this does not require strong outliers elsewhere in the network. Note that we found similar patterns in multiple attention heads, but the exact head indices where we observed such patterns depend on random initialization. In the case of clipped softmax, the attention probabilities are generally more diffused and smaller in magnitude (which comes from the stretching and clipping). In the case of gated attention, the output of the softmax is significantly different since the update of the hidden representation is now further modulated by gating probabilities.

   Model & Method & FP16/32 & Max inf. norm & Avg. kurtosis & W8A8 \\   BERT \\ (ppl.\(\)) \\  } & Vanilla & \(4.49^{ 0.01}\) & \(735^{ 55}\) & \(3076^{ 262}\) & \(1294^{ 1046}\) \\  & Clipped softmax & \(}\) & \(}\) & \(}\) & \(}\) \\  & Gated attention & \(4.45^{ 0.03}\) & \(39.2^{ 26.0}\) & \(201^{ 181}\) & \(4.65^{ 0.04}\) \\   OPT \\ (ppl.\(\)) \\  } & Vanilla & \(15.84^{ 0.05}\) & \(340^{ 47}\) & \(1778^{ 444}\) & \(21.18^{ 1.89}\) \\  & Clipped softmax & \(16.29^{ 0.07}\) & \(63.2^{ 8.8}\) & \(19728^{ 7480}\) & \(37.20^{ 2.40}\) \\  & Gated attention & \(}\) & \(}\) & \(}\) & \(}\) \\   ViT \\ (acc.\(\)) \\  } & Vanilla & \(80.75^{ 0.10}\) & \(359^{ 81}\) & \(1018^{ 471}\) & \(69.24^{ 6.93}\) \\  & Clipped softmax & \(80.89^{ 0.13}\) & \(}\) & \(22.9^{ 1.6}\) & \(79.77^{ 0.25}\) \\   & Gated attention & \(}\) & \(79.8^{ 0.5}\) & \(}\) & \(}\) \\   

Table 2: A summary of results for our proposed methods applied on BERT, OPT-125m, and ViT.

   Model & Method & FP16 & Max inf. norm & Avg. kurtosis & W8A8 \\   OPT-350m \\ (ppl.\(\)) \\  } & Vanilla & \(13.19\) & \(253\) & \(2689\) & \(37.52^{ 3.84}\) \\  & Gated attention & \(13.01\) & \(65.4\) & \(261\) & \(}\) \\   OPT-1.3B \\ (ppl.\(\)) \\  } & Vanilla & \(12.13\) & \(428\) & \(2756\) & \(989.6^{ 175}\) \\  & Gated attention & \(12.21\) & \(67.2\) & \(444\) & \(}\) \\   

Table 3: The performance of gated attention applied on bigger variants of OPT model.

## 6 Discussion

"No-op" behaviorIt is interesting to note that the identified "no-op" behavior is likely not limited to transformers and that convolutional architectures likely learn something similar. We also see that despite the network trying to learn a full "no-op", still a small amount of noise is added to each residual, which may constitute a form of network regularization. Investigating this further might give us a clue as to why neural networks generalize despite being significantly overparametrized if many parameters are rendered unused by not updating the representation in later layers .

LimitationsWe have not studied the effect of our method on large-scale transformers, as it would require training very expensive models from scratch. Given the fundamental understanding of the issue underlying our solutions, we expect the same effect on large-scale models. We show a very small improvement in FP16/FP32 performance due to our methods, but we do not deem our results exhaustive enough to claim that this will hold in general. Lastly, our methods do have a hyperparameter each, although we show that both methods are relatively robust to its hyperparameter, having one is never optimal.

ImpactAs our methods help transformers to be more efficient, we expect only positive outcomes of our work. Making neural networks more efficient will help with their high power consumption at inference. It further helps to move inference from the cloud to edge devices which can overcome potential privacy concerns. We cannot fathom any negative impact from our work that is not severely construed.

## 7 Conclusions

We have thoroughly analyzed the activation outlier problem that makes transformers difficult to quantize. We showed that transformer networks try to learn not to update residuals and that by doing so, through the combination of the softmax, residual connections and LayerNorm, significant outliers appear in transformers. Based on this insight, we proposed two methods to address this at the core - _clipped softmax_ and _gated attention_. These structural changes to transformers give similar, if not better, floating-point performance after training but significantly improve the post-training quantization results. We hope that with these two architectural changes to transformers, anyone can train high-performance transformers that are easy to quantize and can benefit from efficient integer inference.

Figure 8: Visualization of the self-attention patterns for BERT-base trained using vanilla and our proposed techniques, computed on data sequence #5 from MNLI-m validation set. (a), (b): attention probabilities, values, and their product. (c): gating probabilities \(=(())\), attention probabilities (output of softmax), values, and their combined product.