# Why Differentially-Private Local SGD

- An Analysis of Biased Synchronized-Only Iterates

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We argue to use Differentially-Private Local Stochastic Gradient Descent (DP-LSGD) in both centralized and distributed setups, and explain why DP-LSGD enjoys higher clipping efficiency and produces less clipping bias compared to classic Differentially-Private Stochastic Gradient Descent (DP-SGD). For both convex and non-convex optimization, we present generic analysis on noisy synchronized-only iterates in LSGD, the building block of federated learning, and study its applications to differentially-private gradient methods with clipping-based sensitivity control. We point out that given the current _decompose-then-compose_ framework, there is no essential gap between the privacy analysis of centralized and distributed learning, and DP-SGD is a special case of DP-LSGD. We thus build a unified framework to characterize the clipping bias via the second moment of local updates, which initiates a direction to systematically instruct DP optimization by variance reduction. We show DP-LSGD with multiple local iterations can produce more concentrated local updates and then enables a more efficient exploitation of the clipping budget with a better utility-privacy tradeoff. In addition, we prove that DP-LSGD can converge faster to a small neighborhood of global/local optimum compared to regular DP-SGD. Thorough experiments on practical deep learning tasks are provided to support our developed theory.

## 1 Introduction

Local Stochastic Gradient Descent (LSGD) [1; 2] and (Local/Client-Level) Differential Privacy (DP) [3; 4; 5] are two popular methods to address the issues of communication efficiency and data privacy, respectively. Rooted in the _FedAvg_ framework first proposed in , instead of communicating and synchronizing on the local updates from each user at each iteration, LSGD  randomly samples participants to perform gradient descent on their local data in parallel and only aggregates their local updates periodically. Though LSGD is a simple generalization of SGD to a distributed setup with a lower synchronization frequency, empirically it is known to produce promising performance, with regard to both communication efficiency and convergence rate . When each user holds i.i.d. data, LSGD provably achieves a linear speedup in the number of users with also asymptotic improvements on the communication overhead over regular distributed SGD to produce equivalent accuracy [1; 2].

As for privacy preservation, DP [3; 8] provides a semantically precise way to quantify the data leakage from any processing. At a high level, DP is an input-independent guarantee which ensures that an adversary cannot infer the participation of an individual datapoint easily from the release. For example, the classic \((,)\)-DP with small security parameters \(\) and \(\) implies a large Type I or Type II error for an adversarial hypothesis testing to guess whether an arbitrary individual is involved in the processing . In DP research, one key problem is to determine the _sensitivity_, the worst-case influence/change on the output of the objective processing after arbitrarily replacing an individual in an input set. Onlywith tractable sensitivity, one can then apply proper randomization/perturbation such as the Gaussian or Laplace mechanism  to produce required security parameters. Unfortunately, sensitivity is in general NP-hard to compute . To this end, in practice, a commonly-applied alternative is the _decompose-then-compose_ framework: a complicated processing is first (approximately) decomposed into several simpler (possibly adaptive) subroutines such as mean estimation, each of whose sensitivity is controllable. A _white-box_ adversary is then assumed who can observe the intermediate computations, and an upper bound on the privacy loss is derived by the composition of the leakage from the virtual release in each step .

In the applications of machine learning, where the processing function returns a model trained on possibly sensitive data, arguably the most popular and generic DP privatization method is DP-SGD . As a representative of the above-mentioned decompose-then-compose framework, DP-SGD views the SGD as a sequence of adaptive gradient mean estimations. To ensure a bounded sensitivity guarantee, each per-sample gradient is clipped, usually, in \(l_{2}\)-norm  to some constant \(c\), which is essentially a projection to an \(l_{2}\)-norm ball of radius \(c\). Noise, which is determined by both the number of iterations \(T\) and the clipping threshold \(c\) (sensitivity bound), is then added to the clipped stochastic gradient in each iteration to produce satisfied DP parameters \((,)\) under \(T\)-fold composition. A wider dimension and a longer convergence time \(T\) will consequently require a larger DP noise. Though the implementation of DP-SGD does not require any additional assumptions on either model or training data, it is notorious for heavy utility loss, especially for deep learning. Moreover, the understanding of the clipping bias from this artificial sensitivity control remains limited. In general, due to the bias, clipped SGD will not converge even without noise perturbation .

Given the artificial assumption that DP-SGD releases the intermediate computations, there is no essential gap between the privacy analysis of the centralized and local SGD, except that in the distributed setup one may apply different DP metrics such as Local DP (LDP)  or client-level DP  to consider the privacy preservation for each user's local data. More interestingly, it is worth noting the connection among different problems in federated learning and DP-SGD that are essentially equivalent. First, it is not hard to see that DP-SGD is a special case of DP-LSGD. DP-SGD can be viewed as: \(n\) nodes, each holds a sample, and a virtual server collects the clipped stochastic gradient from a subset of sampled nodes _in every iteration_, and publishes a noisy gradient descent. DP-LSGD can be similarly defined where the only difference is that the server may not synchronize on each iteration, but clips and aggregates a linear combinations of local gradients, _periodically_. Thus, as a primary concern in federated learning, a smaller communication overhead in a lower synchronization/aggregation frequency would also imply less leakage and a smaller composition bound of privacy loss. On the other hand, the study on the utility loss by perturbation and artificial sensitivity control (clipping) could also be used to analyze federated learning with compressed communication  where there exists quantification error in broadcasted local updates. Therefore, in this paper, we aim to provide a unified analysis for both noisy LSGD and DP-LSGD/SGD to get new insights. Before we can build useful theory to capture these concerns from different perspectives, several technical challenges need to be addressed.

**Utility of "Synchronized/Published" Iterate Only**: Many existing convergence results  on non-private LSGD are developed on the (weighted) average of all iterates. These include the intermediate iterates produced during the local updates from each user/node, which will not be exposed or shared. To properly characterize the effect of perturbation, a more appropriate and realistic convergence guarantee is to measure the performance of synchronized (shared) iterates only. This is also important to help understand the practical performance of LSGD as neither the server nor users have access to all intermediate computations. Such measurement is especially necessary when we apply LSGD in a private version: the utility of concern is only with respect to the released outputs, and anything assumed to be published would incur privacy loss and increase the scale of DP noise.

**Clipping Bias and Data Heterogeneity**: In practice, tight sensitivity of many data processing algorithms is intractable and thus a very popular but artificial control is clipping. However, clipping could also bring non-negligible bias. In general, there is no convergence guarantee for clipped SGD if we only assume the stochastic gradient is of bounded variance , though under more restrictive assumptions, for example, when the stochastic gradient is in a symmetric  or light-tailed  distribution, or provided generalized smoothness , some (near) convergence results are known. A concise characterization of such clipping bias still largely remains open, especially for deep learning. The bias is even more complicated in the more general DP-LSGD. To provide meaningful theory to instruct systematic bias reduction, we do not want to assume Lipschitz continuity or bounded gradient, which may make the analysis trivial and impractical. Thus, the desired analysis essentially captures the scenario given heavy data heterogeneity, and the results should not require a bounded difference among the local updates.

In this paper, through tackling the above-mentioned challenges, we aim to provide useful and intuitive theory to understand practical performance of LSGD and instruct optimization with DP guarantees. In particular, we want to explain how DP-LSGD out-performs regular DP-SGD. We summarize our contributions as follows.

1. With only a mild assumption that the stochastic gradient is of bounded variance, we present the convergence analysis on the released-only iterates of LSGD under perturbation for both convex and non-convex smooth optimization in Theorem 3.1 and 3.2. In particular, for the general convex case, we show more powerful last iterate convergence, which could be of independent interest in developing generic last-iterate analysis with unbounded gradients.
2. We then generalize our results to study the utility of DP-LSGD, where DP-SGD becomes a special case. In particular, we use the incremental norm of local update (see Definition 4.1) to characterize the clipping bias and show DP-LSGD has a faster convergence rate to a small neighborhood of global/local optimum as compared to DP-SGD.
3. We further show LSGD behaves as an efficient variance reduction of local update, where multiple local GDs with a small learning rate cancel out substantial sampling noise, and enable more efficient clipping compared to DP-SGD. Thorough experiments show that DP-LSGD produces a much sharpened utility-privacy tradeoff in practical deep learning.

### Related Works

**Convergence Analysis of LSGD**: With the increasing scale of both training data and models, federated learning has become an important paradigm in modern machine learning, where LSGD and its variants form the building block. Though the idea of LSGD can be traced back to earlier works [24; 25], the theoretical convergence analysis has only been proved recently. A common strategy to show convergence is to consider a virtual average of all the intermediate iterates produced by each user, and keep track of the divergence (dissimilarity) between the virtual average and the local iterate. In the setup where each user holds i.i.d. data, Stich in  studied strongly-convex optimization with LSGD and showed a linear speedup in the number of users/nodes.  presented non-convex analysis under the Lipschitz continuity assumption where the divergence of local update is also bounded.

For the more general applications with heterogeneous data,  studied the convex case with local GD (without sampling on either users or users' local data) but still under Lipschitz continuity.  presented more generic and tighter analysis for LSGD without assumptions on bounded gradient for both strongly and general convex optimization. Further generalization of LSGD to the decentralized setup under arbitrary network topology was considered in [19; 28; 29]. However, many existing works [2; 19; 28] only showed the convergence rate relying on all the intermediate averages. To our knowledge, the first generic analysis for synchronized-only iterates was shown in .  proposed Scaffold, a generalized LSGD with careful correction on the client-drift caused by data heterogeneity. Compared to existing works, in this paper, we prove more powerful last-iterate analysis for general convex optimization with clipping and perturbation for privacy. It is also worth mentioning that with a different motivation, there is another line of works also studying noisy LSGD to capture the effect of compressed local updates to further save the communication cost. But, in most existing related works [17; 31], the compression error is assumed to be independent with zero-mean. As we need to study DP-LSGD with clipped local update, which introduces bias in the local update generation, in this paper we present more involved analysis to handle such adaptive and biased perturbation.

**Convergence Analysis of DP-SGD and DP-LSGD**: Asymptotically, under Lipschitz continuity, DP-SGD is known to produce a tight utility-privacy tradeoff [32; 33], where no bias is produced given a clipping threshold larger than the Lipschitz constant. However, without Lipschitz continuity, practical understanding of DP-SGD remains limited. On one hand, negative examples are shown in [15; 16] where clipped-SGD in general will not converge, and in practice clipped-SGD does produce bias and has a lower convergence rate, especially in deep learning applications compared to regular SGD . On the other hand, under more restrictive assumptions on the stochastic gradient distribution, clipped-SGD can be shown to (nearly) converge [15; 22; 23]. A generic characterization on the clipping bias still largely remains open. As a consequence, there is little known meaningful theory to systematically instruct optimization algorithms with DP guarantees, and most existing private deep learning works are empirical, which aim to search for the optimal model and hyperparameters for objective training data [34; 35; 36]. As for DP-LSGD, to our knowledge the only known theoretical result that captures the clipping bias is . However,  still assumes globally bounded gradient compared to bounded second moment as assumed in our results, and its main motivation is to study the clipping effect in client-level DP. In this paper, we show more intuitive and generic analysis of DP-LSGD for both convex and non-convex optimization, and our motivations are also very different. We set out to provide usable quantification on the utility loss due to clipping and _we argue to apply DP-LSGD both in the centralized and distributed setup_, since DP-LSGD can significantly reduce the clipping bias with a faster convergence rate.

## 2 Preliminaries

We focus on the classic Empirical Risk Minimization (ERM) problem. Given a dataset \(=\{(x_{i},y_{i}),i=1,2,,n\}\), the loss function is defined as \(F(w)=_{i=1}^{n}fw,x_{i},y_{i}= _{i=1}^{n}f_{i}(w)\). We will consider the cases where the loss function \(f_{i}(w):^{+}\) is convex or non-convex. \(w^{*}=_{w}F(w)\) represents the global optimum. Some formal definitions about the properties of the objective loss function are defined as follows.

**Definition 2.1** (Smoothness).: _A function \(f\) is \(\)-smooth on \(\) if the gradient \( f(w)\) is \(\)-Lipschitz such that for all \(w,w^{}\), \(\| f(w)- f(w^{})\|\|w^{}-w\|\)._

**Definition 2.2** (Convexity and Strong Convexity).: _A function \(f(w)\) is \(\)-convex on \(\) if for all \(w,w^{}\), \(\|w-w^{}\|^{2} f(w)-f(w^{})- f(w ^{}),w-w^{}\). We call \(f(w)\) general convex if \(=0\), and \(f(w)\) is strongly convex if \(>0\)._

**Assumption 2.1** (Bounded Variance of Stochastic Gradient).: _For any \(w\) and an index \(i\) that is randomly selected from \(\{1,2,,n\}\), there exists \(>0\) such that \([\| F(w)- f_{i}(w)\|^{2}]\)._

Assumption 2.1 is the only additional assumption we need for the analysis of non-private LSGD without clipping. We formally present the non-private LSGD algorithm in Algorithm 1 which uses non-clipped local update (3). The whole process is formed of \(T\) phases. In each phase, by \(q\)-Poisson sampling, in expectation \((nq)\) many users will be selected to perform \(K\) local gradient descents on their local data before broadcasting the local update. To match the DP-LSGD where the local function \(f_{i}(w)\) held by each user may only be determined by a single datapoint, we do not consider an additional stochastic gradient oracle on the local function in Algorithm 1, but only assume random sampling on the user level at each phase. However, our results can be easily generalized to the scenario with stochastic local gradient. Moreover, we assume Poisson sampling in Algorithm 1 so as to match the setup of DP-LSGD, since given current studies on privacy amplification by sampling, Poisson sampling can produce the tightest results  (and has become the most popular option in practice [36; 38]). In the following, we introduce the definition of DP.

**Definition 2.3** (Differential Privacy ).: _Given a universe \(^{*}\), we say that two datasets \(X,X^{}^{*}\) are adjacent, denoted as \(X X^{}\), if \(X=X^{} x\) or \(X^{}=X x\) for some additional datapoint \(x\). A randomized algorithm \(\) is said to be \((,)\)-differentially-private (DP) if for any pair of adjacent datasets \(X,X^{}\) and any event set \(O\) in the output domain of \(\), it holds that_

\[((X) O) e^{}(( X^{}) O)+.\]

In Definition 2.3, we apply the unbounded DP definition as adopted in most existing DP-SGD works [16; 35; 38], where the two adjacent datasets are defined to differ in one datapoint. One may also apply the bounded DP definition  by defining the adjacent datasets as arbitrarily replacing a datapoint. However, as a stronger definition, bounded DP will also face a larger sensitivity bound.

We can now formally describe DP-LSGD and DP-SGD. In (2) of Algorithm 1, a clipping operation on a vector \(v\) with threshold \(c\) is defined as \((v,c)=v\{1,c/\|v\|\}\), which ensures a bounded sensitivity up to \(c\). Using the clipped local update (2), by selecting \(Q^{(t)}\) to be proper DP noise, Algorithm 1 captures DP-SGD when \(K=1\) and DP-LSGD for general \(K 1\). DP-LSGD (SGD) is essentially an LSGD (SGD) with clipped local update (per-sample gradient) and additional DP noise. Running for \(T\) iterations with a total privacy budget \((,)\), one may select \(Q^{(t)}(0,^{2}_{d})\) where \(=(qc/)\) by the composition bound . The privacy analysis and the noise bound are identical for both DP-LSGD and DP-SGD given the same clipping threshold \(c\).

We want to stress again that our motivation to study DP-LSGD is not because we only focus on the federated setup, but to provide a unified analysis of the clipping bias and argue for using DP-LSGD _even in the centralized setup_. Our results are straightforwardly applicable to distributed learning with local DP  or client-level DP , where the only difference is that we may add a larger noise \(Q^{(t)}\) determined by the number of local datapoints or the users involved, respectively, for these stronger DP definitions. As for the possible communication restriction where we need to add discrete noise of finite precision, one may replace the Gaussian noise by the Binomial mechanism .

## 3 Convergence of Synchronized-Only Iterate in Noisy Non-Clipped LSGD

In this section, we will study the convergence analysis of LSGD in Algorithm 1 using the non-clipped local update (3) for both convex and non-convex optimization.

**Theorem 3.1** (Last-iterate Convergence of Noisy LSGD in General Convex Optimization).: _For an objective function \(F(w)=_{i=1}^{n}f_{i}(w)\) where \(f_{i}(w)\) is convex and \(\)-smooth with variance-bounded gradient (Assumption 2.1), when \(<\{},,\}\), \((TK) 2\), and \(Q^{(t)}\) is an independent noise such that \([Q^{(t)}]=0\) and \([\|Q^{(t)}\|^{2}]}\), for some parameter \(}\) for \(t=1,2,,T\), Algorithm 1 with (3) ensures_

\[[F(^{(T)})] ^{(0)}-w^{*}\|^{2}}{(TK+1)}+( TK+1)6/(nq)+8K^{2}^{2}+}/ \] \[+5^{2}((TK)+1)\|^{(0)}-w^{*}\|^{ 2}+T8^{3}K^{3}+^{2}^{4}+3K^{2} ^{2}}{nq}+}\] \[=^{(0)}-w^{*}\|^{2}}{}+ nq}++},\; \;=O(1/).\]The proof can be found in Appendix A. To prove Theorem 3.1, with a careful analysis on \(\|^{(t)}-w^{*}\|^{2}\), we develop a new last-iterate analysis framework, different from existing works [40; 41; 42] which must count on the assumption of bounded gradient. In Theorem 3.1, we need to assume the noise \(Q\) to be independent and of zero-mean. Because we do not assume Lipschitz continuity of \(F(w)\), we cannot provide a meaningful upper bound of the deviation between \(F(w)\) and \(F(w+Q)\) for arbitrary \(w\) and \(Q\) in general. However, provided the Lipschitz assumption, Theorem 3.1 can be easily generalized to handle biased perturbation. In Section 4, with an additional assumption on the similarity of the local functions (Assumption 4.2), we will show how to handle the clipping bias as a special biased noise. When there is no noise \(}=0\), provided that \(K=O(T^{1/3}/(nq)^{2/3})\), we show LSSGD achieves \((^{(0)}-w^{*}\|^{2}+/(nq)^{2/3}}{})\) last-iterate convergence in general-convex optimization.

We now study the non-convex scenario.

**Theorem 3.2** (Synchronized-only Iterate Convergence of Noisy LSGD in Non-convex Optimization).: _For an arbitrary objective function \(F(w)=_{i=1}^{n}f_{i}(w)\), where \(f_{i}(w)\) is \(\)-smooth and satisfies Assumption 2.1, and for arbitrary perturbation (not necessarily independent or of zero mean) where \([\|Q^{(t)}\|^{2}]}\), when \(<\{},\}\), Algorithm 1 with (3) ensures that_

\[[^{T}\| F(^{ (t-1)})\|^{2}}{T}]&^{(0)})}{TK}+^{2}K^{2}}{nq}+^{T}[ \|Q^{(t)}_{i}\|^{2}]}{^{2}KT}\\ &=O(}{T^{2/3}(nq)^{1/3}}+^{2/3} K}}{(nq)^{2/3}}),\] (4)

_when we select \(=O(}{T^{1/3}K^{1/3}})\). In particular, when \(Q^{(t)}\) is independent and \([Q^{(t)}]=0\), and \(=(1/K)\), then_

\[[^{T}\| F(^{(t-1)})\|^{2}}{T}] O ^{(0)})}{ TK}++^{T} [\|Q^{(t)}\|^{2}]}{ TK}=O(++}).\]

The proof can be found in Appendix B. In Theorem 3.2, we provide an analysis on the effect of generic perturbation, which can also be used to capture the clipping bias in DP-LSGD. When there is no perturbation, Theorem 3.2 has two implications. First, we show to ensure \([\| F(^{(t)})\|^{2}]\), we need \(T=O(}{^{3/2}})\), which is tighter than the state-of-the-art results \(O(}+}{^{3/2}})\) in . Second, compared to \(O(1/T^{2/3})\), we also show that LSGD can converge faster in \(O(1/T)\) to a \(\)_-neighborhood_ of a saddle point. This is helpful to understand the practical performance of DP-LSGD with bias, as discussed in Section 4.2.

As a final remark, we want to mention it is possible to improve the convergence rate from \(O(1/T^{2/3})\) to \(O(1/T)\) via careful variance reduction or error feedback mechanism, such as Scaffold  or FedLin . However, the proper implementation of those advanced tricks in DP-LSGD with additional sensitivity control is not clear. As a first step to systematically study the generic clipping bias, in this paper we only focus on the regular LSGD. We will explain and discuss possible generalizations in Section 6.

## 4 Utility and Clipping Bias of DP-LSGD and DP-SGD

In this section, we move to study DP-LSGD with clipped local update (2) in Algorithm 1. To have a clear comparison with DP-SGD, we still consider the centralized setup and \(F(w)=1/n f_{i}(w)\) where each local function \(f_{i}(w)\) is determined by a single sample. To capture the clipping bias, we need to introduce a new term, termed _incremental norm_.

**Definition 4.1** (Incremental Norm).: _Consider applying the private and clipping version of Algorithm 1 with (2) on \(F(w)=_{i=1}^{n}f_{i}(w)\). In the \(t\)-th phase, we define \(_{i}^{(t)}=\| w_{i}^{(t)}\|>c(\| w _{i}^{(t)}\|-c)\) as the incremental norm of the local update from \(f_{i}(w)\) compared to the clipping threshold \(c\), for \(t=1,2,,T\)._

In Definition 4.1, the incremental norm \(_{i}^{(t)}\) simply quantifies the difference between the norm of the local update and its clipped version from \(f_{i}(w)\). In the following, we will always assume the DP noise injected \([\|Q^{(t)}\|^{2}]=^{2}d\), following the classic privacy analysis of DP-SGD .

It is not hard to observe that the clipped local update is essentially a scaled version of the original update, and thus virtually one may view DP-LSGD as a generalization of noisy LSGD but each local update applies a different and adaptively-selected learning rate. To show meaningful characterization on the difference among those learning rates, we need the following assumption as a generalization of bounded-variance stochastic gradient.

**Assumption 4.1** (Incremental norm of Bounded Second Moment).: _When applying the clipped version of Algorithm 1 via (2) on an objective function \(F(w)= f_{i}(w)\), \(_{i=1}^{n}(_{i}^{(t)})^{2}/n\) is upper bounded by \(^{2}\), for some global parameter \(\) for \(t=1,2,,T\)._

Assumption 4.1 basically states that in expectation the square of \(l_{2}\)-norm of each local update is bounded. Assumption 4.1 also suggests that \(_{i=1}^{n}_{i}^{(t)}/n \).

### Utility of DP-LSGD in Convex Optimization

Another assumption we need for the analysis of DP-LSGD on general convex optimization is the similarity among the local functions.

**Assumption 4.2** (\(\) Similarity).: _For \(F(w)=1/n_{i=1}^{n}f_{i}(w)\), local functions \(f_{i}\) are of \(\)-similarity to \(F\) such that for any \(w\), \(|f_{i}(w)-F(w)|\), for some constant \(>0\)._

The main reason why we need this additional Assumption 4.2 is because we do not assume Lipschitz continuity of \(F(w)\). Thus, we alternatively consider to use the similarity among local functions to characterize the deviation of the evaluation of \(F()\) on biased iterates.

**Theorem 4.1** (Last-iterate of DP-LSGD in General Convex Optimization).: _For an arbitrary objective function \(F(w)=_{i=1}^{n}f_{i}(w)\) where \(f_{i}(w)\) is convex and \(\)-smooth, and under Assumptions 2.1, 4.1 and 4.2, when \(=O(1/)\) and \(Q^{(t)}\) is independent DP noise such that \([Q^{(t)}]=0\) and \([\|Q^{(t)}\|^{2}]=^{2}d\), \(t=1,2,,T\), then DP-LSGD with clipping threshold \(c\) ensures that_

\[}[F(^{(T)})-F(w^{*})] =(}+)\|^{(0)}-w^{*}\|^ {2}\] (5) \[+(+})(1+}{ }+)+(}{}+1)}{c+}+^{2}d.\]

_When \(K=O(nq)\) and \(K=O(T)\), and for \((,)\)-DP, where \(=(}{n})\), we have that_

\[[F(^{(T)})-F(w^{*})]\] \[=}{c} ^{(0)}-w^{*}\|^{2}}{}+(}+)}_{(A)}+}{c}}_{(B)}+ {}{c}K^{1/2}(1/)dc^{2}}{n^{2} ^{2}}}_{(C)}.\]

The proof can be found in Appendix C. We focus on a practical scenario where \(=O(c)\), i.e., the incremental norm of local updates is in the same order of the clipping threshold \(c\) selected, and thus \((c+)/c=O(1)\). From Theorem 4.1, we show the last-iterate utility of DP-LSGD is captured by three terms: (A) a similar convergence rate as regular LSGD, (B) a clipping bias, and (C) the DP noise variance. First, ignoring the bias and noise, DP-LSGD still enjoys a convergence rate \((^{(0)}-w^{*}\|^{2}}{}+(}+ ))\), which is slightly worse compared to Theorem 3.2 with \((^{(0)}-w^{*}\|^{2}}{}+(nq} +))\) as a consequence of clipping which essentially applies different learning rates in each local update. Second, the clipping bias is captured by \(()/c\). This matches our intuition that a larger incremental norm \(\) combined with a smaller clipping threshold \(c\) will imply a more significant change on the local update and thus a larger bias. The last accumulated perturbation term is determined by the noise injected across each phase with an effect of \((K^{1/2}(1/)dc^{2}}{n^{2}^{2}})\) for \((,)\)-DP under \(T\)-fold composition.

As we consider the very generic setup with non-trival clipping, Theorem 3.2 cannot be directly compared to the classic DP-utility tradeoff  given Lipschitz continuity, where a utility loss \((/n)\)is tight for convex optimization under \((,)\)-DP. However, we have the following interesting observations. First, when we take the clipping threshold \(c=O()=O(1/)\) and \(K=O(T d/(n^{2}^{2}))\), DP-LSGD achieves the same optimal rate \((/n)\) ignoring the clipping bias. Second and more important, when the stochastic gradient variance \(\) is in the same order of the clipping bias \(O(/c)\), then by selecting \(c=()\) and \(K=(T)\), Theorem 4.1 suggests that DP-LSGD will converge in \(O(1/T)\) to an \(O(/c+^{2}})\) neighborhood of the global optimum. As a comparison, when we select \(K=1\) in Theorem 4.1, it becomes the analysis of DP-SGD but the convergence rate to the neighborhood of global optimum in the same scale \(O(/c+^{2}})\) is only \(O(1/)\). Moreover, as we will show in the next section, the local update bound \(\) in DP-SGD with \(K=1\) in practice would be much larger than that of DP-LSGD with a relatively larger \(K\). As a simple generalization, we also include an analysis of DP-LSGD on strongly-convex functions in Appendix D, and we move our focus to the non-convex optimization in the following.

### Utility of DP-LSGD in Non-convex Optimization

**Theorem 4.2** (DP-LSGD in Non-convex Optimization).: _For \(F(w)=_{i=1}^{n}f_{i}(w)\) where \(f_{i}(w)\) is \(\)-smooth and satisfies Assumptions 2.1 and 4.1, when \(=O(1/K)\), DP-LSGD ensures that_

\[[^{T}\| F(^{(t-1)})\|^{2}}{T}] ^{(0)})}{TK}+^{2}K^{2}}{nq}+ ^{2}/q+^{2}d}{^{2}K}.\] (6)

_When we select \(=O(})\) and \(K=(T)\), for \((,)\)-DP we have that_

\[[^{T}\| F(^{(t-1)})\|^{2}}{T}]=(^{(0)})}{T}++^{2}T}{q}+^{2}}).\] (7)

The proof can be found in Appendix E. For the analysis of DP-LSGD in non-convex optimization, we do _not_ need Assumption 4.2 on the similarity among local functions and Theorem 4.2 is simply obtained by substituting the clipping error from each phase into Theorem 3.2. To have a more clear picture, we still consider a practical scenario when \(=_{0}\) for some constant \(_{0}\) and the variance \(\) is also some constant. Then, from (7) we have that

\[[^{T}\| F(^{(t-1)})\|^{2}}{T}]=O ^{(0)})}{T}++_{0}^{2}}{q}+^{2}}=++^{2}}.\]

In other words, similar to the convex case, DP-LSGD will converge at a rate of \(O(1/T)\) to an \((1+d/(n^{2}^{2}))\) neighborhood of a saddle point given some constant sampling rate \(q\). As a comparison, for DP-SGD when \(K=1\), from Theorem 3.2 we can only ensure an \(O(1/)\) convergence rate to a same \((1+d/(n^{2}^{2}))\) neighborhood.

## 5 Why DP-LSGD Produces Less Bias and Better SNR

Throughout the previous section, we showed that asymptotically DP-LSGD enjoys a faster convergence rate to a neighborhood of (global/local) optimum compared to DP-SGD. We characterized the clipping bias mainly based on the second moment upper bound \(^{2}\) of the incremental norm \(_{i}^{(t)}\) of local updates. In this section, we proceed to empirically study the \(_{i}^{(t)}\), and the tradeoff between clipping bias and DP (Gaussian) noise in practical deep learning tasks. We will explain why DP-LSGD could produce smaller bias and enable more efficient clipping compared to DP-SGD.

To produce good utility-privacy tradeoff, a proper selection of the clipping threshold \(c\) is important. Many existing works are devoted to optimizing the selection of \(c\) by either grid searching  or adaptive fine-tuning . A smaller \(c\) requires less DP noise. But, as a tradeoff shown in Theorem 4.1 and 4.2, a smaller \(c\) and a consequently a larger \(\) will also lead to a heavier clipping bias. Thus, from the perspective of signal-to-noise ratio (SNR), an ideal scenario is that the \(l_{2}\)-norm of each local update is _concentrated_ such that we can maximize the efficiency of the clipping power \(c\) with a small clipping effect for most local updates. Interpreted via our developed theory of clipping bias, it is expected that given the clipping threshold \(c\), the incremental norm \(_{i}^{(t)}\) would be small, captured by \(\) in (5) and (7). In Fig. 1 (a,b), we plot various statistics of the incremental norm for DP-LSGD and DP-SGD, respectively, on training CIFAR10 . By our analysis, DP-LSGD usually should apply a smaller learning rate \(\). To have a fair comparison, we consider the normalized incremental norm \(_{i}^{(t)}/\). Given the same clipping threshold, comparing Fig. 1 (a) and (b), the mean of normalized incremental norm, captured by \(/\) in our theorems, of DP-LSGD is only around 32% compared to that of DP-SGD. The corresponding standard deviation is around only 40% compared to that of DP-SGD. One may also compare the 25% and 75% quantiles, which suggest that more local updates bear less clipping influence in DP-LSGD and thus enjoying a higher clipping efficiency. We also report the comparison when training ResNet20  on SVHN  in Fig. 2 in Appendix F with similar observations. Details of experiment setups and the anonymous GitHub code link can be found in Appendix F.

In Fig.1 (c), we record the performance of DP-LSGD and DP-SGD, which coincides with our theory that DP-LSGD has a smaller clipping bias and a faster convergence rate. The smaller incremental norm in DP-LSGD is not surprising. With relatively larger \(K\), for each individual function \(f_{i}(w)\), though the \(K\) local gradients are correlated and essentially determined by a single sample, the aggregation of them still averages out substantial sampling noise and makes the \(l_{2}\)-norm of local updates more concentrated. In Table 1, we include additional comparison between their performance on CIFAR10  and SVHN ; DP-LSGD produces significant improvements.

## 6 Conclusion and Prospects

In this paper, via LSGD, we provide a unified analysis of the clipping bias and the utility loss in privacy-preserving gradient methods for both centralized and distributed setups. Provided the generic analysis, we develop the connections between the bias and the second moment of local updates. This initializes a new direction to systematically instruct private learning by connecting the research of variance reduction in distributed optimization. In this paper we only focus on regular LSGD to show its advantage over DP-SGD, but advanced acceleration methods [30; 31; 43] are known in non-private federated learning to further reduce the "local-update drift" caused by (per-sample) data heterogeneity. This could then further reduce the clipping bias given local updates of smaller variance. Thus, a promising future direction is to understand and incorporate those techniques within the sensitivity control framework. Another important issue we have not fully explored is the software implementation of DP-LSGD in the centralized case. For DP-SGD, many PyTorch libraries with fast per-sample gradient computation in low memory overhead have been developed, such as Opacus . However, in all above-presented experiments, we simulate DP-LSGD in a distributed environment and compute each local update in parallel at a cost of large memory. Given limited hardware resources, this restricts the application of larger batchsize (tens of thousands) and deploying deeper neural networks, which are known to produce much better utility-privacy tradeoffs [36; 49]. We leave empirical efficiency improvement to future work.

   Dataset and Method \(\)\(\) & \(1.5\) & \(2.0\) & \(2.5\) & \(3.0\) & \(3.5\) & \(4.0\) \\  CIFAR10, DP-LSGD (\(K=10\)) & \(59.4( 0.5)\) & \(64.0( 0.3)\) & \(66.2( 0.4)\) & \(67.7( 0.3)\) & \(68.7( 0.2)\) & \(69.9( 0.3)\) \\  CIFAR10, DP-SGD (\(K=1\)) & \(49.8( 1.2)\) & \(58.7( 1.0)\) & \(59.9( 1.2)\) & \(60.6( 0.8)\) & \(62.1( 0.6)\) & \(62.8( 0.6)\) \\  SVHN, DP-LSGD (\(K=10\)) & \(83.2( 0.4)\) & \(84.4( 0.5)\) & \(85.7( 0.5)\) & \(85.4( 0.4)\) & \(86.1( 0.4)\) & \(86.5( 0.3)\) \\  SVHN, DP-SGD (\(K=1\)) & \(74.5( 0.8)\) & \(78.2( 0.6)\) & \(79.8( 0.6)\) & \(80.3( 1.0)\) & \(81.7( 0.4)\) & \(82.2( 0.5)\) \\   

Table 1: **Test Accuracy** of ResNet20 on CIFAR10 and SVHN via DP-LSGD and DP-SGD under various \(\) and fixed \(=10^{-5}\), with expected batch size 1000.

Figure 1: Training ResNet 20 on CIFAR10 with DP-LSGD \((K=10,=0.025,c=1)\) and DP-SGD \((K=1,=1,c=1)\) under \((=2,=10^{-5})\)-DP, with expected batch size 1000.