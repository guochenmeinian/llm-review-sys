# Cardinality-Aware Set Prediction

and Top-\(k\) Classification

 Corinna Cortes

Google Research

New York, NY 10011

corinna@google.com &Anqi Mao

Courant Institute

New York, NY 10012

aqmao@cims.nyu.edu &Christopher Mohri

Stanford University

Stanford, CA 94305

xmohri@stanford.edu &Mehryar Mohri

Google Research & CIMS

New York, NY 10011

mohri@google.com &Yutao Zhong

Courant Institute

New York, NY 10012

yutao@cims.nyu.edu

###### Abstract

We present a detailed study of cardinality-aware top-\(k\) classification, a novel approach that aims to learn an accurate top-\(k\) set predictor while maintaining a low cardinality. We introduce a new target loss function tailored to this setting that accounts for both the classification error and the cardinality of the set predicted. To optimize this loss function, we propose two families of surrogate losses: cost-sensitive comp-sum losses and cost-sensitive constrained losses. Minimizing these loss functions leads to new cardinality-aware algorithms that we describe in detail in the case of both top-\(k\) and threshold-based classifiers. We establish \(\)-consistency bounds for our cardinality-aware surrogate loss functions, thereby providing a strong theoretical foundation for our algorithms. We report the results of extensive experiments on CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets demonstrating the effectiveness and benefits of our cardinality-aware algorithms.

## 1 Introduction

Top-\(k\) classification consists of predicting the \(k\) most likely classes for a given input, as opposed to solely predicting the single most likely class. Several compelling reasons support the adoption of this framework. First, it enhances accuracy by allowing the model to consider the top \(k\) predictions, accommodating uncertainty and providing a more comprehensive prediction. This is particularly valuable in scenarios where multiple correct answers exist, such as image tagging, where a top-\(k\) classifier can identify multiple relevant objects in an image. Second, top-\(k\) classification is applicable in ranking and recommendation tasks such as suggesting the top \(k\) most relevant products in e-commerce based on user queries. The confidence scores associated with the top \(k\) predictions also serve as a means to estimate the model's uncertainty, which is crucial in applications requiring insight into the model's confidence level.

The predictions of a top-\(k\) classifier are also useful in several natural settings. For example, ensemble learning can benefit from top-\(k\) predictions as they can be combined from multiple models, contributing to improved overall performance by introducing a more robust and diverse set of predictions. In addition, top-\(k\) predictions can serve as input for downstream tasks like natural language generation or dialogue systems, enhancing the performance of these tasks by providing a broader range of potential candidates. Finally, the interpretability of the model's decision-making process is enhanced by examining the top \(k\) predicted classes, allowing users to gain insights into the rationale behind the model's predictions.

The appropriate \(k\) for a task at hand may be determined by the application itself like a recommender system always expecting a fixed set size to be returned. For other applications, it may be natural to let the cardinality of the returned set vary with the model's confidence or other properties of the task. Designing effective algorithms with learning guarantees for this setting is our main goal.

In this paper, we introduce the problem of cardinality-aware set prediction, which is to learn an accurate set predictor while maintaining a low cardinality. The core idea is that an effective algorithm should dynamically adjust the cardinality of its prediction sets based on input instances. For top-\(k\) classifiers, this means selecting a larger \(k\) for difficult inputs to ensure high accuracy, while opting for a smaller \(k\) for simpler inputs to maintain low cardinality. Similarly, for threshold-based classifiers, a lower threshold can be used for difficult inputs to minimize the risk of misclassification, whereas a higher threshold can be applied to simpler inputs to reduce cardinality.

To tackle this problem, we introduce a novel target loss function which captures both the classification error and the cardinality of a prediction set. Minimizing this target loss function directly is an instance-dependent cost-sensitive learning problem, which is intractable for most hypothesis sets. Instead, we derive two families of general surrogate loss functions that benefit from smooth properties and favorable optimization solutions.

To provide theoretical guarantees for our cardinality-aware top-\(k\) approach, we first study consistency properties of surrogate loss functions for the general top-\(k\) problem with a fixed \(k\). Unlike standard classification, the consistency of surrogate loss functions for the top-\(k\) problem has been relatively unexplored. A crucial property in this context is the asymptotic notion of _Bayes-consistency_, which has been extensively studied in standard binary and multi-class classification (Zhang, 2004; Bartlett et al., 2006; Zhang, 2004; Bartlett and Wegkamp, 2008). While Bayes-consistency has been explored for various top-\(k\) surrogate losses (Lapin et al., 2015, 2016, 2018; Yang and Koyejo, 2020; Thilagar et al., 2022), some face limitations. Non-convex "hinge-like" surrogates (Yang and Koyejo, 2020), surrogates inspired by ranking (Usunier et al., 2009), and polyhedral surrogates (Thilagar et al., 2022) cannot lead to effective algorithms as they cannot be efficiently computed and optimized. Negative results also indicate that several convex "hinge-like" surrogates (Lapin et al., 2015, 2016, 2018) fail to achieve Bayes-consistency (Yang and Koyejo, 2020). On the positive side, it has been shown that the logistic loss (or cross-entropy loss used with the softmax activation) is a Bayes-consistent loss for top-\(k\) classification (Lapin et al., 2015; Yang and Koyejo, 2020).

We show that, remarkably, several widely used families of surrogate losses used in standard multi-class classification admit \(\)_-consistency bounds_(Awasthi, Mao, Mohri, and Zhong, 2022; Mao, Mohri, and Zhong, 2023; Zhang, 2023) with respect to the top-\(k\) loss. These are strong non-asymptotic consistency guarantees that are specific to the actual hypothesis set \(\) adopted, and therefore also imply asymptotic Bayes-consistency. We establish this property for the broad family of _comp-sum losses_(Mao, Mohri, and Zhong, 2023), comprised of the composition of a non-decreasing and non-negative function with the sum exponential losses. This includes the logistic loss, the sum-exponential loss, the mean absolute error loss, and the generalized cross-entropy loss. Additionally, we extend these results to _constrained losses_, a family originally introduced for multi-class SVM (Lee et al., 2004), which includes the constrained exponential, hinge, squared hinge, and \(\)-margin losses. The guarantees of \(\)-consistency provide a strong foundation for principled algorithms in top-\(k\) classification by directly minimizing these surrogate loss functions.

We then leverage these results to derive strong guarantees for the two families of cardinality-aware surrogate losses: cost-sensitive comp-sum and cost-sensitive constrained losses. Both families are obtained by augmenting their top-\(k\) counterparts (Lapin et al., 2015, 2016; Berrada et al., 2018; Reddi et al., 2019; Yang and Koyejo, 2020; Thilagar et al., 2022) with instance-dependent cost terms. We establish strong \(\)-consistency bounds, implying Bayes-consistency, for both families relative to the cardinality-aware target loss. Our \(\)-consistency bounds for the top-\(k\) problem are further beneficial here in that the cardinality-aware problem can consist of fixing and selecting from a family top-\(k\) classifiers-we now know how to effectively learn each top-\(k\) classifier.

The rest of the paper is organized as follows. In Section 2, we formally introduce the cardinality-aware set prediction problem along with our new families of surrogate loss functions. Section 3 instantiates our algorithms in the case of both top-\(k\) classifiers and threshold-based classifiers, and Section 4 presents strong theoretical guarantees. In Section 5, as well as in Appendix J and Appendix K, we present experimental results on the CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets, demonstrating the effectiveness of our algorithms.

Cardinality-aware set prediction

In this section, we introduce cardinality-aware set prediction, where the goal is to devise algorithms that dynamically adjust the prediction set's size based on the input instance to both achieve high accuracy and maintain a low average cardinality. Specifically, for top-\(k\) classifiers, our objective is to determine a suitable cardinality \(k\) for each input \(x\), with higher values of \(k\) for instances that are more difficult to classify.

To address this problem, we first define a cardinality-aware loss function that accounts for both the classification error and the cardinality of the set predicted (Section 2.1). However, minimizing this loss function directly is computationally intractable for non-trivial hypothesis sets. Thus, to optimize it, we introduce two families of surrogate losses: cost-sensitive comp-sum losses (Section 2.2) and cost-sensitive constrained losses (Section 2.3). We will later show that these loss functions benefits from favorable guarantees in terms of \(\)-consistency (Section 4.3).

### Cardinality-aware problem formulation and loss function

The learning setup for cardinality-aware set prediction is as follows.

Problem setup.We denote by \(\) the input space and \(=[n]=\{1,,n\}\) the label space. Let \(\{_{k} k\}\) denote a collection of given set predictors, induced by a parameterized set predictor \(_{k} 2^{}\), where each \(\) is a set of indices. This could be a subset of the family of top-\(k\) classifiers induced by some classifier \(h\), or a family of threshold-based classifiers based on some scoring function \(s\). In that case, \(_{k}(x)\) then comprises the set of \(y\)s with a score \(s(x,y)\) exceeding the threshold \(_{k}\) defining \(_{k}\). This formulation covers as a special case standard conformal prediction set predictors (Shafer and Vovk, 2008), as well as set predictors defined as confidence sets described in (Denis and Hebiri, 2017). We will denote by \(|_{k}(x)|\) the cardinality of the set \(_{k}(x)\) predicted by \(_{k}\) for the input \(x\). To simplify the discussion, we will assume that \(|_{k}(x)|\) is an increasing function of \(k\), for any \(x\). For a family of top-\(k\) classifiers or threshold-based classifiers, this simply means that they are sorted in increasing order of \(k\) or decreasing order of the threshold values.

To account for the cost associated with cardinality, we introduce a non-negative and increasing function \(_{+}_{+}\), where \((|_{k}(x)|)\) represents the _cost_ associated to the cardinality \(|_{k}(x)|\). Common choices for \(\) include \((|_{k}(x)|)=|_{k}(x)|\), or a logarithmic function \((|_{k}(x)|)=(|_{k}(x)|)\) as in our experiments (see Section 5), to moderate the magnitude of the cost relative to the binary classification loss. Our analysis is general and requires no assumption about \(\).

Our goal is to learn to assign to each input instance \(x\) the most appropriate index \(k\) to both achieve high accuracy and maintain a low average cardinality.

Cardinality-aware loss function.As in the ordinary multi-class classification problem, we consider a family \(\) of scoring functions \(r\). For any \(x\), \(r(x,k)\) denotes the score assigned to the _label_ (or index) \(k\), given \(x\). The label predicted is \((x)=*{argmax}_{k}r(x,k)\), with ties broken in favor of the largest index. To account for both classification accuracy and cardinality cost, we define the _cardinality-aware loss function_ for a scoring function \(r\) and input-output label pair \((x,y)\) as a linearized loss of these two criteria:

\[(r,x,y)=1_{y_{(x)}(x)}+\,| _{(x)}(x)|,\] (1)

where the first term is the standard loss for a top-\(k\) prediction taking the value one when the correct label \(y\) is not included in the top-\(k\) set and zero otherwise, and \(>0\) is a hyperparameter that governs the balance between prioritizing accuracy versus limiting cardinality. The learning problem then consists of using a labeled training sample \((x_{1},y_{1}),(x_{m},y_{m})\) drawn i.i.d. from some (unknown) distribution \(\) to select \(r\) with a small expected cardinality-aware loss \(_{(x,y)}[(r,x,y)]\).

The loss function (1) can be equivalently expressed in terms of an instance-dependent cost function \(c_{+}\):

\[(r,x,y)=c(x,(x),y),\] (2)

where \(c(x,k,y)=1_{y_{k}(x)}+(|_{k}(x)|)\). Minimizing (2) is an instance-dependent cost-sensitive learning problem. However, directly minimizing this target loss is intractable. To optimize this loss function, we introduce two families of surrogate losses in the next sections: cost-sensitive comp-sum losses and cost-sensitive constrained losses. Note that throughout this paper, we will denote all target (or true) losses on which performance is measured with an \(\), while surrogate losses introduced for ease of optimization are denoted by \(\).

### Cost-sensitive comp-sum surrogate losses

Our surrogate cost-sensitive comp-sum, _\(c\)-comp_, losses are defined as follows: for all \((r,x,y)\), \(_{}(r,x,y)=_{k}(1-c(x,k,y)) _{}(r,x,k),\) where the comp-sum loss \(_{}\) is defined as in (Mao, Mohri, and Zhong, 2023). That is, for any \(r\) in a hypothesis set \(\) and \((x,y)\), \(_{}(r,x,y)=_{y^{} y}e^{r (x,y^{})-r(x,y)}\), where \(_{+}_{+}\) is non-decreasing. See Section 4.2 for more details. For example, when the logistic loss is used, we obtain the cost-sensitive logistic loss:

\[_{}(r,x,y)=_{k}(1-c(x,k,y)) _{}(r,x,k)=_{k}(c(x,k,y)-1) -\!(_{k^{}}e^{r(x,k^{})-r(x,k) }).\]

The negative log-term becomes larger as the score \(r(x,k)\) increases. Thus, the loss function imposes a greater penalty on higher scores \(r(x,k)\) through a penalty term (\(c(x,k,y)\) - \(1\)) that depends on the cost assigned to the expert's prediction \(_{k}(x)\).

### Cost-sensitive constrained surrogate losses

Constrained losses are defined as a summation of a function \(\) applied to the scores, subject to a constraint, as in (Lee et al., 2004). For any \(r\) and \((x,y)\), they are expressed as

\[_{}(h,x,y)=_{y^{} y}(-r(x,y^{ })),_{y}r(x,y)=0,\]

where \(_{+}\) is non-increasing. See Section 4.2 for a detailed discussion. Inspired by these constrained losses, we introduce a new family of surrogate losses, _cost-sensitive constrained (\(c\)-cstnd losses) which are defined, for all \((r,x,y)\), by \(_{}(r,x,y)=_{k}c(x,k,y) (-r(x,k)),\) with the constraint \(_{k}r(x,k)=0,\) where \(_{+}\) is non-increasing. For example, for \((t)=e^{-t}\), we obtain the cost-sensitive constrained exponential loss:

\[_{}^{}(r,x,y)=_{k}c(x,k,y)e^{r(x,k)},_{k}r(x,k)=0.\]

## 3 Cardinality-aware algorithms

Minimizing the cost-sensitive surrogate loss functions described in the previous section directly leads to novel cardinality-aware algorithms. In this section, we briefly detail the instantiation of our algorithms in the specific cases of top-\(k\) classifiers (our main focus) and threshold-based classifiers.

**Top-\(k\) classifiers.** Here, the collection of set predictors is a subset of the top-\(k\) classifiers, defined by \(_{k}(x)=\{_{1}(x),,_{k}(x)\}\), where \(_{1}(x),,_{k}(x)\) are the induced top-\(k\) labels for a classifier \(h\). The cardinality in this case coincides with the index: \(|_{k}(x)|=k\), for any \(x\). The cost is defined as \(c(x,k,y)=1_{y\{_{1}(x),,_{k}(x)\}}+ (k)\), where \((k)\) can be chosen to be \(k\) or \((k)\). Thus, our cardinality-aware algorithms for top-\(k\) classification can be described as follows. At training time, we assume access to a sample set \(\{(x_{i},y_{i})\}_{i=1}^{m}\) and the costs each top-\(k\) set incurs, \(\{c(x_{i},k,y_{i})\}_{i=1}^{m}\), where \(k\), a pre-fixed subset. The goal is to minimize the target cardinality-aware loss function \(_{i=1}^{m}(r,x_{i},y_{i})=_{i=1}^{m}c(x_{i},r(x_{i}),y_{i})\) over a hypothesis set \(\). Our algorithm consists of minimizing a surrogate loss such as the cost-sensitive logistic loss, defined as \(=*{argmin}_{r}_{i=1}^{m}_{k }\!(1-c(x_{i},k,y_{i}))\!(_{k^{} }e^{r(x,k^{})-r(x,k)})\). At inference time, we use the top-\((x)\) set \(\{_{1}(x),,_{i(x)}(x)\}\) for prediction, with the accuracy \(1_{y\{_{1}(x),,_{i(x)}(x)\}}\) and cardinality \((x)\) for that instance.

In Section 5, we compare the accuracy-versus-cardinality curves of our cardinality-aware algorithms obtained by varying \(\) with those of top-\(k\) classifiers, demonstrating the effectiveness of our algorithms. What \(\) to select for a given application will depend on the desired accuracy. Note that the performance of the algorithm in (Denis and Hebiri, 2017) in this setting is theoretically the same as that of top-classifiers. The algorithm is designed to maximize accuracy within a constrained cardinality of \(k\), and it always reaches maximal accuracy at the boundary \(K\) after the cardinality is constrained to \(k K\).

**Threshold-based classifiers.** Here, the set predictor is defined via a set of thresholds \(_{k}\): \(_{k}(x)=\{y s(x,y)>_{k}\}\). When the set is empty, we just return \(*{argmax}_{y}s(x,y)\) by default. The description of the costs and other components of the algorithms is similar to that of top-\(k\) classifiers. A special case of threshold-based classifier is conformal prediction (Shafer and Vovk, 2008), which is a general framework that provides provably valid confidence intervals for a black-box scoring function. Split conformal prediction guarantees that \((Y_{m+1} C_{s,}(X_{m+1})) 1-\) for some scoring function \(s\), where \(C_{s,}(X_{m+1})=\{y s(X_{m+1},y)_{}\}\) and \(_{}\) is the \((m+1)/m\) empirical quantile of \(s(X_{i},Y_{i})\) over a held-out set \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) drawn i.i.d. from some distribution \(\) (or just exchangeably). Note, however, that the framework does not supply an effective guarantee on the size of the sets \(C_{s,}(X_{m+1})\).

In Appendix K, we present in detail a series of early experiments for our algorithm used with threshold-based classifiers and include more discussion. Our experiments suggest that, when the training sample is sufficiently large, our algorithm can outperform conformal prediction.

## 4 Theoretical guarantees

Here, we present theory for our cardinality-aware algorithms. Our analysis builds on theory of top-\(k\) algorithms, and we start by providing stronger results than previously known for top-\(k\) surrogates.

### Preliminaries

We denote by \(\) a distribution over \(\) and write \(p(x,y)=(Y=y X=x)\) for the conditional probability of \(Y=y\) given \(X=x\), and use \(p(x)=(p(x,1),,p(x,n))\) to denote the corresponding conditional probability vector. We denote by \(_{} \) a loss function defined for the family of all measurable functions \(_{}\). Given a hypothesis set \(_{}\), the conditional error of a hypothesis \(h\) and the best-in-class conditional error are defined as follows: \(_{}(h,x)=_{y x}[(h,x,y)]=_{y y}p(x,y) (h,x,y)\) and \(_{}^{*}(,x)=_{h}_{ }(h,x)\). Accordingly, the generalization error of a hypothesis \(h\) and the best-in-class generalization error are defined by: \(_{}(h)=_{(x,y)}[(h,x,y)]=_{x}[_{}(h,x)]\) and \(_{}^{*}()=_{h}_{}( h)=_{h}_{x}[_{}(h,x)]\). Given a score vector \((h(x,1),,h(x,n))\) generated by hypothesis \(h\), we sort its components in decreasing order and write \(_{k}(x)\) to denote the \(k\)-th label, that is \(h(x,_{1}(x)) h(x,_{2}(x)) h(x,_{n}(x))\). Similarly, for a given conditional probability vector \(p(x)=(p(x,1),,p(x,n))\), we write \(_{k}(x)\) to denote the \(k\)-th element in decreasing order, that is \(p(x,_{1}(x)) p(x,_{2}(x)) p(x, _{n}(x))\). In the event of a tie for the \(k\)-th highest score or conditional probability, the label \(_{k}(x)\) or \(_{k}(x)\) is selected based on the highest index when considering the natural order of labels.

The target generalization error for top-\(k\) classification is given by the top-\(k\) loss, which is denoted by \(_{k}\) and defined, for any hypothesis \(h\) and \((x,y)\) by

\[_{k}(h,x,y)=1_{y\{_{1}(x),,_{k}(x)\}}.\]

The loss takes value one when the correct label \(y\) is not included in the top-\(k\) predictions made by the hypothesis \(h\), zero otherwise. In the special case where \(k=1\), this is precisely the familiar zero-one classification loss. Like the zero-one loss, optimizing the top-\(k\) loss is NP-hard for common hypothesis sets. Therefore, alternative surrogate losses are typically used to design learning algorithms. A crucial property of these surrogate losses is _Bayes-consistency_. This requires that, asymptotically, nearly minimizing a surrogate loss over the family of all measurable functions leads to the near minimization of the top-\(k\) loss over the same family (Steinwart, 2007).

**Definition 4.1**.: A surrogate loss \(\) is said to be _Bayes-consistent with respect to the top-\(k\) loss \(_{k}\)_ if, for all given sequences of hypotheses \(\{h_{n}\}_{n}_{}\) and any distribution, \(_{n}_{}(h_{n})\) - \(_{}^{*}(_{})=0\) implies \(_{n+}_{_{k}}h_{n}-_{_ {k}}^{*}_{}=0\).

Bayes-consistency is an asymptotic guarantee and applies only to the family of all measurable functions. Recently, Awasthi, Mao, Mohri, and Zhong (2022, 2022) (see also (Awasthi et al., 2021, 2021, 2023, 2023, 2023, 2024, 2024, 2024, 2025, 2026, 2027, 2028)) proposed a stronger consistency guarantee, referred to as \(\)_-consistency bounds_. These are upper bounds on the target estimation error in terms of the surrogate estimation error that are non-asymptotic and hypothesis set-specific.

**Definition 4.2**.: Given a hypothesis set \(\), a surrogate loss \(\) is said to admit an \(\)-consistency bound with respect to the top-\(k\) loss \(_{k}\) if, for some non-decreasing function \(f\), the following inequality holds for all \(h\) and for any distribution: \(f_{_{k}}(h)-_{_{k}}^{*}() _{}(h)-_{}^{*}( )\).

We refer to \(_{_{k}}(h)-_{_{k}}^{*}()\) as the target estimation error and \(_{}(h)-_{}^{*}( )\) as the surrogate estimation error. These bounds imply Bayes-consistency when \(=_{}\), by taking the limit.

A key quantity appearing in \(\)-consistency bounds is the _minimizability gap_, which measures the difference between the best-in-class generalization error and the expectation of the best-in-class conditional error, defined for a given hypothesis set \(\) and a loss function \(\) by: \(_{}()=_{}^{*}()-_{x}_{}^{*}(,x)\). As shown by Mao, Mohri, and Zhong (2023), the minimizability gap is non-negative and is upper bounded by the approximation error \(_{}()=_{}^{*}()-_{}^{*}(_{})\): \(0_{}()_{}()\). When \(=_{}\) or more generally \(_{}()=0\), the minimizability gap vanishes. However, in general, it is non-zero and provides a finer measure than the approximation error. Thus, \(\)-consistency bounds provide a stronger guarantee than the excess error bounds.

### Theoretical guarantees for top-\(k\) surrogate losses

We study the surrogate loss families of _comp-sum_ losses and _constrained_ losses in multi-class classification, which have been shown in the past to benefit from \(\)-consistency bounds with respect to the zero-one classification loss, that is \(_{k}\) with \(k=1\)(Awasthi et al., 2022, Mao et al., 2023) (see also (Zheng et al., 2023, Mao et al., 2023)). We extend these results to top-\(k\) classification and prove \(\)-consistency bounds for these loss functions with respect to \(_{k}\) for any \(1 k n\).

Another commonly used family of surrogate losses in multi-class classification is the _max_ losses, which are defined through a convex function, such as the hinge loss function applied to the margin (Crammer and Singer, 2001, Awasthi et al., 2022). However, as shown in (Awasthi et al., 2022), no non-trivial \(\)-consistency guarantee holds for max losses with respect to \(_{k}\), even when \(k=1\).

We first characterize the best-in-class conditional error and the conditional regret of top-\(k\) loss, which will be used in the analysis of \(\)-consistency bounds. We denote by \(S^{[k]}=\{X S|X|=k\}\) the set of all \(k\)-subsets of a set \(S\). We will study any hypothesis set that is regular.

**Definition 4.3**.: Let \(A(n,k)\) be the set of ordered \(k\)-tuples with distinct elements in \([n]\). We say that a hypothesis set \(\) is _regular for top-\(k\) classification_, if the top-\(k\) predictions generated by the hypothesis set cover all possible outcomes: \(\,x\), \(\{(_{1}(x),,_{k}(x)) h\}=A(n,k)\).

Common hypothesis sets such as that of linear models or neural networks, or the family of all measurable functions, are all regular for top-\(k\) classification.

**Lemma 4.4**.: _Assume that \(\) is regular. Then, for any \(h\) and \(x\), the best-in-class conditional error and the conditional regret of the top-\(k\) loss can be expressed as follows:_

\[_{_{k}}^{*}(,x)=1-_{i=1}^{k}p(x,_{i}(x ))_{_{k},}(h,x)=_{i=1}^{k}[p(x, _{i}(x))-p(x,_{i}(x))].\]

The proof is included in Appendix A. For \(k=1\), the result coincides with the known identities for standard multi-class classification with regular hypothesis sets (Awasthi et al., 2022, Lemma 3).

As with (Awasthi et al., 2022, Mao et al., 2023), in the following sections, we will consider hypothesis sets that are symmetric and complete. This includes the class of linear models and neural networks typically used in practice, as well as the family of all measurable functions. We say that a hypothesis set \(\) is _symmetric_ if it is independent of the ordering of labels. That is, for all \(y\), the scoring function \(x h(x,y)\) belongs to some real-valued family of functions \(\). We say that a hypothesis set is _complete_ if, for all \((x,y)\), the set of scores \(h(x,y)\) can span over the real numbers, that is, \(\{h(x,y) h\}=\). Note that any symmetric and complete hypothesis set is regular for top-\(k\) classification.

Next, we analyze the broad family of comp-sum losses, which includes the commonly used logistic loss (or cross-entropy loss used with the softmax activation) as a special case.

Comp-sum losses are defined as the composition of a function \(\) with the sum exponential losses, as in (Mao et al., 2023f). For any \(h\) and \((x,y)\), they are expressed as

\[_{}(h,x,y)=_{y^{}y}e^{h(x,y^ {})-h(x,y)},\]

where \(_{+}_{+}\) is non-decreasing. When \(\) is chosen as the function \(t(1+t)\), \(t t\), \(t 1-\) and \(t1-()^{q}\), \(q(0,1)\), \(_{}(h,x,y)\) coincides with the most commonly used (multinomial) logistic loss, defined as \(_{}(h,x,y)=\!(_{y^{}}e^{h(x,y^{})-h(x,y)})\)(Verhulst, 1838, 1845, Berkson, 1944, 1951), the sum-exponential loss \(_{}(h,x,y)=_{y^{} y}e^{h(x,y^{ })-h(x,y)}\)(Weston and Watkins, 1998, Awasthi et al., 2022b) which is widely used in multi-class boosting (Saberian and Vasconcelos, 2011, Mukherjee and Schapire, 2013, Kuznetsov et al., 2014), the mean absolute error loss \(_{}(h,x,y)=1-[_{y^{}} e^{h(x,y^{})-h(x,y)}]^{-1}\) known to be robust to label noise for training neural networks (Ghosh et al., 2017), and the generalized cross-entropy loss \(_{}(h,x,y)=1-[_{y^{ }}e^{h(x,y^{})-h(x,y)}]^{-q}\), \(q(0,1)\), a generalization of the logistic loss and mean absolute error loss for learning deep neural networks with noisy labels (Zhang and Sabuncu, 2018), respectively. We specifically study these loss functions and show that they benefit from \(\)-consistency bounds with respect to the top-\(k\) loss.

**Theorem 4.5**.: _Assume that \(\) is symmetric and complete. Then, for any \(1 k n\), the following \(\)-consistency bound holds for the comp-sum loss:_

\[_{_{k}}(h)-_{_{k}}^{*}()+_ {_{k}}() k^{-1}_{_{ }}(h)-_{_{}}^{*}( )+_{_{}}(),\]

_In the special case where \(_{_{}}()=0\), for any \(1 k n\), the following upper bound holds:_

\[_{_{k}}(h)-_{_{k}}^{*}() k^{ -1}_{_{}}(h)-_{ _{}}^{*}(),\]

_where \((t)=(1-t)+(1+t)\), \(t\) when \(_{}\) is \(_{}\); \((t)=1-}\), \(t\) when \(_{}\) is \(_{}\); \((t)=t/n\) when \(_{}\) is \(_{}\); and \((t)=}}+(1-t)^{}}{2}^{1-q}-1\), for all \(q(0,1)\), \(t\) when \(_{}\) is \(_{}\)._

The proof is included in Appendix B. The second part follows from the fact that when \(_{_{}}()=0\), the minimizability gap \(_{_{}}()\) vanishes. By taking the limit on both sides, Theorem 4.5 implies the \(\)-consistency and Bayes-consistency of comp-sum losses with respect to the top-\(k\) loss. It further shows that, when the estimation error of \(_{}\) is reduced to \(>0\), then the estimation error of \(_{k}\) is upper bounded by \(k^{-1}()\), which, for a sufficiently small \(\), is approximately \(k\) for \(_{}\) and \(_{}\); \(kn\) for \(_{}\); and \(k}\) for \(_{}\). Note that different from the other loss functions, the bound for the mean absolute error loss is only linear. The downside of this more favorable linear rate is the dependency on the number of classes and the fact that the mean absolute error loss is harder to optimize (Zhang and Sabuncu, 2018). The bound for the generalized cross-entropy loss depends on both the number of classes \(n\) and the parameter \(q\).

In the proof, we used the fact that the conditional regret of the top-\(k\) loss is the sum of \(k\) differences between two probabilities. We then upper bounded each difference with the conditional regret of the comp-sum loss, using a hypothesis based on the two probabilities. The final bound is derived by summing these differences. In Appendix G, we detail the technical challenges and the novelty.

The key quantities in our \(\)-consistency bounds are the minimizability gaps, which can be upper bounded by the approximation error, or more refined terms, depending on the magnitude of the parameter space, as discussed by Mao et al. (2023f). As pointed out by these authors, these quantities, along with the functional form, can help compare different comp-sum loss functions. In Appendix C, we further discuss the important role of minimizability gaps under the realizability assumption, and the connection with some negative results of Yang and Koyejo (2020).

Constrained losses are defined as a summation of a function \(\) applied to the scores, subject to a constraint, as shown in (Lee et al., 2004). For any \(h\) and \((x,y)\), they are expressed as

\[_{}(h,x,y)=_{y^{} y}(-h(x,y^{ })),_{y y}h(x,y)=0,\]

where \(_{+}\) is non-increasing. In Appendix E, we study this family of loss functions and show that several benefit from \(\)-consistency bounds with respect to the top-\(k\) loss. In Appendix H, we provide generalization bounds for the top-\(k\) loss in terms of finite samples (Theorems H.1 and H.2).

### Theoretical guarantees for cardinality-aware surrogate losses

The strong theoretical results of the previous sections establish the effectiveness of comp-sum and constrained losses as surrogate losses for the target top-\(k\) loss for common hypothesis sets used in practice. Building on this foundation, we expand our analysis to their cost-sensitive variants in the study of cardinality-aware set prediction in Section 2. We derive \(\)-consistency bounds for these loss functions, thereby also establishing their Bayes-consistency. To do so, we characterize the conditional regret of the target cardinality-aware loss function in Lemma I.1, which can be found in Appendix I. For this analysis, we will assume, without loss of generality, that the cost \(c(x,k,y)\) takes values in \(\) for any \((x,k,y)\), which can be achieved by normalizing the cost function.

We will use \(_{}\), \(_{c-}\), \(_{}\) and \(_{}\) to denote the corresponding cost-sensitive counterparts for \(_{}\), \(_{}\), \(_{}\) and \(_{}\), respectively. Next, we show that these cost-sensitive surrogate loss functions benefit from \(\)-consistency bounds with respect to the target loss \(\) given in (1).

**Theorem 4.6**.: _Assume that \(\) is symmetric and complete. Then, the following bound holds for the cost-sensitive comp-sum loss: for all \(r\) and for any distribution,_

\[_{}(r)-_{}^{*}()+_{}( )_{_{} }r)-_{_{}}^{*}()+ _{_{}}();\]

_When \(=_{}\), the following holds: \(_{}(r)-_{}^{*}(_{}) _{_{}}(r)-_{ _{}}^{*}(_{})\), where \((t)=2\) when \(_{}\) is either \(_{}\) or \(_{}\); \((t)=2|^{q}t}\) when \(_{}\) is \(_{}\); and \((t)=||t\) when \(_{}\) is \(_{}\)._

The proof is included in Appendix I.1. The second part follows from the fact that when \(=_{}\), all the minimizability gaps vanish. In particular, Theorem 4.6 implies the Bayes-consistency of cost-sensitive comp-sum losses. The bounds for cost-sensitive generalized cross-entropy and mean absolute error loss depend on the number of set predictors, making them less favorable when \(||\) is large. As pointed out earlier, while the cost-sensitive mean absolute error loss admits a linear rate, it is difficult to optimize even in the standard classification, as reported by Zhang and Sabuncu (2018).

In the proof, we represented the comp-sum loss as a function of the softmax and introduced a softmax-dependent function \(_{}\) to upper bound the conditional regret of the target cardinality-aware loss function by that of the cost-sensitive comp-sum loss. This technique is novel and differs from the approach used in the standard scenario (section 4.2).

We will use \(_{}^{}\), \(_{}\), \(_{}\) and \(_{}\) to denote the corresponding cost-sensitive counterparts for \(_{}^{}\), \(_{}\), \(_{}\) and \(_{}\), respectively. Next, we show that these cost-sensitive surrogate losses benefit from \(\)-consistency bounds with respect to the target loss \(\) given in (1).

**Theorem 4.7**.: _Assume that \(\) is symmetric and complete. Then, the following bound holds for the cost-sensitive constrained loss: for all \(r\) and for any distribution,_

\[_{}(r)-_{}^{*}()+_{}( )_{_{}} (r)-_{_{}}^{*}()+ _{_{}}();\]

_When \(=_{}\), the following holds: \(_{}(r)-_{}^{*}(_{}) _{_{}}(r)-_{ _{}}^{*}(_{})\), where \((t)=2\) when \(_{}\) is \(_{}^{}\) or \(_{}^{*}\); \((t)=t\) when \(_{}\) is \(_{}^{*}\) or \(_{}\)._

The proof is included in Appendix I.2. The second part follows from the fact that when \(=_{}\), all the minimizability gaps vanish. In particular, Theorem 4.7 implies the Bayes-consistency of cost-sensitive constrained losses. Note that while the constrained hinge loss and \(\)-margin loss have a more favorable linear rate in the bound, their optimization may be more challenging compared to other smooth loss functions.

## 5 Experiments

Here, we report empirical results for our cardinality-aware algorithm and show that it consistently outperforms top-\(k\) classifiers on benchmark datasets CIFAR-10, CIFAR-100 (Krizhevsky, 2009), SVHN (Netzer et al., 2011) and ImageNet (Deng et al., 2009).

We used the outputs of the second-to-last layer of ResNet (He et al., 2016) as features for the CIFAR-10, CIFAR-100 and SVHN datasets. For the ImageNet dataset, we used the CLIP (Radford et al., 2021) model to extract features. We adopted a linear model, trained using multinomial logistic loss, for the classifier \(h\) on the extracted features from the datasets. We used a two-hidden-layer feedforward neural network with ReLU activation functions (Nair and Hinton, 2010) for the cardinality selector \(r\). Both the classifier \(h\) and the cardinality selector \(r\) were trained using the Adam optimizer (Kingma and Ba, 2014), with a learning rate of \(1 10^{-3}\), a batch size of \(128\), and a weight decay of \(1 10^{-5}\).

Figure 1 compares the accuracy versus cardinality curves of the cardinality-aware algorithm with that of top-\(k\) classifiers induced by \(h\) for the various datasets. The accuracy of a top-\(k\) classifier is measured by \(_{(x,y) S}[1-_{k}(h,x,y)]\), that is the fraction of the sample in which the top-\(k\) predictions include the true label. It naturally grows as the cardinality \(k\) increases, as shown in Figure 1. The accuracy of the cardinality-aware algorithms is measured by \(_{(x,y) S}1_{y\{h_{1}(x),,h_{(x)}(x)\}}\), that is the fraction of the sample in which the predictions selected by the model \(r\) include the true label, and the corresponding cardinality is measured by \(_{(x,y) S}[r(x)]\), that is the average size of the selected predictions. The cardinality selector \(r\) was trained by minimizing the cost-sensitive logistic loss \(_{c-}\) with the cost \(c(x,k,y)\) defined as \(_{k}(h,x,y)+(k)\) and normalized to \(\) through division by its maximum value over \(\). We allow for top-\(k\) experts with \(k=\{1,2,4,8\}\) and vary \(\). Starting from high values of \(\), as \(\) decreases in Figure 1, our cardinality-aware algorithm yields solutions with higher average cardinality and increased accuracy. This is because \(\) controls the trade-off between cardinality and accuracy. The plots end to the right at \(=0.01\).

Figure 1 shows that the cardinality-aware algorithm is superior across the CIFAR-100, ImageNet, CIFAR-10 and SVHN datasets. For a given average cardinality, the cardinality-aware algorithm always achieves higher accuracy than a top-\(k\) classifier. In other words, to achieve the same level of accuracy, the predictions made by the cardinality-aware algorithm can be significantly smaller in size compared to those made by the corresponding top-\(k\) classifier. In particular, on the CIFAR-100, CIFAR-10 and SVHN datasets, the cardinality-aware algorithm achieves the same accuracy (98%) as the top-\(k\) classifier while using roughly only half of the cardinality on average. As with the ImageNet dataset, it achieves the same accuracy (95%) as the top-\(k\) classifier with only two-thirds of the cardinality. This illustrates the effectiveness of our cardinality-aware algorithm.

Figure 2 presents the comparison of \((|_{k}(x)|)\!=\!k\) and \((|_{k}(x)|)\!=\! k\) in the same setting (for each dataset, the orange curve in Figure 2 coincides with the orange curve in Figure 1). The

Figure 1: Accuracy versus average cardinality plots obtained by varying \(\) for our cardinality-aware algorithm and top-\(k\) classifiers across four datasets, with predictor set \(=\{1,2,4,8\}\) and cardinality cost \((k)= k\). Our cardinality-aware algorithm consistently achieves higher accuracy for any fixed average cardinality across all datasets.

Figure 2: Comparison of cardinality costs \((k)= k\) and \((k)=k\), with predictor set \(=\{1,2,4,8\}\). The accuracy versus average cardinality plots for our cardinality-aware algorithm are similar, suggesting that the choice of cardinality cost has minimal impact on performance.

comparison suggests that the choice between the linear and logarithmic cardinality costs has negligible impact on our algorithm's performance, highlighting its robustness in this regard. We also empirically demonstrate that our algorithm dynamically adjusts the cardinality of prediction sets based on input complexity, selecting larger sets for more challenging inputs to ensure high accuracy and smaller sets for simpler inputs to keep the cardinality low, as illustrated in Figure 3 and Figure 4. We present additional experimental results with different choices of set \(\) in Figure 5 and Figure 6 in Appendix J. Our cardinality-aware algorithm consistently outperforms top-\(k\) classifiers across all configurations.

## 6 Conclusion

We introduced a new cardinality-aware set prediction framework for which we proposed two families of surrogate losses with strong \(\)-consistency guarantees: cost-sensitive comp-sum and constrained losses. This leads to principled and practical cardinality-aware algorithms for top-\(k\) classification, which we showed empirically to be very effective. Additionally, we established a theoretical foundation for top-\(k\) classification with fixed cardinality \(k\) by proving that several common surrogate loss functions, including comp-sum losses and constrained losses in standard classification, admit \(\)-consistency bounds with respect to the top-\(k\) loss. This provides a theoretical justification for the use of these loss functions in top-\(k\) classification and opens new avenues for further research in this area.

Figure 4: Illustration of hard and easy images on the CIFAR-10 dataset as judged by human annotators, for top-\(k\) experts \(=\{1,2,4,8\}\). _Hard images_ are those correctly predicted by our algorithm with a cardinality of \(8\) but misclassified with a cardinality of \(4\). _Easy images_ are correctly predicted with a cardinality of \(1\).

Figure 3: Cardinality distribution for top-\(k\) experts with \(=\{1,2,4,8\}\) on CIFAR-10 and CIFAR-100 datasets, analyzed under two \(\) values. For each dataset, increasing \(\) reduces the number of samples with the highest cardinality (\(k=8\)) and increases those with lower cardinalities, as higher \(\) amplifies the influence of cardinality in the cost function. Across datasets, distributions vary for the same \(\) due to differing task complexities.