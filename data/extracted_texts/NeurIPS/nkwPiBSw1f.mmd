# Dual-Personalizing Adapter for

**Federated Foundation Models**

**Yiyuan Yang**

Australian AI Institute,

Faculty of Engineering & IT

University of Technology Sydney

Yiyuan.Yang-1@student.uts.edu.au

**Guodong Long**

Australian AI Institute,

Faculty of Engineering & IT

University of Technology Sydney

Guodong.Long@uts.edu.au

**Tao Shen**

Australian AI Institute,

Faculty of Engineering & IT

University of Technology Sydney

Tao.Sheng@uts.edu.au

**Jing Jiang**

Australian AI Institute,

Faculty of Engineering & IT

University of Technology Sydney

Jing.Jiang@uts.edu.au

**Michael Blumenstein**

Australian AI Institute,

Faculty of Engineering & IT

University of Technology Sydney

Michael.Blumenstein@uts.edu.au

**Abstract**

Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning diverse instruction data. Notably, federated foundation models (FedFM) emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to FedFM for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications, and conventional methods for test-time distribution shifts in personalized FL are less effective for FedFM due to their failure to adapt to complex distribution shift scenarios and the requirement to train all parameters. To bridge this gap, we refine the setting in FedFM, termed test-time personalization, which aims to learn personalized federated foundation models on clients while effectively handling test-time distribution shifts simultaneously. To address challenges in this setting, we explore a simple yet effective solution, a **Fed**erated **D**ual-**P**ersonalizing **A**dapter (FedDPA) architecture. By co-working with a foundation model, a global adapter and a local adapter jointly tackle the test-time distribution shifts and client-specific personalization. Additionally, we introduce an instance-wise dynamic weighting mechanism that dynamically integrates the global and local adapters for each test instance during inference, facilitating effective test-time personalization. The effectiveness of the proposed method has been evaluated on benchmark datasets across different NLP tasks with released code.

Introduction

Foundation models, especially the large language model (LLM) in natural language processing (NLP), have nearly exhausted public data sources for training. This necessitates alternative solutions to further improve these foundation models by leveraging private or protected data sources, such as business data in companies, smartphones, and so on. **Federated foundation models (FedFM)** offer a promising solution by integrating federated learning (FL) frameworks to enhance the foundation models in a decentralized manner. Built upon existing Parameter-efficient fine-tuning (PEFT) methods , FedFM is a collaboratively fine-tuning framework that leverages private datasets with privacy preservation and avoiding overfitting from client-specific fine-tuning.

Test-time distribution shift in FL  presents a significant challenge in practical scenarios, as clients may encounter unseen learning tasks during the testing and model inference phases. For example, a client accustomed to writing emails in English may require translation assistance when working on a new project in Chinese. Therefore, it is imperative for the deployed machine learning model to be capable of tackling the test-time distribution shifts from the training data, and our paper addresses this critical issue of test-time distribution shifts within the FedFM scenario. Previous works in test-time FL predominantly utilize conventional deep learning models that are trained from scratch in federated settings. Recent FedFM methods mainly focus on addressing specific challenges related to data heterogeneity  and communication overheads . However, none of these methods have discussed test-time distribution shifts in FedFM scenarios.

To fill this gap, we propose a novel FedFM framework that is robust to client-specific alignment and test-time distribution shifts simultaneously. With the support of a foundation model with PEFT methods, we first refine the federated setting, termed _test-time personalization_, which follows: 1) each client needs to train a personalized model using its own data from a target task, and 2) during testing, each client's personalized model needs to be robust to tackle the receiving new tasks (unseen in training) with different distributions (test-time distribution shift). Essentially, the proposed test-time personalization in FL could be simply viewed as an optimization task to seek a sweet trade-off between client-specific model personalization and model generalization to test data.

For test-time personalization in FedFM, two primary challenges--test-time distribution shifts and personalization--necessiate learning tailored to distinct objectives, and the training cost of foundation models also represents a significant concern. To address these issues, we explore a simple yet effective method, dubbed **Fed**erated **Dual-P**ersonalizing **A**dapter (FedDPA), where each client learns a global adapter to learn generic knowledge from the aggregation for test-time tasks and maintains a local adapter for targeted ability personalization. During the inference phase, the local and global adapters are dynamically integrated to facilitate prediction, where an instance-wise dynamic weighting mechanism is proposed to autonomously adjudicate the proportional contribution of the local and global adapters for each test instance. Experimental results demonstrate that our method achieves state-of-the-art performance on benchmarks and all data and code are released 1. Our main contributions are summarized as follows:

* We are the first to explore the test-time distribution shifts problem in federated foundation models for practical application scenarios alignment.
* We introduce a new method, namely dual-personalizing adapter, to emphasize learning both generic and personalized knowledge in the context of FedFM with test-time personalization.
* We conduct an exhaustive analysis using heterogeneous FL benchmarks across diverse NLP tasks. The empirical outcomes reveal that our method attains state-of-the-art performance, underscoring its superior test-time personalization capabilities than existing methods.

## 2 Related Work

### **A** adapter-based PEFT Methods

Given the substantial computational and storage burdens associated with directly fine-tuning foundation models, the community has shifted towards embracing parameter-efficient methods , with the adapter family  being a notable exemplar. According to different architectures, methods in the adapter family can be categorized into four types. The first one is prompt-based learning [18; 21], which is aimed at learning the continuous/soft prompt for discrete optimization. The second one is reparametrization-based methods [13; 9], achieving parameter efficiency by utilizing low-rank techniques to decompose the high-dimensional matrices. The third one is series Adapters , which introduce additional learnable modules in a sequential manner within specific sublayers. The last one is parallel Adapters , which focus on learning additional learnable modules in a parallel way with distinct sublayers. In this context, our exploration delves into the adapter-based PEFT methods of federated foundation models.

### Federated Foundation Models

With the advent of foundation models, there has been a burgeoning interest [44; 41; 26; 5] in integrating these models within the FL setting. Particularly, in light of the inherent computation and communication cost, recent work [17; 43; 6] endeavors have delved deeper into integrating adapter-based parameter-efficient tuning (PEFT) methods with federated foundation models. Building upon this, a multitude of studies have emerged to navigate the challenges of incorporating federated foundation models with adapter-based PEFT methods. The paper  stands at the forefront, initiating the integration of instruction tuning within federated LLM frameworks. Addressing data-related issues, the paper  introduced a data-driven initialization approach to mitigate the primary challenges associated with LoRA in highly heterogeneous data scenarios. In addition, the research presented in  proposed a method to annotate unlabeled client-side data by harnessing the prowess of large models to address data scarcity concerns. To further optimize the communication and computational overheads associated with federated foundation models, the works [35; 27; 34] emphasize advancing gradient-free optimization methods suitable for devices with limited memory and computing power. For personalization, paper  focused on designing a specific training paradigm for LoRA to achieve more effective personalization in visual model-heterogeneous scenarios. Diverging from these approaches, our work delves into the realm of personalization with adapters in federated foundation models, extending the scope of research in this area.

### Personalized Federated Learning

To address the necessity of personalization for individual clients, personalized Federated Learning (PFL) , which aims at training to cater to individual client preferences and needs, is proposed. Broadly, existing PFL methods can be categorized into two primary types: fine-tuning the global model for personalization or learning additional personalized models. Research works [10; 7] in the first category fine-tuned the whole or part of the global model with each client's local dataset for personalization. While research works [19; 22] in the second category is to learn the additional personalized layers or model through local aggregation. Nonetheless, a prevalent limitation among these PFL approaches is their concentrated focus on a specifically targeted task, often at the expense of performance when encountering test-time distribution shifts.

To fill this gap, recent research has shifted focus towards exploring different test-time distribution shifts in PFL. In contrast to studies [39; 8] in federated continual/incremental learning possessing ample annotated data from different distributions for training to address shifts, test-time FL focuses on handling distribution shifts during testing without the availability of annotated data for further training. One strand of research [3; 32] concentrates on addressing test-time distribution shifts that occur when new clients are introduced during the testing phase by module/prior adaptation. Another line of studies[29; 16] aims to tackle distribution shifts in existing clients during testing by aligning test features with existing features. Our paper falls into the second type and differs from previous work by exploring this challenge within the framework of foundation models, which are characterized by extensive parameter scales and more complex test-time distribution shifts.

## 3 Problem Definition

### Test-time Personalization in FedFM

Considering \(M\) clients in an FL system, each client possesses its distinct local training dataset \(_{train}^{m}\) and test dataset \(_{test}^{m}\), where \(m\) indexes a client. One data pair in datasets is denoted as \((,)\), where \(\) is the input data and \(\) is its corresponding label.

In each client \(m\), we will introduce the training and testing phases separately for our test-time personalization setting. In the training phase, model training utilizes solely the local dataset \(_{train}^{m}\), which is derived from the distribution \(P_{s}^{m}\). While in the testing phase, the test dataset \(_{test}^{m}\) comprises two components: the test set \(_{s}^{m}\) driven from the same distribution \(P_{s}^{m}\) as training data, and additional test sets \(_{t}^{m}\) under data distribution shifts \(P_{s}^{m}(,) P_{t}^{m}(,)\). Therefore, the test dataset is \(_{test}^{m}=_{s}^{m}_{t}^{m}\), and we call these datasets \(_{t}^{m}\) as test-time datasets. Unlike previous works[29; 16] in test-time FL concentrating on either feature-level shifts \(P_{s}^{m}() P_{t}^{m}()\) or label shifts \(P_{s}^{m}() P_{t}^{m}()\), we investigate a more complex scenario where various distribution shifts, including semantic shifts, domain shifts and others, exist simultaneously. This is aligned with the practical application of foundation models, which often encounter testing data originating from diverse domains, backgrounds, or populations.

Therefore, the objective of the model in each client should not only perform well on the test set \(_{s}^{m}\) (refer to personalization) but also have comparable results on the test-time dataset \(_{t}^{m}\) (refer to test-time performance). This objective is consistent with the practical scenarios, since users primarily focus on the abilities they often utilize (abundant data available for training) and occasionally also introduce new tasks (limited to test data).

### Challenges

The test-time personalization setting raises two pivotal considerations: _personalization_ and _test-time distribution shifts_. Personalization is the primary focus, followed by optimizing test-time tasks. Our proposed method introduced in section 4 is designed to achieve personalization within FedFM while ensuring comparable results for test-time tasks, and its vital intuition is illustrated below.

Considering a foundation model, it comprises a main body \(f()\), which holds most of the parameters and processes input \(\) to produce output features \(=f(;)\). Additionally, there is a tail \(g(_{t})\) that maps these features to the output space (e.g., vocabulary), resulting in the predicted result \(}=g(;_{t})\). Typically, the focus in tuning and adaptations primarily lies on the main body \(f()\) because the tail \(g(_{t})\), usually a linear function, remains unchanged (frozen) during tuning .

Discordance between Personalization and Test-time Tasks.The key to addressing test-time distribution shifts lies in learning generic features universally applicable across disparate distributions . That is, learning a foundation model \(f()\) to satisfy \(P_{s}(f(;),)=P_{t}(f(;),)\) although

Figure 1: The overall framework of FedDPA. Each client contains a frozen LLM, a trainable global adapter (LoRA) and a trainable local adapter (LoRA) with a specific task, where the global adapter (LoRA) is for test-time tasks and the local adapter (LoRA) is for personalization. During the training, only the parameters of the global adapter (LoRA) are transmitted to the server for aggregation.

\(P_{s}(x,) P_{t}(,)\). FL is a methodology designed to learn generic features across diverse non-IID data (different distributions) through aggregation algorithms [5; 29]. Therefore, we tailor FL training for addressing test-time tasks with the objective \(_{}_{P_{all}}()\), where \(_{P_{all}}\) represents the loss function designed for learning generic features towards all clients' distributions \(P_{all}\). However, personalization focuses on aligning the model with the specific distribution \(P_{s}\), which means learning a foundation model \(f()\) with the objective \(_{}_{P_{s}}()\), where \(_{P_{s}}\) represents the loss function designed for learning personalized features towards the specific distribution. Therefore, the discordance between specific distribution alignment for personalization and generic feature learning for test-time tasks leads to inconsistent optimization objectives.

The above analysis motivates a dual model strategy--one model for test-time tasks and one model for personalization--to realize test-time personalization in FedFM. This strategy, together with our other techniques presented below for FedFM scenarios, constitutes the foundation of our method.

## 4 Proposed Method

To align with the application scenarios, we consider the test-time personalization setting in FedFM. Following a similar assumption from a Mixture of Experts , _any test-time task (distributions) to a client can be approximated as a mixture of training tasks seen by other clients in the federated learning system_. Therefore, each client primarily personalizes its model based on its local training task, while also tackling unseen test-time tasks by leveraging insights gained from other clients in the federated learning system. Discussions of other scenarios can be found in Appendix C.

In test-time personalization, test-time distribution shifts and personalization are two main issues that need to be addressed, and their optimization objectives toward different distributions are inconsistent. To address these challenges and consider the efficient learning of FedFM, we propose a **Fed**erated **D**ual-**P**ersonalizing **A**dapter (FedDPA) system for each client, as shown in Fig 1. During training, a global adapter is employed to acquire generic features by FL training for test-time tasks (Sec. 4.1). Meanwhile, to address personalization, a local adapter is maintained locally to align with the client's specific distribution, and leverages generic knowledge from the global adapter for faster learning (Sec. 4.2). During the inference, the learned global and local adapters are dynamically combined using a weight generated by the instance-wise dynamic weighting mechanism for each input test instance, realizing test-time personalization (Sec. 4.3).

The Overall Objective.Considering the computation and communication cost of FedFM, we utilize the adapter-based PEFT methods, which only learn a small part of parameters \(\) while keeping most of the parameters \(\) frozen. Our proposed FedDPA is to learn the global adapter \(_{g}\) and local adapters \(_{l}^{m}\)_simultaneously_ across \(M\) client to realize test-time personalization,

\[_{_{g},\{_{l} ^{m}\}}&_{m=1}^{M}[_{(,) P_{s}^{m }}(;_{g};_{l}^{m})]\\ s.t.&_{g}^{*}*{arg \,min}_{_{g}}_{(,) P_{all}}(;_{g};_{l}^{m})\] (1)

where the first part is a standard personalized FL loss to find optimal personalized models by minimizing the sum of loss on local training tasks \(_{(,) P_{s}^{m}}(.)\), and the second part is a constraint term to seek an optimal solution by minimizing the test-time loss \(_{(,) P_{all}(.)}\). Because we assume that the test-time task is unseen to a client but observed by other clients, the test-time loss can be estimated using the Empirical Risk Minimization of all client's training tasks \(_{_{g}}_{m=1}^{M}r_{m}_{m}(; _{g})\), where \(P_{all}\) denotes all distributions of tasks in all clients, \(r_{m}\) denotes each client's weight for aggregation (e.g., in FedAvg, \(r_{m}\) is the proportion of each client's data number to all clients' data number) and \(_{m}\) denotes the loss for each client over its local training dataset. Since the above objective cannot be solved directly, _we propose to alternatively learn the global and local adapters in a sequential manner (FedDPA-F with local adapter fine-tuning) or iterative manner (FedDPA-T with local adapter training)_. Detailed algorithms of these two methods are in Appendix A.3.

Remark.To simplify the illustration, we use LLM as the backbone and adopt LoRA  as the adapter-based PEFT method in our framework. The overall framework is easy to adapt to other types of backbone and other adapter-based PEFT methods. LoRA decomposes the training weight into a frozen weight \(\), and a trainable weight derived by the multiplication of two low-rank weights \(=^{b}^{a}\). The data heterogeneity in FL with LLM primarily manifests as distribution shifts across various NLP tasks among different clients, driven by diverse backgrounds, topics, and other contextual factors, and local loss \(_{m}\) for all NLP tasks is a standard language modeling objective .

### Generic Learning of Global Model

Addressing test-time distribution shifts requires the acquisition of generic knowledge that is applicable across various distributions . The conventional federated learning process is inherently designed to aggregate this generic knowledge among different non-IID data. Consequently, we utilize the adapter trained within the FL context as the global adapter for addressing test-time tasks. To further enhance generic learning, our model aggregation strategy is based on the client number rather than the number of data by considering the potential biases stemming from different numbers of tasks.

At each client, there consists of a frozen LLM model \(f(;)\) with a global lightweight global adapter (LoRA) \(_{g}=_{g}^{b}_{g}^{a}\). This global adapter is used for aggregation by sending to the server. Notably, the server's role is limited to computing the aggregated adapter \(_{g}\), thus obviating the need for maintaining a large-scale model. Similar to the standard FL process, for each client \(m\), the adapter weight \(_{g}^{m}\) is learned locally and sent to the server. Upon receipt of the adapter weights from all clients, the server employs FedAvg  to aggregate them and sends \(}_{g}\) back to each client as their initialized parameter in a new round. It can be formulated as:

\[}_{g}=_{m=1}^{M} _{g}^{m},_{g}^{m}=*{arg\,min}_{_{g}} _{m}(;_{g}),}_{g}\] (2)

Remark.Other federated algorithms like FedProx  can also be applied with LoRA tuning of this global model learning (in Appendix B.1). In this paper, we just take FedAvg as an example.

### Personalization of Local Model

The previously developed global model, which focuses on acquiring generic features across diverse datasets, faces challenges with personalization due to inconsistent optimization objectives. To address this, we integrate a local adapter to better align with each client's specific distribution. We explore two methods as shown in Fig 2, 1) Learning sequentially: after global adapter training, the local adapter is initialized by the learned global adapter and directly fine-tuned; 2) Learning iteratively: during each communication round of global adapter training, the local adapter is re-initialized from its last state, fine-tuned alongside the frozen global adapter, and maintained locally without communication.

To be more specific, a local adapter (LoRA) \(_{i}=_{i}^{b}_{i}^{a}\) is introduced. Thus, each client contains three components: a frozen LLM \(\), a global adapter (LoRA) \(_{g}\) and a local adapter (LoRA) \(_{i}^{m}\). As delineated in Fig 2 (a), for the first method, after global training, the local adapter is first initialized by the global adapter denoted as \(_{i}^{m}=_{g}\), then fine-tuned on local data to get the final local adapter. As shown in Fig 2 (b), for the second method, during each communication round in training for each adapter layer, upon receiving an input \(\), it simultaneously traverses the frozen LLM, the frozen global adapter and the local adapter. The process entails an initial fusion of the outputs from

Figure 2: Frameworks of two personalized methods for local adapter (LoRA) are shown on the left, with their overall learning processes on the right.

both the local and global adapters with a predefined weighting factor of \(\), followed by integration with the output of the LLM to yield the final result \(^{{}^{}}=+((1-)_{g} {h}+_{l}^{m})\). Therefore, the learning of the local adapter \(_{l}^{m}\) for these two methods can be unified as:

\[_{l}^{m}=_{_{l}^{m}}_{m} (;_{g};_{l}^{m}),_{g}_{l}^{m}\] (3)

### LLM-enhanced Instance-wise Dynamic Weighting Mechanism

As discussed in previous test-time FL methods , a dynamic combination of global components and personalized components can improve generalization while reducing the cost of hyper-parameter tuning in the deployment stage. Considering the disparate data distributions that characterize test-time tasks and local tasks and the wealth of training instances of local tasks available to each client, we propose an _instance-wise dynamic weighting mechanism_ to calculate the similarity between the input instance and local instances, using this metric to determine the appropriate weight balance for the global and local adapter combination. To facilitate this, the representation of each input instance is essential. Leveraging the robust capability of pre-trained LLMs to abstract input sentences, we utilize the hidden states from the final layer of the LLM as the representation. Given that the LLM is decoder-based, with tokens attending only to preceding tokens, the embedding of the final token is considered representative of the entire input for similarity evaluation. Furthermore, to enhance the representation quality, the global adapter, which embodies generic knowledge, is incorporated into this embedding process.

More specifically, during the inference stage, for each input instance \(\) in a client, we randomly sample \(S\) instances \(\{_{0},_{1},...,_{s}\}\) from the local training dataset. These instances are then fed into the LLM, augmented with the global adapter, to obtain the last token's embeddings from the final layer, denoted as \(_{x}\) and \(\{_{x_{0}},_{x_{1}},...,_{x_{s}}\}\) respectively. Subsequently, we calculate the similarity between the input representation \(_{x}\) and each sampled local representation in \(\{_{x_{0}},_{x_{1}},...,_{x_{s}}\}\), resulting in a score range of \(\). Finally, we average all scores to obtain the final result, represented as \(_{t}=_{i=0}^{S}(_{x},_{x_{i}})\), where \(\) represents the function to calculate the similarity, and \(\) is a scale factor in \((0,1]\) to restrict the maximum similarity score (especially for FedDPA-T).

Through this method, the balancing of weights between the global and local adapters is dynamically adjusted for each test instance, ensuring the model not only tailors to the individual client's specific needs but also benefits from the aggregated model's generic knowledge across test-time tasks.

## 5 Experiment

### Experiment Setting

Datasets.We construct two federated datasets from Flan , which is a collection of various NLP tasks from over 60 datasets for instruction turning. In order to be better suitable for FL settings, we randomly select 8 NLP tasks from different datasets for each federated dataset and downsample the original datasets with more details in Appendix A.1. ROGUE-1 is taken as a metric.

Baselines and Implementation.We compare our methods with four baselines based on the same model architecture: centralized model, Local-finetuned model, FedIT  and FedLoRA . The centralized model is trained on all data of tasks in one center. The local-finetuned model infers that only local data are used to train the model without any communication with other clients or the server.

We distribute data between clients based on the NLP task for data heterogeneity, where different NLP tasks generated from different contextual factors inherently suffer from various complex distribution shifts. Since we select 8 NLP tasks, corresponding to \(M=8\) clients.For each client, the local task serves as the primary focus for personalization, while the tasks from other clients are taken as test-time tasks. To better evaluate the effectiveness of methods, we assume that all clients are activated for every communication round and set the communication round \(K=20\). The alpaca-LoRA2 is adapted as the base model initialized with LLaMA-7B.3 The updating weight of local LoRA training (FedDPA-T) is \(=0.5\) (\(=0.5\)) for federated dataset 1 and \(=0.3\) (\(=0.3\)) for federated dataset2. We set \(S=5\) and choose cosine similarity for instance-wise dynamic weighting mechanism. More details are in Appendix A.2.

### Main Results

We compare FedDPA with other baselines on two main evaluation facets: _personalization_ (scores on targeted local tasks) and _test-time personalization_ (average scores on all tasks including test-time tasks). As evidenced in Table 1 and Table 2, our proposed dual-personalizing adapter methods (both fine-tuning and training) exhibit superior performance in personalization compared to other baseline models, which demonstrates the effectiveness of local adapter maintenance for enhancing performance on the targeted local task. For test-time personalization, the FedDPA-F method stands out as the most effective among all personalized models, which suggests that incorporating learning from the global adapter can be instrumental in adapting to test-time distribution shifts for a more comprehensive model achievement. Additionally, given that the global adapter aggregated on different distributions matin certain generalization capabilities, the local adapter of FedDPA-F has better generalization performance than that of FedDPA-T, which leads to better performance on most test-time tasks. More importantly, it is noteworthy that while centralized or global models may yield higher average

    &  \\  & Para &  &  &  &  &  &  &  &  &  \\  & -phrase & -semet &  &  &  &  &  &  &  &  & \\  _Personalization_ & & & & & & & & & & \\ Centralized & 87.00 & 64.67 & 77.00 & 90.65 & 29.12 & 76.00 & 72.50 & 76.17 & 71.64 \\ FedIT & 86.00 & 63.13 & 79.00 & 89.80 & 30.36 & 75.50 & 72.00 & 81.06 & 72.07 \\ FedLoRA & 87.00 & 64.12 & **84.50** & 89.52 & 27.13 & 76.50 & 73.50 & 79.62 & 72.74 \\ Local-finetuned & 75.00 & 53.51 & 81.00 & 91.28 & 27.51 & 69.00 & 72.50 & 79.31 & 68.64 \\  FedDPA-F & 88.00 & 64.80 & 84.25 & 89.82 & 29.58 & 78.50 & 72.00 & 80.89 & 73.48 \\ FedDPA-T & **90.50** & **70.54** & 82.00 & **91.81** & **30.75** & **81.00** & **75.00** & **91.07** & **75.33** \\  _Test-Time Personalization_ & & & & & & & & & \\ Local-finetuned & 48.21 & 49.07 & 49.75 & 21.86 & 17.35 & 48.57 & 44.04 & 48.19 & 40.88 \\ FedLoRA & 69.60 & 71.64 & 71.09 & 71.28 & 65.63 & 68.89 & 70.32 & 70.44 & 69.86 \\  FedDPA-F & **71.64** & 72.28 & **72.42** & 72.39 & **71.12** & **70.46** & **71.00** & **71.82** & **71.64** \\ FedDPA-T & 71.63 & **72.66** & 71.20 & **72.58** & 70.58 & 69.21 & 70.67 & 71.62 & 71.27 \\   

Table 2: Personalization and test-time personalization results of different models on federated dataset 2. FedDPA-F represents the model with the local fine-tuning adapter and FedDPA-T represents the model with the local training adapter. Reading Com represents the reading comprehension task.

    &  \\  & Para &  &  &  &  &  &  &  &  &  \\  & -phrase &  &  &  &  &  &  &  &  &  & \\  _Personalization_ & & & & & & & & & \\ Centralized & 87.00 & 64.67 & 77.00 & 90.65 & 29.12 & 76.00 & 72.50 & 76.17 & 71.64 \\ FedIT & 86.00 & 63.13 & 79.00 & 89.80 & 30.36 & 75.50 & 72.00 & 81.06 & 72.07 \\ FedLoRA & 87.00 & 64.12 & **84.50** & 89.52 & 27.13 & 76.50 & 73.50 & 79.62 & 72.74 \\ Local-finetuned & 75.00 & 53.51 & 81.00 & 91.28 & 27.51 & 69.00 & 72.50 & 79.31 & 68.64 \\  FedDPA-F & 88.00 & 64.80 & 84.25 & 89.82 & 29.58 & 78.50 & 72.00 & 80.89 & 73.48 \\ FedDPA-T & **90.50** & **70.54** & 82.00 & **91.81** & **30.75** & **81.00** & **75.00** & **91.07** & **75.33** \\  _Test-Time Personalization_ & & & & & & & & \\ Local-finetuned & 48.21 & 49.07 & 49.75 & 21.86 & 17.35 & 48.57 & 44.04 & 48.19 & 40.88 \\ FedLoRA & 69.60 & 71.64 & 71.09 & 71.28 & 65.63 & 68.89 & 70.32 & 70.44 & 69.86 \\  FedDPA-F & **71.64** & 72.28 & **72.42** & 72.39 & **71.12** & **70.46** & **71.00** & **71.82** & **71.64** \\ FedDPA-T & 71.63 & **72.66** & 71.20 & **72.58** & 70.58 & 69.21 & 70.67 & 71.62 & 71.27 \\   

Table 1: Personalization and test-time personalization results of different models on federated dataset 1. FedDPA-F represents the model with the local fine-tuning adapter and FedDPA-T represents the model with the local training adapter. Linguistic represents the linguistic acceptability task, Word Dis represents the word disambiguation task, and Question CLS represents question classification task.

performances across all tasks, they fall short in excelling at specific tasks for personalization, aligning with the conclusions of the previous study .

## 6 Analysis

### Convergence Analysis

We present the convergence analysis of our methods in Figure 4. Figure 4 (a) compares our methods with other baselines for personalization, with the results showcasing the average performance on target local tasks across all clients. Notably, our methods exhibit a more rapid convergence compared to FedIT and achieve notable performance enhancements after five communication rounds. Despite sharing similar trends with FedLoRA, our approaches, particularly the FedDPA-T, ultimately outperform in personalization. For a more granular insight into test-time personalization convergence, Figure 4 (b) compares average performance on all tasks, including each client's targeted local and test-time tasks. The results substantiate that our approaches demonstrate faster convergence rates, further bolstering the efficacy of our methods.

### Ablation Study

Impact of Instance-Wise Dynamic Weighting Mechanism.To explore the impact of the instance-wise dynamic weighting mechanism, we implemented experiments with FedDPA methods on different datasets. As shown in Table 4, the incorporation of an instance-wise dynamic weighting mechanism contributes significantly to enhancing performance in both personalization and test-time personalization scenarios. More ablation studies are in Appendix B.2.

Impact of Updating Weight \(\).In this study, we investigated the influence of the updating weight \(\) during FedDPA-T training with its value \(\{03,0.5,0.7\}\). As can be seen in Table 4, for test-time personalization, increasing updating weight \(\) will decrease the performance due to the increased proportion of the local adapter in the model, while for personalization, different updating weights \(\) are required for different datasets to achieve their optimal results.

Impact of Client Number.To better align with the FL setting in practical application, we scaled up clients to 40 and implemented experiments with sample rate \(\{0.2,0.4,0.6,0.8,1\}\). For each communication round, the server will select clients from each task based on the sample rate (more details in Appendix A.1). As shown in Figure 4, as the client participant rates increase, model accuracy also increases as more participating clients provide more data for knowledge learning. Besides,

    &  &  &  \\  & & P & TTP & P & TTP \\   & 0.3 & 79.69 & **75.85** & **75.33** & **71.27** \\  & 0.5 & **80.22** & 75.47 & 74.10 & 70.72 \\  & 0.7 & 79.88 & 75.01 & 74.04 & 69.95 \\   

Table 4: Ablation study of updating weight. P represents personalization, and TTP represents test-time personalization.

FedDPA-F outperforms all baselines, whereas FedDPA-T exhibits somewhat inferior performance, potentially due to overfitting issues when handling a small dataset.

More experiments and analyses of scalability and efficiency can be found in Appendix B.3 and B.4.

## 7 Conclusion

Federated Foundation Model (FedFM) is a promising direction to enhance existing Foundation Models, e.g. LLM, by leveraging private data sources. Test-time distribution shift is a critically important problem to ensure the practicability of the FedFM system. This work is the first to propose the test-time FedFM setting. To tackle this challenging scenario, we propose a novel dual-personalizing adapter for the FedFM framework. The method is evaluated on public NLP tasks that are adapted to mimic the test-time FedFM setting. This work is the first step towards this direction. We focus on defining a new learning scenario, proposing a basic learning framework, and setting up the benchmark datasets. Our future works will be in two directions: the first is to rethink this problem from a theoretical perspective, and the second is to enhance the benchmark setting with more datasets in real applications.