# PROSPECT PTMs: Rich Labeled Tandem Mass Spectrometry Dataset of Modified Peptides for Machine Learning in Proteomics

**Wassim Gabriel \({}^{1}\)***  Omar Shouman \({}^{1}\)*  Ayla Schroeder \({}^{1}\)  Florian Boessl \({}^{1}\)**

**Mathias Wilhelm \({}^{1,2}\)**

{firstname.lastname}@tum.de

\({}^{1}\) Computational Mass Spectrometry \({}^{2}\) Munich Data Science Institute (MDSI)

School of Life Sciences

Technical University of Munich

Garching, Germany

**Abstract**

_Post-Translational Modifications_ (PTMs) are changes that occur in proteins after synthesis, influencing their structure, function, and cellular behavior. PTMs are essential in cell biology; they regulate protein function and stability, are involved in various cellular processes, and are linked to numerous diseases. A particularly interesting class of PTMs are chemical modifications such as phosphorylation introduced on amino acid side chains because they can drastically alter the physicochemical properties of the peptides once they are present. One or more PTMs can be attached to each amino acid of the peptide sequence. The most commonly applied technique to detect PTMs on proteins is bottom-up Mass Spectrometry-based proteomics (MS), where proteins are digested into peptides and subsequently analyzed using Tandem Mass Spectrometry (MS/MS). While an increasing number of machine learning models are published focusing on MS/MS-related property prediction of unmodified peptides, high-quality reference data for modified peptides is missing, impeding model development for this important class of peptides. To enable researchers to train machine learning models that can accurately predict the properties of modified peptides, we introduce four high-quality labeled datasets for applying machine and deep learning to tasks in MS-based proteomics. The four datasets comprise several subgroups of peptides with 1.2 million unique modified peptide sequences and 30 unique pairs of (amino-acid, PTM), covering both experimentally introduced and naturally occurring modifications on various amino acids. We evaluate the utility and importance of the dataset by providing benchmarking results on models trained with and without modifications and highlighting the impact of including modified sequences on downstream tasks. We demonstrate that predicting the properties of modified peptides is more challenging but has a broad impact since they are often the core of protein functionality and its regulation, and they have a potential role as biomarkers in clinical applications. Our datasets contribute to applied machine learning in proteomics by enabling the research community to experiment with methods to encode PTMs as model inputs and to benchmark against reference data for model comparison. With a proper data split for three common tasks in proteomics, we provide a robust way to evaluate model performance and assess generalization on unseen modified sequences.

Introduction

Proteins are fundamental components of living organisms, performing diverse biological functions essential for cellular processes, signaling, and structural integrity. The field of proteomics aims to study and understand the complex landscape of proteins present within a biological system. To facilitate analysis and identification, proteins are typically digested into smaller components, called peptides, using techniques such as enzymatic digestion . In the widely used bottom-up proteomics approach, peptides serve as the primary unit for investigation and characterization using Mass Spectrometry-based proteomics (MS), with peptides typically composed of 5 to 50 amino acids .

MS-based proteomics has revolutionized the study of proteins by enabling the identification and quantification of peptides in complex biological samples, facilitating the identification and quantification of proteins . Throughout MS experiments, various peptide properties are captured, providing valuable information for downstream applications, including peptide sequence identification and quantification . However, an important aspect that significantly influences peptide properties is the presence of post-translational modifications (PTMs) on amino acid side chains or peptide linkages.

PTMs are frequent structural changes that occur after protein translation and directly affect its function, allowing cells to respond quickly to stimuli . PTMs occur at distinct amino acid side chains, hence modifying its chemical structure . More so than individual modifications, PTM crosstalk is a major factor in defining protein function, and various diseases have been connected to altered PTM patterns [7; 8; 9]. This makes the study of PTM regulation extremely valuable for both fundamental and applied clinical research , including drug response . Finally, PTMs impact a wider scope of tasks and applications such as protein folding [12; 13] and drug reception .

PTMs were discovered over an extended period of time, highlighting their abundance and diversity in biological proteomes. With over 400 PTMs discovered until now , PTMs play an essential role in understanding proteins and their functions. PTMs are instrumental in understanding cellular processes, aging, and diseases. They occur naturally with a varying level of abundance (see Appendix Section C); some PTMs, such as phosphorylation and acetylation, are very frequent, while others, like sumoylation, are relatively rare . These modifications can be naturally occurring or be induced by the researcher or the experimental setup. To facilitate experimentation, researchers introduce modifications such as Cysteine carbamodomethylation and Tandem Mass Tags (TMT), which is a technique for sample multiplexing used to concurrently analyze up to 18 separate samples in a single MS run [15; 16]. Other modifications are introduced unintentionally, such as Oxidation.

PTMs modify multiple peptide properties, including but not limited to mass, charge, retention time, and fragment ion intensities. Copies of peptide sequences can be present in different modification states. Each variant has modifications at different locations and displays different properties. Hence, it is crucial to incorporate PTM information into machine learning tasks to accurately predict properties of peptide sequences. The choice of how to represent sequences and the present modifications on the amino acids dictates the usage of specific model architectures. Although sequence-based models are relatively dominant , Graph Neural Networks can be applied to some problems since amino acids and modifications can be represented as molecular graphs [18; 19; 20].

Datasets and machine learning tasks to predict properties of peptide sequences are well established in the proteomics research community [21; 17]. However, incorporating PTM information in prediction tasks is still in its early phase, specifically when it comes to labeled reference datasets comprising sequences rich in PTMs. Such datasets would facilitate training and benchmarking models with different ways of incorporating PTMs into a machine learning workflow.

Protein Language Models (pLM), leveraging the embeddings from pre-trained language models on large protein databases, are common in down-stream tasks [22; 23], such as PTM site prediction . However, pre-trained pLMs do not typically capture PTM information and focus on protein sequences rather than peptides, hindering the development of down-stream PTM-related tasks.

Here, we introduce four annotated datasets, rich with modified peptide sequences, including 1.2 million unique modified peptide sequences and 30 unique amino acid-PTM combinations (i.e. distinct pairs of amino acid and PTM); more details are shown in Appendix Section A.2 in Table 3. These amino acid-PTM combinations are found in the human proteome and can be mapped back to human proteins. The datasets are based on upstream raw data available in ProteomeTools, a synthetic human proteome measurement [25; 26; 27] as well as novel unpublished data. The novelty of the datasetsis two-fold: the new mass spectrometry data with PTMs and the rich annotations provided. The annotated and processed datasets are aimed to complement and extend the original PROSPECT dataset, which primarily covers various classes of unmodified peptides [21; 28]. The datasets target primarily three tasks in proteomics; retention time (RT), fragment ion intensity, and precursor charge prediction. Each newly introduced dataset includes examples representing a common subgroup of modified peptide sequences; (1) TMT dataset includes peptide sequences labeled with Tandem Mass Tags, (2) Multi-PTM includes examples rich with 13 unique (amino-acid, PTM) pairs, (3) TMT-PTM includes six unique pairs of (amino-acid, PTM) occurring in TMT-labeled sequences. Our final dataset, Test-PTM, comprises 21 unique pairs of (amino acid, PTM) and is well-suited as a hold-out/test dataset. For example, it includes modifications not present in the other three datasets. This characteristic, along with others we detail in section 3.3, makes Test-PTM a good candidate for evaluating and benchmarking models that can encode, process, and learn useful representations of PTMs to predict various peptide properties.

## 2 Related Work

Inspired by models from Natural Language Processing (NLP) that can process sequential data, proteomics researchers developed and trained various model architectures, following a similar workflow as highlighted in . We summarize previous work in literature that tackled PTMs and highlight the different ways of encoding and feeding PTMs as inputs to deep learning models. Figure 1 depicts the life cycle of the input data (peptide sequences with PTMs present) with a focus on representing PTMs. The explained workflow can be considered a common ground for several supervised learning tasks, specifically for the three tasks: retention time, fragment ion intensity, and precursor charge state prediction.

### Input Format and Parsing

Input sequences are stored in a structured file format as strings. Each sequence is composed of amino acids with PTMs present at specific locations (sites) of the sequence. Amino acids are typically represented with a widely-adopted one letter code based on the IUPAC nomenclature , while modifications are represented by either a short name or an ID coming from common ontologies or controlled vocabularies [30; 31; 32]. An example sequence is shown in the input parsing step in Figure 1 part A, where modifications are represented by their Unimod ID .

Presenting input sequences in one field together with modifications at their respective locations is used in some models in the literature, such as Prosit  and PrositTransformer . Opting for this approach reduces the assumptions on input data but implies an additional parsing step in the data preparation before training the model. For parsing sequences and modifications presented as one field, researchers either leverage existing Python packages such as Ptyeomics [35; 36] or write their own utility and helper functions. Figure 1 part B illustrates what the parsing step would produce.

Figure 1: Sequence life cycle with modifications as inputs to a deep learning model.

For easier processing of modified sequences, several published models require the input data to be provided in two separate fields: one with the sequence of amino acids and the other with the modification name and site (i.e., index of the amino acid on which the modification is present). This approach is followed by DeepLC , pdeep2 , pdeep3 , and AlphaPeptDeep . Parsing and loading data is simpler in this case since indices with modification sites and names are readily available in a dedicated field in the input. Yet, for inference, raw data has to be prepared accordingly.

### Encoding and Representation of Sequences with Domain-Specific Features

A short review of encoding methods for input sequences with PTMs reveals two main options for representing modifications as additional inputs aligned with the peptide sequence. The first option involves encoding each modification using one-hot encoding or word embedding. Using embeddings allows for learning representations for all modifications present in the training data, as implemented in . The second option is to parse the modifications before training and represent them with extracted features based on domain knowledge and the changes they introduce to the amino acids. These features can include changes in chemical composition, mass, or other relevant properties of the modified amino acids. This approach is present in DeepLC , pDeep3 , and AlphaPeptDeep . Both alternatives are illustrated in Figure 1 part C.

Using word embeddings for PTMs provides a way to learn numeric representations for known modifications, which is well-suited for the supervised setup. Moreover, with a sufficiently large amount of training data, the subsequent layers in the model would learn to combine inputs to provide robust features relevant to the task, eliminating the need for manually designed feature extractors . However, since embedding vectors are learned from the training dataset and the modifications it contains, models would suffer from the out-of-vocabulary problem. Therefore, prediction of peptide sequences incorporating unseen modifications becomes challenging or is of relatively low quality. In contrast, feature extraction methods based on domain knowledge solve this problem by extracting relevant domain-specific information about the modifications from databases like Unimod  using tools such as Ptyeomics [35; 36]. This approach ensures that unseen modifications can be represented and fed to the model by extracting the relevant features at inference. Nevertheless, to hand-design good features, a considerable amount of domain expertise and time are required compared to automatically learning features . For example, DeepLC  incorporates atom counts at each amino acid after modifications and the sum of atom counts at neighboring pairs of amino acids as extracted features to improve retention time prediction of PTM sequences.

Several model implementations in the literature limit the scope of their training and inference to specific modifications to reduce the complexity of handling PTMs. For instance, Prosit-TMT , pDeep3 , and DeepFLR  have focused on particular types of PTMs. These models leverage the characteristics of the target modifications to improve prediction performance.

### Model Architectures to Process Modifications

To process PTMs effectively, deep learning models typically include a dedicated branch in the architecture to encode the modifications (a sequence aligned with the amino acid sequence without modifications) or process the extracted features. For instance, DeepLC  includes a distinct branch of convolutional layers to consume the extracted features from the modifications. Models that encode modifications with word embeddings, as the case for DeepFLR , usually have a trainable embedding layer for PTMs. For both approaches, models combine the encoded representations of the amino acid sequence and the sequence of modifications using a conditioning technique. The most common conditioning methods applied are element-wise multiplication as in Prosit , concatenation as in DeepLC , and matrix addition as in DeepFLR . Encoding and conditioning are depicted in Figure 1 part D. Depending on the model architecture, conditioning can happen early in the model (directly after the embedding layer) or later after encoding each input in a separate branch (sequence and modifications).

### Challenges

In the traditional database search, peptides and their modifications are identified by comparing their experimental mass spectra (MS2) with theoretical spectra generated from a database of protein sequences . However, as the number of potential modifications increases (due to multiple PTMtypes and potential sites), the search space for possible peptide modifications expands dramatically. This search space explosion can result in more potential matches due to the higher chance of collisions, where multiple peptides could have similar mass/charge. Hence, it becomes more challenging to confidently match precursor ions to peptide sequences. Consequently, search engines might consider more possible peptide matches for the MS2 spectra, leading to difficulties separating correct from incorrect matches. Moreover, precisely determining the location of a PTM within a peptide sequence poses a challenge primarily attributed to the possible occurrence of the PTM at multiple potential residues, as the MS2 spectra for these different permutations are similar . In the meantime, accurate localization of PTMs is crucial, as biological functions are often associated with different PTM sites . While several tools try to improve the accuracy of PTM localization, ground truth datasets like the ones we are introducing here would help improve such tools [41; 46; 47].

## 3 Dataset

Similar to the version of PROSPECT with unmodified peptide sequences [21; 28], ProteomeTools [48; 49] is the base for our four datasets. It contains multimodal liquid chromatography-tandem mass spectrometry analysis for over a million synthetic peptides, representing all canonical human gene products. We leverage the raw data from ProteomeTools and provide annotations and labels for retention time, charge, and MS2 spectra. Our choice of ProteomeTools as an upstream raw dataset is motivated by three reasons: (1) several models in literature use it for training models [33; 34; 37; 38; 50; 51] and for finding diagnostic features for PTMs , (2) it contains high-quality spectra with PTMs already localized, (3) all measurements are from synthetic samples of peptides. We refer to ProteomeTools [48; 49] and PROSPECT  for more details on the advantages of using ProteomeTools as an upstream dataset.

Our datasets are designed to facilitate training and evaluating machine learning models on various tasks in MS-based proteomics, particularly those involving modified peptide sequences. Since modifications change the chemical structure of the target amino acid, they impact the predicted properties of the peptide sequences. Due to the altered properties of the peptide sequences, the presence of modifications adds complexity to prediction tasks, making them more challenging. Furthermore, to support PTM localization tasks , we included peptide sets with permuted phosphosites (more details in Appendix Section A).

This section briefly describes the schema of the datasets, provides summary statistics and exploratory analyses focusing on PTMs, and highlights the impact of the introduced datasets. We primarily focus on retention time, fragment ion intensity, and precursor charge state prediction, yet the datasets can be used for several tasks in proteomics. Appendix Section E presents a full list of supported tasks.

### Dataset Generation and Schema

We apply the same annotation workflow as in the original PROSPECT dataset , which is based on an expert annotation system  to annotate \(y\) and \(b\) fragment ions (up to triple-charged) as well as possible neutral losses. While retention time and precursor charge states are directly extracted from the MS experiments. Our implementation of the annotation pipeline is available in a dedicated GitHub repository under the name Spectrum Fundamentals .

The four datasets comprise 61 packages, each with two main parquet file formats: meta-data and annotation files. Each package has one meta-data file, while the annotations file is split into multiple files per package to facilitate reading the data. Annotation files are sub-organized by pools, where a

   Dataset & Packages & Pools & Unique & Precursors & Spectra & Annot & Raw \\  & & & Peptides & & & Peaks & Peaks \\  TMT & 11 & 1000 & 714 K & 820 K & 28.2 M & 1.8 B & 11.2 B \\ Multi-PTM & 12 & 388 & 306 K & 412 K & 19.6 M & 2 B & 6 B \\ TMT-PTM & 6 & 260 & 118 K & 132 K & 7 M & 456 M & 2.8 B \\ Test-PTM & 29 & 147 & 53 K & 71.5 K & 3.9 M & 248 M & 936 M \\   

Table 1: Summary statistics of the dataset.

pool is a set of \(\) 1k peptides measured in one analysis run. A unique identifier is provided in both files to trace any example to its original raw data file in ProteomeTools. This identifier combines the raw file ID and the scan number. The original ProteomeTools upstream datasets are partially available on PRIDE  and have the same identifier names. Our annotated datasets are fully available online and are hosted on Zenodo . Figure 2 shows the various datasets we make available with this publication. Moreover, it explains the applied steps to the raw upstream datasets to arrive at the full Zenodo annotated datasets and eventually to the processed and split HuggingFace dataset for each of the three tasks in focus. Table 1 shows summary statistics of the data. The modified peptide sequences are represented as strings, allowing flexibility in using the dataset in different encoding and machine learning pipelines. To represent the sequences, we follow one of the recommended notations of ProForma , a standard notation for writing sequences with modifications. More statistics about the data are in Appendix Section C.

### Exploratory Data Analysis

To understand the diversity and distribution of modifications in our datasets, Figure 5 in Appendix Section C shows a heat map displaying the frequency of PTMs (log scale) on the respective amino acid sites. Figure 6 illustrates horizontal bar plots with log scale counts of the occurrence of PTMs and modified amino acids. Several observations indicate the complexity of handling PTMs, including (1) some PTMs occur more frequently than others, (2) PTMs occur only on specific amino acids, and (3) some amino acids are more frequently reported as modification sites in comparison to others.

### Data Split

As highlighted in Table 1, we curated four datasets, each including a subgroup of modified peptide sequences. We recommend using the first three datasets for training and validation splits during model training and hyper-parameter optimization, where data splitting should follow a sequence-based disjoint split. The Test-PTM dataset can solely be used to evaluate the model and its performance on unseen examples. This choice is motivated by several reasons: Test-PTM contains (1) sequences with the same PTM occurring at the same residue as in the other datasets but at different sites within the sequence, (2) sequences with the same PTMs as in the other datasets occurring at different residues (3) sequences with multiple PTMs occurring at different residue sites in the same sequence, (4) modifications that are not present in the other three datasets, (5) sequences with permuted phosphosites with their experimentally-acquired fragmentation spectra A.1, and (6) unmodified counterparts of modified peptide sequences. These characteristics allow for quantifying model performance on

Figure 2: Datasets made available with our publication, with upstream raw data sources and downstream usage highlighted.

sequences with PTMs and performance comparison between modified and unmodified peptide sequences. Further details on splitting the data are in Appendix Section G.

For accessibility to the machine learning community, we provide the preprocessed and filtered datasets with the recommended split for each of the three tasks on the Hugging Face Hub [63; 64; 65].

### Dataset Utility

The utility of the curated datasets can be highlighted by looking at the impact of PTMs on peptide properties, such as retention time and intensity of the fragment ions in the spectrum. We use common evaluation metrics specific to each property to compare modified and unmodified sequences, such as time-delta at \(95\%\), \(R^{2}\) and slope for retention time prediction, and the Spectral Angle (SA) for fragment ion intensity prediction . SA only captures the difference in the peak intensity and not the mass shift introduced by different PTMs. We compute the metrics with the experimental data (ground truth labels) for each task, the indexed Retention Time (iRT), the vector for fragment ion intensity, and the observed precursor charge state distribution. More details are in Appendix D.

Figure 3 shows the relationship between the change in mass induced by the different PTMs versus the effect on peptide properties quantified by the respective metric. For RT, panel A shows the slope versus the introduced change in mass. Panel B depicts the SA versus the change in mass to underscore the impact of PTMs on the MS2 spectra. Except for a limited number of modifications on lysine (K), other modifications do not exhibit a linear correlation between mass and the respective change in peptide properties. The missing correlation indicates that a simple feature as m/z would not help

Figure 4: Impact of different PTMs on retention time and the fragment intensities compared to its unmodified peptide counterparts. A spectral distance of 0.35 and delta iRT of 20 are commonly observed cutoffs in rescoring when differentiating correct from incorrect matches.

Figure 3: Scatter plots summarizing differences in iRT and fragment ion intensity between modified and matching unmodified peptides.

improve predictions; therefore, better approaches to encode PTMs and encapsulate the complexity of their impact are essential to improve predictions for peptide properties. Figure 4 shows the change in iRT and spectra induced by different PTMs to highlight that modifications can change properties in different ways. In quadrants two and three, similar patterns are observed for both properties, wherein the impact is either significant or insignificant. Conversely, quadrants one and four exhibit dissimilar behavior, with one property experiencing a substantial change while the other experiences a minimal one.

Peptides often occur in more than one precursor charge state, with the possible charge states of a peptide being called charge state distribution (CSD) [69; 70]. Next to affecting a peptide's RT and fragment ion intensity, PTMs are also known to alter a peptide's CSD . Training accurate CSD prediction models can enable a meaningful reduction of the size of predicted proteome-wide spectral libraries, thus increasing their specificity. One shortcoming of the PROSPECT PTM datasets is that they mostly contain peptides in a single precursor charge state, as depicted in Figure 7 in Appendix section C.

To further demonstrate the impact of the introduced datasets, we present an explicit example of a downstream task, showing the improvement of PSMs (Peptide-Spectrum Matches) and peptide identification rates on a TMTpro (18plex) phosphoproteome dataset . We chose this dataset because PROSPECT PTM datasets do not contain TMTpro-labeled peptides, and we wanted to showcase the multi-PTM aspect of trained models. More details are in Appendix Section F. Besides Prosit , many other peptide property prediction models, which also generalize well on downstream tasks, were trained or evaluated on the ProteomeTools data, like pDeep , AlphaPeptDeep , SpecEncoder  or InstaNovo , underlining the importance of the datasets and the proposed extension to PTMs. Because of the lack of PTM datasets, most models only explicitly support a small number of PTMs, and thus, our benchmarking highlights the research gap. In the example of fragment ion intensity prediction, while some models claim to support more PTMs, this is often only achieved by shifting the respective fragment ions in m/z space and not addressing the impact of the PTM on intensities. For precursor charge state prediction, currently, no published model can predict more than a few predefined PTMs to the best of our knowledge. Furthermore, PROSPECT PTM datasets are not limited to applications for boosting identifications. Peptide property predictions find applications in spectral library generation that would greatly benefit from additional predictors to reduce the library size, such as precursor charge state. This is critical, for example, in metaproteomics experiments where analysis suffers from huge search space . A smaller library size is preferred to increase sensitivity and specificity while reducing the computational cost. Peptide property predictions can also be used for single peptides, e.g., in targeted proteomics experiments, to pick the best collision energies , or to validate single peptide identifications by visually comparing experimental vs. predicted fragmentation spectra , e.g., in immunopeptides for neo-antigen validation, all relevant for translating findings to medicine .

## 4 Evaluation

Improving peptide identification is one of the main objectives of accurately predicting peptide properties. As we illustrated, predicting properties of peptides with PTMs is more challenging and requires incorporating PTM information. To evaluate models trained on modified sequences for predicting retention time and MS/MS spectra, we choose two intrinsic evaluation metrics that quantify the model performance. As a baseline, we report prediction results from a Prosit model  trained on unmodified sequences and compare them against the experimental values (true labels) to highlight the importance of incorporating PTM information. At inference time, PTM information is not taken into account since the model was trained on unmodified sequences. We additionally report prediction results for retention time on DeepLC  and AlphaPeptDeep  for MS2 spectra since they incorporate PTM information based on atom counts from the chemical structure of PTMs and amino acids.

Table 2 summarizes the performance of the different models used for benchmarking the three tasks with the recommended evaluation metric, with metrics reported on Test-PTM dataset, split in seen/unseen PTMs during training (relevant only to Prosit ). The Prosit  naive model was trained on all datasets except Test-PTM. Hence, it does not support unseen PTMs but rather ignores them. Two variants of Prosit encoded PTMs with domain-specific features: Prosit-DeltaMass and Prosit-DeltaAtoms. Prosit-DeltaMass uses the mass introduced by the PTM as an input feature to the

model, while Prosit-DeltaAtoms uses the atom count introduced by the PTM [40; 37]. AlphaPeptDeep  does not support N-term modifications.

The metrics show that the base model performs poorly on modified sequences since it does not encode present modifications. Improved performance is observed for Prosit naive-encoding only on seen modifications. Prosit-DeltaMass performs better than the base model but does not fully capture the nuances of PTMs since several PTMs with similar mass behave differently. Prosit-DeltaAtoms performs best across all model variations as it encodes PTMs with more complex domain-specific information. More experimental details can be found in Appendix Section H.

### Retention Time Prediction

We use the time delta iRT metric for RT, which provides fine-grained and domain-specific insights into the model performance [51; 33; 37]. As recommended by PROSPECT , we report the time-delta at \(95\%\)\( t_{95\%}\), which is the minimal time window containing the errors (residuals) between observed and predicted retention times for \(95\%\) of the peptides . An implementation of this metric is available at the GitHub repository .

Figure 10 in Appendix Section D.2 shows in more detail the distribution of retention time predictions across different PTMs, comparing our best performing model (Prosit-DeltaAtoms) to DeepLC , one of the state of the art retention time PTM-aware models.

### Fragment Ion Intensity Prediction

For MS/MS spectra, we use the normalized spectral angle used in Prosit  and recommended in PROSPECT . Code for calculating the spectral angle is available at the GitHub repository .

Figure 11 in Appendix Section D.2 shows in more detail the distribution of fragment ion intensity predictions across different PTMs, comparing our best performing model (Prosit-DeltaAtoms) to AlphaPeptDeep , one of the state of the art fragment ion intensity PTM-aware models.

### Precursor Charge State Prediction

We use the mean absolute error (MAE) for precursor charge state prediction. In contrast to the Pearson Correlation Coefficient (PCC) used in previous precursor charge state prediction publications [69; 70], MAE can robustly assess cases where peptides occur only in a single or two distinct charge states. Figure 13 in Appendix Section D.2 shows the distribution of precursor charge state predictions across different PTMs in more detail.

## 5 Conclusion and Limitations

This work introduced PROSPECT PTMs, four annotated datasets for MS proteomics research based on ProteomeTools [48; 49]. The datasets contain peptide sequences with various PTM types occurring at different amino acid sites. Although the datasets are not limited to retention time, fragment ion intensity, and precursor charge state prediction, we focused on these three tasks due to their importance in downstream applications [79; 80; 67]. We recommended metrics and visualizations for model

    &  &  &  \\ Model &  &  &  \\  PTMS & unseen & seen & unseen & seen & unseen & seen \\  Prosit baseline  & 26.1 & 25.8 & 0.17 & 0.16 & 0.75 & 0.74 \\ DeepLC  & 19.5 & 13.7 & – & – & – & – \\ AlphapeptDeep  & 14.2 & 12.7 & – & – & 0.78 & 0.83 \\ Prosit naive-encoding & 25.2 & 16.4 & – & – & 0.79 & 0.89 \\ Prosit - DeltaMass  & 17.2 & 14.2 & – & – & 0.81 & 0.87 \\ Prosit - DeltaAtoms  & 12.7 & 9.9 & – & – & 0.86 & 0.89 \\   

Table 2: Model performance comparison for the three tasks.

evaluation, especially when incorporating PTM information. The Hugging Face Hub versions of the datasets are processed and split to provide all the required annotations for the three tasks to train and evaluate machine learning models . We provide benchmarking results for six in-house novel trained and recent state-of-the-art deep learning models. The models include Prosit  as a baseline pre-trained on unmodified peptides and three variants of Prosit trained on modified peptides: Prosit-Naive, Prosit-DeltaMass, and Prosit-DeltaAtoms. Additionally, we report results on DeepLC  for retention time prediction with PTMs and AlphaPeptDeep  for both retention time and fragment ion intensity prediction with PTMs.

Although the datasets include examples for various PTMs as highlighted in Appendix Section C, we acknowledge the limitations implied by omitting others. However, these limitations are inherent to the biological origin of the data; new PTMs are still being discovered , and only some PTMs can be synthesized efficiently on a large scale. Using experimental data for others bears the risk of generating a training set with an unknown number of false positives. Another limitation is that the mass analyzers used to acquire the datasets only cover Orbitraps and Iontraps. Nevertheless, our experiments showed that models trained on those (e.g., Prosit ) generalize to TimsTOF data and lead to similar increases in peptide identifications . We expect further data examples from other peptide sets (e.g., PTMs) and mass spectrometers (e.g., Waters) to be added over the next years, reducing biases and covering additional experimental settings.

Our annotation pipeline has a few limitations; first, we kept all annotations found for the same peak, which partially led to over-annotation. Second, the same label can be assigned to multiple peaks if they lie within the tolerance of the theoretical mass-to-charge ratio (m/z). Despite these limitations, the datasets still contain all the information required to develop better filtering approaches. A final limitation is that we restricted our annotation pipeline to a subset of possible annotation ions, leaving out some known ones, mainly diagnostic ones and ammonium ions.

The evaluation of our models on three tasks demonstrates their utility and effectiveness in extending existing models to tackle PTMs when predicting peptide properties. The results indicate that incorporating PTM information is required to improve the accuracy of model predictions, especially since model performance may vary depending on the presence of PTMs and their impact on the predicted properties. Future research should explore the development of specialized models tailored to encode and process PTMs.

One future direction is to explore and utilize augmentation techniques to enhance the generalization of models on rare or unseen modifications. Data augmentation can be useful in cases where certain modifications or amino acid sites are rarely present in the data but can be artificially introduced by augmentation. Future work should focus on establishing such techniques, utilizing our reference datasets to increase the robustness and versatility of models in handling a wider range of PTMs.

PTMs do not only alter the chemical properties of peptides but also significantly impact other characteristics, such as precursor charge and the behavior of fragment ions during analysis. Some of the most notable PTMs, such as phosphorylation, citrullination, and malonylation, result in the loss of specific chemical groups from the peptide structure . These losses can lead to changes in the intensities observed during analysis, as a significant portion of the intensity is transferred to the same peak undergoing neutral loss. This phenomenon can profoundly influence the interpretation of spectra and identification of peptide sequences. The newly introduced datasets complement and synergize with PROSPECT , enabling the study of the change in additional less-investigated properties, such as precursor charge, thereby enhancing the depth and scope of PTM analysis. Additionally, the datasets can be used in tandem in novel ways, such as learning spectrum embeddings for matching pairs of modified and unmodified peptides, learning joint embeddings for both spectra, or training models that convert one representation to the other .

Finally, our datasets open space for developing new models that encode and process PTMs, a novel area of applied research in proteomics. Additionally, it can be used to learn spectra from peptide SMILES  representation or vice versa, similar to the work done for small molecules in . Further benchmarking and comparison with new and emerging models should be pursued to advance the field and drive improvements in the prediction performance of peptide properties with PTMs. Our reference datasets can serve as a foundation for benchmarking and comparison of new models.

## Conflict of Interest

Mathias Wilhelm is a founder and shareholder of MSAID GmbH and OmicScouts GmbH, with no operational role in either of the two companies.