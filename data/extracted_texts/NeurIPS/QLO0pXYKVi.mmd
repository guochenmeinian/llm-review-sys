# Fusu: A Multi-temporal-source Land Use Change Segmentation Dataset for Fine-grained Urban Semantic Understanding

Fusu: A Multi-temporal-source Land Use Change Segmentation Dataset for Fine-grained Urban Semantic Understanding

 Shuai Yuan\({}^{1}\)  Guancong Lin\({}^{2}\)  Lixian Zhang\({}^{3}\)  Runmin Dong\({}^{4}\)  Jinxiao Zhang\({}^{4}\)

Shuang Chen\({}^{1}\)  Juepeng Zheng\({}^{2}\)  Jie Wang\({}^{5}\)  Haohuan Fu\({}^{6}\)

Corresponding authors.

###### Abstract

Fine urban change segmentation using multi-temporal remote sensing images is essential for understanding human-environment interactions in urban areas. Although there have been advances in high-quality land cover datasets that reveal the physical features of urban landscapes, the lack of fine-grained land use datasets hinders a deeper understanding of how human activities are distributed across the landscape and the impact of these activities on the environment, thus constraining proper technique development. To address this, we introduce FUSU, the first fine-grained land use change segmentation dataset for Fine-grained Urban Semantic Understanding. FUSU features the most detailed land use classification system to date, with 17 classes and 30 billion pixels of annotations. It includes bi-temporal high-resolution satellite images with 0.2-0.5 \(m\) ground sample distance and monthly optical and radar satellite time series, covering 847 \(km^{2}\) across five urban areas in the southern and northern of China with different geographical features. The fine-grained land use pixel-wise annotations and high spatial-temporal resolution data provide a robust foundation for developing proper deep learning models to provide contextual insights on human activities and urbanization. To fully leverage FUSU, we propose a unified time-series architecture for both change detection and segmentation. We benchmark FUSU on various methods for several tasks. Dataset and code are available at: _https://github.com/yuanshuai0914/FUSU_.

## 1 Introduction

Urban areas, housing 57% of the world's population on just 3% of global land, are dynamic hubs of human activity . The scale and rapid pace of current urbanization, encompassing both internal dynamics and population growth, position urban areas as a crucial catalyst of global climate change and vice versa . Therefore, proper observation and monitoring of urban changes are crucial for modeling human-nature interactions.

In the era of data-driven methods, satellite remote sensing provides abundant data for Earth observation and deep learning-based models to comprehend the changes and mechanisms in such observations. However, urban areas have unique features requiring stringent conditions for high-quality data as Fig. 1 shows. First, multiple semantics are concentrated in small areas, and this dense semantic information is driven by human activities (land use) rather than natural characteristics (land cover) . This necessitates high-resolution images and fine-grained land use annotations over land cover segmentation datasets. Second, urban areas undergo rapid temporal changes, demanding high-frequency observations to capture these dynamics accurately . Third, Fig. 1 highlights the diversity of human activities during the urban changes, including work, construction, relocation, and entertainment, requiring multi-source data for effective monitoring.

Although numerous land cover change segmentation datasets (e.g., LoveDA , SECOND , Hi-UCD , DynamicEarthNet ) have been introduced to advance urban monitoring, their coarse-grained land cover classification systems still limit the ability of fine urban semantic understanding. For example, the SECOND dataset only focuses on six classes, including ground, trees, low vegetation, water, buildings, and playgrounds, which fails to capture the full range of urban elements and detailed land use information, thus inadequately reflecting urban conditions and human-urban interactions. Besides, due to the difficulties of acquiring multi-temporal high-resolution images (e.g., cloud obscuration, accessibility), most change segmentation datasets only comprise bi-temporal images with even single-temporal annotations, which cannot match the pace of urban development, leading to challenges in timely planning and management. A high spatial-temporal resolution change segmentation dataset with a fine land use classification system is required.

In this paper, we introduce FUSU, the first multi-temporal, multi-source land use change segmentation dataset with the finest pixel-wise change segmentation annotations to date, covering 17 land use classes and over 30 billion pixels. It includes bi-temporal high-resolution satellite images (0.2-0.5 \(m\) resolution) and aligns optical and radar satellite data (Sentinel-2, Sentinel-1) with monthly revisits, enriching temporal and multi-sensor information. Spanning 847 \(km^{2}\) across five major urban districts in northern and southern China, FUSU's geographical diversity ensures domain shifts within the dataset. To leverage this spatial-spectral-temporal-resolution diversity, we propose FUSU-Net, a unified time-series architecture, as a baseline to make full utilization of the enriched information in FUSU for change detection and segmentation tasks. FUSU and FUSU-Net aim to advance dataset and algorithm development for improved urban monitoring and understanding. Our contributions include:

* We introduce FUSU, the first land use change segmentation dataset with a fine land use classification system of 17 classes and over 30 billion annotation pixels. FUSU captures timely urban dynamics from different perspectives and bridges the gaps between rich remote sensing data and urban semantic understanding.

Figure 1: The unique features of urban areas. Compared with other geographic regions, urban areas have dense semantics, fast temporal changes, and involve a large amount of human activities.

* We showcase how the constructed time-series data can be leveraged for better urban monitoring by proposing a unified time-series baseline architecture FUSU-Net that conducts end-to-end change detection and segmentation tasks utilizing multi-temporal-source data.
* We benchmark FUSU on kinds of methods in several downstream tasks to provide a comprehensive insight.

## 2 Related Works

### Urban Change Segmentation Data

Urban change segmentation is a critical aspect of Earth observation, garnering significant attention in recent years. Various land cover datasets have been developed to support specific tasks like change detection and segmentation (see Table 1). ISPRS Potsdam2 provides high-resolution images for urban parsing, but it covers small areas and has a limited scale. SpaceNet , EuroSAT , and GID  cover larger areas but suffer from incomplete land cover classification, lower resolution, and single snapshots. LEVIR-CD  and WHU  focus on bi-temporal building change detection, but lack comprehensive semantics. SECOND , Hi-UCD , and WUSU  introduce multi-class semantic change detection. However, WUSU and Hi-UCD cover limited regions, and SECOND's coarse annotations and long intervals reduce continual observation capability. LoveDA  includes patches from various Chinese cities, but the classification system is coarse-grained, and the annotation only covers a single snapshot time. FLAIR  uses aerial and Sentinel-2 images for near-daily observations, yet only one temporal label cannot tell the changes during periods. DynamicEarthNet  provides daily observations and monthly annotations, but also suffers from the coarse-grained land cover classification system, which fails to provide semantics on human-environment interactions.

In summary, there is a lack of attention to land use datasets. Existing datasets usually present a trade-off among resolution, coverage, snapshot time, annotation pixel, and classification system. On the contrary, FUSU aims for the finest urban semantic understanding, providing the fine-grained land use classification system (17 classes), large-scale annotation pixels (30 billion), high-resolution images (0.2-0.5 m), large coverage (847 \(km^{2}\)), temporal information (bi-temporal high-resolution images and monthly Sentinel data), and supporting multiple downstream remote sensing tasks.

### Remote sensing tasks

**Change Detection** identifies surface differences by processing images of the same area captured at different times . It includes binary change detection [12; 13], which detects changes in a single class (changed or unchanged), and semantic change detection [6; 8; 17], which provides detailed land semantics. High-frequency observations are essential for timely geographical change detection, and fine-grained annotations improve precision. However, most datasets provide only bi-temporal

    & Dataset & Source & Images & Size & Area & Revolution & Class & Objects & Temporal & Temporal & Ann pixel \\  & & & & (\(km^{2}\)) & (\(m\)) & & & (mag) & (annotation) & (\(\)10) \\   & Postdam1  & Aerial & 38 & 6600 & 0.05 & 0.05 & 6 & \(LC\) & 1 & 1 & 0.8 \\  & SpaceNet & Maxar & 60,000 & 650 & 5,500 & 0.3-1.24 & 2 & \(BAR\) & 1 & 1 & 1.3 \\  & EuroSAT & Sentinel-2 & 27,000 & 64 & 11,059 & 10 & 10 & \(LC\) & 1 & 1 & 0.1 \\  & GID & GInfoen-2 & 150 & 6800-7200 & 50,000 & 1 & 5715 & \(LC\) & 1 & 1 & 7.3 \\  & LowDA & Google Earth & 5987 & 1024 & 536 & 0.3 & 6 & \(LC\) & 1 & 1 & 6.3 \\  & FLAIR & Aerial/Sentinel-2 & 77,762 & 51240 & 817 & 0.2/10 & 18 & \(LC\) & 4 days & 1 & 20.3 \\   & LEVIR-CD & Google Earth & 637 & 1024 & 167 & 0.5 & 1 & \(B\) & 2 & 1 & 0.005 \\  & WUU & Aerial & 8,189 & 512 & 192 & 0.3 & 1 & \(B\) & 2 & 1 & 0.4 \\  & SECOND & Satellite & 4,662 & 512 & 1,200 & 0.5-1 & 6 & \(LC\) & 2 & 2 & 0.9 \\  & Hi-UCD & Aerial & 1,293 & 1024 & 30 & 0.1 & 9 & \(LC\) & 3 & 3 & 2.7 \\  & WUSU & GInfoen-2 & 2 & 5500-7025 & 80 & 1 & 11 & \(LC\) & 3 & 3 & 1.5 \\  & DynamicEarthNet & PlanetFusion & 54,750 & 1024 & 16,986 & 3 & 7 & \(LC\) & daily & monthly & 1.9 \\   & **Google Earth/Sentinel-1/2** & **62,752** & **512/128** & **847** & **0.2-0.5/10** & **17** & **\(LU\)** & **monthly** & **2** & **32.2** \\   & & & & & & & & & \\ 

Table 1: A survey on open-source urban change segmentation datasets, including segmentation datasets and change detection datasets.

observations due to the challenge of acquiring high-resolution multi-temporal images, resulting in long intervals that impede timely monitoring. Additionally, the lack of fine-grained multi-temporal annotations restricts the development of semantic change detection algorithms.

These challenges highlight the need for richer temporal data and fine-grained land use classifications, as well as methods capable of handling multi-temporal information. Current datasets' coarse-grained classifications do not accurately reflect urban conditions, and integrating multi-temporal data from other accessible sensors to enhance change detection has been underexplored. To address these issues, we propose FUSU, which includes bi-temporal fine-grained annotations and multi-temporal observations from high-resolution and Sentinel images. We also design a new unified architecture FUSU-Net to leverage time-series information for semantic change detection and segmentation.

**Semantic segmentation** has been widely applied in remote sensing for tasks such as land cover mapping , geographical object extraction [19; 20; 21], and cropland cover mapping . Encoder-decoder architectures are well-suited to the diverse nature of remote sensing images . Most studies focus on segmenting objects from static images [5; 23], while some have used time-series images to improve performance [22; 24]. Our FUSU-Net integrates time-series information into the bi-temporal segmentation task. We believe the unique time-series structure of FUSU will inspire the development of more advanced time-series segmentation algorithms in remote sensing.

## 3 FUSU Dataset

We introduce FUSU, a multi-temporal, multi-source change segmentation dataset for fine-grained urban semantic understanding. FUSU consists of 62,752 image patches, each containing 27 images from three sources with different resolutions and snapshot times, and includes two annotations as shown in Fig. 2. FUSU has four key features:

**Fine-grained:** FUSU features the finest land use classification system in change segmentation datasets, with bi-temporal dense annotations. It includes 17 classes--artificial-constructed, agricultural, and natural--that detail urban functional zoning and enhance understanding of urban structural development.

**Multi-temporal:** FUSU offers time-series observations with monthly revisits. Along with bi-temporal high-resolution images and fine-grained annotations, it supports high-frequency urban monitoring, enabling methods to leverage long-range temporal context for better inferences.

**Multi-source:** FUSU combines data from three satellite sources (Google Earth, Sentinel-2, Sentinel-1) with different temporal, resolution, and band compositions. Each image patch unifies spatial, temporal, and spectral contexts, providing richer information than single-source data.

**Domain shifts:** FUSU covers five urban areas in northern and southern China, each with diverse geographical features and urban landscapes. Variability in climate types and class ratios across these regions contribute to representation gaps and pronounced domain shifts in the feature data.

Figure 2: The visualization of the FUSU dataset construction. Each patch has 27 images (25 Sentinel images and 2 high-resolution images), and 2 labels. The content of the high-resolution image is center-surrounded by the Sentinel image as the red rectangle shows.

### Construction of FUSU

**Acquisitions.** FUSU uses three data sources with different resolutions, geographical details, and acquisition times. Google Earth images are \(512 512\) pixels with a 0.3 m resolution and RGB bands. Sentinel-1 and Sentinel-2 images are sourced from Google Earth Engine (GEE). Sentinel-1 images are preprocessed by GEE (noise removal, radiometric calibration, orthorectification). Sentinel-2 images undergo cloud removal, atmospheric correction, radiometric calibration, and orthorectification, then are concatenated with Sentinel-1 data. Each Sentinel image is \(128 128\) pixels with a 10 m resolution and 14 bands. Google Earth and Sentinel patches are not strictly aligned; Google Earth patches cover only the central area of corresponding Sentinel patches (Fig. 2). This approach preserves semantic detail and captures broader context, aiding spatial dynamics understanding. More details are in the Sec. A.3.

**Distribution.** FUSU covers 847 \(km^{2}\) across five urban districts in China: Xiuzhou in Jiaxing, and Yanta, Beilin, Xincheng, and Lianhu in Xi'an. The different climates of Jiaxing and Xi'an are illustrated in Fig. 3(a). FUSU provides continuous monthly observations from August 2018 to August 2020. Google Earth images were captured in August 2018 and August 2020, while Sentinel-1 and Sentinel-2 images were collected monthly between these dates.

**Annotations.** Bi-temporal Google Earth images are manually annotated pixel-wise by two teams of geography experts. Table 2 shows the classes, label values and colors. More details about the annotations can be found in Sec. A.1.

### Statistic

FUSU includes bi-temporal pixel-level annotations covering 17 land use classes. Fig. 4(a) and (b) illustrate the distribution of pixels and polygons for each class at a single time snapshot. Residential land dominates both in terms of polygons and pixels. Some classes, like agriculture construction land, exhibit asymmetrical distributions. The highly unbalanced distribution numbers show a ratio exceeding 90 between the most and least frequent types. Fig. 4(c)-(f) display the class ratios in Xi'an and Jiaxing at two-time snapshots, revealing varying distributions between the cities. Jiaxing is characterized by significant cropland and residential areas, while Xi'an has more commercial land. These class imbalances and city differences pose challenges for urban monitoring using FUSU.

   Color & Class & Label Value & Color & Class & Label Value & Color & Class & Label Value \\    } & Traffic land & 1 & Industrial land & 7 & Special land & 13 \\  & Inland water & 2 & Orchard & 8 & Forest & 14 \\  & Residential land & 3 & Park & 9 & Storage & 15 \\  & Corpland & 4 & Public management & 10 & wetland & 16 \\ Agriculture construction & 5 & Commercial land & 11 & Grass & 17 \\  & Blank & 6 & Public construction & 12 & Background & 0 \\   

Table 2: Land use classification system of FUSU and corresponding label values, colors.

Figure 3: The distribution of the FUSU dataset. (a) Xi’an and Jiaxing are located in different climate zones. (b) The 5 urban districts of Xi’an and Jiaxing in FUSU dataset. (c) The visualization of image samples.

## 4 FUSU-Net

To fully utilize FUSU, we propose a unified time-series baseline architecture named FUSU-Net that conducts end-to-end change detection and segmentation tasks. Fig. 5 shows the architecture.

### Preliminary and Overview

Given T1 image \(_{1}\), T2 image \(_{2}\), the corresponding groundtruth labels \(_{1}\), \(_{2}\), and the time-series temporal images \(_{T}\), we have two ultimate goals: build a segmentation function \(_{s}\) that generates segmentation map \(}=_{s}(_{T})\), and build a change detection function \(_{c}\) that find binary changes between two input images \(}=_{c}(_{1}},_{2}} _{T})\). These two goals mean we need to optimize the loss \(\) between predicted values and labels:

\[^{*}=*{arg\,min}_{}\{^{s}(_{s}( _{T}),)+^{c}(_{c} (_{1}},_{2}}_{T}),_ {c})\},\] (1)

where \(^{*}\) is the optimized learned parameters generated by the optimized \(^{c}\) and \(^{s}\), and \(\) represents the learned parameters, and \(_{c}\) is the binary change groundtruth label, which can be generated by \(_{1}\), \(_{2}\):

\[y_{c}^{(i,j)}=\{0,&y_{1}^{(i,j)}\ =\ y_{2}^{(i,j)} \\ 1,&y_{1}^{(i,j)}\ \ y_{2}^{(i,j)},.\] (2)

where \(y^{(i,j)}\) is the pixel value. Assuming the additional temporal and spectral information in time-series images can guide the high-resolution segmentation and change detection, we further extract the high-level temporal and spectral information and use \(_{1}\) for supervision. Thus the optimization body can be divided into:

\[^{*}= *{arg\,min}_{}\{_{1}^{s}(_{s}(_{s}(_{T},_{1};), _{1};))+_{2}^{s}(_{s}( _{s}(_{T},_{1};),_{2}; ))+\] \[_{T}^{s}(_{s}(_{T},_{ 1};))+^{c}(_{c}(_{1}},_{2}}_{s}(_{T},_{1};)), _{c};)\},\] (3)

where \(_{\{1,2,T\}}^{s}\) is the loss of segmentation of T1 image, T2 image, and time-series images, respectively.

### Overall architecture

As Fig. 5 shows, the overall architecture of FUSU-Net includes two branches: (a) This branch processes Sentinel time-series images and outputs time-series features; (b) This branch processes

Figure 4: The statistic of the FUSU dataset. (a) Pixels distribution. (b) Polygon distribution. (c) Class distribution of T1 Xi’an. (d) Class distribution of T1 Jiaxing. (e) Class distribution of T2 Xi’an. (f) Class distribution of T2 Jiaxing.

bi-temporal high-resolution images and annotations and outputs both bi-temporal segmentation results and change detection results.

As Fig. 5(a) shows, to process the Sentinel time-series images, we use U-TAE  with temporal attention to effectively capture temporal information in feature maps at various resolutions. The input shape is \(25 14 512 512\) (\(T C H W\)) and the output shape is \(64 512 512\). Fig. 5(b) shows that we first use an HR-Net pre-trained on ImageNet as the backbone to extract bi-temporal features. Then we input each feature into separated ASPP  segmentation heads to get the segmentation results. We then conduct a minus operation between bi-temporal segmentation features, and after a Spatial Pyramid Pooling head , we can get the binary change detection result. Note that Fig. 5(c) shows the fusion module. Time-series features fuse with bi-temporal features via two transformations: First, the time-series feature is center-cropped to strictly geographically align with the bi-temporal features. Then after a \(1 1\) convolution and upsampling layer, the center-cropped feature has the same shape with bi-temporal features. Second, we reserve the large spatial information of the time-series feature and after a bottle-neck structure with a dilated convolution, we map it to the same shape of the bi-temporal features. An add operation is conducted for the feature fusion.

### Loss Functions

As discussed in Sec. 4.1, we use 4 loss functions to train FUSU-Net: three segmentation loss \(^{*}_{\{1,2,T\}}\), and a change loss \(^{c}\). The segmentation loss functions are the multi-class cross-entropy loss. Specifically, for time-series supervision, we first centercrop the output for geographical alignment, then upsample it to the same size of groundtruth label \(_{1}\). The change loss is the BCE loss to supervise the binary changes. More details about supervision and implementation can be found in Sec. A.5.3.

## 5 Experiments

We utilize our dataset for semantic segmentation in Sec. 5.1 and change detection in Sec. 5.2 with various experiments on state-of-the-art baseline methods and FUSU-Net. We also validate the feature disparities between Jiaxing and Xi'an in the segmentation task.

### Semantic Segmentation

Land use segmentation is crucial for urban monitoring. We focus on single-temporal images and labels for this semantic segmentation task. We compare other seven baseline segmentation methods with our FUSU-Net: FCN , PSPNet , Fast-SCNN , Deeplab-v3 , HRNet , K-net , and U-TAE . Evaluation is based on intersection over union (IoU) per class and mean IoU (mIoU) across all 17 land use classes, following established protocols. Additionally, we investigate feature disparities between Jiaxing and Xi'an through two experiments: intra-dataset (whole, Xi'an,

Figure 5: The architecture of FUSU-Net. (a) U-TAE branch for time-series images. (b) Bi-temporal branch for segmentation and change detection. (c) Feature fusion.

[MISSING_PAGE_FAIL:8]

**Overall results.** Table 6 and Table 5 present the results of binary and semantic change detection. In binary change detection, with only unchanged and changed pixels, class-specific IoU is not applicable. Our FUSU-Net outperforms other baselines by 7.21%-31.89% in IoU. In semantic change detection, challenging classes such as public management-10, public construction-12, and special land-13 are observed across all methods, consistent with semantic segmentation results. Notably, FUSU-Net achieves better performance compared to other baseline methods than it does in the semantic segmentation task, which can be attributed to continuous observation and change information provided by time-series Sentinel images between two high-resolution image snapshots.

## 6 Discussion

**Effectiveness of time-series.** We evaluate to what extent time-series images enhance the performance. Table 7 shows the results. We choose the number of time-series images as the variable (i.e., all time-series images, partial time-series images, zero time-series images). We can see for the FUSU dataset, more time-series images contribute to better results. It is desirable to use all time-series images as additional temporal information.

**Limitations.** The FUSU dataset has three primary limitations. First, it is limited to five urban districts. Despite its rich geographical diversity and pixel data, including more global urban areas is desirable. We encourage the community to share high-quality, fine-grained land use datasets to advance urban monitoring. Second, land use change segmentation requires understanding human activities and production, unlike land cover, which directly corresponds to pixel values. Relying solely on remote sensing imagery makes high accuracy challenging. Third, as Table 8 shows, because of the sensor gaps between optical images and SAR images, the simple concatenation of Sentinel-1 and Sentinel-2 is not ideal. Better fusion methods should be considered for synergizing both Sentinel-2 and Sentinel-1 strengths. In the future, we aim to design optical-SAR fusion methods and incorporate more multi-source data, such as economic and population data, to develop a multi-modal framework for comprehensive urban semantic understanding.

**Conclusion.** We present FUSU, a comprehensive multi-source, multi-temporal change segmentation dataset for fine-grained urban semantic understanding. FUSU includes a detailed 17-class land use classification system, 30 billion annotated pixels, 847 km\({}^{2}\) coverage, and temporal information from bi-temporal high-resolution images and monthly Sentinel data. This makes FUSU the most comprehensive urban semantic dataset available. We benchmark various methods to demonstrate FUSU's effectiveness in urban land use segmentation and change detection. Additionally, we introduce FUSU-Net, a model that fully utilizes the spatial, spectral, and temporal diversity of FUSU. We anticipate that FUSU and FUSU-Net will advance the development of powerful techniques for multi-source, multi-temporal change segmentation in urban environments without any negative societal impacts.

## 7 Acknowledgement

We acknowledge the annotation team for their high-quality work. This work was supported in part by the National Key Research and Development Plan of China (Grant No. 2023YFB3002400), and in part by the National Natural Science Foundation of China (Grant T2125006 and No. 42401415), and in part by Jiangsu Innovation Capacity Building Program (Project No. BM2022028).

  Method & IoU & Method & IoU \\  BIT  & 47.91 & Change-Former  & 59.64 \\ ICIFNet  & 64.74 & DMINNet  & 72.59 \\ A2Net  & 69.22 & USSFC-Net  & 62.85 \\  FUSUNet & 79.80 & & \\  

Table 6: Binary change detection results obtained from intra-dataset.

  Time-series & 0 & 9 & 18 & 25 \\  mIoU (Sag) & 46.72 & 47.19 & 48.47 & 50.10 \\ IoU (BCD) & 65.51 & 69.35 & 74.39 & 79.80 \\ nIoU (BCD) & 26.64 & 34.14 & 36.55 & 41.09 \\  

Table 7: Ablation results on the effectiveness of time-series.

   & + S1 & - S1 \\  nIoU (Seg) & 50.10 & 51.17 \\  

Table 8: Effectiveness of Sentinel-1.