# 4+3 Phases of Compute-Optimal Neural Scaling Laws

Elliot Paquette

McGill University

elliot.paquette@mcgill.ca

&Courtney Paquette

Google DeepMind & McGill University

courtney.paquette@mcgill.ca

&Lechao Xiao

Google DeepMind & Jeffrey Pennington

Google DeepMind

Corresponding author; website: https://elliotpaquette.github.io/.The authors contributed equally to the paper.

###### Abstract

We consider the solvable neural scaling model with three parameters: data complexity, target complexity, and model-parameter-count. We use this neural scaling model to derive new predictions about the compute-limited, infinite-data scaling law regime. To train the neural scaling model, we run one-pass stochastic gradient descent on a mean-squared loss. We derive a representation of the loss curves which holds over all iteration counts and improves in accuracy as the model parameter count grows. We then analyze the compute-optimal model-parameter-count, and identify 4 phases (+3 subphases) in the data-complexity/target-complexity phase-plane. The phase boundaries are determined by the relative importance of model capacity, optimizer noise, and embedding of the features. We furthermore derive, with mathematical proof and extensive numerical evidence, the scaling-law exponents in all of these phases, in particular computing the optimal model-parameter-count as a function of floating point operation budget. We include a colab notebook nanoChinchilla3 that reproduces some key results of the paper.

## 1 Introduction

The advent of large language models (LLMs) has changed our perceptions of the landscape of optimization and is resulting in the emergence of new interesting questions related to scaling. Prior to LLMs and other large models, we often viewed the large-scale optimization problems as being limited by the amount of data. In training language models, in contrast, data can be effectively infinite. Thus, _compute budgets_ can be the limitation. This leads to the following natural question: given an architecture, given a fixed compute budget, and having unlimited data, _how should one select the model size to minimize loss?_

To formally address this question, let us consider the general learning problem,

\[_{^{d}}\,()=_{x}[ (;x)]},\,:\,^{d} ,\] (1)

the number of parameters \(d\) is large, and the data vector \(x\) is drawn from an unknown distribution. We solve (1) using stochastic algorithms, such as stochastic gradient descent (SGD) with batch size \(B\), under various parameter sizes \(d\), that produce a sequence of iterates \(\{_{r}\}\). A standard formula used in practice to measure compute is the "6ND" formula , that is,

\[^{4}=((r)\ (B))\ \ (d).\] (2)Therefore, we can plot the loss curve \((_{r};d)=(r;d)=(/(d B);d)\) as a function of flops (see Fig. 1). The question now is: given a fixed number of flops \(\) and given batch size \(B\), how should we choose the parameters \(d\) so that we get the best loss, i.e. find \(d^{*}\) solving the constrained problem

\[d^{*}()_{d}}{d  B};d=_{d}(_{r};d)=(r B) d}.\] (3)

**Main contributions.** In this work, we analyze a three parameter simple model, which we call _power-law random features_ (PLRF) . The three parameters in the PLRF are the data complexity (\(\)), target complexity (\(\)) and model-parameter count \(d\). Using this model, we derive a deterministic equivalent for the expected loss, as a function of \(\), \(\), and \(d\), that captures the training dynamics of one-pass SGD. This can be used to derive numerical predictions for the scaling laws. We also extract exact expressions for the compute-optimal scaling laws and the optimal parameter \(d^{*}()_{d}(}{d  B};d)\) for large5\(d\), and give some estimates on the order of \(d\) necessary for these scaling laws to take hold.

We also observe for a large portion of the \((,)\)-phase plane, the optimal parameter is \(d^{*}()=^{1/2}\), suggesting a regime of _universal scaling behavior_ (see Fig. 4a and Table 2). This verifies theoretically the Chinchilla scaling .

The PLRF is not only analyzable, but also exhibits a rich behavior of compute-optimal curves/loss curves, which are qualitatively and quantitatively different depending on the strengths of the data \(()\) vs. target \(()\) complexity. Particularly, we show that there are _4 distinct (+3 sub phases)_ compute-optimal curve/loss curve behaviors.

_Model constrained compute-optimal curves._ In two of the phases (Phase Ia,b,c and Phase II), it is the underlying model that dictates the curves. The algorithm has little/no impact. This appears in two forms. The first behavior are compute-optimal curves controlled by the capacity of the model (Phase Ia,b,c). Here once the algorithm reaches the limiting risk value possible (capacity), it is better to increase the model-parameter \(d\). Another type of loss dynamics is due to poor model feature embedding (Phase II). Here the features are embedded in a way which is difficult to train. After an initial large decrease in the loss value, this feature embedding distortion frustrates the algorithm and training slows, but it continues to solve. However, solving to capacity wastes compute, in that it is compute-favored to increase the model parameter count \(d\).

_Algorithm constrained compute-optimal curves._ For some choices of \((,)\) (Phase III and IV), it is the noise produced by the SGD algorithm that ultimately controls the tradeoff. Here the algorithm matters. Indeed, another algorithm could change the compute-optimal curves for these phases.

Related work.The key source of inspiration for this work are , which identified compute optimality as a fundamental concept in scaling large language models and made a substantial empirical exploration of it. The problem setup was formulated by , where additionally data-limited scalings were considered, but compute optimality was not (nor indeed any algorithmic considerations); see also  where gradient flow is considered in the same setting.

Figure 1: **Toy scaling problem**. We plot the loss function, \((_{r};d)\) as a function of flops \(\) using (2). Consider a fixed number of flops \(=10^{7}\) (dashed line). If we had chosen, e.g., \(d=1600\), we can run for a long time, but our model does not have a lot of capacity and thus the value of the loss function remains high. On the hand, we can increase capacity by choosing a large number of parameters (e.g., \(d=51,200\)), but because our compute is fixed we can not run our algorithm for very long. Thus the loss value is still large. The optimal choice is \(d 6,400\). When done for every choice of \(\) gives the compute-optimal curve (red line). This choice of \((,)\) (Phase I) is an example of where _model capacity_ controls the compute-optimal curve, but it is not the only behavior we show. In other phases the compute-optimal is controlled by _poor model embedding_ (Phase II, III) and _SGD noise_ (Phase III, IV).

There is a substantial body of work considering scaling laws of losses (trained to minimum-loss) of dataset size vs parameter count, in a variety of settings (linear, random features, deep networks). See especially: , wherein a "hidden-manifold" model is considered for the data. We note that as we consider one-pass SGD, some dataset/parameter-count scaling laws are implicit from the results here; however, the training method (one-pass SGD) is, in some regimes, suboptimal given unlimited compute.

For additional related work on random features models (and sample complexity), random matrix theory in machine learning, and other deterministic equivalents for SGD, see Section A. We note that while this paper is fundamentally about computation, but the novel mathematical contributions could also be recast in terms of generalization bounds of one-pass SGD, some of which are new. For a detailed comparison of the convergence rates and sample complexity, see Table 4.

### Problem Setup: SGD on Power-law Random Features

In this work, we analyze the three parameter _power-law random features_ (PLRF) model, that is,

\[_{^{d}}\ ()}}{{=}}_{x}[( W^{T}x,-  x,b)^{2}]}.\] (4)

We embed the data vector \(x^{v}\) in \(^{d}\) through the matrix \(W^{v d}\) and construct noiseless targets6 by dotting a fixed \(b^{v}\) with the sample \(x\). The use of the matrix \(W\) allows the model to have variable capacity (\(d\)) independent of the data set size. The samples \(x^{v}\) and labels \(b^{v}\) have power law dependence, whereas the matrix \(W\) has entries distributed as \(N(0,1/d)\).

Figure 2: **Phase Diagram and Cartoon Plots of Loss Curves in Different Phases. (a) Phase Diagram. Colored regions represent where the training of the risk/compute-optimal curves look qualitatively and quantitatively different depending on \(\) and \(\). This, in term, yields different scaling law \(()\) and parameter count \(()\) exponents for each of the phases. Critical point at \(==1/2\) where all behaviors are observed. The other plots illustrate the components of \(\) (via \(_{0},_{pp},_{ac}\)) and \(_{pp}\) which dominate the loss curve for each phase (see Sec. C.4.1 & Sec. C.4.1 for proofs); tradeoff between the functions where the compute-optimal point occurs is also indicated (see Sec. 2.1 for definitions and Sec. 3.1 & Sec. D for proofs).**

**Assumption 1** (Data and labels, \(\) and \(\)).: _The samples \(x^{v}\) are distributed according to \((x_{j}) j^{-}z_{j}\) for all \(1 j v\) and \(\{z_{j}\}_{j=1}^{} N(0,1)\). The labels are scalars constructed by dotting the sample \(x\) with a signal \(b^{v}\) whose entries \((b_{j})=j^{-}\)._

Without the random matrix \(W\), the \(,\) are related to what is known in the literature as source and capacity conditions . For a detailed comparison of the parameters and related work, see Section A and Table 3.

The dimensions we consider throughout are always such that \(v Cd\) for \(C>1\). Throughout both \(v\) and \(d\) need to be large, but for some choices of \(\) and \(\), the \(v\) will need to be comparable to \(d\).

**Definition 1.1** (Admissible \(v\) and \(d\)).: _We assume that \(v Cd\) with \(C>1\) and \(v,d\). Above the high-dimensional line, which is when \(2>1\), we suppose \(v/d r(1,)\{\}\).7 On the other hand, below the high-dimensional line \((2<1)\) we limit \(v\) to be \(v/d r(1,)\).8_

One can rewrite the expression in (4) using the convenient form:

\[_{^{d}}\ ()= D(W -b),(W-b)},D=(j^{-2}) ^{v v}.\] (5)

Algorithmic set-up.To solve the minimization problem in (5), we use one-pass SGD with mini-batches of size \(B\) (independent of \(d\))9 and constant learning rate \(>0\): letting \(_{0}=0\), we iterate

\[\{x_{r}^{i}\}_{i=1}^{B}_{r+1}=_{r}-_{i=1}^{B}W^{T}x_{r}^{i}  W^{T}x_{r}^{i},_{r}- x_{r}^{i},b.\] (6)

The learning rate and batch size will need to satisfy a condition to ensure convergence (Prop. 2.1).

Main goal.Under this setup, our main goal is to characterize the compute-optimal frontier. Precisely, we want to find the parameter count exponent \(\) and scaling law exponent \(\), such that,

\[d^{}()^{}^{}}{d^{}B};d^{} ^{-}.\]

Notation.We use \((_{r})=(r)\) when we want to emphasize the iteration counter \(r\). We say \((r,v,d)(r,v,d)\) for functions \((r,v,d),(r,v,d)>0\) if for every \(>0\) and for all admissible \(v\) and \(d\), there exists an \(r_{0},d_{0}\) such that for all \(d>d_{0}\) and \(r r_{0}\)

\[(1-)(r,v,d)(r,v,d)(1+) (r,v,d).\]

Figure 3: **Compute-Optimal Front in Phase II-III boundary. (a) The Volterra equations perfectly captures the training dynamics of SGD when model-parameter count ranges from \(d=200 12800\). (b) We apply IsoFLOP approach  to our toy model to extract the optimal-compute front: (compute-optimal loss) (highlighted in red in (a)) and the optimal model size: (compute-optimal model size) (scattered in purple in (c)). Power-law fitting compute-optimal front gives a measurement of the scaling law exponent \(0.648\) (vs. theoretical prediction 0.643 in Table 2). In (c), we power-law fit the relation between compute and (empirical) optimal model size via Approach 1 and 2 used in : \(d^{}^{0.508}\) and \(d^{}^{0.525}\), resp. (vs. theory, \(d^{}^{0.5}\)). See Sec. J for details.**We write \(\) if the upper and lower bounds hold with some constants \(c,C\) in place of \(1\) respectively and \(,\) if only one inequality holds.

## 2 Learning Dynamics of SGD

Compute-optimal curves (3) for the random features model (4) rely on accurate predictions for the learning trajectory of SGD. Similar to the works of [33; 35], we show that the expected loss under SGD satisfies a convolution-type Volterra equation (for background on Volterra equations, see Section C.3)

\[[(_{r})\,|\,W]=(r)+ *[(_{r})\,|\,W]}_{},\,\,\,(*f)(r)=_{s=0}^{r-1}(r-1-s)f(s).\] (7)

The forcing function \((r)\) and kernel function \((r)\) are explicit functions of the matrix \(=D^{1/2}WW^{T}D^{1/2}\), where \(D=(j^{-2},1 j v)\), and \(\) a contour enclosing the spectrum of \(\),

\[(r) }}{{=}}_ {}(-z)^{-1}(D^{1/2}b),(D^{1/2}b)(1-2 Bz+ ^{2}B(B+1)z^{2})^{r}\,\,z\] (8) \[(r) }}{{=}}_{}(-z)^{-1}z^{2}(1-2 Bz+^{2}B(B+1)z^{2})^{r}\,\, z.\]

Intuitively, the forcing function is gradient descent on the random features model and the kernel function is the excess risk due to 1 unit of SGD noise.

Deterministic equivalent.The forcing function \((r)\) and kernel function \((r)\) are random functions depending on the random matrix \(\). Indeed, it is the _resolvent of \(\)_, \((-z)^{-1}\), which plays a significant role in \(\) and \(\). We remove this randomness from the expression by using a deterministic equivalent - a technique from random matrix theory.

Formally, we define the deterministic equivalent for the resolvent of \(\), denoted by \((z)\), implicitly via a fixed point equation

\[m(z)}}{{=}}_{j=1 }^{v}}{j^{-2}m(z)-z}} (z)=m(z)-z}\,:\,1 j v.\] (9)

This deterministic equivalent \((z)\) is viewed, roughly, as \(_{W}[(-z)^{-1}](z)\); though it is not formally the expectation over \(W\). By replacing the resolvent of \(\) with \((z)\), there exists a

  
**Function**\({}^{*}(x)\) is the Gamma function \\  \(_{0}(r) d^{-2+\{0,1-2\}}\) \\  \(_{pp}(r)(2)^{-1}- +1(2 B r)^{-(1+/)+1/(2 )}\) \\  \(_{ac}(r)C_{0}(r),&\,2 >1,\,2<1\\ 0,&\,2<1\) for \(C>0\), independent of \(d\) \\ If \(2>1,2>1\), \(_{ac}(r)_{j=1}^{v}j^{-2}(2)^{-1} 1-(2 B r)^{-1+1/(2 )} d^{-1}\) \\  \(_{pp}(r)(2)^{-1}2- (2 B r)^{-2+1/(2)}\) \\   

Table 1: **Large \(d\) behavior of the forcing function and kernel function.** See Sec. H for proofs.

deterministic function \((r)\) which solves a convolution Volterra equation, matching (7):

\[(r)=(r)}_{}+ *)(r)}_{}\] (10) \[(r)}}{{= }}_{}((z)(D^{1/2}b),(D^{1/2}b))( 1-2 Bz+^{2}B(B+1)z^{2})^{r}\;z\] (11) \[(r)}}{{= }}^{2}B_{}( z)z^{2}(1-2 Bz+^{2}B(B+1)z^{2})^{r}\;z.\] (12)

The solution to the Volterra equation with deterministic equivalent (10) numerically exactly matches the training dynamics of SGD, see Fig. 3. A discussion of the deterministic equivalent for \((-z)^{-1}\) can be found in Sec. E. All our mathematical analysis will be for the deterministic equivalents, going forward.10 The derivation of the Volterra equation for the expected loss can be found in Sec. B.

An immediate consequence of (10) is that for convolution Volterra equations bounded solutions occur if and only if the forcing function is bounded and the _kernel norm_\(\|\|}}{{=}}_{s=0}^{} (s)<1\). This directly translates into a sufficient condition on the batch size and learning rate of SGD.

**Proposition 2.1** (Sufficient conditions on learning rate and batch).: _Suppose learning rate \(\) and batch \(B\) satisfy \(\|\|<1\) and \((B+1)<2\). Then \((r)\) is bounded._

**Remark 2.1**.: _Below the line \(2=1\), the kernel norm diverges with \(v\) for fixed constant \(\), and so we must take \( 0\) to ensure bounded solutions. Thus, provided \( v^{2-1}\), then_

\[\|\|_{j=1}^{v}j^{-2} v^{1-2}\]

_Thus, the kernel norm, \(\|\|\), is always constant order for all \(\)._

The batch \(B\) and \(\) can depend on \(d\). For simplicity, we only consider \(B\) order 1 in this work. For a proof of the necessary and sufficient conditions on \(\) and \(B\), see Prop. C.2, and see Cor. G.1 for the asymptotic on \(\|\|\).

The Volterra equation in (10) can be analyzed to give a more explicit formula for \(\) (see Section C.3.2 for proof).

Figure 4: **(a) Scaling Law Exponents. The heatmap displays scaling law exponents (\(\)) in the \((,)\)-plane. Hatched lines represent region with universal scaling behavior, \(d^{*}^{0.5}\), independent of \((,)\). (b) Exponent Measurements. Compare empirical exponents (following ; see Sec.J for details) to theoretical predictions, traversing the phase diagram horizontally at \(=0.7\) from Phases \(\) as \(\).**

**Theorem 2.1** (Approximation solution for \(\)).: _Suppose \(\) and \(B\) are at most half the convergence threshold and \(2+2>1\), \(>\).11 There exists an \(M>0\) large and a constant \(C=C(,,M)\), independent of \(d\), so that for all admissible \(v\) and \(d\), for all \( Br>M\),_

\[(r)+(*)(r)(r)( r)+C(*)(r).\] (13)

_The convolution \(*\) further simplifies_

\[(r)+(r) (*)(r) {\\ }_{}+. \\ }_{}.\] (14)

_for some constants \(=(,,M)\) and \(=(,,M)>0\) independent of \(d\)._

**Remark 2.2**.: _If we were to run gradient descent instead of SGD (i.e., \(\) small), then we would only have the forcing term, that is, \((r)=(r)\). The measurable effect of SGD comes from the second term that contains the kernel function. For this reason, we refer to SGD noise as \((r)\)._

In light of (13) and (14), we have trapped the training loss between the sum of \(\) and \(\), so it suffices now to understand the forcing and kernel functions.

### Forcing function and kernel function

We decompose the forcing function (11), \(\), and the kernel function, (12), \(\), into

\[(r)=_{0}(r)+_{ac}(r)+_{pp}(r)+ _{}(r)(r)= _{pp}(r)+_{}(r).\] (15)

Each term is explicit and has an asymptotic equivalence (when \(1 Br d^{2}\)) given by

\[_{i}(r,d),_{pp}(r,d) c r^{-} d^{- }c,,>0).}\] (16)

The two error terms are such that for large \(d\) with \(1 Br d^{2}\),

\[|_{}(r)| C(_{0}(r)+_{ ac}(r)+_{pp}(r))|_{}(r)| C _{pp}(r),\]

for some constant \(C>0\). For \( Br d^{2}\), the forcing function \((r)_{0}(r)\), the limiting risk value. The terms arise from different parts of the spectrum of the deterministic equivalent for \(\) (see Fig. 6).

1. _Point mass at \(0\)_: \(_{0}(0)=_{0}(r)\) is the limiting value of \((r) d^{-2+\{0,1-2\}}\) as \(r\). It occurs because the loss is irreducible \((d<v)\), that is a component of the target is not in the image of the RF model (or equivalently that \(\) has a kernel).

Figure 5: **Finite-size effects.****(a)** The ratio of the exact solution of eq. (10) to the estimate in eq. (17) is bounded by constants for all \(r\), confirming the validity of eq. (17); shown here is \((,)=(0.7,1.2)\). **(b)** For non-asymptotic \(d\), the estimate in eq. (17) (solid curves) predicts both the magnitudes and trends of the measured exponents of the empirical compute-optimal frontier (points), shown here for \((,)=(0.7,1.2)\) computed using Approach 0 (see Appendix J) to capture the instantaneous slope; the dashed lines show the asymptotic exponents from Table 2. **(c)** The finite-size behavior relaxes to the asymptotic predictions over horizons whose length can grow exceedingly large, especially in the vicinity of the phase transition, shown here for \(=0.7\) approaching the Phase 4a\(\)4b boundary.

2. _Aligned features:_ The function \(_{pp}(r)\) represents gradient descent on the components of features which are aligned to the underlying population features. Indeed, if we ran gradient descent on the population loss _without_ a random features map (or a diagonal \(W\)), this would be the loss curve.
3. _Distorted features:_ The function \(_{ac}(r)\) is the result of feature distortion, where the matrix \(W\) leads to an embedding where a small component of the leading features is distributed across many different eigenmodes. These are still solvable, and given enough compute these will eventually be used, but they are much slower to solve.
4. _Aligned kernel:_\(_{pp}(r)\) is the excess risk due to \(1\) unit of SGD noise, which is then solved according to population gradient descent.

Out of brevity, we relegate the exact definitions of \(_{0}\), \(_{pp}\), \(_{ac}\), and \(_{pp}\) and all proofs of the asymptotics in Table 1 and analyses of the functions to Section F, G, and H.

## 3 The 4 Phases

We now put together a coherent picture of the effect of different choices of \(\) (data complexity) and \(\) (target complexity) and their impact on the compute-optimal frontier. By Theorem 2.1, we estimate

\[(r,d)_{pp}(r)+_{ac}(r)+_{0} (r)+_{pp}(r).\] (17)

Explicitly, we show that the functional form, or _scaling law_ for the PLRF model is

\[(r,d)}}_{_{pp}(r)}+ }}_{_{0}(r)}+}r^{- _{2}}}_{_{ac}(r)}+}}_{_{pp}(r)},_{i},_{i}>0\] (18)

Fig. (a)a. shows empirically that this equivalence of \((r)\) is quite good. The first two terms \(_{pp}(r)\) (i.e., \(r^{-_{1}}\)) and \(_{0}(r)\) (i.e., \(d^{-_{1}}\)) are often called in the literature as time and model bottlenecks respectively. The functional form using _only_ these two components, i.e., \((r,d) r^{-_{1}}+d^{-_{1}}\) were used to find compute-optimal exponents in [24; 9] and in concurrent work . Because the functional form considered in [9; 28] are missing the two other terms (cross-term \(_{ac}\) and SGD noise \(_{pp}\)), the compute-optimal curves in [9; 28] agree only in Phase Ia with our results. Importantly, we show that the cross-term, i.e., \(_{ac}(r)\), and SGD noise, \(_{pp}(r)\), can indeed affect the compute-optimal exponents. (The cross-term also appeared in concurrent work on ridge regression .)

The **4 distinct phases** (see Fig. (a)a) decompose the \((,)\)-plane based on the shape of the loss curve \((r)\), that is, which of the distinct components of the forcing function (i.e., \(_{0},_{pp},_{ac}\),) and/or kernel function (i.e., \(_{pp}\)) dominate the loss curve at a given iteration \(r\). See Table 2 for loss description in each phase. Cartoon pictures of the different features of the loss curves are shown in Fig. 2. For each phase, we derive a compute-optimal curve in Section 3.1.

The high-dimensional line, which occurs where \(2=1\), distinguishes the phases where the \(v\)-dimension can be big and independent of \(d\) (Phase Ia, II, III, \(2>1\)) and the phases where \(d\) and \(v\) must be related to each other (Phase Ib, Ic, IVa, IVb, \(2<1\)). When \(2+2<1\), the loss does not exhibit any power-law decay as the limit level stops going to \(0\) as \(d\) (purely as a consequence of having selected the regime \(v>d\)). Moreover, there exists an interesting critical point \(==\) where all the parts of the forcing function and kernel mix and interact with each other. The behavior of the loss at the pentuple point (see Fig (a)a) we leave for future research. Across each of the phase boundaries the compute-optimal curves are continuous, but not necessarily differentiable; in contrast, \(d^{}\) is discontinuous across some phase boundaries.

### Compute-optimal Curves

To simplify the computations for compute-optimal curves, we introduce the following curve

\[}(r)}}{{=}} _{pp}(r),_{ac}(r),_{0}(r), _{pp}(r)}.\] (19)

The function \(}(r,d)\) achieves the same power law behavior as the original compute-optimal curve \((r,d)\) (i.e., the slope of the compute-optimal curve is correct) and deviates from the true curve by an absolute constant (independent of \(d\) and \(\)). Note that some of the terms in the max function (19) should be taken to be \(0\) when not defined for the different phases. Therefore, we derive the compute-optimal curves by solving the problem

\[_{d}\,},d,d^{}()}}{{=}} _{d}\,},d,\\ }^{}( )}}{{=}}} () B},d^{}().\] (20)

See Table 2 for the exact expressions for \(d^{}()\) and the compute-optimal curve \(}^{}()\) for each phase. A more detailed description with proofs can be found in Section C.4 and Section D.

Now to derive \(d^{}\) and \(}^{}\), we recall that the functions \(_{0},_{pp},_{ac},_{pp}\) take the form \(c d^{-_{i}}()^{-_{i}}\) (16). Therefore, \(}^{}(/(d^{} B),d^{})\) must occur at corner point where two functions meet. These tradeoffs between the two functions for which the compute-optimal point occurs are shown in Fig. 2 and Table 2.

Details for each phase.We describe the qualitative and quantitative properties of compute-optimal curves for each phase. These are broken down into _model constrained_ (Phase I, II) vs. _algorithm constrained_ (Phase III, IV), i.e., whether the PLRF model or SGD is the constraining feature.

Phase Ia, Ib, Ic. Capacity constrained.Phase Ia (\(2>1,2<1\)), Ib (\(2<1,2<1,2(+)>1\)), Ic are characterized by having the simplest loss description, \((r)_{pp}(r)+_{0}(r)\). Here the SGD noise is irrelevant and one would have the same loss (and thus compute-optimal curve) as gradient descent on the population loss. Compute optimality is characterized by training the model completely (to its limit loss) and choosing the model parameter count large enough so that at the end of training, the smallest loss is attained. The main distinctions between Phase Ia, Ib, Ic are the model capacities (i.e., \(_{0}(r,d)=d^{-2+1-2}\) in Ia, Ib, and \(_{0}(r,d)=d^{-2}\) in Ic) and the dependence of dimension in the learning rate due to Ib,Ic being below the high-dimensional line. Consequently, while the qualitative features of the loss curve are the same for Ia, Ib, and Ic, the actual values of the compute-optimal curve vary across the different regions. Notably, in Phase Ib, the compute-optimal parameter is \(d^{}=^{1/2}\) and it is independent of \(\) and \(\).

Phase II. Distortion constrained.Phase II (\(2>1\), \(2>1\), \(<\)) has a loss curve where the \(_{ac}\) is important, that is, \((r)_{pp}(r)+_{ac}(r)+_{0}(r)\). The \(_{ac}\) term becomes the dominant term after running for some intermediate amount of time \(d^{}\); in fact it is compute-optimal to stop at this point, and then select the number of model parameters so to minimize the loss with this early stopping criterion. It transpires that across _all_ phases, it _never_ pays to solve through the \(_{ac}\) part of the loss curve - it is always better to just increase the number of model parameters.

Phase III. SGD frustrated, distortion constrained.In this phase \((2>1,2>1,>)\), SGD noise is important. The loss curve is \((r)_{ac}(r)+_{0}(r)+_{pp}(r)\). Notably, in this phase, the compute-optimal parameter is \(d^{}()=^{1/2}\), which is independent of \(\) and \(\). PLRF that fall within this phase have the same scaling law regardless of data complexity and target complexity. Moreover, the tradeoff occurs, like in Phase II, once the optimizer reaches the \(_{ac}\)-dominated part of the loss curve. Unlike in Phase II, the optimization is slowed by SGD noise (\(_{pp}\)) leading up to that point. We note that there is a dimension-independent burn-in period required for SGD noise to dominate, and for small numerical simulations, one may actually observe an \((_{pp},_{ac})\) tradeoff.

Phase IV. SGD frustrated, capacity constrained.Like Phase III, SGD noise is important. The SGD algorithm in Phase IV will be distinguished from gradient descent. As one approaches the high-dimensional line (\(2=1\)) in Phase III, the \(_{ac}(r)\) disappears. It becomes too small relative to \(_{pp}\) and \(_{pp}\). Moreover at the high-dimensional line, \(_{pp}\) becomes important again. Thus, the loss curve in Phase IV (a and b) look like \(r,d_{pp}(r,d)+_{0}(r,d)+ _{pp}(r,d)\). The distinction between Phase IVa (\(1-}<<0.5,2>1\)) and Phase IVb (\(<<1-},2>1\)) is where the compute-optimal tradeoff occurs. It changes from \(_{pp}=_{0}\) (Phase IVa) to \(_{pp}=_{pp}\) (Phase IVb). In particular it can be (Phase IVb) the SGD noise is so large that increasing the model parameter count is compute-optimal. We note that in this phase \(d\) must be taken very large (in particular larger than we could numerically attain) to get quantitative agreement between the exponents and theory.

Other observations.In Phase III, Ib, and IVa, the optimal parameter \(d^{}=^{1/2}\) (see dashed lines in Fig. 3(a)). These phases, taken together, encompass a large section of the \((,)\)-phase plane. This suggests that there is a potential universal scaling law. Moreover using 1 A100-GPU-day of compute, one reaches scales of \(d\) where the observed exponents in the scaling laws - SGD, the theoretically-derived Volterra equation eq. (10), and the equivalence of \((r)\) eq. (17) - are still changing (see Fig. 4(b) and c). This serves as a potential warning for empirically derived scaling laws. Additionally, although we have identified the lower-left of the phase diagram (\(+<1/2\)) as "no power-law", this designation relies on the assumption \(v>d\), which could be relaxed to interesting effect in more realistic (e.g. non-linear) models.

Compute-optimal learning rate and batch.Previously, we have used \(B=1\) and the maximal learning rate allowed. One can also consider finding the compute-optimal curves with respect to batch size and learning rate, i.e., find \(d^{},^{},B^{}\) such that

\[(d^{},^{},B^{})_{d,,B}(,,d)\  B<1\ \ \|_{pp}\|<1.\] (21)

In Section I, we show that \((,,d)\) is monotonic in \(B\) and therefore \(B=1\) is optimal. Similarly for \(\), in Phases I, II, III, the loss \((,,d)\) is monotonic and thus the maximally stable learning rate is optimal. For Phase IV, this is not true. There does exist an optimal \(^{}\) (with \(B=1\)),

\[^{}^{}, d^{}()^{},^{} ()^{},\]

where the tradeoff occurs between \(_{pp}(r)=_{0}(r)\) and Phase IVa and IVb collapse to a single phase. This is proven in Proposition I.1.

Conclusion.We analyze a simple three parameter model, PLRF, and derive deterministic expressions for the training dynamics (see Volterra equation (10)). We then extract compute-optimal scaling laws for large \(d\). We identify 4 phases (+3 subphases) in the \((,)\)-phase plane, corresponding to different compute-optimal curve/loss behaviors. These phase boundaries are determined by the relative importance of model capacity (Phase I, IV), poor embedding of the features (Phase II, III), and the noise produced by the SGD algorithm (Phase III, IV). The latter suggesting that another stochastic algorithm might change the compute-optimal curve; we leave this interesting direction to future research. We also show evidence of a universal scaling law which we also leave for future research to explore.