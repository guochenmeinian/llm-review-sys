# Aligning Language Models with Human Preferences

via a Bayesian Approach

Jiashuo WANG\({}^{1}\), Haozhao WANG\({}^{2}\), Shichao SUN\({}^{1}\), Wenjie LI\({}^{1}\)

\({}^{1}\)Department of Computing, The Hong Kong Polytechnic University

\({}^{2}\)School of Computer Science and Technology, Huazhong University of Science and Technology

{csjwang,csssun,cswjli}@comp.polyu.edu.hk

hz_wang@hust.edu.cn

Corresponding author.

###### Abstract

In the quest to advance human-centric natural language generation (NLG) systems, ensuring alignment between NLG models and human preferences is crucial. For this alignment, current popular methods leverage a reinforcement learning (RL) approach with a reward model trained on feedback from humans. However, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the NLG performance. To tackle this issue, previous approaches typically rely on majority voting or averaging to consolidate multiple inconsistent preferences into a merged one. Although straightforward to understand and execute, such methods suffer from an inability to capture the nuanced degrees of disaggregation among humans and may only represent a specialized subset of individuals, thereby lacking the ability to quantitatively disclose the universality of human preferences. To address this challenge, this paper proposes a novel approach, which employs a Bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as **d-PM**. Besides, considering the RL strategy's inefficient and complex training process over the training efficiency, we further propose utilizing the contrastive learning strategy to train the NLG model with the preference scores derived from the d-PM model. Extensive experiments on two human-centric NLG tasks, i.e., emotional support conversation and integrity "Rule-of-Thumb" generation, show that our method consistently exceeds previous SOTA models in both automatic and human evaluations.

## 1 Introduction

Human-centric natural language processing (NLP) aims to develop NLP systems that are finely attuned to human preferences [14; 12; 30]. Consequently, learning from human feedback is well-suited for training models in human-centric NLG tasks [18; 28]. Currently, reinforcement learning (RL) with a reward model is the most popular method to align models with human preferences [26; 11; 41]. Its effectiveness depends heavily on how well human preferences are learned by the reward model [6; 4]. However, modeling human preferences can be challenging.

Due to the high subjectivity of personal standards and human values, it can be difficult to reach a consensus on preferences among individuals, significantly increasing the learning difficulty. As depicted in Figure 1, persons may have varied preferred responses with inconsistent emotions and values given the same context. To tackle this challenge, existing methods mostly adopt aggregation techniques such as majority voting or averaging [11; 6; 4]. However, aggregated preferences potentially caterto specific subsets of people, risking generating controversial content (see Support A in Figure 1). Additionally, subjectivity and inconsistency are intrinsic components of certain tasks, such as emotion analysis  and ethical evaluation , necessitating careful consideration instead of dismissal . Therefore, for widely acceptable and less controversial outputs, it is necessary for the resulting NLG systems to account for capturing disagreements inherent in human preferences .

In this paper, we introduce a novel Bayesian-based approach termed Preference Modeling with Disagreement (**d-PM**). This method is designed to approximate a "universal preference" that comprises the preferences of "all individuals", given the preferences of several individuals. Although a soft label, derived from these several individuals, can intuitively account for disagreement, outliers or extreme labels can disproportionately influence the overall perception. Therefore, we employ Bayesian inference to refine these preferences. Specifically, the observed preference among selected individuals serves as prior knowledge. Our d-PM aims to leverage distribution of all possible universal preferences (likelihood probability) to adjust and smooth the initially observed one, leading to the derivation of a universal preference (posterior). Upon obtaining the universal preference, we calculate the likelihood of the expected preference types to establish a preference score, which is then utilized for further language model alignment.

Based on the d-PM model, we further optimize the language models to generate widely acceptable and less controversial texts. Specifically, we propose utilizing the contrastive learning strategy to calibrate the generation model towards generating texts with high preference scores provided by d-PM. Although existing RL strategies can also be leveraged to make the calibration, they are generally perceived as costly in terms of convergence  and online decoding processes . We assess our proposed method on two human-centric NLG tasks: emotional support conversations and moral integrity RoT generation. Experimental results demonstrate that our framework can be applied to state-of-the-art generation models for each task without performance degradation and meanwhile effectively increases global consensus of human preferences embedded in generated texts.

Our main contributions are three-fold: **(i)**. To the best of our knowledge, we are the first to align text generation models with human preferences while considering inherent disagreement among different individuals. **(ii)**. In order to model human preferences with their disagreement, we propose a Bayesian approach, Preference Modeling with Disagreement (d-PM). Additionally, we use its preference scores to calibrate NLG models via contrastive learning for generations that can be widely acceptable and less controversial. **(iii)**. We conduct experiments on two human-centric NLG tasks, i.e., emotional support conversations and integrity RoT generation. Experimental results demonstrate the effectiveness and versatility of our proposed method. 2

Figure 1: People can have different feelings towards the same response in the emotional support conversation because of their own experiences and values. A trustworthy human-centric system is expected to consider the benefits of universal groups, including minorities, and generate less controversial and more helpful content, like supporter B instead of A.

Framework

Aligning text generation models with human preferences requires two essential components: modeling human preferences and calibrating the text generation model.

Preference Modeling with DisagreementHuman preferences can be inferred from a human-annotated dataset, denoted as \(\). Each instance in the dataset is represented as a triplet \((c,s,l)\). Here \(c\) is a context; \(s\) is a text; and \(l\) is a label indicating the annotators' preferences. The inherent disagreement in human preferences can be encapsulated within the label in two distinct ways. In the first approach, the label can be a soft label derived from multiple annotations, all attributed to the same sentence . These annotations are sourced from multiple human annotators to preserve disagreement among individuals. The second approach is the direct collection of global consensus. In this context, the label signifies the proportion of people who find a particular sentence acceptable, an estimate provided by a single human annotator .

Aimed at capturing human preference with disagreement within the dataset, we assume there is a distribution \(\) over two classes, \(\{acceptable,unacceptable\}\), comprising preferences of all humans, and therefore \(l\) is the sampling result from \(\). We employ a preference model \(()\) to infer \(\) give \(c\) and \(s\) as inputs and the probabilistic format of \(l\) as the prior distribution. Since we focus on whether \(s\) is widely acceptable, the likelihood of the class \(acceptable\) is defined as the preference score:

\[_{(s,c)}=(s,c;)_{acceptable}.\] (1)

Calibration for AlignmentIn aligning NLG models with human preferences, we calibrate the existing generation model \((_{0})\) with preference scores. Here, \((_{0})\) stands for a model already fine-tuned on a dataset \((X,Y)\), where \(X\) and \(Y\) represent the input set and the corresponding output set, respectively, and \(_{0}\) are the optimized parameters. Significantly, if the dataset for preference modeling is identical to the \((X,Y)\) dataset, then \((x,y)(X,Y)\) corresponds to \((c,s)\). If not, these two datasets should belong to a similar domain. We initially decode \(K\) candidate sequences \(\{_{k}\}_{k=1}^{k=K}\) via \((_{k}|x;_{0})\) for each \(x X\). Then, we further train \((_{0})\), aimed at a new objective: aligning the likelihoods of candidate sequences with preference scores \(\{_{(_{k},x)}\}_{k=1}^{k=K}\).

## 3 Method

This section presents our method, whose diagram is shown in Figure 2. We first use a Bayesian approach, i.e., d-PM, to model human preferences with disagreement (Section 3.1). Then we calibrate a text generation model by contrastive learning with preference scores of d-PM to align this model with human preferences (Section 3.2).

Figure 2: Diagram for preference modeling with disagreement and calibration for alignment.

### Preference Modeling with Disagreement

We establish a distribution \(\) to represent the universal preference for the text \(s\) given its context \(c\). Therefore, the observed annotations \(l\) are considered as samples from \(\), and can form a prior distribution \(p_{i}()\). Inspired by , we devise a Bayesian approach to approximate \(\) using this prior.

Specifically, we establish a connection between \(\) and \(l\) through the optimization process of a generative model. This model is designed for generating text \(s\) conditioned on \(c_{i}\) and \(\): \(p(s|c_{i},)\). The log-likelihood of the text can be formulated as \(_{i} p(s_{i}|c_{i})=_{i}(_{}p(s_{i}|c_{i},)p_ {i}())\), where \(p_{i}()\) is the prior preference distribution. Its optimization can be achieved by introducing a variational posterior distribution \(q(|s_{i},c_{i})\) for the \(i\)-th datapoint, and minimizing the free energy (negated evidence lower bound) formulated as:

\[-_{i} p(s_{i}|c_{i})+_{i}_{}q(|s_{i},c_{i})|c_{i},)p_{i}()}{q(|s_{i},c_{i})}.\] (2)

Minimization of the free energy involves estimations of both the forward distribution of text \(s_{i}\): \(p(s_{i}|c_{i},)\), and the posteriors \(q(|s_{i},c_{i})\), which can be computed by our preference model:

\[q(|s_{i},c_{i})=(s_{i},c_{i}|).\] (3)

As for \(p(s_{i}|c_{i},)\), it is defined only on the \(i\)-th datapoint and is computed by minimizing Equation (2) for fixed \(q(|s_{i},c_{i})\), s.t., \(_{i}p(s_{i}|c_{i},)=1\) for all \(\). Thus, the optimum is achieved by:

\[p(s_{i}|c_{i},)=a_{i,}=,c_{i})}{_{j}q(|s_{j},c_{j})}.\] (4)

From Equation (4), the generative model can be regarded as a matrix of variables \(a_{i,}\) describing conditional probabilities of different responses \(s_{i}\) given different latent distributions \(\) with known \(c_{i}\).

In a variational way, Equation (2) can be rewritten as \(- p(s_{i}|c_{i})+_{i}(q(|s_{i},c_{i})\|r_{i}())\). Here, \(r_{i}() p(s_{i}|c_{i},)p_{i}()\) is the posterior model of the generative model, and it can be reformulated with reduction of \(p(s_{i}|c_{i},)\) to the matrix in Equation (4):

\[r_{i}()=_{i} p_{i}()p(s_{i}|c_{i},)=_{i}()q_{i}(|s_{i},c_{i})}{_{j}q_{j}(|s_{j},c_{j})},\] (5)

where \(_{i}\) is a scalar enabling \(_{}r_{i}()=1\). Accordingly, we can minimize the KL divergence between \(q(|s_{i},c_{i})\) and \(r_{i}()\) to minimize the free energy. The minimization of the free energy in Equation (2) can be derived as:

\[_{i}(q(|s_{i},c_{i})\|r_{i}())=_{}_{i} (s_{i},c_{i};)_{i} p_{i} ()(s_{i},c_{i};)}{_{j}(s_{j},c_{j} ;)}.\] (6)

By optimizing the above objective, we can optimize the parameters of our preference model, i.e., \(\).

### Calibration for Alignment

It is nearly possible for \((_{0})\) to generate texts with both high and low preference scores, shown in Figure 3. However, we expect to calibrate the model such that the generation probability aligns with these preference scores. Specifically, we use diverse beam search  to generate multiple candidates and then use our d-PM to evaluate these candidates. For the sake of more likely generating a high preference score text, we propose a model-agnostic module to leverage contrastive learning to calibrate generation likelihood aligning with d-PM. Taking inspiration from recent calibration work [32; 23; 39], we implement this module through the following three steps:

Step 1: Candidates Generation.We generate candidates from the text generator \((_{0})\), which has been fine-tuned on corresponding dataset \((X,Y)\) and its parameters are \(_{0}\), on its own training dataset. Given an input sequence \(x X\), we first use \((_{0})\) to generate \(K\) candidates \(\{_{1},_{2},,_{K}\}\) using diverse beam search. As a result, these candidates will get similar possibilities yet different preference scores according to the above preliminary study.

Step 2: Preference-based Ranking.We use our proposed d-PM \(()\) to measure the preference score \(_{(_{k},x)}\) of each candidate \(_{k}\). Then we rank these candidates according to the above preference score and obtain a list of ranked candidates: \(^{{}^{}}_{1},^{{}^{}}_{2},,^{{}^{ }}_{K}\), where \(_{(^{{}^{}}_{i},x)}>_{(^{{}^{ }}_{j},x)}\) for \(\ i<j\).

Step 3: Likelihood Calibration.As mentioned before, we leverage contrastive learning to assign higher likelihoods to the candidates with higher preference scores. The following pairwise margin loss is used to adjust the generator \(()\).

\[^{r}=_{i}_{j>i}max(0,(^{{}^{}}_{j} ;)-(^{{}^{}}_{i};)+_{ij}),\] (7)

where \(_{ij}\) is the default margin \(\) multiplied by the difference in rank between the samples, i.e., \(_{ij}=*(j-i)\). \((^{{}^{}}_{i};)\) is the length-normalized log-probability of the candidate:

\[(^{{}^{}};)=^{|^{{}^{ }}|}(^{{}^{}}_{t}|x,^{{}^{ }}_{<t};)}{|^{{}^{}}|^{}},\] (8)

where \(\) is the length penalty hyperparameter. To avoid forgetting token-level likelihood information of the ground-truth text, we also use an additional token-level negative log-likelihood. The final calibration loss is as follows:

\[^{c}=-_{t=1}^{|y|}(y_{t}|x, y_{<t};)+^{r}.\] (9)

We minimize \(^{c}\) to optimize the generator's parameters \(\). This process is supervised by our d-PM model and aligns the generation model with human preference.

## 4 Experiments

### Emotional Support Conversation

In an emotional support conversation, a supporter aims to buffer a help-seeker's emotional distress and help the help-seeker to change the difficult situation . In this context, the model functions as the supporter, while the user is always the help-seeker. Due to different personal experiences, different help-seekers may respond with varied feelings and reactions to the same response, as illustrated in Figure 1. Our objective is to enhance the model's ability to generate responses that will not escalate the negative feelings of a diverse range of help-seekers.

Dataset and Base ModelsThe benchmark ESConv , containing approximately \(1\)k conversations with \(31\)k utterances, develops each conversation between a help-seeker and a supporter. We include BlenderBot-Vanilla and BlenderBot-Joint proposed in conjunction with the dataset, and the SOTA model MultiESC. We reproduced the base models in accordance with their respective papers and publicly available codes.

Human Preferences with DisagreementWe derive human preferences from the Motivational-Interviewing-Dataset . This dataset encompasses around \(17\)k supporter responses to help-seekers. Each response is annotated by \(2 4\) experts following the MI codes. The labels can be induced into two classes \(\{acceptable,unacceptable\}\), and the human preferences with disagreement can be estimated by our d-PM method. By fine-tuning a BERT model on this dataset using prefix-tuning [37; 19], we obtain the d-PM. Additional details can be found in Appendix B.2.

Figure 3: The maximum and minimum preference scores of \(10\) candidates generated via diverse beam search given the same context. We test on \(1000\) data instances and three emotional support conversation models.

Experimental SetupWe apply our proposed method to align each base model, thus treating the well-trained base model as the generator \((_{0})\). Additionally, to validate the effectiveness of d-PM, we employ three alternative preference models within our framework for comparative analysis:

1. A preference model (major) trained to predict the majority voting result of annotations from different annotators, denoted as \(l_{m}\), and optimized by cross-entropy loss, formulated as: \[()=-_{(c,s,l_{m})}[p_{l}(l_{m})( ^{l_{m}}(c,s;))].\] (10) where \(p_{l}(l_{m})\) denotes the one-hot vector of \(l_{m}\).
2. A preference model (soft) trained to approximate the direct probabilistic label of annotations, i.e., the soft label \(l\). The model is optimized by: \[()=_{(c,s,l)}[}(c,s; )-l]^{2}.\] (11)
3. A preference model (w/oA) that does not aggregate annotations and takes each annotation as independent. This model is optimized by cross-entropy loss, similar to Equation (10).

When training the aligned models, we aim to retain the same hyperparameters used in the training of the base models. We set the candidate number \(K\) to \(10\). We train each aligned model five times with five different seeds. Subsequently, we test each of the five trained models on the test dataset and compute the average results.

Automatic EvaluationWe adopt the following metrics commonly used in previous work [5; 22] for the automatic evaluation of our proposed method: BLEU  (B-1/2/3/4), ROUGE (R-L) , METEOR , CIDEr , and BOW Embedding-based matching score  (Extreme). Results are shown in Table 1.

Our Aligned\({}_{}\) significantly improves the performance of the base model in almost all automatic metrics, irrespective of the base model, suggesting the overall effectiveness of our proposed method. Alignedmajor and Alignedsoft are able to enhance the performance when the base model is either Blender-Vanilla or Blender-Joint, however, they do not yield an improvement when the base model is MultiESC. This limitation underscores the constraints inherent in using majority voting labels and soft labels to address disagreement of human preferences. Aligned\({}_{}\) surpasses the base model in certain metrics but falls short in others. Notably, it performs significantly lower in CIDEr, a metric evaluating the similarity between TFIDF-weighted n-grams. This shortfall suggests that Alignedw/oA is less likely to generate responses containing critical information found in the ground truth. This issue arises from the preference scores determined by w/oA being closely clustered in value, resulting in its inability to sequence the generated samples logically. These pieces of evidence indicate the potency of our proposed preference model d-PM.

   & **B-1** & **B-2** & **B-3** & **B-4** & **R-L** & **METEOR** & **CIDEr** & **Extreme** \\   & Base & 17.85 & 7.08 & 3.60 & 2.11 & 17.06 & 7.46 & 15.44 & **51.02** \\  & Aligned\({}_{}\) & 19.07 & 7.71 & 3.94 & 2.28 & 17.09 & 7.71 & 15.97 & 50.61 \\  & Aligned\({}_{}\) & 17.88 & 7.21 & 3.68 & 2.12 & 16.52 & 7.31 & 15.50 & 50.73 \\  & Aligned\({}_{}\) & 19.70 & 7.56 & 3.64 & 2.05 & 16.90 & 7.72 & 15.62 & 50.48 \\  & Aligned\({}_{}\) & **20.75** & **8.32** & **4.17** & **2.39** & **17.41** & **8.21** & **16.57** & 50.38 \\   & Base & 18.70 & 7.30 & 3.61 & 2.03 & 17.66 & 7.56 & 16.91 & 50.95 \\  & Aligned\({}_{}\) & 20.37 & 8.61 & 4.47 & 2.65 & 19.23 & 8.32 & **21.86** & 51.57 \\  & Aligned\({}_{}\) & 19.36 & 7.87 & 3.85 & 2.09 & 17.55 & 7.65 & 15.90 & 50.84 \\  & Aligned\({}_{}\) & 21.05 & 8.14 & 3.89 & 2.07 & 17.65 & 8.11 & 15.29 & 50.68 \\  & Aligned\({}_{}\) & **21.05** & **8.97** & **4.74** & **2.78** & **19.39** & **8.48** & 20.34 & **51.81** \\   & Base & 20.36 & 8.80 & 4.92 & 3.14 & 21.00 & 8.58 & 30.69 & 52.74 \\  & Aligned\({}_{}\) & 19.10 & 8.27 & 4.61 & 2.88 & 20.72 & 8.24 & 30.15 & 52.57 \\  & Aligned\({}_{}\) & 19.30 & 8.33 & 4.62 & 2.88 & 20.83 & 8.35 & 30.75 & 52.54 \\  & Aligned\({}_{}\) & 21.58 & 8.80 & 4.74 & 2.96 & 20.47 & 8.78 & 28.58 & 51.65 \\  & Aligned\({}_{}\) & **21.59** & **9.56** & **5.33** & **3.36** & **21.50** & **9.03** & **32.65** & **53.15** \\  

Table 1: Automatic evaluation results on ESConv. All results are significantly better than the corresponding base model with \(p<0.01\).

Human RatingsWe ask human annotators to evaluate the generations of models based on MultiESC since MultiESC can outperform Blender-Vanilla and Blender-Joint in almost all automatic evaluations. Specifically, we randomly sample \(100\) responses generated by different models for human ratings. We asked annotators to imagine they are help-seekers in the corresponding situation and measure each response in five aspects: (1). _Identification_: on a scale of \(1 5\), how much the response can explore your situation in depth and help identify the problems. (2). _Converting_: on a scale of \(1 5\), how skillful the response can comfort you. (3). _Suggestion_: on a scale of \(1 5\), how helpful the response can solve your problems. (4). _Overall_: on a scale of \(1 5\), the overall quality of this response for emotional support; (5) _Global Consensus_: the number of people who deem the response can help them, \(1 5\) represent nobody (\(<1\%\)), rare (\(5\% 25\%\)), controversial (\( 50\%\)), most (\(75\% 90\%\)), and all (\(>99\%\)), respectively. Each response is annotated by three annotators, and we averaged these three annotations as the final result for each metric.

From Table 2, our method performs the best among the methods. Aligned\({}_{}\) obtained the highest score in all aspects, including the global consensus. It demonstrates that our method can generate less controversial and more helpful responses in the task of emotional support conversation.

### Integrity RoT Generation

We also apply our method to the integrity "Rule-of-Thumb" (RoT) generation task. This task is concerned with describing a chatbot's normative rules, which holds great potential for advancing research on morally-consistent conversational agents . When it comes to outlining a chatbot's normative rules, people's values can vary widely. However, morally-consistent conversational agents are expected to accommodate the values of as many individuals as possible. Therefore, the generation of widely acceptable RoTs is crucial for guiding the behavior of these agents.

Dataset and Base ModelsThe MIC dataset  comprises about \(99\)k distinct RoTs that encapsulate the moral assumptions inherent in \(38\)k machine-generated replies to open-ended prompts. Each prompt is associated with three different RoTs, each provided by a distinct annotator. Alongside each RoT, annotators offer a "global consensus" value, \(\), which signifies the estimated proportion of the global population that would agree with the RoT. We utilize T5 (small), Flan-T5 (base) and BART (large) models as our base models, and fine-tune them on the MIC dataset. For model inference, we closely follow the processes presented in . Specifically, we adopt three decoding strategies: greedy decoding, beam search (\(n=3\)), and nucleus sampling (\(p=0.9\)). We generate one RoT for greedy decoding; for the latter two, three hypotheses are generated and the highest-scoring one is selected.

Human Preferences with DisagreementWe learn human preferences with disagreement for normative rules from the MIC dataset. The open-ended prompts are treated as context, and the ground truth RoT is considered the text to be evaluated. A probabilistic label is assigned to each RoT based on its global consensus: the probability for the class _acceptable_ is \(\) while that for \(unacceptable\) is (\(1-\)). We utilize this dataset to train the d-PM; details can be found in Appendix B.2.

Experimental SetupWe apply our proposed framework to align each base model that has been rigorously fine-tuned on the MIC dataset. To assess the effectiveness of d-PM, we also train a preference model (soft) by minimizing the loss computed using Equation (11). The number of candidates \(K\) is set to \(5\). We adopt the same hyperparameters for training each aligned model as are used for the base model. We conduct five training runs for each aligned model using five distinct seeds. Then, we evaluate each of the five trained models on the test dataset and calculate the average results.

 
**Model** & **Identification** & **Comforting** & **Suggestion** & **Overall** & **Global Consensus** \\  Base & 3.017 & 2.562 & 2.918 & 2.598 & 2.693 \\ Aligned\({}_{}\) & 3.032 & 2.572 & 2.880 & 2.598 & 2.763 \\ Aligned\({}_{}\) & 3.007 & 2.557 & 2.905 & 2.568 & 2.747 \\ Aligned\({}_{}\) & **3.052** & **2.587** & **2.952** & **2.637** & **2.783** \\  

Table 2: Human evaluation results on ESConv. The base model is MultiESC.

Automatic EvaluationIn accordance with previous work , we report standard ROUGE  (R-1/2/L), ScareBLEU , BERTScore , and average generation length (Avg. Len) metrics. As each prompt-reply pair in our dataset has three ground truth RoTs, we compute each metric by taking the maximum score from these three, following the method employed by . The results are displayed in Table 3.

The results clearly show that Alignedd-PM generally outperforms its base model. In addition, Alignedd-PM achieves the highest score across all evaluation metrics, except the generation length, when employing a beam decoding strategy. While Alignedsoft slightly improves performance with T5 (small) as the base model, a minor decline is observed when the base model is either Flan-T5 (base) or BART (large). Interestingly, the enhancements observed in this task are not as pronounced as those witnessed in the emotional support conversation task (refer to Table 1). This may be attributed to the MIC dataset inherently accounting for disagreement, as each prompt is paired with three RoTs from different annotators. This enables base models, when fine-tuned on this dataset, to encapsulate various human preferences to a certain degree. Nonetheless, our framework has the potential to further boost model performance by providing more explicit preference information with disagreement during training.

   &  &  &  &  \\  Base & 0.528 & 2.547 & 2.037 & 2.428 \\ Aligned\_soft & 0.550 & **2.602** & 2.028 & 2.502 \\ Aligned\_rPM & **0.568** & **2.602** & **2.103** & **2.555** \\  

Table 4: Human evaluation results on MIC. The base model is BART-large (beam).

   T5 (Small) \\  }} &  &  &  &  &  &  &  \\   T5 (Small) \\  } &  & Base & 53.44 & **32.97** & **52.17** & 93.44 & **29.05** & 8.94 \\  & & Aligned\_soft & 52.14 & 31.48 & 50.79 & 93.34 & 27.05 & 8.85 \\  & & Aligned\_rPM & **53.45** & 32.82 & 52.09 & **93.49\({}^{}\)** & 28.51 & **8.99** \\   &  & Base & 37.04 & 16.30 & 35.27 & 90.94 & 14.27 & **10.47** \\  & & Aligned\_soft & 37.77\({}^{}\) & 16.82\({}^{}\) & 35.97\({}^{}\) & 91.21\({}^{}\) & 14.68\({}^{}\) & 9.83 \\  & & Aligned\_rPM & **38.15\({}^{}\)** & **17.22\({}^{}\)** & **36.40\({}^{}\)** & **91.29\({}^{}\)** & **15.15\({}^{}\)** & 9.74 \\   &  & Base & 40.22 & 19.23 & 38.56 & 91.59 & 16.71 & **9.85** \\  & & Aligned\_soft & 40.90\({}^{}\) & 19.70\({}^{}\) & 39.24\({}^{}\) & 91.79\({}^{}\) & 16.98\({}^{}\) & 9.47 \\  & & Aligned\_rPM & **41.41\({}^{}\)** & **20.22\({}^{}\)** & **39.75\({}^{}\)** & **91.90\({}^{}\)** & **17.75\({}^{}\)** & 9.38 \\   Flan-T5 (Base) \\  } &  & Base & 55.07 & 34.96 & 53.74 & 93.77 & 30.68 & 9.00 \\  & & Aligned\_soft & 54.82 & 34.65 & 53.49 & 93.75 & 30.34 & 9.00 \\  & & Aligned\_rPM & **55.18\({}^{}\)** & **35.07\({}^{}\)** & **53.86\({}^{}\)** & **93.79\({}^{}\)** & **30.83\({}^{}\)** & **9.01** \\   &  & Base & 37.94 & 17.23 & 36.13 & 91.39 & 15.36 & **9.78** \\  & & Aligned\_soft & 37.84 & 17.03 & 36.00 & 91.38 & 15.12 & 9.75 \\  & & Aligned\_rPM & **38.34\({}^{}\)** & **17.52** & **36.53\({}^{}\)** & **91.44\({}^{}\)** & **15.49** & 9.77 \\   &  & Base & 41.41 & 20.41 & 39.70 & 92.02 & 18.02 & 9.30 \\  & & Aligned\_soft & 41.44 & 20.33 & 39.71 & 92.02 & 17.91 & 9.29 \\  & & Aligned\_rPM & **41.78\({}^{}\)** & **20.69\({}^{}\)** & **40.09\({}^{}\)** & **92.07\({}^{}\)** & **18.26\({}^{}\)** & **9.32** \\   BART (Large) \\  } &  & Base & 54.81 & 35.07 & 53.35 & 93.85 & 30.80 & **9.44** \\  & & Aligned\_soft & 54.82 & 34.85 & 53.36 & 93.82 & 30.35 & 9.36 \\   & & Aligned\_rPM & **55.05\({}^{}\)** & **35.18** & **53.62** & **93.86\({}^{}\)** & **30.85** & 9.40 \\    &  & Base & 54.77 & 34.85 & 53.30 & **93.84** & **30.51** & **9.54** \\   & & Aligned\_rPM & 54.54 & 34.53 & 53.10 & 93.80 & 30.01 & 9.47 \\    &  & Base & **54.81** & **34.86** & **53.39** & 93.83 & **30.51** & 9.48 \\    &  & Base & 54.77 & 34.96 & 53.32 & 93.84 & 30.62 & **9.56** \\   & & Aligned\_rPM & 54.65 & 34.68 & 53.23 & 93.82 & 30.16 & 9.45 \\    & \)} & Aligned\_rPM & **54.86** & **35.01** & **53.45** & **93.85\({}^{}\)** & **30.63** & 9.48 \\  

Table 3: Automatic evaluation results on MIC. \(\) represents significantly better than the corresponding base model with \(p<0.01\).

Human RatingWe randomly select \(100\) replies generated by models with BART (large) as the base model for human evaluation. Adhering to previous practice, we assess generated outputs based on the following criteria : (1). _Well-formedness_: yes or no, does the RoT explain the basics of good or bad behavior with a single judgment or action?; (2). _Fluency_: on a scale of 1-5, how much does the RoT align with what an English speaker might naturally say?; and (3) _Relevance_: on a scale of 1-5, how well does the RoT apply to the Answer for this specific Question if we assume the RoT is true? Furthermore, we request annotators to provide a _Global Consensus_: how many people globally will agree with this RoT, similar to the method described in Section 4.1. Three annotators evaluate each RoT, and we average these three evaluations as the final score for each metric.

Results presented in Table 4 indicate that our method produces RoTs that are more universally agreeable than those generated by the other two models. Our Alignd-PM model improves all metrics over the base model and outperforms Alignedsoft.

## 5 Model Analysis

The effect of candidate number \(K\) during calibration.To examine the influence of varying the candidate number \(K\) on model calibration, we modify the MultiESC calibration process using different candidate numbers, namely \(5\), \(10\), \(15\), and \(20\). Specifically, this involves changing the beam widths in the diverse beam search process. Theoretically, a more significant candidate number would encompass more samples under consideration, thereby escalating the upper bound of performance. Nevertheless, as depicted in Figure 4, the model performance initially experiences an augmentation yet subsequently exhibits a reduction with the increment of the variable \(K\). It is because an overly large candidate number could introduce redundant samples with minor differences, and the generation model might erroneously distinguish between them.

Figure 4: Model performances with different candidate numbers \(K\) when calibrating MultiESC with preference scores of d-PM.

Figure 5: Comparison between alignment with RL (RL) and our model (Ours). Left: Automatic evaluation results (#(Samples)/s indicates the number of trained samples per second). Right: Training loss according to training steps.

Contrastive Learning vs. Reinforcement Learning.We opt for contrastive learning to align generation models with human preferences instead of the currently prevalent reinforcement learning (RL) approach. This decision is rooted in several considerations. Firstly, RL requires expensive online decoding procedures, while our framework based on contrastive learning is a one-time offline process . Secondly, RL usually yields a slow convergence speed . To validate this, we align MultiESC using RL. From Figure 5, RL trains fewer samples than our framework per second. After the same steps, the loss of RL is still high, and the model performance is much worse than ours.

## 6 Related Work

Developing human-centric systems, which ensure that human stakeholders benefit from system outcomes , remains a great challenge. To build a human-centric system, it is critical to align models with human preferences . There are various methods to implement the alignment. Currently, the most popular and well-known method is reinforcement learning from human feedback, thanks to the GPT series . This method is also used for text summarization [31; 4], detoxification , and machine translation [15; 16]. The above-mentioned methods adopt one reward model, while some methods combine rewards computed by different reward models to consider fine-grained aspects of human needs [8; 11]. Moreover, some models use human feedback as the supervision signal directly to learn human preferences, such as fine-tuning pre-trained models with well-established datasets . Human feedback can also be used to augment prompts for better performance .

## 7 Conclusion and Future Work

In this work, we strive to align models with human preferences to foster the development of trustworthy human-centric NLG systems. Unlike previous approaches, we take inherent disagreement into account for modeling human preferences. This idea is motivated by two compelling reasons. Firstly, it is impractical to expect consensus in human preferences due to the high degree of subjectivity involved. Secondly, harmonizing preference disagreements can inadvertently disadvantage minority groups. Accordingly, we introduce a Bayesian approach, termed Preference Modeling with Disagreement (short as **d-PM**), to capture the subtleties of disagreement from limited human feedback. We subsequently utilize its preference scores to calibrate pre-existing text generation models. The efficacy of our method is substantiated through experiments in emotional support conversation and integrity Rule-of-Thumb generation.

Despite our focus on disagreement, another critical aspect remains to consider when modeling human preferences. In this work, like most previous studies, we assumed a linear relationship between the preference score and the proportion of the global population that finds the text acceptable. However, this assumption may not always hold true. For example, a sentence that \(20\%\) of people find helpful should ideally have a preference score closer to \(0\) instead of \(0.2\). We believe that this issue merits further exploration in future research. Fortunately, in this study, we sidestepped this problem by using the rank of preference scores rather than the scores themselves.

## 8 Acknowledgements

This work is supported by Research Grants Council of Hong Kong (PolyU/5204018, PolyU/15207920, PolyU/15213323) and National Natural Science Foundation of China (62302184, 62076212).