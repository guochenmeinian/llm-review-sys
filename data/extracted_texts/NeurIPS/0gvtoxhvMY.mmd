# Rethinking Semi-Supervised Imbalanced Node Classification from Bias-Variance Decomposition

Divin Yan\({}^{}\), Gengchen Wei\({}^{}\), Chen Yang\({}^{}\), Shengzhong Zhang\({}^{}\), Zengfeng Huang\({}^{}\)

\({}^{}\)Fudan University, {yanl21, gcwei22, yanc22}@m.fudan.edu.cn

{szzhang17, huangzf}@fudan.edu.cn

Corresponding Author.

###### Abstract

This paper introduces a new approach to address the issue of class imbalance in graph neural networks (GNNs) for learning on graph-structured data. Our approach integrates imbalanced node classification and Bias-Variance Decomposition, establishing a theoretical framework that closely relates data imbalance to model variance. We also leverage graph augmentation technique to estimate the variance, and design a regularization term to alleviate the impact of imbalance. Exhaustive tests are conducted on multiple benchmarks, including naturally imbalanced datasets and public-split class-imbalanced datasets, demonstrating that our approach outperforms state-of-the-art methods in various imbalanced scenarios. This work provides a novel theoretical perspective for addressing the problem of imbalanced node classification in GNNs.

## 1 Introduction

Graphs are ubiquitous in the real world, encompassing social networks, financial networks, chemical molecules  and so on. Recently, graph neural networks (GNNs)  have shown exceptional proficiency in representation learning on graphs, facilitating their broad application in various fields. Nevertheless, the process of learning on graphs, analogous to its counterparts in computer vision and natural language processing, frequently encounters significant obstacles in the form of skewed or insufficient data, leading to the prevalent issue of class imbalance. It is undeniable that real-world data often exhibits biases and numerous categories, which entail data limitations and distribution shifts to be common. Furthermore, coupled with the fact that GNNs often feature shallow architectures to prevent "over-smoothing" or "information loss", training with such datasets leads to severe over-fitting problems in minor classes.

Designing GNN imbalanced learning schemes for graph-structured data poses unique challenges, unlike image data. Graph-structured data often requires consideration of the data's topology and environment, and empirical evidence  shows that topological asymmetries can also affect the model's performance. However, due to the extremely irregular and structured nature of the data, quantifying and solving topological asymmetry is difficult and computationally intensive. As a result, existing methods such as oversampling  and loss function engineering  are problematic for achieving satisfactory outcomes. In depth, the modeling approach for data personalization exhibits very poor scalability and generalization capability. Furthermore, the prevalence of this problem has prompted the community to seek a more effective framework for imbalanced learning. _Thus, a more fundamental and theoretical perspective is urgently needed when considering the imbalance node classification problem._Following the idea, in this work, we propose a novel viewpoint to understand graph imbalance through the lens of _Bias-Variance Decomposition_. The Bias-Variance Decomposition [1; 2; 23; 45] has long been applied in terms of model complexity and fitting capacity. Our theoretical analyses have confirmed the relationship between model variance and the degree of dataset imbalance, whereby an imbalanced training set can lead to poor prediction accuracy due to the resulting increase in variance. Furthermore, we conducted some experiments on real-world dataset and the results demonstrate a significant relationship between the variance and the imbalance ratio of the dataset. As far as we know, we were the first to establish a connection between imbalanced node classification and model variance, which conducts theoretical analysis in this field.

Moreover, we have devised a regularization term for approximating the variance of model, drawing on our theoretical analysis. The main challenge of this idea lies in estimating the expectation across different training sets. Our key insight is to leverage graph data augmentation to model the different training sets in the distribution, which has not been explored before. Our empirical evaluations have consistently demonstrated that our algorithm, leveraging three basic GNN models [16; 35; 9], yields markedly superior performance in diverse imbalanced data settings, surpassing the current state-of-the-art methods by a substantial margin. Notably, we have conducted meticulous experiments on two naturally imbalanced datasets, which serve as the realistic and representative benchmarks for real-world scenarios.

Our contribution can be succinctly summarized as follows: **(i)** We are the first to integrate imbalanced node classification and _Bias-Variance Decomposition_. Our work establishes the close relationship between data imbalance and model variance, based on theoretical analysis. **(ii)** Moreover, we are the first to conduct a detailed theoretical analysis for imbalanced node classification domain. **(iii)** Our principal insight is to leverage graph data augmentation as a means of representing the varied training sets that lie within the distribution, whilst simultaneously designing a regularization term that approximates the model's variance. **(iv)** We conduct exhaustive tests on multiple benchmarks, such as the naturally imbalanced datasets and the public-split class-imbalanced datasets. The numerical results demonstrate that our model consistently outperforms state-of-the-art machine learning methods and significantly outperforms them in heavily-imbalanced scenarios.

## 2 Preliminaries

### Notations

We concentrates on the task of semi-supervised imbalanced node classification within an undirected and unweighted graph, denoted as \(=(,,})\). \(\) represents the node set, \(\) stands for the edge set, and \(}\) denotes the set of labeled nodes. The set of unlabeled nodes is denoted as \(}:=}\). The feature matrix is \(^{n f}\), where \(n\) is the number of nodes and \(f\) is the feature dimension. The adjacency matrix is denoted by \(\{0,1\}^{n n}\). Let \((v)\) represent set of adjacent nodes to node \(v\). The labeled sets for each class are denoted as \((_{1},_{2},,_{k})\), where \(_{i}\) represents the labeled set for class \(i\). The imbalance ratio \(\), is defined as \(=_{i}|_{i}|/_{i}|_{i}|\).

### Graph Dataset Augmentations

To craft diversified perspectives of the graph dataset, we deploy advanced graph augmentation methodologies, leading to the formation of \(}=(},})\) and \(^{}}=(}^{},}^{ })\). Elements such as node attributes and connections within the foundational graph are selectively obfuscated. These augmented perspectives are subsequently channeled through a common Graph Neural Network (GNN) encoder, symbolized as \(f_{}:^{n n}^{n f}^{n d}\), in order to distill compact node-centric embeddings: \(f_{}(},})=^{n  d}\) and \(f_{}(}^{},}^{})=^ {}^{n d}\).

### Related Work

Semi-Supervised Imbalanced Node Classification.Numerous innovative approaches[28; 40; 50; 19; 25; 6; 24; 31; 43] have been proposed to tackle the difficulties arising from imbalanced node classification on graph data. GraphSMOTE  employs the SMOTE  algorithm to perform interpolation at the low dimensional embedding space to synthesize the minority nodes, while ImGAGN  introduces GAN  for the same purpose. Another work GraphENS  synthesizesmixed nodes by combining the ego network of minor nodes with target nodes. To address topology imbalance in imbalanced node classification task, ReNode  adjusts node weights based on their proximity to class boundaries. TAM  leverages local topology that automatically adapts the margins of minor nodes. Due to spatial constraints, we provide a comprehensive exposition of other relevant literature in Appendix A.

## 3 Theory

### Theoretical Motivation

This section presents a succinct overview of the classical Bias-variance Decomposition [1; 2; 23; 45] and its application to semi-supervised node classification. Furthermore, we illustrate the influence of imbalance on classification results, which subsequently amplifies the variance.

Bias-variance Decomposition.Let \(x X\) and \(y Y\) denote the input and label, respectively. Consider an underlying mapping \(f:X Y\) and the label can be expressed as \(y=f(x)+\), where \(\) is some noise. Given a training set \(D=(x_{i},y_{i})\), we train a model \((x)=(x;D)\) using the samples in \(D\).

**Definition 1**: _Bias-Variance Decomposition is that the expected predicted error of \(f\) can be decomposed into Eq(1),_

\[_{D,x}[(y-(x;D))^{2}]=([(x)])^{2}+_{D}[(x;D)]+\; , \]

_where \([(x)]=_{x}[(x;D)-f(x)]=_{D}[ (x;D)]-[y(x)]\), and \(_{D}[(x;D)]=_{D}[(_{D}[(x;D)]-(x;D))^{2}].\)_

It is important to note that this expectation is taken with respect to both the training sets \(D\) and the predicted data \(x\). The bias term reflects the model's ability to fit the given data, while the variance term indicates the stability of the model's results with different training sets. In other words, the variance describes the model's generalization performance.

Next, we consider the specific formulation of variance in the setting of semi-supervised node classification on graph. First, we make some assumptions to simplify the analysis.

Assumptions.We make two assumptions in our approach. Firstly, we assume that the node embeddings \(h^{i}\) of node \(x^{i}\) extracted by a graph neural network for nodes belonging to class \(i\) follow a multivariate normal distribution \(h^{i} N(^{i},^{i})\), where \(^{i}\) is a diagonal matrix for all \(i=1,2,,c\). Here, let \(^{i}=h^{i}-^{i}\) which follows the distribution \(^{i} N(0,^{i})\).

Secondly, we consider a simple classifier that estimates the probability of a node belonging to a particular class based the distance \(h^{T}C^{i}\). Here, \(C^{i}=C^{i}(D)=}(h^{i}_{1}++h^{i}_{n_{i}})\) is the average of labeled node embedding in training set \(D\), where \(n_{i}\) is the number of nodes in class \(i\). \(C^{i}\) follows the distribution \(C^{i} N(^{i},}^{i})\). Similarly, we denote \(e^{i}=C^{i}-^{i}\) and it follows that \(e^{i} N(0,}^{i})\).

Variance for Semi-supervised Node Classification.Under the above assumption, we can write the variance generated by different sampling training set explicitly. For a node \(x\) that belongs to class \(j\), the variance can be written:

\[(x) =_{i=1}^{c}_{D}[(h^{T}(x)C^{i}-_{D}[h^{T}(x)C^{i}])^{2}] \] \[=_{i=1}^{c}_{e^{i}}[(h^{T}(x)(^{i}+e ^{i})-h^{T}(x)^{i})^{2}]\] \[=_{i=1}^{c}_{e^{i}}[(h^{T}(x)e^{i}) ^{2}]\;=_{i=1}^{c}}h^{T}(x)^{i}h(x)\]The last equation is derived from the variance of multivariate normal distribution. The variance over the whole graph \(_{i=1}^{c}_{x}[(x)]=_{i=1}^{c}_ {x}[}h^{T}(x)^{i}h(x)]\) is the expectation of Equation 2 on node \(x\).

Variance and Imbalance.In Equation 2, we notice that the variance of a specific class \(i\) is proportional to \(}\). This reveals a relation between _imbalance_ and variance. If we assume \(_{x}[h^{T}(x)^{i}h(x)]\) is the same for different \(i\), then the following theorem holds:

**Theorem 1**: _Under the condition that \(_{i}n_{i}\) is a constant, the variance \(_{i=1}^{c}_{x}[}h^{T}(x)^{i}h(x)]\) reach its minimum when all \(n_{i}\) equal._

The proof is include in Appendix B.1. As the ratio of imbalance increases, the minority class exhibits a smaller sample size \(n_{i}\), which consequently makes a greater contribution to the overall variance. This result gives an new perspective of explanation about why the model shows poor performance under imbalance training, that variance will increase as the training set becomes imbalanced. To substantiate our analysis, we conducted an experimental study to show the relation between imbalance and the variance. The result is shown in Figure 1.

### Variance Regularization

Optimize Variance during Training.Both our analysis and the experimental results show that the increase of variance is a reason for the deterioration of performance for imbalance training set. Therefore, We propose to estimate the variance and use it as a regularization during training. We decompose the model into two parts. A GNN \(E\) is utilized as a feature extractor, and a classifier \(P\) predict the probability based on the feature extracted by GNN. We define the _variance of \(P\)_ conditioning on \(E\) of as Equation 2 evaluated for a fixed GNN \(E\). Then we can take this as regularization. By optimizing this term, we allow the model to automatically search for the feature extractor that yields the smallest variance during the optimization process.

Estimate the Expectation with Labeled Nodes. The most challenging aspect lies in the estimation of variance, as we lack access to other training sets. Therefore, we propose Lemma 1 to estimate the variance on training set with the variance on labeled data. The proof is included in Appendix B.2.1.

**Lemma 1**: _Under the above assumption for \(h^{i} N(^{i},^{i})\), \(C^{i} N(^{i},}^{i})\), minimizing the \(_{i=1}^{c}_{x}[(x)]\) is equivalent to minimizing Equation 3:_

\[_{x G}_{i=1}^{c}(h(x)^{T}}} (h_{1}^{i}-h_{2}^{i}))^{2}=_{k=1}^{n_{j}} _{j=1}^{c}_{i=1}^{c}((_{k}^{j}+_{k}^{j})^{T}}}(_{1}^{i}-_{2}^{i}))^{2}. \]

Figure 1: We examine the alteration in variance concerning node classification as the imbalance ratio increases on and plot the regression curves for variance and imbalance ratio. We conduct this experiment using a fixed number of training set nodes but different ones, to mitigate the influence of the number of training set nodes on variance. Detailed experimental setup is in Appendix D.2.

Estimate the Expectation with Unlabeled Nodes.Lemma 1 allows us to replace the sampling on training set with sampling on labeled node pairs \((h_{1}^{i},h_{2}^{i})\) of class \(i\). However, to evaluate Equation 3 it still needs to have access to embedding pair \((h_{1}^{i},h_{2}^{i})\) that belong to the same class \(i\). Since labels are scarce, such node pairs is difficult to obtain. To make our algorithm more practical, a way of utilizing unlabeled nodes for estimating Equation 3 is required. We accomplish this goal through two steps. In the first step, we use graph augmentation to obtain pseudo embedding pair \((h_{1},h_{2})\). Graph augmentation is a typical used technique in graph contrastive learning. It generate new view \(G^{}\) of a graph \(G\) by adding noise to the graph structure. For a node \(x\) in two different views \(G,G^{}\), the resulting embedding \(h,h^{}\) can be seen as that of two different nodes. Thus, they can be used to replace true embedding pair in Equation 3.

However, these pseudo node pairs give no information about which class it belongs to. Thus it prohibits us from assign proper \(}}\) for different \(i\) in Equation 3. It is such coefficients that compensate on the minority class, so it's crucial to reintroduce the message about class number for constructing variance regularization. Therefore, in the second step, we use the class center \(C^{i}\) to replace \(h\) in Equation 3, as shown in Equation 4. Since \(C^{i}=^{i}+e^{i}\) and the variance of \(e^{i}\) is proportional to \(}\), using Equation 4 to estimate Lemma 1 can apply the similar compensatory for the minority class. More mathematical details are presented in Appendix B.2.1.

\[&_{x G}_{i=1}^{c}(C^{i})^ {T}h(x)-(C^{i^{}})^{T}h^{}(x)^{2}\\ &=_{k=1}^{n_{j}}_{j=1}^{c}_{i=1}^{c} (^{i})^{T}(_{k}^{j}-_{k}^{j^{}})+(e^{i})^{T}_ {k}^{j}-(e^{i^{}})^{T}_{k}^{j^{}}^{2} \]

From the Viewpoint of Graph Sampling.We notice that an alternative perspective can be adopted to interpret this regularization term. Given a graph data \(G\), we should acknowledge that there exists some noise in its structure or feature, as a result of measuring. This randomness also attribute to the variance of classification. Assume each graph \(G\) in our dataset is a sample drawn from an underlying true graph \(\). Denote the classification result trained on \(G\) as \(f(x;G)\). Then the variance on \(G\) can be written as

\[_{G}=_{x G}[_{G}[[f(x;G)-_{G}[f(x;G)]^{2}]]] \]

Taking \(f(x)=h^{T}(x)C\), it is straitforward to show that Equation 4 and Equation 5 are same.

Despite the algorithm obtained through this new interpretation being identical to the previous one, it is worth noting that the two interpretations differ significantly. The former incorporates information from unlabeled nodes and assumes the variance originates from selecting different nodes as the training set. Graph augmentation facilitates the sampling of node pairs belonging to the same class. The latter, on the other hand, considers the same training set, but the variance originates from the entire graph's information. Graph augmentation is used to simulate the process of sampling the training graph from the underlying true graph.

## 4 The Final Algorithm

In this section, we introduce our ReVar (**R**egularize **V**ariance) framework which is based on previous theoretical frameworks for optimizing model variance. We explain our innovative approach to approximating the model's variance and using it as a regularization term in our algorithm in Section 4.1. In Section 4.2, we discuss how we integrate the GCL framework with a new contrastive learning term designed for semi-supervised learning tasks. Lastly, we present the final formulation of our optimization objective in Section 4.3, which optimizes both the cross-entropy loss function and our variance-constrained regularization term, providing a comprehensive approach to improve the performance of semi-supervised imbalanced node classification tasks.

### Variance-constrained Optimization with Adaptive Regularization

To initiate, we elucidate the methodology underpinning the computation of Equation 4. Initially, we calculate the class center \(C_{i}\) corresponding to each class. Following this, we systematically construct a probability distribution reference matrix, denoted by \(:=[C_{1};C_{2};;C_{k}]\). It's worth noting that the assemblage of class centers is represented by \(C:=(C_{1};C_{2};;C_{k})\).

Given a node \(i\) with its associated embedding \(h_{i}\), the label probability distribution \(_{i}\) pertaining to node \(i\) is ascertainable via the subsequent equation:

\[_{i}^{j}=(h_{i},C_{j})/ )}{_{l=1}^{k}((h_{i},C_{l})/ )} \]

In the aforementioned equation, \(_{i}^{j}\) signifies the \(j\)-th dimensional component of \(_{i}\), \((,)\) denotes the function to compute the cosine similarity between a pair of vectors, and \(\) is a specified temperature hyperparameter.

Subsequent to this, for every node \(i V\), we determine the label probability distribution. Distinctively, \(_{i}\) and \(_{i}^{}\) are derived from \(\) and \(^{}\), respectively. The former acts as the prediction distribution, whereas the latter represents the target distribution. Our approach then aims to minimize the cross-entropy between these two distributions.

It's paramount to underscore that for each labeled node within \(^{}\), its one-hot label vector is directly assigned as \(_{i}^{}=y_{i}\) for all \(i V_{L}\). This diverges from the procedure of deducing the predicted class distribution as described in Equation 6. This methodology is purposed to optimally leverage the extant label information.

However, a simplistic minimization of the aforementioned loss might engender confirmation bias. This is attributed to the potentially inaccurate \(_{i}^{}\) estimated for the unlabeled nodes (i.e., \(V_{U}\)), an aspect that can deleteriously impact the efficacy of pseudo-labeling-oriented semi-supervised methodologies. To address this concern, we introduce a confidence-based label-guided consistency regularization , as detailed below:

\[_{}=}|}_{i V_{}} CE(_{i}^{},_{i})+|}_{i  V_{L}}CE(y_{i},_{i}) \]

where \(V_{}=\{v_{i}_{\{(_{i}^{})>v\}}=1, i V_{U}\}\) represents the set of nodes with confident predictions, \(v\) is the threshold for determining whether a node has a confident prediction, and \(_{\{\}}\) is an indicator function.

Figure 2: Overall pipeline of ReVar. (a) Two different views of the graph \(},}^{}\) are obtained by graph augmentation \(transform\), and are subsequently fed into GNN encoder \(f_{}\). (b) Intra-class and inter-class representations are aggregated, which means, for labeled nodes, it’s positive samples not only belong to the same class in both view but also in the other view. (c) Variance is estimated by Equation 4. Specifically, the label probability distribution is computed for each node in two views based on it’s similarity with each class center. And the difference between two probability distributions is used to approximate the model’s variance and also optimized as one term in the loss function.

In the context of confidence determination for \(_{i}^{}\), we employ a criterion wherein a value is appraised as confident if its maximum constituent surpasses a stipulated threshold. We postulate that a heightened threshold, represented by \(v\), acts as a safeguard against confirmation bias. This stratagem ensures that only high-caliber target distributions, specifically \(_{i}^{}\), have a pivotal influence on Equation 7.

### Intra-Class Aggregation Regularization

In this section, we propose an extension to the concept of graph contrastive learning that emphasizes the invariance of node representations in semi-supervised scenarios. To achieve this, we partition the nodes into two groups: labeled and unlabeled. Specifically, for the unlabeled nodes, we learn node representation invariance through the use of positive examples exclusively. For each labeled node in both views, its positive sample no longer includes itself in the other view, but instead encompasses all labeled nodes in both views belonging to the same class. Through this approach, labeled nodes belonging to the same class can be aggregated, enabling the model to learn that representation invariance now applies not to a single node, but to the complete class representation.

\[_{}=-|}_{_{i}, _{i}^{} V_{U}}(_{i}_{i}^{})-}(_{l=1}^{k}_{_{i}, _{j}^{}_{l}}(_{i} _{j}^{})+_{l=1}^{k}_{_{i}, _{j}_{l}}(_{i} _{j})) \]

where \(N_{all}\) = \(_{i=1}^{k}|_{i}|(|_{i}|-1 )\), \(_{i}\) and \(_{i}^{}\) represent feature embedding in \(\) and \(^{}\) respectively for node \(i\). To the best of our knowledge, we are the first to introduce label information into the contrastive learning paradigm to obtain invariant representations within the context of intra-class variation and address the challenge of semi-supervised imbalanced node classification. Notably, our approach only utilizes positive examples, which significantly reduces the model's training complexity and enhances its scalability and generalization capabilities.

### Objective Function

To derive our ultimate objective function, we incorporate both \(_{}\) and \(_{}\). These are weighted by coefficients \(_{1}\) and \(_{2}\), respectively. Formally, the amalgamation can be represented as:

\[_{}=_{1}_{}+_ {2}_{}+_{} \]

Additionally, we introduce the cross-entropy loss denoted by \(_{}\), which is established over a collection of labeled nodes, denominated as \(V_{L}\).

## 5 Experiment

### Experimental Setups

Datasets and Baselines.We have demonstrated the efficacy of our method on five commonly used benchmark datasets across various imbalance scenarios. For the conventional setting (\(\)=10) of imbalanced node classification in [50; 24; 31], we conducted experiments on Cora, CiteSeer, Pubmed, and Amazon-Computers. More precisely, we select half of the classes as the minority classes and randomly convert labeled nodes into unlabeled ones until the training set reaches an imbalance ratio of \(\). To be more precise, for the three citation networks, we adopt the standard splits proposed by  as our initial splits, with an initial imbalance ratio of \(\). In order to better reflect real-world scenarios, we conducted representative experiments on the naturally imbalanced datasets Amazon-Computers and Coauthor-CS. For this setting, we utilize random sampling to construct a training set that adheres to the true label distribution of the entire graph. The comprehensive experimental settings, including the evaluation protocol and implementation details of our algorithm, are explicated in Appendix E. For baselines, we evaluate our method against classic techniques, including cross-entropy loss with re-weighting , PC Softmax , and Balanced Softmax , as well as state-of-the-art methods for imbalanced node classification, such as GraphSMOTE , GraphENS , ReNode , and TAM . See Appendix E for implementation details of the baselines.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

Conclusion and Future Work

This paper presents a novel approach to address imbalanced node classification. Our method integrates imbalanced node classification and the Bias-Variance Decomposition framework, establishing a theoretical foundation that closely links data imbalance to model variance. Additionally, we employ graph augmentation techniques to estimate model variance and design a regularization term to mitigate the impact of class imbalance. We conduct exhaustive testing to demonstrate superior performance compared to state-of-the-art methods in various imbalanced scenarios. Future work includes extending ReVar and its theories to the fields of computer vision and natural language processing. Moreover, we anticipate the emergence of more effective theories and algorithms to be developed and flourish.

## Reproducibility Statements

The model implementation and data is released at [https://github.com/yanliang3612/ReVar](https://github.com/yanliang3612/ReVar).