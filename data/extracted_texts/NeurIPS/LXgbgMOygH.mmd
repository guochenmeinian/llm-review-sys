# NeuralPlane: An Efficiently Parallelizable Platform

for Fixed-wing Aircraft Control

with Reinforcement Learning

 Chuanyi Xue, Qihan Liu, Xiaoteng Ma, Yang Qi, Xinyao Qin,

**Yuhua Jiang**, **Ning Gui**, **Jinsheng Ren**, **Bin Liang**, **Jun Yang**

Department of Automation, Tsinghua University

{xcy22, lqh20}@mails.tsinghua.edu.cn

pony.xtma@gmail.com

yangjun603@tsinghua.edu.cn

Equal contributionCorresponding author

###### Abstract

Reinforcement learning (RL) demonstrates superior potential over traditional flight control methods for fixed-wing aircraft, particularly under extreme operational conditions. However, the high demand for training samples and the lack of efficient computation in existing simulators hinder its further application. In this paper, we introduce NeuralPlane, the first benchmark platform for large-scale parallel simulations of fixed-wing aircraft. NeuralPlane significantly boosts high-fidelity simulation via GPU-accelerated Flight Dynamics Model (FDM) computation, achieving a single-step simulation time of just 0.2 seconds at a parallel scale of \(10^{6}\) aircraft, far exceeding current platforms. We also provide clear code templates, comprehensive evaluation and visualization tools, and hierarchical frameworks for integrating RL and traditional control methods. We believe that NeuralPlane can accelerate the development of RL-based fixed-wing flight control and serve as a new challenging benchmark for the RL community. Our NeuralPlane is open-source and accessible at https://github.com/xuecy22/NeuralPlane.

## 1 Introduction

Unmanned Aerial Vehicles (UAVs) equipped with autonomous flight control systems have found extensive application in various tasks such as crop protection , pipeline inspection , topographic mapping  and environmental monitoring , particularly in dangerous or inaccessible environments. Fixed-wing aircraft, in contrast to the commonly used multi-rotor drones, have demonstrated broad utility in these missions owing to their long flight endurance and high cruising speeds [5; 6; 7]. Nonetheless, the inherent complex nonlinearity in dynamic models and strong sensitivity to atmospheric disturbances make traditional control approaches inadequate for ensuring agile and stable flight under extreme operational conditions [8; 9; 10]. As a result, addressing the challenges in fixed-wing aircraft control necessitates the utilization of advanced techniques that can deliver rapid responsiveness and robust stability.

Reinforcement learning (RL) has made significant progress in recent years, achieving superhuman performance in various domains, particularly in areas characterized by complex nonlinear dynamics such as robotic manipulation , autonomous driving  and quadrotor flight control . RL operates on the trial-and-error principle, where an agent interacts with the environment, adaptscontinuously based on feedback, and optimizes decision-making to maximize cumulative rewards . In the field of fixed-wing aircraft control, the potential benefits of applying RL are manifold. The complex nonlinear dynamics can be treated as a black-box model and addressed through exploration and exploitation. Moreover, the adaptive nature of RL enables systems to adapt their behavior to novel environmental conditions, such as variations in air density, wind direction, and intensity.

Despite an increasing number of researchers discovering the potential of RL and conducting notable work on fixed-wing aircraft control [15; 16; 17; 18], significant obstacles hinder its wider application in complex control scenarios. One significant challenge is the substantial demand of training samples that RL algorithms require, making it prohibitively expensive to gather sufficient interaction data from real flights. The existing simulation environments for fixed-wing aircraft either lack high fidelity to bridge the sim-to-real disparity , or fail to support large-scale efficient parallelization [20; 21; 22; 23], thus limiting the scalability of RL methods for more complex tasks.

_Can we build a platform that supports large-scale high-fidelity parallel simulations of fixed-wing aircraft and is convenient for training, testing, and evaluating RL algorithms?_

To answer this question, we propose a GPU-accelerated Flight Dynamics Model (FDM) for fixed-wing aircraft dynamics, which significantly improves simulation efficiency at large-scale parallel simulations, far surpassing currently common fixed-wing aircraft simulation platforms. Building on this, we introduce NeuralPlane, the first benchmark platform supporting large-scale parallel simulations for fixed-wing aircraft. This platform also features a clear system framework and interfaces to support the training, testing, and evaluation of RL algorithms. It integrates various task scenarios to validate the performance of RL in controlling fixed-wing aircraft across different problem contexts. Furthermore, the simulation's high fidelity facilitates the transfer of trained RL algorithms to real-world scenarios. Our contributions can be outlined as follows:

1. **Support large-scale parallel high-fidelity simulation of fixed-wing aircraft**. We propose a GPU-accelerated FDM that supports large-scale parallel high-fidelity simulations. Our method achieves a single-step simulation time of 0.2 seconds at a parallel scale of \(10^{6}\) aircraft, surpassing current platforms. Figure 1 shows the comparison results.
2. **Provide multiple fixed-wing aircraft task scenarios and baseline algorithms**. NeuralPlane includes various basic task scenarios for fixed-wing aircraft and allows researchers to customize tasks through provided interfaces. It integrates both traditional control methods and RL methods, offering detailed analysis and evaluation of their performance.
3. **Provide clear code templates and frameworks**. We offer code templates for RL and traditional control methods, along with interfaces for interacting with the simulation environment, enabling researchers to easily train, test, and evaluate their algorithms.
4. **Support algorithm evaluation and visualization of control results**. We propose various metrics to evaluate algorithm performance, making analysis convenient for researchers. We also support different visualization tools for rendering flight trajectories, to intuitively assess control results.

## 2 Related Work

Fixed-wing Aircraft ControlTraditional approaches [24; 25; 26] in fixed-wing aircraft control heavily rely on elaborately designed gain scheduling techniques with linearized plant dynamics(e.g. Proportional-Integral-Derivative (PID) control ). However, these methods have been found to be less flexible to changes in model dynamics, such as task redistribution and wind disturbances [8; 9; 10]. On the other hand, RL has been progressively integrated into this domain, enhancing aircraft control systems with the ability to optimize decision-making, develop robust policies, and adapt to unstable and noisy dynamics through feedback from simulation environments. The adaptive learning facilitated by RL in fixed-wing aircraft control can cover a range of tasks, such as attitude control , autopilot management , control surface coordination , and even the autonomous response to unexpected events without immediate human intervention .

RL methods in Aircraft Control ChallengesVarious RL algorithms have been leveraged for diverse flight control tasks. For instance, Proximal Policy Optimization (PPO)  has been employed in attitude control to manage the complex nonlinear dynamics inherent in aircraft systems [15; 28; 17].

Deep Deterministic Policy Gradient (DDPG)  has exhibited resilience to varying wind disturbance conditions in the context of automatic landing control for fixed-wing aircraft, especially when provided with well-designed reward functions . Soft Actor-Critic (SAC)  has demonstrated superior performance over traditional controllers in both autonomous flight  and maneuver generation [18; 33], particularly in dealing with continuous action spaces. Beyond the field of online model-free RL, fixed-wing aircraft control challenges are increasingly recognized as critical benchmarks across various RL domains. Offline RL methods have been introduced to tackle challenges like off-field landings in unprepared locations  and enhance data efficiency in attitude control tasks . Furthermore, tasks related to navigation  and path planning [37; 38] can be reconceptualized as Goal-Conditioned RL (GCRL) problems, thereby boosting the capacity for generalization. Additionally, the emerging challenges of traffic control [39; 40], collision avoidance [41; 42], and cooperative decision-making  within multi-aircraft systems are establishing themselves as stringent benchmarks for Multi-Agent Reinforcement Learning (MARL) [44; 45; 46; 47; 48; 49; 50; 51]. However, integrating RL into flight control has lots of challenges. Ensuring safety, achieving data efficiency, and sim-to-real transferring are central areas that require careful attention, particularly in complex tasks under extreme operational conditions .

Aircraft Simulation PlatformsThe development of aircraft simulation platforms has become a focal point within academia, primarily due to their essential role in the filed of flight control. XPlane , renowned for its accurate and realistic physics modeling, has been utilized in numerous research projects [52; 53]. However, due to its commercial closed-source nature, researchers must procure a real-time rendering game and install a UDP-based connector, XPlaneConnect . The absence of a headless mode in XPlane, which would allow for simulations to run without graphical rendering, positions it as a more suitable tool for testing rather than training. ArduPilot , an open-source autopilot system, has been specifically designed for unmanned aerial vehicles (UAVs) and has garnered significant favor within drone communities due to its generality across flight modes. While originally tailored for autonomous flight, ArduPilot is mainly employed for remote control applications in quadcopters [55; 56; 57] and requires extra adaptations to directly interface with RL on fixed-wing control tasks. JSBSim  stands out as a highly flexible, open-source flight dynamics simulation platform that supports headless mode operation and integrates with the open-source FlightGear rendering software. While JSBSim presents itself as an entirely cost-free option for RL researchers [16; 17; 58; 59; 60], its reliance on CPU-based Flight Dynamics Model (FDM) computation, devoid of GPU acceleration, restricts its capacity for parallelized deployment. QPlane  proposes the first toolkit which combines RL training on fixed-wing flight with multiple flight simulators (XPlane and JSBSim). Despite inheriting the parallelization limitations from JSBSim, QPlane incorporates a standard Gym interface for reinforcement training. This integration facilitates flexible replacement of RL algorithms and has been effectively utilized in a variety of research

Figure 1: Performance comparison between NeuralPlane and several classic platforms. **Left**: Relationship between the time required to simulate 10 seconds and the number of parallel simulations. **Right**: Relationship between GPU memory usage for simulating 10 seconds and the number of parallel simulations. **Blue line**: Results for NeuralPlane. **Red line**: Results for JSBSim . **Green line**: Results for Ardupilot . **Orange line**: Results for XPlane .

projects [61; 62]. MaCA  is another aircraft platform, which also integrates RL algorithms with heterogeneous multi-agent cooperative decision-making tasks, but treats flights as mass points without actual FDM computation. However, existing aircraft simulation platforms have only implemented standard RL interfaces based on Gym, lacking specialized acceleration designs for large-scale parallel simulation, a critical aspect highlighted in robotics manipulation by IsaacGym . To the best of our knowledge, we are the first to develop the fixed-wing aircraft control platform supporting efficient large-scale parallelization with GPU acceleration for RL.

Hardware-accelerated Simulation PlatformsRunning physics simulations on GPUs can lead to significant speedups, which is crucial for RL training. In recent years, hardware-accelerated simulation platforms have greatly advanced the RL field. For example, NVIDIA's Isaac Gym  enables high-performance training for various robotics tasks directly on GPUs. In autonomous driving, platforms like GPUDrive and Waymax offer similar benefits. GPUDrive , built on the Madrona Game Engine, can generate over a million steps per second, allowing for fast and effective RL training using the Waymo Motion dataset. Waymax , designed for large-scale simulation in multi-agent scenarios, further advances autonomous driving research.

Another example is Pgx , a suite of board game RL environments optimized for GPU/TPU accelerators, which simulates environments 10-100 times faster than existing Python implementations. These developments underscore the importance of building RL training platforms that support massively parallel simulations, as they can significantly enhance the speed and effectiveness of RL algorithms across various domains. We believe that NeuralPlane can also advance RL in the fixed-wing aircraft domain.

## 3 NeuralPlane: Design and Resources

### Preliminaries

The control process of fixed-wing aircraft can be modeled as an MDP. An MDP can be defined by a tuple \(,,P,R,\). Define the state of fixed-wing aircraft at time step \(t\) as \(}\), the control input as \(}\), and the task objective as \(}\). Then the observed state at time step \(t\) can be expressed as \(}=normalize([}-},}])\), and the action at time step \(t\) can be expressed as \(}=normalize(})\), where \(normalize\) denotes the normalizing function. The reward at time step \(t\) can be expressed as \(}=d(},})\), where \(d\) denote the distance metric function. More details are given in the Appendix A.1.

### Architecture and Workflow

The architecture and workflow of NeuralPlane are illustrated in Figure 2. The platform consists of three main modules: the _simulation environment_, the _baseline library_, and the _performance evaluator_. The _simulation environment_ supports dynamic simulations of various fixed-wing aircraft and interacts with control algorithms. It includes interfaces for the aircraft model, environmental parameters, and the interaction between the environment and algorithms. The simulation environment supports large-scale parallel simulations, ensuring high computational efficiency and meeting RL training requirements for sample complexity. The _baseline library_ includes traditional control methods and

   Platform & FDM Computation & Headless Mode & RL Integration & GPU Acceleration \\  XPlane  & ✓ & ✗ & ✗ & ✗ \\ ArduPilot  & ✓ & ✓ & ✗ & ✗ \\ JSBSim  & ✓ & ✓ & ✗ & ✗ \\ QPlane  & ✗ & ✓ & ✓ & ✗ \\ MaCA  & ✗ & ✓ & ✓ & ✗ \\  NeuralPlane & ✓ & ✓ & ✓ & ✓ \\   

Table 1: A summary of related work on aircraft simulation platforms. NeuralPlane is the first work that incorporates designs from all four domains.

RL algorithms. The _performance evaluator_ proposes various metrics and different visualization tools to evaluate the performance of algorithms, making analysis convenient for researchers.

The workflow of the platform consists of three primary stages: _training_, _testing_, and _evaluation_. Before _training_, we first select the fixed-wing aircraft model, the task scenario, and the control algorithm. The algorithm is then trained on the platform in a fully automated process. After training, we conduct the _testing_ stage under the specific testing scenario and compare with baseline algorithms. The platform automates the evaluation process and records flight data. Based on this data, the platform calculates performance metrics to evaluate the algorithm's control performance in the _evaluation_ stage. Additionally, we can replay the flight data with different visualization tools to observe the algorithm's control results directly, facilitating further optimization.

### Simulation Environment

The simulation environment is the core component of NeuralPlane, distinguishing it from other fixed-wing aircraft simulation platforms with its capability for large-scale parallel simulations. This feature is crucial for meeting the sample complexity and data efficiency requirements of RL algorithm training. The simulation environment consists of two main modules: the GPU-accelerated FDM and the task Scenarios. These modules together provide a comprehensive and efficient training ground for advanced control algorithms. For more details, see the Appendix A.1.

GPU-accelerated FDMLarge-scale parallel computations in the simulation environment are enabled by the parallel solution of fixed-wing aircraft dynamics equations. State variables, control variables, and aerodynamic parameters are stored in tensor format, allowing GPUs to accelerate tensor operations and improve efficiency. Traditionally, aerodynamic parameters are read from lookup tables, a process that hinders parallel efficiency due to extensive logical decisions. To address this, we use a multi-layer perceptron (MLP) to approximate these parameters, replacing table lookups with MLP predictions in practical model computations.

The simulation environment includes several classical fixed-wing aircraft dynamics models, such as the Cessna 172P and F16. It also features clear interfaces and documentation, enabling researchers to integrate their own fixed-wing aircraft models.

Task ScenariosTask scenarios in the simulation environment are defined by task objectives and flight conditions. Table 2 and Table 3 describe a brief introduction to several typical task scenarios integrated into the platform. For detailed content, see the Appendix A.2.

   Name & Target & Difficulty \\  Heading & altitude, yaw angle, and speed & easy \\ Control & pitch angle, yaw angle, and speed & middle \\ Tracking & coordinate position (geocentric coordinate) & difficult \\   

Table 2: Task scenarios categorized by objectives.

Figure 2: The overall architecture and workflow of NeuralPlane.

By combining different task objectives and flight conditions, NeuralPlane supports to set up various task scenarios. The platform also offers clear parameter setting interfaces, allowing researchers to configure different task objectives and conditions independently.

### Baseline Library

NeuralPlane integrates two types of baseline algorithms: traditional methods and RL methods. Below is a brief introduction to these algorithms. For detailed content, see the Appendix A.3.

Traditional MethodsThese are based on open-source fixed-wing aircraft control algorithms from the Ardupilot platform, using a hierarchical control approach. The upper layer includes the TECS controller , which manages the aircraft's total flight energy by adjusting throttle and pitch to maintain desired altitude and speed, and the L1 controller , which manages the flight path by adjusting roll and yaw to follow waypoints or desired path characteristics. The lower layer consists of an attitude loop controller using a dual-loop PID algorithm to control the aircraft's surfaces and achieve three-axis attitude tracking.

RL MethodsPPO  is a well-known RL algorithm that has been successfully applied in fixed-wing aircraft control [15; 28; 17]. We use PPO for Heading and Control tasks in fixed-wing aircraft, demonstrating high training efficiency. For the Tracking task, we use a hierarchical RL method: the upper-level algorithm converts the target location into desired pitch, yaw, and speed, while the lower level uses the trained PPO algorithm to control the aircraft's surfaces.

### Performance Evaluator

NeuralPlane evaluates fixed-wing aircraft control performance using maneuverability and safety indicators. Maneuverability indicators assess control performance, while safety indicators evaluate control safety. Below are typical indicators, with all performance metrics normalized. For more details, see the Appendix A.4.

Maneuverability Indicators1) G: Average G-force during flight. 2) TAS: Average True Air Speed during flight. 3) RoC: Average Rate of Climb during flight. 4) AOA: Average Angle of Attack during flight. 5) t: Average time to complete the task objective.

Safety Indicators1) Altitude Safety Margin (ASM): The difference between the average flight altitude and the minimum safe flying altitude. 2) Speed Safety Margin (SSM): The smaller value between the absolute differences of the average flight speed from both the maximum and minimum safe flying speeds. 3) Overload Safety Margin (OSM): The absolute difference between the average G-force and the maximum safe G-force. 4) Angle of Attack Safety Margin (AOASM): The absolute difference between the average angle of attack and the critical safe angle of attack. 5) Sideslip Angle Safety Margin (AOSSM): The absolute difference between the average sideslip angle and the critical safe sideslip angle.

## 4 Benchmarking Study

NeuralPlane is a valuable platform for RL research, making training, testing, and evaluation of algorithms easy to implement. It facilitates the analysis of maneuverability, safety, and robustness in fixed-wing aircraft control. This section presents examples demonstrating NeuralPlane's application

   Name & Key Value & Difficulty & Description \\  HighSpeed & speed & middle & speed exceeding Mach 1 \\ Noisy & noise scale & middle & noisy observations \\ HighAltitude & altitude & difficult & altitude exceeding 30,000 feet \\ Windy & airspeed & difficult & airspeed not equal to 0 \\   

Table 3: Task scenarios categorized by flight conditions.

in experimental research. We also test NeuralPlane's parallel performance, showing the importance of large-scale parallel simulations for controlling flight in fixed-wing aircraft.

### Experimental Setup

In the following experiments, unless specified otherwise, we use the F16 fixed-wing aircraft dynamics model. The training parameters are set as follows: the maximum number of training steps (M) is \(1.35 10^{9}\), the number of parallel rollouts (n) is 3000, and the number of steps per rollout (m) in one iteration is 3000. All experiments are conducted on an NVIDIA A100 GPU with 80GB of memory. NeuralPlane is compatible with other platforms as well. For detailed settings of the simulation environment and algorithm parameters, see the Appendix B.1.

### Platform Performance Analysis

One of the standout advantages of NeuralPlane compared to other fixed-wing aircraft simulation platforms is its support for large-scale parallel simulations. We test the parallel capabilities of NeuralPlane and compare it with several mainstream fixed-wing aircraft dynamics simulation platforms. Figure 1 shows that when the number of parallel simulations exceeds 1000, NeuralPlane's computation time is significantly superior to the other platforms, clearly demonstrating NeuralPlane's excellent performance in parallel simulations.

To enable large-scale parallel simulations of fixed-wing aircraft, NeuralPlane uses MLP to fit aerodynamic parameter data tables, accelerating the lookup computation process. The fitting accuracy of the MLP directly affects the platform's simulation accuracy. We test the MLP's fitting performance comprehensively, with results shown in Table 4. To eliminate the impact of randomness on the MLP fitting results, we repeat the experiments with multiple random seeds and calculate the mean and standard deviation of the \(R^{2}\) and error on the test set. The mean \(R^{2}\) values for the MLP fitting of aerodynamic parameters are all above 0.99, with low standard deviations, demonstrating high fitting accuracy. This confirms the platform's high-fidelity simulation capabilities.

### Training of RL Algorithms

We use two dynamics models, the F16 and an unmanned fixed-wing aircraft (UAV), to train the RL algorithms for all tasks and conditions in the baseline library. Full experimental results are provided in the Appendix B.2, with a subset (F16 model for the Heading task) shown in Figure 4. A comparison using a small UAV dynamics model for the Heading task is shown in Figure 3. These results demonstrate that NeuralPlane supports RL training across various aircraft models and tasks. Owing to large-scale parallel training, the RL algorithms converge quickly, with the average episode reward steadily increasing and showing minimal fluctuations. The PPO algorithm converges in about 100 iterations, taking around one day.

   Name & test set \(R^{2}\) & test set error \\  Cl & \(0.9949 0.0009\) & \(0.0051 0.0006\) \\ Cm & \(0.9962 0.0007\) & \(0.0042 0.0003\) \\ Cn & \(0.9926 0.0011\) & \(0.0067 0.0013\) \\ Cx & \(0.9967 0.0004\) & \(0.0030 0.0007\) \\ Cz & \(0.9991 0.0001\) & \(0.0014 0.0003\) \\   

Table 4: MLP fitting results for some aerodynamic parameters.

Figure 3: Training curves of PPO under different settings. **Left**: Experimental results with varying numbers of parallel rollouts during the training process. **Right**: Experimental results with different fixed-wing aircraft dynamics models.

We also examine the effect of parallel rollout quantity on RL training, comparing results with n set to 32, 256, and 3000. The results, shown in Figure 3, indicate that with a small parallel training quantity, the RL algorithms do not converge and the average episode reward fluctuates significantly. This highlights the importance of NeuralPlane's support for large-scale parallel simulations.

### Comparison of Different Baseline Algorithms

Based on the Heading task, we compare the control performance of the traditional method and the PPO algorithm under different conditions (Normal, HighSpeed, HighAltitude, Windy, Noisy). Performance is evaluated using the proposed maneuverability and safety metrics. The results, shown in Figure 5, indicate that the PPO algorithm significantly outperforms the traditional method in maneuverability metrics under all conditions, especially in key indicators such as G, t, and RoC. In terms of safety metrics, the traditional method slightly outperforms the PPO algorithm. However, in any task scenario, the PPO algorithm's success rate in controlling the aircraft to reach the target is above 95 %, indicating that the safety of using the PPO algorithm to control fixed-wing aircraft is entirely acceptable. These experiments demonstrate the superiority of RL algorithms in controlling flights of fixed-wing aircraft.

### Testing and Evaluation of Algorithms

NeuralPlane enables performance evaluation of algorithms for flights of fixed-wing aircraft using various metrics, and it also allows for the visualization and replay of flight data. This feature helps researchers intuitively assess algorithm performance. We design three visualization methods to aid in performance analysis: 1) using FlightGear for visual rendering of flight scenes; 2) using Tacview for visual analysis of flight trajectories and situations; and 3) plotting flight trajectories to directly

Figure 4: Training curves of PPO in different task scenarios. **From left to right**, the task conditions are different wind speeds, different flight altitudes, different environmental noise levels, and different flight speeds, with the task objective being the Heading task in all cases.

Figure 5: Comparison of control performance between traditional method and PPO in different task scenarios. **Green line**: PPO algorithm results. **Yellow line**: Traditional methods results.

analyze aircraft maneuverability. We demonstrate the visualization of trajectory of the PPO algorithm completing the Tracking task, as shown in Figure 6. Additional visualized flight trajectories are available in the Appendix B.2. This data replay mechanism in NeuralPlane aids researchers in algorithm design and debugging, enhancing the platform's usability.

### Analysis of Algorithm Robustness

NeuralPlane includes methods for evaluating the robustness of algorithms. By testing algorithms under environmental noise disturbances and varying noise levels, researchers can compare control performance across different scenarios. This allows for analysis of the algorithms' robustness and disturbance rejection capabilities. The robustness metric is defined as the maximum noise level at which the algorithm can still control the aircraft safely, allowing for evaluation and comparison of different algorithms. The robustness analysis results, shown in Figure 7, indicate that when the noise scale increases to 0.01, the PPO algorithm can still complete the task, although the task completion time slightly increases. The results demonstrate that the PPO algorithm possesses a certain level of disturbance rejection capability.

## 5 Conclusion and Future Work

In this paper, we introduce NeuralPlane, the first benchmark platform for large-scale parallel simulations of fixed-wing aircraft, designed to advance the development of RL algorithms for flight control. Our platform addresses key challenges in existing simulation environments by offering GPU-accelerated FDM, achieving a single-step simulation time of just 0.2 seconds at a parallel scale of \(10^{6}\), significantly outperforming current platforms.

Our experimental results demonstrate NeuralPlane's superior performance in large-scale parallel simulations, highlighting its efficiency and capability to train RL algorithms rapidly and effectively. Comparative analysis of PPO and traditional methods across various task scenarios reveals the superior maneuverability and acceptable safety performance of RL algorithms.

While our NeuralPlane presents a significant advancement, it also has several limitations that we aim to address. Currently, it supports a limited number of fixed-wing aircraft models and task scenarios. We plan to expand these to provide a richer set of tasks and functionalities for RL applications in fixed-wing aircraft control. Additionally, the platform does not yet support multi-aircraft scenarios. We intend to extend NeuralPlane to include these, facilitating multi-agent RL research. Finally, we aim to improve the platform's user-friendliness by designing clearer interfaces and workflows.

Figure 6: Visualization of fixed-wing aircraft flight trajectories.

Figure 7: Robustness test results of the PPO algorithm.