# Self-Weighted Contrastive Learning among Multiple Views for Mitigating Representation Degeneration

Jie Xu\({}^{1}\)  Shuo Chen\({}^{2}\)  Yazhou Ren\({}^{1}\)  Xiaoshuang Shi\({}^{1}\)

**Heng Tao Shen\({}^{1}\)  Gang Niu\({}^{2}\)  Xiaofeng Zhu\({}^{1,}\)**

\({}^{1}\)University of Electronic Science and Technology of China, China

\({}^{2}\)RIKEN Center for Advanced Intelligence Project, Japan

Corresponding Author (seanzhuxf@gmail.com). Code link: _https://github.com/Submissionshu/SEM_.

###### Abstract

Recently, numerous studies have demonstrated the effectiveness of _contrastive learning_ (CL), which learns feature representations by pulling in positive samples while pushing away negative samples. Many successes of CL lie in that there exists semantic consistency between data augmentations of the same instance. In _multi-view scenarios_, however, CL might cause _representation degeneration_ when the collected multiple views inherently have inconsistent semantic information or their representations subsequently do not capture sufficient discriminative information. To address this issue, we propose a novel framework called _SEM: SEIf-weighted Multi-view contrastive learning with reconstruction regularization_. Specifically, SEM is a general framework where we propose to first measure the discrepancy between pairwise representations and then minimize the corresponding self-weighted contrastive loss, and thus making SEM adaptively strengthen the useful pairwise views and also weaken the unreliable pairwise views. Meanwhile, we impose a self-supervised reconstruction term to regularize the hidden features of encoders, to assist CL in accessing sufficient discriminative information of data. Experiments on public multi-view datasets verified that SEM can mitigate representation degeneration in existing CL methods and help them achieve significant performance improvements. Ablation studies also demonstrated the effectiveness of SEM with different options of weighting strategies and reconstruction terms.

## 1 Introduction

_Contrastive learning_ (CL) explicitly enlarges the feature representation similarity between semantic-relevant samples, and it is adept at capturing high-level semantics while discarding irrelevant information. This learning paradigm has facilitated many research and application fields, such as visual representation , text understanding , and cross-modal agreement . Samples with consistent semantics are typically constructed as positive sample pairs for CL loss (_e.g._, InfoNCE ), which motivates multi-view learning scenarios  where researchers focus on exploring common semantics among multi-view data. However, this kind of data usually is with heterogeneous views and thus cannot be directly processed by previous CL methods with two shared network branches.

To handle this situation, many _multi-view contrastive learning_ (MCL) methods  have been proposed, which treats multiple views as positive sample pairs and achieves important progresses in exploring multi-view common semantics (see Sec. 2 for details). Nevertheless, we find that CL might cause _representation degeneration_ that the representations of high-quality views tend to degenerate. This may make the MCL methods perform worse than the optimal single view (see Sec. 3.1 and Sec. 4.1), and thus heavily limiting the usability of MCL in practical scenarios. Althoughseveral CL work [15; 16] proposed different CL losses aiming at increasing robustness to noise and made important advances on vision and graph data, our experiments discover that these CL losses are still fragile in multi-view scenarios as multi-view data are with more diversity than single-view data. Different from changing CL loss, recent MCL methods [14; 17] focused on changing model structures and successfully improved the effectiveness of clustering the learned representations. Nevertheless, representation degeneration still exists in many cases and it requires further solutions.

We find that there could be two reasons leading to representation degeneration in MCL. **I)** The quality difference among multiple views. The success of CL is based on the priori condition that the constructed positive sample pair has semantic consistency, which generally holds in previous CL applications [1; 5; 8]. Unfortunately, for multi-view learning, the collected views usually have quality difference and the semantic of positive sample pairs might be inconsistent due to view diversity. Consequently, CL causes the representation degeneration of high-quality views due to the existence of low-quality views. **II)** Losing discriminative information during data processing. Multi-view data typically involve heterogeneous data forms [9; 18], _e.g._, different dimensions, modalities, and sparsity. For achieving MCL, the model needs to transform heterogeneous multi-view data into the same form with different encoders. However, data transformation could lose discriminative information as this process has no supervised signals for maintaining information. As a result, CL might miss multiple views' common semantics and focus on semantic-irrelevant information due to inductive bias.

To this end, we propose _SElf-weighted Multi-view contrastive learning with reconstruction regularization (SEM)_ as shown in Figure 1 that takes the \(m,n,o\)-th views in \(V\) views as an example (where \(^{m,n}\) denotes the pairwise weight, \(^{m,n}_{CL}\) is the contrastive loss, and \(^{m}\) is the learned representations). Specifically, SEM minimizes self-weighted contrastive losses \(^{m,n}^{m,n}_{CL}\) and \(^{n,o}^{n,o}_{CL}\) after measuring the discrepancy between pairwise views' representations, _i.e._, (\(^{m}\),\(^{n}\)) and (\(^{n}\),\(^{o}\)), respectively. This makes SEM adaptively strengthen CL between the useful pairwise views and also weaken CL between the unreliable pairwise views. Meanwhile, SEM takes self-supervised reconstruction objectives as regularization terms (\(^{m}\), \(^{n}\), and \(^{o}\)) on the hidden features (\(^{m}\), \(^{n}\), and \(^{o}\)) of encoders for individual views, respectively. This reconstruction regularization assists CL in accessing sufficient discriminative information hidden in raw input data (\(^{m}\), \(^{n}\), and \(^{o}\)), which could be implemented by existing information encoder-decoder models, _e.g._, AE , DAE , and MAE . In SEM, the representations and pairwise weights are alternatively updated to mutually enhance one another.

In summary, our contributions are: **I)** We propose a novel general framework SEM that leverages self-weighting and information reconstruction to address representation degeneration in MCL. **II)** We provide three options with different advantages to implement the weighting strategy of SEM including class mutual information, JS divergence, and maximum mean discrepancy. **III)** Theoretical and experimental analysis verified the effectiveness of SEM. It helps many CL methods (_e.g._, InfoNCE , RINCE , and PSCL ) achieve significant performance improvements in multi-view scenarios.

## 2 Related Work

**Contrastive learning (CL)** As a popular self-supervised learning paradigm, CL focuses on learning semantically informative representations for downstream tasks [22; 23; 24; 25]. The most widely used loss function is InfoNCE  which pulls in the representations between positive sample pairs while pushing away that between negative sample pairs. Some work have attempted to explain the reasons for the success of applying InfoNCE, _e.g._, from perspectives of mutual information [8; 26], task-dependent view , or deep metric learning [28; 29]. Furthermore, [30; 31] pointed out to conduct CL with reconstruction regularization to achieve robust representations for downstream tasks. RINCE  (a short name of Robust InfoNCE) is a variant of InfoNCE contrastive loss that considers noise in false positive sample pairs. The recent work  investigates CL without

Figure 1: The framework of SEM. It leverages different networks to extract information of different views and conducts the proposed self-weighted multi-view contrastive learning with reconstruction regularization.

conditional independence assumption on positive sample pairs and proposes a population spectral contrastive loss (we call it PSCL for short). Despite important progresses have been made, in this work, we discover that these CL losses are still fragile in multi-view scenarios where data qualities are hard to be guaranteed, and even the reconstruction regularized CL is not enough.

**Multi-view contrastive learning (MCL)** Different from many CL methods that usually generate two inputs by data augmentation , MCL aims to handle multi-view data widely exiting in real-world applications. Multi-view data often contain more than two views/modalities and they naturally form multiple inputs [33; 34; 35]. Since the semantic consistency among multiple views is not guaranteed, it is challenging to capture the useful information in multi-view data, while considering the side effects of harmful information. Therefore, MCL attracts increasing attention in recent years [36; 37; 38]. For example, CMC  empirically shows that MCL performed with more scene views obtains the better representations with semantic information. DCP  leverages the maximization of mutual information to conduct consistency learning across different views and aims to achieve a provable sufficient and minimal representation. MFLVC  observes the conflict between consistency and reconstruction objectives in encoder-decoder frameworks and proposes to learn multi-level features for multiple views. DSIMVC  establishes a theoretical framework to reduce the risk of clustering performance degradation from semantic inconsistent views. Although satisfactory results are achieved in many cases, the representation degeneration caused by CL is still not well considered and addressed. In this paper, we point out that the representation degeneration could seriously limit the application of CL in multi-view scenarios, and propose the discrepancy-based self-weighted MCL to address it.

**Notations** This paper leverages bold uppercase characters and bold lowercase characters to denote matrices and vectors, respectively. Operator \(\|\|_{2}\) denotes vector \(_{2}\)-norm and operator \(\|\|_{F}\) is matrix \(F\)-norm. \(\{_{i}^{v}^{v}\}_{i=1,2,,N}^{v=1,2,,V}\) denotes the multi-view dataset with \(N\) samples in \(V\) views.

## 3 Methodology

This section first illustrates the phenomenon of representation degeneration in multi-view contrastive learning. To address this issue, we then establish a general framework of _SEM: Self-weighed Multi-view contrastive learning with reconstruction regularization_. To implement the SEM framework, we further provide different options of weighing strategy, contrastive loss, and reconstruction term.

### Motivation: Representation Degeneration in Multi-View Contrastive Learning

Researchers proposed many contrastive learning approaches and also achieved plenty of progress in multi-view learning. However, multi-view contrastive learning might result in the representation degeneration of high-quality views (_i.e._, those views contain rich semantic information) due to the diversity of multi-view data. Specifically, we illustrate it in Figure 2 that takes a popular multi-view dataset Caltech  (6 views) as an example. We leverage unsupervised linear clustering accuracy obtained by K-Means  to evaluate the representation quality of containing class-level semantics.

Firstly, we leverage self-supervised autoencoders (the setting is shown in Appendix B) to pretrain the representations of each view's data. In Figure 2(a), one can find that different views inherently have different levels of discriminative information and exhibit different qualities, where the worst (view 1)

Figure 2: (a) Clustering accuracy of individual views on Caltech dataset. (b) Contrastive loss and representation similarity between view 1 and view 4. (c) Clustering accuracy of view 1 and view 4 during contrastive learning.

and the best (view 4) have a large gap. Then, we adopt InfoNCE loss to perform contrastive learning between view 1 and view 4 in Figure 2(b), and record the clustering accuracy of their representations in Figure 2(c). We can observe that InfoNCE loss is well-minimized, which makes the representation similarity (evaluated by cosine) between view 1 and view 4 converge to \(1.0\). The performance on view 1 gradually increases. Nevertheless, the cost is that the representations of view 4 degenerate, on which the useful discriminative information reduces and thus the performance gradually decreases.

In multi-view learning, quality difference among multiple views is a common phenomenon. However, the representation degeneration in multi-view contrastive learning might make the representations of some high-quality views tend to be mediocre and thus miss their useful discriminative information.

### Self-Weighted Multi-View Contrastive Learning with Reconstruction Regularization

To mitigate representation degeneration in multi-view contrastive learning, we propose a simple but effective framework called _SEM:__Self-weighted Multi-view contrastive learning with reconstruction regularization_ as shown in Figure 1. Specifically, given view-specific data \(^{v}^{N d_{v}}\), we let \(^{v}^{N z}\) denote the corresponding new representations learned by a view-specific encoder. Between \(^{v}\) and \(^{v}\), we record a precursor state of representations as \(^{v}^{N h_{v}}\) (termed as hidden features), and the encoder is partitioned into two parts (the front and back parts are stacked and denoted as \(f^{v}\) and \(g^{v}\) sequentially). For the \(v\)-th view, we let \(^{v}\) and \(^{v}\) denote the network parameters of \(f^{v}\) and \(g^{v}\), respectively, and then the view-specific model can be formulated as follows:

\[^{v}=g^{v}(^{v};^{v})=g^{v}(f^{v}(^{v};^ {v});^{v}).\] (1)

In SEM, we leverage \(^{m,n}_{CL}(^{m},^{n})\) to denote a contrastive loss2

\[^{m,n}_{InfoNCE} =-_{s^{+}}[s^{+}-(e^{s^{+}} +_{s^{-}}e^{s^{-}})],\] \[^{m,n}_{RINCE} =-_{s^{+}}[ e^{q s ^{+}}-((e^{s^{+}}+_{s^{-} }e^{s^{-}}))^{q}],\] \[^{m,n}_{PSCL} =-_{s^{+}}[2 s^{+}]+ _{s^{-}}[(s^{-})^{2}],\]

 where \(s^{+}\) (\(s^{-}\)) denotes the cosine distance between the representations of positive (negative) sample pair.

\[^{m,n}=((^{m},^{n})).\] (3)

**Self-weighting** In unsupervised settings, it is hard to know which representations within \(\{^{v}\}_{i=1}^{V}\) contain useful semantic information and which are with more noise. To mitigate the representation degeneration caused by contrastive learning, SEM needs to be adaptive to quality difference among multiple views. Therefore, different from using equal-sum manner [11; 14; 17] (_e.g._, \(_{m,n}^{m,n}_{CL}\)), we propose to use the pairwise weighted multi-view contrastive loss, _i.e._, \(_{m,n}^{m,n}^{m,n}_{CL}\). Here, \(^{m,n}\) leverages the discrepancy to achieve the adaptive self-weighting. Concretely, if two views are useful pairwise views and both with informative semantics, contrastive learning between them is adaptively strengthened; if two views are unreliable pairwise views (for example, one or two of them are with less informative semantics), contrastive learning between them is adaptively weakened.

**Reconstruction regularization** In Eq. (2), \(^{v}(^{v},^{v})\) acts as a self-supervised objective to transfer as much discriminative information as possible from \(^{v}\) to \(^{v}\). When we record \(^{v}\) as the hidden features in encoder networks, the information transfer path can be described as \(^{v}^{v}^{v},v\{1,2, ,V\}\). However, information losing might occur in the processing of \(^{v}^{v}\) such that discriminative information from some views' data is lost, and thus making contrastive learning among \(\{^{v}\}_{v=1}^{V}\) focus on harmful noise instead of common semantics across multiple views. To this end, on hidden features \(^{v}\), our SEM leverages \(^{v}\) to build the reconstruction regularization \(^{v}(^{v},^{v})\) to assist contrastive learning in accessing sufficient discriminative information from raw data.

### Different Options for Implementing the SEM Framework

The crucial components of our proposed SEM as Eq. (2) include the weighting strategy \(^{m,n}\), contrastive loss \(_{CL}^{m,n}\), and regularization term \(^{v}\). Next, we concentrate on the implementations of \(^{m,n}\) (including JSD, MMD, and CMI) and briefly introduce the implementations of \(_{CL}^{m,n}\) and \(^{v}\).

Discrepancy measurements of weighting strategyWhen implementing \(^{m,n}=((^{m},^{n}))\) in Eq. (3), many methods can measure the discrepancy \((^{m},^{n})\). Firstly, we can transfer representations to a probability distribution and leverage Jensen-Shannon divergence (JSD) to compute the discrepancy \(_{JSD}(^{m},^{n})\). The advantages of JSD are its symmetry and simplicity, but it might be inapplicable when two distributions are non-overlapping. Furthermore, we can leverage maximum mean discrepancy (MMD) as the second method to obtain the discrepancy \(_{MMD}(^{m},^{n})\). MMD can effectively measure non-overlapping two distributions, but it has higher complexity than JSD3.

\[_{JSD}(^{m},^{n})=_{i=1}^{N}p( }_{i}^{m})(}_{i}^{m})}{ p(}_{i}^{m})+p(}_{i}^{n})})+_{i=1}^{N}p( }_{i}^{n})(}_{i}^{n})}{ p(}_{i}^{n})+p(}_{i}^{n})}),\]

\[_{MMD}(^{m},^{n})=}[_{i=1 }^{N}_{j=1}^{N}k(_{i}^{n},_{j}^{m})+_{i=1}^{N} _{j=1}^{N}k(_{i}^{n},_{j}^{n})-2_{i=1}^{N}_{j=1}^{ N}k(_{i}^{m},_{j}^{n})].\]

 (4)

Intuitively, discrete class information in \(^{v}\) (\(v\{m,n\}\)) is \(1\)-dimensional as well as the most representative information. Hence, we can optimize K-Means objective to extract the class information:

\[^{v*}=*{argmax}_{^{v},^{v}}\| ^{v}-^{v}^{v}\|_{F}^{2},s.t.^{v }(^{v})^{T}=_{N},^{v}\{0,1\}^{N K},\] (5)

where \(^{v}^{K z}\) denotes the \(K\) cluster centers of \(^{v}\). \(^{v*}\{0,1\}^{N K}\) is the indicator matrix that can be further transformed to \(1\)-dimensional discrete vector \(^{v}\) by defining \(y_{i}^{v}:=*{argmax}_{j}y_{ij}^{v*}\) where \(y_{i}^{v}^{v},y_{ij}^{v*}^{v*}\). In this way, the class information in \(^{m}\) and \(^{n}\) can be compressed into \(^{m}\) and \(^{n}\), respectively. Then, the class mutual information \(I(^{m};^{n})\) is normalized and the discrepancy measurement \(_{CMI}(^{m},^{n})\) between pairwise views is defined as follows:

\[_{CMI}(^{m},^{n})=^{m})+H( ^{n})}{2 I(^{m};^{n})},\] (6)

where \(H(^{m})=-_{i=1}^{N}p(y_{i}^{m}) p(y_{i}^{m})\) is the cross-entropy of \(^{m}\). This design of CMI has at least two advantages: 1) It is conducive to maintaining the representative class information while filtering out noise information; 2) Calculation is easy and owns better physical meaning.

Finally, it is also flexible to implement the negative correlation function \(\). Considering \(^{m,n} 0\), we base on the three different discrepancies and simply give the following weighting strategies:

\[^{m,n}_{CMI}=_{CMI}(_{CMI}(^{m}, ^{n}))=e^{1/_{CMI}(^{m},^{n})}-1,\] (7) \[^{m,n}_{JSD}=_{JSD}(_{JSD}( ^{m},^{n}))=e^{1-_{JSD}(^{m},^{n})}-1,\] \[^{m,n}_{MMD}=_{MMD}(_{MMD}( ^{m},^{n}))=e^{-_{MMD}(^{m},^{n})}.\]

Compatibility for contrastive learningWhen implementing the contrastive loss \(_{CL}^{m,n}\), it should be pointed out that multi-view contrastive learning usually has to handle more than two views (_i.e._,\(\{^{v}\}_{v=1}^{V},V>2\)), which is different from two-view setting (_e.g._, \(\{^{1},^{2}\}\)) in traditional contrastive learning. To make our SEM framework be compatible with previous contrastive learning methods, we construct positive/negative sample pairs as follows. Specifically, for two views \(\{_{i}^{m}^{m},_{j}^{n}^{n}\}\), the positive sample pairs are \(\{_{i}^{m},_{i}^{n}\}_{i=1,...,N}\); for any \(_{i}^{m}\), its negative sample pairs are \(\{_{i}^{m},_{j}^{v}\}_{j i}^{v=m,n}\). Cosine with a temperature parameter \(\) is leveraged to measure the representation distance between pairs, _i.e._, \(s=1/_{i}^{m},_{j}^{n}/\|_ {i}^{m}\|_{2}\|_{j}^{n}\|_{2}\). Then, we compute the contrastive loss between two views and sum all combinations as Eq. (2). We formulated three contrastive losses in Sec. 3.2, and the experiments in Sec. 4.1 will verify the compatibility of our SEM framework to them.

**Reconstruction regularization** When implementing the regularization term \(^{v}(^{v},^{v})\) in Eq. (2), we are motivated by the information encoding-decoding process [14; 19; 30], and stack a view-specific decoder \(f_{-}^{v}\) with network parameter \(^{v}\) on each view's \(^{v}\) to perform data recovery of \(^{v}\). In this way, the regularization term in SEM can be implemented with the reconstruction loss of autoencoders4

\[^{v}_{AE}(^{v},^{v}) =\|^{v}-f_{-}^{v}(^{v};^{v})\| _{F}^{2}=\|^{v}-f_{-}^{v}(f^{v}(^{v};^{v}); ^{v})\|_{F}^{2},\] (8) \[^{v}_{DAE}(^{v},}^{v}) =\|^{v}-f_{-}^{v}(}^{v};^{ v})\|_{F}^{2}=\|^{v}-f_{-}^{v}(f^{v}(^{v}+ ;^{v});^{v})\|_{F}^{2},\] \[^{v}_{MAE}(^{v},}^{v}) =\|^{v}-f_{-}^{v}(}^{v};^{ v})\|_{F}^{2}=\|^{v}-f_{-}^{v}(f^{v}(^{v} ;^{v});^{v})\|_{F}^{2},\]

 where \(^{v}+\) denotes the data disturbed by random Gaussian noise \(^{N d_{v}}\) in DAE. \(^{v}\) is the data masked by random \(0-1\) matrix \(\{0,1\}^{N d_{v}}\) in MAE. \(}^{v}\) and \(}^{v}\) denote the representations inferred from data \(^{v}+\) and \(^{v}\) in DAE and MAE, respectively. \(\{^{v}\}_{v=1}^{V},V>2\)), which is different from two-view setting (_e.g._, \(\{^{1},^{2}\}\)) in traditional contrastive learning. To make our SEM framework be compatible with previous contrastive learning methods, we construct positive/negative sample pairs as follows. Specifically, for two views \(\{_{i}^{m}^{m},_{j}^{n}^{n}\}\), the positive sample pairs are \(\{_{i}^{m},_{i}^{n}\}_{i=1,...,N}\); for any \(_{i}^{m}\), its negative sample pairs are \(\{_{i}^{m},_{j}^{v}\}_{j i}^{v=m,n}\). Cosine with a temperature parameter \(\) is leveraged to measure the representation distance between pairs, _i.e._, \(s=1/_{i}^{m},_{j}^{n}/\|_ {i}^{m}\|_{2}\|_{j}^{n}\|_{2}\). Then, we compute the contrastive loss between two views and sum all combinations as Eq. (2). We formulated three contrastive losses in Sec. 3.2, and the experiments in Sec. 4.1 will verify the compatibility of our SEM framework to them.

**Reconstruction regularization** When implementing the regularization term \(^{v}(^{v},^{v})\) in Eq. (2), we are motivated by the information encoding-decoding process [14; 19; 30], and stack a view-specific decoder \(f_{-}^{v}\) with network parameter \(^{v}\) on each view's \(^{v}\) to perform data recovery of \(^{v}\). In this way, the regularization term in SEM can be implemented with the reconstruction loss of autoencoders4

\[^{v}_{AE}(^{v},^{v}) =\|^{v}-f_{-}^{v}(^{v};^{v})\| _{F}^{2}=\|^{v}-f_{-}^{v}(f^{v}(^{v};^{v}); ^{v})\|_{F}^{2},\] (9) \[^{v}_{DAE}(^{v},}^{v}) =\|^{v}-f_{-}^{v}(}^{v};^{ v})\|_{F}^{2}=\|^{v}-f_{-}^{v}(f^{v}(^{v}+ ;^{v});^{v})\|_{F}^{2},\] \[^{v}_{MAE}(^{v},}^{v}) =\|^{v}-f_{-}^{v}(}^{v};^{ v})\|_{F}^{2}=\|^{v}-f_{-}^{v}(f^{v}(^{v} ;^{v});^{v})\|_{F}^{2},\]

 where \(^{v}+\) denotes the data disturbed by random Gaussian noise \(^{N d_{v}}\) in DAE. \(^{v}\) is the data masked by random \(0-1\) matrix \(\{0,1\}^{N d_{v}}\) in MAE. \(}^{v}\) and \(}^{v}\) denote the representations inferred from data \(^{v}+\) and \(^{v}\) in DAE and MAE, respectively. \(\{^{v}\}_{v=1}^{V}\) is the 

**Theorem 1**.: _For any three views (\(v\{m,n,0\}\)), if class mutual information only exists in two views, e.g., \(I(^{m};^{o}) 0\), \(I(^{m}_{i};^{o}) 0\), and \(I(^{m};^{n})=\), \(>0\), we have minimizing the weighted InfoNCE losses \(^{m,n}^{m}_{InfoNCE}(^{m},^{n})+ ^{m,o}^{m,o}_{InfoNCE}(^{m},^{o})+ ^{n,o}^{n,o}_{InfoNCE}(^{n},^{o})\) is equivalent to maximizing the mutual information between the two views \((e^{/ N}-1)I(^{m};^{n})\)._

Combining with the information losing of each layer through encoder networks, the following theorem further reveals that reconstruction regularization on the hidden features \(^{v}\) is conducive to alleviating the losing of discriminative semantic information through data transformation. Hence, we treat the layer output closest to \(^{v}\) in encoders as hidden features to maximize \(_{l=t^{n}+1}^{L^{m}}(1-_{l}^{m})\) and \(_{l=t^{n}+1}^{L^{n}}(1-_{l}^{n})\), aiming at maintaining useful semantic information for contrastive learning.

**Theorem 2**.: _For any two views (\(v\{m,n\}\)) with positive class mutual information, denoting \(L^{v}\) as the total layer number of the \(v\)-th view's encoder network before representation \(^{v}\), the \(l\)-th layer has the information losing rate \(_{l}^{v} 0\). If \(\) is an oracle variable that contains and only contains multiple views' discriminative semantic information, and \(^{v}\) is the \(t^{v}\)-th layer's features, we have minimizing the regularized loss \(^{m,n}^{m,n}_{InfoNCE}(^{m},^{n})+ _{v}^{v}(^{v},^{v})\) is expected to obtain \(I(;^{m};^{n})\{I(;^ {m})_{l=t^{n}+1}^{L^{n}}(1-_{l}^{m}),I(;^ {n})_{l=t^{n}+1}^{L^{n}}(1-_{l}^{n})\}\)._

## 4 Experiments

This section validates the effectiveness of our SEM. Specifically, we first conduct comparison experiments on state-of-the-art contrastive learning baselines and SEM with three options of contrastive losses (_i.e._, \(_{InfoNCE},_{PSCL},_{RINCE}\)). We then conduct ablation studies with three options of weighting strategies (_i.e._, \(_{CMI},_{JSD},_{MMD}\)), as well as with three options of reconstruction terms (_i.e._, \(_{AE},_{DAE},_{MAE}\)). Evaluation is built on the concatenation of all views' representations learned by methods. Finally, we show SEM's training process and its hyper-parameter analysis. We provided more experimental results as well as all implementation details of SEM in Appendix.

**Datasets** Our experiments employ five open-source multi-view datasets. Their information is shown in Table 1, where DHA  is a depth-included human action dataset where each action has RGB and depth features; CCV  refers to the columbia consumer video database whose samples are described with SIFT, STIP, and MFCC features; NUSWIDE  collects web images with multiple views (color histogram, block-wise color moments, color correlogram, edge direction histogram, and wavelet texture); Caltech  is a widely-used image dataset which leverages six views (Gabor, Wavelet moments, CENTRIST, HOG, GIST, and LBP) to represent samples; YoutubeVideo  is a large-scale dataset where each sample has three views including cuboids histogram, HOG, and vision misc. These datasets are diverse in forms and are often organized to comprehensively evaluate the performance of multi-view methods.

### Comparison Experiments on Contrastive Learning

**Baselines** K-Means-BSV denotes K-Means clustering results on the best single-view of raw data, and we leverage this baseline to investigate the representation degeneration in comparison methods. InfoNCE , PSCL , and RINCE  are three kinds of CL methods. Since their original versions are designed to handle single views, we extended them to multi-view scenarios as did in [11; 17]. CMC , DCP , MFLVC , and DSIMVC  are four kinds of MCL methods. We evaluate our SEM with different contrastive losses (_i.e._, SEM+InfoNCE, SEM+PSCL, and SEM+RINCE), where the weighting strategy and reconstruction term are fixed to \(_{CMI}\) and \(_{AE}\), respectively.

We leverage the linear clustering method K-Means to evaluate the performance of learning representations and report the average results of 10 runs in Table 2. The results indicate that: **I)** Our SEM framework is compatible with different contrastive losses (_e.g._, InfoNCE, PSCL, and RINCE) and we can clearly observe that SEM+InfoNCE/PSCL/RINCE successfully improve the baselines for large margins. For instance, SEM+InfoNCE respectively outperforms InfoNCE by about \(25\%\), \(13\%\), \(4\%\), \(7\%\), \(12\%\) ACC on the five datasets. **II)** MCL approaches could access the semantic information from multiple views, and thus outperforming that from single views. However, a side effect is

  Name & View & Size & Class \\  DHA & 2 & 483 & 23 \\ CCV & 3 & 6,773 & 20 \\ NUSWIDE & 5 & 5,000 & 5 \\ Caltech & 6 & 1,400 & 7 \\ YoutubeVideo & 3 & 101,499 & 31 \\  

Table 1: Information of datasets

[MISSING_PAGE_FAIL:8]

\(_{CMI},_{JSD},_{MMD}\)), where the contrastive loss and reconstruction term are fixed to \(_{InfoNCE}\) and \(_{AE}\), respectively. Compared with SEM w/o \(\) (this setting is the reconstruction regularized multi-view contrastive learning) that equally treats contrastive learning between any two views, SEM w/ \(_{CMI/JSD/MMD}\) can adaptively weight the contrastive learning according to specific two views and thus all these three variants of SEM obtain significant improvements. For example, SEM w/ \(_{MMD}\) has a 13.1\(\%\) improvement on DHA and SEM w/ \(_{CMI}\) has a 5.9\(\%\) improvement on CCV. Results on more datasets and time costs are shown in Appendix C, where we find that the proposed weighting strategy of class mutual information \(_{CMI}\) generally achieves the best performance on accuracy and time consumption among the three options of weighting strategy.

Table 4 reports the linear clustering performance (evaluated by ACC) of our SEM framework without reconstruction regularization (_i.e._, SEM w/o \(\)) and that with three reconstruction terms (_i.e._, \(_{AE},_{DAE},_{MAE}\)), where the contrastive loss and weighting strategy are fixed to \(_{InfoNCE}\) and \(_{CMI}\), respectively. We can easily find that the proposed SEM with reconstruction terms obviously outperforms that without reconstruction terms. For instance, compared with SEM w/o \(\), SEM w/ \(_{MAE}\) has 22.5\(\%\) and 10.8\(\%\) improvements on DHA and CCV, respectively. This is because the reconstruction regularization makes the hidden features \(\{^{v}\}_{v=1}^{V}\) avoid losing discriminative information, which promotes the multi-view contrastive learning performed on subsequent \(\{^{v}\}_{v=1}^{V}\). Meanwhile, SEM w/ \(_{MAE}\) and SEM w/ \(_{DAE}\) perform better than SEM w/ \(_{AE}\). This is because, compared with vanilla AE, DAE or MAE (by adding noise or masking on raw data) can make our model more conducive to removing semantic-irrelevant noise as well as capturing hidden patterns.

### Experimental Analysis on Mechanism of SEM

This part presents the visualization and analysis on SEM to give an intuition of its behavior and mechanism, where the combination of \(_{InfoNCE}\)+\(_{CMI}\)+\(_{AE}\) is taken as an example.

Let's first recall the views of Caltech dataset in Figure 2(a), we can consider that view 4 and view 5 are high-quality views, while view 1 is a low-quality view. The performance relation among them is \(_{view}\)\(4>_{view}\)\(5>_{view}\)\(1\). In Figure 2(c), view 4's representation degeneration occurs.

Figure 5 shows the pairwise weights, losses, and clustering accuracy on Caltech dataset during SEM's training process, where 1 iteration corresponds to 100 epochs, _i.e._, the step size is set to 100 epochs. Our SEM is a self-weighted multi-view contrastive learning framework that automatically infers different weights for different pairwise views as shown in Figure 5(a), where we can observe that weights \(^{4,5}>^{1,4}\) and they were dynamically updated for 4 times. As a result, contrastive learning between view 4 and view 5 is strengthened by \(^{4,5}\), while contrastive learning between view 1 and view 4 is weakened by \(^{1,4}\). Meanwhile, loss \(_{InfoNCE}^{4,5}\) is minimized earlier than loss \(_{InfoNCE}^{1,4}\) as shown in Figure 5(b). In other words, since the mutual effect between view 4 and view 5 is strengthened, the effect of view 1 on view 4/view 5 is weakened such that view 4/view 5 does not degenerate. At the same time, the effect of view 4/view 5 on view 1 remains and promotes the representation learning of view 1. Consequently, all views' performance in Figure 5(c) increases through our SEM, and the representation degeneration of view 4 occurring in Figure 2(c) is mitigated.

Figure 5: (a) The change trend of weights \(^{1,4}\) and \(^{1,5}\) in SEM. (b) Loss values \(_{InfoNCE}^{1,4}\) and \(_{InfoNCE}^{4,5}\) during contrastive learning. (c) Clustering accuracy on the learned representations of view 1, view 4, and view 5.

Hyper-parameter analysisSince different datasets have different levels of reconstruction errors, the trade-off coefficient \(\) is introduced to balance the contrastive learning and information recovery in our SEM framework. In Figure 6(a), we change \(\) within the range of \([10^{-3},10^{-2},10^{-1},10^{0},10^{1},10^{2},10^{3}]\) and report the clustering accuracy tested on representations. The experimental results indicate that SEM is not sensitive to \(\) in \([10^{-1},10^{1}]\). In our experiments, \(\) is consistently set to \(1\) for all the five datasets. Regarding self-supervised learning, frameworks with fewer manually set hyper-parameters might be more convenient for their applications.

Additionally, we investigate the effect of cluster number when the weight strategy of our SEM framework is selected as \(_{CMI}\) which needs to pre-define the cluster number when applying K-Means algorithm. As shown in Figure 6(b), when computing the class mutual information, we change the number of clusters within the range of \([K/2,K,2K,4K]\) where \(K\) denotes the truth class number of multi-view datasets. Compared with \(K\), \(K/2\) leads to more coarse-grained class mutual information, while \(2K\) and \(4K\) come in more fine-grained class mutual information. The experimental results demonstrate that SEM with \(_{CMI}\) is not sensitive to the choices of cluster number.

## 5 Conclusion

In this paper, we showcase that the representation degeneration could seriously limit the application of contrastive learning in multi-view scenarios. To mitigate this issue, we propose self-weighted multi-view contrastive learning with reconstruction regularization (SEM), which is a general framework that is compatible with different options of the contrastive loss, weighting strategy, and reconstruction term. Theoretical and experimental analysis verified the effectiveness of SEM, and it can significantly improve many existing contrastive learning methods in multi-view scenarios. Moreover, ablation studies indicated that SEM is effective with different weighting strategies and reconstruction terms.

Our future work is to extend the proposed SEM to be useful not only for multi-view scenarios, but also for other contrastive learning based domains, such as contrastive learning in sequences. Conceptually, the limitation of the self-weighting strategy is that it is more effective when there are over two views. When there are only two views, the self-weighted multi-view contrastive learning framework transforms into traditional contrastive learning but with reconstruction regularization. Therefore, another future work is to extend the view-level weighting of SEM to sample-level weighting.