# AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset

Jiakang Yuan\({}^{1,*}\), Bo Zhang\({}^{2,}\), Xiangchao Yan\({}^{2}\), Tao Chen\({}^{1,}\), Botian Shi\({}^{2}\), Yikang Li\({}^{2}\), Yu Qiao\({}^{2}\)

\({}^{1}\)School of Information Science and Technology, Fudan University

\({}^{2}\)Shanghai Artificial Intelligence Laboratory

jkyuan22@m.fudan.edu.cn, zhangbo@pjlab.org.cn, eetchen@fudan.edu.cn

This work was done when Jiakang Yuan was an intern at Shanghai Artificial Intelligence Laboratory.Corresponding to Tao Chen and Bo Zhang.

###### Abstract

It is a long-term vision for Autonomous Driving (AD) community that the perception models can learn from a large-scale point cloud dataset, to obtain unified representations that can achieve promising results on different tasks or benchmarks. Previous works mainly focus on the self-supervised pre-training pipeline, meaning that they perform the pre-training and fine-tuning on the same benchmark, which is difficult to attain the performance scalability and cross-dataset application for the pre-training checkpoint. In this paper, for the first time, we are committed to building a large-scale pre-training point-cloud dataset with diverse data distribution, and meanwhile learning generalizable representations from such a diverse pre-training dataset. We formulate the point-cloud pre-training task as a semi-supervised problem, which leverages the few-shot labeled and massive unlabeled point-cloud data to generate the unified backbone representations that can be directly applied to many baseline models and benchmarks, decoupling the AD-related pre-training process and downstream fine-tuning task. During the period of backbone pre-training, by enhancing the scene- and instance-level distribution diversity and exploiting the backbone's ability to learn from unknown instances, we achieve significant performance gains on a series of downstream perception benchmarks including Waymo, nuScenes, and KITTI, under different baseline models like PV-RCNN++, SECOND, CenterPoint. Project page: [https://jiakangyuan.github.io/AD-PT.github.io/](https://jiakangyuan.github.io/AD-PT.github.io/).

## 1 Introduction

LiDAR sensor plays a crucial role in the Autonomous Driving (AD) system due to its high quality for modeling the depth and geometric information of the surroundings. As a result, a lot of studies aim to achieve the AD scene perception via LiDAR-based 3D object detection baseline models, such as CenterPoint , PV-RCNN , and PV-RCNN++ .

Although the 3D object detection models can help autonomous driving recognize the surrounding environment, the existing baselines are hard to generalize to a new domain (such as different sensor settings or unseen cities). A long-term vision of the autonomous driving community is to develop a scene-generalizable pre-trained model, which can be widely applied to different downstream tasks. To achieve this goal, researchers begin to leverage the Self-Supervised Pre-Training (SS-PT) paradigm recently. For example, ProposalContrast  proposes a contrastive learning-based approach to enhance region-level feature extraction capability. Voxel-MAE  designs a masking and reconstructing task to obtain a stronger backbone network.

However, as illustrated in Fig.1, it should be pointed out that there is a crucial difference between the above-mentioned SS-PT and the desired Autonomous Driving Pre-Training (AD-PT) paradigm. The SS-PT aims to learn from a single set of unlabeled data to generate suitable representations for the **same** dataset, while AD-PT is expected to learn unified representations from as large and diversified data as possible, so that the learned features can be easily transferred to various downstream tasks. As a result, SS-PT may only perform well when the test and pre-training data are sampled from the same single dataset (such as Waymo  or nuScenes ), while AD-PT presents better generalized performance on different datasets, which can be continuously improved with the increase of the number of the pre-training dataset.

Therefore, this paper is focused on achieving the AD-related pre-training which can be easily applied to different baseline models and benchmarks. By conducting extensive experiments, we argue that there are two key issues that need to be solved for achieving the real AD-PT: 1) how to build a unified AD dataset with diverse data distribution, and 2) how to learn generalizable representations from such a diverse dataset by designing an effective pre-training method.

For the first item, we use a large-scale point cloud dataset named ONCE , consisting of few-shot labeled (_e.g._, \(\)0.5%) and massive unlabeled data. First, to get accurate pseudo labels of the massive unlabeled data that can facilitate the subsequent pre-training task, we design a class-wise pseudo labeling strategy that uses multiple models to annotate different semantic classes, and then adopt semi-supervised methods (_e.g._, MeanTeacher ) to further improve the accuracy on the ONCE validation set. Second, to get a unified dataset with diverse raw data distribution from both LiDAR beam and object sizes, inspired by previous works [29; 24; 33; 34], we exploit point-to-beam playback re-sampling and object re-scaling strategies to diversify both scene- and region-level distribution.

For the second item, we find that the taxonomy differences between the pre-training ONCE dataset and different downstream datasets are quite large, resulting in that many hard samples with taxonomic inconsistency are difficult to be accurately detected during the fine-tuning stage. As a result, taxonomy differences between different benchmarks should be considered when performing the backbone pre-training. Besides, our study also indicates that during the pre-training process, the backbone model tends to fit with the semantic distribution of the ONCE dataset, impairing the perception ability on downstream datasets having different semantics. To address this issue, we propose an unknown-aware instance learning to ensure that some background regions on the pre-training dataset, which may be important for downstream datasets, can be appropriately activated by the designed pre-training task. Besides, to further mine representative instances during the pre-training, we design a consistency loss to constrain the pre-training representations from different augmented views to be consistent.

Our contributions can be summarized as follows:

1. For the first time, we propose the AD-PT paradigm, which aims to learn unified representations by pre-training a general backbone and transfers knowledge to various benchmarks.
2. To enable the AD-PT paradigm, we propose a diversity-based pre-training data preparation procedure and unknown-aware instance learning, which can be employed in the backbone pre-training process to strengthen the representative capability of extracted features.
3. Our study provides a more unified approach, meaning that once the pre-trained checkpoint is generated, it can be directly loaded into multiple perception baselines and benchmarks. Results further verify that such an AD-PT paradigm achieves large accuracy gains on different benchmarks (_e.g._, \(3.41\%\), \(8.45\%\), \(4.25\%\) on Waymo, nuScenes, and KITTI).

Figure 1: Differences between previous pre-training paradigm and the proposed AD-PT paradigm.

## 2 Related Works

### LiDAR-based 3D Object Detection

Current prevailing LiDAR-based 3D object detection works [11; 16; 17; 19; 27; 18; 4] can be roughly divided into point-based methods, voxel-based methods, and point-voxel-based methods. Point-based methods [16; 30] extract features and generate proposals directly from raw point clouds. PointRCNN  is a prior work that generates 3D RoIs with foreground segmentation and designs an RCNN-style two-stage refinement. Unlike point-based methods, voxel-based methods [27; 32] first transform unordered points into regular grids and then extract 3D features using 3D convolution. As a pioneer, SECOND  utilizes sparse convolution as 3D backbone and greatly improves the detection efficiency. CenterPoint  takes care of both accuracy and efficiency and proposes a one-stage method. To take advantage of both point-based and voxel-based methods, PV-RCNN  and PV-RCNN++  propose a point-voxel set abstraction to fuse point and voxel features.

### Autonomous Driving-related Self-Supervised Pre-Training

Inspired by the success of pre-training in 2D images, self-supervised learning methods have been extended to LiDAR-based AD scenarios [6; 13; 12; 28; 26; 10; 31]. Previous methods mainly focus on using contrastive learning or masked autoencoder (MAE) to enhance feature extraction. Contrastive learning-based methods [12; 31; 7] use point clouds from different views or temporally-correlated frames as input, and further construct positive and negative samples. STRL  proposes a spatial-temporal representation learning that employs contrastive learning with two temporally-correlated frames. GCC-3D  and ProposalContrast constructs a consistency map to find the correspondence between different views, developing contrastive learning strategies at region-level. CO3  utilizes LiDAR point clouds from the vehicle- and infrastructure-side to build different views. MAE-based [6; 10] methods utilize different mask strategies and try to reconstruct masked points by a designed decoder. Voxel-MAE  introduces a voxel-level mask strategy and verifies the effectiveness of MAE. BEV-MAE  designs a BEV-guided masking strategy. More recently, GD-MAE  and MV-JAR  explore masking strategies in transformer architecture. Different from the existing works that use a designed self-supervised approach to pre-train on unlabeled data and then fine-tune on labeled data with the same dataset, AD-PT aims to pre-train on a large-scale point cloud dataset and fine-tune on multiple different datasets (_i.e._, Waymo , nuScenes, KITTI ).

Figure 2: The overview of the proposed AD-PT. By leveraging the proposed method to train on the unified large-scale point cloud dataset, we can obtain well-generalized pre-training parameters that can be applied to multiple datasets and support different baseline detectors.

## 3 Method

To better illustrate our AD-PT framework, we first briefly describe the problem definition and overview of the proposed method in Sec. 3.1. Then, we detail the data preparation and model design of AD-PT in Sec. 3.2 and Sec. 3.3, respectively.

### Preliminary

**Problem Formulation.** Different from previous self-supervised pre-training methods, AD-PT performs large-scale point cloud pre-training in a **semi-supervised manner.** Specifically, we have access to \(N\) samples which are composed of a labeled set \(D_{L}=\{(x_{i},y_{i})\}_{i=1}^{N_{L}}\) and a unlabeled set \(D_{U}=\{x_{i}\}_{i=1}^{N_{U}}\), where \(N_{L}\) and \(N_{U}\) denote the number of labeled and unlabeled samples. Note that \(N_{L}\) can be much smaller than \(N_{U}\) (_e.g.,_\(\)5K _v.s._\(\)1M, about 0.5\(\%\) labeled samples). The purpose of our work is to pre-train on a large-scale point cloud dataset with few-shot \(D_{L}\) and massive \(D_{U}\), such that the pre-trained backbone parameters can be used for many down-stream benchmark datasets or 3D detection models.

**Overview of AD-PT.** As shown in Fig. 2, AD-PT mainly consists of a large-scale point cloud dataset preparation procedure and a unified AD-focused representation learning procedure. To initiate the pre-training, a **class-aware pseudo labels generator** is first developed to generate the pseudo labels of \(D_{U}\). Then, to get more diverse samples, we propose a **diversity-based pre-training processor**. Finally, in order to pre-train on these pseudo-labeled data to learn their generalizable representations for AD purposes, an **unknown-aware instance learning** coupled with a consistency loss is designed.

### Large-scale Point Cloud Dataset Preparation

In this section, we detail the preparation of the unified large-scale dataset for AD. As shown in Fig. 4, our data creation consists of a class-aware pseudo label generator and a diversity-based pre-training processor to be introduced below.

#### 3.2.1 Class-aware Pseudo Labels Generator

It can be observed from Tab. 7 that, pseudo-labels with high accuracy on the pre-training dataset are beneficial to enhance the detection accuracy on downstream datasets such as Waymo  and nuScenes . Therefore, to get more accurate pseudo labels, we design the following procedure.

**Class-aware Pseudo Labeling.** ONCE benchmark3 is utilized to evaluate the pseudo labeling accuracy and more results are shown in the supplementary material. We find that different baseline

   Detector & Head Choice & Vehicle & Pedestrian & Cyclist \\  ONCE Benchmark (Best) & Center Head & 66.79 & 49.90 & 63.45 \\ CenterPoint (ours) & Center Head & & **56.01** & & \\ PV-3CNN++ (ours) & Anchor Head & **82.50** & & & **71.19** \\   

Table 1: Performance using different detectors on ONCE validation set. We report mAP using ONCE evaluation metric.

   ONCE labeled set &  \\ Vehicle & Ped. & Cyclist & Vehicle & Ped. & Cyclist \\ 
19.01 & 4.52 & 5.63 & 15.67 & 1.63 & 1.90 \\   

Table 2: Statistics on the number of pseudo-labeled instances per frame. We compare it with ONCE labeled data set.

Figure 4: Statistics of object re-scaling.

Figure 3: Overall dataset preparation procedure. Figure 4: Statistics of object re-scaling.

models have different biased perception abilities for different classes. For example, the center-based method (_i.e._, CenterPoint ) tends to detect better for small-scale targets (_e.g._, Pedestrian), while anchor-based methods perform better for other classes. According to the observation, we utilize the PV-RCNN++  with anchor-head to annotate Vehicle and Cyclist classes on ONCE, where the accuracy on Vehicle and Cyclist is found to be much better than the methods listed in the ONCE benchmark. On the other hand, CenterPoint  is employed to label the Pedestrian class on ONCE. Finally, we use PV-RCNN++ and CenterPoint to perform a class-wise pseudo labeling process.

**Semi-supervised Data Labeling.** We further employ the semi-supervised learning method to fully exploit the unlabeled data to boost the accuracy of the pseudo labels. As shown in ONCE benchmark, MeanTeacher  can better improve the performance on ONCE, and therefore, we use the MeanTeacher to further enhance the class-wise detection ability. Tab. 1 shows that the accuracy on ONCE validation set can be improved after leveraging massive unlabeled data, significantly surpassing all previous records.

**Pseudo Labeling Threshold.** To avoid labeling a large number of false positive instances, we set a relatively high threshold. In detail, for Vehicle, Pedestrian, and Cyclist, we filter out bounding boxes with confidence scores below 0.8, 0.7, and 0.7. As a result, it can be seen from Tab. 2 that, compared with ONCE labeled data, some hard samples with relatively low prediction scores are not annotated.

#### 3.2.2 Diversity-based Pre-training Processor

As mentioned in , the diversity of data is crucial for pre-training, since highly diverse data can greatly improve the generalization ability of the model. The same observation also holds in 3D pre-training. However, the existing datasets are mostly collected by the same LiDAR sensor within limited geographical regions, which impairs the data diversity. Inspired by , discrepancies between different datasets can be categorized into scene-level (_e.g._, LiDAR beam) and instance-level (_e.g._, object size). Thus, we try to increase the diversity from the LiDAR beam and object size, and propose a point-to-beam playback re-sampling and an object re-scaling strategy.

**Data with More Beam-Diversity: Point-to-Beam Playback Re-sampling.** To get beam-diverse data, we use the range image as an intermediate variable for point data up-sampling and down-sampling. Specifically, given a LiDAR point cloud with \(n\) beam (_e.g._, 40 beam for ONCE dataset) and \(m\) points per ring, the range image \(R^{n m}\) can be obtained by the following equation:

\[r=+y^{2}+z^{2}},\ \ =arctan(x/y),\ \ =arcsin(z/r), \]

where \(\) and \(\) are the inclination and azimuth of point clouds, respectively, and \(r\) denotes the range of the point cloud. Each column and row of the range image corresponds to the same azimuth and inclination of the point clouds, respectively. Then, we can interpolate or sample over rows of a range image, which can also be seen as LiDAR beam re-sampling. Finally, we reconvert the range image to point clouds as follows:

\[x=rcos()cos(),\ \ y=rcos()sin(),\ \ z=rsin(), \]

where \(x\), \(y\), \(z\) denote the Cartesian coordinates of points, and Fig. 5 shows that point-to-beam playback re-sampling can generate scenes with different point densities, improving the scene-level diversity.

**Data with More RoI-Diversity: Object Re-scaling.** According to the statement in  that different 3D datasets were collected in different locations, the object size has inconsistent distributions. As shown in Fig. 4, a single dataset such as ONCE cannot cover a variety of object-size distributions,

Figure 5: Visualization of point-to-beam playback re-sampling.

resulting in that the model cannot learn a unified representation. To overcome such a problem, we propose an object re-scaling mechanism that can randomly re-scale the length, width and height of each object. In detail, given a bounding box and points within it, we first transform the points to the local coordinate and then multiply the point's coordinates and the bounding box size by a provided scaling factor. Finally, we transform the scaled points with the bounding box to the ego-car coordinate. It can be seen from Fig. 4 that after object re-scaling, the produced dataset contains object sizes with more diversified distributions, further strengthening the instance-level point diversity.

### Learning Unified Representations under Large-scale Point Cloud Dataset

By obtaining a unified pre-training dataset using the above-mentioned method, the scene-level and instance-level diversity can be improved. However, unlike 2D or vision-language pre-training datasets which cover a lot of categories to be identified for downstream tasks, our pseudo dataset has limited category labels (_i.e._, Vehicle, Pedestrian and Cyclist). Besides, as mentioned in Sec. 3.2.1, in order to get accurate pseudo annotations, we set a high confidence threshold, which may inevitably ignore some hard instances. As a result, these ignored instances, which are not concerned in the pre-training dataset but may be seen as categories of interest in downstream datasets (_e.g._, Barrier in the nuScenes dataset), will be suppressed during the pre-training process.

To mitigate such a problem, it is necessary that both pre-training-related instances and some unknown instances with low scores can be considered when performing the backbone pre-training. From a new perspective, we regard the pre-training as an open-set learning problem. Different from traditional open-set detection  which aims at detecting unknown instances, our goal is to activate as many foreground regions as possible during the pre-training stage. Thus, we propose a two-branch unknown-aware instance learning head to avoid regarding potential foreground instances as the background parts. Further, a consistency loss is utilized to ensure the consistency of the calculated corresponding foreground regions.

**Overall Model Structure.** In this part, we briefly introduce our pre-training model structure. Following the prevailing 3D detectors , as shown in Fig. 2, the designed pre-training model consists of a voxel feature extractor, a 3D backbone with sparse convolution, a 2D backbone and the proposed head. Specifically, given point clouds \(^{N(3+d)}\), we first transform points into _different views_ through different data augmentation methods \(_{1}\) and \(_{2}\). Then, voxel features are extracted by a 3D backbone and mapped to BEV space. After that, dense features generated by a 2D backbone can be obtained, and finally, the dense features are fed into the proposed head.

**Unknown-aware Instance Learning Head.** Inspired by previous open-set detection works , we consider _background region proposals_ with relatively high objectness scores to be unknown instances that are ignored during the pre-training stage but may be crucial to downstream tasks, where the objectness scores are obtained from the Region Proposal Network (RPN). However, due to that these unknown instances contain a lot of background regions, directly treating these instances as the foreground instances during the pre-training will cause the backbone network to activate a large number of background regions. To overcome such a problem, we utilize a two-branch head as a committee to discover which regions can be effectively presented as foreground instances. Specifically, given the RoI features \(^{_{1}}=[f_{1}^{_{1}};f_{2}^{_{1}};...;f_{1}^{ _{1}}]^{N C}\), \(^{_{2}}=[f_{1}^{_{2}};f_{2}^{_{2}};...;f_{N}^{ _{2}}]^{N C}\) and their corresponding bounding boxes \(^{_{1}}^{N N}\), \(^{_{2}}^{N}\), where \(N\) is the number of RoI features and \(C\) denotes the dimension of features, we first select \(M\) features \(}^{_{1}}^{M C}\), \(}^{_{2}}^{M C}\) and its corresponding bounding boxes \(}^{_{1}}^{N}\), \(}^{_{2}}^{N}\) with the highest scores. Then, to obtain the positional relationship corresponding to the activation regions of the two branches, we calculate the distance of the box center between \(}^{_{1}}\) and \(}^{_{2}}\), and the feature correspondence can be obtained by the following equation:

\[(}^{_{1}},}^{_{2}})=\{( _{i}^{_{1}},_{j}^{_{2}})|^{_{1}}-c_{ j,x}^{_{2}})^{2}+(c_{i,y}^{_{1}}-c_{j,y}^{_{2}})^{2}+(c_{i,z}^{ _{1}}-c_{j,z}^{_{2}})^{2}}<\}, \]

where \((c_{i,x}^{_{1}},c_{i,y}^{_{1}},c_{i,z}^{_{1}})\) and \((c_{j,x}^{_{2}},c_{j,y}^{_{2}},c_{j,z}^{_{2}})\) denote \(i\)-th and \(j\)-th box center of \(}^{_{1}}\) and \(}^{_{2}}\), \(\) is a threshold. Once the correspondence features from different input views are obtained, these unknown instances will be updated as the foreground instances that can be fed into their original class head.

**Consistency Loss.** After obtaining the corresponding activation features of different branches, a consistency loss is utilized to ensure the consistency of the corresponding features as follows:\[_{consist}=_{i=1}^{B}_{j=1}^{K}(_{j}^{_{1 }}-_{j}^{_{2}})^{2}, \]

where \(B\) is the batch size and \(K\) is the number of the corresponding activation features.

**Overall Function.** The overall loss function can be formulated as:

\[_{total}=_{cls}+_{reg}+_{consist}, \]

where \(_{cls}\) and \(_{reg}\) represent classification loss and regression loss of the dense head, respectively, and \(_{consist}\) is the activation consistent loss as shown in Eq. 4.

## 4 Experiments

### Experimental Setup

**Pre-training Dataset.** ONCE  is a large-scale dataset collected in many scenes and weather conditions. ONCE contains 581 sequences composed of 20 labeled (\(\)19k frames) and 561 unlabeled sequences(\(\)1M frames). The labeled set divides into a train set with 6 sequences (\(\)5K samples), a validation set with 4 sequences (\(\)3k frames), and a test set with 10 sequences (\(\)8k frames). We merge Car, Bus, Truck into a unified category (_i.e._, Vehicle) when performing the pre-training. Our main results are based on ONCE small split and use a larger split to verify the pre-training scalability.

**Description of Downstream Datasets. _1) Waymo Open Dataset_** contains \(\)150k frames. _2) nuScenes Dataset_** provides point cloud data from a 32-beam LiDAR consisting of 28130 training samples and 6019 validation samples. _3) KITTI Dataset_** includes 7481 training samples and is divided into a train set with 3712 samples and a validation set with 3769 samples.

**Description of Selected Baselines.** In this paper, we compare our method with several baseline methods including both Self-Supervised Pre-Training (SS-PT) and semi-supervised learning methods. _SS-PT methods_: as mentioned in Sec. 2.2, We mainly compare with contrastive learning-based (_i.e._, GCC-3D , ProposalContrast ) and MAE-based SS-PT methods (_i.e._, Voxel-MAE , BEV-MAE ). All these methods are pre-trained and fine-tuned on the same dataset. _Semi-supervised learning methods_: to verify the effectiveness of our method under the same experimental setting, we also compare with commonly-used semi-supervised technique (_i.e._, MeanTeacher ).

    &  &  &  \\   & & & & Overall & Vehicle & Pedestrian & Cyclist \\  From scratch (SECOND) & - & 3\% & 52.00 / 37.70 & 58.11 / 57.44 & 51.34 / 27.38 & 46.57 / 28.28 \\ From scratch (SECOND) & - & 20\% & 60.62 / 56.86 & 64.26 / 63.73 & 59.72 / 50.38 & 57.87 / 56.48 \\ ProposalContrast (SECOND)  & SS-PT & 20\% & 60.91 / 57.16 & 64.50 / 63.69 & **60.33** / 51.00 & 57.90 / 56.60 \\ BEV-MAE (SECOND)  & SS-PT & 20\% & 61.03 / 57.30 & 64.42 / 63.87 & 59.97 / 50.65 & 58.69 / 57.39 \\ MeanTeacher (SECOND)  & Semi & 20\% & 60.93 / 57.31 & 64.22 / 63.73 & 59.54 / 50.80 & 58.66 / 57.41 \\ Ours (SECOND) & AD-PT & 3\% & 55.41 / 51.78 & 60.53 / 59.93 & 54.91 / 47.58 & 50.79 / 49.65 \\ Ours (SECOND) & AD-PT & 20\% & **61.26 / 57.69** & **64.54 / 64.00** & 60.25 / **51.21** & **59.00 / 57.86** \\  From scratch (CenterPoint) & - & 3\% & 59.00 / 56.25 & 57.12 / 56.57 & 58.66 / 52.44 & 61.24 / 59.89 \\ From scratch (CenterPoint) & - & 20\% & 66.47 / 64.01 & 64.91 / 64.42 & 66.03 / 60.34 & 68.49 / 67.28 \\ GCC-3D (CenterPoint)  & SS-PT & 20\% & 65.29 / 62.79 & 63.97 / 63.47 & 64.23 / 58.47 & 67.68 / 66.44 \\ ProposalContrast (CenterPoint)  & SS-PT & 20\% & 66.67 / 64.20 & 65.22 / 64.80 & 66.40 / 60.49 & 68.48 / 67.38 \\ BEV-MAE (CenterPoint)  & SS-PT & 20\% & 66.92 / 64.45 & 64.78 / 64.29 & 66.25 / 60.53 & **69.73 / 68.52** \\ MeanTeacher (CenterPoint)  & Semi & 20\% & 66.66 / 64.23 & 64.94 / 64.43 & 66.35 / 60.61 & 68.69 / 67.65 \\ Ours (CenterPoint) & AD-PT & 3\% & 61.21 / 58.46 & 60.35 / 59.79 & 60.57 / 54.02 & 62.73 / 61.57 \\ Ours (CenterPoint) & AD-PT & 20\% & **67.17 / 64.65** & **65.33** / **64.83** & **67.16 / 62.0** & 69.39 / 68.25 \\  From scratch (PV-RCNN++) & - & 3\% & 63.81 / 61.10 & 64.42 / 63.93 & 64.33 / 57.79 & 62.69 / 61.59 \\ From scratch (PV-RCNN++) & 20\% & 69.97 / 67.58 & 69.18 / 68.75 & 70.88 / 65.21 & 69.84 / 68.77 \\ ProposalContrast (PV-RCNN++)  & SS-PT & 20\% & 70.30 / 67.78 & 69.45 / 69.00 & 71.42 / 65.68 & 70.04 / 69.05 \\ BEV-MAE (PV-RCNN++)  & SS-PT & 20\% & 70.54 / 68.11 & 69.21 / 68.71 & 71.96 / 66.42 & 70.17 / 69.21 \\ MeanTeacher (PV-RCNN++)  & Semi & 20\% & 70.62 / 68.14 & 69.21 / 68.71 & 71.96 / 66.42 & 70.16 / 67.00 \\ Ours (PV-RCNN++) & AD-PT & 3\% & 68.33 / 65.69 & 68.17 / 67.70 & 68.82 / 62.39 & 68.00 / 67.00 \\ Ours (PV-RCNN++) & AD-PT & 20\% & **71.55** / **69.23** & **70.62** / **70.19** & **72.36** / **66.82** & **71.69** / **70.70** \\   

Table 3: Fine-tuning performance on Waymo benchmark (LEVEL_2 metric). Note that we only use a single checkpoint parameter to initialize all downstream baselines including SECOND, CenterPoint, PV-RCNN++. Semi denotes the semi-supervised method training on unlabeled ONCE split.

**Implementation Details.** We fine-tune the model on several different detectors including SECOND , CenterPoint  and PV-RCNN++ . Note that the transformer-based detectors are not used, since we consider directly applying the pre-trained backbone parameters to _the most commonly used downstream baselines_. For different views generation, we consider 3 types of data augmentation methods, including random rotation ([-180\({}^{}\), 180\({}^{}\)]), random scaling ([0.7, 1.2]), and random flipping along X-axis and Y-axis. We use Adam optimizer with one-cycle learning rate schedule and the maximum learning rate is set to 0.003. We pre-train for 30 epochs on ONCE small split and large split using 8 NVIDIA Tesla A100 GPUs. The number of selected RoI features is set to 256 and the cross-view matching threshold \(\) is set to 0.3. Our code is based on 3DTrans .

**Evaluation Metric.** We use dataset-specific evaluation metrics to evaluate fine-tuning performance on each downstream dataset. _For Waymo_, average precision (AP) and average precision with heading (APH) are utilized for three classes (_i.e._, Vehicle, Pedestrian, Cyclist). Following , we mainly focus on the more difficult LEVEL_2 metric. _For nuScenes_, we report mean average precision (mAP) and NuScenes Detection Score (NDS). _For KITTI_, we use mean average precision (mAP) with 40 recall to evaluate the detection performance and report AP\({}_{}\) results.

### Main Results

**Results on Waymo.** Results on Waymo validation set are shown in Tab. 3. We first compare the proposed method with previous SS-PT methods. It can be seen that all three detectors achieve the best results using AD-PT initialization, surpassing previous SS-PT methods even using a smaller pre-training dataset (_i.e._, \(\)100k frames on ONCE small split). For example, the improvement achieved by PV-RCNN++ is \(1.58\%\) / \(1.65\%\) in terms of L2 AP / APH. Note that the compared SS-PT

    &  &  &  \\   & & & Overall & Vehicle & Pedestrian & Cyclist & mAP & NDS \\  Baseline & None & 67.12 & 64.55 & 67.45 & 66.97 & 67.74 & 61.15 & 66.19 & 65.24 & 36.26 & 45.04 \\ Baseline+re-scaling & Object-size & 67.39 & 64.68 & 67.52 & 67.03 & 67.82 & 61.24 & 66.83 & 65.79 & 39.72 & 49.93 \\ Baseline+re-sampling & LiDAR-beam & 67.37 & 64.70 & 67.70 & 67.21 & 68.21 & 61.71 & 66.15 & 65.18 & 41.35 & 51.03 \\ Baseline+re-scaling-re-sampling & Both & **67.77** & **65.09** & **68.01** & **67.61** & **68.32** & **61.69** & **66.99** & **65.98** & **43.11** & **52.41** \\   

Table 6: Ablation study on data preparation.

   Method & Setting & D.A. & mAP & NDS & Car & Truck & CV. & Bus & Trailer & Barrier & Motor. & Bicycle & Ped. & TC. \\  From scratch (SECOND) & - & 5\% & 29.24 & 39.74 & 67.69 & 33.02 & 7.15 & 45.91 & 17.67 & 25.23 & 11.92 & 0.00 & 53.00 & 30.74 \\ From scratch (SECOND) & - & 100\% & 50.5 & 62.29 & - & - & - & - & - & - & - & - & - & - & - & - \\ Ours (SECOND) & AD-PT & 5\% & 37.69 & 47.95 & 74.89 & 41.82 & 12.05 & 54.77 & 28.91 & 34.41 & 23.63 & 3.19 & 63.61 & 39.54 \\ Ours (SECOND) & AD-PT & 100\% & **52.23** & **63.04** & 83.12 & 52.86 & 15.24 & 68.58 & 37.54 & 59.48 & 46.01 & 20.44 & 78.96 & 60.05 \\  From scratch (C.P) & - & 5\% & 42.68 & 50.41 & 77.82 & 43.61 & 10.65 & 44.01 & 18.71 & 52.95 & 36.26 & 16.76 & 67.62 & 54.52 \\ From scratch (C.P) & - & 100\% & 56.2 & 64.5 & 54.83 & 53.99 & 16.88 & 67.0 & 35.9 & 64.48 & 55.56 & 36.4 & 8.31 & 63.44 \\ GCC-3D (C.P)  & SS-PT & 100\% & **57.3** & 65.0 & **85.47** & **17.6** & 67.22 & 35.7 & 65.0 & 56.2 & 36.0 & 82.9 & 63.7 \\ BEV-MAE (C.P)  & SS-PT & 100\% & 57.2 & 65.1 & 84.9 & 54.9 & 16.5 & 67.2 & 35.9 & **65.2** & 56.0 & 36.2 & 83.2 & 63.5 \\ Ours (C.P) & AD-PT & 5\% & 44.99 & 52.99 & 78.90 & 43.82 & 11.13 & 55.16 & 21.22 & 55.10 & 39.03 & 17.76 & 72.28 & 55.43 \\ Ours (C.P) & AD-PT & 100\% & 57.17 & **65.48** & 84.6 & 54.37 & 16.09 & **67.34** & **36.06** & 64.31 & **58.50** & **40.58** & **83.53** & **66.05** \\   

Table 4: Fine-tuning performance on nuScenes benchmark. C.P. denotes that CenterPoint is employed as the baseline detector and D.A. represents the Data Amount. We fine-tune on 5% and 100% data for 20 epochs. Compared with other works , our pre-training process is performed on a unified dataset rather than nuScenes.

    &  &  &  &  &  &  \\   & & & & (Mod.) & Easy & Mod. & Hand & Easy & Mod. & Hard & Easy & Mod. & Hard \\  From scratch (SECOND) & - & 20\% & 61.10 & 89.78 & 78.33 & 76.21 & 52.08 & 47.23 & 43.37 & 76.35 & 59.06 & 55.24 \\ From scratch (SECOND) & - & 100\% & 66.70 & 89.63 & 80.78 & 78.21 & 58.05 & 52.61 & 48.24 & 84.56 & 67.16 & 62.50 \\ Ours (SECOND) & AD-PT & 20\% & 65.95 & 90.23 & 80.70 & 78.29 & 55.63 & 49.67 & 45.21 & 83.78 & 67.50 & 63.40 \\ Ours (SECOND) & AD-PT & 100\% & **67.58** & **90.36** & **81.39** & **74.11** & **83.88** & **53.88** & **47.82** & **80.47** & **60.78** & **63.98** \\  From scratch (P-RCNN) & - & 20\% & 66.71 & 91.81 & 82.52 & 80.11 & 58.78 & 53.33 & 47.61 & 86.74 & 64.28 & 59.53 \\ ProposalContext (P-RCNN)  & SSF-PT & 20\% & 68.13 & 91.96 & 82.65 & 80.15 & 62.58 & 55.06 & 50.06 & 88.58 & 66.68 & 62.32 \\ From scratch (P-RCNN) & 100\% & 70.57 & - & 84.50 & - & - & - & - & - & - & - & - \\ GCC-3D (PV-RCNN)  & SS-PT & 100\% & 71.26 & - & 84.50 & - & - & - & - & - & - & - \\ STIRL (PV-RCNN)  & SSF-PT & 100\% & 71.46 & - & 84.70 & - & - & - & - & - & - & - & - \\ PointContext (PV-RCNN)  & SS-PT & 100\% & 71.55 & 91.40 & 84.18 & 82.25 & 65.73 & 57.74 & 52.46 & 91.47 & 72.72 & 67.95 \\ ProposalContext (PV-RCNN)  & SSF-PT & 100\% & 72.92 & **92.45** & 84.72 & 82.47 & 68.43 & 60.36 & 55.01 & **92.77** & **73.69** & **69.51** \\ Ours (PV-RCNN) & AD-PT & 20\% & 69.43 & 918 & 82.75 & 82.12 & 65.50 & 57.59 & 51.84 & 81.65 & 67.96 & 64.73 \\ Ours (PV-RCNN) & AD-PT & 100\% & **73.01** & 91.96 & **84.75** & **82.53** & **68.87** & **60.79** & **55.42** & 91.81 & 73.49 & 69.21 \\   

Table 5: Fine-tuning performance (AP\({}_{}\)) on KITmethods are pre-trained on Waymo 100% unlabeled train set (\(\)150k frames), which has a smaller domain gap with fine-tuning data. Further, to verify the effectiveness of fine-tuning with a small number of samples, we conduct experiments of fine-tuning on 3% Waymo train set (\(\)5K frames). We can observe that, with the help of pre-trained prior knowledge, fine-tuning with a small amount of data can achieve much better performance than training from scratch (_e.g._, \(3.41\%\) / \(14.08\%\) in L2 using SECOND as baseline). In addition, to ensure the fairness of the experiments, we compare our method with semi-supervised learning methods which are identical to our pre-training setup.

**Results on nuScenes.** Due to the huge domain discrepancies, few works can transfer the knowledge obtained by pre-training on other datasets to nuScenes dataset. Thanks to constructing a unified large-scale dataset and considering taxonomic differences in the pre-training stage, our methods can also significantly improve the performance on nuScenes. As shown in Tab. 4, AD-PT improves the performance of training from scratch by \(0.93\%\) and \(0.98\%\) in mAP and NDS.

**Results on KITTI.** As shown in Tab. 5, we further fine-tune on a relatively small dataset (_i.e._, KITTI). Note that previous methods often use models pre-trained on Waymo as an initialization since the domain gap is quite small. It can be observed that using AD-PT initialization can further improve the accuracy when fine-tuning on both 20% and 100% KITTI training data under different detectors. For instance, the AP\({}_{}\) in moderate level can improve 2.72% and 1.75% when fine-tuning on 20% and 100% KITTI data using PV-RCNN++ as the baseline detector.

### Insight Analyses

In this part, we further discuss the 3D pre-training. Note that in ablation studies, we fine-tune on 3% Waymo data and 5% nuScenes data under the PV-RCNN++ and CenterPoint baseline setting.

#### 4.3.1 Insight Analyses on the Data Preparation

**Discussion on Diversity-based Pre-training Processor.** From Tab. 6, we observe that both beam re-sampling and object re-scaling during the pre-training stage can improve the performance for other downstream datasets. For example, the overall performance can be improved by \(0.65\%\) / \(0.54\%\) on Waymo and \(7.37\%\) on nuScenes. Note that when pre-training without our data processor, the fine-tuning performance on nuScenes will drop sharply compared with training from scratch, due to the large domain discrepancy. Since our constructed unified dataset can cover diversified data distributions, the backbone can learn more general representations.

**Pseudo-labeling Performance.** Tab. 7 indicates the impact of pseudo-labeling operation on the downstream perception performance. We use three types of pseudo-labeling methods to observe the low, middle, and high performance on ONCE. We find that the performance of fine-tuning is positively correlated with the accuracy of pseudo-labeling on ONCE. This is mainly due to that pseudo labels with relatively high accuracy can guide the backbone to activate more precise foreground features and meanwhile suppress background regions.

    &  &  &  \\   & Overall & Overall & Vehicle & Pedestrian & Cyclist & mAP & NDS \\  SECOND (Low Performance) & 57.10 & 65.96 / 63.29 & 65.95 / 65.46 & 66.87 / 60.36 & 65.07 / 64.06 & 41.49 & 50.82 \\ CenterPoint (Middle Performance) & 60.84 & 66.79 / 64.10 & 67.09 / 66.60 & 67.79 / 61.16 & 65.51 / 64.55 & 41.91 & 51.64 \\ Ours (High Performance) & 69.90 & **67.77** / **65.09** & **68.01** / **67.61** & **68.32** / **61.69** & **66.99** / **65.98** & **43.11** & **52.41** \\   

Table 7: The impact of pseudo-labeling methods on downstream datasets.

Figure 6: Different budgets

**Scalability.** To verify the scaling ability of the AD-PT paradigm, we conduct experiments in Tab 8 to show the Waymo performance initialized by different pre-trained checkpoints, which are obtained using pre-training datasets with different scales. Please refer to Appendix for more results.

#### 4.3.2 Insight Analyses on the Unified Representations Learning

**Discussion on Unknown-aware Instance Learning Head.** It can be seen from Tab. 9 that, Unknown-aware Instance Learning (UIL) head and Consistency Loss (CL) can further boost the perception accuracy on multiple datasets. It can be observed that the gains are larger on the Pedestrian and Cyclist categories. The reason is that the UIL head can better capture downstream-sensitive instances, which are hard to be detected during the pseudo-labeling **pre-training** process.

**Training-efficient Method.** In the main results, we verify that our methods can bring performance gains under different amounts of fine-tuning data (_e.g._, 1%, 5%, 10% budgets). As shown in Fig. 6, results demonstrate that our method can consistently improve performance under different budgets.

#### 4.3.3 Insight Analysis on Different Types of Baseline Detectors

To verify the generalization of our proposed method, we further conduct experiments on different types of 3D object detection backbones (_i.e._, pillar-based and point-based). As shown in Tab. 10 and Tab. 11, the performance of multiple types of detectors can improve when initialized by AD-PT pre-trained checkpoints which further shows that AD-PT is a general pre-training pipeline that can be used on various types of 3D detectors.

## 5 Conclusion

In this work, we have proposed the AD-PT paradigm, aiming to pre-train on a unified dataset and transfer the pre-trained checkpoint to multiple downstream datasets. We comprehensively verify the generalization ability of the built unified dataset and the proposed method by testing the pre-trained model on different downstream datasets including Waymo, nuScenes, and KITTI, and different 3D detectors including PV-RCNN, PV-RCNN++, CenterPoint, and SECOND.

## 6 Limitation

Although the AD-PT pre-trained backbone can improve the performance on multiple downstream datasets, it needs to be verified in more actual road scenarios. Meanwhile, training a backbone with more generalization capabilities through data from different sensors is also a future direction.

   &  &  &  \\   & & & Overall & Vehicle & Pedestrian &  \\  From scratch (PointPillar) & 20\% & 48.56 / 39.30 & 54.28 / 53.51 & 47.11 / 25.50 & 44.29 / 38.89 \\ AD-PT (PointPillar) & 20\% & **52.01 / 43.39** & **58.51 / 57.85** & **50.22 / 32.52** & **47.31 / 41.59** \\ From scratch (PointPillar) & 100\% & 57.85 / 50.69 & 62.18 / 61.64 & 58.18 / 40.64 & 53.18 / 49.80 \\ AD-PT (PointPillar) & 100\% & **59.71 / 53.49** & **64.10 / 63.54** & **59.00 / 43.13** & **56.04 / 53.80** \\  

Table 10: Ablation study on the pillar-based backbone. We conduct experiments on Waymo dataset.

   &  &  \\   & Overall & Vehicle & Pedestrian & Cyclist & mAP & NDS \\  Baseline & 67.77 / 65.09 & 68.01 / 67.61 & 68.32 / 61.69 & 66.99 / 65.98 & 43.11 & 52.41 \\ Baseline+UIL & 67.97 / 65.35 & 67.99 / 67.58 & 68.62 / 62.12 & 67.32 / 66.35 & 43.92 & 52.65 \\ Baseline+UIL+CL & **68.33 / 65.69** & **68.17 / 67.70** & **68.82 / 62.39** & **68.00 / 67.00** & **44.99** & **52.99** \\  

Table 9: Ablation study on the designed UIL and CL.