# Vivid-ZOO: Multi-View Video Generation with Diffusion Model

Bing Li\({}^{*}\)  Cheng Zheng\({}^{*}\) Wenxuan Zhu\({}^{*}\) Jinjie Mai Biao Zhang

Peter Wonka Bernard Ghanem

King Abdullah University of Science and Technology

https://hi-zhengcheng.github.io/vividzoo/

Equal contributions. Corresponding author.

###### Abstract

While diffusion models have shown impressive performance in 2D image/video generation, diffusion-based Text-to-Multi-view-Video (T2MVid) generation remains underexplored. The new challenges posed by T2MVid generation lie in the lack of massive captioned multi-view videos and the complexity of modeling such multi-dimensional distribution. To this end, we propose a novel diffusion-based pipeline that generates high-quality multi-view videos centered around a dynamic 3D object from text. Specifically, we factor the T2MVid problem into viewpoint-space and time components. Such factorization allows us to combine and reuse layers of advanced pre-trained multi-view image and 2D video diffusion models to ensure multi-view consistency as well as temporal coherence for the generated multi-view videos, largely reducing the training cost. We further introduce alignment modules to align the latent spaces of layers from the pre-trained multi-view and the 2D video diffusion models, addressing the reused layers' incompatibility that arises from the domain gap between 2D and multi-view data. In support of this and future research, we further contribute a captioned multi-view video dataset. Experimental results demonstrate that our method generates high-quality multi-view videos, exhibiting vivid motions, temporal coherence, and multi-view consistency, given a variety of text prompts.

## 1 Introduction

Multi-view videos capture a scene/object from multiple cameras with different poses simultaneously, which are critical for numerous downstream applications  such as AR/VR, 3D/4D modeling, media production, and interactive entertainment. More importantly, the availability of such data holds substantial promise for facilitating progress in research areas such as 4D reconstruction , 4D generation , and long video generation  with 3D consistency. However, collecting multi-view videos often requires sophisticated setups  to synchronize and calibrate multiple cameras, resulting in a significant absence of datasets and generative techniques for multi-view videos.

In the meantime, diffusion models have shown great success in 2D image/video generation. For example, 2D video diffusion models  generate high-quality 2D videos by extending image diffusion models . Differently, multi-view image diffusion models  are proposed to generate multi-view images of 3D objects, which have demonstrated significant impact in 3D object generation , 3D reconstruction , and related fields. However, to the best of our knowledge, no other works have explored Text-to-Multi-view-Video (T2MVid) diffusionmodels. Motivated by recent 2D video and multi-view image diffusion models, we aim to propose a diffusion-based method that generates multi-view videos of dynamic objects from text (see Fig. 1).

Compared to 2D video generation, T2MVid generation poses two new challenges. First, modeling multi-view videos is complex due to their four-dimensional nature, which involves different viewpoints as well as the dimensions of time and space (2D). Consequently, it is nontrivial for diffusion models to model such intricate data from scratch without extensive captioned multi-view video datasets. Second, there are no publicly available large-scale datasets of captioned multi-view videos, but it has been shown that billions of text and 2D image pairs are essential for powerful image diffusion models [74; 76; 83]. For example, Stable Diffusion  is trained on the massive LAION-5B dataset . Unlike downloading 2D images available on the Internet, collecting a large quantity of multi-view videos is labor-intensive and time-consuming. This challenge is further compounded when high-quality captioned videos are needed, hindering the extension of diffusion models to T2MVid generation.

In this paper, instead of the labor-intensive task of collecting a large amount of captioned multi-view video data, we focus on the problem of enabling diffusion models to generate multi-view videos from text using only a comparable small dataset of captioned multi-view videos. This problem has not been taken into account by existing diffusion-based methods (_e.g._, ). However, studies have revealed that naively fine-tuning a large pre-trained model on limited data can result in overfitting [30; 75; 115]. Our intuition is that we can factor the multi-view video generation problem into viewpoint-space and time components. The viewpoint-space component ensures that the generated multi-view videos are geometrically consistent and aligned with the input text, and the temporal component ensures temporal coherence. With such factorization, a straightforward approach is to leverage large-scale multi-view image datasets (_e.g._, ) and 2D video datasets (_e.g._, Web10M ) to pre-train the viewpoint-space component and temporal component, respectively. However, while this approach can largely reduce the reliance on extensive captioned multi-view videos, it remains costly in terms of training resources.

Instead, we explore a new question: _can we jointly combine and reuse the layers of pre-trained 2D video and multi-view image diffusion models to establish a T2MVid diffusion model?_ The large-scale pre-trained multi-view image diffusion models (_e.g._, MVdream ) have learned how to model multi-view images, and the 2D temporal layers of powerful pre-trained video diffusion models (_e.g._, AnimateDiff ) learned rich motion knowledge. However, new challenges are posed. We observe that naively combining the layers from these two kinds of diffusion models leads to poor generation

Figure 1: The proposed Vivid-ZOO generates high-quality multi-view videos of a dynamic 3D object from text. Each row illustrates six frames drawn from a generated video for one viewpoint.

results. More specifically, the training data of multi-view image diffusion models are mainly rendered from synthetic 3D objects (_e_.\(g\)., Objaverex [17; 16] ), while 2D video diffusion models are mainly trained on real-world 2D videos, posing a large domain gap issue.

To bridge this gap, we propose a novel diffusion-based pipeline, namely, Vivid-ZOO, for T2MVid generation. The proposed pipeline effectively connects the pre-trained multi-view image diffusion model  and 2D temporal layers2 of the pre-trained video model by introducing two kinds of layers, named 3D-2D alignment layers and 2D-3D alignment layers, respectively. The 3D-2D alignment layers are designed to align features to the latent space of the pre-trained 2D temporal layers, and the introduced 2D-3D alignment layers project the features back. Furthermore, we construct a comparable small dataset consisting of 14,271 captioned multi-view videos to facilitate this and future research line. Although our dataset is much smaller compared to the billion-scale 2D image dataset (LAION ) and the million-scale 2D video dataset (e.g., WebVid10M ), our pipeline allows us to effectively train a large-scale T2MVid diffusion model using such limited data. Extensive experimental results demonstrate that our method effectively generates high-quality multi-view videos given various text prompts.

We summarize our contributions as follows:

* We present a novel diffusion-based pipeline that generates high-quality multi-view videos from text prompts. This is the first study on T2MVid diffusion models.
* We show how to combine and reuse the layers of the pre-trained 2D video and multi-view image diffusion models for a T2MVid diffusion model. The introduced 3D-2D alignment and 2D-3D alignment are simple yet effective, enabling our method to utilize layers from the two diffusion models across different domains, ensuring both temporal coherence and multi-view consistency.
* We contribute a multi-view video dataset that provides multi-view videos, text descriptions, and corresponding camera poses, which helps to advance the field of T2MVid generation.

## 2 Related work

**2D video diffusion model.** Many previous approaches have explored autoregressive transformers (_e_.\(g\)., [18; 29; 107]), physical models  or GANs (_e_.\(g\)., [8; 56; 77; 41]) for video generation. Recently, more and more efforts have been devoted to diffusion-based video generation [21; 26; 65; 94; 99; 102; 110; 121; 37], inspired by the impressive results of image diffusion models [11; 12; 74; 76; 83; 115].

The amount of available captioned 2D video data is significantly less than the vast number of 2D image-text pairs available on the Internet. Most methods [7; 22; 23; 90; 91; 92] extend pre-trained 2D image diffusion models to video generation to address the challenge of limited training data. Some methods employ pre-trained 2D image diffusion models (_e_.\(g\)., ) to generate 2D video from texts in a zero-shot manner  or using few-shot tuning strategies . These methods avoid the requirement of large-scale training data. Differently, another research line is to augment pre-trained 2D image diffusion models with various temporal modules or trainable parameters, showing impressive temporal coherence performance. For example, Ho et al.  extend the standard image diffusion architecture by inserting a temporal attention block. Animatediff and AYL  freeze 2D image diffusion model and solely train additional motion modules on large-scale datasets of captioned 2D videos such as WebVid10M . In addition, image-to-2D-video generation methods [6; 71; 105; 117] are proposed based on diffusion models to generate a monocular video from an image. Methods  focus on controllable video generation through different conditions such as pose and depth. MotionCtrl  and Direct-a-video  can generate videos conditioned by the camera and object motion. CameraCtrl  can also control the trajectory of a moving camera for generated videos. However, these text-to-2D-video diffusion models are designed for monocular video generation, which does not explicitly consider the spatial 3D consistency of multi-view videos.

**Multi-view image diffusion model.** Recent works have extended 2D image diffusion models for multi-view image generation. Zero123  and Zero123++  propose to fine-tune an image-conditioned diffusion model so as to generate a novel view from a single image. Inspired by this,many novel view synthesis methods [19; 32; 52; 53; 59; 93; 97; 100; 108; 112; 120] are proposed based on image diffusion models. For example, IM3D  and Free3D  generate multiple novel views simultaneously to improve spatial 3D consistency among different views. Differently, a few methods [13; 33; 89] adapt pre-trained video diffusion models (_e.g._, ) to generate multi-view images from a single image. MVDream  presents a text-to-multi-view-image diffusion model to generate four views of an object each time given a text, while SPAD  generates geometrically consistent images for more views. Richdreamer  trains a diffusion model to generate depth, normal, and albedo.

**4D generation using diffusion models.** Many approaches [47; 51; 64; 69; 79; 80; 85; 96; 2] have exploited pre-trained diffusion models to train 3D representations for 3D object generation via score distillation sampling . Recently, a few methods [3; 49; 70; 82; 113] leverage pre-trained diffusion models to train 4D representations for dynamic object generation. For example, Ling _et al._ represent a 4D object as Gaussian spatting , while Bahmani _et al._ adopt a NeRF-based representation [60; 61; 86]. Then, pre-trained 2D image, 2D video, and multi-view image diffusion models are employed to jointly train the 4D representations. In addition, diffusion models are used to generate 4D objects from monocular videos [14; 31]. Diffusion4D  presents a diffusion model that generates an orbital video around 4D content, and 4Diffusion  presents a video-conditioned diffusion model that generates MV videos from a monocular video. Different from all these methods, our approach focuses on presenting a T2MVid diffusion model. LMM  generates 3D motion for given 3D human models. DragAPart  can generate part-level motion for articulated objects. Unlike our method, Kuang _et al._ focuses on generating multiple videos of the same scene given multiple camera trajectories.

## 3 Multi-view video diffusion model

**Problem definition.** Our goal for T2MVid generation is to generate a set of multi-view videos centered around a dynamic object from a text prompt. Motivated by the success of diffusion models in 2D video/image generation, we aim to design a T2MVid diffusion model. However, T2MVid generation is challenging due to the complexity of modeling multi-view videos and the difficulty of collecting massive captioned multi-view videos for training.

We address the above challenges by exploring two questions. (1) Can we design a diffusion model that effectively learns T2MVid generation, yet only needs a comparable small dataset of multi-view video data? (2) Can we jointly leverage, combine, and reuse the layers of pre-trained 2D video and multi-view image diffusion models to establish a T2MVid diffusion model? Addressing these questions can reduce the reliance on large-scale training data and decrease training costs. However, this question remains unexplored for diffusion-based T2MVid.

**Overview.** We address the above questions by factoring the T2MVid generation problem over viewpoint-space and time. With the factorization, we propose a diffusion-based pipeline for T2MVid generation (see Fig 2), including the multi-view spatial modules and multi-view temporal modules. Sec 3.1 describes how we adapt a pre-trained multi-view image diffusion model as the multi-view spatial modules. Multi-view temporal modules effectively leverage temporal layers of the pre-trained 2D video diffusion model with the newly introduced 3D-2D alignment layers and 2D-3D alignment layers (Sec 3.2). Finally, we describe training objectives in Sec 3.3 and the dataset construction to support our pipeline for T2MVid generation in Sec 3.4.

### Multi-view spatial module

Our multi-view spatial modules ensure that the generated multi-view videos are geometrically consistent and aligned with the input text. Recent multi-view image diffusion models [34; 80] generate high-quality multi-view images by fine-tuning Stable Diffusion and modifying its self-attention layers. We adopt the architecture of Stable Diffusion for our multi-view spatial modules. Furthermore, we leverage a pre-trained multi-view image diffusion model based on Stable Diffusion by reusing its pre-trained weights in our spatial modules, which avoids training from scratch and reduces the training cost. However, the self-attention layers of Stable Diffusion are not designed for multi-view videos. We adapt these layers for multi-view self-attention as below.

**Multi-view self-attention.** We inflate self-attention layers to capture geometric consistency among generated multi-view videos. Let \(^{b K N d h w}\) denote the 6D feature tensor of multi-view videos in the diffusion model, where \(b\), \(K\), \(N\), \(d\) and \(h w\) are batch size, view number, frame number, feature channel and spatial dimension, respectively. Inspired by , we reshape \(\) into a shape of \((b N) d(K h w)\), leading to a batch of feature maps \(}^{n}\) of 2D images, where \((b N)\) is the batch size, \(}^{n}\) denotes a feature map representing all views at frame index \(n\), and \((K h w)\) is the spatial size. We then feed the reshaped feature maps \(}^{n}\) into self-attention layers. Since \(}^{n}\) consists of all views at frame index \(n\), the self-attention layers learn to capture geometrical consistency among different views. We also inflate other layers of stable diffusion (see Appendix E) so that we can reuse their pre-trained weight.

**Camera pose embedding.** Our diffusion model is controllable by camera poses, achieved by incorporating a camera pose sequence as input. These poses are embedded by MLP layers and then added to the timestep embedding, following MVdream . Here, our multi-view spatial module reuses the pre-trained multi-view image diffusion model MVDream .

### Multi-view temporal module

Besides spatial 3D consistency, it is crucial for T2MVid diffusion models to maintain the temporal coherence of generated multi-view videos simultaneously. Improper temporal constraints would break the synchronization among different views and introduce geometric inconsistency. Moreover, training a complex temporal module from scratch typically requires a large amount of training data.

Instead, we propose to leverage the 2D temporal layers of large pre-trained 2D video diffusion models (_e.g._, ) to ensure temporal coherence for T2MVid generation. These 2D temporal layers have learned rich motion priors, as they have been trained on millions of 2D videos (_e.g._, ). Here, we employ the 2D temporal layers of AnimateDiff  due to its impressive performance in generating temporal coherent 2D videos.

However, we observed that naively combining the pre-trained 2D temporal layers with the multi-view spatial module leads to poor results. The incompatibility is due to the fact that the pre-trained 2D temporal layers and the multi-view spatial modules are trained on data from different domains (_i.e._, real 2D and synthetic multi-view data) that have a large domain gap. To address the domain gap issue, one approach is to fine-tune all 2D temporal layers of a pre-trained 2D video diffusion model

Figure 2: Overview of the proposed Vivid-ZOO. **Left**: Given a text prompt, our diffusion model generates multi-view videos. Instead of training from scratch, the multi-view spatial module reuses the pre-trained multi-view image diffusion model, and the multi-view temporal module leverages the 2D temporal layers of the pre-trained 2D video diffusion model to enforce temporal coherence. **Right**: Jointly reusing the pre-trained multi-view image diffusion model and temporal 2D layers poses new challenges due to the large gap between their training data (multi-view images of synthetic 3D objects versus real-world 2D videos). We introduce 3D-2D alignment and 2D-3D alignment to address the domain gap issue.

on multi-view video data. However, such an approach not only needs to train many parameters but can also harm the learned motion knowledge  if a small training dataset is given. We present a multi-view temporal module (see Fig. 3) that reuses and freezes all 2D temporal layers to maintain the learned motion knowledge and introduce the 3D-2D alignment layer and the 2D-3D alignment layer.

**3D-2D alignment.** We introduce the 3D-2D alignment layers to effectively combine the pre-trained 2D temporal layers with the multi-view spatial module. Recently, a few methods [23; 6] add motion LoRA to 2D temporal attention for personalized/customized video generation tasks. However, our aim is different, _i.e_., we expect to preserve the learned motion knowledge of 2D temporal layers, such that our multi-view temporal module can leverage the knowledge for ensuring temporal coherence.

Since motion prior knowledge is captured by the pre-trained 2D temporal attention layers, we insert the 3D-2D alignment layers before the 2D temporal attention layers. The 3D-2D alignment layers are learned to align the features into the latent space of the pre-trained 2D temporal layers. Furthermore, inspired by ControlNet  and , the 3D-2D alignment layers are inserted via residual connections and are zero-initialized, providing an identity mapping at the beginning of training. The process is described as follows:

\[=^{2D}()+^{3D 2D}()\] (1)

where \(^{3D 2D}\) is the 3D-2D alignment layer. \(^{2D}\) is the 2D temporal layer followed by the 2D temporal attention layers and we refer to it as _2D in-layer_ (see more details in Appendix). The 3D-2D alignment layer is plug-and-play and is simply implemented as an MLP.

**Multi-view temporal coherence.** We reuse and freeze the pre-trained 2D temporal layers in our multi-view temporal module to ensure the temporal coherence of each generated video. However, the 2D temporal layer is designed to handle 2D videos. We inflate the 2D temporal layer by reshaping the feature \(\) to the 2D video dimension via the _rearrange_ operation . Then, 2D temporal layers \(()\) model temporal coherence across frames by calculating the attention of points at the same spatial location in \(\) across frames for each video:

\[ =(,\;b\;K\;N\;h\;w\;d(b\;K\; h\;w)\;N\;d)\] (2) \[ =()\] (3) \[ =(,(b\;K\;h\;w)\;N\;d\;b\; K\;N\;h\;w\;d)\] (4)

**2D-3D alignment.** We add the 2D-3D alignment layers after 2D temporal attention layers to project the feature back to the feature space of the multi-view spatial modules.

\[^{a}=^{2D}()+^{2D 3D}()\] (5)

where \(^{3D 2D}\) is the 2D-3D alignment layer. \(^{2D}\) is the 2D temporal layer following the 2D temporal attention layer. The 2D-3D alignment layers are implemented as an MLP.

### Training objectives

We train our diffusion model to generate multi-view videos. Note that we freeze most layers/modules in the diffusion model and only train the 3D-2D and 2D-3D alignment layers during training, which largely reduces the training cost and reliance on large-scale data. Let \(\) denote the training dataset, where a training sample \(\{,y,\}\) consists of \(N\) multi-view videos \(=\{x\}_{1}^{N}\), \(N\) corresponding camera poses \(\), and a text prompt \(y\). The training objective \(\) on \(\) is defined as follows:

\[=_{_{t}^{v},y,,t}[\| -_{}(_{t}^{v},t,_{}(y),)\|^ {2}]\] (6)

where \(_{}()\) is a text encoder that encodes the text into text embedding, \(_{}()\) is the denoising network. \(_{0}^{v}\) is the latent code of a multi-view video sequence and \(_{t}^{v}\) is its noisy code with added noise \(\).

Figure 3: Our multi-view temporal module, where 3D-2D alignment layers are trained to align features to the latent space of the 2D temporal attention layers, and the 2D-3D alignment layers project them back.

### Multi-view video dataset

Different from 2D images that are available in vast numbers on the Internet, it is much more difficult and expensive to collect a large amount of multi-view videos centered around 3D objects and corresponding text captions. Recently, multi-view image datasets (_e_.\(g\)., [51; 67]), rendered from synthetic 3D models, have shown a significant impact on various tasks such as novel view synthesis [51; 93], 3D generation (Gaussian Splatting , large reconstruction model ), multi-view image generation  and associated applications. Motivated by this, we resort to rendering multi-view videos from synthetic 4D models (animated 3D models).

We construct a dataset named MV-VideoNet that provides 14,271 triples of a multi-view video sequence, its associated camera pose sequence, and a text description. In particular, we first select animated objects from Objavverse . Objavverse is an open-source dataset that provides high-quality 3D objects and animated ones (_i_.\(e\)., 4D object). We select 4D objects from the Objavverse dataset and discard those without motions or with imperceptible motions. Given each selected 4D object, we render 24-view videos from it, where the azimuth angles of camera poses are uniformly distributed. To improve the quality of our dataset, we manually filter multi-view videos with low-quality \(e\)._g_., distorted shapes or motions, very slow or rapid movement. For text descriptions, we adopt the captioning method Cap3D [57; 58] to caption a multi-view video sequence. Cap3D leverages BLIP2  and GPT4  to fuse information from multi-view images, generating text descriptions.

## 4 Experiments

**Implementation details.** We reuse the pre-trained MVDream V1.5 in our multi-view spatial module and reuse the pre-trained 2D temporal layers of AnimateDiff V2.0 in our multi-view temporal module. We train our model using AdamW  with a learning rate of \(10^{-4}\). During training, we process the training data by randomly sampling 4 views that are orthogonal to each other from a multi-view video sequence, reducing the spatial resolution of videos to \(256 256\), and sample video frames with a stride of 3. Following AnimateDiff, we use a linear beta schedule with \(_{start}\) = 0.00085 and \(_{end}\) = 0.012. (Please refer to the Appendix for more details).

**Evaluation metrics.** Quantitatively evaluating multi-view consistency and temporal coherence remains an open problem for T2MVid generation. We quantitatively evaluate text alignment via CLIP  and temporal coherence via Frechet Video Distance (FVD) . Yet, Ge _et al_. pointed out FVD leans more towards per-frame quality than temporal consistency. To compensate for FVD,

Figure 4: Comparison on T2MVid generation. Although MVDream generates spatially 3D consistent images among views (the 1st column), MVDream + IP-AnimateDiff breaks the spatial 3D consistency among its generated videos. Instead, our method generates high-quality multi-view videos with large motions while maintaining temporal coherence and spatial 3D consistency.

[MISSING_PAGE_FAIL:8]

**Design of multi-view spatial module.** We build a baseline named **w/o MS w SD** that employs original Stable Diffusion 1.5  as our multi-view spatial module and reuses its pre-trained weights. We also insert the camera embedding into the Stable Diffusion to enable viewpoint control. That is, **w/o MS w SD** is to generate a single-view video (2D) conditioned on input text and camera poses. We train **w/o MS w SD** on our dataset, where single-view videos are used as training data.

Since single-view video generation is much simpler than multi-view video generation, **w/o MS w SD** achieves high performance in video quality. However, **w/o MS w SD** fails to maintain multi-view consistency among different views (see Fig. 5) and has degraded overall generation performance (see Tab. 2). For example, the motion and shapes of the dragon are significantly inconsistent among views. Instead, by simply adapting a pre-trained multi-view image diffusion model as our spatial module, our method effectively ensures multi-view consistency.

**Design of multi-view temporal module.** Recent methods apply LoRA  to the 2D temporal attention layers of a pre-trained 2D video diffusion model and fine-tune only LoRA for personalized and customized 2D video generation tasks . Following these methods, we build a temporal module named **TM LoRA** by inflating 2D temporal layers of AnimateDiff to handle multi-view videos and adding LoRA to the 2D temporal attention layers. We replace our multi-view temporal module with **TM LoRA**, and denote it by **w/o MT w TM LoRA**. Fig. 6 and Tab. 2 shows **w/o MT w TM LoRA** generates low-quality results, despite being fine-tuned on our dataset. Instead, our multi-view temporal module inserts 3D-2D alignment and 2D-3D alignment layers before and after the 2D temporal attention layers, enabling the multi-view temporal module to be compatible with the multi-view spatial module.

**Effect of 3D-2D alignment.** We remove the proposed 3D-2D alignment from our model and train the model on our dataset with the same settings. Tab. 2 shows **w/o 3D-2D alignment** degrades

   Method &  &  & Overall \(\) \\  MVDream + IP-AnimateDiff & 2038.66 \(\) 44.36 & **32.71 \(\) 0.67** & 28\% \\ Ours & **1634.28 \(\) 45.24** & 32.24 \(\) 0.78 & **72\%** \\   

Table 1: Multi-view video generation. Best in bold.

   Method & Overall \(\) \\  w/o MS w SD & 44.88\% \\ w/o MT w TM LoRA & 11.25\% \\ w/o 3D-2D alignment & 53.50\% \\ w/o 2D-3D alignment & 54.50\% \\ Ours & **80.25\%** \\   

Table 2: The ablation study results. The overall performance is assessed by a user study using paired comparison .

Figure 6: Visual comparison of the contributions of our multi-view temporal moduleour temporal coherence and video quality performance. Instead, by projecting the feature to the latent space of the pre-trained 2D attention layers, our 3D-2D alignment layer effectively enables the 2D attention layers to align temporally correlated content, ensuring the video quality and temporal coherence.

**Effect of 2D-3D alignment.** As shown in Tab. 2, "w/o 2D-3D temporal alignment" achieves lower performance with the same training settings due to the removal of 2D-3D temporal alignment. The results indicate that only 3D-2D alignment is insufficient in jointly leveraging the pre-trained 2D temporal layers  and the multi-view image diffusion model  in our diffusion model. Instead, our 2D-3D alignment projects the features processed by the pre-trained 2D temporal layers back to the latent space of the multi-view image diffusion model, leading to high-quality results.

**Training cost.** MVDream is trained on 32 Nvidia Tesla A100 GPUs, which takes 3 days, and AnimateDiff takes around 5 days on 8 A100 GPUs. By combining and reusing the layers of MVDream and AnimateDiff, our method only needs to train the proposed 3D-2D alignment and 2D-3D layers, reducing the training cost to around 2 days with 8 A100 GPUs.

## 5 Conclusions

In this paper, we propose a novel diffusion-based pipeline named Vivid-ZOO that generates high-quality multi-view videos centered around a dynamic 3D object from text. The presented multi-view spatial module ensures the multi-view consistency of generated multi-view videos, while the multi-view temporal module effectively enforces temporal coherence. By introducing the proposed 3D-2D temporal alignment and 2D-3D temporal alignment layers, our pipeline effectively leverages the layers of the pre-trained multi-view image and 2D video diffusion models, reducing the training cost and accelerating the training of our diffusion model. We also construct a dataset of captioned multi-view videos, which facilitates future research in this emerging area.