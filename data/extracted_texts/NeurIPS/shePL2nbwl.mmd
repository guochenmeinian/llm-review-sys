# Survival Instinct in Offline Reinforcement Learning

Anqi Li

University of Washington

&Dipendra Misra

Microsoft Research

&Andrey Kolobov

Microsoft Research

&Ching-An Cheng

Microsoft Research

###### Abstract

We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with "wrong" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of _pessim_ in offline RL algorithms and certain implicit biases in common data collection practices. As we prove in this work, pessimism endows the agent with a _survival instinct_, i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival policies. Formally, given a reward class -- which may not even contain the true reward -- we identify conditions on the training data distribution that enable offline RL to learn a near-optimal and safe policy from any reward within the class. We argue that the survival instinct should be taken into account when interpreting results from existing offline RL benchmarks and when creating future ones. Our empirical and theoretical results suggest a new paradigm for offline RL, whereby an agent is "nudged" to learn a desirable behavior with imperfect reward but purposely biased data coverage. Please visit our website https://survival-instinct.github.io for accompanied code and videos.

## 1 Introduction

In offline reinforcement learning (RL), an agent optimizes its performance given an offline dataset. Despite being its main objective, we find that return maximization is not sufficient for explaining some of its empirical behaviors. In particular, _in many existing benchmark datasets, we observe that offline RL can produce surprisingly good policies even when trained on utterly wrong reward labels_.

In Fig. 1, we present such results on the hopper task from D4RL , a popular offline RL benchmark, using a state-of-the-art offline RL algorithm ATAC . The goal of an RL agent in the hopper task is to move forward as fast as possible while avoiding falling down. We trained ATAC agents on the original datasets and on three modified versions of each dataset, with "wrong" rewards: _1) zero_: assigning a zero reward to all transitions, _2) random_: labeling each transition with a reward sampled uniformly from \(\), and _3) negative_: using the negation of the true reward. Although these wrong rewards contain no information about the underlying task or are even misleading, the policies learned from them in Fig. 1 (left) often perform significantly better than the behavior (data collection) policy and the behavior cloning (BC) policy. They even outperform policies trained with the true reward (denoted as _original_ in Fig. 1) in some cases. This is puzzling, since RL is notorious for being sensitive to reward mis-specification : in general, maximizing the wrong rewards with RL leads to sub-optimal performance that can be worse than simply performing BC. In addition, these wrong-reward policies demonstrate a "safe" behavior, which keeps the hopper from falling down for a longer period than other comparators in Fig. 1 (right). This is yet another peculiarity hard to link toreturn maximization, as none of the wrong rewards encourage the agent to stay alive. As we will show empirically in Section 4, these effects are not unique to ATAC or the hopper task. They occur with multiple offline RL algorithms, including ATAC , PSPI , IQL , CQL  and the Decision Transformer (DT) , on dozens of datasets from D4RL  and Meta-World  benchmarks.

This robustness of offline RL is not only counter-intuitive but also cannot be explained fully by the literature. Offline RL theory  provides performance guarantees only when the data reward is the true reward. Although offline imitation learning (IL)  makes no assumptions about reward, it only shows that the learned policy can achieve performance comparable to that of the behavior policy, not beyond. Robust offline RL  shows that specialized algorithms can perform well when the size of data perturbation is small. In contrast, we demonstrate that standard offline RL algorithms can produce good policies even when we completely change the reward. Constrained offline RL algorithms  can learn safe behaviors when constraint violations are both explicitly labeled and optimized. However, in the phenomena we observe, no safety signal is given to off-the-shelf, unconstrained offline RL algorithms. Recently,  observes a similar robustness phenomena of offline RL, but does not provide a complete explanation.1 In Appendix B, we discuss the gap between the related work and our findings in more detail.

In this work, we provide an explanation for this seemingly surprising observation. In theory, we prove that this robustness property is attributed to the interplay between the use of pessimism in offline RL algorithms and the implicit bias in typical data collection processes. Offline RL algorithms often use pessimism to avoid taking actions that lead to unknown future events. We show that this risk-averse tendency bakes a _"survival instinct"_ into the agent, an incentive to stay within the data coverage in the long term. On the other hand, the limited coverage of offline data further constrains the set of _survival policies_ (policies that remain in the data support in the long term). When this set of survival policies correlates with policies that achieve high returns w.r.t. the true reward (as in the example in Fig. 1), robust behavior emerges.

Our theoretical and empirical results have two important implications. First and foremost, offline RL has a survival instinct that leads to inherent robustness and safety properties that online RL does not have. Unlike online RL, offline RL is _doubly robust_: as long as the data reward is correct or the data has a positive implicit bias, a pessimistic offline RL agent can perform well. Moreover, offline RL is safe as long as the data only contains safe states; thus safe RL in the offline setup can be achieved without specialized algorithms. Second, because of the existence of the survival instinct, the data coverage has a profound impact on offline RL. While a large data coverage improves the best policy that can be learned by offline RL with the true reward, it can also make offline RL more sensitive to imperfect rewards. In other words, collecting a large set of diverse data might not be necessary or helpful (see Section 4). This goes against the common wisdom in the RL community that data should be as exploratory as possible .

We emphasize that survival instinct's implications should be taken into account when interpreting results on existing offline RL benchmarks as well as when designing future ones. We should treat the evaluation of offline RL algorithms differently from online RL algorithms. We suggest evaluating the performance of an offline RL algorithm by training it with wrong rewards in addition to the true reward so that we can isolate the performance due to return maximization from the compounded effects of survival instinct and implicit data bias. We propose to use this performance as a score to _quantify_ the data bias (see Eq. (3) in Section 4.1) in practice.

Figure 1: On the hopper task from D4RL , ATAC , an offline RL algorithm, can produce high-performance and safe policies even when trained on wrong rewards.

We believe that our findings shed new light on RL applicability and research. To practitioners, we demonstrate that offline RL does not always require the correct reward to succeed. This opens up the possibility of using offline RL in domains where obtain high-quality rewards is challenging. Research-wise, the existence of the survival instinct raises the question of how to design data collection or data filtering procedures that would help offline RL to leverage this instinct in order to improve RL's performance with incorrect or missing reward labels. While in this paper we focus on positive data biases, we caution that in practice the data bias might be negatively correlated with a user's intention. In that case, running offline RL even with the right reward would not lead to the right behavior.

## 2 Background

The goal of offline RL is to solve an unknown Markov decision process (MDP) from offline data. Typically, an offline dataset is a collection of tuples, \(\{(s,a,r,s^{})|(s,a)(,),r=r(s,a),s^ {} P(|s,a)\}\), where \(r\) is the reward, \(P\) captures the MDP's transition dynamics, and \(\) denotes the state-action data distribution induced by some data collection process. Modern offline RL algorithms adopt pessimism to address the issue of policy learning when \(\) does not have the full state-action space as its support. The basic idea is to optimize for a performance lower bound that penalizes actions leading to out-of-support future states. Such a penalty can take the form of behavior regularization [8; 29; 30], negative bonuses to discourage visiting less frequent state-action pairs [12; 31; 32], pruning less frequent actions [13; 33], adversarial training [2; 7; 14; 34; 35] or value penalties in modified dynamic programming [9; 36].

We are interested in settings where the offline RL agent learns not from \(\) itself but from its corrupted version \(}\) with a wrong reward \(\), i.e., \(}\{(s,a,,s^{})|(s,a,s^{}) ,=(s,a)[-1,1]\}\). We assume \(\) is from a reward class \(}\) -- which may not necessarily contain the true reward \(r\) -- and we wish to explain why offline RL can learn good behaviors from the corrupted dataset \(}\) (e.g., as in Fig. 1). To this end, we introduce notations and assumptions we will use in the paper.

NotationWe focus on the setting of infinite-horizon discounted MDPs. We denote the task MDP that the agent aims to solve as \(=(,,r,P,)\), where \(\) is the state space, \(\) is the action space, and \([0,1)\) is the discount. Without loss of generality, we assume \(r:\). Let \(()\) denote the set of probability distributions over a set \(\). We denote a policy as \(:()\). For a reward function \(r\), we define a policy \(\)'s state value function as \(V_{r}^{}(s)_{,P}[_{t=0}^{}^{t}r(s_{t },a_{t})|s_{0}=s]\) and the state-action value function as \(Q_{r}^{}(s,a) r(s,a)+_{s^{} P(|s,a )}[V_{r}^{}(s^{})]\). Solving MDP \(\) requires learning a policy that maximizes the return at an initial state distribution \(d_{0}()\), that is, \(_{}_{s d_{0}}[V_{r}^{}(s)]\). We denote the optimal policy as \(^{*}\) and the optimal value functions as \(V_{r}^{*}\) and \(Q_{r}^{*}\). Given \(d_{0}\), we define the average state-action visitation of a policy \(\) as \(d^{}(s,a)(1-)_{,P}[_{t=0}^{}^{t }d_{t}^{}(s,a)]\), where \(d_{t}^{}(s,a)\) denotes the probability of visiting \((s,a)\) at time \(t\) when running \(\) starting at an initial state sampled from \(d_{0}\). Note that \((1-)_{s d_{0}}[V_{r}^{}(s)]=_{(s,a) d^{ }}[r(s,a)]\). For a function \(f:\), we use the shorthand \(f(s,)=_{a|s}[f(s,a)]\); similarly for \(f:\), we write \(f(p)=_{s p}[f(s)]\), e.g., \(V_{r}^{}(d_{0})=_{s d_{0}}[V_{r}^{}(s)]\). For a distribution \(p\), we use \((p)\) to denote its support.

AssumptionWe make the typical assumption in offline RL that the data distribution \(\) assigns positive probabilities to these state-actions visited by running the optimal policy \(^{*}\) starting from \(d_{0}\).

**Assumption 1** (Single-policy Concentrability).: We assume \(_{s,a}}(s,a)}{(s,a)}<\).

This is a standard assumption in the offline RL literature, which in the worst case is a necessary condition to have no regret w.r.t. \(^{*}\). There are generalized notions [7; 34] of this kind, which are weaker but requires other smoothness assumptions. We note that Assumption 1 does not assume that \(\) is the average state-action visitation frequency of a single behavior policy, nor does it assume \(\) has a full coverage of all states and actions or all policy distributions [27; 38].

## 3 Why Offline RL can Learn Right Behaviors from Wrong Rewards

In this section, we provide conditions under which offline RL's aforementioned robustness w.r.t. misspecified rewards emerges. Our main finding is summarized in the theorem below.

**Theorem 1**.: _(Informal) Under Assumption 1 and certain regularity assumptions, if an offline RL algorithm \(Algo\) is set to be sufficiently pessimistic and the data distribution \(\) has a positive bias, for any data reward \(}\), the policy \(\) learned by \(Algo\) from the dataset \(}\)_has performance guarantee \(V_{r}^{^{*}}(d_{0})-V_{r}^{}(d_{0}) O()\) as well as safety guarantee \((1-)_{=0}^{}^{t}([0,t],s_{}()|) O()\) for a small \(\) that decreases to zero as the degree of pessimism and dataset size increase._

In other words, the robustness originates from an _interplay_ between pessimism and an implicit positive bias in data. Here is the insight on why Theorem 1 is true. Because of pessimism, offline RL endows agents with a "survival instinct" -- it implicitly solves a constrained MDP (CMDP) problem  that enforces the policy to stay within the data support. When combined with a training data distribution that has a positive bias (e.g., all policies staying within data support are near-optimal) such a survival instinct results in robustness to reward misspecification.

Overall, Theorem 1 has two important implications:

1. Offline RL is _doubly robust_: it can learn near optimal policies so long as either the reward label is correct or the data distribution is positively biased;
2. Offline RL is _intrinsically safe_ when data are safe, regardless of reward labeling, without the need of explicitly modeling safety constraints.

In the remaining of this section, we provide details and discussion of the statements above. The complete theoretical statements and proofs can be found in Appendix D.

### Intuitions for Survival Instinct and Positive Data Bias

We first use a grid world example to build some intuitions. Fig. 2 shows a goal-directed problem, where the true reward is +1 and -1 upon touching the key and the lava, respectively. The offline data is suboptimal and does not have full support. All data trajectories that touched the lava were stopped early, while others were allowed to continue until the end of the episode. The goal state (key) is an absorbing state, where the agent can stay _forever_ beyond the episode length. We use the wrong rewards in Fig. 1 to train PEVI , a finite-horizon, tabular offline RL method. PEVI performs dynamic programming similar to value iteration, but with a pessimistic value initialization and an instantaneous pessimism-inducing penalty of \(O(-^{1})\), where \(n(s,a)\) is the empirical count in data. The penalties ensure that the learned value lower bounds the true one.

We see the PEVI agent learned with any wrong reward in Fig. 1 is able to solve the problem despite data imperfection, while the BC agent that mimics the data directly fails. The main reasons are: _1)_ There is a _data bias_ whereby longer trajectories end closer to the goal (especially, the longest trajectories are the ones that reach the goal, since the goal is an absorbing state). We call this a _length bias_ (see Section 3.3). This positive data bias is due to bad trajectories (touching the lava or not reaching the goal) being cut short or timing out. _2)_ Pessimism in PEVI gives the agent an _algorithmic bias_ (i.e., survival instinct) that favors longer data trajectories. Because of the pessimistic value initialization, PEVI treats trajectories shorter than the episode length as having the lowest return. As a result, by maximizing the pessimistically estimated values, PEVI learns good behaviors despite wrong rewards by leveraging the survival instinct and positive data bias _together_. We now make this claim more general.

### Survival Instinct

Survival instinct is a pessimism induced behavior that offline RL algorithms tend to favor policies leading to _in-support trajectories_. We formally characterize this risk aversion behavior by the concept of constrained MDP (CMDP) , which we define below.

**Definition 1**.: Let \(f,g:[-1,1]\). A _CMDP_\((,,f,g,P,)\) is a constrained optimization problem: \(_{}V_{f}^{}(d_{0})\) s.t. \(V_{g}^{}(d_{0}) 0\). Let \(^{}\) denote its optimal policy. For \( 0\), we define the set of \(\)_-approximately optimal policies \(^{}_{f,g}()\{:V_{f}^{^{}}(d_{0})-V_{f}^{ }(d_{0}),V_{g}^{}(d_{0})\}\)._

We prove that when trained on \(\), offline RL, because of its pessimism, implicitly solves the CMDP below even when the algorithm does not explicitly model any constraints:

\[_{}()(,, ,c_{},P,),\] (1)

where \(c_{}(s,a)[(s,a)=0]\) indicates whether \((s,a)\) is out of the support of \(^{2}\) i.e., the

Figure 2: A grid world. BC (red); offline RL with wrong rewards (blue). The opacity indicates the frequency of a state in the data (more opaque means more frequent). Offline RL with the three wrong rewards produces the same policy.

constraint in Eq. (1) enforces the agent's trajectories to stay within the data support. Note the constraint in this CMDP is feasible because of Assumption 1.

**Proposition 1** (Survival Instinct).: _(Informal) Under certain regularity conditions on \(_{}()\), the policy learned by an offline RL algorithm \(\) with the offline dataset \(\) is \(\)-approximately optimal with respect to \(_{}()\), for some small \(\) which decreases as the algorithm becomes more pessimistic._

Proposition 1 says if an offline RL algorithm is sufficiently pessimistic, then the learned policy is approximately optimal to \(_{}()\). The learned policy has not only small regret with respect to the data reward \(\), but also small chances of escaping the support of the data distribution \(\).

We highlight that the survival instinct in Proposition 1 is a _long-term_ behavior. Such a survival behavior cannot be achieved by myopically taking actions in the data support in general (such as BC).3 For instance, when some trajectories generating the data are truncated (e.g., due to early-stopping or intervention for safety reasons), taking in-support actions may still lead to out-of-support states in the future, as the BC agent in Section 3.1.

Proof SketchThe key to prove Proposition 1 is to show that an offline RL algorithm by pessimism has small regret for not just \(\) but a set of reward functions consistent with \(\) on data but different outside of data, including the Lagrange reward of (1) (i.e., \(- c_{}\), with a large enough \( 0\)). We call this property admissibility and we prove in Appendix F that many existing offline RL algorithms are admissible, including model-free algorithms ATAC , VI-LCB  PPI/PQI , PSPI , as well as model-algorithms, ARMOR , MOPO , MOReL , and CPPO . By Proposition 1 these algorithms have survival instinct. Please see Appendix D for details.

### Positive Data Bias

In addition to survival instinct, another key factor is an implicit positive bias in common offline datasets. Typically these data manifest meaningful behaviors. For example, in collecting data for goal-oriented problems, data recording is normally stopped if the agent fails to reach the goal within certain time limit. Another example is problems (like robotics or healthcare) where wrong decisions can have detrimental consequences, i.e., problems that safe RL studies. In these domains, the data are collected by qualified policies only or under an intervention mechanism to prevent catastrophic failures. Such a one-sided bias can creates an effect that staying within data support would lead to meaningful behaviors. Below we formally define the positive data bias; Later in Section 4 (Fig. 5), we provide empirical estimates of the degree of positive data bias.

**Definition 2** (Positive Data Bias).: A distribution \(\) is \(\)_-positively biased_ w.r.t. a reward class \(}\) if

\[_{}}_{^{}_{,c_{ }}()}V_{r}^{^{*}}(d_{0})-V_{r}^{}(d_{0})+O()\] (2)

for all \( 0\), where \(^{}_{,c_{}}()\) denotes the set of \(\)-approximately optimal policy of \(_{}()\).

We measure the degree of positive data bias based on how bad a policy can perform in terms of the true reward \(r\) when approximately solving the CMDP \(_{}()\) in (1) defined with the wrong reward \(}\). If the data distribution \(\) is \(\)-positively biased, then any approximately optimal policy to \(_{}()\) (which includes the policies learned by offline RL, as shown earlier) can achieve high return in the true reward \(r\). We also can view the degree of positiveness as reflecting whether \(\) provides a similar ranking as \(r\), _among policies within the support of \(\)_. When there is a positive bias, offline RL can learn with \(\) to perform well under \(r\), even when \(\) is not aligned with \(r\) globally. This is in contrast to online RL, which requires global alignment due to the exploratory nature of online RL.

ExamplesWe provide a few examples of positive data bias. (See Appendix D.4 for proofs.)

* The distribution \(d^{}\) induced by any policy \(\) is \(\)-positively biased for any rewards resulting from potential-based reward shaping  since it provides the same ranking for all policies.
* For an IL setup, the data distribution \(\) is \(\)-positively biased for any rewards \(:\) if the data is generated by the optimal policy (i.e., \(=d^{^{*}}\)). This is because following the optimal policy is the only way to stay within the data support. In Section 4, we empirically show that offline RL algorithms can learn good policies with wrong rewards on D4RL expert datasets, which are collected by near-optimal policies.
* A positive bias happens when longer trajectories in the data have smaller optimality gap. This is a generalized formal definition of the _length bias_ mentioned in Section 3.1. This condition is typically satisfied when intervention is taken in the data collection process (despite the data collection policy being suboptimal), as in the motivating example in Fig. 1. Later in Section 4, we will investigate deeper into this kind of length bias empirically.

RemarkWe highlight that the positive data bias assumption in Definition 2 is _different_ from assuming that the data is collected by expert policies, which is typical in the IL literature. Positive data bias assumption can hold in cases when data is generated by highly suboptimal policies, which we observe in the hopper example from Section 1. On the other hand, there are also cases where IL can learn well, while positive data bias does not exist (e.g., when learning from data collected by a stochastic expert policy that covers highly suboptimal actions with small probabilities).

### Summary: Offline RL is Doubly Robust and Intrinsically Safe

We have discussed the survival instinct from pessimism and the potential positive data bias in common datasets. In short, survival instinct enables offline RL algorithms to learn policies that benefit from a favorable inductive bias of staying within the support of a positive data distribution. As a result, offline RL becomes robust to reward mis-specification (namely, Theorem 1).

Below we discuss some direct implications of Theorem 1. First, we can view this phenomenon as a doubly robust property of offline RL. We borrow the name "doubly robust" from the offline policy _evaluation_ literature  to highlight the robustness of offline RL to reward mis-specification as an offline policy _optimization_ approach.

**Corollary 1**.: _Under Assumption 1, offline RL can learn a near optimal policy, as long as the reward is correct, or the data has a positive implicit bias._

Theorem 1 also implies that offline RL is an intrinsically safe learning algorithm, unlike its counterpart online RL where additional penalties or constraints need to be explicitly modelled [43; 44; 45; 46; 47].

**Corollary 2**.: _If \(\) only covers safe states and there exists a safe policy staying within the support of \(\), then the policy of offline RL only visits safe states with high probability (see Theorem 1)._

We should note that covering safe states is a very mild assumption in the safe RL literature [45; 48], e.g., compared with all (data) states have a safe action. It does not require the data collection policies that generate \(\) are safe. This condition can be easily satisfied by filtering out unsafe states in post processing. The existence of a safe policy is also mild and common assumption. In Section 4.2, we validate this inherent safety property on offline SafetyGymnasium , an offline safe RL benchmark.

## 4 Experiments

We conduct two group of experiments. In Section 4.1, we conduct large scale experiment, showing that multiple offline RL algorithms can be robust to reward mis-specification on a variety of datasets. In Section 4.2, we experimentally validate the inherent safety of offline RL algorithms stated in Corollary 2. We show that admissible offline RL algorithms, without modifications, can achieve state-of-the-art performance on an offline safe RL benchmark .

### Robustness to Mis-specified Rewards

We empirically study the performance of offline RL algorithms under wrong rewards in a variety of tasks. We use the same set of wrong rewards as in Fig. 1. _1)_ zero: the zero reward, _2)_ random: labeling each transition with a reward value randomly sampled from \(\), and _3)_ negative: the negation of true reward. We consider five offline RL algorithms, ATAC , PSPI , IQL , CQL  and decision transformer (DT)4. We deliberately choose offline RL algorithms to cover those that are provably pessimistic [2; 7] and those that are popular among practitioners [8; 9], as well as an unconventional offline RL algorithm . We consider a variety of tasks from D4RL  and Meta-World  ranging from safety-critical tasks (i.e., the agent dies when reaching bad states),goal-oriented tasks, and tasks that belong to neither. We train a total of around \(16\)k offline RL agents (see Appendix C.8). Please see Appendix C for details.

MessagesWe would like to convey three main messages through our experiments. First, implicit data bias _can_ exist naturally in a wide range of datasets, and offline RL algorithms that are sufficiently pessimistic can leverage such a bias to succeed when given wrong rewards. Second, offline RL algorithms, regardless of how pessimistic they are, become sensitive to reward when the data does not possess a positive bias. Third, offline RL algorithms without explicit pessimism, e.g., IQL , can sometimes still be pessimistic enough to achieve good performance under wrong rewards.

Remark on negative resultsWe consider "negative" results, i.e., when offline RL _fails_ under wrong rewards, as important as the positive ones. Since they tell us _how_ positive data bias can be broken or avoided. We hope our study can provide insights to researchers who are interested in actively incorporating positive bias in data collection, as well as who hope to design offline RL benchmarks specifically with or without positive data bias.

#### 4.1.1 Locomotion Tasks from D4RL

We evaluate offline RL algorithms on three locomotion tasks, hopper, walker2d5 and halfcheetah, from D4RL , a widely used offline RL benchmark. For each task, we consider five datasets with different qualities: random, medium, medium-replay, medium-expert, and expert. We refer readers to  for the construction of these datasets. We measure policy performance in D4RL normalized scores . We provide three baselines 1) behavior: normalized score directly computed from the dataset, 2) BC: the behavior cloning policy, and 3) original: the policy produced by the offline RL algorithm using the original dataset (with true reward). The normalized scores for baselines and offline RL algorithms with wrong rewards are presented in Fig. 3. The exact numbers can be found in tables in Appendix C.

Positive bias exists in some D4RL datasets.We visualize the length bias of D4RL datasets in Fig. 4. Here are our main observations. _1)_ Most datasets for hopper and walker2d, with the

Figure 3: Normalized scores for locomotion tasks from D4RL . The mean and standard error for normalized scores are computed across \(10\) random seeds. For each random seed, we evaluate the final policy of each algorithm over \(50\) episodes.

exception of the medium-replay datasets, have a strong length bias, where longer trajectories have higher returns. This length bias is due to the safety-critical nature of hopper and walker2d tasks, as the trajectories get terminated when reaching bad states. The length bias is especially salient in hopper-medium, where the normalized score is almost proportional to episode length. _2)_ The medium-replay datasets for hopper and walker2d have more diverse behavior, so the bias is smaller. _3)_ All halfcheetah datasets do not have length bias, as they all have the same length of \(1000\). _4)_ hopper-expert dataset has a length bias, while walker2d and halfcheetah-expert datasets do not have an obvious length bias. However, it is worth noting that all expert datasets have an IL-type positive bias as discussed in Section 3.3.

Offline RL can learn good policies with wrong rewards on datasets with strong length bias.On datasets with strong length bias (hopper-random, hopper-medium and walker2d-medium), we observe that ATAC and PSPI with wrong rewards generally produce well-performing policies, in a few cases even out-performing the policies learned from the true reward (original in Fig. 4). DT is mostly insensitive to reward quality. IQL and CQL with wrong rewards can sometimes achieve good performance; among the two, we find IQL to be more robust to wrong rewards and CQL with wrong rewards almost fails completely on all walker2d datasets.

Offline RL can learn good policies with wrong rewards on expert datasetsIn Section 3, we point out that the data has a positive bias when it is generated by the optimal policy. In Fig. 3, we observe that all offline RL algorithms, when trained with wrong rewards, can achieve expert-level performance on walker2d and halfcheetah-expert datasets. ATAC, PSPI and DT perform well on the hopper-expert dataset while IQL and CQL receive lower scores when using wrong rewards.

Offline RL needs stronger reward signals on datasets with diverse behavior policy.The medium-replay datasets of hopper and walker2d are generated from multiple behavior policies. Due to their diverse nature, they are a multiple ways to stay within data support in a long term. As a result, the survival instinct of offline RL by itself is not sufficient to guarantee good performance. Here algorithms with wrong rewards generally under-perform the policies trained with true reward. As datasets have a more diverse coverage, they can be less positively biased and offline RL requires a stronger reward signal to differentiate good survival policies and the bad ones. Practically speaking, when high-quality reward is not available, a diverse dataset can even hurt offline RL performance. This is contrary to the common belief that larger data support is more preferable .

Offline RL requires good reward to perform well on datasets without length bias.In all four halfcheetah datasets, the trajectories all have the same length, as is demonstrated in Fig. 4. This means that there is no data bias. We observe that all algorithms with wrong rewards at best perform similarly as the behavior and BC policies in most cases; this is an imitation-like effect due to the survival instinct. In the halfcheetah-medium-expert dataset, since there are a variety of surviving trajectories, with score from \(0\) to near \(100\), we observe that the performance of the resulting policy degrades as data reward becomes more different from the true reward.

Offline RL algorithms with provable pessimism produce safer policies.For hopper and walker2d, we observe that policies learned by ATAC and PSPI, regardless of the reward, can keep the agent from falling for longer. The episode lengths of IQL and CQL policies are often

Figure 4: A visualization of length bias in datasets from D4RL . Each plot corresponds to a dataset for a task (row) with a dataset (column). Each trajectory in a dataset is represented as a data point with the \(x\)-coordinate being its episode length and \(y\)-coordinate being its normalized score.

comparable to that of the behavior policies. We provide statistics of episode lengths in Appendix C. In Section 4.2, we provide further results validating the safety properties of ATAC and PSPI.

On empirically estimating positive data biasInspired by Definition 2, we propose an empirical estimate of positive data bias. Let \(J_{}\), \(J_{}\) and \(J_{}\) be the return (with respect to the true reward) of an offline RL algorithm learned with zero, random, and negative rewards, respectively. Then, given an estimated optimal return \(^{}\), we propose to empirically estimate the positive data bias by

\[=}}{{\{^{} -\{J_{},J_{},J_{}\},0\}}}.\] (3)

In Fig. 5, we visualize the estimated positive data bias of all \(15\) D4RL datasets given by ATAC. Estimated positive data bias of all D4RL and Meta-World datasets is given in Fig. 9.

Remark on benchmarking offline RLOur observations on the popular D4RL datasets raise an interesting question for benchmarking offline RL algorithms. An implicit positive data bias can give certain algorithms a hidden advantage, as they can already achieve good performance without using the reward. Without controlling data bias, it is hard to differentiate whether the performance of those algorithms are due to their ability to maximize returns or simply due to their survival instinct.

#### 4.1.2 Goal-oriented Manipulation Tasks from Meta-World

We study goal-oriented manipulation tasks from the Meta-World benchmark . We consider 15 goal-oriented tasks from Meta-World: the 10 tasks from the MT-10 set and the 5 testing tasks from the ML-45 set. For each task, we generate a dataset of \(110\) trajectories using the scripted policies provided by ; \(10\) are produced by the scripted policy, and \(100\) are generated by perturbing the scripted policy with a zero-mean Gaussian noise of standard deviation \(0.5\). In the datasets, unsuccessful trajectories receive a time-out signal after \(500\) steps (maximum episode length for Meta-World).

For each task, we measure the success rate of policies for \(50\)_new_ goals unseen during training. Similar to D4RL experiments, we consider BC policies and policies trained with the true reward (original) as baselines. Since the training dataset is generated for a different set of goals, we do not compare the success rate of learned policies with the success rate in data.

Goal-oriented problems have data bias.Goal-oriented problems are fundamentally different from safety-critical tasks (such as hopper and walker2d). A good goal-oriented policy should reach the goal as fast as possible rather than wandering around until the end of an episode. But another form of length bias still exists here, as successful trajectories are labeled with a termination signal that indicates an unbounded length and failed trajectories have a bounded length (see Section 3.1).

Offline RL can learn with wrong rewards on goal-oriented tasks.The success rate of learned policies with different rewards are shown in Fig. 6. We observe that ATAC and PSPI with wrong rewards generally achieve comparable or better success rate than BC policies. The exceptions are often due to that ATAC or PSPI, even with the true reward, does not work as well as BC in those tasks, e.g. drawer-open, bin-picking and box-close. This could be caused by overfitting or optimization errors (due to challenges of dynamics programming over long horizon problems). In a number of tasks, such as peg-insert-side, push and reach, ATAC and PSPI with wrong rewards can out-perform BC by a margin. This shows that data collection for goal-oriented problems have a positive bias that offline RL algorithms can succeed without true reward. This is remarkable as when learning with random and negative rewards, unsuccessful trajectories are long and generally have significantly higher return, as reward for each step is non-negative. The offline RL algorithms need to be sufficiently pessimistic and be able to plan over a long horizon to propagate pessimism to the unsuccessful data trajectories. For IQL, there is a gentle performance decrease as the reward becomes more different than the true reward, even though policies learned with wrong rewards are not much worse than BC in many cases. CQL shows robustness to reward in a few tasks such as button-press, drawer-close and door-unlock. Interestingly, DT performs almost uniformly across all rewards, potentially because DT does not explicitly maximize returns.

Figure 5: Estimated positive data bias of all \(15\) D4RL datasets given by ATAC. Datasets marked by “\(\)”, i.e., hopper-expert and walker2d-expert, have infinite positive bias.

Remark on BC performanceIt is noticeable that BC policies in general achieve high performance, in many tasks often out-performing offline RL policies learned with true reward. This effect has also been observed in existing work on similar tasks . We would like to clarify that the goal of our experiments is to study the behavior of offline RL algorithms when trained with wrong rewards, rather than showing that offline RL performs better than BC. We refer interested readers to existing studies [50; 51] on when using offline RL algorithms is or is not preferable over BC.

### Inherent Safety Properties of Offline RL Algorithms

We show offline RL algorithms (without any modifications) can behave as a safe RL algorithm. We conduct experiments on offline SafetyGymnasium  using ATAC  and PSPI . We first remove all transitions with non-zero cost and then run offline RL algorithms on the filtered datasets.

The results are summarized in Table 6 in Appendix C.6. We observe that _offline RL with this naive data filtering strategy (using less data) can achieve comparable performance as the per-task best performing state-of-the-art offline safe RL algorithm_, which uses the full dataset and has knowledge of the cost target. Our agents in general incur low cost except for the circle tasks. We hypothesize that learning a safe policy from the circle datasets is hard, as other baselines also struggle to produce safe policies. Note that these are preliminary results, and better performance might be achievable through using a more sophisticated data filtering strategy. Please see Appendix C.6 for experiment details.

## 5 Concluding Remarks

We present unexpected results on the robustness of offline RL to reward mis-specification. We show that this property originates from the interaction between the _survival instinct_ of offline RL and hidden positive biases in common data collection processes. Our findings suggest extra considerations should be taken when interpreting offline RL results and designing future benchmarks. In addition, our findings open a new space for offline RL research: the possibility of designing algorithms that proactively leverage the survival instinct to learn policies for domains where rewards are nontrivial to specify or unavailable. Please see Appendix A for a discussion on limitations and broader impacts.

Figure 6: Success rate for goal-oriented tasks from Meta-World . The average success rate and confidence interval are computed across \(10\) random seeds. For each seed, we evaluate final policies for \(50\) episodes, each with a new goal unseen in the dataset.