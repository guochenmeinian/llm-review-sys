# Adaptive Important Region Selection with Reinforced Hierarchical Search for Dense Object Detection

Dingrong Wang

Rochester Institute of Technology

Rochester, NY 14623

dw7445@rit.edu

&Hitesh Sapkota

Amazon Inc.

Sunnyvale, CA 94089

sapkoh@amazon.com

&Qi Yu

Rochester Institute of Technology

Rochester, NY 14623

qi.yu@rit.edu

Corresponding author,  Work was completed during the PhD study at RIT, which is not related to the position at Amazon.

###### Abstract

Existing state-of-the-art dense object detection techniques tend to produce a large number of false positive detections on difficult images with complex scenes because they focus on ensuring a high recall. To improve the detection accuracy, we propose an Adaptive Important Region Selection (AIRS) framework guided by Evidential Q-learning coupled with a uniquely designed reward function. Inspired by human visual attention, our detection model conducts object search in a top-down, hierarchical fashion. It starts from the top of the hierarchy with the coarsest granularity and then identifies the potential patches likely to contain objects of interest. It then discards non-informative patches and progressively moves downward on the selected ones for a fine-grained search. The proposed evidential Q-learning systematically encodes epistemic uncertainty in its evidential-Q value to encourage the exploration of unknown patches, especially in the early phase of model training. In this way, the proposed model dynamically balances exploration-exploitation to cover both highly valuable and informative patches. Theoretical analysis and extensive experiments on multiple datasets demonstrate that our proposed framework outperforms the SOTA models.

## 1 Introduction

Dense object detection enjoys a wide range of applications in diverse domains [21; 40]. Representative use cases include surveillance video tracking by the police and merchandise recognition for online shopping [42; 6]. Despite its importance, dense object detection is an inherently challenging task as it requires predicting the bounding boxes for all objects present in a given image irrespective of their shape, size, and number. The inborn complexity of images, such as shadow/occlusion, image size, shape, color, and texture could also pose a significant hindrance in the detection process resulting in a lower accuracy .

Existing efforts have contributed different techniques to address the key challenges in dense object detection. For instance, two-stage approaches have been popularized where the first stage extracts candidate objects and the second stage classifies the extracted object while providing the bounding boxes through a regression network . Representative two-stage detectors include R-CNN  and Faster R-CNN . Two-stage detectors are limited in the number of candidate object found in first stage through the regional proposal network (RPN) and suffer from a low recall, especially in dense scenarios. To tackle this, one-stage approaches have been explored that achieve a higher recall along with faster training and inference [26; 38]. There are mainly two types of one-stage approaches: Anchor-based (_e.g.,_ RetinaNet ) and Anchor-free (_e.g.,_ FCOS ). The former computes the bounding box of an object by regressing the offsets from a predefined anchor box whereas the latter directly outputs the position and size of an object. However, existing one-stagedetectors usually exhibit inconsistency in localization quality estimation between training and testing, stemming from the lack of supervision and resulting in many false positive anchors as shown in . In testing, some negative anchors may generate an unusually high-quality estimation score and be selected as positive anchors (_i.e.,_ false positives) due to lack of supervision. GFocal  alleviates this issue by leveraging the Focal Loss and aligning the localization quality and classification branches into a unified representation. While it achieved improved performance compared to previous one-stage detectors, GFocal still suffers from the problem of generating too many false positive predictions on small objects in a complex background because of selected low quality positive anchors, as shown in 1 (a) and (c).

Our analysis reveals that the non-adaptive criterion (which favors a high recall) in existing one-stage detectors does not capture diverse types of candidate anchors residing on the Feature Pyramid Network (FPN)  and will result into many false positive anchors. This phenomenon becomes prominent when testing images are difficult with complex/noisy background. To tackle this challenge, we propose to conduct **A**daptive **I**mportant **R**egion **S**election (**AIRS**) that is guided by a reinforcement learning (RL) agent performing _Evidential Q-learning_ with a uniquely designed reward function. Similar to human visual attention, **A**IRS** conducts object search in a top-down, hierarchical fashion. Benefiting from the top-down paradigm, the top layer in the hierarchy with a coarser granularity helps the model quickly identify most interesting regions that likely contain objects of interest. Only within those regions, the model performs a fine-grained search to more precisely locate the objects. Intuitively, the model only searches the patches from lower levels if the RL agent collects sufficient evidence on higher level supporting the presence of a potentially valuable object according to the learned evidential Q-value.

Furthermore, in the early phase of RL agent training, **A**IRS** also encourages the agent to explore highly uncertain patches by leveraging the epistemic uncertainty provided by our evidential Q-value. Exploration of novel patches is also dynamically balanced with the exploitation of predicted high quality region. As a result, **A**IRS** ensures that all potential patches have been adequately covered during the search process while avoiding the attendance of low-quality patches from fine-grained layers, leading to much improved precision without sacrificing the recall. As can be seen from Figure 1 (b) and (d), **A**IRS** is able to identify all objects without producing any false positive bounding boxes. In contrast, GFocal, as shown in Figure 1(a) and (c), produces many false positive patches by paying too much attention on low-level details to maintain a high recall. To assess the effectiveness of **A**IRS**, we perform extensive experiments on multiple real-world datasets with complex objects/backgrounds. Furthermore, we conduct a thorough theoretical analysis to show the convergence guarantee of our proposed evidential Q-learning. Our contributions are summarized below:

* an adaptive hierarchical object detection paradigm supported by an RL agent to mimic human visual attention that performs searching in the top-down fashion,
* novel evidential Q-learning driven by a unique reward function, covering both potentially positive and highly uncertain patches through dynamically balancing exploitation and exploration,
* theoretical guarantee on the fast convergence of the proposed evidential Q-learning algorithm,
* SOTA object detection performance outperforming strong baselines on challenging datasets.

## 2 Related Work

**Object Detection.** In dense object detection , both two-stage and one-stage detectors have been investigated in existing literature. Representative techniques in the former group include R-CNN , Faster R-CNN , where the detection involves two stages. Considering the limitation of

Figure 1: Bounding boxes produced by GFocal , GFocal-V2 , and **A**IRS**, where GFocal, GFocal-V2 still tend to generate unnecessary bounding boxes resulting from false positive anchors, comparing to the proposed **A**IRS** model.

slow training and inference in two-stage detectors, multiple one-stage detectors have been proposed, such as FCOS, ATSS and GFocal series [38; 45; 23; 22]. However, these methods still suffer from attending too many false positive anchors (region proposals) resulting from the architecture design and the training setting. To overcome this, we design an uncertainty-guided RL to perform hierarchical search that effectively reduces the false positive detections.

**Deep Reinforcement Learning for Object Detection.** DRL formulates object detection problem as a Markov-Decision Process (MDP). It tries to find the salient parts of an image that are more probable to contain a target object, and then further zoom into them [4; 8]. There are generally two different action settings for this MDP. In the first setting, a hierarchical method is proposed by , where the agent chooses to focus on one of the 5 sub-regions of the image (ie. top-left, top-right, bottom-left, bottom-right, center) at each time step. In the second setting, a dynamic method is proposed by , where the agent deforms a bounding box using simple transformation actions (horizontal moves, vertical moves, scale changes, and aspect ratio changes) at each step to find the specific location of an object in the image. However, the above methods only detect a fixed number of objects. To overcome this issue, Ba et al. introduce a deep recurrent attention model (RAM) to recognize multiple objects . Further, Zhou et al. propose ReinforceNet, which performs region selection and refinement by integrating RL's action space with CNN based feature space . Different from all these works, AIRS leverages the advanced Feature Pyramid Network structure and performs RL-driven hierarchical search guided by epistemic uncertainty with much improved detection performance.

**Uncertainty in Deep Learning.** There have been different approaches to quantify uncertainty in deep learning models. Sensoy et al.  propose an Evidential Deep Learning (EDL) network, which treats the predicted multi-class probability as a multinomial opinion as developed in subjective logic . Malinin et al.  propose Prior Networks (PNs) that consider distributional mismatch to explicitly quantify the distributional uncertainty. Amini et al.  propose an evidential regression network that quantifies the aleatoric and epistemic uncertainty based on the hyper-parameters. Different from previous work, we propose novel evidential deep Q-learning through an evidential regression Q-network to quantify the epistemic uncertainty, which is used to explore regions that the RL-agent is less familiar with.

## 3 Methodology

In this section, we first present the overall object detection process. We then go through each of the key component in our framework. We conduct a theoretical analysis to show how the proposed AIRS leverages effective exploration and hierarchical search to ensure theoretically guaranteed performance. Finally, we describe the training and inference process and explain how AIRS could be used for real-world object detection problems.

### The Overall Detection Process

The overall detection process is guided by an RL agent as shown in Figure 1(a). Our RL environment consists of a Feature Pyramid Network (FPN) that projects an input image into different resolutions organized into a hierarchical structure. As an example, the FPN in Fig. 1(a) has three layers with P5 having the lowest resolution and P3 having the highest resolution (these numbers follow the original FPN ). Once a patch is selected by the RL agent, it is passed through the feature extractor followed by the recurrent neural network (RNN) to generate the state representation. The state representation then goes through the evidential Q-network, which formulates an evidential Normal-inverse Gaussain (NIG) distribution and outputs the Q-value estimate for each available action. Then by combining with the corresponding epistemic uncertainty in the Q-value estimate, we have the evidential Q-value which balances the estimated Q-value with the (lack of) knowledge of the RL agent on the chosen action. Thus, in the initial phase of RL training, there are more uncertain patches and the RL agent is expected to explore more actively. As training progresses, the agent should try to choose those actions leading to patches (regions) that are most valuable. Based on the evidential Q-value obtained by balancing epistemic uncertainty and estimated Q-value, the agent takes action, and then selects the next patch with the goal of maximizing the expected reward.

### Description of Key Components

**State Generation.** To generate a state, we rely on the patches produced by RL environment from one of the \(L+1\) layers in the FPN, where \(L\) is the top layer and 0 being the bottom one.

In the FPN, each patch in the higher layer (low resolution) is mapped to multiple patches in a lower layer (high resolution). Once a patch from layer \(l\) is selected, it is passed through the feature extractor to get the feature embedding vector \(_{t}^{d}\). Finally, the feature embedding and previous state representation (\(_{t-1}\)) are concatenated and passed through the RNN to generate the next state representation \(_{t}=(_{t},_{t-1};_{r})\) where \(_{0}=\) and \(_{r}\) is the parameters associated with the RNN. In this way, every state \(_{t}\) captures the knowledge of previous observations from environment via \(_{t-1}\).

**Evidential Q-Learning.** To ensure detection of all positive patches, it is crucial to perform effective exploration. To achieve this, we design evidential Q-learning that performs exploration in a systematic way by leveraging epistemic uncertainty. Let \(D\) denote the size of the action space. The Q-value for \(d^{th}\) action \(q_{d,t}\) given state \(_{t}\) is assumed to draw from a Gaussian distribution with mean \(_{d,t}\) and variance \(^{2}_{d,t}\). We further place Gaussian and Inverse-Gamma priors on the mean and variance, respectively :

\[q_{d,t}(|_{d,t},^{2}_{d,t}),\ _{d,t}(| _{d,t},^{2}_{d,t}^{-1}_{d,t}),\ ^{2}_{d,t}(|_{d,t},_{d,t})\] (1)

where \((_{d,t},_{d,t})\) is the Inverse Gamma distribution  and \((_{d,t},_{d,t},_{d,t},_{d,t})\) are evidential Q-network outputs that form the evidential distributions as specified above. From these distributions, we can sample mean \(_{d,t}\) and variance \(^{2}_{d,t}\) to generate Q-value estimate \(q_{d,t}\). It should be noted that, because of the Inv-Gamma term, effective sampling for \(q_{d,t}\) through the reparametrization trick  becomes difficult. We instead generate mean and variance with their expectations:

\[[_{d,t}]=_{d,t},[^{2}_{d,t}]=}{(_{d,t}-1)}\] (2)

Thus, the \(q\)-value is sampled as

\[q_{d,t}(|_{d,t},}{(_{d, t}-1)})\] (3)

Using this trick, the gradient could be easily traced back to evidential Q-network's hyper-parameter outputs \((_{d,t},_{d,t},_{d,t},_{d,t})\) for each action \(a_{d,t}\) from the corresponding Q-value \(q_{d,t}\) in evidential Q-learning. In addition to the Q-value estimate, it is also essential to integrate uncertainty to facilitate exploration of unknown patches, leading to the following evidential Q-value

\[q^{e}_{d,t}=q_{d,t}+[_{d,t}],[_{d,t}]= [^{2}_{d,t}]}{_{d,t}}=}{_{d,t}( _{d,t}-1)}\] (4)

where \([_{d,t}]\) captures the epistemic uncertainty and \(\) balances epistemic uncertainty and Q-value. The evidential decomposition of the total uncertainty allows us to separate uncertainty caused by the noise in the data (_i.e.,_ aleatoric uncertainty or \([^{2}_{d,t}]\)) and uncertainty caused by lack of knowledge (_i.e.,_ epistemic uncertainty or \([_{d,t}]\)). Since the evidential-Q value only integrates with the epistemic uncertainty, it ensures that the exploration will focus on improving the knowledge of the agent while being robust to the noise in the data.

To generate the action vector, we consider both \(q^{e}_{d,t}\) and the constraints that avoid the agent moving into an invalid region that includes already visited patches and void space (_e.g.,_ downward movements from layer 0, upward movement from layer \(L\)). To this end, we define a mask vector. Let

Figure 2: AIRS training and test pipelines

\(m_{l,t}^{d}\) be the mask value (binary) associated with the \(d^{th}\) action in \(l^{th}\) layer in \(t^{th}\) time step then the masked evidential Q-value is

\[_{d,t}^{e}}=_{d,t}^{e}_{l,t}^{d}\] (5)

Let \(j=_{d}\{^{e}}\}_{d=0}^{D-1}\), then the action value for each entry \(d\{0,1,..,D-1\}\) is updated:

\[a_{d,t}=1,d=j;\\ 0,\] (6)

More detailed information of the constraint mask design can be found in Appendix C.3.

Based on the masked evidential Q-value \(_{d,t}^{e}}\), the RL agent selects the best action \(a_{t}\) and receives a reward \(r(_{t},_{t})\). The agent repeats the selection process until reaching a limit \(T\) steps or arriving at the terminal condition (_i.e.,_ upward movement in top-most layer \(L\)) is triggered. During each step, the agent stores the tuple of \(_{t},_{t},r_{t},_{t+1}\) into a replay buffer. After collecting \(K\) training tuples, one batch of training tuples is sampled for off-policy Q-learning. The following loss is used to update the feature extractor \((_{f})\), RNN \((_{r})\), and evidential Q-network \((_{e})\):

\[_{_{f},_{r},_{e}}=(q_{d,t}(_{f},_{r}, _{e})-})^{2}\,,\;}=r( _{t},_{t})+_{s_{t+1} D}_{d}(q_{d,t+1})\] (7)

where \(}\) is evaluated using the Bellman equation.

**Action Interaction.** The action interaction module translates the action into the location of the next patch to be selected. It considers a \(D\) dimensional action vector, where the first \(D-1\) actions: \(a_{d,t},d[0,..,D-2]\) are downward movements that direct the agent currently located on layer \(l\) into one of its mapping sub-patches in layer \(l-1\) with \(p_{i,t}^{l}\) being the current selected patch from \(l\) layer. The location of the selected sub-patch from \(l-1\) layer is provided by the index of the action-value whose entry is set to 1 (_e.g.,_\(a_{0,t}=1\) means top-left and \(a_{1,t}=1\) means top-right). The last action \(a_{D-1,t}\) denotes the upward movement to the parent patch in layer \(l+1\). The action interaction process is essentially a hierarchical tree search in FPN (with a virtual layer \(L\) as the root node) and we provide illustrative examples in Appendix C.3.

**Reward Design.** The action space of the RL agent involves two major types of movements in the hierarchical search, downward and upward. To facilitate each type, we define a unique reward function. For **downward movement** actions, we compute the reward based on the ranking of the patch selected by the movement action compared to all other patches located on the same layer in terms of the number of the positive anchors they contain. Specifically, we compute the quality measure estimate of each anchor by investigating a range of metrics: centerness , IoU, GIoU, and DIoU . It should be worth noting that all these metrics encode the supervised signal with a threshold to decide whether an anchor is positive or not. We follow the RetinaNet setting but use DIoU as our positive anchor criterion and conduct an ablation study to verify its superiority. After getting the positive anchors for each patch, we calculate the quality score \(g(_{t},_{t})\) for each patch in terms of the number of positive anchors on it as "ground truth" information. The details can be seen in Appendix C.1. In addition, we set up a penalty term with the downward movement in each time step representing the searching cost. Such a cost should be related to both training progress and the depth of search. For example, searching a bottom layer's patch in a later training phase when the model gets enough knowledge of the input should be considered costly. Given this insight and by combining the above two factors, we design our unique reward

\[r(_{t},_{t})=g(_{t},_{t})-}{N_{epoch}}P_{s_{t},a_{t}}^{l}\] (8)

where \(n_{epoch},N_{epoch}\) are the current and total training epoch, \(P_{s_{t},a_{t}}^{l}\) is the penalty term for each layer \(l\), and \(l\) is the layer index of next selected patch given \(_{t},_{t}\) in time step \(t\). For **upward movement**, the reward is simply set to 0, which means when the downward movement's benefits for exploration cannot cover the search cost, the model prefers to return back and search other patches in the same layer. In this way, we achieve the exploitation-exploration balance in the reward design, besides the evidential Q-learning.

### Theoretical Analysis

We establish the statistical guarantee for AIRS that integrates evidential Deep-Q learning with hierarchical search. Let \(Q^{*}\) be the optimal action-value function and \(Q^{_{k}}\) be the action-value function corresponding to the policy \(_{k}\).

[MISSING_PAGE_FAIL:6]

### Datasets

* _MS COCO_: It contains 91 categories. Following [38; 26], we use the COCO trainval35k split containing 115K images for training and minival split containing 5K images for testing.
* _PASCAL Visual Object Classes (VOC) 2012_: It contains 20 categories and is partitioned into three subsets: 5,717 images for training, 5,823 images for validation, and 10,991 images for testing.
* _Google Open Images V4_: It contains 9M (million) image with 600 object categories, where training set contains 1.74 M images, validation set contains 125K images, and testing contains 41K images. It is worth noting that images in this dataset are very diverse and often contain complex scenes with several objects _i.e.,_ on average 8.4 objects per image.
* _Challenging subset:_ From each dataset, we construct a subset (denoted as 'CH') to include most challenging images using the following criteria: (a) images where the ratio of large and medium objects (area \( 32^{2}\)) to small objects (area \(<32^{2}\)) ranging from 1 to 1/2 to ensure smaller objects coexist and embedd within large objects making detection task much more challenging, (b) images where multiple objects overlap with each other, and (c) images where multiple small objects are embedded into the bigger one. Appendix D.7 show examples of selected images.

### Experimental Settings

Evaluation Metrics.Following evaluation performed in the benchmark COCO dataset , we assess the performance using Average Precision (AP). Additionally, we separately report the AP performance for small, medium, and large objects named as AP\({}^{S}\), AP\({}^{M}\), AP\({}^{L}\) respectively. For the challenging subsets, we report the AP score named as AP\({}^{CH}\). It should be noted that small objects and constructed subsets correspond to the more challenging detection tasks and therefore we would expect a more significant performance gap compared with the baselines.

Experimental setup.For the FPN, we follow the same experiment setting as GFocal, which uses its pre-trained ResNet-50 as backbone, and applies a 3-layer FPN with patch size defined as a quarter of the area of the layer \(L-1\) (the top layer \(L\) in our case is a virtual layer as the root node in the tree structure), but instead use DIoU as positive anchor criterion. For the feature extractor, we use a three-layer Multi-Layer Perceptron (MLP) structure. Through grid based hyper-parameter search using a validation dataset, we set the total training epochs \(N_{epoch}=12\), action space \(D=5\), maximum time step \(T=60\), discount factor \(=0.9\), learning rate = \(0.001\), and \(=1\). We gradually shrink \(\) to balance exploitation and exploration. For the penalty term \(P^{l}_{s_{t},a_{t}}\) set up, we choose (0.3,0.6,0.9) for layers (P5, P4, P3) after hyper-parameter searching. For other baselines, we train them until convergence and test in the same data sets for fair comparison.

### Quantitative Study

Comparison baselines.In our quantitative study, we include baselines that are most relevant to our model, including representative or latest two-stage detectors: Faster R-CNN , Cascade R-CNN , Reppoints , TridentNet , DETR , Co-DETR , EVA , and DINO , as well as most recent one stage detectors: RetinaNet , FCOS , ATSS , SAPD , SpineNet  and GFocal . Faster R-CNN and Cascade R-CNN use an ROI pooling layer to extract candidate ROI regions first then regress from those regions, while Reppoints and TridentNet apply deformable convolution technique or scale-specific feature maps to handle scale and perspective variations in images. DETR leverages a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture to effectively remove the need for Non maximal suppression and anchor generations. Co-DETR further applies a novel collaborative hybrid assignments training scheme on top of it. DINO improves DETR on de-noising the anchor boxes for end-to-end training. All one-stage methods use FPN, but the difference resides in the training loss (GFocal), and positive anchor criterion choices (RetinaNet, FCOS, ATSS, SAPD). Instead, EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches for exploring the limits of visual representation at scale. For fair comparison, all baselines apply the same pre-trained backbone (_i.e.,_ ResNet-50) to extract image features. For YOLO series comparison, since they are different from other one-stage detectors based on FPN, we separately compare with them in Appendix D.1.

Comparison results.Table 1 shows the performance in terms of AP for multiple datasets compared to competitive baselines. As shown, our approach has achieved better detection performance in general. Comparing to those RPN based two-stage frameworks which suffer from a low recall given limited candidate object predictions, our approach leverages abundant positive anchors provided bythe underlying one-stage framework. Additionally, our approach can more effectively avoid false positive predictions benefiting from the learned RL masks that precisely detect positive anchors. This is clearly demonstrated through improved performance on small object detection (_i.e.,_ AP\({}^{S}\)) and the challenging subset (_i.e.,_ AP\({}^{CH}\)). As can be seen, the performance improvement compared to its base detector GFocal (without RL augmentation) is as high as around \(4\%\) in certain cases. It is noted that the performance advantage is not as prominent in the medium and large object detection as these are relatively easy cases and can be adequately handled by commonly used models. Our proposed technique is designed to focus on difficult images while remaining competitive on easier detection tasks.

**Results on different backbones.** We also test our model's performance on multiple latest backbones such as Swin-T , Efficientnet-b3  and ConvNeXt , and compare with the most competitive baselines of each category with the same backbone setting as shown in Table 2. The results show a highly consistent trend as in Table 1.

**Inference speed comparison.** Finally, we compare the parameter size as well as inference speed of our model with representative baselines. As the results shown in Table 3, the inference speed of AIRS does not bring extra detection burden compared to those latest baselines.

    &  &  &  &  \\   & & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP \\  Two-stage & Faster R-CNN  & 36.2 & 182 & 39.0 & 48.2 & 19.4 & 73.8 & 25.2 & 75.2 & 78.4 & 26.5 & 37.4 & 19.6 & 38.5 & 42.2 & 20.5 \\  & Cascade R-CNN  & 47.8 & 33.7 & 45.5 & 52.2 & 28.7 & 29.5 & 73.6 & 8.5 & 28.6 & 38.6 & 38.6 & 45.4 & 40.4 & 4.8 & 23.7 \\  & RepPoints  & 41.0 & 23.6 & 44.1 & 51.7 & 22.2 & 83.1 & 29.1 & 74.0 & 83.0 & 27.6 & 39.1 & 24.2 & 39.1 & 42.5 & 21.5 \\  & Trivalent  & 42.7 & 23.9 & 46.6 & 56.6 & 20.5 & 82.5 & 29.5 & 64.3 & 84.7 & 28.4 & 40.5 & 22.5 & 41.9 & 45.8 & 20.4 \\  & DETR  & 42.0 & 20.5 & 45.8 & 61.1 & 71.9 & 80.5 & 25.4 & 84.5 & 26.3 & 39.6 & 23.5 & 41.5 & 45.9 & 17.8 \\  & Co-DETR  & 42.5 & 20.8 & 46.2 & 61.5 & 17.9 & 80.5 & 25.4 & 62.4 & 84.9 & 26.5 & 39.2 & 41.8 & 46.3 & 18.3 \\  & EVA  & 46.7 & 25.8 & 48.2 & 61.9 & 28.8 & 84.7 & 31.5 & 72.4 & 86.5 & 28.7 & 44.1 & 25.8 & 46.5 & 50.9 & 26.7 \\  & DINO-Acelle  & 47.8 & 30.2 & 50.1 & 62.3 & 29.0 & 86.9 & 33.4 & 77.2 & 88.5 & 30.9 & 46.2 & 29.8 & 47.8 & 52.3 & 28.1 \\  & DINO-Scelle  & 47.9 & 30.0 & **50.4** & **62.5** & 29.0 & 87.1 & 33.3 & 77.4 & 88.6 & 31.2 & 46.9 & 39.9 & 47.7 & 52.4 & 28.2 \\  One-stage & ReinaNet  & 39.1 & 21.8 & 42.7 & 50.2 & 21.6 & 77.0 & 82.6 & 81.5 & 27.3 & 38.5 & 24.8 & 40.2 & 42.4 & 21.1 \\  & FCOS  & 41.5 & 24.4 & 44.8 & 51.6 & 23.5 & 83.3 & 31.4 & 62.4 & 85.3 & 30.5 & 40.3 & 26.1 & 41.8 & 45.4 & 23.2 \\  & AISS  & 43.6 & 21.6 & 47.0 & 53.6 & 23.8 & 84.2 & 32.6 & 73.0 & 86.9 & 31.3 & 42.2 & 26.9 & 42.5 & 46.8 & 24.0 \\  & SAPD  & 43.5 & 24.9 & 46.8 & 54.6 & 22.4 & 83.8 & 31.5 & 75.3 & 86.2 & 29.5 & 41.1 & 25.9 & 41.6 & 45.8 & 23.5 \\  & SpaneNet  & 41.5 & 23.3 & 45.0 & 58.0 & 21.2 & 82.6 & 29.3 & 71.3 & 85.7 & 24.4 & 40.2 & 25.8 & 41.2 & 45.3 & 21.6 \\  & Gfocal  & 45.0 & 27.5 & 48.5 & 54.5 & 25.4 & 86.5 & 35.0 & 78.0 & 90.5 & 32.6 & 45.8 & 29.5 & 46.5 & 51.4 & 26.3 \\  Our & AIRS & **48.3** & **32.1** & 48.5 & 54.3 & **29.4** & **88.7** & **37.3** & **79.0** & **91.5** & **35.6** & **47.5** & **31.5** & **48.1** & **53.1** & **29.0** \\   

Table 1: Detection performance comparison on all three datasets along with their challenging subsets

    &  &  &  &  \\   & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP \\  Two-stage & DINO/Swin-T  & 48.0 & 31.5 & **50.5** & **55.6** & 27.6 & 87.9 & 34.6 & 78.0 & 89.4 & 32.5 & 47.4 & 31.2 & 48.7 & 53.5 & 29.0 \\  & DINO/Swin/Cwin/D  & 47.8 & 31.1 & **50.3** & **55.4** & 27.3 & 87.5 & 34.2 & 77.8 & 89.1 & 32.3 & 47.1 & 31.0 & 48.4 & 53.1 & 28.7 \\  & DINO/ConvNeXt  & 48.1 & 31.7 & **50.4** & **55.5** & 27.7 & 88.2 & 34.7 & 78.1 & 89.6 & 32.8 & 47.5 & 31.6 & 48.7 & 53.2 & 29.2 \\  One-stage & GFocal [Swin-T] & 46.0 & 27.6 & 49.5 & 54.8 & 25.7 & 87.3 & 35.8 & 78.7 & 91.1 & 33.2 & 46.4 & 30.0 & 46.9 & 51.9 & 26.7 \\  & Gfocal [EfficientNet] & 45.8 & 27.5 & 49.1 & 54.6 & 26.5 & 87.0 & 35.4 & 78.2 & 90.7 & 32.9 & 46.2 & 46.8 & 46.6 & 51.5 & 26.5 \\  & Gfocal [ConvNeXt] & 46.2 & 27.7 & 49.8 & 53.0 & 25.8 & 87.0 & 36.0 & 78.9 & 91.4 & 33.5 & 46.6 & 30.3 & 47.1 & 52.2 & 27.0 \\  Ours & AIRS (Swin-T) & **48.9** & **32.8** & -49.1 & 54.9 & **29.8** & **89.8** & **38.4** & **79.8** & **92.8** & **36.5** & **48.5** & **32.3** & **49.0** & **83.9** & **39.4** \\  & AIRS (Gfocal) & **48.7** & **32.6** & 48.8 & 55.4 & **29.5** &

**Results on RL baselines.** We also compare AIRS with existing RL based object detectors [47; 4; 3; 8; 43] in Table 4. It is worth to note that these works either focus on active object localization which can only detect limited objects or leverage CNN and recurrent networks to detect multiple objects step by step that lacks the flexibility to conduct dense detection from complex background. In , ReinforceNet leverages a hierarchical DRL framework for visual object tracking, which predicts target object's movement locations in the next frame given the last frame's state information. Since the primary goal is different from ours, their policy network is for mode switch among four modes (search, stop, update, re-initialization) given the last state. In contrast, our policy network gives directional movement actions given the current state (_i.e._, patch location in the feature pyramid network) to support hierarchical search of objects. We ignore the specific \(AP^{S},AP^{M},AP^{L}\) metrics and only report the mAP performance averaged over all categories, which is also the most commonly used metric on the Pascal VOC dataset. The result demonstrates the superiority of AIRS in dense scenarios with a large performance gap comparing to existing RL baselines.

### Qualitative Analysis

Accurately selecting positive anchors via the generated RL masks has a strong impact on the final detection performance. For the more difficult images from the COCO and OpenImage V4 data sets, GFocal produces many false positive bounding boxes since it focuses on achieving a high recall on even small objects. In contrast, AIRS has precisely identified the true objects (high recall) while avoiding the unnecessary small bounding boxes (high precision). This phenomenon is supported by statistical counts in Figure 3 (a)-(b). This clearly justifies its effectiveness especially in those challenging images. Appendix D.7 shows more results in the even more challenging subsets.

### Ablation Study

We demonstrate the effectiveness of each proposed component through ablation study on MS COCO dataset. Specifically, we analyze the impact of different model design choices, such as positive anchor criterion choices and the epistemic uncertainty design in evidential Q-learning. As shown in Table 5, among all positive anchor criterion choices, DIoU is the most effective positive anchor criterion as it considers both overlapping area ratio and the centerness relativity between the prediction and target. Furthermore, without epistemic uncertainty, there is a significant drop in detection performance. This justifies the importance of exploring the unknown patches by leveraging epistemic uncertainty in our proposed framework. In particular, for the challenging and small objects, it is more critical to conduct a deep exploration to identify various types of objects. The deeper reason justifying the importance of epistemic uncertainty can be explained using Figure 3c, where we use

Figure 3: (a)-(b) Average number of detections per test image based on the bounding box area on MS COCO and OpenImages V4. (c) Ablative study on epistemic uncertainty to deep Q-evaluation.

    &  &  \\  IoU & Centress & GIoU & GIoU+Uncertainty & DIoU & DIoU+Uncertainty & AP & AP\({}^{S}\) & AP\({}^{IJ}\) & AP\({}^{L}\) & AP\({}^{CH}\) \\   \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 44.6 & 28.7 & 45.9 & 52.1 & 25.8 \\ \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 42.4 & 26.2 & 45.8 & 51.6 & 23.7 \\ \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 44.3 & 28.5 & 45.6 & 51.5 & 25.4 \\ \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 46.7 & 30.2 & 47.5 & 53.4 & 28.1 \\ \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 45.4 & 29.5 & 46.8 & 52.4 & 26.7 \\  \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & **47.6** & **31.0** & **48.5** & **54.3** & **28.9** \\   

Table 5: Ablation study on model design choicesone image's Q-learning curve in the search process as an example. The figure shows the Q-value (Q), evidential Q-value (Evi Q), epistemic uncertainty (EU) of the selected patch in each step, and the corresponding reward (Reward) as a supervision signal. We also include the Q-value and the corresponding reward in each step if we train the model without epistemic uncertainty, denoted as Q w/o EL (_i.e.,_ evidential learning) and Reward w/o EL. Without epistemic uncertainty, at the early phase of training, the RL agent tends to select the patches with a high immediate reward and therefore abandons patches with a low estimated Q-value (with low immediate reward). As many patches that require a deep exploration to find objects may be missed, the agent only selects those regions with a high Q-value (shown in brown color) in the current step. Due to the top-down search strategy, the RL agent may never search the skipped region again, including all patches in the lower layers. This results in a significant reward drop (shown in the purple color) in the later steps along with an early termination (in step 40), resulting into a low cumulative reward in the long run. However, AIRS chooses regions based on the evidential Q-value (green curve) and therefore patches with even a low estimated Q-value (black curve) but high epistemic uncertainty (blue curve) may still have a chance to be selected. In this way, AIRS can explore patches with objects requiring deep exploration resulting into high cumulative rewards shown in red.

### Discussion

**Performance of AIRS on large objects in MS COCO.** In this work, we aim to improve the detection performance by having a good balance between objects of different sizes and the \(AP\) metric is designed to assess the overall effectiveness in terms of detecting objects in all granularities. Compared to competitive baselines, AIRS is superior on all datasets. We observe that by placing more focus on smaller and more difficult objects, AIRS achieves lower performance on \(AP^{L}\) and \(AP^{M}\) in MS COCO. However, this is an expected behavior as MS COCO has most of the objects being very large and therefore, the cost of missing smaller objects in the existing two-stage detectors seem to be low. As such, many two-stage detectors have superior performance (see Table 1). In contrast, as our technique leverages a one-stage detector to better cover dense objects, it is relatively less effective to detect very large objects (which is evidenced by the lower performance by all one-stage detectors in Table 1). It is worth mentioning that in other datasets, AIRS outperforms all baselines even on the large objects. In the case of Pascal VOC 2012, it is relatively easier and does not contain very large objects. As such, one-stage detectors perform comparable or even better than the two-stage detectors. As for Open Image V4, despite being challenging, it contains a good amount of training samples with larger objects, which provides enough supervision for models to detect these large objects. As such, all single-detectors including our technique perform comparable or even better compared to two-stage detectors.

**Comparison with two stage detectors like RPN.** There are key differences between two stage detectors and AIRS. The former usually relies on a Region Proposal Network (RPN), which is less effective to capture all targeted objects especially in a dense scenario. This is because, RPN selects anchors from the candidate anchors provided by the RPN based on the confidence score resulting into missing many true positive object anchors with a low confidence. In contrast, FPN in AIRS is based on multi-scale feature representations. Thus, the number of selected anchors in all layers is far more than the ones proposed by the RPN, which avoids missing important object anchors. To tackle the many false positive anchors in the FPN based approaches, we propose a novel hierarchical search mechanism coupled with an effective exploration-exploitation strategy leveraging evidential Q-learning. As a result, AIRS effectively removes the false positive bounding boxes without removing the less confident true positive objects. This phenomenon is also demonstrated in Table 1, where two-stage detectors result in a lower performance compared to AIRS in dense object detection.

## 5 Conclusion

We propose a novel Adaptive Important Region Selection (referred to as AIRS) framework guided by evidential Q-learning built upon a uniquely designed reward function. AIRS encourages object search in a hierarchical, top-down fashion, where the RL agent moves down to a fine-grained level only when it is likely to contain an object of interest. In addition, to facilitate detection of unknown patches, evidential Q-learning leverages the epistemic uncertainty to guide the exploration process. Our proposed technique dynamically balances exploration-exploitation where in the early phase the priority is given to the highly uncertain patches and in the latter phase priority is dynamically shifted to the potentially positive patches. Both theoretical analysis and empirical results on challenging object detection datasets demonstrate the effectiveness of our proposed framework.