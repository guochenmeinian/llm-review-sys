# DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models

Shangqian Gao *

Florida State University

&Chi-Heng Lin

Samsung Research America

&Ting Hua

Samsung Research America

&Tang Zheng

Samsung Research America

&Yilin Shen

Samsung Research America

&Hongxia Jin

Samsung Research America

&Yen-Chang Hsu

Samsung Research America

Part of this project was completed at Samsung Research America. Correspondence to sgao@cs.fsu.edu

###### Abstract

Large Language Models (LLMs) have achieved remarkable success in various natural language processing tasks, including language modeling, understanding, and generation. However, the increased memory and computational costs associated with these models pose significant challenges for deployment on resource-limited devices. Structural pruning has emerged as a promising solution to reduce the costs of LLMs without requiring post-processing steps. Prior structural pruning methods either follow the dependence of structures at the cost of limiting flexibility, or introduce non-trivial additional parameters by incorporating different projection matrices. In this work, we propose a novel approach that relaxes the constraint imposed by regular structural pruning methods and eliminates the structural dependence along the embedding dimension. Our dimension-independent structural pruning method offers several benefits. Firstly, our method enables different blocks to utilize different subsets of the feature maps. Secondly, by removing structural dependence, we facilitate each block to possess varying widths along its input and output dimensions, thereby significantly enhancing the flexibility of structural pruning. We evaluate our method on various LLMs, including OPT, LLaMA, LLaMA-2, Phi-1.5, and Phi-2. Experimental results demonstrate that our approach outperforms other state-of-the-art methods, showing for the first time that structural pruning can achieve an accuracy similar to semi-structural pruning.

## 1 Introduction

Large Language Models (LLMs) have revolutionized the field of natural language processing by leveraging deep learning techniques to process and generate human-like text. Compared to smaller models, LLMs exhibit unique characteristics and demonstrate remarkable abilities in tackling a wide range of complex tasks . Despite their impressive capabilities, the vast number of parameters in LLMs often hinders their deployment on resource-constrained devices, such as mobile phones. Consequently, there is significant interest in reducing the computational and memory requirements of LLMs.

Existing compression techniques for large language models (LLMs) include weight sparsification , structural pruning , and quantization . In this work, we focus on structural pruning andaddress the limitations of previous methods in this category. Structural pruning  is a general-purpose compression solution that maintains LLM performance across various tasks, facilitates deployment on devices, and is computationally efficient. However, existing methods may restrict pruning flexibility or add significant overhead to the compressed model. For instance, LLM-Pruner  follows structural dependence during pruning, requiring different layers to use the same subset of feature maps, which limits pruning flexibility. SliceGPT  alleviates this issue by applying orthogonal projections for each layer but introduces a non-trivial number of additional parameters (e.g., **5% to 13% of the parameters** of the original model for LLaMA-2 7B). Our approach aims to overcome these drawbacks and offer a better performance-cost trade-off for structural pruning.

We aim to increase the flexibility of current structural pruning methods and consequently improve performance. Our method provides different sub-spaces or subsets of features to different layers, but unlike SliceGPT, it doesn't introduce additional parameters. To achieve this, we break the structural dependence of regular structural pruning methods, allowing different layers to have different subsets of features along the embedding dimension and an example is given in Fig. 1. After pruning, we employ index selection and index addition operations to sample subsets of features from the residual connection and add them back after the computation of each layer. Furthermore, our method introduces an additional level of flexibility by learning different widths for each layer. Our approach significantly improves the flexibility of structural pruning without adding additional parameters.

Extensive experimental results show that our method can outperform state-of-the-art structural pruning methods for LLMs while still maintaining low computational costs. Our method does not require recovery fine-tuning to obtain such performance. In addition, our method does not update the remained model weights during pruning which is a distinct departure from several other methods, such as SparseGPT  and LLM Surgeon . Our contributions are as follows:

* We break the structural dependence of regular structural pruning methods, significantly increasing the flexibility of structural pruning. This allows different layers to select their own subset of features from the embedding dimension. Importantly, our method achieves this without introducing additional parameters, unlike SliceGPT.
* We propose to learn the widths of each layer using gradient-based optimization methods. A hypernetwork generates the column or row selection matrices, while the width of each layer is controlled globally. This approach allows for fine-grained control over the pruning process and enhances the adaptability of our method to various models and tasks.
* Our method demonstrates superior performance compared to state-of-the-art structural pruning techniques for LLMs across a range of models, including OPT, LLaMA, LLaMA-2, Phi-1.5, and Phi-2. Notably, the resulting model from our method is a sub-network that exists within the original model, indicating the effectiveness of our method in discovering strong sub-networks.

## 2 Related Works

Magnitude-based pruning is the most straightforward approach to reduce model size, where weights with the smallest magnitude are pruned. _Han et al._ employ this strategy for pruning with \(L_{1}\) or \(L_{2}\) norm of weights. Filter pruning  extends this setting to structures of the model instead of performing weight-level sparsification. Although magnitude-based pruning methods are very efficient, they result in significant performance drops for LLMs, even for weight pruning . Another

Figure 1: We use an MLP layer as an example. **Left:** Regular pruning methods have to follow structural dependence thus their flexibility is limited. **Right:** Our dimension-independent structural pruning method breaks the structural dependence via index operations and thus largely improves the flexibility for pruning.

line of research, Optimal Brain Damage  and Optimal Brain Surgeon , utilize second-order information to remove connections. These methods require calculating the inverse of the Hessian matrix, which is computationally intensive for modern neural network architectures like Convolutional Neural Networks (CNNs) [22; 16], Transformers , or Large Language Models (LLMs) . To reduce the cost of computing the Hessian inverse matrix, Optimal Brain Surgeon can be applied in a layer-wise fashion [7; 8], making the computation tractable. However, further scaling up these methods for LLMs remains challenging.

Recent methods like SparseGPT  or GPTQ  aim to minimize the squared error before and after pruning or quantization of a given layer. In this setting, the Hessian inverse matrix becomes easy to compute, as it is simply the multiplication between the feature map and its transpose for a given layer. GPTQ and SparseGPT then quantize or sparsify model weights in a column-by-column manner, and the unpruned or unquantized weights are updated to compensate for the error of pruning and quantization. Wanda  further avoids computing the inverse of the Hessian matrix by only considering the diagonal of the Hessian matrix. While SparseGPT and Wanda achieve good results, unstructured sparsity is known to be harder to achieve actual speedup. They also applied their methods on semi-structured settings , but the performance becomes much worse.

Several researches [28; 19; 44; 13; 42; 12] apply learnable parameters for specific structures when pruning vision or language models. However, many of these methods cannot be scaled up to LLMs since they need to learn weights and structures together. In contrast, our method explores sub-networks within the optimization model without updating model weights. Additionally, our method mainly explores the regime of pruning without recovery fine-tuning, which is rarely presented in previous methods with learnable parameters on structures. Our method is also related to the unconstrained channel pruning for CNNs . However, our method explores this idea from the perspective of breaking structural dependence and scales it to much larger models than . Moreover, our method thoroughly explores the global allocation of parameters, where  fails to do.

Recently, several works have been proposed to reduce the size of LLMs. LLM-Pruner  aims to remove connected structures using importance calculated from Taylor expansions. SliceGPT  offers more flexibility than regular pruning by projecting the feature maps to different spaces but introduces extra parameters in the residuals. LLM Surgeon  periodically updates model weights and structures, resulting in a higher cost than LLM-Pruner and SliceGPT. Our proposed DISP-LLM breaks the structural dependence relied on by LLM-Pruner, without additional transformation matrices in the residual connections like SliceGPT. Furthermore, in contrast to LLM Surgeon, which requires extensive computational resources, our method is significantly more efficient.

## 3 Preliminary

### Notations

To better understand our paper, we first define some notations. We use \(d\) to denote the model dimension or embedding dimension of LLMs. \(^{b n d}\) is used to represent feature maps, and \(b\) is the mini-batch size, \(n\) is the number of tokens. \(^{d_{1} d_{2}}\) is the model weights of size \(d_{1} d_{2}\). Let \(\) denote a pseudo-index selection matrix of size \(d d\), which is a diagonal matrix filled with 0 or 1 and the positions of the ones indicate the selected index. We further use \(}\) of size \(d d_{}\) to

Figure 2: Our method, DISP-LLM, applies different selection matrices to the input and output dimension of the Attention layer and MLP layer (\(_{1}/_{2}\): Attention in/out; \(_{3}/_{4}/_{5}\): MLP in/middle/out). When pruning the model, we add “Index Selection” before Layer Norm and we replace addition with “Index Add.” \(}_{1}\), \(\), \(}_{5}\) are applied for pruning weight matrices.

represent the actual selection matrix by removing \(d-d_{}\) columns with all zeros from \(\). For any matrix \(\), \(()\) represents the number of nonzero entries of \(\).

### Revisit SliceGPT

The core idea of SliceGPT  is to achieve computational invariance within the transformer architecture. It demonstrates that orthogonal projections can be applied to the output of each block and subsequently undone in the next block. This transformation is computed using Principal Component Analysis (PCA), allowing the feature maps between blocks to be projected into their principal components. A significant advantage of this approach is that it projects the feature maps of different blocks into distinct spaces, thereby introducing an additional degree of freedom for compression. This flexibility is not captured by regular structural pruning methods like LLM-Pruner , which rely on structural dependence.

After slicing (pruning), the feature map and weight matrix of \(l\)th layer of SliceGPT become:

\[}_{l}=_{l}_{l}},\;}_{l}=}^{}_{l}^{}_{l}.\] (1)

where \(}\) is a \(d d_{}\) selection matrix, \(_{l}\) is the output of the \(l\)th block, and \(_{l}\) contains eigenvectors of \(_{l}\):

\[_{l}=_{i}_{l,i}^{}_{l,i}\]

and \(_{l,i}\) is the \(i\)-th column of \(_{l}\) (corresponding to the \(i\)th sequence in the calibration dataset). From Eq. 1, we can see that SliceGPT uses the same selection matrix \(}\) for all layers, but the feature map \(_{l}\) is firstly projected by \(_{l}\), and the pruning for different layers is along with different directions. One crucial drawback of SliceGPT also comes from the projection matrix \(_{l}\), since the residual connection must be multiplied by the linear transformation \(_{l}^{}_{l+1}\) (shown in Fig. 7**left** in the Appendix). These additional operations bring a non-trivial amount of additional parameters. For a model that has \(L\) blocks, with the model dimension \(d\) and the remaining percentage of parameters \(p\), it brings approximately \(Ld^{2}p^{2}\) additional parameters to the model (more than **10% of model parameters** in some cases, and more details are given in Fig 10 in the Appendix).

### Residual Connections Limit the Flexibility of Structural Pruning

SliceGPT offers significant flexibility, but achieving similar flexibility with regular structural pruning methods without adding extra parameters is challenging. This section explains the reasons behind this difficulty. To simplify our reasoning, we replace \(}\) with its pseudo selection matrix \(\).

Assume we follow the basic setting of dependence-based structural pruning but allow each layer the flexibility to have its own selection matrix, \(_{l}\), along the embedding dimension. Under this assumption, due to structural dependence, all layers will share the same width of \((_{0}_{1}_{L})\).

In order to prune different positions for different layers, we need to add a transformation matrix to align the width of layers \(l\) and \(l+1\). Intuitively, if we have \(_{l}\) and \(_{l+1}\), we can then insert \(_{l}^{}_{l+1}\) in the residual connection to align consecutive layers.

Figure 3: Comparison of the projection matrices for structural pruning. We use \(_{}\) and \(_{}\) in Fig. 1 as an example. **Left:** SliceGPT employs orthogonal projection matrices, and it has to insert the projection matrices into the residual connections. **Middle:** Regular structural pruning methods remove structures based on their dependence, requiring to use the unified selection matrix \(\) for all blocks, which limits flexibility. **Right:** Our method breaks the structural dependence, allowing the use of different selection matrices \(_{in}\) and \(_{out}\) for the embedding dimension, significantly improving the flexibility of pruning.

With this setup, we can use \(_{l}_{l}\) to select subsets of features for different layers, mimicking \(_{l}\) for SliceGPT. Although it seems promising, this formulation has issues with layer widths, as detailed in Proposition 1.

**Proposition 1** (Decreasing feature dimensions for deeper layers).: _Let the pseudo-selection matrices in layers \(l\) and \(l+1\) be \(_{l}\) and \(_{l+1}\), respectively. The number of nonzero entries in the residual adapter satisfies_

\[(_{l}^{}_{l+1})\{(_{l}),(_{l+1})\}.\]

_For compression strategies that remove dependent structures for layer \(l+1\) following \(_{l}^{}_{l+1}\), this implies that the dimension in layer \(l+1\) is less than or equal to that in layer \(l\), with equality holding when the feature indices selected in layer \(l+1\) are contained within those in layer \(l\) or vice versa._

**Remark.** The proof of Proposition. 1 is straightforward and it is given in the Appendix A.1. From Proposition 1, we observe that if we naively apply \(_{l}\) for different layers, the model width will progressively decrease as we go deeper into the network. It also fails to provide different sets of features for different layers; instead, it merely passes a subset of features from the previous layer to the next. To avoid this restriction, all blocks must share the same width and the same pruned columns or rows. And we then fall back to the regime of previous structural pruning methods such as LLM-Pruner , Shared LLaMA , etc.

Proposition 1 highlights two significant obstacles. First, dependence-based structural pruning methods result in a uniform width along the embedding dimension. Second, inserting selection matrices in the residual connections causes the embedding dimension to decrease with depth. These challenges are unavoidable due to the residual connections linking structures across layers. To enhance flexibility along the embedding dimension, bypassing the residual connections is crucial.

## 4 Dimension-Independent Large Language Model

### Break the Structural dependence

Section 3.3 demonstrates that the residual connection is the primary barrier preventing pruning methods from achieving better flexibility. To avoid modifying the residual connection, we relocate the selection matrices inside the residual connection. This approach allows us to successfully create different subsets from the feature maps for different layers.

Based on this idea, we propose a solution that involves pruning different positions in consecutive blocks and selecting or merging feature maps from or back to the residual connection. This approach breaks the structural dependence inherent in previous pruning methods. Formally, given a transformer block, we apply the following operations:

\[() =(_{1}^{}_{q}, _{1}^{}_{k},_{1}^{} _{v})_{o}_{2},\] (2) \[() =((_{3}^{}_{1}_ {4})(_{3}^{}_{2}_{4}))_{4}{}^{}_{3}_{5},\] (3)

where \(_{1},,_{5}\) are pseudo selection matrices of size \(d d\), and \(\) denotes element-wise multiplication. Eq. 3 gives an example operation for gated MLP modules used in LLaMA . For Phi models  or OPT , the MLP operation is defined as \(()=(_{3}^{}_{1} _{4})_{4}{}^{}_{3}_{5}\). Fig 2 illustrates how to insert these selection matrices into a transformer block.

Given the operations defined in Eq. 2 and Eq. 3, we successfully remove the constraint in Proposition 1. The input and output of both the Attention layer and the MLP layer can be selected differently from the original feature maps for different layers, mimicking the function of \(_{i}\) in SliceGPT. Additionally, our method eliminates the need for extra parameters in \(_{i}^{}_{l+1}\) as it does not alter the residual connection. We also enhance flexibility by pruning the middle dimension of the MLP layer. Additionally, this flexibility can be further improved by allowing the query, key, and value weight matrices to use different selection matrices. Our current form is kept for two reasons: (1) SliceGPT uses one \(_{i}\) per layer, and we followed this design for a fair comparison, and (2) adding separate selection matrices would increase indexing operations, potentially slowing down the inference. Fig 3 further compares the projection matrices for SliceGPT, regular structural pruning, and the proposed method.

Once we have the final selection matrices \(_{1},,_{5}\), the pruned model will use a combination of index selection and index addition for inference as shown in Algorithm 1, where \(_{i}\) is a set containing all indices equal to one in the diagonal of \(_{i}\):

\[_{i}=\{j\;_{i[j]}=1\},\;_{i}= (_{i}).\]

The same color is used to mark the index set \(_{i}\) and its corresponding selection matrix \(}_{i}\). \((,,)\) adds matrices \(\) and \(\) along the last dimension on selected positions from \(\), then returns \(\) after index addition. With index selection and addition, the block dimension can be freely changed. Index selection and addition introduce some overhead, but as demonstrated in the experiment section, we still observe improvements in throughput.

### Learning the Width for Dimension-Independent LLMs

Building on the dimension-independent setting introduced in Section 4.1, our approach offers much greater flexibility in selecting sub-networks from the original dense model compared to the constrained settings in LLM-Pruner . The next challenge is determining the width of each layer. Given the large search space of our dimension-independent structural pruning and the computationally intensive nature of LLMs, it is impractical to use reinforcement learning  or evolutionary search-based algorithms . Therefore, we adopt gradient-based methods to address this challenge. Given the diagonal vector \(_{i}\{0,1\}^{d}\) from \(_{i}\), the Straight-Through (ST) gradient estimator  is used to estimate the gradients with respect to learnable continuous latent parameters. More specifically, we use the recently proposed gradient estimator ReinMax  to estimate the gradients through the binary operation. A detailed explanation of ReinMax for the binary case is provided in Appendix A.2.

Given the large search space of our method, we find that only using element-wise learnable parameters is insufficient. To address this issue, a hypernetwork is introduced to generate latent parameters for ReinMax, as detailed below:

\[=(()),\] (4)

where \(\) represents the parameters of the hypernetwork and \(\) contains \(_{i}\) from all blocks. The hypernetwork is composed of GRU  and fully connected layers, where the GRU captures block-wise relationships and the fully connected layers capture relationships between different dimensions. With the hypernetwork and ReinMax, we can effectively learn the width of each block. The details of the hypernetwork are provided in Appendix A.3.

### Dimension-Independent Structural Pruning as an Optimization Problem

With the methods described above, we can formulate dimension-independent structural pruning as an optimization problem, with regularization to control the number of remaining parameters. We insert \(\) back into \(\) as defined in section 4.1 for forward and backward calculations. The overall objective function is listed below:

\[_{}\;(;,)+ (T(),pT_{}),\] (5) \[(T(),pT_{})=((T(),pT_{})/(T(),pT_{})),\] (6)

where \(\) is the language modeling loss function of next word prediction, \(\) represents the input tokens, \(\) is the collection of model weights, \(\) is defined in Eq. 4, and \(\) is a parameter regularization loss function defined in Eq. 6. Here, \(T()\) denotes the number of parameters controlled by the current structure \(\), \(T_{}\) is the total number of parameters of the model, and \(p(0,1]\) is a user-defined

[MISSING_PAGE_FAIL:7]

performance gap is even larger when compared to SliceGPT. The advantage is particularly clear in better-trained models like LLaMA-2 7B and 13B. For instance, our method surpasses LLM Surgeon by margins of 5.54 and 2.22 when pruning 50% of parameters of LLaMA-2 7B and 13B, respectively. Against K-OBD, our performance advantage extends to 36.80 and 9.49 under the same setting. For consistency, we let the pruning ratio of SliceGPT equal the slicing ratio. However, the actual pruning ratio for SliceGPT is much lower than the slicing ratio. More details are given in Appendix A.5.

In Table 2, we report the perplexity of pruned LLaMA and LLaMA-2 models and we compare our method with semi-structure pruning methods. From the table, we can see that our method outperforms both SparseGPT and Wanda on LLaMA 13B and LLaMA-2 7B/13B models. Our method performs on par with SparseGPT and Wanda with the LLaMA 7B model, and our DISP-LLM is a little bit worse than SparseGPT and is similar to Wanda. We are the first to show that **structural pruning methods can have a better or similar performance than semi-structural pruning methods**.

### Zero-shot Performance

In Tab. 3, we present the zero-shot performance of the pruned model. For the LLaMA 7B model, we compare our method against LLM-Pruner with and without recovery fine-tuning. Our method consistently outperforms LLM-Pruner without fine-tuning, and the gap ranges from 2.63 to 8.88 across different pruning rates for average task performance. After fine-tuning, the performance of LLM-Pruner is largely boosted, however, our method is still able to outperform it demonstrating the existence of strong sub-networks within the original model. For the LLaMA-2 7B model, we compare our method against SliceGPT, K-OBD, and LLM Surgeon. With weight updates, LLM

    &  &  &  &  &  &  &  &  \\    & & & acc & acc-norm & acc-norm & acc-norm & acc-norm & & \\ 
0\% & LLMaTA 7B & - & 69.85 & 76.21 & 72.81 & 44.71 & 79.16 & 68.55 \\   & LLM-Pruner  & ✗ & 61.33 & 65.34 & 59.18 & 37.12 & 75.57 & 59.71 \\  & +fineuning & ✓ & 65.11 & 68.11 & 63.43 & **37.88** & 76.44 & 62.19 \\  & DISP-LLM Ours (Ours) & ✗ & **66.54** & **68.75** & 59.60 & 35.24 & 74.97 & 61.02 \\  & DISP-LLM Apaca (Ours) & ✗ & 64.72 & 68.39 & **64.81** & 37.12 & **76.66** & **62.34** \\   & LLM-Pruner  & ✗ & 53.20 & 35.64 & 35.50 & 27.22 & 59.63 & 41.84 \\  & +fineuning & ✓ & 55.09 & 47.56 & 46.46 & 28.24 & **68.82** & 49.23 \\  & DISP-LLM Ours (Ours) & ✗ & **58.41** & 47.71 & 44.40 & 28.50 & 64.09 & 48.62 \\  & DISP-LLM Alexa (Ours) & ✗ & 56.91 & **48.76** & **48.91** & **31.57** & 67.46 & **50.72** \\  
0\% & LLMaA-2 7B & - & 69.14 & 75.99 & 74.58 & 46.15 & 79.11 & 68.99 \\   & SliceGPT & ✗ & 61.33 & 49.62 & 51.77 & 31.23 & 63.55 & 51.50 \\  & K-OBD  & ✓ & 56.83 & 53.07 & 51.05 & 33.11 & 71.82 & 53.18 \\  & LLM Surgeon  & ✓ & 61.09 & 60.72 & **63.09** & 36.69 & 73.56 & 59.03 \\  & DISP-LLM Ours (Ours) & ✗ & 62.27 & **63.43** & 59.81 & 33.19 & 71.82 & 58.10 \\  & DISP-LLM Alexa (Ours) & ✗ & **63.93** & 62.87 & 60.10 & **37.03** & **73.72** & **59.53** \\   & K-OBD  & ✓ & 53.04 & 36.84 & 36.11 & 26.71 & 60.66 & 42.67 \\  & LLM Surgeon  & ✓ & 52.57 & 40.29 & 44.91 & 26.28 & 64.36 & 45.68 \\  & DISP-LLM (Ours) & ✗ & 54.54 & 46.33 & 43.06 & 25.85 & 63.93 & 46.72 \\  & DISP-LLM Alexa (Ours) & ✗ & **56.20** & **49.35** & **51.14** & **30.20** & **68.34** & **51.05** \\  
0\% & Ph-1.5 & - & 72.77 & 62.58 & 73.11 & 48.04 & 75.63 & 66.43 \\ 
30\% & SliceGPT  & ✗ & **64.96** & 42.54 & 53.66 & 31.91 & 65.45 & 51.52 \\  & DISP-LLM Ours (Ours) & ✗ & 61.48 & **47.97** & **57.66** & **33.01** & **71.08** & **54.24** \\  
0\% & Ph-2 & - & 75.61 & 73.86 & 78.24 & 54.01 & 79.11 & 72.17 \\ 
30\% & SliceGPT  & ✗ & 63.14 & 47.56 & 53.03 & 30.29 & 65.94 & 51.99 \\  & DISP-LLM Ours (Ours) & ✗ & **65.19** & **54.43** & **63.59** & **38.48** & **73.34** & **59.00** \\  

Table 3: Zero-shot performance of the compressed LLaMA 7B, LLaMA-2 7B and Phi models. The structure of _DISP-LLM_ is based on the WikiText dataset, and the structure of _DISP-LLM A1paca_ is based on the Alpaca dataset.

Figure 4: The pruned model architecture along the embedding dimension (model dimension) for the LLaMA-2 7B model when the pruning ratio equals 50%.

Surgeon performs well with a lower pruning ratio like 30%. At this pruning ratio, our method performs similarly to LLM Surgeon, and our method outperforms other comparison baselines. When increasing the pruning ratio to 50%, the advantage of our method becomes obvious, and the gap between our method and LLM Surgeon is 5.37 for average task performance. We further compare our method with SliceGPT on Phi-1.5 and Phi-2, and our method consistently achieves better performance.

### Analysis

**Ablation Study.** We visualize the results of ablation studies in Figs. 4(a), 4(b), 4(c), 4(d) with Phi-1.5 model. The setting _"DISP-LLM w/o HN"_ refers to using element-wise gates for learning sub-networks. The setting _"Constrained LLM w HN"_ refers to pruning the embedding dimension following the structural dependence like LLM-Pruner. From Figs. 4(a), 4(b), we can see that using the hypernetwork largely accelerates the learning process for DISP-LLM, which is also verified in Figs. 4(c), 4(d). From Figs. 4(c), 4(d), we also see that our DISP-LLM always outperforms constrained structural pruning, demonstrating the value of added flexibility by breaking the dependence. To further study the impact of the HyperNetwork architecture, we provide more results in Tab. 4. _"w/o HN"_ is equivalent to _"DISP-LLM w/o HN"_. The setting _"w/o Bi-GRU"_ simply removes GRU and adds a fixed input (initialized in the same way as see Appendix A.3 for more details) for each linear layer. These results indicate that both GRU and linear layers within the HyperNetwork affect the final performance. One explanation is that linear layers connect different dimensions of the model, accelerating learning, while GRU layers capture inter-layer relationships, further reducing the difficulty of learning sub-network structures. Therefore, both GRU and linear layers are essential to the HyperNetwork.

**Different Pruning Ratios, Costs and Throughput.** In Figs. 4(e), 4(f), we show the language modeling loss \(\) and regularization loss \(\) in Obj 5 given different pruning ratios \(p\) with Phi-1.5 model. We can see that our method consistently minimizes the language modeling loss across different \(p\). In addition,

Figure 5: The training dynamics when learning the hypernetwork are shown in Figs. 4(a), 4(b), 4(e), 4(f). The results of different settings are in Figs. 4(c), 4(d), throughput is in Fig. 4(g), and cost is in Fig. 4(h).

Figure 6: Model width after pruning for the LLaMA-2 7B model when the pruning ratio equals 50%.

our method quickly pushes the regularization loss \(\) to near 0 values within 200 iterations. In Fig. 5g, the pruned model from LLaMA-13B improves the throughput of the dense model by \(1.08\) to \(1.50\). In Fig. 5h, we compare the costs of our method against LLM Surgeon. Our method is \(\) and \(\) cheaper compared to LLM Surgeon with LLaMA-2 7B and LLaMA-2 13B models.

**Every Embedding Dimension is Important.** In Fig. 4, we visualize the pruning decisions along the embedding dimension and depth for the LLaMA-2 7B model, we can see that all embedding dimensions have been used across different layers. This becomes more obvious in the second right figure of Fig. 4, where we sum all pruning decisions along the depth dimension, and we can see that every embedding dimension is kept around \(80\%\) given all layers. We further visualize the model width after pruning for the LLaMA-2 7B model in Fig. 6, where we can see that several layers are more severely pruned than other layers.

## 6 Conclusion

In this paper, we proposed dimension-independent structural pruning for Large Language Models. By breaking the structural dependence imposed by previous compression methods, our method creates sub-networks with a lot more flexibility than regular structural pruning methods and also avoids the overhead brought by SliceGPT. The flexibility of our method is reflected in two perspectives. Firstly, our method provides different subsets of the feature maps for different layers. Secondly, our method freely selects the width for each layer without considering architecture dependence. With dramatically increased flexibility, our method outperforms other structural pruning and semi-structural pruning methods given similar pruning ratios. The novel design of the unconstrained pruning space along with strong empirical performance opens new possibilities for structural pruning for LLMs.

   &  \\   & 0\% & 10\% & 20\% & 30\% & 40\% & 50\% \\  w/o HN & & 20.37 & 22.30 & 28.66 & 34.33 & 47.29 \\ w/o Bi-GRU & 21.82 & 19.90 & 21.65 & 26.11 & 30.88 & 37.43 \\ Full HyperNetwork & & 18.75 & 20.23 & 22.81 & 25.49 & 32.89 \\  

Table 4: The impact of the Hypernetwork architecture on the Phi-1.5 model. Performance is measured by PPL (perplexity).