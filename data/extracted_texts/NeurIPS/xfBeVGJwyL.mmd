# Physics-Informed Bayesian Optimization of

Variational Quantum Circuits

Kim A. Nicoli\({}^{1,2,3}\) Christopher J. Anders\({}^{3,4}\)1 Lena Funcke\({}^{1,2}\) Tobias Hartung\({}^{5,6}\)

**Karl Jansen\({}^{7}\) Stefan Kuhn\({}^{7}\) Klaus-Robert Muller\({}^{3,4,8,9}\) Paolo Stornati\({}^{10}\) Pan Kessel\({}^{11}\) Shinichi Nakajima\({}^{3,4,12}\)**

\({}^{1}\)Transdisciplinary Research Area (TRA) Matter, University of Bonn, Germany

\({}^{2}\)Helmholtz Institute for Radiation and Nuclear Physics (HISKP)

\({}^{3}\)Berlin Institute for the Foundations of Learning and Data (BIFOLD)

\({}^{4}\) Technische Universitat Berlin, Germany, \({}^{5}\) Northeastern University London, UK

\({}^{6}\) Khoury College of Computer Sciences, Northeastern University, USA

\({}^{7}\) CQTA, Deutsches Elektronen-Synchrotron (DESY), Zeuthen, Germany

\({}^{8}\) Department of Artificial Intelligence, Korea University, Korea

\({}^{9}\)Max Planck Institut fur Informatik, Saarbrucken, Germany

\({}^{10}\)Institute of Photonic Sciences, The Barcelona Institute of Science and Technology (ICFO)

\({}^{11}\)Prescient Design, gRED, Roche, Switzerland, \({}^{12}\)RIKEN Center for AIP, Japan

Correspondence to knicoli@uni-bonn.de and {anders,nakajima}@tu-berlin.de

###### Abstract

In this paper, we propose a novel and powerful method to harness Bayesian optimization for Variational Quantum Eigensolvers (VQEs)--a hybrid quantum-classical protocol used to approximate the ground state of a quantum Hamiltonian. Specifically, we derive a _VQE-kernel_ which incorporates important prior information about quantum circuits: the kernel feature map of the VQE-kernel exactly matches the known functional form of the VQE's objective function and thereby significantly reduces the posterior uncertainty. Moreover, we propose a novel acquisition function for Bayesian optimization called _Expected Maximum Improvement over Confident Regions_ (EMICoRe) which can actively exploit the inductive bias of the VQE-kernel by treating regions with low predictive uncertainty as indirectly "observed". As a result, observations at as few as three points in the search domain are sufficient to determine the complete objective function along an entire one-dimensional subspace of the optimization landscape. Our numerical experiments demonstrate that our approach improves over state-of-the-art baselines.

## 1 Introduction

Quantum computing raises the exciting future prospect to efficiently tackle currently intractable problems in quantum chemistry , many-body physics , combinatorial optimization , machine learning , and beyond. Rapid progress over the last years has led to the development of the first noisy intermediate-scale quantum (NISQ) devices ; quantum hardware with several hundreds of qubits that can be harnessed to outperform classical computers on specific tasks (see, e.g., Ref. ). However, these tasks have been specifically designed to be hard on classical computers and easy on quantum computers, while being of no practical use. As we have entered the NISQ era, one of the grand challenges of quantum computing lies in the development of algorithms for NISQ devices that may exhibit a quantum advantage on a task of practical relevance.

One promising approach toward using NISQ devices is hybrid quantum-classical algorithms, such as VQEs [7; 8], which can be used to compute ground states of quantum Hamiltonians. VQEs can be seen as the quantum counterpart of neural networks: while classical neural networks model functions by parametric layers, the parametric quantum circuits used in VQEs represent variational wave functions by parametric quantum gates acting on qubits. During training, we aim to find a suitable choice for the variational parameters of the wave function such that the quantum mechanical expectation value of the Hamiltonian is minimized. From the optimization perspective, the quantum mechanical nature of the energy measurement is of little relevance. The training of VQEs can thus be regarded as a specific, albeit particularly challenging, noisy black-box optimization problem. Namely, we solve

\[_{[0,2)^{D}}f^{*}(),\] (1)

where \(f^{*}()\) is estimated from costly noisy observation, \(y=f^{*}()+\), on the quantum device.

Efficiently exploring the energy landscape of the VQE is crucial for successful optimization. This requires leveraging strong prior knowledge about the VQE objective function \(f^{*}()\). For instance, gradient-based optimization methods commonly utilize the parameter shift rule, which takes advantage of the specific structure of variational quantum circuits to compute gradients efficiently [9; 10]. Another approach is the _Nakanishi-Fuji-Todo_ (NFT) method , a version of sequential minimal optimization (SMO) , which solves sub-problems sequentially. Nakanishi et al.  derived the explicit functional form of the VQE objective, enabling coordinate-wise global optimization with only two observations per iteration.

Given the highly non-trivial nature of classical optimization in VQE, machine learning represents an appealing tool to leverage the informative underlying physics toward a more effective search for a global optimum. Bayesian Optimization (BO) [13; 14] has been recently applied to VQE . These methods have the distinct advantage that they can take the inherently noisy nature of the NISQ circuits into account. Unfortunately, these methods are yet to be competitive, especially in the high dimensional regime, because of their poor scalability and overemphasized-exploration behavior .

To overcome this limitation, this paper introduces a novel and powerful method for BO of VQEs, capitalizing on VQE-specific properties as physics-informed prior knowledge. To this end, we propose a _VQE-kernel_, designed with feature vectors that precisely align with the basis functions of the VQE objective. This incorporation of a strong inductive bias maximizes the statistical efficiency of Gaussian process (GP) regression and guarantees that GP posterior samples reside within the VQE function space. To further harness this powerful inductive bias, we present a novel acquisition function named _Expected Maximum Improvement over Confident Regions_ (EMICoRe). EMICoRe operates by initially predicting the posterior variance and treating the points with low posterior variances as "observed" points. Subsequently, these indirectly observed points, alongside the directly observed ones, form the Confident Regions (CoRe) on which safe optimization of the GP mean is conducted to determine the current optimum. EMICoRe evaluates the expected maximum improvement of the best points in CoRe before and after the candidate points are observed. By utilizing EMICoRe, our approach combines the strengths of NFT  and BO, complementing each other in a synergistic manner. BO enhances the efficiency of NFT by replacing sub-optimal deterministic choices of next observation points, while the NFT procedure significantly constrains the exploration space of BO, leading to remarkable improvements in scalability.

Our numerical experiments demonstrate the performance gains of using the VQE kernel, and significant improvement of our NFT-with-EMICoRe approach in VQE problems with different Hamiltonians, different numbers of qubits, etc. As an additional contribution, we prove that two known important properties of VQE, the parameter shift rule  and the sinusoidal function-form , are equivalent, implying that they are not two different properties but two expressions of a single property.

Related WorkNumerous optimization methods for the VQE protocol have been proposed: gradient-based methods often rely on the parameter shift rule [9; 10], while NFT harnesses the specific functional form of the VQE objective  to establish SMO. BO has also been applied for VQE minimization . Therein, it was shown that combining periodic kernel  and noisy expected improvement acquisition function  improves the performance of BO over the standard RBF kernel, and can make BO comparable to the state-of-the-art methods in the regime of small qubits and high observation noise.

BO is a versatile tool for black-box optimization in many applications [19; 20; 21; 22] including hyperparameter tuning of deep neural networks . Most work on BO uses GP regression, and computes an acquisition function whose maximizer is suggested as the next observation point. Many acquisition functions, including lower confidence bound [24; 25], probability of improvement , Expected Improvement (EI) , entropy search [28; 29], and knowledge gradient  have been proposed. The most common acquisition function is EI, of which many generalizations exist: noisy EI (NEI) [18; 31] for dealing with observation noise, parallel EI  for batch sample suggestions, and EI per cost  for cost sensitivity. Our EMICoRe acquisition function is a generalization of noisy EI and parallel EI with the key novelty of introducing CoRe, which defines the indirectly observed points. Note the difference between the trust region  and CoRe: based on the predictive uncertainty, the former restricts the region to be explored, while the latter expands the observed points.

## 2 Preliminaries

### Bayesian Optimization

Let \(f^{*}():\) be an unknown (black-box) objective function to be minimized. BO [13; 14] approximates the objective with a surrogate function \(f() f^{*}()\), and suggests promising points to be observed in each step, such that the objective is likely to be improved considerably. A commonly used surrogate is the Gaussian process (GP) regression model  with one-dimensional Gaussian likelihood and GP prior,

\[p(y|,f())=_{1}(y;f(),^{2}), p(f( ))=(f();(),k(,)),\] (2)

which is trained on the (possibly) noisy observations \(y=f^{*}()+\) made in the previous iterations. Here, \(^{2}\) is the variance of observation noise \(\), and \(()\) and \(k(,)\) are the prior mean and the kernel (covariance) functions, respectively. Throughout the paper, we set the prior mean function to be zero, i.e., \(()=0,\). Let \(\{,\}\) be \(N\) training samples, where \(=(_{1},,_{N})^{N}\) and \(=(y_{1},,y_{N})^{}^{N}\). Since the GP prior is conjugate for the Gaussian likelihood8, the posterior of the GP regression model (2) is also a GP, i.e., \(p(f()|,)=(f();_{}(),s_{ }(,))\), and thus, for arbitrary \(M\) test inputs \(^{}=(^{}_{1},,^{}_{M}) ^{M}\), the posterior of the function values \(^{}=(f(^{}_{1}),,f(^{}_{M}))^{} ^{M}\) is the \(M\)-dimensional Gaussian with its mean and covariance

Figure 1: Illustration of EMICoRe (ours) and NFT  (baseline) procedures. In each step of NFT (bottom row), a) given the current best point \(}^{t}\) and the direction \(d^{t}\) to be explored, b) next observation points are chosen in a deterministic fashion. Then, c) the minimum along the line is found by the sinusoidal function fitting to the three points, and d) the next optimization step for the new direction \(d^{t+1}\) starts from the found minimum \(}^{t+1}=}^{t}_{}\). The EMICoRe procedure (top row) uses GP regression and BO in the steps highlighted by the light blue box: b) the next observation points are chosen by BO with the EMICoRe acquisition function based on the GP trained on the previous observations, and c) minimizing the predictive mean function of the updated GP with the new observations gives the best point \(}^{t}_{}\).

analytically given as

\[p(^{}|,)=_{M}(^{}; ^{}_{},^{}_{}),\] (3) \[^{}_{}=^{}(+^ {2}_{N})^{-1},^{}_{}=^{ }-^{}(+^{2}_{N})^{-1}^{}.\] (4)

Here, \(=k(,)^{N N},^{}=k(, {X}^{})^{N M}\), and \(^{}=k(^{},^{})^{M  M}\) are the train, train-test, and test kernel matrices, respectively, where \(k(,^{})\) denotes the kernel matrix evaluated at each column of \(\) and \(^{}\) such that \((k(,^{}))_{n,m}=k(_{n},_{m})\). Moreover, \(_{N}^{N N}\) denotes the identity matrix, and the subscript \(\) of posterior means and covariances specifies the input points on which the GP was trained (see Appendix B for details of GP regression).12

In the general BO framework , \(M 1\) input points to be observed next are chosen by (approximately) solving the following maximization problem in each iteration: \(_{^{}}a_{^{t-1}}(^{})\), where \(^{t}\) denotes the training input points observed until the \(t\)-th iteration, and \(a_{}()\) is an _acquisition function_ computed based on the GP trained on the observations \(\) at \(\). The acquisition function evaluates the _promising-ness_ of the new input points \(^{}\), and should therefore give a high value if observing \(^{}\) likely improves the current best score considerably. Common choices for the acquisition function are EI [27; 34], \(a_{}^{}(^{})=(0,-f ^{})_{p(f^{}|,)},\) and its variants. Here, \(\) denotes the current best observation, i.e., \(=_{n\{1,,N\}}f(_{n})\), and \(_{p}\) denotes the expectation value with respect to the distribution \(\). EI covers the case where the observation noise is negligible and only one sample is chosen in each iteration, i.e., \(^{2} 1,M=1\), and its analytic solution makes EI handy (see Appendix B). In the general case where \(^{2}>0,M 1\), generalizations of EI should be considered, such as NEI [18; 31],

\[a_{}^{}(^{})=(0,()- (^{}))_{p(f,^{}|,)},\] (5)

which appropriately takes into account the correlation between observations. NEI is estimated by quasi Monte Carlo sampling, and its maximization is approximately performed by sequentially adding a point to \(^{}\) until \(M\) points are collected.

### Variational Quantum Eigensolver (VQE)

The VQE [7; 8] is a hybrid quantum-classical computing protocol for estimating the ground-state energy of a given quantum Hamiltonian for a \(Q\)-qubit system. The quantum computer is used to prepare a parametric quantum state \(|_{}\), which depends on \(D\) angular parameters \(=[0,2)^{D}\). This trial state \(|_{}\) is generated by applying \(D^{}( D)\)_quantum gate operations_, \(G()=G_{D^{}} G_{1}\), to an initial quantum state \(|_{0}\), i.e., \(|_{}=G()|_{0}\). All gates \(\{G_{d^{}}\}_{d^{}=1}^{D^{}}\) are unitary, and we assume in this paper that \(x_{d}\) parametrizes only a single gate \(G_{d^{}(d)}(x_{d})\), where \(d^{}(d)\) specifies the gate parametrized by \(x_{d}\). Thus, \(D\) of the \(D^{}\) gates are parametrized by each entry of \(\)_exclusively_.13 We consider parametric gates of the form \(_{d^{}}(x)=U_{d^{}}(x)=(-ixP_{d^{}}/2)\), where \(P_{d^{}}\) is an arbitrary sequence of the Pauli operators \(\{_{q}^{X},_{q}^{Y},_{q}^{Z}\}_{q=1}^{Q}\) acting on each qubit at most once. This form covers not only the single-qubit gates such as \(R_{X}(x)=(-i_{q}^{X})\), but also the entangling gates, such as \(R_{XX}(x)=(-ix_{q_{1}}^{X}_{q_{2}}^{X})\) and \(R_{ZZ}(x)=(-ix_{q_{1}}^{Z}_{q_{2}}^{Z})\) for \(q_{1} q_{2}\), which are commonly realized in trapped-ion quantum hardwares [35; 36].

The quantum computer evaluates the energy of the resulting quantum state \(|_{}\) by observing

\[y=f^{*}()+, f^{*}()= _{}|H|_{}=_{0}|G()^{}HG()|_{0}\] (6)

and \(\) denotes the Hermitian conjugate. The observation noise \(\) in our numerical experiments will only incorporate _shot noise_, and we will not consider the hardware-dependent errors induced by imperfect qubits, gates, and measurements. For each observation, multiple readout shots \(N_{}\) are acquired to suppress the variance \(^{*2}(N_{})\) of the noise \(\). Since the observation \(y\) is the sum of many random variables, it approximately follows a Gaussian distribution, according to the central limit theorem. The Gaussian likelihood in the GP regression model (2) therefore approximates the observation \(y\) well if \(f() f^{*}()\) and \(^{2}^{*2}(N_{})\). With the quantum computer that provides noisy estimates of \(f^{*}()\), a classical computer solves the minimization problem (1) and finds the minimizer \(}\).

Several approaches, including stochastic gradient descent (SGD) [9; 10], SMO , and BO , have been proposed for solving the VQE optimization problem (1). Among others, state-of-the-art methods effectively incorporate unique properties of VQE. Let \(\{_{d}\}_{d=1}^{D}\) be the standard basis.

**Proposition 1**.: _[_9_]_ _(Parameter shift rule) The VQE objective function \(f^{*}()\) in Eq. (6) for any parametric quantum circuit \(G()\), Hermitian operator \(H\), and initial state \(|_{0}\) satisfies_

\[2}f^{*}()=f^{*}(+_{d})-f^{*}(-_{d}), [0,2)^{D},d=1,,D.\] (7)

Most SGD approaches rely on Proposition 1, which allows accurate gradient estimation from \(2 D\) observations. Another useful property is used for tailoring SMO  to VQE:

**Proposition 2**.: _[_11_]_ _For the VQE objective function \(f^{*}()\) in Eq. (6) with any \(G(),H\), and \(|_{0}\),_

\[^{3^{D}} f^{*}()= ^{}(_{d=1}^{D}(1, x_{d}, x_{d})^{ }),[0,2)^{D},\] (8)

_where \(\) and \(()\) denote the tensor product and the vectorization operator for a tensor, respectively._

Proposition 2 provides a strong prior knowledge on the VQE objective function \(f^{*}()\)--if we fix the function values at three general points along a coordinate axis, e.g., \(\{,_{d}\}\), for any \(\) and \(d\), then the whole function along the axis, i.e., \(f^{*}(+_{d}),[0,2)\) is fixed because it is a first-order sinusoidal function. Leveraging this property, a version of SMO was proposed . This optimization strategy, named after its authors _Nakanishi-Fuji-Todo_ (NFT), finds the global optimum in a single direction by sinusoidal fitting from three function values (two observed and one estimated) at each iteration, and showed state-of-the-art performance (see Appendix F.1 for details of NFT).

## 3 Proposed Method

In this section, we introduce our approach that combines BO and NFT by using a novel kernel and a novel acquisition function. After introducing these two ingredients, we propose our approach for VQE optimization. Lastly, we prove the equivalence between Propositions 1 and 2. We refer the reader to Appendix F, in particular Algorithms 1 to 3, for the pseudo-codes and further algorithmic details complementing the brief introduction to the algorithms presented in the following sections.

### VQE Kernel

We first propose the following VQE kernel, and use it for the GP regression model (2):

\[k^{}(,^{})=_{0}^{2}_{d=1}^{D}( +2(x_{d}-x_{d}^{})}{^{2}+2}),\] (9)

where \(_{0}^{2},^{2}>0\) are kernel parameters. \(_{0}^{2}\) corresponds to the prior variance, while \(^{2}\) controls the smoothness of the kernel. For \(^{2}=1\), the VQE kernel is the product of Dirichlet kernels , each of which is associated with each direction \(d\). We can show that this kernel (9) exactly gives the finite-dimensional feature space specified by Proposition 2 (the proof is given in Appendix D):

**Theorem 1**.: _The VQE kernel (9) is decomposed as_

\[k^{}(,^{})=()^{}( {x}^{}),()=}{(^{2}+2)^{D/2}} (_{d=1}^{D}(, x_{d}, x_{d})^{} ).\]

Let \(^{}\) be the set of all possible VQE objective functions specified by Proposition 2. Theorem 1 guarantees that the support of the GP prior in Eq. (2) matches \(^{}\), if we use the VQE kernel function (9) with a prior mean function such that \(()^{}\). Thus, the VQE kernel drastically limits the expressivity of GP _without model misspecification_, which enhances statistical efficiency. A more important consequence of Theorem 1 is that Proposition 2 holds for any sample from the GP posterior with the VQE kernel (9). This implies that if GP is certain about three general points along an axis, it must be certain about the whole 1-D subspace going through those points. Theorem 1 can be generalized to the non-exclusive case, where each entry of \(\) may parametrize multiple gates simultaneously (see Appendix E).

### EMICoRe: Expected Maximum Improvement over Confident Regions

In GP regression, the predictive covariance does not depend on the observations \(\) (see Eq. (4)), implying that, for given training inputs \(^{N}\), we can compute the predictive variance \(s_{}(,)\) at any \(\)_before observing the function values_. Let us define _Confident Regions_ (CoRe) as

\[_{}=\{;s_{}(,) ^{2}\},\] (10)

which corresponds to the set on which the predicted uncertainty by GP is lower than a threshold \(\). For sufficiently small \(\), an appropriate kernel (which does not cause model misspecification), and a sufficiently weak prior, the GP predictions on CoRe are already accurate, and therefore CoRe can be regarded as "observed points." This leads to the following acquisition function, which we call Expected Maximum Improvement over Confident Regions (EMICoRe):

\[a^{}(^{})=(0, _{_{}}f()-_{_{ {}}}f())_{p(f)|,)},\] (11)

where \(}=(,^{})^{N+M}\) denotes the augmented training set with the new input points \(^{}^{M}\). This acquisition function evaluates the expected maximum improvement (per new observation point) when CoRe is expanded from \(_{}\) to \(_{}}\) by observing the objective at the new input points \(^{}\). EMICoRe can be seen as a generalization of existing methods. If CoRe consists of the training points, it reduces to NEI . If we set \(\) so that the whole space is in the CoRe, and the random function \(f()\) in Eq. (11) is replaced with its predictive mean of the current (first term) and the updated (second term) GP, EMICoRe reduces to knowledge gradient (KG) . Thus, KG can be seen as a version of EMICoRe that ignores the uncertainty of the updated GP.

#### 3.2.1 NFT-with-EMICoRe

We enhance the state-of-the-art NFT approach  with the VQE kernel and EMICoRe. We start from a brief overview of the NFT algorithm (detailed algorithms of NFT, NFT-with-EMICoRe, and the EMICoRe subroutine are given in Appendix F).

Nft:First, we initialize with a random point \(}^{0}\) with \(^{0}=f^{*}(}^{0})+\). Then, for each iteration step \(t\), we proceed as follows:

1. Select an axis \(d\{1,,D\}\) sequentially or randomly and observe the objective \(^{}=(y^{}_{1},y^{}_{2})^{}\) at _deterministically_ chosen two points \(^{}=(^{}_{1},^{}_{2})=\{}^{t-1 }-2/3_{d},}^{t-1}+2/3_{d}\}\) along the axis \(d\).
2. Fit the sinusoidal function \(()=c_{0}+c_{1}+c_{2}\) to the two new observations \(y^{}_{1},y^{}_{2}\) as well as the previous best estimated score \(^{t-1}\). The optimal shift \(\) that minimizes \(()\) is analytically computed, which is used to get the new optimum \(}^{t}=}^{t-1}+_{d}\).
3. The best score is updated as \(^{t}=(}^{t})\).

We stress that if the observation noise is negligible, i.e., \(y f()\), each step of NFT reaches the global optimum in the one-dimensional subspace along the chosen axis \(d\), and thus performs SMO, see Proposition 2. In this case, the choice of the two new observation points \(^{}\) is not important, as long as any pair of the three points are not exactly the same. However, when the observation noise is significant, the estimated global optimum in the chosen subspace is not necessary accurate, and the accuracy highly depends on the choice of the new observation points \(^{}\). In addition, errors can be accumulated in the best score \(^{t}\), and therefore an additional measurement needs to be performed at \(}^{t}\) after a certain iteration interval.

NFT-with-EMICoRe (ours):We propose to apply BO to NFT by using the VQE kernel and EMICoRe. Specifically, we use the GP regression model with the VQE kernel as a surrogate for BO, and choose new observation points \(^{}\) by using the EMICoRe acquisition function. NFT-EMICoRe starts from \(T_{}\) NFT iterations until GP gets informative with a sufficient number of observations. After this initial phase, we proceed for each iteration \(t\) as follows:

1. Select an axis \(d\{1,,D\}\) sequentially, and new observation points \(^{}\) by BO with EMICoRe, based on the previous optimum \(}^{t-1}\), the previous training data \(\{^{t-1},^{t-1}\}\), and the current CoRe threshold \(^{t}\) (this subroutine for EMICoRe will be explained below).

2. We observe \(^{}\) at the new points \(^{}\) chosen by EMICoRe, and train GP with the updated training data \(^{t}=(^{t-1},^{}),^{t}=(^{t-1},^{ })^{}\).
3. The subspace optimization is performed by fitting a sinusoidal function \(()\) to the GP posterior means \(=((}^{t-1}-2/3_{d}),(}^{t-1}), (}^{t-1}+2/3_{d}))^{}\) at three points. With the analytic minimum of the sinusoidal function, \(=*{argmin}_{}()\), the current optimum is computed: \(}^{t}=}^{t-1}+_{d}\).
4. For the current best score, we simply use the GP posterior mean \(^{t}=(}^{t})\) and we set the new CoRe threshold to \[^{t+1}=^{t-T_{}}-^{t}}{T_{}}.\] (12)

Note that the GP posterior mean function lies in the VQE function space \(^{}\), and therefore the fitting is done without error, and the choice of the three points in Step 3 does not affect the accuracy. Note also that the CoRe threshold \(^{t+1}\) adjusts the required accuracy to the average reduction of the best score over the \(T_{}\) latest iterations--in the early phase where the energy \(^{t}\) decreases steeply, a large \(\) encourages crude optimization, while in the converging phase where the energy decreases slowly, a small \(\) enforces accurate optimization. Figure 1 illustrates the procedures of NFT and our EMICoRe approach.

The EMICoRe subroutine receives the previous optimum \(}^{t-1}\), the previous training data \(\{^{t-1},^{t-1}\}\), and the current CoRe threshold \(^{t}\), and returns the new points \(^{}\) that maximizes EMICoRe. We fix the number of new samples to \(M=2\), and perform grid search along the chosen direction \(d\). To this end,

1. We prepare \(J_{}(J_{}-1)\) combinations of \(J_{}\) search grid points \(\{}^{t-1}+_{j}_{d}\}_{j=1}^{J_{}}\), where \(=(_{1},_{J_{}})^{}=}+1}(1,,J_{})^{}\), as a candidate set \(=\{}^{j}^{D 2}\}_{j=1}^{J_{}(J_{ }-1)}\).
2. For each candidate \(}\), we compute the _updated_ GP posterior variance \(s_{}}(,),^{}\), where \(s_{}}(,)\) is the posterior covariance function of the GP trained on the augmented training points \(}=(^{t-1},})\), and \(^{}=\{}^{t-1}+_{j}_{d}\}_{j=1}^{J_{ }}\) with \(=(_{1},_{J_{}})^{}=}+1}(1,,J_{})^{}\) are \(J_{}\) grid points along the axis \(d\).
3. We obtain a discrete approximation to the updated CoRe as \(_{}}=\{^{};s_{}}(,)^{2}\}\). For simplicity, we approximate the previous CoRe to the previous optimum, i.e., \(_{^{t-1}}=\{}^{t-1}\}\). After computing the mean and the covariance of the previous GP posterior, \(p(,^{}|^{t-1},^{t-1})\), at the previous best point \(}^{t-1}\) and the updated CoRe points (as the test set \(^{}=_{}}\))--which is \(\)-dimensional Gaussian for \(=|_{^{t-1}}_{}}|= 1+|_{}}|\)--we estimate \(a_{^{t-1}}^{}=\{0,-( ^{})\}_{p(,^{}|^{t-1}, ^{t-1})}\) by quasi Monte Carlo sampling.

The subroutine iterates this process for all candidates, and returns the best one,

\[^{}=*{argmax}_{}a_{^{t-1}}^{ }(})\,.\]

Parameter SettingOur approach has the crucial advantage that the sensitive parameters can be automatically tuned. The kernel smoothness parameter \(\) is optimized by maximizing the marginal likelihood of the GP, and the CoRe threshold \(\) is set to the average energy decrease of the last iterations as explained above.1 The noise variance \(^{2}\) is set by observing \(^{*}()\) several times at several random points and estimating \(^{2}=^{*2}(N_{})\). For the GP prior, the zero mean function \(()=0,\) is used, and the prior variance \(_{0}^{2}\) is roughly set so that the absolute value of the ground-state energy is in the same order as \(_{0}\). Other parameters, including the number of grid points for search and CoRe discretization, should be set to sufficiently large values. See Appendix F for more details of parameter setting.

### Equivalence Between VQE Properties

Our method, NFT-with-EMICoRe, adapts BO to the VQE problem by harnessing one of the useful properties of VQE, namely the highly constrained objective function form (Proposition 2). One may now wonder if we can further improve NFT-EMICoRe by harnessing the other property, e.g., by regularizing GP so that its gradients follow the parameter shift rule (Proposition 1). We give a theory that answers to this question. Although the two properties were separately derived by analyzing the VQE objective (6), they are actually mathematically equivalent (the proof is given in Appendix G):

**Theorem 2**.: _For any periodic function \(f^{*}:[0,2)^{D}\), Eq. (7) \(\) Eq. (8)._

This means that any sample from the GP prior with the VQE kernel (and any prior mean such that \(()^{}\)) already satisfies the parameter shift rule. Theorem 2 implies that the parameter shift rule and the VQE function form are not two different properties, but two expressions of a _single_ property, which is an important result that--to our knowledge--was not known in the literature.

## 4 Experiments

We numerically demonstrate the performance of our approach for several setups, where the goal is to find the ground state of the quantum Heisenberg Hamiltonian

\[H=-[_{j=1}^{Q-1}(J_{X}_{j}^{X}_{j+1}^{X}+J_{Y}_{j}^ {Y}_{j+1}^{Y}+J_{Z}_{j}^{Z}_{j+1}^{Z})+_{j=1}^{Q}(h_{X} _{j}^{X}+h_{Y}_{j}^{Y}+h_{Z}_{j}^{Z})],\] (13)

where \(\{_{j}^{X},\,_{j}^{Y},\,_{j}^{Z}\}\) represent the Pauli matrices applied on the qubit in site \(j\). The Heisenberg Hamiltonian, which represents a standard benchmarks of high practical relevance, is commonly used for evaluating the VQE performance (see, e.g., ),6 and its ground-truth ground state \(}}\) along with the ground-state energy \(^{*}=}}H}}\)--which gives a lower-bound of the VQE objective (6)--can be analytically computed for small \(Q\). For the variational quantum circuit \(G()\), we use the \(L\)-layered Efficient SU(2) circuit (see Appendix C) with the open boundary, for which the search domain of the angular variables is \([0,2)^{D}\) with \(D=(2+(L 2)) Q\). We classically simulate the quantum computation with the Qiskit  library, and our Python implementation along with detailed tutorials on how to reproduce the results is publicly available on GitHub  at _https://github.com/emicore/emicore_.

To measure the optimization performance, i.e., the quality of the final solution \(}^{T}\) after \(T\) iterations, we use two metrics: the _true_ achieved lowest energy after \(T\) steps, \( f^{*}(}^{T})=}G(}^{T})^{}HG(}^{T})}\), which is evaluated by simulating the noiseless observation with \(N_{}\), and the fidelity \(}}}^{T}}=}}G(}^{T})}\), which measures how similar the best solution is to the true ground state. As the cost of observations, we count the total number of observed points, ignoring the cost of classical computation. We test each method 50 times, using the same set of initial points, for each method, for fair comparison. The initial points were randomly drawn from the uniform distribution in \([0,2)^{D}\), for fair comparisons. Details of the VQE circuits and the experimental settings can be found in Appendices C and H, respectively.

### VQE-kernel Analysis

We first investigate the benefit of using the VQE-kernel for the following optimization problem: the task is to minimize the VQE objective (6) for the Ising Hamiltonian, a special case of the Heisenberg Hamiltonian with the coupling parameters set to \(J_{X}=-1,\,J_{Y}=J_{Z}=0,h_{X}=h_{Y}=0,\,h_{Z}=-1\), with the \((L=3)\)-layered quantum circuit with \((Q=3)\)-qubits. Namely, we solve the problem (1) in the \((D=2(L+1)Q=24)\)-dimensional space. We use the standard BO procedure with GP regression, and compare our VQE-kernel with the Gaussian radial basis function (RBF)  and the periodic kernel , where the EI acquisition function is maximized by L-BFGS . Figure 2 shows the achieved energy (left) and the fidelity (right) with different kernels. In each panel, the leftplot shows the progress of optimization (median as a solid curve and the 25- and 75-th percentiles as shadows) as a function of the observation cost, i.e., the number of observed points after corresponding iterations. The portrait square on the right shows the distribution (by kernel density estimation ) of the best solutions after 150 observations have been performed. We see that the proposed VQE kernel (red), which achieves \(0.93 0.05\) fidelity, converges more stably than the baseline RBF and periodic kernels, both of which achieve \(0.90 0.09\) fidelity. Therefore, the VQE kernel leads to better statistical efficiency albeit its effect appears rather mild in the regime of a small number of qubits.

### NFT-with-EMICoRe Analysis

We now evaluate our NFT-with-EMICoRe approach and show that this improves the state-of-the-art baselines of NFT . Specifically, we compare our method with two versions of NFT: NFT-Sequential updates along each axis sequentially, while NFT-Random chooses the axis randomly. Figure 3 shows the results of VQE for the Ising Hamiltonian (top row) and the Heisenberg Hamiltonian (bottom row) with the parameters set to \(J_{X}=J_{Y}=J_{Z}=1\) and \(h_{X}=h_{Y}=h_{Z}=1\). For both

Figure 3: Comparison (in the same format as Figure 2) between our EMICoRe (red) and the NFT baselines (green and purple) in the VQE for the Ising (top row) and Heisenberg (bottom row) Hamiltonians with the \((L=3)\)-layered \((Q=5)\)-qubits quantum circuit (thus, \(D=40\)) and \(N_{ shots}=1024\). We confirmed for the Ising Hamiltonian that **longer optimization by EMICoRe up to 6000 observed points reaches the ground state with \(98\%\) fidelity (see Appendix I.1)**.

Figure 2: Comparison of our VQE-kernel (red) to the RBF and the periodic kernel benchmarks (blue and orange) in the VQE optimization using the standard BO procedure, for the Ising Hamiltonian with the \((L=3)\)-layered \((Q=3)\)-qubits quantum circuit. The search domain dimension is \(D=24\), and \(N_{ shots}=1024\) readout shots are taken for each observation. The energy (left) and the fidelity (right) are plotted, and in each plot, optimization progress is shown with the median (solid) and the 25- and 75-th percentiles (shadows) over 50 trials. The portrait square shows the distribution of the final solution after 150 observations have been performed.

Hamiltonians, we use \((L=3)\)-layered \((Q=5)\)-qubits variational quantum circuits, thus \(D=40\), with each observation performed by \(N_{}=1024\) readout shots. The figure shows that our NFT-with-EMICoRe method consistently outperforms the baselines in terms of both achieved energy (left) and fidelity (right). Moreover, the variance over the trials is reduced, implying its robustness against the initialization. We conducted additional experiments to answer the important questions of whether our approach converges to the ground state with high fidelity, and how the Hamiltonian, the number of qubits, and the number of readout shots affect the optimization performance. The results reported in Appendix I show that our NFT-with-EMICoRe method **reaches the ground state with 98% fidelity after 6000 observed points**, see Appendix I.1, and consistently outperforms the baseline methods in various settings of Hamiltonians, qubits, and the number of readout shots, see Appendices I.2 and I.3. Thus, we conclude that our approach of combining NFT and BO with the VQE-kernel and the EMICoRe acquisition function is suited for efficient VQE optimization, dealing with different levels of observation noise.

Figure 4 shows the evolution of the GP during optimization by NFT-with-EMICoRe. The blue curve and the shadow show the posterior mean (solid) and the uncertainty (shadow) at four different steps \(t\) (columns). The top and the bottom rows show the GP before and after the two new points \(^{}=(^{}_{1},^{}_{2})\), chosen by EMICoRe, are observed. The red solid curve shows the true energy \(f^{*}()\). We observe the following: In the early phase (the left two columns), GP is uncertain except at the current optimum \(}^{t}\) before new points are observed; After observing the chosen two points \(^{}\), GP is certain on the whole subspace, thanks to the VQE kernel; and in the converging phase (the right two columns), GP is certain over the whole subspace even before new observations, and the fine-tuning of the angular variable \(\) is performed.

## 5 Conclusion

Efficient, noise-resilient algorithms for optimizing hybrid quantum-classical algorithms are pivotal for the NISQ era. In this paper, we propose EMICoRe, a new Bayesian optimization approach, in combination with a novel VQE-kernel which leverages strong inductive biases from physics. Our physics-informed search with EMICoRe achieves faster and more stable convergence to the minimum energy, thus identifying optimal quantum circuit parameters. Remarkably, the physical inductive bias makes the optimization more resilient to observation noise. The insight that more physical information helps to gain a better practical understanding of quantum systems is a starting point for further algorithmic improvements and further exploration of other possible physical biases. This may ultimately lead to more noise-resilient algorithms for quantum circuits, thus enabling the accessibility of larger-scale experiments in the quantum computing realm. In future work, we plan to investigate the effect of hardware-dependent noise, and test our approach on real quantum devices.

Figure 4: Evolution of the GP in NFT-with-EMICoRe. The posterior mean (blue solid) with uncertainty (blue shadow) along the direction \(d\) to be optimized in the steps \(t=0,1,293,294\) (columns) are shown before (top row) and after (bottom row) the chosen two new points \(^{}=(^{}_{1},^{}_{2})\) are observed. The red solid curve shows the true energy \(f^{*}()\). On the top, the number of observed samples \(||\) until step \(t-1\) and the direction \(d\) are shown.