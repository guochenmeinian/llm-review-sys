# Dynamo-Depth: Fixing Unsupervised Depth Estimation for Dynamical Scenes

Yihong Sun

Cornell University

yihong@cs.cornell.edu

&Bharath Hariharan

Cornell University

bharath@cs.cornell.edu

###### Abstract

Unsupervised monocular depth estimation techniques have demonstrated encouraging results but typically assume that the scene is static. These techniques suffer when trained on dynamical scenes, where apparent object motion can equally be explained by hypothesizing the object's independent motion, or by altering its depth. This ambiguity causes depth estimators to predict erroneous depth for moving objects. To resolve this issue, we introduce Dynamo-Depth, an unifying approach that disambiguates dynamical motion by jointly learning monocular depth, 3D independent flow field, and motion segmentation from unlabeled monocular videos. Specifically, we offer our key insight that a good initial estimation of motion segmentation is sufficient for jointly learning depth and independent motion despite the fundamental underlying ambiguity. Our proposed method achieves state-of-the-art performance on monocular depth estimation on Waymo Open  and nuScenes  Dataset with significant improvement in the depth of moving objects. Code and additional results are available at https://dynamo-depth.github.io.

## 1 Introduction

Embodied agents acting in the real world need to perceive both the 3D scene around them as well as how objects around them might behave. For instance, a self-driving car executing a lane change will need to know where the nearby cars are and how fast they are moving. Instead of relying on expensive sensors such as LiDAR to estimate this structure, a promising alternative is to use commodity cameras. This has motivated a long line of work on monocular depth estimation using neural networks.

While neural networks can learn to estimate depth, the dominant training approach requires expensive 3D sensors in the training phase. This limits the amount of training data we can capture, resulting in downstream generalization challenges. One would prefer to train these depth estimators _without supervision_, for e.g., using unlabeled videos captured from a driving car. This is possible to do if the videos are produced by a camera moving in a static scene, since the apparent motion (i.e., optical flow) of each pixel is then inversely proportional to the depth of the pixel. This provides a useful supervisory signal for learning depth estimation and has been used effectively in the past .

However, for unsupervised approaches to learn depth, _dynamic scenes_ with moving objects pose a challenge. Here, the apparent pixels motion on the image plane is a combined effect of _camera ego-motion_ (or rigid scene motion) and the _independent motion_ of objects in the scene. Methods that ignore the presence of moving objects and treat the scene as static will then learn erroneous depth, where the depths of moving objects are altered to match their observed motion, as shown in Figure 1 (b). Past works have tried to overcome this issue by having another module predict independent object motion , but the presence of multiple equivalent solutions makes training difficult: even with sophisticated regularizers, the training can converge to the degenerate solution of either predicting the whole scene as static with incorrect depth, or predicting the entire scene as moving on a flat canvas. This results in depth error on moving objects at least 4 times as large as the error on staticbackground even for state-of-the-art methods. Since navigating safely around moving objects is a primary challenge in self-driving, poor depth estimation of moving objects is particularly problematic.

In this paper, we propose a new architecture and training formulation for this unsupervised learning problem that substantially improves depth accuracy for moving objects. Our key insight is that we need a good initial estimate for which pixels are independently moving (a "motion mask"). With this motion mask at hand, one can easily train depth estimation using the static pixels, and account for independent motion to correctly estimate the depth of the moving objects. However, the challenge is obtaining this motion mask in the first place: how do we identify which pixels are independently moving? First, we introduce two separate network modules that predict _complete 3D scene flow_ and _camera ego-motion_. The difference between these can then be used to identify the moving objects. Second, we introduce a novel initialization scheme that stops depth training early to prevent the depth estimator from learning erroneous depth for the moving objects. This allows the motion estimation networks to learn the correct scene flow. In sum, we make the following contributions:

1. We propose a new approach, Dynamo-Depth, for learning depth estimation for dynamic scenes solely from unlabeled videos. In addition to monocular depth, Dynamo-Depth also predicts camera ego-motion, 3D independent flow, and motion segmentation.
2. We propose a new architecture that identifies independent object motion via the difference between the rigid flow field induced by camera ego-motion and a complete scene flow field to facilitate learning and regularization.
3. We propose a novel motion initialization technique that learns an early estimation of motion segmentation to explicitly resolve the ambiguity and disentangle independent motion from camera ego-motion (Figure 1). This allows the entire architecture to learn end-to-end without any additional supervision from auxiliary model predictions or ground-truth labels.
4. We evaluate on both nuScenes  and Waymo Open  and demonstrate not only state-of-the-art performance in terms of overall depth accuracy and error, but also show large improvements on moving objects by up to 62% relative gain in accuracy and 68% relative reduction in error, while achieving up to 71.8% in F1 score in motion segmentation without any supervision.

## 2 Related Work

### Unsupervised Monocular Depth Estimation

The framework for jointly learning monocular depth and ego-motion from unlabeled consecutive frames was first explored in [48; 36], where the objective was posed as a novel-view synthesis problem. Here, the monocular depth estimated from the target frame is coupled with the predicted ego-motion

Figure 1: Visualization of erroneous cases of infinite object depth and floating objects that arise from the static scene constraint. The motion of the black SUVs and white van are explained via (b) _incorrect_ depth predictions to generate (c) the correct rigid flow for reconstruction under the static scene constraint. In contrast, Dynamo-Depth predicts (d) _correct_ monocular depth by explicitly disentangling (f) the 3D independent flow field from (e) the camera ego-motion induced rigid flow.

to warp the reference frame to be photometrically consistent with the target frame. Following these works, 3D geometric consistency  and depth prediction consistency  were imposed across consecutive frames to further constrain the unsupervised learning objective. Furthermore, discrete volumes , uncertainty estimation , optical flow based pose estimation , minimal projection loss with auto-masking , and multiple input frames during inference [40; 25] were also proposed to improve depth predictions and mitigate influence of occlusion and relative stationary pixels.

**Unsupervised Scene Flow.** In addition to predicting depth from a single frame, a closely related task of scene flow estimation - obtaining both the 3D structure and 3D motion from two temporally consecutive images - can also be learned by leveraging the same novel-view reconstruction objective. When trained on sequences of stereo-frames, scene flow can be estimated from both stereo [18; 41] and monocular [16; 17] inputs during inference. Additionally, DRAFT  learned from monocular sequences only by utilizing synthetic data and multiple consecutive frames during inference.

### Mitigating Static Scene Constraint

In essence, the reconstruction objective encourages pixels to be predicted at an appropriate distance away to explain its apparent motion on the image plane, when coupled with the predicted ego-motion. This assumes that all pixel motion can be sufficiently modeled by depth and camera ego-motion alone, which implies an underlying static scene. However, since dynamical objects are common in the wild, such static scene assumption is often violated. To disambiguate independent motion, recent works were proposed to leverage stereo-view information [38; 26; 11], additional input modalities [45; 8], synthetic data , supervised segmentation task , and auxiliary monocular depth network . Extending from these methods, additional works proposed to mitigate the negative effects by modeling the behavior of dynamical objects without the use of additional modalities and labels.

**Modeling Independent Motion via Optical Flow.** Although an explainability mask  can be used to ignore the independently moving regions, recent works exploited the jointly learned optical flow to refine the depth prediction under non-rigid scene settings. This includes predicting a residual optical flow [43; 35] to account for dynamical objects and enforcing forward-backward flow consistency  to discount dynamical regions. EPC++  directly modeled the dynamical objects by integrating its depth, ego-motion, and optical flow predictions. Additionally, CC  proposed a coordinated learning framework for joint depth and optical flow estimation assisted by a segmentation branch.

**Modeling Independent Motion via 3D Non-Rigid Flow Field.** Instead of modeling dynamical objects via 2D optical flow, additional works proposed to explicitly model their 3D independent flow field to encourage accurate depth learning for dynamical objects. To facilitate independent motion learning, various methods [4; 12; 22; 23; 2; 32] leveraged semantic priors from off-the-shelf pretrained detection or segmentation models to estimate regions that are "possibly moving." Additionally, Dyna-DepthFormer  used multi-frame information to compute the depth map, while iteratively refining the residual 3D flow field via a jointly trained motion network. Finally, Li _et al._ proposed to jointly learn monocular depth, ego-motion, and residual flow field via a sparsity loss and RM-Depth  improved upon it via an outlier-aware regularization loss. Unlike [24; 15] where the residual/independent flow is predicted directly, we propose to model the 3D independent flow field via the complete scene flow, which facilitates training and effectively enforces sparsity.

## 3 Motivation and background

When a camera moves in a rigid scene, the apparent motion of image pixels is entirely explained by (a) their 3D position relative to the camera, and (b) the motion of the camera. This fact is used by unsupervised techniques that learn to predict depth from unlabeled videos captured as a camera moves through the scene (as in driving footage). Concretely, one network predicts depth from a target frame and another predicts the motion of the camera from the source frame to the target frame. The two predictions are then used to reconstruct a target frame from the source frame, and a reconstruction loss would serve as the primary training objective.

What happens if there are moving objects in the scene? Clearly, depth and camera-motion are not sufficient to explain the apparent image motion of these objects, resulting in a high reconstruction loss for these dynamical regions. In principle, the motion of these objects are unpredictable from a single frame, and so even though the network sees a high loss for these objects, the best it can do is to fit the average case of a static object. As such, in principle, moving objects should not be a concern for learning-based techniques. However, in practical driving scenes, there is a large class of moving objects for which the network can in fact minimize the reconstruction loss by predicting an incorrect depth estimate. We describe this class below.

**Epipolar ambiguity in driving scenes:**

Consider the simple case of a pinhole camera moving forward along the Z axis (i.e., along its viewing direction). This is in fact the common case of a car driving down the road. In this case, the image location \((x,y)\) and corresponding optical flow \((,)\) for a 3D point \((X,Y,Z)\) in the world would be:

\[x=, y=,=}{Z^{2}}=- {}{Z}x,=}{Z^{2}}=-}{Z}y\] (1)

Consider what happens when this 3D point is on an object moving _collinear_ to the camera (e.g., cars in front of the ego-car, or oncoming traffic). In this case, there are two contributions to \(\): the camera motion and the independent motion of the object. Thus, given ego-motion, one can produce the same optical flow by adding the appropriate amount of independent motion to \(\), or changing the depth \(Z\). For example, an object can appear to move faster if it is either closer and static (i.e., \(Z\) is smaller) or it is farther and moving towards the camera (i.e., \(\) is larger). This is a special case of _epipolar ambiguity_. This implies that it is possible to model independent motion under the static scene constraint using an erroneous depth estimation. Unfortunately, there are statistical regularities that enable such erroneous learning.

**Statistical regularities:**

In self-driving scenes, there is often enough information in object appearance to predict its motion even from a single frame. The high capacity monocular depth network can learn to recover this information, and then alter its depth prediction to better reconstruct the target frame. For instance, in Figure 1 (b) the depth network may have learnt to recognize the backs of cars on the same or nearby lanes. In the training data, cars seen from the back often travel in the same direction with similar speed as the ego car, and thus have a very small optical flow in consecutive frames. The depth network therefore predicts that the car should be very far away; thus correctly predicting its optical flow, but incorrectly predicting its depth. A similar effect happens when the white van is recognized in its frontal view on the opposite lane, and the depth network predicts it to be closer to the ego-car. This allows the depth network to model the statistically-likely higher optical flow as a result of the object moving towards the camera. Furthermore, the speed of the object can also be roughly estimated from object identity (pedestrian < cyclist < car) and background context (residential < city < highway). Therefore, by recognizing the statistical regularities that give away object motion, the depth model can model the behavior of dynamical objects by predicting incorrect depth values to minimize the reconstruction objective.

This suggests that simply training a depth network on dynamic scenes is not a good solution, and an alternative framework is needed. However, we do note that learning these statistical regularities about object motion is much harder than learning the depth of the static background, since the background is a lot more consistent in its optical flow across videos and frames. Thus, we observe that this kind of overfitting to dynamical objects only happens later in training; a fact we use below.

## 4 Method

Following past works, we formulate the learning objective as a novel-view synthesis where the target frame is reconstructed from the source frame. As shown in Section 3, modeling the rigid flow induced by camera motion alone is not sufficient for modeling the behavior of dynamical objects and causes performance degradation for moving objects. We address this by learning a separate 3D flow field that captures the independent motion of dynamical objects.

### Architecture

The proposed architecture (Figure 2) contains two components that collaborate together to explain the apparent pixels motion on the image plane. Section 4.1.1 details the rigid motion networks that model the 3D rigid flow field (motion induced by camera while assuming a static scene), which includes the depth network \(\) and ego-motion pose network \(\). Section 4.1.2 details the independent motion networks that model the 3D independent flow field (motion of dynamical objects), which includes the complete flow network \(\) and motion mask network \(\).

#### 4.1.1 Rigid Motion Networks: Depth \(\) & Pose \(\)

Given a source frame \(}\) and a target frame \(}\), we compute the 3D rigid flow induced by camera motion, \(}(p_{t})\), for each target pixel \(p_{t}\) as follows. We first obtain the monocular depth map \(}\) by passing \(}\) to the depth network \(\) (Eq. 2). We pass \(}\) and \(}\) to the ego-motion pose network \(\) to obtain the relative camera pose \(s}}\) (rotation and translation) (Eq. 3). We then use the depth and inverse camera intrinsics \(}\) to back-project every target pixel \(p_{t}\) to compute its 3D location \(P_{t}\), and transform \(P_{t}\) into its corresponding 3D location, \(_{s}\), in the source frame via \(s}}\). (Eq. 4). The "rigid flow" induced by camera motion is then simply \(}(p_{t})=_{s}-P_{t}\). 2

\[:^{H W 3} ^{H W} }=(})\] (2) \[:^{H W 3}^{H  W 3} ^{3 4} s}}=(},})\] (3) \[P_{t}=}(p_{t})K^{-1}},_{s}=s}}}, }(p_{t})=_{s}-P_{t}^{3}\] (4)

#### 4.1.2 Independent Motion Networks: Complete Flow \(\) & Motion Mask \(\)

In addition to the rigid flow \(}\), we also wish to model object motion via the 3D independent flow, denoted as \(}\). With independent motion explicitly modeled by \(}\), we reformulate \(_{s}\) as \(_{s}=(}(p_{t})+}(p_{t})+P_{t})\) to integrate the contribution of the independent flow vector \(}(p_{t})\). However, directly estimating \(}\) via a network turns out to be difficult for two reasons:

**(a) Learning.** During training, the network will need to fill in the missing independent motion vector that is appropriate for the current separately predicted monocular depth and ego-motion for a correct reconstruction: a "moving target" during training. Furthermore, it is intuitively much more difficult to learn and predict independent motion directly when the apparent motion in the input frames consists of both rigid and independent motion entangled together, especially in the ambiguous scenarios illustrated in Section 3.

**(b) Regularization.** Adding to this complexity, \(}(p_{t})\) should be non-zero if and only if the pixel \(}(p_{t})\) is independently moving. One could regularize this network to encourage sparsity, but early on when depth and ego-motion predictions are noisy, a high sparsity term would encourage this

Figure 2: **Overview of the proposed Dynamo-Depth.** The target frame \(}\) is reconstructed from source frame \(}\) by composing the 3D rigid flow \(}\) (predicted via Depth \(\) & Pose \(\)) and 3D independent flow \(}\) (predicted via Complete Flow \(\) & Motion Mask \(\)). 1

network to turn off entirely. At the other end of the spectrum, a weak sparsity regularization allows independent flow \(}\) to be active on static regions of the scene, which would "explain away" the rigid motion \(}(p_{t})\) when computing \(_{s}\) and corrupt the learning signal for depth estimation. Also, conventional sparsity regularization on \(}\) (e.g., \(L_{1/2}\) sparsity loss ) would dampen the magnitude of predicted motion, which reduces \(}\)'s capacity to model fast or far-away moving objects.

To address these issues, we propose to decompose the prediction of independent flow \(}(p_{t})\) in two: (1) predicting whether pixel \(}(p_{t})\) is independently moving and (2) predicting how it is moving. For (1), we model the probability of independent motion as a binary motion segmentation task where motion mask network \(\) takes the consecutive frames \(}\) and \(}\) and predict a per-pixel motion mask \(\) as shown in Eq. 5. For (2), instead of directly predicting the independent motion, we predict the complete scene flow \(}(p_{t})\) via a complete flow network \(\) as shown in Eq. 6.

\[:^{H W 3}^{H  W 3}^{H W} =((},}))\] (5) \[:^{H W 3}^{H  W 3}^{H W 3} }=(},})\] (6)

From these predictions, we formulate the independent flow field as \(}=(}-})\), i.e., the residual flow between \(}\) and \(}\) gated by the motion mask \(\). Intuitively, computing \(_{s}=(}(p_{t})+}(p_{t})+P_{t})\) can be thought of as combining the contribution of the complete flow \(}(p_{t})\) and rigid flow \(}(p_{t})\) as a weighted average via \((p_{t})\) (Eq. 8).

\[}=(}-})\] (7) \[_{s}=}(p_{t})+}(p_{t})+P_{t}=(p_{t })}(p_{t})+(1-(p_{t}))}(p_{t})+P_{t}\] (8)

This formulation directly resolves the two aforementioned issues. For (a), \(\) has a simpler learning objective of modeling the full scene flow with per-pixel predictions, i.e. \(_{s}=}(p_{t})+P_{t}\). For (b), the sparsity regularization can be applied to \(\) alone without any impact to the learning of \(\) and its ability to learn the complete scene flow.

Finally, we compute the reconstruction \(_{t}}(p_{t})\) by sampling \(}\) at the corresponding pixel coordinate \(_{s}\) via \(_{s}} K_{s}\) for every target pixel \(p_{t}\).

### Loss Function

We present the overall loss function in Eq. 9. The photometric loss \(L_{recon}\) in Eq. 10 serves as the main learning objective, which evaluates the reconstruction \(_{t}}\) via SSIM  and L1 weighted by \(\).

\[L =L_{recon}+L_{s}+_{c}L_{c}+_{m}L_{m}+_{g}L_{g}\] (9) \[L_{recon} =(1-(},_{t}}))+(1 -)||}-_{t}}||_{1}\] (10)

The smoothness loss \(L_{s}\) in Eq. 11 regularizes the smoothness of the predicted depth map \(}\), complete flow \(}\), and motion mask \(\). The edge-aware smoothness loss \(l_{s}(z,I)=|_{x}z|(-|_{x}I|)+|_{y}z||(-|_{y}I|)\) is weaker around pixels with high color variation. We also use the mean-normalized inverse depth \(^{}}\) to discourage shrinking of the estimated depth .

\[L_{s}=_{sd}l_{s}(^{}},})+_{sc}l_{s}(},})+_{sm}l_{s}(,})\] (11)

The motion consistency loss \(L_{c}\) in Eq. 12 computes the flow discrepancy \(F_{D}(p)\) between the complete scene flow \(}(p)\) and rigid flow \(}(p)\) for static pixels. The probability of pixel \(p\) being static is approximated by \((1-(p))\).

\[L_{c}=_{p}(1-(p)) F_{D}(p), F_{D}(p)=||}(p)-}(p)||_{1}\] (12)

The motion sparsity loss \(L_{m}\) in Eq. 13 considers all pixels with flow discrepancy \(F_{D}\) lower than the mean value over the frame as putative background, and encourages the motion mask to be 0 by using the cross entropy against label \(\), which is denoted as \(g()\).

\[L_{m}=g\{(p)\,:\, pF_{D}(p)(F_{D})\} \] (13)

Finally, the above-ground loss \(L_{g}\) in Eq. 14 penalizes projected 3D points below the estimated ground plane (via RANSAC) where \(d_{t}^{g}\) denotes the mean-normalized inverse depth of the ground plane.

\[L_{g}=_{p}(d_{t}^{g}(p)-^{}}(p))\] (14)

### Motion Initialization

Since the reconstruction loss is the main learning signal in the unsupervised approach, the gradient from the reconstruction loss between \(}\) and \(}}\) at pixel \(p_{t}\) would propagate through the computed sample coordinate \(_{s}} K_{s} K(}(p_{t})+}(p_{t})+P_{t})\). Here, although it may be ideal to jointly train all models end-to-end from start to finish, in practice, updating the rigid flow vector \(}(p_{t})\) and the independent flow vector \(}(p_{t})\) jointly from the start would cause the learning problem to be ill-posed, since each component has the capacity to overfit and explain away the other's prediction. Due to the ambiguity between camera motion and object motion as discussed in Section 3, the image reconstruction error alone is not a strong enough supervisory signal to enforce the accurate prediction of all underlying sub-tasks jointly, as a _family_ of solutions of depth and object motion would all satisfy the training objective, ranging from predicting the whole scene as static with incorrect depth to predicting the scene as moving objects on a flat canvas.

Here, we offer our key insight that a good initial estimate of motion segmentation \(\) would allow the system to properly converge without arising degenerate solutions. As shown in Eq. 8, \(_{s}=(p_{t})}(p_{t})+(1-(p_{t})) }(p_{t})+P_{t}\). For any pixel \(p_{t}\), \(_{s}}(p_{t})+P_{t}\) if \((p_{t})\) is near \(1\), and approximates \(}(p_{t})+P_{t}\) if \((p_{t})\) is near \(0\). From this, it is clear that a good motion mask would route the back-propagating gradient to the complete flow network \(\) via \(}\) if \(p_{t}\) is moving and route the gradient to the rigid motion networks \(\) and \(\) via \(}(p_{t})\) if \(p_{t}\) is static.

Of course, the question is how one can initialize a good motion mask, since it requires identifying the moving objects. Ideally, moving objects would be identified as the pixels that are poorly reconstructed based on depth and ego-motion alone, but as discussed in Section 3, the depth network has the capacity to alter its depth predictions and reconstruct even the moving objects. Nevertheless, we observe that overfitting to the moving objects in this way is difficult, and only happens in later iterations. In earlier training iterations, reconstruction errors for dynamical pixels are ignored (Appendix B.5).

To ensure a good motion mask \(\), we first train a depth network assuming a static scene, but stop updates to the depth network at an early stage. We then initialize the independent motion networks \(\) and \(\) from a coarse depth sketch predicted by this frozen early-stage depth network. Thus, without access to any labels or auxiliary pretrained networks, we obtain an initialization for motion mask \(\) that helps in disambiguating and accurately estimating the rigid flow \(}\) and independent flow \(}\).

## 5 Experiments

**Dataset.** While Dynamo-Depth is proposed for general applications where camera motion is informative for scene geometry, we focus our attention on the challenging self-driving setting, where the epipolar ambiguity and the statistical regularities mentioned in Section 3 that lead to erroneous depth predictions are highly prevalent. Specifically, we evaluate on three datasets - Waymo Open , nuScenes , and KITTI  with Eigen split .

On all datasets, we only use the unlabeled video frames for training; ground truth LiDAR depth is only used for evaluation. We evaluate both overall depth estimation accuracy, as well as accuracy computed separately for static scene and moving objects. For the latter evaluation, we use Waymo Open and nuScenes and identify moving objects by rectifying the panoptic labels with 3D box labels to obtain masks for static/moving objects. It is worth quantifying the number of moveable objects per frame in each dataset. Waymo Open has a mean of \(12.12\) with a median of \(9\). In nuScenes, the mean is \(7.78\) with a median of \(7\). In KITTI, due to lack of per-frame labels, we approximate using its curated object detection dataset, which only has a mean of \(5.26\) with a median of \(4\). Additional dataset information and distribution histograms are found in Appendix C.

**Model and Training setup.** The proposed method is trained on four NVIDIA 2080 Ti with a total batch size of 12 and an epoch size of 8000 sampled batches. Adam optimizer  is used with an initial learning rate of 5e-5 and drops to 2.5e-5 after 10 epochs. Motion Initialization lasts \(5\) epochs and takes place after the depth network and complete flow network have been trained for 1 epoch each. After initialization, the system is trained for 20 epochs, totalling approximately 20 hours. The hyperparameter values are the same for all experiments and are provided in Appendix B.1, along with the rest of the details of the model architecture. To demonstrate the generality of the proposed framework, we adopt two architectures for the depth network \(\), one with a ResNet18  backbonefrom _Monodepth2_ and another with a CNN/Transformer hybrid backbone from _LiteMono_, denoted as _Dynamo-Depth (MD2)_ and _Dynamo-Depth_, respectively. To be commensurate with baselines, the encoders are initialized with ImageNet  pretrained weights.

**Metrics.** Depth performance is reported via commonly used metrics proposed in , including 4 error metrics (Abs Rel, Sq Rel, RMSE, and RMSE log) and 3 accuracy metrics (\(<1.25\), \(<1.25^{2}\), and \(<1.25^{3}\)). We also report precision-recall curve for evaluating binary motion segmentation.

### Monocular Depth Estimation

As shown in Table 1, our proposed approach outperforms prior arts on both nuScenes and Waymo Open across all metrics, with over \(57\%\) and \(21\%\) reduction in overall Abs Rel for nuScenes and Waymo Open, respectively. This improvement is significant: we repeat the experiment for _Dynamo-Depth_ on Waymo Open \(3\) times and obtained a \(95\%\) confidence interval of \(0.119 0.003\) for Abs Rel and \(0.874 0.004\) for \(<1.25\). This suggests the challenge and importance of estimating accurate depth for moving objects in realistic scenes. On KITTI, our approach is competitive with prior art. However, KITTI has fewer moveable objects, which better conforms to the static scene constraint and diminishes the importance in modeling independent motion. For instance, _LiteMono_, which does not model independent motion, demonstrates superior performance on KITTI but drastically underperforms in nuScenes and Waymo Open, with both datasets having many more moving objects.

To further evaluate the effectiveness of our approach in modeling independent motion, we split nuScenes and Waymo Open into static background, static objects and moving objects and evaluate depth estimation performance on each partition. As we adopt our depth model from _Monodepth2_ and _LiteMono_, in Table 2, we compare our approach against the respective baseline with the same depth architecture. For simplicity, we report Abs Rel and \(<1.25\). Notably, by explicitly modeling

    & _IM Sem_ & \(D\) &  &  \\   & & Abs Rel & Sq Rel & RMSE & RMSE log & \(<1.25\) & \(<1.25^{2}\) & \(<1.25^{3}\) \\  Monodepth2  & ✗ & K & 0.115 & 0.882 & 4.701 & 0.190 & 0.879 & 0.961 & 0.982 \\ LiteMono  & ✗ & K & **0.101** & 0.729 & **4.454** & **0.178** & **0.897** & **0.965** & 0.983 \\ Struct2Depth  & m & K & 0.141 & 1.026 & 5.290 & 0.215 & 0.816 & 0.945 & 0.979 \\ Dyna-DM  & m & K & 0.115 & 0.785 & 4.698 & 0.192 & 0.871 & 0.959 & 0.982 \\ Lee _et al._ & b & K & 0.114 & 0.876 & 4.715 & 0.191 & 0.872 & 0.955 & 0.981 \\ SGDepth  & m & K & 0.113 & 0.835 & 4.693 & 0.191 & 0.879 & 0.961 & 0.981 \\ Boulahbal _et al._ & m & K & 0.110 & 0.719 & 4.486 & 0.184 & 0.878 & 0.964 & **0.984** \\ GeoNet  & & K & 0.155 & 1.296 & 5.857 & 0.233 & 0.793 & 0.931 & 0.973 \\ EPC++  & & K & 0.141 & 1.029 & 5.350 & 0.216 & 0.816 & 0.941 & 0.976 \\ CC  & & K & 0.140 & 1.070 & 5.326 & 0.217 & 0.826 & 0.941 & 0.975 \\ TrianFlow  & & K & 0.113 & 0.704 & 4.581 & 0.184 & 0.871 & 0.961 & **0.984** \\ Li _et al._ & & K & 0.130 & 0.950 & 5.138 & 0.209 & 0.843 & 0.948 & 0.978 \\ RM-Depth  & & K & 0.107 & **0.687** & 4.476 & 0.181 & 0.883 & 0.964 & **0.984** \\
**Dynamo-Depth** (MD2) & K & 0.120 & 0.864 & 4.850 & 0.195 & 0.858 & 0.956 & 0.982 \\
**Dynamo-Depth** & & K & 0.112 & 0.758 & 4.505 & 0.183 & 0.873 & 0.959 & **0.984** \\  Monodepth2\({}^{}\) & ✗ & N & 0.425 & 16.592 & 10.040 & 0.402 & 0.723 & 0.837 & 0.887 \\ LiteMono\({}^{}\) & ✗ & N & 0.419 & 15.578 & 9.807 & 0.449 & 0.720 & 0.831 & 0.879 \\
**Dynamo-Depth** (MD2) & & N & 0.193 & 2.285 & 7.357 & 0.287 & 0.765 & 0.885 & 0.935 \\
**Dynamo-Depth** & & N & **0.179** & **2.118** & **7.050** & **0.271** & **0.787** & **0.896** & **0.940** \\  Monodepth2\({}^{}\) & ✗ & W & 0.173 & 2.731 & 7.708 & 0.227 & 0.797 & 0.930 & 0.968 \\ LiteMono\({}^{}\) & ✗ & W & 0.158 & 2.305 & 7.394 & 0.210 & 0.816 & 0.944 & 0.976 \\ Struct2Depth  & m & W & 0.180 & 1.782 & 8.583 & 0.244 & - & - & - \\ Li _et al._ & & m & W & 0.157 & 1.531 & 7.090 & 0.205 & - & - & - \\ Lee _et al._ & b & W & 0.148 & 1.686 & 7.420 & 0.210 & - & - & - \\ Li _et al._ & & W & 0.162 & 1.711 & 7.833 & 0.223 & - & - & - \\
**Dynamo-Depth** (MD2) & & W & 0.130 & 1.439 & 6.646 & 0.183 & 0.851 & 0.959 & 0.985 \\
**Dynamo-Depth** & & W & **0.116** & **1.156** & **6.000** & **0.166** & **0.878** & **0.969** & **0.989** \\   

Table 1: Depth evaluation on the KITTI (K), nuScenes (N), and Waymo Open (W) Dataset. _IM_ stands for independent motion where \(\ ✗\) denotes a lack of independent motion modeling. _Sem_ indicates the amount of semantic information given during training, where ‘m’ indicates mask-level supervision and ‘b’ indicates box-level supervision. Manual replication with released code is indicated by \({}^{}\).

independent motion, we observe a consistent and significant improvement on moving objects (_M.O._) across both architectures and both datasets, with over \(48\%\) relative improvement in accuracy and over \(67\%\) relative reduction in error for both architectures on Waymo Open. Additionally, we see a substantial improvement on both static moveable objects (_S.O._) and static background (_S.B._) for both datasets and both architectures. Interestingly, the large reduction in error on static background in nuScenes does not match the corresponding improvement in accuracy, as the large errors on background mainly come from night-time instances where all methods fail, but differ in their respective artifacts (discussed further in Appendix D.2).

    &  &  &  \\   & & _All_ & _S.B._ & _S.O._ & _M.O._ & _All_ & _S.B._ & _S.O._ & _M.O._ \\  Monodepth2\({}^{}\) & N & 0.425 & 0.447 & 0.232 & 0.418 & 0.723 & 0.735 & 0.704 & 0.570 \\ Dynamo-Depth (MD2) & N & 0.193 & 0.196 & 0.196 & 0.228 & 0.765 & 0.761 & 0.759 & 0.684 \\ \(\)**(\%)** & & **-54.6** & **-56.2** & **-15.5** & **-45.5** & **+5.8** & **+3.5** & **+7.8** & **+20.0** \\  & N & 0.419 & 0.431 & 0.248 & 0.502 & 0.720 & 0.734 & 0.718 & 0.517 \\ Dynamo-Depth & N & 0.179 & 0.184 & 0.182 & 0.198 & 0.787 & 0.781 & 0.774 & 0.753 \\ \(\)**(\%)** & & **-57.3** & **-57.3** & **-26.6** & **-60.6** & **+9.3** & **+6.4** & **+7.8** & **+45.6** \\  Monodepth2\({}^{}\) & W & 0.173 & 0.152 & 0.215 & 0.749 & 0.797 & 0.810 & 0.683 & 0.416 \\ Dynamo-Depth (MD2) & W & 0.130 & 0.122 & 0.175 & 0.234 & 0.851 & 0.862 & 0.778 & 0.674 \\ \(\)**(\%)** & & **-24.9** & **-19.7** & **-18.6** & **-68.8** & **+6.8** & **+6.4** & **+13.9** & **+62.0** \\  & W & 0.158 & 0.140 & 0.179 & 0.599 & 0.816 & 0.827 & 0.733 & 0.506 \\ Dynamo-Depth & W & 0.116 & 0.110 & 0.155 & 0.194 & 0.878 & 0.891 & 0.812 & 0.750 \\ \(\)**(\%)** & & **-26.6** & **-21.4** & **-13.4** & **-67.6** & **+7.6** & **+7.7** & **+10.8** & **+48.2** \\   

Table 2: Depth evaluation with semantic split on the nuScenes (N) and Waymo Open (W) Dataset. _S.B._, _S.O._ and _M.O._ denotes the partition of pixels that are static background, static moveable object and moving object, respectively. Manual replication with released code is indicated by \({}^{}\).

Figure 3: Qualitative results of our proposed approach that learns from (a) unlabeled video sequences and predicts (b) monocular depth, (c) rigid flow, (d) independent flow, and (e) motion segmentation. 3

In sum, by explicitly modeling independent motion, we are able to outperform the respective baselines and achieve state-of-the-art performances on both Waymo Open and nuScenes Dataset. Qualitative results are found in Figure 1 and Figure 3, where "Static Scene Constraint" refers to performance of _LiteMono_ in Figure 1.

### Motion Segmentation and Method Ablation

Figure 4 evaluates the quality of the jointly learned binary motion mask network using precision and recall. We observe a high precision with increasing recall, achieving a F1 score of \(71.8\%\) on Waymo Open . Note that this segmentation quality is achieved without any labels. On nuScenes, Dynamo-Depth achieves a F1 score of \(42.8\%\) with \(57.2\%\) on day-clear conditions and \(21.1\%\) on all other conditions. Notably, the motion mask network is able to segment out small dynamical objects, such as the cyclist in the first row of Figure 3.

In addition, we show our ablation results on Waymo Open in Table 3. First, we found that ablating the motion consistency loss removes the regularization on the complete flow prediction, which causes the degenerate solution of moving objects on a flat canvas, as indicated by the poor segmentation performance in Figure 4. Similarly, by ablating motion initialization, initial independent motion prediction lead to large reconstruction errors, which diverges depth training. In addition, we observe that ablating the above ground prior preserves the performance of motion segmentation and depth of moving objects, but degrades the performance of depth on static background. Intuitively, this prior regularizes the structural integrity of the ground plane. Finally, we observed that the original set of hyperparameter values leads to training divergence when ablating the ImageNet pretrained weights. To compensate, we increase the initial number of epochs for depth learning from 1 to 2 (marked as \(}^{}\)) without performing any hyperparameter sweeps.

## 6 Conclusion

In this work, we identify the effects of dynamical objects in unsupervised monocular depth estimation with static scene constraint. To mitigate the negative impacts, we propose to jointly learn depth, ego-motion, 3D independent motion and motion segmentation from unlabeled videos. Our key insight is that a good initial estimation of motion segmentation encourages joint depth and independent motion learning and prevents degenerate solutions from arising. As a result, our approach is able to achieve state-of-the-art performance on both Waymo Open and nuScenes Dataset.

**Limitations and Societal Impact.** Our work makes a brightness consistency assumption, where the brightness of a pixel will remain the same. Although common in optical flow, this assumption limits the our method's ability to model scenes with dynamical shadows, multiple moving light sources, and lighting phenomenons (e.g., lens flare). In addition, our work does not introduce any foreseeable societal impacts, but will generally promote more label-efficient and robust computer vision models.

   \)} &  Motion \\ Initialization \\  } & \)} &  &  &  \\   & & & & _All_ & _S.B._ & _S.O._ & _M.O._ & _All_ & _S.B._ & _S.O._ & _M.O._ \\  ✗ & & & & 0.525 & 0.489 & 0.625 & 0.447 & 0.254 & 0.274 & 0.210 & 0.325 \\  & ✗ & & & 0.522 & 0.487 & 0.621 & 0.444 & 0.256 & 0.275 & 0.212 & 0.326 \\  & & ✗ & & 0.266 & 0.237 & 0.167 & 0.211 & 0.816 & 0.844 & 0.792 & 0.743 \\  & & & ✗\({}^{}\) & 0.136 & 0.132 & 0.177 & 0.201 & 0.833 & 0.839 & 0.778 & 0.730 \\  & & & & 0.116 & 0.110 & 0.155 & 0.194 & 0.878 & 0.891 & 0.812 & 0.750 \\   

Table 3: Depth ablation results on the Waymo Open Dataset with Dynamo-Depth.

Figure 4: Precision-Recall curve for motion segmentation on the Waymo Open Dataset.

Acknowledgement

This research is based upon work supported in part by the National Science Foundation (IIS-2144117 and IIS-2107161). Yihong Sun is supported in part by an NSF graduate research fellowship.