# Predictor-Corrector Enhanced Transformers with Exponential Moving Average Coefficient Learning

Bei Li\({}^{1,2}\) Tong Zheng\({}^{1}\) Rui Wang\({}^{3}\) Jiahao Liu\({}^{2}\) Qingyan Guo\({}^{4}\) Junliang Guo\({}^{3}\) Xu Tan\({}^{3}\) Tong Xiao\({}^{1}\) Jingbo Zhu\({}^{1}\) Jingang Wang\({}^{2}\) Xunliang Cai\({}^{2}\)

\({}^{1}\)Northeastern University \({}^{2}\)Meituan Inc. \({}^{3}\)Microsoft Research \({}^{4}\)Tsinghua University {libei17,liujiahao12,wangjingang02,caixunliang}@meituan.com

tzheng24@umd.edu {ruiwa,junliangguo,xuta}@microsoft.com

ggy22@mails.tsinghua.edu.cn {xiaotong,zhujingbo}@mail.neu.edu.cn

Part of work was done during an internship at Microsoft Research AsiaCorresponding Author.

###### Abstract

Residual networks, as discrete approximations of Ordinary Differential Equations (ODEs), have inspired significant advancements in neural network design, including multistep methods, high-order methods, and multi-particle dynamical systems. The precision of the solution to ODEs significantly affects parameter optimization, thereby impacting model performance. In this work, we present a series of advanced explorations of Transformer architecture design to minimize the error compared to the true "solution." First, we introduce a predictor-corrector learning framework to minimize truncation errors, which consists of a high-order predictor and a multistep corrector. Second, we propose an exponential moving average-based coefficient learning method to strengthen our higher-order predictor. Extensive experiments on large-scale machine translation, abstractive summarization, language modeling, and natural language understanding benchmarks demonstrate the superiority of our approach. On the WMT'14 English-German and English-French tasks, our model achieved BLEU scores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual machine translation task, our model surpasses a robust 3.8B DeepNet by an average of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats LLama models by 5.7 accuracy points on the LM Harness Evaluation.

## 1 Introduction

Residual networks , formally \(y_{t+1}=y_{t}+(y_{t},_{t})\), represent a cornerstone in the development of deep neural networks [59; 10], primarily due to their capacity to facilitate the flow of information across multiple layers. Beyond their pivotal role in convolutional networks, residual connections have become an essential element in the architecture of more complex models, including the Transformer  and its various derivatives. This concept can be likened to the discretization process in the Euler method [63; 37; 15; 7; 51; 29], which serves as a first-order solver for ordinary differential equations (ODEs), where \(y(t)}{t}=(y(t),(t))\). In both cases, the new state (be it the next layer's output in ResNets or the solution at the next time step in the Euler method) is computed by taking the current state and adding an adjustment term.

Given this analogy with ODEs, there has been a surge of interest in improving residual network architectures by using more powerful numerical methods for ODEs. For instance, the linear multistep method [62; 37; 71; 24] has been employed to bolster the optimization of deep models. Other efforts have included redesigning the Transformer architecture from a multi-particle dynamical systemperspective [38; 12] and improving parameter learning efficiency through high-order methods . Additionally, ODEs have been extensively studied for their potential to accelerate diffusion processes, with multistep and high-order solvers offering more accurate predicted noise among each denoising process, generating comparable images but consuming much fewer NFEs [31; 35; 36].

In this work, we mainly focus on advancing the architecture design, specifically by minimizing the truncation error across each timestep. Building upon the ODE Transformers , which replace the first-order Euler method with a high-order method for more precise numerical solutions, our focus extends to addressing two key limitations. First, high-order solutions, such as those from the Runge-Kutta method or multistep methods, are found not to lead to significant improvements when we scale up training data and/or model size. Second, the gated fusion coefficient learning method, which is widely used in previous work, is not well suited for higher-order solutions.

Our work draws inspiration from the predictor-corrector method , a well-established approach in numerical analysis known for its accuracy in solving differential equations. This method involves a two-step process: a prediction step that estimates the solution based on known conditions, followed by a correction step that refines the prediction for a more accurate result. We introduce a novel family of PCformers that embrace this predictor-corrector paradigm. Our approach integrates the final solution using an exponential moving average (EMA) method, capitalizing on the insight that _higher-order intermediate approximations tend to be more accurate_. This assertion is supported by the truncation error analysis presented in Section 3.1.2. Our method is not only readily extensible to arbitrary higher orders but also consistently outperforms the gated fusion method.

Our contributions are summarized below:

* We extend explicit ODE solutions to implicit ODE solutions via a predictor-corrector learning paradigm. This kind of iterative refinement can attain more accurate solutions than previous studies both theoretically and empirically. In particular, we choose the high-order method as the predictor and the multistep method as the corrector.
* To further strengthen the learning ability and training stability for high-order methods, we propose an exponential moving average coefficient learning method to replace the constant coefficients. This leads to a much stronger predictor.
* Our extensive experimental evaluation on several benchmarks, including WMT'14 English-German, WMT'14 English-French, WMT'16 Romanian-English, and the OPUS multilingual machine translation benchmark, demonstrates the superior effectiveness of our PCformer models. Notably, our model surpasses the 3.8B DeepNet by an average BLEU score of 2.9 with only 1/3 of the parameters. Furthermore, our model can be extended to other domains. Results on abstractive summarization, language modeling, and language understanding tasks demonstrate its generality.

## 2 Background

We build our method upon Transformer  as it is one of the most popular models in NLP. The encoder is a stack of identical layers. Each layer consists of a self-attention block and a feedforward network (FFN) block. Both of them are equipped with a residual connection  and a layer normalization unit . The output of a block can be defined as

\[y_{t+1}=y_{t}+(y_{t},_{t})\] (1)

where \(()\) is either the self-attention or FFN block. This equation illustrates that the layer output \(y_{t+1}\) is determined by the layer input \(y_{t}\) and a learnable derivative estimated by the current function \(\). This approach aligns with the benefits of the Euler method, which we will discuss in the following sections. In this work, we use \(F_{t}\) to denote the \(t\)-th layer representation and \(_{i}\) to denote the \(i\)-th order intermediate approximations.

The Euler MethodThe Euler method is the most basic solution to solve ODEs given the initial value, involving a function \(y(t)\) of a variable \(t\) and its derivatives. The Euler method defines the first-order derivative of \(y(t)\)

\[y(t)}{t}=f(y(t),t)\] (2)

where \(f(y(t),t)\) defines a time-dependent vector field if we know its value at all points of \(y\) and all instants of time \(t\). Eq. 2 illustrates that the change of a variable is determined by its current value and a time variable \(t\). In deep learning, we can use a trainable function \(()\) to estimate \(f(y(t),t)\). In a nutshell, residual networks could be regarded as a 1st-order discretization of the Euler method [63; 29]. The advantage is obvious since residual networks deliver consistent performance gains in the artificial intelligence, but the precision of \(y_{t+1}\) is limited. Fortunately, we can move forward along the numerical analysis perspective as more advanced numerical methods can alleviate this issue.

The Linear Multistep MethodCompared with the Euler method, the linear multistep method uses previously obtained "solutions" to estimate the current one, leading to more accurate results. Formally, a multistep method could be defined as: \(y_{t+1}=y_{t}+_{i=1}^{t}_{i}F_{i}\), where \(F_{t}=(y_{t},_{t})\).

The High-Order ODE MethodAnother family of numerical methods is high-order ODE solvers by repeatedly refining the solutions within a single step. Previous work  employed the Runge-Kutta methods [50; 23; 6; 2] for a higher-order solution to ODEs, where Runge-Kutta is a classic family of iterative methods with different orders of precision. More formally, the explicit Runge-Kutta methods of an \(n\)-order solution is defined to be: \(y_{t+1}=y_{t}+_{i=1}^{n}_{i}_{i}\), where \(_{1}=(y_{t},_{t})\), \(_{i}=(y_{t}+_{j=1}^{i-1}_{ij}_{j},_{t})\). Note that \(_{i}\) is the intermediate approximation to the solution at an inner step. \(\) and \(\) are coefficients to model the scale of the input and the output of \(_{i}\). This kind of method can be adapted to Transformer blocks by reusing \(()\) within a block.

## 3 Predictor-Corrector Transformer

In this section, we first show the core design of Predictor-Corrector paradigm to more accurately solve ODEs. Then we propose an alternative coefficient learning strategy that could be applied to arbitrary orders using the merit of the exponential moving average. At last, we show some additional training techniques for stable and well-performance training.

### Predictor-Corrector Method

A genuine problem-solving process involves the repeated use of available information to initiate exploration, which discloses, in turn, more information until a way to attain the solution is finally discovered. - Newell et al. 

The Predictor-Corrector framework leverages an iterative process of using available information to refine approximations continuously. Initially, the Predictor generates a rough estimate, which is subsequently refined by the Corrector using newly available data. This cyclical process mirrors the

Figure 1: Illustration of several advanced numerical methods and our proposed predictor-corrector paradigm. The right part plots a 4-order method as the predictor to obtain \(P_{t+1}\); \(F_{t+1}\) is then estimated via a function \(()\); A 4-step method as the corrector to obtain the \(y_{t+1}\).

problem-solving strategy described earlier, where each iteration uncovers additional information that enhances the final solution's accuracy.

#### 3.1.1 Adams-Bashforth-Moulton Methods

A predictor-corrector method typically uses an explicit method for the predictor and an implicit method for the corrector. Here we take the 4-step Adams-Bashforth-Moulton  methods as an instance, where Adams-Bashforth is the predictor and Adams-Moulton is the corrector. Adams-Bashforth is a 4-step method, which defined as

\[y_{t+1}=y_{t}+(55F_{t}-59F_{t-1}+37F_{t-2}-9F_{t-3}),\] (3)

where \(F_{t}=(y_{t},_{t})\). \(()\) denotes the \(t\)-th function, and \(_{t}\) is corresponding parameters. Obviously, the Adams-Bashfroth methods are explicit since \(y_{t+1}\) only depends on the "observed" statistics (\(F_{i t}\)). Similarly, Adams-Moulton is also a 4-step method as below

\[y_{t+1}=y_{t}+(9F_{t+1}+19F_{t}-5F_{t-1}+F_{t-2}).\] (4)

Formally, both Eq. 3 and Eq. 4 reused \(F_{t}\), \(F_{t-1}\), \(F_{t-2}\) to improve the accuracy, but the corrector necessitates an approximate current "solution" \(F_{t+1}\) to substitute \(F_{t-3}\), which is an implicit method. This is because that \(y_{t+1}\) is the value to be solved, thus we cannot compute \(F_{t+1}\). To solve this, the Adams-Bashforth-Moulton methods utilize Eq. 3 to obtain the approximate value (\(P_{t+1}\)) for \(y_{t+1}\). Then \(F_{t+1}\) could be approximated following \(F_{t+1}=(P_{t+1},_{t})\). Concretely, the predictor provides a rough approximation, which is the combination of the preceding four layer representations. And the corrector then improves the approximation, offering a more precise sample derived from the data.

However, applying the Adams-Bashforth-Moulton method directly to Transformer architecture design leads to unstable training and limited benefits due to the difficulty in optimizing constant coefficients. Similar issues have been observed in training a Runge-Kutta (RK4) network with numerically suggested coefficients . To address these challenges, Wang et al.  proposed a Dynamic Linear Combination of Layers (DLCL) method. This approach utilizes learnable coefficients and adjusts steps based on layer depth, effectively transforming it into a variable multistep method. Additionally, the Adams-Bashforth method is not the only choice for the predictor; other numerical methods, such as high-order methods, are also considered strong alternatives as they often provide more accurate solutions .

#### 3.1.2 High-order Predictor and Multistep Corrector

The aforementioned discussion motivates us to design a more powerful and stable architecture based on the above principles. Our preliminary experiments indicate that more accurate predictors indeed improve the performance, thus we choose a high-order method to serve as the predictor. A 2-order method could be defined as

\[y_{t+1}=y_{t}+(_{1}+_{2})\] (5)

where \(_{1}=(y_{t},_{t})\), and \(_{2}=(y_{t}+_{1},_{t})\). Rather than utilizing the previously obtained representations in multistep methods, high-order methods iteratively estimate the approximations upon the last timestep. Similarly, a 4-order method is:

\[y_{t+1}=y_{t}+(_{1}+2_{2}+2_{3}+_{4})\] (6)

where \(_{1}=(y_{t},_{t})\), \(_{2}=(y_{t}+_{1},_{t})\), \(_{3}=(y_{t}+_{2},_{t})\), and \(_{4}=(y_{t}+_{3},_{t})\). To break the limit of constant coefficients, Li et al.  employed a gated network to dynamically compute the coefficients of \(_{1}\) and \(_{2}\), however, this method cannot applied to higher-order methods, e.g., RK4. To facilitate higher-order optimization, we design a more flexible coefficient learning method via an exponential moving average strategy.

Predictor with Exponential Moving Average Coefficient LearningThe Exponential Moving Average (EMA) method  is widely used for estimating time-series data by assigning variable weights to past observations, giving more importance to recent data compared to simple weighted averaging methods. We hypothesize that high-order approximations at each step should have a largerimpact on the final output, as they provide a more accurate initial state than previous ones. To support this claim, we replaced Eq. 6 by \(y_{t+1}=y_{t}+_{i}\), where \(i[1,...,4]\). We used a single-layer decoder to compute the perplexity (PPL) on the validation set to simulate truncation errors. Our expectation is that _the fewer truncation errors, the larger the coefficient it should own_.

Figure 2 displays the perplexity comparisons. It shows that a 4-th order approximation, used as a replacement for the linear aggregation in Eq. 6, delivers comparable results and outperforms other cases. This observation motivates us to combine the benefits of EMA with the coefficient learning method. EMA is more flexible to the order of ODE solvers, that is could be easily extended to 2 orders, 4 orders, or even larger. Figure 1(b) illustrates the design merit of our proposed PCformer with an EMA coefficient predictor and a parameterization multistep corrector. Here, we use the RK4-block as an example. It is apparent that the original scales have been replaced by \(\), \((1-)\), \((1-)^{2}\), and \((1-)^{3}\) from \(_{4}\) to \(_{1}\), where \(\) is learnable and the initialization is 0.5 empirically. In this way, our \(n\)-order predictor approximates \(P_{t+1}\) as follows:

\[P_{t+1}=y_{t}+_{i=1}^{n}(1-)^{n-i}_{i}.\] (7)

Corrector with ParameterizationLeveraging a robust predictor, our corrector is designed to be computationally lightweight, striking an optimal balance between performance and efficiency. Utilizing the Adams-Moulton method, we parameterize the coefficients of previous states with learnable parameters. These coefficients are initialized using an EMA value, where the newly estimated \(F_{t+1}\) is assigned a larger weight (\(=0.5\)), and the weights of previous states decrease in a descending order. In this way, we rewrite the Eq. 4 by

\[y_{t+1}=y_{t}+(P_{t+1},_{t})+_{i=t-2}^{t} (1-)^{t-i+1} F_{i}.\] (8)

where \(P_{t+1}\) is obtained with Eq. 7. Empirically, we found that when the dataset is limited, an Backward Euler method  as the corrector is enough to provide precise correction, where \(y_{t+1}=y_{t}+(P_{t+1},_{t})\). We will discuss this in the analysis for more insights.

### Improving Training Stability

Step Normalization (RK-Norm)We built our PCformer following pre-norm architecture , by rewritten Eq. 1 to \(y_{t+1}=y_{t}+((y_{t}),_{t})\), where \(()\) denotes the normalization. This ensures that the representation is normalized before computing the derivative \(F_{i}\). To achieve this, we normalize the obtained intermediate approximations \(_{i}\) at each inner step and then compute the offset, e.g., \(y_{t}+(_{1})\) to obtain the \(_{2}\) for the next timestep. Meanwhile, the \(_{i}\) in Eq. 7 is rewritten by \((_{i})\). If not, this oversight can cause instability when computing the final ODE solution, where we will make ablations in the analysis section. The algorithm (right part) presents a more detailed computation flow of a single layer in our PCformer, where \(\) stores the previously obtained \(F_{t+1}\).

```
1:procedurePredictorCorrector(\(}\), \(\))
2:\(\)\(\) Initialize an empty list to store \(_{i}\)
3:for\(i 1\)to 4do
4:if\(i==1\)then
5:\(_{1}(},_{t})\)\(\) Compute \(_{1}\)
6:else\(}_{1}(},[ ],_{t})\)\(\) Compute \(_{i}\)
7:endif
8:\((}_{1})(}_{ 1})\)\(\) Apply RK-Norm
9:\(.((}_{1}))\)\(\) Store \((}_{1})\)
10:endfor
11:Compute \(P_{t+1}\) using \(\) via Eq. 7\(\) Predictor
12:\(_{t+1}(P_{t+1})\)\(\) Compute \(F_{t+1}\)
13: Compute \(y_{t+1}\) using \(\) via Eq. 8\(\) Corrector
14:\(.(_{t+1})\)\(\) Store \(F_{t+1}\)
15:return\(}\)\(\) Return the layer output
16:endprocedure ```

**Algorithm 1** Predictor-Corrector Paradigm

Step Normalization (RK-Norm)We built our PCformer following pre-norm architecture , by rewritten Eq. 1 to \(y_{t+1}=y_{t}+((y_{t}),_{t})\), where \(()\) denotes the normalization. This ensures that the representation is normalized before computing the derivative \(F_{i}\). To achieve this, we normalize the obtained intermediate approximations \(_{i}\) at each inner step and then compute the offset, e.g., \(y_{t}+(_{1})\) to obtain the \(_{2}\) for the next timestep. Meanwhile, the \(_{i}\) in Eq. 7 is rewritten by \((_{i})\). If not, this oversight can cause instability when computing the final ODE solution, where we will make ablations in the analysis section. The algorithm (right part) presents a more detailed computation flow of a single layer in our PCformer, where \(\) stores the previously obtained \(F_{t+1}\).

Figure 2: Truncation errors with different intermediate approximations.

#### 3.3.2 Sublayer Dropping

Additionally, we observe that our models benefit from the rich information brought by high-order predictor and subsequent implicit multistep corrector. To prevent from overfitting (settling into sub-optimal solutions) as the learning ability is quite strong, we borrowed the sublayer dropping technique [28; 34]. The drop rate is empirically set as 0.1 which delivers robust results in previous studies.

## 4 Experimental Results

We mainly evaluated the proposed method on machine translation, abstractive summarization, language modeling, and language understanding benchmarks. The details of datasets, and corresponding hyper-parameters please refer to Appendix C. For a clear comprehension, note that RK2-block (gated) is 2-order method with learnable coefficients in 's work. And RK2-block (EMA) denotes our EMA strategy.

#### 4.0.1 Results of En-De and En-Fr

Table 1 compares the proposed PCformer with state-of-the-art systems in base and large configurations. As ODE Transformer is a strong baseline to ours, we implemented their results for a fair comparison. We can see that the proposed EMA coefficient learning method can further strengthen high-order methods, leading to better results than the gated fusion method in Li et al. 's work (comparisons in RK2-block). And EMA can facilitate RK4-block to deliver a further gain of 0.40 BLEU points. The performance gains are more obvious for wider models, that PCformer sets or matches the new state-of-the-art with fewer parameters. Notably, a 6-layer PCformer (2-order) achieves a BLEU score of 30.90, surpassing the previous best of 30.77 by a 12-layer RK2-block with gated fusion . For En-Fr, PCformer outperforms the standard Big model

    &  &  &  \\   & & **\#Param** & **Steps** & **BLEU** & **SBLEU** & **\#Param** & **Steps** & **BLEU** & **SBLEU** \\  Transformer  & 6-6 & 213M & 100K & 28.40 & - & 222M & 300K & 41.00 & - \\ MacaronNet  & 6-6 & - & - & - & 30.20 & - & - & - & - \\ Transformer-DLCL  & 30-6 & 137M & 50K & 29.30 & 28.6 & - & - & - & - \\  Transformer-Base & 6-6 & 61M & 50K & 27.89 & 26.8 & 69M & 100K & 41.05 & 39.1 \\ RK2-block (Gated)  & 6-6 & 61M & 50K & 28.89 & 27.7 & 69M & 100K & 42.31 & 40.3 \\ RK2-block (EMA) & 6-6 & 61M & 50K & 29.11 & 28.1 & 69M & 100K & 42.44 & 40.4 \\ RK4-block  & 6-6 & 61M & 50K & 29.03 & 27.9 & 69M & 100K & 42.56 & 40.6 \\ RK4-block (EMA) & 6-6 & 61M & 50K & 29.43 & 28.4 & 69M & 100K & 42.72 & 40.7 \\  Transformer-Big & 6-6 & 211M & 100K & 29.21 & 28.1 & 221M & 100K & 42.89 & 40.9 \\ RK2-block (Gated)  & 6-6 & 211M & 100K & 30.53 & 29.4 & 221M & 100K & 43.59 & 41.6 \\ RK4-block  & 6-6 & 211M & 100K & 30.39 & 29.3 & 221M & 100K & 43.51 & 41.6 \\ PCformer (2-order) & 6-6 & 211M & 100K & 30.90 & 29.8 & 221M & 100K & 43.85 & 41.8 \\ Transformer-Big & 12-6 & 286M & 100K & 29.91 & 28.9 & 297M & 100K & 43.22 & 41.2 \\ RK2-block (Gated)  & 12-6 & 286M & 100K & 30.77 & 29.6 & 297M & 100K & 43.96 & 42.1 \\ RK4-block  & 12-6 & 286M & 100K & 30.55 & 29.4 & 297M & 100K & 43.81 & 41.8 \\ RK4-block (EMA) & 12-6 & 286M & 100K & 30.66 & 29.5 & 297M & 100K & 44.17 & 42.2 \\ PCformer (2-order) & 12-6 & 286M & 100K & **30.95** & **29.8** & 297M & 100K & **44.27** & **42.4** \\   

Table 1: Comparison with the state-of-the-arts on the WMT En-De and WMT En-Fr tasks. We both report the tokenized BLEU and SacreBLEU scores for comparison with previous work.

  
**Models** & **Layers** & **Hidden** & **Params** & \(\) & \(\) & **Avg** \\   & 200 & 512 & 863M & 33.2 & 29.0 & 31.1 \\  & 1000 & 512 & 3.8B & 33.9 & 30.2 & 32.1 \\   & 200 & 512 & 863M & 34.2 & 28.5 & 31.4 \\  & 1000 & 512 & 3.8B & 35.0 & 29.6 & 32.3 \\   & 12 & 1024 & 466M & 34.0 & 27.6 & 30.8 \\  & 24 & 1024 & 618M & 34.9 & 28.1 & 31.5 \\   & 12 & 1024 & 466M & 36.0 & 29.1 & 32.6 \\  & 24 & 1024 & 618M & 36.9 & 30.5 & 33.7 \\   & 24 & 1536 & 1.2B & **37.7** & **32.2** & **35.0** \\   

Table 3: Average SacreBLEU on the OPUS-100.

by 1.00 and 1.05 BLEU points with 2-order and 4-order configurations. This demonstrates that the predictor-corrector paradigm is a more parameter-efficient option than pure high-order methods.

Results of En-RoTable 2 exhibits a similar phenomenon on the En-Ro task. Our predictor-corrector paradigm with EMA method achieves much better performance (35.49 _v.s._ 34.70) with DeLight within much less training cost. For a bigger model (line 7), it obtains a BLEU score of 35.80. A much higher performance (36.00) could be achieved by a carefully designed corrector which would be discussed in the subsequent analyses.

Results of OPUSTable 3 provides the comparison of PCformer against existing state-of-the-art models [69; 61] on the OPUS-100 testset. The findings here are three aspects: 1) Across all configurations, PCformer delivers significant BLEU gains over vanilla baselines. 2) Our 12-layer EMA Pre-Cor model attains an average SacreBLEU score of 32.6, which not only outperforms the 3.8B DeepNet but also beats its further optimized variant, BranchNorm, with only 1/8 model parameters. 3) PCformer can benefit from the enlarging width and depth. Notably, our 1.2B model shows an average SacreBLEU of 35.0, thereby setting a new state-of-the-art on the OPUS-100 testset.

Abstractive SummarizationTable 4 presents the results of the abstractive summarization task. As shown, our PCformer consistently improves upon pure the high-order method , in terms of three rouge scores. Notably, PCformer (2-order) even beats RK4-block which consumes less computation cost. Additionally, PCformer (4-order) sets a new state-of-the-art on the summarization task which excludes models based on pre-trained models. This result strongly supports our hypothesis that an appropriate coefficient learning schedule is essential for the effectiveness of higher-order methods, thereby enhancing the performance of our PCformer model.

Language ModelingTable 5 presents a comparative analysis of our PCformer against vanilla Transformers in Adaptive Input Representation  and Shortformer  settings. Our 2nd-order configuration achieves significant reductions in perplexity (PPL), outperforming Adaptive and Shortformer by 1.79 and 1.23 PPL, respectively, even within identical model capacity constraints. Remarkably, PCformer surpasses the high-order method (RK2-block) by a substantial margin in both settings on both validation and test sets, demonstrating the superiority of PCformer.

  
**Model** & **Param** & **Tokens** & **Wiki.** & **LMB.** & **LMB.** & **PIQA** & **Hella.** & **SciQ** & **ARC-c** & **Wino.** & **Avg.** \\  & & & ppl \(\) & ppl \(\) & acc \(\) & acc \(\) & acc\(\_\)norm \(\) & acc \(\) & acc\(\_\)norm \(\) & acc \(\) & acc \(\) & acc \(\) \\  Transformer++ & 340M & 6B & 38.5 & 96.1 & 21.4 & 60.3 & 29.1 & 69.2 & 21.5 & 50.4 & 41.9 \\ PCformer & 340M & 6B & 35.3 & 78.8 & 23.6 & 61.6 & 30.1 & 71.6 & 22.9 & 51.8 & 43.6 \\ Transformer++ & 340M & 16B & 28.3 & 65.3 & 29.8 & 63.2 & 33.9 & 73.2 & 23.1 & 51.4 & 45.8 \\ PCformer & 340M & 16B & 25.6 & 39.7 & 34.5 & 65.2 & 36.9 & 79.6 & 23.2 & 52.2 & 48.6 \\  Transformer++ & 1.3B & 16B & 23.8 & 26.2 & 37.3 & 65.7 & 37.6 & 78.6 & 23.7 & 51.5 & 49.0 \\ PCformer & 1.3B & 16B & 20.9 & 23.2 & 42.5 & 68.3 & 43.4 & 81.5 & 25.1 & 52.4 & 52.2 \\ Transformer++ & 1.3B & 100B & 16.3 & 11.8 & 51.6 & 71.0 & 51.7 & 86.7 & 28.1 & 54.6 & 57.2 \\ PCformer & 1.3B & 50B & 16.2 & 9.4 & 55.1 & 71.9 & 54.8 & 88.6 & 29.6 & 57.2 & 59.5 \\ PCformer & 1.3B & 100B & 14.0 & 7.4 & 59.6 & 73.8 & 60.0 & 90.7 & 31.7 & 61.7 & 62.9 \\  PCformer & 3B & 50B & 13.6 & 6.5 & 62.1 & 74.4 & 61.9 & 90.6 & 32.4 & 61.9 & 63.9 \\ PCformer & 3B & 100B & **12.1** & **5.8** & **64.3** & **76.3** & **66.7** & **92.6** & **35.3** & **64.0** & **66.5** \\   

Table 6: PCformer results against Transformer++  on various configurations. All models are trained on the same subset of the SlimPajama dataset (from 6B to 100B) with the Mistral tokenizer . The last column shows the average over all benchmarks that use (normalized) accuracy as the metric.

  
**Model** & **RG-1** & **RG-2** & **RG-L** \\  Surface Connection  & 41.00 & 18.30 & 37.90 \\ Transformer  & 40.47 & 17.73 & 37.29 \\ RK2-block (gated)  & 41.58 & 18.57 & 38.41 \\ PCformer (2-order) & 41.96 & 18.99 & 38.74 \\ RK4-block  & 41.83 & 18.84 & 38.68 \\ PCformer (4-order) & **42.10** & **19.13** & **38.87** \\   

Table 4: ROUGE results on CNN/DailyMail summarization dataset.

  
**Model** & **Layers** & **Params** & **Valid** & **Test** \\  Adaptive  & 8L & 146M & 21.11 & 21.00 \\ RK2-block (gated)  & 8L & 146M & 20.02 & 19.98 \\ PCformer (2-order) & 8L & 146M & 19.50 & 19.21 \\  Shortformer  & 8L & 146M & 19.04 & 19.78 \\ RK2-block (gated)  & 8L & 146M & 18.67 & 19.23 \\ PCformer (2-order) & 8L & 146M & 18.01 & 18.55 \\   

Table 5: Perplexity results on Wikitext-103. Adaptive refers to Adaptive Input Transformer .

LM Evaluation HarnessIn response to the increasing significance of attention mechanisms in LLMs, we conducted a comprehensive evaluation of PCformer using established benchmarks, LM Evaluation Harness , focusing on a diverse range of downstream tasks including common-sense reasoning and question-answering. Table 6 presents our findings, where we utilized a llama-like model3, Transformer++, as the foundation for PCformer. We trained models with parameter sizes ranging from 340M to 3B, using datasets comprising 6B to 100B tokens from Slimpajama. The experimental results indicate that PCformer consistently surpasses the performance of a well-tuned Transformer of equivalent capacity. Notably, PCformer achieves an average score improvement of 1.7 points for the 340M model and 5.7 points for the 1B model across six challenging subtasks. When scaled to a 3B parameter size, PCformer demonstrates even greater gains, achieving an additional 3.5 average score improvement compared to the 1B model, underscoring its scalability and potential with larger model capacities and richer training datasets.

Language UnderstandingWe also validate our method on the widely used natural language understanding benchmarks, namely GLUE, which consists of 8 sub downstream tasks. The evaluation metrics are as follows: The result for STS-B is the Pearson correlation; Matthew's correlation is used for CoLA; Other tasks are measured by Accuracy. The results are presented in Table 7. We can see that PCformer achieves 2.1 points (on average) improvement over the BERT-large, which demonstrates the effectiveness of PCformer.

## 5 Analysis

In this section, we explore several significant issues, comprising the visualization of truncation errors, COMET results, and a set of essential ablation studies. We primarily conducted ablation studies on machine translation tasks, but the conclusions are generalizable.

Quantization of the Truncation ErrorFollowing the suggestion in 's work, we use the perplexity between the single-layer Transformer decoder output and the ground truth to approximate the "truncation error". The results of Table 8 were conducted on the Penn Treebank dataset. We see that the proposed EMA method achieves a lower perplexity than the learnable coefficient (gated) learning method, similar observation in 4-order (EMA _v.s._ Rk4-block). Additionally, the Predictor-Correct paradigm can further reduce the truncation error, which demonstrates the effectiveness of our method.

Evaluation by COMETOur PCformer consistently outperforms baselines, showing an even larger gap in COMET than BLEU, as shown in Table 9. Both metrics exhibit similar performance trends, highlighting our approach's effectiveness. Increasing model depth from 6 to 12 layers does not improve BLEU for the En-De task but results in a 0.64 COMET gain. A similar pattern is observed in the En-Fr task, where PCformer (4-order) achieves comparable BLEU to its 2-order counterpart but gains 0.24 in COMET.

    & CoLA & QQP & MNLI-m/mm & SST-2 & STS-B & QNLI & RTE & MRPC & Avg. \\  & Mcc & Acc & Acc & Acc & Corr & Acc & Acc & Acc & \\  BERT & 60.6 & 91.3 & 86.6/- & 93.2 & 90.0 & 92.3 & 70.4 & 88.0 & 84.0 \\ PCformer & 65.9 & 92.0 & 87.3/- & 93.6 & 90.8 & 92.8 & 74.7 & 91.5 & 86.1 \\   

Table 7: Comparison results on the GLUE development set.

  
**Model** & **1-Layer** & **2-Layer** \\  Residual-Block & 142.33 & 136.07 \\ RK2-block & 131.80 & 123.12 \\ RK2-block (gated)  & 128.48 & 121.02 \\ RK2-block (EMA) & 124.01 & 119.65 \\ PCformer (2-order) & 120.91 & 118.37 \\ RK4-block & 126.89 & 119.46 \\ RK4-block (EMA) & 121.82 & 116.77 \\ PCformer (4-order) & **119.27** & **114.32** \\   

Table 8: Comparison of PPL on PTB.

  
**Model** & **En-De** & **En-Fr** \\   & **BLEU** & **COMET** & **BLEU** & **COMET** \\  Transformer-big (6L) & 29.21 & 51.87 & 42.89 & 71.21 \\ PCformer (RK2) & 30.90 & 54.74 & 43.85 & 73.96 \\ PCformer (RK4) & - & - & 44.10 & 74.76 \\ Transformer-big (12L) & 29.91 & 52.90 & 43.22 & 72.33 \\ PCformer (RK2) & 30.95 & 55.38 & 44.27 & 75.09 \\ PCformer (RK4) & - & - & 44.21 & 75.33 \\   

Table 9: COMET (%) _v.s._ BLEU (%) results.

Ablation Study on Predictor-Corrector FrameworkThe predictor-corrector framework is crucial in our work, with the choice of ODE solutions for each component significantly impacting performance. We experimented with various combinations of predictors and correctors, including high-order methods, linear multistep methods, and the Backward Euler method. Table 10 summarizes these results. We chose RK2-block with EMA as the default for high-order solutions due to its performance and efficiency. Key insights include: 1) The predictor must be highly accurate, as it sets the performance lower bound. High-order predictors outperform the multistep method (DLCL) and the Euler method. 2) A complex corrector isn't always optimal; a Backward Euler method suffices for small and medium datasets (e.g., En-Ro and En-De), while more complex methods may cause overfitting. 3) Combining a multi-step method predictor with a high-order corrector performed worse than other combinations, highlighting the importance of predictor choice.

Ablation Study on Core Design TechniqueTable 11 presents the performance of our PCformer with various initial coefficients for the EMA method and stable training techniques. Our default, with \(\) set to 0.5, performs best because smaller \(\) values disrupt the numerical bound, and larger values overly focus on recent approximations. As detailed in Section 3.2, RK-Norm is essential for training stability, as shown by the BLEU score drop without it. Testing different layer-wise coefficients and replacing the learnable scalar \(\) with a learnable matrix vector showed no significant performance difference, so these were excluded from our default settings.

Inference Speed and Memory ConsumptionTable 12 presents a detailed comparison of inference speed and memory consumption across various large model configurations, revealing that the proposed PCformer models achieve satisfactory inference performance. This is primarily because the computational overhead is concentrated on the decoder side rather than the encoder, as demonstrated in our experiments. Additionally, PCformer is memory-efficient, as shown by the memory usage comparison between the baseline and the ODE Transformer. Despite the fact that our PCformer models are more than twice as slow as the vanilla baseline in encoder-only and decoder-only configurations, they deliver significantly superior performance, making the trade-off worthwhile. These performance gains are clearly illustrated in Table 6, where the substantial improvement in model effectiveness justifies the increased inference time. We acknowledge that further optimization to accelerate PCformer's inference speed is a promising direction for future research.

   Predictor & Corrector & En-De & En-Fr & En-Ro & OPUS \\  First-order Baseline & - & 29.91 & 43.22 & 34.20 & 31.5 \\ ODE Transformer & - & 30.77 & 43.96 & 35.28 & 32.3 \\ RK2-block with EMA & Multistep Method & 30.70 & **44.27** & 35.55 & **33.7** \\ RK2-block with EMA & Backward Euler Method & **30.95** & 43.68 & **36.00** & 33.2 \\ Multistep Method & Multi-step Method & 30.30 & 43.92 & 35.30 & 33.0 \\ Multistep Method & RK2-block with EMA & 29.78 & 42.68 & 34.40 & 32.5 \\ Multistep Method & Backward Euler Method & 30.30 & 43.62 & 35.27 & 32.8 \\   

Table 10: Ablation on the several choices of the predictor and corrector on four translation tasks.

   Model & \(\) & BLEU & Model & BLEU \\  Transformer-base & - & 27.89 & Transformer-big & 29.21 \\ RK4-block (EMA) & 0.25 & 28.90 & RK2-block (EMA + Pre-Cor) & 30.95 \\ RK4-block (EMA) & 0.50 & **29.43** & w/o RK-Norm & failed \\ RK4-block (EMA) & 0.75 & 28.99 & Layer-wise \(\) & 30.63 \\ RK4-block (EMA) & 0.99 & 29.20 & Vector \(\) & 30.88 \\   

Table 11: Ablations on the PCformer design on the machine translation task. The evaluation metric is BLEU (%).

   Model & **Layers Inference** & **Memory** & **BLEU** \\  Transformer & 6 & 98.7 & 13.2 & 29.2 \\ Transformer & 12 & 94.5 & 18.7 & 29.7 \\ Transformer & 24 & 87.3 & 23.5 & 29.8 \\ ODE Transformer (RK2) & 6 & 93.5 & 15.1 & 30.7 \\ PCformer (RK2 predicitor) & 6 & 90.3 & 16.2 & 30.9 \\ ODE Transformer (RK4) & 6 & 87.1 & 17.3 & 30.5 \\   

Table 12: Comparison of inference speed (sentences/s) and memory consumption (GB) between the vanilla Transformer and numerical Transformers.

More AnalysesDue to the limited space in the main content, we summarized more detailed analyses in Appendix D, including the parameter efficiency (Figure 3), illustration of training and validation curves (Figure 4), and visualization of coefficients during the learning procedure (Figure 5). We anticipate that these analyses will offer a deeper and more comprehensive understanding of our method.

## 6 Related Work

Ordinary Differential EquationsThe connection between ResNet and ODEs was first proposed by Weinan , while Neural ODENet  introduced a new perspective on neural architecture design. Several architectures [71; 24; 37; 17; 72; 38; 53] can be interpreted from the ODE perspective. Recent studies leverage ODE benefits for Transformers. Lu et al.  proposed MacaronNet using the Strang-Marchuk Splitting Scheme, and Zhang et al.  introduced continuous self-attention models. Dutta et al.  redesigned Transformer architecture for efficiency from a multi-particle dynamic system view. Li et al.  showed that first-order ODE blocks could cause error accumulation, and high-order methods were suggested as solutions. In this work, we advance the Transformer design with a more accurate Predictor-Corrector paradigm and a general coefficient learning strategy inspired by the exponential moving average, showing significant performance improvements on NLP benchmarks.

ODE and Diffusion ModelsODEs and numerical methods are also popular in diffusion models, reducing prediction errors in denoising processes. Text-to-image generation typically uses a two-stage model, including a text-to-image diffusion model and super-resolution models. The standard diffusion model, DDPM , requires up to 1000 iterations to recover images from Gaussian noise. Subsequent work accelerated DDPMs using denoising equations  or scheduled variance , though often at the cost of performance. Liu et al.  proposed treating DDPMs as solving differential equations on manifolds, introducing a pseudo linear multi-step method for efficiency and performance. Further diffusion acceleration efforts [35; 36] were motivated by ODE benefits. More recently, Xu et al.  presented a deep generative model solving the Poisson equation, opening new possibilities in text-to-image generation. ODEs also show promise in discrete diffusion models, as demonstrated by Lezama et al. , who used a predictor-corrector paradigm to enhance the accuracy.

## 7 Conclusions

This paper advances the design of parameter-efficient neural network backbones through a numerical analysis perspective. Previous work has utilized high-order ODE solutions for more accurate approximations at each block, yielding promising results on various sequence generation tasks. However, challenges remain, such as the scalability of learnable coefficients to RK4-blocks and the lack of exploration into implicit ODE methods. To address these issues, we introduce a predictor-corrector framework to improve estimation precision. Additionally, we proposed an EMA coefficients learning strategy to promote coefficients learning for high-order methods with high flexibility. Experimental results across 8 benchmarks demonstrate the general ability and strong effectiveness of our approach. More concretely, Our PCformer achieves 30.95 and 44.27 BLEU scores on the WMT'14 En-De and En-Fr, setting a new state-of-the-art result on both testsets without considering data augmentation methods. Also, it delivers an average BLEU of 35.0 on the OPUS multilingual dataset, beating DeepNet and other variants with much fewer model parameters. Notably, PCformer demonstrates strong potential in large language model scenarios, outperforming vanilla LLama models by a considerable margin across various configurations on LM Harness Evaluation. Our codebase could be found at https://github.com/libeineu/PCformer.