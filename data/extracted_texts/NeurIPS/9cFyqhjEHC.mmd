# A Flexible, Equivariant Framework for Subgraph

GNNs via Graph Products and Graph Coarsening

Guy Bar-Shalom

Computer Science

Technion - Israel Institute of Technology

guy.b@campus.technion.ac.il

&Yam Eitan

Electrical & Computer Engineering

Technion - Israel Institute of Technology

yameitan1997@gmail.com

&Fabrizio Frasca

Electrical & Computer Engineering

Technion - Israel Institute of Technology

fabrizio.frasca.effe@gmail.com

Equal contribution.

Electrical & Computer Engineering

Technion - Israel Institute of Technology

NVIDIA Research

haggaimaron@gmail.com

###### Abstract

Subgraph GNNs enhance message-passing GNNs expressivity by representing graphs as sets of subgraphs, demonstrating impressive performance across various tasks. However, their scalability is hindered by the need to process large numbers of subgraphs. While previous approaches attempted to generate smaller subsets of subgraphs through random or learnable sampling, these methods often yielded suboptimal selections or were limited to small subset sizes, ultimately compromising their effectiveness. This paper introduces a new Subgraph GNN framework to address these issues. Our approach diverges from most previous methods by associating subgraphs with node clusters rather than with individual nodes. We show that the resulting collection of subgraphs can be viewed as the product of coarsened and original graphs, unveiling a new connectivity structure on which we perform generalized message passing.

Crucially, controlling the coarsening function enables meaningful selection of any number of subgraphs. In addition, we reveal novel permutation symmetries in the resulting node feature tensor, characterize associated linear equivariant layers, and integrate them into our Subgraph GNN. We also introduce novel node marking strategies and provide a theoretical analysis of their expressive power and other key aspects of our approach. Extensive experiments on multiple graph learning benchmarks demonstrate that our method is significantly more flexible than previous approaches, as it can seamlessly handle any number of subgraphs, while consistently outperforming baseline approaches. Our code is available at https://github.com/BarSGuy/Efficient-Subgraph-GNNs.

## 1 Introduction

Subgraph GNNs  have recently emerged as a promising direction in graph neural network research, addressing the expressiveness limitations of Message Passing Neural Networks (MPNNs) . In essence, a Subgraph GNN operates on a graph by transforming it into a collection of subgraphs, generated based on a specific selection policy. Examples of such policies include removing a single node from the original graph or simply marking a node without changing the graph's original connectivity . The model then processes these subgraphs using an equivariant architecture, aggregates the derived representations, and makes graph- or node-level predictions. The growing popularity of Subgraph GNNs stems not only from their enhancedexpressive capabilities over MPNNs but also from their impressive empirical results, as notably demonstrated on well-known molecular benchmarks .

Unfortunately, Subgraph GNNs are hindered by substantial computational costs as they necessitate message-passing operations across all subgraphs within the bag. Typically, the number of subgraphs is the number of nodes in the graph, \(n\)-- for bounded degree graphs, this results in a time complexity scaling quadratically (\((n^{2})\)), in contrast to the linear complexity of a standard MPNN. This significant computational burden makes Subgraph GNNs impractical for large graphs, hindering their applicability to important tasks and widely used datasets. To overcome this challenge, various studies have explored methodologies that process only a subset of subgraphs from the bag. These methods range from simple random sampling techniques  to more advanced strategies that learn to select the most relevant subset of the bag to process . However, while random sampling of subgraphs yields subpar performance, more sophisticated learnable selection strategies also have significant limitations. Primarily, they rely on _training-time_ discrete sampling which complicates the optimization process, as evidenced by the high number of epochs required to train them . As a result, these methods often allow only a very small bag size, yielding only modest performance improvements compared to random sampling and standard MPNNs.

Our approach.The goal of this paper is to devise a Subgraph GNN architecture that can flexibly generate and process variable-sized bags, and deliver strong experimental results while sidestepping intricate and lengthy training protocols. Specifically, our approach aims to overcome the common limitation of restricting usage to a very small set of subgraphs.

Our proposed method builds upon and extends an observation made by Bar-Shalom et al. , who draw an analogy between using Subgraph GNNs and performing message-passing operations over a larger "product graph". Specifically, it was shown that when considering the maximally expressive (node-based) Subgraph GNN suggested by 2, the bag of subgraphs and its update rules can be obtained by transforming a graph through the _graph cartesian product_ of the original graph with itself, i.e., \(G G\), and then processing the resulting graph using a standard MPNN. In our approach, we propose to modify the first term of the product and replace it with a _coarsened_ version of the original graph, denoted \((G)\), obtained by mapping nodes to _super-nodes_ (e.g., by applying graph clustering, see Figure 1(left)), making the resulting product graph \((G) G\) significantly smaller. This construction is illustrated in Figure 1(right). This process effectively associates each subgraph - a row in Figure 1(right) - with a set of nodes produced by the coarsening function \(\). Different choices of \(\) allow for both flexible bag sizes and a simple, meaningful selection of the subgraphs.

While performing message passing on \((G) G\) serves as the core update rule in our architecture, we augment our message passing operations with another set of operations derived from the symmetry structure of the resulting node feature tensor, which we call _symmetry-based updates_. Specifically, our node feature tensor is indexed by pairs \((S,v)\) where \(S\) is a super-node and \(v\) is an original node. Accordingly, \(\) is a \(T n d\) tensor, where \(d\) is the feature dimension, and \(T\) is the number of super-nodes (a constant hyper-parameter). As super-nodes are sets of nodes, \(\) can also be viewed as a (very) sparse \(2^{n} n d\) tensor where \(2^{n}\) is the number of all subsets of the vertex set. Since the symmetric group \(S_{n}\) acts naturally on this representation, we use it to develop symmetry based updates.

Interestingly, we find that this node feature tensor, \(\), adheres to a specific set of symmetries, which, to the best of our knowledge, is yet unstudied in the context of machine learning: applying

Figure 1: Product graph construction. **Left:** Transforming of the graph into a coarse graph; **Right:** Cartesian product of the coarsened graph with the original graph. The vertical axis corresponds to the subgraph dimension (super-nodes), while the horizontal axis corresponds to the node dimension (nodes).

a permutation \( S_{n}\) to the nodes in \(S\) and to \(v\) results in an equivalent representation of our node feature tensor. We formally define the symmetries of this object and characterize all the affine equivariant operations in this space. We incorporate these operations into our message-passing by encoding the parameter-sharing schemes  as additional edge features. These additional update rules significantly improve experimental results. We note that our symmetry analysis may be useful for processing bags derived from other high-order generation policies [29; 20] by treating tuples of nodes as sets.

Inspired by these symmetries and traditional binary-based  and shortest path-based _node-marking_ strategies, we propose four natural marking strategies for our framework. Interestingly, unlike the full-bag scenario, they vary in expressiveness, with the shortest path-based technique being the most expressive.

The flexibility and effectiveness of our full framework are illustrated in Figure 2, depicting detailed experimental results on the popular Zinc-12k dataset . Our method demonstrates a significant performance boost over baseline models in the _small bag_ setting (for which they are designed), while achieving results that compare favourably to state-of-the-art Subgraph GNNs in the _full bag_ setting. Additionally, we can obtain results in-between these two regimes.

Contributions.The main contributions of this paper are: (1) the development of a novel, flexible Subgraph GNN framework that enables meaningful construction and processing of bags of subgraphs of any size; (2) a characterization of all affine invariant/equivariant layers defined on our node feature tensors; (3) a theoretical analysis of our framework, including the expressivity benefits of our node-marking strategy; and (4) a comprehensive experimental evaluation demonstrating the advantages of the new approach across both small and large bag sizes, achieving state-of-the-art results, often by a significant margin.

## 2 Related work

**Subgraph GNNs.** Subgraph GNNs [39; 8; 27; 4; 40; 26; 12; 29; 17; 38; 3] represent a graph as a collection of subgraphs, obtained by a predefined generation policy. For example, each subgraph can be generated by marking exactly one node in the original graph (see inset 3) - an approach commonly referred to as _node marking_; this marked node is considered the root node in its subgraph. Several recent papers focused on scaling these methods to larger graphs, starting with basic random selection of subgraphs from the bag, and extending beyond with more sophisticated techniques that aim to learn how to select subgraphs. To elaborate,  introduced _Policy-Learn_ (PL), an approach based on two models, where the first model predicts a distribution over the nodes of the original graph, and the second model processes bags of subgraphs sampled from this distribution. _MAG-GNN_ employs a similar approach utilizing Reinforcement Learning.

Similarly to our approach, this method permits high-order policies by associating subgraphs with tuples rather than individual nodes, allowing for the marking of several nodes within a subgraph.

Figure 2: The performance landscape of Subgraph GNNs with varying number of subgraphs: Our method leads in the lower bag-size set, outperforming other approaches in nearly all cases. Additionally, our method matches the performance of state-of-the-art Subgraph GNNs in the full-bag setting. The full mean absolute error (MAE) scores along with standard deviations are available in Table 9 in the appendix.

However, as mentioned before, these approaches involve discrete sampling while training, making them very hard to train (1000-4000 epochs vs. \(\)400 epochs of state-of-the-art methods [3; 38] on the Zinc-12k dataset), and limiting their usage to very small bags. Finally, we mention another high-order method, _OSAN_, introduced by , which learns a distribution over tuples that represent subgraphs with multiple node markings. In contrast to these previous approaches, we suggest a simpler and more effective way to select subgraphs and also show how to leverage the resulting symmetry structure to augment our message-passing operations.

**Symmetries in graph learning.** Many previous works have analyzed and utilized the symmetry structure that arises from graph learning setups [22; 23; 18; 2]. Specifically relevant to our paper is the work of  that characterized basic equivariant linear layers for graphs, the work of  that characterizes equivariant maps for many other types of incidence tensors that arise in graph learning, and the works [4; 12] that leveraged group symmetries for designing Subgraph GNNs in a principled way.

## 3 Preliminaries

**Notation.** Let \(\) be a family of undirected graphs, and consider a graph \(G=(V,E)\) within this family. The adjacency matrix \(A^{n n}\) defines the connectivity of the graph4, while the feature matrix \(X^{n d}\) represents the node features. Here, \(V\) and \(E\) represent the sets of nodes and edges, respectively, with \(|V|=n\) indicating the number of nodes. We use the notation \(v_{1}_{A}v_{2}\) to denote that \(v_{1}\) and \(v_{2}\) are neighboring nodes according to the adjacency \(A\). Additionally, we define \([n]\{1,2, n\}\), and \(([n])\) as the power set of \([n]\).

**Subgraph GNNs as graph products.** In a recent work,  demonstrated that various types of update rules used by current Subgraph GNNs can be simulated by employing the _Cartesian graph product_ between the original graph and another graph, and running standard message passing over that newly constructed product graph. Formally, the cartesian product of two graphs \(G_{1}\) (\(n_{1}\) nodes) and \(G_{2}\) (\(n_{2}\) nodes), denoted by \(G_{1} G_{2}\), forms a graph with vertex set \(V(G_{1}) V(G_{2})\). Two vertices \((u_{1},u_{2})\) and \((v_{1},v_{2})\) are adjacent if either \(u_{1}=v_{1}\) and \(u_{2}\) is adjacent to \(v_{2}\) in \(G_{2}\), or \(u_{2}=v_{2}\) and \(u_{1}\) is adjacent to \(v_{1}\) in \(G_{1}\). We denote by \(^{n_{1} n_{2} n_{1} n_{2}}\) and \(^{n_{1} n_{2} d}\) the adjacency and node feature matrices of the product graph; in general, we use calligraphic letters to denote the adjacency and feature matrices of product graphs, while capital English letters are used for those of the original graphs. In particular, for the graph cartesian product, \(G_{1} G_{2}\), the following holds:

\[_{G_{1} G_{2}}=A_{1} I+I A_{2}.\] (1)

For a detailed definition of the cartesian product of graphs, please refer to Definition A.1. As a concrete example for the analogy between Subgraph GNNs and the Cartesian product of graphs, we refer to a result by , which states that the maximally expressive node-based Subgraph GNN architecture GNN-SSWL\(+\), can be simulated by an MPNN on the Cartesian product of the original graph with itself, denoted as \(G G\). As we shall see, our framework utilizes a cartesian product of the original graph and a coarsened version of it, as illustrated in Figure 1 (right).

**Equivariance.** A function \(L:U W\) is called equivariant if it commutes with the group action. More formally, given a group element, \(g\), the function \(L\) should satisfy \(L(g v)=g L(v)\) for all \(v U\) and \(g\). \(L\) is said to be invariant if \(L(g v)=L(v)\).

## 4 Coarsening-based Subgraph GNN

**Overview.** This section introduces the _Coarsening-based Subgraph GNN_ (CS-GNN) framework. The main idea is to select and process subgraphs in a principled and flexible manner through the following approach: (1) coarsen the original graph via a coarsening function, \(\) - see Figure 1(left); (2) Obtain the product graph - Figure 1(right) defined by the combination of two adjacencies, \(_{(G)}\) (red edges), \(_{G}\) (grey edges), which arise from the graph Cartesian product operation (details follow); (3) leveraging the symmetry of this product graph to develop _symmetry-based_ updates, described by \(_{}\) (this part is not visualized in Figure 1). The general update of our suggested layer takes the following form 5,\[^{t+1}(S,v)=f^{t}(S,v)^{t},\] (2) \[(S^{},v^{})^{t}\}_{}}_{},(S^{},v^{})^{t}\}_{(S^{},v^{})_{(G)}(S,v)$}} }_{},(S^{},v^{})^{t}\}_{(S^{},v^{})_{}(S,v)$}}}_{ },\]

where the superscript \({}^{t}\) indicates the layer index. In what follows, we further elaborate on these three steps (in Sections 4.1 to 4.2).

We note that each connectivity in Equation (2) is processed using a distinct MPNN, and after stacking of those layers, we apply a pooling layer5 to obtain a graph representation; that is, \((^{})=^{}_{S}_{ v=1}^{n}^{}(S,v)\); \(\) denotes the final layer.

For more specific implementation details, we refer to Appendix F.

### Construction of the coarse product graph

As mentioned before, a maximally expressive node-based Subgraph GNN can be realized via the Cartesian product of the original graph with itself \(G G\). In this work, we extend this concept by allowing the left operand in the product to be the coarsened version of \(G\), denoted as \((G)\), as defined next. This idea is illustrated in Figure 1.

**Graph coarsening.** Consider a graph \(G=(V,E)\) with \(n\) nodes and an adjacency matrix \(A\). Graph coarsening is defined by the function \(:\), which maps \(G\) to a new graph \((G)=(V^{},E^{})\) with an adjacency matrix \(A^{}^{2^{n} 2^{n}}\) and a feature matrix \(X^{}^{2^{n} d}\). Here, \(V^{}\), the vertex set of the new graph represents super-nodes - defined as subsets of \([n]\). Additionally, we require that nodes in \(V^{}\) induce a partition over the nodes of the original graph6. The connectivity \(E^{}\) is extremely sparse and induced from the original graph's connectivity via the following rule:

\[A^{}(S_{1},S_{2})=1& v S_{1},  v S_{2}A(v,u)=1,\\ 0&,\] (3)

To clarify, in our running example (Figure 1), it holds that \(A^{}(\{a,b,c,d\},\{e\})=1\), while \(A^{}(\{e\},\{f\})=0\). For a more formal definition, refer to Definition A.3.

More specifically, our implementation of the graph coarsening function \(\) employs spectral clustering7 to partition the graph into \(T\) clusters, which in our framework controls the size of the bag. This results in a coarsened graph with fewer nodes and edges than \(G\). We highlight and stress that the space complexity of this sparse graph, \((G)\), is upper bounded by that of the original graph \(G\) (we do not store \(2^{n}\) nodes).

Defining the (coarse) product graph \((G) G\).We define the connectivity of the product graph, see Figure 1(right), by applying the cartesian product between the coarsened graph, \((G)\), and the original graph, \(G\). The product graph is denoted by \((G) G\), and is represented by the matrices \(_{(G) G}^{(2^{n} n)(2^{n}  n)}\) and \(^{2^{n} n d8}\), where by recalling Equation (1), we obtain,

\[_{(G) G}=(G)} I}^{ _{(G)}}+^{ _{G}}.\] (4)

The connectivity in this product graph induces the horizontal (\(_{G}\)) and vertical updates (\(_{(G)}\)) in Equation (2), visualized in Figure 1(right) via grey and red edges, respectively.

### Symmetry-based updates

In the previous subsection, we used a combination of a coarsening function and the graph Cartesian product to derive the two induced connectivities \(_{G},_{(G)}\) of our product graph. We use these connectivities to to perform message-passing on our product graph (see Equation (2)).

Inspired by recent literature on Subgraph GNNs [12; 3; 38], which incorporates and analyzes additional non-local updates arising from various symmetries (e.g., updating a node's representation via all nodes in its subgraphs), this section aims to identify potential new updates that can be utilized over our product graph. To that end, we study the symmetry structure of the node feature tensor in our product graph, \((S,v)\).The new updates described below will result in the third term in Equation (2), dubbed _Symmetry-based updates_ (\(_{}\)). For better clarity in this derivation, we change the notation from nodes (\(v\)) to indices (\(i\)).

#### 4.2.1 Symmetries of our product graph

Since the order of nodes in the original graph \(G\) is arbitrary, each layer in our architecture must exhibit equivariance to any induced changes in the product graph. This requires maintaining equivariance to permutations of nodes in both the original graph and its transformation \((G)\). As a result, recalling that \(^{(2^{n} n)(2^{n} n)}\) and \(^{2^{n} n d}\) represent the adjacency and feature matrices of the product graph, the symmetries of the product graph are defined by an action of the symmetric group \(S_{n}\). Formally, a permutation \( S_{n}\) acts on the adjacency and feature matrices by:

\[()S_{1},i_{1},S_{2},i_{2} =^{-1}(S_{1}),^{-1}(i_{1}),^{ -1}(S_{2}),^{-1}(i_{2}),\] (5) \[()(S,i) =^{-1}(S),^{-1}(i),\] (6)

where we define the action of \( S_{n}\) on a set \(S=\{i_{1},i_{2},,i_{k}\}\) of size \(k\) as: \( S:=\{^{-1}(i_{1}),^{-1}(i_{2}),,^{-1}(i_{ k})\}^{-1}(S)\).

#### 4.2.2 Derivation of linear equivariant layers for the node feature tensor

We now characterize the linear equivariant layers with respect to the symmetry defined above, focusing on Equation (6). We adopt a similar notation to , and assume for simplicity that the number of feature channels is \(d=1\) (extension to multiple features is straightforward ). In addition, our analysis considers the case where \(V^{}\) encompasses all potential super-nodes formed by subsets of \([n]\) (i.e we use the sparse coarsened adjacency9).

Our main tool is the characterization of linear equivariant layers for permutation symmetries as parameter-sharing schemes [34; 30; 22]. In a nutshell, this characterization states that the parameter vectors of the biases, invariant layers, and equivariant layers can be expressed as a learned weighted sum of basis tensors, where the basis tensors are indicators of the orbits induced by the group action on the respective index spaces. We focus here on presenting the final results and summarize them in Proposition 4.1 at the end of this subsection. Detailed discussion and derivations are available in Appendix E.

**Equivariant bias and invariant layers.** The bias vectors of the linear layers in our space are in \(^{2^{n} n}\). As shown in Figure 3(right), the set of orbits induced by the action of \(S_{n}\) satisfies:

\[(([n])[n])/S_{n}\{^{k^{*}}:k=1,,n;*\{ +,-\}\}.\] (7)

Here, \(^{k^{+}}\) corresponds to all pairs \((S,i)([n])[n]\) with \(|S|=k\) and \(i S\), and \(^{k^{-}}\) to all pairs with \(|S|=k\) and \(i S\).

As stated in [34; 30; 22], the tensor set \(\{^{}_{S,i}\}_{(([n])[n])/S_{n}}\) where:

\[^{}_{S,i}=1,&(S,i);\\ 0,&.\] (8)

are a basis of the space of bias vectors of the invariant linear layers induced by the action of \(S_{n}\).

**Weight matrices.** Following similar reasoning, consider elements \((S_{1},i_{1},S_{2},i_{2})(([n])[n]([n]) [n])\). In Appendix E we characterize the orbits of \(S_{n}\) in this space as a partition in which each partition set is defined according to six conditions. Some of these conditions include the sizes of \(S_{1}\), \(S_{2}\) and \(S_{1} S_{2}\), which remain invariant under permutations. Given an orbit, \((([n])[n]([n])[n])/S_{n}\), we define a basis tensor, \(^{}^{2^{n} n 2^{n} n}\) by setting:

\[^{}_{S_{1},i_{1};S_{2},i_{2}}=1,&(S_{1},i_{1},S_{2},i_{2});\\ 0,&\] (9)

A visualization of the two basis vectors in Equations (8) and (9), is available in Figure 3. The following (informal) proposition summarizes the results in this section (the proof is given in Appendix G),

**Proposition 4.1** (Basis of Invariant (Equivariant) Layers).: _The tensors \(^{}\) (\(^{}\)) in Equation (8) (Equation (9)) form an orthogonal basis (in the standard inner product) of the invariant layers and biases (Equivariant layers - weight matrix)._

#### 4.2.3 Incorporating symmetry-based updates in our framework

In the previous subsection, we derived all possible linear invariant and equivariant operations that respect the symmetries of our product graph. We now use this derivation to define the symmetry-based updates in Equation (2), which correspond to the construction of \(_{}\) and the application of an MPNN.

To begin, we note that any linear equivariant layer can be realized through an MPNN  applied to a fully connected graph with appropriate edge features. This is formally stated in Lemma F.1, the main idea is to encode the parameters on the edges of this graph (see visualization inset). Thus, the natural construction of \(_{}\) corresponds to a fully connected graph, with appropriate edge features derived from the parameter-sharing scheme we have developed.

However, one of our main goals and guidelines in developing our flexible framework is to maintain efficiency, and to align with the (node-based) maximally expressive GNN, namely GNN-SSWL\(+\), for the case of a trivial coarsening function, \((G)=G\) (which correspond to the full-bag setting). To achieve this, we opt for a sparser choice by using only a subset of the basis vectors (defined in Equation (9)) to construct \(_{}\). Specifically, the matrix \(_{}\) corresponding to the chosen subset of basis vectors is visualized inset - the parameter-sharing scheme is represented by edges with matching

Figure 3: Visualization via heatmaps (different colors correspond to different parameters) of the parameter-sharing scheme determined by symmetries for a graph with \(n=6\) nodes, zooming-in on the block which corresponds to sets of size two. **Left:** Visualization of the weight matrix for the equivariant basis \(^{}_{S_{1},i_{1};S_{2},i_{2}}\) (a total of 35 parameters in the block). **Right:** Visualization of the bias vector for the invariant basis \(^{}_{S,i}\) (a total of 2 parameters in the block). Symmetry-based updates reduce parameters more effectively than previously proposed linear equivariant layers by treating indices as unordered tuples (see Appendix E.3 for a discussion).

colors. To clarify, the nodes \((S,v)\) that satisfy \(v S\) "send messages" (i.e., broadcast their representation) to all the nodes \((S^{},v^{})\) such that \(v=v^{}\). A more formal discussion regarding our implementation of those symmetry based updates is given in Appendix F.4.

**Maintaining sparsity.** While the updates above are defined over the sparse representation of the coarse product graph, in practice we use its dense representation, treating it as a graph over the set of nodes \(V V^{}\), which requires space complexity \((T|V|)\). The update rules above are adapted to this representation simply by masking all nodes \((S,v)\) in the sparse representation such that \(S V^{}\). We note the models using the resulting update rule remain invariant to the action of \(S_{n}\). See discussion in .

### Marking Strategies and Theoretical Analysis

One of the key components of subgraph architectures is their marking strategy. Two widely used approaches in node-based subgraph architectures are binary-based node marking  and distance-based marking , which were proven to be equally expressive in the full-bag setup . Empirically, distance-based marking has been demonstrated to outperform other strategies across several standard benchmarks. In this section, our aim is to develop and theoretically justify an appropriate marking strategy, specifically tailored to the structure of our product graph. We present and discuss here our main results, and refer to Appendix C for a more formal discussion.

Building on existing marking strategies and considering the unique structure of our product graph, we propose two natural extensions to both the binary node marking  and distance-based marking strategies . Extending binary node marking, we first suggest _Simple Marking_ (\(_{S}\)), where an element \((S,v)\) is assigned a binary feature that indicates whether node \(v\) belongs to subgraph \(S\) (\(v S\)). The second extension, _Node + Size Marking_ (\(_{SS}\)), builds on the _simple marking_ by assigning an additional feature that encodes the size of the super-node \(S\).

For distance-based strategies, we propose _Minimum Distance_ (\(_{MD}\)), where each element \((S,v)\) is assigned the smallest (minimal) shortest path distance (SPD) from node \(v\) to any node \(u S\). Finally, _Learned Distance Function_ (\(_{LD}\)) extends this further by assigning to each element \((S,v)\) the output of a permutation-invariant learned function, which takes the set of SPDs between node \(v\) and the nodes in \(S\) as input.

Surprisingly, unlike the node-based full-bag case, we find that these marking strategies are not all equally expressive. We conveniently gather the first three strategies as \(=\{_{S},_{SS},_{MD}\}\) and summarize the relation between all variants as follows:

**Proposition 4.2** (Informal - Expressivity of marking strategies.).: _(i) Strategies in \(\) are all equally expressive, independently of the transformation function \(\). (ii) The strategy \(_{LD}\) is at least as expressive as strategies in \(\). Additionally, there exists transformation functions s.t. it is strictly more expressive than all of them._

The above is formally stated in Propositions C.1 and C.2, and more thoroughly discussed in Appendix C. In light of the above proposition, we instantiate the learned distance function \(_{LD}\) strategy when implementing our model, as follows,

\[_{S,v}_{u S}z_{d_{G}(v,u)}\] (10)

where \(d_{G}(v,u)\) denotes the _shortest path distance_ between nodes \(v\) and \(u\) in the original \(G\)10.

**Coarsening Function and Expressivity.** We investigate whether our CS-GNN framework offers more expressiveness compared to directly integrating information between the coarsened graph and the original graph.

The two propositions below illustrate that a simple, straight forward integration of the coarsen graph with the original graph (this integration is referred to as the _sum graph_ - formally defined in Definition D.2), and further processing it via standard message-passing, results in a less expressive architecture. Furthermore, when certain coarsening functions are employed within the CS-GNN framework, our resulting architecture becomes strictly more expressive than conventional node-basedsubgraph GNNs. These results suggest that the interplay between the coarsening function and the subgraph layers we have developed enhances the model's overall performance. We summarize this informally below and provide a more formal discussion in Appendix D.

**Proposition 4.3** (Informal - CS-GNN goes beyond coarsening).: _For any transformation function \(\), CS-GNN can implement message-passing on the sum graph, hence being at least as expressive. Also, there exist transformations \(\)'s s.t. CS-GNN is strictly more expressive than that._

**Proposition 4.4** (Informal - CS-GNN vs node based subgraphs).: _There exist transformations \(\)'s s.t. our CS-GNN model using \(\) as its coarsening function is strictly more expressive than GNN-SSWL\(+\)._

## 5 Experiments

We experimented extensively over seven different datasets to answer the following questions: _(Q1) Can CS-GNN outperform efficient Subgraph GNNs operating on small bags? (Q2) Does the additional symmetry-based updates boost performance? (Q3) Does CS-GNN offer a good solution in settings where full-bag Subgraph GNNs cannot be applied? (Q4) Does CS-GNN in the full-bag setting validate its theory and match state-of-the-art full-bag Subgraph GNNs?_

In the following sections, we present our main results and refer to Appendix F for additional experiments and details.

**Baselines.** For each task, we include several baselines. The Random baseline corresponds to random subgraph selection. We report the best performing random baseline from all prior work [5; 20; 29; 3]. The other two (non-random) baselines are: (1) Learned[5; 20; 29], which represents methods that learn the specific subgraphs to be used; and (2) Full[38; 3], which corresponds to full-bag Subgraph GNNs.

**ZINC.** We experimented with both the ZINC-12k and ZINC-Full datasets [31; 14; 10], adhering to a \(500k\) parameter budget as prescribed. As shown in Table 1, CS-GNN outperforms all efficient baselines by a significant margin, with at least a \(+0.008\) MAE improvement for bag sizes \(T\{3,4,5\}\). Additionally, in the full-bag setting, our method recovers state-of-the-art results. The results for ZINC-Full are available in Table 8 in the Appendix.

**OGB.** We tested our framework on several datasets from the OGB benchmark collection . Table 4 shows the performance of our method compared to both efficient and full-bag Subgraph GNNs. Our CS-GNN outperforms all baselines across all datasets for bag sizes \(T\{2,5\}\), except for the moldiv dataset with \(T=2\), where PL achieves the best results and our method ranks second. In the full-bag setting, CS-GNN is slightly outperformed by the top-performing Subgraph GNNs but still offers comparable results.

**Peptides.** We experimented on the Peptides-func and Peptides-struct datasets  - which full-bag Subgraph GNNs already struggle to process - evaluating CS-GNN's ability to scale to larger graphs. The results are summarized in Table 2. CS-GNN outperforms all MPNN variants, even when incorporating structural encodings such as GATEDGCN+RWSE. Additionally, our method surpasses the random11 baseline on both datasets.

    & Peptides-func & Peptides-struct \\  & (AP \(\)) & (MAE \(\)) \\  GCN  & \(0.5930 0.0023\) & \(0.3496 0.0013\) \\ GIN  & \(0.5498 0.0079\) & \(0.3547 0.0045\) \\ GatedGCN  & \(0.5864 0.0077\) & \(0.3420 0.0013\) \\ GatedGCN+RWSE  & \(0.6069 0.0035\) & \(0.3357 0.0006\) \\  Random  & \(0.5924 0.005\) & \(0.2594 0.0021\) \\ Ours & \( 0.0080\) & \( 0.0015\) \\   

Table 2: Results on Peptides dataset.

  
**Method** & Bag size & ZINC (MAE \(\)) \\  GNN  & \(T=1\) & \(0.163 0.004\) \\  OGAN  & \(T=2\) & \(0.177 0.016\) \\ Random  & \(T=2\) & \(0.131 0.005\) \\ PL  & \(T=2\) & \(0.120 0.003\) \\ Mag-GNN  & \(T=2\) & \( 0.014\) \\ Ours & \(T=2\) & \( 0.005\) \\  Random  & \(T=3\) & \(0.124 N/A\) \\ Mag-GNN  & \(T=3\) & \( N/A\) \\ Ours & \(T=3\) & \( 0.005\) \\  Random  & \(T=4\) & \(0.125 N/A\) \\ Map-GNN  & \(T=4\) & \( N/A\) \\ Ours & \(T=4\) & \( 0.003\) \\  Random  & \(T=5\) & \(0.113 0.006\) \\ PL  & \(T=5\) & \( 0.005\) \\ Ours & \(T=5\) & \( 0.003\) \\  GNN-SSWL\(+\) & Full & \(0.070 0.005\) \\ Subgraphformer  & Full & \(0.067 0.007\) \\ Subgraphformer+PE  & Full & \( 0.001\) \\ Ours & Full & \( 0.0007\) \\   

Table 1: Results on Zinc-12k dataset. Top two results are reported as First and Second.

[MISSING_PAGE_EMPTY:10]