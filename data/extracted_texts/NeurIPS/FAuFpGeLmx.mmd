# Segmenting Watermarked Texts From Language Models

Xingchi Li

Department of Statistics

Texas A&M University

College Station, TX 77843

anthony.li@stat.tamu.edu

Equal contribution

&Guanxun Li1

Department of Statistics

Beijing Normal University at Zhuhai

Zhuhai, Guangdong 519087

guanzxun@bnu.edu.cn

&Xianyang Zhang

Department of Statistics

Texas A&M University

College Station, TX 77843

zhangxiany@stat.tamu.edu

###### Abstract

Watermarking is a technique that involves embedding nearly unnoticeable statistical signals within generated content to help trace its source. This work focuses on a scenario where an untrusted third-party user sends prompts to a trusted language model (LLM) provider, who then generates a text from their LLM with a watermark. This setup makes it possible for a detector to later identify the source of the text if the user publishes it. The user can modify the generated text by substitutions, insertions, or deletions. Our objective is to develop a statistical method to detect if a published text is LLM-generated from the perspective of a detector. We further propose a methodology to segment the published text into watermarked and non-watermarked sub-strings. The proposed approach is built upon randomization tests and change point detection techniques. We demonstrate that our method ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. To validate our technique, we apply it to texts generated by several language models with prompts extracted from Google's C4 dataset and obtain encouraging numerical results.1

## 1 Introduction

With the increasing use of large language models in recent years, it has become essential to differentiate between text generated by these models and text written by humans. Some of the most advanced LLMs, such as GPT-4, Llama 3, and Gemini, are very good at producing human-like texts, which could be challenging to distinguish from human-generated texts, even for humans. However, it is crucial to distinguish between human-produced texts and machine-produced texts to prevent the spread of misleading information, improper use of LLM-based tools in education, model extraction attacks through distillation, and the contamination of training datasets for future language models.

Watermarking is a principled method for embedding nearly unnoticeable statistical signals into text generated by LLMs, enabling provable detection of LLM-generated content from its human-writtencounterpart. This work focuses on a scenario where an untrusted third-party user sends prompts to a trusted large language model (LLM) provider, who then generates a text from their LLM with a watermark. This makes it possible for a detector to later identify the source of the text if the user publishes it. The user is allowed to modify the generated text by making substitutions, insertions, or deletions before publishing it. We aim to develop a statistical method to detect if a published text is LLM-generated from the perspective of a detector. When the text is declared to be watermarked, we study a problem that has been relatively less investigated in the literature: to separate the published text into watermarked and non-watermarked sub-strings. In particular, we divide the texts into a sequence of moving sub-strings with a pre-determined length and sequentially test if each sub-string is watermarked based on the \(p\)-values obtained from a randomization test. Given the \(p\)-value sequence, we examine if there are structural breaks in the underlying distributions. For non-watermarked sub-strings, the \(p\)-values are expected to be uniformly distributed over \(\), while for watermarked sub-strings, the corresponding \(p\)-values are more likely to concentrate around zero. By identifying the change points in the distributions of the \(p\)-values, we can segment the texts into watermarked and non-watermarked sub-strings.

In theory, we demonstrate that our method ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. Specifically, we obtain the convergence rate for the estimated change point locations. To validate our technique, we apply it to texts generated by different language models with prompts extracted from the real news-like data set from Google's C4 dataset, followed by different modifications/attacks that can introduce change points in the text.

### Related works and contributions

Digital watermarking is a field that focuses on embedding a signal in a medium, such as an image or text, and detecting it using an algorithm. Early watermarking schemes for natural language processing-generated text were presented by Atallah et al. (2001). Hopper et al. (2007) formally defined watermarking and its desired properties, but their definitions were not specifically tailored to LLMs. More recently, Abdelnabi and Fritz (2021) and Munyer and Zhong (2023) proposed schemes that use machine learning models in the watermarking algorithm. These schemes take a passage of text and use a model to produce a semantically similar altered passage. However, there is no formal guarantee for generating watermarked text with desirable properties when using machine learning.

The current work is more related to Aaronson (2023), Kirchenbauer et al. (2023), Kuditipudi et al. (2023). In particular, Kirchenbauer et al. (2023) introduced a "red-green list" watermarking technique that splits the vocabulary into two lists based on hash values of previous tokens. This technique slightly increases the probability of embedding the watermark into "green" tokens. Other papers that discuss this type of watermarking include Kirchenbauer et al. (2023), Cai et al. (2024), Liu and Bu (2024), and Zhao et al. (2023). However, this watermarking technique is biased, as the next token prediction (NTP) distributions have been modified, leading to a performance degradation of the LLM. Aaronson (2023) describes a technique for watermarking LLMs using exponential minimum sampling to sample tokens from an LLM, where the inputs to the sampling mechanism are also a hash of the previous tokens. This approach is closely related to the so-called Gumbel trick in machine learning. Kuditipudi et al. (2023) proposed an inverse transform watermarking method that can be made robust against potential random edits. Other unbiased watermarks in this fast-growing line of research include Zhao et al. (2024), Fernandez et al. (2023), Hu et al. (2023), Wu et al. (2023).

However, less attention has been paid to understanding the statistical properties of watermark generation and detection schemes. The paper by Huang et al. (2023) considers watermark detection as a problem of composite dependence testing. The authors aim to understand the minimax Type II error and the most powerful test that achieves it. However, Huang et al. (2023) assumes that the NTP distributions remain unchanged from predicting the first token to the last token, which can be unrealistic. On the other hand, Li et al. (2024) introduced a flexible framework for determining the statistical efficiency of watermarks and designing powerful detection rules. This framework reduces the problem of determining the optimal detection rule to solving a minimax optimization program.

Compared to the existing literature, we make two contributions:

(a) We rigorously study the Type I and Type II errors of the randomization test to test the existence of a watermark; see Theorems 1-2. We apply these results to the recently proposed inverse transform watermark and Gumbel watermark schemes; see Corollary 1.

(b) We develop a systematic statistical method to segment texts into watermarked and non-watermarked sub-strings. We have also investigated the theoretical and finite sample performance of this methodology. As far as we know, this problem has not been studied in recent literature on generating and detecting watermarked texts from LLMs.

## 2 Problem setup

### Watermarked text generation

Denote by \(\) the vocabulary (a discrete set), and let \(P\) be an autoregressive LLM which maps a string \(y_{-n_{0}:t-1}=y_{-n_{0}}y_{-n_{0}+1} y_{t-1}^{t+n_{0}}\) to a distribution over the vocabulary, with \(p(|y_{-n_{0}:t-1})\) being the distribution of the next token \(y_{t}\). Here \(y_{-n_{0}:0}\) denotes the prompt provided by the user. Set \(V=||\) be the vocabulary size, and let \(_{1:t}=_{1}_{2}_{t}\) be a watermark key sequence with \(_{i}\) for each \(i\), where \(\) is a general space. Given a prompt sent from a third-party user, the LLM provider calls a generator to autoregressitvely generate text from an LLM using a decoder function \(\), which maps \(_{t}\) and a distribution \(p_{t}\) over the next token to a value in \(\). The watermarking scheme should preserve the original text distribution, i.e., \(P((_{t},p_{t})=y)=p_{t}(y)\). A watermark text generation algorithm recursively generates a string \(y_{1:n}\) by

\[y_{i}=(_{i},p(|y_{-n_{0}:i-1})), 1 i n,\]

where \(n\) is the number of tokens in the text \(y_{1:n}\) generated by the LLM, and \(_{i}\)'s are assumed to be independently generated from some distribution \(\) over \(\). In other words, given \(p(|y_{-n_{0}:i-1})\), \(y_{i}\) is completely determined by \(_{i}\) and \(y_{-n_{0}:i-1}\).

For ease of presentation, we associate each token in the vocabulary with a unique index from \([V]:=\{1,2,,V\}\), and we remark that the generator should be invariant to this assignment.

**Example 1** (Inverse transform sampling).: In this example, we describe a watermarked text generation method developed in Kuditipudi et al. (2023) and discuss an alternative formulation of their approach. Write \(_{i}(k)=p(k|y_{-n_{0}:i-1})\) for \(1 k V\) and \(1 i n\). To generate the \(i\)th token, we consider a permutation \(_{i}\) on \([V]\) and \(u_{i}\) (which denotes the uniform distribution over \(\)), which jointly act as the key \(_{i}\). Let

\[((_{i},u_{i}),_{i})=_{i}^{-1}(\{_{i}(l):_{i}(j:_{i }(j)_{i}(l)) u_{i}\}).\]

We note that \(((_{i},u_{i}),_{i})=k\) if \(\{_{i}(l):_{i}(j:_{i}(j)_{i}(l)) u_{i}\}=_{i}(k)\), which implies that

\[_{i}(j:_{i}(j)_{i}(k)) u_{i}>_{i}(j:_{i}(j)<_{i}(k)).\]

As the length of this interval is \(_{i}(k)\), \(P(((_{i},u_{i}),_{i})=k)=_{i}(k)\). An alternate way to describe the same generator is as follows: Given a partition of the interval \(\) into \(V\) sub-intervals denoted by \(_{1},,_{V}\), we can order them in such a way that each interval \(_{i}\) is adjacent to its immediate right neighbor \(_{i+1}\). Now let \((k;i)\)'s be \(V\) intervals with the length \(|(k;i)|=p(k|y_{-n_{0}:i-1})\) for \(1 k V\). Given the permutation \(_{i}\), we can order the \(V\) intervals through \(_{i}\), i.e., \(\{_{_{i}(k)}(k;i):k=1,2,,V\}\). Define \(_{ik}=\{u_{i}_{_{i}(k)}(k;i)\}\) and set \(y_{i}=k\) if \(_{ik}=1\). Clearly \(P(y_{i}=k)=P(_{ik}=1)=P(u_{i}_{_{i}(k)}(k;i))=p(k|y_{-n_{0}: i-1})\).

Now, for a string \(_{1:n}\) (with the same length as the key) that is possibly watermarked, Kuditipudi et al. (2023) suggest the following metric to quantify the dependence between the watermark key and the string:

\[(_{1:n},_{1:n})=_{i=1}^{n}(u_{i}-1/2 )((_{i})-1}{V-1}-).\] (1)

Observe that if \(_{i}\) is generated using the above scheme with the key \(_{i}=(_{i},u_{i})\), then \(u_{i}\) and \(_{i}(_{i})\) are positively correlated. Thus, a large value of \(\) indicates that \(_{1:n}\) is potentially watermarked.

**Example 2** (Exponential minimum sampling).: We describe another watermarking technique proposed in Aaronson (2023). To generate each token of a text, we first sample \(_{ik}\) independently for \(1 k V\). Let

\[y_{i}=*{arg\,max}_{1 k V})}{p(k|y_{-n_ {0}:i-1})}=*{arg\,min}_{1 k V})}{p(k|y _{-n_{0}:i-1})}=*{arg\,min}_{1 k V}E_{ik},\]where \(E_{ik}:=-(_{ik})/p(k|y_{-n_{0}:i-1})(p(k|y_{-n_{0}:i-1}))\) with \((a)\) denoting an exponential random variable with the rate \(a\). For two exponential random variables \(X(a)\) and \(Y(b)\), we have two basic properties: (i) \((X,Y)(a+b)\); (ii) \(P(X<Y)=[1-(-aY)]=a/(a+b).\) Using (i) and (ii), it is straightforward to verify that

\[P(y_{i}=k)=P(E_{ik}<_{j k}E_{ij})=p(k|y_{-n_{0}:i-1}).\]

Hence, this generation scheme preserves the original text distribution.

Aaronson (2023) proposed to measure the dependence between a string \(_{1:n}\) and the key sequence \(_{1:n}\) using the metric

\[(_{1:n},_{1:n})=_{i=1}^{n}\{ (_{i,_{i}})+1\}.\] (2)

The idea behind the definition of this function is that if \(_{i}\) was generated using the key \(_{i}\), then \(_{i,_{i}}\) tends to have a higher value than the other components of \(_{i}\). Therefore, a larger value of the metric indicates that the string \(_{1:n}\) is more likely to be watermarked.

### Watermarked text detection

We now consider the detection problem, which involves determining whether a given text is watermarked or not. Consider the case where a string \(_{1:m}\) is published by the third-party user and a key sequence \(_{1:n}\) is provided to a detector. The detector calls a detection method to test

\[H_{0}:_{1:m}H_{a}:_{1:m},\]

by computing a \(p\)-value with respect to a test statistic \((_{1:n},_{1:m})\). It is important to note that the text published by the user can be quite different from the text initially generated by the LLM using the key \(_{1:n}\), which we refer to as \(y_{1:n}\). To account for this difference, we can use a transformation function \(\) that takes \(y_{1:n}\) as the input and produces the published text \(_{1:m}\) as the output, i.e., \(_{1:m}=(y_{1:n})\). This transformation can involve substitutions, insertions, deletions, paraphrases, or other edits to the input text.

The test statistic \(\) measures the dependence between the text \(_{1:m}\) and the key sequence \(_{1:n}\). Throughout our discussions, we will assume that a large value of \(\) provides evidence against the null hypothesis (e.g., stronger dependence between \(_{1:m}\) and \(_{1:n}\)). To obtain the \(p\)-value, we consider a randomization test. In particular, we generate \(_{i}^{(t)}\) independently over \(1 i n\) and \(1 t T\), and \(_{i}^{(t)}\)s are independent with \(_{1:m}\). Then the randomization-based \(p\)-value is given by

\[p_{T}=(1+_{t=1}^{T}\{(_{1:n}, {y}_{1:m})(_{1:n}^{(t)},_{1:m})\}).\]

**Theorem 1**.: For the randomization test, we have the following results.

1. Under the null, \(P(p_{T})=(T+1)/(T+1)\), where \( a\) denotes the greatest integer that is less than or equal to \(a\);
2. Suppose the following three conditions hold: 1. \(\{((_{1:n},_{1:m})|_{m}),((_{1:n}^{},_{1:m})|_{m})\} C_{v}/n\) with \(C_{v}>0\); 2. \([(_{1:n}^{},_{1:m})|_{m}]=O(n^{ -1/2})\); 3. \(_{n}[(_{1:n},_{1:m})| _{m}]=\). Here \(_{m}=[y_{-n_{0}:0},_{1:m}]\) and \(_{1:n}^{}\) is a key sequence generated in the same way as \(_{1:n}\) but is independent of \(_{1:m}\). Given any \(>0\), when \(T>2/-1\), \[P(p_{T}|_{m}) 1-C_{1}(-2T^{2})+o(1),\] (3) as \(n+\), where \(C_{1}>0\).

We now apply the results in Theorem 1 to Examples 1-2 with \(m=n\) and \((_{1:n},_{1:n})=(_{1:n},_{1:n})\) for \(\) defined in (1) and (2).

**Corollary 1**.: _If \(_{1:n}=y_{1:n}\) and_

\[}_{i=1}^{n}1-p(y_{i}|y_{-n_{0}:i-1}),\] (4)

_then Conditions (a)-(c) in Theorem 1 are satisfied for Examples 1-2. Consequently, the power of the randomization test converges to \(1\) in these examples as \(T+\)._

However, as the published text can be modified, it is not expected that every token in \(_{1:m}\) will be related to the key sequence. Instead, we expect certain sub-strings of \(_{1:m}\) to be correlated with the key sequence under the alternative hypothesis \(H_{a}\). To measure the dependence, we use a scanning method that looks at every segment/sub-string of \(_{1:m}\) and a segment of \(_{1:n}\) with the same length \(B\). We use a measure \((_{a:a+B-1},_{b:b+B-1})\) to quantify the dependence between \(_{a:a+B-1}\) and \(_{b:b+B-1}\), chosen based on the watermarked text generation method described above. Given \(\) and the block size \(B\), we can define the maximum test statistic as

\[(_{1:n},_{1:m})=_{1 a n-B+1}_{1 b m -B+1}(_{a:a+B-1},_{b:b+B-1}).\] (5)

**Theorem 2**.: Consider the maximum statistic defined in (5), where the dependence measure takes the form of \((_{a:a+B-1},_{b:b+B-1})=B^{-1}_{i=0}^{B-1}h_{i }(_{a+i},_{b+i})\) and \(h_{i}\)s are independent conditional on \([_{1:m},y_{-n:n}]\). Under the setting of Example 1, \(_{i}|h_{i}| 1/4\). In this case, suppose

\[C_{N,B}^{-1}_{a,b}[(_{a:a+B-1},_{b: b+B-1})|_{1:m},y_{-n_{0}:n}]+,\] (6)

where \(N=\{n,m\}\) and \(C_{N,B}=\). Then, (3) holds true. Under the setting of Example 2, \(h_{i}\)s are exponentially distributed conditional on \([_{1:m},y_{-n_{0}:n}]\). In this case, (3) is still true under (6) with \(C_{N,B}=(N)/B\).

## 3 Sub-string identification

In this section, we aim to address the following question, which seems less explored in the existing literature: given that the global null hypothesis \(H_{0}\) is rejected, how can we identify the sub-strings from the modified text \(_{1:m}\) that are machine-generated?

To describe the setup, we suppose the text published by the third-party user has the following structure:

\[_{1}_{2}_{_{1}} }_{}_{_{1}+1} {y}_{_{2}}}_{}_{_{2}+1} _{_{3}}}_{}_{_{3}+1}_{_{4}}}_{} ,\] (7)

in which case the sub-strings \(_{_{1}+1}_{_{2}}\) and \(_{_{3}+1}_{_{4}}\) are watermarked. We emphasize that the orders of the watermarked and non-watermarked sub-strings can be arbitrary and do not affect our method described below. The goal here is to separate the text into watermarked and non-watermarked sub-strings accurately. Our key insight to tackling this problem is translating it into a change point detection problem. To describe our method, we define a sequence of moving windows \(_{i}=[(i-B/2) 1,(i+B/2) m]\) with \(B\) being the window size which is assumed to be an even number (for the ease of presentation) and \(1 i m\). For each sub-string, we define a randomization-based \(p\)-value given by

\[p_{i}=(1+_{t=1}^{T}\{(_{1:n}, _{_{i}})(_{1:n}^{(t)},_{ _{i}})\}), 1 i m,\] (8)

where we let \((_{1:n},_{_{i}})=_{1 k n}(_{_{k}},_{_{i}})\) with \(_{k}=[(k-B/2) 1,(k+B/2) n]\).

We have now transformed the text into a sequence of \(p\)-values: \(p_{1},,p_{m}\). Under the null, the \(p\)-values are roughly uniformly distributed, while under the alternatives, the \(p\)-values will concentrate around zero. Consider a simple setup where the published text can be divided into two halves, with the first half watermarked and the second half non-watermarked (or vice versa). Then, we can identify the change point location through

\[=*{arg\,max}_{1<m}S_{1:m}(), S_{1:m}( ):=_{t}}|F_{1:}(t)-F_{+1:m} (t)|,\] (9)where \(F_{a:b}(t)\) denotes the empirical cumulative distribution function (cdf) of \(\{p_{i}\}_{i=a}^{b}\).

We shall use the block bootstrap-based approach to determine if \(\) is statistically significant. Note that conditional on \(_{m}\), the \(p\)-value sequence is \(B\)-dependent in the sense that \(p_{i}\) and \(p_{j}\) are independent only if \(|i-j|>B\). Hence, the usual bootstrap or permutation methods for independent data may not work as they fail to capture the dependence from neighboring \(p\)-values. Instead, we can employ the so-called moving block bootstrap for time series data (Kunsch, 1989; Liu et al., 1992). Given a block size, say \(B^{}\), we create \(m-B^{}+1\) blocks given by \(\{p_{i},,p_{i+B^{}-1}\}\) for \(1 i m-B^{}+1\). We randomly sample \(m/B^{}\) (assuming that \(m/B^{}\) is an integer for simplicity) blocks with replacement and paste them together. Denote the resulting resampled \(p\)-values by \(p_{1}^{*},,p_{m}^{*}\). We then compute \(F_{a,b}^{*}(t)\) based on the bootstrapped \(p\)-values and define

\[S_{1:m}^{*}()=_{t}}|F_{1:}^{*} (t)-F_{+1:m}^{*}(t)|.\]

Repeat this procedure \(T^{}\) times and denote the statistics by \(S_{1:m}^{*,(t)}()\) for \(t=1,2,,T^{}\). Define the corresponding bootstrap-based \(p\)-value as

\[_{T^{}}=+1}(1+_{t=1}^{T^{}} \{_{1<m}S_{1:m}()_{1<m}S_{1:m}^ {*,(t)}()\}).\] (10)

We claim that there is a statistically significant change point if \(_{T^{}}\).

**Remark 1**.: Alternatively, one can consider the Cramer-von Mises type statistic \(_{}C_{1:m}()\) with

\[C_{1:m}()=_{0}^{1}(m-)^{2}}{m^{3}}|F_{1:}(t)-F_{ +1:m}(t)|^{2}w(t)dt,\]

to examine the existence of a change point, where \(w()\) is a non-negative weight function defined over \(\). If a change point exists, we can estimate its location through \(=_{1<m}C_{1:m}()\). Other methods to quantify the distributional shift for change point detection include the distance and kernel-based two-sample metrics (Matteson and James, 2014; Chakraborty and Zhang, 2021) and graph-based test statistics (Chen and Zhang, 2015).

Consider the case where there is a single change point located at \(^{*}\). Without loss of generality, let us assume that before the change, the \(p\)-values are uniformly distributed, i.e., \(p_{1},,p_{^{*}} F_{0}\) with \(F_{0}(t)=t\) (the cdf of \(\)), while after the change, the \(p\)-values follow different distributions that concentrate around zero. We assume that \(_{m+}D(F_{0},[F_{^{*}+1:m}(t)])>0\), where \(D(F_{0},[F_{^{*}+1:m}(t)]):=_{t}|F_{0}(t)-[F_{^{*}+1:m}(t)]|\) is the Kolmogorov-Smirnov distance. In other words, the empirical cdf of the \(p\)-values from the watermarked sub-strings converges to a distribution that is different from the uniform distribution. This mild assumption allows for the detection of the structure break. As discussed before, we consider the scan statistic \(_{1<m}S_{1:m}()\) with \(S_{1:m}()\) defined in (9) for examining the existence of a change point.

**Proposition 1**.: _Assuming that \(B+\) and \(B/m 0,\) we have_

\[_{1<m}S_{1:m}()^{*}(1-^{*})D(F_{0}, [F_{^{*}+1:m}(t)])+o_{p}(1)+,\]

_where \(^{*}=_{m+}^{*}/m(0,1)\)._

Next, we shall establish the consistency of the change point estimator \(=_{1<m}S_{1:m}()\). In particular, we obtain the following result, which establishes the convergence rate of the change point estimate.

**Theorem 3**.: _Under the Assumptions in Proposition 1, we have_

\[|-^{*}|=O_{p}(}{D(F_{0}, [F_{^{*}+1:m}(t)])}).\]

**Remark 2**.: Our scenario differs from the traditional nonparametric change point detection problem in a few ways: (i) Rather than testing the homogeneity of the original data sequence (which is the setup typically considered in the change point literature), we convert the string into a sequence of p-values, based on which we conduct the change-point analysis; (ii) In the classical change point literature, the observations (in our case, the p-values) within the same segment are assumed to follow the same distribution. In contrast, for the watermark detection problem, the p-values from the watermarked segment could follow different distributions, adding a layer of difficulty to the analysis; (iii) The p-value sequence is dependent (where the strength of dependence is controlled by \(B\)), making our setup very different from the one in Carlstein (1988), which assumed the underlying data sequence to be independent; (iv) The technical tool used in our analysis must account for the particular dependence structure within the p-value sequence.

### Binary segmentation

In this section, we describe an algorithm to separate watermarked and non-watermarked sub-strings by identifying multiple change point locations. There are two main types of algorithms for identifying multiple change points in the literature: (i) exact or approximate optimization by minimizing a penalized cost function (Harchaoui and Levy-Leduc, 2010; Truong et al., 2020; R. Killick and Eckley, 2012; Li and Zhang, 2024; Zhang and Dawn, 2023) and (ii) approximate segmentation algorithms. Our proposed algorithm is based on the popular binary segmentation method, a top-down approximate segmentation approach for finding multiple change point locations. Initially proposed by Vostrikova (1981), binary segmentation identifies a single change point in a dataset using a CUSUM-like procedure and then employs a divide-and-conquer approach to find additional change points within sub-segments until a stopping condition is reached. However, as a greedy algorithm, binary segmentation can be less effective with multiple change points. Wild binary segmentation (WBS) (Fryzlewicz, 2014) and seeded binary segmentation (SeedBS) (Kovacs et al., 2022) improve upon this by defining multiple segments to identify and aggregate potential change points. SeedBS additionally addresses the issue of overly long sub-segments in WBS that may contain several change points. SeedBS uses multiple layers of intervals, each with a fixed number of intervals of varying lengths and shifts, to enhance the search for change points. When comparing multiple candidates for the next change point, the narrowest-over-threshold (NOT) method (Baranowski et al., 2019) prioritizes narrower sub-segments. Built upon these ideas, we develop an effective algorithm to identify the change points that separate watermarked and non-watermarked sub-strings. The details are described in Algorithm 1 below.

```
0: Sequence of \(p\)-values \(\{p_{i}\}_{i=1}^{m}\), decay parameter \(a[1/2,1)\) for SeedBS, threshold \(\) for NOT.
0: Locations of the change points.  Define \(I_{1}=(0,m]\).  for\(k 2,,_{1/a}m\)do\(\) Start of SeedBS  Number of intervals in the \(k\)-th layer: \(n_{k}=2(1/a)^{k-1}-1\).  Length of intervals in the \(k\)-th layer: \(l_{k}=ma^{k-1}\).  Shift of intervals in the \(k\)-th layer: \(s_{k}=(m-l_{k})/(n_{k}-1)\). \(k\)-th layer intervals: \(_{k}=_{i=1}^{n_{k}}\{((i-1)s_{k},(i-1)s _{k}+l_{k}]\}\). endfor  Define all seeded intervals \(=_{k=1}^{_{1/a}m}_{k}\). \(\) End of SeedBS for\(i 1,,||\)do\(\) Start of NOT  Define the \(i\)-th interval \(I_{i}=(r_{i},s_{i}]\).  Define \(S_{r_{i}+1:s_{i}}():=_{t})(s_{i}-)}{(s_{ i}-r_{i})^{3/2}}|F_{r_{i}+1:}(t)-F_{+1:s_{i}}(t)|\).  Let \(_{i}=_{r_{i}<<s_{i}}S_{r_{i}+1:s_{i}}()\).  Obtain \(_{i}\) through block bootstrap (10). endfor  Define the set of potential change point locations \(=\{i:_{i}<\}\) and the final set of change point locations \(=\). while\(\)do  Select \(i=_{i=1,,||}\{|I_{i}|\}=_{i=1,,| |}\{s_{i}-r_{i}\}\). \(\{_{i}\}\); \(\{j||:_{i} I_{j}\}\). endwhile return\(\). \(\) End of NOT ```

**Algorithm 1** SeedBS-NOT for change point detection in potentially partially watermarked texts

## 4 Numerical experiments

We conduct extensive real-data-based experiments following a similar empirical setting in Kirchenbauer et al. (2023), where we generate watermarked text based on the prompts sampled from the news-like subset of the colossal clean crawled corpus (C4) dataset (Raffel et al., 2020). We utilized three LLMs, namely openai-community/gpt2 (Radford et al., 2019), facebook/opt-1.3b (Zhang et al., 2022) and Meta-Llama-3-8B (AI@Meta, 2024), to evaluate the effectiveness of the proposed method. We consider the following four watermark generation and detection methods:

* ITS: The inverse transform sampling method with the dependence measure defined in (1).
* ITSL: The inverse transform sampling method with the dependence measure defined in (B.2), which is based on the Levenshtein cost (B.1) with base alignment cost (B.3).
* EMS: The exponential minimum sampling method with the dependence measure defined in (2).
* EMSL: The exponential minimum sampling method with the dependence measure defined in (B.3), which is based on the Levenshtein cost (B.1) with base alignment cost (B.4).

The details of the Levenshtein cost are deferred to Appendix B.1. For each of the experiment settings, \(100\) prompts were used to generate the watermarked text. We fix the length of text \(m=500\), the size of sliding window \(B=20\), and the block size used in the block bootstrap-based test \(B^{}=20\). Results for other choices of \(B\) are shown in Appendix C. In Algorithm 1, we set the decay parameter \(a=\) and the minimum length of the intervals generated by SeedBS to be 50 such that the block bootstrap-based test is meaningful, and the threshold \(\{0.05,0.01,0.005,0.001\}\). We present the results for openai-community/gpt2 in the main text and defer the results for facebook/opt-1.3b and Meta-Llama-3-8B to Appendix B.3 and Appendix B.4, respectively.

### False positive analysis

We begin by analyzing the false discoveries of the change point detection method. We will generate watermarked text with a length of \(m=500\), where no change points exist.

* Setting 1 (no change point): Generate 500 tokens with a watermark.

The results for openai-community/gpt2 are showed in Figure 1. The left panel illustrates that the two exponential minimum sampling methods result in fewer false discoveries compared to the two inverse transform sampling methods. Indeed, the existence of false discoveries highly depends on the quality of the obtained sequence of \(p\)-values. In the right panel, we fixed one prompt and plotted the sequence of \(p\)-values for the four methods. Most of the \(p\)-values are near \(0\). However, certain sub-strings have relatively high \(p\)-values for the two inverse transform methods, indicating that these methods failed to detect the watermark in these segments, resulting in false discoveries for change point detection.

Figure 1: Left panel: boxplots of the number of false detections with respect to different thresholds \(\). Right panel: sequences of \(p\)-values from different methods in Setting 1 for Prompt 1 with threshold \(=0.005\). The detected change point locations are marked with dashed lines at the index \(157\) for ITS and \(158\) for ITSL, respectively.

### Change point analysis

When users modify the text generated by LLM, there may be some sub-strings with watermarks and others without. Our goal is to accurately separate the text into watermarked and non-watermarked sub-strings. In this section, we will focus on two types of attacks: insertion and substitution. To demonstrate, we will consider the following three settings:

* Setting 2 (insertion attack): Generate \(250\) tokens with watermarks, then append with \(250\) tokens without watermarks. In this setting, there is a single change point at the index \(251\).
* Setting 3 (substitution attack): Generate \(500\) tokens with watermarks, then substitute the token with indices ranging from \(201\) to \(300\) with non-watermarked text of length \(100\). In this setting, there are two change points at the indices \(201\) and \(301\), respectively.
* Setting 4 (insertion and substitution attacks): Generate \(400\) tokens with watermarks, substitute the token with indices ranging from \(101\) to \(200\) with non-watermarked text of length \(100\), and then insert \(100\) tokens without watermarks at the index \(300\). In this setting, there are four change points located at the indices \(101\), \(201\), \(301\), and \(401\), respectively.

For more complex simulation settings, please refer to Appendix B.5.

We compare the clusters identified through the detected change points with the true clusters separated by the true change points using the Rand index (Rand, 1971). A higher Rand index indicates better performance of different methods.

Figure 2 shows the Rand index for four methods in Settings 2-4 corresponding to different thresholds \(\). For each method, their performance in Settings 2-3 is better than that of Setting 4 when the threshold \( 0.01\). This is because Setting 4 includes two types of attacks, making the problem more difficult than in Settings 2 and 3. In all cases, the two exponential minimum sampling methods outperform the two inverse transform sampling methods, and EMS delivers the highest Rand index value. We want to emphasize again that the performance of the change point detection method highly depends on the quality of the obtained sequence of \(p\)-values. Figure 3 shows the \(p\)-value sequence for all methods given one fixed prompt in Setting 4. The change points detected by the EMS and EMSL methods are closer to the true change points compared to those detected by the ITS and ITSL methods. Additionally, the sequence of \(p\)-values for all methods in Settings 1-4 with the first 10 prompts extracted Google C4 dataset are shown in Figure B.1 in the Appendix.

## 5 Discussions

In this study, we have introduced a method for detecting whether a text is generated using LLM through randomization tests. We have demonstrated that our method effectively controls Type I and Type II errors under appropriate assumptions. Additionally, we have developed a technique to partition the published text into watermarked and non-watermarked sub-strings by treating it as a change point detection problem. Our proposed method accurately identifies watermarked sub-strings by determining the locations of change points. Simulation results using real data indicate that the EMS method outperforms the other methods.

Figure 2: The boxplots of the Rand index comparing the clusters identified through the detected change points with the true clusters separated by the true change points with respect to different thresholds \(\).

The performance of the segmentation algorithm depends crucially on the quality of the randomization-based \(p\)-values from each sub-string. Intuitively, a more significant discrepancy between the \(p\)-value distributions under the null and alternative will lead to better segmentation results. Thus, a powerful watermark detection algorithm is crucial to the success of the segmentation procedure. Motivated by Condition (4), an interesting future direction is to develop an adaptive watermark generation and detection procedure where the LLM provider adaptively embeds the key according to NTP, and the detector uses a corresponding adaptive procedure to detect the watermark.

Another interesting direction to explore is extending the algorithm to handle scenarios where the published text combines watermarked texts from different LLMs with varying watermark generation schemes. In this case, the goal is to separate the texts into sub-strings from different LLMs. This scenario involves multiple sequences of keys from different LLMs, each producing a sequence of \(p\)-values and change points. Figuring out how to aggregate these results to separate different LLMs and user-modified texts would be an intriguing problem.