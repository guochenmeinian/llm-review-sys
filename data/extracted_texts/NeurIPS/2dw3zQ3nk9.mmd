# Vript: A Video Is Worth Thousands of Words

Dongjie Yang\({}^{1}\), Suyuan Huang\({}^{2}\), Chengqiang Lu\({}^{3}\), Xiaodong Han\({}^{3}\), Haoxin Zhang\({}^{3}\), Yan Gao\({}^{3}\), Yao Hu\({}^{3}\), Hai Zhao\({}^{1}\)

\({}^{1}\)Shanghai Jiao Tong University, \({}^{2}\) Beihang University, \({}^{3}\) Xiaohongshu Inc.

\({}^{1}\){djyang.tony@,zhaohai@cs.}sjtu.edu.cn, \({}^{2}\)huangsuyuan@buaa.edu.cn,

\({}^{3}\){lusuo,shuweng,haoli9,yadun,xiahou}@xiaohongshu.com

 Corresponding author.

###### Abstract

Advancements in multimodal learning, particularly in video understanding and generation, require high-quality video-text datasets for improved model performance. Vript addresses this issue with a meticulously annotated corpus of 12K high-resolution videos, offering detailed, dense, and script-like captions for over 420K clips. Each clip has a caption of ~145 words, which is over 10x longer than most video-text datasets. Unlike captions only documenting static content in previous datasets, we enhance video captioning to video scripting by documenting not just the content, but also the camera operations, which include the shot types (medium shot, close-up, etc) and camera movements (panning, tilting, etc). By utilizing the Vript, we explore three training paradigms of aligning more text with the video modality rather than clip-caption pairs. This results in Vriptor, a top-performing video captioning model among open-source models, comparable to GPT-4V in performance. Vriptor is also a powerful model capable of end-to-end generation of dense and detailed captions for long videos. Moreover, we introduce Vript-Hard, a benchmark consisting of three video understanding tasks that are more challenging than existing benchmarks: Vript-HAL is the first benchmark evaluating action and object hallucinations in video LLMs, Vript-RR combines reasoning with retrieval resolving question ambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the temporal understanding of events in long videos rather than actions in short videos in previous works. All code, models, and datasets (Vript, Vript_CN, Vript_Multilingual) are available in [https://github.com/mutonix/Vript](https://github.com/mutonix/Vript).

## 1 Introduction

With the rapid development of multimodal learning [2; 3; 4], researchers are increasingly focusing on understanding [5; 6; 7] and generation [8; 9; 10] of the video modality. This has triggered a surge in demand for high-quality video-text datasets containing high-resolution videos and detailed captions. Compared to image-text pairs [11; 12], video-text pairs are harder to obtain and annotate. As a video has an additional temporal dimension, it contains more information than a single image. Additionally, a video often comprises numerous events, and each event can consist of several scenes. For instance, a travel vlog might feature events such as preparing for the journey and visiting various destinations. Each event can be depicted using different shots. Video captioning takes more labor for annotators to check through the whole video and write down thousands of words to annotate everydetail. Therefore, most previous video-text datasets only have short and coarse-grained descriptions for short video clips. For example, as shown in Table 1, WebVid-10M  and Panda-70M  comprise captions of 1-3 sentences for video clips shorter than 15 seconds.

To address the limitations of existing datasets, we construct a fine-grained video-text dataset called Vript, including 12K high-resolution videos (over 420K clips) annotated by GPT-4V . The annotation process of Vript is inspired by the format of video scripts. A video script organizes the process of shooting a video consisting of multiple scenes. For each scene, we care not only about the content but also the camera operations, including shot types (medium shot, close-up, etc) and how the camera moves (panning, tilting, etc). Unlike most previous video-text datasets , we densely annotate the untrimmed videos, and each scene in the video has a long caption of ~145 words. Besides the vision modality, we transcribe the voice-over into text and put it along with the video title to supplement background information, which greatly reduces the hallucinations in the captions.

Existing studies  report that detailed captions help improve better vision-language alignment. Most datasets  have short captions and are not densely annotated. Therefore, we can only align one short video clip with one short caption at a time during the training. To align more text with the video, we explore three paradigms that are not commonly used in vision-language alignment for videos: 1) Video-script alignment: We sample multiple successive scenes to form a longer video and concatenate the corresponding captions to create a "sub-script" as a longer text target. 2) Voice-over transcription: We combine the voice-over transcription and the video as input. 3) Video timestamp: We introduce the timestamps of both voice-over and video as additional information. Based on these, we train a video captioning model, dubbed Vriptor. Vriptor is good at generating dense captions both for short and long videos end to end and reaches SOTA performance in video captioning among open-source models.

Figure 1: (a) We present a comparison between captions from our Vript and those produced by large multimodal models (LMMs). Compared to captions with hallucinations (marked in red) from LLaVA , Vript consists of the most detailed and accurate descriptions (marked in orange) for the videos. (b) Videos in Vript are densely annotated akin to video scripts, encompassing thousands of words. (c) Vript provides captions for open-domain videos in high resolution and various aspect ratios.

Moreover, we propose Vript-Hard, a video understanding benchmark consisting of three tasks that are more challenging than most benchmarks : 1) **Vript-HAL (Hallucination Evaluation)**: Vript-HAL is the first benchmark to comprehensively evaluate object and action hallucinations in video LLMs, providing the detailed ground truth 25x longer than MSR-VTT . 2) **Vript-RR (Retrieval then Reasoning)**: Long video QA benchmarks ask questions about details in long videos that easily lead to ambiguity because the answers may vary in different timestamps. To solve this issue, we construct a long video reasoning task dubbed Vript-RR, by giving a hint for locating the relevant scene and then asking questions about the scene. Vript-RR features harder questions that need multi-hop reasoning and longer videos (2min-40min) than previous long video benchmarks, e.g., EgoSchema  (3min). 3) **Vript-ERO (Event Re-ordering)**: Different from previous benchmarks  of temporal understanding that only care about chronological order of actions in short videos, we build a new challenging task called event re-ordering, requiring the model to sequence sampled events in long videos. In Vript-ERO, each video contains over 40 scenes on average and models need to re-order three of them in the correct order.

To sum up, we construct a high-quality video-text dataset called Vript, with dense and detailed captions for videos. Based on Vript, we train a top-performing video captioning model dubbed Vriptor. We propose Vript-Hard, a challenging video understanding benchmark that solves deficiencies in previous benchmarks, consisting of three tasks: Vript-HAL, Vript-RR, and Vript-ERO.

## 2 Related Work

Video-text DatasetBuilding powerful video foundation models  requires high-quality video-text datasets for vision-language alignment. In Table 1, we compare video-text datasets using different annotation methods. Datasets such as HD-VILA-100M  utilize subtitles as captions, which often can not precisely describe videos. Annotating videos manually  gives accurate descriptions, yet it is challenging to scale up the dataset size. Recent datasets like HD-V6-130M  leverage large multimodal models (LMMs) to automatically generate captions but only short captions are provided due to the limitation of the model's ability. Compared to the above, Vript provides dense and detailed captions 10x longer for untrimmed videos by using GPT-4V .

   Dataset & Domain & Text Len & Clips & Duration & Resolution & Lang \\  HowTo100M  & Open & 4.0 & 136M & 134Kh & 240p & en \\ ACAV100M  & Open & - & 100M & 278h & - & en \\ HD-VILA-100M  & Open & 32.5 & 103M & 371Kh & 720p & en \\ WebVid-10M  & Open & 12 & 10M & 52Kh & 360p & en \\ YT-Temporal-180  & Open & 10 & 180M & - & 480p & en \\  MSVD  & Open & 8.7 & 1970 & 5.3h & - & en \\ MSR-VTT  & Open & 9.3 & 10K & 40h & 240p & en \\ DiDeMo  & Flickr & 8.0 & 27K & 87h & - & en \\ ActivityNet  & Action & 13.5 & 100K & 849h & 144p-720p & en \\ YouCook2  & Cooking & 8.8 & 14K & 176h & - & en \\ VATEX  & Open & 15.2 & 41K & 115h & - & en \\  HD-VG-130M  & Open & 10 & 130M & 180Kh & 720p & en \\ Panda-70M  & Open & 13.2 & 70M & 167Kh & 720p & en \\ InternVid  & Open & 17.6 & 234M & 760.3Kh & 720p & en \\ 
**Vript** & Open & 145 & 420K & 1.3Kh & 720p-2K & en \\
**Vript-CN** & Open & 150 & 293K & - & 720p-1080p & zh \\
**Vript-Multilingual** & Open & 150 & 677K & - & 720p-1080p & multi \\   

Table 1: **Comparisons between Vript and other video-text datasets. We divide the datasets into three parts. For the first part, the captions of these datasets come from subtitles (ASR) or descriptions scraped from the Internet. For the second part, the captions are collected by crowdworkers. For the third part, the captions are generated by multimodal models automatically.**Video Understanding BenchmarkExisting benchmarks [16; 25; 19; 18; 7] including captioning and QA tasks evaluate models on the short videos (<5min) and test the superficial understanding of the videos. In contrast, Vript-Hard scales up the videos to be much longer, e.g., Vript-RR (2min-40min) and Vript-ERO (2min-2h) and requires models to watch videos more carefully, e.g, Vript-HAL evaluating hallucinations of video LLMs and Vript-RR testing multi-hop reasoning ability.

## 3 Refine Video Captioning into Video Scripting

In the construction of Vript, our goal is to annotate a video as detailed as possible so that we can even visualize the video via the text description. For each scene in the video, we describe events with detailed actions and interactions rather than coarse-grained descriptions. Besides events, we record more details: the appearance of all objects and characters, environment, light, video style, etc.

In addition to the static description above, we inspect how the camera moves and shoots the scenes (Camera language). Previous works [14; 6; 13] leverage the pipeline of describing an image to describe a video, ignoring the cameras. For a video clip about a man riding a bike, if we only describe what is in the frames, we can say "A man in a dark blue shirt is riding a black bike along the road". However, to be specific, we actually observe "As the camera pans to a close-up shot, a man in a dark blue shirt is riding a black bike. As the camera zooms out, we can see an overview of a man riding along the road with mountains behind him." Thus, to enhance the description of a video, it is necessary to record the camera language in addition to the content.

Combining both static description and camera language is like how we write a scene in a video script. In Vript, following the format of the video script, we first split the video into scenes using the PySceneDetect 2 and annotate each scene with static description and camera language, dubbed Video Scripting. We select 10K YouTube long videos from HD-VILA-100M  and collect 1.5K short videos from YouTube Shorts and TikTok from the Internet. We leverage the advanced multimodal model, GPT-4V , to annotate the following items for each scene: 1) title: a brief summarization of the scene within 10 words; 2) content: detailed description of around 150 words; 3) shot type: full view, close-up, etc; 4) camera movement: panning, zooming, etc. To make a "full" script of a video, we densely annotate the untrimmed videos (lasting from 5s to 2.9h) from the start to the end.

Besides video frames, we also add more external information to assist the annotation. We leverage the voice-over transcribed by the Whisper model  and also the video title, which helps the model to know what the original video is about. This external information greatly reduces the hallucinations and improves the caption granularity, helping the models to better understand what is happening in the video rather than what they have seen visually. For example, as shown in Figure 2, by watching the frames of Scene-010, we can not infer what ingredients are added to the bowl with the spoon and the squeeze bottle. The highlighted words from the voice-over illustrate they are mayonnaise and mustard, which improves the granularity of the caption shown in the top-right panel.

## 4 Vriptor: A Long Video Is Worth Thousands of Words

In the common paradigm of vision-language alignment for video foundation model training, assuming the batch size is 1, we align one video with one text caption. Existing video-text datasets like Panda-70M  and WebVid-10M  only have brief captions where inadequate details result in suboptimal vision-language alignment. To alleviate this issue, we showcase how we can align more text with videos by training on the Vript dataset. We explore three not commonly used paradigms beyond the common one. Based on these, we train the Vriptor, a powerful model for video captioning, which reaches SOTA performance among open-source video LLMs.

### Method

Video-Script AlignmentIf videos are densely annotated, a possible way to increase the amount of text for alignment is to concatenate captions of multiple successive clips. Though clips can be easily concatenated to create a longer video, captions are annotated separately so that the concatenated caption may not have coherence in the semantics. Inspired by video scripts, we reformulate the successive captions into scenes of the video script. In the right panel of Figure 2, a script in Vript with multiple scenes is coherent in the semantics despite they are annotated separately because: 1) each scene caption is very detailed and has similar descriptions for the shared background or context and 2) title of each scene acts as a separator rather than concatenating them directly. In Vript, We can easily sample several successive clips to create a "sub-script", e.g., 10 successive clips with corresponding "sub-script" containing about 1.5K words, which is nearly 100x longer than short captions.

Voice-over TranscriptionWe add voice-over transcription as the additional speech modality. As the Vript is annotated with joint input of voice-overs and video frames, the captions contain information that comes from the voice-over as shown in Figure 2.

Video TimestampCommonly video LLMs [7; 34] implement a certain sampling strategy to extract multiple frames as the video input. These models are weak in time awareness as they only know the order of frames but do not know how long the frames last. We find that timestamps are crucial for the video-script alignment of multiple scenes. As shown in Figure 2, we add two kinds of timestamps in the text format: voice-over timestamps in the input and video timestamps in the output caption. Predicting the timestamps of the video helps the model to know the start and the end of each scene.

### Experiment and Analysis

We aggregate these paradigms to train Vriptor. In Figure 2, we combine four types of inputs and outputs: 1) 1 scene \(\) 1 caption; 2) 1 scene + voice-over \(\) 1 caption; 3) many scenes \(\) 1 script; 4) many scenes + voice-over \(\) 1 script. We add the timestamp information for all four types. We train the Vriptor based on ST-LLM  for two stages. We evaluate the captioning ability of the Vriptor on the Vript-HAL and the MSR-VTT , where the Vript-HAL and metrics are introduced in Sec 5.1 later. More details of training Vriptor can be checked in Appendix D.

Figure 2: The input and output combinations of Vriptor training.

Video-Script Alignment Helps Model Watch MoreAs shown in Figure 2, Vriptor supports two types of instructions: describe the whole video and scene by scene. For the whole-video instruction, Vriptor gives a general description of 100-150 words. For the scene-by-scene instruction, Vriptor gives a dense description of the video with each scene of 100-150 words. In Table 2, compared to the whole-video description, Vriptor gives more details of the video in the scene-by-scene description with an increasing recall in the Vript-HAL and the MSR-VTT as the number of output scenes increases. However, as the captions get longer and more detailed (more scenes), models are easier to generate hallucinations with a drop in precision. In Figure 3, we showcase the ability of Vriptor to caption long videos with longer texts. Models like VideoChat2  only give a relatively fixed length of captions for videos of different lengths. Vriptor-S (scene-by-scene) can scale up the caption length as the video gets longer, just like writing a longer video script.

Voice-overs Help Model Understand What They AreIn the last two rows in Table 2, we showcase the increments in both precision and recall that the model can give more detailed and accurate descriptions with the help of voice-over. We also observe a 14% increment in the proportion of proper nouns of all nouns in the captions. This suggests that the model is capable of inferring the names of objects rather than only their appearance by analyzing the voice-over.

Timestamps Help Model Know the Starts and the EndsTo verify the effectiveness of adding timestamps, we also train another model without adding timestamps. Comparing these two models, we find the improvement is minor in whole-video description but significant in scene-by-scene description. The model with timestamps is less likely to generate duplicated descriptions from previous scenes because it can understand the start and end of each scene and identify which scene corresponds to which period. Besides, the model with timestamps gives more detailed captions with a 12% higher recall on Vript-HAL while the model without timestamps is more likely to forget to describe some parts of the videos.

## 5 Vript-Hard

As multimodal models advance in performance, a more challenging benchmark is required to evaluate their capabilities. We propose a hard video understanding benchmark, dubbed Vript-Hard, consisting of three challenging tasks: HAL (Hallucination Evaluation), RR (Retrieval then Reasoning), and ERO (Event Re-ordering). We evaluate a large range of image LLMs, namely BLIP2 , InstructBLIP , Qwen-VL , LLaVA 1.6 34B , and video LLMs, namely VideoChatGPT , VideoChat , VideoChat2 , ST-LLM , PLLaVA 7B , VILA-1.5 8B . For open-source image and video LLMs, we sample 4 and 16 frames (following VideoChat2) per video respectively. We also evaluate sophisticated close-source models, namely Claude 3-Sonnet and Opus , GPT-4V , Claude 3.5-Sonnet , Gemini-1.5-Pro , GPT-4O . For the close-source models, we sample 10 frames per video because GPT-4V can only accept a maximum of

    &  & MSR-VTT \\   & Precision & Recall & F1 & Recall \\ 
2 scenes & **75.8** & 40.9 & 53.1 & 122.0 \\
3 scenes & 74.1 & 49.5 & 59.4 & 135.8 \\
4 scenes & 72.3 & 55.8 & 63.0 & 138.1 \\
5 scenes & 71.4 & **57.5** & **63.7** & **139.5** \\  Whole & 79.1 & 26.8 & 40.0 & 83.0 \\ Whole (voice) & **80.3** & **27.7** & **41.1** & - \\   

Table 2: Different strategies of video-script alignment and voice-over transcription.

10 frames. We also introduce a special setting for GPT-4O that we sample with 1 fps or a maximum of 100 frames (1 fps/100 fr). More details about Vript-Hard can be checked in Appendix E.

### Vript-HAL: A Hallucination Evaluation Benchmark for Video LLMs

Evaluating Hallucinations in Video LLMsPrevious researchers  have explored methods to detect and evaluate hallucinations of powerful image LLMs. Similar to image LLMs, current video LLMs have a deeper understanding of videos and a stronger ability to generate more detailed captions for videos but also suffer from severe hallucinations. If we ask the video LLMs to describe a video, they may misread the objects and actions and generate a description with hallucinations. Captioning benchmarks, e.g., MSR-VTT  and MSVD , consist of short captions of no more than 10 words, giving superficial video descriptions without details. Thus we can not use them to evaluate hallucinations if many objects and actions are not included in the ground truth. To fill this gap, we construct Vript-HAL, a benchmark to evaluate object and action hallucinations in the video captions. Each video in Vript-HAL is annotated with two captions separately, approximately 250 words each, which are 25x longer than those in MSR-VTT. By building such strong ground truth captions, we can check if the video LLMs generate hallucinations in the captions.

Hallucination Evaluation MetricsTraditional metrics, such as BLEU , ROUGE , and CIDEr , focus on word-for-word precision by measuring the token similarity between the predicted and ground truth texts, which are not suitable for evaluating if the objects and actions are correctly described. Following previous works , we evaluate whether the nouns (objects) and verbs (actions) are correctly described in the captions by using the precision score. In addition to evaluating accuracy through precision, it is noted that various models give descriptions varying in length and detail. We observe that shorter captions typically include fewer details thus tending to contain fewer hallucinations. To balance this, we introduce the recall score, which measures how many objects and actions in the ground truth are correctly described. We calculate the F1 score as the comprehensive score of hallucination evaluation as follows:

\[(,)=\}}{\#\{ \}},(,)= \}}{\#\{\}}, F_{1}=2 }{+}, \]

Figure 4: The precision and recall scores of various models on Vript-HAL. The sizes of the circles stand for the F1 values. The full results can be checked in Table 6.

where \(\#\{\}\) and \(\#\{\}\) represent the number of objects and actions described in the prediction and ground truth caption respectively. We leverage the SpaCy 3 to extract the nouns, proper nouns, and verbs as the objects and actions. \(\#\{\}\) represents the number of objects and actions that are correctly described in the prediction. We then encode the objects and actions into word embeddings using the sentence-transformers 4. Instead of using the exact match, for each object or action, we consider it to be correctly described if the cosine similarity between the prediction and the ground truth is greater than 0.5. It is noted that using similarity may result in many-to-one matching because objects or actions with similar meanings in the prediction are all matched by one object or action in the ground truth, potentially yielding a score greater than 1 if the prediction is much longer than the ground truth, e.g., the recall score in MSR-VTT in Table 2.

EvaluationWe evaluate a large range of models on Vript-HAL, including image LLMs supporting multiple image inputs and video LLMs. From Figure 4, we observe some models, e.g., BLIP2 and VideoChat 2 have fewer hallucinations only because they give shorter captions containing fewer details. Vriptor-W (whole-video) giving general descriptions has a higher precision while Vript-S (scene-by-scene) giving dense descriptions describes more details in the videos with a higher recall. Both models have performance on par with the GPT-4V in video captioning.

### Vript-RR: A Hard Reasoning Benchmark for Long Video Understanding

Retrieving the Scene then Reasoning the AnswerIf we ask about details in the long video, we may encounter ambiguity in the questions that: 1) there are multiple answers that match the question in the different timestamps; 2) the answer changes as time goes on. The ambiguity issue can be commonly seen in the long video understanding benchmarks, e.g., EgoShecma . We propose Vript-RR (Retrieval then Reasoning), a long video reasoning benchmark that has no such worries. Different from these benchmarks  that only provide questions, we first give a hint for the model to locate the scene in the video that the question refers to. The hint is a detailed description of the relevant scene. We then ask the question based on the scene, which eliminates the ambiguity. In practice, as shown in Figure 7, we input the hint and the question along with the entire video **together**, and the models directly output the answer, which is an end-to-end process. We carefully craft the hints to ensure the model can not find short paths through hints. We design various questions for Vript-RR to evaluate the different capabilities of video LLMs, where each question requires at least one reasoning step or additional processing, e.g., text reading, and meticulous inspection of details.

EvaluationVript-RR consists of two subtasks differing in the video inputs: one is inputting the whole videos and another is directly inputting the related scenes. Vript-RR provides questions both in multiple-choice and open-ended formats. For the open-ended outputs, we leverage the GPT-4 turbo  as the judge  to evaluate if the answer is correct by comparing the prediction with the ground truth. As shown in Table 3, the "Scene" columns represent using the related scene as input, which is an easier task because the models do not need to retrieve across the entire video to find the related scene. The results of the "Scene" columns mainly showcase the models' video reasoning ability. For "Whole" columns using the whole video as input, we require models to first find the relevant scenes using the hint, requiring the additional long video understanding ability. The closed-source models like GPT-4V and Claude 3 have better performance than open-source video LLMs.

Finding A "Needle" In A "Timestack"For each video in Vript-RR, we design the questions for scenes extracted from four various timestamps, corresponding to 15%, 40%, 60%, and 85% of the video respectively. We want to explore whether the temporal positions of scenes in the long video will influence the results of Vript-RR. We describe it as finding a "needle" in the "timestack", whose name is derived from the "needle-in-a-haystack" task  for testing the long-context ability of LLMs. We require models to go through visual tokens instead of text tokens to find the "needles" (related scenes). In the "needle-in-a-haystack" task, there is a phenomenon that the model performance dropssignificantly when the "needle" falls between 15% and 85% of the long context, particularly when the text length exceeds at least 16K tokens. As shown in Figure 5 (a), though the number of visual tokens is significantly smaller than 16K, performance drops are also observed for most of the models if the scenes fall in the middle of the visual tokens (40% and 60% of the video).

### Vript-ERO: A Temporal Understanding Benchmark of Long Videos

Re-ordering the Events in Different TimestampsThere have been some benchmarks [5; 19; 7] that test the temporal understanding ability of the models. Unfortunately, they focus on asking questions about the temporal order of the actions happening in a short clip but few explore the temporal understanding of events in the long videos. To fill the gap, we propose the Vript-ERO (Event Re-ordering) task. We sample three distinct scenes (lasting 10s on average) in different timestamps from a long video (varying from 2min to 2h) and shuffle their chronological order. Given the long

   &  &  \\   & Scene-M & Scene-O & Whole-M & Whole-O & @1 & @2 & @3 \\  VideoChatGPT  & 34.2 & 28.9 & 29.6 & 17.8 & - & - & - \\ Video-LLaMA  & 38.2 & 19.7 & 28.3 & 14.5 & - & - & - \\ VideoChat  & 33.6 & 23.0 & 22.4 & 15.1 & 46.2 & 17.1 & 17.1 \\ VideoChat2  & 52.0 & 32.2 & 42.1 & 22.4 & - & - & - \\ ST-LLM  & 43.4 & 34.9 & 33.6 & 26.3 & - & - & - \\ PLLaVA 7B  & 62.5 & 46.1 & 55.3 & **36.2** & - & - & - \\ VILA-1.5 8B  & **75.0** & **48.7** & **55.3** & 32.3 & - & - & - \\  Claude 3-Sonnet  & 60.5 & 53.9 & 56.6 & 42.1 & 67.9 & 24.6 & 19.4 \\ Claude 3-Opus  & 63.8 & 60.52 & 60.5 & 43.4 & **70.2** & 26.9 & 23.9 \\ GPT-4V  & **80.9** & **75.0** & **71.7** & **61.0** & 59.2 & **28.4** & **27.7** \\  Claude 3.5-Sonnet  & 80.9 & 59.2 & 54.6 & 42.8 & 56.7 & 18.7 & 6.7 \\ Gemini-1.5-Pro  & 85.1 & 68.5 & 59.9 & 47.5 & 35.1 & 18.2 & 9.1 \\ GPT-4O  & 92.1 & 77.5 & 72.4 & 54.6 & 75.0 & 32.6 & 32.6 \\ GPT-4O (1 fps/100 fr) & 91.4 & 78.2 & 78.5 & 66.0 & 81.0 & 40.2 & 38.6 \\  

Table 3: The metric of Vript-RR and Vript-ERO is accuracy. In Vript-RR, "M" and "O" stand for multiple-choice and open-ended questions respectively. In Vript-ERO, "@x" denotes the number of positions correctly predicted in the order of three shuffled scenes at different timestamps.

Figure 5: (a) The accuracies of Vript-RR questions regarding scenes at different timestamps (15%, 40%, 60%, and 85% of the video). (b) The reasons why the models (GPT-4V, Claude 3-Sonnet, and Opus) sequence the events inaccurately in Vript-ERO.

video and the detailed descriptions of shuffled three scenes, the model is required to give the correct temporal order of the scenes based on the understanding of the entire video.

EvaluationIn Table 3, "-" means these models fail to give answers. Different from previous tasks that only have questions, Vript-ERO also contains long descriptions of scenes, which indicates these models are weak in processing long instructions. For models having scores, they only give the correct orders of all three scenes (@3) in about 20% of questions. In Figure 5 (b), we collect answers to the questions that are answered incorrectly and analyze the reasons. We observe that the models can be easily misled by the provided descriptions. For example, environment descriptions like sunlight may imply the morning or evening, however, these events may come from different days in the video rather than sequentially happening in one day. In 31.4% of cases, some events are absent in the input frames due to the limitation of the number of input images for models like GPT-4V. Besides, in 25.1% of cases, the models do not recognize which scene to be sequenced based on the descriptions. For the GPT-4O (1 fps/100 fr), which operates at 1 fps or 100 frames, an increased number of frames within the input significantly enhances the overall scores. This is because the probability of the relevant events being omitted decreases with a larger input frame count.

## 6 Conclusion

We introduce Vript, a high-quality video-text dataset consisting of dense and detailed captions for videos. Based on Vript, we train Vriptor, a top-performing video captioning model among open-source models. Besides, we propose Vript-Hard, a challenging video understanding benchmark evaluating hallucinations and the long video understanding ability of video LLMs.