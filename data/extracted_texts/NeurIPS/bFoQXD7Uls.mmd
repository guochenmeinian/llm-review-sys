# VeLoRA: Memory Efficient Training using

Rank-1 Sub-Token Projections

 Roy Miles\({}^{}\)   Pradyumna Reddy   Ismail Elezi\({}^{}\)   Jiankang Deng

Huawei Noah's Ark Lab

###### Abstract

Large language models (LLMs) have recently emerged as powerful tools for tackling many language-processing tasks. Despite their success, training and fine-tuning these models is still far too computationally and memory intensive. In this paper, we identify and characterise the important components needed for effective model convergence using gradient descent. In doing so we find that the intermediate activations used to implement backpropagation can be excessively compressed without incurring any degradation in performance. This result leads us to a cheap and memory-efficient algorithm for both fine-tuning and pre-training LLMs. The proposed algorithm simply divides the tokens up into smaller sub-tokens before projecting them onto a fixed 1-dimensional subspace during the forward pass. These features are then coarsely reconstructed during the backward pass to implement the update rules. We confirm the effectiveness of our algorithm as being complimentary to many state-of-the-art PEFT methods on the VTAB-1k fine-tuning benchmark. Furthermore, we outperform QLoRA for fine-tuning LLaMA and show competitive performance against other memory-efficient pre-training methods on the large-scale C4 dataset. Code: https://github.com/roymiles/VeLoRA

## 1 Introduction

Large language models (LLMs) have achieved remarkable success on a wide range of natural language processing tasks . However, training these massive deep learning models remains computationally expensive, requiring vast amounts of data, compute, and memory resources. A key bottleneck for training or adapting these models is the large memory needed to store all the intermediate features required to compute the gradients for optimization. This makes it challenging to fully leverage the scalability and performance gains promised by larger models on currently available hardware.

Several techniques have been proposed to reduce the memory requirements, such as GaLore , gradient checkpointing , reversible backpropagation , parameter-efficient finetuning , quantization  and activation offloading . GaLore uses a low-rank projection of the gradients during training to reduce the memory footprint. Gradient checkpointing reduces the activation memory demands by recomputing the activations in the backward pass instead of storing them. While these methods are promising and lower the memory cost, they also might introduce a substantial computational overhead, are limited in their memory savings, or require specialized hardware . Knowing that compute is the primary mover for the advancements in machine learning , it is to be expected that the LLM sizes will continue growing exponentially. Thus, it is imperative to develop more efficient and scalable methods that allow better utilization of compute power and training data.

In this work, we present a novel approach for efficient training and finetuning, which we name **V**ector projected **LoRA** (VeLoRA). Our approach is based on a key observation that the intermediateactivations produced during the forward propagation of deep neural networks, and kept in memory for computing the gradients during backpropagation, can be effectively represented and reconstructed from a single and fixed one-dimensional vector without losing any accuracy. This compressed representation can be made very memory efficient, with a controllable hyperparameter that provides a trade-off between the compression ratio and the model's performance. By compressing and then reconstructing the activations on the fly, our method reduces the peak activation memory footprint to a tiny fraction of what is required to store the original activations. This enables fitting much larger models into limited GPU memory compared to approaches like GaLore or gradient checkpointing.

More concretely, during the forward pass, we propose to divide each input token into a set of much smaller sub-tokens. Using a single projection vector, we then project these individual sub-tokens onto a one-dimensional subspace. Importantly, we notice that we can initialize this projection vector cheaply using first-order batch statistics and then keep it fixed throughout training. We then reconstruct the original tokens using the same vector during the backward pass. Although this reconstruction loses the original gradient properties such as the direction or magnitude, we find that it jointly encourages sparsity and locally preserves the gradient similarity, which we attribute to the overall effectiveness of the algorithm. By storing these compact representations, we can substantially reduce the memory footprint of the network during training, enabling the accommodation of larger models on hardware with limited memory capacity.

Our **contributions** are the following:

* We propose a novel compression method that reduces the memory requirement for gradient computation during training and fine-tuning of large neural network models like LLMs.
* We show that, unlike other methods, our compression method does not need expensive operations such as SVD and gradient checkpointing.
* We achieve state-of-the-art results in various benchmarks while requiring lower GPU memory compared to the baseline methods.

## 2 Related Work

Memory-Efficient Training.The increase in model size has necessitated the development of smart methods that make training more memory efficient. Gradient checkpointing  significantly lowers the memory requirements during model training by recomputing activations for the backward pass instead of storing them during the forward pass. However, doing so increases the training time from the need to re-compute gradients. Adafactor  and its followup  lowers the memory by working with the row-column outer-product factorized moments of adaptive optimizers. LOMQ  was developed for large models and works by fusing the gradient computation and the parameter update in one step to reduce memory usage, effectively only saving the current layer gradients in memory. Recently, GaLore  proposed projecting the gradients onto a lower-dimensional space [9; 6], and can reduce the memory during both pre-training and finetuning. However, they store all the full intermediate activations to compute gradient updates. Their memory advantage is derived from

Figure 1: The memory overhead for backpropagation on a single layer consists of storing the intermediate activations and the weights. (a) demonstrates that PEFT methods can reduce the memory by using cheap low-rank adapters. (b) VeLoRA additionally compresses the saved intermediate activations to further reduce the memory usage.

computing the first and second-order statistics of the gradients in a lower-dimensional space, thus limiting it to second-order optimizers only. Furthermore, GaLore needs an expensive SVD operation that introduces some significant overheads in terms of both memory and computation costs. Unlike these methods, VeLoRA does not introduce any large computation overhead while at the same time comes with a significant memory reduction. Furthermore, VeLoRA is in principle independent of the underlying optimizer.

Parameter-Efficient Fine-Tuning (PEFT)is an emerging field that focuses on fine-tuning a large model with a minimal number of trainable parameters. This typically involves freezing and then augmenting the original model with adapter modules. LoRA (Low-Rank Adaptation)  is a technique that optimizes a few rank-decomposed weight matrices during fine-tuning, rather than updating the entire set of pre-trained weights for each attention layer. This approach substantially reduces the number of trainable parameters, thereby accelerating the fine-tuning process and making it more memory-efficient. The method was later extended to also work with multi-layer perceptrons in Transformers [21; 13]. Several other methods built upon these works improving the capacity or performance of the model [21; 27; 7; 42; 35; 52; 49; 26; 45; 17]. These works can be well-complemented with quantization, further reducing the memory while keeping the performance [10; 11; 24]. Our memory-efficient algorithm is complementary to PEFT and can be used to provide additional memory efficiency in the fine-tuning regime.

Subspace trainingIn [22; 15], the authors show that most of the learning process occurs within a significantly low-rank parameter space and that model weights can be effectively optimized within this lower-dimensional subspace. These subspace learning techniques have been widely adopted in various machine learning domains, including meta-learning  and continual learning . However, unlike VeLoRA, resource-efficient training/fine-tuning is not the focus of these methods, therefore, often resulting in an overhead to meet other requirements.

Gradient Sparsification.Recently, there has been a surge in interest for memory-efficient training methods. In  only a sparse subset of the gradient vector components are stored zeroing out the remaining components. Different criteria have been proposed for selecting which gradient components to retain, such as Top-K SGD  which keeps only the top-k largest components, Sparsified-SGD  and various other sparsification methods [37; 38; 40; 25; 28; 16]. More recently, techniques combining quantization and sparsification have been proposed for resource-efficient training. Examples include TernGrad , Qsparse-local-SGD , sparse ternary compression , and the sparse-signSGD  method which combine sparsity with quantizing gradients to just the sign. A key difference is how VeLoRA compresses the intermediate activations that are used to compute gradients. Our compression algorithm is fully dense-to-dense without any pruning or sparsification of the activations. This prevents accuracy degradation issues associated with sparse updates and facilitates memory-efficient training.

## 3 Method

**The task.** In this work, we propose a memory-efficient modification to the back-propagation algorithm. Our primary motivation is to reduce the GPU memory footprint during training without

Figure 2: (a) Stable rank for the input activations using a different number of groups, with \(=1\) indicating no sub-division of the tokens into smaller sub-tokens. (b) Approximate probability of the feature similarity diverging by at least \(k\). (c) visualisation the rank-1 projection of sub-tokens.

resorting to any hardware-specific quantization schemes  and without trading compute for memory as is done with gradient checkpointing .

To formalize the problem statement, let us take a step back and look at the components needed to implement back-propagation. Firstly, during the forward pass, each trainable layer in the neural network needs to store two key tensors in memory - the input activations received by that layer, and the layer's trainable weight parameters. Retaining these tensors is required for performing the parameter update rules and computing the input gradients. More specifically, during the backward pass, the previously stored input activations and model weights are used to calculate the gradients with respect to the weights and the input via the chain rule of differentiation (see Fig. 1). Storing both these sets of tensors comes with a significant memory overhead which scales with the model size. We focus on optimizing the cost of storing the input activations. We do this by compressing intermediate activation vectors and then reconstructing them when the original activations are needed for gradient calculations. This is orthogonal to PEFT  methods which address the overhead of saving the full-precision weights in memory by introducing cheaper trainable adapters. Further, in Section 4, we show how to combine our method with PEFT methods to achieve state-of-the-art results.

### VeLoRA

**Overview.** Here we address the challenge of compressing intermediate activations tensors while preserving the necessary training dynamics for model convergence. Our memory-efficient algorithm consists of two components: (i) The grouping strategy to divide the original high-dimensional tokens into much smaller sub-tokens; and (ii) Fixed rank-1 projections of these sub-tokens using cheap heuristically initialized principal components. Given a large pre-trained model, we apply these steps to compress the intermediate activations saved during training while preserving most of the original model's training dynamics. We illustrate the general overview of this pipeline in Fig 1 and show PyTorch-like pseudo-code in Algorithm 1.

Consider a set of input activations that need to be saved in GPU memory during the forward pass. We denote an element in this set as \(_{i}=_{w}f(_{i};w)^{N D}\), where \(N\) is the number of tokens, and \(D\) is the feature depth. We propose to introduce a simple grouping (reshape) operation that partitions the tokens across the depth dimension: \(group():^{B N D}^{B ND/M M}\) with \(M\) being the new size of each token, now coined a sub-token. This operation can be described as partitioning a batch of \(N\) tokens into a collection of these much smaller non-overlapping \(ND/M\) sub-tokens. Then we project each of the sub-tokens onto a rank-1 subspace. The idea is that this grouping operation enables a much lower-dimensional fixed subspace to be used throughout training without any degradation in model convergence or performance. We describe the compression steps concisely as follows:

\[[]{group()}^{B ND /M M}[]{compress(:)}_{p} ^{B ND/M 1},\] (1)

where \(\) is used to denote the sub-tokens of \(\) and \(_{p}\) are the compressed sub-tokens. This compression is achieved using the function \(compress(\ ;\ )=\), which projects each sub-token onto a one-dimensional sub-space before saving them in memory. Since \(M<<D\) it is more memory efficient to store \(_{p}\) instead of \(\). The initialization strategy for \(\) is important for the performance of the model, however we later show that a simple and cheap average over the first batch of sub-tokens can be very effective. Finally, the compressed sub-tokens \(_{p}\) are reconstructed for the gradient calculation during backward pass as follows:

\[_{p}[]{ reconstruct(:)}} ^{B ND/M M}[]{ungroup()}}^{B N D},\] (2)

Here \(}\) and \(}\) refer to the reconstructed sub-tokens and tokens of \(\) and \(\) respectively. The \(reconstruct\) function projects the sub-tokens \(_{p}\) back onto the vector \(\) as a very coarse reconstruction of \(\) and it is defined as \(reconstruct(_{p}\ ;\ )=_{p}^{T}\). The overall compression and reconstruction operation is given as \(proj_{}()=()^{T}\), where \(^{M 1}\) is a fixed vector of unit length.

To summarize, during the forward pass, we _compress_ the intermediate activation tensor \(\) into a compact representation \(_{}\) using \(\). Then, in the backward pass when the original activation \(\) is needed for gradient computation, we reconstruct an approximation \(}\) by projecting \(_{p}\) back onto \(\).

These steps are fundamentally different and complementary to recent works that leverage the low-rank property of gradients like GaLore  in two ways: Firstly, they store the uncompressed intermediate activations in memory for the gradient computation. In contrast, we compress these activations explicitly during the forward pass. Secondly, GaLore relies on periodically computing the principal components with SVD to optimally capture the underlying gradient subspace. Our compression method avoids such costly operations, making it much more efficient and scalable.

### Insights and properties of VeLoRA

**On the importance of compressing sub-tokens.** Computing the optimal low-rank subspace of the gradients using SVD is very computationally and memory intensive and often needs offloading the operation to a CPU . Moreover, periodically updating the projection may be necessary to track any shift in the gradient distribution . This is why dividing the tokens up into smaller sub-tokens is necessary. By doing so, it allows us to use a cheap surrogate rank-1 projective map that is initialised and frozen throughout training. Finally, one surprising observation of this grouping operation is that the sub-tokens will naturally lie on a higher-dimensional subspace than the original tokens (see figure 1(a)). Thus, our method cannot be faithfully explained through a better reconstruction of the gradients, but instead by a suppression of the inherently larger eigenvalues that can in turn help reduce overfitting.

**Why does a vector projection make sense?** Using a fixed vector projection throughout training fails to capture any shift in the gradients' distribution. Under the assumption that preserving the original gradient reconstruction is important, this may seem like an undesirable trait. However, quite surprisingly, we find that this shift does not hinder the model's convergence or performance at all. An explanation behind this phenomenon can be twofold: (i) The gradients become more sparse as they shift away from the initial distribution and this helps prevent the model from overfitting to the fine-tuned dataset; (ii) Although the vector projection destroys the original gradients' magnitudes and directions, it still locally preserves the gradient similarity and this similarity will govern the model's training dynamics .

Consider a rank-1 decomposition of two sub-tokens: \(z_{i}\) and \(z_{j}\). We will use the dot-product as the similarity measure \(sim()\) for which we wish to locally preserve. Let us assume that both \(_{i}\) and \(_{j}\) are distributed such that the angles between them and the vector \(\) are normally distributed with a mean of \(0\) and a standard deviation \(\). With a first-order approximation, the probability of the projection and reconstruction scaling the gradient similarity by at least \(k\) is given as follows (see the Appendix for the full derivation):

\[Pr(|sim(proj_{}(_{i}),proj_{}(_{ j}))-sim(_{i},_{j})|>k) 2(1-( }{}))\] (3)

With \(k>0\) and \(>0\), this probability is bounded by \(\). Here we can see that similarity is trivially preserved in the limit as \( 0\). This indicates that the sub-tokens already lie on a 1-dimensional subspace spanned by \(\). To further see how these gradient similarities diverge for various values of \(k\) and \(\), we plot equation 11 in Fig. 1(c). We empirically observe that although the gradient similarity is very dependent on the distribution of features, this non-linear relationship does not hinder the model's ability to converge and generalise. Finally, we also provide an illustrative visualisation in Fig. 1 (right) that shows the locality sensitivity for preserving gradient similarity and the sparsification of gradients when they are orthogonal to the vector \(\). Both of these components are important properties that we attribute to the effectiveness of VeLoRA.

**Connection to parameter efficient fine-tuning.** Although VeLoRA is complimentary to LoRA, it can indeed be viewed under the same umbrella. First let us consider LoRA, which will freeze the original weights and only update a low-rank adapter:

\[y=Wx+ABx=(W+AB)x\] (4)

Following the same analysis from FLoRA , we will freeze \(A\) and initialise \(B\) with all zeroes. i.e. \(A=A_{0}\) and \(B_{0}=0\). The weight update rule can then be given as follows:

\[ W^{}=W+A_{0}(B_{0}-)A_{0}A_{0}^{T}},\] (5)

with learning rate \(\) and \(=\) - see FLoRA  for the original full derivation under the small learning rate assumption. In contrast, VeLoRA (with \(M=D\) i.e. no sub-tokens) will update the original weights directly but with compressed gradients:

\[((  v)v^{T})=()vv^{T},\] (6)

which leads to the following similar weight update rule to equation 5:

\[ W^{}=W-=vv^{T}}\] (7)

This result highlights that VeLoRA is a special case of LoRA with a data-driven initialisation for \(A_{0}\). Furthermore, due its construction, VeLoRA is implemented using a custom forward and backward operation rather than by modifying the architecture and fusing weights after training. Finally, **VeLoRA also provides additional compression through having a smaller shared projection \(v\) for each sub-token**. This would resemble reshaping the input tensor before and after the LoRA adapter to enable smaller projection matrices.

**Cheap initialisation strategies.** GaLore  proposes to use the periodically updated SVD principle components to track any shifts in the underlying sub-space of the gradients. Unfortunately, this SVD operation can be very expensive both in terms of memory and compute. Another limitation is that SVD may fail to converge, and it requires casting the features back to 32-bit for numerical stability . For this reason, we propose a cheap initialisation strategy. We relax the constraint on tracking the feature distribution. For all of our experiments, we use a rank-1 decomposition of sub-tokens and propose to initialize the vector \(\) using the average of sub-tokens from the first batch.

## 4 Comparison with the state-of-the-art

In this section, we thoroughly evaluate VeLoRA and its individual components. In section 4.2 we demonstrate the complementary nature of VeLoRA in conjunction with many other existing PEFT methods. Section 4.4 then scales up these results to the LLaMA models, whereby we achieve a significant memory reduction when coupled with 4-bit scalar quantisation. Finally, section 4.5 extends VeLoRA to the pre-training setting where we see competitive performance alongside a real reduction for the on-device GPU memory usage.

For the VTAB-1k experiments, we applied VeLoRA to all the down projections in the trainable adapters, while for SSF we applied it to the input scale and shift layers only. For all the other experiments we simply applied VeLoRA to the value projection layer and the down projection layer of the MLP blocks.

### Implementation details

We performed all the vision experiments on a subset of the VTAB-1k  benchmark for a combination of 16 different vision datasets. We finetuned a ViT-B  model pre-trained on ImageNet-21K using the AdamW optimizer with a learning rate of 5e-4 and a weight decay of 1e-4. We performedthe language models experiments on the GLUE  benchmark using the encoder-decoder RoBERTa transformer . We then scaled our model to large language models causal transformers using the LLama  family of models, finetuned in Alpaca dataset  and reported results on the MMLU benchmark . We finally applied our method to the pre-training stage using some smaller LLama  models on the C4 dataset . All our models were trained using the AdamW optimizer with a learning rate of 1e-3 and a weight decay of 0. We give all the other major hyper-parameters to replicate these experiments in the Appendix and also in the code.

### Vision experiments

We conduct experiments evaluating the performance of VeLoRA for full-tuning and how it complements other PEFT methods. In Tab. 1 we reproduce a large set of results for LoRA , SSF , and Hydra  on a subset of the VTAB-1K benchmark, where the sub-token size for each experiment is given in the Appendix. Unlike what is more common in the PEFT literature [20; 21], we do not perform any task-specific hyperparameter tuning that would change the memory, such as batch size and rank, and to also avoid any potential overfitting to the specific task. For all experiments we used the authors provided implementations for the adapters and integrated them into the same training framework for a fair comparison. We observe that VeLoRA improves the performance compared to full-tuning by \(1.5\) percentage points (\(pp\)), while lowering the memory requirements. We also observe that when combined with PEFT methods, VeLoRA can come with improvement in memory and performance. VeLoRA lowers the memory requirement of SSF  by 16% with only a minor degradation (\(0.1pp\)) in accuracy. It lowers the memory requirement of Hydra  by 7% while improving the accuracy by \(0.1pp\). Finally, it lowers the memory requirements of LoRA  by 4% while improving the accuracy by \(0.6pp\).

### Roberta experiments

We now evaluate our method with \(M=16\) on various language tasks, using RoBERTa-Base  in the GLUE benchmark, and compare it with full fine-tuning, LoRA  and GaLore , presenting the results in Tab. 2. We observe that both GaLore and LoRA lower the memory requirements compared to fine-tuning from 4.64GB to 4.04GB, respectively to 2.71GB, at a cost of accuracy degradation with GaLore performance lowered by 0.39 \(pp\), while LoRA accuracy drops by 0.67 \(pp\)

    &  &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  _Method_ & & & & & & & & & & & & & & & & & & \\  Full tuning & 4.25 & 89.4 & 53.3 & 66.1 & 97.3 & 87.3 & 90.7 & 39.2 & 83.2 & 95.3 & 86.1 & 75.4 & 62.8 & 47.2 & 77.5 & 31.2 & 32.8 & 69.7 \\ + VolLoRA & 4.02 & 89.9 & 55.9 & 67.8 & 97.2 & 88.4 & 90.4 & 38.9 & 15.8 & 95.8 & 86.7 & 75.7 & 74.7 & 50.2 & 77.9 & 31.3 & 31.6 & 71.2 (1.15) \\ Linear pooling & 1.84 & 41.6 & 86.4 & 65.9 & 97.6 & 87.2 & 36.8 & 51.1 & 79.0 & 88.4 & 72.9 & 74.0 & 34.1 & 34.8 & 59.6 & 13.2 & 22.9 & 59.1 \\  SSF & 4.13 & 89.4 & 74.0 & 72.9 & 99.2 & 91.1 & 80.7 & 56.0 & 83.3 & 94.8 & 85.3 & 75.6 & 78.5 & 45.0 & 76.9 & 23.0 & 36.9 & 72.7 \\ + VolLoRA & 3.46 & 89.1 & 74.3 & 70.9 & 99.1 & 91.3 & 80.8 & 56.3 & 82.8 & 94.9 & 85.4 & 74.8 & 76.6 & 44.7 & 75.5 & 24.6 & 36.5 & 72.6 (1.01) \\ Hybrids & 3.10 & 93.7 & 72.6 & 70.9 & 99.2 & 91.3 & 88.6 & 55.7 & 32.3 & 95.2 & 85.1 & 76.1 & 81.9 & 51.7 & 78.9 & 34.5 & 40.5 & 74.7 \\ + VolLoRA & 2.28 & 91.0 & 72.8 & 70.6 & 99.2 & 91.4 & 88.2 & 56.0 & 83.2 & 94.9 & 84.3 & 75.9 & 82.5 & 51.6 & 79.9 & 34.2 & 41.4 & 43.8 (7.01) \\ LoRA & 2.86 & 93.3 & 64.7 & 68.8 & 99.1 & 90.0 & 82.3 & 52.6 & 81.7 & 95.3 & 83.7 & 74.4 & 80.4 & 47.3 & 77.9 & 28.0 & 38.1 & 72.1 \\ + VolLoRA & 2.74 & 88.9 & 67.3 & 69.6 & 99.1 & 90.7 & 83.5 & 53.3 & 81.9 & 95.2 & 83.4 & 74.3 & 79.8 & 47.1 & 78.9 & 29.7 & 40.3 & 72.7 (1.66) \\   

Table 1: Results on a subset of the VTAB-1k benchmark. All methods use a ViT-Base-224/16 model pre-trained on ImageNet-21k. The batch sizes and ranks are the same across all tasks.

    & **Memory (GB)** & **CoLA** & **STS-B** & **MRPC** & **RTE** & **SST2** & **MNLI** & **QNLI** & **QQP** & **Avg** \\  Full Fine-Tuning & 4.64 & 62.24 & 90.92 & 91.30 & 79.42 & 94.57 & 87.18 & 92.33 & 92.28 & 86.28 \\  GaLore & 4.04 & 60.35 & 90.73 & **92.25** & **79.42** & 94.04 & **87.00** & **92.24** & 91.06 & 85.89 \\ LoRA & 2.71 & 61.38 & 90.57 & 91.07 & 78.70 & 92.89 & 86.82 & 92.18 & **91.29** & 85.61 \\ VeLoRA & **2.23** & **64.56** & **90.81** & 91.26 & 77.98 & **94.38** & 86.29 & 92.09 & 89.91 & **85.91** \\   

Table 2: Comparison of our method with full fine-tuning, GaLore and LORA on GLUE benchmark using pre-trained RoBERTa-Base. Our method reaches the best overall results while showing significant memory improvements, especially compared to GaLore. We bold the best results from the considered PEFT methods. The GPU memory is measured on-device.

Our method further reduces the memory needed for training to 2.23GB, an improvement of 18% compared to LoRA, and 45% compared to GaLore, while still reaching higher results than either of them. More impressively, VeLoRA reduces the memory by half compared to full fine-tuning with an accuracy degradation of only 0.37 _pp_, reaching the best tradeoff between memory and accuracy.

### Scaling up to LLaMA

We now scale our method to large language models, demonstrating the effectiveness of VeLoRA in finetuning them. We do comparisons with LoRA on both BFloat16 and Float4, in addition to the recent method of QLoRA  which is widely used for fine-tuning LLMs with very low memory budget. We aim to further lower this budget, showing in the process that VeLoRA is also complementary to QLoRA, resulting in a much lower memory consumption. We present the results in Tab. 4.4 using \(M=32\) for 7B and \(M=128\) for 13B. We can see that our method outperforms QLoRA by 0.5_pp_ in the Llama model, while reaching a massive performance increase compared to LoRA models. Furthermore, we reach this performance improvement, while at the same time further reducing the memory. In particular, we reduce the memory for 0.89GB, a relative improvement of 15.4% from QLoRA. We observe that this performance improvement is maintained on the larger model of 13B parameters, where again our method outperforms QLoRA by 0.5_pp_ and lowers the memory requirements by 1.43GB, a relative improvement of 14.4%.

### Pre-training on C4

We now perform an experiment where we use VeLoRA to train language models from scratch in the large C4-English dataset, presenting the results in Tab. 4. We use \(M=128\) for both the 60M and 130M, while following the same training pipeline and evaluation as GaLore and comparison with LoRA. However, unlike in the GaLore paper , which estimates the memory usage using the optimizer weights and memory alone, we choose to compute the real on-device memory. This quantity would take into account the cost of additionally storing the intermediate activations and also highlight the benefits of LoRA in terms of memory since the base weights will be frozen. In contrast to other experiments, our use of VeLoRA here is not with any additional adapters and is simply compressing the input activations for the original trainable base layers. We observe that our method significantly outperforms the other methods, reaching 1.08 _pp_ lower perplexity than GaLore. We observe that our method outperforms GaLore in Llama-130M too.

## 5 Ablations Studies

### Convergence properties

We observed that the rank-1 projections would encourage much higher levels of gradient sparsity (see Fig. 1(b)). A natural question to ask from this observation is if the gradient sparsity will come at the cost

    & 60M & 130M \\  Full-Rank & 33.52 (1.30G) & 25.08 (2.32G) \\  GaLore & 34.88 (1.27G) & 25.36 (2.02G) \\ LoRA & 34.99 (0.86G) & 33.92 (1.24G) \\ FLoRA & 34.35 (1.27G) & 25.88 (2.01G) \\ VeLoRA & **33.76** (1.18G) & **25.29** (1.83G) \\  \(r/d_{model}\) & 128 / 256 & 256 / 768 \\ Training Tokens & 1.1B & 2.2B \\   

Table 4: Comparison with low-rank algorithms on pre-training various sizes of LLaMA models on C4 dataset. Validation perplexity is reported, along with the on-device GPU memory usage.

   LLaMA Size &  &  & Mean \\ Method & Alpaca & Memory & Alpaca & Memory & \\  LoRA w/ BFloat16 & 38.4 & 8.79 & 47.2 & 15.82 & 42.8 \\ LoRA w/ Float4 & 37.2 & 5.77 & 47.3 & 9.91 & 42.3 \\ QLoRA & 39.0 & 5.77 & 47.5 & 9.91 & 43.3 \\ + VeLoRA & **39.5** & **4.88** & **48.0** & **8.48** & **43.8** \\   

Table 3: Mean 5-shot MMLU test accuracy for LLaMA models finetuned with adapters on Alpaca. The GPU memory estimate consists of the frozen weights, trainable adapters, and input activations.

of the model's convergence. In other words, we want to verify if our model needs to be trained longer to compensate for the gradient compression. To do so, we evaluate the performance of our model at the end of each epoch and compare it with the performance of a competing model, the QLoRA. As shown in Tab. 4(a), VeLoRA and QLoRA improve at the same rate. Our model outperforms QLoRA by \(0.3pp\) by the end of the first epoch, and keeps this improvement rate, outperforming QLoRA by \(0.4pp\) at the end of the training. In this way, we verify that the additional compression of input activations does not affect the model's convergence.

### Sub-token size

We provide an ablation on the impact on the size of each sub-token and the model's performance, showing the results in Tab. 4(b). We can see that there is a sweet spot for which the rank-1 projections of sub-tokens using an average initialisation is very effective both in terms of memory and accuracy. However, if the size of the sub-tokens is too large (i.e. when \(M=D/8\)), the gradients will become too sparse, which will hinder performance (see also figure 2). In contrast, if the size of each sub-token is too small, for example with \(D/64\), there is a more significant memory compression but at the cost of model performance.

### Initialisation strategy

In Tab. 4(c), we show the performance after training with various ways of initialising the vectors for each group. To avoid exceeding memory requirements we sub-sampled the tokens for SVD and we also consider the case of instance-wise initialisation. Although we would have expected better performance since the vector will always align with each incoming batch, we found that it did not lead to any performance improvement. In contrast, doing SVD initialization comes with a drop in performance. This result further confirms that the performance improvement from VeLoRA is not specifically correlated with a lower reconstruction error of the input activations.

### Choice of layers

A key design decision for many Parameter-Efficient Fine-Tuning (PEFT) methods, including LoRA, is where to insert the adapters within the model layers. Since VeLoRA shares this core architectural choice, we aim to provide stronger intuition on VeLoRA's suitability by performing a thorough ablation study analyzing the trade-offs between memory consumption and accuracy when considering different layer placements. We present the results in Tab. 6. We observe that we achieve memory improvement in all cases where we adopt VeLoRA. However, to improve the accuracy, VeLoRA

   Query & Key & Value & Down & Memory (GB) & Acc \\   & — none & & & 1.67 & 38.1 \\  ✓ & & & & 1.42 & 36.2 \\  & ✓ & & & 1.42 & 36.2 \\  & & ✓ & & 1.42 & 36.7 \\  & & ✓ & & 1.01 & 38.9 \\  ✓ & & ✓ & & 1.18 & 37.4 \\  & & ✓ & ✓ & 0.76 & **39.5** \\ ✓ & & ✓ & ✓ & 0.51 & 38.4 \\ ✓ & ✓ & ✓ & ✓ & 0.24 & 37.0 \\   

Table 6: Memory v.s. accuracy trade-off for VeLoRA on different layers. We use a LLaMA-7B trained on alpaca and evaluated on MMLU. We report the GPU memory estimate from the input activations only.

Table 5: All three ablations are done using LLama-7B model. (a) VeLoRA has no loss in performance when trained for fewer or more training epochs than QLoRA despite both reducing the memory footprint. (b) Importance in choosing the correct number of sub-token size to find an optimal memory v.s. accuracy trade-off. Using a GPU memory estimate for the input activations only. (c) Ablating various initialisation strategies for a rank-1 projection and with \(M=D\ /\ 32\).

must be used in the MLP down-projection. A possible explanation is that this layer might suffer from overfitting on the training data or forgetting  from the pre-trained data. Overall, we conclude that applying VeLoRA to the value and down projection appears to be the best choice. We strengthen this claim by using this setting for all other experiments.

### Comparison with gradient checkpointing

We compare VeLoRA to gradient checkpointing, another widely used technique to reduce the memory consumption during training. While both methods aim to minimize memory overhead, gradient checkpointing achieves this by recomputing the original activations during the backward pass, leading to a reduced memory consumption at the cost of additional compute. In contrast, VeLoRA uses a lossy compression of the activations during the forward pass and then reconstructs them in a coarser manner during the backwards, thus avoiding the need for any expensive recomputation. Our results in table 7 show that VeLoRA not only offers a comparable reduction in memory usage but also leads to faster training times compared to gradient checkpointing, as it reduces the recomputation burden and overhead.

## 6 Conclusion

In this work, we proposed VeLoRA, a novel framework that enables the training of networks, including large language models in a highly memory-efficient manner. Our approach compresses intermediate activations during the forward pass and coarsely reconstructs them during backpropagation. VeLoRA complements PEFT methods and is able to significantly reduce memory requirements while improving the performance. VeLoRA is effective when tested in both moderately-sized vision transformers as well as in large language models. We performed experiments to demonstrate the method's effectiveness on VTAB-1K, MMLU, GLUE, and C4 benchmarks outperforming state-of-the-art methods such as LoRA, QLoRA or GaLore.

### Limitations and Broader Impact

**Limitations.** We performed all experiments on Transformer models. Although Transformers have become dominant in machine learning and computer vision, there are other important deep learning networks such as CNNs, RNNs and SSMs. It remains unclear whether our methods can be extended to non-Transformer-based models and how such an extension could be accomplished. Moreover, although our method is computationally more efficient than competing methods, its primary advantage lies in the substantial reduction of GPU memory. However, the issue of training time still persists.

**Broader Impact.** As compute power grows exponentially, model sizes grow even faster, making it challenging for smaller institutions, especially academic ones, to conduct high-quality research. This work aims to democratize AI research, particularly in large language models, by significantly reducing the memory needed for training, enabling researchers with limited compute resources to train networks and contribute to their research. However, the _democratization_ of AI is controversial, with leading institutions like OpenAI, Anthropic, and Google DeepMind becoming more closed due to the potential risks of LLMs in the wrong hands. We acknowledge this concern and do not endorse misuse of our research.

    &  &  \\   &  &  &  &  \\ Method & it/s & memory (GB) & it/s & memory (GB) & it/s & memory (GB) & it/s & memory (GB) \\  Full & 3.12 & 1.30 & 1.40 & 2.32 & 1.64 & 13.64 & 1.24 & 21.35 \\ Gradient Checkpoint & 2.47 & 1.19 & 1.03 & 1.85 & 1.14 & 7.31 & 0.78 & 11.72 \\ VeLoRA & 2.90 & 1.18 & 1.34 & 1.83 & 1.50 & 8.35 & 1.15 & 13.28 \\   

Table 7: **On-device training time and memory costs** for pre-training LLaMA. Unlike VeLoRA, gradient checkpointing incurs a much more significant training time overhead. Batch size of 1. Our method is 17%, 30%, 30%, 47% faster than gradient checkpointing in LLama 60M, 130M, 7B and 13B. We see that the larger the model, the larger the time performance gain.