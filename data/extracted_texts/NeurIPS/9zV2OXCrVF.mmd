# Chenglin Fan, Ping Li, Xiaoyun Li

\(k\)**-Median Clustering via Metric Embedding: Towards Better Initialization with Differential Privacy**Cognitive Computing Lab

Baidu Research

10900 NE 8th St. Bellevue, WA 98004, USA

{fanchenglin, pingli98, lixiaoyun996}@gmail.com

###### Abstract

1In clustering, the choice of initial centers is crucial for the convergence speed of the algorithms. We propose a new initialization scheme for the \(k\)-median problem in the general metric space (e.g., discrete space induced by graphs), based on the construction of metric embedding tree structure of the data. We propose a novel and efficient search algorithm which finds initial centers that can be used subsequently for the local search algorithm. The so-called HST initialization method can produce initial centers achieving lower error than those from another popular method \(k\)-median++, also with higher efficiency when \(k\) is not too small. Our HST initialization are then extended to the setting of differential privacy (DP) to generate private initial centers. We show that the error of applying DP local search followed by our private HST initialization improves prior results on the approximation error, and approaches the lower bound within a small factor. Experiments demonstrate the effectiveness of our proposed methods.

## 1 Introduction

Clustering is an important classic problem in unsupervised learning that has been widely studied in statistics, data mining, machine learning, network analysis, etc. (Punj and Stewart, 1983; Dhillon and Modha, 2001; Banerjee et al., 2005; Berkhin, 2006; Abbasi and Younis, 2007). The objective of clustering is to divide a set of data points into clusters, such that items within the same cluster exhibit similarities, while those in different clusters distinctly differ. This is concretely measured by the sum of distances (or squared distances) between each point to its nearest cluster center. One conventional notion to evaluate a clustering algorithms is: with high probability, \(cost(C,D) OPT_{k}(D)+\), where \(C\) is the centers output by the algorithm and \(cost(C,D)\) is a cost function defined for \(C\) on dataset \(D\). \(OPT_{k}(D)\) is the cost of optimal clustering solution on \(D\). When everything is clear from context, we will use \(OPT\) for short. Here, \(\) is called _multiplicative error_ and \(\) is called _additive error_. Alternatively, we may also use the notion of expected cost.

Two popularly studied clustering problems are 1) the \(k\)-median problem, and 2) the \(k\)-means problem. The origin of \(k\)-median dates back to the 1970's (e.g., Kaufman et al. (1977)), where one tries to find the best location of facilities that minimizes the cost measured by the distance between clients and facilities. Formally, given a set of points \(D\) and a distance measure, the goal is to find \(k\) center points minimizing the sum of absolute distances of each sample point to its nearest center. In \(k\)-means, the objective is to minimize the sum of squared distances instead. There are two general frameworks for clustering. One heuristic is the Lloyd's algorithm (Lloyd, 1982), which is built upon an iterative distortion minimization approach. In most cases, this method can only be applied to numerical data,typically in the (continuous) Euclidean space. Clustering in general metric spaces (discrete spaces) is also important and useful when dealing with, for example, the graph data, where Lloyd's method is no longer applicable. A more generally applicable approach, the local search method (Kanungo et al., 2002; Arya et al., 2004), has also been widely studied. It iteratively finds the optimal swap between the center set and non-center data points to keep lowering the cost. Local search can achieve a constant approximation (i.e., \(=O(1)\)) to the optimal solution (Arya et al., 2004). For general metric spaces, the state of the art approximation ratio is 2.675 for \(k\)-median Byrka et al. (2015) and 6.357 for \(k\)-means Ahmadian et al. (2017).

**Initialization of cluster centers.** It is well-known that the performance of clustering can be highly sensitive to initialization. If clustering starts with good initial centers with small approximation error, the algorithm may use fewer iterations to find a better solution. The \(k\)-median++ algorithm (Arthur and Vassilvitskii, 2007) iteratively selects \(k\) data points as initial centers, favoring distant points in a probabilistic way, such that the initial centers tend to be well spread over the data points (i.e., over different clusters). The produced initial centers are proved to have \(O( k)\) multiplicative error. Follow-up works further improved its efficiency and scalability, e.g., Bahmani et al. (2012); Bachem et al. (2016); Lattanzi and Sohler (2019); Choo et al. (2020); Cohen-Addad et al. (2021); Grunau et al. (2023); Fan et al. (2023). In this work, we propose a new initialization framework, called HST initialization, which is built upon a novel search algorithm on metric embedding trees constructed from the data. Our method achieves improved approximation error compared with \(k\)-median++. Moreover, importantly, our initialization scheme can be conveniently combined with the notion of differential privacy (DP) to protect the data privacy.

**Clustering with Differential Privacy.** The concept of differential privacy (Dwork, 2006; McSherry and Talwar, 2007) has been popular to rigorously define and resolve the problem of keeping useful information for machine learning models, while protecting privacy for each individual. DP has been adopted to a variety of algorithms and tasks, such as regression, classification, principle component analysis, graph distance release, matrix completion, optimization, and deep learning (Chaudhuri and Monteleoni, 2008; Chaudhuri et al., 2011; Abadi et al., 2016; Ge et al., 2018; Wei et al., 2020; Dong et al., 2022; Fan and Li, 2022; Fan et al., 2022; Fang et al., 2023; Li and Li, 2023, 2023). Private \(k\)-means clustering has also been widely studied, e.g., Feldman et al. (2009); Nock et al. (2016); Feldman et al. (2017), mostly in the continuous Euclidean space. Balcan et al. (2017) considered identifying a good candidate set (in a private manner) of centers before applying private local search, which yields \(O(^{3}n)\) multiplicative error and \(O((k^{2}+d)^{5}n)\) additive error. Later on, the private Euclidean \(k\)-means error ais further improved by Stemmer and Kaplan (2018), with more advanced candidate set selection. Huang and Liu (2018) gave an optimal algorithm in terms of minimizing Wasserstein distance under some data separability condition.

For private \(k\)-median clustering, Feldman et al. (2009); Ghazi et al. (2020) considered the problem in high dimensional Euclidean space. However, it is rather difficult to extend their analysis to more general metrics in discrete spaces (e.g., on graphs). The strategy of Balcan et al. (2017) to form a candidate center set could as well be adopted to \(k\)-median, which leads to \(O(^{3/2}n)\) multiplicative error and \(O((k^{2}+d)^{3}n)\) additive error in the Euclidean space where \(n\) is the sample size. In discrete space, Gupta et al. (2010) proposed a private method for the classical local search heuristic, which applies to both \(k\)-medians and \(k\)-means. To cast privacy on each swapping step, the authors applied the exponential mechanism of McSherry and Talwar (2007). Their method produced an \(\)-differentially private solution with cost \(6OPT+O( k^{2}^{2}n/)\), where \(\) is the diameter of the point set. In this work, we will show that our proposed HST initialization can improve the DP local search for \(k\)-median of Gupta et al. (2010) in terms of both approximation error and efficiency. Stemmer and Kaplan (2018); Jones et al. (2021) proposed \((,)\)-differentially private solution also with constant multiplicative error but smaller additive error.

**The main contributions** of this work include the following:

* We introduce the Hierarchically Well-Separated Tree (HST) as an initialization tool for the \(k\)-median clustering problem. We design an efficient sampling strategy to select the initial center set from the tree, with an approximation factor \(O(\{k,\})\) in the non-private setting, which is \(O(\{k,d\})\) when \(=O( d)\). This improves the \(O( k)\) error of \(k\)-median++. Moreover, the complexity of our HST based method can be smaller than that of \(k\)-median++ when the number of clusters \(k\) is not too small (\(k n\)), which is a common scenario in practical applications.

* We propose a differentially private version of HST initialization under the setting of Gupta et al. (2010) in discrete metric space. The so-called DP-HST algorithm finds initial centers with \(O( n)\) multiplicative error and \(O(^{-1} k^{2}^{2}n)\) additive error. Moreover, running DP-local search starting from this initialization gives \(O(1)\) multiplicative error and \(O(^{-1} k^{2}( n) n)\) additive error, which improves previous results towards the well-known lower bound \(O(^{-1} k(n/k))\) on the additive error of DP \(k\)-median (Gupta et al., 2010) within a small \(O(k n)\) factor. This is the first clustering initialization method with \(\)-differential privacy guarantee and improved error rate in general metric space.
* We conduct experiments on simulated and real-world datasets to demonstrate the effectiveness of our methods. In both non-private and private settings, our proposed HST-based approach achieves smaller cost at initialization than \(k\)-median++, which may also lead to improvements in the final clustering quality.

## 2 Background and Setup

The definition of differential privacy (DP) is as follows.

**Definition 2.1** (Differential Privacy (DP) (Dwork, 2006)).: _If for any two adjacent datasets \(D\) and \(D^{}\) with symmetric difference of size one and any \(O Range()\), an algorithm \(\) with map \(f\) satisfies_

\[Pr[(D) O] e^{}Pr[(D^{}) O],\]

_then algorithm \(\) is said to be \(\)-differentially private (\(\)-DP)._

Intuitively, DP requires that after removing any data point from \(D\) (e.g., a node in a graph), the output of \(D^{}\) should not be too different from that of the original dataset \(D\). The _Laplace mechanism_ adds Laplace(\((f)/\)) noise to the output where \((f)=_{|D-D^{}|=1}|f(D)-f(D^{})|\) is the sensitivity of \(f\), which is known to achieve \(\)-DP. The _exponential mechanism_ is also a tool for many DP algorithms with discrete outputs. Let \(O\) be the output domain. The utility function \(q:D O\) is what we aim to maximize. The exponential mechanism outputs an element \(o O\) with probability \(P[(D)=o]()\). Both mechanisms will be used in our paper.

### \(k\)-Median Clustering and Local Search

In this paper, we follow the classic problem setting in the metric clustering literature, e.g. Arya et al. (2004); Gupta et al. (2010). Specifically, the definitions of metric \(k\)-median clustering problem (DP and non-DP) are stated as follow.

**Definition 2.2** (\(k\)-median).: _Given a universe point set \(U\) and a metric \(:U U\), the goal of \(k\)-median to pick \(F U\) with \(|F|=k\) to minimize_

\[k\;cost_{k}(F,U)=_{v U}_{f F}(v,f).\] (1)

_Let \(D U\) be a set of "demand points". The goal of DP \(k\)-median is to minimize_

\[\;cost_{k}(F,D)=_{v D}_{f F}(v,f),\] (2)

_and the output \(F\) is required to be \(\)-differentially private with respect to \(D\). We may drop "\(F\)" and use "\(cost_{k}(U)\)" or "\(cost_{k}(D)\)" if there is no risk of ambiguity._

Note that in Definition 2.2, our aim is to protect the privacy of a subset \(D U\). To better understand the motivation and application scenario, we provide a real-world example as below.

**Example 2.3**.: _Consider \(U\) to be the universe of all users in a social network (e.g., Facebook, LinkedIn, etc.). Each user (account) has some public information (e.g., name, gender, interests, etc.), but also has some private personal data that can only be seen by the data server. Let \(D\) be a set of users grouped by some feature that might be set as private. Suppose a third party plans to collaborate with the most influential users in \(D\) for e.g., commercial purposes, thus requesting the cluster centers of \(D\). In this case, we need a differentially private algorithm to safely release the centers, while protecting the individuals in \(D\) from being identified (since the membership of \(D\) is private)._The (non-private) local search procedure for \(k\)-median proposed by Arya et al. (2004) is summarized in Algorithm 1. First, we randomly pick \(k\) points in \(U\) as the initial centers. In each iteration, we search over all \(x F\) and \(y U\), and do the swap \(F F-\{x\}+\{y\}\) such that the new centers improve the cost the most, and if the improvement is more than \((1-/k)\) for some \(>0\)2. We repeat the procedure until no such swap exists. Arya et al. (2004) showed that the output center set \(F\) achieves 5 approximation error to the optimal solution, i.e., \(cost(F) 5OPT\).

### \(k\)-median++ Initialization

Although local search is able to find a solution with constant error, it takes \(O(n^{2})\) per iteration (Resende and Werneck, 2007) in expected \(O(k n)\) steps (which gives total complexity \(O(kn^{2} n)\)) when started from a random center set, which would be slow for large datasets. Indeed, we do not need such complicated/meticulous algorithm to reduce the cost at the beginning, i.e., when the cost is large. To accelerate the process, efficient initialization methods find a "roughly" good center set as the starting point for local search. In the paper, we compare our new initialization scheme mainly with a popular (and perhaps the most well-known) initialization method, the \(k\)-median++ (Arthur and Vassilvitskii, 2007)3 as presented in Algorithm 2. The output centers \(C\) by \(k\)-median++ achieve \(O( k)\) approximation error with time complexity \(O(nk)\). Starting from the initialization, we only need to run \(O(k k)\) steps of the computationally heavy local search to reach a constant error solution. Thus, initialization may greatly improve the clustering efficiency.

``` Input: Data points \(U\), number of centers \(k\) Randomly pick a point \(c_{1} U\) and set \(F=\{c_{1}\}\) for\(i=2,...,k\)do  Select \(c_{i}=u U\) with probability \( U}(u^{},F)}\) \(F=F\{c_{i}\}\) Output:\(k\)-median++ initial center set \(F\) ```

**Algorithm 2**\(k\)-median++ initialization (Arthur and Vassilvitskii, 2007)

## 3 Initialization via Hierarchical Well-Separated Tree (HST)

In this section, we propose our new initialization scheme for \(k\)-median clustering, and provide our analysis in the non-private case solving (1). The idea is based on the metric embedding theory. We will start with an introduction to the main tool used in our approach.

### Hierarchically Well-Separated Tree (HST)

In this paper, for an \(L\)-level tree, we will count levels in a descending order down the tree. We use \(h_{v}\) to denote the level of \(v\), and let \(n_{i}\) be the number of nodes at level \(i\). The Hierarchically Well-Separated Tree (HST) is based on the padded decompositions of a general metric space in a hierarchical manner (Fakcharoenphol et al., 2004). Let \((U,)\) be a metric space with \(|U|=n\), and we will refer to this metric space without specific clarification. A \(\)-padded decomposition of \(U\)is a probabilistic partition of \(U\) such that the diameter of each cluster \(U_{i} U\) is at most \(\), i.e., \((u,v)\), \( u,v U_{i}\), \(i=1,...,k\). The formal definition of HST is given as below.

**Definition 3.1**.: _Assume \(_{u,v U}(u,v)=1\) and denote the diameter \(=_{u,v U}(u,v)\). An \(\)-Hierarchically Well-Separated Tree (\(\)-HST) with depth \(L\) is an edge-weighted rooted tree \(T\), such that an edge between any pair of two nodes of level \(i-1\) and level \(i\) has length at most \(/^{L-i}\)._

Our analysis will consider \(=2\)-HST for conciseness, since \(\) only affects the constants in our theoretical analysis. Figure 1 is an example 2-HST (right panel) with \(L=3\) levels, along with its underlying padded decompositions (left panel). Using Algorithm 3, a 2-HST can be built as follows: we first find a padded decomposition \(P_{L}=\{P_{L,1},...,P_{L,n_{L}}\}\) of \(U\) with parameter \(=/2\). The center of each partition in \(P_{L,j}\) serves as a root node in level \(L\). Then, we re-do a padded decomposition for each partition \(P_{L,j}\), to find sub-partitions with diameter \(=/4\), and set the corresponding centers as the nodes in level \(L-1\), and so on. Each partition at level \(i\) is obtained with \(=/2^{L-i}\). This process proceeds until a node has a single point (leaf), or a pre-specified tree depth is reached. It is worth mentioning that, Blelloch et al. (2017) proposed an efficient HST construction in \(O(m n)\) time, where \(n\) and \(m\) are the number of nodes and edges in a graph, respectively. Therefore, the construction of HST can be very efficient in practice.

The first step of our method is to embed the data points into an HST (see Algorithm 4). Next, we will describe our new strategy to search for the initial centers on the tree (w.r.t. the tree metric). Before moving on, it is worth mentioning that, there are polynomial time algorithms for computing an _exact_\(k\)-median solution in the tree metric (Tamir (1996); Shah (2003)). However, the dynamic programming algorithms have high complexity (e.g., \(O(kn^{2})\)), making them unsuitable for the purpose of fast initialization. Moreover, it is unknown how to apply them effectively to the private case. The three key merits of our new algorithm are: (1) It is more efficient than \(k\)-median++ when \(k\) is not too small, which is a very common scenario in practice; (2) It achieves \(O(1)\) approximation error in the tree metric; (3) It can be easily extended to incorporating differential privacy (DP).

``` Input: Data points \(U\) with diameter \(\), \(L\) Randomly pick a point in \(U\) as the root node of \(T\) Let \(r=/2\) Apply a permutation \(\) on \(U\)// so points will be chosen in a random sequence for each \(v U\)do  Set \(C_{v}=[v]\) for each \(u U\)do  Add \(u U\) to \(C_{v}\) if \(d(v,u) r\) and \(u_{v^{} v}C_{v^{}}\)  Set the non-empty clusters \(C_{v}\) as the children nodes of \(T\) for each non-empty cluster \(C_{v}\)do  Run 2-HST\((C_{v},L-1)\) to extend the tree \(T\); stop until \(L\) levels or reaching a leaf node Output:\(2\)-HST \(T\) ```

**Algorithm 3**Build 2-HST\((U,L)\)

Figure 1: An example of a 3-level padded decomposition and the corresponding 2-HST. **Left:** The thickness of the ball represents the level. The colors correspond to different levels in the HST in the right panel. “\(\)”s are the center nodes of partitions (balls), and “\(\)”s are the non-center data points. **Right:** The 2-HST generated from the padded decomposition. Bold indices represent the centers.

### HST Initialization Algorithm

Let \(L=\) and suppose \(T\) is a level-\(L\) 2-HST in \((U,)\), where we assume \(L\) is an integer. For a node \(v\) at level \(i\), we use \(T(v)\) to denote the subtree rooted at \(v\). Let \(N_{v}=|T(v)|\) be the number of data points in \(T(v)\). The search strategy for the initial centers, NDP-HST initialization ("NDP" stands for "Non-Differentially Private"), is presented in Algorithm 4 with two phases.

**Subtree search.** The first step is to identify the subtrees that contain the \(k\) centers. To begin with, \(k\) initial centers \(C_{1}\) are picked from \(T\) who have the largest \(score(v)=N(v) 2^{h_{v}}\). This is intuitive, since to get a good clustering, we typically want the ball surrounding each center to include more data points. Next, we do a screening over \(C_{1}\): if there is any ancestor-descendant pair of nodes, we remove the ancestor from \(C_{1}\). If the current size of \(C_{1}\) is smaller than \(k\), we repeat the process until \(k\) centers are chosen (we do not re-select nodes in \(C_{1}\) and their ancestors). This way, \(C_{1}\) contains \(k\) root nodes of \(k\) disjoint subtrees.

``` Input:\(U\), \(\), \(k\) Initialization:\(L=\), \(C_{0}=\), \(C_{1}=\)  Call Algorithm 3 to build a level-\(L\) 2-HST \(T\) using \(U\) for each node \(v\) in \(T\)do \(N_{v}|U T(v)|\), \(score(v) N_{v} 2^{h_{v}}\) while\(|C_{1}|<k\)do  Add top \((k-|C_{1}|)\) nodes with highest score to \(C_{1}\) for each \(v C_{1}\)do \(C_{1}=C_{1}\{v\}\), if \(\,v^{} C_{1}\) such that \(v^{}\) is a descendant of \(v\) \(C_{0}=(T,C_{1})\) Output: Initial center set \(C_{0} U\) ```

**Algorithm 4**NDP-HST initialization

**Leaf search.** After we find \(C_{1}\) the set of \(k\) subtrees, the next step is to find the center in each subtree using Algorithm 5 ("FIND-LEAF"). We employ a greedy search strategy, by finding the child node with largest score level by level, until a leaf is found. This approach is intuitive since the diameter of the partition ball exponentially decays with the level. Therefore, we are in a sense focusing more and more on the region with higher density (i.e., with more data points).

The complexity of our search algorithm is given as follows. All proofs are placed in Appendix B.

**Proposition 3.2** (Complexity).: _Algorithm 4 takes \(O(dn n)\) time in the Euclidean space._

**Remark 3.3** (Comparison with \(k\)-median++).: _The complexity of of \(k\)-median++ is \(O(dnk)\) in the Euclidean space (Arthur and Vassilvitskii, 2007). Our algorithm would be faster when \(k> n\), which is a common scenario. Similar comparison also holds for general metrics._

### Approximation Error of HST Initialization

We provide the error analysis of our algorithm. Firstly, we show that the initial center set produced by NDP-HST is already a good approximation to the optimal \(k\)-median solution. Let \(^{T}(x,y)=d_{T}(x,y)\) denote the "2-HST metric" between \(x\) and \(y\) in the 2-HST \(T\), where \(d_{T}(x,y)\) is the tree distance between nodes \(x\) and \(y\) in \(T\). By Definition 3.1 and since \(=2^{L}\), in the analysis we assumeequivalently that the edge weight of the \(i\)-th level is \(2^{i-1}\). The crucial step of our analysis is to examine the approximation error in terms of the 2-HST metric, after which the error can be adapted to the general metrics by the following qwl-known result.

**Lemma 3.4** (Bartal (1996)).: _In a metric space \((U,)\) with \(|U|=n\) and diameter \(\), it holds that \( x,y U\), \(E[^{T}(x,y)]=O(\{ n,\})(x,y)\). In the Euclidean space \(^{d}\), \(E[^{T}(x,y)]=O(d)(x,y)\), \( x,y U\)._

Recall \(C_{0},C_{1}\) from Algorithm 4. We define

\[cost_{k}^{T}(U) =_{y U}_{x C_{0}}^{T}(x,y),\] (3) \[cost_{k}^{T}(U,C_{1}) =_{|F T(v)|=1,\\  v C_{1}}_{y U}_{x F}^{T}(x,y),\] (4) \[OPT_{k}^{T}(U) =_{F U,|F|=k}_{y U}_{x F}^{T}(x,y) _{C_{1}^{}}\;cost_{k}^{T}(U,C_{1}^{}).\] (5)

For simplicity, we will use \(cost_{k}^{T}(U)\) to denote \(cost_{k}^{T}(U,C_{1})\). Here, \(OPT_{k}^{T}\) (5) is the cost of the global optimal solution with the 2-HST metric. The last equivalence in (5) holds because the optimal centers can always be located in \(k\) disjoint subtrees, as each leaf only contains one point. (3) is the \(k\)-median cost with 2-HST metric of the output \(C_{0}\) of Algorithm 4. (4) is the optimal cost after the subtrees are chosen. That is, it represents the minimal cost to pick one center from each subtree in \(C_{1}\). We first bound the error of the subtree search step and the leaf search step, respectively.

**Lemma 3.5** (Subtree search).: \(cost_{k}^{T}(U) 5OPT_{k}^{T}(U)\)_._

**Lemma 3.6** (Leaf search).: \(cost_{k}^{T}(U) 2cost_{k}^{T}(U)\)_._

Combining Lemma 3.5 and Lemma 3.6, we obtain:

**Theorem 3.7** (2-HST error).: _Running Algorithm 4, we have \(cost_{k}^{T}(U) 10OPT_{k}^{T}(U)\)._

Thus, HST-initialization produces an \(O(1)\) approximation to the optimal cost in the 2-HST metric. Define \(cost_{k}(U)\) as (1) for our HST centers, and the optimal cost w.r.t. \(\) as

\[OPT_{k}(U)=_{|F|=k}_{y U}_{x F}(x,y).\] (6)

We have the following result based on Lemma 3.4.

**Theorem 3.8**.: _In the general metric space, the expected \(k\)-median cost of NDP-HST (Algorithm 4) is \(E[cost_{k}(U)]=O(\{ n,\})OPT_{k}(U)\)._

**Remark 3.9**.: _In the Euclidean space, Makarychev et al. (2019) showed that using \(O( k)\) random projections suffices for \(k\)-median to achieve \(O(1)\) error. Thus, if \(=O( d)\), by Lemma 3.4, HST initialization is able to achieve \(O((\{d,k\}))\) error, which is better than \(O( k)\) of \(k\)-median++ (Arthur and Vassilvitskii, 2007) when \(d\) is small._

**NDP-HST Local Search.** We are interested in the approximation quality of standard local search (Algorithm 1), when the initial centers are produced by our NDP-HST.

**Theorem 3.10**.: _When initialized by NDP-HST, local search achieves \(O(1)\) approximation error in expected \(O(k\{n,\})\) number of iterations for input in general metric space._

We remark that the initial centers found by NDP-HST can be used for \(k\)-means clustering analogously. For general metrics, \(E[cost_{km}(U)]=O(\{ n,\})^{2}OPT_{km}(U)\) where \(cost_{km}(U)\) is the optimal \(k\)-means cost. See Appendix C for more detaileds.

## 4 HST Initialization with Differential Privacy

In this section, we consider initialization and clustering with differential privacy (DP). Recall (2) that in this problem, \(U\) is the universe of data points, and \(D U\) is a demand set that needs to be clustered with privacy. Since \(U\) is public, simply running initialization algorithms on \(U\) would preserve the privacy of \(D\). However, 1) this might be too expensive; 2) in many cases one would probably want to incorporate some information about \(D\) in the initialization, since \(D\) could be a very imbalanced subset of \(U\). For example, \(D\) may only contain data points from one cluster, out of tens of clusters in \(U\). In this case, initialization on \(U\) is likely to pick initial centers in multiple clusters, which would not be helpful for clustering on \(D\).

Next, we show how our proposed HST initialization can be easily combined with differential privacy and at the same time contains useful information about the demand set \(D\), leading to improved approximation error (Theorem 4.3). Again, suppose \(T\) is an \(L=\)-level 2-HST of universe \(U\) in a general metric space. Denote \(N_{v}=|T(v) D|\) for a node point \(v\). Our private HST initialization (DP-HST) is similar to the non-private Algorithm 4. To gain privacy, we perturb \(N_{v}\) by adding i.i.d. Laplace noise: \(}=N_{v}+Lap(2^{(L-h_{v})}/)\), where \(Lap(2^{(L-h_{v})}/)\) is a Laplace random number with rate \(2^{(L-h_{v})}/\). We will use the perturbed \(}\) for node sampling instead of the true value \(N_{v}\), as described in Algorithm 6. The DP guarantee of this initialization scheme is straightforward by the composition theory (Dwork, 2006).

**Theorem 4.1**.: _Algorithm 6 is \(\)-differentially private._

Proof.: For each level \(i\), the subtrees \(T(v,i)\) are disjoint to each other. The privacy budget used in \(i\)-th level is \(/2^{(L-i)}\), so by composition the total privacy budget is \(_{i}/2^{(L-i)}<\). 

**Theorem 4.2**.: _Algorithm 6 finds initial centers such that_

\[E[cost_{k}(D)]=O( n)(OPT_{k}(D)+k^{-1} n).\]

**DP-HST Local Search.** Similarly, we can use private HST initialization to improve the performance of private \(k\)-median local search, which is presented in Algorithm 7. After DP-HST initialization, the DP local search procedure follows Gupta et al. (2010) using the exponential mechanism.

**Theorem 4.3**.: _Algorithm 7 achieves \(\)-differential privacy. The output centers achieve \(cost_{k}(D) 6OPT_{k}(D)+O(^{-1}k^{2}( n) n)\) in \(O(k n)\) iterations with probability \((1-)\)._

In prior literature, the DP local search with random initialization (Gupta et al., 2010) has 6 multiplicative error and \(O(^{-1} k^{2}^{2}n)\) additive error. Our result improves the \( n\) term to \( n\) in the additive error. Meanwhile, the number of iterations needed is improved from \(T=O(k n)\) to \(O(k n)\) (see Appendix A for an empirical justification). Notably, it has been shown in Gupta et al. (2010) that for \(k\)-median problem, the lower bounds on the multiplicative and additive error of any \(\)-DP algorithm are \(O(1)\) and \(O(^{-1} k(n/k))\), respectively. Our result matches the lower bound on the multiplicative error, and the additive error is only worse than the bound by a factor of \(O(k n)\). To our knowledge, Theorem 4.3 is the first result in the literature to improve the error of DP local search in general metric space.

Numerical Results

### Datasets and Algorithms

**Discrete Euclidean space.** Following previous work, we test \(k\)-median clustering on the MNIST hand-written digit dataset (LeCun et al., 1998) with 10 natural clusters (digit 0 to 9). We set \(U\) as 10000 randomly chosen data points. We choose the demand set \(D\) using two strategies: 1) "balance", where we randomly choose 500 samples from \(U\); 2) "imbalance", where \(D\) contains 500 random samples from \(U\) only from digit "0" and "8" (two clusters). We note that, the imbalanced \(D\) is a very practical setting in real-world scenarios, where data are typically not uniformly distributed. On this dataset, we test clustering with both \(l_{1}\) and \(l_{2}\) distance as the underlying metric.

**Metric space induced by graph.** Random graphs have been widely considered in testing \(k\)-median methods (Balcan et al., 2013; Todo et al., 2019). Our construction of graphs follows a similar approach as the synthetic _pmedinfo_ graphs provided by the popular OR-Library (Beasley, 1990). The metric \(\) for this experiment is the (weighted) shortest path distance. To generate a size-\(n\) graph, we first randomly split the nodes into \(10\) clusters. Within each cluster, each pair of nodes is connected with probability \(0.2\), and with weight drawn from uniform \(\). For every pair of clusters, we randomly connect some nodes from each cluster, with weights following uniform \([0.5,r]\). A larger \(r\) makes the graph more separable, i.e., clusters are farther from each other (see Appendix A for example graphs). For this task, \(U\) has 3000 nodes, and the private set \(D\) (500 nodes) is chosen using the similar "balanced" and "imbalanced" approachs as described above. In the imbalanced case, we choose the demand set \(D\) randomly from only two clusters.

**Algorithms.** We compare the following clustering algorithms in both non-DP and DP setting: (1) **NDP-rand:** Local search with random initialization; (2) **NDP-kmedian++:** Local search with \(k\)-median++ initialization (Algorithm 2); (3) **NDP-HST:** Local search with NDP-HST initialization (Algorithm 4), as described in Section 3; (4) **DP-rand:** Standard DP local search algorithm (Gupta et al., 2010), which is Algorithm 7 with initial centers randomly chosen from \(U\); (5) **DP-kmedian++:** DP local search with \(k\)-median++ initialization run on \(U\); (6) **DP-HST:** DP local search with HST-initialization (Algorithm 7). For non-DP tasks, we set \(L=6\). For DP clustering, we use \(L=8\).

For non-DP methods, we set \(=10^{-3}\) in Algorithm 1 and the maximum number of iterations as 20. To examine the quality of initialization as well as the final centers, We report both the cost at initialization and the cost of the final output. For DP methods, we run the algorithms for \(T=20\) steps and report the results with \(=1\) (comparisons/results with other \(T\) and \(\) are similar). We test \(k\{2,5,10,15,20\}\). The average cost over \(T\) iterations is reported for robustness. All the results are averaged over 10 independent repetitions.

### Results

The results on MNIST and graph data are given in Figure 2. Here we present the \(l_{2}\)-clustering on MNIST, and the simulated graph with \(r=1\) (clusters are less separable). The comparisons are similar for both \(l_{1}\) metric on MNIST and \(r=100\) graph (see Figure 4 in Appendix A):

* From the left column, the initial centers found by HST has lower cost than \(k\)-median++ and random initialization, for both non-DP and DP setting, and for both balanced and imbalanced demand set \(D\). This confirms that the proposed HST initialization is more powerful than \(k\)-median++ in finding good initial centers.
* From the right column, we also observe lower final cost of HST followed by local search in DP clustering. In the non-DP case, the final cost curves overlap, which means that despite HST offers better initial centers, local search can always find a good solution eventually.
* The advantage of DP-HST, in terms of both the initial and the final cost, is more significant when \(D\) is an imbalanced subset of \(U\). As mentioned before, this is because our DP-HST initialization approach also privately incorporates the information of \(D\).

To sum up, the proposed HST initialization scheme could perform better with various metrics and data patterns, in both non-private and private setting--in all cases, HST finds better initial centers with smaller cost than \(k\)-median++. HST considerably outperforms \(k\)-median++ in the private and imbalanced \(D\) setting, for MNIST with both \(l_{2}\) and \(l_{1}\) metric, and for graph with both \(r=100\) (highly separable) and \(r=1\) (less separable).

## 6 Conclusion

We develop a new initialization framework for the \(k\)-median problem in the general metric space. Our approach, called **HST initialization**, is built upon the HST structure from metric embedding theory. We propose a novel and efficient tree search approach which provably improves the approximation error of the \(k\)-median++ method, and has lower complexity (higher efficiency) than \(k\)-median++ when \(k\) is not too small, which is a common practice. Moreover, we propose differentially private (DP) HST initialization algorithm, which adapts to the private demand point set, leading to better clustering performance. When combined with subsequent DP local search heuristic, our algorithm is able to improve the additive error of DP local search, which is close to the theoretical lower bound within a small factor. Experiments with Euclidean metrics and graph metrics verify the effectiveness of our methods, which improve the cost of both the initial centers and the final \(k\)-median output.

Figure 2: \(k\)-median cost on the MNIST (\(l_{2}\)-metric) and graph dataset (\(r=1\)). **1st column:** initial cost. **2nd column:** final output cost.