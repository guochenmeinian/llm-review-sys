# Towards Hybrid-grained Feature Interaction Selection for Deep Sparse Network

Fuyuan Lyu\({}^{1}\), Xing Tang\({}^{2}\)\({}^{}\), Dugang Liu\({}^{4}\), Chen Ma\({}^{3}\),

Weihong Luo\({}^{2}\), Liang Chen\({}^{2}\), Xiuqiang He\({}^{2}\), Xue Liu\({}^{1}\)

\({}^{1}\)McGill University, \({}^{2}\)FiT, Tencent, \({}^{3}\)City University of Hong Kong,

\({}^{4}\)Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)

\({}^{}\) corresponding author

fuyuan.lyu@mail.mcgill.ca, xing.tang@hotmail.com,

dugang.ldg@gmail.com, chenma@cityu.edu.hk,

{lobbyluo,leocchen,xiuqianghe}@tencent.com,

xue.liu@cs.mcgill.ca

###### Abstract

Deep sparse networks are widely investigated as a neural network architecture for prediction tasks with high-dimensional sparse features, with which feature interaction selection is a critical component. While previous methods primarily focus on how to search feature interaction in a coarse-grained space, less attention has been given to a finer granularity. In this work, we introduce a hybrid-grained feature interaction selection approach that targets both feature field and feature value for deep sparse networks. To explore such expansive space, we propose a decomposed space which is calculated on the fly. We then develop a selection algorithm called OptFeature, which efficiently selects the feature interaction from both the feature field and the feature value simultaneously. Results from experiments on three large real-world benchmark datasets demonstrate that OptFeature performs well in terms of accuracy and efficiency. Additional studies support the feasibility of our method. All source code are publicly available1.

## 1 Introduction

Deep Sparse Networks (DSNs) are commonly utilized neural network architectures for prediction tasks, designed to handle sparse and high-dimensional categorical features as inputs. These networks find widespread application in real-world scenarios such as advertisement recommendation, fraud detection, and more. For instance, in the context of advertisement recommendation, the input often comprises high-dimensional features like _user id_ and _City_, which significantly contribute to the final prediction.

As depicted in Figure 1(a), the general DSN architecture consists of three components. Firstly, the embedding layer transforms different feature values into dense embeddings. Following this, feature interaction layers create feature interactions  based on the embeddings of raw features, as illustrated in Figure 1(b). Finally, the predictor makes the final prediction based on the features and their interactions. A core challenge in making accurate DSNs predictions is effectively capturing suitable feature interactions among input features .

Various methods have been proposed to address the issue of modelling feature interactions. Wide&Deep first models human-crafted feature interactions using linear regression . To eliminate the need for human expertise, DeepFM  models all second-order feature interactions by utilizinga factorization machine  and adopts a multi-layer perceptron (MLP) as a predictor. Further advancements have replaced the factorization machine with different operations, such as product operations , cross network [24; 25], or an MLP component . However, these methods have inherent drawbacks as they directly model all possible feature interactions. This inevitably introduces noise into the models, increasing their complexity and potentially degrading performance.

Neural architecture search(NAS)  has been introduced as a powerful approach for feature interaction selection in DSNs for both efficiency and effectiveness [11; 7; 15]. AutoFIS  first propose to select feature interactions by adding an attention gate to each possible one. PROFIT  suggests progressively selecting from a distilled search space with fewer parameters. Alongside feature interaction selection, some works like AutoFeature  and OptInter  aim to search for the interaction operation. However, all these works focus on evaluating the whole feature field as a whole instead of individual feature value. To be more precise, the entire _user id_ or _City_ feature field would be retained or discarded as a unit instead of considering individual feature values like _User x479bs_ or _New Orleans_. Considering only feature fields inevitably be coarse-grained, which may overlook informative values in uninformative fields, and vice versa.

In this work, we propose extending the selection granularity of feature interactions from the field to the value level. The extension of granularity would significantly increase the complexity of the entire selection space, leading to an increase in exploration time and memory usage. We manage this challenge by decomposing the selection space using tensor factorization and calculating the corresponding parameters on the fly. To further improve the selection efficiency, we introduce a hybrid-grained feature interaction selection space, which explicitly considers the relation between field-level and value-level. To perform feature interaction selection, we develop a sparsification-based selection algorithm named **OptFeature**(short for **Optimizing Feature** Interaction Selection), which efficiently selects the feature interaction concurrently from both feature fields and feature values. We conduct experiments over three large-scale real-world benchmarks and compare accuracy and efficiency with state-of-the-art models. Empirical results demonstrate the superiority of our method on both dimensions. Additional studies support the feasibility of our method.

## 2 Related Work

### Deep Sparse Networks (DSNs)

It is commonly believed that feature interaction is the critical challenge towards accurate prediction in deep sparse networks . Various models have been proposed to address the feature interaction layer. DeepFM  utilizes a factorization machine  to model all second-order feature interactions, followed by a multi-layer perceptron as the predictor. More sophisticated feature interaction layers have been proposed to replace the factorization machine, such as the inner product operation in IPNN , the cross-network in DCN [24; 25] or the MLP component in PIN .

With the advancement of neural architecture search [31; 14; 12] and continuous sparsification , various methods have been proposed to select the informative feature interactions [11; 7] and reduce computational costs. AutoFis  employs a sparse optimizer to select suitable feature interactions at the field level. PROFIT  formulates field-level feature interaction selection within a distilled search space. It employs a progressive search for efficient exploration. AutoIAS  takes one step

Figure 1: Illustration of deep sparse network and feature interaction layer

further to integrate the feature interaction selection as part of the search space and jointly conduct the search with other components like MLP architecture or embedding dimension. GAIN , on the other hand, focuses on the DCN -like architectures and conducts the feature interaction selection jointly with the model training. However, all previous works conduct feature interaction selection on the field level. Our work builds on the existing approaches for modelling feature interactions in DSNs. We extend the selection granularity to the value level and propose a hybrid-grained selection approach.

Furthermore, there also exists some works such as OptInter , AutoFeature , and NAS-CTR  take a different angle and search for suitable operations (such as inner product, outer product or element-wise sum) to model each feature interaction properly. These works are perpendicular to our study.

### Feature Interaction Selection in DSNs

To conduct feature interaction selection in DSNs, previous works [11; 7; 26; 13] borrow the ideas from neural architecture search [26; 11] and continuous sparsification [7; 13]. We briefly introduce these two techniques in the following. Previous works focusing on feature selection  are excluded from the discussion.

#### 2.2.1 Neural Architecture Search

Neural architecture search (NAS) automatically identifies optimal architectures for specific tasks and datasets, eliminating the need for expert design [31; 12; 14; 20]. This approach has led to substantial advancements in various domains, including vision [31; 12; 14], natural language processing  and sparse representation learning [11; 7]. There are two key aspects to neural architecture search: _search space_ and _search algorithm_. The _search space_ comprises all potential architectures and is usually task-dependent. An effective search space should be neither too complex, leading to high search costs, nor too shallow, ensuring the inclusion of suitable architectures. The _search algorithm_ explores the search space to find appropriate architectures. Search algorithms can generally be categorized into controller-based , evaluation-based , and gradient-based [14; 12] classes. Our work distinguishes itself from existing research by addressing the challenge of feature interaction selection for deep sparse networks, where the primary obstacles involve high search costs and expansive search spaces.

#### 2.2.2 Continuous Sparsification

Continuous sparsification focuses on reducing a continuous value to a sparse form and is closely related to our method . It is often used to prune less informative components or parameters from neural networks, which is intrinsically equivalent to solving an \(L_{0}\) normalization problem. This technique has been leveraged in various aspects of deep sparse networks (DSNs), notably in the embedding layer  and the feature interaction layer . Our work employs continuous sparsification to discretize the selection of value-level feature interactions during the search process. By doing so, we can effectively manage the complexity of the search space and maintain a high level of efficiency in the search algorithm.

## 3 Methods

This section will first formulate the general DSNs in Section 3.1. Then it will elaborate on the feature interaction selection problem from field and value perspectives in Section 3.2. We further concrete the proposed OptFeature method, which conducts hybrid-grained feature interaction selection in Section 3.3.

### DSNs Formulation

Feature Field and ValueThe input of DSNs comprises multiple feature fields, each containing a set of feature values. Hence, two forms of expression exist for each data sample, deriving from feature field and feature value perspectives. More precisely, a data sample \(\) consists of \(n\) feature fields can be written as:\[  =[_{1},_{2},...,_{n}],\] \[  =[x_{k_{1}},x_{k_{2}},...,x_{k_{n}}].\] (1)

The notation \(_{i}=\{x_{k_{i}} k_{i}[1,m_{i}],k_{i} N_{+}\}\) represents the set of feature values within the \(i\)-th field. Here \(m_{i}\) denotes the number of values in each field, and \(m=_{i=1}^{n}m_{i}\) refers to that across all fields. An illustration figure is shown in Figure 1(c). We highlight this formulation as it is one of the core differences between our method, which focuses on both _field_ and _value_ perspectives, and previous works [11; 7], which focuses only on the _field_ perspective. This allows us to capture more granular and informative interactions, potentially leading to better model performance.

Deep Sparse NetworksAs mentioned in Section 1, the DSN commonly consists of three major components: the embedding layer, the feature interaction layer and the predictor. The embedding layer usually employs an embedding table to convert \(_{i}\) from high-dimensional, sparse vectors into low-dimensional, dense embeddings. This can be formulated as:

\[_{i}=_{i}= x_{k_{i}},\] (2)

where \(^{m d}\) is the embedding table, \(d\) is the pre-defined embedding size. These embeddings are further stacked together as the embedding vector \(=[_{1},_{2},...,_{n}]\), which also serves as the input for feature interaction layer.

The feature interaction layer further performs interaction operations over its input. The \(t\)-th order feature interaction can be generally represented as:

\[^{t}=^{t}(),\] (3)

where \(()\), as the \(t\)-th order interaction operation, \(2 t n\), can vary from a multiple layer perceptron  to cross layer . In this paper, we select the inner product, which is recognized to be the most commonly used operation for feature interaction [18; 7], as our interaction operation. For \(t\)-th order feature interactions, cardinality \(|^{t}|=C_{n}^{t}\). Note that the selection of \(t\) is usually task-dependent. For instance, in the click-through rate prediction, \(t=2\) is considered sufficient .

In the prediction layer, both the embedding vector \(\) and feature interaction vectors \(^{t}\) are aggregated together to make the final prediction \(_{}(,\{^{t} 2 t n\})\), where \(_{}()\) represents the prediction function parameterized on \(\). The mainstream prediction function is MLP [18; 19]. Finally, the loss function is calculated over each data sample and forms the training object:

\[(_{})=_{ }|}_{(,y)_{}}(y,) =_{}|}_{(,y)_{ }}(y,_{}(,\{^{t}  2 t n\})),\] (4)

where \(=\{,\}\) denotes the model parameters, \(_{}\) refers to the training set.

### Feature Interaction Selection

Feature interaction selection is crucial in DSNs [11; 7]. In general DSNs, all feature interactions, whether informative or not, are included as inputs for the final predictor. This indiscriminate inclusion of all interactions inevitably introduces noise into the model, which can hinder its performance [11; 7]. Selecting informative feature interactions is called _feature interaction selection_. Proper feature interaction selection can significantly improve the accuracy and efficiency of DSNs by reducing the amount of noise and focusing on learning the most informative interactions .

For a \(t\)-th order feature interaction \(^{t}=\{_{(i_{1},i_{2},,i_{t})}\},\;1 i_{t} m _{i_{t}}\), we aim to determine whether to keep each element or not. We take \(t=2\) as an example to illustrate the problem. The selection can be formulated as learning an gate weight \(_{(i_{1},i_{2})}\{0,1\}\) corresponding to each \(_{(i_{1},i_{2})}\). All \(_{(i_{1},i_{2})}\) compose the second-order feature interaction selection tensor \(^{2}\), which is formulated as:

\[^{2}=I&_{(1,2)}&_{(1,3)}&& _{(1,n)}\\ _{(1,2)}^{T}&I&_{(2,3)}&&_{(2,n)}\\ _{(1,3)}^{T}&_{(2,3)}^{T}&I&&_{(3,n)}\\ &&&&\\ _{(1,n)}^{T}&_{(2,n)}^{T}&_{(3,n)}^{T}&&I \\ .\] (5)Here the tensor is symmetric due to the feature interaction being invariant to perturbation and the diagonal is composed of identical tensors. According to different perspectives in Equation 5, there are two forms for Equation 1: (i) when \(_{(i_{1},i_{2})}\) is a scalar, \(^{2}\) is a 2-\(D\) matrix which indicates the field level interaction selection \(_{f}^{2}\{0,1\}^{n^{2}}\). (ii) when \(_{(i_{1},i_{2})}\{0,1\}^{m_{i_{1}} m_{i_{2}}}\) is a tensor, \(A^{2}\) is a 2-\(D\) matrix which indicates the value level interaction selection \(_{v}^{2}\{0,1\}^{m^{2}}\). Therefore, feature interaction selection tensor \(^{t}\) of different order \(t\) compose the selection tensor set \(=\{^{t} 2 t n\}\), which is the learning goal of the _feature interaction selection_ problem.

Field-grainedIn previous methods [7; 11], the feature interaction selection is usually conducted at the field level. This can be formulated as learning the field-grained selection tensor set \(=\{_{f}^{t}_{f}^{t}\{0,1\}^{n^{t}},2 t  n\}\), where \(_{f}^{t}\) refers to the field-grained feature interaction selection tensor at \(t\)-th order. Finding a suitable \(_{f}^{t}\) is an NP-hard problem . Some works have been devoted to investigating the search strategy. AutoFis  intuitively assigns a continuous tensor \(_{f}^{t}^{n^{t}}\) and adopts a sparse optimizer for continuous sparsification so that \(_{f}^{t}\) will approximate \(_{f}^{t}\) eventually. PROFIT , on the other hand, uses symmetric CP decomposition  to approximate \(_{f}^{t}\) result. This can be formulated as \(}_{f}^{t}_{r=1}^{R}^ {r}^{r}}_{}\), where \(\) denotes broadcast time operation, \(^{r}^{1 n}\) refers to the decomposed vector and \(R n\) is a pre-defined positive integer.

Value-grainedTo extend the feature interaction selection from field-grained to value-grained, we focus on the value-grained selection tensor set \(=\{_{v}^{t}_{v}^{t}\{0,1\}^{m^{t}},2  t n\}\). Although this formulation is analogous to the field-level one, it is substantially more complex given \(m n\). This expansion in complexity poses a challenge, referred to as the _size explosion_ problem, as the computational cost and memory usage for the value-level selection increases dramatically. So it is hard to utilize previous methods [11; 7] directly into solving the value-grained selection problem. We will discuss how we tackle this _size explosion_ problem in Section 3.3.1 in detail.

Hybrid-grainedApart from learning the field and value-grained selection tensor \(_{f}\) and \(_{v}\), the hybrid-grained selection also needs to learn a proper hybrid function, we formulate the hybrid function as a binary selection equation,

\[=\{^{t} 2 t n\},\ ^{t}=^{t}_{f}^{t}+(1-^{t})_{v}^{t}\] (6)

where tensor \(^{t}\{0,1\}^{n^{t}}\) denoting the hybrid choice. As the quality of the field and value-grained selection tensors will heavily influence the hybrid tensor \(^{t}\), joint optimization of both selection tensors \(_{f}\) & \(_{v}\) and hybrid tensor \(\) is highly desired. We will discuss how we tackle this _joint training_ problem in Section 3.3.2 in detail.

Previous field-grained and value-grained selection can be viewed as a special case for Equation 6. By setting the hybrid tensor \(\) in Equation 6 as an all-one tensor \(\), the original hybrid-grained selection problem is reduced to a field-grained one. Similarly, the value-grained selection problem can also be obtained from Equation 6 by assigning the hybrid tensor \(\) as an all-zero tensor \(\).

### OptFeature

In this section, we formalize our proposed method **OptFeature** aiming to tackle the hybrid-grained feature interaction selection problem depicted in Section 3.2. Such a problem contains two critical issues:

* _Size explosion_ problem: As the number of feature values \(m\) is significantly larger than the number of feature fields \(n\), denoted as \(m n\), the value-grained selection tensor \(_{v}\) increase dramatically and is hard to be stored or explored. To provide a concrete example regarding the size explosion, we take the commonly used benchmark Criteo2 as an example. This dataset contains 39 feature fields and \(6.8 10^{6}\) feature values. Even if we only consider \(t=2\) for the second-order feature interactions, the corresponding selection tensor increase from \(\|_{f}^{2}\|=39^{2}=1521\) to \(6.8}^{2}=4.6}\), making it impossible to be stored or explored using vanilla NAS approaches  or continuous sparsification methods .
* _Joint training_ problem: As the quality of the field and value-grained selection tensors \(_{f}^{t}\) & \(_{v}^{t}\) will heavily influence the hybrid tensor \(\), jointly optimization of both selection tensors and hybrid tensor is required. How to efficiently optimize so many large binary tensors remains unsolved.

To address the two critical issues mentioned in Section 3.2 and above, we propose our method **OptFeature** with 1) the _selection tensor decomposition_ to address the _size explosion_ issue and 2) _sparsification-based selection algorithm_ for the _joint training_ problem.

#### 3.3.1 Selection Tensor Decomposition

To efficiently explore a large space \(_{v}\), we first decompose the feature interaction selection tensor. Without the loss of generality, we only focus on the second order, i.e. \(t=2\), value-grained feature interaction selection tensor \(_{v}^{2}\). More details of the general cases \(t 2\) are listed in the Appendix A. We omit the superscript for simplicity and use \(_{v}\) instead of \(_{v}^{2}\) in later sections. Given that the selection tensor is semi-positive and symmetric, we have the Takagi factorization  as:

\[_{v}_{v}^{ T}.\] (7)

Here \(_{v}\) is a \(d^{{}^{}} d^{{}^{}}\) diagonal tensor, \( R^{m d^{{}^{}}}\) and \(d^{{}^{}}<m\). To further reduce the consumption of memory and reduce factorization error, we introduce the deep multi-mode tensor factorization  that replaces the \(\) as an output of one neural network, denoted as:

\[ f_{_{v}}(_{v}),\] (8)

where \(f_{_{v}}:^{m}^{m d^{ {}^{}}},\  d^{{}^{}}\) is a neural network with parameter \(_{v}\) and \(_{v}^{m}\) is an additional embedding table for generating feature interaction selection tensor. Figure 2 shows a detailed illustration of our decomposition.

Notably, during the training and inference stage, we do not need to calculate the entire metric \(\) all in once. We only need to calculate a batch of data in each trial. The element of architecture metric \(_{v}[k_{i},k_{j}]\) can be calculated given the following equation:

\[_{v}[k_{i},k_{j}]=f_{_{v}}(_{v}[k_{i},:]) _{v} f_{_{v}}^{T}(_{v}[k_{j},:]).\] (9)

The original value-grained selection tensor \(_{v}\) consists of \((m^{2})\) elements. By introducing the Takagi factorization , the number of trainable elements is reduced to \((md^{{}^{}})\). We further reduce that number to \(((m+d^{{}^{}}))\) through the multi-mode tensor factorization . Hence, we can train the neural network over the value-level selection on modern hardware.

#### 3.3.2 Sparsification-based Selection Algorithm

After decomposing the selection tensor, we still need to jointly conduct feature interaction selection and train the model parameters for an efficient search. To help convert the continuous feature selection tensor into an accurate binary selection, we adopt the straight-through estimator(STE) function  for continuous sparsification. The STE can be formulated as a customized function \(()\), with its forward pass as a unit step function

\[(x)=0,&x 0\\ 1,&x>0,\] (10)

and backward pass as \((x)=1\), meaning that it will directly pass the gradient backward. Therefore, we can mimic a discrete feature interaction selection while providing valid gradient information for the value-level selection parameters \(_{v}\) & \(_{v}\), making the whole process trainable. Hence, Equation 9 is re-written as:

\[_{v}[k_{i},k_{j}]=S(f_{_{v}}(_{v}[k_{i},:]) _{v} f_{_{v}}^{T}(_{v}[k_{j},:])).\] (11)

Figure 2: Illustration of the selection tensor decomposition

Similar to Equation 11, we can obtain the field-level selection matrix \(_{f}\) using the following equation:

\[_{f}[i,j]=S(g_{_{f}}(_{f}[i,:])_ {f} g_{_{f}}^{T}(_{f}[j,:])),\] (12)

where \(_{f}^{n d}\) is the field-level embedding table for feature interaction selection and \(g_{_{f}}:^{n d}^{n d^{{}^{ }}}\) is another neural network parameterized by \(_{f}\).

After obtaining the value-level selection matrix \(_{v}\) and field-level selection matrix \(_{f}\), we need to merge them and obtain the hybrid selection result \(\). Inspired by DARTS  and its success in previous works [11; 15; 13], we relax the hybrid tensor \(\) into a continuous tensor \(_{c}^{n^{}}\), which can be trained jointly with other parameters via gradient descent. To ensure convergence, we apply the sigmoid function \(()\) over \(_{c}\). Hence, during training time, Equation 6 can be re-written as:

\[=(_{c})_{f}+(1-(_{c})) _{v}.\] (13)

Hence, the final search objective can be formulated as:

\[}^{*}=_{}}\;(_{ {tra}}|\{^{*},}\}),\;\;^{*}= _{}\;(_{}|\{,}^{*}\}).\] (14)

Here \(}=\{_{v},_{v},_{f},_{f}, \}\) denoting the parameter required in the interaction selection process, and \(_{}\) denotes the validation set. We summarize the overall process of our _sparsification-based selection algorithm_ in Algorithm 1.

```
0: training and validation set \(_{}\) and \(_{}\)
0: main model parameters \(\) and feature interaction selection parameters \(}\)
1:for epoch = 1, \(\), T do
2:while epoch not finished do
3: Sample mini-batch \(_{}\) and \(_{}\) from validation set \(_{}\) and training set \(_{}\)
4: Update selection parameter \(}\) using gradients \(_{}}(_{}|\{, }\})\)
5: Update model parameter \(\) using gradients \(_{}(_{}|\{,}\})\)
6:endwhile
7:endfor ```

**Algorithm 1** Sparsification-based Selection Algorithm

#### 3.3.3 Retraining with Optimal Selection Result

During the re-training stage, the optimal hybrid tensor is determined as \(^{*}=_{_{c}>0}\) following previous works [12; 15; 13]. Hence, we can obtain the optimal selection result

\[^{*}^{*} S(g_{_{f}^{*}}(_{f}^{*}) _{f}^{*} g_{_{f}^{*}}^{T}(_{f}^{*}) )+(1-^{*}) S(f_{_{v}^{*}}(_{v}^{*})_{v}^{*} f_{_{v}^{*}}^{T}(_{v}^{*})).\] (15)

We freeze the selection parameters \(}\) and retrain the model from scratch following the existing works [12; 7]. This reason for introducing the retraining stage is to remove the influence of sub-optimal selection results during the selection process over the model parameter. Also,

## 4 Experiment

### Experiment Setup

DatasetsTo validate the effectiveness of our proposed method OptFeature, we conduct experiments on three benchmark datasets (Criteo, Avazu3, and KDD124), which are widely used in previous work on deep sparse networks for evaluation purposes [7; 11]. The dataset statistics and preprocessing details are described in the Appendix B.

Baseline ModelsWe compare our OptFeature with previous works in deep sparse networks. We choose the methods that are widely used and open-sourced:

* Shallow network: Logistic Regression(LR)  and Factorization Machine(FM) .
* DSNs: FNN , DeepFM , DCN , IPNN .
* DSNs with feature interaction selection(DSNs with FIS): AutoFIS5 and PROFIT6. 

For the shallow networks and deep sparse networks, we implement them following the commonly used library torchfm 7. These baselines and our OptFeature are available here8. For AutoFIS and PROFIT, we re-use their original implementations, respectively.

Evaluation MetricsFollowing the previous works[8; 30], we adopt the commonly-used performance evaluation metrics for click-through rate prediction, **AUC** (Area Under ROC curve) and **Log loss** (also known as cross-entropy). As for efficiency evaluation, we also use two standard metrics, inference time and model size, which are widely adopted in previous works of deep sparse network [7; 11].

Parameter SetupTo ensure the reproducibility of experimental results, here we further introduce the implementation setting in detail. We implement our methods using PyTorch. We adopt the Adam optimizer with a mini-batch size of 4096. We set the embedding sizes to 16 in all the models. We set the predictor as an MLP model with  for all methods. All the hyper-parameters are tuned on the validation set with a learning rate from [1e-3, 3e-4, 1e-4, 3e-5, 1e-5] and weight decay from [1e-4, 3e-5, 1e-5, 3e-6, 1e-6]. We also tune the learning ratio for the feature interaction selection parameters from [1e-4, 3e-5, 1e-5, 3e-6, 1e-6] and while weight decay from [1e-4, 3e-5, 1e-5, 3e-6, 1e-6, 0]. The initialization parameters for the retraining stage are selected from the best-performed model parameters and randomly initialized ones.

Model StabilityTo make the results more reliable, we ran the repetitive experiments with different random seeds five times and reported the average value for each result.

Hardware PlatformAll experiments are conducted on a Linux server with one Nvidia-Tesla V100-PCIe-32GB GPU, 128GB main memory and 8 Intel(R) Xeon(R) Gold 6140 CPU cores.

### Benchmark Comparison

Model PerformanceWe present the model performance across three benchmark datasets in Table 1 and make the following observations. Firstly, OptFeature consistently outperforms other models on all three benchmark datasets, validating the effectiveness of our hybrid-grained selection space and selection algorithm. Secondly, DSNs generally yield better performances than shallow models, underscoring the importance of designing powerful sparse networks. Thirdly, DSNs with feature interaction selection tend to perform superiorly compared to those incorporating all possible interactions. This confirms the significance of conducting feature interaction selection within DSNs. Lastly, methods that decompose the selection space (e.g., OptFeature and PROFIT) consistently outperform AutoFIS, which directly navigates the original selection space.

Model EfficiencyWe also evaluate model efficiency and present the results in Figure 3. We can observe that OptFeature outperforms all other methods in terms of inference time, while its model size is comparable to other feature interaction selection methods. Both model accuracy and inference efficiency are critical factors when deploying DSNs. Our OptFeature not only achieves the best performance but also reduces inference time, indicating its practical value.

### Investigating the Selection Process

In this section, we delve into the selection process of OptFeature. We introduce two variants of OptFeature for this purpose: (i) OptFeature-f, which carries out field-level interaction selection, and (ii) OptFeature-v, which conducts value-level interaction selection. Both methods adopt the same selection algorithm outlined in Section 3.3.2. We will compare OptFeature and its two variants to other DSNs interaction selection baselines regarding effectiveness and efficiency.

Investigating the Selection EffectivenessWe evaluate the effectiveness of various DSN feature interaction methods in terms of comparable performance. Several observations can be made from the results presented in Table 2. Firstly, OptFeature consistently delivers superior results on all three datasets. This confirms that the hybrid-grained selection space allows OptFeature to explore finer-grained while the selection algorithm effectively performs based on the decomposition. Secondly, the value-level selection method consistently surpasses field-level selection, suggesting that field-level selection might be too coarse-grained to leverage informative values within uninformative fields. Lastly, OptFeature-f outperforms both AutoFIS and PROFIT in terms of model performance.

   &  &  &  \\  Category & Model & AUC & Logloss & AUC & Logloss & AUC & Logloss \\   & LR & 0.7882 & 0.4609 & 0.7563 & 0.3928 & 0.7411 & 0.1637 \\  & FM & 0.8047 & 0.4464 & 0.7839 & 0.3783 & 0.7786 & 0.1566 \\   & FNN & 0.8101 & 0.4414 & 0.7891 & 0.3762 & 0.7947 & 0.1536 \\  & DeepFM & 0.8097 & 0.4418 & 0.7896 & 0.3757 & 0.7969 & 0.1531 \\  & DCN & 0.8096 & 0.4419 & 0.7887 & 0.3767 & 0.7955 & 0.1534 \\  & IPNN & 0.8103 & 0.4413 & 0.7896 & 0.3741 & 0.7945 & 0.1537 \\   & AutoFIS & 0.8089 & 0.4428 & 0.7903 & 0.3749 & 0.7959 & 0.1533 \\  & PROFIT & 0.8112 & 0.4406 & 0.7906 & 0.3756 & 0.7964 & 0.1533 \\    & OptFeature & **0.8116** & **0.4402** & **0.7925\({}^{*}\)** & **0.3741\({}^{*}\)** & **0.7982\({}^{*}\)** & **0.1529\({}^{*}\)** \\   Here \({}^{*}\) denotes statistically significant improvement (measured by a two-sided t-test with p-value \(<0.05\)) over the best baseline. The best and second best performed results are marked in **bold** and underline format

Table 1: Overall Performance Comparison

   &  &  &  \\  Model & Granularity & AUC & Logloss & AUC & Logloss & AUC & Logloss \\  AutoFIS & Field & 0.8089 & 0.4428 & 0.7903 & 0.3749 & 0.7959 & 0.1533 \\ PROFIT & Field & 0.8112 & 0.4406 & 0.7906 & 0.3756 & 0.7964 & 0.1533 \\  OptFeature-f & Field & 0.8115 & 0.4404 & 0.7920 & 0.3744 & 0.7978 & 0.1530 \\ OptFeature-v & Value & 0.8116 & 0.4403 & 0.7920 & 0.3742 & 0.7981 & 0.1529 \\ OptFeature & Hybrid & 0.8116 & 0.4402 & 0.7925 & 0.3741 & 0.7982 & 0.1529 \\  

Table 2: Performance Comparison over Different Granularity.

Figure 3: Efficiency comparison between OptFeature and DSN baselines. Model size excludes the embedding layer. Inference time refers to the average inference time per batch across validation set.

The distinguishing factor among these three methods lies in the selection algorithm. AutoFIS directly optimizes the gating vector for each interaction field, rendering the space too large for efficient exploration. PROFIT, on the other hand, adopts a progressive selection algorithm, leading to sub-optimal interaction selection. This finding further emphasizes the superiority of our sparsification-based selection algorithm in selecting informative interactions.

Investigating the Selection EfficiencyNext, we perform a comparison study on the efficiency of the selection methods. In Figure 4, we report the search cost, measured in GPU hours. The search cost reflects the GPU resource consumption when selecting interactions from scratch. It's important to note that the cost of re-training is excluded as our focus is on the selection process. We observe that the search cost of OptFeature is lower than other baselines on the Criteo and KDD12 datasets. OptFeature surpasses the other two variants as it typically converges in fewer epochs. These comparisons underscore the feasibility of our proposed method. However, on the Avazu dataset, OptFeature is slightly outperformed by its two variants, i.e., OptFeature-f and OptFeature-v, because this dataset usually converges in one epoch . As a result, all three methods converge within the same epoch. Furthermore, compared to AutoFIS and PROFIT, OptFeature-f incurs a lower cost with the same complexity of selection space. This observation further highlights the superiority of our selection algorithm in terms of convergence speed.

## 5 Conclusions and Limitations

In this work, we introduce a hybrid-grained selection approach targeting both feature field and feature value level. To explore the vast search space that extends from field to value, we propose a decomposition method to reduce space complexity and make it trainable. We then developed a selection algorithm named OptFeature that simultaneously selects interactions from field and value perspectives. Experiments conducted on three real-world benchmark datasets validate the effectiveness and efficiency of our method. Further ablation studies regarding search efficiency and selection granularity illustrate the superiority of our proposed OptFeature.

LimitationsDespite OptFeature demonstrating superior performance over other baselines on model performance and inference time, it requires a larger model size than certain DSNs [8; 27]. Additionally, it lacks online experiments to validate its effectiveness in more complex and dynamic scenarios.