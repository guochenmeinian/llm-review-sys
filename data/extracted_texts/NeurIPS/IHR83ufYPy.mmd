# Leveraging sparse and shared feature activations for disentangled representation learning

Marco Fumero

Sapienza, University of Rome

&Florian Wenzel

Amazon AWS

&Luca Zancato

Alessandro Achille

Amazon AWS

Emanuele Rodola

Sapienza, University of Rome

&Stefano Soatto

Amazon AWS

&Bernhard Scholkopf

Amazon AWS

&Francesco Locatello

IST Austria

###### Abstract

Research on recovering the latent factors of variation of high dimensional data has so far focused on simple synthetic settings. Mostly building on unsupervised and weakly-supervised objectives, prior work missed out on the positive implications for representation learning on real world data. In this work, we propose to leverage knowledge extracted from a diversified set of supervised tasks to learn a common disentangled representation. Assuming that each supervised task only depends on an unknown subset of the factors of variation, we disentangle the feature space of a supervised multi-task model, with features activating sparsely across different tasks and information being shared as appropriate. Importantly, we never directly observe the factors of variations, but establish that access to multiple tasks is sufficient for identifiability under sufficiency and minimality assumptions. We validate our approach on six real world distribution shift benchmarks, and different data modalities (images, text), demonstrating how disentangled representations can be transferred to real settings.

## 1 Introduction

A fundamental question in deep learning is how to learn meaningful and reusable representation from high dimensional data observations . A core area of research pursuing is centered on disentangled representation learning (DRL)  where the aim is to learn a representation which recovers the factors of variations (FOVs) underlying the data distribution. Disentangled representations are expected to contain all the information present in the data in a compact and interpretable structure  while being independent from a particular task . It has been argued that separating information into interventionally independent factors  can enable robust downstream predictions, which was partially validated in synthetic settings . Unfortunately, these benefits did not materialize in real world representations learning problems, largely limited by a lack of scalability of existing approaches.

In this work we focus on leveraging knowledge from different task objectives to learn better representations of high dimensional data, and explore the link with disentanglement and out-of-distribution (OOD) generalization on real data distributions. Representations learned from a large diversity of tasks are indeed expected to be richer and generalize better to new, possibly out-of-distribution, tasks. However, this is not always the case, as different tasks can compete with each other and lead to weaker models. This phenomenon, known as negative transfer  in the context of transfer learning or task competition  in multitask learning, happens when a limited capacity model is used to learn two different tasks that require expressing high feature variability and/or coverage. Aiming to use the same features for different objectives makes them noisy and often increases the sensitivity to spurious correlations , as features can be both predictive and detrimental fordifferent tasks. Instead, we leverage a diverse set of tasks and assume that each task only depends on an unknown subset of the factors of variation. We show that disentangled representations naturally emerge without any annotation of the factors of variations under the following two representation constraints:

* _Sparse sufficiency_: Features should activate sparsely with respect to tasks. The representation is _sparsely sufficient_ in the sense that any given task can be solved using few features.
* _Minimality_: Features are maximally shared across tasks whenever possible. The representation is _minimal_ in the sense that features are encouraged to be reused, i.e., duplicated or split features are avoided.

These properties are intuitively desirable to obtain features that (i) are disentangled w.r.t. to the factors of variations underlying the task data distribution (which we also theoretically argue in Proposition 2.1), (ii) generalize better in settings where test data undergo distribution shifts with respect to the training distributions, and (iii) suffer less from problems related to negative transfer phenomena. To learn such representations in practice, we implement a meta learning approach, enforcing feature sufficiency and sharing with a _sparsity_ regularizer and an entropy based _feature sharing_ regularizer, respectively, incorporated in the base learner. Experimentally, we show that our model learns meaningful disentangled representations that enable strong generalization on real world data sets. Our contributions can be summarized as follows:

* We demonstrate that is possible to learn disentangled representations leveraging knowledge from a distribution of tasks. For this, we propose a meta learning approach to learn a feature space from a collection of tasks while incorporating our sparse sufficiency and minimality principles favoring task specific features to coexist with general features.
* Following previous literature, we test our approach on synthetic data, validating in an idealized controlled setting that our sufficiency and minimality principles lead to disentangled features w.r.t. the ground truth factors of variation, as expected from our identifiability result in Proposition 2.1.
* We extend our empirical evaluation to non-synthetic data where factors of variations are not known, and show that our approach generalizes well out-of-distribution on different domain generalization and distribution shift benchmarks.

## 2 Method

Given a distribution of tasks \(t\) and data \((},y_{t})_{t}\) for each task \(t\), we aim to learn a disentangled representation \(g()=}}^{M}\), which generalizes well to unseen tasks. We learn this representation \(g\) by imposing the sparse sufficiency and minimality inductive biases.

### Learning sparse and shared features

Our architecture (see Figure 1) is composed of a backbone module \(g_{}\) that is shared across all tasks and a separate linear classification head \(f_{_{t}}\), which is specific to each task \(t\). The backbone is responsible to compute and learn a general feature representation for all classification tasks. The linear head solves a specific classification problem for the task-specific data \((},y_{t})_{t}\) in the feature space \(}\) while enforcing the feature sufficiency and minimality principles. Adopting the typical meta-learning setting , the backbone module \(g_{}\) can be viewed as the _meta learner_ while the task-specific classification heads \(f_{_{t}}\) can be viewed as the _base learners_. In the meta-learning setting we assume to have access to samples for a new task given by a _support set_\(U\), with elements \((^{U},y^{U}) U\). These samples are used to fit the linear head \(f_{^{*}}\) leading to the optimal feature weights for the given task. For a _query_\(^{Q} Q\), the prediction is obtained by computing the forward pass \(=f_{^{*}}(g_{}(^{Q}))\).

Enforcing feature minimality and sufficiency.To solve a task in the feature space \(}\) of the backbone module we impose the following regularizer \(Reg()\) on the classification heads \(f_{}\) with parameter \(^{T M C}\), where \(T\) is the number of tasks, \(M\) the number of features, and \(C\) the number of classes. The regularizer is responsible for enforcing the feature minimality and sufficiency properties. It is composed of the weighted sum of a sparsity penalty \(Reg_{L1}\) and an entropy-based feature sharing penalty: \(Reg_{sharing}\)

\[Reg()= Reg_{L_{1}}()+ Reg_{sharing}(), \]

with scalar weights \(\) and \(\). The penalty terms are defined by:

\[Reg_{L_{1}}()=_{t,c,m}|_{t,m,c}| \] \[Reg_{sharing}()=H(_{m})=-_{m}_{m} log(_{m}) \]

where \(_{m}=|_{t,c,m}|}{_{t,c,m}|_ {t,c,m}|}\) are the normalized classifier parameters. Sufficiency is enforced by a sparsity regularizer given by the \(L_{1}\)-norm, which constrains classification head to use only a sparse subset of the features. Minimality is enforced by the feature sharing term: minimizing the entropy of the distribution of feature importances (i.e. normalized \(|_{t}|\)) averaged across a mini batch of \(T\) tasks, leads to a more peaked distribution of activations across tasks. This forces features to cluster across tasks and therefore be reused by different tasks, when useful.We remark that different choices for the regularizers coming from the linear multitask learning literature (e.g. [59; 39; 38]) to enforce sparse sufficiency and minimality are indeed possibile. We leave their exploration as a future direction.

### Training method

We train the model in meta-learning fashion by minimizing the test error over the expectation of the task distribution \(t\). This can be formalized as a _bi-level optimization problem_. The optimal backbone model \(g_{^{*}}\) is given by the _outer optimization problem_:

\[_{}_{t}[_{outer}(f_{^{*}}(g_{}(_{t}^{Q}),y_{t}^{Q}))], \]

where \(f_{^{*}}\) are the optimal classifiers obtained from solving the _inner optimization problem_, and \((_{t}^{Q},y_{t}^{Q}) Q_{t}\) are the test (or query) datum from the query set \(Q_{t}\) for task \(t\). Let \(U_{t}\) be the support set with samples \((_{t}^{U},y_{t}^{U}) U\) for task \(t\), where typically the support set is distinct from the query set, i.e., \(U Q=\). The optimal classifiers \(f_{^{*}}\) are given by the _inner optimization problem_:

\[_{}_{t}_{inner}(_{t}^{U},y_{t}^{U})+ Reg(), \]

where \(_{t}^{U}=f_{}(g_{}(_{t}^{U})\). For both the inner loss \(_{inner}\) and outer loss \(_{outer}\) we use the cross entropy loss.

**Task generation.** Our method can be applied in a standard supervised classification setting where we construct the tasks on the fly as follows. We define a task \(t\) as a \(C\)-way classification problem. We first select a random subset of \(C\) classes from a training domain \(D_{train}\) which contains \(K_{train}\) classes. For each class we consider the corresponding data points and select a random support set \(U_{t}\) with elements \((_{t}^{U},y^{U}) U\) and a disjoint random query set \(Q_{t}\) with elements \((_{t}^{Q},y^{Q}) Q_{t}\).

**Algorithm.** In practice we solve the bi-level optimization problem (4) and (5) as follows. In each iteration we sample a batch of \(T\) tasks with the associated support and query set as described above. First, we use the samples from the support set \(S_{t}\) to fit the linear heads \(f_{}\) by solving the inner optimization problem (5) using stochastic gradient descent for a fixed number of steps. Second, we

Figure 1: _Model scheme_: Illustrations of the (_Top_) the inner loop stage and outer loop following the steps of the algorithmic procedure described in Section B.1 in the Appendix.

use the samples from the query set \(Q_{t}\) to update the backbone \(g_{}\) by solving the outer optimization problem (4) using implicit differentiation [11; 31]. Since the optimal solution of the linear heads \(^{*}\) depend on the backbone \(g_{}\), a straightforward differentiation w.r.t. \(\) is not possible. We remedy this issue by using the approximation strategy of  to compute the implicit gradients. The algorithm is summarized in section B.1 of the Appendix.

### Theoretical analysis

We analyze the implications of the proposed minimality and sparse sufficiency principles and show in a controlled setting that they indeed lead to identifiability. As outlined in Figure 2, we assume that there exists a set of independent latent factors \(_{i=1}^{d}p(z_{i})\) that generate the observations via an unknown mixing function \(=g^{*}()\). Additionally, we assume that the labels \(y_{t}\) for a task \(t\) only depend on a subset of the factors indexed by \(S_{t} P(S)\), where \(S\) is an index set on \(\), via some unknown mixing function \(y_{t}=f_{t}^{*}()\) (potentially different for different tasks). We formalize the two principles that are imposed on \(f^{*}\) by:

1. _sufficiency_: \(f_{t}^{*}=f_{t}^{*}|_{S_{t}}\) for \(S_{t} p(S)\)
2. _minimality_: \( S^{} S_{t}\) s.t. \(f_{t}^{*}|_{S^{}}=f_{t}^{*}\),

where \(f|_{S_{t}}\) denotes that the input to a function \(f\) is restricted to the index set given by \(S_{t}\) (all remaining entries are set to zero). (1) states that \(f_{t}^{*}\) only uses a subset of features, and (2) states that there are not be duplicate features.

**Proposition 2.1**.: _Assume that \(g^{*}\) is a diffeomorphism (smooth with smooth inverse), \(f^{*}\) satisfies the sufficiency and minimality properties stated above, and \(p(S)\) satisfies: \(p(S S^{}=\{i\})>0\) or \(p(\{i\}(S S^{})-(S^{} S))>0\). Observing unlimited data from \(p(X,Y)\), it is possible to recover a representation \(}\) that is an axis aligned, component wise transformation of \(\)._

**Remarks:** Overall, we see this proposition as validation that in an idealized setting our inductive biases are sufficient to recover the factors of variation. Note that the proof is non-constructive and does not entail a specific method. In practice, we rely on the same constraints as inductive biases that lead to this theoretical identifiability and experimentally show that disentangled representations emerge in controlled synthetic settings. On real data, (1) we cannot directly measure disentanglement, (2) a notion of global ground-truth factors may even be ill-posed, and (3) the assumptions of Proposition 2.1 are likely violated. Still, sparse sufficiency and minimality yield some meaningful factorization of the representation for the considered tasks.

**Relation to  and **: Our theoretical result can be reconnected with concurrent work  and can be seen as a corollary with a different proof technique and slightly relaxed assumptions. The main difference is that our feature minimality allows us to also cover the case where the number of factors of variations is unknown, which we found critical in real world data sets (the main focus of our paper). Instead, they only assume sparse sufficiency, which is enough for identifiability if the ground-truth number of factors is known, but is not enough to recover high disentaglement when this is not the case (see Figure 3) and does not translate well to real data, see Table 16 with the empirical comparison in Appendix D.8. Interestingly, their analysis also hints at the fact that our approach also benefits in terms of sample complexity on transfer learning downstream tasks. Our proof technique follows the general construction developed for multi-view data in , adapted to our different setting. Instead of observing multiple views with shared factors of variation, we observe a single task that only depend on a subset of the factors.

## 3 Related work

**Learning from multiple tasks and domains.** Our method addresses the problem of learning a general representation across multiple and possibly unseen tasks [15; 103] and environments [105; 32; 44; 97; 63; 94; 64] that may be competing with each other during training [61; 91; 83]. Prior research tackled task competition by introducing task specific modules that do not interact during training [67; 101; 80]. While successfully learning specialized modules, these approaches can not leverage synergistic information between tasks, when present. On the other hand, our approach is closer to multi-task methods that aim at learning a generalist model, leveraging multi-task interactions [106; 5]. Other approaches that leverage a meta-learning objective for multi-task learning have been formulated [18; 81; 50; 9]. In particular,  proposes to learn a generalist model in a few-shot learning setting without explicitly favoring feature sharing, nor sparsity. Instead, we rephrase the multi-task objective function encoding both feature sharing and sparsity to avoid task competition.

Similar to prior work in domain generalization, we assume the existence of stable features for a given task [64; 4; 86; 40; 90] and amortize the learning over the multiple environments. Differently than prior work, we do not aim to learn an invariant representation a priori. Instead, we learn sufficient and minimal features for each task, which are selected at test time fitting the linear head on them. In light of , one can interpret our approach as learning the final classifier using empirical risk minimization but over features learned with information from the multiple domains.

**Disentangled representations.** Disentanglement representation learning [8; 33] aims at recovering the factors of variations underlying a given data distribution.  proved that without any form of supervision (whether direct or indirect) on the Factors of Variation (FOV) is not possible to recover them. Much work has then focused on identifiable settings [58; 25] from non-i.i.d. data, even allowing for latent causal relations between the factors. Different approaches can be largely grouped in two categories. First, data may be non-independently sampled, for example assuming sparse interventions or a sparse latent dynamics [30; 55; 13; 100; 2; 79; 48]. Second, data may be non-identically distributed, for example being clustered in annotated groups [37; 41; 82; 95; 60]. Our method follows the latter, but we do not make assumptions on the factor distribution across tasks (only their relevance in terms of sufficiency and minimality). This is also reflected in our method, as we train for supervised classification as opposed to contrastive or unsupervised learning as common in the disentanglement literature. The only exception is the work of  discussed in Section 2.3.

## 4 Experiments

We start by highlighting here the experimental setup of this paper along with its motivation.

**Synthetic experiments.** We first evaluate our method on benchmarks from the disentanglement literature [62; 14; 71; 49] where we have access to ground-truth annotations and we can assess quantitatively how well we can learn disentangled representations. We further investigate how minimality and feature sharing are correlated with disentanglement measures (Section 4.1) and how well our representations, which are learned from a limited set of tasks, generalize their composition. The purpose of these experiments is to validate our theoretical statement, showing that if the assumptions of Proposition 2.1 hold, our methods quantitatively recover the factors of variation.

**Domain generalization.** On real data sets, we can neither quantitatively measure disentanglement nor are we guaranteed identifiability (as assumptions may be violated). Ultimately, the goal of disentangled representations is to learn features that lend themselves to be easily and robustly transferred to downstream tasks. Therefore, we first evaluate the usefulness of our representations with respect to downstream tasks subject to distribution shifts, where isolating spurious features was found to improve generalization in synthetic settings [19; 58] To assess how robust our representations are to distribution shifts, we evaluate our method on domain generalization and domain shift tasks on six different benchmarks (Section 4.2). In a domain generalization setting, we do not have access to samples coming from the testing domain, which is considered to be OOD w.r.t. to the training domains. However, in order to solve a new task, our method relies on a set labeled data at test time to fit the linear head on top of the feature space. Our strategy is to sample data points from the training

Figure 2: Assumed causal generative model: the gray variables are unobserved. Observations \(\) are generated by some unknown mixing of a set of factors of variations \(\). Additionally, we observe a distribution of supervised tasks, only depending on a subset of factors of variations indexed by \(S\).

distribution, balanced by class, assuming that the label set \(Y\) does not change in the testing domain, although its distribution may undergo subpopulation shifts.

**Few-shot transfer learning.** Lastly, we test the adaptability of the feature space to new domains with limited labeled samples. For transfer learning tasks, we fit a linear head using the available limited supervised data. The sparsity penalty \(\) is set to the value used in training; the feature sharing parameter \(\) is defaulted to zero unless specified.

**Experimental setting.** To have a fair comparison with other methods in the literature, we adopt the standard experimental setting of prior work . Hyperparameters \(\) and \(\) are tuned performing model selection on validation set, unless specified otherwise. For comparison with baselines, we substitute our backbone with that of the baseline (e.g. for ERM models, we detach the classification head) and then fit a new linear head on the same data. The linear head module trained at test time on top of the features is the same both for our and compared methods. Despite its simplicity, we report the ERM baseline for comparison in our experiments in the main paper, since it has been shown to perform best in average on domain generalization benchmarks . We further compare with other consolidated approaches in the literature such as IRM , CORAL  and GroupDRO  and include a large and comprehensive comparison with  in AppendixD.4. Experimental details are fully described in Appendix C.

### Synthetic experiments

We start by demonstrating that our approach is able to recover the factors of variation underlying a synthetic data distribution like . For these experiments, we assume to have partial information on a subset of factors of variation \(Z\), and we aim to learn a representation \(}\) that aligns with them while ignoring any spurious factors that may be present. We sample random tasks from a distribution \(\) (see Appendix C.3 for details) 5and focus on binary tasks, with \(Y=\{0,1\}\). For the DSprites dataset an example of valid task is _"There is a big object on the left of the image"_. In this case, the partially observed factors (quantized to only two values) are the _x position_ and _size_. In Table 1, we show how the feature sufficiency and minimality properties enable disentanglement in the learned representations. We train two identical models on a random distribution of sparse tasks defined on FOVs, showing that, for different datasets , the same model without regularizers achieves a similar in-distribution (ID) accuracy, but a much lower disentanglement.

We then randomly draw and fix 2 groups of tasks with supports \(S_{1},S_{2}\) (18 in total), which all have support on two FOVs, \(|S_{1}|=|S_{2}|=2\). The groups share one factor of variation and differ in the other one, i.e. \(S_{1} S_{2}=\{i\}\) for some \(\{i\} Z\). The data in these tasks are subject to spurious correlations, i.e. FOVs not in the task support may be spuriously correlated with the task label. We start from an overestimate of the dimension of \(}\) of 6, trying to recover \(\) of size \(3\). We train our network to solve these tasks, enforcing sufficiency and minimality on the representation with different regularization degrees. In Figure 3, we show how the alignment of the learned features with the ground truth factors of variations depend on the choice of \(,\), going from no disentanglement (\(DCI=27.8\)). to good alignment as we enforce more sufficiency and minimality. The model that attains the best alignment (\(DCI=98.8\)) uses both sparsity and feature sharing. Sufficiency alone (akin to the method of ) is able to select the right support for each task, but features are split or duplicated, attaining lower disentanglement (\(DCI=71.9\)). The feature sharing penalty ensures clustering

Figure 3: _Role of minimality:_ We plot the DCI metric of a set of models (_red dots_) trained on fixed tasks from DSprites: Training without regularizers leads to no disentanglement (_green_). Enforcing sparsity alone (_yellow_, akin to ) achieves good disentanglement (\(DCI=71.9\)), but some features may be split or duplicated. Enforcing both minimality and sparse sufficiency (_magenta_) attains the best \(DCI\) (\(98.8\)). When \(\) is too high (\(>0.25\)) activated features collapses into few clusters with respect to tasks. For complete results and experiments on additional datasets see Table 8 and Figures 6, 7 in Appendix.

in the feature space w.r.t. tasks, ensuring to reach high disentanglement, although it may result in the failure cases, when \(\) is too high (\(>0.25\)).

**Disentanglement and minimality are correlated.** In the synthetic setting, we also show the role of the feature sharing penalty. Minimizing the entropy of feature activations across mini-batches of tasks results in clusters in the feature space. We investigate how the strength of this penalty correlates well with disentanglement metrics  training different models on Dsprites which differ by the value of \(\). For 15 models trained increasing \(\) from \(0\) to \(0.2\) linearly, we observe a correlation coefficient with the DCI metric associated to representations compute by each model of \(94.7\), showing that the feature sharing property strongly encourages disentanglement. This confirms again that sufficiency alone (i.e. enforcing sparsity) is not enough to attain good disentanglement.

**Task compositional generalization.** Finally, we evaluate the generalization capabilities of the features learned by our method by testing our model on a set of unseen tasks obtained by combining tasks seen during training. To do this, we first train two models on the AbstractDSprites dataset using a random distribution of tasks, where we limit the support of each task to be within 2 (i.e. \(|S|=2\)). The models differ in activating/deactivating the regularizers on the linear heads. Then, we test on 100 tasks drawn from a distribution with increasing support on the factors of variation \((|S|=3,|S|=4,|S|=5)\), which correspond to composition of tasks in the training distribution; see Figure 4, with the accompaning Table 9 in Appendix D.

### Domain Generalization

In this section we evaluate our method on benchmarks coming from the domain generalization field  and subpopulation distribution shifts , to show that a feature space learned with our inductive biases performs well out of real world data distribution.

**Subpopulation shifts.** Subpopulation shifts occur when the distribution of minority groups changes across domains. Our claim is that a feature space that satisfies sparse sufficiency and minimality is more robust to spurious correlations which may affect minority groups, and should transfer better to new distributions. To validate this, we test on two benchmarks Waterbirds , and CivilComments  (see Appendix C.1).

For both, we use the train and test split of the original dataset. In Table 4, last row, we report the results on the test set of Waterbirds for the different groups in the dataset (landbirds on land, landbirds on water, waterbirds on land, and waterbirds on water, respectively). We fit the linear head

    & Dsprites & 3Dshapes & SmallNorb & Cars \\  _No reg_ & & & & \\ (DCI,Acc) & (16.6,94.4) & (44.4,96.2 ) & (16.5,96.1) & (60.5,99.8) \\  \(,\) & & & & \\ (DCI,Acc) & (**69.9**,95.8) & (**87.7**, 95.8) & (**55.8**,95.6 ) & (**92.3**,99.8 ) \\   

Table 1: _Enforcing disentanglement_: DCI  disentanglement scores and ID accuracy on test samples for a model trained without enforcing sufficiency and minimality (top row), and model with the regularizers activated (bottom row). While attaining similar performance on accuracy, the model with the activated regularizer always show higher disentanglement. See Table 7 for additional scores.

Figure 4: _Task compositional generalization_: Mean accuracy over 100 random test tasks reported for group of tasks of growing support (_second, third, fourth column_) for a model trained without inductive biases (_blue_, attaining \(DCI=29.4\)) and enforcing them (_orange_, \(DCI=59.4\)). The latter show better compositional generalization resulting from the properties enforced on the representation. Exact values are reported in Table 9 in Appendix.

on a random subset of the training domain, balanced by class, repeat 10 times and report accuracy and standard deviation on test. For CivilComments we report the average and worst accuracy in Figure 5, where we compare with ERM and groupDRO . While performing almost on par w.r.t. ERM, our method is more robust to spurious correlation in the dataset, showing the higher worst group accuracy. Importantly, we outperform GroupDRO, which uses information on the subdomain statistics, while we do not assume any prior knowledge about them. Results per group are reported in the Appendix (Table 11).

**DomainBed.** We evaluate the domain generalization performance on the PACS, VLCS and OfficeHome datasets from the DomainBed  test suite (see Appendix C.1 for more details). On these datasets, we train on \(N-1\) and leave one out for testing. Regularization parameters \(\) and \(\) are tuned according to validation sets of PACS, and used accordingly on the other dataset. For these experiments we use a ResNet50 pretrained on Imagenet  as a backbone, as done in  To fit the linear head we sample 10 times with different samples sizes from the training domains and we report the mean score and standard deviation. Results are reported in Table 4, showing how enforcing sparse sufficiency and minimality leads consistently to better OOD performance. Comparisons with 13 additional baselines is in Appendix D.4.

**Camelyon17.** The model is trained according to the original splits in the dataset. In Table 3 we report the accuracy of our model on in-distribution and OOD splits, compared with different baselines [84; 4]. Our method shows the best performance on the OOD test domains. The intuition of why this happens is that, due to minimality, we retain more features which are shared across the three training domains, giving less importance to the ones that are domain-specific (which contain the spurious correlations with the hospital environmental informations). This can be further enforced at test time, as we show in the ablation in Appendix D.9, trading off in distribution performance for OOD accuracy.

    &  \\ 
**1-shot** & PACS & VLCS & OfficeHome & Waterbirds \\ ERM & 80.5 & \(59.7\) & 56.4 & 79.8 \\ Ours & \(\) & \(\) & \(\) & \(\) \\ 
**5-shot** & & & & \\ ERM & 87.1 & 71.7 & 75.7 & 79.8 \\ Ours & \(\) & \(\) & \(\) & \(\) \\ 
**10-shot** & & & & \\ ERM & 87.9 & 74.0 & 81.0 & 84.2 \\ Ours & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Quantitative results for few-shot transfer learning, with our method consistently outperforming ERM across all sample sizes and data sets.

Figure 5: _Quantitative results on CivilComments:_ we report the accuracy on test averaged across all demographic groups (_left group_), and the worst group accuracy, on the _right_. Our method (_green_) performs similarly in terms of average accuracy and outperforms in terms of worst group accuracy, without using any knowledge on the group composition in the training data. For exact values and error estimates, see Table 10 in the Appendix.

    & Validation(ID) & Validation (OOD) & Test (OOD) \\  ERM & 93.2 & 84 & 70.3 \\ CORAL & \(\) & 86.2 & 59.5 \\ IRM & 91.6 & 86.2 & 64.2 \\ Ours & 93.2 \(\)0.3 & \(\)0.6 & \(\)0.2 \\   

Table 3: Quantitative evaluation on Camelyon17: we report accuracy both on ID and OOD splits. Our approach achieves significantly higher validation and test OOD accuracy.

### Few-shot transfer learning.

We finally show the ability of features learned with our method to adapt to a new domain with a small number of samples in a few-shot setting. We compare the results with ERM in Table 2, averaged by domains in each benchmark dataset. The full scores for each domain are in Appendix D.5 for 1-shot, 5-shot, and 10-shot setting, reporting the mean accuracy and standard deviations over 100 draws. Our approach achieves consistently higher accuracy than ERM, showing the better adaptation capabilities of our minimal and sufficently sparse feature space.

### Additional results

In Appendix D we report a large collection of additional results, including comparison with 14 baseline methods on the domain shift benchmarks (D.4), a qualitative and quantitative analysis on the minimality and sparse sufficiency properties in the real setting (D.2), a favorable additional comparison on meta learning benchmarks, with 6 other baselines including (D.8), an ablation study on the effect of clustering features at test time (D.9), and a demonstration on the possibility to obtain a task similarity measure as a consequence of our approach (D.7).

## 5 Conclusions

In this paper, we demonstrated how to learn disentangled representations from a distribution of tasks by enforcing feature sparsity and sharing. We have shown this setting is identifiable and have validated it experimentally in a synthetic and controlled setting. Additionally, we have empirically shown that these representations are beneficial for generalizing out-of-distribution in real-world settings, isolating spurious and domain specific factors that should not be used under distribution shift.

**Limitations and future work**: The main limitation of our work is the global assumption on the strength of the sparsity and feature sharing regularizers \(\) and \(\) across all tasks. In real settings these properties of the representations might need to change for different tasks. We have already observed this in the synthetic setting in Figure 3, where when \(>0.25\) features cluster excessively and are unable to achieve clear disentanglement and do not generalize well. Future work may exploit some level of knowledge on the task distribution (e.g. some measure of distance on tasks) in order to tune \(,\) adaptively during training, or to train conditioning on a distribution of regularization parameters as in , enabling more generalization at test time. Another limitation is in the sampling procedure to fit the linear head at test time: sampling randomly from the training set (balanced by class) may not be enough to achieve the best performance under distributions shifts. Alternative sampling procedures, e.g. ones that incorporate knowledge on the distribution shift if available (as in ), may lead to better performance at test time.

  
**Dataset/Algorithm** &  \\ 
**PACS** & S & A & P & C & Average \\ ERM & 77.9 \(\) 0.4 & **88.1**\(\) 0.1 & 97.8 \(\) 0.0 & 79.1 \(\) 0.9 & 85.7 \\ Ours & **83.1**\(\) 0.1 & 86.7\(\) 0.8 & **97.8**\(\) 0.1 & **83.5**\(\) 0.1 & **87.5** \\ 
**VLCS** & C & L & V & S & Average \\ ERM & 97.6\(\) 1.0 & 63.3 \(\) 0.9 & 76.4 \(\) 1.5 & 72.2 \(\) 0.5 & 77.4 \\ Ours & **98.1**\(\) 0.2 & **63.4**\(\) 0.5 & **78.2**\(\) 0.7 & **73.9**\(\) 0.8 & **78.4** \\ 
**OfficeHome** & C & A & P & R & Average \\ ERM & 53.4\(\) 0.6 & 62.7 \(\) 1.1 & 76.5 \(\) 0.4 & 77.3 \(\) 0. & 67.5 \\ Ours & **56.3**\(\) 0.1 & **66.7**\(\) 0.7 & **79.2**\(\) 0.5 & **81.3**\(\) 0.4 & **70.9** \\ 
**Waterbirds** & LL & LW & WL & WW & Average \\ ERM & 98.6 \(\) 0.3 & 52.05 \(\) 3 & 68.5 \(\) 3 & 93 \(\) 0.3 & 81.3 \\ Ours & **99.5**\(\) 0.1 & **73.0**\(\) 2.5 & **85.0**\(\) 2 & **95.5**\(\) 0.4 & **90.5** \\   

Table 4: Results for domain generalization on DomainBed. Our approach achieves consistently higher average OOD generalization, outperforming ERM in all cases except one.