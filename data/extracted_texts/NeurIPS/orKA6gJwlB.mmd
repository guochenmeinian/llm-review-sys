# Probabilistic predictions with Fourier neural operators

Christopher Bulte

Ludwig-Maximilians-Universitat Munchen

Munich, Germany

buelte@math.lmu.de

&Philipp Scholl

Ludwig-Maximilians-Universitat Munchen

Munich, Germany

scholl@math.lmu.de

&Gitta Kutyniok

Ludwig-Maximilians-Universitat Munchen

University of Tromso

DLR-German Aerospace Center

Munich Center for Machine Learning (MCML)

Munich, Germany

kutyniok@math.lmu.de

###### Abstract

Neural networks have been successfully applied in modeling partial differential equations, especially in dynamical systems. Commonly used models, such as neural operators, are performing well at deterministic prediction tasks, but lack a quantification of the uncertainty inherent in many complex systems, for example weather forecasting. In this paper, we explore a new approach that combines Fourier neural operators with generative modeling based on strictly proper scoring rules in order to create well-calibrated probabilistic predictions of dynamical systems. We demonstrate improved predictive uncertainty for our approach, especially in settings with very high inherent uncertainty.

## 1 Introduction

Many complex phenomena in the sciences are described via time-dependent partial differential equations (PDEs), making their study a crucial research topic. Recent developments in machine learning led to an effective class of neural networks for solving PDEs, called neural operators (Kovachki et al., 2023). In dynamical systems, these models aim to learn the operator that maps an initial system state to the corresponding solution across time and have been applied to problems such as weather forecasting (Pathak et al., 2022) or fluid dynamics (Renn et al., 2023). However, neural operators are usually studied in the context of deterministic predictions, not accounting for the inherent uncertainty in complex and chaotic dynamical systems. Several methods have been proposed to enhance neural network architectures to quantify uncertainty. While some approaches focus on perturbing initial conditions (Pathak et al., 2022), many approaches are applied to the network post-hoc. These include statistical post-processing (Bulte et al., 2024) or Bayesian methods (Gal and Ghahramani, 2016). For neural operators, which learn an output in function space, uncertainty quantification can be more complex. Gal and Ghahramani (2016) propose to generate samples from a posterior predictive distribution by utilizing dropout in the model inference phase. Weber et al. (2024) and Magnani et al. (2024) use a Laplace approximation for the Fourier neural operator, which utilizes a linearized neural network to generate a tractable posterior distribution in function space.

In the context of spatial predictions, especially weather forecasting, methods based on the notion of proper scoring rules are commonly applied and have shown to work very well in combination with neural networks (Pacchiardi et al., 2024; Chen et al., 2024). However, this has not yet beentransferred to the setting of operator learning, which requires additional analysis of the corresponding scoring rules in infinite dimensional spaces. In this paper, we utilize proper scoring rules in separable Hilbert spaces in order to train a neural operator to estimate a predictive distribution over functions. We theoretically prove that this is well-motivated by showing that the energy score is strictly proper in infinite dimensional spaces. Our primary aim is to demonstrate the advantages of the approach in predicting dynamical systems. Our approach, which we refer to as probabilistic Fourier neural operator (PFO), leads to better-calibrated predictive distributions and adequate uncertainty representations even for long dynamical trajectories.

## 2 Neural operators

The aim of operator learning is to utilize a neural network to learn a mapping between two function spaces from a finite collection of input-output pairs. Consider an operator \(:\), acting on two separable Banach spaces of functions. A neural operator is a map \(_{}:\), that is parametrized by finitely many weights \(^{p}\) and trained on observational data \(\{(a_{n},u_{n})\}_{n=1}^{N}\), which aims to approximate the operator \(\). Here, \(a_{n}\) is usually an initial system state and \(u_{n}\) is the solution state of the PDE after some time \(T\). The most commonly used architecture is the Fourier neural operator (FNO) , which acts on an input function \(a\) by specifying several layers of integral kernels that are parametrized in Fourier space. By utilizing the convolution theorem, one so-called Fourier block is given as

\[G_{i}v_{i}(x)=(^{-1}(R_{i}(v_{i}))(x)+W _{i}v_{i}(x)),\] (1)

where \(\) and \(^{-1}\) are the Fourier transform and its inverse. Here, the matrix-valued functions \(R_{i}\) are directly parametrized in the Fourier domain as neural network weights. The whole network is specified as a combination of several Fourier blocks with some additional lifting and projection functions.

## 3 Probabilistic predictions using neural operators

The method we propose is based on generating a predictive distribution via a sample-based empirical distribution. We denote the initial condition and the solution of the PDE as \(a\) and \(u\), respectively, the spatio-temporal domain as \(\) and the empirical predictive distribution, for a fixed initial condition \(a\), as \(_{}^{M}=\{^{m}\}_{m=1}^{M}\) with samples \(^{m}\) for \(m=1,...,M\). For this analysis, we restrict ourselves to data from separable Hilbert spaces, which includes most solution spaces of PDEs, such as the Sobolev space \(H^{k},k\).

Scoring rule minimizationA scoring rule \(S\) is a function that assigns a real-valued score to the fit between a probability distribution and a corresponding observation . Define the so-called _expected score_ as \(S(Q,P):=_{X P}[S(Q,X)]\). The scoring rule is called _proper_ with respect to a class of probability measures \(\) if \(S(P,P) S(Q,P), P,Q\) and it is called _strictly proper_ if equality implies \(P=Q\). In other words, a scoring rule is strictly proper if the true distribution of the observation uniquely minimizes the expected score. More details on proper scoring rules can be found in Appendix A. Here, we focus mainly on the well-known _energy score_, which is defined as

\[(P,x):=_{P}[\|X-x\|_{}]- _{P}[\|X-X^{}\|_{}],\] (2)

where \(X,X^{}}{}P,x\) and \(\) is a separable Hilbert space. Pacchiardi et al.  show how generative neural networks can be trained via scoring rule minimization in the finite-dimensional setting. Consider data observation pairs of the form \((a_{i},u_{i})_{i=1}^{n}\), where \(a_{i} P_{}\) and \(u_{i} P_{}\) follow some distributions over a separable Hilbert space and let \(P_{}( a)\) denote an approximate posterior generated by the network. In the conditional data setting, we assume that \(u_{i} P^{*}( a_{i})\). For a (strictly) proper scoring rule, the minimization objective is given as

\[*{argmin}_{}_{a P_{}}_{ u P^{*}( a)}S(P_{}( a),u)\]

and leads to \(P_{}( a)=P^{*}( a)\) almost everywhere. In the finite data setting, this objective is approximated with a Monte Carlo estimator. While closed-form expressions of \(S\) are not always available, the energy score has a representation that admits an unbiased estimator, which requires the output from our neural network to consist of multiple samples of the predictive distribution, e.g. \((^{m})_{m=1}^{M} P_{}( a)\). In our case, the minimization objective for the neural network with the energy score then becomes

\[*{argmin}_{}_{i=1}^{n}(_ {m=1}^{M}\|_{i}^{m}-u_{i}\|_{}-_{ m,h=1\\ m h}^{M}\|_{i}^{m}-_{i}^{h}\|_{} ).\] (3)

In this paper, we extend this approach to the infinite-dimensional setting of neural operators. This is mathematically motivated, as we prove in Appendix A that the energy score is strictly proper in separable Hilbert spaces, which allows network training via scoring rule minimization. As the base architecture we utilize FNOs, and refer to our approach as probabilistic Fourier neural operator (PFNO). In order to create an empirical distribution as the network output, we focus on utilizing stochastic forward passes via dropout (Gal and Ghahramani, 2016). To further account for the structure of the FNO, we apply additional dropout over the parameters in Fourier space, which then acts globally on the network prediction. The PFNO has the advantage that it is easy to implement in an existing architecture and creates a nonparametric predictive distribution, allowing for more flexibility. We compare the PFNO against the MCDropout and Laplace approximation as baselines.

**Baselines:**Gal and Ghahramani (2016) show that a neural network with dropout before each layer is mathematically equivalent to a variational approximation of a Gaussian process. This leads to a simple and efficient way of creating a predictive distribution, referred to as MCDropout. In our setting, the predictive distribution is given as

\[_{}^{M}=\{_{}^{*}(a,_{m})\}_{m=1}^{M},\] (4)

where \(_{m}\) is the random dropout variable and \(_{}^{*}\) denotes a neural operator trained.

Weber et al. (2024) propose to utilize the Laplace approximation (LA) for FNOs, which is based on building a second-order approximation of the weights around the maximum a posteriori (MAP) estimate. By assuming a Gaussian weight prior, the weight-space uncertainty of the LA is given by

\[p(,)(;_{},), :=-(_{}^{2}(;)|_{_{ }})^{-1},\] (5)

where \(=\{(a_{n},u_{n})\}_{n=1}^{N}\). The corresponding predictive distribution in function space is an analytically available Gaussian and is used to generate \(M\) predictive samples. In contrast to the PFNO, both methods quantify uncertainty for an already trained neural operator.

## 4 Experimental results

We analyze our methods on two highly uncertain dynamical systems. First, the Kuramoto-Sivashinsky (KS) equation, which is a one-dimensional chaotic fourth-order parabolic PDE, described by

\[_{t}u(x,t)+u_{x}u(x,t)+_{x}^{2}u(x,t)+_{x}^{4 }u(x,t)=0, u(x,0)=u_{0}(x).\] (6)

We generate 10.000 samples (10% for validation/evaluation) over the domain \(=\). In addition, we evaluate the models on a 2-meter surface temperature prediction task, where we utilize the ERA5 dataset, provided via the WeatherBench benchmark (Rasp et al., 2024) with a spatial

  Experiment & Method & RMSE & ES & \(_{0.05}\) \\   & PFNO & 0.8793 (\(\) 0.0072) & **0.6195** (\(\) 0.0049) & **0.7640** (\(\) 0.0191) \\   & MCDropout & **0.8635** (\(\) 0.0048) & 0.7541 (\(\) 0.0049) & 0.3600 (\(\) 0.0082) \\   & Laplace & 0.8772 (\(\) 0.0055) & 0.7332 (\(\) 0.0103) & 0.3955 (\(\) 0.0189) \\    & PFNO & 0.0242 (\(\) 0.0000) & **0.0174** (\(\) 0.0000) & **0.8623** (\(\) 0.0033) \\   & MCDropout & **0.0240** (\(\) 0.0001) & 0.0202 (\(\) 0.0000) & 0.4456 (\(\) 0.0015) \\    & Laplace & 0.0251 (\(\) 0.0001) & 0.0195 (\(\) 0.0006) & 0.6956 (\(\) 0.1169) \\  

Table 1: Evaluation metrics and respective standard deviation for the Kuramoto-Sivashinsky equation and the 2-meter surface temperature prediction. The best model is highlighted in bold.

resolution of \(0.25^{}\) across Europe and a time resolution of \(6h\). We use data from 2011-2022 with the last two years as validation and test data respectively. For the KS data the model takes and predicts 20 timesteps, for the ERA5 it takes and predicts 10 timesteps (60 hours). For evaluation, we consider the following metrics:

\[(_{}^{M},u) =\|_{M}-u\|_{L^{2}},\] (7) \[(_{}^{M},u) =_{m=1}^{M}^{m}-u_{L^{2}}- _{m h}^{M}^{m}-^{h}_{L^{2}},\] (8) \[_{}(_{}^{M},u) =_{}\{u(x,t)[_{}^{ /2}(x,t),_{}^{1-/2}(x,t)]\}\,dx\;dt,\] (9)

where \(_{M}\) denotes the mean prediction, and \(_{}^{}\) denotes the empirical \(\) quantile of the predictive distribution. All metrics are then averaged over the validation or test dataset. The RMSE evaluates the match between the mean of the predictive distribution and the observation, while the energy score evaluates the match for the predictive distribution as a whole. The coverage \(_{}\) is calculated pointwise and describes, whether the predictive \(1-\) interval entails the true value. It should be close to \(1-\) for a well-calibrated prediction.

For a fair comparison, all methods use the same architecture, namely an FNO with 20 hidden channels, 10 modes in the time dimension, and 12 modes in the spatial dimension (2d or 3d respectively), and generate a predictive distribution of size \(M=100\). Furthermore, we tune the dropout for all methods separately via grid search, as they highly depend on this parameter. Finally, we run each experiment ten times to obtain robustness across the evaluation. The results for both experiments can be found in Table 1, while Figure 1 shows an additional analysis of the temporal behavior of the predictions for the Kuramoto-Sivashinsky equation.

## 5 Conclusion

The PFO obtains the best fit between the observation and the predictive distribution in terms of the energy score. Furthermore, for the temperature prediction task, the RMSE is comparable to or even lower than the other methods, although it is not explicitly minimized by the network. Finally, the

Figure 1: The figure shows the squared error, standard deviation, and 95%-coverage for the different methods on a random test sample of the Kuramoto-Sivashinsky equation. The mean coverage values from left to right are \(36.19\%,45.74\%\) and \(76.27\%\).

PFOO provides the best-calibrated prediction intervals, although the coverage is generally below the optimal value. For the temperature prediction task, the performance is significantly worse on the test set for all models and future research might revolve around making the approaches more robust against out-of-distribution data. While the Laplace approximation is very easy to use and admits an approximate analytically available Gaussian distribution, this might not be flexible enough for complex dynamical systems, if, for example, the uncertainty does not follow a symmetric distribution. Although it leads to a better mean estimation, as it is based on the MAP estimate, the predictive uncertainty is not as adequate. The MCDropout method lacks calibration in terms of coverage but also generally provides a good mean prediction.

While our approach shows improved performance, is easy to implement, and can be used with basically any architecture, it requires an additional training step and more computational power, as multiple samples are necessary to calculate the loss function. Still, these findings are very promising and encourage further analysis of neural operators trained with scoring rule minimization for complex dynamical systems. Some aspects to investigate are different suitable scoring rules, different ways of generating the samples, as well as ways to improve coverage and calibration of the prediction.