# One Risk to Rule Them All:

A Risk-Sensitive Perspective on Model-Based Offline Reinforcement Learning

Marc Rigter, Bruno Lacerda, Nick Hawes

Oxford Robotics Institute

University of Oxford

Correspondence: marcrigter@gmail.com

###### Abstract

Offline reinforcement learning (RL) is suitable for safety-critical domains where online exploration is not feasible. In such domains, decision-making should take into consideration the risk of catastrophic outcomes. In other words, decision-making should be _risk-averse_. An additional challenge of offline RL is avoiding _distributional shift_, i.e. ensuring that state-action pairs visited by the policy remain near those in the dataset. Previous offline RL algorithms that consider risk combine offline RL techniques (to avoid distributional shift), with risk-sensitive RL algorithms (to achieve risk-aversion). In this work, we propose risk-aversion as a mechanism to jointly address _both_ of these issues. We propose a model-based approach, and use an ensemble of models to estimate epistemic uncertainty, in addition to aleatoric uncertainty. We train a policy that is risk-averse, and avoids high uncertainty actions. Risk-aversion to epistemic uncertainty prevents distributional shift, as areas not covered by the dataset have high epistemic uncertainty. Risk-aversion to aleatoric uncertainty discourages actions that are risky due to environment stochasticity. Thus, by considering epistemic uncertainty via a model ensemble and introducing risk-aversion, our algorithm (1R2R) avoids distributional shift in addition to achieving risk-aversion to aleatoric risk. Our experiments show that 1R2R achieves strong performance on deterministic benchmarks, and outperforms existing approaches for risk-sensitive objectives in stochastic domains.

## 1 Introduction

In safety-critical applications, online reinforcement learning (RL) is impractical due to the requirement for extensive random exploration that may be dangerous or costly. To alleviate this issue, offline RL aims to find the best possible policy using only pre-collected data. In safety-critical settings we often want decision-making to be _risk-averse_, and take into consideration variability in the performance between each episode. However, the vast majority of offline RL research considers the expected value objective, and therefore disregards such variability.

A core issue in offline RL is avoiding distributional shift: to obtain a performant policy, we must ensure that the state-action pairs visited by the policy remain near those covered by the dataset. Another way that we can view this is that we need the policy to be averse to _epistemic_ uncertainty (uncertainty stemming from a lack of data). In regions not covered by the dataset, epistemic uncertainty is high and there will be large errors in any learned model or value function. A naively optimised policy may exploit these errors, resulting in erroneous action choices.

In risk-sensitive RL, the focus is typically on finding policies that are risk-averse to environment stochasticity, i.e. _aleatoric_ uncertainty. Previous approaches to risk-sensitive offline RL combine two techniques: offline RL techniques, such as conservative value function updates  or behaviour cloning constraints , to avoid distributional shift; and risk-sensitive RL algorithms, to achieve risk-aversion. Therefore, the issues of epistemic uncertainty and aleatoric uncertainty are handled _separately_, using different techniques. In contrast, we propose to avoid epistemic uncertainty due to distributional shift, as well as aleatoric uncertainty due to environment stochasticity, by computing a policy that is risk-averse to _both_ sources of uncertainty. This enables us to obtain a performant and risk-averse offline RL algorithm that is simpler than existing approaches.

In this work we propose 1R2R (Figure 1), a model-based algorithm for risk-averse offline RL. The intuition behind our approach is that the policy should avoid the risk of a bad outcome, regardless of whether this risk is due to high uncertainty in the model (epistemic uncertainty) or due to high stochasticity in the environment (aleatoric uncertainty). To represent epistemic uncertainty, we learn an ensemble of models. To optimise a risk-averse policy, we penalise taking actions that are predicted to have highly variable outcomes. In out-of-distribution regions, the disagreement between members of the ensemble is high, producing high variability in the model predictions. Therefore, the risk-averse policy is trained to avoid out-of-distribution regions. Risk-aversion also ensures that the policy avoids risk due to highly stochastic outcomes, i.e. avoids aleatoric risk. Our work demonstrates that incorporating _risk-aversion alone_ is a promising approach to offline RL, and proposes an algorithm that is simpler than existing approaches to risk-averse offline RL [47; 68]. Our main contributions are (a) proposing risk-aversion towards epistemic uncertainty as a mechanism to address distributional shift in offline RL, and (b) introducing the first model-based algorithm for risk-averse offline RL, which jointly addresses epistemic and aleatoric uncertainty.

We evaluate our approach in several scenarios. Our experiments show that our algorithm achieves strong performance relative to state-of-the-art baselines on deterministic benchmarks , and outperforms existing approaches for risk-sensitive objectives in stochastic domains.

## 2 Related Work

**Offline RL:** Offline RL addresses the problem of learning policies from fixed datasets. Most works on offline RL are _risk-neutral_, meaning that they optimise the expected value . A key issue in offline RL is avoiding distributional shift so that the states visited by the learnt policy remain within the distribution covered by the dataset . Approaches to model-free offline RL include: constraining the learnt policy to be similar to the behaviour policy [23; 38; 71; 22], conservative value function updates [11; 37; 40; 72], importance sampling algorithms [43; 51], or using Q-ensembles for more robust value estimates [1; 3; 73].

Model-based approaches learn a model of the environment and generate synthetic data from that model  to optimise a policy via planning  or RL algorithms [76; 75]. By training a policy on additional synthetic data, model-based approaches have the potential for broader generalisation . Approaches to preventing model exploitation include constraining the trained policy to be similar to the behaviour policy  or applying conservative value function updates  in the same fashion as model-free approaches. Another approach is to modify the learnt MDP by either applying reward penalties for state-action pairs with high uncertainty [35; 44; 74; 76], or modifying the transition model to produce pessimistic synthetic transitions [29; 58]. Our work is similar in spirit to [29; 58], but these existing works do not consider risk.

**Risk-Sensitive RL:** Many works have argued for risk-sensitive algorithms in safety-critical domains [25; 48]. Risk-sensitive algorithms for MDPs include approaches that adapt policy gradients to risk objectives [12; 66; 65; 67]. More recently, a number of works have utilised distributional RL [9; 50], which learns a distributional value function that predicts the full return distribution. Knowledge of the return distribution can then be used to optimise a risk-sensitive criterion [15; 33; 46].

Figure 1: Illustration of the 1R2R algorithm.

The main focus of research on risk-sensitive online RL is risk due to environment stochasticity  (i.e. aleatoric uncertainty). However, there are a number of works that explicitly address risk due to epistemic uncertainty [14; 16; 19; 18; 57]. To our knowledge, this is the first work to consider risk-aversion to epistemic uncertainty as a means to mitigate distributional shift in offline RL.

**Risk-Sensitive Offline RL:** To our knowledge, there are two previous algorithms for risk-sensitive offline RL: ORAAC  and CODAC . To avoid distributional shift, ORAAC utilises policy constraints, while CODAC employs pessimistic value function updates to penalise out-of-distribution state-action pairs. Both algorithms learn a distributional value function  which is then used to optimise a risk-averse policy. Thus, both of these works are model-free, and combine techniques from offline RL and distributional RL to address the issues of distributional shift and risk-sensitivity, respectively. In this work, we provide a new perspective by proposing to optimise for risk-aversion towards _both_ epistemic uncertainty (stemming from distributional shift) and aleatoric uncertainty (stemming from environment stochasticity). Unlike previous approaches, our approach (a) is conceptually simpler as it does not combine different techniques, (b) demonstrates that risk-aversion alone is sufficient to avoid distributional shift, (c) does not require a distributional value function (which might be computationally demanding to train), and (d) is model-based.

## 3 Preliminaries

An MDP is defined by the tuple \(M=(S,A,T,R,s_{0},)\). \(S\) and \(A\) are the state and action spaces, \(R(s,a)\) is the reward function, \(T(s^{}\,|\,s,a)\) is the transition function, \(s_{0}\) is the initial state, and \((0,1)\) is the discount factor. In this work we consider Markovian policies, \(\), which map each state to a distribution over actions. In offline RL we only have access to a fixed dataset of transitions from the MDP: \(=\{(s_{i},a_{i},r_{i},s^{}_{i})\}_{i=1}^{||}\). The goal is to find a performant policy using the fixed dataset.

**Model-Based Offline RL**: These approaches utilise a model of the MDP. The learnt dynamics model, \(\), is typically trained via maximum likelihood estimation: \(_{}}_{(s,a,s^{})} -(s^{}|s,a)\). A model of the reward function, \((s,a)\), is also learnt if it is unknown. This results in the learnt MDP model: \(=(S,A,,,s_{0},)\). Thereafter, any planning or RL algorithm can be used to recover the optimal policy in the learnt model, \(=_{}J_{}^{}\). Here, \(J\) indicates the optimisation objective, which is typically the expected value of the discounted cumulative reward.

However, directly applying this approach does not perform well in the offline setting due to distributional shift. In particular, if the dataset does not cover the entire state-action space, the model will inevitably be inaccurate for some state-action pairs. Thus, naive policy optimisation on a learnt model in the offline setting can result in _model exploitation_[32; 41; 55], i.e. a policy that chooses actions that the model erroneously predicts will lead to high reward. We propose to mitigate this issue by learning an ensemble of models and optimising a risk-averse policy.

Following previous works [35; 75; 76], our approach utilises model-based policy optimisation (MBPO) . MBPO performs standard off-policy RL using an augmented dataset \(}\), where \(}\) is synthetic data generated by simulating short rollouts in the learnt model. To train the policy, minibatches of data are drawn from \(}\), where each datapoint is sampled from the real data, \(\), with probability \(f\), and from \(}\) with probability \(1-f\).

**Risk Measures** We denote the set of all probability distributions by \(\). Consider a probability space, \((,,P)\), where \(\) is the sample space, \(\) is the event space, and \(P\) is a probability distribution over \(\). Let \(\) be the set of all random variables defined over the probability space \((,,P)\). We will interpret a random variable \(Z\) as a reward, i.e. the greater the realisation of \(Z\), the better. We denote a \(\)-weighted expectation of \(Z\) by \(_{}[Z]:=_{}P()()Z() \), where \(:\) is a function that re-weights the original distribution of \(Z\).

A _risk measure_ is a function, \(:\), that maps any random variable \(Z\) to a real number. An important class of risk measures are _coherent_ risk measures, which satisfy a set of properties that are consistent with rational risk assessments. A detailed description and motivation for coherent risk measures is given by . An example of a coherent risk measure is the conditional value at risk (CVaR). CVaR at confidence level \(\) is the mean of the \(\)-portion of the worst outcomes.

All coherent risk measures can be represented using their dual representation [6; 61]. A risk measure, \(\), is coherent if and only if there exists a convex bounded and closed set, \(_{}\), such that

\[(Z)=_{_{}(P)}_{}[Z]\] (1)

Thus, the value of any coherent risk measure for any \(Z\) can be viewed as an \(\)-weighted expectation of \(Z\), where \(\) is the worst-case weighting function chosen from a suitably defined _risk envelope_, \(_{}(P)\). For more details on coherent risk measures, we refer the reader to .

Static and Dynamic RiskIn MDPs, the random variable of interest is usually the total reward received at each episode, i.e. \(Z_{}=_{t=0}^{}^{t}R(s_{t},a_{t})\). The _static_ perspective on risk applies a risk measure directly to this random variable: \((Z_{})\). Thus, static risk does not take into account the temporal structure of the random variable in sequential problems, such as MDPs. This leads to the issue of _time inconsistency_, meaning that actions which are considered less risky at one point in time may not be considered less risky at other points in time. This leads to non-Markovian optimal policies, and prohibits the straightforward application of dynamic programming due to the coupling of risk preferences over time.

This motivates _dynamic_ risk measures, which take into consideration the sequential nature of the stochastic outcome, and are therefore time-consistent. Markov risk measures  are a class of dynamic risk measures which are obtained by recursively applying static coherent risk measures. Throughout this paper we will consider dynamic Markov risk over an infinite horizon, denoted by \(_{}\). For policy \(\), MDP \(M\), and static risk measure \(\), the Markov coherent risk \(_{}^{}(M)\) is defined as

\[_{}^{}(M)=R(s_{0},(s_{0}))+ R(s_{1},(s_{1} ))+^{2}R(s_{2},(s_{2}))+,\] (2)

where \(\) is a static coherent risk measure, and \((s_{0},s_{1},s_{2},)\) indicates random trajectories drawn from the Markov chain induced by \(\) in \(M\). Note that in Equation 2 the static risk measure is evaluated at each step according to the distribution over possible successor states.

We define the risk-sensitive value function under some policy \(\) as \(V^{}(s,M)=_{}^{}(M|s_{0}=s)\). This value function can be found by recursively applying the _risk-sensitive_ Bellman equation :

\[V^{}(s,M)=R(s,(s))+_{_{}((s, (s),))}_{s^{}}T(s,(s,s^{})(s^{})  V^{}(s^{},M).\] (3)

Likewise, the optimal risk-sensitive value, \(V^{*}(s,M)=_{}_{}(M|s_{0}=s)\), is the unique solution to the risk-sensitive Bellman optimality equation:

\[V^{*}(s,M)=_{a A}R(s,a)+_{_{}( (s,a,))}_{s^{}}T(s,a,s^{})(s^{ }) V^{*}(s^{},M)}.\] (4)

Thus, we can view Markov dynamic risk measures as the expected value in an MDP where the transition probabilities at _each step_ are modified _adversarially_ within the risk-envelope for the chosen one-step static risk measure.

## 4 One Risk to Rule Them All

In this section, we present _One Risk to Rule Them All_ (1R2R), a new algorithm for risk-averse offline RL (Figure 1). Our approach assumes that we can compute an approximation to the posterior _distribution over MDPs_, given the offline dataset. This distribution represents the epistemic uncertainty over the real environment, and enables us to reason about risk due to epistemic uncertainty. 1R2R then uses an RL algorithm to train a policy offline using synthetic data generated from the distribution over MDP models. To achieve risk-aversion, the transition distribution of the model rollouts is modified adversarially according to the risk envelope of a chosen risk measure. This simple modification to standard model-based RL penalises high uncertainty actions, thus penalising actions that have high epistemic uncertainty (i.e. are out-of-distribution) as well as those that have high aleatoric uncertainty (i.e. are inherently risky). Therefore, by modifying the transition distribution of model rollouts to induce risk-aversion, 1R2R avoids distributional shift _and_ generates risk-averse behaviour.

### Problem Formulation

To simplify the notation required, we assume that the reward function is known and therefore it is only the transition function that must be learnt. Note that in practice, we also learn the reward function from the dataset. Given the offline dataset, \(\), we assume that we can approximate the posterior distribution over the MDP transition function given the dataset: \(P(T)\). We can integrate over all possible transition functions to obtain a single expected transition function:

\[(s,a,s^{})=_{T}T(s,a,s^{}) P(T) T.\] (5)

This gives rise to a single MDP representing the distribution over plausible MDPs, \(=(S,A,,R,s_{0},)\). This is similar to the _Bayes-Adaptive_ MDP  (BAMDP), except that in the BAMDP the posterior distribution over transition functions is updated after each step of online interaction with the environment. Here, we assume that the posterior distribution is fixed given the offline dataset. To make this distinction, we refer to \(\) as the Bayesian MDP (BMDP) .

In this work, we pose offline RL as optimising the dynamic risk in the BMD induced by the dataset.

**Problem 4.1** (Risk-Averse Bayesian MDP for Offline RL).: Given offline dataset, \(\), static risk measure \(\), and belief over the transition dynamics given the dataset, \(P(T)\), find the policy with the optimal dynamic risk in the corresponding Bayesian MDP \(\):

\[^{*}=*{arg\,max}_{}_{}^{}()\] (6)

### Motivation

The motivation for Problem 4.1 is that optimising for risk-aversion in the Bayesian MDP results in risk-aversion to both aleatoric and epistemic uncertainty. We first provide intuition for this approach via the following empirical example. Then, we demonstrate this theoretically for the case of Gaussian transition models in Proposition 4.2.

Illustrative ExampleFigure 2 illustrates a dataset for an MDP where each episode consists of one step. A single reward is received, equal to the value of the successor state, \(s^{}\). We approximate \(P(T\!\!)\) using an ensemble of neural networks, and the shaded region represents the aggregated Bayesian MDP transition function \((s,a,s^{})\).

The dashed red line indicates the standard Q-values computed by drawing transitions from \((s,a,s^{})\). For this value function, the action with the highest Q-value is \(a=1\), which is outside of the data distribution. The solid red line indicates the _risk-sensitive_ value function for the risk measure \(_{0.1}\) (i.e. \(=0.1\)). This is computed by drawing transitions from a uniform distribution over the worst 10% of transitions from \((s,a,s^{})\). We observe that the risk-sensitive value function penalises straying far from the dataset, where epistemic uncertainty is high. The action in the dataset with highest expected value is \(a=0.4\). However, the action \(a=-0.35\) has a higher risk-sensitive value than \(a=0.4\), due to the latter having high-variance transitions (i.e. high aleatoric uncertainty). Thus, we observe that choosing actions according to the risk-sensitive value function penalises actions that are out-of-distribution (i.e. have high epistemic uncertainty) _or_ have high aleatoric uncertainty.

We now consider the specific case where each member of the ensemble is a Gaussian transition function with standard deviation \(_{A}\). We assume that in the ensemble, the mean of each Gaussian is also normally distributed with standard deviation \(_{E}\). The latter assumption allows us to quantify the epistemic uncertainty in terms of the level of disagreement between members of the ensemble. Proposition 4.2 shows that risk-averse optimisation of transitions sampled from the ensemble results in risk-aversion to both the aleatoric and epistemic uncertainty.

Figure 2: MDP with a single transition to a successor state \(s^{}\), with reward \(R(s^{})=s^{}\). Black dots illustrate dataset. Shaded region indicates \( 2\) S.D. of successor states drawn from an approximation of \((s,a,)\) (represented by an ensemble ). Dashed red line indicates the standard Q-values from running MBPO . Solid red line indicates the risk-sensitive value function computed by reweighting the transitions according to \(_{0.1}\). Vertical arrows indicate optimal actions for each Q-function.

**Proposition 4.2**.: _Consider some state \(s\) in a 1D state space, and some action \(a\). Assume (a) that there is an ensemble of \(N\) Gaussian transition functions, each denoted \(T_{i}\) with mean \(_{i}\) and standard deviation \(_{A}\): \(\{T_{i}(s^{}|s,a)=(_{i},_{A}^{2})\}_{i=1}^{N}\); and (b) that the mean of each Gaussian, \(_{i}\), is normally distributed with mean \(_{0}\) and standard deviation \(_{E}\): \(_{i}(_{0},_{E}^{2})\). The \(N\) transition functions jointly define \(\), the transition function for Bayesian MDP, \(\). Assume that for some policy \(\), the risk-sensitive value is linear around \(_{0}\) with some linearity constant, \(K\):_

\[V^{}(s^{},)=V^{}(_{0},)+K(s^{}- _{0})\]

_Then, the value of a randomly drawn successor state is distributed according to \(=V^{}(_{0},),^{2}=K^{2}(_{ A}^{2}+_{E}^{2})\)._

In Proposition 4.2, \(_{E}\) defines the level of disagreement between the members of the ensemble, and therefore represents the level of epistemic uncertainty. \(_{A}\) represents the aleatoric uncertainty. The proposition shows that if either \(_{A}\) or \(_{E}\) are high, then there is high variability over the value of the successor state when sampling from the ensemble (i.e. sampling from \(\) in Equation 5). Risk measures penalise high variability. Therefore, applying the risk-sensitive Bellman equation (Equation 3) to samples from \(\) penalises executing state-action pairs for which either \(_{A}\) or \(_{E}\) is high. For the specific case of CVaR, Corollary 4.3 illustrates how the risk-sensitive value is reduced as \(_{A}\) or \(_{E}\) increase.

**Corollary 4.3**.: _Under the assumptions in Proposition 1, the CVaR at confidence level \(\) of the value of a randomly drawn successor state is:_

\[_{}V^{}(s^{},) s^{} ( s,a)=V^{}(_{0},)-^{2}+_{A}^{2}}}{}^{-( ^{-1}())^{2}}\]

_where \(^{-1}\) is the inverse of the standard normal CDF._

These results demonstrate that optimising for risk-aversion in the Bayesian MDP (Problem 4.1) results in risk-aversion to both aleatoric and epistemic uncertainty, as state-action pairs with _either_ high epistemic or high aleatoric uncertainty are penalised. Therefore, Problem 4.1 favours choosing actions that have both low aleatoric and low epistemic uncertainty.

### Approach

Our algorithm is inspired by the model-free online RL method for _aleatoric risk only_ introduced by , but we have adapted it to the model-based offline RL setting. Following previous works on model-based offline RL, our algorithm makes use of a standard actor-critic off-policy RL algorithm for policy optimisation [58; 75; 76]. However, the key idea of our approach is that to achieve risk-aversion, we modify the distribution of trajectories sampled from the model ensemble. Following Equation 3, we first compute the adversarial perturbation to the transition distribution:

\[^{*}=*{arg\,min}_{_{}((s,a, ))}_{s^{}}(s,a,s^{})(s^{})  V^{}(s^{},),\] (7)

where the value function \(V^{}\) is estimated by the off-policy RL algorithm. When generating synthetic rollouts from the model, for each state-action pair \((s,a)\) we sample successor states from the perturbed distribution \(^{*}\):

\[s^{}^{*}(s^{})(s,a,s^{})\;\; s ^{}\] (8)

This perturbed distribution increases the likelihood of transitions to low-value states. The transition data sampled from this perturbed distribution is added to the synthetic dataset, \(}\). This approach modifies the sampling distribution over successor states according to the risk-sensitive Bellman equation in Equation 3. Therefore, the value function learnt by the actor-critic RL algorithm under this modified sampling distribution is equal to the risk-sensitive value function. The policy is then optimised to maximise the risk-sensitive value function.

For the continuous problems we address, we cannot compute the adversarially modified transition distribution in Equation 7 exactly. Therefore, we use a sample average approximation (SAA)  to approximate the solution. Specifically, we approximate \((s,a,)\) as a uniform distribution over \(m\) states drawn from \((s,a,)\). Then, we compute the optimal adversarial perturbation to this uniform distribution over successor states. Under standard regularity assumptions, the solution to the SAAconverges to the exact solution as \(m\). Details of how the perturbed distribution is computed for CVaR and the Wang risk measure are in Appendix B.

**Algorithm** Our approach is summarised in Algorithm 1 and illustrated in Figure 1. At each iteration, we generate \(N_{}\) truncated synthetic rollouts which are branched from an initial state sampled from the dataset (Lines 4-6). For a given \((s,a)\) pair we sample a set \(\) of \(m\) candidate successor states from \(\) (Line 8). We then approximate the original transition distribution by \(\), a uniform distribution over \(\), in Line 9. Under this approximation, it is straightforward to compute the worst-case perturbed transition distribution for a given risk measure in Line 10 (see Appendix B). The final successor state \(s^{}\) is sampled from the perturbed distribution in Line 11, and the transition to this final successor state is added to \(}\).

After generating the synthetic rollouts, the policy and value function are updated using an off-policy actor-critic algorithm by sampling data from both the synthetic dataset and the real dataset (Line 14). Utilising samples from the real dataset means that Algorithm 1 does not precisely replicate the risk-sensitive Bellman equation (Equation 3). However, like [58; 75] we found that additionally utilising the real data improves performance (see Appendix D.4).

**Implementation Details** Following a number of previous works [29; 54; 76] we use a uniform distribution over an ensemble of neural networks to approximate \(P(T\,|\,)\). We also learn an ensemble of reward functions from the dataset, in addition to the transition model. Each model in the ensemble, \(T_{i}\), outputs a Gaussian distribution over successor states and rewards: \(T_{i}(s^{},r\,|\,s,a)=(_{i}(s,a),_{i}(s,a))\). For policy training we used soft-actor critic (SAC)  in Line 14. For policy improvement, we used the standard policy gradient that is utilised by SAC. We found this worked well in practice and meant that our approach requires minimal modifications to existing RL algorithms. Research on specialised policy gradients for risk-sensitive objectives can be found in [65; 12].

## 5 Experiments

In our experiments, we seek to: (a) verify that 1R2R generates performant policies in deterministic environments by avoiding distributional shift, (b) investigate whether 1R2R generates risk-averse behaviour on stochastic domains, and (c) compare the performance of 1R2R against existing baselines on both stochastic and deterministic environments. To examine each of these questions, we evaluate our approach on the following domains. More information about the domains can be found in Appendix E.2, and code for the experiments can be found at github.com/marc-rigter/1R2R.

**D4RL MuJoCo** There are three deterministic robot environments (_HalfCheetah, Hopper, Walker2D_), each with 4 datasets (_Random, Medium, Medium-Replay, Medium-Expert_).

**Currency Exchange** We adapt the Optimal Liquidation problem  to offline RL. The agent aims to convert 100 units of currency A to currency B, subject to a stochastic exchange rate, before a deadline. The dataset consists of experience from a random policy.

**HIV Treatment** We adapt this online RL domain [33; 20] to the offline setting. The 6-dimensional continuous state vector represents the concentrations of various cells and viruses. The actions correspond to the dosages of two drugs. Transitions are stochastic due to the efficacy of each drug varying stochastically at each step. The reward is determined by the health outcomes for the patient, and penalises side-effects due to the quantity of drugs prescribed. The dataset is constructed in the same way as the D4RL _Medium-Replay_ datasets.

**Stochastic MuJoCo** We modify the D4RL benchmarks by adding stochastic perturbation forces. We focus on Hopper and Walker2D as there is a risk that any episode may terminate early due to the robot falling over. We vary the magnitude of the perturbations between two different levels (_Moderate-Noise_, _High-Noise_). For each domain and noise level we construct _Medium_, _Medium-Replay_, and _Medium-Expert_ datasets in the same fashion as D4RL.

**Baselines** We compare 1R2R against offline RL algorithms designed for risk-sensitivity (ORAAC  and CODAC ) in addition to performant risk-neutral model-based (RAMBO , COMBO , and MOPO ) and model-free (IQL , TD3+BC , CQL , and ATAC ) algorithms. Due to computational constraints, we only compare a subset of algorithms for Stochastic MuJoCo. Following , the results are normalised to approximately between 0 and 100.

**Objectives and Hyperparameters** A disadvantage of the dynamic risk perspective used by 1R2R is that it is unclear how to evaluate performance for dynamic risk. For this reason, we evaluate the algorithms using static risk measures. For D4RL, which is deterministic, we optimise the standard expected value objective. Following [68; 47], we use static CVaR\({}_{0.1}\) (CVaR at confidence level \(=0.1\)) as the optimisation objective for all other domains. This is the average total reward received on the worst 10% of runs. For ORAAC and CODAC, we set the optimisation objective to the desired objective for each domain. For all other baselines, we tune hyperparameters to obtain the best online performance for the desired objective on each dataset (see Appendix E.5). Likewise, we tune the hyperparameters for 1R2R (the rollout length and risk measure parameter) to achieve the best online performance as described in Appendix C.4.

**Versions of 1R2R** We test two versions of 1R2R which utilise different risk measures to re-weight the transition distribution to successor states: 1R2R\({}_{}\) and 1R2R\({}_{}\). These risk measures are defined formally in Appendix B. We chose CVaR because it is commonly used and easy to interpret. However, CVaR disregards the best possible outcomes, and this can lead to an unnecessary loss in performance . Therefore, we also investigate using the Wang risk measure which is computed by reweighting the entire distribution over outcomes.

We present results for two ablations. _Ablate risk_ is the same as 1R2R, except that the transition distribution of the rollouts is not modified, making this ablation equivalent to standard model-based RL. _Ablate ensemble_ uses only a single model, and therefore the risk aversion is only with respect to the aleatoric uncertainty.

**Results Presentation** The results in Tables 1-3 are from evaluating the policies over the last 10 iterations of training, and averaged over 5 seeds. The highlighted numbers indicate results within 10% of the best score, \(\) indicates the standard deviation over seeds, and "div." indicates that the value function diverged for at least one training run.

### Results

**D4RL MuJoCo** For these domains, the objective is to optimise the expected performance. Therefore, to generate the results in Table 1 we tune all algorithms to obtain the best expected performance. Note that to obtain strong expected performance on these domains, 1R2R still utilises risk-aversion to avoid distributional shift. Table 1 shows that both configurations of 1R2R outperform most of the baselines, and are competitive with the strongest risk-neutral baselines. The two existing offline RL algorithms that are able to consider risk, ORAAC and CODAC, perform poorly on these benchmarks. The ablation of the risk-sensitive sampling shows that when risk-sensitivity is removed from 1R2R, for many domains the training diverges (denoted by "div.") or performance degrades. This verifies that risk-aversion is crucial in our approach to ensure that the policy avoids distributional shift, and therefore also avoids the associated problem of value function instability . Likewise, the ablation of the model ensemble also leads to unstable training for some datasets, indicating that risk-aversion to epistemic uncertainty (represented by the ensemble) is crucial to our approach.

**Currency Exchange** From now on, we consider stochastic domains where the objective is to obtain the best performance for the static CVaR\({}_{0.1}\) objective. All algorithms are tuned to optimise this

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]