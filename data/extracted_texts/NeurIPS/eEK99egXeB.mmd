# OpenDataVal: a Unified Benchmark for Data

Valuation

 Kevin Fu Jiang

Columbia University

&Weixin Liang

Stanford University

&James Zou

Stanford University

&Yongchan Kwon

Columbia University

Co-first author

###### Abstract

Assessing the quality and impact of individual data points is critical for improving model performance and mitigating undesirable biases within the training dataset. Several data valuation algorithms have been proposed to quantify data quality, however, there lacks a systemic and standardized benchmarking system for data valuation. In this paper, we introduce OpenDataVal, an easy-to-use and unified benchmark framework that empowers researchers and practitioners to apply and compare various data valuation algorithms. OpenDataVal provides an integrated environment that includes (i) a diverse collection of image, natural language, and tabular datasets, (ii) implementations of eleven different state-of-the-art data valuation algorithms, and (iii) a prediction model API that can import any models in scikit-learn. Furthermore, we propose four downstream machine learning tasks for evaluating the quality of data values. We perform benchmarking analysis using OpenDataVal, quantifying and comparing the efficacy of state-of-the-art data valuation approaches. We find that no single algorithm performs uniformly best across all tasks, and an appropriate algorithm should be employed for a user's downstream task. OpenDataVal is publicly available at https://opendataval.github.io with comprehensive documentation. Furthermore, we provide a leaderboard where researchers can evaluate the effectiveness of their own data valuation algorithms.

## 1 Introduction

Real-world data often exhibit heterogeneity and noise as they are collected from a wide range of sources and are susceptible to measurement and annotation errors . When such data are incorporated into model development, they can introduce undesirable biases and potentially hinder model performance. Managing and addressing these challenges is crucial to ensure the reliability and accuracy of the insights derived from real-world data. As such, it is becoming increasingly important to understand and evaluate the intrinsic properties of data, such as data quality, data bias, and their influences on the model training process . From all these motivations, data valuation has emerged as a principled evaluation framework that aims to quantify the impact of individual data points on model predictions or model performance.

There have been substantial recent efforts in developing different data valuation methods, and they have demonstrated promising outcomes in various real-world applications. One widely used data valuation method is DataShapley , which is based on the Shapley value from game theory . This method has been applied to detect low-quality chest X-ray image data  and design a datamarketplace . BetaShapley generalizes DataShapley by relaxing the efficiency axiom in the Shapley value and has demonstrated its efficacy in identifying mislabeled images in the CIFAR-100 test dataset . Recently, Ilyas et al.  propose datamodels and provide qualitative analyses of the train-test leakage that could hinder fair evaluations. Apart from these methods, many data valuation algorithms have been developed to capture different aspects of data values  or improve computational efficiency . Nevertheless, there has been little attention to establishing a user-friendly and standardized benchmarking system, and this is the main goal of this work.

Our contributionsWe introduce OpenDataVal, an easy-to-use and unified benchmark framework that allows researchers and practitioners to apply and compare different data valuation algorithms with a few lines of Python codes. OpenDataVal provides an integrated environment that includes (i) a diverse collection of image, natural language, and tabular datasets that are publicly available at open platforms OpenML , scikit-learn , and PyTorch , (ii) implementations of eleven different state-of-the-art data valuation algorithms in Table 1, and (iii) a prediction model API that enables to import any machine learning models in scikit-learn and PyTorch-based neural network models. In addition, OpenDataVal provides four downstream machine learning tasks for evaluating the quality of data values, noisy label data detection, noisy feature data detection, point removal experiment, and point addition experiment. We also demonstrate how OpenDataVal can be employed through benchmarking analysis in Section 4. We find no single data valuation algorithm performs uniformly superior across all the benchmark tasks. That is, different algorithms have their own advantages and capture different aspects of data values. We encourage users to choose an appropriate algorithm for their analysis.

OpenDataVal is designed to advance the transparency and reproducibility of data valuation works and to foster user engagement in data valuation. To support this endeavor, we have made OpenDataVal open-source at https://github.com/opendataval/opendataval, enabling anyone to contribute to our GitHub repository. Additionally, we have developed a leaderboard where users can participate in noisy data detection competitions. OpenDataVal will be regularly updated and maintained by the authors and participants.

Related worksSim et al.  conducted a technical survey of data valuation algorithms, focusing on the necessary assumptions and their desiderata. While their analysis provides mathematical insights, it lacks a comprehensive empirical comparison. Our framework primarily focuses on this problem, creating an easy-to-use benchmarking system that facilitates empirical comparisons. Valda3 is a Python package with a set of data valuation algorithms including LOO, DataShapley , and BetaShapley . OpenDataVal provides most of the data valuation algorithms available in Valda, plus additional methods, as well as an easy-to-use environment in which practitioners

Figure 1: OpenDataVal is an open-source, easy-to-use and unified benchmark framework for data valuation. It contains a diverse collection of datasets, implementations of eleven state-of-the-art data valuation algorithms, and a variety of modern prediction models. In addition, OpenDataVal provides several downstream tasks for evaluating the quality of data valuation algorithms.

and data analysts compute and evaluate various state-of-the-art data valuation algorithms in diverse downstream tasks.

DataPerf  has provided six realistic benchmark tasks: training set creation, test set creation, selection algorithm, debugging algorithm, slicing algorithm, and valuation algorithm. These tasks cover important data-centric problems that can arise in the model development process, but we emphasize that our main objective differs from that of DataPerf work. While they mainly focus on creating benchmark tasks and competitions, our main objective is to provide a unified environment for quantifying and comparing various data valuation algorithms.

## 2 A taxonomy of data valuation methods

In this section, we introduce a taxonomy of data valuation methods. For \(d\), we denote an input space and an output space by \(^{d}\) and \(\), respectively. We denote a training dataset by \(=\{(x_{i},y_{i})\}_{i=1}^{n}\) where \(x_{i}\) and \(y_{i}\) are an input and its label for the \(i\)-th datum. Then, all data valuation methods can be seen as a mapping that assigns a scalar score to each sample in \(\), and each quantifies the impact of the point on the performance of a model trained on \(\).

For instance, the leave-one-out (LOO) is defined as \(_{}(x_{i},y_{i}):=U()-U(\{(x_ {i},y_{i})\})\) where \(U\) represents a preset utility function that takes as input a subset of the training dataset. In classification settings, a common choice for \(U\) is the test accuracy of a model trained on the input. In this case, the \(i\)-th LOO value \(_{}(x_{i},y_{i})\) measures the change in the test accuracy when the \(i\)-th data point \((x_{i},y_{i})\) is removed from the training procedure.

This concept of LOO has been generalized to the marginal contribution, which captures the average change in utility when a certain datum is removed from a set with a given cardinality. To be more specific, for a given cardinality \(1 j n\), the marginal contribution of \((x_{i},y_{i})\) with respect to \(j\) samples is defined as \(_{j}(x_{i},y_{i}):=^{-1}_{S_{j} ^{(x_{i},y_{i})}}U(S\{(x_{i},y_{i})\})-U(S)\) where \(_{j}^{(x_{i},y_{i})}:=\{S:S\{(x_{i},y _{i})\},|S|=j-1\}\). Many data valuation methods are expressed as a function of marginal contributions, for instance, DataShapley  considers the simple average of marginal contributions \(_{}(x_{i},y_{i}):=_{j=1}^{n}_{j}(x_{i}, y_{i})\) and BetaShapley  considers a weighted average of marginal contributions \(_{}(x_{i},y_{i}):=_{j=1}^{n}w_{,j}_{j }(x_{i},y_{i})\) for some

    &  &  \\   & & Noisy Label & Noisy Feature & Point & Point \\  & & Detection & Detection & Removal & Addition \\  LOO & Marginal contribution & - & - & + & + \\ DataShapley  & Marginal contribution & + & + & ++ & ++ \\ KNNShapley  & Marginal contribution & + & + & ++ & ++ \\ Volume-based Shapley  & Marginal contribution & - & - & - & - \\ BetaShapley  & Marginal contribution & + & + & ++ & ++ \\ DataBanzhaf  & Marginal contribution & - & - & + & + \\ AME  & Marginal contribution & - & - & ++ & + \\ InfluenceFunction  & Gradient & - & - & + & + \\ LAVA  & Gradient & - & ++ & + & + \\ DVRL  & Importance weight & + & - & + & ++ \\ Data-OB  & Out-of-bag estimate & ++ & + & - & ++ \\   

Table 1: A taxonomy of data valuation algorithms available in OpenDataVal. LOO, KNN Shapley, Volume-based Shapley, DataShapley, BetaShapley, DataBanzhaf, and AME can be expressed as functions of marginal contributions. There have been alternative approaches to defining data values: the gradient, importance weights, and out-of-bag estimate. KNNShapley only works for KNN predictors; the other data valuation methods are compatible with any prediction models. The ‘Quality of Data Values’ section summarizes the benchmark results in Section 4. The symbols ‘- / + / ++’ indicate that a corresponding data valuation method achieves a ‘similar or worse / better / much better’ performance than a random baseline, respectively. There is no single method that uniformly outperforms others in every task. Therefore, it is critical for users to selectively consider data valuation algorithms for their own downstream tasks.

weight vector \((w_{,1},,w_{,n})\). LOO, DataBanzhaf , DataModels , and the average marginal effect (AME)  are also included in this category, and they differ in how to combine marginal contributions into a single score.

There are alternative approaches to describing the concept of data values: gradient, importance weight, and out-of-bag estimate. The gradient methods are to quantify the rate at which a utility value changes when a particular data point is more weighted. The influence function  and LAVA  are included in this category. They differ depending on what utility value is being measured: the influence function uses the validation accuracy as a utility while LAVA uses the optimal transport cost.

The importance weight method learns a weight function \(w:\) so that an associated model \(f_{w}\) achieves a good accuracy on the holdout validation dataset. Here, the model \(f_{w}:\) is often obtained by minimizing a weighted risk \([w(X,Y)(Y,f(X))]\) for some loss function \(:\) and a preset weight function \(w\). Data valuation using reinforcement learning (DVRL)  is in this category, and they proposed a reinforcement learning technique to obtain a weight function.

An out-of-bag (OOB) estimation approach measures the contribution of each data point to out-of-bag accuracy when a bagging model is employed. Data-OOB  is in this category, and it captures how much a nominal label differs from predictions generated by weak learners. We provide a rigorous definition of each data valuation method along with their properties in Appendix. A taxonomy of data valuation algorithms with a brief summary of empirical results is presented in Table 1.

## 3 Overview of OpenDataVal

OpenDataVal provides a unified API called ExperimentMediator, which enables the quantification of data values and systemic comparative experiments. With ExperimentMediator, users can specify various datasets, data valuation algorithms, and hyperparameters (_e.g._, a prediction model, model performance metric, and synthetic noise). The following code snippet illustrates how to compute data values using the embedding of the CIFAR10 dataset. Here, the embedding is obtained from the penultimate layer of the ResNet50 model  pretrained on the ImageNet dataset .

``` fromopendataval.experimentimportExperimentMediator exper_med=ExperimentMediator.model_factory_setup( dataset_name="cifar10-embedding",model_name="ClassifierMLP", metric_name="accuracy",output_dir="outputs/", add_noise="mix_labels",noise_kwargs=("noise_rate":0.2) ) data_evaluators=[DataShapley(), DVRL()] exper_med=exper_med.compute_data_values(data_evaluators) ```

Code snippet 1: This code snippet shows how to compute DataShapley  and DVRL  values for the CIFAR-10 dataset using a multilayer perceptron classifier as a prediction model, with classification accuracy as the evaluation metric. With 'add_noise' and 'noise_kwargs' arguments, a synthetic noise has been applied: 20% of the training dataset is randomly chosen and their original labels have been changed to different labels.

In addition, OpenDataVal provides a collection of evaluation tasks as demonstrated in Section 4. Using the computed data values in the previous step, users can conduct noisy label data detection tasks to systematically evaluate the performance of data valuation algorithms.

OpenDataVal provides a convenient way to compute and compare various state-of-the-art data valuation algorithms with a few lines of Python codes. It is also designed to be highly extensible and easily adaptable to custom experiment settings. We provide details on customization in Appendix.

LeaderboardsOpenDataVal introduces the public leaderboards to promote transparency and healthy competition in the field of data valuation. In the initial release, we provide a noisy label data detection task for a selection of datasets. The challenge datasets, accessible within the OpenDataVal platform, contain label noise that is unknown to the user. The goal is to develop their own data valuation algorithms and find which data points have had noise added. Users can submit their algorithm outputs in the form of a CSV file, and our interactive ranking system will periodically update the leaderboard, showcasing the performance of different submissions. Submissions and competition details are available at https://opendataval.github.io/leaderboards.html.

## 4 Benchmarking Analysis

We present the practical applications of OpenDataVal through benchmarking analysis. We conduct the four machine learning tasks: noisy label data detection, noisy feature data detection, point addition experiment, and point removal experiment. These experiments have been widely used in the literature [7; 42; 13]. We provide details on evaluation metrics in Appendix. All the datasets, data valuation algorithms, prediction models, and downstream tasks employed in the benchmarking analysis are readily accessible within OpenDataVal, ensuring the transparency and reproducibility of all benchmark results.

Experimental settingsWe use the nine datasets in Table 2. These datasets are standard and widely used within the literature [7; 18]. Due to space limitations in the main text, we primarily focus on the analysis of the six tabular datasets, but our main findings are consistently observed across different modalities and datasets. Results for the text and image datasets are provided in Appendix. We compare the eleven different data valuation algorithms in Table 1. Additionally, we include a baseline algorithm, which assigns random numbers to data values. The InfluenceFunction, Volume-based Shapley, DataBanzhaf, AME, and Data-OOB require training models multiple times, and the number of trained models has a substantial impact on both performance and runtime. To ensure a fair comparison, we have set this parameter to a fixed number of \(1000\). As for the other algorithms that do not require training models multiple times, we maintain fair comparisons by utilizing hyperparameters similar to those specified in the original papers. As for the base prediction model, we use a logistic regression model. However, it is important to note that the logistic regression model is not compatible with KNNShapley as it is implicitly based on KNN predictors and has a closed-form expression. The sample size for the training dataset is set to \(n=1000\). Implementation details are provided in Appendix.

    &  & Input & Number of & Minor Class &  &  \\  & & Dimension & Classes & Proportion & & \\  electricity & 38474 & 6 & 2 & 0.5 & Tabular &  \\ fried & 40768 & 10 & 2 & 0.498 & Tabular & OpenML-901 \\
2dplanes & 40768 & 10 & 2 & 0.499 & Tabular & OpenML-727 \\ pol & 15000 & 48 & 2 & 0.336 & Tabular & OpenML-722 \\ MiniBooNE & 72998 & 50 & 2 & 0.5 & Tabular &  \\ nomo & 34465 & 89 & 2 & 0.285 & Tabular &  \\ bbc-embedding & 2225 & 768 & 5 & 0.17 & Text &  \\ IMDB-embedding & 50000 & 768 & 2 & 0.5 & Text &  \\ CIFAR10-embedding & 50000 & 2048 & 10 & 0.1 & Image &  \\   

Table 2: A summary of classification datasets used in the benchmarking analysis. For natural language and image datasets, the pretrained DistilBERT  and ResNet50  models are employed to extract an embedding. For ‘fried’, ‘2dplanes’, and ‘pol’ datasets, the number indicates their OpenML-Data-ID. All datasets are readily available in OpenDataVal.

### Noisy data detection

We compare the detection capabilities of different data valuation algorithms using synthetically generated noisy datasets. We consider two types of synthetic noise. The first type is label noise where we flip the original label to its opposite label. The second type is feature noise where we add standard Gaussian random errors to the original features. After selecting one of two noises, we then randomly choose \(p_{}\%\) of the training dataset to perturb. Here, the four different levels of noise proportion \(p_{}\{5,10,15,20\}\) is considered.

Data values are computed for this noisy dataset, and we investigate which data valuation algorithm is the most effective to identify these noisy data points. Based on the literature [39; 17; 18], we follow the approach of applying the \(k\)-means clustering algorithm to the data values . This allows us to dichotomize the training dataset into two groups: beneficial and detrimental. We assign the detrimental label to the cluster with a lower average of data values. Given our expectation that mislabeled data points are likely to have low values, we predict data points within the detrimental group as noisy samples. We evaluate the F1-score between this prediction and the ground-truth annotation. Note that we do not use ground-truth annotations for noisy data points when computing data values. We only use them when we evaluate the quality of data valuation algorithms.

Noisy label data detectionFigure 2 shows the F1-score of data valuation algorithms for the noisy label data detection tasks. The results can be grouped into three clusters based on their performance. The first cluster consists of a single algorithm, Data-OOB. It achieves significantly better results than other data valuation algorithms on most of the datasets and noise proportions. For instance, on the 'MiniBooNE' dataset with \(p_{}=20\%\), Data-OOB shows \(0.65\) F1-score when the second best algorithm KNNShapley shows \(0.53\) and the random baseline shows \(0.29\). The second cluster consists of DVRL, KNNShapley, DataShapley, and BetaShapley. They show reasonably sound detection ability compared to the random baseline. Volume-based Shapley, LAVA, LOO, InfluenceFunction, AME, and DataBanzhaf belong to the third cluster and show similar performance to the random baseline.

Noisy feature data detectionFigure 3 shows the F1-score of different data valuation algorithms for the noisy feature data detection tasks. In contrast to the noisy label data detection, LAVA generally outperforms other methods with a substantial performance gap. This difference is potentially due to the transport cost function, one of the key hyperparameters in LAVA . The default choice is designed to be sensitive to noises in features, leading to a good performance in noisy feature

Figure 2: **Noisy label data detection.** F1-score of different data valuation algorithms on the four noise proportion settings. The higher the F1-score is, the better the data valuation algorithm is. The error bar indicates a 95% confidence interval based on 50 independent experiments. Data-OOB demonstrates significantly superior performance in detecting mislabeled data points in various situations.

data detection. However, we observe that LAVA's performance varies a lot across different datasets, suggesting that this choice of the transport cost function is important. Following LAVA, Data-OOB and Shapley-based methods demonstrate a better performance than the random baseline, showing their potential efficacy in noisy feature data detection. Volume-based Shapley, DVRL, InfluenceFunction, AME, and LOO show only comparable performance to the random baseline in this task.

### Point removal and addition experiments

Data values can be used to find influential samples. To evaluate this quantitatively, we perform the point addition and removal experiments used in [7; 18]. The point removal experiment is performed with the following steps: For each data valuation algorithm, we remove data points from the entire training dataset in descending order of the data values. Each time the datum is removed, we fit a logistic regression model with the remaining dataset and evaluate its test accuracy on the holdout dataset. As we remove the data points in descending order, in the ideal case we remove the most helpful data points first, and thus model accuracy is expected to decrease. For the point addition experiment, we perform a similar procedure but add data points in ascending order. Similar to the point removal experiment, the model accuracy is expected to be low as we add detrimental data points first. Throughout the experiments, we used a perturbed dataset with label noise. The noise ratio is set to \(p_{}=20\%\). The sample size of the holdout test dataset is set to \(3000\). All the procedures here can be easily performed in OpenDataVal.

Point removal experimentFigure 4 shows test accuracy curves for the point removal experiment. Overall, all data valuation algorithms except Data-OOB perform better than the random baseline. Data-OOB often shows worse performance than the random baseline, indicating its effectiveness for finding low-quality data points does not imply finding helpful data points. AME, DataShapley, BetaShapley, and KNNShapley generally show superior performance. In particular, on the 'fried' dataset, these methods achieve at most 75% test accuracy after removing 20% of the most influential data points. DataBanzhaf and InfluenceFunction also show solid performance, while DVRL shows large performance differences across different datasets. LOO shows a slightly better performance than the random baseline.

Point addition experimentFigure 5 shows test accuracy curves for the point addition experiment. All data valuation algorithms generally outperform the random baseline, showing their efficacy in identifying low-quality data points. Data-OOB uniformly shows the lowest test accuracy compared

Figure 3: **Noisy feature data detection.** F1-score of different data valuation algorithms on the four noise proportion settings. The higher the F1-score is, the better the data valuation algorithm is. The error bar indicates a 95% confidence interval based on 50 independent experiments. LAVA generally outperforms other data valuation algorithms with a big performance gap.

to other algorithms, demonstrating the best performance in finding low-quality samples. Following Data-OOB, DVRL, DataShapley, and BetaShapley show promising performance in this task. Volume-based Shapley, LVAVA, LOO, and InfluenceFunction show similar test accuracy, but InfluenceFunction exhibits a slightly better performance than other methods across all datasets. Volume-based Shapley is generally worse or only better than the random baseline, which has also been observed in the literature . This is because the Volume-based Shapley is independent of the label information, and thus it incompletely reflects the quality of data points.

### Runtime comparison

We also conduct a comparison of the runtime required to compute \(1000\) data values for the noisy label data detection task. In this task, we use the three tabular datasets (2dplanes, electricity, and

Figure 4: **Point removal experiment. Test accuracy curves as a function of the most valuable data points removed. Lower curve indicates better data valuation algorithm. The error bar indicates a 95% confidence interval based on 50 independent experiments. AME and Shapley-based methods exhibit superior performance.**

Figure 5: **Point addition experiment. Test accuracy curves as a function of the least valuable data points added. Lower curve indicates better data valuation algorithm. The error bar indicates a 95% confidence interval based on 50 independent experiments. Data-OOB, DVRL, and Shapley-based methods exhibit superior performance.**

MiniBooNE), two text datasets (BBC and IMDB), and one image dataset (CIFAR10) in order to show that findings are overall consistent across different modalities. We find a similar pattern with other tabular datasets. We utilize a noise dataset containing label noise with a noise ratio of \(p_{}=20\%\), but we highlight that the runtime is independent of the specific type of noise or its intensity. Figure 6 illustrates the runtime of the data valuation algorithms, along with the corresponding F1-scores for the noisy label data detection. Across most datasets, LAVA and KNNShapley demonstrate the shortest computation time because LAVA does not require model training and KNNShapley has a closed-form expression. In contrast, DataShapley and BetaShapley are shown to be computationally costly because they need to estimate the marginal contribution, which requires a large number of model training. Aside from these three algorithms, most of the other data valuation algorithms exhibit similar computational times as their key hyperparameter, the number of models to train, is set to the same value of \(1000\). Among them, Data-OOB demonstrates the best F1-score in noisy label data detection.

## 5 Discussion

In this paper, we propose OpenDataVal, an easy-to-use benchmark environment, and provide a convenient way to compute and compare data valuation algorithms with a few lines of Python codes. To ensure transparency and reproducibility, we have made our framework publicly available.

Our benchmarking analysis demonstrates no single method is uniformly superior across all evaluation metrics. Data-OOB achieves a powerful performance in the noisy label data detection tasks, but it performs less well in the point removal experiment. That is, it is very effective in identifying low-quality data points, but it fails to find the most positively influential data points. In contrast, AME shows the opposite results. By leveraging the LASSO model, it shows competitive performance in identifying the most beneficial data points in the point removal experiment, while it shows weak performance in finding low-quality data points in the noisy data detection task. We also find that Shapley-based methods (DataShapley, BetaShapley, and KNNShapley) have comparable performances. KNNShapley is faster but is limited to using KNN predictors, while DataShapley and BetaShapley can flexibly work with other predictors. Many theoretically rigorous and empirically

Figure 6: **Runtime comparison. The runtime to compute \(1000\) data values is measured for the noisy label data detection task. The \(x\)-axis represents the F1-score used in Section 4.1 and the \(y\)-axis represents the runtime in the log scale. KNNShapley shows the best performance in terms of speed, while Data-OOB achieves a better F1-score in a reasonable time frame. Both DataShapley and BetaShapley are computationally expensive because they are based on marginal contribution estimation algorithms. They perform similarly in BBC, IMDB, and CIFAR10 datasets, and their points are visually overlapped.**

comprehensive analyses are critical, and we believe OpenDataVal opens the first step and facilitates fair and reproducible analyses in various experimental settings.

There are several intriguing future directions. In many real-world applications data can be duplicated and a fraction of them can be hard-encoded with a certain value (_e.g._, -9999 for missing data) . Furthermore, data can contain incomplete information in survival analysis, semi-supervised learning, and weakly-supervised learning settings, raising questions about how to evaluate the influence of such data points. Developing data valuation for sequentially observed data is also an important direction. For instance, in time-series forecasting or multi-armed bandit problems, previously observed data can substantially affect the subsequent data points, which makes it difficult to deploy techniques in typical i.i.d. learning settings. Developing a notion of data values in these scenarios will be an interesting and potentially influential problem.

In a broader context, the practical implementation of data valuation can give rise to several economic and societal questions. In data marketplaces, for instance, data providers might duplicate, transform, or even maliciously modify data to maximize their business profits. Most of the existing data valuation algorithms are not designed to handle these types of attacks, and as a result, erroneously evaluated data values can negatively affect the establishment of reliable and transparent data marketplaces. Developing incentive-compatible data valuation is an interesting new direction . Another potential concern is data security in distributed and federated learning scenarios. Data owners may hesitate to share their data with the main server due to privacy issues, especially if data owners are competing with each other . Evaluating data points based on aggregated summaries (e.g., gradients) will be an interesting topic as existing data valuation algorithms require direct data access.