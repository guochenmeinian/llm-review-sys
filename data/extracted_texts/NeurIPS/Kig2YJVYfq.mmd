# Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings

Klim Kireev, Maksym Andriushchenko, Carmela Troncoso, Nicolas Flammarion

EPFL

###### Abstract

Research on adversarial robustness is primarily focused on image and text data. Yet, many scenarios in which lack of robustness can result in serious risks, such as fraud detection, medical diagnosis, or recommender systems often do not rely on images or text but instead on _tabular data_. Adversarial robustness in tabular data poses two serious challenges. First, tabular datasets often contain _categorical features_, and therefore cannot be tackled directly with existing optimization procedures. Second, in the tabular domain, algorithms that are not based on deep networks are widely used and offer great performance, but algorithms to enhance robustness are tailored to neural networks (e.g. adversarial training). The code for our method is publicly available at https://github.com/spring-epfl/Transferable-Cat-Robustness.

In this paper, we tackle both challenges. We present a method that allows us to train adversarially robust deep networks for tabular data and to transfer this robustness to other classifiers via _universal robust embeddings_ tailored to categorical data. These embeddings, created using a bilevel alternating minimization framework, can be transferred to boosted trees or random forests making them robust _without the need for adversarial training_ while preserving their high accuracy on tabular data. We show that our methods outperform existing techniques within a practical threat model suitable for tabular data. The code for our method is publicly available 1.

## 1 Introduction

Works on adversarial machine learning primarily focus on deep networks and are mostly evaluated on image data. Apruzzese et al. (2022) estimate that approximately 70%-80% of works in the literature fall in this category. Yet, a large number of high-stake tasks across fields like medical diagnosis (Shehab et al., 2022), fraud detection (Altman, 2021), click-through rate prediction (Yang and Zhai, 2022), or credit scoring (Shi et al., 2022) neither only rely on deep networks nor operate on images (Grinsztajn et al., 2022). These tasks often involve many discrete categorical features (e.g., country, email, day of the week), and the predominant models used are discrete tree-based (e.g., boosted tree ensembles, random forests). These two characteristics raise a number of challenges when trying to achieve robustness using previous works which focus on continuous features and continuous models (Goodfellow et al., 2014; Madry et al., 2018).

**Accurate modelling of adversarial capability.** The de-facto standard in adversarial robustness evaluation (Croce et al., 2020) is to model robustness to perturbations bounded in some \(_{p}\) ball, mostly in the image domain. This approach, however, was shown to not accurately represent the capabilities of a real adversary in other domains (Kireev et al., 2022; Apruzzese et al., 2022). Instead, a realistic threat model would constrain the adversary with respect to their _financial_ capabilities. This can be achieved by associating a financial cost with every input feature transformation, limiting the adversary to perform transformations within their total financial budget. Such a constraint is commonfor computer security problems as it ties security, in this case, robustness, to real-world limitations. The inaccuracy of threat models in the literature translates on a lack of well-motivated benchmarks for robustness research on tabular data, unlike for image-oriented vision tasks (Croce et al., 2020; Koh et al., 2020).

**Accounting for discrete categorical features.** Tabular data is usually heterogeneous and often includes _categorical_ features which can be manipulated by an adversary in a non-uniform way, depending on the characteristics of the real-world concept they represent. For example, buying an email address is not the same as buying a credit card or moving to a different city. Moreover, for a given feature, not all transformations have equal cost or are even possible: for example, one can easily change their email address to *@gmail.com, while changing the domain name to *@rr.com can be impossible, since this domain name is unavailable for new users. Hence, the definitions of perturbation sets describing the capabilities of potential adversaries should support complex and heterogeneous constraints.

**Robustness for models other than neural networks.** Gradient-based attacks based on projected gradient descent provide a simple and efficient method for crafting adversarial examples. They are effectively used for _adversarial training_, which became a de-facto standard defence against adversarial perturbations (Madry et al., 2018). Albeit defences and attacks are proposed for decision tree models, they often employ combinatorial methods and can be very inefficient time-wise (Kantchelian et al., 2016; Calzavara et al., 2020; Kireev et al., 2022). However, in tasks involving tabular data, these models must be prioritized as they are widely used because they can provide superior performance on some tabular datasets than neural networks (Grinsztajn et al., 2022).

**Contributions.** Our contributions address the challenges outlined above as follows:

* We propose a practical adversarial training algorithm supporting complex and heterogeneous constraints for categorical data that can accurately reflect financial costs for the adversary. Our training algorithm is based on the continuous relaxation of a discrete optimization problem and employs approaches from projections onto an intersection of convex sets.
* We propose a method to generate _universal robust embeddings_, which can be used for transferring robustness from neural networks to other types of machine learning models such as decision trees or random forests.
* We use existing datasets to build the first benchmark that allows us to evaluate robustness for tabular tasks in which the adversary is constrained by financial capabilities.
* Using the proposed benchmark, we empirically show that our proposed methods provide significantly better robustness than previous works.

## 2 Related works

Here we discuss the most relevant references related to the topics outlined in the introduction.

**Gradient-based adversarial training.** Adversarial training is the key algorithm for making neural networks robust to standard \(_{p}\)-bounded adversarial examples. Szegedy et al. (2013) demonstrate that modern deep networks are susceptible to adversarial examples that can be easily found via gradient descent. Madry et al. (2018) perform successful adversarial training for deep networks where each iteration of training uses projected gradient descent to find approximately optimal adversarial examples. Related to our method of universal first-layer embeddings, Bakiskan et al. (2022) perform _partial_ adversarial training (i.e., training the whole network adversarially/normally and then reinitializing some layer and training them in the opposite way) which is somewhat, showing that earlier layers are more important for robustness. On a related note, Zhang et al. (2019) do adversarial training on the whole network but do multiple forward-backward passes on the first layer with the goal of speeding up adversarial training. Yang et al. (2022) use metric learning with adversarial training to produce word embeddings robust to word-level adversarial attacks which can be used for downstream tasks. Dong et al. (2021) also produce robust word embeddings via a smart relaxation of the underlying discrete optimization problem. We take inspiration from this line of work when we adapt adversarial training for neural networks to categorical features and transfer the first-layer embeddings to tree-based models.

**Robustness of tree-based models.** Tree-based models such as XGBoost (Chen & Guestrin, 2016) are widely used in practice but not amenable to gradient-based adversarial training. Most of the works on adversarial robustness for trees focus on \(_{}\) adversarial examples since they are easier to work with due to the coordinate-aligned structure of decision trees. Kantchelian et al. (2016) is the first algorithm for training robust tree ensembles which are trained on a pool of adversarial examples updated on every iteration of boosting. This approach is refined by approximately solving the associated min-max problem for \(_{}\) robustness in Chen et al. (2019) and by minimizing an upper bound on the robust loss in Andriushchenko & Hein (2019) on each split of every decision tree in the ensemble. Chen et al. (2021) extend the \(_{}\) approach of Chen et al. (2019) to arbitrary _box_ constraints and apply it to a cost-aware threat model on continuous features. Only few works tackle non-\(_{}\) robust training of trees since their coordinate-aligned structure is not conducive to other norms. Most related work to ours is Wang et al. (2020) extend the upper bounding approach of Andriushchenko & Hein (2019) for arbitrary \(_{p}\)-norms. However, they report that \(_{}\) robust training in most cases works similarly to \(_{1}\) robust training, though in a few cases \(_{1}\) adversarial training yields better performance. Moreover, they do not use categorical features which are typically present in tabular data which is the focus of our work.

**Threat modeling for tabular data.** Many prior works do not consider a realistic threat model and adversarial capabilities. First, popular benchmarks like Madry et al. (2018); Croce et al. (2020) assume that the adversary has an _equal_ budget for perturbing each feature which is clearly not realistic for tabular data. To fix this, Chen et al. (2021) propose to consider different perturbation costs for different features. Kireev et al. (2022) argue that, as imperceptibility and semantic similarity are not necessarily meaningful considerations for tabular datasets, these costs must be based on financial constraints - i.e., how much money it costs for the adversary to execute an attack for a _particular_ example. The importance of monetary costs was corroborated by studies involving practitioners dealing with adversarial machine learning in real applications (Apruzzese et al., 2022; Grosse et al., 2023). Second, popular benchmarks treat equally changes from the correct to any other class, but in practice the adversary is typically interested only in a _specific_ class change, e.g., modifying a fraudulent transaction to be classified as non-fraudulent. To address this shortcoming, past works (Zhang & Evans, 2019; Shen et al., 2022) propose class-sensitive robustness formulations where an unequal cost can be assigned to every pair of classes that can be changed by the adversary. As a result, this type of adversarial training can achieve a better robustness-accuracy tradeoff against a cost-sensitive adversary. We take into account all these works and consider a cost-based threat model where we realistically model the costs and classes which are allowed to be changed for the particular datasets we use.

## 3 Definitions

**Input domain and Model.** The input domain's _feature space_\(\) is composed of \(m\) features: \(_{1}_{2} _{m}\). We denote as \(x_{i}\) the value of the \(i\)-th feature of \(x\). Features \(x_{i}\) can be categorical, ordinal, or numeric, and we define tabular data as data consisting of these three types of features in any proportion. Categorical features are features \(x_{i}\) such that \(_{i}\) is a finite set of size \(|_{i}|=t_{i}\), i.e., \(t_{i}\) is the number of possible values that can take \(x_{i}\). We denote by \(t=_{i=1}^{m}t_{i}\). We also denote as \(x_{i}^{j}\), the \(j\)-th value of the feature \(x_{i}\). We further assume that each example \(x\) is associated with a binary label \(y\{0,1\}\). Finally, we consider a general learning model with parameters \(\) and output \((,x)\). The model we consider in our framework can take on two forms: differentiable, such as a neural network, or non-differentiable, like a tree-based model.

**Perturbation set.** Following the principles described by Apruzzese et al. (2022); Kireev et al. (2022), we make the assumption that a realistic adversary is constrained by financial limitations. Specifically, we denote the cost of modifying the feature \(x_{i}\) to \(x_{i}^{i}\) as \(c_{i}(x_{i},x_{i}^{})\). For categorical features with a finite set \(_{i}\), the cost function \(c_{i}(x_{i},x_{i}^{})\) can be conveniently represented using a _cost matrix_\(C_{i}_{ 0}^{t_{i} t_{i}}\). The cost of changing the feature \(x_{i}\) from the value \(j\) to the value \(k\) is given by

\[c_{i}(x_{i}^{j},x_{i}^{k})=C_{i}^{jk}.\]

If such transformation is impossible, \(C_{i}^{jk}\) is set to \(\). In a realistic threat model, any cost matrix \(C_{i}\) is possible, and costs need not be symmetric. The only reasonable assumption is that \(C_{i}^{jj}=0\), indicating no cost for staying at the same value. See realistic cost matrix examples in the Appendix A.

We assume that the cost of modifying features is additive: the total cost of crafting an adversarial example \(x^{}\) from an example \(x\) can be expressed as:

\[c(x,x^{})=_{i=1}^{m}c_{i}(x_{i},x^{}_{i}).\] (1)

**Encoding and Embeddings.** Categorical features are typically preprocessed by encoding each value \(x_{i}\) using a one-hot encoding vector \(_{i}\). For instance, if a categorical feature \(x_{i}\) can take four possible values \(\{1,2,3,4\}\) and \(x_{i}=2\), it is represented as \(_{i}=(0,1,0,0)^{}\). We can then represent the feature-wise cost function as

\[c_{i}(x_{i},x^{}_{i})=\|w_{i}(_{i}-^{ }_{i})\|_{1}=l_{1,w_{i}}(_{i},^{}_{i}),\] (2)

where \(l_{1,w}\) denotes the weighted \(l_{1}\) norm and \(w_{i}=C_{i}_{i}\) is the costs of transforming \(x_{i}\) to any other possible value in \(_{i}\). Then according to Equation (1), the per sample cost function is:

\[c(x,x^{})=\|w_{i}(-^{})\|_{1}=l_{1,w }(,^{}),\] (3)

where the vectors \(w,,^{}\) are the contenation of the vectors \(w_{i}\), \(_{i}\), \(^{}_{i}\) respectively for \( i:0 i m\).

The one-hot-encoded representation may not be optimal for subsequent processing due to high dimensionality or data sparsity. We instead use _embeddings_ and replace the categorical features with their corresponding embedding vector \((x) R^{n}\). The model can then be represented by a function \(f\) of the features as

\[(,x)=f(,(x)).\]

The embedding function \(\) is composed of embedding functions \(_{i}\) for the subvector \(x_{i}\) as \((x)=(_{i}(x_{i}))_{i=1}^{m}\). The embedding function \(_{i}\) of the \(i\)-th feature can always be represented by a matrix \(Q_{i}\) since \(x_{i}\) takes discrete values. The columns of this matrix are _embedding vectors_ which satisfy

\[_{i}(x_{i})=Q_{i}_{i}.\]

Therefore, we parametrize the embedding function \((x,Q)\) with a family of \(m\) matrices \(Q=(Q_{i})_{i=1}^{m}\).

**Numerical features.** For numerical features, we use a binning procedure which is a common practice in tabular machine learning. This technique facilitates the use of decision-tree-based classifiers as they naturally handle binned data (Chen & Guestrin, 2016; Ke et al., 2017). Binning transforms numerical features into categorical ones, allowing a unique treatment of all the features. It is worth noting that for differentiable classifiers, the approach by Kireev et al. (2022) can enable us to directly use numerical features without binning.

## 4 Adversarial training

In this section, we outline our adversarial training procedure. When adapted to our specific setup, the adversarial training problem (Madry et al., 2018) can be formulated as:

\[_{,Q}}_{x,y D}_{c(x,x^{}) }((f((x^{},Q)),),y).\] (4)

Such model would be robust to adversaries that can modify the value of any feature \(x_{i}\), as long as the total cost of the modification is less than \(\). While this formulation perfectly formalizes our final goal, direct optimization of this objective is infeasible in practice:

1. Optimizing directly with categorical features can be computationally demanding due to the need for discrete optimization algorithms. In our evaluation, we employ a graph-search-based procedure developed by Kireev et al. (2022). However, finding a single example using this algorithm can take between 1 to 10 seconds, making it impractical for multi-epoch training even on medium-sized datasets (approximately 100K samples).
2. There is currently no existing algorithm in the literature that enables us to operate within this cost-based objective for decision-tree based classifiers.

### Adversarial training for differentiable models

We begin by examining the scenario where our model \(f\) is a deep neural network. In this case, we are constrained solely by the first restriction. To address this constraint, we employ a _relaxation technique_: instead of working with the discrete set of feature vectors, we consider the convex hull of their one-hot encoding vectors. For each feature, this relaxation allows us to replace the optimization over \(x^{}_{i}_{i}\) with an optimization over \(^{}_{i}^{t_{i}}_{ 0}\). More precisely, we first replace the feature space \(_{i}\) by the set \(\{^{}_{i}\{0,1\}^{t_{i}},_{j}^{ j}_ {i}=1\}\) using the one-hot encoding vectors. We then relax this discrete set into \(\{^{}_{i}^{t_{i}}_{ 0},_{j}^{  i}_{i}=1\}\). The original constraint \(\{x^{},c(x,x^{})\}\) is therefore relaxed as

\[\{^{}_{i}^{t_{i}}_{ 0},_{j}^{  j}_{i}=1,l_{1,w}(,^{})^{}=(_{i})_{i=1}^{m}\}.\]

The set obtained corresponds to the convex hull of the one-hot encoded vectors, providing the tightest convex relaxation of the initial discrete set of vectors \(\). In contrast, the relaxation proposed by Kireev et al. (2022) relaxes \(_{i}\) to \(^{t_{i}}_{ 0}\), leading to significantly worse performance as shown in Section 5.

The adversarial training objective for the neural networks then becomes:

\[_{,Q}\,}\,_{l_ {1,w}(,^{})\\ \|^{}_{i}\|_{1}=1,^{}_{i}^{t_{i}}_{  0}1 i m}\,(f(Q^{}, ),y).\] (5)

In order to perform the inner maximization, we generate adversarial examples using projected gradient descent. The projection is computed using the Dykstra projection algorithm (Boyle and Dykstra, 1986) which enables us to project onto the intersection of multiple constraints. In each iteration of the projection algorithm, we first project onto the convex hull of each categorical feature (\(_{simplices}\)), followed by a projection onto a weighted \(l_{1}\) ball (\(_{cost}\)). This dual projection approach allows us to generate perturbations that simultaneously respect the cost constraints of the adversary and stay inside the convex hull of original categorical features. We refer to this method as Cat-PGD, and its formal description can be found in Algorithm 1.

``` Input: Data point \(,y\), Attack rate \(\), Cost bound \(\), Cost matrices \(C\), \(PGD\_steps\), \(D\_steps\) Output: Adversarial sample \(^{}\). \(:=0\) for\(i=1\)to\(PGD\_steps\)do \(:=_{}(f(Q(+)),y))\) \(:=+\) \(p:=0\) \(q:=0\) for\(i=1\)to\(D\_steps\)do \(z:=_{simplices}(,+p)\) \(p:=+p-z\) \(:=_{cost}(z+q,C)\) \(q:=z+q-\) endfor endfor \(^{}:=+\) ```

**Algorithm 1** Cat-PGD. Relaxed projected gradient descent for categorical data.

### Bilevel alternating minimization for universal robust embeddings

While the technique described above is not directly applicable to non-differentiable models like decision trees, we can still leverage the strengths of both neural networks and tree-based models. By transferring the learnt robust embeddings from the _first_ layer of the neural network to the decision tree classifier, we can potentially combine the robustness of the neural network with the accuracy of boosted decision trees.

**Difference between input and output embeddings.** Using embeddings obtained from the last layer of a neural network is a common approach in machine learning (Zhuang et al., 2019). However, in our study, we prefer to train and utilize embeddings from the _first layer_ where the full information about the original features is still preserved. This choice is motivated by the superior performance of decision-tree-based classifiers over neural networks on tabular data in certain tasks. By using first layer embeddings, we avoid a potential loss of information (unless the embedding matrix \(Q\) has exactly identical rows which are unlikely in practice) that would occur if we were to use embeddings from the final layer.

To show this effect we ran a simple experiment where we compare first and last layer embeddings as an input for a random forest classifier. We report the results in Appendix C.1.

**Step 1: bilevel alternating minimization framework.** A natural approach is to use standard adversarial training to produce robust first layer embeddings. However, we noticed that this method is not effective in producing optimal first layer embeddings. Instead, we specifically produce robust first layer embeddings and ignore the robustness property for the rest of the layers. This idea leads to a new objective that we propose:

\[_{Q}}_{x,y D}_{ l_{1,\,v}(,^{})\|^{}_{i}\| =1,^{}_{i}^{ 1}_{ 0}1 i m} fQ,\,*{arg\,min}_{}}_{x,y D}f(Q^{},),y,y.\] (6)

This optimization problem can be seen as the relaxation of the original adversarial training objective. This relaxation upper bounds the original objective because we robustly optimize only over \(Q\) and not jointly over \(Q\) and \(\). If we disregard the additional inner maximization, it is a classical _bilevel optimization problem_. Such problem can be solved using alternating gradient descent (Ghadimi and Wang, 2018). To optimize this objective for neural networks on large tabular datasets, we propose to use _stochastic_ gradient descent. We alternate between \(Q\_steps\) for the inner minimum and \(\_steps\) for the outer minimum. At each step, we run projected gradient descent for \(PGD\_steps\) using the relaxation described in Eq. 5. We detail this approach in Algorithm 2.

**Step 2: embedding merging algorithm.** We observed that directly transferring the embeddings \(Q\) has little effect in terms of improving the robustness of decision trees (Appendix C.1). This is expected since even if two embeddings are very close to each other, a decision tree can still generate a split between them. To address this issue and provide decision trees with information about the relationships between embeddings, we propose a merging algorithm outlined in Algorithm 3. The main idea of the algorithm is to merge embeddings in \(Q\) that are very close to each other and therefore pass the information about distances between them to the decision tree.

**Step 3: standard training of trees using universal robust embeddings.** As the final step, we use standard tree training techniques with the _merged embeddings_\(Q^{}\). This approach allows us to avoid the need for custom algorithms to solve the relaxed problem described in Eq. 5. Instead, we canleverage highly optimized libraries such as XGBoost or LightGBM (Chen & Guestrin, 2016; Ke et al., 2017). The motivation behind this multi-step approach is to combine the benefits of gradient-based optimization for generating universal robust embeddings with the accuracy of tree-based models specifically designed for tabular data with categorical features.

## 5 Evaluation

In this section we evaluate the performance of our methods.

**Models.** We use three 'classic' classifiers widely used for tabular data tasks: RandomForest (Liaw & Wiener, 2002), LGBM (Ke et al., 2017), and Gradient Boosted Stumps. Both for adversarial training and for robust embeddings used in this section, we use TabNetArik & Pfister (2019) which is a popular choice for tabular data. Additionally, we also show the same trend with FT-Transformer in Appendix C. The model hyperparameters are listed in Appendix B.

**Attack.** For evaluation, we use the graph-search based attack described by Kireev et al. (2022). This attack is model-agnostic and can generate adversarial examples for both discrete and continuous data, and is thus ideal for our tabular data task.

**Comparison to previous work.** In our evaluation, we compare our method with two previous proposals: the method of Wang et al. (2020), where the authors propose a verified decision trees robustness algorithm for \(l_{1}\) bounded perturbations; and the method of Kireev et al. (2022), which considers financial costs of the adversary, but using a different cost model than us and weaker relaxation.

### Tabular data robustness evaluation benchmark

There is no consistent way to evaluate adversarial robustness for tabular data in the literature. Different works use different datasets, usually without providing a justification neither for the incentive of the adversary nor for the perturbation set. For example, Wang et al. (2020) and Chen et al. (2021) use the breast cancer dataset, where participants would not have incentives to game the classifier, as it would reflect poorly on their health. Even if they had an incentive, it is hard to find plausible transformation methods as all features relate to breast images taken by doctors. To solve this issue, we build our own benchmark. We select datasets according to the following criteria:

* Data include both numerical and categorical features, with a financial interpretation so that it is possible to model the adversarial manipulations and their cost.
* Tasks on the datasets have a financial meaning, and thus evaluating robustness requires a cost-aware adversary.
* Size is large enough to accurately represent the underlying distribution of the data and avoid overfitting. Besides that, it should enable the training of complex models such as neural networks.

It is worth mentioning that the financial requirement and the degree of sensitivity of data are highly correlated. This kind of data is usually not public and even hardly accessible due to privacy reasons. Moreover, the datasets are often imbalanced. Some of these issues can be solved during preprocessing stage (e.g., by balancing the dataset), but privacy requirements imply that benchmarks must mostly rely on anonymized or synthetic versions of a dataset.

**Datasets.** We select three publicly-available datasets that fit the criteria above. All three datasets are related to real-world financial problems where robustness can be crucially important. For each dataset, we select adversarial capabilities for which we can outline a plausible modification methodology, and we can assign a plausible cost for this transformation.

* IEEECIS. The IEEECIS fraud detection dataset (Kaggle, 2019) contains information about around 600K financial transactions. The task is to predict whether a transaction is fraudulent or benign.
* BAF. The Bank Account Fraud dataset was proposed in NeurIPS 2022 by Jesus et al. (2022) to evaluate different properties of ML algorithms for tabular data. The task is to predict if a given credit application is a fraud. It contains 1M entries for credit approval applications with different categorical and numerical features related to fraud detection.
* Credit. The credit card transaction dataset (Altman, 2021) contains around 20M simulated card transactions, mainly describing purchases of goods. The authors simulate a "virtual world" with a "virtual population" and claim that the resulting distribution is close to a real banking private dataset, which they cannot distribute.

All datasets were balanced for our experiments. The features for the adversarial perturbations along with the corresponding cost model are described in Appendix A.

### Results

**Cost-aware Adversarial Training.** We follow a standard evaluation procedure: we first perform adversarial training of the TabNet models using different cost bounds \(\), representing different financial constraints for the adversary. Then, we evaluate these models' robustness using the attack by Kireev et al. (2022) configured for the same cost adversarial constraint as used during the training. The details about our training hyperparameters are listed in the Appendix B.

We show the resulting clean and robust performance in Figure 1. Our method outperforms the baseline in both metrics. The better performance can be attributed to a better representation of the threat model. For example, let us consider the email address feature in IEEECIS. The cost to change this feature varies from 0.12$ (e.g., hotmail.com) to \(\) (for emails that are no longer available). It

Figure 1: **Clean and robust model accuracy for cost-aware adversarial training. Each point represents a TabNet model trained with an assumption of an adversary’s budget of \(\) S attacked by the adversary with that budget. Our method outperforms Kireev et al. (2022) in all setups.**is unlikely that an adversary uses an expensive email (sometimes even impossible as in the case of unavailable domains), and therefore such domains are useful for classification as they indicate non-fraudulent transactions. On the other hand, an adversary can easily buy a _gmail.com_ address even when their budget \(\) is a few dollars. Our method captures this difference. We see how there is a steep decrease in accuracy when the cost of the modification is low (under 1$) and thus the training needs to account for likely adversarial actions. Then, it enters a plateau when the cost of emails increases and no changes are possible given the adversary's budget. When the budget \(\) grows enough to buy more expensive emails, the accuracy decreases again. We also observe that the gain in clean accuracy is higher than for robust accuracy. This effect is due to a better representation of the underlying categorical data distribution.

**Universal Robust Embeddings.** In order to evaluate our Bilevel alternating minimization framework, we run Algorithm 2 on TabNet to produce robust embeddings, and we merge these embeddings using Algorithm 3. After that, we attack the model using budgets \(\) set to 10$ for IEEECIS, 1$ for BAF, and 100$ for Credit. These budgets enable the adversary to perform most of the possible transformations. We compare our methods on gradient-boosted stumps because training gradient boosted decision-trees with the method of Wang et al. (2020) is computationally infeasible for an adequate number of estimators and high dimensional data.

We show the results in Table 1. In almost all setups, our method yields a significant increase in robustness. For example, for IEEECIS we increase the robust accuracy of LGBM by 24%, obtaining a model that combines both high clean and robust accuracy, and that outperforms TabNet. Our method outperforms Wang et al. (2020) both for clean and robust accuracy. These results confirm that training based on only \(l_{1}\) distance is not sufficient for tabular data.

We also compare the performance of the different methods with respect to the time spent on training and merging (see Table 2). We see that our method is considerably faster than previous work. The improvement is tremendous for IEEECIS and Credit where the dimensionality of the input is more than 200 and the number of estimators are 80 and 100 respectively. In the Appendix C, we also discuss how robust embeddings can be applied to a neural network of the same type.

## 6 Conclusion

In this work, we propose methods to improve the robustness of models trained on categorical data both for deep networks and tree-based models. We construct a benchmark with datasets that enable

 _Dataset_ & RF & RF-R & RF-C & LGBM & LGBM-R & GBS & GBS-R & GBS-W & CB & CB-R \\  IEEECIS & & & & & & & & & & \\ _Clean_ & 83.6 & 81.0 & 69.0 & 81.2 & 79.3 & 66.9 & 66.3 & 52.4 & **76.5** & 76.1 \\ _Robust_ & 48.0 & **81.0** & 69.0 & 53.9 & 78.5 & 44.7 & **66.3** & 11.1 & 51.1 & **72.0** \\  BAF & & & & & & & & & & \\ _Clean_ & 72.3 & 65.8 & 61.3 & 74.1 & 68.1 & 74.1 & 68.1 & 64.8 & 74.4 & 67.2 \\ _Robust_ & 42.8 & **65.8** & 61.3 & 49.2 & **67.5** & 46.8 & **67.7** & 33.5 & 48.2 & **67.2** \\  Credit & & & & & & & & & & \\ _Clean_ & 78.0 & 73.4 & - & 83.1 & 79.6 & 82.1 & 80.6 & 61.7 & 83.3 & 79.7 \\ _Robust_ & 55.2 & **66.7** & - & 69.9 & **72.5** & 70.9 & **71.4** & 61.3 & 71.7 & 71.9 \\ 

Table 1: **Universal robust embedding evaluation.** We report clean and robust accuracy (in percentage) for Light Gradient Boosting (LGBM), Random Forest (RF), Gradient Boosted Stumps (GBS), and CatBoost (CB). We indicate the robustness technique applied as a suffix: -R for robust embeddings, -W for the training proposed by Wang et al. (2020), and -C for method in Chen et al. (2019). Models fed with robust embeddings have higher robustness and outperform both clean and \(l_{1}\)-trained models (Wang et al., 2020).

 _Dataset_ & Training RE & GBS-RE & GBS-W \\  IEEECIS & 9.5 & 0.06 & 233.3 \\ BAF & 3.17 & 0.02 & 4.97 \\ Credit & 3.24 & 0.07 & 174.7 \\ 

Table 2: **Computation time.** Time is measured in minutes. The total time of training RE and training GBS-RE is less than Wang et al. (2020)’s training robustness evaluation considering the financial constraints of a real-world adversary. Using this benchmark, we empirically show that our methods outperform previous work while providing a significant gain in efficiency.

Limitations and Future Work.In our work, we focused on development of a method to improve the robustness of a machine learning model, having as small degradation in performance as possible. However, there are applications where even a small accuracy drop would incur more financial losses than potential adversarial behaviour. These setups can be still not appropriate for our framework. Quantifying these trade-off can be a promising direction for future work, and one way of doing it would be to follow a utility-based approach introduced in Kireev et al. (2022). Besides that, we do not cover the extreme cases where the dataset is either too small and causes overfitting or too large and makes adversarial training more expensive. Addressing these extreme cases can be also considered as a research direction. Finally, we leave out of the scope potential problems in the estimation of adversarial capability, e.g., if the cost model which we assume is wrong and how it can affect both the robustness and utility of our system.