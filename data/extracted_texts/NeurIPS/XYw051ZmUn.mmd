# How does Gradient Descent Learn Features - A Local Analysis for Regularized Two-Layer Neural Networks

How does Gradient Descent Learn Features - A Local Analysis for Regularized Two-Layer Neural Networks

 Mo Zhou

University of Washington

mozhou17@cs.washington.edu

Work done at Duke University.

Rong Ge

Duke University

rongge@cs.duke.edu

###### Abstract

The ability of learning useful features is one of the major advantages of neural networks. Although recent works show that neural network can operate in a neural tangent kernel (NTK) regime that does not allow feature learning, many works also demonstrate the potential for neural networks to go beyond NTK regime and perform feature learning. Recently, a line of work highlighted the feature learning capabilities of the early stages of gradient-based training. In this paper we consider another mechanism for feature learning via gradient descent through a local convergence analysis. We show that once the loss is below a certain threshold, gradient descent with a carefully regularized objective will capture ground-truth directions. We further strengthen this local convergence analysis by incorporating early-stage feature learning analysis. Our results demonstrate that feature learning not only happens at the initial gradient steps, but can also occur towards the end of training.

## 1 Introduction

Feature learning has long been considered to be a major advantage of neural networks. However, how gradient-based training algorithms can learn useful features is not well-understood. In particular, the most widely applied analysis for overparametrized neural networks is the neural tangent kernel (NTK) (Jacot et al., 2018; Du et al., 2019; Allen-Zhu et al., 2019). In this setting, the neurons don't move far from their initialization and the features are determined by the network architecture and random initialization (Chizat et al., 2019).

While there are empirical and theoretical evidence on the limitation of NTK regime (Chizat et al., 2019; Arora et al., 2019), extending the analysis beyond the NTK regime has been challenging. For 2-layer networks, an alternative framework for analyzing overparametrized neural networks called mean-field analysis was introduced. Earlier mean-field analysis (e.g., Chizat and Bach, 2018; Mei et al., 2018) require either infinite or exponentially many neurons. Later works (e.g., Li et al., 2020; Ge et al., 2021; Bietti et al., 2022; Mahankali et al., 2024) can analyze the training dynamics of _mildly overparametrized networks_ with polynomially many neurons with stronger assumptions on the ground-truth function.

Recently, a growing line of works (Daniely and Malach, 2020; Damian et al., 2022; Abbe et al., 2021, 2022, 2023; Yehudai and Shamir, 2019; Shi et al., 2022; Ba et al., 2022; Mousavi-Hosseini et al., 2023; Barak et al., 2022; Dandi et al., 2023; Wang et al., 2024; Nichani et al., 2024, 2024) showed that early stages of gradient training (either one/a few steps of gradient descent or a small amount of time of gradient flow) can be useful in feature learning. These works show that after the early stages of gradient training, the first layer in a 2-layer neural network already captures useful features (usually in the form of a low dimensional subspace), and continuing training the second layer weights will give performance guarantees that are stronger than any kernel or random feature based models. In this work, we consider the natural follow-up question:

_Does feature learning only happen in the early stages of gradient training?_

We show that this is not the case by demonstrating feature learning capability for the final stage of gradient training - local convergence. In particular, we prove the following result:

**Theorem 1** (Informal).: _If the data is generated by a 2-layer teacher network \(f_{*}\), as long as the width of student network \(m\) is at least some quantity \(m_{0}\) that only depends on \(f_{*}\), a variant of gradient descent algorithm (Algorithm 1, roughly gradient descent with decreasing weight decay) can recover the target network within polynomial time. Moreover, the student neurons align with the teacher neurons at the end._

Our result highlights the different mechanisms of feature learning: previous works show that in the early stages of gradient descent, the network learns the _subspace_ spanned by the neurons in the teacher network. Our local convergence result shows that at later stages, gradient descent is able to learn the _exact directions_ of the teacher neurons, which are much more informative compared to the subspace and lead to stronger guarantees.

Analyzing the entire training dynamics is still challenging so in our algorithm (see Algorithm 1) we use a convex second stage to "fast-forward" to the local analysis. Our technique for local convergence is similar to the earlier work (Zhou et al., 2021), however we consider a more complicated setting with ReLU activation and allow second-layer weights to be both positive or negative. This change requires additional regularization in the form of standard weight decay and new dual certificate analysis.

### Related works

Neural Tangent KernelEarly works often studied neural network optimization using NTK theory (Jacot et al., 2018; Allen-Zhu et al., 2019; Du et al., 2019). It is shown that highly-overparametrized neural nets are essentially kernel methods under certain initialization scale. However, NTK theory cannot explain the performance of neural nets in practice (Arora et al., 2019) and leads to lazy training dynamics that neurons stay close to their initialization (Chizat et al., 2019). Hence, later research efforts (e.g., Allen-Zhu et al., 2019; Bai and Lee, 2020; Li et al., 2020), as well as current paper, focus on feature learning regime where neural nets can learn features and outperform kernel methods.

Early stage feature learningResearchers have recently tried to understand how neural networks trained with gradient descent (GD) can learn features, going beyond the kernel/lazy regime (Jacot et al., 2018; Chizat et al., 2019). A typical setup is to use 2-layer neural networks to learn certain target function, often equipped with low-dimensional structure. Examples include learning polynomials (Yehudai and Shamir, 2019; Damian et al., 2022), single-index models (Ba et al., 2022; Mousavi-Hosseini et al., 2023; Moniri et al., 2024; Cui et al., 2024), multi-index models (Dandi et al., 2023), sparse boolean functions (Abbe et al., 2021, 2022, 2023), sparse parity functions (Daniely and Malach, 2020; Shi et al., 2022; Barak et al., 2022) and causal graph (Nichani et al., 2024b). Also, few works use 3-layer networks as learner model (Nichani et al., 2024a; Wang et al., 2024). These works essentially showed that feature learning happens in the early stage of training. Specifically, they often use 2-stage layer-wise training procedure: first-layer weights/features are only trained with one or few steps of gradient descent/flow and only update the second-layer afterwards. Our results give a complementary view that feature learning can also happen in the _final stage_ training that leading student neurons eventually align with ground-truth directions. This cannot be achieved if first-layer weights are fixed after few steps.

Learning single/multi-index models with neural networksSingle/Multi-index models are the functions that only depend on one or few directions of the high dimensional input. Many recent works have studied the problem of using 2-layer networks to learn single-index models (Soltanolkotabi, 2017; Yehudai and Ohad, 2020; Frei et al., 2020; Wu, 2022; Bietti et al., 2022; Xu and Du, 2023; Berthier et al., 2023; Mahankali et al., 2024) and multi-index models (Damian et al., 2022; Bietti et al., 2023; Suzuki et al., 2024; Glasgow, 2024). These works show the advantages of feature learning over fixed random features in various settings. In this paper, we consider target multi-index function that can be represented by a small 2-layer network, and show a variant of GD with weight decay can learn it and, moreover, recover the ground-truth directions.

Local loss landscapeSafran et al. (2021) showed that in the overparametrized case with orthogonal teacher neurons, even around the local region of global minima, the landscape neither is convex nor satisfies PL condition. Chizat (2022) considered square loss with \(_{2}\) regularization similar to our setup and showed the local loss landscape is strongly-convex under certain non-degenerate assumptions. However, it is not known when such assumptions actually hold and the proof cannot handle ReLU. Later Akiyama and Suzuki (2021) gives a result for ReLU, but the non-degeneracy assumption is still required (and also focus on effective \(_{1}\) regularization instead of \(_{2}\) regularization). Zhou et al. (2021) studies a similar local convergence setting but restricts second-layer weights to be positive and uses absolute activation. In this paper, we focus on a more natural but technically challenging case that second-layer can be positive and negative and using ReLU activation. We develop new techniques to overcome the above challenges (additional assumption, ReLU, standard second-layer, etc).

## 2 Preliminary

NotationLet \([n]\) be set \(\{1,,n\}\). For vector \(\), we use \(_{2}\) for its 2-norm and \(}=/_{2}\) as its normalized version. For two vectors \(,\) we use \((,)=(^{}/( _{2}_{2}))[0, /2]\) as the angle between them (up to a sign).For matrix \(\) let \(_{F}\) be its Frobenius norm. We use standard \(O,,\) to hide constants and \(,,\) to hide polylog factors. We use \(O_{*},_{*},_{*}\) to hide problem dependent parameters that only depend on the target network (see paragraph above (1)).

Teacher-student setupWe will consider the teacher-student setup for two-layer neural networks with Gaussian input \( N(,_{d})\). The goal is to learn the teacher network of size \(m_{*}\)

\[f_{*}()=_{i=1}^{m^{*}}a_{i}^{*}(^{*}}^{})+ ^{*}}^{}+b_{0}^{*},\]

where \((x):=\{0,x\}\) is ReLU activation, \(S_{*}:=\{_{1}^{*},,_{m^{*}}^{*}\}\) is the target subspace. Without loss of generality, we will assume \(_{i}^{*}_{2}=1\) due to the homogeneity of ReLU.

Following the recent line of works in learning single/multi-index models (Ba et al., 2022; Damian et al., 2022), we assume the target network has low dimensional structure.

**Assumption 1**.: _Teacher neurons form a low dimensional subspace in \(^{d}\), that is_

\[(S_{*})=(\{_{1}^{*},,_{m^{*}}^{*}\})=r  d.\]

We will also assume the teacher neurons are non-degenerate in the following sense:

**Assumption 2**.: _Teacher neurons are \(\)-separated, that is angle \((_{i}^{*},_{j}^{*})\) for all \(i j\)._

**Assumption 3**.: \(:=_{i=1}^{m^{*}}a_{i}^{*}_{i}^{*}_{i}^{*}\) _is non-degenerate in target subspace \(S_{*}\), i.e., \((H)=r\). Denote \(:=|_{r}()|\)._

Assumption 2 simply requires all teacher neurons pointing to different directions, which is crucial for identifiability (Zhou et al., 2021).

Assumption 3 says the target network contains low-order (second-order) information, which is related with the notion of information exponent (Arous et al., 2021). In our setting, the information exponent is at most 2 due to Assumption 3. Indeed, one can show \(_{}[f_{*}()h_{2}(^{})]=_{2} ^{}\), where \(h_{2}(x)\) is the 2nd-order normalized Hermite polynomial and \(_{2}\) is the 2nd Hermite coefficient of ReLU. See Appendix A for more details. Many previous works also rely on same or similar assumption to show neural networks can learn features to perform better than kernels (Damian et al., 2022; Abbe et al., 2022; Ba et al., 2022).

In this paper, we are interested in the case where the complexity of target network is small. Therefore, we will use \(O_{*},_{*},_{*}\) to hide \((r,m_{*},,|a_{1}|,,|a_{m_{*}}|,)\), which is the polynomial dependency on relevant parameters of target \(f_{*}\) (does not depend on student network).

We will use the following overparametrized student network:

\[f(;) =_{i=1}^{m}a_{i}(_{i}^{})++^{},\] (1)

where \(=(a_{1},,a_{m})^{}^{m}\), \(=(_{1}_{m})^{}^{m d}\) and \(=(,,,)\).

Loss and algorithmConsider the square loss function with \(_{2}\) regularization under Gaussian input

\[L_{}()=_{ N(0,_{d})}[(f(;)-)^{2}]+\|\|_{2}^{2}+ \|\|_{2}^{2}.\] (2)

We will use \(L\) to denote the square loss for simplicity. The \(_{2}\) regularization is the same as the commonly used weight decay in practice. Our goal is to find the minima of unregularized problem (\(=0\)) to recover teacher network \(f_{*}\). However, directly analyzing the unregularized problem is challenging so instead we choose to analyze the regularized problem and will gradually let \( 0\).

In above, we use preprocessed data \((x,)\) in the loss function as in Damian et al. (2022). Specifically, given any \((,y)\) with \(y=f_{*}()\), denote \(_{*}=_{}[y]\) and \(_{*}=_{}[y]\), we get

\[_{*}()==y-_{*}-_{*}^{} {x}.\] (3)

This preprocessing process essentially removes the 0-th and 1-st order term in the Hermite expansion of \(\). See Appendix A for a brief introduction of Hermite polynomials and Claim B.1.

Our algorithm is shown in Algorithm 1. It is roughly the standard GD following a given schedule of weight decay \(_{t}\) that goes to 0. Due to the difficulty in analyzing gradient descent training beyond early and final stage, we choose to only train the norms in Stage 2 as a tractable way to reach the local convergence regime.

We will use symmetric initialization that \(a_{i}=-a_{i+m/2}\), \(_{i}=_{i+m/2}\) with \(a_{i}\{\}\), \(_{i}((1/)^{d-1})\), \(=0\), \(=\). Our analysis is not sensitive to the initialization scale we choose here. The choice is just for the simplicity of the proof.

```
0: initialization \(^{(0)}\), weight decay \(_{t}\) and stepsize \(_{t}\) Data preprocess: get \((,)\) according to (3) Stage 1: one step gradient update \(^{(1)}^{(0)}-_{0}_{}L_{ _{0}}(^{(0)})\) Stage 2: norm adjustment by convex program \(^{(T_{2})},^{(T_{2})},^{(T_{2})}_{, ,}L(,^{(1)},,)+_{i} \|_{i}\|_{2}|a_{i}|\) Balancing norm between two layers s.t. \(|a_{i}|=\|_{i}\|_{2}\) for all \(i\) Stage 3: local convergence for\(k K\)do// for each epoch, run GD until convergence for\(T_{3,k-1} t T_{3,k}\)do\(^{(t+1)}^{(t)}-_{}L_{ _{3,k}}(^{(t)})\) Output:\(^{(T_{3,K})}=(^{(T_{3,K})},^{(T_{3,K})},^{(T_{3,K})},^{(T_{3,K})})\) ```

**Algorithm 1**Learning 2-layer neural networks

## 3 Main results

In this section, we give our main result that shows training student network using Algorithm 1 can recover the target network within polynomial time. We will focus on the case that \(d_{*}(1)\) when the complexity of target function is small.

**Theorem 2** (Main result).: _Under Assumption 1, 2, 3, consider Algorithm 1 on loss (2). There exists a schedule of weight decay \(_{t}\) and step size \(_{t}\) such that given \(m m_{0}=_{*}(1)(1/_{0})^{O(r)}\) neurons with small enough \(_{0}=_{*}(1)\), with high probability we will recover the target network \(L()\) within time \(T=O_{*}(1/^{2})\) where \(=(,1/d,1/m)\)._

_Moreover, when \( 0\) every student neuron \(_{i}\) either aligns with one of teacher neuron \(_{j}^{*}\) as \((_{i},_{j}^{*})=0\) or vanishes as \(|a_{i}|=\|_{i}\|=0\)._

Note that our results can be extended to only have access to polynomial number of samples by using standard concentration tools. We omit the sample complexity for simplicity. See more discussion in Appendix J. We emphasize that the required width \(m_{0}\) only depends on the complexity of target function \(f_{*}\) (only quantities that are related to \(f_{*}\), not student network \(f\) or error \(\)), so any mildly overparametrized networks can learn \(f_{*}\) efficiently to arbitrary small error.

The analysis consists of three stages: early-stage feature learning (Stage 1 and 2) and final-stage feature learning/local convergence (Stage 3). It will be clear in the later section that \(_{0}\) is in fact the threshold to enter the local convergence regime. See Section 4 for more details.

Our result improves the previous works that only train the first layer weight with small number of gradient steps at the beginning (Damian et al., 2022; Ba et al., 2022; Abbe et al., 2021, 2022, 2023). In these works, neural networks only learn the target subspace and do random features within it (see Section 4.1 for more details). Intuitively, these random features need to span the whole space of the target function class to perform well, which means its number (the width) should be on the order of the dimension of target function class. For 2-layer networks, random features in the target subspace need \((1/)^{O(r)}\) neurons to achieve desired accuracy \(\). In contrast, continue training both layer at the last phase of training allows us to learn not only subspace but also exactly the ground-truth directions. Moreover, we only use \((1/_{0})^{O(r)}\) neurons that only depends on the complexity of target network. This highlights the benefit of continue training first layer weights instead of fixing them after first step.

## 4 Proof overview

In this section, we give the proof overview of these three stages separately.

Denote the optimality gap \(\) at time \(t\) as the difference between current loss and the best loss one could achieve with networks of any size (including infinite-width networks)

\[_{t}= L_{_{t}}(^{(t)})-_{( ^{d-1})}L_{_{t}}(),\] (4)

where \((^{d-1})\) is the set of measures on the sphere \(^{d-1}\). As an example, if \(=_{i}a_{i}\|_{i}\|_{}_{i}}\), then \(L_{}()\) recovers \(L_{}()\) when linear term \(,\) are perfectly fitted and norms are balanced \(|a_{i}|=\|_{i}\|\). We defer the precise definition of \(L_{}()\) to (6) in appendix.

### Stage 1

For Stage 1, we show in the lemma below that the first step of gradient descent identifies the target subspace and ensures there always exists student neuron that is close to every teacher neuron.

**Lemma 3** (Stage 1).: _Under Assumption 1,2,3, consider Algorithm 1 with \(_{0}=_{0}=1\) and \(m m_{0}=_{*}(1)(1/_{0})^{O(r)}\) with any \(_{0}=_{*}(1)\). After first step, with probability \(1-\) we have_

1. _for every teacher neuron_ \(_{i}^{*}\)_, there exists at least one student neuron_ \(_{j}\) _s.t._ \((_{i}^{*},_{j})_{0}\)_._
2. \(\|_{i}^{(1)}\|_{2}=_{*}(1)\)_,_ \(|a_{i}^{(1)}| O_{*}(1/)\) _for all_ \(i[m_{*}]\)_,_ \(_{1}=0\) _and_ \(_{1}=\)_._

The key observation here is similar to Damian et al. (2022) that \(_{i}^{(1)}-2_{0}a_{i}^{(0)}(_{2}^{2} }_{i})\) so that given \(\) is non-degenerate in target subspace \(S_{*}\) we essentially sample \(_{i}^{(1)}\) from the target subspace. It is then natural to expect that the neurons form an \(_{0}\)-net in the target subspace given \(m_{0}\) neurons.

### Stage 2

Given the learned features (first-layer weights) in Stage 1, we now perform least squares to adjust the norms and reach a low loss solution in Stage 2.

**Lemma 4** (Stage 2).: _Under Assumption 1,2,3, consider Algorithm 1 with \(_{t}=}\). Given Stage 1 in Lemma 3, we have Stage 2 ends within time \(T_{2}=_{*}(1/_{0})\) such that optimality gap \(_{T_{2}}=O_{*}(_{0})\)._

It remains an open problem to prove the convergence when training both layers simultaneously beyond early and final stage. To overcome this technical challenge, we choose to use a simple least square for Stage 2. We use the simple (sub)gradient descent to optimize this loss. There exist many other algorithms that can solve this Lasso-type problem, but we omit it for simplicity as this is not the main focus of this paper.

Note that the regularization in Algorithm 1 is the same as standard weight decay when we train both layers. This regularization leads to several desired properties at the end of Stage 2: (1) prevent norm cancellation between neurons: neurons with similar direction but different sign of second layer weights cancel with each other; (2) neurons mostly concentrate around ground-truth directions. As we will see later, these nice properties continue to hold in Stage 3, thanks to the regularization.

### Stage 3

After Stage 2 we are in the local convergence regime. The following lemma shows that we could recover the target network within polynomial time using a multi-epoch gradient descent with decreasing weight decay \(\) at every epoch. Note that this result only requires the initial optimality gap is small and width \(m m_{*}\) (target network width, not \(m_{0}\)).

**Lemma 5** (Stage 3).: _Under Assumption 1,2,3, consider Algorithm 1 on loss (2). Given Stage 2 in Lemma 4, if the initial optimality gap \(_{3,0} O_{*}(_{3,0}^{9/5})\), weight decay \(\) follows the schedule of initial value \(_{3,0}=O_{*}(1)\), and \(k\)-th epoch \(_{3,k}=_{3,k-1}/2\) and stepsize \(_{3k}= O_{*}(_{3,k}^{12}d^{-3})\) for all \(T_{3,k} t T_{3,k+1}\) in epoch \(k\), then within \(K=O_{*}((1/))\) epochs and total \(T_{3}-T_{2}=O_{*}(_{3,0}^{-4}^{-1}^{-2})\) time we recover the ground-truth network \(L()\)._

The lemma above relies on the following result that shows the local landscape is benign in the sense that it satisfies a special case of Lojasiewicz property (Lojasiewicz, 1963). This means GD can always make progress until the optimality gap \(\) is small.

**Lemma 6** (Gradient lower bound).: _When \(_{*}(^{2}) O_{*}(^{9/5})\) and \( O_{*}(1)\), we have_

\[_{}L_{}_{F}^{2}_{*}( ^{4}/^{2}).\]

Note that this generalizes previous result in Zhou et al. (2021) that only focuses on 2-layer networks with positive second layer weights. This turns out to be technically challenging as two neurons with different signs can cancel each other. We discuss how to deal with this challenge in the next section.

## 5 Descent direction in local convergence (Stage 3): the benefit of weight decay

In this section, we give the high-level proof ideas for the most technical challenging part of our results -- characterize the local landscape in Stage 3 (Lemma 6).

The key idea is to construct descent direction -- a direction that has positive correlation with the gradient direction. The gradient lower bound follows from the existence of such descent direction.

It turns out that the existence of both positive and negative second-layer weights introduces significant challenge for the analysis: there might exist neurons with similar directions (e.g., \((a,)\) and \((-a,)\)) that can cancel with each other to have no effect on the output of network. Intuitively, we would hope all of them to move towards 0, but they have no incentive to do so. Moreover, if they are not exactly symmetric it's hard to characterize which directions these neurons will move.

We use standard weight decay to address the above challenge. Specifically, weight decay helps us to

* _Balance norm between neurons_. When norm between two layers are balanced, the \(_{2}\) regularization \(_{i}|a_{i}|^{2}+_{i}^{2}\) would become the effective \(_{1}\) regularization \(2_{i}|a_{i}|_{i}\) over the distribution of neurons. Such sparsity penalty ensures most neurons concentrate around the ground-truth directions, especially preventing norm cancellation between far-away neurons.
* _Reduce cancellation between close-by neurons_. For close-by neurons, weight decay helps to reduce the norm of neurons with the 'incorrect' sign (different sign with the ground-truth neuron). This is because weight decay prefers low norm solutions, and reducing cancellations between neurons can reduce total norm (regularization term) while keeping the square loss same.

We will group the neurons (i.e., partitioning \(^{d-1}\)) based on their distance to the closest teacher neurons: denote \(_{i}=\{:(,_{i}^{*})(,_{j}^{*})j i\}\) (break the tie arbitrarily) so that \(_{i}_{i}=^{d-1}\). We will also use \(_{j}\) to denote \((_{j},_{i}^{*})\) for \(j_{i}\).

As described above, weight decay can always lead to descent direction when norms are not balanced or norm cancellation happens (see Lemma F.15 and Lemma F.16). The following lemma shows that in other scenarios we can always improve features towards the ground-truth directions.

**Lemma 7** (Feature improvement descent direction, informal).: _When norms are balanced and no norm cancellation happens, there exists properly chosen \(q_{ij} 0\) and \(_{j_{i}}a_{j}q_{ij}=a_{i}^{*}\) such that_

\[_{i[m_{*}]}_{j_{i}}_{_{i}}L_{ },_{j}-q_{ij}_{i}^{*}=().\]In words, this descent direction is the following: we move neuron \(_{j}_{i}\) toward either ground-truth direction \(_{i}^{*}\) or 0 depending on whether it is in the neighborhood of teacher neuron \(_{i}^{*}\). Specifically, we move far-away neurons towards 0 (and thus setting \(q_{ij}=0\)) and move close-by neurons towards its 'closest' minima \(q_{ij}_{i}^{*}\) (the fraction of \(_{i}^{*}\) that neuron \(_{j}\) should target to approximate). See Figure 1 for an illustration.

The proof of the above lemma requires a dedicated characterization of the low loss solution's structure, which we describe in Section 6.

## 6 Structure of (approximated) minima

In this section, we first highlight the importance of understanding local geometry by showing the challenges in proving the existence of descent direction (Lemma 7). Then after presenting the main result of this section to show the structure of (approximated) minima (Lemma 8), we discuss several proof ideas such as dual certificate analysis in the remaining part.

### Constructing descent direction requires better understanding of local geometry

To show the existence of descent direction in Lemma 7, we compute the inner product between gradient and constructed descent direction. We can lower bound it by (assuming norms are balanced)

\[+2_{i[m_{*}]}_{j_{i}}_{}[R()a_{j}q_{ij}_{i}^{*}(^{}(_{i}^{*})- ^{}(_{j}^{}))],\]

where \(R()=f()-_{*}()\) is the residual. Thus, in order to get a lower bound, the goal is to show second term above is small than \(\). As we can see, this term is quite complicated and can be viewed as the inner product between \(R()\) and \(h()=_{i[m_{*}]}_{j_{i}}a_{j}q_{ij}_{i}^{* }(^{}(_{i}^{*})-^{}(_{ j}^{}))\).

Average neuron and residual decompositionTo deal with above challenge, we use the idea of average neuron and residual decomposition. For each teacher neuron \(_{i}^{*}\), denote \(_{i}=_{j_{i}}a_{j}_{j}\) as the average neuron. Intuitively, this average neuron \(_{i}\) stands for an idealize case where all neurons belong to \(_{i}\) (closer to \(_{i}^{*}\) than other \(_{j}^{*}\)) collapse into a single neuron.

We decompose the residual \(R()=f()-_{*}()\) into the 3 terms below: denote \(}_{i}=_{i}-_{i}^{*}\)

\[R_{1}() =_{i[m_{*}]}}_{i}^{} (_{i}^{*}),R_{2}()=_{i [m_{*}],j_{i}}a_{j}_{j}^{}( (_{j}^{})-(_{i}^{*})),\] \[R_{3}() =}(_{i[m_{*}]}a_{i}^{*}\| _{i}^{*}\|_{2}-_{i[m]}a_{i}\|_{i}\|_{2} )+-+(-})^{}.\]

\(R_{1}\) can be thought as the exact-parametrization setting (use \(m_{*}\) neurons to learn \(m_{*}\) neurons), where the average neurons \(\{_{i}\}_{i=1}^{m_{*}}\) are the effective neurons. The difference between this exact-parametrization and overparametrization setting is then characterized by the term \(R_{2}\), which captures the difference in nonlinear activation pattern. This term in fact suggests the loss landscape is degenerate in overparametrized case and slows down the convergence (Zhou et al., 2021; Xu and Du, 2023). Overall, this residual decomposition is similar to Zhou et al. (2021), with additional modification of \(R_{3}\) to deal with ReLU activation and linear term \(,\).

Figure 1: Illustration of descent direction

To some extent, our residual decomposition can be viewed as a kind of 'bias-variance' decomposition in the sense that the 'bias' term \(R_{1}\) captures the overall average contribution of all neurons, and the 'variance' term \(R_{2}\) captures the individual contributions of each neuron that are not reflected in \(R_{1}\).

High-level proof plan of Lemma 7We now are ready to give a proof plan for Lemma 7. The key is to show properties of minima that can help us to bound \( R,h\).

1. Show that neurons mostly concentrate around ground-truth directions.
2. Show that average neuron \(_{i}\) is close to teacher neuron \(_{i}^{*}\) for all \(i[m]\).
3. Use above structure to bound \( R_{i},h\). Specifically, bounding \( R_{1},h\) relies on the fact that average neuron is close to teacher neuron (step 2); a bound on \( R_{2},h\) follows from far-away neurons are small (step 1); third term \( R_{3},h\) can be directly bounded using the loss. Detailed calculations are deferred into Appendix H.3.

We give main result of this section that shows the desired local geometry properties more precisely ((i)(ii) corresponding to step 1 and (iii) corresponding to step 2 above).

**Lemma 8** (Informal).: _Suppose the optimality gap is \(\), we have_

1. _Total norm of far-away neurons is small:_ \(_{i[m_{*}]}_{j_{i}}|a_{j}|\|_{j}\| _{2}_{j}^{2}=O_{*}(/),\) _where angle_ \(_{j}=(_{j},_{i}^{*})\) _for_ \(_{j}\) _that_ \(j_{i}\)_._
2. _For every_ \(_{i}^{*}\)_, there exists at least one close-by neuron_ \(\) _s.t._ \((,_{i}^{*})_{close}=O_{*}(^{1/3})\)_._
3. _Average neuron is close to teach neurons: we have_ \(\|_{i}-_{i}^{*}\|_{2} O_{*}((/)^{3/4})\)_._

These properties give us a sense of what the network should look like when loss is small: neurons have large norm only if they are around the ground-truth directions. Moreover, when \(/ 0\), student neuron must align with one of teacher neurons (\(_{j}=0\)) or norm becomes 0 (\(|a_{j}|\|_{j}\|=0\)). This can be understood from the \(_{1}\) regularized loss (equivalent to \(_{2}\) regularization on both layers) that promotes the sparsity over the distribution of neurons. In the rest of this section, we discuss new techniques such as dual certificate that we develop for the proof.

### Neurons concentrate around teacher neurons: dual certificate analysis and test function

We focus on Lemma 8(i)(ii) here. We will use a dual certificate technique similar to Poon et al. (2023) to prove Lemma 8(i), and a more general construction of test function to prove Lemma 8(ii). In below, we consider a relaxed version of original optimization problem (2) by allowing infinite number of neurons, i.e., distribution of neurons, with \(_{ 2}(x)=(x)-1/-x/2\) instead of ReLU:

\[_{(^{d-1})}L_{}():=L(;_{ _{2}})+||_{1},\] (5)

where \(_{}^{*}\) is the minimizer. We use \(_{ 2}\) activation because this is the effective activation when linear terms \(,\) are perfectly fitted (remove 0th and 1st order Hermite expansion of ReLU, see Claim B.1 and (6) in appendix).

This is the loss function we would have in the idealized setting: (1) linear term \(,\) reach their global minima (this is easy to achieve as loss is convex in them); (2) use \(_{1}\) regularization instead of \(_{2}\) regularization, since this is the case when the first and second layer norm are balanced (weight decay encourages this to happen). Note that the results in this part can handle almost all activation as long as its Hermite expansion is well-defined, generalizing Zhou et al. (2021) that can only handle absolute/ReLU activation. In below we will focus on the activation \(_{ 2}\) for simplicity.

Dual certificateThis optimization problem (5) can be viewed as a natural extension of the classical compressed sensing problem (Donoho, 2006; Candes et al., 2006) and Lasso-type problem (Tibshirani, 1996) in the infinite dimensional space, which has been studied in recent years (Bach, 2017; Poon et al., 2023). One common way is to study its dual problem. The dual solution \(p_{0}()\) (maps \(^{d}\) to \(\)) of (5) when \(=0\) satisfies \(_{}[p_{0}()_{ 2}(^{})] |_{*}|(^{d-1})\) (more detailed discussions on this dual problem can be found in e.g., Poon et al. (2023)). Here \(()=_{}[p()_{ 2}(^{})]\) is often called dual certificate, as it serves as a certificate of whether a solution \(\) is optimal. Its meaning will be clear in the discussions below.

We now introduce the notion of non-degenerate dual certificate, motivated by Poon et al. (2023). Note that the condition \(()|_{*}|(^{d-1})\) implies that \((_{i}^{*})=(a_{i}^{*})\) and \(_{} 1\). The following definition is a slightly stronger version of the above implications as it requires \(\) to decay at least quadratic when moves away from \(_{i}^{*}\).

**Definition 1** (Non-degenerate dual certificate).: \(()\) _is called a non-degenerate dual certificate if there exists \(p()\) such that \(()=_{}[p()_{ 2}(^{})]\) for \(^{d-1}\) and_

* \((_{i}^{*})=(a_{i}^{*})\) _for_ \(i=1,,m_{*}\)_._
* \(|()| 1-_{}(,_{i}^{*})^{2}\) _if_ \(_{i}\)_, where_ \((,_{i}^{*})=(,_{i}^{*})\)_._

The existence and construction of the non-degenerate dual certificate is deferred to Appendix G. We focus on the implications of such non-degenerate dual certificate below.

Roughly speaking, the dual certificate only focuses on the position of ground-truth directions \(_{i}^{*}\) as it decays fast when moving away from these directions (Figure 2). Thus, if \(\) exactly recovers ground-truth \(_{*}\), then we have \(,_{*}=|_{*}|_{1}\). The gap between \(,\) and \(||_{1}\) is large when \(\) is away from \(_{*}\). Therefore, \(\) can be viewed as a certificate to test the optimality of \(\). The lemma below makes it more precise.

**Lemma 9**.: _Given a non-degenerate dual certificate \(\), then_

* \(,^{*}=|^{*}|_{1}\) _and_ \(|,-^{*}| p_{2}\)_._
* _For any measure_ \((^{d-1})\)_,_ \(|,|||_{1}-_{}_{i[m_{*}]}_{ _{i}}(,_{i}^{*})^{2}\,||()\)_._

In the finite width case, we have \(_{i[m_{*}]}_{_{i}}(,_{i}^{*})^{2}\, ||()=_{i}|a_{i}|_{i} _{i}^{2}\). This is exactly the quantity that we are interested in Lemma 8.

To see the usefulness of Lemma 9, we show a proof for total norm bound of the optimal solution \(_{}^{*}\). The proof for general \(\) with optimality gap \(\) is similar (Lemma F.5).

**Claim 1** (Lemma 8(i) for \(_{}^{*}\)).: \(_{i[m_{*}]}_{_{i}}(,_{i}^{*})^{2}\, |_{}^{*}|() O_{*}()\)__

Proof.: It is not hard to show \(|_{}^{*}|_{1}|^{*}|_{1}\) (Lemma F.3) so we have

\[|_{}^{*}|_{1}-|^{*}|_{1}-,_{}^{*}-^{*} -,_{}^{*}-^{*}.\]

Using Lemma 9 and the fact \(L(_{}^{*})=O_{*}(^{2})\) from Lemma F.3,

\[= |_{}^{*}|_{1}-,_{}^{*} _{}_{i[m_{*}]}_{_{i}}(,_{ i}^{*})^{2}\,|_{}^{*}|(), p _{2}^{*})}=O_{*}().\]

We have \(_{i[m_{*}]}_{_{i}}(,_{i}^{*})^{2}\, |_{}^{*}|()=O_{*}()\). 

Test functionThe idea of using test function is to identify certain properties of the target function/distribution that we are interested in. Specifically, we construct test function so that it only correlates well with the target function that has the desired property. Generally speaking, the dual certificate above can be consider as a specific case of a test function: the correlation between dual certificate \(\) and distribution of neurons \(\) is large (reach \(||_{1}\)) only when \(_{*}\).

In below, we use this test function idea to show that every ground-truth direction has close-by neuron (Lemma 8(ii)). Denote \(_{i}():=\{j:(_{j},_{i})\} _{i}\) as the neurons that are \(\)-close to \(_{i}^{*}\).

**Lemma 10** (Lemma 8(ii), informal).: _Given the optimality gap \(\), we have the total mass near each target direction is large, i.e., \((_{i}())(a_{i}^{*})|a_{i}^{*}|/2\) for all \(i[m_{*}]\) and any \(_{*}(^{1/3})\)._

Note that although the results in the dual certificate part (Lemma 9(ii)) can imply that there are neurons close to teacher neurons, the bound we get here using carefully designed test function are sharper (\(^{1/3}\) vs. \(^{1/4}\)). This is in fact important to the descent direction construction (Lemma 7).

Figure 2: Dual certificate \(\).

In the proof, we view the residual \(R()=f_{n}()-f_{*}()\) as the target function and construct test function that will only have large correlation if there is a teacher neuron that have no close student neurons. Specifically, the test function \(g\) only consists of high-order Hermite polynomial such that it is large around the ground-truth direction and decays fast when moving away (Figure 3). It looks like a single spike in dual certificate \(\), but in fact decays much faster than \(\) when moving away. It is more flexible to choose test function than dual certificate, so test function \(g\) can focus only on a local region of one ground-truth direction and give a better guarantee than dual certificate analysis.

### Average neuron is close to teacher neuron: residual decomposition and average neuron

We give the proof idea for Lemma 8(iii) that shows average neuron \(_{i}\) is close to teacher neuron \(_{i}^{*}\) using the residual decomposition \(R=R_{1}+R_{2}+R_{3}\).

The key is to observe that \(R_{1}\) is an analogue to exact-parametrization case where loss is often strongly-convex, so we have \( R_{1}_{2}^{2}=_{*}(1)_{i} _{i}-_{i}^{*}_{2}^{2}\). Then the goal is to upper bound \( R_{1}\). Given the decomposition \(R=R_{1}+R_{2}+R_{3}\), it is easy to bound \( R_{1} R+ R_{2 }+ R_{3}\). We focus on \( R_{2}\) as the other two are not hard to bound (loss is small in local regime). \(R_{2}\) is in fact closely related with the total weighted norm bound in Lemma 8: we show \( R_{2}=O_{*}(1)(_{j_{i}}|a_{ j}|_{j}_{2}_{j}^{2})^{3/2}=O_{*}(( /)^{3/2})\). Thus, we get a bound for \(_{i}-_{i}^{*}\). See Appendix F.1.4 for details.

## 7 Conclusion

In this paper we showed that gradient descent converges in a large local region depending on the complexity of the teacher network, and the local convergence allows 2-layer networks to perform a strong notion of feature learning (matching the directions of ground-truth teacher networks). We hope our result gives a better understanding of why gradient-based training is important for feature learning in neural networks. Our results rely on adding standard weight decay and new constructions of dual certificate and test functions, which can be helpful in understanding local optimization landscape in other problems. A natural but challenging next step is to understand whether the intermediate steps are also important for feature learning.