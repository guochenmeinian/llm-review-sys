# Pretrained Transformer Efficiently Learns

Low-Dimensional Target Functions In-Context

 Kazusato Oko\({}^{1,3}\), Yujin Song\({}^{2,3}\), Taiji Suzuki\({}^{2,3}\), Denny Wu\({}^{4,5}\)

\({}^{1}\)University of California, Berkeley, \({}^{2}\)University of Tokyo, \({}^{3}\)RIKEN AIP

\({}^{4}\)New York University, \({}^{5}\)Flatiron Institute

oko@berkeley.edu, song-yujin139@g.ecc.u-tokyo.ac.jp,

taiji@mist.i.u-tokyo.ac.jp, dennywu@nyu.edu

###### Abstract

Transformers can efficiently learn in-context from example demonstrations. Most existing theoretical analyses studied the in-context learning (ICL) ability of transformers for linear function classes, where it is typically shown that the minimizer of the pretraining loss implements one gradient descent step on the least squares objective. However, this simplified linear setting arguably does not demonstrate the statistical efficiency of ICL, since the pretrained transformer does not outperform directly solving linear regression on the test prompt. In this paper, we study ICL of a nonlinear function class via transformer with nonlinear MLP layer: given a class of _single-index_ target functions \(f_{*}()=_{*}(,)\), where the index features \(^{d}\) are drawn from a \(r\)-dimensional subspace, we show that a nonlinear transformer optimized by gradient descent (with a pretraining sample complexity that depends on the _information exponent_ of the link functions \(_{*}\)) learns \(f_{*}\) in-context with a prompt length that only depends on the dimension of the distribution of target functions \(r\); in contrast, any algorithm that directly learns \(f_{*}\) on test prompt yields a statistical complexity that scales with the ambient dimension \(d\). Our result highlights the adaptivity of the pretrained transformer to low-dimensional structures of the function class, which enables sample-efficient ICL that outperforms estimators that only have access to the in-context data.

## 1 Introduction

Pretrained transformers  possess the remarkable ability of _in-context learning (ICL)_, whereby the model constructs a predictor from a prompt sequence consisting of pairs of labeled examples without updating any parameters. A common explanation is that the trained transformer can implement a learning algorithm, such as gradient descent on the in-context examples, in its forward pass . Such explanation has been empirically studied in synthetic tasks such as linear regression , which has motivated theoretical analyses of the statistical and computational complexity of ICL in learning simple function classes.

Many recent theoretical works on ICL focus on learning the function class of _linear models_ using linear transformers trained via empirical risk minimization. In this setting, it can be shown that minima of the pretraining loss implements one (preconditioned) gradient descent step on the least squares objective computed on the test prompt . This implies that the forward pass of the pretrained transformer can learn linear targets with \(n d\) in-context examples, hence matching the sample complexity of linear regression on the test prompt. Subsequent works also studied how the distribution and "diversity" of pretrained tasks affect the ICL solution in similar problem settings .

The motivation of our work is the observation that the simple setting of learning linear models with linear transformers does not fully capture the statistical efficiency and adaptivity of ICL. Specifically,

* A linear transformer has limited expressivity and hence only implements simple operations in the forward pass, such as one gradient step for least squares regression. This limits the class of algorithms that can be executed in-context, and consequently, the pretrained transformer cannot outperform directly solving linear regression on the test prompt. We therefore ask the question: _With the aid of MLP layer, can a pretrained transformer learn a nonlinear function class in-context, and outperform baseline algorithms that only have access to the test prompt?_
* A key feature of ICL is the _adaptivity_ to structure of the learning problem. For example,  empirically showed that transformers can match the performance of either ridge regression or LASSO, depending on parameter sparsity of the target class;  observed that transformers transitions from a weighted-sum estimator to ridge regression as the number of pretraining tasks increases. Such adaptivity enables the pretrained model to outperform algorithms that only have access to the test prompt, which cannot take into account the "prior" distribution of target functions. Hence a natural question to ask is _Can a pretrained transformer adapt to certain structures of the target function class, and how does such adaptivity contribute to the statistical efficiency of ICL?_

### Our Contributions

#### 1.1.1 Function Class: Gaussian Single-index Models

To address the above questions, we study the in-context learning of arguably the simplest _nonlinear_ extension of linear regression where a nonlinearity is applied to the response. Specifically, we consider the following class of _single-index_ target functions, where the \(t\)-th pretraining task is constructed as

\[_{1}^{t},_{2}^{t},,_{N}^{t},^{t}}{}(0,_{d}), y_{i}^{t}=_{*}^{t }((_{i}^{t},^{t}))+_{i}^{t},\] (1.1)

where \(_{*}^{t}:\) is the unknown link function, and \(^{t}^{d}\) is the index feature vector which is drawn from some fixed \(r\)_-dimensional subspace_ for some \(r d\). The task is to learn this input-output relation by reading the context \((_{1},y_{1},,_{N},y_{N})\) and predict the output corresponding to the query \(\) (See Section 2 for details). This problem setting is based on the following considerations.

* **Nonlinearity of function class.** Due to the nonlinear link function, single-index targets cannot be learned by linear transformers. For this class of functions, the statistical efficiency of simple algorithms has been extensively studied: given a link function with degree \(P\) and information exponent \(Q\) (defined as the index of the smallest non-zero coefficient in the Hermite expansion), we know that kernel methods require at least \(n d^{P}\) samples , whereas two-layer neural network trained by gradient descent can achieve better sample complexity \(n d^{(Q)}\). Hence these algorithms, if directly applied to the test prompt, require a context length _polynomial_ in the ambient dimensionality \(d\), which arguably deviates from practical settings of ICL where the pretrained transformer learns from a few in-context examples which may come from high dimensional data space. These algorithms serve as a baseline for comparing the statistical efficiency of ICL.
* **Two types of low-dimensional structures.** Prior works on gradient-based feature learning highlights the adaptivity of neural network to _low-dimensional functions_, i.e., single-index model \(_{*}(_{i},)\) that depends on one direction (index features) \(\) in \(^{d}\). In such setting, the complexity of gradient descent is dominated by the search for direction \(^{d}\), the difficulty of which can be characterized by various computational lower bounds . Importantly, our pretraining problem setup introduces _another notion of low dimensionality_: the "distribution" of target functions is low-dimensional, since the index features for each task \(_{t}\) are drawn from a rank-\(r\) subspace. This low-dimensionality of _function class_ cannot be exploited by any algorithms that directly estimate the target function from test prompt, but as we will see, transformers can adapt to this additional structure via gradient-based pretraining, which reduces the search problem (for the index features \(\)) to \(r\)-dimensional in the in-context phase. Therefore, when \(r d\), we expect pretrained transformers to outperform baseline algorithms on the in-context examples (kernel methods, neural network, etc.).

Empirical observations.We pretrain a GPT-2 model  (with the same configurations as the in-context linear regression setting in ) to learn the Gaussian single-index task (1.1) with degree-3 link function, and compare its in-context sample complexity against baseline algorithms (see Section 4 for details). In Figure 1 we observe that the pretrained transformer achieves low prediction risk using fewer in-context examples than two baseline algorithms: kernel ridge regression, and neural network trained by gradient descent. Moreover, we observe that unlike the baseline methods, _Transformer achieves an in-context sample complexity (almost) independent of the ambient dimensionality \(d\)._

The goal of this work is to rigorously establish a \(d\)-independent in-context sample complexity in an idealized theoretical setting for a shallow transformer optimized via gradient-based pretraining.

#### 1.1.2 Main Result: Learning Single-index Models In-Context

We characterize the sample complexity of learning (1.1) in-context, using a nonlinear transformer optimized by gradient descent. Each single-index task is specified by an unknown index feature vector \(^{d}\) drawn from some \(r\)-dimensional subspace, and a link function \(_{*}\) with degree \(P\) and information exponent \(Q P\); we allow the degree and information exponent to vary across tasks, to model the scenario where the difficult of pretraining tasks may differ. We show that pretraining of the nonlinear MLP layer can extract the low-dimensional structure of the function class, and the attention layer efficiently approximates the nonlinear link function. Our main theorem upper bounds the in-context generalization error of the pretrained transformer.

**Theorem** (Informal).: _Let \(f:(_{1},y_{1},,_{N},y_{N},) y\) be a transformer with nonlinear MLP layer pretrained with gradient descent (Algorithm 1) on the single-index regression task (1.1). With probability at least 0.99, the model \(f\) achieves in-context prediction risk \(|f()-f_{*}()|-=o_{d}(1)\), where \(\) is the noise level, if the number of pretraining tasks \(T\), the number of training examples \(N\), the test prompt length \(N^{*}\), and the network width \(m\) satisfy (we omit polylogarithmic factors)_

\[}_{},}_{}, r ^{(P)}}_{},}_{},\]

_where \(Q,P\) are the information exponent and the highest degree of link functions, respectively._

To the best of our knowledge, this is the first end-to-end optimization and statistical guarantee for in-context regression of this nonlinear function class. We make the following remarks.

* The required sample size for _pretraining_\(T,N\) scale with the ambient dimensionality \(d\). In particular, the number of pretraining tasks \(T\) is parallel to the complexity of learning a single-index model with information exponent \(Q\) using a two-layer neural network. On the other hand, the sample complexity for the _in-context_ phase only depends on the dimensionality of the function class \(r d\).
* Note that any estimator that only has access to the in-context examples requires \(n d\) samples to learn the single-index model, as suggested by the information theoretic lower bound (e.g., see ). Therefore, when \(r d\), we see a separation between pretrained transformer and algorithms that directly learn from the test prompt, such as linear/kernel regression and neural network + gradient descent. This highlights the adaptivity (via pretraining) of transformers to low-dimensional structures of the target function class.
* Our analysis of pretraining reveals the following mechanism analogous to : the nonlinear MLP layer extract useful features and adapt to the low-dimensionality of the function class, whereas the attention layer performs in-context function approximation on top of the learned features.

### Related Works

Theory of in-context learning.Many recent works studied the in-context learning ability of transformers trained by gradient descent.  studied

Figure 1: In-context generalization error (with standard deviation) of kernel ridge regression, neural network + gradient descent, and pre-trained transformer. The target function is a polynomial single-index model. We fix \(r=8\) and vary \(d=16,32\).

the training of linear transformer models to learn linear target functions in-context by implementing one gradient descent step in the forward pass. Similar optimization results have been established for looped linear transformers , transformers with SoftMax attention  or nonlinear MLP layer . Our problem setting resembles , where a nonlinear MLP block is used to extract features, followed by a linear attention layer; the main difference is that we establish end-to-end guarantees on the optimization and sample complexity for learning a concrete nonlinear function class, whereas  focused on convergence of optimization.  showed that transformers can learn nonlinear functions in-context via a functional gradient mechanism, but no statistical and optimization guarantees were given. If we do not take gradient-based optimization into account, the function class that can be implemented in-context by transformers has been characterized in many prior works . These results typically aim to encode specific algorithms (LASSO, gradient descent, etc.) in the forward pass or directly analyze the Bayes-optimal estimator.

Gradient-based learning of low-dimensional functions.The complexity of learning low-dimensional functions with neural network has been extensively studied in the feature learning theory literature. Typical target functions include single-index models  and multi-index models . While a shallow neural network can efficiently approximate such low-dimensional functions, the efficiency of gradient-based training is governed by properties of the nonlinearity \(^{*}\). In the single-index setting, prior works established a sufficient sample size \(n d^{(K)}\), where \(K\) is the _information exponent_ for algorithms utilizing correlational information , or the _generative exponent_ for algorithms that employ suitable label transformations . Moreover, \(n d\) samples are information theoretically necessary without additional structural assumptions; this entails that estimators that only access the test prompt inevitably pay a sample size that scales with the ambient dimensionality. As we will see, pretrained transformers can avoid this "curse of dimensionality" by exploiting the low-dimensionality of the task distribution. Low-dimensional structure of the _function class_ similar to our setting has been assumed to study the efficiency of transfer learning  and multi-task learning .

## 2 Problem Setting

Notations.\(\|\|\) denotes the \(_{2}\) norm for vectors and the \(_{2}_{2}\) operator norm for matrices. For a vector \(\), we use \(_{a:b}\) for \(a b\) to denote the vector \([w_{a},w_{a+1},,w_{b}]^{}\). \(_{N}\) denotes the all-one vector of size \(N\). The indicator function of \(A\) is denoted by \(_{A}\). Let \(N\) be a nonnegative integer; then \([N]\) denotes the set \(\{n 1 n N\}\). For a nonnegative integer \(i\), the \(i\)-th Hermite polynomial is defined as \(_{i}(z)=(-1)^{i}^{}{2}}^{i}}{ z^{i}}}{2}\). For a set \(S\), \((S)\) denotes the uniform distribution over \(S\). We denote the unit sphere \(\{^{d}\|\|=1\}\) by \(^{d-1}\). \((),()\) represent \(O()\) and \(()\) notations where polylogarithmic terms are hidden. We write \(a b\) when there exists a constant \(c\) such that \(a cb\). If both \(a b\) and \(b a\) holds, we write \(a b\).

### Data Generating Process

#### 2.1.1 In-context learning

We first introduce the basic setting of ICL  of simple function classes as investigated in . In each _task_, the learner is given a sequence of inputs and outputs \((_{1},y_{1},,_{N},y_{N},)\) referred to as _prompt_, where \(_{i},^{d}\) and \(y_{i}\). The labeled examples \(=(_{1}_{N})^{d N}\), \(=(y_{1} y_{N})^{}^{N}\) are called _context_, and \(\) is the _query_. Given input distribution \(_{1},,_{N},}{}_{ }\), the output \(y_{i}\) is expressed as

\[y_{i}=f_{*}(_{i})+_{i}, i[N],\]

where \(f_{*}\) is the true function describing the input-output relation and \(_{i}}{}_{}\) is i.i.d. label noise. Note that \(f_{*}\) also varies across tasks -- we assume \(f_{*}\) is drawn i.i.d. from some true distribution \(_{f_{*}}\). In the pretraining phase, we optimize the model parameters given training data from \(T\) distinct tasks \(\{(_{1}^{t},y_{1}^{t},,_{M}^{t},y_{M}^{t},^{t},y^{t}) \}_{t=1}^{T}\), which is composed of prompts \(\{(_{1}^{t},y_{1}^{t},,_{M}^{t},y_{M}^{t},^{t})\}_{t=1}^ {T}\) and responses \(\{y^{t}\}_{t=1}^{T}\) for queries \(\{^{t}\}_{t=1}^{T}\), where \(y^{t}=f_{*}^{t}(^{t})+^{t}\) and \(^{t}}{}_{}\).

We say a model learns these functional relations _in-context_, when the model can predict the output \(f_{*}()\) corresponding to query \(\) by solely examining the context \((,)\), without updating model parameters for each task. Given the pretrained model \(f(,,;)\) with parameter \(\) which predicts the label of query \(\) from context \((,)\), we define the _expected ICL risk_ as

\[_{N^{*}}(f)[|f(_{1:N^{*}},_{1:N^{*} },;)-y|],\] (2.1)

where \(y=f_{*}()+\) and the expectation is taken over the in-context data: \(_{1},,_{N^{*}},_{},f_{*} _{f_{*}},_{1},,_{N^{*}}, _{}\). Note that we take the expectation with respect to contexts \((_{1:N^{*}},_{1:N^{*}})^{d N^{*}}^{N^{*}}\) of length \(N^{*}\), in order to examine the behavior of ICL at a specific context length.

#### 2.1.2 Gaussian single-index models

We consider the situation where the true input-output relation is expressed by _single-index models_, i.e., functions that only depend on the direction of the index vector \(\) in the input space. An efficient learning algorithm should adapt to this feature and identify the relevant subspace from high-dimensional observations; hence this problem setting has been extensively studied in the deep learning theory literature  to demonstrate the adaptivity of gradient-based feature learning.

**Assumption 1**.: _Let \( 0\) be the noise level. The prompt \((_{1},y_{1},,_{N},y_{N},)\) is generated as_

\[_{1},_{2},,_{N},}{ }_{}=(0,_{d}), y_{i}=f_{*}(_{ i})+_{i}, f_{*}(_{i})=_{*}(_{i}, ),\]

_where \(_{1},,_{N}}{}(\{-,\})\), and the distribution \(_{f_{*}}\) of the true function \(f_{*}\) is specified as:_

1. _[leftmargin=*]_
2. _Index Features._ _Let_ \(\) _be an_ \(r d\)_-dimensional linear subspace of_ \(^{d}\)_. We draw_ \(\) _uniformly from the unit sphere_ \(()\) _in_ \(\)_, i.e., from_ \(()\{,\| \|=1\}\)_._
3. _Link Function._ \(_{*}(z)=_{i=Q}^{P}}{i!}_{i}(z)\)_, where_ \(2 Q P\)_. We draw the Hermite coefficients_ \(\{c_{i}\}_{i=Q}^{P}\) _from any distribution satisfying_ \[[c_{Q}^{2}]=_{d,r}(1) 0,_{i=Q}^{P}c_{i}^{2} R_{c}^{2 }\;(a.s.)\;(c_{Q},,c_{P})(0,,0)\;(a.s.).\] (2.2)

Throughout the paper, we assume that \(P d,r\) and \(r d\); specifically, we take \(P=_{d,r}(1)\) and \(r d^{1/2}\). Note that the condition \(r d\) entails that the class of target functions is _low-dimensional_, and as we will see, such structure can be adapted by the transformer via pretraining.

**Remark 1**.: _We make the following remarks on the assumption of single-index function class._

* _For each task, the target is a single-index model with different index features drawn from some rank-_ \(r\) _subspace, and different link function with degree at most_ \(P\) _and information exponent (defined as the index of the lowest degree non-zero coefficient in the Hermite expansion of the link function, i.e,_ \(\{i c_{i} 0\}\) _in this case; see_ _[_1, 10_]_ _at least_ \(Q\)_. This heterogeneity reflects the situation where the difficulty of learning the input-output relation varies across tasks. Note that we allow for different distributions of the Hermite coefficients_ \(\{c_{i}\}\)_: for example, we may set_ \((c_{Q},,c_{P})(c_{Q}, c_{P})|_{i=Q}^ {P}^{2}}{i!}=1}\) _(manifold of coefficients satisfying_ \(_{}[f_{*}()]=1\)_),_ \((\{0,1\}^{P-Q+1}(0,,0))\)_, or_ \((\{(1,,0),,(0,,1)\})\)_._
* _The condition_ \(Q 2\) _(i.e., the Hermite expansion of_ \(_{*}\) _does not contain constant and linear terms) ensures that the gradient update detects the entire_ \(r\)_-dimensional subspace instead of the trivial rank-1 component. For generic polynomial_ \(_{*}\)_, this assumption can be satisfied by a simple preprocessing step that subtracts the low-degree components, as done in_ _[_1_]__._

### Student Model: Transformer with Nonlinear MLP Layer

We consider a transformer composed of a single-layer self-attention module preceded by an embedding module using a nonlinear multi-layer perceptron (MLP). Let \(^{d_{e} d_{N}}\) be an embeddingmatrix constructed from prompt \((_{1},y_{1},,_{N},y_{N},)\). A single-layer SoftMax self-attention module  is given as

\[f_{}(;^{P},^{V},^{K},^{Q})=+ {W}^{P}^{V}^{K})^{ }^{Q}}{},\] (2.3)

where \(\) is the temperature, and \(^{K},^{Q}^{d_{k} d_{e}}\), \(^{V}^{d_{v} d_{e}}\) and \(^{P}^{d_{e} d_{v}}\) are the key, query, value, and projection matrix, respectively. Following prior theoretical works , we remove the SoftMax and instead analyze the _linear_ attention with \(=N\); it has been argued that such simplification can reproduce phenomena in practical transformer training . We further simplify the original self-attention module (2.3) by merging \(^{P}^{V}\) as \(^{PV}^{d_{e} d_{e}}\) and \((^{K})^{}^{Q}\) as \(^{KQ}^{d_{e} d_{e}}\), and consider the following parameterization also introduced in , WZC\({}^{+}\)23],

\[^{PV}=*&*\\ _{1(d_{e}-1)}&v,^{KQ}=&*\\ _{1(d_{e}-1)}&*,\] (2.4)

where \(v\) and \(^{(d_{e}-1)(d_{e}-1)}\). Then, the simplified attention module is written as \(_{}(;^{PV},^{KQ})=+^{PV} ^{}^{KQ}}{N}\), and we take the right-bottom entry as the prediction of \(y\) corresponding to query \(\).

Prior analyses of linear transformers  defined the embedding matrix \(\) simply as the input-output pairs; however, the combination of linear embedding and liner attention is not sufficient to learn nonlinear single-index models. Instead, we set \(d_{e}=m+1,d_{N}=N+1\) for \(m\) and construct \(\) using an MLP layer:

\[=(_{1}^{}_{1}+b_{1})&&( _{1}^{}_{N}+b_{1})&(_{1}^{}+b_{1})\\ &&&\\ (_{m}^{}_{1}+b_{m})&&(_{m}^{}_{N}+b_{m})&(_{m}^{}+b_{m})\\ y_{1}&&y_{N}&0^{(m+1)(N+1)},\] (2.5)

where \(_{1},,_{m}^{d}\) and \(b_{1},,b_{m}\) are trainable parameters and \(:\) is a nonlinear activation function; we use \((z)=(z)=\{z,0\}\). This is to say, we define the embedding as the "hidden representation" \((^{}+b)\) of a width-\(m\) two-layer neural network. For concise notation we write \(=(_{1},,_{m})^{},=(b_{1},,b_{m})^{}\).

**Remark 2**.: _We make the following remarks on the considered architecture._

* _The MLP embedding layer before attention has been adopted in recent works_ _[_1_, 12, 13, 14]__. This setting can be interpreted as an idealized version of the mechanism that lower layers of the transformer construct useful representation, on top of which upper attention layers implement the in-context learning algorithm._
* _Our architecture is also inspired by recent theoretical analyses of gradient-based feature learning, where it is shown that gradient descent on the MLP layer yields adaptivity to features of the target function and hence improved statistical efficiency_ _[_1_, 12, 13]__._

Combining the attention \(_{}\) and MLP embedding (2.5), we can express the model prediction \(y\) as

\[f(,,;,,)=(+_{N}^{})}{N}, (_{1}^{}+b_{1})\\ \\ (_{m}^{}+b_{m}),\] (2.6)

where \(=v^{}\) and \((+_{N}^{})\) denotes the \(m N\) matrix whose \((i,j)\)-th entry is \((_{i}^{}_{j}+b_{i})\); see Appendix E for full derivation. We refer to \(\) as the _attention matrix_.

### Pretraining: Empirical Risk Minimization via Gradient Descent

We pretrain the transformer (2.6) by the gradient-based learning algorithm specified in Algorithm 1, which is inspired by the layer-wise training procedure studied in the neural network theory literature . In particular, we update the trainable parameters in a sequential manner.

* In Stage I we optimize the parameters of the MLP (embedding) layer, which is a non-convex problem due to the nonlinear activation function. To circumvent the nonlinear training dynamics, we follow the recipe in recent theoretical analyses of gradient-based feature learning ; specifically, we zoom into the _early phase_ of optimization by taking one gradient step on on the regularized empirical risk. As we will see, the first gradient step already provides a reliable estimate of the target subspace \(\), which enables the subsequent attention layer to implement a sample-efficient in-context learning algorithm.
* In Stage II we train the attention layer, which is a convex problem and the global minimizer can be efficiently found. We show that the optimized attention matrix \(\) performs regression on the polynomial basis (defined by the MLP embedding), which can be seen as an in-context counterpart to the second-layer training (to learn the polynomial link) in .

## 3 Transformer Learns Single-index Models In-Context

### Main Theorem

Our main theorem characterizes the pretraining and in-context sample complexity of the transformer with MLP layer (2.6) optimized by layer-wise gradient-based pretraining outlined in Algorithm 1.

``` Input :Learning rate \(_{1}\), weight decay \(_{1},_{2}\), prompt length \(N_{1},N_{2}\), number of tasks \(T_{1},T_{2}\), attention matrix initialization scale \(\).
1Initialize\(_{j}^{(0)}(^{d-1})\) (\(j[m]\)); \(b_{j}^{(0)}([-1,1])\) (\(j[m]\)); \(_{j,j}^{(0)}(\{\})\) (\(j[m]\)) and \(_{i,j}^{(0)}=0\) (\(i j[m]\)).
2Stage I: Gradient descent for MLP layer
3 Draw data \(\{(_{1}^{t},y_{1}^{t},,_{N_{1}}^{t},y_{N_{1} }^{t},^{t},y^{t})\}_{t=1}^{T_{1}}\)with prompt length \(N_{1}\). \(_{j}^{(1)}_{j}^{(0)}-_{1} _{_{j}^{(0)}}}_{t=1}^{T_{1}}(y^{t}-f( ^{t},^{t},^{t};^{(0)}, ^{(0)},^{(0)}))^{2}+_{1} _{j}^{(0)}\).
4Initialize\(b_{j}([- d, d])\).
5Stage II: Empirical risk minimization for attention layer
6 Draw data \(\{(_{1}^{t},y_{1}^{t},,_{N_{2}}^{t},y_{N_{2} }^{t},^{t},y^{t})\}_{t=T_{1}+1}^{T_{1}+T_{2}}\) with prompt length \(N_{2}\). \(^{*}_{}}_{t=T_{1}+1}^{T_{1}+T_{2}}(y^{t}-f(^{t},^{t},^{t};^{(1)},,))^{2}+ }{2}\|\|_{F}^{2}\). ```

**Output :**Prediction function \( f(,,; ^{(1)},^{*},)\).

**Algorithm 1**Gradient-based training of transformer with MLP layer

**Theorem 1**.: _Given Assumption 1 and \(r d^{}\). We pretrain the transformer (2.6) using Algorithm 1 with \(m=(r^{P}),T_{1}=(d^{Q+1}r^{Q})\), \(N_{1}T_{1}=(d^{2Q+1}r)\), \(}r^{}d^{Q}}\),\(_{1} m^{}rd^{2Q-}\). \(( d)^{-C_{}}\) for constant \(C_{}\). Then, for appropriately chosen regularization parameters \(_{1},_{2}>0\), with probability at least 0.99 over the data distribution and random initialization, the ICL prediction risk (2.1) with test prompt length \(N^{*}\) for the output \(f\) of Algorithm 1 can be upper bounded as_

\[_{N^{*}}(f)-=}{m}+r^{4P} }+}+}}.\]

Theorem 1 suggests that to achieve low in-context prediction risk, it is sufficient to set \(T_{1},N_{1}=(d^{(Q)})\), and \(m,T_{2},N_{2},N^{*}=(r^{(P)})\). Observe that the _pretraining_ complexity \(T_{1},N_{1}\) scales with the ambient dimensionality \(d\), but the _in-context_ sample complexity \(N^{*}\) only scales with the dimensionality of the function class \(r d\). This illustrates the in-context efficiency of pretrained transformers and aligns with our observations in the GPT-2 experiment reported in Figure 1.

**Remark 3**.: _We make the following remarks._

* _The sample complexity highlights different roles of the two sources of low dimensionality in our setting. The low dimensionality of single-index_ \(f_{*}\) _entails that the pretraining cost scales as_ \(N d^{(Q)}\)_, which is consistent with prior analyses on gradient-based feature learning_ _[_2_,_ 2_]__. On the other hand, the low dimensionality of function class (i.e.,_ \(\) _lies in_ \(r\)_-dimensionalsubspace) leads to an in-context sample complexity that scales as \(N^{*} r^{(P)}\), which, roughly speaking, is the rate achieved by polynomial regression or kernel models on \(r\)-dimensional data._
* _The multiplicative scaling between_ \(N_{1}\) _and_ \(T_{1}\) _in the sample complexity suggests that one can tradeoff between the two quantities, that is, pretraining on more diverse tasks (larger_ \(T_{1}\)_) can reduce the required pretraining context length_ \(N_{1}\)_._
* _Similar low-dimensional function class has been considered in the setting of transfer or multi-task learning with two-layer neural networks_ _[_14, CHS\({}^{+}\)23_]__, where the first-layer weights identify the span of all target functions, and the second-layer approximates the nonlinearity. However, the crucial difference is that we do not update the network parameters based on the in-context examples; instead, the single-index learning is implemented by the forward pass of the transformer._

Comparison against baseline methods.Below we summarize the statistical complexity of commonly-used estimators that only have access to \(N^{*}\) in-context examples, but not the pretraining data. Note that the in-context sample complexity all depends on the ambient dimensionality \(d r\).

* **Kernel models.** Recall that our target function is a degree-\(P\) polynomial, and hence kernel ridge regression requires \(N^{*} d^{P}\) in-context examples .
* **CSQ learners.** The correlational statistical query (CSQ) lower bound suggests that an algorithm making use of correlational information requires \(N^{*} d^{(Q)}\) samples to learn a single-index model with information exponent \(Q\). This sample complexity can be achieved by online SGD training of shallow neural network .
* **Information theoretic limit.** Since the single-index model (1.1) contains \(d\) unknown parameters, we can infer an information theoretic lower bound of \(N^{*} d\) samples to estimate this function. This complexity can be achieved (up to polylog factors) by tailored SQ algorithms  or modified gradient-base training of neural network .

### Proof Sketch of Main Theorem

We provide a sketch of derivation for Theorem 1. The essential mechanism is outlined as follows: after training the MLP embedding via one gradient descent step, the MLP parameters \(\{_{j}^{(1)}\}\) align with the common subspace of the target functions \(\). Subsequently, the (linear) attention module estimates the input-output relation \(f_{}\) (which varies across tasks) on this \(r\)-dimensional subspace. We explain these two ingredients in the ensuing sections.

#### 3.2.1 Training the MLP Layer

We first show that the first gradient descent step on \(\) results in significant alignment with the target subspace \(\), using a proof strategy pioneered in . Note that under sufficiently small initialization and appropriately chosen weight decay, we have

\[_{j}^{(1)}_{1}}_{t=1}^{T_{1}}y^{t} _{_{j}^{(0)}}f(^{t},^{t},^{t};^{(0)},^{(0)},^{(0)}).\]

The key observation is that this correlation between \(y\) and the gradient of model output contains information of \(\). We establish the concentration of this empirical gradient around the expected (population) one, which determines the required rates of \(T_{1}\) and \(N_{1}\). For the population gradient, we make use of the Hermite expansion of \(_{b_{j}}(z)=(z+b_{j})\) and show that its leading term is proportional to \(_{1:r}^{(0)}\\ _{d-r}\), which is contained in the target subspace \(\); see Appendix B for details.

#### 3.2.2 Attention Matrix with Good Approximation Property

Next we construct the attention matrix \(}\) which satisfies the following approximation property:

**Proposition 2** (Informal).: _There exists \(}\) such that with high probability,_

\[|f(^{t},^{t},^{t};^{(1)},},)-y^{t}|-=\!(/m+r^{2P}/N_{2}})\]

_for all \(t\{T_{1}+1,,T_{2}\}\). Moreover, we have \(\|}\|_{F}=\!(r^{2P}/m)\)._We provide an intuitive explanation of this construction. Recall that the target function can be written in its Hermite expansion \(f_{*}()=_{i=Q}^{P}}{}_{i}(,)\). We build an orthonormal basis for \(f_{*}\) as follows. Let \(\{_{1},,_{r}\}\) be an orthonormal basis of \(\). Then, any \(f_{*}\) can be expressed as a linear combination of functions in \(=\{_{j=1}^{r}_{p_{j}}_{j},  Q p_{1}++p_{r} P,p_{1} 0,,p_{r}  0\}\), where \(_{(0,I_{d})}[h()h^{}()]= _{h=h^{}}\) holds for \(h,h^{}\). We write \(=\{h_{1},,h_{B_{P}}\}\) with \(B_{P}=||=(r^{P})\), and observe that a two-layer neural network can be constructed to approximate each \(h_{n}\). Specifically, there exist vectors \(^{1},,^{B_{P}}^{m}\) such that \(_{j=1}^{m}a_{j}^{n}_{j}^{(1)},+b_{ j} h_{n}()\) for each \(n[B_{P}]\).

Consequently, we can build the desired attention matrix using coefficients \(^{1},,^{B_{P}}\). Let \(=^{1}^{B_{P}}^{m  B_{P}}\) and \(}=^{}\), the attention module can recover the true function as

\[}}(^{(1)} ^{t}+_{N_{2}}^{})^{t},(^{(1)}+ )\] \[=_{n=1}^{B_{P}}}_{i=1}^{N_{2}} _{j=1}^{m}_{j}^{n}_{j}^{(1)},_{i}+b_{j}y_{i}_{j=1}^{m}_{j}^{ n}_{j}^{(1)},+b_{j}\] \[}{}_{n=1}^{B_{P}}}_{i=1}^{N_{2}}h_{n}(_{i})y_{i}h_{n}()}{}_{n=1}^{B_{P}}[h_{n}()f_{*}()]h_{n}( )=f_{*}().\] (3.1)

Roughly speaking, the self-attention architecture computes the correlation between the target function and basis element \(h_{i}\) to estimate the corresponding coefficient in this basis decomposition. We evaluate (a) the approximation error of two-layer neural network in Appendix C.1, and (b) the discrepancy between the empirical and true correlations in Appendix C.2.

Note that the approximation errors and the norm of \(}\) at this stage scale only with \(r\) up to polylogarithmic terms; this is because \(^{(1)}\) already identifies the low-dimensional target subspace \(\). Consequently, the in-context sample size \(N^{*}\) only scales with the target dimensionality \(r d\).

#### 3.2.3 Generalization Error Analysis

Finally, we transfer the learning guarantee from the constructed \(}\) to the (regularized) empirical risk minimization solution \(^{*}\). By the equivalence between optimization with \(L_{2}\) regularization and norm-constrained optimization, there exists \(_{2}\) such that \(\|^{*}\|_{F}\|}\|_{F}\) and the empirical ICL loss by \(^{*}\) is no larger than that of \(}\). Hence, we can bound the generalization error by \(^{*}\) using a standard Rademacher complexity bound for norm-constrained transformers provided in Appendix D.1. One caveat here is that the context length \(N^{*}\) at test time may differ from training time; hence we establish a context length-free generalization bound, which is discussed in Appendix D.2.

## 4 Synthetic Experiments

### Experimental Setting

We pretrain a GPT-2 model  to learn the Gaussian single-index function class (1.1). Specifically, we consider the 12-layer architecture (with 22.3M parameters) used in  for in-context linear regression. The pretraining data is generated from random single-index models: for each task \(t\), the context \(\{(_{i}^{t},y_{i}^{t})\}_{i=1}^{N}\) is generated as \(_{i}^{t}}{}(0,_{d})\) and \(y_{i}^{t}=_{i=Q}^{P}^{t}}{}_{i}(_{i}^{t},^{t})\), where \((c_{Q}^{t}, c_{P})}{}(c_{Q}, c_{P})_{i=Q}^{P}^{2}}{!}=1}\) and \(^{t}}{}| =[_{1},,_{r},0,,0]^{}|,\|\|=1}\). See Appendix F for further details.

### Empirical Findings

Ambient dimension-free sample complexity.In Figure 2 we examine how the in-context sample complexity of the GPT-2 model depends on the ambient dimensionality \(d\) and the function class dimensionality \(r\). For each problem setting the model is pretrained for \(100,000\) steps using the data of degree \(P=4\) and information exponent \(Q=2\) (see Appendix F for details). In Figure 2(a) we observe that for fixed \(r=8\), varying the ambient dimensionality \(d=16,32,64\) leads to negligible change in the model performance for the in-context phase. In contrast, Figure 2(b) illustrates that for fixed \(d\), the required sample size \(N^{*}\) scales with the dimensionality of the function class \(r=2,4,8\). This confirms our theoretical finding that transformers can adapt to low-dimensional structure of the distribution of target functions via gradient-based pretraining.

Superiority over baseline algorithms.In Figure 1 we compare the in-context sample complexity of the GPT-2 model pretrained by data of \(Q=3\) and \(P=2\) against two baseline algorithms that directly learn \(f_{*}\) on the test prompt: \((i)\) kernel ridge regression with the Gaussian RBF kernel \(k(,^{})=-\|-^{}\|^{2}/^{2} \), and \((ii)\) two-layer neural network with ReLU activation \(f_{}()=_{i=1}^{m}a_{i}(,_{i})\) trained by the Adam optimizer . We observe that for \(r=8,d=16,32\), the pretrained transformer outperforms both KRR and two-layer NN; moreover, the performance of these two baseline algorithms deteriorates significantly as the ambient dimensionality \(d\) becomes larger.

## 5 Conclusion and Future Direction

We study the complexity of in-context learning for the Gaussian single-index models using a pre-trained transformer with nonlinear MLP layer. We provide an end-to-end analysis of gradient-based pretraining and establish a generalization error bound that takes into account the number of pretraining tasks, the number of pretraining and in-context examples, and the network width. Our analysis suggests that when the distribution of target functions exhibits low-dimensional structure, transformers can identify and adapt to such structure during pretraining, whereas any algorithm that only has access to the test prompt necessarily requires a larger sample complexity.

We outline a few limitations and possible future directions.

* The in-context sample complexity we derived \(r^{(P)}\) corresponds to that of polynomial regression or kernel methods in \(r\)-dimensional space. This rate is natural as discussed in Section 3.2.2, where the linear self-attention module extracts the coefficients with respect to fixed basis functions (of size \(r^{(P)}\)). An interesting question is whether transformers can implement a more efficient in-context algorithm that matches the complexity of gradient-based feature learning in \(r\) dimensions. This can be achieved if the pretrained model learns features in-context.
* Our pretraining complexity is based on one GD step analysis similar to , BES\({}^{+}\) which makes use of the correlational information; hence the information exponent of the link functions plays an important role. We conjecture that the sample complexity can be improved if we modify the pretraining procedure or training objective, as done in .