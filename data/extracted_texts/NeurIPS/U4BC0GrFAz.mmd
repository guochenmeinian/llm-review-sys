# Do causal predictors generalize better to new domains?

Vivian Y. Nastl

Max Planck Institute for Intelligent Systems, Tubingen, Germany and Tubingen AI Center

Max Planck ETH Center for Learning Systems

vivian.nastl@tuebingen.mpg.de

Moritz Hardt

Max Planck Institute for Intelligent Systems, Tubingen, Germany and Tubingen AI Center

hardt@is.mpg.de

###### Abstract

We study how well machine learning models trained on causal features generalize across domains. We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. For each prediction task, we select features that have a causal influence on the target of prediction. Our goal is to test the hypothesis that models trained on causal features generalize better across domains. Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features. Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features. In addition, we show that recent causal machine learning methods for domain generalization do not perform better in our evaluation than standard predictors trained on the set of causal features. Likewise, causal discovery algorithms either fail to run or select causal variables that perform no better than our selection. Extensive robustness checks confirm that our findings are stable under variable misclassification.

## 1 Introduction

The accuracy of machine learning models typically drops significantly when a model trained in one domain is evaluated in another. This empirical fact is the fruit of numerous studies (Torralba and Efros, 2011; Gulrajani and Lopez-Paz, 2020; Miller et al., 2021). But it's less clear what to do about it. Many machine learning researchers see hope in causal modeling. Causal relationships, the story goes, reflect stable mechanisms invariant to changes in an environment. Models that utilize these invariant mechanisms should therefore generalize well to new domains (Peters et al., 2017). The idea may be sound in theory. Intriguing theoretical results carve out assumptions under which causal machine learning methods generalize gracefully from one domain to the other (Heinze-Deml et al., 2018; Meinshausen, 2018; Scholkopf et al., 2021; Pearl and Bareinboim, 2022; Subbaswamy et al., 2022; Wang et al., 2022).

These theoretical developments have fueled optimism about the out-of-domain generalization abilities of causal machine learning. The general sentiment is that causal methods enjoy greater external validity than kitchen-sink model fitting. In this work, we put the theorized external validity of causal machine learning to an empirical test in a wide range of concrete datasets.

Our results.We consider 16 prediction tasks on tabular datasets from prior work (Ding et al., 2021; Hardt and Kim, 2023; Gardner et al., 2023) covering application settings including health, employment, education, social benefits, and politics. Each datasets comes with different domains intended for research on domain generalization. For each task we conservatively select a set of _causal features_. Causal features are those that we most strongly believe have a causal influence on the target of prediction. We also select a more inclusive set of _arguably causal_ variables that include variables that may be considered causal depending on modeling choices. For each task, we compare the performance of machine learning methods trained on causal variables and arguably causal variables with those trained on all available features. In all 16 tasks, our primary finding can be summarized as:

_Predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features_.

Across 16 datasets, we were unable to find a single example where causal predictors generalize better to new domains than a standard machine learning model trained on all available features. Figure 1 summarizes the situation. In greater detail, our empirical results are:

* Using all features Pareto-dominates both causal selections, with respect to in-domain and out-of-domain accuracy (up to error bars). We provide a closer look at the Pareto-frontiers of four representative tasks in Figure 2.
* The inclusive selection of arguably causal features Pareto-dominates the conservative selection of causal features, with respect to in-domain and out-of-domain accuracy (up to error bars).
* The absolute drop in accuracy from one domain to the other is smaller for all features than for causal features.
* Adding anti-causal features--i.e., features caused by the target variable--to the set of causal features improves out-of-domain performance.
* Special-purpose causal machine learning methods, such as IRM and REx, typically perform within the range of standard models trained on the conservative and inclusive selection of causal features.
* Classic causal discovery algorithms, like PC and ICP, do not provide causal parent estimates that improve upon the inclusive selection of causal features.
* Extensive robustness checks confirm that our findings are stable under misclassifications of single features.

Figure 1: Best out-of-domain accuracy (left) and corresponding shift gap (right) by feature selection. Predictors based on all features have better out-of-domain accuracy than predictors using causal feature selections. Their shift gap is smaller too, up to error bars.

To be sure, our findings don't contradict the theory. Rather, they point at the fact that the assumptions of existing theoretical work are unlikely to be met in the tabular data settings we study. It is, of course, always possible that those causal prediction techniques yield better results on other datasets. From this perspective, our study suggests that the burden of proof is on proponents of causal techniques to provide real benchmark datasets where these methods succeed. On the many datasets we investigated, it proved infeasible to make use of causal techniques for better out-of-domain generalization.

Figure 2: (Left) Pareto-frontiers of in-domain and out-of-domain accuracy by feature selection. (Right) Pareto-frontiers of shift gap and out-of-domain accuracy by feature selection. Predictors using all features Pareto-dominate predictors using causal features, with respect to in-domain and out-of-domain accuracy. Other tasks are in Appendix C.

### Related work

Existing work in causal machine learning relies on the assumption of the invariance of causal mechanisms (Havelmo, 1944; Aldrich, 1989; Hoover, 1990; Pearl, 2009; Scholkopf et al., 2012). The conditional distribution of the target, given the complete set of its direct causal parents, shall remain identical under interventions on variables other than the target itself. In their influential work, Peters et al. (2016) utilize this invariance property for causal discovery. In further works, it is extended to non-linear models (Heinze-Deml et al., 2018), and discovery of invariant features (Rojas-Carulla et al., 2018). To overcome the computational burden in high-dimensional settings, Arjovsky et al. (2019) propose Invariant Risk Minimization (IRM), which learns an invariant representation of the features instead of selecting individual features. Rosenfeld et al. (2021) however identify major failure cases of IRM. In response, multiple extensions of IRM have been proposed (Krueger et al., 2021; Wang et al., 2022; Ahuja et al., 2022; Jiang and Veitch, 2022; Chen et al., 2023). Another line of research assumes graphical knowledge to remove variables or apply independence constraints for regularization (Subsawamy and Saria, 2018; Subsawamy et al., 2019; Kaur et al., 2022; Salaudeen and Koyejo, 2024). We refer the reader to Kaddour et al. (2022) for an overview. Aside from causal learning approaches, various domain generalization algorithms and distributional robustness methods have been developed (Ajakan et al., 2015; Sun et al., 2015; Sun and Saenko, 2016; Li et al., 2018; Levy et al., 2020; Sagawa et al., 2020; Xu et al., 2020; Zhang et al., 2021). Each method assumes a unique type of (untestable) invariance across domains.

Gulrajani and Lopez-Paz (2020) conduct extensive experiments on image datasets to compare the performance of domain generalization algorithms, including the causal methods IRM and Risk Extrapolation (REx) (Krueger et al., 2021), in realistic settings. They find that no domain generalization methods systematically outperforms empirical risk minimization. Recently, Gardner et al. (2023) demonstrate a similar behavior for tabular data.

In our work, we shift the focus from the out-of-domain performance of specific causal machine learning _algorithms_ to the performance of causal _feature sets_.

### Theoretical background and motivation

To frame our empirical study, we recall some relevant theoretical background first. A _domain_\(\) is composed of samples \((x_{i},y_{i}) P\), where \(x_{i}^{p}\) are the features and \(y\) is the target (Wang et al., 2022). Let \(X\) and \(Y\) denote the random variables corresponding to the features and the target.

We are given \(m\) training domains \(^{}=\{^{d}:d=1,,m\}\). The joint distributions of features and target differ across domains, i.e. \(P^{d} P^{e}\) for \(d e\). Our goal is to learn a prediction \(f_{}\) from the training domains \(^{}\) that achieves minimum prediction error on an _unseen_ test domain \(^{}\),

\[^{*}=*{arg\,min}_{}_{P^{}}[ (Y,f_{}(X))],\] (1)

where \((,)\) is some loss function. We can compose the objective into two parts

\[_{P^{}}[(Y,f_{}(X))]-,\] (2)

where \(=_{P^{}}[(Y,f_{}(X))]-_{P^{ }}[(Y,f_{}(X))]\) is the _shift gap_. Hence, we aim to learn a classifier with the best trade-off between predicting accurately and having a low shift gap. In our empirical work, we measure the shift gap as the difference in accuracy,

\[_{}=(f_{},^{})-(f_{},^{}).\] (3)

Distributional robustness of causal mechanisms.Suppose we have a directed acyclic graph \(G=(V,E)\) with nodes \(V=\{1,,q\}\), a random variable \((Z,Y)\) and noise variables \(^{q}\). A common assumption is that the target is described by the prediction \(f_{}\) via the coefficient \(^{}\)

\[Y f_{^{}}(Z)+_{q}\,.\] (4)

The invariance of the causal mechanism implies that these causal coefficients provide the robust estimator for the set of do-interventional distributions on the features (Meinshausen, 2018),

\[^{}=*{arg\,min}_{}_{Q^{}}E_{Q}[(Y,f_{}(Z))], ^{}:=\{P^{}_{a,V\{q\}};a ^{q-1}\}.\] (5)To link to domain generalization, we need to assume that all causal parents of \(Y\) are included in the feature set \(X\). We set \(X=Z\) w.l.o.g. We also presume that the distribution of the testing domain is a do-intervention on the features, i.e., \(P^{}^{}\). Intuitively, this postulates that the causal mechanism generating \(Y\) stays the same across domains, while features may encounter arbitrarily large interventions.

Then, the prediction error of the causal coefficients in the test domain is minimax optimal bounded,

\[_{P^{}}[(Y,f_{^{}}(X))]_{ }_{Q^{}}_{Q}[(Y,f_ {}(X))]\,.\] (6)

Recent work in causal machine learning already pointed out that the minimum prediction error on test domains with mild interventions can be much smaller that the prediction error achieved by the causal coefficients (Rothenhausler et al., 2020; Subbaswamy et al., 2022). We conduct synthetic experiments similar to Rothenhausler et al. (2020), further supporting the insights that that a strong shift is needed before causal features achieve best out-of-domain accuracy. The details on the setup and results of the synthetic experiments are provided in Appendix D.

Our empirical study complements these theoretical developments, as we evaluate domain generalization abilities of causal features in typical tabular datasets. We emphasize that we do not challenge the validity of causal theory like (6), but rather challenge how realistic their assumptions are.

## 2 Methodology

We conduct experiments on 16 classification tasks with natural domain shifts. They cover applications in multiple application areas, e.g., health, employment, education, social benefits, and politics. Most tasks are derived from the distribution shift benchmark for tabular data _TableShift_(Gardner et al., 2023). Some others are from prior work (Hardt and Kim, 2023). All tabular datasets contain interpretable personal information, e.g., age, education status, or individual's habits. Therefore, we can consult social and biomedical research on the causal relationships between features and target. To reflect existing epistemic uncertainty, we propose a pragmatic scheme to classify the relationship between features and target.

We term features that clearly have a causal influence on the target **causal**. We are conservative and only label features as causal when: (1) The feature has almost certainly a causal effect on the target, and (2) reverse causation from target to feature is hard to argue. We sort out any spuriously related or possibly anti-causal feature (Scholkopf et al., 2012). However, we risk excluding relevant causal parents of the target.

For this reason, we propose the concept of **arguably causal features**. It is epistemically uncertain how these features are causally linked to the target. To be specific, we term a feature arguably causal when it suffices one of the following criteria: (1) The feature is a causal feature, or (2) the feature has a causal effect on the target and reverse causation is possible, or (3) it is plausible but not certain that the feature has a causal effect on the target. We exclude variables where it is implausible that they affect the target. Ideally, the arguably causal features cover all causal parents present in the dataset. We emphasize that both, causal features and arguably causal features, are merely approximations of the true causal parents based on current expert knowledge and restricted to available features. We further note that relationships between causal features and target might be confounded.

In some datasets and tasks we are also confronted with features that are plausibly **anti-causal**, that is: (1) The target has almost certainly a causal effect on the feature, and (2) a reverse causation from feature to target is hard to argue.

We apply this scheme to the features of every task, after seeking advice from current research, governmental institutions and a medical practitioner. We describe the selection procedure for diagnosing diabetes in the following, and give more examples in Appendix A. Details on the feature selections of all tasks are provided in Appendix E.

### Example: Variables in diabetes classification

The task is to classify whether a person is diagnosed with diabetes (Gardner et al., 2023). The domains are defined by the preferred race of the individuals. We illustrate the feature grouping in Figure 3.

Causal features. Socio-economic status, in particular education level and former smoking, are widely acknowledged risk factors for diabetes (Brown et al., 2004; Agardh et al., 2011; Madduta et al., 2017; Centers for Disease Control and Prevention, 2024b). Recent research in health care found evidence that an individual's sex impacts their diabetes diagnosis, e.g., pregnancies unmask pre-existing metabolic abnormalities in female individuals (Kautzky-Willer et al., 2023).1 We also include marital status as a causal feature, as recent research showed that marital stress adversely affects the risk of developing diabetes (Whisman et al., 2014).

Arguably causal features. The individual's lifestyle, health and socio-economic status impacts their risk to develop diabetes, i.e., obesity, current smoking, healthy food, alcohol consumption, physical activities, mental health and utilization of health care services (Lindstrom et al., 2003; Brown et al., 2004; Engum, 2007; Baliunas et al., 2009; Agardh et al., 2011; Madduta et al., 2017; Centers for Disease Control and Prevention, 2024b; Klein et al., 2022; Centers for Disease Control and Prevention, 2024a). At the same time, a person with diabetes is incentivized to improve their behavior to control their blood sugar and improve insulin sensitivity (Klein et al., 2004). They are also more at risk to increase their weight due to the insulin therapy (McFarlane, 2009), experience distress (Centers for Disease Control and Prevention, 2024c), and have limited economic opportunities (American Diabetes Association, 2011). Because of these bidirectional relationships, we regard features encoding these behaviors as arguably causal.

Anti-causal features. Researchers found evidence that diabetes increases the risk of hypertension, high blood cholesterol, coronary heart disease, myocardial infarction and strokes (Petrie et al., 2018; Schofield et al., 2016). Due to treatment costs of diabetes, affected individuals are encouraged to obtain a health insurance (National Institute of Diabetes and Digestive and Kidney Diseases, 2019). Therefore, we regard the current health care coverage as anti-causal to diabetes.

### Tasks and datasets

We consider 16 classification tasks, listed in Table 1. The data is collected from a multitude of sources. We build on 14 classification tasks with natural domain shifts proposed in _TableShift_. We use the _TableShift_ Python API to preprocess and transform raw public forms of the data.2 In addition, we conduct experiments on two established classification tasks (MEPS, SIPP). Data preprocessing is adapted from Hardt and Kim (2023). Further details on the tasks and their distribution shifts are in Appendix E.

### Machine learning algorithms

In our experiments, we evaluate multiple machine learning algorithms. We list them in the following.

Figure 3: An example grouping for the task ‘Diabetes’.

Baseline and tabular methods.We include tree ensemble methods: XGBoost (Chen and Guestrin, 2016), LightGBM (Ke et al., 2017) and histogram-based GBM. We also evaluate multilayer perceptrons (MLP) and state-of-the-art deep learning methods for tabular data: SAINT (Sompealli et al., 2021), TabTransformer (Huang et al., 2020), NODE (Popov et al., 2019), FT Transformer (Gorishniy et al., 2021) and tabular ResNet (Gorishniy et al., 2021).

Domain robustness and generalization methods.We consider distributionally robust optimization (DRO) (Levy et al., 2020), Group DRO (Sagawa et al., 2020) using domains and labels as groups, respectively, and the adversarial label robustness method by Zhang et al. (2021). We also include Domain-Adversarial Neural Networks (DANN) (Ajakan et al., 2015), Deep CORAL (Sun and Saenko, 2016), Domain MixUp (Xu et al., 2020) and MMD (Li et al., 2018).

Causal methods.We assess Invariant Risk Minimization (IRM) (Arjovsky et al., 2019), Risk Extrapolation (REx) (Krueger et al., 2021), Information Bottleneck IRM (IB-IRM) (Ahuja et al., 2022), AND-Mask (Parascandolo et al., 2021) and CausIRL (Chevalley et al., 2022).

Domain generalization and causal methods require at least two training domains with a sufficient number of data points. This is provided in eight of our tasks. Detailed descriptions of the machine learning algorithms and hyperparameter choices are given in Appendix B and Gardner et al. (2023).

### Experimental procedure

We conduct the following procedure for each task. First, we define up to four sets of features based on expert knowledge: all features, causal features, arguably causal features and anti-causal features. Second, we split the full dataset into in-domain set and out-of-domain set. We adopt the choice of domains from Gardner et al. (2023). We have a train/test/validation split within the in-domain set, and a test/validation split within the out-of-domain set. For each feature set:

1. We apply the machine learning methods listed in Section 2.3. For each method: 1. We conduct a hyperparameter sweep using HyperOpt (Bergstra et al., 2013) on the in-domain validation data. A method is tuned for 50 trials. We exclusively train on the training set. 2. The trained classifiers are evaluated on in-domain and out-of-domain test set. 3. We select the best model according to their in-domain validation accuracy. This follows the selection procedure in previous work (e.g., (Gulrajani and Lopez-Paz, 2020, Gardner et al., 2023)). To ensure compatibility with _TableShift_, we add the best in-domain and

   Task & Data Source & \#Features & \#Arg. causal & \#Causal & \#Anti-causal \\  Food Stamps & ACS & 28 & 25 & 12 & - \\ Income & ACS & 23 & 15 & 4 & 3 \\ Public Coverage & ACS & 19 & 16 & 8 & - \\ Unemployment & ACS & 26 & 21 & 11 & 3 \\ Voting & ANES & 54 & 36 & 8 & - \\ Diabetes & BRFSS & 25 & 17 & 4 & 6 \\ Hypertension & BRFSS & 18 & 14 & 5 & 2 \\ College Scorecard & ED & 118 & 34 & 11 & - \\ ASSISTments & Kaggle & 15 & 13 & 9 & - \\ Stay in ICU & MIMIC-iii & 7491 & 1445 & 5 & - \\ Hospital Mortality & MIMIC-iii & 7491 & 1445 & 5 & - \\ Hospital readmission & UCI & 46 & 42 & 5 & - \\ Childhood Lead & NHANES & 7 & 6 & 5 & - \\ Sepsis & PhysioNet & 40 & 39 & 5 & - \\ Utilization & MEPS & 218 & 129 & 20 & - \\ Poverty & SIPP & 54 & 43 & 15 & 6 \\   

Table 1: Description of tasks, data sources and number of features in each selection. Details and licenses are provided in Appendix E.

out-of-domain accuracy pair observed by (Gardner et al., 2023). We restrict our further analysis to this selection.
2. We find the Pareto-set \(\) of in-domain and out-of-domain accuracy pairs. We compute the shift gaps, and find the Pareto-set of shift gap and out-of-domain accuracy of the set \(\).

We provide further details and illustrations of the individual steps in Appendix B.

## 3 Empirical results

In this section, we present and discuss the results of the experiments on all 16 tasks. A total of 42K models were trained for the main results and an additional 468K models for robustness tests. Our code is based on Gardner et al. (2023); Hardt and Kim (2023) and Gulrajani and Lopez-Paz (2020). It is available at https://github.com/socialfoundations/causal-features.

In our experiments, we analyze the performance of feature selections based on domain-knowledge causal relations. A summary of the results is shown in Figure 1. Details on four representative tasks are given in Figure 2. The other tasks are in Appendix C. The accuracy results are presented along with 95% Clopper-Pearson intervals. They are the baseline for the approximate 95% confidence intervals of the shift gap. See Appendix B for the exact computation and justification of the confidence intervals.

In-domain and out-of-domain accuracy.Models trained on the whole feature set accomplish the highest in-domain and out-of-domain accuracy, up to error bars (16/16 tasks). The arguably causal features Pareto-dominate the causal features, up to error bars (16/16 tasks). Recall that arguably causal features are a superset of the causal features, and have considerably more features (Table 1). Models based on causal features often essentially predict the majority label (7/16 tasks).

Shift gap.The shift gap measures the absolute performance drop of the feature sets when employed out-of-domain. All features often experience a significantly smaller shift gap than causal features (7/16 tasks). The causal features solely surpass all features (within the error bounds) for the task 'Hospital Mortality' by predicting the majority label. In most cases, the shift gaps of all features and arguably causal features are indistinguishable (15/16 tasks).

Anti-causal features.In five tasks, we have features that we regard as anti-causal. Results are shown in Figure 4 and Appendix C.2. The anti-causal features do not perform significantly different from the constant predictor in-domain (5/5 tasks). However, they sometimes perform extremely poor out-of-domain (2/5 tasks). It is therefore astounding that the out-of-domain performance of the (arguably) causal features is improved by adding anti-causal features (5/5 tasks).

Figure 4: (Left) Pareto-frontiers of in-domain and out-of-domain accuracy by feature selection. (Right) Pareto-frontiers of shift gap and out-of-domain accuracy accomplished. Adding anti-causal features improves out-of-domain accuracy. Results of other tasks in Appendix C.

Causal machine learning methods.We restrict ourselves to the Pareto-set of the standard models for each feature set and compare them to causal methods.3 We showcase a representative performance in Figure 5. Details are in Appendix C.3. The causal methods do not improve upon the arguably causal features trained on standard models (8/8 tasks). In fact, their performance typically spans between the causal features and arguably causal methods trained on standard models. The in-domain and out-of-domain accuracy is even indistinguishable from the causal selections in multiple cases (IRM: 3/8, REx: 4/8, IB-IRM: 2/8, CausIRL: 5/8, AND-Mask: 5/8 tasks). Possible explanations are: (1) the causal methods manage to extract a causal representation of the features similar to our selection of causal and/or arguably causal features; or (2) it is an artifact of having low predictive power.

Causal discovery algorithms.We apply invariant causal prediction (ICP) (Peters et al., 2016) and classic causal discovery algorithm, Peter-Clark (PC) algorithm (Spirtes et al., 2000) and Fast causal inference (FCI) algorithm (Spirtes et al., 1995), to our tasks. See Appendix C.4 for the results. The algorithms rarely outputs any causal parents (ICP: 1/6, PC: 4/14, FCI: 1/14 tasks). When they do, they select very few features as causal parents. Some of them are features we also regard as causal based on domain knowledge, others anti-causal or without causal relations to the target. For example, PC outputs an individual's occupation and the number of weeks worked in the last 12 months as causal parents of unemployment. While we agree that the occupation has a causal influence on unemployment, we view the amount of weeks an individual worked as a _result_ of their unemployment rather than the _reason_. The causal parents estimated by the causal discovery algorithms often perform similar to the causal features though (ICP: 1/1, PC: 1/4, FCI: 1/1 tasks). Note that their performance is always Pareto-dominated by our arguably causal features (ICP: 1/1, PC: 4/4, FCI: 1/1 tasks). Therefore, whichever feature selection one choose to believe, ours or causal discovery algorithms', one never improves upon the whole feature set.

Robustness tests.Results are in Appendix C.1, C.5 and C.6. We test whether our conclusions are sensitive to misclassifying one feature. Therefore, we form subsets of the set of causal features by removing one feature at a time. The test subsets do not achieve higher out-of-domain accuracy than using all features, with one exception in the task 'ASSISTments'. We find that the supersets of arguably causal features with one additional features obtain similar or better out-of-domain accuracy. We randomly sample 500 feature subsets for each task and check whether any subset significantly outperforms the whole feature set. None of the sampled subset does, with few outliers in the task 'ASSISTments'. We consider the divergent task in detail. The task is about predicting whether a student answers a question correct. Surprisingly, all outperforming random subsets and the subset from the misclassification tests coincide in one regard: _missing_ the feature encoding the tested skill, e.g., rounding. We encourage further work to explain this oddity, as the tested skill of a task

Figure 5: (Left) Pareto-frontiers of in-domain and out-of-domain accuracy of causal methods and domain-knowledge features selection. (Right) Pareto-frontiers of shift gap and out-of-domain accuracy attained. The performance of the causal methods interpolates between the performance of the causal and/or arguably causal features. Results of remaining tasks are in Appendix C.

clearly has a causal influence. We also provide insights into which non-causal features improve the out-of-domain performance, and discuss potential explanations.

Our findings remain valid when using balanced accuracy as a metric.

## 4 Discussion and limitations

Our findings may not come as a surprise to everyone. Unlike causal machine learning researchers, social scientists generally see no reason to believe in the universality of causal relationships. For example, smaller classroom sizes may cause better teaching outcomes in Tennessee (Mosteller, 1995), but much less so in California (Jepsen and Rivkin, 2009). Such variation is the rule rather than the exception. Indeed, philosopher of science Cartwright (1999, 2007) argued that causal regularities are often more narrowly scoped than commonly held.

Our study mirrors these robust facts in a machine learning context. In the many common tabular datasets we consider, we find no evidence that causal predictors have greater external validity than their conventional counterparts. If the goal is to generalize to new domains in these datasets, our findings suggest we might as well train the best possible model on all available features. The one exception to the rule we found is the case of the _skill_ variable in the Kaggle ASSISTments task. It appears as though removing this variable increases out-of-domain generalization. Curiously, the variable is also almost certainly one of the better examples of a _causal_ variable in our study. Removing it therefore gives no advantage to causal predictors. For all tasks, we used available research and our own knowledge to classify variables as causal and arguably causal. We likely made some mistakes in this classification. This is why we extended our study with extensive robustness checks that confirm our findings. In addition, we did not find any relief in state-of-the-art causal methods, or causal discovery algorithms.

Demonstrating the utility of causal methods therefore likely requires other benchmark datasets than the ones currently available. We consider this a promising avenue for future work that derives further motivation from our work. We point to two classification tasks, where recent research suggests that causal prediction methods have utility for better domain generalization (Schulam and Saria, 2017; Subbaswamy and Saria, 2019): predicting the probability pneumonia mortality outside the hospitals (Cooper et al., 1997; Caruana et al., 2015), and hospital mortality across changes in the clinical information system (Nestor et al., 2019). We refer to Appendix A for details. Another direction for future research is to evaluate to which extent our findings generalize to other applications and data modalities. Recent advances in causal machine learning suggest, for example, promising results in real-world image datasets for classifying wild animals (Terra Incognita) and urban vs. non-urban examples (Spurious PACS), see (Salaudeen and Koyejo, 2024).

In light of our results, it's worth finding theoretical explanations for why using all features, regardless of causality, has the best performance in typical tabular datasets. In this vein, Rosenfeld et al. (2021) point to settings where risk minimization is the right thing to do in theory. We seed the search for additional theoretical explanations with a simple observation: If all domains are positive reweightings of one another, then the Bayes optimal predictor with respect to classification error in one domain is also Bayes optimal in any other domain. Standard models, such as gradient boosting or random forests, often achieve near optimal performance on tabular data with a relatively small number of features. In such cases, our simple observation applies and motivates a common sense heuristic: Do the best you can to approximate the optimal predictor on all available features.