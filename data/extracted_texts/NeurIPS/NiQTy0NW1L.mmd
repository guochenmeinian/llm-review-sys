# Lexinvariant Language Models

Qian Huang\({}^{1}\)qhwang@cs.stanford.edu

&Eric Zelikman\({}^{1}\)ezelikman@cs.stanford.edu

&Sarah Li Chen\({}^{1}\)sachen@stanford.edu

&Yuhuai Wu\({}^{12}\)yuhuai@cs.stanford.edu&Gregory Valiant\({}^{1}\)gvaliant@cs.stanford.edu

&Percy Liang\({}^{1}\)pliang@cs.stanford.edu

\({}^{1}\)Stanford University

\({}^{2}\)Google Research

###### Abstract

Token embeddings, a mapping from discrete lexical symbols to continuous vectors, are at the heart of any language model (LM). However, lexical symbol meanings can also be determined and even redefined by their structural role in a long context. In this paper, we ask: is it possible for a language model to be performant without _any_ fixed token embeddings? Such a language model would have to rely entirely on the co-occurence and repetition of tokens in the context rather than the _a priori_ identity of any token. To answer this, we study _leximvariant_ language models that are invariant to lexical symbols and therefore do not need fixed token embeddings in practice. First, we prove that we can construct a leximvariant LM to converge to the true language model at a uniform rate that is polynomial in terms of the context length, with a constant factor that is sublinear in the vocabulary size. Second, to build a leximvariant LM, we simply encode tokens using random Gaussian vectors, such that each token maps to the same representation within each sequence but different representations across sequences. Empirically, we demonstrate that it can indeed attain perplexity comparable to that of a standard language model, given a sufficiently long context. We further explore two properties of the leximvariant language models: First, given text generated from a substitution cipher of English, it implicitly implements Bayesian in-context deciphering and infers the mapping to the underlying real tokens with high accuracy. Second, it has on average 4X better accuracy over synthetic in-context reasoning tasks. Finally, we discuss regularizing standard language models towards lexim invariance and potential practical applications.

## 1 Introduction

All language processing systems rely on a stable lexicon, which assumes that a token (a word or subword such as _tree_) has a consistent contribution to the meaning of a text (though of course this meaning is mediated by context). In neural language models (LMs), this contribution is the token embedding, which _stably_ maps each token into a continuous vector . However, in real language, a token's contribution might be determined by its structural role; in math and code, novel variable names are arbitrarily defined to carry new meaning, and poems such as Jabberwocky exploit humans' lexical flexibility in interpreting novel words such as _vorpal_. Besides standard language understanding, this lexical flexibility also correlates with a stronger in-context reasoning performance. For example, GPT-3  and other large language models that demonstrate high lexical flexibility show strong performance on tasks involving in-context reasoning over new concepts and rules.

Motivated by the above, we ask whether we can push this flexibility to the extreme: can we build a language model without _any_ stable lexical mapping? To this end, we formulate and study such _leximariant_ language models. We define a leximvariant language model as a language model that assigns the same probability to all lexical permutations of a sequence. Formally, we define a lexical permutation \(\) to be a one-to-one mapping of a set of lexical symbols 1 onto itself. Then the leximvariant language model is defined as a language model over the symbol sequence \(x_{1}\),...,\(x_{n}\) with the following property:

\[p(x_{1},,x_{n})\!=\!p((x_{1}),,(x_{n}))\;\] (1)

For example, a leximvariant language model (whose vocabulary is letters and space) should assign the same probability to the phrase "a big banana" as "e cop celeke" because the two are the same up to the permutation \(\!=\!\{ {k}\}\) (Figure 0(a)).

The central question is: how well can leximvariant language models predict the next token given an increasingly long context? We find the answer is almost as well as standard language models, both theoretically and empirically. This is rather surprising given that leximariance seems like a strong limitation (a model doesn know what any individual symbol means!) However, the intuition is that given longer contexts, a leximvariant model can both infer the latent permutation \(\) (lazily) up to whatever ambiguity is present in the language model, and do the standard next word prediction task jointly.

Theoretically, we prove that a constructed leximvariant language model can converge to the true language model as the context length increases--that is, the average L1 distance between the predictions of the two models decreases with a convergence rate of \(O()^{}\), where \(T\) is the length of the context and \(d\) is the vocabulary size, and where the big-O notation hides polylogarithmic factors of \(d\) and \(T\) and an absolute constant that is _independent_ of the language model.

Empirically, we train a leximvariant LM by replacing standard embeddings in a decoder-only Transformer  with per-sequence random Gaussian vectors, such that the same symbols get the same embedding within each sequence but get different embedding across sequences (Figure 0(b)). We indeed see that the perplexity gap between the leximvariant LM and the standard LM shrinks as context length increases, as shown in Section 3.2. With a 150M parameters Transformer and a small character-level vocabulary (130 tokens), the average perplexity gap shrinks from 9X to less than 1X the average perplexity of a standard LM after observing 512 tokens over The Pile . With a larger 32K vocabulary, the gap also shrinks, especially on the more structured text like GitHub code, albeit at a much slower rate.

We then explore two additional properties of the leximvariant LM: in-context deciphering and symbol manipulation. First, we show that given a ciphertext generated by applying a substitution cipher to English text, the leximvariant LM can be seen as implicitly approximating Bayesian inference of the lexical permutation, i.e., cipher key, in-context. To show this empirically, we train a small MLP probe on top of a frozen pretrained leximvariant LM to predict the deciphered token corresponding to the last

Figure 1: Definition (a) and construction (b) of leximvariant language model

seen cipher token. We can then read out the inferred cipher key with each prefix of the sequence. We show that the accuracy of this inferred cipher key quickly improves as context length grows, reaching 99.6% average accuracy. We also show examples in Section 3.4 that visualize the uncertainties over different possible lexical mappings maintained by the lexinvariant LM when the cipher key is ambiguous and that the semantic meaning of a symbol with very rare occurrence can be inferred efficiently relative to other common symbols in context. Second, we show that lexinvariant models perform better than traditional models over synthetic pure in-context reasoning tasks that involve symbol manipulation. We observe a significant 4X improvement over a standard language model.

While the primary motivation of this paper is scientific exploration of a new idea, lexinvariance, we were also curious to see if it could help improve certain tasks, generalizing the performance gain we see on synthetic tasks. We stress that for most practical applications, lexinvariance is far too strong, so these experiments are intended to be illustrative rather than be a recipe for improving state-of-the-art. We discuss potential approaches to integrate the idea of lexinvariant LM into standard language modeling as a form of regularization, such that the LM assumes some form of partially stable symbol representations. The resulting LM can improve upon a standard language model over some BIG-bench tasks .

## 2 Lexinvariant Language Model

We define a language model as a probability distribution \(p(x_{1},...,x_{n})\) over input token sequences \(x_{1},...,x_{n}\!\!^{n}\), where \(\) is some vocabulary over symbols. A language model is lexinvariant if for all permutations \(\!:\!\!\!\) and for all token sequences \(x_{1},...,x_{n}\!\!^{n}\), \(p(x_{1},...,x_{n})\!=\!p((x_{1}),...,(x_{n}))\). For example, if \(\!=\!\{\}\), then the model should assign the same probability to \(aab\) and \(bba\). One example \(p\) that satisfies this could simply be

\[p(x)\!=\!1/2&x\!\!\{aab,bba\}\\ 0&\] (2)

Can such a lexinvariant language model predict language well, even though it can only make next token predictions based on the structure of co-occurence and repetition of input tokens in a single context?

### Convergence on Language Modeling Performance

We show that we can construct a lexinvariant LM (as shown in Figure 2) to model the true language distribution faithfully, given a long enough context. The lexinvariant language model can essentially infer back the latent permutation \(\) as it observes more symbols.

As an intuitive example, suppose that \(\!=\!\{a,b\}\) and the true language only contains two sequences \(babbbb\) and \(ababab\) (and their prefixes) with even probability. When given only the first three letters, a lexinvariant model can't tell the latent permutation and can only assign the same probability to \(a\) and \(b\) for the next letter: Due to the lexinvariant property, it assigns the same probability to \(p(a|aba)\!=\!p(b|bab)\) as well as to \(p(b|aba)\!=\!p(a|bab)\). Further, \(p(a|aba)\!=\!p(b|aba)\) because the permutations to prefixes \(aba\) and \(bab\) are equally probable. In contrast, when considering the prefix \(abab\), the fourth letter resolves the ambiguity in possible permutations \(\). (Since \(baba\) is not in the true language distribution, \(\) cannot map \(a\) to \(b\).) Therefore, the model can correctly predict that \(p(a|abab)\!=\!1\) and \(p(b|abab)\!=\!0\).

Formally, for a given language model \(p\), we define the associated lexinvariant language model \(p^{}(x_{1},\!...,\!x_{n})\) as \(_{}[p(^{-1}(x_{1}),\!...,\!^{-1}(x_{n}))]\). Analyzing it, we have the following theorem:

**Theorem 2.1**.: _Let \(x_{1},...,x_{n}\) be any token sequence generated by an arbitrary language distribution \(p\) with an alphabet of size \(d\). Let \(p^{}(x_{1},...,x_{n})=_{}[p(^{-1}(x_{1}),...,^{-1}(x_ {n}))]\). Then, for any

Figure 2: Probabilistic graphical model for the lexinvariant LM associated with the true language distribution \(p(x_{1},\!...,\!x_{n})\).

\(0\!<\!\),\(\!<\!1/2\),

\[\!_{t=1}^{T}\!\|p(x_{t}|x_{1},...,\!x_{t-1})\!-\!p^{}(x_{t}| x_{1},...,\!x_{t-1})\|_{1}\!\!\]

_with probability greater than \(1-\), when \(T}(d,,),\) where the polylogarithmic term hides an absolute constant that is independent of \(p\)._

This theorem says that this associated leinvariant language model converges to modeling the true language distribution fairly efficiently--with polynomial rate and near-linear dependence on vocabulary size \(d\). Strikingly, this holds irrespective of the properties of the language distribution \(p\)2. In other words, _a language model can indeed infer the operational meaning of the tokens in context based solely on the structure of the symbols_!

We give a complete proof of this theorem in Appendix A. At a high level, this convergence happens because at most timesteps \(t\), the new observation \(x_{t}\) either provides new information about the permutation \(\), or \(x_{t}\) has similar likelihood under the permutations that are likely given \(x_{1},...,x_{t-1}\). In the simplest case, if the posterior \((\,|\,x_{1},...,x_{n})\) concentrates on the correct \(\), then we converge to the standard LM. But even if it doesn't, that means the uncertainty about \(\) should not matter for predicting the next token. We make this precise by interpreting \(p^{}(x_{t}|x_{1},...,x_{t-1})\) as performing a multiplicative weights algorithm with the Hedge strategy of Freund and Schapire , and then relate the regret bounds to the average KL divergence between the predictions of \(p\) and \(p^{}\), and ultimately the average \(_{1}\) distance between these predictions.

### In-context Bayesian Deciphering

We can see the associated leinvariant language model as implicitly learning to approximate an in-context Bayesian deciphering process, i.e. inferring a probability distribution over possible lexical permutations based on seen tokens, with the language modeling prior:

\[p^{}(x_{n+1}|x_{1},...,x_{n})\] (3) \[= \!\!\!_{}\!(x_{1}),...,^{- 1}(x_{n+1}))}{p^{}(x_{1},...,x_{n})}\] \[= \!\!\!_{}\!(x_{1}),...,\!^{-1}(x_{n+1} ))}{p(^{-1}(x_{1}),...,\!^{-1}(x_{n}))}p(^{-1}(x_{1 }),...,\!^{-1}(x_{n}))}{p^{}(x_{1},...,x_{n})}\] \[= \!\!\!_{}\!(x_{n+1})|^{-1}(x_{1}),..., ^{-1}(x_{n}))}{}(|x_{1},..., x_{n})}_{}\]

As shown above, \(p^{}\) can be reduced to two parts, where the first part is normal language modeling and the second part is the probability distribution of lexical permutations based on seen tokens. So the leinvariant language model is implicitly learning to model \((|x_{1},...,x_{n})\).

We can make this approximate in-context Bayesian deciphering explicit by training a small probe to predict \((|x_{1},...,x_{n})\) given the internal representation of the leinvariant language model. We will show that this indeed recovers \(\) reasonably accurately in the experiment section.

### Constructing a Lexinvariant Language Model

We now consider how to construct a leinvariant LM in practice. A typical neural language model, such as a Transformer, converts input tokens to continuous vectors using token embedding and then passes these vectors as input to the rest of the neural network. Thus, the language model \(p\) it parameterizes depends on the token embedding \(E\!:\!\!\!^{d}\):

\[p(x_{1},...,\!x_{n})\!=\!T(E(x_{1}),...,\!E(x_{n}))\] (4)

To make a neural LM leinvariant, we can replace the standard stable token embedding \(E\) with a randomized \(E\) and take the expectation over \(E\). Each token \(x\) has an independent embedding\(E(x)(0,_{d})\), and the language model becomes

\[p(x_{1},,x_{n})=[T(E(x_{1}),,E(x_{n}))]\] (5)

Since \(EE\), the right-hand side is the same when \(x_{i}\) are applied with any permutation \(\), i.e., for any \(x_{1},,x_{n}\):

\[[T(E(x_{1}),,E(x_{n}))]=[T(E((x_{1})),,E((x_{n})))],\] (6)

showing that the Transformer with random \(E\) is leximvariant as in Eq. 1. Now we can train this leximvariant LM similarly to a standard LM. Concretely, we sample a new \(E\) for each training sequence and minimize the standard language modeling loss as in a standard neural LM. Here we are stochastically optimizing a variational lower bound of the standard language modeling loss with this randomized model by taking the expectation to the outside of the loss over log likelihood. Effectively, the same token gets the same random embedding within each training sequence, but different embedding across training sequences.

In practice, we focus on training decoder-only Transformers with a next token prediction objective in this work, where the model directly models \(p(x_{n+1}|x_{1},,x_{n})\) instead of the joint distribution. Our definitions and analysis above still hold in general. The only modification is that the final readout matrix also needs to be replaced with the same \(E\), so that the Transformer can predict the embedding of the next token based on the embeddding of input tokens.

## 3 Experiments

### Setup

**Architecture.** For all experiments, we use decoder-only Transformer architecture with T5 relative position bias . We use models with 150M parameters, with 12 layers, 8 heads, head dimension 128, and MLP dimension 4096.

**Training.** We use the Adafactor optimizer , with a cosine decay learning rate schedule  from 0.01 to 0.001 based on preliminary experiments. We train the models from scratch for 250K steps on all the settings, with 512 sequence length and 64 batch size. We ran all of our experiments on 8 TPU cores. Our models are implemented in JAX .

**Datasets.** For datasets, we mainly use the Pile , a large open-source corpus that contains text collected from 22 diverse high-quality sources. We also run experiments on two additional datasets to explore their effects on the behavior of leximvariant models: Wiki-40B , which contains high quality processed Wikipedia text in 40+ languages, and GitHub (subset of the Pile), which contains code from GitHub repositories with more than 100 stars and less than 1GB files.

### Convergence to Standard Language Models

We first show empirically that leximariant LMs can mostly recover the next token prediction performance of standard LMs after a long enough context. As already discussed in section 2.1, the leximvariant LM will theoretically converge to a standard LM as the context becomes long enough to resolve ambiguity. Here we verify this experimentally and show the variation of this convergence across corpora and tokenizations.

To show this, we train leximvariant and standard LMs with both character-level vocabulary (128 ascii characters) and T5 default vocab (32k tokens) over the three datasets. For each model, we measure

Figure 3: Perplexity over the Pile with character-level vocabulary (left) and T5 default vocab (right).

the perplexity of each token in each sequence w.r.t. context length, smoothed by moving average within each sequence, i.e. \(P(x_{i},\)...,\(x_{i+k}|x_{1},\)...,\(x_{i})^{}\) for context length \(i\). We set the moving average window \(k\!=\!100\). We plot results over 100 sequences. As shown in figure 3, the perplexity gap between leinvariant LM and standard LM gradually shrinks as the prefix becomes longer and longer, albeit much more slowly with a larger vocabulary. This makes intuitive sense since a larger vocabulary has more possibilities of permutations and requires many more prefix tokens to disambiguate. For the 32K vocabulary, the 512 context length will only allow the model to see a very small number of tokens, let alone to see tokens more than once. Nonetheless, the model still manages to show the trend of convergence, since even a small number of repetitions can form common patterns in grammar (such as the usage of spaces, punctuation, articles, etc). For the character-level vocabulary, the perplexity gap shrinks from 9X to less than 1X the average perplexity of the standard LM. With a context length of 511, the leinvariant LM converges to perplexity 3.38, almost comparable to the perplexity of the standard LM of 2.00. Additionally, we observe that the gap shrinks significantly faster for models trained over Github than standard English text like Wiki-40B since code is more structured and it is easier to decipher the token permutation. We show the comparison across different datasets in Figure 7 in Appendix.

### Recovering Substitution Ciphers

Here we show that leinvariant LM is implicitly performing Bayesian in-context deciphering by testing its ability to recover cipher keys (e.g. Figure 3(a)) from character-level substitution ciphers, e.g. uC; kvR5W 4mzfad @f| Svcgnfw;m uCRmu;d ]%_> :fBn. For the lexinvariant LM, this cipher text is perceived as the same as the quick brown fox jumps over thirteen lazy dogs, due to the lexinvariant property. It will then proceed to complete the cipher text with %d: uC; @f| with the same probability as it will complete the normal text with and the fox.

Because of this, we cannot directly read out the distribution of possible cipher keys \((|x_{1},\)...,\(x_{n-1})\) implicitly inferred by the lexinvariant LM. To do this, we train a small two-layer MLP probe on top of a frozen trained lexinvariant LM. For each training sequence, we first embed the input sequence with a randomly sampled token embedding \(E\) as described in section 2.3 and obtain the hidden activation of the final layer generated by the frozen lexinvariant LM. Then, we pass this activation through the two-layer MLP probe. Finally, instead of decoding the output activations to classification logits with the same \(E\) as in the lexinvariant LM, we instead use another learnable non-randomized token embedding matrix \(E^{}\) so that the probe can recover the deciphered token with stable token embeddings. Overall, we train the probe jointly with this embedding matrix \(E^{}\) to predict the current token. Effectively, we are training the probe to decipher the current token using the representation provided by the lexinvariant LM. We train the probe over the same corpus as the original lexinvariant LM for 10k steps. With this probe, we can directly visualize \((^{-1}(x_{n})|x_{1},\)...,\(x_{n})\) inferred by the lexinvariant LM, which is effectively one row in the permutation matrix representing \(\).

Now we can use this probe to explicitly recover the cipher key. An example ground truth cipher key that we want to recover is shown in Figure 3(a). Note that although the substitution cipher is only among lowercase letters, the character-level lexinvariant model we use assumes that all permutations among the 128 characters are possible, making the deciphering even more challenging.

Figure 4: (a) (b): Cipher key matrix, where the vertical axis shows the cipher characters and the horizontal axis shows the deciphered letters. The highlighted entries show the correspondences between cipher characters and the actual letters, e.g. % deciphers to 1. (c): Cipher key prediction accuracy, averaged across 1000 input sequences. Context length denotes the start index of the window.

Concretely, we first input ciphertext through the frozen lexinvariant LM with the probe to produce a deciphered sequence. We then select a window of size 100 in the middle of the sequence and perform a majority vote over the corresponding deciphered tokens of each cipher token seen in this window. This essentially produces a predicted cipher key matrix for each window, and we can measure its precision against the ground truth. As shown in Figure 3(c), such a cipher key prediction generally has increasingly higher precision as the window is selected later in the context, and it becomes near-perfect by the end of the sequence. Specifically, the cipher key matrix produced by the last window has an average precision of 99.6% over 1000 input sequences.

Finally, we aggregate over the last window of the 1000 sequences to recover a full cipher key, in case certain letters never appear in the last window of certain sequences. We again recover a full cipher key via majority vote. In Figure 3(b), we show the highly accurate predicted cipher key recovered from ciphertext produced using the example ground truth cipher key in Figure 3(a).

To perform a more detailed analysis showing the Bayesian deciphering process of the lexinvariant model, we use the logits of the probe to recover the predicted distribution of the cipher key \((|x_{1},,x_{n-1})\). Instead of taking the majority vote of the predicted decipher tokens in the window, we take the mean of logits predicted for each ciphered token. This essentially gives a locally averaged predicted distribution of cipher key matrices. Specifically, the cipher key matrices are generated across windows of 50 characters, and the probabilities are averaged over 1000 input sequences encoded using the same ground truth cipher. As shown in Figure 5, the predicted distribution of cipher key matrix becomes sharper as the prefix becomes longer.

### In-context Bayesian Deciphering Examples

Here, we show several qualitative examples of in-context Bayesian deciphering. We first show how the lexinvariant LM maintains uncertainty over possible lexical permutations while iteratively updating them at each index, using examples from a character-level lexinvariant model. Then, we also show an example of semantic in-context deciphering with a 32K vocabulary lexinvariant model, where the meaning of a novel word is inferred relative to common words in-context.

#### 3.4.1 Uncertainty over Lexical Permutations

In Figure 5(a), we input the following ciphered sequence to the frozen character-level lexinvariant LM with the probe: "I saw lots of people in town today, walking and talking around me. I greeted my friend Alice and my classmate Alex. I saw a guy, Joe, walking outside carrying a zat. Joe's zat was taken off zy wind. Today's wind was strong, so Joe's zat flew zackward. Joe lost Joe's zat for good. Joe will miss Joe's zat." For each instance of z in the sequence, we display the predicted deciphering of that instance as a row of probabilities across non-cipher letters a-z.

The lexinvariant model starts off assuming uniform probability for all possible lexical permutations \(\). After seeing more and more text, the lexinvariant model quickly realizes that z only has a few main plausible decipherings (b, h, c, m). Eventually, the lexinvariant model is able to narrow the possibilities down to z maps to b near the end of the sequence. The predicted probabilities shift with the seen context accordingly, demonstrating an example of how the predicted cipher key is iteratively updated at each index.

Figure 5(b) shows another example with a similar set up, but with text: "I saw a man in the pazk with a zat. The man was walking with the zat zight beside him. I've nevez seen anything like that before." While context initially suggests that z may be deciphered as c, it becomes clear that z must correspond to r after the appearance of "right". The disambiguation is reflected in the depicted probabilities.

Figure 5: Predicted cipher key for windows of size 50, at indices 0, 50, 100, 200, and 400. Generated using temperature of \(T\!=\!1\).

In Figure 5(c) and 5(d), we show two deciphering examples over code. We consider two code examples in which it is initially ambiguous whether the character z deciphers to : or \(\{\). The ambiguity is eventually resolved by the use of Python-like or Java-like syntax.

#### 3.4.2 Semantic Deciphering

In addition to character-level deciphering, we show examples of semantic deciphering with the larger vocabulary of 32k. Although the lexinvariant LM could not possibly figure out the true lexical permutation among 32k tokens using a small 512 context, it is possible to construct a simple context that repetitively uses simple words so that these words are easier to decipher. Then the lexinvariant LM can decipher the approximate semantics of the rare symbols relative to other easier-to-decipher words.

One example is the following: given the prompt 'crash! 'aaah!'i looked up from my cup of coffee. 'crash!'- that was the cafe window. and 'aaah! [... more text...] what one here is a drink [] - restaurants [] - music [] - coffee [] - father [] - the one here that drink is, where the word coffee, music, and father all only appear once before the question and restaurants appeared 4 times, the model is able to correctly answer that coffee is drinkable. See the full example in the appendix.

### Synthetic Reasoning Tasks

As discussed in the introduction, lexical flexibility is correlated with in-context reasoning performance, as demonstrated by existing large LMs. Thus, we study whether the lexinvariant model also learns in-context reasoning capabilities through the challenging lexinvariant training.

Specifically, we measure the performance of lexinvariant models over two pure in-context symbol manipulation tasks: LookUp, where the task is to predict the next token based on the given lookup table, e.g. A->[]C->[]G->[]G->[]C-> (should predict 4 here); and Permutation, where the task is to permute an arbitrary subsequence of the given sequence the same way as in the given few demonstrations, e.g. A 2 C->C A[]4 1 D-> (should predict D 4 here). In each of the tasks, the symbols are randomly sampled from the vocabulary so that we measure the pure reasoning ability independent from any knowledge of specific words. We measure the model performance in terms of generated token accuracy over 1000 examples. The results are shown in Table 1. As shown in the table, the lexinvariant models achieve drastically higher accuracy, with an average of 4X improvement.

   Dataset & Vocab & LookUp Acc &  \\   & & Standard & LI & Standard & LI \\   & char & 48.50 & 91.80 & 27.66 & 59.35 \\  & 32k & 21.45 & 92.10 & 22.84 & 55.63 \\   & char & 38.25 & 59.70 & 20.77 & 60.51 \\  & 32k & 8.75 & 59.35 & 9.94 & 50.91 \\   & char & 42.40 & 86.65 & 21.03 & 71.59 \\  & 32k & 4.25 & 80.20 & 8.59 & 67.39 \\   

Table 1: Accuracy over synthetic reasoning tasks.

Figure 6: Probe predictions for deciphering “z” at each occurrence of “z” in context.

### Regularizing Language Models with Lexinvariance

Although lexinvariant LM has various interesting properties, it is not suitable for practical tasks since it would require the context to be extremely long so that all required words and knowledge are defined in the context. Here, we explore how to construct more practical semi-lexinvariant LMs that maintain some properties of lexinvariant LMs via regularization. We emphasize that this exploration is intended to be illustrative rather than directly improving state-of-the-art.

Instead of using random Gaussian embedding matrices in place of a learned embedding matrix entirely, we can use random embeddings for only some of the tokens in each sequence, while others use the learned embedding. This means that the learned LM assumes that certain tokens have stable meanings but not others, which can be seen as a form of regularization towards lexinvariance. Specifically, we randomly select tokens to randomize based on a Bernoulli distribution, which can essentially be seen as a form of dropout on token embeddings. On the BIG-bench tasks, we found that a model with dropout rate \(p=0.2\) for randomization was 25% more likely to improve performance than to harm performance when evaluated with three shots, relative to a comparably-sized LM, with improvements especially over retrieval type of tasks. See full details in the Appendix G.

More broadly, this regularization view could potentially bring the benefit of lexinvariant LMs to practical applications. For example, the regularization could improve 1) the robustness of LMs by making them less sensitive to adversarial attacks or noise in the input data, 2) generalization across different languages or domains by being less tied to specific lexical items and more prone to learn the shared language structure, and 3) reasoning over more realistic tasks as we have started to explore with BIG-Bench. These areas are promising directions for future work to explore.

## 4 Related Work

### Symbol Grounding

Beyond a modeling choice, the main question of our paper (that being whether an LM can learn language without a stable token representation) is also analogous to the symbol grounding problem: Can meaning be acquired when symbols are not even grounded stably, i.e. they can be mapped to completely random meanings in different sequences? It has long been argued by the symbol grounding literature that symbolic representations must be grounded bottom-up in nonsymbolic representations , with famous arguments like Searle's Chinese room: It describes a person in a room given a step-by-step set of instructions by which they can respond to Chinese text with reasonable-sounding Chinese text. To an outside observer, the person in the room appears to understand Chinese, but the individual does not know a word of Chinese. This is widely used to argue that understanding language requires grounding the symbols in the real world. It leads to an ongoing debate on whether LMs can learn meaning purely from large amounts of text, without grounding to any real-world objects . Although intuitively, leinvariant LMs appears one step further removed from physical grounding than standard LM, we find that given enough context they can still infer the meaning of symbols based on lexical structures within the context.

### Group invariances and Data augmentation

Our implementation of lexinvariant LMs can be seen as performing a form of very aggressive data augmentation, where we randomize the identity of each token in each sequence. From this perspective, it is somewhat similar to the data recombination in [14; 2] and augmentation of named entities in , where certain parts of the sentence are swapped with other words while still maintaining the original grammatical structure. In contrast to these augmentations, the training for our lexinvariant LMs completely swaps out all parts of the input text.

### Byte-level T5

There is existing work on absorbing tokenization completely into part of language modeling by using extremely small tokens, such as Byte-level T5 . In the extreme, such a model would become closer and closer to lexinvariant LM, since bytes or bits have almost no stable meaning, so their embeddings are likely not used for prediction. In this paper, we study general lexinvariant LMs with the lexinvariant property baked in and without requiring specific tokenizers.

### Deciphering Substitution Cipher using LMs

In general, solving substitution ciphers, where the cipher key is a permutation of the original alphabet, is a NP-hard problem when only having access to LMs that can assign probabilities to sequences . There have been several works focusing on solving substitution ciphers using LMs, including approaches from searching over the permutation space guided by LMs' scores  to training a seq-to-seq model directly to perform deciphering as translation . Although our work does not focus specifically on the task of deciphering substitution ciphers, we show that our lexinvariant model can efficiently perform in-context deciphering as a byproduct of language modeling.

### Reasoning

It has been shown that large language models acquire surprising in-context reasoning capabilities [6; 15; 23]. Many of them are related to lexical flexibility through training for purely next-token prediction, such as modified arithmetic, data reformatting, and redefining single word etc. However, LLMs also memorize an enormous amount of knowledge along the way, which is often unnecessary. This work can also be seen as an exploration of whether a (semi-)lexinvariant LM can discount knowledge and prioritize learning the diverse structural reasoning patterns in language, therefore achieving the strong reasoning capability of LLMs with a smaller model.

## 5 Conclusion

In this work, we define and study lexinvariant language models, which do not have stable embeddings and learn to infer the meaning of symbols in-context. We show several surprising properties of this model theoretically and empirically, including convergence to standard language modeling, in-context deciphering, and better reasoning capabilities. We also explore a less extreme lexinvariance regularized language model and demonstrate its potential for solving more practical tasks efficiently.