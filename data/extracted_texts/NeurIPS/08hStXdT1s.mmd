# Knowledge Diffusion for Distillation

Tao Huang\({}^{1,2}\) Yuan Zhang\({}^{3}\) Mingkai Zheng\({}^{1}\) Shan You\({}^{2}\)

Fei Wang\({}^{4}\) Chen Qian\({}^{2}\) Chang Xu\({}^{1}\)

\({}^{1}\)School of Computer Science, Faculty of Engineering, The University of Sydney

\({}^{2}\)SenseTime Research \({}^{3}\)Peking University

\({}^{4}\)University of Science and Technology of China

Correspondence to: Shan You \(<\)youshan@sensetime.com\(>\).

###### Abstract

The representation gap between teacher and student is an emerging topic in knowledge distillation (KD). To reduce the gap and improve the performance, current methods often resort to complicated training schemes, loss functions, and feature alignments, which are task-specific and feature-specific. In this paper, we state that the essence of these methods is to discard the noisy information and distill the valuable information in the feature, and propose a novel KD method dubbed DiffKD, to explicitly denoise and match features using diffusion models. Our approach is based on the observation that student features typically contain more noises than teacher features due to the smaller capacity of student model. To address this, we propose to denoise student features using a diffusion model trained by teacher features. This allows us to perform better distillation between the refined clean feature and teacher feature. Additionally, we introduce a light-weight diffusion model with a linear autoencoder to reduce the computation cost and an adaptive noise matching module to improve the denoising performance. Extensive experiments demonstrate that DiffKD is effective across various types of features and achieves state-of-the-art performance consistently on image classification, object detection, and semantic segmentation tasks. Code is available at https://github.com/hunto/DiffKD.

## 1 Introduction

The success of deep neural networks is generally accomplished with the requirements of large computation and memory, which restricts their applications on resource-limited devices. One widely-used solution is knowledge distillation (KD) , which aims to boost the performance of efficient model (student) by transferring the knowledge of a larger model (teacher).

The key to knowledge distillation lies in how to transfer the knowledge from teacher to student by matching the output features (_e.g._, representations and logits). Recently, some studies  have shown that the discrepancy between student feature and teacher feature can be significantly large due to the capacity gap between the two models. Directly aligning those mismatched features would even disturb the optimization of student and weaken the performance. As a result, the essence of most state-of-the-art KD methods is to shrink this discrepancy and only select the valuable information for distillation. For example, TAKD  introduces multiple middle-sized teach assistant models to bridge the gap; SFTN  learns a student-friendly teacher by regularizing the teacher training with student; DIST  relaxes the exact matching of teacher and student features of Kullback-Leibler (KL) divergence loss by proposing a correlation-based loss; MaskKD  distills the valuable information in the features and ignores the noisy regions by learning to identify receptive regions that contribute to the task precision. However, these methods need to resort to either complicated training schemes or task-specific priors, making them challenging to apply to various tasks and feature types.

In this paper, we proceed from a different perspective and argue that the devil of knowledge distillation is in the noise within the distillation features. Intuitively, we regard the student as a _noisy_ version of the teacher due to its limited capacity or training recipe to learn truly valuable and decent features. However, distilling knowledge with this noise can be detrimental for the student, and may even lead to undesired degradation. Therefore, we propose to eliminate the noisy information within student and distill only the valuable information accordingly. Concretely, inspired by the success of generative tasks, we leverage diffusion models [15; 40], a class of probabilistic generative models that can gradually remove the noise from an image or a feature, to perform the denoising module. An overview of our DiffKD is illustrated in Fig. 1. We empirically show that this simple denoising process can generate a denoised student feature that is very similar to the corresponding teacher feature, ensuring that our distillation can be performed in a more consistent manner.

Nevertheless, directly leveraging diffusion models in knowledge distillation has two major issues. (1) _Expensive computation cost_. The conventional diffusion models use a UNet-based architecture to predict the noise, and take a large amount of computations to generate high-quality images2. In DiffKD, a lighter diffusion model would suffice since we only need to denoise the student feature. We therefore propose a light-weight diffusion model consisting of two bottleneck blocks in ResNet . Besides, inspired by Latent Diffusion , we also adopt a linear autoencoder to compress the teacher feature, which further reduces the computation cost. (2) _Inexact noisy level of student feature._ The reverse denoising process in diffusion requires to start from a certain initial timestep, but in DiffKD, the student feature is used as the initial noisy feature and we cannot directly get its corresponding noisy level (timestep); therefore, the inexact noisy level would weaken the denoising performance. To solve this problem, we propose an adaptive noise matching module, which measures the noisy level of each student feature adaptively and specifies a corresponding Gaussian noise to the feature to match the correct noisy level in initialization. With these two improvements, our resulting method DiffKD is efficient and effective, and can be easily implemented on various tasks.

It is worth noting that one of the merits of our method DiffKD is feature-agnostic, and the knowledge diffusion can be applied to different types of features including intermediate feature, classification output, and regression output. Extensive experimental results show our DiffKD surpasses current state-of-the-art methods consistently on standard model settings of image classification, object detection, and semantic segmentation tasks. For instance, DiffKD obtains 73.62% accuracy with MobileNetV1 student and ResNet-50 teacher on ImageNet, surpassing DKD  by 1.57%; while on semantic segmentation, DiffKD outperforms MaskD  by 1% with PSPNet-R18 student on Cityscapes test set. Moreover, to demonstrate our efficacy in eliminating discrepancy between teacher and student features, we also implement DiffKD on stronger teacher settings that have much more advanced teacher models, and our method significantly outperforms existing methods. For example, with Swin-T student and Swin-L teacher, our DiffKD achieves remarkable \(82.5\%\) accuracy on ImageNet, improving KD baseline with a large margin of \(1\%\).

Figure 1: **Diffusion model in DiffKD. The diffusion model is trained with teacher feature in diffusion process \(q\) (red-dashed arrow), while we feed the student feature to the reverse denoising process \(p_{}\) (blue arrows) to obtain a denoised feature for distillation. We find that due to capacity limitation, student feature contains more noises and its semantic information is not as salient as the teacherâ€™s. Therefore, we treat student feature as a noisy version of teacher feature, and propose to denoise student feature using a diffusion model trained with teacher feature.**

## 2 Preliminaries

### Knowledge distillation

Conventional knowledge distillation methods transfer the knowledge of a pretrained and fixed teacher model to a student model by minimizing the discrepancies between teacher and student outputs. Typically, the outputs can be the predictions (_e.g._, logits in classification task) and intermediate features of model. Given the student outputs \(^{(s)}\) and teacher outputs \(^{(t)}\), the knowledge distillation loss is defined as

\[_{}:=d(^{(s)},^{(t)}),\] (1)

where \(d\) denotes distance function that measures the discrepancy of two outputs. For example, we can use Kullback-Leibler (KL) divergence for probabilistic outputs, and mean square error (MSE) for intermediate features and regression outputs.

### Diffusion models

Diffusion models are a class of probabilistic generative models that progressively add noise to the sample data, and then learn to reverse this process by predicting and removing the noise. Formally, given the sample data \(_{0}^{C H W}\), the forward noise process iteratively adds Gaussian noise to it, _i.e._,

\[q(_{t}|_{0}):=(_{t}|_{t}} _{0},(1-_{t})),\] (2)

where \(_{t}\) is the transformed noisy data at timestep \(t\{0,1,...,T\}\), \(_{t}:=_{s=0}^{t}_{s}=_{s=0}^{t}(1-_{s})\) is a notation for directly sampling \(_{t}\) at arbitrary timestep with noise variance schedule \(\). Therefore, we can express \(_{t}\) as a linear combination of \(_{0}\) and noise variable \(_{t}\):

\[_{t}=_{t}}_{0}+_{t}}_{t},\] (3)

where \(_{t}(,)\). During training, a neural network \(_{}(_{t},t)\) is trained to predict the noise in \(_{t}\) w.r.t. \(_{0}\) by minimizing the L2 loss between them, _i.e._,

\[_{}:=||_{t}-_{}(_{t},t)|| _{2}^{2}.\] (4)

During inference, with the initial noise \(_{t}\), the data sample \(_{0}\) is reconstructed with an iterative denoising process using the trained network \(_{}\):

\[p_{}(_{t-1}|_{t}):=(_{t-1};_{}( _{t},t),_{t}^{2}),\] (5)

where \(_{t}^{2}\) denotes the transition variance in DDIM , which accelerates denoising process by sampling with a small number of score function evaluations (NFEs), _i.e._, \(_{T}_{T-}..._{0}\), where \(\) is the sampling interval.

In this paper, we leverage a diffusion model to eliminate the noises in student feature in our knowledge distillation method DiffKD, which will be introduced in the next section.

## 3 Method

In this section, we formulate our proposed knowledge distillation method DiffKD. We first convert the feature alignment task in KD to the denoising procedure in diffusion models, this enables us to use diffusion models to match student and teacher outputs for more accurate and effective distillation.

Figure 2: **Visualization of student features, denoised student features and teacher features on COCO dataset.** Details and more visualizations can be found in Appendix F.

To further improve the computation efficiency, we introduce a feature autoencoder to reduce the dimensions of feature maps, thereby streamlining the diffusion process. Additionally, we propose an adaptive noise matching module to enhance the denoising performance of student feature. The architecture of DiffKD is illustrated in Fig. 3.

### Knowledge diffusion for distillation

Generally, models with different capacities and architectures produce varying preferences on feature representations, even when trained on the same dataset. This discrepancy between teacher and student is crucial to the success of knowledge distillation. Previous studies [19; 21] have investigated the differences between teacher and student features. Kundu et al.  observes that the predicted probabilistic distribution of teacher is more sharp and confident than the student's. Similarly, ATS  discovers that the variance of wrong class probabilities of teacher is smaller than that of student, indicating that the teacher output is cleaner and more salient. In summary, both studies find that the student has larger values and variances on wrong classes than the teacher, suggesting that the predictions of student contains more noises than the teacher's.

In this paper, we demonstrate that the same trend holds for intermediate features. We visualize the first feature map of FPN in RetinaNet  on COCO dataset  in Fig. 2 and find that the semantic information in teacher feature is much more salient than the student feature. Therefore, we can conclude that both predictions and intermediate features of student model contain more noises than the teacher's, and these noises are difficult to eliminate through simple imitation of the teacher model in KD due to the capacity gap . As a result, a proper solution is to disregard the noises and only imitate the valuable information from both teacher and student. Inspired by the success of eliminating noises in diffusion models [15; 35; 40], we propose to treat the student feature as a noisy version of teacher feature, and train a diffusion model with teacher feature, then use it to denoise the student feature. With the denoised feature that contains only valuable information as teacher feature, we can perform noiseless distillation on them.

Formally, with teacher feature \(^{(tea)}\) and student feature \(^{(stu)}\) used in distillation, we use \(^{(tea)}\) in the forward noise process \(q(^{(tea)}_{t}|^{(tea)})\) (Eq. (2)) to train the diffusion model with \(_{}\) (Eq. (4)). Then the student feature is fed into the iterative denoising process of the learned diffusion model, _i.e._, \(p_{}(^{(stu)}_{t-1}|^{(stu)}_{t})\) in Eq. (5), where \(^{(stu)}\) is the initial noisy feature of the process. After this denoising process, we obtain a denoised student feature \(}^{(stu)}\), which is used to compute the KD loss with the original teacher feature \(^{(tea)}\) in Eq. (1).

### Efficient diffusion model with linear autoencoder

However, we find that the denoising process in DiffKD can be computationally expensive due to the large dimensions of the teacher feature. During training, DiffKD needs to forward the noise prediction network \(_{}\) for \(T\) times (we use \(T=5\) in our method) for denoising the student feature and \(1\) time for training the noise prediction network with teacher feature. This \(T+1\) times of forwarding can result in a high computation cost when the dimension of teacher feature is large. To reduce the computation cost of diffusion model, we propose a light diffusion model which is stacked with two bottleneck block in ResNet . Then we follow Latent Diffusion Models  and propose to compress the number of channels using a linear autoencoder module. The compressed latent feature is used as the input of diffusion model. As shown in Fig. 3, our linear autoencoder module is composed of two convolutions only, one is the encoder for reducing the number of channels, another one is the decoder

Figure 3: **Architecture of DiffKD.**_Bottleneck_ denotes the Bottneck block in ResNet .

for reconstruct the teacher features. The output feature of encoder is used to train the diffusion models (Eq. (2)) and supervise the student.

The autoencoder is trained with a reconstruction loss only, which is the mean square error between the original teacher teacher \(^{(tea)}\) and reconstructed teacher feature \(}_{ae}^{(tea)}\), _i.e._,

\[_{}:=||}^{(tea)}-^{(tea)}||_{2}^{2}.\] (6)

Note that the latent teacher feature \(^{(tea)}\) used to train diffusion model is detached and has no gradient backward from the diffusion model.

We also use a convolution layer to project the student feature to the same dimension as teacher latent feature \(^{(tea)}\), denoted as \(^{(stu)}\). It is then passed to the diffusion model to perform a reverse denoising process (Eq. (5)) and generate the denoised student feature \(}^{(stu)}\). Now we have the latent representation \(^{(tea)}\) of teacher and the denoised representation of student \(}^{(stu)}\), they are then used to compute the KD loss and supervise the student, _i.e._,

\[_{}:=d(}^{(stu)},^{(tea)}).\] (7)

Note that our DiffKD is generic and applies to various tasks (_e.g._, classification, object detection, and semantic segmentaion) and feature types (_e.g._, intermediate feature, classification output, and regression output). We use simple MSE loss and KL divergence loss as the distance function \(d\) to compute the discrepancy of denoised student feature and teacher feature as a baseline implementation of DiffKD, while it is possible to achieve better performance with more advanced distillation losses.

### Adaptive noise matching

As previously discussed, we treat student feature as a noisy version of teacher feature. However, the noisy level, which represents the gap between the teacher and student features, is unknown and may vary depending on different training samples. Therefore, we cannot directly determine which initial timestep we should start the diffusion process. To address this issue, we introduce an adaptive noise matching module to match the noise level of student feature to a pre-defined noise level.

As shown in Fig. 3, we construct a simple convolutional module to learn a weight \(\) that fuses student output and Gaussian noise, which helps us match the student output to the same noisy level of noisy feature at initial time step \(T\). Therefore, the initial noisy feature in the denoising process becomes

\[_{T}^{(stu)}=^{(stu)}+(1-)_{T}.\] (8)

This noise adaptation can be naturally optimized with the KD loss \(_{}\), since the optimal denoised student feature that has minimal discrepancy to the teacher feature is obtained when the student feature matches the appropriate noise level in the denoising process.

**Overall loss function.** The overall loss function of DiffKD is composed of the original task loss, a diffusion loss that optimize the diffusion model, a reconstruction loss to learn the autoencoder, and a KD loss for distillation on teacher features and denoised student features, _i.e._,

\[_{}=_{}+_{1}_{ }+_{2}_{}+_{3}_{ },\] (9)

where \(_{1}\), \(_{2}\), and \(_{3}\) are loss weights to balance the losses. We simply set \(_{1}=_{2}=1\) in all experiments.

    &  &  &  &  &  &  & LS & EMA & LR scheduler &  \\  & & & BS & LR & Optimizer & WD & LS & EMA & LR scheduler &  \\  A1 & CIFAR-100 & 240 & 64 & 0.05 & SGD & \(5 10^{-4}\) & - & - & \( 0.1\) at 150,180,210 epochs & crop + flip \\  B1 & ImageNet & 100 & 256 & 0.1 & SGD & \(1 10^{-4}\) & - & - & \( 0.1\) every 30 epochs & crop + flip \\ B2 & ImageNet & 450 & 768 & 0.048 & RMSProp & \(1 10^{-5}\) & 0.1 & 0.9999 & \( 0.97\) every 2.4 epochs & \(\{\)_B1_\(\}\) + RA + RE \\ B3 & ImageNet & 300 & 1024 & 5e-4 & AdamW & \(5 10^{-2}\) & 0.1 & - & cosine & \(\{\)_B2_\(\}\) + CJ + Mixup + CutMix \\   

Table 1: **Training strategies on image classification tasks.**_BS_: batch size; _LR_: learning rate; _WD_: weight decay; _LS_: label smoothing; _EMA_: model exponential moving average; _RA_: RandAugment ; _RE_: random erasing; _CJ_: color jitter.

## 4 Experiments

In this paper, to sufficiently validate the generalization of our DiffKD, we conduct extensive experiments on image classification, object detection, and semantic segmentation tasks.

### ImageNet classification

**Settings.** Following DIST , we conduct experiments on baseline settings and stronger teacher settings. The training strategies for CIFAR-100 and ImageNet datasets are summarized in Tab. 1. On baseline settings, we use ResNet-18  and MobileNet V1  as student models, and ResNet-34 and ResNet-50 as teachers, respectively; and the training strategy (B1) is the most common one in previous methods [6; 17; 53]. While for the stronger teacher settings, we train students with much stronger teacher models (ResNet-50 and Swin-L ) and strategies (B2 and B3).

We implement our DiffKD on the output feature of backbone before average pooling, and the output logits of classification head, and the distance functions are MSE and KL divergence (with a temperature factor of 1), respectively. We set \(_{1}=_{2}=_{3}=1\).

**Results on baseline settings.** We summarized the results on baseline settings in Tab. 2. Our methods outperforms existing KD methods with a large margin, especially on the MobileNet and ResNet-50 setting, DiffKD significantly surpasses previous state-of-the-art DIST  by \(0.38\%\) on top-1 accuracy. For comparisons with our baseline one feature distillation, we also report the MSE results with the same distillation location as DiffKD. We can see that, by only adding our diffusion model for feature alignment, DiffKD obviously improves the MSE results by \(1.74\%\) on ResNet-18 and \(1.23\%\) on MobileNet V1. Moreover, we replace the KL divergence loss in the output logits of DiffKD with the advanced loss function DIST, which achieves further improvements. For instance, DiffKD with DIST loss improves DIST by 0.42% on ResNet-18. This indicates that our feature alignment in DiffKD is generic to different KD losses and can be further improved by changing the losses.

**Results on stronger teacher settings.** To fully investigate the efficacy of DiffKD on reducing the distillation gap between teacher feature and student feature, we further conduct experiments on much stronger teachers and training strategies following DIST. From the results summarized in Tab. 3, we can see that DiffKD outperforms DIST on all model settings, especially for the most light-weight MobileNetV2 in the table, DiffKD surpasses DIST by \(0.5\%\). It is worth to remind that our DiffKD only uses the simple KL divergence loss and MSE loss on the stronger settings, the performance could be further improved if we use more advanced loss functions such as DKD  and DIST .

   Student (teacher) &  & Stu. & KD  & Review  & DKD  & DIST  & MSE & DiffKD & DiffKD\({}^{}\) \\   & Top-1 & 73.31 & 69.76 & 70.66 & 71.61 & 71.70 & 72.07 & 70.58 & 72.22 & **72.49** \\  & Top-5 & 91.42 & 89.08 & 89.88 & 90.51 & 90.41 & 90.42 & 89.95 & 90.64 & **90.71** \\   & Top-1 & 76.16 & 70.13 & 70.68 & 72.56 & 72.05 & 73.24 & 72.39 & 73.62 & **73.78** \\  & Top-5 & 92.86 & 89.49 & 90.30 & 91.00 & 91.05 & 91.12 & 90.74 & 91.34 & **91.48** \\   

Table 2: **Evaluation results of baseline settings on ImageNet. We use ResNet-34 and ResNet-50 released by Torchvision  as our teacher networks, and follow the standard training strategy (B1). MSE: we implement our baseline for comparisons. \(\): we replace KL divergence loss with more advanced DIST loss in DiffKD.**

    &  &  \\   & & Tea. & Stu. & KD  & RKD  & SRRL  & DIST  & DiffKD \\   & ResNet-34 & 76.8 & 77.2 & 76.6 & 76.7 & 77.8 & **78.1** \\  & MobileNetV2 & 80.1 & 73.6 & 71.7 & 73.1 & 69.2 & 74.4 & **74.9** \\  & EfficientNet-B0 & & 78.0 & 77.4 & 77.5 & 77.3 & 78.6 & **78.8** \\   & ResNet-50 &  & 78.5 & 80.0 & 78.9 & 78.6 & 80.2 & **80.5** \\  & Swin-T & & 81.3 & 81.5 & 81.2 & 81.5 & 82.3 & **82.5** \\   

Table 3: **Performance of students trained with strong strategies on ImageNet. The _Swin-T_ is trained with strategy B3 in Table 1, others are trained with B2. The ResNet-50 is trained by TIMM , and Swin-L is pretrained on ImageNet-22K.**

### Results for CIFAR-100 dataset are summarized in Appendix C.

### Object detection

**Settings.** Following FGD , we conduct experiments on baseline settings and stronger teacher settings. On baseline settings, we adopt various network architectures, including two-stage detector Faster-RCNN , one-stage detector RetinaNet , and anchor-free detector FCOS , and use ResNet-50  as student models, and ResNet-101 as teachers, respectively. The training strategy is the most common one in previous methods [9; 18; 48]. While for the stronger teacher settings, we train students with much stronger teacher models, including two-stage detector Cascade Mask RCNN , one-stage detector RetinaNet , and anchor-free detector RepPoints , and stronger backbone ResNeXt-101 (X101) .

We conduct feature distillation on the predicted feature maps, and train the student with our DiffKD loss \(_{}\), regression KD loss, and task loss. Note that we do not use linear autoencoder in DiffKD since the number of channels in FPN is only 256. We set \(_{1}=_{2}=1\). Besides using MSE loss in feature distillation, we also adopted a simple attention-based MSE loss (marked with \({}^{}\) in Tab. 4) inspired by FGD  to balance the foreground and background distillations, which is acknowledged important for detection KD [10; 18; 48]. Details can be found in Appendix B.

**Results on baseline settings.** Our results compared with previous methods are summarized in Table 4. Our DiffKD can significantly improve the performance of student models over their teachers on various network architectures. For instance, DiffKD improve FCOS-R50 by \(4.0\) AP and surpasses FGD  by \(0.4\) AP. Besides, the attention-based MSE loss affords consistent improvements on vanilla DiffKD by \( 0.1\) AP.

**Results on stronger teacher settings.** We further investigate our efficacy on stronger teachers whose backbones are replaced by stronger ResNeXts . As in Table 5, student detectors achieve

   Method & AP & AP\({}_{50}\) & AP\({}_{75}\) & AP\({}_{S}\) & AP\({}_{M}\) & AP\({}_{L}\) \\   \\ T: Faster RCNN-R101 & 39.8 & 60.1 & 43.3 & 22.5 & 43.6 & 52.8 \\ S: Faster RCNN-R50 & 38.4 & 59.0 & 42.0 & 21.5 & 42.1 & 50.3 \\ Fitnet  & 38.9 & 59.5 & 42.4 & 21.9 & 42.2 & 51.6 \\ FRS  & 39.5 & 60.1 & 43.3 & 22.3 & 43.6 & 51.7 \\ FGD  & 40.4 & - & - & 22.8 & 44.5 & 53.5 \\  DiffKD & 40.6 & 60.9 & 43.9 & 23.0 & 44.5 & **54.0** \\ DiffKD\({}^{}\) & **40.7** & **61.0** & **44.3** & 22.6 & **44.6** & 53.7 \\   \\ T: RetinaNet-R101 & 38.9 & 58.0 & 41.5 & 21.0 & 42.8 & 52.4 \\ S: RetinaNet-R50 & 37.4 & 56.7 & 39.6 & 20.0 & 40.7 & 49.7 \\ Fitnet  & 37.4 & 57.1 & 40.0 & 20.8 & 40.8 & 50.9 \\ FRS  & 39.3 & 58.8 & 42.0 & 21.5 & 43.3 & 52.6 \\ FGD  & 39.6 & - & - & **22.9** & 43.7 & **53.6** \\  DiffKD & 39.7 & 58.6 & 42.1 & 21.6 & **43.8** & 53.3 \\ DiffKD\({}^{}\) & **39.8** & **58.7** & **42.5** & 21.5 & 43.6 & 53.2 \\   \\ T: FCOS-R101 & 40.8 & 60.0 & 44.0 & 24.2 & 44.3 & 52.4 \\ S: FCOS-R50 & 38.5 & 57.7 & 41.0 & 21.9 & 42.8 & 48.6 \\ FRS  & 40.9 & 60.3 & 43.6 & 25.7 & 45.2 & 51.2 \\ FGD  & 42.1 & - & - & **27.0** & 46.0 & 54.6 \\ DiffKD & 42.4 & 61.0 & **45.8** & 26.6 & 45.9 & 54.8 \\ DiffKD\({}^{}\) & **42.5** & **61.1** & 45.6 & 25.2 & **46.8** & **55.1** \\   

Table 4: **Object detection performance with baseline settings on COCO val set.** T: teacher. S: student. \(\): we replace MSE with an attention-based MSE loss.

   Method & AP & AP\({}_{50}\) & AP\({}_{75}\) & AP\({}_{S}\) & AP\({}_{M}\) & AP\({}_{L}\) \\   \\ T: CM RCNN-X101 & 45.6 & 64.1 & 49.7 & 26.2 & 49.6 & 60.0 \\ S: Faster RCNN-R50 & 38.4 & 59.0 & 42.0 & 21.5 & 42.1 & 50.3 \\ COFD  & 38.9 & 60.1 & 42.6 & 21.8 & 42.7 & 50.7 \\ FKD  & 41.5 & 62.2 & 45.1 & 23.5 & 45.0 & 55.3 \\ FGD  & 42.0 & - & - & 23.7 & 46.4 & **55.5** \\ DiffKD & 42.2 & 62.8 & 46.0 & **24.2** & 46.6 & 55.3 \\ DiffKD\({}^{}\) & **42.4** & **62.9** & **46.4** & 24.0 & **46.7** & 55.2 \\   \\ T: RetinaNet-X101 & 41.2 & 62.1 & 45.1 & 24.0 & 45.5 & 53.5 \\ S: RetinaNet-R50 & 37.4 & 56.7 & 39.6 & 20.0 & 40.7 & 49.7 \\ COFD  & 37.8 & 58.3 & 41.1 & 21.6 & 41.2 & 48.3 \\ FKD  & 39.6 & 58.8 & 42.1 & 22.7 & 43.3 & 52.5 \\ FGD  & 40.4 & - & - & **23.4** & 44.7 & 54.1 \\ DiffKD & 40.7 & 60.0 & 43.2 & 22.2 & 45.0 & 55.2 \\ DiffKD\({}^{}\) & **41.4** & **60.7** & **44.0** & 23.0 & **45.4** & **55.8** \\   \\ T: RepPoints-X101 & 44.2 & 65.5 & 47.8 & 26.2 & 48.4 & 58.5 \\ S: RepPoints-R50 & 38.6 & 59.6 & 41.6 & 22.5 & 42.2 & 50.4 \\ FKD  & 40.6 & 61.7 & 43.8 & 23.4 & 44.6 & 53.0 \\ FGD  & 41.3 & - & - & **24.5** & 45.2 & 54.0 \\ DiffKD & 41.7 & 62.6 & 44.9 & 23.6 & 45.4 & **55.9** \\ DiffKD\({}^{}\) & **41.9** & **62.8** & 45.0 & 24.4 & **45.7** & 55.3 \\   

Table 5: **Object detection performance with stronger teachers on COCO val set.**_CM RCNN_: Cascade Mask RCNN. \(\): we replace MSE with an attention-based MSE loss.

more enhancements with our DiffKD, especially when with a Retina-X101 teacher, DiffKD gains a substantial improvement of \(4.0\) AP over the Retina-R50. Additionally, our methods outperforms existing KD methods with a large margin, and significantly surpasses FGD  when distilling on RetinaNet and RepPoints, by \(1.0\) and \(0.6\) AP, respectively. Moreover, comparing Table 4 with 5, we can infer that when the teacher is stronger, the benefit of DiffKD is more significant, as the discrepancy between the student and a stronger teacher is larger.

### Semantic segmentation

**Settings.** Following CIRKD , we use DeepLabV3  framework with ResNet-101 (R101)  backbone as the teacher network. While for the students, we use various frameworks (DeepLabV3 and PSPNet ) and backbones (ResNet-18  and MobileNetV2 ) to valid our efficacy.

We conduct feature distillation on the predicted segmentation maps, and train the student with our DiffKD loss and task loss, as formulated in Eq. (9). Note that we do not use linear autoencoder in DiffKD since the number of channels in segmentation map is only 19. Detailed training strategies are summarized in Appendix B.

**Results.** The experimental results are summarized in Tab. 6. Our DiffKD significantly outperforms the state-of-the-art MasKD on all settings. For example, on DeepLabV3-R18 student, DiffKD improves MasKD by \(0.78\%\) on val set and \(0.65\) on test set.

### Ablation study

**Effects of adopting DiffKD on different types of features.** On image classification, we distill both intermediate feature (the output feature of backbone before average pooling) and output logits with DiffKD. Here we conduct experiments to compare the effects of distillations on different features of MobileNetV1 student and ResNet-50 teacher in Tab. 7. We can see that, by adopting DiffKD to align the student features, both feature-level DiffKD and logits-level DiffKD can obtain obvious improvements without changing the loss functions. Meanwhile, combining feature and logits distillations together in our final method achieves the optimal 73.62% top-1 accuracy.

**Effects of linear autoencoder.** We compare different dimensions of linear autoencoder in Tab. 8. We can see that, the linear autoencoder can significantly reduces the FLOPs of diffusion model, but if we

    & Params & FLOPs &  \\  & (M) & (G) & Val & Test \\  T: DeepLabV3-R101 & 61.1 & 2371.7 & 78.07 & 77.46 \\  S: DeepLabV3-R18 & 13.6 & 572.0 & 74.21 & 73.45 \\ CWD  & 13.6 & 572.0 & 75.55 & 74.07 \\ CIRKD  & 13.6 & 572.0 & 76.38 & 75.05 \\ MasKD  & 13.6 & 572.0 & 77.00 & 75.59 \\ DiffKD & 13.6 & 572.0 & **77.78** & **76.24** \\  S: DeepLabV3-R18\({}^{}\) & 13.6 & 572.0 & 65.17 & 65.47 \\ CWD  & 13.6 & 572.0 & 67.74 & 67.35 \\ CIRKD  & 13.6 & 572.0 & 68.18 & 68.22 \\ MasKD  & 13.6 & 572.0 & 73.95 & 73.74 \\ DiffKD & 13.6 & 572.0 & **74.45** & **74.52** \\   

Table 6: **Semantic segmentation results on Cityscapes dataset. \(\): trained from scratch. Other models are pretrained on ImageNet. FLOPs is measured based on an input size of \(1024 2048\).**

   Method & Top-1 (\%) \\  w/o KD & 70.13 \\ MSE & 72.39 \\ KL div. & 70.68 \\  DiffKD (feature) & 73.16 \\ DiffKD (logits) & 72.89 \\ DiffKD (feature + logits) & **73.62** \\   

Table 7: **Comparisons of different distillation features in DiffKD.**conduct high compression ratios like 128 and 256 dimensions, the distillation performance will be severely weakened. Interestingly, AE with 512, 1024, and 2048 dimensions can outperform the one without AE, a possible reason is that the AE encodes common and valuable information in the original feature for reconstruction and would have better feature representations than the original feature. As a result, we use AE with \(1024\) dimension in our experiments for better performance-efficiency trade-off.

**Effects of adaptive noise matching.** We propose adaptive noise matching (ANM) to match the noisy level of student feature to the correct initial level in the denoising process. Here we conduct experiments to validate its efficacy. We train MobileNetV1 student with the only removal of ANM in our final method, and the DiffKD without ANM obtains 73.34% top-1 accuracy, which has a decrease of \(0.28\) on our DiffKD with ANM. This indicates that ANM can improve the performance by generating better denoised student feature.

**Ablation on numbers of score function evaluations (NFEs).** In diffusion models, the number of score function evaluations (NFEs) is a important factor for controlling the generation quality and efficiency. The early method such as DDPM  requires to run a complete timesteps in the reverse denoising process as training, which leads to a heavy computational budget. Recently, some works  have been proposed to accelerate the denoising process by sampling a small number of timesteps. In this paper, we use DDIM  for speedup. Here, we conduct experiments to show the influence of different NFEs. As shown in Fig. 4, compared to the \(72.39\%\) accuracy of MSE baseline without denoising on student feature, only one-step denoising also achieves a significant improvement. However, its performance is weaker than that of those larger NFEs due to the limitation of denoising quality. A NFEs of \(5\) would suffice in our KD setting to achieve promising performance, and we use it in all experiments for a better efficiency-accuracy trade-off.

## 5 Conclusion

In this paper, we investigate the discrepancy between teacher and student in knowledge distillation. To reduce the discrepancy and improve the distillation performance, we proceed from a new perspective and propose to explicitly eliminate the noises in student feature with a diffusion model. Based on this idea, we further introduce a light-weight diffusion model with a linear autoencoder to reduce the computation cost of our method, and an adaptive noise matching module to align the student feature with the correct noisy level, thus improving the denoising performance. Extensive experiments on image classification, object detection, and semantic segmentation tasks validate our efficacy and generalization.