# STEM-PoM: Evaluating Language Models

Math-Symbol Reasoning in Document Parsing

 Jiaru Zou

University of Illinois at Urbana-Champaign

Champaign, IL

jiaruz2@illinois.edu

&Qing Wang

University of Illinois at Urbana-Champaign

Champaign, IL

qingw3@illinois.edu

&Pratyush Thakur

University of Illinois at Urbana-Champaign

Champaign, IL

pthakur3@illinois.edu

&Nickvash Kani

University of Illinois at Urbana-Champaign

Champaign, IL

kani@illinois.edu

###### Abstract

Advances in large language models (LLMs) have spurred research into enhancing their reasoning capabilities, particularly in math-rich STEM documents. While LLMs can generate equations or solve math-related queries, their ability to fully understand and interpret abstract mathematical symbols in long, math-rich documents remains limited. In this paper, we introduce STEM-PoM, a comprehensive benchmark dataset designed to evaluate LLMs' reasoning abilities on math symbols within contextual scientific text. The dataset, sourced from real-world ArXiv documents, contains over 2K math symbols classified as main attributes of variables, constants, operators, and unit descriptors, with additional sub-attributes including scalar/vector/matrix for variables and local/global/discipline-specific labels for both constants and operators. Our extensive experiments show that state-of-the-art LLMs achieve an average of 20-60% accuracy under in-context learning and 50-60% accuracy with fine-tuning, revealing a significant gap in their mathematical reasoning capabilities. STEM-PoM fuels future research of developing advanced Math-AI models that can robustly handle math symbols.

## 1 Introduction

Large language models (LLMs) have demonstrated exceptional reasoning abilities across numerous fields . With the increasing shift towards applying LLMs to complex tasks , the need for supplementary data beyond the general pre-trained datasets has become increasingly important. Among these, mathematical reasoning tasks  have recently drawn the attention of several researchers  (see Backgrounds in Appendix B). In particular, Part-of-Math Tagging , the mathematical analog to part-of-speech tagging  where mathematical tokens are classified according to a given taxonomy of attributes, continues to gain interest but lacks the foundational datasets that can support advanced NLP tasks . In addition, integrating mathematical language into NLP models remains a substantial challenge , especially in the realm of document parsing . Traditional semantic parsing methods like LateXML  or arXMLiv  often fall short when applied to math-rich documents, where precision and structured syntax are paramount . These methods struggle to accurately perform pattern matching between abstract mathematical symbols and their corresponding XML tag notations. Similarly, recent advanced LLMs, such as ChatGPT , also face difficulties in understanding and reasoning with abstract mathematical symbols due to their contextual polymorphism  (as Figure 3 shown).

For example, in the linear equation: \(y=mx+p\), \(y\) is categorized as a variable. Whereas in the cross-entropy loss function: \((x,y)=-_{i=1}^{N}x_{i}(y_{i})\), the symbol \(y\) represents the fixed target labels, which is considered a constant for a given dataset. Without the corresponding contextual information of a mathematical symbol, LLMs are unable to distinguish between different attributes of the symbol and cannot effectively process related mathematical reasoning tasks. Thus, tagging math symbols within domain-specific contexts is essential for language models.

In this paper, we introduce a novel benchmark dataset, **STEM-PoM**, designed to evaluate the reasoning capabilities of language models on mathematical symbols across different domains. The STEM-PoM dataset consists of 2,109 instances extracted from a random sampling of over 10,000 arXiv manuscripts, which are math-rich documents spanning domains such as Mathematics, Physics, Chemistry, and more. We provide a mathematical symbol for each dataset instance, its order in the document, its main and sub-level attributes, and the corresponding text or expression from the original arXiv paper containing the symbol. Each mathematical symbol in the dataset is classified according to two levels of attributes . The first-level attribute categorizes the symbol as variable, constant, operator, or unit descriptor. The second-level attribute further classifies the symbol into one of six types based on its first-level category: scalar, vector, matrix, local, global, or discipline-specific. Figure 1 illustrates the dataset's category distribution. To further enrich the STEM-PoM dataset with additional arXiv manuscripts and other math-rich document resources, we also design the **STEM-PoM Labeler**, a feasible method for assisting dataset generation by automatically searching, extracting, and recording hand-labeled mathematical symbols and their corresponding context from long-text documents.

We conduct thorough experiments on the STEM-PoM dataset to assess the mathematical reasoning abilities of seven open- and closed-source language models, including LSTM , Mixtral-8x7B , Llama2-13B , Llama3-80B , Claude-3.5-sonnet , GPT-3.5, and GPT-4o  with various prompting and fine-tuning strategies. The experimental results indicate that STEM-PoM distinguishes the performance levels across different LLMs, offering insights into the mathematical symbol reasoning abilities of these models. In addition, we investigate and analyze the influence of context length on the ability of language models to understand mathematical symbols.

## 2 STEM-PoM Dataset

### Data Annotation

**Source of Mathematical Symbols.** The first crucial step in constructing the dataset is selecting high-quality mathematical symbols. For STEM-PoM, we primarily collect these symbols from two sources: 1. _Public math-symbol datasets_, where we directly utilize candidate mathematical symbols

Figure 1: The overall pipeline for constructing the STEM-PoM dataset. We extract math symbols with corresponding text information to formulate the dataset. Each math symbol is initially classified into one of four primary categories based on its definition. Then, the symbol is further categorized into secondary categories by the context in which it appears or by the symbol’s dimensionality. An LLM is evaluated via the first-level and second-level classification tasks.

from the mathematical token definition extraction benchmark, MTDE . 2. _Raw ArXiv papers_, where we apply the STEM-PoM Labeler to identify and extract mathematical symbols from the ArXiv dataset. We include a detailed description of each source dataset in Appendix A.2.

**Dataset Construction.** After obtaining the mathematical symbols, we categorize each symbol into different attributes and assign corresponding information to construct the STEM-PoM dataset. Specifically, we first extract the file name and symbol order for each mathematical symbol. Then, for each symbol, we extract the contexts in which the symbol appears, using several predefined lengths. Following this, we manually classify each symbol into four primary categories: Variable, Constant, Operator, and Unit Descriptor. For Variable, Constant, and Operator, we further categorize them into sub-level categories. The variable is classified as Vector, Scalar, or Matrix, while Constant and Operator are categorized as Local, Global, or Discipline-Specific. Table 2 outlines the overall dataset structure. We manually examine each entry of the dataset thoroughly to ensure its robustness and correctness. We provide a detailed explanation of the dataset structure in Appendix A.3 and the definitions of each level's attributes in Appendix A.4. Additionally, the STEM-PoM Labeler is described in Appendix A.5.

### Dataset Statistics

We summarize the key statistics of our dataset in this section. Table 1 presents the categorical statistics, including the math symbols along with their first- and second-level attributes. The distribution of Variables, Constants, Operators, and Unit Descriptors is 58.5%, 18.2%, 17.2%, and 6.1%, respectively. In addition, Figure 2 illustrates the discipline distribution of the source arXiv papers. Our dataset covers mathematical symbols from various fields, including Mathematics, Physics, Chemistry, Economics, Computer Science, etc.

  
**Statistic** & **Number** \\ 
**Total Symbols** & **2,109** \\ 
**Unit Descriptor** & **129** \\
**Constant** & **384** \\ - Local & 171 \\ - Global & 121 \\ - Discipline Specific & 92 \\
**Operator** & **363** \\ - Local & 181 \\ - Global & 105 \\ - Discipline Specific & 77 \\
**Variable** & **1,233** \\ - Scalar & 601 \\ - Vector & 599 \\ - Matrix & 33 \\  Avg symbols per article & 4.7 \\ Avg tokens per sentence & 31.8 \\ Avg tokens per math symbol & 1.07 \\   

Table 1: STEM-PoM Dataset Statistics Figure 2: Discipline Distribution from Source ArXiv

## 3 Experiments

### Setup

**Models.** To thoroughly evaluate our dataset across models with varying parameter sizes, we utilize the following models: LSTM framework , Llama-2-13B , Mixtral-8x7B-v0.1 , and GPT-3.5-turbo-0125 .

**Evaluation Metrics.** We apply the _Precision Accuracy_ as our metric for the mathematical symbol classification task, the metric can be formulated as: \(=}{}\)

**Training & Inference Details.** We evaluate several models under both pre-training and fine-tuning settings. Specifically, we train an LSTM model with varying layers and apply the LoRA method [16; 47], a PEFT technique, to GPT-3.5. We evaluate other models in the in-context learning setting. Appendix C provides the training and model parameter details.

  
**Models** &  &  &  \\  _(Vanilla)_ & **Scalar** & **Vector** & **Matrix** & **Local** & **DS** & **Global** & **Local** & **DS** & **Global** \\  LSTM & 13.8\% & 15.1\% & 17.2\% & 19.2\% & 17.8\% & 22.2\% & 16.6\% & 11.3\% & 14.6\% \\  Llama2-13B & 27.3\% & 24.4\% & 21.8\% & 33.6\% & 31.5\% & 33.6\% & 32.4\% & 28.3\% & 32.7\% \\  Mistral-8x7B & 36.9\% & 35.8\% & 21.6\% & 34.8\% & 31.2\% & 37.8\% & 36.4\% & 34.8\% & 35.7\% \\  Llama3-80B & 38.2\% & 34.1\% & 26.7\% & 37.6\% & 35.2\% & 36.1\% & 39.1\% & 32.3\% & 40.2\% \\  Claude3.5-Sonnet & 53.2\% & 49.7\% & 55.8\% & 55.9\% & 53.1\% & 49.6\% & 56.3\% & 52.2\% & 55.9\% \\  GPT-3.5 & 44.5\% & 45.8\% & 48.3\% & 48.5\% & 42.9\% & 44.3\% & 48.4\% & 43.5\% & 49.7\% \\  GPT-4o & 54.6\% & 51.3\% & 58.6\% & 58.4\% & 54.1\% & 56.2\% & 60.5\% & 57.3\% & 58.5\% \\   

Table 4: Second-level classification accuracy with full manuscript input (Ten-sentence input for LSTM). We abbreviate ”Discipline Specific” as ”DS”.

  
**Models** & **Context Length** & **Overall** & **Variable** & **Constant** & **Operator** & **Unit Descriptor** \\   & One Sentence & 18.7\% & 24.5\% & 13.2\% & 10.3\% & 27.1\% \\  & Ten Sentences & 22.6\% & 28.1\% & 16.8\% & 15.5\% & 30.2\% \\  & Full Manuscript & - & - & - & - & - \\   & One Sentence & 36.8\% & 24.1\% & 39.3\% & 41.4\% & 42.7\% \\  & Ten Sentences & 42.7\% & 35.6\% & 39.8\% & 46.9\% & 48.5\% \\  & Full Manuscript & 45.9\% & 38.2\% & 42.8\% & 50.1\% & 52.4\% \\   & One Sentence & 47.3\% & 38.5\% & 41.7\% & 52.9\% & 56.2\% \\  & Ten Sentences & 49.8\% & 41.8\% & 45.9\% & 58.6\% & 56.7\% \\  & Full Manuscript & 53.6\% & 45.7\% & 48.9\% & 61.4\% & 58.2\% \\   & One Sentence & 48.9\% & 41.3\% & 44.6\% & 48.5\% & 61.5\% \\  & Ten Sentences & 53.0\% & 44.8\% & 48.8\% & 54.7\% & 63.7\% \\  & Full Manuscript & 51.7\% & 42.7\% & 43.2\% & 55.2\% & 65.8\% \\   & One Sentence & 63.7\% & 58.6\% & 62.5\% & 65.7\% & 67.8\% \\  & Ten Sentences & 65.9\% & 61.3\% & 64.3\% & 67.9\% & 70.2\% \\  & Full Manuscript & 66.7\% & 62.9\% & 65.8\% & 68.6\% & 69.3\% \\   & One Sentence & 56.8\% & 51.5\% & 53.5\% & 59.4\% & 62.4\% \\  & Ten Sentences & 58.7\% & 54.5\% & 53.6\% & 61.3\% & 65.1\% \\   & Full Manuscript & 60.6\% & 57.2\% & 56.6\% & 63.2\% & 65.2\% \\   & One Sentence & 64.9\% & 60.5\% & 64.2\% & 64.9\% & 70.1\% \\   & Ten Sentences & 67.4\% & 63.7\% & 66.1\% & 66.4\% & 73.5\% \\   & Full Manuscript & 68.5\% & 64.2\% & 67.8\% & 68.1\% & 73.8\% \\   

Table 3: First-level classification accuracy with various context lengths.

### First-Level Classification Results.

Table 3 presents the accuracy results for different models across varying context lengths. The result shows that the small-parameter-size language model such as the LSTM struggles with lower accuracy, achieving between 18.7% and 22.6%. In contrast, larger models, such as Llama2-13B and Mistral-8x7B, show marked improvements as context length increases, with Mistral-8x7B reaching up to 53.6% on the full manuscript. In addition, Claude3.5-Sonnet achieves comparable performance with GPT-4o across all context lengths, with accuracy consistently above 63.7% and up to 66.7%. GPT-based models exhibit stronger performance overall, with GPT-3.5 achieving between 56.8% and 60.6%. GPT-4o further improves across all context lengths, outperforming other models with an overall accuracy of 68.5% with the full manuscript input. We observe that the performance gap between models remains consistent as context length increases. For instance, GPT-4o outperforms Llama3-80B by 16.0%, 14.4%, and 16.8% for context lengths of one sentence, ten sentences, and the full manuscript, respectively. This consistent performance gap suggests that larger models with more pre-trained knowledge, such as GPT-4o and Claude3.5-Sonnet, exhibit superior scalability with longer contexts. These models are able to more effectively leverage extended context lengths to distinguish between mathematical symbols and other nuanced elements in the input prompts. On the other hand, the overall performance gain from increasing context length is more pronounced in smaller models, such as Llama2-13B and Mistral-8x7B, which have less pre-trained knowledge. These models benefit more from extended context as they rely on additional information to compensate for their limited pre-training. Larger models like GPT-4o and Claude3.5-Sonnet, which come with extensive pre-trained knowledge, show relatively smaller performance gains as context length increases.

### Second-Level Classification Results.

Table 4 shows second-level classification accuracy with full manuscript input. In this experiment, we assume that the model got the first-level classification correct. From the results, LSTM performs poorly, with an accuracy as low as 11.3% for predicting the DS. Larger models, like Llama2-13B and Mistral-8x7B, improve performance, especially in classifying Constants (up to 37.8%). Llama3-80B shows moderate improvements, with 40.2% accuracy for Global Operators, indicating reasonable capabilities in operator classification tasks. Claude3.5-Sonnet and GPT-3.5 show further improvements, particularly in Global Constants and Operators classification. GPT-3.5 achieves 48.5% accuracy for Local Constants and 49.7% for Global Operators. Lastly, GPT-4o provides the highest accuracy overall, reaching 60.5% for Local Operators and 58.6% for Matrix classification. By horizontally comparing the same model performance on different sub-attribute classifications, we find that the attribute Constants are generally easier to classify compared to Variables and Operators across all sizes of models, as seen by the overall higher accuracy in Constant-related tasks. However, Matrix and DS classification continue to present challenges, even for the largest models, indicating that certain structures or content types within manuscripts remain difficult to categorize accurately at the sub-attribute level.

**Overall,** performance across all models on both first-level and second-level classification tasks shows a clear trend of improvement with increasing context length, highlighting the importance of context for accurately classifying mathematical symbols. Additionally, both small and large-size language models show a relatively higher accuracy in identifying Unit Descriptors and Operators compared to Variables and Constants, indicating that symbols with more distinct contextual or syntactical patterns are easier for models to classify. Through the above results, we aim to gain insights into the extent to which different category attributes of mathematical symbols influence LLMs' understanding of math-rich documents by correctly classifying the symbols in real-world scenarios. We leave additional experiments in Appendix D.

## 4 Conclusion

In this paper, we introduce STEM-PoM, a comprehensive benchmark for evaluating language models' mathematical reasoning abilities to classify math symbols from scientific texts. The dataset includes over 2,000 math instances sourced from ArXiv papers. Extensive experiments show that the best-performing model, achieves only 73.8% and 60.5% for first and second-level Part-of-Math Tagging classification accuracy, highlighting the challenge of extracting and categorizing mathematical symbols from large text corpora.