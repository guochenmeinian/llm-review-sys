# From Biased to Unbiased Dynamics:

An Infinitesimal Generator Approach

 Timothee Devergne

CSML & ATSIM, Istituto Italiano di Tecnologia

timothee.devergne@iit.it

&Vladimir R. Kostic

CSML, Istituto Italiano di Tecnologia

University of Novi Sad

vladimir.kostic@iit.it

Michele Parrinello

ATSIM, Istituto Italiano di Tecnologia

michele.parrinello@iit.it

&Massimiliano Pontil

CSML, Istituto Italiano di Tecnologia

AI Centre, University College London

massimiliano.pontil@iit.it

###### Abstract

We investigate learning the eigenfunctions of evolution operators for time-reversal invariant stochastic processes, a prime example being the Langevin equation used in molecular dynamics. Many physical or chemical processes described by this equation involve transitions between metastable states separated by high potential barriers that can hardly be crossed during a simulation. To overcome this bottleneck, data are collected via biased simulations that explore the state space more rapidly. We propose a framework for learning from biased simulations rooted in the infinitesimal generator of the process and the associated resolvent operator. We contrast our approach to more common ones based on the transfer operator, showing that it can provably learn the spectral properties of the unbiased system from biased data. In experiments, we highlight the advantages of our method over transfer operator approaches and recent developments based on generator learning, demonstrating its effectiveness in estimating eigenfunctions and eigenvalues. Importantly, we show that even with datasets containing only a few relevant transitions due to sub-optimal biasing, our approach recovers relevant information about the transition mechanism.

## 1 Introduction

Dynamical systems and stochastic differential equations (SDEs) provide a general mathematical framework to study natural phenomena, with broad applications in science and engineering. Langevin SDEs, the main focus of this paper, are widely used to simulate physical processes such as protein folding or catalytic reactions [see e.g. 47, and references therein]. A main objective is to describe the dynamics of the process, forecast its evolution from a starting state, ultimately gaining insights on macroscopic properties of the system.

In molecular dynamics, the motion of a molecule is sampled according to a potential energy \(U(x)\), where the state vector \(x\) represents the positions of all the atoms. Specifically, the Langevin equation \(dX_{t}=- U(X_{t})dt+ dW_{t}\) describes the stochastic behavior of the system at thermal equilibrium, where \(X_{t}\) is the random position of the state at time \(t\), the scalar \(\) is a multiple of the square root of the system's temperature, and \(W_{t}\) is a vector random variable describing thermal fluctuations (Brownian motion). Most often, the atoms evolve in metastable states that are separated by barriers which can hardly be crossed during a simulation. For instance, for a protein the free energy barrier between the folded and unfolded states is larger than thermal agitation, making the transition between thetwo states a rare event. Consequently, long trajectories need to be simulated before such interesting events are observed. In fact, one needs to observe many events to get the relevant thermodynamics (free energy) and kinetics (transition rates) information . Beyond molecular dynamics, the slow mixing behavior of many systems modeled by SDEs is a major bottleneck in the study of rare events, and so designing methodologies which can accelerate the process is paramount.

A general idea to overcome the above problem is to perturb the system dynamics. One important approach which has been put in place in molecular dynamics is the so-called "bias potential enhanced sampling" [25; 44; 11]. The main idea is to add to the potential energy a bias potential \(V\), thereby lowering the barrier and allowing the system state to be explored more rapidly. To make this approach tractable in large systems, \(V\) is often chosen as a function of a few wisely selected variables called collective variables (CVs). For instance, if a chemical reaction involves a bond breaking, physical intuition suggests to choose the distance between the reactive atoms [26; 29]. However, for complex processes, hand-crafted CVs might be "suboptimal", meaning that some of the degrees of freedom important for the transition are not taken into account, making the biasing process inefficient.

In recent years, machine learning approaches have been employed to find the most relevant CVs [8; 42; 14; 9; 10; 4; 27]. A key idea is to use available dynamical information to construct the CVs [41; 31; 7; 42]. For instance, if one can identify the slowest degrees of freedom of the system, one can accurately describe the transitions between metastable states. These approaches are based on learning the transfer operator of the system, which models the conditional expectation of a function (or observable) of the state at a future time, given knowledge of the state at the initial time. It is learned from the behavior of dynamical correlation functions at large lag times which reflects the slow modes of the system. The leading eigenfunctions of the learned transfer operator can then be used as CVs in biased simulations. Moreover, they provide valuable insights into the transition mechanism, such as the location of the transition state ensemble . Still, this approach suffers from the same shortcoming described above, namely if the system is slowly mixing, long trajectories are needed to learn the transfer operator and extract good eigenfunctions.

More recently, there has been growing interest in learning the infinitesimal generator of the process [15; 1; 50; 20], which allows one to overcome the difficult choice of the lag-time. The statistical learning properties of generator learning have been addressed in , where an approach based on the resolvent operator has been proposed in order to bypass the unbounded nature of the generator. However the key difficulty of learning from biased simulations remains an open question. In this work, we prove that the infinitesimal generator is the adequate tool to deal with dynamical information from biased data. Leveraging on the statistical learning considerations in [23; 21], we introduce a novel procedure to compute the leading eigenpairs of the infinitesimal generator from biased dynamics, opening the doors to numerous applications in computational chemistry and beyond.

**Contributions** In summary, our main contributions are: **1)** We introduce a principle approach, based on the resolvent of the generator, to extract dynamical properties from biased data; **2)** We present a method to learn the generator from a prescribed dictionary of functions; **3)** We introduce a neural network loss function for learning the dictionary, with provable learning guarantees; **4)** We report experiments on popular molecular dynamics benchmarks, showing that our approach outperforms state-of-the-art transfer operator and recent generator learning approaches in biased simulations. Remarkably, even with datasets containing only a few relevant transitions due to sub-optimal biasing, our method effectively recovers crucial information about the transition mechanism.

**Paper organization** In Section 2, we introduce the learning problem. Section 3 explores limitations of transfer operator approaches. In Section 4, we review a recent generator learning approach  and adapt it to nonlinear regression with a finite dictionary of functions. Section 5 presents our method for learning from biased dynamics. Finally, in Section 6, we report our experimental findings.

## 2 Learning dynamical systems from data

In this section, we address learning stochastic dynamical systems from data. After introducing the main objects, we review existing data-driven approaches and conclude with practical challenges. We ground the discussion in the recently developed statistical learning theory, [22; 23; 24], contributing in particular to the existence of physical priors and feasibility of data acquisition for successful learning.

[MISSING_PAGE_FAIL:3]

We stress that transfer operator approaches crucially relies on the definition of the time-lag \(t\) from which dynamics is observed. Setting this value is a delicate task, depending on the events one wants to study. If \(t\) is chosen too small, the cross-covariance matrices will be too noisy for slowly mixing processes. On the other hand, if \(t\) is too large, because the relevant phenomena occur at large time scales, a very long simulation is needed to compute the covariance matrices. In order to overcome this problem biased simulations can be used, which we discuss next.

## 3 Learning from biased simulations

As discussed above, in molecular dynamics, the desired physical phenomena often cannot be observed within an affordable simulation time. To address this, one solution is to modify the potential,

\[U^{}(x):=U(x)+V(x),\;\;x\]

where we assume that the introduced perturbation (a form of bias in the data) \(V(x)\) is known. For example the bias potential \(V\) may be constructed from previous system states to promote transitions to not yet visited regions. One of the prototypical examples is metadynamics , where \(V\) is a sum of Gaussians built on the fly in order to reduce the barrier between metastable states. However, the bias potential alters the invariant distribution , making it challenging to recover the unbiased dynamics from biased data. Denoting the invariant measure of the perturbed process by \(^{}\) and its generator by \(^{} H^{1,2}_{^{}}() H^{1,2}_{ ^{}}()\), our principal objective is thus to:

_Gather data from simulations generated by \(^{}\) to learn the spectral decomposition of the unperturbed generator \(\)._

To tackle this problem, we note that since the eigenfunctions of the generator \(\) are also eigenfunctions of every transfer operator \(_{t}=e^{t}\), we can address the related problem of learning the transfer operator from perturbed dynamics. Unfortunately, there is an inherent difficulty in doing so. While one _typically knows the perturbation_ in the generator, that is \(^{}=+ V,(),\) this knowledge is not easily transferred to the perturbation of the transfer operator. Indeed, recalling that \(:=_{1}=e^{}\), and since the differential operator \( V,()\) in general does not share the same eigenstructure of \(\), one has that

\[^{}:=e^{^{}}=e^{- V,()}e^{- V,()}.\]

Simply put, the generator depends linearly on the bias, while the transfer operator does not. One strategy to overcome the data distribution change, is to _adapt_ the notion of the risk. To discuss this idea, recall that the invariant distribution of overdamped Langevin dynamics is the Boltzmann distribution defined by the potential. Hence, we have that

\[(dx)=dx}{ e^{- U(x)}dx},\;^{}(dx)= (x)}dx}{ e^{- U^{}(x)}dx}\;\;\;\;}(x)=}{ e^{ V(x)}^{ }(dx)}\] (7)

where the last term is the Radon-Nikodym derivative, which exposes the data-distribution change. Consequently, we can express the covariance operators for the unperturbed process as weighted expectations of the perturbed data features

\[\!=\!_{X^{}^{}}[( X^{})z(X^{})z(X^{})^{}].\] (8)

However, since the transition kernel of the process \((X^{}_{t})_{t 0}\) generated by \(^{}\) is different from that of the original process, the above reasoning does not hold for the cross-covariance matrix, that is,

\[_{t}:=\!_{X_{0}^{}}[}(X_{0})\,z(X_{0})z(X_{t})^{}]_{X^{ }_{0}^{}}[}(X^{}_{0})\,z(X^ {}_{0})z(X^{}_{t})^{}]=:^{}_{t},\]

Consequently, the estimator \(_{t}\) obtained by minimizing the reweighed risk functional \(^{}(_{t}):=_{X_{0}^{}} }(X_{0})\,\|z(X^{}_{t})-}^{}_{t}z(X_{0})\|^{2}_{2}\)_does not_ minimize the true risk since \(^{}(_{t})(_{t})\). Despite this difference, whenever the perturbation is small or controlled and the _time-lag \(t\) is small enough_, estimating the true transfer operator of the process from the perturbed dynamics via reweighed covariance/cross-covariance operators has been systematically used as the state-of-the art approach in the field of atomistic simulations [7; 9; 31; 49]. The (limited) success of such approaches is based on a delicate balance of a small enough lag-time and biased potential, since for small \(t>0\) one can approximate \(_{t}\) by \(^{}_{t}\) and minimize \(^{}(T)(T)\) over \(T\).

Infinitesimal generator learning

In this section, we address generator learning. While there has been significant progress on this topic [24; 15; 35; 50; 20], we follow the recent approach in  for learning the generator \(\) on an a priori fixed hypothesis space \(\) through its resolvent. Leveraging on its strong statistically guarantees, we adapt it from kernel regression to nonlinear regression over a dictionary of basis functions, setting the stage for the development of our deep-learning method.

While transfer operator learning does not require any prior knowledge of the system's drift and diffusion, making use of this information helps learning the generator and avoids the need for setting the time lag parameter. We briefly discuss how to achieve this for over-damped Langevin processes when the constant diffusion term is known. We estimate the generator _indirectly_ via its resolvent \(( I-)^{\!-\!1}\), where \(>0\) is a prescribed parameter. To this end, we observe that the action of the resolvent in \(\) can be expressed as \((( I-)^{\!-\!1}h_{})(x)=_{}(x)^{}u\), where \(_{}\) is the embedding of the resolvent \(( I-)^{\!-\!1}\) into \(\), given by \(_{}(x)=_{0}^{}[z(X_{t})e^{- t}\,|\,X_{0}{=}x] dt,x\), see . We then aim to approximate \(_{}(x)^{*}z(x)\) by a matrix \(^{m m}\). Unfortunately the embedding of the resolvent is not known in close form. To overcome this, we contrast the resolvent by defining a _regularized energy kernel_\(_{}^{} H_{}^{1,2}() H_{}^{1,2}( )\), given by \(_{}^{}[f,g]\!=\!_{x}[ f(x)g(x)-f (x)[g](x)]\), which using (3) becomes

\[_{}^{}[f,g]\!\!=\!\!_{x}[ f(x)g(x )\!+\!f(x) U(x)^{} g(x)\!-\!f(x) g (x)],\] (9)

and, due to the identity \( fgd=-^{-1}( f)^{}( g)d\), also

\[_{}^{}[f,g]\!\!=\!\!_{x}[ f(x)g( x)\!+\!\!_{k[d]}\!_{k}f(x)_{k}g(x) ].\] (10)

Since \(\) is negative semi-definite, the above kernel induces the _regularized squared energy norm_\(_{}^{} H_{}^{1,2}()[0,+)\) by \(_{}^{}[f]:=_{}^{}[f,f]=_{x }[ f^{2}(x)-f(x)[f](x)]\). It counteracts the resolvent and balances the transient dynamics (energy) of the process with the invariant distribution \(\). In a nutshell, instead of using the mean square error of \(f(x):=\|_{}(x)-^{}z(x)\|_{2}\) to define the risk, we _"fight fire with fire"_ and penalize the energy to formulate the _generator regression problem_

\[_{G}\,_{}(G) _{}():=_{}^{}\|_{ }()\!-\!}^{}z()\|_{2}.\] (11)

Indeed, this risk overcomes the difficulty of not knowing \(_{}\). To show this, let us define the space \(_{}^{}():=\{f H_{}^{1,2}()\,| \,_{}^{}[f]<\}\) associated to the energy norm \(\|f\|_{_{}^{}}:=_{}^{}[f]}\), and recalling that the operator \(G\) is identified with a matrix \(^{m m}\) via \(Gh_{u}=z()^{}(u)\), define the (injection) operator \(^{m}_{}^{}\) by \(u=z()^{}u\), for every \(u^{m}\). Then, since \((^{m},_{}^{})(,_{}^{})\), the norm is the sum of squared \(_{}^{}\) norm over the standard basis in \(^{m}\), and one obtains

\[_{}(G) =\|( I\!-\!)^{\!-\!1}-G\|_{(,_{}^{})}^{2}\] \[=}( I\!-\!)^{\!-\!1}-G \|_{(,_{}^{})}^{2}}_{}+})( I\!-\!)^{\!-\!1}\|_{ (,_{}^{})}^{2}}_{()},\] (12)

where \(P_{}\) is the orthogonal projector in \(_{}^{}()\) onto \(\). In learning theory \(()\) is known as the approximation error of the hypothesis space \(\) [see e.g. 43]. While this error may vanish for infinite-dimensional spaces, when \(\) is finite dimensional, controlling \(()\) is crucial to achieving statistical consistency. This can be accomplished by minimizing (11), which is equivalent to

\[_{G}\|P_{}( I-)^{ \!-\!1}\!-G\|_{(,_{}^{})}^{2}\!\!=\!\! \|(^{*})^{\!1}^{*}( I-)^{\!-\!1}-\|_{(^{m}, _{}^{})}^{2}\] (13)

where \(()^{\!}\) is the Moore-Penrose's pseudoinverse. Using the covariance matrices

\[^{*}( I\!-\!)^{\!-\!1}==( _{x}[z_{i}(x)z_{j}(x)])_{i,j[m]},=^{*}=(_{}^{}[z_{i},z_{j} ])_{i,j[m]},\] (14)

w.r.t. the invariant distribution and energy, respectively, gives the ridge regularized (RR) solution \(=(+)^{\!-\!1}\), \(>0\). The induced RR estimator of the resolvent, \(G_{,}:\) is given, for every \(h_{u}\), by \(G_{,}h_{u}:=(+)^{\!-\!1}u= z()^{}(+)^{\!-\!1}u\), and it can be estimated given data from \(\) by replacing expectation and the energy in (14) with their empirical counterparts.

Unbiased learning of the infinitesimal generator from biased simulations

In this section, we present the main contributions of this work: approximating the leading eigenfunctions (corresponding to the slowest time scales) of the infinitesimal generator from biased data.While the general pipeline for the method can be found in figure 1, in the following, we first address regressing the generator on an a priori fixed hypothesis space \(\). Then we introduce our deep-learning method to either build a suitable space \(\), or even directly learn the eigenfunctions.

Unbiasing generator regressionWhenever \(\) is absolutely continuous w.r.t. \(^{}\), the regularized energy kernel (9) satisfies the simple identity

\[_{}^{}[f,g]=_{^{}}^{}f}},g}}, f,g H_{}^{1,2}(),\] (15)

which, recalling the rightmost equation in (7), implies that when the bias \(V\) and the diffusion coefficient \(\) are known, the energy kernel can be empirically estimated through samples from \(^{}\) via (10). Moreover, when the potential \(U\) is known too, we can use (9). Now, leveraging on (15) we directly obtain that

\[_{}(G)_{}()\!\!=\! \!_{x^{}}^{}\|_{}(x)\!-\!}^{}\!z(x)\|_{2}}(x)} _{V}_{x^{}}^{}\|_{}(x)\!-\! }^{}\!z(x)\|_{2}\] (16)

where \(_{V}=_{x^{}}(x)\), which recalling (7) is finite whenever the bias \(V\) is essentially bounded. Therefore, in sharp contrast to transfer operator learning, whenever the true embedding \(_{}(x)\) can be estimated, one can derive principled estimators of the true generator \(\)'s dominant eigenpairs from the biased dynamics generated by \(^{}\). This is established by the following proposition, the proof of which is presented in Appendix B.

**Theorem 1**.: _Let \(_{n}=(x^{}_{i})_{i[n]}\) be the biased dataset generated from \(^{}\). Let \(w(x)=e^{ V(x)}\) and define the empirical covariances w.r.t. the empirical distribution \(^{}=n^{-1}_{i[n]}_{x^{}_{i}}\) by_

\[}=_{x^{}^{ }}[w(x^{})z_{i}(x^{})z_{j}(x^{})]_{i,j[m]} }=_{^{ }}^{}[z_{i},z_{j}]_{i,j[m]}.\] (17)

_Compute the eigenpairs \((_{i},v_{i})_{i[m]}\) of the RR estimator \(}_{,}=(}+)^{-1}}\), and estimate the eigenpairs in (4) as \((_{i},_{i})\!=\!(\!-\!1/_{i},z()^{ }\!v_{i})\). If the elements of \(\) and their gradients are essentially bounded, and \(_{m}()\!=\!0\), then for every \(>0\), there exist \((m,n,)\!\!_{+}\), such that, for every \(i[m]\), \(|_{i}-_{i}|\) and \(_{L_{}^{2}}((f_{i},_{i}))\!\!\), with high probability._

Note that, due to the form of the estimator, the normalizing constant \( w(x)dx\) does not need be computed. Moreover, relying on the upper bound in (16) we can alternatively compute \(}\) and \(}\) without the weights \(w\) and still ensure that the above result holds true.

Neural network based learningTheorem 1 guarantees successful estimation of the eigendecomposition of the generator in (4) whenever the energy-based _representation error_\(()\) in (12) is controlled. It is therefore natural to minimize \(()\) by choosing an appropriate basis function \(z_{i}\)'s. Inspired by the recent work , we parameterize them by a neural network, and optimize them to span the leading invariant subspace of the generator.

Let \(z^{}=(z^{}_{i})_{i[m]}^{m}\) be a neural network (NN) embedding parameterized by \(\!\!\) weights with continuously differentiable activation functions, and let \(^{}_{i}\), \(i[m]\), be real non-positive

Figure 1: Pipeline of our method: from biased simulations to timescales and metastable states.

(trainable) weights. We propose to optimize the NN to find the slowest time-scales \(^{}_{i}\) that solve the eigenvalue equation \(z^{}_{i}\!=\!^{}_{i}z^{}_{i}\), \(i\!\![m]\). Letting \(_{}\!:^{m}^{n}_{}()\) be the (parameterized) injection operator, given, for every \(u^{m}\) by \(_{}u\!=\!_{i[m]}z^{}_{i}u_{i}\), and denoting \(^{}_{}\!=\!( I\!-\!(^{}_{1}, ,^{}_{m}))^{-1}\), the eigenvalue equations for the resolvent then become \(( I-)^{-1}_{}\!=\!_{} ^{}_{}\). In other words, we aim to find the best rank-\(m\) decomposition of resolvent \(( I\!-\!)^{-1}\!\!_{}^{}_{ }^{*}_{}\). Therefore, for some hyperparameter \(\!\!0\) we introduce the loss

\[_{}():=\|( I\!-\!)^{-1}-_{ }^{}_{}^{*}_{}\|^{2}_{( ^{n}_{})}-\|( I\!-\!)^{-1}\|^{2}_{( ^{n}_{})}+\!_{i,j[m]}( z^{}_{i},z^{ }_{j}_{L^{2}_{}}\!-\!_{i,j})^{2}.\]

While the first term measures the approximation error in the energy space, it cannot be used as a loss, because the action of the resolvent is not known. To mitigate this, the second term is introduced, under the assumption that \(( I\!-\!)^{-1}(^{}_{}( ))\) (see Appendix C for a discussion). The third term is optional; specifically, if the goal is not only to identify the proper invariant subspace of the generator (\(=0\)), but also to optimize the neural network to extract eigenfunctions as features, then this last term (\(>0\)) encourages the orthonormality of features in \(L^{2}_{}()\), an idea successfully exploited in machine learning and computational chemistry [see e.g. 24, and references therein].

Recalling (14) and denoting by \(_{}\) and \(_{}\) the covariance matrices associated to the parameterized features, after some algebra, we obtain that

\[_{}()=[_{}^{ }_{}_{}^{}_{}-2_{} ^{}_{}+(_{}-)^{2}].\] (18)

In turn, this can be estimated from biased data by two independent samples \(^{}_{1}\) and \(^{}_{2}\) as

\[_{}^{^{}_{1},^{}_{2}} ()\!=\!\!(}^{1}_{}^{ }_{}}^{2}_{}^{}_{}\!+\! }^{2}_{}^{}_{}}^ {1}_{}^{}_{})/2\!-\!^{1} }^{2}_{}^{}_{}\!-\!_{2} }^{1}_{}^{}_{}\!+\!(}^{1}_{ }\!-\!_{1})(}^{2}_{}\!-\! _{2}),\] (19)

where \(}^{k}_{}\) and \(}^{k}_{}\) are the empirical covariances given by (17) for distribution \(^{}_{k}\), while \(^{k}=_{x^{}^{}_{k}x}w(x^{ })\), \(k\!\!\). Importantly, the computational complexity of the loss (19) is of the order \((nm^{2}d)\), where \(d\) is the state dimension and \(n\) the sample size, however it can be reduced to \((nm\,d)\) (see Appendix C) allowing its application to learn large dictionaries for high-dimensional problems with big amounts of (biased) data.

The following result, linked to controlling of the representation error as detailed in Theorem 1, provides theoretical guarantees for our approach. The proof and discussion are provided in Appendix B.

**Theorem 2**.: _Given a compact operator \(( I\!-\!)^{-1}\), \(>0\), if \((z^{})_{i[m]}^{}_{}()\) for all \(\!\!\), then_

\[\![^{^{}_{1},^{ }_{2}}_{}()]=^{2}\,_{}( )-\!_{i[m]}\!^{2}}{(-_{i})^{2}}, ,\] (20)

_where \(=_{x}[w(x)]\). Moreover, if \(>0\) and \(_{m+2}<_{m+1}\), then the equality holds if and only if \((^{}_{i},z^{}_{i})\!=\!(_{i},f_{i})\)\(\)-a.e., up to the ordering of indices and choice of eigenfunction signs for \(i\!\![m]\)._

This theorem provides a justification for minimizing the loss in (19), which can be achieved by stochastic optimization algorithms, to obtain an approximation of either the leading invariant subspace of the resolvent \(( I\!-\!)^{-1}\) (without orthonormality loss, i.e. \(=0\)), on which the estimator in Theorem 1 can be computed, or even the individual eigenpairs (\(>0\)). A pseudocode of our method is provided below. The main advantage of this method is that it exploits the knowledge of the process. namely, if only the bias \(V\) and the diffusion coefficient \(\) are known, recalling (10), the computation of loss relies just of the gradient of the features. On the other hand, the knowledge of the potential can also be exploited via (9). Finally, even if the neural network features are not perfectly learned, one can still resort to Theorem 1 to compute the approximate eigendecomposition of \(\).

## 6 Experiments

In this section, we test the method described above on well-established [14; 9; 32; 36] molecular dynamics benchmarks, featuring biased simulations of increasing complexity. We first start by showing the efficiency of our method on a simple one dimensional double well potential. We then proceed to the Muller-Brown potential which is a 2D potential, where this time, sampling is accelerated by a bias potential built on the fly. Finally, we study the conformational landscape of alanine dipeptide. This small molecule is a classical testing ground for rare event methods. To showcase the efficiency of our method we analyse two different sets of data both generated in a metadynamics-like approach and showcase the efficiency of our approach, even with a small number of transitions in the training set. The codes used to train the models can be found in the following repository: https://github.com/DevergneTimothee/GenLearn

**One dimensional double well potential** We first showcase the efficiency of our method on a simple one dimensional toy model. We sample transitions from \(U+V\), where \(U\) is a double well potential and \(V\) is a bias potential. The results are shown Figure 7 in the appendix, where our method clearly outperforms transfer operator approaches and recovers the true underlying dynamics.

**Muller Brown potential with metadynamics biasing** Muller Brown is a 2 dimensional potential presenting metastable states often studied in the context of enhanced sampling . It presents two minima, with one of them separated into two sub-basins. We thus expect two relevant eigenpairs: the slowest one corresponding to the transition between the two basins and the second slowest one describing the transition between the two sub-basins. However, at low temperature crossing the barrier occurs rarely. To expedite the rate of transition we use metadynamics and instead of having a predefined bias potential, as in the previous section, the bias is built on the fly using metadynamics . The results of the training procedure are presented in Figure 2. We compare the results with deepTICA and a state of the art generator learning approach in . From this figure, we see that we managed to accurately learn the dynamical behavior of the system despite the fact that the dynamics was performed using a bias potential. As expected, it is clearly outperforming transfer operator approaches. We achieve similar or slightly better results (particularly near the transition state) on the qualitative shape of the eigenfunctions. On the other hand, our method performs better than previous work on generator learning on the estimation of eigenvalues, and is the closest to the ground truth eigenvalues. This is likely to be due to the fact that the method in  requires the tuning of hyperparameters in the loss function, while in our case, these coefficients are trainable. It should be noted that here, the eigenfunctions were fitted with well-learned features. However, we present in the appendix results where the features are not perfectly learned, but we still manage to recover the eigenfunctions.

**Alanine dipeptide with OPES biasing** We next treat a more concrete molecular dynamics example with the study of the conformational change of alanine dipeptide in gas phase. It is a molecule containing 22 atoms, of which 10 are heavy. For the remaining of this study, we will only take into account the positions of the heavy atoms, making it a 30 dimensional system. This molecule has widely been used to test methods in enhanced sampling : it presents a conformational change which is a rare event described by the angles \(\) and \(\). In the studies made on this system, the angle \(\) has been shown to be a good CV: the transition between the two states is very well described, and thus a bias potential can easily be built with this CV. On the other hand, the angle \(\) misses most of the transition and is a non optimal CV. We generated biased dataset using a variation of metadynamics called on the fly probability enhanced sampling (OPES) , which allows a more extensive and faster exploration of the state space than metadynamics :

_Dataset 1:_ 800ns simulation, biasing on the \(\) dihedral angle, with OPES leading to few transitions between the two states. The bias potential was built during the first 100ns of the simulation. For the remaining 700ns, the potential built during the first part was kept fixed to enhance transitions.

_Dataset 2:_ 50ns simulation, biasing on \(\) dihedral angle, with OPES leading to many transitions between the two states. The bias potential was built during the first 20ns of the simulation. For the remaining 30ns, the potential built during the first part was kept to enhanced transitions.

Dataset 1 mimics situations where one has only a basic prior knowledge of the system: only a "suboptimal" CV is used yielding to only a few transitions between the metastable states within the affordable simulation time. In order to ensure translational and rotational invariant vectors, we use Kabsch  algorithm, which has been used in previous studies  to transform the positions of the atoms. The results are presented in Figure 3. Panels **a)** and **b)** display the first and second eigenfunctions learned by our method respectively. Notice that, even though only 2 transitions are present in dataset 1, the first eigenfunction separates the two metastable states, and the second identifies a faster transition in one metastable state. Panel **c)** showcases the good out-of-sample generalization ability of the method. It visualizes the first eigenfunction obtained as above, but this time visualized on points from dataset 2 and in the plane of dihedral angles \(\) and \(\). Interestingly, we discover that a linear relationship is present in the transition region, in agreement with recent findings in the molecular dynamics literature .

To further improve the description of the transition and to enhance the training set without any prior knowledge of the mechanism, one could perform biased simulations using the first eigenfunction. Nonetheless, this is not the scope of this paper. To push our method further and see its capabilities when training on a good dataset, we trained it on Dataset 2. One key quantity in molecular dynamics is the committor function for metastable states A and B, which is defined as the probability of, starting from A, going to B before going back to A. Theory tells us that the committor is linearly related to the first eigenfunction of the generator, a result going back to Kolmogorov [see 5, for a discussion]. This relation is exposed in panel **d)** of Figure 3, when comparing to the committor model obtained in  indicating the good performance of our method.

**Chignolin miniprotein** In this section, we report the results of our method obtained on a larger scale experiment: the folding/unfolding mechanism of the chignolin miniprotein. This system has

Figure 2: Muller Brown potential. Comparison of the ground truth two first relevant eigenfunctions of the potential (**first column**) with this work (**second column**), transfer operator approach deepTICA  (**third column**) and the work of Zhang et al.  (**fourth column**). x and y axis are the coordinates of the system and points are colored according to the value of the eigenfunction. The underlying potential is represented by the level lines in white. Associated eigenvalues \(\) are also reported.

extensively been studied [46; 19; 39; 7]. We first performed a 1 \(\)s biased simulation using the deep-TDA collective variable [46; 37] to gather transitions. Then we chose descriptors as input of the neural networks that are known to describe well the folding process . Finally, we trained the method described in the current work with this trajectory and compared it with the results obtained when training on a \(106\)s unbiased trajectory provided by D.E. Shaw research . The results are presented in figure 4, showing a very good agreement between the training on an unbiased trajectory and on a biased one.

## 7 Conclusions

We presented a method to learn the eigenfunctions and eigenvalues of the generator of Langevin dynamics from biased simulations, with strong theoretical guarantees. We contrasted this approach with those based on the transfer operator and a recent generator learning approach based on Rayleigh quotients. In experiments, we observed that our approach is effective even when trained from suboptimal biased simulations. In the future our method could be applied to larger-scale simulations to discover rare events such as protein-ligand binding or catalytic processes. A main limitation of our method is that, in its current form, it is formulated for time-homogeneous bias potentials. However, the proposed framework could be naturally extended to time-dependent biasing, broadening its applicability in computational chemistry. Furthermore, given the quality of our results on alanine dipeptide, in the future, we can use our method to compute accurate eigenfunctions from old, possibly poorly converged, metadynamics simulations, thereby gaining novel and more accurate physical information.

Figure 4: Our method for the chignolin miniprotein. The data points are represented in the plane of the distance between the nitrogen atom of the residue 3: ASP (ASP3N) and the oxygen atom of the residue 7: Gly (Gly7O) and the distance between ASP3N and the oxygen atom of residue 8: THR (THR8) which allow visualizing the folded and unfolded states.

Figure 3: Alanine Dipeptide. Results of our method trained on Dataset 1 **a)** and **b)** first and second eigenfunctions represented on dataset 1, in the plane of the \(\) and \(\) dihedral angles. **c)** first eigenfunction represented on dataset 2, in the plane of the \(\) and \(\) dihedral angles, indicating that our method is effective even when trained from poor CVs (see text for more discussion). On all three panels, points are colored according to the value of the eigenfunction. **d)** Comparison of our method with the committor (\(q\)) of