# Exponential Hardness of Optimization from the Locality in Quantum Neural Networks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Quantum neural networks (QNNs) have become a leading paradigm for establishing near-term quantum applications in recent years. The trainability issue of QNNs has garnered extensive attention, spurring demand for a comprehensive analysis of QNNs in order to identify viable solutions. In this work, we propose a perspective that characterizes the trainability of QNNs based on their locality. We prove that the entire variation range of the loss function via adjusting any local quantum gate vanishes exponentially in the number of qubits with a high probability for a broad class of QNNs. This result reveals extra harsh constraints independent of gradients and unifies the restrictions on gradient-based and gradient-free optimizations naturally. We showcase the validity of our results with numerical simulations of representative models and examples. Our findings, as a fundamental property of random quantum circuits, deepen the understanding of the role of locality in QNNs and serve as a guideline for assessing the effectiveness of diverse training strategies for quantum neural networks.

## 1 Introduction

Quantum computing is a rapidly growing technology that exploits quantum mechanics to solve intricate problems that classical computers cannot solve. With enormous efforts having been made to develop noisy intermediate scale quantum (NISQ) devices , current quantum devices have demonstrated the ability to achieve near-term quantum advantage for practical applications in key areas including many-body physics [2; 3; 4], chemistry , finance [6; 7; 8], and machine learning . Specifically, quantum machine learning (QML) represents an exciting, emerging interdisciplinary field that seeks to enhance machine learning algorithms by harnessing the inherent parallelism of quantum systems [10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20]. Quantum neural networks (QNNs) stand at the forefront of QML, capitalizing on the unprecedented potential of quantum computing to revolutionize data analysis and pattern recognition. Inspired by classical neural networks, QNNs employ quantum gates and quantum states as fundamental building blocks within their computational framework. These networks can be trained using a diverse range of methods, including gradient-based optimization techniques akin to classical neural network training [21; 22; 23; 24].

With the aim to show quantum advantage on certain tasks, a critical issue is whether QNNs can be extended to solve large-scale systems, i.e., scalability. Unfortunately, many studies point out that training of QNNs requires exponential resources with the system size under certain conditions [25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36]. Besides the practical limitations such as noises , even ideal quantum devices will suffer from the so-called _barren plateau_ phenomenon , which is the quantum counterpart of vanishing gradient problem in classical machine learning. It was shown that the gradient of the cost function vanishes exponentially in the number of qubits with a high probability for a random initialized QNN with sufficient depth, analogous to the vanishing gradient issue in classical neural networks.

Consequently, exponentially vanishing gradients demand exponential precision in the cost function measurement on a quantum device  to make progress in the gradient-based optimization, and hence an exponential complexity in the number of qubits.

Several attempts have been made to avoid barren plateaus, such as higher order derivatives , gradient-free optimizers including gate-by-gate optimization [39; 40], proper initialization , pre-training including adaptive methods [42; 43; 44; 45; 46], QNN architectures [47; 48] and cost function choices [49; 50]. More efforts are needed to study the general effectiveness of these attempts [26; 27] and develop new strategies to improve the trainability and scalability of QNNs. As a guide for exploring effective training strategies, it is crucial to uncover the essential mechanisms behind the barren plateau phenomenon.

However, few rigorous scaling results are known for generic QNNs besides phenomenological calculations, i.e., gradient analyses and their descendent [26; 27; 28]. Instead of just the limited information of vicinity from gradient analyses, it would be quite helpful for designing efficient algorithms if we could gain information on the entire variation range of the cost function when adjusting a single [39; 40] or several parameters. Combined with the fact that parameters usually enter the circuit independently through local quantum gates, all of which motivate our work where we are chiefly concerned with the variation range of the cost function via varying a local unitary within a quantum circuit.

In this work, we present a rigorous scaling theorem on the trainability of QNNs beyond gradients from the perspective of QNN locality. As summarized in Fig. 1, we prove that when varying a _local unitary_ within a sufficiently random circuit, the expectation and variance of the variation range of the cost function vanish exponentially in the number of qubits. Then through simple derivations, we show that this theorem implies exponentially vanishing gradients and cost function differences, and hence unifies the restrictions on gradient-based and gradient-free optimizations. Meanwhile, this theorem further delivers extra meaningful information about the training landscapes and optimization possibilities of QNNs. In this sense, we obtain a fundamental limitation on QNN training. Next, we illustrate the applications of our theorem on representative QNN models, where a tighter bound for the fidelity-type cost function is provided specifically even with shallow random circuits. At last, we perform numerical simulations on these representative models, where the scaling exponents coincide with our analytical results almost precisely.

Comparison with Previous Works.The advances of our results compared to previous works [25; 27; 26; 28] exist in two aspects. Firstly, the exponentially vanishing quantity we claim is the _entire_ variation range of the cost function in the _whole parameter subspace_ corresponding to the local unitary. This provides constraints on multiple parameters at finite intervals simultaneously, instead of an infinitesimal vicinity or two fixed-parameter points. Secondly, our results are irrelevant with the parameterization of the local unitary like \(e^{-i 0}\) used previously. Hence, our results are much more general whose only condition is the circuit locality and open a new avenue for analyzing the QNN trainability.

Figure 1: **Training limitations from QNN locality.** The left part depicts a PQC on \(n\) qubits composed of local unitaries. The right part symbolically depicts the cost function on a classical device vs. the local unitary highlighted in the left part. This work proves that the cost function will fluctuate in an exponentially small range in the number of qubits with a high probability when we vary an arbitrary local unitary within the QNN in certain cases.

Preliminaries

Quantum State.We first introduce basic concepts and notations in quantum computing. A pure single-qubit quantum state is a linear combination of two computational basis states, represented as \(|=|0+|1\) in Dirac notation, where \(,,||^{2}+||^{2}=1\). Here, \(|0\) and \(|1\) denote the basis states \(^{T}\) and \(^{T}\) in the single-qubit space \(^{2}\), respectively. The \(n\)-qubit space \(^{2^{n}}\) is formed by the tensor product of \(n\) single-qubit spaces. Additionally, the quantum state can be represented by a positive semidefinite matrix, also known as a density matrix. The density matrix \(\) of a pure state \(|\) consisting of \(n\) qubits is expressed as \(=|\!|\), where \(|=|^{}\). A general mixed quantum state is represented by \(=_{k}c_{k}|_{k}\!_{k}|\), where \(c_{k},_{k}c_{k}=1\).

Quantum Gate.Quantum gates are mathematically described as unitary operators. Common single-qubit gates include the Pauli rotations \(\{R_{P}()=e^{-iP}|P\{X,Y,Z\}\}\), which are in the matrix exponential form of Pauli matrices

\[X=0&1\\ 1&0, Y=0&-i\\ i&0, Z=1&0\\ 0&-1.\] (1)

Common two-qubit gates include controlled-X gate \(=I X\) (\(\) is the direct sum) and controlled-Z gate \(=I Z\), which can generate quantum entanglement among qubits.

Quantum Measurement.Quantum measurement is a quantum operation to obtain information from the quantum system. For example, for a single-qubit state \(|=|0+|1\), the outcome of a computational basis measurement is either \(|0\) with probability \(||^{2}\) or \(|1\) with probability \(||^{2}\). This measurement operation can be mathematically referred to as the average of the observable \(O=Z\) under the state \(|\): \(|O|=[Z|\!|]=||^{ 2}-||^{2}\). Generally, quantum observables \(O\) are Hermitian matrices and \((1/^{2})\) times of measurements could give an \(\|O\|_{}\)-error estimation to the value \([O]\), where \(\|\|_{}\) is the spectral norm of the matrix.

Quantum Neural Network.While classical neural networks operate on classical bits and use classical logic gates, quantum neural networks (QNNs) use quantum bits, or qubits, and quantum gates to process and store information. QNNs are often described as parameterized quantum circuits (PQCs) that are composed of rotation gates with adjustable rotating angles. In general, a QNN takes the mathematical form \(()=_{}U_{}(_{})W_{}\), where \(U_{}(_{})=e^{-i_{}_{}}\) denotes a parameterized gate, such as a single-qubit rotation gate with \(_{}\) representing a Hermitian operator, and \(W_{}\) corresponds to fixed gates like the CNOT gate and SWAP gate. Commonly used templates of QNNs include the hardware efficient ansatz, the alternating-layered ansatz, and the tensor-network-based ansatz [49; 51]. Note that QNNs with intermediate classical controls such as QCNNs  can also be included in this general form theoretically.

## 3 Limitations of Local Unitary Optimization in QNN

We start by introducing a general setting of a QNN model used throughout our analysis. A hybrid quantum-classical framework in QML usually uses a classical optimizer to train a QNN, denoted by \(\), with an input state \(\) by minimizing a task-dependent cost function \(C\), which is typically chosen as the expectation value of some Hermitian operator \(H\):

\[C_{H,}()=(H^{}).\] (2)

Note that other cost function forms can be regarded as compositions of observable expectations and some other classical post-processing functions. Here we focus on (2) for simplicity. Divide the whole qubit system into two parts \(A,B\) with \(m\) qubits and \(n-m\) qubits, respectively. Here \(m\) is a fixed constant not scaling with \(n\) so that we call \(A\) a local subsystem. The QNN \(\) is often composed of local unitaries on real devices, such as the single-qubit rotation gates and the CNOT gate. We focus on a local unitary \(U_{A}\) within \(\) acting on subsystem \(A\). As shown in Fig. 2, we denote the sub-circuit of \(\) before \(U_{A}\) as \(V_{1}\) and that behind \(U_{A}\) as \(V_{2}\), such that \(=V_{2}(U_{A} I_{B})V_{1}\) where \(I_{B}\) is the identity operator on \(B\). \(V_{1}\), \(V_{2}\) and \(U_{A}\) are independent of each other. We also remark that this circuit setting is sufficiently general to cover common representative QNN models, e.g., the variational quantum eigensolver, the quantum autoencoder, and the quantum state learning.

To characterize the training landscape beyond the limited information of the vicinity from gradient analyses, we introduce a central quantity throughout this work, i.e., the _variation range of the cost function_ via varying a local unitary.

**Definition 1**: _For a generic cost function \(C_{H,}()\) with a QNN \(\) in Eq. (2), we define its variation range with given \(V_{1},V_{2}\) as_

\[_{H,}(V_{1},V_{2}):=_{U_{A}}C_{H,}()-_{U_{A}}C _{H,}(),\] (3)

_where the maximum and minimum with respect to \(U_{A}\) are taken over the unitary group \((2^{m})\) of degree \(2^{m}\)._

The quantity \(_{H,}(V_{1},V_{2})\) intuitively reflects the maximal possible influence that the local unitary \(U_{A}\) can have on the cost function. We establish an upper bound on \(_{H,}(V_{1},V_{2})\) in the sense of probability by Theorem 1, which thus delivers a limitation on optimizing an arbitrary local unitary. To be specific, we prove that if either \(V_{1}\), \(V_{2}\), or both match the Haar distribution up to the second moment, i.e., are sampled from unitary 2-designs , the expectation of \(_{H,}(V_{1},V_{2})\) vanishes exponentially in the number of qubits. See Appendix A for preliminaries on unitary designs.

**Theorem 1**: _Suppose \(_{1},_{2}\) are ensembles from which \(V_{1},V_{2}\) are sampled, respectively. If either \(_{1}\) or \(_{2}\), or both form unitary \(2\)-designs, then for arbitrary \(H\) and \(\), the following inequality holds_

\[_{V_{1},V_{2}}[_{H,}(V_{1},V_{2})]},\] (4)

_where \(_{V_{1},V_{2}}\) denotes the expectation over \(_{1},_{2}\) independently. \(w(H)=_{}(H)-_{}(H)\) denotes the spectral width of \(H\), where \(_{}(H)\) is the maximum eigenvalue of \(H\) and \(_{}(H)\) is the minimum._

Theorem 1 demonstrates that the maximal influence of a local unitary within a random QNN on the cost function diminishes exponentially in the number of qubits, with a high probability. This inherent locality of QNN poses an exponential hardness of optimization in QNN training and we would like to make several remarks to better reveal the underlying implications of the theorem below. The main proof idea of Theorem 1 is to calculate the expectation value over \(_{1},_{2}\) separately. To tackle the maximization over \(U_{A}\), the main technique is to employ Holder's inequality to extract \(U_{A}\) out and bound the remaining part with specific calculations of \(2\)-design element-wise integrals. For the detailed proof, we defer to Appendix B.

**Remark 1** Firstly, due to the non-negativity and boundedness of the variation range, i.e., \(_{H,}[0,w(H)]\), the variance of \(_{H,}\) can be bounded by its expectation times \(w(H)\). Thus from Theorem 1 we know that the variance also vanishes exponentially:

\[_{V_{1},V_{2}}[_{H,}(V_{1},V_{2})](H)}{2^ {n/2-3m-2}}.\] (5)

Figure 2: **Partition of the QNN in our analysis**. The QNN is decomposed as \(=V_{2}(U_{A} I_{B})V_{1}\) with an input state \(\) and an observable \(H\). A tunable local unitary \(U_{A}\) is implemented by some local quantum gates with the left and right parts assembled as \(V_{1}\) and \(V_{2}\).

Note that \(w(H)((n))\) holds for common VQAs. Moreover, Theorem 1 together with Markov's inequality provides an exponentially small upper bound of the probability that \(_{H,}(V_{1},V_{2})\) deviates from zero, i.e.,

\[[_{H,}(V_{1},V_{2})]},>0.\] (6)

That is to say, the probability that \(_{H,}\) is non-zero to some fixed precision is exponentially small.

**Remark 2** Secondly, we can even establish an exponentially small bound using Theorem 1 for the case where \(U_{A}\) is a _global unitary_ satisfying the parameter-shift rule . Suppose \(U_{A}=e^{-i}\) with the Hermitian generator \(\) satisfying \(^{2}=I\). Since \(\) has only two different eigenvalues \( 1\), there exists a unitary \(W\) such that \(We^{-i}W^{}\) becomes a local unitary acting on a single qubit non-trivially. \(W\) and \(W^{}\) could be absorbed into the rest of the circuit with \(W^{}_{1}\) or \(_{2}W\) still forming \(2\)-designs . Therefore, the proof for global unitaries satisfying the parameter-shift rule can be reduced back to the case of local unitaries.

**Remark 3** Moreover, it is worth noticing that the compact bound in (4) only involves the spectral width \(w(H)\) and does not depend on any detail of the Hermitian operator \(H\). But if some specific structures about \(H\) are known, e.g., the Pauli decomposition of \(H\), a tighter bound could be derived in Appendix B which depends on the coupling complexity of \(H\). In addition, if the cost function reduces to the form of the fidelity between pure states, we could have a tighter bound with scaling \((2^{-n})\) in Proposition 2. Theorem 1 can be generalized to arbitrary dimensions besides qubit systems of dimension \(2^{n}\), e.g., qutrit and qudit systems. The detailed proof is provided in Appendix B.

In fact, Theorem 1 has a natural physical interpretation: the effect of a local operation on a physical observable will vanish exponentially after a chaotic evolution. Remarkably, the concept of local operations yielding minor global influences is a physically intuitive yet mathematically intricate notion. For instance, even a single-qubit unitary is enough to rotate an arbitrary \(n\)-qubit pure state to a new state with zero fidelity with the original one, showcasing local operations do make a great global influence. Hence, Theorem 1 may be invaluable as a rigorous formulation of the aforementioned argument within the domain of QNN training, elucidating the locality of QNNs.

## 4 Unifying the Limitations on Training QNNs

Here we briefly demonstrate how Theorem 1 unifies the restrictions on gradient-based  and gradient-free optimizations  in a more natural manner, and indicates the extra restrictions besides them on QNN training. In the following, we focus on a PQC applicable for Theorem 1 with \(M\) trainable parameters \(\{_{}\}_{=1}^{M}\) and denote the variation range of the cost function via varying \(_{}\) as \(_{}\).

Consider the gradient-based optimization first. On the one hand, in the case where the parameter-shift rule is valid , Theorem 1 can strictly deduce vanishing gradients. Suppose \(\{_{}\}_{=1}^{M}\) are applicable for the parameter-shift rule (e.g., hardware-efficient ansatzes). Namely, \(_{}\) enters the unitary \(e^{-i_{}_{}}\) within the circuit where \(_{}\) is a Hermitian generator satisfying \(_{}^{2}=I\). From Theorem 1 we know that the expectation of \(_{}\) vanishes exponentially. Therefore, the derivative \(_{}C:=}\) with respect to \(_{}\) satisfies

\[[\|_{}C\|=[|C( +_{})-C(-_{})|][ _{}](2^{-n/2}),\] (7)

where \(_{}\) is the unit vector in the parameter space corresponding to \(_{}\). From Markov's inequality as in (6), we know that the probability that the derivative \(_{}C\) deviates from zero by a small constant is exponentially small.

On the other hand, even in the absence of the parameter-shift rule, vanishing gradients could still be obtained approximately by the following arguments. Consider the vicinity of a random initialized parameter point where the linear approximation error is negligible, denoted as an \(\)-ball \(_{}\) of radius \(\) (here \(\) plays the same role as the learning rate). As shown in Fig. 3, the linearity in \(_{}\) together with Theorem 1 leads to

\[[|_{}C|][}{2}](2^{-n/2}),\] (8)up to the linear approximation error, where \(1/\) is not an essential factor since it reflects the frequencies of the landscape fluctuation rather than magnitudes, similar to the role of the factor \((V^{2})\) in the expression of \([_{}C]\).

For the gradient-free optimization based on the cost function difference between any two _fixed_ parameter points \(^{}\) and \(\), Theorem 1 leads to

\[[|C(^{})-C()|]\,[_{=1}^{M}|C(^{()})-C(^{(-1)})|]\,_{=1}^{M}[| _{}|](M2^{-n/2}),\] (9)

where \(^{()}=+_{=1}^{}(_{}^{} -_{})_{}\) for \(=1,...,M\) and \(^{()}=\) for \(=0\). Thus, as long as the number of parameters satisfies \(M((n))\), the cost function difference between any two points vanishes exponentially with a high probability, demanding an exponential precision to make progress in the gradient-free optimization.

Furthermore, Theorem 1 goes beyond vanishing gradients and vanishing differences between two fixed points. The exponentially vanishing quantity claimed by Theorem 1 is the variation range of the cost function in the _whole parameter subspace_ corresponding to a local unitary, e.g., the subspace of the \(3\) Euler angles in a single-qubit rotation gate from \((2)\), or the subspace of the \(15\) parameters in a two-qubit rotation gate from \((4)\), etc. This gives constraints on multiple parameters at finite intervals simultaneously, instead of a vicinity or two fixed parameter points.

## 5 Application on Representative QNN Models

To better illustrate the meaning of our findings in practice, we investigate the applications of Theorem 1 on three representative QNN models, including the variational quantum eigensolver (VQE), quantum autoencoder, and quantum state learning. The corresponding numerical simulation results are summarized in Fig. 5.

Application on VQE.The variational quantum eigensolver is the most famous implementation of a hybrid quantum-classical algorithm with the goal to prepare the ground state of a given Hamiltonian \(\) of a physical system . The cost function is the energy expectation with respect to an ansatz state \(|0\), i.e.,

\[C_{}()= 0|^{}| 0.\] (10)

For most physical models with local interactions, the spectral width is proportional to the system size, i.e., \(w()(n)\). For common repeated-layer-type ansatzes, e.g., the hardware-efficient ansatzes , linear depth \((n)\) is enough to make a randomly initialized circuit to be a sample from an approximate \(2\)-design ensemble [25; 62; 63]. Hence from Theorem 1 we know that \(_{}(V_{1},V_{2})\) vanishes exponentially with a high probability for random circuits forming \(2\)-designs. We conduct

Figure 3: **Sketch of our results implying vanishing gradients.** The left panel sketches the whole training landscape with one of the parameters \(_{}\) as the \(x\)-axis, all of the other parameters \(\{_{}\}_{}\) as the \(y\)-axis symbolically and the cost function value \(C\) as the \(z\)-axis. The right panel depicts a typical sample of the \(z\)-\(x\) cross-section from the landscape on the left with variation range \(_{}\). Up to the linear approximation error, \(_{}\) serves as an upper bound for the absolute derivative \(|_{}C|\) times the vicinity size \(2\).

numerical simulations for the variation range of the VQE cost function \(_{}\) using the \(1\)-dimensional spin-\(1/2\) antiferromagnetic Heisenberg model:

\[=_{i=1}^{n}(X_{i}X_{i+1}+Y_{i}Y_{i+1}+Z_{i}Z_{i+1}),\] (11)

with periodic boundary condition, as shown in Fig. 5(a).

Application on Quantum Autoencoder.The quantum autoencoder (QAE) is an approach for quantum data compression . As shown in Fig. 4, a QNN \(\) is trained as an encoder to compress a given state \(_{QR}\) on a bipartite system \(QR\) into a reduced state \(_{Q}=_{R}(_{QR}^{})\) on subsystem \(Q\), such that \(_{QR}\) can be reproduced from \(_{Q}\) by the decoder isometry \( 0|_{R}^{}\) with a high fidelity. According to the monotonicity of the fidelity under partial trace, an easy-to-measure cost function could be reduced from the fidelity between \(_{QR}\) and the reconstructed state as

\[C_{}():=1-((|0\! 0|_{R}  I_{Q})_{QR}^{}),\] (12)

where the second term is exactly the fidelity between the state of the discarded part \(_{R}=_{Q}(_{QR}^{})\) and the zero state \(|0_{R}\) on subsystem \(R\). The spectral width for the QAE cost function (12) is \(w(H_{})=1\) with \(H_{}=I_{QR}-|0\! 0|_{R} I_{Q}\). Thus again from Theorem 1 we know that \(_{}(V_{1},V_{2})\) vanishes exponentially in the number of qubits, specifically with the scaling \((2^{-n/2})\) as shown in Fig. 5(b).

Application on Quantum State Learning.The fidelity between pure states is a special case of the cost function in (2) with a low-rank observable. Many QML applications make use of fidelity as their cost functions . Here we uniformly call them quantum state learning (QSL) tasks. Denote the input state as \(|\) and the target state as \(|\). The QSL cost function can be written as

\[C_{}()=1-|||| ^{2}.\] (13)

Theorem 1 can be applied here with \(H_{}=I-|\!|\) and \(w(H_{})=1\). Here a tighter bound for \(_{}\) is provided in Proposition 2, which generally holds for the Bures fidelity. The proof of Proposition 2 is detailed in Appendix C.

**Proposition 2**: _If either \(_{1}\) or \(_{2}\), or both form unitary \(1\)-designs, then for the variation range of the fidelity-type cost function \(_{}\), the following inequality holds_

\[_{V_{1},V_{2}}[_{}(V_{1},V_{2})] }.\] (14)

Compared with Theorem 1, the bound \((2^{-n})\) becomes tighter and the demanded randomness becomes weaker in this special case. Notably, even a random circuit of constant depth is enough to form a \(1\)-design, which is much shallower than \(2\)-designs. Like in (5) and (6), the variance and the probability that \(_{}\) deviates from zero also vanish exponentially, but only require random circuits forming unitary \(1\)-designs. Moreover, still with \(1\)-designs, Proposition 2 implies exponentially vanishing cost gradients and cost differences in the same way as Theorem 1, which may be considered as the underlying mechanism behind the severe barren plateaus for global cost functions even with shallow quantum circuits .

Figure 4: **Circuit setting of the quantum autoencoder.**\(_{QR}\) is the given state to be compressed and \(_{Q}\) is the compressed state through the encoder \(\). The quantum autoencoder aims to train \(\) such that \(_{QR}\) can be reconstructed from \(_{Q}\) with high fidelity through the decoder \(^{}\) combined with an ancilla zero state \(|0\! 0|_{R}\). \(_{R}\) denotes the state of the discarded part after compression.

## 6 Numerical Simulations of Experiments

Previously, we have theoretically shown that with a high probability, the maximal influence of a local unitary within a random QNN on the cost function will vanish exponentially in the number of qubits. We further demonstrate the validity of our results with numerical simulations of experiments on the three representative QNN models. All of these experimental results show the exponentially vanishing variation range in the number of qubits, which is consistent with Theorem 1 and Proposition 2.

Circuit Setting.Consider subsystem \(A\) only containing a single qubit, namely \(m=1\), and parameterize the local unitary \(U_{A}(2)\) with \(3\) Euler angles up to a global phase, i.e., \(U_{A}(,,)=R_{z}()R_{y}()R_{z}()\), where \(R_{y}\) and \(R_{z}\) are single-qubit rotation gates with generators being \(Y\) and \(Z\) Pauli matrices. To construct random circuits forming \(2\)-designs as \(V_{1}\) or \(V_{2}\) used in the VQE and QAE examples, we employ the following hardware-efficient ansatz as in  for comparison.

\[\] (15)

A single layer of \(R_{y}(/4)=(-iY/8)\) gates are laid at the very beginning of the circuit to make the three rotation axes have equal status, then followed by \(10 n\) repeated layers. Each layer consists of \(n\) single-qubit rotation gates \(R_{P}()\) on each qubit together with \(n-1\) controlled phase gates between nearest neighboring qubits aligned as a 1-dimensional array, where the rotation axes \(P\{x,y,z\}\) is chosen with uniform probability and \([0,2)\) is also chosen uniformly. A such random circuit with \((n)\) repeated layers could be considered as an approximate \(2\)-design (here we employ \(10 n\)) . Experimental results with different numbers of layers are also presented in Appendix D to show how the expectation of the cost variation range \(_{H,}\) vanishes with the circuit depth. To construct random circuits forming \(1\)-designs used in the QSL example, we just replace the repeated layers above with a single layer of \((2)\) elements \(R_{z}()R_{y}()R_{z}()\) on each qubit with \(,,[0,2)\) are chosen with uniform probability.

Implementation Details.To compute \(_{U_{A}}C\) and \(_{U_{A}}C\) in the definition of \(_{H,}(V_{1},V_{2})\) with respect to \(U_{A}\), we employ the Adam optimizer to update \(U_{A}\) iteratively until convergence for

Figure 5: **Exponentially vanishing variation range of the cost function via varying a local unitary.** The data points represent the sample averages of the cost variation range \(_{H,}\) via varying a single-qubit unitary over the spectral width \(w(H)\) as a function of the number of qubits on semi-log plots. Panel (a) and (b) correspond to the VQE with the \(1\)-dimensional Heisenberg model and the quantum autoencoder with one qubit discarded, respectively, where the error bars represent the standard deviations over samples. Panel (c) corresponds to the quantum state learning with the cost function being the fidelity with the zero state. Different legends stand for \(_{1}\), \(_{2}\) or both being approximate \(2\)-designs in (a), (b) and \(1\)-designs in (c). The dashed lines depict our theoretical upper bounds for the three tasks where the scaling exponents show a good coincidence with the experimental results.

each of the \(100\) samples of \(V_{1},V_{2}\). We consider the converged value as a good estimation with a tolerable error at least for circuits with a small number of qubits (\( 10\)) and a modest depth (\( 10 n\)). We repeat this procedure for different numbers of qubits and different statistics of \(_{1}\) and \(_{2}\), i.e., \(_{1}\) or \(_{2}\) being a \(2\)-design (\(1\)-design) while the other being identity.

Numerical Results.We summarize the simulation results of the three examples in Fig. 5. The slopes of the lines imply the rates of exponential decay. The data points represent the sample averages of the cost variation range \(_{H,}\) via varying \(U_{A}\) over \(w(H)\), and the error bars represent the standard deviations over samples. We specially rescale the error bar in the QSL example as a quarter of the standard deviation for better presentation on semi-log plots. One can see that in all the cases, the expectations of \(_{H,}(V_{1},V_{2})\) vanish exponentially in the number of qubits. The data lines are almost parallel to the dashed lines depicting the theoretical upper bounds. That is to say, the scaling behaviors almost coincide with the predictions from Theorem 1 and Proposition 2. These results suggest that while optimizing a local unitary within a random QNN, the cost function exhibits fluctuations within an exponentially small range relative to the number of qubits. It is this phenomenon that elucidates the vanishing gradient issue and contributes to the exponential difficulty of training as the QNN scales up. A detailed derivation can be found in Appendix B for the tighter task-dependent upper bounds used in Fig. 5(a) and (b).

## 7 Conclusion and Discussion

We have shown that the maximal possible influence of a local unitary within a QNN on the cost function vanishes exponentially in the number of qubits with a high probability. This finding unveils the exponential hardness associated with training QNNs as they scale up. The randomness required is just a \(2\)-design for the generic cost function and a \(1\)-design for the fidelity-type cost function, in spite that the integrand \(_{H,}(V_{1},V_{2})\) is not necessarily a polynomial of degree at most \(2\) or \(1\) in the entries of \(V_{1}\) and \(V_{2}\). We remark that a \(2\)-design circuit can be achieved approximately by only \((n)\) depth  for common repeated-layer-type ansatzes, e.g., the hardware-efficient ansatzes , and a \(1\)-design circuit can be achieved more easily by only \((1)\) depth.

From the perspective of quantum information theory, our results can be regarded as a basic property of random quantum circuits. That is, a local unitary within a random circuit of polynomial depth has an exponentially small impact on the expectation of physical observables, which is expected to have potential applications in other areas involving random quantum circuits. This property may also provide insight into QNN design to address the critical trainability issue.

For the training of QNN, our results unify the restrictions on gradient-based and gradient-free optimizations in a natural way and hence can be regarded as the underlying mechanism behind the barren plateau phenomenon. Therefore, a fundamental limitation is unraveled in training QNNs, which can serve as a guide for designing better training strategies to improve the scalability of QNNs. A direct consequence is that the gate-by-gate optimization strategy  is ineffective no matter what optimizers are utilized. Reparameterization within local unitaries is also unhelpful. For future research, it will be of great interest to explore potential solutions via proper initialization , pre-training including adaptive methods , circuit architectures  and cost function choices .