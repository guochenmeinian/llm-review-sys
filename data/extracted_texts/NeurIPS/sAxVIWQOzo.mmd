# GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps

GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps

 Muhammad Umair Nasir

University of the Witwatersrand

Johannesburg, South Africa

muhammad.nasir@wits.ac.za &Steven James

University of the Witwatersrand

Johannesburg, South Africa

steven.james@wits.ac.za &Julian Togelius

New York University

New York, USA

julian@togelius.com

###### Abstract

Large language models (LLMs) have recently demonstrated great success in generating and understanding natural language. While they have also shown potential beyond the domain of natural language, it remains an open question as to what extent and in which way these LLMs can plan. We investigate their planning capabilities by proposing GameTraversalBenchmark (GTB), a benchmark consisting of diverse 2D grid-based game maps. An LLM succeeds if it can traverse through given objectives, with a minimum number of steps and a minimum number of generation errors. We evaluate a number of LLMs on GTB and found that GPT-4-Turbo achieved the highest score of \(44.97\%\) on GTB_Score (GTBS), a composite score that combines the three above criteria. Furthermore, we preliminarily test large reasoning models, namely o1, which scores \(67.84\%\) on GTBS, indicating that the benchmark remains challenging for current models. Code, data, and documentation are available at https://github.com/umair-nasir14/Game-Traversal-Benchmark.

## 1 Introduction

Large language models, built atop the transformer architecture , are widely influential in the field of natural language processing , and have shown great promise in a variety of applications that were not originally envisioned as target domains, thereby, hinting towards more general Artificial Intelligence models . In recent years, numerous LLMs have emerged, each competing for state-of-the-art performance by continuously expanding the limits of their capabilities . As a result, the accurate evaluation of LLMs has become a key focus for researchers .

LLMs are trained to predict the next token based on their current context, but an ongoing debate is whether these next-token predictors are capable of planning . Planning may include tasks in games , robot control , questioning answering , or visual programming . While these works show the capabilities of LLMs to plan, multiple complementary planning benchmarks are required to know whether these task-specific examples of planning are a common LLM ability. Therefore, we present a benchmark designed to evaluate the planning abilities of LLMs from a novel perspective. This benchmark serves as a complement to existing LLM assessments, focusing on tasks that are unlikely to be extensively represented in the training datasets of current LLMs. While prior evaluations of planning abilities have been conducted in natural language environments [16; 17], our work introduces a task represented as a string of characters that depicts a 2D grid-based map. This representation is infrequently encountered by LLMs during training, preventing them from merely performing a lookup to obtain answers; instead, they must engage in effective planning to determine the optimal path.

Previous works have shown that LLMs can generate 2D grid-based maps with fine-tuning and training [18; 19; 20], and without [21; 12]. Consequently, the question of whether LLMs can successfully create 2D grid-based maps has been addressed. However, while an LLM can generate sequences of characters devoid of natural language meaning, we seek to investigate whether it can effectively plan using these same character sequences. This inquiry is the focus of our research.

A natural question arises: Why evaluate LLMs on a 2D map? The rationale is that a 2D map, represented as a sequence of characters, is interpretable by LLMs. Given adequate instruction, LLMs should therefore be capable of processing and comprehending the map's state, which encompasses the map itself, the agent's position, and the locations of the objectives. Previous studies have demonstrated that LLMs can process instructions that are out-of-distribution from their training data. Therefore, when provided with the map's state and the available actions for moving the agent, an LLM capable of planning should be able to generate a sequence of actions to reach the target. To generate the correct action sequence, the LLM must plan each move. Similar to how humans remain aware of their surroundings while traversing a path, an LLM needs to continuously observe the environment to produce the correct actions. But do LLMs actually observe and plan as they generate action sequences? To answer this, we introduce the GameTraversalBenchmark (GTB), which consists of a dataset of diverse maps.

In the following sections, we begin by describing the dataset collection process and the creation of GTB. Next, we evaluate several LLMs on the GTB benchmark, demonstrating that while the state-of-the-art GPT-4 achieves the highest performance, it still falls short of reaching \(50\%\). We conclude with a discussion on how traversal problems can be made more challenging to encourage the research community to improve LLMs' planning capabilities.

## 2 GameTraversalBenchmark

In this section, we will discuss how we curated the dataset and the GameTraversalBenchmark (GTB). The dataset is generated via the pipeline introduced by our previous work, named as Word2World . Word2World is an LLM-based game design system that creates a story and narrative for the game by extracting useful information, such as tileset and character descriptions, and goals for the player. After extracting useful information, Word2World first places alphanumeric characters for the environment of the world in a 2D grid-based map. After that, it places alphanumeric characters for the game characters, such as the protagonist and antagonist of the story, and the interactive object tiles needed for the protagonist to play the game. Then Word2World generates the coordinates of objectives needed to complete the game. The setting of the environment and game

Figure 1: Top: An example of a level produced by the Word2World  algorithm and rendered using its tileset. Middle: Binary mask of the above map for visualising walkable tiles (teal), non-walkable tiles (purple), and the position of objectives (numbered). Bottom: Character representation of the map provided as input to the LLM.

characters is iteratively refined to generate more coherent maps. These worlds are generated using GPT-4-Turbo, GPT-4, GPT-3.5, Claude-3-Opus and Claude-3-Haiku.

We extract \(150\) of these maps, an example of which is illustrated by Figure 1. Each of the objectives for the LLM is to reach a certain coordinate and then to reach another coordinate, which serves as the next objective. These maps make good environments for judging the traversal abilities of an agent, as they have diverse patterns and different sizes. While there are datasets available from previous games , GTB provides many different sized levels with deceptive paths to traverse.

### Benchmark

We consider a generated map as one data point for the benchmark. The map's generated objective coordinates are the points where the LLM agent needs to traverse to attain the most rewards. Since we are interested in observing an LLM's planning ability from a traversal agent perspective, we compute the optimal path using A\({}^{*}\) search, which is set to find the optimal (shortest) path within 25000 iterations.

The goal of the LLM agent is to traverse the world through these objectives. The LLM agent is required to generate a sequence of actions that should take the agent from current position to the position of the objective. Therefore, LLM agent can generate a sequence of actions for the current objective once. All generated actions in the sequence are then rolled out in simulation and the position of the LLM agent decides the rewards it gets. Table 1 shows how rewards are distributed. This new position of the LLM agent is the starting positon for the next objective. The LLM agent also needs to find the shortest path to each objective. Thus, an LLM agent receives the highest rewards for achieving the objective while minimising the number of actions and making the fewest errors while generating the solution. Such errors include incorrect actions generated by the LLM, failure to adhere to imposed constraints, or the generation of syntactically incorrect outputs.

Thus the maximum and minimum achievable reward per level is:

\[R^{(m)}_{max}=_{m=0}^{M}(200-A^{*(m)}_{PL})\] (1)

and

\[R^{(m)}_{min}=_{m=0}^{M}(-100-A^{*(m)}_{PL^{m}}-^{(m)}_{max}),\] (2)

where \(200\) is the maximum reward obtained by reaching the exact coordinate of the objective, \(-100\) is the minimum reward for being farthest away from the coordinate, and \(A^{*}_{PL^{m}}\) is the path length of the objective calculated by an \(A^{*}\) agent. \(^{(m)}_{max}\) is the maximum number of generation errors possible during the level \(m\), and \(M\) is the total number of 2D game map in the dataset. Thus, we have the following as the normalised and scaled reward calculations across the dataset, which is also called GTB_Score:

\[GTB\_Score=_{m=0}^{M}(-LLM^{(m)}_{PL}-^ {(m)})-R^{(m)}_{min}}{R^{(m)}_{max}-R^{(m)}_{min}}),\] (3)

   Rewards allowed & Reason & Reward \\  At each step & For each action taken. & -1 \\  & For each generation mistake. & -1 \\ At the end of & 8 or more tiles away from the objective. & -100 \\ each objective & 5 - 8 tiles away. & -50 \\  & 3 - 5 tiles away. & +25 \\  & 2 - 3 tiles away. & +50 \\  & 1 tile away. & +100 \\  & Exactly on the coordinate. & +200 \\   

Table 1: Reward distribution for an LLM agent.

where \(M\) is the number of levels in the dataset.

\(R^{(m)}\) is determined by reaching a specific coordinate in the level and querying Table 1 for the corresponding reward, which is based on the distance to the goal. \(LLM_{PL}^{(m)}\) is the path length the LLM-agent took to reach the coordinates across all objectives in the level \(m\). \(^{(m)}\) is the negative reward for the total error given to the LLM agent for making an error while generating. \(R_{min}^{(m)}\) and \(R_{max}^{(m)}\) are the minimum and maximum possible rewards for the level \(m\), respectively. Thus, this reward function takes into account how close the LLM agent has reached to the exact coordinate of the objective, how many steps it took, and if it made any errors.

The negative reward for error is placed to judge the LLM by its capability to control the generation. We give LLMs \(10\) attempts per objective and record the errors they make before succeeding in generating the right actions. The errors could be a wrong action generated or a syntax error. Errors are recorded as:

\[MGE=_{m=0}^{M}_{GE}^{(m)},\] (4)

where \(MGE\) represents the average number of errors generated \(_{GE}^{(m)}\) for the level \(m\), across the whole dataset.

Similarly,

\[MPL=_{m=0}^{M}PL^{(m)}\] (5)

\[MAT=_{m=0}^{M}AT^{(m)}\] (6)

In Equation 5, we have \(PL_{m}\) as the path length for the map \(m\). We do not add it to the path length if the agent moves up and then moves down, or vice versa, and moves left and moves right, or vice versa, as the position of the agent stays the same. This calculates the _mean path length_ (MPL). For all the actions, including actions that are not included in MPL, we calculate them through Equation 6 to find _mean actions taken_ (MAT), where we count the length of actions taken \(AT_{m}\) by an LLM for the map \(m\).

Therefore, \(\) provides the following metrics for evaluations:

1. **GTB_Score**: Score that is calculated by Equation 3. This is the score that determines the place of an LLM on the scoreboard.
2. **MGE**: Score that indicates how many errors occurred across the dataset.
3. **MPL**: Score that indicates the path length taken by the agent.
4. **MAT**: Score that indicates total actions taken by the agent.
5. **Top-0 Accuracy**: Score that indicates how many times the agent has reached the exact coordinate of the objective. The score is calculated by: \[}{}\] (7)
6. **Top-1 Accuracy**: Score that indicates how many times the agent has reached the 1-tile window of the objective. Reaching objective coordinates is not counted in this score. The score is calculated by: \[}{}\] (8)7. **Top-5 Accuracy**: Score that indicates how many times the agent has reached 2 - 5 tiles away from the objective coordinate. Reaching the coordinate and 1-tile window is not counted in the score. The score is calculated by: \[\] (9) \[\]

Figure 2 shows how \(\) metrics are generated. A game map \(M\) is picked from the dataset and the current state of the map \(s\) is given to the LLM agent which includes the map, the position of the agent, and the position of the objective. Some more constant information is given to the LLM, which includes what each tiles refers to in the map, walkable tiles, and important tiles. This loop is iterated over \(N\) objectives of the map. The LLM agent produces the action sequence which is required for the agent to traverse from the agent's position to the objective's position. Then the state is updated by the current position of the agent and the next objective position. This time the rewards for the previous objectives are also provided. Figure 3 illustrates how exactly the input is provided to the LLM, while Figure 4 illustrates how we prompt an LLM in \(\).

\(\) provides zero and one-shot evaluations for all the above metrics. One-shot evaluation is provided for researchers who are interested in few-shot evaluations. Primarily, we are interested in evaluating LLMs that may achieve high scores on \(\) via zero-shot. Here, zero-shot means that the LLM agent will have to generate action sequences in one go. If it makes a generation error, it is not given the previous actions to re-generate the actions. For one-shot, we give the LLM one extra chance per objective in each game map, if it has not reached the exact coordinate of the objective. In this case, the LLM receives the actions it took in the previous shot, the distance between the objective and the final position, and the reward it received.

### Exploring the Benchmark

We now show the diversity of the dataset that \(\) evaluates the LLM agents on. We first consider the diverse map sizes contained in the dataset. We observe in Figure 4(a) that there is a vast distribution of map sizes in the dataset. If we look into _width_\(\)_height_ of the different maps in the dataset, we see that we have small maps \(5 10\) size and a cluster around the size \(8 15\). The majority of the distribution lies in between the width of \(20\) to \(40\) to a height of \(10\) to \(20\). The distribution extends with a considerable amount of maps with heights of more than \(20\) and around \(35\), while a good spread of maps has widths up to just below \(100\). These map sizes express diversity, allowing other factors, like a number of objectives and optimal path lengths, to be more diverse.

Figure 4(c) also adds to the diversity of the dataset. While we have more maps with optimal paths around \(20\) to \(30\) actions, but we have a maximum of just below \(120\) actions and a minimum of \(5\). This also shows diversity in the difficulty of traversing the maps. The dataset includes simpler maps, solvable by an A\({}^{*}\) agent in 5 to 10 actions, which provide LLMs with an opportunity to achieve high

Figure 2: An illustration of the \(\) evaluation loop. Each game map \(M\) is evaluated turn-by-turn for all objectives \(N\) present in it. A game state \(S\) includes the game map, the position of LLM agent, the position of objectives, and current rewards. The updated state, \(S+1\), has the updated position of the LLM agent in it and updated rewards. LLM agent produces a sequence of actions for that particular objective and is evaluated for that objective. Once all objectives are iterated over, the agent evaluation is stored and the loop moves to the next map. Once all maps are evaluated, the \(\) metrics are calculated.

Figure 4: Example of a prompt in GTB to generate actions.

Figure 3: Illustrates an example of an input to the LLM and the output of the action sequence for the objective.

rewards. However, true planning capabilities are better assessed on maps where the optimal paths exceed 100 steps.

Lastly, we show the distribution of the number of objectives in Figure 4(d) across the dataset. We observe a sufficient amount of diversity by having a distribution spread from \(3\) to \(8\) objectives but we also have a map with \(11\) objectives. This distribution, along with path length from Figure 4(c), also indicates a difficult traversal dataset as we have more than \(60\) maps with \(8\) objectives. These span across maps with different path lengths as we have a nice spread of distribution in Figure 4(a) and 4(c). Thus, smaller maps with 8 objectives will also be challenging for LLMs, similar to larger maps with \(8\) objectives. We also look into the correlation of the number of objectives and path lengths of the maps in Figure 4(b). We can observe that the correlation is not high which makes the benchmark more interesting. We have path length around \(100\) with only \(3\) objectives while a map with \(11\) objectives has a path length of around \(30\). This distribution makes it interesting as there is no set rule for the LLM to follow which may lead to a plan for all maps. The LLM may need to generate very large number of actions per objective and very small number of actions as well.

## 3 Baseline Results

To show the relevance of GTB in the current state-of-the-art LLMs, we evaluate a few families of LLMs. Table 2 shows results for different LLMs. The best-performing LLMs, such as _GPT-4-Turbo_ and _Claude-3-Opus_, are among the state-of-the-art LLMs as well when it comes to benchmarks for natural language understanding tasks, code generation tasks, as well as on PlanBench . Thus, the results are consistent with other benchmarks, making GTB a uniques option to evaluate planning in LLMs. These results demonstrate that the benchmark is a tough challenge for LLMs and there is a big room for improvement in the planning abilities of today's LLMs.

Furthermore, we can observe from Table 2 that all LLMs except the first \(4\) LLMs are worse than a _Random-FP_ agent. The Random-FP agent computes the difference in distance between two objectives, using this value to determine the length of the required action sequence. Then it produces that many actions uniformly at random. MGE, MPL and MAT are not applicable in this case as there will be no generation error and the path length is fixed. Questions therefore arise as to whether LLMs with lesser capacity are just randomly generating sequences of actions. Observation of the behaviour exhibited

Figure 5: Exploring GTB in terms of path lengths and number of objectives.

by the various LLMs suggests that the actions taken by LLMs other than GPT-4 and Claude-3-Opus do not appear meaningful, and could very well be random. The most likely explanation for this is that these LLMs are unable to internally construct a map or representation of the environment's state. A truly random agent, _Random-RP_ generates random action sequence of random lengths and achieves a GTBS of \(3\), implying that generating the right sequence length is not a random task--it needs to be planned. LLMs performing better than Random-RP agent also means the LLM may have a sense of how long the path should be, but were not able to produce the right actions. This suggests that a less capable LLM might grasp the relationship between the start and target positions of the objective, but fails to comprehend the path planning process required to navigate between them.

   Model & GTBS\(\) & MGE\(\) & MPL\(\) & MAT\(\) & Top-0 & Top-1 & Top-5 \\  & \(0.25\) & \(0.01\) & \(0.71\) & \(0.7\) & \(0.19\) & \(0.61\) & \(2.01\) \\  LLaMa-3- & \(15.17 0.56\) & \(0.22 0.01\) & \(213.28 0.07\) & \(213.35 0.07\) & \(2.16 0.07\) & \(19.93 0.05\) \\  Gamma- & \(13.63 0.07 0.06\) & \(35.16 0.07\) & \(35.41 0.07\) & \(0.18 0.07\) & \(2.02 0.07\) & \(15.95 0.07\) \\   

Table 3: Results for selected LLMs on one-shot GTB.

   Model & GTBS\(\) & MGE\(\) & MPL\(\) & MAT\(\) & Top-0 & Top-1 & Top-5 \\  & \(0.22\) & \(0.01\) & \(0.69\) & \(0.62\) & \(0.24\) & \(0.46\) & \(1.03\) \\  GPT-4-o & \(30.95 0.05 0.06\) & \(0.53 0.58 0.06\) & \(85.91 0.07\) & \(7.84 0.07\) & \(11.34 0.07\) & \(18.99 0.05\) \\  Claude-3- & \(28.65 0.05\) & \(0.02 0.01\) & \(100.41 0.07\) & \(100.44 0.06\) & \(5.49 0.05\) & \(12.35 0.07\) & \(22.72 0.07\) \\  Claude-3- & \(18.54 0.07 0.07\) & \(75.05 0.08\) & \(76.22 0.04\) & \(0.73 0.05\) & \(3.80 0.07\) & \(13.64 0.07\) \\ Sonnet & \(0.22\) & & \(0.38\) & \(0.41\) & \(0.05\) & \(0.11\) & \(0.31\) \\  Random- & \(18.02 0.05\) & N/A & N/A & N/A & \(0.91 0.07\) & \(2.77 0.07\) & \(12.41 0.07\) \\  Gamma- & \(15.65 0.01 0.02\) & \(37.06 0.05\) & \(40.16 0.07\) & \(0.29 0.05\) & \(2.05 0.05\) & \(10.95 0.05\) \\  GPT-3.5- & \(14.34 0.07 0.07\) & \(46.83 0.07\) & \(48.23 0.07\) & \(0.44 0.07\) & \(3.49 0.07\) & \(11.88 0.07\) \\ Turbo & \(0.31\) & \(0.39\) & \(0.69\) & \(0.91\) & \(0.23\) & \(0.34\) & \(0.31\) \\  LLaMa-3- & \(14.08 0.07 0.07\) & \(55.38 0.07\) & \(56.06 0.07\) & \(0.21 0.07\) & \(2.27 0.07\) & \(8.54 0.07\) \\  LLaMa-3- & \(11.39 0.07 0.07\) & \(266.88 0.04\) & \(267.02 0.07\) & \(1.06 0.07\) & \(4.84 0.07\) & \(16.63 0.07\) \\  Claude-3- & \(10.81 0.07 0.07\) & \(69.14 0.07\) & \(70.09 0.07\) & \(0.09 0.07\) & \(1.25 0.07\) & \(7.32 0.07\) \\ Haiku & \(1.15\) & & \(8.18\) & \(8.21\) & \(0.06\) & \(0.88\) & \(1.96\) \\  Mixtral- & \(9.35 0.07 0.07\) & \(9.61 0.07\) & \(152.73 0.07\) & \(152.99 0.07\) & \(0.67 0.07\) & \(2.85 0.07\) & \(10.19 0.07\) \\
8\(\)7B & \(0.56\) & \(0.41\) & \(5.43\) & \(5.46\) & \(0.24\) & \(0.16\) & \(0.19\) \\  Random- & \(3.04 0.07 0.07\) & \(35.16 0.07\) & \(35.41 0.07\) & \(0.18 0.07\) & \(2.02 0.07\) & \(15.37 0.07\) \\ RP &Furthermore, it is noteworthy that smaller LLMs tend to produce shorter path lengths, possibly recognising the need to generate fewer steps. However, they often fail to produce the correct actions. LLaMa-3-70B and Mixtral-8\(\)7 (a 56B parameter model) traverse more than the lesser capacity models. These two LLMs reach more objectives but explore more while reaching them. Gemma-7B, LLaMa-7B, and Claude-3-Haiku, the lower capacity LLMs, reach the objectives fewer times than the above-mentioned LLMs, but also take fewer actions to reach them. Therefore, they score better.

For one-shot GTB we observe that GPT-4-Turbo and LLaMa-3-70B improve, while Gemma-7B performs worse than zero-shot. A possible reason might be how much attention a lesser capable model can give to its context, while better models can give better attention to all of its contexts. This behaviour is also noticed in PlanBench .

### Preliminary Results On Large Reasoning Models

We further test GTB on o1 , a recent large reasoning model (LRM), to see how the current state-of-the-art performs on GTB. The underlying LLM of o1 is trained using reinforcement learning to curate its outputs through a private Chain-of-Thought reasoning process , which enhances its ability to simulate a "thinking" phase before generating responses. Furthermore, o1 is a reasoning model, meaning that the inference stage, which contains many more steps than the typical LLM, is computationally and monetarily expensive. For this reason, it cannot be straightforwardly compared with other LLMs - it is fundamentally a different kind of model. For the same reason, we only include preliminary results based on a single run in this paper. In the single run, we found that the model outperformed the top-performing LLM on GTB. However, despite surpassing GPT-4-Turbo, it was unable to exceed a score of 70 on GTB, as demonstrated in Table 4.

Figure 6 presents a comparison between o1 and the best-performing seed from GPT-4-Turbo in terms of the number of objectives and path lengths. While o1 outperforms GPT-4-Turbo across all ranges of both metrics, the observed differences are relatively modest. As shown in Table 4, although o1 achieves twice the Top-0 accuracy of GPT-4-Turbo, the latter produces fewer generation errors, as indicated by MGE, and demonstrates significantly higher Top-1 and Top-5 accuracy. Nonetheless, o1 consistently identifies shorter paths compared to GPT-4-Turbo. These results suggest that while LRMs perform well on GTB, there remains considerable room for improvement.

   Model & GTB\(\) & MGE\(\) & MPL\(\) & MAT\(\) & Top-0 & Top-1 & Top-5 \\  & & & & Acc.\(\) & Acc.\(\) & Acc.\(\) & Acc.\(\) \\  O1 & \(67.84\) & \(0.12\) & \(51.35\) & \(51.73\) & \(50\) & \(10.76\) & \(13.19\) \\  O1-mini & \(61.36\) & \(0.83\) & \(82.83\) & \(82.95\) & \(46.98\) & \(6.70\) & \(14.38\) \\  GPT-4- & \(44.97\) & \(0.03\) & \(80.91\) & \(80.97\) & \(19.2\) & \(17.66\) & \(23.05\) \\ Turbo & \(0.22\) & \(0.01\) & \(0.69\) & \(0.62\) & \(0.24\) & \(0.46\) & \(1.03\) \\   

Table 4: Results of o1 and o1-mini. GPT-4-Turbo added for reference.

Figure 6: Comparison of o1 and GPT-4-Turbo

Related Work

Conventionally, LLMs are evaluated for their natural language understanding and generation abilities. In this context, Hendrycks et al.  introduced the Massive Multitask Language Understanding (MMLU) benchmark which consists of 57 tasks spanning subjects such as elementary mathematics, computer science, and law. Similarly, Beyond the Imitation Game benchmark (BIG-bench)  was also introduced to evaluate LLMs across 204 diverse tasks. Other evaluations, such as Chatbot Arena  and MT-Bench , focus on measuring the general capabilities of chatbots. While these benchmarks assess overall performance across multiple tasks, there are numerous specialised benchmarks that evaluate specific downstream tasks. For example, SocKET  assesses social knowledge, TRUSTGPT  focuses on ethics, MATH  evaluates mathematical problem-solving, APPS  and HumanEval  test code generation abilities, FreshQA  and TRUTHFULQA  examine question-answering capabilities, and SafetyBench  evaluates the safety performance of LLMs.

Benchmarks specifically designed to evaluate the planning abilities of LLMs are also related to GTB. API-Bank  evaluates an LLM on the ability to continuously plan, retrieve, and call multiple APIs. The most closely related works are PlanBench  and AutoPlanBench  which uses Planning Domain Description Language (PDDL)  and convert them into natural language to evaluate the planning abilities of LLMs. While previous works have demonstrated the capacity of LLMs to play games [37; 38; 39; 40; 12] to the best of our knowledge, no prior work has focused on evaluating LLMs specifically within the context of games.

## 5 Limitations

Although GTB has provided valuable insights into the planning abilities of LLMs, it is important to acknowledge its limitations, which, if addressed, could further advance the field. The 2D game maps are static, which is still challenging but can be increased in difficulty, such as moving non-player characters, enemies that may attack, tiles that may end in terminating conditions etc. Furthermore, GTB focuses solely on pathfinding abilities, which, while critical, represent only one dimension of planning. The action space is also limited to only \(4\) discrete actions. GTB has a static prompt that evaluates all the LLMs, which may be easier for larger LLMs to follow instructions then smaller ones.

## 6 Conclusion And Future Directions

This research introduces GameTraversalBenchmark (GTB), a novel benchmark for evaluating LLMs planning abilities through traversal in a 2D grid-based map. We provide extensive metrics to give insights towards planning abilities in LLMs. We then evaluate many LLMs on GTB_Score. We also evaluate LRMs on GTB_Score. We believe that there is a lot of room in this direction for research as LLMs have not been evaluated as traversal agents before but have been used in previous research as game-playing abilities. For future work, we would like to see how LLMs of all scales perform when they are fine-tuned on GTB. We would like to see how much smaller LLMs can improve upon fine-tuning, and after fine-tuning, how much such fine-tuning will generalise. Once we have LLMs that can perform well on GTB, we would like to extend the work by letting LLMs generate the state representation as well. It would also be valuable to have a prompt generation mechanics that can create prompts for all LLMs separately. A mechanism introduced by  could be useful to enhance LLMs on GTB. We hope that our contribution will push the boundaries of current state-of-the-art LLMs planning abilities as they are not able to achieve high scores on GTB, yet.

## 7 Acknowledgments

We would like to acknowledge our peers at Game Innovation Lab, New York University, USA for their support as funding and fellow members for advice during experimentation. We would also like to acknowledge the Robotics and Autonomous Intelligence Lab, University of the Witwatersrand, South Africa for their advice during experimentation. We would also like to acknowledge that there are no negative societal impacts of our work, to the best of our knowledge, even though it involvespre-trained LLMs but as these LLMs were not trained by us therefore there are no ethical concerns that need to be addressed.