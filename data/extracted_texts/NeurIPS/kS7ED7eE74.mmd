# A Fractional Graph Laplacian Approach

to Oversmoothing

Sohir Maskey

Department of Mathematics,

LMU Munich

maskey@math.lmu.de

&Raffaele Paolino

Department of Mathematics & MCML,

LMU Munich

paolino@math.lmu.de

&Aras Bacho

Department of Mathematics,

LMU Munich

&Gitta Kutyniok

Department of Mathematics & MCML,

LMU Munich

Equal contribution.

###### Abstract

Graph neural networks (\(\)) have shown state-of-the-art performances in various applications. However, \(\) often struggle to capture long-range dependencies in graphs due to oversmoothing. In this paper, we generalize the concept of oversmoothing from undirected to directed graphs. To this aim, we extend the notion of Dirichlet energy by considering a directed symmetrically normalized Laplacian. As vanilla graph convolutional networks are prone to oversmooth, we adopt a neural graph \(\) framework. Specifically, we propose fractional graph Laplacian neural \(\), which describe non-local dynamics. We prove that our approach allows propagating information between distant nodes while maintaining a low probability of long-distance jumps. Moreover, we show that our method is more flexible with respect to the convergence of the graph's Dirichlet energy, thereby mitigating oversmoothing. We conduct extensive experiments on synthetic and real-world graphs, both directed and undirected, demonstrating our method's versatility across diverse graph homophily levels. Our code is available on GitHub.

## 1 Introduction

Graph neural networks (\(\)) (Gori et al., 2005; Scarselli et al., 2009; Bronstein et al., 2017) have emerged as a powerful class of machine learning models capable of effectively learning representations of structured data. \(\) have demonstrated state-of-the-art performance in a wide range of applications, including social network analysis (Monti et al., 2019), molecular property prediction (Gilmer et al., 2017), and recommendation systems (J. Wang et al., 2018; Fan et al., 2019). The majority of existing work on \(\) has focused on undirected graphs (Defferrard et al., 2016; Kipf et al., 2017; Hamilton et al., 2017), where edges have no inherent direction. However, many real-world systems, such as citation networks, transportation systems, and biological pathways, are inherently directed, necessitating the development of methods explicitly tailored to directed graphs.

Despite their success, most existing \(\) models struggle to capture long-range dependencies, which can be critical for specific tasks, such as node classification and link prediction, and for specific graphs, such as heterophilic graphs. This shortcoming also arises from the problem of _oversmoothing_, where increasing the depth of \(\) results in the node features converging to similar values that only convey information about the node's degree (Oono et al., 2019; Cai et al., 2020). Consequently,scaling the depth of \(\)s is not sufficient to broaden receptive fields, and other approaches are necessary to address this limitation. While these issues have been extensively studied in undirected graphs (Q. Li et al., 2018; G. Li et al., 2019; Luan, M. Zhao, et al., 2019; D. Chen et al., 2020; Rusch et al., 2022), their implications for directed graphs remain largely unexplored. Investigating these challenges and developing effective solutions is crucial for applying \(\)s to real-world scenarios.

Over-smoothing has been shown to be intimately related to the graph's _Dirichlet energy_, defined as

\[()_{i,j=1}^{N}a_{i,j}\| _{i}}{}}-_{j}}{}}\| _{2}^{2},\]

where \(=(a_{i,j})_{i,j=1}^{N}\) represents the adjacency matrix of the underlying graph, \(^{N K}\) denotes the node features, and \(d_{i}\) the degree of node \(i\). Intuitively, the Dirichlet energy measures the smoothness of nodes' features. Therefore, a \(\) that minimizes the Dirichlet energy is expected to perform well on _homophilic_ graphs, where similar nodes are likely to be connected. Conversely, a \(\) that ensures high Dirichlet energy should lead to better performances on _heterophilic_ graphs, for which the nodes' features are less smooth.

This paper aims to bridge the gap in understanding oversmoothing for directed graphs. To this aim, we generalize the concept of Dirichlet energy, providing a rigorous foundation for analyzing oversmoothing. Specifically, we consider the directed symmetrically normalized Laplacian, which accommodates directed graph structures and recovers the usual definition in the undirected case. Even though the directed symmetrically normalized Laplacian has been already used (Zou et al., 2022), its theoretical properties remain widely unexplored.

However, a vanilla graph convolutional network (\(\)) (Kipf et al., 2017) implementing this directed Laplacian alone is not able to prevent oversmoothing. For this reason, we adopt a graph neural \(\) framework, which has been shown to effectively alleviate oversmoothing in undirected graphs (Bodnar et al., 2022; Rusch et al., 2022; Di Giovanni et al., 2023).

### Graph Neural \(\)s

The concept of neural \(\) was introduced by Haber et al. (2018) and R. T. Q. Chen et al. (2018), who first interpreted the layers in neural networks as the time variable in \(\)s. Building on this foundation, Poli et al. (2021), Chamberlain et al. (2021), and Eliasof et al. (2021) extended the connection to the realm of \(\)s, resulting in the development of graph neural \(\)s. In this context, each node \(i\) of the underlying graph is described by a state variable \(_{i}(t)^{K}\), representing the node \(i\) at time \(t\). We can define the dynamics of \((t)\) via the node-wise \(\)

\[^{}(t)=f_{}((t))\,,\ t[0,T]\,,\]

subject to the initial condition \((0)=_{0}^{N K}\), where the function \(f_{}:^{N K}^{N K}\) is parametrized by the learnable parameters \(\).

The graph neural \(\) can be seen as a continuous learnable architecture on the underlying graph, which computes the final node representation \((T)\) from the input nodes' features \(_{0}\). Typical choices for \(f_{}\) include attention-based functions (Chamberlain et al., 2021), which generalize graph attention networks (\(\)) (Velickovic et al., 2018), or convolutional-like functions (Di Giovanni et al., 2023) that generalize \(\)(Kipf et al., 2017).

How can we choose the learnable function \(f_{}\) to accommodate both directed and undirected graphs, as well as different levels of homophily? We address this question in the following subsection.

### Fractional Laplacians

The continuous fractional Laplacian, denoted by \((-)^{}\) for \(>0\), is used to model non-local interactions. For instance, the fractional heat equation \(_{t}u+(-)^{}u=0\) provides a flexible and accurate framework for modeling anomalous diffusion processes. Similarly, the fractional diffusion-reaction, quasi-geostrophic, Cahn-Hilliard, porous medium, Schrodinger, and ultrasound equations are more sophisticated models to represent complex anomalous systems (Pozrikidis, 2018).

Similarly to the continuous case, the fractional graph Laplacian (\(\)) (Benzi et al., 2020) models non-local network dynamics. In general, the \(\) does not inherit the sparsity of the underlying graph, allowing a random walker to leap rather than walk solely between adjacent nodes. Hence, the \(\) is able to build long-range connections, making it well-suited for heterophilic graphs.

### Main Contributions

We present a novel approach to the fractional graph Laplacian by defining it in the singular value domain, instead of the frequency domain (Benzi et al., 2020). This formulation bypasses the need for computing the Jordan decomposition of the graph Laplacian, which lacks reliable numerical methods. We show that our version of the \(\) can still capture long-range dependencies, and we prove that its entries remain reasonably bounded.

We then propose two \(\)-based neural \(\): the fractional heat equation and the fractional Schrodinger equation. Importantly, we demonstrate that solutions to these \(\)-based neural \(\) offer increased flexibility in terms of the convergence of the Dirichlet energy. Notably, the exponent of the fractional graph Laplacian becomes a learnable parameter, allowing our network to adaptively determine the optimal exponent for the given task and graph. We show that this can effectively alleviate oversmoothing in undirected and directed graphs.

To validate the effectiveness of our approach, we conduct extensive experiments on synthetic and real-world graphs, with a specific focus on supervised node classification. Our experimental results indicate the advantages offered by fractional graph Laplacians, particularly in non-homophilic and directed graphs.

## 2 Preliminaries

We denote a graph as \(=(,)\), where \(\) is the set of nodes, \(\) is the set of edges, and \(N=||\) is the number of nodes. The adjacency matrix \(:=\{a_{i,j}\}\) encodes the edge information, with \(a_{i,j}=1\) if there is an edge directed from node \(j\) to \(i\), and \(0\) otherwise. The in- and out-degree matrices are then defined as \(_{}=()\), \(_{}=(^{})\), respectively. The node feature matrix \(^{N K}\) contains for every node its feature in \(^{K}\).

Given any matrix \(^{n n}\), we denote its spectrum by \(()\{_{i}()\}_{i=1}^{n}\) in ascending order w.r.t. to the real part, i.e., \(_{1}()_{2}() _{n}()\). Furthermore, we denote by \(\|\|_{2}\) and \(\|\|\) the Frobenius and spectral norm of \(\), respectively. Lastly, we denote by \(_{n}\) the identity matrix, where we omit the dimension \(n\) when it is clear from the context.

Homophily and HeterophilyGiven a graph \(=(,)\) with labels \(=\{y_{i}\}_{i}\), the _homophily_ of the graph indicates whether connected nodes are likely to have the same labels; formally,

\[()=_{i=1}^{N}: a_{i,j}=1 y_{i}=y_{j}\}|}{|\{j\{1,,N\}:a_{i,j}=1\}|},\]

where the numerator represents the number of neighbors of node \(i\) that have the same label \(y_{i}\)(Pei et al., 2019). We say that \(\) is _homophilic_ if \(() 1\) and _heterophilic_ if \(() 0\).

## 3 Dirichlet Energy and Laplacian for (Directed) Graphs

In this section, we introduce the concept of Dirichlet energy and demonstrate its relationship to a directed Laplacian, thereby generalizing well-known results for undirected graphs.

**Definition 3.1**.: _The Dirichlet energy is defined on the node features \(^{N K}\) of a graph \(\) as_

\[()_{i,j=1}^{N}a_{i,j}\| {_{i}}{^{m}}}-_{j}}{^{m}}} \|_{2}^{2}\,.\] (1)

The Dirichlet energy measures how much the features change over the nodes of \(\), by quantifying the disparity between the normalized outflow of information from node \(j\) and the normalized inflow of information to node \(i\).

**Definition 3.2**.: _We define the symmetrically normalized adjacency (\(\)) as \(_{in}^{-1/2}_{out}^{-1/2}\)._Note that \(\) is symmetric if and only if \(\) is undirected; the term "symmetrically" refers to the both-sided normalization rather than the specific property of the matrix itself.

It is well-known that the SNA's spectrum of a connected undirected graph lies within \([-1,1]\)(Chung, 1997). We extend this result to directed graphs, which generally exhibit complex-valued spectra.

**Proposition 3.3**.: _Let \(\) be a directed graph with SNA_\(\). For every \(()\), it holds \(|| 1\)._

Proposition 3.3 provides an upper bound for the largest eigenvalue of any directed graph, irrespective of its size. However, many other spectral properties do not carry over easily from the undirected to the directed case. For example, the SNA may not possess a one-eigenvalue, even if the graph is strongly connected (see, e.g., Figures 1 to 2). The one-eigenvalue is of particular interest since its eigenvector \(\) corresponds to zero Dirichlet energy \(()=0\). Therefore, studying when \(1()\) is crucial to understanding the behavior of the Dirichlet energy. We fully characterize the set of graphs for which \(1()\); this is the scope of the following definition.

**Definition 3.4**.: _A graph \(=(,)\) is said to be balanced if \(d_{i}^{}=d_{i}^{}\) for all \(i\{1,,N\}\), and weakly balanced if there exists \(^{N}\) such that \( 0\) and_

\[_{j=1}^{N}a_{i,j}(}{^{}}}-}{^{}}})=0\,,\  i\{1,,N\}\.\]

It is straightforward to see that a balanced graph is weakly balanced since one can choose \(k_{i}=^{}}\). Hence, all undirected graphs are also weakly balanced. However, as shown in Figure 2, the set of balanced graphs is a proper subset of the set of weakly balanced graphs.

**Proposition 3.5**.: _Let \(\) be a directed graph with SNA_\(\). Then, \(1()\) if and only if the graph is weakly balanced. Suppose the graph is strongly connected, then \(-1()\) if and only if the graph is weakly balanced with an even period._

Proposition 3.5 generalizes a well-known result for undirected graphs: \(-1()\) if and only if the graph is bipartite, i.e., has even period. The next result shows that the Dirichlet energy defined in (1) and the SNA are closely connected.

**Proposition 3.6**.: _For every \(^{N K}\), it holds \(()=(( ^{}(-)))\). Moreover, there exists \(\) such that \(()=0\) if and only if the graph is weakly balanced._

Proposition 3.6 generalizes the well-known result from the undirected (see, e.g., Cai et al., 2020, Definition 3.1) to the directed case. This result is an important tool for analyzing the evolution of the Dirichlet energy in graph neural networks.

Figure 2: Examples of non-weakly balanced (left), weakly balanced (center), and balanced (right) directed graphs. The Perron-Frobenius eigenvalue of the left graph is \(_{} 0.97 1\), while for the middle and right graphs \(_{}=1\).

## 4 Fractional Graph Laplacians

We introduce the fractional graph Laplacian through the singular value decomposition (SVD). This approach has two key advantages over the traditional definition (Pozrikidis, 2018; Benzi et al., 2020) in the spectral domain. First, it allows defining the fractional Laplacian based on any choice of graph Laplacian, including those with negative or complex spectrum such as the SNA. Secondly, the SVD is computationally more efficient and numerically more stable than the Jordan decomposition, which would be necessary if the fractional Laplacian was defined in the spectral domain.

Consider a directed graph with SNA \(\) and its SVD \(=^{}\), where \(\), \(^{N N}\) are unitary matrices and \(^{N N}\) is a diagonal matrix. Given \(\), we define the _\(\)-fractional graph Laplacian_2 (\(\)-\(F\!GL\) in short) as

\[^{}^{}^{ }\,.\]

In undirected graphs, the \(\)-FGL preserves the sign of the eigenvalues \(\) of \(\) while modifying their magnitudes, i.e., \(()||^{}\). 3

The \(\)-FGL is generally less sparse than the original SNA, as it connects nodes that are not adjacent in the underlying graph. The next theorem proves that the weight of such "virtual" edges is bounded.

**Theorem 4.1**.: _Let \(\) be a directed graph with SNA \(\). For \(>0\), if the distance \(d(i,j)\) between nodes \(i\) and \(j\) is at least 2, then_

\[|(^{})_{i,j}|(1+}{ 2})(\|}{2(d(i,j)-1)})^{}\,.\]

We provide a proof of Theorem 4.1 in Appendix C. In Figure 2(a), we visually represent the cycle graph with eight nodes and the corresponding \(\)-FGL entries. We also refer to Figure 2(b), where we

Figure 3: Visual representation of long-range edges built by the fractional Laplacian.

depict the distribution of \(\)-FGL entries for the real-world graphs Cora (undirected) and Chameleon (directed) with respect to the distance in the original graph. Our empirical findings align with our theoretical results presented in Theorem 4.1.

## 5 Fractional Graph Laplacian Neural ODE

This section explores two fractional Laplacian-based graph neural ODEs. First, we consider the fractional heat equation,

\[^{}(t)=-^{}(t)\,,\; (0)=_{0}\,,\] (2)

where \(_{0}^{N K}\) is the _initial condition_, \((t)^{N K}\) for \(t>0\) and \(\). We assume that the _channel mixing matrix_\(^{K K}\) is a symmetric matrix. Second, we consider the fractional Schrodinger equation,

\[^{}(t)=i\;^{}(t)\,,\; (0)=_{0}\,,\] (3)

where \(_{0},(t)^{N K}\) and \(^{K K}\) is unitary diagonalizable. Both (2) and (3) can be analytically solved. For instance, the solution of (2) is given by \(()(t)=(-t\,^{}) (_{0})\), where \(\) denotes the Kronecker product and \(()\) represents the vectorization operation. However, calculating the exact solution is computationally infeasible since the memory required to store \(^{}\) alone grows as \((NK)^{2}\). Therefore, we rely on numerical schemes to solve (2) and (3).

In the remainder of this section, we analyze the Dirichlet energy for solutions to (2) and (3). We begin with the definition of oversmoothing.

**Definition 5.1**.: _Neural ODE-based GNNs are said to oversmooth if the normalized Dirichlet energy decays exponentially fast. That is, for any initial value \(_{0}\), the solution \((t)\) satisfies for every \(t>0\)_

\[|((t)}{\|(t)\|_{2}})- (-)|(-Ct)\,,\;C>0\,.\]

Definition 5.1 captures the _actual_ smoothness of features by considering the normalized Dirichlet energy, which mitigates the impact of feature amplitude (Cai et al., 2020; Di Giovanni et al., 2023). Additionally, Proposition 3.6 shows that the normalized Dirichlet energy is intimately related to the numerical range of \(-\) of the underlying graph. This shows that the Dirichlet energy and eigenvalues (or _frequencies_) of the SNA are intertwined, and one can equivalently talk about Dirichlet energy or frequencies (see also Lemma D.2). In particular, it holds that

\[0((t)}{\|(t)\|_{2}}) -\|}{2}\,.\]

As seen in Section 3, the minimal possible value attained by the normalized Dirichlet energy is often strictly greater than \(0\) for directed graphs. This indicates that GNNs on general directed graphs inherently cannot oversmooth to the same extent as in undirected. However, we prove that a vanilla GCN implementing the directed SNA oversmooths with respect to Definition 5.1, see Appendix E.3.

### Frequency Analysis for Graphs with Normal SNA

This subsection focuses on the frequency analysis of FGL-based Neural ODEs for undirected graphs. Most classical GNNs (Kipf et al., 2017; Velickovic et al., 2018) and also graph neural ODEs (Chamberlain et al., 2021; Eliasof et al., 2021) have been shown to oversmooth. Di Giovanni et al. (2023) proved that the normalized Dirichlet energy for GNNs based on (2) with \(=1\) can not only converge to its minimal value but also to its maximal possible value. A GNN exhibiting this property is then termed _Highest-Frequency-Dominant (HFD)_.

However, in real-world scenarios, most graphs are not purely homophilic nor purely heterophilic but fall somewhere in between. Intuitively, this suggests that mid-range frequencies might be more suitable. To illustrate this intuition, consider the cycle graph as an example. If we have a homophily of \(1\), low frequencies are optimal; with a homophily equal to \(0\), high frequencies are optimal. Interestingly, for a homophily of \(}{{2}}\), the mid-range frequency is optimal, even though the eigendecomposition is label-independent. More information on this example can be found in Figure 4 and Appendix F. Based on this observation, we propose the following definition to generalize the concept of HFD, accommodating not only the lowest or highest frequency but all possible frequencies.

**Definition 5.2**.: _Let \( 0\). Neural ODE-based GNNs initialized at \(_{0}\) are \(\)-Frequency-Dominant (\(\)-FD) if the solution \((t)\) satisfies_

\[((t)}{\|(t)\|_{2}}).\]

_Suppose \(\) is the smallest or the largest eigenvalue with respect to the real part. In that case, we call it Lowset-Frequency-Dominant (LFD) or Highest-Frequency-Dominant (HFD), respectively._

In the following theorem, we show that (2) and (3) are not limited to being LFD or HFD, but can also be mid-frequency dominant.

**Theorem 5.3**.: _Let \(\) be an undirected graph with SNA\(\). Consider the initial value problem in (2) with \(^{K K}\) and \(\). Then, for almost all initial values \(_{0}^{N K}\) the following holds._

* _The solution to (_2_) is either_ HFD _or_ LFD_._
* _Let_ \(_{+}()\) _and_ \(_{-}()\) _be the smallest positive and negative non-zero eigenvalue of_ \(\)_, respectively. The solution to (_2_) is either_ \((1-_{+}())\)_-FD or_ \((1-_{-}())\)_-FD._

_Furthermore, the previous results also hold for solutions to the Schrodinger equation (3) if \(^{K K}\) has at least one eigenvalue with non-zero imaginary part._

Theorem 5.3\((>0)\) generalizes the result by Di Giovanni et al. (2023) for \(=1\) to arbitrary positive values of \(\). The convergence speed in Theorem 5.3\((>0)\) depends on the choice of \(\). By selecting a variable \(\) (e.g., as a learnable parameter), we establish a flexible learning framework capable of adapting the convergence speed of the Dirichlet energy. A slower or more adjustable convergence speed facilitates broader frequency exploration as it converges more gradually to its maximal or minimal value. Consequently, the frequency component contributions (for finite time, i.e., in practice) are better balanced, which is advantageous for graphs with different homophily levels. Theorem 5.3\((<0)\) shows that solutions of the fractional neural ODEs in (2) and (3) are not limited to be LFD or HFD. To demonstrate this and the other results of Theorem 5.3, we solve (2) using an explicit Euler scheme for different choices of \(\) and \(\) on the Cora and Chameleon graphs. The resulting evolution of the Dirichlet energy with respect to time is illustrated in Figure 5. Finally, we refer to Theorem D.5 in Appendix D.1 for the full statement and proof of Theorem 5.3.

**Remark 5.4**.: _Theorem 5.3 is stated for the analytical solutions of (2) and (3), respectively. As noted in Section 5, calculating the analytical solution is infeasible in practice. However, we show in Appendices D.2 to D.3 that approximations of the solution of (2) and (3) via explicit Euler schemes satisfy the same Dirichlet energy convergence properties if the step size is sufficiently small._

**Remark 5.5**.: _Theorem 5.3 can be generalized to all directed graphs with normal SNA, i.e., satisfying the condition \(^{}=^{}\). For the complete statement, see Appendix D.1._

Figure 4: Eigendecomposition of \(\) for the cycle graph \(C_{8}\) (see Appendix F). The first two rows show the eigenvectors corresponding to the eigenvalues \(\). The last row shows how the (label-unaware) eigendecomposition can be used to study homophily, whose definition requires the labels.

### Frequency Dominance for Directed Graphs

Section 5.1 analyzes the Dirichlet energy in graphs with normal SNA. However, the situation becomes significantly more complex when considering generic directed graphs. In our experiments (see Figure 5), we observe that the solution to (2) and (3) does not necessarily lead to oversmoothing. On the contrary, the solution can be controlled to exhibit either LFD or HFD for \(>0\), and mid-frequency-dominance for \(<0\) as proven for undirected graphs in Theorem 5.3. We present an initial theoretical result for directed graphs, specifically in the case of \(=1\).

**Theorem 5.6**.: _Let \(\) be a directed graph with SNA\(\). Consider the initial value problem in (2) with diagonal channel mixing matrix \(^{K K}\) and \(=1\). Suppose \(_{1}()\) is unique. For almost all initial values \(_{0}^{N K}\), the solution to (2) is either HFD or LFD._

The proof of Theorem 5.6 is given in Appendix E.1. Finally, we refer to Appendix E.2 for the analogous statement and proof when the solution of (2) is approximated via an explicit Euler scheme.

## 6 Numerical Experiments

This section evaluates the fractional Laplacian ODEs in node classification by approximating (2) and (3) with an explicit Euler scheme. This leads to the following update rules

\[_{t+1}=_{t}-h\,^{}_{t}\,,\ _{t+1}=_{t}+i\;h\;^{}_{t} \,,\] (4)

for the heat and Schrodinger equation, respectively. In both cases, \(\), \(\) and \(h\) are learnable parameters, \(t\) is the layer, and \(_{0}\) is the initial nodes' feature matrix. In accordance with the results in Section 5, we select \(\) as a diagonal matrix. The initial features \(_{0}\) in (4) are encoded through a \(\), and the output is decoded using a second MLP. We refer to the resulting model as _FLODE_ (fractional Laplacian ODE). In Appendix A, we present details on the baseline models, the training setup, and the exact hyperparameters.

Figure 5: Convergence of Dirichlet energy for the solution of equation (2) using an explicit Euler scheme with a step size of \(h=10^{-1}\). We consider different \(\)-FGL in (2) and choose \(\) as a random diagonal matrix. In the left plot, \(\) has only a negative spectrum, while in the right plot, \(\) has only a positive spectrum. The black horizontal line represents the theoretical limit based on Theorem 5.3.

Ablation Study.In Appendix A.3, we investigate the influence of each component (learnable exponent, ODE framework, directionality via the SNA) on the performance of FLODE. The adjustable fractional power in the FGL is a crucial component of FLODE, as it alone outperforms the model employing the ODE framework with a fixed \(=1\). Further, Appendix A.3 includes ablation studies that demonstrate FLODE's capability to efficiently scale to large depths, as depicted in Figure 8.

Real-World Graphs.We report results on \(6\) undirected datasets consisting of both homophilic graphs, i.e., Cora (McCallum et al., 2000), Citeseer (Sen et al., 2008) and Pubmed (Namata et al., 2012), and heterophilic graphs, i.e., Film (Tang et al., 2009), Squirrel and Chameleon (Rozemberczki et al., 2021). We evaluate our method on the directed and undirected versions of Squirrel, Film, and Chameleon. In all datasets, we use the standard \(10\) splits from (Pei et al., 2019). The choice of the baseline models and their results are taken from (Di Giovanni et al., 2023). Further, we test our method on heterophily-specific graph datasets, i.e., Roman-empire, Minesweeper, Tolokers, and Questions (Platonov et al., 2023). The splits, baseline models, and results are taken from (Platonov et al., 2023). The top three models are shown in Table 1, and the thorough comparison is reported in Table 4. Due to memory limitations, we compute only \(30\%\) of singular values for Pubmed, Roman-Empire, and Questions, which serve as the best low-rank approximation of the original SNA.

Synthetic Directed Graph.We consider the directed stochastic block model (DSBM) datasets (Zhang et al., 2021). The DSBM divides nodes into \(5\) clusters and assigns probabilities for interactions between vertices. It considers two sets of probabilities: \(\{_{i,j}\}\) for undirected edge creation and \(\{_{i,j}\}\) for assigning edge directions, \(i,j\{1, 5\}\). The objective is to classify vertices based on their clusters. In the first experiment, \(_{i,j}=^{*}\) varies, altering neighborhood information's importance. In the second experiment, \(_{i,j}=^{*}\) varies, changing directional information. The results are shown in Figure 6 and Table 6. The splits, baseline models, and results are taken from (Zhang et al., 2021).

Results.The experiments showcase the flexibility of FLODE, as it can accommodate various types of graphs, both directed and undirected, as well as a broad range of homophily levels. While other methods, such as MagNet (Zhang et al., 2021), perform similarly to our approach, they face limitations when applied to certain graph configurations. For instance, when applied to undirected graphs, MagNet reduces to ChebNet, making it unsuitable for heterophilic graphs. Similarly, GRAFF

Table 1: Test accuracy (Film, Squirrel, Chameleon, Citeseer) and test AUROC (Minesweeper, Tolokers, Questions) on node classification, top three models. The thorough comparison is reported in Table 4, Appendix A: FLODE consistently outperforms the baseline models GCN and GRAFF, and it achieves results comparable to state-of-the-art.

(Di Giovanni et al., 2023) performs well on undirected graphs but falls short on directed graphs. We note that oftentimes FLODE learns a non-trivial exponent \( 1\), highlighting the advantages of \(\)-based \(\) (see, e.g., Table 5). Furthermore, as shown in Table 9 and Appendix A.3, our empirical results align closely with the theoretical results in Section 5.

## 7 Conclusion

In this work, we introduce the concepts of Dirichlet energy and oversmoothing for directed graphs and demonstrate their relation with the \(\). Building upon this foundation, we define fractional graph Laplacians in the singular value domain, resulting in matrices capable of capturing long-range dependencies. To address oversmoothing in directed graphs, we propose fractional Laplacian-based graph ODEs, which are provably not limited to \(\) behavior. We finally show the flexibility of our method to accommodate various graph structures and homophily levels in node-level tasks.

Limitations and Future Work.The computational cost of the SVD grows cubically in \(N\), while the storage of the singular vectors grows quadratically in \(N\). Both costs can be significantly reduced by computing only \(k N\) singular values via truncated SVD (Figure 7), giving the best \(k\)-rank approximation of the \(\). Moreover, the SVD can be computed offline as a preprocessing step.

The frequency analysis of \(\)-\(\) neural \(\) in directed graphs is an exciting future direction. It would also be worthwhile to investigate the impact of choosing \( 1\) on the convergence speed of the Dirichlet energy. Controlling the speed could facilitate the convergence of the Dirichlet energy to an _optimal_ value, which has been shown to exist in synthetic settings (Keriven, 2022; X. Wu et al., 2022). Another interesting future direction would be to analyze the dynamics when approximating the solution to the \(\) neural \(\) using alternative numerical solvers, such as adjoint methods.

Figure 6: Experiments on directed stochastic block model. Unlike other models, FLODE’s performances do not deteriorate as much when changing the inter-cluster edge density \(^{*}\).

Figure 7: Effect of truncated SVD on test accuracy (orange) for standard directed real-world graphs. The explained variance, defined as \(_{i=1}^{k}_{i}^{2}/_{j=1}^{N}_{j}^{2}\), measures the variability the first \(k\) singular values explain. For chameleon, the accuracy stabilized after \(570\) (\(25\%\)) singular values, corresponding to an explained variance of \(0.998\). For squirrel, after \(1600\) (\(31\%\)) singular values, which correspond to an explained variance \(0.999\), the improvement in test accuracy is only marginal.