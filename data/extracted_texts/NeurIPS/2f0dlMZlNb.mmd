# Effective Targeted Attacks for

Adversarial Self-Supervised Learning

Mineeon Kim\({}^{1}\), Hyeonjeong Ha\({}^{1}\), Sooel Son\({}^{1}\), Sung Ju Hwang\({}^{1,2}\)

\({}^{1}\)Korea Advanced Institute of Science and Technology (KAIST), \({}^{2}\)DeepAuto.ai

{mineeonkim, hyeonjeongha, sl.son, sjhwang82}@kaist.ac.kr

###### Abstract

Recently, unsupervised adversarial training (AT) has been highlighted as a means of achieving robustness in models without any label information. Previous studies in unsupervised AT have mostly focused on implementing self-supervised learning (SSL) frameworks, which maximize the instance-wise classification loss to generate adversarial examples. However, we observe that simply maximizing the self-supervised training loss with an untargeted adversarial attack often results in generating ineffective adversaries that may not help improve the robustness of the trained model, especially for non-contrastive SSL frameworks without negative examples. To tackle this problem, we propose a novel positive mining for targeted adversarial attack to generate effective adversaries for adversarial SSL frameworks. Specifically, we introduce an algorithm that selects the most confusing yet similar target example for a given instance based on entropy and similarity, and subsequently perturbs the given instance towards the selected target. Our method demonstrates significant enhancements in robustness when applied to non-contrastive SSL frameworks, and less but consistent robustness improvements with contrastive SSL frameworks, on the benchmark datasets.

## 1 Introduction

Enhancing the robustness of deep neural networks (DNN) remains a crucial challenge for their real-world safety-critical applications, such as autonomous driving. DNNs have been shown to be vulnerable to various forms of attacks, such as imperceptible perturbations , various types of image corruptions , and distribution shifts , which can lead DNNs to make incorrect predictions. Many prior studies have proposed using supervised adversarial training (AT)  to mitigate susceptibility to imperceptible adversarial perturbation, exploiting class label information to generate adversarial examples. However, achieving robustness in the absence of labeled information has been relatively understudied, despite the recent successes of self-supervised learning across various domains and tasks.

Recently, self-supervised learning (SSL) frameworks have been proposed to obtain transferable visual representations by learning the similarity and differences between instances of augmented training data. Such prior approaches include those utilizing contrastive learning between positive and negative pairs (e.g., Chen et al.  (SimCLR), He et al.  (MoCo), Zbontar et al.  (Barlow-twins)), as well as those utilizing similarity loss solely between positive pairs (e.g., Grill et al.  (BYOL), Chen and He  (SimSiam)). To achieve robustness in these frameworks, Kim et al.  and Jiang et al.  have proposed adversarial SSL methods using contrastive learning , which generate adversarial examples that maximize the instance-wise classification loss.

Unfortunately, deploying this contrastive framework often becomes computationally expensive as it requires a large batch size for training in order to attain a high level of performance . Specifically, when a given memory and computational budget is limited, such as with edge devices, performingcontrastive SSL becomes no longer viable or practical as an option, as it may not obtain sufficiently high performance using a small batch size.

Alternatively, non-contrastive, positive-only SSL frameworks have been proposed resort to maximizing consistency across two differently augmented samples of the same instance, i.e., positive pairs, [17; 7; 39], without the need of negative instances. These approaches improve the practicality of SSL for those limited computational budget scenarios. However, leveraging prior adversarial attacks that maximize the self-supervised learning loss in these frameworks results in extremely poor performance compared to those of adversarial contrastive SSL methods (Table 1). The suboptimality of the deployed attacks causes to learn limited robustness and leads to the generation of ineffective adversarial examples, which fail to improve robustness in the SSL frameworks trained using them. As shown in Figure 0(c), the attack in the inner loop of the adversarial training loss, designed to maximize the distance between two differently augmented samples, perturbs a given example to a random position in the latent space. Thus, the generated adversarial samples have little impact on the final robustness. The suboptimality of the attacks also can be occurred in contrastive adversarial SSL that also contains positive pairs, when simply maximizing the contrastive loss. As shown in Figure 0(b), contrastive learning treats all positive and negative pairs equally regardless of their varying importance in generating effective adversarial examples.

To address this issue, we propose **T**argeted **A**ttack for **RO**bust self-supervised learning (TARO). TARO is designed to guide the generation of effective adversarial examples by conducting **targeted attacks** that perturb a given instance toward a target instance to enhance the robustness of an SSL framework (Figure 1). The direction of attacks is assigned using our target selection algorithm that chooses the most confusing yet similar sample for a given instance based on the entropy and similarity. By targeting the attacks toward specific latent spaces that are more likely to improve robustness on positive-pairs, TARO improves the robustness of SSL, regardless of the underlying SSL frameworks. Notably, as the positive-pair only SSL has gained attention in recent times, our proposed method becomes crucial for the ongoing safe utilization of these frameworks in real-world applications.

The main contributions can be summarized as follows:

* We observe that simply maximizing the training loss of self-supervised learning (SSL) may lead to suboptimality of attacks as the main cause of the limited robustness in SSL frameworks, especially those that rely on maximizing the similarity between the single pair of augmented instances.
* To address this issue, we propose a novel approach, **T**argeted **A**ttack for **RO**bust self-supervised learning (TARO), which aims to improve the robustness of SSL by conducting targeted attacks on the positive-pair that perturb the given instance toward the most confusing yet similar latent space, based on entropy and similarity of the latent vectors.
* We experimentally show that TARO is able to obtain consistently improved robustness of SSL, regardless of underlying SSL frameworks, including contrastive- and positive-pair only SSL frameworks.

Figure 1: **Motivation. In supervised adversarial learning (a), perturbation is generated to maximize the cross-entropy loss, which pushes adversarial examples to the decision boundaries of other classes. In adversarial contrastive SSL (b), perturbation is generated to minimize the similarity (red line) between positive pairs while maximizing the similarity (blue lines) between negative pairs. In positive-only adversarial SSL (c), minimizing the similarity (red) between positive pairs. However, adversarial examples in adversarial SSL impose weaker constraints in generating effective adversarial examples than does supervised AT due to ineffective positive pairs. To overcome this limitation, we suggest a selectively targeted attack for SSL that maximizes the similarity (blue) to the most confusing target instance (yellow oval in (b) and (c)).**

Related Work

Adversarial trainingSzegedy et al.  showed that imperceptible perturbation to a given input image may lead a DNN model to misclassify the input into a false label, demonstrating the vulnerability of DNN models to adversarial attacks. Goodfellow et al.  proposed the fast gradient sign method (FGSM), which perturbs a given input to add imperceptible noise in the gradient direction of decreasing the loss of a target model. They also demonstrated that training a DNN model over perturbed as well as clean samples improves the robustness of the model against FGSM attacks. Follow-up works [27; 2] proposed diverse gradient-based strong attacks, and Madry et al.  proposed a projected gradient descent (PGD) attack and a robust training algorithm leveraging a minimax formulation; they find an adversarial example that achieves a high loss while minimizing the adversarial loss across given data points. TRADES  proposed minimizing the Kullback-Leibler divergence (KLD) over clean examples and their adversarial counterparts, thus enforcing consistency between their predictions. Recently, leveraging additional unlabeled data  and conducting additional attacks  have been proposed. Carmon et al.  proposed using Tiny ImageNet  images as pseudo labels, and Gowal et al.  proposed using generated images from generative models to learn richer representations with additional data.

Self-supervised learningDue to the high annotation cost of labeling data, SSL has gained a wide attention [11; 41; 35; 36]. Previously, SSL focused on solving a pre-task problem of collaterally obtaining visual representation, such as solving a jigsaw puzzle , predicting the relative position of two regions , or impainting a masked area . However, more recently, SSL has shifted to utilizing inductive bias to learn the invariant visual representation of paired transformed images. This is accomplished through contrastive learning, which utilizes both positive pairs and negative pairs, that is differently transformed images and other images from the same batch, respectively [6; 19]. Additionally, some studies have proposed using only positive pairs in SSL and have employed techniques such as momentum networks  or stop-gradient . In this paper, we annotate these approaches as contrastive SSL, and positive-pair only SSL, respectively.

Adversarial self-supervised learningThe early stage of adversarial SSL methods [23; 22] employed contrastive learning to achieve a high level of robustness without any class labels. Adversarial self-supervised contrastive learning [23; 22] generated an instance-wise adversarial example that maximizes the contrastive loss against its positive and negative samples by conducting untargeted attacks. Both methods achieved robustness, but at the cost of requiring high computation power due to the large batch size needed for contrastive learning. On the other hand, Gowal et al.  utilized only positive samples to obtain adversarial examples by maximizing the similarity loss between the latent vectors from the online and target networks, allowing this method greater freedom regarding the batch size. However, it exhibited relatively worse robustness than the adversarial self-supervised contrastive learning frameworks. Despite the advances in the SSL framework (i.e., positive-pair only SSL), a simple combination of untargeted adversarial learning and advanced SSL does not guarantee robustness. To overcome such a vulnerability in positive-pair only SSL, we propose a targeted attack leveraging a novel score function designed to improve robustness.

## 3 Positive-Pair Targeted Attack in Adversarial Self-Supervised Learning

Adversarial SSL and supervised adversarial learning utilize adversarial examples in a similar manner. Specifically, adversarial SSL generates instance-wise adversarial examples in the direction of maximizing the training loss for better robustness. However, this approach exhibits an insufficient level of robustness especially in the positive-pair only self-supervised learning framework due to generating highly suboptimal adversarial examples.

We argue that simply maximizing the training loss, dubbed as an untargeted attack, in positive-pair only SSL limits the diversity of adversarial examples which eventually leads to limited robustness. We theoretically show that range of perturbation is smaller when the positive-pair only SSL objective is employed in an untargeted attack than the contrastive objective in simple two-class tasks. Furthermore, we empirically demonstrate poorer robustness when we naively merge untargeted attack and positive-pair only SSL approaches [17; 7], compared to contrastive-based adversarial SSL [23; 22].

To remedy such a shortcoming, we propose a simple yet effective targeted adversarial attack to increase the diversity of the generated attack. Moreover, we empirically suggest novel positive mining the target for the targeted adversarial attack that contributes to generating more effective and stronger adversarial examples, thus improving the robustness beyond that of previous adversarial SSL approaches. In this section, we first recap supervised adversarial training, self-supervised learning, and previous adversarial SSL methods. We then demonstrate theoretical intuition on our motivation and describe our proposed targeted adversarial SSL framework, TARO, in detail.

### Preliminary

Supervised adversarial trainingWe first recap supervised adversarial training with our notations. We denote the dataset \(=\{(x_{i},y_{i})\}\), where \(x_{i} R^{D}\) is a input, and \(y_{i} R^{N}\) is its corresponding label from the \(N\) classes. In this supervised learning task, the model is \(f_{}:X Y\), where \(\) is a set of model parameters to train.

Given \(\) and \(f_{}\), an _adversarial attack_ perturbs a given source image that maximizes the loss within a certain radius from it (e.g., \(_{}\) norm balls). For example, \(_{}\) attack is defined as follows:

\[^{t+1}=_{B(0,)}^{t}+ _{^{t}}_{}f(,x+^{t}), y,\] (1)

where \(B(0,)\) is the \(_{}\) norm-ball of radius \(\), \(\) is the projection function to the norm-ball, \(\) is the step size of the attacks, and \(()\) is the sign of the vector. Also, \(\) represents the perturbations accumulated by \(()\) over multiple iterations \(t\), and \(_{}\) is the cross-entropy loss. In the case of PGD , the attack starts from a random point within the epsilon ball and performs \(t\) gradient steps, to obtain a perturbed sample. _Adversarial training_ (AT) is a straightforward way to improve the robustness of a DNN model; it minimizes the training loss that embeds the adversarial perturbation (\(\)) in the inner loop (Eq. 1).

Self-supervised learningRecent studies on self-supervised learning (SSL) have proposed methods to allow their models to learn invariant features from transformed images, thus learning semantic visual representations that are beneficial for diverse tasks [6; 19; 17; 7; 39]. In this paper, we aim at improving the robustness of the two most popular types of SSL frameworks: positive-pair only SSL (e.g., BYOL, SimSiam) and contrastive SSL (e.g., SimCLR) frameworks.

We start by briefly describing a representative contrastive SSL, SimCLR . SimCLR is designed to maximize the agreement between different augmentations of the same instance in the learned latent space while minimizing the agreement between different instances. Differently augmented examples from the same instance are defined as positive pairs, and all other instances in the same batch are considered negative examples. Then, the training loss of SimCLR is defined as follows:

\[_{}(x,\{x_{}\},\{x_{}\}) -\{x_{}\}}(( ,x_{p})/)}{_{x_{p}\{x_{}\}}( (,x_{p})/)+_{x_{n}\{x_{}\}}(( ,x_{n})/)},\] (2)

where \(z\) is the latent vector of input \(x\), \(\), \(\) stands for positive pair and negative pairs of \(x\), respectively, and \(\) denotes the cosine similarity function.

A representative positive-pair only SSL framework is SimSiam . SimSiam consists of the encoder \(f\), followed by the projector \(g\), and then the predictor \(h\); both \(g\) and \(h\) are multi-layer perceptrons (MLPs). Given the dataset \(}=\{X\}\) and the transformation function \(\) that augments the images \(x X\), it is designed to maximize the similarity between the differently transformed images and avoid representational collapse by applying the stop-gradient operation to one of the transformed images as follows:

\[_{}(x,x_{})=-} }}{||z_{}||_{2}}-}}{||p_{}||_{2}}},\] (3)

where \(z=g f(_{1}(x))\), \(z_{}=g f(_{2}(x))\), and \(p=h z\), \(p=h z_{}\) are output vectors of the projector \(g\) and predictor \(h\), respectively. Before calculating the loss, SimSiam detaches the gradient on the \(z\), which is called the _stop-gradient_ operation. This stop-gradient operation helps the model prevent representational collapse without any momentum networks, by making an encoder to act as a momentum network.

Adversarial SSLTo achieve robustness in SSL frameworks, prior studies have proposed adversarial SSL methods . They generate adversarial examples by maximizing the training loss, dubbed as an untargeted attack, of their base SSL frameworks. For example, the inner loop of an adversarial attack for Kim et al.  is structured as follows:

\[^{t+1}=_{B(0,)}^{t}+_{^{t}}_{1}(x)+^{t}, _{2}(x),\] (4)

where the perturbation maximizes the \(\). For adversarial contrastive SSL approaches , \(=_{}\) is the contrastive loss in Eq. 2, so that adversarial examples are generated to minimize the similarity between positive pairs and maximize the similarity between negative pairs. For the positive-pair only SSL, adversarial examples are generated to maximize the similarity loss, \(=_{}\) (Eq. 3), between positive-pairs only. However, as shown in Table 1, positive-pair only SSL results in significantly poor robustness compared to the adversarial contrastive SSL approaches. This is because using the naive training loss function of positive-pair only SSL in the attack hinders the generation of effective attack images for robust representation, as we theoretically show the range of perturbations is smaller (Section 3.2). To address this issue, we propose a targeted adversarial attack that can select more effective examples to make more diverse perturbations.

### Theoretical Motivation: Adversarial Perturbations in Positive-only SSL

A model is considered to have a better generalization of adversarial robustness when the model can maintain its performance across a wide range of adversarial perturbations. Hence, the ability of the attack loss to generate a diverse range of perturbations during training is a crucial factor that influences the model's final robust generalization.

However, we found the theoretical motivation that positive-pair only SSL loss (\(_{}\)) could not provide a wide range of adversarial perturbations as contrastive loss (\(_{}\)) does. We simplify the problem into simple binary classification with the linear layer model to demonstrate our theoretical motivation. Let us denote adversarial perturbations that are generated with both losses as follows,

\[x_{}^{}=x+_{}\{\}\| \|,\] (5)

where we approximate the loss of \(_{}\) in the \(_{1}\) distance function between the positive pair and the loss of \(_{}\) into combination of two \(_{1}\) distance functions of one positive- and one negative- pair. In both cases, a \(\) maximizes the respective loss, subject to the constraint that the norm of \(\) is less than or equal to \(\). The objective in positive-only SSL is to make the perturbed and original samples dissimilar as follows,

\[_{}=*{arg\,max}_{}|f(x)-f(x+)|.\] (6)

while the objective of nt-sent is to make the perturbed sample dissimilar to the positive pair and similar to the negative pair as follows,

\[_{}=*{arg\,max}_{}|f(x)-f(x+)| -|f(x_{neg})-f(x+)|.\] (7)

**Theorem 3.1** (Perturbation range of self-supervised learning loss).: _Given a model trained under the positive-only distance loss, the adversarial perturbations \(_{}\) are likely to be smaller than those perturbations \(_{}\), from a model trained under the positive-pair and negative-pair distance loss. Formally, \(\|_{}\|_{}<\|_{}\|_{}\)._

These theoretical insights are also supported by the empirical experiments in Table 1 that a model trained with adversarial examples generated using positive- and negative- paired contrastive loss (\(_{}\)) have better adversarial robustness generalization because it is exposed to a wider range of perturbations during training than models that are trained with the positive-only similarity loss (\(_{}\)). The detailed proof and the empirical analysis are in the Supplementary.

   Attack loss & Method & Clean & PGD \\   & ACL  & 79.96 & 39.37 \\  & RoCL  & 78.14 & 42.89 \\  Positive-only & BYORL  & 72.65 & 16.20 \\ similarity & SimSiam* & 71.78 & 32.28 \\    \\  \\ 

Table 1: Comparison of different attack losses on CIFAR-10 using PGD attack.

### Targeted Adversarial SSL

We propose a simple yet effective targeted adversarial attack to generate effective adversarial examples in a positive-only SSL scenario. In this section, we first show the theoretical intuition of our approach and describe our overall framework to further improve the robustness of the adversarial SSL method by performing targeted attacks wherein targets are selected according to the proposed score function.

Targeted adversarial attack to different sampleWe argue that leveraging untargeted adversarial attacks in positive pairs only SSL still leaves a large room for better robustness. To enlarge the diversity of the attacks, we propose simple targeted adversarial attacks for positive-pair only SSL. The loss for such adversarial attacks is as follows:

\[^{t+1}=_{B(0,)}^{t}+_{^{t}}_{}x+^{t},x^{ }),\] (8)

where \(_{}\) is \(_{}\)=\(-_{}\), and \(x^{}\) is a _selected target_ within the batch.

Therefore, in the previous simplified scenario described in Section 3.2, conducting the randomly selected targeted attack could increase the range of the perturbation that is generated with positive-pair only similarity loss as follow,

\[_{}=*{arg\,max}_{}|f(x+ )-f(x_{})|.\] (9)

Through triangle inequality, the targeted attack may increase the range of the perturbation and eventually leverage the overall robustness.

**Theorem 3.2** (Perturbation range of targeted attack).: _Given a model trained under the \(_{}\) loss, the adversarial perturbations \(_{}\) are larger than the adversarial perturbations \(_{}\) from a model trained under the \(_{}\). Formally, \(\|_{}\|_{}>\|_{}\|_{}\)._

However, these are theoretical expectations in a simplistic scenario. To further substantiate this, we empirically observed that even a simple targeted attack, with a random target in the batch, significantly improves robustness in a positive-pair only SSL scenario, as shown in Table 2. Therefore, based on these theoretical and empirical insights, we propose to search more effective target for positive-pair targeted attack to boost the robustness of the self-supervised learning frameworks through experimental observations. The detailed proof of Theorem 3.2 is in the Supplementary.

Similarity and entropy-based target selection for targeted attackIn our theoretical analysis and empirical observations, we established that targeted attacks can significantly enhance overall robustness in SSL, except for the target itself. To this end, we propose a score function, denoted as \((x,)\), which aims to identify the most suitable target that is distinct from the input while effectively contributing to improved robustness. Following the studies of Kim et al. , Ding et al. , Hitaj et al. , we prioritize high-entropy examples or those located near decision boundaries as crucial for generating effective adversarial examples in supervised adversarial training. Accordingly, we recommend selecting a target distinct from itself, yet induces confusion, creating adversarial examples that are located close to decision boundaries (Eq.11). The score function yields the most potent target (\(x^{}\)) for a given base image (\(x\)). Subsequently, the targeted attack generates a perturbation, maximizing the similarity to the target \(x^{}\) for the base image \(x\).

To this end, we design the score function based on the similarity and entropy values, without using any class information, as follows:

\[_{}(x,x^{})=p^{}/(p^{ }/),\ _{}(x,x^{})=}}{|e^{}|_{2}},\] (10)

\[_{}(x,x^{})=_{}+ _{}.\] (11)

where \(p=h g f(x)\) and \(e=f(x)\) are output vectors of predictor \(h\) and encoder \(f\), respectively. Overall, the score function \(\) incorporates both cosine similarity and entropy. The cosine similarity

   SSL & Attack Type & Clean & PGD \\   & untargeted attack & 75.4 & 4.34 \\  & targeted attack & **83.50** & **31.62** \\   & untargeted attack* & 66.36 & 36.53 \\  & targeted attack & **77.08** & **47.58** \\    \\ 

Table 2: Effect of random targeted attack in positive-pair only SSL in CIFAR-5.

is calculated between features of base images and candidate images in the differently augmented batch. The entropy is calculated with the assumption that the vector \(p\) represents the logit of an instance as Caron et al. , Kim et al. . Our score function is designed to select an instance (\(x^{}\)) that is different but confused with the given image (\(x\)), thus facilitating the generation of effective adversarial examples for targeted attack (Figure 1). The experimental results in Figure 2b verify that the score function successfully selects such instances, as intended.

Robust self-supervised learning with targeted attacksThe TARO framework starts by selecting a target image based on the score function (\(\)). It then generates adversarial examples using the selected target and performs adversarial training with them.

For a positive pair, represented as differently transformed augmentations \(_{1}(x),_{2}(x)\), the target images \(_{2}(x^{})\) and \(_{1}(x^{})\) are selected respectively, as ones with the maximum score within the batch from the score function (\(\)) in Eq. 11. Then, we generate adversarial examples, i.e., \(_{1}(x)^{adv},_{2}(x)^{adv}\), for each transformed input with our proposed targeted attack (Eq. 8), where the targeted loss \(_{}=-_{}\) maximizes the similarity to the selected target \(_{2}(x^{})\) and \(_{1}(x^{})\), respectively. Finally, we maximize the agreement between the representations of adversarial images (\(_{1}(x)^{adv}\) and \(_{2}(x)^{adv}\)) and the clean image \(t_{1}(x)\) as follows:

\[_{}=(_{1}(x),_{1}(x)^{ adv})+(_{1}(x)^{adv},_{2}(x)^{adv})+( _{2}(x)^{adv},_{1}(x)),\] (12)

where \(\) is Eq. 3 for the SimSiam framework. Since all three instances have the same identity, we maximize the similarity between the clean and adversarial examples.

TARO could be also applied to positive pairs in contrastive adversarial SSL methods (e.g., RoCL , ACL ). Since contrastive SSL does not have a predictor, we use the output of the projector as \(p\) in Eq. 10 to select the target for positive-pair. Then, when we apply our targeted attack to their instance-wise attacks, as follows:

\[_{}=_{}(_ {1}(x),\{\},_{1}(x)_{})+_{ }(_{1}(x),_{2}(x)),\] (13)

where the adversarial loss is a sum of the modified nt-xent loss  and similarity loss. Since TARO alters the untargeted attack of the positive pair with a targeted attack between the base image (\(_{1}(x)\)) and target image (\(_{1}(x^{})\)), we eliminate the positive pair term in nt-xent loss and add similarity loss instead. The similarity loss maximizes the cosine similarity between the \(_{1}(x)\) images and the \(_{1}(x^{})\) images which are searched by the score function. Overall, we generate adversarial examples that maximize the \(_{}\) loss as shown in Algorithm 1.

## 4 Experiment

In this section, we extensively evaluate the efficacy of TARO with both contrastive and positive-pair only adversarial SSL frameworks. First, we compare the performance of our model to previous adversarial SSL methods that do not utilize any targeted attacks in Section 4.1. Moreover, we evaluate the robustness of the learned representations across different downstream domains in Section 4.2. Finally, we analyze the reason behind the effectiveness of targeted attacks in achieving better robust representations compared to models using untargeted attacks in Section 4.3.

   Evaluation type & SSL & Attack Type & Clean & PGD & AutoAttack \\   & BYOL & \(_{}\) & 72.65 & 16.20 & 0.01 \\  & BYOL & \(_{}\) & **84.52** & **31.20** & **22.01** \\  & SimSiam & \(_{}\) & 71.78 & 32.28 & 24.41 \\  & SimSiam & \(_{}\) & **74.87** & **44.71** & **36.39** \\   & BYOL & \(_{}\) & 54.01 & 27.24 & 4.49 \\  & BYOL & \(_{}\) & **74.33** & **40.84** & **29.91** \\   & SimSiam & \(_{}\) & 68.88 & 37.84 & 31.44 \\   & SimSiam & \(_{}\) & **76.19** & **45.57** & **39.25** \\   

Table 3: Experimental results against white-box attacks on CIFAR-10. To see the effectiveness, we test TARO on positive-pair only self-supervised learning approaches, i.e., SimSiam, and BYOL.

Experimental setupWe compare TARO against previous contrastive and positive-pair only adversarial SSL approaches. Specifically, we adapt TARO on top of two contrastive adversarial SSL frameworks, RoCL , ACL  and a positive-pair only SSL framework, SimSiam , to demonstrate its efficacy in enhancing their robustness. All models use the ResNet18 backbones that are trained on CIFAR-10 and CIFAR-100 with \(_{}\) PGD attacks with the attack step of \(10\) and epsilon \(8/255\). We evaluate the robustness of our method against two types of attack, AutoAttack*  and \(_{}\) PGD attacks, with the epsilon size of \(8/255\), using the attack step of \(20\) iterations. Clean denotes the classification accuracy of the ResNet18 backbone on the original images. We further describe the experimental details in Appendix B. Code is available in https://github.com/Kim-Munseon/TARO.git

Footnote *: https://github.com/fra31/auto-attack

### Efficacy of Targeted Attacks in Adversarial SSL

We first validate whether the proposed targeted attacks in TARO contribute to improving the robustness of positive-pair adversarial SSL frameworks. To evaluate the quality of the learned representations with the SSL frameworks, we utilize linear and robust linear evaluation, as shown in Table 3. Then, we validate the generality of TARO to contrastive-based adversarial SSL frameworks (Table 5).

Robustness improvements in positive-pair only SSLWe evaluate the efficacy of TARO by comparing those to untargeted attacks on positive-pair only SSL frameworks, i.e., SimSiam and BYOL. As shown in Table 3, when replacing untargeted attacks with TARO in the positive-only SSL, TARO contributes to attaining significant gains in both robustness accuracy against PGD attacks and clean accuracy. This is due to the inherent limitations of untargeted attacks in positive-pair only SSL frameworks. In such frameworks, perturbations in any direction away from the other pair of samples will inevitably increase the SSL loss, making it challenging to generate effective adversarial examples. However, with the guidance provided by TARO, the model is able to generate stronger attack images, leading to meaningfully improved performance both on clean and adversarially perturbed images. Furthermore, we show that the untargeted attacks are not only ineffective for learning robust features, but also hinder the learning of good visual representation for clean images.

Switching from an untargeted to a targeted attack approach leads to a substantial increase in performance across both contrastive-based and positive-pair only approaches, as shown in Table 4. This advancement is particularly evident when addressing the challenge of selecting appropriate targets within positive pairs. As we have discussed in the Limitations section, our empirical score function may not be the absolute optimal algorithm for target selection. Nevertheless, it is clear that concentrating on targeted attacks in the context of positive pairs is crucial for enhancing robust representation, applicable to both clean and adversarial examples.

Robustness improvements in contrastive adversarial SSLThe robustness gains through TARO in contrastive adversarial SSL, specifically RoCL and ACL, are demonstrated in Table 5. Given that our TARO algorithm mines the positive-pair in contrastive loss, its effects on contrastive-based SSL might be more limited compared to positive-pair SSL. Despite this, TARO enhances RoCL's robustness against PGD attacks from 42.89% to 45.37% without compromising the clean accuracy. In the case of ACL, TARO fortifies the robustness against PGD attacks while maintaining performance comparable to AutoAttack.

### Evaluation on CIFAR-100

Robustness on larger benchmarks datasetsWe further validate our method on a larger dataset, CIFAR-100. In Table 6, TARO demonstrates consistent robust accuracy when compared with those of the adversarial SSL frameworks using untargeted attacks, with notably significant robustness improvements on the positive-pair only SSL. Although the clean and original robust accuracy of the positive-only SSL method is noticeably lower than that of the contrastive learning method on this particular dataset, it achieves significantly higher robust accuracy than the contrastive counterpart

   Method & Selection & Clean & PGD \\   & None & 78.14 & 42.89 \\  & Random & 79.26 & 43.45 \\  & Ours & 80.06 & 45.37 \\   & None* & 71.78 & 32.28 \\  & Random & 73.25 & 42.85 \\   & Ours & 74.87 & 44.71 \\   

*naïve adversarial training applied in SimSiam

Table 4: Ablation results on target selection.

when using our targeted attack. The results further suggest that the proposed targeted attack plays a crucial role in creating effective adversarial examples.

Transferable robustnessThe main objective of SSL is to learn transferable representations for diverse downstream tasks. Therefore, we further evaluate the transferable robustness of the pretrained representations trained using our targeted attack on novel tasks from a different dataset. We adopt the experimental setting from the previous works on supervised adversarial transfer learning  which freeze the encoder and train only the fully connected layer. We pretrained the model on CIFAR-100 and evaluate the robust transferability to CIFAR-10. In Table 7, our model also shows impressive transferable robustness both with contrastive and positive-pair only SSL, compared to those obtained by the representations learned with untargeted adversarial SSL.

### Effectiveness of TARO

In this section, we further analyze the effect of the targeted attacks in adversarial SSL to see how and why it works. 1) Analysis of the selected images by \(\), 2) Visual representation of adversarial examples that are generated with untargeted attack/targeted attack, and 3) ablation experiment on each component of the score function.

Analysis of the selected targetTo analyze which target images are selected by our score function (\(\)), we use a supervised adversarial training (AT) model. We select the target images of a single class (airplane) with the score function, and forward them to the supervised AT model to obtain their class distribution. To further examine which are the most confusing classes for the original images, we forward the base airplane images to the supervised AT model as well. As shown in Figure 1(a), airplane images are easily confused with the ship class and the bird class. Surprisingly, 1/3 of the target images are selected using our target selection function for airplane images belonging to either ship or the bird class, which are the most confusing classes for the images belonging to the airplane class (See Figure 1(b)). These results strongly support that our score function effectively selects targets that are similar yet confused, as intended, without using any label information.

Visualization of embedding spaceTo examine the differences between images that are generated with targeted and untargeted attacks, we visualize their embedding space. In Figure 3, black markers represent adversarial examples, and light blue markers represent clean examples, both belonging to the same class. As shown in Figure 2(a), untargeted adversarial examples are located near clean examples,

    & Method & Attack Type & Clean & PGD \\   & RoCL  & \(_{}\) & 78.14 & 42.89 & 27.19 \\  & +TARO & \(_{}\) & **80.06** & **45.37** & **27.95** \\  & \(_{}\) & **79.96** & 39.37 & **35.97** \\  & +TARO & \(_{}\) & 78.45 & **39.71** & 35.81 \\   

Table 6: Results of linear evaluation in a larger dataset, CIFAR-100.

   Method & Clean & PGD \\  RoCL & **73.93** & 18.62 \\ +TARO & 65.21 & **19.13** \\  SimSiam* & **53.34** & 11.24 \\ +TARO & 50.50 & **25.44** \\   

Table 7: Results of adversarial transfer learning to CIFAR-10 from CIFAR-100.

Figure 3: Visualize embedding

   Evaluation type & Method & Attack Type & Clean & PGD & AutoAttack \\   & RoCL  & \(_{}\) & 78.14 & 42.89 & 27.19 \\  & +TARO & \(_{}\) & **80.06** & **45.37** & **27.95** \\  & +CL  & \(_{}\) & **79.96** & 39.37 & **35.97** \\  & +TARO & \(_{}\) & 78.45 & **39.71** & 35.81 \\   

Table 5: Experimental results against white-box attacks on ResNet18 trained on the CIFAR-10 dataset. To see the effectiveness, we test TARO on contrastive adversarial SSL, i.e., RoCL, and ACL.

Figure 2: Analysis of target from score function (\(\))

and far from the class boundaries. On the other hand, targeted adversarial examples are located near the class boundaries (Figure 2(b)), although it is generated in an unsupervised manner without any access to class labels. This visualization shows that our targeted attack generates relatively more effective adversarial examples than untargeted attacks, which is likely to push the decision boundary to learn more discriminative representation space for instances belonging to different classes.

Ablation study of the score functionTo demonstrate the effect of each component in our score function, we conduct an ablation study of the score function \(\). The score function consists of two terms: the entropy term and the cosine similarity term (Eq. 10), which together contribute to finding an effective target that is different but confusing. We empirically validate each term by conducting an ablation experiment using only a single term in the score function during adversarial SSL training in Eq. 11. The experimental results in Table 8, suggest that the entropy term leads to good clean accuracy while the similarity term focuses on achieving better robust performance. Thus the combined score function enables our model to achieve good robustness while maintaining its accuracy on clean examples.

## 5 Conclusion

In this paper, we demonstrate that a simple combination of supervised adversarial training with self-supervised learning is highly suboptimal due to the ineffectiveness of adversarial examples generated by untargeted attacks in positive-pair only SSL, which perturb to random latent space without considering decision boundaries. To address this limitation, we proposed an instance-wise targeted attack scheme for adversarial self-supervised learning. This scheme selects the target instance based on similarity and entropy, such that the given instance is perturbed to be similar to the selected target. Our targeted adversarial self-supervised learning yields representations that achieve better robustness when applied to any type of adversarial self-supervised learning, including positive-pair only SSL and contrastive SSL. We believe that our work paves the way for future research in exploring more effective attacks for adversarial self-supervised learning.

## Limitations

Our method's main constraint is that our score function's design relies on empirical design based on the previous works. Establishing the most optimal score function theoretically for a high-dimensional, non-linear deep learning model is a complex task. Despite this, we've provided a theoretical basis for how a targeted attack can improve robustness in a simple scenario for positive pairs. Our experimental results also confirm our score function's effectiveness, suggesting we've made various efforts to counterbalance our limitations. Additionally, our method demands more computational time than a simple untargeted adversarial training, given the need to select a target instance. Yet, this extra computational time is less than 5% compared to original training time. Considering the significant boost in robustness, we believe it's a reasonable trade-off to implement our method. Despite these limitations, we've identified a significant vulnerability in the untargeted attack method--an essential discovery for adversarial self-supervised learning. Moreover, we suggest a simple yet effective way to address this vulnerability in adversarial self-supervised learning.