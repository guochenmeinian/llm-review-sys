# Uncovering Meanings of Embeddings

via Partial Orthogonality

Yibo Jiang

Department of Computer Science, University of Chicago

Bryon Aragam

Booth School of Business, University of Chicago

Victor Veitch

###### Abstract

Machine learning tools often rely on embedding text as vectors of real numbers. In this paper, we study how the _semantic_ structure of language is encoded in the _algebraic_ structure of such embeddings. Specifically, we look at a notion of "semantic independence" capturing the idea that, e.g., "eggplant" and "tomato" are independent given "vegetable". Although such examples are intuitive, it is difficult to formalize such a notion of semantic independence. The key observation here is that any sensible formalization should obey a set of so-called independence axioms, and thus any algebraic encoding of this structure should also obey these axioms. This leads us naturally to use _partial orthogonality_ as the relevant algebraic structure. We develop theory and methods that allow us to demonstrate that partial orthogonality does indeed capture semantic independence. Complementary to this, we also introduce the concept of _independence preserving embeddings_ where embeddings preserve the conditional independence structures of a distribution, and we prove the existence of such embeddings and approximations to them.

## 1 Introduction

This paper concerns the question of how semantic meaning is encoded in neural embeddings, such as those produced by . There is strong empirical evidence that these embeddings--vectors of real numbers--capture the semantic meaning of the underlying text. For example, classical results show that word embeddings can be used for analogical reasoning [e.g., 13, 21], and such embeddings are the backbone of modern generative AI systems [e.g., 1, 19, 20, 21, 22, 23]. The high-level question we're interested in is: _How is the **semantic** structure of text encoded in the **algebraic** structure of embeddings?_ In this paper, we provide evidence that the concept of _partial orthogonality_ plays a key role.

The first step is to identify the semantic structure of interest. Intuitively, words or phrases possess a notion of semantic independence, which does not have to be statistical in nature. For example, the word "eggplant" seems more similar to "tomato" than to "ennui". Yet, if we were to "condition" on the common property of "vegetable", then "eggplant" and "tomato" should be "independent". And, if we condition on both "vegetable" and "purple", then "eggplant" may be "independent" of all other words. However, it is difficult to formalize what is meant by "independent" and "condition on" in these informal statements. Accordingly, it is hard to establish a formal definition of semantic independence, and thus it is challenging to explore how this structure might be encoded algebraically!

The key observation in this paper is to recall that most reasonable concepts of "independence" adhere to a common set of axioms similar to those defining probabilistic conditional independence. Formally, this abstract idea is captured by the axioms of the so-called _independence models_. Thus, ifsemantic independence is encoded algebraically, it should be encoded as an algebraic structure that respects these axioms. In this paper, we use a natural candidate independence model in vector spaces known as _partial orthogonality_. Here, for two vectors \(v_{a}\) and \(v_{b}\) and a conditioning set of vectors \(v_{C}\), partial orthogonality takes \(v_{a}\) independent \(v_{b}\) given \(v_{C}\) if the residuals of \(v_{a}\) and \(v_{b}\) are orthogonal after projecting onto the span of \(v_{C}\). _We discover that this particular tool is indeed valuable for understanding CLIP embeddings._ For instance, Figure 1 shows that after projecting onto the linear subspace spanned by CLIP embeddings of "purple" and "vegetable", the residual of embedding "eggplant" has on average low cosine similarity with the residuals of random test embeddings, which also matches our intuitive understanding of the word.

Since partial orthogonality is an independence model, we can go one step further to define _Markov boundaries_ for embeddings as well. Drawing inspiration from graphical models, it is reasonable to expect that the Markov boundary of any target embedding should constitute a minimal collection of embeddings that encompasses valuable information regarding the target. Unlike classical applications of partial orthogonality in regression and Gaussian models, however, the geometry of embeddings presents several subtle technical challenges to directly adopting the usual notion of Markov boundary. First, the _intersection axiom_ never holds for practical embeddings, which makes the standard Markov boundary non-unique. More importantly, practical embeddings could potentially incorporate distortion, noise and undergo phenomena resembling superposition . Therefore, in this paper, we introduce _generalized Markov boundaries_ for studying the structure of text embeddings.

ContributionsSpecifically, we make the following contributions:

1. We adapt ideas from graphical independence models to specify the structure that should be satisfied by semantic independence. We discover that partial orthogonality in the embedding space offers a natural way of encoding semantic independence structure (Section 2).
2. We study the semantic structure of partial orthogonality via Markov boundaries. Due to the unique characteristics of embeddings and noise in learning, exact orthogonality is unlikely to hold. So, we give a distributional relaxation of the Markov boundary and use this to provide a practical algorithm for finding generalized Markov boundaries and measuring the semantic independence induced by generalized Markov boundaries (Section 3.2).
3. We introduce the concept of _independence preserving embeddings_, which studies how embeddings can be used to maintain the independence structure of distributions. This holds its own intrigue for further research (Section 4).
4. Finally, we design and conduct experimental evaluations on CLIP text embeddings, finding that the partial orthogonality structure and generalized Markov boundary encode semantic structure (Section 5).

Throughout, we use CLIP text embeddings as a running example, though the method and theory presented can be applied more broadly.

Figure 1: For target embedding of “eggplant”, the set of embeddings that include “purple” and “vegetable” forms the subspace such that after projection, the residual of ‘eggplant” has the lowest cosine similarity with residuals of other test embeddings. This matches our intuition for the meaning of “eggplant”. Similarly, for target embedding of “zebra”, the set of embeddings that include “striped” and “animal” forms the most suitable subspace.

Related workThere are many papers [e.g., Aro+16; GAM17; AH19; EDH19; Tra+23; Per+23; Lee+23; MEP23; Wan+23] connecting semantic meanings and algebraic structures of popular embeddings like CLIP [Rad+21], Glove [PSM14] and word2vec [Mik+13]. Simple arithmetic on these embeddings reveals that they carry semantic meanings. The most popular arithmetic operation is called linear analogy [EDH19]. There are several papers trying to understand the reasoning behind this phenomenon. Arora et al. [Aro+16] explains this by proposing the latent variable model but it requires the word vectors to be uniformly distributed in the embedding space which generally is not true in practice [MT17]. Alternatively, [GAM17; AH19] adopts the paraphrase model that also does not fit practice. [EDH19], on the other hand, studies the geometry of embeddings that decomposes the shifted pointwise mutual information (PMI) matrix. Trager et al. [Tra+23] and Perera et al. [Per+23] decomposes embeddings into combinations of a smaller set of vectors that are more interpretable. On the other hand, similar to using vector orthogonality to represent (conditional) independence, kernel mean embeddings [Mua+17] are Hilbert space embeddings of distributions that can also be used to represent conditional independences [Son+09; SFG13]. It is a popular method for machine learning, and causal inference [Gre+05; Moo+09; GS20]. But unlike independence preserving embeddings, kernel mean embeddings use the kernel and do not explicitly construct finite-dimensional vector representations.

## 2 Independence Model and Markov Boundary

Let \(\) be a finite set of embeddings with \(|\,\,|=n\) and each embedding is of size \(d\). Every embedding is a vector representation of a word. In other words, there exists a function \(f\) that maps words to \(n\) vectors in \(^{d}\). As explained above, we might expect embeddings to encode "independence structures" between words. These independence structures are difficult to define formally, though the structure is similar to that of probabilistic conditional independence. We will use independence models as an abstract formalization of this structure.

### Independence Model

Throughout this paper, we use many standard definitions and facts about graphical models and more generally, abstract independence models. A detailed overview of this material can be found, for instance, in [Lau96; Stu05].

Suppose \(V\) is a finite set. In the case of embeddings, \(V\) would be a set of vectors. An _independence model_\(_{}\) is a ternary relation on \(V\). Let \(A,B,C,D\) be disjoint subsets of \(V\). Then a _semi-graphoid_ is an independence model that satisfies the following axioms:

1. [label=(A0)]
2. (Symmetry) If \(A_{}B|C\), then \(B_{}A|C\);
3. (Decomposition) If \(A_{}(B D)|C\), then \(A_{}B|C\) and \(A_{}D|C\);
4. (Weak Union) If \(A_{}(B D)|C\), then \(A_{}B|(C D)\);
5. (Contraction) If \(A_{}B|C\) and \(A_{}D|(B C)\), then \(A_{}(B D)|C\).

The independence model is a _graphoid_ if it also satisfies

1. (Intersection) If \(A_{}B|(C D)\) and \(A_{}C|(B D)\), then \(A_{}(B C)|D\).

And, the graphoid is called a _compositional graphoid_ if it also satisfies

1. (Composition) If \(A_{}B|C\) and \(A_{}D|C\), then \(A_{}(B D)|C\).

We also use \(_{}(V)\) to be the set of conditional independent tuples under the independence model \(_{}\). In other words, if \((A,B,C)_{}(V)\), then \(A_{}B|C\) where \(A,B,C\) are disjoint subsets of \(V\).

Probabilistic Conditional Independence (\(_{}\))Given a finite set of random variables \(V\), probabilistic conditional independence over \(V\) defines an independence model that satisfies 1-4 which means that probabilistic independence models are semi-graphoids. In general, however, they are not compositional graphoids. If the distribution has strictly positive density w.r.t. a product measure, then the intersection axiom is true. In this case, probabilistic independence models are graphoids. Still, in general, the composition axiom is not satisfied because pairwise independence does not imply joint independence. One notable exception is when the distribution is regular multivariate Gaussian; then the probabilistic independence model is a compositional graphoid.

Undirected Graph Separations (\(_{}\))For a finite undirected graph \(=(V,E)\). One can easily show that ordinary graph separation in undirected graphs is a compositional graphoid. The relations between probabilistic conditional independences and graph separations are well-studied in the graphical modeling literature . We recall a few important definitions here for completeness. Consider a natural bijection between graphical nodes and random variables. Then if \(_{G}(V)_{P}(V)\), we say the distribution \(\) over \(V\) satisfies the _Markov property_ with respect to \(\) and \(\) is called an _I-map_ of \(\). An I-map \(\) for \(\) is minimal if no subgraph of \(\) is also an I-map of \(\). It is not difficult to show that there exists a minimal I-map \(\) for any distribution \(\).

_Remark 1_.: Not every compositional graphoid can be represented by an undirected graph. Sadeghi  provides sufficient and necessary conditions for this.

Partial Orthogonality (\(_{}\))Let \(V\) be a finite collection of vectors in \(^{d}\). If \(a V,b V\) and \(C V\), then we say that \(a\) and \(b\) are _partially orthogonal given_\(C\) if

\[a\,_{}\,b|C_{C}^{}[a ],\ _{C}^{}[b]=0,\]

where \(_{C}^{}[a]=a-_{C}[a]\) is the residual of \(a\) after projection onto the span of \(C\). It is not hard to verify that \(_{}\) is a semi-graphoid that also satisfies the composition axiom (A6). When \(V\) is a set of linearly independent vectors, then \(_{}\) satisfies (A5) and thus is a compositional graphoid. Partial orthogonality has been studied under different names in the statistics literature for many decades. For example, if we replace Euclidean space with the \(L^{2}\) space of random variables, partial orthogonality is equivalent to the well-known concept of _partial correlation_ or _second-order independence_ (Example 2.26 in ). The concept of geometric orthogonality (Example 2.27 in ) is closely related but does not always satisfy the intersection axiom. More recently, the concept of partial orthogonality in abstract Hilbert spaces was defined and studied extensively in . Finally, when \(V\) is a linearly independent collection of vectors, partial orthogonality yields a stronger independence model known as a _Gaussoid_, which is well-studied [e.g. 1, 13, and the references therein]. It is worth emphasizing that in the present setting of text embedding, we typically have \(d n\), and hence \(V\) cannot be linearly independent.

### Markov boundaries

Suppose \(_{}\) is an independence model over a finite set \(V\). Let \(v_{i}\) be an element in \(V\), then the Markov blanket \(\) of \(v_{i}\) is any subset of \(V\{v_{i}\}\) such that

\[v_{i}\,_{}\,V(\{v_{i}\})|\,\]

A _Markov boundary_ is a minimal Markov blanket.

A Markov boundary, by definition, always exists and can be an empty set. However, it might not be unique. It is well-known that _the intersection property is a sufficient condition to guarantee Markov boundaries are unique_. Thus, the Markov boundary is unique in any graphoid. The proof is presented here for completeness.

**Theorem 2**.: _If \(_{}\) is a graphoid over \(V\), then the Markov boundary is unique for any element in \(V\)._

Proof.: Let \(v_{i} V\). Suppose \(v_{i}\) has two distinct Markov boundaries \(_{1}\), \(_{2}\). Then they must be non-empty and \(v_{i}\,_{}\,_{1}\), \(v_{i}\,_{}\,_{2}\), \(v_{i}\,_{}\,_{2}\,|\,_{1}\), \(v_{i}\,_{}\,_{1}\,|\,_{2}\). By the intersection axiom, \(v_{i}\,_{}\,_{1}_{1}\). Then by the decomposition axiom, \(v_{i}\,_{}\,_{1}\) and \(v_{i}\,_{}\,_{2}\) which is a contradiction. 

_Remark 3_.: For any semi-graphoid, the intersection property is not a necessary condition for the uniqueness of Markov boundaries. See Remark 1 in .

The connection between orthogonal projection and graphoid axioms is well-known . But graphoid axioms find their primary applications in graphical models . In particular, there are many existing papers on Markov boundary discovery for graphical models . They typically assume faithfulness or the distributions are strictly positive, which are sufficient conditions for the intersection property and thus ensure unique Markov boundaries. As an important axiom for graphoids, the intersection property has also been thoroughly investigated . But the intersection property rarely holds for embeddings (See Section 3), which means there could be multiple Markov boundaries.  study this case for graphical models and causal inference.

## 3 Markov Boundary of Embeddings

As indicated in Section 2, partial orthogonality (\(_{}\)) can be used as an independence model over vectors in Euclidean space and is a compositional semi-graphoid. Thus, one can use partial orthogonality to study embeddings, which are real vectors. When \(n d\) and the vectors in \(\) are linearly independent, every vector in \(\) has a unique Markov boundary by Theorem 2.

Unfortunately, when \(d<n\), which happens in practice with embeddings as there are usually more objects to embed than the embedding dimension, there is a possibility of having multiple Markov boundaries. In fact, the main challenge with Markov boundary discovery for embeddings is that _the intersection property generally does not hold_, as opposed to graphical models where this property is commonly assumed .

While the Markov boundary might not be unique, the following theorem says that all Markov boundaries of the target vector capture the same _"information"_ about that vector.

**Theorem 4**.: _Let partial orthogonality \(_{}\) be the independence model over a finite set of embedding vectors \(\). Suppose \(_{1},_{2}\) are two distinct Markov boundaries of \(v_{i}\), then,_

\[_{_{1}}[v_{i}]=_{_{2}}[v_{i}]\]

When \(d n\), then it is likely that the target embedding \(v_{i}\) lies in the linear span of other embeddings (i.e, \(v_{i}(\{v_{i}\})\)), Corollary 5 below shows that, in this case, the span of any Markov boundary is precisely the subspace that contains \(v_{i}\):

**Corollary 5**.: _Let partial orthogonality \(_{}\) be the independence model over a finite set of embedding vectors \(\). Suppose \(_{1}\) is a Markov boundary of \(v_{i}\) and \(v_{i}(\{v_{i}\})\), then,_

\[_{_{1}}[v_{i}]=v_{i}.\]

In other words, to find a Markov boundary of \(v_{i}\), we need to find some vectors such that their linear combination is exactly \(v_{i}\). This seems very strict but is necessary because the formal definition of the Markov boundary requires residual orthogonalities between \(v_{i}\) and every other vector. In the sequel, we show how to relax the definition of the Markov boundary.

### From Elementwise Orthogonality to Distributional Orthogonality

Corollary 5 suggests that the span of the Markov boundary for any target vector should contain that target vector. This is a consequence of the elementwise orthogonality constraint because the definition of the Markov boundary requires the residual of a target vector to be orthogonal to the residual of any test vector. The implicit assumption here is that embeddings are distortion-free and every non-zero correlation is meaningful. However, due to the inherent limitation of the embedding dimension--which often restricts the available space for storing all the orthogonal vectors--and noises introduced from training, embeddings are likely prone to distortion when compressed into a relatively small Euclidean space. In fact, we empirically show in Section 5.2 that inner products in embedding space do not necessarily respect semantic meanings faithfully. Therefore, the notion of elementwise orthogonality loses practical significance.

Instead of enforcing elementwise orthogonality, we relax the definition of the Markov boundary of embeddings such that intuitively, after projection, the residual of the target vector and the residuals of test vectors should be orthogonal in a distributional sense where the distribution is the empirical distribution over test vectors. To capture distributional orthogonalities, this paper focuses on the average of cosine similarities.

In particular, we have the following definition of _generalized Markov boundary_ for partial orthogonality.

**Definition 6** (Generalized Markov Boundary for Partial Orthogonality).: Given a finite set \(\) of embedding vectors. Let \(v\) be an element in \(\), then a _generalized Markov boundary_\(\) of \(v\) is a minimal subset of \(\!\{v\}\) such that

\[_{}(v,)=_{}^{ v}\,|}_{w_{}^{v}}^{}_{}(v,u)=0\]

where \(^{}_{}(v,u)\) is the cosine similarity of \(u\) and \(v\) after projection and \(_{}^{v}=\!\{\{v\}\}\). Specifically, \(^{}_{}(v,u)=_{ }^{}\!\{v\},_{}^{}\!\{u\} }{||_{}^{}\!\{v\}||||_{ }^{}\!\{u\}||}\).

Intuitively, this suggests that, on average, there is no particular direction of residuals that have nontrivial correlations with the residual of the target embedding.

_Remark 7_.: It is evident that the conventional definition of Markov boundary implies Definition 6 (Lemma 15 in Appendix A).

### Finding Generalized Markov Boundary

With a formal definition of the generalized Markov boundary established, our objective is now to identify this boundary. One can always use brute force by enumerating all possible subsets of \(\), but the algorithm would be infeasible when \(|\,\,|\) is large.

Suppose \(v\) is a target vector and \(\) is its generalized Markov boundary, then we can write \(v=v_{}+v_{}\) where \(v_{}=_{}^{}\!\{v\}\) and \(v_{}=_{}\!\{v\}\). Intuitively, Definition 6 suggests that the residual of test vectors can appear in any direction relative to \(v_{}\). Therefore, if one samples random test vectors \(\{u_{i}\}\), their span is likely to be close to \(v_{}\). In other words, the residual of \(v\) after projection onto \((\{u_{i}\})\) should contain more information about the generalized Markov boundary direction \(v_{}\).

This motivates the approximate method Algorithm 1. For any target embedding \(v\), one first sample subspaces spanned by randomly selected embeddings. Embeddings that, on average have high cosine similarities with the target embedding after projecting onto orthogonal complements of previously sampled random subspaces, are considered to be candidates for the generalized Markov boundary. The final selection of generalized Markov boundary searches over these top \(K\) candidates.

Empirically, for text embedding models like CLIP, random projections prove to be advantageous in revealing semantically related concepts. In Section 5.2, we provide several examples where, for a given target embedding, the embeddings that exhibit high correlation after random projections are more semantically meaningful compared to embeddings with merely high cosine similarity with the target embedding before projections.

## 4 Independence Preserving Embeddings (IPE)

In the previous sections, we discussed the Markov boundary of embeddings under the partial orthogonality independence model. In Section 5, we will test its effectiveness at capturing the "semantic independence structure" through experiments conducted on CLIP text embeddings. The belief is that the linear algebraic structure possesses the capacity to uphold the independence structure of semantic meanings.

A natural question to ask is: _Is it always possible to use vector space embeddings to preserve independence structures of interest?_ In this section, we study the case for random variables. Consider an embedding function \(f\) that maps a random variable \(X\) to \(f(X)^{d}\). Ideally, it is desirable for the partial orthogonalities of embeddings to mirror the conditional independences present in the joint distribution of \(X\). We call such representations _independence preserving embeddings (IPE)_ (Definition 8). In this section, we delve into the theoretical feasibility of these embeddings by initially demonstrating the construction of IPE and then showing how one can use random projection to reduce the dimension of IPE. We believe that studying IPE lays the theoretical foundation to understand embedding models in general.

**Definition 8** (Independence Preserving Embedding Map).: Let \(V\) be a finite set of random variables with distribution \(P\). A function \(f:V^{d}\) is called an _independence preserving embedding map_ (IPE map) if

\[_{O}(f(V))_{P}(V).\]

An IPE map is called a _faithful_ IPE map if

\[_{O}(f(V))=_{P}(V).\]

### Existence and Universality of IPE Maps

We first show that for _any distribution_\(P\) over random variables \(V\), we can construct an IPE map.

For any distribution \(P\) over \(V\), there exists a minimal \(I\)-map \(=(V,E)\) such that \(_{G}(V)_{P}(V)\) (See Section 2). We will use \(_{P}\) to be a minimal \(I\)-map of \(P\) and \((_{P})\) to be the adjacency matrix of \(_{P}\). We further define \(_{}(_{P})\) to be an _adjusted adjacency matrix_ with \(\) where

\[_{}(_{P})=+ (_{P})\]

and \(\) is the identity matrix.

Ideally, this matrix is invertible, however, it turns out that not every \(\) produces an invertible \(_{}(_{P})\). We therefore define the following _perfect perturbation factor_. For any matrix \(A^{n n}\), define \(A_{,}\) to be the submatrix of \(A\) with row and column indices from \(\) and \(\), respectively. If \(=\), the submatrix is called a _principal submatrix_ and we denote it simply as \(A_{}\).

**Definition 9** (Perfect Perturbation Factor).: For a given graph \(=(V,E)\) where \(n=|V|\), \(\) is called a _perfect perturbation factor_ if (1) \(_{}(_{P})\) is invertible and (2) for any \([n]\), \((_{}(_{P})_{})_{ij}^{-1}=0\) if and only if \(v_{_{i}}_{}v_{_{j}}|\{v_ {k}:k\}\) where \(_{i}\) is the \(i\)th element of \(\).

**Theorem 10**.: _Let \(V\) be a finite set of random variables with distribution \(P\). \(_{P}\) is a minimal \(I\)-map of \(P\). Let \(A\) be equal to \(_{}(_{P})^{-1}\) with eigen decomposition \(A=U U^{T}\). If \(\) is a perfect perturbation factor, then the function \(f\) with_

\[f(v_{i})=U_{i}^{1/2}\]

_is an IPE map of \(P\) where \(v_{i}\) is a random variable in \(V\) and \(U_{i}\) is the \(i\)-th row of \(U\). Furthermore, if \(P\) is faithful to \(_{P}\), then \(f\) is a faithful IPE map for \(\)._

_Remark 11_.: One can always normalize these embeddings to have unit norms without changing the partial orthogonality structures.

Finding a perfect perturbation factor might seem daunting, but the following lemma, which is a direct consequence of Theorem 1 in Lnenicka and Matus , shows that almost every \(\) is a perfect perturbation factor.

**Lemma 12**.: _For any simple graph \(G\), \(\) is perfect for all but finitely many \(\)._

### Dimension Reduction of IPE

Theorem 10 shows how to learn a perfect IPE but it requires the dimension of embeddings to be the same as the number of variables in \(V\). In the worst case, this is inevitable for a faithful IPE map: If the random variables in \(V\) are mutually independent, then we need at least \(|V|\) dimensions in the embedding space to contain \(V\) orthogonal vectors.

But this is not practical. Suppose we want to embed millions of random variables (e.g. tokens) in a vector space, having the dimension of each embedding be in the magnitude of millions is less than ideal. Therefore, one needs to do dimension reduction.

In this section, we show that by using random projection, the partial orthogonalities induced by Markov boundaries are preserved approximately. Intuitively, this is guaranteed by the Johnson-Lindenstrauss lemma .

**Theorem 13**.: _Let \(U\) be a set of vectors in \(^{n}\) where \(n=|U|\) and every vector is a unit vector. Let \(\) be a matrix in \(^{n n}\) where \(_{ij}= u_{i},u_{j}\). Assume \(_{1}=_{}()>0\). Then there exists a mapping \(g:^{n}^{k}\) where \(k=[20(2n)/(^{})^{2}]\) with \(^{}=\{1/2,/C,_{1}/2r^{2}\}\) and \((0,1)\) such that for any \(u_{i} U\) with its unique Markov boundary \(M_{i} U\) and any \(u_{j} U(\{u_{i}\} M_{i})\), we have_

\[|_{g(M_{i})}^{}[g(u_{i})],_{g(M_{i })}^{}[g(u_{j})]|\]

_where \(r_{i}=|M_{i}|\), \(r=_{i}|M_{i}|\) and \(C=(r+1)^{3}(()+2(r+1)^{2}}{_{}()}) ^{r}\)._

Theorem 13 shows that as long as the partial orthogonality structure of embeddings is sparse in the sense that the size of the Markov boundary for each embedding is small. Then one can reduce the dimension of the embedding and the residuals of target and test vectors after projection onto the Markov boundary are _almost orthogonal_.

_Remark 14_.: The assumption in Theorem 13 is satisfied by the construction of IPE in Section 4.1.

## 5 Experiments

One of the central hypotheses of the paper is that the partial orthogonality of embeddings, and its byproduct generalized Markov boundary, carry semantic information. To verify this claim, we provide both _quantitative_ and _qualitative_ experiments. Throughout this section, we consider the set of normalized embeddings \(\) that represent the \(49815\) words in the Brown corpus . For each target embedding of a word, under any experiment setting, we automatically filter words, whose embeddings have \(0.9\) or above cosine similarities with the target embedding, or words, whose Wul Palmer similarity measure with the target word is almost \(1\). The purpose of this filtering step is to prevent the inclusion of synonyms.

### Semantic Structure of Partial Orthogonality

To examine the rule of partial orthogonality, nine categories are chosen, each with 10 words in it (See Table 3 in Appendix B). Specifically, each word within a given category is a hyponym for that category in WordNet . We assess how much, on average, the cosine similarities between words within each category decrease when conditioned on these different nine categories. By conditioning, we use the clip embedding of the category word of interest and project out the subspace of that clip embedding. The results are shown in Figure 2. We normalize reduction values by sampling 10,000 embeddings and calculating the mean and standard deviation of cosine reductions between these embeddings. It is apparent that on average, cosine similarities of intra-category words decrease more than inter-category words. One interesting finding is that when conditioned on the category word "food", the average similarities between word pairs in "beverage" also drop considerably. We suspect this is because one synset of "food" is also a hypernom of "beverage". Although words in the "food" category are chosen to mean solid food, it could also mean nutrient which also encompasses the meaning of "beverage".

### Sampling Random Subspaces

The first step of Algorithm 1 is to find embeddings that have high similarities with the target embeddings even after projecting onto orthogonal complements of subspaces spanned by randomly selected embeddings. It turns out that this step can reveal semantic meanings. In this section, we design experiments to show both quantitatively and qualitatively that embeddings of words that remain highly correlated with the target embedding after projection are semantically closer to the target word. In various experimental configurations, we employ \(10\) sets of \(50\) randomly chosen embeddings to form random projection subspaces for each target embedding. Qualitatively, Table 4 in Appendix B gives a few examples showing that the words that on average remain highly correlated with the target word tend to possess greater semantic significance. Quantitatively, we calculate the average Wu-Palmer similarities between target words and the top 10 correlated words before and after random projections. We conduct experiments on 1000 random words as well as 300 common nouns provided by ChatGPT. The results are shown in Table 4 verify our claims. This set of experiments also indirectly shows that the embeddings are noisy and that generalized Markov boundaries are indeed needed.

### Generalized Markov Boundaries

We first demonstrate that Algorithm 1 can find generalized Markov boundaries. The experiments are run over \(1000\) randomly selected words. In particular, Table 2 shows that with a relatively small candidate set, the algorithm can already approximate generalized Markov boundaries well, suggesting that the size of generalized Markov boundaries for CLIP text embeddings should be small.

Semantic Meanings of Markov BoundariesThe estimated generalized Markov boundaries returned by Algorithm 1 is a set of embeddings. It is reasonable to anticipate that the linear spans of these embeddings hold semantic meanings. To evaluate this hypothesis, we propose to calculate the smallest principal angles  between the span of generalized Markov boundaries and the span of selected embeddings that are meaningful to the target word.

We again conducted both quantitative and qualitative experiments. Qualitatively, Figure 4 in Appendix B give a few examples comparing target words' generalized Markov boundaries with the span of selected embeddings. For instance, the generalized Markov boundary of 'car' is more aligned with the subspace spanned by embeddings of 'road' and'vehicle' than the span of'sea' and 'boat' and randomly selected subspaces. This suggests that the estimated generalized Markov boundaries hold semantic significance. To verify this quantitatively, we ask ChatGPT to provide a list of common nouns with short descriptions (selected examples are provided in Table 5). We then use CLIP text embedding to convert the description sentence into one vector and compare the smallest angle between the description vector with generalized Markov boundaries and random linear spans. Figure 3 shows that the generalized Markov boundaries are more semantically meaningful than random subspaces.

## 6 Conclusion

This paper studies the role of partial orthogonality in analyzing embeddings. Specifically, we extend the idea of Markov boundaries to embedding space. Unlike Markov boundaries in graphical models, the boundaries for embeddings are not guaranteed to be unique. We propose alternative relaxed definitions of Markov boundaries for practical use. Empirically, these tools prove to be useful in finding the semantic meanings of embeddings. We also introduce the concept of independence preserving embeddings where embeddings use partial orthogonalities to preserve the conditional independence structures of random variables. This opens the door for substantial future work. In particular, one promising theoretical direction is to study if CLIP text embeddings preserve the structures in the training distributions.

## 7 Acknowledgments

This work is supported by ONR grant N00014-23-1-2591, Open Philanthropy, NSF IIS-1956330, NIH R01GM140467, and the Robert H. Topel Faculty Research Fund at the University of Chicago Booth School of Business.