# Self-attention-based knowledge distillation transfers discriminative image representation.

KOALA: Empirical Lessons Toward Memory-Efficient and Fast Diffusion Models for Text-to-Image Synthesis

 Youngwan Lee1,2 Kwanyong Park1 Yoorhim Cho3 Yong-Ju Lee1 Sung Ju Hwang2,4

###### Abstract

As text-to-image (T2I) synthesis models increase in size, they demand higher inference costs due to the need for more expensive GPUs with larger memory, which makes it challenging to reproduce these models in addition to the restricted access to training datasets. Our study aims to reduce these inference costs and explores how far the generative capabilities of T2I models can be extended using only publicly available datasets and open-source models. To this end, by using the de facto standard text-to-image model, Stable Diffusion XL (SDXL), we present three key practices in building an efficient T2I model: (1) **Knowledge distillation**: we explore how to effectively distill the generation capability of SDXL into an efficient U-Net and find that self-attention is the most crucial part. (2) **Data**: despite fewer samples, high-resolution images with rich captions are more crucial than a larger number of low-resolution images with short captions. (3) **Teacher**: Step-distilled Teacher allows T2I models to reduce the noising steps. Based on these findings, we build two types of efficient text-to-image models, called KOALA-Turbo &-Lightning, with two compact U-Nets (1B & 700M), reducing the model size up to 54% and 69% of the SDXL U-Net. In particular, the KOALA-Lightning-700M is \(4\) faster than SDXL while still maintaining satisfactory generation quality. Moreover, unlike SDXL, our KOALA models can generate 1024px high-resolution images on consumer-grade GPUs with 8GB of VRAMs (3060Ti). We believe that our KOALA models will have a significant practical impact, serving as cost-effective alternatives to SDXL for academic researchers and general users in resource-constrained environments.

## 1 Introduction

Since Stable Diffusion XL  (SDXL) has become the de facto standard model for text-to-image (T2I) synthesis due to its ability to generate high-resolution images and its open-source nature, many models for specific downstream tasks [7; 47; 3; 58; 56] now leverage SDXL as their backbone. However, the model's massive computational demands and large size necessitate expensive hardware, thus incurring significant costs in terms of training and inference. Moreover, recent T2I works [5; 6; 39; 1] do not release their training datasets, making it challenging for the open-source community to reproduce their performance due to internal or commercial restrictions.

To alleviate the computation burden, previous works have resorted to quantization , hardware-aware optimization , denoising step reduction [38; 29; 39; 23], and architectural model optimization . In particular, the denoising step reduction (_i.e._, step-distillation) and architectural model compression methods adopt the knowledge distillation (KD) scheme [14; 11] by allowing the model to mimic the output of the SDMs as a teacher model. For the architectural model compression, BK-SDM  exploits KD when compressing the most heavy-weight part, U-Net, in SDM-v1.4 . BK-SDM builds a compressed U-Net by simply removing some blocks, which allows the compressed U-Net to mimic the last features at each stage and the predicted noise from the teacher model during the pre-training phase. However, the compression method proposed by BK-SDM achieves a limited compression rate (33% in Tab. 2) when applied to the larger SDXL than SDM-v1.4, and the strategy for feature distillation in U-Net has _not yet been fully explored_.

In this work, our goal is to build an efficient T2I model based on **kn**owledge distillation in the **l**atent diffusion model (KOALA) by exploring how far we can push the performance using only open-source data and models alone. To this end, we first design two compressed U-Nets, KOALA-1B and KOALA-700M, using not only block removal but also _layer-wise removal_ to reduce the model size of the SDXL U-Net by up to 54% and 69% (vs. BK's method: 33%) in Tab. 2. Then, we explore how to enhance the generative capability of the compact U-Net based on three empirical findings; (1) **self-attention based knowledge distillation**: we investigate how to effectively distill SDXL as a teacher model and find _essential factors_ for feature-level KD. Specifically, we found that self-attention features are the most crucial for distillation since self-attention-based KD allows models to learn more discriminative representations between objects or attributes. (2) **Data**: When performing KD-training, high-resolution images and longer captions are more critical, even with fewer samples, than a larger number of low-resolution images with short captions. (3) **Teacher model**: The teacher model with higher performance improves the capability of the student model, and step-distilled teachers allow the student model to reduce the denoising steps, which results in further speed-up.

Based on these findings, we train efficient text-to-image synthesis models on _publicly available_ LAION-POP  by using two types of distilled teacher models, SDXL-Turbo  and SDXL-Lightning , with 512px and 1024px resolutions, respectively. We observe that our KD method consistently outperforms the BK  method in both U-Net and Diffusion Transformer backbones in Tabs. 7 and 8. In addition, KOALA-Lightning-700M outperforms SSD-1B , which is trained using the BK method, at \(3\) faster speed. Furthermore, KOALA-Lightning-700M achieves \(4\) faster speed and \(3\) model efficiency than SDXL-Base while exhibiting satisfactory generation quality. Lastly, to validate its practical impact, we perform inference analysis on a variety of _consumer-grade_**GPUs** with different memory sizes (_e.g._, 8GB, 11GB, and 24GB), and the results show that SDXL models cannot be mounted on an 8GB GPU, whereas our KOALA models can operate on it, demonstrating that our KOALA models are cost-effective alternatives for practitioners1 in resource-constrained environments.

Figure 1: **Samples by KOALA-Lightning-700M** with \(}\) resolution and 10 denoising steps, generated in 0.65 seconds on NVIDIA 4090 GPU. The prompts and more qualitative comparisons are illustrated in App. C.

Our main contributions are as follows:

1. We design two efficient denoising U-Net with model sizes (1.13B/782M) that are more than twice as compact SDXL's U-Net (2.56B).
2. We perform a comprehensive analysis to build efficient T2I models, drawing three key lessons: self-attention distillation, the impact of data characteristics, and the influence of the teacher model.
3. We conduct a systematic analysis of inference on consumer-grade GPUs, highlighting that our KOALA models operate efficiently on an 8 GB GPU, unlike other state-of-the-art models.

## 2 In-depth Analysis: Stable Diffusion XL

SDXL , the latest version of the SDM series [35; 36; 34], exerts a significant influence on both the academic community and the industry due to its unprecedented \(1024^{2}\) high-resolution image quality and open source resources. It has several key improvement points from the previous SDM-v2.0 , _e.g._, multiple sizes- & crop-conditioning, an improved VAE, a much larger U-Net, and an ad hoc style of refinement module, which leads to significantly improved generation quality. However, the significant enlargement of U-Net in model size results in increased computational costs and significant memory (or storage) requirements, hampering the accessibility of SDXL. Thus, we investigate the U-Net in SDXL to design a more lightweight U-Net for knowledge distillation. We dissect the components of SDXL, quantifying its size and latency during the denoising phase, as detailed in Tab. 1. The enlarged U-Net (2.56B) is the primary cause of the increasing SDXL model size (vs. SDM-v2.0 (865 M)). Furthermore, the latency of U-Net is the main inference time bottleneck in SDXL. Therefore, it is necessary to reduce U-Net's model budget for better efficiency.

The SDXL's U-Net varies in the number of transformer blocks for each stage, unlike SDM-v2.0, which employs a transformer block for each stage (see Tab. 2). At the highest feature levels (_e.g._, DW-1&UP-3 in Fig. 2), SDXL uses only residual blocks without transformer blocks, instead distributing more transformer blocks to lower-level features. So, in Fig. 13, we analyze the parameter distribution of each stage in the U-Net. Most parameters (83%) are concentrated on the transformers with ten blocks in the lowest feature map (_e.g._, \(32^{2}\) of DW-3, Mid, UP-1 in Fig. 2), making the main

 SDXL & Text Enc. & VAE Dec. & U-Net &  \\   \#Param. & 817M & 83M & 2,567M & 1,717M & 1,161M & 782M \\ Latency (s) & 0.008 & 0.002 & 3.133 &  &  &  \\ 

Table 1: **SDXL-Base-1.0 model budget. Latency is measured under the image scale of \(1024^{2}\), FP16-precision, and 25 denoising steps in NVIDIA 4090 GPU (24GB).**

 U-Net & SDM-v2.0 & SDXL-1.0 & BK-SDXL & **KOALA-1B** & **KOALA-700M** \\   \#Param. & 868M & 2,567M & 1,717M & 1,161M & 782M \\ CKPT size & 3,46GB & 10,3GB & 6,8GB & 4,4GB & 3,0GB \\ Tx blocks &  &  &  &  &  \\ Mid block & \(\) & \(\) & \(\) & \(\) & \(\) \\ Latency & 1,138 & 3,13s & 2,42s & 1.60s & 1.25s \\ 

Table 2: **U-Net Comparison. Tx means Transformer. SDM-v2.0  uses \(768^{2}\) resolution, while SDXL and KOALA models use \(1024^{2}\) resolution. CKPT means the trained checkpoint file.**

Figure 2: **Overview of KnOWledge-DistillAction in LAtent diffusion model based on SDXL and architecture of KOALA. We omit skip connections for simplicity. We perform feature distillation in transformer blocks using self-attention layers.**

parameter bottleneck. Thus, it is essential to address this bottleneck when designing an efficient U-Net architecture.

## 3 Three lessons for building an efficient text-to-image model

In this section, we introduce three empirical lessons to realize an efficient text-to-image synthesis; first, we design a lightweight U-Net architecture and perform a comprehensive analysis with the proposed efficient U-Net to find knowledge-distillation (KD) strategies in Sec. 3.1.2. Secondly, we investigate the training data characteristics that affect image generation quality in Sec. 3.2. Finally, we explore how different teacher models influence the student model in Sec. 3.3. For these empirical studies, we adopt two evaluation metrics, Human Preference Score (HPSv2)  and CompBench , instead of FID. Recently, several works [2; 55; 31] have claimed that FID  is not closely correlated with visual fidelity. HPSv2  is for a visual aesthetics metric, which allows us to evaluate visual quality in terms of more specific types. As an image-text alignment metric, Compbench  is a more comprehensive benchmark for evaluating the compositional text-to-image generation capability than the single CLIP score . We report average scores for HPS and Compbench, respectively.

### Lesson. 1: Self-attention based Knowledge Distillation with Efficient U-Net Architecture

#### 3.1.1 Efficient U-Net architecture

A prior strategy for compressing the text-to-image model is to remove a pair of residual and transformer blocks at each stage, namely block removal [21; 10]. While this method may be sufficient for compressing shallow U-Nets like in SDM-v1.4 , it shows limited effectiveness for recent, more complex U-Nets. For cases of SDXL , the compression rate is reduced only from 2.5B to 1.7B, as shown in Tab. 2. To address this limitation, we investigate a new approach for compressing these heavier U-Nets to achieve more efficient text-to-image generation.

**Transformer layer-wise removal is the core of efficient U-Net architecture design.** According to the discussion in Sec. 2, the majority of parameters are concentrated in the transformer blocks at the lowest feature levels. Each block comprises multiple consecutive transformer layers, specifically ten layers per block in SDXL (see Fig. 2). We address this computational bottleneck by reducing the number of transformer layers (i.e., depth), a strategy we term _layer-wise removal_.

Using this removal strategy as the core, we instantiate two compressed U-Net variants: KOALA-1B and KOALA-700M. First, we apply the prior block-removal strategy  to the heavy U-Net of SDXL. We note that in the decoder part (_e.g._, UP-1 to UP-3), we remain more blocks than in the encoder because the decoder part plays a more important role in knowledge distillation, which is addressed in Sec. 3.1.2 and Tab. 2(b). On this block-removed backbone, we then adopt _layer-wise removal_ at different ratios. Specifically, we reduce the transformer layers at the lowest feature level (_i.e._, DW-3, Mid and UP-1 in Fig. 2) from 10 to 5 for KOALA-700M and to 6 for KOALA-1B. For KOALA-700M, we also removed the Mid block. An overview of the compressed U-Nets is presented in Tab. 2 and Fig. 2. Our KOALA-1B model has 1.16B parameters, making it half the size of SDXL (2.56B). KOALA-700M, with 782M parameters, is comparable in size to SDM-v2.0 (865M).

#### 3.1.2 Exploring Knowledge Distillation for SDXL

Prior work  that attempts to distill an early series of stable diffusion (_i.e._, SDM-v1.4 ) directly follows traditional knowledge distillation literature [37; 11]. The compressed student U-Net model \(S_{}\) is jointly trained to learn the target task and mimic the pre-trained U-Net of SDM-v1.4 as a teacher network. Here, the target task is the reverse denoising process , and we denote the corresponding learning signal as \(_{}\). Besides the task loss, the compressed student model is trained to match the output of the pre-trained U-Net at both output and feature levels. \(_{}\) and \(_{}\) represent the knowledge distillation (KD) loss at the output- and feature-level, respectively. For designing the feature-level KD-loss, BK-SDM  simply considers only the last feature (LF) map of the teacher \(f^{i}_{T}()\) and

Table 3: **Analysis of feature level knowledge distillation of U-Net in SDXL .**student network \(f^{i}_{S}()\) at each stage as follows:

\[_{}=_{S_{}}_{z,c,c,t}||_{i}f^{i} _{T}(z_{t},t,c)-f^{i}_{S}(z_{t},t,c)||^{2}_{2},\] (1)

where \(t\) and \(c\) denote given diffusion timestep and text embeddings as conditions. Thus, the feature distillation approach for text-to-image diffusion models has _not been sufficiently explored_, leaving room for further investigation.

In this section, we extensively explore feature distillation strategies to distill the knowledge from the U-Net of SDXL effectively to our efficient U-Net, KOALA-1B. We start from a baseline trained only by \(_{}\) and add \(_{}\) without \(_{}\) to validate the effect of feature distillation. More training details are described in Sec. 4.1 and App. A.With the increasing complexity of U-Net and its stage, relying solely on the last feature (LF) as in BK  may not be sufficient to mimic the intricate behavior of the teacher U-Net. Thus, we revisit which features provide the richest guidance for effective knowledge distillation. We focus on key intermediate features from each stage: outputs from the self-attention (SA), cross-attention (CA), and feedforward net (FFN) in the transformer block, as well as outputs from convolutional residual block (Res) and LF. Tab. 3a summarizes the experimental results. While all types of features help obtain higher performance over the naive baseline with only the task loss, distilling _self-attention features_ achieves the most performance gain. Considering the prior studies [22; 46; 49] which suggest that SA plays a vital role in capturing semantic affinities and the overall structure of images, the results emphasize that such information is crucial for the distillation process.

To understand the effects more clearly, we illustrate a representative example in the Fig. 3. To reason about how the distilled student U-Net captures self-similarity, we perform a PCA analysis [19; 50] on self-attention maps. Specifically, we apply PCA on self-attention maps from SA- and LF-based models and show the top three principal components in Fig. 3-(b). Interestingly, in the SA-based model, each principal component distinctly represents individual objects (_i.e._, unique color assignments to each object). This indicates that the SA-based model effectively distinguishes different objects in modeling self-similarity, which plays a crucial role in accurately rendering the distinct appearance of each object. In contrast, the LF-based model exhibits less distinction between objects, resulting in _appearance leakage_ between them (_e.g._, a small hippo with rabbit ears). More PCA analyses are detailed in Fig. 10.

**Self-attention at the decoder has a larger impact on the quality of generated images.**

We further explore the role and significance of each self-attention stage. To this end, we first visualize the self-attention map in Fig. 3-(c). The self-attention maps initially capture general contextual

Figure 3: **Analysis on self-attention maps of distilled student U-Nets.** (a) Generated images of LF- and SA-based distilled models, which are BK-SDM  and our proposal, respectively. In BK-SDM’s result, a rabbit is depicted like a hippopotamus (_i.e._, appearance leakage). (b) Visualization of PCA analysis results on self-attention maps of UP-1 stage. (c) Representative visualization of self-attention map from different U-Net stages. Red boxes denote the query patches. Note that from the MID stage, the SA-based model _attends_ to the rabbit more _discriminatively_ than the LF model, demonstrating that self-attention-based KD allows to generate objects more distinctly.

information (_e.g._, DW-2&DW-3) and gradually focus on localized semantics (_e.g._, MID). In the decoder, self-attentions increasingly correlate with higher-level semantics (_e.g._, object) to accurately model appearances and structures. Notably, in this stage, the SA-based model attends corresponding object regions (given the query patch, red box) more _discriminatively_ than the LF-based model, which results in improved compositional image generation performance.

In addition, we ablate the significance of each self-attention stage in the distillation process. Specifically, we adopt an SA-based loss at a single stage alongside the task loss. As shown in Tab. 2(b), the results align with the above understanding: distilling self-attention knowledge within the _decoder_ stages significantly enhances generation quality. In comparison, the impact of self-attention solely within the encoder stages is less pronounced. Consequently, we opt to retain more SA layers within the decoder (see Fig. 2).

In summary, we train our efficient KOALA U-Nets using the following objectives: \(_{}+_{}+_{}\). We apply our proposed self-attention-based knowledge distillation (KD) methods to \(_{}\). Further analyses on featKD, including how to locate features and combine different types of features, are provided in App. B.1.

### Lesson 2. Data: the impact of sample size, image resolution, and caption length

We investigate various data factors--such as image resolution, caption length, and the number of samples--that impact the quality of the final text-to-image model. To ensure reproducibility, we design three data variants using open-source data. (i) LAION-Aesthetic-6+ (LAION-A-6+)  includes a large volume of image-text pairs (8,483,623) with images filtered for high aesthetics. Most images are low-resolution (average \(580 676\)), and the corresponding captions are brief (average length of 13 words). (ii) Description-augmented LAION-A-6+ is designed to demonstrate the impact of detailed descriptions. For each image in LAION-A-6+, we use a large multimodal model  (LMM) to generate detailed descriptions. These synthesized captions, referred to as synCap, convey significantly more semantic information and are longer (e.g., an average length of 72 words; more details on synCap in App. A.1). This data source is denoted in the second row of the table. (iii) LAION-POP  features high-resolution images (average \(1274 1457\)) and descriptive captions (average length of 81 words), although the dataset size is relatively small (e.g., 491,567 samples). The descriptions are generated by LMM, CogVML  and LLaVA-v1.5.

We train KOALA-700M models using the same training recipes for each data source. From the results summarized in Tab. 4, we make several observations. First, detailed captions significantly boost performance, enabling the model to learn detailed correspondences between images and text (See (a) and (b) in the Compbench score). Second, high-resolution images, which convey complex image structures, are a valuable source for training T2I models. Despite having fewer samples, LAION-POP further boosts overall performance. Based on these findings, we opt to use LAION-POP as the main training data, as it features high-resolution images and descriptive captions.

### Lesson 3. The influence of Teacher model

Following the tremendous success of SDXL , recent large-scale text-to-image models have adopted its U-Net backbone. SDXL-Turbo  and SDXL-Lightning  are notable examples, enabling high-quality image generation in low-step regime through progressive distillation . This section investigates whether our distillation framework can effectively exploit these diverse models. To this end, we leverage SDXL-Base and its variants, SDXL-Turbo and SDXL-Lightning, as teacher models, transferring their knowledge into KOALA-700M. We apply the former two lessons and more training details are described in App. A.2.

 ID & Data & \#lmsg & AR & ACL & HP8v2 &  CompBench \\  } \\  (a) & LAION-A-6+  & 8M & \(580 676\) & 13 & 27.43 & 0.3791 \\ (b) & (a) + synCap & 8M & \(580 676\) & 72 & 27.61 & 0.4168 \\ (c) & LAION-POP  & 491K & \(1274 1457\) & 81 & **27.79** & **0.4290** & \\ 

Table 4: **Training Data comparison**. AR and ACL mean aver- Table 5: **Teacher model** comparison. age resolution and average caption length, respectively. synCap We use KOALA-700M as a student model.

As shown in Tab. 5, all KOALA-700M models distilled from different teacher models demonstrate decent image generation capabilities. This highlights the generality of our knowledge distillation framework. More interestingly, when using SDXL-Turbo and SDXL-Lightning as teachers, KOALA-700M models exhibit comparable or even better image quality than when SDXL-Base is used as the teacher, despite requiring fewer denoising steps (_e.g._, 10 vs. 25). Note that the KD framework or noise schedule (Euler discrete schedule , the same as SDXL-Base) for the different KOALA models does not require specific modifications. Thus, KOALA models seamlessly inherit the ability to illustrate realistic structures and details in images, even in the short-step regime, from their step-distilled teachers (See \(2^{nd}\) and \(3^{rd}\) rows in Fig. 4). This results in robust performance across a diverse range of denoising steps (See Fig. 4). In contrast, when SDXL is used as the teacher model, KOALA models struggle to depict realistic structures in a few steps, leading to flawed features (e.g., missing handles and lights on a motorcycle in Fig. 4). For more efficient text-to-image synthesis, we leverage step-distilled teachers, enabling KOALA models to generate high-quality images in just a few steps.

Therefore, combining all these lessons, we build **KnO**wledge-distill**A**tion-based **LA**tent diffusion models, called KOALA. As discussed in each section, the three proposed lessons complement each other. Both image quality and image-text alignment improve progressively as each lesson is added, as shown in Fig. 6. In particular, we build two types of KOALA models: KOALA-Turbo with 512px and KOALA-Lightning with 1024px using two KOALA U-Net (1B&700M), respectively.

## 4 Experiments

### Implementation details

**Dataset.** As the datasets used for state-of-the-art methods in Tab. 6 are proprietary or not released, we opt for the _publicly accessible_ only LAION dataset  to ensure the reproducibility of our work. From the data recipe in Sec. 3.2, we finally use LAION-POP  for KOALA models in Tab. 6. More details of the dataset we used are described in App. A.1

**Training.** According to the recipe about the Teacher model in Sec. 3.3, we use SDXL-Turbo  and SDXL-Lightning  as teacher models, building two types of KOALA models, KOALA-Turbo and KOALA-Lightning. Since SDXL-Turbo and -Lightning are based on SDXL-Base model , we use the same two text encoders, OpenCLIP ViT-bigG  and CLIP ViT-L  and only replace the

Figure 4: Teacher model comparisons across denoising steps with KOALA-700M.

Figure 5: Denoising process of different teacher models. Figure 6: Qualitative analysis on proposed lessons.

original U-Net with our efficient KOALA U-Net. Our U-Nets are initialized with the teacher's U-Net weights at the exact block location. Using our self-attention-based KD method in Sec. 3.1.2, we train our KOALA models on the LAION-POP dataset using four NVIDIA A100 (80GB) GPUs with \(512^{2}\) and \(1024^{2}\) resolutions for KOALA-Turbo and KOALA-Lightning, respectively. **Inference.** We use Euler discrete scheduler  as the same sampler in SDXL . All KOALA models generate images with 10 denoising steps, FP16, and cfg-sale  of 3.5. Please see further details of training and inference in App. A.2 and App. A.3, respectively.

### Main results

**vs. SDXL models:** Compared to SDXL-Base-1.0 , our KOALA-Lightning-700M/-1B models achieve better performance in terms of HPSv2 and CompBench while showing about 5\(\) and 4\(\) faster speed, respectively. Compared to SDXL-Turbo  and SDXL-Lightning , our KOALA-Turbo and KOALA-Lightning models show comparable or inferior HPSv2 scores but achieve higher CompBench scores with up to \(3\) smaller model sizes and \(1.7\) lower memory usage. **vs.****Pixart:** KOALA-Lightning models fall short in HPSv2 and CompBench. Especially, Pixart-\(\) achieves the best CompBench and the second-best HPSv2 scores. This result is attributed to data quality, as Pixart-\(\) collects high-quality internal data consisting of 33 million images above 1K resolution and uses synthetic longer captions (with an average length of 184 words). However, our KOALA-Lightning-700M shows \(6\) faster speed and \(2\) better memory efficiency. **vs. SSD:** Note that due to the difference in training datasets, we cannot make a direct comparison with SSD models, which are trained by BK 's KD method. Except for HPSv2 of SSD-Vega , KOALA-Lighting models show better HPSv2 and CompBench scores while achieving up to \(3\) faster speed. More qualitative comparisons in App. C support the quantitative results.

### Discussion

**Comparison with BK-SDM.** For a fair comparison to BK-SDM , we train our KOALA U-Net backbones with their distillation method under the same data setup (See training details in A.2). As shown in Tab. 7, our KD method consistently achieves higher HPSv2 and CompBench scores than the BK-SDM  when using different U-Net backbones. These results demonstrate two main implications as follows: 1) the proposed distillation of the self-attention layer is more helpful for

 Model & Resolution & Steps & Latency (s) & U-Net Param. & Memory & HPSv2 & CompBench \\  SDM-v2.0  & \(768^{2}\) & 25 & 1.236 & 0.86B & 5.6GB & 25.86 & 0.3672 \\ SDXL-Base-1.0  & \(1024^{2}\) & 25 & 3.229 & 2.56B & 11.9GB & 30.82 & 0.4445 \\ SDXL-Turbo  & \(512^{2}\) & 8 & 0.245 & 2.56B & 8.5GB & 29.93 & 0.4489 \\ SDXL-Lightning  & \(1024^{2}\) & 8 & 0.719 & 2.56B & 11.7GB & 32.18 & 0.4445 \\ Pixart-\(\) & \(1024^{2}\) & 25 & 3.722 & 0.6B & 17.3GB & 32.06 & 0.3880 \\ Pixart-\(\) & \(1024^{2}\) & 25 & 3.976 & 0.6B & 17.3GB & 31.75 & 0.4612 \\ SSD-1B  & \(1024^{2}\) & 25 & 2.094 & 1.3B & 9.4GB & 31.43 & 0.4497 \\ SSD-Vega  & \(1024^{2}\) & 25 & 1.490 & 0.74B & 8.2GB & 32.17 & 0.4461 \\ 
**KOALA-Turbo-700M** & \(512^{2}\) & 10 & 0.7194 & 0.78B & 4.9GB & 29.98 & 0.4555 \\
**KOALA-Turbo-1B** & \(512^{2}\) & 10 & 0.238 & 1.16B & 5.7GB & 29.84 & 0.4560 \\
**KOALA-Lightning-700M** & \(1024^{2}\) & 10 & 0.655 & 0.78B & 8.3GB & 31.50 & 0.4505 \\
**KOALA-Lightning-1B** & \(1024^{2}\) & 10 & 0.790 & 1.16B & 9.1GB & 31.71 & 0.4590 \\ 

Table 6: **Performance comparison to state-of-the-art models. We measure latency and memory usage with a bath size of 1 on NVIDIA 4090 GPU. We obtain HPSv2 and Compbench scores of all models on the same GPU and library environment by using their official weights. We highlight the best value in green, and the second-best value in blue. The full scores of HPSv2 and Compbench are shown in Tab. 10.**

 KD method & Backbone & HPSv2 & CompBench \\  BK  & BK-Small & 26.72 & 0.3237 \\
**Ours** & BK-Small & **26.86** & **0.3417** \\ BK  & KOALA-1B & 27.01 & 0.3599 \\
**Ours** & KOALA-1B & **27.15** & **0.3712** \\ 

Table 7: **Comparison to BK . All models are trained for 50K iterations same as BK-SDM.**visual aesthetics than simply distilling the last layer feature by BK . 2) our self-attention-based KD approach allows the model to learn more discriminative representations between objects or attributes so that it can follow prompts faithfully (as shown in Sec. 3.1.2 and Fig. 3). More qualitative comparisons are demonstrated in Fig. 19.

**Applicability of self-attention based KD to Diffusion Transformer.** To validate the generality of our self-attention distillation method, we also apply it to a diffusion transformer (DiT) based T2I model, Pixart-\(\). To this end, We compress the DiT-XL  backbone and build DiT-M by reducing the number of layers from 28 to 14 with the same hidden dimension (see more training details in App. A.4.). Following our KD strategy, we conduct an ablation study by simply changing the distillation location due to DiT's architectural simplicity, which consists of only transformer blocks without resolution changes. Tab. 8 illustrates that distilling the self-attention feature outperforms other features while using the last features proposed in BK  shows the worst performance, demonstrating that the self-attention layer is still the most crucial part for diffusion transformer.

**Synergy Effect with Step-Distillation Method, PCM .** Since step-distillation methods [23; 39; 53] and our KD approach are orthogonal, applying our KOALA backbones to the step-distillation methods could yield synergistic effects, leading to further speed improvements. To verify the synergy effect between the step-distillation method (_e.g._, PCM ) and our KOALA backbones, we conduct step-distillation training using PCM with our KOALA backbones, and the results are presented in Tab. 9. Thanks to their efficiency, our KOALA backbones allow PCM2.3 to achieve additional speed-up with only a slight performance drop compared to using the SDXL-Base backbone. Furthermore, we provide qualitative comparisons between PCM-KOALA models and PCM-SDXL-Base in Fig. 20, demonstrating that the generated images achieve visual quality comparable to those of SDXL-Base.

Figure 7: **Latency and Memory comparison on across different _consumer-grade_ GPUs. We run each model with the denoising steps in Tab. 6 and FP16. For a fair comparison, we use the official pre-trained weights and inference code in the Huggingface without any other tricks such as torch.compile or quantization. Note that only our KOALA models and SSD-Vega can run all types of GPUs.**

### Model budget comparison on consumer-grade GPUs

We further validate the efficiency of our model by measuring its inference speed and memory usage on a variety of _consumer-grade_**GPUs** with different memory sizes, such as 8GB (3060Ti), 11GB (2080Ti), and 24GB (4090), because the GPU environment varies for individual practitioners. On the GPU-8GB, all SDXL models can't fit, while only KOALA models and SSD-Vega  can run. KOALA-Lightning-700M consumes comparable GPU memory but shows \(2\) faster than SSD-Vega. On the GPU-11GB, SDXL models can run, but KOALA-Lightning-700M still runs at approximately \(5\) faster speed than SDXL-Base . It is noted that Pixart-\(\) & \(-\) cannot operate on GPUs with 8GB and 11GB of memory due to their higher memory usage, but they can run on a GPU-24GB, albeit at the slowest speed. It is worth noting that our KOALA models can operate _efficiently_ on all types of GPUs, highlighting the versatility and accessibility of our approach. Furthermore, our KOALA-Lightning-700M is the best alternative for high-resolution image generation that can replace SDXL models in resource-constrained GPU environments.

## 5 Limitations

While our KOALA models generate images with decent aesthetic quality, such as photo-realistic or 3D-art renderings, they still show limitations in synthesizing legible texts in the generated image as shown in Fig. 8 (Left). Also, our models have difficulty in generating complex prompts with multiple attributed or object relationships, as shown in Fig. 8 (Right). Additionally, since SDXL is the de facto T2I model, we have tried to compress the SDXL U-Net by addressing its bottlenecks. However, this approach is somewhat specific to the SDXL U-Net and heuristic. This limitation arises because the SDXL U-Net has a complex and heterogeneous architecture, comprising both convolutional and transformer blocks, which hinders the formulation of a more general compression principle. More detailed investigations and examples are described in App. D.

## 6 Conclusion

In this work, we have explored how to build memory-efficient and fast T2I models, designing compact denoising U-Nets and presenting three critical lessons for boosting the performance of the efficient T2I models: 1) the importance of self-attention in knowledge distillation, 2) data characteristics, and 3) the influence of teacher models. Thanks to these empirical insights, our KOALA-Lightning-700M model substantially reduces the model size (69%\(\)) and the latency (79%\(\)) of SDXL-Base while exhibiting satisfactory generation quality. We hope that our KOALA models can serve as cost-effective alternatives for practitioners in limited GPU environments and that our lessons benefit the open-source community in their attempts to improve the efficiency of T2I models.

Additionally, since we have identified the potential of applying our self-attention-based KD to Diffusion Transformer (DiT) models [30; 5; 6] in Tab. 8 due to their architectural simplicity compared to the U-Net in SDXL, we plan to further explore more general model compression methods for DiT, as in the language model literature [9; 28], and KD techniques based on our self-attention distillation.

## 7 Acknowledgments

This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training, 45%), (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration, 45%) and (No.2019-0-00075, Artificial Intelligence Graduate School Program(KAIST), 10%).

Figure 8: **Failure cases of KOALA-700M**