# Cryptographic Hardness of Score Estimation

Min Jae Song

Paul G. Allen School of Computer Science and Engineering

University of Washington

mjsong32@cs.washington.edu

###### Abstract

We show that \(L^{2}\)-accurate score estimation, in the absence of strong assumptions on the data distribution, is computationally hard even when sample complexity is polynomial in the relevant problem parameters. Our reduction builds on the result of Chen et al. (ICLR 2023), who showed that the problem of generating samples from an unknown data distribution reduces to \(L^{2}\)-accurate score estimation. Our hard-to-estimate distributions are the "Gaussian pancakes" distributions, originally due to Diakonikolas et al. (FOCS 2017), which have been shown to be computationally indistinguishable from the standard Gaussian under widely believed hardness assumptions from lattice-based cryptography (Bruna et al., STOC 2021; Gupte et al., FOCS 2022).

## 1 Introduction

Diffusion models  have firmly established themselves as a powerful approach to generative modeling, serving as the foundation for leading image generation models such as DALL-E 2 , Imagen , and Stable Diffusion . A diffusion model consists of a pair of forward and reverse processes. In the forward process, noise drawn from a standard distribution, such as the standard Gaussian, is sequentially applied to data samples, leading its distribution to a pure noise distribution in the limit. The reverse process, as the name suggests, reverses the noising process and takes the pure noise distribution "backward in time" to the original data distribution, thereby allowing us to generate new samples from the data distribution. A key element in implementing the reverse process is the _score function_ of the data distribution, which is the gradient of its log density. Since the data distribution is typically unknown, the score function must be learned from samples .

Recent advances in the theory of diffusion models have revealed that the task of sampling, in fact, reduces to score estimation under minimal assumptions on the data distribution . In particular, Chen et al.  have shown that \(L^{2}\)-accurate score estimates along the forward process are sufficient for efficient sampling. Thus, assuming access to an oracle for \(L^{2}\)-accurate score estimation, one can efficiently sample from essentially any data distribution. However, this leaves open the question of whether score estimation oracles themselves can be implemented efficiently, in terms of both required sample size and computation, for interesting classes of distributions.

We show that \(L^{2}\)-accurate score estimation, in the absence of strong assumptions on the data distribution, is computationally hard, even when sample complexity is polynomial in the relevant problem parameters. This establishes a _statistical-to-computational gap_ for \(L^{2}\)-accurate score estimation, which refers to an intrinsic gap between what is statistically achievable and computationally feasible. Our hard-to-estimate distributions are the "Gaussian pancakes" distributions, which previous works  have shown are computationally indistinguishable from the standard Gaussian under plausible and widely believed hardness assumptions. In fact, "breaking" the hardness of Gaussian pancakes, by means of an efficient detection or estimation algorithm, has profound implications for lattice-based cryptography, which is central to the post-quantum cryptography standardization led by the National Institute of Standards and Technology (NIST) . Building on the sampling-toscore estimation reduction by Chen et al. , we show that computationally efficient \(L^{2}\)-accurate score estimation for Gaussian pancakes implies an efficient algorithm for distinguishing Gaussian pancakes from the standard Gaussian. Thus, while sampling may ultimately reduce to \(L^{2}\)-accurate score estimation under minimal assumptions on the data distribution, score estimation itself requires stronger assumptions on the data distribution for computational feasibility. It is worth noting that the presence of statistical-to-computational gaps in \(L^{2}\)-accurate score estimation was anticipated by Chen et al. [17, Section 1.1], who mentioned it without formal statement or proof.

### Main contributions

Our main result is a simple reduction from the Gaussian pancakes problem (i.e., the problem of distinguishing Gaussian pancakes from the standard Gaussian) to \(L^{2}\)-accurate score estimation. We show that given oracle access to \(L^{2}\)-accurate score estimates along the forward process (Assumption A3), one can compute a test statistic that distinguishes, with non-trivial success probability, whether the given score estimates belong to a Gaussian pancakes distribution or the standard Gaussian.

A Gaussian pancakes distribution \(P_{}\) with secret direction \(^{d-1}\) can be viewed as a "backdoored" Gaussian. It is distributed as a (noisy) discrete Gaussian along the direction \(\) and as a standard Gaussian in the remaining \(d-1\) directions (see Figure 1).1 A _class_ of Gaussian pancakes \((P_{})_{^{d-1}}\) is parameterized by two parameters, \(\) and \(\), which govern the spacing and thickness of pancakes, respectively. For instance, a Gaussian pancakes distribution \(P_{}\) with spacing \(\) and thickness \( 0\) is essentially supported on the one-dimensional lattice \((1/)\) along the secret direction \(\). The Gaussian pancakes _problem_, then, is a sequence of hypothesis testing problems indexed by the data dimension \(d\) in which the goal is to distinguish between samples from a Gaussian pancakes distribution (with unknown \(\)) and the standard Gaussian distribution \((0,I_{d})\) with success probability slightly better than random guessing (see Section 2.3 for formal definitions). Thus, our result can be summarized informally as follows.

**Theorem 1.1** (Informal, see Theorem 3.1).: _Let \((d)>1,(d)>0\) be sequences such that \( 1/(d)\) and the corresponding \((,)\)-Gaussian pancakes distributions \((P_{})_{^{d-1}}\) all satisfy \((P_{},(0,I_{d}))>1/2\). Then, there exists a polynomial-time randomized algorithm with access to a score estimation oracle of \(L^{2}\)-error \(O(1/)\) that solves the Gaussian pancakes problem._

We emphasize that the hardness of estimating score functions of Gaussian pancakes distributions arises solely from hardness of _learning_. The score function of \(P_{}\) is efficiently approximable by function classes commonly used in practice for generative modeling, such as residual networks . In addition, under the scaling \(=O(1)\), which includes the cryptographically hard regime, the secret parameter \(\) can be estimated upto \(L^{2}\)-error \(\) via brute-force search over \(^{d-1}\) with \((d,,1/)\) samples (Theorem 4.2). The estimated parameter \(}\) in turn enables \(L^{2}\)-accurate score estimation (see Section 4). Our estimator, based on projection pursuit , may be of independent interest.

We also analyze properties of Gaussian pancakes using Banaszczyk's theorems on the Gaussian mass of lattices . This serves two purposes. Firstly, it allows us to verify that Gaussian pancakes distributions readily satisfy the assumptions for the sampling-to-score estimation reduction of Chen et al. , namely Lipschitzness of the score functions along the forward process (Assumption A1). This is necessary as the proof of our main theorem crucially relies on the reduction. Secondly, Banaszczyk's theorems provide simple means of analyzing properties of Gaussian pancakes, which are interesting mathematical objects in their own right. While these theorems are standard tools in lattice-based cryptography (see e.g., ), they are likely less known outside the community.

### Related work

Theory of diffusion models.Recent advances in the theoretical study of diffusion models have focused on convergence rates of discretized reverse processes . Of particular relevance to our work is the result of Chen et al.  who showed that \(L^{2}\)-accurate score estimates are sufficient to guarantee convergence rates that are polynomial in all the relevant problem parameters under minimal assumptions on the data distribution, namely Lipschitz scores throughout the forward process and finite second moment. Prior studies fell short by requiring strong structural assumptions on the data distribution, such as a log-Sobolev inequality , assuming \(L^{}\)-accurate score estimates , or providing convergence rates that are exponential in the problem parameters [9; 23; 22]. We note that recent works have made the "minimal" assumptions of Chen et al.  even more minimal by considering early stopped reverse processes and dropping the Lipschitz score assumption [15; 6]. We refer to the book draft of Chewi  for more background.

Gaussian pancakes.The Gaussian pancakes problem stands out among problems exhibiting statistical-to-computational gaps due to its versatility and strong hardness guarantees. Initially introduced as hard-to-learn Gaussian mixtures in the work of Diakonikolas et al. , which established their SQ hardness, Gaussian pancakes have been extensively utilized in establishing SQ lower bounds for various statistical inference problems such as robust Gaussian mean estimation  and agnostically learning halfspaces and ReLUs over Gaussian inputs . For further details, we refer to the textbook by Diakonikolas and Kane [29; Chapter 8]. Gaussian pancakes distributions themselves serve as instances of fundamental high-dimensional inference problems such as non-Gaussian component analysis  and Wasserstein distance estimation in the spiked transport model .

Bruna et al.  initiated the exploration of the _cryptographic_ hardness of Gaussian pancakes. They showed that assuming hardness of worst-case lattice problems, fundamental to lattice-based cryptography [56; 60], both the Gaussian pancakes and the closely related _continuous_ learning with errors (CLWE) problems are hard. Follow-up work by Gupte et al.  showed that the learning with errors (LWE) problem , a versatile problem which lies at the heart of numerous lattice-based cryptographic constructions, reduces to the Gaussian pancakes as well. These cryptographic hardness results have sparked a wave of recent works showcasing various applications of this newly discovered property of Gaussian pancakes. Notable examples include planting undetectable backdoors in machine learning models , novel public-key encryption schemes based on Gaussian pancakes , and cryptographic hardness of agnostically learning halfspaces [26; 75].

For additional related work on score estimation and statistical-to-computational gaps, see Section A.

### Future directions

Our work brings together the latest advances from the theory of diffusion models and computational complexity of statistical inference. This intersection provides fertile ground for future research, some avenues of which we outline below.

Figure 1: Top: Scatter plot of 2D Gaussian pancakes \(P_{}\) with secret direction \(=(-1/,1/)\), spacing \(=6\), and thickness \(\{0.01,0.05,0.25\}\). Bottom: Re-scaled probability densities of Gaussian pancakes (blue) for each \(\{0.01,0.05,0.25\}\) and the standard Gaussian (black) along \(\). For fixed \(\), the pancakes “blur into each other” as \(\) increases.

Stronger data assumptions for efficient score estimation.Our result shows that for any class of distributions that encompasses hard Gaussian pancakes, computationally efficient \(L^{2}\)-accurate score estimation is impossible. This implies that stronger assumptions on the data, which exclude Gaussian pancakes, are necessary for efficient score estimation. Finding assumptions that exclude hard instances while being able to capture realistic models of data is an interesting open problem.

Weaker criteria for evaluating sample quality.In the context of learning Gaussian pancakes, if the goal of the sampling algorithm were to merely fool computationally bounded testers that lack knowledge of the secret \(^{d-1}\), then it could simply generate standard Gaussian samples and fool any polynomial-time test. Thus, sampling is strictly easier than \(L^{2}\)-accurate score estimation if the criteria for evaluating sample quality is less stringent. This suggests exploring sampling under different evaluation criteria, such as "discriminators" with bounded compute or memory. For example, Christ et al.  have used weaker notions of sample quality in the context of watermarking large language models (LLMs). More precisely, they used the notion of computational indistinguishability to guarantee quality of the watermarked model relative to the original model. Exploring potential connections to the literature on leakage simulation [45; 18; 76] and outcome indistinguishability [41; 32; 33] is also an interesting future direction, as these areas have addressed related questions for distributions on finite sets.

Extracting "knowledge" from sampling algorithms.A key difficulty in directly reducing the Gaussian pancakes problem to sampling is that the Gaussian pancakes problem is hard for polynomial-time distinguishers, even with access to _exact_ sampling oracles (see Section 2.3 for more details).2 Thus, any procedure utilizing the learned sampler in a black box manner cannot solve the Gaussian pancakes problem. This is puzzling since for the algorithm to have "learned" to generate samples from the given distribution, it _ought to_ possess some non-trivial information about it (e.g., leak information about the secret parameter \(\))!

This raises the question: How much "knowledge" can we extract with _white box_ access to the sampling algorithm? Under reasonable structural assumptions on the sampling algorithm, can we extract privileged information about the data distribution it simulates, beyond what is obtainable solely via sample access? Our work provides one such example. White box access to a diffusion model gives access to its score estimates. These score estimates, which enable efficient solutions to the Gaussian pancakes problem, constitute privileged knowledge that cannot be learned efficiently even with unlimited access to bona fide samples.

## 2 Preliminaries

Notation.We denote by \((a_{t})\) the sequence \(a_{1},a_{2},a_{3},\) indexed by \(t\). When there is no natural ordering on the index set (e.g., \((a_{})_{^{d-1}}\)), we interpret it as a set. We write \(a b\) or \(a=O(b)\) to mean that \(a Cb\) for some universal constant \(C>0\). The notation \(a b\) and \(a=(b)\) are defined analogously. We write \(a b\) or \(a=(b)\) to mean that \(a b\) and \(a\) both hold. We write \(,\) to mean logical AND and OR, respectively. We also write \(a b\) and \(a b\) to mean \((a,b)\) and \((a,b)\), respectively. We denote by \(Q_{d}\) the "standard" Gaussian \((0,1/(2)I_{d})\) (see Remark 2.3). We omit the subscript \(d\) when it is clear from context.

### Background on denoising diffusion probabilistic modeling (DDPM)

We give a brief exposition on _denoising diffusion probabilistic models_ (DDPM) , a specific type of diffusion model, since the main reduction of Chen et al.  pertains to DDPMs. This section closely follows [17, Section 2.1] and [80, Section 3]

Let \(D\) be the target distribution defined on \(^{d}\). In DDPMs, we begin with the **forward process**, an Ornstein-Uhlenbeck (OU) process that converges towards \(Q=(0,(1/2)I_{d})\), which is described by the following stochastic differential equation (SDE). Note that the constant \(1/\) in front of \(dW_{t}\) in Eq.(1) is non-standard.3 See Remark 2.3 for an explanation of our unconventional choice of variance for the resulting stationary distribution \(Q\).

\[dX_{t}=-X_{t}dt+(1/)dW_{t}\, X_{0} D_{0}=D\,\] (1)

where \(W_{t}\) is the standard Brownian motion in \(^{d}\).

Let \(D_{t}\) be the distribution of \(X_{t}\) along the OU process. It is well-known that for any distribution \(D\), \(D_{t} Q\) exponentially fast in various divergences and metrics such as the KL divergence . In this work, we only consider Gaussian pancakes \((P_{})\) and the standard Gaussian \(Q\). If \(D=P_{}\) and \(t>0\), then the distribution \(D_{t}\) is simply another Gaussian pancakes distribution with a larger thickness parameter (see Definition 2.5). Meanwhile, if \(D=Q\), then \(D_{t}=Q\) for any \(t 0\). We run the OU process until time \(T>0\), and then simulate the **reverse process**, described by the following SDE.

\[dY_{t}=(Y_{t}+(1/) D_{T-t}(Y_{t}))dt+(1/)dW_{t}\, Y_{0} F_{0}=D_{T}\.\] (2)

Here, \( D_{t}\) is called the _score function_ of \(D_{t}\). Since the target \(D\) is not known, in order to implement the reverse process the score function must be estimated from data samples. Assuming for the moment that we have _exact_ scores \(( D_{t})_{t[0,T]}\), if we start the reverse process from \(F_{0}=D_{T}\), we have \(Y_{t} F_{t}=D_{T-t}\) for any \(0 t T\), and ultimately \(Y_{T} F_{T}=D\). Thus, starting from (approximately) pure noise \(D_{T} Q\), the reverse process generates fresh samples from the target distribution \(D\). We need to make several approximations to algorithmically implement this reverse process. In particular, we need to approximate \(D_{T}\) by \(Q\), discretize the continuous-time SDE, and approximate scores along the (discretized) forward process. Let \(h>0\) be the step size for the SDE discretization and denote \(N:=T/h\). Given score estimates \((s_{kh})_{k[N]}\), the DDPM algorithm performs the following update (see e.g., [67, Chapter 4.3]).

\[_{k+1}=e^{h}_{k}+(1/)(e^{h}-1)s_{(N-k)h}(_{k})+-1}_{k}\,\] (3)

where \(_{k} Q\) is an independent Gaussian vector. Note that the only dependence of the reverse process on the target distribution \(D\) arises through the score estimates \((s_{kh})_{k[N]}\).

The result of Chen et al.  demonstrates polynomial convergence rates of this process to the target distribution \(D\), assuming access to \(L^{2}\)-accurate score estimates \((s_{kh})_{k[N]}\) and minimal conditions on the target \(D\). We refer to Section 3.1 for a formal statement of their assumptions and theorem.

### Lattices and discrete Gaussians

Lattices.A _lattice_\(^{d}\) is a discrete additive subgroup of \(^{d}\). In this work, we assume all lattices are full rank, i.e., their linear span is \(^{d}\). For a \(d\)-dimensional lattice \(\), a set of linearly independent vectors \(\{_{1},,_{d}\}\) is called a _basis_ of \(\) if \(\) is generated by the set, i.e., \(=B^{d}\) where \(B=[_{1},,_{d}]\). The _determinant_ of a lattice \(\) with basis \(B\) is defined as \(()=|(B)|\).

The _dual lattice_ of a lattice \(\), denoted by \(^{*}\), is defined as

\[^{*}=\{^{d}, \}\.\]

If \(B\) is a basis of \(\) then \((B^{T})^{-1}\) is a basis of \(^{*}\); in particular, \((^{*})=()^{-1}\).

Fourier analysis.We define the Fourier transform of a function \(f:^{d}\) by

\[()=_{^{d}}f()(-2 i, )d\.\]

The Poisson summation formula offers a valuable tool for analyzing functions defined on lattices.

**Lemma 2.1** (Poisson summation formula).: _For any lattice \(^{d}\) and any function \(f:^{d}\),4_

\[f()=(^{*})(^{*})\,\]

_where we denote \(f(S)=_{x S}f(x)\) for any set \(S\)._Discrete Gaussians.A discrete Gaussian is a discrete distribution whose probability mass function is given by the Gaussian function. These distributions are closely related to Gaussian pancakes distributions and their properties will be crucial for our analysis.

**Definition 2.2** (Gaussian function).: _We define the Gaussian function \(_{s}:^{d}\) of width \(s>0\) by_

\[_{s}():=(-\|\|^{2}/s^{2})\.\]

_When \(s=1\), we omit the subscript and simply write \(\). In addition, for any lattice \(^{d}\), we denote by \(_{s}()=_{}_{s}()\) the corresponding Gaussian mass of \(\)._

**Remark 2.3** (Non-standard choice of "standard" variance).: _We refer to \(Q_{d}=(0,1/(2)I_{d})\) as the "standard" Gaussian. This is indeed the standard choice in lattice-based cryptography because it simplifies normalization factors that arise from taking Fourier transforms. For instance, it allows us to simply write \(_{s}=s^{n}_{1/s}\) and \(=\) for \(s=1\). To translate these results for the "usual" standard Gaussian, we can simply replace \(s\) with \(s/\) for each occurrence._

**Definition 2.4** (Discrete Gaussian).: _For any lattice \(^{d}\), parameter \(s>0\), and shift \(^{d}\), the discrete Gaussian \(D_{-,s}\) is a distribution supported on the coset \(-\) with probability mass_

\[D_{-,s}()=(-)}{_{s}( -)}\.\]

We denote by \(A_{}\) the discrete Gaussian of width \(s=1\) supported on the one-dimensional lattice \((1/)\). The distribution of a Gaussian pancakes distribution \(P_{}\) along the hidden direction is a smoothed discrete Gaussian, which we formalize in the following.

**Definition 2.5** (Smoothed discrete Gaussian).: _For any \(>0\), let \(A_{}\) be the discrete Gaussian of width \(1\) on the lattice \((1/)\). We define the \(\)-smoothed discrete Gaussian \(A_{}^{}\) as the distribution of the random variable \(y\) induced by the following process._

\[y=}}(x+ z)\,\ x A_{} z Q\.\]

_Furthermore, the density of \(A_{}^{}\) is given by_

\[A_{}^{}(z)=}}{((1/) )}_{k}(k/)_{/}}(z-k/})\.\] (4)

Likelihood ratio of smoothed discrete Gaussians.Let \(A_{}^{}\) be the \(\)-smoothed discrete Gaussian on \((1/)\). Its likelihood ratio \(T_{}^{}\) with respect to the standard Gaussian is given by

\[T_{}^{}(z)=}}{((1/) )}_{k}_{}(z-}k/)\.\] (5)

When \(\) and \(\) are clear from context, we omit them and simply denote the likelihood ratio by \(T\).

### Gaussian pancakes

We define Gaussian pancakes distributions using the likelihood ratio \(T_{}^{}=A_{}^{}/Q\). It is important to note that our parametrization differs from the one used in previous works [13; 39]. We believe our parametrization is more convenient as it elucidates a natural partial ordering on the space of parameters \((,)\). In addition, there is an explicit mapping between the two different parametrizations, so computationally hard parameter regimes identified by previous works [13; 39] can readily be translated into setting. See Remark 2.7 for more details.

**Definition 2.6** (Gaussian pancakes).: _For any \(d\), spacing and thickness parameters \(,>0\), we define the \((,)\)-Gaussian pancakes distribution \(P_{,}^{}\) with secret direction \(^{d-1}\) by_

\[P_{,}^{}():=Q() T_{}^{}( ,)\,\]

_where \(Q=(0,(1/2)I_{d})\) and \(T_{}^{}\) is the likelihood ratio of \(A_{}^{}\) with respect to \(Q\). When parameters \(,\) are clear from context, we omit them in the notation and simply denote the distribution by \(P_{}\) to avoid clutter._

**Remark 2.7** (Partial ordering on Gaussian pancakes).: _The smoothed discrete Gaussian \(A_{}^{}\) arises in the OU process for the discrete Gaussian \(A_{}\) at time \(t=(})\). Consequently, for any fixed \(>0\), there exists a natural partial ordering on the family of Gaussian pancakes parametrized by \((,)\), given by \((,_{1})(,_{2})\) whenever \(_{1}_{2}\). This ordering arises from the fact that \(A_{}^{_{1}}\) reduces to \(A_{}^{_{2}}\) whenever \(_{1}_{2}\) via the OU process starting at \(t_{1}=(^{2}})\) and run until \(t_{2}=(^{2}})\)._

**Definition 2.8** (Advantage).: _Let \(:\{0,1\}\) be any decision rule (i.e., distinguisher). For any pair of distributions \((,)\) on \(\), we define the advantage of \(\) by_

\[():=|[(X)=1]-[(X)=1]|.\]

_For a sequence of decision rules \((_{d})_{d}\) and distribution pairs \((_{d},_{d})_{d}\), we say \((_{d})\) has non-negligible advantage with respect to \((_{d},_{d})\) if its advantage sequence \(_{d}=(_{d})\) is a non-negligible function in \(d\), i.e., a function in \((d^{-c})\) for some constant \(c>0\)._

**Definition 2.9** (Computational indistinguishability).: _A sequence of distribution pairs \((_{d},_{d})\) is computationally indistinguishable if no \((d)\)-time computable decision rule achieves non-negligible advantage._

**Definition 2.10** (Gaussian pancakes problem).: _For any sequences \((d),(d)>0\) and \(n(d)\), the \((,,n)\)-Gaussian pancakes problem is to distinguish \((_{d},_{d})\) with non-negligible advantage, where \(_{d}\) is the \(n\)-sample distribution induced by the following two-stage process: 1) draw \(\) uniformly from \(^{d-1}\), 2) draw \(n\) i.i.d. Gaussian pancakes samples \(_{1},,_{n} P_{}^{ n}\), and \(_{d}=Q_{d}^{ n}\), i.e., the distribution of \(n\) i.i.d. standard Gaussian vectors._

As will be explained next, the exact number of samples \(n\) is irrelevant for most applications due to the cryptographic hardness of the Gaussian pancakes problem. For certain parameter regimes of \((,)\), the problem maintains its computational intractability regardless of the sample size \(n\).

Hardness of Gaussian pancakes.There is an abundance of evidence demonstrating the hardness of the Gaussian pancakes problem. This makes it compelling to directly assume that Gaussian pancakes and the standard Gaussian are _computationally indistinguishable_ (see Definition 2.9) for certain parameter regimes of \((,)\). Initial results by Bruna et al. [13, Corallary 4.2] showed that the Gaussian pancakes problem is as hard as worst-case lattice problems for any parameter sequence \((,)\) satisfying \( 2\) and \( 1/(d)\). SQ hardness of the problem has been demonstrated as well [31; 13; 27]. Perhaps surprisingly, the reduction of Bruna et al. shows that even with _unlimited_ access to an _exact_ sampling oracle, no polynomial-time algorithm \(\) can achieve non-negligible advantage on the \((,)\)-Gaussian pancakes problem. This stems from the fact that the running time of \(\) naturally restricts the number of samples it can "see", resolving the apparent mystery.

An important follow-up work by Gupte et al.  reduced the well-known LWE problem to the Gaussian pancakes problem. Assuming sub-exponential hardness of LWE , a standard assumption underlying post-quantum cryptosystems expected to be standardized by NIST, the Gaussian pancakes problem is hard for any \(( d)^{1+}\), where \(>0\) is any constant, and \( 1/(d)\)[39; Section 1.2]. Taken together, these findings strongly support the hardness of Gaussian pancakes for the specified regimes of \((,)\). Note, however, that the condition \( 1/(d)\) is _necessary_ for hardness as there exist polynomial-time algorithms, based on lattice basis reduction, for exponentially small \(\)[83; 25].

## 3 Hardness of Score Estimation

Our main result is Theorem 3.1, which presents a reduction from the Gaussian pancakes problem to \(L^{2}\)-accurate score estimation. Since the Gaussian pancakes problem exhibits both cryptographic and SQ hardness in the parameter regime \( 2\) and \( 1/(d)\)[13; 39], these notions of hardness extend to the task of estimating scores of Gaussian pancakes. Further details on the hardness of the Gaussian pancakes problem can be found in Section 2.3.

**Theorem 3.1** (Main result).: _Let \((d)>1,(d)>0\) be any pair of sequences such that \( 1/(d)\) and the corresponding (sequence of) \((,)\)-Gaussian pancakes distributions \((P_{})_{^{d-1}}\)satisfies \((P_{},Q_{d})>1/2\) for any \(d\). Then, for any \((0,1)\), there exists a \((d)(1/)\)-time algorithm with access to a score estimation oracle of \(L^{2}\)-error \(O(1/)\) that solves the \((,)\)-Gaussian pancakes problem with probability at least \(1-\)._

The requirement \((P_{},Q_{d})>1/2\) is mild and entirely captures interesting parameter regimes of \((,)\) for which cryptographic and SQ hardness of Gaussian pancakes are known. We provide a sufficient condition in Lemma B.9, which shows that \(<C\) for some constant \(C>0\) ensures separation in TV distance.

Theorem 3.1 implies that even a score estimation oracle running in time \((d,2^{1/^{2}})\), where \(>0\) is the \(L^{2}\) estimation error bound, implies a \((d)\) time algorithm for the Gaussian pancakes problem. This means that estimating the score functions of Gaussian pancakes to \(L^{2}\)-accuracy \(\) even in \((d,2^{1/^{2}})\) time is impossible under standard cryptographic assumptions.

### Proof outline of Theorem 3.1

Here, we sketch the proof of Theorem 3.1 and defer full details to Section B.1. We first recall the sampling-to-score estimation reduction of Chen et al.  and its required assumptions to illustrate the main idea behind our reduction. The precise formulation of our idea is given in Lemma 3.3.

Assumptions on data distribution.The reduction of Chen et al.  requires the following assumptions on the data distribution \(D\) over \(^{d}\).

1. [label=**A0**]
2. (Lipschitz score). For all \(t 0\), the score \( D_{t}\) is \(L\)-Lipschitz.
3. (Finite second moment). \(D\) has finite second moment, i.e., \(_{2}^{2}:=_{ D}[\|\|^{2}]<\).
4. (Score estimation error). For step size \(h:=T/N\) and all \(k=1,,N\), \[_{D_{kh}}[\|s_{kh}- D_{kh}\|^{2}]_{}^{2}\.\]

**Theorem 3.2** ([17, Theorem 2]).: _Suppose assumptions_ **A1-A3** _hold. Let \(Q_{d}\) be the standard Gaussian on \(^{d}\) and let \(F_{T}\) be the output of the DDPM algorithm (Section 2.1) at time \(T\) with step size \(h:=T/N\) such that \(h 1/L\), where \(L 1\). Then, it holds that_

\[(F_{T},D)(D Q_{d})} (-T)}_{}++L _{2}h)}_{}+ }}_{}\.\] (6)

_In particular, if \(_{2} d\), then \(T(((D Q_{d})/),1)\) and \(h^{2}/(L^{2}d)\) gives_

\[(F_{T},D)+_{} ((D Q_{d})/)},1)\,N=d}{^{2}}\.\]

Theorem 3.2 shows that if the unknown data distribution \(D\) has Lipschitz scores and satisfies \(_{2} d\), then its \(\)-accurate score estimates along the discretized forward process \((s_{kh})_{k[N]}\) can be used to compute a _certificate_ of Gaussianity defined as follows.

\[:=_{k[N]}_{Q_{d}}\|s_{kh}()+2\|^{2}\.\] (7)

Using \(\) as a test statistic, we decide \(D(P_{})_{^{d-1}}\) if \(\) for some carefully chosen threshold \(>0\) and \(D=Q_{d}\) otherwise. This test statistic is motivated by the observation that the discretized reverse process depends on the data distribution \(D\)_solely_ through its score estimates (see Eq.3). For _any_ sequence of score estimates \((s_{t})_{t[0,T]}\) the reverse process outputs \(F_{T}\) that is TV-close to \(D\) provided its \(L^{2}\) error along the forward process \((D_{t})_{t[0,T]}\) is small.

We claim that if \(^{2}\)_and_ the score estimates \((s_{kh})\) are \(\)-accurate for \((D_{kh})\), then the output of the reverse process \(F_{T}\) is roughly \((+)\)-close in TV distance to the standard Gaussian. This is because the standard Gaussian is invariant throughout the OU process, so \(\) is, in fact, the \(L^{2}\) score estimation error bound for the case where the data distribution \(D\) is equal to \(Q_{d}\). In other words, \(^{2}\) means that for all \(k[N]\), the score estimates \((s_{kh})\) are \(\)-close to \(-2\) which is the score function of \(Q_{d}\). Thus, \(\) is small only if \(D\) is close in TV distance to \(Q_{d}\), which shows that \(\) distinguishes between \(D=Q_{d}\) and \(D(P_{})_{^{d-1}}\) provided \((P_{},Q_{d})>1/2\). The following lemma formalizes this idea. Theorem 3.1 follows from Lemma 3.3 and Lipschitzness of the score functions (Lemma B.5).

**Lemma 3.3** (Gaussianity testing with scores).: _For any \((0,1)\) and \(K 2\), let \(D\) be any distribution on \(^{d}\) such that \(_{2} d\) and \((D Q_{d}) K\) with \(L\)-Lipschitz score \( D_{t}\) for any \(t 0\), and let \(</\) be the \(L^{2}\) score estimation error bound with discretization parameters \(T(K/),h^{2}/(L^{2}d)\), and \(N:=T/h\) so that \((F_{T} D)\) (via Theorem 3.2). If \((s_{kh})_{k[N]}\) are \(\)-accurate score estimates for the forward process \((D_{kh})_{k[N]}\) and \(=_{k[N]}_{Q_{d}}\|s_{kh}()+2\|^{2}\), then_

\[(D,Q_{d})+\.\]

_In particular, if \((D,Q_{d})>1/2\) then there exist constants \(C_{1},C_{2}>0\) such that for any score estimates \((s_{kh})_{k[N]}\) of the forward process satisfying \(_{k[N]}_{D_{kh}}\|s_{kh}()- D_{kh}()\|^ {2} C_{1}/ K\), it holds_

\[ C_{2}/ K\.\]

Proof.: By Theorem 3.2 and our choice of \(L^{2}\) score estimation error bound \(\), we have \((F_{T},D)\). In addition, the score estimates \((s_{t})\) also satisfy a \(\)-error bound with respect to the forward process of \(Q_{d}\), which is invariant with respect to time \(t\). Thus, Theorem 3.2 applied with \(D=Q_{d}\) as the data distribution, discretization parameters \(T,h\), and score estimates \((s_{kh})\) gives \((F_{T},Q_{d})\). By the triangle inequality, we have

\[(D,Q)(F_{T},D)+(F_{T}, Q)+\.\]

The second part of the theorem follows from using the assumptions \((D,Q)>1/2\), \(K 2\), and fixing \(>0\) to a sufficiently small constant, which gives us

\[1\.\]

## 4 Sample Complexity of Gaussian Pancakes

To establish that there is indeed a _gap_ between statistical and computational feasibility, we demonstrate a polynomial upper bound on the sample complexity of \(L^{2}\)-accurate score estimation for Gaussian pancakes. In particular, we show that a sufficiently good estimate \(}\) of the hidden direction \(\) is enough (Lemma 4.1). The polynomial sample complexity of score estimation then follows from Theorem 4.2, which shows that if \((d)\) and \((d)\) satisfy \(=O(1)\), then \(1-},^{2}^{2}\) is _statistically_ achievable with \((d,,1/)\) samples, albeit through brute-force search over \(^{d-1}\).

Our estimator \(}\) is based on projection pursuit [35; 43]. We design a functional \(E:^{d-1}\) of the form \(E():=_{ P_{}}g(,)\) with \(g:\) carefully chosen to ensure that \(E(_{1}) E(_{2})\) if and only if \(|_{1},||_{2},|\). Given such a "monotonic" functional (its empirical counterpart \(\), to be precise), our estimator for the secret direction \(\) is essentially

\[}=*{arg\,max}_{^{d-1}}( {v})\.\]

We remark that parameter regime \(=O(1)\) in Theorem 4.2 encompasses the cryptographically hard regime of Gaussian pancakes. It is also worth noting that if \((,)=()\), then Gaussian pancakes are _statistically_ indistinguishable from \(Q_{d}\) by Lemma B.10.

We defer the full proofs of Lemma 4.1 and Theorem 4.2 to Section C.

**Lemma 4.1** (Score-to-parameter estimation reduction).: _For any \(>1,>0\), let \(P_{}\) be the \((,)\)-Gaussian pancakes distribution with secret direction \(^{d-1}\). Given any \((0,1)\) and \(}^{d-1}\) such that \(1-},^{2}^{2}\), the score estimate \(()=-2+ T_{}^{}(,})\) satisfies_

\[_{ P_{}}\|()-s()\|^{2} (1,1/^{8})^{2}d\,\]

_where \(s()=-2+ T_{}^{}(,)\) is the true score function of \(P_{}\)._

**Theorem 4.2** (Sample complexity of parameter estimation).: _For any constant \(C>0\), given \((d)>1,(d)>0\) such that \(<C\), estimation error parameter \(>0\), and \((0,1)\), there exists a brute-force search estimator \(}:^{d n}^{d-1}\) that uses \(n=(d,,1/,1/)\) samples and achieves \(\|}(_{1},,_{n})-\|^{2}^{2}\) with probability at least \(1-\) over i.i.d. samples \(_{1},,_{n} P_{}\), where \(P_{}\) is the \((,)\)-Gaussian pancakes distribution with secret direction \(^{d-1}\)._