# Aggregate-and-Adapt Natural Language Prompts

for Downstream Generalization of CLIP

 Chen Huang, Skyler Seto, Samira Abnar, David Grangier, Navdeep Jaitly & Josh Susskind

Apple

{chen-huang,sseto,abnar,grangier,njaitly,jsusskind}@apple.com

###### Abstract

Large pretrained vision-language models like CLIP have shown promising generalization capability, but may struggle in specialized domains (_e.g._, satellite imagery) or fine-grained classification (_e.g._, car models) where the visual concepts are unseen or under-represented during pretraining. Prompt learning offers a parameter-efficient finetuning framework that can adapt CLIP to downstream tasks even when limited annotation data are available. In this paper, we improve prompt learning by distilling the textual knowledge from natural language prompts (either human- or LLM-generated) to provide rich priors for those under-represented concepts. We first obtain a prompt "summary" aligned to each input image via a learned prompt aggregator. Then we jointly train a prompt generator, optimized to produce a prompt embedding that stays close to the aggregated summary while minimizing task loss at the same time. We dub such prompt embedding as **A**aggregate-and-**A**dapted **P**rompt **E**mbedding (**AAPE**). AAPE is shown to be able to generalize to different downstream data distributions and tasks, including vision-language understanding tasks (_e.g._, few-shot classification, VQA) and generation tasks (image captioning) where AAPE achieves competitive performance. We also show AAPE is particularly helpful to handle non-canonical and OOD examples. Furthermore, AAPE learning eliminates LLM-based inference cost as required by baselines, and scales better with data and LLM model size.

## 1 Introduction

Most existing vision-language tasks rely on large pretrained models like CLIP , which are often adapted to downstream tasks using a small amount of labeled data (as compared to the web-scale pretraining data). This is shown by many studies () to be likely to obtain poor generalization performance in special domains, such as satellite imagery and fine-grained classification of car models or flower species. Such overfitting behavior is a result of limited data for those _tail class concepts_ in both pretraining and downstream tasks. The domain gap between pretraining and downstream data further compounds the generalization problem. For instance, CLIP may not see enough image-text pairs to identify different car models during pretraining. This makes downstream generalization to fine-grained car models difficult, especially in low-data scenarios.

In this paper, we investigate using _pure text-based knowledge_ to boost the downstream generalization of CLIP over different data distributions and tasks, with a special focus on few-shot and OOD tasks. Note similar ideas have been explored in recent works that leverage the implicit textual knowledge in Large Language Models (LLMs) to aid vision-language tasks. For example in , GPT-2/3  is used to generate image descriptions for the tasks of Visual Question Answering (VQA) and image captioning. While for CLIP-based image classification,  use GPT-3 to generate natural language attributes or captions for each class, and then classify images based on such information. Despite the success of these methods, they all suffer from large inference cost due to the use of LLMat test time. More critically, the LLM-generated texts are not necessarily beneficial, since they might be noisy and irrelevant to the considered task (see one example in Fig. 1(a)).

To address the two issues, we propose a new prompt learning method that is boosted by task-relevant language priors but does not incur any LLM cost at test time. The high-level idea is to learn prompts via distillation of input-adapted textual knowledge, which is especially useful to recognize under-represented visual concepts. Specifically, for classification of _object-centric images_, we follow  to first query GPT-3 for a set of natural language prompts that describe each class. While for more complex tasks like VQA, we use human-generated image captions that can depict _multi-object images_ with object interactions in cluttered background. More importantly, for both cases, we _learn_ to aggregate the collected reference prompts into a single prompt embedding, which is optimized by the CLIP reward to have high similarity with input image. This allows us to obtain a condensed prompt embedding that is image-aligned, ruling out redundant and irrelevant information, _e.g._, ignoring the prompt elements about headlights for an image of rear facing car. Finally, we jointly train a prompt generator conditioned on input image to generate a prompt embedding with two objectives: 1) staying close to the aggregated embedding (_i.e._, distillation from the condensed textual knowledge), 2) minimizing the task loss (_i.e._, downstream adaptation).

Fig. 1 illustrates our "aggregate-and-adapt" method for prompt learning. Note prompt aggregation and distillation is only required for the learning stage. At test time, we will discard the aggregator and use the learned prompt generator as a standalone module. This leads to compute-efficiency when compared to prior works  since we entirely eliminate the LLM-induced inference cost. Our generated **A**ggreate-and-**A**dapted **P**rompt **E**mbedding (or **AAPE**) prove highly effective on various downstream vision-language tasks. We show AAPE is a new state-of-the-art for few-shot image classification on 11 datasets, under different OOD generalization settings. AAPE can also generalize zero-shot to tasks like image-to-text retrieval, image captioning and VQA. When finetuned on these tasks, AAPE achieves even better performance than SOTA vision-language models (_e.g._, MAGMA ) whose entire image and text networks are fine-tuned at large cost.

To summarize, our **main contributions** are:

* A new prompt learning method that distills the textual knowledge from human- or LLM-generated natural language prompts to improve the downstream generalization of CLIP.
* Our learned AAPE achieves compelling performance on various downstream vision-language tasks, including image-to-text retrieval, few-shot classification, image captioning and VQA.
* We offer insightful findings that AAPE is especially helpful when there are under-represented concepts in few-shot and OOD settings or ambiguous visual cues in non-canonical image views. AAPE learning is also data-efficient and scales better than baselines with LLM model size.

Figure 1: Aggregate-and-adapt the textual knowledge in natural language prompts for downstream tasks. **(a)** For classification of object-centric images, we query GPT-3 to obtain a list of prompts for each class, _e.g._, the car model of “Jeep Compass SUV 2012”. Note how redundant the reference prompts can be (_e.g._, the first two), and how they can be irrelevant to the image (_e.g._, the last prompt). Alternatively, for complex tasks like VQA, we use human-generated image captions to depict multi-object images. For all tasks, we first learn to aggregate the reference prompts into an image-aligned “summary” (prompt embedding) based on CLIP reward. Then a prompt generator is jointly trained to generate Aggregate-and-Adapted Prompt Embedding (AAPE), such that the distance between AAPE and the aggregated summary is minimized and the task loss is minimized too for adaptation purpose. **(b)** At test time, we only keep the prompt generator with the prompt aggregator discarded. Our AAPE is applicable to different vision-language tasks with strong generalization performance.

Related Work

Vision-language models.Large-scale vision-language models achieve remarkable performance on a variety of downstream tasks. One learning paradigm is based on generative encoder-decoder models, which allows a sequence-to-sequence learning format that can connect visual data to free-form language prompts [2; 8; 11; 33]. Recent works like LiMBeR  show it is also possible for an LLM to operate on simple linear mappings of visual features. Another learning paradigm is based on contrastive learning with image-text pairs, which is popularized by CLIP  and numerous follow-ups [18; 24; 29; 41; 46; 59; 63; 66]. However, both categories of vision-language models (_e.g._, CLIP and generative PaLI ) are found to struggle with special visual concepts or domains. In this paper, we focus on CLIP and improve its downstream generalization, especially under few-shot and OOD settings, via distillation of language priors. Nevertheless, our approach can be applied to other vision-language models, which we leave as future work.

Prompt learningis a parameter-efficient yet effective framework to finetune CLIP even in low-data settings. Most prompt learning methods learn text prompt vectors [67; 68] in place of hand-written sentence prompts. Other methods show the possibility of learning prompts in the image space , or in both image and text spaces [20; 64]. To reduce overfitting to seen classes during prompt learning, recent works focus on new class feature synthesis [52; 65], improved optimization [25; 45] and regularization  strategies. More related to our approach are [7; 58; 69] that align the learned prompts with hand-written prompts, with the goal of not forgetting the text knowledge from human input. These methods can be interpreted as a way of knowledge distillation from only short prompt templates. We will show the distillation from such prompt templates is suboptimal when compared to distillation from natural language prompts using LLMs.

Leveraging language in vision tasks.There is a long line of works on leveraging language to aid vision or multimodal tasks. One family of methods rely on external natural language datasets to retrieve text knowledge of image categories. For example, [6; 44] show improvements on ImageNet classification using the class descriptions retrieved from WordNet  and ImageNet-Wiki . More recent works use LLMs to generate text for downstream tasks. GPT-3 is used in [48; 56] to help with the VQA and image captioning tasks. GPT-3 is also used in [34; 39; 55] to generate class-wise attributes or captions for CLIP-based classification. Unfortunately, all these prior works suffer from the noisy text that may be task-irrelevant. In this paper, we learn to adapt LLM-generated text to the target task, but without incurring any LLM-induced inference cost.

## 3 Method

Our "aggregate-and-adapt" method for prompt learning consists of three key components: 1) generating natural language prompts per image or class, 2) learning an aggregated prompt embedding that aligns with input image and 3) learning to generate Aggregate-and-Adapted Prompt Embedding (AAPE) for downstream tasks. In the following, we provide the details for each component. Note our method is based on CLIP , but it can be easily applied to other CLIP-like vision-language models.

### Generating Natural Language Prompts

Prompt engineering.For CLIP-based image classification, the standard approach requires a set of hand-written prompts, _e.g._, "a {} in a video game" and "a dark photo of a {}", which are completed with the class name. However, this is costly because one needs to hand-construct a different set of prompt templates for each dataset (_e.g._, 80 for ImageNet in ), and that requires excessive prior knowledge about the target domain. Moreover, such prompt templates lack the descriptive details for discriminating fine-grained classes.

LLM-generated prompts.For image classification, we make use of the rich knowledge in LLMs to generate natural language prompts for a given class. One benefit of using LLMs is the ability to generate an arbitrary number of prompts, without relying on any domain knowledge. In particular, we follow the CuPL method  and query GPT-3  for a prompt set in a scalable way. Specifically, GPT-3 is queried with a few _LLM-prompts_ such as "Describe what a(n) {} looks like" and "How can you identify a(n) {}?". Then for each LLM-prompt, GPT-3 generates 10 reference prompts using a high temperature of 0.99 for diversity. Fig. 1(a) shows some example prompts for a particular car model "Jeep Compass SUV 2012". Note how the reference prompts specify the car's discriminating characteristics in its sleek exterior. For the 11 classification datasets considered in our work, we follow the full generation setting in : for each dataset, we use a different set of LLM-prompts (between 2 to 9), resulting in 20-90 reference prompts generated for each class.

Human-generated image captions.As illustrated in Fig. 2, the LLM-generated class-wise prompts are mainly suited for the classification task, where there are often object-centric images with clean background. For more complex vision-language tasks like VQA, deeper understanding is required for multi-object scenes with varying object interactions and cluttered background. To capture the textual knowledge for describing multi-object images, we use their image captions available from image-text datasets as a source of natural language prompts. Here we use COCO dataset  that consists of 5 human-annotated captions per image. It will be shown that our prompt embedding learned on COCO suffices to generalize to three difficult vision-language tasks (image-to-text retrieval, image captioning and VQA) with varying data distributions.

### Input-Adapted Prompt Aggregator

The LLM-generated prompts have one notable issue: they are not necessarily a good representation of input image. For example in Fig. 1(a), it is inaccurate to describe the input image of a silver Jeep SUV as a red one in the last prompt, whereas other prompts are more relevant. Obviously, it would be detrimental to directly use the noisy prompts to supervise the following learning stage. Another issue with both the LLM- and human-generated prompts is that they are highly redundant with repeated information. To find a better supervisory signal, we propose to first aggregate the reference prompts into an image-aligned, condensed "summary". Such prompt summary is expected to have filtered noise as well as reduced redundancy.

Given \(n\) generated prompts, we first use the text encoder of CLIP to obtain the prompt embeddings \(=[_{1},_{2},,_{n}]\). Then we learn an adaptive prompt aggregator to condense \(\) into \(m\) (\( n\)) embeddings of the same size. Ideally, the aggregation should be invariant to permutations of \(\), and scales as \((n+m)\). Here we set \(m=1\) for efficiency concerns. The aggregated result, a single prompt embedding, is denoted as \(^{a}\). One simple aggregator that has the properties of permutation invariance and high efficiency is based on just averaging \(\) into \(}\). Simple averaging is widely used in prior works [34; 39]. However, the mean \(}\) would still be compromised by the irrelevant information in \(\). Here we introduce an attention-based aggregator which allow us to align the aggregated \(^{a}\) with input image. At the same time, our prompt aggregator remains efficient and permutation invariant.

Figure 2: LLM-generated image prompts for ImageNet categories, and the hand-constructed image captions on COCO and Flickr30k datasets. Note ImageNet mainly contains **object-centric images** with relatively clean background, and the LLM-generated image prompts can describe distinct characteristics of the given classes. While COCO and Flickr30k contain **multi-object images** with cluttered background, and the hand-constructed captions can represent varying object relations.

Fig. 3(a) shows the architecture of our input-adapted prompt aggregator based on just one attention layer. The attention layer takes as input the reference prompts \(\) and a learnable prompt embedding (initialized as \(}\)). Then all the embeddings are updated as follows:

\[[^{a},^{a}]=([ },]),\] (1)

where \(^{a}\) is the desired prompt aggregation. The attention layer consists of standard multi-head cross-attention and feed-forward networks together with LayerNorm .

To make \(^{a}\) semantically related to the input image (with embedding \(\)), we optimize our prompt aggregator using the CLIP reward  in form of \((,^{a})=s((,^{a}),0)\). The CLIP reward allows \(^{a}\) to selectively blend image-related prompts through the attention mechanism. Fig. 7 in Appendix B.1 confirms that redundant or irrelevant reference prompts tend to have low attention scores, hence they are suppressed during prompt aggregation.

### Learning AAPE

The per-image prompt aggregation \(^{a}\) offers useful textual knowledge to supervise the following prompt learning stage. In this section, we elaborate how to improve prompt learning by distilling the aggregated text knowledge from \(^{a}\). Note CLIP is kept frozen during prompt learning.

As a key innovation of this paper, we propose to train a prompt generator \(h\) that directly generates the prompt embedding \(h()\) conditioned on image features \(\). We parameterize \(h\) as a lightweight network with two fully connected layers and ReLU nonlinearity. \(h\) is trained using a distillation loss \(_{}\) for knowledge distillation from \(^{a}\), as well as a task loss \(_{}\) for downstream adaptation. We call such learned \(h()\) as Aggregate-and-Adapted Prompt Embedding (AAPE). AAPE can also be viewed as an _image captioning embedding_ in the latent space, since useful text knowledge is distilled in AAPE. In the following, we detail the two training losses.

Distillation lossis simply defined as the Euclidean distance between \(h()\) and \(^{a}\):

\[_{}=\|h()-^{a}\|_{2}^{2}.\] (2)

Task loss - image classification.Besides distilling the textual knowledge from \(^{a}\), \(h()\) should adapt to the downstream task too. Here we start with the instantiation of adapting \(h()\) to the most studied task of image classification.

Note existing prompt learners for classification (_e.g._, [67; 68]) learn _individual word tokens_\(\{_{l}\}_{l=1}^{L}\) in a prompt, and then combine the learned tokens with class name embeddings \(_{i[1,,C]}\) to obtain the full prompt. By contrast, we directly generate a full prompt embedding \(h()\) without token-wise prediction. For classification, we simply combine \(h()\) and the embedding of a prompt template "a photo of a \(\{\}\)" to act as the classifier weights (to be matched to image features \(\)).

Fig. 3(b) shows the overall prompt learning framework. For the prompt template filled with the \(i\)-th class name, we use the text encoder of CLIP to obtain the template embedding \(_{i}^{d}\). Next, we concatenate \(_{i}\) and our \(h()^{d}\) followed by a projection \(g\), giving \(_{i}()=g([_{i}^{},h()^{}]^{})\). Note \(_{i}()\) does not involve any prompt engineering effort. We rely on \(_{i}\) to mainly encode the class name, while \(h()\) enriches that with input-adapted class descriptions in the latent space.

Figure 3: **(a)** Input-adapted prompt aggregator which aggregates the embeddings of reference prompts \(\) into an image-aligned, condensed prompt embedding \(^{a}\) based on CLIP reward. **(b)** Instantiation of our prompt learning approach for image classification. The CLIP model is kept frozen.

We parameterize \(g\): \(^{2d}^{d}\) by one fully connected layer with ReLU nonlinearity. The nonlinearity is important since it ensures \(_{i}()\) is not trivially equivalent to the linear combination of \(_{i}\) and \(h()\), which will ignore \(h()\) if we match the linear combination to \(\) for classification. The classification probability is given as:

\[p(y=c)=,_{c}( ))/)}{_{i=1}^{C}((,_{i}())/)},\] (3)

where \(C\) is the total number of classes, \(\) and \((,)\) denote the temperature and cosine similarity. Appendix B.2 (Table 5) compares with an \(h()\)-only baseline for classification, without combining \(_{i}\) or using projection \(g\). Results show our default classification framework achieves solid gains over the \(h()\)-only baseline.

Finally, we arrive at the overall loss function to train the prompt generator \(h\) together with projection \(g\) for image classification:

\[=_{}+_{}, \ \ _{}=- p(y=c),\] (4)

and \(=5\) is a weighting parameter. Table 6 in Appendix C provides the sensitivity analysis for \(\), which shows performance is quite robust to the \(\) value in a wide range.

Note during testing we only use the prompt generator \(h\), without querying LLM or using the prompt aggregator anymore. This removes the LLM-induced inference cost as required in , shifting such cost into our learning stage.

Task loss - beyond classification.Will the textual knowledge in \(h()\) or AAPE benefit other vision-language tasks? We consider three tasks beyond classification: **image-to-text retrieval, image captioning**\(\&\)**VQA**. Note these tasks involve multi-object images as mentioned in Section 3.1. Hence we use COCO image captions that contain textual descriptions of object relations. For all the three tasks, we train both the prompt aggregator and prompt generator \(h\) on 5 COCO captions per image. The same \(_{}\) in Eq. (2) is used, while \(_{}\) is the CLIP loss.

For image-text retrieval, we simply use AAPE as the query and evaluate its zero-shot text retrieval performance on Flickr30k dataset . We also evaluate the finetuning performance on Flickr30k when the prompt generator \(h\) is finetuned using \(_{}\) and the corresponding retrieval loss \(_{}\).

For image captioning and VQA tasks, we conjecture that the textual knowledge encoded in AAPE will be especially useful when the visual cues are confusing or missing. To test this hypothesis, we use the COCO-trained AAPE in a straightforward manner. Concretely, we follow the LiMBeR baseline in  which linearly transforms the CLIP image representation into a sequence of prompt embeddings that an LLM can process. Then AAPE is appended to the prompt sequence as a "prefix" to offer rich language priors. We similarly evaluate performance of the zero-shot or finetuned prompt generator \(h\) on downstream datasets. For finetuning, \(h\) is optimized using \(_{}\) and the corresponding task loss \(_{}\) (captioning or VQA).

## 4 Experimental Setup and Datasets

### Few-shot Image Classification

Datasets.We use 11 datasets: ImageNet , Caltech101 , OxfordPets , StanfordCars , Flowers102 , Food101 , FGVC-Aircraft , SUN397 , UCF101 , DTD  and EuroSAT . These datasets cover a wide range of generic objects and scenes, fine-grained object classes, as well as special domains with textural and satellite images. The various visual concepts in these datasets are perfect to test whether and when the textual knowledge in LLM will help. We further evaluate domain generalization on ImageNetV2 , ImageNet-Sketch , ImageNet-A  and ImageNet-R , which have different types of domain shift from ImageNet.

Implementation.We follow the prompt learning details in , including the CLIP vision backbone (ViT-B/16), learning rate schedule and the number of epochs for each dataset. Appendix C (Table 7) provides a detailed analysis of the compute cost measured on Nvidia V100 GPU, where all prompt learners are evaluated for fair efficiency comparisons.

Evaluations.We follow the few-shot evaluation protocol in , using 1, 2, 4, 8 and 16 shots per class for training (default 16), and the full testset for evaluation. Two OOD generalization settings are considered as in . 1) Generalization from base to new classes within one dataset, _i.e._, training on the base class split but testing on both base and new class splits. This helps evaluate the ID and OOD performance under a class-incremental domain shift. We follow  to also measure the harmonic mean of base and new class accuracies to quantify the ID and OOD performance trade-off. 2) Domain generalization where one trains on ImageNet (with 16 shots) and evaluates on four ImageNet variants. For all experiments, we report results as an average over three random seeds.

### Vision-Language Understanding and Generation Tasks

As mentioned in Section 3.3, we perform prompt learning on COCO dataset  before evaluation on three vision-language tasks. For the task of image-to-text retrieval, we use the same CLIP vision backbone VIT-L/14 as in [24; 40] for fair comparisons. We show both zero-shot and finetuned results (Recall@K) on Flickr30k .

For captioning and VQA tasks, we follow LiMBeR  to use the same language model and CLIP vision backbone (RN50x16). We evaluate on image captioning datasets COCO and NoCaps . Zero-shot and finetuned results are reported in terms of CIDEr-D , CLIPScore, and Ref-CLIPScore . For VQA, we prompt the model with the "[image] Q: [q] A:" format. The generation is truncated to the length of the longest ground truth answer. For evaluation, we use the VQA2 dataset  and follow the few-shot setting in  to report accuracy metric for every K-shots.

Appendix C (Table 7) shows the high efficiency with the straightforward use of AAPE for tasks beyond classification. When compared to the SOTA fully fine-tuned model MAGMA , AAPE is about 2.8/1.2 times faster for training/inference on Nvidia A100 GPU.

## 5 Results

### Image-to-Text Retrieval

We use this task to verify if AAPE can act as a meaningful image captioning embedding, which is learned to distill image-aligned text knowledge from available image captions. We do not aim to push for state-of-the-art performance for the retrieval task. Table 1 shows that we can indeed achieve strong training and finetuning performance on COCO and Flickr30k datasets, respectively. This indicates our prompt learning method is competent with producing high-quality text or captioning embedding that can successfully fulfill the task at hand. It is also worth noting that our AAPE learned on COCO can perform zero-shot retrieval on Flickr30k, obtaining competitive results with SOTA zero-shot models (_e.g._, CLIP and SigLIP). This further demonstrates the good generalization capability of AAPE over different data distributions.

    & & & COCO &  \\   & & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\    } & Unicoder-VL  & 62.3 & 87.1 & 92.8 & 86.2 & 96.3 & 99.0 \\  & Oscar  & 73.5 & 92.2 & 96.0 & - & - & - \\  & ERNIE-ViL  & - & - & - & 88.7 & 98.0 & 99.2 \\  & AAPE & \(_{ 0.1}\) & \(_{ 0.1}\) & \(_{ 0.1}\) & \(_{ 0.2}\) & \(_{ 0.1}\) & \(_{ 0.1}\) \\    } & CLIP  & 58.4 & 81.5 & 88.1 & 88.0 & 98.7 & 99.4 \\  & SigLIP  & 65.4 & 85.1 & 91.1 & 91.5 & 98.1 & 99.4 \\   & Lip  & **68.1** & **87.6** & **92.5** & 93.2 & 99.0 & 99.4 \\   & BLIP-2  & - & - & - & **96.9** & **100.0** & **100.0** \\   & AAPE & - & - & - & 90.8\({}_{ 0.2}\) & 98.6\({}_{ 0.1}\) & 99.4\({}_{ 0.1}\) \\   

Table 1: **Image-to-Text Retrieval:** zero-shot and finetuned results (Recall@K) on Flickr30k dataset. Note our AAPE is learned on the COCO dataset.

### Few-shot Image Classification

Base-to-new class generalization.Table 2 compares AAPE with two categories of prompt learning methods: 1) CoOp , CoCoOp , MaPLe , CLIPood , PromptSRC  and OGEN . These methods learn prompt vectors without using any text-based knowledge, but heavily rely on advanced optimization and regularization strategies to improve generalization. Our AAPE, on the other hand, outperforms by distilling the textual knowledge from LLMs. Notably, on average (across 11 datasets), AAPE achieves better classification accuracies than the previous SOTA approach OGEN for both base and new classes, setting a new SOTA mean accuracy 80.97% (vs. 80.34%). 2) ProGrad , KgCoOp  and LASP-V . These methods choose to align the learned prompts with hand-written ones like "a dark photo of a {class}", hence distilling knowledge from only basic, non-descriptive templates. This proves less effective than our LLM-derived natural language priors.

Recall our prompt learning method is built on top of the CuPL approach  to obtain the knowledge of GPT-3. Here we show both the LLM knowledge and our learning algorithm (that adaptively distills the knowledge) are indispensable. We first compare with the CuPL baseline that leverages LLM knowledge, but without any learning. Specifically, CuPL averages the LLM-generated prompts to perform zero-shot classification. This is contrasted with our method that _learans_ to aggregate the noisy prompts into AAPE which is then adapted for classification. Table 2 shows such "aggregate-and-adapt" learning method leads to significant gains over the learning-free CuPL, especially for base classes.

    &  &  &  \\   & CoOp & CoCoOp & MaPLe & CLIPood & PromptSRC & OGEN & ProGrad & KgCoOp & LASP-V & CuPL & AAPE \\   & Base & 82.69 & 80.47 & 82.28 & 83.90 & 84.26 & 84.17 & 82.48 & 80.73 & 83.18 & 74.31 & **84.72\({}_{ 0.18}\)** \\  & New & 63.22 & 71.69 & 75.14 & 74.50 & 76.10 & 76.86 & 70.75 & 73.60 & 76.11 & 75.25 & **77.54\({}_{ 0.29}\)** \\  & H & 71.66 & 75.83 & 78.55 & 78.90 & 79.97 & 80.34 & 76.16 & 77.00 & 79.48 & 74.78 & **80.97\({}_{ 0.19}\)** \\   & Base & 76.74 & 75.98 & 76.66 & 77.50 & 77.60 & 77.50 & 77.02 & 75.83 & 76.25 & 75.05 & **78.10\({}_{ 0.11}\)** \\  & New & 67.88 & 70.43 & 70.54 & 70.30 & 70.73 & 79.07 & 66.66 & 69.96 & 71.17 & 68.43 & **71.98\({}_{ 0.14}\)** \\  & H & 71.92 & 73.10 & 73.47 & 73.70 & 74.01 & 74.09 & 71.46 & 72.78 & 73.62 & 71.59 & **74.92\({}_{ 0.12}\)** \\   & Base & 98.00 & 97.96 & 97.74 & **98.70** & 98.10 & 98.32 & 98.02 & 97.72 & 98.17 & 98.24 & 98.34\({}_{ 0.07}\) \\  & New & 89.81 & 93.81 & 94.36 & 94.60 & 94.03 & 94.76 & 93.89 & 94.39 & 94.33 & 94.34 & **94.79\({}_{ 0.09}\)** \\  & H & 93.73 & 95.84 & 96.02 & **96.60** & 96.02 & 96.50 & 95.91 & 96.03 & 96.21 & 96.25 & 96.53\({}_{ 0.07}\) \\   & Base & 93.67 & 95.20 & 95.43 & 95.70 & 95.33 & 95.96 & 95.97 & 94.65 & 97.53 & 95.30 & **96.89\({}_{ 0.12}\)** \\  & New & 95.29 & 97.69 & 97.76 & 96.40 & 97.30 & 97.48 & 97.63 & 97.76 & 97.87 & 97.74 & **80.02\({}_{ 0.16}\)** \\  & H & 94.47 & 96.43 & 96.58 & 96.00 & 96.30 & 96.71 & 96.33 & 96.18 & 96.79 & 96.50 & **97.45\({}_{ 0.13}\)** \\   & Base & 78.12 & 70.49 & 72.94 & **78.60** & 78.27 & 77.59 & 77.68 & 71.76 & 75.23 & 68.88 & 77.51\({}_{ 0.39}\) \\  & New & 60.40 & 73.59 & 74.00 & 73.50 & 74.97 & 75.17 & 86.63 & 75.04 & 71.77 & 75.09 & **77.37\({}_{ 0.56}\)** \\  & H & 68.13 & 72.01 & 73.47 & 75.90 & 76.58 & 76.38 & 72.88 & 73.36 & 73.46 & 71.85 & **77.44\({}_{ 0.42}\)** \\   & Base & 97.60 & 94.87 & 95.92 & 93.50 & **98.07** & 97.34 & 95.54 & 95.00 & 97.17 & 77.79 & **97.81\({}_{ 0.19}\)** \\  & New & 59.67 & 71.75 & 72.46 & 74.50 & 76.50 & 77.67 & 71.87 & 74.73 & 73.53 & 78.10 & **78.75\({}_{ 0.31}\)** \\  & H & 74.06 & 81.71 & 82.56 & 82.90 & 85.95 & 86.39 & 82.03 & 83.65 & 83.71 & 77.94 & **87.25\({}_{ 0.21}\)** \\   & Base & 83.33 & 90.70 & 90.71 & 90.70 & 90.67 & 90.69 & 90.37 & 90.50 & 91.20 & 90.56 & **91.28\({}_{ 0.08}\)** \\  & New & 82.26 & 91.29 & 92.05 & 91.70 & 91.53 & 91.68 & 89.95 & 91.70 & 91.90 & 91.86 & **92.

Next we compare with a variant of our approach, with only task loss \(_{}\) but no \(_{}\) to distill LLM knowledge. This variant allows decoupling the contribution of LLM knowledge for prompt learning under a fair setting. Fig. 4 shows that using \(_{}\) leads to consistent gains for both seen and unseen classes from all the considered datasets. The distilled textual knowledge makes an especially large impact for those fine-grained actions (UCF101) and visual classes (StanfordCars, Flowers102 and FGVCircraft), which can be under-represented during both CLIP pretraining and prompt learning. Large gains are also observed for the special domains of textures (DTD) and satellite images (EuroSAT) with large distribution shift.

We further show how our distilled AAPE can disambiguate the classification task. Fig. 5 shows our AAPE augments the basic prompt template with descriptive details for each image, as exemplified by the reference prompts. Such input-specific details are often helpful for non-canonical views (_e.g._, hard cases on ImageNet) and OOD examples (_e.g._, on DTD and EuroSAT), where the visual cues are either ambiguous or barely visible (hence low similarity between the image and basic template). Eventually, we use a projection network to blend textual information from the template and AAPE, resulting in increased image-text similarity.

More comparisons.Table 8 in Appendix D includes **domain generalization** results. We see that AAPE is robust to different types of domain shift, outperforming prior works on 4 ImageNet variants. Table 9 and Appendix E further show the advantage of AAPE over two recent prompt learning methods ProText  and ArGue-N .

Figure 4: **Quantifying the role of LLM knowledge (distilled with \(_{}\)) in prompt learning. \(_{}\) consistently improves the base and new class accuracies on 11 classification datasets.**

Figure 5: **AAPE helps disambiguate the classification task. To highlight the textual knowledge encoded in AAPE, we show some reference prompts generated by GPT-3. For both the prompt template and AAPE (before concatenation and projection), we measure their Cosine similarity score with the image. Note the similarity score can be small when using a basic prompt template to match the “altar” class instance on ImageNet. Indeed, in this non-canonical image view, the altar is small and the whole scene can be classified as the easily confused class of “church”. Whereas AAPE is able to eliminate confusion by providing additional cues like altar “is a raised table” often at the location of “church”. This results in increased image-text similarity. Similarly, the textual cues from AAPE are helpful for the OOD examples in special domains of DTD and EuroSAT.**

**Ablation studies.** Fig. 6(a) shows that AAPE learning is data-efficient. We see AAPE consistently outperforms two prompt learners that do not benefit from LLM's text knowledge, _i.e._, AAPE w/o \(_{}\) and a similar baseline CoCoOp . Encouragingly, using 1 shot for AAPE is already far better than using 16 shots for the compared baselines. AAPE with varying shots is also consistently better than the LLM-based but learning-free approach CuPL . Fig. 6(b) further shows that AAPE scales better with the number of used prompts than CuPL. Note when we use only 1 prompt generated by each LLM-prompt, CuPL is even worse than the CLIP baseline. Whereas AAPE performs much better by distilling task-related information from the limited number of prompts. Finally, Fig. 6(c) shows the benefits of AAPE over CuPL in terms of the scaling performance with LLM model size.

### Image Captioning & VQA

Table 3 shows that AAPE can adapt to other vision-language tasks. Note AAPE is trained on COCO captions that describe complex scenes other than object-centric images. We once again find the benefits of distilling the text knowledge from image captions into AAPE. We observe consistent gains over the LiMBeR baseline, using either a trained or finetuned prompt generator (finetuned on NoCaps captioning and VQA2 tasks). More importantly, AAPE shows great generalization capability on both tasks. Its zero-shot performance is consistently better than that of LiMBeR and MAGMA, the latter of which finetunes both image and text networks. Fig. 9 in Appendix F exemplifies how AAPE can help on the captioning task, especially when the visual cues are ambiguous.

## 6 Conclusion

In this paper, we show the distillation of text-based knowledge into CLIP improves its downstream generalization. We propose a new prompt learning method where a prompt embedding AAPE is distilled from human- or LLM-generated natural language prompts. A prompt generator is trained to predict AAPE, which is shown to generalize to various vision-language tasks. We further demonstrate the benefits of AAPE for handling non-canonical examples as well as few-shot and OOD settings.

Limitations and future work.Given a sufficiently large set of image prompts, it is preferable to aggregate them into more than one prompt embeddings to model text diversity. However, learning to predict such an embedding set is hard on data-deficient tasks, since it will not only increase the computation cost but also incur performance degradation. For future work we hope to address this limitation by scaling up data to learn a diversified universal prompt generator. Another plan is to go beyond CLIP and apply AAPE learning to more vision-language models (contrastive or generative).

    &  &  \\   &  &  &  & 0 & 1 & 2 & 4 \\   & CIDEr-D & CLIP-S & Ref-S & In & Out & Near & All & CLIP-S & Ref-S & & \\  MAGMA  & 47.5 & 75.3 & 79.6 & 30.4 & 43.4 & 36.7 & 38.7 & 74.3 & 78.7 & 24.6 & 39.3 & 40.6 & 41.5 \\ LMBeR  & 54.9 & 76.2 & 80.4 & 34.3 & 48.4 & 41.6 & 43.9 & 74.7 & 79.4 & 33.3 & 39.9 & 40.8 & 40.3 \\  LMBeR+AAPE (train/finetune) & **57.8** & **80.8** & **83.6** & **42.1** & **49.8** & **44.2** & **47.3** & **77.6** & **81.7** & **36.5** & **42.7** & **44.2** & **45.9** \\ LMBeR+AAPE (zero-shot) & - & - & - & 36.1 & 48.8 & 42.9 & 45.1 & 76.3 & 80.3 & 34.9 & 41.0 & 42.3 & 43.1 \\   

Table 3: **Image captioning and VQA performance. Note our AAPE is learned on COCO dataset, and we show both its zero-shot and finetuned results on the two tasks with different testing datasets.**

Figure 6: **AAPE scales better with data (a-b) and LLM size (c) than alternatives. Experiments are conducted under the base-to-new generalization setting for few-shot classification. We measure the Harmonic mean (H) of base and new class accuracies. To adjust the total number of reference prompts per class to supervise AAPE learning, we vary the number of prompts generated by each LLM-prompt template. Four models of GPT-3 are considered: Ada, Babbage, Curie and Davinci.**