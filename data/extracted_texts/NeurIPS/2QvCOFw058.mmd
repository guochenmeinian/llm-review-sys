# Globally Q-linear Gauss-Newton Method for Overparameterized Non-convex Matrix Sensing

Xixi Jia\({}^{1}\)  Fangchen Feng\({}^{2}\)  Deyu Meng\({}^{3,4}\)  Defeng Sun\({}^{5}\)

\({}^{1}\)School of Mathematics and Statistics, Xidian University

\({}^{2}\)L2TI Laboratory, University Sorbonne Paris Nord

\({}^{3}\) School of Mathematics and Statistics, Xi'an Jiaotong University

\({}^{4}\) Macao Institute of Systems Engineering, Macau University of Science and Technology

\({}^{5}\) Department of Applied Mathematics, The Hong Kong Polytechnic University

hsijiazidian@gmail.com, fangchen.feng@univ-paris13.fr,

dymeng@xjtu.edu.cn, defeng.sun@polyu.edu.hk

Part of this work was completed while Xixi Jia was a research fellow in the Department of Applied Mathematics at The Hong Kong Polytechnic University.Corresponding author.

###### Abstract

This paper focuses on the optimization of overparameterized, non-convex low-rank matrix sensing (LRMS)--an essential component in contemporary statistics and machine learning. Recent years have witnessed significant breakthroughs in first-order methods, such as gradient descent, for tackling this non-convex optimization problem. However, the presence of numerous saddle points often prolongs the time required for gradient descent to overcome these obstacles. Moreover, overparameterization can markedly decelerate gradient descent methods, transitioning its convergence rate from linear to sub-linear. In this paper, we introduce an approximated Gauss-Newton (AGN) method for tackling the non-convex LRMS problem. Notably, AGN incurs a computational cost comparable to gradient descent per iteration but converges much faster without being slowed down by saddle points. We prove that, despite the non-convexity of the objective function, AGN achieves Q-linear convergence from random initialization to the global optimal solution. The global Q-linear convergence of AGN represents a substantial enhancement over the convergence of the existing methods for the overparameterized non-convex LRMS. The code for this paper is available at [https://github.com/hsijiazidian/AGN](https://github.com/hsijiazidian/AGN).

## 1 Introduction

Matrix sensing aims to recover an unknown low-rank matrix \(M^{n n}\) from its linear measurement \(=(M)\). Here each elements \(_{i}\) is defined as \(_{i}= A_{i},M\), with \(i=1,,m\), and \(()\) is a nearly isometric linear operator. It holds significance not only in practical applications but also in the realm of non-convex optimization . As the problem often involves finding the optimal solution of a non-convex minimization problem

\[_{U^{n d},V^{n d}}f(U,V):=\|(UV^{})-\|_{F}^{2}, \]

where \((M)=r n\), of particular interest is the overparameterized case where \(d>r\). The objective function \(f(U,V)\) to be minimized is **non-convex**, **non-smooth3** and meanwhile **lacks**

**coercivity**, presenting significant challenges in solving the optimization problem. Since the objective function exhibits certain key characteristics akin to the loss function of deep neural networks, problem (1) stands as a cornerstone in the study of more challenging non-convex problems, such as those encountered in deep learning . For further discussions, please refer to .

Recent years have witnessed significant progress in the study of this non-convex optimization problem.

**(a) Progress on gradient descent algorithms.** Existing works such as  demonstrate that the non-convex objective function \(f(U,V)\) possesses benign loss landscape, wherein all local minima are global, and concurrently, the Hessian exhibits negative eigenvalues at saddle points, allowing perturbed gradient descent algorithms to effectively escape them. To handle the non-smooth problem, prior studies  introduce a regularization term \(\|U^{}U-V^{}V\|_{F}^{2}\) to the objective function. This regularization ensures balance between the norms of \(U\) and \(V\).

Very recently, Ye and Du  make a breakthrough on low-rank matrix factorization (LRMF), a specific setting of the problem (1) (\(d=r,m\)), and prove that gradient descent, without perturbation and without the balance regularization on the objective function, converges at an R-linear rate to the global optimal solution of the non-convex problem from random initialization. Meanwhile, Stoger and Soltanolkotabi  study the global convergence of gradient descent for overparameterized (\(d>r\)) low-rank matrix sensing. However, overparameterization can significantly slow down gradient descent from achieving linear convergence to sub-linear rates, as analyzed in . Furthermore, Xiong et al.  proves that imbalanced initialization can expedite the convergence of gradient descent from sub-linear to linear rate. Nevertheless, gradient descent still requires a considerable amount of time to navigate away from saddle points, as discussed in Section 4. Additionally, the convergence rate of gradient descent is heavily reliant on the condition number of the matrix \(M\), rendering it inefficient for solving ill-conditioned non-convex optimization problems.

**(b) Progress on advanced algorithms.** Given these deficiencies of first-order gradient methods, it is intriguing and crucial to investigate how computationally efficient higher-order algorithms, behave on this non-convex problem. Previously, Liu et al.  introduces a Gauss-Newton type method for symmetric LRMF (with \(d=r\)), and proved that Gauss-Newton method converges Q-linearly fast to a _critical point_ of the non-convex optimization problem. Recently, Zilber and Nadle  prove that the Gauss-Newton method enjoys local quadratic convergence if the initialization lies within a small basin of attraction of the global optimal solution. All these works only guarantee local convergence and neglect the influence of saddle points on the convergence. _Global convergence of the Gauss-Newton method remains ambiguous._ Yue et al.  proves that the Newton method with cubic regularization converges quadratically fast from random initialization. However, their results are only applicable for symmetric MS with \(d=r\), and the computational cost of Newton method in  for LRMS is very high. Lee and Stoger  prove that for rank-one matrix sensing, alternating least square method converges to the global optimal solution at a linear rate from random initialization. However, it is uncertain whether the results of  are applicable to the \(r>1\) case as well as the challenging overparameterized scenario (\(d>r\)).

Another line of work deals with the deficiencies of gradient descent by incorporating preconditioning matrices into the gradient direction, as demonstrated by . Tanner and Wei  introduce a scaled alternating steepest descent method with diminishing step size and provides asymptotic convergence. Tong et al.  introduces ScaledGD and proves that, given a spectral initialization, ScaledGD converges at a linear rate to the global optimal solution of problem (1). Additionally, the convergence rate is independent of the condition number of \(M\). The works in  and  focus on the symmetric matrix sensing, extending the ScaledGD to the overparameterized case by introducing a damping factor \(\) to control the singularity of the preconditioning matrix. However, the damping factor \(\) can decelerate the ScaledGD from escaping the saddle regions and the global iteration complexity becomes \(( n)+(1/)\) as given in Table 1.

**Our contributions.** In this paper, we focus on the general model (1) which covers both symmetric matrix sensing (\(M\) is symmetric and positive semi-definite) and asymmetric matrix sensing (\(M\) is rectangular matrix), particularly with \(d>r\). Building upon the insights from , we use an approximated Gauss-Newton (AGN) method for solving the non-convex optimization problem

  Algorithm & init. & iteration complexity \\  GD  & random & \(^{*1}(^{*1})/^{*1}(^{*}/)\) \\ RecGD  & spectral & \((1/)\) \\ ScaledGD(\(\)) & random & \(( n)+(1/)\) \\ AGN & random & \((1/)\) \\  

Table 1: Comparisons of iteration complexity, with \(\) as the condition number of the \(n n\) matrix. ‘init.’ denotes initialization.

(1). Notably, in each iteration, AGN performs computations akin to gradient descent, yet it exhibits a Q-linear convergence rate towards the global optimal solution from random initialization, with global iteration complexity \((1/)\) as shown in Table 1 AGN. Moreover, the Q-linear factor is independent of the condition number of \(M\). Under certain conditions on the sensing operator \(()\), we can prove super-linear convergence of the AGN. This distinctive convergence property substantially enhances outcomes compared to existing methods in achieving global convergence, as shown in Table 1. Additionally, as a byproduct result, we establish that the prevailing preconditioned gradient descent methods are analogous to the Levenberg-Marquardt method within the Gauss-Newton framework.

To conclude, the contributions of this paper are as follows. First, we reformulate the symmetric and asymmetric matrix sensing in a unified way, based on which we design an approximated Gauss-Newton (AGN) method. We show that the existing preconditioned gradient descent algorithms, ScaledGD(\(\)) and PrecGD in [15; 21] respectively, correspond to a certain type of the Gauss-Newton method. Then we analyze the behavior of GD, Scaled(\(\))/PrecGD and AGN at \(\)-neighborhood of the saddle points. The saddle point analysis shows that AGN is not attracted to saddle points, unlike GD and PrecGD. Specifically, GD and Scaled(\(\))/PrecGD achieve an objective function decrease of \(o()\), while AGN achieves a significantly larger, \(\)-independent decrease \((1)\). Finally, we prove the global Q-linear convergence of AGN for overparameterized non-convex LRMS. This significantly improves over algorithms like ScaledGD(\(\)) and PrecGD, which achieve R-linear convergence but are hindered by saddle regions.

This paper is organized in the following ways. Section 2 provides an overview of related works. Section 3 outlines our AGN method for both symmetric MS and asymmetric MS, and explores the connections between AGN and Scaled(\(\))/PrecGD. Section 4 provides insights into the behavior of GD, ScaledGD, and AGN in the vicinity of saddle points. Section 5 outlines the main convergence result with a proof sketch, Section 6 presents experimental results, and Section 7 concludes the paper.

## 2 Related work

### Overparameterization in matrix sensing and beyond

Overparameterization significantly impacts the optimization of both the non-convex low-rank matrix recovery problem [9; 15; 19; 29; 30; 31] and deep learning [32; 33; 34; 35]. Specifically, [15; 29] analyzed that overparamterization can eliminate the spurious local minima of the non-convex low-rank matrix recovery problem. Meanwhile, since the exact rank parameter \(r\) is not predetermined, in real-world applications one often relies on a moderately higher rank \(d>r\), as elaborated in . Geyer et al.  study the solution uniqueness in the overparamterized low-rank matrix sensing. Additional works examining overparameterized low-rank models include, but are not limited to [37; 38; 39].

### Preconditioned/Scaled gradient descent

Preconditioned/Scaled gradient descent (ScaledGD) aims to enhance convergence by adjusting the gradient direction using preconditioning matrices, as specified in [15; 26; 27; 21; 40]. It resolves the non-convex optimization problem (1) by the following iteration

\[U_{t+1}=U_{t}-_{U}f(U_{t},V_{t})(V_{t}^{}V_{t})^{-1 },\\ V_{t+1}=V_{t}-_{V}f(U_{t},V_{t})(U_{t}^{}U_{t})^{-1}, \]

which corresponds to the methods in [27; 28; 40] for \(r=d\). The author in  updates \(U\) and \(V\) in an alternating manner. If \(U=V\) and \(d>r\), then the iteration becomes ScaledGD(\(\))/PrecGD

\[U_{t+1}=U_{t}-_{U}f(U_{t},U_{t})(U_{t}^{}U_{t}+_{t}I)^{-1}, \]

where \(_{t}>0\) can be either constant or time-varying, and the iteration corresponds to the methods in [15; 21]. ScaledGD(\(\))  and PrecGD  have been shown to achieve linear convergence to the global optimal solution, whether in a local context or in a global context. However, the parameter \(_{t}\) can degrade the global convergence, as shown in Fig. (2) where PrecGD struggles to escape saddle regions before achieving local linear convergence.

### Gauss-Newton method

The Gauss-Newton (GN) method is a widely recognized approach for nonlinear least-square

\[_{^{n}}()=\|()\|_{2}^{2}, \]where \(:^{n}^{m}\) is a nonlinear, twice continuously differentiable function. In each iteration \(t\), GN aims to solve least-squares of the linear approximation of \(()\) at point \(_{t}\)

\[_{t+1}=_{t}+_{t},\;_{t}=_{ ^{n}}()=\|(_{t})+J( {x}_{t})\|_{2}^{2}, \]

where \(J(_{t})=^{}(_{t})\) is the Jacobian of the nonlinear function \(()\) at \(_{t}\), and \(>0\) is the step-length. \(=1\) corresponds to the GN method, \(<1\) corresponds to the damped Gauss-Newton method. The \(_{t}\) is calculated by \(_{t}=-[J(_{t})^{}J(_{t})]^{-1}J(_{t})^{}( _{t})\), and \(J(_{t})^{}J(_{t})\) is an approximation of the Hessian \(H(_{t})=J(_{t})^{}J(_{t})+_{i=1}^{n}_{i}(_ {t})_{i}^{}(_{t})\) if the second-order term is small. The GN method generally has a local convergence guarantee, indicating its effectiveness primarily within the vicinity of a solution [22; 41; 23]. When applied to the LRMS problem (1), we will show in Section 3.3 that the AGN method is closely related to the ScaledGD(\(\))/PrecGD method.

## 3 Proposed method

In this section, we begin by unifying the formulation of both symmetric matrix sensing (where \(U=V\) and \(M\) is positive semi-definite) and asymmetric matrix sensing problems. Based on the corresponding nonlinear least-squares problem, we introduce our approximated Gauss-Newton (AGN) method. Then we prove that the proposed AGN is a descent method. Furthermore, when addressing symmetric matrix sensing, we explore two distinct parameterization settings and demonstrate that employing an asymmetric parameterization can significantly enhance convergence. At last, we give some discussions on the relation between ScaledGD(\(\))/PrecGD and the proposed AGN method.

### Approximated-Gauss-Newton (AGN) method for LRMS

We unify the formulation of symmetric and asymmetric LRMS into a single, simplified expression:

\[_{X^{2n d}}(X):=\|(PXX^{} Q)-\|_{2}^{2}, \]

where \(P=[I]^{n 2n}\), \(Q=\\ I^{2n n}\) and \(I^{n n}\) is the identity matrix. The case \(X=U\\ V\) corresponds to asymmetric matrix sensing, \(X=U\\ U\) corresponds to symmetric matrix sensing with \(U^{n n}\) and \(V^{n n}\). The function \((X)\) can be further rewritten as \((X)=\|(X)\|_{2}^{2}\), where \((X)=(PXX^{}Q)-\). For simplicity in notation, we denote \((X,Y)=(PXY^{}Q)\). By employing the Gauss-Newton framework as presented in Section 2.3, one can update the variable as \(X_{t+1}=X_{t}+(X_{t})\) where

\[(X_{t})=_{^{2n d}}\| (,X_{t})+(X_{t},)+(X_{t},X_{t})- \|_{2}^{2}. \]

However, as the Jacobian of the linear operator in the \(l_{2}\)-norm of Eq. (7) tends to be singular in our overparameterized setting (\(d>r\)), the Gauss-Newton method cannot be directly applied and one may consult for the Levenberg-Marquardt method. In this work, however, unlike the Levenberg-Marquardt method, we resolve the problem in Eq. (7) using the Gauss-Seidel method, whose advantages over the Levenberg-Marquardt method will be discussed in Section 3.3. Specifically, we update \(X_{t}\) by

\[X_{t+}=X_{t}+(X_{t}),\,(X_{t})= _{^{2n d}}\|(,X_{t})+ (X_{t},X_{t})-\|_{2}^{2}, \] \[X_{t+1}=X_{t+}+(X_{t+}),\, (X_{t+})=_{^{2n d}}\| (X_{t+},)+(X_{t+},X_{t+ {1}{2}})-\|_{2}^{2}.\]

The sub-problems in Eq. (8) are quadratic minimization problems that can generally be solved very easily. In our matrix sensing problem, leveraging the RIP condition, we can approximate \((X_{t})\) by

\[(X_{t})=^{+}_{^{2n d}} \|}(,X_{t})-^{*}((X_{t },X_{t})-)\|_{F}^{2}, \] \[(X_{t+})=^{+}_{ ^{2n d}}\|}(X_{t+},)- ^{*}((X_{t+},X_{t+})-)\|_{F }^{2},\]where \(}(,X_{t})=P X_{t}^{}Q\) and \(}(X_{t+},)=PX_{t+}^{}Q\), \(^{+}\) denotes the minimum norm solution as

\[&(X_{t})=P^{}^{*}( (X_{t},X_{t})-)Q^{}X_{t}(X_{t}^{}QQ^{}X_{t}^{ })^{},\\ &(X_{t+})=Q^{}[^{*}( (X_{t+},X_{t+})-)]^{}PX_{t+}(X_{t+}^{}PP^{}X_{t+})^{}, \]

which is a natural choice for degenerate least squares problem. \(\) denotes the Moore-Penrose-Pseudo inverse, and \(P^{}=P^{}\), \(Q^{}=Q\) in our context. Then the AGN becomes 4

\[X_{t+}=X_{t}-(X_{t}),\ \ X_{t+1}=X_{t+}- (X_{t+}). \]

The specifics of the AGN method are presented in Algorithm 1 in Appendix. We demonstrate that the solution in Eq. (10) renders the AGN in Eq. (11) with a constant step-size \(>0\) as a descent method.

**Lemma 1**.: (Descent lemma) _For asymmetric matrix sensing, as long as \(0< 2/(1+)\) and the Assumption 1 is satisfied, then there exists positive constant \(=(2-(1+)^{2})/2\) such that_

\[&(X_{t+})(X_{t})-\|}((X_{t}),X_{t})\|_{F}^{2},\\ &(X_{t+1})(X_{t+})-\|}( X_{t+},(X_{t+}))\|_{F}^{2}. \]

Lemma 1 suggests that AGN with a constant step-size is indeed a descent method for the overparameterized LRMS. In Section 5, we will prove that AGN converges globally at Q-linear rate.

The AGN method can be used not only for asymmetric MS but also for symmetric MS, as Eq. (6) offers a unified formulation for MS. While the application of AGN on symmetric MS will differ slightly from the asymmetric case.

### Symmetric matrix sensing

Now we consider symmetric matrix sensing, which is a special case of model (6) and the matrix \(M^{n n}\) is symmetric positive semi-definite (PSD). There are two different ways to deal with the symmetric case, depending on whether we constrain \(X\) (symmetric parameterization), where \(=\{Z|Z=U\\ U,U^{n d}\}^{2n d}\) or \(X^{2n d}\) (asymmetric parameterization).

**Setting 1: Symmetric parameterization.** In this case, the optimization variable \(X\) such that \((X,X^{})=(UU^{})\) for matrix \(U^{n d}\) and \(}(,X_{t})=[}(X_{t},)]^{}\), thus the subproblems in Eq. (9) become

\[(X_{t})=_{}\|}(,X_{t})-^{*}((X_{t},X_{t})-)\| _{F}^{2}, \]

where \(X_{t}\). The optimal solution to the problem in Eq. (13) is provided by the following lemma.

**Lemma 2**.: _Let \((X_{t})\) be the optimal solution of problem (9), then the optimal solution of problem (13) is \((X_{t})=P(X_{t})\\ P(X_{t})\)._

However, we find that if we constrain the search space to \(\) and use the update \((X_{t})\), the AGN with a constant step-size \(\) may not qualify as a descent method for our over-parameterized LRMS. Specifically, let \(X_{t+1}=X_{t}-(X_{t})\), where \(X_{t},(X_{t})\) and assume \(X_{t}=U_{t}\\ U_{t}\), we have

\[(X_{t+1})&=\|(X_{t},X_ {t})-(X_{t},(X_{t}))-((X_{t}),X_{t})+^{2}((X_{t}),(X_{t}))-\|_{2}^{2}\\ &=(X_{t})+^{2}\|(X_{t},(X_{t}))\| _{2}^{2}-\|}(X_{t},(X_{t}))\|_{F}^{2}+}{2}\|((X_{t}),(X_{t}))\|_{2}^ {2}\\ &-^{3}((X_{t}),X_{t}), ((X_{t}),(X_{t}))+^{2} (X_{t},X_{t}),((X_{t}), {}(X_{t})). \]Note that the term \(\|(X_{t},(X_{t}))\|_{2}^{2}(1+)\|_{t}\| ^{2}_{F}\), \(\|(X_{t},(X_{t}))\|_{F}^{2}\|_{t}\|_{F}^ {2}\) are all bounded and are closely related to \((X_{t})\), and \(_{t}=^{*}((U_{t}U_{t}^{})-)\). While the higher-order term w.r.t. \((X_{t})\)

\[\|((X_{t}),(X _{t}))\|_{2}^{2}&=\|(_{t}U_{t}(U_{t}^{ }U_{t})^{-2}U_{t}^{}_{t}^{})\|_{2}^{2}\\ &(1-)\|_{t}U_{t}(U_{t}^{}U_{t})^{-}(U_{t}^{}U_{t})^{-1}(U_{t}^{}U_{t})^{-}U_{t}^{} _{t}^{}\|_{F}^{2} \]

can be extremely large such that \((X_{t+1})(X_{t})\), as \(\|_{t}U_{t}(U_{t}^{}U_{t})^{-}\|_{F}^{2}\| _{t}\|_{F}^{2}\) is bounded while \(U_{t}^{}U_{t}\) tends to be singular in the over-parametrized case. Therefore, one cannot guarantee that the AGN method decreases with a constant step-size \(\), as illustrated by Fig. 3 AGN\({}_{ sym}\) in Section 6. How should we approach the symmetric matrix sensing problem using AGN? One possible strategy is to relax the constraint \(X\) to a larger search space \(X^{2n d}\).

**Setting 2: Asymmetric parameterization.** Despite \(M\) being a symmetric PSD matrix, one can still consider problem (6) with \(X^{2n d}\) instead of \(X\). Denote by \(X^{*}\) and \(X^{*}_{c}\) the optimal solution with \(X^{*}^{2n d}\) and \(X^{*}_{c}\) respectively, then it is easy to verify that \(PX^{*}X^{*}Q=PX^{*}_{c}X^{*}_{c}Q\) for symmetric matrix sensing. Therefore, one can readily apply AGN using Algorithm 1 to solve problem (6) with \(M\) being a symmetric PSD matrix. We will achieve the same convergence guarantee as in the case of asymmetric matrix sensing.

**Remark 1**.: The symmetric MS discussed in setting 1 is a specific instance of problem (6), involving significantly fewer intrinsic variables compared to the asymmetric case discussed in setting 2, the search space \(\) resides in a lower dimensional subspace of \(^{2n d}\). Meanwhile, one can also explore the optimal solution for the symmetric MS within the expanded space \(^{2n d}\) while maintaining the same minimum objective function value, as demonstrated in setting 2. These two approaches lead to significantly different optimization paths. From the above analysis, it is evident that different optimization paths demonstrate distinct decreasing properties in our over-parameterized LRMS problem. If we confine the optimization variable to \(\), then the AGN with a constant step-size may not function as a descent method5. If we extend the optimization variable to the entire \(^{2n d}\), we have a larger search space from which we can find a solution path that guarantees a significant decrease in the objective function. In Section 5, we will prove that in this case, AGN converges Q-linearly fast. These observations suggest that expanding the search space for a given optimization problem can lead to more efficient methods.

### Comparisons with related works

Of particular relevance to this work are ScaledGD(\(\))  and PrecGD , which focus on over-parameterized symmetric low-rank matrix sensing. While these preconditioned gradient descent methods are not easily applicable to the general asymmetric matrix sensing problem, as we will discuss in the appendix Section A.1.3. In this section, we demonstrate that these preconditioned gradient descent methods are instances of the Levenberg-Marquardt method (specifically, the Gauss-Newton method for singular least square problems) applied to the symmetric low-rank matrix sensing problem. However, the Levenberg-Marquardt method is a more general approach than the preconditioned gradient descent method, particularly for nonlinear least square problems.

Since ScaledGD(\(\))  and PrecGD  consider over-parameterized symmetric matrix sensing, we constraint \(X\) where \(\) is defined in Section 3.2. It can be easily verified that the ScaledGD(\(\))/PrecGD for problem (6) corresponds to

\[X_{t+1}=X_{t}-(X_{t},),\\ (X_{t},)=_{}\|}(,X_{t})-^{*}((X_{t},X_{t})-) \|_{F}^{2}+\|\|_{F}^{2}. \]

It is apparent from Eq. (16) that \(\) constrains the magnitude of the update \((X_{t},)\) to be small compared to Eq. (13), thus ensuring that the preconditioned gradient descent method exhibits monotonically decreasing behavior, as analyzed in  and . The Lemma 6 in  ensures that PrecGD is a descent method, as summarized by the following corollary 

**Corollary 1**.: _For symmetric matrix sensing, there exists positive constant \(_{X,}\) such that as long as \(0< 2/_{X,}\), the iteration given by Eq. (16) satisfies_

\[(X_{t+1})(X_{t})-}{2}\|}( (X_{t},),X_{t})\|_{F}^{2}, \]

_where \(_{X,}=(1+)[4+}(X_{t},X_{t})-M\| _{F}+4\|}((X_{t},),X_{t})\|_{F}}{_{ }(X_{t}^{}X_{t})+}+(}(( X_{t},),X_{t})\|_{F}}{_{}(X_{t}^{}X_{t})+})^{2}]\)._

Similar to the Lemma 1 of our AGN method, the value \(\|}((X_{t},),X_{t})\|_{F}^{2}\) plays very important role for the convergence of ScaledGD(\(\))/PrecGD. While we observe that the parameter \(\) can notably impede the progress of ScaledGD(\(\))/PrecGD in escaping the saddle point, as illustrated in Fig. 2 and discussed in Section 6. In Section 4, we will prove that when \(X_{t}\) is \(\)-close to the saddle points, \(\|}((X_{t},),X_{t})\|_{F}^{2}\) will be as small as \(o()\), which explains why ScaledGD(\(\))/PrecGD converges slowly near saddle points. While in contrast, even if \(X_{t}\) is \(\)-close to the saddle points, the value \(\|}((X_{t}),X_{t})\|_{F}^{2}\) in Eq. (12) is almost independent of \(\), thus, saddle points cannot impede the convergence of the AGN method, which is also demonstrated in the left subfigure of Fig. 2.

## 4 Saddle point analysis on the population risk

Saddle points are special critical points in non-convex optimization problem, contributing significantly to the global convergence analysis of gradient-based algorithms in non-convex optimization. Past researches  has demonstrated that gradient descent may encounter difficulties in navigating away from saddle points, while our empirical findings in Section 5 demonstrate that the proposed AGN does not experience slowdowns caused by saddle points. Therefore, it's quite intriguing and crucial to understand the behaviors of gradient descent and AGN in the vicinity of saddle points. To this end, we study the saddle points of the population risk of the problem (6).

The population risk6 associated with the objective function in Eq. (6) corresponds to the following non-convex matrix factorization problem:

\[_{X^{2n d}}\|PXX^{}Q-M\|_{2}^{2}. \]

The objective function corresponds to \(g(U,V)=\|UV^{}-M\|_{F}^{2},U^{n d},V ^{n d}\) which plays very important role in the saddle point analysis of Eq. (6). The saddle point of the non-convex objective \(g(U,V)\) is denoted by \((U_{s},V_{s})\), where the set \(\) is defined as follows:

\[=\{(U_{s},V_{s})|U_{s}V_{s}^{}=() ^{},M=^{},/ \}, \]

where \(M=^{}\) is the SVD of the matrix \(M\) and \(^{n r},^{n r},^{r r}\), \(\) is the set of mask operator7 and \(\) is the identity operator.

Note that the gradient norm \(\| g\|_{F}^{2}\) is intricately linked to the reduction of the objective function for gradient descent method. In our approach, the values \(\|}((X_{t}),X_{t})\|_{F}^{2}\) and \(\|}(X_{t},(X_{t}))\|_{F}^{2}\) in Eq. (12), which correspond to \(\|_{U}g(V^{}V)^{-}\|_{F}^{2}\) and \(\|_{V}g(U^{}U)^{-}\|_{F}^{2}\) respectively, are directly tied to the reduction observed in the AGN method. Correspondingly for symmetric matrix sensing, the values \(\|}((X_{t},),X_{t})\|_{F}^{2}\) in Eq. (17), which corresponds to \(\|_{U}g(U^{}U+ I)^{-}\|_{F}^{2}\), is related to the reduction of the non-convex objective by ScaledGD  and PrecGD . Now, we present the following theorem to describe the behavior of gradient descent, ScaledGD(\(\))/PrecGD and AGN in the vicinity of saddle points, by quantifying the values associated with their reductions in objective functions. For simplicity, we consider here \((M)=1\).

**Theorem 1**.: _Assume that \(M\) is rank-\(1\), the point \((,)\) with \(=U_{s}+ N_{u},=V_{s}+ N_{v}\) is at the vicinity of the saddle point \((U_{s},V_{s})\) and \(\) is sufficiently small, \(N_{u}\) and \(N_{v}\) are random Gaussian matrices that follow a standard normal distribution. Then with high probability we have the following results_

\[()\| g\|_{F}^{2}=o()e_{s}+o(^{2}), \]\[\| g_{}(^{})^{-}\|_{F}^{2}=(1)e_{s}+o(^{2}),\\ \| g_{}(^{})^{-}\|_{F}^{2}=(1)e _{s}+o(^{2}), \]

_where \(e_{s}=\|U_{s}V_{s}^{}-M\|_{F}^{2}\). Furthermore, by constraining \(M\) to be positive semi-definite and \(=\), \(U_{s}=V_{s}\) (for symmetric matrix sensing), for bounded constant \(c>0\), we have_

\[(())\| g_{}(^{}+ I)^{-}\|_{F}^{2}=(}{ ^{2}+/c})e_{s}+o(^{2}). \]

Theorem 1 indicates that when the optimization variable \((,)\) is \(\)-close to the saddle point of the non-convex objective function \(g(,)\), the norm of the gradient \(\| g\|_{F}^{2}\) at \((,)\) becomes as small as \(o()\). Hence, the convergence of gradient descent will be relatively slow, as shown in Fig. 2. Moreover, in Eq. (22), the value of \(/c\) is typically much larger than \(\). Consequently, the reduction achieved by ScaledGD(\(\))/PrecGD for symmetric matrix sensing is nearly identical to that of gradient descent \(o()\). In contrast, even if \((,)\) is \(\)-close to the saddle point, the reduction in the non-convex objective achieved by AGN in Eq. (21) is as significant as \((1)e_{s}+o()\), where \((1)e_{s}\) is larger than \(\) and is not dependent on \(\), thus ensuring that AGN achieves a substantial decrease in the objective function near the saddle points. We also plot the gradient norm of GD, PrecGD and AGN in Fig. 1 for solving problem (6). It can be seen from Fig. 1 that the gradient norm of AGN decreases linearly to zero. While the gradient norm of PrecGD suffers from ups and downs before it is smaller than about \(1 10^{-7}\). This indicates that PrecGD is attracted to saddle points but can quickly escape, depending on the value of \(\) as per Eq. (22). It becomes more challenging for GD to quickly escape all saddle points, which is due to Eq. (20). As shown in Fig. 1, GD's iterations encounter multiple saddle points before reaching the global minimum.

## 5 Global convergence analysis

We first recall the celebrated Restricted Isometry Property (RIP) , then we make some mild assumptions on the restricted isometry constant and the initialization of the variable \(X_{0}\).

### Assumptions and main result

**Assumption 1**.: _The operator \(()\) satisfies the rank-\(r+1\) RIP with constant \(_{r+1}:=\) for sufficiently small \(\)._

**Assumption 2**.: _Let \(X_{0}^{2n d}\) be random Gaussian with elements sampled from \((0,)\) with \( c_{0}\|\|_{2}/n\) for small \(c_{0}\) and the step size \(0< 2/(1+)\)._

Now we present our main result, which characterizes the global Q-linear convergence of the proposed AGN for the over-parameterized, non-convex, low-rank matrix sensing problem.

**Theorem 2** (Global Q-linear convergence).: _Under the Assumption 1 and Assumption 2. Let \(^{*}\) be the global minimal value of \((X)\) in Eq. (6) and \(X_{t}, t>0\) is generated by Algorithm 1, then there exists constants \(1>0\) such that_

\[(X_{t+1})-^{*} c_{q}[(X_{t})-^{*}], t>0, \]

_where \(c_{q}=(1-)<1\) and \(=2-(1+)^{2}\). Meanwhile, if \(=0,=1\), \(c_{q}\) becomes \(0\)._

Theorem 2 echoes the observation in the left subfigure of Fig. 2 that AGN converges rapidly from random initialization and does not become trapped in saddle regions. The convergence result of AGN differs significantly from existing methods like ScaledGD(\(\)) , PrecGD  and GD [19; 20], both empirically and theoretically. In the next section, we will present the sketch of our proof.

Figure 1: Illustration of the gradient norm for GD, PrecGD, and the proposed AGN, with the right subfigure showing a zoomed-in region of the left for iterations from 100 to 500.

### Proof sketch

The global Q-linear convergence of the AGN method relies on two conditions: **monotonically decreasing** (Lemma 1) and **decrease dominant** (Lemma 3). The monotonically decreasing condition ensures that the objective function decreases in each iteration, while the decrease dominant condition guarantees that the decrease in the function value is significantly larger than the distance between the current function value and the global minimum.

**Lemma 3** (Decrease dominant).: _Under Assumption 1 and 2, let \(X_{t}\) be updated by AGN method in Algorithm 1, then there exist \(_{t}^{1},_{t}^{2}\) and constant \(\) with \(1\{_{t}^{1},_{t}^{2}\}\{_{t}^{1},_{t}^{2}\} >0\) and for constant \(>0,1>_{c}>0\)such that_

\[&\|}((X_{t}),X_{t})\|_{F}^{2 }}{1+}_{t}^{1}[(X_{t})-^{*}],\\ &\|}(X_{t+},(X_{t+}))\|_{F}^{2}}{1+}_{t}^{2}[(X_{t+})-^{*}], \]

_where \((X_{t})\) and \((X_{t+})\) is from Eq. (10)._

Note that the linear operator \(()\) and the \(l_{2}\)-norm is unitarily invariant, therefore for simplicity we consider \(M\) to be a diagonal matrix \(^{n n}\) with \(r\) nonzero elements on the diagonal. Specifically one can simply write \(=^{}M\) where \(M=^{}\) is the SVD of matrix \(M\), as analyzed in  and . Let \(X_{t}=U_{t}\\ V_{t}\), then \(\|}((X_{t}),X_{t})\|_{F}^{2}=\|^{*}( (U_{t}V_{t}^{}-))V_{t}V_{t}^{}\|_{F}^{2}\) and correspondingly \((X_{t})-^{*}=\|(U_{t}V_{t}^{}-)\|_{F} ^{2}\). According to the following Lemma 4 and the RIP condition in Definition 1, to guarantee the inequality in Eq. (24), we need to ensure that \(\|(U_{t}V_{t}^{}-)V_{t}V_{t}^{}\|_{F}^{2}_{1}\|U_{t}V _{t}^{}-\|_{F}^{2}\) as presented by Lemma 5.

**Lemma 4**.: _Assume that the operator \(()\) satisfies the RIP condition in Definition 1 and Assumption 1 with constant \(\), for any \(U,V^{n d}\), \(^{n n}\) and \(Z^{n n}\), then we have that_

\[\|^{*}(UV^{}-)Z\|_{F}(1-_{c})\|(UV^ {}-)Z\|_{F} \]

_for some \(0<_{c}<1\)._

**Lemma 5**.: _Under Assumptions 1 and 2, if \(X_{t}, t>0\) is generated by the AGN method in Algorithm 1 and let \(X_{t}=U_{t}\\ V_{t}\), then there exist constant \(\), \(_{t}^{1},_{t}^{2}\) with \(\{_{t}^{1},_{t}^{2}\}\{_{t}^{1},_{t}^{2}\}>0\) such that_

\[&\|(U_{t}V_{t}^{}-)V_{t}V_{t}^{}\| _{F}^{2}_{t}^{1}\|U_{t}V_{t}^{}-\|_{F}^{2},\\ &\|(V_{t}U_{t+}^{}-)U_{t+}U_{t+ }^{}\|_{F}^{2}_{t}^{2}\|U_{t+}V_{t}^{ }-\|_{F}^{2}. \]

Theorem 2 can be proven readily by combining all of these lemmas. Please refer to the Appendix for a more detailed proof.

## 6 Numerical experiments

In this section, we conduct experiments to demonstrate the effectiveness of the proposed AGN method for solving the over-parameterized non-convex matrix sensing problem. We set the ground truth matrix \(M=U^{*}}^{}\), with \(U^{*}^{n r},V^{*}^{n r}\) random orthogonal matrices and \(\) is a diagonal matrix with condition number \(\). We set \(n=100,r=5,d=3r\) and the number of sensing matrices \(m=50nr\). All experiments were conducted using MATLAB on a MacBook Pro with a 2.4 GHz quad-core Intel Core i5 CPU and 8 GB of memory.

**Comparison with representative methods.** We compare AGN with GD  and PrecGD  on asymmetric over-parameterized matrix sensing.

Figure 2: Comparison of convergence for PrecGD, GD, and AGN across various condition numbers, with the right subfigure extending the left by iterating from \(300\) to \(1000\).

All the competing methods are initialized with random Gaussian matrix with zero mean the variance \(1/n\). We plot the training curves of the competing methods in Fig. 2, where the relative error is defined by \(\|U_{t}V_{t}^{}-M^{*}\|_{F}/\|M^{*}\|_{F}\). GD's slow convergence is evident as it struggles to escape saddle points, as shown in the saddle regions of Fig. 2. Meanwhile, GD's final convergence rate depends on the condition number \(\). Fig. 2 further shows that PrecGD's final convergence rate is independent of \(\), though it still progresses slowly in saddle regions. In contrast, the relative error of AGN decreases rapidly, and its convergence remains unaffected by saddle points, consistent with Theorem 2.

Meanwhile, we compare the computational time of GD , PrecGD , ScaledGD(\(\)) , and the proposed AGN on matrices of varying dimensions \(n n\) under different condition number \(\) in Table 2. It can be seen from Table 2 that AGN is significantly faster than the competing methods, particularly PrecGD and ScaledGD(\(\)), while vanilla gradient descent converges much more slowly.

**Asymmetric vs. symmetric parameterization of the symmetric MS.** We also conduct experiments to illustrate the differences between symmetric and asymmetric parameterization in symmetric matrix sensing, as discussed in subsection 3.2. As analyzed in section 3.2 setting 1, the matrix \(U_{t}^{}U_{t}\) tends to be singular in over-parameterized matrix sensing, causing \(\|((X_{t}),(X_{t}))\|_{2}^{2}\) to become extremely large, leading to \((X_{t+1})(X_{t})\), as shown in Eq. (14). Thus, we cannot guarantee that AGN in symmetric parameterization is a reliable descent method, as demonstrated by AGN\({}_{ sym}\) in Fig. 3. However, with asymmetric parameterization, as outlined in subsection 3.2 setting 2, we can ensure the linear convergence of the AGN method, also illustrated by AGN\({}_{ asym}\) in Fig. 3.

## 7 Conclusion

In this paper, we present an approximated Gauss-Newton (AGN) method for overparameterized non-convex low-rank matrix sensing problem. We demonstrate the close relationship between existing methods like ScaledGD(\(\)) and PrecGD, and the Levenberg-Marquardt method, which is a variant of the Gauss-Newton method. Through saddle point analysis, we partially explain why gradient descent, Scaled(\(\))/PrecGD may be slowed down by saddle points, whereas the proposed AGN achieves fast convergence. Finally, we prove that the proposed AGN achieves Q-linear convergence from random Gauss initialization for the non-convex optimization problem. Our findings highlight the efficacy of (approximate) second-order methods in non-convex optimization, particularly for structured problems like non-convex matrix sensing. Moreover, our results can be extended to more complex challenges, such as optimizing deep neural networks.

**Limitations:** While the AGN method converges quickly and is efficient for low-rank matrix sensing problems, it may be less effective than gradient descent for general problems without low-rank structure due to the need to solve a least-squares problem for the Gauss-Newton direction. Future work will explore approximate methods, like the conjugate gradient, to address this. Additionally, our current saddle point analysis focuses on a simple non-convex LRMS case with a zero RIP constant, which we will generalize to more complex scenarios with larger RIP constants.