# Single-cell Masked Autoencoder: An Accurate and Interpretable Automated Immunophenotyer

Jaesik Kim, Matei Ionita, Matthew Lee, Michelle McKeague, Ajinkya Pattekar,

Mark Painter, Joost Wagenaar, Van Truong, Dylan T. Norton, Divij Mathew,

Yonghyun Nam, Sokratis Apostolidis, Cynthia Clendenin, Patryk Orzechowski,

Sang-Hyuk Jung, Jakob Woerner, Yidi Huang, Nuala J. Meyer,

Allison R. Greenplate, Dokyoon Kim, E. John Wherry

Institute for Immunology and Immune Health

University of Pennsylvania

Philadelphia, PA 19104

{jaesik.kim, matei.ionita,

Allie.Greenplate, Dokyoon.Kim, wherry}@penmmedicine.upenn.edu

These authors contributed equallyCorresponding authors

###### Abstract

High-throughput single-cell cytometry data are crucial for understanding the immune system's role in diseases and treatment response. However, the prevailing methods used for analyzing cytometry data, specifically manual gating and clustering methods, have certain limitations with scalability, robustness, and accuracy. In this study, we propose a single-cell masked autoencoder (scMAE), which offers an automated solution for immunophenotyping tasks such as cell type prediction. Our model aims to preserve the cell type definitions designed by the user, making interpretation and cross-study comparisons more accessible. The scMAE model follows a pre-train and fine-tune paradigm. During pre-training, scMAE utilizes Masked Single-cell Modelling (MScM) to learn relationships between protein markers in immune cells without the need for prior labeling information. Subsequently, the scMAE is fine-tuned on multiple specialized tasks, using a smaller designated portion of labeled data. Through evaluation experiments, we demonstrated that the pre-trained scMAE overcomes limitations of manual gating and clustering methods, providing accurate and interpretable cellular immunophenotyping. The introduction of scMAE represents a significant advancement in immunology research, enabling prediction and interpretation of cellular-level in immune disease.

## 1 Introduction

High-throughput, single-cell protein expression data as acquired through flow and mass cytometry are essential to understanding the immune system's role in infectious diseases, autoimmune diseases, or cancer, and its response after treatment. Cytometry assays typically profile millions of cells from a biological sample, allowing scientists to quantify cell-type specific biomarkers, even for rare cell types. For example, we can see which cell populations are differentially abundant, or which proteins are differentially expressed between subject groups. Immune profiling maps the similarity and diversity of the immune landscape for all subjects and patients, supporting individual-level prediction and precision medicine in the clinic.

Currently, the predominant approach for analyzing cytometry data is manual gating: the application of sequential filters to bivariate plots of protein markers to focus the analysis on particular cellsubsets of interest . These bivariate plots visually represent the distribution of protein markers, allowing a human analyst to manually identify and select cells based on their prior knowledge of the marker distribution. However, manual gating has several serious drawbacks [15; 17]. First, it is time-consuming for panels larger than a dozen markers, as the number of 2D plots increases quadratically with the number of markers. Second, results from manual gating may be difficult to reproduce. Researchers' diverse gating strategies encompass distinct gating sequences, shapes, and boundaries which may affect robustness and replicability of cell subsets. The level of gating stringency also varies between individuals, contributing to inconsistencies in results. Third, manual gating workflows fail to use all information available in the data, because they only consider two dimensions at a time, rather than the full multivariate protein expression profile.

The ability to measure multiple protein markers simultaneously has led to high-dimensional data, prompting the development of automated analysis techniques, particularly unsupervised clustering methods like FlowSOM , PhenoGraph , Scaffold Maps , and X-shift . While clustering approaches address the shortcomings of manual gating and offer speed, they also come with their own set of constraints. First, although unsupervised clustering methods can detect variability in the data, they cannot distinguish between biological or technical origin. Consequently, clustering methods are sensitive to batch effects, data distribution shifts and non-specific binding of antibodies . Another challenge arises in cross-study comparisons, where slight changes in panel choice, dataset specifics, or stochastic elements can lead to discontinuous changes in cluster boundaries. For example, CD4 T cells might cluster differently based on memory subtype in one study and functional subtype in another, making a direct comparison challenging. To strike a balance between labor-intensive manual analysis and unpredictable unsupervised analysis, we focus on a combination of unsupervised and supervised learning to develop an automated method in this study. This has the advantage that our automated method can predict the immunophenotype of cells in future samples while using the same cell type ontology found in the training data.

In this study, we propose an accurate and interpretable automated immunophenotyper for single-cell cytometry data through _Masked Single-cell Modelling (MScM)_, which uses self-supervised pre-training techniques on single-cell cytometry data. During _MScM_, our model learns the relationships and dependencies between markers in immune cells by identifying expression patterns in the massive amount of data itself, without any additional labels. The pre-trained model can then be exported using a useful representation, giving it an advantage over using the original data in many downstream tasks. We show here that our model can overcome the limitations of manual gating and clustering methods. Our model **accurately** identifies complex cell types and offers **interpretability** on which protein markers it paid attention to when predicting targets. We will also demonstrate additional properties such as scalability and reproducibility. While several pre-trained models for single-cell RNA sequencing (scRNA-seq) data have been published in similar approachs [20; 8; 24; 7; 6], to our best knowledge, this is the first model pre-trained using single-cell cytometry data. To the best of our knowledge, our model is the first pre-trained model using single-cell cytometry data. This approach to immunophenotyping will have the potential to advance the field of immunology by extending it to predict and explain the cellular-level and individual-level phenotypes of various immune diseases.

## 2 Single-cell Masked Autoencoder (scMAE) Algorithm

We propose scMAE, a single-cell Masked Autoencoder model that constructs and employs latent embeddings of single-cell cytometry data to obtain state-of-the-art performance on several cell-level tasks. scMAE is built upon a Masked Autoencoder (MAE)  backbone structure, consisting of stacked transformer blocks in both encoder and decoder. Drawing from established practices in the domains of computer vision and natural language processing, scMAE is trained in two stages: self-supervised pre-training and supervised fine-tuning (Figure 0(a)). The main advantage of this approach is leveraging large-scale, easily obtainable unlabeled data in the first stage, while requiring smaller amounts of labor-intensive labeled data in the second stage. During pre-training, a random subset of the protein expression data is masked and fed to an encoder which produces latent embeddings of the masked data. In turn, these embeddings are fed to a decoder that attempts to reconstruct the unmasked, original data (Figure 0(b), Figure P.1). The encoder-decoder system learns to optimize the embeddings to minimize reconstruction error. The true goal of pre-training is to obtain informative embeddings of the data, and the reconstruction of masked data allows the model to accomplish this goal even in the absence of any ground truth labels. During the second fine-tuning stage, the full protein expression data without masking is used to generate latent cell representations through the pre-trained encoder in the first stage. This can then be used for several downstream tasks, which may or may not require labeled data. Cell representations generated by the pre-trained encoder can be used for unsupervised tasks or plugged into another classifier to solve tasks through supervised fine-tuning. Specifically, we tested on two cell-level tasks using the pre-trained scMAE: cell type prediction, and imputation.

### Masked Single-cell Modeling (MScM)

scMAE learns to maximize

\[P(V_{i,masked}|V_{i,unmasked},_{unmasked})\] (1)

where \(i\) indexes cells, \(V_{i,masked}^{r p 1}\) denotes masked protein expressions of cell \(i\), and \(_{masked}^{r p(d-1)}\) denotes masked protein embeddings after masking. \(r\) is a masking ratio, \(p\) is the number of proteins in the data, and \(d\) is a hidden dimension size. Likewise,

Figure 1: (a) Overview of Single-cell Masked Autoencoder (scMAE). In the pre-training step, protein expression data is randomly masked. Unmasked protein expressions are concatenated with its learnable protein embeddings and inputted into the encoder. The encoder generates unmasked latent representations, and they are combined with the learnable mask embeddings and fed to the decoder for reconstruction of the masked values. In the fine-tuning step, the pre-trained encoder produces latent representations for cells, facilitating cell-level downstream tasks. (b) From left to right, masked, imputed (reconstructed), and original data. Each row represents a marker protein, and each column represents a randomly sampled cell. The original data was 25% randomly masked, and those regions are colored white in the masked data. The masked regions are reconstructed accurately through scMAE.

\(^{(1-r) p 1}\) denotes unmasked protein expressions of cell \(i\), and \(_{unmasked}^{(1-r) p(d-1)}\) denotes unmasked protein embeddings.

The encoder (\(f_{e}\)) generates a latent representation of the cell. The unmasked latent representation \(_{i,unmasked}^{(1-r) p d}\) of cell \(i\) is defined as the following,

\[_{i,unmasked}=f_{e}((_{unmasked} V_{i,unmasked })+_{unmasked}),\] (2)

where \(_{unmasked}^{(1-r) p d}\) is sine-cosine positional embeddings for masked proteins. The idea of the concatenation (\(\)) of protein embeddings with expression values was inspired from MET .

The decoder (\(f_{d}\)) reconstructs the masked values as following,

\[_{i,masked}=f_{d}((_{i,unmasked})+ )\] (3)

Let \(M\) denote a learnable mask token embedding represented as a row vector \(M^{1 d}\). We construct a matrix \(\) by stacking this vector \(rp\) times, such that the resulting matrix \(\) has dimensions \((r p d)\). \(^{p d}\) are sine-cosine positional embeddings. To calculate the reconstruction loss, we use mean square error (MSE) loss for all cells,

\[Loss=_{i}MSE(_{i,masked},V_{i,masked})\] (4)

### Datasets

We analyzed Cytometry by time of flight (CyTOF) data originating from three distinct COVID-19 studies conducted at the University of Pennsylvania. These data were acquired with a 30-marker panel and are referred to as the Acute dataset, Vaccine dataset, and iSPY dataset. The Acute dataset includes 6.5M cells from the 26 individual single-cell cytometry files, corresponding to 13 COVID-19 patients and 13 healthy individuals. The Vaccine dataset was obtained longitudinally from individuals before and after vaccination for SARS-CoV-2. This dataset is composed of 36.7M cells across 150 files, measuring cells from 44 individuals at 4 timepoints. Lastly, the iSPY dataset was obtained longitudinally from 42 COVID-19 infected individuals at the time of admission and after one week of treatment. It includes 11.9M cells from 56 measurements. We used the Acute dataset for pre-training and used all the three datasets in the downstream evaluations. Each of the datasets underwent a standard manual cleanup procedure, which involves removing aggregates, debris, doublets, beads, and dead cells from the data (see Appendix B).

## 3 scMAE is an Accurate Cell Immunophenotyper

Cell type annotation is the primary outcome of manual gating and clustering methods. Our model can perform automated cell type prediction on single cell datasets by fine-tuning the model with cell type labels. A total of 46 cell types obtained for each cell from manual gating were used as ground truth labels (Figure P.2). We used 60% of the Vaccine data for training, 20% as validation and the remaining 20% as an internal test set. We used the iSPY dataset and the Acute dataset as external sets (External set 1 and 2, respectively). We compared scMAE with a gradient boosting decision tree (GBDT) , a fully connected deep neural network (DNN), and a convolutional neural network (CNN) (see Appendix E) as well as cytometry-specific analysis methods: static gating and unsupervised clustering with FlowSOM.

Static gating is a baseline method which involves applying the filters manually defined in the training data to the testing datasets, without adjustments for inter-sample variability (Figure P.3). This is equivalent to manually constructing a decision tree and then applying it on the testing data. The other supervised models used here can be seen as refinements of this idea: they attempt to learn a more robust encoding of the gating information, by using multivariate rather than bivariate expression patterns. In the case of scMAE, it uses masking in the pre-training stage. Alongside the supervised classification methods, we include FlowSOM, a popular unsupervised clustering method for cytometry. To match our supervised paradigm, we add an inference mode to FlowSOM by mapping each unseen test datapoint to the nearest SOM node (see Appendix E).

Since most of the cells, over 60%, are Neutrophils, an imbalanced cell type distribution, rather than using Accuracy as an evaluation metric, we used Balanced accuracy (Bacc) to give a fair assessment of imbalanced label. The experimental results showed consistently high Bacc on both internal test sets and two external sets (Figure 2a). The internal test set showed a 93.1% Bacc, while the external sets showed 82.5% and 81.0% Bacc, respectively. When we looked at performance by cell type, we found that our model is more accurate than others for most cell types (Figure P.4). In addition, the fine-tuned scMAE from the pre-trained outperformed the scMAE from scratch (non-pre-trained), which demonstrates the benefit of pre-training (Table O.1).

Our model performed particularly well on rare cell types. Accurate prediction of rare cell types is difficult because it is easy for a model to be trained with a bias toward more frequent cell types. However, when comparing performance on cells with a frequency of less than 0.1% in Figure 2b, both internal test set and external sets show more accurate predictions for rare cell types than the comparison models in most cases.

FlowSOM scores lower on our accuracy metrics, as expected, because it makes no use of the training labels. We included it in the comparison to illustrate one important pitfall of unsupervised analysis: it uncovers true variability in the data, which nonetheless may not be biologically interesting (for example, splitting the dominant population of neutrophils into 6 clusters, based on non-specific binding of anti-CD3 or anti-TCRgd), while missing subtle but biologically meaningful differences (for example, differences between EM1, EM2, EM3 T cell phenotypes, based on CD27 and CCR7 expression).

These results show that our model is robust to technical variation between datasets, even when applied to the analysis of datasets derived and processed separately. For example, the cell immunophenotyping model was trained on the Vaccine dataset, run on frozen samples from healthy subjects in 2021, and it performed better than all other methods on the Acute dataset, run on fresh samples from subjects with acute COVID in 2020.

Figure 2: (a) Model comparisons in the cell type prediction. (b) Accuracy of cell type predictions for 5 abundant and 15 rare cell types. Deep neural nets (DNN) denotes the fully-connected neural network proposed by Cheng, L. _et al._ and Li, H. _et al._ for cytometry data analysis. Convolution neural nets (CNN) denotes a model architecture that removes only pooling layer from the CNN proposed by Hu. Z _et al._ for cytomegaloviruslovirus (CMV) classification. (c) Few-shot learning performance for cell type prediction. Each green dashed line represents the performance of the full fine-tuned scMAE when used all available training set, reported in the (a).

scMAE is a Few-shot Learner

Unlike full fine-tuning, few-shot learning trains a model with a limited amount of training data. _N_-shot uses only \(N\) samples for each class in the classification problem. A pre-trained large language model trained by self-supervised learning is known as a good few-shot learner . Similarly, our model was tested on a cell type prediction task for a few-shot learning setting. We ran 5-shot, 10-shot, 15-shot and 20-shot experiments. Training, validation, testing, and external testing sets are the same as in the previous cell type prediction tasks.

As expected, the pretrained scMAE approaches the performance of training with the full training set as the number of N-shots increases (Figure 2c). On the other hand, since the scMAE from scratch (non-pre-trained) has many parameters and no pre-trained information, we can see that it does not learn at all with small samples. It is worth noting that GBDT also performed reasonably well, but scMAE outperformed it based on the pre-trained knowledge. This shows that our pre-trained model can learn in the right direction even when there is very little labeled data for the new downstream task.

## 5 scMAE Enhances Regression Imputation

Current technology for flow and mass cytometry only allows a few dozen markers, and sometimes cost considerations may reduce the number even further. Traditionally, cytometry technologies were used by immunologists to answer very specific experimental questions which explains why a limited panel of markers and manual gating were sufficient at the time. However, it would be helpful to exploit high-dimensional patterns of protein expression to predict measurements from large panel sizes using only a smaller, cheaper panel. To investigate the feasibility of this, Becht, E. _et al._ proposed Infinity Flow, applying a Gradient Boosting tree model  to impute the expression of over 300 markers using only 15. To test whether our cell latent representations can enhance regression imputation, we conducted experiments in which we masked 7 markers, including those commonly associated with memory subsets in T cells (CD27, CD28, CD45RA, CD45RO, CD127, CD197). Then we used the remaining information to predict the masked marker expressions using Infinity Flow and the scMAE with imputation supervised finetuning. We used the Acute data for training, and the Vaccine dataset and iSPY dataset as external sets (External set 1 and 2, respectively). R-squared is used as an evaluation metric.

The scMAE achieved moderate imputation performance (0.2-0.6 R-squared, Figure 3a), despite only having access to 23 markers not known to be predictive of T cell memory states and their associated masked markers. Overall, our method showed improvement over Infinity Flow for five of the seven markers. Moreover, scMAE learned more than just patterns of constitutively expressed proteins, such as CD45RA in NK cells and CD45RO in neutrophils. Correlations between true and predicted values are high even when restricted to T cells, or to CD27 expression in B cells (Figure 3b, Figure P.5,P.6,P.7). This suggests that our method infers information about T cell and B cell memory states, even in the absence of the standard memory markers.

## 6 scMAE is an Interpretable Immunophenotyper

A multi-head self-attention of the transformer blocks in scMAE enables interpretable predictions for downstream tasks. Attention scores show which marker information and relationships are important to the prediction tasks. High attention score means that marker information is used a lot from the other markers. We first measured the attention score of each feature for each cell type in cell type prediction. (Figure 4a) Notably, the marker with the highest attention score in all cell types is CD45, which distinguishes between the two main immune cell lineages: granulocytes and mononuclear cells. Aside from this, most markers were highly attended in cell types in which they are highly expressed: for example, CD19 in B cells, CD123 in basophils and pDCs, CD294 in basophils and eosinophils.

Similarly, we measured the attention score of 23 markers for each cell type used to predict the expression of 7 markers in the Imputation task (Figure 4b). For cell types where the masked markers are either constitutively expressed or constitutively not expressed, the model mostly attended to the available markers which determine the cell type (CD294, CD66b, CD45 for eosinophils; CD16, CD45 and, interestingly, HLA-DR for neutrophils). In the case of T cells, where knowing the cell type is insufficient for predicting expression of the masked proteins, the model attended to the T cell marker protein CD3, but also to CD45 and HLA-DR, both of which are shown to be negatively correlated to CD45RA (Figure P.11).

This cell type-specific attention score shows a consistent pattern with external datasets (Figure P.8). In addition, the variance of attention scores between samples is not significant (Figure P.9,P.10). Overall, scMAE attention scores help both to confirm that known marker proteins were used by the model to make predictions, and to discover possibly unexpected correlations, such as those between CD45, HLA-DR and CD45RA.

Figure 3: (a) R-squared comparison between Infinity Flow and scMAE in the imputation task. Total 7 markers were masked and predicted by the two models. (b) Plots of true expression and predicted expression for each marker in the external set (Vaccine dataset). The dashed line represents the ideal relationship as a reference line to assess the performance.

## 7 Conclusion

In this manuscript, we introduced scMAE, a masked autoencoder model which builds latent embeddings of single-cell cytometry data and uses them to achieve good performance across a range of cell-level tasks. scMAE employs a training and inference paradigm that enhances scalability and reproducibility, outperforming alternative methods in making inferences on new datasets. Pre-training scMAE models with limited label information leads to improved performance, faster convergence, and stability, with potential for even greater gains by pre-training on larger and more diverse datasets. Especially, the fine-tuned scMAE is as accurate as manual gating, with the labor-free advantages of automated analysis. To the best of our knowledge, scMAE is the first such model which specializes on cytometry data. Our results are a proof of concept for applying a combination of unsupervised and supervised analysis in the training-inference paradigm to multiple cytometry datasets that use the same panel. The promise of this approach is that it generalizes easily to thousands of samples across multiple studies, providing robust and interpretable results while minimizing manual analysis.