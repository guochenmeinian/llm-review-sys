# Optimal Unbiased Randomizers for

Regression with Label Differential Privacy

 Ashwinkumar Badanidiyuru

Google

Mountain View, CA

&Badih Ghazi

Google Research

Mountain View, CA

&Pritish Kamath

Google Research

Mountain View, CA

&Ravi Kumar

Google Research

Mountain View, CA

&Ethan Leeman

Google

Cambridge, MA

&Pasin Manurangsi

Google Research

Bangkok, Thailand

&Avinash V Varadarajan

Google

Mountain View, CA

&Chiyuan Zhang

Google Research

Mountain View, CA

###### Abstract

We propose a new family of label randomizers for training _regression_ models under the constraint of label differential privacy (DP). In particular, we leverage the trade-offs between bias and variance to construct better label randomizers depending on a privately estimated prior distribution over the labels. We demonstrate that these randomizers achieve state-of-the-art privacy-utility trade-offs on several datasets, highlighting the importance of reducing bias when training neural networks with label DP. We also provide theoretical results shedding light on the structural properties of the optimal unbiased randomizers.

## 1 Introduction

Differential privacy (DP)  has gained significant importance in recent years as a mathematically sound metric for rigorously quantifying the potential disclosure of personal user information through ML models . DP guarantees that the model generated by the training process remains statistically indistinguishable, even if the data contributed by any individual user to the training dataset is modified arbitrarily.

In certain scenarios, the training _features_ of a specific example are already accessible to potential adversaries, while only the training _label_ is considered sensitive. For instance, within the context of digital advertising, it is typical to train conversion models that aim to predict whether a user, who interacts with an advertisement on a publisher's website, will ultimately make a purchase of the advertised item on the advertiser's website. The conversion label, which indicates whether the user completed the purchase or not, is not initially known to the publisher website and is considered sensitive information that spans multiple websites.1 This particular setting, referred to as label DP, was initially studied in the work of . Subsequently, it has garnered attention in various recent works, such as .

For regression objectives (such as the squared loss and the Poisson log loss), the state-of-the-art is given by the prior work of . Their algorithm works in the so-called _features-oblivious_ setting (Figure 1), which consists of a _features party_ that has access to all the features, and a _labels party_ that has access to all the labels. The labels party applies a mechanism that is DP with respect to the labels in order to produce the message \(M()\) that it sends to the features party, who then uses thefeatures along with the incoming message in order to train the model (which as a consequence would also satisfy label DP). It proceeds by using part of the privacy budget in order to estimate a prior over the labels, and then constructs a randomizer optimizing the "noisy label loss" (see Definition4) with respect to this prior.

In this work, we introduce a novel mechanism with bias-limiting constraints, motivated by the theory of bias-variance trade-offs. We show that while these constraints lead to significantly higher noisy label loss, the models trained on the privatized labels performs surprisingly well, achieving the state-of-the-art utility-privacy trade-off on multiple real world datasets.

Organization.Section2 provides some background definitions related to (label-)DP and learning. In Section3, we present the new label DP randomizers obtained by imposing unbiasedness constraints. Section4 provides an experimental evaluation demonstrating that our method achieves state-of-the-art privacy-utility trade-offs across multiple datasets. We provide some related work in Section5. We conclude with some future directions in Section6. All missing proofs along with additional related work, as well as details on the experimental evaluation are provided in the Appendix.

## 2 Preliminaries

Let \(\) be an unknown distribution on \(\). We consider the regression setting where \(\); let \(\) denote the marginal distribution of \(\) on \(\). In supervised learning, we have a set \(\{(x,y)\}\) of examples drawn from \(\). The goal is to learn a predictor \(f_{}\) to minimize the expected loss \(_{}(f_{}):=_{(x,y)}\, (f_{}(x),y)\), for some loss function \(:\). Some commonly-used loss functions used in regression tasks are the _Poisson log loss_\(_{}(,y):=-y()\) and the _squared loss_\(_{}(,y):=(-y)^{2}\).

We recall the definition of DP; for more background, see the book of Dwork and Roth .

**Definition 1** (Dp; ).: Let \(_{>0}\). A randomized algorithm \(\) is said to be _\(\)-differentially private_ (denoted \(\)-DP) if for any two "adjacent" input datasets \(X\) and \(X^{}\), and any measurable subset \(S\) of outputs of \(\), it holds that \([(X) S] e^{}[(X^{})  S]\).

In the context of supervised learning, an algorithm produces a model as its output, while the labeled training set serves as the input. Two inputs are considered "adjacent" if they differ on a single training example. This concept of adjacency is intended to safeguard both the features and the label of any individual example. However, in certain scenarios, protecting the features may either be unnecessary or infeasible, and the focus is solely on protecting the labels. This leads to the following definition:

**Definition 2** (Label DP; ).: A randomized training algorithm \(\) is said to be _\(\)-label differentially private_ (denoted \(\)-LabelDP) if it is \(\)-DP when two input datasets are "adjacent" if they differ on the label of a single training example.

We recall the notion of feature-oblivious label DP . In this scenario, there are two parties: the _features party_ and the _labels party_. The features party has the sequence \((x_{i})_{i=1}^{n}\) of all feature vectors across the \(n\) users; the labels party has the sequence \((y_{i})_{i=1}^{n}\) of the corresponding labels. The labels party sends a single (possibly randomized) message \(M(y_{1},,y_{n})\) to the features party. Based on its input and the received message, the features party generates an ML model as its output.

**Definition 3** (Feature-Oblivious Label DP ).: In the above scenario, the output of the features party satisfies _feature-oblivious_\(\)-LabelDP if the message \(M(y_{1},,y_{n})\) is \(\)-DP where two inputs are considered "adjacent" if they differ on a single \(y_{i}\).

Figure 1: Feature-oblivious label DP model training studied in .

Label-DP Randomizers

A standard recipe for learning with label DP is to: (i) compute noisy labels using a local DP randomizer \(\) and (ii) use a learning algorithm on the dataset with noisy labels. Many of the baseline algorithms that we consider follow this recipe, through different ways of generating noisy labels, such as, (a) (continuous/discrete) Laplace mechanism, (b) (continuous/discrete) staircase mechanism, etc. (see Appendix F for formal definitions). Prior work by  argues that intuitively such a learning algorithm will be most effective when the noisy label "mostly agrees" with the true label. This was formalized in the goal of minimizing the _noisy label loss_, which is the expected loss between the true label and the noisy label, for the true label drawn from a prior distribution, for some loss function \(\).

**Definition 4** (Noisy label loss).: For a loss function \(g:\), the _noisy label loss_ of a randomizer \(\) with respect to prior \(\) is \((;):=_{y, (y)}\,g(,y)\), where \(\) is the noisy label generated by \(\) on input \(y\).

This was motivated by the triangle inequality [6, Eq. (1)], namely,

\[}_{(x,y)}(f_{}(x),y)\ \ }_{y\\ (y)}(,y)\ +\ }_{(x,y)\\ (y)}(f_{}(x),)\,.\] (1)

The work of  focused on optimizing noisy label loss; they showed that the optimal randomizer takes the form of "Randomized-Response on Bins" (see Appendix F for more details). However, if we notice carefully, the noisy label loss does not depend on the number of examples, so the RHS of (1) does not go to zero as the number of examples goes to infinity, even though we would like the excess population loss of the learnt predictor to asymptotically go to zero. In this paper, we provide an alternative insight into the goal of minimizing the noisy label loss.

For any distribution \(\) over \(\), and any label randomizer \(\) mapping \(\) to \(}\), let \(_{}\) be the distribution over \(}\) sampled as \((x,(y))\) for \((x,y)\). The _Bayes optimal predictor_ for any distribution \(\) over \(\) w.r.t. loss \(\), is a predictor \(f_{}^{*}:\) that minimizes \(_{}(f)\), which roughly corresponds to the best predictor we could hope to learn with unbounded number of samples from \(\). We show that, when training with a loss from a broad family that includes \(_{}\) and \(_{}\), the Bayes optimal predictor is preserved after applying the label randomizer iff \([(y)]=y\) holds for all \(y\) (i.e., the randomizer is _unbiased_).

**Theorem 5**.: _Suppose \(\) is a loss function such that for all distributions \(\) over \(\), the minimizer \(_{*}:=_{}_{y}( {y},y)\) exists and is given as \(_{*}=_{y}[y]\). Then the following are equivalent:_

* \([(y)]=y\) _holds for all_ \(y\)_,_
* _For all distributions_ \(\) _over_ \(\)_, it holds that_ \(f_{}^{*}=f_{_{}}^{*}\)_._

The main observation is that \(f_{}^{*}(x_{0})=[y x=x_{0}]\) with probability \(1\) over \(x_{0}\), and similarly, \(f_{_{}}^{*}(x_{0})=[(y) x=x_{0}]\) with probability \(1\) over \(x_{0}\). We defer the full proof to Appendix A, but for now we demonstrate a simple example where a predictor learned using noisy labels produced by the optimal \(\)-on-\(\) randomizer can in fact be sub-optimal. Let \(=\{,\}\) and \(=\{0,1,2\}\) and the distribution \(\) be defined in Table 1. The Bayes optimal predictor for \(\) w.r.t. \(_{}\) (or even \(_{}\)) is given as \(f_{}^{*}()=0.4\) and \(f_{}^{*}()=0.7\). On the other hand, the optimal \(=\)-on-\(_{e}^{}\) randomizer for \(=0.5\) (see Appendix F for notation), corresponds to the map \((0) 0.396\) and \((1)=(2) 0.720\). The Bayes optimal predictor for \(_{}\) is given as \(f_{_{}}^{*}()=0.542\) and \(f_{_{}}^{*}()=0.558\). The squared loss of these predictors are given as \(_{}(f_{}^{*})=2.625\) and \(_{}(f_{_{}}^{*})=2.726\), the latter being sub-optimal.

Thus, preserving the Bayes optimal predictor is a desirable property of a label randomizer, as any learning algorithm that converges to the optimal predictor can also be trained on the noisy labels and will approach the predictor for the original distribution.

Additionally, we can relate the property \([(y)]=y\) to the unbiasedness of the gradients obtained in SGD. First, we observe that the gradient with respect to the parameters for \(_{}\) and \(_{}\) are affine in \(y\), namely,

\[_{}_{}(f_{}(x),y)=(f_{}(x)-y)_{ }f_{}(x)\,,_{}_{}(f_{}(x),y)=( 1-y/f_{}(x))_{}f_{}(x).\]Thus, the error in the gradient estimate when using the noisy label \(\) instead of \(y\) is given as

\[_{}_{}(f_{}(x),)-_{ }_{}(f_{}(x),y) = (y-)_{}f_{}(x),\] \[_{}_{}(f_{}(x),)-_{ }_{}(f_{}(x),y) = (y-)f_{}(x)}{f_{}(x)}.\]

Let \(S=\{(x_{1},y_{1}),,(x_{b},y_{b})\}\) be a mini-batch of examples and let \(=\{(x_{1},_{1}),,(x_{b},_{b})\}\) be the mini-batch with noisy labels as returned by the randomizer \(\). Also consider the dataset with expected noisy labels \(=\{(x_{1},_{1}),,(x_{b},_{b})\}\), where \(_{i}=_{(y)}\,\). The difference between a mini-batch gradient w.r.t. noisy labels and the gradient of the population loss decomposes for \(_{}\) as follows (similar decomposition holds for \(_{}\)):

\[_{}_{}(f_{})-_{} _{}(f_{})\] \[= _{}_{S}(f_{})-_{} _{}(f_{})\ +\ _{}_{}(f_{})-_{} _{S}(f_{})\ +\ _{}_{}(f_{})-_{ }_{}(f_{})\] \[= _{S}(f_{})-_{ }_{}(f_{})}_{(a)}\ +\ }_{(x,y) S}(y- \,)_{}f_{}(x)}_{(b)}\ +\ }_{(x,y) S}( \,-)_{}f_{}(x)}_{(c)}.\]

The term \((a)\) is the difference between the mini-batch gradient and the population gradient, the inherent stochasticity in mini-batch SGD, which has zero bias. The terms \((b)\) and \((c)\) form precisely the bias-variance decomposition for the additional stochasticity in the gradient introduced by the label noise, with the term \((c)\) having zero bias. In the case of stochastic convex optimization, it is well known that with access to biased gradients, it is impossible to achieve vanishingly small excess loss; whereas, with access to gradients with zero bias and any finite variance, it is possible to achieve an arbitrarily small excess loss using sufficient number of steps of stochastic gradient descent (see Appendix D for more details). Motivated by this reasoning, our approach is to use a randomizer that has the _smallest variance possible while ensuring zero bias_, namely the term \((b)\) is zero.

### Computing Optimal Unbiased Randomizers

We consider a label randomizer that minimizes the noisy label loss, while being unbiased. Namely, we say that an \(\)-DP randomizer \(\) that maps the label set \(\) to \(\) is _unbiased_ if it satisfies \(_{(y)}\,=y\) for all input labels \(y\). We use a linear programming (LP) based algorithm (Algorithm 1) to compute an unbiased randomizer that minimizes the noisy label loss. In order to keep this approach computationally tractable we require that both \(\) and \(}\) are finite; as we discuss shortly, it is possible to handle general \(\) by discretization using randomized rounding, and moreover the excess noisy label loss due to consideration of a discrete \(}\) can be bounded as well. Even though Algorithm 1 is general, we will only consider \(g=_{}\) henceforth (irrespective of the training loss \(\)).

``` Parameters: Privacy parameter \( 0\). Input:\(=(p_{y})_{y}\)--prior over input labels \(\), \(}=(_{i})_{i}\)--a finite sequence of potential output labels. Output: An \(\)-DP label randomizer. ```

**Algorithm 1**ComputeOptUnbiasedRand\({}_{}\)

In Figure 2, we illustrate a prior \(\) over \(=\{0,1,2\}\), using the example in Table 1, and the corresponding unbiased randomizer returned by \(_{=0.5}(,})\) for a certain fine-grained choice of \(}\). It is worth noting that this randomizer is quite unlike "randomized response" or any additive noise mechanism! Additionally, the output labels are all outside \(\), which is the convex hull of \(\). We have observed the same in our experiments in Section4 as well. One may find it surprising that these "out of domain" noisy labels in fact yield better trained models! This could be attributed to the Bayes optimal predictor for this randomizer being precisely \(f_{}^{*}\), since \([(y)]=y\) for all \(y\) (Theorem5).

There are however a couple of challenges to be addressed. First, it is unclear if the optimal unbiased randomizer has only finitely many labels, and even if so, what is the number of distinct output labels. Secondly, one needs to fix the choice of \(}\) before hand, and it is unclear how one can choose it in a way that the LP is feasible and moreover, the solution is close to the optimal unbiased randomizer. We address these challenges as follows.

Structure of optimal unbiased randomizers.Addressing the first challenge, we show that there is an optimal unbiased randomizer that has a small number of distinct output labels.

**Theorem 6**.: _For any convex loss \(g\), all distributions \(P\) over \(\) with finite support, and \(>0\), there exists an \(\)-\(\) unbiased randomizer \(_{*}_{}}(;)\) with at most \(2||\) output labels._

The main ideas in the proof are as follows: we use the convexity of \(g\) to show that an optimal \(\)-\(\) unbiased randomizer \(\) must be of a certain form; if not we can use Jensen's inequality to construct another randomizer \(^{}\) with that form such that \((^{};P)(;)\). In particular, we show that the randomizer must be a _staircase mechanism_ as defined in , a mechanism that maximally satisfies the DP constraints. This already implies that the number of output labels is at most \(2^{||}\). We use the ordering of the labels, along with similar reductions using convexity of \(g\), to show that the optimal \(\)-\(\) unbiased randomizer further lies within a special subset of staircase mechanisms with \(2||\) output labels. The detailed proof is provided in AppendixB.

Choosing \(}\) to ensure feasibility and good coverage.To address the second challenge, we use a finite set of output labels. We show an upper bound on the excess noisy label loss due to discretization.

We use a heuristic (Algorithm2) for setting \(}\) to be a grid, in a way that ensures that the LP in Algorithm1 has a feasible solution while keeping \(}\) small enough to be able to efficiently solve the LP. To compute the endpoints of the grid, we use an unbiased randomizer with a finite and bounded support: the \(\)-\(\)_debiased randomized response_ (\(\)-\(\)) on the set of labels, which operates by mapping the inputs to a unique set of values such that under randomized response the randomizer is unbiased (see AppendixF for a definition). We choose the endpoints of our grid to be precisely the minimum and maximum among the possible outputs of \(\)-\(\) (see Algorithm2 for details). With those two endpoints defined, we create the grid by evenly spacing output labels along this interval. The number of output labels we generate is as large as possible while maintaining that the LP solver terminates in a reasonable amount of time. We show that having these endpoints suffices to ensure feasibility of the LP in Algorithm1.

**Proposition 7**.: _If \(}\) contains just the smallest and largest values among the outputs of \(\)-\(\), the LP in \(_{}(P,})\) is feasible._

Figure 2: (a) An example prior \(\) over \(=\{0,1,2\}\). (b) The optimal unbiased randomizer returned by \(_{=0.5}(,})\) using a fine grid for \(}\), indicated by the probability mass function for each input label.

We defer the proof to Appendix F; the main idea is that these endpoints create an interval that contains the interval one would obtain from the debiased randomized response on the two label set of the minimum label and maximum label. Because the \(}\) we choose has labels less than this minimum and larger than this maximum, all labels in between can be generated by interpolation.

Additionally, we show that using the randomizer optimized on the grid \(}\) gives bounded excess noisy label loss compared to optimizing over a continuous set of labels for Lipschitz loss functions.

**Lemma 8**.: _Let \(\) be the optimal unbiased randomizer with \(}=[L,U],\) and let \(_{}\) be the optimal unbiased randomizer with \(}=\{L,L+,,U-,U\}\). If \(g(,y)\) is \(K\)-Lipschitz in \(\), then \((_{};)(; )+K\)._

In the case of \(g(,y)=(-y)^{2}\), we have \(K=(U-L).\) Therefore, as the grid gets finer, the excess noisy label loss of \(_{}\) obtained from 1 over the noisy label loss of \(\) scales linearly in the discretization parameter \(\). We present the proof in Appendix E.

For large \(\) values, we experimentally observe that the optimal unbiased randomizer appears to approach \(\)-\(\). For small \(\) values, we also experimentally observe that the optimal unbiased randomizer appears to be supported on the labels of the debiased randomized response on the two label set of the minimum label and the maximum label. This justifies our choice of endpoints for grid.

``` Parameters: Privacy parameter \( 0\). Input: Set of input labels \(\). Positive integer \(n 2\) representing size of output \(|}|\). Output: Set of output values \(}\) that guarantee feasibility of LP in Algorithm 1. \(L((e^{}+||-1)()-_ {y}y)/(e^{}-1)\) \(U((e^{}+||-1)()- _{y}y)/(e^{}-1)\) \((U-L)/(n-1)\) return\(}=(L,L+,L+2,,U-,U)\) ```

**Algorithm 2**\(_{}\)

**Splitting budget between prior and label.** So far, we have assumed a known prior \(P\) over input labels \(\). However, this is typically not the case, and so we use the standard Laplace mechanism to privately estimate the prior. Given \(n\) samples drawn from \(P\), \(_{}^{}\) constructs a histogram over \(\) and adds Laplace noise with scale \(2/\) to each entry, followed by clipping (to ensure that entries are non-negative) and normalization. For completeness, we include a formal description of \(_{}^{}\) in Appendix C.

Our randomizer for the unknown prior case, described in Algorithm 3, thus operates by splitting the privacy budget into \(_{1},_{2}\), using \(_{_{1}}^{}\) to get an approximate prior distribution \(P^{}\), and using the randomizer returned by \(_{_{2}}(^{},})\) to randomize the labels. It follows that the entire algorithm is \((_{1}+_{2})\)-\(\).

Handling continuous \(}\).While we focused on the case of finite sets \(\), the approach can be extended to the case where \(\) is a continuous set, say \(=[L,U]\). In this case, we can first choose a finite discrete subset \(}\) that contains both \(L\) and \(U\), e.g., \(}=\{L,L+,,U\}\), and then apply _unbiased randomized rounding_\(_{}}()\) to all labels before applying any mechanism \(\).

Namely, for any label \(y[L,U]\) such that \(y_{1} y y_{2}\) for \(y_{1},y_{2}\) being consecutive elements of \(}\), \(_{}}(y)\) returns \(\) drawn as \(y_{1}\) with probability \(-y}{y_{2}-y_{1}}\) and \(y_{2}\) with probability \(}{y_{2}-y_{1}}\). This ensures that \([]=y\) and hence, for any unbiased mechanism \(\) that acts on inputs in \(}\), it holds for all \(y\) that \([(_{}}(y))]=[ _{}}(y)]=y\). Furthermore, as the gap between consecutive points of \(}\) decreases, the variance introduced by \(_{}}\) also decreases.

## 4 Experimental Evaluation

We evaluate our randomizer on three datasets, and compare with the Laplace mechanism , the additive staircase mechanism , and the \(\)-\(\)-\(\) method . The Laplace mechanism and the additive staircase mechanism both have a discrete and a continuous variant. Following , we use the continuous variants for real-valued labels (the Criteo Sponsored Search Conversion dataset), and the discrete variants for integer-valued labels (the US Census dataset and the App Ads Conversion Count dataset). Note that in the experiments from , the noised labels from both the Laplace mechanism and the additive staircase mechanism were clipped to be in the valid label value range, as for small \(\)'s, the magnitude of the noised labels could be orders of magnitudes larger than the original label values, potentially causing numerical instability in model training. However, in our study, we found that with more careful hyperparameter tuning and early stopping (see Appendix G), the learning could be stabilized for sufficiently small values (e.g., \( 0.3\)) that is practically useful in ML, and in this case, the unclipped (therefore also unbiased) version of both the Laplace and the additive staircase mechanisms outperform their clipped counterpart. For reference, we present the results for both clipped and unclipped variants for those two mechanisms. We note that all of these mechanisms can be implemented in the feature-oblivious label DP setting (Figure 1). More details on model and training configurations are presented in Appendix G.

### Criteo Sponsored Search Conversion

The Criteo Sponsored Search Conversion Log Dataset  is a collection of \(15,995,634\) data points derived from a sample of \(90\)-day logs of live traffic from Criteo Predictive Search (CPS). Each example contains information of an user action (e.g., a click on an advertisement) and a subsequent conversion (purchase of the related product) within a \(30\)-day attribution window. We use the setup of feature-oblivious label DP to predict the revenue in \(\) (i.e., the \(\) field in the dataset) of a conversion. We apply the following preprocessing steps: filtering out examples with \(\) being \(-1\), and clipping the labels at the \(95\)th percentile of the value distribution (\(400\)).

In Figure 3, we visualize an example of the various label randomizers. For \(=4\), the optimal \(\)-\(\)-\(\) randomizer maps the input values to one of four distinct values. On the other hand, for the optimal unbiased randomizer, the joint distribution of the sensitive labels and randomized labels maintains an overall concentration along the diagonal. The joint distribution for the Laplace mechanism is quite spread out.

In Figure 4, we compare the noisy label loss on the training set and the mean squared error on the test set for all the randomizers considered. For each randomizer, the label randomization and training was run with 10 independent random seeds, and the plots show the average and standard deviation bars. For \(\)-\(\)-\(\) and the optimal unbiased randomizer, we use \(_{1}=0.017\) for privately estimating the prior using \(_{_{1}}^{}\), and \(_{2}=-_{1}\) for randomizing the label, following the guidance in Appendix C.

Figure 3: Illustration of various \(\)-DP label randomizers for \(=4\). The 2D density plot contours are generated in log scale using Gaussian kernel density estimates. The legends show the MSE between the original labels and the \(\)-DP randomized labels.

We find that the optimal unbiased randomizer (Algorithm 3) achieves the smallest test mean squared error across a wide range of \(\) values. It is interesting to note that the unbiased randomizers (Laplace and additive staircase mechanisms and the optimal unbiased randomizer) vastly outperform the biased randomizers (clipped versions of Laplace and additive staircase mechanisms as well as the RR-on-Bins) in terms of test loss, even though the noisy label loss is an order of magnitude larger for the unbiased randomizers.

### US Census

The \(1940\) US Census dataset3 is widely used in the evaluation of data analysis with DP . This digitally released dataset in 2012 contains \(131,903,909\) examples. We follow the task studied in  and evaluate label DP algorithms by learning to predict the number of weeks each respondent worked during the previous year (the WKSWORK1 field).

In Figure 4(a) we compare the mean squared error on the test set for all the randomizers considered. For each randomizer, the label randomization and training was run with 10 independent random seeds, and the plots show the average and standard deviation bars. For RR-on-Bins and the optimal unbiased randomizer, we use \(_{1}=0.002\) for privately estimating the prior using \(_{_{1}}^{}\), and \(_{2}=-_{1}\) for randomizing the label, following the guidance in Appendix C. We find that the optimal unbiased randomizer achieves the smallest test mean squared error across a wide range of \(\) values.

Figure 4: Comparison of different label DP randomizers on the Criteo Search dataset: (a) shows the mean squared error between the original labels and the privatized labels on the training set from each randomizer; (b) shows the mean squared error between the model predictions and the groundtruth labels on the test set. The solid line indicates the non-private baseline.

Figure 5: Comparison of different label DP randomizers: (a) shows the mean squared error of the prediction on the US Census test set. The solid line indicates the non-private baseline. (b) shows the relative Poisson loss (i.e., \((L-L^{})/L^{}\)) with respect to the non-private baseline Poisson loss \(L^{}\).

### App Ads Conversion Count

We also evaluate on a conversion count prediction dataset collected from a commercial mobile app store. Each example in this dataset corresponds to an ad click and the task is to predict the number of post-click conversion events in the app after a user installs the app within a certain time window.

In Figure 4(b) we compare the relative Poisson loss on the test set for all the randomizers considered. For each randomizer, the label randomization and training was run with 6 independent random seeds. For the optimal unbiased randomizer, Laplace and additive staircase mechanisms, at low \(\)'s we experience blowup in the training loss during training. The plot show the average and standard deviation bars, at the time of lowest training loss, right before blowup. We see that for all \(\)'s, all of the unbiased randomizers vastly outperform the others, including RR-on-Bins. Among the unbiased randomizers (optimal unbiased randomizer, Laplace and additive staircase mechanisms), the optimal unbiased randomizer performs the best at all \(\)'s. One caveat is that for the smaller \(\)'s, one can see the standard error increase. We hypothesize this is not an inherent feature of the randomizers but due to the blow up of the model training, reducing the training length and adding additional variance to the error, a behavior that seems specific to the AppAds dataset.

## 5 Related Work

DP learning has been the subject of considerable research spanning different settings that include empirical risk minimization , PAC learning , training neural networks , online learning , and regression . For the label DP setting,  studied the sample complexity of classification, while  studied sparse linear regression in the local DP model . For training deep neural networks with label DP,  studied the classification setting and gave a multi-stage training procedure where the priors on the labels in the previous stage are used to define an LP that is optimized to find the randomization mechanism for the next stage. For the classification loss, they characterized the optimal solution as the so-called RRTop-\(k\). By contrast, the work of  showed that for regression objectives the optimal solution to the LP is an RR-on-Bins solution. A crucial insight in our work is that the addition of an unbiasedness constraint to the LP leads to solutions that (i) are not RR-on-Bins solutions, (ii) can have substantially higher variance than RR-on-Bins, and (iii) nevertheless have a much lower train and test error due to the reduction in bias.

Kairouz et al.  defined a family of staircase mechanisms and showed they are optimal among local DP algorithms for minimizing an objective function given a prior. We extend the notion of staircase mechanisms to include real-valued labels that affect the optimization function. The structural properties that we prove (Theorem 6) utilize the ordering of the labels and the unbiasedness condition, which we show is critical for training neural networks for regression with label DP and high accuracy. Moreover, their work shows the presence of staircase mechanisms in the context of optimizing mutual information and KL-divergence objectives, whereas we show their presence in the context of regression.

A two-party learning setting with one features party and one labels party was recently studied in the work of . They focus on the interactive setting, which is arguably less practical than the (non-interactive) feature-oblivious setting studied in  that we also consider.

We also note that the label DP mechanism of , which builds on the PATE framework , and the works of , which leverage unsupervised and semi-supervised learning algorithms, cannot be implemented in the feature-oblivious setting. The DP-SGD algorithm of , which protects both the features and the label of each training example and which has been applied to different domains including computer vision  and language models  also cannot implemented in the feature-oblivious label DP setting. The same is also true for label DP algorithms for logistic regression  that are based on linear queries (where the features are assumed to be known and non-sensitive, and the linearity is with respect to the labels).

Over the past decade, there has been a significant body of research on DP ML (e.g., ). In particular, DP regression has been the focus of several prior papers including .

The label DP setting has also been studied in several papers including . The Randomized Response (RR) mechanism , a basic form of label DP, was introduced several decades ago and is widely studied/used.

## 6 Conclusion

In this work, we show that using unbiased \(\)-DP label randomizers lead to better trained models, and in particular, choosing the label randomizer that minimizes the noisy label squared loss seems to perform the best in terms of test performance, by empirically demonstrating this on three datasets. We also provide theoretical results shedding light on the structure of these randomizers, as well as why they might lead to better trained models.

Discussion.While we focused entirely on \(\)-DP ("pure-DP"), for our specific approach, it does not seem that relaxing to \((,)\)-DP ("approximate-DP") will be beneficial. For the first stage of estimating the histogram privately, one could potentially use an approximate-DP mechanism, but we believe that is unlikely to change the prior significantly. For the second stage of randomizing labels, it is known that approximate-DP may not be helpful in the local model .

In this work, we use hyperparameter tuning to choose the best architecture, which in general has additional privacy costs, and how to tune hyperparameters privately and efficiently is an active research topic . Consequently, it is common in the private ML literature to separate the question of private hyperparameter tuning and private training, and focus on comparing the privacy-utility trade-off under optimal hyperparameters, e.g., . We also follow this convention.

Limitations and future work.While LPs are somewhat efficient, it would be desirable to have a more efficient algorithm for computing the optimal unbiased randomizer. The RR-on-Bins randomizer introduced in  has an important advantage that one does not need to construct the set of potential output labels \(}\) before hand, and in fact, there is a simple dynamic programming algorithm to compute the optimal randomizer along with the output labels. Moreover, the RR-on-Bins family of randomizers has a clean structure. Understanding more structural properties of the optimal unbiased randomizers, and designing better algorithms for computing them, would be an interesting future direction to pursue.

To further improve the benefit of this optimal prior-based unbiased label randomizer, it might be possible to first partition the input examples into groups of "related examples", purely using the input features and compute the optimal unbiased randomizer for each group, by privately estimating the prior over labels for each group. This could lead to better test performance, as the randomizer can adapt to the different priors across these groups, unlike the "static" label randomizers such as Laplace and additive staircase mechanisms. For example, such an approach was used in the setting of image classification using self-supervised learning in .

As shown in (2), there is a bias-variance trade-off when it comes to choosing a label randomizer. While RR-on-Bins minimizes the noisy label loss (corresponding to the variance), the method in our work minimizes the bias (in fact, making it zero), at the cost of greatly increasing the variance. However, it might be possible in some settings that allowing a small amount of bias might greatly reduce the variance, thereby improving performance. This might be especially relevant when \(\) is very small. This could be done for example by adding a regularizer to reduce the bias, without enforcing hard unbiased constraints, as currently done in Algorithm 1. We leave this investigation to future work.