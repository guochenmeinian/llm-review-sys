# LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential Recommendation

Qidong Liu1,2, Xian Wu3, Yejing Wang2, Zijian Zhang2,4,

**Feng Tian5, Yefeng Zheng3,6, Xiangyu Zhao2**

\({}^{1}\) School of Auto. Science & Engineering, MOEKLINNS Lab, Xi'an Jiaotong University

\({}^{2}\) City University of Hong Kong

\({}^{3}\) Jarvis Research Center, Tencent YouTu Lab, \({}^{4}\) Jilin University

\({}^{5}\) School of Comp. Science & Technology, MOEKLINNS Lab, Xi'an Jiaotong University

\({}^{6}\) Medical Artificial Intelligence Lab, Westlake University

liuqidong@stu.xjtu.edu.cn, {kevinxwu, yefengzheng}@tencent.com,

yejing.wang@my.cityu.edu.hk, zhangzijian@jlu.edu.cn,

fengtian@mail.xjtu.edu.cn, xianzhao@cityu.edu.hk

Corresponding authors: Xian Wu, Feng Tian and Xiangyu Zhao

###### Abstract

Sequential recommender systems (SRS) aim to predict users' subsequent choices based on their historical interactions and have found applications in diverse fields such as e-commerce and social media. However, in real-world systems, most users interact with only a handful of items, while the majority of items are seldom consumed. These two issues, known as the long-tail user and long-tail item challenges, often pose difficulties for existing SRS. These challenges can adversely affect user experience and seller benefits, making them crucial to address. Though a few works have addressed the challenges, they still struggle with the seesaw or noisy issues due to the intrinsic scarcity of interactions. The advancements in large language models (LLMs) present a promising solution to these problems from a semantic perspective. As one of the pioneers in this field, we propose the **L**arge **L**anguage **M**odels **E**nhancement framework for **S**e**oential **R**ecommendation (**LLM-ESR**). This framework utilizes semantic embeddings derived from LLMs to enhance SRS without adding extra inference load from LLMs. To address the long-tail item challenge, we design a dual-view modeling framework that combines semantics from LLMs and collaborative signals from conventional SRS. For the long-tail user challenge, we propose a retrieval augmented self-distillation method to enhance user preference representation using more informative interactions from similar users. To verify the effectiveness and versatility of our proposed enhancement framework, we conduct extensive experiments on three real-world datasets using three popular SRS models. The results show that our method surpasses existing baselines consistently, and benefits long-tail users and items especially. The implementation code is available at https://github.com/Applied-Machine-Learning-Lab/LLM-ESR.

## 1 Introduction

The objective of sequential recommendation is to predict the next likely item for users based on their historical records . Owing to its wide-ranging applicability in various domains such as e-commerce  and social media , sequential recommendation has garnered considerable attention in recent years. Given that the essence of sequential recommendation revolves around extracting user preferences from their interaction records, several innovative architectures have been proposed. Forinstance, SASRec  applies the self-attention technique to capture the users' long-term preference, while FMLPRec  introduces a pure MLP architecture to identify dynamics in users' preference.

Despite significant advancements in sequential recommendation, the long-tail challenges continue to undermine its practical utility. Generally, these challenges can be categorized into two types, affecting either the user or the item side. To illustrate, we present the performance of a well-known SRS model, SASRec , on the Amazon Beauty dataset, along with its statistics in Figure 1. **i) Long-tail User Challenge**: In Figure 1 (a), we note that above \(80\%\) users have interacted with fewer than 10 items (_i.e.,_ long-tail users), and SASRec's performance is subpar for these users compared to those with more interaction records. This suggests that the majority of users receive less than optimal recommendation services. **ii) Long-tail Item Challenge**: Figure 1 (b) demonstrates that SASRec performs significantly better on more popular items. However, the histogram indicates that around \(71.4\%\) items own no more than \(30\) interaction records, meaning they are less frequently consumed. Addressing these long-tail challenges is crucial for elevating user experience and seller benefits.

To tackle the long-tail item challenge, existing studies [17; 20] examine the co-occurrence pattern between popular and long-tail items, aiming to enrich the representation of long-tail items with that of popular ones. Nevertheless, ignorance of the true relationship between items may cause a seesaw problem . As for the long-tail user challenge, existing research [37; 34] explores the interaction history of all users, attempting to augment pseudo items for tail users. However, these approaches still only rely on collaborative information, which inclines to generate noisy items due to inaccurate similarity between users . At this time, superb semantic relations between users or items can make an effect, which indicates the potential of utilizing semantics to face long-tail challenges.

Recent advancements in large language models (LLMs) offer promise for alleviating long-tail challenges from a semantic perspective. However, LLMs are initially designed for natural language processing tasks but not for recommendation ones. Some works [63; 43] have made efforts to adapt, but two problems still exist. **i) Inefficient Integration**: Recent research has explored deriving informative prompts to activate ChatGPT [55; 10] or modifying the tokenization method of LLaMA [25; 27; 59] for sequential recommendation. Despite their impressive performance, these approaches are challenging to apply in industrial settings. This is because recommender systems typically require low latency for online deployment, whereas LLMs often entail high inference costs . **ii) Deficiency of Semantic Information**: Several recent works [13; 16] propose utilizing embeddings derived from LLMs to initialize the item embedding layer of sequential recommendation models, thereby integrating semantic information. However, the fine-tuning process, if not done without freezing the embedding layer, may erode the original semantic relationships between items. Additionally, these approaches focus solely on the item side, neglecting the potential benefits of incorporating semantic information on the user side which could aid the sequence encoder of an SRS.

In this paper, to better integrate LLMs into SRS for addressing long-tail challenges, we design a Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR). Firstly, we derive the semantic embeddings of items and users by encoding prompt texts from LLMs. Since these embeddings can be cached in advance, our integration does not impose any extra inference burden from LLMs. To tackle the long-tail item challenge, we devise a dual-view modeling framework that combines semantic and collaborative information. Specifically, the embeddings derived from LLMs are frozen to avoid deficiency of semantics. Next, we propose a retrieval augmented self-distillation method to enhance the sequence encoder of an SRS model using similar users. The similarity between users is measured by the user representations from LLMs. Finally, it is important

Figure 1: The preliminary experiments of SASRec on Beauty dataset.

to note that the proposed framework is model-agnostic, allowing it to be adapted to any sequential recommendation model. The contributions of this paper are as follows:

* We propose a large language models enhancement framework, which can alleviate both long-tail user and item challenges for SRS by introducing semantic information from LLMs.
* To avoid the inference burden of LLMs, we design an embedding-based enhancement method. Besides, the derived embeddings are utilized directly to retain the original semantic relations.
* We conduct extensive experiments on three real-world datasets with three backbone SRS models to validate the effectiveness and flexibility of LLM-ESR.

## 2 Problem Definition

The goal of the sequential recommendation is to give out the next item that users are possible to interact with based on their interaction records. The set of users and items are denoted as \(=\{u_{1},,u_{i},,u_{||}\}\) and \(=\{v_{1},,v_{i},,v_{||}\}\), respectively, where \(||\) and \(||\) are the number of users and items. Each user has an interaction sequence, which arranges the interacted items by timeline, denoted as \(_{u}=\{v_{1}^{(u)},,v_{i}^{(u)},,v_{n_{u}}^{(u)}\}\). \(n_{u}\) represents the interaction number of user \(u\). For simplicity, we omit the superscript \((u)\) in the following sections. Then, the problem of sequential recommendation can be defined as follows:

\[arg_{v_{i}}P(v_{n_{u}+1}=v_{i}|_{u})\] (1)

Following the existing works related to long-tailed SRS [17; 20], we can split the users and items into tail and head groups. Let \(n_{u}\) and \(p_{v}\) denote the length of the user's interaction sequence and the popularity of the item \(v\) (_i.e.,_ the total interaction number). Firstly, we sort the users and items by the values of \(n_{u}\) and \(p_{v}\) in descending order. Then, take out the top \(20\%\) users and items as **head user** and **head item** according to Pareto principle , denoted as \(_{head}\) and \(_{head}\). The rest of the users and items are the **tail user** and **tail item**, _i.e.,_\(_{tail}=_{head}\) and \(_{tail}=_{head}\). To alleviate the long-tail challenges, we aim to elevate the recommending performance for \(_{tail}\) and \(_{tail}\).

Figure 2: The overview of the proposed LLM-ESR framework.

Llm-Esr

### Overview

The overview of the proposed LLM-ESR is shown in Figure 2. To acquire the semantic information, we adopt LLMs to encode textual users' historical interactions and items' attributes into LLMs user embedding and LLMs item embedding. Then, two modules are proposed to augment long-tail items and long-tail users, respectively, _i.e.,_ Dual-view Modeling and Retrieval Augmented Self-Distillation. **i) Dual-view Modeling**: This module consists of two branches. One is _semantic-view modeling_, which aims to extract the semantic information from the user's interaction sequence. It first utilizes the semantic embedding layer, derived from LLMs item embedding, to encode the items. Then, an adapter is designed for dimension adaptation and space transformation. The output item embedding sequence will be fed into cross-attention for fusion and then sequence encoder to get the user representation in semantic view. The other branch is _collaborative-view modeling_, which transforms the interaction sequence into an embedding one by a collaborative embedding layer. Next, followed by a cross-attention and the sequence encoder, the collaborative user preference is obtained. At the end of this module, the user representations in the two views will be fused for the final recommendations. **ii) Retrieval Augmented Self-Distillation**: This module expects to enhance long-tail users through informative interactions of similar users. First, the derived LLMs user embedding is considered as a semantic user base for retrieving similar users. Then, similar users are fed into dual-view modeling to get their user representations, which are the guide signal for self-distillation. Finally, the derived distillation loss will be utilized as an auxiliary loss for training.

### Dual-view Modeling

The traditional SRS models are skilled in capturing collaborative signals, which can recommend for popular items well [20; 17]. However, they compromise on long-tail items due to the lack of semantics . Therefore, we model the preferences of users from the dual views to cover all items simultaneously. Besides, we propose a two-level fusion to better combine the benefits from both two.

**Semantic-view Modeling**. In general, the attributes and descriptions of items contain abundant semantics. To utilize the powerful semantic understanding abilities of LLMs, we organize the attributes and descriptions into textual prompts (the template of prompts can be found in **Appendix**A.1). Then, in avoid of possible inference burden brought by LLMs, we cache the embeddings derived from LLMs for usage. In specific, the embeddings can be obtained by taking out the last hidden state of open-sourced LLMs, such as LLaMA , or the public API, such as text-embedding-ada-0022. We adopt the latter one in this paper. Let \(_{se}^{|| d_{llm}}\) denotes the LLMs embedding of all items, where \(d_{llm}\) is dimension of LLMs embedding. Then, the semantic embedding layer \(_{se}\) from LLMs can be used for semantic-view modeling to enhance long-tail items. However, previous works [13; 16] often adapt it as the initialization of the item embedding layer, which may ruin the original semantic relations during fine-tuning. In order to retain the semantics, we freeze the \(_{se}\) and propose an adapter to transform the raw semantic space into the recommending space. For each item \(i\), we can get its LLMs embedding \(_{i}^{llm}\) by taking the \(i\)-th row of \(_{se}\). Then, it will be fed into the tunable adapter to get the semantic embedding:

\[_{i}^{se}=_{2}^{a}(_{1}^{a}_{i}^{llm }+_{1}^{a})+_{2}^{a}\] (2)

where \(_{1}^{a}^{}{l} d_{llm}},_ {2}^{a}^{d}{l}}\) and \(_{1}^{a}^{}{l} 1},_{2}^{a} ^{d 1}\) are the weight matrices and bias of adapter. Following this process, we can obtain the item embedding sequence of the user's interaction records, denoted as \(^{se}=[_{1}^{se},,_{n_{n}}^{se}]\). Similar to a general SRS model, we employ a sequence encoder \(f_{}\) (_e.g.,_ self-attention layers  for SASRec ) to get the representation of user preference in semantic view as follows:

\[^{se}=f_{}(^{se})\] (3)

where \(^{se}^{d 1}\) is the user preference representation in semantic view and \(\) denotes the parameters of sequence encoder in an SRS model.

**Collaborative-view Modeling**. To utilize the collaborative information, we adopt a trainable item embedding layer and supervised update it by interaction data. Let \(_{co}^{|| d}\) denotes the collaborative embedding layer of the item. Then, the item embedding sequence \(^{co}=[^{co}_{1},,^{co}_{n_{u}}]\) is acquired by extracting the corresponding rows from \(_{co}\). To get the user preference \(^{co}\) in the collaborative view, we input embedding sequence to sequence encoder, _i.e._, \(^{co}=f_{}(^{co})\). It is worth noting that, the sequence encoder \(f_{}\) is the same one in both semantic and collaborative views for the shared sequential pattern and higher efficiency . Besides, the embedding layers in the two views are in unbalanced training stages (one is pretrained, while the other is from scratch), which may lead to optimization difficulty . To handle such a problem, we initialize the \(_{co}\) by dimension-reduced \(_{se}\). The Principal Component Analysis (PCA)  is used as the dimension reduction method in this paper.

**Two-level Fusion**. The effective integration of both semantic-view and collaborative-view is essential to absorb the benefits of these two. However, the direct merge of the user representations in dual views may overlook the nuanced inter-relationships between item sequences. Thus, we design a two-level fusion method for the dual-view modeling module, _i.e._, sequence-level and logit-level. The former aims to implicitly capture the mutual relationships between the item sequences of dual views, while the latter explicitly targets the combination of recommending abilities. In specific, we propose a cross-attention mechanism for sequence-level fusion. To simplify the description, we only take the semantic view interacting with the collaborative view for illustration, and the other view is the same. Specifically, \(^{se}\) is considered as the _query_, and \(^{co}\) as the _key_ and _value_ in attention mechanism. Let \(=^{se}^{Q}\), \(=^{co}^{K}\), \(=^{co}^{V}\), where \(^{Q},^{K},^{V}^{d d}\) are weight matrices. Then, the interacted collaborative embedding sequence can be formulated as follows:

\[}^{co}=(^{T}}{})\] (4)

Following the same process of cross-attention, we can also get the corresponding semantic embedding sequence \(}^{se}\). Finally, \(^{se},^{co}\) are substituted by \(}^{se},}^{se}\) to be fed into \(f_{}()\). As for logit-level fusion, we concatenate the two-view user and item embeddings for recommendation. The probability score of recommending item \(j\) for the user \(u\) is therefore calculated as:

\[P(v_{n_{u}+1}=v_{j}|v_{1:n_{u}})=[^{se}_{j}:^{co}_{j}]^{T} [^{se}:^{co}]\] (5)

where ":" denotes the concatenation operation of two vectors. Based on the probability score, we adopt the pairwise ranking loss to train the framework:

\[_{Rank}=-_{u}_{k=1}^{n_{u}}\!(P(v^ {+}_{k+1}=|v_{1:k})-P(v^{-}_{k+1}=|v_{1:k}))\] (6)

where \(v^{+}_{k+1}\) and \(v^{-}_{k+1}\) are the ground-truth item and paired negative item. It is worth noting that the ranking loss may differ a little according to different backbone SRS models, _e.g._, sequence-to-one pairwise loss for GRU4Rec .

### Retrieval Augmented Self-Distillation

The long-tail user problem originates from the lack of enough interactions for the sequence encoder in an SRS to capture users' preferences. Thus, we propose a self-distillation method to augment the extraction capacity of the sequence encoder. Self-distillation [12; 61] is a type of knowledge distillation that considers one model as both the student and teacher for model enhancement. As for the SRS, since multiple similar users have more informative interactions, it is promising to transfer their knowledge to the target user for strengthening. Thereafter, there are two key challenges for such knowledge transfer, _i.e.,_ how to retrieve similar users and how to transfer the knowledge.

**Retrieve Similar Users**. Previous works have confirmed that LLMs can understand the semantic meanings of textual user interaction records for recommendation [25; 10]. Based on their observation, we organize the item's title that interacted by users into the textual prompts (the template of prompts can be found in **Appendix**A.1). Then, similar to the derivation of LLMs item embedding \(_{se}\), we can obtain and save the LLMs user embedding, denoted as \(_{llm}^{|| d_{llm}}\). It is also dubbed as the semantic user base in this paper, because the semantic relations are encoded in it. For each target user \(k\), we can retrieve the similar user set \(_{k}\) as follows:

\[_{k}=(\{(^{llm}_{k},^{llm}_{j} )\}_{j=1}^{||},N)\] (7)where \((,)\) is the cosine similarity function to measure the distance between two vectors. \(N\) represents the size of similar user sets, which is a hyper-parameter.

**Self-Distillation**. As mentioned before, we design the self-distillation to transfer the knowledge from several similar users to the target user. Since the representation of user preference, _i.e.,_\(^{se}\) and \(^{co}\), encode the comprehensive knowledge of the user, we configure such representation as the mediator for the distillation. To get the teacher mediator, we first utilize the dual-view modeling framework (Section 3.2) to get the user representation for each similar user, denoted as \(\{^{se}_{j},^{co}_{j}\}_{j=1}^{|_{k}|}\). Then, the teacher mediator is calculated by mean pooling, as the following formula:

\[[^{se}_{T_{k}}:^{co}_{T_{k}}]=(\{[^{se}_{j}:^{co}_{j}]\}_{j=1}^{|_{k}|})\] (8)

The student mediator is the representation of target user \(k\), _i.e.,_\([^{se}_{k}:^{co}_{k}]\). Based on the teacher and student mediators, the self-distillation loss can be formulated as:

\[_{SD}=|}_{k=1}^{||}|\{^{se}_{k}:^{co}_{k}\}-[^{se}_{T_{k}}:^{co}_{T _{k}}]|^{2}\] (9)

Note that the gradients of \(^{se}_{T_{k}}\) and \(^{co}_{T_{k}}\) are stopped, because they only provide the guidance signal instead of optimizing the model.

### Train and Inference

**Train**. Based on the illustration in Section 3.2 and Section 3.3, we only update the collaborative embedding layer, adapter, cross-attention and sequence encoder during the training, while freezing the semantic embedding layer and semantic user base. Since the original LLMs embeddings \(_{se}\) and \(_{llm}\) are frozen, the original semantic relations get preserved well. The training loss for optimization is the combination of pairwise ranking loss and self-distillation loss, which can be written as follows:

\[=_{Rank}+_{SD}\] (10)

where \(\) is a hyper-parameter to adjust the magnitude of self-distillation.

**Inference**. During the inference process of the LLM-ESR, the retrieval augmented self-distillation module is exempted due to no need for the auxiliary loss. Thus, we follow the dual-view modeling process for the final recommendation by Equation (5). Besides, since the semantic embedding layer can be cached in advance, the call for LLMs is avoided, which prevents the extra inference costs. Due to the limited space, the algorithm lies in **Appendix**A.2 for more clarity.

## 4 Experiment

### Experimental Settings

**Dataset**. There are three real-world datasets applied for evaluation, _i.e.,_ Yelp, Amazon Fashion and Amazon Beauty. We follow the previous SRS works [18; 50] for preprocessing and data split. More details about the datasets and preprocessing can be seen in **Appendix**B.1.

**Baselines**. To validate the flexibility, we combine the competing baselines and LLM-ESR with three well-known backbone SRS models: GRU4Rec , Bert4Rec  and SASRec . Then, two groups of baselines are compared in the experiments. One group is the traditional enhancement framework for the long-tailed sequential recommendation, including CITIES  and MELT . The other group is the LLM-based enhancement framework, which contains RLMRec  and LLMInit [13; 16]. The more details about baselines are put into **Appendix**B.2.

**Implementation Details**. The hardware used in all experiments is an Intel Xeon Gold 6133 platform with Tesla V100 32G GPUs, while the basic software requirements are Python 3.9.5 and PyTorch 1.12.0. The hyper-parameters \(N\) and \(\) are searched from \(\{2,6,10,14,18\}\) and \(\{1,0.5,0.1,0.05,0.01\}\). More details about the implementation details are in **Appendix**B.3. The implementation code is available at https://github.com/Applied-Machine-Learning-Lab/LLM-ESR.

**Evaluation Metrics**. In the experiments, we adopt the metrics of _Top-10_ list for evaluation. Specifically, the _Hit Rate_ (**H@10**) and _Normalized Discounted Cumulative Gain_ (**N@10**) are used. Following , we randomly sample \(100\) items that the user has not interacted with as the negatives paired with the ground truth for calculation of the metrics. To guarantee the robustness of the experimental results, we report the average results of the triplicate test with random seeds \(\{42,43,44\}\).

### Overall Performance

To validate the effectiveness and flexibility of the proposed LLM-ESR, we show the overall, tail and head performance on three datasets in Table 1. At a glance, we find that the proposed LLM-ESR can outperform all competing baselines with all SRS models across all user or item groups, which verifies the usefulness of our framework. Then, we probe more conclusions by the following analysis.

    &  &  &  &  &  &  \\   & & H@10 & N@10 & H@10 & N@10 & H@10 & N@10 & H@10 & N@10 & H@10 & N@10 \\   & GRt4Rec & 0.4879 & 0.2751 & 0.0171 & 0.0059 & 0.6265 & 0.3544 & 0.4919 & 0.2777 & 0.4726 & 0.2653 \\  & - CITIES & 0.4988 & 0.2749 & 0.0134 & 0.0051 & 0.6301 & 0.3543 & 0.4936 & 0.2783 & 0.4756 & 0.2618 \\  & - MELT & 0.4985 & 0.2825 & 0.0201 & 0.0079 & 0.6393 & 0.3633 & 0.5046 & 0.2865 & 0.4750 & 0.2671 \\  & - LLMRec & 0.4886 & 0.2777 & 0.0188 & 0.0067 & 0.6269 & 0.3574 & 0.4920 & 0.2804 & 0.4756 & 0.2671 \\  & - LLMInit & 0.4872 & 0.2749 & 0.0201 & 0.0072 & 0.6246 & 0.3537 & 0.4908 & 0.2775 & 0.4732 & 0.2647 \\   & **LLM-ESR** & **0.5724*** & **0.3413*** & **0.0763*** & **0.0318*** & **0.7184*** & **0.4324*** & **0.5782*** & **0.3456*** & **0.5501*** & **0.3247*** \\   & Bert@Rec & 0.5307 & 0.3035 & 0.0115 & 0.0044 & 0.6836 & 0.3916 & 0.5325 & 0.3047 & 0.5241 & 0.2988 \\  & - CITIES & 0.5249 & 0.3015 & 0.0041 & 0.0041 & 0.6783 & 0.3899 & 0.5274 & 0.3032 & 0.5155 & 0.2954 \\  & - MELT & 0.6206 & 0.3770 & 0.4029 & 0.0149 & 0.7907 & 0.4836 & 0.6210 & 0.3780 & 0.6191 & 0.3733 \\  & - RLMRec & 0.5306 & 0.3039 & 0.0104 & 0.0040 & 0.6938 & 0.3922 & 0.5351 & 0.3065 & 0.5137 & 0.2936 \\  & - LLMInit & 0.6199 & 0.3781 & 0.0874 & 0.0330 & 0.7766 & 0.4797 & 0.6204 & 0.3796 & 0.6118 & 0.3723 \\  & **LLM-ESR** & **0.6623*** & **0.4222*** & **0.1227*** & **0.0504*** & **0.2812*** & **0.5318*** & **0.6637*** & **0.4247*** & **0.6571*** & **0.4127*** \\   & SASRec & 0.5940 & 0.3597 & 0.1142 & 0.0495 & 0.7353 & 0.4511 & 0.5893 & 0.3578 & 0.6122 & 0.3672 \\  & - CITIES & 0.5828 & 0.3540 & 0.1532 & 0.0700 & 0.7093 & 0.4376 & 0.5785 & 0.3511 & 0.5994 & 0.3649 \\  & - MELT & 0.6257 & 0.3791 & 0.0115 & 0.0371 & 0.7801 & 0.4799 & 0.6246 & 0.3804 & 0.6299 & 0.3744 \\  & - RLMRec & 0.5990 & 0.3623 & 0.0953 & 0.0412 & 0.7474 & 0.4568 & 0.5966 & 0.3613 & 0.6084 & 0.3658 \\  & - LMInit & 0.6415 & 0.3997 & 0.1760 & 0.7809 & 0.7785 & 0.4941 & 0.6403 & 0.4010 & 0.6462 & 0.3948 \\  & **LM-ESR** & **0.6673*** & **0.4208*** & **0.1893*** & **0.0458*** & **0.5808*** & **0.5199*** & **0.6655*** & **0.4229*** & **0.6627*** & **0.4128*** \\   & GRt4Rec & 0.4798 & 0.3809 & 0.0257 & 0.0101 & 0.6660 & 0.5285 & 0.3781 & 0.2577 & 0.6118 & 0.5408 \\  & - CITIES & 0.4762 & 0.3743 & 0.0252 & 0.0103 & 0.6557 & 0.5191 & 0.3729 & 0.2501 & 0.6103 & 0.5354 \\  & - MELT & 0.4884 & 0.3975 & 0.0291 & 0.0112 & 0.6712 & 0.5513 & 0.3890 & 0.2770 & 0.6173 & 0.5538 \\  & - RLMRec & 0.4795 & 0.3808 & 0.0253 & 0

**Overall Comparison**. From the results, we observe that the proposed LLM-ESR leads the overall performance under both two metrics, which indicates better-enhancing effects. LLMInit is often the secondary. This phenomenon shows that the injection of semantics from LLMs actually augments the SRS. However, RLMRec often underperforms compared with other LLM-based methods, because it is devised for collaborative filtering algorithms, incompatible with SRS. As for the traditional baselines, MELT stays ahead in most cases. The reason lies in that it addresses the long-tail user and long-tail item challenges simultaneously. By comparison, CITIES is even sometimes inferior to the backbone SRS model due to the seesaw problem, _i.e.,_ drastic drops for popular items.

**Long-tail Item and User Comparison**. According to the split method illustrated in Section 2, the items are grouped into Tail Item and Head Item. From Table 1, we observe that our LLM-ESR not only achieves the best on the tail item group but also gets the first place on the head item group. Such performance comparison highlights the combination of semantics and collaborative signals by our dual-view modeling. LLMInit leads the tail group across all baselines, which suggests that semantic information can benefit long-tail items. It is worth noting that CITIES sometimes perform better for the tail group but harm those popular items, which means it has a seesaw problem. Additionally, the results illustrate that MELT, LLMInit and LLM-ESR can augment the tail user group markedly. MELT is devised to enhance tail user, but underperforms our method because of its limitations to collaborative perspective. Though LLMInit can also benefit tail users by introducing semantics, it ignores the utilization of LLMs from the user side.

**Flexibility**. Table 1 shows that the proposed framework can get the largest performance improvements on all three backbone SRS models, which indicates the flexibility of LLM-ESR. By comparison, the other baselines incline to depend on the type of SRS. The traditional method, _i.e.,_ CITIES and MELT, tend to perform better for GRU4Rec, while LLMInit is more beneficial to Bert4Rec and SASRec.

### Ablation Study

The results of the ablation study are shown in Table 2. Firstly, we remove the collaborative view or semantic view to investigate the dual-view modeling, denoted as _w/o Co-view_ and _w/o Se-view_. The results show that _w/o Co-view_ downgrades performance dramatically on the head group, while _w/o Se-view_ harms tail items evidently. Such changes indicate the distinct specialty of collaborative and semantic information, highlighting the combination of both. _w/o SD_ means dropping self-distillation, which shows performance drops for long-tail users. It suggests the effects of the proposed retrieval augmented self-distillation. The results of these three variants validate the motivation for designing each component for LLM-ESR. _w/o Share_ and _w/o CA_ represent using split sequence encoder and removing cross-attention. The decrease in performance of these two illustrates the effectiveness of the sharing design and sequence-level fusion. More results can be seen in **Appendix** C.1.

Furthermore, we have two designs to ease the optimization of the entire LLM-ESR framework. One is that we use dimension-reduced LLM item embeddings to initialize the collaborative embedding layer instead of random initialization. On the other hand, we propose a two-layer adapter to fill the large dimension gap between LLM embeddings and item embeddings. To illustrate the effectiveness of these two designs, we compare _1-layer Adapter_ and _Random Init_ variants of LLM-ESR. The results, shown in Table 2, indicate that both variants underperform the original LLM-ESR, verifying the success of our special designs.

    &  &  &  &  &  \\   & H@10 & N@10 & H@10 & N@10 & H@10 & N@10 & H@10 & N@10 & H@10 & N@10 \\  - **LLM-ESR** & **0.6673** & **0.4208** & 0.1893 & 0.0845 & **0.8080** & **0.5199** & **0.6685** & **0.4229** & **0.6627** & **0.4128** \\ - _w/o Co-view_ & 0.6320 & 0.3816 & 0.1898 & 0.0856 & 0.7621 & 0.4687 & 0.6318 & 0.3823 & 0.6325 & 0.3787 \\ - _w/o Se-view_ & 0.6468 & 0.4038 & 0.1105 & 0.0460 & 0.8047 & 0.5091 & 0.6459 & 0.4043 & 0.6501 & 0.4018 \\ - _w/o SD_ & 0.6572 & 0.4121 & **0.2030** & **0.0898** & 0.7911 & 0.5071 & 0.6566 & 0.4130 & 0.6574 & 0.4091 \\ - _w/o Share_ & 0.6595 & 0.4158 & 0.1728 & 0.0783 & 0.8027 & 0.5152 & 0.6606 & 0.4186 & 0.6552 & 0.4055 \\ - _w/o CA_ & 0.6644 & 0.4160 & 0.1850 & 0.0803 & 0.8004 & 0.5119 & 0.6652 & 0.4175 & 0.6616 & 0.4105 \\ 
1-layer Adapter & 0.6108 & 0.3713 & 0.1107 & 0.0469 & 0.7580 & 0.4668 & 0.6065 & 0.3702 & 0.6269 & 0.3754 \\ Random Init & 0.6440 & 0.3984 & 0.1899 & 0.0839 & 0.7777 & 0.4910 & 0.6454 & 0.4018 & 0.6388 & 0.3853 \\   

Table 2: The ablation study on the Yelp dataset with SASRec as the backbone SRS model. The boldface refers to the highest score and the underline indicates the next best result of the models.

### Hyper-parameter Analysis

To investigate the effects of the hyper-parameters in LLM-ESR, we show the performance trend along with their changes in Figure 3. The hyper-parameter \(\) controls to what extent the designed self-distillation affects the optimization. With \(\) ranging from \(1\) to \(0.01\), the recommending accuracy rises first and drops then. The reason for the compromised performance of large \(\) lies in that overemphasis on self-distillation will affect the convergence of ranking loss. Smaller \(\) also downgrades the performance, which indicates the usefulness of the designed self-distillation. As for the number of retrieved users \(N\), the best is \(10\). The reason is that more users can provide more informative interactions. However, too large \(N\) may decrease the relatedness of the retrieved users.

### Group Analysis

For more meticulous analysis, we split the users and items into \(5\) groups according to sequence length \(n_{u}\) and popularity \(p_{v}\), and show the performance of each group in Figure 4. From the results, we observe that LLM-based frameworks derive increases in every user and item group, while MELT has a positive effect on some specific groups. It reflects the seesaw problem of MLET and reveals the benefit of making use of semantic embeddings from LLMs. Comparing LLMInit with LLM-ESR, LLM-ESR can get more increments on the long-tail groups (_e.g.,_\(1\)-\(4\) user group and \(1\)-\(9\) item group), which proves the better reservation of semantic information from LLMs by our framework. The group analysis of Bert4Rec and GRU4Rec as backbones are shown in Appendix C.3.

## 5 Related Works

### Sequential Recommendation

The core of sequential recommendation refers to capturing the sequence pattern for the next likely item [29; 39; 31; 38; 60; 24; 23; 26; 40]. Thus, at the early stage, researchers focus on fabricating the

Figure 4: The results of the proposed LLM-ESR and competing baselines in meticulous user and item groups. The results are based on the Beauty dataset with the SASRec model.

Figure 3: The hyper-parameter experiments on the weight of self-distillation loss \(\) and the number of retrieved similar users \(N\). The results are based on the Yelp dataset with the SASRec model.

architecture to improve model capacity. GRU4Rec  and Caser  apply RNNs and CNNs  for sequence modeling. Later, inspired by the great success of self-attention  in natural language processing, SASRec  and Bert4Rec  verify its potential in SRS. Also, Zhou _et al._ proposes a pure MLP architecture, achieving similar accuracy but higher efficiency compared with SASRec. Despite the great progress in SRS, long-tail problems are still underexplored. As for the long-tail item problem, CITIES  designs an embedding inference function for those long-tail items specially. In terms of the long-tail user problem, data augmentation is the main way [37; 34]. Only one work, MELT , addresses both two problems simultaneously but still sticks to a collaborative perspective. By comparison, the proposed LLM-ESR handles both the two long-tail problems better from a semantic view by introducing LLMs.

### LLMs for Recommendation

Large language models [63; 43] have attracted widespread attention due to their powerful abilities in semantic understanding. Recently, There emerge several works to explore how to utilize LLMs in recommender systems (RS) [64; 28; 22; 41; 57; 58; 65; 32; 30; 35], which can be categorized into two lines, _i.e._, LLMs as RS and LLMs enhancing RS. The first line of research aims to complete recommendation tasks by LLMs directly. At the early stage, researchers tend to fabricate the prompt templates to stimulate the recommending ability of LLMs by dialogues. For example, ChatRec  proposes a dialogue process to complete recommendation tasks step by step. DRDT  integrates a retrieval-based dynamic reflection process for SRS by in-context learning . LLMRerank  and UniLLMRec  fabricate the chain-of-thought prompts to target the reranking stage and whole recommendation process, respectively. Besides, some other researchers explore fine-tuning open-sourced LLMs for RS. TALLRec  is the first one, which fine-tunes a LLaMA-7B by parameter-efficient fine-tuning techniques [15; 33]. Some following works, including E4SRec , LLaRA  and RecInterpreter , target combining collaborative signals into LLMs by modifying the tokenization. However, this line of work faces the challenge of high inference costs. Another line, LLMs enhancing RS, is more practical, because they avoid the use of LLMs while recommending. For instance, RLMRec  aligns with LLMs by an auxiliary loss. AlphaRec  adopts LLMs embedding to enhance the collaborative filtering models. On the other hand, LLM4MSR  and Uni-CTR  propose to utilize LLMs to augment the multi-domain recommendation models. As for LLMs enhancing sequential recommendation, Harte _et al._ and Hu _et al._ adopt LLMs embedding as the initialization for the traditional models. The proposed LLM-ESR belongs to the latter category but further alleviates the problem of defect of semantic information.

## 6 Conclusion

In this paper, we propose a large language model enhancement framework for sequential recommendation (LLM-ESR) to handle the long-tail user and long-tail item challenges. Firstly, we acquire and cache the semantic embeddings derived from LLMs, which is for inference efficiency. Then, a dual-view modeling framework is proposed to combine the semantics from LLMs and collaborative signals contained in the traditional model. It can help augment the long-tail items in SRS. Next, we design the retrieval augmented self-distillation to alleviate the long-tail user challenge. Through the comprehensive experiments, we verify the effectiveness and flexibility of our LLM-ESR.

## 7 Acknowledgements

This research was partially supported by National Key Research and Development Program of China (2022YFC3303600), National Natural Science Foundation of China (No.62192781, No.62177038, No.62293551, No.62277042, No.62137002, No.61721002, No.61937001, No.62377038), Project of China Knowledge Centre for Engineering Science and Technology, "LENOVO-XJTU" Intelligent Industry Joint Laboratory Project, Research Impact Fund (No.R1015-23), Collaborative Research Fund (No.C1043-24GF), APRC - CityU New Research Initiatives (No.9610565, Start-up Grant for New Faculty of CityU), CityU - HKIDS Early Career Research Grant (No.9360163), Hong Kong ITC Innovation and Technology Fund Midstream Research Programme for Universities Project (No.ITS/034/22MS), Hong Kong Environmental and Conservation Fund (No. 88/2022), SIRG - CityU Strategic Interdisciplinary Research Grant (No.7020046), and Tencent (CCF-Tencent Open Fund, Tencent Rhino-Bird Focused Research Program).