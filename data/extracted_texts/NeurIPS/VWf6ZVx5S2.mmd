# Transforming Vision Transformer: Towards Efficient Multi-Task Asynchronous Learning

Hanwen Zhong\({}^{1,2}\) Jiaxin Chen\({}^{1,2}\) Yutong Zhang\({}^{1,2}\) Di Huang\({}^{2}\) Yunhong Wang\({}^{1,2}\)

\({}^{1}\)State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China

\({}^{2}\)School of Computer Science and Engineering, Beihang University, Beijing, China

{hanwenzhong,jiaxinchen,ytzhang_mq,dhuang,yhwang}@buaa.edu.cn

Corresponding author.

###### Abstract

Multi-Task Learning (MTL) for Vision Transformer aims at enhancing the model capability by tackling multiple tasks simultaneously. Most recent works have predominantly focused on designing Mixture-of-Experts (MoE) structures and integrating Low-Rank Adaptation (LoRA) to efficiently perform multi-task learning. However, their rigid combination hampers both the optimization of MoE and the effectiveness of reparameterization of LoRA, leading to sub-optimal performance and low inference speed. In this work, we propose a novel approach dubbed Efficient Multi-Task Learning (EMTAL) by transforming a pre-trained Vision Transformer into an efficient multi-task learner during training, and reparameterizing the learned structure for efficient inference. Specifically, we firstly develop the MoEfied LoRA structure, which decomposes the pre-trained Transformer into a low-rank MoE structure and employ LoRA to fine-tune the parameters. Subsequently, we take into account the intrinsic asynchronous nature of multi-task learning and devise a learning Quality Retaining (QR) optimization mechanism, by leveraging the historical high-quality class logits to prevent a well-trained task from performance degradation. Finally, we design a router fading strategy to integrate the learned parameters into the original Transformer, archiving efficient inference. Extensive experiments on public benchmarks demonstrate the superiority of our method, compared to the state-of-the-art multi-task learning approaches. The project page is available at https://github.com/Yewen1486/EMTAL.

## 1 Introduction

Multi-task learning (MTL) [1; 2; 3] for Vision Transformer (ViT) aims at simultaneously learning multiple tasks, which has gained popularity in the computer vision community recently. It solves multiple relevant problems through sharing feature representations and forming a unified multi-task learner, thus enhancing the training and inference efficiency, and reducing the storage overhead. Moreover, by virtue of a well-designed MTL framework, the performance of each task can be further improved. Due to these merits, MTL has been used in a wide range of applications, such as the scene understanding [4; 5] and the erudite fine-grained recognition .

Despite both theoretical  and practical [8; 9] validations of their potential on enhancing the model generalizability, conventional MTL approaches [6; 10] often suffer performance degradation when compared to training tasks independently, which is primarily due to two reasons. Firstly, they adopt suboptimal learning strategies leading to conflicting task gradients and varying loss scales , which increase competitive interference during task optimization. Secondly, their model structures often fail to extract representative features for each task when utilizing a shared backbone network [12; 4; 6; 5].

Several recent works have attempted to deal with the above issues, which concentrate on the following two aspects. 1) _Efficient multi-task learners_. As shown in Figure 2, instead of designing complex but incompact network structures  that incur a large number of tunable parameters, recent works  such as MLoRE and MOELoRA explore the advantages of Mixture-of-Experts (MoE) in extracting task-specific features by enhancing the diversity of parameters and features , and Parameter-Efficient Fine-Tuning (PEFT) in reducing the tunable parameters and storage overhead . Nevertheless, MLoRE  still relies on a substantial number of additional parameters, limiting the overall efficiency and feasibility of training. MOELoRA  adopts a unitary LoRA structure to tune the experts, which weakens the learning capability of individual experts. Moreover, both methods utilizes task-driven routers, requiring either a static network with a fixed number of tasks or a dynamic routing network. The former results in significant storage overhead, while the latter increases the inference cost. 2) _Multi-Task Optimization (MTO) strategies_. Existing works on MTO can be broadly categorized into the gradient-based methods and the loss-based methods. The gradient-based methods  seek to balance the gradients across multiple tasks in the last shared layer, by decreasing differences in their magnitude or direction, and aggregating sub-gradients into a unified one. The loss-based methods  optimize the MTL process by balancing the multi-task losses. Recently, IMTL  is proposed to treat all tasks equally without bias, while AMTL  synchronizes learning progress across tasks. However, as each task has its own intrinsic optimization pace due to varying levels of training difficulty for distinct tasks, forcing synchronization in MTO disrupts these inherent properties, thus leading to suboptimal solutions. For more detailed discussion of related works, we refer to Appendix A.

To overcome the drawbacks of existing works, we propose a novel MTL framework dubbed Efficient Multi-Task Asynchronous Learning (EMTAL). Basically, EMTAL consists of the MoEfied LoRA structure, the Quality Retaining (QR) optimization mechanism and the router fading strategy, of which MoEfied LoRA decomposes a pre-trained Vision Transformer model into an efficient multi-task learner, QR accomplishes asynchronous learning of multi-task knowledge and enable establishing an efficient unified model by combining with the router fading strategy. Specifically, inspired by MoEfication , the proposed MoEfied LoRA firstly decomposes the FFN layer of Vision Transformer into a MoE structure by clustering similar channels into experts, creating specialized low-rank experts as demonstrated in Figure 1. Considering the inherent low-rank property of each expert, LoRA is naturally employed to perform efficient training of MoE. Subsequently, in order to

Figure 1: _FFN as Mixture of Low-rank Experts_. Given an up-projection weight matrix in FFN, a straightforward way of splitting it into MoE is to divide every \(K\) channels into separate experts, resulting in highly dissimilar experts and a high-rank MoE, which is inherently unsuitable for integration with LoRA. In contrast, our proposed MoLE approach rearranges the weight matrix into groups of similar channels as experts, creating specialized low-rank experts that are better suited for integrating with LoRA.

achieve asynchronous learning of multi-task knowledge based on the MoEfied LoRA module, QR constrains logits of early converged tasks to retain near the optima when continuously optimizing the insufficiently converged tasks, thus avoiding severe interference between tasks and improving multi-task optimization. Finally, the router fading strategy is combined with MoEfied LoRA and QR by gradually diminishing the router's fole in the last training epochs, seamlessly integrating learned parameters into the model structure without incurring extra inference time cost and storage overhead.

The main contributions of this paper are summarized as follows. 1) We propose a novel efficient multi-task learning framework dubbed EMTAL. To the best of our knowledge, our work makes the first investigation on decomposing a pre-trained Vision Transformer model for multi-task learning and reparameterizing the learned multi-task knowledge into a unified model. 2) We design a MoEfied LoRA structure, a QR multi-task optimization mechanism combined with a router fading strategy to accomplish an efficient asynchronous multi-task learner. 3) We extensively evaluate the proposed method on challenging multi-task fine-grained visual classification datasets and the VTAB benchmark, and the experimental results demonstrate that our method significantly improves the performance of single-task learning and the state-of-the-art MTL approaches.

## 2 The Proposed Approach

In this section, we mainly introduce the preliminary concepts of Vision Transformer, and describe the technical details of the proposed EMTAL approach.

### Preliminary

Given an input image \(I^{3 H W}\), a standard Vision Transformer  model with \(L\) layers first divides \(I\) into \(m\) non-overlapping patches, which are further fed into a patch embedding layer, generating \(m\)\(D\)-dimensional visual tokens. After concatenating with a class token, the input tokens are finally formed as \(}^{(1+m) D}\). Each transformer layer contains a Multi-headed Self-Attention (MSA)  block, a Feed-Forward Networks (FFN) block and a Layer Normalziation (LN). The tokens of the \(l\)-th layer are generated based on those in the (\(l-1\))-th layer formulated as below:

\[}^{}=((^{l-1}))+ ^{l-1},\ \ ^{l}=((}^{}))+ }^{}.\] (1)

Figure 2: Summary of representative architectures of multi-task learning.

Similar to [30; 31], our work mainly focuses on FFN, which usually consists of two linear layers \(\{_{up}^{D(r D)},_{up}^{r D}\}\), \(\{_{down}^{(r D) D},_{down}^{D}\}\) and a GELU activation operation, where \(r\) represents the scaling factor. Accordingly, FNN processes the normalized input \(_{n}^{l\,}=(^{l\,})\) as follows:

\[_{}^{l}=(_{n}^{l\,}_{up}+ _{up})_{down}+_{down}.\] (2)

This procedure ensures the effective transformation and projection of the input through the encoder layers, enabling the prediction of the class probability distribution \(y\) for downstream tasks.

### Framework Overview

As shown in Figure 3 (a), in order to establish a unified model for \(N_{T}\) tasks, we follow  by unifying the label spaces for multiple tasks into an overall one with \(N_{class}\) classes, and merging the training samples as \(S=_{t=1}^{N_{T}}S_{t}\), where \(S_{t}\) denotes the set of training samples for the \(i\)-th task. Supposing a pre-trained Vision Transformer with an embedding backbone \((;_{}):\), where \(_{}\) represents the parameters to be frozen in the network, it maps an input \(\) to the feature space \(\). We decompose it into an efficient multi-task learner dubbed **MoEfled-LoRA** denoted by \(^{}(;[_{};_{t}])\), where \(_{t}\) indicates the set of newly employed tunable parameters and \([]\) refers to the concatenation operation. Subsequently, the **Quality Retaining** multi-task optimization mechanism as well as the router fading strategy are applied to asynchronously learning the tunable parameters \(_{t}\). Finally, we reparameterize \(_{t}\) into the original backbone, achieving an efficient unified model \((;_{}^{})\).

### MoEfled-LoRA

Basically, MoE-based learners benefits MTL in the following two ways. First, they enable dynamic encoding of different samples across tasks via a router and multiple experts, significantly enhancing feature diversity. Second, they reduce the number of parameters and computational cost, remarkably promoting the training and inference efficiency. However, early attempts include delicately designed MMoE , M\({}^{3}\)ViT  and Mod-Squad  fail to establish a unified structure and inevitably introduce additional inference overhead, considering that LoRA increases inference latency by 20-30% without reparameterization . Recently, MoEfication methods [30; 31] aimed at "_group together the neurons that are often activated simultaneously_" have demonstrated promising performance in constructing effective MoE structures from pre-trained ViT models. Inspired by this, in this work we attempt to "_group together the neurons with similar weights_" to construct the MoE structure. As the corresponding experts naturally meet the low-rank conditions in LoRA , we can therefore obtain an excellent efficient multi-task learner by combining with LoRA. By employing a router fading strategy to preserve the reparameterization property of LoRA, we can further eliminate additional inference latency.

Figure 3: Illustration of the proposed EMTAL framework. Given a pre-trained ViT, we firstly decompose it into a MoE-based multi-task learner by using the balanced k-means. LoRA is then applied to the low-rank experts, creating an efficient multi-task learner dubbed MoEfied LoRA. During multi-task optimization, the Quality Retaining is employed to maintain the high-quality knowledge for tasks that have already converged. Finally, with the aid of the router fading strategy, the learned knowledge is reparameterized back into the pre-trained ViT, eliminating the extra inference cost.

Specifically, as shown in Figure 3 (b), we firstly decompose the FFN into a MoLE (Mixture of Low-rank Experts) structure. To convert the FFN in the \(l\)-th layer into MoE, we draw on insights from [35; 36], which view the FFN as a memory bank that retains knowledge from pre-trained models. Each column of the up-projection matrix \(_{up}\) serves as a key to be matched, which each row of the down-projection matrix \(_{down}\) act as the corresponding value. Since similar keys in the \(_{up}\) tend to serve similar functions, they should be grouped within the same expert. Formally, \(_{up}\) is clustered column-wise into \(K\) clusters by using the balanced \(k\)-means , with a mapping function \(C()\) that assigns the \(idx\)-th column to cluster \(C(idx)\). Therefore, columns within the same cluster constitute an expert. Additionally, \(_{up},_{down}\) should match \(_{up}\) channel by channel, and also adhere to the clustering results of \(_{up}\) to construct the experts \(\{^{i}\}_{i=1}^{K}\). Consequently, they are concatenated for the MoEfication process and then split to obtain the corresponding experts, formulated as below:

\[=[_{up};_{up};_{down}^{}],^{(2D+1)(rD)},\] (3)

\[^{i}=_{\{idx|C(idx)=i\}},i 1,2,,K,^{i}^{ (2D+1)()},\] (4)

\[^{i}_{up},^{i}_{b},^{i}_{down}=^{i}_{1:D},^{i}_ {D},^{i}_{D+1:2D+1}.\] (5)

Since the column vectors within these experts are relatively similar, exhibiting low-rank characteristics, they naturally satisfy the low-rank conditions in LoRA . Therefore, we apply a set of LoRA parameters \(\{^{i}_{up},^{i}_{up}\}\) and \(\{^{i}_{down},^{i}_{down}\}\) for \(^{i}_{up}\) and \(^{i}_{down}\) to enable more efficient learning and improved performance, which is referred as MoLE and formally described as below:

\[^{i}_{up}{}^{}=^{i}_{up}+^{i}_{up}^{i}_{up},\] (6)

\[^{i}_{down}{}^{}=^{i}_{down}+^{i}_{down}^{i}_{ down}.\] (7)

Moreover, to leverage the benefits of dynamic routing for extracting diverse features during training, we initially establish a sample-driven soft router for each MoLE, denote as \(_{r}^{D K}\), to reweight the experts. For the \(l\)-th layer of MoLE, we calculate weights for each experts as \(^{l}\) by adopting the following formulation:

\[^{l}=K((^{l}{}^{ })\,_{r}^{l}}{}),\] (8)

During training, the router is employed to fully optimize the MoLE. The output \(^{l}_{}\) of the decomposed FFN is calculated as follows:

\[^{l}_{}=_{i=1}^{K}([^{l}_{1}( ^{l}{}_{n}^{}^{1}_{up}{}^{}+^{1}_{b});; ^{l}_{K}(^{l}{}_{n}^{}^{K}_{up}{}^{}+ ^{K}_{b})])^{i}_{down}{}^{}+_{down}.\] (9)

### Quality Retaining Optimization

The goal of multi-task learning (MTL) is to ensure that the final model performs well across all tasks. However, due to distinct task difficulties and individual optimization schedules, achieving optimal performance on all tasks simultaneously with a single model can be challenging. As displayed in Figure 3 (d), despite introducing strong priors to synchronize the optimization schedules across tasks, existing methods [26; 29; 25] can disrupt the inherent schedules of tasks with varying difficulties, creating challenges for MTL optimization. Therefore, we propose a different perspective: maintaining the inherent optimization pace of each task is crucial. Specifically, we allow asynchronous convergence of tasks and introduce the Quality Retaining (QR) MTO strategy to preserve high-quality knowledge from already converged tasks during subsequent optimization.

Specifically, at iteration \(iter\), we maintain an optimal knowledge bank \(^{N_{clasar} N_{class}}\), which records the Exponential Moving Average (EMA) logits of each class learned during the optimization process from iteration \(0\) to \(iter-1\). This knowledge is distilled into the currently optimized model using a distillation loss. Formally, we maintain the knowledge bank \(\) using EMA. For a sample \(s\) in the current training batch with label \(label_{s}\), we update \(^{iter}_{label_{s}}\) as follows:

\[^{iter}_{label_{s}}=m^{iter-1}_{label_{s}}+(1-m)_ {s},\] (10)where \(_{s}\) indicates the logits of sample \(s\) and \(m(0,1)\) is a momentum coefficient. This results in a real-time updated knowledge repository \(\).

To ensure the retention of high-quality knowledge for already optimized tasks, we employ a straightforward method, _i.e._, weighting the distillation process by the reciprocal of the loss from each task. This procedure implies that tasks with lower loss (already optimized) should rely more on the learned knowledge, while tasks with higher loss (still being optimized) will depend more on the ground truth. Therefore, the Quality Retaining loss \(L_{QR}\) for samples within a mini-batch \(S_{batch}\) is defined as below:

\[_{QR}=_{t=1}^{N_{T}}_{CE,t}}_{s S _{batch}}((_{s}),(_{label_{s}}))(s S_{t}),\] (11)

where \(()\) and \(\) indicates the Kullback-Leibler divergence  and the indicator function, respectively. \(_{CE,t}\) is the Cross-Entropy loss for the \(t\)-th task.

Based on Eq. (11), the overall training loss is formulated as \(=_{t=1}^{N_{T}}_{CE,t}+_{QR}\). This strategy ensures that the model maintains optimal performance for already converged tasks while allowing other tasks to continue their optimization at their inherent pace.

### Router Fading and Insights on the Unified Model Structure

MTL aims to develop a universal model capable of executing multiple tasks simultaneously. To achieve this goal, we transform the unified pre-trained model into an MoEfied LoRA and develop the quality retaining mechanism to preserve multi-task knowledge as discussed in the previous sections. However, the dynamic routing in MoEfied LoRA limits LoRA's capability of reparameterizing the learned parameters into a unified, static pre-trained structure. To address this issue, we design a router fading strategy that gradually diminishes the router's role in the later stages of training. This approach allows the knowledge embedded within the optimized router to be implicitly absorbed as follows:

\[^{l}=*^{l}+(1-),\] (12)

where \(\) is the trade-off hyper-parameter. Finally, as shown in Figure 3 (e), we completely remove the router after training by setting \(=0\). In the mean time, we reparameterize knowledge learned by LoRA back using Eq. (6) and Eq. (7), and concatenate them to replace the original parameters \(_{up},_{up},_{down}\) of the \(l\)-th layer with \(}^{},^{}_{up},_{down}{}^{}\) as below:

\[}^{}=[}^{1}{}^{};}^{2}{}^{}; ;}^{K^{}}],\] (13)

\[}^{}=[}^{1}{}^{};}^{2}{}^{}; ;}^{K^{}}],\] (14)

\[}^{}=[}^{1}{}^{};}^{2}{}^{ };;}^{K}{}^{}].\] (15)

The above technique avoids extra computational costs and maintains a unified model structure, delivering a new perspective for multi-task learning.

## 3 Experimental Results and Analysis

### Datasets and Evaluation Metric

By following , we mainly evaluate the performance of our proposed EMTAL method on the challenging _Multi-task FGVC_ benchmark. In addition, we conduct experiments on the _Specialized VTAB-1k_ dataset to validate the effectiveness over previous solutions. _Multi-task FGVC_ is a collection of public datasets specifically for multi-task fine-grained visual classification, including CUB-200-2011 , Stanford Cars , FGVC-Aircraft  and Oxford Flowers . In order to make fair comparisons, we adopt the standard training/testing split as depicted in . _Specialized VTAB-1k_ consists of specialist images from specialized equipment, where we employ multi-task learning to fully leverage these expensively annotated data. We follow the standard training/validation splits used in  for fair comparisons. The top-1 accuracy is utilized as the evaluation metric. To further demonstrate the effectiveness of our method on the tasks of pixel-to-pixel dense prediction, we also conduct experiments on the NYUv2 dataset . Specifically, we integrate our method with TaskPromper  by applying MoEfied LoRA and QR to the FFN layers and the semantic segmentation task head, respectively.

In addition, we evaluate on _Multi-task FGVC_ for few-shot learning under 1, 2, 4, 8, and 16 shots, by following existing works [46; 47].

### Implementation Details

We utilize ViT-B/16 2 pre-trained on ImageNet-21K  as the base model. We use the AdamW optimizer  to fine-tune our models for 100 epochs and adopt the cosine learning rate decay with a linear warm-up for 10 epochs in all experiments. We fix the hyper-parameters \(\) in Eq. (8) to 5, since it exhibits stable performance with distinct values. As for data augmentation, we employ random resize cropping to 224 \(\) 224 pixels and a random horizontal flip during training and resize to 248 \(\) 248 pixels with a center crop to 224 \(\) 224 pixels. All experiments are conducted on a single Nvidia GeForce RTX 3090 GPU.

### Comparison with the State-of-the-Art Approaches

To comprehensively evaluate the performance of our approach, we compare with the MTL full fine-tuning baseline and the following categories of state-of-the-art approaches: 1) MTO Loss-based methods, including GLS  and AMTL ; 2) MTO Gradient-based methods, including Nash-MTL  and Aligned-MTL  combined with vanilla LoRA-16; 3) Efficient multi-task learners methods, including Dual-Prompt , Erudite , MLoRE  and MOELoRA . As the performance of EMTAL depends on the inherent low-rank properties, we report the results of our method using distinct ranks including 1, 2 and 4, denoted by EMTAL-1, EMTAL-2 and EMTAL-4, respectively.

As summarized in Table 1, the proposed EMTAL method consistently improves the performance at different ranks, promoting the top-1 accuracy of the Separate full-finetuning baseline by an average of 3.47%, 3.86% and 4.24%, respectively. More importantly, EMTAL tunes only a negligible amount of parameters (_i.e._, 1.20M) compared to the original model (_i.e._, 343.92M), and incurs no extra inference

    &  & Unified & CUB-200 & Stanford & FGVC- & Oxford &  & Tunable & Inference \\  & Model & -2011 & Cars & Aircraft & Flowers & & & & Time (ms) \\   \\ Separate FT & Baseline & & 86.4 & 87.6 & 77.2 & 98.8 & 87.49 & 343.92 & 14.30 \\ Union FT  & Baseline & & 83.1 & 90.7 & 78.3 & 97.5 & 87.39 & 85.98 & **7.15** \\   \\ Nash-MTL  & ICLR’ 22 & ✓ & 88.3 & 90.2 & 80.6 & 99.5 & 89.65 & 2.82 & **7.15** \\ Aligned-MTL  & CVPR ’23 & ✓ & 88.9 & 90.6 & 81.6 & **99.7** & 90.17 & 2.82 & **7.15** \\  \\ GLS  & CVPR ’19 & ✓ & 88.4 & 90.1 & 80.0 & 99.6 & 89.55 & 2.82 & **7.15** \\ AMTL  & ICCV ’23 & ✓ & 88.2 & 90.7 & 81.5 & **99.7** & 90.04 & 2.82 & **7.15** \\   \\  \\  \\ Dual-Prompt  & ECCV ’22 & ✗ & 87.8 & 73.5 & 53.1 & 99.4 & 78.4 & 1.1 & 15.48 \\ Erudite  & CVPR ’23 & ✗ & 79.7 & 81.4 & 70.2 & 98.1 & 82.35 & 101.34 & 9.74 \\ MLoRE  & CVPR ’24 & ✗ & 74.8 & 59.5 & 49.9 & 99.2 & 70.76 & 188.01 & 42.02 \\ MOELoRA  & SIGIR ’24 & ✗ & 88.4 & 88.2 & 75.0 & **99.7** & 88.04 & 2.82 & 38.71 \\   \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\ 

cost, implying that our framework is significantly more effective and efficient than the traditional separate training paradigm.

Moreover, EMTAL consists of a reparameterizable and efficient multi-task learner, a Quality Retaining MTO mechanism and a router fading strategy. Compared to the state-of-the-art methods, each component of our approach shows significant advantages. In terms of the design of MTL structures, MoEfied LoRA seamlessly integrates low-rank experts with LoRA, resulting in improved performance. Furthermore, the router fading strategy and the reparameterization effectively reduce inference time, significantly enhancing the overall efficiency. Meanwhile, by considering the intrinsic task-specific optimization pace, the QR mechanism clearly improves previous MTO strategies. Overall, it utilizes a sample-driven router and multiple experts to extract diverse feature representations while preserving high-quality knowledge during training, making it highly beneficial for multi-task learning. In this way, our method achieves the highest accuracy compared to the state-of-the-art approaches, surpassing the second best Aligned-MTL by 1.56% while tuning fewer parameters.

We further evaluate the performance on _Specialized VTAB-1k_, where MTL is highly beneficial against previous solutions. As displayed in Table 2, some existing works focus on applying parameter-efficient fine-tuning on each task individually to avoid over-fitting. However, we find that performance can be significantly enhanced by incorporating multi-task learning. Specifically, applying AMTL and MOELoRA on the vanilla LoRA yields improvements of 0.29% and 1.51%, respectively. Furthermore, when directly applying our EMTAL to these tasks, we observe consistent improvements across different ranks, achieving the highest accuracy compared to the state-of-the-art approaches. Notably, it enhances the overall performance by 1.43%, while utilizing fewer parameters. In addition, on the NYUv2 dataset, our method significantly enhances performance in semantic segmentation and depth estimation tasks, while achieving comparable results in surface normal estimation and object boundary detection tasks. Overall, this led to an average relative improvement of 1.57%, validating the effectiveness of our approach for pixel-level prediction tasks. We provide more results, including using self-supervised pre-trained model DINov2-large and the few-shot learning in Appendix B and Appendix C, respectively.

### Ablation Study

In this section, we evaluate the effectiveness of the proposed main components, _i.e._ MoEfied LoRA and Quality Retaining by extensive ablation studies.

   Method & Reference & Patch Camelyon & EuroSAT & Resisc45 & Retinopathy & Mean & Tunable Params. (M) \\   \\  Separate FT & Baseline & 79.7 & 95.7 & 84.2 & 73.9 & 83.38 & 343.36 \\ Union FT  & Baseline & 84.3 & 93.9 & 83.0 & 75.2 & 84.08 & 85.99 \\   \\ Adapter  & ICML! 19 & 76.3 & 88.0 & 73.1 & 70.5 & 76.98 & 1.08 \\ LoRA  & ICLR? 22 & 85.5 & 95.3 & 86.1 & 75.3 & 85.50 & 2.41 \\ VPT-D  & ECCV’ 22 & 81.8 & 96.1 & 83.4 & 68.4 & 82.42 & 2.40 \\ SSF  & Neuf’25 & **87.4** & 95.9 & 87.4 & 75.5 & 86.56 & 0.96 \\ SPIT- & ICCV’ 23 & 85.7 & **96.2** & 85.9 & 75.9 & 85.92 & 2.16 \\ ARC  & Neuf’PS’ 23 & 84.9 & 95.7 & 86.7 & 75.8 & 85.78 & 0.52 \\   \\ AMTL  & ICCV’ 23 & 86.4 & 95.0 & 85.8 & 75.9 & 85.79 & 2.41 \\ MOELoRA  & SIGIR’ 24 & 86.9 & 96.1 & 88.4 & 76.7 & 87.01 & 2.41 \\  \\  \\  \\  \\  \\   

Table 2: Comparison results (%) with the state-of-the-art PEFT and MTL methods on Specialized VTAB-1k by using ViT-B/16 models supervised pre-trained on ImageNet-21K. ‘FT’ denotes ‘Full Fine-tuning’. The best results are highlighted in **bold** and the second best is underlined.

   Method & Semseg mIoU \(\) & Depth RMSE \(\) & Normal mErr \(\) & Boundary odsF \(\) & Mean \(\) (\%) \(\) \\  TaskPrompter-Base  & 50.40 & 0.5402 & **18.91** & **77.60** & - \\
**+ EMTAL (Ours)** & **52.90** & **0.5284** & 18.95 & 77.10 & **1.57** \\   

Table 3: More evaluation results on NYUv2 with ViT-B/16. The best results are highlighted in **bold**.

**Effect of the Main Components.** We evaluate the proposed components across the _Multi-task FGVC_ Benchmark based on ViT-B/16. As Table 4 shows, MoEfied LoRA consistently boost the performance, achieving an average of 2.88% improvement with less tunable parameters. The results indicate that MoEfied LoRA is significantly beneficial to multi-task learning by providing more diverse features. In the mean time, the Quality Retaining MTO mechanism can further remarkably promote the accuracy, with a 3.65% improvement on average. A combination of these two components, _i.e._ MoEfied LoRA and QR, further boosts the overall performance across datasets, implying that MoEfied LoRA and Quality Retaining are complementary in multi-task learning.

**On MoEfied LoRA and Router Fading.** We further validate the effectiveness of MoEfied LoRA and the router fading strategy across the _Multi-task FGVC_ benchmark based on ViT-B/16. Initially, we begin by decomposing the pre-trained model into a MOLE structure. A straightforward clustering and splitting of the FFN, combined with a sample-driven router, can achieve a 1.81% improvement by enhancing the the diversity of feature representations. Furthermore, applying LoRA to the low-rank experts yields a 0.94% gain in performance, as the small amount of tunable parameters reduces the risk of overfitting, and the the low-rank property of the experts aligns well with LoRA. Additionally, the proposed router fading strategy gradually diminishes the influence of the router over 50 epochs, effectively preserving the reparameterizable nature of LoRA and reducing the inference time.

Moreover, we conduct more ablation studies on the proposed MoEfied LoRA. The number of clusters \(k\) significantly influence the performance of MoEfied LoRA, considering that a large number of clusters intends to incur simple experts with few channels, and a small number of clusters results in high-rank experts, either of which degrades the effectiveness of MoEfied LoRA. We empirically study the effect of \(k\) by using 1, 4, 6, 64 and 192 clusters. As Table 6 displays, MoEfied LoRA reaches the highest accuracy when \(k=16\). Moreover, we compare different ways to construct experts, including the co-activation clustering  that groups weights based on activations for each channel and the gradient-cluster  that clusters weights according to cumulative gradients. As shown in Table 6, our method achieves the best performance, clearly demonstrating its effectiveness.

    &  &  \\  & 1 & 4 & 16 & 64 & 192 & Co-activation & Gradient-cluster & **Ours** \\  Params. (M) & 1.05 & 1.09 & 1.20 & 1.64 & 2.82 & 1.20 & 1.20 & 1.20 \\ Mean Acc & 88.83 & 89.12 & **90.27** & 89.34 & 89.02 & 89.30 & 89.34 & **90.27** \\   

Table 6: Ablation results (%) of the proposed MoEfied LoRA with different numbers of clusters (_i.e._\(k\)) and distinct ways to construct experts.

    &  &  & Stanford & FGVC- & Oxford &  &  \\  & & -2011 & Cars & Aircraft & Flowers & & Params. (M) \\  ✗ & ✗ & 83.1 & 90.7 & 78.3 & 97.5 & 87.39 & 86.26 \\  ✓ & ✗ & 88.5 & 91.3 & 81.5 & **99.7** & 90.27 & **1.20** \\ ✗ & ✓ & 88.5 & 91.6 & 84.4 & 99.6 & 91.04 & 86.26 \\ ✓ & ✓ & **89.8** & **92.3** & **85.2** & **99.7** & **91.73** & **1.20** \\   

Table 4: Ablation results (%) of the main components on the Multi-task FGVC by using ViT-B/16 backbone. The best results are highlighted in **bold**.

    &  & Stanford & FGVC- & Oxford &  &  &  \\  & -2011 & Cars & Aircraft & Flowers & & & \\  Union FT & 83.1 & 90.7 & 78.3 & 97.5 & 87.39 & 86.26 & **7.15** \\  +MoEfied & 85.2 & 91.3 & 80.8 & 98.7 & 89.20 & 86.40 & 13.87 \\ +LoRA  & 88.2 & 91.0 & **81.7** & 99.6 & 90.14 & **1.20** & 13.87 \\ +Router fading & **88.5** & **91.3** & 81.5 & **99.7** & **90.27** & **1.20** & **7.15** \\   

Table 5: Ablation results (%) of the MoEfied LoRA and router fading strategy on Multi-task FGVC by using ViT-B/16 backbone. The best results are highlighted in **bold**.

### Visualization on the Low-rank Property of MoLE

As shown in Figure 4 (a), we measure the low-rank properties of experts by using the Ky Fan 2-k norm , and the results indicate that the experts generated by our method consistently exhibit statistically more significant low-rank properties across distinct ranks. Additionally, we analyze the low-rank properties of experts across different layers of ViT when the ranks is fixed as \(1\). As Figure 4 (b) demonstrates, the low-rank properties of experts are more significant in lower layers that those in higher layers.

We kindly refer to Appendix E for more detailed discussion about the broader impacts and limitations of our work.

## 4 Conclusion

In this paper, we focus on decomposing a pre-trained Vision Transformer model for multi-task learning, reparameterizing the learned multi-task knowledge back into the original model and establishing a unified model. We propose a novel efficient multi-task learning framework dubbed EMTAL, which mainly consists of the MoEfied LoRA module, the Quality Retaining (QR) mechanism and the router fading strategy. Concretely, MoEfied LoRA decomposes a pre-trained ViT into multi-task learners by clustering similar weight of FFN into experts and applies LoRA to tune the experts with low-rank properties. Subsequently, we leverage the inherent asynchronous convergence property of tasks and employ QR to preserve the optimized performance for converged tasks. Finally, the router fading strategy is introduced to eliminate extra inference cost. Extensive experiments on the public benchmarks demonstrate that our method substantially promotes performance by comparing with the state-of-the-art multi-task learning approaches, while also being effective in few-shot learning with limited data. Our work delivers a new perspective for efficient multi-task learning by decomposing a pre-trained model and reparameterizing back with a low rank updating.