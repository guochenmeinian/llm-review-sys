# Model Tells You What to Discard:

Adaptive KV Cache Compression for LLMs

 Suyu Ge\({}^{1}\), Yunan Zhang\({}^{1}\), Liyuan Liu\({}^{2}\), Minjia Zhang\({}^{2}\), Jiawei Han\({}^{1}\), Jianfeng Gao\({}^{2}\)

\({}^{1}\)University of Illinois Urbana-Champaign, \({}^{2}\)Microsoft

{suyuge2,yunanz2,hanj}@illinois.edu

{lucliu,minjiaz,jfgao}@microsoft.com

Authors contributed equally to this research.

###### Abstract

In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial GPU memory reduction with negligible generation quality loss2.

## 1 Introduction

Autoregressive language models have attracted extensive attention (OpenAI, 2023; Touvron et al., 2023). Along with the remarkable success achieved by these models, their computational complexity and GPU memory consumption present significant challenges (Shazeer et al., 2017), underscoring an imperative need for serving these models in an economically feasible manner.

The generative inference of LLMs usually involves using the _KV Cache_ mechanism to enhance the generation speed. KV cache stores previously computed Key/Value vectors in attention calculation and reuses those values for the current token generation. As such, it avoids recalculations of previous tokens at each token generation step at the cost of extra memory consumption. Despite being a prominent technique, the memory consumption of KV cache increases rapidly as the model size and generation length increase, drastically increasing the pressure of on-device memory.

When memory usage exceeds GPU capacity, the generative inference of LLMs typically resorts to offloading (Aminabadi et al., 2022; Sheng et al., 2023). However, offloading KV cache to CPU/NVMe adds non-trivial overheads, due to the limited PCIe bandwidth between the GPU and CPU on many devices. Therefore, it is crucial to reduce the memory footprint of KV cache.

Our study starts from the observation that there are abundant structures observed in attention modules (Michel et al., 2019; Voita et al., 2019; Clark et al., 2019; Wang et al., 2020; Child et al., 2019), and as in Figure 1, not all attention modules need to attend to all tokens (Liu et al., 2023; Zhang et al., 2023; Liu et al., 2023). Intuitively, harvesting such structures could substantially reduce memory consumption and accelerate text generation.

Based on this intuition, we propose FastGen to _accelerate the generative inference by adaptively compressing the KV cache on the fly_. First, we employ an efficient profiling algorithm to recognize the structural patterns for attention modules. Under the guidance of this profiling, we then construct the KV cache for various modules adaptively. Specifically, FastGen recognizes five fundamental attention structures: some attention heads mostly attend to local contexts, for which we construct a KV cache that evicts long-range contexts; some primarily attend to specific tokens/puncutations, for which we create a KV cache that retains only special tokens/puncutations; some have attention maps that are column-wise sparse, for which we discard the least frequently attended tokens; and some broadly attend to all tokens, for which we employ the standard KV cache and store all tokens.

In this way, FastGen is able to compress the KV cache while retaining the original functionality of attention modules, thus does not require any fine-tuning and can be applied in a plug-and-play manner. With a suite of major benchmarks covering generative tasks in math, code, knowledge, and common sense reasoning, we evaluate FastGen on LLaMa (Touvron et al., 2023b) and observe FastGen brings significant improvements over nonadaptive KV cache compression baselines.

``` Input: Feasible Policy Set (\(\)), Prompt Output: Adaptive KV Cache
1forAttention Head \(H_{i}\) in LLMdo
2\(^{i},^{i},^{i} H_{i}()\)
3\(^{i}(^{i}^{i^{T}})\)
4\(^{i}^{i}\) /* \(}^{i}\): optimal policy
5\(^{i}_{^{i}},^{i}_{^{i}} f(^{i}, ^{i},^{i})\)
6\(}^{i},}^{i}^{i}_{^{i}},^{i} _{^{i}}\)
7return\(\{^{i},}^{i},}^{i}\}\) ```

**Algorithm 1**FastGen-Prompt Encoding.

Figure 1: Different attention heads usually have different structures. Left: Four common attention structures. Right: Attention map compositions of three attention heads that are in the same layer.

Figure 2: Performance of Adaptive KV Cache (FastGen) and Nonadaptive KV Cache (Frequency, Local, and Frequency+Local; Zhang et al., 2023 and Liu et al., 2023a) on AlpacaEval.

Related Work

**Token Dropping and KV Cache Compression.** Many efforts have been made to improve the model efficiency for LLMs. For recurrent neural networks, one method is to skip multiple tokens at a given time step (Campos et al., 2017; Seo et al., 2017; Hansen et al., 2019). Since Transformer models quickly attracted lots of attention, Goyal et al. (2020) proposes to eliminate redundant words in BERT (Devlin et al., 2019) based on their attention scores, while Dai et al. (2020) compresses the input sequence by adding pooling layers to the encoding modules of the transformer architecture. Recently, Huang et al. (2022) adds a token selection task to the original BERT model that learns to select performance-crucial tokens, and Kim et al. (2022) designs a learnable threshold to detect unimportant tokens to prune. Meanwhile, many efforts have been made to explore the possibility of compressing the hidden state of tokens rather than explicitly reducing the sequence length (Guan et al., 2022; Sun et al., 2022; Zhou et al., 2020).

Nevertheless, these methods can only be applied to non-autoregressive models and typically require an additional re-training phrase, making them less suitable for large LLMs like ChatGPT and LLaMa. Recognizing this gap, researchers started examining the potential of pruning tokens within the KV cache of autogressive LLMs. Mu et al. (2023) learns to compress the prompts into a few special tokens to reduce memory pressure during caching. However, the token prediction requires training and could be an expensive overhead during inference. Meanwhile, several concurrent methods propose to leverage accumulated attention score as the criteria to identify important tokens in the KV cache (Sheng et al., 2023; Zhang et al., 2023; Liu et al., 2023). Our work, instead of investigating a specific eviction policy, aims to synergistically coordinate diverse eviction policies, adapting them to align more closely with model-specific attributes.

**Underlying Structure of Attention.** Inspired by the success of Transformer, extensive studies have been conducted to explore the underlying mechanism of different self-attention heads. Voita et al. (2019) analyzed the self-attention heads in BERT (Devlin et al., 2019) using LRF (Bach et al., 2015) and characterized them into interepretable roles, one of which is attending adjacent tokens all the time. Michel et al. (2019) demonstrated that even heads in the same layer could have different impact on the performance while the importance of each head change across tasks. Clark et al. (2019) and Kovaleva et al. (2019) identified the patterns that some heads primarily attend to separator tokens, adjacent tokens and a combination of these. We observe consistent patterns in decoder-only models, despite previous studies are mainly done on encoder models. Our work shares similar spirits with these studies but focus on characterizing the KV cache of different attention heads.

## 3 Adaptive KV Cache Compression

Typical generative inference involves two stages. In **prompt encoding**, LLMs encode prompt inputs.For autoregressive transformers, when generating the \(i\)-th token, the attention module will reference information from all the preceding \(i-1\) tokens. To convey such contextual information, the attention module processes both key and value vectors of the preceding \(i-1\) tokens. To circumvent redundant KV vectors computations when generating succeeding tokens, all key and value vectors are retained once encoded, collectively termed as the KV cache. In **token generation**, LLMs conduct incremental token generation. In the process, the associated key and value vectors of newly generated tokens are appended to the KV cache.

As highlighted in Appendix 2, various efforts have sought to compress the KV cache to minimize memory usage and boost generation speed. Yet, these methods often overlook the intricate attention structure in LLMs. As detailed in Appendix 4, attention heads in these models often function distinctively, indicating the need for tailoring compression strategies to each attention head.

In this study, we introduce FastGen: a dual-phase algorithm for crafting an adaptive KV cache.

### Model Profiling

FastGenconducts model profiling based on the result of prompt encoding. Specifically, for a compression policy \(\), we mark the corresponding KV cache compression as \(},}=f(,,)\), where \(}\) and \(}\) are the compressed KV cache. Then, for attention map \(=(^{T})\)we pick the optimal policy based on the following objective.

\[^{*}=*{arg\,min}_{}()|-(_{}^{T})| 1-T,\] (1)

where \(\) is the set of all feasible compression policies, \(()\) is the target KV cache budget of the compression policy \(\), and \(T\) is a predefined hyper-parameter representing how much we want the policy to recover \(\). The final profiling algorithm is outlined as Algorithm 1.

Intrinsically, our method assumes that, for all attention heads, the structure of the attention map is stable at different positions, i.e., it is sufficient to use only the encoded prompt for picking the proper compression policy. It is worth mentioning that, existing literature has provided theoretical justification for using only the encoded prompts to capture attention structures for the full contexts (Zhang et al., 2023; Liu et al., 2023). In our study, we also have provided empirical verification for this assumption (elaborated in Appendix 4).

### KV Cache Compression Policies

We observe that a large number of attention heads closely followed certain patterns (elaborated in Appendix 4). While FastGen's design accommodates an expansion to incorporate other strategies, we consider four KV cache compression policies besides the conventional full KV cache (\(_{}\)):

* \(_{}\) only keeps special tokens, e.g., begin-of-sentence token \(<\)s\(>\), instruction token [INST].
* \(_{}\) exclusively preserves punctuation tokens including "\(\)", "\(\)", "?", etc.
* \(_{}\) evicts long-range contexts, i.e., once the relative distance between the context token and the current token exceeds a threshold, the KV cache of the context token will be evicted. The length budget of local context over the current sequence length is controlled by a ratio \(r_{l}\).
* \(_{}\) was proposed in multiple studies (Sheng et al., 2023; Zhang et al., 2023; Liu et al., 2023). It operates like a least-frequently-used cache by equating the cumulative attention score to a token's frequency. The cache budget over the sequence length is controlled by a ratio \(r_{f}\).

Naive Strategy Combination.In practice, we noticed that it is necessary to combine those above-mentioned strategies. However, the total number of the resulting hybrid policy grows at a combinatorial speed. Here, we employed a naive strategy to construct hybrid-policies. Specifically, together with the conventional full KV cache, we consider the following feasible compression policies:

\[=\{_{},_{},_{ },_{}, _{}\},\] (2)

where the sum of two compression strategies is to compute the union of their compressed KV cache. More discussions on the strategy priorities are included in Appendix 5.5. The final algorithm for token generation is outlined as Algorithm 2.

## 4 Diversity and Stability of Attention Structures

In this section, we present empirical verifications on our intuition that KV cache compression can and should be conducted adaptively. Specifically, we aim to first demonstrate that different attention heads typically possess distinct structures; and then, to show that these attention head structures remain relatively consistent. For this purpose, we analyze the attention scores of LLaMa 65B using random samples from GSM8k (Cobbe et al., 2021) as our case study.

### Head Distinctive Attention Structure

**Setting.** We apply our proposed profiling method (with a recover threshold of \(0.95\)) and compute the distribution of profiling results for \(\{1,10,20,30,40,50,60,70,80\}\) layers. The resulting distribution is visualized in Figure As in Figure 3.

**Observation.** As in Figure 3, attention heads in different layers have vastly different structures. Specifically, for the initial and final layers, they have more attention heads assigned to the full KV cache, indicating more attention heads of these layers broadly attend to all tokens. Meanwhile, for middle layers, the attention map focuses more on special tokens, indicating most attention heads of these layers primarily attend to special tokens (i.e., the accumulated attention score on special tokens is higher than \(0.95\) for these attention heads). As in Figure 1, we also provide a case study to illustrate the structure of different attention heads in the same layer, which demonstrates that the attention structure differs across different layers and different heads.

Correspondingly, it is suboptimal to follow the same pattern and apply the same KV cache to all layers without adaptation. Also, it could be beneficial to first diagnose the cache of each attention head before deciding how to construct the cache. We'll further discuss the benefit of this diagnose-to-compress strategy in the experiment section.

### Profile Tends to Be Consistent in One Sequence

The previous section demonstrates the great potential for constructing an adaptive KV cache in accordance with the structure of different attention heads. Here, we aim to demonstrate that, it is sufficient to leverage only the user-provided prompts and conduct one-shot profiling, as outlined in Section 3. Specifically, we aim to illustrate that user-provided prompts share the same attention structure in the generation process.

Figure 4: Accumulated Attention Score at 1st (prompt encoding), 10th, 20th, 30th decoding steps.

Figure 3: Attention Profiling Result Distribution across different layers.

Setting.Following Figure 1, we compute the accumulated attention score for attention heads in different layers of LLaMa 65B at multiple decoding steps (i.e., 1st, 10th, 20th, 30th). We visualized the resulting accumulated score in Figure 4.

Observation.Despite some fluctuations in the exact accumulated attention scores across time steps, the pattern of the attention maps remains relatively stable. For example, Layer 33 Head 0 and Layer 23 Head 2 almost only attend to the special token, while the locality and punctuation plays an important role in Layer 23 Head 0. As to Layer 23 Head 3, more than 10 percent of the attention score is allocated to the Others portion, making it suitable for the uncompressed KV cache \(_{}\).

Also, we observe that a large part of attention scores focuses on special tokens in all cases, which matches our intuition for conducting the naive strategy combinations in Section 3.

## 5 Experiment

We conduct comprehensive experiments to demonstrate the effectiveness of FastGen on memory footprint reduction and generation quality preserving. First, we report the trade-off between memory reduction and end-to-end generation quality in Section 5.1, and discuss the compression ratio of FastGen in Section 5.2. Finally, we present ablation studies and discussions in Section 5.5.

### Trade-off between performance and memory reduction

Backbones.We conduct experiments with both LLaMa1 (Touvron et al., 2023) and its fine-tuned variants, with model sizes ranging from 7B to 65B. For fined-tuned variants, we do not choose the open-sourced ChatLLaMa2 (Touvron et al., 2023) model due to its grouped-query attention tech

Figure 5: Performance of Adaptive KV Cache (FastGen) and Fixed KV Cache (Frequency+Local; Zhang et al., 2023 and Liu et al., 2023) of LLaMa on GSM8k, Human Eval, NQ, and TQA.

[MISSING_PAGE_FAIL:7]

### Memory Footprint Reduction Analysis

We report the KV cache memory footprint reduction in Table 1. For all the evaluated 7B-65B models, we evaluate the memory consumption with a fixed batch size of 16, sequence length of 512, and model weights in fp16 format. We observe that FastGen substantially reduces the KV cache memory footprint across all model sizes, with more significant reductions for larger models. Taking a win rate over 45% as no quality regression, FastGen can achieve \(\)50% memory reduction in LLaMa-65B, \(\)30% in LLaMa-30B, \(\)20% in LLaMa-13B and LLaMa-7B.

### End-to-end Latency Improvement

To analyze the end-to-end speedup of FastGen, we present the end-to-end latency improvement in Table 2. In the experiment, we record the total duration in seconds, measured from the start of prompt encoding, until the end of generation as the end-to-end latency. For the Full-cache baseline, we use the widely used Hugging Face Accelerate (HF). For FastGen, we implemented a customized kernel to handle the KV cache pruning operation. Specifically, we adapt the kernel from Deepspeed Aminabadi et al. (2022) by adding the KV cache sparsity operation. We include the Deepspeed performance for fair comparison. All methods are tested on the same Nvidia V100 GPUs.

As shown in Table 2, we can observe that FastGen achieves significant end-to-end speed-up across all the generation settings. For the least significant case, Fastgen can have a decent \(16.04\%\) latency improvement over the full-cache baseline on a short generation length of 512. In the best cases, we can achieve up to \(55.0\%\) latency reduction with Fastgen at a generation length of 16k. We can also observe that the relative speedup is greater with longer generation length. For example, given batch size = 1, FastGen's relative speed-up rises from \(16.04\%\) to \(55.0\%\), as the generation length grows from 512 to 16k. When comparing FastGen to DeepSpeed, we can still observe significant speed-up that gets bigger with batch size and generation length. Considering DeepSpeed is a full-stack optimized inference system, there is still much room to further improvement FastGen by polishing the sparsity kernel. We leave this unique research and engineering challenge to future works.

### Profiling Cost

To better understand the overhead of the profiling step, we compare the profiling time with the total generation time across different generation lengths. We present the result in Table 3.

We can observe that the profiling time only accounts for a very small percentage of the total generation duration, up to \(0.35\%\) in our tested cases. Also, the overhead decreases as the generation length increases, dropping to \(0.07\%\) when the generation length comes to 1024.

In terms of extra memory usage, it's mainly introduced by one of the compression strategies, \(_{}\), which needs to store an extra cumulative sum of attention scores for each attention head. To provide a detailed analysis, for each layer, the dimension of the KV cache is

   Batch size &  &  &  &  \\  \{[compact len, gen len] &  &  &  &  &  &  &  &  &  &  \\  Pati-cache & 13.35 & 57.37 & 299 & 799.14 & 1.12 & 19.16 & 167.64 & 23.44 & OOM & OOM \\ DeepSpeed & 11.58 & 47.12 & 201.23 & 435.74 & 0.79 & 10.45 & 90.14 & 12.93 & 127.94 & OOM \\ FastGen & 11.21 & 44.65 & 179.43 & 359.83 & 0.73 & 9.71 & 15.93 & 10.57 & 82.16 & OOM \\ Speed-up (s) over Full-cache & 16.03\% & 22.30\% & 40.69\% & 35.00\% & 34.80\% & 49.80\% & 54.00\% & 54.00\% & OOM \\ Speed-up(s) over DeepSpeed & 3.20\% & 5.35\% & 10.83\% & 17.42\% & 7.59\% & 7.08\% & 15.00\% & 18.25\% & 35.78\% & OOM \\   

Table 2: End-to-end latency comparison on LLaMA7b.

   Generation Length & Overall Generation Duration (s) & Profiling Duration (s) & Decoding Time Per Token (s) & ProfilingOverall (\%) \\ 
128 & 30.98 & 0.11 & 0.1 & 0.35\% \\
256 & 50.1 & 0.11 & 0.1 & 0.21\% \\
512 & 94.98 & 0.11 & 0.1 & 0.12\% \\
1024 & 157.43 & 0.11 & 0.1 & 0.07\% \\   

Table 3: Profiling time of LLaMA65b. The Overall Generation Duration is measured from the start of decoding to the end of the generation length. The Profiling Duration is measured from the start of the decoding until Fastgen finishes the policy search.

(batch_size, num_of_head, sequence_len, hidden_dimension), while the dimension of extra memory for the cumulative attention scores is (batch_size, num_of_head, equence_len). Considering hidden_dimension = 128 for all model sizes, the memory overhead is 1/128=\(0.78\%\) compared to storing KV cache only, which is a negligible cost.

### Ablations

For all the ablations, we use a fixed targeted recovery ratio \(=0.98\).

How one policy affect all the other policies?We study the complementary effects of each policy on the combination of all other policies in our framework. We examine changes in pruned KV cache and win rate while fixing the targeted recovery ratio \(T\). We take the full policy set as our control set \(\). For each ablation, we remove one of the policies from all policy combination in \(\). We summarized the results in Table 4, which suggests the \(_{}\), and the \(_{}\) are the most important policies. Removing them will incur a \(3.67\%\) and a \(2.11\%\) win rate drop respectively. We can also observe from the pruned cache ratio that \(_{}\) and \(_{}\) reduce more KV caches than the others. However, their standalone non-adaptive deployment yields suboptimal performance, as depicted in Figure 2, further verifying the importance of adapting different compression policies.

Which policy should we add first (and last)?As in Section 3.2, we use a greed method to construct adaptive KV cache. Here, we examine how the order of introducing each policy affects the performance. Similar to the previous study, we fix the targeted recovery ratio to 0.98, and keep allocating cache budget until the constructed cache hit the recovery ratio. For simplicity, we make every examined order opt-in the \(_{}\) first, as it's typically the most important tokens and of super-low memory cost, as suggested in Figure 1. We summarize the results in Table 5. Our current order (as in Equation 2) achieves the highest win-rates and the highest pruned ratios. Meanwhile, using alternative orders leads to a different trade-off between KV cache compression and generation quality. For example, using \(_{}_{}_{ }\) leads to an improved KV cache compression ratio at the cost of generation quality.

  
**Feasible Policy Set** & **Pruned KV Ratio** & **Win Rate** \\  \(\) & 36.04\% & 49.75\% \\  \(\{_{},C_{},C_{},C_{}\}\) & 31.16\% & 47.64\% \\  \(\{_{},C_{},C_{},C_{}\}\) & 34.23\% & 49.56\% \\  \(\{_{},C_{},C_{},C_{}\}\) & 30.18\% & 49.06\% \\  \(\{_{},C_{},C_{},C_{}\}\) & 21.26\% & 46.08\% \\   

Table 4: Complementary effects of each policy. We display the win rate of each method over full cache setting. We evaluate the fine-tuned LLaMa 65B on AlpacaEval with the same parameters.

  
**Cache Order** & **Pruned KV Ratio** & **Win Rate** \\  \(C_{} C_{} C_{}  C_{}\) & 36.04\% & 49.75\% \\  \(C_{} C_{} C_{} C_{}\) & 36.40\% & 47.64\% \\   

Table 5: Policy order ablation on fine-tuned LLaMa 65B with AlpacaEval.

Figure 6: Hyper-parameter ablation on fine-tuned LLaMa 65B with AlpacaEval.

Sensitivity Study.We analyze the sensitivity of selecting different hyper-parameters for FastGen, as illustrated in Figure 6. We observe that altering these hyper-parameters does not have a visible impact on the generation quality, as the model maintains a winrate over 45% in all situations. Meanwhile, it leads to a relative large change on the compression ratio. For example, changing the ratio for the frequency policy from 0.3 to 0.1 leads to +10% more KV cache. In our experiments, we set the ratio to 0.3 for both \(r_{l}\) and \(r_{f}\).

## 6 Conclusion

In this work, we introduce FastGen, a novel method that improves the efficiency of large language models through lightweight attention profiling and adaptive key-value caching. Our approach enables more efficient deployment across diverse models and datasets without loss of generation quality. Further work includes combining FastGen with other optimization techniques like quantization and distillation. Meanwhile, another direction could be accommodating other efficient attention architecture, such as grouped-query attention. We hope our initial results will spur further developments in efficient and deployable LLM technologies.