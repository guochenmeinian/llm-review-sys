# SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities

Hugues Van Assel

ENS de Lyon, CNRS

UMPA UMR 5669

hugues.van_assel@ens-lyon.fr

&Titouan Vayer

Univ. Lyon, ENS de Lyon, UCBL, CNRS, Inria

LIP UMR 5668

titouan.vayer@inria.fr

&Remi Flamary

Ecole polytechnique, IP Paris, CNRS

CMAP UMR 7641

remi.flamary@polytechnique.edu

&Nicolas Courty

Universite Bretagne Sud, CNRS

IRISA UMR 6074

nicolas.courty@irisa.fr

###### Abstract

Many approaches in machine learning rely on a weighted graph to encode the similarities between samples in a dataset. Entropic affinities (EAs), which are notably used in the popular Dimensionality Reduction (DR) algorithm t-SNE, are particular instances of such graphs. To ensure robustness to heterogeneous sampling densities, EAs assign a kernel bandwidth parameter to every sample in such a way that the entropy of each row in the affinity matrix is kept constant at a specific value, whose exponential is known as perplexity. EAs are inherently asymmetric and row-wise stochastic, but they are used in DR approaches after undergoing heuristic symmetrization methods that violate both the row-wise constant entropy and stochasticity properties. In this work, we uncover a novel characterization of EA as an optimal transport problem, allowing a natural symmetrization that can be computed efficiently using dual ascent. The corresponding novel affinity matrix derives advantages from symmetric doubly stochastic normalization in terms of clustering performance, while also effectively controlling the entropy of each row thus making it particularly robust to varying noise levels. Following, we present a new DR algorithm, SNEkhorn, that leverages this new affinity matrix. We show its clear superiority to existing approaches with several indicators on both synthetic and real-world datasets.

## 1 Introduction

Exploring and analyzing high-dimensional data is a core problem of data science that requires building low-dimensional and interpretable representations of the data through dimensionality reduction (DR). Ideally, these representations should preserve the data structure by mimicking, in the reduced representation space (called _latent space_), a notion of similarity between samples. We call _affinity_ the weight matrix of a graph that encodes this similarity. It has positive entries and the higher the weight in position \((i,j)\), the higher the similarity or proximity between samples \(i\) and \(j\). Seminal approaches relying on affinities include Laplacian eigenmaps , spectral clustering  and semi-supervised learning . Numerous methods can be employed to construct such affinities. A common choice is to use a kernel (_e.g._, Gaussian) derived from a distance matrix normalized by a bandwidth parameter that usually has a large influence on the outcome of the algorithm. Indeed, excessively small kernelbandwidth can result in solely capturing the positions of closest neighbors, at the expense of large-scale dependencies. Inversely, setting too large a bandwidth blurs information about close-range pairwise relations. Ideally, one should select a different bandwidth for each point to accommodate varying sampling densities and noise levels. One approach is to compute the bandwidth of a point based on the distance from its \(k\)-th nearest neighbor . However, this method fails to consider the entire distribution of distances. In general, selecting appropriate kernel bandwidths can be a laborious task, and many practitioners resort to greedy search methods. This can be limiting in some settings, particularly when dealing with large sample sizes.

**Entropic Affinities and SNE/t-SNE.** Entropic affinities (EAs) were first introduced in the seminal paper _Stochastic Neighbor Embedding_ (SNE) . It consists in normalizing each row \(i\) of a distance matrix by a bandwidth parameter \(_{i}\) such that the distribution associated with each row of the corresponding stochastic (_i.e._, row-normalized) Gaussian affinity has a fixed entropy. The value of this entropy, whose exponential is called the _perplexity_, is then the only hyperparameter left to tune and has an intuitive interpretation as the number of effective neighbors of each point . EAs are notoriously used to encode pairwise relations in a high-dimensional space for the DR algorithm t-SNE , among other DR methods including . t-SNE is increasingly popular in many applied fields [20; 32] mostly due to its ability to represent clusters in the data [27; 5]. Nonetheless, one major flaw of EAs is that they are inherently directed and often require post-processing symmetrization.

**Doubly Stochastic Affinities.** Doubly stochastic (DS) affinities are non-negative matrices whose rows and columns have unit \(_{1}\) norm. In many applications, it has been demonstrated that DS affinity normalization (_i.e._, determining the nearest DS matrix to a given affinity matrix) offers numerous benefits. First, it can be seen as a relaxation of k-means  and it is well-established that it enhances spectral clustering performances [10; 52; 1]. Additionally, DS matrices present the benefit of being invariant to the various Laplacian normalizations . Recent observations indicate that the DS projection of the Gaussian kernel under the KL geometry is more resilient to heteroscedastic noise compared to its stochastic counterpart . It also offers a more natural analog to the heat kernel . These properties have led to a growing interest in DS affinities, with their use expanding to various applications such as smoothing filters , subspace clustering  and transformers .

**Contributions.** In this work, we study the missing link between EAs, which are easy to tune and adaptable to data with heterogeneous density, and DS affinities which have interesting properties in practical applications as aforementioned. Our main contributions are as follows. We uncover the convex optimization problem that underpins classical entropic affinities, exhibiting novel links with entropy-regularized Optimal Transport (OT) (Section 3.1). We then propose in Section 3.2 a principled symmetrization of entropic affinities. The latter enables controlling the entropy in each point, unlike t-SNE's post-processing symmetrization, and produces a genuinely doubly stochastic affinity. We show how to compute this new affinity efficiently using a dual ascent algorithm. In Section 4, we introduce SNEkhorn: a DR algorithm that couples this new symmetric entropic affinity with a doubly stochastic kernel in the low-dimensional embedding space, without sphere concentration issue . We finally showcase the benefits of symmetric entropic affinities on a variety of applications in Section 5 including spectral clustering and DR experiments on datasets ranging from images to genomics data.

**Notations.**[\(n\)] denotes the set \(\{1,...,n\}\). \(\) and \(\) applied to vectors/matrices are taken element-wise. \(=(1,...,1)^{}\) is the vector of \(1\). \(,\) is the standard inner product for matrices/vectors. \(\) is the space of \(n n\) symmetric matrices. \(_{i}\): denotes the \(i\)-th row of a matrix \(\). \(\) (_resp._\(\)) stands for element-wise multiplication (_resp._ division) between vectors/matrices. For \(,^{n}, ^{n n}\) is \((_{i}+_{j})_{ij}\). The entropy of \(_{+}^{n}\) is \(()=-_{i}p_{i}((p_{i})-1)=-, -\). The Kullback-Leibler divergence between two matrices \(,\) with nonnegative entries such that \(Q_{ij}=0 P_{ij}=0\) is \((|)=_{ij}P_{ij}((}{Q_{ ij}})-1)=,()- ^{}\).

## 2 Entropic Affinities, Dimensionality Reduction and Optimal Transport

Given a dataset \(^{n p}\) of \(n\) samples in dimension \(p\), most DR algorithms compute a representation of \(\) in a lower-dimensional latent space \(^{n q}\) with \(q p\) that faithfully captures and represents pairwise dependencies between the samples (or rows) in \(\). This is generally achievedby optimizing \(\) such that the corresponding affinity matrix matches another affinity matrix defined from \(\). These affinities are constructed from a matrix \(^{n n}\) that encodes a notion of "distance" between the samples, _e.g._, the squared Euclidean distance \(C_{ij}=\|_{i:}-_{j:}\|_{2}^{2}\) or more generally any _cost matrix_\(:=\{_{+}^{n n}: =^{}C_{ij}=0 i=j\}\). A commonly used option is the Gaussian affinity that is obtained by performing row-wise normalization of the kernel \((-/)\), where \(>0\) is the bandwidth parameter.

**Entropic Affinities (EAs).** Another frequently used approach to generate affinities from \(\) is to employ _entropic affinities_. The main idea is to consider _adaptive_ kernel bandwidths \((_{i}^{})_{i[n]}\) to capture finer structures in the data compared to constant bandwidths . Indeed, EAs rescale distances to account for the varying density across regions of the dataset. Given \([n-1]\), the goal of EAs is to build a Gaussian Markov chain transition matrix \(^{}\) with prescribed entropy as

\[ i, j,\ P_{ij}^{} =/_{i}^{})}{_{ }(-C_{i}/_{i}^{})}\] (EA) \[_{i}^{}_{+}^{} (_{i:}^{})=+1\,.\]

The hyperparameter \(\), which is also known as _perplexity_, can be interpreted as the effective number of neighbors for each data point . Indeed, a perplexity of \(\) means that each row of \(^{}\) (which is a discrete probability since \(^{}\) is row-wise stochastic) has the same entropy as a uniform distribution over \(\) neighbors. Therefore, it provides the practitioner with an interpretable parameter specifying which scale of dependencies the affinity matrix should faithfully capture. In practice, a root-finding algorithm is used to find the bandwidth parameters \((_{i}^{})_{i[n]}\) that satisfy the constraints . Hereafter, with a slight abuse of language, we call \(e^{(_{i:})-1}\) the perplexity of the point \(i\).

**Dimension Reduction with SNe/t-SNE.** One of the main applications of EAs is the DR algorithm SNE. We denote by \(}=(\|_{i:}-_{j:}\|_{2}^{2})_{ij}\) and \(}=(\|_{i:}-_{j:}\|_{2}^{2})_{ij}\) the cost matrices derived from the rows (_i.e._, the samples) of \(\) and \(\) respectively. SNE focuses on minimizing in the latent coordinates \(^{n q}\) the objective \((^{}|})\) where \(^{}\) solves (EA) with cost \(}\) and \([}]_{ij}=(-[}]_{ij})/(_{}(-[}]_{i}))\). In the seminal paper , a newer proposal for a _symmetric_ version was presented, which has since replaced SNE in practical applications. Given a symmetric normalization for the similarities in latent space \([}_{}]_{ij}=(-[}]_{ij})/_ {,}(-[}]_{})\) it consists in solving

\[_{^{n q}}(^{ }}|}_{}) ^{}}=(^{}+^{})\,.\] (Symmetric-SNE)

In other words, the affinity matrix \(^{}}\) is the Euclidean projection of \(^{}\) on the space of symmetric matrices \(\): \(^{}}=_{}^{f_{2}}(^{})=_{}\|-^{}\|_{2}\) (see Appendix A.1). Instead of the Gaussian kernel, the popular extension t-SNE  considers a different distribution in the latent space \([}_{}]_{ij}=(1+[}]_{ij})^{-1}/ _{,}(1+[}]_{})^{-1}\). In this formulation, \(}_{}\) is a joint Student \(t\)-distribution that accounts for crowding effects: a relatively small distance in a high-dimensional space can be accurately represented by a significantly greater distance in the low-dimensional space.

Considering symmetric similarities is appealing since the proximity between two points is inherently symmetric. Nonetheless, the Euclidean projection in (Symmetric-SNE) _does not preserve the construction of entropic affinities_. In particular, \(^{}}\) is not stochastic in general and \((^{}_{i:}})(+1)\) thus the entropy associated with each point is no longer controlled after symmetrization (see the bottom left plot of Figure 1). This is arguably one of the main drawbacks of the approach. By contrast, the \(^{}\) affinity that will be introduced in Section 3 can accurately set the entropy in each point to the desired value \(+1\). As shown in Figure 1 this leads to more faithful embeddings with better separation of the classes when combined with the t-SNEkhorn algorithm (Section 4).

Figure 1: Top: COIL  embeddings with silhouette scores produced by t-SNE and t-SNEkhorn (our method introduced in Section 4) for \(=30\). Bottom: \(e^{(_{i:})-1}\) (_perplexity_) for each point \(i\).

**Symmetric Entropy-Constrained Optimal Transport.** Entropy-regularized OT  and its connection to affinity matrices are crucial components in our solution. In the special case of uniform marginals, and for \(>0\), entropic OT computes the minimum of \(,-_{i}( _{i:})\) over the space of doubly stochastic matrices \(\{_{+}^{n n}:=^{ }=\}\). The optimal solution is the _unique_ doubly stochastic matrix \(^{}\) of the form \(^{}=() ()\) where \(=(-/)\) is the Gibbs energy derived from \(\) and \(,\) are positive vectors that can be found with the celebrated Sinkhorn-Knopp's algorithm [8; 42]. Interestingly, when the cost \(\) is _symmetric_ (_e.g._, \(\)) we can take \(=\)[17; Section 5.2] so that the unique optimal solution is itself symmetric and writes

\[^{}=((-)/ )\;^{n}\,.\] (DS)

In this case, by relying on convex duality as detailed in Appendix A.2, an equivalent formulation for the symmetric entropic OT problem is

\[_{_{+}^{n n}}\;\;\;,\;\;\;\;\;=,\;= ^{}\;\;_{i}(_{i:})\,\] (EOT)

where \(0 n( n+1)\) is a constraint on the global entropy \(_{i}(_{i:})\) of the OT plan \(\) which happens to be saturated at optimum (Appendix A.2). This constrained formulation of symmetric entropic OT will provide new insights into entropic affinities, as detailed in the next sections.

## 3 Symmetric Entropic Affinities

In this section, we present our first major contribution: symmetric entropic affinities. We begin by providing a new perspective on EAs through the introduction of an equivalent convex problem.

### Entropic Affinities as Entropic Optimal Transport

We introduce the following set of matrices with row-wise stochasticity and entropy constraints:

\[_{}\{_{+}^{n n}\;\;=\;\; i,\;( _{i:})+1\}\.\] (1)

This space is convex since \(_{+}^{n}()\) is concave, thus its superlevel set is convex. In contrast to the entropic constraints utilized in standard entropic optimal transport which set a lower-bound on the _global_ entropy, as demonstrated in the formulation (EOT), \(_{}\) imposes a constraint on the entropy of _each row_ of the matrix \(\). Our first contribution is to prove that EAs can be computed by solving a specific problem involving \(_{}\) (see Appendix A for the proof).

**Proposition 1**.: _Let \(^{n n}\) without constant rows. Then \(^{}\) solves the entropic affinity problem (EA) with cost \(\) if and only if \(^{}\) is the unique solution of the convex problem_

\[_{_{}}\;,.\] (EA as OT)

Interestingly, this result shows that EAs boil down to minimizing a transport objective with cost \(\) and row-wise entropy constraints \(_{}\) where \(\) is the desired perplexity. As such, (EA as OT) can be seen as a specific _semi-relaxed_ OT problem [39; 14] (_i.e._, without the second constraint on the marginal \(^{}=\)) but with entropic constraints on the rows of \(\). We also show that the optimal solution \(^{}\) of (EA as OT) has _saturated entropy i.e._, \( i,\;(_{i:}^{})=+1\). In other words, relaxing the equality constraint in (EA) as an inequality constraint in \(_{}\) does not affect the solution while it allows reformulating entropic affinity as a convex optimization problem. To the best of our knowledge, this connection between OT and entropic affinities is novel and is an essential key to the method proposed in the next section.

**Remark 2**.: The kernel bandwidth parameter \(\) from the original formulation of entropic affinities (EA) is the Lagrange dual variable associated with the entropy constraint in (EA as OT). Hence computing \(^{}\) in (EA) exactly corresponds to solving the dual problem of (EA as OT).

**Remark 3**.: Let \(_{}=(-/)\). As shown in Appendix A.5, if \(^{}\) solves (EA) and \((^{})\), then \(^{}=_{_{}}^{}( _{})=*{arg\,min}_{_{}} (|_{})\). Therefore \(^{}\) can be seen as a \(\) Bregman projection  of a Gaussian kernel onto \(_{}\). Hence the input matrix in (Symmetric-SNE) is \(^{}}=_{}^{t_{2}}( _{_{}}^{}(_{}))\) which corresponds to a surprising mixture of \(\) and orthogonal projections.

### Symmetric Entropic Affinity Formulation

Based on the previous formulation we now propose symmetric entropic affinities: a symmetric version of EAs that enables keeping the entropy associated with each row (or equivalently column) to the desired value of \(+1\) while producing a symmetric doubly stochastic affinity matrix. Our strategy is to enforce symmetry through an additional constraint in (EA as OT), in a similar fashion as (EOT). More precisely we consider the convex optimization problem

\[_{_{}}, \,.\] (SEA)

where we recall that \(\) is the set of \(n n\) symmetric matrices. Note that for any \( n-1\), \(^{}_{}\) hence the set \(_{}\) is a non-empty and convex set. We first detail some important properties of problem (SEA) (the proofs of the following results can be found in Appendix A.4).

**Proposition 4** (Saturation of the entropies).: _Let \(\) with zero diagonal, then (SEA) with cost \(\) has a unique solution that we denote by \(^{}\). If moreover \(\), then for at least \(n-1\) indices \(i n\) the solution satisfies \((^{}_{i:})=+1\)._

In other words, the unique solution \(^{}\) has at least \(n-1\) saturated entropies _i.e._, the corresponding \(n-1\) points have exactly a perplexity of \(\). In practice, with the algorithmic solution detailed below, we have observed that all \(n\) entropies are saturated. Therefore, we believe that this proposition can be extended with a few more assumptions on \(\). Accordingly, problem (SEA) allows accurate control over the point-wise entropies while providing a symmetric doubly stochastic matrix, unlike \(^{}}\) defined in (Symmetric-SNE), as summarized in Table 1. In the sequel, we denote by \(_{}()=((_{i:}))_ {i}\) the vector of row-wise entropies of \(\). We rely on the following result to compute \(^{}\).

**Proposition 5** (Solving for SEA).: _Let \(,(,, )=,+ ,(+1)-_{}() +,-\) and \(q(,)=_{_{+}^{n  n}}(,, )\). Strong duality holds for (SEA). Moreover, let \(^{},^{}*{ argmax}_{ 0,}q(, )\) be the optimal dual variables respectively associated with the entropy and marginal constraints. Then, for at least \(n-1\) indices \(i n,_{i}^{}>0\). When \( i n\), \(_{i}^{}>0\) then \(_{}(^{})=(+1)\) and \(^{}\) has the form_

\[^{}=((^{} ^{}-2)(^{} ^{}))\,.\] (2)

By defining the symmetric matrix \((,)=((-2)( ))\), we prove that, when \(>0,_{}(, ,)\) has a unique solution given by \((,)\) which implies \(q(,)=(( {},),,)\). Thus the proposition shows that when \(^{}>0,\ ^{}=(^{}, ^{})\) where \(^{},^{}\) solve the following convex problem (as maximization of a _concave_ objective)

\[_{>0,}(( ,),,).\] (Dual-SEA)

Figure 2: Samples from a mixture of three Gaussians with varying standard deviations. The edges’ strength is proportional to the weights in the affinities \(^{}\) (DS) and \(^{}\) (SEA) computed with \(=5\) (for \(^{}\), \(\) is the average perplexity such that \(_{i}(^{}_{i:})=_{i}( ^{}_{i:})\)). Points’ color represents the perplexity \(e^{(_{i:})-1}\). Right plot: smallest eigenvalues of the Laplacian for the two affinities.

Consequently, to find \(^{}\) we solve the problem (Dual-SEA). Although the form of \(^{}\) presented in Proposition 5 is only valid when \(^{}\) is positive and we have only proved it for \(n-1\) indices, we emphasize that if (Dual-SEA) has a finite solution, then it is equal to \(^{}\). Indeed in this case the solution satisfies the KKT system associated with (SEA).

**Numerical optimization.** The dual problem (Dual-SEA) is concave and can be solved with guarantees through a dual ascent approach with closed-form gradients (using _e.g._, SGD, BFGS or ADAM). At each gradient step, one can compute the current estimate \((,)\) while the gradients of the loss _w.r.t._\(\) and \(\) are given respectively by the constraints \((+1)-_{}((,))\) and \(-(,)\) (see _e.g._, [4, Proposition 6.1.1]). Concerning time complexity, each step can be performed with \((n^{2})\) algebraic operations. From a practical perspective, we found that using a change of variable \(^{2}\) and optimize \(^{n}\) leads to enhanced numerical stability.

**Remark 6**.: In the same spirit as Remark 3, one can express \(^{}\) as a \(\) projection of \(_{}=(-/)\). Indeed, we show in Appendix A.5 that if \(0<_{i}_{i}^{}\), then \(^{}=_{_{}}^{}(_{})\).

**Comparison between \(^{}\) and \(^{}\).** In Figure 2 we illustrate the ability of our proposed affinity \(^{}\) to adapt to varying noise levels. In the OT problem that we consider, each sample is given a mass of one that is distributed over its neighbors (including itself since self-loops are allowed). For each sample, we refer to the entropy of the distribution over its neighbors as the _spreading_ of its mass. One can notice that for \(^{}\) (DS) (OT problem with global entropy constraint (EOT)), the samples do not spread their mass evenly depending on the density around them. On the contrary, the per-row entropy constraints of \(^{}\) force equal spreading among samples. This can have benefits, particularly for clustering, as illustrated in the rightmost plot, which shows the eigenvalues of the associated Laplacian matrices (recall that the number of connected components equals the dimension of the null space of its Laplacian ). As can be seen, \(^{}\) results in many unwanted clusters, unlike \(^{}\), which is robust to varying noise levels (its Laplacian matrix has only \(3\) vanishing eigenvalues). We further illustrate this phenomenon on Figure 3 with varying noise levels.

## 4 Optimal Transport for Dimension Reduction with SNEkhorn

In this section, we build upon symmetric entropic affinities to introduce SNEkhorn, a new DR algorithm that fully benefits from the advantages of doubly stochastic affinities.

**SNEkhorn's objective.** Our proposed method relies on doubly stochastic affinity matrices to capture the dependencies among the samples in both input _and_ latent spaces. The \(\) divergence, which is the central criterion in most popular DR methods , is used to measure the discrepancy between the two affinities. As detailed in sections 2 and 3, \(^{}\) computed using the cost \([_{}]_{ij}=\|_{i:}-_{j:}\|_{2}^{2}\), corrects for heterogeneity in the input data density by imposing point-wise entropy constraints. As we do not need such correction for embedding coordinates \(\) since they must be optimized, we opt for the standard affinity (DS) built as an OT transport plan with global entropy constraint (EOT). This OT plan can be efficiently computed using Sinkhorn's algorithm. More precisely, we propose the optimization problem

\[_{^{n q}}\ ^{ }|_{}^{}\,,\] (SNEkhorn)

where \(_{}^{}=(_{} _{}-_{})\) stands for the (DS) affinity computed with cost \([_{}]_{ij}=\|_{i:}-_{j:}\|_{2}^{2}\) and \(_{}\) is the optimal dual variable found by Sinkhorn's algorithm. We set the bandwidth to \(=1\) in \(_{}^{}\) similarly to  as the bandwidth in the low dimensional space only affects the scales

   Affinity matrix & \(^{}\) & \(^{}}\) & \(^{}\) & \(^{}\) \\ Reference &  &  &  & (SEA) \\   \(=^{}\) & \(\) & \(}\) & \(}\) & \(}\) \\ \(=^{}=\) & \(\) & \(\) & \(}\) & \(}\) \\ \(_{}()=(+1)\) & \(}\) & \(\) & \(\) & \(}\) \\   

Table 1: Properties of \(^{}\), \(^{}}\), \(^{}\) and \(^{}\)

Figure 3: ARI spectral clustering on the example of three Gaussian clusters with variances: \(^{2}\), \(2^{2}\) and \(3^{2}\) (as in Figure 2).

of the embeddings and not their shape. Keeping only the terms that depend on \(\) and relying on the double stochasticity of \(^{}\), the objective in (SNEhorn) can be expressed as \(^{},_{}-2_ {},\).

**Heavy-tailed kernel in latent space.** Since it is well known that heavy-tailed kernels can be beneficial in DR , we propose an extension called t-SNEhorn that simply amounts to computing a doubly stochastic student-t kernel in the low-dimensional space. With our construction, it corresponds to choosing the cost \([_{}]_{ij}=(1+\|_{i:}-_{j:}\|_{2}^ {2})\) instead of \(\|_{i:}-_{j:}\|_{2}^{2}\).

**Inference.** This new DR objective involves computing a doubly stochastic normalization for each update of \(\). Interestingly, to compute the optimal dual variable \(_{}\) in \(_{}^{}\), we leverage a well-conditioned Sinkhorn fixed point iteration [19; 13], which converges extremely fast in the symmetric setting:

\[ i,\;[_{}]_{i}([ _{}]_{i}-_{k}([_{}]_ {k}-[_{}]_{ki}))\;.\] (Sinkhorn)

On the right side of Figure 4, we plot \(\|_{}^{}-\|_{}\) as a function of (Sinkhorn) iterations for a toy example presented in Section 5. In most practical cases, we found that about 10 iterations were enough to reach a sufficiently small error. \(\) is updated through gradient descent with gradients obtained by performing backpropagation through the Sinkhorn iterations. These iterations can be further accelerated with a _warm start_ strategy by plugging the last \(_{}\) to initialize the current one.

**Related work.** Using doubly stochastic affinities for SNE has been proposed in , with two key differences from our work. First, they do not consider EAs and resort to \(^{}\) (DS). This affinity, unlike \(^{}\), is not adaptive to the data heterogeneous density (as illustrated in Figure 2). Second, they use the affinity \(}_{}\) in the low-dimensional space and illustrate empirically that matching the latter with a doubly stochastic matrix (_e.g._, \(^{}\) or \(^{}\)) can sometimes impose spherical constraints on the embedding \(\). This is detrimental for projections onto a \(2D\) flat space (typical use case of DR) where embeddings tend to form circles. This can be verified on the left side of Figure 4. In contrast, in SNEkhorn, the latent affinity _is also doubly stochastic_ so that latent coordinates \(\) are not subject to spherical constraints anymore. The corresponding SNEkhorn embedding is shown in Figure 5 (bottom right).

## 5 Numerical experiments

This section aims to illustrate the performances of the proposed affinity matrix \(^{}\) (SEA) and DR method SNEkhorn at faithfully representing dependencies and clusters in low dimensions. First, we showcase the relevance of our approach on a simple synthetic dataset with heteroscedastic noise. Then,

Figure 4: Left: SNEkhorn embedding on the simulated data of Section 5 using \(}_{}\) instead of \(_{}^{}\) with \(=30\). Right: number of iterations needed to achieve \(\|_{}^{}-\|_{} \) with (Sinkhorn).

we evaluate the spectral clustering performances of symmetric entropic affinities before benchmarking t-SNEkhorn with t-SNE and UMAP  on real-world images and genomics datasets.2

**Simulated data.** We consider the toy dataset with heteroscedastic noise from . It consists of sampling uniformly two vectors \(_{1}\) and \(_{2}\) in the \(10^{4}\)-dimensional probability simplex. \(n=10^{3}\) samples are then generated as \(_{i}=}_{i}/(_{j}_{ij})\) where

\[}_{i}\{(1000,_{1}),&1 i 500\\ (1000,_{2}),&501 i 750\\ (2000,_{2}),&751 i 1000\,..\]

where \(\) stands for the multinomial distribution. The goal of the task is to test the robustness to heteroscedastic noise. Indeed, points generated using \(_{2}\) exhibit different levels of noise due to various numbers of multinomial trials to form an estimation of \(_{2}\). This typically occurs in real-world scenarios when the same entity is measured using different experimental setups thus creating heterogeneous technical noise levels (_e.g._, in single-cell sequencing ). This phenomenon is known as _batch effect_. In Figure 5, we show that, unlike \(^{}}\) (Symmetric-SNE), \(^{}\) (SEA) manages to properly filter the noise (top row) to discriminate between samples generated by \(_{1}\) and \(_{2}\), and represent these two clusters separately in the embedding space (bottom row). In contrast, \(^{}}\) and SNE are misled by the batch effect. This shows that \(^{}}\) doesn't fully benefit from the adaptivity of EAs due to poor normalization and symmetrization. This phenomenon partly explains the superiority of SNEkhorn and t-SNEkhorn over current approaches on real-world datasets as illustrated below.

**Real-world datasets.** We then experiment with various labeled classification datasets including images and genomic data. For images, we use COIL 20 , OLIVETTI faces , UMNIST  and CIFAR 10 . For CIFAR, we experiment with features obtained from the last hidden layer of a pre-trained ResNet  while for the other three datasets, we take as input the raw pixel data. Regarding genomics data, we consider the Curated Microarray Database (CuMiDa)  made of microarray datasets for various types of cancer, as well as the pre-processed SNAREseq (chromatic accessibility) and scGEM (gene expression) datasets used in . For CuMiDa, we retain the datasets with most samples. For all the datasets, when the data dimension exceeds \(50\) we apply a pre-processing step of PCA in dimension \(50\), as usually done in practice . In the following experiments, when not specified the hyperparameters are set to the value leading to the best average score on five different seeds with grid-search. For perplexity parameters, we test all multiples of \(10\) in the interval \([10,(n,300)]\) where \(n\) is the number of samples in the dataset. We use the same grid for the \(k\) of the self-tuning affinity \(^{}\) and for the n_neighbors parameter of UMAP. For scalar bandwidths, we consider powers of \(10\) such that the corresponding affinities' average perplexity belongs to the perplexity range.

**Spectral Clustering.** Building on the strong connections between spectral clustering mechanisms and t-SNE  we first consider spectral clustering tasks to evaluate the affinity matrix \(^{}\) (SEA) and compare it against \(^{}}\) (Symmetric-SNE). We also consider two versions of the Gaussian affinity with scalar bandwidth \(=(-/)\): the symmetrized row-stochastic \(^{}}=_{}^{_{ 2}}(^{})\) where \(^{}\) is \(\) normalized by row and \(^{}\) (DS). We also consider the adaptive Self-Tuning \(^{}\) affinity from  which relies on an adaptive bandwidth corresponding to the distance from the \(k\)-th nearest neighbor of each point. We use the spectral clustering implementation of scikit-learn  with default parameters which uses the unnormalized graph Laplacian. We measure the quality of clustering using the Adjusted Rand Index (ARI). Looking at both Table 2 and Figure 6, one can notice that, in general, symmetric entropic affinities yield better results than usual entropic affinities with significant improvements in some datasets (_e.g._, throat microarray and SNAREseq). Overall

   Data set & \(^{}}\) & \(^{}\) & \(^{}\) & \(^{}}\) & \(^{}\) \\  Liver (14520) & \(75.8\) & \(75.8\) & \(84.9\) & \(80.8\) & \(\) \\ Breast (70947) & \(\) & \(\) & \(26.5\) & \(23.5\) & \(28.5\) \\ Leukemia (82497) & \(43.7\) & \(44.1\) & \(49.7\) & \(42.5\) & \(\) \\ Colorectal (44076) & \(\) & \(\) & \(\) & \(\) & \(\) \\ Liver (76427) & \(76.7\) & \(76.7\) & \(\) & \(81.1\) & \(81.1\) \\ Breast (45827) & \(43.6\) & \(53.8\) & \(74.7\) & \(71.5\) & \(\) \\ Colorectal (21510) & \(57.6\) & \(57.6\) & \(54.7\) & \(\) & \(79.3\) \\ Renal (53757) & \(47.6\) & \(47.6\) & \(\) & \(\) & \(\) \\ Prostate (6919) & \(12.0\) & \(13.0\) & \(13.2\) & \(16.3\) & \(\) \\ Throm (42743) & \(9.29\) & \(9.29\) & \(11.4\) & \(11.8\) & \(\) \\  scGEM & \(57.3\) & \(58.5\) & \(\) & \(69.9\) & \(71.6\) \\ SNAREseq & \(8.89\) & \(9.95\) & \(46.3\) & \(55.4\) & \(\) \\   

Table 2: ARI (\( 100\)) clustering scores on genomics.

\(^{se}\) outperforms all the other affinities in \(8\) out of \(12\) datasets. This shows that the adaptivity of EAs is crucial. Figure 6 also shows that this superiority is verified for the whole range of perplexities. This can be attributed to the fact that symmetric entropic affinities combine the advantages of doubly stochastic normalization in terms of clustering and of EAs in terms of adaptivity. In the next experiment, we show that these advantages translate into better clustering and neighborhood retrieval at the embedding level when running SNEkhorn.

**Dimension Reduction.** To guarantee a fair comparison, we implemented not only SNEkhorn, but also t-SNE and UMAP in PyTorch. Note that UMAP also relies on adaptive affinities but sets the degree of each node (related to the hyperparameter n_neighbors which plays a similar role to the perplexity) rather than the entropy. All models were optimized using ADAM with default parameters and the same stopping criterion: the algorithm stops whenever the relative variation of the loss becomes smaller than \(10^{-5}\). For each run, we draw independent \((0,1)\) coordinates and use this same matrix to initialize all the methods that we wish to compare. To evaluate the embeddings' quality, we make use of the silhouette and trustworthiness

    &  &  \\   & UMAP & t-SNE & t-SNEkhorn & UMAP & t-SNE & t-SNEkhorn \\  COIL & \(20.4 3.3\) & \(30.7 6.9\) & \(\) & \(99.6 0.1\) & \(99.6 0.1\) & \(\) \\ OILVETTI & \(6.4 4.2\) & \(4.5 3.1\) & \(\) & \(96.5 1.3\) & \(96.2 0.6\) & \(\) \\ UMNIST & \(-1.4 2.7\) & \(-0.2 1.5\) & \(\) & \(93.0 0.4\) & \(99.6 0.2\) & \(\) \\ CIFAR & \(13.6 2.4\) & \(18.3 0.8\) & \(\) & \(90.2 0.8\) & \(90.1 0.4\) & \(\) \\  Liver (14520) & \(49.7 1.3\) & \(50.9 0.7\) & \(\) & \(89.2 0.7\) & \(90.4 0.4\) & \(\) \\ Breast (70947) & \(28.6 0.8\) & \(29.0 0.2\) & \(\) & \(90.9 0.5\) & \(91.3 0.3\) & \(\) \\ Leukemia (23497) & \(22.3 0.7\) & \(20.6 0.7\) & \(\) & \(90.4 1.1\) & \(92.3 0.8\) & \(\) \\ Colorectal (44076) & \(67.6 2.2\) & \(69.5 0.5\) & \(\) & \(93.2 0.7\) & \(93.7 0.5\) & \(\) \\ Liver (76427) & \(39.4 4.3\) & \(38.3 0.9\) & \(\) & \(85.9 0.4\) & \(89.4 1.0\) & \(\) \\ Breast (45827) & \(35.4 3.3\) & \(39.5 1.9\) & \(\) & \(93.2 0.4\) & \(94.3 0.2\) & \(\) \\ Colorectal (21510) & \(38.0 1.3\) & \(\) & \(35.1 2.1\) & \(85.6 0.7\) & \(\) & \(88.2 0.7\) \\ Renal (3537) & \(44.4 1.5\) & \(45.9 0.3\) & \(\) & \(93.9 0.2\) & \(\) & \(94.0 0.2\) \\ Prostate (6919) & \(5.4 2.7\) & \(8.1 0.2\) & \(\) & \(77.6 1.8\) & \(\) & \(73.1 0.5\) \\ Throat (42743) & \(26.7 2.4\) & \(28.0 0.3\) & \(\) & \(\) & \(88.6 0.8\) & \(86.8 1.0\) \\  scGEM & \(26.9 3.7\) & \(33.0 1.1\) & \(\) & \(95.0 1.3\) & \(96.2 0.6\) & \(\) \\ SNAREseq & \(6.8 6.0\) & \(35.8 5.2\) & \(\) & \(93.1 2.8\) & \(99.1 0.1\) & \(\) \\   

Table 3: Scores for the UMAP, t-SNE and t-SNEkhorn embeddings.

Figure 6: ARI spectral clustering score as a function of the perplexity parameter for image datasets.

Figure 7: SNAREseq embeddings produced by t-SNE and t-SNEkhorn with \(=50\).

scores from scikit-learn  with default parameters. While the former relies on class labels, the latter measures the agreement between the neighborhoods in input and output spaces, thus giving two complementary metrics to properly evaluate the embeddings. The results, presented in Table 3, demonstrate the notable superiority of t-SNEhorn compared to the commonly used t-SNE and UMAP algorithms. A sensitivity analysis on perplexity can also be found in Appendix B. Across the \(16\) datasets examined, t-SNEhorn almost consistently outperformed the others, achieving the highest silhouette score on \(15\) datasets and the highest trustworthiness score on \(12\) datasets. To visually assess the quality of the embeddings, we provide SNAREseq embeddings in Figure 7. Notably, one can notice that the use of t-SNEhorn results in improved class separation compared to t-SNE.

## 6 Conclusion

We have introduced a new principled and efficient method for constructing symmetric entropic affinities. Unlike the current formulation that enforces symmetry through an orthogonal projection, our approach allows control over the entropy in each point thus achieving entropic affinities' primary goal. Additionally, it produces a DS-normalized affinity and thus benefits from the well-known advantages of this normalization. Our affinity takes as input the same perplexity parameter as EAs and can thus be used with little hassle for practitioners. We demonstrate experimentally that both our affinity and DR algorithm (SNEhorn), leveraging a doubly stochastic kernel in the latent space, achieve substantial improvements over existing approaches.

Note that in the present work, we do not address the issue of large-scale dependencies that are not faithfully represented in the low-dimensional space . The latter shall be treated in future works. Among other promising research directions, one could focus on building multi-scale versions of symmetric entropic affinities  as well as fast approximations for SNEkhorn forces by adapting _e.g._, Barnes-Hut  or interpolation-based methods  to the doubly stochastic setting. It could also be interesting to use SEAs in order to study the training dynamics of transformers .