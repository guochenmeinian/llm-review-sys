# General bounds on the quality of Bayesian coresets

Trevor Campbell

Department of Statistics

University of British Columbia

trevor@stat.ubc.ca

https://trevorcampbell.me

###### Abstract

Bayesian coresets speed up posterior inference in the large-scale data regime by approximating the full-data log-likelihood function with a surrogate log-likelihood based on a small, weighted subset of the data. But while Bayesian coresets and methods for construction are applicable in a wide range of models, existing theoretical analysis of the posterior inferential error incurred by coreset approximations only apply in restrictive settings--i.e., exponential family models, or models with strong log-concavity and smoothness assumptions. This work presents general upper and lower bounds on the Kullback-Leibler (KL) divergence of coreset approximations that reflect the full range of applicability of Bayesian coresets. The lower bounds require only mild model assumptions typical of Bayesian asymptotic analyses, while the upper bounds require the log-likelihood functions to satisfy a generalized subexponentiality criterion that is weaker than conditions used in earlier work. The lower bounds are applied to obtain fundamental limitations on the quality of coreset approximations, and to provide a theoretical explanation for the previously-observed poor empirical performance of importance sampling-based construction methods. The upper bounds are used to analyze the performance of recent subsample-optimize methods. The flexibility of the theory is demonstrated in validation experiments involving multimodal, unidentifiable, heavy-tailed Bayesian posterior distributions.

## 1 Introduction

Large-scale data is now commonplace in scientific and commercial applications of Bayesian statistics. But despite its prevalence, and the corresponding wealth of research dedicated to scalable Bayesian inference, there are still suprisingly few general methods that provably provide inferential results, within some reasonable tolerated error, at a significant computational cost savings. Exact Markov chain Monte Carlo (MCMC) methods require many full passes over the data [1, Ch. 6-12, 2, Ch. 11-12], limiting the utility of these methods when even a single pass is expensive. A wide range of MCMC methods that access only a subset of data per iteration, e.g., via delayed acceptance [3-6], pseudomarginal or auxiliary variable methods [7-9], and basic subsampling [10-13], provide at most a minor improvement over full-data MCMC [14-16]. On the other hand, methods including carefully constructed log-likelihood function control variates can provide substantial gains [17-19]. However, black-box control variate constructions for large-scale data often rely on assumptions such as posterior density differentiability and unimodality that do not hold in many popular models, e.g., those with discrete variables or multimodality. See  for a survey of scalable MCMC methods. Parametric approximations via variational inference  or the Laplace approximation  can be obtained scalably using stochastic optimization methods, but existing general theoretical guarantees for these methods again typically rely on posterior normality assumptions [24, p. 141-144,25-30] (see  for a review).

Although many existing methods rely on asymptotic normality or unimodality in the large-scale data regime, the problem of handling large-scale data in Bayesian inference does not fundamentally require this structure. Instead, one can more generally exploit _redundancy_ in the data (i.e., the existence of good approximate sufficient statistics), which can be used to draw principled conclusions about a large data set based only on a small fraction of examples. Indeed, while approximate posterior normality often does not hold in models with latent discrete or combinatorial objects, weakly identifiable or unidentifiable parameters, persisting heavy tails, multimodality, etc., such models can and regularly do exhibit significant redundancy in the data that can be exploited for faster large-scale inference. _Bayesian coresets_--which involve replacing the full dataset during inference with a sparse weighted subset--are based on this notion of exploiting data redundancy. Empirical studies have shown the existence of high-quality coreset posterior approximations constructed from a small fraction of the data, even in models that violate posterior normality assumptions and for which standard control variate techniques work poorly . However, existing theoretical support for Bayesian coresets in the literature is limited. There exist no lower bounds on Bayesian coreset approximation error, and while upper bounds do exist, they currently impose restrictive assumptions. In particular, the best available theoretical upper bounds to date apply to exponential family models  and models with strongly log-concave and locally smooth log-densities .

This article presents new theoretical techniques and results regarding the quality of Bayesian coreset approximations. The main results are two general large-data asymptotic lower bounds on the KL divergence (Theorems 3.3 and 3.5), as well as a general upper bound on the KL divergence (Theorem 5.3) under the assumption that the log-likelihoods satisfy a multivariate generalization of subexponentiality (Definition 5.2). The main general results in this paper lead to various novel insights about specific Bayesian coreset construction methods. Under mild assumptions,

* common importance-weighted coreset constructions (e.g. ) require a coreset size \(M\) proportional to the dataset size \(N\) (Corollary 4.1), even with post-hoc optimal weight scaling (Corollary 4.2), and thus yield a negligible improvement over full-data inference;
* _any_ construction algorithm requires a coreset size \(M>d\) when the log-likelihood function is determined by \(d\) parameters locally around a point of concentration (Corollary 4.3);
* subsample-optimize coreset construction algorithms (e.g. ) achieve an asymptotically bounded error with a coreset size \(N\) in a wide variety of models (Corollary 6.1).

The paper includes empirical validation of the main theoretical claims on two models that violate common assumptions made in the literature: a multimodal, unidentifiable Cauchy location model with a heavy-tailed prior, and an unidentifiable logistic regression model with a heavy-tailed prior and persisting posterior heavy tails. Experiments were performed on a computer with an Intel Core i7-8700K and 32GB of RAM. Proofs of all theoretical results may be found in Appendix A.

**Notation.** We use standard asymptotic growth symbols \(O,,,o,\) (see, e.g., [40, Sec. 3.3]), and their probabilistic variants \(O_{p},_{p},_{p},o_{p},_{p}\) (see, e.g., [24, Sec. 2.2]). We use the same symbol to denote a measure \(\) and its density \(()\) with respect to a specified dominating measure. We also regularly suppress integration variables and differential symbols in integrals throughout for notational brevity when these are clear from context; for example, \(()\) is shorthand for \(()(())\). Finally, the pushforward of a measure \(\) by a map \(\) is denoted simply \(\).

## 2 Background

Define a target probability distribution \(\) on a space \(\) comprised of a sum of \(N\) potentials \(_{n}:\), \(n=1,,N\) and a base distribution \(_{0}()\),

\[()=(())_{0}( ), ()=_{n=1}^{N}_{n}(), ,\]

where the normalization constant \(Z\) is not known. In the Bayesian context, this distribution corresponds to a Bayesian posterior distribution for a statistical model with prior \(_{0}\) and conditionally i.i.d. data \(X_{n}\), where \(_{n}()= p(X_{n}|)\). The goal is to compute or approximate expectations under \(\); but the likelihood \(\) (and its gradient) becomes expensive to evaluate when \(N\) is large. Toavoid this cost, _Bayesian coresets_ involve replacing the target with a surrogate density

\[_{w}()=(_{w}() )_{0}(),_{w}()=_{n=1}^{N}w_{ n}_{n}(),,\]

where \(w^{N}\), \(w 0\) are a set of weights, and \(Z(w)\) is the new normalizing constant. If \(w\) has at most \(M N\) nonzeros, the \(O(M)\) cost of evaluating \(_{n}w_{n}_{n}\) (and its gradient) is a significant improvement upon the original \(O(N)\) cost. In this work, the problem of coreset construction is formulated in the data-asymptotic limit; a coreset construction method should

* run in \(o(N)\) time and memory (or at most \(O(N)\) with a small leading constant),
* produce a small coreset of size \(M=o(N)\),
* produce a coreset with \(O(1)\) posterior forward/reverse KL divergence as \(N\).

These three desiderata ensure that the effort spent constructing and sampling from the coreset posterior is worthwhile: the coreset provides a meaningful reduction in computational cost compared with standard Markov chain Monte Carlo algorithms, and has a bounded approximation error.

## 3 Lower bounds on approximation error

This section presents lower bounds on the KL divergence of coreset approximations for general models and data generating processes. The first key steps in the analysis are to write all expectations in terms of distributions that do not depend on \(w\), and to remove the difficult-to-control influence of the tails of \(\) and \(_{w}\) by restricting certain integrals to some small subset \(B\) of the parameter space. Lemma 3.1, the key theoretical tool used in this section, achieves both of these two goals; note that the result has no major assumptions and applies generally in any setting that a Bayesian coreset can be used. For convenience, define

\[}(w):=\{(_{w}||), (||_{w})\},\]

and the decreasing, nonnegative function \(f:_{+}_{+}\),

\[f(x)=\{- x+x-1&0 x 1\\ 0&x>1..\]

**Lemma 3.1** (Basic KL Lower Bound).: _For all measurable \(B\) and coreset weights \(w\),_

\[}(w) f(J_{B}(w)) 0,\]

_where_

\[J_{B}(w)=_{0}(+_{w})}{ ()_{0}(_{w})}}+)}.\]

Note that while the integrals in the fraction denominator in \(J_{B}(w)\) range over the whole \(\) space, a further lower bound on \(}(w)\) can be obtained by restricting their domains arbitrarily. Also, crucially, the bound in Lemma 3.1 does not depend on \(_{w}(B^{c})\), which would be difficult to analyze without detailed knowledge of the tail behaviour of \(_{w}\) as a function of the coreset weights \(w\). Although the bound in Lemma 3.1 applies generally, it is most useful when \(B\) is small (so that simple local approximations of \(\) and \(_{w}\) can be used), \(\) concentrates on \(B\) (so that \((B^{c}) 0\)), and \(\) and \(_{w}\) are very different when restricted to \(B\); the behaviour of the bound in this case is roughly (see the proof in Appendix A) \(f(J_{B}(w))-(1-(,_{w}))\). Finally, note that Lemma 3.1 remains valid if one replaces \(_{w}\) with \(_{w}-c\) and \(\) with \(-c^{}\) for any constants \(c,c^{}\) that do not depend on \(\) but may depend on the data and coreset weights \(w\).

For the remainder of this section, consider the setting where \(\) is a measurable subset of \(^{d}\) for some \(d\), fix some \(_{0}\), and assume each \(_{n}\) is differentiable in a neighbourhood of \(_{0}\). Let

\[=_{n}w_{n} g=(_{0}) g_{w} =_{w}(_{0}).\]Theorems 3.3 and 3.5 characterize KL divergence lower bounds in terms of the sum of the coreset weights \(\) and the log-likelihood gradients \(g,g_{w}\). Intuitively for the full data set where all \(w_{n}=1\) and \(=N\), and an i.i.d. data generating process from the likelihood with parameter \(_{0}\), the central limit theorem asserts under mild conditions that \(g_{w}/}{{}}0\) at a rate of \(N^{-1/2}\). Theorems 3.3 and 3.5 below provide KL lower bounds when the coreset construction algorithm does not match this behavior. In particular, Theorem 3.3 provides results that are useful when \(g_{w}/}{{}}0\) occurs reasonably quickly but slower than \(N^{-1/2}\), while Theorem 3.5 strengthens the conclusion when \(g_{w}/}{{}}0\) very slowly or not at all. The major benefit of Theorems 3.3 and 3.5 for analyzing coreset construction methods is that they reduce the problem of analyzing posterior KL divergence to the much easier problem of analyzing the 2-norm \(\|\|_{2}\) of a weighted sum of random vectors in \(^{d}\).

Consider a sequence \(r 0\) as \(N\), and for a fixed matrix \(H 0\) let

\[B=\{:(-_{0})^{T}H(-_{0}) r^{2}\}\]

be a sequence of neighbourhoods around \(_{0}\); these will appear in Assumptions 3.2 and 3.4 and Theorems 3.3 and 3.5 below. Note that throughout, all asymptotics will be taken as \(N\), and various sequences (e.g., \(r\) and \(B\)) are implicitly indexed by \(N\). To simplify notation, this dependence is suppressed. Assumption 3.2 makes some weak assumptions about the model and data generating process: it intuitively asserts that the potential functions are sufficiently smooth around \(_{0}\), that \(r 0\) slowly, and that \(\) concentrates at \(_{0}\) at a usual rate. Note that Assumption 3.2 does not assume data are generated i.i.d. and places no conditions on the coreset construction algorithm.

**Assumption 3.2**.: \(_{0}\) _has a density with respect to the Lebesgue measure, \(_{0}(_{0})>0\), each \(_{n}()\) and \(_{0}()\) are twice differentiable in \(B\) for sufficiently large \(N\), and_

\[_{ B}-^{2}()-H_{2 }=o_{p}(1),_{2}=O_{p}N^{-1/2} , Nr^{2}=(1).\]

Two additional assumptions related to the coreset construction algorithm--namely, that it works well enough that \(}_{n}w_{n}^{2}_{n}()}{{}}H\) and \(g_{w}/}{{}}0\) at a rate faster than \(r 0\)--lead to asymptotic lower bounds on the best possible quality of coresets produced by the algorithm, as well as lower bounds even after optimal post-hoc scaling of the weights.

**Theorem 3.3**.: _Suppose Assumption 3.2 holds. If_

\[_{ B}-}^{2}_{w}() -H_{2}=o_{p}(1),}{} _{2}=o_{p}(r),\]

_then_

\[}(w)  O_{p}(1)+_{p}(1)-(B^{c}),}{N+}-}{} _{2}^{2}+d)^{2}}{N\{,1/r^{ 2}\}}}\] \[_{ 0}}( w)  O_{p}(1)+_{p}(1)-(B^{c}),d(N \|-}{}\|_{2}^{2})}.\]

Theorem 3.3 is restricted to the case where the coreset algorithm is performing reasonably well. Theorem 3.5 extends the bounds to the case where the algorithm is performing poorly, in the sense that it is unable to make \(}{}}{{}}0\) at a rate faster than \(r 0\) (or perhaps \(}{}\) does not converge to 0 at all). In order to draw conclusions in this setting, we need a weak global assumption on the potential functions. A function \(f:\) is \(L\)_-smooth below at \(_{0}\)_ if

\[, f() f(_{0})+ f(_{0})^ {T}(-_{0})-\|-_{0}\|_{2}^{2}.\] (1)

Note that \(L\)-smoothness below is weaker than Lipschitz smoothness and does not imply concavity; Eq. (1) restricts the growth of the function only in the negative direction, and only when the expansion is taken at \(_{0}\). Assumption 3.4 asserts that the potential functions are smooth below.

**Assumption 3.4**.: _There exist \(L_{0},,L_{N},L>0\) such that \(_{0}\) is \(L_{0}^{2}\)-smooth below at \(_{0}\), for each \(n[N]\)\(_{n}\) is \(L_{n}^{2}\)-smooth below at \(_{0}\), and \(_{n=1}^{N}L_{n}^{2}}{{}}L^{2}\)._Theorem 3.5 uses Assumptions 3.2 and 3.4 and additional assumptions related to the coreset construction algorithm to obtain lower bounds in a setting that relaxes the "performance" conditions in Theorem 3.3: \(-}_{n}w_{n}^{2}_{n}()\) no longer needs to converge to \(H\) in probability, and \(g_{w}/\) can converge to 0 slowly or not at all.

**Theorem 3.5**.: _Suppose Assumptions 3.2 and 3.4 hold. If there exist \(,>0\) such that_

\[ B,\ -}^{2} _{w}() H 1,}_{n}w_{n}L_{n}^{2} L^{2} 1, }{}=_{p}(r),\]

_then_

\[}(w) O_{p}(1)+_{p}(1)-( B^{c}),dN\|}{}\|^{ 2},1}}.\]

An important final note in this section is that while Theorems 3.3 and 3.5, as stated, require choosing \(\) to be some measurable subset of \(^{d}\) and that the posterior \(\) concentrates around some point of interest \(_{0}^{d}\), these results can be generalized to a wider class of models and spaces. In particular, Corollary 3.6 demonstrates that if \(\) is arbitrary, but the potential functions \(_{n}\) only depend on \(\) through some other function \(:^{d}\), that the conclusions of Theorems 3.3 and 3.5 still hold.

**Corollary 3.6**.: _Suppose \(\) is an arbitrary measurable space, and the potential functions take the form \(_{n}(())\) for some measurable function \(:^{d}\). Then if the assumptions of Theorems 3.3 and 3.5 hold for potentials \((_{n})_{n=1}^{N}\) as functions on \(^{d}\) and pushforward prior \(_{0}\) on \(^{d}\), the stated lower bounds also hold for \(\{(||_{w}),(_{w}||)\}\)._

## 4 Lower bound applications

In this section, the general theoretical results from Section 3 are applied to specific algorithms, Bayesian models, and data generating processes to explain previously observed empirical behaviour of coreset construction, as well as to place fundamental limits on the necessary size of coresets. Consider a setting where the data \(X_{n}\) arise as an i.i.d. sequence drawn from some probability distribution \(\), \(_{n}(())= p(X_{n}|())\) for \(:^{d}\), \(_{0}=(_{0})\), and the following technical criteria hold (where \(\) denotes expectation under the data generating process):

1. \([_{n}(_{0})]=0\) and \(H=[-^{2}_{n}(_{0})]=[ _{n}(_{0})_{n}(_{0})^{T}] 0\).
2. \([\|_{n}(_{0})\|_{2}^{2+}]<\) for some \(>0\) and \([\|^{2}_{n}(_{0})\|_{F}^{2}]<\).
3. On a neighbourhood of \(_{0}\), \(\|^{2}_{n}()-^{2}_{n}(_{0})\|_{2} R(X_{n})\| -_{0}\|_{2}\), \([R(X_{n})]<\).
4. \(_{0}\) is twice differentiable a neighbourhood of \(_{0}\), and \((_{0})>0\).
5. For all \(r 0\) such that \(r^{2}=( N/N)\), \(-(\|-_{0}\|>r)=_{p}(Nr^{2})\).

These conditions apply to a wide range of models, e.g., an unidentifiable, multimodal location model posterior with heavy tails on \(=\), where the Bayesian model is specified by

\[(0,1)(X_{n})_{n=1}^{N} }}{{}}(^{2},1),\] (2)

and the data are generated from the likelihood with parameter \(_{0}=5\), and an unidentifiable logistic regression posterior with heavy tails on \(^{2}\), where the Bayesian model is specified by

\[(0,I) Y_{n}}}{{}}^{T}A}}  A=1&1\\ 1&1,\] (3)

the covariates are generated via \(X_{n}}}{{}}(\{x ^{2}:\|x\|_{2} 1\})\), and the observations \(Y_{n}\) are generated from the likelihood with parameter \(_{0}=[1 6]^{T}\). See Proposition A.6 in Appendix A for the verification of (A1-5) for these two models. Example posterior log-densities for these models are displayed in Fig. 1.

### Minimum coreset size for importance-weighted coresets

A popular algorithm for coreset construction that has appeared in a wide variety of domains--e.g., Bayesian inference [32; 33; Section 4.1], frequentist inference (e.g., [41; 42; 43; 44; 45]), and optimization (see  for a recent survey)--involves subsampling of the data followed by an importance-weighting correction. The pseudocode is given in Algorithm 1. Note that \([w_{n}]=1\), and so \([_{w}]=\); the coreset potential is an unbiased estimate of the exact potential. The advantage of this method is that it is straightforward and computationally efficient. If the sampling probabilities are uniform \(p_{n}=}{{N}}\), then Algorithm 1 constructs a coreset in \(O(M)\) time and \(O(M)\) memory. Nonuniform probabilities \(p_{n}\) require \((N)\) time, as they require a pass over all \(N\) data points to compute each \(p_{n}\)[32; 42] followed by sampling the coreset, e.g., via an alias table [47; 48]. However, empirical results produced by this methodology have generally been underwhelming, even with carefully chosen sampling probabilities; see, e.g., Figure 2 of .

```  Compute probabilities \((p_{n})_{n=1}^{N}\) (may depend on the data and model)  Draw \(I_{1},,I_{M}}}{{}}(p_{1},,p_{N})\)  For each \(n\), set \(w_{n}=}_{m=1}^{M}[I_{m}=n]\). return\((w_{n})_{n=1}^{N}\) ```

**Algorithm 1** Importance-weighted coreset construction

Corollary 4.1 explains these poor results: Bayesian coresets constructed via Algorithm 1 must satisfy \(M N\) in order to maintain a bounded \(}(w)\) in the data-asymptotic limit. In other words, such coresets do not satisfy the desiderata in Section 2. The only restriction is that there exist constants \(c,C>0\) such that for all \(N\), the sampling probabilities \((p_{n})_{n=1}^{N}\) satisfy

\[ 0<c_{n}Np_{n}_{n}Np_{n} C< a.s.\] (4)

The lower threshold ensures that the variance of the importance-weighted log-likelihood is not too large, while the upper threshold ensures sufficient diversity in the draws from subsampling. The condition in Eq. (4) is not a major restriction, in the sense that performance should deteriorate even further when it does not hold. The \((p_{n})_{n=1}^{N}\) may otherwise depend arbitrarily on the data and model.

Corollary 4.1 ().: _Given (A1-6), \(M\), and \(M=o(N)\), coresets produced by Algorithm 1 satisfy_

\[}(w)=_{p}().\] (5)

Figure 1: Example unnormalized posterior densities given 50 data points for (1a) the Cauchy location model and (1b) the logistic regression model. The orange and blue dashed lines in (1b) indicate one-dimensional slices that are shown in the rightmost panels.

The intuition behind Corollary 4.1 is that both the true posterior and the importance-weighted coreset posterior are asymptotically approximately normal with variance \( 1/N\) as \(N\); however, the coreset posterior mean is roughly \( M^{-1/2}\) away from the posterior mean, because the subsample is of size \(M\). The KL divergence between two Gaussians is lower-bounded by the inverse variance times the mean difference squared, yielding \( N/M\) as in Eq. (5).

Given the intuition that the coreset posterior mean is far from the posterior mean relative to their variances, it is worth asking whether one can apply a small amount of effort to "correct" the importance-weighted coreset by scaling the weights (and hence the variance) down, as shown in Algorithm 2. Unfortunately, Corollary 4.2 demonstrates that even with optimal scaling, \(M N\) is still required in order to maintain a bounded KL divergence as \(N\).

**Corollary 4.2**.: _Given (A1-6), \(M\), and \(M=o(N)\), coresets produced by Algorithm 1 satisfy_

\[_{>0}}( w)=_{p}( ).\]

Fig. 2 provides empirical confirmation of Corollaries 4.1 and 4.2 on the Cauchy location and logistic regression models in Eqs. (2) and (3). In particular, these figures show that the empirical rates of growth of KL as a function of \(N\) closely matches \(_{p}()\) for importance-weighted coresets, and \(_{p}()\) for the same with post-hoc scaling, for a wide range of coreset sizes \(M\{ N,,}{{2}}N\}\). Thus, importance weighted coreset construction methods do not satisfy the desiderata in Section 2 for a wide range of models, and alternate methods should be considered.

### Minimum coreset size for any coreset construction

This section extends the minimum coreset size results from importance-weighted schemes to _any_ coreset construction algorithm. In particular, Corollary 4.3 shows that under (A7)--a strengthening of (A3) and Assumption 3.4--and (A8)--which asserts that \(_{1}(_{0}),,_{M}(_{0})\) are linearly independent a.s. and satisfy a technical moment condition--at least \(d\) coreset points are required to keep the KL divergence bounded as \(N\).

* Assumption 3.4 holds and there exists \(>0\) such that for all sufficiently large \(N\), \[ B,n[N],-^{2}_{n}() H  L_{n}^{2}<^{-1}L^{2}.\]
* For all coreset sizes \(M<d\), there exists a \(>0\) such that \[1^{T}(G^{T}G)^{-1}1^{M+}<  G=[_{1}(_{0})_{M}(_{0})] ^{d M}.\]

**Corollary 4.3**.: _For a fixed coreset size \(M<d\), given (A1-5,7,8),_

\[_{w^{N}_{+}\|w\|_{0} M}}(w)= _{p}( N).\]

## 5 Upper bounds on approximation error

This section presents upper bounds on the KL divergence of coreset approximations. As in Section 3, the first step is to write all expectations in terms of distributions that do not depend on \(w\). Lemma 5.1 does so without imposing any major assumptions; the result again applies generally in any setting that a Bayesian coreset can be used. For convenience, define

\[}(w):=\{(_{w}||), (||_{w})\}.\]

**Lemma 5.1** (Basic KL Upper Bound).: _For all coreset weights \(w\),_

\[}(w)_{>0} (1+)(_{w}-),\]

_where for all \(n[N]\), \(_{n}=_{n}-_{n}\), \(=_{n}_{n}\), and \(_{w}=_{n}w_{n}_{n}\)._

The upper bound in Lemma 5.1 is nonvacuous (i.e., finite) as long as there exists a \(>1\) such that the \(\) Renyi divergence \(D_{}(_{w}||)\)[49, p. 3799] is finite. Note that as in Lemma 3.1, the bound in Lemma 5.1 remains valid if one replaces \(_{w}\) with \(_{w}-c\) and \(\) with \(-c^{}\) for any constants \(c,c^{}\) that do not depend on \(\) but may depend on the coreset weights \(w\) and data.

More practical bounds necessitate an assumption about the behaviour of the potentials \((_{n})_{n=1}^{N}\). Definition 5.2 below asserts that the multivariate moment generating function of \((_{n})_{n=1}^{N}\) is bounded when the vector is close to 0. This definition is a generalization of the usual definition of subexponentiality for the univariate setting (e.g., [50, Sec. 2.7]). Theorem 5.3 subsequently shows that Definition 5.2 is sufficient to obtain simple bounds on \(}\).

**Definition 5.2**.: _For \(A^{N N}\), \(A 0\), and monotone function \(h:_{+}_{+}\) such that \(_{x 0}h(x)=h(0)=0\), the potentials \((_{n})_{n=1}^{N}\) are \((h,A)\)-subexponential if_

\[ w^{N}:w^{T}Aw 1,_{ w}h(w^{T}Aw).\]

**Theorem 5.3**.: _If the potentials \((_{n})_{n=1}^{N}\) are \((h,A)\)-subexponential, then_

\[ w_{+}^{N}:4(w-1)^{T}A(w-1) 1,}( w) h(4(w-1)^{T}A(w-1)).\]

Definition 5.2, the key assumption in Theorem 5.3, is satisfied by a wide range of models when choosing \(h(x)=x\) and \(A_{}(_{n})_{n=1}^{N}\), as demonstrated by Proposition 5.4. Because this case applies widely, let \(A\)-subexponential be shorthand for \((h,A)\)-subexponentiality with \(h(x)=x\).

**Proposition 5.4**.: _If for all \(w\) in a ball centered at the origin, \((_{w})<\), then there exists \(>0\) such that the potentials \((_{n})_{n=1}^{N}\) are \(\,_{}(_{n})_{n=1}^{N}\)-subexponential._

Figure 2: Importance-weighted coreset quality, showing the minimum of the forward and reverse KL divergences on the vertical axis as a function of dataset size \(N\) for 3 coreset sizes: \( N\) (black), \(\) (blue), and \(}{{2}}N\) (red). Dashed lines indicate predictions from the theory in Corollaries 4.1 and 4.2, solid lines indicate the mean over 10 trials, and error bars indicate standard error. The top row shows the quality of basic importance-weighted coresets (note that both horizontal and vertical axes are in log scale), while the bottom row shows the quality with optimal post-hoc scaling (note that only the horizontal axis is in log scale). The left column corresponds to the Cauchy location model, while the right column corresponds to the logistic regression model. Sampling probabilities \(p_{n}\) for both models are set proportional to \(X_{n}^{2}\), thresholded to lie between \(0.1/N\) and \(10/N\).

In other words, intuitively, if a coreset construction algorithm produces weights such that \(_{}(_{w}-)\) is small, then \(}(w)\) is also small. That being said, the generality of Definition 5.2 to allow arbitrary \(h,A\) is still helpful in obtaining upper bounds in specific cases; see, e.g., Propositions A.1 and A.2.

## 6 Upper bound application: subsample-optimize coresets

A strategy to construct Bayesian coresets that has recently emerged in the literature, shown in Algorithm 3, is to first subsample the data to select \(M\) data points, and subsequently optimize the weights for those selected data points [36; 37; 38]. The subsampling step serves to pick a reasonably flexible basis of log-likelihood functions for coreset approximation, and avoids the slow greedy selection routines from earlier work [33; 34; 35]. The optimization step tunes the weights for the selected basis, avoiding the poor approximations of importance-weighting methods. Indeed, Algorithm 3 creates exact coresets \(_{w^{*}}=\) with high probability in Gaussian location models [36, Prop. 3.1] and finite-dimensional exponential family models [37, Thm. 4.1], and near-exact coresets with high probability in strongly log-concave models [37, Thm. 4.2] and Bayesian linear regression [38; Prop. 3].

Corollary 6.1 generalizes these results substantially, and demonstrates that coresets of size \(M=O((N))\) produced by the subsample-optimize method in Algorithm 3 maintain a bounded KL divergence as \(N\). Two key assumptions are subexponentiality of the potentials and a polynomial (in \(N\)) growth of \(_{}(())\); these conditions are not stringent and should hold for a wide range of Bayesian models and i.i.d. data generating processes. The last key assumption in Eq. (6) is that a randomly-chosen potential function \(_{I}\), \(I(p_{1},,p_{N})\) (with probabilities as in Algorithm 3) is well-aligned with the residual coreset error function. Similar alignment conditions have appeared in past results for more restrictive settings (see, e.g., \(J()\) in [37, Thm. 4.1]).

**Corollary 6.1**.: _Suppose there exist \(,>0\) and \(0,<1\) such that the potential functions \((_{n})_{n=1}^{N}\) are \(_{}((_{n})_{n=1}^{N})\)-subexponential with probability increasing to 1 as \(N\), \(_{}(())=O_{p}(N^{})\), \(M=( N)^{}\), and_

\[0,_{}_{I_{M }}(),()-_{M-1}^{*}()}^{2} 1- (_{n})_{n=1}^{N}=_{p}(M^{-})\] (6)

_Then Algorithm 3 produces a coreset with \(}(w)=O_{p}(1)\) as \(N\)._

Fig. 3 confirms that subsample-optimize coreset construction methods applied to the logistic regression and Cauchy location models in Eqs. (2) and (3) (which both violate the conditions of past upper bounds in the literature) are able to provide high-quality posterior approximations for very small coresets--in this case, \(M N\).

## 7 Conclusions

This article presented new general lower and upper bounds on the quality of Bayesian coreset approximations, as measured by the KL divergence. These results were used to draw novel conclusions regarding importance-weighted and subsample-optimize coreset methods, which align with simulation experiments on two synthetic models that violate the assumptions of past theoretical results. Avenues for future work include general bounds on the subexponentiality constant \(\) in Proposition 5.4, as well as the alignment probability in Eq. (6), in the setting of Bayesian models with i.i.d. data generating processes. A limitation of this work is that both quantities currently require case-by-case analysis.