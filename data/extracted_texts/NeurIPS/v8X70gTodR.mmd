# Analysing the Generalisation and Reliability of Steering Vectors

Daniel Tan\({}^{1}\) David Chanin\({}^{1}\) **Aengus Lynch\({}^{1}\)**Brooks Paige\({}^{1}\)**Dimitrios Kanoulas\({}^{1,3}\)**Adria Garriga-Alonso\({}^{2}\)**Robert Kirk\({}^{1}\)

\({}^{1}\) AI Centre, Department of Computer Science, University College London

\({}^{2}\) FAR AI \({}^{3}\) Archimedes/Athena RC

Correspondence to: daniel.tan.22@ucl.ac.uk, robert.kirk.20@ucl.ac.uk

###### Abstract

Steering vectors (SVs) are a new approach to efficiently adjust language model behaviour at inference time by intervening on intermediate model activations. They have shown promise in terms of improving both capabilities and model alignment. However, the reliability and generalisation properties of this approach are unknown. In this work, we rigorously investigate these properties, and show that steering vectors have substantial limitations both in- and out-of-distribution. In-distribution, steerability is highly variable across different inputs. Depending on the concept, spurious biases can substantially contribute to how effective steering is for each input, presenting a challenge for the widespread use of steering vectors. We additionally show steerability is also mostly a property of the dataset rather than the model by measuring steerability across multiple models. Out-of-distribution, while steering vectors often generalise well, for several concepts they are brittle to reasonable changes in the prompt, resulting in them failing to generalise well. Similarity in behaviour between distributions somewhat predicts generalisation performance, but there is more work needed to understand when and why steering vectors generalise correctly. Overall, our findings show that while steering can work well in the right circumstances, there remain many technical difficulties of applying steering vectors to robustly guide models' behaviour at scale.

## 1 Introduction

Steering Vectors (SVs)  have been recently proposed as a technique for guiding language model behaviour at inference time. Existing work has shown promising results in using these SVs to detect and guide models towards high-level traits such as honesty , sycophancy , and positive sentiment . They have also been shown to be useful for improving model capabilities  and red-teaming . SVs are of interest as they enjoy a number of practical benefits over other model adjustment techniques that require adding more information into the context window [5, 38, In-Context Learning], or performing training to adjust model parameters (fine-tuning). Recent work shows that steering vectors can be learned in an unsupervised way , thus removing another obstacle for their use. It may even be possible for different steering vectors to be used in combination for multiple behaviours . It would thus be very important and useful in practice if steering vectors were truly effective.

However, existing work has mostly evaluated SVs in-distribution, and looked at aggregate behaviour. It is unknown how reliable the change in behaviour caused by SVs is, and how well SVs generalise todifferent system or user prompts. In this paper, we extensively evaluate the in-distribution reliability and out-of-distribution generalisation of SVs, extending analysis in Rimsky et al.  to a much broader variety of behaviours from the Model Written Evals (MWE) datasets by Perez et al. . In addition, we consider targeted distribution shifts in the form of inserting prompts via the user message or system message. This setting mimics the practically important setting where we will need to apply SVs to different system and user prompts, and where we would require SVs to generalise well to be robustly useful.

Our first key result is that **for many behaviours studied, steering is unreliable** (Figure 1 and Section 5). For all behaviours evaluated, steerability takes on a large range of values across different inputs, including negative values, where SVs produce the opposite of the desired behaviour. Previous work [30; 33; 43; 18] does not study this variance, which potentially leads to over-optimistic claims on performance due to a lack of error bars. In explaining this variance, we demonstrate a novel type of bias, _steerability bias_, in which models are easier to steer towards outputs with a certain property (i.e. answer position or token choice). The lack of steerability and high variance in steering performance demonstrates that in many cases, a steering vector extracted may not correspond to the intended concept, and applying steering vectors may only be effective in the presence of spurious factors associated with the prompt template or other potential biases.

Our second set of results focuses on the out-of-distribution setting. Here, we find that **SVs generalise reasonably well across different prompt settings, but the generalisation behaviour is not perfect or entirely predictable** (Section 6). SVs generalise better over some shifts than others and generally perform worse out-of-distribution vs in-distribution. We investigate what causes this difference in generalisation properties, finding that (i) steerability is mostly a dataset-level property, with similar datasets being steerable and producing generalisable SVs for two different models; and (ii) SVs generalise better when model behaviour is similar in the source and target prompt setting. This relationship is a potential issue for SVs, as SVs will need to be applied to guide models towards behaviours they do not normally produce.

Overall, our findings indicate that steering vectors in their current form are not a panacea for aligning model behavior at inference time. Despite their promise, more work is required to ensure that steering vectors reliably produce the desired behaviour in a generalisable way and are practically useful.

Figure 1: **Steering effects are not reliable, and often steer in the opposite direction**. We show per-sample steerability and the fraction of anti-steerable examples for a representative sample of 13 datasets (out of 40 total). Many dataset have a high variation in per-sample steerability, and several datasets produce the opposite behaviour for almost 50% of inputs. For all datasets see Figure 18. Some dataset names have been shortened.

Related Work

Steering Vectors (SVs, also known as activation engineering) and related ideas were introduced by Turner et al. , Zou et al. , Liu et al. . SVs can be seen as an inference-time intervention  technique in the representation engineering  toolkit, which is an umbrella term for the broad approach of improving the transparency and controllability of neural networks by examining and intervening on population-level representations and activations of the network. . Rimsky et al.  recently introduced Contrastive Activation Addition (CAA), a specific technique for extracting and applying SVs which we use in this work, due to its simplicity, effectiveness and popularity in the community. Rimsky et al.  demonstrate the effectiveness of CAA in-distribution on several AI alignment-relevant behaviours, while we test on a much broader range of behaviours, investigate the reliability of the steering intervention, and examine out-of-distribution generalisation of SVs. We describe the CAA method in more detail in Section 3.

Compared to fine-tuning [42; 27; 23], steering vectors don't involve changing model parameters, hence potentially avoiding catastrophic forgetting [3; 19]. Compared to in-context learning [5; 38; ICL], steering does not require adding tokens to the prompt, saving inference cost and enabling it to scale beyond the length of the context window. Furthermore, Zou et al.  show that steering interventions are robust to adversarial attacks capable of breaking prompt-based and fine-tuning-based alignment methods [26; 37; 13]. An extended related work section including discussing the relationship between SVs and the Linear Representation Hypothesis [25; LRH] and other works that evaluate the generalisation behaviour of model adjustment methods can be found in Appendix B.

## 3 Preliminaries

Rimsky et al.  propose Contrastive Activation Addition (CAA) to extract and apply steering vectors on datasets. We follow this protocol in our experiments, and so we summarise the main steps here.

Multiple-Choice Contrastive Prompts.We construct a prompt consisting of a question or statement followed by two multiple-choice options labelled "(A)" and "(B)". The model is tasked with reading the question and available options (\(x\)), then choosing one of the options (\(y_{+}\) or \(y_{-}\)). For some datasets these two options are statements, and for others the two options are either "Yes" or "No". A typical example is shown in Figure 9. During preprocessing, we randomise whether 'A' or 'B' (and 'Yes' or 'No' where appropriate) are used as the positive \(y_{+}\) or negative \(y_{-}\) options, to ensure that we do not simply extract a steering vector for e.g. the token 'A' vs the token 'B'.

Steering Vector Extraction.For a given dataset \(\) consisting of triples of the form \((x,y_{+},y_{-})\), and a given layer \(L\), activations are extracted from the residual stream at the multiple-choice option token position for the positive and negative option, to get \(a_{L}(x,y_{+})\) and \(a_{L}(x,y_{-})\) respectively. We extract a steering vector \(v_{MD}\) using the mean difference (MD) of positive and negative activations:

\[v_{MD}=|}_{(x,y_{-},y_{+})}a_{L }(x,y_{+})-a_{L}(x,y_{-})\] (1)

We note that other aggregation methods have been proposed, but literature does not suggest these perform better than mean-difference. We discuss alternatives in Appendix D.4.

Steering Intervention.To apply a steering intervention at layer \(L\) using a steering vector \(v_{L}\), we add \(*v_{L}\) into the activations at the last token position at layer \(L\) during model inference. Here, \(\) is a multiplier that controls the strength of the steering intervention. For any metric of (change in) behaviour, we can evaluate that metric for a range of \(\)s to ascertain the effectiveness of a steering intervention; more details including our specific choice of metric are discussed subsequently in Section 4.2Experiment Design

### Datasets and Prompts

Datasets.We focus on the Model-Written Evaluations (MWE) datasets , a large dataset consisting of prompts from over 100 distinct categories designed to evaluate many specific aspects of models' behaviour. Each category contain 1000 samples generated by an LLM, covering a variety of persona and behaviors. For each of these datasets, we construct a 40-10-50 train-val-test split. We also include TruthfulQA  and the sycophancy dataset , as they were used in CAA . The validation split is used for hyperparameter selection; we discuss this in Section 4.3. We randomly choose three persona datasets from each MWE persona dataset category, while keeping the sycophancy, TruthfulQA, and AI risk datasets used in CAA for a total of 40 datasets.

Distribution Shifts.To evaluate how well steering vectors generalise to out-of-distribution settings, we construct systematic distribution shifts by injecting additional text into the prompts. We design the prompts to elicit more or less of the target behaviour through direct instruction. Sample prompt injections are shown in Table 1. As we investigate instruction-tuned models, there are two valid prompt injection strategies: (i) replacing the default system prompt with the injection, and (ii) pre-pending the injection to the user prompt. We evaluate in both settings for completeness. To evaluate generalisation across these distribution shifts, we extract a SV in one of the prompt settings (e.g. BASE), and apply it to steer behaviour in another setting (e.g. SYS-POS), and denote this BASE \(\) SYS-POS. BASE \(\) BASE hence represents the standard in-distribution evaluation.

To measure OOD generalisation, we define _relative steerability_. This measures how well a steering vector \(v_{A}\) trained on dataset variation \(_{A}\) works on dataset variation \(_{B}\) with multipliers \(\) as:

\[s_{rel}(v_{A},_{B},)=,_{B},) }{s(v_{B},_{B},)}\] (2)

### Metrics

To measure the effectiveness of steering, we need a metric of the model's _propensity_ to exhibit a behavioural trait (e.g. sycophancy , truthfulness , helpfulness ). Given a propensity metric, we then define _propensity curves_ and _steerability_ as summary metrics of the steering vector's effectiveness.

Propensity.In our multiple choice setting, the model exhibits a target trait by outputting the positive option (either "A" or "B", see Figure 9). As such, a natural metric is to compare the logits of the positive and negative tokens (either A or B) respectively. We define the _logit-difference propensity_ metric \(m_{LD}\) as the logit of the positive token minus the logit of the negative token. Concretely:

\[m_{LD}=(y_{+})-(y_{-})\] (3)

Rimsky et al.  instead uses the normalised probability of the positive answer, which is the same except for a softmax applied to the logits. We note that normalised probabilities are a monotonic function of the logit difference, so propensity is order-invariant between these two methods. However, logit-difference is likely to be more linear with respect to the model's intermediate activations (as it doesn't include a softmax), facilitating downstream analysis.

We note that propensity can be measured _per-sample_ or in _aggregate_. Aggregate propensity is useful for measuring broad changes in behaviour across a distribution, and we primarily use this metric when studying steering vector generalisation in Section 6. A concern is that this loses granular per-sample information; we analyse per-sample propensity in detail when steering in-distribution in Section 5.

Propensity Curve.To get a sense of how well steering works as a function of the multiplier \(\), we compute \(m_{LD}\) for various values of \(=\{-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5\}\). We refer to this as a _propensity curve_, which was proposed by . If steering works well, we expect the trend to be monotonic and increasing with high slope.

Steerability.To summarise a propensity curve, we propose a _steerability_ metric. Given a steering vector \(v\), dataset \(\), and multipliers \(=[_{0}_{n}]\), we define steerability \(s(v,,)\) as the slope of a mean-squares line fit to the mean LD scores for \(v\) steering \(\) at each \(_{i}\). The steerability score takes values \(s\). A high positive steerability score indicates that the steering vector is effective. Conversely, a negative steerability score indicates that the steering vector has the opposite of the intended effect. See Figure 2 for a visual example.

### Steering Vector Extraction

Models.Following previous work, we focus on steering instruction-tuned models. We include Llama-2-7b-Chat  as it was used in previous work. In order to draw conclusions that generalise beyond a single model, we also consider Qwen-1.5-14b-Chat , which differs in many aspects, including architecture, parameter count, and training data distributions.

Steering Layer.The choice of which layer to steer at is an important hyperparameter. Loosely, we expect that each layer captures a different level of abstraction in the model's internal computation , and steering will work best if we choose the layer that best matches the target concept's level of abstraction. In order to determine the optimal layer, we sweep over all layers using the validation split. In line with Rimsky et al. , we find that the optimal choice of layer is remarkably consistent across many datasets. Thus, we fix layer 13 for Llama and layer 21 for Qwen for all subsequent experiments. Layer response curves used in selecting the optimal layer are presented in Appendix D.7.

## 5 Evaluating Steering Vector Reliability

We first evaluate how reliably SV produce the desired change in model behaviour in-distribution. For SVs to be useful they need to robustly shift the model's behaviour in the desired direction for all inputs, rather than working on some inputs and not on others. However, we find that for many datasets this is not the case: steerability has high variance, with many inputs being steered in the opposite direction to what is intended.

Steerability Varies Widely Across and Within Concepts.We find that both the sign and magnitude of steerability can vary widely within a concept and across different concepts. As shown in Figure 1, steering has a range of behaviours for different datasets. For some datasets with high median steerability (e.g. corrigible-neutral-HHH), the distribution is unimodal; high probability mass is concentrated around the median (though still with high variance). At the low end of median steerability, it is more common for the distribution to be bimodal, with there being two clusters of steerability which are located further away from the median (e.g. myopic-reward). In some cases, steerability is _negative_ for one of these clusters, which means that the steering vector is having the opposite of the intended effect on these examples. We term this phenomenon _anti-steerability_. Many of these datasets have almost half of the inputs being anti-steerable, implying that the effect of steering is highly unreliable.

Steering is Affected by Spurious Factors.In order to understand the high variance in steerability, we take a closer look at datasets with a high fraction of anti-steerable examples. In these cases, we hypothesise that the steering vector extracted encodes spurious factors, as opposed to the underlying behaviour. Hence, we study whether there are biases that predict steerability.

Figure 2: Example propensity curve and steerability fit for high steerability (left), and low (right).

Due to the multiple-choice template used for steering vector extraction, one such potential bias is towards whether A or B was used to represent the positive option. In the case of the 'persona' datasets, where the responses are always either Yes or No, another potential bias is whether Yes or No represents the positive option. Neither of these biases are present in the training data, as we have randomised the data during steering vector extraction such that the examples are split equally between the two (or four) choices. Despite this, we find these two biases are present (Figure 16) and are often highly predictive of the steerability, explaining a large part of the variance in per-example steerability (Figure 4).

This bias is different from the standard position or token bias exhibited by LLMs , as it is a _steerability_ bias: the model is _more steerable_ towards the positive answer when it a particular position or token compared to the other position or token. The preferred token or position is different for each dataset; for example corrigible-neutral-HHH has a B-steerability bias, whereas self-aware-lm has an A-steerability bias (see Figure 16). This is problematic, as it is not fixable by simple dataset debiasing (which was already performed) or logit calibration adjustments  (as they effect propensity, not the change in propensity, i.e. steerability). Further, it implies that there may be other steerability biases present in models, determining when they are more or less steerable towards specific answers or behaviours. Indeed, there is still a high degree of unexplained variance present in many datasets in Figure 4.

Some Behaviours are Un-Steerable.We empirically observe that many behaviours turn out to be unsteerable, as measured by median steerability shown in Figure 1. One possible explanation is that the datasets we used were too small or low-quality. Other explanations include that unsteerable behaviours are not linearly represented in the model, or that they correspond to multiple separate behaviours within the model's ontology. In the latter case, it would be interesting to develop

Figure 3: **Models exhibit large dataset-dependent steerability bias**. The figure shows mean steerability per dataset for each way in which the positive option is presented. And entirely unbiased result would have all bars being identical. Despite datasets being balanced amongst all possible combinations of options, the mean steerability differs greatly between these splits. While there is a general trend towards preferring ‘Yes’ vs ‘No’, there is still a lot of dataset-dependent variation, and there is no clear trend for ‘A’ vs ‘B’. For full results see Figure 19. Note that some datasets have only two bars, indicating that only the ‘A’/’B’ split is relevant.

methods to disentangle these separate sub-behaviours in an unsupervised way. We consider follow-up investigations for these two hypotheses to be promising directions for future work.

## 6 Steering Out-of-Distribution

SVs will often be applied in situations different from when they are extracted, particularly when the system and user prompt changes, and so we aim to analyse how well SVs generalise in this setting. We find that SVs generalise reasonably well but not perfectly, with some prompt changes having better generalisation that others. We investigate what affects when SVs will generalise, finding that it is mostly a property of the dataset, and that the similarity in behaviour of the unsteered model in the source and target prompt setting is also predictive of SV generalisation.

OOD Settings.For each dataset, we define the ID setting to be when we extract the steering vector from the BASE train split and evaluate it on the BASE test split, as defined in Table 1. We define four OOD distribution shifts. Firstly, we consider the cases where a user prompts the model to stimulate or suppress the target behaviour (BASE\(\)USER_NEG, BASE\(\)USER_POS). Additionally, we hypothesise that the model's base propensity affects the effectiveness of steering vectors. Therefore, we also study the case where the user instruction conflicts with the system prompt for the model, as encapsulated by system prompts (SYS_POS\(\)USER_NEG, SYS_NEG\(\)USER_POS).

ID and OOD Steerability are Correlated.Figure 5 shows that steerability ID and OOD are correlated. We would expect that unsteerable concepts in-distribution are unlikely to steer out-of-distribution, but it is promising for the usefulness of steering vectors that, conditioned on steering vectors working in-distribution, they continue to work well out-of-distribution. However, generalisation is not perfect, and on average steerability is worse OOD than ID, particularly for Owen. The correlation for Owen is also weaker than for Llama.

We also examine how the variance in steerability we demonstrated in Section 5 changes OOD. Figure 6 shows that ID and OOD variance are reasonably well-correlated, with OOD variance perhaps slightly lower than ID variance. This is somewhat surprising, although it may be explained by slightly lower steerability OOD (as lower steerability means lower variance in steerability as shown in Figure 22).

Figure 4: **SVs exhibit high variance, some of which is explained by spurious factors.** The figure shows variance in per-sample steerability by dataset, with attributions to known spurious factors annotated. Marginal Var Explained refers to the variance explained by the ’Yes’/’No’ split after removing variance from the ’A’/’B’ split. For some datasets, spurious factors (orange, green) explain a large percentage of the variance, while for others, most of the variance remains unexplained. For full results see Figure 20.

Steerability is Mostly a Property of the Dataset.We compare aggregate in-distribution and out-of-distribution steerability between Llama and Qwen in Figure 7. We find that both ID and OOD steerabilities are highly correlated across models, despite them having different sizes, architectures, and training procedures. The consistency between different model architectures indicates that the effectiveness of a steering vector is mostly a property of the dataset used to extract the dataset, as opposed to the model used. This may also be evidence that different models converge to similar ontologies .

Model Propensity is Predictive of Steering Generalisation.While steerability and SV generalisation is mostly a dataset-level property, there is still variation in generalisation performance that is not captured by dataset; for example, SVs generalise better over some shifts than others for the same dataset. In Figure 8 show that the similarity in the propensity of the model in two prompt settings is correlated with the relative steerability (Equation (2)), a measure of generalisation. In other words, if the model behaves similar in two prompt settings, then SVs will transfer better between those two settings than if the model behaves differently in the two settings. We show a similar result but for SV cosine similarity in Appendix E.3.

Figure 5: **In-distribution and out-of-distribution steerability are reasonably well-correlated.** We show OOD vs ID steerability for Llama-2-7b (left; \(==0.891\)) and Qwen-1.5-14b (right; \(=0.694\)). While OOD steerability seems correlated with ID steerability, we observe that there are some points far above or below the \(x=y\) line, and this is more noticeable for the Qwen model. Throughout, \(\) refers to Spearman’s rank correlation coefficient.

Figure 6: **In-distribution and out-of-distribution variance in steerability are somewhat correlated.** We show OOD vs ID variance in steerability for Llama-2-7b (left; \(=0.535\)) and Qwen-1.5-14b (right; \(=0.341\)). Generally, variance is slightly lower OOD than ID (as the slope of the lines is \(<1\), although results are somewhat noisy.

## 7 Discussion and Conclusion

Our work is the first to report and analyse the variance in steerability at a per-example level, and in doing so reveal a major limitation in SV reliability. In Section 5, we demonstrated SVs's effects on model behaviour are often unreliable, with some concepts being unsteerable and some SVs producing the opposite behaviour to what is desired. We found that this unreliability is often driven by token- and position-_steerability bias_, a new type of bias we discovered that is distinct from standard token and position biases in LLMs. Although these are very simple biases which can be easily understood, simple interventions in data preprocessing fail to address the problem, and there are likely to be other steerability biases that will affect the effectiveness and reliability of SVs. In Appendix E.2 we show that this variance is partially a dataset property rather than a model property, implying that future work investigating what causes these biases should at least partially focus on the dataset, as well as analysing whether other techniques for extracting and applying SVs can mitigate these biases.

Figure 8: **Propensity similarity is correlated with SV generalisation.**. We plot relative steerability (Equation (2)) against the difference in unsteered training dataset \(m_{LD}\) (Equation (3)) for Llama2-7B (left; \(=-0.26\)) and Qwen-1.5-14b (right; \(=-0.46\)). In general we see a weak correlation, although it is stronger for Qwen than Llama. We filter out any datapoints where the base steerability of the dataset variation is less than 0.25, as having low baseline steerability means any relative steerability score is likely just noise.

Figure 7: **Steerability is mostly a property of the dataset.** We show the correlation between steerability in Llama-2-7b and Qwen-1.5-14b both ID (left; \(=0.769\)) and OOD (right; \(=0.586\)). Given steerability is highly correlated between Llama and Qwen despite differences in architecture, size and training data, this suggests steerability is mostly a property of the dataset rather that the model.

In Section 6 we evaluated the generalisation properties of SVs, finding that while they often generalise reasonably well (conditioned on their in-distribution performance being good), generalisation is not always perfect. We find that SV generalisation is mostly a property of the Mut dataset, and is correlated by the similarity in un-steered propensity of the model in the source and target setting. This correlation is problematic, as often we would want to apply steering vectors to guide model behaviour towards something it does not normally do, but in these scenarios SVs tend to generalise less well. Investigating methods to improve SV generalisation, and investigating scenarios where they generalise better or worse is important future work.

Overall, while steering vectors are a promising approach to efficiently guiding model behaviour at inference time, they are currently not a panacea to guarantee model helpful, harmless, and honest behavior, and substantial work is needed to improve their reliability and understand their generalisation properties.