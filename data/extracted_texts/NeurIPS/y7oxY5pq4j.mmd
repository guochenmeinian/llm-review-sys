# RobIR: Robust Inverse Rendering for

High-Illumination Scenes

 Ziyi Yang\({}^{1}\) Yanzhen Chen\({}^{1}\) Xinyu Gao\({}^{1}\) Yazhen Yuan\({}^{2}\)

**Yu Wu\({}^{2}\) Xiaowei Zhou\({}^{1}\) Xiaogang Jin\({}^{1}\)**

\({}^{1}\)State Key Lab of CAD&CG, Zhejiang University \({}^{2}\)Tencent

###### Abstract

Implicit representation has opened up new possibilities for inverse rendering. However, existing implicit neural inverse rendering methods struggle to handle strongly illuminated scenes with significant shadows and slight reflections. The existence of shadows and reflections can lead to an inaccurate understanding of the scene, making precise factorization difficult. To this end, we present _RobIR_, an implicit inverse rendering approach that uses ACES tone mapping and regularized visibility estimation to reconstruct accurate BRDF of the object. By accurately modeling the indirect radiance field, normal, visibility, and direct light simultaneously, we are able to accurately decouple environment lighting and the object's PBR materials without imposing strict constraints on the scene. Even in high-illumination scenes with shadows and specular reflections, our method can recover high-quality albedo and roughness with no shadow interference. _RobIR_ outperforms existing methods in both quantitative and qualitative evaluations. Code is available at https://github.com/ingra14m/RobIR.

## 1 Introduction

Inverse rendering, the task of extracting the geometry, materials, and lighting of a 3D scene from 2D images, is a longstanding challenge in computer graphics and computer vision. Previous methods, such as providing geometry for the entire scene , modeling shape representation  or pre-providing multiple known light information , have achieved plausible results using prior information. To achieve clear albedo and roughness decomposition, factors such as light obscuration, reflection, or refraction must be taken into account. Among these, hard and soft shadows are particularly challenging to eliminate, as they play a critical role not only in obtaining cleaner material but also in accurately modeling geometry and light sources. Although some data-driven approaches  have performed plausible shadow removal at the image level, these methods are not generally applicable for inverse rendering.

Since the advent of NeRF , implicit representation has garnered significant interest in portraying scenes as neural radiance fields. By applying implicit neural representation to inverse rendering , plausible factorization can be achieved in simple scenes with weak light intensity. Thanks to NeRFactor  and its relevant work , which extend previous works by explicitly representing visibility, implicit inverse rendering can be improved with simple shadow removal and clear edge in albedo and roughness. Recently, InvRender  has taken the scene factorization problem to a new level by modeling indirect illumination, serving as the baseline in our experiment.

However, in high-illumination scenarios with strong shadows or subtle specular reflections, the current methods for implicit inverse rendering have shown limitations in accurately modeling each decomposed part for BRDF estimation. Especially, it will lead to shadow baking in albedo and roughness, thereby causing serious artifacts in relighting and other downstream applications. Todeal with such scenes, the following challenges arise in order to obtain high-quality physically based rendering (PBR) materials.

First, previous methods for inverse rendering struggle to correctly decouple environment lighting, shadows, and the object's PBR materials. While these methods perform well in scenes with weak light intensity, where shadows and specular reflections are minimal, they struggle to accurately reconstruct BRDF of the object in scenarios with intense lighting. As shown in Fig. 4 and Fig. 5, shadow and specular reflection lead to poor albedo and messy environment map. To address the aforementioned challenge, we propose a novel approach that applies Academy Color Encoding System (ACES)  tone mapping  to nonlinearly and monotonically convert the PBR color output from the rendering equation to a range within \(\). Specifically, we introduce a scaled parameter \(\) to adjust the standard ACES tone mapping curve for specific scenes, better adapting to varying lighting conditions. Unlike previous methods, which either directly output PBR color within \(\), or convert linear PBR color outputted within \(\) to sRGB color also lying in \(\)[17; 26], our method can calculate PBR color over a broader value range. For areas with extremely strong or weak lighting, ACES tone mapping can reduce information loss in reconstruction through more refined contrast control, thereby better estimating BRDF without baking shadow or specular highlights.

Second, existing methods encounter difficulties in accurately modeling visibility. Typically these methods [56; 17] model the visibility field \(V(,)\) through a learned SDF field and sphere tracing, which takes position and view direction as inputs. However, the visibility field is not compatible with direct light modeled based on Spherical Gaussian (SG), resulting in many stubborn shadows remaining at the edges. To address this, we introduce a regularized visibility estimation (RVE) distilled from the visibility field to directly predict the visibility for each SG to achieve more accurate visibility. This technique significantly contributes to the BRDF estimation, enabling the separation of environment maps, albedo, and roughness without the baked shadows. We also apply octree tracing instead of sphere tracing to improve the precision of the visibility field modeling.

In summary, the major contributions of our work are:

* A novel scene-dependent ACES tone mapping for inverse rendering. It enables the high-quality albedo and roughness reconstruction in scenes with intense lighting and strong shadows.
* A novel regularized visibility estimation designed for direct SGs. It improves the visibility accuracy for each direct SG and reduces shadow residue, enhancing the overall BRDF quality of the ill-posed inverse rendering.
* The first neural field-based inverse rendering framework to achieve robust shadow removal in BRDF estimation under high-illumination scenes.

## 2 Related Work

### Implicit Neural Representation

Neural rendering has gained popularity due to its ability to produce photorealistic images. Recently, NeRF  enables photo-realistic novel view synthesis using MLPs. It can handle complex light scattering and reconstruct high-quality scenes for downstream tasks.

Subsequent work has enhanced NeRF's efficiency in various ways, elevating it to new heights and enabling its use in other domains. Structure-based techniques [51; 13; 37; 15; 9; 12] have explored ways to improve inference or training efficiency by caching or distilling implicit neural representation into the efficient data structure. Hybrid methods [25; 27; 42; 43; 7] aim to improve the efficiency by incorporating explicit voxel-based data structures. Among them, Instant-NGP  achieves minute training by additionally incorporating hash encoding. In addition, some follow-up methods [36; 47; 50] are dedicated to recovering clear surfaces for scenes with complex solid objects by modeling a learnable SDF network, the value of which indicates the minimum distance between the input coordinate and surfaces in the scene.

In our work, we employ NeuS , an SDF-based volume rendering framework, to learn geometry priors for inverse rendering. Furthermore, drawing inspiration from PlenOctree , we construct an Octree tracer from the SDF to improve inference efficiency and accuracy compared to sphere tracing.

### Inverse Rendering

Inverse rendering is a process in computer graphics that aims to derive an understanding of the physical properties of a scene from a set of images. Because the problem is highly ill-posed, most previous works have incorporated priors such as illumination, shape, and shadow, as well as additional observations such as scanned geometry [35; 38; 20] and known light conditions . Simplified approaches, such as those assuming outdoor and natural light  or white light , aim to reduce the number of fitting parameters in an ill-posed problem.

Recently, there has been a surge of interest in implicit inverse rendering, building on the success of NeRF and its fully differentiable implicit representation. To model spatially-varying bidirectional reflectance distribution function (SVBRDF) under more casual capture conditions, many recent methods [3; 19; 5; 4; 49; 53; 54] have relied on implicit representation. Other works [55; 41; 48; 17; 26] have focused on physical-based modeling for complex scenes via visibility prediction. L-Tracing  introduced a new algorithm for estimating visibility without training, while NeRFactor  proposed a canonical normal and BRDF smoothness to address NeRF's poor geometric quality. InvRender  extends previous work by modeling indirect illumination. Reliable-GS  and GS-IR , based on the representation of 3D-GS , have achieved real-time inverse rendering. However, none of these methods are able to decouple shadows and materials under high-illuminance conditions.

### The Rendering Equation

For non-emitted object, the color \(c\) of the surface point **x** is calculated by the rendering equation:

\[c(,_{o})=_{}f_{r}( _{o},_{i},)L( ,_{i})(_{i} )d_{i},\] (1)

where \(c(,_{})\) is the output color leaving point **x** in the view direction \(_{}\), \(f_{r}(,_{},_{ {o}})\) is the BRDF function, \(L(,_{})\) is the incoming radiance at point **x** from direction \(_{}\), and **n** is the surface normal. Following PhySG  and InvRender , we use spherical Gaussians (SGs) to efficiently approximate the rendering equation shown in Eq. (1). An SG is a spherical function that takes the following form:

\[G(;,,)=e^{(-1)},\] (2)

where \( R^{3}\) is the lobe axis, \( R^{1}\) is the lobe sharpness, and \( R^{3}\) is the lobe amplitude. Please refer to the supplementary material for the complete details.

In NeuS , we can determine the surface point **x** along a specific direction using sphere tracing. By substituting the color function with the shading function based on Eq. (1), we can achieve BRDF decomposition through image loss.

Figure 1: **The pipeline of our method. During the pre-processing stage, we reconstruct the scene as an implicit representation by NeuS . From the implicit representation, we extract scene priors such as normal, visibility, and indirect illumination. During BRDF estimation, we optimize environmental lighting, the scaled parameter \(\), albedo \(a\), and roughness \(r\), to minimize reconstruction loss under the constraint of the rendering equation. After 100 epochs, we perform regularized visibility estimation and employ an MLP to learn the visibility ratio \(\) of the direct SGs to obtain more accurate visibility specified for SGs, which is critical for eliminating stubborn shadows at the edges and boundaries.**Methodology

### Overview

Given a set of multi-view RGB images with known camera poses as input, our target is to reconstruct BRDF of the object even under high-illuminance scenes. As shown in Fig. 1, the pipeline of RobIR consists of two stages. In the pre-processing stage, we train NeuS \(S(x,)\) as the representation of the scene, which can provide scene priors like normals, visibility, and indirect illumination (Sec. 3.2). In the BRDF estimation stage, we fix the scene priors and optimize the direct illumination and scaled parameter to compute an accurate BRDF of the object under the constraint of rendering equation (Sec. 3.3). To improve the visibility accuracy for direct illumination and decomposition stability, we introduce the regularized visibility estimation after 100 epochs (Sec. 3.4).

### Stage 1: Pre-processing

In this stage, we adopt the same neural SDF representation and the volume rendering as NeuS  to reconstruct the scene. Then we can obtain the necessary prior information for the BRDF estimation stage, such as normal, visibility, and indirect illumination from NeuS.

Normal smoothing.In our framework, the accuracy of normal is crucial for BRDF estimation. However, we observed that normals estimated from NeuS tend to be noisy. To overcome this, we drew inspiration from Ref-NeRF  and employ a spatial MLP \(()\) to predict smooth normals aligned with the density gradient normals (See Fig. 2) obtained from NeuS using \(_{2}\) loss. We further employ a smooth loss to fix the broken normals caused by specular reflection:

\[_{norm}=\|()-\|_{2}^{2}+\|( )-(+)\|_{2}^{2},\] (3)

where \(\) denotes the normal at point **x** learned by MLP, \(\) denotes the supervision normal from NeuS, and \(\) is a \(0.02\) Gaussian noise.

Visibility and indirect illumination.With the availability of NeuS SDF, we can use sphere tracing to model secondary shading effects such as visibility and indirect illumination. However, performing sphere tracing requires a significant amount of time and memory. Inspired by PlenOctree , we use an octree tracer derived from the NeuS SDF, replacing sphere tracing to accelerate the tracing and achieve more precise intersection results. Moreover, We can further improve the inference efficiency by compressing the visibility and indirect illumination field into MLP.

As for indirect illumination, we follow InvRender  and model the indirect radiance field \(L_{I}(,})\) using \(M=24\) SGs under the supervision of NeuS radiance field. At point **x**, we first perform octree tracing along direction \(_{i}\) to get the second intersection point \(\). Then the indirect radiance field can be supervised by the out-going radiance \(S(,-_{i})\) from NeuS. Then, the indirect illumination \(L_{I}\) is computed by:

\[L_{I}(,;)=_{j=1}^{M}G(;( ,)),\] (4)

where we use an MLP \(\) to output the \(j\)th indirect SG parameters, and \(\) denotes the scaled parameter, which will be illustrated in Sec. 3.3.

As for visibility, we learn an MLP that maps the point **x** and direction \(\) to visibility \(V(,)\), which is supervised by the result of octree tracer from point **x** along direction \(\). The \(_{}\) and \(_{}\) are optimized by \(_{1}\) and binary cross entropy loss as follows:

\[_{}=\|_{I}-L_{I}\|_{1},_{}=(V(,),(,)),\] (5)

where \((p_{i} y_{i})\) represents the binary cross-entropy (BCE) loss, \(_{I}\) is the radiance value at the second intersection point \(\) obtained by querying NeuS, and \((,)\) is obtained using an octree tracer from point **x** along direction \(\).

### Stage 2: BRDF Estimation

So far, we have faithfully reconstructed the prior information of the scene such as the normal, visibility and the indirect illumination. In this stage, we aim to accurately evaluate the rendering equation in order to precisely estimate the surface BRDF i.e. albedo \(a\), roughness \(r\) and direct environment light with the fixed priors from stage 1. However, previous approaches tend to leave shadow and specular reflection in PBR materials under scenes with high illumination. Thus, we apply a scene-specific ACES tone mapping to the PBR color output by the rendering equation. The ACES tone mapping can calculate the PBR color over a broader value range, better estimating BRDF without baking shadow through more refined contrast control. We adopt SGs to efficiently approximate the rendering equation as PhySG . See complete SGs approximation in the supplementary materials.

Scene-specific ACES tone mapping.We adopt the widely used the ACES tone mapping , which is a type of high dynamic range (HDR) tone mapping. Several recent works  have incorporated HDR tone mapping into NeRF for specific applications. Specifically, we apply the ACES tone mapping \(\) to convert the PBR color \(e\) lying in \([0,+)\) to color lying in \(\):

\[(e)=,\] (6)

whereas the ACES inverse tone mapping \(_{}\) is given by:

\[_{}(c)=+1.3702c+0.0009} }{2(2.51-2.43c)}.\] (7)

Given that the light intensity varies across different scenes, applying ACES tone mapping universally is not feasible. Thus, we introduce an additional learnable parameter \((0,1]\). This scaled parameter modifies the ACES tone mapping curve, enabling it to automatically adapt to each scene's unique illumination intensity. The resulting deformed tone mapping function is defined as follows:

\[^{}(e)=^{-0.2}(e),_{}^{ }(c)=_{}(c^{0.2}).\] (8)

Indirect illumination with scaled parameter.In Sec. 3.2, we model the indirect illumination under the supervision from NeuS's radiance field. To convert indirect illumination to the same value range as BRDF estimation, we need to map the supervised values from NeuS through ACES inverse tone mapping \(_{}^{}\). Since we are not certain of the \(\) that best fits the scene during stage 1, we train indirect illumination using randomly sampled \(\) to obtain indirect illumination under all possible \(\) settings. Consequently, the loss function \(_{indir}\) in Eq. (5) is then revised to include \(\) as follows:

\[_{indir}=\|_{}^{}(_{I})-L_{I}\|_{ 1}.\] (9)

Then in stage 2, we stop training the indirect illumination and treat \(\) as a learnable parameter. The optimal \(\) for the current scene will be determined as the decomposition model converges.

BRDF estimation.We use the simplified Disney BRDF  model with albedo, roughness, and environment light as parameters and assume dielectric materials with a fixed Fresnel term value of \(F_{0}=0.02\). During the BRDF estimation stage, we adopt \(N=128\) learnable SGs to model direct illumination and represent the PBR materials using an encoder-decoder network. The network initially encodes the input surface point \(\) into its corresponding latent code \(\) and then decodes it into albedo \(\) and roughness \(\). To further reduce noise in materials, we incorporate the smooth loss similar to Eq. (3) to both the albedo and roughness, and apply sparsity loss to \(\) to ensure that most of the channels are close to zero:

\[_{smooth}=\|(),(+) \|_{2}^{2},_{sparse}=( 0.05),\] (10)

where \(\) is the decoder of the PBR material network, \(()= log}+(1-) log}\) represents Kullback-Leibler (KL) divergence loss that measures the relative entropy of two distributions.

Figure 2: Smooth loss to fix broken part. Figure 3: Visualization of direct SGs.

SGs approximation for rendering equation.In RobIR, we follow PhySG  and adopt SGs to approximate the rendering equation in Eq. (1):

\[ f_{r}(_{o},_{i}, )&=}{}+f_{s}(_{o},_{i},)\\ _{i}& G(_{i};0.0315,,32.7080)-31.7003,\\ L(,_{i})&=_{ k=1}^{N}G(_{i};_{k},_{k},()_{k} )+_{j=1}^{M}G_{I}(_{i};(,)),\] (11)

where \(G\) is the direct SGs learned in this stage, \(G_{I}\) is the indirect SGs learned in stage 1, \(\) is the surface normal, \(()=^{S}G(_{i})V(,_{i})}{_{i=0}^{S}G(_{i})}\) signifies the visibility for direct SGs obtained by randomly sampling \(S\) directions, \(f_{s}\) denotes the specular component that can be converted to a single SG. Then, we can integrate the multiplication of these SGs in closed-form  to compute the final PBR color \(_{o}\). For more details about \(f_{s}\), please see the supplementary materials.

### Regularized Visibility Estimation

One of our primary goals is to achieve clean albedo with no residual shadows, which are typically caused by inaccurate visibility. Despite all efforts of the previous modeling, a small amount of stubborn visibility errors still exist. Therefore, after 100 epochs of BRDF estimation, we introduce regularized visibility estimation, directly using an MLP \((,)\) to predict the visibility of \(\) relative to \(N\) direct SGs instead of \(\) calculated through previously learned visibility network \(V(,)\). Specifically, \((,)\) is a visibility prediction network learned from scratch under the supervision of \(\), while \(\) represents the \(N N\) identity matrix used to add information for N direct SGs and \( R^{3}\) is expanded to \(R^{N 3}\) to predict visibility for each direct SG. Since visibility errors primarily occur at the edges, which are also sparse in the scene, we leverage the edge loss to make the residual sparse:

\[_{}=((,)-( ) 0.01).\] (12)

In the first 100 epochs, we fix \(V(,)\) using \(\) to obtain a stable visibility estimate, avoiding the early collapse of BRDF estimation caused by directly using \((,)\). After 100 epochs, with a rough BRDF estimation in place, we introduce regularized visibility estimation. By using \(V(,)\) to distill \((,)\), we directly predict the visibility of point \(\) relative to direct SGs, circumventing errors caused by the sampling direction when calculating \(\). Thus, we can achieve a more accurate visibility estimate designed for direct SGs (See Fig. 3).

Figure 4: **Albedo in synthetic scenes.** We compare our method to InvRender , NVDiffrec , TensoIR , NeRO , Relightable-GS , and GS-IR . The results show that our method outperforms previous approaches without baking specular highlights and shadows into albedo.

Final loss.After incorporating regularized visibility estimation into inverse rendering, our final loss function in the BRDF estimation stage is:

\[=\|^{}(C_{}),C_{}\|_{2}^{2}+ _{}_{}+_{} _{}+_{}_{},\] (13)

where \(C_{}\) is the physically-based color from the rendering equation, \(^{}\) is the scene-specific ACES tone mapping, \(C_{}\) is the ground-truth color. In our experiments, \(_{}\), \(_{}\), and \(_{}\) are set to 0.001, 0.01, and 1.0 respectively.

## 4 Experiments

In this section, we present the experimental evaluation of our methods. To assess the effectiveness of our approach, we collect synthetic and real-world datasets from NeRF and NeuS **without any post-processing**. In addition, we use Blender to render our own datasets to further demonstrate the superiority of our methods in high-illumination scenes. It should be noted that unlike previous methods [17; 55] that used a hotdog scene with reduced illumination, we use the original hotdog from NeRF  without reduced illumination. See more comparison in the supplementary materials.

Our model hyperparameters consisted of a batch size of 1024, with 200k iterations for the NeuS training. The model was implemented in PyTorch and optimized with the Adam optimizer at a learning rate of \(5e^{-4}\). All tests were conducted on a single Tesla V100 GPU with 32GB memory. The training time without NeuS is around 5 hours.

   &  &  & Roughness \\  Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & MAE \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) & MAE \(\) \\  NVDiffree & 16.89 & 0.8252 & 0.1965 & 6.63 & 0.1397 & 0.3897 & 17.33 & 0.8235 & 0.2008 & 0.112 \\ InvRender & 19.12 & 0.8757 & 0.1652 & 13.47 & 0.5796 & 0.1624 & 22.57 & 0.8967 & 0.1354 & 0.073 \\  & PSNR & 0.52 & 0.8679 & 0.1537 & 5.19 & 0.4064 & 0.4903 & 18.66 & 0.8260 & 0.1981 & 0.066 \\  & 17.63 & 0.8343 & 0.1695 & 9.96 & 0.3354 & 0.2413 & - & - & - & 0.104 \\ GS-IR & 14.88 & 0.7618 & 0.2170 & 5.10 & 0.1569 & 0.4530 & 17.18 & 0.8307 & 0.1891 & 0.142 \\  Ours-no aces & 21.24 & 0.8851 & 0.1421 & 10.50 & 0.5446 & 0.2379 & 23.61 & 0.09059 & 0.1221 & 0.065 \\ Ours-no rev & 18.51 & 0.8786 & 0.1403 & 10.10 & 0.5650 & 0.2486 & 23.20 & 0.981 & 0.1243 & 0.059 \\ Ours-Log & 21.13 & 0.8883 & 0.1294 & 17.07 & 0.6431 & 0.1091 & 24.07 & 0.9003 & 0.1095 & 0.077 \\ Ours & 25.09 & 0.9303 & 0.0972 & 16.52 & 0.6351 & 0.1215 & 24.65 & 0.9118 & 0.0972 & 0.045 \\  

Table 1: **Quantitative evaluations.** We present the results of the synthetic scenes. We color each cell as **best**, **second best**, and **third best**. Our method can produce high-quality albedo, roughness, and environment map while maintaining the relighting fidelity.

Figure 5: **Environment map.** Compared to existing approaches, our method can truly achieve high-quality environment light decoupling, avoiding messy results.

Figure 6: **Roughness in synthetic scenes.** The results show that our method can achieve clean roughness, even in scenes with intense shadow interference.

### Comparisons with previous methods

We compare our method with previous state-of-the-art neural field-based inverse rendering approaches: NVDiffrec , InvRender , TensoIR , NeRO , Relightable-GS , and GS-IR .

As shown in Fig. 4 and Fig. 6, our method can truly achieve robust BRDF estimation, correctly decoupling shadows, ambient lighting, and PBR materials without baking shadows and specular highlights into albedo and roughness. Other methods tend to bake shadows into albedo, which also affects the correct decomposition of object roughness, reflecting their inability to properly separate the various components of BRDF estimation. Even in more challenging real-world scenarios shown in Fig. 7, our method can achieve robust decomposition results without baking shadows and specular highlights into albedo and roughness.

The estimated environment maps are shown in Fig. 5. Our method can accurately estimate the position of the light source and generate more precise light intensity in high-illumination scenes. As far as we know, we are the first to incorporate the accuracy of the estimated environment map into the quality assessment of neural field-based inverse rendering.

Tab. 1 shows the accuracy of the albedo, roughness, relighting, and environment map averaged over synthetic scenes. We did not measure the relighting of Relightable-GS because it does not support relighting of a single object. The term "Log" refers to the use of sigmoid mapping instead of ACES. We can observe that our method achieve the best results in all inverse rendering tasks. Inaccurate BRDF estimation significantly affects the results of relighting, causing methods with high-quality reconstruction to bake shadows and thus leading to a decline in rendering quality during relighting. Overall, our approach can achieve robust inverse rendering in high-illumination scenes.

### Ablation Studies

We perform an ablation study to analyze the importance of the key components in our proposed method. As illustrated in Fig. 8, our method is unable to eliminate both shadows or specular

Figure 8: **Ablation. We conduct ablation experiments on the key components in the BRDF estimation stage. The ablation results emphasize the critical importance of each component in our proposed framework for attaining high-quality albedo.**

Figure 7: **Comparisons on real-world scenes. Columns 2 to 5 are albedo, the last four columns are roughness. Even in complex real-world scenarios, our method can robustly decouple shadow and material, resulting in high-quality albedo and roughness.**reflection in the absence of ACES tone mapping. Without regularized visibility estimation, inaccurate predictions of direct SGs visibility results in residual shadows. The "Log Tone" result indicates that ACES is a more effective tone mapping than the sigmoid to remove shadow within our framework. Finally, our full method can correctly estimate BRDF of the object, resulting in the best performance.

### Application

De-shadowing.De-shadowing is a challenging task in the field of inverse rendering, often requiring strong priors and large data-driven models. Our proposed method correctly understands various lighting effects and is capable of effectively eliminating strong and irregular shadows, particularly in scenes with intense lighting. As shown in Fig. 9, by setting the visibility of direct SGs to 1, we can remove the shadow caused by direct light occlusion. It should be noted that our method **cannot remove the areas with reflections and the dark regions caused by the backlighting phenomenon**.

Relighting.To demonstrate the practical utility of the materials from our method, we conducted relighting experiments. As shown in Fig. 10, our estimated BRDF results can be accurately relighted in various lighting environments without shadow or illumination artifacts.

## 5 Conclusions and Discussions

We presented a novel inverse rendering framework for estimating BRDF of the object under high-illumination scenes. The key innovation lies in the use of ACES tone mapping, which shifts the calculation of PBR color to a wider value range, significantly reducing the impact of shadows and specular parts on BRDF estimation. In addition, regularized visibility estimation are employed to ensure more acuurate visibility for direct SGs. Experiment results on both synthetic and real-world data show that our method outperforms previous approaches in eliminating shadows and specular reflection under high-illumination scenes.

Currently, the proposed method has some limitations. First, non-solid, translucent, and thin objects cannot be correctly handled due to the limitations of NeuS. Second, the employment of SGs to model both direct and indirect lighting presents challenges in dealing with anisotropic objects, consequently leading to our method's deficiency in incorporating the metallic learnable parameters present in the Disney BRDF model. Third, we have not considered scenes with dynamic lighting like [28; 44]. Finally, our method's prior information is limited to multi-view images. We will consider integrating with LLM models in the future work.

Figure 10: **Relighting. Our method not only achieves high-quality relighting results in scenarios with specular highlights but can also robustly decouple shadows, obtaining high-quality relighting outcomes without baked shadows even in scenes with severe shadows.**

Figure 9: **De-shadow. Given an input image from a specific viewpoint, our proposed method can accurately remove shadows caused by direct light occlusion without sacrificing rendering quality.**

## 6 Acknowlegements

This work was supported by Key R&D Program of Zhejiang (No. 2024C01069). We thank Wenxin Sun for her help in pipeline illustration. We also thank Yuan Liu and Wen Zhou for the constructive suggestions.