# Addressing Negative Transfer in Diffusion Models

Hyojun Go\({}^{1}\)1 JinYoung Kim\({}^{1}\)1 Yunsung Lee\({}^{2}\)1 Seunghyun Lee\({}^{3*}\)

Shinhyeok Oh\({}^{3}\) Hyeongdon Moon\({}^{4}\) Seungtaek Choi\({}^{5}\)

Twvelvelabs\({}^{1}\) Wrtn Technologies\({}^{2}\) Riiid\({}^{3}\) EPFL\({}^{4}\) Yanolja\({}^{5}\)

{gohyojun15, seago0828}@gmail.com\({}^{1}\), sung@wrtn.io\({}^{2}\), {seunghyun.lee shinhyeok.oh}@riiid.co\({}^{3}\), hyeongdon.moon@epfl.ch\({}^{4}\), seungtaek.choi@yanolja.com\({}^{5}\)

###### Abstract

Diffusion-based generative models have achieved remarkable success in various domains. It trains a shared model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of _negative transfer_, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we first aim to analyze diffusion training from an MTL standpoint, presenting two key observations: **(O1)** the task affinity between denoising tasks diminishes as the gap between noise levels widens, and **(O2)** negative transfer can arise even in diffusion training. Building upon these observations, we aim to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL methods, but the presence of a huge number of denoising tasks makes this computationally expensive to calculate the necessary per-task loss or gradient. To address this challenge, we propose clustering the denoising tasks into small task clusters and applying MTL methods to them. Specifically, based on **(O2)**, we employ interval clustering to enforce temporal proximity among denoising tasks within clusters. We show that interval clustering can be solved using dynamic programming, utilizing signal-to-noise ratio, timestep, and task affinity for clustering objectives. Through this, our approach addresses the issue of negative transfer in diffusion models by allowing for efficient computation of MTL methods. We validate the efficacy of proposed clustering and its integration with MTL methods through various experiments, demonstrating 1) improved generation quality and 2) faster training convergence of diffusion models. Our project page is available at https://gohyojun15.github.io/ANT_diffusion/.

## 1 Introduction

Diffusion-based generative models  have accomplished remarkable achievements in various generative tasks, including image , video , 3D shape , and text generation . In particular, they have shown excellent performance and flexibility in a wide range of image generation settings, including unconditional , class-conditional , and text-conditional image generation . Consequently, improving diffusion models has garnered significant interest.

The framework of diffusion models  comprises gradually corrupting the data towards a given noise distribution and its subsequent reverse process. A model is optimized by minimizing the weighted sum of denoising score-matching losses across various noise levels  for learning the reverse process. This can be interpreted as diffusion training aiming to train a single shared model todenoising its input across various noise levels. Therefore, diffusion training is inherently multi-task learning (MTL) in nature, where _each noise level_ represents _a distinct denoising task_.

However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, sharing one model between tasks may lead to competition between conflicting tasks, resulting in a phenomenon known as _negative transfer_[24; 25; 57; 78], leading to poorer performance compared to learning individual tasks with separate models. _Negative transfer_ has been a critical issue in MTL research, and related works have demonstrated that the performance of multi-task models can be improved by remediating _negative transfer_[24; 25; 57; 78; 83]. Considering these, we argue that _negative transfer_ should be investigated in diffusion models, and if present, addressing it is a potential direction for improving diffusion models.

In this paper, we characterize how _multi-task_ diffusion model is, and whether there exists _negative transfer_ in denoising tasks. In particular, **(O1)** we first observe that task affinity [12; 78] between two denoising tasks is negatively correlated with the difference in noise levels, indicating that they may be less conflict as the noise levels become more similar . This suggests that adjacent denoising tasks should be considered more harmonious tasks than non-adjacent tasks in terms of noise levels.

Next, **(O2)** we observe the presence of _negative transfer_ from diffusion model training. During sampling within a specific timestep interval, utilizing a model trained exclusively on denoising tasks within that interval generates higher-quality samples compared to a model trained on all denoising tasks simultaneously. This finding implies that simultaneously learning all denoising tasks can cause degraded denoising within a specific time interval, indicating the occurrence of _negative transfer_.

Based on these observations, we focus on improving diffusion models by addressing _negative transfer_. To achieve this, we first propose to leverage the existing multi-task learning techniques, such as dealing with issues of conflicting gradients [5; 83], differences in gradient magnitudes [42; 46; 64], and imbalanced loss scales [4; 16; 29]. However, unlike previous MTL studies that typically focused on small sets of tasks, the presence of a large number of denoising tasks (\(\) thousands) in diffusion models makes it computationally expensive since MTL methods generally require calculating per-task loss or gradient in each iteration [4; 5; 16; 24; 29; 42; 46; 64; 78; 83].

To address this, we propose a strategy that first clusters the entire denoising tasks and then applies multi-task learning methods to the resulting clusters. Specifically, inspired by **(O1)**, we formulate the interval clustering problem which groups denoising tasks by pairwise disjoint timestep intervals. Based on the interval clustering, we propose timesteps, signal-to-noise ratios, and task affinity score-based interval clustering and show that these can be clustered by dynamic programming as [2; 76; 49]. Through our strategy, we can address the issue of _negative transfer_ in diffusion models by allowing for efficient computation of multi-task learning methods.

We evaluated our proposed methods through extensive experiments on widely-recognized datasets: FFHQ , CelebA-HQ , and ImageNet . For a comprehensive analysis, we employed various models, including Ablated Diffusion Model (ADM) , Latent Diffusion Model (LDM) , and Diffusion Transformer (DiT) . These models represent diverse diffusion architectures spanning pixel-space, latent-space, and transformer-based paradigms. Our results underscore a significant enhancement in image generation quality, attributed to a marked reduction in _negative transfer_. This affirms the merits of our clustering proposition and its synergistic integration with MTL techniques.

## 2 Related Work

**Diffusion Models** Diffusion models [20; 66; 71] are a family of generative models that generate samples from noise via a learned denoising process. Diffusion models beat other likelihood-based models, such as autoregressive models [62; 75], flow models [9; 10], and variational autoencoders  in terms of sample quality, and sometimes outperform GANs  in certain cases . Moreover, pre-trained diffusion models can be easily applied to downstream image synthesis tasks such as image editing [30; 45] and plug-and-play generation [13; 15]. From these advantages, several works have applied diffusion models for various domains [3; 23; 38; 44; 54] and large-scale models [48; 56; 58].

Several studies have focused on improving diffusion models in various aspects, such as architecture [1; 8; 28; 52; 82], sampling speed [33; 60; 67], and training objectives [6; 17; 31; 70; 74]. Among these, the most closely related studies are improving training objectives, as we aim to enhance optimization between denoising tasks from the perspective of multi-task learning (MTL). Several works [31; 70; 74]redesign training objectives to improve likelihood estimation. However, these objectives may lead to sample quality degradation and training instability and require additional techniques such as importance sampling [70; 74] and sophisticated parameterization  to be successfully applied. On the other hand, P2  proposes a weighted training objective that prioritizes denoising tasks for certain noise levels, where the model is expected to learn perceptually rich features. Similar to P2, we aim to improve the sample quality of diffusion models from an MTL perspective, and we will show that our method is also beneficial to P2.

As a concurrent work, MinSNR  shares a common insight with us that diffusion training is essentially multi-task learning. However, their observation lacks a direct connection to _negative transfer_ in terms of sample quality. They address the instability and inefficiency of multi-task learning optimization in diffusion models, mainly due to a large number of denoising tasks. In contrast, our work delves deeper into exploring _negative transfer_ and task affinity, and we propose the application of MTL methods through task clustering to overcome the identified challenges in MinSNR.

Multi-Task LearningMulti-Task Learning (MTL) is an approach that trains a single model to perform multiple tasks simultaneously . Although sharing parameters between tasks can reduce the overall number of parameters, it may also result in a _negative transfer_, causing performance degradation because of conflicting tasks during training procedure [24; 25; 57; 78].

Prior works have tracked down three causes of _negative transfer_: (1) _conflicting gradient_, (2) _the difference in gradient magnitude_, and (3) _imbalanced loss scale_. First, _Conflicting gradients_ among different tasks may negate each other, resulting in poorer updates for a subset of, or even for all tasks. PCgrad  and Graddrop  mitigate this by projecting conflicting parts of gradients and dropping elements of gradients based on the degree of conflict, respectively. Second, tasks with larger gradients may dominate tasks with smaller gradients due to _differences in gradient magnitude_ across tasks. Different optimization schemes have been proposed to equalize gradient magnitudes, including MGDA-UB , IMTL-G , and NashMTL . Similarly, _imbalanced loss scales_ may cause tasks with smaller losses to be dominated by those with larger losses. To balance task losses, uncertainty , task difficulty , and gradient norm  is exploited.

Adapting MTL methods and _negative transfer_ formulation to diffusion models is challenging since these techniques are typically designed for scenarios with a small number of tasks and easily measurable individual task performance. Our goal is to address this challenge and demonstrate that observing _negative transfer_ in diffusion models and mitigating it can improve them.

## 3 Preliminaries and Observation

We first provide the necessary background information on diffusion models and their multi-task nature. Next, we conduct analyses that yield two important observations: **(O1)** task affinity between two tasks is negatively correlated with the difference in noise levels, and **(O2)**_negative transfer_ indeed exists in diffusion training, i.e., the model is overburdened with different, potentially conflicting tasks.

### Preliminaries

Diffusion model [20; 66; 71] consists of two processes: a forward process and a reverse process. The forward process \(q\) gradually injects noise into a datapoint \(_{0}\) to obtain noisy latents \(\{_{1},,_{T}\}\) as:

\[q(_{t}|_{0})=(_{t}|a_{t}_{0 },_{t}^{2}), q(_{t}|_{s})=(_{t}|_{t|s}_{s},(_{t}^{2}-_{t|s}^{2} _{s}^{2})), 1 s<t T\] (1)

where \(_{t},_{t}\) characterize the signal-to-noise ratio \((t)=_{t}^{2}/_{t}^{2}\), and \(_{t|s}=_{t}/_{s}\). Here, \((t)\) decreases in \(t\), such that by the designated final timestep \(t=T\), \(q(_{T})(,)\).

The reverse process is a parameterized model trained to restore the original data from data corrupted during the forward process. The widely adopted training scheme uses a simple noise-prediction objective [8; 20; 34; 56; 59] that trains the model to predict the noise component \(\) of the latent \(_{t}=_{t}_{0}+\), \((,)\). More formally, the objective is as follows:

\[L_{simple}=_{t,x_{0},}[L_{t}],L_{t}=|| -_{}(_{t},t)||_{2}^{2}.\] (2)

Let us denote by \(^{t}\) the denoising task at timestep \(t\) trained by minimizing the loss \(L_{t}\) (Eq. 2). Then, since a diffusion model jointly learns multiple denoising tasks \(\{D_{t}\}_{t=1,,T}\) using a single shared model \(_{}\), it can be regarded as a multi-task learner. Also, we denote by \(^{[t_{1},t_{2}]}\) the set of tasks \(\{^{t_{1}},^{t_{1}+1},,^{t_{2}}\}\) henceforth.

### Observation

By considering diffusion training as a form of multi-task learning, we can analyze how the diffusion model learns the denoising task. We experimentally analyze diffusion models with two concepts in multi-task learning: 1) Task affinity [72; 12]: measuring which combinations of denoising tasks may yield a more positive impact on performance. 2) Negative transfer [68; 24; 25; 57; 78; 83]: degradation in denoising tasks caused by multi-task learning. We use a lightweight ADM  used in  and LDM  with FFHQ 256\(\)256 dataset  for analyze diffusion models trained on both pixel and latent space.

**(O1) Task Affinity Analysis** We first analyze how the denoising tasks \(^{[1,T]}\) relate to each other by measuring task affinities [72; 12]. In particular, we adopt the gradient direction-based task affinity score : for two given tasks \(^{i}\) and \(^{j}\), we calculate the pairwise cosine similarity between gradients from each task loss, i.e., \(_{}L_{i}\) and \(_{}L_{j}\), then average the similarities across training iterations. Task affinity score assumes that cooperative (conflicting) tasks produce similar (conflicting) gradient directions, and it has been to correlate with the MTL model's overall performance . Although there have been attempts to divide diffusion model phases using signal-to-noise ratio  and a trace of covariance of training targets , we are the first to provide an explicit and fine-grained analysis of task affinities among denoising tasks.

In Fig. 1, we visualize the task affinity scores among denoising tasks, for both ADM and LDM, with both timestep and log-SNR as axes. As can be seen in Fig. 1, task affinity between two tasks \(^{i},^{j}\) is high for neighboring tasks, i.e., \(i j\), and decreases smoothly as the difference in SNRs (or timesteps) increases. This suggests that tasks sharing temporal/noise-level proximity can be cooperatively learned without significant conflict. Also, this result hints at the possibility that denoising tasks for vastly different SNRs (distant in timesteps) may potentially be conflicting.

**(O2) Negative Transfer Analysis** Next, we show that there exist negative transfers among different denoising tasks \(^{[1,T]}\). Negative transfer refers to a multi-task learner's performance degradation due to task conflicts, and it can be identified by observing the performance gap between a multi-task learner and specific-task learners. For ease of observation, we group up tasks by intervals, based on

Figure 1: Task affinity scores plotted against timestep and log-SNR axes in ADM and LDM. As the timestep and SNR differences decrease, task affinity increases, implying more aligned gradient directions between denoising tasks and reduced negative impact on their joint training.

Figure 2: Negative transfer gap (\(NTG\)) with FID score of ADM and LDM for denoising tasks \(^{[,]}\). If \(NTG\) is negative, \(^{[,]}\)-trained model outperforms the entire denoising tasks-trained model in terms of denoising latent \(\{_{t}\}_{t[,]}\), showing the occurrence of negative transfer. Negative transfer occurs in both ADM and LDM.

the observation **(O1)** that more neighboring tasks in timesteps have higher task affinity. Specifically, we investigate whether the task group \(^{[t_{1},t_{2}]}\) suffers negative impacts from the remaining tasks.

To quantify the negative transfer, we follow the procedure: First, we generate samples \(\{}_{0}\}\) using a model trained on all denoising tasks \(^{[1,T]}\). Next, we repeat the same sampling procedure, except we replace the model with a model trained on \(^{[t_{1},t_{2}]}\) for the latent \(\{_{t}\}_{t[t_{1},t_{2}]}\); We denote the resulting samples by \(\{}_{0}^{[t_{1},t_{2}]}\}\). If \(\{}_{0}^{[t_{1},t_{2}]}\}\) exhibits superior quality compared to \(\{}_{0}\}\), it indicates that the model trained solely on \(^{[t_{1},t_{2}]}\) performs better in denoising the latent \(\{_{t}\}_{t[t_{1},t_{2}]}\) than the model trained on the entire denoising task. This suggests that \(^{[t_{1},t_{2}]}\) suffers from negative transfer by learning other tasks. More formally, given a performance metric \(P\), FID  in this paper, we define the negative transfer gap:

\[NTG(^{[t_{1},t_{2}]}):=P(\{}_{0}^{[t_{1},t_{2}]} \})-P(\{}_{0}\}),\] (3)

where \(NTG<0\) indicates that negative transfer occurs. The relationship between the negative transfer gap in previous literature and our negative transfer gap is described in Appendix A.

We visualize the negative transfers among denoising tasks for both lightweight ADM [6; 8] and LDM  in Fig. 2. The results indicate that negative transfer occurs in three out of the five considered task groups for both models. Notably, negative transfers often have a significant impact, such as a 7.56 increase in FID for ADM in the worst case. Therefore, we hypothesize that there is room for improving the performance of diffusion models by mitigating negative transfer, which motivates us to leverage well-designed MTL methods for diffusion training.

## 4 Methodology

In Section 3.2, we make two observations: **(O1)** Denoising tasks with a larger difference in \(t\) and \((t)\) exhibit lower task affinity, **(O2)** Negative transfer occurs in diffusion training. Inspired by these observations, we aim to remediate the negative transfer in diffusion by leveraging MTL methods. Although MTL methods are reported effective when there are only a few tasks, they are impractical for diffusion models with a large number of denoising tasks since they require computing per-task gradients or loss at each iteration. In this section, to deal with challenges, we propose a strategy that first groups the denoising tasks as task clusters and then applies the multi-task learning methods by regarding each task cluster as one distinct task.

### Interval Clustering

Here, we first introduce a scheme that groups all denoising tasks \(^{[1,T]}\) into a small number of task clusters. This is a necessary step for applying well-established MTL methods, for they usually involve computationally expensive subroutines such as computing per-task gradients or loss in each training iteration. Our key idea is to enforce temporal proximity of denoising tasks within task clusters, given our observation **(O1)** that task affinity is higher for tasks closer in timesteps. Therefore, we assign tasks in pairwise disjoint time intervals.

To obtain the disjoint time intervals, we leverage an interval clustering algorithm [2; 49] that optimizes for various clustering costs. In our case, interval clustering assigns diffusion timesteps \(=\{1,,T\}\) to \(k\) contiguous intervals \(I_{1},,I_{k}\), with \( k=\), where \(\) denotes disjoint union. Let \(I_{i}=[l_{i},r_{i}]\), \(l_{i} r_{i}\) for \(i=1,,k\), then we have \(l_{1}=1\), and \(r_{i}=l_{i+1}-1\) (\(i<k\) and \(r_{k}=T\)). The interval clustering problem is defined as:

\[_{l_{1}=1<l_{2}<<l_{k}}_{i=1}L_{cluster}(I_{i}),\] (4)

where \(L_{cluster}\) denotes the cluster cost.

Generally, it is known that an interval clustering problem of \(n\) data points with \(k\) intervals can be solved via dynamic programming in \(O(n^{2}k(n))\), where \((n)\) is the time required to calculate the one-cluster cost for \(L_{cluster}()\). If the size of each cluster is too small, it is challenging to learn the corresponding task cluster, so we add constraints on the cluster size for dynamic programming. More details regarding the dynamic programming algorithm can be found in Appendix G.

It remains to design the clustering cost function \(L_{cluster}\) to optimize for. We present three clustering cost functions: timestep-based, SNR-based, and gradient-based.

**1. Timestep-based Clustering Cost** Intuitively, one simple clustering cost is based on timesteps. We use the absolute timestep difference for the clustering objective by setting \(L_{cluster}(I_{i})=_{t=l_{i}}^{r_{i}}||t_{center}^{i}-t||_{1}^ {2}\) in Eq. 4 where \(t_{center}^{i}\) denotes the center of interval \(I_{i}\). The resulting intervals divide up the timesteps into \(k\) uniform intervals.

**2. SNR-based Clustering Cost** Another useful metric to characterize a denoising task is its signal-to-noise ratio (SNR). Indeed, it has been previously observed that a denoising task encounters perceptually different noisy inputs depending on its SNR . Also, we already observed that denoising tasks with similar SNRs show high task affinity scores (see Section 3.2). Based on this, we use the absolute log-SNR difference for clustering cost. We define the clustering cost as \(L_{cluster}(I_{i})=_{t=l_{i}}^{r_{i}}||(t_{ center}^{i})-(t)||_{1}^{2}\).

**3. Gradient-based Clustering Cost** Finally, we consider the gradient direction-based task affinity scores (see Section 3.2 for a definition) for clustering cost. Task affinity scores have been used as a metric to group cooperative tasks . Based on a similar intuition, we design a clustering cost as follows: \(L_{cluster}(I_{i})=-_{t=l_{i}}^{r_{i}}(t_{ center}^{i},t)\) where \(()\) is the gradient-based task affinity score. While leveraging more fine-grained information regarding task affinities, this cost function requires computing and storing gradients throughout training.

### Incorporating MTL Methods into Diffusion Model Training

After dividing the denoising tasks into task clusters via interval clustering, we apply multi-task learning methods to the resulting task clusters. As mentioned in Section 2, previous multi-task learning works have tracked down the following causes for negative transfer: (1) _conflicting gradient_, (2) _difference in gradient magnitude_, and (3) _imbalanced loss scale_. In this work, we leverage one representative method that tackles each of the causes mentioned above, namely, (1) PCgrad , (2) NashMTL , and (3) Uncertainty Weighting .

For each training step in diffusion modeling, we compute the noise prediction loss \(L^{l}\) for the \(l\)-th data within the minibatch. As shown in Eq 2, calculating \(L^{l}\) involves sampling the timestep \(t^{l}\), in which case \(L^{l}\) is a loss incurred on the denoising task \(^{t^{l}}\). We may then assign \(L^{i}\) to the appropriate task cluster by considering the corresponding timestep. Subsequently, we may group up the losses as \(\{L_{I_{i}}\}_{i=1,,k}\), where \(L_{I_{i}}\) is the loss for the \(i\)-th task cluster. (More details in Appendix C)

**1. PCgrad** In each iteration, PCgrad projects the gradient of a task onto the normal plane of the gradient of another task when there is a conflict between their gradients. Specifically, PCgrad first calculates the per-interval gradient \(_{}L_{I_{i}}\). Then, if the other interval gradient \(_{}L_{I_{j}}\) for \(i j\) has negative cosine similarity with \(_{}L_{I_{i}}\), it projects \(_{}L_{I_{i}}\) onto the normal plane of \(_{}L_{I_{j}}\). PCgrad repeats this process with all of the other interval gradients for all interval gradients, resulting in a projected gradient per interval. Finally, model parameters are updated with the summation of projected gradients.

**2. NashMTL** In NashMTL, the aggregation of per-task gradients is treated as a bargaining game. It aims to update model parameters with weighted summed gradients \(=_{i=i}^{k}_{i}_{}L_{I_{i}}\) by obtaining the Nash bargaining solution to determine \(_{i}\), where \(\) is in the ball of radius \(\) centered zero, \(B_{}\). They define the utility function for each player as \(u_{i}=_{}L_{I_{i}},\), then the unique Nash bargaining solution can be obtained by \(_{ B_{}}_{i}(u_{i})\). By denoting \(G\) as matrix whose columns contain the gradients \(_{}L_{I_{i}}\), \(_{+}^{k}\) is the solution to \(G^{}G=1/\) where \(1/\) is the element-wise reciprocal. To avoid the optimization to obtain \(\) for each iteration, they update \(\) once every few iterations.

**3. Uncertainty Weighting (UW)** UW uses task-dependent (homoscedastic) uncertainty to weight task cluster losses. By utilizing observation noise parameter \(_{i}\) for \(i\)-th task clusters, the total loss function is \(_{i}L_{I_{i}}/_{i}^{2}+(_{i})\). As the noise parameter for the \(i\)-th task clusters loss \(_{i}\) increases, the weight of \(L_{I_{i}}\) decreases, and vice versa. The \(_{i}\) is discouraged from increasing too much by regularizing with \((_{i})\).

## 5 Experiments

In this section, we demonstrate the efficacy of our proposed method by addressing the negative transfer issue in diffusion training. First, we provide the comparative evaluation in Section 5.1, whereour method can boost the quality of generated samples significantly. Next, we compare previous loss weighting methods for diffusion models to UW with interval clustering in Section 5.2, verifying our superior effectiveness to existing methods. Then, we analyze the behavior of adopted MTL methods, which serve to explain the effectiveness of our method in Section 5.3. Finally, we demonstrate that our method can be readily combined with more sophisticated training objectives to boost performance even further in Section 5.4. Extensive information on all our experiments can be found in Appendix E.

### Comparative Evaluation

Experimental SetupHere, we demonstrate that incorporating MTL methods into diffusion training improves the performance of diffusion models. For comparison, we consider unconditional and class-conditional image generation. For unconditional image generation, we used FFHQ  and CelebA-HQ  datasets, where all images were resized to \(256 256\). For class-conditional image generation experiments, we employed the ImageNet dataset , also resized to \(256 256\) resolution.

For architecture, we adopt widely recognized architectures for image generation. Specifically, we use the lightweight ADM  and LDM  for unconditional image generation, while employing DiT-S/2  with classifier-free guidance  for class-conditional image generation. We train the model using our method: We consider every possible pair of (1) interval clustering (timestep-, SNR-, and gradient-based) and (2) MTL method (PCgrad, NashMTL, and Uncertainty Weighting (UW)), and report the results. We used \(k=5\) in interval clustering throughout experiments.

For evaluation metrics, we use FID  and precision  for measuring sample quality, and recall  for assessing sample diversity and distribution coverage. IS  is additionally used for the evaluation metric in the class-conditional image generation setting. Finally, for sample generation, we use DDIM  sampler with 50 steps for unconditional generation and DDPM 250 steps for class conditional generation, and all evaluation metrics are calculated using 10k generated samples.

Comparison in Unconditional GenerationAs seen in Table 1 our method significantly improves performance upon conventionally trained diffusion models (denoted vanilla in the table). In particular, there is an improvement in FID in all cases, and an improvement in precision scores in all but two cases, which highlights the efficacy of our method. Also, given strong results for both pixel- and latent-space models, we can reasonably infer that our method is generally applicable.

We also observe the distinct characteristics of each multi-task learning method considered. Uncertainty Weighting tends to achieve higher improvements in sample quality compared to PCgrad and NashMTL. Indeed, UW achieves superior FID and Precision for ADM, while excelling in Precision for LDM.

   &  &  &  \\   & & &  &  \\   & & & FID (\(\)) & Precision (\(\)) & Recall (\(\)) & FID (\(\)) & Precision (\(\)) & Recall (\(\)) \\    &  & Vanilla & 24.95 & 0.5427 & 0.3996 & 22.27 & 0.5651 & 0.4328 \\   & & PGrad  & 22.29 & 0.5566 & 0.4027 & 21.31 & 0.5610 & 0.4238 \\  & & NashMTL  & 21.45 & 0.5510 & **0.4193** & 20.58 & 0.5724 & 0.4303 \\  & & UW  & 20.78 & 0.5995 & 0.3881 & **17.74** & **0.6323** & 0.4023 \\   &  & PCgrad  & 20.60 & 0.5743 & 0.4026 & 20.47 & 0.5608 & 0.4298 \\  & & NashMTL  & 23.09 & 0.5581 & 0.3971 & 20.11 & 0.5733 & **0.4388** \\  & & UW  & **20.19** & **0.6297** & 0.3635 & 18.54 & 0.6060 & 0.4092 \\   &  & PCgrad  & 23.07 & 0.5526 & 0.3962 & 20.43 & 0.5777 & 0.4348 \\  & & NashMTL  & 22.36 & 0.5507 & 0.4126 & 21.18 & 0.5682 & 0.4369 \\  & & UW  & 21.38 & 0.5961 & 0.3685 & 18.23 & 0.6011 & 0.4130 \\    &  & Vanilla & 10.56 & 0.7198 & 0.4766 & 10.61 & 0.7049 & 0.4732 \\   & & P&P & 9.599 & 0.7349 & 0.4845 & 9.817 & 0.7076 & 0.4951 \\    & & NashMTL  & 9.400 & 0.7296 & 0.4877 & 9.247 & 0.7119 & 0.4945 \\  & & UW  & 9.386 & 0.7489 & 0.4811 & 9.220 & 0.7181 & 0.4939 \\    &  & PCgrad  & 9.715 & 0.7262 & 0.4889 & 9.498 & 0.7071 & 0.5024 \\   & & NashMTL  & 10.33 & 0.7242 & 0.4710 & 9.429 & 0.7062 & 0.4883 \\   & & UW  & 9.734 & 0.7494 & 0.4797 & **9.030** & **0.7202** & 0.4938 \\    &  & PCgrad  & **9.189** & 0.7359 & 0.4904 & 10.31 & 0.6954 & 0.4927 \\   & & NashMTL  & 9.294 & 0.7234 & **0.4962** & 9.740 & 0.7051 & **0.5067** \\   & & UW  & 9.439 & **0.7499** & 0.4855 & 9.414 & 0.7199 & 0.4952 \\   

Table 1: Quantitative comparison to vanilla training (Vanilla) on the unconditional generation. Integration of MTL methods using interval clustering consistently improves FID scores and generally enhances precision compared to vanilla training.

However, UW sacrifices distribution coverage in exchange for sample quality, resulting in lower Recall compared to other methods. Meanwhile, NashMTL scores higher in recall and lower in precision compared to other methods, suggesting it has better distribution coverage while sacrificing sample quality. Finally, PCgrad tends to show a balanced performance in terms of precision and recall. We further look into behaviors of different MTL methods in Section 5.3.

Due to space constraints, we provide a comprehensive collection of generated samples in Appendix F. In summary, diffusion models trained with our method produce more realistic and high-fidelity images compared to conventionally trained diffusion models.

Comparison in Class-Conditional GenerationWe illustrate the results of quantitative comparison on class-conditional generation in Fig. 3. The results show that our methods outperform vanilla training in FID, IS, and Precision. In particular, UW and Nash-MTL significantly boost these metrics, showing superior improvement in generation quality. These results further support the generalizability of MTL methods through the interval clustering on class-conditional generation and the transformer-based diffusion model.

### Comparison to Loss Weighting Methods

Since UW is a loss weighting method, validating the superiority of UW with interval clustering compared to previous loss weighting methods such as P2  and MinSNR  highlights the effectiveness of our method. We name UW by incorporating interval clustering as Addressing Negative Transfer (ANT)-UW. We trained DiT-L/2 with MinSNR and UW with \(k=5\) on the ImageNet across 400K iterations, using a batch size of 240. All methods are trained by AdamW optimizer  with a learning rate of \(1e-4\). Table 2 shows that ANT-UW dramatically outperforms MinSNR, emphasizing the effectiveness of our method. An essential note is that the computational cost of ANT-UW remains remarkably similar to vanilla training as shown in Section 5.3, ensuring that our enhanced performance does not come at the expense of computational efficiency. Additionally, we refer to the results in , showing that our ANT-UW outperforms P2 and MinSNR when DIT-L/2 is trained on the FFHQ dataset.

### Analysis

To provide a better understanding of our method, we present various analysis results here. Specifically, we compare the memory and runtime of MTL methods, analyze the behavior of MTL methods adopted, provide a convergence analysis, and assess the extent to which negative transfer has been addressed.

   Method & FID & IS & Precision & Recall \\   Vanilla & 12.59 & 134.60 & 0.73 & 0.49 \\ MinSNR & 9.58 & 179.98 & 0.78 & **0.47** \\ ANT-UW & **6.17** & **203.45** & **0.82** & **0.47** \\   

Table 2: Comparison between MinSNR and ANT-UW. DiT-L/2 is trained on ImageNet.

   Method & GPU memory usage (GB) & \# Iterations / Sec \\  Vanilla & 34.126 & **2.108** \\  PCgrad & **28.160** & 1.523 \\ NashMTL & 38.914 & 2.011 \\ UW & 34.350 & 2.103 \\   

Table 3: GPU memory usage and runtime comparison on FFHQ dataset in LDM architecture.

Figure 3: Quantitative comparison to vanilla training (Vanilla) on ImageNet 256\(\)256 dataset with DiT-S/2 architecture and classifier-free guidance. Integration of MTL methods using interval clustering consistently improves FID, IS, and Precision compared to vanilla training.

Memory and Runtime ComparisonWe first compared the memory usage and runtime between MTL methods and vanilla training for a deeper understanding of their cost. We conducted measurements of memory usage and runtime with \(k=5\) on the FFHQ dataset using the LDM architecture and timestep-based clustering, and the results are shown in Table 3. PCgrad has a slower speed of 1.523 iterations/second compared to vanilla training, but its GPU memory usage is lower due to the partitioning of minibatch samples. Meanwhile, NashMTL has a runtime of 2.011 iterations/second. Even though NashMTL uses more GPU memory, it has a better runtime than PCgrad because it computes per-interval gradients occasionally. Concurrently, UW shows similar runtime and GPU memory usage as vanilla training, which is attributed to its use of weighted loss and a single backpropagation process.

Behavior of MTL MethodsWe analyze the behavior of different multi-task learning methods during training. For PCgrad, we calculate the average number of gradient conflicts between task clusters per iteration. For UW, we visualize the weights allocated to the task cluster losses over training iterations. Finally, for NashMTL, we visualize the weights allocated to per-task-cluster gradients over training iterations. We used LDM trained on FFHQ for our experiments. Although we only report results for time-based interval clustering for conciseness, we note that MTL methods exhibit similar behavior across different clustering methods. Results obtained using other clustering methods can be found in Appendix D.1.

The resulting visualizations are provided in Fig. 4. As depicted in Fig. 3(a), the task pair that shows the most gradient conflicts is \(I_{1}\) and \(I_{5}\), namely, task clusters apart in timesteps. This result supports our hypothesis that temporally distant denoising tasks may be conflicting, and as seen in Section 5.1, PCgrad seems to mitigate this issue. Also, as depicted in Fig. 3(b) and 3(b), both UW and NashMTL tend to allocate higher weights to task clusters that handle noisier inputs, namely, \(I_{4},I_{5}\). This result suggests that handling noisier inputs may be a difficult task that is underrepresented in conventional diffusion training.

Faster ConvergenceIn Fig. 5, we plot the trajectory of the FID score over training iterations, as observed while training on FFHQ. We can observe that all our methods enjoy faster convergence and better final performance compared to the conventionally trained model. Notably, for pixel space

Figure 4: Behavior of multi-task learning methods across training iterations. (a): With increasing timestep difference, gradient conflicts between task clusters become more frequent in PCgrad. (b) and (c): Both UW and NashMTL allocate higher weights to task clusters that handle noisier inputs.

Figure 5: Convergence analysis on FFHQ dataset. Compared to baselines, all methods exhibit fast convergence and achieve good final performance.

diffusion (ADM), UW converges extremely rapidly, while beating the vanilla method by a large margin. Overall, these results show that our method may not only make diffusion training more effective but also more efficient.

**Reduced Negative Transfer Gap** We now demonstrate that our proposed method indeed mitigates the negative transfer gap we observed in Section 3.2. We used the same procedure introduced in Section 3.2 to calculate the negative transfer gap for all methods considered, for the FFHQ dataset.

As shown in Fig. 6 our methods improve upon negative transfer gaps. Specifically, for tasks that exhibit severe negative transfer gaps in the baseline (e.g., ,  for ADM, and ,  for LDM), our methods mitigate the negative transfer gap for most cases, even practically removing it in certain cases. Another interesting result to note is that models less effective in reducing negative transfer (NashMTL-SNR for LDM and PCgrad-Grad for ADM) indeed show worse FID scores, which supports our hypothesis that resolving negative transfer leads to performance gain. We also note that even the worst-performing methods still beat the vanilla model.

### Combining MTL Methods with Sophisticated Training Objectives

Finally, we show that our method is readily applicable on top of more sophisticated training objectives proposed in the literature. Specifically, we train an LDM by applying both UW and PCgrad on top of the P2 objective  and evaluate the performance on the FFHQ dataset. We chose UW and PCgrad based on a previous finding that combining the two methods leads to performance gain . Also, we chose the gradient-based clustering method due to its effectiveness for LDM on FFHQ. As seen in Table 4, when combined with P2, our method improves the FID from 7.21 to 5.84.

## 6 Conclusion

In this work, we studied the problem of better training diffusion models, with the distinction of reducing negative transfer between denoising tasks in a multi-task learning perspective. Our key contribution is to enable the application of existing multi-task learning techniques, such as PCgrad and NashMTL, that were challenging to implement due to the increasing computation costs associated with the number of tasks, by clustering the denoising tasks based on their various task affinity scores. Our experiments validated that the proposed method effectively mitigated negative transfer and improved image generation quality. Overall, our findings contribute to advancing diffusion models. Starting from our work, we believe that addressing and overcoming negative transfer can be the future direction to improve diffusion models.

   Type & Method & FID-50k \\   GAN & PGAN  & 3.39 \\  AR & VQGAN  & 9.6 \\   Diffusion \\ (LDM) \\  } & D2C  & 13.04 \\   & Vanilla & 9.1 \\   & P2 & 7.21 \\   & P2 + Ours & **5.84** \\   

Table 4: Combining our method with P2 on the FFHQ dataset. DDIM 200-step sampler is used.

Figure 6: Negative transfer gap (NTG) comparison on the FFHQ dataset. Integration of MTL methods tends to improve the negative transfer gap. Methods that fail to improve NTG in areas where the baseline records low NTG tend to achieve lesser improvements in the baseline.