# The role of tail dependence in estimating posterior expectations

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Many tasks in modern probabilistic machine learning and statistics require estimating expectations over posterior distributions. While many algorithms have been developed to approximate these expectations, reliably assessing their performance in practice, in absence of ground truth, remains a significant challenge.

In this work, we observe that the well-known \(k\)-hat diagnostic for importance sampling (IS)  can be unreliable, as it fails to account for the fact that the common self-normalized IS (SNIS) estimator is a ratio. First, we demonstrate that examining separate \(k\)-hat statistics for the numerator and denominator can be insufficient. Then, we we propose a new statistic that accounts for the dependence between the estimators in the ratio. In particular, we find that the concept of tail dependence between numerator and denominator weights contains essential information for determining effective performance of the SNIS estimator.

## 1 Introduction and background

Algorithms for Bayesian computation continue to be used for increasingly complex probabilistic models, remaining an active research field . Yet, in the absence of ground truth, it remains challenging in practice to determine how and in which sense an approximate inference algorithm has found a "good" solution, as studied by several recent works, for Markov Chain Monte Carlo (MCMC) [3; 4; 5], variational inference (VI) [6; 7; 8], and importance sampling [1; 9; 10; 11] (the latter two being closely connected). In this work, we focus on diagnostics that apply to IS and VI algorithms.

**Problem statement.** Let \(\) (commonly, \(^{d_{}}\)) be the parameter of a Bayesian statistical model \(\{p(y|)\}_{}\) for data \(y\) with posterior PDF \((|)}}{{=}}Z_{}^{-1} (|)=Z_{}^{-1}_{n}p(y_{n}| )()\) with \(}}{{=}}\{y_{n}\}_{n=1}^{N}\), \(Z_{}\) the normalizer and prior PDF \(()\). Formally, we aim at constructing Monte Carlo estimates of a posterior expectation \(I_{>0}\), defined as

\[I}}{{=}}_{(|) }[f()]= f()(|)d,\] (1)

where \(f:_{ 0}\) is a suitably integrable test function. In particular, we are interested in obtaining diagnostics to determine the quality of an estimator \(\). As a concrete example, when we set \(f()=p(y^{(n+1)}|)\) for a test point \(y^{(n+1)}\), \(I\) is often written as \(p(y^{(n+1)}|)\), i.e., the evaluation of the posterior predictive PDF \(p(y|)\) at point \(y^{(n+1)}\).1

**Self-normalized IS, combination with VI.** Approximating integrals like in Eq. (1) accurately is challenging. MCMC is a natural solution, but there are notable cases where it is not appropriate. Forexample, when even exact i.i.d. sampling from \((|)\) is inefficient, or when it is too expensive. In these cases one usually resorts to IS , where we obtain samples from a chosen proposal PDF \(q\), as \(^{(s)}}}{{}}q()\), and construct estimators for \(I\) as

\[_{}=_{s=1}^{S}^{(s)}f(^{(s)}),\ ^{(s)}}}{{=}}}{ _{s^{}=1}^{S}w^{(s^{})}},\ w^{(s)}=w(^{(s)})=(^{(s)}|)}{q(^{(s)})}.\] (2)

Many theoretical properties of this estimator are known (see, e.g.,  for a review). When the normalizing constant \(Z_{}\) is unknown (i.e., almost always), the normalization of the weights in Eq. (2) is not optional. In practice, it is difficult to find a good proposal, i.e., leading to estimates that are close to \(I\). It is natural to use proposals that are the result of a VI algorithm , which is done implicitly or explicitly in the VI literature. See [6; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25] as examples for the many connections between VI and IS. A consequence of using a bad proposal is that the distribution of the weights \(w_{s}\) tends to have a few very large values.

**Pareto-smoothed IS.** Exploiting the above observation,  proposed Pareto-smoothed IS (PSIS), which replaces the largest \(M\) unnormalized weights 2 to get SNIS estimators with better behaviour. They fit a generalized Pareto distribution (GPD) to the weights \(\{w^{(s)}\}_{s=1}^{S}\). The new ("smoothed") weights introduce bias but reduce variance. The GPD has three parameters, the most important of which is the shape parameter \(k\).  propose to use an estimate of \(k\), i.e., \(\), as a diagnostic for IS.

**The \(\) diagnostic.** use the estimated value of \(k\), i.e., \(\), as a diagnostic for deciding whether the SNIS estimates with PSIS-corrected weights are reliable. The GPD has \(1/k\) finite fractional moments when the true \(k>0\), which suggests finite variance as soon as \(k<0.5\). Note that this guarantees finite variance only for the normalizing constant estimator \(_{}=1/S_{s=1}^{S}w^{(s)}\), which is implicit in the denominator of SNIS .  find empirically that when \(S>2000\), estimation with PSIS-corrected weights is reliable for \(<0.7\), a threshold less stringent than \(0.5\). An advantage of \(\) is that it is not an IS estimate itself, unlike the effective sample size (ESS) , attempting to address the issues with variance-based diagnostics .

## 2 Methodology

Several works [26; 27; 28] have shown theoretically and empirically that accurately estimating posterior expectations such as \(I\) in Eq. (1) involves more than simply finding a proposal \(q()\) that is close to the posterior \((|)\). This is because the SNIS estimator is a ratio estimator, as \(I\) itself is the ratio of two integrals,

\[I=(|)d}{ (|)d}}}{{=}}}}{Z_{}}}}{{ =}}}}{I_{}},\] (3)

where we relabelled the normalizing constant \(I_{}\). Therefore, we can write the SNIS estimator as

\[_{}=_{s=1}^{S}w^{(s)}f(^{(s) })}{_{s=1}^{S}w^{(s)}}=_{}}{ {I}_{}},\ ^{(s)}}}{{}}q(),\] (4)

where the two estimators \(_{}\) and \(_{}\) are unbiased, but \(_{}\) is not. As elaborated in , the asymptotic variance of the SNIS estimator is driven by the variance of the numerator estimator, the variance of the denominator, and the covariance between them. For convenience, we define two unnormalized importance weight functions, the one used in the numerator for \(_{}\) and the one used in \(_{}\), as

\[w_{}()=(|)}{q( )}, w_{}()=(| )}{q()}.\] (5)

We can then write the SNIS estimator as a ratio of two unbiased IS estimators,

\[_{}=_{s=1}^{S}w_{}( ^{(s)})}{_{s=1}^{S}w_{}(^{(s)})},\ ^{(s)}}}{{}}q().\] (6)Given that there are two IS weights, \(w_{}(^{(s)}),w_{}(^{(s)})\) in the above, it is natural to consider that one may track reliability \(_{}\) by computing two diagnostics \(_{}\), \(_{}\) separately for weights \(\{w_{}^{(s)}\}_{s=1}^{S}\) and \(\{w_{}^{(s)}\}_{s=1}^{S}\).  explored this option empirically, reporting that in their experiments it was sufficient to take \((_{},_{})\) to determine reliability of the ratio. In this work, we will argue that this heuristic misses useful information and propose a new diagnostic.

### Capturing error cancellation with tail dependence

The diagnostics \(_{}\) and \(_{}\) describe how well \(_{}\) and \(_{}\) respectively approximate \(I_{}\) and \(I_{}\), serving as an (improved) substitute for estimates of variance (like the ESS). Yet, the variance of the SNIS estimator \(_{}\) is not only affected by the variance of the numerator of Eq. (6), the variance of the denominator. It is also affected by the covariance \(_{q}[_{},_{}]\).

A straightforward idea to capture this missing piece of information from \(_{}\) and \(_{}\) is to construct an estimate of \(_{q}[_{},_{}]\), using the same samples from \(q\) used to estimate \(I\). Yet, doing so would suffer the same drawbacks of variance-based diagnostics, which was a motivation for \(\). Thus, we will develop a diagnostic that is not a direct estimate of \(_{q}[_{},_{}]\). Like , we also exploit the fact that the distribution of \(w_{}\) and \(w_{}\) can be well approximated with a power-law distribution in the tails. Specifically, we will look at a suitable notion of dependence between the tails of \(w_{}\) and \(w_{}\). This notion will replace the covariance \(_{q}[_{},_{}]\) as our target estimate. In fact, covariance, up to normalization, is equivalent to Pearson's correlation \(\), which is only a very specific form of dependence, with many known limitations .

**Dependence and error cancellation.** An intuition for why higher covariance between the estimators \(_{q}[_{},_{}]\), or other dependence metrics, can lead to lower error is that, in a ratio, error cancellation can happen. Error cancellation in ratios has been exploited to derive better convergence rates for other numerical integration methods . In IS, it is known that large IS weights lead to high errors. Therefore, error cancellation in the ratio of Eq. (6) could happen when a large weight in the numerator is offset by another similarly large weight in the denominator. We now formalize this using the notion of tail dependence.

**Definition 1** (Upper tail dependence coefficient and tail dependence): _Let \(W_{1},W_{2}\) be two real-valued random variables. Let their (continuous) marginal CDFs be \(F_{1},F_{2}\). Then,_

\[_{q 1^{-}}[W_{2}>F_{2}^{-1}(q)|W_{1}>F_{1}^{-1}(q) ]=_{U},\] (7)

_provided the limit exists, is known as upper tail dependence coefficient \(_{U}\). If \(_{U}>0\), we say that \(W_{1},W_{2}\) are asymptotically tail dependent, with the magnitude of \(_{U}\) determining the strength of depedence._

Next, we discuss how to relate the above concept to the estimation of \(I\).

### Proposed reliability checks

We propose to diagnose whether the estimate in Eq. (6) is reliable by examining three quantities: \(_{}\), \(_{}\) and a new diagnostic that is constructed as an approximation of the tail dependence coefficient \(_{U}\) between \(w_{},w_{}\). Our aim is to study how these quantities relate to the effective performance of \(_{}\) as an estimator of \(I\), which we define as follows.

**Definition 2** (Effective performance): _We define the effective performance of an estimator \(\) of \(I\) as ensuring that the value of \((/I)\) is close to \(1\) with high probability. This takes into account the possibility of \(I\) being very small, e.g., \(10^{-7}\) following the reccomendation of . In log-space, it is equivalent to look at how \( I-\) is close to zero (recall \(I>0\))._

Semi-parametric estimation of tail dependenceIn mathematical finance, various estimators of tail dependence have been developed . We begin by studying semi-parametric estimators, following the assumption used by  and common in heavy-tailed distribution inference .

Specifically, we assume the distribution of \(w_{},w_{}\) is well approximated by a GPD in the tails. Similarly, to estimate tail dependence, we assume the _copula_ of their joint distribution is well approximated by an extreme value copula , also only in the tails.3 We hypothesize that tail dependence between \(w_{}\) and \(w_{}\) improves \(_{}\) performance, similar to the effect of \(_{q}[w_{},w_{}]\), but easier to estimate and more reliable.

## 3 Preliminary results on Bayesian linear regression and conclusions

We look at the distribution of \( I-\) over different replications. We consider estimating the posterior predictive of a Bayesian linear regression (BLR) model where we can compute the exact value of \(I\). That is, from Eq. (1), we set \(f()=p(y^{(n+1)}|)\) for a test point \(y^{(n+1)}\), and \((|)\) is a Gaussian with known mean and covariance (BLR posterior).4

To validate our hypothesis that tail dependence contains useful information, we check the behaviour of the diagnostics \(_{}\), \(_{}\) our tail dependence diagnostic \(_{U}\) estimated from a _Gumbel copula_\(C(u_{1},u_{2};,)\) (which we found performing better than a t-copula), given by \(2-2^{1/}\).5 We find that, when \(k\)-diagnostics between competitors are similar for numerator and denominator, a higher tail dependence coefficient (TDC) explains the better performance. To explain our results, we need to introduce a recent generalization of the SNIS estimator proposed in , i.e., sampling from an extended space \(^{d_{}}^{d_{}}\), as \(_{}=_{s=1}^{S}w_{}( _{s}^{(s)})}{_{s=1}^{S}w_{}(_{s}^{(s)})} \), \([_{1}^{(s)},_{2}^{(s)}]}{{}}q_{1,2}( _{1},_{2})\). SNIS is a special case where the joint is a degenerate joint with \(_{1}=_{2}\). Another special case is taking \(q_{1,2}(_{1},_{2})=q_{1}(_{1})q_{2}(_{2})\), which is done in previous works including notably target-aware Bayesian inference . Finally, for these experiments we consider the choice of \(q_{1,2}(_{1},_{2})\) that uses a common random number (CRN) for numerator and denominator, but has different marginals. Concretely we used Gaussian proposals \((_{1};_{1},_{1})\) and \((_{2};_{2},_{2})\) for numerator and denominator, respectively. The parameters are set to the optimal ones (given by the BLR true posteriors for numerator and denominator) perturbed by an error term. The SNIS estimator uses only one distribution \(q()\), so we take the midpoint between the two optimal IS means and covariances for its parameters. Fig. 1 shows the results. We indeed find in other settings (for \(d_{}\), noise variance, and covariate distributions) that when \(\) values are similar for numerator and denominator, tail dependence explains the remaining performance if a difference exists. We plan to test further TDC metrics and Bayesian models.

Figure 1: Results (\(d_{}=100\)) over 100 replications. We compare SNIS, GenSNIS (see Section 3) with a common random number (CRN) and GenSNIS with independent marginals. From Fig. 0(a), we see that GenSNIS with CRN performs best; this cannot be captured by \(\) values, but only by the higher TDC. Note that in such high dimension all methods perform poorly. We found similar results for lower dimensions and showcase here only a high-dimensional case.