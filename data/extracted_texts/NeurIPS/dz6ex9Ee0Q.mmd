# Robust Graph Neural Networks

via Unbiased Aggregation

Zhichao Hou\({}^{1}\)

Equal contribution.

Ruiqi Feng\({}^{1}\)

Equal contribution.

Tyler Derr\({}^{2}\)

Xiaorui Liu\({}^{1}\)

Corresponding author.

###### Abstract

The adversarial robustness of Graph Neural Networks (GNNs) has been questioned due to the false sense of security uncovered by strong adaptive attacks despite the existence of numerous defenses. In this work, we delve into the robustness analysis of representative robust GNNs and provide a unified robust estimation point of view to understand their robustness and limitations. Our novel analysis of estimation bias motivates the design of a robust and unbiased graph signal estimator. We then develop an efficient Quasi-Newton Iterative Reweighted Least Squares algorithm to solve the estimation problem, which is unfolded as robust unbiased aggregation layers in GNNs with theoretical guarantees. Our comprehensive experiments confirm the strong robustness of our proposed model under various scenarios, and the ablation study provides a deep understanding of its advantages. Our code is available at https://github.com/chris-hzc/RUNG.

## 1 Introduction

Graph neural networks (GNNs) have gained tremendous popularity in recent years due to their ability to capture topological relationships in graph-structured data . However, most GNNs are vulnerable to adversarial attacks, which can lead to a substantial decline in predictive performance . Despite the numerous defense strategies proposed to robustify GNNs, a recent study has revealed that most of these defenses are not as robust as initially claimed . Specifically, under adaptive attacks, they easily underperform the multi-layer perceptrons (MLPs) which do not utilize the graph topology information at all . Therefore, it is imperative to thoroughly investigate the limitations of existing defenses and develop innovative robust GNNs to securely harness the topology information in graphs.

Existing defenses attempt to bolster the resilience of GNNs using diverse approaches. For instance, Jaccard-GCN  and SVD-GCN  aim to denoise the graph by removing potential adversarial edges during the pre-processing procedure, while ProGNN  learns the clean graph structure during the training process. GRAND  and robust training  also improve the training procedure through data augmentation. GNNGuard  and RGCN  reinforce their GNN architectures by heuristically reweighting edges in the graph. Additionally, there emerge some ODEs-inspired architectures including the GraphCON  and HANG  that demonstrate decent robustness. Although most of these defenses exhibit decent robustness against transfer attacks, i.e., the attack is generated through surrogate models, they encounter catastrophic performance drops when confronted with adaptive adversarial attacks that directly attack the victim model .

Concerned by the false sense of security, we provide a comprehensive study on existing defenses under adaptive attacks. Our preliminary study in Section 2 indicates that SoftMedian , TWIRLS , and ElasticGNN  exhibit closely aligned performance and notably outperform other defenses despite their apparent architectural differences. However, as attack budgets increase, these defenses still experience a severe performance decrease and underperform the graph-agnostic MLPs. These observations are intriguing, but the underlying reasons are still unclear.

To unravel the aligned robustness and performance degradation of SoftMedian, TWIRLS, and ElasticGNN, we delve into their theoretical understanding and unveil their inherent connections and limitations in the underlying principles. Specifically, their improved robustness can be understood from a unified view of \(_{1}\)-based robust graph smoothing. Moreover, we unearth the problematic estimation bias of \(_{1}\)-based graph smoothing that allows the adversarial impact to accumulate as the attack budget escalates, which provides a plausible explanation of their declining robustness. Motivated by these understandings, we propose a robust and unbiased graph signal estimator to reduce the estimation bias in GNNs. We design an efficient Quasi-Newton IRLS algorithm that unrolls as robust unbiased aggregation layers to safeguard GNNs against adversarial attacks. Our contributions can be summarized as follows:

* We provide a unified view of \(_{1}\)-based robust graph signal smoothing to justify the improved and closely aligned robustness of representative robust GNNs. Moreover, we reveal their estimation bias, which explains their severe performance degradation as the attack budgets increase.
* We propose a robust and unbiased graph signal estimator to mitigate the estimation bias in \(_{1}\)-based graph signal smoothing and design an efficient Quasi-Newton IRLS algorithm to solve the non-smooth and non-convex estimation problem with theoretical guarantees.
* The proposed algorithm can be readily unfolded as feature aggregation building blocks in GNNs, which not only provides clear interpretability but also covers many classic GNNs as special cases.
* Comprehensive experiments demonstrate that our proposed GNN significantly improves the robustness while maintaining clean accuracy. We also provide comprehensive ablation studies to validate its working mechanism.

## 2 Estimation Bias Analysis of Robust GNNs

In Section 2.1, we conduct a preliminary study to evaluate the robustness of several representative robust GNNs. In Section 2.2, we establish a unified view as \(_{1}\)-based models to uncover the inherent connections of three well-performing GNNs, including SoftMedian, TWIRLS and ElasticGNN. In Section 2.3, we leverage the bias of \(_{1}\)-based estimation to explain the catastrophic performance degradation in the preliminary experiments.

**Notation.** Let \(=\{,\}\) be a graph with node set \(=\{v_{1},,v_{n}\}\) and edge set \(=\{c_{1},,e_{m}\}\). The adjacency matrix of \(\) is denoted as \(\{0,1\}^{n n}\) and the graph Laplacian matrix is \(=-\). \(=(d_{1},,d_{n})\) is the degree matrix where \(d_{i}=|(i)|\) and \((i)\) is the neighborhood set of \(v_{i}\). The node feature matrix is denoted as \(=[_{1},,_{n}]^{}^{n d}\), and \(^{(0)}\) (\(^{(0)}\)) denotes the node feature vector (matrix) before graph smoothing in decoupled GNN models. Let \(\{-1,0,1\}^{m n}\) be the incidence matrix whose \(l\)-th row denotes the \(l\)-th edge \(e_{l}=(i,j)\) such that \(_{li}=-1,_{lj}=1,_{lk}=0\  k\{i,j\}\). \(\) is its normalized version : \(_{lj}=_{lj}/}\). For a vector \(^{d}\), we use \(_{1}\)-based graph smoothing penalty to denote either \(\|\|_{1}=_{i}|_{i}|\) or \(\|\|_{2}=_{i}^{2}}\). Note that we use \(_{2}\)-based graph smoothing penalty to denote \(\|\|_{2}^{2}=_{i}_{i}^{2}\).

### Robustness Analysis

To test the robustness of existing GNNs without the false sense of security, we perform a preliminary evaluation of existing robust GNNs against adaptive attacks. We choose various baselines including the undefended MLP, GCN , some of the most representative defenses in , and two additional robust models TWIRLS  and ElasticGNN . We execute adaptive local evasion topological attacks and test the node classification accuracy on the Cora ML and Citeseer datasets. The detailed settings follow Section 4.1. From Figure 1, it can be observed that:* Among all the selected robust GNNs, only SoftMedian, TWIRLS, and ElasticGNN exhibit notable and closely aligned improvements in robustness whereas other GNNs do not show obvious improvement over undefended GCN.
* SoftMedian, TWIRLS, and ElasticGNN encounter a similar catastrophic performance degradation as the attack budget scales up. Their accuracy easily drops below that of the graph-unware MLP, indicating their failure in safely exploiting the topology of the data.

### A Unified View of Robust Estimation

Our preliminary study provides intriguing observations in Section 2.1, but the underlying reasons behind these phenomena remain obscure. This motivates us to delve into their theoretical understanding and explanation. In this section, we will compare the architectures of three well-performing GNNs, aiming to reveal their intrinsic connections.

_SoftMedian_ substitutes the GCN aggregation for enhanced robustness with the dimension-wise median \(_{i}^{d}\) for all neighbors of each node \(i\). However, computing the median involves operations like ranking and selection, which is not compatible with the back-propagation training of GNNs. Therefore, the median is approximated as a differentiable weighted sum \(}_{i}=_{j(i)}w(_{j},_{i })_{j}, i\), where \(_{i}\) is the exact non-differentiable dimension-wise median, \(_{j}\) is the feature vector of the \(j\)-th neighbor, \(w(,)=e^{-\|-\|_{2}}\), and \(Z=_{k}w(_{k},_{k})\) is a normalization factor. In this way, the aggregation assigns the largest weights to the neighbors closest to the actual median.

_TWIRLS_ utilizes the iteratively reweighted least squares (IRLS) algorithm to optimize the objective with parameter \(\), and \((y)=y\) is the default:

\[2_{(i,j)}(\|}_{i}-}_{j} \|_{2})+_{i}\|}_{i}-}^{(0)}\|_{2} ^{2},}_{i}=(1+ d_{i})^{-}_{i}.\] (1)

_ElasticGNN_ proposes the elastic message passing which unfolds the proximal alternating predictor-corrector (PAPC) algorithm to minimize the objective with parameter \(_{\{1,2\}}\):

\[_{i}\|_{i}-_{i}^{(0)}\|_{2}^{2}+ _{1}_{(i,j)}\|_{i}}{}}- _{j}}{}}\|_{p}+_{2}_{(i,j) }\|_{i}}{}}-_{j}}{}}\|_{2}^{2},p\{1,2\}.\] (2)

A Unified View of Robust Estimation.While these three approaches have seemingly different architectures, we provide a unified view of robust estimation to illuminate their inherent connections. First, the objective of TWIRLS in Eq. (1) can be considered as a particular case of ElasticGNN

Figure 1: Robustness analysis under adaptive local attack. The perturbation budget (\(x\)-axis) is the number of edges allowed to be perturbed relative to the target nodeâ€™s degree. SoftMedian, TWIRLS, and ElasticGNN (blue curves) exhibit similarly aligned competitive robustness among all the selected robust GNNs, but all models experience catastrophic performance degradation as the attack budget increases.

with \(_{2}=0\) and \(p=2\) when neglecting the difference in the node degree normalization. However, TWIRLS and ElasticGNN leverage different optimization solvers, i.e., IRLS and PAPC, which leads to vastly different GNN layers. Second, SoftMedian approximates the computation of medians in a soft way of weighted sums, which can be regarded as approximately solving the dimension-wise median estimation problem : \(*{arg\,min}_{_{i}}_{j(i)}\|_{i}- _{j}\|_{1}\). Therefore, SoftMedian can be regarded as the ElasticGNN with \(_{2}=0\) and \(p=1\). We also note that the SoftMedoid  approach also resembles ElasticGNN with \(_{2}=0\) and \(p=2\), and the Total Variation GNN  also utilizes an \(_{1}\) estimator in spectral clustering.

The above analyses suggest that SoftMedian, TWIRLS, and ElasticGNN share the same underlying design principle of \(_{1}\)-based robust graph signal estimation, i.e. a similar graph smoothing objective with edge difference penalties \(\|_{i}-_{j}\|_{1}\) or \(\|_{i}-_{j}\|_{2}\). However, they adopt different approximation solutions that result in distinct architecture designs. This unified view of robust estimation clearly explains their closely aligned performance. Besides, the superiority \(_{1}\)-based models over the \(_{2}\)-based models such as GCN , whose graph smoothing objective is essentially \(_{(i,j)}\|_{i}/}-_{j}/}\| _{2}^{2}\), can be explained since \(_{1}\)-based graph smoothing mitigates the impact of the outliers .

### Bias Analysis and Performance Degradation

The unified view of \(_{1}\)-based graph smoothing we established in Section 2.2 not only explains their aligned robustness improvement but also provides a perspective to understand their failure as attack budgets scale up through an estimation bias analysis.

**Bias of \(_{1}\)-based Estimation.** In the literature of high-dimensional statistics, it has been well understood that the \(_{1}\) regularization will induce an estimation bias. In the context of denoising  or variable selection , small coefficients \(\) are undesirable. To exclude small \(\) in the estimation, a soft-thresholding operator can be derived as \(_{}()=()(||-,0)\). As a result, large \(\) are also shrunk by a constant, so the \(_{1}\) estimation is biased towards zero.

A similar bias effect also occurs in graph signal estimation in the presence of adversarial attacks. For example, in TWIRLS (Eq. (1)), after the graph aggregation \(}_{i}^{(k+1)}=_{j(i)}w_{ij}}_{j}^ {(k)}\) where \(w_{ij}=\|}_{i}-}_{j}\|_{2}^{-1}\), the edge difference \(}_{i}-}_{j}\) will shrink towards zero. Consequently, every adversarial edge the attacker adds will induce a bias that can be accumulated and amplified when the attack budget scales up.

**Numerical Simulation.** To provide a more intuitive illustration of the estimation bias of \(_{1}\)-based models, we simulate a mean estimation problem on synthetic data since most message passing schemes in GNNs essentially estimate the mean of neighboring node features. The results in Figure 2

Figure 2: Different mean estimators in the presence of outliers. The clean samples are the majority of data points following the Gaussian distribution \(((0,0),1 I)\), while the outliers are data points that deviate significantly from the main data pattern, following \(((8,8),0.5 I)\). \(_{2}\)-estimator deviates far from the true mean, while the \(_{1}\)-based estimator is more resistant to outliers. However, as the ratio of outliers escalates, the \(_{1}\)-based estimator encounters a greater shift from the true mean, but our estimator still maintains a position close to the ground truth.

shows that \(_{1}\)-based estimator is more resistant than \(_{2}\)-based estimator. However, as the ratio of outliers escalates, the \(_{1}\)-based estimator encounters a greater shift from the true mean due to the accumulated bias caused by outliers. This observation explains why \(_{1}\)-based graph smoothing models suffer from catastrophic degradation under large attack budgets. The detailed simulation settings and results are available in Appendix A.

## 3 Robust GNNs with Unbiased Aggregation

In this section, we first design a robust unbiased estimator to reduce the bias in graph signal estimation in Section 3.1 and propose an efficient second-order IRLS algorithm to compute the robust estimator with theoretical convergence guarantees in Section 3.2. Finally, we unroll the proposed algorithm as the robust unbiased feature aggregation layers in GNNs in Section 3.3.

### Robust and Unbiased Graph Signal Estimator

Our study and analysis in Section 2 have shown that while \(_{1}\)-based methods outperform \(_{2}\)-based methods in robustness, they still suffer from the accumulated estimation bias, leading to severe performance degradation under large perturbation budgets. This motivates us to design a robust and unbiased graph signal estimator that derives unbiased robust aggregation for GNNs with stronger resilience to attacks.

Theoretically, the estimation bias in Lasso regression has been discovered and analyzed in high-dimensional statistics . Statisticians have proposed adaptive Lasso  and many non-convex penalties such as Smoothly Clipped Absolute Deviation (SCAD)  and Minimax Concave Penalty (MCP)  to alleviate this bias. Motivated by these advancements, we propose a Robust and Unbiased Graph signal Estimator (RUGE) as follows:

\[*{arg\,min}_{}()=_{(i,j)}_{}(\|_{i}}{}}-_{j}}{ }}\|_{2})+_{i}\|_{i}-_{i }^{(0)}\|_{2}^{2},\] (3)

where \(_{}(y)\) denotes the function that penalizes the feature differences on edges by MCP:

\[_{}(y)=y-}{2}&y<\\ &y.\] (4)

As shown in Figure 3, MCP closely approximates the \(_{1}\) norm when \(y\) is small since the quadratic term \(}{2}\) is negligible, and it becomes a constant value when \(y\) is large. This transition can be adjusted by the thresholding parameter \(\). When \(\) approaches infinity, the penalty \(_{}(y)\) reduces to the \(_{1}\) norm. Conversely, when \(\) is very small, the "valley" of \(_{}\) near zero is exceptionally sharp, so \(_{}(y)\) approaches the \(_{0}\) norm and becomes a constant for a slightly larger \(y\). This enables RUGE to suppress smoothing on edges whose node differences exceeding the threshold \(\). This not only mitigates the estimation bias against outliers but also maintains the estimation accuracy in the absence of outliers. The simulation in Figure 2 verifies that our proposed estimator (\(()_{}(\|\|_{2})\)) can recover the true mean despite the increasing outlier ratio when the outlier ratio is below the theoretical optimal breakdown point.

### Quasi-Newton IRLS

Despite the advantages discussed above, the proposed RUGE in Eq. (3) is non-smooth and non-convex, which results in challenges for deriving efficient numerical solutions that can be readily unfolded as neural network layers. In the literature, researchers have developed optimization algorithms for MCP-related problems, such as the Alternating Direction Multiplier Method (ADMM) and Newton-type algorithms [24; 25; 26]. However, due to their excessive computation and memory requirements as well as the incompatibility with back-propagation training, these algorithms are not well-suited

Figure 3: Penalties.

for the construction of feature aggregation layers in GNNs. To solve these challenges, we propose an efficient Quasi-Newton Iteratively Reweighted Least Squares algorithm (QN-IRLS) to solve the estimation problem in Eq. (3).

**IRLS.** Before stepping into our QN-IRLS, we first introduce the main idea of iteratively reweighted least squares (IRLS)  and analyze its weakness in convergence. IRLS aims to circumvent the non-smooth \(()\) in Eq. (3) by computing its quadratic upper bound \(}\) based on \(^{(k)}\) in each iteration \(k\) and optimizing \(}()\):

\[}()=_{(i,j)}_{ij}^{(k)}\| _{i}}{}}-_{j}}{}}\|_{2}^{2 }+_{i}\|_{i}-_{i}^{(0)}\|_{2}^{2},\] (5)

where \(_{ij}^{(k)}=_{i j}(y_{ij})}{dy_{ij}^{2}} _{y_{ij}=y_{ij}^{(k)}}\)3, where \(y_{ij}^{(k)}=_{i}^{(k)}/}-_{j}^{(k)}/}_{2}\) and \(_{}()\) is the MCP function. For the detailed proof of the upper bound, please refer to Lemma 1 in Appendix B. Then, each iterative step of IRLS can be formulated as the first-order gradient descent for \(}()\):

\[^{(k+1)}=^{(k)}-}(^{(k)})=^{ (k)}-((}^{(k)}-2^{(k)}})^{( k)}-2^{(0)}),\] (6)

where \(\) is the step size, \(}^{(k)}=2((^{(k)})+)\), and \(_{m}^{(k)}=_{j}_{mj}^{(k)}_{mj}/d_{m}\). Its convergence condition is given in Theorem 1, with a proof in Appendix B.

**Theorem 1**.: _If \(^{(k)}\) follows the update rule in Eq. (6) where \(\) defining \(\) satisfies that \(}\) is non-decreasing \( y(0,)\), then a sufficient condition for \((^{(k+1)})(^{(k)})\) is that the step size \(\) satisfies \(0<\|(^{(k)})-^{(k)}}+ \|_{2}^{-1}\)._

**Quasi-Newton IRLS.** Theorem 1 suggests the difficulty in the proper selection of stepsize for (first-order) IRLS due to its non-trivial dependency on the graph (\(}\)) and the dynamic terms (\(^{(k)}\) and \(^{(k)}\))4. The dilemma is that a small stepsize will lead to slow convergence but a large step easily causes divergence and instability as verified by our experiments in Section 4.3 (Figrue 5), which reveals its critical shortcoming for the construction of GNN layers.

To overcome this limitation, we aim to propose a second-order Newton method, \(^{(k+1)}=^{(k)}-(^{2}}(^{(k)}))^{-1} }(^{(k)})\), to achieve faster convergence and stepsize-free hyperparameter tuning by better capturing the geometry of the optimization landscape. However, obtaining the analytic expression for the inverse Hessian matrix \((^{2}}(^{(k)}))^{-1}^{n n}\) is intractable, and the numerical solution requires expensive computation for large graphs. Therefore, we propose a novel Quasi-Newton IRLS algorithm (QN-IRLS) that approximates the Hessian matrix \(^{2}}(^{(k)})=2((^{(k)})-^{ (k)}}+)\) by the diagonal matrix \(}^{(k)}=2((^{(k)})+)\) such that the inverse is trivial. The proposed QN-IRLS works as follows:

\[^{(k+1)}=^{(k)}-}^{(k)}^{-1}}(^{(k)})=((^{(k)})+)^{-1} ((^{(k)}})^{(k)}+^{(0)} ),\] (7)

where \((}^{(k)})^{-1}\) automatically adjusts the per-coordinate stepsize according to the local geometry of the optimization landscape, \(^{(k)}\) and \(^{(k)}\) are defined as in Eq. (5) and (6). In this way, QN-IRLS provides faster convergence without needing to select a stepsize. The convergence is guaranteed by Theorem 2 with the proof in Appendix B.

**Theorem 2**.: _If \(^{(k+1)}\) follows update rule in Eq. (7), where \(\) satisfies that \(}\) is non-decreasing \( y(0,)\), it is guaranteed that \((^{(k+1)})(^{(k)})\)._
The proposed QN-IRLS provides an efficient algorithm to optimize the RUGE in Eq. (3) with a theoretical convergence guarantee. Instantiated with \(=_{}\), each iteration in QN-IRLS in Eq. (7) can be used as one layer in GNNs, which yields the Robust Unbiased Aggregation (RUNG):

\[^{(k+1)}=((^{(k)})+)^{-1}( (^{(k)}})^{(k)}+^{(0)}),\] (8)

where \(_{m}^{(k)}=_{j}_{mj}^{(k)}_{mj}/d_{m}\) as in Eq. (6), \(_{ij}^{(k)}=_{i j}(0,^{(k)}}-)\) and \(y_{ij}^{(k)}=^{(k)}}{}}-^{(k)}}{ }}_{2}\).

**Interpretability.** The proposed RUNG can be interpreted intuitively with edge reweighting. In Eq. (8), the normalized adjacency matrix \(}\) is reweighted by \(^{(k)}\), where \(_{ij}^{(k)}=}_{y=y_{ij}^{(k)}}\). It is shown in Figure 4 that \(_{ij}\) becomes zero for any edge \(e_{k}=(i,j)\) with a node difference \(y_{ij}^{(k)}\), thus pruning suspicious edges. This implies RUNG's strong robustness under large-budget adversarial attacks. With the inclusion of the skip connection \(^{(0)}\), \((^{(k)})+\) can be seen as a normalizer of the layer output.

**Relations with Existing GNNs.** RUNG can adopt different \(\) that Theorem 2 allows, thus covering many classic GNNs as special cases. When \((y)=y^{2}\), RUNG in Eq. (8) exactly reduces to APPNP  (\(^{(k+1)}=}^{(k)}+^{(0)}\)) and GCN (\(^{(k+1)}=}^{(k)}\)) if chosing \(=0\). When \((y)=y\), the objective of RUGE is equivalent to ElasticGNN with \(p=2\), which is analogous to SoftMedian and TWIRLS due to their inherent connections as \(_{1}\)-based graph smoothing.

**Complexity analysis.** RUNG is scalable with time complexity of \(O(k(m+n)d)\) and space complexity \(O(m+nd)\), where \(m\) is the number of edges, \(d\) is the number of features, \(n\) is the number of nodes, and \(k\) is the number of GNN layers. Therefore, the complexity of our RUNG is comparable to normal GCN (with a constant difference) and it is feasible to implement. The detailed discussions about computation efficiency can be found in Appendix C.

## 4 Experiment

In this section, we perform comprehensive experiments to validate the robustness of the proposed RUNG. Besides, ablation studies show its convergence and defense mechanism.

### Experiment Setting

**Datasets.** We test our RUNG with the node classification task on two widely used real-world citation networks, Cora ML and Citeseer , as well as a large-scale networks Ogbn-Arxiv . We adopt the data split of \(10\%\) training, \(10\%\) validation, and \(80\%\) testing, and report the classification accuracy of the attacked nodes following . Each experiment is averaged over \(5\) different random splits.

**Baselines.** To evaluate the performance of RUNG, we compare it to \(_{2}\) other representative baselines. Among them, MLP, GCN , APPNP , and GAT  are undefended vanilla models. GNNGuard , RGCN , GRAND , ProGNN , Jaccard-GCN , SVD-GCN , EvenNet , HANG , NoisyGNN , and GARNET  are representative robust GNNs. Besides, SoftMedian and TWIRLS are representative methods with \(_{1}\)-based graph smoothing 5. We also evaluate a variant of TWIRLS with thresholding attention (TWIRLS-T). For RUNG, we test two variants: default RUNG (Eq. (8)) and RUNG-\(_{1}\) with \(_{1}\) penalty (\((y)=y\)).

**Hyperparameters.** The model hyperparameters including learning rate, weight decay, and dropout rate are tuned as in . Other hyperparameters follow the settings in the original papers. RUNG uses an MLP connected to 10 graph aggregation layers following the decoupled GNN architecture of APPNP. \(=}\) is tuned in {0.7, 0.8, 0.9}, and \(\) tuned in {\(0.5,1,2,3,5\)}. We chose the hyperparameter setting that yields the best robustness without a notable impact (smaller than \(1\%\)) on the clean accuracy following .

**Attack setting.** We use the PGD attack  to execute the adaptive _evasion_ and _poisoning_ topology attack since it delivers the strongest attack in most settings . The adaptive attack setting is provided in Appendix F. The adversarial attacks aim to misclassify specific target nodes (_local attack_) or the entire set of test nodes (_global attack_). To avoid a false sense of robustness, our _adaptive_ attacks directly target the victim model instead of the surrogate model. Additionally, we include the _transfer_ attacks with a 2-layer GCN as the surrogate model. We also include graph injection attack following the setting in TDGIA .

### Adversarial Robustness

   Model & 0\% (Clean) & \(20\%\) & \(50\%\) & \(100\%\) & \(150\%\) & \(200\%\) \\  MLP & \(72.6 6.4\) & \(72.6 6.4\) & \(72.6 6.4\) & \(\) & \(\) \\ GCN & \(82.7 4.9\) & \(40.7 10.2\) & \(12.0 6.2\) & \(2.7 2.5\) & \(0.0 0.0\) & \(0.0 0.0\) \\ APPNP & \(\) & \(50.0 13.0\) & \(27.3 6.5\) & \(14.0 5.3\) & \(3.3 3.0\) & \(0.7 1.3\) \\ GAT & \(80.7 10.0\) & \(30.7 16.1\) & \(16.0 12.2\) & \(11.3 4.5\) & \(1.3 1.6\) & \(2.0 1.6\) \\  GNNGuard & \(82.7 6.7\) & \(44.0 11.6\) & \(30.7 11.6\) & \(14.0 6.8\) & \(5.3 3.4\) & \(2.0 2.7\) \\ RGCN & \(84.6 4.0\) & \(46.0 9.3\) & \(18.0 8.1\) & \(6.0 3.9\) & \(0.0 0.0\) & \(0.0 0.0\) \\ GRAND & \(84.0 6.8\) & \(47.3 9.0\) & \(18.7 9.1\) & \(7.3 4.9\) & \(1.3 1.6\) & \(0.0 0.0\) \\ ProGNN & \(\) & \(47.3 10.4\) & \(21.3 7.8\) & \(4.0 2.5\) & \(0.0 0.0\) & \(0.0 0.0\) \\ Jaccard-GCN & \(81.3 5.0\) & \(46.0 6.8\) & \(17.3 4.9\) & \(4.7 3.4\) & \(0.7 1.3\) & \(0.7 1.3\) \\ GARNET & \(82.4 6.8\) & \(70.9 7.5\) & \(61.9 7.9\) & \(42.7 9.3\) & \(11.6 3.4\) & \(9.6 3.5\) \\ HANG & \(83.1 7.4\) & \(71.2 7.8\) & \(60.1 6.3\) & \(39.5 3.4\) & \(9.4 2.3\) & \(5.6 2.3\) \\ NoisyGNN & \(82.5 5.6\) & \(57.4 5.7\) & \(47.8 6.2\) & \(36.1 4.5\) & \(5.8 3.4\) & \(4.1 1.2\) \\ EvenNet & \(83.4 8.1\) & \(64.8 6.9\) & \(56.1 5.6\) & \(29.5 5.3\) & \(3.1 1.1\) & \(1.1 1.3\) \\ GraphCON & \(81.3 7.1\) & \(67.3 7.3\) & \(54.7 7.1\) & \(41.3 6.2\) & \(4.1 2.1\) & \(2.3 1.2\) \\ SoftMedian & \(80.0 10.2\) & \(72.7 13.7\) & \(62.7 12.7\) & \(46.7 11.0\) & \(8.0 4.5\) & \(8.7 3.4\) \\ TWIRLS & \(83.3 7.3\) & \(71.3 8.6\) & \(60.7 11.0\) & \(36.0 8.8\) & \(20.7 10.4\) & \(12.0 6.9\) \\ TWIRLS-T & \(82.0 4.5\) & \(70.7 4.4\) & \(62.7 7.4\) & \(54.7 6.2\) & \(44.0 11.2\) & \(40.7 11.8\) \\  RUNG-\(_{1}\) (Ours) & \(84.0 6.8\) & \(72.7 7.1\) & \(62.7 11.2\) & \(53.3 8.2\) & \(22.0 9.3\) & \(14.0 7.4\) \\ RUNG (Ours) & \(84.0 5.3\) & \(\) & \(\) & \(70.7 10.6\) & \(69.3 9.8\) & \(69.3 9.0\) \\   

Table 1: Adaptive local attack on Cora ML. The **best** and second are marked.

   Model & 0\% (Clean) & \(5\%\) & \(10\%\) & \(20\%\) & \(30\%\) & \(40\%\) \\  MLP & \(65.0 1.0\) & \(65.0 1.0\) & \(65.0 1.0\) & \(65.0 1.0\) & \(65.0 1.0\) \\ GCN & \(85.0 0.4\) & \(75.3 0.5\) & \(69.6 0.5\) & \(60.9 0.7\) & \(54.2 0.6\) & \(48.4 0.5\) \\ APPNP & \(\) & \(75.8 0.5\) & \(69.7 0.7\) & \(60.3 0.9\) & \(53.8 1.2\) & \(49.0 1.6\) \\ GAT & \(83.5 0.5\) & \(75.8 0.8\) & \(71.2 1.2\) & \(65.0 0.9\) & \(60.5 0.9\) & \(56.7 0.9\) \\  GNNGuard & \(83.1 0.7\) & \(74.6 0.7\) & \(70.2 1.0\) & \(63.1 1.1\) & \(57.5 1.6\) & \(51.0 1.2\) \\ RGCN & \(85.7 0.4\) & \(75.0 0.8\) & \(69.1 0.4\) & \(59.8 0.7\) & \(52.8 0.7\) & \(46.1 0.7\) \\ GRAND & \(86.1 0.7\) & \(76.2 0.8\) & \(70.7 0.7\) & \(61.6 0.7\) & \(56.7 0.8\) & \(51.9 0.9\) \\ ProGNN & \(85.6 0.5\) & \(76.5 0.7\) & \(71.0 0.5\) & \(63.0 0.7\) & \(56.8 0.7\) & \(51.3 0.6\) \\ Jaccard-GCN & \(83.7 0.7\) & \(73.9 0.5\) & \(68.3 0.7\) & \(60.0 1.1\) & \(54.0 1.7\) & \(49.1 2.4\) \\ GARNET & \(84.0 0.5\) & \(76.5 0.4\) & \(72.1 0.1\) & \(66.4 0.7\) & \(62.1 1.3\) & \(58.7 1.5\) \\ HANG & \(84.5 0.2\) & \(75.7 0.6\) &Here we evaluate the the performance of RUNG against the baselines under different settings. The results of local and global adaptive attacks on Cora ML are presented in Table 1 and Table 2, while those on Citeseer are presented in Table 3 and Table 4 in Appendix E due to space limits. We summarize the following analysis from Cora ML, noting that the same observations apply to Citeseer.

* Under adaptive attacks, many existing defenses are not significantly more robust than undefended models. The \(_{1}\)-based models such as TWIRLS, SoftMedian, and RUNG-\(_{1}\) demonstrate considerable and closely aligned robustness under both local and global attacks, which supports our unified \(_{1}\)-based robust view analysis in Section 2.2.
* RUNG exhibits significant improvements over all baselines across various budgets under both global and local attacks. Local attacks are stronger than global attacks since local attacks concentrate on targeted nodes. The robustness improvement of RUNG appears to be more remarkable in local attacks.
* When there is no attack, RUNG largely preserves an excellent clean performance. RUNG also achieves state-of-the-art performance under small attack budgets.

### Ablation study

**Convergence.** To verify the advantage of our QN-IRLS method in Eq (7) over the first-order IRLS in Eq (6), we show the objective \(\) on each layer in Figure 6. It can be observed that our stepsize-free QN-IRLS demonstrates the best convergence as discussed in Section 3.

**Estimation bias.** The bias effect in \(_{1}\)-based GNNs and the unbiasedness of our RUNG can be empirically verified. We quantify the bias with \(_{i}\|_{i}-_{i}^{}\|_{2}^{2}\), where \(_{i}^{}\) and \(_{i}\) denote the aggregated feature on the clean graph and attacked graph, respectively. As shown in Figure 6, when the budget scales up, \(_{1}\) GNNs exhibit a notable bias, whereas RUNG has nearly zero bias. We provide comprehensive discussion of unbiasedness of RUNG in Appendix D.

**Defense Mechanism** To further investigate how our defense takes effect, we analyze the edges added under adaptive attacks. The distribution of the node feature differences \(\|_{i}/}-_{j}/}\|_{2}\) of attacked edges is shown in Figure 7 for different graph signal estimators. It can be observed that our RUNG forces the attacker to focus on the edges with a small feature difference, indicating that our RUNG can improve robustness by down-weighting or pruning some malicious edges that connect distinct nodes. Therefore, the attacks become less influential, which explains why RUNG demonstrates outstanding robustness.

**Transfer Attacks.** In addition to the adaptive attack, we also conduct a set of transfer attacks that take every baseline GNN as the surrogate model to comprehensively test the robustness of RUNG, following the unit test attack protocol proposed in . We summarize the results on Cora ML and Citeseer in Figure 9 and Figure 10 in Appendix E due to space limits. All transfer attacks are weaker than the adaptive attack in Section 4.2, indicating the necessity to evaluate the strongest adaptive attack to avoid the false sense of security emphasized in this paper. Note that the attack transferred from RUNG model is slightly weaker than the adaptive attack since the surrogate and victim RUNG models have different model parameters in the transfer attack setting.

**Hyperparameters.** Due to the space limit, we provide the additional ablation studies on the hyperparameters (including \(\) and \(\) in MCP as well as the number of layers) of RUNG in Appendix G.

The results offer an overview strategy for the choice of optimal hyperparameters. We can observe that \(\) in MCP has a significant impact on the performance of RUNG. Specifically, a larger \(\) makes RUNG closer to an \(_{1}\)-based model, while a smaller \(\) encourages more edges to be pruned. This pruning helps RUNG to remove more malicious edges and improve robustness, although a small \(\) may slightly reduce clean performance.

**Robustness under various scenarios.** Besides the evaluation under strong adaptive attacks, we also validate the consistent effectiveness of our proposed RUNG under various scenarios, including _Transfer attacks_ (Appendix E.2), _Poisoning attacks_ (Appendix E.3), _Large scale datasets_ (Appendix E.4), _Adversarial training_ (Appendix E.5), _Graph injection attacks_ (Appendix E.6).

## 5 Related Work

To the best of our knowledge, although there are works unifying existing GNNs from a graph signal denoising perspective , no work has been dedicated to uniformly understand the robustness and limitations of robust GNNs such as SoftMedian , SoftMedoid , TWIRLS , ElasticGNN , and TVGNN  from the \(_{1}\) robust statistics and bias analysis perspectives. To mitigate the estimation bias, MCP penalty is promising since it is well known for its near unbiased-ness property  and has been applied to the graph trend filtering problem  to promote piecewise signal modeling, but their robustness is unexplored. Nevertheless, other robust GNNs have utilized alternative penalties that might alleviate the bias effect. For example, GNNGuard  prunes the edges whose cosine similarity is too small. Another example is that TWIRLS  with a thresholding penalty can also exclude edges using graph attention. However, the design of their edge weighting or graph attention is heuristic-based and exhibits suboptimal performance compared to the RUNG proposed in this work.

## 6 Conclusion & Limitation

In this work, we propose a unified view of \(_{1}\) robust graph smoothing to uniformly understand the robustness and limitations of representative robust GNNs. The established view not only justifies their improved and closely aligned robustness but also explains their severe performance degradation under large attack budgets by a novel estimation bias analysis. To mitigate the estimation bias, we propose a robust and unbiased graph signal estimator. To solve this non-trivial estimation problem, we design a novel and efficient Quasi-Newton IRLS algorithm that can better capture the landscape of the optimization problem and converge stably with a theoretical guarantee. This algorithm can be unfolded and used as a building block for constructing robust GNNs with Robust Unbiased Aggregation (RUNG). As verified by our experiments, RUNG provides the best performance under strong adaptive attacks among all the baselines. Furthermore, RUNG also covers many classic GNNs as special cases. Most importantly, this work provides a deeper understanding of existing approaches and reveals a principled direction for designing robust GNNs.

Regarding the limitations, first, the improvement of RUNG is more significant under large budgets compared to the robust baselines. Second, we primarily include experiments on homophilic graphs in the main paper, but we can generalize the proposed robust aggregation to heterophilic graphs in future work. Third, although our Quasi-Newton IRLS algorithm has exhibited excellent convergence compared to the vanilla IRLS, the efficiency of RUNG could be further improved.