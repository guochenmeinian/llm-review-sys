# NeuralSteiner: Learning Steiner Tree for

Overflow-avoiding Global Routing in Chip Design

 Ruizhi Liu\({}^{1,2,4}\) Zhisheng Zeng\({}^{1,2,5}\) Shizhe Ding\({}^{1,2}\) Jingyan Sui\({}^{1,2}\) Xingquan Li\({}^{5,6}\) Dongbo Bu\({}^{1,2,3}\)

\({}^{1}\)SKLP, Institute of Computing Technology,

Chinese Academy of Sciences, Beijing 100190, China

{liuruizhi19s, dinghizhe19s, suijingyan18b, dbu}@ict.ac.cn

\({}^{2}\)University of Chinese Academy of Sciences, Beijing 101408, China

\({}^{3}\)Central China Artificial Intelligence Research Institute,

Henan Academy of Sciences, Zhengzhou 450046, Henan, China

\({}^{4}\)Beijing Institute of Open Source Chip, Beijing 100089, China

\({}^{5}\)Peng Cheng Laboratory, Shenzhen 518000, Guangdong, China

{zengzhsh, lixq01}@pcl.ac.cn

\({}^{6}\)School of Mathematics and Statistics,

Minnan Normal University, Zhangzhou 363000, Fujian, China

Corresponding author

###### Abstract

Global routing plays a critical role in modern chip design. The routing paths generated by global routers often form a rectilinear Steiner tree (RST). Recent advances from the machine learning community have shown the power of learning-based route generation; however, the yielded routing paths by the existing approaches often suffer from considerable overflow, thus greatly hindering their application in practice. We propose NeuralSteiner, an accurate approach to overflow-avoiding global routing in chip design. The key idea of NeuralSteiner approach is to learn Steiner trees: we first predict the locations of highly likely Steiner points by adopting a neural network considering full-net spatial and overflow information, then select appropriate points by running a graph-based post-processing algorithm, and finally connect these points with the input pins to yield overflow-avoiding RSTs. NeuralSteiner offers two advantages over previous learning-based models. First, by using the learning scheme, NeuralSteiner ensures the connectivity of generated routes while significantly reducing congestion. Second, NeuralSteiner can effectively scale to large nets and transfer to unseen chip designs without any modifications or fine-tuning. Extensive experiments over public large-scale benchmarks reveal that, compared with the state-of-the-art deep generative methods, NeuralSteiner achieves up to a 99.8% reduction in overflow while speeding up the generation and maintaining a slight wirelength loss within only 1.8%.

## 1 Introduction

In the modern design flow of Very Large Scale Integration (VLSI), global routing has become one of the most complex and time-consuming steps. Given the complexity of VLSI netlist  that contains millions or even billions of nets requiring routing, global routers must interconnect pins of nets, minimize the total wirelength of the routes while avoiding overflow (or congestion) in a strictly limited area of chip[2; 24]. Overflow occurs when the number of routes in a particular area of thechip exceeds the available routing resources or routes cross through impassable obstacles, which significantly impacts the subsequent design flow and functionality realization of the chip [24; 22]. Even the two-pin routing under design constraints or obstacles turns out to be NP-complete. When the number of pins exceeds two, the routing problem can often be transformed into the construction problem of rectilinear Steiner minimum tree (RSMT) , which is also NP-complete and becomes even more challenging when considering avoiding overflow [22; 16; 24; 6].

Traditional global routers propose various human-designed heuristics to obtain near-optimal solutions for RSMT [7; 27; 13; 16; 24] or directly solve the integer programming problem for concurrent routing of multiple nets [6; 33]. Recent advances in applying learning-based methods to chip design problems have shown feasibility and powerful abilities and even surpassing the performance of human expert-designed algorithms, such as using deep reinforcement learning (DRL) for placement [29; 18; 19] and using convolutional neural network (CNN) for predicting design rule violations . In global routing, DRL is firstly adopted to explore surrounding directions of current positions and achieve successful connectivity on small-scale nets , while REST  decomposes multi-pin net into 2-pin pairs and explores the sequence of pin pairs through DRL to form the RST. Moreover, recent works adopt deep generative models [32; 4] to perform one-shot generation of nets. To address the connectivity problems of generative methods, HubRouter decomposes global routing generation into hub-generation phase and pin-hub-connection phase to sequentially connect the pin-hub pairs, thereby successfully ensuring the connectivity of the routes.

However, current learning-based routing methods suffer from high overflow within their routing results, primarily due to inadequate consideration of resource availability during the routing process. Existing DRL-based approaches [20; 23; 28] tend to solely focus on wirelength as the reward during action space exploration, while other generative methods do incorporate the current routing area's resource status in their inputs but mainly aim for connectivity or wirelength optimization in their post-processing phases [4; 32; 5; 8]. Consequently, our experimental findings indicate that routes generated by HubRouter still exhibit significant congestion, which is illustrated in Fig. 1f.

To address these challenges, we propose a congestion-aware learning scheme named NeuralSteiner, which consists of two main phases: \(i)\)**Candidate point prediction phase**: Utilizing a neural network combined with full-image spatial and overflow information aggregation to predict the accurate locations of what we call candidate points for overflow-avoiding rectilinear Steiner tree; \(ii)\)**Overflow

Figure 1: **An illustrative example to show overflow-avoiding global routing in chip design.** (a) Chip layout of a real-world net extracted from ISPD07_adapter1. (b) Two-dimensional grid graph. (c) The Hanan grid of a 4-pin net. (d) The resource map and pin map (actually divided into two channels) obtained from (a) for this 4-pin net. (e) The predicted hub points (black circles) and stripe mask applied by Hubrouter . (f) The routing result generated by HubRouter  suffers from congestion (red edges). (g) The candidate points (green circles) predicted by NeuralSteiner and the corresponding net augmented graph (NAG). (h) The final routing result generated by NeuralSteiner that avoids overflow.

**avoiding RST construction phase**: Constructing an augmented graph of the net based on the predicted candidate points and calculating the overflow-avoiding RST using a simple but effective greedy algorithm. Through this two-phase setup, NeuralSteiner successfully ensures connectivity and enables the generation of overflow-avoiding routing results for large-scale nets.

This paper has **three main contributions**:

* We propose NeuralSteiner, a two-phase global routing scheme, which to our knowledge, is the first learning-based approach capable of optimizing both wirelength and overflow and effectively addressing the routing problem of large-scale nets.
* We devise a neural network architecture that integrates the deep residual network with recurrent crisscross attention mechanism to learn the Steiner point locations from a carefully curated expert dataset and propose a post-processing algorithm based on augmented graphs to construct routes with substantially less overflow than recent works.
* We conduct extensive experiments on 14 public large-scale routing benchmarks compared with the state-of-the-art learning-based method, where the NeuralSteiner achieves up to 99.6% reduction in total overflow with a wirelength loss within 1.8%. Moreover, NeuralSteiner can generate overflow-avoiding routes for nets with more than 1000 pins, previously challenging for recent works, which narrows the gap between learning-based methods and practical routing applications.

## 2 Preliminaries and Related Works

**Global Routing.** Given the complexity of VLSI routing problems, the circuit layout like 1a is partitioned into rectangular areas known as global cells (GCells) . The global routing problem can be modeled as a grid graph \(G(V,E)\), where each GCell is represented as a vertex \((v V)\), and adjacent GCells are connected by an edge \((e E)\) that represents the boundary between GCells. Chip designs often contain two or more metal layers for routing. Each metal layer is dedicated to either horizontal or vertical direction and the projection of these layers onto a two-dimensional grid graph is shown in Fig.1b. Global router will assign a set of GCells interconnected by numerous edges to each net as its routing result to connect all pins, which often forms a Rectilinear Steiner Tree (RST) . The concepts of Hanan grid  and escape graph  are often used for the generation of the shortest RSMT avoiding obstacles , considering the intersection points in these graphs as candidate locations for Steiner points. However, due to the complex and irregular distribution of congestion, the construction of escape graph becomes complicated, while the Hanan grid is ineffective at circumventing congestion, which is shown in Fig.1c.

**Overflow.** Give edge \(e(u,v) E\) is the boundary between GCell \(u\) and GCell \(v\), the capacity \(c(u,v)\) is the routing resource of edge \(e\) that can be provided to global router and demand \(d(u,v)\) is the number of routes passing through edge \(e\). The resource \(r(u,v)\) of edge \(e\) is the part of the capacity that can still be utilized to route, which is defined in Equation (1):

\[r(u,v)=c(u,v)-d(u,v)\] (1)

Overflow occurs when \(r(u,v)<0\). The routing results containing overflow generated by the global router will not be accepted by subsequent routing process and will trigger a time-consuming rip-up and reroute iteration in order to eliminate overflow . Therefore, the global router should not only attempt to find the shortest connection for each net but also minimize the number of overflow.

**Traditional Global Router.** Traditional routing algorithms typically divide global routing into two main stages to address congestion: Steiner topology generation and rip-up and reroute (RRR). The former utilizes the FLUTE algorithm , based on lookup tables, to generate Steiner trees with nearly minimal wirelength for each net. However, FLUTE is unaware of congestion. During this phase, most routers only use edge shifting to partially mitigate congestion by moving some edges out of congested areas , while CUGR-2  applies the construction of augmented graphs to build candidate paths for nets' RSTs, adjusting the position of certain Steiner points to circumvent potential congestion. In order to resolve congestion in the RSTs, traditional routers will invoke RRR, iteratively removing all initially routed nets in congested areas and employing maze routing that optimizes wirelength and congestion simultaneously. This process becomes dramatically time-consuming as the chip design's scale and complexity rise. Hence, accelerating congestion resolution through deep learning-based methods can enhance the overall performance of global routing algorithm.

**Learning-based RST construction.** Various works explore the feasibility and advantages in wire-length and efficiency of applying deep neural networks to global routing, including generation of the pin-connection order , segments [4; 32] or custom hub points of RST . However, most of the challenges in actual global routing come from the complexity of large-scale nets and how to avoiding overflow when routing resources are limited. Under the circumstances, detours are indispensable to get rid of congestions, while the shortest RST like Fig. 1f generated by HubRouter  is not practically usable. The chip layout can be viewed as an image, where each pixel represents a tile in global routing, and images of different channels represent the locations of the pins and capacity of the grid edges. The output points can also be represented as a binary image. But unlike the four kinds of hub points defined in Hubrouter, we simplify the learning target in RST construction and select Steiner points and corner points in RST as candidate points to learn. Formally, we have

**Definition 1**.: _Candidate point Given an \(m n\) binary image representing the RST, where each pixel \(p_{xy}(1 x m,1 y n)\) represents whether the position is occupied by a route and \(p_{0y}=p_{(m+1)y}=p_{x0}=p_{x(n+1)}=0\). The pixel \(p_{xy}\) is a candidate point if and only if it satisfies:_

\[\{p_{xy}=1,&(d_{xy}>2)\\ p_{xy}=1,p_{(x-1)y}+p_{(x+1)y}=1,p_{x(y-1)}+p_{x(y+1)}=1,&(d_{xy}=2) .\] (2)

_where \(d_{xy}=p_{(x-1)y}+p_{(x+1)y}+p_{x(y-1)}+p_{x(y+1)}\) denotes the degree of this point in the RST._

Using Definition. 1, the Steiner points and corner points can be recognized as candidate points in the pixel image of RST. The differences between hub point in  and candidate point are visualized in Fig. S1.

## 3 Method

### Overall Pipeline

NeuralSteiner decomposes the global routing process into two main phases to optimizes wirelength and overflow of the routing result simultaneously. Before introducing the main methods, we first propose our parallel task construction in Sec. 3.2. We then introduce the candidate point prediction method with aggregation of full-scale spatial and overflow information in Sec. 3.3. An augmented graph-based overflow-avoiding RST construction method will be proposed in Sec. 3.4. The overall pipeline of NeuralSteiner is illustrated in Fig.2.

Figure 2: **Overview of NeuralSteiner. (a) The parallel routing tasks accelerate routing by grouping the non-overlapping nets into one batch. (b) During the first phase, NeuralSteiner predicts the candidate point locations for the RST with full-image aggregation of spatial and overflow information. (c) During the second phase, NeuralSteiner constructs the net augmented graphs based on the predicted candidate points and generates overflow-avoiding RSTs.**

### Parallel Routing Tasks Construction.

Routing of two nets cannot be parallelized if the bounding boxes of their pins have overlap, which is defined as conflict between two nets. Inspired by , we scan and group the non-conflicting nets into a set \(t\), which is called a routing task, which divides the numerous nets in the design into a set of mutually conflicting routing tasks. Nets within a task \(t\) can be batched together and fed into the neural network for prediction and post-processing, significantly enhancing the parallelism of routing. Please refer to App. B.3 for the detail of our routing tasks construction algorithm.

### Candidate Point Prediction

Candidate points prediction can be formulated as an image segmentation task , which involves training a neural network model to perform pixel-level classification to recognize the locations of candidate points found in the expert RSTs data. We will first introduce the expert routes dataset optimized for both wirelength and overflow, then the network architecture incorporating the recurrent crisscross attention module to tackle the complex large-scale nets, as well as the design of our training protocol.

**Expert Routing Dataset Construction.** We utilize a state-of-the-art traditional global router named CUGR  to perform routing on public benchmarks  also used by  and extract the overflow map and pin map of every net in real-time. We adopt the logistic function in CUGR to calculate the overflow value using resource \(r(u,v)\):

\[lg(u,v)=(1.0+exp(slope r(u,v)))^{-1}.\] (3)

where \(slope\) (which is set to 1 here) is an adjustable parameter that determines the global router's sensitivity to overflow and the overflow value will increase rapidly as the resources are being used up. After that, we directly employ CUGR's maze routing algorithm to execute the rip-up and reroute process to obtain the congestion-avoiding routing results. We mark the Steiner points and corner points in the RSTs constructed by CUGR as candidate points and generate the label candidate point map for every net. Rather than clipping all images to the same scale \(64 64\), which is set in HubRouter , we maintain three maps of every net at the original scale of its bounding box. This preserves the precise spatial and overflow information and does not exclude any large-scale nets.

**Network Architecture.** In order to tackle the problem of large variation in net scale, we employ a ResNet structure as the backbone of our model and combined it with the recurrent crisscross attention mechanism  to encoding full-net overflow and long-range associations in the input features. Convolutional neural networks (CNN) have been proven to be efficiently applied in chip design like predicting chip congestion distribution , DRV distribution , and thermal distribution . However, due to the fixed geometric structures, CNN is inherently limited to local receptive fields that face difficulties in capturing long-range correlations. Thus, we introduce the recurrent crisscross attention mechanism (RCCA) to aggregate features from all pixels on the feature map. We insert one RCCA module with two e crisscross attention blocks in ResNet. Fig.2 illustrates the network architecture of NeuralSteiner. We also remove the down-sampling operations to retain more spatial details of feature maps because the construction of RST requires accurate spatial location information when connecting pins that are far apart in large-scale nets. Through the computation of RCCA, the network can aggregate information of pins and congestion over the whole scale of feature maps, thereby enhancing the quality of candidate points prediction for large RSTs. This will be further demonstrated in ablation study in Sec. 4.4. The implementation details of our network and RCCA calculation are shown in App. B.1.

**Model Training.** We adopt focal loss \(_{focal}\) to mitigate the imbalance between positive and negative class samples in training data where the candidate points in RST only occupy a minority of pixels in the entire routing area. Let \(p_{t}\) be the predicted probability for the ground truth class \(t\), \(_{focal}\) is defined as:

\[_{focal}=-_{t}(1-p_{t})^{}(p_{t})\] (4)

where \(_{t}\) is the weighting factor while \(\) is the focusing parameter that reduces the loss for well-classified examples. We also adopt the dice loss \(_{dice}\) to measure the similarities between the predicted candidate points and the ground truth. Using \(p_{xy}\) to represent the probability of pixel at position \((x,y)\) predicted as a candidate point and \(g_{xy}\) to represent the label, \(_{dice}\) can be expressed as:

\[_{dice}=1-p_{xy}g_{xy}+}{_{x,y}p_{xy}+_{x, y}g_{xy}+}\] (5)where \(\) is a small constant added to avoid division by zero. Additionally, since the global routing problem is NP-complete, even expert router may not generate the optimal routing solution for the net that achieves the shortest wirelength with the minimal overflow. Therefore, we further add an overflow loss \(_{of}\) to measure the congestion status of predicted points. Let \(o_{xy}\) be the value at position \((x,y)\) of the overflow map, \(_{of}\) can be calculated by:

\[_{of}=p_{xy}o_{xy}+}{_{x,y}p_{xy}+}\] (6)

The inclusion of overflow loss helps the model identify potential candidate points that are not in the label set but have a lower intrinsic congestion, benefiting the post-processing algorithm for overflow-avoiding RST construction. Then the trainable model \(\) is determined at the training stage by minimizing the loss function as follows:

\[()=c_{fl}_{focal}+c_{di}_{dice}+c_{of} _{of}\] (7)

where \(c_{fl},c_{di},c_{of}\) represent weight of corresponding loss item. The parameters used in training process are provided in App. B.2.

### Overflow-avoiding RST Construction

To construct an overflow-avoiding Rectilinear Steiner Tree (RST) based on the candidate points predicted by the neural network, we will first introduce the construction of net augmented graph that contains potential overflow-free edges and then propose a simple and effective greedy RST construction algorithm. Unlike previous works that focus solely on minimizing wirelength, the inclusion of the irregular distribution of congestion makes solving for an RST more challenging.

**Net Augmented Graph.** We introduce the concept of the net augmented graph (NAG) based on neural network-predicted points to avoid congestion. We first merge the predicted candidate point map and pin map, then sequentially examine each point \(p_{xy} 1\) from the merge map according to the following two conditions: 1) if this point shares the same horizontal (X) or vertical (Y) coordinates with another point \(q\), and 2) if there is no other points on the line connecting \(p\) and \(q\). Then an edge \(e(p,q)\) will be established if the above two conditions are met and the weight of \(e(p,q)\) is set as

\[(e)=w_{d}(|x_{p}-x_{q}|+|y_{p}-y_{q}|)+w_{o}_{x,y}o_{xy}\] (8)

where \(min(x_{p},x_{q}) x max(x_{p},x_{q}),min(y_{p},y_{q}) y max(y_ {p},y_{q})\). \((e)\) balances the wirelength and congestion of the edge by using weights \(w_{d}=1.0\) and \(w_{o}=5.0\). After examining all the points, to ensure the connectivity of the net, we will check the connectivity of the current NAG and add candidate point and edge between different connected components if this NAG is disconnected. App. B.4 provides a detailed introduction to the construction algorithm.

Note that in HubRouter , stripe mask is introduced as a filter that removes noise hub points to limit the solution space similar to the Hanan grid, which ensure that the wirelength as short as possible. However, as dipicted in Fig.1e, the addition of stripe mask in HubRouter limits its ability to generate RST avoiding congested areas. On the contrary, we here retain all candidate points predicted by the model and constructed the NAG based on them, which reduces the complexity of solving RST while preserving the solution space to avoid overflow.

**Overflow-avoiding RST Construction.** We convert the calculation of the overflow-avoiding RST into a greedy construction of minimal spanning tree that connects all pins. Initially, we consider all pins as separate connected components containing only one node. In each iteration, based on the NAG, we greedily select and connect the path between the two nearest connected components, then update the shortest distance (the sum of the weights of all edges on the path) of the newly formed connected component to all other connected components. This operation repeats until all pins are included in one connected component. Since this method may generate additional detours, we use a simple algorithm to detect potential feasible path reuse to shorten the wirelength. Furthermore, to accelerate the construction of RST, we parallelize the computation of the shortest distances between pins or connected components on the NAG. For the detailed algorithm and analysis of time complexity and scalability, please refer to App. B.5.

Results and Discussion

### Datasets and Experiment Setting

For training, we construct the training set from ISPD07 using the method described in Sec. 3.3. Since our network's input size is variable, we limit the nets' Half-perimeter wirelength (HPWL) in the training set to \(HPWL 128\), instead of fixing both width and height to 64. For test, in Sec. 4.2 we use the same settings from HubRouter  to divide samples outside the training set into four groups of small-scale nets to compare the connectivity and wirelength of NeuralSteiner and HubRouter. For more extensive experiments, in Sec. 4.3 we select six public chip designs (ibm01-06) from ISPD98  and eight two-layer large-scale chip designs (adapter(01-05)_2d, newblue(01-03)_2d) from ISPD07 (with no overlap with the training set) to perform global routing on all nets in these designs, comparing total overflow, wirelength and generation time. The ablation and generalization studies for NeuralSteiner are also conducted on chip designs from ISPD07. We repeat 3 times under different seeds for HubRouter on the small nets test set and ISPD98, and then choose the seed with best overflow for HubRouter (GAN) to conduct the ISPD07 experiment. More details about the experimental benchmark information and hyperparameter settings can be found in App. B.2.

### Connectivity and Wirelength on Small Nets

We compare NeuralSteiner with three different architectures of HubRouter on the same test set from part of ISPD07 benchmarks, which is divided into 'Route-small-4', 'Route-small', 'Route-large-4' and 'Route-large'. The number '4' in their names represents no more than or more than 4 pins, while'small' and 'large' represent whether the Half-perimeter wirelength (HPWL) of the net is less or more than 16. The size of all nets' input map is fixed at \(64 64\). We do not include PRNet  as it shows very poor connectivity on 'large' net in previous work . As shown in Table S2, NeuralSteiner ensures connectivity on this small-scale net test set, while achieving a wirelength rate (WLR) comparable to HubRouter. Due to the presence of recurrent crisscross attention calculation, our method is slightly behind in generation time.

### Global Routing on Large-scale Benchmarks

To conduct extensive experiments, we first compare the proposed NeuralSteiner with three versions of HubRouter  and traditional global routers Boxrouter , GeoSteiner  and FLUTE + Edge Shifting  on ibm01-06 benchmarks from ISPD98. We then conduct fully routing of 8 chip designs from ISPD07 using GeoSteiner, FLUTE + Edge Shifting, HubRouter and our method. Note that in our experiments, we do not use the randomly generated nets from previous works , as they are relatively simple and have no overflow in the results. Moreover, it has been already studied in HubRouter that the DQN method takes excessively long time to run on even very small cases and PRNet  also lags behind HubRouter in terms of wirelength, time and overflow, so they are not included in the comparison.

   Metric & Model & ibm01 & ibm02 & ibm03 & ibm04 & ibm05 & ibm06 \\   & GeoSteiner & **60142** & **165863** & **145678** & **162734** & **409709** & **275868** \\  & Bourzater & 62659 & 17110 & 146634 & 162775 & 410614 & 277913 \\  & FLUTE+BS & 6192 & 169251 & 146287 & 16547 & 411936 & 280447 \\  & HR-VAE & 64812 = 1252 & 176838 \(\) 6419 & 161032 \(\) 3231 & 179018 \(\) 4791 & 440302 \(\) 4577 & 301035 \(\) 5836 \\  & HR-DPM & 66575 \(\) 1394 & 190142 \(\) 2511 & 168505 \(\) 2103 & 1830514 & 191467463 & 2042342 \(\) 2958 \\  & HR-GAN & 60971 \(\) 1290 & 167316 \(\) 578 & 146893 \(\) 315 & 164084 \(\) 299 & 411887 \(\) 4529 & 277977 \(\) 514 \\  & NeuralSteiner & 61735 & 170405 & 148036 & 166648 & 415684 & 283727 \\   & GeoSteiner & **1.00** & **2.21** & **1.68** & **2.19** & **3.69** & **3.38** \\  & Bourzater & 5.33 & 9.76 & 8.42 & 31.69 & 10.75 & 24.94 \\   & FLUTE+ES & 2.90 & 4.71 & 5.87 & 17.16 & 6.83 & 13.64 \\   & HR-VAE & 8.41 \(\) 0.03 & 8.47 \(\) 0.06 & 8.59 \(\) 0.04 & 19.85 \(\) 0.04 & 12.44 \(\) 0.18 & 15.83 \(\) 0.11 \\   & HR-DPM & 1701.57 \(\) 34.19 & 25893 \(\) 19.63 & 2669.28 \(\) 22.77 & 3593.04 \(\) 24.10 & 3995.47 \(\) 19.57 & 4305.82 \(\) 132.85 \\   & HR-GAN & 37.40 \(\) 0.37 & 41.55 \(\) 0.51 & 50.84 \(\) 2.84 & 59.94 \(\) 2.75 & 69.42 \(\) 4.03 & 81.96 \(\) 3.98 \\   & NeuralSteiner & 27.18 & 34.79 & 46.24 & 50.37 & 75.99 & 70.32 \\   

Table 1: **Wirelength (WL) and running time on ISPD-98 (ibm01-06).** NeuralSteiner is compared with 2 traditional baselines and HubRouter with 3 generative structures (HR-VAE,HR-DPM, HR-GAN). Optimal results of WL and time are in bold.

**Routing Results on ISPD98.** Table 1 shows the total wirelength and generation time for all methods on ISPD98 benchmark. Since the total overflow of the traditional router Boxrouter is \(0\), we depict the routing overflow of the other methods in Fig. 3. NeuralSteiner significantly reduces the total overflow compared to the state-of-the-art learning-based method HubRouter (GAN), with an average reduction of 61.1% and up to 95% on ibm05. In terms of wirelength, NeuralSteiner does not incur much additional loss, maintaining it within 1.8%. Furthermore, due to the construction of the net parallel routing tasks, NeuralSteiner achieves shorter generation time compared with HubRouter (GAN). The comparison of the actual solutions between NeuralSteiner and Hubrouter is given in Fig. S2.

**Routing Results on ISPD07.** Based on the experimental results on ISPD98, we select four methods GeoSteiner, HubRouter (HR-GAN), and NeuralSteiner for comparison on the larger-scale ISPD07 chip designs. The summary of ISPD07 benchmarks we use is detailed introduced in Table S1, as well as the number of predicted candidate points for ISPD07. According to Table S1, the average number of candidate points added by NeuralSteiner is not significantly more than the average number of pins, which means that for the vast majority of nets, the number of nodes in the NAG will remain at a small scale and keep friendly to the calculation of the overflow-avoiding RST algorithm introduced in Sec. 3.4. The total overflow (OF), wirelength (WL) and generation time are shown in Table 2. According to Table 2, as the sizes of chip designs and nets further increase, NeuralSteiner achieves more dramatic reduction in total overflow, with an average reduction of 97.8% across all eight designs, and up to a 99.8% reduction on design adaptec04_2d, while the increase in wirelength still remains within 1.8% compared to HubRouter.

   Metric & Method & adaptec01\_2d & adaptec02\_2d & adaptec03\_2d & adaptec04\_2d & adaptec05\_2d & newblue01\_2d & newblue02\_2d & newblue03\_2d \\   & GeoSteiner & 35945 & 53848 & 142254 & 45950 & 102300 & 1734 & 1832 & 584761 \\  & FLUTE-483 & 35218 & 50947 & 137104 & 42306 & 957907 & 1348 & 1713 & 585907 \\  & HR-GAN & 35441 & 53652 & 142131 & 45230 & 102108 & 1516 & 1857 & 585901 \\   & NeuralSteiner & **82** & **255** & **728** & **97** & **431** & **5** & **35** & **1043** \\   & GeoSteiner & **338961** & **2390727** & **3302048** & **886543** & **9788471** & **230246** & **495925** & **7371273** \\  & FLUTE-485 & 341361 & 3238503 & 9417934 & 896070 & 986249 & 247941 & 4651033 & 745720 \\  & HR-GAN & 3407033 & 3229110 & 935590 & 8888775 & 9832110 & 2393024 & 4623006 & 7391055 \\   & NeuralSteiner & 3438.1717 & 3247429 & 45951171 & 9030932 & 9915795 & 2365699 & 4668079 & 7480769 \\   & GeoSteiner & **84.7** & **111.2** & **320.06** & **267.13** & **264.13** & **214.68** & **183.82** & **135.48** \\  & FLUTE-485 & 118.48 & 187.03 & 396.51 & 376.72 & 360.68 & 169.36 & 223.55 & 438.79 \\  & HR-GAN & 593.02 & 790.44 & 1324.81 & 1387.01 & 1384.96 & 849.34 & 1221.16 & 1526.86 \\   & NeuralSteiner & 347.20 & 461.35 & 1351.91 & 1138.66 & 1105.54 & 390.34 & 446.68 & 1225.79 \\   

Table 2: **Evaluating NeuralSteiner and comparing it with state-of-the-art approaches on ISPD-07 (adaptec01-05)_2d, newblue(01-03)_2d).** Overflow (OF), wirelength (WL) and running time are compared among traditional router GeoSteiner, FLUTE + Edge Shift and HubRouter with GAN structures (HR-GAN), which achieves the best overflow and wirelength among three kinds of HubRouters on ISPD98. Optimal results of overflow, wirelength and time are in bold.

Figure 3: **Overflow on ISPD98 (ibm01-06).** Overflow of Geosteiner, HubRouter (VAE, DPM, GAN) and NeuralSteiner on ISPD-98 (ibm01-06) cases. Note that NeuralSteiner causes only 18 overflows on ibm05, which is annotated in the figure.

### Generalization and Ablation Study

**Generalization.** Note that the NeuralSteiner proposed by this work is trained on small-scale nets with HPWL less than 128 and is tested on 14 large-scale public benchmarks, which contain net with HPWL even larger than 2000. Its ability to generalize to unseen and large-scale nets can be demonstrated. Fig. 4 shows the remaining resource map of adaptec01_2d design after routing by HubRouter and the proposed NeuralSteiner respectively, which demonstrates that NeuralSteiner can generalize to larger chips by using routing resource more evenly and avoiding the vast majority of overflow. Moreover, to extensively examine the ability of NeuralSteiner to mitigate overflow in the post-routing results, we integrate our method into CUGR and compare it with the original CUGR on post-detailed routing metrics on ISPD18/19 benchmarks, which are much larger than ISPD98 and ISPD07. The detailed routing is conducted by a commonly used detailed router DRCU. Short and space are two kinds of design rule violation caused by overflow in the detailed routing process. Table 3 shows that by integrating NeuralSteiner into CUGR, we achieve 4.4% and 19.1% reduction on average in shorts and spaces respectively, with minimal losses in wirenlength and vias. This demonstrates that NeuralSteiner, as a pre-routing overflow mitigation method, is beneficial for reducing overflow in the post-routing results.

**Ablation Study.** To study the role of recurrent crisscross attention (RCCA) module, as well as our loss function, we respectively ablate the RCCA module from the network architecture and the overflow loss \(_{of}\) from the loss function and keep other training settings the same. The modified models are tested in comparison with the unchanged NeuralSteiner on ibm01 and adaptec05_2d. Furthermore, to validate the effectiveness of our two-phase method, especially the effectiveness of predicted candidate points in reducing overflow, we ablate the output of the neural network and compare it with the unchanged NeuralSteiner. Results in Table 4 indicate that, although the wirenlength of three modified models are not significantly affected due to graph-based post-processing, there are notable increases in overflow, especially on larger adaptec05_2d design. This implies that both RCCA module and overflow loss can help NeuralSteiner learn congestion-avoiding candidate points and acquire a better generalization ability to larger nets. Additionally, relying solely on the graph-based RST construction to generate RST on the Hanan grid without predicted points leads to more than \(20\) increase in overflow on adaptec05_2d. This demonstrates that the neural network in our NeuralSteiner has learned an distribution of better candidate points for overflow-avoiding RST under tight resource

Figure 4: **The Overflow Distribution after routing by HubRouter and NeuralSteiner. (a), (c): the horizontal and vertical overflow of HubRouter; (b), (d): the horizontal and vertical overflow of NeuralSteiner. Depth of red color indicates the number of overflow.**

    & \))} & \))} &  &  \\  & CUGR & CUGR+NS & CUGR & CUGR+NS & CUGR & CUGR+NS & CUGR & CUGR+NS \\  ispd18\_t5m5 & 2.878 & **2.874** & **9.154** & 9.180 & 389.5 & **362.5** & 16 & **7** \\ ispd18\_t8m5 & **6.653** & 6.671 & 22.466 & **22.452** & 414.6 & **412.8** & 66 & **65** \\ ispd19\_t7 & **12.556** & 12.621 & **40.446** & 40.956 & 2117.6 & **2042.3** & 7084 & **6472** \\ ispd19\_7m5 & **11.273** & 11.303 & **40.356** & 40.613 & 2368.5 & **2219.0** & 7715 & **6964** \\ 
**Average** & **1** & 1.002 & **1** & 1.005 & 1 & **0.956** & 1 & **0.809** \\   

Table 3: **Comparison of CUGR + NeuralSteiner and original CUGR on post-detailed routing metrics on ISPD18/19 benchmarks. The detailed routing is conducted by DRCU. Short and space are two kinds of design rule violation caused by overflow.**constraints. We also study the role of our post-processing by replacing the REST method and the stripe mask used in the hub-pin-connection phase of HubRouter by our NAG-based RST construction algorithm. As shown in Table 5, although the congestion of this combination is still \(11\) larger than that of NeuralSteiner, it has decreased by nearly 95% compared to the original HubRouter, which fully demonstrates the effectiveness of our post-processing method.

## 5 Conclusion

In this paper, we introduce NeuralSteiner, a two-phase learning-based global routing scheme. By combining neural network-predicted candidate points with a post-processing method based on net augmented graph, NeuralSteiner can generate overflow-avoiding and connectivity-assured routing solutions for unseen large-scale nets in one shot, substantially reducing the overflow by up to 99.8% on real-world chip benchmarks, which narrows the gap between learning-based routing method and practical chip routing applications.

The main limitation of our method is that we still rely on heuristic post-processing algorithms for Rectilinear Steiner Tree (RST) construction, which leads to time-consuming calculations and a slight increase in wirelength. In the future, we will explore using continuous probabilistic candidate point maps and investigate end-to-end learning with neural networks for generating overflow-avoiding RSTs. Addressing this limitation could lead to enhanced performance in routing efficiency and quality.

## 6 Acknowledgements

We would like to thank the National Key Research and Development Program of China (2020YFA0907000), the National Natural Science Foundation of China (32370657, 32271297, 82130055, 62072435), and the Major Key Project of PCL (No. PCL2023A03) for providing financial supports for this study and publication charges. The numerical calculations in this study were supported by ICT Computer X center, CAS Xiandao-1 and Pengcheng Cloudbrain.