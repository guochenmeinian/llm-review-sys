# Causal Effect Regularization: Automated Detection

and Removal of Spurious Correlations

 Abhinav Kumar

Microsoft Research

abhinavkumar.wk@gmail.com &Amit Deshpande

Microsoft Research

amitdesh@microsoft.com &Amit Sharma

Microsoft Research

amshar@microsoft.com

###### Abstract

In many classification datasets, the task labels are _spuriously_ correlated with some input attributes. Classifiers trained on such datasets often rely on these attributes for prediction, especially when the spurious correlation is high, and thus fail to generalize whenever there is a shift in the attributes' correlation at deployment. If we assume that the spurious attributes are known a priori, several methods have been proposed to learn a classifier that is invariant to the specified attributes. However, in real-world data, information about spurious attributes is typically unavailable. Therefore, we propose a method that automatically identifies spurious attributes by estimating their causal effect on the label and then uses a regularization objective to mitigate the classifier's reliance on them. Although causal effect of an attribute on the label is not always identified, we present two commonly occurring data-generating processes where the effect can be identified. Compared to recent work for identifying spurious attributes, we find that our method, AutoACER, is more accurate in removing the attribute from the learned model, especially when spurious correlation is high. Specifically, across synthetic, semi-synthetic, and real-world datasets, AutoACER shows significant improvement in a metric used to quantify the dependence of a classifier on spurious attributes (\(\)Prob), while obtaining better or similar accuracy. Empirically we find that AutoACER mitigates the reliance on spurious attributes even under noisy estimation of causal effects or when the causal effect is not identified. To explain the empirical robustness of our method, we create a simple linear classification task with two sets of attributes: causal and spurious. Under this setting, we prove that AutoACER only requires the _ranking_ of estimated causal effects to be correct across attributes to select the correct classifier.

## 1 Introduction

When trained on datasets where the task label is spuriously correlated with some input attributes, machine learning classifiers have been shown to rely on these attributes (henceforth known as _spurious_ attributes) for prediction [10; 23; 11]. For example in a sentiment classification dataset that we evaluate (Twitter-AAE ), a demographic attribute like race could be spuriously correlated with the sentiment of a sentence . Classifiers trained on such datasets are at risk of failure during deployment when the correlation between the task label and the spurious attribute changes [2; 22; 30; 10].

Assuming that a set of auxiliary attributes is available at training time (but not at test time), several methods have been proposed to mitigate the classifier's reliance on the spurious attributes. The first category of methods assumes that the spurious attributes are known a priori. They develop regularization [35; 15; 22], optimization  or data-augmentation [17; 16; 14] strategies to train a classifier invariant to the specified spurious attributes. The second category of methods relaxes the assumption of known spurious attributes by automatically identifying the spurious attributes andregularizing the classifier to be invariant to them. To identify spurious attributes, these methods impose assumptions on the type of spurious correlation they deal with. For example, they may assume that attributes are modified in input via symmetry transformation  or a group transformation , or the data-generating process follows a specific graph structure such as anti-causal . However, all these methods assume a strict dichotomy--every attribute is either spurious or not--which makes them susceptible to imposing incorrect invariance whenever there is a mistake in identifying the spurious attribute.

In this paper, we propose a method that regularizes the effect of attributes on a classifier proportional to their _average causal effect_ on the task label, instead of strictly binning them as spurious (or not) using an arbitrary threshold. We propose a two-step method that (**1**) uses an effect estimation algorithm to find the causal effect of a given attribute; (**2**) regularizes the classifier proportional to the estimated causal effect for each attribute estimated in the first step. We call the method _AutoACER_, Causal Effect Regularization for automated removal of spurious correlations. If the estimated causal effects are correct, AutoACER is perfect in removing a classifier's reliance on spurious attributes (i.e., attributes with causal effect close to zero), while retaining the classifier's acccuracy. But in practice it is difficult to have good estimates of the causal effect due to issues like non-identifiability, noise in the labels , or finite sample estimation error . Under such conditions, we find that AutoACER is more robust than past work  since it does not group the attributes as spurious or causal and regularizes the classifier proportional to the estimated effect.

We analyze our method both theoretically and empirically. First, we provide the conditions under which causal effect is identified. An implication of our analysis is that causal effect is identified whenever the relationship between the attribute and label is fully mediated through the observed input (e.g, text or image). This is often the case in real-world datasets where human labellers use only the input to generate the label. Even if the causal effect is not identified, we show that AutoACER is robust to errors in effect estimation. Theoretically, on a simple classification setup with two sets of attributes--causal and spurious--we prove that only the correct ranking of the causal effect estimates is needed to learn a classifier invariant to spurious attributes. For the general case with multiple disentangled high-dimensional causal and spurious attributes, under the same condition on correct causal effect rankings, we prove that the desired classifier (that does not use spurious attributes) is preferred by AutoACER over the baseline ERM classifier.

To confirm our result empirically, we use three datasets that introduce different levels of difficulty in estimating the causal effect of attributes: (**1**) Syn-Text: a synthetic dataset that includes an unobserved confounding variable and violates the identifiability conditions of causal effect estimators; (**2**) MNIST34: a semi-synthetic dataset from , where we introduce noise in the image labels; (**3**) Twitter-AAE:, which is a real-world text dataset. To evaluate the robustness of methods to spurious correlation, we create multiple versions of each dataset with varying levels of correlation with the spurious attribute, thereby increasing the difficulty to estimate the causal effect. Even with a noisy

Figure 1: To reduce dependence on spurious attributes in a classifier, our method AutoACER has two stages. Stage 1 (middle panel) estimates the causal effect of attributes on the task label. Stage 2 (right panel) ensures that the causal effect of each attribute on the classifier’s prediction matches its estimated causal effect on the task label. In the bottom panel, due to high spurious correlation the estimation of average causal effect is error-prone. AutoACER works well even in such high correlation scenarios as shown by the Spuriousness score, \(\)Prob (see definition in §4.2).

estimate of the causal effect of attributes, our method shows significant improvement over previous algorithms in reducing the dependence of a classifier on spurious attributes, especially in the high correlation regime. Our contributions include,

* Causal effect identifiability guarantee for two realistic causal data-generating processes, which is the first step to automatically distinguish between causal and spurious attributes.
* A method, AutoACER, to train a classifier that obeys the causal effect of attributes on the label. Under a simplified setting, we show that it requires only the correct ranking of attributes.
* Evaluation on three datasets showing that AutoACER is effective at reducing spurious correlation even under noisy, high correlation, or confounded settings.

## 2 OOD Generalization under Spurious Correlation: Problem Statement

For a classification task, let \((^{i},y^{i},^{i})_{i=1}^{n}_{m}\) be the set of example samples from the data distribution \(\), where \(^{i}\) are the input features, \(y^{i} Y\) are the task labels and \(^{i}=(a^{i}_{1},,a^{i}_{k})\), \(a^{i}_{j} A_{j}\) are the auxiliary attributes, henceforth known as _attributes_ for the example \(^{i}\). We use "\(_{a}\)," to denote an example "\(\)" to have attribute \(a_{j} A_{j}\). These attributes are only observed during the training time. The goal of the classification task referred to as _task_ henceforth, is to predict the label \(y^{i}\) from the given input \(^{i}\). The task classifier can be written as \(c(h())\) where \(h:\) is an encoder mapping the input \(\) to a latent representation \(:=h()\) and \(c: Y\) is the classifier on top of the representation \(\).

**Generalization to shifts in spurious correlation.** We are interested in the setting where the data-generating process is _causal_ i.e. there is a certain set of _causal_ attributes that affect the task label \(y\). Upon changing these causal attributes, the task label changes. Apart from these attributes, there could be other attributes defining the overall data-generating process (see Fig. 2 for examples). A attribute \(c\) is called spurious attribute if it is correlated with the task label in the training dataset and thus could be used by a classifier trained to predict the task label, as a _shortcut_[38; 22]. But their correlation could change at the time of deployment, affecting the classifier's accuracy.

Using the attributes available at training time, our goal is to train a classifier \(c(h(x))\) that is robust to shift in correlation between the task label and the spurious attributes. We use the fact that changing spurious attributes will not lead to a change in the task label i.e. they have zero causal effect on the task label \(y\). Hence, we use the _estimated causal effect_ of an attribute to automatically identify its degree of spuriousness. For a _spurious attribute_, its true causal effect on the label is zero and hence the goal is to ensure its causal effect on the classifier's prediction is also zero. More generally, we would like to regularize the effect of each attribute on the classifier's prediction to its causal effect on the label. In other words, unlike existing methods that aim to discover a subset of attributes that are spurious [24; 25; 38], we aim to estimate the causal effect of each attribute and match it. Since our method avoids hard categorization of attributes that downstream regularizers need to follow, we show that is a more robust way of handling estimation errors when spurious correlation may be high.

## 3 Causal Effect Regularization: Minimizing reliance on spurious correlation

We now describe our method to train a classifier that generalizes to shift in attribute-label correlation by automatically identifying and imposing invariance w.r.t to spurious attributes. In SS3.1, we provide sufficient conditions for identifying the causal effect, a crucial first step to detect the spurious attributes. Next, in SS3.2 and SS3.3, we present the AutoACER method and its theoretical analysis.

### Causal Effect Identification

Since our method relies on estimating the causal effect of attributes on the task label, we first show that the attributes' causal effect is _identified_ for many commonly ocurring datasets. The identifiability of the causal effect of an attribute depends on the data-generating process (DGP). Below we present two illustrative DGPs (DGP-1 and DGP-2 from Fig. 2) under which the causal effect is identified. _DGP-1_ refers to the case where the task label is generated based on the observed input \(X\) which in turn may be caused by observed and unobserved attributes. _DGP-1_ is common in many real-world datasets where the task labels are annotated based on the observed input \(X\) either automatically using some deterministic function or using human annotators [17; 29]. Thus _DGP-1_ is applicable for all the settings where the input \(X\) has all the sufficient information for creating the label. Mouli and Ribeiro  consider another DGP where a set of _transformations_ (like rotation or vertical-flips in image) over a base (unobserved) image \(x\) generates the input \(X\). We adapt their graph to our setting where we include each observed attribute as a node in the graph, along with hidden confounders. Depending on whether they are spurious or not, these attributes may cause the task label \(Y\). DGP-2 and DGP-3 represent two such adaptations from their work, with the difference that DGP-3 also allows unobserved attributes to cause the task label.

Generally for identifying the causal effect one assumes that we only have access to observational data (\(\)). But here we also assume access to the interventional distribution for the input, \(P(X|do(A))\) where the attribute \(A\) is set to a particular value, as in . This is commonly available in vision datasets via data augmentation strategies over attributes like rotation or brightness, and also in text datasets using generative language models [5; 37]. Having access to interventional distribution \(P(X|do(A))\) could help us identify the causal effect in certain cases where observational data (\(\)) alone cannot as we see below.

**Proposition 3.1**.: _Let DGP-1 and DGP-2 in Fig. 2 be the causal graphs of two data-generating processes. Let \(A,C,S\) be different attributes, \(X\) be the observed input, \(Y\) be the observed task label and \(U\) be the unobserved confounding variable. In DGP-2, \(x\) is the (unobserved) core input feature that affects the label \(Y\). Then:_

1. _DGP-1 Causal Effect Identifiability:_ _Given the interventional distribution_ \(P(X|do(A))\)_, the causal effect of the attribute_ \(A\) _on task label_ \(Y\) _is identifiable using observed data distribution._
2. _DGP-2 Causal Effect Identifiability:_ _Let_ \(C\) _be a set of observed attributes that causally affect the task label_ \(Y\) _(unknown to us),_ \(S\) _be the set of observed attributes spuriously correlated with task label_ \(Y\) _(again unknown to us), and let_ \(=C S\) _be the given set of all the attributes. Then if all the causal attributes are observed then the causal effect of all the attributes in_ \(V\) _can be identified using observational data distribution alone._

Proof Sketch.: **(1)** We show that we can identify the interventional distribution \(P(Y|do(A))\) which is needed to estimate the causal effect of \(A\) on \(Y\) using the given observational distribution \(P(Y|X)\) and interventional distribution \(P(X|do(A))\). **(2)** For both causal attribute \(C\) and spurious attribute \(S\), we show that we can identify the interventional distribution \(P(Y|do(S))\) or \(P(Y|do(C))\) using purely observational data using the same identity without the need to know whether the variable is causal or spurious a priori. See SSB for proof. 

Note that _DGP-1_ and _DGP-2_ are not exhaustive. We provided them simply as illustrative examples to show that there exist DGPs, corresponding to commonly occurring datasets, where the attributes' effect is identified. However, the effect is not identified in _DGP-3_ and possibly other DGPs. Moreover, in practice, the underlying DGP for a given task may not be known or mis-specified. Hence, in SS3.2, we design our method AutoACER such that it does not depend on knowledge of the true DGP. When the effect is identified, our method is theoretically guaranteed to work well. Empirically, as we show in Section 4, our method also works to remove the spurious correlation for datasets corresponding to DGP-3 where the causal effect of A on Y is not identified. In comparison, prior methods like Mouli and Ribeiro  provably fail under DGP-3 (see SSC for the proof that their method would fail to detect the spurious attribute).

### AutoACER: Causal Effect Regularization for predictive models

Our proposed method proceeds in two stages. In the first stage, it uses a causal effect estimator to identify the causal effect of an attribute on the task label. Then in the second stage, it regularizes the classifier proportional to the causal effect of every attribute.

**Stage 1: Causal Effect Estimation.** Given a set of attributes \(=\{a_{1},,a_{k}\}\), the goal of this step is to estimate the causal effect (\(TE_{a_{i}}\)) of every attribute \(a\). The causal effect of a attribute on the label \(Y\) is defined as the expected change in the label \(Y\) when we change the attribute (see SSA for formal definition). If the data-generating process of the task is one of DGP-1 or DGP-2, our Prop 3.1 gives us sufficient conditions needed to identify the causal effect. Then one can use appropriate causal effect estimators that work under those conditions or build their own estimators using the closed form causal effect estimand given in the proof of Prop 3.1. However, to build a general method, we propose a technique that does not assume knowledge of the DGP. Specifically, we use a conditioning-based estimator (based on the backdoor criterion ) and assume the observed variables (X) as the backdoor variables. There is a rich literature on estimating causal effects for high dimensional data [12; 33; 6]. We use a deep learning-based estimator from Chernozhukov et al.  to estimate the causal effect (henceforth called _Riesz_). Given the treatment along with the rest of the covariates, it learns a common representation that approximates backdoor adjustment to estimate the causal effect (see SSE for details). Even if the causal effect in the relevant dataset is identifiable, we might get a noisy estimate of the effect due to finite sample error or noise in the labels. Later in SS3.3 and SS4 we will show that our method is robust to error in the causal effect estimate of attributes both theoretically and empirically. Finally, as a baseline effect estimator, we use the direct effect estimator (henceforth called _Direct_) defined as \(_{X}((Y|X,a=1)-(Y|X,a=0))\) for attribute \(a\) that has limited identifiability guarantees (see SSE for details).

**Stage2: Regularization.** Here our method regularizes the model prediction with the estimate of causal effect \(=\{TE_{a_{1}},,TE_{a_{k}}\}\) of each attribute \(=\{a_{1},,a_{k}\}\). The loss objective is,

\[_{AutoACER}_{(,y)} _{task}c(h()),y+R_{Reg} ,y\] (1)

where \(R\) is the regularization strength hyperparameter, \(\) is the training data distribution (SS2). The first term \(_{task}(c(h()),y)\) can be any training objective e.g. cross-entropy or max-margin loss for training the encoder \(h\) and task classifier \(c\) jointly to predict the task label \(y\) given input \(\). Our regularization loss term \(_{Reg}\) aims to regularize the model such that the causal effect of an attribute on the classifier's output matches the estimated causal effect of the attribute \(a_{i}\) on the label. Formally,

\[_{Reg}_{i=\{1,2,,||\}}_{( _{a^{}_{i}})(_{a_{i}})}c(h( {x})_{a_{i}})-c(h(^{}_{a_{i}}))-TE_{a_{i}}^{2}\] (2)

where \(_{a^{}_{i}}(_{c_{i}})\) be a sample from counterfactual distribution \((_{c^{}_{i}}|_{c_{i}})\) and \(_{c^{}_{i}}\) is the input had the attribute in input \(_{c_{i}}\) been \(c^{}_{i}\).

### Robustness of AutoACER with noise in the causal effect estimates

Our proposed regularization method relies primarily on the estimates of the causal effect of any attribute \(c_{i}\) to regularize the model. Thus, it becomes important to study the efficacy of our method under error or noise in causal effect estimation, which is expected in real-world tasks due to finite sample issues, incorrect choice of causal effect estimators, or due to properties of the unknown underlying DGP. We consider a simple setup to theoretically analyze the condition under which our method will train a better classifier than the standard ERM max-margin objective (following previous work [19; 13; 26]) in terms of generalization to the spurious correlations. Let \(=\{ca_{1},,ca_{K},sp_{1},,sp_{J}\}\) be the set of available attributes where \(ca_{k}\) are causal attributes and \(sp_{j}\) are the spurious attributes. For simplicity, we assume that the representation encoder mapping \(}\) to \(\) i.e \(h:\) is frozen and the final _task_ classifier (\(c\)) is a linear operation over the representation. Following Kumar et al. , we also assume that \(\) is a disentangled representation

Figure 2: Causal graphs showing different data-generating processes. Shaded nodes denote observed variables; the red arrow denotes an unknown target relationship. Node X denotes the observed input features and node Y denotes the task label in all the DGPs. Node A refers to attributes; could denote either causal or spurious attribute. If A is a causal attribute then the red arrow will be present in the underlying true graph otherwise not. Node U denotes the unobserved confounding variable which could be the potential reason for spurious correlation between an attribute (A or S) and Y. The node C in DGP-2 denotes causal attributes and node S denotes spuriously correlated attributes. The node x in DGP-2 and DGP-3 denotes the unobserved causal attribute that creates the label Y along with other observed causal attributes (C or A). In the first two graphs, the causal effect of attributes (A and C, S respectively) on Y is identified (see Prop 3.1).

w.r.t. the causal and spurious attributes, i.e., the representation vector \(\) can be divided into two subsets, features corresponding to the causal and spurious attribute respectively. Thus the task classifier takes the form \(c()=_{k=1}^{K}_{ca_{k}}_{ca_{k}}+_{j=1}^{J} _{sp_{j}}_{sp_{j}}\). Note that we make the disentanglement assumption only for creating a simple setup so that we can theoretically study the effectiveness of our method in the presence of noisy estimates of the causal effects. Our method does not require this assumption.

Let \(_{task}(;(,y_{m}))\) be the max-margin objective (see SSA for details) used to train the task classifier \(``c"\) to predict the task label \(y_{m}\) given the _frozen_ latent space representation \(\). Let the task label \(y\) and the attribute \(``a"\) where \(a\) be binary taking value from \(\{0,1\}\). The causal effect of an attribute \(a\) on the task label \(Y\) is given by \(TE_{a}=(Y|do(a)=1)-(Y|do(a)=0)=P(y=1|do(a)=1)-P(y=1|do(a)=0)\) (see SSA for definition). Thus the value of the causal effect is bounded s.t. \(_{a}[-1,1]\) where the ground truth causal effect of spuriously correlated attribute \(``sp"\) is \(TE_{sp}=0\) and for causal attribute \(``ca"\) is \(|TE_{ca}|>0\). Given that we assume a linear model, we instantiate a simpler form of the regularization term \(_{Reg}\) for the training objective given in Eq. 1:

\[_{Reg}_{k=\{1,2,,K\}}_{ca_{k}}\|_{ ca_{k}}\|_{p}+_{j=\{1,2,,J\}}_{sp_{j}}\|_{sp_{j}}\|_{p}\] (3)

where \(_{ca_{k}} 1/|TE_{ca_{k}}|\) and \(_{sp_{j}} 1/|TE_{sp_{j}}|\) are the regularization strength for the causal and spurious features \(_{ca_{k}}\) and \(_{sp_{j}}\) respectively, \(||\) is the absolute value operator and \(\|\|_{p}\) is the \(L_{p}\) norm of the vector \(\). Since \(|TE_{()}|\), we have \(_{()}=1/|TE_{()}| 1\). In practice, we only have access to the empirical estimate of the causal effect \(TE_{()}\) denoted as \(_{()}\) and the regularization coefficient becomes \(_{()}=1/|_{()}|\). Now we are ready to state the main theoretical result that shows our regularization objective will learn the correct classifier which uses only the causal attribute for its prediction, given that the ranking of estimated treatment effect is correct up to some constant factor. Let \([S]\) denote the set \(\{1,,S\}\).

**Theorem 3.1**.: _Let the latent space be frozen and disentangled such that \(=[_{ca_{1}},,_{ca_{K}},_{sp_{1}},,_{ sp_{J}}]\) (Asm D.1). Let the desired classifier \(c^{des}()=_{k=1}^{K}_{ca_{k}}^{des}_{ca_{k}}\) be the max-margin classifier among all linear classifiers that use only the causal features \(_{ca_{k}}\)'s for prediction. Let \(c^{mm}()=_{k=1}^{K}_{ca_{k}}^{mm}_{ca_{k}}+_{j =1}^{J}_{sp_{j}}^{mm}_{sp_{j}}\) be the max-margin classifier that uses both the causal and the spurious features, and let \(_{sp_{j}}^{mm} 0\), \( j[J]\). We assume \(_{sp_{j}}^{mm} 0\), \( j[J]\), without loss of generality because otherwise, we can restrict our attention only to those \(j[J]\) that have \(_{sp_{j}}^{mm} 0\). Let the norm of the parameters of both the classifier be set to \(1\) i.e \(_{k=1}^{K}\|_{ca_{k}}^{mm}\|_{p=2}^{2}+_{j=1}^{J}\|_{sp_{j }}^{mm}\|_{p=2}^{2}=_{k=1}^{K}\|_{ca_{k}}^{des}\|_{p=2}^{2}=1\). Then if regularization coefficients are related s.t. \((\{}}{_{sp_{j}}} _{k,j}\}_{k[K],j[J]})<\) where \(_{k,j}=_{ca_{k}}^{des}\|-\|_{ca_{k}}^{mm}\|_{p}}{\| _{sp_{j}}^{mm}\|_{p}}\), then_

1. _[leftmargin=0.5cm]_
2. _Preference:_ \(_{AutoACER}(c^{des}())<_{AutoACER}c^{mm}( )\)_. Thus, our causal effect regularization objective (Def 3) will choose the_ \(c^{des}()\) _classifier over the max-margin classifier_ \(c^{mm}()\) _which uses the spuriously correlated feature._
3. _Global Optimum: The desired classifier_ \(c^{des}()\) _is the global optimum of our loss function_ \(_{AutoACER}\) _when_ \(J=1\)_,_ \(K=1\)_,_ \(p=2\)_, the regularization strength are related s.t._ \(_{ca_{1}}<_{sp_{1}}|_{ca_{1}}|>|_{sp_{1}}|\) _and search space of linear classifiers_ \(c()\) _are restricted to have the norm of parameters equal to 1._

**Remark**.: _The result of Theorem 3.1 holds under a more intuitive but stricter constraint on the regularization coefficient \(\) which states that \(_{sp_{j}}>(_{k,j})_{ca_{k}}| _{sp_{j}}|<(_{k,j})|_{ca_{k}}|\)\( k[K]\) and \(j[J]\). The above constraint states that if the treatment effect of the causal feature is more than that of the spurious feature by a constant factor then the claims in Theorem 3.1 hold._

Proof Sketch.: **(1)** We compare both the classifier \(c^{des}()\) and \(c^{mm}()\) using our overall training objective to our training objective \(_{AutoACER}\) (Eq. 1). Given the relation between the regularization strength mentioned in the above theorem is satisfied, we then show that one can always choose a regularization strength \(``R"\) greater than a constant value s.t classifier \(c^{mm}()\) incurs a greater regularization penalty than \(c^{des}()\) (Eq. 3) which is not compensated by the gain in the max-margin objective of \(c^{mm}()\) over \(c^{des}()\). Thus, the desired classifier \(c^{des}()\) has lower overall loss than the \(c^{mm}()\) in terms of our training objective \(_{AutoACER}\). **(2)** We use the result from the first claim to show that \(c^{des}()\) has a lower loss than any other classifier that uses the spurious feature \(_{sp_{1}}\). Then, among the classifier that only uses the causal feature \(_{ca_{1}}\), we show that again \(c^{des}()\) has the lower loss w.r.t. \(_{AutoACER}\). Thus the desired classifier has a lower loss than all other classifiers w.r.t \(_{AutoACER}\) with parameter norm \(1\) hence a global optimum. Refer SSD for proof.

## 4 Empirical Results

### Datasets

Theorem 3.1 showed that our method can find the desired classifier in a simple linear setup. We now evaluate the method on a synthetic, semi-synthetic and real-world dataset. Details are in SSE.

**Syn-Text.** We introduce a synthetic dataset where the ground truth causal graph is known and thus the ground truth spuriously correlated feature is known apriori (but unknown to all the methods). The dataset contains two random variables (_causal_ and _confound_) that cause the binary main task label \(y_{m}\) and the variable _spurious_ is spuriously correlated with the _confound_ variable. Given the values of spurious and causal features, we create a sentence as input. We define Syn-Text-Obs-Conf- a version of the Syn-Text dataset where all three variables/attributes are observed. Next, to increase difficulty, we create a version of this dataset -- Syn-Text-Unobs-Conf-- where the _confound_ attribute is not observed in the training dataset, corresponding to DGP-3 from Fig. 2.

**MNIST34.** We use MNIST  to compare our method on a similar semi-synthetic dataset as used in Mouli and Ribeiro . We define a new task over this dataset but use the same attributes, color, rotation, and digit, whose associated transformation satisfies the _lumpability_ assumption in their work. We define the digit attribute (\(3\) and \(4\)) and the color attribute (red or green color of digit) as the causal attributes which creates the binary main task label using the \(XOR\) operation. Then we add rotation (\(0^{@math@degree}\) or \(90^{@math@degree}\)) to introduce spurious correlation with the main task label. This dataset corresponds to DGP-2, where \(C\) is color and digit attribute, \(S\) is rotation attribute and \(x\) is an empty set.

**Twitter-AAE .** This is a real-world dataset where the main task is to predict a binary sentiment label from a tweet's text. The tweets are associated with _race_ of the author which is spuriously correlated with the main task label. Since this is a real-world dataset where we have not artificially introduced the spurious attribute we don't have a ground truth causal effect of _race_ on the sentiment label. But we expect it to be zero since changing _race_ of a person should not change the sentiment of the tweet. We use GPT3  to create the counterfactual example by prompting it to change the race-specific information in the given sentence (see SSF for examples). This dataset corresponds to DGP-1, where the node \(A\) is the spurious attribute race.

**Varying spurious correlation in the dataset.** Since the goal is to train a model that doesn't rely on the spuriously correlated attribute, we create multiple settings for every dataset with different levels of correlations between the main task labels \(y_{m}\) and spurious attribute \(y_{c_{sp}}\). Following , we use a _predictive correlation_ metric to define the label-attribute correlation that we vary in our experiments. The predictive correlation (\(\)) measures how informative one label or attribute (\(s\)) is for predicting the other (\(t\)), \( Pr(s=t)=_{i=1}^{N}[s=t]/N\), where \(N\) is the size of the dataset and \([]\) is the indicator function that is \(1\) if the argument is true otherwise \(0\). Without loss of generality, assuming \(s=1\) is correlated with \(t=1\) and similarly, \(s=0\) is correlated with \(t=0\); predictive correlation lies in \([0.5,1]\) where \(=0.5\) indicates no correlation and \(=1\) indicates that the attributes are fully correlated. For Syn-Text dataset, we vary the predictive correlation between the confound attribute and the spurious attribute; for MNIST34, between the combined causal attribute (digit and color) and the spurious attribute (rotation); and for Twitter-AAE, between the task label and the spurious attribute (race). See SSE for details.

### Baselines and evaluation metrics

**Baselines.** We compare our method with five baseline algorithms: (i) _ERM_: Base algorithm for training the main task classifier using cross-entropy loss without any additional regularization. (ii) _Mouli+CAD_: The method proposed in  to automatically detect the spurious attributes and train a model invariant to those attributes. Given a set of attributes, this method computes a score for every subset of attributes, selects the subset with a minimum score as the spurious subset, and finally enforces invariance with respect to those subsets using counterfactual data augmentation (CAD) [32; 16]. (iii) _Mouli+AutoACER_ : Empirically, we observe that CAD does not correctly impose invariance for a given attribute (see SSF for discussion). Thus we add a variant of Mouli+CAD's method where instead of using CAD it uses our regularization objective (Eq. 2) to impose invariance using the causal effect \(TE_{a}=0\) for some attribute \(``a"\). (iv) _Just Train Twice_ (_JTT_): Given a known spurious attribute, _JTT_ aims to train a model that performs well on all subgroups of the dataset, where subgroups are defined based on different correlations between the task label and spurious attribute. Unlike methods like GroupDRO , _JTT_ requires (expensive) subgroup labels only for the examples in the validation set. (v) _Invariant Risk Minimization  (IRM)_: This method requires access to multiple _environments_ or subsets of the dataset where the correlation between the spurious attribute and task label changes and then it trains a classifier that only uses the attributes with stable correlations across environments (see E for experimental setup).

**Metrics.** We use two metrics for evaluation. Since all datasets have binary task labels and attributes, we define a group-based metric (_average group accuracy_) to measure generalization under distribution shift. Specifically, given binary task label \(y\{0,1\}\) and spurious attribute \(a\{0,1\}\). Following Kumar et al.  we define \(2 2\) groups, one for each combination of \((y,a)\). The subset of the dataset with \((y=1,a=1)\) and \((y=0,a=0)\) are the majority group \(S_{maj}\) while groups \((y=1,a=0)\) and \((y=0,a=1)\) make up the minority group \(S_{min}\). We expect the main classifier to exploit this correlation and hence perform better on \(S_{maj}\) but badly on \(S_{min}\) where the correlation breaks. Thus we want a method that performs better on the average accuracy on both the groups i.e \()+Acc(S_{maj})}{2}\), where \(Acc(S_{maj})\) and \(Acc(S_{min})\) are the accuracy on majority and minority group respectively. The second metric (\(\)Prob) measures the reliance of a classifier on the spurious feature. For every given input \(_{a}\) we have access to the counterfactual distribution \((_{a^{}})(_{a})\) (SS2) where the attribute \(A=a\) is changed to \(A=a^{}\). \(\)Prob is defined as the change in the prediction probability of the model on changing the spurious attribute \(a\) in the input, thus directly measuring the reliance of the model on the spurious attribute. For background on baselines refer SSA a detailed description of our experiment setup refer SSE.

### Evaluating Stage 1: Automatic Detection of Spurious Attributes

**Failure of Mouli+CAD in detecting the spurious attributes at high correlation.** In Fig. 3, we test the effectiveness of Mouli+CAD to detect the subset of attributes which are spurious on different datasets with varying levels of spurious correlation (\(\)). In Syn-Text dataset, at low correlations (\(<0.8\)) Mouli+CAD correctly detects _spurious_ attribute (orange line is lower than blue). As the correlation increases, their method incorrectly states that there is no spurious attribute (blue line lower than orange). In MNIST34 dataset, Mouli+CAD does not detect any attributes as spurious (shown by blue line for all \(\)). For Twitter-AAE dataset, Mouli+CAD's method is correctly able to detect the spuriously correlated attribute (_race_) for all the values of predictive correlation, perhaps because the spurious correlation is weak compared to the causal relationship from causal features to task label.

Figure 3: **Detecting spurious attribute using Mouli+CAD: x-axis is the predictive correlation and y-axis shows the score defined by Mouli+CAD. The orange curve shows the score for true spurious attribute. The blue curve shows the score for setting when Mouli+CAD considers none of the given attributes as spurious. The attribute with the lowest score is selected as spurious. If the blue curve has the lowest score for a dataset with predictive correlations (\(\)), Mouli+CAD will not declare any attributes as spurious for that \(\). (a) and (b). Mouli+CAD fails to detect the spurious attribute at high \(\). (c) Mouli+CAD correctly identifies the race attribute as spurious.**

AutoACER is robust to error in the estimation of spurious attributes. Unlike Mouli+CAD's method that does a hard categorization, AutoACER estimates the causal effect of every attribute on the task label as a fine-grained measure of whether an attribute is spurious or not. Table 1 summarizes the estimated treatment effect of spurious attributes in every dataset for different levels of predictive correlation (\(\)). We use two different causal effect estimators named _Direct_ and _Riesz_ with the best estimate selected using validation loss (see SSE). Overall, the Riesz estimator gives a better or comparable estimate of the causal effect of spurious attribute than Direct, except in the Syn-Text-Unobs-Conf dataset where the causal effect is not identified. At high predictive correlation(\(>=0.9\)), as expected, the causal effect estimates are incorrect. But as we will show next, since AutoACER uses a continuous effect value to detect the spurious attribute, it allows for errors in the first (detection) step to not affect later steps severely.

### Evaluating Stage 2: Evaluation of AutoACER and other baselines

Fig. 4 compares the efficacy of our method with other baselines in removing the model's reliance on spurious attributes. On **Syn-Text-Unobs-Conf** dataset, our method (AutoACER) performs better than all the other baselines for all levels of predictive correlation (\(\)) on _average group accuracy_

    & & &  \\  Dataset & True & Method & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & 0.99 \\   &  & Direct & **0.00** & **0.00** & **0.01** & 0.08 & 0.16 & 0.22 \\  & & Riesz & **0.00** & 0.03 & 0.03 & **0.03** & **0.05** & **0.15** \\   &  & Direct & **-0.01** & **0.13** & **0.24** & **0.37** & **0.49** & **0.67** \\  & & Riesz & 0.06 & 0.19 & 0.31 & 0.42 & 0.58 & 0.70 \\   &  & Direct & 0.02 & **0.03** & **0.05** & 0.06 & **0.15** & **0.27** \\  & & Riesz & **-0.01** & 0.06 & **0.05** & **0.04** & 0.2 & 0.31 \\   &  & Direct & **0.01** & **0.07** & **0.12** & 0.20 & **0.27** & **0.31** \\  & & Riesz & 0.02 & **0.07** & **0.12** & **0.17** & **0.27** & 0.37 \\   

Table 1: **Causal Effect Estimate of spurious attribute. _Direct_ and _Riesz_ are two different causal effect estimators as described in §3.2. Overall we see that _Riesz_ performs better or comparable to the baseline _Direct_ and closer to ground truth value \(0\). Since Twitter-AAE is a real dataset we don’t have a ground causal effect of _race_ attribute but we expect it to be zero (see §4.1).**

Figure 4: **Average group accuracy (top row) and \(\)Prob (bottom row) for different methods. x-axis denotes predictive correlation (\(\)) in the dataset. Compared to other baselines, (a) and (b) show that AutoACER performs better or comparable in average group accuracy and trains a classifier significantly less reliant to spurious attribute as measured by \(\)Prob. In (a), IRM has lower \(\)Prob as compared to other methods but performs worse on average group accuracy. Mouli+CAD performs better in average group accuracy in (c) but relies heavily on spurious attribute shown by high \(\)Prob whereas AutoACER performs equivalent to all other baselines on average group accuracy with lowest \(\)Prob among all.**

(first row in Fig. 3(a)). In addition, \(\)Prob (the sensitivity of model on changing spurious attribute in input, see 4.1) for our method is the lowest compared to other baselines (see bottom row of Fig. 3(a)). For \( 0.8\), Mouli+CAD is the same as ERM since it fails to detect the spurious attribute and thus doesn't impose invariance w.r.t the spurious attribute. (Fig. 2(a) for details). On **MNIST34** dataset, the average group accuracy of all methods is comparable (except JTT that performs worse), but AutoACER has a substantially lower \(\)Prob than baselines for all values of \(\). Again, the main reason why Mouli+CAD fails is that it is not able to detect the spurious attribute for all \(\) and thus doesn't impose invariance w.r.t them (see Fig. 2(b) and SS4.3). On **Twitter-AAE** dataset, Mouli+CAD correctly detects the _race_ attribute as spurious and performs better in terms of Average Group Accuracy compared to all other baselines. But if we look at \(\)Prob, the gain in accuracy is not because of better invariance: in fact, the reliance on the spurious attribute (race) is worse than ERM. In contrast, AutoACER has lowest \(\)Prob among all while obtaining comparable accuracy to ERM, JTT and IRM. To summarize, in all datasets, AutoACER ensures a higher or comparable accuracy to ERM while yielding the lowest \(\)Prob. Note that AutoACER does not require knowledge of the spurious attribute whereas JTT and IRM do; hence we selected the best model using overall accuracy on the validation set for all the methods (see Eq. 33). For a fair comparison to JTT and IRM, in SSF.2 (Fig. 7) we show results under the setting where all the methods (including AutoACER) assume that the spurious attribute is known. Even with access to this additional information, we observe a similar trend as in Fig. 4. AutoACER performs equivalent to all other baselines on average group accuracy with the lowest \(\)Prob among all. Finally, in SSF we provide an in-depth analysis of our results, provide recommendations for selecting among different causal effect estimators used by our method (SSF.1) and evaluate AutoACER and other baselines on an additional dataset --Civil-Comments --a popular dataset for studying generalization under spurious correlation (SSF.2).

## 5 Related Work

**Known spurious attributes.** When the spuriously correlated attributes are known a priori, three major types of methods have been proposed, based on worst-group optimization , conditional independence constraints [35; 15; 22], or data augmentation [16; 17]. Methods like GroupDRO  create multiple groups in the training dataset based on the spurious attribute and optimize for worst group accuracy. To reduce labelling effort for the spurious attribute, methods like JTT  and EIIL  improve upon GroupDRO by requiring the spurious attribute labels only on the validation set. Other methods assume knowledge of causal graphs and impose conditional independence constraints for removing spurious attributes [35; 15; 22]. Methods based on data augmentation add counterfactual training data where only the spurious attribute is changed (and label remains the same) [16; 14; 17].

**Automatically discovering spurious attributes.** The problem becomes harder if spurious attributes are not known. Mouli and Ribeiro's work [24; 25] provides a method assuming specific kinds of transformations on the spurious attributes, either transformations that form finite linear automorphism groups  or symmetry transformations over equivalence classes . Any attribute changed via the corresponding transformation that does not hurt the training accuracy is considered spurious. However, they do not consider settings with correlation between the transformed attributes and the task labels. Our work considers a more realistic setup where we don't impose any constraint on the transformation or the attribute values, and allows attributes to be correlated with the task label (at different strengths). Using conditional independencies derived from a causal graph, Zheng and Makar  propose a method to automatically discover the spurious attributes under the anti-causal classification setting. Our work, considering the _causal_ classification setting (features cause label) complements their work and allows soft regularization proportional to the causal effect.

## 6 Limitations and Conclusion

We presented a method for automatically detecting and removing spurious correlations while training a classifier. While we focused on spurious attributes, estimation of causal effects can be used to regularize the effect of non-spurious features too. That said, our work has limitations: we can guarantee the identification of causal effects only in certain DGPs. In future work, it will be useful to characterize the datasets on which our method is likely to work and where it fails. Another limitation is that our method is not guaranteed to work on datasets with an anti-causal data-generating process, wherein the observed input \(\) and other attributes are generated from the task label \(y\).