# Replay-and-Forget-Free Graph Class-Incremental Learning: A Task Profiling and Prompting Approach

Chaoxi Niu\({}^{1}\), Guansong Pang\({}^{2}\), Ling Chen\({}^{1}\), Bing Liu\({}^{3}\)

\({}^{1}\) AAII, University of Technology Sydney, Australia

\({}^{2}\) School of Computing and Information Systems, Singapore Management University, Singapore

\({}^{3}\) Department of Computer Science, University of Illinois at Chicago, USA

chaoxi.niu@student.uts.edu.au, gspang@smu.edu.sg

ling.chen@uts.edu.au, liub@uic.edu

Corresponding author: G. Pang (gspang@smu.edu.sg)

###### Abstract

Class-incremental learning (CIL) aims to continually learn a sequence of tasks, with each task consisting of a set of unique classes. Graph CIL (GCIL) follows the same setting but needs to deal with graph tasks (_e.g._, node classification in a graph). The key characteristic of CIL lies in the absence of task identifiers (IDs) during inference, which causes a significant challenge in separating classes from different tasks (_i.e._, _inter-task class separation_). Being able to accurately predict the task IDs can help address this issue, but it is a challenging problem. In this paper, we show theoretically that accurate task ID prediction on graph data can be achieved by a Laplacian smoothing-based graph task profiling approach, in which each graph task is modeled by a task prototype based on Laplacian smoothing over the graph. It guarantees that the task prototypes of the same graph task are nearly the same with a large smoothing step, while those of different tasks are distinct due to differences in graph structure and node attributes. Further, to avoid the _catastrophic forgetting_ of the knowledge learned in previous graph tasks, we propose a novel _graph prompting_ approach for GCIL which learns a small discriminative graph prompt for each task, essentially resulting in a separate classification model for each task. The prompt learning requires the training of a single graph neural network (GNN) only once on the first task, and no data replay is required thereafter, thereby obtaining a GCIL model being both **replay-free** and **forget-free**. Extensive experiments on four GCIL benchmarks show that i) our task prototype-based method can achieve 100% task ID prediction accuracy on all four datasets, ii) our GCIL model significantly outperforms state-of-the-art competing methods by at least 18% in average CIL accuracy, and iii) our model is fully free of forgetting on the four datasets. Code is available at https://github.com/mala-lab/TPP.

## 1 Introduction

Graph continual learning (GCL) [5; 28; 43; 39] aims to continually learn a model that not only accommodates the new emerging graph data but also maintains the learned knowledge of previous graph tasks, with each graph task comprising nodes from a set of unique classes in a graph. Due to privacy concerns and the hardware limitations in storage and computation, GCL assumes that the data of previous graph tasks is not accessible when learning new graph tasks. This leads to _catastrophic forgetting_ of the learned knowledge, _i.e._, degraded classification accuracy on previous tasks due to model updating on new tasks.

_Graph class-incremental learning_ (GCIL) is one key setting of GCL, in which task identifiers (IDs) are not provided during inference. _Graph task-incremental learning_ (GTIL) is another GCL setting where the task ID is given for each test sample. As a result, a set of separate classifiers can be learned for different graph tasks in GTIL and the task-specific classifier can be used for each test sample. Compared to GTIL, the absence of task IDs in GCIL presents an additional challenge, known as _inter-task class separation_, _i.e._, the class separation in one graph task is obstructed by the presence of classes from other tasks. Consequently, the classification performance in GCIL is typically far below that in GTIL . This paper focuses on the GCIL setting - a more compelling GCL problem - aiming to bridge the performance gap between GCIL and GTIL.

Existing GCL methods often alleviate the catastrophic forgetting through preserving important model parameters of previous tasks , continually expanding model parameters for new tasks , or augmenting with a memory module for data replay . However, their ability to handle the inter-task class separation is limited, leading to poor GCIL classification accuracy, especially the accuracy of the previous graph tasks. Thus, existing GCL methods typically show substantially higher forgetting in GCIL than in GTIL.

To address these issues, we introduce a novel GCIL approach, namely **Task Profiling and Prompting (TPP)**. In particular, we reveal for the first time theoretically and empirically that the task ID of a test sample can be accurately predicted by using a Laplacian smoothing-based method to profile each graph task with a prototypical embedding. With the existence of edges between nodes, this task profiling method guarantees that the task prototypes of the same graph task are nearly the same with a large smoothing step, while those of different tasks are distinct due to differences in graph structure and node attributes. High task ID prediction accuracy helps confine the classification space of the test samples to the classes of the predicted task (_e.g._, Task 1 or 2 in Fig. 1b,c) instead of all the learned tasks (_e.g._, both tasks as in Fig. 1a), eliminating the inter-task class separation issue. There have been some studies on task ID prediction , but they are designed for Euclidean data that are i.i.d. (independent and identically distributed). As a result, they fail to leverage the graph structure and node attributes in the non-Euclidean graph data and are not suited for graph task ID prediction.

To address the catastrophic forgetting problem, we further propose a novel graph prompting approach for GCIL. Specifically, we optimize a single, small learnable prompt using a simple frozen pre-trained graph neural network (GNN) to capture the task-specific knowledge for each graph task during training. Despite being small, the task-specific knowledge learned in the prompts can ensure the intra-task class separation, as shown in Fig. 1d,e. At test time, given a test graph, the task prototype constructed with its structure and node attributes is utilized for task ID prediction, and the graph prompt of the predicted task is incorporated into the test graph for classification with the GNN. Since the graph prompts are learned task by task, _no data replay_ is required in our TPP model. Further, the graph prompts are task-specific, so we essentially have a separate classification model for each graph task, _i.e._, no continual model updating, completely avoiding the forgetting problem (_i.e._, _forget-free_).

Overall, this work makes the following main contributions. **(1)**: We propose a novel graph task profiling and prompting (TPP) approach, which is the first replay- and forget-free GCIL approach. **(2)**: We reveal theoretically that a simple Laplacian smoothing-based graph task profiling approach can achieve accurate graph task ID prediction. To the best of our knowledge, it is the first work that leverages the non-Euclidean properties of graph data to enable graph task ID prediction. It achieves

Figure 1: **(a) Classification space of two graph tasks when no task ID is provided. The classification space is split into two separate spaces in Task 1 in (b) and Task 2 in (c) when the task ID can be accurately predicted. This helps alleviate the inter-task class separation issue. To mitigate catastrophic forgetting, we learn a graph prompt for each task that absorbs task-specific discriminative information for better class separation within each task, as shown in (d) and (e) respectively. This essentially results in a separate classification model for each task, achieving fully forget-free GCIL models.**

100% prediction accuracy across all four datasets used, eliminating the inter-task class separation issue in GCLL. **(3)**: We further introduce a novel graph prompting approach that learns a small prompt for each task using a frozen pre-trained GNN, without any data replay involved. With the support of our accurate task ID prediction, the graph prompts result in a separate classification model for each task, resulting in the very first GCLL model being both replay-free and forget-free. **(4)**: Extensive experiments on four GCL benchmarks show that our TPP model significantly outperforms state-of-the-art competing methods by at least 18% in average CIL accuracy while being fully forget-free. It even exceeds the joint training on all tasks simultaneously by a large margin.

## 2 Related Work

**Graph Continual Learning.** Various methods have been proposed for GCL [7; 15; 21; 22; 24; 25; 31; 37; 38; 41; 42; 44] and can be divided into three categories, _i.e._, regularization-based, parameter isolation-based, and data replay-based methods. Regularization-based methods typically preserve parameters that are important to the previous tasks when learning new tasks via additional regularization terms. For example, TWP  preserves the important parameters in the topological aggregation and loss minimization for previous tasks via regularization terms. Parameter isolation-based methods maintain the performance on previous tasks by continually introducing new parameters for new tasks such as  proposes to continually expand model parameters to learn new emerging graph patterns. Differently, replay-based methods [17; 20; 41; 42; 44] employ an additional memory buffer to store the information of previous tasks and replay them when learning new tasks. The ways to construct the memory buffer play a vital role in replay-based methods. Despite they have shown good performance in alleviating the forgetting problem, inter-class separation is still a significant challenge to these methods, especially for the CIL setting where the task IDs are not provided when testing.

To improve the CIL performance, an emerging research direction focuses on performing task ID prediction during testing. For example, CCG  utilizes a separate network for task identification. HyperNet  and PR-Ent  use the entropy to predict the task of the test sample. More recently,  proves that OOD detection is the key to task ID prediction and proposed an identification method based on an OOD detector. TPL  further improves it by exploiting the information available in CIL for better task identification. Since these methods were designed for Euclidean data, they are not suited for GCL. Following this line, we propose a task ID prediction method specifically for GCIL in this paper. Different from previous methods that rely on additional networks or OOD detectors, the proposed task identification is accomplished by using a Laplacian smoothing-based method to profile each graph task with a prototypical embedding. Despite its simplicity, this graph task profiling method can achieve accurate task ID prediction with theoretical support.

**Prompt Learning.** Originating from natural language processing, prompt learning aims to facilitate the adaptation of frozen large-scale pre-trained models to various downstream tasks by introducing learnable prompts . In other words, prompt learning designs task-specific prompts to instruct the pre-trained models to perform downstream tasks conditionally. The prompts capture the knowledge of the corresponding tasks and enhance the compatibility between inputs and pre-trained models. Recently, prompt-based graph learning methods have also been proposed , which aims to unify multiple graph tasks  or improve the transferability of graph models . Due to the ability to leverage the strong representative capacity of the pre-trained model and learn the knowledge of tasks in prompts, many prompting-based continual learning methods have been proposed [32; 33; 34] and achieved remarkable success without employing replaying memory or regularization terms. Despite that, no work is done on prompt learning for GCLL. The main challenge is the lack of pre-trained GNN models for all tasks in GCIL and the absence of task IDs to retrieve corresponding prompts during testing. In this work, we show that effective graph prompts can be learned for different tasks using a GNN backbone trained based on the first task with graph contrastive learning [36; 45], and our task ID prediction method and the graph prompts can be synthesized to address the GCIL problem.

## 3 Methodology

### The GCIL Problem

Formally, GCL can be formulated as learning a model on a sequence of connected graphs (tasks) \(\{^{1},,^{T}\}\) where \(T\) is the number of tasks. Each \(^{t}=(A^{t},X^{t})\) is a newly emerging graph,where \(A^{t}\) denotes the relations between \(N\) nodes of the current/new task, \(X^{t}^{N F}\) represents the node attributes with dimensionality of \(F\), and the labels of nodes can be denoted as \(Y^{t}\). Each task contains a unique set of classes in a graph, i.e., \(\{Y^{t} Y^{j}=|t j\}\). When learning task \(t\), the model trained from previous tasks only has access to the current task data \(^{t}\). The goal is to accommodate the model to current graph \(^{t}\) while maintaining the classification performance on the previous graphs \(\{^{1},,^{t-1}\}\). In GICIL, the task IDs are not available during inference. Thus, assuming that each task has \(C\) classes, after learning all tasks, a GCIL model is required to classify a test instance into one of all the \(T C\) classes.

### Overview of The Proposed TPP Approach

Inspired by prior studies [10; 11], we decompose the class probability of a test sample \(^{}\) belonging to the \(j\)-th class in task \(t\) in GCIL into two parts :

\[H(y^{t}_{j}|^{})=H(y^{t}_{j}|^{},t) H(t|^{})\,,\] (1)

where \(H(t|^{})\) represents the task ID prediction probability of task \(t\) and \(H(y^{t}_{j}|^{},t)\) denotes the prediction within the task \(t\). This indicates that accurate GCIL classification accuracy can be achieved when both accurate task ID prediction and intra-task class classification are achieved.

To this end, in this paper, we propose the Task Profiling and Prompting (**TPP**) approach for GCIL. As shown in Fig. 2, a novel Laplacian smoothing-based task profiling approach is first devised in TPP for graph task ID prediction, which can well guarantee the task prediction accuracy as we will demonstrate theoretically below. Moreover, to obtain accurate intra-task class classification within the identified task, a novel graph prompting approach is further proposed to learn a small prompt for each task using a frozen GNN pre-trained on the first graph task. By learning and storing task knowledge separately, there is no knowledge interference between tasks during training, resulting in a model being both replay-free and forget-free. During inference, given a test sample, TPP first performs the task ID prediction and then retrieves the corresponding task graph prompt to concatenate with the sample for the GCIL classification. Below we introduce the TPP approach in detail.

### Laplacian Smoothing-based Task Profiling for Graph Task ID Prediction

To leverage the graph structure and node attribute information, we propose to use a Laplacian smoothing approach to generate a prototypical embedding for each graph task for task ID prediction. Specifically, for the task \(t\) with graph data \(^{t}=(A^{t},X^{t})\), we construct a task prototype \(^{t}\) for this task based on the train set denoted as \(\{x_{i}|i^{t}_{}\}\), where \(^{t}_{}\) is the train set of \(^{t}\). Given \(^{t}\), the Laplacian smoothing is first applied on the graph \(_{t}\) to obtain the smoothed node embeddings \(Z^{t}\):

\[Z^{t}=(I-(^{t})^{-}^{t}(^{t})^{-})^ {s}X^{t}\,,\] (2)

where \(s\) denotes the number of Laplacian smoothing steps, \(I\) is an identity matrix, and \(^{t}\) is the graph Laplacian  matrix of \(^{t}=A^{t}+I\) (_i.e._, \(^{t}=^{t}-^{t}\) with \(^{t}\) being the diagonal degree matrix

Figure 2: Overview of the proposed TPP approach. During training, for each graph task \(t\), the task prototype \(^{t}\) is generated by applying Laplacian smoothing on the graph \(^{t}\) and added to \(=\{^{1},,^{t-1}\}\). At the same time, the graph prompt \(^{t}\) and the classification head \(^{t}\) for this task are optimized on \(^{t}\) through a frozen pre-trained GNN. During inference, the task ID of the test graph is first inferred (_i.e._, task identification). Then, the graph prompt and the classifier of the predicted task are retrieved to perform the node classification in GCIL. The GNN is trained on \(^{1}\) and remains frozen for subsequent tasks.

of \(^{t}\) and \(^{t}_{ii}=_{j}_{ij}\)). Then, the task prototype \(^{t}\) is constructed by averaging the smoothed embeddings of train nodes:

\[^{t}=^{t}_{}|}_{i^ {t}_{}}^{t}_{i}(^{t}_{ii})^{-}\,.\] (3)

Similarly, the task prototypes for all tasks can be separately constructed and stored, denoted as \(=\{^{1},,^{T}\}\). Given a test graph \(^{}\) at testing time, we predict the task ID of \(^{}\) by querying the task prototype pool \(\). Specifically, the task prototype of \(^{}\) is obtained with the set of test nodes in a similar way as on training graphs via Eq. (2) and Eq. (3), _i.e._,

\[^{}=^{}|}_{i ^{}}^{}_{i}(^{}_ {ii})^{-}\,,\] (4)

where \(^{}\) denotes the set of nodes to be classified in \(^{}\) and \(^{}_{i}\) is the smoothed embedding of the test node \(i\) after \(s\)-step Laplacian smoothing. Then, we query the task prototype pool \(\) with the test prototype \(^{}\) and return the task ID whose task prototype is most similar to \(^{}\):

\[t^{}=(d(^{},^{1}),,d( ^{},^{T})))\,,\] (5)

where \(d()\) represents an Euclidean distance function and \(t^{}\) is the predicted task ID of \(^{}\).

As discussed in Sec. 3.2, more accurate task ID prediction leads to better classification performance for GCIL. Below we show theoretically that the task ID of the test graphs can be accurately predicted with our simple Laplacian smoothing-based task profiling approach.

**Theorem 1**.: _If graphs for all tasks are not isolated and the test graph \(^{}\) comes from the task \(t\), i.e., \(^{}\) and \(^{t}\) have the same set of classes, then the distance between \(^{}\) and \(^{t}\) approaches to zero with a sufficiently large number of Laplacian smoothing steps \(s\):_

\[_{s+}d(^{},^{t})=0\,.\] (6)

**Theorem 2**.: _Suppose the test graph comes from task \(t\), and let \(\) and \(\) be the differences in node degrees and node attributes between two different tasks \(t\) and \(j\) respectively, which are defined by \((^{j})^{}=(^{t})^{}+()\) and \(X^{j}=X^{t}+\). Then the distance between the task prototypes of task \(t\) and \(j\) obtained with large steps of Laplacian smoothing can be explicitly calculated as:_

\[d(^{},^{j})=\|(^{t}_{N})^{T}+ ()^{T}X^{j}\|_{2}\,,\] (7)

_where \(^{t}_{N}=(^{t})^{}[1,1,,1]^{T}\) is the \(N\)-th eigenvector of task \(t\) and \((^{t}_{N})^{T}\) denotes its transpose._

The two theorems indicate that i) if the test graph belongs to task \(t\), with a large \(s\), the distance between \(^{}\) and \(^{t}\) would become zero with the proposed Laplacian smoothing and prototype construction method (Theorem 1); and ii) for graphs from different tasks, since they contain different set of classes, they have large differences in graph structure and node attributes, which can lead to a large distance between task prototypes \(^{t}\) and \(^{j}\) (Theorem 2), thereby having the following inequality hold if \(^{}\) comes from task \(t\):

\[d(^{},^{t})<d(^{},^{j})\,.\,\,\, j t\,.\] (8)

Without loss of generality, we empirically investigate the differences between two randomly chosen graph tasks of the CorFull dataset in Fig. 3. We can see that the two graphs of the tasks have a rather large difference in both graph structure and node attributes. The larger the differences in \(\) and \(\) of the two graphs, the larger the gap is between \(d(^{},^{t})\) and \(d(^{},^{j})\). As a result, the task of the test graph can be predicted accurately with Eq. (5). In the experimental section, we empirically evaluate the proposed task ID prediction and report its accuracy on different datasets.

Figure 3: The differences between two graphs in structure and node attributes.

### Graph Prompt Learning for GCIL

Instead of utilizing regularization or replaying memory as in existing GCL methods, TPP aims to learn a task-specific prompt for each graph task. The information of each graph task can be explicitly modeled and stored in a separate task-specific graph prompt, with the GNN backbone being frozen. This effectively avoids the forgetting of knowledge of any previous tasks and the interference between tasks. To this end, the graph prompt in TPP is designed as a set of learnable tokens that can be incorporated into the feature space of the graph data for each task. Specifically, for a task \(t\), the graph prompt can be represented as \(^{t}=[_{1}^{t},,_{k}^{t}]^{T}^{k F}\) where \(k\) is the number of vector-based tokens \(^{i}\). For each node in \(^{t}\), the node attribute is augmented by the weighted combination of these tokens, with the weights obtained from \(k\) learnable linear projections:

\[}_{i}^{t}=_{i}^{t}+_{j}^{k}_{j}_{j}^{t }\,,\,\,\,_{j}=_{j})^{T}_{i}^{t}}}{_{l }^{k}e^{(_{l})^{T}_{i}^{t}}}\,,\] (9)

where \(_{j}\) denotes the importance score of the token \(^{j}\) in the prompt and \(_{j}\) is a learnable projection. For convenience, we denote the graph modified by the graph prompt as \(}^{}=(A^{t},X^{t}+^{t})\). Then, \(}^{t}\) is fed into a frozen pre-trained GNN model \(f()\) to obtain the embeddings for classification. In TPP, we employ a single-layer MLP as the classifier attached to the GNN, denoting \(^{t}\) for task \(t\). The node classification at task \(t\) can be formulated as:

\[^{t}=^{t}(f(A^{t},X^{t}+^{t})).\] (10)

Therefore, the graph prompt and the MLP-based classification head are optimized by minimizing a node classification loss:

\[_{^{t},^{t}}_{}^{t}|}_{i _{}^{t}}_{}(_{i}^{t},y_{i}^{t})\,,\] (11)

where \(_{}^{t}\) is the train set of \(^{t}\), \(y_{i}^{t}\) is the label of node \(i\), \(_{i}^{t}^{t}\) is the predicted label, and \(_{()}\) is a cross-entropy loss. By minimizing Eq. (9), the graph prompt and the classifier are learned to leverage the generic, cross-task knowledge in the frozen GNN \(f()\) for the task \(t\). Meanwhile, \(^{t}\) and \(^{t}\) learn specific knowledge for the task \(t\). This essentially results in a separate classification model for each task, and no data replay is required for all tasks. As a result, TPP is fully free of catastrophic forgetting for GCIL. An alternative approach to overcoming the forgetting is to learn a separate GNN model for each task. However, this would introduce heavy burdens on optimization and storage with the increasing number of tasks. By contrast, the proposed graph prompting only introduces minimal parameters for each task as the prompts are very small.

### Training and Inference in TPP

**Training.** The training of TPP can be divided into two parts. First, for each task \(^{t}\), the prototypical embedding \(^{t}\) is generated based on Laplacian smoothing and stored in \(\) for task ID prediction. Then, the information of \(^{t}\) is explicitly modeled and stored with the proposed graph prompt learning, _i.e._, Eq. (11). For the GNN backbone \(f()\) in graph prompt learning, we propose to learn it based on the first task \(^{1}=(A^{1},X^{1})\) via graph contrastive learning due to its ability to obtain transferable models  across graphs (see Appendix B). Despite being only learned on \(^{1}\), \(f()\) can effectively adapt to all subsequent tasks with graph prompts. Overall, after learning all tasks in \(\{^{1},,^{T}\}\), the task profiles and task-specific information are explicitly modeled in \(=\{^{1},,^{T}\}\), \(\{^{1},,^{T}\}\) and \(\{^{1},,^{T}\}\).

**Inference.** Given the test graph \(^{}\), the task prototype \(^{}\) is constructed with Eq. (4) and then used to obtain the task ID \(t^{}\) by querying \(=\{^{1},,^{T}\}\), _i.e._, via Eq. (5). Finally, the test graph \(^{}\) is augmented with the corresponding graph prompt \(^{t^{}}\) and fed into the GNN and the classification head constructed with \(f()\) and \(^{t^{}}\) respectively to get the node classification results. Formally, the inference can be formulated as:

\[t^{}=(d(^{},^{1} ),,d(^{},^{T})))\,,\\ Y^{}=^{t^{}}(f(A^{},X^{}+ ^{t^{}}))\,.\] (12)

The algorithms of the training and inference of TPP are provided in Appendix C.

[MISSING_PAGE_FAIL:7]

data generally do not achieve satisfactory performance for GCIL, which verifies the fact that the unique graph properties should be taken into consideration for GCIL. (3) Replay-based methods generally achieve much better performance than the other baselines, showing the effectiveness of using an external memory buffer to overcome catastrophic forgetting. However, all of them still suffer from forgetting, in addition to the inter-task separation issue. (4) The performance of OODCIL demonstrates that despite achieving impressive AF performance, current OOD detection-based CIL methods are not effective for GCIL due to the overlook of graph properties in its OOD detector and classification model. (5) Different from the baselines that involve the forgetting problem to varying extents, the proposed method TPP is a fully forget-free GCIL approach, achieving an AF value of zero across all four datasets. TPP is also consistently the best performer in AA, outperforming the best-competing method by over 18% in AA averaged over the four datasets. This superiority is attributed to the highly accurate task ID prediction module in TPP and its effective task-specific graph prompt learning (see Sec. 4.2). (6) Our method lifts the SOTA AA performance by a large margin and even significantly outperforms the oracle baseline Joint in all cases. This is because although the Joint method can mitigate catastrophic forgetting due to its access to the data of all graphs, it is still challenged by the inter-task class separation issue since it is not given task ID during inference. TPP effectively tackles both catastrophic forgetting and inter-task class separation issues, thus achieving significantly better AA than Joint and very comparable AA to the Oracle Model.

**Enabling Existing GCIL Methods with Our Task ID Prediction Module.** Existing GCIL Methods often suffer from a severe inter-task class separation issue. Our task ID prediction is devised as a module to tackle this issue. To show its effectiveness as an individual plug-and-play module, we evaluate its performance when combined with existing GCIL methods. Our task ID prediction method does not change the training process of existing GCIL models. It is directly incorporated into them at the inference stage only, _i.e._, our task ID predictor produces a task ID for each test sample and the existing GCIL models then perform intra-task classification in the predicted task. Without loss of generality, a parameter regularization-based method (TWP ) and a memory-replay method (DeLoMe ) are used as exemplars for this experiment. The results are shown in Table 2. We can see that both AA and AF performance of the two existing GCIL models are largely enhanced by the proposed task identification module. For relatively weak GCIL models like TWP, the improvement is much more substantial than the strong ones like DeLoMe. The reason is that being able to predict the task ID accurately enables the subsequent CIL classification within the original task space of the test graph, not the space containing all the learned classes, significantly simplifying (reducing) the classification space. Essentially, such a task ID prediction converts the GCIL task into the GTIL task, so that much better AA and AF results are expected.

### Ablation Study

**Importance of Task ID Prediction.** In GCIL, the test samples are required to be classified into one of all the learned classes. To evaluate the importance of task ID prediction that helps confine the classification space of the test samples to the classes of the predicted task, we conduct the experiments of TPP without the proposed task profiling approach and report the results in Table 3. Specifically, we obtain the class probabilities of the test sample for all tasks and prompts, and the class with the highest probability is treated as the class for the test sample. As shown in the table, this TPP variant can barely work on all four datasets. This is mainly due to that the graph prompts are learned task by task during training. Without the guidance of task identification, the non-normalized within-task prediction probabilities obtained with corresponding prompts pose great challenges for classifying the test samples into the correct classes.

**Importance of Graph Prompting.** Besides the task ID prediction, we also evaluate the importance of graph prompting. There are two modules for each task in the proposed graph promoting, _i.e._, graph prompt \(^{t}\) and classification head \(^{t}\). The results with and without each module are shown in

   &  &  &  &  \\   & AA/\%\(\) & AF/\%\(\) & AA/\%\(\) & AF/\%\(\) & AA/\%\(\) & AF/\%\(\) & AA/\%\(\) & AF/\%\(\) \\  TWP & 62.6+2.2 & -30.6+4.3 & 6.7+1.5 & -50.6+13.2 & 8.0+5.2 & -18.8+9.0 & 14.1+4.0 & -11.4+2.0 \\ +TP & 94.3+0.9 & -1.6+0.4 & 89.4+0.4 & 0.0 \(\)0.3 & 78.0+18.5 & -0.2+0.4 & 81.8+3.3 & -0.3+0.8 \\  DeLoMe & 81.0+0.2 & -3.3+0.3 & 50.6+0.3 & 5.1+0.4 & 97.4+0.1 & -0.1+0.1 & 67.5+0.7 & -17.3+0.3 \\ +TP & 95.4+0.1 & 2.0+0.6 & 90.4\(\)0.3 & -1.1+0.2 & 99.4+0.0 & -0.1+0.0 & 94.8+0.1 & -2.2+0.2 \\  

Table 2: AA and AF results of enabling existing GCIL methods with our task ID prediction (TP).

Table 3. We can see that the TPP variant without both \(^{t}\) and \(^{t}\), which is equivalent to the direct use of the GNN backbone learned only from the first task for all subsequent tasks, achieves the worst AA performance, though it is free of forgetting since there is no model updating. By incorporating either \(^{t}\) or \(^{t}\), the performance can be largely improved, which can be attributed to the transferable knowledge in the pre-trained GNN \(f\) and the effective adaptation of the prompts or the classifier to the subsequent tasks. Note that the variant with only \(^{t}\) obtains much better performance than that with only \(^{t}\), demonstrating that the learned graph prompts can more effectively model the task-specific information and bridge the gap between the first task and subsequent tasks. The results also explain the visualization of node embeddings with and without the graph prompt in Fig. 1, where the graph prompt can largely enhance the intra-task separation. By integrating all the components, the full TPP model achieves the best performance across all datasets.

**Sensitivity w.r.t the Size of Graph Prompts.** We evaluate the sensitivity of the proposed method w.r.t the size of the graph prompt, _i.e._, the number of tokens per prompt. We vary \(k\) in the range of \(\) to verify the sensitivity and report the results in Fig. 4(a). It is clear that the performance of TPP increases quickly from \(k=1\) to \(k=2\) and remains stable when \(k>2\), demonstrating that TPP can be effectively adapted to different tasks with a small size of prompt for each task. This also demonstrates the transferability of the learned GNN backbone for all tasks.

**Accuracy of Task ID Prediction.** We further evaluate the accuracy of the proposed task ID prediction method. We compare it to a variant of our method that constructs the task prototype based on the node attributes without considering the graph structure. In this variant, each task prototype is constructed by simply averaging the attributes of training nodes of each task. The task prototype of a test graph is constructed with the test nodes in the same way. The inference process remains the same as the proposed method. The results of these two methods are shown in Fig. 4(b). We see that the task identification sorely based on node attributes achieves high accuracy for all datasets and even predicts all of the tasks correctly for Arxiv and Reddit. This is largely attributed to the discriminative node attributes between tasks in these datasets, as demonstrated in Fig. 3. However, it fails to discriminate tasks with similar node attributes. By contrast, the proposed method based on Laplacian smoothing can handle all the cases, resulting in perfect task ID prediction for all tasks and datasets, which builds a strong foundation for superior GCIL performance of TPP.

**Performance of TPP with different task formulations.** For the task formulation, we set each task to contain two different classes of nodes and follow the commonly used task formulation strategy in  to have fair comparisons with the baselines. Specifically, given a graph dataset with several classes, we split these classes into different tasks in numerically ascending order of the original classes, i.e., classes 0 and 1 form the first task, classes 2 and 3 form the second task, and so on. To evaluate the performance of TPP with different task formulations, we further perform the class splitting in two other manners, including numerically descending and random ordering of the two classes per task. In Table 4, we report the average performance of the TPP and the Oracle Model with different task formulations.

Figure 4: **(a) The AA results of TPP w.r.t. the size of the graph prompts. (b) Task ID prediction accuracy on all four datasets using Laplacian smoothing (LS) and its variant based on solely node features (NF).**

  Task ID Prediction &  &  &  &  &  \\   & Prompt & Classification Head & AA/W+ & AF/AF+ & AA/W+ & AF/AF+ & AF/W+ & AF/AF+ & AF/W+ \\  \(\) & \(\) & \(\) & 2.0 & -5.5 & 3.0 & -10.9 & 2.8 & -16.8 & 2.7 & -8.2 \\ ✓ & \(\) & \(\) & 50.7 & 0.0 & 54.0 & 0.0 & 47.4 & 0.0 & 51.8 & 0.0 \\ ✓ & \(\) & \(\) & 73.8 & 0.0 & 76.3 & 0.0 & 98.6 & 0.0 & 90.0 & 0.0 \\ ✓ & \(\) & \(\) & 92.8 & 0.0 & 82.9 & 0.0 & 99.0 & 0.0 & 90.7 & 0.0 \\ ✓ & \(\) & \(\) & 93.4 & 0.0 & 85.4 & 0.0 & 99.5 & 0.0 & 94.0 & 0.0 \\  

Table 3: Results of TPP and its variants on ablating task ID prediction and graph prompting modules.

From the table, we observe that the proposed TPP method can still achieve comparable performance to the Oracle Model with different task formulations, highlighting the robustness and effectiveness of TPP w.r.t. the formulation of individual tasks. Note that the performances of TPP and the Oracle Model both drop on Products with random task formulation. This is attributed to the heavily imbalanced class distribution of Products and the performance is evaluated by the balanced classification accuracy. Specifically, for Products, some classes contain hundreds of thousands of nodes while the number of nodes in some classes is less than 100. The ascending and descending task formulations have a relatively balanced class distribution for each task. However, the random task formulation results in some tasks with heavily imbalanced class distribution. To address this problem, debiased learning is required and we leave it for future research. Please also note that TPP learns the GNN backbone only on the first task and is frozen during the subsequent prompt learning. Different task formulations result in the GNN backbone being learned with different first tasks. The above results also reveal that the proposed graph prompting enables the learned GNN backbone to effectively adapt to all subsequent tasks despite the backbone being learned on different initial tasks.

## 5 Conclusion

This paper proposes a novel approach for GCIL via task profiling and prompting. The absence of task IDs during inference poses significant challenges for GCIL. To address this issue, this paper proposes a novel Laplacian smoothing-based graph task profiling approach for GCIL, where each task is modeled by a task prototype based on Laplacian smoothing over the graph. We prove theoretically that the task prototypes of the same graph task are nearly the same with a large smoothing step and the prototypes of different tasks are distinct due to the differences in graph structure and node attributes, ensuring accurate task ID prediction for GCIL. To avoid catastrophic forgetting and achieve high within-task prediction, we further propose the first graph prompting method for GCIL which is learned to absorb the within-task information into the small task-specific graph prompts. This results in a memory-efficient TPP as i) no memory buffer is required for data replay due to its replay-free characteristic and ii) the graph prompting only requires the training of a single GNN once and a small number of tokens per prompt for each task. Extensive experiments show that TPP is fully forget-free and significantly outperforms the state-of-the-art baselines for GCIL.