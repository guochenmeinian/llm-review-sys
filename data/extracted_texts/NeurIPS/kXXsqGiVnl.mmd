# How Transformers Reason: A Case Study on a Synthetic Propositional Logic Problem

Guan Zhe Hong\({}^{*}\)\({}^{1}\) & Nishanth Dikkala \({}^{2}\) & Emming Luo \({}^{2}\) & Cyrus Rashtchian \({}^{2}\) & Xin Wang \({}^{2}\) & Rina Panigrahy \({}^{2}\)

\({}^{1}\)Purdue University \({}^{2}\) Google Research

hong288@purdue.edu,

{nishanthd, enming, cyroid, wanxin, rinap}@google.com

Work done as a student researcher at Google Research.

###### Abstract

Large language models (LLMs) have demonstrated remarkable performance in tasks that require reasoning abilities. Motivated by recent works showing evidence of LLMs being able to plan and reason on abstract reasoning problems in context, we conduct a set of controlled experiments on a synthetic propositional logic problem to provide a mechanistic understanding of how such abilities arise. In particular, for a decoder-only Transformer trained solely on our synthetic dataset, we identify the specific mechanisms by which a three-layer Transformer solves the reasoning task. In particular, we identify certain "planning" and "reasoning" circuits which require cooperation between the attention blocks to in totality implement the desired reasoning algorithm. To expand our findings, we then study a larger model, Mistral 7B. Using activation patching, we characterize internal components that are critical in solving our logic problem. Overall, our work systemically uncovers novel aspects of small and large transformers, and continues the study of how they plan and reason.

## 1 Introduction

Language models using the transformer architecture  have shown remarkable capabilities on many natural language tasks . Trained with causal language modeling wherein the goal is next-token prediction on huge amounts of text, these models exhibit deep language understanding and generation skills. An essential milestone in the pursuit of models which can achieve a human-like artificial intelligence, is the ability to perform human-like reasoning and planning in complex unseen scenarios. While some recent works using probing analyses have shown that the activations of the deeper layers of a transformer contain rich information about certain mathematical reasoning problems , the question of what mechanisms inside the model enables such abilities remains unclear.

While the study of how transformers reason in general remains a daunting task, in this work, we aim to improve our _mechanistic_ understanding of how a Transformer reason through simple propositional logic problems. For concreteness' sake, consider the following problem:

Rules: A or B implies C. D implies E. Facts: A is true. B is false. D is true.

Question: what is the truth value of C?

An answer with _minimal_ proof is "A is true. A or B implies C; C is true."

The reasoning problem, while simple-looking on the surface, requires the model to perform several actions that are essential to more complex reasoning problems, all without chain of thought (CoT).

Before writing down any token, the model has to first discern the _rule_ which is being queried: in this case, it is "A or B implies C". Then, it needs to rely on the premise variables A and B to the locate the relevant _facts_, and find "A is true" and "B is false". Finally, it needs to decide that "A is true" is the correct one to invoke in its answer due to the nature of disjunction. It follows that, to write down the first token "A", the model already has to form a "mental map" of the variable relations, value assignments and query! Therefore, we believe that this is close to the minimal problem to examine how a model internalizes and plans for solving a nontrivial mathematical reasoning problem where apparent ambiguities in the problem specification cannot be resolved trivially.

To understand the internal mechanisms of how a transformer solves problems resembling the minimal form above, we perform two flavors of experiments. The first is on shallow transformers trained purely on the synthetic propositional logic problems. This enables a fine-grained analysis in a controlled setting. The other set of experiments are on a pre-trained LLM (Mistral-7B), where we primarily rely on activation patching to uncover necessary circuits for solving the reasoning problem, including specialized roles of certain components. At a high level, we make the following discoveries based on our two fronts of analysis:

1. We discover that small transformers, trained purely on the synthetic problem, utilize certain "_routing embeddings_" to significantly alter the information flow of the deeper layers when solving different sub-categories of the reasoning problem. We also characterize the different reasoning pathways: we find that problems querying for reasoning chains involving logical operators typically require greater involvement of all the layers in the model.
2. We uncover properties of the circuit which the pretrained LLM Mistral-7B-v0.1 employs to solve the minimal version of the reasoning problem. We find four families of attention heads, which have surprisingly specialized roles in processing different sections of the context: queried-rule locating heads, queried-rule mover heads, fact-processing heads, and decision heads. We find evidence suggesting that the model follows the natural reasoning path of "QUERY\(\)Relevant Rule\(\)Relevant Fact(s)\(\)Decision".

We discuss related works and scope of this work in detail in Appendix A.

## 2 Problem setting

In this section, we present the core properties of the synthetic propositional logic problem which shall be the data model of this paper. We delay finer details and more examples of the problem to Appendix B.

### Data model: a propositional logic problem

Our problem follows an implicit causal structure, as illustrated in Figure 1. The structure consists of two distinct chains: One containing a logical operator at the end of the chain, and the other forming a purely linear chain.

We require the model to generate a _minimal_ reasoning chain, consisting of "relevant facts", proper rule invocations, and intermediate truth values, to answer the truth-value query. Consider an example constructed from the causal graph in Figure 1, written in English:

* Rules: K implies D. D or E implies A. V implies E. T implies S. P implies T.
* Facts: K is true. P is true. V is false.
* Query: A.
* Answer: K is true. K implies D; D is true. D or E implies A; A is true.

In this example, the QUERY token A is the terminating node of the OR chain. Since any _true_ input to an OR gate (either D or E) results in

Figure 1: Synthetic data model. The causal structure has two chains: one with a logical operator (LogOp) at the end and the other being purely a linear causal chain. This example is the length-3 case.

A being _true_, the minimal solution chooses only one of the starting nodes from the OR chain to construct its argument: in this case, node K is chosen.

## 3 Mechanisms of planning and reasoning: a case study of the length-3 problem

In this section, we discuss the internal mechanisms of a small transformer trained purely on the synthetic problem. While there are many parts of the answer of the transformer which can lead to interesting observations, due to space limitations, we primarily focus on the model's "mental process" for producing the most important part of the answer, namely the _first token_. To further justify this choice, we find that on our problem, a model's full-answer accuracy strongly correlates with its accuracy of the first answer token, as detailed in Figure 4 in the Appendix.

**Architecture choice**. We study a decoder-only attention-only transformer closely resembling the form of GPT-2 (Radford et al., 2019). We discuss training and architecture details in the Appendix. We select the smallest transformer that can achieve 100% accuracy (or sufficiently close to it) to initiate our analysis, a 3-layer 3-head variant.

### Empirical observations

We begin our analysis with mechanisms that are universal to how the model plans and reasons for predicting the first token. Then we describe the mechanisms that only arise when the model needs to deal with specific situations. We discuss the main observations here, and leave the quantitative details to Appendix D.

**Mental notes at the QUERY position**. The QUERY token is likely the most important token in the context: it determines which chain is being queried. The transformer makes use of this token in its answer in an intriguing way.

_Observation 1: chain-type disentanglement at QUERY._ We observe that, the second layer's attention block exhibit disentanglement in its output direction dependent on whether it is the linear chain that is being queried. Intriguingly, the third layer's attention heads place greater than 90% of their attention weights on the QUERY position on average when the linear chain is queried.

Based on Observation 1, we hypothesize that given a chain type (linear or LogOp), there exists certain directions at the second attention block which somehow change the behavior of the third attention block: attracting its attention to QUERY when it is the linear chain, and pushing its attention away from QUERY when it is the LogOp chain. We confirm the existence and role of this "routing" signal.

_Observation 2: existence of an abstract "routing signal"._ We compute the average of the second attention block's output on 1k samples whose QUERY is for the linear chain, which we denote as \(_{route}\). There are two interesting properties of this embedding direction:

1. (Linear\(\)LogOp intervention) We generate 500 test samples where QUERY is for the linear chain. _Subtracting_ the embedding \(_{route}\) from the second attention block's output results in the model outputting the correct first token for the _LogOp chain_ of the problem 100% of the time on the test samples. In other words, the "mode" in which the model reasons is flipped from "linear" to "LogOp".
2. (LogOp\(\)linear intervention) We generate 500 test samples where QUERY is for the LogOp chain. _Adding_\(_{route}\) to the second attention block's output causes the three attention heads in layer 3 to focus on the QUERY position: greater than 99% of the attention weights are on this position averaged over the test samples. In this case, however, the model does not output the correct starting node for the linear chain on more than 90% of the test samples.

It follows that there indeed exists an _abstract embedding direction_ inside the transformer which _significantly changes the information flow depending on the chain type being queried_.

**Linear chain**. At this point, it is clear to us that, when QUERY is for the linear chain, the third layer mainly serves a simple "message passing" role at the QUERY position. A natural question arises: does the input to the third layer truly contain the information to determine the first token of the answer, namely the starting node of the linear chain? The answer is yes.

_Observation 3: linearly-decodable linear-chain answer at layer 2._ We train an affine classifier with the same input as the third attention block, with the target being the start of the linear chain; the training samples only query for the linear chain, and we generate 5k of them. We obtain a test accuracy above 97% for this classifier (on 5k test samples), confirming that layer 2 already has the answer at the QUERY position.

**LogOp chain: partial answer in layers 1 & 2 + refinement in layer 3**. To predict the correct starting node of the LogOp chain, the model employs the following strategy:

1. The first two layers encode the LogOp and only a "partial answer". More specifically, we find evidence that (1) when the LogOp is an AND gate, layers 1 and 2 tend to pass the node(s) with FALSE assignment to layer 3, (2) when the LogOp is an OR gate, layers 1 and 2 tend to pass node(s) with TRUE assignment to layer 3.
2. The third layer, combining information of the two starting nodes of the LogOp chain, and the information in the layer-2 residual stream at the ANSWER position, output the correct answer.

We delay the full set of evidence for the above claims to Appendix D.2.

## 4 The reasoning circuit in Mistral-7B

We now turn to examine how a pretrained LLM, namely Mistral-7B solves this reasoning problem. We choose this LLM as it is amongst the smallest accessible model which achieves above 70% accuracy on (a minimal version of) our problem. We present a hypothesis for the reasoning circuit inside the model for predicting the crucial first token of the length-2 problem in Figure 2, and provide evidence relying on a popular technique in mechanistic interpretability, activation patching.

We describe the main properties of the reasoning circuit inside the model for this prediction task in Figure 2. At a high level, there are several intriguing properties of the reasoning circuit of the LLM:2

1. Compared to the attention blocks, the MLPs are relatively unimportant to correct prediction.
2. There is a sparse set of attention heads that are found to be central to the reasoning circuit: * (Queried-rule locating head) Attention heads (9,25;26), (12,9), (14,24;26) locate the queried rule using the QUERY token, and stores this information at the QUERY position. * (Queried-rule mover head) Attention heads (13,11), (15,8) move QUERY and the queried-rule information from the QUERY position to the ":" position. * (Fact processing heads) Attention heads (16,12;14), (17,25) locate the relevant facts, and move information to the ":" position.

Figure 2: High-level properties of Mistral-7Bâ€™s reasoning circuit. The (chunks of) input tokens are on the left, which are passed into the residual stream and processed by the attention heads. We illustrate the information flow manipulated by the different types of attention heads we identified to be vital to the reasoning task.

* (Decision head) Attention head (19,8), relying on the aggregated information, makes a decision on which token to output.

### Circuit analysis

We only discuss high-level intuitions and results here due to space limitations, and delay the full set of experiments and their interpretations to Appendix E.

Intuitively speaking, to support our hypothesis for the reasoning circuit employed by Mistral-7B to solve the reasoning problem, we rely on activation patching to discover the attention heads which have the greatest influence on the model's output distribution (recall that the MLPs are not as important in this problem). We combine such "causal-mediation" evidence with inspections on these heads' attention patterns. This leads to the set of evidence that is (partially) visualized in Figure 3 below.

## 5 Conclusion

We studied the reasoning mechanisms of both small transformers and LLMs on a synthetic propositional logic problem. We analyzed a shallow decoder-only attention-only transformer trained purely on this problem as well as a pretrained Mistral-7B LLM. We uncovered interesting mechanisms the small and large transformers adopt to solve the problem. For the small models, we found the existence of "routing" signals that significantly alter the model's reasoning pathway depending on the sub-category of the problem instance. For Mistral-7B, we found four families of attention heads that implement the reasoning pathway of "QUERY\(\)Relevant Rule\(\)Relevant Facts\(\)Decision". These findings provide valuable insights into the inner workings of LLMs on mathematical reasoning problems.