# Attack-Aware Noise Calibration

for Differential Privacy

 Bogdan Kulynych

Lausanne University Hospital (CHUV)

&Juan Felipe Gomez

Harvard University

Georgios Kaissis

Technical University Munich

&Flavio du Pin Calmon

Harvard University

&Carmela Troncoso

EPFL

Contributed equally.

###### Abstract

Differential privacy (DP) is a widely used approach for mitigating privacy risks when training machine learning models on sensitive data. DP mechanisms add noise during training to limit the risk of information leakage. The scale of the added noise is critical, as it determines the trade-off between privacy and utility. The standard practice is to select the noise scale to satisfy a given _privacy budget_\(\). This privacy budget is in turn interpreted in terms of operational _attack risks_, such as accuracy, sensitivity, and specificity of inference attacks aimed to recover information about the training data records. We show that first calibrating the noise scale to a privacy budget \(\), and then translating \(\) to attack risk leads to overly conservative risk assessments and unnecessarily low utility. Instead, we propose methods to directly calibrate the noise scale to a desired attack risk level, bypassing the step of choosing \(\). For a given notion of attack risk, our approach significantly decreases noise scale, leading to increased utility at the same level of privacy. We empirically demonstrate that calibrating noise to attack sensitivity/specificity, rather than \(\), when training privacy-preserving ML models substantially improves model accuracy for the same risk level. Our work provides a principled and practical way to improve the utility of privacy-preserving ML without compromising on privacy.

## 1 Introduction

Machine learning and statistical models can leak information about individuals in their training data, which can be recovered by membership inference, attribute inference, and reconstruction attacks (Fredrikson et al., 2015; Shokri et al., 2017; Yeom et al., 2018; Balle et al., 2022). The most common defenses against these attacks are based on differential privacy (DP) (Dwork et al., 2014). Differential privacy introduces noise to either the data, the training algorithm, or the model parameters (Chaudhuri et al., 2011). This noise provably limits the adversary's ability to run successful attacks at the cost of reducing the utility of the model.

In DP, the parameters \(\) and \(\) control the privacy-utility trade-off. These parameters determine the scale (e.g., variance) of the noise added during training: Smaller values of these parameters correspond to larger noise. Larger noise provides stronger privacy guarantees but reduces the utility of the trained model. Typically, \(\) is set to a small fixed value (usually between \(10^{-8}\) and \(10^{-5}\)), leaving \(\) as the primary tunable parameter. Without additional analyses, the values of parameters \((,)\) alone do not provide a tangible and intuitive operational notion of privacy risk (Nanayakkara et al., 2023). This begs the question: how should practitioners, regulators, and data subjects decide on acceptable values of \(\) and \(\) and calibrate the noise scale to achieve a desired level of protection?A standard way of assigning operational meaning to DP parameters is mapping them to _attack risks_. One common approach is computing attacker's posterior belief (or equivalently, accuracy or advantage) of membership inference attacks, that concrete values of \((,)\) allow (Wood et al., 2018). An alternative is to compute the trade-off between sensitivity and specificity of feasible membership inference attacks (Wasserman and Zhou, 2010; Kairouz et al., 2015; Dong et al., 2022), which was recently shown to also be directly related to success of record reconstruction attacks (Hayes et al., 2024; Kaissis et al., 2023a). Such approaches map \((,)\) to a quantifiable level of risk for individuals whose data is present in the dataset. Studies have shown that such risk-based measures are the most useful way to interpret the guarantees afforded by DP for practitioners and data subjects (Cummings et al., 2021; Franzen et al., 2022; Nanayakkara et al., 2023).

In this work, we show that directly calibrating the level of noise to satisfy a given level of attack risk, as opposed to satisfying a certain \(\), enables a significant increase in utility (see Figure 1). We enable this direct calibration to attack risk by working under \(f\)-DP (Dong et al., 2022), a hypothesis testing interpretation of DP. In particular, we extend the tight privacy analysis method by Doroshenko et al. (2022) to directly estimate operational privacy risk notions in \(f\)-DP. Then, we use our extended algorithm to directly calibrate the level of noise to satisfy a given level of attack risk. Concretely, our contributions are:

1. We provide efficient methods for calibrating noise to (a) maximum accuracy (equivalently, advantage), (b) sensitivity and specificity of membership inference attacks, in any DP mechanism, including DP-SGD (Abadi et al., 2016) with arbitrarily many steps.
2. We empirically show that our calibration methods reduce the required noise scale for a given level of privacy risk, up to \(2\) as compared to standard methods for choosing DP parameters. In a private language modeling task with GPT-2 (Radford et al., 2019), we demonstrate that the decrease in noise can translate to a 18 p.p. gain in classification accuracy.
3. We demonstrate that relying on membership inference accuracy as an interpretation of privacy risk, as is common practice, can increase attack power in privacy-critical regimes, and that calibration for sensitivity and specificity does not suffer from this drawback.

Figure 1: Test accuracy (x-axis) of a privately finetuned GPT-2 on SST-2 text sentiment classification dataset (top) and a convolutional neural network on CIFAR-10 image classification dataset (bottom). The DP noise is calibrated to guarantee at most a certain level of privacy attack sensitivity (y-axis) at three possible attack false-positive rates \(\{0.01,0.05,0.1\}\). See Section 4 for details.

4. We provide a Python package which implements our algorithms for analyzing DP mechanisms in terms of the interpretable \(f\)-DP guarantees, and calibrating to operational risks: \[\]

Ultimately, we advocate for practitioners to calibrate the noise level in privacy-preserving machine learning algorithms to a sensitivity and specificity constraint under \(f\)-DP as outlined in Section 3.2.

**Related Work.** Prior work has studied methods for communicating the privacy guarantees afforded by differential privacy (Nanayakkara et al., 2023, 2022; Franzen et al., 2022; Mehner et al., 2021; Wood et al., 2018), and introduced various principled methods for choosing the privacy parameters (Abowd and Schmutte, 2015; Nissim et al., 2014; Hsu et al., 2014). Unlike our approach, these works assume that the mechanisms are calibrated to a given \(\) privacy budget parameter, and do not aim to directly set the privacy guarantees in terms of operational notions of privacy risk.

Cherubin et al. (2024); Ghazi and Issa (2023); Izzo et al. (2024); Mahloujifar et al. (2022) use variants of DP that directly limit the advantage of membership inference attacks. We show that calibrating noise to a given level of advantage can increase privacy risk in security-critical regimes and provide methods that mitigate this issue. Leemann et al. (2024) provide methods for evaluating the success of membership inference attacks under a weaker threat model than in DP. Unlike their work, we preserve the standard strong threat model in differential privacy but set and report the privacy guarantees in terms of an operational notion of risk under \(f\)-DP as opposed to the \(\) parameter.

## 2 Problem Statement

### Preliminaries

**Setup and notation.** Let \(^{n}\) denote the set of all datasets of size \(n\) over a space \(\), and let \(S S^{}\) denote a neighboring relation, e.g. \(S,S^{}\) that differ by one datapoint. We study randomized algorithms (_mechanisms_) \(M(S)\) that take as input a dataset \(S 2^{}\), and output the result of a computation, e.g., statistical queries or an ML model. We denote the output domain of the mechanism by \(\). For ease of presentation, we mainly consider randomized mechanisms that are parameterized by a single noise parameter \(\), but our results extend to mechanisms with multiple parameters. For example, in the _Gaussian mechanism_(Dwork et al., 2014), \(M(S)=q(S)+Z\), where \(Z(0,^{2})\) and \(q(S)\) is a non-private statistical algorithm, the parameter is \(=\) with \(=^{ 0}\). We denote a parameterized mechanism by \(M_{}(S)\). We summarize the notation in Table 1 in the Appendix.

**Differential Privacy.** For any \( 0\), we define the hockey-stick divergence from distribution \(P\) to \(Q\) over a domain \(\) by

\[D_{}(P Q)_{E}Q(E)- P(E)\] (1)

where the supremum is taken over all measurable sets \(E\). We define differential privacy (DP) (Dwork et al., 2006) as follows:

**Definition 2.1**.: A mechanism \(M()\) satisfies \((,)\)-DP iff \(_{S S^{}}D_{e^{*}}(M(S) M(S^{}))\).

Lower values of \(\) and \(\) mean more privacy which in turn requires more noise, and vice versa. In the rest of the paper we assume that a larger value of the parameter \(\) for \(\), e.g., standard deviation of Gaussian noise \(=\) in the Gaussian mechanism, means that the mechanism \(M_{}()\) is more noisy, which translates into a higher level of privacy (smaller \(,\)), but lower utility.

Most DP algorithms satisfy a collection of \((,)\)-DP guarantees. We define the _privacy profile_(Balle and Wang, 2018), or _privacy curve_(Gopi et al., 2021; Alghamdi et al., 2023) of a mechanism as:

**Definition 2.2**.: A parameterized mechanism \(M_{}()\) has a privacy profile \(_{}:\) if for every \(\), \(M_{}()\) is \(((),)\)-DP.

We refer to the function \(_{}()\), defined analogously, also as the privacy profile.

**DP-SGD.** A common algorithm for training neural networks with DP guarantees is DP-SGD (Abadi et al., 2016). The basic building block of DP-SGD is the _subsampled Gaussian mechanism_, defined as \(M(S)=q(_{p} S)+Z\), where \(Z(0,_{2}^{2}^{2} I_{d})\), and \(_{p}\) is a procedure which subsamples a dataset \(S\) such that every record has the same probability \(p(0,1)\) to be in the subsample. DP-SGD, parameterized by \(p,,\) and \(T 1\), is a repeated application of the subsampled Gaussian mechanism: \(M^{(1)} M^{(2)} M^{(T)}(S)\), where \(q^{(i)}()\) is a single step of gradient descent with per-record gradient clipping to \(_{2}\) Euclidean norm. In line with a standard practice (Ponomareva et al., 2023), we regard all parameters but \(\) as fixed, thus \(=\).

Privacy profiles for mechanisms such as DP-SGD are computed via numerical algorithms called _accountants_ (see, e.g., Abadi et al., 2016; Gopi et al., 2021; Doroshenko et al., 2022; Alghamdi et al., 2023). These algorithms compute the achievable privacy profile to accuracy nearly matching the lower bound of a privacy audit where the adversary is free to choose the entire (pathological or realistic) training dataset (Nasr et al., 2021, 2023). Given these results, we regard the analyses of these accountants as tight, and use them for calibration to a particular \((,)\)-DP constraint.

Standard Calibration.The procedure of choosing the parameter \(\) to satisfy a given level of privacy is called _calibration_. In _standard calibration_, one chooses \(\) given a target DP guarantee \(^{}\) and an accountant that supplies a privacy profile \(_{}()\) for any noise parameter \(\), to ensure that \(M_{}(S)\) satisfies \((^{},^{})\)-DP:

\[_{}\ _{}(^{ })^{},\] (2)

with \(^{}\) set by convention to \(^{}=}{{c-n}}\), where \(n\) is the dataset size, and \(c>1\) (see, e.g., Ponomareva et al., 2023; Near et al., 2023). The parameter \(^{}\) is also commonly chosen by convention between \(2\) and \(10\) for privacy-persevering ML algorithms with practical utility (Ponomareva et al., 2023). In Eq. (2) and the rest the paper we denote by \({}^{}\) the target value of privacy risk.

After calibration, the \((,)\) parameters are often mapped to some operational notation of privacy attack risk for interpretability. In the next section, we introduce the hypothesis testing framework of DP, \(f\)-DP, and the notions of risk that \((,)\) parameters are often mapped to. In contrast to standard calibration, in Section 2.3, we calibrate \(\) to directly minimize these privacy risks.

### Operational Privacy Risks

We can interpret differential privacy through the lens of membership inference attacks (MIAs) in the so-called _strong-adversary model_ (see, e.g., Nasr et al., 2021). In this framework, the adversary aims to determine whether a given output \(\) came from \(M(S)\) or \(M(S^{})\), where \(S^{}=S\{z\}\) for some target example \(z\).+ The adversary has access to the mechanism \(M()\), the dataset \(S\), and the target example \(z\). Such an attack is equivalent to a binary hypothesis test (Wasserman and Zhou, 2010; Kairouz et al., 2015; Dong et al., 2022):

Footnote †: We use add relation in this exposition, i.e., \(S S^{}\) iff \(S^{}=S\{z\}\), but our results hold for any relation.

\[H_{0}: M(S), H_{1}: M(S^{}),\] (3)

where the MIA is modelled as a test \(:\) that maps a given mechanism output \(\) to the probability of the null hypothesis \(H_{0}\) being rejected. We can analyze this hypothesis test through the trade-off between the achievable _false positive rate_ (FPR) \(_{}_{M(S)}[]\) and _false negative rate_ (FNR) \(_{} 1-_{M(S^{})}[]\), where the expectations are taken over the coin flips in the mechanism.++ Dong et al. (2022) formalize the _trade-off function_ and define \(f\)_-DP_ as follows:

Footnote ‡: Note that sensitivity (TPR) is \(1-\) and specificity (TNR) is \(1-\).

**Definition 2.3**.: A _trade-off function_\(T(M(S),M(S^{})):\) outputs the FNR of the most powerful attack at any given level \(\):

\[T(M(S),M(S^{}))()=_{:\ }\{_{ }_{}\}\] (4)

See Figure 5 in the Appendix for an illustration.

**Definition 2.4**.: A mechanism \(M()\) satisfies \(f\)-DP, where \(f\) is the trade-off curve for some other mechanism, if for all \(\), we have \(_{S S^{}}T(M(S),M(S^{}))() f()\).

Next, we state the equivalence between \((,)\)-DP guarantees and \(f\)-DP guarantees.

**Proposition 2.1** (Dong et al. (2022)).: _If a mechanism \(M()\) is \((,)\)-DP, then it is \(f\)-DP with_

\[f()=\{0,\ 1--e^{},\ e^{-}(1- -)\}.\] (5)

_Moreover, a mechanism \(M()\) satisfies \(((),)\)-DP for all \(\) iff it is \(f\)-DP with_

\[f()=_{}\{0,\ 1--e^{()},\ e^{- ()}(1--)\}.\] (6)We overview three particular notions of attack risk: advantage/accuracy of MIAs, FPR/FNR of MIAs, and reconstruction robustness. These risks can be thought of as summary statistics of the \(f\) curve.

**Advantage/Accuracy.**Wood et al. (2018) proposedSS to measure the attack risk as the maximum achievable attack accuracy. To avoid confusion with task accuracy, we use _advantage_ over random guessing, which is the difference between the attack TPR \(1-_{}\) and FNR \(_{}\):

Footnote §: Wood et al. (2018) used _posterior belief_, which is equivalent to accuracy under uniform prior.

\[_{S S^{}\ :\ }1-_{}- _{}.\] (7)

The advantage \(\) is a linear transformation of the maximum attack accuracy \(}{{2}}(1-_{})+}{{2}}(1- _{})\), where supremum is over \(S S^{}\) and \(:\). Moreover, \(\) can be obtained from a fixed point \(^{*}=f(^{*})\) of the \(f\) curve as \(1-2^{*}\), and it is bounded given an \((,)\)-DP guarantee:

**Proposition 2.2** (Kairouz et al. (2015)).: _If a mechanism \(M()\) is \((,)\)-DP, then we have:_

\[-1+2}{e^{}+1}.\] (8)

**FPR/FNR Risk.** Recent work (Carlini et al., 2022; Rezaei and Liu, 2021) has argued that MIAs are a relevant threat only when the attack true positive rate \(1-_{}\) is high at low enough \(_{}\). As a concrete notion of risk, we thus consider minimum level of attack FNR \(^{*}\) within an FPR region \([0,^{*}]\), where \(^{}\) is a low value. This approach is similar to the statistically significant p-values often used in the sciences. Following the scientific standards and Carlini et al. (2022), we consider \(^{}\{0.01,0.05,0.1\}\).

**Reconstruction Robustness.** Another privacy threat is the reconstruction of training data records (see, e.g., Balle et al., 2022). Denoting by \(R(;z)\) an attack that aims to reconstruct \(z\), its success probability can be formalized as \([(z,R(;z))]\) over \( M(S\{z\}),z\) for some loss function \(:^{2}\) and prior \(\). Kaissis et al. (2023) showed that MIA error rates bound reconstruction success as \( 1-f(_{})\) for an appropriate choice of \(_{}\). Therefore, the FPR/FNR trade-off curve can also be thought as a notion of robustness to reconstruction attacks.

### Our Objective: Attack-Aware Noise Calibration

The standard practice in DP is to calibrate the noise scale \(\) of a mechanism \(M_{}()\) to some target \((^{},^{})\)-DP guarantee, with \(^{}\) from a recommended range, e.g., \(^{}\), and \(^{}\) fixed to \(^{}<}{{n}}\), as in Eq. (2). Then, the privacy guarantees provided by the chosen \((^{},^{})\) are obtained by mapping these values to bounds on sensitivity and specificity (by Proposition 2.1) or advantage (by Proposition 2.2) of membership inference attacks. In this work, we show that if the goal is to provide an operational and interpretable guarantee such as attack advantage or FPR/FNR, this approach leads to unnecessarily pessimistic noise requirements and a deterioration in utility due to the intermediate step of setting \((^{},^{})\). We show it is possible to skip this intermediate step by using the hypothesis-testing interpretation of DP to _directly_ calibrate noise to operational notions of privacy risk. In practice, this means replacing the constraint in Eq. (2) with an operational notion of risk:

\[_{}\ _{} ^{}.\] (9)

Solving this optimization problem requires two components. First, a way to optimize \(\) given a method to compute \(_{}\). As we assume that risk is monotonic in \(\), Eq. (9) can be solved via binary search (see, e.g., Paszke et al., 2019) using calls to the \(_{}\) function to an arbitrary precision. Second, we need a way to compute \(_{}\) for any value \(\). In the next section, we provide efficient methods for doing so for general DP mechanisms, including composed mechanisms such as DP-SGD, by extending the tight privacy analysis from Doroshenko et al. (2022) to computing \(f\)-DP. Having these methods, we instantiate Eq. (9) for the notions of risks introduced in Section 2.2.

## 3 Numeric Calibration to Attack Risks

In this section, we provide methods for calibrating DP mechanisms to the notions of privacy risk in Section 2.2. As a first step, we introduce the core technical building blocks of our calibration method: methods for evaluating advantage \(_{}\) and the trade-off curve \(f_{}()\) for a given value of \(\).

**Dominating Pairs and PLRVs.** We make use of two concepts, originally developed in the context of computing tight privacy profiles under composition: _dominating pairs_(Zhu et al., 2022a) and _privacy loss random variables_ (PLRV) (Dwork and Rothblum, 2016).

**Definition 3.1**.: We say that a pair of distributions \((P,Q)\) is a _dominating pair_ for a mechanism \(M()\) if for every \(\), we have \(_{S S^{}}D_{e^{}}(M(S) M(S^{}))  D_{e^{}}(P Q)\).

Importantly, a dominating pair also provides a lower bound on the trade-off curve of a mechanism:

**Proposition 3.1**.: _If \((P,Q)\) is a dominating pair for a mechanism \(M\), then for \(\),_

\[_{S S^{}}T(M(S),M(S^{}))() T(P,Q)().\] (10)

The proofs of this and all the following statements are in Appendix E. Proposition 3.1 implies that a mechanism \(M()\) is \(f\)-DP with \(f=T(P,Q)\). Next, we introduce privacy loss random variables, which provide a natural parameterization of the curve \(T(P,Q)\).

**Definition 3.2**.: Suppose that a mechanism \(M()\) has a discrete-valued dominating pair \((P,Q)\). Then, we define the _privacy loss random variables_ (PLRVs) \((X,Y)\) as \(Y}{{P(o)}}\), with \(o Q\), and \(X)}}{{P(o^{})}}\) with \(o^{} P\).

We can now state the result which serves as a main building block for our calibration algorithms, and forms the main theoretical contribution of our work.

**Theorem 3.3** (Accounting for advantage and \(f\)-DP with PLRVs).: _Suppose that a mechanism \(M()\) has a discrete-valued dominating pair \((P,Q)\) with associated PLRVs \((X,Y)\). The attack advantage \(\) for this mechanism is bounded:_

\[[Y>0]-[X>0].\] (11)

_Moreover, for any \(\{,-\}\) and \(\), define_

\[^{*}(,)=[Y]-[Y=].\] (12)

_For any level \(\), choosing \(=(1-)\)-quantile of \(X\) and \(=\) guarantees that \(T(P,Q)()=^{*}(,)\)._

To show this, we use the Neyman-Pearson lemma to explicitly parameterize the most powerful attack at level \(\) in terms the threshold \(\) on the Neyman-Pearson test statistic and the probability \(\) of guessing when the test statistic exactly equals the threshold. See Appendix E.2 for the detailed proof.

We remark that similar results for the trade-off curve appear in (Zhu et al., 2022a) without the \(\) terms, as Zhu et al. assume continuous PLRVs \((X,Y)\). In our work, we rely on the technique due to Doroshenko et al. (2022), summarized in Appendix D, which _discretizes_ continuous mechanisms such as the subsampled Gaussian in DP-SGD, and provides a dominating pair that is _discrete_ and finitely supported over an evenly spaced grid. As the dominating pairs are discrete, the \(\) terms are non-zero, thus are necessary to fully reconstruct the trade-off curve.

### Calibration to Advantage

First, we show how to instantiate Eq. (9) to calibrate noise to a target advantage \(^{}\). Let \(_{}\) denote the advantage of the mechanism \(M_{}()\) as defined in Eq. (7):

\[_{}_{}^{}.\] (13)

Given the PLRVs \((X_{},Y_{})\), we can obtain a substantially tighter bound than converting \((,)\) guarantees using Proposition 2.2 under standard calibration. Specifically, Theorem 3.3 provides the following way to solve the problem:

\[_{}[Y_{}>0]-[X_{ }>0]^{}\] (14)

We call this approach _advantage calibration_, and show how to practically implement it in Algorithms 3 and 4 in the Appendix. Given a method for obtaining valid PLRVs \(X_{},Y_{}\) for any \(\), such as the one by Doroshenko et al. (2022), advantage calibration is _guaranteed_ to ensure bounded advantage, which follows by combining Proposition 3.1 and Theorem 3.3:

**Proposition 3.2**.: _Given PLRVs \((X_{},Y_{})\) of a discrete-valued dominating pair of a mechanism \(M_{}()\), choosing \(^{}\) using Eq. (14) ensures \(_{^{}}^{}\)._

Utility Benefits.We demonstrate how calibration for a given level of attack advantage can increase utility. As a mechanism to calibrate, we consider DP-SGD with \(p=0.001\) subsampling rate, \(T=10{,}000\) iterations, and assume that \(^{}=10^{-5}\). Our goal is to compare the noise scale \(\) obtained via advantage calibration to the standard approach.

As a baseline, we choose \(\) using standard calibration in Eq. (2), and convert the resulting \((,)\) guarantees to advantage using Proposition 2.2. We detail this procedure in Algorithm 2 in the Appendix. We consider target values of advantage \(^{}[0.01,0.25]\). As we show in Figure 1(a), our direct calibration procedure enables to reduce the noise scale by up to \(3.5\).

Pitfalls of Calibrating for Advantage.Calibration to a given level of membership advantage is a compelling idea due to the decrease in noise required to achieve better utility at the same level of risk as with the standard approach. Despite this increase in utility, we caution that this approach comes with a deterioration of privacy guarantees other than maximum advantage compared to standard calibration. Concretely, it allows for _increased attack TPR_ in the privacy-critical regime of low attack FPR (see Section 2.2). The next result quantifies this pitfall:

**Proposition 3.3** (Cost of advantage calibration).: _Fix a dataset size \(n>1\), and a target level of attack advantage \(^{}(^{},1)\), where \(^{}=}{{c n}}\) for some \(c>1\). For any \(0<<}{2}\), there exists a DP mechanism for which the gap in FNR \(f_{}()\) obtained with standard calibration for \(^{}\) that ensures \(^{}\), and FNR \(f_{}()\) obtained with advantage calibration is lower bounded:_

\[() f_{}()-f_{} ()^{}-^{}+2}{^{ }-1}.\] (15)

For example, if we aim to calibrate a mechanism to at most \(^{}=0.5\) (or, 75% attack accuracy), we could potentially increase attack sensitivity by \(() 30\) p.p. at FPR \(=0.1\) compared to standard calibration with \(^{}=10^{-5}\) (see the illustration in Figure 1(b)). Note that the difference \(\) in Proposition 3.3 is an overestimate in practice: the increase in attack sensitivity can be significantly lower for mechanisms such as the Gaussian mechanism (see Figure 6 in the Appendix).

### Safer Choice: Calibration to FNR within a Given FPR Region

In this section, we show how to calibrate the noise in any practical DP mechanism to a given minimum level of attack FNR \(^{}\) within an FPR region \([0,^{}]\), which enables to avoid the pitfalls of advantage calibration. We base this notion of risk off the previous work Carlini et al. (2022); Rezaei and Liu (2021) which argued that MIAs are a relevant threat only when the achievable TPR \(1-\) is high at low FPR \(\). We instantiate the calibration problem in Eq. (9) as follows, assuming \(M_{}()\) satisfies \(f_{}()\)-DP:

\[_{}\ \ \ _{0^{ }}f_{}()^{}.\] (16)

To solve Eq. (16), we begin by showing that such calibration is in fact equivalent to requiring a given level of attack FNR \(^{}\) and FPR \(^{}\).

Figure 2: Benefits and pitfalls of advantage calibration.

**Proposition 3.4**.: _For any \(^{} 0,^{} 0\) such that \(^{}+^{} 1\), and any \(f\)-DP mechanism \(M()\):_

\[_{0^{}}f()^{}f(^{})^{}.\] (17)

This follows directly by monotonicity of the trade-off function \(f\)(Dong et al., 2022). The optimization problem becomes:

\[_{}\;f_{}(^{})^{ }.\] (18)

Unlike advantage calibration to \(^{}\), the approach in Eq.18 limits the adversary's capabilities without increasing the risk in the privacy-critical low-FPR regime, as we can explicitly control the acceptable attack sensitivity for a given low FPR.

To obtain \(f_{}()\), we use the PLRVs \(X_{},Y_{}\) along with Theorem3.3 to compute \(f=T(P,Q)\)4 (see Algorithm1), and solve Eq.18 using binary search over \(\). We provide the precise procedure in Algorithm6 in the Appendix. This approach _guarantees_ the desired level of risk:

**Proposition 3.5**.: _Given PLRVs \((X_{},Y_{})\) of a discrete-valued dominating pair of a mechanism \(M_{}()\), choosing \(^{}\) using Eq.18 and Algorithm1 to compute \(f_{}()\) ensures \(f_{^{}}(^{})^{}\)._

### Other Approaches to Trade-Off Curve Accounting

In this section, we first contextualize the proposed method within existing work. Then, we discuss settings in which alternatives to PLRV-based procedures could be more suitable.

Benefits of PLRV-based Trade-Off Curve Accounting.Computational efficiency is important when estimating \(f_{}()\), as the calibration problem requires evaluating this function multiple times for different values of \(\) as part of binary search. Algorithm1 computes \(f_{}()\) for a single \(\) in \( 500\)ms, enabling fast calibration, e.g., in \( 1\) minute for DP-SGD with \(T=10{,}000\) steps on commodity hardware (see AppendixH). Existing methods for estimating \(f_{}()\), on the contrary, either provide weaker guarantees than Proposition3.5 or are substantially less efficient. In particular, Dong et al. (2022) introduced \(\)-GDP, an asymptotic expression for \(f_{}()\) as \(T\), that _overestimates_ privacy (Gopi et al., 2021), and thus leads to mechanisms that do not satisfy the desired level of attack resilience when calibrating to it. Nasr et al. (2023); Zheng et al. (2020) introduced a discretization-based approach to approximate \(f_{}()\) (discussed next) that can be orders of magnitude less efficient than the direct estimation in Algorithm1, e.g., 1-6 minutes (\(\) 100-700\(\) slower) for a single evaluation of \(f_{}()\) in the same setting as before, depending on the coarseness of discretization.

Calibration using Black-Box Accountants.Most DP mechanisms are accompanied by \((,)\)-DP accountants, i.e., methods to compute their privacy profile \(_{}()\) or \(_{}()\). Black-box access to these accountants enables to estimate \(_{}\) and \(f_{}()\). In particular, Proposition2.2 tells us that \((0,)\)-DP mechanisms bound advantage as \(\). Thus, advantage calibration can also be performed with any \(_{}()\) accountant by calibrating noise to ensure \(_{}(^{})=0\). Estimating \(f_{}()\), as mentioned previously, is less straightforward. Existing numeric approaches (Nasr et al., 2023; Zheng et al., 2020) are equivalent to approximating Eq.6 on a discrete grid over \(\{_{1},,_{n}\}\). This requires \(u\) calls to the accountant \(_{}()\), thus quickly becomes inefficient for estimating \(f_{}()\) to high precision. We provide a detailed discussion of such black-box approaches in AppendixA.

Calibration of Mechanisms with Known Trade-Off Curves.An important feature of our calibration methods is that they enable calibration of mechanisms whose privacy profile is unknownin the exact form, e.g., DP-SGD for \(T>1\). Simpler mechanisms, such as the Gaussian mechanism, which are used for simpler statistical analyses, e.g., private mean estimation, admit exact analytical solutions to the calibration problems in Eqs. (13) and (18). In Appendix G, we provide such solutions for the standard Gaussian mechanism, which enable efficient calibration without needing Algorithm 1.

## 4 Experiments

In this section, we empirically evaluate the utility improvement of our calibration method over traditional approaches. We do so in simulations as well as in realistic applications of DP-SGD. In Appendix H, we also evaluate the utility gain when performing simpler statistical analyses.

**Simulations.** First, we demonstrate the noise reduction when calibrating the DP-SGD algorithm for given error rates using the setup in Section 3.1. We fix three low FPR values: \(^{}\{0.01,0.05,0.1\}\), and vary maximum attack sensitivity \(1-^{}\) from \(0.1\) to \(0.5\) in each FPR regime. We show the results in Figure 3. We observe a significant decrease in the noise scale for all values. Although the decrease is smaller than with calibration for advantage (see Figure 1(a)), calibrating directly for risk in the low FPR regime avoids the pitfall of advantage calibration: inadvertently increasing risk in this regime.

**Language Modeling and Image Classification.** We showed that FPR/FNR calibration enables to significantly reduce the noise scale. Next, we study how much of this reduction in noise translates into actual utility improvement in downstream applications. We evaluate our method for calibrating noise in private deep learning on two tasks: text sentiment classification using the SST-2 dataset (Socher et al., 2013), and image classification using the CIFAR-10 dataset (Krizhevsky et al., 2009).

For sentiment classification, we fine-tune GPT-2 (small) (Radford et al., 2019) using a DP version of LoRA (Yu et al., 2021). For image classification, we follow the approach of Tramer and Boneh (2021) of training a convolutional neural network on top of ScatterNet features (Oyallon and Mallat, 2015) with DP-SGD (Abadi et al., 2016). See additional details in Appendix H. For each setting, by varying the noise scale, we obtain several models at different levels of privacy. For each of the models

Figure 4: Trade-off curves obtained via our method in Algorithm 1 provide a significantly tighter analysis of the attack risks, compared to the standard method of interpreting the privacy risk for a given \((,)\) with fixed \(<}{{n}}\) via Eq. (5). The trade-off curves are shown for three runs of DP-SGD with different noise multipliers in the language modeling experiment with GPT-2. The dotted line - shows the trade-off curve which corresponds to perfect privacy.

Figure 3: Calibration to attack TPR (i.e., \(1-\)FNR) significantly reduces the noise scale in low FPR regimes. Unlike calibration for attack advantage, this approach does not come with a deterioration of privacy for low FPR, as it directly targets this regime.

we compute the guarantees in terms of TPR \(1-\) at three fixed levels of FPR \(^{}\{0.01,0.05,0.1\}\) that would be obtained under standard calibration, and using our Algorithm 1.

Figure 1 shows that FPR/FNR calibration significantly increases _task_ accuracy (a notion of utility; not to confuse with _attack_ accuracy, a notion of privacy risk) at the same level of \(1-\) for all values of \(^{}\). For instance, for GPT-2, we see the accuracy increase of 18.3 p.p. at the same level of privacy risk (top leftmost plot). To illustrate the reasons behind such a large difference between the methods, in Figure 4, we show the trade-off curves obtained with our Algorithm 1, and with the standard method of deriving the FPR/FNR curve from a single \((,)\) pair for a fixed \(<}{{n}}\) via Eq. (5). We can see that the latter approach drastically overestimates the attack risks, which translates to significantly higher noise and lower task accuracy when calibrating with standard calibration.

## 5 Concluding Remarks

In this work, we proposed novel methods for calibrating noise in differentially private learning targeting a given level of operational privacy risk: advantage and FPR/FNR of membership inference attacks. We introduced an accounting algorithm which directly and tightly estimates privacy guarantees in terms of \(f\)-DP, which characterizes these operational risks. Using simulations and end-to-end experiments on common use cases, we showed that our attack-aware noise calibration significantly decreases the required level of noise compared to the standard approach at the same level of operational risk. In the case of calibration for advantage, we also showed that the noise decrease could be harmful as it could allow for increased attack success in the low FPR regime compared to the standard approach, whereas calibration for a given level of FPR/FNR mitigates this issue. Next, we discuss limitations and possible directions for future work.

**Choice of Target FPR/FNR.** We leave open the question on how to choose the target FPR \(^{}\) and FNR \(^{}\), e.g., whether standard significance levels in sciences such as \(^{}=0.05\) are compatible with data protection regulation and norms. Further work is needed to develop concrete guidance on the choice of target FPR and FNR informed by legal and practical constraints.

**Catastrophic Failures.** It is possible to construct pathological DP mechanisms which admit catastrophic failures (see, e.g., Ponomareva et al., 2023), i.e., mechanisms which allow non-trivial attack TPR at FPR \(=0\) so that their trade-off curve is such that \(T(M(S),M(S^{}))(0)<1\) for some \(S S^{}\). A classical example in the context of private data release is a mechanism that releases a data record in the clear with probability \(>0\), in which case we have \(T(M(S),M(S^{}))(0)=1-\). See the proof of Proposition 3.3 in Appendix E for a concrete construction. In the case that such a pathological mechanism is used in practice, one should use standard calibration to \((,)\) with \(}{{n}}\) to directly limit the chance of catastrophic failures. Fortunately, practical mechanisms such as DP-SGD do not admit catastrophic failures, as they ensure \(T(M(S),M(S^{}))(0)=1\).

**Tight Bounds for Privacy Auditing.** Multiple prior works on auditing the privacy properties of ML algorithms (Nasr et al., 2021; Liu et al., 2021; Jayaraman and Evans, 2019; Erlingsson et al., 2019) used conversions between \((,)\) and operational risks like in Proposition 2.1, which we have shown to significantly overestimate the actual risks. Beyond calibrating noise, our methods provide bounds on attack success rates for audits in a more precise and computationally efficient way than a recent similar approach by Nasr et al. (2023).

**Accounting in the Relaxed Threat Models.** Although we have focused on DP, our methods apply to any notion of privacy that is also formalized as a hypothesis test. In particular, our method can be used as is to compute privacy guarantees of DP-SGD in a relaxed threat model (RTM) proposed by Kaissis et al. (2023). Previously, there was no efficient method for accounting in the RTM.

**Applications Beyond Privacy.** Our method can be applied to ensure provable generalization guarantees in deep learning. Indeed, prior work has shown that advantage \(\) bounds generalization gaps of ML models (Kulynych et al., 2022, 2022). Thus, even though advantage calibration can exacerbate certain risks, it can be a useful tool for ensuring a desired level of generalization in models that usually do not come with non-vacuous generalization guarantees, e.g., deep neural networks.