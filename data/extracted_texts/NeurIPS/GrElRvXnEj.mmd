# Score-based Generative Modeling through

Stochastic Evolution Equations in Hilbert Spaces

 Sungbin Lim\({}^{1,6,7}\)

Corresponding Author. e-mail: sungbin@korea.ac.kr.

Eunbi Yoon\({}^{1}\)

This work is done at UNIST.

Taehyun Byun\({}^{2}\)

Taewon Kang\({}^{3}\)

Seungwoo Kim\({}^{4}\)

Kyungjae Lee\({}^{5}\)

Sungjoon Choi\({}^{2}\)

This work is done at UNIST.

\({}^{1}\)Department of Statistics, Korea University

\({}^{2}\)Department of Artificial Intelligence, Korea University

\({}^{3}\)Department of Computer Science and Engineering, Korea University

\({}^{4}\)Artificial Intelligence Graduate School, UNIST

\({}^{5}\)Department of Artificial Intelligence, Chung-Ang University

\({}^{6}\)LG AI Research

\({}^{7}\)SNU-LG AI Research Center

###### Abstract

Continuous-time score-based generative models consist of a pair of stochastic differential equations (SDEs)--a forward SDE that smoothly transitions data into a noise space and a reverse SDE that incrementally eliminates noise from a Gaussian prior distribution to generate data distribution samples--are intrinsically connected by the time-reversal theory on diffusion processes. In this paper, we investigate the use of stochastic evolution equations in Hilbert spaces, which expand the applicability of SDEs in two aspects: sample space and evolution operator, so they enable encompassing recent variations of diffusion models, such as generating functional data or replacing drift coefficients with image transformation. To this end, we derive a generalized time-reversal formula to build a bridge between probabilistic diffusion models and stochastic evolution equations and propose a score-based generative model called **H**ilbert **D**iffusion **M**odel (HDM). Combining with Fourier neural operator, we verify the superiority of HDM for sampling functions from functional datasets with a power of kernel two-sample test of 4.2 on Quadratic, 0.2 on Melbourne, and 3.6 on Gridwatch, which outperforms existing diffusion models formulated in function spaces. Furthermore, the proposed method shows its strength in motion synthesis tasks by utilizing the Wiener process with values in Hilbert space. Finally, our empirical results on image datasets also validate a connection between HDM and diffusion models using heat dissipation, revealing the potential for exploring evolution operators and sample spaces.

## 1 Introduction

Score-based generative models  have shown success in various domains, including the generation of images , texts , videos , motions .  proposes a framework for continuous-time score-based generative models that use a pair of stochastic differential equations (SDEs); a forward SDE smoothly transitions data into noise space, and a reverse SDE incrementally eliminates noise from a Gaussian prior distribution to generate samples from the data distribution. Both SDEs have the same marginal distribution when their coefficients satisfy the time-reversal formula , which is an ingenious application of the Kolmogorov-Fokker-Planck equation for diffusion processes.

Recently, there has been an active proposal of variations of diffusion models, such as generating functional data [18; 19; 23; 29; 38; 48; 49] or replacing drift coefficients with image transformation [4; 27; 50], which cannot be covered by the original SDE framework  formulated in the Euclidean space. Motivated by this problem, we propose a unified framework for continuous-time score-based generative models by using _stochastic evolution equations_ in Hilbert spaces [6; 14; 34; 35],

\[_{t}=_{t}(_{t})t+_{t}_{t},}_{t}=[- _{T-t}(}_{t})+(T-t, }_{t})]t+_{T-t}_{t},\] (1)

where \((_{t},}_{t})\) is a pair of forward and reverse stochastic processes with evolution operators \((_{t},_{t})\), \(_{t}\) is a Wiener process with values in some Hilbert space \(\), and \((t,):[0,T]\) is a _score operator_, which generalizes the notion of score functions in the Euclidean space. Since we can choose \(\) and \((_{t},_{t})\) in an abstract setting, stochastic equations (1) naturally extend the use of original SDEs to the infinite-dimensional setting and include stochastic partial differential equations (SPDEs), such as a parabolic equation driven by white noise (see Figure 1). Consequently, we can expand the SDE framework  in two aspects: sample space and evolution operator, enabling it to encompass recent variations of diffusion models in a continuous-time score-based generative model.

ContributionsAn essential key to identifying the relationship between the forward process \(_{t}\) and the reverse process \(}_{t}\) in (1) is the derivation of _time reversal formula_ with _time-dependent_ operators \(_{t}\) and \(_{t}\), which has been a challenging problem [23; 29; 38; 49] when we utilize coefficient schedulings [46; 52]. Contrary to previous approaches using semigroup theory , our approach is grounded in Kolmogorov equations with functional derivatives, well-studied in [2; 3; 7; 15; 16; 17], so that we can consider a wide scope of stochastic evolution equations with time-dependent operators.

* We derive a generalized time-reversal formula and unify the SDE framework  with stochastic equations in Hilbert spaces. Consequently, we propose a class of continuous-time score-based generative models, **H**ilbert **D**iffusion **M**odel (HDM), which enables using SDEs in infinite-dimensional settings (HDM-SDE) and parabolic SPDEs (HDM-SPDE).
* Combining with neural operators , HDM-SDE shows the superiority for sampling functions from functional datasets with multi-modalities compared to GP , NP , DDPM , and other diffusion models formulated in function spaces, NDP  and SP-SGM . We also validate that HDM-SDE plays a crucial role in successful human motion synthesis compared to DDPM, by utilizing the Wiener process in the Hilbert space.
* We build a bridge between parabolic SPDE and diffusion models using heat dissipation . HDM-SPDE improves the performance in image generation as a continuous-time version of IHDM. Also, the proposed method shows the inductive bias in controllable coarse-to-fine image generation, e.g., inpainting, deblurring, and depixelation, without additional training.

Figure 1: A unified framework with various stochastic evolution equations to yield score-based generative models for different datasets. (a) SDE in infinite-dimensional spaces can generate functions as samples from the function space. (b) Utilizing a prior distribution generated from heat dissipation-type forward evolution \(_{t}=b(t)(-)\), the same as IHDM , we can generate image samples by adding Gaussian noise (\(\)) and executing reverse evolution through the parabolic SPDE.

A Foundation on Time-Reversal of Stochastic Evolution Equations

We first present a preliminary theory on the stochastic evolution equations in Hilbert space and introduce the logarithmic derivatives of probability measures to propose a Hilbert space valued _score operator_ (see Section 2.2) which generalizes the notion of score functions in the Euclidean space. Then we derive the _time-reversal formula_ (Theorem 2.1), which is a key to obtain the time-reversal of the diffusion processes by using the Kolmogorov equations in Hilbert space.

### Preliminaries: Stochastic Evolution Equations in Hilbert Spaces

In preliminaries, we introduce elements of stochastic evolution equations in Hilbert spaces. See B.1 or we refer [5; 6; 14; 34; 35] for basic setting.

Gelfand Triple and Cameron-Martin SpaceLet \(\) be a separable Hilbert space with the \(\)-norm defined by the inner product \(\|\|_{}=}}\). Let \(()\) denotes the set of all bounded linear operators on \(\). We assume there exists a Radon centered Gaussian measure \(\) on the Borel \(\)-field \(()\) with the nonnegative, symmetric, self-adjoint, and the finite trace covariance operator \(()\), hence there exists eigensystem \(\{(^{()},^{()})_{+}: \}\) such that \((^{()})=^{()}^{()}\) holds and \(()=_{=1}^{}^{()}<\). We define _Cameron-Martin space_ by \(_{}:=^{1/2}()\) with the inner product \( h_{1},h_{2}_{_{}}:=^{-1/2}(h_{ 1}),^{-1/2}(h_{2})_{}\). Let \(^{*}^{*}_{}\) be the dual space of \(\) and \(_{}\), respectively. By identifying \(_{}\) with \(^{*}_{}\) via the Riesz isomorphism, we can consider a continuous inclusion map \(i_{_{}}:^{*}_{}\) such that

\[()= i_{_{}}(),_{_{}}=^{-1/2}(i_{_{}}()), ^{-1/2}()_{},^{*}, _{}.\] (2)

Thus, we can also identify \(^{*}\) with \(^{1/2}(_{})\) (see [8, Example 1.1]). For notational convenience, we write \(i_{_{}}()\) as \(()\) by regarding \(\) as an element in \(\). In summary,

\[_{}=^{1/2}(), i_{_{ }}(^{*})=(), i_{_{}}( ^{*})_{}.\] (3)

We refer to \((^{*},_{},)\) as _Gelfand triple_ which provides groundwork for the SDE theory in \(\).

**Example 2.1** (Euclidean space).: Let \(:=^{d}\) and \(=(0,)\) be a Gaussian measure with a positive-definite covariance matrix \(^{d d}\). Then \(^{-1/2}<\) for \(^{d}\), hence \(_{}=\). \(\)

**Example 2.2** (Reproducing kernel Hilbert space).: Let \(\) denote the class of square-integrable functions on finite measure space \((,_{})\) with the usual \(L_{2}\)-norm, \(\|f\|_{}:=(_{}|f(x)|^{2}_{}( x))^{1/2}\). Due to the assumption on \(\), there exists symmetric and positive definite kernel function \(K_{}\) such that

\[(f)()=_{}K_{}(,)f( )_{}(),,\ f.\] (4)

Hence Cameron-Martin space \(_{}\) is reproducing kernel Hilbert space (RKHS) with \(K_{}\). \(\)

Wiener Processes and Stochastic Evolution Equations in Hilbert SpacesFor \(T>0\), let \((_{t})_{t[0,T]}\) be \(\)-valued \(_{t}\)-adapted \(\)-Wiener process (see Definition B.1 for details). By the Kosambi-Karhunen-Loeve theorem, \(_{t}\) has a series representation:

\[_{t}:=^{1/2}(_{t})=_{=1}^{}}W_{t}^{()}^{()}, t[0,T],\] (5)

where \(_{t}=\{W_{t}^{()}:\}\) is _white noise_ of which the components \(W_{t}^{()}\) are independent real-valued Wiener processes. Then \((_{t})=t\) holds and \(_{t},_{s}_{}=\{t,s \}()<\) since \(\) has the finite trace. If \(_{}\) is RKHS with \(K_{}\) (Example 2.2), \(_{t}\) preserves spatial correlation,

\[[_{t}(_{1})_{s}(_{2})]=\{t, s\}K_{}(_{1},_{2}),_{1},_{2} .\] (6)

Let \(_{2}()\) denote the class of Hilbert-Schmidt operator on \(\). Following the usual construction of the stochastic integral with respect to \(_{t}\) (e.g., see [14, Section 4] and [35, Section 2]), we can define the following stochastic evolution equation in \(\),

\[_{t}=_{0}+_{0}^{t}_{s}(_{s}) s+_{0}^{t}_{s}_{s},_{0 }, t[0,T],\] (7)where \(_{t}:[0,T]\) and \(_{t}:[0,T]_{2}()\) are progressively measurable satisfying regular conditions [35, Section 3.2] which guarantee solvability in \(\). Under these assumptions, there exists a unique \(\)-valued solution \(_{t}\) to (7) for every \(t[0,T]\) such that \(_{0}\) and \(_{t}\) are independent. We assume \(_{t}\) is constant on \(\), which still recovers the SDE framework of .

### Score Operator and Time-Reversal Formula

Now we consider a time-reversal \(}_{t}:=_{T-t}\) of (7) and its corresponding diffusion process,

\[}_{t}=}_{0}+_{0}^{t}}_{s}(}_{s})t+_{0}^{t}}_{s}_{s},}_{0} (0,), t[0,T].\] (8)

A condition for the existence of the time-reversal process (8) is well-studied for diffusion processes in abstract settings (e.g., see ), hence we focus on identifying evolutionary operators \(}\) and \(}\).

Logarithmic Derivative of Probability Measure and Score OperatorA score function in the Euclidean space is defined by a gradient of logarithmic probability density \(_{x} p_{t}(x)\). For Hilbert spaces, we employ logarithmic derivatives of Fomin differentiable probability measures . Let us define the class of smooth cylinder functions , which is dense in the Sobolev space on \(\), by

\[^{}_{b}:=\{f_{_{1:m}}=f(_{1},,_{m })::f C^{}_{b}(^{m}),_{i} ^{*},m\}.\] (9)

For \(f_{_{1:m}}^{}_{b}\), we compute the \(_{}\)-gradient at \(u\) in the direction of \(_{}\) by

\[ Df_{_{1:m}}(u),_{_{}}=_{i=1}^{ m}_{i}f(_{1}(u),,_{m}(u)), (_{i})_{_{}}, u,\] (10)

where \(Df_{_{1:m}}\) denotes a Gateaux derivative. A differentiable measure \(\) on \(\) along a vector \(_{}\) has the _partial logarithmic derivative_\(^{}_{} L_{1}(,)\) if the following integration by parts holds,

\[_{} Df_{_{1:m}}(u),_{_{ }}(u)=-_{}f_{_{1:m}}(u)^{}_{ }(u)(u), f_{_{1:m}}^{}_{b}.\] (11)

If there exists a Borel map \(^{}_{_{}}():\) such that \(^{}_{_{}}(u),_{_{} }=^{}_{}(u)\) for \(_{}\), then \(^{}_{_{}}\) is called the _vector logarithmic derivative_ of \(\) associated with \(_{}\). For instance, if \(\) is a Gaussian measure with mean \(m_{}\) and covariance operator \(_{}()\) satisfying \(()_{}()\), then \(^{}_{_{}}\) is computed by (see [8, Example 2.5] and [14, Proposition 2.26]),

\[^{}_{_{}}(u)=_{}^{-1}(m_{}- u), u,\] (12)

which generalizes the score function \(_{} p()\) of a Gaussian density \(p()(m_{},_{})\). For a sequence of probability measures \((_{t})_{t[0,T]}\) of the solution to (7) with \(_{0}=_{}\), we define a _score operator_\(_{}(t,):[0,T]\) by multiplying \(_{t}_{t}^{*}\) to the vector logarithmic derivative \(^{_{_{}}}_{_{}}\),

\[_{}(t,u):=_{t}_{t}^{*}^{}_{ _{}}(u).\] (13)

Hence the score operator \(_{}(t,)\) is analogous to \(g^{2}(t)_{} p_{t}()\) in the reverse SDE of .

Kolmogorov Equations and Time-Reversal FormulaScore-based generative model through SDE is grounded on the Fokker-Planck-Kolmogorov equations . Following , we define _Kolmogorov operator_\(_{t}\) on the class of smooth cylinder functions \(^{}_{b}\) by

\[_{t}f_{_{1:m}}(u):=_{_{ }}(_{t}(u) D^{2}f_{_{1:m}}(u))+  Df_{_{1:m}}(u),_{t}(u)_{_{}},\] (14)

where \(_{t}:=_{t}_{t}^{*}\) and \(D^{2}f_{_{1:m}}\) denotes the second-order Gateaux derivative. The Kolmogorov operators \((_{t})_{t[0,T]}\) describes evolution of \((_{t})_{t[0,T]}\) and becomes a _generator_ of Markov diffusion process . Now we present our main result on the time-reversal formula.

**Theorem 2.1** (Time-Reversal Formula).: _Time reversal \(}_{t}\) satisfying (8) has generator_

\[_{t}}f_{_{1:m}}(u):=_{_ {}}(}_{t} D^{2}f_{_{1:m}}(u))+  Df_{_{1:m}}(u),}_{t}(u)_{_ {}},\] (15)

_where \(}_{t}\) and \(}_{t}\) satisfy the following **time-reversal formula**_

\[}_{t}=_{T-t},}_{t}(u)=- _{T-t}(u)+_{}(T-t,u).\] (16)See Appendix B.1 for the proof of Theorem 2.1. The proof is based on the Kolmogorov forward equation and the Kolmogorov backward equation with functional derivatives to show that (15) is the generator of the time-reversal diffusion process (8), which is analogous to the proof in the Euclidean space . Due to (16), we can rewrite (8) by

\[}_{t}=}_{0}+_{0}^{t}[-_{T-s}(}_{s})+_{}(T-s,}_{s} )]s+_{0}^{t}_{T-s}_{s}.\] (17)

Hence we can generate a sample from \(}_{T}_{}\) by running (17) if we obtain \(_{}(t,)\) for \(t[0,T]\).

### Applications of Theorem 2.1

This section presents two important applications of Theorem 2.1; SDEs in infinite-dimensional space and parabolic SPDEs in finite-dimensional space. We utilize these stochastic evolution equations to propose a unified framework of continuous-time score-based generative models in Section 3.

#### 2.3.1 SDEs in Infinite-Dimensions

We first generalize the SDE framework  in the infinite-dimensions based on Theorem 2.1. Let \(\{^{()},^{()}\}_{t}\) be an eigensystem of \(\) and consider the following SDE system with values in \(\),

\[X_{t}^{()}=b_{t}^{()}X_{t}^{()}t+}_{t}^{()}W_{t}^{()}, t[0,T], ,\] (18)

where \(b_{t}^{()}:[0,T]\) and \(_{t}^{()}:[0,T]_{+}\) are real-valued functions for each \(\). Set \(_{t}^{()}:=(_{0}^{t}b_{s}^{()}s)\) and \(_{}^{()}(t):=(_{t}^{()}/_{t}^{()} )^{2}\). Conditioning at \(X_{0}^{()}\), the law of \(X_{t}^{()}\) follows Gaussian distribution, hence we can apply [44, Theorem 5.3] to (18), which induces \(}_{t}()\) by

\[}_{t}()=_{}(-b_{T-t}^{ ()}x^{()}+_{}^{()}(T-t)^{()}X_{0} ^{()}-x^{()}}{_{0}^{T-t}_{}^{()}(s)s} ).\] (19)

Indeed, (19) is a special case of (16) (see Appendix C.1).

**Example 2.3** (VP-SDE and sub-VP-SDE in Infinite-Dimensions).: Set \(b_{t}^{()}=-b(t)\) and \(_{t}^{()}=\) for every \(\). Then we have \(_{t}=e^{-_{0}^{t}b(s)s}\) and \(_{}(t)=b(t)e^{_{0}^{t}b(s)s}\). Thus,

\[m_{_{t}|_{0}}=e^{-_{0}^{t}b(s)s} _{0},_{_{t}|_{0}}=(1-e^{-_{0}^{t}b(s) s}).\] (20)

Hence we can compute the conditional score function \(_{}(t,_{t}|_{0})\) by using (19). As a result, (20) generalizes VP-SDE  in infinite-dimensions. Similarly, we can recover the sub-VP-SDE in infinite-dimensions, hence (19) recovers SDEs proposed in  when \(=\) and \(=^{d}\).

#### 2.3.2 Parabolic SPDEs

We introduce parabolic SPDEs  based on Theorem 2.1. For \(k>-1\), we define

\[u_{t}=b(t)[ u_{t}- u_{t}]t+( -)^{-}_{t}, t[0,T],\] (21)

on a bounded domain \(\) with a zero-Neumann boundary condition. By following , we use the discrete cosine transformation (DCT) from a coordinate vector \(u_{t}\) expressed in the standard basis to a coordinate vector \(_{t}\), with the eigendecomposition \(^{}\). Hence we can rewrite (21) as

\[_{t}=b(t)(_{t}-_{t})t +(-)^{-}}_{t}, t[0,T],\] (22)

where \(_{t}=^{}u_{t}\) and \(}_{t}=^{}_{t}\). Since \(\) is orthogonal, we have \(}_{t}_{t}\) for each \(t\), so we can regard \(}_{t}\) as standard space-time white noise. Thus, we can apply Theorem 2.1 to (22) with \(_{t}=b(t)(-)\) and \(_{t}=(-)^{-}\) in \(^{d}\) (see Appendix C.2). Therefore,

\[}_{t}=b(T-t)(-)+2b(T-t)( -)^{-k}_{_{}}^{_{T-t}}.\] (23)If we set \(b(t)=1\), \(=0\), and \(k=0\), then (21) becomes stochastic heat equation (SHE) and has a connection between the generative process of HIDM . Indeed, the mild solution of SHE is

\[u_{t}=e^{ t}u_{0}+_{0}^{t}e^{(t-s)}_{s},  t[0,T],\] (24)

where \(e^{ t}\) is a \(C_{0}\)-semigroup, which induces the mean vector of \(u_{t}\) as \(e^{ t}u_{0}=(t)^{}u_{0}\), and the remaining stochastic integration part becomes Gaussian process [14, Theorem 5.2]. If we consider a continuous-time version of IHDM with a variance scheduling, then (23) induces the appropriate reverse SPDE for sampling. See Section 4.3 for empirical results.

## 3 Hilbert Diffusion Models

This section is devoted to describing Hilbert Diffusion Models (HDMs), a class of score-based generative models based on Theorem 2.1 and stochastic evolution equations presented in Section 2.3. We introduce two versions of HDM, HDM-SDE and HDM-SPDE, which utilize SDEs in infinite-dimensions (18) and parabolic SPDEs (21), respectively.

### Estimating Score Operators

To estimate the score operator \(_{}(t,)\) in Hilbert space, we propose a time-dependent score model \(_{}(t,):[0,T]\) parameterized with \(^{p}\), which is trained by the following objective,

\[^{*}=^{p}}{}\ _{t[0,T]} _{_{t}_{t}}\|_{}(t,_{t} )-_{}(t,_{t})\|_{}^{2},\] (25)

where \(\) denotes the uniform distribution on \([0,T]\). Due to , note that minimizing (25) is equivalent to minimizing the following objective,

\[^{*}=^{p}}{}\ _{t [0,T]}_{_{0}_{}} _{_{t}_{t}|_{0}}\|_{}(t,_{t})-_{}(t,_{t}|_{0})\|_{ }^{2},\] (26)

where \(_{t}|_{0}\) denotes the conditional distribution of \(_{t}\) conditioned at \(_{0}_{}\), and \(_{}(t,_{t}|_{0})\) is the corresponding score operator, which is estimated by a time-dependent neural network.

Architectures of HDMWe use different architectures for modeling HDM-SDE and HDM-SPDE since the corresponding stochastic evolution equations (18) and (21) have distinct values in infinite-dimensions and Euclidean space, respectively. For HDM-SDE, same as other diffusion models in infinite-dimensions , we modify Fourier Neural Operator (FNO) , called _time-conditioned_ FNO, by adding a positional embedding and a linear layer to embed the time variable \(t\) (see Figure 2). The spatial variable \(\) first passes through the lifting layer, increasing its dimensions to match the lifting channels. Then, the embedded time variable undergoes the linear layer to adjust its dimensions to match the lifted spatial variable, and is pointwisely added. The Fourier layer takes the input and applies Fast Fourier Transformation (FFT) to transform it to the frequency space, followed by spectral convolution. See Appendix C.3 for more details.

For HDM-SPDE, we use U-Net  similar to other diffusion models , mostly varying the number of multipliers of resolutions and the number of residual blocks. Detailed architecture parameters according to datasets are described in Table A.3.

Figure 2: The architecture of time-conditioned FNO.

### Training and Sampling

Loss FunctionsBased on (26), both HDM-SDE and HDM-SPDE use the mean-square loss as the training loss function. Fix \(_{0}_{}\) and let \(_{t}\) be a generated sample at time \(t\) from \(_{t}|_{0}\) so that \(_{t}(m_{_{t}|_{0}},_{_{t}| _{0}})\). Then the loss functions are represented by

\[L_{}() =_{t(0,T)}_{_{0} _{}}_{(0,)} \|_{}(t,_{t})+(t)_{_{t}|_{0}}^{1/2}()\|_{2}^{2},\] (27) \[L_{}() =_{t[0,T]}_{_{0} _{}}_{(0,)} \|_{}(t,_{t})+((t) _{_{t}|_{0}}^{1/2}())\|_{2}^{2}.\] (28)

Here we use the cosine beta schedule  with \((t)=()\) for both HDM-SDE and HDM-SPDE for a small number \(s>0\).

Solving Reverse Stochastic Evolution EquationsAfter training a score model \(_{}(t,)\) for all \(t[0,T]\), we can generate samples from \(_{}\) by running the reverse process (17) with the trained score model instead of \(_{}(t,)\). Numerically, we utilize the Euler-Maruyama method to approximate a solution of (17) starting at \(}_{t_{0}}(0,)\) (see Algorithms 1 and 2),

\[}_{t_{m+1}}=}_{t_{m}}+(-_{T-t_{m}}(}_{t_{m}})+_{}(T-t_{m}, {}_{t_{m}})) t+_{T-t_{m}} _{t_{m}},\] (29)

where \(_{t_{m}}}{}(0,)\), and \( t:=t_{m+1}-t_{m}\) for \(m\{0,,M\}\) with \(t_{0}=0\) and \(t_{M}=T\).

## 4 Experiments

This section presents the experimental settings and results that apply HDM-SDE to generating function and motion and HDM-SPDE to image generation. For more details, see Appendix D.

### Function Generation through HDM-SDE

Datasets and ImplementationWe validate HDM-SDE for generating functions from 1D functional datasets; Quadratic, Melbourne, and Gridwatch, to follow the setting in . We refer to Section D.1 for more details about datasets and settings. We tune the kernel hyperparameter of len parameter for 0.8 in Quadratic, 2.0 in Melbourne, and 1.8 in Gridwatch. We commonly use gain parameter 1.0 for all datasets. Grid parameter adjusted to 100 in Quadratic, 24 in Melbourne, and 288 in Gridwatch. In the Quadratic and Melbourne datasets, we train our model during 1K iterations (1.5K in Gridwatch). For sampling, we set the number of sampling steps as 1,000. For a quantitative evaluation, we calculate the power of a kernel two-sample test  based on Maximum Mean Discrepancy (MMD).

ResultsTable 1 shows the results of the test power analysis, and HDM-SDE demonstrates excellence compared to Gaussian process , Neural process , DDPM (VP-SDE) , and other diffusion models (NDP  and SP-SGM ) formulated on function spaces. Our method records a power(%) of 4.2\({}_{ 0.3}\) on Quadratic, 0.2\({}_{ 0.1}\) on Melbourne, and 3.6\({}_{ 0.0}\) on Gridwatch. Figures 3 and A.1 show that HDM-SDE method has outperformed results compared to the diff

Figure 3: Comparison of functional samples generated by DDPM (VP-SDE) , NDP , and HDM-SDE (ours) on Quadratic dataset.

white-noise, due to their limited adaptability to variable discretizations as discussed in . A trace-class diffusion model using Langevin dynamics  records power of 6.9\({}_{ 0.5}\) on Quadratic and of 4.0\({}_{ 0.3}\) on Melbourne, which is comparable to SP-SGM but are less favorable than HDM-SDE.

### Motion Generation through HDM-SDE

Datasets and ImplementationWe further investigate the performance of HDM-SDE for motion generation tasks using a HumanML3D dataset . The HumanML3D dataset consists of 14,616 motions, where each motion consists of 144-dimensional trajectories, derived from 24 joints and a 6D representation and we randomly sample 100 motions for the experiment. The root poses of the skeletons are fixed to the origin, and the motion length is trimmed to 128. We compare our method with the baseline (DDPM)  utilizing the FNO architecture except the input and output channel sizes of FNO are changed from 1 to 144 to handle 144-dimensional trajectories. Hence, our proposed method and the baseline have identical network architectures and loss functions, and the only difference is the noise sampling procedures in forward and reverse processes. We utilize the squared-exponential kernel for generating the Wiener process in RKHS where the length parameters of each 144-dimensional trajectory are selected using a likelihood-based model selection of a Gaussian process . See Appendix D.3 for details.

ResultsThe synthesized motions of the proposed method are shown in Figure 4, where we overlay right and left hands trajectories with red and blue, respectively. While the proposed method successfully generates human motions, the baseline fails to generate meaningful motions. Our proposed method significantly outperforms the baseline in terms of motion synthesis as the baseline fails to generate human-like motions. This clearly highlights the advantage of utilizing \(\)-valued Wiener process in modeling multi-dimensional complex trajectories. The quantitative results demonstrate better performance in both hand and foot trajectories within the task space are shown in Table A.2.

### Image Generation through HDM-SPDE

Datasets2D image experiment includes MNIST , CIFAR10 , LSUN-church , AFHQ , and FFHQ  datasets. We adjust the image resolutions as IHDM , LSUN-church images are resized into 128\(\)128; higher resolutions, including AFHQ and FFHQ, have resized into 256\(\)256. See Appendix D.4 for implementation details.

EvaluationTo evaluate the sampling quality, we use the clean FID  using InceptionV3 . Clean-FID scales generated and reference images into 299\(\)299 resolution via bicubic interpolation and compute statistics. Following IHDM , we sample 50,000 images with a resolution smaller than 256\(\)256 and calculate the reference samples as the training set. Otherwise, we sample 10,000 images and use the entire dataset as reference data for evaluating metrics.

    & GP  & NP  & DDPM  & NDP  & SP-SGM  & HDM-SDE (Ours) \\  Quadratic & \(100.0_{ 0.0}\) & \(8.6_{ 1.5}\) & \( 99.0\) & \( 99.0\) & \(5.4_{ 0.7}\) & **4.2\({}_{ 0.3}\)** \\ Melbourne & \(20.1_{ 4.0}\) & \(10.1_{ 1.9}\) & \(3.3_{ 0.2}\) & \(12.8_{ 0.4}\) & \(5.3_{ 0.7}\) & **0.2\({}_{ 0.1}\)** \\ Gridwatch & \(29.2_{ 5.5}\) & \(51.8_{ 15.1}\) & \(16.6_{ 1.9}\) & \(16.3_{ 1.8}\) & \(4.7_{ 0.5}\) & **3.6\({}_{ 0.0}\)** \\   

Table 1: Power(%) of a kernel two-sample test on 1D datasets. Lower is better.

Figure 4: Generated motion by the baseline (DDPM)  and HDM-SDE (ours).

[MISSING_PAGE_FAIL:9]

Related Works

Diffusion models in infinite-dimensional space have been actively investigated [18; 23; 29; 48; 49; 29] introduces a discrete-time denoising diffusion model in infinite-dimensional space, which provided one of the key inspirations for studying a continuous-time approach connecting with score-based methods through the SDE framework .  is another discrete-time approach proposed to generalize neural processes , but it utilizes non-trace class white noise that causes an ill-posedness in the infinite-dimensional setting. See  for a reference of Hilbert space valued Wiener processes.

A challenging part of the continuous-time approach in Hilbert space is defining the score function without using the density function that no longer exists in the infinite-dimensional setting.  proposes using a discretized process to circumvent this problem and  suggests a continuous-time model based on spectral decomposition stemming from the Karhunen-Loeve theorem; however, they do not include the general SDE framework  nor time-dependent operators. Based on ,  only considers constant-time diffusion coefficients, which do not fully recover the desired framework requiring variance scheduling . Section 2.3.1 shows the intersection between [19; 23] and our approach and how the proposed framework covers SDEs in infinite dimensions.

Concurrently, [49; 38] propose continuous-time models; however, their theoretical results are also confined to SDEs with constant-time operators since their approach is primarily grounded in semigroup theory , which has an intrinsic limitation when dealing with variable coefficients . We note that the multiple noise scale case is revealed in [38, Section 4.4], yet it is constrained to discrete-time models. On the contrary, our approach is grounded in the study of variational approach to stochastic evolution equations [34; 35] and forward and backward Kolmogorov equations with functional derivatives [6; 15], which serves as a more established approach to identifying relations (Theorem 2.1) between time-dependent evolution operators and invariant measures of stochastic equations in Hilbert spaces . It is noteworthy that proof of Theorem 2.1 (Appendix B.2) can be viewed as a generalization of the classical techniques employed by  and  in finite-dimensional spaces to general Hilbert spaces. Consequently it is able to fully recover the SDE framework proposed from , thereby enabling the variance scheduling , and building a bridge between the discrete-time model presented in IHDM  and the continuous-time score-based approach.

Similar to [11; 23; 29; 38], which combine diffusion models with neural operators [32; 37], we propose a modified _time-conditioned_ FNO for the HDM-SDE in the functional and motion generation task, which has slightly different modules from the original neural operator. For the image generation task with the HDM-SPDE, however, we use the U-Net  to compare with the setting in variations of diffusion models [4; 27; 50], which utilize image transformations instead of drift coefficients.

## 6 Limitation and Conclusion

We extend the SDE framework proposed by  and propose a class of continuous-time score-based generative models based on stochastic evolution equations in Hilbert spaces. Our derivation of the time-reversal formula considering time-dependent coefficients enlarges the practical application of score-based generative models in function spaces and advances the performance of recent variations of diffusion models which use blurring  or pixelation . Although performance gaps remain in image generation compared to the state-of-the-art results, we conclude that the proposed framework opens the potential for exploring various evolution operators and sample spaces in this area.