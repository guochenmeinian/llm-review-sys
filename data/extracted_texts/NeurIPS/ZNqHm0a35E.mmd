# Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection

Jieren Deng

1Institute of Automation, Chinese Academy of Sciences (CAS)

{dengjieren2019, jianhua.hu, zhanghaojian2014, yunkuan.wang}@ia.ac.cn

Haojian Zhang

Corresponding author Institute of Automation, Chinese Academy of Sciences (CAS)

{dengjieren2019, jianhua.hu, zhanghaojian2014, yunkuan.wang}@ia.ac.cn

Kun Ding

1Institute of Automation, Chinese Academy of Sciences (CAS)

{dengjieren2019, jianhua.hu, zhanghaojian2014, yunkuan.wang}@ia.ac.cn

Jianhua Hu

1Institute of Automation, Chinese Academy of Sciences (CAS)

{dengjieren2019, jianhua.hu, zhanghaojian2014, yunkuan.wang}@ia.ac.cn

Xingxuan Zhang

3Shanghai Sixth People's Hospital Affiliated to Shanghai Jiao Tong University School of Medicine

zhangxingxuan@sjtu.edu.cn

Yunkuan Wang

1Institute of Automation, Chinese Academy of Sciences (CAS)

{dengjieren2019, jianhua.hu, zhanghaojian2014, yunkuan.wang}@ia.ac.cn

###### Abstract

This paper presents Incremental Vision-Language Object Detection (IVLOD), a novel learning task designed to incrementally adapt pre-trained Vision-Language Object Detection Models (VLODMs) to various specialized domains, while simultaneously preserving their zero-shot generalization capabilities for the generalized domain. To address this new challenge, we present the Zero-interference Reparameterizable Adaptation (ZiRa), a novel method that introduces Zero-interference Loss and reparameterization techniques to tackle IVLOD without incurring a significant increase in memory usage. Comprehensive experiments on COCO and ODinW-13 datasets demonstrate that ZiRa effectively safeguards the zero-shot generalization ability of VLODMs while continuously adapting to new tasks. Specifically, after training on ODinW-13 datasets, ZiRa exhibits superior performance compared to CL-DETR and iDETR, boosting zero-shot generalizability by substantial **13.91** and **8.74** AP, respectively. Our code is available at https://github.com/JarintotionDin/ZiRaGroundingDINO.

## 1 Introduction

Object detection has achieved remarkable strides in recent years [2; 44]. However, most object detection models are typically trained to predict a predefined, closed set of categories [32; 2], constraining their prospect in real-world applications. Recent research endeavors [12; 27; 45; 19; 22] focus on developing intelligent systems capable of detecting objects specified by natural language inputs, giving rise to the field of Vision-Language Object Detection (VLOD) (also named Open Vocabulary Object Detection), also referred to as Open Vocabulary Object Detection. In this framework, the models are termed Vision-Language Object Detection Models (VLODMs). By incorporating natural language inputs, VLODMs can recognize a much broader set of visual concepts beyond a fixed category set, resulting in exceptional zero-shot generalizability.

Despite VLODMs' great zero-shot recognition ability in the general domain, VLODMs often exhibit suboptimal performance in more specialized domains, such as identifying aquatic organisms in aquariums or interpreting remote sensing images from aerial drones. In real-world scenarios, the necessity to adapt VLODMs to various unforeseen downstream tasks remains crucial in order to attain the desired accuracy, as highlighted in recent research [19; 45].

A straightforward idea is to adapt different individual VLODMs to different downstream tasks. However, a general agent often needs to simultaneously recognize objects from diverse tasks, eachlearned at distinct times. Moreover, in a dynamically changing environment, equipping VLODMs with the ability of incremental learning is paramount [20; 31], considering that downstream tasks typically arrive in a streaming manner and incremental learning paradigm better aligns with the learning and cognition processes of a general intelligent agent.

To endow VLODMs with the incremental learning ability, a naive idea is applying existing methods [10; 23] designed for detector on closed set directly. However, from our observations, it will weaken the excellent zero-shot generalization ability of VLODMs to detect unseen categories. To highlight the distinctiveness of incremental learning for VLODMs, we believe it is necessary to introduce a new task, Incremental Vision-Language Object Detection (IVLOD), which stresses the importance of maintaining the zero-shot generalization ability of VLODMs while adapting to various downstream tasks incrementally.

To further demonstrate the uniqueness of the IVLOD task, we have compared three different variants of adapting VLODMs to multiple downstream tasks: zero-shot learning (_i.e_., no adaptation), conventional incremental learning (applying CL-DETR  to VLODM), and zero-shot generalizable incremental learning (_i.e_., the IVLOD task) in Fig. 1. Unlike general incremental object detection (general IOD, ref. second column), IVLOD is characterized by preserving the original zero-shot performance of VLODM while also performing incremental learning. From this comparison, IVLOD faces two major challenges. One is the issue known as catastrophic forgetting , where the performance on previously learned tasks may sharply decline when new tasks are introduced. The other challenge is to maintain the zero-shot generalizability of VLODMs while learning new downstream tasks.

From a unified perspective, both challenges can be attributed to the forgetting problem. Technically, two kinds of methods can tackle this issue: selecting exemplar samples from past datasets for replaying [31; 23] or maintaining a duplicate model to facilitate knowledge distillation [20; 23]. In order to prevent forgetting, the replaying-based approach necessitates preserving enough examples to ensure the representativeness of the samples for replaying, but it is hard to do this with a limited-size memory, specifically when the model is pre-trained on a large-scale vision-language dataset. At the same time, current knowledge distillation techniques do not adequately focus on preventing the forgetting of pre-training knowledge, rendering them suboptimal for IVLOD tasks. Furthermore, they often need to store an entire model copy, which requires a large memory budget. Such storage demands are often untenable, particularly in scenarios that use resource-constrained edge devices.

To effectively address IVLOD's challenges while improving memory efficiency, we introduce a novel approach named Zero-interference Reparameterizable Adaptation (ZiRa). ZiRa retains the parameters of the original model and introduces a parallel dual branch structure named Reparameterizable Dual Branch (RDB) designed for efficient tuning on downstream tasks. The RDB structure is

Figure 1: Incremental Vision-Language Object Detection (IVLOD) aims to enhance VLODMsâ€™ performance across specialized domains via incremental learning, while also preserving their zero-shot generalization capability, enabling them to handle both known and unknown objects simultaneously and effectively.

reparameterizable, allowing the model to adapt to downstream tasks without increasing memory usage. Furthermore, the RDB structure serves a dual purpose--it achieves the branch labor division for downstream continual learning, protecting the learned knowledge from being excessively overwritten, and more crucially, it lays the structural foundation for the key novel element of ZiRa named Zero-interference Loss (ZiL).

ZiL simultaneously penalizes the output norms of both the entire RDB and a high-learning-rate branch within the RDB, guiding the RDB to learn in a direction that protects both knowledge learned from pre-training and downstream tasks. The idea of ZiL comes from the following insight: During pre-training, the model is trained with visual-language inputs that contain significant noise, and the model has learned how to handle this noise. As a result, the pre-trained VLODMs have a certain degree of robustness to the input, and they can handle a certain range of interference without their performance being affected. ZiL ensures that the fine-tuned residual input of RDB is of small norm, thereby guaranteeing that the model's original performance. At the same time, for adaptation, only a small adjustment to the input is needed to learn new concepts. Although ZiL constrains the input norm of the RDB, it still allows the RDB to acquire sufficient downstream knowledge. As a result, ZiL effectively addresses the two challenges of IVLOD at the same time. Crucially, ZiRa accomplishes this without necessitating the duplication of the entire model or any exemplars, thereby ensuring efficient utilization of memory.

Compared to existing methods [34; 23], ZiRa leverages the advantages offered by VLODMs more fully through a principled design (_i.e._, the ZiL based on RDB), offering a superior and more effective solution without the need for excessive additional memory. The contributions of this paper can be summarized as follows:

* We introduce Incremental Vision-Language Object Detection (IVLOD), a novel learning task that incrementally adapts VLODMs to multiple specialized domains while preserving zero-shot generalization ability.
* We present a novel approach called ZiRa to tackle IVLOD challenges. Based on the reparameterizable parallel tuning structure of RDB, ZiRa introduces ZiL to simultaneously minimize interference with zero-shot performance and prevent forgetting on downstream tasks. Notably, ZiRa makes these achievements in a memory-efficient manner.
* We conduct comprehensive experiments on COCO and ODinW-13 datasets, showcasing the effectiveness of our method. For instance, ZiRa outperforms CL-DETR  and iDETR  by a significant margin, making a **13.91** and **8.74** improvement in zero-shot AP on COCO, respectively, while offering competitive results on downstream tasks incremental learning.

## 2 Related Work

**Vision-Language (or Open Vocabulary) Object Detection.** Vision-Language (or Open Vocabulary) Object Detection Models combine natural language descriptions with object detection in images. Early research in this area primarily focused on grounding textual phrases to image regions, such as grounding referring expressions [28; 39; 4] and visual question answering [1; 25; 42]. More recent studies have developed unified and scalable approaches for simultaneous object detection and grounding tasks. For instance, GLIP  unifies phrase grounding and object detection tasks by treating object detection as context-free phrase grounding and treating phrase grounding as a contextualized object detection task. This approach enables the integration of multi-layered textual information into the detection model. Grounding DINO , based on the DINO , employs a DETR-like  architecture that uses a Transformer as the detector, which allows for unified processing of image and language data and offers the outstanding capacity to utilize large-scale datasets. Based on Grounding DINO, our work further addresses the newly introduced IVLOD tasks.

**Incremental Learning and Incremental Object Detection.** Incremental learning, also known as continual learning or lifelong learning, tackles the challenge of learning new tasks without forgetting previous ones. Incremental learning has given rise to two primary strategies, memory replay, and knowledge distillation, to tackle its inherent challenges [20; 31; 3; 5; 38; 37; 40]. For example, Learning without Forgetting (LwF)  employs knowledge distillation to maintain performance on old tasks while learning new ones. Another notable approach, iCaRL , combines exemplar memory and a nearest-mean-of-exemplars classification method for incremental learning. Incremental object detection, a specialized subfield within incremental learning, focuses on improving the adaptability of object detection models to new object categories while maintaining performance on previously learned categories [34; 17; 30; 41; 11; 10; 23]. ILOD  was the first to introduce incremental object detection. More recently, CL-DETR  effectively combined knowledge distillation and exemplar memory techniques to enhance incremental object detection using Transformer-based models. Our work distinguishes itself by performing incremental learning on VLODMs, which are more favorable for open-world problems. Besides the need to continually adapt across multiple specialized tasks, it is also important to preserve their zero-shot generalization ability. To the best of our knowledge, our work is the first to pose and tackle IVLOD tasks.

**Open-World Object Detection.** Open-World Object Detection (OWOD) [16; 13; 26] also targets the detection of both seen and unseen objects, but it involves simultaneous learning of new objects and detecting unknown objects during the incremental learning process. This means that as new tasks are introduced, the model is trained to recognize new objects while also developing its ability to detect unknown objects. In contrast, Incremental Vision-Language Object Detection (IVLOD) pre-trains models to detect unknown objects before the incremental learning process begins. The focus of IVLOD during incremental learning is on preserving the model's ability to detect these unknown objects while adapting to new tasks. Given the superior zero-shot generalization capabilities of current vision-language models, we believe that starting with a pre-trained vision-language model for IVLOD is a more effective approach compared to the traditional OWOD paradigm.

**Reparameterization.** Reparameterization techniques optimize model inference speed by transforming the training-time architecture into an equivalent, simplified form for inference [8; 9; 7]. RepVGG  exemplified this approach by employing \(3 3\) and \(1 1\) convolutional branches for training, which are streamlined to \(3 3\) convolutional branches and ReLU activations during inference. RepMLPNet  introduced "Locality Injection" to merge trained parameters of parallel convolutional kernels into fully connected layers, enhancing MLP-based model performance. Our work builds upon the Reparameterizable Dual Branch (RDB) concept initially proposed by . The key innovations introduced in our approach include the use of differentiated learning rates for the High Learning Rate Branch and Low Learning Rate Branch, and the implementation of Zero-interference Loss (ZiL). These enhancements allow for a more effective balance between learning new tasks and preserving previously acquired knowledge, distinguishing our method from prior work.

## 3 Methodology

In this section, we systematically present our approach. We begin with an overview (Sec. 3.1), and then introduce Zero-interference Reparameterizable Adaptation (ZiRa), which includes the Reparameterizable Dual Branch (RDB) (Sec. 3.2) and the Zero-interference Loss (ZiL) (Sec. 3.3).

### Overview

The framework of our model for addressing the IVLOD task is illustrated in Fig. 2, with the selected VLODM being Grounding DINO . Other VLODMs with a similar structure are also suitable. Specifically, the VLODM takes image features and category text prompts as inputs and produces class-agnostic proposals whose features are closely aligned with the text prompts. These text prompts are also refined to obtain text classification features by fusing the image features. Subsequently, the model classifies the class-agnostic proposals based on the cosine similarity between the visual features of proposals and the text classification features. These VLODMs are pre-trained on extensive grounding datasets annotated with detailed human language and object boxes, which equips them with great zero-shot recognition capabilities. To address the challenges of IVLOD, we propose ZiRa which is based on the structure of RDB and constrains the output of RDB by ZiL.

### Reparameterizable Dual Branch

**Adaption on Both Language and Vision Sides.** Fine-tuning the entire model is a common method for adapting the original model to downstream tasks. However, it has been demonstrated that tuning only a few newly introduced parameters in a pre-trained model is more effective [14; 15], while full fine-tuning often leads to significant forgetting. In view of this, we introduce additional parallel branches on both the language and vision sides of the VLODM to adapt the model for sequential downstream tasks.

As shown in Fig. 2, these additional branches are termed the Reparameterizable Dual Branch (RDB). On the language side, we integrate an RDB alongside a pre-trained linear layer positioned between the detector and the language backbone network, aiming to learn the new high-level semantic concepts. On the visual side, we also connect a supplementary RDB in parallel with a pre-trained convolutional layer located between the visual backbone and the detector, aiming to learn the visual features of new classes. It's pivotal to adapt on both sides, as these two sides capture distinct structures of vital knowledge and the lack of either will lead to insufficient adaptation to downstream tasks.

**Dual Branch Structure within the RDB.** Within the RDB, illustrated in Fig. 3, our design features a dual-branch structure, comprising the Low-learning rate Branch (LLRB) and the High-learning rate Branch (HLRB). The HLRB is set to a high learning rate for rapid task adaptation, whereas the LLRB employs a more conservative learning rate. Specifically, the LLRB is set at \(\) (\(0<<1\)) times the learning rate of the HLRB. This differentiation in learning rates brings division of labor between LLRB and HLRB, especially when combined with our next strategy that reparameterizes the HLRB into the LLRB after learning each task. The LLRB's low learning rate can help maintain downstream task knowledge, while the HLRB's high learning rate can help swift adaptation to new tasks. This division protects the knowledge stored in the LLRB from being excessively overwritten when incrementally learning downstream tasks, achieving a better balance between stability the plasticity, while the single branch or naive dual branch structure can not take advantage of this division mechanism.

Specifically, the collective output of the RDB \(x_{rdb}\), is formulated as:

\[x_{rdb}=(x) s+(x),\] (1)

where \(x\) is the input and \(s\) is a learnable scaling factor.

For the language side, the RDB is structured as two parallel linear layers, and its output \(x_{rdb}^{l}\)is expressed as:

\[x_{rdb}^{l}=(W_{hlrb}^{l}x+b_{hlrb}^{l}) s+W_{llrb}^{l}x+b_{llrb}^{l},\] (2)

where \(W_{hlrb}^{l}\) and \(W_{llrb}^{l}\) represent the weights of HLRB and LLRB, respectively, while \(b_{hlrb}^{l}\) and \(b_{llrb}^{l}\) denote their corresponding biases.

Figure 3: The structure of the Reparameterizable Dual Branch (RDB).

Figure 2: Our framework, features two Reparameterizable Dual Branch with Zero-interference Loss on both the vision and language sides.

For the vision side, the RDB is structured as two parallel convolution layers, and its output \(x^{v}_{rdb}\) is illustrated as:

\[x^{v}_{rdb}=(W^{v}_{hlrb} x+b^{v}_{hlrb}) s+W^{v}_{llrb} x+b^{ v}_{llrb}.\] (3)

Here, \(W^{v}_{hlrb}\) and \(W^{v}_{llrb}\) stand for the kernel of HLRB and LLRB, respectively, while \(b^{v}_{hlrb}\) and \(b^{v}_{llrb}\) represent their corresponding biases. \(\) denotes the convolution operation.

**Reparameterization from the HLRB to the LLRB.** With each new task, ZiRa resets the HLRB parameters to zero and, after completing learning on a new task, merges the trained parameters from the HLRB into the LLRB. This approach not only allows the LLRB to incrementally acquire new knowledge but also avoids the linear increase in the number of additional branches as the number of tasks increases, effectively managing memory consumption. Moreover, it offers a structural prerequisite for ZiL to prevent forgetting on downstream tasks by penalizing the output norm of HLRB, which will be detailed in the next section.

On the language side, ZiRa combines the HLRB and LLRB into a consolidated LLRB with new weights \(W^{l}_{hlrb} s+W^{l}_{llrb}\) and new biases \(b^{l}_{hlrb} s+b^{l}_{llrb}\), given by:

\[x^{l}_{rdb}=(W^{l}_{hlrb} s+W^{l}_{llrb})x+(b^{l}_{hlrb} s+b^{l}_{ llrb}).\] (4)

Similarly, on the vision side, the HLRB and LLRB are merged into a singular LLRB, as depicted by:

\[x^{v}_{rdb}=(W^{v}_{hlrb} s+W^{v}_{llrb}) x+(b^{v}_{hlrb} s+ b^{v}_{llrb}),\] (5)

where \(W^{v}_{hlrb} s+W^{v}_{llrb}\) and \(b^{v}_{hlrb} s+b^{v}_{llrb}\) can be considered as the new kernel and new biases.

**Single Branch for Inference.** Sub-branches inside the RDB like the LLRB and HLRB can increase computational overhead. We efficiently address this by reparameterizing all sub-branches in RDB and the frozen pre-trained branch into a new one for inference. Since the frozen pre-trained branch is also a linear structure, after learning all the tasks, we can create a new branch that merges the frozen pre-trained branch and the RDB via reparameterization. During inference, we exclusively use this newly consolidated branch, but we still retain the parameters frozen pre-trained branch and the RDB for future learning. This approach significantly reduces the computational cost during inference, as only one branch is required for prediction

### Zero-interference Loss

**Protect Zero-shot Generalizability with ZiL.** Preventing performance degradation on the original pre-training domain can be viewed as preventing the VLODM from forgetting the original pre-training domain knowledge. Two common methods to avoid forgetting is to reserve some old samples for replaying or distill the current model from the previous ones. However, preserving old samples and storing the previously trained model requires significant additional memory. On the contrary, Zero-interference Loss (ZiL) can prevent forgetting in a memory-efficient manner.

Specifically, ZiL penalizes the norm of the whole RDB's output (ref. Fig. 3), aiming at safeguarding the zero-shot performance of VLODM. The ZiL for the RDB, \(L_{rdb}\), is defined as follows:

\[L_{rdb}=L_{1}(x_{rdb}).\] (6)

Here, \(L_{1}\) corresponds to the \(L_{1}\) norm.

**Why ZiL works.** The effectiveness of ZiL can be explained as follows. First, the VLODM's input (What we fine-tuned in this paper) is robust to small-norm random perturbations, which have little impact on the model's performance. To verify this, we add Gaussian noise to the input of the pre-trained VLODM's detector and observe the model's performance. As shown in Fig. 4, the model's performance is not significantly affected by the addition of small-norm random noise to the input.

Second, the ZiL ensures that the fine-tuned input of the VLODM has a small norm additional term. To further investigate the effectiveness of ZiL, we plot curves illustrating the evolution of the RDB's output norm throughout the IVLOD process, as shown in Fig. 5. It is evident that training the RDB with ZiL leads to a substantial reduction in the output norm of the RDB on COCO, compared to training the RDB without ZiL. This preliminary observation strongly suggests that ZiL effectively preserves the zero-shot generalizability of VLODMs. Examining the evolution curves in Fig. 5 more closely, we observe that in the initial stages, ZiL plays a relatively minor role in curbing the growth of the RDB's output norm. However, as the process of accumulating new knowledge progresses, the influence of ZiL becomes more significant, leading to stronger constraint effects. This increase in ZiL's impact leads to a dynamic balance: the interference caused by integrating new knowledge is counteracted by the interference reduction achieved by ZiL, which is reflected in the fact that as the number of tasks increases, the norm of the RDB's output doesn't rise unboundedly but rather stabilizes after learning a certain number of tasks. This dynamic balance is crucial for maintaining the zero-shot generalizability of VLODMs.

**Prevent forgetting on downstream tasks with ZiL.** The \(L_{rdb}\) serves primarily to preserve the zero-shot generalization capability, leaving the challenge of downstream task forgetting unresolved. By exploiting the advantage of RDB, we address this challenge by further applying ZiL on HLRB, which is computed as:

\[L_{hlrb}=L_{1}((x) s),\] (7)

where \(s\) is a learnable scaling parameter.

The \(L_{hlrb}\) can effectively protect the knowledge that is learned from previous tasks and encompassed within the combined LLRB and the pre-trained branch. Specifically, given that the knowledge learned from the previous task has already been reparameterized into LLRB and LLRB learns at a lower learning rate, the total output of the branches previously trained can be regarded as \((x)+(x)\), where PTB is the pre-trained branch. The total output of the current branches is \((x)+(x) s+(x)\), which has an additional component \((x) s\) compared to the output from the branches trained on the previous task. Hence, optimizing \(L_{hlrb}\) equates to penalizing the discrepancy in the outputs between the models trained on the previous and the current task, effectively guiding the HLRB to learn new tasks without much downstream task forgetting.

**Total Optimization Objective.** For a general VLODM, we continue to employ its original training loss. To prevent forgetting, we incorporate an additional loss term, \(L_{zil}\). Taking Grounding DINO as an example, the total loss function becomes:

\[L_{total}=L_{cls}+L_{loc}+ L_{zil},\] (8)

where \(L_{zil}=L_{rdb}+L_{hlrb}\) is the proposed ZiL, \(\) is its loss weight, and \(L_{cls}\) and \(L_{loc}\) are the contrastive classification loss and localization loss of Grounding DINO. Throughout the training process, the entire original VLODM is frozen, except for the RDB.

## 4 Experiments

### Setup

**Datasets.** We conduct our experiments on the COCO  datasets and the "Object Detection in the Wild (ODinW)"  benchmark. ODinW is a more challenging benchmark designed to test model performance under real-world scenarios. It comprises numerous sub-datasets from various domains for evaluation, such as Thermal (to detect objects in heat map images) and Aquarium (to detect different marine animals). Following GLIP , we use ODinW-13 datasets, they are labeled as Ae

[MISSING_PAGE_FAIL:8]

* TFA : A classical linear-probing-based incremental few-shot object detection baseline.
* OW-DETR : An open-world object detection approach based on DETR, primarily addressing forgetting through exemplar replaying, which necessitates additional memory to store exemplars.
* CL-DETR : A DETR-based method employing refined knowledge distillation and exemplar replaying. This method requires a complete model copy and exemplars, incurring significant extra memory costs.
* iDETR : An incremental few-shot object detection method specifically focusing on tuning the projection layer of DETR-like detectors via knowledge distillation and self-supervision.
* Adapting-tuning (AT) : A parameter-efficient adaptation method that has demonstrated its effectiveness in incremental few-shot object detection .

We first compare the above methods implemented on the Grounding DINO model. Experiments are conducted under both few-shot and full-shot settings. Since CL-DETR and OW-DETR are not designed for few-shot IOD, we only compare them under the full-shot setting. The results presented in Tab. 1 demonstrate that ZiRa consistently outperforms existing IOD methods in terms of "Avg" on downstream tasks. Moreover, ZiRa exhibits remarkable performance in preserving the zero-shot generalization ability of VLODMs under both the few-shot and full-shot settings. In particular, ZiRa surpasses CL-DETR and iDETR by substantial margins, with improvements of **13.91** and **8.74** AP under the full-shot setting in terms of "ZCOCO", respectively.

We then compare ZiRa with existing methods implemented on the OV-DINO model. The results in Tab. 2 show that ZiRa outperforms existing IOD methods in terms of "Avg" on downstream tasks. ZiRa also demonstrates excellent performance in preserving the zero-shot generalization ability of VLODMs under the full-shot setting. Specifically, ZiRa surpasses CL-DETR and iDETR by significant margins, with improvements of 14.55 and 11.36 AP in the full-shot setting for "ZCOCO," respectively.

These results emphasize ZiRa's effectiveness in maintaining the zero-shot AP and its potential as a superior solution for IOD. Distinctively, ZiRa, unlike CL-DETR and OW-DETR, requires minimal extra memory for a few branches without storing image exemplars and the model copy, making it more memory-efficient than other IOD methods.

### Ablation Study

**Main Components of ZiRa.** Our approach combines the ZiL and the RDB to counteract forgetting in both the general domain and downstream tasks. In this study, we analyze the impact of these components. We utilize "Rep+" to denote whether to reparameterize the HLRB into LLRB after each task as well as using differentiated learning rates. Please note that even when we do not use "Rep+", we still retain the dual-branch structure like RDB. ZiL encompasses the RDB's ZiL (\(L_{rdb}\)) and HLRB's ZiL (\(L_{hlrb}\)), here we separate them and study their impact separately. We also evaluate performance with the "hAP" metric, which is the harmonic mean of "ZCOCO" and "Avg".

The results in Tab. 3 provide valuable insights. First, the comparison between the first and second rows demonstrates that "Rep+" can mitigate forgetting on both pre-training and downstream tasks. This effect comes from the labor division between branches that are built on differentiated learning rates and reparameterization. Second, the comparison between the first row and third rows highlights that ZiL (\(L_{rdb}+L_{hlrb}\)) significantly enhances the "ZCOCO". However, it cannot address forgetting on downstream tasks without "Rep+", and it even reduces the "Avg" since it also limits the plasticity

  Rep+ & \(L_{rab}\) & \(L_{drb}\) & ZCOCO & Avg & HAP & Ae & Ao & Co & Eg & Mu & Pa & Pv & Fr & Po & Ra & Sh & Th & Ve \\  \(\) & \(\) & \(\) & 39.72 & 58.12 & 47.19 & 30.26 & 47.11 & 69.30 & 66.16 & 56.82 & 58.20 & 64.34 & 69.08 & 48.48 & 63.35 & 41.53 & 76.59 & 64.37 \\ \(\) & \(\) & \(\) & 42.11 & 59.98 & 49.48 & 32.17 & 48.37 & 70.97 & 69.28 & 59.32 & 58.05 & **66.39** & **71.50** & 51.35 & 67.46 & 42.01 & 78.15 & 64.71 \\ \(\) & \(\) & \(\) & **46.09** & 54.24 & 49.83 & 30.14 & 33.84 & 68.80 & 65.29 & 42.27 & **60.20** & 61.42 & 69.54 & 34.79 & 64.99 & 39.31 & 74.41 & 60.12 \\ \(\) & \(\) & \(\) & 46.06 & 59.73 & **52.01** & **32.81** & 48.19 & 70.33 & 69.67 & 59.33 & 58.05 & 64.04 & 70.67 & 50.06 & 67.49 & 45.51 & 76.76 & 63.54 \\  \(\) & \(\) & \(\) & 46.01 & 58.43 & 51.48 & 30.77 & **48.91** & 69.69 & 68.93 & 51.98 & 58.05 & 61.80 & 70.58 & 45.71 & 67.72 & 43.35 & 78.19 & 63.95 \\ \(\) & \(\) & \(\) & 44.93 & **60.83** & 51.68 & 32.62 & 47.82 & **71.57** & **70.06** & **60.13** & 58.05 & 66.33 & 71.03 & **52.72** & **68.59** & **47.42** & **78.28** & **66.20** \\  

Table 3: Main ablation results.

of the RDB. The best results lie in the fourth row, underscoring that combining "Rep+" and ZiL can effectively mitigate forgetting on both pre-training and downstream tasks to a greater extent. Conversely, the results in the last row illustrate that using (\(L_{rdb}\)) alone does not optimally address forgetting on downstream tasks. Comparing the second row and the last row, we can find that \(L_{hlrb}\) can improve both "ZCOCO" and "Avg", showcasing that \(L_{hlrb}\) can mitigate forgetting both on downstream tasks and pre-training, but without \(L_{rdb}\), using \(L_{hlrb}\) alone can not achieve the best "hAP".

**Learning on Different Modalities with ZiRa.** We carried out a series of experiments to investigate the effects of learning only on the vision side using parallel convolution layers (indicated as "V") and only on the language side using parallel linear layers (denoted as "L"). The results in Tab. 4 demonstrate that ZiRa can function independently on either the language or vision side, and it can also effectively function when simultaneously applied to both modalities, highlighting the generality of ZiRa's impact. Since the language and vision side learning acquires knowledge in distinct structures, neglecting either can hinder optimal learning. Consequently, simultaneously learning on both sides can outperform individual tuning on either the vision or language sides when employing ZiRa to avoid forgetting.

**Branch Structure.** To study the impact of the additional branch structure introduced in VOLDM, we compared the results of introducing a single branch (denoted as SB), dual branches (denoted as DB), and RDB (DB + Rep+). Besides RDB, both the output norm of SB and DB are also penalized with ZiL. As we can see in Tab. 5, the SB structure, due to its inability to introduce inter-branch division of labor, falls short in its ability to prevent forgetting in downstream tasks compared to the RDB. Moreover, we can see that merely introducing DB with ZiL cannot prevent forgetting on downstream tasks, on the contrary, it brings worse forgetting due to increased plasticity. RDB not only incorporates the mechanism of branch labor division but also fully utilizes \(l_{hlrb}\) to learn to minimize the interference of HLRB, thereby more effectively preventing the forgetting on downstream tasks.

## 5 Conclusion

This paper presents a novel learning task, Incremental Vision-Language Object Detection (IVLOD), which aims to continually adapt Vision-Language Object Detection Models (VLODMs) to multiple specialized domains while preserving VLODMs' zero-shot generalization ability. To solve this new task in a memory-efficient way, we introduce Zero-interference Reparameterizable Adaptation (ZiRa). ZiRa inserts Reparameterizable Dual Branche (RDB) on the both language side and vision side of the VLODM and constrains the RDB by Zero-interference Loss (ZiL) to protect the original generalizability of VLODMs and prevent forgetting on downstream tasks at the same time. Notably, ZiRa eliminates the need for saving the entire model copy for distillation or maintaining exemplars for replaying, which makes it a highly memory-efficient method. Extensive experiments conducted on the COCO and ODinW-13 datasets showcase the superiority of ZiRa for IVLOD.

**Limitations.** While ZiRa has demonstrated its effectiveness in IVLOD tasks, there are still some limitations. First, the current implementation of ZiRa is based on the DETR architecture, which may not be optimal for all VLODMs. Second, the current ZiRa method is designed for the IVLOD task, and its generalization to other incremental learning tasks remains to be explored.

  V & L & ZiRa & ZCOCO & Avg & hAP \\  âœ— & âœ— & âœ— & **47.37** & 46.70 & 47.03 \\ âœ“ & âœ— & âœ— & 39.44 & 58.23 & 47.03 \\ âœ— & âœ“ & âœ— & 45.83 & 54.77 & 49.90 \\ âœ“ & âœ“ & âœ— & 39.72 & 58.12 & 47.19 \\  âœ“ & âœ— & âœ“ & 45.96 & 57.64 & 51.14 \\ âœ— & âœ“ & âœ“ & 46.41 & 56.49 & 50.96 \\ âœ“ & âœ“ & âœ“ & 46.06 & **59.73** & **52.01** \\  

Table 4: Comparison of learning on different modalities.

  Structure & ZCOCO & Avg & hAP \\  SB & **46.16** & 55.07 & 50.22 \\ DB & 46.09 & 54.24 & 49.83 \\ RDB & 46.06 & **59.73** & **52.01** \\  

Table 5: Comparison of different additional branch structures. We compared the results of introducing a single branch (denoted as SB), dual branches (denoted as DB), and RDB (DB + Rep+).