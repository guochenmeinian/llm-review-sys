# Calibrating Reasoning in Language Models with Internal Consistency

Zhihui Xie\({}^{}\)\({}^{@sectionsign}\)   Jizhou Guo\({}^{}\)   Tong Yu\({}^{}\)   Shuai Li\({}^{*}\)\({}^{}\)

\({}^{}\)Shanghai Jiao Tong University  \({}^{}\)Adobe Research  \({}^{@sectionsign}\)The University of Hong Kong

zhxieml@gmail.com shuaili8@sjtu.edu.cn

Corresponding author.

###### Abstract

Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious mistakes and contradictions, raising doubts about their ability to robustly process and utilize generated rationales. In this work, we investigate reasoning in LLMs through the lens of internal representations, focusing on how these representations are influenced by generated rationales. Our preliminary analysis reveals that while generated rationales improve answer accuracy, inconsistencies emerge between the model's internal representations in middle layers and those in final layers, potentially undermining the reliability of their reasoning processes. To address this, we propose _internal consistency_ as a measure of the model's confidence by examining the agreement of latent predictions decoded from intermediate layers. Extensive empirical studies across different models and datasets demonstrate that internal consistency effectively distinguishes between correct and incorrect reasoning paths. Motivated by this, we propose a new approach to calibrate reasoning by up-weighting reasoning paths with high internal consistency, resulting in a significant boost in reasoning performance. Further analysis uncovers distinct patterns in attention and feed-forward modules across layers, providing insights into the emergence of internal inconsistency. In summary, our results demonstrate the potential of using internal representations for self-evaluation of LLMs.

## 1 Introduction

Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning (Wei et al., 2022; Merrill and Sabharwal, 2024). This approach directs the model to articulate step-by-step rationales before answering a question, simulating the reasoning process used by humans. With these verbalized rationales, it is expected that not only will the model's problem-solving capabilities be enhanced, but also the interpretability of its predictions will improve. Understanding how LLMs reason is essential for aligning them with human values (Bai et al., 2022; Li et al., 2024).

Despite the continued improvement in performance and the emergence of new capabilities, LLMs often generate text with obvious mistakes and contradictions, raising doubts about their ability to robustly process and utilize generated rationales (Ye and Durrett, 2022; Turpin et al., 2023). One notable failure mode is unfaithful reasoning, where LLMs provide rationales that contradict their final predictions (Lyu et al., 2023; Lanham et al., 2023). This makes it difficult to determine the trustworthiness of their predictions, highlighting the need for effective calibration methods to assess the reliability of rationales.

In this work, we present the first exploration of leveraging internal representations to calibrate reasoning in LLMs. Our study is based on the intuition that the inner workings of LLMs contain latent structures that can assess the reliability of reasoning paths. To investigate this, we begin by probing the model's intermediate activations for answer correctness in CoT reasoning. Our preliminary analysis reveals that CoT reasoning leads to inconsistencies between the model's internal representations in middle layers and those in final layers, potentially indicating a degree of uncertainty.

Based on this observation, we propose _internal consistency_ (illustrated in Figure 1) as a measure of the model's confidence in the reasoning process. Inspired by previous works on eliciting latent predictions from intermediate layers (nostalgebraist, 2020; Belrose et al., 2023), we examine internal consistency by assessing the agreement of latent predictions decoded from intermediate layers. Unlike methods that require additional training and human annotations (Ye and Durrett, 2022; Khalifa et al., 2023), internal consistency provides a reliable and off-the-shelf self-evaluation for reasoning.

We conduct extensive empirical studies on various reasoning tasks, including reading comprehension, symbolic reasoning, and logical reasoning. Our analysis across different models and datasets demonstrates that internal consistency effectively distinguishes between correct and incorrect reasoning paths. Moreover, by up-weighting reasoning paths with high internal consistency, we achieve a significant improvement in reasoning performance. These results highlight the potential of using internal representations for the self-evaluation of LLMs.

This work makes three main contributions. 1) We identify the emergence of inconsistencies between intermediate and final layer representations in LLM reasoning, highlighting a potential issue where CoT reasoning leads to higher uncertainty. 2) We propose internal consistency as a novel measure to evaluate the model's confidence and calibrate reasoning by up-weighting paths with high internal consistency. 3) We provide insights into the cause of internal inconsistency in reasoning by analyzing Transformer components (i.e., attention and feed-forward networks) across layers. We believe our results show promise in leveraging internal representations to enhance reasoning in LLMs.

## 2 Preliminaries

In this section, we present background information, notations, and preliminary analysis to provide context for our study.

### Transformer Architecture

Our analysis focuses on the prevalent decoder-only Transformer architecture (Vaswani et al., 2017; Radford et al., 2018). To set notation and context, we briefly describe the key components as follows.

Figure 1: **An illustration of internal consistency.** Given a true-or-false question with the ground truth being true, we elicit latent predictions (i.e., predictions decoded from intermediate layers, represented by “T” for true and “F” for false) from the answer token of reasoning paths. By defining internal consistency as the agreement of latent predictions with the final stated one, we observe a high correlation between internal consistency and prediction accuracy, which aids in calibrating reasoning. Note that the figure on the right is synthesized for illustration purposes, but the distributions reflect actual observations as shown in Figure 3.

Given a sequence \(=(x_{1},,x_{n})\) of input tokens, the inference process of the Transformer begins by projecting these tokens into a sequence of representations \(_{1}^{0},,_{n}^{0}^{d}\). This representation is then updated by a series of \(L\) residual blocks (Elhage et al., 2021), each consisting of a multi-head self-attention (MHSA) layer followed by a feed-forward network (FFN) layer2. In each block \([0,L-1]\), the representation of each token \(i\) is updated as follows:

\[_{i}^{+1}=_{i}^{}+^{}(_ {i}^{}+^{}(_{i}^{})).\]

Finally, the next token distribution is produced by applying an unembedding on \(_{i}^{L}\):

\[p(x_{n+1})=((_{i}^{L} ))_{x_{n+1}}.\]

As the dimension of \(_{i}\) remains constant across layers, we can view it as a dynamic distribution processed by the model (Geva et al., 2022), providing insight into how LLMs reason internally.

### Preliminary Analysis of Internal Representations in CoT Reasoning

Building upon works that explore the inner workings of LLMs (Burns et al., 2023; Ferrando et al., 2024), our study starts with exploring the encoded information in internal representations during reasoning. In the context of CoT prompting--a prevalent technique for eliciting reasoning capabilities of LLMs--we analyze these representations along two dimensions: _horizontally_ across reasoning steps, and _vertically_ across network layers.

We conduct preliminary experiments using the Llama-2-7B model (Touvron et al., 2023) on the PrOntoQA dataset (Saparov and He, 2023), which challenges the model to determine whether a proposition is true based on a series of assumptions. We apply 4-shot CoT prompting and greedy decoding to generate reasoning paths and predictions. To discern encoded information crucial for reasoning, we utilize linear probing (Alain and Bengio, 2016; Belinkov, 2022) to train classifiers on the activations with respect to the ground truth labels, aiming to differentiate between true and false propositions. Reasoning paths are segmented into individual steps using the NLTK punkt sentence tokenizer (Bird et al., 2009), with a dedicated probe classifier for each layer at the last token of every reasoning step. Implementation details are provided in Appendix B.1.

Figure 2 presents an evaluation of the probe accuracy on the validation set, including a detailed zoom-in on the probe accuracy at the last two reasoning steps, which reveals two distinct patterns:

1. **Improved accuracy through verbalized reasoning:** Probe accuracy increases monotonically as the model processes and verbalizes reasoning paths, indicating that LLMs extract and utilize task-solving information from their generated rationales.
2. **Emergent inconsistency across layers:** A closer examination at the last two reasoning steps reveals a notable inconsistency in probe accuracy between middle and later layers. This inconsistency suggests that while middle layers capture essential reasoning information, it may not be fully utilized or maintained by the later layers, potentially impacting the model's overall reasoning performance.

This analysis provides an initial understanding of how LLMs internally process and handle information during CoT reasoning. While LLMs effectively gather information from verbalized reasoning paths, there is a noticeable tendency for these models to underutilize the processed information at intermediate layers. This motivates us to further investigate into the internal representations of LLMs and explore methods to better harness them to enhance reasoning capabilities.

Figure 2: **CoT reasoning improves answer accuracy but exacerbates the inconsistency between hidden and stated reasoning.** Left: Heatmap of linear probe accuracies at all reasoning steps and intermediate layers. Right: A zoom-in on the results of the last two steps.

### Calibration

Calibration refers to the alignment between a model's predicted probability estimates and their actual prediction correctness (Guo et al., 2017; Geng et al., 2023). Calibration is crucial for assessing the reliability and trustworthiness of LLMs, and can also help improve performance (Wang et al., 2022).

More formally, we want to find a measure \(\) such that for any two prompt-answer-prediction triplets \((_{i},_{i},}_{i})\) and \((_{j},_{j},}_{j})\), the following holds:

\[(_{i},}_{i})(_{j},}_{j}) p( }_{i}=_{i}_{i}) p(}_{j}=_{j}_{j}).\]

In this work, we reveal the potential of using internal representations for calibration and introduce a new measure based on internal consistency, as described in Section 3.2. The new measure does not require additional training and is more accurate than logit-based methods.

## 3 Internal Consistency as a Lens to Calibrate Reasoning

In Section 2.2, we uncover patterns of internal inconsistency in reasoning with linear probes. While probe accuracy indicates the "extractability" of task-solving information, it does not necessarily show how LLMs process internal representations for generation (Alain and Bengio, 2016; Belinkov, 2022). Therefore, we introduce a new method to measure internal consistency in this section.

### Interpreting Internal Representations

The internal reasoning process of a Transformer towards its final prediction can be comprehended through iterative inference (Jastrzebski et al., 2017; Geva et al., 2022). A direct method to inspect this internal process during generation is to early exit from intermediate layers. Specifically, instead of applying unembedding on \(_{i}^{L}\), we can obtain the token distribution over the vocabulary from any intermediate layer using the logit lens (nostalgebraist, 2020):

\[p^{}(x_{n+1})=((_ {i}^{}))_{x_{n+1}}.\]

Building on this interpretation, we can decode _latent predictions_ from any layer \(\):

\[_{n+1}^{}=_{x}p^{}(x).\] (1)

While Equation 1 provides a measurement of latent predictions, we find that the decoded distributions are often miscalibrated, biasing towards specific answers (Zhao et al., 2021; Belrose et al., 2023). For instance, in the PrOntoQA task with Llama-2-7B, the penultimate layer consistently assigns over 90% probability to the answer "True," regardless of the context. To address this, we balance the latent predictions across the possible answers for each layer separately. See Appendix B.4 for more details.

### Calibration Measurement with Internal Consistency

Built on our findings in Section 2.2, we investigate consistency of internal representations as an indication of uncertainty. Intuitively speaking, if the model's internal representations are highly inconsistent with those of later layers, it may not faithfully say as it thinks. Based on these intuitions, we propose a simple metric called _internal consistency_ to measure the agreement of latent predictions.

Internal ConsistencyDuring inference, we collect latent predictions at the answer tokens and measure how often latent predictions match the final prediction:

\[(,})=_{ =1}^{L-1}\{}^{}=}^{L}\}.\] (2)

This measure offers a straightforward method to gauge the level of internal consistency, serving as a useful indicator of the output's correctness (i.e., calibration), without requiring additional training.

Calibrating Reasoning with Internal ConsistencyAs LLMs often require long reasoning paths to resolve complex tasks (Wei et al., 2022), a process that introduces cumulative errors and uncertainty, we integrate this consistency measure into the generation process to enhance reasoning calibration. Specifically, we propose a new decoding method that assigns weights to each sampled reasoning path based on their calculated consistency score at the final answer token. Paths that exhibit higher internal consistency--indicating robust alignment between intermediate and final predictions--are assigned greater weight in determining the final model output. This weighted strategy aims to prioritize reasoning paths that not only maintain self-consistency but are also more likely to converge on the correct answer, thus potentially enhancing the model's accuracy in complex reasoning tasks.

## 4 Experiments

To study internal consistency, we conduct extensive experiments to address the following questions: 1) To what degree does internal consistency correlates with reasoning performance? 2) Can we leverage internal consistency to boost reasoning? 3) How do different components of the Transformer architecture contribute to the emergence of internal inconsistency?

### Setup

We evaluate internal consistency across a spectrum of models and tasks. See Appendix B.2 for further details of the experimental setup.

ModelsOur experiments are conducted using two prominent series of open-source Transformer-based models: the Llama-2 series (Touvron et al., 2023) and the Mistral series (Jiang et al., 2023). Specifically, we evaluate both the 7B and 13B configurations of the Llama-2 series to understand how model scale impacts internal consistency. Additionally, for the Mistral series, we explore the 7B and the 8\(\)7B versions, the latter of which incorporates mixture of experts layers (Jiang et al., 2024). These models were chosen due to their extensive use and distinct architectural features, enabling a comprehensive evaluation of internal consistency.

DatasetsWe evaluate various datasets that span reading comprehension, symbolic reasoning, and logical reasoning tasks. We choose these datasets because they involve explicit reasoning processes with unambiguous single-token answers (i.e., "True" and "False"), which facilitates our analysis. To standardize the evaluation process, each dataset is transformed into a true-or-false QA format. For a detailed description of the datasets and examples, please refer to Appendix B.2.

1. BoolQ (Clark et al., 2019): A reading comprehension dataset where each instance involves a yes/no question grounded in a related passage.
2. CoinFlip (Wei et al., 2022): A dataset that challenges the model's symbolic reasoning abilities by presenting a task where the model must determine the outcome of a coin (heads or tails) after a series of flips.
3. PrOntoQA (Saparov and He, 2023): A dataset designed for logical reasoning. Each question requires a 3-hop deduction to determine the truth value of a proposition based on a set of assumptions with fictional concepts.
4. ProofWriter (Tafjord et al., 2020): A logical reasoning dataset that, in contrast to PrOntoQA, uses real concepts for all assumptions. Each question requires 3 hops of reasoning.

We balance the labels and each dataset has at least 500 samples for evaluation. Our method requires no training procedure. To evaluate reasoning performance, we use calibrated accuracy following Zhao et al. (2021); Burns et al. (2023), balancing predictions to be 50/50 across the two labels.

### Internal Consistency is a Good Calibration Measure

Our analysis begins by examining patterns of internal consistency across different layers of LLMs. As illustrated in Figure 6 (Appendix C), latent predictions exhibit variable convergence during inference; notably, there are significant increases in consistency in the middle and final layers, while the early layers are characterized by considerable noise. Additionally, these patterns vary across models and datasets. Smaller models, such as Llama-2-7B, show high variability in latent predictions across layers, indicating less stability in their reasoning processes. In contrast, larger models like Mikral 8\(\)7B demonstrate more robust internal reasoning, as evidenced by more reliable convergence of latent predictions. For BoolQ, the inconsistency is less pronounced compared to other datasets that require more complex symbolic and logical reasoning.

Furthermore, the correlation between internal consistency and model accuracy is particularly evident when distinguishing between correct and incorrect answers in CoT reasoning. As depicted in Figure 3, the impact of prompting techniques on internal consistency is significant, highlighting the emergence of inconsistency brought by CoT prompting as discussed in Section 2.2. While few-shot demonstrations generally enhance consistency, CoT prompting decreases it. A closer examination of CoT reasoning reveals that reasoning paths leading to incorrect answers typically exhibit lower internal consistency. These results underscore the potential of internal consistency not only as a diagnostic tool but also as a reliable measure for calibrating reasoning in LLMs.

### Enhancing Reasoning with Internal Consistency

Our analysis reveals a strong correlation between internal consistency and prediction accuracy, suggesting its potential as a mechanism for improving reasoning performance. As demonstrated in Section 4.2, CoT prompting, while effective, tends to decrease internal consistency. This decrease is possibly attributable to the generated reasoning paths that incorporate incorrect rationales, which confuse the model to make inconsistent predictions across layers. To investigate the generality of this phenomenon and its potential solutions, we also least-to-most (L2M) prompting (Zhou et al., 2022), which decomposes complex problems into simpler sub-problems for reasoning. For a detailed description of our least-to-most decomposition approach, please refer to Appendix B.3.

To incorporate internal consistency, we build on the self-consistency (SC) approach (Wang et al., 2022), which utilizes an ensemble of multiple sampled reasoning paths to increase accuracy. Our proposed method, which we term SC+IC, integrates internal consistency to weight these reasoning paths accordingly. Specifically, if a reasoning path demonstrates high internal consistency in its answer prediction, the probability of accepting its corresponding answer as the final answer increases. We accumulate the sum of internal consistency scores across all reasoning paths for each answer and choose the answer with the highest sum. For baselines, we also compare with greedy decoding Greedy and a logit-based approach SC+\(\)(Wang and Zhou, 2024) that selects the final answer based on a confidence measure \(_{k}=p(}^{1})-p(}^{2})\), where \(}^{1}\) and \(}^{2}\) represent the top two tokens for answer prediction of the \(k\)-th path.

Table 1 summarizes the results, which demonstrate that SC+IC consistently outperforms the others across different models and tasks, with improvements ranging between 1.8% to 4.9% across models. Figure 4 plots the calibrated accuracy with respect to varying numbers of sampled paths. We observe clear improvements of SC+IC over the baselines, highlighting the effectiveness of leveraging internal

Figure 3: **Internal consistency is a reliable measure of prediction confidence in CoT reasoning. From left to right: 1) the effect of different prompting techniques on the model’s internal consistency; 2) the distribution discrepancy of internal consistency between correct and incorrect model predictions; 3) pattern variations in agreement values (representing the ratio of data instances where the latent predictions match the final predictions) across layers; and 4) a calibration plot with bins according to the model’s internal consistency on the x-axis and the accuracy within each bin on the y-axis. Results are averaged over all models and datasets. See Appendix C for the full results.**

consistency to enhance reasoning. This improvement is particularly notable in tasks involving symbolic and logical reasoning, which depend heavily on the correctness of reasoning paths. In addition, we observe that internal consistency effectively distinguish between correct and incorrect paths (e.g., in the PrOntoQA examples in Table 2, internal consistency helps filter out flawed rationales), validating our hypothesis on why internal consistency can enhance CoT reasoning.

### Layer-weighted Aggregation Finds Transferable Patterns

While results of SC+IC have demonstrated the effectiveness of leveraging latent predictions for reasoning, the relative importance of different layers remains unexplored. Recent studies have shown that certain intermediate layers specialize in specific types of reasoning (Yang et al., 2024). These observations suggest that treating all layers equally when computing internal consistency might be suboptimal. Motivated by these insights, we introduce an aggregation strategy for internal consistency with two variants: SC+IC (tune) and SC+IC (transfer). Specifically, instead of assigning equal weights to each layer in Equation 2, we consider a learned weight vector \(^{L}\) (where \(L\) is the number of layers) for aggregation. For SC+IC (tune), we optimize layer weights using 500 held-out samples per dataset. The weights are learned using Adam optimizer with a learning rate of 0.01 for 1,000 iterations. SC+IC (transfer) evaluates cross-task generalization by applying weights tuned on PrOntoQA to other datasets.

The results are presented in Table 1 and Figure 4. Despite introducing only \(L\) parameters, this weighted approach yields substantial improvements. Notably, the strong performance of SC+IC

    &  &  &  &  &  \\  & CoT & L2M & CoT & L2M & CoT & L2M & CoT & L2M & CoT & L2M \\   & & & & & & & & & \\  Greedy & 67.1 & 61.4 & 71.2 & 81.9 & **51.7** & 51.1 & 52.4 & 60.7 & 60.6 & 63.8 \\ SC & 70.0 & 73.2 & 76.0 & 89.2 & 49.0 & 50.0 & 52.2 & 66.8 & 61.8 & 69.8 \\ SC+\(\) & 69.9 & 73.0 & 76.2 & 90.1 & 50.2 & 50.1 & 52.4 & 68.2 & 62.2 & 70.3 \\ SC+IC & 70.7 & 73.4 & 77.6 & 90.2 & 49.0 & 51.2 & 52.2 & 69.0 & 62.4 & 71.0 \\ SC+IC (tune) & **71.5** & **73.5** & 78.6 & **93.9** & 50.8 & **55.7** & **54.1** & **70.7** & **63.8** & **73.4** \\ SC+IC (transfer) & **71.5** & **73.5** & **78.7** & **93.9** & 50.8 & **55.7** & 53.9 & **70.7** & **63.8** & **73.4** \\   & & & & & & & & & & \\  Greedy & 78.5 & 71.3 & 78.9 & 90.5 & 53.6 & 54.0 & 60.8 & 78.1 & 67.9 & 73.5 \\ SC & 80.8 & 78.3 & 81.0 & 94.2 & 52.6 & 55.4 & 61.0 & 88.6 & 68.9 & 79.1 \\ SC+\(\) & 81.2 & 78.9 & 81.8 & 94.2 & 55.0 & **57.6** & 61.6 & 87.9 & 69.9 & 79.6 \\ SC+TC & **81.4** & 78.7 & 84.0 & 93.2 & **55.4** & 57.0 & 61.8 & 88.6 & 70.6 & 79.4 \\ SC+IC (tune) & **81.4** & **80.4** & 86.2 & **96.5** & 54.5 & 56.6 & **62.9** & 89.6 & **7(transfer) across diverse tasks indicates that optimal layer importance patterns generalize well, demonstrating the broad applicability of internal consistency for reasoning tasks.

### Investigating the Role of Transformer Components in Internal Inconsistency

Having observed that CoT prompting exaggerates internal inconsistency and that reasoning performance can be improved by re-weighting reasoning paths based on internal consistency, we now aim to understand why internal inconsistency emerges from CoT reasoning. Specifically, we explore how different network components contribute to the final predictions in CoT reasoning and, consequently, the calibration effect of internal consistency. One hypothesis is that this effect arises from better elicitation of information processed in the middle layers, which tends to be underutilized in later layers. To investigate this, we conduct a detailed analysis focusing on the roles of self-attention and feed-forward network (FFN) layers.

Self-attention LayersWe analyze the average attention weights across all attention heads at each layer, focusing on what the answer tokens attend to. Consistent with the CoT process, we segment the processed tokens into three parts: the context, the query, and the generated rationale. Further details on the methodology and calculations can be found in Appendix B.5.

FFN LayersFor FFN layers, we use a probe vector trained on the model's last hidden state of the answer token with respect to the model's output. We compute cosine similarities between this probe vector and the _value vectors_(Geva et al., 2022; Lee et al., 2024), each corresponding to a column

}  
**Question**: Zumpuses are not transparent. Each zumpus is a wumpus. Wumpuses are red. Each wumpus is a jompus. Impuses are amenable. Every jompus is not spicy. Jompuses are tumpuses. Every tumpus is floral. Tumpuses are yumpus. Every yumpus is not amenable. Every yumpus is a vumpus. Each vumpus is metallic. Every vumpus is a numpus. Numpuses are temperate. Numpuses are dumpuses. Dumpuses are not small. Dumpuses are rompuses. Polly is a jompus. True or false: Polly is amenable. \\ 
**Sampled Paths**: **P1 (IC=0.875)**: Polly is a jompus. Jompuses are tumpuses. Polly is a tumpus. Every tumpus is floral. Tumpuses are yumpuses. Polly is a yumpus. Every yumpus is not amenable. Polly is not amenable. False \\ 
**P2 (IC=0.656)**: Polly is a jompus. Jompuses are impulses. Impuses are amenable. Polly is amenable. True \\ 
**Ground-truth Path**: Polly is a jompus. Jompuses are tumpuses. Polly is a tumpus. Tumpuses are yumpus. Polly is a yumpus. Every yumpus is not amenable. Polly is not amenable. False \\   

Table 2: **Examples where internal consistency helps select correct reasoning paths for Mistral-7B on PrOntoQA. We show the paths with the highest and lowest internal consistency. The question has been reformatted for better presentation. Incorrect steps are colored in gray, whereas wrong predictions are highlighted in red.**

Figure 4: **Internal consistency brings larger gains for complex reasoning tasks. The figure shows calibrated accuracy as a function of the number of votes for three types of tasks: reading comprehension (BoolQ), symbolic reasoning (CoinFlip), and logical reasoning (PrOntoQA and ProofWriter). Results are averaged over all models and random seeds.**

in the last FFN matrix of the layer. Our analysis focuses on the vectors with the top 0.1% of cosine similarity values, which play a pivotal role in forming final predictions. For more details, please see Appendix B.6.

ResultsFigure 5 presents our findings: 1) Self-attention layers in the middle layers show a marked focus on the query and reasoning steps, a pattern consistent across both models and datasets; 2) FFN layers in the later layers dominate the final model outputs, as indicated by the fact that value vectors with the highest cosine similarities to the probe vector tend to cluster in the later layers. The layers with strong attention on query and reasoning steps and those where high cosine similarity value vectors cluster do not align, providing insight into the possible mechanism behind the emergence of internal inconsistency. This finding is consistent with Figure 2 and Figure 6, suggesting that the model may make correct predictions in the middle layers but fails to fully utilize them in the later layers. To further understand the specific concepts these high-similarity value vectors represent and verify their importance in determining the final prediction, we project them onto the vocabulary space (nostalgebraist, 2020). Additional analysis are provided in Appendix B.6.3.

## 5 Related Work

Understanding the Inner Workings of Language ModelsThe rapid progress in LLM development has necessitated simultaneous efforts to interpret the inner workings of advanced models (Bricken et al., 2023; Ferrando et al., 2024). These works aim to provide an internal view of mechanisms to ensure safety and fairness (Burns et al., 2023; Zou et al., 2023; Li et al., 2024) and to further improve model inference (Schuster et al., 2022; Raposo et al., 2024). Some studies also examine CoT reasoning from an internal perspective and propose methods to teach models to perform implicit CoT (Deng et al., 2023). Unlike these approaches, which require additional training for interpretation, our internal consistency measure offers an off-the-shelf solution to calibrate CoT reasoning, providing a practical and efficient tool for enhancing model reliability.

Calibration in Language ModelsTraditional calibration methods (Platt et al., 1999; Naeini et al., 2015; Guo et al., 2017) train a parameterized classifier on validation data to adjust the final output of a neural network towards expected outcomes. In the context of LLMs, previous works apply trained parameterized models to the logits at the final layer (Zhao et al., 2021; Shen et al., 2024). In comparison, our study focuses on the phenomenon of internal inconsistency in CoT reasoning and demonstrates that internal consistency is a reliable unsupervised calibration measure.

Faithfulness of CoT ReasoningAlthough the reasoning capabilities of LLMs have been greatly enhanced with techniques like CoT reasoning (Wei et al., 2022; Yao et al., 2022; Xia et al., 2024), previous investigations into the faithfulness of CoT reasoning have shown that the model's generated rationales often are not consistent with their predictions (Lyu et al., 2023; Lanham et al., 2023). These studies primarily focus on the alignment between a model's verbalized reasoning and its outcomes,

Figure 5: The emergence of internal inconsistency in CoT reasoning could be attributed to **the misalignment between layers with high attention weights on critical tokens and those promotes certain predictions.** The histograms display attention weights for each part (context, query, and rationale) across self-attention layers, accompanied by a gray line indicating the count of value vectors in FFN layers that achieve high cosine similarity to the model’s final prediction.

without delving into the model's internal reasoning processes. In contrast, our work provides new insights into how unfaithfulness emerges internally during CoT reasoning and proposes solutions to calibrate reasoning with internal consistency.

Closest to our work is that of Halawi et al. (2023), which studies harmful imitation behaviors of LLMs and the underlying internal mechanisms. Unlike their study that focuses on few-shot learning with false demonstrations, we provide new insights on the phenomenon that CoT reasoning leads to internal inconsistency and that we can calibrate reasoning with internal consistency.

## 6 Conclusions

This work explores the use of internal representations to enhance the reliability of reasoning in LLMs. By examining CoT reasoning, we identify that although generated rationales can improve answer accuracy, they also lead to inconsistencies between middle and final layer representations, potentially affecting reliability. To mitigate this, we introduce internal consistency as a confidence measure, which evaluates the alignment of latent predictions from intermediate layers. Our extensive empirical studies across multiple models and datasets demonstrate that internal consistency is a robust indicator of correct reasoning paths, which we show can be further used for enhancing CoT reasoning by prioritizing paths with high internal consistency. Finally, our analysis on the patterns in attention and feed-forward networks across layers provides insights on why internal inconsistency emerges.

This work has several limitations. First, we restrict our empirical study on decoder-only models. While techniques like decoder lens (Langedijk et al., 2023) is a promising way to extend the concept of internal consistency to encoder-decoder models, we leave it for future work. Second, our analysis focuses on vanilla CoT prompting as the simplest approach for reasoning, whereas many other prompting techniques have been proposed (Gao et al., 2023; Yao et al., 2024). It would be interesting to further investigate internal consistency under these more complex scenarios.