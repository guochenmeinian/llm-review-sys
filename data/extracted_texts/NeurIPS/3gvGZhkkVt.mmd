# SEA: State-Exchange Attention for High-Fidelity Physics Based Transformers

Parsa Esmati

University of Bristol

parsa.esmati@bristol.ac.uk

&Amirhossein Dadashzadeh

University of Bristol

a.dadashzadeh@bristol.ac.uk

&Vahid Goodarzi Ardakani

Sabe Technology Limited

vahid.goodarzi@sabe.tech

&Nicolas Larrosa

University of Bristol

nicolas.larrosa@bristol.ac.uk

&Nicolo Grilli

University of Bristol

nicolo.grilli@bristol.ac.uk

###### Abstract

Current approaches using sequential networks have shown promise in estimating field variables for dynamical systems, but they are often limited by high rollout errors. The unresolved issue of rollout error accumulation results in unreliable estimations as the network predicts further into the future, with each step's error compounding and leading to an increase in inaccuracy. Here, we introduce the State-Exchange Attention (SEA) module, a novel transformer-based module enabling information exchange between encoded fields through multi-head cross-attention. The cross-field multidirectional information exchange design enables all state variables in the system to exchange information with one another, capturing physical relationships and symmetries between fields. Additionally, we introduce an efficient ViT-like mesh autoencoder to generate spatially coherent mesh embeddings for a large number of meshing cells. The SEA integrated transformer demonstrates the state-of-the-art rollout error compared to other competitive baselines. Specifically, we outperform PbGMR-GMUS Transformer-RealNVP and GMR-GMUS Transformer, with a reduction in error of 88% and 91%, respectively. Furthermore, we demonstrate that the SEA module alone can reduce errors by 97% for state variables that are highly dependent on other states of the system. The repository for this work is available at: https://github.com/ParsaEsmati/SEA

## 1 Introduction

Solving partial differential equations (PDE) has been a primary concern of many fields in science and engineering, including physics (Salvini et al., 2024), chemistry (Grilli et al., 2020), and material sciences (Grilli et al., 2018; Esmati et al., 2024). In many cases where a direct analytical solution of the PDE is impossible to obtain, numerical simulations are used. To solve these equations numerically, the domain is discretized into smaller cells using a discretization method such as finite element or finite volume methods. In some cases, the domain of interest is divided into millions of smaller elements forming large matrices representing the equation (Moukalled et al., 2016). Solving these discretized equations typically follows an iterative technique, which can take days and sometimes weeks of runtime to converge to the full solution on the defined temporal domain (Liu et al., 2024; Jiang et al., 2023). In some cases to resolve some specific features of the system a fine mesh isrequired. The graphical representation of small droplet (Um et al., 2018), cracking and brittle behaviour in thin components (Pfaff et al., 2014), multiphase and turbulent flows in computational fluid dynamics (CFD)(Kochkov et al., 2021; Thuerey et al., 2020; Heyse et al., 2021, 2021) are all instances of such scenario.

Finer mesh however comes at the price of computational cost, which may not be feasible in industry settings. Consequently, there is a critical need for frameworks that can either bypass these detailed simulations or accelerate the solvers.

Recent advances in sequential networks have shown promising results in estimating the state variables of dynamical systems over time. However, they have not yet been effectively utilized to bypass or accelerate numerical models (Yousif et al., 2022). Key challenges include lengthy training times and steep gradients in rollout errors (Sun et al., 2023; Han et al., 2022). Additionally, many of these networks are task-specific, necessitating retraining for each unique test case (Li et al., 2020). If a model is to be integrated into a solver, steep error accumulation necessitates frequent retraining of networks to maintain efficacy when bypassing numerical solvers. This leads to unnecessary computational costs, making it challenging to integrate these models with solvers. Ideally, a model should be capable of learning the underlying physics and constitutive laws to minimize error gradients. Such a model could reduce retraining frequency and enable reuse through transfer learning approaches (Yosinski et al., 2014).

The current trend towards probabilistic models, such as diffusion models for science and physics, tends to overcome the rollout errors (Han et al., 2024; Valencia et al., 2024). However, regardless of their domain (science, video, etc.), these models can produce unrealistic motions, especially for long sequence generations, and result in a temporal limitation (Weng et al., 2024). Hence, the autoregressive generation seems unavoidable for longer sequences.

Transformer-based architectures, in particular, are at the forefront of the autoregressive generations (Sun et al., 2023; Zhao et al., 2023). By drawing a parallel between the video and spatio-temporal PDE simulations, we intend to improve the unavoidable autoregressive rollout error for long sequence generation using transformers as the building block. Hence, inspired by the spatio-temporal cross-attention mechanisms used in vision models (Lin et al., 2022; Chen et al., 2021), we propose a State-Exchange Attention (SEA) module for physics-based transformers, designed to mimic PDE by explicitly capturing variable coupling. The effective multidirectional information exchange between fields enables the correction of some fields by others, allowing the model to learn the complex features of the physical system. This work employs a Vision Transformer (ViT) like (Dosovitskiy et al., 2020) encoder for mesh embedding. The embedded mesh at each instance is then used in the temporal State-Exchange Attention (SEA) integrated Transformer to predict the system's future states.

In summary, the **contributions** of this work include:

1. Design and integration of a novel SEA module for physics-domain Transformer models. This module enables learning the underlying physics through a multidirectional information exchange process between the state variables.
2. Assembly of a full ViT-SEA integrated framework that demonstrates state-of-the-art results in generating the complete sequence of the physical system given the initial sequence and specific time-invariant parameters representing the model.
3. Comprehensive evaluation of SEA module, and ViT-SEA integrated transformer across different computational fluid dynamics cases, showing over 60% reduction in error in all cases compared to state-of-the-art models.

## 2 Related work

In recent years, deep learning has led to major advancements in modeling physical systems, with contributions ranging from applying computer vision techniques to enhance the resolution of coarse meshes to incorporating physical symmetries and constraints through innovative modifications to learning objectives.

The work of (Raissi et al., 2019) demonstrated the feasibility of directly incorporating physical information into the objective function, including initial conditions and necessary physical residuals. Other examples of such approaches include (Jeon et al., 2024; Haghighat et al., 2021; Yu et al., 2022).

While these techniques embed physical knowledge into the objective function, they lack generality, as the obtained parameters tend to be case-specific. The broader underlying physics is not fully captured, and only the physical symmetries specific to a particular case are addressed.

Another class of methods is neural operators (Li et al., 2020). These methods generalize well across the PDEs they are trained on and are not case-specific. However, these models require data to be represented in higher dimensions to capture complex relationships. The application of an integral kernel in a high-dimensional representation of complex geometries with dense data points poses a significant computational challenge (Li et al., 2020). While neural operators can employ different architectures to reduce rollout error, they lack explicit mechanisms for integrating temporal data, unlike transformers, which handle sequence dependencies robustly through their attention mechanisms. This limitation can affect their effectiveness in applications that require sequential data processing.

A notable trend in recent years involves the use of encoder-decoder pairs to process dynamical states in latent space. The work of (Wiewel et al., 2019) is an early example of this approach. In general, the input must first be encoded into the latent space while preserving the context. A sequential network, such as LSTM, GRU, or other variants, is then trained on the encoded data. Thus, these approaches typically require two components: an encoder-decoder pair and a temporal model. Another example of this approach is Mesh Graph Networks (MGN) (Pfaff et al., 2020) and Graph-Network-based Simulators (GNS) (Sanchez-Gonzalez et al., 2020). The encoder modules of these models are based on graph networks. After processing nodes, edges, and features, they use a simple multi-layer perceptron (MLP) to compute the derivatives of the features over time and update the state using a forward-Euler scheme. While the encoder-decoder pair is a powerful tool, especially for unstructured mesh spaces, the lack of a robust time-stepping model significantly limits performance, with rollout error becoming a dominant issue.

Current state-of-the-art models that demonstrate optimal performance on baseline datasets typically combine a graph network encoder with more advanced time-stepping algorithms. Notably, the study by (Han et al., 2022) employs a Graph Mesh Reducer (GMR) for encoding, along with a sequential time-stepping network and a Graph Mesh Up-Sampling (GMUS) decoder. This research explored various sequential models, including LSTM, GRU, and Transformers, with the latter achieving the lowest rollout error. Building on these principles, (Sun et al., 2023) introduced a modified version of GMR-GMUS, adding a ReallNVP normalizing flow model to the time-stepping transformer. While adding a normalizing flow model does not yield a fully tractable model, it allows for the direct maximization of log-likelihood over the final data point, improving the overall objective and resulting in the most competitive baseline model reported thus far. However, these models still cannot fully address the rollout error in a systematic way that incorporates our physical understanding of the system.

As a result, improving rollout errors and reducing training time remain significant challenges in this field. In this work, we demonstrate that both objectives can be addressed by separating the fields and allowing them to learn the inherent physical relationships and symmetries. To this end, we introduce the SEA module, implemented on top of a Vision Transformer (ViT) based mesh autoencoder.

## 3 Methodology

### Problem statement

Temporal generation of the states in a dynamical system from an initial condition is analogous to video generation tasks, where the process is conditioned on both the initial state and an external input. Similarly, the evolution of a dynamical system can be conditioned on known parameters, such as the Reynolds number in fluid flow and the system's initial condition. Given this similarity to autoregressive generative models like ART-V (Weng et al., 2024), we formulate the temporal evolution as an autoregressive generation of states, conditioned on both the initial state and a time-invariant parameter.

However, autoregression on the mesh space is challenging due to the large size of the elements and the number of variables stored on each element. Hence the mesh must be embedded into a manageable embedding, and our formulation becomes an autoregressive sequence generation in latent space.

If we denote the fields with index \(i\), and time with index \(t\), then the stored data on field \(i\) at time \(t\) is presented by \(_{i}^{i}\). Given the proposed framework with the SEA module, we intend to encode the field groups separately. The field group refers to fields with the same dimensions (e.g., velocity in the \(x\)- and \(y\)-directions belong to the same group, while pressure lies in a separate group). Let \(\) represent the partition of fields into groups based on their dimensions (e.g., velocity, pressure). Each group is then encoded separately with the mesh encoder \(\), resulting in an encoded representation \(_{t}^{G_{k}}\) at time \(t\). The final encoded representation at time \(t\) is then given by:

\[_{t}^{G_{k}}=((\{_{t}^{i }\}_{i G_{k}})), G_{k}\]

We now denote \(_{t}^{j}\) as the encoded representation of the group \(G_{k}\), where \(j\) indexes the group embedding space. This embedding is then used to formulate the autoregressive sequence generation in time where the initial condition \(_{0}^{j}\) and the time invariant parameter \(\) (e.g., Reynolds number) are always available and used to condition the generation. We can formally express this problem as \(_{_{1:T}^{j}}P(_{1:T}^{j}_{0}^{j}, )\), where we aim to find the sequence of latent variables \(_{1:T}^{j}\) that maximizes the conditional probability given the initial condition \(_{0}^{j}\) and the time-invariant parameter \(\). This is achieved through a pointwise estimation of the conditional probabilities in the continuous embedding space, with optimization performed by minimizing the L2 loss. Further detail on objectives and training is provided in Appendix C.

### ViT mesh autoencoder

The backbones commonly used to create embedding spaces in image and video models, such as Latent Diffusion Models (LDM) (Rombach et al., 2022) and Video Diffusion Models (VDM) (Ho et al., 2022), cannot be directly applied to mesh data due to their inherent structured pixel inductive biases. Therefore, our proposed autoencoder must specifically overcome these biases for unstructured mesh space. Given that the temporal model employs a transformer backbone, the embedding must generate tokens compatible with the transformer's input requirements. These tokens are generated similarly to that of ViT (Dosovitskiy et al., 2020). Following the approach used in ViT, the space is divided into multiple patches, with each patch containing a number of cells. Let \(\) denote the domain in which our study is conducted, with dimension \(d\). Assume that the domain is discretized into a set of nodes \(\{x_{i}\}_{i=1}^{N}\), where each node \(x_{i}\) represents a point in \(^{d}\). To construct patches, we define a partitioning function \(f_{P}:^{d}\{1,2,,M\}\), which assigns each node \(x_{i}\) to one of \(M\) patches based on its coordinates.

Assuming the boundaries between patches are showing with \(B=\{b_{1},b_{2},,b_{m}\}\) then the function \(f_{P}\) is defined as follows:

\[f_{P}(x_{i})=1&x_{i}<b_{1},\\ j&b_{j-1} x_{i}<b_{j}2 j m,\\ m+1&x_{i} b_{m}.\]

To address the challenge of irregular and unstructured meshes, which lead to varying numbers of nodes per patch, each patch is padded to align with the length of the largest patch. A padding value of 0 is used throughout the framework. Moreover, bias terms are excluded in the embedding process, and the Gaussian Error Linear Unit (GELU) activation function (Hendrycks and Gimpel, 2016) is applied to ensure that the padded elements do not influence the spatial encoding. To achieve spatially aware embeddings and coherent reconstructions, we apply a multi-head self-attention mechanism (MHSA). This padding and embedding strategy is illustrated in Figure 1.

The output generated by the Vision Transformer (ViT) embedding module is subsequently flattened and utilized as tokens within the temporal model. The complete process is thoroughly explained in appendix A.

### Temporal and State-Exchange Attention model

The State-Exchange Attention module is integrated into the temporal model in this work to enhance the autoregressive generation of sequences in time. The temporal model utilizes a transformer architecture to capture the temporal dependencies of the state variables. This transformer includes the adaptive layer-norm by (Peebles and Xie, 2023) and the rotational positional embedding (RoPE) as developed in (Su et al., 2024) and adapted by state of the art autoregressive image generation models (Lu et al., 2024; Sun et al., 2024). The adaptive layer-norm employed in this work is modified to take the continuous time invariant parameters as input and act as a secondary conditioning mechanism on the temporal model. Full detail of the temporal model is provided in appendix B.

This work initializes a decoder block for each state variable in the given PDE For instance, the Navier-Stokes equations governing the fluid dynamics, requires the resolution of variables such as velocity and pressure each of these are assigned an expert decoder. The SEA module is designed to allow the exchange of information amongst these experts with cross attention. We further investigate other modes of information exchange in appendix D.

The flow of information through the expert layers and the SEA module can be formulated by building on the well-known attention mechanism. For clarity, the terms regarding the positional embedding are omitted here. We start from the encoded groups of variables presented in section 3.1, denoted by \(_{t}^{j}\). To simplify the notation and remove the explicit dependence on \(t\), we represent the sequence of encoded variables across all time steps as a single matrix, \(^{j}\), which stacks the encodings of all time steps. The attention mechanism then reads:

\[(^{j})=(^{j})K (^{j})^{T}}{}})V(^{j})\] (1)

Where \(K(^{j})\), \(Q(^{j})\), and \(V(^{j})\) represent the key, query, and value matrices, and are obtained from the linear transformation of the input \(^{j}\) by a set of learnable weights \(_{}\), \(_{}\), and \(_{}\). Additionally \(d_{k}\) represents the model dimension.

Furthermore the adaptive layer-norm is demonstrated by 'AdaLN' and hence following a pre-norm convention the multihead self-attention (MHSA) becomes:

\[(^{j})_{}=^{j}+((^{j})), j\] (2)

Given the autoregressive task at hand all the attention mechanisms including the presented MHSA have causal mask to improve autoregressive generation.

The field information flows through the information exchange module after the temporal self attention. This information exchange module is represented by the state-exchange attention in Figure 2, where we allow the state variables to exchange relevant information with a causal cross attention mechanism. This is formulated as:

Figure 1: The ViT-based mesh autoencoder divides the domain into patches and pads them to ensure equal sizes. An MLP is then applied to reduce the dimensionality, and the MHSA mechanism provides global awareness to create spatially coherent reconstructions with minimal patch artifacts. These patches are subsequently flattened and adapted as tokens for the temporal model.

\[(^{j})_{}=(^{j})_{}+_{ k\\ k j}_{}(((^{j})_ {}),((^{k})_{})), j \] (3)

Here, \(_{}(,)\) represents our information exchange mechanism SEA. This module takes in two arguments: the first is the adaptive layer norm of the expert for which the attention is taking place, and the second is the adaptive layer norm from the expert to which the module is attending. These are represented by \((^{j})\) and \((^{k})\), where \(k\) and \(j\) are non-equal embedding field indices.

To enhance efficiency during information exchange, we adopt a bottleneck mechanism inspired by expert-based architectures such as (Lee et al., 2024). Introducing a bottleneck at this stage helps keep the model scalable by enabling selective information exchange in a lower-dimensional space. Another instance of such strategy is the Perceiver architecture (Jaegle et al., 2021), where cross-attention creates a bottleneck for high-dimensional data from different modalities. In our case, we employ a simpler method, using a down-projection with learnable parameters, following the approach in (Lee et al., 2024). If the down- and up-projection matrices for mapping to the bottleneck are denoted by \(^{j}_{d}\) and \(^{j}_{u}\), respectively, the State-Exchange Attention mechanism is fully described by Equation 4. The arguments to this function are the AdaLN of \((^{j})_{}\) and \((^{k})_{}\), as shown in Equation 3. However, for clarity in illustrating the equation, we use \(^{j}\) and \(^{k}\) to represent the input arguments here.

\[_{}(^{j},^{k})=^{k}_{u}( (^{j}_{d}^{j})K(^{k}_ {d}^{k})^{T}}{}})V(^{k}_{d}^{k} ))\] (4)

In the presented equation \(K(^{j}_{d}^{j})\), \(Q(^{j}_{d}^{j})\), and \(V(^{j}_{d}^{j})\) represent the key, query and value matrices obtained form the down projection of the inputs.

Figure 2: (a) State-Exchange Attention (SEA) integrated Transformer model architecture, incorporating the additional modules of SEA and Time Invariant Parameter Injection (TIPI). Dashed lines represent the inclusion of additional fields. (b) Representation of the TIPI, designed to incorporate time-invariant information after the SEA module. (c) Schematic of the SEA module, illustrating how fields communicate through this module.

The conditioning of the generations on external parameters such as the Reynolds number is done using an indirect method of adaptive layer norm and a direct method of time invariant parameter injection module (TIPI). These components replace the more computationally intensive attention mechanism commonly used in physics domain autoregressive models. The TIPI component processes the time-invariant parameters using a multilayer perceptron (MLP) with a GELU activation function. The MLP maps the time-invariant parameters \(\) to the model's embedding dimension through learnable parameters. As shown in Equation 5, the information injector, represented by TIPI\([,]\), injects these processed parameters into the current state by summing the model's embedded information with the upscaled time-invariant parameters.

\[(_{i}^{k})_{}=(_{i}^{k})_{}+(([(_{i}^{k})_{}, ]))\] (5)

Figure 2 illustrates the architecture's detailed schematic.

## 4 Experiments

### Procedure

In this section, we evaluate the proposed model on two benchmark datasets and compare its performance with other frameworks. First, the complete model is tested on the cylinder flow benchmark, a widely used dataset in computational fluid dynamics. This evaluation includes the comparison of the full model with recent physics domain autoregressive models.

We then explore a multiphase case, where the model is tasked to resolve the interface by taking into account the fluxes caused by the velocities using the SEA module. In this section, only the temporal aspects of the model are varied (SEA module), while the ViT mesh autoencoder is fixed to isolate and eliminate the impact of encoding method on model's performance. To this end, separate decoders are assigned, one for velocity and another for volume fraction, similar to structure depicted in 2. The inclusion of the volume fraction allows us to study the extent of the model's capability to resolve interfaces and the effect of State-Exchange Attention on capturing the multiphase scenarios.

For consistent comparison with state-of-the-art models (Sun et al., 2023; Han et al., 2022), relative mean squared error is used to quantify the errors. The model was trained on an A100 GPU for approximately 2 hours for both datasets. Furthermore, a consistent Transformer architecture was adopted in both cases, utilizing 1 layer and 8 attention heads. The embedding dimension of the model for the cylinder flow case was set to 1024, in line with the literature (Sun et al., 2023), while a dimension of 2048 was used for the multiphase flow case to effectively capture the interface. Full details of the configurations and datasets are provided in Appendix G and F, respectively.

### Evaluation of ViT mesh autoencoder

The autoencoder used in the following experiments was trained exclusively with a reconstruction objective. The complete training procedure for this model is detailed in Appendix A. Reconstruction errors, compared to recent graph-based autoencoders, are presented in Table 1.

### Evaluation of temporal model on general case

We assess our complete architecture using the 2D cylinder flow, a benchmark dataset employed by other leading baseline models. In this case, the Navier-Stokes equation governs the motion of the fluid throughout the domain. Consequently, the trajectory to track is the velocity and pressure. The

   Dataset & Ours (ViT based autoencoder) & GMR-GMUS & PbGMR-GMUS \\  Cylinder flow & **1.7** & 14.3 & 1.9 \\ Multiphase flow & 6.3 & - & - \\   

Table 1: Relative reconstruction error for cylinder flow and multiphase flow reported in the unit of \(1 10^{-3}\).

mesh was initially tokenized at all time steps based on the explained ViT mesh encoder. These tokens were then fed through the SEA integrated model illustrated in Figure 2. During the training, the entire sequence was fed to the network for each batch. During inference, the model was set to estimate the trajectory autoregressively, and hence, the test errors correspond to the total rollout error. For the cylinder flow dataset, the error was evaluated over the case with Reynold's number of 400. This case was chosen to keep consistent with the other competitive models.

The recorded rollout error, and its comparison with other baseline models is illustrated in Figure 3.

In Figure 2(a), we show that our model outperforms the established state-of-the-art models across the board. Specifically, our results indicate an average improvement of 88% and 91% in reducing the error in autoregressive sequence generation compared to the PbGMR-GMUS Transformer-RealNVP and GMR-GMUS Transformer architectures, respectively. Furthermore, our model outperforms both variants of MGN architecture with 99% and 98% improvement over the base MGN model and MGN-NI, respectively. Full detail of the errors are presented in Table 2.

The evaluated test cases were post-processed to generate a contour map for further observation of the learned patterns and potential areas of error. For consistency with the work in literature (Han et al., 2022; Sun et al., 2023) the contour map of the case with Reynolds number of 400 is presented here. To fully explore our model's ability to capture complex flow features, such as the downstream vortex, we present visualizations at the 250th timestep, a point at which the Von Karman vortex street is fully developed, as shown in Figure 4. This timestep was deliberately chosen to visualize complex dynamics that are often challenging for traditional models to capture.

### Evaluation of temporal model on multi-phase

Evaluation of the presented model is extended to include a multiphase flow scenario, which complements the analysis presented in Section 4.3. The evaluated test case in this experiment corresponds to an immiscible collapse of two blocks of liquid due to density differential. In studies of multiphase flows, a critical aspect is the precise identification of fluid interfaces. This is accomplished by using a volume fraction state variable, denoted by \(\), which indicates the region occupied by the fluid. The value of \(\) ranges from 0 in one fluid to 1 in the other, effectively distinguishing between the

  Models & u & v & p & **Avg** \\  MGN (Pfaff et al., 2020) & 98 & 2036 & 673 & 935.6 \\ MGN-NI (Pfaff et al., 2020) & 25 & 778 & 136 & 313 \\ GMR-GMUS Transformer (Han et al., 2022) & 4.9 & 89 & 38 & 43.96 \\ PbGMR-GMUS Transformer-RealNVP (Sun et al., 2023) & 3.8 & 74 & 20 & 32.6 \\ Ours (Full ViT-SEA Transformer) & **0.35** & **10.7** & **0.3** & **3.7** \\   

Table 2: Time average rollout error of the presented model compared to other competitive models. The presented error is after decoding and corresponds to the real field error. The reported values are in the unit of \(1 10^{-3}\)

Figure 3: Comparison of the rollout error for the cylinder flow dataset.

two phases. To capture the phase, an additional block is assigned to the volume fraction which can communicate with other fields (Velocity in this case). Given the minor variations of pressure, we discard this variable here and only focus on the importance of the field communication between velocities and volume fraction through SEA module.

We investigate three possible variations of the Transformers to achieve this. First, we estimate the rollout error of the model with the SEA module. Second, we evaluate a basic model that encodes the fields into different latent spaces with no mode of information exchange. Finally, we assess a model that encodes all fields together into a single latent space, referred to as the Field Fusion Encoder (FFE) Transformer. This latter model corresponds to the Transformer architecture used in the PbGMR-GMUS Transformer-RealNVP and GMR-GMUS Transformer, as indicated in the provided results. The rollout error of these models are presented in Figure 5.

From the comparative results presented in Figure 5, it is evident that the models with the SEA module outperform the other variations. Most of the error observed in the average error plot in Figure 5 corresponds to the error in the volume fraction. The mean volume fraction errors over all time steps are 0.12, 0.25, and 4.61 for the SEA-integrated Transformer, basic Transformer, and FFE Transformer,

Figure 4: Contour maps of the generated fields at Re=400, and time step of 250 where Von Karman vortex street is formed.

Figure 5: Comparison between the Transformer with SEA module and other variations of Transformers used in the literature over the multi-phase dataset.

respectively. This represents approximately a 52% and 97% reduction in error with the integration of the SEA module, compared to the basic and FFE Transformers. Additionally, improvements of 48.5% and 40% are observed in the averaged velocity components.

Further demonstration of the contour maps is provided in Figure 6 where the actual interface tracking capability of the SEA enhanced transformer can be observed. Further visual results on the velocities are provided in Appendix H.

## 5 Discussion

The presented ViT-based mesh autoencoder and State-Exchange Attention (SEA) integrated Transformer module were evaluated through two different experiments on computational fluid dynamics (CFD) problems. A significant improvement in relative mean squared error was observed when the SEA module was deployed. During the cylinder flow evaluation, the full ViT-SEA integrated transformer framework achieved over 80% improvement compared to all competitive baselines. This improvement was accompanied by a lower gradient in the error, demonstrating a form of self-correction through information exchange between the velocity and pressure fields. The isolated SEA module was then tested on a multiphase case, resulting in a 97% improvement in the volume fraction compared to the field fusion encoder transformer, where the entire field is encoded into the same latent space. In the multiphase case, it was evident that the velocities exhibited a marginal error difference; approximately 40-50%; however, the volume fraction dominated the overall improvement. This is due to the significance of velocity in the displacement of the interface, whereas the volume fraction did not provide any valuable information to the velocity field. The error reduction observed with the deployment of the SEA module suggests that SEA module provides the necessary tools for the underlying physics of the governing to be captured.

The presented module, however, may face challenges when scaling to equations involving a large number of state variables. For instance, in multiphase flows with more than two phases or in grain growth within materials where each grain is represented by a state variable, the model would require a corresponding number of transformers to operate in parallel. This could lead to inefficiencies as the number of variables increases significantly.

## 6 Conclusion

The presented work introduces SEA, a novel module that enables the state variables of a physical system to exchange information within the transformer architecture. Transformers integrated with SEA demonstrated state-of-the-art performance, surpassing previously established benchmarks by other transformer-based models with improved autoregressive rollout error. The improved rollout error is an indication that the SEA module is enabling the model to learn the underlying physics. In future works, we will explore the scaling complexity of SEA to handle more state variables and its application in other domains. Furthermore, the integration of SEA with probabilistic models, such as diffusion models, will be investigated.

Figure 6: Comparison of the contour maps of the volume fraction (\(\)), between the predictions and ground truth in the case of \(=850\)